{"cell_type":{"a100d311":"code","19c2ad03":"code","1bfd999c":"code","26165d80":"code","fae739bb":"code","4b62e962":"code","56f59f29":"code","0a9810f9":"code","44d8e0de":"code","66a30785":"code","c300cc45":"code","535e256b":"code","ad6d6f69":"code","8bf90c55":"code","0e7d5bac":"code","c72d7fe8":"code","0be72b69":"code","507cb83b":"code","9964560c":"code","9e043a89":"markdown","ac00d278":"markdown","5c696b35":"markdown","7eb8bf62":"markdown","0d980ca0":"markdown","aeaa5312":"markdown","600f0502":"markdown","a1cfbe32":"markdown","cd1f0c00":"markdown","79b7e878":"markdown","00f5e458":"markdown"},"source":{"a100d311":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy.optimize import minimize\n\nimport gc\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations,callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras.utils import plot_model\n\nfrom keras.models import Model\nfrom kerastuner import RandomSearch, BayesianOptimization\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom tqdm.notebook import tqdm","19c2ad03":"RANDOM_STATE = 2021\n\n# OPTIM MODES\n# 1 - Only Keras Tuner\n# 2 - Keras Tuner + TOP models crossvalidation and submission\n# 3 - TOP models (NN configuration in dictionaty - params from local experiments) crossvalidation and submission\n\nOPTIM_MODE = 3","1bfd999c":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv', index_col = 'id')\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\", index_col = 'id')\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\n\ntarget = train.target\ntargets = pd.get_dummies(train['target'])\ntarget_optim = train['target'].apply(lambda x: int(x.split(\"_\")[-1])-1)\n\ntrain_knn = np.load(\"..\/input\/tps06-knn-feature-extraction\/add_feat_train.npy\")\ntest_knn = np.load(\"..\/input\/tps06-knn-feature-extraction\/add_feat_test.npy\")\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ntrain_knn = scaler.fit_transform(train_knn)\ntest_knn = scaler.transform(test_knn)","26165d80":"train = pd.concat([train.drop('target', axis = 1), \n                   pd.DataFrame(train_knn, columns = ['knn_1', 'knn_2', 'knn_3', 'knn_4', 'knn_5', 'knn_6', 'knn_7', 'knn_8', 'knn_9'])], axis = 1)\ntest = pd.concat([test.reset_index().drop('id', axis = 1), \n                   pd.DataFrame(test_knn, columns = ['knn_1', 'knn_2', 'knn_3', 'knn_4', 'knn_5', 'knn_6', 'knn_7', 'knn_8', 'knn_9'])], axis = 1, ignore_index=False)\n\ntrain['target'] = target","fae739bb":"X_train, X_valid, y_train, y_valid = train_test_split(train.drop('target', axis =1), targets, test_size = 0.2, stratify = targets, random_state = RANDOM_STATE)","4b62e962":"def custom_metric(y_true, y_pred):\n    y_pred = K.clip(y_pred, 1e-15, 1-1e-15)\n    loss = K.mean(cce(y_true, y_pred))\n    return loss\n\ncce = tf.keras.losses.CategoricalCrossentropy()\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_custom_metric', min_delta=0.00001, patience=6, verbose=0,\n    mode='min', baseline=None, restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_custom_metric', factor=0.04, patience=5, verbose=0,\n    mode='min')","56f59f29":"def model_builder(hp):\n\n    #--------- List of hyperparameters --------\n    # This is example to illustrate how it works. \n    # Feel free to use list of parameters as you want. Be aware .... the more parameters you specify the more resources (time) it will take\n    \n    emb_units = hp.Int('emb_units', min_value = 7, max_value = 8, step = 1)\n    #conv1d_filters = hp.Int('conv1d_units', min_value = 1, max_value = 2, step = 1)\n    \n    dropout_rates = [0.2, 0.4] #[0.2, 0.3, 0.4]\n    dropout1 = hp.Choice(\"drop_out1\", values = dropout_rates)\n    dropout2 = hp.Choice(\"drop_out2\", values = dropout_rates)\n    dropout3 = hp.Float(\"drop_out3\", min_value = 0.0, \n                        max_value = 0.5, \n                        default = 0.25, \n                        step = 0.05,)\n    \n    lin_nodes = [16, 64] #[16, 32, 64]\n    l1_nodes = hp.Choice(\"l1_units\", values = lin_nodes)\n    l2_nodes = hp.Choice(\"l2_units\", values = lin_nodes)\n    l3_nodes = hp.Choice(\"l3_units\", values = lin_nodes)\n    \n    learning_rates = hp.Choice(\"learning_rate\", [1e-2]) #[1e-2, 1e-3]\n    \n    non_linears = ['relu', 'elu'] #['relu', 'selu', 'elu']\n    act1 = hp.Choice('dense_act1', values = non_linears, default='relu')\n    act2 = hp.Choice('dense_act2', values = non_linears, default='relu')\n    act3 = hp.Choice('dense_act3', values = non_linears, default='relu')\n    \n    ker_inits = ['lecun_normal', 'he_uniform']\n    ker_init1 = hp.Choice('kern_init1', values = ker_inits, default = 'lecun_normal')\n    ker_init2 = hp.Choice('kern_init2', values = ker_inits, default = 'lecun_normal')\n    ker_init3 = hp.Choice('kern_init3', values = ker_inits, default = 'lecun_normal')\n    ker_init4 = hp.Choice('kern_init4', values = ker_inits, default = 'lecun_normal')\n    \n    conv_kernel = hp.Int('conv_kernel', min_value = 5, max_value = 20, step = 1)\n    #--------------------------------------\n    \n    conv_inputs = layers.Input(shape = (75))\n    knn_inputs = layers.Input(shape = (9))\n        \n    #----------- Embedding layers ----------------------\n    embed = layers.Embedding (input_dim = 353, \n                              output_dim = emb_units,\n                              embeddings_regularizer='l2')(conv_inputs)\n    \n    #----------- Convolution layers ----------------------\n    \n    embed = layers.Conv1D(conv_kernel, 1, activation = 'relu')(embed) \n    embed = layers.Flatten()(embed)\n    hidden = layers.Dropout(dropout1)(embed)\n    \n    #----------- Residual blocks layers ----------------------\n    hidden = tfa.layers.WeightNormalization(\n                layers.Dense(\n                units = l1_nodes,\n                activation = act1, #selu\n                kernel_initializer = ker_init1))(hidden)\n   \n    \n    output = layers.Dropout(dropout2)(layers.Concatenate()([embed, hidden, knn_inputs]))\n   \n    output = tfa.layers.WeightNormalization(\n    layers.Dense(\n                units = l2_nodes,\n                activation = act2,\n                kernel_initializer = ker_init2))(output) \n    \n\n    output = layers.Dropout(dropout3)(layers.Concatenate()([embed, hidden, output]))\n    output = tfa.layers.WeightNormalization(\n    layers.Dense(\n                units = l3_nodes, \n                activation = act3, #elu\n                kernel_initializer = ker_init3))(output)\n    \n    #----------- Final layer -----------------------\n    \n    conv_outputs = layers.Dense(\n                units = 9, \n                activation = 'softmax',\n                kernel_initializer = ker_init4)(output)\n    \n    #----------- Model instantiation  ---------------\n    model = Model([conv_inputs, knn_inputs],conv_outputs)\n    model.compile(loss = 'categorical_crossentropy',\n                  optimizer = keras.optimizers.RMSprop(learning_rates), \n                  metrics = custom_metric)\n    \n    return model","0a9810f9":"tuner = BayesianOptimization(\n    model_builder,\n    objective = \"val_loss\",\n    max_trials = 100, # This is only demo - you can play with more trials on local machine (Kaggle is resource limited). I usually run from 100-1000 trials for best params.\n    executions_per_trial = 2,\n    overwrite = True,\n    seed = RANDOM_STATE,\n    directory = \"tps-06\",\n    project_name = \"nn-embeddings\",\n)","44d8e0de":"tuner.search_space_summary()","66a30785":"# Epochs 2-5 is the best in this competition\nif not (OPTIM_MODE == 3):\n    tuner.search([X_train.iloc[:, :75], X_train.iloc[:, 75:]], y_train, epochs = 3, validation_data = ([X_valid.iloc[:, :75], X_valid.iloc[:, 75:]], y_valid))","c300cc45":"if not (OPTIM_MODE == 3):\n    tuner.results_summary(num_trials = 5)","535e256b":"if not (OPTIM_MODE == 3):\n    best_hp = tuner.get_best_hyperparameters()[0]\n    model = tuner.hypermodel.build(best_hp)\n    model.summary()","ad6d6f69":"if not (OPTIM_MODE == 3):\n    plot_model(model)","8bf90c55":"# Here are list of TOP3 params found during my research (1000 trials)   \nnet_params = [{'emb_units': 8, 'conv1d_units': 1, \n               'drop_out1': 0.3, 'drop_out2': 0.4, 'drop_out3': 0.2, \n               'l1_units': 16, 'l2_units': 64, 'l3_units': 16, \n               'learning_rate': 0.001, \n               'dense_act1': 'elu', 'dense_act2': 'relu', 'dense_act3': 'relu',\n              'kern_init1': 'he_uniform', 'kern_init2': 'he_uniform', 'kern_init3': 'he_uniform', 'kern_init4': 'lecun_normal'},\n              {'emb_units': 8, 'conv1d_units': 1, \n               'drop_out1': 0.3, 'drop_out2': 0.4, 'drop_out3': 0.2, \n               'l1_units': 16, 'l2_units': 64, 'l3_units': 16, \n               'learning_rate': 0.001, \n               'dense_act1': 'elu', 'dense_act2': 'relu', 'dense_act3': 'relu',\n              'kern_init1': 'he_uniform', 'kern_init2': 'he_uniform', 'kern_init3': 'he_uniform', 'kern_init4': 'lecun_normal'},\n              {'emb_units': 7, 'conv1d_units': 1, \n               'drop_out1': 0.3, 'drop_out2': 0.2, 'drop_out3': 0.2, \n               'l1_units': 16, 'l2_units': 128, 'l3_units': 32, \n               'learning_rate': 0.001, \n               'dense_act1': 'elu', 'dense_act2': 'relu', 'dense_act3': 'relu',\n              'kern_init1': 'he_uniform', 'kern_init2': 'he_uniform', 'kern_init3': 'he_uniform', 'kern_init4': 'lecun_normal'}\n              ]","0e7d5bac":"def model_builder_optimized(net_config):\n \n    emb_units = net_config['emb_units']\n    conv1d_filters = net_config['conv1d_units']\n    \n    dropout1 = net_config[\"drop_out1\"]\n    dropout2 = net_config[\"drop_out2\"]\n    dropout3 = net_config[\"drop_out3\"]\n\n    l1_nodes = net_config[\"l1_units\"]\n    l2_nodes = net_config[\"l2_units\"]\n    l3_nodes = net_config[\"l3_units\"]\n    \n    learning_rates = net_config[\"learning_rate\"]\n\n    act1 = net_config['dense_act1']\n    act2 = net_config['dense_act2']\n    act3 = net_config['dense_act3']\n    \n\n    ker_init1 = net_config['kern_init1']\n    ker_init2 = net_config['kern_init2']\n    ker_init3 = net_config['kern_init3']\n    ker_init4 = net_config['kern_init4']\n    #--------------------------------------\n    \n    \n    \n    conv_inputs = layers.Input(shape = (75))\n    knn_inputs = layers.Input(shape = (9))\n    #----------- Embedding layers ----------------------\n    embed = layers.Embedding (input_dim = 353, \n                              output_dim = emb_units,\n                              embeddings_regularizer='l2')(conv_inputs)\n    \n    #----------- Convolution layers ----------------------\n    \n    embed = layers.Conv1D(10, conv1d_filters, activation = 'relu')(embed) \n    embed = layers.Flatten()(embed)\n    hidden = layers.Dropout(dropout1)(embed)\n    \n    #----------- Residual blocks layers ----------------------\n    hidden = tfa.layers.WeightNormalization(\n                layers.Dense(\n                units = l1_nodes,\n                activation = act1, #selu\n                kernel_initializer = ker_init1))(hidden)\n    \n   \n    output = layers.Dropout(dropout2)(layers.Concatenate()([embed, hidden, knn_inputs]))\n   \n    output = tfa.layers.WeightNormalization(\n    layers.Dense(\n                units = l2_nodes,\n                activation = act2,\n                kernel_initializer = ker_init2))(output) \n    \n\n    output = layers.Dropout(dropout3)(layers.Concatenate()([embed, hidden, output]))\n    output = tfa.layers.WeightNormalization(\n    layers.Dense(\n                units = l3_nodes, \n                activation = act3, #elu\n                kernel_initializer = ker_init3))(output)\n    \n    \n    #----------- Final layer -----------------------\n    \n    conv_outputs = layers.Dense(\n                units = 9, \n                activation = 'softmax',\n                kernel_initializer = ker_init4)(output)\n    \n    #----------- Model instantiation  ---------------\n    model = Model([conv_inputs, knn_inputs], conv_outputs)\n    model.compile(loss = 'categorical_crossentropy',\n                  optimizer = keras.optimizers.Adam(learning_rates), \n                  metrics = custom_metric)\n    \n    return model","c72d7fe8":"def inter_class_optimizer(weights, a0, a1, a2, a3, a4, a5, a6, a7, a8):\n    oof = np.array([weights[0]*a0, \n                    weights[1]*a1, \n                    weights[2]*a2, \n                    weights[3]*a3, \n                    weights[4]*a4, \n                    weights[5]*a5, \n                    weights[6]*a6, \n                    weights[7]*a7, \n                    weights[8]*a8]).transpose()\n    \n    oof = oof \/ np.sum(oof, axis=1).reshape(-1, 1)\n    return log_loss(y_val, oof)\n\n\ndef pred_fold_optimizer(oof_preds, test_preds):\n    \n    cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n    res = minimize(fun=inter_class_optimizer,\n                   x0=[1\/9 for _ in range(9)],\n                   args=tuple(oof_preds[ :, i] for i in range(9)),\n                   method= 'Nelder-Mead',\n                   options={'maxiter': 300},\n                   bounds=[(0.0, 1.0)] * len(oof_class_preds),\n                   constraints=cons)\n\n    oof_preds = np.array([res.x[i]*oof_preds[ :, i] for i in range(9)]).transpose()\n    oof_preds = oof_preds \/ np.sum(oof_preds, axis=1).reshape(-1, 1)\n    \n    test_preds = np.array([res.x[i]*test_preds[:, i] for i in range(9)]).transpose()\n    test_preds = test_preds \/ np.sum(test_preds, axis=1).reshape(-1, 1)\n\n    return res[\"fun\"], test_preds, oof_preds","0be72b69":"def inter_model_optimizer(weights):\n    final_prediction = 0\n    \n    for weight, prediction in zip(weights, oof_class_preds):\n        final_prediction += weight * prediction\n    \n    return log_loss(y_val, final_prediction)\n\ndef pred_model_optimizer(oof_class_preds, test_class_preds):\n    optmized_oof_nn_preds = 0\n    optmized_test_nn_preds = 0\n    \n    starting_values = [1\/len(oof_class_preds)] * len(oof_class_preds)\n    \n    cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n    res = minimize(inter_model_optimizer, \n                   starting_values,\n                   method='Nelder-Mead',\n                   bounds=[(0.0, 1.0)] * len(oof_class_preds),\n                   constraints=cons)\n    \n    print(f'--- Inter model optimized logloss: {(res[\"fun\"]):.5f} using {res[\"x\"]} weights (sum:{np.sum(res[\"x\"])}) ---\\n')\n\n    for weight, prediction in zip(res[\"x\"], oof_class_preds):\n        optmized_oof_nn_preds += weight * prediction\n    \n    for weight, prediction in zip(res[\"x\"], test_class_preds):\n        optmized_test_nn_preds += weight * prediction\n\n        \n    return optmized_oof_nn_preds, optmized_test_nn_preds","507cb83b":"EPOCH = 70\n\nN_FOLDS = 10\nRANDOM_STATES_NUM = 3\nNUM_TOP_MODELS = 3\n\ny_val = []\npred_NN_a = np.zeros((test.shape[0],9))\npred_NN_a_optimized = np.zeros((test.shape[0],9))\n\n\nif not (OPTIM_MODE == 1): \n    tuners = tuner.get_best_hyperparameters(num_trials = NUM_TOP_MODELS)\n    print(f'----- Training and blending {N_FOLDS * RANDOM_STATES_NUM * NUM_TOP_MODELS} models -----')\n\n    for rs_n in range(RANDOM_STATES_NUM):\n        print(F\"\\n- RANDOM STATE {RANDOM_STATE + rs_n} -\")\n        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state = (RANDOM_STATE + rs_n))\n\n        oof_NN_a = np.zeros((train.shape[0],9))\n        oof_NN_a_optim = np.zeros((train.shape[0],9))\n        oof_NN_fold_optimized = np.zeros((train.shape[0],9))\n       \n        for fold, (tr_idx, ts_idx) in enumerate(skf.split(train, train.target)):\n\n            X_train = train.iloc[:, :75].iloc[tr_idx]\n            X_train_knn = train.iloc[:, 75:-1].iloc[tr_idx]\n            y_train = targets.iloc[tr_idx]\n            \n            X_test = train.iloc[:, :75].iloc[ts_idx]\n            X_test_knn = train.iloc[:, 75:-1].iloc[ts_idx]\n            y_test = targets.iloc[ts_idx]\n            \n            oof_class_preds = []\n            test_class_preds = []\n\n            for n_models in range(NUM_TOP_MODELS):\n\n                K.clear_session()  \n\n                if OPTIM_MODE == 2:\n                    params = tuners[n_models]   \n                    model_conv = tuner.hypermodel.build(params)\n                    l_rate = best_hp.get('learning_rate')\n                else:\n                    model_conv = model_builder_optimized(net_params[n_models])\n                    l_rate = net_params[n_models][\"learning_rate\"]\n\n                model_conv.compile(loss='categorical_crossentropy', \n                                        optimizer = keras.optimizers.Adam(learning_rate = l_rate), \n                                        metrics=custom_metric)\n\n                model_conv.fit([X_train, X_train_knn], y_train,\n                          batch_size = 128, epochs = EPOCH,\n                          validation_data=([X_test, X_test_knn], y_test),\n                          callbacks=[es, plateau],\n                          verbose = 0)\n\n                pred_a = model_conv.predict([X_test, X_test_knn]) \n                score_NN_a = log_loss(y_test, pred_a)  \n                \n                test_NN_preds = model_conv.predict([test.iloc[:, :75], test.iloc[:, 75:]]) \n                \n                y_val = target_optim.iloc[ts_idx]\n                optim_score, test_preds_optim, oof_preds_optim = pred_fold_optimizer(pred_a, test_NN_preds)\n                 \n                print(f\"  * FOLD {fold + 1} -> MODEL {n_models + 1} -> SCORE: {(score_NN_a):.5f} -> OPTIMIZED SCORE: {optim_score:.5f} (GAIN: {(optim_score-score_NN_a):.5f})\")\n                \n                pred_NN_a += test_preds_optim\n                oof_NN_a[ts_idx] += pred_a \n                oof_NN_a_optim[ts_idx] += oof_preds_optim \n                \n                # ---\n                oof_class_preds.append(oof_preds_optim)\n                test_class_preds.append(test_preds_optim)   \n                # ---\n        \n            oof_NN_fold_optimized[ts_idx], pred_NN_optimized = pred_model_optimizer(oof_class_preds, test_class_preds)\n            pred_NN_a_optimized += pred_NN_optimized\n\n        score_a = log_loss(targets, (oof_NN_a \/ NUM_TOP_MODELS))\n        score_o = log_loss(targets, oof_NN_fold_optimized)\n        print(f\"- FINAL SCORE FOR {n_models + 1} MODELS IN RANDOM STATE {RANDOM_STATE + rs_n}: {score_a:.5f} - OPTIMIZED (inter class and model): {score_o:.5f} (GAIN: {(score_o-score_a):.5f})\")\n\n    pred_NN_a = pred_NN_a \/ (N_FOLDS * RANDOM_STATES_NUM * NUM_TOP_MODELS)\n    pred_NN_a_optimized = pred_NN_a_optimized \/  (N_FOLDS * RANDOM_STATES_NUM)","9964560c":"if not (OPTIM_MODE == 1):\n    pred_embedding = pred_NN_a_optimized\n    submission['Class_1']=pred_embedding[:,0]\n    submission['Class_2']=pred_embedding[:,1]\n    submission['Class_3']=pred_embedding[:,2]\n    submission['Class_4']=pred_embedding[:,3]\n    submission['Class_5']=pred_embedding[:,4]\n    submission['Class_6']=pred_embedding[:,5]\n    submission['Class_7']=pred_embedding[:,6]\n    submission['Class_8']=pred_embedding[:,7]\n    submission['Class_9']=pred_embedding[:,8]\n\n    submission.to_csv(\"26-tps06-keras-tuner.csv\", index=False)\n","9e043a89":"## BEST MODEL PARAMETERS - DEFINED MODEL\nHere you can find model parameters found in my research - no OPTIM mode required (this is for speed up learning process)","ac00d278":"## KERAS TUNING\nWarning! Time tuning for max_trials = 100 takes (in this case - network configuration\/data) about 4h on Kaggle GPU machine. ","5c696b35":"### Simple NN (Embeddings -> Conv1D -> Residual) and Keras Tuner (finding best nn architecture and parameters)\n\nContribution:\n- This is fork from **Laurent Pourchot** great contribution in TPS-06 (so please upvote source kernel): https:\/\/www.kaggle.com\/pourchot\/simple-neural-network\n- KNN Feature extraction by Melanie (Melanie7744)- [TPS6-Boost your score with KNN features](https:\/\/www.kaggle.com\/melanie7744\/tps6-boost-your-score-with-knn-features)  (I tried to link data directly from notebook but unfortunately has not found it in \"Notebook outputs\")\n\nSince I see that many of us uses NN in this competition I made this notebook to illustrate how to search for best NN parameters using [Keras Tuner](https:\/\/keras.io\/api\/keras_tuner\/) implementation. We will search for:\n- Linear layer nodes number\n- Learning rate\n- Dropout parameters (rate)\n- Embedding dimension\n- Conv1D kernel size\n- Conv1D number of filters\n- Non linear modules \n- Linear layer initializers\n\n**Now you can run thin notebook in three modes**:\n1. (MEDIUM RESOURCES and TIME) - Hyperparameter search only mode using Keras Tuner\n2. (MASSIVE RESOURCES and TIME) - Hyperparameter seach -> TOP models selection -> models crossvalidated training -> submission\n3. (FAST) - No hyperparameter search - models are build based on nn config (parameters found on my local machine) -> model crossvalidation -> submission  \n<br>\n\n<div align=\"center\"><img src=\"https:\/\/i.imgur.com\/vS33JbV.jpeg\"\/><\/div>\n<br>\n<div class=\"alert alert-warning\" role=\"alert\">\n  I know that on Kaggle we have limited resources (9h session and GPU limit) but this notebook could help you in finding best params locally and then use the best one in final Cross Validation (second part of notebook). \n<\/div>\n<div class=\"alert alert-danger\" role=\"alert\">\n  This notebook is UNDER development. Next steps:\n    <ul><li>Currently I am looking for best hyperparameters on local machine (about 1000 trials = 48-56h).<\/li> \n        <li>When finished I will upload results and create optimal NN search classes<\/li>\n        <li>Best voting weights for model under development - scipy (minimize) <\/li>\n    <\/ul>\n<\/div>\n","7eb8bf62":"### MODEL CROSSVALIDATION AND OPTIMIZATION","0d980ca0":"## FINAL SUBMISSION","aeaa5312":"## K-FOLD NN TRAINING BASED ON BEST PARAMETERS SEARCH RESULT- 90 MODELS\n- Different random states\n- TOP 3 models from Keras Tuner hyperparameter optimization\n- NN Cross Validation ","600f0502":"## BEST MODEL PARAMETERS - OPTIM MODE (KERAS TUNER RESULTS)","a1cfbe32":"## KERAS TUNER OPTIMIZATION","cd1f0c00":"## KERAS TUNER LEADERBOARD (TOP 5)","79b7e878":"## PARAMETER SEARCH ","00f5e458":"### SIMPLEX OPTIMIZER\nInspired by BIZEN notebook: https:\/\/www.kaggle.com\/hiro5299834\/tps06-nns-gbts-optimization\n"}}