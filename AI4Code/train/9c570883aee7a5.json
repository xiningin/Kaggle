{"cell_type":{"3fd40023":"code","2da2d02e":"code","125a3ff9":"code","7718fa96":"code","266bcce4":"code","13d88d63":"code","2c5f9a0c":"code","16d367b0":"code","6128b3cf":"code","92304b26":"code","94c6462a":"code","feb371b2":"code","5c72db36":"code","05af0414":"code","50546e73":"code","4d301ef8":"code","f1f782c0":"code","495e2840":"code","7fa8f065":"code","34eb641d":"code","4113a35d":"code","23b086fd":"code","e1aeed9f":"code","4096b62f":"code","692ccbc1":"markdown","1ab9fab9":"markdown","05d8b558":"markdown","fbcc4641":"markdown","9c869d1c":"markdown","afb4e1b8":"markdown","337a3270":"markdown","5e927bf6":"markdown","3e6a97e0":"markdown","ecf2f7d3":"markdown","9f7746f9":"markdown","c1f82bad":"markdown","7bb85f60":"markdown","8945fcc0":"markdown","613cb8de":"markdown","2c28eaa4":"markdown","7a06e0b8":"markdown","6ed2039a":"markdown","99de0c92":"markdown","ec5f7ac9":"markdown","1a5ecd4d":"markdown","613adb31":"markdown","9c9b7459":"markdown","d909e507":"markdown","87c9545a":"markdown","e2ff417b":"markdown","dee0d597":"markdown","757aeb1b":"markdown"},"source":{"3fd40023":"import tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nwarnings.filterwarnings('ignore')","2da2d02e":"# Read the test and train data sets. \ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","125a3ff9":"print(\"Shapes of training and test datasets:\");print(df_train.shape,df_test.shape)\nprint(\"Training Data Sample\");display(df_train.head())\nprint(\"Test Data Sample\");display(df_test.head())","7718fa96":"# The following will be true if the column names in the training and test data sets are identical\nset(df_train.columns[df_train.columns != 'target']) == set(df_test.columns)","266bcce4":"print(\"Training Data Summary\");df_train.describe()","13d88d63":"print(\"Test Data Summary\");df_test.describe()","2c5f9a0c":"fig, ax = plt.subplots()\nrandom_cols = np.random.choice(range(1,df_train.shape[1]-1),16)\nfor col in df_train.columns[random_cols]:\n    sns.kdeplot(df_train[col], ax = ax)\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.title('Distribution of features in the Training Set (Sample of 16 features)')","16d367b0":"fig, ax = plt.subplots()\nrandom_cols = np.random.choice(range(1,df_test.shape[1]),16)\nfor col in df_test.columns[random_cols]:\n    sns.kdeplot(df_train[col], ax = ax)\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.title('Distribution of features in the Test Set (Sample of 16 features)')","6128b3cf":"sns.set()\nax = sns.scatterplot(\n    x = df_train.loc[:,(df_train.columns != 'target') & (df_train.columns != 'id') ].median(axis = 0),\n    y = df_train.loc[:,(df_train.columns != 'target') & (df_train.columns != 'id') ].std(axis = 0), alpha = 0.8)\nax.set(xlabel='Median Values', ylabel='standard deviations')\nplt.title('Medians vs. Standard Deviations of all the columns in the training data')","92304b26":"median_vals = df_train.loc[:,(df_train.columns != 'target') & (df_train.columns != 'id') ].median(axis = 0)\nmedian_vals[median_vals > 100]","94c6462a":"col = 'wheezy-copper-turtle-magic'\nsns.distplot(df_train[col])","feb371b2":"X_train, X_val, y_train, y_val = train_test_split(df_train.drop(columns = ['id','target']),df_train['target'], test_size=0.15, random_state=2)","5c72db36":"sns.countplot(df_train['target'])\nplt.title('Distribution of target variable in the training data')","05af0414":"# Randomly choose either True or False for each row. \ny_blind_guess = np.random.randint(2, size = len(y_val))\nprint(\"Accuracy with blind guessing: %.2f\" % accuracy_score(y_val, y_blind_guess, normalize=True))","50546e73":"logreg = LogisticRegression().fit(X_train,y_train)\n#y_pred = logreg.predict(X_val)\nprint(\"Accuracy of logisic Regression on Validation Set: %.2f\" % logreg.score(X_val, y_val))","4d301ef8":"fig, axs = plt.subplots(ncols = 2 ,nrows=2, figsize=(8,8))\nrandom_cols = df_train.columns[np.random.choice(range(1,df_train.shape[1]-1),8)]\nsns.scatterplot(x = random_cols[0], y = random_cols[1], hue = 'target', data = df_train, ax=axs[0,0])\nsns.scatterplot(x = random_cols[2], y = random_cols[3], hue = 'target', data = df_train, ax=axs[0,1])\nsns.scatterplot(x = random_cols[4], y = random_cols[5], hue = 'target', data = df_train, ax=axs[1,0])\nsns.scatterplot(x = random_cols[6], y = random_cols[7], hue = 'target', data = df_train, ax=axs[1,1])","f1f782c0":"set(df_test['wheezy-copper-turtle-magic']) - set(df_train['wheezy-copper-turtle-magic'])","495e2840":"df_train_2 = pd.get_dummies(df_train, columns = ['wheezy-copper-turtle-magic'], prefix='wctm-')\ndf_test_2 = pd.get_dummies(df_test, columns = ['wheezy-copper-turtle-magic'], prefix='wctm-')\nX_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(df_train_2.drop(columns = ['id','target']),df_train_2['target'], test_size=0.15, random_state=2)","7fa8f065":"logreg_2 = LogisticRegression().fit(X_train_2,y_train_2)\n#y_pred = logreg.predict(X_val)\nprint(\"Accuracy of logisic Regression on Validation Set: %.2f\" % logreg_2.score(X_val_2, y_val_2))","34eb641d":"# Code from: https:\/\/www.kaggle.com\/cdeotte\/logistic-regression-0-800\n    \nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ncols = [c for c in train.columns if c not in ['id', 'target']]\noof = np.zeros(len(train))\nskf = StratifiedKFold(n_splits=5, random_state=42)\n\n# INITIALIZE VARIABLES\ncols.remove('wheezy-copper-turtle-magic')\ninteractions = np.zeros((512,255))\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor i in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    test2.reset_index(drop=True,inplace=True)\n    \n    skf = StratifiedKFold(n_splits=25, random_state=42)\n    for train_index, test_index in skf.split(train2.iloc[:,1:-1], train2['target']):\n        # LOGISTIC REGRESSION MODEL\n        clf = LogisticRegression(solver='liblinear',penalty='l1',C=0.05)\n        clf.fit(train2.loc[train_index][cols],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train2.loc[test_index][cols])[:,1]\n        preds[idx2] += clf.predict_proba(test2[cols])[:,1] \/ 25.0\n        # RECORD INTERACTIONS\n        for j in range(255):\n            if clf.coef_[0][j]>0: interactions[i,j] = 1\n            elif clf.coef_[0][j]<0: interactions[i,j] = -1\n    #if i%25==0: print(i)\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('LR with interactions scores CV =',round(auc,5))","4113a35d":"nn_model = keras.Sequential([\n    keras.layers.Dense(1024, input_dim = X_train_2.shape[1]),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation('relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(256),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation('relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(64),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation('relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nnn_model.compile(optimizer= 'adam',loss='binary_crossentropy',metrics=['accuracy'])","23b086fd":"nn_model.summary()","e1aeed9f":"nn_history = nn_model.fit(X_train_2.values, y_train_2.values,\n                                  epochs=5,\n                                  batch_size=128,\n                                  validation_data=(X_val_2.values, y_val_2.values),\n                                  verbose=2)","4096b62f":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('submission.csv',index=False)","692ccbc1":"We can see from the above that only \"wheezy-copper-turtle-magic\" is the outlier column. Let us take a look at its distribution. We can see that its distribution is signficantly different from other columns","1ab9fab9":"Let us first read the input data, and display the first few rows of the dataframes with titles. ","05d8b558":"From the above we can imagine that it might be hard to classify the target variable based on the linear combinations of the columns. ","fbcc4641":"Above operation shows that the column \"wheezy-copper-turtle-magic\", doesn't take any additional values in the test data. ","9c869d1c":"**It looks like the data is mostly numerical, with random column names, and we need to predict the \"target\" column**.","afb4e1b8":"Let us first divide the data and training and validation data sets","337a3270":"So, as highlighted in Chris's excellent Kernel here: https:\/\/www.kaggle.com\/cdeotte\/logistic-regression-0-800, let us build a logistic regression model for each category to see if that improves the score significantly. ","5e927bf6":"### Model 3: Neural Network (in-progress)","3e6a97e0":"Let us first start with the blind guess model where we select each row as either 1 or 0 randomly. Based on the disribution of the target variable as shown below, we expect about 50% accuracy for the blind guess model. ","ecf2f7d3":"**Background**: When I started working on this kernel, I wanted to do some EDA and explore a few classification methods to see how I could achieve a decent score. Now that there are so many other excellent kernels out there with very high scores, I want to devote this kernel to shed some light one why some approaches worked better the others. I hope this will be useful for others who are confused by all these new records being set every other day in this competition. ","9f7746f9":"# Comparing Various Models:\n\n## Model 0: Blind guessing\n","c1f82bad":"**As expected, blind guessing gave us a 50% accuracy based on the distribution of the data. Let us see how quickly we can improve the score beyond the 50%.**","7bb85f60":"**It was identified by others that wheezy-copper-turtle-magic appears to be a categorical variable, with a distribution that is diferent from rest of the variables**. How do we go about finding such a thing ? \n\nSince we expected all the columns to have similar median \/ standard deviation values, let us plot all the medians vs. standard deviations to see if we can find an outliers. \n","8945fcc0":"**It looks like all the training and test, columns are very similar in distribution with mean value of about 0, and standard deviation of about 1.7 **\n**Let us confirm that by looking at distbrutions of 16 randomly sampled columns**","613cb8de":"**So, our model got a 52% accuracy. That is very slightly better than blind guessing, which should give us 50% accuracy as seen in the previous guess. There is lot of room for improvement !**\n\n### Question 1: Why did our logistic regression give only 2% improvement on the baseline blind guess model ? \n\nOther than saying that probably the data (without any feature engineering) didn't have a clear linear correlation with the target variable, we cannot conclude much at this point. But for a better understanding, let us take a look at how target variable is encoded with respect to a few randomly selected columns. ","2c28eaa4":"**Great, we can see tha the distributions of the variables are very close to each other, centered around zero. There shouldn't be much need for normalizing the variables. ** \n\n\n**Let us also look at the test set to make sure.**","7a06e0b8":"We can see that the accuracy greatly improved once we identified that we needed to have separate models for each category, and that just having the \"magic\" column as a categorical column was not enough. ","6ed2039a":"### Overview: \nWe will start with linear classifiers like logistic regression and see why or why not they are better than blind guessing for this data and then go with more complicated classifiers. Emphasis would be in understanding the results of each iteration rather than getting the best performance. ","99de0c92":"Let us try a neural network with Tensorflow, to see if we can achieve better accuracy. \n\n\n**Note:** Some of this model was inspired by: https:\/\/www.kaggle.com\/dimitreoliveira\/instant-gratification-deep-learning","ec5f7ac9":"**Next step is too look at distributions of various columns in the test and training data sets. You have to scroll from left right to see all the columns.**","1a5ecd4d":"We can clearly see that there are one or more points with Median value close to 250 and standard deviation close to 140. Let us filter these columns to figure out what they are. ","613adb31":"| Model  | Validation Set Accuracy   | Comments |  \n|---|---|---|---|\n| Blind Guessing  | 50%  |Based-line Model |\n|  Logistic Regress with all columns as numeric | 52%    |   |  \n|  Logistic Regression with one column as categorical | 51%  |   No Interactions between columns |  \n|  Logistic Regression with one column as categorical, with interactions | 80%  |  With Interactions between columns |  ","9c9b7459":"**It looks like our neural network model is giving better accuracy than the simple logistic regression\/random forest models**","d909e507":"## Model 1: Logistic Regression with all features considered as numerical columns\nLet us first start with a linear model for logistic regression using all the features","87c9545a":"**Let us also make sure the training and test column names are same, other than the target column**","e2ff417b":"### Question2 : Why did changing the \"wheezy-copper-turtle-magic\" to categorical didn't improve the accuracy as we were expecting ?\n\n**Answer**: As mentioned in this discussion post (https:\/\/www.kaggle.com\/c\/instant-gratification\/discussion\/92930#latest-537424), it looks like the data consists of several subsets each of which is indicated by a value of the \"wheezy-copper-turtle-magic\" column. Sometimes changing such a column to category will be enough, but some times, we have to develop a separate model for each of the values in the category. \n\nFor example if you are developing a pricing model for a store that sells groceries and antique items, one can imagine that antiques will have a completely different price vs. age model, compared to perishable items. So, just having the item category as a categorical variable is not enough, we need to have a separate model for each category. \n\n","dee0d597":"## Model 1: Logistic regression with wheezy-copper-turtle-magic as a categorical variable\n\nAs mentioned earlier, it was identified by others that wheezy-copper-turtle-magic appears to be a categorical variable, with a distribution that is diferent from rest of the variables, so converting it to a categorical, column and using one-hot encoding might help improve accuracy. \n\nBefore we convert this column to a categorical column, let us first make sure all the values of this column in the test data are a subset of this column's values in the training data","757aeb1b":"## Let us submit the best model so far"}}