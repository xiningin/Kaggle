{"cell_type":{"5302ca97":"code","9635d139":"code","d046854a":"code","a0cfed8f":"code","3bb49d5b":"code","b2f9de39":"code","86ca6019":"code","dbe4d9d3":"code","008e2f6f":"code","12f70d76":"code","1d4c8df4":"code","79e56d9f":"code","f3d50bd0":"code","e37586db":"code","f566d51f":"code","a7c4b025":"code","769df7e2":"code","5baba44f":"code","a85e2c9c":"code","ae03e02e":"code","c864d547":"code","095b8960":"markdown","55241d99":"markdown","8baac98d":"markdown","fe6044d6":"markdown","5f0f932f":"markdown","7fe3e6e9":"markdown","af3fcd4a":"markdown","5eaeb597":"markdown","256ba4b0":"markdown","84fd5c6d":"markdown","437600ce":"markdown","08ab18c2":"markdown","5c9def28":"markdown","96ca9aa0":"markdown","8ebeae59":"markdown","a6f5acb3":"markdown","81f0a138":"markdown"},"source":{"5302ca97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9635d139":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d046854a":"train=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nid=test['Id']\ntrain.head()","a0cfed8f":"train.info()","3bb49d5b":"test.info()","b2f9de39":"train.isnull().sum()","86ca6019":"test.isnull().sum()","dbe4d9d3":"train.columns","008e2f6f":"test.columns","12f70d76":"train.drop(columns=['Id', 'PoolQC', 'Fence','MSZoning','MiscFeature','Alley'], axis=1)\ntest=test.drop(columns=['PoolQC', 'Fence','MSZoning','MiscFeature','Alley'], axis=1)\n# train.head()\ntest.head()","1d4c8df4":"cat_train=[cat for cat in train.columns if train[cat].dtype=='object']\nnum_train=[cat for cat in train.columns if train[cat].dtype=='int64' or train[cat].dtype=='float64']\n\ncat_train\n# num_train","79e56d9f":"cat_test=[cat for cat in test.columns if test[cat].dtype=='object']\nnum_test=[cat for cat in test.columns if test[cat].dtype=='int64' or test[cat].dtype=='float64']\ncat_test\n# num_test","f3d50bd0":"# from sklearn.preprocessing import StandardScaler\n# ss= StandardScaler()\n# train[num_train]= ss.fit_transform(train[num_train])\n# test[num_test]= ss.fit_transform(test[num_test])\n# train.head()\n# test.head()","e37586db":"train1= pd.get_dummies(train, columns=cat_train, drop_first= True)\ntest1= pd.get_dummies(test, columns=cat_test, drop_first= True)\n# train1\ntest1","f566d51f":"train2=pd.concat([train,train1],axis=1)\ntest2=pd.concat([test,test1],axis=1)\ntest2","a7c4b025":"train=train2.drop(cat_train,axis=1)\ntest=test2.drop(cat_test,axis=1)\n# test123=test.copy()\n# train\nid=test['Id'].iloc[:,1]\nid","769df7e2":"y=train['SalePrice'].iloc[:,1]\nX=train.drop(['Id','SalePrice'],axis=1)","5baba44f":"from sklearn.impute import SimpleImputer\nsi=SimpleImputer()\nX=si.fit_transform(X)\ntest=si.fit_transform(test)\n# X\ntest","a85e2c9c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\ny_train","ae03e02e":"from sklearn.ensemble import GradientBoostingRegressor\nreg=GradientBoostingRegressor()\nreg.fit(X_train,y_train)","c864d547":"predictions= reg.predict(X_test)\nreg.score(X_test, y_test)","095b8960":"### 3. Finding the number of null values in each columns","55241d99":"### 4. Finding the columns in each dataset","8baac98d":"### 9.Concatenating the Original Dataset & the One after creating Dummies(get_dummies() creates a new DF containing JUST the dummies, MOST People get wrong here)","fe6044d6":"### **If you like this notebook, give an upvote. Any suggestions or comments are appreciated**","5f0f932f":"### 1. Loading Data with Pandas","7fe3e6e9":"### 13. Doing the Train_Test_Split","af3fcd4a":"### 2. Finding the Data-type of each column","5eaeb597":"### 8. Handling Categorical Data using Get_Dummies()","256ba4b0":"# **HousePrice - Solution - Easiest Way**\n\n### **I used following techniques in this notebook**\n\n* 1. Loading Data using Pandas\n* 2. Finding the Data-type of each column\n* 3. Finding the number of null values in each columns\n* 4. Finding the columns in each dataset\n* 5. Dropping some useless column. Also dropping some columns with more NULL values\n* 6. Finding all columns with Categorical & Numerical values(to be treated seperately later)\n* 7. Using the StandardScaler library to Standardize the numeric values\n* 8. Handling Categorical Data using Get_Dummies()\n* 9.Concatenating the Original Dataset & the One after creating Dummies\n* 10. Dropping the columns already concatenated after Get_Dummies()\n* 11. Splitting X & y\n* 12. Imputing the missing values\n* 13. Doing the Train_Test_Split\n* 14. Using XGBoost to fit the Data\n* 15. Using the Trained Model to Predict\n","84fd5c6d":"### 14. Using XGBoost to fit the Data","437600ce":"### 7. Using the StandardScaler library to Standardize the numeric values\n","08ab18c2":"### 15. Using the Trained Model to Predict","5c9def28":"### 5. Dropping some useless column. Also dropping some columns with more NULL values","96ca9aa0":"### 6. Finding all columns with Categorical & Numerical values(to be treated seperately later)\n\nThis method is called List Comprehension-where a list is created satisfying some condition","8ebeae59":"### 12. Imputing the missing values","a6f5acb3":"### 10. Dropping the columns already concatenated after Get_Dummies()","81f0a138":"### 11. Splitting X & y"}}