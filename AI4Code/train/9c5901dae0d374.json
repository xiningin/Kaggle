{"cell_type":{"643b78fc":"code","35f07538":"code","6fd2a473":"code","50149091":"code","7b251891":"code","0a141c5e":"code","d95b2a64":"code","338475c0":"code","46fcde50":"code","42dd3ab2":"code","6231b749":"code","862fce50":"code","fdb0c34b":"code","40102f23":"code","dafdfe13":"code","c9c2a4c9":"code","42a808bb":"code","ed1890ed":"code","47ed041d":"code","5ede5019":"code","d07e936c":"code","4e39aed6":"code","0550e1e1":"code","0722f6c9":"code","9a688c68":"code","c9e6bc4a":"code","fd83a09d":"markdown","8373f82f":"markdown","d0a38066":"markdown","81b5c73d":"markdown","3eabdc7c":"markdown","1e12ec56":"markdown","289b516e":"markdown","1d3f3bc2":"markdown"},"source":{"643b78fc":"from sklearn.datasets import load_boston\nimport pandas as pd\n","35f07538":"\nboston = load_boston()\n\nX = boston.data\ny = boston.target","6fd2a473":"print(X.shape)\nprint(y.shape)","50149091":"print(boston.feature_names)","7b251891":"print(boston.DESCR)","0a141c5e":"import pandas as pd\ndf = pd.DataFrame(X)\ndf.columns = boston.feature_names\ndf.head()\n","d95b2a64":"df.describe()","338475c0":"\n# Normalise this dataset\n# Each feature must have 0 mean, unit variance\nimport numpy as np\nu = np.mean(X,axis=0)\nstd = np.std(X,axis=0)\n#print(u.shape,std.shape)","46fcde50":"# Normalise the Data\nX = (X-u)\/std","42dd3ab2":"# Normalised Data\npd.DataFrame(X[:5,:]).head()","6231b749":"\n# Plot Y vs any feature\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn')\nplt.scatter(X[:,5],y)\nplt.show()","862fce50":"X.shape, y.shape","fdb0c34b":"ones = np.ones((X.shape[0],1))\nX = np.hstack((ones,X))\nprint(X.shape)","40102f23":"print(X[3])","dafdfe13":"\n# X - Matrix ( m x n)\n# x - Vector (Single Example with n features)\n\ndef hypothesis(x,theta):\n    y_ = 0.0\n    n = x.shape[0]\n    for i in range(n):\n        y_  += (theta[i]*x[i])\n    return y_\n\ndef error(X,y,theta):\n    e = 0.0\n    m = X.shape[0]\n    \n    for i in range(m):\n        y_ = hypothesis(X[i],theta)\n        e += (y[i] - y_)**2\n        \n    return e\/m\n\ndef gradient(X,y,theta):\n    m,n = X.shape\n    \n    grad = np.zeros((n,))\n    \n    # for all values of j\n    for j in range(n):\n        #sum over all examples\n        for i in range(m):\n            y_ = hypothesis(X[i],theta)\n            grad[j] += (y_ - y[i])*X[i][j]\n    # Out of the loops\n    return grad\/m\n\ndef gradient_descent(X,y,learning_rate=0.1,max_epochs=300):\n    m,n = X.shape\n    theta = np.zeros((n,))\n    error_list = []\n    \n    for i in range(max_epochs):\n        e = error(X,y,theta)\n        error_list.append(e)\n        \n        # Gradient Descent\n        grad = gradient(X,y,theta)\n        for j in range(n):\n            theta[j] = theta[j] - learning_rate*grad[j]\n        \n    return theta,error_list","c9c2a4c9":"\nimport time\nstart = time.time()\ntheta,error_list = gradient_descent(X,y)\nend = time.time()\nprint(\"Time taken is \", end-start)","42a808bb":"\nprint(theta)","ed1890ed":"plt.plot(error_list)\nplt.show()","47ed041d":"y_ = []\nm = X.shape[0]\n\nfor i in range(m):\n    pred = hypothesis(X[i],theta)\n    y_.append(pred)\ny_ = np.array(y_)","5ede5019":"def r2_score(y,y_):\n    num = np.sum((y-y_)**2)\n    denom = np.sum((y- y.mean())**2)\n    score = (1- num\/denom)\n    return score*100","d07e936c":"# SCORE\nr2_score(y,y_)\n","4e39aed6":"def hypothesis(X,theta):\n    return np.dot(X,theta)\n\ndef error(X,y,theta):\n    e = 0.0\n    y_ = hypothesis(X,theta)\n    e = np.sum((y-y_)**2)\n    \n    return e\/m\n    \ndef gradient(X,y,theta):\n    \n    y_ = hypothesis(X,theta)\n    grad = np.dot(X.T,(y_ - y)) \n    m = X.shape[0]\n    return grad\/m\n\ndef gradient_descent(X,y,learning_rate = 0.1,max_iters=300):\n    \n    n = X.shape[1]\n    theta = np.zeros((n,))\n    error_list = []\n    \n    for i in range(max_iters):\n        e = error(X,y,theta)\n        error_list.append(e)\n        \n        #Gradient descent\n        grad = gradient(X,y,theta)\n        theta = theta - learning_rate*grad\n        \n    return theta,error_list","0550e1e1":"\nstart = time.time()\ntheta,error_list = gradient_descent(X,y)\nend = time.time()\nprint(\"Time taken by Vectorized Code\",end-start)\n","0722f6c9":"theta","9a688c68":"plt.plot(error_list)\nplt.show()","c9e6bc4a":"y_ = hypothesis(X,theta)\nr2_score(y,y_)","fd83a09d":"### prediction","8373f82f":"## Step 2 --> LINEAR REGRESSION ON MULTIPLE FEATURES \nBOSTON hOUSING DATASET","d0a38066":"### really very fast! :D","81b5c73d":"## Step -1 Load Dataset","3eabdc7c":"### QUITE SLOW 18 SECONDS JUST 506 EXAMPLES IN THE TRAINING DATA\n","1e12ec56":"## Step 3 --> optimsing code using vectorization\nAn Efficent Implemenation for Linear Regression using Vectorization\n\nAvoid loops in the implemenation, except gradient descent main loop\n\n\nUse numpy functions like np.sum(), np.dot() which are quite fast and already optimised","289b516e":"## Step 4-->Score ","1d3f3bc2":"# Multiple Linear regression \n1 - here we deal with boston dataset\n\nThese are the steps\n\nStep 1 -->Load boston dataset\n\nStep 2 -->Apply Multiple Linear Regression from scratch\n\nStep 3 -->Optimize code with vectorization\n\nStep 4 -->Score"}}