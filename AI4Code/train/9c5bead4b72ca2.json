{"cell_type":{"ad7e4b8f":"code","385554dc":"code","016bdc30":"code","1090e949":"code","caa0859e":"code","26995b7e":"code","5b13e414":"code","1011e174":"code","e3341434":"code","3ffda997":"code","74e224e1":"code","728f6a4e":"code","f945bdea":"code","7d09f8a4":"code","710fbd42":"code","378274f4":"code","9ea4c601":"code","7b3f0622":"code","8259baa1":"code","96152bf8":"code","c705796c":"code","2aec658b":"code","27319179":"code","532943bb":"code","a9b2eec7":"code","2e3f271a":"code","ccdda70e":"code","b0022135":"markdown","d6be53d9":"markdown","492aee3b":"markdown","2df95887":"markdown","dc49c249":"markdown","981a5abb":"markdown","45e43269":"markdown","d28f8419":"markdown"},"source":{"ad7e4b8f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import SelectFromModel\nimport time\n%matplotlib inline\n# Classification\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cat\nfrom catboost import CatBoostClassifier,Pool, cv\n\n# Preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom mlxtend.classifier import StackingClassifier\nfrom datetime import datetime, timedelta\nfrom sklearn.naive_bayes import MultinomialNB\n\nimport joblib\nfrom sklearn.preprocessing import LabelEncoder","385554dc":"test = pd.read_csv('..\/input\/janatahack-hr-analytics\/test.csv')\ntrain = pd.read_csv('..\/input\/janatahack-hr-analytics\/train.csv')\nprint(test.shape,train.shape)","016bdc30":"target = 'target'\ntest_ids = test['enrollee_id'] \norg_feat = train.columns\ndf=train.append(test,ignore_index=True)\nsns.heatmap(df.isnull())","1090e949":"df.info()","caa0859e":"def logic_missing(df):\n    df.loc[df['education_level'] == 'High School', 'major_discipline'] = 'not applicable'\n    df.loc[df['education_level'] == 'Primary School', 'major_discipline'] = 'not applicable'\n    df.loc[(df['experience']=='<1') & (df['company_type'].isna()) & (df['company_size'].isna()),'experience']=0\n    df.loc[(df['experience']==0) & (df['company_type'].isna()) & (df['company_size'].isna()),'company_type']='Not Applicable'\n    df.loc[(df['experience']==0) & (df['company_type']=='Not Applicable') & (df['company_size'].isna()),'company_size']='Not Applicable'\n    return df","26995b7e":"def impute_missing(df) : \n    missing_cols = list(df.columns[df.isnull().any()])\n    print(f\"Columns with missing values : {missing_cols}\")\n    df_org = df\n    \n    #### Impute Gender with unkniwn and add a _ismissing feature\n    for col in missing_cols :\n        if col!='target' :\n            df[col+'_ismissing'] = 0\n            df[col+'_ismissing'] = df[col].apply(lambda x : 1 if x!=x else 0) \n            df[col] = df[col].fillna('unknown')\n        \n    #df['gender'] = df['gender'].fillna('Unknown')\n    return df","5b13e414":"def cat_to_num(df) :\n    df['company_size']=df['company_size'].apply(lambda x : 75 if x=='50-99' else x)\n    df['company_size']=df['company_size'].apply(lambda x : 300 if x=='100-500' else x)\n    df['company_size']=df['company_size'].apply(lambda x : 20000 if x=='10000+' else x)\n    df['company_size']=df['company_size'].apply(lambda x : 30 if x=='10\/49' else x)\n    df['company_size']=df['company_size'].apply(lambda x : 3000 if x=='1000-4999' else x)\n    df['company_size']=df['company_size'].apply(lambda x : 5 if x=='<10' else x)\n    df['company_size']=df['company_size'].apply(lambda x : 750 if x=='500-999' else x)\n    df['company_size']=df['company_size'].apply(lambda x : 7500 if x=='5000-9999' else x)\n    df['company_size']=df['company_size'].apply(lambda x : -999 if x=='unknown' else x)\n    df['company_size']=df['company_size'].apply(lambda x : -999 if x=='Not Applicable' else x)\n    \n    df['last_new_job'] = df['last_new_job'].apply(lambda x : 10 if x=='>4' else x)\n    df['last_new_job'] = df['last_new_job'].apply(lambda x : 0 if x=='never' else x)\n    df['last_new_job'] = df['last_new_job'].apply(lambda x : -999 if x=='unknown' else x)\n    \n    df['experience']=df['experience'].apply(lambda x : 0 if x=='<1' else x)\n    df['experience']=df['experience'].apply(lambda x : 25 if x=='>20' else x)\n    df['experience']=df['experience'].apply(lambda x : -999 if x=='unknown' else x)\n    \n    def ed_to_numeric(x):\n        if x=='unknown' or x=='Primary School':\n            return 0\n        if x=='High School':\n            return 1\n        if x=='Graduate':\n            return 2\n        if x=='Masters':\n            return 3\n        if x=='Phd':\n            return 4\n    \n    df['education_level'] = df['education_level'].apply(ed_to_numeric)\n    \n    df['last_new_job']=df['last_new_job'].astype(int)\n    df['experience']=df['experience'].astype(int)\n    #print(df['company_size'].value_counts())\n    df['company_size']=df['company_size'].astype(int)\n\n    \n    return df","1011e174":"def feat_eng(df,cat) :\n    df['experience_more_than20']=df['experience'].apply(lambda x : 1 if x>20 else 0)\n    \n    ##aggregate features\n#     #cat_agg=['count','nunique']\n#     num_agg=['min','mean','max','sum']\n#     agg_col={\n#         'experience':num_agg, 'company_size':num_agg, 'training_hours':num_agg,'last_new_job':num_agg}\n\n#     agg_df=df.groupby('city').agg(agg_col)\n#     agg_df.columns=['agg_' + '_'.join(col).strip() for col in agg_df.columns.values]\n#     agg_df.reset_index(inplace=True)\n    \n#     df=df.merge(agg_df,on='city',how='left')\n    \n    ##ONE HOT ENCODING \n    if cat == 0:\n        df=pd.get_dummies(df,columns=list(df.select_dtypes(include=['object']).columns),drop_first=True)\n        cat_feat = 0\n    else :\n        cat_col = list(df.select_dtypes(include=['object']).columns)\n        for c in cat_col :\n            df[c] = df[c].astype('category')\n        cat_feat = np.where(df.dtypes =='category')[0]\n\n    return df,cat_feat","e3341434":"df_log = logic_missing(df)","3ffda997":"df_imp = impute_missing(df_log)","74e224e1":"df_num= cat_to_num(df_imp)","728f6a4e":"df_feat,cat_feat = feat_eng(df_num,0)","f945bdea":"df_feat.info()","7d09f8a4":"df = df_feat.drop(columns = ['enrollee_id'])\ncat_feat = cat_feat-1","710fbd42":"from imblearn.over_sampling import RandomOverSampler\nos =RandomOverSampler(1)\n\n\ndf_train=df[df[target].isnull()==False].copy()\ndf_test=df[df[target].isnull()==True].copy()\n\ndf_test.drop(columns=[target],axis=1, inplace=True)\n\nx = df_train.drop(target,axis=1)\ny = df_train[target]\nfeat = df_test.columns\n\nprint(df_train.shape,df_test.shape)\n\nx, y= os.fit_sample(x, y)","378274f4":"def make_sub(y_pred,name):\n    df_sub = pd.DataFrame({'enrollee_id':test_ids,target:y_pred})\n    import time\n    times = time.strftime(\"%Y%m%d-%H%M%S\")\n    filename = 'submission-'+name+'_'+times+'.csv'\n    df_sub.to_csv(filename,index=False)\n    print(f\"{filename} generated!\")","9ea4c601":"def lgb_tune(x, y, target, plot=True):\n    \n    \n    print(\"Parameter Tuning :\")\n\n    param_grid = {\n    'num_leaves':[50],\n    'max_depth':[-1],\n    'colsample_bytree': [0.8],#,0.6,0.8],\n    'min_child_samples': [10],#,10],\n    'min_split_gain':[1],\n    'subsample' : [0.7],\n    'reg_alpha' : [0.7],#,0.6],\n    'reg_lambda' : [0.6],#,0.7,0.8],\n    'device': ['gpu']\n    } \n\n    model = lgb.LGBMClassifier(\n        objective='binary',\n        boosting_type='gbdt', \n        learning_rate=0.003, \n        n_estimators=4000, \n        silent=False,\n        #categorical_feature = cat_feat\n    )\n    \n    skf = StratifiedKFold(n_splits=4, shuffle = True, random_state = 1001)\n    \n    lgb_grid = GridSearchCV(model, param_grid, cv=skf.split(x,y), scoring='roc_auc', verbose=1, n_jobs=4)\n    lgb_grid.fit(x, y)\n\n    print(lgb_grid.best_params_)\n    print(lgb_grid.best_score_)\n\n    #predictions = lgb_grid.predict(test[features]) \n    \n    return lgb_grid.best_estimator_, #predictions, lgb_grid.best_params","7b3f0622":"def lgb_run (lgbM, cv) :\n    err=[]\n    y_pred_tot=[]\n    from sklearn.model_selection import KFold,StratifiedKFold\n\n    fold=StratifiedKFold(n_splits=cv,shuffle=True,random_state=1996)\n\n    i=1\n\n    for train_index, test_index in fold.split(x,y):\n        print(f\"\\n\\n-----------------FOLD {i}------------------------\")\n        x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        m=lgbM\n        m.fit(x_train,y_train,eval_set=[(x_test, y_test)],eval_metric='auc', early_stopping_rounds=200,verbose=100)#,categorical_feature = cat_feat)\n        preds=m.predict_proba(x_test)[:,-1]\n        print(\"err: \",roc_auc_score(y_test,preds.round()))\n        err.append(roc_auc_score(y_test,preds.round()))\n        p = m.predict_proba(df_test[feat])[:,-1]\n        i=i+1\n        y_pred_tot.append(p)\n    print (f\"Mean score : {np.mean(err,0)}\")\n    y_pred_lgb = np.mean(y_pred_tot, 0)\n    return y_pred_lgb","8259baa1":"lgbM = lgb_tune(x, y, target, True)","96152bf8":"y_pred_lgb= lgb_run (lgbM[0], 10)","c705796c":"make_sub(y_pred_lgb,'lgb')","2aec658b":"def xgb_tune(x, y,  target):\n    \n    print(\"Parameter Tuning :\")\n\n    param_grid = {\n    'max_depth':[3,6], ##\n    'subsample':[0.8],\n    'colsample_bytree': [1],\n    'min_child_weight': [0.4],\n    'gamma': [0.5],\n    'reg_lambda': [1],\n    } \n\n    model = xgb.XGBClassifier(\n        objective='binary:logistic',\n        learning_rate=0.003, \n        n_estimators=4000, \n        tree_method = \"gpu_hist\",\n        silent=False\n    )\n\n    skf = StratifiedKFold(n_splits=4, shuffle = True, random_state = 1001)\n\n    xgb_grid = GridSearchCV(model, param_grid, cv=skf.split(x,y), scoring='roc_auc', verbose=1, n_jobs=4)\n   \n    xgb_grid.fit(x, y)\n\n    print(xgb_grid.best_params_)\n    print(xgb_grid.best_score_)\n\n\n    return  xgb_grid.best_estimator_","27319179":"def xgb_run (xgbM, cv) :\n    err=[]\n    y_pred_tot=[]\n    from sklearn.model_selection import KFold,StratifiedKFold\n\n    fold=StratifiedKFold(n_splits=cv,shuffle=True,random_state=1996)\n\n    i=1\n\n    for train_index, test_index in fold.split(x,y):\n\n        x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        m=xgbM\n        m.fit(x_train,y_train,eval_set=[(x_test, y_test)],eval_metric='auc', early_stopping_rounds=200,verbose=100)\n        preds=m.predict_proba(x_test)[:,-1]\n        print(\"err: \",roc_auc_score(y_test,preds.round()))\n        err.append(roc_auc_score(y_test,preds.round()))\n        p = m.predict_proba(df_test[feat])[:,-1]\n        i=i+1\n        y_pred_tot.append(p)\n    print (f\"Mean score : {np.mean(err,0)}\")\n    y_pred_lgb = np.mean(y_pred_tot, 0)\n    return y_pred_lgb","532943bb":"xgbM = xgb_tune(x, y, target)","a9b2eec7":"y_pred_xg= xgb_run (xgbM, 10)","2e3f271a":"make_sub(y_pred_xg,'xgb')","ccdda70e":"make_sub(y_pred_xg*0.5+y_pred_lgb*0.5,'xgblgb')","b0022135":"## Modelling","d6be53d9":"## Data Prep\n","492aee3b":"## Read data","2df95887":"![image.png](attachment:image.png)\n","dc49c249":"## Clean Data and Feature Engineering","981a5abb":"### XGB","45e43269":"## Import Libraries","d28f8419":"### LGB"}}