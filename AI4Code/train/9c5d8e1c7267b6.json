{"cell_type":{"2b55da22":"code","52337790":"code","68673b71":"code","a81271a2":"code","38738d4a":"code","bbd8f030":"code","0ae2d883":"code","aac72ddc":"code","be5fe42a":"code","2ef109e0":"code","0318bfd5":"code","67587f66":"code","4ff8c57c":"code","b3ae3b39":"code","4e59c469":"markdown","6a37c503":"markdown","a85131f5":"markdown","d435d4f2":"markdown","5c73b985":"markdown","1a655b00":"markdown","055e7981":"markdown"},"source":{"2b55da22":"import os\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nfrom glob import glob\nimport seaborn as sns\nfrom PIL import Image\nnp.random.seed(11) # It's my lucky number\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nimport itertools\n\nimport keras\nfrom keras.utils.np_utils import to_categorical # used for converting labels to one-hot-encoding\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras import backend as K\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.applications.resnet50 import ResNet50\nfrom keras import backend as K \n","52337790":"folder_benign_train = '..\/input\/data\/train\/benign'\nfolder_malignant_train = '..\/input\/data\/train\/malignant'\n\nfolder_benign_test = '..\/input\/data\/test\/benign'\nfolder_malignant_test = '..\/input\/data\/test\/malignant'\n\nread = lambda imname: np.asarray(Image.open(imname).convert(\"RGB\"))\n\n# Load in training pictures \nims_benign = [read(os.path.join(folder_benign_train, filename)) for filename in os.listdir(folder_benign_train)]\nX_benign = np.array(ims_benign, dtype='uint8')\nims_malignant = [read(os.path.join(folder_malignant_train, filename)) for filename in os.listdir(folder_malignant_train)]\nX_malignant = np.array(ims_malignant, dtype='uint8')\n\n# Load in testing pictures\nims_benign = [read(os.path.join(folder_benign_test, filename)) for filename in os.listdir(folder_benign_test)]\nX_benign_test = np.array(ims_benign, dtype='uint8')\nims_malignant = [read(os.path.join(folder_malignant_test, filename)) for filename in os.listdir(folder_malignant_test)]\nX_malignant_test = np.array(ims_malignant, dtype='uint8')\n\n# Create labels\ny_benign = np.zeros(X_benign.shape[0])\ny_malignant = np.ones(X_malignant.shape[0])\n\ny_benign_test = np.zeros(X_benign_test.shape[0])\ny_malignant_test = np.ones(X_malignant_test.shape[0])\n\n\n# Merge data \nX_train = np.concatenate((X_benign, X_malignant), axis = 0)\ny_train = np.concatenate((y_benign, y_malignant), axis = 0)\n\nX_test = np.concatenate((X_benign_test, X_malignant_test), axis = 0)\ny_test = np.concatenate((y_benign_test, y_malignant_test), axis = 0)\n\n# Shuffle data\ns = np.arange(X_train.shape[0])\nnp.random.shuffle(s)\nX_train = X_train[s]\ny_train = y_train[s]\n\ns = np.arange(X_test.shape[0])\nnp.random.shuffle(s)\nX_test = X_test[s]\ny_test = y_test[s]","68673b71":"# Display first 15 images of moles, and how they are classified\nw=40\nh=30\nfig=plt.figure(figsize=(12, 8))\ncolumns = 5\nrows = 3\n\nfor i in range(1, columns*rows +1):\n    ax = fig.add_subplot(rows, columns, i)\n    if y_train[i] == 0:\n        ax.title.set_text('Benign')\n    else:\n        ax.title.set_text('Malignant')\n    plt.imshow(X_train[i], interpolation='nearest')\nplt.show()","a81271a2":"y_train = to_categorical(y_train, num_classes= 2)\ny_test = to_categorical(y_test, num_classes= 2)","38738d4a":"# With data augmentation to prevent overfitting \nX_train = X_train\/255.\nX_test = X_test\/255.","bbd8f030":"# See learning curve and validation curve\n\ndef build(input_shape= (224,224,3), lr = 1e-3, num_classes= 2,\n          init= 'normal', activ= 'relu', optim= 'adam'):\n    model = Sequential()\n    model.add(Conv2D(64, kernel_size=(3, 3),padding = 'Same',input_shape=input_shape,\n                     activation= activ, kernel_initializer='glorot_uniform'))\n    model.add(MaxPool2D(pool_size = (2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(64, kernel_size=(3, 3),padding = 'Same', \n                     activation =activ, kernel_initializer = 'glorot_uniform'))\n    model.add(MaxPool2D(pool_size = (2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer=init))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.summary()\n\n    if optim == 'rmsprop':\n        optimizer = RMSprop(lr=lr)\n\n    else:\n        optimizer = Adam(lr=lr)\n\n    model.compile(optimizer = optimizer ,loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\n# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=5, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=1e-7)\n\n","0ae2d883":"input_shape = (224,224,3)\nlr = 1e-5\ninit = 'normal'\nactiv = 'relu'\noptim = 'adam'\nepochs = 50\nbatch_size = 64\n\nmodel = build(lr=lr, init= init, activ= activ, optim=optim, input_shape= input_shape)\n\nhistory = model.fit(X_train, y_train, validation_split=0.2,\n                    epochs= epochs, batch_size= batch_size, verbose=0, \n                    callbacks=[learning_rate_reduction]\n                   )\n                   \n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","aac72ddc":"K.clear_session()\ndel model\ndel history","be5fe42a":"# define 3-fold cross validation test harness\nkfold = KFold(n_splits=3, shuffle=True, random_state=11)\n\ncvscores = []\nfor train, test in kfold.split(X_train, y_train):\n  # create model\n    model = build(lr=lr, \n                  init= init, \n                  activ= activ, \n                  optim=optim, \n                  input_shape= input_shape)\n    \n    # Fit the model\n    model.fit(X_train[train], y_train[train], epochs=epochs, batch_size=batch_size, verbose=0)\n    # evaluate the model\n    scores = model.evaluate(X_train[test], y_train[test], verbose=0)\n    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    cvscores.append(scores[1] * 100)\n    K.clear_session()\n    del model\n    \nprint(\"%.2f%% (+\/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))","2ef109e0":"# Fitting model to all data\nmodel = build(lr=lr, \n              init= init, \n              activ= activ, \n              optim=optim, \n              input_shape= input_shape)\n\nmodel.fit(X_train, y_train,\n          epochs=epochs, batch_size= batch_size, verbose=0,\n          callbacks=[learning_rate_reduction]\n         )\n\n# Testing model on test data to evaluate\ny_pred = model.predict_classes(X_test)\n\nprint(accuracy_score(np.argmax(y_test, axis=1),y_pred))","0318bfd5":"# save model\n# serialize model to JSON\nmodel_json = model.to_json()\n\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n    \n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")\n\n# Clear memory, because of memory overload\ndel model\nK.clear_session()","67587f66":"input_shape = (224,224,3)\nlr = 1e-5\nepochs = 50\nbatch_size = 64\n\nmodel = ResNet50(include_top=True,\n                 weights= None,\n                 input_tensor=None,\n                 input_shape=input_shape,\n                 pooling='avg',\n                 classes=2)\n\nmodel.compile(optimizer = Adam(lr) ,\n              loss = \"binary_crossentropy\", \n              metrics=[\"accuracy\"])\n\nhistory = model.fit(X_train, y_train, validation_split=0.2,\n                    epochs= epochs, batch_size= batch_size, verbose=2, \n                    callbacks=[learning_rate_reduction]\n                   )\n\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n","4ff8c57c":"# Train ResNet50 on all the data\nmodel.fit(X_train, y_train,\n          epochs=epochs, batch_size= epochs, verbose=0,\n          callbacks=[learning_rate_reduction]\n         )\n","b3ae3b39":"# Testing model on test data to evaluate\ny_pred = model.predict(X_test)\nprint(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n\n# save model\n# serialize model to JSON\nresnet50_json = model.to_json()\n\nwith open(\"resnet50.json\", \"w\") as json_file:\n    json_file.write(resnet50_json)\n    \n# serialize weights to HDF5\nmodel.save_weights(\"resnet50.h5\")\nprint(\"Saved model to disk\")","4e59c469":"# Step 7: Testing the model\n\nFirst the model has to be fitted with all the data, such that no data is left out.","6a37c503":"# Step 6: Cross-Validating Model\n","a85131f5":"# Step 1 : importing Essential Libraries","d435d4f2":"# Step 8: ResNet50\nThe CNN above is not a very sophisticated model, thus the resnet50, is also tried","5c73b985":"# Step 3: Categorical Labels\nTurn labels into one hot encoding","1a655b00":"# Step 4 : Normalization\nNormalize all Values of the pictures by dividing all the RGB values by 255","055e7981":"# Step 2 : Loading pictures and making Dictionary of images and labels\nIn this step I load in the pictures and turn them into numpy arrays using their RGB values. As the pictures have already been resized to 224x224, there's no need to resize them. As the pictures do not have any labels, these need to be created. Finally, the pictures are added together to a big training set and shuffeled."}}