{"cell_type":{"5a173eb2":"code","df117b5e":"code","5ae276b9":"code","3a4f93bd":"code","6631dbb8":"code","c16bf674":"code","6116e42a":"code","7921f6de":"code","771da56d":"code","d455b89e":"code","90bc8f81":"code","739e278a":"code","e0b41caf":"code","e9490c8c":"code","7d280800":"code","da526ab0":"code","7113f09c":"code","6a947b5b":"code","3d54a287":"code","78d6749c":"code","ba80c2c5":"code","3b82d48a":"code","55d525be":"code","0dc36829":"code","7735c1ba":"code","a7bda57e":"code","2c87947c":"code","a2f63f86":"code","00aa64d6":"code","835e72ef":"code","916cbd11":"code","80a62795":"code","22026e17":"code","4e3ab816":"code","68e12460":"code","f53beb6c":"code","52c8b6b5":"code","5ba87d0c":"code","88445e5b":"code","588ae232":"code","90b243c3":"code","58e6ceee":"code","81c5e696":"code","67cd33ee":"markdown","14852870":"markdown","7188d69c":"markdown","ce8ea3ae":"markdown","a268c739":"markdown","c65322c3":"markdown","4997df82":"markdown","248bd237":"markdown","6dd912a5":"markdown"},"source":{"5a173eb2":"#!pip install catalyst --upgrade","df117b5e":"# Install required libs\n!pip install catalyst\n\n### please update Albumentations to latest version for `Lambda` transform support\n!pip install -U git+https:\/\/github.com\/albu\/albumentations@bdd6a4e\n!pip install -U git+https:\/\/github.com\/qubvel\/segmentation_models.pytorch","5ae276b9":"import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom PIL import Image as plim\nimport matplotlib.pyplot as plt","3a4f93bd":"import fastai\nfrom fastai.vision import *\nfrom fastai.callbacks import MixedPrecision","6631dbb8":"tr = pd.read_csv(\"..\/input\/train.csv\")","c16bf674":"def open_mask_rle(mask_rle:str, shape:Tuple[int, int])->ImageSegment:\n    \"Return `ImageSegment` object create from run-length encoded string in `mask_lre` with size in `shape`.\"\n    x = FloatTensor(rle_decode(str(mask_rle), shape).astype(np.uint8))\n    x = x.view(shape[1], shape[0], -1)\n    return ImageSegment(x.permute(2,1,0))","6116e42a":"tr['ClassId'] = tr['ClassId'].apply(lambda x: x.split('_')[0])\ntr['code'] = tr['ClassId']#.apply(lambda x: x.split('_')[0])\ncode = list(set(tr['code'].values))","7921f6de":"trs = tr.sample(frac=0.1).reset_index(drop=True)\ntrs.shape","771da56d":"# helper function for data visualization\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('.')[0]).title())\n        plt.imshow(image)\n    plt.show()","d455b89e":"from torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset as BaseDataset","90bc8f81":"class Dataset(BaseDataset):\n    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        images_dir (str): path to images folder\n        masks_dir (str): path to segmentation masks folder\n        class_values (list): values of classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n    \n    '''CLASSES = ['sky', 'building', 'pole', 'road', 'pavement', \n               'tree', 'signsymbol', 'fence', 'car', \n               'pedestrian', 'bicyclist', 'unlabelled']'''\n    \n    CLASSES = code#tr['ClassId'].values\n    \n    \n    def __init__(\n            self, \n            images_dir, \n            df,\n            masks_dir=None, \n            classes=None, \n            augmentation=None, \n            preprocessing=None,\n    ):\n        self.df = df\n        self.ids = [f for f in self.df.ImageId] #os.listdir(images_dir)\n        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n        self.rle = [i for i in self.df.EncodedPixels]\n        self.h = [i for i in self.df.Height]\n        self.w = [i for i in self.df.Width]\n        # convert str names to class values on masks\n        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n        \n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read data\n        image = open_image(self.images_fps[i]).resize(224).data\n        image = image.permute(1,2,0).numpy().astype(np.float32)\n        mask = open_mask_rle(self.rle[i], (self.h[i], self.w[i])).resize(224).data.numpy().astype(np.uint8)\n        \n        # extract certain classes from mask (e.g. cars)\n        #masks = [(mask == v) for v in self.class_values]\n        #mask = np.stack(masks, axis=-1).astype('float')\n        \n        # apply augmentations\n        if self.augmentation:\n            mask = mask.squeeze()\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            #mask = mask.squeeze()\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n        \n    def __len__(self):\n        return len(self.ids)","739e278a":"# Lets look at data we have\n\ndataset = Dataset('..\/input\/train', tr, classes=['0'])\n\nimage, mask = dataset[4] # get some sample\nvisualize(\n    image=image,#.permute(1,2,0).numpy(), \n    this_fn_guy=mask.squeeze(),\n)","e0b41caf":"imag, mak = [],[]\nfor i in range(6):\n    im, mk = dataset[i+299]\n    imag.append(im)#.permute(1,2,0).numpy())\n    mak.append(mk.squeeze())","e9490c8c":"fig, ax = plt.subplots(3, 2, figsize=(16, 18))\nfor i, (img, seg) in enumerate(zip(imag, mak)):\n    ax[i\/\/2, i%2].imshow(img)#cv2plt(img))\n    #seg = seg.astype(np.uint8)\n    seg[seg == 45] = 255\n    ax[i\/\/2, i%2].imshow(seg, cmap='tab20_r', alpha=0.6)\n    ax[i\/\/2, i%2].set_title(\"Sample {}\".format(i))","7d280800":"import albumentations as albu","da526ab0":"def get_training_augmentation():\n    train_transform = [\n\n        albu.HorizontalFlip(p=0.5),\n        \n#        albu.Resize(224, 224, cv2.INTER_CUBIC, always_apply=True),\n\n#        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n\n#        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n#        albu.RandomCrop(height=320, width=320, always_apply=True),\n\n#        albu.IAAAdditiveGaussianNoise(p=0.2),\n        albu.IAAPerspective(p=0.1),\n\n        albu.OneOf(\n            [\n#                albu.CLAHE(p=1),\n                albu.RandomBrightness(limit=0.1, p=1),\n#                albu.RandomGamma(p=1),\n            ],\n            p=0.9,\n        ),\n\n        albu.OneOf(\n            [\n                albu.IAASharpen(p=1),\n#                albu.Blur(blur_limit=3, p=1),\n #               albu.MotionBlur(blur_limit=3, p=1),\n            ],\n            p=0.9,\n        ),\n\n        albu.OneOf(\n            [\n                albu.RandomContrast(limit=0.1, p=1),\n#                albu.HueSaturationValue(hue_shift_limit=10, p=1),\n            ],\n            p=0.9,\n        ),\n    ]\n    return albu.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n    test_transform = [\n#        albu.Resize(224, 224, always_apply=True),\n        albu.PadIfNeeded(384, 384)\n    ]\n    return albu.Compose(test_transform)\n\n\ndef img_to_tensor(x, **kwargs):\n        return torch.tensor(x, dtype=torch.float32).permute(2, 0, 1)\n\n    \ndef mask_to_tensor(x, **kwargs):\n        return torch.tensor(x).unsqueeze(dim=0).float()\n\n\ndef get_preprocessing(preprocessing_fn):\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=img_to_tensor, mask=mask_to_tensor),\n    ]\n    return albu.Compose(_transform)\n","7113f09c":"#### Visualize resulted augmented images and masks\n\naugmented_dataset = Dataset(\n    '..\/input\/train', \n    tr, \n    augmentation=get_training_augmentation(), \n    classes=['0'],\n)\n\n# same image with different random transforms\nfor i in range(6):\n    image, mask = augmented_dataset[1]\n    visualize(image=image, mask=mask.squeeze())","6a947b5b":"def json2df(data):\n    df = pd.DataFrame()\n    for index, el in enumerate(data):\n        for key, val in el.items():\n            df.loc[index, key] = val\n    return df","3d54a287":"label_description = open('..\/input\/label_descriptions.json').read()\nlabel_description = json.loads(label_description)\nlabel_description_info = label_description['info']\nlabel_description_categories = pd.DataFrame(label_description['categories'])\nlabel_description_attributes = pd.DataFrame(label_description['attributes'])\nlabel_description_attributes['id'] = label_description_attributes['id'].replace(0, 999)\nlbd = dict(zip(label_description_attributes['id'], label_description_attributes['name']))","78d6749c":"category_df = json2df(label_description[\"categories\"])\ncategory_df[\"id\"] = category_df[\"id\"].astype(int)\ncategory_df[\"level\"] = category_df[\"level\"].astype(int)\nattribute_df = json2df(label_description[\"attributes\"])\nattribute_df[\"id\"] = attribute_df[\"id\"].astype(int)\nattribute_df[\"level\"] = attribute_df[\"level\"].astype(int)","ba80c2c5":"lbd = dict(zip(category_df['id'], category_df['name']))","3b82d48a":"x_train_dir = '..\/input\/train'","55d525be":"vx = trs.sample(frac=0.2).index\nval = trs.loc[trs.index.isin(vx)].reset_index(drop=True)\ntrn = trs.loc[~trs.index.isin(vx)].reset_index(drop=True)","0dc36829":"trn.shape, val.shape","7735c1ba":"import torch\nimport numpy as np\nimport segmentation_models_pytorch as smp","a7bda57e":"ENCODER = 'resnet18'\nENCODER_WEIGHTS = 'imagenet'\nDEVICE = 'cuda'\n\nCLASSES = ['1']\nACTIVATION = 'sigmoid'","2c87947c":"# create segmentation model with pretrained encoder\nmodel = smp.Unet(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES), \n    activation=ACTIVATION,\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","a2f63f86":"train_dataset = Dataset(\n    x_train_dir, \n    trn, \n    augmentation=get_training_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES,\n)\n\nvalid_dataset = Dataset(\n    x_train_dir, \n    val, \n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES,\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=12)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=12)","00aa64d6":"x,y = next(iter(train_loader))","835e72ef":"x.shape","916cbd11":"y.shape","80a62795":"from catalyst.dl.callbacks import CheckpointCallback, MetricCallback, OneCycleLR","22026e17":"scheduler = OneCycleLR(1, 25, 0.5, (0.95, 0.85))","4e3ab816":"import torch\nimport torch.nn as nn\nfrom catalyst.dl.experiments import SupervisedRunner\nfrom catalyst.dl.callbacks import CheckpointCallback, MetricCallback, OneCycleLR\n\n# experiment setup\nnum_epochs = 4  # change me\nlogdir = \".\/logs\/segmentation_notebook\"\nloaders = {\n    \"train\": train_loader,\n    \"valid\": valid_loader\n}\n\n# model, criterion, optimizer\n# model = # already defined\ncriterion = smp.utils.losses.BCEDiceLoss(eps=1.)\noptimizer = torch.optim.Adam([\n    {'params': model.decoder.parameters(), 'lr': 1e-4}, \n    \n    # decrease lr for encoder in order not to permute \n    # pre-trained weights with large gradients on training start\n    {'params': model.encoder.parameters(), 'lr': 1e-6},  \n])\nscheduler = None\n\n# @TODO: add metrics support \n# (catalyst expects logits, rather than sigmoid outputs)\n# metrics = [\n#     smp.utils.metrics.IoUMetric(eps=1.),\n#     smp.utils.metrics.FscoreMetric(eps=1.),\n# ]\n\n\n# model runner\nrunner = SupervisedRunner()\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=logdir,\n    #num_epochs=num_epochs,\n    scheduler=scheduler,\n    verbose=True\n)","68e12460":"from catalyst.dl.utils import UtilsFactory\n# you can use plotly and tensorboard to plot metrics inside jupyter\n# by default it only plots loss\n# not sure if it correctly works in Colab\nUtilsFactory.plot_metrics(logdir=logdir)","f53beb6c":"test = pd.read_csv('..\/input\/sample_submission.csv')","52c8b6b5":"#test","5ba87d0c":"class Dataset_test(BaseDataset):\n    \n    CLASSES = code#tr['ClassId'].values\n    \n    \n    def __init__(\n            self, \n            images_dir, \n            df,\n            masks_dir=None, \n            classes=None, \n            augmentation=None, \n            preprocessing=None,\n    ):\n        self.df = df\n        self.ids = [f for f in self.df.ImageId] \n        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n        # convert str names to class values on masks\n        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n        \n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read data\n        image = open_image(self.images_fps[i]).resize(224).data\n        image = image.permute(1,2,0).numpy().astype(np.float32)\n        \n        # extract certain classes from mask (e.g. cars)\n        #masks = [(mask == v) for v in self.class_values]\n        #mask = np.stack(masks, axis=-1).astype('float')\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image)\n            image = sample['image']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image)\n            image = sample['image']\n            \n        return image\n        \n    def __len__(self):\n        return len(self.ids)","88445e5b":"x_test_dir = '..\/input\/test'","588ae232":"len(os.listdir(x_test_dir))","90b243c3":"# create test dataset\ntest_dataset = Dataset_test(\n    x_test_dir, \n    test, \n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES,\n)\n\ntest_dataloader = DataLoader(test_dataset)\nloaders[\"infer\"] = test_dataloader","58e6ceee":"import collections\nfrom catalyst.dl.callbacks import InferCallback, CheckpointCallback\nloaderz = collections.OrderedDict([(\"infer\", valid_loader)])\nrunner.infer(\n    model=model,\n    loaders=loaderz,\n    callbacks=[\n        CheckpointCallback(\n            resume=f\"{logdir}\/checkpoints\/best.pth\"),\n        InferCallback()\n    ],\n)","81c5e696":"threshold = 0.5\nbreak_at = 8\n\nfor i, (input, output) in enumerate(zip(\n        valid_dataset, runner.callbacks[1].predictions[\"logits\"])):\n    image,mask = input\n    #mask = output\n    \n    image_vis = image.permute(1, 2, 0)\n    gt_mask = mask[0].astype('uint8')\n    pr_mask = (output[0] > threshold).astype('uint8')\n        \n    visualize(\n        image=image_vis, \n        ground_truth_mask=gt_mask, \n        predicted_mask=pr_mask\n    )\n    \n    if i >= break_at:\n      break","67cd33ee":"## Create model and train","14852870":"For this example we will use **CamVid** dataset. It is a set of:\n - **train** images + segmentation masks\n - **validation** images + segmentation masks\n - **test** images + segmentation masks\n \nAll images have 320 pixels height and 480 pixels width.\nFor more inforamtion about dataset visit http:\/\/mi.eng.cam.ac.uk\/research\/projects\/VideoRec\/CamVid\/.","7188d69c":"## Inference","ce8ea3ae":"## Loading data","a268c739":"**In this kernel I use the incredible repo https:\/\/github.com\/qubvel\/segmentation_models.pytorch along with [catalyst](https:\/\/github.com\/catalyst-team\/catalyst).\nMost of the code here is adapted from qubvel's camvid example notebook in the segmentation models repo, I adapted it to work with these data.**","c65322c3":"### Augmentations","4997df82":"### Dataloader\n\nWriting helper class for data extraction, tranformation and preprocessing  \nhttps:\/\/pytorch.org\/docs\/stable\/data","248bd237":"### Visualization","6dd912a5":"### Visualization"}}