{"cell_type":{"daf22fdb":"code","ea3200ac":"code","f0f860ad":"code","d600587a":"code","55999e8d":"code","75666fe7":"code","12508327":"code","8c4bbea2":"code","17eead3c":"code","02032724":"code","6c7dfc15":"code","710a6181":"code","f6ee4e32":"code","26f2b495":"code","fcade978":"code","42600baf":"code","d4b86cd1":"code","f6a2b3f7":"code","63629b77":"code","45776815":"code","f745d9e4":"code","116761d8":"code","6228f134":"code","169a00f2":"code","100b6d8a":"code","2b2ecf1a":"code","2aa6d4c1":"code","22d789b7":"code","b167f413":"code","c6d5cc79":"code","553eaa89":"code","413a6bf2":"code","3812f48a":"code","2a97f967":"code","b75d3437":"code","450d107c":"code","a3e2694e":"code","ac92b3b5":"code","df3077dc":"markdown","08349537":"markdown","f968e7e8":"markdown","1fffc4fb":"markdown","2e066101":"markdown","bb69aedb":"markdown","2ad5ce65":"markdown","9090ab65":"markdown","5b4ba9f3":"markdown","f07e8b47":"markdown","19084d70":"markdown","1444ee13":"markdown","4eb9b5d1":"markdown","7c552023":"markdown","6d418514":"markdown","a445bd55":"markdown","aeec2f17":"markdown","9f61ee7f":"markdown","e84dd421":"markdown","916994a1":"markdown","5853157f":"markdown","1cdf64bd":"markdown","1ec139ed":"markdown","289cb06f":"markdown","d285f1cc":"markdown","d2246c58":"markdown","2944af6c":"markdown","466d74c0":"markdown","d261d5f3":"markdown","6fdde980":"markdown","7a2d8f26":"markdown","5df33782":"markdown","62490bf3":"markdown","bd09b20b":"markdown","8db554bf":"markdown","12077bf2":"markdown","953d2a43":"markdown"},"source":{"daf22fdb":"import pandas as pd\nimport numpy as np\nfrom plotly import subplots\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport datetime\nimport plotly.express as px\n\n\n# this is just a helper function for plotting \ndef annotation_helper(fig, texts, x, y, line_spacing, align=\"left\", bgcolor=\"rgba(0,0,0,0)\", borderpad=0, ref=\"axes\", width=100):\n    \n    is_line_spacing_list = isinstance(line_spacing, list)\n    total_spacing = 0\n    \n    for index, text in enumerate(texts):\n        if is_line_spacing_list and index!= len(line_spacing):\n            current_line_spacing = line_spacing[index]\n        elif not is_line_spacing_list:\n            current_line_spacing = line_spacing\n        \n        fig.add_annotation(dict(\n            x= x,\n            y= y - total_spacing,\n            width = width,\n            showarrow=False,\n            text= text,\n            bgcolor= bgcolor,\n            align= align,\n            borderpad=4,\n            xref= \"x\" if ref==\"axes\" else \"paper\",\n            yref= \"y\" if ref==\"axes\" else \"paper\"\n        ))\n        \n        total_spacing  += current_line_spacing\n\n# codes for different colors\npalette_darkgrey = \"#383C45\"\npalette_silver = \"#A2A5A9\"\npalette_green = \"#4DC000\"\npalette_blue = \"#278BD3\"\npalette_platinum = \"#E3E4E5\" \npalette_red = '#d32744'\npalette_grey2 = \"#676A6C\"\npalette_grey3 = \"#959894\"\npalette_grey4 = \"#C4C5BB\"  ","ea3200ac":"df = pd.read_csv('..\/input\/cc-fraud-data\/transactionV2.csv')\ndf.head()","f0f860ad":"df.isna().sum()","d600587a":"df.dtypes","55999e8d":"df.drop_duplicates().shape[0]","75666fe7":"df.shape","12508327":"df.drop_duplicates(inplace=True)","8c4bbea2":"df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\nprint(\"First date of transaction data {}\".format(min(df['trans_date_trans_time'])))\nprint(\"Last date of transaction data {}\".format(max(df['trans_date_trans_time'])))\nprint(\"Number of transactions {}\".format(df.shape[0]))","17eead3c":"init_notebook_mode(connected=True)\nlayout = dict(\n    margin = dict(t=150),\n    xaxis = dict(showline=True, linewidth=1, linecolor=palette_darkgrey, dtick=\"M1\",tickformat=\"%b\\n%Y\"),\n    yaxis = dict(showline=False, showgrid=True, gridwidth=1, gridcolor='#ddd', linecolor=palette_darkgrey),\n    showlegend = False,\n    hovermode=\"x unified\",\n    width = 800,\n    height = 500,\n    plot_bgcolor= \"#fff\",\n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=12\n    )\n)\n\nfig = go.Figure(layout=layout)\n\ndf['month_year'] = pd.to_datetime(df['trans_date_trans_time']).dt.to_period('M')\ncount_by_time = df.groupby('month_year').count().reset_index()\ncount_by_time['Number of Samples'] = count_by_time['trans_date_trans_time']\ncount_by_time = count_by_time[['month_year','Number of Samples']]\ncount_by_time['month_year'] = count_by_time['month_year'].dt.to_timestamp()\n\nfig.add_trace(go.Scatter(\n                    x= count_by_time['month_year'], \n                    y= count_by_time['Number of Samples'],\n                    mode='lines',\n                    line= dict(color=palette_green, width=3),\n                    name='     train'))\n\n\ntext = [\n    \"<span style='color:%s; font-family:Tahoma; font-size:14px'>Uneven number of observations for each month<\/span>\" % palette_darkgrey\n]\n\nannotation_helper(fig, text, datetime.date(2019, 4, 15), 9500, [35], width=350)\n\n\n# title annotation\ntext = [\n    \"<span style='font-size:26px; font-family:Times New Roman;'>Number of Data Points for each month in dataset <\/span>\", \n    \"\"\"<span style='font-size:13px; font-family:Helvetica'><b style='color:%s'>Data is not distributed evenly over time<\/b><\/span>\"\"\" % (palette_red) \n    \n]\n\nannotation_helper(fig, text, 1.1, 1.375, [0.12,0.055,0.055], ref=\"paper\", width=700)\nfig.update_layout( xaxis = go.layout.XAxis( tickangle = 45) )\nfig.show()","02032724":"init_notebook_mode(connected=True)\nlayout = dict(\n    margin = dict(t=150),\n    xaxis = dict(showline=True, linewidth=1, linecolor=palette_darkgrey, dtick=\"M1\",tickformat=\"%b\\n%Y\"),\n    yaxis = dict(showline=False, showgrid=True, gridwidth=1, gridcolor='#ddd', linecolor=palette_darkgrey),\n    showlegend = False,\n    hovermode=\"x unified\",\n    width = 800,\n    height = 500,\n    plot_bgcolor= \"#fff\",\n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=12\n    )\n)\n\nfig = go.Figure(layout=layout)\n\nfraud_by_time = df.groupby('month_year')[['is_fraud','trans_date_trans_time']].sum().reset_index()\nfraud_by_time['num_samples']  = count_by_time['Number of Samples']\nfraud_by_time['fraud_rate'] = fraud_by_time['is_fraud']\/fraud_by_time['num_samples']\nfraud_by_time['month_year'] = fraud_by_time['month_year'].dt.to_timestamp()\n\nfig.add_trace(go.Scatter(\n                    x= fraud_by_time['month_year'], \n                    y= fraud_by_time['fraud_rate'],\n                    mode='lines',\n                    line= dict(color=palette_green, width=3),\n                    name='     train'))\n\n\ntext = [\n    \"<span style='color:%s; font-family:Tahoma; font-size:14px'>Even fraud rate per month<\/span>\" % palette_darkgrey\n]\n\nannotation_helper(fig, text, datetime.date(2019, 4, 15), 0.0375, [0.5], width=350)\n\n\n# title annotation\ntext = [\n    \"<span style='font-size:26px; font-family:Times New Roman;'>Fraud Rate by month <\/span>\", \n    \"\"\"<span style='font-size:13px; font-family:Helvetica'><b style='color:%s'><\/b>Not a large variance day to day<\/span>\"\"\" % (palette_darkgrey) \n    \n]\n\nannotation_helper(fig, text, 1.1, 1.375, [0.12,0.055,0.055], ref=\"paper\", width=700)\nfig.update_layout( xaxis = go.layout.XAxis( tickangle = 45) )\nfig.show()","6c7dfc15":"init_notebook_mode(connected=True)\nlayout = dict(\n    margin = dict(t=150),\n    xaxis = dict(showline=True, linewidth=1, linecolor=palette_darkgrey, dtick=\"M1\",tickformat=\"%b\\n%Y\"),\n    yaxis = dict(showline=False, showgrid=True, gridwidth=1, gridcolor='#ddd', linecolor=palette_darkgrey),\n    showlegend = False,\n    hovermode=\"x unified\",\n    width = 800,\n    height = 500,\n    plot_bgcolor= \"#fff\",\n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=12\n    )\n)\n\nfig = go.Figure(layout=layout)\n\ndf['day'] = pd.to_datetime(df['trans_date_trans_time']).dt.dayofweek\ncount_by_dow = df.groupby('day').count().reset_index()\ncount_by_dow['Number of Samples'] = count_by_dow['trans_date_trans_time']\ncount_by_dow = count_by_dow[['day','Number of Samples']]\n\nfig.add_trace(go.Scatter(\n                    x= count_by_dow['day'], \n                    y= count_by_dow['Number of Samples'],\n                    mode='lines',\n                    line= dict(color=palette_green, width=3),\n                    name='     train'))\n\n\ntext = [\n    \"<span style='color:%s; font-family:Tahoma; font-size:14px'>Uneven number of observations for each day of the week<\/span>\" % palette_darkgrey\n]\n\nannotation_helper(fig, text, datetime.date(2020, 8, 15), 19000, [35], width=350)\n\n\n# title annotation\ntext = [\n    \"<span style='font-size:26px; font-family:Times New Roman;'>Number of Data Points for each day in dataset <\/span>\", \n    \"\"\"<span style='font-size:13px; font-family:Helvetica'><b style='color:%s'>Data is not distributed evenly over the week<\/b><\/span>\"\"\" % (palette_red) \n    \n]\n\nannotation_helper(fig, text, 1.1, 1.375, [0.12,0.055,0.055], ref=\"paper\", width=700)\nfig.update_layout( xaxis = go.layout.XAxis( tickangle = 45) )\nfig.show()","710a6181":"init_notebook_mode(connected=True)\nlayout = dict(\n    margin = dict(t=150),\n    xaxis = dict(showline=True, linewidth=1, linecolor=palette_darkgrey, dtick=\"M1\",tickformat=\"%b\\n%Y\"),\n    yaxis = dict(showline=False, showgrid=True, gridwidth=1, gridcolor='#ddd', linecolor=palette_darkgrey),\n    showlegend = False,\n    hovermode=\"x unified\",\n    width = 800,\n    height = 500,\n    plot_bgcolor= \"#fff\",\n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=12\n    )\n)\n\nfig = go.Figure(layout=layout)\n\n\nfraud_by_day = df.groupby('day')[['is_fraud','trans_date_trans_time']].sum().reset_index()\nfraud_by_day['num_samples']  = count_by_dow['Number of Samples']\nfraud_by_day['fraud_rate'] = fraud_by_day['is_fraud']\/fraud_by_day['num_samples']\n# fraud_by_day['day'] = fraud_by_day['day'].dt.to_timestamp()\n\nfig.add_trace(go.Scatter(\n                    x= fraud_by_day['day'], \n                    y= fraud_by_day['fraud_rate'],\n                    mode='lines',\n                    line= dict(color=palette_green, width=3),\n                    name='     train'))\n\n\ntext = [\n    \"<span style='color:%s; font-family:Tahoma; font-size:14px'>Uneven number of observations for each day of the week<\/span>\" % palette_darkgrey\n]\n\nannotation_helper(fig, text, datetime.date(2021, 10, 15), 0.025, [10], width=450)\n\n\n# title annotation\ntext = [\n    \"<span style='font-size:26px; font-family:Times New Roman;'>Fraud rate for each day in dataset <\/span>\", \n    \"\"\"<span style='font-size:13px; font-family:Helvetica'><b style='color:%s'>Fraud rate is mostly even<\/b><\/span>\"\"\" % (palette_green) \n    \n]\n\nannotation_helper(fig, text, 1.1, 1.375, [0.12,0.055,0.055], ref=\"paper\", width=700)\nfig.update_layout( xaxis = go.layout.XAxis( tickangle = 45) )\nfig.show()","f6ee4e32":"init_notebook_mode(connected=True)\nlayout = dict(\n    margin = dict(t=150),\n    xaxis = dict(showline=True, linewidth=1, linecolor=palette_darkgrey, dtick=\"M1\",tickformat=\"%b\\n%Y\"),\n    yaxis = dict(showline=False, showgrid=True, gridwidth=1, gridcolor='#ddd', linecolor=palette_darkgrey),\n    showlegend = False,\n    hovermode=\"x unified\",\n    width = 800,\n    height = 500,\n    plot_bgcolor= \"#fff\",\n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=12\n    )\n)\n\nfig = go.Figure(layout=layout)\n\ndf['hour'] = pd.to_datetime(df['trans_date_trans_time']).dt.hour\ncount_by_hour = df.groupby('hour').count().reset_index()\ncount_by_hour['Number of Samples'] = count_by_hour['trans_date_trans_time']\ncount_by_hour = count_by_hour[['hour','Number of Samples']]\n\nfig.add_trace(go.Scatter(\n                    x= count_by_hour['hour'], \n                    y= count_by_hour['Number of Samples'],\n                    mode='lines',\n                    line= dict(color=palette_green, width=3),\n                    name='     train'))\n\n\n# text = [\n#     \"<span style='color:%s; font-family:Tahoma; font-size:14px'>Uneven number of observations for each day of the week<\/span>\" % palette_darkgrey\n# ]\n\n# annotation_helper(fig, text, datetime.date(2020, 8, 15), 6000, [35], width=350)\n\n\n# title annotation\ntext = [\n    \"<span style='font-size:26px; font-family:Times New Roman;'>Number of Data Points for each hour in the day<\/span>\", \n    \"\"\"<span style='font-size:13px; font-family:Helvetica'><b style='color:%s'>More transactions in the afternoon<\/b><\/span>\"\"\" % (palette_red) \n    \n]\n\nannotation_helper(fig, text, 1.1, 1.375, [0.12,0.055,0.055], ref=\"paper\", width=700)\nfig.update_layout( xaxis = go.layout.XAxis( tickangle = 45) )\nfig.show()","26f2b495":"init_notebook_mode(connected=True)\nlayout = dict(\n    margin = dict(t=150),\n    xaxis = dict(showline=True, linewidth=1, linecolor=palette_darkgrey, dtick=\"M1\",tickformat=\"%b\\n%Y\"),\n    yaxis = dict(showline=False, showgrid=True, gridwidth=1, gridcolor='#ddd', linecolor=palette_darkgrey),\n    showlegend = False,\n    hovermode=\"x unified\",\n    width = 800,\n    height = 500,\n    plot_bgcolor= \"#fff\",\n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=12\n    )\n)\n\nfig = go.Figure(layout=layout)\n\n\nfraud_by_hour = df.groupby('hour')[['is_fraud','trans_date_trans_time']].sum().reset_index()\nfraud_by_hour['num_samples']  = count_by_hour['Number of Samples']\nfraud_by_hour['fraud_rate'] = fraud_by_hour['is_fraud']\/fraud_by_hour['num_samples']\n\nfig.add_trace(go.Scatter(\n                    x= fraud_by_hour['hour'], \n                    y= fraud_by_hour['fraud_rate'],\n                    mode='lines',\n                    line= dict(color=palette_green, width=3),\n                    name='     train'))\n\n\n# text = [\n#     \"<span style='color:%s; font-family:Tahoma; font-size:14px'>Uneven number of observations for each day of the week<\/span>\" % palette_darkgrey\n# ]\n\n# annotation_helper(fig, text, datetime.date(2020, 8, 15), 0.04, [35], width=350)\n\n\n# title annotation\ntext = [\n    \"<span style='font-size:26px; font-family:Times New Roman;'>Fraud rate at each hour<\/span>\"\n]\n\nannotation_helper(fig, text, 1.1, 1.375, [0.12,0.055,0.055], ref=\"paper\", width=700)\nfig.update_layout( xaxis = go.layout.XAxis( tickangle = 45) )\nfig.show()","fcade978":"init_notebook_mode(connected=True)\nlayout = dict(\n    margin = dict(t=150),\n    xaxis = dict(showline=True, linewidth=1, linecolor=palette_darkgrey, dtick=\"M1\",tickformat=\"%b\\n%Y\"),\n    yaxis = dict(showline=False, showgrid=True, gridwidth=1, gridcolor='#ddd', linecolor=palette_darkgrey),\n    showlegend = False,\n    hovermode=\"x unified\",\n    width = 500,\n    height = 400,\n    plot_bgcolor= \"#fff\",\n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=12\n    )\n)\nfig = px.histogram(df['cc_num'].value_counts(), \n    nbins=100, \n    title=\"Number of transactions per cc_num\")\nfig.update_layout( xaxis = go.layout.XAxis( tickangle = 45) )\n\nfig.show()","42600baf":"init_notebook_mode(connected=True)\nfig = px.histogram(df.groupby(['cc_num'])['is_fraud'].sum().reset_index()['is_fraud'], \n    nbins=100, \n    title=\"Number of credit card numbers per fraud cases\")\nfig.show()","d4b86cd1":"print(df['merchant'].nunique())\ndf['merchant'].value_counts()","f6a2b3f7":"init_notebook_mode(connected=True)\nfig = px.histogram(df.groupby(df['merchant'])['is_fraud'].sum()\/df.groupby(df['merchant'])['is_fraud'].count(), \n    nbins=100, \n    title=\"Histogram of fraud rates for each merchant\")\nfig.show()","63629b77":"print(df['category'].nunique())\ndf['category'].value_counts()","45776815":"df.groupby(df['category'])['is_fraud'].sum()\/df.groupby(df['category'])['is_fraud'].count()","f745d9e4":"init_notebook_mode(connected=True)\nfig = px.histogram(df['amt'], \n    nbins=100, \n    title=\"Histogram of amount of each transaction\")\nfig.show()","116761d8":"init_notebook_mode(connected=True)\nfig = px.histogram(df.groupby('cc_num')['amt'].median(), \n    nbins=100, \n    title=\"Median transaction amount per cc num\")\nfig.show()","6228f134":"init_notebook_mode(connected=True)\nfig = px.bar(df.groupby('gender')['is_fraud'].mean(), \n    # nbins=100, \n    title=\"Mean transaction amount per gender in the dataset\")\nfig.show()","169a00f2":"df['state'].value_counts()","100b6d8a":"init_notebook_mode(connected=True)\nfig = go.Figure(data=go.Scattergeo(\n        lon = df['long'],\n        lat = df['lat'],\n        text = df['is_fraud'],\n        mode = 'markers',\n        marker = dict(\n            size = 3,\n            opacity = 0.25\n            ),\n        # marker_color = df['is_fraud'],\n        ))\n\nfig.update_layout(\n        title = 'Map of CC users locations',\n        geo_scope='usa'\n    )\nfig.show()","2b2ecf1a":"# init_notebook_mode(connected=True)\n# fig = go.Figure(data=go.Scattergeo(\n#         lon = df['merch_long'],\n#         lat = df['merch_lat'],\n#         text = df['is_fraud'],\n#         mode = 'markers',\n#         marker = dict(\n#             size = 3,\n#             opacity = 0.25\n#             ),\n#         # marker_color = df['is_fraud'],\n#         ))\n\n# fig.update_layout(\n#         title = 'Map of Merchants locations',\n#         geo_scope='usa'\n#     )\n# fig.show()","2aa6d4c1":"df.groupby(df['job'])['is_fraud'].mean().sort_values()","22d789b7":"# fig = go.Figure(data=go.Scattergeo(\n#         lon = df['long'],\n#         lat = df['lat'],\n#         text = df['is_fraud'],\n#         mode = 'heatmap',\n#         marker_color = df['is_fraud'],\n#         ))\n\n# fig.update_layout(\n#         title = 'Map of credit',\n#         geo_scope='usa',\n#     )\n# fig.show()\n","b167f413":"import math\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\n\ndef create_features(df):\n    \"\"\"\n    calculates all features given a pandas dataframe\n    \"\"\"\n\n    # change dataset types\n    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n\n    # create binary labels from true\/false\n    df['is_fraud'] = np.where(df['is_fraud'] == False, 0, 1)\n\n    # one hot encode whether gender of individual is male or female (1 for male 0 for female)\n    df['is_male'] = np.where(df['gender']=='M', 1, 0)\n    \n    # create age of customer at time of transaction\n    df['dob'] = pd.to_datetime(df['dob'])\n    df['age_of_customer'] = np.round((df['trans_date_trans_time'] - df['dob'])\/np.timedelta64(1,'Y'))\n    df[\"dob_year\"] = df[\"dob\"].dt.year\n    df[\"dob_day\"] = df[\"dob\"].dt.day\n    df[\"dob_month\"] = df[\"dob\"].dt.month\n    df[\"dob_dow\"] = df[\"dob\"].dt.dayofweek\n    df[\"dob_doy\"] = df[\"dob\"].dt.dayofyear\n    \n    # get some data from transaction\n    df[\"trans_date_trans_time_year\"] = df[\"trans_date_trans_time\"].dt.year\n    df[\"trans_date_trans_time_hour\"] = df[\"trans_date_trans_time\"].dt.hour\n    df[\"trans_date_trans_time_day\"] = df[\"trans_date_trans_time\"].dt.day\n    df[\"trans_date_trans_time_month\"] = df[\"trans_date_trans_time\"].dt.month\n    df[\"trans_date_trans_time_dow\"] = df[\"trans_date_trans_time\"].dt.dayofweek\n    df[\"trans_date_trans_time_doy\"] = df[\"trans_date_trans_time\"].dt.dayofyear\n    \n    # order by date to create features that have no time leakage\n    df = df.sort_values(by=['trans_date_trans_time','cc_num'])\n    \n    # get time since last transaction\n    df['seconds_since_last_transaction'] = (df['trans_date_trans_time'] - df.groupby(['cc_num'])['trans_date_trans_time'].shift(1)).dt.seconds\n    df['time_since_last_trans_diff'] = df['seconds_since_last_transaction'] - df.groupby(['cc_num'])['seconds_since_last_transaction'].shift(1)\n    \n    # distance in KM between customer location and merchant location\n    df['distance_between_merchant_and_customer_location'] = 6367 * 2 * np.arcsin(np.sqrt(np.sin((np.radians(df['lat']) \\\n            - np.radians(df['merch_lat']))\/2)**2 + np.cos(np.radians(df['merch_lat'])) * np.cos(np.radians(df['lat'])) \\\n                * np.sin((np.radians(df['long']) - np.radians(df['merch_long']))\/2)**2))\n    \n    # get lagged distance of last transaction\n    df['distance_between_merchant_and_customer_location_last'] = df.groupby(['cc_num'])['distance_between_merchant_and_customer_location'].shift(1)\n    \n    # get coordinates of previous merchant\n    df['prev_merch_long']  = df.groupby(['cc_num'])['merch_long'].shift(1)\n    df['prev_merch_lat'] = df.groupby(['cc_num'])['merch_lat'].shift(1)\n\n    # previous merchant long and lat\n    if df['prev_merch_long'].values[-1] == None:\n        # null value, no previous merchant data\n        df['distance_between_last_transaction_and_current_transaction'] = 0\n    else:\n        # distance between current and last transaction\n        df['distance_between_last_transaction_and_current_transaction'] = 6367 * 2 * np.arcsin(np.sqrt(np.sin((np.radians(df['prev_merch_lat']) \\\n            - np.radians(df['merch_lat']))\/2)**2 + np.cos(np.radians(df['merch_lat'])) * np.cos(np.radians(df['prev_merch_lat'])) \\\n                * np.sin((np.radians(df['prev_merch_long']) - np.radians(df['merch_long']))\/2)**2))\n    \n    # fill null values\n    df['distance_between_last_transaction_and_current_transaction'] = df['distance_between_last_transaction_and_current_transaction'].fillna(0)\n    \n    # get previous merchant data\n    df['prev_merch_long']  = df.groupby(['cc_num'])['merch_long'].shift(1)\n    df['prev_merch_lat'] = df.groupby(['cc_num'])['merch_lat'].shift(1)\n    \n    # get OHE for category\n    T_dummies = pd.get_dummies(df['category'])\n    df = pd.concat([df, T_dummies], axis = 1)\n    \n    # OHE for merchant\n    T_dummies = pd.get_dummies(df['merchant'])\n    df = pd.concat([df, T_dummies], axis = 1)\n    \n    # OHE for job\n    T_dummies = pd.get_dummies(df['job'])\n    df = pd.concat([df, T_dummies], axis = 1)\n    \n    # last amount value\n    df['amt_lag'] = df.groupby('cc_num')['amt'].shift(1)\n    \n    # delta between last value\n    df['amt_diff_lag'] = df['amt'] - df['amt_lag']\n    \n    # rolling average\n    # amount and group by\n    group_by_lst = ['cc_num','merchant','job','category']\n    for col in group_by_lst:\n        df['amt_{}_past_last_3_avg'.format(col)] = df.groupby([col])['amt'].rolling(3).mean().reset_index(level=0, drop=True)\n        df['amt_{}_past_last_10_avg'.format(col)] = df.groupby([col])['amt'].rolling(10).mean().reset_index(level=0, drop=True)\n        df['amt_{}_past_last_15_avg'.format(col)] = df.groupby([col])['amt'].rolling(15).mean().reset_index(level=0, drop=True)\n\n        df['amt_{}_diff_past_last_3_avg'.format(col)] = df.groupby([col])['amt_diff_lag'].rolling(3).mean().reset_index(level=0, drop=True)\n        df['amt_{}_diff_past_last_10_avg'.format(col)] = df.groupby([col])['amt_diff_lag'].rolling(10).mean().reset_index(level=0, drop=True)\n        df['amt_{}_diff_past_last_15_avg'.format(col)] = df.groupby([col])['amt_diff_lag'].rolling(15).mean().reset_index(level=0, drop=True)\n\n        df['amt_{}_diff_rolling_avg_3'.format(col)] = df['amt'] - df['amt_{}_past_last_3_avg'.format(col)]\n        df['amt_{}_diff_rolling_avg_10'.format(col)] = df['amt'] - df['amt_{}_past_last_10_avg'.format(col)]\n        df['amt_{}_diff_rolling_avg_15'.format(col)] = df['amt'] - df['amt_{}_past_last_15_avg'.format(col)]\n\n        df['amt_{}_past_last_3_stddev'.format(col)] = df.groupby([col])['amt'].rolling(3).std().reset_index(level=0, drop=True)\n        df['amt_{}_past_last_10_stddev'.format(col)] = df.groupby([col])['amt'].rolling(10).std().reset_index(level=0, drop=True)\n        df['amt_{}_past_last_15_stddev'.format(col)] = df.groupby([col])['amt'].rolling(15).std().reset_index(level=0, drop=True)\n    \n    df['amt\/time_since_last_trans'] = df['amt']\/df['seconds_since_last_transaction']\n    \n    # get counts for last N days\n    df.index = pd.to_datetime(df['trans_date_trans_time'])\n    df['val_for_agg'] = 1\n    rolling_60_day_count = df.groupby(['cc_num'])['val_for_agg'].rolling('60D').count().shift().reset_index().fillna(0)\n    rolling_60_day_count['60D_rolling_count'] = rolling_60_day_count['val_for_agg']\n    rolling_60_day_count.drop(columns=['val_for_agg'],inplace=True)\n    rolling_60_day_count.reset_index(inplace=True, drop=True)\n    rolling_60_day_count.index.name = None\n    df.reset_index(inplace=True, drop=True)\n    df.drop(columns='val_for_agg',inplace=True)\n    df = df.merge(rolling_60_day_count, left_on = ['cc_num','trans_date_trans_time'], right_on = ['cc_num','trans_date_trans_time'],how = 'left')\n    \n    # get counts for last N days\n    df.index = pd.to_datetime(df['trans_date_trans_time'])\n    df['val_for_agg'] = 1\n    rolling_30_day_count = df.groupby(['cc_num'])['val_for_agg'].rolling('30D').count().shift().reset_index().fillna(0)\n    rolling_30_day_count['30D_rolling_count'] = rolling_30_day_count['val_for_agg']\n    rolling_30_day_count.drop(columns=['val_for_agg'],inplace=True)\n    rolling_30_day_count.reset_index(inplace=True, drop=True)\n    rolling_30_day_count.index.name = None\n    df.reset_index(inplace=True, drop=True)\n    df.drop(columns='val_for_agg',inplace=True)\n    df = df.merge(rolling_30_day_count, left_on = ['cc_num','trans_date_trans_time'], right_on = ['cc_num','trans_date_trans_time'],how = 'left')\n    \n    # get counts for last N days\n    df.index = pd.to_datetime(df['trans_date_trans_time'])\n    df['val_for_agg'] = 1\n    rolling_15_day_count = df.groupby(['cc_num'])['val_for_agg'].rolling('15D').count().shift().reset_index().fillna(0)\n    rolling_15_day_count['15D_rolling_count'] = rolling_15_day_count['val_for_agg']\n    rolling_15_day_count.drop(columns=['val_for_agg'],inplace=True)\n    rolling_15_day_count.reset_index(inplace=True, drop=True)\n    rolling_15_day_count.index.name = None\n    df.reset_index(inplace=True, drop=True)\n    df.drop(columns='val_for_agg',inplace=True)\n    df = df.merge(rolling_15_day_count, left_on = ['cc_num','trans_date_trans_time'], right_on = ['cc_num','trans_date_trans_time'],how = 'left')\n    \n    # drop columns\n    df.drop(columns = ['Unnamed: 0',\n     'cc_num',\n     'merchant',\n     'category',\n     'first',\n     'last',\n     'gender',\n     'street',\n     'city',\n     'state',\n     'job',\n     'dob',\n     'trans_num',\n     'unix_time'], inplace=True)\n\n    return df","c6d5cc79":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# code to get the data\n\n# read in data\ndf = pd.read_csv('..\/input\/cc-fraud-data\/transactionV2.csv')\n\n# drop duplicates in dataset\ndf.drop_duplicates(inplace=True)\n\n# sort data by time\ndf = df.sort_values(by='trans_date_trans_time')\n\ndf = create_features(df)\ndf.head()","553eaa89":"# import ML libraries\nimport lightgbm as lgb # same thing as xgboost\nimport itertools\nimport re\nfrom sklearn.model_selection import TimeSeriesSplit # to split by time","413a6bf2":"# change columns names\ndf = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\n# get training and testing split\ntrain_data = df[df['trans_date_trans_time'] < '2020-04-21'].drop(columns=['trans_date_trans_time'])\nx_test, y_test = df[df['trans_date_trans_time'] >= '2020-04-21'].drop(columns=['trans_date_trans_time','is_fraud']), df[df['trans_date_trans_time'] >= '2020-04-21']['is_fraud']","3812f48a":"print(train_data.shape)\nprint(x_test.shape)","2a97f967":"def get_class_lgb_model(df, param_grid, n_splits):\n    \"\"\"\n    get_class_lgb_model: creates lightgbm regression model\n    \n    inputs: \n        df: training dataset, assuming conditions from competition website\n        \n    outputs:\n        lgb_model: lightgbm classification model to predict on\n    \n    \"\"\"\n    \n    # code to nicely create parameter grid\n    keys, values = zip(*param_grid.items())\n    param_list = [dict(zip(keys, v)) for v in itertools.product(*values)]\n    \n    # holds results\n    all_results = []\n    \n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    \n    # run CV on each parameter option\n    for param in param_list:\n        # prepare data for lightgbm\n        train_data = lgb.Dataset(df.drop(columns=['is_fraud']), \n                                 label=df['is_fraud'],\n                                params={'max_bin':param['max_bin']})\n        print(param)\n        # run cv on folds\n        cv_model = lgb.cv(param, \n                       train_data, \n                       1000, \n                       early_stopping_rounds=100,\n                       folds = tscv.split(df),\n                       verbose_eval=10,\n                       return_cvbooster=True)\n        \n        # get results\n        param['auc-mean'] = max(cv_model['auc-mean'])\n        param['num_its'] = len(cv_model['auc-mean'])\n        all_results.append(param)\n    \n    # cv is done, get best performing parameters\n    params = pd.DataFrame(all_results).sort_values(by='auc-mean').loc[0].to_dict()\n    print('Best Score: {}'.format(params['auc-mean']))\n    \n    # train final model with final parameters\n    train_data = lgb.Dataset(df.drop(columns=['is_fraud']), \n                             label=df['is_fraud'],\n                             params={'max_bin':params['max_bin']})\n    \n    lgb_model = lgb.train(params, train_data, params['num_its'])\n    \n    return lgb_model\n\nclass_params = {'num_leaves': [8],\n                'scale_pos_weight':[20],\n              'min_data_in_leaf':[20],\n              'max_bin': [128],\n              'learning_rate':[0.01],\n              'objective': ['binary'],\n             'metrics':['auc'],\n              'seed':[0]\n             }\n\nmodel = get_class_lgb_model(train_data, class_params, 10)","b75d3437":"import plotly.express as px\nfrom sklearn.metrics import precision_recall_curve, auc\n\ny_score = model.predict(x_test)\n\nprecision, recall, thresholds = precision_recall_curve(y_test, y_score)\n\nfig = px.area(\n    x=recall, y=precision,\n    title='Precision-Recall Curve',\n    labels=dict(x='Recall', y='Precision'),\n    width=700, height=500\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\n\nfig.show()","450d107c":"import shap\nimport copy\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer(x_test)\nshap_values2 = copy.deepcopy(shap_values)\nshap_values2.values = shap_values2.values[:,:,1]\nshap_values2.base_values = shap_values2.base_values[:,1]","a3e2694e":"shap.plots.beeswarm(shap_values2, max_display=25)","ac92b3b5":"shap.plots.bar(shap_values2, max_display=25)","df3077dc":"Some customers use their cards more often than others, this distribution makes sense. \n\nDo credit cards have more than 1 fraud cases? Below we group by the credit card number to understand if multiple fraud alerts occur on single credit cards.","08349537":"As an evaluation metric I decided to use AUC which is the area under the curve of the ROC curve. AUC plots the true positive rate on the y axis vs the false positive rate ont he x axis. Intuitively this means that an AUC metric of 0.5 is the same as a random guess. If the model created has any value it should have a higher AUC than 0.5 certainly. However in our case the AUC score is about 0.5 which means that the model didn't learn much and that possibly you cannot create a model with the data.\n\nThis means the model created has about the same performance as a random guess.\n\nThis means that the model didn't learn much about fraud from the transactions and feature provided. I have a personal hunch that the data wasn't informative enough to create a ML model and this was a part of the exercise to see whether a particular candidate would create a model incorrectly. \n\nHowever, if the model were to generalize well on the validation set, I would use AUC under the precision recall curve to evaluate the precision and recall of the model. Below I plot auc pr in a interactive graph. What this metric would tell you is whether the model peformed well on lower amounts of recall. \n","f968e7e8":"Drop duplicate rows, it doesn't make sense for there to be duplicate transactions that occur at the exact same time with all the same data","1fffc4fb":"# Deployment in Cloud Env","2e066101":"# Modelling\n\nIn this section I will explain my intuition behind how to train, validate, and test a machine learning model to predict fraud. \n\nIts important to understand that there is a time dimension to the data and the transactions. When the model is deployed it is using past transactions and past behaviour to understand what future fraud behaviour looks like. \n\nWith this in mind I want to use past data to train a model, some future data to validate the model and then the final testing dataset should be the most recent data to simulate what deployment performance looks like. This should give us the best understanding of how the model should perform because when the model is deployed. This intuition differs from a simple random split in the cross validation as it seeks to simulate a true deployment scenario.\n\n\nFor the modelling aspect I decide to focus on one of the most popular ML algorithms for tabular data\n\n- Gradient Boosted decision trees (gbdt)\n\nThe reason for this choice is I wanted to use models that are both interpretable and provide great performance. GBDT are are an ensemble method and are less likely to overfit to the data. Algorithms that use decisions trees can capture interactions between many variables at different thresholds. These models are known to perform very well on tabular datasets.","bb69aedb":"Lets take a look at the gender and how it relates to fraud","2ad5ce65":"This makes a lot of sense a very left tailed distribution which makes sense as most transactions shouldn't be large","9090ab65":"There doesn't seem to be a visible trend in when fraud occurs on a day of the week basis.\n\nLets see how this breaks down on an hour basis.","5b4ba9f3":"The data provided spans from the start of 2019 to June 21st 2021.\n\nLets see how its distributed across time, grouped by month. Below we plot the number of transactions by each month.","f07e8b47":"Due to not having too much data and wanting a representative sample to train a model on the following breakdown is chosen for training, validation and testing.\n\nDates in YYYY-MM-DD format\n\ntrain and validate: 2019-01-01 to 2020-04-20.\n\nTraining and validation works using a time series CV split, where the training set is the older data and the split is always the forward data. To do this I use sklearn time series CV split to split the data by the transaction time in the dataset provided.\n\ntest: 2020-04-21 to 2020-06-21.\n\nIn reality the training, validation and testing split would be determined based on how often new fraud labels are given, where a new model is trained based on those new labels and then redeployed. The format provided here is just to simulate how model performance should look like if a model is trained on past data, validated on some newer data and then tested on the newest data so we mimic the real testing process that the deployment would look like.","19084d70":"Nothing incredibly noticeable in the fraud rate per hour.","1444ee13":"Most of the feature engineering is based on the past transactions for a credit card number, mostly one hot encoding categorical features, getting lagged values, calculating running totals, getting distances in KMs between geographic coordinates.\n\nIn the feature engineering step, I make sure to not introduce any features that include the target so as to not introduce any target leakage.","4eb9b5d1":"Here it looks like it just fit to random noise in the dataset. ","7c552023":"# Model Explanations\n\nBelow is some same code for how one can look at feature importances. Since the data doesn't generalize whatsoever it doesn't make a lot of sense to look at this too much.\n\nShap values provide a explanation for each sample and each feature input which means we can get a relative explanation for each single input for each credit card transaction which allows us to better understand why a particular transaction may or may not be fraudulent. ","6d418514":"Lets map out where the customers are located across the USA.","a445bd55":"Each category has about the same fraud rate. Not a large difference\n\nLets take a look at the amount of each transaction:","aeec2f17":"Here I will write a bit about how I think a deployment for real time fraud inference should look like:\n\n## Business Problem\n\nWith this deployment in mind I seek to uncover the business problem and how to tackle it the best way to reduce losses for the business. Since this is a binary prediction scenario there are multiple scenarios to keep in mind. In the case of a false negative: where a fraudulent transaction is allowed to occur it can cost the company a lot of money. However when a non-fraudulent transaction is not allowed to occur there isn't a large cost apart from the customer finding it annoying.\n\nThis creates an interesting trade off. If too many transactions are flagged as fraud then it ruins the customer experience and the customer stops using the service. However if all the transactions are allowed to pass it can cost the company money. In this case its important to minimize the number of fraudulent transactions and keeping the number of false negatives low as the cost is unbalanced between false positives and false negatives, where false negatives cost far more money.\n\nBasically, we want to create a low latency service that minimizes the potential cost of fraud, that doesn't cost too much money to run on the cloud that also provides a good enough service for a customer.\n\n## Real Time Fraud Detection Rules + ML\n\nSimply put fraud detection is about finding behaviour that is an anomaly. Given the data for this challenge there are many things that can make a particular transaction look suspicious:\n\n- Transactions that occur very far away from where credit card holder lives\n- Transactions that are very large can incur a massive cost if fraudulent\n- Mismatch of personal details\n- Any sort of value that indicates an outlier for the customer\n- Past services or places where the customer has experienced fraud before\n\nHowever most transactions should not be fraudulent and should be within the normal bounds that are easy to create rules for\n\n- Smaller purchases that match customer profile\n- Places the customer is known to shop and has shopped at before\n- Exact same transaction customer has had before \n\nThe reason for these simple rulesets is that they can decrease the latency of this system and is easy to code and deploy, and should also minimize cost if deployed because the runtime is much shorter for obvious cases that should be investigated.\n\nAfter both sets of rules have been exhausted for both fraud and non fraud related cases we should only then turn to inference from a model. The reason for this is because inference from a model will take more time than a simple rule check for fraud cases.\n\nNow we can pull the ML model and the feature from a materialized feature store to run predictions. After this we can use the ML model to infer whether to identify as fraud or not fraud.\n\nAn edge case to keep in mind that was presented in this data challenge is the presence of duplicate transactions that have the exact same values. In the case of something like that the very first transaction should be processed but following transactions should be thrown away if they are all coming in with the exact same time.\n\n\n## Cloud Environment\n\nThis solution should be multi region so it can minimize latency, wherever the transaction takes place should be routed to the closest server. Lets break down further how this looks like when training a model and deploying this solution so it can have low latency.\n\nA ML model is trained offline and re-trained whenever labels are updated. The model file is held in some form of storage (S3) for which our lambda functions (https:\/\/aws.amazon.com\/lambda\/) can quickly access it. Similarly the features for the ML algorithm should be materialized ahead of time for quick access so the lambda function that determine whether the transaction is fraud or not just needs to read the data in and then run it through a model file, minimizing possible latency. \n\nA possible issue that may arise from running every single transaction through a lambda function is that it may be very costly to run a lambda function for each single request. Instead we can minibatch the transactions that come in, in say a small time window for. The only requirement for this is that the api that requests this must be able to handle the return of multiple predictions of fraud. This may come at the cost of some latency but it should be a measurable amount that doesn't make a massive difference in terms of user experience.\n\nA lambda function is basically a severless, stateless, remote, event-driven service that lets you run code for any application or service. The code for the rules and data reading and model predictions would be dockerized and ran in the cloud using a lambda function. \n\n## Workflow\n\n- Transaction arrives into queue for processing at server region. A lambda function (https:\/\/aws.amazon.com\/lambda\/) is invoked for each transaction (or batch of transactions in some timed period) so the resources are independent and can be processed at the same time. \n- Transaction data is logged into a DB.\n- A simple ruleset is deployed for quick inference of transactions, in which case the code should terminate earlier for that single prediction\n- Finally if the simple ruleset does not apply, a ML model is applied to determine whether the transaction was fraudulent as a last resort if the simple rules haven't decided that particular transaction\n- DB is updated with the decision\n- Data is sent back to the caller to show to the customer the result of their transaction\n- If the transaction is predicted to be fraudulent then a follow up message should be shown to the customer to remedy the situation\n- If the transaction is not deemed fraud then it should be processed regularly\n\n## Addition to workflow\n\n- There should be some monitoring occuring on the quality of the data to make its accurate and falls within regular ranges\n- There should be monitoring of key metrics like the percentage of transactions marked as fraud to make sure the system doesn't mark everything as fraud or nothing as fraudulent\n\n\nThis should properly summarize how I believe real time inference in the cloud should work for the model.\n\n\nThank you for reading! \n\nPavle","9f61ee7f":"0 = 12 AM, and so forth. It seems we have much more observations starting at 12 PM, after which the number of transactions stays the same.\n\nDoes the fraud rate change on an hourly basis?","e84dd421":"Lets see the fraud rate by merchant as a histogram","916994a1":"## Circle Data Science Challenge\n\nThis repository contains a sample of synthetic data for the Circle Data Scientist Recruiting process. \nWe have the transactions below with fraud labeling. \nWe need your help to analyze the data and build fraud detection model.\nWe expect you to do the following dasks:\n* Analyze the data as your data science daily job and help us to find critical insights for fraud detection.\n* Build and evaluate some machine learning models, and explain how and why you do this.\n* Share some thoughts about how you plan do the realtime inference for production env in the cloud. What is your consideration for trade-off. \n* EXTRA: Any interesting thing you learn from the data\n\nPlease send your answer without the transaction data to your recruiter, and include your name in the title of notebook. Respect you have your daily job, we can give you a week of time to send back the code resuilt. \nThank you for your interest, and look forward to hearing from you soon.","5853157f":"Most customers are located in the northeast of the country and there is a large concentration in california, this is expected, if this would be compared to the population distribution of the united states it would match pretty closely.\n\nLets see how this compares to where transactions take place ","1cdf64bd":"0 = monday\n1 = tuesday\n2 = wednesday\n3 = thursday\n4 = friday\n5 = saturday\n6 = sunday\n\nWe have the most transactions occuring on monday and sunday.","1ec139ed":"Lets take a look at the job of the credit card users","289cb06f":"It seems like a majority of cards will eventually experience atleast 1 fraudulent transaction, however due to the nature of fraud most transactions are still non fraudulent.\n\nThere is also merchant data in the dataset. Lets take a look closer at that data.","d285f1cc":"This below here is just plotted for fun to show what shap values and a plot would look like. This isn't meant to show what I believe influences fraud as the model wasn't performant whatsoever. ","d2246c58":"From the above graph we can see that the number of transactions peaks in december which coincides with the holidays as most of the data is based in the USA, while the lowest number of transactions occur right afterwards in january and febuary.\n\nNow lets try to understand how fraud may differ by month. Below the fraud percentage by month is plotted.","2944af6c":"Each fraudulent transaction is associated with a particular credit card numbers, lets understand the distribution of the number of transactions of each credit card. This can give us an idea of how often customers are using credit cards.\n\nNormally when a fraud alert occurs the customer is unable to do any transactions with the card.","466d74c0":"# Intro:\n\nCredit card fraud is important to prevent as it can cost companies lots of money if fraudlent transactions are allowed to occur. Fraudulent transactions both cost companies money and add anxiety to customers. Its important to prevent and predict whether credit card fraud occurs. \n\nWith this in mind I explore this dataset to uncover trends, features, and create models to prevent credit card fraud. In this notebook I aim to complete a few things listed below:\n\n- Exploratory data analysis (EDA) of the data and how it relates to credit card fraud\n- Modelling of the data using tree based methods\n- Exploration of the feature importances\n- Real time inference for production environment in the cloud\n\n\n","d261d5f3":"There doesn't seem to be too much variability from month to month in terms of the fraud rate, febuary 2019 has the peak of 2.37% fraud while january 2020 has the lowest fraud rate of 1.6%. \n\nNow lets try to understand how fraud rate may break down on a daily basis. To do this we plot the fraud rate by day of the week","6fdde980":"Fraud rate is the same for the genders in the dataset.\n\nLets take a look at the location data for the credit card users in the dataset","7a2d8f26":"Below a tree booster is trained with a time split with some simply chosen parameters. The function below outputs the CV results and a trained model on all the data.","5df33782":"\n\n# Summary of features in credit card fraud dataset:\n\n- trans_date_trans_time: \n- cc_cum: credit card number\n- merchant: merchant name\n- category: type of purchase\n- amt: amount spent\n- first: first name\n- last: last name\n- gender: gender of individual\n- street: street of customer\n- city: city of customer\n- state: state of customer\n- zip: zip code of transaction\n- lat: latitude of customer\n- long: longitute of customer\n- city_pop: population of city customer lives in\n- job: job of customer\n- dob: date of birth of customer\n- trans_num: unique transaction of every transaction\n- unix_time: time of transaction in unix time\n- merch_lat: merchant latitude\n- merch_long: merchant longitude\n- is_fraud: is the transaction fraud or not\n\n\nLets explore the data at hand with a simple exploratory data analysis. The goal of the EDA is to better understand the underlying data and to try to extract insights about fraud.\n","62490bf3":"Lets take a look at the category of transactions","bd09b20b":"# Feature creation\n\nThe way feature creation is structured is in a time based order. We can only create and infer features about the user based on past transactions, not future transactions. This means when we are processing say a transaction on 2020-02-01 we can only use past data based on that credit card number. If we use future data we may be introducing biases that may not be present when the model is deployed. \n\nWe also need to be congnizant to leak the label into the training dataset.","8db554bf":"As we can see here the performance on the testing set tends to the average fraud rate in the data which is what the random guess corresponds to.\n\nHowever if the model were to extrapolate we would evaulate the importances using shap values.","12077bf2":"Below I wrote some code to read the data, clean the data, make labels, change data types and create features","953d2a43":"\nThere are no null values in the dataset which makes understanding the dataset much easier as we do not need to worry about imputation or whether null values in the dataset have some particular meaning."}}