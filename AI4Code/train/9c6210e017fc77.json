{"cell_type":{"a3509142":"code","8673bc6b":"code","937a902a":"code","a448763b":"code","47beb111":"code","43b8a88e":"code","4d6066e0":"code","47a2a291":"code","7a03ce5c":"code","d5e5e363":"code","5cb618ad":"code","86087f2e":"code","aa8ffd64":"code","27f4d00a":"code","96d36f6b":"code","81ce2f94":"code","9afacb7e":"code","417bb0e9":"code","81c65b47":"code","df139e87":"code","37b55f79":"code","624af9b0":"code","a461bd26":"code","4085f2b0":"code","be64f030":"code","03ff64fd":"code","421f32c8":"code","b03e0da7":"code","088f6181":"code","0eafc34e":"code","1689f8a8":"markdown","9fb918d9":"markdown","68903e0f":"markdown","aea7bde6":"markdown","8cfb59f0":"markdown","6cbb5238":"markdown","77e6c0d1":"markdown","7a827676":"markdown","fad00983":"markdown","aa117979":"markdown","daa2015e":"markdown","64630542":"markdown","d7ffe4ce":"markdown","39fd777c":"markdown","acb927df":"markdown","e2db5292":"markdown","aa4f869e":"markdown","c96ca463":"markdown","f8037b1d":"markdown","94dcffaa":"markdown","4155956c":"markdown","8ca657d2":"markdown","9ea7eeef":"markdown","3c42cbd2":"markdown","84fa1290":"markdown","3750f84c":"markdown","ddb0c4cc":"markdown","21c1c468":"markdown","280cfeac":"markdown"},"source":{"a3509142":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Scikit learn librairies\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import scale\n\nimport pandas as pd\nimport numpy as np\n\n%matplotlib inline","8673bc6b":"df= pd.read_csv('..\/input\/train.csv')\ndftmp= pd.read_csv('..\/input\/train.csv')\ndftmp2= pd.read_csv('..\/input\/train.csv')\n#dfe= pd.read_csv('test.csv')\n#dftest= pd.read_csv('loan_car_short.csv')\n#df=dftest\n#print(dftest.shape,df.shape,df.columns)","937a902a":"df.head()","a448763b":"df.columns","47beb111":"fig11=plt.figure()\nax11=plt.axes()\nthe_target = dftmp['loan_default']\nthe_target.replace(to_replace=[1,0], value= ['YES','NO'], inplace = True)\nplt.title('Target repartition')\nax11 = ax11.set(xlabel='Default proportion', ylabel='Number of people')\nthe_target.value_counts().plot.bar()\nplt.show()","43b8a88e":"#Correlation Matrix calculation\ncorr_mat = df.corr()\n\nfig2=plt.figure()\nsns.set(rc={'figure.figsize':(20,15)})\nk = 10\ncols = corr_mat.nlargest(k, 'loan_default')['loan_default'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.title('Correlation Matrix')\nplt.show()","4d6066e0":"corr_mat['loan_default'].sort_values(ascending = False)","47a2a291":"#A function to print every graph with the ID as \ndef print_all_values():\n    df1=df.drop('UniqueID',axis=1)\n    cols=df1.columns\n    for col in cols:\n        if (df[col].dtypes !='object'):\n\n            fig1=plt.figure()\n            ax1=plt.axes()\n            plt.scatter(df[[col]],df.UniqueID,alpha=1,)\n            plt.title(col)\n            ax1 = ax1.set(xlabel=col, ylabel='ID')\n            plt.show()\n            \n            \nprint_all_values()","7a03ce5c":"#Delete to high or to low values\ndef delete_absurd_values(df_transformed,cols,max_value,percentage):\n        \n        \n        for col in cols:\n            if (df_transformed[col].dtypes !='object'):\n                       \n                q99=df_transformed[col].quantile(q=percentage)\n                q01=df_transformed[col].quantile(q=(1-percentage))\n                for i in df_transformed.index:\n                    \n                    if (df_transformed.loc[i,col]> max_value*q99 or df_transformed.loc[i,col]< q01\/max_value):\n                        df_transformed=df_transformed.drop(index=i)\n        \n        return df_transformed\n\ncols=['disbursed_amount', 'asset_cost', 'PRI.NO.OF.ACCTS', 'PRI.ACTIVE.ACCTS','PRI.OVERDUE.ACCTS','PRI.CURRENT.BALANCE', 'PRI.SANCTIONED.AMOUNT','PRI.SANCTIONED.AMOUNT',\n       'PRI.DISBURSED.AMOUNT', 'SEC.NO.OF.ACCTS', 'SEC.ACTIVE.ACCTS','SEC.CURRENT.BALANCE', 'SEC.SANCTIONED.AMOUNT',\n       'SEC.DISBURSED.AMOUNT', 'PRIMARY.INSTAL.AMT', 'SEC.INSTAL.AMT',\n       'NEW.ACCTS.IN.LAST.SIX.MONTHS', 'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS',\n       'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH', 'NO.OF_INQUIRIES']\ndf=delete_absurd_values(df,cols,5,0.999)","d5e5e363":"#The repartition of the target\nfig7=plt.figure()\nax7=plt.axes()\nthe_target = dftmp2['loan_default']\nthe_target.replace(to_replace=[1,0], value= ['YES','NO'], inplace = True)\nplt.title('Target repartition')\nax7 = ax7.set(xlabel='Default proportion')\nthe_target.value_counts().plot.pie()\nplt.show()","5cb618ad":"#Printing the types of the features\ndf.dtypes","86087f2e":"def nan_count_df(df_to_print):\n    \n    nan_count = df_to_print.isnull().sum()\n\n    nan_percentage = (nan_count \/ len(df))*100\n\n    nan_df=pd.concat([nan_percentage], axis=1)\n    nan_df=nan_df.rename(columns={0:'Percentage'})\n    nan_df=nan_df[nan_df.Percentage != 0]\n    nan_df = nan_df.sort_values(by='Percentage',ascending=False)\n    return nan_df\n\nnan_count_df(df)","aa8ffd64":"#Number of unique values\ndf.nunique()","27f4d00a":"df=df.rename(columns={'Date.of.Birth': 'Date_of_Birth','Employment.Type': 'Employment_Type', 'PERFORM_CNS.SCORE.DESCRIPTION': 'PERFORM_CNS_SCORE_DESCRIPTION'})\n\ndf.columns","96d36f6b":"now = pd.Timestamp('now')\ndf['Date_of_Birth'] = pd.to_datetime(df['Date_of_Birth'], format='%d-%m-%y')\ndf['Date_of_Birth'] = df['Date_of_Birth'].where(df['Date_of_Birth'] < now, df['Date_of_Birth'] -  np.timedelta64(100, 'Y'))\ndf['Age'] = (now - df['Date_of_Birth']).astype('<m8[Y]')\ndf=df.drop('Date_of_Birth',axis=1)","81ce2f94":"#Creating a function for encoding 2 categories features\ndef two_cat_encoding(df_to_transf):\n    le = LabelEncoder()\n\n    for cols in df_to_transf:\n        if df_to_transf[cols].dtype == 'object':\n            if len(list(df_to_transf[cols].unique())) == 2:\n                le.fit(df_to_transf[cols])\n                df_to_transf[cols] = le.transform(df_to_transf[cols])\n    return df_to_transf\ndf=two_cat_encoding(df)","9afacb7e":"df['PERFORM_CNS_SCORE_DESCRIPTION'].replace(to_replace=['Not Scored: More than 50 active Accounts found', 'Not Scored: No Activity seen on the customer (Inactive)','Not Scored: No Updates available in last 36 months','Not Enough Info available on the customer','Not Scored: Only a Guarantor','Not Scored: Sufficient History Not Available','Not Scored: Not Enough Info available on the customer'], value= 'Not Scored', inplace = True)","417bb0e9":"columns_to_drop = ['UniqueID','MobileNo_Avl_Flag','DisbursalDate','AVERAGE.ACCT.AGE','CREDIT.HISTORY.LENGTH','SEC.OVERDUE.ACCTS']\ndf=df.drop(columns=columns_to_drop)","81c65b47":"df = pd.get_dummies(df)\ndf.columns","df139e87":"df.shape","37b55f79":"df.dtypes.value_counts()","624af9b0":"X =df.drop('loan_default',axis=1)\ny = df['loan_default']  \n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)","a461bd26":"from sklearn.linear_model import LogisticRegression\nlogisticRegr = LogisticRegression(C=1.0, class_weight=None,fit_intercept=True,max_iter=100)\nlogisticRegr.fit(X_train, y_train)","4085f2b0":"#ERROR\nerror = (1 - logisticRegr.score(X_test, y_test))*100\nprint('Score  = ',logisticRegr.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","be64f030":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=300, oob_score=True, random_state=0)\nrf.fit(X_train,y_train)\nerror = (1 - rf.score(X_test, y_test))*100\nprint('Score  = ',rf.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","03ff64fd":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nlda = LDA(n_components=1)  \nX_lda_sklearn = lda.fit_transform(X_train, y_train)\nerror = (1 - lda.score(X_test, y_test))*100\nprint('Erreur: %f' % error, '%')","421f32c8":"from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train,y_train)\nerror = (1 - clf.score(X_test, y_test))*100\nprint('Erreur: %f' % error, '%')","b03e0da7":"from sklearn.metrics import classification_report, confusion_matrix\npredictions = logisticRegr.predict(X_test)\n\nprint(classification_report(y_test,predictions))\nprint('\\n')\nprint(confusion_matrix(y_test,predictions))","088f6181":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(estimator = logisticRegr , \n                         X=X_train, \n                         y=y_train, \n                         cv=3)\nprint('Cross-validation accuracy scores: %s' %(scores))\nprint('CV accuracy: %.3f +\/- %.3f' %(np.mean(scores), np.std(scores)))","0eafc34e":"print('Success rate by model:\\n\\nLogistic Regression:',logisticRegr.score(X_test, y_test)*100,'%','\\n\\nLDA:',lda.score(X_test, y_test)*100,'%','\\n\\nRandom Forest Classifier:',rf.score(X_test, y_test)*100,'%','\\n\\nDecision Tree Classifier:',clf.score(X_test, y_test)*100,'%')","1689f8a8":"# Decision Tree Classifier Implementation","9fb918d9":"Some features have to high or to low values, we want to delete them to help our model on regular values.\n\nWe decided to create a function that takes as parameters the quatile to compare the value to. We multiplie this value with a max value to fix a value limite\n\nWe are targeting columns that have disproportionated values. We are dropping to high value to concentrate more on mid range value.\nCustumers with extreme values (good or bad) are easy to predict, we want to improve our model on average custumer","68903e0f":"---------------------------------------------------------------------------------------------\n#                   Machine Learning Algorithms\n----------------------------------------------------------------------------------------------","aea7bde6":"We want to plot the correlation matrix of the data","8cfb59f0":"An other good way to understant the data is to print the sorted correlation coefficients of the matrix","6cbb5238":"Let's plot the repartition of the target","77e6c0d1":"We plot the distribution of the target","7a827676":"-------------------------------------------------------------------------\nIn this kernel we will analyse and make predictions of this dataset\n\n--------------------------------------------------------------------------","fad00983":"# Linear Discriminant Analysis Implementation","aa117979":"Let's see the percentage of missing value for each column","daa2015e":"We want to know the new shape of the data after the encoding","64630542":"# Cross Validation","d7ffe4ce":"We want to create a function for encoding the two categories variables","39fd777c":"There are two many different value type possible for a missing value so we will merge them into one column to reduce the dimension of the probleme when applying oneHotEncoding (dummies)","acb927df":"We finally apply the oneHotEncoding methode to the features that hasn't been encodeed yet","e2db5292":"To have a clearer view of the data we created a function to print every value for every features in function of the id of the custumer to have a better idea of the shape of the data","aa4f869e":"  # Logistic Regression Implementation","c96ca463":"We convert the column 'Date_of_Birth' into the age of the custumer for more clearancy","f8037b1d":"And we also want to make sure that there are no 'object' type that hasn't been encodeed left:","94dcffaa":"# Feature Engineering","4155956c":"# Label Encoding and One-Hot Encoding","8ca657d2":"# Random Forest Classifier Implementation","9ea7eeef":"We will now drop the features that does not seem to improve our model","3c42cbd2":"#### Spliting the data between training and testing","84fa1290":"We charge the dataset and transform it into a pandas DataFrames","3750f84c":"So there is only one column with missing values: \"Employment Type\"","ddb0c4cc":"We begin by importing the different librairies that will be needed for our analysis","21c1c468":"# Exploratory Data Analysis","280cfeac":"# Vehicle Loan Default Prediction"}}