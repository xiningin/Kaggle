{"cell_type":{"a5bda9c7":"code","4c5df3dd":"code","9dab4fbc":"code","b80f1cbf":"code","ddcee1f1":"code","8ad09948":"code","94d7c691":"code","1e9878a5":"code","b784b807":"code","1bd90d3e":"code","5f5a3298":"code","fcb52446":"code","35655a92":"code","291496d6":"code","393f0b6c":"code","e3c84876":"code","0a4554ba":"code","1496924d":"code","1b7d07da":"code","987f0557":"code","c862de05":"code","6a7de623":"code","ba016a5e":"code","80629e8d":"code","cda0e315":"code","7046cfb6":"code","8d6186ed":"code","ac647212":"code","42088d51":"code","f5a477f9":"code","4815bda7":"code","a2bc8ee8":"code","3457949d":"code","f943be1d":"code","13e96c9c":"code","6ab59806":"code","1e1db482":"code","d08ae672":"code","1a8cea12":"code","93b148a5":"code","d4b4b2c5":"code","df1ecaaa":"code","55427488":"code","d8be01e0":"code","5ad51b1a":"code","67775866":"code","2a03e0d7":"code","a3164012":"code","26a3e4f3":"code","b6a6b304":"code","5cb4945f":"code","25c7ade9":"code","4a562b90":"code","89422af6":"code","c6ca14f3":"code","7d8f71f8":"code","a43fd2e9":"code","7ec0721e":"code","7bb5a193":"code","7f4834c2":"code","b7120e0e":"code","11bcee5b":"code","36334d02":"code","a8ff9b86":"code","475f3d91":"code","597f774c":"code","8018a228":"code","35166cda":"code","5919ec57":"code","a2a9f24d":"code","85d128d2":"code","7d101f1e":"code","3a9a8dc3":"code","fe865156":"markdown","cb8c8546":"markdown","5f1021e9":"markdown","ba870f5b":"markdown","c040051e":"markdown","166418a0":"markdown","453dbdb9":"markdown","30a90ab2":"markdown","a8cb6305":"markdown","9031666e":"markdown","108b455b":"markdown","957fa405":"markdown","daebcf86":"markdown","febc6095":"markdown","3aa65ee1":"markdown","b757a605":"markdown","6af569f1":"markdown","2fe42a94":"markdown","6dc02fe5":"markdown","4bef9e46":"markdown","aac12bdd":"markdown","3d24f9d5":"markdown","dc106bf5":"markdown","abc159e1":"markdown","c7bef67c":"markdown","f234a84e":"markdown","01d51cae":"markdown","c0a6fe9e":"markdown","a256f3e0":"markdown","3726bc3f":"markdown","0a5a15b5":"markdown","ba147d80":"markdown","363de7fd":"markdown","dfe66429":"markdown","c73f2836":"markdown","39e9f817":"markdown","640f466f":"markdown"},"source":{"a5bda9c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c5df3dd":"import matplotlib.pyplot as plt\nimport seaborn as sns","9dab4fbc":"train_data=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","b80f1cbf":"train_data","ddcee1f1":"test_data","8ad09948":"train_data.describe()","94d7c691":"x=train_data.columns","1e9878a5":"#getting the colimns with high NaN values\nnan=[]\nfor i in x:\n    n=train_data[i].isnull().sum()\n    if n>0:\n        nan.append(i)\nnan","b784b807":"less=[]\nhigh=[]\nfor i in x:\n    l=train_data[i].nunique()\n    if l<50:\n        less.append(i)\n    else :\n        high.append(i)\n        ","1bd90d3e":"numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in train_data.columns:\n    if train_data[i].dtype in numeric_dtypes:\n        numeric.append(i)\n_=numeric.pop(0)\n_=numeric.pop(-1)\n# poped out Id,SalePrice  you will understand the reason later\nnumeric","5f5a3298":"category = []\nfor i in x:\n    if train_data[i].dtypes == 'object':\n        category.append(i)\n    \ncategory    ","fcb52446":"plt.figure(1,figsize=(15,7))\nsns.distplot(train_data['SalePrice'])\nplt.xlabel('Sales price')\nplt.ylabel('Frequency')\nplt.title('Sales Price Distribution')\nplt.show()","35655a92":"# Skew and kurt\nprint(\"Skewness: %f\" % train_data['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_data['SalePrice'].kurt())","291496d6":"# log(1+x) transform\ncorrected_price= np.log1p(train_data[\"SalePrice\"])\n#we will be substituting the value at last","393f0b6c":"plt.figure(1,figsize=(15,7))\nsns.distplot(corrected_price)\nplt.xlabel('Sales price')\nplt.ylabel('Frequency')\nplt.title('Sales Price Distribution')\nplt.show()","e3c84876":"zones=train_data[['MSZoning','SalePrice']]\nsns.catplot(x='MSZoning',y='SalePrice',kind='swarm',data=zones)","0a4554ba":"z=zones.groupby(['MSZoning']).mean()\nz.plot(kind='bar')\n_=plt.xticks(rotation=0)","1496924d":"neighbor=train_data[['LandContour','Neighborhood','SalePrice']]\nplt.figure(1,figsize=(25,10))\nsns.swarmplot(x=neighbor['Neighborhood'],y=neighbor['SalePrice'])\n_=plt.xticks(rotation=90)","1b7d07da":"plt.figure(1,figsize=(25,10))\nsns.scatterplot(x=neighbor['Neighborhood'],y=neighbor['SalePrice'],hue=neighbor['LandContour'])\n_=plt.xticks(rotation=90)","987f0557":"bldg = train_data[['BldgType','HouseStyle','OverallQual','OverallCond','SalePrice','MiscVal']]","c862de05":"bldg","6a7de623":"plt.figure(1,figsize=(15,7))\nsns.jointplot(x='OverallQual',y='SalePrice',data=bldg,kind='kde')\nplt.figure(2,figsize=(15,7))\nsns.jointplot(x='OverallCond',y='SalePrice',data=bldg,kind='kde')\nplt.show()","ba016a5e":"bldg=bldg.groupby(['BldgType']).sum()","80629e8d":"bldg","cda0e315":"#bldg['SalePrice'].plot(kind='bar')\nsns.regplot(x=bldg.OverallQual,y=bldg.SalePrice,units=bldg.OverallCond)","7046cfb6":"bldg.plot(kind='hist')","8d6186ed":"garage=train_data[['GarageType','GarageFinish','GarageCars','GarageQual','GarageCond','SalePrice']]\ngarage.head()","ac647212":"g=garage.groupby(['GarageType'])\nga=g.plot(x='GarageFinish',y='SalePrice',alpha=.6)","42088d51":"sns.pointplot(x=garage['SalePrice'],y=garage['GarageType'],hue=garage['GarageFinish'],join=False)\n","f5a477f9":"sns.pointplot(x=garage['GarageType'],y=garage['GarageCars'],hue=garage['GarageFinish'],color='blue',join=False)\nplt.show()","4815bda7":"g=g=garage.groupby(['GarageType']).mean()\ng['GarageCars']=g['GarageCars'].astype(int)\ng['SalePrice']=g['SalePrice'].astype(int)","a2bc8ee8":"sns.regplot(x=g['SalePrice'],y=g['GarageCars'],data=g,scatter=True,fit_reg=True)","3457949d":"sns.regplot(x=g['GarageCars'],y=g['SalePrice'],data=g)","f943be1d":"misc=train_data[['MiscVal','SaleType','SalePrice']]\nmisc.groupby(['SaleType']).mean()","13e96c9c":"sns.lmplot(x='MiscVal',y='SalePrice',hue='SaleType',data=misc)","6ab59806":"sns.catplot(x='MiscVal',y='SalePrice',hue='SaleType',data=misc)\nplt.figure(1,figsize=(55,15))\n_=plt.xticks(rotation=90)","1e1db482":"date=train_data[['MoSold','YrSold','SalePrice','SaleCondition']]","d08ae672":"sns.boxplot(x='YrSold',y='SalePrice',data=date)","1a8cea12":"sns.lmplot(x='MoSold',y='SalePrice',hue='YrSold',data=date)","93b148a5":"sns.pointplot(x='MoSold',y='SalePrice',hue='YrSold',data=date,join=False)","d4b4b2c5":"corr = train_data.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"Greens\", square=True)\n","df1ecaaa":"missing = round(train_data.isnull().mean()*100,2)\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot(kind='bar')\nplt.figure(figsize=(15,7))","55427488":"#separating the label\ntrain = train_data.copy()\ntest=test_data.copy()\nlabel=train_data['SalePrice'].reset_index(drop=True)\ntrain=train.drop(['SalePrice'],axis=1)","d8be01e0":"#combining  the trainand test data\ndata=pd.concat([train,test]).reset_index(drop=True)","5ad51b1a":"#removing the Id column\ntrain_id=train['Id']\ntest_id=test['Id']\ndata=data.drop(['Id'],axis=1)\ndata.reset_index(drop=True)","67775866":"num_nan=[]\nfor i in numeric:\n    for j in nan:\n        if i==j:\n            num_nan.append(j)\n            print(j)\n    ","2a03e0d7":"# imputing the numeric missing values\nfrom sklearn.impute import SimpleImputer\nimputer=SimpleImputer()\n","a3164012":"imp_data=pd.DataFrame(imputer.fit_transform(data[num_nan]))\nimp_data.columns=num_nan\ndata[num_nan]=imp_data","26a3e4f3":"# these are the columns whose missing values are to be added\ncat_nan=[set(nan)-set(num_nan)]\ncat_nan","b6a6b304":"data['Alley'] = data['Alley'].fillna(data[\"Alley\"].mode()[0])\nfor i in ('BsmtCond','BsmtFinType1','BsmtFinType2','BsmtQual','BsmtExposure'):\n    data[i] = data[i].fillna('None')\ndata['Electrical'] = data['Electrical'].fillna('SBrkr')\ndata['MasVnrType']=data['MasVnrType'].fillna(data['MasVnrType'].mode()[0])\ndata['Fence'] = data['Fence'].fillna(data['Fence'].mode()[0])\nfor i in ('GarageFinish','GarageCond','GarageQual','GarageType'):\n    data[i]=data[i].fillna('None')\ndata['FireplaceQu'] = data['FireplaceQu'].fillna('Fire')    \ndata['MiscFeature']=data['MiscFeature'].fillna('Misc')\ndata['PoolQC']=data['PoolQC'].fillna('Pool')\n\n","5cb4945f":"# doing the final check if any mising value left by mistake\ndata.update(data[numeric].fillna(0))\ndata.update(data[category].fillna('None'))","25c7ade9":"# adding columns of average\/good features\ndata['hasallutilities']=1*(data['Utilities'] == 'AllPub')\ndata['average']=1*(data['OverallQual'] == 5 | 6)\ndata['ext_qual']=1*(data['ExterQual'] == 'Gd')\ndata['fencing']=1*(data['Fence'] == 'GdPrv')\n\n#adding column on basis of features\ndata['total_qual'] = data['OverallQual'] + data['OverallCond']\ndata['total_floors_area'] =  data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\ndata['total_sqr'] =  data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\ndata['total_poarch_sf'] = (data['OpenPorchSF'] + data['3SsnPorch'] +data['EnclosedPorch'] + data['ScreenPorch'] +data['WoodDeckSF'])\n\n#adding columns for special features\ndata['hasfireplace'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ndata['haspool'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndata['has2ndfloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasgarage'] = data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n\n# dropping some columns\ndata=data.drop(['PavedDrive','Street','PavedDrive'],axis=1)","4a562b90":"data.head(3)","89422af6":"data=pd.get_dummies(data).reset_index(drop=True)\ndata.head(3)","c6ca14f3":"train = data.iloc[:len(label),:]\ntest = data.iloc[len(label):,:]\ntrain.shape,test.shape","7d8f71f8":"# for models\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\n\n#for cross validation\nfrom sklearn.model_selection import KFold, cross_val_score\n\n#for calculation of error\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\n\n","a43fd2e9":"# RAndom Forest Regressor\nmodel1 = RandomForestRegressor(n_estimators=1000,max_depth=12,min_samples_split=5,min_samples_leaf=5,oob_score=True,random_state=50)\n\n#XGBRegressor\nmodel2 = XGBRegressor(n_estimators=5000,learning_rate=0.01,early_stopping_round=10,max_depth=4,min_child_weight=0,gamma=0.6,verbose=False,random_state=50)\n\n#Light LGB Regressor\nmodel3=  LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=5000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       verbose=-1,\n                       random_state=50)","7ec0721e":"def cv(model,x=train):\n    er = -1*cross_val_score(model,x,label,cv=10,scoring='neg_mean_absolute_error')\n    # root mean square \n    rmse=np.sqrt(er)\n    return rmse","7bb5a193":"#for RandomForest\nscore1=cv(model1)\nRf=[score1.mean(),score1.std()]\nRf","7f4834c2":"score2=cv(model2)\nXg_mean=score2.mean()\nXg_std=score2.std()\nXg=[Xg_mean,Xg_std]\nXg","b7120e0e":"score3=cv(model3)\nLg_mean=score3.mean()\nLg_std=score3.std()\nLg=[Lg_mean,Lg_std]\nLg","11bcee5b":"# what do you understand with this data","36334d02":"model2.fit(train,label)","a8ff9b86":"#predicting our data on test\npred= model2.predict(test)","475f3d91":"#this is our predicted data\npred","597f774c":"plt.plot(pred,test.index,color='blue')\nplt.title(\"This is the Predicted Price\")","8018a228":"plt.plot(train_data['SalePrice'],train_data.index,color='green')\nplt.title(\"This was the trained Price\")","35166cda":"sample=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","5919ec57":"sample","a2a9f24d":"Id= np.array(range(1461,2920))","85d128d2":"Id","7d101f1e":"out1 =pd.DataFrame({\"Id\":Id,\"SalePrice\": pred})\nout1","3a9a8dc3":"out1.to_csv('my_submission.csv', index=False)","fe865156":"**Let's categorize the columns on basis of data types**","cb8c8546":"Hmmmm These are the troublesome \nSo we'll  treat these columns later","5f1021e9":"so these are the columns which have missing value and have numeric data\n> if you don't know how to remove missing values take a look at this : https:\/\/www.kaggle.com\/alexisbcook\/missing-values","ba870f5b":"## 1. Creating diffrent Models\n## 2. Cross Validation of data\n## 3. Selecting the best model","c040051e":"Combining train and test data and separating the label","166418a0":"## Let's check the columns which have less unique values","453dbdb9":"# Hey Folks!\n## Welcome to my kernel","30a90ab2":"# Adding New Features","a8cb6305":"# **EDA**","9031666e":"now we will add the categorial missing values manually","108b455b":"For missing values","957fa405":"# Thank You For Giving Time To This Notebook","daebcf86":"## We are data scientist (thiugh not now) and we deal with real facts . And one real fact is that no one can become an expert with one notebook**\n**In this notebook  I have tried to provide max. information one can gain from a notebbok additional  techniques would have  create confusion , the msin aim for this draft is to get an idea about hoew thing work, so thie output submiited is not much efficient and precise .I would keep adding new version with more advanced stuffs, which will generate a more efficient and precise output.**\n\n## Till then get through with it and accept it as the first draft\n\n> \nTip : Never Submit your first draft in a competition , figure out more ways to improve results","febc6095":"## now we will understand data with the help of visualization techniques","3aa65ee1":"Please upvote if you found it informative in any way","b757a605":"## **Training the model**","6af569f1":"# Encoding Our Data","2fe42a94":"Before  doing it dig deeply into the data description :https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data","6dc02fe5":"## Checking the NaN values","4bef9e46":"Let's get started\nWe wil fill them with our intution by analyzing the data thoroughly","aac12bdd":"Creating models","3d24f9d5":"This kernel is develpoed for beginners which will help to clear many doubts.\nThis kernel will definately provide an idea about  \"How to deal with data\".\n> Before we move further let's understand the flow of the noteebook","dc106bf5":"## Filling the missing values","abc159e1":"Cross validation of these models","c7bef67c":"our data is submitted successfully","f234a84e":"# ** ** Let's First try to understand the data","01d51cae":"## The flow of the notebook goes as\n1.  **Importing the data ** : we will import the data and will try to understand it\n2. **Treatment of sick data** : It includes processing of the data\n3. **Visualization techniques** : We will try to analyze our data and convert our magic numbers into beautiful & meaningfull graphs\n4. **EDA** : We will convert data into efficient trainable form\n4. **Feature Addition** : Adding new features to our dat set\n5. **Training Our Model**\n6. **Submitting our predictiom**","c0a6fe9e":"Check out other encoding techniques :https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables","a256f3e0":"Calculating score of each model","3726bc3f":"Although these graphs might appear as images for you by now, but with practise these graph would be speaking out loud for you","0a5a15b5":"have a look if you don't get the above code :https:\/\/www.kaggle.com\/alexisbcook\/cross-validation","ba147d80":"Our Features Are Based Over Our Intution.\n\n\nWe will  try to create those features which would be directly related to the Sale Price and special features in it\n\nAnd will remove columns which are not useful enough","363de7fd":"# **Note**","dfe66429":"## Spliting back the train and test data","c73f2836":"# **Zero To Hero**","39e9f817":"We will convert these columns with dummies","640f466f":"we need to correct the skewness of the price"}}