{"cell_type":{"3aea662c":"code","ed301473":"code","57b4df9f":"code","b694df2b":"code","ce31b5d7":"code","e211d334":"code","141e06f0":"code","2bf63286":"code","84eebee8":"code","2beefd49":"code","f7a0182f":"code","20c9c808":"code","6c6dd646":"code","4089bd61":"code","92c64158":"markdown","7fe3320b":"markdown","88a139b3":"markdown"},"source":{"3aea662c":"import numpy as np\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport os\n\nN_MEASUREMENTS = 800_000\nN_FISR_SIGNAL_INDEX = 8712\nN_LAST_SIGNAL_INDEX = 29048\nN_TOTAL_SIGNALS = N_LAST_SIGNAL_INDEX - N_FISR_SIGNAL_INDEX","ed301473":"idx = N_FISR_SIGNAL_INDEX + np.arange(N_TOTAL_SIGNALS)\nidx","57b4df9f":"np.random.shuffle(idx) # more fun on each run!","b694df2b":"# the whole set is just too big to be read at once, will use data generator instead\ndef generator(indexes, batch_size):\n    # From the docs: The generator is expected to loop over its data indefinitely.\n    # An epoch finishes when steps_per_epoch (see fit_generator()) batches have been seen by the model.\n    while True:\n        # 3 phases per signal\n        for start_col in range(0, N_TOTAL_SIGNALS, batch_size * 3):\n            # uncomment to see what signals are being loaded right now\n            # print(f'\\nloading signals from {start_col} to {start_col + batch_size * 3}')\n            \n            cols = [str(c) for c in indexes[start_col:start_col + batch_size * 3]]\n            n_signals = len(cols) \/\/ 3   # could be less than batch_size!\n            signals = pq.read_pandas('..\/input\/test.parquet', columns=cols).to_pandas().values\n            # transform data from column-wise to row-wise\n            signals = np.vstack(np.split(signals, n_signals, axis=1))\n            signals = signals.reshape(-1, N_MEASUREMENTS, 3).astype(np.float16) \/ 130. \n            \n            # X == y, cool\n            yield signals, signals # shape=(?, 800_000, 3)","ce31b5d7":"from keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Conv1D, ELU, BatchNormalization, GRU, Conv2DTranspose, Reshape, Activation","e211d334":"# our toy model\ndef make_model(ts=N_MEASUREMENTS,):\n    #\n    encoder = Sequential(\n        [\n            # 400 samples per filter, @ 4MHz sampling rate that should detect 10KHz and below\n            Conv1D(8, 400, strides=80, padding='same', input_shape=(ts, 3)), # (10_000, 8)\n            ELU(),\n            Conv1D(16, 20, strides=2, padding='same'), # (5_000, 16)\n            ELU(),\n            Conv1D(32, 4, strides=2, padding='same'), # (2_500, 32)\n            BatchNormalization(),\n            ELU(),\n\n            # LSTM will hopefully detect some periods\n            GRU(128,  return_sequences=False, recurrent_dropout=0.3),\n            Dense(128),\n            BatchNormalization(),\n            ELU(),\n            \n            # finally, compress our signal to 9 dimentions (3 per phase? who knows..)\n            Dense(9, activation='tanh'),\n        ]\n    )\n    encoder.summary()\n\n    decoder = Sequential(\n        [\n            Dense(128, input_shape=(9,)),\n            ELU(),\n            Dense(ts \/\/ 2000),  # [?, 400]\n            BatchNormalization(),\n            ELU(),\n            Reshape((ts \/\/ 2000, 1, 1)),           # [?, rows, cols, ch]\n            Conv2DTranspose(16, kernel_size= 4, strides=(2,  5), padding='same'),\n            # [?,  800,   5, 16]\n            Conv2DTranspose( 8, kernel_size=16, strides=(2,  2), padding='same'),    \n            # [?, 1600,  10,  8]\n            Conv2DTranspose( 3, kernel_size=16, strides=(5, 10), padding='same'),    \n            # [?, 8000, 100,  3]\n            Reshape((ts, 3)),  # [?, ts, 3]\n            Activation('tanh')\n        ]\n    )\n    decoder.summary()\n\n    inp = Input(shape=(ts, 3))\n    output = decoder(encoder(inp))\n    ae = Model(inputs=inp, outputs=output)\n    ae.summary()\n\n    return encoder, decoder, ae","141e06f0":"encoder, decoder, ae = make_model()\n\nmetric = 'mae'\nloss = 'mse'\noptimizer = 'adam' # RMSprop()\nae.compile(optimizer=optimizer, loss=loss, metrics=[metric])","2bf63286":"# keep this low, as each signal is 800K * 3 samples ( or batch_size * 800_000 * 3 * 2 in bytes )\nbatch_size = 12\n\n# keep this low to get nice history graph\nsteps_per_epoch = 10\n\n# may wary, depending on how logn you are willing to train\nn_epochs = 10\nprint(f'will train on {batch_size * steps_per_epoch * n_epochs} signals')","84eebee8":"history = ae.fit_generator(\n            generator(idx, batch_size),\n            steps_per_epoch=steps_per_epoch,\n            epochs=n_epochs,\n            validation_data=None, # need another generator for this\n            shuffle=False,        # already shuffled\n        )","2beefd49":"import matplotlib.pyplot as plt","f7a0182f":" def plot_history(history):\n    loss = history.history['loss']\n    mae = history.history['mean_absolute_error']\n    #\n    epochs = range(len(loss))\n    #\n    plt.plot(epochs, loss, 'bo', label='loss')\n    plt.plot(epochs, mae, 'b', label='mae')\n    plt.legend()","20c9c808":"# training is hard\nplot_history(history)","6c6dd646":"def plot_signals(x, y_pred):\n    n_samples = N_MEASUREMENTS\n    fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(40, 20), dpi=80, facecolor='w', edgecolor='k', sharey=False)\n    for ph in range(3):\n        ax[0][0].plot(x[:, ph])\n        ax[0][1].plot(x[:, ph][n_samples\/\/2:n_samples\/\/2 + n_samples\/\/16])\n        ax[0][2].plot(x[:, ph][n_samples\/\/2:n_samples\/\/2 + n_samples\/\/64])\n    for ph in range(3):\n        ax[1][0].plot(y_pred[0, :, ph])\n        ax[1][1].plot(y_pred[0, :, ph][n_samples\/\/2:n_samples\/\/2 + n_samples\/\/16])\n        ax[1][2].plot(y_pred[0, :, ph][n_samples\/\/2:n_samples\/\/2 + n_samples\/\/64])        ","4089bd61":"signals, _ = next(generator(idx, 2))\ny_pred = ae.predict(signals[0].reshape(1, -1, 3))\nplot_signals(signals[0], y_pred)","92c64158":"nice, our model was able to encode the source signal into 9-dimentional vector (or just 18 bytes!), and train the decoder to restore a pretty complex shape from it\n\nhow to use this vector for something usefull is another kernel..","7fe3320b":"the fun part, let's plot our predicted signals!","88a139b3":"let's build an [autoencoder](http:\/\/https:\/\/blog.keras.io\/building-autoencoders-in-keras.html) on test data, shall we?"}}