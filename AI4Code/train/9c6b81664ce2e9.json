{"cell_type":{"14730100":"code","c9b549a1":"code","09816044":"code","290fba1a":"code","fc0a6a92":"code","c0f3ba39":"code","9e851e61":"code","2cd0dd21":"code","9bd4ef7e":"code","61183c2e":"code","a0f2a218":"code","d8e6a754":"code","1754b13e":"code","1b2b90c8":"code","17f8f05a":"code","4fa014a5":"code","6c231f73":"code","ed6e3d7c":"code","aaa81aa1":"code","15b1b69c":"code","0f88ff23":"code","4addcfc4":"code","16a38f95":"code","02903f1c":"code","37c70ccd":"code","0ddd2325":"code","f465fb9c":"code","f16efb4d":"code","520dff40":"code","4a181951":"code","f3f121f6":"code","26896b5c":"code","9587acc1":"code","028ae29b":"code","ab8cffcc":"code","ef9d73c0":"code","09741fff":"code","8c3c01d1":"code","44ce1e8b":"code","5ac7f66a":"code","d88e73d2":"code","9128648b":"code","f0f23be5":"code","dba2a4c9":"code","0ed22f1e":"code","389ecb45":"code","2fff12f5":"code","3d457f18":"code","469d3988":"code","23ff511b":"code","f0db5866":"code","0f7a781c":"code","41588afd":"code","f95bca04":"code","3c236454":"code","f9edeb27":"code","a0785328":"code","9f5e8f4f":"code","20f062e5":"code","0a92e700":"code","3ff7be5d":"code","8fcec1f7":"code","8610094d":"code","979f6027":"code","21fa5449":"code","183ee27c":"code","9b1fe12d":"code","da002710":"code","aab7b29e":"code","5c2ebc33":"code","0c0a264b":"code","9137649d":"code","72c04d88":"code","52814fdc":"code","28a43217":"code","ca305f42":"code","a8bd72c7":"code","2bdb1493":"code","558a1068":"code","2477e751":"code","d76b8b15":"code","267592b8":"code","76848f5b":"code","fe81fbcb":"markdown","51d3c620":"markdown","bcc368a1":"markdown","f28c2dc8":"markdown","399fd09a":"markdown","38b2655a":"markdown","91ee2d4b":"markdown","9d3981fc":"markdown","d2830f67":"markdown","1589f2f1":"markdown","ff41a4b1":"markdown","faa2582d":"markdown","011b99fe":"markdown","47b1197d":"markdown","a1c821a9":"markdown","f9406560":"markdown","daaedf0a":"markdown","c231a90a":"markdown","eb1450a4":"markdown","8f3cdd45":"markdown","28ec964c":"markdown","423a878b":"markdown","fdfbae64":"markdown","9e33464d":"markdown","8f260045":"markdown","a65b32ca":"markdown","45d32fed":"markdown","c8c44d34":"markdown","ce18f156":"markdown","4aba17bb":"markdown","1621a81d":"markdown","7809b75a":"markdown","29a46de7":"markdown","828a7318":"markdown","1f80d339":"markdown","de1db3da":"markdown","46b309cb":"markdown","57b2aecb":"markdown","32b115d8":"markdown","d135447d":"markdown","155c1bd5":"markdown","5153fec8":"markdown","c1c3b99d":"markdown","25aeb900":"markdown","f5399984":"markdown","fc4b46f5":"markdown","10857643":"markdown","d076c267":"markdown","d67d4fdd":"markdown"},"source":{"14730100":"import pandas as pd \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport itertools\n\nfrom sklearn import datasets, metrics\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import preprocessing\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import make_classification\n\nfrom sklearn.decomposition import FastICA\nfrom sklearn.metrics import accuracy_score, log_loss\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score","c9b549a1":"\ndata = pd.read_csv('..\/input\/parkinsons-disease-speech-signal-features\/pd_speech_features.csv') \ndata.info()","09816044":"data.head()","290fba1a":"data.describe()","fc0a6a92":"null_values=data.isnull().sum()\r\nnull_values=pd.DataFrame(null_values, columns=['null'])\r\nj=1\r\nsum_tot=len(data)\r\nnull_values['percent']=null_values['null']\/sum_tot\r\nround(null_values*100,3).sort_values('percent', ascending=False)","c0f3ba39":"from sklearn.model_selection import train_test_split\ny = data.loc[:,'class']\nX = data.drop(['class', 'id'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)","9e851e61":"X","2cd0dd21":"from sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)","9bd4ef7e":"sns.set_style('whitegrid')\r\nsns.set_context('paper')\r\nsns.set_palette('GnBu_d')\r\na = sns.catplot(x='class', data=data, kind='count')\r\na.fig.suptitle('Number of Samples in Each Class', y=1.03)\r\na.set(ylabel='Number of Samples', xlabel='Have Parkinson')\r\nplt.show()","61183c2e":"from sklearn.dummy import DummyClassifier\r\n\r\n# setting up testing and training sets\r\ny = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)\r\n\r\n# DummyClassifier to predict only target 0\r\ndummy = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\r\ndummy_pred = dummy.predict(X_test)\r\n\r\n# checking unique labels\r\nprint('Unique predicted labels: ', (np.unique(dummy_pred)))\r\n\r\n# checking accuracy\r\nprint('Test score: ', accuracy_score(y_test, dummy_pred))","a0f2a218":"# Modeling the data as is\r\n# Train model\r\nfrom sklearn.linear_model import LogisticRegression\r\nlr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\r\n \r\n# Predict on training set\r\nlr_pred = lr.predict(X_test)\r\n\r\n# Checking accuracy\r\naccuracy_score(y_test, lr_pred)\r\n","d8e6a754":"from sklearn.utils import resample\r\n\r\n# Separate input features and target\r\ny = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\n# setting up testing and training sets\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\r\n\r\n# concatenate our training data back together\r\nX = pd.concat([X_train, y_train], axis=1)\r\n\r\n# separate minority and majority classes\r\nparkinson = X.loc[X['class'] == 1]\r\nnot_parkinson = X.loc[X['class'] == 0]\r\n\r\n\r\n# upsample minority\r\nfraud_upsampled = resample(not_parkinson,\r\n                          replace=True, # sample with replacement\r\n                          n_samples=len(parkinson), # match number in majority class\r\n                          random_state=27) # reproducible results\r\n\r\n# combine majority and upsampled minority\r\nupsampled = pd.concat([parkinson, fraud_upsampled])\r\ny_train_up = upsampled.loc[:,'class']\r\nX_train_up = upsampled.drop(['class'], axis=1)\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train_up = min_max_scaler.fit_transform(X_train_up)\r\nX_test = min_max_scaler.transform(X_test)\r\nupsampled['class'].value_counts()","1754b13e":"smote = LogisticRegression(solver='liblinear').fit(X_train_up, y_train_up)\r\n\r\nsmote_pred = smote.predict(X_test)\r\nprint(\"--------------------------------------------------\")\r\nprint(\"||==============================================||\")\r\nprint(\"|| Oversample Minority Class Accuracy:=> {:.2f} % ||\".format(accuracy_score(y_test, smote_pred)*100))\r\nprint(\"||==============================================||\")\r\nprint(\"--------------------------------------------------\")","1b2b90c8":"from imblearn.over_sampling import SMOTE\n# \n# Separate input features and target\ny = data.loc[:,'class']\nX = data.drop(['class', 'id'], axis=1)\n\n# setting up testing and trainingsets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n\n\nmin_max_scaler = preprocessing.MinMaxScaler()\n              \nX_train = min_max_scaler.fit_transform(X_train)\nX_test = min_max_scaler.transform(X_test)\n\nsm = SMOTE(sampling_strategy='mirity', random_state=27)\n# X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n\n# oversampled_train = pd.concat([pd.DataFrame(y_train_smote, columns=['class']), pd.Datrame(X_train_smote)], axis=1)\n# oversampled_train['class'].value_counts()\n# oversampled_train oversampled_train","17f8f05a":"# smote = LogisticRegression(solver='liblinear').fit(X_train_smote, y_train_smote)\r\n# \r\nsmote_pred = smote.predict(X_test)\r\n\r\nprint(\"--------------------------------------------------\")\r\nprint(\"||==============================================||\")\r\nprint(\"|| Oversample Minority Class Accuracy:=> {:.2f} % ||\".format(accuracy_score(y_test, smote_pred)*100))\r\nprint(\"||==============================================||\")\r\nprint(\"--------------------------------------------------\")","4fa014a5":"def plot_confusion_matrix(cm, classes,\r\n                          normalize=False,\r\n                          title='Confusion matrix',\r\n                          cmap=plt.cm.Blues):\r\n    \"\"\"\r\n    This function prints and plots the confusion matrix.\r\n    Normalization can be applied by setting `normalize=True`.\r\n    \"\"\"\r\n    plt.figure(figsize = (5,5))\r\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n    plt.title(title)\r\n    plt.colorbar()\r\n    tick_marks = np.arange(len(classes))\r\n    plt.xticks(tick_marks, classes, rotation=90)\r\n    plt.yticks(tick_marks, classes)\r\n    if normalize:\r\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\r\n\r\n    thresh = cm.max() \/ 2.\r\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n        plt.text(j, i, cm[i, j],\r\n                 horizontalalignment=\"center\",\r\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\r\n    plt.tight_layout()\r\n    plt.ylabel('01')\r\n    plt.xlabel('01')","6c231f73":"def plot_roc_curve(y_test, y_pred):\r\n    # calculate the fpr and tpr for all thresholds of the classification\r\n    fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\r\n    roc_auc = metrics.auc(fpr, tpr)\r\n    plt.figure(figsize=(8, 6))\r\n\r\n    # method I: plt\r\n    plt.title('Receiver Operating Characteristic', fontsize=14)\r\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\r\n    plt.legend(loc = 'lower right', fontsize=11)\r\n    plt.plot([0, 1], [0, 1],'r--')\r\n    plt.xlim([-0.005, 1])\r\n    plt.ylim([0, 1.005])\r\n    plt.ylabel('True Positive Rate', fontsize=12)\r\n    plt.xlabel('False Positive Rate', fontsize=12)\r\n    plt.grid(color='r', linestyle='--', linewidth=0.2)\r\n    plt.show()","ed6e3d7c":"X_train = X_train_up\r\ny_train = y_train_up","aaa81aa1":"def center(X):\r\n    newX = X - np.mean(X, axis = 0)\r\n    return newX\r\n\r\ndef standardize(X):\r\n    newX = center(X)\/np.std(X, axis = 0)\r\n    return newX","15b1b69c":"y = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\n\r\nfrom sklearn.feature_selection import VarianceThreshold\r\n\r\n# Create VarianceThreshold object with a variance with a threshold of 0.5\r\n# thresholder = VarianceThreshold(threshold=.5)\r\n\r\n# # Conduct variance thresholding\r\n# X_high_variance = thresholder.fit_transform(X)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\r\n\r\nfrom sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)\r\n\r\nX = min_max_scaler.transform(X)\r\n\r\nplt.style.use('default')\r\nfrom sklearn.metrics import accuracy_score","0f88ff23":"def my_GaussianNB(X_train, y_train, X_test, y_test, X, y):\n    clf = GaussianNB()\n\n    # fitting the classifier\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n\n    y_pred = clf.predict(X_test)\n    y_pred_train = clf.predict(X_train)\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Train Accuracy GaussianNB Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Test Accuracy GaussianNB Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Binary Cross Entropy - GaussianNB Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n    acc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n\n    confusion_mtx = confusion_matrix(y_test, y_pred)\n    print(classification_report(y_test, y_pred, target_names=\"FT\"))\n    plot_confusion_matrix(confusion_mtx, \"FT\")\n\n    plot_roc_curve(y_test, y_pred)\n\nmy_GaussianNB(X_train, y_train, X_test, y_test, X, y)","4addcfc4":"from sklearn.neighbors import NearestCentroid\n\ndef my_NearestCentroid(X_train, y_train, X_test, y_test, X, y):\n    clf = NearestCentroid(metric='euclidean')\n    # fitting the classifier\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n    y_pred_train = clf.predict(X_train)\n\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Train Accuracy Minimum Distance Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Test Accuracy Minimum Distance Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Binary Cross Entropy - Minimum Distance Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n\n    acc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n\n    confusion_mtx = confusion_matrix(y_test, y_pred)\n    print(classification_report(y_test, y_pred, target_names=\"FT\"))\n    plot_confusion_matrix(confusion_mtx, \"FT\")\n\n    plot_roc_curve(y_test, y_pred)\n\nmy_NearestCentroid(X_train, y_train, X_test, y_test, X, y)","16a38f95":"from sklearn.linear_model import LogisticRegression\n\ndef my_LogisticRegression(X_train, y_train, X_test, y_test, X, y):\n    clf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\n    # fitting the classifier\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n    y_pred_train = clf.predict(X_train)\n\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Train Accuracy LogisticRegression Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Test Accuracy LogisticRegression Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Binary Cross Entropy - LogisticRegression Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n\n    acc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n\n    confusion_mtx = confusion_matrix(y_test, y_pred)\n    print(classification_report(y_test, y_pred, target_names=\"FT\"))\n    plot_confusion_matrix(confusion_mtx, \"FT\")\n\n    plot_roc_curve(y_test, y_pred)\n\n\nmy_LogisticRegression(X_train, y_train, X_test, y_test, X, y)","02903f1c":"from sklearn.neighbors import KNeighborsClassifier\n\ndef my_KNN(X_train, y_train, X_test, y_test, X, y):\n    clf = KNeighborsClassifier(n_neighbors=1)\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n    y_pred_train = clf.predict(X_train)\n\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Train Accuracy KNN Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Test Accuracy KNN Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Binary Cross Entropy - KNN Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n\n    acc = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\n    print(\"--------------------------------------------------------\")\n    print(\"||====================================================||\")\n    print(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\n    print(\"||====================================================||\")\n    print(\"--------------------------------------------------------\")\n\n    confusion_mtx = confusion_matrix(y_test, y_pred)\n    print(classification_report(y_test, y_pred, target_names=\"FT\"))\n    plot_confusion_matrix(confusion_mtx, \"FT\")\n\n    plot_roc_curve(y_test, y_pred)\n\nmy_KNN(X_train, y_train, X_test, y_test, X, y)","37c70ccd":"def my_MLP(X_train, y_train, X_test, y_test, X, y):\r\n    hidden_layer_size=300\r\n    max_iteration=30000\r\n    activation_function='relu'\r\n    optimizer='adam'\r\n    early_stopping = True\r\n    ###################################\r\n    mlp_adam = MLPClassifier(hidden_layer_sizes=(hidden_layer_size, 30), max_iter=max_iteration, \r\n                            activation=activation_function, solver=optimizer, \r\n                            learning_rate='adaptive', early_stopping=early_stopping)\r\n    mlp_adam.fit(X_train, y_train)\r\n\r\n    y_pred = mlp_adam.predict(X_test)\r\n\r\n    print(\"======================================================\")\r\n    print(classification_report(y_test, y_pred))\r\n    print(\"======================================================\")\r\n\r\n    y_pred_train = mlp_adam.predict(X_train)\r\n    print(\"-------------------------------------------------------------------------------------------------------------\")\r\n    print(\"||=========================================================================================================||\")\r\n    print(\"|| Train Accuracy [Optimizer:{} - AF:{} - Max Iter:{} - Early Stop:{} - Hidden Layer Size:{}]:=> {:.2f} %\".format(optimizer, \r\n                                                                                activation_function, \r\n                                                                                max_iteration,\r\n                                                                                early_stopping,\r\n                                                                                hidden_layer_size,\r\n                                                                                accuracy_score(y_train, y_pred_train)*100\r\n                                                                                )\r\n    , \"||\")\r\n    print(\"||=========================================================================================================||\")\r\n    print(\"-------------------------------------------------------------------------------------------------------------\")\r\n\r\n    print(\"-------------------------------------------------------------------------------------------------------------\")\r\n    print(\"||=========================================================================================================||\")\r\n    print(\"|| Test Accuracy [Optimizer:{} - AF:{} - Max Iter:{} - Early Stop:{} - Hidden Layer Size:{}]:=> {:.2f} %\".format(optimizer, \r\n                                                                                activation_function, \r\n                                                                                max_iteration,\r\n                                                                                early_stopping,\r\n                                                                                hidden_layer_size,\r\n                                                                                accuracy_score(y_test, y_pred)*100\r\n                                                                                )\r\n    , \"||\")\r\n    print(\"||=========================================================================================================||\")\r\n    print(\"-------------------------------------------------------------------------------------------------------------\")\r\n\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"||====================================================||\")\r\n    print(\"|| Binary Cross Entropy - MLP Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\n    print(\"||====================================================||\")\r\n    print(\"--------------------------------------------------------\")\r\n\r\n    # acc = cross_val_score(mlp_adam, X, y, cv=5, scoring='accuracy')\r\n    # print(\"--------------------------------------------------------\")\r\n    # print(\"||====================================================||\")\r\n    # print(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\r\n    # print(\"||====================================================||\")\r\n    # print(\"--------------------------------------------------------\")\r\n\r\n    confusion_mtx = confusion_matrix(y_test, y_pred)\r\n    print(classification_report(y_test, y_pred, target_names=\"FT\"))\r\n    plot_confusion_matrix(confusion_mtx, \"FT\")\r\n\r\n    # method I: plt\r\n    plot_roc_curve(y_test, y_pred)\r\n\r\nmy_MLP(X_train, y_train, X_test, y_test, X, y)","0ddd2325":"from sklearn import svm\r\n\r\ndef my_SVM(X_train, y_train, X_test, y_test, X, y):\r\n    X = X_train\r\n    y = y_train\r\n\r\n    # class_weight=None\r\n    class_weight='balanced'\r\n\r\n    models = (svm.SVC(kernel='linear', decision_function_shape='ovr', class_weight=class_weight),\r\n            svm.SVC(kernel='linear', decision_function_shape='ovo', class_weight=class_weight),\r\n            svm.SVC(kernel='rbf',class_weight=class_weight, decision_function_shape='ovr'),\r\n            svm.SVC(kernel='poly',class_weight=class_weight, degree=3, decision_function_shape='ovr')\r\n            )\r\n    models_fit = (clf.fit(X, y) for clf in models)\r\n\r\n    # title for the plots\r\n    titles = ('SVM with linear Kernel, One-vs-Rest',\r\n            'SVM with linear Kernel, One-VS-One',\r\n            'SVM with RBF Kernel, One-vs-Rest',\r\n            'SVM with Polynomial (degree 3) Kernel, One-vs-Rest')\r\n\r\n\r\n\r\n    scores = []\r\n    for clf, title in zip(models_fit, titles):\r\n        scores.append(clf.score(X_test, y_test))\r\n\r\n\r\n    print(\"||============================================================================||\")\r\n    print(\"||----------------------------------------------------------------------------||\")\r\n    print('|| Accuracy of SVM with linear Kernel, One-vs-Rest:=> {:.2f} %'.format(scores[0]*100), \"               ||\")\r\n    print(\"||----------------------------------------------------------------------------||\")\r\n    print('|| Accuracy of SVM with linear Kernel, One-vs-One:=> {:.2f} %'.format(scores[1]*100), \"                ||\")\r\n    print(\"|| ---------------------------------------------------------------------------||\")\r\n    print('|| Accuracy of SVM with RBF Kernel, One-vs-Rest:=> {:.2f} %'.format(scores[2]*100), \"                  ||\")\r\n    print(\"||----------------------------------------------------------------------------||\")\r\n    print('|| Accuracy of SVM with polynomial (degree 3) Kernel, One-vs-Rest:=> {:.2f} %'.format(scores[3]*100), \"||\")\r\n    print(\"||============================================================================||\")\r\n    print(\"||----------------------------------------------------------------------------||\")\r\n\r\n    models_fit_mat = (clf.fit(X, y) for clf in models)\r\n\r\n    plt.style.use('default')\r\n    for title, clf in zip(titles, models_fit_mat):\r\n        y_pred = clf.predict(X_test)\r\n        confusion_mtx = confusion_matrix(y_test, y_pred)\r\n        plot_confusion_matrix(confusion_mtx, \"FT\", title=title+\" Confusion Matrix\")\r\n\r\nmy_SVM(X_train, y_train, X_test, y_test, X, y)","f465fb9c":"from sklearn import tree\r\n\r\ndef my_Tree(X_train, y_train, X_test, y_test, X, y):\r\n    clf = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, \r\n                                    min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \r\n                                    max_leaf_nodes=None, min_impurity_decrease=0.0, \r\n                                    class_weight='balanced', presort='deprecated', ccp_alpha=0.0)\r\n    clf.fit(X_train, y_train)\r\n    y_pred = clf.predict(X_test)\r\n    y_pred_train = clf.predict(X_train)\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"||====================================================||\")\r\n    print(\"|| Train Accuracy Decision Tree Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\n    print(\"||====================================================||\")\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"||====================================================||\")\r\n    print(\"|| Test Accuracy Decision Tree Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\n    print(\"||====================================================||\")\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"||====================================================||\")\r\n    print(\"|| Binary Cross Entropy - Decision Tree Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\n    print(\"||====================================================||\")\r\n    print(\"--------------------------------------------------------\")\r\n    acc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"||====================================================||\")\r\n    print(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100, np.std(acc)*100), \" ||\")\r\n    print(\"||====================================================||\")\r\n    print(\"--------------------------------------------------------\")\r\n    confusion_mtx = confusion_matrix(y_test, y_pred)\r\n    print(classification_report(y_test, y_pred, target_names=\"FT\"))\r\n    plot_confusion_matrix(confusion_mtx, \"FT\")\r\n    # method I: plt\r\n    plot_roc_curve(y_test, y_pred)\r\n\r\nmy_Tree(X_train, y_train, X_test, y_test, X, y)","f16efb4d":"def get_distance(x1, x2):\r\n    sum = 0\r\n    for i in range(len(x1)):\r\n        sum += (x1[i] - x2[i]) ** 2\r\n    return np.sqrt(sum)\r\n\r\n\r\ndef kmeans(X, k, max_iters):\r\n  \r\n    centroids = X[np.random.choice(range(len(X)), k, replace=False)]\r\n\r\n    converged = False\r\n    \r\n    current_iter = 0\r\n\r\n    while (not converged) and (current_iter < max_iters):\r\n\r\n        cluster_list = [[] for i in range(len(centroids))]\r\n\r\n        for x in X:  # Go through each data point\r\n            distances_list = []\r\n            for c in centroids:\r\n                distances_list.append(get_distance(c, x))\r\n            cluster_list[int(np.argmin(distances_list))].append(x)\r\n\r\n        cluster_list = list((filter(None, cluster_list)))\r\n\r\n        prev_centroids = centroids.copy()\r\n\r\n        centroids = []\r\n\r\n        for j in range(len(cluster_list)):\r\n            centroids.append(np.mean(cluster_list[j], axis=0))\r\n\r\n        pattern = np.abs(np.sum(prev_centroids) - np.sum(centroids))\r\n\r\n        print('K-MEANS: ', int(pattern))\r\n\r\n        converged = (pattern == 0)\r\n\r\n        current_iter += 1\r\n\r\n    return np.array(centroids), [np.std(x) for x in cluster_list]\r\n\r\n\r\nclass RBF:\r\n    def __init__(self, X, y, tX, ty, num_of_classes,\r\n                 k, std_from_clusters=True):\r\n        self.X = X\r\n        self.y = y\r\n\r\n        self.tX = tX\r\n        self.ty = ty\r\n\r\n        self.number_of_classes = num_of_classes\r\n        self.k = k\r\n        self.std_from_clusters = std_from_clusters\r\n\r\n    def convert_to_one_hot(self, x, num_of_classes):\r\n        arr = np.zeros((len(x), num_of_classes))\r\n        for i in range(len(x)):\r\n            x = np.array(x)\r\n            c = int(x[i])\r\n            arr[i][c] = 1\r\n        return arr\r\n\r\n    def rbf(self, x, c, s):\r\n        distance = get_distance(x, c)\r\n        return 1 \/ np.exp(-distance \/ s ** 2)\r\n\r\n    def rbf_list(self, X, centroids, std_list):\r\n        RBF_list = []\r\n        for x in X:\r\n            RBF_list.append([self.rbf(x, c, s) for (c, s) in zip(centroids, std_list)])\r\n        return np.array(RBF_list)\r\n\r\n\r\n    def fit(self):\r\n        self.centroids, self.std_list = kmeans(self.X, self.k, max_iters=1000)\r\n\r\n        if not self.std_from_clusters:\r\n            dMax = np.max([get_distance(c1, c2) for c1 in self.centroids for c2 in self.centroids])\r\n            self.std_list = np.repeat(dMax \/ np.sqrt(2 * self.k), self.k)\r\n\r\n        RBF_X = self.rbf_list(self.X, self.centroids, self.std_list)\r\n\r\n        self.w = np.linalg.pinv(RBF_X.T @ RBF_X) @ RBF_X.T @ self.convert_to_one_hot(self.y, self.number_of_classes)\r\n\r\n        RBF_list_tst = self.rbf_list(self.tX, self.centroids, self.std_list)\r\n\r\n        self.pred_ty = RBF_list_tst @ self.w\r\n\r\n        self.pred_ty = np.array([np.argmax(x) for x in self.pred_ty])\r\n\r\n        diff = self.pred_ty - self.ty\r\n        print(\"--------------------------------------------------------\")\r\n        print(\"||====================================================||\")\r\n        print('|| Accuracy of RBF Model:=> {:.2f}'.format(len(np.where(diff == 0)[0]) \/ len(diff)*100), '% ||')\r\n        print(\"||====================================================||\")\r\n        print(\"--------------------------------------------------------\")\r\n\r\n\r\nRBF_CLASSIFIER = RBF(X_train, y_train, X_test, y_test, num_of_classes=2,\r\n                     k=30, std_from_clusters=False)\r\n\r\nRBF_CLASSIFIER.fit()","520dff40":"from sklearn.neighbors import KernelDensity\nmodel = KernelDensity(kernel='gaussian', bandwidth=1)\nmodel.fit(X_train)\nlog_dens = model.score_samples(X_test)\nlog_dens","4a181951":"def my_Parzen(X_train, y_train, X_test, y_test, X, y):\r\n    def opt_bayes_parzen(estimators, priors, X_test):\r\n        classes_preds = []\r\n        for estm in range(len(estimators)):\r\n            X_test = np.array(X_test)\r\n            kde = estimators[estm]\r\n            estimation = kde.score_samples(X_test)\r\n            if priors[estm] == 0:\r\n                priors[estm] = 1e-6\r\n            classes_preds.append(estimation + np.log(priors[estm]))\r\n        classes_preds = np.transpose(classes_preds)\r\n        return np.argmax(classes_preds , axis = 1)\r\n\r\n    classes = np.unique(y, return_counts=True)[0]\r\n    estimator_list = []\r\n    priors = []\r\n    acc = []\r\n    y_pred = np.array([])\r\n    y_tests = np.array([])\r\n    for cls in range(len(classes)):\r\n        X_train = np.array(X_train)\r\n        one_class = X_train[np.array(y_train).reshape(len(y_train)) == classes[cls]]\r\n        priors.append(0.5) #.append(len(one_class)\/len(X))\r\n        kde = KernelDensity(bandwidth=.5, algorithm='auto', kernel='gaussian')\r\n        kde.fit(one_class)\r\n        estimator_list.append(kde)\r\n        # print(priors)\r\n        preds = opt_bayes_parzen(estimators=estimator_list, priors=priors, X_test=X_test)\r\n        labels = np.full(len(preds), cls)\r\n        acc.append(accuracy_score(preds, labels))\r\n        y_pred = np.concatenate((y_pred, preds))\r\n        y_tests = np.concatenate((y_tests, labels))\r\n\r\n\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"||====================================================||\")\r\n    print(\"|| Test Accuracy Parzen Model :=> %.2f%%\" % (accuracy_score(y_tests, y_pred)*100), \" ||\")\r\n    print(\"||====================================================||\")\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"||====================================================||\")\r\n    print(\"|| Binary Cross Entropy - Parzen Model :=> {:.2f}\".format(log_loss(y_tests, y_pred)), \" ||\")\r\n    print(\"||====================================================||\")\r\n    print(\"--------------------------------------------------------\")\r\n    confusion_mtx = confusion_matrix(y_tests, y_pred)\r\n    print(classification_report(y_tests, y_pred, target_names=\"FT\"))\r\n    plot_confusion_matrix(confusion_mtx, \"FT\")\r\n    # method I: plt\r\n    plot_roc_curve(y_tests, y_pred)\r\n\r\nmy_Parzen(X_train, y_train, X_test, y_test, X, y)","f3f121f6":"def my_generative_KNN(X_train, y_train, X_test, y_test, X, y):\r\n    def opt_bayes_knn(estimators, priors, X_test):\r\n            classes_preds = []\r\n            proba = estimators.predict_proba(X_test)\r\n            for i in range(len(proba)):\r\n                for j in range(len(proba[i])):\r\n                    if proba[i][j] == 0:\r\n                        proba[i][j] = 1e-6\r\n            for item in range(len(proba[0])):\r\n                if priors[item] == 0:\r\n                    priors[item] = 1e-6\r\n                classes_preds.append(np.log(proba[: , item]) + np.log(priors[item])) \r\n            classes_preds = np.transpose(classes_preds)\r\n            return np.argmax(classes_preds , axis = 1)\r\n\r\n    priors = []\r\n    classes = np.unique(y, return_counts=True)[0]\r\n    one_class1 = X_train[np.array(y_train).reshape(len(y_train)) == classes[0]]\r\n    priors.append(len(one_class1)\/len(X_train))\r\n    one_class2 = X_train[np.array(y_train).reshape(len(y_train)) == classes[1]]\r\n    priors.append(len(one_class2)\/len(X_train))\r\n    knn = KNeighborsClassifier(n_neighbors=1)\r\n    knn.fit(X_train, y_train)\r\n    priors = [0.5, 0.5]\r\n    y_pred = opt_bayes_knn(estimators=knn, priors=priors, X_test=X_test)\r\n\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"||====================================================||\")\r\n    print(\"|| Test Accuracy Generative KNN Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\n    print(\"||====================================================||\")\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"||====================================================||\")\r\n    print(\"|| Binary Cross Entropy - Generative KNN Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\n    print(\"||====================================================||\")\r\n    print(\"--------------------------------------------------------\")\r\n    confusion_mtx = confusion_matrix(y_test, y_pred)\r\n    print(classification_report(y_test, y_pred, target_names=\"FT\"))\r\n    plot_confusion_matrix(confusion_mtx, \"FT\")\r\n    # method I: plt\r\n    plot_roc_curve(y_test, y_pred)\r\n\r\nmy_generative_KNN(X_train, y_train, X_test, y_test, X, y)","26896b5c":"from sklearn.mixture import GaussianMixture\r\nn_components = np.arange(1, 10)\r\nmodels = [GaussianMixture(n, covariance_type='full', random_state=0).fit(X_train)\r\n          for n in n_components]\r\nplt.style.use('default')\r\nplt.plot(n_components, [m.bic(X_train) for m in models], label='BIC')\r\nplt.plot(n_components, [m.aic(X_train) for m in models], label='AIC')\r\nplt.legend(loc='best')\r\nplt.xlabel('n_components');","9587acc1":"from sklearn.mixture import GaussianMixture\r\n\r\n# GMM = GaussianMixture(n_components=2, covariance_type='full', random_state=0)\r\n# GMM.fit(X_train) # Instantiate and fit the model\r\n# print('Converged:', GMM.converged_) # Check if the model has converged\r\n# means = GMM.means_ \r\n# covariances = GMM.covariances_\r\n\r\n# print('\\u03BC = ', means, sep=\"\\n\")\r\n# print('\\u03A3 = ', covariances, sep=\"\\n\")","028ae29b":"from sklearn.mixture import GaussianMixture\r\n\r\ndef my_GMM(X_train, y_train, X_test, y_test, X, y):\r\n    GMM = GaussianMixture(n_components=2, covariance_type='full', random_state=0)\r\n    GMM.fit(X_train) # Instantiate and fit the model\r\n    print('Converged:', GMM.converged_)\r\n    y_pred = GMM.predict(X_test)\r\n    y_pred_train = GMM.predict(X_train)\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"||====================================================||\")\r\n    print(\"|| Train Accuracy GMM Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\n    print(\"||====================================================||\")\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"||====================================================||\")\r\n    print(\"|| Test Accuracy GMM Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\n    print(\"||====================================================||\")\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"||====================================================||\")\r\n    print(\"|| Binary Cross Entropy - GMM Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\n    print(\"||====================================================||\")\r\n    print(\"--------------------------------------------------------\")\r\n    acc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"||====================================================||\")\r\n    print(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100, np.std(acc)*100), \" ||\")\r\n    print(\"||====================================================||\")\r\n    print(\"--------------------------------------------------------\")\r\n    confusion_mtx = confusion_matrix(y_test, y_pred)\r\n    print(classification_report(y_test, y_pred, target_names=\"FT\"))\r\n    plot_confusion_matrix(confusion_mtx, \"FT\")\r\n    # method I: plt\r\n    plot_roc_curve(y_test, y_pred)\r\n\r\nmy_GMM(X_train, y_train, X_test, y_test, X, y)","ab8cffcc":"y = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\r\n\r\nfrom sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)\r\nX = min_max_scaler.fit_transform(X)","ef9d73c0":"from sklearn.decomposition import PCA\r\n\r\npca_parkinson = PCA(n_components=30)\r\nprincipalComponents_parkinson = pca_parkinson.fit_transform(X)\r\n\r\nxs = np.array(range(1,31))\r\nplt.figure()\r\nplt.style.use('fivethirtyeight')\r\nplt.figure(figsize=(8, 6));\r\nplt.xticks(fontsize=12);\r\nplt.yticks(fontsize=12);\r\nplt.plot(xs, pca_parkinson.explained_variance_ratio_);\r\nplt.xlabel('Principal Component', fontsize=13);\r\nplt.ylabel('Covered Variance by this PC', fontsize=13);\r\nplt.title(\"Explained variation per principal component\", fontsize=15);\r\nplt.show();\r\n\r\nprint(\"||============================================================================||\")\r\nprint(\"||----------------------------------------------------------------------------||\")\r\nprint('|| Explained variation per PCs for the first two: {} ||'.\r\n      format(pca_parkinson.explained_variance_ratio_[0:2]))\r\nprint(\"||----------------------------------------------------------------------------||\")\r\nprint(\"||============================================================================||\")","09741fff":"pca_total = PCA(n_components=753)\r\nprincipalComponents_total = pca_total.fit_transform(X)","8c3c01d1":"n_components = 753\r\nxs = np.array(range(1,n_components+1))\r\nplt.figure()\r\nplt.figure(figsize=(8,7))\r\nplt.xticks(fontsize=12)\r\nplt.yticks(fontsize=12)\r\nplt.plot(xs, pca_total.explained_variance_ratio_)\r\nplt.xlabel('Principal Component',fontsize=13)\r\nplt.ylabel('Covered Variance by this PC',fontsize=13)\r\nplt.title(\"Explained variation per principal component\",fontsize=15)\r\nplt.show()\r\nprint('Explained variation per principal component for the first two PCs: {}'.format(pca_total.explained_variance_ratio_[0:2]))","44ce1e8b":"from sklearn.decomposition import PCA\n\ntransformer = PCA(n_components=337, whiten=False, svd_solver='auto')\n\ny = data.loc[:,'class']\nX = data.drop(['class', 'id'], axis=1)\ny = data.loc[:,'class']\nX = data.drop(['class', 'id'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_train = min_max_scaler.fit_transform(X_train)\nX_test = min_max_scaler.transform(X_test)\n\nX_train = transformer.fit_transform(X_train)\n\n# X_train = transformer.transform()\nX_test = transformer.transform(X_test)","5ac7f66a":"from sklearn.neighbors import KNeighborsClassifier\r\nclf = KNeighborsClassifier(n_neighbors=1)\r\nclf.fit(X_train, y_train)\r\n\r\ny_pred = clf.predict(X_test)\r\ny_pred_train = clf.predict(X_train)\r\n\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy KNN Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy KNN Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Binary Cross Entropy - KNN Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nacc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n\r\nplot_roc_curve(y_test, y_pred)","d88e73d2":"hidden_layer_size=300\r\nmax_iteration=3000\r\nactivation_function='relu'\r\noptimizer='adam'\r\nearly_stopping = True\r\n###################################\r\nmlp_adam = MLPClassifier(hidden_layer_sizes=(hidden_layer_size, 30), max_iter=max_iteration, \r\n                         activation=activation_function, solver=optimizer, \r\n                         learning_rate='adaptive', early_stopping=early_stopping)\r\nmlp_adam.fit(X_train, y_train)\r\n\r\ny_pred = mlp_adam.predict(X_test)\r\n\r\nprint(\"======================================================\")\r\nprint(classification_report(y_test, y_pred))\r\nprint(\"======================================================\")\r\n\r\ny_pred_train = mlp_adam.predict(X_train)\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"|| Train Accuracy [Optimizer:{} - AF:{} - Max Iter:{} - Early Stop:{} - Hidden Layer Size:{}]:=> {:.2f} %\".format(optimizer, \r\n                                                                            activation_function, \r\n                                                                            max_iteration,\r\n                                                                            early_stopping,\r\n                                                                            hidden_layer_size,\r\n                                                                            accuracy_score(y_train, y_pred_train)*100\r\n                                                                            )\r\n, \"||\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\n\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"|| Test Accuracy [Optimizer:{} - AF:{} - Max Iter:{} - Early Stop:{} - Hidden Layer Size:{}]:=> {:.2f} %\".format(optimizer, \r\n                                                                            activation_function, \r\n                                                                            max_iteration,\r\n                                                                            early_stopping,\r\n                                                                            hidden_layer_size,\r\n                                                                            accuracy_score(y_test, y_pred)*100\r\n                                                                            )\r\n, \"||\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\n\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Binary Cross Entropy - MLP Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\n# acc = cross_val_score(mlp_adam, X, y, cv=5, scoring='accuracy')\r\n# print(\"--------------------------------------------------------\")\r\n# print(\"||====================================================||\")\r\n# print(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\r\n# print(\"||====================================================||\")\r\n# print(\"--------------------------------------------------------\")\r\n\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n\r\n\r\n\r\n# method I: plt\r\nplot_roc_curve(y_test, y_pred)","9128648b":"from sklearn.linear_model import LogisticRegression\r\nclf = LogisticRegression(random_state=27, max_iter=1000).fit(X_train, y_train)\r\n# fitting the classifier\r\nclf.fit(X_train, y_train)\r\n\r\ny_pred = clf.predict(X_test)\r\ny_pred_train = clf.predict(X_train)\r\n\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy LogisticRegression Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy LogisticRegression Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Binary Cross Entropy - LogisticRegression Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nacc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n\r\nplot_roc_curve(y_test, y_pred)","f0f23be5":"from sklearn import svm\r\n\r\ndef make_meshgrid(x, y, h=.02):\r\n    \"\"\"Create a mesh of points to plot in\r\n\r\n    Parameters\r\n    ----------\r\n    x: data to base x-axis meshgrid on\r\n    y: data to base y-axis meshgrid on\r\n    h: stepsize for meshgrid, optional\r\n\r\n    Returns\r\n    -------\r\n    xx, yy : ndarray\r\n    \"\"\"\r\n    x_min, x_max = x.min() - 1, x.max() + 1\r\n    y_min, y_max = y.min() - 1, y.max() + 1\r\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\r\n                         np.arange(y_min, y_max, h))\r\n    return xx, yy\r\n\r\n\r\ndef plot_contours(ax, clf, xx, yy, **params):\r\n    \"\"\"Plot the decision boundaries for a classifier.\r\n\r\n    Parameters\r\n    ----------\r\n    ax: matplotlib axes object\r\n    clf: a classifier\r\n    xx: meshgrid ndarray\r\n    yy: meshgrid ndarray\r\n    params: dictionary of params to pass to contourf, optional\r\n    \"\"\"\r\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\r\n    Z = Z.reshape(xx.shape)\r\n    out = ax.contourf(xx, yy, Z, **params)\r\n    return out\r\n\r\nX = X_train\r\ny = y_train\r\n\r\n# class_weight=None\r\nclass_weight='balanced'\r\n\r\nmodels = (svm.SVC(kernel='linear', decision_function_shape='ovr', class_weight=class_weight),\r\n          svm.SVC(kernel='linear', decision_function_shape='ovo', class_weight=class_weight),\r\n          svm.SVC(kernel='rbf',class_weight=class_weight, decision_function_shape='ovr'),\r\n          svm.SVC(kernel='poly',class_weight=class_weight, degree=3, decision_function_shape='ovr')\r\n          )\r\nmodels_fit = (clf.fit(X, y) for clf in models)\r\n\r\n# title for the plots\r\ntitles = ('SVM with linear Kernel, One-vs-Rest',\r\n          'SVM with linear Kernel, One-VS-One',\r\n          'SVM with RBF Kernel, One-vs-Rest',\r\n          'SVM with Polynomial (degree 3) Kernel, One-vs-Rest')\r\n\r\n\r\n\r\nscores = []\r\nfor clf, title in zip(models_fit, titles):\r\n    scores.append(clf.score(X_test, y_test))\r\n\r\n\r\nprint(\"||============================================================================||\")\r\nprint(\"||----------------------------------------------------------------------------||\")\r\nprint('|| Accuracy of SVM with linear Kernel, One-vs-Rest:=> {:.2f} %'.format(scores[0]*100), \"               ||\")\r\nprint(\"||----------------------------------------------------------------------------||\")\r\nprint('|| Accuracy of SVM with linear Kernel, One-vs-One:=> {:.2f} %'.format(scores[1]*100), \"                ||\")\r\nprint(\"|| ---------------------------------------------------------------------------||\")\r\nprint('|| Accuracy of SVM with RBF Kernel, One-vs-Rest:=> {:.2f} %'.format(scores[2]*100), \"                  ||\")\r\nprint(\"||----------------------------------------------------------------------------||\")\r\nprint('|| Accuracy of SVM with polynomial (degree 3) Kernel, One-vs-Rest:=> {:.2f} %'.format(scores[3]*100), \"||\")\r\nprint(\"||============================================================================||\")\r\nprint(\"||----------------------------------------------------------------------------||\")\r\n\r\nmodels_fit_mat = (clf.fit(X, y) for clf in models)\r\n\r\nplt.style.use('default')\r\nfor title, clf in zip(titles, models_fit_mat):\r\n    y_pred = clf.predict(X_test)\r\n    plot_roc_curve(y_test, y_pred)\r\n    confusion_mtx = confusion_matrix(y_test, y_pred)\r\n    plot_confusion_matrix(confusion_mtx, \"FT\", title=title+\" Confusion Matrix\")","dba2a4c9":"from sklearn import tree\r\n\r\nclf = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, \r\n                                  min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \r\n                                  max_leaf_nodes=None, min_impurity_decrease=0.0, \r\n                                  class_weight='balanced', presort='deprecated', ccp_alpha=0.0)\r\nclf.fit(X_train, y_train)\r\ny_pred = clf.predict(X_test)\r\ny_pred_train = clf.predict(X_train)\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy Decision Tree Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy Decision Tree Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Binary Cross Entropy - Decision Tree Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nacc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100, np.std(acc)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n# method I: plt\r\nplot_roc_curve(y_test, y_pred)","0ed22f1e":"from sklearn.decomposition import PCA\r\ntransformer = PCA(n_components=337, whiten=True, svd_solver='full')\r\n\r\ny = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)\r\n\r\n\r\ntransformer.fit(X_train)\r\n\r\nX_train = transformer.transform(X_train)\r\nX_test = transformer.transform(X_test)","389ecb45":"from sklearn.neighbors import KNeighborsClassifier\r\nclf = KNeighborsClassifier(n_neighbors=1)\r\nclf.fit(X_train, y_train)\r\n\r\ny_pred = clf.predict(X_test)\r\ny_pred_train = clf.predict(X_train)\r\n\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy KNN Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy KNN Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Binary Cross Entropy - KNN Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nacc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n\r\nplot_roc_curve(y_test, y_pred)","2fff12f5":"from sklearn.linear_model import LogisticRegression\r\nclf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\r\n# fitting the classifier\r\nclf.fit(X_train, y_train)\r\n\r\ny_pred = clf.predict(X_test)\r\ny_pred_train = clf.predict(X_train)\r\n\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy LogisticRegression Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy LogisticRegression Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Binary Cross Entropy - LogisticRegression Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nacc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n\r\nplot_roc_curve(y_test, y_pred)","3d457f18":"hidden_layer_size=300\r\nmax_iteration=30000\r\nactivation_function='relu'\r\noptimizer='adam'\r\nearly_stopping = True\r\n###################################\r\nmlp_adam = MLPClassifier(hidden_layer_sizes=(hidden_layer_size, 30), max_iter=max_iteration, \r\n                         activation=activation_function, solver=optimizer, \r\n                         learning_rate='adaptive', early_stopping=early_stopping)\r\nmlp_adam.fit(X_train, y_train)\r\n\r\ny_pred = mlp_adam.predict(X_test)\r\n\r\nprint(\"======================================================\")\r\nprint(classification_report(y_test, y_pred))\r\nprint(\"======================================================\")\r\n\r\ny_pred_train = mlp_adam.predict(X_train)\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"|| Train Accuracy [Optimizer:{} - AF:{} - Max Iter:{} - Early Stop:{} - Hidden Layer Size:{}]:=> {:.2f} %\".format(optimizer, \r\n                                                                            activation_function, \r\n                                                                            max_iteration,\r\n                                                                            early_stopping,\r\n                                                                            hidden_layer_size,\r\n                                                                            accuracy_score(y_train, y_pred_train)*100\r\n                                                                            )\r\n, \"||\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\n\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"|| Test Accuracy [Optimizer:{} - AF:{} - Max Iter:{} - Early Stop:{} - Hidden Layer Size:{}]:=> {:.2f} %\".format(optimizer, \r\n                                                                            activation_function, \r\n                                                                            max_iteration,\r\n                                                                            early_stopping,\r\n                                                                            hidden_layer_size,\r\n                                                                            accuracy_score(y_test, y_pred)*100\r\n                                                                            )\r\n, \"||\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\n\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Binary Cross Entropy - MLP Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\n# acc = cross_val_score(mlp_adam, X, y, cv=5, scoring='accuracy')\r\n# print(\"--------------------------------------------------------\")\r\n# print(\"||====================================================||\")\r\n# print(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\r\n# print(\"||====================================================||\")\r\n# print(\"--------------------------------------------------------\")\r\n\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n\r\n\r\n\r\n# method I: plt\r\nplot_roc_curve(y_test, y_pred)","469d3988":"from sklearn import svm\r\n\r\ndef make_meshgrid(x, y, h=.02):\r\n    \"\"\"Create a mesh of points to plot in\r\n\r\n    Parameters\r\n    ----------\r\n    x: data to base x-axis meshgrid on\r\n    y: data to base y-axis meshgrid on\r\n    h: stepsize for meshgrid, optional\r\n\r\n    Returns\r\n    -------\r\n    xx, yy : ndarray\r\n    \"\"\"\r\n    x_min, x_max = x.min() - 1, x.max() + 1\r\n    y_min, y_max = y.min() - 1, y.max() + 1\r\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\r\n                         np.arange(y_min, y_max, h))\r\n    return xx, yy\r\n\r\n\r\ndef plot_contours(ax, clf, xx, yy, **params):\r\n    \"\"\"Plot the decision boundaries for a classifier.\r\n\r\n    Parameters\r\n    ----------\r\n    ax: matplotlib axes object\r\n    clf: a classifier\r\n    xx: meshgrid ndarray\r\n    yy: meshgrid ndarray\r\n    params: dictionary of params to pass to contourf, optional\r\n    \"\"\"\r\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\r\n    Z = Z.reshape(xx.shape)\r\n    out = ax.contourf(xx, yy, Z, **params)\r\n    return out\r\n\r\nX = X_train\r\ny = y_train\r\n\r\n# class_weight=None\r\nclass_weight='balanced'\r\n\r\nmodels = (svm.SVC(kernel='linear', decision_function_shape='ovr', class_weight=class_weight),\r\n          svm.SVC(kernel='linear', decision_function_shape='ovo', class_weight=class_weight),\r\n          svm.SVC(kernel='rbf',class_weight=class_weight, decision_function_shape='ovr'),\r\n          svm.SVC(kernel='poly',class_weight=class_weight, degree=3, decision_function_shape='ovr')\r\n          )\r\nmodels_fit = (clf.fit(X, y) for clf in models)\r\n\r\n# title for the plots\r\ntitles = ('SVM with linear Kernel, One-vs-Rest',\r\n          'SVM with linear Kernel, One-VS-One',\r\n          'SVM with RBF Kernel, One-vs-Rest',\r\n          'SVM with Polynomial (degree 3) Kernel, One-vs-Rest')\r\n\r\n\r\n\r\nscores = []\r\nfor clf, title in zip(models_fit, titles):\r\n    scores.append(clf.score(X_test, y_test))\r\n\r\n\r\nprint(\"||============================================================================||\")\r\nprint(\"||----------------------------------------------------------------------------||\")\r\nprint('|| Accuracy of SVM with linear Kernel, One-vs-Rest:=> {:.2f} %'.format(scores[0]*100), \"               ||\")\r\nprint(\"||----------------------------------------------------------------------------||\")\r\nprint('|| Accuracy of SVM with linear Kernel, One-vs-One:=> {:.2f} %'.format(scores[1]*100), \"                ||\")\r\nprint(\"|| ---------------------------------------------------------------------------||\")\r\nprint('|| Accuracy of SVM with RBF Kernel, One-vs-Rest:=> {:.2f} %'.format(scores[2]*100), \"                  ||\")\r\nprint(\"||----------------------------------------------------------------------------||\")\r\nprint('|| Accuracy of SVM with polynomial (degree 3) Kernel, One-vs-Rest:=> {:.2f} %'.format(scores[3]*100), \"||\")\r\nprint(\"||============================================================================||\")\r\nprint(\"||----------------------------------------------------------------------------||\")\r\n\r\nmodels_fit_mat = (clf.fit(X, y) for clf in models)\r\n\r\nplt.style.use('default')\r\nfor title, clf in zip(titles, models_fit_mat):\r\n    y_pred = clf.predict(X_test)\r\n    confusion_mtx = confusion_matrix(y_test, y_pred)\r\n    plot_confusion_matrix(confusion_mtx, \"FT\", title=title+\" Confusion Matrix\")","23ff511b":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\ny = data.loc[:,'class']\nX = data.drop(['class', 'id'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_train = min_max_scaler.fit_transform(X_train)\nX_test = min_max_scaler.transform(X_test)\nX = min_max_scaler.transform(X)\n\nlda = LinearDiscriminantAnalysis(n_components=1)\nlda.fit(X_train, y_train)\n\nX_train = lda.transform(X_train)\nX_test = lda.transform(X_test)","f0db5866":"from sklearn import tree\r\n\r\nclf = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, \r\n                                  min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \r\n                                  max_leaf_nodes=None, min_impurity_decrease=0.0, \r\n                                  class_weight='balanced', presort='deprecated', ccp_alpha=0.0)\r\nclf.fit(X_train, y_train)\r\ny_pred = clf.predict(X_test)\r\ny_pred_train = clf.predict(X_train)\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy Decision Tree Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy Decision Tree Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Binary Cross Entropy - Decision Tree Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nacc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100, np.std(acc)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n# method I: plt\r\nplot_roc_curve(y_test, y_pred)","0f7a781c":"from sklearn.linear_model import LogisticRegression\r\nclf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\r\n# fitting the classifier\r\nclf.fit(X_train, y_train)\r\n\r\ny_pred = clf.predict(X_test)\r\ny_pred_train = clf.predict(X_train)\r\n\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy LogisticRegression Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy LogisticRegression Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Binary Cross Entropy - LogisticRegression Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nacc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n\r\nplot_roc_curve(y_test, y_pred)","41588afd":"from sklearn.neighbors import KNeighborsClassifier\r\nclf = KNeighborsClassifier(n_neighbors=1)\r\nclf.fit(X_train, y_train)\r\n\r\ny_pred = clf.predict(X_test)\r\ny_pred_train = clf.predict(X_train)\r\n\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy KNN Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy KNN Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Binary Cross Entropy - KNN Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nacc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n\r\nplot_roc_curve(y_test, y_pred)","f95bca04":"hidden_layer_size=300\r\nmax_iteration=3000\r\nactivation_function='relu'\r\noptimizer='adam'\r\nearly_stopping = False\r\n###################################\r\nmlp_adam = MLPClassifier(hidden_layer_sizes=(hidden_layer_size, 73), max_iter=max_iteration, \r\n                         activation=activation_function, solver=optimizer, \r\n                         learning_rate='adaptive', early_stopping=early_stopping)\r\nmlp_adam.fit(X_train, y_train)\r\n\r\ny_pred = mlp_adam.predict(X_test)\r\n\r\nplot_confusion_matrix(confusion_matrix(y_test, y_pred), \"FT\")\r\nprint(\"======================================================\")\r\nprint(classification_report(y_test, y_pred))\r\nprint(\"======================================================\")\r\n\r\ny_pred_train = mlp_adam.predict(X_train)\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"|| Train Accuracy [Optimizer:{} - AF:{} - Max Iter:{} - Early Stop:{} - Hidden Layer Size:{}]:=> {:.2f} %\".format(optimizer, \r\n                                                                            activation_function, \r\n                                                                            max_iteration,\r\n                                                                            early_stopping,\r\n                                                                            hidden_layer_size,\r\n                                                                            accuracy_score(y_train, y_pred_train)*100\r\n                                                                            )\r\n, \"||\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\n\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"|| Test Accuracy [Optimizer:{} - AF:{} - Max Iter:{} - Early Stop:{} - Hidden Layer Size:{}]:=> {:.2f} %\".format(optimizer, \r\n                                                                            activation_function, \r\n                                                                            max_iteration,\r\n                                                                            early_stopping,\r\n                                                                            hidden_layer_size,\r\n                                                                            accuracy_score(y_test, y_pred)*100\r\n                                                                            )\r\n, \"||\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\n\r\n# calculate the fpr and tpr for all thresholds of the classification\r\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\r\nroc_auc = metrics.auc(fpr, tpr)\r\nplt.figure(figsize=(8, 6))\r\n\r\n# method I: plt\r\nplt.title('Receiver Operating Characteristic', fontsize=14)\r\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\r\nplt.legend(loc = 'lower right', fontsize=11)\r\nplt.plot([0, 1], [0, 1],'r--')\r\nplt.xlim([-0.05, 1.05])\r\nplt.ylim([0, 1.05])\r\nplt.ylabel('True Positive Rate', fontsize=12)\r\nplt.xlabel('False Positive Rate', fontsize=12)\r\nplt.show()","3c236454":"from sklearn.decomposition import FastICA\ntransformer = FastICA(n_components=50, random_state=8, max_iter=2000)\n\ny = data.loc[:,'class']\nX = data.drop(['class', 'id'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_train = min_max_scaler.fit_transform(X_train)\nX_test = min_max_scaler.transform(X_test)\nX = min_max_scaler.fit_transform(X)\n\ntransformer.fit(X_train)\n\nX_train = transformer.transform(X_train)\nX_test = transformer.transform(X_test)","f9edeb27":"from sklearn.neighbors import KNeighborsClassifier\r\n\r\nmy_KNN(X_train, y_train, X_test, y_test, X, y)","a0785328":"from sklearn.linear_model import LogisticRegression\r\n\r\nmy_LogisticRegression(X_train, y_train, X_test, y_test, X, y)","9f5e8f4f":"my_Tree(X_train, y_train, X_test, y_test, X, y)","20f062e5":"my_MLP(X_train, y_train, X_test, y_test, X, y)","0a92e700":"from sklearn.neighbors import KNeighborsClassifier\r\n# from sklearn.datasets import load_iris\r\n\r\n# iris = load_iris()\r\n# X = iris.data\r\n# y = iris.target\r\nknn = KNeighborsClassifier(n_neighbors=4)","3ff7be5d":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\r\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\r\n\r\n\r\n\r\n# Sequential Forward Selection\r\nsfs = SFS(knn, \r\n          k_features=X_train.shape[1], \r\n          forward=True, \r\n          floating=False, \r\n          scoring='accuracy',\r\n        #   verbose=5,\r\n          cv=4,\r\n          n_jobs=-1)\r\nsfs = sfs.fit(X_train, y_train)\r\n\r\n# print(sfs.k_feature_names_)\r\n\r\nprint('\\nSequential Forward Selection (k=3):')\r\nprint(sfs.k_feature_idx_)\r\nprint('CV Score:')\r\nprint(sfs.k_score_)\r\n\r\nfig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\r\n\r\nplt.ylim([0.0, 1])\r\nplt.title('Sequential Forward Selection (w. StdDev)')\r\nplt.grid()\r\nplt.show()","8fcec1f7":"# Sequential Backward Floating Selection\r\nsbfs = SFS(knn, \r\n           k_features=1, \r\n           forward=False, \r\n           floating=False, #True, \r\n           scoring='accuracy',\r\n           cv=4,\r\n           n_jobs=-1)\r\nsbfs = sbfs.fit(X_train, y_train)\r\n\r\nprint('\\nSequential Backward Floating Selection (k=3):')\r\nprint(sbfs.k_feature_idx_)\r\nprint('CV Score:')\r\nprint(sbfs.k_score_)\r\n\r\nfig1 = plot_sfs(sbfs.get_metric_dict(), kind='std_dev')\r\n\r\nplt.ylim([0.8, 1])\r\nplt.title('Sequential Backward Selection (w. StdDev)')\r\nplt.grid()\r\nplt.show()","8610094d":"y = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\r\n\r\nfrom sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)","979f6027":"# train autoencoder for classification with with compression in the bottleneck layer\r\nfrom sklearn.datasets import make_classification\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Dense, LeakyReLU, BatchNormalization\r\nfrom tensorflow.keras.utils import plot_model\r\nfrom matplotlib import pyplot\r\n# define dataset\r\n# number of input columns\r\nn_inputs = X.shape[1]\r\n# define encoder\r\nvisible = Input(shape=(n_inputs,))\r\n# encoder level 1\r\ne = Dense(n_inputs*2)(visible)\r\ne = BatchNormalization()(e)\r\ne = LeakyReLU()(e)\r\n# encoder level 2\r\ne = Dense(n_inputs)(e)\r\ne = BatchNormalization()(e)\r\ne = LeakyReLU()(e)\r\n# bottleneck\r\nn_bottleneck = round(float(n_inputs) \/ 2.0)\r\nbottleneck = Dense(n_bottleneck)(e)\r\n# define decoder, level 1\r\nd = Dense(n_inputs)(bottleneck)\r\nd = BatchNormalization()(d)\r\nd = LeakyReLU()(d)\r\n# decoder level 2\r\nd = Dense(n_inputs*2)(d)\r\nd = BatchNormalization()(d)\r\nd = LeakyReLU()(d)\r\n# output layer\r\noutput = Dense(n_inputs, activation='linear')(d)\r\n# define autoencoder model\r\nmodel = Model(inputs=visible, outputs=output)\r\n# compile autoencoder model\r\nmodel.compile(optimizer='adam', loss='mse')\r\n# plot the autoencoder\r\nplot_model(model, 'autoencoder_compress.png', show_shapes=True)\r\n# fit the autoencoder model to reconstruct input\r\nhistory = model.fit(X_train, X_train, epochs=333, batch_size=32, verbose=2, validation_data=(X_test,X_test))\r\n# plot loss\r\npyplot.plot(history.history['loss'], label='train')\r\npyplot.plot(history.history['val_loss'], label='test')\r\npyplot.legend()\r\npyplot.grid()\r\npyplot.show()\r\n# define an encoder model (without the decoder)\r\nencoder = Model(inputs=visible, outputs=bottleneck)\r\nplot_model(encoder, 'encoder_compress.png', show_shapes=True)\r\n# save the encoder to file\r\nencoder.save('encoder.h5')","21fa5449":"# plot loss\r\npyplot.plot(history.history['loss'], label='train')\r\npyplot.plot(history.history['val_loss'], label='test')\r\npyplot.legend()\r\npyplot.grid()\r\npyplot.show()\r\n# define an encoder model (without the decoder)\r\nencoder = Model(inputs=visible, outputs=bottleneck)\r\nplot_model(encoder, 'encoder_compress.png', show_shapes=True)\r\n# save the encoder to file\r\n# encoder.save('encoder.h5')","183ee27c":"# evaluate logistic regression on encoded input\r\nfrom tensorflow.keras.models import load_model\r\ny = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\r\n\r\nfrom sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)\r\n\r\n# load the model from file\r\nencoder = load_model('encoder.h5', compile=False)\r\n# encode the train data\r\nX_train_encode = encoder.predict(X_train)\r\n# encode the test data\r\nX_test_encode = encoder.predict(X_test)\r\nX_train = X_train_encode\r\nX_test = X_test_encode\r\n# define the model\r\nfrom sklearn.linear_model import LogisticRegression\r\nclf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\r\n# fitting the classifier\r\nclf.fit(X_train, y_train)\r\n\r\ny_pred = clf.predict(X_test)\r\ny_pred_train = clf.predict(X_train)\r\n\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy LogisticRegression Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy LogisticRegression Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Binary Cross Entropy - LogisticRegression Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nacc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n\r\nplot_roc_curve(y_test, y_pred)","9b1fe12d":"from sklearn.neighbors import KNeighborsClassifier\r\ny = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\r\n\r\nfrom sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)\r\n# load the model from file\r\nencoder = load_model('encoder.h5', compile=False)\r\n# encode the train data\r\nX_train_encode = encoder.predict(X_train)\r\n# encode the test data\r\nX_test_encode = encoder.predict(X_test)\r\nX_train = X_train_encode\r\nX_test = X_test_encode\r\n\r\nclf = KNeighborsClassifier(n_neighbors=1)\r\nclf.fit(X_train, y_train)\r\n\r\ny_pred = clf.predict(X_test)\r\ny_pred_train = clf.predict(X_train)\r\n\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy KNN Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy KNN Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Binary Cross Entropy - KNN Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nacc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n\r\nplot_roc_curve(y_test, y_pred)","da002710":"y = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\r\n\r\nfrom sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)\r\n# load the model from file\r\nencoder = load_model('encoder.h5', compile=False)\r\n# encode the train data\r\nX_train_encode = encoder.predict(X_train)\r\n# encode the test data\r\nX_test_encode = encoder.predict(X_test)\r\nX_train = X_train_encode\r\nX_test = X_test_encode\r\n\r\nhidden_layer_size=300\r\nmax_iteration=30000\r\nactivation_function='relu'\r\noptimizer='adam'\r\nearly_stopping = True\r\n###################################\r\nmlp_adam = MLPClassifier(hidden_layer_sizes=(hidden_layer_size, 30), max_iter=max_iteration, \r\n                         activation=activation_function, solver=optimizer, \r\n                         learning_rate='adaptive', early_stopping=early_stopping)\r\nmlp_adam.fit(X_train, y_train)\r\n\r\ny_pred = mlp_adam.predict(X_test)\r\n\r\nprint(\"======================================================\")\r\nprint(classification_report(y_test, y_pred))\r\nprint(\"======================================================\")\r\n\r\ny_pred_train = mlp_adam.predict(X_train)\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"|| Train Accuracy [Optimizer:{} - AF:{} - Max Iter:{} - Early Stop:{} - Hidden Layer Size:{}]:=> {:.2f} %\".format(optimizer, \r\n                                                                            activation_function, \r\n                                                                            max_iteration,\r\n                                                                            early_stopping,\r\n                                                                            hidden_layer_size,\r\n                                                                            accuracy_score(y_train, y_pred_train)*100\r\n                                                                            )\r\n, \"||\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\n\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"|| Test Accuracy [Optimizer:{} - AF:{} - Max Iter:{} - Early Stop:{} - Hidden Layer Size:{}]:=> {:.2f} %\".format(optimizer, \r\n                                                                            activation_function, \r\n                                                                            max_iteration,\r\n                                                                            early_stopping,\r\n                                                                            hidden_layer_size,\r\n                                                                            accuracy_score(y_test, y_pred)*100\r\n                                                                            )\r\n, \"||\")\r\nprint(\"||=========================================================================================================||\")\r\nprint(\"-------------------------------------------------------------------------------------------------------------\")\r\n\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Binary Cross Entropy - MLP Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\n# acc = cross_val_score(mlp_adam, X, y, cv=5, scoring='accuracy')\r\n# print(\"--------------------------------------------------------\")\r\n# print(\"||====================================================||\")\r\n# print(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100,np.std(acc)*100), \" ||\")\r\n# print(\"||====================================================||\")\r\n# print(\"--------------------------------------------------------\")\r\n\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n\r\n\r\n\r\n# method I: plt\r\nplot_roc_curve(y_test, y_pred)","aab7b29e":"y = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\r\n\r\nfrom sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)\r\n# load the model from file\r\nencoder = load_model('encoder.h5', compile=False)\r\n# encode the train data\r\nX_train_encode = encoder.predict(X_train)\r\n# encode the test data\r\nX_test_encode = encoder.predict(X_test)\r\nX_train = X_train_encode\r\nX_test = X_test_encode\r\nfrom sklearn import tree\r\n\r\nclf = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, \r\n                                  min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \r\n                                  max_leaf_nodes=None, min_impurity_decrease=0.0, \r\n                                  class_weight='balanced', presort='deprecated', ccp_alpha=0.0, random_state=0)\r\nclf.fit(X_train, y_train)\r\ny_pred = clf.predict(X_test)\r\ny_pred_train = clf.predict(X_train)\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy Decision Tree Model :=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy Decision Tree Model :=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Binary Cross Entropy - Decision Tree Model :=> {:.2f}\".format(log_loss(y_test, y_pred)), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nacc = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Cross Entropy Accuracy :=> %.2f%% +- %.2f%%\" %(np.mean(acc)*100, np.std(acc)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n# method I: plt\r\nplot_roc_curve(y_test, y_pred)","5c2ebc33":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X_train, y_train)\ny_pred = clf.predict((X_test))\nprint(\"The accuracy of the model is: %.1f%%\" % (accuracy_score(y_test, y_pred)*100))","0c0a264b":"y = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\r\n\r\nfrom sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)","9137649d":"from sklearn.svm import SVC\r\nfrom sklearn.ensemble import BaggingClassifier\r\nfrom sklearn.datasets import make_classification\r\n\r\nclf = BaggingClassifier(base_estimator=SVC(),\r\n                         n_estimators=10, random_state=0).fit(X_train, y_train)\r\n","72c04d88":"y_pred = clf.predict(X_test)\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Accuracy SVM Model - Using <<Bagging>>:=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")","52814fdc":"from sklearn.ensemble import BaggingClassifier\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\n\r\ny = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\r\n\r\nfrom sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)\r\nX = min_max_scaler.transform(X)\r\n\r\nbagging = BaggingClassifier(KNeighborsClassifier(), max_samples=1.0, max_features=1.0).fit(X_train, y_train)","28a43217":"y_pred = bagging.predict(X_test)\r\ny_pred_train = bagging.predict(X_train)\r\nprint(\"---------------------------------------------------------------\")\r\nprint(\"||===========================================================||\")\r\nprint(\"|| Test Accuracy KNN Model - Using <<Bagging>>:=> %.2f%%\" % (accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||===========================================================||\")\r\nprint(\"|| Train Accuracy KNN Model - Using <<Bagging>>:=> %.2f%%\" % (accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||===========================================================||\")\r\nscores = cross_val_score(clf, X, y, scoring='accuracy', cv=10)\r\nprint(\"|| Cross Entropy Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean()*100, scores.std(), \"Bagging\"), \" ||\")\r\nprint(\"||===========================================================||\")\r\nprint(\"---------------------------------------------------------------\")\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n# method I: plt\r\nplot_roc_curve(y_test, y_pred)","ca305f42":"from sklearn.model_selection import cross_val_score\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.ensemble import AdaBoostClassifier\r\n\r\ny = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\r\n\r\nfrom sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)\r\nX = min_max_scaler.transform(X)\r\nclf = AdaBoostClassifier(n_estimators=300, learning_rate=0.1)\r\nclf.fit(X_train, y_train)\r\ny_pred = clf.predict(X_test)\r\ny_pred_train = clf.predict(X_train)\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy [%s] Model :=> %.2f%%\" % (\"Gradient Boosting\", accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy [%s] Model :=> %.2f%%\" % (\"Gradient Boosting\", accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"|| Cross Entropy Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean()*100, scores.std(), \"Gradient Boosting\"), \" ||\")\r\nprint(\"||====================================================||\")\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n# method I: plt\r\nplot_roc_curve(y_test, y_pred)","a8bd72c7":"from sklearn import datasets\r\nfrom sklearn.model_selection import cross_val_score\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.naive_bayes import GaussianNB\r\nfrom sklearn.ensemble import GradientBoostingClassifier\r\nfrom sklearn.ensemble import VotingClassifier\r\n\r\ny = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\r\n\r\nfrom sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)\r\nX = min_max_scaler.transform(X)\r\n\r\nclf1 = LogisticRegression(random_state=8, max_iter=1000)\r\nclf2 = GradientBoostingClassifier(n_estimators=300, learning_rate=0.3)\r\nclf3 = KNeighborsClassifier(n_neighbors=1)\r\n\r\neclf = VotingClassifier(\r\n     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\r\n     voting='hard')\r\n\r\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'GradientBoosting', 'KNN', 'Ensemble']):\r\n    clf.fit(X_train, y_train)\r\n    y_pred = clf.predict(X_test)\r\n    y_pred_train = clf.predict(X_train)\r\n    print(\"--------------------------------------------------------\")\r\n    print(\"||====================================================||\")\r\n    print(\"|| Test Accuracy [%s] Model :=> %.2f%%\" % (label, accuracy_score(y_test, y_pred)*100), \" ||\")\r\n    print(\"|| Train Accuracy [%s] Model :=> %.2f%%\" % (label, accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\n    # scores = cross_val_score(clf, X, y, scoring='accuracy', cv=10)\r\n    # print(\"|| Cross Entropy Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean()*100, scores.std(), label), \" ||\")\r\n    print(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\n","2bdb1493":"confusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n# method I: plt\r\nplot_roc_curve(y_test, y_pred)","558a1068":"from sklearn.ensemble import GradientBoostingClassifier\r\ny = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\r\n\r\nfrom sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)\r\nX = min_max_scaler.transform(X)\r\n\r\nclf = GradientBoostingClassifier(n_estimators=300, learning_rate=0.3).fit(X_train, y_train)\r\n\r\ny_pred = clf.predict(X_test)\r\ny_pred_train = clf.predict(X_train)\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy [%s] Model :=> %.2f%%\" % (\"Gradient Boosting\", accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy [%s] Model :=> %.2f%%\" % (\"Gradient Boosting\", accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\n\r\nconfusion_mtx = confusion_matrix(y_test, y_pred)\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n# method I: plt\r\nplot_roc_curve(y_test, y_pred)","2477e751":"from sklearn.experimental import enable_hist_gradient_boosting  # noqa\r\nfrom sklearn.ensemble import HistGradientBoostingClassifier\r\nimport numpy as np\r\ny = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\r\n\r\nfrom sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)\r\nX = min_max_scaler.transform(X)\r\n\r\nclf = HistGradientBoostingClassifier(min_samples_leaf=1, learning_rate=0.3).fit(X_train, y_train)\r\n\r\ny_pred = clf.predict(X_test)\r\ny_pred_train = clf.predict(X_train)\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy [%s] Model :=> %.2f%%\" % (\"HistGradientBoosting\", accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy [%s] Model :=> %.2f%%\" % (\"HistGradientBoosting\", accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")","d76b8b15":"from sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.svm import SVC\r\nfrom itertools import product\r\nfrom sklearn.ensemble import VotingClassifier\r\n\r\ny = data.loc[:,'class']\r\nX = data.drop(['class', 'id'], axis=1)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\r\n\r\nfrom sklearn import preprocessing\r\n\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nX_train = min_max_scaler.fit_transform(X_train)\r\nX_test = min_max_scaler.transform(X_test)\r\nX = min_max_scaler.transform(X)\r\n\r\n# Training classifiers\r\nclf1 = LogisticRegression(max_iter=1000)\r\nclf2 = KNeighborsClassifier(n_neighbors=1)\r\nclf3 = SVC(kernel='poly', probability=True, degree=5)\r\neclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],\r\n                         voting='soft', weights=[2, 3, 2])\r\n\r\nclf1 = clf1.fit(X_train, y_train)\r\nclf2 = clf2.fit(X_train, y_train)\r\nclf3 = clf3.fit(X_train, y_train)\r\neclf = eclf.fit(X_train, y_train)","267592b8":"y_pred = eclf.predict(X_test)\r\ny_pred_train = eclf.predict(X_train)\r\nprint(\"--------------------------------------------------------\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Test Accuracy [%s] Model :=> %.2f%%\" % (\"Gradient Boosting\", accuracy_score(y_test, y_pred)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"|| Train Accuracy [%s] Model :=> %.2f%%\" % (\"Gradient Boosting\", accuracy_score(y_train, y_pred_train)*100), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nscores = cross_val_score(eclf, X, y, scoring='accuracy', cv=10)\r\nprint(\"|| Cross Entropy Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean()*100, scores.std(), label), \" ||\")\r\nprint(\"||====================================================||\")\r\nprint(\"--------------------------------------------------------\")\r\nprint(classification_report(y_test, y_pred, target_names=\"FT\"))","76848f5b":"confusion_mtx = confusion_matrix(y_test, y_pred)\r\nplot_confusion_matrix(confusion_mtx, \"FT\")\r\n# method I: plt\r\nplot_roc_curve(y_test, y_pred)","fe81fbcb":"### **<font color=green>1) Naive Bayes Classifier:<\/font>** ###","51d3c620":"### **<font color=green>5) MLP Classifier:<\/font>** ###","bcc368a1":"### **<font color=Green>(PCA) - Without Whitening- SVM:<\/font>** ###","f28c2dc8":"### **<font color=Green>(PCA) - Without Whitening- KNN:<\/font>** ###","399fd09a":"## **<font color=deepskyblue>Important Modules:<\/font>** ##","38b2655a":"### **<font color=Green>(PCA) - Without Whitening:<\/font>** ###","91ee2d4b":"\r\n## **<font color=deepskyblue>Gradient Boosting Classifier:<\/font>** ##","9d3981fc":"###  **<font color=red>2) KNN:<\/font>** ###","d2830f67":"\r\n## **<font color=deepskyblue>Weighted Average Probabilities (Soft Voting):<\/font>** ##","1589f2f1":"### **<font color=deepskyblue>- KNN - Autoencoder:<\/font>** ###\r\n","ff41a4b1":"# **<font color=deepskyblue>- Feature Selection:<\/font>** #","faa2582d":"\r\n### **<font color=green>4) KNeighbors Classifier:<\/font>** ###","011b99fe":"## **<font color=deepskyblue>AdaBoost methods:<\/font>** ##","47b1197d":"## **<font color=deepskyblue>- Sequential Backward Feature Elimination:<\/font>** ##\r\n","a1c821a9":"### **<font color=lightgreen>- Decision Tree (ICA):<\/font>** ###\r\n","f9406560":"## **<font color=deepskyblue>Bagging methods:<\/font>** ##","daaedf0a":"### **<font color=deepskyblue>ROC Curve Plotting Code:<\/font>** ###","c231a90a":"### **<font color=deepskyblue>Confusion Matrix Plotting Code:<\/font>** ###","eb1450a4":"\r\n## **<font color=deepskyblue>Hist Gradient Boosting Classifier:<\/font>** ##","8f3cdd45":"### **<font color=deepskyblue>MLP - LDA:<\/font>** ###\r\n","28ec964c":"###  **<font color=red>1) Parzen Window<\/font>** ###","423a878b":"# **<font color=RED>Testing Different Classifiers:<\/font>** #","fdfbae64":"## **<font color=green>Check for Unbalance Dataset:<\/font>** ##","9e33464d":"\r\n# **<font color=deepskyblue>- Dimension Reduction:<\/font>** #","8f260045":"### **<font color=green>8) RBF Classifier:<\/font>** ###","a65b32ca":"### **<font color=Green>(PCA) - With Whitening:<\/font>** ###","45d32fed":"## **<font color=Deepskyblue>2- Generative Classifier:<\/font>** ##","c8c44d34":"## **<font color=green>- Discriminative Classifiers:<\/font>** ##","ce18f156":"## **<font color=deepskyblue>- Autoencoder:<\/font>** ##\n","4aba17bb":"### **<font color=green>2) Minimum Distance Classifier:<\/font>** ###","1621a81d":"\n### **<font color=green>3) Logistic Regression:<\/font>** ###\n","7809b75a":"### **<font color=deepskyblue>2. Generate synthetic samples:<\/font>** ###","29a46de7":"\r\n## **<font color=deepskyblue>Principle Component Analysis (PCA):<\/font>** ##\r\n","828a7318":"### **<font color=deepskyblue>1. Resampling Techniques \u2014 Oversample minority class:<\/font>** ###","1f80d339":"### **<font color=lightgreen>- KNN (ICA):<\/font>** ###\r\n","de1db3da":"### **<font color=Green>(PCA) - Without Whitening- Logistic Regression:<\/font>** ###","46b309cb":"## **<font color=green>Balance Dataset:<\/font>** ##","57b2aecb":"### **<font color=lightgreen>- Logistic Regression (ICA):<\/font>** ###\r\n","32b115d8":"\r\n## **<font color=deepskyblue>Majority Class Labels (Majority\/Hard Voting):<\/font>** ##","d135447d":"### **<font color=Green>(PCA) - Without Whitening- MLP:<\/font>** ###","155c1bd5":"###  **<font color=red>3) GMM:<\/font>** ###","5153fec8":"## **<font color=deepskyblue>- Independent Component Analysis (ICA):<\/font>** ##\n","c1c3b99d":"### **<font color=deepskyblue>- Logistic Regression - Autoencoder:<\/font>** ###\r\n","25aeb900":"### **<font color=deepskyblue>KNN - LDA:<\/font>** ###\r\n","f5399984":"## **<font color=Orange>Choose Final Dataset for our Model:<\/font>** ##","fc4b46f5":"### **<font color=green>6) SVM Classifier:<\/font>** ###","10857643":"## **<font color=deepskyblue>Linear Discriminant Analysis (LDA):<\/font>** ##\n","d076c267":"# **<font color=deepskyblue>Ensemble methods:<\/font>** #","d67d4fdd":"### **<font color=green>7) Decision Tree Classifier:<\/font>** ###"}}