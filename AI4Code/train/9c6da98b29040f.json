{"cell_type":{"9427bc2f":"code","9b6196ff":"code","d882e39f":"code","f7240daa":"code","88d0e9b6":"code","3343318e":"code","c3af101d":"code","e026c87c":"code","181217df":"code","dffc7e3c":"code","0f6ba6ed":"code","f31665b2":"code","ebbddb2a":"code","d661a2ea":"code","1f4e8dfc":"code","9783e9cb":"code","f126094d":"code","162cd206":"code","aac78d89":"code","8410e0b7":"code","e75ab700":"code","c1d2941d":"code","06a0dd6e":"code","687629b6":"code","cc8068fc":"code","040bf06c":"code","57fe49ba":"code","03c9ac3f":"code","b7d240ea":"code","11d9a4e8":"code","17279542":"code","e56bcf20":"code","5281bf8b":"code","a1575182":"code","bdf89366":"code","6383412f":"code","c791e767":"code","eb5363f6":"code","8c830201":"code","48258e22":"code","61c38a6a":"code","7655667c":"code","8de0bff7":"code","c2f2b2c0":"code","0a66d2f8":"code","f52f96ac":"code","431337d5":"code","68cb4729":"code","df4b14a4":"code","30550053":"code","efd24648":"code","b26cc67f":"code","58e70907":"code","d598ea8e":"code","843ebe15":"code","f2927fe6":"code","93ece15d":"code","4ac0147e":"code","b79f98ee":"code","0fa24599":"code","30a0ba6a":"markdown","1f7f642b":"markdown","d649b95b":"markdown","306f8582":"markdown","6aa154b6":"markdown","776b78a0":"markdown","8d4707ec":"markdown","b94bd2a1":"markdown","68f1fd0b":"markdown","07f756d5":"markdown","59ea5ee9":"markdown","21fcb1f7":"markdown","38f6f0e3":"markdown","287b7983":"markdown","f5d7e7b4":"markdown","f2a1f956":"markdown"},"source":{"9427bc2f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport xgboost as xgb\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer, FunctionTransformer, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nfrom datetime import datetime\nimport pickle","9b6196ff":"df_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv')","d882e39f":"df_train.head(4)","f7240daa":"df_train.describe()","88d0e9b6":"# Count missing value\nlist(zip(df_test.columns, df_train.drop('Survived', axis=1).isna().sum().values, df_test.isna().sum().values))","3343318e":"df_train['Embarked'].value_counts()","c3af101d":"df_train.info()","e026c87c":"df_train_id = df_train['PassengerId']\ndf_test_id = df_test['PassengerId']","181217df":"def create_submission(y_pred, file_name):\n    d = {'PassengerId': df_test_id, 'Survived': y_pred}\n    submission_file = pd.DataFrame(d)\n    submission_file.to_csv(file_name,  sep=',',  index=False)","dffc7e3c":"X = df_train.drop('Survived', axis=1)\ny = df_train.Survived\n\nX_test = df_test.copy(deep=True)","0f6ba6ed":"sns.pairplot(X.drop('PassengerId', axis=1))","f31665b2":"sns.heatmap(X.drop('PassengerId', axis=1).isna(), cbar=False, cmap='viridis', yticklabels='False')","ebbddb2a":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n# Create the correlation matrix\ncorr = X.select_dtypes(include=numerics).astype(float).corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# plot graph\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(10,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\n\nsns.heatmap(corr, linewidths=0.1, vmax=1.0, mask=mask,\n            square=True, cmap=colormap, linecolor='white', annot=True)","d661a2ea":"# Get validation data\nX_train, X_validation, y_train, y_validation = train_test_split(X, y,\n                                                                test_size=0.20, \n                                                                random_state=42)\n\nX_train.shape, X_validation.shape, X_test.shape","1f4e8dfc":"y_validation.shape, y_validation.sum()","9783e9cb":"def substrings_in_string(big_string, substrings):\n    for substring in substrings:\n        if big_string.startswith(substring):\n            return substring\n    return 'NA'","f126094d":"X_all = [X_train, X_validation, X_test, X]\n\nmedian_age = X_train.loc[:,'Age'].median()\nmedian_fare = X_train.loc[:, 'Fare'].median()\ncabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T', 'NA']\n# Specify the boundaries of the bins\nbins_age = [-np.inf, 20, 40, 60, 80, np.inf]\n# Bin labels\nlabels_age = ['Age_1', 'Age_2','Age_3', 'Age_4', 'Age_5']\n\npd.options.mode.chained_assignment = None  # default='warn'\nfor X_ in X_all:\n    # Imputing\n    # Imputing Age by median value\n    X_.loc[:,'Age'].fillna(median_age, inplace=True)\n\n    # Imputing Fare in test set by median value\n    X_.loc[:, 'Fare'].fillna(median_fare, inplace=True)\n\n    # Imputing Embarked and Cabin by 'NA' value\n    X_.loc[:, 'Embarked'].fillna('NA', inplace=True)\n    X_.loc[:, 'Cabin'].fillna('NA', inplace=True)\n    \n    \n    # Add new features\n    # Calculating FamilySize\n    X_.loc[:, 'FamilySize'] = X_['SibSp'] + X_['Parch']\n    \n    # Deck\n    # This is going be very similar, we have a \u2018Cabin\u2019 column not doing much, only 1st class passengers have cabins, \n    # the rest are \u2018Unknown\u2019. A cabin number looks like \u2018C123\u2019. The letter refers to the deck, \n    # and so we\u2019re going to extract these just like the titles.\n    # Turning cabin number into Deck    \n    X_.loc[:, 'Deck'] = X_['Cabin'].map(lambda x: substrings_in_string(str(x), cabin_list))\n    \n    # Age*Class\n    # This is an interaction term, since age and class are both numbers we can just multiply them.\n    X_.loc[:, 'Age*Pclass'] = X_['Age'] * X_['Pclass']\n    \n    # Fare per Person\n    # Here we divide the fare by the number of family members traveling together, \n    X_.loc[:, 'FarePerPerson'] = X_['Fare'] \/ (X_['FamilySize'] + 1)\n    \n    # Specify Age\n    # Bin the continuous variable using these boundaries\n    X_.loc[:, 'Age_name'] = pd.cut(X_['Age'], bins=bins_age, labels=labels_age).astype('object')\n    \n     #\n    X_.loc[:, 'LastName_len'] = X_['Name'].str.split(', ').str[0].str.len()\n    X_.loc[:, 'FirstName_len'] = X_['Name'].str.split(', ').str[1].str.len()\n    \n    #\n    X_.loc[:, 'Family_size_group'] = X_['FamilySize'].map(lambda x: 'f_single' if x == 0\n                                                         else('f_usual' if 4 > x >= 1\n                                                             else('f_big' if 7 > x >= 4\n                                                                 else('f_large'))))\n    \n#     # Is alone\n#     X_['IsAlone'] = 0\n#     X_.loc[X_['FamilySize'] == 0, 'IsAlone'] = 1\n    \n    # Delete unuse columns\n    X_.drop(['PassengerId', 'Ticket', 'Cabin', 'Name'], axis=1, inplace=True)\n\npd.options.mode.chained_assignment = 'warn'   ","162cd206":"imput_median_columns = ['Age', 'Fare']\nimput_median_indices = np.array([(column in imput_median_columns) for column in X_train.columns], dtype = bool)","aac78d89":"X_train.head(4)","8410e0b7":"binary_data_columns = ['SibSp', 'Parch', 'FamilySize']\nbinary_data_indices = np.array([(column in binary_data_columns) for column in X_train.columns], dtype = bool)","e75ab700":"print(binary_data_columns)\nprint(binary_data_indices)","c1d2941d":"# Columns for One-Hot-Encoder\ncategorical_data_columns = ['Pclass', 'Sex','Embarked', 'Deck',  'Age_name', 'Family_size_group'] \ncategorical_data_indices = np.array([(column in categorical_data_columns) for column in X_train.columns], dtype = bool)","06a0dd6e":"print(categorical_data_columns)\nprint(categorical_data_indices)","687629b6":"# Columns for Standardization\nnumeric_data_columns = ['Age', 'LastName_len', 'FirstName_len']\nnumeric_data_indices = np.array([(column in numeric_data_columns) for column in X_train.columns], dtype = bool)","cc8068fc":"print(numeric_data_columns)\nprint(numeric_data_indices)","040bf06c":"# Apply a power transform featurewise to make data more Gaussian-like\nnumeric_data_columns_log = ['Fare', 'Age*Pclass', 'FarePerPerson']\nnumeric_data_indices_log = np.array([(column in numeric_data_columns_log) for column in X_train.columns], dtype = bool)","57fe49ba":"print(numeric_data_columns_log)\nprint(numeric_data_indices_log)","03c9ac3f":"combined_features = FeatureUnion(transformer_list = [\n            # binary\n            ('binary_variables_processing', FunctionTransformer(lambda data: data.iloc[:, binary_data_indices])), \n                    \n            # numeric\n            ('numeric_variables_processing', Pipeline(steps = [\n                ('selecting', FunctionTransformer(lambda data: data.iloc[:, numeric_data_indices])),\n                ('scaling', StandardScaler(with_mean=True))            \n                        ])),\n            # numeric_log\n            ('numeric_variables_log_processing', Pipeline(steps = [\n                ('selecting', FunctionTransformer(lambda data: data.iloc[:, numeric_data_indices_log])),\n                ('scaling', PowerTransformer(standardize=True))            \n                        ])),\n        \n            # categorical\n            ('categorical_variables_processing', Pipeline(steps = [\n                ('selecting', FunctionTransformer(lambda data: data.iloc[:, categorical_data_indices])),\n                ('hot_encoding', OneHotEncoder(handle_unknown = 'ignore'))            \n                        ])),\n        ],\n                                n_jobs = -1)","b7d240ea":"tree_class = DecisionTreeClassifier(random_state=42)","11d9a4e8":"estimator_tree = Pipeline(steps = [\n    ('feature_processing', combined_features),\n    ('classifier', tree_class)\n])","17279542":"%%time\n\ngrid_param = {\n    'classifier__max_depth': np.arange(2, 5, dtype=int),\n    'classifier__min_samples_leaf':  np.arange(2, 5, dtype=int)\n}\n\ngsearch_tree = GridSearchCV(estimator = estimator_tree,\n                              param_grid = grid_param,\n                              scoring='accuracy',\n                              return_train_score=True,\n                              n_jobs=-1,\n                              verbose=2,\n                              cv=5)\n\ngsearch_tree.fit(X_train, y_train)","e56bcf20":"gsearch_tree.best_params_, gsearch_tree.best_score_","5281bf8b":"# Validation\ny_pred_train_tree = gsearch_tree.best_estimator_.predict(X_train)\ny_pred_val_tree = gsearch_tree.best_estimator_.predict(X_validation)\naccuracy_val = accuracy_score(y_validation, y_pred_val_tree)\nprint('accuracy_val: {}'.format(accuracy_val))","a1575182":"# Save submission file\ntime_now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\ny_pred_test_tree = gsearch_tree.best_estimator_.predict(X_test)\ncreate_submission(y_pred_test_tree, 'submissions\/tree_gridSearch_submission_{}.csv'.format(time_now))","bdf89366":"xgb_class = xgb.XGBClassifier(objective='binary:logistic',\n                              eval_metric='logloss',\n                              use_label_encoder=False,\n                              nthread=-1,\n                              seed=42)","6383412f":"estimator_xgb = Pipeline(steps = [\n    ('feature_processing', combined_features),\n    ('classifier', xgb_class)\n])","c791e767":"%%time\ngrid_param = {\n    'classifier__learning_rate': [0.01, 0.1, 0.2, 0.5],\n    'classifier__n_estimators': range(80, 301, 20),\n    'classifier__max_depth': range(3,8),\n    'classifier__min_child_weight': range(2,6),\n    'classifier__learning_rate': np.arange(0.01, 0.505, 0.05),\n    'classifier__gamma': np.arange(0.1, 0.805, 0.1),\n    'classifier__subsample': np.arange(0.7, 1.0, 0.1),\n    'classifier__colsample_bytree': np.arange(0.6, 1.0, 0.1)\n}\n\nrsearch_xgb = RandomizedSearchCV(estimator=estimator_xgb,\n                                 n_iter=50,\n                                 param_distributions=grid_param,\n                                 scoring='accuracy',\n                                 return_train_score=True,\n                                 n_jobs=-1,\n                                 verbose=2,\n                                 random_state=42,\n                                 cv=4)\nrsearch_xgb.fit(X_train, y_train)","eb5363f6":"rsearch_xgb.best_params_, rsearch_xgb.best_score_","8c830201":"# Validation\ny_pred_train_xgb = rsearch_xgb.best_estimator_.predict(X_train)\ny_pred_val_xgb = rsearch_xgb.best_estimator_.predict(X_validation)\naccuracy_val = accuracy_score(y_validation, y_pred_val_xgb)\nprint('accuracy_val: {}'.format(accuracy_val))","48258e22":"# Save submission file\ntime_now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\ny_pred_test_xgb = rsearch_xgb.best_estimator_.predict(X_test)\ncreate_submission(y_pred_test_xgb, 'submissions\/xgb_randomSearch_submission_{}.csv'.format(time_now))","61c38a6a":"logreg = LogisticRegression(random_state=42,\n                            max_iter=200,\n                            n_jobs=-1)","7655667c":"estimator_logreg = Pipeline(steps = [\n    ('feature_processing', combined_features),\n    ('classifier', logreg)\n])","8de0bff7":"%%time\ngrid_param = {\n    'classifier__C': np.logspace(-5, 8, 15)\n}\n\ngsearch_logreg = GridSearchCV(estimator = estimator_logreg,\n                              param_grid = grid_param,\n                              scoring='accuracy',\n                              return_train_score=True,\n                              n_jobs=-1,\n                              verbose=2,\n                              cv=5)\n\ngsearch_logreg.fit(X_train, y_train)","c2f2b2c0":"gsearch_logreg.best_params_, gsearch_logreg.best_score_","0a66d2f8":"# Validation\ny_pred_train_lr = gsearch_logreg.best_estimator_.predict(X_train)\ny_pred_val_lr = gsearch_logreg.best_estimator_.predict(X_validation)\naccuracy_val = accuracy_score(y_validation, y_pred_val_lr)\nprint('accuracy_val: {}'.format(accuracy_val))","f52f96ac":"gsearch_logreg.cv_results_['params'][0], gsearch_logreg.cv_results_['mean_test_score'][0]","431337d5":"# Save submission file\ntime_now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\ny_pred_test_lr = gsearch_logreg.best_estimator_.predict(X_test)\ncreate_submission(y_pred_test_lr, 'submissions\/logreg_gs_submission_{}.csv'.format(time_now))","68cb4729":"svm = SGDClassifier(random_state=42,\n                    n_jobs=-1,\n                    early_stopping=True,\n                    validation_fraction=0.2,\n                    n_iter_no_change=10,\n                    verbose=2)","df4b14a4":"estimator_svm = Pipeline(steps = [\n    ('feature_processing', combined_features),\n    ('classifier', svm)\n])","30550053":"%%time\ngrid_param = {\n    'classifier__alpha': np.logspace(-4, 1, 10),\n    'classifier__loss': ['hinge', 'modified_huber', 'squared_hinge', 'perceptron'] \n}\n\ngsearch_svm = GridSearchCV(estimator = estimator_svm,\n                           param_grid = grid_param,\n                           scoring='accuracy',\n                           return_train_score=True,\n                           n_jobs=-1,\n                           verbose=2,\n                           cv=4)\n\ngsearch_svm.fit(X_train, y_train)","efd24648":"gsearch_svm.best_params_, gsearch_svm.best_score_","b26cc67f":"# Validation\ny_pred_train_svm = gsearch_svm.best_estimator_.predict(X_train)\ny_pred_val_svm = gsearch_svm.best_estimator_.predict(X_validation)\naccuracy_val = accuracy_score(y_validation, y_pred_val_svm)\n\nprint('accuracy_val: {}'.format(accuracy_val))","58e70907":"# Save submission file\ntime_now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\ny_pred_test_svm = gsearch_svm.best_estimator_.predict(X_test)\ncreate_submission(y_pred_test_svm, 'submissions\/svm_gs_submission_{}.csv'.format(time_now))","d598ea8e":"# Save the model to disk\npickle.dump(gsearch_svm.best_estimator_.named_steps['classifier'], \\\n            open('model\/svm_grid_search_{}.sav'.format(time_now), 'wb'))","843ebe15":"feature_important = rsearch_xgb.best_estimator_.named_steps['classifier'].get_booster().get_score(importance_type='weight')\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\nprint(rsearch_xgb.best_estimator_.named_steps['classifier'].get_booster().feature_names)\n\nprint(X_train.columns)","f2927fe6":"feat_imp = pd.Series(feature_important).sort_values(ascending=False)\nfeat_imp.plot(kind='bar', title='Feature Importances')\nplt.ylabel('Feature Importance Score')","93ece15d":"%%time\n\n# Define the list classifiers\nclassifiers=[('dt', gsearch_tree.best_estimator_), \n             ('xgb', rsearch_xgb.best_estimator_), \n             ('lr', gsearch_logreg.best_estimator_), \n             ('svm', gsearch_svm.best_estimator_),]\n\n# Instantiate a VotingClassifier vc\nvc = VotingClassifier(estimators=classifiers, voting='hard', n_jobs=-1) \n\n# Fit vc to the training set\nvc.fit(X_train, y_train)","4ac0147e":"# Validation\ny_pred_train_vc = vc.predict(X_train)\ny_pred_val_vc = vc.predict(X_validation)\naccuracy_val = accuracy_score(y_validation, y_pred_val_vc)\n\nprint('accuracy_val: {}'.format(accuracy_val))","b79f98ee":"# Log info\nlogger.info('VotingClassifier')\nlogger.warning('accuracy_val: {}'.format(accuracy_val))","0fa24599":"# Save submission file\ntime_now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\ny_pred_test_vc = vc.predict(X_test)\ncreate_submission(y_pred_test_vc, 'submissions\/vc_submission_{}.csv'.format(time_now))","30a0ba6a":"#### Logistic regression classifier","1f7f642b":"### Models","d649b95b":"#### Separate features and label variable","306f8582":"### Plot features importances from XGBoost","6aa154b6":"# Ensemble of classifiers","776b78a0":"### Split into train and validation","8d4707ec":"#### DecisionTreeModel","b94bd2a1":"#### Pearson Correlation Heatmap","68f1fd0b":"##### Load datasets","07f756d5":"##### Multiple feature extraction","59ea5ee9":"#### SVM","21fcb1f7":"### Plot graphics","38f6f0e3":"#### XGB Classifier","287b7983":"### Features engineering","f5d7e7b4":"### Select columns for transformation","f2a1f956":"#### Missing value"}}