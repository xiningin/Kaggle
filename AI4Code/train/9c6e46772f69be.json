{"cell_type":{"dae3bff0":"code","ce338b9b":"code","fe31f4b2":"code","b175c55d":"code","fd42fa2a":"code","db479da2":"code","1ecfe7b6":"code","f23f9eb1":"code","23695c56":"code","468f2279":"code","86c28e63":"code","2549c0b2":"code","f28272cd":"code","7338ecff":"code","b0325172":"code","6e7dc4e4":"code","26e8d5cb":"code","1d780ce0":"code","aad74771":"code","4ceb20f1":"code","1b4c786f":"code","be74c7fc":"code","a5e2b553":"code","a71f83bd":"code","5dc6406a":"code","a40ccbed":"code","412ef2c6":"code","a737c073":"code","47f935b2":"code","f3a3d5c3":"code","b8634675":"code","f4a198ca":"code","bcd18f13":"code","ed4e92b7":"code","22ed548b":"code","c979f913":"code","e75c1e8c":"code","7ad79925":"code","5e547a2c":"markdown","9d02734f":"markdown","f85aee8c":"markdown","aa5e0a9f":"markdown","74107518":"markdown","f0fc1a92":"markdown","dac977c7":"markdown","5b37f9ad":"markdown","e3a94a59":"markdown","0f52690b":"markdown","75baade8":"markdown","fb5fbf62":"markdown","9cb8a319":"markdown"},"source":{"dae3bff0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ce338b9b":"data = pd.read_csv(\"..\/input\/auto_clean.csv\")\ndata.head(20)","fe31f4b2":"data.isnull().values.any() #\u00a0So we have missing value\n","b175c55d":"data.info()","fd42fa2a":"data[\"stroke\"].value_counts(dropna = False) #\u00a0That shows us we have 4 missing value.","db479da2":"avg = data[\"stroke\"].mean() # we can fill the Nan values with the average\navg","1ecfe7b6":"data[\"stroke\"].fillna(avg, inplace = True)\ndata[\"stroke\"].value_counts(dropna = False)\n","f23f9eb1":"data[\"horsepower-binned\"].value_counts(dropna = False)\n\navg_list = []\nfor each in data[\"horsepower-binned\"]:\n    if each == \"Low\":\n        avg_list.append(1)\n    elif each == \"Medium\":\n        avg_list.append(2)\n    else:\n        avg_list.append(3)\nsum = 0\ncount = 0\nfor num in avg_list:\n    sum = sum +num\n    count += 1\n\navg = sum\/count\navg","23695c56":"data[\"horsepower-binned\"].fillna(\"Medium\", inplace = True)\ndata.isnull().values.any() #\u00a0We get false, so now our data has not any missin value.","468f2279":"data.columns = data.columns.str.replace(\"-\",\"_\")\n\ngas_or_diesel = [\"diesel\"if each == 0 else \"gas\" for each in data.gas]\ndata[\"gas_or_diesel\"] = gas_or_diesel","86c28e63":"data[\"make\"].unique()","2549c0b2":"labels = data[\"make\"].unique()\nfig = {\n  \"data\": [\n    {\n      \"values\": data[\"make\"].value_counts(),\n      \"labels\": labels,\n      \"domain\": {\"x\": [0, .5]},\n      \"hole\": .3,\n      \"type\": \"pie\"\n    },],\n  \"layout\": {\n        \"title\":\"Distribution of Car makers\",\n        \"annotations\": [\n            { \"font\": { \"size\": 20},\n              \"showarrow\": False,\n              \"text\": \"Pie Chart\",\n                \"x\": 0.20,\n                \"y\": 1\n            },\n        ]\n    }\n}\niplot(fig)","f28272cd":"ax = sns.countplot(data.gas_or_diesel, label = \"Counts\")\nGas, Diesel = data.gas_or_diesel.value_counts()\nprint(\"Number of gas user car : {}\".format(Gas))\nprint(\"Number of diesel user car : {}\".format(Diesel))","7338ecff":"\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'gas_or_diesel', y = 'horsepower', data = data)\n","b0325172":"fig = plt.figure (figsize = (10,6))\nsns.barplot(x = \"gas_or_diesel\", y = \"city_L\/100km\" ,data = data )","6e7dc4e4":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = \"gas_or_diesel\" , y = \"price\", data = data)","26e8d5cb":"fig = plt.figure (figsize = (10,6))\nsns.barplot(x = \"gas_or_diesel\", y = \"stroke\",data = data)","1d780ce0":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = \"gas_or_diesel\", y = \"engine_size\", data = data)","aad74771":"y = data.gas_or_diesel\nx= data[[\"normalized_losses\",\"wheel_base\",\"length\",\n        \"width\",\"height\",\"curb_weight\",\"engine_size\",\n        \"bore\",\"stroke\",\"horsepower\",\"peak_rpm\",\"city_mpg\",\n        \"highway_mpg\",\"price\",\"city_L\/100km\"]]\n\nx_norm = (x- x.min()) \/ (x.max()- x.min())\n\nnew_data = pd.concat([y,x_norm], axis = 1)\n\nnew_data = pd.melt(new_data,\n                  id_vars = \"gas_or_diesel\",\n                  var_name = \"features\",\n                  value_name = \"values\")\nsns.violinplot(x = \"features\", y = \"values\", hue = \"gas_or_diesel\", data= new_data,\n              split=True, inner = \"quart\")\nplt.xticks(rotation = 90)","4ceb20f1":"plt.figure(figsize = (10,10))\nsns.boxplot(x = \"features\", y = \"values\", hue = \"gas_or_diesel\", data = new_data)\nplt.xticks(rotation = 90)","1b4c786f":"sns.set(style =\"darkgrid\",color_codes = True)\n\na = sns.jointplot(x_norm.loc[:,\"width\"],x_norm.loc[:,\"height\"],data = x_norm,\n                 kind=\"reg\", height=8,color=\"#ce1414\")\na.annotate(stats.pearsonr)\nplt.show()","be74c7fc":"sns.set(style=\"darkgrid\", color_codes = True)\na = sns.jointplot(x_norm.loc[:,\"engine_size\"],x_norm.loc[:,\"horsepower\"],\n             data = x_norm, kind =\"reg\", height = 8, color =\"#ce1414\")\na.annotate(stats.pearsonr)\nplt.show()","a5e2b553":"sns.set(style =\"darkgrid\", color_codes = True)\n\na = sns.jointplot(x_norm.loc[:,\"city_L\/100km\"], x_norm.loc[:,\"horsepower\"],\n                  data = x_norm, kind =\"reg\", height = 8, color = \"#ce1414\")\n\na.annotate(stats.pearsonr)\nplt.show()","a71f83bd":"sns.set(style=\"darkgrid\", color_codes = True)\na = sns.jointplot(x_norm.loc[:,\"engine_size\"], x_norm.loc[:,\"peak_rpm\"],\n                 data = x_norm, kind = \"reg\", height = 8 , color = \"#ce1414\")\na.annotate(stats.pearsonr)\nplt.show()","5dc6406a":"sns.set(style=\"darkgrid\", color_codes = True)\na = sns.jointplot(x_norm.loc[:,\"stroke\"],x_norm.loc[:,\"horsepower\"],data = x_norm,\n                 kind = \"reg\", height = 8 , color = \"#ce1414\")\na.annotate(stats.pearsonr)\nplt.show()","a40ccbed":"f,ax = plt.subplots(figsize = (18,18))\nsns.heatmap(x_norm.corr(),annot = True)\n","412ef2c6":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size = 0.3,\n                                                    random_state = 42)\n","a737c073":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 100, random_state = 42)\n\nrf.fit(x_train,y_train)\n\nscore = rf.score(x_test,y_test)\n\nprint(\"The score of the RandomForestClassifier is {}\".format(score))","47f935b2":"y_pred = rf.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true,y_pred)\n\nf,ax = plt.subplots(figsize = (5,5))\n\nsns.heatmap(cm, annot = True)","f3a3d5c3":"from sklearn.metrics import classification_report\ntarget_names = [\"class 0\",\"class 1\"]\n\nprint(classification_report(y_true, y_pred, target_names=target_names))","b8634675":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nscore = dt.score(x_test, y_test)\n\nprint(\"Score of the Decision Three Classifier is {}\".format(score))","f4a198ca":"y_pred = dt.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true,y_pred)\n\nf,ax = plt.subplots(figsize = (5,5))\n\nsns.heatmap(cm, annot = True)","bcd18f13":"\nprint(classification_report(y_true, y_pred, target_names=target_names))","ed4e92b7":"from sklearn.svm import SVC\nsvm = SVC(random_state = 42)\nsvm.fit(x_train,y_train)\nscore = svm.score(x_test,y_test)\nprint(\"Score of the Support Vector Machine is {}\".format(score))","22ed548b":"y_pred = svm.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true,y_pred)\n\nf,ax = plt.subplots(figsize = (5,5))\n\nsns.heatmap(cm, annot = True)","c979f913":"\nprint(classification_report(y_true, y_pred, target_names=target_names))","e75c1e8c":"\ncluster_data = x_norm[[\"engine_size\", \"horsepower\"]]\n\n\nfrom sklearn.cluster import KMeans\nwcss = []\n\nfor k in range (1,15):\n    kmeans = KMeans(n_clusters = k)\n    kmeans.fit(cluster_data)\n    wcss.append(kmeans.inertia_)\n    \nplt.plot(range(1,15), wcss)\nplt.show()","7ad79925":"kmeans2 = KMeans(n_clusters =3 )\nclusters = kmeans2.fit_predict(cluster_data)\n\ncluster_data[\"label\"] = clusters\n\nplt.scatter(cluster_data.engine_size[cluster_data.label == 0],\n            cluster_data.horsepower[cluster_data.label == 0],\n            color=\"red\")\nplt.scatter(cluster_data.engine_size[cluster_data.label == 1],\n            cluster_data.horsepower[cluster_data.label == 1],\n            color=\"purple\")\nplt.scatter(cluster_data.engine_size[cluster_data.label == 2],\n            cluster_data.horsepower[cluster_data.label == 2],\n            color=\"yellow\")\n\nplt.scatter(kmeans2.cluster_centers_[:,0],\n            kmeans2.cluster_centers_[:,1])\n\nplt.show()\n\n\n","5e547a2c":"Thank you for reading this kernell.","9d02734f":"****Support Vector Machine Algorithm","f85aee8c":"Bottom two graphs show us the all features comparision between each other, so we can understand that are two features are positive correlated or negative correlated between each other.","aa5e0a9f":"Heat map is also important tool for ML.","74107518":"****Decision Tree Classifier","f0fc1a92":"The upper graph demonstrates us that which k value should we pick, k = 3 and k = 4 are a good option for us to get more accurate value accordin to Elbow rule.","dac977c7":"The reason of giving numbers to the list is, trying to find best average value for NaN.\nSo the result is 1.54 that means we can write Medium instead of Nan","5b37f9ad":"**** K-Means Clustering between engine size and horsepower","e3a94a59":"First we start with a pie chart to look our car makers and their distributions.","0f52690b":"We also need to know the relations between parameters, in the below you can find the relationships.","75baade8":"Then we count diesel user and gas user car numbers.","fb5fbf62":"****Random Forest Classifier","9cb8a319":"Also , if we want to go deep, we can use jointplot and see the pearsonr value. Which shows the correlation."}}