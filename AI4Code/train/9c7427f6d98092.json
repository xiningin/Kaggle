{"cell_type":{"1643d9d5":"code","dea6bd0f":"code","8811caa2":"code","9008c2a6":"code","be4701f2":"code","57027271":"code","427896dd":"code","a1e8c92a":"code","82ec3fe6":"code","83bb9385":"code","401578b5":"code","8ef065d2":"code","135ad2a7":"code","72c2391c":"code","b8329b06":"code","e8f81dc0":"code","7d2d887c":"code","bed5af0b":"code","0480d234":"code","6f1795ae":"code","d5d3bc67":"code","7c31dff3":"code","fbc5b66b":"code","fab47f87":"code","15282b5a":"code","d0a6db03":"code","6251df99":"code","7ab6dc4a":"code","dcf85827":"code","4668e573":"code","46cca32d":"code","743c47bf":"code","1aee4605":"code","e67e8d49":"code","eec5a324":"code","4ed4fa85":"code","e67e199e":"code","d8deea7e":"code","69abfe73":"code","b8aad05a":"code","10d80d34":"code","41c45ef8":"code","63dd5e6a":"code","e153c105":"code","947ce2c4":"code","4bb39927":"code","53cae17b":"code","b470a003":"code","7e2f9150":"code","bb648fd3":"markdown","d4e1bb1d":"markdown","24741dc6":"markdown","1e1a8e82":"markdown","2b03a378":"markdown","fc70868b":"markdown","8580e366":"markdown","5a933a2a":"markdown","8ca81f35":"markdown","46bc62f4":"markdown","3045b477":"markdown","167c3c93":"markdown","14de3ed6":"markdown","35df49dd":"markdown","d4ea3f15":"markdown","5cf29c58":"markdown","69e1c235":"markdown","93098d85":"markdown","c648c955":"markdown","81df5537":"markdown","7d0f557f":"markdown","fc7b5de6":"markdown","cf11d200":"markdown","0e9e41c4":"markdown","069303a7":"markdown","33fab680":"markdown"},"source":{"1643d9d5":"import numpy as np #Linear Algebra\nimport pandas as pd #Importing and manipulating datasets\n\n#String manipulation and search\nimport string\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom statistics import mean\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk import word_tokenize\n\n#Library to show progress bar for specific operations\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')","dea6bd0f":"reviews = pd.read_csv('..\/input\/reviews\/Reviews.csv', header = 0)","8811caa2":"reviews.head(2)","9008c2a6":"#Selecting only the relevant columns\nbaseData = reviews[['Text','Score']]","be4701f2":"#Creating a function to bucket score values based on a flag\ndef bucket(flag,x):\n    bucket=-1\n    if ((flag==0) & (x<3)):\n        bucket=0 #Negative\n    elif ((flag==0)&(x>3)):\n        bucket=1 #Positive\n    elif ((flag==1)&(x<3)):\n        bucket=0 #Negative\n    elif ((flag==1)&(x==3)):\n        bucket=1 #Neutral\n    elif ((flag==1)&(x>3)):\n        bucket=2 #Positive\n    return bucket","57027271":"# Creating Positive and Negative buckets from data and filtering out Neutral scores\nprint (\"Creating Positive\/Negative dataset\")\n%time baseData['PosNegFlag'] = [bucket(0,score) for score in baseData['Score']]\nposnegdata = baseData[baseData['PosNegFlag']!=-1][['Text','PosNegFlag']].rename(columns={'PosNegFlag':'Flag'})\n\n# Creating Positive, Neutral & Negative buckets from data\nprint (\"-\"*80,\"\\nCreating Positive\/Negative\/Neutral dataset\")\n%time baseData['PosNegNeuFlag'] = [bucket(1,score) for score in baseData['Score']]\nposnegneudata = baseData[['Text','PosNegNeuFlag']].rename(columns={'PosNegNeuFlag':'Flag'})","427896dd":"print(\"Installing vadersentiment library\\n\")\n%time !pip install vadersentiment\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n#Creating a function to build sentiment score\ndef generateSentimentScore(sentence):\n    sentence = re.sub(r\"<.*?>\", \"\", sentence) #Removing HTML tags\n    sid = SentimentIntensityAnalyzer()\n    sentiment_scores = sid.polarity_scores(sentence)\n    return sentiment_scores[\"compound\"]\n\ndef bucketScores(score):\n    if score>=0.05:\n        return 2 #Positive\n    elif score<=-0.05:\n        return 0 #Negative\n    else:\n        return 1 #Neutral\n\n#Creating a subset of data to run Vader Sentiment on\nvaderData = baseData.sample(30000,random_state=10)\nprint(\"-\"*80,\"\\nScoring reviews with Vader sentiment library to generate sentiment scores\")\n%time vaderData['Sent'] = vaderData['Text'].progress_apply(generateSentimentScore)\n\n\nprint(\"-\"*80,\"\\nCreating sentiment buckets (0-Negative\/1-Neutral\/2-Positive)\")\n%time vaderData['Score_Sent'] = vaderData['Sent'].progress_apply(bucketScores)\nvaderData['Score_Sent_5'] = pd.qcut(vaderData.Sent,5,labels=False)+1\nvaderData.head(3)","a1e8c92a":"from textblob import TextBlob\n\n#Creating a function to build sentiment score\ndef generateSentimentScore2(sentence):\n    sentence = re.sub(r\"<.*?>\", \"\", sentence) #Removing HTML tags\n    sentiment_scores = TextBlob(sentence)\n    return sentiment_scores.sentiment[0]\n\n#Creating a function to bucket sentiment score\ndef bucketScores2(score):\n    if score > 0:\n        return 2 #Positive\n    elif score < 0:\n        return 0 #Negative\n    else:\n        return 1 #Neutral\n    \n#Creating a subset of data to run Vader Sentiment on\ntextblobData = baseData.sample(30000,random_state=10)\n%time textblobData['Sent'] = textblobData['Text'].progress_apply(generateSentimentScore2)\n\nprint(\"-\"*80,\"\\nCreating sentiment buckets (0-Negative\/1-Neutral\/2-Positive)\")\n%time textblobData['Score_Sent'] = textblobData['Sent'].progress_apply(bucketScores2)\ntextblobData['Score_Sent_5'] = pd.qcut(textblobData.Sent,5,labels=False)+1\ntextblobData.head(3)","82ec3fe6":"#Normalizing scores\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nvaderData['ScoreScaled'] = 0\nvaderData['SentScaled'] = 0\nvaderData[['ScoreScaled','SentScaled']] = scaler.fit_transform(vaderData[['Score','Sent']])\n\n#Plotting correlations\nplt.figure(figsize=(3.5,2))\nsns.heatmap(vaderData[['ScoreScaled','SentScaled']].corr(),annot=True,fmt='.2g',cmap='Blues')\nplt.title('Correlation between Base Scores and Vader sentiment scores',size=14)\nplt.xticks(rotation=0)\nplt.show()","83bb9385":"#Normalizing scores\nscaler = MinMaxScaler()\ntextblobData['ScoreScaled'] = 0\ntextblobData['SentScaled'] = 0\ntextblobData[['ScoreScaled','SentScaled']] = scaler.fit_transform(textblobData[['Score','Sent']])\n\n#Plotting correlations\nplt.figure(figsize=(3.5,2))\nsns.heatmap(textblobData[['ScoreScaled','SentScaled']].corr(),annot=True,fmt='.2g',cmap='Blues')\nplt.title('Correlation between Base Scores and TextBlob sentiment scores',size=14)\nplt.xticks(rotation=0)\nplt.show()","401578b5":"#Creating a function to take stratified samples\ndef stratifiedSample(df,column,samplesize=1000):\n    classes = len(set(df[column]))\n    df_sample = df.groupby(column, group_keys=False).apply(lambda x: x.sample(min(len(x), int(samplesize\/classes))))\n    return df_sample","8ef065d2":"samplesize=1000\n\n#Random sampling\n%time posneg_sample = posnegdata.sample(samplesize,random_state=10).rename(columns={'Flag':'Y'}).reset_index(drop=True)\n%time posnegneu_sample = posnegneudata.sample(samplesize,random_state=10).rename(columns={'Flag':'Y'}).reset_index(drop=True)\n%time base_sample = baseData[['Text','Score']].sample(samplesize,random_state=10).rename(columns={'Score':'Y'}).reset_index(drop=True)\n%time vader_sample = vaderData[['Text','Score_Sent']].sample(samplesize,random_state=10).rename(columns={'Score_Sent':'Y'}).reset_index(drop=True)\n\n#Stratified sampling\n%time posneg_st_sample = stratifiedSample(posnegdata,'Flag',samplesize).rename(columns={'Flag':'Y'}).reset_index(drop=True)\n%time posnegneu_st_sample = stratifiedSample(posnegneudata,'Flag',samplesize).rename(columns={'Flag':'Y'}).reset_index(drop=True)\n%time base_st_sample = stratifiedSample(baseData[['Text','Score']],'Score',samplesize).rename(columns={'Score':'Y'}).reset_index(drop=True)\n%time vader_st_sample = stratifiedSample(vaderData[['Text','Score_Sent']],'Score_Sent',samplesize).rename(columns={'Score_Sent':'Y'}).reset_index(drop=True)","135ad2a7":"#Defining functions and variables required for processing of Texts\n\nstemmer = PorterStemmer()\nfrom nltk.corpus import stopwords\n\ndef stem_tokens(tokens, stemmer):\n    stemmed = []\n    for item in tokens:\n        stemmed.append(stemmer.stem(item))\n    return stemmed\n\ndef tokenize(text):\n    tokens = word_tokenize(text)\n    stems = stem_tokens(tokens, stemmer)\n    return ' '.join(stems)\n\nsymbols = string.punctuation\nreplacement = \"                                \" # No. of spaces equal to number of symbols in \"symbols\" variable\nmappingDict = str.maketrans(symbols, replacement)\n\ncount_vect = CountVectorizer()\ntfidf_transformer = TfidfTransformer()\n\ndef processData(data):\n    corpus = []\n    for comment in data:\n        comment = str(comment).lower()\n        comment = re.sub('[\\W_]', '', comment)\n        comment = comment.translate(mappingDict)\n        comment=tokenize(comment)\n        corpus.append(comment)\n    \n    data_counts = count_vect.fit_transform(corpus)\n    data_tfidf = tfidf_transformer.fit_transform(data_counts)\n    data = pd.concat([data.reset_index(drop=True),pd.DataFrame(data_tfidf.todense())], axis=1)\n    return data","72c2391c":"# Runing data processing steps on following dataframes one by one\n# 1. posneg_sample\n%time posneg_sample_x = processData(posneg_sample['Text'])\nposneg_sample_y = posneg_sample['Y']\n\n# 2. posnegneu_sample\n%time posnegneu_sample_x = processData(posnegneu_sample['Text'])\nposnegneu_sample_y = posnegneu_sample['Y']\n\n# 3. base_sample\n%time base_sample_x = processData(base_sample['Text'])\nbase_sample_y = base_sample['Y']\n\n# 4. vader_sample\n%time vader_sample_x = processData(vader_sample['Text'])\nvader_sample_y = vader_sample['Y']\n\n# 5. posneg_st_sample\n%time posneg_st_sample_x = processData(posneg_st_sample['Text'])\nposneg_st_sample_y = posneg_st_sample['Y']\n\n# 6. posnegneu_st_sample\n%time posnegneu_st_sample_x = processData(posnegneu_st_sample['Text'])\nposnegneu_st_sample_y = posnegneu_st_sample['Y']\n\n# 7. base_st_sample\n%time base_st_sample_x = processData(base_st_sample['Text'])\nbase_st_sample_y = base_st_sample['Y']\n\n# 8. vader_st_sample\n%time vader_st_sample_x = processData(vader_st_sample['Text'])\nvader_st_sample_y = vader_st_sample['Y']","b8329b06":"from imblearn.over_sampling import SMOTE\n\ndef balanceData(data_x,data_y,label=''):\n    sm = SMOTE(random_state=12)\n    data_x,data_y = sm.fit_sample(data_x.drop('Text',axis=1), data_y.ravel())\n    data_x = pd.DataFrame(data_x)\n    data_y = pd.Series(data_y)\n    return data_x,data_y","e8f81dc0":"%time posneg_smote_x,posneg_smote_y = balanceData(posneg_sample_x,posneg_sample_y,'Pos\/Neg')\n%time posnegneu_smote_x,posnegneu_smote_y = balanceData(posnegneu_sample_x,posnegneu_sample_y,'Pos\/Neg\/Neu')\n%time base_smote_x,base_smote_y = balanceData(base_sample_x,base_sample_y,'5 Class')\n%time vader_smote_x,vader_smote_y = balanceData(vader_sample_x,vader_sample_y,'3 Class Vader')","7d2d887c":"fig,axes = plt.subplots(3,4,figsize=(20,15))\nsns.countplot(posneg_sample_y,ax=axes[0,0])\naxes[0,0].set(title=\"Pos\/Neg Random\")\nsns.countplot(posnegneu_sample_y,ax=axes[0,1])\naxes[0,1].set(title=\"Pos\/Neg\/Neu Random\")\nsns.countplot(base_sample_y,ax=axes[0,2])\naxes[0,2].set(title=\"5 Class Random\")\nsns.countplot(vader_sample_y,ax=axes[0,3])\naxes[0,3].set(title=\"Vader 3 Class Random\")\nsns.countplot(posneg_st_sample_y,ax=axes[1,0])\naxes[1,0].set(title=\"Pos\/Neg Stratified\")\nsns.countplot(posnegneu_st_sample_y,ax=axes[1,1])\naxes[1,1].set(title=\"Pos\/Neg\/Neu Stratified\")\nsns.countplot(base_st_sample_y,ax=axes[1,2])\naxes[1,2].set(title=\"5 Class Stratified\")\nsns.countplot(vader_st_sample_y,ax=axes[1,3])\naxes[1,3].set(title=\"Vader 3 Class Stratified\")\nsns.countplot(posneg_smote_y,ax=axes[2,0])\naxes[2,0].set(title=\"Pos\/Neg with SMOTE\")\nsns.countplot(posnegneu_smote_y,ax=axes[2,1])\naxes[2,1].set(title=\"Pos\/Neg\/Neu with SMOTE\")\nsns.countplot(base_smote_y,ax=axes[2,2])\naxes[2,2].set(title=\"5 Class with SMOTE\")\nsns.countplot(vader_smote_y,ax=axes[2,3])\naxes[2,3].set(title=\"Vader 3 Class with SMOTE\")\nplt.suptitle(\"Distribution of classes in all generated datasets (Sample size=\"+str(samplesize)+\")\",size=14)\nplt.show()","bed5af0b":"#Creating some functions for use in modelling\n\ndef plot_roc_curve(fprs, tprs):\n    \"\"\"Plot the Receiver Operating Characteristic from a list\n    of true positive rates and false positive rates.\"\"\"\n    \n    # Initialize useful lists + the plot axes.\n    tprs_interp = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    f, ax = plt.subplots(figsize=(5,5))\n    \n    # Plot ROC for each K-Fold + compute AUC scores.\n    for i, (fpr, tpr) in enumerate(zip(fprs, tprs)):\n        tprs_interp.append(np.interp(mean_fpr, fpr, tpr))\n        tprs_interp[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        ax.plot(fpr, tpr, lw=1, alpha=0.3,\n                 label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n        \n    # Plot the luck line.\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n             label='Random', alpha=.8)\n    \n    # Plot the mean ROC.\n    mean_tpr = np.mean(tprs_interp, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    ax.plot(mean_fpr, mean_tpr, color='b',\n             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n             lw=2, alpha=.8)\n    \n    # Plot the standard deviation around the mean ROC.\n    std_tpr = np.std(tprs_interp, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                     label=r'$\\pm$ 1 std. dev.')\n    \n    # Fine tune and show the plot.\n    ax.set_xlim([-0.05, 1.05])\n    ax.set_ylim([-0.05, 1.05])\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.set_title('Receiver operating characteristic')\n    ax.legend(loc=\"lower right\")\n    plt.show()\n    return (f, ax)\n\ndef compute_roc_auc(index,clf):\n    y_predict = clf.predict_proba(X_train.iloc[index])[:,1]\n    fpr, tpr, thresholds = roc_curve(y_train.iloc[index], y_predict)\n    auc_score = auc(fpr, tpr)\n    return fpr, tpr, auc_score\n\ndef runCrossVal(X_train,y_train,clf):\n    cv = StratifiedKFold(n_splits=10, random_state=123, shuffle=True)\n    results = pd.DataFrame(columns=['training_score', 'test_score'])\n    fprs, tprs, scores = [], [], []\n    for (train, test), i in zip(cv.split(X_train, y_train), range(10)):\n        clf.fit(X_train.iloc[train], y_train.iloc[train])\n        _, _, auc_score_train = compute_roc_auc(train,clf)\n        fpr, tpr, auc_score = compute_roc_auc(test,clf)\n        scores.append((auc_score_train, auc_score))\n        fprs.append(fpr)\n        tprs.append(tpr)\n    return fprs,tprs,scores\n\ndef runCrossVal_multiclass(X_train,y_train,clf):\n    #Define tasks to be performed\n    pipe= Pipeline([('clf', clf)])\n    #Binarizing multiple classes\n    y_bin = label_binarize(y_train, classes=list(np.unique(y_train)))\n    n_classes = y_bin.shape[1]\n    \n    #Running cross_val\n    y_score = cross_val_predict(pipe, X_train, y_bin, cv=10 ,method='predict')\n    \n    #Plotting results\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    colors = cycle(['blue', 'red', 'green','cyan','black'])\n    for i, color in zip(range(n_classes), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n                 label='ROC curve of class {0} (area = {1:0.2f})'\n                 ''.format(i+1, roc_auc[i]))\n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n    plt.xlim([-0.05, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic for multi-class data')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    return roc_auc\n\ndef splitData(data_x,data_y):\n    return train_test_split(data_x,data_y, test_size=0.25, random_state=12)","0480d234":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn import linear_model\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import metrics\n\nclf_rf = RandomForestClassifier(\n    n_estimators=50, criterion='gini',  max_depth=5,  min_samples_split=2,  min_samples_leaf=1,  min_weight_fraction_leaf=0.0,\n    max_features='auto',  max_leaf_nodes=None,  min_impurity_decrease=0.0,  min_impurity_split=None,  bootstrap=True,\n    oob_score=False,  n_jobs=-1,  random_state=0,  verbose=0,  warm_start=False,  class_weight='balanced'\n)\n\nclf_nb = BernoulliNB(alpha=1.0)\n\nclf_lr = linear_model.LogisticRegression(C=1e5)\n\nclf_svm = SVC(gamma='auto',probability=True)\n\nclf_knn = KNeighborsClassifier(n_neighbors=2)\n\nclf_mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(25,), random_state=1)\n\n#Creating a list of all classifiers\nbinary_classifiers = [clf_rf,clf_nb,clf_lr,clf_svm,clf_knn,clf_mlp]\nmulticlass_classifiers = [clf_rf,clf_knn,clf_mlp]","6f1795ae":"binary_datasets = {'posneg_sample':[posneg_sample_x,posneg_sample_y],\n                      'posneg_st_sample':[posneg_st_sample_x,posneg_st_sample_y],\n                      'posneg_smote':[posneg_smote_x,posneg_smote_y]}\n\nmulticlass_datasets = {'posnegneu_sample':[posnegneu_sample_x,posnegneu_sample_y],\n                       'base_sample':[base_sample_x,base_sample_y],\n                       'vader_sample':[vader_sample_x,vader_sample_y],\n                       'posnegneu_st_sample':[posnegneu_st_sample_x,posnegneu_st_sample_y],\n                       'base_st_sample':[base_st_sample_x,base_st_sample_y],\n                       'vader_st_sample':[vader_st_sample_x,vader_st_sample_y],\n                       'posnegneu_smote':[posnegneu_smote_x,posnegneu_smote_y],\n                       'base_smote':[base_smote_x,base_smote_y],\n                       'vader_smote':[vader_smote_x,vader_smote_y]}","d5d3bc67":"#Binary classifications\n\naccdf = pd.DataFrame(columns=[\"Model\",\"Data\",\"AUC\"])\ni = 0\n\nfor clf in binary_classifiers:\n    for data,dataxy in binary_datasets.items():\n        print(str(clf).split('(')[0],\" with \",data)\n        if \"smote\" not in data:\n            X_train, X_test, y_train, y_test = splitData(dataxy[0].drop('Text',axis=1),dataxy[1])\n        else:\n            X_train, X_test, y_train, y_test = splitData(dataxy[0],dataxy[1])\n        %time fprs,tprs,score = runCrossVal(X_train,y_train,clf)\n        plot_roc_curve(fprs, tprs)\n        %time clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        print (\"Model performance metrics for \",str(clf).split('(')[0],\" with \",data)\n        print(metrics.classification_report(y_test, y_pred))\n        accdf.loc[i] = [str(clf).split('(')[0], data, mean(score[1])]\n        i+=1\n        print(\"-\"*80)","7c31dff3":"accdf.sort_values(by=\"AUC\",ascending=False)","fbc5b66b":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.pipeline import Pipeline\nfrom itertools import cycle","fab47f87":"accdf = pd.DataFrame(columns=[\"Model\",\"Data\"])\ni = 0\n\nfor clf in multiclass_classifiers:\n    for data,dataxy in multiclass_datasets.items():\n        print(str(clf).split('(')[0],\" with \",data)\n        if \"smote\" not in data:\n            %time score = runCrossVal_multiclass(dataxy[0].drop('Text',axis=1),dataxy[1],clf)\n            X_train, X_test, y_train, y_test = splitData(dataxy[0].drop('Text',axis=1),dataxy[1])\n        else:\n            %time score = runCrossVal_multiclass(dataxy[0],dataxy[1],clf)\n            X_train, X_test, y_train, y_test = splitData(dataxy[0],dataxy[1])\n        %time clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        print (\"Model performance metrics for \",str(clf).split('(')[0],\" with \",data)\n        print(metrics.classification_report(y_test, y_pred))\n        accdf = accdf.append(score, ignore_index=True)\n        accdf.loc[i,\"Model\"] = str(clf).split('(')[0]\n        accdf.loc[i,\"Data\"] = data\n        i+=1\n        print(\"-\"*80)","15282b5a":"for i,row in accdf.iterrows():\n    accdf.loc[i,\"AvgAUC\"] = np.mean(row[2:])","d0a6db03":"accdf.sort_values(by=\"AvgAUC\",ascending=False)","6251df99":"# MLP with PosNeg SMOTE dataset\nX_train, X_test, y_train, y_test = splitData(posneg_smote_x,posneg_smote_y)\nclf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(25,), random_state=1)\n%time clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)","7ab6dc4a":"from sklearn import metrics\n\nprint (\"Model performance metrics for MLP with 2 class (pos\/neg) dataset balanced with SMOTE\")\nprint(metrics.classification_report(y_test, y_pred))","dcf85827":"false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplt.plot(false_positive_rate, true_positive_rate, label='%s: AUC %0.2f'% (\"MLP\",roc_auc))\n\nplt.title('ROC curve for MLP Classifier with 2 class (Pos\/Neg) dataset balanced with SMOTE')\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","4668e573":"cm = confusion_matrix(y_test, y_pred)\ncm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nnp.set_printoptions(precision=2)","46cca32d":"fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,3))\nax1.set(title = \"Confusion Matrix\")\nax2.set(title = \"Normalized Confusion Matrix\")\ncm_plt1 = sns.heatmap(cm, annot=True, ax = ax1, cmap='Blues',fmt = 'g',linewidths = 0.4,linecolor='white',xticklabels = ['Positive','Negative'], yticklabels = ['Positive','Negative']);\ncm_plt2 = sns.heatmap(cm_normalized, annot=True, ax = ax2, cmap='Blues',fmt = 'g',linewidths = 0.4,linecolor='white',xticklabels = ['Positive','Negative'], yticklabels = ['Positive','Negative']);\ncm_plt1.set_xlabel('Predicted')\ncm_plt1.set_ylabel('True')\ncm_plt2.set_xlabel('Predicted')\ncm_plt2.set_ylabel('True')\nplt.suptitle('Confusion matrices for MLP Classifier',size=14).set_position([.5, 1.05])\nplt.show()","743c47bf":"#Misclassified records\nc = 0\nfor i in y_test[y_test!=y_pred].index:\n    print(\"Mis-classified record:\")\n    print(posneg_sample_x.iloc[[i]].Text)\n    print(\"Actual Label : \",y_test[i])\n    print(\"Predicted Label : \",y_pred[y_test!=y_pred][c])\n    c+=1\n    print(\"-\"*80)\n    ","1aee4605":"# MLP with PosNeg SMOTE dataset\nX_train, X_test, y_train, y_test = splitData(vader_smote_x,vader_smote_y)\nclf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(25,), random_state=1)\n%time clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)","e67e8d49":"from sklearn import metrics\n\nprint (\"Model performance metrics for MLP with 2 class (pos\/neg) dataset balanced with SMOTE\")\nprint(metrics.classification_report(y_test, y_pred))","eec5a324":"#Binarizing multiple classes\ny_test_bin = label_binarize(y_test, classes=list(np.unique(y_test)))\ny_pred_bin = label_binarize(y_pred, classes=list(np.unique(y_pred)))\nn_classes = y_test_bin.shape[1]\n\n\n#Plotting results\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_bin[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\ncolors = cycle(['blue', 'red', 'green','cyan','black'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i+1, roc_auc[i]))\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic for multi-class data')\nplt.legend(loc=\"lower right\")\nplt.show()","4ed4fa85":"def plot_cm(cm,cm_normalized,cls):\n    fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,3))\n    ax1.set(title = \"Confusion Matrix\")\n    ax2.set(title = \"Normalized Confusion Matrix\")\n    cm_plt1 = sns.heatmap(cm, annot=True, ax = ax1, cmap='Blues',fmt = 'g',linewidths = 0.4,linecolor='white',xticklabels = ['Positive','Negative'], yticklabels = ['Positive','Negative']);\n    cm_plt2 = sns.heatmap(cm_normalized, annot=True, ax = ax2, cmap='Blues',fmt = 'g',linewidths = 0.4,linecolor='white',xticklabels = ['Positive','Negative'], yticklabels = ['Positive','Negative']);\n    cm_plt1.set_xlabel('Predicted')\n    cm_plt1.set_ylabel('True')\n    cm_plt2.set_xlabel('Predicted')\n    cm_plt2.set_ylabel('True')\n    plt.suptitle('Confusion matrices for MLP Classifier for class '+str(cls),size=14).set_position([.5, 1.05])\n    plt.show()\n\nfor i in range(n_classes):\n    cm = confusion_matrix(y_test_bin[:, i], y_pred_bin[:, i])\n    cm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    np.set_printoptions(precision=2)\n    plot_cm(cm,cm_normalized,i)","e67e199e":"#Misclassified records\nc = 0\nfor i in y_test[y_test!=y_pred].index:\n    print(\"Mis-classified record:\")\n    print(vader_sample_x.iloc[[i]].Text)\n    print(\"Actual Label : \",y_test[i])\n    print(\"Predicted Label : \",y_pred[y_test!=y_pred][c])\n    c+=1\n    print(\"-\"*80)","d8deea7e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","69abfe73":"yelpreviews = pd.read_csv('..\/input\/yelp-reviews-dataset\/yelp.csv')\nyelpreviews = yelpreviews[['text','stars']]\nyelpreviews.head(2)","b8aad05a":"%time yelpdata_x = processData(yelpreviews['text'])\nyelpdata_y = yelpreviews['stars']","10d80d34":"from imblearn.over_sampling import SMOTE\n\ndef balanceData(data_x,data_y,label=''):\n    sm = SMOTE(random_state=12)\n    data_x,data_y = sm.fit_sample(data_x.drop('text',axis=1), data_y.ravel())\n    data_x = pd.DataFrame(data_x)\n    data_y = pd.Series(data_y)\n    return data_x,data_y","41c45ef8":"%time yelpdata_smote_x,yelpdata_smote_y = balanceData(yelpdata_x,yelpdata_y,'Yelp 5 class')","63dd5e6a":"# MLP with PosNeg SMOTE dataset\nX_train, X_test, y_train, y_test = splitData(yelpdata_smote_x,yelpdata_smote_y)\nclf = MLPClassifier(solver='sgd',learning_rate='adaptive', alpha=1e-5, hidden_layer_sizes=(35,), random_state=1)\n%time clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)","e153c105":"from sklearn import metrics\n\nprint (\"Model performance metrics for yelp 5 class dataset balanced with SMOTE\")\nprint(metrics.classification_report(y_test, y_pred))","947ce2c4":"from sklearn import metrics\n\nprint (\"Model performance metrics for yelp 5 class dataset balanced with SMOTE\")\nprint(metrics.classification_report(y_test, y_pred))","4bb39927":"#Binarizing multiple classes\ny_test_bin = label_binarize(y_test, classes=list(np.unique(y_test)))\ny_pred_bin = label_binarize(y_pred, classes=list(np.unique(y_pred)))\nn_classes = y_test_bin.shape[1]\n\n\n#Plotting results\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_bin[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\ncolors = cycle(['blue', 'red', 'green','cyan','black'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i+1, roc_auc[i]))\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic for multi-class data')\nplt.legend(loc=\"lower right\")\nplt.show()","53cae17b":"def plot_cm(cm,cm_normalized,cls):\n    fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,3))\n    ax1.set(title = \"Confusion Matrix\")\n    ax2.set(title = \"Normalized Confusion Matrix\")\n    cm_plt1 = sns.heatmap(cm, annot=True, ax = ax1, cmap='Blues',fmt = 'g',linewidths = 0.4,linecolor='white',xticklabels = ['Positive','Negative'], yticklabels = ['Positive','Negative']);\n    cm_plt2 = sns.heatmap(cm_normalized, annot=True, ax = ax2, cmap='Blues',fmt = 'g',linewidths = 0.4,linecolor='white',xticklabels = ['Positive','Negative'], yticklabels = ['Positive','Negative']);\n    cm_plt1.set_xlabel('Predicted')\n    cm_plt1.set_ylabel('True')\n    cm_plt2.set_xlabel('Predicted')\n    cm_plt2.set_ylabel('True')\n    plt.suptitle('Confusion matrices for MLP Classifier for class '+str(cls),size=14).set_position([.5, 1.05])\n    plt.show()\n\nfor i in range(n_classes):\n    cm = confusion_matrix(y_test_bin[:, i], y_pred_bin[:, i])\n    cm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    np.set_printoptions(precision=2)\n    plot_cm(cm,cm_normalized,i)","b470a003":"y_test[y_test!=y_pred].index","7e2f9150":"#Misclassified records\nc = 0\nfor i in y_test[y_test!=y_pred].index:\n    print(\"Mis-classified record:\")\n    print(yelpdata_x.iloc[[i]].text)\n    print(\"Actual Label : \",y_test[i])\n    print(\"Predicted Label : \",y_pred[y_test!=y_pred][c])\n    c+=1\n    print(\"-\"*80)\n    if c==5: break","bb648fd3":"#### Text pre-processing","d4e1bb1d":"#### Multiclass classification","24741dc6":"#### Balancing the data using SMOTE","1e1a8e82":"# Problem Statement","2b03a378":"### Comparing the effectiveness of Sentiment Analysis libraries","fc70868b":"## Global Imports","8580e366":"# Modelling","5a933a2a":"As seen from the correlation plot, the VaderSentiment outputs slightly higher correlation with the actual score given to the text labels as compared to that of TextBlob.","8ca81f35":"# 3rd Party dataset","46bc62f4":"# Approach","3045b477":"#### Binary Classification","167c3c93":"# Interpretation of results","14de3ed6":"From these results, following combinations gave the best results:\n\n__Binary Classification:__\n* <code>LogisticRegression Classifier<\/code> with 2 class (Positive\/Negative) dataset balanced with <code>SMOTE<\/code> : __AUC 0.99__\n* <code>MultiLayerPerceptron Classifier<\/code> with 2 class (Positive\/Negative) dataset balanced with <code>SMOTE<\/code> : __AUC 0.99__\n* <code>RandomForest Classifier<\/code> with 2 class (Positive\/Negative) dataset balanced with <code>SMOTE<\/code> : __AUC 0.93__\n* <code>Bernouli NaiveBays Classifier<\/code> with 2 class (Positive\/Negative) dataset balanced with <code>SMOTE<\/code> : __AUC 0.90__\n\n__Multiclass Classification:__\n* <code>MultiLayerPerceptron Classifier<\/code> with 3 class (Positive\/Negative\/Neutral) dataset balanced with <code>SMOTE<\/code> : __AUC 0.81,0.99,0.78__\n* <code>KNN Classifier<\/code> with 3 class (Positive\/Negative\/Neutral) dataset balanced with <code>SMOTE<\/code> : __AUC 0.93,0.87,0.50__\n* <code>MultiLayerPerceptron Classifier<\/code> with 3 class <code>VaderSentiment<\/code> (Positive\/Negative\/Neutral) dataset balanced with <code>SMOTE<\/code> : __AUC 0.75,0.95,0.97__\n* <code>KNN Classifier<\/code> with 3 class <code>VaderSentiment<\/code> (Positive\/Negative\/Neutral) dataset balanced with <code>SMOTE<\/code> : __AUC 0.51,0.96,0.83__\n* <code>MultiLayerPerceptron Classifier<\/code> with 5 class dataset balanced with <code>SMOTE<\/code> : __AUC 0.99,1,0.78,0.97,0.83__\n* <code>KNN Classifier<\/code> with 5 class dataset balanced with <code>SMOTE<\/code> : __AUC 0.99,0.98,0.99,0.96,0.50__","35df49dd":"# Solution","d4ea3f15":"* The overall scores are binned into 2 labels and named Overall Sentiment - 1 indicating a positive score (>3) and 0 indicating a negative score (<3)\n* All scores =3 are removed as they are neutral and won't give any insight into positivity\/negativity towards a product\n* The text data is also converted into a corpus. Corpus is a collection of sentences (called documents) and is the data type understood by textmining library in R\n* Corpus data is then cleaned by removing commonly used symbols and punctuation\n* The cleaned corpus is then converted into a Matrix containing TF-IDF values. Term Frequence - Inverse Document Frequency (TF-IDF) is a metric used to indentify relative importance of each word in a sentence based on it's frequency of appearance in the sentence\n* The words with their IDF weights are used as predictors while the Overall Sentiment is treated as a dependent variable\n* The text data is then also fed into Sentiment analyser <code>VaderSentiment<\/code> that scores each sentence based on a predefined dictionary and labels them into <code>Positive<\/code>,<code>Negative<\/code> & <code>Neutral<\/code> classes\n* Since sentiment data renders imbalance class output, we will use <code>SMOTE<\/code> to balance both classes\n* This data is then split into test and train and different classifiers are trained on the training data and used to predict Overall Sentiment score (0 or 1) for test data\n* This exercise done using Traditional as well as Deep learning models\n* AUC-ROC (Area Under Curve for Reciever Operating Characteristics) is used to finalize a classifier of the lot\n* Confusion matrix is then used to validate the final model results","5cf29c58":"#### Multiclass Classification","69e1c235":"#### Binary classification","93098d85":"#### Creating datasets for different use-cases\n\n* Positive (4&5 scores) and Negative (1&2 scores) buckets and filtering out Neutral (3 scores)\n* Positive (4&5 scores), Neutral (3 scores) and Negative (1&2 scores) buckets\n* Multiclass dataset (without any bucketing)\n* 3 class dataset (Positive, Negative & Neutral) using <code>VaderSentiment<\/code>","c648c955":"#### Visualizing the final datasets to be used for the exercise","81df5537":"In this problem we want to study the effectiveness of traditional models in utilizing text data as predictor variable in machine learning purposes. We are given a dataset of user reviews about certain products with overall Score for each product.\n\nWe want to predict product score based on the comments provided for that product","7d0f557f":"### Model selection using Cross Validation","fc7b5de6":"### Traditional Models:\n\n__Binary Classification__: In case of 2 classes, tree based classifiers like <code>RandomForest<\/code> do not perform as good as the simple linear classifiers like <code>LogisticRegression<\/code>. Also, balancing the data with <code>SMOTE<\/code> improves the accuracy by a huge amount.\n\n__Multiclass Classification__: <code>K-Nearest Neighbours<\/code> classifier performed the best among the selected classifiers in both 3 and 5 class datasets. Balancing the data with <code>SMOTE<\/code> turns out to be a bonus here as well.\n\n### Deep Learning Models:\n<code>MultiLayer Perceptron<\/code> based neural network classifiers outperformed the traditional classifiers in every aspect. Their performance is aided by presence of class balancing techniques like <code>SMOTE<\/code>","cf11d200":"Creating 3 buckets using <code>VADER<\/code> library","0e9e41c4":"Let's select the best performing model and plot classification metrics for the test datasets","069303a7":"Now, we have 12 datasets in total:\n* 2 class random sample\n* 2 class stratified sample\n* 2 class SMOTE balanced sample\n* 3 class Vader Sentiment random sample\n* 3 class Vader Sentiment stratified sample\n* 3 class Vader Sentiment SMOTE balanced sample\n* 5 class random sample\n* 5 class stratified sample\n* 5 class SMOTE balanced sample\n\n\nWe'll be using a list of binary classifiers : RandomForest, NaiveBays, LogisticRegression, SVM, KNN and MLP & a list of multiclass classifiers : RandomForest, KNN & MLP. We'll be generating all combinations of 2 class datasets with binary classifiers & multiclass datasets with multiclass classifiers and running them on 10-fold cross validation and plotting ROC-AUC for every combination.\n\nThis will give us a series of graphs to look at and help us decide which Classifier-Dataset combination gives the best result.","33fab680":"#### Creating random and stratified samples from the data"}}