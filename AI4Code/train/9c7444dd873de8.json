{"cell_type":{"9d8977d7":"code","dfff6b8d":"code","811904af":"code","fb6f9a8f":"code","44be57f8":"code","a29ff71f":"code","c1b9ba14":"code","f0e8366a":"code","7d8bc014":"code","a8139bfc":"code","4db06513":"code","d842d55d":"code","b57ec0cf":"code","fcb60c50":"code","742e6ac1":"code","cd931696":"code","3a33338f":"code","b64758d9":"code","6e55cd27":"code","4cc91640":"markdown","58438d34":"markdown","c5605ec2":"markdown","cd8e983d":"markdown","7c219964":"markdown","28ca5e30":"markdown","c672c9a6":"markdown","c44036a5":"markdown"},"source":{"9d8977d7":"import numpy as np\nimport pandas as pd","dfff6b8d":"df = pd.read_csv('..\/input\/booksrecommendedbyworldfamouspersonnalities\/books_clean.csv')","811904af":"df.info()","fb6f9a8f":"df.head()","44be57f8":"# Checking for na values\n\nprint(df['category'].isna().sum())\nprint(df['publication_date'].isna().sum())","a29ff71f":"#Dropping all rows with na values\n\ndf = df.dropna()\ndf.shape","c1b9ba14":"# Conversion to lowercase\n# Eliminating the space b\/w first and last name as to avoid the recommender to bias towards similar occurences of a word in different people names as this would affect the similarity\n\ndf['recommender'] = df['recommender'].str.lower().str.replace(\" \",\"\").str.replace(\"|\",' ')\ndf['recommender'].head()","f0e8366a":"# Conversion of author names to lowercase\ndf['author'] = df['author'].str.lower()\ndf['author'].head()","7d8bc014":"# Conversion of category to lowercase\ndf['category'] = df['category'].str.lower()\ndf['category'].head()","a8139bfc":"# Total no of categories\ndf['category'].value_counts().count()","4db06513":"# Find duplicated rows\n\ndf[df.duplicated(['title'], keep = False)] ","d842d55d":"#Our Data after now looks like\ndf.head()","b57ec0cf":"# Creating a function that creates our content that will be used to calculate the similarity between two books\n#this content will act as our meta data that will help us determine \n\ndef create_content(x):\n    return ''.join(x['title']) + \" \" + ''.join(x['recommender']) + \" \" + ''.join(x['author']) +\" \"+''.join(x['category'])\ndf['content'] = df.apply(create_content, axis=1)","fcb60c50":"# Not using tf-idf vectorizer as it would downweight more repetitive words, but we do not need that \n# as our similarity will be reduced\n\n# Import CountVectorizer and create the count matrix\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english')\ncount_matrix = count.fit_transform(df['content'])\n\ncount_matrix","742e6ac1":"# Compute the Cosine Similarity matrix based on the count_matrix\n\nfrom sklearn.metrics.pairwise import cosine_similarity\ncosine_sim = cosine_similarity(count_matrix, count_matrix)","cd931696":"#Construct a reverse map of indices and movie titles\n\nindices = pd.Series(df.index, index=df['title']).drop_duplicates()\n\ndef get_recommendations(title, cosine_sim = cosine_sim):\n    # Get the index of the movie that matches the title\n    id_ = indices[title]\n\n    # Get the pairwsie similarity scores of all movies with that movie\n    sim_scores = list(enumerate(cosine_sim[id_]))\n\n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:11]\n\n    # Get the movie indices\n    book_indices = [i[0] for i in sim_scores]\n\n    # Return the top 10 most similar movies\n    data = df['title'].iloc[book_indices]\n    data.index = np.arange(1, len(data)+1)\n    data = pd.DataFrame(data)\n\n    return data\n    \n","3a33338f":"get_recommendations(\"QED\")","b64758d9":"#Construct a reverse map of indices and movie titles\n\nindices = pd.Series(df.index, index=df['title']).drop_duplicates()\n\ndef get_recommendations(title, cosine_sim = cosine_sim):\n    try:\n        # Get the index of the movie that matches the title\n        id_ = indices[title]\n\n        # Get the pairwsie similarity scores of all movies with that movie\n        sim_scores = list(enumerate(cosine_sim[id_]))\n\n        # Sort the movies based on the similarity scores\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n        # Get the scores of the 10 most similar movies\n        sim_scores = sim_scores[1:11]\n\n        # Get the movie indices\n        book_indices = [i[0] for i in sim_scores]\n\n        # Return the top 10 most similar movies\n        data = df['title'].iloc[book_indices]\n        data.index = np.arange(1, len(data)+1)\n        data = pd.DataFrame(data)\n\n        return data\n    \n    except:\n        \n        data = df.sort_values(['recommender_count', 'publication_date'], ascending=[ False, False])\n        data = data['title'].head(10)\n        data.index = np.arange(1, len(data)+1)\n        data = pd.DataFrame(data)\n        \n        return data","6e55cd27":"get_recommendations(\"some_random_text\")","4cc91640":"Creating a column that would be our metadata soup and will be used to determine the similarity between two books","58438d34":"Since the recommender system is content based thus for new entries it will generate an error which can be mitigated by suggesting the most recommended books, followed by latest books.\n\nSo, editing the previously created function","c5605ec2":"### Similarity Based Recommender implementattion","cd8e983d":"### Data Cleaning","7c219964":"# Content based recommender    ","28ca5e30":"As we can see that there are duplicate titles with different authors, thus it is intuitively incorrect to include titles in the content column as inspite of having similarity in their title names ( that would indicate them to possibly be sequels in other case)  they would not be similar from the perspective of a reader actually.","c672c9a6":"### Overview of data","c44036a5":"####  To determine the most similar books the measure of similarity used is cosine similarity.\nCosine Similarity\nI will be using the Cosine Similarity to calculate a numeric quantity that denotes the similarity between two movies. Mathematically, it is defined as follows:\n\ncosine(x,y)=x.y\u22ba||x||.||y|| \nSince we have used the Count Vectorizer, calculating the Dot Product will directly give us the Cosine Similarity Score. Therefore, we will use sklearn's linear_kernel instead of cosine_similarities since it is much faster."}}