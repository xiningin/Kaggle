{"cell_type":{"28b36b54":"code","30049300":"code","d6b9a53e":"code","b13dab0d":"code","06e5ca5f":"code","db525896":"code","b4dfdd8e":"code","342f6e84":"code","367ccbe5":"code","1e14fe2c":"code","d7a248db":"code","07a286d7":"code","97fcc056":"code","d7835826":"code","2682cb2e":"code","a42684b4":"code","79e1bc9e":"code","4c661590":"code","b535a325":"code","626f12ca":"code","1be961b3":"code","fb696d13":"code","109ed917":"code","88df3994":"code","ea30e98d":"code","e0ffec72":"code","de7d1c52":"code","fe019eae":"code","b993c447":"code","5ea27ad5":"code","7a322ff7":"code","c32e2614":"code","4d71c5ab":"code","192b3771":"code","ab7a55ae":"code","3b740d3a":"code","09078e4c":"code","fc7c00f6":"code","41b7f79b":"code","f5a9f0ac":"code","e6431b19":"code","edaf2eae":"code","100b3fa4":"markdown","e3eefeac":"markdown","38664635":"markdown","45fb2cbe":"markdown","55f67963":"markdown","bca17b12":"markdown","02feafbb":"markdown","81b37b77":"markdown","84c5520b":"markdown","2f2cd700":"markdown","199adf95":"markdown","834e4304":"markdown","56f84d2d":"markdown","c8509dd3":"markdown","16c4386b":"markdown","a46abd1a":"markdown","ba412221":"markdown","d087ebfe":"markdown","4b62cb3a":"markdown","19565ad0":"markdown","4e95ea8d":"markdown","6cc92fb4":"markdown","b9ac9b91":"markdown","85eb0567":"markdown","a8954488":"markdown","567e0ae4":"markdown"},"source":{"28b36b54":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\nimport itertools\nfrom scipy import interp\nfrom matplotlib import rcParams\n\nfrom time import time\nimport datetime\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\n\nfrom keras.layers import Concatenate, Input, Dense, Embedding, Flatten, Dropout, BatchNormalization, SpatialDropout1D\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.models import Model\nfrom keras.optimizers import  Adam\nimport keras.backend as k\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\ngc.enable()\n\nimport os\nos.chdir('\/kaggle\/input\/ieeedatapreprocessing') # Set working directory\nprint(os.listdir('\/kaggle\/input\/ieeedatapreprocessing'))","30049300":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","d6b9a53e":"%%time\ntrain_df = reduce_mem_usage(pd.read_pickle('train_df.pkl'))\ny_train = train_df['isFraud'].astype('uint8')\ntest_df = reduce_mem_usage(pd.read_pickle('test_df.pkl'))\nprint (\"Data is loaded!\")","b13dab0d":"print('train_transaction shape is {}'.format(train_df.shape))\nprint('test_transaction shape is {}'.format(test_df.shape))","06e5ca5f":"count = sns.countplot(y_train)\nplt.title('Distribution of our target variable in the training df')\ntotal = train_df.shape[0]\nfor p in count.patches:\n    height = p.get_height()\n    count.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=15)","db525896":"train_df = train_df.reset_index()\ntest_df = test_df.reset_index()","b4dfdd8e":"train_df['nulls1'] = train_df.isna().sum(axis=1)\ntest_df['nulls1'] = test_df.isna().sum(axis=1)","342f6e84":"train_df = train_df.drop([\"TransactionDT\"], axis = 1)\ntest_df = test_df.drop([\"TransactionDT\"], axis = 1)","367ccbe5":"train_df","1e14fe2c":"test_df","d7a248db":"for c1, c2 in train_df.dtypes.reset_index().values:\n    if c2=='O':\n        train_df[c1] = train_df[c1].map(lambda x: str(x).lower())\n        test_df[c1] = test_df[c1].map(lambda x: str(x).lower())","07a286d7":"numerical = [\"TransactionAmt\", \"nulls1\", \"dist1\", \"dist2\"] + [\"C\" + str(i) for i in range(1, 15)] + \\\n            [\"D\" + str(i) for i in range(1, 16)] + \\\n            [\"V\" + str(i) for i in range(1, 340)]\n\ncategorical = [\"ProductCD\", \"card1\", \"card2\", \"card3\", \"card4\", \"card5\", \"card6\", \"addr1\", \"addr2\",\n               \"P_emaildomain_bin\", \"P_emaildomain_suffix\", \"R_emaildomain_bin\", \"R_emaildomain_suffix\",\n               \"P_emaildomain\", \"R_emaildomain\",\n              \"DeviceInfo\", \"DeviceType\"] + [\"id_0\" + str(i) for i in range(1, 10)] +\\\n                [\"id_\" + str(i) for i in range(10, 39)] + \\\n                 [\"M\" + str(i) for i in range(1, 10)]","97fcc056":"numerical = [col for col in numerical if col in train_df.columns]\ncategorical = [col for col in categorical if col in train_df.columns]","d7835826":"# Label Encoding\ncategory_counts = {}\nfor f in categorical:\n    train_df[f] = train_df[f].replace(\"nan\", \"other\")\n    train_df[f] = train_df[f].replace(np.nan, \"other\")\n    test_df[f] = test_df[f].replace(\"nan\", \"other\")\n    test_df[f] = test_df[f].replace(np.nan, \"other\")\n    lbl = LabelEncoder()\n    lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n    train_df[f] = lbl.transform(list(train_df[f].values))\n    test_df[f] = lbl.transform(list(test_df[f].values))\n    category_counts[f] = len(list(lbl.classes_)) + 1","2682cb2e":"for column in numerical:\n    scaler = StandardScaler()\n    if train_df[column].max() > 100 and train_df[column].min() >= 0:\n        train_df[column] = np.log1p(train_df[column])\n        test_df[column] = np.log1p(test_df[column])\n    scaler.fit(np.concatenate([train_df[column].values.reshape(-1,1), test_df[column].values.reshape(-1,1)]))\n    train_df[column] = scaler.transform(train_df[column].values.reshape(-1,1))\n    test_df[column] = scaler.transform(test_df[column].values.reshape(-1,1))","a42684b4":"target = 'isFraud'","79e1bc9e":"tr_df, val_df = train_test_split(train_df, test_size = 0.2, random_state = 42, stratify = train_df[target])","4c661590":"print(tr_df.shape)\nprint(val_df.shape)","b535a325":"def get_input_features(df):\n    X = {'numerical':np.array(df[numerical])}\n    for cat in categorical:\n        X[cat] = np.array(df[cat])\n    return X","626f12ca":"categorical.remove(\"card1\")","1be961b3":"category_counts","fb696d13":"k.clear_session()","109ed917":"categorical_inputs = []\nfor cat in categorical:\n    categorical_inputs.append(Input(shape=[1], name=cat))\n\ncategorical_embeddings = []\nfor i, cat in enumerate(categorical):\n    categorical_embeddings.append(\n        Embedding(category_counts[cat], int(np.log1p(category_counts[cat]) + 1), name = cat + \"_embed\")(categorical_inputs[i]))\n\ncategorical_logits = Concatenate(name = \"categorical_conc\")([Flatten()(SpatialDropout1D(.1)(cat_emb)) for cat_emb in categorical_embeddings])\n\nnumerical_inputs = Input(shape=[tr_df[numerical].shape[1]], name = 'numerical')\nnumerical_logits = Dropout(.1)(numerical_inputs)","88df3994":"x = Concatenate()([\n    categorical_logits, \n    numerical_logits,\n])","ea30e98d":"x = BatchNormalization()(x)\nx = Dense(200, activation = 'relu')(x)\nx = Dropout(.2)(x)\nx = Dense(100, activation = 'relu')(x)\nx = Dropout(.2)(x)\nout = Dense(1, activation = 'sigmoid')(x)","e0ffec72":"model = Model(inputs=categorical_inputs + [numerical_inputs],outputs=out)","de7d1c52":"model.summary()","fe019eae":"loss = \"binary_crossentropy\"\nmodel.compile(optimizer=Adam(lr = 0.001), loss = loss)","b993c447":"X_train = get_input_features(tr_df)\nX_valid = get_input_features(val_df)\nX_test = get_input_features(test_df)\ny_train = tr_df[target]\ny_valid = val_df[target]","5ea27ad5":"rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=2, mode='auto', verbose = False)","7a322ff7":"best_score = 0\npatience = 0","c32e2614":"for i in range(100):\n    if patience < 4:\n        model_history = model.fit(X_train, y_train, validation_data = (X_valid,y_valid), batch_size = 1000, epochs = 1, verbose = 1)\n        valid_preds = model.predict(X_valid, batch_size = 8000, verbose = True)\n        score = roc_auc_score(y_valid, valid_preds)\n        print(score)\n        if score > best_score:\n            model.save_weights(\"\/kaggle\/working\/model.h5\")\n            best_score = score\n            patience = 0\n        else:\n#             patience += 1\n            pass","4d71c5ab":"model.load_weights(\"\/kaggle\/working\/model.h5\")","192b3771":"# confusion matrix\n# Predict the values from the validation dataset\ny_pred = model.predict(X_valid, batch_size = 500, verbose = True) ","ab7a55ae":"# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_valid, y_pred.round())","3b740d3a":"# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","09078e4c":"#Predict on test set\npredictions_NN_prob = y_pred\npredictions_NN_prob = predictions_NN_prob[:,0]","fc7c00f6":"#Print Area Under Curve\nfalse_positive_rate, recall, thresholds = roc_curve(y_valid, predictions_NN_prob)\nroc_auc = auc(false_positive_rate, recall)\nplt.figure()\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1], [0,1], 'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.ylabel('Recall')\nplt.xlabel('Fall-out (1-Specificity)')\nplt.show()","41b7f79b":"predictions = model.predict(X_test, batch_size = 2000, verbose = True)","f5a9f0ac":"submission = pd.read_csv('sample_submission.csv', index_col='TransactionID')\nsubmission.isFraud = predictions\nsubmission.head()","e6431b19":"plt.hist(submission.isFraud,bins=100)\nplt.ylim((0,5000))\nplt.title('Neural Network submission')\nplt.show()","edaf2eae":"submission.to_csv('\/kaggle\/working\/neural_network_submission.csv')","100b3fa4":"<div align='left'><font size='4' color='#229954'>Load package<\/font><\/div>","e3eefeac":"#  <a style=\"color:#6699ff\"> Team <\/a>\n- <a style=\"color:#6699ff\">Mohamed NIANG <\/a>\n- <a style=\"color:#6699ff\">Fernanda Tchouacheu <\/a>\n- <a style=\"color:#6699ff\">Hypolite Chokki <\/a>","38664635":"<div style=\"text-align: justify\"> We will take our categorical features fill the nans and assign them an integer ID per category and write down the number of total categories per column. We'll use this later in an embedding layer of the NN. <\/div>","45fb2cbe":"<div align='left'><font size='4' color='#229954'>ROC curve and AUC<\/font><\/div> ","55f67963":"<h1 align=\"center\" style=\"color:#6699ff\"> DataCamp IEEE Fraud Detection <\/h1>","bca17b12":"<div align='left'><font size='4' color='#229954'>Neural Network Model<\/font><\/div>","02feafbb":"<div align='left'><font size='4' color='#229954'>Label Encoding<\/font><\/div> ","81b37b77":" <div style=\"text-align: justify\"> Our neural network will be fairly standard. We will use the embedding layer for categoricals and the numericals will go through feed forward dense layers. <\/div>\n\n<br>\n\n <div style=\"text-align: justify\"> We create our embedding layers such that we have as many rows as we had categories and the dimension of the embedding is the log1p + 1 of the number of categories. So this means that categorical variables with very high cardinality will have more dimensions but not signficantly more so the information will still be compressed down to only about 13 dimensions and the smaller number of categories will be only 2-3. <\/div>\n\n<br>\n\n <div style=\"text-align: justify\"> We will then pass the embeddings through a spatial dropout layer which will drop dimensions within the embedding across batches and then flatten and concatenate. Then we will concatenate this to the numerical features and apply batch norm and then add some more dense layers after. <\/div>","84c5520b":"We will then extract the features we actually want to pass to the NN.","2f2cd700":"<div style=\"text-align: justify\"> We already dropped a lot of these features because in some trial and error it was shown that these caused rapid overfitting for some reason or otherwise introduced unnesecary noise into the data. We will make sure we only list the features we actually have still in the df now. <\/div>","199adf95":"# <a style=\"color:#6699ff\"> II. Deep leaning Models<\/a>","834e4304":"<div align='left'><font size='4' color='#229954'>Confusion matrix<\/font><\/div> ","56f84d2d":"<div align='left'><font size='4' color='#229954'>Numerical Scaling<\/font><\/div> ","c8509dd3":"- We will now split the train dataset into train and validation set with stratification to tackle unbalanced classes.\n- We will keeep 20% of data for validation.","16c4386b":"<div align='left'><font size='4' color='#229954'>Twelfth submission using Neural Network<\/font><\/div> ","a46abd1a":"<div align='left'><font size='4' color='#229954'>Numerical and Categorical<\/font><\/div> ","ba412221":"Dropping time since this likely isnt something we want our model to directly learn from.","d087ebfe":"<div align='left'><font size='4' color='#229954'>Target variable: imbalance problem<\/font><\/div>","4b62cb3a":"<div align='left'><font size='4' color='#229954'>Splitting to train and validation<\/font><\/div>","19565ad0":"<div align='left'><font size='4' color='#229954'>Reducing memory usage<\/font><\/div>","4e95ea8d":"<div style=\"text-align: justify\"> Now we will do some scaling of the data so that it will be in a more NN friendly format. First we will do log1p for any values that are above 100 and not below 0. This is in order to scale down any numerical variables that might have some extremely high values that screws up the statistics of the standard scaler. <\/div>\n\n<br>\n\n<div style=\"text-align: justify\"> After that we will pass them through the standard scaler so that the values have a normal mean and std. This makes the NN converge signficantly faster and prevents any blowouts. <\/div>","6cc92fb4":"<img src=\"https:\/\/github.com\/DataCampM2DSSAF\/suivi-du-data-camp-equipe-tchouacheu-niang-chokki\/blob\/master\/img\/credit-card-fraud-detection.png?raw=true\" width=\"800\" align=\"right\">","b9ac9b91":"<div align='left'><font size='4' color='#229954'>Load data<\/font><\/div>","85eb0567":"# <a style=\"color:#6699ff\">  Table of Contents<\/a> \n\n<a style=\"color:#6699ff\"> I. Preprocessing<\/a>\n\n<a style=\"color:#6699ff\"> II. Deep leaning Models<\/a>","a8954488":"# <a style=\"color:#6699ff\"> I. Preprocessing<\/a>","567e0ae4":"<div style=\"text-align: justify\"> Listing off and categorizing the various variables available to us. We have numerical and categoricals. We will treat both of these slightly differently later. <\/div>"}}