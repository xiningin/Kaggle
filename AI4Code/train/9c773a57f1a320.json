{"cell_type":{"a0572be4":"code","a7823349":"code","96d201e3":"code","4718330e":"code","1bd3a38c":"code","bce1f081":"code","3ca638d0":"code","32779a52":"code","ca422f1f":"code","a91f773d":"code","81cfdb1a":"code","7da490dd":"code","9f2814d2":"code","9607cf8b":"code","da287f08":"code","36c8e370":"code","4fb258ba":"code","ba86a21c":"code","e3618ce2":"code","7e89dc58":"code","aa270b2a":"code","adcec33a":"code","67522cce":"code","5b84805b":"code","1b3eaeb0":"code","92a60235":"code","a0a5b0de":"code","3d16f231":"code","f2516228":"code","082572ee":"code","bc2420a2":"code","9d355fb5":"code","ac16c6b1":"code","8c39cf6b":"code","22729ec4":"code","9d88872e":"code","a527bdf6":"code","b716a5c0":"code","1154b6c9":"code","75dfc9ac":"code","c236107e":"code","af520f80":"code","5af394be":"code","3cde7eab":"code","9146b985":"code","81973dfe":"code","5553d04b":"code","3126817d":"markdown","c24789af":"markdown","7dbfedd2":"markdown","853face9":"markdown","9a926cdf":"markdown","f367796a":"markdown","da30f620":"markdown","f3577ead":"markdown","0786dd80":"markdown","9b383c49":"markdown","0169ad03":"markdown","6ac92bab":"markdown","8ce7be6b":"markdown","cf0821d4":"markdown","13ea2aa3":"markdown","c581aa02":"markdown","e451293a":"markdown","f69d1dd9":"markdown","e9f40de9":"markdown","cc2c5292":"markdown","b26c59e5":"markdown","eec6f156":"markdown","ed0ed876":"markdown","96c476d7":"markdown"},"source":{"a0572be4":"!pip install chart_studio","a7823349":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pylab as plt\nimport seaborn\nimport warnings\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn import linear_model, metrics\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom imblearn.over_sampling import SMOTE\n\nfrom keras.layers import Input, Dense, BatchNormalization, Dropout\nfrom keras import optimizers, regularizers, initializers, Model\nimport keras.backend as K\n\nimport lightgbm\nfrom sklearn.ensemble import GradientBoostingClassifier as GBC\nfrom sklearn.ensemble import StackingClassifier as SC\nfrom sklearn.ensemble import ExtraTreesClassifier as ETC\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\nprint(tf.version.VERSION)\nprint('GPU is available' if tf.config.experimental.list_physical_devices('GPU') \n                          else 'GPU is NOT available')","96d201e3":"loan_df = pd.read_csv('..\/input\/lending-club-loan-data\/loan.csv', infer_datetime_format=True, \n                      parse_dates=['issue_d'])\nloan_df = loan_df[loan_df.issue_d.dt.year==2018]\nloan_df.info(verbose=True, null_counts=True)","4718330e":"loan_df.drop(['id', 'member_id', 'url', 'desc', 'zip_code', 'title'], axis=1, inplace=True)\nprint(loan_df.shape)","1bd3a38c":"colnames = ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate']\n\nfig, axes = plt.subplots(1, 4, figsize=(22, 4))\nfor ii, cname in enumerate(colnames) :\n    seaborn.distplot(loan_df[cname], bins=25, ax=axes[ii], color=[1-ii\/3,ii\/3,ii\/4])\n    axes[ii].set_title(cname)","bce1f081":"fig, axes = plt.subplots(1, 2, figsize=(8,3))\n    \nloan_df.loan_amnt.groupby(loan_df.issue_d.dt.year).count().plot(kind='bar', ax=axes[0], color='b')\naxes[0].set_ylabel('num. of issued loans')\naxes[0].set_xlabel('issued year')\n\nloan_df.loan_amnt.groupby(loan_df.issue_d.dt.year).sum().plot(kind='bar', color='c', ax=axes[1])\naxes[1].set_ylabel('amount of issued loans')\naxes[1].set_xlabel('issued year');","3ca638d0":"print(loan_df.loan_status.value_counts())","32779a52":"bad_loans = ['Charged Off','Late (31-120 days)','In Grace Period','Late (16-30 days)',\n             'Does not meet the credit policy. Status:Fully Paid','Default']\n\nloan_df['good_loan'] = loan_df.loan_status.apply(lambda x: 0 if x in bad_loans else 1)\nloan_df['bad_loan'] = loan_df.loan_status.apply(lambda x: 1 if x in bad_loans else 0)","ca422f1f":"fig, axes = plt.subplots(1, 2, figsize=(8,3))\n\nloan_df.loan_amnt[loan_df.good_loan==1].groupby(loan_df.issue_d.dt.year).sum().plot(kind='bar', ax=axes[0], color='b')\naxes[0].set_ylim([0, 8e9])\naxes[0].set_ylabel('Loan amount')\naxes[0].set_xlabel('Issue year')\n\nloan_df.loan_amnt[loan_df.good_loan==0].groupby(loan_df.issue_d.dt.year).sum().plot(kind='bar', ax=axes[1], color='r')\naxes[1].set_ylim([0, 8e9])\naxes[1].set_ylabel('Loan amount')\naxes[1].set_xlabel('Issue year');","a91f773d":"plt.style.use('ggplot')\nfig, axes = plt.subplots(1, 1, figsize=(18,3))\n\ncc = loan_df.corrwith(loan_df.good_loan).sort_values(ascending=False)\nmycols = [(0.5*(x+1),0,0) for x in cc]\ncc.plot(kind='bar', color=mycols, ax=axes)\naxes.set_ylabel('correlation values');","81cfdb1a":"ls = loan_df.loan_status.unique()\nstates = loan_df.addr_state.unique()\ndf = pd.DataFrame(columns=ls, index=states)\nfor col in ls :\n    df[col] = loan_df.loan_amnt[loan_df.loan_status==col].groupby(loan_df.addr_state).count()","7da490dd":"plt.style.use('ggplot')\nfig, ax = plt.subplots(1, 1, figsize=(8, 24))\n\nseaborn.set(font_scale=1.2)\nseaborn.heatmap(df, vmin=0, cmap='PiYG', center=0, annot=True, fmt='.0f', annot_kws={\"size\":10}, \n                linewidths=0.2, linecolor='white', cbar=False);","9f2814d2":"plt.style.use('seaborn')\nfig, ax = plt.subplots(1, 1, figsize=(18,4))\n\nloan_df.int_rate.groupby(loan_df.addr_state).mean().plot(color='b', lw=2, ax=ax)\nax.set_ylabel('interest rate')\n\nax2 = ax.twinx()\nloan_df.annual_inc.groupby(loan_df.addr_state).mean().plot(color='r', ax=ax2)\nax2.set_ylabel('average anuual income');","9607cf8b":"df = pd.DataFrame(columns=['st', 'avg_inc', 'int_rate', 'amnt_loans'], \n                 index=loan_df.addr_state.unique())\ndf.st = loan_df.addr_state.unique()\ndf.avg_inc = loan_df.annual_inc.groupby(loan_df.addr_state).mean()\ndf.int_rate = loan_df.int_rate.groupby(loan_df.addr_state).mean()\ndf.amnt_loans = loan_df.loan_amnt.groupby(loan_df.addr_state).sum()\ndf.head()","da287f08":"for col in df.columns:\n    df[col] = df[col].astype(str)\n\ndf['text'] = df['st'] + '<br>' +\\\n                'Average loan interest rate: ' + df['int_rate'] + '<br>'+\\\n                'Average annual income: ' + df['avg_inc'] \n\ndata = [dict(type='choropleth', colorscale=['red', 'pink', 'yellow', 'green'], autocolorscale = False,\n        locations = df['st'], z = df['amnt_loans'], locationmode = 'USA-states',\n        text = df['text'], marker = dict(line=dict(color='rgb(255,255,255)', width=2)),\n        colorbar = dict(title = \"$USD\"))]\n\nlayout = dict(title = 'Lending Clubs Issued Loans',\n              geo = dict(scope = 'usa',\n                         projection = dict(type='albers usa'),\n                         showlakes = True,\n                         lakecolor = 'rgb(255, 255, 255)'))\n\nfig = dict(data=data, layout=layout)\niplot(fig, filename='d3-US-map')","36c8e370":"cols = ['installment','annual_inc','total_pymnt','annual_inc_joint']       \nloan_df.drop(loan_df[loan_df.annual_inc>2e7].index, inplace=True)\nloan_df.drop(loan_df[loan_df.annual_inc_joint>1.9e6].index, inplace=True)\n\nplt.style.use('dark_background')\nfig, axes = plt.subplots(1, 4, figsize=(18, 3))\naxes = axes.flatten()\nfig.subplots_adjust(hspace=0.6)\ncolors=[plt.cm.prism_r(each) for each in np.linspace(0, 1, len(cols))]\nfor ii, cname in enumerate(cols) :\n    seaborn.regplot(x=loan_df[cname], y=loan_df.loan_amnt\/1e3, color=colors[ii], \n                    fit_reg=False, marker='o', scatter_kws={'s':50,'alpha':0.4}, ax=axes[ii])\n    axes[ii].set_ylabel('loan amount', fontsize=12)\n    axes[ii].set_xlabel(cname, fontsize=12)\n    axes[ii].set_title('loan amount vs. ' + cname, color=colors[ii], fontweight='bold', size=14)","4fb258ba":"lst = loan_df.emp_length.unique()\ndef emp_len (s) :\n    if s=='10+ years' : return 10\n    elif s=='9 years' : return 9\n    elif s=='8 years' : return 8\n    elif s=='7 years' : return 7\n    elif s=='6 years' : return 6\n    elif s=='5 years' : return 5\n    elif s=='4 years' : return 4\n    elif s=='3 years' : return 3\n    elif s=='2 years' : return 2\n    elif s=='1 year' : return 1\n    else: return 0\n\nloan_df['emp_len_yrs'] = loan_df.emp_length.apply(emp_len)","ba86a21c":"by_status = pd.crosstab(loan_df.addr_state, loan_df.good_loan)\ndf = pd.DataFrame(columns=['st', 'total_ratio', 'good_loan_ratio', 'bad_loan_num',\n                           'avg_dti', 'avg_emp_len'], \n                 index=loan_df.addr_state.unique())\n\ndf.st = loan_df.addr_state.unique()\ndf.bad_loan_num = (loan_df.good_loan.groupby(loan_df.addr_state).count() - \n                   loan_df.good_loan.groupby(loan_df.addr_state).sum())\ndf.total_ratio = loan_df.good_loan.groupby(loan_df.addr_state).count()\/loan_df.shape[0]\ndf.good_loan_ratio = (loan_df.good_loan.groupby(loan_df.addr_state).sum()\/\n                      loan_df.good_loan.groupby(loan_df.addr_state).count())\ndf.avg_dti = loan_df.dti.groupby(loan_df.addr_state).mean()\ndf.avg_emp_len = loan_df.emp_len_yrs.groupby(loan_df.addr_state).mean()\ndf.head()","e3618ce2":"for col in df.columns:\n    df[col] = df[col].astype(str)\n    \ndf['text'] = df['st'] + '<br>' +\\\n                'Nation-wide num. loan ratio: ' + df.total_ratio + '<br>'+\\\n                'Good loan ratio: ' + df.good_loan_ratio + '<br>'+\\\n                'Avearge debt-to-income: ' + df.avg_dti\n\ndata = [dict(type='choropleth', autocolorscale = False, colorscale=['red', 'pink', 'yellow', 'green'],\n        locations = df['st'], z = df.good_loan_ratio, locationmode = 'USA-states',\n        text = df['text'], marker = dict(line=dict(color='rgb(255,255,255)', width=2)),\n        colorbar = dict(title = \"ratio\"))]\n\nlayout = dict(title = 'Lending Clubs Good Loans',\n              geo = dict(scope = 'usa',\n                         projection = dict(type='albers usa'),\n                         showlakes = True,\n                         lakecolor = 'rgb(255, 255, 255)'))\n\nfig = dict(data=data, layout=layout)\niplot(fig, filename='d3-US-map')","7e89dc58":"## Simple working example\ndff = loan_df.loc[9530:9540, ['addr_state', 'grade', 'good_loan']]\nprint(dff)\nprint(pd.crosstab(dff.addr_state, dff.grade, values=dff.good_loan, aggfunc='sum'))","aa270b2a":"plt.style.use('ggplot')\nfig, ax = plt.subplots(1, 2, figsize=(14, 12))\nseaborn.set(font_scale=1.6)\n\n## num of good loans in each grade and state divided by the total number of loans in each state\nby_state_grade = pd.crosstab(loan_df.addr_state, loan_df.grade, values=loan_df.good_loan, aggfunc='sum')\nfor cname in by_state_grade.columns :\n    by_state_grade[cname] = by_state_grade[cname]\/loan_df.good_loan.groupby(loan_df.addr_state).count()\nseaborn.heatmap(by_state_grade, vmin=0, vmax=0.35, cmap='PiYG', center=0, annot=True, \n                fmt='.3f', annot_kws={\"size\":10}, linewidths=0.2, linecolor='white', \n                cbar=True, ax=ax[0], cbar_kws={\"shrink\": 0.5});\nax[0].set_title('ratio of good loans by state and grade\\n to all loans in each state')\n\n## num of bad loans in each grade and state divided by the total number of loans in each state\nby_state_grade = pd.crosstab(loan_df.addr_state, loan_df.grade, values=loan_df.bad_loan, aggfunc='sum')\nfor cname in by_state_grade.columns :\n    by_state_grade[cname] = by_state_grade[cname]\/loan_df.good_loan.groupby(loan_df.addr_state).count()\nseaborn.heatmap(by_state_grade, vmin=0, vmax=0.35, cmap='PiYG', center=0, annot=True, \n                fmt='.3f', annot_kws={\"size\":10}, linewidths=0.2, linecolor='white', \n                cbar=True, ax=ax[1], cbar_kws={\"shrink\": 0.5});\nax[1].set_title('ratio of bad loans by state and grade\\n to all loans in each state');","adcec33a":"plt.style.use('ggplot')\nfig, ax = plt.subplots(1, 2, figsize=(14, 12))\nseaborn.set(font_scale=1.3)\n\n## num of good loans in each grade and state divided by the total number of loans in each state and grade\nby_state_grade = (pd.crosstab(loan_df.addr_state, loan_df.grade, values=loan_df.good_loan, aggfunc='sum')\/\n                  pd.crosstab(loan_df.addr_state, loan_df.grade, values=loan_df.good_loan, aggfunc='count'))\nseaborn.heatmap(by_state_grade, cmap='PiYG', center=0, annot=True, fmt='.3f', annot_kws={\"size\":10}, \n                linewidths=0.1, linecolor='white', cbar=True, ax=ax[0], cbar_kws={\"shrink\": 0.5});\nax[0].set_title('ratio of good loans by state and grade\\n to all loans in each state and grade')\n\n## num of bad loans in each grade and state divided by the total number of loans in each state and grade\nby_state_grade = (pd.crosstab(loan_df.addr_state, loan_df.grade, values=loan_df.bad_loan, aggfunc='sum')\/\n                  pd.crosstab(loan_df.addr_state, loan_df.grade, values=loan_df.bad_loan, aggfunc='count'))\nseaborn.heatmap(by_state_grade, cmap='PiYG', center=0, annot=True, fmt='.3f', annot_kws={\"size\":10}, \n                linewidths=0.1, linecolor='white', cbar=True, ax=ax[1], cbar_kws={\"shrink\": 0.5});\nax[1].set_title('ratio of bad loans by state and grade\\n to all loans in each state and grade');","67522cce":"fig, axes = plt.subplots(1, 2, figsize=(25, 6))\n\ndf = loan_df.good_loan.groupby(loan_df.purpose).sum()\ndf.plot(kind='barh', ax=axes[0], colormap='spring')\naxes[0].set_xlabel('count')\naxes[0].set_title('Number of good loans')\n\ndf = loan_df.good_loan.groupby(loan_df.purpose).sum()\/loan_df.good_loan.groupby(loan_df.purpose).count()\ndf.plot(kind='barh', ax=axes[1], colormap='spring')\naxes[1].set_xlabel('ratio')\naxes[1].set_title('Ratio of good loans')","5b84805b":"loan_df.drop(['emp_length','loan_status','earliest_cr_line','inq_last_6mths','initial_list_status',\n              'out_prncp','out_prncp_inv','total_pymnt','total_pymnt_inv','total_rec_prncp',\n              'total_rec_int','total_rec_late_fee','recoveries','collection_recovery_fee','last_pymnt_d',\n              'last_pymnt_amnt','next_pymnt_d','last_credit_pull_d','collections_12_mths_ex_med',\n              'mths_since_last_major_derog','policy_code','annual_inc_joint','dti_joint',\n              'verification_status_joint','tot_coll_amt','open_acc_6m','open_act_il','open_il_12m',\n              'open_il_24m','mths_since_rcnt_il','total_bal_il','il_util','open_rv_12m','open_rv_24m',\n              'all_util','total_cu_tl','inq_last_12m','mths_since_recent_bc_dlq','num_tl_120dpd_2m',\n              'num_tl_30dpd','num_tl_90g_dpd_24m','num_tl_op_past_12m','revol_bal_joint',\n              'sec_app_earliest_cr_line','sec_app_inq_last_6mths','sec_app_mort_acc','sec_app_open_acc',\n              'sec_app_revol_util','sec_app_open_act_il','sec_app_num_rev_accts','sec_app_chargeoff_within_12_mths',\n              'sec_app_collections_12_mths_ex_med','sec_app_mths_since_last_major_derog','hardship_flag',\n              'hardship_type','hardship_reason','hardship_status','deferral_term','hardship_amount',\n              'hardship_start_date','hardship_end_date','payment_plan_start_date','hardship_length',\n              'hardship_dpd','hardship_loan_status','hardship_payoff_balance_amount','issue_d',\n              'hardship_last_payment_amount','debt_settlement_flag','debt_settlement_flag_date',\n              'settlement_status','settlement_date','settlement_amount','settlement_percentage',\n              'orig_projected_additional_accrued_interest','settlement_term','bad_loan'], axis=1, inplace=True)","1b3eaeb0":"cols = ['term','grade','sub_grade','emp_title','home_ownership','verification_status',\n        'pymnt_plan','purpose','addr_state','application_type','disbursement_method']\nfor cname in cols :\n    le = LabelEncoder()\n    le.fit(list(loan_df[cname]))\n    loan_df[cname] = le.transform(list(loan_df[cname].values))","92a60235":"good_loan = loan_df.good_loan\nloan_df.drop(['good_loan'], axis=1, inplace=True)","a0a5b0de":"good_loan = good_loan.to_numpy()","3d16f231":"iter_impute = IterativeImputer(max_iter=10, tol=0.001, initial_strategy='mean')\n\ndata = loan_df.to_numpy()\niter_impute.fit(data[:, [10,15,16,17,18,19,20,22,23,25]])\ndata[:, [10,15,16,17,18,19,20,22,23,25]] = iter_impute.transform(data[:, [10,15,16,17,18,19,20,22,23,25]])","f2516228":"iter_impute = IterativeImputer(max_iter=10, tol=0.001, initial_strategy='mean')\niter_impute.fit(data[:, range(26,36)])\ndata[:, range(26,36)] = iter_impute.transform(data[:, range(26,36)])","082572ee":"iter_impute = IterativeImputer(max_iter=10, tol=0.001, initial_strategy='mean')\niter_impute.fit(data[:, range(36,46)])\ndata[:, range(36,46)] = iter_impute.transform(data[:, range(36,46)])","bc2420a2":"iter_impute = IterativeImputer(max_iter=10, tol=0.001, initial_strategy='mean')\niter_impute.fit(data[:, range(46,56)])\ndata[:, range(46,56)] = iter_impute.transform(data[:, range(46,56)])","9d355fb5":"iter_impute = IterativeImputer(max_iter=10, tol=0.001, initial_strategy='mean')\niter_impute.fit(data[:, range(56,64)])\ndata[:, range(56,64)] = iter_impute.transform(data[:, range(56,64)])","ac16c6b1":"assert(np.any(np.isnan(data)) == False)\nassert(np.any(np.isnan(good_loan)) == False)","8c39cf6b":"sss = StratifiedShuffleSplit(n_splits=2, test_size=0.1)\nfor train_idx, test_idx in sss.split(data, good_loan) :\n    X_train, X_test = data[train_idx, :], data[test_idx, :]\n    y_train, y_test = good_loan[train_idx], good_loan[test_idx]\n    \nprint(X_train.shape, X_test.shape)\nprint(y_train.sum()\/y_train.shape[0], ' vs. ', y_test.sum()\/y_test.shape[0])","22729ec4":"# sm = SMOTE()\n# X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n# X_test_sm, y_test_sm = sm.fit_resample(X_test, y_test)\n\n# r1 = y_train.sum()\/y_train.shape[0]\n# r2 = y_train_sm.sum()\/y_train_sm.shape[0]\n# print('Before sampling--good loan ratio: %0.4f, bad loan ratio: %0.4f' %(r1, 1-r1))\n# print('After sampling--good loan ratio: %0.4f, bad loan ratio: %0.4f' %(r2, 1-r2))\n# print(X_train.shape, X_train_sm.shape)","9d88872e":"X_train_sm = StandardScaler().fit_transform(X_train)\nX_test_sm = StandardScaler().fit_transform(X_test)\n\ny_train_sm = y_train\ny_test_sm = y_test","a527bdf6":"def loan_model (input_shape) :\n    x_input = Input(input_shape)\n\n    x = Dense(64, activation='tanh', kernel_initializer='glorot_uniform')(x_input)\n    x = BatchNormalization(epsilon=0.01, momentum=0.99)(x)\n    \n    x = Dense(256, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(x)\n    x = BatchNormalization(epsilon=0.01, momentum=0.99)(x)\n    \n    x = Dense(128, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(x)\n    x = BatchNormalization(epsilon=0.01, momentum=0.99)(x)\n    \n    x = Dense(16, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(x)\n    x = BatchNormalization(epsilon=0.01, momentum=0.99)(x)\n\n    x_output = Dense(1, activation='sigmoid', use_bias=True, kernel_regularizer=regularizers.l2(0.01),\n              bias_regularizer=regularizers.l2(0.02))(x)\n\n    model = Model(inputs=x_input, outputs=x_output, name='loan_model')\n    \n    return model","b716a5c0":"loanModel = loan_model(np.shape(X_train_sm[1,:]))\nprint(loanModel.summary())","1154b6c9":"optim = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.99)\nloanModel.compile(optimizer=optim, loss='mean_squared_error', metrics=['accuracy'])","75dfc9ac":"loanModel.fit(x=X_train_sm, y=y_train_sm, batch_size=2048, epochs=50, verbose=1, \n              shuffle=True, validation_data=(X_test_sm, y_test_sm))","c236107e":"fig, axes = plt.subplots(1, 2, figsize=(8,3))\n\naxes[0].plot(loanModel.history.history['loss'], 'r', label='train')\naxes[0].plot(loanModel.history.history['val_loss'], 'b', label='train')\naxes[0].set_ylabel('Loss')\naxes[0].legend()\n\naxes[1].plot(loanModel.history.history['accuracy'], 'r', label='train')\naxes[1].plot(loanModel.history.history['val_accuracy'], 'b', label='train')\naxes[1].set_ylabel('accuracy')\naxes[1].legend()","af520f80":"log_reg = linear_model.LogisticRegression()\nlog_reg.fit(X_train_sm, y_train_sm)\ny_pred = log_reg.predict(X_test_sm)\nprint('accuracy: %0.4f' %(metrics.accuracy_score(y_test_sm, y_pred)))","5af394be":"gbc = GBC(loss='deviance', learning_rate=0.02, n_estimators=10, min_samples_split=2, \n          min_samples_leaf=2, max_depth=3, verbose=1, tol=0.0001)\ngbc.fit(X_train_sm, y_train_sm)\ny_pred = gbc.predict(X_test_sm)\nprint('accuracy: %0.4f' %(metrics.accuracy_score(y_test_sm, y_pred)))","3cde7eab":"import gc\ngc.collect()","9146b985":"lgbm = lightgbm.LGBMClassifier(boosting_type='gbdt', num_leaves=14, max_depth=8, learning_rate=0.02, \n                               n_estimators=100, min_child_weight=0.001, min_child_samples=20, \n                               n_jobs=-1, verbose=1)\nlgbm.fit(X_train_sm, y_train_sm)\ny_pred = lgbm.predict(X_test_sm)\nprint('accuracy: %0.4f' %(metrics.accuracy_score(y_test_sm, y_pred)))","81973dfe":"sgd = linear_model.SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, \n                               fit_intercept=True, max_iter=1000, tol=0.0001, shuffle=True, \n                               verbose=0, epsilon=0.1, n_jobs=-1, learning_rate='optimal')\nsgd.fit(X_train_sm, y_train_sm)\ny_pred = sgd.predict(X_test_sm)\nprint('accuracy: %0.4f' %(metrics.accuracy_score(y_test_sm, y_pred)))","5553d04b":"estimators = [('log', log_reg),   \n              ('sgd', sgd),   \n              ('gbc', gbc),   \n              ('lgbm', lgbm)]\n\nstack_class = SC(estimators=estimators, final_estimator=ETC(n_estimators=1), n_jobs=-1, verbose=2)\nstack_class.fit(X_train_sm, y_train_sm)\ny_pred = stack_class.predict(X_test_sm)\nprint('accuracy: %0.4f' %(metrics.accuracy_score(y_test_sm, y_pred)))","3126817d":"## 6. Loan condition for each state","c24789af":"## 4. Separating loans into \"good\" and \"bad\"","7dbfedd2":"## 17.1.Logistic Regression","853face9":"## 17.3.LightGBM","9a926cdf":"## 12. Replace missing values","f367796a":"## **17.4. Stocastic Gradient Descent**","da30f620":"## 1. Load libraries","f3577ead":"## 10. Drop irrelevant columns","0786dd80":"## 16. Building a model in keras","9b383c49":"## 13. Separate data into train and test","0169ad03":"## 3. Plot some columns","6ac92bab":"## 2. Load data","8ce7be6b":"## 17.4.Stacking Classifiers","cf0821d4":"## 7. Two very important factors\n\nTwo very important factors are:\n- Debt-to-income `dti` which is an indication of the level of debt of each individual consumer with respect to its total income\n- Average length of employment `emp_len` which is an indication of how stable labor market is in that state","13ea2aa3":"## 9. Purpose of the loan is important","c581aa02":"## 11. Turn categorical columns into numerics","e451293a":"Remove obviously irrelevant columns!","f69d1dd9":"## 8. One more important factor\n\n- Credit scores `grade` indicate the overall level of risk. In general, the lower the grade of the credit score, the higher the risk for investors. In this section we look at the grade but a finer credit score `sub-grade` will also be used in our classification models","e9f40de9":"## 5. Plot good and bad loans","cc2c5292":"California, Texas, New York and Florida are the states in which the highest amount of loans were issued. Annual income in these states is above the average annual income (maybe not Florida)","b26c59e5":"# Lending Club: Risk Analysis and Gradient Boosting\n\nHere is a short description of the data. For more info see: https:\/\/www.kaggle.com\/wendykan\/lending-club-loan-data\n\nThese files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of about 890 thousand observations and 75 variables. A data dictionary is provided in a separate file.","eec6f156":"## 15. z-score all columns","ed0ed876":"## 14. Resmple for a more balanced dataset","96c476d7":"## 17.2.Gradient Boosting Regressor"}}