{"cell_type":{"fabe254b":"code","72ce3c84":"code","f28c58c0":"code","10ffa2b7":"code","9ce697d0":"code","626fc47d":"code","f37ee5de":"code","00bbbe4c":"code","78c1f4dc":"code","0e17742f":"code","a18ec2cd":"code","8cca85bb":"code","6c69ae7f":"code","f958a99e":"code","58c5f791":"code","bf65d310":"code","ae01f9bd":"code","8404d3d5":"code","4205574c":"code","b190e459":"code","ffad0b9f":"code","49edb666":"code","8e8d9023":"code","4b63f64a":"code","09c1f5a2":"code","c5fa4ac6":"code","fc304692":"code","b9ba2593":"code","7505b9a0":"code","81cd1819":"code","e89bd565":"code","4237cdbb":"code","e1475183":"code","08c602a2":"code","b04564e9":"code","1565e11a":"code","638f266d":"code","f93ac15a":"code","012ce1bb":"code","9c371703":"code","8d41567c":"code","961eb5e5":"markdown","4c678a42":"markdown","01d5fe21":"markdown","95b8bb2f":"markdown","b50eebd4":"markdown"},"source":{"fabe254b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom kaggle.competitions import twosigmanews\nfrom collections import Counter\nfrom sklearn import linear_model\nfrom xgboost import XGBRegressor\nimport scipy\nimport lightgbm as lgb\nimport itertools\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint, truncnorm, uniform\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\npd.options.mode.chained_assignment = None","72ce3c84":"env = twosigmanews.make_env()\n(market_train, _) = env.get_training_data()","f28c58c0":"market_train.shape","10ffa2b7":"market_train[\"time\"] = market_train[\"time\"].apply(lambda x: pd.Timestamp(x))","9ce697d0":"# first date\nmin_time = np.min(market_train[\"time\"])\nprint(min_time)","626fc47d":"# last date\nmax_time = np.max(market_train[\"time\"])","f37ee5de":"def get_score(df_):\n    assert \"pred\" in df_.columns\n    df_[\"score\"] = df_[\"returnsOpenNextMktres10\"] * \\\n        df_[\"pred\"] * df_[\"universe\"]\n    x_t_sum = df_.groupby(\"time\").sum()[\"score\"]\n    score = x_t_sum.mean() \/ x_t_sum.std()\n    return score\n\ndef sigma_score_lgb_f(preds, valid_data, df_valid_):\n    df_valid_[\"pred\"] = preds\n    score = get_score(df_valid_)\n    \n    return 'sigma_score', score, True\n\ndef get_market_X(df_,\n                 X_columns=['volume',\n                            'close',\n                            'open',\n                            'returnsClosePrevRaw1',\n                            'returnsOpenPrevRaw1',\n                            'returnsClosePrevMktres1',\n                            'returnsOpenPrevMktres1',\n                            'returnsClosePrevRaw10',\n                            'returnsOpenPrevRaw10',\n                            'returnsClosePrevMktres10',\n                            'returnsOpenPrevMktres10'],\n                 fillna_value=-1000):\n\n    X = df_[X_columns]\n    X.fillna(fillna_value, inplace=True)\n    X = X.values\n    return X\n\n\ndef get_lgb_dataset(df_,\n                    X_columns=['volume',\n                            'close',\n                            'open',\n                            'returnsClosePrevRaw1',\n                            'returnsOpenPrevRaw1',\n                            'returnsClosePrevMktres1',\n                            'returnsOpenPrevMktres1',\n                            'returnsClosePrevRaw10',\n                            'returnsOpenPrevRaw10',\n                            'returnsClosePrevMktres10',\n                            'returnsOpenPrevMktres10']):\n    \n\n    X = get_market_X(df_)\n    y = df_['returnsOpenNextMktres10'].clip(-1, 1)\n    return lgb.Dataset(X, y, feature_name=X_columns, free_raw_data=False)\n\n\n\n\ndef get_trained_model_and_score(train_beginning,\n                                train_end,\n                                valid_beginning,\n                                valid_end,\n                                base_df,\n                                model):\n    # Selecting time\n\n    df_train = base_df.loc[(base_df[\"time\"] > pd.Timestamp(train_beginning, tz='UTC')) & (base_df[\"time\"] < pd.Timestamp(train_end, tz='UTC'))]\n    df_valid = base_df.loc[(base_df[\"time\"] > pd.Timestamp(valid_beginning, tz='UTC')) & (base_df[\"time\"] < pd.Timestamp(valid_end, tz='UTC'))]\n\n    # Creating X and y arrays\n\n    X_train = get_market_X(df_train)\n    y_train = df_train[['returnsOpenNextMktres10']].values\n    X_valid = get_market_X(df_valid)\n\n    # Training the model, storing the prediction, and getting score\n    model.fit(X_train, y_train)\n    df_valid[\"pred\"] = np.clip(model.predict(X_valid), -1, 1)\n    score = get_score(df_valid)\n\n    return model, score\n\n\ndef get_trained_model_and_score_lgb(train_beginning,\n                                    train_end,\n                                    valid_beginning,\n                                    valid_end,\n                                    base_df,\n                                    lgb_params_):\n    # Selecting time\n\n    df_train = base_df.loc[(base_df[\"time\"] > pd.Timestamp(train_beginning, tz='UTC')) & (base_df[\"time\"] < pd.Timestamp(train_end, tz='UTC'))]\n    df_valid = base_df.loc[(base_df[\"time\"] > pd.Timestamp(valid_beginning, tz='UTC')) & (base_df[\"time\"] < pd.Timestamp(valid_end, tz='UTC'))]\n    \n    sigma_score_lgb = lambda x,y: sigma_score_lgb_f(x,y,df_valid)\n    \n    lgb_train =  get_lgb_dataset(df_train)\n    lgb_valid =  get_lgb_dataset(df_valid)\n\n    evals_result = {}\n    model = lgb.train(lgb_params_,\n                      lgb_train,\n                      num_boost_round=1000,\n                      valid_sets=(lgb_valid,),\n                      valid_names=('valid',),\n                      verbose_eval=25,\n                      early_stopping_rounds=100,\n                      feval=sigma_score_lgb,\n                      evals_result=evals_result)\n\n    score = np.max(evals_result['valid']['sigma_score'])\n\n    return model, score\n","00bbbe4c":"# lgb_params = dict(\n#     objective = 'regression_l1',\n#     learning_rate = 0.1,\n#     num_leaves = 10,\n#     max_depth = -1,\n# #     min_data_in_leaf = 1000,\n# #     min_sum_hessian_in_leaf = 10,\n#     bagging_fraction = 0.75,\n#     bagging_freq = 2,\n#     feature_fraction = 0.5,\n#     lambda_l1 = 0.0,\n#     lambda_l2 = 1.0,\n#     metric = 'None', # This will ignore the loss objetive and use sigma_score instead,\n#     seed = 42 # Change for better luck! :)\n# )","78c1f4dc":"# year = 2008\n# train_beginning=\"{}-01-01\".format(year)\n# train_end=\"{}-01-01\".format(year+1)\n# valid_beginning=\"{}-01-01\".format(year+1)\n# valid_end=\"{}-01-01\".format(year+2)\n\n# get_trained_model_and_score_lgb(train_beginning=train_beginning,\n#                                     train_end=train_end,\n#                                     valid_beginning=valid_beginning,\n#                                     valid_end=valid_end,\n#                                     base_df=market_train,\n#                                     lgb_params_=lgb_params)","0e17742f":"# year = 2012\n# train_beginning=\"{}-01-01\".format(year)\n# train_end=\"{}-01-01\".format(year+1)\n# valid_beginning=\"{}-01-01\".format(year+1)\n# valid_end=\"{}-01-01\".format(year+2)\n\n# m0,score = get_trained_model_and_score_lgb(train_beginning=train_beginning,\n#                                     train_end=train_end,\n#                                     valid_beginning=valid_beginning,\n#                                     valid_end=valid_end,\n#                                     base_df=market_train,\n#                                     lgb_params_=lgb_params)","a18ec2cd":"last_valid_df = market_train.loc[(market_train[\"time\"] > pd.Timestamp(\"2016-01-01\", tz='UTC')) & (market_train[\"time\"] < pd.Timestamp(\"2017-01-01\", tz='UTC'))]\nX_valid_last = get_market_X(last_valid_df) ","8cca85bb":"def predict_market_using_model(market_obs_df_, predictions_template_df_, model_):\n    X = get_market_X(market_obs_df_)\n    market_obs_df_[\"pred\"] = np.clip(model_.predict(X), -1, 1)\n    pred_dict = (market_obs_df_.set_index(\"assetCode\")[\"pred\"]).to_dict()\n    pred_dict_f = lambda x: pred_dict[x] if x in pred_dict else 0.0 \n    predictions_template_df_[\"confidenceValue\"] = predictions_template_df_[\"assetCode\"].apply(pred_dict_f)\n    return predictions_template_df_\n\ndef write_sub_using_model(model_):\n    days = env.get_prediction_days()\n    for (market_obs_df, _ , predictions_template_df) in days:\n        predictions_template_df_pred = predict_market_using_model(market_obs_df, predictions_template_df, model_)\n        env.predict(predictions_template_df_pred)\n    env.write_submission_file()\n    print('Done!')","6c69ae7f":"years = range(2007, 2016)\n\nall_XGB_models = []\nall_XGB_scores = []\n\nfor year in years:\n    lmodel = XGBRegressor()\n    train_beginning=\"{}-01-01\".format(year)\n    train_end=\"{}-01-01\".format(year+1)\n    valid_beginning=\"{}-01-01\".format(year+1)\n    valid_end=\"{}-01-01\".format(year+2)\n    model, score = get_trained_model_and_score(train_beginning=train_beginning,\n                                               train_end=train_end,\n                                               valid_beginning=valid_beginning,\n                                               valid_end=valid_end,\n                                               base_df=market_train,\n                                               model=lmodel)\n    all_XGB_models.append(model)\n    all_XGB_scores.append(score)\n    print(\"train: {}   -- {}\".format(train_beginning, train_end))\n    print(\"valid: {}   -- {}\".format(valid_beginning, valid_end))\n\n    print(\"valid score = {:.5f}\".format(score))\n    print()\n    \nprint(\"mean score = {:.5f}\".format(np.mean(all_XGB_scores)))\nprint(\"std score = {:.5f}\".format(np.std(all_XGB_scores)))","f958a99e":"years = range(2007, 2016)\n\nall_lgb_models = []\nall_lgb_scores = []\n\nlgb_params = dict(\n    objective = 'regression_l1',\n    learning_rate = 0.1,\n    num_leaves = 10,\n    max_depth = -1,\n#     min_data_in_leaf = 1000,\n#     min_sum_hessian_in_leaf = 10,\n    bagging_fraction = 0.75,\n    bagging_freq = 2,\n    feature_fraction = 0.5,\n    lambda_l1 = 0.0,\n    lambda_l2 = 1.0,\n    metric = 'None', # This will ignore the loss objetive and use sigma_score instead,\n    seed = 42 # Change for better luck! :)\n)\n\n\nfor year in years:\n    train_beginning=\"{}-01-01\".format(year)\n    train_end=\"{}-01-01\".format(year+1)\n    valid_beginning=\"{}-01-01\".format(year+1)\n    valid_end=\"{}-01-01\".format(year+2)\n    model, score = get_trained_model_and_score_lgb(train_beginning=train_beginning,\n                                                   train_end=train_end,\n                                                   valid_beginning=valid_beginning,\n                                                   valid_end=valid_end,\n                                                   base_df=market_train,\n                                                   lgb_params_=lgb_params)\n\n    all_lgb_models.append(model)\n    all_lgb_scores.append(score)\n    print(\"train: {}   -- {}\".format(train_beginning, train_end))\n    print(\"valid: {}   -- {}\".format(valid_beginning, valid_end))\n\n    print(\"valid score = {:.5f}\".format(score))\n    print()\n    \nprint(\"mean score = {:.5f}\".format(np.mean(all_lgb_scores)))\nprint(\"std score = {:.5f}\".format(np.std(all_lgb_scores)))","58c5f791":"print(all_XGB_scores)\nprint(\"mean score = {:.5f}\".format(np.mean(all_XGB_scores)))\nprint(\"std score = {:.5f}\".format(np.std(all_XGB_scores)))","bf65d310":"print(all_lgb_scores)\nprint(\"mean score = {:.5f}\".format(np.mean(all_lgb_scores)))\nprint(\"std score = {:.5f}\".format(np.std(all_lgb_scores)))","ae01f9bd":"class CombinedModel:\n    def __init__(self, model_list, weigths=None):\n        self.model_list = model_list\n        if weigths is None:\n            weigths = np.random.randint(1,100,len(model_list))\n            self.weigths = weigths \/ np.sum(weigths)\n        else:\n            self.weigths =  weigths\n\n    \n    def predict(self, X):\n        pred = np.zeros((X.shape[0],))\n        for model, weigth in zip(self.model_list,  self.weigths):\n            pred += model.predict(X) *  weigth\n        return pred","8404d3d5":"model2score ={}","4205574c":"w = np.clip(all_lgb_scores, 0, float(\"inf\"))\nw = w * 10\n\ncombined_lgb = CombinedModel(all_lgb_models, weigths=w)\n\nlast_valid_df[\"pred\"] = np.clip(combined_lgb.predict(X_valid_last), -1, 1)\nscore = get_score(last_valid_df)\nprint(score)\nmodel2score[\"combined_lgb\"] = score","b190e459":"w = np.clip(all_XGB_scores, 0, float(\"inf\"))\nw = w * 10\n\ncombined_XGB = CombinedModel(all_XGB_models, weigths=w)\n\nlast_valid_df[\"pred\"] = np.clip(combined_XGB.predict(X_valid_last), -1, 1)\nscore = get_score(last_valid_df)\nprint(score)\nmodel2score[\"combined_XGB\"] = score","ffad0b9f":"w = np.array([np.mean(all_lgb_scores), np.mean(all_XGB_scores)])\nw = w * 10\n# w = [1,1]\nw = w \/ np.sum(w)\n\ncombined_list = [combined_lgb, combined_XGB]\n\n\ncombined_m = CombinedModel(combined_list, weigths=w)\nlast_valid_df[\"pred\"] = np.clip(combined_m.predict(X_valid_last), -1, 1)\nscore = get_score(last_valid_df)\nprint(score)\nmodel2score[\"combined_m\"] = score","49edb666":"lmodel = XGBRegressor(n_jobs=4, n_estimators=374, max_depth=7, eta=0.51, reg_lambda=5.0, gamma=0.127)\ntrain_beginning=\"2007-01-01\"\ntrain_end=\"2016-01-01\"\nvalid_beginning=\"2016-01-01\"\nvalid_end=\"2017-01-01\"\n\nXGBRmodel_all, score = get_trained_model_and_score(train_beginning=train_beginning,\n                                                   train_end=train_end,\n                                                   valid_beginning=valid_beginning,\n                                                   valid_end=valid_end,\n                                                   base_df=market_train,\n                                                   model=lmodel)\n\nprint(\"train: {}   -- {}\".format(train_beginning, train_end))\nprint(\"valid: {}   -- {}\".format(valid_beginning, valid_end))\n\nprint(\"valid score = {:.5f}\".format(score))\nprint()\nlast_valid_df[\"pred\"] = np.clip(XGBRmodel_all.predict(X_valid_last), -1, 1)\nscore = get_score(last_valid_df)\nprint(score)\nmodel2score[\"XGBRmodel_all\"] = score","8e8d9023":"train_beginning=\"2007-01-01\"\ntrain_end=\"2016-01-01\"\nvalid_beginning=\"2016-01-01\"\nvalid_end=\"2017-01-01\"\n\nlgbmodel_all, score = get_trained_model_and_score_lgb(train_beginning=train_beginning,\n                                               train_end=train_end,\n                                               valid_beginning=valid_beginning,\n                                               valid_end=valid_end,\n                                               base_df=market_train,\n                                               lgb_params_=lgb_params)\n\nprint(\"train: {}   -- {}\".format(train_beginning, train_end))\nprint(\"valid: {}   -- {}\".format(valid_beginning, valid_end))\n\nprint(\"valid score = {:.5f}\".format(score))\nprint()\nlast_valid_df[\"pred\"] = np.clip(lgbmodel_all.predict(X_valid_last), -1, 1)\nscore = get_score(last_valid_df)\nprint(score)\nmodel2score[\"lgbmodel_all\"] = score","4b63f64a":"lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n\ndef prep_data(market_data):\n    # add asset code representation as int (as in previous kernels)\n    market_data['assetCodeT'] = market_data['assetCode'].map(lbl)\n    market_col = ['assetCodeT', 'volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', \n                        'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', \n                        'returnsOpenPrevMktres10']\n    # select relevant columns, fillna with zeros (where dropped in previous kernels that I saw)\n    # getting rid of time, assetCode (keep int representation assetCodeT), assetName, universe\n    #market_data = market_data[market_data['universe'] == True]\n    X = market_data[market_col].fillna(0).values\n    if \"returnsOpenNextMktres10\" in list(market_data.columns):#if training data\n        up = (market_data.returnsOpenNextMktres10 >= 0).values\n        r = market_data.returnsOpenNextMktres10.values\n        universe = market_data.universe\n        day = market_data.time.dt.date\n        assert X.shape[0] == up.shape[0] == r.shape[0] == universe.shape[0] == day.shape[0]\n    else:#observation data without labels\n        up = []\n        r = []\n        universe = []\n        day = []\n    return X, up, r, universe, day","09c1f5a2":"train_beginning=\"2007-01-01\"\ntrain_end=\"2016-01-01\"\ntrain_df = market_train.loc[(market_train[\"time\"] > pd.Timestamp(train_beginning, tz='UTC')) & (market_train[\"time\"] < pd.Timestamp(train_end, tz='UTC'))]","c5fa4ac6":"X, up, r, universe, day = prep_data(train_df)\n\n# r, u and d are used to calculate the scoring metric on test\nX_train, X_test, up_train, up_test, _, r_test, _, u_test, _, d_test = \\\ntrain_test_split(X, up, r, universe, day, test_size=0.25, random_state=99)","fc304692":"xgb_market = XGBClassifier(n_jobs=4, n_estimators=374, max_depth=7, eta=0.51, reg_lambda=5.0, gamma=0.127)\nt = time.time()\nprint('Fitting Up')\nxgb_market.fit(X_train,up_train)\nprint(f'Done, time = {time.time() - t}s')","b9ba2593":"def gen_conf(prediction_probs):\n    new_conf  = np.empty([prediction_probs.shape[0]])\n    for i in range(prediction_probs.shape[0]):\n        if abs(prediction_probs[i][0] - prediction_probs[i][1]) < 0.01:\n            new_conf[i] = 0\n        elif prediction_probs[i][0] > prediction_probs[i][1]:\n            new_conf[i] = - 1\n        else:\n            new_conf[i] = 1\n    return new_conf\n\nclass ClassificationModel:\n    def __init__(self, model):\n        self.model = model\n    \n    def predict(self, X):\n        pred = gen_conf(self.model.predict_proba(X))\n        return pred","7505b9a0":"xgb_classification =  ClassificationModel(model=xgb_market)","81cd1819":"X, up, r, universe, day = prep_data(last_valid_df)\n\nlast_valid_df[\"pred\"] = np.clip(xgb_classification.predict(X), -1, 1)\nscore = get_score(last_valid_df)\nprint(score)\nmodel2score[\"xgb_classification\"] = score\nlast_valid_df[\"xgb_classification\"] = last_valid_df[\"pred\"]","e89bd565":"for i,m in enumerate(all_XGB_models):\n    name = \"XGB_model_{}\".format(i) \n    last_valid_df[\"pred\"] = np.clip(m.predict(X_valid_last), -1, 1)\n    score = get_score(last_valid_df)    \n    last_valid_df[name] = last_valid_df[\"pred\"]\n    model2score[name] = score","4237cdbb":"for i,m in enumerate(all_lgb_models):\n    name = \"lgb_model_{}\".format(i) \n    last_valid_df[\"pred\"] = np.clip(m.predict(X_valid_last), -1, 1)\n    score = get_score(last_valid_df)    \n    last_valid_df[name] = last_valid_df[\"pred\"]\n    model2score[name] = score","e1475183":"model2score","08c602a2":"last_valid_df[\"combined_lgb\"] = np.clip(combined_lgb.predict(X_valid_last), -1, 1)\nlast_valid_df[\"combined_m\"] = np.clip(combined_m.predict(X_valid_last), -1, 1)\nlast_valid_df[\"XGBRmodel_all\"] = np.clip(XGBRmodel_all.predict(X_valid_last), -1, 1)\nlast_valid_df[\"lgbmodel_all\"] = np.clip(lgbmodel_all.predict(X_valid_last), -1, 1)\nlast_valid_df[\"combined_XGB\"] = np.clip(combined_XGB.predict(X_valid_last), -1, 1)","b04564e9":"select = list(model2score.keys())\nselect = [m for m in select if model2score[m] > 0.4]\nlast_valid_df_simple = last_valid_df[select]\npred_corr = last_valid_df_simple.corr()","1565e11a":"def plot_corr(names_,\n              corr_,\n              title,\n              cmap=plt.cm.Oranges,\n              figsize=(9, 9)):\n    \"\"\"\n    Plot a correlation matrix.\n    \n    cmap reference:\n    https:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n    \n    :param names_: row\/collum names\n    :type names_: [str]\n    :param corr_: matrix with correlations\n    :type corr_: np.array\n    :param title: image title\n    :type title: str\n    :param cmap: plt color map\n    :type cmap: plt.cm\n    :param figsize: plot's size\n    :type figsize: tuple\n    \"\"\"\n    plt.figure(figsize=figsize)\n    plt.imshow(corr_, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=24, fontweight='bold')\n    plt.colorbar()\n    tick_marks = np.arange(len(names_))\n    plt.xticks(tick_marks, names_, rotation=45)\n    plt.yticks(tick_marks, names_)\n    thresh = corr_.max() \/ 2.\n    for i, j in itertools.product(range(corr_.shape[0]), range(corr_.shape[1])):\n        plt.text(j, i, format(corr_[i, j], '.2f'),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if corr_[i, j] > thresh else \"black\")\n\n    plt.tight_layout()","638f266d":"for m in select:\n    print(m, model2score[m])\n\nplot_corr(pred_corr.columns,\n           pred_corr.values,\n           \"correlation between models\\n(all above 0.4)\",\n           cmap=plt.cm.Oranges,\n           figsize=(10, 10))","f93ac15a":"# w = np.array([np.mean(all_lgb_scores), np.mean(all_XGB_scores)])\n# w = w * 10\nw = [1,1]\nw = w \/ np.sum(w)\n\ncombined_list = [xgb_classification, all_lgb_models[2]]\n\nXclass , _, _, _, _ = prep_data(last_valid_df)\n\nlast_valid_df[\"pred\"] = (np.clip(xgb_classification.predict(Xclass), -1, 1) + np.clip(all_lgb_models[2].predict(X_valid_last), -1, 1)) \/ 2 \nscore = get_score(last_valid_df)\nprint(score)","012ce1bb":"def write_sub_using_class_and_ref_models(class_model, reg_model):\n    days = env.get_prediction_days()\n    for (market_obs_df, _ , predictions_template_df) in days:\n        Xreg = get_market_X(market_obs_df)\n        Xclass , _, _, _, _ = prep_data(market_obs_df)\n        market_obs_df[\"pred\"] = (np.clip(class_model.predict(Xclass), -1, 1) + np.clip(reg_model.predict(Xreg), -1, 1)) \/ 2 \n        pred_dict = (market_obs_df.set_index(\"assetCode\")[\"pred\"]).to_dict()\n        pred_dict_f = lambda x: pred_dict[x] if x in pred_dict else 0.0 \n        predictions_template_df[\"confidenceValue\"] = predictions_template_df[\"assetCode\"].apply(pred_dict_f)\n        env.predict(predictions_template_df)\n    env.write_submission_file()\n    print('Done!')","9c371703":"write_sub_using_class_and_ref_models(class_model=xgb_classification, reg_model=all_lgb_models[2])","8d41567c":"# write_sub_using_model(model_=combined_XGB)\n","961eb5e5":"# Data Visu","4c678a42":"# making predictions and writting submissions","01d5fe21":"# Market Data Only Baseline (XGBRegressor)\n\n\nThis is a fit of market data only (no news data used) showing relatively good results. ","95b8bb2f":"## training different models lightgbm","b50eebd4":"1. ## training different models XGBRegressor"}}