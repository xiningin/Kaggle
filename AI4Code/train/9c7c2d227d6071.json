{"cell_type":{"04132666":"code","33eb241d":"code","20686c4b":"code","f4b06eba":"code","b33e5db4":"code","78f67a11":"code","b2e2cd8e":"code","da043cf6":"code","0a479140":"code","202ec4f4":"code","486ae015":"code","042ad414":"code","b45d54eb":"code","23022e2b":"code","59580ac5":"code","080e1248":"code","f1cd5279":"code","09dab212":"code","e523d614":"code","445efb4e":"code","12cb0149":"code","cf4fe249":"code","b7cd79a8":"code","a4cfb4f1":"code","0a5fa74a":"code","a2c65c2d":"code","4c144bbd":"code","619d4996":"code","f208b607":"code","96130a03":"code","564d5eba":"code","b12e8c62":"code","55ccdc9f":"code","83479cd1":"code","72910421":"code","43212a7a":"code","39e50e50":"code","25a25710":"code","7d5e9636":"code","e4833453":"code","247a4c21":"code","34e8da2a":"code","f6fc73a3":"code","ecb1fb3d":"code","16ffb4e5":"code","98c7af45":"code","16e53607":"code","68199d3e":"code","493adf9b":"code","728cf823":"code","0a78a250":"code","4765d235":"code","df5911cd":"code","8cebe287":"code","e5d6d0fd":"code","1faf0789":"code","f7d5d22e":"code","d62ac330":"code","d1aceae2":"code","e57b5a68":"code","381c2513":"code","8408540e":"code","78a51e5e":"code","ec944034":"code","f1edc327":"code","eb9e1084":"code","ecef6690":"code","aeec41e5":"code","6984ce2b":"code","503e498e":"code","b522f440":"code","83c2e5a6":"code","88db6c92":"code","df4ee568":"code","35c43874":"code","da9b1522":"code","9b6521c6":"code","5d3f9e25":"code","96d5f353":"markdown","9a401c9a":"markdown","4c5ce0bb":"markdown","b7c11ed2":"markdown","e10de638":"markdown","13398a8b":"markdown","dd6697f3":"markdown","47098d28":"markdown","1905b0ac":"markdown","e029ef7a":"markdown","a965407d":"markdown","f2a610df":"markdown","8e2c6876":"markdown","4a739347":"markdown","c4d246eb":"markdown","b468f4cb":"markdown","1331bfb2":"markdown","4f120eb0":"markdown","6495f90c":"markdown"},"source":{"04132666":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","33eb241d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n%matplotlib inline","20686c4b":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n# chacking train data:\n\ntrain.head()","f4b06eba":"# chacking test data:\n\ntest.head()","b33e5db4":"# Importing this libraries for the Text Color:\n\nimport termcolor\nfrom termcolor import colored","78f67a11":"# Let's check the shape of both datasets:\n\nprint(f\"Shape of traning data= {colored(train.shape,'blue')}, test data:{colored(test.shape,'green')}\")","b2e2cd8e":"# checking for duplicates in the train data:\n\ntrain.duplicated(subset='Id').sum()","da043cf6":"# checking for duplicates in the test data:\n\ntest.duplicated(subset='Id').sum()","0a479140":"# Checking the info regardinfg type of variable and nulls values of all the variables:\n\ninfo =  train.info()","202ec4f4":"# Let's describe the train data getting statistics os the variables:\n\ntrain.describe()","486ae015":"# checking for the number of unique values of each variable of the train data:\n\nfor col in train.columns:\n    print(\"The number of values for\", col + \":\" + colored(str(len(train[col].unique())), 'blue'))","042ad414":"# checking for the number of unique values of each variable of the test data:\n\nfor col in test.columns:\n    print(\"The number of values for\", col + \"is :\" + colored(str(len(test[col].unique())), 'green'))","b45d54eb":"# Checking number of types of data in the train dataset:\n\ntrain.dtypes.value_counts()","23022e2b":"# Checking number of types of data in the test dataset:\n\ntest.dtypes.value_counts()","59580ac5":"# Save the identification column:\n\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n# Dropping it from the datasets:\n\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","080e1248":"# Concatenating both datasets:\n\nconcat = [train,test]\n\n# Dropping the index of the DataFrame and replaces it with an index of increasing integers:\n\nall_data = pd.concat(concat).reset_index(drop=True)  \n                                                    \n# Deleting SalePrice from all_data:\n\nall_data.drop(['SalePrice'], axis=1, inplace=True)","f1cd5279":"# Checking the new dataframe:\n\nall_data","09dab212":"# Let's check the shape of the new dataframe:\n\nprint(f\"Shape of traning data= {colored(all_data.shape,'blue')}\")","e523d614":"# 'Utilities' feature contains almost all the values of one type of utility only. Since it wont be usefull \n# in modeling we can drop this feature:\n\nall_data['Utilities'].value_counts()","445efb4e":"# 'Street' feature also contains the unbalance data of type of road access to property. We can drop it:\n\nall_data['Street'].value_counts()","12cb0149":"# 'PoolQC' most of the data is missing for this feature, we can drop it:\n\nall_data['PoolQC'].value_counts()","cf4fe249":"# Let's drop all these unuseful variables:\n\nall_data = all_data.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\nprint('Shape of all_data= {}'.format(all_data.shape))","b7cd79a8":"# visualizing missing values:\n\nplt.figure(figsize=(10,5))\nsns.heatmap(all_data.isna())","a4cfb4f1":"# Checking the categorical variables that content Null values:\n\ndisplay('Categorical Columns that content Null values')\ncate = all_data.select_dtypes(include=object).isna().sum().sort_values(ascending=False)\ncate[cate > 0]","0a5fa74a":"# Let's check for the percentage of these categorical variables that content Null values:\n\ncategorical_miss_perct = round(100*(all_data.select_dtypes(include=object).isna().sum().sort_values(ascending=False)\/len(all_data.index)),2)\\\n.to_frame().rename(columns={0:'Null values percentage'})[:10]\ncategorical_miss_perct","a2c65c2d":"# 'MiscFeature', 'Alley', 'Fence' for all of these featurs most of their 90% data is missing, so we can drop it:\n\nall_data = all_data.drop(['MiscFeature', 'Alley', 'Fence',], axis=1)\nprint('Shape of all_data= {}'.format(all_data.shape))","4c144bbd":"# Fixing categorical variables:\n\n# MasVnrType, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, Electrical, FireplaceQu, GarageType,\n# GarageType, GarageFinish, GarageQual, GarageCond, MSZoning, Functional, SaleType, Exterior2nd, Exterior1st, \n# KitchenQual    = replace with None:\n\nall_data['MasVnrType'] = all_data['MasVnrType'].fillna('None')\nall_data['BsmtQual'] = all_data['BsmtQual'].fillna('None')\nall_data['BsmtCond'] = all_data['BsmtCond'].fillna('None')\nall_data['BsmtExposure'] = all_data['BsmtExposure'].fillna('None')\nall_data['BsmtFinType1'] = all_data['BsmtFinType1'].fillna('None')\nall_data['BsmtFinType2'] = all_data['BsmtFinType2'].fillna('None')\nall_data['Electrical'] = all_data['Electrical'].fillna('None')\nall_data['FireplaceQu'] = all_data['FireplaceQu'].fillna('None')\nall_data['GarageType'] = all_data['GarageType'].fillna('None')\nall_data['GarageFinish'] = all_data['GarageFinish'].fillna('None')\nall_data['GarageQual'] = all_data['GarageQual'].fillna('None')\nall_data['GarageCond'] = all_data['GarageCond'].fillna('None')\nall_data['MSZoning'] = all_data['MSZoning'].fillna('None')\nall_data['Functional'] = all_data['Functional'].fillna('None')\nall_data['SaleType'] = all_data['SaleType'].fillna('None')\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna('None')\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna('None')\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna('None')","619d4996":"# Checking the numerical variables that content Null values:\n\ndisplay('Numerical Columns that content Null values')\nnum = all_data.select_dtypes(include=np.number).isna().sum().sort_values(ascending=False)\nnum[num > 0]","f208b607":"# Let's check for the percentage of these numerical variables that content Null values:\n\nnumerical_miss_perct = round(100*(all_data.select_dtypes(include=np.number).isna().sum().sort_values(ascending=False)\/len(all_data.index)),2)\\\n.to_frame().rename(columns={0:'Null values percentage'})[:5]\nnumerical_miss_perct\n\n# As we can see, the percentage of missing values in the numerical variebles doesn't even reach the 17% in those variables with the \n# highest amount of nulls. So I am not going to elimante any of them.","96130a03":"# Fixing numerical variables:\n\n# Groupping by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood:\n\nall_data['LotFrontage'] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n# MasVnrArea, GarageYrBlt, BsmtFullBath, BsmtHalfBath, TotalBsmtSF, GarageArea, BsmtFinSF2, BsmtFinSF1,\n# BsmtUnfSF and GarageCars = replace with zero:\n\nall_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(int(0))\nall_data['GarageYrBlt'] = all_data['GarageYrBlt'].fillna(int(0))\nall_data['BsmtFullBath'] = all_data['BsmtFullBath'].fillna(int(0))\nall_data['BsmtHalfBath'] = all_data['BsmtHalfBath'].fillna(int(0))\nall_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].fillna(int(0))\nall_data['GarageArea'] = all_data['GarageArea'].fillna(int(0))\nall_data['BsmtFinSF2'] = all_data['BsmtFinSF2'].fillna(int(0))\nall_data['BsmtFinSF1'] = all_data['BsmtFinSF1'].fillna(int(0))\nall_data['BsmtUnfSF'] = all_data['BsmtUnfSF'].fillna(int(0))\nall_data['GarageCars'] = all_data['GarageCars'].fillna(int(0))","564d5eba":"# Checking if there is any miss value:\n\nsum(all_data.select_dtypes(include=object).isna().sum() != 0)","b12e8c62":"# Visualizing whether there is any null value or not after recoding:\n\nplt.figure(figsize=(10, 5))\nsns.heatmap(all_data.isna())","55ccdc9f":"# Checking for any miss value without visualization:\nall_data.loc[:, all_data.isna().any()]","83479cd1":"plt.subplots(figsize=(12,9))\nsns.distplot(train['SalePrice'], fit=stats.norm)\n\n# Getting the fitted parameters used by the function:\n\n(mu, sigma) = stats.norm.fit(train['SalePrice'])\n\n# plotting with the distribution:\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\nplt.ylabel('Frequency')\n\n# Probablity plot:\n\nfig = plt.figure()\nstats.probplot(train['SalePrice'], plot=plt)\nplt.show()","72910421":"# analysis of the kurtosis and the skewness to check the normality of the variable:\n\nkurt = train['SalePrice'].kurtosis()\nskew = train['SalePrice'].skew()\nprint('Sale Price Kurtorsis {}'.format(kurt))\nprint('Sale Price Skewness {}'.format(skew))","43212a7a":"# Using log function for normalize the target variable:\n\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n\n# Checking to see the  normal distribution of the variable now:\n\nplt.subplots(figsize=(12,9))\nsns.distplot(train['SalePrice'], fit=stats.norm)\n\n# Getting the fitted parameters used by the function:\n\n(mu, sigma) = stats.norm.fit(train['SalePrice'])\n\n# plotting with the distribution now:\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\nplt.ylabel('Frequency')\n\n# robablity plot:\n\nfig = plt.figure()\nstats.probplot(train['SalePrice'], plot=plt)\nplt.show()","39e50e50":"# lets see the correlation between columns and target column:\n\ncorr = train.corr()\ncorr['SalePrice'].sort_values(ascending=False)[1:15].to_frame()\\\n.style.background_gradient(axis=1,cmap=sns.light_palette('green', as_cmap=True))","25a25710":"# Separating variable into new dataframe from original dataframe which has only numerical values\n# There is 38 numerical attribute from 81 attributes:\n\ntrain_corr = train.select_dtypes(include=[np.number])\ntrain_corr.shape","7d5e9636":"# Let's visualize a big Correlation plot:\n\ncorr = train_corr.corr()\nplt.subplots(figsize=(20,9))\nsns.heatmap(corr, annot=True)","e4833453":"# Checking now for correlation higher than 0.5:\n\ntop_feature = corr.index[abs(corr['SalePrice']>0.5)]\nplt.subplots(figsize=(12, 8))\ntop_corr = train[top_feature].corr()\nsns.heatmap(top_corr, annot=True)\nplt.show()","247a4c21":"# let's separeting the categorical variables into ordinals and nominals:\n\n# Ordinals:\n\ncategorical_ordinal = ['LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'BldgType', 'RoofStyle', 'RoofMatl', \n                     'ExterQual', 'ExterCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType2', 'BsmtFinType1',\n                     'HeatingQC', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n                     'GarageCond','PavedDrive']","34e8da2a":"# Nominals:\n\ncategorical_nominal = ['MSSubClass', 'MSZoning', 'Neighborhood', 'Condition1', 'Condition2', 'HouseStyle', 'CentralAir',\n                     'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'Electrical', 'MasVnrType', 'Exterior1st',\n                     'Exterior2nd', 'Heating', 'Foundation']","f6fc73a3":"# From sklearn importing LabelEncoder library to encode the ordinal variables:\n\nfrom sklearn.preprocessing import LabelEncoder\nfor col in categorical_ordinal:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[col].values)) \n    all_data[col] = lbl.transform(list(all_data[col].values))","ecb1fb3d":"# Get k-1 dummies in nominal variables to avoid multicollinearity:\n\ncat_feats_nominal_one_hot = pd.get_dummies(all_data[categorical_nominal], drop_first= True).reset_index(drop=True)\n\n# checking it:\n\nprint(f'Shape of cat_feats_nominal_one_hot: {cat_feats_nominal_one_hot.shape}')\ncat_feats_nominal_one_hot.head()","16ffb4e5":"# Now lets concatenate the one Hot Encoded categorcal nominal features with main data frame.\n\n# First we need to drop the catgorical nominal columns from all_data:\n\nall_data = all_data.drop(categorical_nominal, axis= 'columns')\n\n# Now let;s to concat it:\n\nall_data = pd.concat([all_data, cat_feats_nominal_one_hot], axis='columns')\n\n# Checking shape now:\n\nprint(f'Shape of all_data: {all_data.shape}')\n\n# Checking sfinal dataframe:\n\nall_data.head()","98c7af45":"# Let's separate data:\n\ntrain_data = pd.DataFrame(all_data[:1460])\ntest_data = pd.DataFrame(all_data[1460:])\n\ntest_data.head()","16e53607":"# Taking values X and y:\n\nX = train_data\ny = train['SalePrice']","68199d3e":"# Split data into train and test formate:\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)","493adf9b":"# Checking:\n\nprint(f'Training set sizes: {X_train.shape}, {y_train.shape}')\nprint(f'Validation set sizes: {X_test.shape}, {y_test.shape}')","728cf823":"# Training the model:\n\nfrom sklearn import linear_model\nmodel_lineal = linear_model.LinearRegression()","0a78a250":"# Fitting the model:\n\nmodel_lineal.fit(X_train, y_train)","4765d235":"# First evaluation of the LinearRegression model:\n\nprint(\"Accuracy --> \", round(model_lineal.score(X_test, y_test)*100),\"%\")","df5911cd":"# Importing these libraries for calculating RMSE $ MAE:\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","8cebe287":"# Making predictions:\n\npredictions_1 = model_lineal.predict(X_test)\n\n# RMSE Calculation:\n\nprint('RMSE is: \\n', mean_squared_error(y_test, predictions_1))\n\n# Visulization to compare real prices <-> Predicted Prices:\n\nactual_values = y_test\nplt.scatter(predictions_1, actual_values, alpha=.75,\n            color='b')  # alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Linear Regression Model')\nplt.show()","e5d6d0fd":"# Train the model:\n\nfrom sklearn.ensemble import RandomForestRegressor\nmodel2 = RandomForestRegressor(n_estimators=100)","1faf0789":"# Fitting the model:\n\nmodel2.fit(X_train, y_train)","f7d5d22e":"# First evaluation of the RandomForestRegression model:\n\nprint(\"Accuracy  --> \", round(model2.score(X_test, y_test)*100),\"%\")","d62ac330":"# Making predictions:\n\npredictions_2 = model2.predict(X_test)\n\n# RMSE Calculation:\n\nprint('RMSE is: \\n', mean_squared_error(y_test, predictions_2))\n\n# Visulization to compare real prices <-> Predicted Prices:\n\nactual_values = y_test\nplt.scatter(predictions_2, actual_values, alpha=.75,\n            color='c')  # alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('RandomForestRegression Model')\nplt.show()","d1aceae2":"# Train the model:\n\nfrom sklearn.linear_model import BayesianRidge\nbaysian_ridge = BayesianRidge()#(compute_score=True,alpha_2=0.85)\n\n# Fitting the model:\n\nbaysian_ridge.fit(X_train, y_train)","e57b5a68":"# First evaluation of the BayesianRidge model:\n\nprint(\"Accuracy  --> \", round(baysian_ridge.score(X_test, y_test)*100),\"%\")","381c2513":"# Making predictions:\n\npredictions_3 = baysian_ridge.predict(X_test)\n\n# RMSE Calculation:\n\nprint('RMSE is: \\n', mean_squared_error(y_test, predictions_3))\n\n# Visulization to compare real prices <-> Predicted Prices:\n\nactual_values = y_test\nplt.scatter(predictions_3, actual_values, alpha=.75,\n            color='g')  # alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('BayesianRidge Model')\nplt.show()","8408540e":"# Train the model:\n\nfrom sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor()\n\n# Fitting the model:\n\ngbr.fit(X_train, y_train)\n","78a51e5e":"# First evaluation of the GradientBoostingRegressor model:\n\nprint(\"Accuracy --> \",round(gbr.score(X_test, y_test)*100),\"%\")","ec944034":"# Making predictions:\n\npredictions_4 = gbr.predict(X_test)\n\n# RMSE Calculation:\n\nprint('RMSE is: \\n', mean_squared_error(y_test, predictions_4))\n\n# Visulization to compare real prices <-> Predicted Prices:\n\nactual_values = y_test\nplt.scatter(predictions_4, actual_values, alpha=.75,\n            color='y')  # alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('GradientBoostingRegressor Model')\nplt.show()","f1edc327":"# Importing Libraries to carry out Cross Valitation:  \n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","eb9e1084":"# Creating a list with the four models:\n\nAll_models = ['Linear_Reg.','Random_Forest_Reg.','Bayesian_Ridge_Reg.','Grad_Boost_Reg.']","ecef6690":"# Creation of the RMSE formula:\n    \ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","aeec41e5":"# Cross Valitation formula:\n\ndef RMSE_CV(estimator,X_train, Y_train, cv=5,n_jobs=4):\n    cv_results = cross_val_score(estimator,X_train,Y_train,cv=cv,scoring=\"neg_mean_squared_error\",n_jobs=n_jobs)\n    return (np.sqrt(-cv_results)).mean()","6984ce2b":"# Creating List for each measure:\n\nRMSE_CV_scores = []\nRMSE_scores = []\nMAE_scores = []","503e498e":"# Calculating RMSE_CV for each one:\n\nmodel_lineal_RMSE_CV = RMSE_CV(model_lineal, X_train, y_train)\nRMSE_CV_scores.append(model_lineal_RMSE_CV)\nRForest_model_RMSE_CV = RMSE_CV(model2, X_train, y_train)\nRMSE_CV_scores.append(RForest_model_RMSE_CV)\nbaysian_ridge_model_RMSE_CV = RMSE_CV(baysian_ridge, X_train, y_train)\nRMSE_CV_scores.append(baysian_ridge_model_RMSE_CV)\ngbr_model_RMSE_CV = RMSE_CV(gbr, X_train, y_train)\nRMSE_CV_scores.append(gbr_model_RMSE_CV)\n\n# Calculating rmse for each one:\n\nlinear_model_rmse = rmse(y_test, predictions_1)\nRMSE_scores.append(linear_model_rmse)\nRForest_model_rmse = rmse(y_test, predictions_2)\nRMSE_scores.append(RForest_model_rmse)\nBayRidge_model_rmse = rmse(y_test, predictions_3)\nRMSE_scores.append(BayRidge_model_rmse)\nGBoosting_model_rmse = rmse(y_test, predictions_4)\nRMSE_scores.append(GBoosting_model_rmse)\n\n# Calculating mean_absolute_error for each one:\n\nlinear_model_mae = mean_absolute_error(predictions_1, y_test)\nMAE_scores.append(linear_model_mae)\nRForest_model_mae = mean_absolute_error(predictions_2, y_test)\nMAE_scores.append(RForest_model_mae)\nBayRidge_model_mae = mean_absolute_error(predictions_3, y_test)\nMAE_scores.append(BayRidge_model_mae)\nGBoosting_model_mae = mean_absolute_error(predictions_4, y_test)\nMAE_scores.append(GBoosting_model_mae)","b522f440":"# Making all the results togheter for comparison:\n\nfinal_evaluation_scores = pd.DataFrame(All_models, columns = ['Regressors'])\nfinal_evaluation_scores['RMSE_CV_scores'] = RMSE_CV_scores\nfinal_evaluation_scores['RMSE_scores'] = RMSE_scores\nfinal_evaluation_scores['MAE_scores'] = MAE_scores\nfinal_evaluation_scores","83c2e5a6":"# Visualization RMSE_CV_scores:\n\nplt.figure(figsize = (8,6))\nsns.barplot(final_evaluation_scores['Regressors'],final_evaluation_scores['RMSE_CV_scores'])\nplt.xlabel('Regressors', fontsize = 10)\nplt.ylabel('RMSE_CV_scores', fontsize = 10)\nplt.xticks(rotation=45)\nplt.show()","88db6c92":"# Visualization RMSE_scores:\n\n\nplt.figure(figsize = (8,6))\nsns.barplot(final_evaluation_scores['Regressors'],final_evaluation_scores['RMSE_scores'])\nplt.xlabel('Regressors', fontsize = 10)\nplt.ylabel('RMSE_scores', fontsize = 10)\nplt.xticks(rotation=45)\nplt.show()","df4ee568":"# Visualization MAE_scores:\n\nplt.figure(figsize = (8,6))\nsns.barplot(final_evaluation_scores['Regressors'],final_evaluation_scores['MAE_scores'])\nplt.xlabel('Regressors', fontsize = 10)\nplt.ylabel('MAE_scores', fontsize = 10)\nplt.xticks(rotation=45)\nplt.show()","35c43874":"# Mean of all model's prediction:\n\n# I am doing an ensemble model that weighs each of the four different models equally, then he average prediction at each data \n# point becomes the final prediction at this data point.\n\n# I am using np.expm1 ( ) to calculate exp(x) - 1 for all elements in the array","da9b1522":"Final_model = (np.expm1(model_lineal.predict(test_data)) + np.expm1(model2.predict(test_data)) + np.expm1(baysian_ridge.predict(test_data)) + np.expm1(gbr.predict(test_data)) ) \/ 4\nFinal_model","9b6521c6":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = Final_model\nsub.to_csv('submission.csv',index=False)","5d3f9e25":"result4 =  pd.read_csv('submission.csv')\nresult4.head()","96d5f353":"# Import Libraries\nIn this step I import libraries required for data processing and feature engineering.","9a401c9a":"### Delete Unuseful Features:","4c5ce0bb":"### Missing Values analysis:","b7c11ed2":"# FINAL STEP FOR SUBMISSION:\n","e10de638":"# Missing Values","13398a8b":"# Exploratory Data Analysis (EDA)","dd6697f3":"After this inspection of the datasets, we see that there are a total of 1460 training examples and 1459 test examples.\nIn the train dataset there is a total number of attributes\/features equals 81, which includes ID and SalePrice.\nThere is 80 attributes\/features on the test data and it doesn't contain the variable 'SalePrice'. which is going to be \nthe target variable.","47098d28":"# Encoding ","1905b0ac":"Categorical variables:","e029ef7a":"# Load Data\nIn this step I load the training and testing datasets into pandas dataframe.","a965407d":"# MODELING","f2a610df":"## Linear Regression","8e2c6876":"## BayesianRidge\n","4a739347":"# EVALUATION MODELS","c4d246eb":"# Correlation","b468f4cb":"# Analysis of the target variable\n","1331bfb2":"## GradientBoostingRegressor","4f120eb0":"Numerical variables:","6495f90c":"## RandomForestRegression"}}