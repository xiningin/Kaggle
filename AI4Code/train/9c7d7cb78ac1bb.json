{"cell_type":{"347f7f91":"code","0e647223":"code","ea732f63":"code","fc38c64f":"code","9f026746":"code","b8b8c592":"code","257b935b":"code","b24d81e9":"code","fa502abd":"code","7f1933fd":"code","1ce3c0bf":"code","a6a4621e":"code","7e07d7f9":"code","fd9eb027":"code","3e8b6d23":"code","292d837a":"code","dd31fcfa":"code","cac38b10":"code","d027973c":"code","688a8dbe":"code","8921e128":"code","f7b36db8":"code","99bacdad":"code","f8d8456c":"code","d61828ba":"code","ab032849":"code","655272a9":"code","2ec2365c":"code","9d12a987":"code","e3a76fc2":"code","0322401a":"code","23b1e5cb":"code","98ed459a":"code","7b42ae26":"code","cf2fd361":"code","1bca9671":"code","fbf7afe7":"code","0a0339ff":"code","5c9fb5fe":"code","004b923e":"code","044c4cda":"code","d11aa968":"code","b3ce38c0":"code","32c927d0":"code","a141349f":"code","2f62c073":"code","65d8c5c2":"code","cc4a125a":"code","438d140e":"code","ef5b9340":"code","e68f8357":"code","8a1f7b57":"code","bb8099c6":"code","2da9b915":"code","0a8ec998":"code","3cd96277":"code","b6bc5873":"code","b10831fe":"code","9ccdd81a":"code","7ee321d9":"code","e6b01e4b":"code","78a69c2a":"code","b4f001a4":"code","0b3087f5":"code","5c1188d2":"code","de56ad7a":"code","457c4fe3":"markdown","51f67cfa":"markdown","a96e09e7":"markdown","9ceb3ee0":"markdown","4185b29d":"markdown","0bb522a9":"markdown","a9cd1dc3":"markdown","ad318856":"markdown","3ce34898":"markdown","71bc3f12":"markdown","e6cefba3":"markdown","7f339774":"markdown","c8b7d667":"markdown"},"source":{"347f7f91":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0e647223":"data = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","ea732f63":"data.head()","fc38c64f":"data.info()","9f026746":"plt.figure(figsize=(18,10))\nsns.heatmap(data.corr(),annot=True,cmap='viridis')\n","b8b8c592":"sns.countplot(x=\"target\", data=data, palette=\"bwr\")\nplt.show()","257b935b":"sns.countplot(x='sex', data=data, palette=\"mako_r\")\nplt.xlabel(\"Sex (0 = female, 1= male)\")\nplt.show()","b24d81e9":"plt.figure(figsize=(20,6))\nsns.countplot(x='age', data=data, hue='target')\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.legend([\"Haven't Disease\", \"Have Disease\"])","fa502abd":"plt.figure(figsize=(14,10))\n\nsns.countplot(x='sex', data=data, hue='target', palette='rainbow')\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('Sex (0 = Female, 1 = Male)')\nplt.ylabel('Frequency')\nplt.legend([\"Haven't Disease\", \"Have Disease\"])","7f1933fd":"plt.figure(figsize=(14,10))\nsns.countplot(x='cp',data=data, hue='target',palette='rainbow')\nplt.title('Heart Disease Frequency According to The chest pain experienced')\nplt.xlabel('The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)')\nplt.ylabel('Frequency')\nplt.legend([\"Haven't Disease\", \"Have Disease\"])","1ce3c0bf":"plt.figure(figsize=(14,10))\nsns.countplot(x='fbs',data=data, hue='target',palette='Set1')\nplt.title('Heart Disease Frequency According To FBS')\nplt.xlabel('FBS - (Fasting Blood Sugar > 120 mg\/dl) (1 = true; 0 = false)')\nplt.xticks(rotation = 0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency')\nplt.show()","a6a4621e":"plt.figure(figsize=(14,10))\nsns.countplot(x='slope',data=data, hue='target',palette='Set1')\nplt.title('Heart Disease Frequency According To Slope')\nplt.xlabel('the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)')\nplt.xticks(rotation = 0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","7e07d7f9":"plt.figure(figsize=(14,10))\nsns.countplot(x='restecg',data=data, hue='target',palette='rainbow')\nplt.title('Heart Disease Frequency According To Resting electrocardiographic measurement ')\nplt.xlabel(\"Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\")\nplt.xticks(rotation = 0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","fd9eb027":"plt.figure(figsize=(14,10))\nsns.countplot(x='exang',data=data, hue='target',palette='bwr')\nplt.title('Heart Disease Frequency According To Exercise induced angina')\nplt.xlabel(\"Exercise induced angina (1 = yes; 0 = no)\")\nplt.xticks(rotation = 0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","3e8b6d23":"df_cp=pd.get_dummies(data['cp'], prefix = \"cp\", drop_first=True)\ndf_thal=pd.get_dummies(data['thal'], prefix = \"thal\", drop_first=True)\ndf_slope=pd.get_dummies(data['slope'], prefix = \"slope\", drop_first=True)\ndf_sex=pd.get_dummies(data['sex'], prefix = \"sex\", drop_first=True)\ndf_fbs=pd.get_dummies(data['fbs'], prefix = \"fbs\", drop_first=True)\ndf_restecg=pd.get_dummies(data['restecg'], prefix = \"restecg\", drop_first=True)\ndf_exang=pd.get_dummies(data['exang'], prefix = \"exang\", drop_first=True)\n","292d837a":"df_num=data.drop(['cp','thal','slope','sex','fbs','restecg','exang'], axis=1)","dd31fcfa":"frames = [df_num,df_cp, df_thal, df_slope, df_sex,df_fbs,df_restecg,df_exang]\ndf= pd.concat(frames, axis = 1)\ndf.info()","cac38b10":"scale=MinMaxScaler()\ndf[['age','trestbps','chol','thalach','oldpeak','ca']]= pd.DataFrame(scale.fit_transform(df[['age','trestbps','chol','thalach','oldpeak','ca']].values), columns=['age','trestbps','chol','thalach','oldpeak','ca'], index=df.index)","d027973c":"df","688a8dbe":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score","8921e128":"X_train, X_test, y_train, y_test = train_test_split(df.drop('target', 1), df['target'], test_size = 0.33, random_state=10) #split the data","f7b36db8":"from sklearn.tree import DecisionTreeClassifier","99bacdad":"dtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)","f8d8456c":"from sklearn.metrics import classification_report,confusion_matrix","d61828ba":"print(classification_report(y_test,dtree_predictions))\n","ab032849":"def rate(pred):\n    cfm = confusion_matrix(y_test, pred)\n    cfm = cfm.astype(np.float)\n    total=sum(sum(cfm))\n    sensitivity = cfm[0,0]\/(cfm[0,0]+cfm[1,0])\n    specificity = cfm[1,1]\/(cfm[1,1]+cfm[0,1])\n    accuracy = (cfm[0,0]+cfm[1,1])\/total\n    dt={\n        \"Accuracy\": accuracy,\n        \"Sensivity\": sensitivity,\n        \"Specificity\": specificity\n    }    \n    return dt","655272a9":"dtree_report=pd.DataFrame(rate(dtree_predictions), index=['Decision Tree'])\ndtree_report","2ec2365c":"from sklearn.ensemble import RandomForestClassifier","9d12a987":"X_train.head()","e3a76fc2":"rfc = RandomForestClassifier(n_estimators=600)\nrfc.fit(X_train,y_train)","0322401a":"cross_val_score(rfc, X_train,y_train,cv=10)","23b1e5cb":"rfc_predictions = rfc.predict(X_test)","98ed459a":"print(classification_report(y_test,rfc_predictions))\n","7b42ae26":"rfc_report=pd.DataFrame(rate(rfc_predictions),index=['Random Forest'])\n","cf2fd361":"from sklearn.svm import SVC","1bca9671":"svc_model = SVC()\nsvc_model.fit(X_train,y_train)","fbf7afe7":"svc_predictions = svc_model.predict(X_test)","0a0339ff":"print(classification_report(y_test,svc_predictions))","5c9fb5fe":"svc_report=pd.DataFrame(rate(svc_predictions),index=['Support Vector Machine'])\nsvc_report","004b923e":"from sklearn.neighbors import KNeighborsClassifier","044c4cda":"error_rate = []\n\n# Will take some time\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","d11aa968":"\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","b3ce38c0":"# NOW WITH K=34\nknn = KNeighborsClassifier(n_neighbors=34)\n\nknn.fit(X_train,y_train)\nknn_predictions = knn.predict(X_test)","32c927d0":"print('WITH K=34')\nprint('\\n')\nprint(classification_report(y_test,knn_predictions))","a141349f":"knn_report=pd.DataFrame(rate(knn_predictions),index=['K_Nearest Neighbor'])\n","2f62c073":"from sklearn.linear_model import LogisticRegression","65d8c5c2":"logmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)","cc4a125a":"log_predictions = logmodel.predict(X_test)","438d140e":"print(classification_report(y_test,log_predictions))","ef5b9340":"log_predictions","e68f8357":"from sklearn.naive_bayes import GaussianNB","8a1f7b57":"gnb = GaussianNB()\ngnb.fit(X_train, y_train)","bb8099c6":"gnb_predictions=gnb.predict(X_test)","2da9b915":"print(classification_report(y_test,gnb_predictions))","0a8ec998":"gnb_report=pd.DataFrame(rate(gnb_predictions),index=['Naives Bayes'])\ngnb_report","3cd96277":"from tensorflow import keras\nfrom tensorflow.keras import layers\n","b6bc5873":"X_train.shape","b10831fe":"model = keras.Sequential([\n    layers.Dense(13, activation='relu', input_shape=[19]),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(16, activation='relu'), \n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(2, activation='relu'), \n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1, activation='sigmoid'),\n])","9ccdd81a":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)","7ee321d9":"history = model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    batch_size=20,\n    epochs=250,\n    verbose=0, # hide the output because we have so many epochs\n)","e6b01e4b":"history_df = pd.DataFrame(history.history)\nhistory_df","78a69c2a":"history_df = pd.DataFrame(history.history)\n# Start the plot at epoch 5\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))\n","b4f001a4":"ann_predictions = model.predict(X_test)\nrounded = [int(round(x[0])) for x in ann_predictions]\n","0b3087f5":"ann_report=pd.DataFrame(rate(rounded),index=['Neural Network'])\nann_report","5c1188d2":"performance_rate=pd.concat([dtree_report, rfc_report, svc_report, knn_report,gnb_report,ann_report,log_report])\nperformance_rate","de56ad7a":"plt.figure()\nperformance_rate.plot.bar(figsize=(20,8),fontsize=14,rot=0.5, colormap='viridis')\nplt.xlabel('Classifiers',fontsize=18)\nplt.ylabel('Performance rate',fontsize=18)\nplt.title('Performance of different classifiers on full features',fontsize=20)\n\n","457c4fe3":"**4. K-Nearest neighbor (KNN)**","51f67cfa":"**1. Decision Tree**","a96e09e7":"# The model","9ceb3ee0":"**2. Random Forest**","4185b29d":"- **age**: The person's age in years\n- **sex**: The person's sex (1 = male, 0 = female)\n- **cp:** The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n- **trestbps:** The person's resting blood pressure (mm Hg on admission to the hospital)\n- **chol:** The person's cholesterol measurement in mg\/dl\n- **fbs:** The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false) \n- **restecg:** Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n- **thalach:** The person's maximum heart rate achieved\n- **exang:** Exercise induced angina (1 = yes; 0 = no)\n- **oldpeak:** ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more [here](https:\/\/litfl.com\/st-segment-ecg-library\/))\n- **slope:** the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n- **ca:** The number of major vessels (0-3)\n- **thal:** A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n- **target:** Heart disease (0 = no, 1 = yes)","0bb522a9":"**6. Naive Bayes**","a9cd1dc3":"# Content\n1. Preprocessing data \n* Create dummy data (Object data type)\n* Scaling data (numberic data) by MinMaxScaler\n2. The model\n* Decision Tree\n* Random Forest\n* Support Vector Machine\n* K-Nearest Neighbor\n* Logistic Regression\n* Artificial Neural Network","ad318856":"**7. Artificial Neural Network**","3ce34898":"# Preprocessing data","71bc3f12":"**Scaling data using MinMaxScaler**","e6cefba3":"**Create dummy data**","7f339774":"**5. Logistic Regression**","c8b7d667":"**3. Support Vector Machine**"}}