{"cell_type":{"d63c0e66":"code","de46f5db":"code","b6d91d6c":"code","7aaa3acd":"code","be92eafa":"code","5c50151e":"code","59ef7a0d":"code","58b083d9":"code","a6e4c467":"code","560bab0f":"code","7230734c":"code","68261e41":"code","99ca4fcd":"code","1956389f":"markdown"},"source":{"d63c0e66":"#The following is an example for creating an SVM classifier by using kernels. We will be using iris dataset from scikit-learn \u2212\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import svm,datasets\nimport matplotlib.pyplot as plt\n","de46f5db":"#load the input data\niris = datasets.load_iris()","b6d91d6c":"#From the dataset we are taking first two features\n\nX = iris.data[:, :2]\ny = iris.target\n","7aaa3acd":"#Next we will plot SVM boundries with original data\nx_min , x_max =  X[:,0].min() - 1, X[:,0].max()+1\ny_min , y_max = X[:,1].min() -1, X[:,1].max()+1\n\nh = (x_max\/x_min)\/100\n\nxx , yy = np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max,h))\n\nX_plot = np.c_[xx.ravel(),yy.ravel()]","be92eafa":"#Now, we need to provide the value of regularization parameter\nc = 1.0\n","5c50151e":"#Next, SVM classifier object can be created as follows \u2212\n\nsvc_classifier = svm.SVC(kernel='linear', C=c).fit(X, y)\n\nZ = svc_classifier.predict(X_plot)\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(15, 5))\nplt.subplot(121)\nplt.contourf(xx, yy, Z, cmap=plt.cm.tab10, alpha=0.3)\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\nplt.xlim(xx.min(), xx.max())\nplt.title('Support Vector Classifier with linear kernel')","59ef7a0d":"import random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_random_dataset(size):\n    \"\"\" Generate a random dataset and that follows a quadratic  distribution\n    \"\"\"\n    x = []\n    y = []\n    target = []   \n    for i in range(size):\n        # class zero\n        x.append(np.round(random.uniform(0, 2.5), 1))\n        y.append(np.round(random.uniform(0, 20), 1))\n        target.append(0)        \n        # class one\n        x.append(np.round(random.uniform(1, 5), 2))\n        y.append(np.round(random.uniform(20, 25), 2))\n        target.append(1)        \n        x.append(np.round(random.uniform(3, 5), 2))\n        y.append(np.round(random.uniform(5, 25), 2))\n        target.append(1)    \n    df_x = pd.DataFrame(data=x)\n    df_y = pd.DataFrame(data=y)\n    df_target = pd.DataFrame(data=target)    \n    data_frame = pd.concat([df_x, df_y], ignore_index=True, axis=1)\n    data_frame = pd.concat([data_frame, df_target], ignore_index=True, axis=1)    \n    data_frame.columns = ['x', 'y', 'target']\n    return data_frame\n\n\n# Generate dataset\nsize = 100\ndataset = generate_random_dataset(size)\nfeatures = dataset[['x', 'y']]\nlabel = dataset['target']\n\n# Hold out 20% of the dataset for training\ntest_size = int(np.round(size * 0.2, 0))\n\n# Split dataset into training and testing sets\nx_train = features[:-test_size].values\ny_train = label[:-test_size].values\n\nx_test = features[-test_size:].values\ny_test = label[-test_size:].values\n\n# Plotting the training set\nfig, ax = plt.subplots(figsize=(12, 7))\n\n# removing to and right border\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# adding major gridlines\nax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\nax.scatter(features[:-test_size]['x'], features[:-test_size]['y'], color=\"#8C7298\")\nplt.show()","58b083d9":"\"\"\"There is a little space between two groups of data points.\nBut closer to the center,it's not clear which data point belong to which class.\n\nA quadratic curve might be a good candidate to separate these classes.\n\"\"\"\n\n\nfrom sklearn import svm\nmodel = svm.SVC(kernel = 'poly',degree=2)\nmodel.fit(x_train,y_train)","a6e4c467":"\"\"\" To see the result of fitting this model, we can plot the decision boundary and the margin along with the dataset.\"\"\"\n\nfig,ax = plt.subplots(figsize= (12,7))\n\n#Removing to and right borders\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n#Create grid to eval model\nxx = np.linspace(-1,max(features['x'])+1,len(x_train))\nyy = np.linspace(0, max(features['y']) + 1, len(y_train))\nYY,XX = np.meshgrid(yy,xx)\nxy = np.vstack([XX.ravel(), YY.ravel()]).T\n\ntrain_size = len(features[:-test_size]['x'])\n\n# Assigning different colors to the classes\ncolors = y_train\ncolors = np.where(colors == 1, '#8C7298', '#4786D1')\n\n# Plot the dataset\nax.scatter(features[:-test_size]['x'], features[:-test_size]['y'], c=colors)\n\n# Get the separating hyperplane\nZ = model.decision_function(xy).reshape(XX.shape)\n# Draw the decision boundary and margins\nax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n\n# Highlight support vectors with a circle around them\nax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100, linewidth=1, facecolors='none', edgecolors='k')\n\nplt.show()","560bab0f":"\"\"\" If we calculate the accuracy of this model against the testing set we get a good result, \ngranted the dataset is very small and generated at random \"\"\"\n\nfrom sklearn.metrics import accuracy_score\npredictions_poly = model.predict(x_test)\naccuracy_poly = accuracy_score(y_test, predictions_poly)\nprint(\"2nd degree polynomial Kernel\\nAccuracy (normalized): \" + str(accuracy_poly))\n","7230734c":"\"\"\"The accuracy is good, but let's see if a more simplistic approach could have solved our problem.\n To fit an SVM with a linear kernel we just need to update the kernel parameter.\n \"\"\"\n\nmodel = svm.SVC(kernel='linear')\nmodel.fit(x_train,y_train)","68261e41":"fig,ax = plt.subplots(figsize= (12,7))\n\n#Removing to and right borders\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n#Create grid to eval model\nxx = np.linspace(-1,max(features['x'])+1,len(x_train))\nyy = np.linspace(0, max(features['y']) + 1, len(y_train))\nYY,XX = np.meshgrid(yy,xx)\nxy = np.vstack([XX.ravel(), YY.ravel()]).T\n\ntrain_size = len(features[:-test_size]['x'])\n\n# Assigning different colors to the classes\ncolors = y_train\ncolors = np.where(colors == 1, '#8C7298', '#4786D1')\n\n# Plot the dataset\nax.scatter(features[:-test_size]['x'], features[:-test_size]['y'], c=colors)\n\n# Get the separating hyperplane\nZ = model.decision_function(xy).reshape(XX.shape)\n# Draw the decision boundary and margins\nax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n\n# Highlight support vectors with a circle around them\nax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100, linewidth=1, facecolors='none', edgecolors='k')\n\nplt.show()","99ca4fcd":"print(\"2nd degree polynomial Kernel\\nAccuracy (normalized): \" + str(accuracy_poly))\n","1956389f":"**Kindly upvote !! This will make my day.....**"}}