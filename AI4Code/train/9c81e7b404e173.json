{"cell_type":{"ea5417db":"code","e27e5e20":"code","3c8e85bb":"code","a728c1c7":"code","645f67df":"code","045d1247":"code","2d03d4f8":"code","fbf4dfd1":"code","ab5e4cfd":"code","bf5e7734":"code","0395bd78":"code","f6c04034":"code","6bde52ac":"code","bb3b5b80":"code","c2e520f6":"code","0a382bb5":"code","0b797f6d":"code","00401e0c":"code","3f61accc":"code","c1084a77":"code","79808e91":"code","133a0d57":"code","ff911754":"code","411c692a":"code","3342095b":"code","634d13a8":"code","d446059b":"code","202e3548":"code","6e69054e":"code","058fa194":"code","fb704bb7":"code","e84d42db":"code","49b4c197":"code","c826e943":"code","cb07b410":"code","6a862a07":"markdown","c328e8fa":"markdown","2890c26c":"markdown","176ce829":"markdown","b0401f10":"markdown","1a16ac80":"markdown","96556b49":"markdown","8f43a231":"markdown","f573e503":"markdown","cc72c800":"markdown","beab2e1e":"markdown","bd2cf1bc":"markdown","3fe83ede":"markdown","8ecf697d":"markdown","32fc8db9":"markdown","b7b9e7ed":"markdown","0103ff66":"markdown","9bf22975":"markdown","f1382b92":"markdown","fda9b344":"markdown","72f90fb6":"markdown"},"source":{"ea5417db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score\nfrom scipy.stats import shapiro\nfrom sklearn.model_selection import cross_val_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.stats.diagnostic import normal_ad, het_breuschpagan\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import minmax_scale\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","e27e5e20":"df = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","3c8e85bb":"df.head()","a728c1c7":"df.info()","645f67df":"msno.matrix(df)","045d1247":"def plotBoxplot(data):\n    fig, axes = plt.subplots(ncols=3, nrows=4, figsize=(15,15))\n    fig.tight_layout(pad=4.0)\n\n    col = 0\n    row = 0\n    colors = ['#bad9e9', '#7ab6d6', '#3c8abd']\n\n    for i, column in enumerate(data.columns):\n        sns.boxplot(y=column, data=data, ax=axes[row][col], color=colors[col])\n\n        if (i + 1) % 3 == 0:\n            row += 1\n            col = 0\n        else:\n            col += 1\n            \nplotBoxplot(df)","2d03d4f8":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\n\nprint(IQR)","fbf4dfd1":"df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\ndf.shape","ab5e4cfd":"#create tmp train\/test split for assumptions test\nX = df.drop(['alcohol'], axis=1)\ny = df['alcohol']\n\nX = sm.add_constant(X)\nx_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=50)\n\nmodel = sm.OLS(y_train, x_train).fit()\nprint(model.summary())","bf5e7734":"y_pred = model.predict(x_test)","0395bd78":"#plot the actual vs predicted values\nsns.regplot(y_test, y_pred, line_kws={'color':'red'}, ci=None)\n\nplt.xlabel('Actual')\nplt.ylabel('Predictions')\nplt.title('Prediction vs Actual')\n\nplt.show()","f6c04034":"plt.figure(figsize=(25, 10))\nsns.heatmap(df.loc[:, df.columns != 'alcohol'].corr(), annot=True, fmt='.2f')","6bde52ac":"def calVIF(X):\n    vif = pd.DataFrame()\n    vif['variables'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n\n    return vif\n\ncalVIF(df.loc[:, df.columns != 'alcohol'])","bb3b5b80":"#Breusch-Pagan test\nbp = het_breuschpagan(model.resid, model.model.exog)\nprint('Lagrange multiplier statistic: {0} and p-value: {1}'.format(bp[0], bp[1]))","c2e520f6":"plt.title('Distribution of Residuals')\nsns.distplot(model.resid)\nplt.show()","0a382bb5":"#Anderson-Darling test\ndef normalityError(residuals):\n    p_value = normal_ad(residuals)[1]\n\n    print('p-value from the test:', p_value)\n\n    if p_value < 0.05:\n        print('Residuals are not normally distributed')\n    else:\n        print('Residuals are normally distributed')\n        \nnormalityError(model.resid)","0b797f6d":"calVIF(df[['citric acid','residual sugar','density', 'sulphates']])","00401e0c":"#skewness and kurtosis\ndef skew(columns, df):\n    dfs = df[columns].agg(['skew', 'kurtosis']).transpose()\n    for index, row in dfs.iterrows():\n        if (abs(row['skew']) > 1):\n            dfs.loc[index,'label'] = 'highly skewed'\n        elif (0.5 < abs(row['skew']) < 1):\n            dfs.loc[index,'label'] = 'moderately  skewed'\n        else:\n            dfs.loc[index,'label'] = 'approximately symmetric'\n        \n    return dfs\n\ncolumns = ['citric acid','residual sugar','density', 'sulphates']\nskew(columns, df)","3f61accc":"#Shapiro-Wilk test\ndef shapiroWilk(columns, df):\n    data = []\n\n    for column in columns:\n        stat, p = shapiro(df[column])\n        if (p < 0.05):\n            label = 'The null hypothesis can be rejected.'\n        else:\n            label = 'The null hypothesis cannot be rejected.'\n\n        data.append([column, stat, p, label])\n\n    return pd.DataFrame(data, columns=['column', 'statistic', 'p-value', 'label'])\n    \n\ncolumns = ['residual sugar', 'sulphates']\nshapiroWilk(columns, df)","c1084a77":"def transformDistribution(columns, method):\n    fig, axes = plt.subplots(ncols=2, nrows=len(columns), figsize=(10, 8))\n    fig.tight_layout(pad=4)\n    \n    for i, column in enumerate(columns):\n        if method == 'sqrt':\n            trans = np.sqrt(df[column])\n        elif method == 'log':\n            trans = np.log10(df[column]+1)\n        else:\n            trans, params = stats.boxcox(df[column]+1)\n    \n        sns.distplot(df[column], ax=axes[i][0])\n        sns.distplot(trans, ax=axes[i][1])\n\n        axes[i][0].set_title('Distribution of {0}'.format(column))\n        axes[i][1].set_title('Distribution of {0} (log)'.format(column))\n        axes[i][1].set_xlabel('{0} (log)'.format(column))\n        \n    plt.show()\n    \ntransformDistribution(['residual sugar', 'sulphates'], 'log')","79808e91":"df['residual_sugar_log'] = np.log(df['residual sugar'])\ndf['sulphates_log'] = np.log(df['sulphates'])","133a0d57":"skew(['residual_sugar_log', 'sulphates_log'], df)","ff911754":"dfScaled = minmax_scale(df[['citric acid','density','residual_sugar_log','sulphates_log']])\n \ndf['citric_acid_norm'] = dfScaled[:,0]\ndf['density_norm'] = dfScaled[:,1]\ndf['residual_sugar_norm'] = dfScaled[:,2]\ndf['sulphates_norm'] = dfScaled[:,3]\n\ndf.head()","411c692a":"df = df[['citric_acid_norm','density_norm','residual_sugar_norm','sulphates_norm', 'alcohol']]\ndf.head()","3342095b":"X = df.drop(['alcohol'], axis=1)\ny = df['alcohol']\n\nX = sm.add_constant(X)\nx_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=50)","634d13a8":"model = sm.OLS(y_train, x_train).fit()\nprint(model.summary())","d446059b":"y_pred = model.predict(x_test)","202e3548":"sns.regplot(y_test, y_pred, line_kws={'color':'red'}, ci=None)\n\nplt.xlabel('Actual')\nplt.ylabel('Predictions')\nplt.title('Prediction vs Actual')\n\nplt.show()","6e69054e":"lr = LinearRegression()\nlr.fit(x_train, y_train)","058fa194":"print(lr.intercept_)","fb704bb7":"print(lr.coef_)","e84d42db":"y_pred = lr.predict(x_test)","49b4c197":"r2_score(y_test, y_pred)","c826e943":"mean_squared_error(y_test, y_pred)","cb07b410":"np.sqrt(mean_squared_error(y_test, y_pred))","6a862a07":"Hope you've enjoyed my work, if you like my work and need to share something with me leave a comment :)","c328e8fa":"**R-squared:** Signifies the \u201cpercentage variation in dependent that is explained by independent variables\u201d. In our case 53.7% variation of Y(alcohol) explained by citric acid, density, residual sugar and sulphates.\n\n**Adj. R-squared:** This is the modified version of R-squared which is adjusted for the number of variables in the regression. It increases only when an additional variable adds to the explanatory power to the regression.\n\nAccording to the results, we can see that all four coefficients are statistically significant. Based on the results, we can interpret the regression equation:\n\n\n$$ Y = 10.2203 + 0.9384(citric_i) -4.0054(density_i) +  1.7621(residual_i) + 1.4542(sulphates_i) $$ \n\nBases on the equation, we can interpret the impact of density as: 1 unit increase of density, the alcohol content decreased by 4.0054 on average, holding all other variables constant.","2890c26c":"After removing multicollinearity we have four variables. Let's address the next issue.\n\n### 2. Fix Normality of the Error Terms\n\nLet's measure the skewness of the selected variables, if skewness is 0, the data are perfectly symmetrical, although it is quite unlikely for real-world data. As a general rule of thumb:\n* If skewness is less than - 1 or greater than 1, the distribution is highly skewed.\n* If skewness is between - 1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\n* If skewness is between - 0.5 and 0.5, the distribution is approximately symmetric.","176ce829":"Even though the histogram looks normally distributed, the Anderson-Darling test suggests that residuals are not normally distributed. To fix this we can apply nonlinear transformations, excluding specific variables (such as long-tailed variables), or removing outliers. Based on the assumption test, we can see that there are two violations.\n\n1. Multicollinearity\n2. Normality of the Error Terms\n\nTherefore, let's fix those violations.\n\n<div class='alert alert-block alert-success' id='fsu'><strong>Fix Assumptions<\/strong><\/div>\n\n### 1. Fix Multicollinearity\n\nAs I mentioned earlier, to fix multicollinearity we have to remove highly correlated variables(with a high variance inflation factor). There is no upper limit in VIF and VIF that exceeds 10 is often regarded as indicating multicollinearity.","b0401f10":"<div class=\"alert alert-block alert-success\" id='rf'><strong>References<\/strong><\/div>\n\n* https:\/\/learningwithdata.com\/posts\/tylerfolkman\/the-ultimate-guide-to-linear-regression\/\n* https:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/all-you-need-to-know-about-your-first-machine-learning-model-linear-regression\/\n* https:\/\/towardsdatascience.com\/the-complete-guide-to-linear-regression-in-python-3d3f8f06bf8\n* https:\/\/www.keboola.com\/blog\/linear-regression-machine-learning\n* https:\/\/rstudio-pubs-static.s3.amazonaws.com\/57835_c4ace81da9dc45438ad0c286bcbb4224.html\n* https:\/\/www.itl.nist.gov\/div898\/handbook\/prc\/section1\/prc16.htm\n* https:\/\/www.kaggle.com\/nareshbhat\/outlier-the-silent-killer\n* https:\/\/towardsdatascience.com\/ways-to-detect-and-remove-the-outliers-404d16608dba\n* https:\/\/community.gooddata.com\/metrics-and-maql-kb-articles-43\/normality-testing-skewness-and-kurtosis-241\n* https:\/\/towardsdatascience.com\/methods-for-normality-test-with-application-in-python-bb91b49ed0f5\n* https:\/\/medium.com\/@TheDataGyan\/day-8-data-transformation-skewness-normalization-and-much-more-4c144d370e55\n* https:\/\/machinelearningmastery.com\/standardscaler-and-minmaxscaler-transforms-in-python\/  \n* https:\/\/medium.com\/@TheDataGyan\/day-8-data-transformation-skewness-normalization-and-much-more-4c144d370e55\n* https:\/\/jyotiyadav99111.medium.com\/statistics-how-should-i-interpret-results-of-ols-3bde1ebeec01","1a16ac80":"Let's understand the dataset, dataset contains 1599 samples and 12 variables including the target variable. We can see that 11 variables are numerical and 1 variable is ordinal.\n\n<div class=\"alert alert-block alert-success\" id='ds'><strong>The Dataset<\/strong><\/div>\n\n* fixed acidity: most acids involved with wine or fixed or nonvolatile (do not evaporate readily)\n* volatile acidity: the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste\n* citric acid: found in small quantities, citric acid can add \u2018freshness\u2019 and flavor to wines\n* residual sugar: the amount of sugar remaining after fermentation stops, it\u2019s rare to find wines with less than 1 gram\/liter and wines with greater than 45 grams\/liter are considered sweet\n* chlorides: the amount of salt in the wine\n* free sulfur dioxide: the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine\n* total sulfur dioxide: amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine\n* density: the density of water is close to that of water depending on the percent alcohol and sugar content\n* pH: describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale\n* sulphates: a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant\n* quality: score between 0 and 10\n* alcohol: the percent alcohol content of the wine - output varaible \n\nOk, now we have some idea about the data. Let's see if there are any missing values in the data.\n\n<div class=\"alert alert-block alert-success\" id='mda'><strong>Missing Data Aanalysis<\/strong><\/div>","96556b49":"We were able to train a decent linear regression model using available data. But our model was only able to explain 53.7% variance of Y(alcohol) and I think there will be some additional important factors that are related to red wine alcohol content. What do you think ?\n\n<div class=\"alert alert-block alert-success\" id='mtt'><strong>Model Training and Testing<\/strong><\/div>","8f43a231":"Well, our model is able to explain 59.6% variance of Y(alcohol) for the test data. While R Square\/Adjusted R Square are relative measure of how well the model fits dependent variables. MSE, RMSE, or MAE are better be used to compare performance between different regression models.","f573e503":"<div class=\"alert alert-block alert-success\" id='ols'><strong>Ordinary Least Squares(OLS)<\/strong><\/div>","cc72c800":"As I told earlier, there are different ways to treat outliers such as remove outliers from the dataset, KNN imputation, etc. However, I'm not a domain expert therefore in this scenario, I will remove outliers from the dataset.","beab2e1e":"We can see two variables are not normally distributed, to further confirm this we can conduct the Shapiro-Wilk test.\n\nShapiro-Wilk uses below hypothesis:\n* H0 = The sample comes from a normal distribution.\n* H1 = The sample is not coming from a normal distribution.\n\nIf the data is not normalized, we can apply a transformation to convert skewed distribution to a normal distribution\/less-skewed distribution.","bd2cf1bc":"We can see that there are some high correlated variables such as density, pH, and fixed acidity. To fix multicollinearity, we can remove highly correlated variables(with a high variance inflation factor). Let's go to the next assumption.\n\n### 3. Homoscedasticity\n\nThis assumes that the same variance within our error terms (no Heteroscedasticity).\n\n**If this assumption is not satisfied** - standard errors in output cannot be relied upon.\n\n**How to detect ?** conduct Breusch-Pagan test or Goldfeld-Quandt test.","3fe83ede":"We can see that predictions are relatively even spread around the diagonal line and this indicates that there is a linear relationship between independent and dependant variables. Ok, let's test the next assumption.\n\n### 2. No Multicollinearity among Independant Varaibles\n\nThis assumes that the independent variables used in the regression are not correlated with each other.\n\n**If this assumtions is not satisfied** - coefficients and standard errors of affected varaibles are unreliable.\n\n**How to detect ?** There are a few ways such as a heatmap of the correlation or variance inflation factor (VIF).","8ecf697d":"Ok, now it's time to test assumptions. To test the assumption, I'm going to use OLS(Ordinary Least Squares) model from statsmodels. Later, I will use LinearRegression model from sklearn and sklearn -> LinearRegression model also uses the Ordinary Least Squares techique.\n\n<div class='alert alert-block alert-success' id='asu'><strong>Assumptions<\/strong><\/div>\n\n### 1. Linearity\n\nThis assumes that there is a linear relationship between the independent variables and the dependent variable.\n\n**If this assumption is not statified** - The predictions will be extremely inaccurate because our model is underfitting. This is a serious violation that should not be ignored.\n\n**How to detect ?** If there is only one independent variable, this is pretty easy to test with a scatter plot. For multiple independent variables, we use a scatter plot to see our predicted values vs the actual values (residuals). The points should lie on or around a diagonal line on the scatter plot.","32fc8db9":"Ok, after log transformation distributions are approximately symmetric. Let's normalize data, normalization or scaling refers to bringing all the columns into the same range.","b7b9e7ed":"There are no missing data available on the dataset. The next most important thing is to test linear regression assumptions. If the assumptions are not satisfied, the interpretation of the results will not always be valid. This can be very dangerous depending on the application.\n\n<div class=\"alert alert-block alert-success\" id='out'><strong>Outliers<\/strong><\/div>\n\n> In statistics, an outlier is an observation point that is distant from other observations. - Wikipedia\n\nOutliers should be investigated carefully. Often they contain valuable information about the process under investigation or the data gathering and recording process. Before considering the possible elimination of these points from the data, one should try to understand why they appeared and whether it is likely similar values will continue to appear. Of course, outliers are often bad data points.  We can use a box plot to see the outliers graphically.","0103ff66":"Model evaluation is very important. It helps you to understand the performance of yhe model and makes it easy to present the model. Tere are 3 main metrics for model evaluation in regression:\n\n1. R Square\/Adjusted R Square\n2. Mean Square Error(MSE)\/Root Mean Square Error(RMSE)\n3. Mean Absolute Error(MAE)","9bf22975":"<div class=\"alert alert-block alert-info\"><strong>Content<\/strong><\/div>\n<div class=\"list-group\">\n    <a class=\"list-group-item list-group-item-action\" href=\"#ds\">The Dataset<\/a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#mda\">Missing Data Aanalysis<\/a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#out\">Outliers<\/a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#asu\">Assumptions<\/a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#fsu\">Fix Assumptions<\/a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#ols\">Ordinary Least Squares(OLS)<\/a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#mtt\">Model Training and Testing<\/a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#rf\">References<\/a>\n    \n<\/div>\n\n\nIn this notebook, I'm going to take a different approach and try to find whether there is approximately a linear relationship between Alcohol content and other provided factors. Moreover, try to find how accurately we can predict Alcohol content. \n\nBefore starting the analysis let me give you a brief introduction about the technique that we are going to use. We are going to use multiple linear regressions and linear Regression is a simple approach for supervised learning. In particular, Linear Regression is a useful tool for prediction a quantitavie response. Linear Regression can be perfomed as a Simple Linear Regression or Multiple Linear Regression. Both simple and multiple linear regressions assume that there is approximately a linear relationship between the inputs(X) and the output(Y). The main difference is the number of independent variables that they take as inputs(X). Simple linear regression just takes a single input(X), while multiple linear regression takes multiple inputs(X). Mathamatically we can write multiple linear regression as:\n\n$$ Y = W_0 + W_1X_1 + W_2X_2 + ... + W_nX_n $$ \n\nI'm not going to discuss more linear regression here, there are plenty of very good articles available so you can check them if you want to gain your knowledge about linear regression. In this task, our output variable (target variable in machine learning or the dependent variable in statistical modeling) is Alcohol content and all other inputs (features in machine learning or independent variables in statistical modeling) are input variables. ","f1382b92":"Box plot use the Inter Quartile Range(IQR) to display data and outliers(shape of the data) but in order to be get a list of identified outlier, we have to calculate IQR.\n\n> The interquartile range (IQR), also called the midspread or middle 50%, or technically H-spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q3 \u2212 Q1. - Wikipedia\n","fda9b344":"According to the results of the Breusch-Pagan test, we can see that the Lagrange multiplier statistic for the test is 302.83 and the corresponding p-value is 3.91. And hypotheses of this test are:\n\n* H0: Homoscedasticity is present\n* H1: Homoscedasticity is not present (i.e. heteroscedasticity exists)\n\nBecause this p-value is not less than 0.05, we fail to reject the null hypothesis. Therefore, we do not have sufficient evidence to say that heteroscedasticity is present in the regression model.\n\n### 4. Normality of the Error Terms\n\nThis assumes that the error terms of the model are normally distributed.\n\n**If this assumption is not satisfied** - violation of this assumption could affect standard errors in the output.\n\n**How to detect ?** There are different ways to do so, histogram or Q-Q plot, Shapiro-Wilk test, Kolmogorov-Smirnov, and the Anderson-Darling test for normality.","72f90fb6":"To further clarify this, we can do the Anderson-Darling test. Hypotheses for the Anderson-Darling test for the normal distribution are given below:\n\n* H0: The data follows the normal distribution\n* H1: The data do not follow the normal distribution"}}