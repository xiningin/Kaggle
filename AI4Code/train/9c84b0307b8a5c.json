{"cell_type":{"b0593467":"code","2e5aebb3":"code","692fd405":"code","50e9e307":"code","3d02d530":"code","f3d409bc":"code","f5b11d39":"code","4990d05f":"code","794cb71b":"code","b6412bc7":"code","2a02a795":"code","7eb7f39f":"code","0837a547":"code","3f9b3790":"code","d3772d76":"code","edc793c0":"code","0f8afd98":"code","ef4bf25b":"code","db82c884":"code","c1a67f9d":"code","a0ae7aa5":"code","323c088c":"code","46b2ab59":"code","87c8c946":"code","cc8f8a75":"code","d36fe31d":"code","c6fbbf15":"code","97b22bfe":"code","3d98ee10":"code","364ca685":"markdown","370bf274":"markdown","e5301654":"markdown","8585adda":"markdown","5314ba14":"markdown","7f0b005c":"markdown","9ae6aef9":"markdown","36f2a08e":"markdown","aa182f86":"markdown","6d818d9c":"markdown"},"source":{"b0593467":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom keras.utils.np_utils import to_categorical\nfrom tensorflow.keras.optimizers import RMSprop\nfrom sklearn.model_selection import train_test_split\n\nimport os\n\n\nnp.random.seed(92)\n# Any results you write to the current directory are saved as output.","2e5aebb3":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","692fd405":"train_data = '\/kaggle\/input\/digit-recognizer\/train.csv'\ntest_data = '\/kaggle\/input\/digit-recognizer\/test.csv'\n","50e9e307":"train_df = pd.read_csv(train_data)\nprint(train_df.shape)\ntrain_df.head()\n\n","3d02d530":"test_df = pd.read_csv(test_data)\nprint(test_df.shape)\ntest_df.head()","f3d409bc":"# LET'S SEPARATE THE LABELS FROM THE ACTUAL DATA\n\nif 'label' in train_df.columns:   \n    y_train = train_df['label'].values.astype('int32')\n#     y_train = train.iloc[:,0].values.astype('int32')\n#     y_train = \n    train_df = train_df.drop('label', axis = 1)\nelse:\n    pass\n\n# NOW TO SET UP THE X PART OF THE TRAINING DATA.\n\nx_train = train_df.values.astype('float32')\nx_test = test_df.values.astype('float32')","f5b11d39":"print('the shape of the training data is ;',x_train.shape)\nprint('the shape for training labels is  :',y_train.shape[0])\n\nprint('\\nthe shape for the testing data is:', x_test.shape)","4990d05f":"# LET'S TAKE A LOOK AT SOME OF THE IMAGES.\n\n#Convert train datset to (num_images, img_rows, img_cols) format \nx_train = x_train.reshape(x_train.shape[0], 28, 28)\n\nfor i in range(6, 9):\n    plt.subplot(330 + (i+1))\n    plt.imshow(x_train[i], cmap=plt.get_cmap('gray'))\n    plt.title(y_train[i])","794cb71b":"x_train = x_train.reshape(x_train.shape[0], 28,28,1)","b6412bc7":"x_train.shape","2a02a795":"x_test  = x_test.reshape(x_test.shape[0],28,28,1)\nx_test.shape","7eb7f39f":"y_train.shape","0837a547":"train_max = np.max(x_train)\ntrain_min = np.min(x_train)\n# print(train_max)\n\ntest_max = np.max(x_test)\ntest_min = np.min(x_test)\n","3f9b3790":"x_train = x_train\/255.0\nx_test = x_test\/255.0","d3772d76":"norm_train_max = np.max(x_train)\nnorm_train_min = np.min(x_train)\n# print(train_max)\n\nnorm_test_max = np.max(x_test)\nnorm_test_min = np.min(x_test)","edc793c0":"print('The maximum of the training data before normalization was',train_max,'but after normalizing it is', norm_train_max )\nprint('The maximum of the testing data before normalization was',test_max,'but after normalizing it is', norm_test_max )\n\nprint('\\nThe minimum of the training data before normalization was',train_min,'but after normalizing it is', norm_train_min )\nprint('The minimum of the testing data before normalization was',test_min,'but after normalizing it is', norm_test_min )","0f8afd98":"y_train= to_categorical(y_train)\nnum_classes = y_train.shape[1]\nnum_classes\n","ef4bf25b":"model = tf.keras.models.Sequential([\n    \n    # 1st BATCH OF COVOLUTIONS \n    tf.keras.layers.Conv2D(16, (5,5),activation='relu', input_shape=(28,28,1)),\n    tf.keras.layers.Conv2D(16, (5,5), activation= 'relu'),\n    tf.keras.layers.Conv2D(16, (5,5), activation= 'relu'),\n    tf.keras.layers.BatchNormalization(),\n#     tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.25),\n    \n    # 2nd BATCH OF CONVOLUTIONS \n    tf.keras.layers.Conv2D(32, (3,3), activation= 'relu'),\n    tf.keras.layers.Conv2D(32, (3,3), activation= 'relu'),  \n    tf.keras.layers.Conv2D(32, (3,3), activation = 'relu'),\n    tf.keras.layers.BatchNormalization(),\n# #     tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.25),\n    \n#     # 3nd BATCH OF CONVOLUTIONS \n#     tf.keras.layers.Conv2D(64, (3,3), activation= 'relu'),\n#     tf.keras.layers.Conv2D(64, (3,3), activation= 'relu'),  \n#     tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'),\n#     tf.keras.layers.BatchNormalization(),\n# #     tf.keras.layers.MaxPooling2D(2,2),\n#     tf.keras.layers.Dropout(0.25),\n    \n    tf.keras.layers.BatchNormalization(),    \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.50),\n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256, activation = 'relu'),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(10, activation='softmax')   \n   \n   \n])\n\nprint('input shape  :', model.input_shape)\nprint('output shape :', model.output_shape)","db82c884":"# VERSION 3\n\n# model = tf.keras.models.Sequential([\n    \n#    # 1ST BACTCH OF CONVOLUTIONS\n    \n#     # This is the first convolution batch \n#     tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,1), data_format = 'channels_last'),\n#     # The second convolution\n#     tf.keras.layers.Conv2D(16, (3,3), activation='relu'),     \n#     # The third convolution\n#     tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n    \n#     tf.keras.layers.BatchNormalization(),    \n#     tf.keras.layers.Dropout(0.25),\n    \n#     # 2ND BACTCH OF CONVOLUTIONS\n    \n#     # This is the first convolution batch \n#     tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n#     # The second convolution\n#     tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n#     # The third convolution\n#     tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n\n#     tf.keras.layers.BatchNormalization(),    \n#     tf.keras.layers.MaxPooling2D(2,2),\n#     tf.keras.layers.Dropout(0.25),\n    \n#     # 3RD BACTCH OF CONVOLUTIONS\n    \n#     # This is the first convolution batch \n#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n# #     tf.keras.layers.MaxPooling2D(2,2),\n#     # The second convolution\n#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n# #     tf.keras.layers.MaxPooling2D(2,2),\n#     # The third convolution\n#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    \n#     tf.keras.layers.BatchNormalization(),    \n#     tf.keras.layers.MaxPooling2D(2,2),\n#     tf.keras.layers.Dropout(0.25),\n    \n#     # Flatten the results to feed into a DNN\n#     tf.keras.layers.Flatten(),\n#     tf.keras.layers.Dense(256),\n#     tf.keras.layers.Dropout(0.25),\n#     tf.keras.layers.Dense(10, activation='softmax')\n# ])\n\n# print('input shape  :', model.input_shape)\n# print('output shape :', model.output_shape)","c1a67f9d":"# # VERSION 2\n\n# model = tf.keras.models.Sequential([\n    \n#    # 1ST BACTCH OF CONVOLUTIONS\n    \n#     # This is the first convolution batch \n#     tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,1), data_format = 'channels_last'),\n#     tf.keras.layers.BatchNormalization(),\n#     # The second convolution\n#     tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n#     tf.keras.layers.BatchNormalization(),\n#     # The third convolution\n#     tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n#     tf.keras.layers.BatchNormalization(),\n#     tf.keras.layers.Dropout(0.4),\n    \n#     # 2ND BACTCH OF CONVOLUTIONS\n    \n#     # This is the first convolution batch \n#     tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n#     tf.keras.layers.BatchNormalization(),\n#     # The second convolution\n#     tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n#     tf.keras.layers.BatchNormalization(),\n#     # The third convolution\n#     tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n#     tf.keras.layers.BatchNormalization(),    \n#     tf.keras.layers.Dropout(0.4),\n    \n#     # 3RD BACTCH OF CONVOLUTIONS\n    \n#     # This is the first convolution batch \n#     tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n#     tf.keras.layers.BatchNormalization(),\n#     # The second convolution\n#     tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n#     tf.keras.layers.BatchNormalization(),\n#     # The third convolution\n#     tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n#     tf.keras.layers.BatchNormalization(),    \n#     tf.keras.layers.Dropout(0.4),\n    \n#     # Flatten the results to feed into a DNN\n#     tf.keras.layers.Flatten(),\n#     tf.keras.layers.BatchNormalization(),\n#     tf.keras.layers.Dense(10, activation='softmax')\n# ])\n\n# print('input shape  :', model.input_shape)\n# print('output shape :', model.output_shape)","a0ae7aa5":"# VERSION 1\n\n# model = tf.keras.models.Sequential([\n   \n#     # This is the first convolution\n#     tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,1)),\n#     tf.keras.layers.MaxPooling2D(2, 2),\n    \n#     # The second convolution\n#     tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n#     tf.keras.layers.MaxPooling2D(2,2),\n    \n#     # The third convolution\n#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n#     tf.keras.layers.MaxPooling2D(2,2),\n    \n#     # Flatten the results to feed into a DNN\n#     tf.keras.layers.Flatten(),\n  \n#     tf.keras.layers.Dense(10, activation='softmax')\n# ])\n\n# print('input shape  :', model.input_shape)\n# print('output shape :', model.output_shape)","323c088c":"model.summary()","46b2ab59":"### COMPILE THE MODEL\nmodel.compile(loss = 'categorical_crossentropy', optimizer= RMSprop(lr=0.003), metrics = ['acc'])","87c8c946":"from keras.preprocessing import image\n\ntrain_generator = image.ImageDataGenerator()","cc8f8a75":"X = x_train\nY = y_train \n\nX_train, X_val, Y_train , Y_val = train_test_split(x_train,y_train, test_size= 0.05, random_state = 92)\nprint(X_train.shape)\nbatches = train_generator.flow(X_train, Y_train, batch_size=32)\nval_batches = train_generator.flow(X_val, Y_val, batch_size=32)","d36fe31d":"history = model.fit_generator(\n      generator=batches, \n      steps_per_epoch=batches.n,\n#       steps_per_epoch = 100,\n      epochs=20, \n      validation_data=val_batches,\n      validation_steps=val_batches.n,\n#     validation_steps = 100\n)","c6fbbf15":"# history.history['loss']\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training set', 'Validation set'], loc='center left')\nplt.show()","97b22bfe":"predictions = model.predict_classes(x_test, verbose=0)\n\n","3d98ee10":"submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": predictions})\nsubmissions.to_csv(\"DR.csv\", index=False, header=True)","364ca685":"## Version 3 (CNN architecture):\n    * Committed on 09-20-2019.\n    \n    * 3 batches of CNNs. In each batch you have. \n        *1. 3 layers of Convolutions. *    \n        *2. Batch Normalization layersat the end of each batch.    \n        *3. Dropout at the end of the batch with dropout likelihood of 0.25.*\n      \n    * Dropout layer with probability of 0.25 right before the data is fed to the output layer. \n    \n    * Score = 99.43% ","370bf274":"## VERSION 2 OF THE MODEL\n\n* Committed on 09-15-2019\n\n\n* 3 batches of CNNs. In each batch you have. \n\n    *1. 3 layers of Convolutions. *\n    \n    *2. Batch Normalization layers following each convolutional layer.\n    \n    *3. Dropout at the end of the batch with dropout likelihood of 0.4.*","e5301654":"### Tensorflow expects the input to be of [batch_size, image_height, image_width, channels]. So I will reformat the training and testing data to match that.\n","8585adda":"## Normalizing data by dividing  the x_train and x_test value by 255.\n\nThis helps our model find a local or global minima faster.\n","5314ba14":"### One-hot encoding the labels","7f0b005c":"## GETTING THINGS IN ORDER.\n\n![https:\/\/media.giphy.com\/media\/tJMVcTfzDdL1pOGxlk\/giphy.gif](https:\/\/media.giphy.com\/media\/tJMVcTfzDdL1pOGxlk\/giphy.gif)","9ae6aef9":"## Let's take a look at the validation and testing accuracy","36f2a08e":"## Version 4 (CNN architecture):\n\n* Committed on 09-201-2019.\n* 3 batches of CNNs. In each batch you have. \n\n    * 3 layers of Convolutions. with increasing filter sizes and decreasing kernel size. \n    \n    * Batch Normalization layersat the end of each batch followed by a dropout layer at the end of each batch.\n    \n* Dropout layer with probability of 0.25 right before the data is fed to the output layer. \n    ","aa182f86":"## In this kernel, I am trying to use Tensorflow to create a convolutional neural network which would predict the MNIST labels in the test set. \n\nThe code used here is a mixture of my own, some code from [Poonam Ligade's Kernel](https:\/\/www.kaggle.com\/poonaml\/deep-neural-network-keras-way), [Vivk's kernel](https:\/\/www.kaggle.com\/vivkvv\/digits-cnn-3-times-by-10-iterations-top-10),  [Yassine Ghouzam's kernel](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6#2.-Data-preparation)and following documentation on the official Tensorflow website. Over time I might come back to this kernel, update or add methods which might move the score up or down. \n\nI don't write a lot in detail here about what is going on under the hod of the neural network. However, I have a separate kernel [here](https:\/\/www.kaggle.com\/sanwal092\/3-layer-neural-network-from-scratch) which breaks down a lot more in detail in case anyone needs help getting started. \n\n\n## Version 4 (CNN architecture):\n    \n    * Committed on 09-20-2019.\n   \n    * 3 batches of CNNs. In each batch you have. \n        *1. 3 layers of Convolutions. *    \n        *2. Batch Normalization layers at the end of each batch.    \n        *3. Dropout at the end of the batch with dropout likelihood of 0.25.*\n              \n    * Dropout layer with probability of 0.25 right before the data is fed to the output layer. \n    \n    *\n        \n    \n\n## Version 3 (CNN architecture):\n    * Committed on 09-20-2019.\n    \n    * 3 batches of CNNs. In each batch you have. \n        *1. 3 layers of Convolutions. *    \n        *2. Batch Normalization layersat the end of each batch.    \n        *3. Dropout at the end of the batch with dropout likelihood of 0.25.*\n      \n    * Dropout layer with probability of 0.25 right before the data is fed to the output layer. \n    \n    * Score = 99.43% \n        \n    \n\n## Version 2 (CNN architecture):\n    * Committed on 09-15-2019.\n    \n    * 3 batches of CNNs. In each batch you have. \n        *1. 3 layers of Convolutions. *    \n        *2. Batch Normalization layers following each convolutional layer.    \n        *3. Dropout at the end of the batch with dropout likelihood of 0.4.*\n        \n    * Score= 99.257% \n\n## Version 1 (CNN architecture):\n    * Committed on 09-14-2019.\n    * 3 Convolutional layers. \n    * Max Pooling  layers after each convolutional layer. \n    * RELU activation used with each convolutional layers.\n    * Score= 97.2% \n    ","6d818d9c":"## VERSION 1 OF THE MODEL\n\n* Committed on 09-14-2019.\n* 3 Convolutional layers. \n* Max Pooling  layers after each convolutional layer. \n* RELU activation used with each convolutional layers.\n    "}}