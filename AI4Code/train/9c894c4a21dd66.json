{"cell_type":{"56fc2dd3":"code","2a5c19e2":"code","3a24423b":"code","dd3d785b":"code","d5da45a2":"code","5d61fc94":"code","ab72a017":"code","57e7861e":"code","f8bed746":"code","3c680957":"code","5c179702":"code","a693ce70":"code","0f571ef4":"code","0daa4c1c":"code","9c7d9380":"code","ac5f5982":"code","02dc5d3b":"code","f8ca3da1":"code","e194aa20":"code","bf1a7678":"code","6152dfbc":"code","c2363684":"code","56c53f8d":"code","8779ec66":"code","1d20d229":"code","62d7cc27":"code","9471de69":"code","01dd0532":"code","30631c52":"code","0b0950eb":"code","e63f4a87":"code","86af6da1":"code","167baca1":"code","4f073f63":"code","2e63891f":"code","69ec62c7":"code","c0ccfb97":"code","34b6fb77":"code","1205ea02":"code","ac82e1de":"code","e27d8197":"code","47598dbb":"code","85986789":"code","c83c4e8d":"code","ea493007":"code","8e01c514":"code","4ed61157":"markdown","6450c5b6":"markdown","35e77232":"markdown","7abbfc79":"markdown","564b7ca5":"markdown","bb2b3ff0":"markdown","d55def80":"markdown","4b2b43e7":"markdown","328a224a":"markdown","4b16ddf5":"markdown","e7ea1af7":"markdown"},"source":{"56fc2dd3":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport re\nimport string\nfrom collections import Counter\nimport nltk\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import confusion_matrix as CM\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2a5c19e2":"data=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\") #train\n\ntest=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\") #test\n\nsub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv') #submission","3a24423b":"#Checking GPU availability\n\nimport torch\nif torch.cuda.is_available():  \n    device = torch.device(\"cuda\")\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n    \nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","dd3d785b":"data.head() #Checking the first 5 rows in the train data","d5da45a2":"data.describe() #analysis of numerical values in the train data","5d61fc94":"data.shape #train data dimension","ab72a017":"test.shape #test data shape","57e7861e":"test.head() #Checking the first 5 rows in the test data","f8bed746":"#Count-plot of train set \n\nplt.figure(figsize=(9,6))\nsns.countplot(y=data.keyword, order = data.keyword.value_counts().iloc[:10].index)\nplt.title('Top 10 keywords')\nplt.show()","3c680957":"#Bar plot of disaster tweets and non-disaster tweets\n\nkw_d = data[data.target==1].keyword.value_counts().head(10) #First 10 Disaster tweets \nkw_nd = data[data.target==0].keyword.value_counts().head(10) #First 10 Non-disaster tweets\n\nplt.figure(figsize=(13,5))\n\n#Plot-1 for disaster tweets\nplt.subplot(121)\nsns.barplot(kw_d, kw_d.index, color='c')\nplt.title('Top keywords for disaster tweets')\n\n#Plot-2 for Non-disaster tweets\nplt.subplot(122)\nsns.barplot(kw_nd, kw_nd.index, color='y')\nplt.title('Top keywords for non-disaster tweets')\nplt.show()","5c179702":"\ntop_d = data.groupby('keyword').mean()['target'].sort_values(ascending=False).head(10)\ntop_nd = data.groupby('keyword').mean()['target'].sort_values().head(10)\n\nplt.figure(figsize=(13,5))\n\nplt.subplot(121)\nsns.barplot(top_d, top_d.index, color='pink')\nplt.title('Keywords with highest % of disaster tweets')\n\nplt.subplot(122)\nsns.barplot(top_nd, top_nd.index, color='yellow')\nplt.title('Keywords with lowest % of disaster tweets')\nplt.show()","a693ce70":"# Most common locations in the tweets\n\nplt.figure(figsize=(9,6))\nsns.countplot(y=data.location, order = data.location.value_counts().iloc[:15].index)\nplt.title('Top 15 locations')\nplt.show()","0f571ef4":"#locations plot\n\nraw_loc = data.location.value_counts()\ntop_loc = list(raw_loc[raw_loc>=10].index)\ntop_only = data[data.location.isin(top_loc)]\n\ntop_l = top_only.groupby('location').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l.index, y=top_l)\nplt.axhline(np.mean(data.target))\nplt.xticks(rotation=80)\nplt.show()","0daa4c1c":"#data-cleaning\n\ndef clean_text(text):\n    \n    sw = stopwords.words('english') #Stopwords from NLTK.Corpus\n    \n    text = re.sub(r\"[^a-zA-Z?.!,\u00bf]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n\n    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n    \n    html=re.compile(r'<.*?>') \n    \n    text = html.sub(r'',text) #Removing html tags\n    \n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\" + '_'\n    for p in punctuations:\n        text = text.replace(p,'') #Removing punctuations\n        \n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    \n    text = \" \".join(text) #removing stopwords\n    \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text) #Removing emojis\n    \n    return text","9c7d9380":"#list of mispelled words\n\nmispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}","ac5f5982":"#getting the misspell words\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\n\n#replacing the missepells\ndef replace_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)","02dc5d3b":"#applying the changes to the train data\n\ndata['text'] = data['text'].str.lower() #convert everything to lower-case words\n\ndata['text'] = data['text'].apply(lambda x: replace_misspell(x)) #replace the misspell words\n\ndata['text'] = data['text'].apply(lambda x: clean_text(x)) #clean the data using the above function\n\n\n\ndata.head(3) #check the changes","f8ca3da1":"#repeat the same process for the test data too\n\ntest['text'] = test['text'].str.lower()\n\ntest['text'] = test['text'].apply(lambda x: replace_misspell(x))\n\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\n\n\ntest.head(3)","e194aa20":"# Split data: 80% training + 20% validation data\n\ntraining_size = 6090\n\nx_train = data.text[0:training_size] #train input data\ny_train = data.target[0:training_size] #train output\n\nvalid_sentences = data.text[training_size:] #validation input data\nvalid_labels = data.target[training_size:] #validation output","bf1a7678":"#pre-processing\n#generate vectors\n\ntokenizer = Tokenizer(num_words=None)\ntokenizer.fit_on_texts(x_train)\n\nword_index = tokenizer.word_index\nnum_words = len(tokenizer.word_index) + 1","6152dfbc":"#convert into sequence and pad it\n\ntraining_sequences = tokenizer.texts_to_sequences(x_train)\ntraining_padded = pad_sequences(training_sequences, maxlen=20, padding='post', truncating='post')","c2363684":"# Similarly, vectorizing and padding for the validation data.\n\nvalidation_sequences = tokenizer.texts_to_sequences(valid_sentences)\nvalidation_padded = pad_sequences(validation_sequences, maxlen=20, padding='post', truncating='post')","56c53f8d":"# Similarly, vectorizing and padding for the test data.\n\ntest_sequences = tokenizer.texts_to_sequences(test.text)\ntest_padded = pad_sequences(test_sequences, maxlen=20, padding='post', truncating='post')","8779ec66":"# decode function to convert integer values back to text sequences to check the result\n\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndef decode(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n","1d20d229":"model1 = tf.keras.Sequential([\ntf.keras.layers.Embedding(num_words, 100),\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,recurrent_dropout=0.1)),\ntf.keras.layers.Dense(64, activation='relu'),\ntf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel1.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])","62d7cc27":"model1.summary()","9471de69":"#train the Model-1\n\nEPOCHS = 15\nhistory1 = model1.fit(training_padded, y_train, validation_data=(validation_padded, valid_labels), batch_size=30, epochs=EPOCHS, verbose=2)","01dd0532":"#Plotting Accuracy and Loss of Training and Validation set\n\ndef plot_history(history, metric, val_metric,EPOCHS):\n    acc = history.history[metric]\n    val_acc = history.history[val_metric]\n    loss=history.history['loss']\n    val_loss=history.history['val_loss']\n\n    epochs_range = range(EPOCHS)\n\n    fig=plt.figure(figsize=(12, 10))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.ylim([min(plt.ylim()),1])\n    plt.grid(True)\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.grid(True)\n    plt.title('Training and Validation Loss')\n    plt.show()\n\n\nplot_history(history1,'accuracy','val_accuracy',EPOCHS)","30631c52":"#predict using the trained Model-1\n\npred1 = model1.predict_classes(validation_padded)","0b0950eb":"#Measuring the performance using various metrics: Precision, Recall, F1 score, Overall Accuracy\n\ncnf_matrix = confusion_matrix(valid_labels,pred1)\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');\n\nprecision_scoreM1=precision_score(valid_labels,pred1, average=None)\nprint(\"\\n Model-1 Precision_score\\n\", precision_scoreM1)\n\nrecall_scoreM1=recall_score(valid_labels,pred1, average=None)\nprint(\"\\n Model-1 Recall_score\\n\",recall_scoreM1)\n\naccuracy_scoreM1=accuracy_score(valid_labels,pred1)\nprint(\"\\n Model-1 Accuracy_score\\n\",accuracy_scoreM1)\n\nf1_scoreM1=f1_score(valid_labels,pred1, average=None)\nprint(\"\\n Model-1 F1_score\\n\",f1_scoreM1)","e63f4a87":"embedding_dict={}\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","86af6da1":"words_na = []\nembedding_matrix = np.zeros((num_words,100))\nword_index = tokenizer.word_index\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    emb_vec=embedding_dict.get(word)\n    \n    if emb_vec is None:\n        \n        \n        words_na.append(word)\n    \n    elif emb_vec is not None:\n        embedding_matrix[i]=emb_vec","167baca1":"print(\"Out of vocabulory words:\",len(words_na))","4f073f63":"model2 = tf.keras.Sequential([\ntf.keras.layers.Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),trainable=False),\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\ntf.keras.layers.Dense(64, activation='relu'),\ntf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel2.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy']) ","2e63891f":"model2.summary()","69ec62c7":"#train Model-2\n\nhistory21 = model2.fit(training_padded, y_train,epochs=EPOCHS,batch_size=12, validation_data=(validation_padded, valid_labels), verbose=2)","c0ccfb97":"#Plot the results of Model-2\n\nplot_history(history21,'accuracy','val_accuracy',EPOCHS)","34b6fb77":"#Predict using the trained Model-2\n\npred21 = model2.predict_classes(validation_padded)","1205ea02":"#Measuring the performance using various metrics: Precision, Recall, F1 score, Overall Accuracy\n\ncnf_matrix = confusion_matrix(valid_labels,pred21)\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');\n\n\nprecision_scoreM2=precision_score(valid_labels,pred21, average=None)\nprint(\"\\n Model-21 Precision_score\\n\", precision_scoreM2)\n\nrecall_scoreM2=recall_score(valid_labels,pred21, average=None)\nprint(\"\\n Model-21 Recall_score\\n\",recall_scoreM2)\n\naccuracy_scoreM2=accuracy_score(valid_labels,pred21)\nprint(\"\\n Model-21 Accuracy_score\\n\",accuracy_scoreM2)\n\nf1_scoreM2=f1_score(valid_labels,pred21, average=None)\nprint(\"\\n Model-21 F1_score\\n\",f1_scoreM2)","ac82e1de":"#configure the early-stop\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0,restore_best_weights=True, mode='auto')\n\n#train the Model-2 again but this time with early-stop\nhistory22 = model2.fit(training_padded, y_train,epochs=EPOCHS,batch_size=12, validation_data=(validation_padded, valid_labels), verbose=2,callbacks=[earlystop])","e27d8197":"#Plot results of early-stop\nplot_history(history22,'accuracy','val_accuracy',6)","47598dbb":"#Predict\npred22 = model2.predict_classes(validation_padded)","85986789":"#Measuring the performance using various metrics: Precision, Recall, F1 score, Overall Accuracy\n\ncnf_matrix = confusion_matrix(valid_labels,pred22)\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');\n\n\nprecision_scoreM22=precision_score(valid_labels,pred22, average=None)\nprint(\"\\n Model-22 Precision_score\\n\", precision_scoreM22)\n\nrecall_scoreM22=recall_score(valid_labels,pred22, average=None)\nprint(\"\\n Model-22 Recall_score\\n\",recall_scoreM22)\n\naccuracy_scoreM22=accuracy_score(valid_labels,pred22)\nprint(\"\\n Model-22 Accuracy_score\\n\",accuracy_scoreM22)\n\nf1_scoreM22=f1_score(valid_labels,pred22, average=None)\nprint(\"\\n Model-22 F1_score\\n\",f1_scoreM22)","c83c4e8d":"#predict the test data\n\ntest_pred = model2.predict(test_padded) ","ea493007":"sub['target'] = (test_pred > 0.5).astype(int)","8e01c514":"sub.to_csv(\"submission.csv\", index=False, header=True)","4ed61157":"# 4. Data Cleaning & Pre-processing","6450c5b6":"# 6. Model-2\n\nUsing a pre-trained model","35e77232":"# 3. Data Description and Visualization","7abbfc79":"# 1. Importing Libraries","564b7ca5":"**EE258 Neural Networks**\nProject-2\nFall 2020\n\n\nBy:\n1)Rojin Zandi\n2)Shifa Shaikh","bb2b3ff0":"Using a Regularization technique called \"Early Stopping\"","d55def80":"# 5. Model-1","4b2b43e7":"**Tokenization**","328a224a":"# 2. Reading data from the directories","4b16ddf5":"**Splitting data**","e7ea1af7":"Reference - https:\/\/www.kaggle.com\/mlwhiz\/textcnn-pytorch-and-keras"}}