{"cell_type":{"e882923c":"code","aeb91abc":"code","cdd50a4f":"code","ff12807b":"code","fffe8428":"code","b4cb9ebf":"code","344780e7":"code","bb3cf4dc":"code","e767a779":"code","59dbe158":"code","8ff23088":"code","31c18b7e":"code","c0d14844":"code","ef800805":"code","f4bbd784":"code","fc5ede9f":"code","c124f9c4":"code","096d3fc3":"markdown","37682fd5":"markdown","ab08f272":"markdown","2d49ce17":"markdown","db46451d":"markdown","8a6c5a76":"markdown","1a9cb17b":"markdown","02d0d1c3":"markdown","b180cbde":"markdown","69fffec1":"markdown","13d4a830":"markdown","052a29ca":"markdown","3c736998":"markdown","56daf014":"markdown","c46f2dc2":"markdown","33c64e99":"markdown","ed77e591":"markdown"},"source":{"e882923c":"%matplotlib inline\nimport mxnet as mx\nfrom mxnet import gluon\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport json\nimport os\nfrom tqdm.autonotebook import tqdm\nfrom pathlib import Path\n\n!pip install gluonts","aeb91abc":"single_prediction_length = 28\nsubmission_prediction_length = single_prediction_length * 2\nm5_input_path=\"..\/input\/m5-forecasting-uncertainty\"\nsubmission=True\n\nif submission:\n    prediction_length = submission_prediction_length\nelse:\n    prediction_length = single_prediction_length","cdd50a4f":"calendar = pd.read_csv(f'{m5_input_path}\/calendar.csv')\nsales_train_validation = pd.read_csv(f'{m5_input_path}\/sales_train_validation.csv')\nsample_submission = pd.read_csv(f'{m5_input_path}\/sample_submission.csv')\nsell_prices = pd.read_csv(f'{m5_input_path}\/sell_prices.csv')","ff12807b":"cal_features = calendar.drop(\n    ['date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1', 'event_name_2', 'd'], \n    axis=1\n)\ncal_features['event_type_1'] = cal_features['event_type_1'].apply(lambda x: 0 if str(x)==\"nan\" else 1)\ncal_features['event_type_2'] = cal_features['event_type_2'].apply(lambda x: 0 if str(x)==\"nan\" else 1)\n\ntest_cal_features = cal_features.values.T\nif submission:\n    train_cal_features = test_cal_features[:,:-submission_prediction_length]\nelse:\n    train_cal_features = test_cal_features[:,:-submission_prediction_length-single_prediction_length]\n    test_cal_features = test_cal_features[:,:-submission_prediction_length]\n\ntest_cal_features_list_orig = [test_cal_features] * len(sales_train_validation)\ntrain_cal_features_list_orig = [train_cal_features] * len(sales_train_validation)","fffe8428":"state_ids = sales_train_validation[\"state_id\"].astype('category').cat.codes.values + 1\nstate_ids_un , state_ids_counts = np.unique(state_ids, return_counts=True)\nstate_ids_dict = dict(zip(sales_train_validation[\"state_id\"], sales_train_validation[\"state_id\"].astype('category').cat.codes + 1))\n\nstore_ids = sales_train_validation[\"store_id\"].astype('category').cat.codes.values + 1\nstore_ids_un , store_ids_counts = np.unique(store_ids, return_counts=True)\nstore_ids_dict = dict(zip(sales_train_validation[\"store_id\"], sales_train_validation[\"store_id\"].astype('category').cat.codes + 1))\n\ncat_ids = sales_train_validation[\"cat_id\"].astype('category').cat.codes.values + 1\ncat_ids_un , cat_ids_counts = np.unique(cat_ids, return_counts=True)\ncat_ids_dict = dict(zip(sales_train_validation[\"cat_id\"], sales_train_validation[\"cat_id\"].astype('category').cat.codes + 1))\n\ndept_ids = sales_train_validation[\"dept_id\"].astype('category').cat.codes.values + 1\ndept_ids_un , dept_ids_counts = np.unique(dept_ids, return_counts=True)\ndept_ids_dict = dict(zip(sales_train_validation[\"dept_id\"], sales_train_validation[\"dept_id\"].astype('category').cat.codes + 1))\n\nitem_ids = sales_train_validation[\"item_id\"].astype('category').cat.codes.values + 1\nitem_ids_un , item_ids_counts = np.unique(item_ids, return_counts=True)\nitem_ids_dict = dict(zip(sales_train_validation[\"item_id\"], sales_train_validation[\"item_id\"].astype('category').cat.codes + 1))\n\nstat_cat_list = [item_ids, dept_ids, cat_ids, store_ids, state_ids]\n\nstat_cat_orig = np.concatenate(stat_cat_list)\nstat_cat_orig = stat_cat_orig.reshape(len(stat_cat_list), len(item_ids)).T\n\nstat_cat_cardinalities = [len(item_ids_un) + 1, len(dept_ids_un) + 1, len(cat_ids_un) + 1, len(store_ids_un) + 1, len(state_ids_un) + 1]","b4cb9ebf":"import itertools\n\nstates = [\"CA\", \"TX\", \"WI\"]\nstates_alt = [\"WI\", \"CA\", \"TX\"]\nstores = [\"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\", \"TX_1\", \"TX_2\", \"TX_3\", \"WI_1\", \"WI_2\", \"WI_3\"]\ncategories = [\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"]\ndepartments = [\"FOODS_1\", \"FOODS_2\", \"FOODS_3\", \"HOBBIES_1\", \"HOBBIES_2\", \"HOUSEHOLD_1\", \"HOUSEHOLD_2\"]\ndepartments_counts = {\n    \"FOODS_1\": 219,\n    \"FOODS_2\": 399,\n    \"FOODS_3\": 827,\n    \"HOBBIES_1\": 424,\n    \"HOBBIES_2\": 149,\n    \"HOUSEHOLD_1\": 541,\n    \"HOUSEHOLD_2\": 516\n}\n\ndef select_series(identifier=\"\", item_id=\"\", dept_id=\"\", cat_id=\"\", store_id=\"\", state_id=\"\"):\n    return sales_train_validation[(sales_train_validation['id'].str.contains(identifier)) &\n                                  (sales_train_validation['item_id'].str.contains(item_id)) &\n                                  (sales_train_validation['dept_id'].str.contains(dept_id)) &\n                                  (sales_train_validation['cat_id'].str.contains(cat_id)) &\n                                  (sales_train_validation['store_id'].str.contains(store_id)) &\n                                  (sales_train_validation['state_id'].str.contains(state_id))]\n\nagg_series = np.empty((0,1913), int)\nagg_cat = np.empty((0,5), int)\n\nagg_series = np.append(agg_series, [select_series().iloc[:,6:].values.sum(axis=0)], axis=0)\nagg_cat = np.append(agg_cat, [[0,0,0,0,0]], axis=0)\n\nfor state in tqdm(states):\n    agg_series = np.append(agg_series, [select_series(identifier=state).iloc[:,6:].values.sum(axis=0)], axis=0)\n    agg_cat = np.append(agg_cat, [[0,0,0,0,state_ids_dict[state]]], axis=0)\n    \nfor store in tqdm(stores):\n    agg_series = np.append(agg_series, [select_series(identifier=store).iloc[:,6:].values.sum(axis=0)], axis=0)\n    agg_cat = np.append(agg_cat, [[0,0,0,store_ids_dict[store],0]], axis=0)\n\nfor category in tqdm(categories):\n    agg_series = np.append(agg_series, [select_series(identifier=category).iloc[:,6:].values.sum(axis=0)], axis=0)\n    agg_cat = np.append(agg_cat, [[0,0,cat_ids_dict[category],0,0]], axis=0)\n\nfor department in tqdm(departments):\n    agg_series = np.append(agg_series, [select_series(identifier=department).iloc[:,6:].values.sum(axis=0)], axis=0)\n    agg_cat = np.append(agg_cat, [[0,dept_ids_dict[department],0,0,0]], axis=0)\n    \n    \nstates_categories = list(itertools.product(states,categories))\nfor sc in tqdm(states_categories):\n    agg_series = np.append(agg_series, [select_series(state_id=sc[0], cat_id=sc[1]).iloc[:,6:].values.sum(axis=0)], axis=0)\n    agg_cat = np.append(agg_cat, [[0,0,cat_ids_dict[sc[1]],0,state_ids_dict[sc[0]]]], axis=0)\n    \nstates_departments = list(itertools.product(states,departments))\nfor sd in tqdm(states_departments):\n    agg_series = np.append(agg_series, [select_series(state_id=sd[0], dept_id=sd[1]).iloc[:,6:].values.sum(axis=0)], axis=0)\n    agg_cat = np.append(agg_cat, [[0,dept_ids_dict[sd[1]],0,0,state_ids_dict[sd[0]]]], axis=0)\n    \nstores_categories = list(itertools.product(stores,categories))\nfor sc in tqdm(stores_categories):\n    agg_series = np.append(agg_series, [select_series(store_id=sc[0], cat_id=sc[1]).iloc[:,6:].values.sum(axis=0)], axis=0)\n    agg_cat = np.append(agg_cat, [[0,0,cat_ids_dict[sc[1]],store_ids_dict[sc[0]],0]], axis=0)\n    \nstores_departments = list(itertools.product(stores,departments))\nfor sd in tqdm(stores_departments):\n    agg_series = np.append(agg_series, [select_series(store_id=sd[0], dept_id=sd[1]).iloc[:,6:].values.sum(axis=0)], axis=0)\n    agg_cat = np.append(agg_cat, [[0,dept_ids_dict[sd[1]],0,store_ids_dict[sd[0]],0]], axis=0)\n    \nfor department in tqdm(departments):\n    for t_id in tqdm(range(1, departments_counts[department] + 1)):\n        ident_str = f'{department}_{t_id:03}'\n        series = select_series(item_id=ident_str).iloc[:,6:].values.sum(axis=0)\n        if np.sum(series) > 0:\n            agg_series = np.append(agg_series, [series], axis=0)\n            agg_cat = np.append(agg_cat, [[item_ids_dict[ident_str],dept_ids_dict[department],0,0,0]], axis=0)\n            \nfor state in tqdm(states_alt):\n    for department in tqdm(departments):\n        for t_id in tqdm(range(1, departments_counts[department] + 1)):\n            ident_str = f'{department}_{t_id:03}'\n            series = select_series(item_id=ident_str, state_id=state).iloc[:,6:].values.sum(axis=0)\n            if np.sum(series) > 0:\n                agg_series = np.append(agg_series, [series], axis=0)\n                agg_cat = np.append(agg_cat, [[item_ids_dict[ident_str],dept_ids_dict[department],0,0,state_ids_dict[state]]], axis=0)\n                \nnp.save(f'{m5_input_path}\/aggregates.npy', agg_series) \nnp.save(f'{m5_input_path}\/aggregates_cat.npy', agg_cat)","344780e7":"test_cal_features_list = [test_cal_features] * len(agg_series)\ntest_cal_features_list.extend(test_cal_features_list_orig)\ntrain_cal_features_list = [train_cal_features] * len(agg_series)\ntrain_cal_features_list.extend(train_cal_features_list_orig)\n\nstat_cat = np.append(agg_cat, stat_cat_orig, axis=0)\n\ntrain_df = sales_train_validation.drop([\"id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"], axis=1)\nseries_tot = np.append(agg_series, train_df.values, axis=0)","bb3cf4dc":"from gluonts.dataset.common import load_datasets, ListDataset\nfrom gluonts.dataset.field_names import FieldName\n\nif submission == True:\n    train_target_values = [ts for ts in series_tot]\n    test_target_values = [np.append(ts, np.ones(submission_prediction_length) * np.nan) for ts in series_tot]\nelse:\n    test_target_values = series_tot.copy()\n    train_target_values = [ts[:-single_prediction_length] for ts in series_tot]\n    \nm5_dates = [pd.Timestamp(\"2011-01-29\", freq='1D') for _ in range(len(train_target_values))]\n\ntrain_ds = ListDataset([\n    {\n        FieldName.TARGET: target,\n        FieldName.START: start,\n        FieldName.FEAT_DYNAMIC_REAL: fdr,\n        FieldName.FEAT_STATIC_CAT: fsc\n    }\n    for (target, start, fdr, fsc) in zip(train_target_values,\n                                         m5_dates,\n                                         train_cal_features_list,\n                                         stat_cat)\n], freq=\"D\")\n\ntest_ds = ListDataset([\n    {\n        FieldName.TARGET: target,\n        FieldName.START: start,\n        FieldName.FEAT_DYNAMIC_REAL: fdr,\n        FieldName.FEAT_STATIC_CAT: fsc\n    }\n    for (target, start, fdr, fsc) in zip(test_target_values,\n                                         m5_dates,\n                                         test_cal_features_list,\n                                         stat_cat)\n], freq=\"D\")","e767a779":"next(iter(train_ds))","59dbe158":"next(iter(test_ds))","8ff23088":"from gluonts.model.deepar import DeepAREstimator\nfrom gluonts.distribution.neg_binomial import NegativeBinomialOutput\nfrom gluonts.trainer import Trainer\n\nestimator = DeepAREstimator(\n    prediction_length=prediction_length,\n    freq=\"D\",\n    distr_output = NegativeBinomialOutput(),\n    use_feat_dynamic_real=True,\n    use_feat_static_cat=True,\n    cardinality=stat_cat_cardinalities,\n    trainer=Trainer(\n        learning_rate=1e-3,\n        epochs=100,\n        num_batches_per_epoch=200,\n        batch_size=100\n    )\n)\n\npredictor = estimator.train(train_ds)","31c18b7e":"from gluonts.evaluation.backtest import make_evaluation_predictions\n\nforecast_it, ts_it = make_evaluation_predictions(\n    dataset=test_ds,\n    predictor=predictor,\n    num_samples=100\n)\n\nprint(\"Obtaining time series conditioning values ...\")\ntss = list(tqdm(ts_it, total=len(test_ds)))\nprint(\"Obtaining time series predictions ...\")\nforecasts = list(tqdm(forecast_it, total=len(test_ds)))\n\nquantiles = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995]","c0d14844":"if submission == False:\n    \n    from gluonts.evaluation import Evaluator\n    \n    evaluator = Evaluator(quantiles=quantiles)\n    agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))\n    print(json.dumps(agg_metrics, indent=4))","ef800805":"if submission == True:\n    def quantile_calc(q):\n        res = np.ones((len(forecasts), prediction_length))\n        for i, f in enumerate(forecasts):\n            res[i] = f.quantile(q)\n        return res\n\n    allresults = list(map(quantile_calc, quantiles))\n    allresults_conc = np.concatenate(allresults, axis=0)","f4bbd784":"if submission == True:\n    forecasts_acc_sub = np.zeros((len(allresults_conc)*2, single_prediction_length))\n    forecasts_acc_sub[:len(allresults_conc)] = allresults_conc[:,:single_prediction_length]\n    forecasts_acc_sub[len(allresults_conc):] = allresults_conc[:,single_prediction_length:]","fc5ede9f":"if submission == True:\n    import time\n\n    sample_submission = pd.read_csv(f'{m5_input_path}\/sample_submission.csv')\n    sample_submission.iloc[:,1:] = forecasts_acc_sub\n\n    submission_id = 'submission_{}.csv'.format(int(time.time()))\n\n    sample_submission.to_csv(submission_id, index=False)","c124f9c4":"plot_log_path = \".\/plots\/\"\ndirectory = os.path.dirname(plot_log_path)\nif not os.path.exists(directory):\n    os.makedirs(directory)\n    \ndef plot_prob_forecasts(ts_entry, forecast_entry, path, sample_id, inline=True):\n    plot_length = 150\n    prediction_intervals = (50, 67, 95, 99)\n    legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n\n    _, ax = plt.subplots(1, 1, figsize=(10, 7))\n    ts_entry[-plot_length:].plot(ax=ax)\n    forecast_entry.plot(prediction_intervals=prediction_intervals, color='g')\n    ax.axvline(ts_entry.index[-prediction_length], color='r')\n    plt.legend(legend, loc=\"upper left\")\n    if inline:\n        plt.show()\n        plt.clf()\n    else:\n        plt.savefig('{}forecast_{}.pdf'.format(path, sample_id))\n        plt.close()\n\nprint(\"Plotting time series predictions ...\")\nfor i in tqdm(range(5)):\n    ts_entry = tss[i]\n    forecast_entry = forecasts[i]\n    plot_prob_forecasts(ts_entry, forecast_entry, plot_log_path, i)","096d3fc3":"# M5 Forecasting Competition GluonTS Template\n\nThis notebook can be used as a starting point for participating in the [M5 uncertainty forecasting competition](https:\/\/www.kaggle.com\/c\/m5-forecasting-uncertainty\/overview) using GluonTS-based tooling.\n\n","37682fd5":"### Standard imports\n\nFirst we import standard data manipulation libraries.","ab08f272":"### Generating forecasts\n\nOnce the estimator is fully trained, we can generate predictions from it for the test values.","2d49ce17":"Just to be sure, we quickly verify that dataset format is correct and that our dataset does indeed contain the correct target values as well as dynamic and static features.","db46451d":"Finally, we can build both the training and the testing set from target values and both static and dynamic features.","8a6c5a76":"### Local performance validation (if `submission` is `False`)\n\nSince we don't want to constantly submit our results to Kaggle, it is important to being able to evaluate performace on our own validation set offline. We can use a standard GluonTS evaluator to calculate the the mean weighte quantile loss (`mean_wQuantileLoss`) and lots of other metrics.","1a9cb17b":"### Reading the M5 data into GluonTS\n\nFirst we need to convert the provided M5 data into a format that is readable by GluonTS. At this point we assume that the M5 data, which can be downloaded from Kaggle, is present under `m5_input_path`.","02d0d1c3":"We start the data convertion process by building dynamic features (features that change over time, just like the target values). Here, we are mainly interested in the event indicators `event_type_1` and `event_type_2`. We will mostly drop dynamic time features as GluonTS will automatically add some of these as part of many models' transformation chains.","b180cbde":"Once we have obtained these predictions, we still need to reshape them to the correct submission format.","69fffec1":"### Define the estimator\n\nHaving obtained our training and testing data, we can now create a GluonTS estimator. In our example we will use the `DeepAREstimator`, an autoregressive RNN which was developed primarily for the purpose of time series forecasting. Note however that you can use a variety of different estimators. Also, since GluonTS is mainly target at probabilistic time series forecasting, lots of different output distributions can be specified. In the M5 case, we think that the `NegativeBinomialOutput` distribution best describes the output.\n\nFor a full list of available estimators and possible initialization arguments see https:\/\/gluon-ts.mxnet.io\/api\/gluonts\/gluonts.model.html.\n\nFor a full list of available output distributions and possible initialization arguments see https:\/\/gluon-ts.mxnet.io\/api\/gluonts\/gluonts.distribution.html.","13d4a830":"We also define globally accessible variables, such as the prediction length and the input path for the M5 data. Note that `single_prediction_length` corresponds to the length of the validation\/evaluation periods, while `submission_prediction_length` corresponds to the length of both these periods combined.\n\nBy default the notebook is configured to run in submission mode (`submission` will be `True`), which means that we use all of the data for training and predict new values for a total length of `submission_prediction_length` for which we don't have ground truth values available (performance can be assessed by submitting prediction results to Kaggle). In contrast, setting `submission` to `False` will instead use the last `single_prediction_length`-many values of our training set as validation points (and hence these values will not be used for training), which enables us to validate our model's performance offline.","052a29ca":"Once we have calculated all aggreated series, we still need to ensure that both static and dynamic features are also available for these series and that the aggreagted and the original series are concatenated into the same array.","3c736998":"We then go on to build static features (features which are constant and series-specific). Here, we make use of all categorical features that are provided to us as part of the M5 data.","56daf014":"### Converting forecasts back to M5 submission format (if `submission` is `True`)\n\nIn order to come up with proper uncertainty estimates from our model, wen can directly evaluate the forecasts at multiple quantile levels.","c46f2dc2":"### Plotting sample predictions\n\nFinally, we can also visualize our predictions for some of the time series.","33c64e99":"Then, we save our submission into a timestamped CSV file which can subsequently be uploaded to Kaggle.","ed77e591":"In contrast to the accuracy forecasting challenge, the uncertainty challenge does not only require us to forecast individual time series from the `sales_train_validation` dataset, but also aggregate series. Due to the hierarchical nature of the M5 dataset, different aggregation levels are possible. See the [M5 guidelines](https:\/\/mofc.unic.ac.cy\/m5-competition\/) for a description of the respective levels and number of series in each level. Since this process can take a while, we will save these aggregate values such that they can be simply read from disk at a later point."}}