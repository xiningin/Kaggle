{"cell_type":{"2b2cb5cf":"code","e4e09c22":"code","f399d144":"code","6fcc2aa6":"code","94a218ae":"code","24ac4e8b":"code","af59e446":"code","01b7b00e":"code","c4f18e12":"code","b2821b2d":"code","244211e4":"code","140617ca":"code","690ed470":"code","678c20b6":"code","5e9e7580":"code","d1ae2008":"code","36b4d47c":"code","c1757449":"code","1b5f2fc3":"code","a33bb345":"code","1eee5e58":"code","82c23d95":"code","918c5291":"code","0a60afa9":"code","66611532":"code","1eeda27c":"code","3d914a46":"code","6cf3c6d6":"code","d0e397a2":"code","e9d43760":"code","558359e7":"code","b58c7757":"code","762e20b9":"code","10bba8d1":"code","e77e9b7a":"code","3c207ac5":"code","e572841a":"code","0a5258eb":"code","293ae088":"code","ab8d9a9a":"code","96159976":"code","cba41641":"code","f82e1a7b":"code","44aac757":"code","5bd364f3":"code","ce3de838":"code","6eeba523":"code","6dede889":"code","56b76e11":"code","e3cc506b":"code","bc7e8acd":"code","5c823753":"markdown","b8466ce8":"markdown","3758a9cc":"markdown","d764c904":"markdown","3564ea7f":"markdown","be8874f1":"markdown","54d9fb84":"markdown","8555c431":"markdown","354d45b9":"markdown","32e7542b":"markdown","07f1ff03":"markdown","c6fcbc06":"markdown","b67a7cad":"markdown","5f186afe":"markdown","8842a7d4":"markdown","59110d3f":"markdown","24ebd238":"markdown","069a1e6c":"markdown","f40e16f2":"markdown","acf63beb":"markdown","5015a269":"markdown","92f90339":"markdown","357bb753":"markdown","bfffc158":"markdown","06ff2a98":"markdown","fd15d862":"markdown","22d343a5":"markdown","845a5b3f":"markdown","7cb4c815":"markdown","5df9d47b":"markdown","11d9fae3":"markdown","4cbc64f0":"markdown","aa89e904":"markdown","165a306d":"markdown","1d81fc46":"markdown","b5a3326f":"markdown","80847a6d":"markdown","5c101ed5":"markdown","047db606":"markdown","8f919a3e":"markdown","f4e9d357":"markdown","2072024e":"markdown","532aa9b4":"markdown","d61dbc2d":"markdown","42defcfd":"markdown","dc017d26":"markdown","09af416a":"markdown","b1abd11b":"markdown"},"source":{"2b2cb5cf":"from sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784')","e4e09c22":"X, y = mnist[\"data\"], mnist[\"target\"]\nprint(X.shape)\nprint(y.shape)","f399d144":"%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nsome_digit = X[36000]\nsome_digit_image = some_digit.reshape(28,28)\n\nplt.imshow(some_digit_image, cmap=matplotlib.cm.binary,\n          interpolation='nearest')\nplt.axis('off')\nplt.show()","6fcc2aa6":"y[36000]","94a218ae":"def plot_digits(instances, images_per_row=10, **options):\n    size = 28\n    images_per_row = min(len(instances), images_per_row)\n    images = [instance.reshape(size,size) for instance in instances]\n    n_rows = (len(instances) - 1) \/\/ images_per_row + 1\n    row_images = []\n    n_empty = n_rows * images_per_row - len(instances)\n    images.append(np.zeros((size, size * n_empty)))\n    for row in range(n_rows):\n        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n        row_images.append(np.concatenate(rimages, axis=1))\n    image = np.concatenate(row_images, axis=0)\n    plt.imshow(image, cmap = matplotlib.cm.binary, **options)\n    plt.axis(\"off\")    ","24ac4e8b":"import numpy as np\n\nplt.figure(figsize=(9,9))\nexample_images = np.r_[X[:12000:600], X[13000:30600:600], X[30600:60000:590]]\nplot_digits(example_images, images_per_row=10)\nplt.show()","af59e446":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000)","01b7b00e":"shuffle_index = np.random.permutation(60000)\nX_train, y_train = X_train[shuffle_index], y_train[shuffle_index]","c4f18e12":"y_train_5 = (y_train=='5')\ny_test_5 = (y_test=='5')","b2821b2d":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)","244211e4":"sgd_clf.predict([some_digit])","140617ca":"five_digit = X[0]\nfive_digit_image = five_digit.reshape(28,28)\n\nplt.imshow(five_digit_image, cmap=matplotlib.cm.binary,\n          interpolation='nearest')\nplt.axis('off')\nplt.show()","690ed470":"sgd_clf.predict([five_digit])","678c20b6":"from sklearn.model_selection import cross_val_score\n\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring='accuracy')","5e9e7580":"from sklearn.base import BaseEstimator\n\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X),1), dtype=bool)\n\nnever_5_clf = Never5Classifier()\ncross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring='accuracy')","d1ae2008":"from sklearn.model_selection import cross_val_predict\n\ny_train_predict = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)","36b4d47c":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_train_5, y_train_predict)","c1757449":"from sklearn.metrics import precision_score, recall_score\n\nprint(precision_score(y_train_5, y_train_predict))\nprint(recall_score(y_train_5, y_train_predict))","1b5f2fc3":"from sklearn.metrics import f1_score\n\nprint(f1_score(y_train_5, y_train_predict))","a33bb345":"y_scores = sgd_clf.decision_function([five_digit])\ny_scores","1eee5e58":"threshold = 0\ny_some_digit_pred = (y_scores>threshold)\ny_some_digit_pred","82c23d95":"threshold = 700\ny_some_digit_pred = (y_scores>threshold)\ny_some_digit_pred","918c5291":"y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method='decision_function')\ny_scores","0a60afa9":"from sklearn.metrics import precision_recall_curve\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(14,6))\n    plt.plot(thresholds, precisions[:-1], 'b--', label='Precision')\n    plt.plot(thresholds, recalls[:-1], 'g-', label='Recall')\n    plt.xlabel('Threshold')\n    plt.legend(loc='center left')\n    plt.ylim([0,1])\n    \nprecision, recall, thresholds = precision_recall_curve(y_train_5, y_scores)\nplot_precision_recall_vs_threshold(precision, recall, thresholds)\nplt.show()","66611532":"def plot_precision_vs_recall(precisions, recalls):\n    plt.figure(figsize=(10,6))\n    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n    plt.xlabel(\"Recall\", fontsize=16)\n    plt.ylabel(\"Precision\", fontsize=16)\n    plt.axis([0, 1, 0, 1])\n\nplot_precision_vs_recall(precision, recall)","1eeda27c":"from sklearn.metrics import roc_curve\n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.figure(figsize=(10,6))\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0,1], [0,1], 'k--')\n    plt.axis([0,1,0,1])\n    plt.xlabel('False positive ratio')\n    plt.ylabel('True positive ratio')\n\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\nplot_roc_curve(fpr, tpr)\nplt.show()","3d914a46":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_train_5,y_scores)","6cf3c6d6":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(n_estimators=100)\ny_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method='predict_proba')","d0e397a2":"y_scores_forest = y_probas_forest[:,1]\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)\n\nplt.figure(figsize=(8, 6))\nplot_roc_curve(fpr_forest, tpr_forest, 'Random Forest')\nplt.plot(fpr, tpr, 'b:', label='SGD')\nplt.legend(loc='lower right')\nplt.show()","e9d43760":"roc_auc_score(y_train_5, y_scores_forest)","558359e7":"sgd_clf.fit(X_train, y_train)\nsgd_clf.predict([some_digit]) #9 - Wrong prediction!","b58c7757":"some_digit_scores = sgd_clf.decision_function([some_digit])\nsome_digit_scores","762e20b9":"from sklearn.multiclass import OneVsOneClassifier\n\novo_clf = OneVsOneClassifier(SGDClassifier(random_state=42))\novo_clf.fit(X_train, y_train)\novo_clf.predict([some_digit]) #Correct prediction!","10bba8d1":"forest_clf.fit(X_train, y_train)\nforest_clf.predict([some_digit]) ","e77e9b7a":"forest_clf.predict_proba([some_digit])","3c207ac5":"cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')","e572841a":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\ncross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring='accuracy')","0a5258eb":"y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\nconf_mx = confusion_matrix(y_train, y_train_pred)\nconf_mx","293ae088":"plt.matshow(conf_mx, cmap=plt.cm.gray)\nplt.show()","ab8d9a9a":"row_sums = conf_mx.sum(axis=1, keepdims=True)\nnorm_conf_mx = conf_mx\/row_sums\n\nnp.fill_diagonal(norm_conf_mx, 0)\nplt.matshow(norm_conf_mx, cmap=plt.cm.gray)\nplt.show()","96159976":"cl_a, cl_b = '8', '9'\nX_aa = X_train[(y_train==cl_a) & (y_train_pred==cl_a)]\nX_ab = X_train[(y_train==cl_a) & (y_train_pred==cl_b)]\nX_ba = X_train[(y_train==cl_b) & (y_train_pred==cl_a)]\nX_bb = X_train[(y_train==cl_b) & (y_train_pred==cl_b)]\n\nplt.figure(figsize=(8,8))\nplt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\nplt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\nplt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\nplt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)","cba41641":"cl_a, cl_b = '3', '5'\nX_aa = X_train[(y_train==cl_a) & (y_train_pred==cl_a)]\nX_ab = X_train[(y_train==cl_a) & (y_train_pred==cl_b)]\nX_ba = X_train[(y_train==cl_b) & (y_train_pred==cl_a)]\nX_bb = X_train[(y_train==cl_b) & (y_train_pred==cl_b)]\n\nplt.figure(figsize=(8,8))\nplt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\nplt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\nplt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\nplt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)","f82e1a7b":"cl_a, cl_b = '5', '8'\nX = X_train[(y_train==cl_a) & (y_train_pred==cl_b)]\n\nplt.figure(figsize=(8,8))\nplot_digits(X[:25], images_per_row=5)","44aac757":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\nparam_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\nknn_clf = KNeighborsClassifier()\ngrid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3, n_jobs=-1)\ngrid_search.fit(X_train, y_train)","5bd364f3":"print(\"Grid search best parameters: \", grid_search.best_params_)\nprint(\"Grid search best scores: \", grid_search.best_score_)","ce3de838":"from sklearn.metrics import accuracy_score\n\ny_pred = grid_search.predict(X_test)\naccuracy_score(y_pred, y_test)","6eeba523":"from scipy.ndimage.interpolation import shift\n\ndef shift_image(image, dx, dy):\n    image = image.reshape((28,28))\n    shifted_image = shift(image, [dx, dy], cval=0, mode='constant')\n    return shifted_image.reshape([-1])","6dede889":"shifted_right_some_digit = shift_image(some_digit, 0, 5)\nshifted_up_some_digit = shift_image(some_digit, -5, 0)\n\nplt.figure(figsize=(12,3))\nplt.subplot(131)\nplt.title('Original', fontsize=14)\nplt.imshow(some_digit.reshape((28,28)), interpolation='nearest', cmap='Greys')\nplt.subplot(132)\nplt.title('Shifted right', fontsize=14)\nplt.imshow(shifted_down_some_digit.reshape((28,28)), interpolation='nearest', cmap='Greys')\nplt.subplot(133)\nplt.title('Shifted up', fontsize=14)\nplt.imshow(shifted_left_some_digit.reshape((28,28)), interpolation='nearest', cmap='Greys')","56b76e11":"X_train_augmented = [image for image in X_train]\ny_train_augmented = [label for label in y_train]\n\nfor dx,dy in ((1,0),(-1,0),(0,1),(0,-1)):\n    for image,label in zip(X_train, y_train):\n        X_train_augmented.append(shift_image(image, dx, dy))\n        y_train_augmented.append(label)\n\nX_train_augmented = np.array(X_train_augmented)\ny_train_augmented = np.array(y_train_augmented)\n\nshuffle_index = np.random.permutation(len(X_train_augmented))\nX_train_augmented = X_train_augmented[shuffle_index]\ny_train_augmented = y_train_augmented[shuffle_index]","e3cc506b":"knn_clf = KNeighborsClassifier(**grid_search.best_params_)\nknn_clf.fit(X_train_augmented, y_train_augmented)","bc7e8acd":"y_pred = knn_clf.predict(X_test)\naccuracy_score(y_test, y_pred)","5c823753":"This set of hyperparameters provides an accuracy of more than 97 % (on the test set)","b8466ce8":"Handwriting is characterized by a great variety - this is well shown by a random sample from the MNIST collection containing several representatives of a given class (i.e. several digits)","3758a9cc":"The bright band corresponding to the digit 8 indicates that the classifier is wrongly classifying many examples as an eight. On the other hand, you can see some brighter fields in the row corresponding to digit 9, so many nines are wrongly classified as another digit.\n\nSo let's check how well classified the eights and nines and the ones that were misclassified look","d764c904":"In addition, let's pay attention to the fives that have been classified as eights, with the most errors","3564ea7f":"We have a 98% accuracy classifier, which means that date augmentation has allowed for a noticeable improvement.","be8874f1":"# Multiclass classification - MNIST","54d9fb84":"Note that with this strategy, the number of classifiers increases squarely with the number of classes, so it is suitable for small and medium sets with a moderate number of classes (it is the default for support vector machines).\n\nAs before, you can try to train a model of a random forest","8555c431":"At the beginning we will choose a very simple linear classifier, whose learning algorithm is based on the stochastic gradient method","354d45b9":"From such a chart, it is easy to read the threshold value of interest for the given set accuracy or recall values. The threshold is a parameter here - if you give up on it, you get a precision-recall curve","32e7542b":"Such an inexpensive procedure allowed the performance to be significantly improved.\n\n## Error analysis\n\nIn the case of multi-class classification, there is also a concept of confusion matrix - its structure is analogous to that of binary classification","07f1ff03":"Similar considerations may be made in this case.\n\nThe above remarks suggest that in order to increase the accuracy of the classifier, a larger amount of data should be obtained or existing data should be transformed accordingly (for example by rotation and translation)","c6fcbc06":"## 'Five' classifier\n\nThe issue under consideration is a multi-class classification, i.e. it is possible to choose several classes (exactly ten - each corresponding to one digit).  Let's first reduce the problem to binary classification - we want to recognize if an arbitrary number (for example a five) is shown in a given picture. The output of the classifier in such a situation is a logical value saying whether the presented example represents the number five or not","b67a7cad":"The metric that combines the two previous ones is F1 Score (harmonic mean of precision and recall)","5f186afe":"The quality of the binary classifiers can be gathered by a confusion matrix. It consists of two rows and two columns. The first line corresponds to the negative examples (non-fives) and the second to the positive ones. The situation is similar for the columns except that it corresponds to the perspective of the classifier (the first column is the negative examples).","8842a7d4":"A standard way to improve the effectiveness of some learning algorithms is to feature scaling","59110d3f":"Let's set an example of such a threshold","24ebd238":"To increase the learning efficiency of most algorithms, it is good to shuffle the test set","069a1e6c":"The visualization shows that eights misclassified as nines are characterized by a small bottom loop, which can be seen by the classifier as the absence of such a loop. \n\nNines classified as eights have a curved and rounded leg, which is perceived by the classifier as a bottom loop. In addition, in the same figure, it should be noted that in the penultimate row and the penultimate column, a digit even for human perception would undoubtedly be classified as an eighth.\n\nThe same situation applies to threes and fives","f40e16f2":"Graphical representation highlights fields with a large number of elements","acf63beb":"Okay, we already know that the algorithm promises to work. So let's check out the more global statistics of classifier quality assessment. For this purpose, we can use the cross-validation test, which is able to show potential problems with the overfitting. As a performance metric, let's use accuracy","5015a269":"In this case, we have:\n1. 53505 correctly classified non-fives\n2. 4084 correctly classified fives\n3. 1075 Non-fives which are incorrectly classified as fives (these are the so-called first type errors)\n4. 1341 Fives that have been incorrectly classified as non-fridays\n\n\nOn this basis we can determine certain measures of accuracy:\n1. Precision as the number of correctly classified positive examples in relation to all positive examples (relative to the classifier)\n2. Recall as a number of correctly classified positive examples in relation to all positive examples (relative to the facts)","92f90339":"Bright areas along the diagonal line suggest that most examples have been correctly classified.\n\nTo find out more about misclassified cases we can use this graphic representation by skipping diagonal elements. Additionally, we will standardize the result - then we will find out about the relative error ","357bb753":"It turns out that the best results for the algorithm k-nearest neighbours are obtained with `k=4` and the weight function based on distances","bfffc158":"Following the same procedure for each element of the learning set (without modification of the test set) we extend it ","06ff2a98":"In this particular case, the model is confident that the number nine is presented to him (85 % chance).\n\nLet's go back to the linear classifier for a moment","fd15d862":"In this case, it is more efficient to apply the OvA strategy (numerical results represent probabilities of belonging to a specific class)","22d343a5":"## Data preparation\n\nFirst, let's break down the data set into a training and test part. Due to the homogeneity of the features, we can apply a random sampling without any fear (omitting more advanced sampling methods such as stratified sampling)","845a5b3f":"## Import dataset\n\nA flagship example of the classification issue is the recognition of digits based on the MNIST collection consisting of handwritten examples. This collection is so popular that it is available in most repositories of different packages - we will use, for example, the Scikit-Learn library data sets","7cb4c815":"Let's see what the input structure looks like","5df9d47b":"The curve representing the new model lies closer to the upper left corner - so it has a larger area under the curve and therefore has better predictions.\n\nSo let's check the performance of this model","11d9fae3":"In this light, our predictions are not so good. Therefore, we need to find more suitable metrics for a trained classifier - most of them are based on a comparison of predictions with expected values","4cbc64f0":"Depending on its value, the sample can be classified as positive or negative","aa89e904":"Visually, it looks like a nine - let's get on with it. ","165a306d":"## Multiclass classification\n\nA multi-class classifier can be viewed as a set of binary classifiers - each corresponds to a single class and issues numerical predictions representing the value of which represents the strength of the model's belief that the example belongs to that class. The final verdict is given to the classifier whose predictions have the highest value - this strategy is called one-versus-all (OvA)","1d81fc46":"An exemplary representative of the learned class passes the membership test","b5a3326f":"Another characteristic is the ROC curve (reveiver operating characteristic)","80847a6d":"### Random Forest Classifier\n\nWith the right metrics, let's try to train a decision multi-tree model - Random Forest Model","5c101ed5":"Note that the Scikit-Learn classifiers detect multiple classification issues and set OvA strategy by default.\n\nAnother approach may be to train as many classifiers as different pairs of classes - each of them is responsible for predicting which class is more likely. The target class is selected as the one that has won the most such 'duels'. This strategy is called one-versus-one (OvO)","047db606":"For the purpose of visualizing these curves we need numerical results representing the decision-making function","8f919a3e":"We can now choose the best of the tested classifiers and try to improve its performance by learning on the extended training set","f4e9d357":"## Precision improving\n\nWe will now try to improve the accuracy of the classifier by tuning the hyperparameters via `GridSearchCV`.","2072024e":"Note that F1 Score has a high value only if none of its components are close to zero. Additionally, it prefers situations where precision and recall are at a similar level, which does not always have to be a project priority.\n\nTherefore, it is good to work out a procedure that allows to achieve a proper balance between precision and recall - for this purpose, for linear classifiers, we can modify the threshold on the basis of which the decision is made about the example belonging to a class","532aa9b4":"We get great results with very little effort - this is quite rare. Since the results for all three tests do not differ significantly from each other, we do not suspect the model of bias. However, let's note a characteristic feature of the classifier under consideration - it has two classes - fives and non-fives, of which the number of representatives of the latter is significantly higher (having the same number of examples of each digit we have 10% of fridays and 90% of non-fives).\n\nIn view of the above, it is concluded that a classifier which would assign a logical value to any digit of false (i.e. a non-five) would have an accuracy of close to 90 %. This can be seen by writing a simple class representing such a dummy classifier","d61dbc2d":"Let's see how the threshold value affects both precision and recall","42defcfd":"We have 70000 examples. Each of them has 784 attributes (i.e. 784 coordinates corresponding to the brightness of individual image pixels). We can see how such an exemplary element looks","dc017d26":"Let's see if it can distinguish the number nine (example element) from the learned class representing the fives","09af416a":"Precision-recall and ROC curves look similar - the former is more suitable when the positive class is rare (i.e. in the case under consideration) and the latter is used in other cases.\n\nThe numerical characteristic corresponding to the ROC curve is the AUC (area under the curve) - values close to unity mean high quality of the classifier","b1abd11b":"## Data augmentation\n\nAn accuracy of 97% is impressive. Further improvement proves to be a relatively difficult task. You can try with other models (for example, artificial neural networks). In order to improve the current model, we will use the technique of artificial extension of the training set.\n\nImages are invariable in relation to translation (i.e. shifts). Therefore, we can change the position of the MNIST collection digits in relation to the geometric center of the image without changing the corresponding label."}}