{"cell_type":{"8d8e53c3":"code","301dfccb":"code","4b6628e2":"code","94a24aef":"code","7e8689b3":"code","060f9df5":"code","269f6e16":"code","bbaf215c":"code","8b0da85c":"code","ceaf8319":"code","ae8e2768":"code","3ad6f70d":"code","67c8e0ec":"code","ad29dc6d":"code","cd6a505d":"code","e8446ef8":"code","03db2ece":"code","e9d70a98":"code","8119db1c":"code","4d66041d":"code","633963f2":"code","50898e98":"code","1cb6c770":"code","f370330a":"code","8c2c1642":"code","7f6aaf0b":"code","319196fe":"code","87d2b635":"code","65488ef1":"code","1eb25c5f":"code","b907f653":"code","8ac7e1c3":"code","eab2913a":"code","da7eb77a":"code","3bb7e13c":"code","65071a9e":"code","4bf8af67":"code","e1093cfb":"code","1beef3e5":"code","33dabc26":"code","c4d8ca76":"code","4d8ce9bd":"code","1f01dfe3":"code","1d6c4b24":"code","81ff180c":"code","122f3362":"code","1eb9401f":"code","8ff34470":"code","8991e00d":"code","723aece0":"code","230c0acb":"code","d9f06a84":"code","b13bdac2":"code","4c10e0c7":"code","23d44067":"code","18fe7197":"code","7c7e6b37":"code","0545f80b":"code","a0269aea":"code","59ffb68b":"code","997b85ed":"code","f4cc1ee7":"code","90460768":"code","ee5e25a4":"code","8af38444":"code","27626f46":"code","0c248ced":"markdown","afe414d1":"markdown","d197e334":"markdown","62e6b71a":"markdown","956950b1":"markdown","2889d058":"markdown","3683d472":"markdown","1e9fb861":"markdown","ea3f1b9b":"markdown","d5a0151a":"markdown","53ca935a":"markdown","3554607f":"markdown","2de0b78d":"markdown","f00f46e7":"markdown","715250db":"markdown","41c4ef86":"markdown","4c4ed63c":"markdown","d10b10a0":"markdown","5a31808d":"markdown","c5e29ce1":"markdown","ae911f54":"markdown","3a7e790f":"markdown","d015f2df":"markdown","ec7a912f":"markdown","2c0426c8":"markdown","738acdbb":"markdown"},"source":{"8d8e53c3":"%matplotlib inline","301dfccb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly_express as px\nfrom sklearn.model_selection import KFold,cross_val_score,GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import BaggingRegressor,AdaBoostRegressor,RandomForestRegressor,GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import Lasso,Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler,MinMaxScaler\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4b6628e2":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(train.shape,test.shape)","94a24aef":"pd.pandas.set_option('display.max_columns',None)\ntrain.head()","7e8689b3":"na_columns = [col for col in train.columns if train[col].isna().sum()>=1]\n# percentage of missing values\nfor col in na_columns:\n    print(col,np.round(train[col].isna().mean(),4))","060f9df5":"for col in na_columns:\n    df = train.copy()\n    df[col] = np.where(df[col].isna(),1,0)\n    #median salesprice for missing values and non missing values\n    fig = px.bar(y = df.groupby(col)['SalePrice'].mean(),width=500,height = 500,template=\"plotly_dark\",labels={'x':col,'y':'Mean Saleprice'})\n    fig.show()","269f6e16":"numerical_col = [col for col in train.columns if train[col].dtypes !='O']\nprint(numerical_col)\nprint(len(numerical_col))","bbaf215c":"df[numerical_col].head()","8b0da85c":"year_col = ['YearBuilt','YearRemodAdd','GarageYrBlt','YrSold']\nfig = px.line(train.groupby('YrSold')['SalePrice'].median(),width=600,height=400,template='plotly_dark')\nfig.show()","ceaf8319":"for col in year_col:\n    if col != 'YrSold':\n        df = train.copy()\n        df[col] = df['YrSold']-df[col]\n        fig = px.scatter(x = df[col],y = df['SalePrice'],width=650,height=450,labels={'x':col,'y':'Saleprice'},template=\"plotly_dark\")\n        fig.show()","ae8e2768":"descrete_col = [col for col in numerical_col if len(train[col].unique())<25 and col not in year_col]\nlen(descrete_col)","3ad6f70d":"train[descrete_col].head()","67c8e0ec":"for col in descrete_col:\n    df = train.copy()\n    plt.style.use('ggplot')\n    with plt.style.context('dark_background'):\n        df.groupby(col)['SalePrice'].median().plot.bar(color = 'c')\n    plt.ylabel('Median SalePrice')\n    plt.show()","ad29dc6d":"conti_col = [col for col in numerical_col if col not in descrete_col+year_col+['Id']]\nprint(len(conti_col),'continuous features')\nprint(conti_col)","cd6a505d":"for col in conti_col:\n    df = train.copy()\n    fig = px.histogram(x = col,data_frame= df,width=600,height=450,template='plotly_dark')\n    fig.show()","e8446ef8":"for col in conti_col:\n    df = train.copy()\n    if 0 in df[col].unique():\n        pass\n    else:\n        df[col] = np.log(df[col])\n        fig = px.scatter(x = df[col],y = np.log(df['SalePrice']),labels = {'x' : col,'y':'Saleprice'},width=650,height=400,template='plotly_dark')\n        fig.show()","03db2ece":"for col in conti_col:\n    df = train.copy()\n    if 0 in df[col].unique():\n        pass\n    else:\n        df[col] = np.log(df[col])\n        fig = px.box(data_frame=df,y = df[col],labels={'x':col,'y': col},width=500,height=400,template = 'plotly_dark')\n        fig.show()","e9d70a98":"cat_col = [col for col in train.columns if train[col].dtypes == 'O']\nprint(cat_col)\nprint('We have {} categorical features'.format(len(cat_col)))","8119db1c":"train[cat_col].head()","4d66041d":"for col in cat_col:\n    print(col,': {}'.format(train[col].nunique()))","633963f2":"for col in cat_col:\n    df = train.copy()\n    fig = px.bar(df.groupby(col)['SalePrice'].mean(),height=400,width=600,template='plotly_dark')\n    fig.show()","50898e98":"train.head()","1cb6c770":"test.tail()","f370330a":"y = train['SalePrice'].reset_index(drop=True)\nprevious_train = train.copy()","8c2c1642":"all_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)","7f6aaf0b":"##Let's first handle categorical variables which are missing\nnan_feature = [col for col in all_data.columns if all_data[col].isnull().sum()>=1 and all_data[col].dtypes == 'O']\nprint(len(nan_feature))\nfor feture in nan_feature:\n    print('{} has {} % missing values'.format(feture,np.round(all_data[feture].isnull().mean(),4)))","319196fe":"#replace missing values with a new label\ndef replace_missing(data,missing_feature):\n    df = data.copy()\n    df[missing_feature] = df[missing_feature].fillna('missing')\n    return df\n\nall_data = replace_missing(all_data,nan_feature)\nall_data[nan_feature].isnull().sum()","87d2b635":"## Now let's check the numerical columns contaning missing values\nnum_na_col = [col for col in all_data.columns if all_data[col].isnull().sum()>=1 and all_data[col].dtypes != 'O']\nfor col in num_na_col:\n    print('{} has {}% null values'.format(col,np.round(all_data[col].isnull().mean(),4)))","65488ef1":"#replacing by median since data contain outliers\nfor col in num_na_col:\n    median_value = all_data[col].median()\n    \n    all_data[col+'NaN'] = np.where(all_data[col].isnull(),1,0)##New Feature to capture missing value\n    all_data[col].fillna(median_value,inplace = True) ##Replacing with median in corresponding feature \n    \nall_data[num_na_col].isnull().sum()","1eb25c5f":"for col in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n    all_data[col] = all_data['YrSold']-all_data[col]\n    \nall_data[year_col].head()","b907f653":"num_col = ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\n\nfor col in num_col:\n    all_data[col] = np.log1p(all_data[col])","8ac7e1c3":"## Handling Rare lables occuring in categorical variables\ncat_col = [col for col in all_data.columns if all_data[col].dtypes == 'O']\nfor col in cat_col:\n    temp = all_data[col].value_counts()\/len(all_data)\n    temp_df = temp[temp>0.01].index\n    all_data[col] = np.where(all_data[col].isin(temp_df),all_data[col],'RareVar')","eab2913a":"all_data.head()","da7eb77a":"all_data.shape","3bb7e13c":"cat_train = all_data.iloc[:len(y), :]\n\ncat_test = all_data.iloc[len(y):, :]","65071a9e":"cat_train.head()","4bf8af67":"cat_test.tail()","e1093cfb":"all1 = pd.get_dummies(all_data,drop_first=True)","1beef3e5":"all1.shape","33dabc26":"x_train = all1.iloc[:len(y), :]\n\nx_test = all1.iloc[len(y):, :]","c4d8ca76":"x_train.head()","4d8ce9bd":"x_test.head()","1f01dfe3":"x_train.drop('Id',axis = 1,inplace = True)\nx_test.drop('Id',axis = 1,inplace = True)\ncat_train.drop('Id',axis = 1,inplace = True)\ncat_test.drop('Id',axis = 1,inplace = True)","1d6c4b24":"x_train.describe()","81ff180c":"y_transformed = np.log1p(y)","122f3362":"y_transformed","1eb9401f":"kf = KFold(n_splits=9, random_state=42, shuffle=True)","8ff34470":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X,y_true):\n    rmse = np.sqrt(-cross_val_score(model, X, y_true, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","8991e00d":"lasso_reg =make_pipeline(Min(),Lasso(alpha=0.0005,random_state=44))","723aece0":"scores = {}\n\nscore = cv_rmse(lasso_reg,x_train,y_transformed)\nprint(\"Lasso: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['Lasso'] = (score.mean(), score.std())","230c0acb":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","d9f06a84":"scores = {}\n\nscore = cv_rmse(GBoost,x_train,y_transformed)\nprint(\"gboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gboost'] = (score.mean(), score.std())","b13bdac2":"model_xgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213,\n                             random_state =7, nthread = -1)","4c10e0c7":"scores = {}\n\nscore = cv_rmse(model_xgb,x_train,y_transformed)\nprint(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgboost'] = (score.mean(), score.std())","23d44067":"model_lgb = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","18fe7197":"scores = {}\n\nscore = cv_rmse(model_lgb,x_train,y_transformed)\nprint(\"lgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgbm'] = (score.mean(), score.std())","7c7e6b37":"ridge = make_pipeline(RobustScaler(),Ridge(alpha = 0.0005,random_state = 12))","0545f80b":"scores = {}\n\nscore = cv_rmse(ridge,x_train,y_transformed)\nprint(\"ridge: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['ridge'] = (score.mean(), score.std())","a0269aea":"bag = BaggingRegressor(base_estimator=model_lgb,n_estimators=2,random_state=2,n_jobs=-1)","59ffb68b":"scores = {}\n\nscore = cv_rmse(bag,x_train,y_transformed)\nprint(\"bag: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['bag'] = (score.mean(), score.std())","997b85ed":"print('Lightgbm')\nmodel_lgb.fit(x_train,y_transformed)\n\nprint('model_xgb')\nmodel_xgb.fit(x_train,y_transformed)\n\nprint('Gradient_boosting')\nGBoost.fit(x_train,y_transformed)\n\nprint('bagging')\n\nbag.fit(x_train,y_transformed)","f4cc1ee7":"def blend_models_predict(X):\n    return (#(0.16  * elastic_model.predict(X)) + \\\n            #(0.16 * lasso.predict(X)) + \\\n            (0.2 * GBoost.predict(X)) + \\\n            (0.3 * model_lgb.predict(X)) + \\\n            (0.2 * model_xgb.predict(X)) + \\\n#             (0.1 * xgb_model_full_data.predict(X)) + \\\n            (0.3 * bag.predict(np.array(X))))","90460768":"print('RMSLE score on train data:')\nprint(rmsle(y_transformed, blend_models_predict(x_train)))","ee5e25a4":"print('Predict submission')\nsubmission = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = (np.expm1(blend_models_predict(x_test)))","8af38444":"submission.to_csv(\"submission2.csv\", index=False)","27626f46":"q1 = submission['SalePrice'].quantile(0.0042)\nq2 = submission['SalePrice'].quantile(0.99)\n#Quantiles helping us get some extreme values for extremely low or high values \nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission3.csv\", index=False)","0c248ced":"# 3.Categorical varibales","afe414d1":"We can see that for column with more missing values mean values of houses are more than the non misssing values instances, hence i will replace missing values with some meaningful value in featue engineering section","d197e334":"**Till this version of the notebook i haven't done any hyperparameter tuning maybe in the next version i will do some and imporve the score**","62e6b71a":"## 5.Cardinality of categorical features ","956950b1":"b. Descrete Variables\n\nLet's filter out descrete columns by the rule that if a particular column has less than 25 unique values then it is descrete","2889d058":"# Data Anlysis\n\n   1. Missing values\n   2. Numerical Variables\n   3. Distribution of numerical variables\n   4. Categorical variables\n   5. Cardinality of categorical variables\n   6. Outliers\n   7. Relationship between dependent and independent variables","3683d472":"   **Distributions of continuons numerical features**","1e9fb861":"## 2. Temporal variables","ea3f1b9b":"**Now lets find out the relationship between dependent variable and missing value**\n\n\nLet's vizualize it by some plots for this i will replace missing values by 1 and non null values by 0","d5a0151a":"## 1.Missing values","53ca935a":"**Types of numerical variables**\n\na. Temporal Variables(datetime)\n\nb. Descrete Variables\n\nc. Continuous Variables\n\n \na. We have 4 datetime columns(temporal variables) in the numerical variables let's try to see relationship of these with dependent variable","3554607f":"We can see that the house price is decreasing with time but this is not the full picture there are some other raltions with other temporal variables","2de0b78d":"## 3. Numerical Variables\n\n**Since the numerical variables are skewed we will perform log transformation**","f00f46e7":"## 4.Handling categorical variables\n\n**I will first remove rare label that are present less than 1% in the categorical feature**\n\n**In the next step i will encode the categorical variables**","715250db":"From the plots we can see for some of the faetures saleprice is Exponentialy increasing with the descrete variables, for some of them it is decreasing and for some of them it's behaviour is complex","41c4ef86":"   **How Saleprice is varying with continuous numerical features**","4c4ed63c":"**Visualizing outliers using boxoplot**","d10b10a0":"**Features Neighborhood, Exterior1st,Exterior2nd have high cardinality hence I will deal with them in feature engineering**\n\n Rest of them can be easily dealt by One-Hot Encoding","5a31808d":"## 1.Missing values","c5e29ce1":"## 2.Numerical variables","ae911f54":"**Relationship between categorical and Dependent variable**","3a7e790f":"# Feature Engineering\n\n1. Missing Values\n2. Temporal variables\n3. Trnsforming Numerical Variables for removing outliers\n4. Categorical variables:remove rare labels\n5. Encoding Categorical Variables","d015f2df":"c. Continuous Varible\n\nLet's see the distribution and relationship of continuous numerical features with SalePrice","ec7a912f":"# WorkFlow\n\n   1. Data Analysis\n   2. Feature Engineering \n   3. Model Building\n   4. Ensembling","2c0426c8":"**That is all the necessory data analysis from my end I will do the feature engineering in the next version**","738acdbb":"Now here is the full picture we can see that the as year difference between the yearSold and year of build,year of modifiaction or yeargaragebuild is increases the price of houses decreses.\n\n**Price of old or not recently modified houses are less** "}}