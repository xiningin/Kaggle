{"cell_type":{"adde8ced":"code","d21a484f":"code","4574a2cf":"code","bf949458":"code","8e7d8207":"code","64d00262":"code","3fa0fe19":"code","b7a81a04":"code","575eb34b":"code","2e77265f":"code","bd11b16c":"code","27cdbb6b":"code","bfd22108":"code","6b3e7184":"code","f3f9c347":"code","e6b77e78":"code","fac03ffa":"code","00e7d860":"code","f61f9c89":"code","c0607586":"code","c8b74282":"code","18507321":"code","81c9cae3":"code","23fafffa":"code","555f476d":"code","4ba4ee77":"code","d63487d0":"code","c2bb09b0":"code","280b5c8b":"code","2a8c7094":"markdown","02fd1db8":"markdown","27b79b96":"markdown","6e691cca":"markdown","8caf740f":"markdown","3cb0d26b":"markdown","79721d26":"markdown","24897c48":"markdown","97ab7770":"markdown","fbb3948a":"markdown","5fe23a8a":"markdown","c47da875":"markdown","b9df3df1":"markdown","16d18070":"markdown","aef9ffb2":"markdown"},"source":{"adde8ced":"# Internet needs to be on\n!pip install tensorflow-gpu==2.0a0","d21a484f":"import tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.utils import shuffle\nimport os\nprint(os.listdir(\"..\/input\"))","4574a2cf":"# Make sure tf 2.0 alpha has been installed\nprint(tf.__version__)","bf949458":"#is it using the gpu?\ntf.test.is_gpu_available(\n    cuda_only=False,\n    min_cuda_compute_capability=None\n)\n","8e7d8207":"datadir = \"..\/input\/\"\n\nnodes_train     = np.load(datadir + \"champs-basic-graph\/nodes_train.npz\" )['arr_0']\nin_edges_train  = np.load(datadir + \"champs-basic-graph\/in_edges_train.npz\")['arr_0']\nout_edges_train = np.load(datadir + \"champs-basic-graph\/out_edges_train.npz\" )['arr_0']\n\nnodes_test     = np.load(datadir + \"champs-basic-graph\/nodes_test.npz\" )['arr_0']\nin_edges_test  = np.load(datadir + \"champs-basic-graph\/in_edges_test.npz\")['arr_0']","64d00262":"out_labels = out_edges_train.reshape(-1,out_edges_train.shape[1]*out_edges_train.shape[2],1)\nin_edges_train = in_edges_train.reshape(-1,in_edges_train.shape[1]*in_edges_train.shape[2],in_edges_train.shape[3])\nin_edges_test  = in_edges_test.reshape(-1,in_edges_test.shape[1]*in_edges_test.shape[2],in_edges_test.shape[3])","3fa0fe19":"nodes_train, in_edges_train, out_labels = shuffle(nodes_train, in_edges_train, out_labels)","b7a81a04":"\nclass Message_Passer_NNM(tf.keras.layers.Layer):\n    def __init__(self, node_dim):\n        super(Message_Passer_NNM, self).__init__()\n        self.node_dim = node_dim\n        self.nn = tf.keras.layers.Dense(units=self.node_dim*self.node_dim, activation = tf.nn.relu)\n      \n    def call(self, node_j, edge_ij):\n        \n        # Embed the edge as a matrix\n        A = self.nn(edge_ij)\n        \n        # Reshape so matrix mult can be done\n        A = tf.reshape(A, [-1, self.node_dim, self.node_dim])\n        node_j = tf.reshape(node_j, [-1, self.node_dim, 1])\n        \n        # Multiply edge matrix by node and shape into message list\n        messages = tf.linalg.matmul(A, node_j)\n        messages = tf.reshape(messages, [-1, tf.shape(edge_ij)[1], self.node_dim])\n\n        return messages","575eb34b":"class Message_Agg(tf.keras.layers.Layer):\n    def __init__(self):\n        super(Message_Agg, self).__init__()\n    \n    def call(self, messages):\n        return tf.math.reduce_sum(messages, 2)","2e77265f":"class Update_Func_GRU(tf.keras.layers.Layer):\n    def __init__(self, state_dim):\n        super(Update_Func_GRU, self).__init__()\n        self.concat_layer = tf.keras.layers.Concatenate(axis=1)\n        self.GRU = tf.keras.layers.GRU(state_dim)\n        \n    def call(self, old_state, agg_messages):\n    \n        # Remember node dim\n        n_nodes  = tf.shape(old_state)[1]\n        node_dim = tf.shape(old_state)[2]\n        \n        # Reshape so GRU can be applied, concat so old_state and messages are in sequence\n        old_state = tf.reshape(old_state, [-1, 1, tf.shape(old_state)[-1]])\n        agg_messages = tf.reshape(agg_messages, [-1, 1, tf.shape(agg_messages)[-1]])\n        concat = self.concat_layer([old_state, agg_messages])\n        \n        # Apply GRU and then reshape so it can be returned\n        activation = self.GRU(concat)\n        activation = tf.reshape(activation, [-1, n_nodes, node_dim])\n        \n        return activation","bd11b16c":"# Define the final output layer \nclass Edge_Regressor(tf.keras.layers.Layer):\n    def __init__(self, intermediate_dim):\n        super(Edge_Regressor, self).__init__()\n        self.concat_layer = tf.keras.layers.Concatenate()\n        self.hidden_layer_1 = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n        self.hidden_layer_2 = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n        self.output_layer = tf.keras.layers.Dense(units=1, activation=None)\n\n        \n    def call(self, nodes, edges):\n            \n        # Remember node dims\n        n_nodes  = tf.shape(nodes)[1]\n        node_dim = tf.shape(nodes)[2]\n        \n        # Tile and reshape to match edges\n        state_i = tf.reshape(tf.tile(nodes, [1, 1, n_nodes]),[-1,n_nodes*n_nodes, node_dim ])\n        state_j = tf.tile(nodes, [1, n_nodes, 1])\n        \n        # concat edges and nodes and apply MLP\n        concat = self.concat_layer([state_i, edges, state_j])\n        activation_1 = self.hidden_layer_1(concat)  \n        activation_2 = self.hidden_layer_2(activation_1)\n\n        return self.output_layer(activation_2)","27cdbb6b":"# Define a single message passing layer\nclass MP_Layer(tf.keras.layers.Layer):\n    def __init__(self, state_dim):\n        super(MP_Layer, self).__init__(self)\n        self.message_passers  = Message_Passer_NNM(node_dim = state_dim) \n        self.message_aggs    = Message_Agg()\n        self.update_functions = Update_Func_GRU(state_dim = state_dim)\n        \n        self.state_dim = state_dim         \n\n    def call(self, nodes, edges, mask):\n      \n        n_nodes  = tf.shape(nodes)[1]\n        node_dim = tf.shape(nodes)[2]\n        \n        state_j = tf.tile(nodes, [1, n_nodes, 1])\n\n        messages  = self.message_passers(state_j, edges)\n\n        # Do this to ignore messages from non-existant nodes\n        masked =  tf.math.multiply(messages, mask)\n        \n        masked = tf.reshape(masked, [tf.shape(messages)[0], n_nodes, n_nodes, node_dim])\n\n        agg_m = self.message_aggs(masked)\n        \n        updated_nodes = self.update_functions(nodes, agg_m)\n        \n        nodes_out = updated_nodes\n        # Batch norm seems not to work. \n        #nodes_out = self.batch_norm(updated_nodes)\n        \n        return nodes_out","bfd22108":"adj_input = tf.keras.Input(shape=(None,), name='adj_input')\nnod_input = tf.keras.Input(shape=(None,), name='nod_input')\nclass MPNN(tf.keras.Model):\n    def __init__(self, out_int_dim, state_dim, T):\n        super(MPNN, self).__init__(self)   \n        self.T = T\n        self.embed = tf.keras.layers.Dense(units=state_dim, activation=tf.nn.relu)\n        self.MP = MP_Layer( state_dim)     \n        self.edge_regressor  = Edge_Regressor(out_int_dim)\n        #self.batch_norm = tf.keras.layers.BatchNormalization() \n\n        \n    def call(self, inputs =  [adj_input, nod_input]):\n      \n      \n        nodes            = inputs['nod_input']\n        edges            = inputs['adj_input']\n\n        # Get distances, and create mask wherever 0 (i.e. non-existant nodes)\n        # This also masks node self-interactions...\n        # This assumes distance is last\n        len_edges = tf.shape(edges)[-1]\n        \n        _, x = tf.split(edges, [len_edges -1, 1], 2)\n        mask =  tf.where(tf.equal(x, 0), x, tf.ones_like(x))\n        \n        # Embed node to be of the chosen node dimension (you can also just pad)\n        nodes = self.embed(nodes) \n        \n        #nodes = self.batch_norm(nodes)\n        # Run the T message passing steps\n        for mp in range(self.T):\n            nodes =  self.MP(nodes, edges, mask)\n        \n        # Regress the output values\n        con_edges = self.edge_regressor(nodes, edges)\n        \n        \n        return con_edges\n        ","6b3e7184":"def mse(orig , preds):\n \n    # Mask values for which no scalar coupling exists\n    mask  = tf.where(tf.equal(orig, 0), orig, tf.ones_like(orig))\n\n    nums  = tf.boolean_mask(orig,  mask)\n    preds = tf.boolean_mask(preds,  mask)\n\n\n    reconstruction_error = tf.reduce_mean(tf.square(tf.subtract(nums, preds)))\n\n\n    return reconstruction_error","f3f9c347":"def log_mse(orig , preds):\n \n    # Mask values for which no scalar coupling exists\n    mask  = tf.where(tf.equal(orig, 0), orig, tf.ones_like(orig))\n\n    nums  = tf.boolean_mask(orig,  mask)\n    preds = tf.boolean_mask(preds,  mask)\n\n\n    reconstruction_error = tf.math.log(tf.reduce_mean(tf.square(tf.subtract(nums, preds))))\n\n\n    return reconstruction_error","e6b77e78":"def mae(orig , preds):\n \n    # Mask values for which no scalar coupling exists\n    mask  = tf.where(tf.equal(orig, 0), orig, tf.ones_like(orig))\n\n    nums  = tf.boolean_mask(orig,  mask)\n    preds = tf.boolean_mask(preds,  mask)\n\n\n    reconstruction_error = tf.reduce_mean(tf.abs(tf.subtract(nums, preds)))\n\n\n    return reconstruction_error","fac03ffa":"def log_mae(orig , preds):\n \n    # Mask values for which no scalar coupling exists\n    mask  = tf.where(tf.equal(orig, 0), orig, tf.ones_like(orig))\n\n    nums  = tf.boolean_mask(orig,  mask)\n    preds = tf.boolean_mask(preds,  mask)\n\n    reconstruction_error = tf.math.log(tf.reduce_mean(tf.abs(tf.subtract(nums, preds))))\n\n    return reconstruction_error","00e7d860":"learning_rate = 0.001\ndef step_decay(epoch):\n    initial_lrate = learning_rate\n    drop = 0.1\n    epochs_drop = 20.0\n    lrate = initial_lrate * np.power(drop,  \n           np.floor((epoch)\/epochs_drop))\n    tf.print(\"Learning rate: \", lrate)\n    return lrate\n\nlrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\nstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 15, restore_best_weights=True)\n\n#lrate  =  tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n#                              patience=5, min_lr=0.00001, verbose = 1)\n\nopt = tf.optimizers.Adam(learning_rate=learning_rate)\n","f61f9c89":"mpnn = MPNN(out_int_dim = 512, state_dim = 128, T = 4)\nmpnn.compile(opt, log_mae, metrics = [mae, log_mse])","c0607586":"train_size = int(len(out_labels)*0.8)\nbatch_size = 16\nepochs = 25\n","c8b74282":"mpnn.call({'adj_input' : in_edges_train[:10], 'nod_input': nodes_train[:10]})","18507321":"mpnn.fit({'adj_input' : in_edges_train[:train_size], 'nod_input': nodes_train[:train_size]}, y = out_labels[:train_size], batch_size = batch_size, epochs = epochs, \n         callbacks = [lrate, stop_early], use_multiprocessing = True, initial_epoch = 0, verbose = 2, \n         validation_data = ({'adj_input' : in_edges_train[train_size:], 'nod_input': nodes_train[train_size:]},out_labels[train_size:]) )","81c9cae3":"preds = mpnn.predict({'adj_input' : in_edges_test, 'nod_input': nodes_test})","23fafffa":"np.save(\"preds_kernel.npy\" , preds)","555f476d":"train = pd.read_csv(datadir + \"champs-scalar-coupling\/train.csv\")\ntest = pd.read_csv(datadir + \"champs-scalar-coupling\/test.csv\")\n\ntest_group = test.groupby('molecule_name')\n\nscale_min  = train['scalar_coupling_constant'].min()\nscale_max = train['scalar_coupling_constant'].max()\nscale_mid = (scale_max + scale_min)\/2\nscale_norm = scale_max - scale_mid","4ba4ee77":"def make_outs(test_group, preds):\n    i = 0\n    x = np.array([])\n    for test_gp, preds in zip(test_group, preds):\n        if (not i%1000):\n            print(i)\n\n        gp = test_gp[1]\n        \n        x = np.append(x, (preds[gp['atom_index_0'].values, gp['atom_index_1'].values] + preds[gp['atom_index_1'].values, gp['atom_index_0'].values])\/2.0)\n        \n        i = i+1\n    return x","d63487d0":"max_size = 29\npreds = preds.reshape((-1,max_size, max_size))","c2bb09b0":"out_unscaled = make_outs(test_group, preds)","280b5c8b":"test['scalar_coupling_constant'] = out_unscaled\ntest['scalar_coupling_constant'] = test['scalar_coupling_constant']*scale_norm + scale_mid\ntest[['id','scalar_coupling_constant']].to_csv('submission.csv', index=False)","2a8c7094":"## Output layer\n\nThis is where the model diverges with the paper.   \nAs the paper predicts bulk properties, but we are interested in edges, we need something different.   \n\nHere the each edge is concatenated to it's two nodes and a MLP is used to regress the scalar coupling for each edge","02fd1db8":"## Define the loss functions. \n\nHere the losses are MSE, MAE and LMAE.  \n(**note**: that for LMAE, as the values have been scaled down values will be much smaller than for unscaled values)","27b79b96":"## Put it all together to form a MPNN\n\nDefines the full mpnn that does T message passing steps, where T is a hyperparameter.   \nAs in the paper, the same MP layer is re-used, but this is not a requirement. ","6e691cca":"## Finally create the model, and compile","8caf740f":"# Prediction done!\n\nNow rescale outputs and create submission.csv","3cb0d26b":"Call once as tensorflow likes this","79721d26":"## Message passing layer\n\nPut all of the above together to make a message passing layer which does one round of message passing and node updating","24897c48":"## Update function\n\nDefine the Update function (a GRU)  \nThe GRU basically runs over a sequence of length 2, i.e. [ old state, agged_messages ]","97ab7770":"## Message passer\nDefine the message passer like the Gilmer paper\n\nUse a NN to embed edges as matrices, then matrix multiply with nodes.","fbb3948a":"## Message Passing Neural Network\n\nSo, as many of you might have surmised by now the dataset for this challenge is essentially the QM9 dataset with some new values calculated for it. \n\nThe first thing I though of when seeing this challenge was the [Gilmer paper](https:\/\/arxiv.org\/abs\/1704.01212), as it uses the QM9 dataset. ([see this talk](https:\/\/vimeo.com\/238221016))\n\nThe major difference in this challenge is that we are asked to calulate bond properties (thus edges in a graph) as opposed to bulk properties in the paper. \n\nHere the model is laid out in a modular way so the parts can easily be replaced\n","5fe23a8a":"## Aggregator\n\nDefine the message aggregator (just sum)  \nProbably overkill to have it as its own layer, but good if you want to replace it with something more complex\n","c47da875":"Define some hyperparameters","b9df3df1":"## Define some callbacks, the initial learning rate and the optimizer","16d18070":"## Predict on the test set","aef9ffb2":"## Let the learning begin!"}}