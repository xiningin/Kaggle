{"cell_type":{"9fb8fc6c":"code","ee0a1350":"code","d4b9f136":"code","ac21653c":"code","1dc29b02":"code","9e6b643e":"code","8db36084":"code","8142484c":"code","21fb6121":"code","14c839e2":"code","538275a7":"code","12f264ae":"code","7a50da17":"code","dbf66aa9":"code","c795b079":"code","d7fd7afa":"code","147fc847":"code","f6094e10":"code","963844ff":"code","2264403a":"code","be6bf4c6":"code","55202ce0":"code","3b2a5086":"code","6932b1ed":"code","e8044f3b":"code","c118db7f":"code","453c7cb7":"code","9e496997":"code","30bd48e4":"code","cdab8880":"code","28f26d9d":"code","2e8a5cbd":"code","c068386b":"code","f17ee2a8":"code","e0925eff":"code","f1e5e1bd":"code","99af0e70":"code","33b925d8":"code","b4a9d7a6":"code","e16c2004":"code","be679ef8":"code","9776d50d":"code","1d688b7d":"code","46074c4e":"code","629d7afb":"code","457309ac":"code","37fdd70e":"code","f265a180":"code","fc098ff8":"code","6dc78e5a":"code","72a83a72":"code","af63d323":"code","9122b4da":"code","2ac77978":"code","decd51c8":"code","769c12c4":"code","56e13e47":"code","3903e095":"code","137ff342":"code","edfc4015":"code","b41ef6ac":"code","91a1874e":"code","0db9bbb3":"code","6518934c":"code","72567f37":"code","ca92ca4e":"code","1d415c51":"code","41c29357":"code","88744bd8":"code","fa1d7562":"code","d0b89e02":"code","12d65241":"code","e0bd5989":"code","1e6a56d6":"code","cb96f444":"code","b011f0ae":"code","1a0f0a46":"code","a26686b3":"markdown","a2c8e7f5":"markdown","3505911d":"markdown","b5dd2399":"markdown","d9243b70":"markdown","08c61c6a":"markdown","938f4120":"markdown","95b33732":"markdown","aa1129f7":"markdown","0dc0d559":"markdown","443a1307":"markdown"},"source":{"9fb8fc6c":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re as re\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')","ee0a1350":"# loading the dataset\ntrain = pd.read_csv('..\/input\/train.csv', index_col='PassengerId')\ntest = pd.read_csv('..\/input\/test.csv', index_col='PassengerId')","d4b9f136":"train.shape, test.shape","ac21653c":"Y = train['Survived']\ntrain = train.drop('Survived', axis=1)\ntrain['Training_set'] = True\ntest['Training_set'] = False\ndf_comb = pd.concat([train, test])\ndf_comb.isnull().sum()","1dc29b02":"# Feature Engineering\n##1. Family Size as combination of SibSb + Parch - from Sina\ndf_comb['FamilySize'] = df_comb['SibSp'] + df_comb['Parch'] + 1","9e6b643e":"# check if columns have duplicate ticket number. Duplicate ticket numbers symbolize family\/friends\ntks = df_comb['Ticket']\ndataset_dup = df_comb[tks.isin(tks[tks.duplicated()])]\ndf_comb['TkDup'] = df_comb.Ticket.isin(dataset_dup['Ticket'])","8db36084":"df_comb.head(5)","8142484c":"df_comb[df_comb['Ticket'] == '113803']","21fb6121":"# 2. IsAlone using FamilySize and DupTicket\ndf_comb['IsAlone'] = 0\ndataset_filter = (df_comb['FamilySize'] == 1) & (df_comb['TkDup'] == False)\ndf_comb['IsAlone'] = np.where(dataset_filter, 1, 0)","14c839e2":"df_comb.head(5)","538275a7":"# 3. Cabin\ndf_comb.loc[df_comb.Cabin.notnull() & df_comb.Cabin.str.contains('F'), 'Cabin']","12f264ae":"deck_list = list(map(lambda x: x[0], df_comb.Cabin.dropna().tolist()))\nprint(deck_list)","7a50da17":"# Deck as the initial of the Cabin. If no cabin, use 'X'\ncrit = df_comb['Cabin'].isnull()\ndf_comb['Deck'] = df_comb['Cabin'].astype(str).str[0].where(~crit, other='X')\ndf_comb.head()\ndf_comb.drop('Cabin', axis=1, inplace=True)","dbf66aa9":"# some passengers booked more than a single cabin\ndf_comb.loc[[28, 76, 89, 129]]","c795b079":"df_comb.head(3)","d7fd7afa":"# 4. Title derived from Name\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name) # regex starting with space ending with .\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\ndf_comb['Title'] = df_comb['Name'].apply(get_title)\n\nprint(pd.crosstab(df_comb['Title'], df_comb['Sex']))","147fc847":"df_comb['Title'] = df_comb['Title'].replace(['Mlle','Ms', 'Countess', 'Lady', 'Dona'], 'Miss')\ndf_comb['Title'] = df_comb['Title'].replace('Mme', 'Mrs')\ndf_comb['Title'] = df_comb['Title'].replace(['Capt','Col', 'Don', 'Major', 'Rev', 'Sir', 'Jonkheer'], 'Mr')\nprint(pd.crosstab(df_comb['Title'], df_comb['Sex']))","f6094e10":"# 5., 6. setting age and fare to median value in its class\ndf_comb.loc[df_comb.Age.isnull(), 'Age'] = df_comb.groupby(['Title', 'Pclass']).Age.transform('mean')\ndf_comb.loc[df_comb.Fare.isnull(), 'Fare'] = df_comb.groupby(['Title', 'Pclass']).Fare.transform('mean')","963844ff":"#7. Embarked\ndf_comb[df_comb.Embarked.isnull()]","2264403a":"df_comb[df_comb['Fare'].between(75, 85) & (df_comb.Pclass==1) & (df_comb.Sex=='female') & (df_comb.Deck.str.startswith('B'))]","be6bf4c6":"# cannot identify which Embark, so marking to mode\ndf_comb['Embarked']= df_comb['Embarked'].fillna(value=df_comb['Embarked'].value_counts().index[0])\ndf_comb.loc[[62, 830]]","55202ce0":"#8. Time travelled S(11), C(5), Q(4) in Days before hitting iceberg (15th April 1912) \n# calculated based on embarkment port route of  S(11), C(5), Q(4) using info \n# from https:\/\/discovernorthernireland.com\/things-to-do\/attractions\/titanic\/titanic-sailing-route-map\/\n\ndf_comb['TimeTravelled'] = 11 # default for S\ndataset_filter = (df_comb['Embarked'] == 'C')\ndataset_filter_1 = (df_comb['Embarked'] == 'Q')\ndf_comb['TimeTravelled'] = np.where(dataset_filter, 5, df_comb['TimeTravelled'].values)\ndf_comb['TimeTravelled'] = np.where(dataset_filter_1, 4, df_comb['TimeTravelled'].values)","3b2a5086":"df_comb.head(3)","6932b1ed":"df_comb.drop([\"Name\", \"TkDup\", \"Ticket\"], axis=1, inplace=True)","e8044f3b":"df_comb.head(3)","c118db7f":"#oneHotEncoding Sex, Embarked, Title,Deck\ndf_comb = df_comb.join(pd.get_dummies(df_comb[['Sex', 'Embarked','Title', 'Deck']]))","453c7cb7":"df_comb.drop(['Sex','Embarked','Title', 'Deck'], axis=1, inplace=True)\ndf_comb.head(3)","9e496997":"df_comb.info()","30bd48e4":"# solving dummy variable drop by dropping one categorical value in each encong\ndf_comb.drop(['Sex_female','Embarked_C','Title_Dr', 'Deck_X'], axis=1, inplace=True)","cdab8880":"df_comb.info()","28f26d9d":"df_comb.head()","2e8a5cbd":"train = df_comb[df_comb['Training_set'] == True]\ntrain.drop('Training_set', axis=1, inplace=True)\ntest = df_comb[df_comb['Training_set'] == False]\ntest.drop('Training_set', axis=1, inplace=True)\ntrain.shape, test.shape","c068386b":"train.head()","f17ee2a8":"train_surv = pd.concat([train, Y], axis =1)","e0925eff":"train_surv.head()","f1e5e1bd":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train_surv.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","99af0e70":"g = sns.pairplot(train_surv[[u'Survived', u'Pclass', u'Sex_male', u'Age', u'Parch', u'Fare', u'Embarked_S', u'IsAlone', u'TimeTravelled',\n       u'FamilySize', u'Deck_A', u'Deck_B', u'Deck_F', u'Deck_G', u'Deck_T']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","33b925d8":"train_bfe = train.copy() # duplicating\ntest_bfe = test.copy() # duplicating","b4a9d7a6":"train_bfe.insert(0,'Bias',1) # adding bias for statsmodel\ntest_bfe.insert(0,'Bias',1)","e16c2004":"train_bfe.head(5)","be679ef8":"import statsmodels.formula.api as sm\n# new regressor from sm\nregressor_OLS = sm.OLS(endog=Y, exog=train_bfe).fit()\nregressor_OLS.summary()","9776d50d":"# automated backward elimination with Adjusted R Square\nimport statsmodels.formula.api as sm\ndef backwardElimination(X, Y, SL):\n    x = X.values\n    y = Y.values\n    index_r = np.arange(X.shape[1])\n    index_r = np.reshape(index_r, (1,index_r.shape[0]))\n    index_d = index_r\n    print(index_r)\n    numVars = len(x[0])\n    for i in range(0, numVars):\n        regressor_OLS = sm.OLS(y, x).fit()\n        maxVar = max(regressor_OLS.pvalues).astype(float)\n        adjR_before = regressor_OLS.rsquared_adj.astype(float)\n        if maxVar > SL:\n            for j in range(0, numVars - i):\n                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n                    temp = x\n                    x = np.delete(x, j, 1)\n                    index_r_temp = index_r\n                    index_r = np.delete(index_r, j, 1)\n                    print(index_r)\n                    tmp_regressor = sm.OLS(y, x).fit()\n                    adjR_after = tmp_regressor.rsquared_adj.astype(float)\n                    if (adjR_before >= adjR_after):\n                        x_rollback = temp\n                        index_rollback = index_r_temp\n                        print (regressor_OLS.summary())\n                        deleted = np.append(np.setdiff1d(index_d, index_rollback),0)\n                        return x_rollback, deleted\n                    else:\n                        continue\n    regressor_OLS.summary()\n    return x\nSL = 0.05\nX_Modeled, index_deleted = backwardElimination(train_bfe, Y, SL)\nprint(index_deleted)","1d688b7d":"train.head(2)","46074c4e":"train_bfe.drop(columns=train_bfe.columns[index_deleted]).head(3)","629d7afb":"test_bfe.drop(columns=test_bfe.columns[index_deleted]).head(3)","457309ac":"from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import ConstantKernel, RBF, RationalQuadratic, ExpSineSquared\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","37fdd70e":"print(train.shape,test.shape,Y.shape)","f265a180":"type(Y)","fc098ff8":"# cross validation stratgey \n# Splitting the Training dataset into further Training set and Cross validation Set, this will allow us to validate the final model\n# Stack layer - 0\nSEED = 0 # for reproducibility\n\nX_train = train_bfe.values\nY_train = Y.values\nx_train, x_cv, y_train, y_cv = train_test_split(X_train, Y_train, test_size = 0.15, random_state = SEED)\nx_test = test_bfe.values\nprint(\"Train %s, CV %s, Test %s\"%(x_train.shape, x_cv.shape, x_test.shape))","6dc78e5a":"#Validation function\nNFOLDS = 10\n\nkf_i = KFold(NFOLDS, shuffle=True, random_state=42)\nkf = kf_i.get_n_splits(x_train)\n\ndef f1s_cv(model):\n    f1s= cross_val_score(model, x_train, y_train, scoring=\"f1\", cv = kf_i)\n    return(f1s)\n\n# Making the Confusion Matrix and f1score function\n\ndef f1s_e(y, y_pred):\n    cm = confusion_matrix(y, y_pred)\n    precision = (cm[0,0]\/(cm[0,0]+cm[0,1]))*100\n    recall = (cm[0,0]\/(cm[0,0]+ cm[1,0]))*100\n    FS = (2*(precision*recall))\/(precision+recall)\n    return FS\n\ndef f1s_ec(y, y_pred_ec, threshold = .5):\n        #convert into binary values\n        for i in range(0,y_pred_ec.shape[0]):\n            if y_pred_ec[i]>=threshold:       # setting threshold to .5\n                y_pred_ec[i]=1\n            else:  \n                y_pred_ec[i]=0\n        cm = confusion_matrix(y, y_pred_ec)\n        precision = (cm[0,0]\/(cm[0,0]+cm[0,1]))*100\n        recall = (cm[0,0]\/(cm[0,0]+ cm[1,0]))*100\n        FS = (2*(precision*recall))\/(precision+recall)\n        return FS\n    \n# to time randomized or grid search\nfrom datetime import datetime\n\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","72a83a72":"# Feature Scaling all three sets\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train_scaled = sc.fit_transform(x_train)\nx_cv_scaled = sc.transform(x_cv)\nx_test_scaled = sc.transform(x_test)","af63d323":"# testing various classification models\npipelines = []\n\npipelines.append(('ScaledSAG', Pipeline([('Scaler', StandardScaler()),('SAG', LogisticRegression(solver='sag', tol=1e-1, C=1.e4 \/ x_train.shape[0]))])))\npipelines.append(('ScaledXGBC', Pipeline([('Scaler', RobustScaler()),('XGBC', xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05))])))\npipelines.append(('ScaledKNC', Pipeline([('Scaler', StandardScaler()),('KNC', KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2))])))\npipelines.append(('ScaledSVC', Pipeline([('Scaler', StandardScaler()),('SVC', SVC(kernel = 'rbf'))])))\npipelines.append(('ScaledGNB', Pipeline([('Scaler', StandardScaler()),('GNB', GaussianNB())])))\npipelines.append(('DTC', DecisionTreeClassifier(criterion = 'entropy')))\npipelines.append(('RFC', RandomForestClassifier(n_estimators = 100)))\npipelines.append(('RobustABC', Pipeline([('Robust', RobustScaler()),('ABC', AdaBoostClassifier())])))\npipelines.append(('ScaledGBC', Pipeline([('Scaler', StandardScaler()),('GBC', GradientBoostingClassifier())])))\npipelines.append(('ScaledETR', Pipeline([('Scaler', StandardScaler()),('ETR', ExtraTreesClassifier())])))\npipelines.append(('ScaledPAC', Pipeline([('Scaler', StandardScaler()),(\"Passive-Aggressive II\", PassiveAggressiveClassifier(loss='squared_hinge',\n                                                          C=1.0))])))\npipelines.append(('ScaledGPC', Pipeline([('Scaler', StandardScaler()),('GPC', GaussianProcessClassifier(1.0 * RBF(1.0)))])))\npipelines.append(('ScaledMLPC', Pipeline([('Scaler', StandardScaler()),('MLPC', MLPClassifier(alpha=1))])))\n","9122b4da":"results = []\nnames = []\nfor name, model in pipelines:\n    cv_results = f1s_cv(model)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","2ac77978":"# Class to extend the Sklearn classifier\nclass SklearnWrapper(object):\n    def __init__(self, clf, params=None):\n        self.clf = clf(**params)\n\n    def train(self, x, y):\n        self.clf.fit(x, y)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)","decd51c8":"# Class to extend the Sklearn classifier for XGB\n\nclass XgbWrapper(object):\n    def __init__(self,params=None):\n        self.param = params\n        self.nrounds = params.pop('nrounds', 250)\n\n    def train(self, x_train, y_train):\n        dtrain = xgb.DMatrix(x_train, label=y_train)\n        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n\n    def predict(self, x, threshold = .5):\n        y_pred_xgb=self.gbdt.predict(xgb.DMatrix(x))\n        #convert into binary values\n        for i in range(0,x.shape[0]):\n            if y_pred_xgb[i]>=threshold:       # setting threshold to .5\n                y_pred_xgb[i]=1\n            else:  \n                y_pred_xgb[i]=0\n        return y_pred_xgb","769c12c4":"# Class to extend the Sklearn classifier for LGBM\n\nclass LgbWrapper(object):\n\n    def __init__(self,params=None):\n        self.param = params\n\n    def train(self, x, y, tsize = .2):\n        lgb_x_train, lgb_x_cv, lgb_y_train, lgb_y_cv = train_test_split(x, y, test_size = tsize, random_state = SEED)\n        d_train = lgb.Dataset(lgb_x_train, lgb_y_train)\n        d_valid = lgb.Dataset(lgb_x_cv, lgb_y_cv)\n\n        self.lgbt = lgb.train(self.param,\n                d_train, \n                100000,\n               valid_sets=[d_valid],\n               early_stopping_rounds=100,\n                verbose_eval=1000)\n\n    def predict(self, x, threshold = .5):\n        y_pred_lgb=self.lgbt.predict(x)\n        #convert into binary values\n        for i in range(0,x.shape[0]):\n            if y_pred_lgb[i]>=threshold:       # setting threshold to .5\n                y_pred_lgb[i]=1\n            else:  \n                y_pred_lgb[i]=0\n        return y_pred_lgb","56e13e47":"def get_oof(clf, o_x_train, o_y_train, o_x_cv, o_x_test):\n    oof_train = np.zeros((o_x_train.shape[0],))\n    oof_cv = np.zeros((o_x_cv.shape[0],))\n    oof_cv_skf = np.empty((NFOLDS, o_x_cv.shape[0]))\n    oof_test = np.zeros((o_x_test.shape[0],))\n    oof_test_skf = np.empty((NFOLDS, o_x_test.shape[0]))\n\n    for i, (train_index, test_index) in enumerate(kf_i.split(o_x_train)):\n        x_tr = o_x_train[train_index]\n        y_tr = o_y_train[train_index]\n        x_te = o_x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n        oof_train[test_index] = clf.predict(x_te)\n\n        oof_cv_skf[i, :] = clf.predict(o_x_cv)\n        oof_test_skf[i, :] = clf.predict(o_x_test)\n\n    oof_cv[:] = oof_cv_skf.mean(axis=0)\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_cv.reshape(-1, 1), oof_test.reshape(-1, 1)","3903e095":"# knc_grid = GridSearchCV(\n#   estimator = KNeighborsClassifier(),\n#     param_grid = {\n#         'n_neighbors':[5,15,20],\n#         'leaf_size':[2,5,8],\n#         'p':[1,2]},\n#     cv = kf,\n#     scoring = \"f1\"    \n# )\n# knc_grid.fit(x_train_scaled,y_train)\n# knc_params = knc_grid.best_params_\n# print(\"KNC Params%s, Score%s\"%(knc_grid.best_params_,knc_grid.best_score_))","137ff342":"# svc_grid = GridSearchCV(\n#   estimator = SVC(kernel = 'rbf'),\n#     param_grid = {\n#         'C':[0.1, 1, 2, 3, 10],\n#         'gamma':[0.01, 0.02, 0.03, 0.05, .1, .2, .3]\n#     },\n#     cv = kf,\n#     scoring = \"f1\"    \n# )\n# svc_grid.fit(x_train_scaled,y_train)\n# svc_params = svc_grid.best_params_\n# print(\"SVC Params%s, Score%s\"%(svc_grid.best_params_,svc_grid.best_score_))\n","edfc4015":"# et_grid = GridSearchCV(\n#   estimator = ExtraTreesClassifier(),\n#     param_grid = {\n#         \"n_estimators\": [10, 100, 450,500,550, 575],\n#         \"max_depth\": [1, 10, 100, 500, 1000],\n#         \"min_samples_leaf\": [1,3,4,5, 10], \n#         'max_features': ('auto', 'sqrt','log2')\n#     },\n#     n_jobs=-1,\n#     cv = kf,\n#     scoring = \"f1\"    \n# )\n# et_grid.fit(x_train_scaled,y_train)\n# et_params = et_grid.best_params_\n# print(et_grid.grid_scores_)\n# print(\"ET Params%s, Score%s\"%(et_grid.best_params_,et_grid.best_score_))","b41ef6ac":"# mlp_grid = GridSearchCV(\n#   estimator = MLPClassifier(),\n#     param_grid = {'activation':['relu'], 'solver': ['lbfgs'], 'max_iter': [500], \n#                   'alpha': [.001], \n#                   'hidden_layer_sizes':np.arange(9, 11), 'random_state':[3]}, \n#     cv = kf,\n#     scoring = \"f1\",\n#     n_jobs = -1\n# )\n# mlp_grid.fit(x_train,y_train)\n# mlp_params = mlp_grid.best_params_\n# print(\"MLP Params%s, Score%s\"%(mlp_grid.best_params_,mlp_grid.best_score_))","91a1874e":"# ada_grid = GridSearchCV(\n#   estimator = AdaBoostClassifier(),\n#     param_grid = {'n_estimators': [1, 10, 50, 100, 200, 500],\n#                   'learning_rate': [0.01, 0.03, 0.05, .1, 1],\n#                   'algorithm': ['SAMME', 'SAMME.R']},\n#     cv = kf,\n#     scoring = \"f1\"    \n# )\n# ada_grid.fit(x_train_scaled,y_train)\n# print(ada_grid.cv_results_)\n# ada_params = ada_grid.best_params_\n# print(\"ADA Params%s, Score%s\"%(ada_grid.best_params_,ada_grid.best_score_))","0db9bbb3":"# # ker_rbf = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")\n\n# ker_rq = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RationalQuadratic(alpha=0.1, length_scale=1)\n\n# # ker_expsine = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1))\n\n# kernel_list = [ker_rq]\n\n# gpc_grid = GridSearchCV(\n#   estimator = GaussianProcessClassifier(),\n#     param_grid = {\"kernel\": kernel_list,\n#               \"optimizer\": [\"fmin_l_bfgs_b\"],\n#               \"n_restarts_optimizer\": [1],\n#               \"copy_X_train\": [True]},\n#     cv = kf,\n#     scoring = \"f1\"    \n# )\n\n# gpc_grid.fit(x_train_scaled,y_train)\n# print(gpc_grid.cv_results_)\n# gpc_params = gpc_grid.best_params_\n# print(\"GPC Params%s, Score%s\"%(gpc_grid.best_params_,gpc_grid.best_score_))","6518934c":"# rf_grid = GridSearchCV(\n#   estimator = RandomForestClassifier(warm_start=True,max_features='sqrt'),\n#     param_grid = {\n#         \"n_estimators\": [100, 200],\n#         \"max_depth\": [2,3,4,5,6],\n#         \"min_samples_leaf\": [1,2,3], \n#         'max_features': ['log2']\n#     },\n#     cv = kf,\n#     scoring = \"f1\"    \n# )\n# rf_grid.fit(x_train,y_train)\n# rf_params = rf_grid.best_params_\n# print(\"RF Params%s, Score%s\"%(rf_grid.best_params_,rf_grid.best_score_))","72567f37":"# from scipy.stats import expon\n\n# C_distr = [0.01, 0.05, 0.09, 0.1, 0.9, 1]\n\n# lr_grid = GridSearchCV(\n#     estimator = LogisticRegression(),\n#     param_grid = {'penalty': ['l1'], 'solver': [ 'saga'], 'C': C_distr},\n#     cv = kf,\n#     scoring = \"f1\"    \n# )\n# lr_grid.fit(x_train_scaled,y_train)\n# lr_params = lr_grid.best_params_\n# print(\"LR Params%s, Score%s\"%(lr_grid.best_params_,lr_grid.best_score_))","ca92ca4e":"# from sklearn.model_selection import StratifiedKFold\n\n# skf_xgb = StratifiedKFold(n_splits=5, shuffle = True, random_state = 1001)\n\n\n# params_xgb = {\n#          'min_child_weight': [1, 2,3, 5],\n#         'subsample': [0.7, 0.8, 1.0],\n#         'colsample_bytree': [0.7, 0.8, 0.9],\n#         'max_depth': [3, 4, 5, 7],\n#         }\n\n# xgb_model_l1 = xgb.XGBClassifier(learning_rate=0.02, n_estimators=1000, objective='binary:logistic',\n#                     silent=True, nthread=1)\n\n# xgb_l1_grid = RandomizedSearchCV(xgb_model_1, param_distributions=params_xgb,scoring='f1', \n#                                     cv=skf_1.split(x_train_scaled,y_train), verbose=3, random_state=1001)\n\n# start_time = timer(None) # timing starts from this point for \"start_time\" variabl\n# xgb_l1_grid.fit(x_train_scaled,y_train)\n# timer(start_time) # timing ends here for \"start_time\" variable","1d415c51":"# print(xgb_l1_grid.cv_results_)\n# xgb_l1_params = xgb_l1_grid.best_params_\n# print(\"XGB1 Params%s, Score%s\"%(xgb_l1_grid.best_params_,xgb_l1_grid.best_score_))","41c29357":"knc_params = {'leaf_size': 2, 'n_neighbors': 15, 'p': 1}\nsvc_params = {'C': 2, 'gamma': 0.01}\net_params = {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 4, 'n_estimators': 500}\nmlp_params = {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': 9, 'max_iter': 500, 'random_state': 3, 'solver': 'lbfgs'}\nada_params = {'algorithm': 'SAMME.R', 'learning_rate': 0.05, 'n_estimators': 500}\ngpc_params = {'copy_X_train': True, 'kernel': 1**2 * RationalQuadratic(alpha=0.1, length_scale=1), 'n_restarts_optimizer': 1, 'optimizer': 'fmin_l_bfgs_b'}\nrf_params = {'max_depth': 5, 'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 100}\nlr_params = {'solver':'sag', 'tol':1e-1, 'C': 1.e4 \/ x_train.shape[0]}\nxgb_l1_params = {'subsample': 1.0, 'min_child_weight': 5, 'max_depth': 4, 'colsample_bytree': 0.7}","88744bd8":"# Stack Layer 1\nknc = SklearnWrapper(clf=KNeighborsClassifier, params=knc_params)\nsvc = SklearnWrapper(clf=SVC, params=svc_params)\net = SklearnWrapper(clf=ExtraTreesClassifier, params=et_params)\nmlp = SklearnWrapper(clf=MLPClassifier, params=mlp_params)\nada = SklearnWrapper(clf=AdaBoostClassifier, params=ada_params)\ngpc = SklearnWrapper(clf=GaussianProcessClassifier, params=gpc_params)\nrf = SklearnWrapper(clf=RandomForestClassifier, params=rf_params)\nlr = SklearnWrapper(clf=LogisticRegression, params=lr_params)\nxg_l1 = XgbWrapper(params=xgb_l1_params)\n\nknc_oof_train_l1f1,knc_oof_cv_l1f1,knc_oof_test_l1f1 = get_oof(knc,x_train_scaled, y_train, x_cv_scaled, x_test_scaled)\nsvc_oof_train_l1f1,svc_oof_cv_l1f1,svc_oof_test_l1f1 = get_oof(svc,x_train_scaled, y_train, x_cv_scaled, x_test_scaled)\net_oof_train_l1f1,et_oof_cv_l1f1,et_oof_test_l1f1 = get_oof(et, x_train_scaled, y_train, x_cv_scaled, x_test_scaled)\nmlp_oof_train_l1f1,mlp_oof_cv_l1f1,mlp_oof_test_l1f1 = get_oof(mlp, x_train_scaled, y_train, x_cv_scaled, x_test_scaled)\nada_oof_train_l1f1,ada_oof_cv_l1f1,ada_oof_test_l1f1 = get_oof(ada, x_train_scaled, y_train, x_cv_scaled, x_test_scaled)\ngpc_oof_train_l1f1,gpc_oof_cv_l1f1,gpc_oof_test_l1f1 = get_oof(gpc,x_train_scaled, y_train, x_cv_scaled, x_test_scaled)\nrf_oof_train_l1f1,rf_oof_cv_l1f1,rf_oof_test_l1f1 = get_oof(rf,x_train, y_train, x_cv, x_test)\nlr_oof_train_l1f1,lr_oof_cv_l1f1,lr_oof_test_l1f1 = get_oof(lr,x_train_scaled, y_train, x_cv_scaled, x_test_scaled)","fa1d7562":"%%capture capt\nxg_l1_oof_train_l1f1,xg_l1_oof_cv_l1f1,xg_l1_oof_test_l1f1 = get_oof(xg_l1,x_train_scaled, y_train, x_cv_scaled, x_test_scaled)","d0b89e02":"# performance of layer 0 inputs via various models on cv data\nprint(\"KNC-CV f1: {}\".format(f1s_ec(y_cv, knc_oof_cv_l1f1)))\nprint(\"SVC-CV f1: {}\".format(f1s_ec(y_cv, svc_oof_cv_l1f1)))\nprint(\"ET-CV f1: {}\".format(f1s_ec(y_cv, et_oof_cv_l1f1)))\nprint(\"MLP-CV f1: {}\".format(f1s_ec(y_cv, mlp_oof_cv_l1f1)))\nprint(\"ADA-CV f1: {}\".format(f1s_ec(y_cv, ada_oof_cv_l1f1)))\nprint(\"GPC-CV f1: {}\".format(f1s_ec(y_cv, gpc_oof_cv_l1f1)))\nprint(\"RF-CV f1: {}\".format(f1s_ec(y_cv, rf_oof_cv_l1f1)))\nprint(\"LR-CV f1: {}\".format(f1s_ec(y_cv, lr_oof_cv_l1f1)))\nprint(\"XGB-CV f1: {}\".format(f1s_ec(y_cv,xg_l1_oof_cv_l1f1)))","12d65241":"# preparing layer 2 inputs:\n\nx_train_l2f_i = np.concatenate((lr_oof_train_l1f1.astype(int),ada_oof_train_l1f1.astype(int),\n                                gpc_oof_train_l1f1.astype(int)), axis=1)\nx_cv_l2f_i = np.concatenate((lr_oof_cv_l1f1.astype(int),ada_oof_cv_l1f1.astype(int), \n                             gpc_oof_cv_l1f1.astype(int)), axis=1)\nx_test_l2f_i = np.concatenate((lr_oof_test_l1f1.astype(int),ada_oof_test_l1f1.astype(int),\n                               gpc_oof_test_l1f1.astype(int)), axis=1)","e0bd5989":"print(\"%s, %s, %s\"%(x_train_l2f_i.shape, x_cv_l2f_i.shape, x_test_l2f_i.shape))","1e6a56d6":"# fitting second layer input via LGBM\n\nparams_init_l = {}\nparams_init_l['learning_rate'] = 0.02\nparams_init_l['boosting_type'] = 'gbdt'\nparams_init_l['objective'] = 'binary'\nparams_init_l['metric'] = 'binary_logloss'\nparams_init_l['sub_feature'] = 0.5\nparams_init_l['num_leaves'] = 255\nparams_init_l['num_trees '] = 500\nparams_init_l['min_data'] = 50\nparams_init_l['max_depth'] = 10\nparams_init_l['num_threads'] = 16\nparams_init_l['min_sum_hessian_in_leaf '] = 100\n\n\nlg_layer = LgbWrapper(params=params_init_l)\nlg_layer.train(x_train_l2f_i, y_train)\nprint(f1s_e(y_cv,lg_layer.predict(x_cv_l2f_i)))","cb96f444":"# fit and test on LGBM directly on layer 0\n\nparams_init = {}\nparams_init['learning_rate'] = 0.1\nparams_init['boosting_type'] = 'gbdt'\nparams_init['objective'] = 'binary'\nparams_init['metric'] = 'binary_logloss'\nparams_init['sub_feature'] = 0.5\nparams_init['num_leaves'] = 255\nparams_init['min_data'] = 50\nparams_init['max_depth'] = 10\nparams_init['num_threads'] = 16\nparams_init['min_sum_hessian_in_leaf '] = 16\n\n\nlg = LgbWrapper(params=params_init)\nlg.train(x_train_scaled, y_train)\nprint(f1s_e(y_cv,lg.predict(x_cv_scaled)))","b011f0ae":"# Generate Submission File for top prictions for cv\npredictions_lg_direct = lg.predict(x_test_scaled).astype(int)\npredictions_lg_layer = lg_layer.predict(x_test_l2f_i).astype(int)\npredictions_ada = ada_oof_test_l1f1.astype(int)","1a0f0a46":"StackingSubmission_lg_direct = pd.DataFrame({ 'PassengerId': test.index,\n                            'Survived': predictions_lg_direct })\nStackingSubmission_lg_layer = pd.DataFrame({ 'PassengerId': test.index,\n                            'Survived': predictions_lg_layer })\nStackingSubmission_ada = pd.DataFrame({ 'PassengerId': test.index,\n                            'Survived': predictions_ada.reshape((418,)) })\nStackingSubmission_lg_direct.to_csv(\"Submission_lg_direct_1.csv\", index=False)\nStackingSubmission_lg_layer.to_csv(\"StackingSubmission_lg_layer_1.csv\", index=False)\nStackingSubmission_ada.to_csv(\"StackingSubmission_ada_1.csv\", index=False)","a26686b3":"To improve this further, may be we should use more features like one created by https:\/\/www.kaggle.com\/reisel\/save-the-families","a2c8e7f5":"# Automated Backward Feature Elimindation to reduce coorelated featured\nAnother option to reduce the number of features is Backward Feature Elimination (BFE). The idea is very similar to greedy elimination, however, here features are eliminated with respect to their importance. There are many possiblities to evalute the importance of features. In this kernel I do logistic regression and calculate the features\u2019 p-value. The lower the p-value the more relevant the feature. Thus, features with the highest p-value get eliminated first until the selected number of features is reached. ","3505911d":"# Layer Plan ( to be executed on all train, cv and test data)\n* Layer 0:\n    * x_train_scaled features\n    \n* Layer 1:\n    * Feature 1: output of KNC\n    * Feature 2: output of SVC RBF\n    * Feature 3: output of ET\n    * Feature 4: output of MLP\n    * Feature 5: output of ADA\n    * Feature 6: output of GPC\n    * Feature 7: output of RFC\n    * Feature 8: output of LR\n    * Feature 9: output of XGBM \n\n* Layer 2: \n    * Feature 1: final predictions using LGBM ( over uncoorelated top performing models of layer 1)","b5dd2399":"**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows","d9243b70":"# Stacking","08c61c6a":"## Layer 2","938f4120":"here we can see model drops lot of important fields like IsAlone as pvalue was high, now this might have happened as Isalone was dependent on SibSb + arch + TicketDup","95b33732":"## Layer 1","aa1129f7":"# Base Models","0dc0d559":" multicollinearity is not a big issue for prediction, so first running model using original list of features","443a1307":"## executing grid search to find optimium parameters for first layer models"}}