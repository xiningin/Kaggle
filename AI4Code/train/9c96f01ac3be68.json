{"cell_type":{"92aae0a9":"code","ce80e22c":"code","ad6a25d6":"code","9f3c0453":"code","653ea6c8":"code","35f114e5":"code","9c827ad3":"code","5060b389":"code","bb8ace2e":"code","5ee1a6ba":"code","251fa579":"code","af435819":"code","e110cff8":"code","25c8d7bd":"code","7759a6fd":"code","e403a3d3":"code","7b7e8674":"code","26d52edf":"code","2601091e":"code","e46c9ca7":"code","981a4f64":"code","75508f91":"code","7cf3e8ef":"code","798a6ce3":"markdown","a80c7e64":"markdown","ff297c3c":"markdown","dd2858df":"markdown","bde673a0":"markdown","00c59452":"markdown"},"source":{"92aae0a9":"import re\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom itertools import chain \nimport matplotlib.pyplot as plt\nfrom nltk.collocations import *\nfrom nltk.corpus import stopwords\nfrom sklearn.pipeline import Pipeline\nfrom nltk.stem import WordNetLemmatizer\nimport nltk.corpus.reader.wordnet as wordnet\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report, accuracy_score","ce80e22c":"data = pd.read_csv('..\/input\/medium-post-titles\/medium_post_titles.csv')\ndata.head()","ad6a25d6":"data.shape","9f3c0453":"data.fillna('', inplace=True, axis=0)","653ea6c8":"def combine(columns):\n    title = columns[0]\n    subtitle = columns[1]\n    return title + ' ' + subtitle\n\ndata['text'] = data[['title', 'subtitle']].apply(combine, axis=1)","35f114e5":"data.drop(['title', 'subtitle', 'subtitle_truncated_flag'], axis=1, inplace=True)","9c827ad3":"def horizontal_bar(plot_data, size, title):\n    plt.figure(figsize=(size, size)) #set the size of the figure.\n    plt.barh(list(plot_data.keys()), list(plot_data.values())) #plot the horizontal bar.\n    plt.title(title) #set the title.\n    \ndef tokenizer(sentences):\n    tokens = []\n    for sent in sentences:\n        sent = sent.lower()  #convert the text to lower case. As Car has same meaning as that of car.\n        matches = re.findall('[a-z]+', sent) #For classification words do the job.\n        tokens.append(matches)\n    return tokens\n\ndef most_frequent_words(words_lists, size=5):\n    merge = []\n    word_freq = {}\n    \n    for tokens in words_lists: #convert to 1d list.\n        merge += tokens\n    frequency = nltk.FreqDist(merge) #find the frequencies of each word.\n    \n    common = frequency.most_common(size) #get the most common words.\n    for word, freq in common:\n        word_freq[word] = freq\n    return word_freq\n\ndef remove_stopwords(words_lists):\n    stop_words = stopwords.words('english')\n    new_tokens = []\n    for lis in words_lists:\n        new_list = []\n        for word in lis:\n            if word not in stop_words: #filter out the stopwords.\n                new_list.append(word)\n        new_tokens.append(new_list)\n    return new_tokens\n\ndef get_longest_words_frequency(words_lists, length=5, freq=250):\n    merge = []\n    for tokens in new_tokens:\n        merge += tokens\n    frequency = nltk.FreqDist(merge)\n    \n    words = {}\n    for word_list in words_lists:\n        for word in word_list:\n            if len(word) >= length and word not in words and frequency[word] >= freq: #filter.\n                words[word] = frequency[word]\n    return words\n\ndef detokenize(words_lists):\n    sentences = []\n    for words in words_lists:\n        sentence = ' '.join(words) #join words seperating by a space.\n        sentences.append(sentence)\n    return sentences\n\ndef replace_collocations(sentences, scores, limit=10):\n    for i in range(len(sentences)):\n        for j in range(limit):\n            word = scores[j][0][0] + ' ' + scores[j][0][1]\n            if word in sentences[i]:\n                sentences[i] = re.sub(word, scores[j][0][0] + scores[j][0][1], sentences[i])\n    return sentences\n\ndef lemmatize(words_lists):\n    lemmatizer = WordNetLemmatizer()\n    \n    for i in range(len(words_lists)):\n        pos_tags = nltk.pos_tag(words_lists[i])\n        \n        tags = {}\n        for t in pos_tags:\n            tags[t[0]] = t[1]\n        \n        pos = {\n            'NN' : wordnet.NOUN,\n            'VB' : wordnet.VERB,\n            'JJ' : wordnet.ADJ,\n            'RB' : wordnet.ADV\n        }\n        for j in range(len(words_lists[i])):\n            if tags[words_lists[i][j]][:2] in ['NN', 'VB', 'JJ', 'RB']:\n                words_lists[i][j] = lemmatizer.lemmatize(words_lists[i][j], pos[tags[words_lists[i][j]][:2]])\n            else:\n                words_lists[i][j] = lemmatizer.lemmatize(words_lists[i][j])\n            \n    return words_lists","5060b389":"class ContitionalFrequencyHelper:\n    \n    def __init__(self, words, categories, words_lists):\n        self.words = words\n        self.categories = categories\n        self.tokens = words_lists\n        \n    def get_processed_list(self):\n        word_condition = []\n        for index in range(len(self.tokens)):\n            for word in self.words:\n                if word in self.tokens[index]:\n                    word_condition.append((self.categories[index], word))\n        return word_condition\n    \n    def conditional_frequency(self):\n        conditions = self.get_processed_list()\n        cfd = nltk.ConditionalFreqDist(conditions)\n        return cfd","bb8ace2e":"counts = dict(data.category.value_counts())\nhorizontal_bar(counts, 20, 'Category Counts')","5ee1a6ba":"sentences = list(data['text'])\ntokens = tokenizer(sentences)","251fa579":"freq_words = most_frequent_words(tokens, 10)\nhorizontal_bar(freq_words, 5, 'Most Frequent Words')","af435819":"new_tokens = remove_stopwords(tokens)","e110cff8":"high_frequency = get_longest_words_frequency(new_tokens, 10, 500)\n\n#analyse the first two words. blockchain and cryptocurrency.\nwords = ['blockchain', 'cryptocurrency', 'artificial', 'intelligence']\ncategories = list(data['category'])\n\ncfd_helper = ContitionalFrequencyHelper(words, categories, new_tokens)\ncfd = cfd_helper.conditional_frequency()","25c8d7bd":"cfd.tabulate()","7759a6fd":"bigram_measures = nltk.collocations.BigramAssocMeasures()\n\nmerge = []\n    \nfor tokens in new_tokens:\n    merge += tokens\n\ncollocation_finder = BigramCollocationFinder.from_words(merge)\n\ncollocation_scored = collocation_finder.score_ngrams(bigram_measures.raw_freq)","e403a3d3":"sentences = detokenize(new_tokens)\nlimit = 30 #top 50 collocations.\nnew_sentences = replace_collocations(sentences, collocation_scored, limit) #remove the spces between the words.","7b7e8674":"final_tokens = tokenizer(new_sentences)","26d52edf":"#final data to train.\nfinal_sentences = detokenize(final_tokens)\ndata = {\n    'Category' : categories,\n    'Text' : final_sentences\n}\n    \nfinal_data = pd.DataFrame(data)\nfinal_data.head()","2601091e":"text = list(final_data['Text'])\nlabels = list(final_data['Category'])\nX_train, X_test, y_train, y_test = train_test_split(text, labels, stratify=labels, test_size=0.20)","e46c9ca7":"steps = [('vectorize', CountVectorizer()), ('NB', MultinomialNB())]\npipeline = Pipeline(steps)\npipeline.fit(X_train,y_train)\ny_pred = pipeline.predict(X_test)\nprint('accuracy: ', accuracy_score(y_pred, y_test))","981a4f64":"print(classification_report(y_pred, y_test))","75508f91":"model = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=50, tol=0.001))\n])\n\n\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\n\nprint('accuracy: ', accuracy_score(y_pred, y_test))","7cf3e8ef":"print(classification_report(y_pred, y_test))","798a6ce3":"93 categories.","a80c7e64":"SGD classifier (46%) performed better than Naive Bias (39%).","ff297c3c":"## Consider upvoting the notebook if you have learnt something from this. Thank You.","dd2858df":"From the table above we can say that:\n1. Long words will usually carry some meaning in it and help in classification.\n2. Blockchain word is frequently used in sentences of Blockchain category which makes sense. But we see that blockchain word is used more than cryptocurrency in the sentences of category Cryptocurrency. This is because Blockchain is the technology used in Cryptocurrency hence Blockchain is used there as well.\n3. Some words makes sense when they exist together. Like artificial and intelligence in this case. Such words are called collocations.","bde673a0":"These most frequent words have zero contribution towards calssifying the sentence as they donot have a domain specific meaning and are used in almost all the sentences. These words are called stop words and have to be removed.","00c59452":"Contitional Frequency Distribution: https:\/\/www.kaggle.com\/thecobbler\/conditional-frequency-distribution-basics by Arun"}}