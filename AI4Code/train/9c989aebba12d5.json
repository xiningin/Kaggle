{"cell_type":{"3c75e738":"code","0fb31a93":"code","7371974c":"code","6984711b":"code","d6652555":"code","55acfd1f":"code","edd945c2":"code","f9dccffd":"code","02289b78":"code","8259b579":"code","df10743e":"code","0962fabe":"code","c3f34966":"code","e1dea7d3":"code","798a12e9":"code","e24ab8e2":"code","b833da4b":"code","8fd53264":"code","d2406b3c":"code","c5f7622b":"code","0dbbe08f":"code","7f809fdc":"code","8f0c03e5":"code","12949754":"code","e05088a7":"code","47216edc":"code","abae7ba4":"code","b7cad724":"code","bd753bd5":"code","ddfaadf1":"code","b1a4fe7e":"code","794607bb":"code","d1637f7a":"code","00c8cf75":"code","75b5bd3c":"code","a4476e80":"code","9089b7a8":"code","019fef33":"code","7f4574c4":"code","22171d65":"code","882ee3a3":"code","1b926c49":"code","bf6a261b":"code","d3068e69":"code","48c8ba59":"code","83cc70e2":"code","58931f8a":"code","43d70063":"code","fabbc3a7":"code","5393e3e8":"code","e9edf917":"code","c722d3ba":"code","09fc481f":"code","fb9fa271":"code","faa87290":"code","d1097a59":"code","4a031466":"code","93388bea":"code","c68bdfa0":"code","b526bc81":"code","92239e39":"code","a400ec3c":"code","e8aee399":"code","7d15af3a":"code","5afb4c67":"code","373d5b83":"code","594b11ff":"code","9d1a7e2b":"code","00fd8a3d":"code","6dc3e691":"code","615bf08a":"code","9eed9205":"code","c2705861":"code","fb3629e0":"code","1c9c7568":"code","d1d439b9":"code","f20e5187":"code","be65bf90":"code","fef75a45":"code","51e8c4ec":"code","fc22eac4":"code","cf3dcb2b":"code","6d9003d6":"code","7a3693d3":"code","ffec6a18":"code","4f80a860":"code","ff019a3d":"code","f20186d6":"code","bf413728":"code","2c904d38":"code","bb19f6ec":"code","f8e8d134":"code","51c7f132":"code","7ca9c3e4":"code","a0ee3115":"code","6f4d3b40":"code","cfe806c0":"code","df06ee30":"code","b6c514c4":"code","5a638410":"code","50fcfc61":"code","41d1c990":"code","d59fd5de":"code","1a25d1d4":"code","a0730235":"code","de5c44ce":"code","e43c5a6e":"code","0fb31262":"code","4785c0d5":"code","83a28e17":"code","ab3e49b5":"code","fc58df9e":"code","c9c07958":"code","ef7587d5":"markdown","9176068d":"markdown","71d69721":"markdown","843631da":"markdown","7c0d28e8":"markdown","8df210aa":"markdown","1f29adc1":"markdown","8cb58235":"markdown","b5f859e6":"markdown","594abe27":"markdown","9ec5a9fa":"markdown","275626a7":"markdown","0fcebf6c":"markdown","004feeba":"markdown","d331e351":"markdown","e2ae7b8b":"markdown","8cef5dc0":"markdown","d06f9ba0":"markdown","d5e28012":"markdown","472930e8":"markdown","8955ec1b":"markdown","31b2a4c2":"markdown","7e932468":"markdown","c8f76d89":"markdown","ecf75850":"markdown","b287cf89":"markdown","8009de31":"markdown","2675e9ce":"markdown","d4cd05f0":"markdown","543d7d66":"markdown","d12f6263":"markdown","24364da1":"markdown","9ae541f7":"markdown","45586a20":"markdown","0289ddcd":"markdown","f99f0d04":"markdown","3ef15854":"markdown","37151419":"markdown","98e48474":"markdown","298bfde1":"markdown","034cd62d":"markdown","5e13e908":"markdown","7060889b":"markdown","f045e0aa":"markdown","b7925c6f":"markdown","82e3535b":"markdown","c81f21aa":"markdown","06303dc3":"markdown","a2aa28f4":"markdown"},"source":{"3c75e738":"# Imports\n\n# pandas\nimport pandas as pd\nfrom pandas import Series,DataFrame\n\n# numpy, matplotlib, seaborn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport missingno as missing\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\nimport random\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import learning_curve, validation_curve\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.metrics import roc_curve, roc_auc_score ,auc, plot_roc_curve\nfrom sklearn import svm\nimport sklearn.metrics\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n","0fb31a93":"\ndf = pd.read_csv(\"..\/input\/studentperformancebig\/StudentsPerformanceBig.csv\")\n\n# preview the data\ndf.head()","7371974c":"df.info()","6984711b":"df.describe()","d6652555":"# lets check the no. of unique items present in the categorical column\n\ndf.select_dtypes('object').nunique()","55acfd1f":"plt.figure(figsize=(25,6))\nplt.subplot(1, 3, 1)\nsns.distplot(df['math score'])\n\nplt.subplot(1, 3, 2)\nsns.distplot(df['reading score'])\n\nplt.subplot(1, 3, 3)\nsns.distplot(df['writing score'])\n\nplt.suptitle('Checking for Skewness', fontsize = 15)\nplt.show()\n\n","edd945c2":"\nplt.figure(figsize=(25,6))\nplt.subplot(1, 3, 1)\nsns.boxplot(x=\"race\/ethnicity\", y=\"math score\", hue=\"gender\", data=df)\nplt.title('MATH SCORES')\nplt.subplot(1, 3, 2)\nsns.boxplot(x=\"race\/ethnicity\", y=\"reading score\", hue=\"gender\", data=df)\nplt.title('READING SCORES')\nplt.subplot(1, 3, 3)\nsns.boxplot(x=\"race\/ethnicity\", y=\"writing score\", hue=\"gender\", data=df)\nplt.title('WRITING SCORES')\nplt.show()","f9dccffd":"plt.figure(figsize=(25,6))\nplt.subplot(1, 3, 1)\nsns.boxplot(x=\"lunch\", y=\"math score\", hue=\"gender\", data=df)\nplt.title('MATH SCORES')\nplt.subplot(1, 3, 2)\nsns.boxplot(x=\"lunch\", y=\"reading score\", hue=\"gender\", data=df)\nplt.title('READING SCORES')\nplt.subplot(1, 3, 3)\nsns.boxplot(x=\"lunch\", y=\"writing score\", hue=\"gender\", data=df)\nplt.title('WRITING SCORES')\nplt.show()","02289b78":"plt.figure(figsize=(25,6))\nplt.subplot(1, 3, 1)\nsns.boxplot(x=\"parental level of education\", y=\"math score\", hue=\"gender\", data=df)\nplt.title('MATH SCORES')\nplt.xticks(rotation = 90)\nplt.subplot(1, 3, 2)\nsns.boxplot(x=\"parental level of education\", y=\"reading score\", hue=\"gender\", data=df)\nplt.title('READING SCORES')\nplt.xticks(rotation = 90)\nplt.subplot(1, 3, 3)\nsns.boxplot(x=\"parental level of education\", y=\"writing score\", hue=\"gender\", data=df)\nplt.title('WRITING SCORES')\nplt.xticks(rotation = 90)\nplt.show()\n","8259b579":"plt.figure(figsize=(25,6))\nplt.subplot(1, 3, 1)\nsns.boxplot(x=\"test preparation course\", y=\"math score\", hue=\"gender\", data=df)\nplt.title('MATH SCORES')\nplt.subplot(1, 3, 2)\nsns.boxplot(x=\"test preparation course\", y=\"reading score\", hue=\"gender\", data=df)\nplt.title('READING SCORES')\nplt.subplot(1, 3, 3)\nsns.boxplot(x=\"test preparation course\", y=\"writing score\", hue=\"gender\", data=df)\nplt.title('WRITING SCORES')\nplt.show()","df10743e":"plt.figure(figsize=(25,6))\nsns.pairplot(data=df,hue='gender',plot_kws={'alpha':0.2})\nplt.show()","0962fabe":"df['math_pass']=np.where(df['math score'] >= 65,'P','F')\ndf['reading_pass']=np.where(df['reading score'] >= 65,'P','F')\ndf['writing_pass']=np.where(df['writing score'] >= 65,'P','F')\ndf['Pass'] = df.apply(lambda x :1 if x['math score'] >= 65 and \n                      x['reading score'] >= 65 and \n                      x['writing score'] >= 65 \n                      else 0, axis =1)\ndf.head()\ndf.Pass.value_counts()","c3f34966":"plt.figure(figsize=(20,15))\n\nplt.subplot(4,3,1)\nsns.countplot(x='parental level of education', hue='writing_pass', data=df)\nplt.xticks(rotation=45)\nplt.subplot(4,3,2)\nsns.countplot(x='parental level of education', hue='math_pass', data=df)\nplt.xticks(rotation=45)\nplt.subplot(4,3,3)\nsns.countplot(x='parental level of education', hue='reading_pass', data=df)\nplt.xticks(rotation=45)\n\nplt.subplot(4,3,4)\nsns.countplot(x='gender', hue='writing_pass', data=df)\nplt.xticks(rotation=45)\nplt.title(\"Gender - Writing Pass\")\nplt.subplot(4,3,5)\nsns.countplot(x='gender', hue='math_pass', data=df)\nplt.xticks(rotation=45)\nplt.title(\"Gender - Math Pass\")\nplt.subplot(4,3,6)\nsns.countplot(x='gender', hue='reading_pass', data=df)\nplt.xticks(rotation=45)\nplt.title(\"Gender - Reading Pass\")\n\nplt.subplot(4,3,7)\nsns.countplot(x='test preparation course', hue='writing_pass', data=df)\nplt.xticks(rotation=45)\nplt.title(\"Preparation - Writing Pass\")\nplt.subplot(4,3,8)\nsns.countplot(x='test preparation course', hue='math_pass', data=df)\nplt.xticks(rotation=45)\nplt.title(\"Preparation - Math Pass\")\nplt.subplot(4,3,9)\nsns.countplot(x='test preparation course', hue='reading_pass', data=df)\nplt.xticks(rotation=45)\nplt.title(\"Preparation - Reading Pass\")\n\nplt.subplot(4,3,10)\nsns.countplot(x='race\/ethnicity', hue='writing_pass', data=df)\nplt.xticks(rotation=45)\nplt.title(\"Race - Writing Pass\")\nplt.subplot(4,3,11)\nsns.countplot(x='race\/ethnicity', hue='math_pass', data=df)\nplt.xticks(rotation=45)\nplt.title(\"Race - Math Pass\")\nplt.subplot(4,3,12)\nsns.countplot(x='race\/ethnicity', hue='reading_pass', data=df)\nplt.xticks(rotation=45)\nplt.title(\"Race - Reading Pass\")\n\nplt.tight_layout()\nplt.show()\n","e1dea7d3":"map1 = {\"high school\": 1, \"some high school\": 1,\n        \"associate's degree\": 2,\n        \"some college\": 3,\n        \"bachelor's degree\": 4,\n        \"master's degree\": 5}\ndf['parental level of education']  = df['parental level of education'].map(map1)\n\nmap2 = {\"free\/reduced\": 0,\n        \"standard\": 1}\ndf['lunch']  = df['lunch'].map(map2)\n\nmap3 = {\"none\": 0,\n        \"completed\": 1}\ndf['test preparation course']  = df['test preparation course'].map(map3)\n\nmap4 = {\"female\": 0,\n        \"male\": 1}\ndf['gender']  = df['gender'].map(map4)\n\nmap5 = {\"group A\": 1,\n        \"group B\": 2,\n        \"group C\": 3,\n        \"group D\": 4,\n        \"group E\": 5}\ndf['race\/ethnicity']  = df['race\/ethnicity'].map(map5)\n","798a12e9":"plt.figure(figsize=(13,10))\n\nplt.subplot(4,3,1)\nsns.barplot(x = \"parental level of education\" , y=\"writing score\" , data=df)\nplt.title(\"Parental level - Writing Scores\")\nplt.subplot(4,3,2)\nsns.barplot(x = \"parental level of education\" , y=\"math score\" , data=df)\nplt.title(\"Parental level - Math Scores\")\nplt.subplot(4,3,3)\nsns.barplot(x = \"parental level of education\" , y=\"reading score\" , data=df)\nplt.title(\"Parental level - Reading Scores\")\n\nplt.subplot(4,3,4)\nsns.barplot(x = \"gender\" , y=\"writing score\" , data=df)\nplt.title(\"Gender - Writing Scores\")\nplt.subplot(4,3,5)\nsns.barplot(x = \"gender\" , y=\"math score\" , data=df)\nplt.title(\"Gender - Math Scores\")\nplt.subplot(4,3,6)\nsns.barplot(x = \"gender\" , y=\"reading score\" , data=df)\nplt.title(\"Gender - Reading Scores\")\n\nplt.subplot(4,3,7)\nsns.barplot(x = \"test preparation course\" , y=\"writing score\" , data=df)\nplt.title(\"Preparation - Writing Scores\")\nplt.subplot(4,3,8)\nsns.barplot(x = \"test preparation course\" , y=\"math score\" , data=df)\nplt.title(\"Preparation - Math Scores\")\nplt.subplot(4,3,9)\nsns.barplot(x = \"test preparation course\" , y=\"reading score\" , data=df)\nplt.title(\"Preparation - Reading Scores\")\n\nplt.subplot(4,3,10)\nsns.barplot(x = \"race\/ethnicity\" , y=\"writing score\" , data=df)\nplt.title(\"Race - Writing Scores\")\nplt.subplot(4,3,11)\nsns.barplot(x = \"race\/ethnicity\" , y=\"math score\" , data=df)\nplt.title(\"Race - Math Scores\")\nplt.subplot(4,3,12)\nsns.barplot(x = \"race\/ethnicity\" , y=\"reading score\" , data=df)\nplt.title(\"Race - Reading Scores\")\n\nplt.tight_layout()\nplt.show()","e24ab8e2":"plt.subplots(figsize=(15,10)) \nsns.heatmap(df.corr(), annot = True, fmt = \".2f\")\nplt.show()","b833da4b":"dfDrop = df.drop(['math score','reading score','writing score', 'math_pass', 'reading_pass','writing_pass'], axis=1)\ndfDrop.head()\n","8fd53264":"dfDrop.info()","d2406b3c":"plt.subplots(figsize=(15,10)) \nsns.heatmap(dfDrop.corr(), annot = True, fmt = \".2f\")\nplt.show()","c5f7622b":"def plotLearningCurves(X_train, y_train, classifier, title):\n    train_sizes, train_scores, test_scores = learning_curve(\n            classifier, X_train, y_train, cv=5, scoring=\"accuracy\")\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\" ,label=\"Training Error\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\" ,label=\"Cross Validation Error\")\n    \n    plt.legend()\n    plt.grid()\n    plt.title(title, fontsize = 18, y = 1.03)\n    plt.xlabel('Data Size', fontsize = 14)\n    plt.ylabel('Error', fontsize = 14)\n    plt.tight_layout()","0dbbe08f":"def plotValidationCurves(X_train, y_train, classifier, param_name, param_range, title):\n    train_scores, test_scores = validation_curve(\n        classifier, X_train, y_train, param_name = param_name, param_range = param_range,\n        cv=5, scoring=\"accuracy\")\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    plt.plot(param_range, train_scores_mean, 'o-', color=\"b\" ,label=\"Training Error\")\n    plt.plot(param_range, test_scores_mean, 'o-', color=\"r\" ,label=\"Cross Validation Error\")\n\n    plt.legend()\n    plt.grid()\n    plt.title(title, fontsize = 18, y = 1.03)\n    plt.xlabel('Complexity', fontsize = 14)\n    plt.ylabel('Error', fontsize = 14)\n    plt.tight_layout()","7f809fdc":"def printConfusionMatrix(y_train, pred):\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_test, pred))\n    print(\"Classification Report:\",)\n    print (classification_report(y_test, pred))\n    print(\"Accuracy:\", accuracy_score(y_test, pred))","8f0c03e5":"X = dfDrop.iloc[:, :-1].values\ny = dfDrop.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","12949754":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=3,\n                                    min_samples_split=9,\n                                    min_samples_leaf=5\n                                   )\nrf.fit(X_train, y_train)\nrf_pred1 = rf.predict(X_test)","e05088a7":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 1'\nplotLearningCurves(X_train, y_train, rf, title)","47216edc":"title = 'Random Forest Validation Curve 1'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)","abae7ba4":"printConfusionMatrix(y_test, rf_pred1)","b7cad724":"plot_roc_curve(rf, X_test, y_test)\nplt.show()\n","bd753bd5":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=3,\n                                    criterion='entropy',\n                                    min_samples_split=9,\n                                    min_samples_leaf=5\n                                   )\nrf.fit(X_train, y_train)\nrf_pred2 = rf.predict(X_test)","ddfaadf1":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 2'\nplotLearningCurves(X_train, y_train, rf, title)","b1a4fe7e":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Validation Curve 2'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)","794607bb":"printConfusionMatrix(y_test, rf_pred2)","d1637f7a":"plot_roc_curve(rf, X_test, y_test)\nplt.show()","00c8cf75":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=3,\n                                    criterion='entropy',\n                                    min_samples_split=10,\n                                    min_samples_leaf=5\n                                   )\nrf.fit(X_train, y_train)\nrf_pred3 = rf.predict(X_test)","75b5bd3c":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 3'\nplotLearningCurves(X_train, y_train, rf, title)","a4476e80":"title = 'Random Forest Validation Curve 3'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)","9089b7a8":"printConfusionMatrix(y_test, rf_pred3)","019fef33":"plot_roc_curve(rf, X_test, y_test)\nplt.show()","7f4574c4":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=5,\n                                    criterion='entropy',\n                                    min_samples_split=9,\n                                    min_samples_leaf=10\n                                   )\nrf.fit(X_train, y_train)\nrf_pred4 = rf.predict(X_test)","22171d65":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 4'\nplotLearningCurves(X_train, y_train, rf, title)","882ee3a3":"title = 'Random Forest Validation Curve 4'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)","1b926c49":"printConfusionMatrix(y_test, rf_pred4)","bf6a261b":"plot_roc_curve(rf, X_test, y_test)\nplt.show()","d3068e69":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=5,\n                                    criterion='entropy',\n                                    max_features='sqrt',\n                                    min_samples_split=9,\n                                    min_samples_leaf=5\n                                   )\nrf.fit(X_train, y_train)\nrf_pred5 = rf.predict(X_test)","48c8ba59":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 5'\nplotLearningCurves(X_train, y_train, rf, title)\n","83cc70e2":"title = 'Random Forest Validation Curve 5'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)\n","58931f8a":"\nprintConfusionMatrix(y_test, rf_pred5)\n","43d70063":"plot_roc_curve(rf, X_test, y_test)\nplt.show()","fabbc3a7":"Classifier = RandomForestClassifier()\ngrid_obj = GridSearchCV(Classifier,\n                        {'n_estimators': [4, 6, 9],\n                         'max_features': ['log2', 'sqrt','auto'],\n                         'criterion': ['entropy', 'gini'],\n                         'max_depth': [2, 3, 5, 8],\n                         'min_samples_split': [2, 5, 8, 10],\n                         'min_samples_leaf': [1, 3, 5]\n                        },\n                        scoring=make_scorer(accuracy_score))\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nClassifier = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nClassifier.fit(X_train, y_train)\n\npredictions = Classifier.predict(X_test)\n\nprint(\"Best Params: \" , grid_obj.best_estimator_)\nprint(\"Best Score: \" , grid_obj.best_score_)","5393e3e8":"X = dfDrop.iloc[:, :-1].values\ny = dfDrop.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n","e9edf917":"svmC=svm.SVC(kernel = 'linear' , gamma=0.01, C=0.05)\nsvmC.fit(X_train,y_train)\n\nsvm_pred1=svmC.predict(X_test)","c722d3ba":"plt.figure(figsize=(16,5))\ntitle='Support Vector Machine Learning Curve 1'\nplotLearningCurves(X_train,y_train,svmC,title)","09fc481f":"title = 'Support Vector Machine Validation Curve 1'\nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svmC, param_name, param_range, title)","fb9fa271":"printConfusionMatrix(y_test, svm_pred1)","faa87290":"plot_roc_curve(svmC, X_test, y_test)\nplt.show()","d1097a59":"svmC=svm.SVC(kernel = 'rbf' , gamma=0.05, C=1)\nsvmC.fit(X_train,y_train)\n\nsvm_pred2=svmC.predict(X_test)","4a031466":"plt.figure(figsize=(16,5))\ntitle='Support Vector Machine Learning Curve 2'\nplotLearningCurves(X_train,y_train,svmC,title)","93388bea":"title = 'Support Vector Machine Validation Curve 2'\nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svmC, param_name, param_range, title)","c68bdfa0":"printConfusionMatrix(y_test, svm_pred2)\n","b526bc81":"plot_roc_curve(svmC, X_test, y_test)\nplt.show()","92239e39":"svmC=svm.SVC(kernel = 'sigmoid' , gamma=1, C=100)\nsvmC.fit(X_train,y_train)\n\nsvm_pred3=svmC.predict(X_test)\n","a400ec3c":"plt.figure(figsize=(16,5))\ntitle='Support Vector Machine Learning Curve 3'\nplotLearningCurves(X_train,y_train,svmC,title)","e8aee399":"title = 'Support Vector Machine Validation Curve 3' \nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svmC, param_name, param_range, title)","7d15af3a":"printConfusionMatrix(y_test, svm_pred3)\n","5afb4c67":"plot_roc_curve(svmC, X_test, y_test)\nplt.show()","373d5b83":"param_grid = {'C': [0.05, 1,10, 20], 'gamma': [0.01,0.1,0.2,1],'kernel': ['sigmoid', 'rbf','linear']}\ngrid = GridSearchCV(svm.SVC(),param_grid,refit=True,verbose=2)\nsvclassifier = grid.fit(X_train,y_train)\nSvcPredictions = svclassifier.predict(X_test)\n\nprint(\"Best Params: \" , grid.best_estimator_)\nprint(\"Best Score: \" , grid.best_score_)","594b11ff":"X = dfDrop.iloc[:, :-1].values\ny = dfDrop.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","9d1a7e2b":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=3)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred1=knn.predict(X_test)","00fd8a3d":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 1'\nplotLearningCurves(X_train,y_train,knn,title)","6dc3e691":"title = 'KNN Validation Curve 1' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","615bf08a":"printConfusionMatrix(y_test, knn_pred1)","9eed9205":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","c2705861":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=7)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred2=knn.predict(X_test)","fb3629e0":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 2'\nplotLearningCurves(X_train,y_train,knn,title)","1c9c7568":"title = 'KNN Validation Curve 2' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","d1d439b9":"printConfusionMatrix(y_test, knn_pred2)","f20e5187":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","be65bf90":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=10)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred3=knn.predict(X_test)","fef75a45":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 3'\nplotLearningCurves(X_train,y_train,knn,title)","51e8c4ec":"title = 'KNN Validation Curve 3' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","fc22eac4":"printConfusionMatrix(y_test, knn_pred3)","cf3dcb2b":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","6d9003d6":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=20)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred4=knn.predict(X_test)","7a3693d3":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 4'\nplotLearningCurves(X_train,y_train,knn,title)","ffec6a18":"title = 'KNN Validation Curve 4' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","4f80a860":"printConfusionMatrix(y_test, knn_pred4)","ff019a3d":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","f20186d6":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=17)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred5=knn.predict(X_test)","bf413728":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 5'\nplotLearningCurves(X_train,y_train,knn,title)","2c904d38":"title = 'KNN Validation Curve 5' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","bb19f6ec":"printConfusionMatrix(y_test, knn_pred5)","f8e8d134":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","51c7f132":"#create new a knn model\nknn2=KNeighborsClassifier()\n#create a dictionary of all values we want to test for n_neighbors\nparam_grid= {'n_neighbors': np.arange(1, 20)}\n#use gridsearch to test all values for n_neighbors\nknn_gscv=GridSearchCV(knn2, param_grid, cv=5)\n#fit model to data\nknn_gscv.fit(X, y)\n\nprint(\"Best Params: \" , knn_gscv.best_estimator_)\nprint(\"Best Score: \" , knn_gscv.best_score_)","7ca9c3e4":"X = dfDrop.iloc[:, :-1].values\ny = dfDrop.iloc[:, -1].values","a0ee3115":"# Encoding categorical inputs\nencoder = OneHotEncoder(handle_unknown=\"ignore\")\nencoder.fit(X)\nX = encoder.transform(X)\n\n# 80\/20 train split ratio\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=1)","6f4d3b40":"mlp = MLPClassifier(\n    max_iter=3000,\n    hidden_layer_sizes=[17, 13, 7], \n    solver=\"sgd\", \n    random_state=1,\n    verbose=False\n).fit(X_train, y_train)\n\nmlp_pred1 = mlp.predict(X_test)","cfe806c0":"def format_scores_as_dataframe(labels, train_scores, test_scores):\n    learning_data = {\"labels\": [], \"type\": [], \"score\": []}\n\n    for i in range(len(train_sizes)):\n        for j in range(len(train_scores)):\n            learning_data[\"labels\"].append(labels[i])\n            learning_data[\"type\"].append(\"train\")\n            learning_data[\"score\"].append(train_scores[i][j])\n            learning_data[\"labels\"].append(labels[i])\n            learning_data[\"type\"].append(\"test\")\n            learning_data[\"score\"].append(test_scores[i][j])\n            \n    return pd.DataFrame.from_dict(learning_data)","df06ee30":"train_sizes, train_scores, test_scores = learning_curve(mlp, X, y)\n\nlearning_curve_df = format_scores_as_dataframe(train_sizes, train_scores, test_scores)\n\n# train and test learning scores results\nax = sns.lineplot(x=\"labels\", y=\"score\", hue=\"type\", data=learning_curve_df, marker=\"o\", ci=None)\nax.set_title(\"Learning Curve for MLP Algorithm\")\ndev_null = ax.set(xlabel=\"Samples\", ylabel=\"Error\")","b6c514c4":"scores = cross_val_score(mlp, X, y)\n\nscores, scores.mean(), scores.std()\n\ndev_null = sns.lineplot(x=[1,2,3,4,5], y=scores)\ndev_null.set_title(\"Cross Score Distribution\")\ndev_null = dev_null.set(xlabel=\"# of runs\", ylabel=\"Accuracy\")","5a638410":"\ncross_val_result = cross_validate(mlp, X, y, return_train_score=True)\n\n\n#validation_curve(mlp, X, y, param_name=\"alpha\", param_range=[0.0001, 0.001, 0.05])\ntrain_scores, test_scores = validation_curve(mlp, X, y, param_name=\"hidden_layer_sizes\", param_range=([5], [10], [10,5], [15, 10], [25,10,5]))\n\nval_curve_data = {\"labels\": [], \"type\": [], \"scores\": []}\nparam_ranges = [\"[5]\", \"[10]\", \"[10,5]\", \"[15,10]\", \"[25,10,5]\"]\n\nfor i in range(len(train_scores)):\n    for j in range(len(train_scores[i])):\n        val_curve_data[\"labels\"].append(param_ranges[i])\n        val_curve_data[\"type\"].append(\"train\")\n        val_curve_data[\"scores\"].append(train_scores[i][j])\n        val_curve_data[\"labels\"].append(param_ranges[i])\n        val_curve_data[\"type\"].append(\"test\")\n        val_curve_data[\"scores\"].append(test_scores[i][j])\n        \nval_curve_df = pd.DataFrame.from_dict(val_curve_data)\n\nax = sns.lineplot(x=\"labels\", y=\"scores\", hue=\"type\", data = val_curve_df, marker=\"o\", ci=None)\nax.set_title(\"Validation Curve for our MLP model\")\ndev_null = ax.set(xlabel=\"Layers\/Neurons\", ylabel=\"Accuracy Score\")","50fcfc61":"\nprintConfusionMatrix(y_test, mlp_pred1)","41d1c990":"plot_roc_curve(mlp, X_test, y_test)\nplt.show()","d59fd5de":"mlp = MLPClassifier(\n    max_iter=3000,\n    hidden_layer_sizes=[17, 13, 7], \n    solver=\"sgd\",\n    activation=\"logistic\",\n    random_state=1,\n    verbose=False\n).fit(X_train, y_train)\n\nmlp_pred2 = mlp.predict(X_test)","1a25d1d4":"train_sizes, train_scores, test_scores = learning_curve(mlp, X, y)\n\nlearning_curve_df = format_scores_as_dataframe(train_sizes, train_scores, test_scores)\n\n# train and test learning scores results\nax = sns.lineplot(x=\"labels\", y=\"score\", hue=\"type\", data=learning_curve_df, marker=\"o\", ci=None)\nax.set_title(\"Learning Curve for MLP Algorithm\")\ndev_null = ax.set(xlabel=\"Samples\", ylabel=\"Error\")","a0730235":"scores = cross_val_score(mlp, X, y)\n\nscores, scores.mean(), scores.std()\n\ndev_null = sns.lineplot(x=[1,2,3,4,5], y=scores)\ndev_null.set_title(\"Cross Score Distribution\")\ndev_null = dev_null.set(xlabel=\"# of runs\", ylabel=\"Accuracy\")","de5c44ce":"cross_val_result = cross_validate(mlp, X, y, return_train_score=True)\n\n#validation_curve(mlp, X, y, param_name=\"alpha\", param_range=[0.0001, 0.001, 0.05])\ntrain_scores, test_scores = validation_curve(mlp, X, y, param_name=\"hidden_layer_sizes\", param_range=([5], [10], [10,5], [15, 10], [25,10,5]))\n\nval_curve_data = {\"labels\": [], \"type\": [], \"scores\": []}\nparam_ranges = [\"[5]\", \"[10]\", \"[10,5]\", \"[15,10]\", \"[25,10,5]\"]\n\nfor i in range(len(train_scores)):\n    for j in range(len(train_scores[i])):\n        val_curve_data[\"labels\"].append(param_ranges[i])\n        val_curve_data[\"type\"].append(\"train\")\n        val_curve_data[\"scores\"].append(train_scores[i][j])\n        val_curve_data[\"labels\"].append(param_ranges[i])\n        val_curve_data[\"type\"].append(\"test\")\n        val_curve_data[\"scores\"].append(test_scores[i][j])\n        \nval_curve_df = pd.DataFrame.from_dict(val_curve_data)\n\nax = sns.lineplot(x=\"labels\", y=\"scores\", hue=\"type\", data = val_curve_df, marker=\"o\", ci=None)\nax.set_title(\"Validation Curve for our MLP model\")\ndev_null = ax.set(xlabel=\"Layers\/Neurons\", ylabel=\"Accuracy Score\")","e43c5a6e":"printConfusionMatrix(y_test, mlp_pred2)\n","0fb31262":"plot_roc_curve(mlp, X_test, y_test)\nplt.show()","4785c0d5":"parameters = {\n    \"hidden_layer_sizes\": [[8], [5]], #, [2], [8,8], [8,5], [5,8], [5,2], [2,2], [8,5,2], [8,5,5], [13,8,4], [17,13,7]\n    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"], \n    \"solver\": [\"lbfgs\", \"sgd\", \"adam\"], \n    \"max_iter\": [200, 500, ] #1000, 2000, 3000, 5000\n}\n\n# Brace yourself, this will take a while\nmlp = MLPClassifier()\ngs = GridSearchCV(mlp, parameters)\ngs.fit(X_train, y_train)\ngs.predict(X_test)\n\nprint(\"Best Params: \" , gs.best_estimator_)\nprint(\"Best Score: \" , gs.best_score_)","83a28e17":"\n# Instantiate the classfiers and make a list\nclassifiers = [RandomForestClassifier(),\n                MLPClassifier(), \n               svm.SVC(),\n               KNeighborsClassifier()]\n\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n\n\n# print('auc =', auc)\nlr_fpr1, lr_tpr1, _ = roc_curve(y_test, rf_pred3)\nlr_fpr2, lr_tpr2, _ = roc_curve(y_test,  mlp_pred1)\nlr_fpr3, lr_tpr3, _ = roc_curve(y_test, svm_pred1)\nlr_fpr4, lr_tpr4, _ = roc_curve(y_test, knn_pred5)\n\n# fpr , tpr, _= roc_curve(X_test, predict6_test)\nauc1 = roc_auc_score(y_test, rf_pred3)\nauc2 = roc_auc_score(y_test,  mlp_pred1)\nauc3 = roc_auc_score(y_test, svm_pred1)\nauc4 = roc_auc_score(y_test, knn_pred5)\n \n    \nresult_table = result_table.append({'classifiers':RandomForestClassifier.__class__.__name__,\n                                     'fpr':lr_fpr1, \n                                     'tpr':lr_tpr1, \n                                     'auc':auc1}, ignore_index=True)\n\nresult_table = result_table.append({'classifiers':MLPClassifier.__class__.__name__,\n                                     'fpr':lr_fpr2, \n                                     'tpr':lr_tpr2, \n                                     'auc':auc2}, ignore_index=True)\n\nresult_table = result_table.append({'classifiers':svm.SVC.__class__.__name__,\n                                     'fpr':lr_fpr3, \n                                     'tpr':lr_tpr3, \n                                     'auc':auc3}, ignore_index=True)\n\nresult_table = result_table.append({'classifiers':KNeighborsClassifier.__class__.__name__,\n                                     'fpr':lr_fpr4, \n                                     'tpr':lr_tpr4, \n                                     'auc':auc4}, ignore_index=True)\n\nfig = plt.figure(figsize=(8,6))\n\nplt.plot(result_table.loc[0]['fpr'], \n         result_table.loc[0]['tpr'], \n         label=\"RandomForestClassifier, AUC={:.3f}\".format( result_table.loc[0]['auc']))\n\nplt.plot(result_table.loc[1]['fpr'], \n         result_table.loc[1]['tpr'], \n         label=\"MLPClassifier, AUC={:.3f}\".format( result_table.loc[1]['auc']))\n\nplt.plot(result_table.loc[2]['fpr'], \n         result_table.loc[2]['tpr'], \n         label=\"SVM, AUC={:.3f}\".format( result_table.loc[2]['auc']))\n\nplt.plot(result_table.loc[3]['fpr'], \n         result_table.loc[3]['tpr'], \n         label=\"KNeighborsClassifier, AUC={:.3f}\".format( result_table.loc[3]['auc']))\n\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()\n    ","ab3e49b5":"!apt-get remove swig \n!apt-get install swig3.0 build-essential -y\n!ln -s \/usr\/bin\/swig3.0 \/usr\/bin\/swig\n!apt-get install build-essential\n!pip install --upgrade setuptools\n!pip install auto-sklearn","fc58df9e":"X = dfDrop.iloc[:, :-1].values\ny = dfDrop.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","c9c07958":"import autosklearn.classification\nimport sklearn.model_selection\nimport sklearn.datasets\nimport sklearn.metrics\nimport os  \nimport autosklearn.regression\n\n\nautoml = autosklearn.classification.AutoSklearnClassifier(\n    time_left_for_this_task=120,\n    per_run_time_limit=30,\n    tmp_folder='\/tmp\/autosklearn_cv_example_tmp5',\n    output_folder='\/tmp\/autosklearn_cv_example_out5',\n    delete_tmp_folder_after_terminate=False,\n    resampling_strategy='cv',\n    resampling_strategy_arguments={'folds': 5},\n)\n\n# fit() changes the data in place, but refit needs the original data. We\n# therefore copy the data. In practice, one should reload the data\nautoml.fit(X_train.copy(), y_train.copy(), dataset_name='Students')\n# During fit(), models are fit on individual cross-validation folds. To use\n# all available data, we call refit() which trains all models in the\n# final ensemble on the whole dataset.\nautoml.refit(X_train.copy(), y_train.copy())\n\nprint(automl.show_models())\n\npredictions = automl.predict(X_test)\nprint(\"Accuracy as per AutoML: \", sklearn.metrics.accuracy_score(y_test, predictions))","ef7587d5":"Learning curve is a measurement to check how well the model learns. This is measured by taking a reading of the accuracy of the algorithm as it trains and also while it is testing. This are plotting to see the convergence.","9176068d":"## 1.4 Training Data Info","71d69721":"### 2.2.6 Overall Comparison by mapping","843631da":"### 2.2.5 Relation between scores","7c0d28e8":"we used kernel=sigmoid, C=100, gamma=1 it showed us in the learning curve as the data size increased and the variance is decreasing, there is low variance and the error started to decrease as data increased. In the validation curve we can see that there is low variance as \"C\"(Cost) increases they are overlapping and the error is decreasing. And we got 37% AUC which is bad, lets try other kernel with some hyper parameter. Lets try other hyper-parameter and see our result with grid search\n","8df210aa":"### This function is for drawing the learning curve.","1f29adc1":"The confusion matrix shows the frequency for True Positives, True Negatives, False Positives, and False Negative. Also a summary of the different properties can be presented here, along with the accuracy for predicted values.","8cb58235":"> Our data seems to be clean of missing values.","b5f859e6":"## 4.2 AutoML","594abe27":"# 1. Data Set Selection\n\nObjective is to understand the influence of various factors like economic, personal and social on the students performance\nInferences would be :\n* How to imporve the students performance in each test ?\n* What are the major factors influencing the test scores ?\n* Effectiveness of test preparation course?","9ec5a9fa":"### Grid Search Results","275626a7":"### 2.2.3 Parental level of education vs Score","0fcebf6c":"## 1.3 Data Dictionary\n1. **gender** -- Male or Female\n1. **race\/ethnicity** -- group A,B, ...\n1. **Parental Level of education** --\tMaster, Bachelor, ... \n1. **lunch** -- standared or free\t\n1. **test preparation course**\t-- complete, none\n1. **math score** -- score of math course\n1. **reading score** -- score of reading course\t\n1. **writing score** -- score of writing course","004feeba":"## 1.6 Distict values","d331e351":"## 3. Models","e2ae7b8b":"### 2.2.6 Mapping score to Pass or Fail\nTo be passed in a course, you have to get 60 or more. and to be marked as \"Passed\" you have to pass the 3 courses.","8cef5dc0":"svm uses three hyperparameters kernel,Cost,gamma.From Kernel we used linear, sigmoid and rbf, with some hyper-parameter.They gave us different results based on the hyper-parameter. With grid search we put different values hyper-parameter and the best it gave us is with C=10 gamma=0.2 and score is 68%","d06f9ba0":"# 4. Best Model (Over All AUC) and AutoML","d5e28012":"## 2.1 Checking for Skewness","472930e8":"# References: \n\n* http:\/\/roycekimmons.com\/tools\/generated_data\/exams\n* https:\/\/www.kaggle.com\/jeffd23\/scikit-learn-ml-from-start-to-finish\n* https:\/\/www.kaggle.com\/roshansharma\/student-performance-analysis\n* https:\/\/www.kaggle.com\/spscientist\/student-performance-in-exams\n* https:\/\/www.kaggle.com\/nitindatta\/eda-in-depth\n* http:\/\/scikit-learn.sourceforge.net\/stable\/auto_examples\/model_selection\/plot_validation_curve.html#example-model-selection-plot-validation-curve-py\n* https:\/\/chrisalbon.com\/machine_learning\/model_evaluation\/plot_the_validation_curve\/\n* https:\/\/datascience.stackexchange.com\/questions\/76304\/gridsearchcv-with-random-forest-classifier\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#:~:text=A%20random%20forest%20classifier.%20A%20random%20forest%20is,to%20improve%20the%20predictive%20accuracy%20and%20control%20over-fitting.\n* https:\/\/scikit-learn.org\/stable\/modules\/multiclass.html#multioutput-regression\n* https:\/\/www.datacamp.com\/community\/tutorials\/random-forests-classifier-python\n* https:\/\/www.tutorialspoint.com\/machine_learning_with_python\/machine_learning_with_python_classification_algorithms_random_forest.htm\n* https:\/\/florianhartl.com\/thoughts-on-machine-learning-dealing-with-skewed-classes.html\n* https:\/\/www.kaggle.com\/ahmedengu\/lanl-master-s-features-autosklearn","8955ec1b":"## 3.4 MLP","31b2a4c2":"### 2.2.2 Lunch vs Score","7e932468":"### 2.2.4 Test preparation course vs Score","c8f76d89":"**Using Entropy instead of default (gini)","ecf75850":"### Grid Search Results","b287cf89":"### This function is for drawing the validation curve.","8009de31":"## 3.3 KNN","2675e9ce":"The curve above shows the cross-validation scores for the default 5 runs in the cross-validation process for the MLP model.","d4cd05f0":"### Grid Search Results","543d7d66":"## 1.2 Reading the data","d12f6263":"# ML Project 1\n## Introduction\nThe objective of this project is to train several classification models, and practice model tuning (bias\/varience) tradeoff. \n\n## Agenda\n1. Data Set Selection \n1. EDA\n1. Models\n1. AutoML\n\n## Team members\n1. Eden Zere\n1. Essey Abraham Tezare\n1. Hussien Mohamed Bayoumy Mohamed Elgabry\n1. Mario Arismendi Matos\n1. Moustafa Ahmed Galal Bahnasawy\n1. Youssef Samy Mounir\n","24364da1":"## 1.1 Import libraries","9ae541f7":"we used kernel=rbf, C=1, gamma=0.05 it showed us in the learning curve as the data increased the variance is decreasing too. In the validation curve we can see that there is high Error as \"C\"(Cost) increases. And we got 70% AUC which is okay, lets try other kernel with some hyper parameter. Lets try other hyper-parameter and see our result\n","45586a20":"### Compute learning curve for MLP","0289ddcd":"# 2. EDA","f99f0d04":"oneHotEncoder is used to encode categiorical columns into values that can be digested by the used algorithm implementation, in our case it.\n\nThe MLP configured above will iterate 3000 times, use hidden layers and 17, 13, 7, solver stochastic gradient descent. Our data was devided into a 80\/20 train\/set splits to train and evaluate your classifier.","3ef15854":"## 3.1 DTree ","37151419":"## 2.2 Relation between features and Score","98e48474":"### 2.2.1 Race\/ethnicity VS Score","298bfde1":"### This function is for printing the confusion matrix","034cd62d":"## 1.5 Data description","5e13e908":"SVM is used for classification.It uses a technique called kernel trick to transform data and based on these transformation it finds an optimal boundary between the possible outputs","7060889b":"Cross validation is a measure of how well our model can generalize from what it learns. How well will it perform with data it has neven seen before. This is done by saving part of the data to later predict and measure the accuracy. The training data is split with differing testing folds to be used. Default in this case is k=5 folds.","f045e0aa":"### Compute cross-validation curve","b7925c6f":"## 3.2 SVM ","82e3535b":"### 2.2.7 Correclation Matrix between features","c81f21aa":"## 4.1 AUC curve over all models","06303dc3":"### GridSearch Results","a2aa28f4":"### 2.2.9 Drop unneccessary columns\nNow we can drop math score, reading score and writing score, as we will use the pass column instead."}}