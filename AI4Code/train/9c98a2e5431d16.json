{"cell_type":{"09ca703b":"code","7639faf4":"code","f39392e2":"code","bd0257cd":"code","3afa604e":"code","8981d3e1":"code","dedbc90c":"code","56b8af8b":"code","475e420f":"code","88c633b2":"code","4b2c6517":"code","23a5b2dd":"markdown","6f2a27f4":"markdown","f8397688":"markdown"},"source":{"09ca703b":"\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom random import choices\n\n\nSEED = 1111\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n'''\ntrain = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\ntrain = train.query('date > 85').reset_index(drop = True) \ntrain = train[train['weight'] != 0]\n\ntrain.fillna(train.mean(),inplace=True)\n\ntrain['action'] = ((train['resp'].values) > 0).astype(int)\n\n\nfeatures = [c for c in train.columns if \"feature\" in c]\n\nf_mean = np.mean(train[features[1:]].values,axis=0)\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\nX_train = train.loc[:, train.columns.str.contains('feature')]\n#y_train = (train.loc[:, 'action'])\n\ny_train = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\n\n\n\n\ndef create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n\n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model\n\n\nbatch_size = 5000\nhidden_units = [150, 150, 150]\ndropout_rates = [0.2, 0.2, 0.2, 0.2]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\nclf = create_mlp(\n    len(features), 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n    )\n\nclf.fit(X_train, y_train, epochs=200, batch_size=5000)\n\n'''\n","7639faf4":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","f39392e2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport keras\nfrom sklearn.metrics import accuracy_score\nimport gc\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Activation\nfrom keras.utils.generic_utils import get_custom_objects\nfrom scipy.stats import reciprocal\nfrom sklearn.model_selection import RandomizedSearchCV","bd0257cd":"train_raw = reduce_mem_usage(pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv'))","3afa604e":"train = train_raw[train_raw['weight'] != 0]\ntrain = train[train['resp'] != 0]\nfeatures = [c for c in train.columns if \"feature\" in c]\n\nf_mean = np.mean(train[features[1:]].values,axis=0)\n\ntrain = train.fillna(-999)","8981d3e1":"f_mean","dedbc90c":"features","56b8af8b":"X = train.iloc[:, train.columns.str.contains('feature')]\ny = (train.loc[:, 'resp'] > 0).astype(int)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, random_state = 0)\n\ndel train\ngc.collect()","475e420f":"hidden_layers = 1 \nneuron_numbers = 30 \nrate_of_learning = 3e-3 \n\n\ndef gelu(x):\n    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 \/ np.pi) * (x + 0.044715 * tf.pow(x, 3))))\nget_custom_objects().update({'gelu': Activation(gelu)})\n\n\ndef ModelTuner1(drop_rate, hiddenN = hidden_layers, neuronsN = neuron_numbers, learnR = rate_of_learning):\n    model = keras.models.Sequential()\n    model.add(keras.layers.Flatten(input_shape=[130]))\n    model.add(keras.layers.BatchNormalization())\n    for layers in range(hiddenN):\n        model.add(keras.layers.Dense(neuronsN, activation=\"gelu\"))\n        model.add(keras.layers.Dropout(drop_rate))\n        model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n\n    optim = keras.optimizers.Nadam(lr = learnR)\n    model.compile(loss=\"binary_crossentropy\", optimizer = optim, metrics = [\"AUC\"])\n    return model\n\n\nparams = {\n            \"hiddenN\": [1, 2, 3, 4, 5, 6, 7, 8, 9],\n            \"neuronsN\": np.arange(1, 1000),\n            \"learnR\": reciprocal(3e-4, 3e-2),\n            \"drop_rate\": reciprocal(1e-1, 5e-1)\n          }\n\nreg_keras = keras.wrappers.scikit_learn.KerasRegressor(ModelTuner1)\nrsCV = RandomizedSearchCV(reg_keras, params, n_iter=10, cv=5)","88c633b2":"\ndrop_rate = 0.1263138694014944\nhiddenN = 4\nlearnR = 0.01861769765675586\nneuronsN = 745\n\n\ndef ModelTuned(drop_rate, hiddenN, neuronsN, learnR):\n    model = keras.models.Sequential()\n    model.add(keras.layers.Flatten(input_shape=[130]))\n    model.add(keras.layers.BatchNormalization())\n    for layers in range(hiddenN):\n        model.add(keras.layers.Dense(neuronsN, activation=\"gelu\"))\n        model.add(keras.layers.Dropout(drop_rate))\n        model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n\n    optim = keras.optimizers.Nadam(lr = learnR)\n    model.compile(loss=\"binary_crossentropy\", optimizer = optim, metrics = [\"AUC\"])\n    return model\n\nmodel = ModelTuned(drop_rate, hiddenN, neuronsN, learnR)\n\nmodel.fit(X_train, y_train, epochs=12, validation_data = (X_val, y_val), callbacks=[keras.callbacks.EarlyStopping(patience = 50)], batch_size = 4096)\n","4b2c6517":"models = []\n\nmodels.append(model)\n\nth = 0.503\n\n\nf = np.median\nmodels = models[-3:]\nimport janestreet\nenv = janestreet.make_env()\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n        pred = f(pred)\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","23a5b2dd":"ghjkjhg","6f2a27f4":"Hello to all Kagglers! I present you a relatively simple Neural Network model that I designed for this task and got a score of ~ 9766. Good luck to everyone! :) ","f8397688":"That's it!"}}