{"cell_type":{"aaf6d7be":"code","1aa1e328":"code","e7da807e":"code","bbde6fd4":"code","13c683e5":"code","8381d925":"code","64001754":"code","2cba3827":"code","d5d5c420":"code","6fcdbae7":"code","c30e0dff":"code","a6f62a49":"code","e5982689":"code","953368e9":"markdown","6cc98b2d":"markdown","f9d98e73":"markdown","aa368f42":"markdown"},"source":{"aaf6d7be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1aa1e328":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int64)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float64)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e7da807e":"%%time\nplayers = pd.read_csv('\/kaggle\/input\/mlb-player-digital-engagement-forecasting\/players.csv')\nplayers = reduce_mem_usage(players,verbose = False)\n\nteams = pd.read_csv('\/kaggle\/input\/mlb-player-digital-engagement-forecasting\/teams.csv')\nteams = reduce_mem_usage(teams,verbose = False)\n\nseasons = pd.read_csv('\/kaggle\/input\/mlb-player-digital-engagement-forecasting\/seasons.csv')\nseasons = reduce_mem_usage(seasons,verbose = False)\n\ntrain = pd.read_csv('\/kaggle\/input\/mlb-player-digital-engagement-forecasting\/train.csv')\ntrain = reduce_mem_usage(train,verbose = False)\n\nawards = pd.read_csv('\/kaggle\/input\/mlb-player-digital-engagement-forecasting\/awards.csv')\nawards = reduce_mem_usage(awards,verbose = False)","bbde6fd4":"pids_test = players.playerId[players.playerForTestSetAndFuturePreds == True]","13c683e5":"def unpack_raw_data(raw_data,dfs_name):\n    \n    unnested_data_dict = dict()\n# columns = train.drop('date', axis = 1).columns.values.tolist()\n# columns\n\n    for col in dfs_name:\n\n        data_nested_info = raw_data[['date',col]]\n\n        data_nested_info = (data_nested_info[\n              ~pd.isna(data_nested_info[col])\n              ].\n              reset_index(drop = True)\n              )\n\n        daily_dfs_collection = []\n        for data_index, data_row in data_nested_info.iterrows():\n            daily_df = pd.read_json(data_row[col])\n\n            daily_df['dailydate'] = data_row['date']\n\n            daily_dfs_collection = daily_dfs_collection + [daily_df]\n\n        unnested_table = (pd.concat(daily_dfs_collection,\n              ignore_index = True).\n                # Set and reset index to move 'dailyDataDate' to front of df\n              set_index('dailydate').\n              reset_index()\n              )\n\n    #     display(col)\n        unnested_table = reduce_mem_usage(unnested_table,verbose = False)\n\n        unnested_data_dict[col] = unnested_table\n        del daily_dfs_collection,unnested_table\n    return unnested_data_dict \n    ","8381d925":"# check = unpack_raw_data(train,['playerBoxScores','games'])\nfeatures = ['dailydate','engagementMetricsDate','target1','target2','target3','target4','flyOuts','strikeOuts','stolenBases','homeRunsPitching']","64001754":"def make_train_data(raw_data,features):\n    \n    nday_pl_eng = raw_data['nextDayPlayerEngagement']\n    pl_box_scores = raw_data['playerBoxScores']\n  \n    pl_eng_w_scores = pd.merge(nday_pl_eng,pl_box_scores,on=['dailydate','playerId'],how = 'inner')\n    train_data = pl_eng_w_scores[features]\n    del nday_pl_eng,pl_box_scores\n    return train_data,features","2cba3827":"raw_data = unpack_raw_data(train,['nextDayPlayerEngagement','playerBoxScores'])\ntrain_data,features = make_train_data(raw_data,features)\ndel(raw_data)","d5d5c420":"train_data.fillna(-1,inplace = True)\nsample_y = train_data[['target1','target2','target3','target4']]\nsample_X = train_data[['flyOuts','strikeOuts','stolenBases','homeRunsPitching']]\n\n# display(sample_X.head())\n# display(sample_y.head())\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(sample_X,sample_y)\ndel train_data,sample_X,sample_y","6fcdbae7":"from keras.models import Sequential\nfrom keras.layers import Dense\nmodel = Sequential()\nmodel.add(Dense(6,input_dim = 4,activation = 'relu'))\nmodel.add(Dense(12,activation = 'relu'))\nmodel.add(Dense(12,activation = 'relu'))\nmodel.add(Dense(4))\nmodel.compile(optimizer = 'adam',loss = 'mae', metrics = ['mae'])\n\nfit_model = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=64)","c30e0dff":"features = ['flyOuts','strikeOuts','stolenBases','homeRunsPitching']\nprimary_cols = 'playerBoxScores'","a6f62a49":"def make_features(unnested_data_dict,primary_cols,features,sample_prediction_df):\n    \n    test_set = unnested_data_dict[primary_cols]\n    tmp = features.copy()\n    tmp.append('playerId')\n    test_set = test_set[tmp]\n    test_set = test_set.groupby('playerId').sum().reset_index()\n    test_set = test_set.merge(pids_test,on = 'playerId',how = 'right')\n    test_set = test_set.fillna(-1)\n    sub_df = sample_prediction_df.copy()\n    sub_df['playerId'] = sub_df['date_playerId'].map(lambda x: int(x.split('_')[1]))\n    test_set = sub_df.merge(test_set,on = 'playerId', how = 'left')\n    test_set = test_set[features]\n\n    return test_set ","e5982689":"import mlb\nenv = mlb.make_env() # initialize the environment\niter_test = env.iter_test() # iterator which loops over each date in test set\n\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    test_df = test_df.reset_index().rename(columns = {'index':'date'})\n    test_data = unpack_raw_data(test_df,['playerBoxScores'])\n    \n    fit_data = make_features(test_data,primary_cols,features,sample_prediction_df)\n    \n    pred = model.predict(fit_data)\n    pred = pred.clip(0,100)\n    pred = pred.round(2)\n    \n    sample_prediction_df[['target1','target2','target3','target4']] = pred\n                        \n    \n    \n#     sample_prediction_df['target1'] = 0.4\n#     sample_prediction_df['target2'] = 2\n#     sample_prediction_df['target3'] = 0.4\n#     sample_prediction_df['target4'] = 0.7\n    \n#     sample_prediction_df = sample_prediction_df[['date_playerId']].reset_index().merge(submission,\n#                                 how='left', on='date_playerId').set_index('date')\n#     del submission\n    env.predict(sample_prediction_df)","953368e9":"# ***Submission***","6cc98b2d":"# **Reading Data**","f9d98e73":"***Function To reduce the size of the data***","aa368f42":"# ***Building ANN and training the model***"}}