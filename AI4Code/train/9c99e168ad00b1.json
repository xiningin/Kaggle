{"cell_type":{"cc147c3f":"code","4c693878":"code","ffe156bb":"code","83ee3196":"code","0d22b5ce":"code","1b55cfc5":"code","6c434825":"code","e1917e13":"code","8ec3b02c":"code","8b59bc21":"code","b9a6df97":"code","db65cad2":"code","d4d86a22":"code","f5df1c0b":"code","8cff0aff":"code","cb578e61":"code","21d98ccd":"code","6a33a0b9":"code","26e10232":"code","7f5778c4":"code","63caf877":"code","6321c79c":"code","bc6eb340":"code","70116ae9":"code","384609db":"code","03d399aa":"code","ae543ea0":"code","6335b865":"code","98e986eb":"code","93a6d289":"code","3cf458b7":"markdown","e43f7ef8":"markdown","91bccd4f":"markdown","4a070628":"markdown","f68126f5":"markdown","9c879fe5":"markdown","f89417d3":"markdown","064f1199":"markdown","a640112f":"markdown","31fe5390":"markdown","c54a52f0":"markdown","a087fa7c":"markdown","a0f56b54":"markdown"},"source":{"cc147c3f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4c693878":"#importing libraries\nimport warnings\nimport numpy as np\nimport pandas as pd\n\nimport langdetect\nfrom langdetect import detect\nimport string \nimport spacy \nfrom spacy.lang.en.stop_words import STOP_WORDS \nfrom spacy.lang.en import English\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pyLDAvis\nimport pyLDAvis.sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline","ffe156bb":"warnings.filterwarnings(\"ignore\")#not show warning for deprecated","83ee3196":"# read data\ndf=pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")","0d22b5ce":"df.shape","1b55cfc5":"df.info()","6c434825":"df.isnull().sum()","e1917e13":"abstract=df[\"abstract\"].dropna()#drooping all rows with all NaN values in abstract columns\n\nlen(abstract)","8ec3b02c":"# fuction to make a mask selecting text by languaje\ndef detect_lang(text,lang):\n    try:\n        return detect(text['abstract']) == lang\n    except:\n        return False\n    ","8b59bc21":"# bulding a mask to filter out text written in English\ndf_abstracts = pd.DataFrame(abstract)\nen_abstracts_mask = df_abstracts.apply(lambda row: detect_lang(row, \"en\"), axis=1)\n","b9a6df97":"abstracts_en=df_abstracts[en_abstracts_mask]\nprint(len(abstracts_en), \"of our initial abstracts are writing in english languaje, we will use just those papers to made the analysis\")","db65cad2":"# Join the different processed titles together.\nlong_string = ','.join(list(abstracts_en[\"abstract\"].values))\n# Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n# Generate a word cloud\nwordcloud.generate(long_string)\n# Visualize the word cloud\nwordcloud.to_image()","d4d86a22":"# list of punctuation and simbols\nsymbols_punctuations = string.punctuation\n\n#  disabling Named Entity Recognition for speed\nnlp = spacy.load('en',disable=['parser', 'ner'])\n\n# Create our list of stopwords\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n\ndef tokenizer(text):\n    # Creating our token object\n    mytokens = nlp(text)\n\n    # Lemmatizing each token and converting each token into lowercase if not pronoum\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n\n    # Removing stop words and puntuation\n    mytokens = [ word for word in mytokens if word not in symbols_punctuations  and word not in stop_words]\n \n    mytokens = [ word for word in mytokens if len(word) > 2]\n\n    \n    return mytokens","f5df1c0b":"np.random.seed()\n#creation of the bag of word with CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# bag of words using our tokenizer .defining  ngram_range=(1,1) means only unigrams \n\nCountVectorizer_bow = CountVectorizer(tokenizer = tokenizer, ngram_range=(1,1),max_df=0.80,min_df=2) \nbow_raw_count=CountVectorizer_bow.fit_transform(abstracts_en[\"abstract\"])","8cff0aff":"bow_raw_count.shape","cb578e61":"\n\"\"\"from sklearn.decomposition import  LatentDirichletAllocation\nfrom sklearn.model_selection import GridSearchCV\n\n# initializing the LDA object\n# online Method used to update _component much faster\n#0.5, 1.0] to guarantee asymptotic convergence for online method\nLDA = LatentDirichletAllocation(max_iter=10, learning_method='online', learning_offset=50,random_state=0,batch_size=200,n_jobs=-1)\n\n# Define Search Param\nparameters = {\"n_components\": [7, 6, 9, 13, 16],\"learning_decay\": [0.5,0.7,0.9]}\n\n\n# initializing gridsearchcv\ngrid_cv = GridSearchCV(LDA, param_grid=parameters)\"\"\"","21d98ccd":"# in a previus exploratory study we use the GridSearchCV to tunning the parameters as in the cell above, An we get the best reults for a learning_decay of 0.7 and 7 topics.\n","6a33a0b9":"LDA = LatentDirichletAllocation(n_components=7, max_iter=10, learning_method='online', learning_offset=50,random_state=0,batch_size=200,n_jobs=-1,learning_decay=0.7)","26e10232":"LDA_topics=LDA.fit(bow_raw_count)","7f5778c4":"\n# Perplexity\nprint(\"Model Perplexity: \", LDA_topics.perplexity(bow_raw_count))\n\n# Best score\nprint(\"Best Score: \", LDA_topics.score(bow_raw_count))","63caf877":"\ndef display_topics(model, feature_label, no_top_words):\n\n    for id, topic in enumerate(model.components_):\n        print( \"Topic:\", (id))# print fisrt topic label\n        print(\" \".join([feature_label[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))# get sorted higher frecuency terms","6321c79c":"\nlabels=CountVectorizer_bow.get_feature_names()","bc6eb340":"display_topics(LDA_topics, labels, 10)#","70116ae9":"# getting the topic and term performance for LDA\n\npyLDAvis.enable_notebook()\npanel = pyLDAvis.sklearn.prepare(LDA_topics, bow_raw_count, CountVectorizer_bow, mds='tsne',n_jobs=-1)\npanel","384609db":"topic_values = LDA_topics.transform(bow_raw_count)\nabstracts_en['topic_LDA'] = topic_values.argmax(axis=1)\n","03d399aa":"abstracts_en.head(2)","ae543ea0":"document_by_topics_LDA=abstracts_en[\"topic_LDA\"].value_counts()","6335b865":"document_by_topics_LDA_df=pd. DataFrame(document_by_topics_LDA).reset_index()\ndocument_by_topics_LDA_df.columns=[\"topic_LDA\",\"number_documents\"]","98e986eb":"document_by_topics_LDA_df","93a6d289":"\nfig=plt.figure(figsize=(7,7))\n\n\nplt.barh(document_by_topics_LDA_df[\"topic_LDA\"],document_by_topics_LDA_df[\"number_documents\"])\nplt.ylabel(\"topic_LDA\")\nplt.xlabel(\"number_documents\")\nplt.title(\"Number of document by topic selected using LDA\")\n\nplt.show()\n","3cf458b7":"Plotting to see in the algorith performance","e43f7ef8":"Looking to the areas of the circles we can infer that the marginal topic distribution in the corpus is very similar for each topic, although topic 7 and 6 are smaller. Aslo we can see this seven topics have not overlapping between them(are relative far),there are distributed  into different two-dimensional planes.","91bccd4f":"*Terms that are tunique and we will add to the prev useful for interpreting each topic.*\n\nWe will take in account corpus-wide frequency of a given term as well as the topic-specific frequency of the term\n\n- Topic 1: SAR-Cov structure and mechanism of infections\n- Topic 2: Infection in animals(cat, bat) and atirretrovirals(INF)\n- Topic 3: not well define\n- Topic 4: respiratory infections in group of age (child)\n- Topic 5: Covid outbreack\n- Topic 6:detection and diagnostics methods\n- topic 7: drug design and general terms\n\nLook like there are mixed topics we propouse perform LDA with at least 10 topics, to see the term frequency behavior and see if there is higher match with the chalenge topics\n","4a070628":"### Clean Data","f68126f5":"The aproach that are going to follow is try to find topic in the documents collections and see how much those match with the aspects that are asked there. For that we are plane to use Latent Dirichlet Allocation(LDA), method reported at literature for Topic modeling by Kavita Ganesan,Priya Dwivedi, Aneesha Bakharia for just cite some author.\nWe will use that method implemented in sklearn.","9c879fe5":"Looking at the most reperesnet terms in each topic we can see some topic that seem have a well define theme:\nAbstracts included in **Topic 0** detection method.\n\n**Topic 1** not well define theme(general)\n\n**Topic 2** not well define theme(general)\n\n**Topic 3** is tolking include abstract that are talking about structure and virus infection mechanisms\n\n**Topic 4** clinical studies by age groups, interesting term child\n\n**Topic 5** vaccine and interesting term but other are generals\n\n**Topic 6** more specific for Cov-19 outbreack\n\nAny way those are early conclusions because we need to see not just in the more frecuent terms but also which word are specific for each topic. To take this in account we move to see the graph generate by pyLDAvis library","f89417d3":"*General terms detected and Higher frecuency Specific terms overall corpus:* \n\nWe can see from the bar chart above that many words like virus that are  frequent in the corpus  are present in the overall term frecuency even when we have select **max_df as 0.80**, maybe we could run gain the feature extraccion algorith **tunning max_df** with different values. Another strategy here would be build **our own stopwords dictionary**, because we are analysing text for an especific field(but in that case we need some  expertician in this domain)\n\nNote:We have convert the capital words onto lower case, then those we can found some acronym for deseases or drugs that can appear in lower case and can be a little confuse.\n\nTerm **Child** is interesting because could mean that we have studies dedicated to early age population in this topic. Mostly represented in topic 5\nSome animal also appear as pre dominants terms\n\n","064f1199":"### Getting words by topics","a640112f":"### Raw  Bag_of_words","31fe5390":"As result of this analysis we can show some generals topics found in the collected studies. A next iteration selecting a higher ammount of topic is nescesary to go in more specific topic as vaccines, drug design and detection method. Incluid stopwords typical of the field also coul be help to get better cluster or use lower max_df. Also incluid in the study those paper in  differents languajes than english(we have removed at the beggining) can apport valuable information","c54a52f0":"Since abtracts relate  the main results fields where those are contributed. We propose made a clustering to see around wich goals are centered those groups of studies.","a087fa7c":"Some of the task asked in the challege consists of knowing how much you know about certain topics according to the reported studies. Here we show that about which aspects are asked.\n\nWhat is known about **transmission**, **incubation**, and **environmental stability**?\n\nWhat do we know about COVID-19 **risk factors**?\n\nWhat do we know about virus **genetics**, **origin**, and **evolution**?\n\nWhat do we know about **vaccines** and **therapeutics**?\n\nWhat has been published about **medical care**?\n\nWhat do we know about **non-pharmaceutical interventions**?\n\nWhat do we know about **diagnostics** and **surveillance**?\n\nWhat has been published about **ethical and social science considerations**?\n\nWhat has been published about **information sharing** and **inter-sectoral collaboration**?","a0f56b54":"How many abstract we have by the topic we have found?"}}