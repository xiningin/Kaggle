{"cell_type":{"bbfa85f7":"code","baac7ab4":"code","87401cca":"code","964b5042":"code","7de2c0a3":"code","945ded36":"code","3a09127c":"code","5b89f214":"code","424af639":"code","9c424167":"code","ab8c0b7a":"code","8a2fc4a7":"code","de4783f6":"code","d84f29fd":"code","5ada244d":"code","03efa0af":"code","78decd5c":"code","6e562cde":"code","642e13ff":"code","1f1f4799":"code","fec640c6":"code","e5071da0":"code","ee368830":"code","4d692484":"code","9942c7fd":"code","8e0f365b":"markdown","1de217ab":"markdown","5cff4897":"markdown","a4065c7b":"markdown","17762cdc":"markdown","21c5bf62":"markdown","6f8726e7":"markdown","9a8c344f":"markdown","30b42b40":"markdown","b215457d":"markdown","53642d00":"markdown","ec13190f":"markdown","79ccecdc":"markdown","7e8c1e95":"markdown","8a71d84f":"markdown","dbb2f296":"markdown","c93f2f0a":"markdown","840ecd9d":"markdown","550bab1b":"markdown","913ef3f5":"markdown","d6717a36":"markdown","a70896fe":"markdown","060dffff":"markdown","c6eb3cf3":"markdown","ac57e7e1":"markdown","058af0e6":"markdown","b3ada513":"markdown","0eaf1753":"markdown","68b49d55":"markdown","d7990fdc":"markdown"},"source":{"bbfa85f7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","baac7ab4":"df = pd.read_csv('..\/input\/iris\/Iris.csv')\ndf.head()","87401cca":"df = df.drop('Id', axis=1)\ndf.head()","964b5042":"df.isnull().sum()","7de2c0a3":"df.info()","945ded36":"#How many kinds of data are in column y, and how many of each kind? let's see that.\n\ndf[\"Species\"].value_counts()","3a09127c":"# Let's show what we see above with a bar graph.\n\nplt.figure(figsize = (8,6),facecolor='#9DF08E')\nplt.bar(df['Species'].unique(), df['Species'].value_counts(), color ='b')\nplt.title('Iris Kinds')\nplt.xlabel('Names')\nplt.ylabel('Numbers')\nplt.show()","5b89f214":"sns.pairplot(data=df, hue=\"Species\")\nplt.show()","424af639":"df.describe()","9c424167":"df.corr()","ab8c0b7a":"cor = df.corr()\nplt.figure(figsize=(12,8))\nsns.heatmap(data = cor, annot = True, cmap = plt.cm.YlGn)\nplt.show()","8a2fc4a7":"# x and y assignment\n\nx = df.iloc[:,0:4]\ny = df.iloc[:,4:]","de4783f6":"#Let's see how x and y look\n\nx.head(3)","d84f29fd":"y.head(3)","5ada244d":"# # Splitting the Data Set into Independent Variables and Dependent Variables\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.33, random_state=8)","03efa0af":"from sklearn.linear_model import LogisticRegression\n\nlogr = LogisticRegression(random_state=8)\nlogr.fit(x_train,y_train)\n\ny_pred = logr.predict(x_test)\n\n\n# confusion matrix and accuracy score\naccuracy_score_logr = accuracy_score(y_test,y_pred,3)\ncm_logr = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_logr, annot=True,fmt=\".0f\",linewidths=3,square=True, cmap='Blues',color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title(f'accuracy score: {accuracy_score_logr}',size=12,color='red')\nplt.show()","78decd5c":"\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=10, metric = 'minkowski')\nknn.fit(x_train,y_train)\n\ny_pred = knn.predict(x_test)\n\n\n# confusion matrix and accuracy score\naccuracy_score_knn = accuracy_score(y_test,y_pred,3)\ncm_knn = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_knn, annot=True,fmt=\".0f\",linewidths=3,square=True, cmap='Blues',color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title(f'accuracy score: {accuracy_score_knn}',size=12,color='red')\nplt.show()","6e562cde":"from sklearn.svm import SVC\n\nsvc = SVC(kernel = 'rbf', probability=True)\nsvc.fit(x_train,y_train)\n\ny_pred = svc.predict(x_test)\n\n\n# confusion matrix and accuracy score\naccuracy_score_svc = accuracy_score(y_test,y_pred,3)\ncm_svc = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_svc, annot=True,fmt=\".0f\",linewidths=3,square=True, cmap='Blues',color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title(f'accuracy score: {accuracy_score_svc}',size=12,color='red')\nplt.show()","642e13ff":"from sklearn.naive_bayes import MultinomialNB\n\nmnb = MultinomialNB()\nmnb.fit(x_train,y_train)\n\ny_pred = mnb.predict(x_test)\n\n# confusion matrix and accuracy score\naccuracy_score_mnb = accuracy_score(y_test,y_pred,3)\ncm_mnb = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_mnb, annot=True,fmt=\".0f\",linewidths=3,square=True, cmap='Blues',color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title(f'accuracy score: {accuracy_score_mnb}',size=12,color='red')\nplt.show()","1f1f4799":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(criterion = 'entropy')\ndtc.fit(x_train,y_train)\n\ny_pred = dtc.predict(x_test)\n\n\n# confusion matrix and accuracy score\naccuracy_score_dtc = accuracy_score(y_test,y_pred,3)\ncm_dtc = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_dtc, annot=True,fmt=\".0f\",linewidths=3,square=True, cmap='Blues',color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title(f'accuracy score: {accuracy_score_dtc}',size=12,color='red')\nplt.show()","fec640c6":" from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators = 9, criterion = 'entropy')\nrfc.fit(x_train,y_train)\n\ny_pred = rfc.predict(x_test)\n\n\n# confusion matrix and accuracy score\naccuracy_score_rfc = accuracy_score(y_test,y_pred,3)\ncm_rfc = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_rfc, annot=True,fmt=\".0f\",linewidths=3,square=True, cmap='Blues',color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title(f'accuracy score: {accuracy_score_rfc}',size=12,color='red')\nplt.show()","e5071da0":"print('\\033[93m' + '--CONFUS\u0130ON MATR\u0130CES --\\n' + '\\033[0m')\n\nprint(f'Logistic Regression          \\n{cm_logr}\\n')\nprint(f'KNN-K Nearest Neighbors      \\n{cm_knn}\\n')\nprint(f'SVC-Support Vector Classifier\\n{cm_svc}\\n')\nprint(f'Multinomial Naive Bayes      \\n{cm_mnb}\\n')\nprint(f'Decision Tree Classifier     \\n{cm_dtc}\\n')\nprint(f'Random Forest Classifier     \\n{cm_rfc}\\n')","ee368830":"# Let's create a data frame and show scores.\n\nNamesOfAlgorithms_df = pd.DataFrame(['Logistic Regression','KNN - K Nearest Neighbors','SVC - Support Vector Classifier','Multinomial Naive Bayes','Decision Tree Classifier','Random Forest Classifier'])\nAcScoresOfAlgorithms_df = pd.DataFrame([accuracy_score_logr,accuracy_score_knn,accuracy_score_svc,accuracy_score_mnb,accuracy_score_dtc,accuracy_score_rfc])\n\ndf3 = pd.concat([NamesOfAlgorithms_df,AcScoresOfAlgorithms_df],axis=1)\ndf3.columns=['ALGOR\u0130THM','ACCURACY SCORE',]\n\nprint(df3)","4d692484":"# To better understand the Accuracy score, let's calculate it manually.\n# Accuracy Score = (TP + TN) \/ (TP + TN + FN + FP)  \n#                = (Sum of Main Diagonal Values) \/ (Sum of All Matrix Values)\n\nprint('\\033[1m' + 'Number of predictions of all algorithms: 50\\n' + '\\033[0m')\n\ntruePredictsNumber_logr = cm_logr[0][0] + cm_logr[1][1] + cm_logr[2][2]\nprint('\\033[4m' + 'Logistic Regression' + '\\033[0m')\nprint(f'True Predicts: {truePredictsNumber_logr}')\nprint(f'True Predicts Ratio: {truePredictsNumber_logr\/50:.2f} \\n')\n\ntruePredictsNumber_knn = cm_knn[0][0] + cm_knn[1][1] + cm_knn[2][2]\nprint('\\033[4m' + 'KNN' + '\\033[0m')\nprint(f'True Predicts: {truePredictsNumber_knn}')\nprint(f'True Predicts Ratio: {truePredictsNumber_knn\/50:.2f} \\n')\n\ntruePredictsNumber_svc = cm_svc[0][0] + cm_svc[1][1] + cm_svc[2][2]\nprint('\\033[4m' + 'SVC' + '\\033[0m')\nprint(f'True Predicts: {truePredictsNumber_svc}')\nprint(f'True Predicts Ratio: {truePredictsNumber_svc\/50:.2f} \\n')\n\ntruePredictsNumber_mnb = cm_mnb[0][0] + cm_mnb[1][1] + cm_mnb[2][2]\nprint('\\033[4m' + 'Naive Bayes' + '\\033[0m')\nprint(f'True Predicts: {truePredictsNumber_mnb}')\nprint(f'True Predicts Ratio: {truePredictsNumber_mnb\/50:.2f} \\n')\n\ntruePredictsNumber_dtc = cm_dtc[0][0] + cm_dtc[1][1] + cm_dtc[2][2]\nprint('\\033[4m' + 'Decision Tree' + '\\033[0m')\nprint(f'True Predicts: {truePredictsNumber_dtc}')\nprint(f'True Predicts Ratio: {truePredictsNumber_dtc\/50:.2f} \\n')\n\ntruePredictsNumber_rfc = cm_rfc[0][0] + cm_rfc[1][1] + cm_rfc[2][2]\nprint('\\033[4m' + 'Random Forest' + '\\033[0m')\nprint(f'True Predicts: {truePredictsNumber_rfc}')\nprint(f'True Predicts Ratio: {truePredictsNumber_rfc\/50:.2f}')","9942c7fd":"# Let's compare the accuracy scores on the bar graph.\n\nalgorithms_names = np.array(['Logistic Reg.','KNN','SVC','Naive Bayes','Decision Tree','Random Forest'])\naccuracy_scores = np.array([accuracy_score_logr,accuracy_score_knn,accuracy_score_svc,accuracy_score_mnb,accuracy_score_dtc,accuracy_score_rfc])\naccuracy_scores = accuracy_scores*100\n\nplt.figure(figsize = (12,8),facecolor='#9DF08E')\ncolors_set = ['green','blue','purple','brown','teal','orange']\nplt.bar(x=algorithms_names, height=accuracy_scores, color = colors_set)\nplt.title('Accuracy Score Comparision\\n', fontsize=30, color = 'red')\nplt.xlabel('\\nAlgorithms', fontsize=20)\nplt.ylabel('Accuracy Score %', fontsize=20)\nplt.xticks(horizontalalignment='center', fontsize='15')\nplt.yticks(np.arange(0,101,5),fontsize='15')\nplt.tight_layout()\nplt.show()","8e0f365b":"KNN has the highest accuracy score.","1de217ab":"# <a id=\"12\"><\/a>\n<h1 style=\"background-color:#35F237;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Conclusion<\/h1>","5cff4897":"Overall, the data distribution is very clean.","a4065c7b":"<font color = '#F08841'>\n    Content:\n    \n1. [Loading and Checking Data](#1)\n1. [Creating Models](#2)\n    *          [Logistic Regression ](#3)\n    *          [KNN -K Nearest Neighbors ](#4)\n    *          [SVC -Support Vector Classifier ](#5)\n    *          [Naive Bayes  ](#6)\n    *          [Decision Tree Classifier ](#7)\n    *          [Random Forest Classifier ](#8)\n1. [Evaluating Models](#9)\n    *          [Confusion Matrix ](#10)\n    *          [Accuracy Score ](#11)\n1. [Conclusion](#12)","17762cdc":"Why is Multinomial Naive Bayes more suitable for this data set?\n\nIf the column to be predicted is binomial, ie binary, ie 1 or 0, 'Bernoulli Naive Bayes' is used. I did not use 'Bernoulli' as the y column in this dataset is not binomial. (Similar to not using sigmoid in the svr just before)\n\nIf the column to be predicted consists of contunious numbers, that is, if it consists of real numbers, 'Gaussian Naive Bayes' is used. However, in this dataset, column y consists of strings, not real numbers. That's why I didn't use 'Gaussian Naive Bayes'.\n\nIf the column to be predicted consists of nominal data, 'Multinomial Naive Bayes' is used. In suitable with this kind of Naive Bayes, column y in this data set consists of nominal data. That's why I used 'Multinomial Naive Bayes'.\n","21c5bf62":"There is an 'ID' column, I will drop it right away, because the 'ID' column is meaningless for machine learning, it does not affect what the y column is and negatively affects the performance of our models.","6f8726e7":"I didn't use 'sigmoid' as Kernel Function because 'sigmoid' returns 1 or 0 so it makes sense to use sigmoid when column y has 2 kinds whereas column y has 3 kinds of data Among the remaining possibilities, 'poly' and 'rbf' made sense, I tried them and 'rbf' gave better performance, therefore I chose 'rbf'.","9a8c344f":"The highest correlation is between Petal Length Cm and Petal WidthCm with a value of 0.96.\n\nAlso, the correlation between PetalLengthCm - SepalWidthCm is -0.42, meaning that as one increases, the other decreases, and vice versa.","30b42b40":"They are as they should be, nice.","b215457d":"Notice that the values we found using the 'accuracy_score' in the 'sklearn' library are the same as the values we found manually.","53642d00":"# <a id=\"7\"><\/a>\n<h1 style=\"background-color:#35F237;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Decision Tree<\/h1>","ec13190f":"# <a id=\"6\"><\/a>\n<h1 style=\"background-color:#35F237;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Naive Bayes<\/h1>","79ccecdc":"There are 3 kinds of species, and each kind is 50 pcs.","7e8c1e95":"# <a id=\"11\"><\/a>\n<h1 style=\"background-color:#35F237;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Accuracy Scores<\/h1>","8a71d84f":" <h1 style=\"background-color:#10DEFF\n;font-family:Comic Sans MS;font-size:225%;text-align:center;border-radius: 15px 50px;\"> Thank you for reading\ud83d\ude03 If you like it, please upvote\ud83d\udc4d  Please do not forget to write down your positive or negative opinions\ud83d\udcac<\/h1><a id=note><\/a>","dbb2f296":"# <a id=\"8\"><\/a>\n<h1 style=\"background-color:#35F237;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Random Forest<\/h1>","c93f2f0a":"# <a id=\"4\"><\/a>\n<h1 style=\"background-color:#35F237;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">KNN (K Neighbors Classifier)<\/h1>","840ecd9d":"Now it is better.","550bab1b":"# <a id=\"3\"><\/a>\n<h1 style=\"background-color:#35F237;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Logistic Regression<\/h1>","913ef3f5":"<a id=\"table\"><\/a>\n<h1 style=\"background-color:yellow;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\"> \ud83c\udf3a  Iris Classification  \ud83c\udf3a<\/h1>\n\n## ![](https:\/\/media.giphy.com\/media\/ew2jp85lQIAJjcn4FG\/giphy.gif)","d6717a36":"# <a id=\"1\"><\/a>\n<h1 style=\"background-color:#35F237;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Loading and Checking Data<\/h1>","a70896fe":"There are no null values, that's good.","060dffff":"df's type is 'Data Frame' as it should be and df has 150 columns. When the 'ID' column is dropped, all x columns will be of float type.","c6eb3cf3":"## <a id=\"Classification Algorithms\"><\/a>\n<h1 style=\"background-color:red;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">~ Classification Algorithms ~<\/h1>\n   ","ac57e7e1":"First of all, I would like to say that since the data set is balanced, I will use 'Accuracy Score', not 'F1 Score' when comparing models. If the data set was imbalanced I would use 'F1 Score'.","058af0e6":"# <a id=\"5\"><\/a>\n<h1 style=\"background-color:#35F237;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">SVC (Support Vector Classification - SVM Classifier)<\/h1>\n ","b3ada513":"# <a id=\"10\"><\/a>\n<h1 style=\"background-color:#35F237;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Confusion Matrix<\/h1>","0eaf1753":"# <a id=\"9\"><\/a>\n<h1 style=\"background-color:red;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Evaluating Models<\/h1>","68b49d55":"* The algorithm with the highest accuracy score in the classification of the Iris dataset was KNN.\n* Since the data set is very clean, the score of other algorithms other than KNN was also very high, maybe some of them could have achieved a slightly higher score, but this also affects the cost negatively. For example, in Random Forest, we can increase the number of n_estimators even more, maybe good results, but if our data set consisted of thousands of rows instead of 150 rows, this would cause the algorithm to work slowly.\n* The highest correlation is between Petal Length Cm and Petal WidthCm with a value of 0.96.\nAlso, the correlation between PetalLengthCm - SepalWidthCm is -0.42, meaning that as one increases, the other decreases, and vice versa.","d7990fdc":"# <a id=\"2\"><\/a>\n<h1 style=\"background-color:#35F237;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Creating Models<\/h1>"}}