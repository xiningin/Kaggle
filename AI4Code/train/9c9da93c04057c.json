{"cell_type":{"e95a6046":"code","8de87730":"code","247e3d0d":"code","2f477025":"code","3796270e":"code","520aa4f9":"code","d1ba9406":"code","9df0de76":"code","06503b48":"code","e0b1a039":"code","83a404bb":"code","4d49d967":"code","ba3466c5":"code","4be5fcc9":"code","47e39945":"code","4288465c":"code","37d86fbd":"code","55f0e830":"code","38ee842b":"code","92fa99da":"code","56c9b549":"code","8569c350":"code","ded8470f":"code","17245ff7":"code","d9b02be9":"code","55fd9abf":"code","c4996f2a":"code","81da6640":"code","69bd27e8":"code","311f54a4":"code","55ce5d6e":"markdown","63deac63":"markdown","00cf6260":"markdown","27be1e71":"markdown","b0a778fb":"markdown"},"source":{"e95a6046":"# Python standard libraries are loaded\nfrom __future__ import print_function\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Keras libraries loaded\nimport keras\nfrom keras import metrics\nfrom keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Activation\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\nfrom keras.utils import plot_model\nfrom keras.models import load_model","8de87730":"# The file is read in from a CSV file\nkc_data_org = pd.read_csv(\"..\/input\/kc_house_data.csv\")","247e3d0d":"# Describes dataframe features\nkc_data_org.info()","2f477025":"# Displays the first 10 rows of the kc_data_org dataframe.\nkc_data_org.head(10)","3796270e":"# Checks the kc_data_org for any null values.\nkc_data_org.isnull().sum().sum()","520aa4f9":"# Transform dates into year, month and day and select columns.\nkc_data_org['sale_yr'] = pd.to_numeric(kc_data_org.date.str.slice(0, 4))\nkc_data_org['sale_month'] = pd.to_numeric(kc_data_org.date.str.slice(4, 6))\nkc_data_org['sale_day'] = pd.to_numeric(kc_data_org.date.str.slice(6, 8))\n\nkc_data = pd.DataFrame(kc_data_org, columns=[\n        'sale_yr','sale_month','sale_day',\n        'bedrooms','bathrooms','sqft_living','sqft_lot','floors',\n        'condition','grade','sqft_above','sqft_basement','yr_built',\n        'zipcode','lat','long','sqft_living15','sqft_lot15','price'])\nlabel_col = 'price'\n","d1ba9406":"#Prints out statistical summary of features\nprint(kc_data.describe())","9df0de76":"# Distribution plots of the numerical features in the kc_data dataframe are visualized.\ndistributions = kc_data.select_dtypes([np.int, np.float])\nfor i, col in enumerate(distributions.columns):\n    plt.figure(i)\n    sns.distplot(distributions[col])","06503b48":"# Scatter plot of house price vs zipcode.\nplt.figure(figsize=(12,7))\nsns.scatterplot(kc_data['price'],kc_data['zipcode'])\nplt.title('Price VS Zipcode')\nplt.xlabel('Price')\nplt.ylabel('Zipcode')\nplt.show()","e0b1a039":"# Scatter plot of house price vs year house was built.\nplt.figure(figsize=(12,8))\nsns.scatterplot(kc_data['price'],kc_data['yr_built'])\nplt.title('Price VS Year Built')\nplt.xlabel('Price')\nplt.ylabel('Year Built')\nplt.show()","83a404bb":"# Scatter plot of year house was built vs the square feet of the lot.\nplt.figure(figsize=(12,7))\nsns.scatterplot(kc_data['yr_built'],kc_data['sqft_lot'])\nplt.title('Year Built VS Square Feet of Lot')\nplt.xlabel('Year Built')\nplt.ylabel('Square Feet of Lot')\nplt.show()","4d49d967":"# Scatter plot of price vs number of bedrooms in a home.\nplt.figure(figsize=(12,7))\nsns.scatterplot(kc_data['price'],kc_data['bedrooms'])\nplt.title('Price Vs Number of Bedrooms')\nplt.xlabel('Price')\nplt.ylabel('# of Bedrooms')\nplt.show()","ba3466c5":"# Boxplot of house condition distribution is shown. The higher the number the better the condition.\nsns.boxplot(kc_data['condition'])","4be5fcc9":"# Boxplot of house prices\nplt.figure(figsize=(12,5))\nsns.boxplot(kc_data['price'])","47e39945":"# Function to split a range of data frame & array indeces into three sub-ranges.\ndef train_validate_test_split(df, train_part=.6, validate_part=.2, test_part=.2, seed=None):\n    np.random.seed(seed)\n    total_size = train_part + validate_part + test_part\n    train_percent = train_part \/ total_size\n    validate_percent = validate_part \/ total_size\n    test_percent = test_part \/ total_size\n    perm = np.random.permutation(df.index)\n    m = len(df)\n    train_end = int(train_percent * m)\n    validate_end = int(validate_percent * m) + train_end\n    train = perm[:train_end]\n    validate = perm[train_end:validate_end]\n    test = perm[validate_end:]\n    return train, validate, test","4288465c":"# Split index ranges into three parts, however, ignore the third.\ntrain_size, valid_size, test_size = (70, 30, 0)\nkc_train, kc_valid, kc_test = train_validate_test_split(kc_data, \n                              train_part=train_size, \n                              validate_part=valid_size,\n                              test_part=test_size,\n                              seed=2017)","37d86fbd":"# Extract data for training and validation into x and y vectors.\nkc_y_train = kc_data.loc[kc_train, [label_col]]\nkc_x_train = kc_data.loc[kc_train, :].drop(label_col, axis=1)\nkc_y_valid = kc_data.loc[kc_valid, [label_col]]\nkc_x_valid = kc_data.loc[kc_valid, :].drop(label_col, axis=1)\n\nprint('Size of training set: ', len(kc_x_train))\nprint('Size of validation set: ', len(kc_x_valid))\nprint('Size of test set: ', len(kc_test), '(not converted)')","55f0e830":"# Function to get statistics about a data frame.\ndef norm_stats(df1, df2):\n    dfs = df1.append(df2)\n    minimum = np.min(dfs)\n    maximum = np.max(dfs)\n    mu = np.mean(dfs)\n    sigma = np.std(dfs)\n    return (minimum, maximum, mu, sigma)","38ee842b":"# Function to Z-normalise the entire data frame - note stats for Z transform passed in.\ndef z_score(col, stats):\n    m, M, mu, s = stats\n    df = pd.DataFrame()\n    for c in col.columns:\n        df[c] = (col[c]-mu[c])\/s[c]\n    return df","92fa99da":"# Normalize training and validation predictors using the stats from training data only\n# (to ensure the same transformation applies to both training and validation data),\n# and then convert them into numpy arrays to be used by Keras.\nstats = norm_stats(kc_x_train, kc_x_valid)\narr_x_train = np.array(z_score(kc_x_train, stats))\narr_y_train = np.array(kc_y_train)\narr_x_valid = np.array(z_score(kc_x_valid, stats))\narr_y_valid = np.array(kc_y_valid)\n\nprint('Training shape:', arr_x_train.shape)\nprint('Training samples: ', arr_x_train.shape[0])\nprint('Validation samples: ', arr_x_valid.shape[0])\n","56c9b549":"# Three functions to define alternative Keras models\n#The first is very simple, consisting of three layers and Adam optimizer\n\ndef basic_model_1(x_size, y_size):\n    t_model = Sequential()\n    t_model.add(Dense(100, activation=\"tanh\", input_shape=(x_size,)))\n    t_model.add(Dense(50, activation=\"relu\"))\n    t_model.add(Dense(y_size))\n    print(t_model.summary())\n    t_model.compile(loss='mean_squared_error',\n        optimizer=Adam(),\n        metrics=[metrics.mae])\n    return(t_model)","8569c350":"# The second with Adam optimizer consists of 4 layers and the first uses 10% dropouts.\ndef basic_model_2(x_size, y_size):\n    t_model = Sequential()\n    t_model.add(Dense(100, activation=\"tanh\", input_shape=(x_size,)))\n    t_model.add(Dropout(0.1))\n    t_model.add(Dense(50, activation=\"relu\"))\n    t_model.add(Dense(20, activation=\"relu\"))\n    t_model.add(Dense(y_size))\n    print(t_model.summary())\n    t_model.compile(loss='mean_squared_error',\n        optimizer=Adam(),\n        metrics=[metrics.mae])\n    return(t_model)","ded8470f":"# The third is the most complex, it extends the previous model with Nadam optimizer, dropouts and L1\/L2 regularisers.\ndef basic_model_3(x_size, y_size):\n    t_model = Sequential()\n    t_model.add(Dense(80, activation=\"tanh\", kernel_initializer='normal', input_shape=(x_size,)))\n    t_model.add(Dropout(0.2))\n    t_model.add(Dense(120, activation=\"relu\", kernel_initializer='normal', \n        kernel_regularizer=regularizers.l1(0.01), bias_regularizer=regularizers.l1(0.01)))\n    t_model.add(Dropout(0.1))\n    t_model.add(Dense(20, activation=\"relu\", kernel_initializer='normal', \n        kernel_regularizer=regularizers.l1_l2(0.01), bias_regularizer=regularizers.l1_l2(0.01)))\n    t_model.add(Dropout(0.1))\n    t_model.add(Dense(10, activation=\"relu\", kernel_initializer='normal'))\n    t_model.add(Dropout(0.0))\n    t_model.add(Dense(y_size))\n    t_model.compile(\n        loss='mean_squared_error',\n        optimizer='nadam',\n        metrics=[metrics.mae])\n    return(t_model)","17245ff7":"# Now we create the model - The code below will run basic_model_3\nmodel = basic_model_3(arr_x_train.shape[1], arr_y_train.shape[1])\nmodel.summary()","d9b02be9":"# Define how many epochs of training should be done and what is the batch size\nepochs = 500\nbatch_size = 128\n\nprint('Epochs: ', epochs)\nprint('Batch size: ', batch_size)","55fd9abf":"# Specify Keras callbacks which allow additional functionality while the model is being fitted\n# ModelCheckpoint allows to save the models as they are being built or improved\n# TensorBoard interacts with TensorFlow interactive reporting system\n# EarlyStopping watches one of the model measurements and stops fitting when no improvement\n\nkeras_callbacks = [\n    # ModelCheckpoint('\/tmp\/keras_checkpoints\/model.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', save_best_only=True, verbose=2)\n    # ModelCheckpoint('\/tmp\/keras_checkpoints\/model.{epoch:02d}.hdf5', monitor='val_loss', save_best_only=True, verbose=0)\n    # TensorBoard(log_dir='\/tmp\/keras_logs\/model_3', histogram_freq=0, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None),\n    EarlyStopping(monitor='val_mean_absolute_error', patience=20, verbose=0)\n]","c4996f2a":"# Fit the model and record the history of training and validation.\n# As we specified EarlyStopping with patience=20, with luck the training will stop in less than 200 epochs.\n# Be patient, the fitting process takes time, use verbose=2 for visual feedback\nhistory = model.fit(arr_x_train, arr_y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    shuffle=True,\n    verbose=0, # Change it to 2, if wished to observe execution\n    validation_data=(arr_x_valid, arr_y_valid),\n    callbacks=keras_callbacks)","81da6640":"train_score = model.evaluate(arr_x_train, arr_y_train, verbose=0)\nvalid_score = model.evaluate(arr_x_valid, arr_y_valid, verbose=0)\n\nprint('Train MAE: ', round(train_score[1], 4), ', Train Loss: ', round(train_score[0], 4)) \nprint('Val MAE: ', round(valid_score[1], 4), ', Val Loss: ', round(valid_score[0], 4))","69bd27e8":"# Using matplotlib to create a plot of performance history \ndef plot_hist(h, xsize=6, ysize=10):\n    # Prepare plotting\n    fig_size = plt.rcParams[\"figure.figsize\"]\n    plt.rcParams[\"figure.figsize\"] = [xsize, ysize]\n    fig, axes = plt.subplots(nrows=4, ncols=4, sharex=True)\n    \n    # summarize history for MAE\n    plt.subplot(211)\n    plt.plot(h['mean_absolute_error'])\n    plt.plot(h['val_mean_absolute_error'])\n    plt.title('Training vs Validation MAE')\n    plt.ylabel('MAE')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    \n    # summarize history for loss\n    plt.subplot(212)\n    plt.plot(h['loss'])\n    plt.plot(h['val_loss'])\n    plt.title('Training vs Validation Loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    \n    # Plot it all in IPython (non-interactive)\n    plt.draw()\n    plt.show()\n\n    return","311f54a4":"plot_hist(history.history, xsize=8, ysize=12)","55ce5d6e":"**Creating a Keras model**","63deac63":"****Data Exploration****","00cf6260":"**Case:** A real estate company that specializes in selling homes wanted to know the value of inventory on hand. Some of their homes haven't been appraised yet. Using data that contains attributes of homes throughout King County a predictive model was created to estimate the price of a home.","27be1e71":"**Evaluate and report performance of the trained model**\n","b0a778fb":"**Data Cleaning**"}}