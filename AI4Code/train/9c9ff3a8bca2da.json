{"cell_type":{"9b8800e6":"code","0b7406ef":"code","0d89321b":"code","80e9c7cf":"code","57150319":"code","9a115ae4":"code","392149c6":"code","fa42b41d":"code","79b7eaca":"code","4d3ca127":"code","b4f1fbf5":"code","7348cda4":"code","688c0dd6":"code","a51702f5":"code","a4d9fd9a":"code","25acce89":"code","db90e1c2":"code","49652448":"code","bf21ff82":"code","7d7ebfe2":"code","4af4daa2":"code","82b2ef3d":"code","8bd145ce":"code","af5a190b":"code","89698bf5":"code","f1732a75":"code","d7772d4e":"code","87f098b9":"code","0ecde6db":"code","22c1604b":"code","adc175e6":"code","975001a8":"code","1b70cb6a":"code","b4a61a82":"code","3c6c0b7e":"code","fdfec10f":"code","4ac61bcd":"code","662cc1b7":"markdown","fae50f49":"markdown","47f4ff24":"markdown","b43e315f":"markdown","f56f7d8a":"markdown","f1dd0fed":"markdown","75704c5a":"markdown","8fb1bc64":"markdown","8c64a88d":"markdown","f8eab083":"markdown","8956a57a":"markdown","5d657200":"markdown","587c79d3":"markdown","4af489e3":"markdown","bb829d9e":"markdown"},"source":{"9b8800e6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.api.types import CategoricalDtype\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import mutual_info_classif\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom category_encoders import MEstimateEncoder\nimport random\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0b7406ef":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","0d89321b":"df = pd.concat([train_data,test_data])\nprint('Pclass',df.Pclass.unique())\nprint('Sex',df.Sex.unique())\nprint('SibSp',df.SibSp.unique())\nprint('Parch',df.Parch.unique())\nprint('Ticket',df.Ticket.unique())\nprint('Name', df.Name.unique())\nprint('Fare',df.Fare.unique())\nprint('Cabin',df.Cabin.unique())\nprint('Embarked',df.Embarked.unique())\nprint('Age',df.Age.unique())\n# display(df.info())\n\n# # print(sorted(cabin, key= lambda x: x[0]))","80e9c7cf":"def clean(df):\n    #no cleaning\n    return df","57150319":"df.dtypes","9a115ae4":"\nfeatures_nom = [\"Sex\",\"Embarked\", \"Cabin\"]  #list down unordered categorical features\n\nordered_levels = {             # list down un ordered categorical coloumn and assign categories\n    \"Pclass\": [1,2,3]\n}\n\n# Add a None level for missing values\nordered_levels = {key: [\"None\"] + value for key, value in ordered_levels.items()}\n\n\ndef encode(df):\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories and df[name].isnull().sum() !=0:\n            df[name].cat.add_categories(\"None\", inplace=True)\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,\n                                                    ordered=True))\n    return df\n","392149c6":"def impute(df):\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes(\"category\"):\n        if \"None\" in df[name].cat.categories:\n            df[name] = df[name].fillna(\"None\")\n    for name in df.select_dtypes(\"object\"):\n        df[name] = df[name].fillna(\"None\")\n    return df\ndef place_NaN_by_mean(d_train,d_test,feature):\n    d_train_mean = d_train[feature].mean()\n    d_test_std = d_test[feature].std()\n    d_train[feature] = d_train[feature].replace(np.NaN, np.random.randint(d_train_mean - d_test_std, d_train_mean + d_test_std))\n    d_test[feature] = d_test[feature].replace(np.NaN, d_train_mean)\n    df = pd.concat([d_train, d_test])\n    return df","fa42b41d":"def load_data():\n    # Read data\n    df_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n    df_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n    df_train=df_train.set_index([pd.Index(range(891))])\n    df_test=df_test.set_index([pd.Index(range(891,1309))])\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing\n    df = place_NaN_by_mean(df_train, df_test, \"Age\")\n    df = place_NaN_by_mean(df_train, df_test, \"Fare\")\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test","79b7eaca":"d_train, d_test = load_data()\nd_train[\"Survived\"] = d_train.Survived.astype(\"category\")","4d3ca127":"d_train.head()","b4f1fbf5":"features = [\"Pclass\", \"Sex\",\"Age\", \"SibSp\", \"Parch\", \"Fare\",\"Cabin\",\"Embarked\"]","7348cda4":"def score_dataset(X_train, y, model= RandomForestClassifier(criterion='gini', \n                             n_estimators=700,\n                             min_samples_split=10,\n                             min_samples_leaf=1,\n                             max_features='auto',\n                             oob_score=True,\n                             random_state=1,\n                             n_jobs=-1)):\n\n    for colname in X_train.select_dtypes([\"category\", \"object\"]):\n        X_dummies = pd.get_dummies(X_train[colname], drop_first=False,prefix = colname)\n        X_train = X_train.join(X_dummies)\n        X_train = X_train.drop(colname,1)\n\n    score = cross_val_score(\n        model, X_train, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","688c0dd6":"X = d_train[features].copy()\ny = d_train.Survived.copy()\n# print(X,y)\nbaseline_score = score_dataset(X.copy(), y.copy())\nprint(f\"Baseline score: {baseline_score:.5f} RMSLE\")","a51702f5":"def make_mi_scores(X, y, is_one_hot_encode):\n    X = X.copy()\n    if is_one_hot_encode:\n        for colname in X.select_dtypes([\"category\", \"object\"]):\n            X_dummies = pd.get_dummies(X[colname], drop_first=False,prefix = colname)\n            X = X.join(X_dummies)\n            X = X.drop(colname,1)\n    else:\n        for colname in X.select_dtypes([\"object\", \"category\"]):\n            X[colname], _ = X[colname].factorize()\n\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_classif(X, y, discrete_features='auto', random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","a4d9fd9a":"X = d_train.copy()\ny = X.pop(\"Survived\")\nmi_scores = make_mi_scores(X.copy(), y, False)\nprint(mi_scores)","25acce89":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","db90e1c2":"sns.countplot(d_train['Survived'])","49652448":"sns.countplot(d_train['Pclass'], hue=d_train['Survived'])","bf21ff82":"sns.countplot(d_train['Sex'], hue=d_train['Survived'])","7d7ebfe2":"sns.countplot(d_train['Embarked'], hue=d_train['Survived'])","4af4daa2":"def name_breakdown(X_train_test,features):\n    X_new = pd.DataFrame()\n    X_new[\"Title\"] = X_train_test[\"Name\"].str.split(\", \", n=1, expand=True)[1]\n    X_new[\"Title\"] = X_new[\"Title\"].copy().str.split(\".\", n=1, expand=True)[0]\n\n    X = X_train_test[features].copy()\n    X['Title'] = X_new\n    X['Title'] = X['Title'].astype(\"category\")\n    return X\ndef name_len_feature(X_train_test,data):\n    X_new = pd.DataFrame()\n    X_new[\"Name_len\"] = data[\"Name\"].apply(lambda x: len(x))\n\n#     X = X_train[features].copy()\n    X_train_test['Name_len'] = X_new\n    return X_train_test\ndef ticket_first_litter(X_train_test,data):\n    X_new = pd.DataFrame()\n    X_new[\"Ticket_letter\"] = data[\"Ticket\"].apply(lambda x: str(x)[0])\n\n#     X = X_train[features].copy()\n    X_train_test['Ticket_letter'] = X_new\n    X_train_test['Ticket_letter'] = X_train_test['Ticket_letter'].astype(\"category\")\n    return X_train_test\ndef Cabin_break_down(X_train_test):\n    X_new = pd.DataFrame()\n#     X = X_train[features].copy()\n    X_train_test[\"Cabin_break_down\"] = np.where(X_train_test.Cabin != \"None\", X_train_test[\"Cabin\"].astype(str).str[0],\"None\")\n    X_train_test['Cabin_break_down'] = X_train_test['Cabin_break_down'].astype(\"category\")\n    return X_train_test\ndef age_binning(X_train_test):\n    X_new = pd.DataFrame()\n    cut_list = [0,4,8,12,18,30,40,55,65,80]\n    X_new['AgeBin']=pd.cut(X_train_test['Age'], cut_list, labels=False)\n#     X = X_train[features].copy()\n    X_train_test['AgeBin'] = X_new\n    X_train_test['AgeBin'] = X_train_test['AgeBin'].astype(\"category\")\n    return X_train_test\ndef fare_binning(X_train_test):\n    X_new = pd.DataFrame()\n    cut_list = list(range(0,100,10))\n    cut_list.extend(list(range(100,700,100)))\n    X_new['FareBin']=pd.cut(X_train_test['Fare'],cut_list,labels=False,right=False)\n#     X = X_train[features].copy()\n    X_train_test['FareBin'] = X_new\n    X_train_test['FareBin'] = X_train_test['FareBin'].astype(\"category\")\n    return X_train_test\n\ndef embarked_fare_group(X_train_test):\n    X_new = pd.DataFrame()\n    X_new[\"FareEmbarked\"] = X_train_test.groupby(\"Embarked\")[\"Fare\"].transform(\"median\")\n\n#     X = X_train[features].copy()\n    X_train_test[\"FareEmbarked\"] = X_new\n    X_train_test['FareEmbarked'] = X_train_test['FareEmbarked'].astype(\"category\")\n    return X_train_test    ","82b2ef3d":"def cluster_labels(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n#     print(X_scaled)\n    X_new = X.loc[:, features]\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    return X_new, kmeans\n\n\ndef cluster_distance(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features and join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    return X_cd","8bd145ce":"def cluster_from_Age_fare(X_train_test):\n    cluster_features = [\"Age\", \"Fare\"]\n    X_clustered, kmeans_feature_engineering = cluster_labels(X_train_test.copy(), cluster_features,10)\n    print(X_clustered.head())\n    sns.relplot(\n        x=\"Age\", y=\"Fare\", hue=\"Cluster\", data=X_clustered, height=5\n    );\n#     X_clustered_data = X_train.copy()\n    X_train_test['Cluster'] = X_clustered.Cluster\n    X_train_test['Cluster'] = X_train_test['Cluster'].astype(\"category\")\n    return X_train_test\n# print(X_modified_7)\n# X_modified_8 = cluster_from_Age_fare(X_modified_6.copy())","af5a190b":"\ndef apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n","89698bf5":"features_for_PCA = [\n    \"Age\",\n    \"Fare\",\n    \"SibSp\",\n    \"Parch\",\n    \n]\nprint(\"Correlation with Survived:\\n\")\n# print(X_modified[features_for_PCA].corrwith(X_modified.Survived))","f1732a75":"pca, X_pca, loadings= apply_pca(d_train[features_for_PCA])\nprint(loadings)\n# Look at explained variance\nplot_variance(pca);","d7772d4e":"def SibSp_and_Parch(X_train_test):\n    X_train_test[\"SibSp_Parch\"] = X_train_test.SibSp + X_train_test.Parch\n    return X_train_test\n# X_modified_9 = SibSp_and_Parch(X_modified_8.copy())","87f098b9":"def model_run(X_train_test,d_train,d_test,y,model= RandomForestClassifier(criterion='gini', \n                             n_estimators=700,\n                             min_samples_split=10,\n                             min_samples_leaf=1,\n                             max_features='auto',\n                             oob_score=True,\n                             random_state=1,\n                             n_jobs=-1)):\n    for colname in X_train_test.select_dtypes([\"category\", \"object\"]):\n        X_dummies = pd.get_dummies(X_train_test[colname], drop_first=False,prefix = colname)\n        X_train_test = X_train_test.join(X_dummies)\n        X_train_test = X_train_test.drop(colname,1)\n    model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n    model.fit(X_train_test.loc[d_train.index,:].copy(), y)\n    predictions = model.predict(X_train_test.loc[d_test.index,:].copy())\n    predictions = predictions.astype(int)\n\n\n    output = pd.DataFrame({'PassengerId': d_test.index+1, 'Survived': predictions})\n    output.to_csv('my_submission_11.csv', index=False)\n    print(output)\n    print(\"Your submission was successfully saved!\")","0ecde6db":"# # Encoding split\n# X_for_encoding = X_modified_9.copy()\n# X_for_encoding['Survived']= X_modified_9[[\"Survived\"]].copy()\n# X_for_encoding['Ticket']= d_train[[\"Ticket\"]].copy()\n# X_encode = X_for_encoding.sample(frac=0.9, random_state=0)\n# y_encode = X_encode.pop(\"Survived\")\n\n# # Training split\n# X_pretrain = X_for_encoding.drop(X_encode.index)\n# y_train = X_pretrain.pop(\"Survived\")\n\n# # Choose a set of features to encode and a value for m\n# encoder = MEstimateEncoder(cols=[\"Cabin\"], m=5)\n\n\n# # Fit the encoder on the encoding split\n# encoder.fit(X_encode, y_encode)\n\n# # Encode the training split\n# X_train = encoder.transform(X_pretrain, y_train).copy()\n","22c1604b":"# feature = encoder.cols\n\n# plt.figure(dpi=90)\n# ax = sns.distplot(y_train, kde=True, hist=False)\n# ax = sns.distplot(X_train[feature], color='r', ax=ax, hist=True, kde=False, norm_hist=True)\n# ax.set_xlabel(\"Survived\");","adc175e6":"d_train_y = d_train['Survived'].copy()\nd_train_x = d_train.drop('Survived',1)\nd_test_y = d_test['Survived']\nd_test_x = d_test.drop('Survived',1)\n\nX_modified_1 = name_breakdown(pd.concat([d_train_x,d_test_x]),features)\nprint(f\"New score after breaking-down Names: {score_dataset(X_modified_1.loc[d_train.index, :].copy(), d_train_y.copy()):.5f} RMSLE\")\n\nX_modified_2 = name_len_feature(X_modified_1,pd.concat([d_train,d_test]))\nprint(f\"New score after taking name length feature: {score_dataset(X_modified_2.loc[d_train.index, :].copy(), d_train_y.copy()):.5f} RMSLE\")\n\nX_modified_3 = ticket_first_litter(X_modified_2,pd.concat([d_train,d_test]))\nprint(f\"New score after taking ticket first letter breakdown feature: {score_dataset(X_modified_3.loc[d_train.index, :].copy(), d_train_y.copy()):.5f} RMSLE\")\n\nX_modified_4 = Cabin_break_down(X_modified_3)\nprint(f\"New score after taking cabin first letter breakdown feature: {score_dataset(X_modified_4.loc[d_train.index, :].copy(), d_train_y.copy()):.5f} RMSLE\")\n\nX_modified_5 = fare_binning(X_modified_4)\nprint(f\"New score after binning fare feature: {score_dataset(X_modified_5.loc[d_train.index, :].copy(), d_train_y.copy()):.5f} RMSLE\")\n\nX_modified_6 = age_binning(X_modified_5)\nprint(f\"New score after binning age feature: {score_dataset(X_modified_6.loc[d_train.index, :].copy(), d_train_y.copy()):.5f} RMSLE\")\n\nX_modified_7 = embarked_fare_group(X_modified_6)\nprint(f\"New score after grouping embarked feature: {score_dataset(X_modified_7.loc[d_train.index, :].copy(), d_train_y.copy()):.5f} RMSLE\")\n\nX_modified_8 = cluster_from_Age_fare(X_modified_7.copy())\nprint(f\"New score after cluster age and fare feature: {score_dataset(X_modified_8.loc[d_train.index, :].copy(), d_train_y.copy()):.5f} RMSLE\")\n\nX_modified_9 = SibSp_and_Parch(X_modified_8.copy())\nprint(f\"New score after add SibSp and Parch feature: {score_dataset(X_modified_9.loc[d_train.index, :].copy(), d_train_y.copy()):.5f} RMSLE\")\nX_modified_9","975001a8":"# Ticket_letter\nsns.countplot(X_modified_9.loc[d_train.index, :].copy()['Ticket_letter'], hue= d_train['Survived'])\n# sns.countplot(d_train['Embarked'], hue=d_train['Survived'])","1b70cb6a":"sns.countplot(X_modified_9.loc[d_train.index, :].copy()['Cabin_break_down'], hue= d_train['Survived'])","b4a61a82":"sns.countplot(X_modified_9.loc[d_train.index, :].copy()['FareEmbarked'], hue= d_train['Survived'])","3c6c0b7e":"sns.countplot(X_modified_9.loc[d_train.index, :].copy()['AgeBin'], hue= d_train['Survived'])","fdfec10f":"model_run(X_modified_9,d_train,d_test, d_train_y)","4ac61bcd":"mi_score_for_original_feature = make_mi_scores(X_modified_9.loc[d_train.index, :].copy(), d_train_y, False)\nprint(mi_score_for_original_feature)\ninformative_feature_original = []\nfor (column_name,score) in mi_score_for_original_feature.items():\n    if score > 0:\n        informative_feature_original.append(column_name)\nprint(informative_feature_original)\nX_modified_9_droped_unifomative= X_modified_9.copy()\nfor x in X_modified_9.columns:\n    if x not in informative_feature_original:\n        X_modified_9_droped_unifomative = X_modified_9_droped_unifomative.drop(x,1)\nX_modified_9_droped_unifomative\n# score_dataset(X_modified_9_droped_unifomative, y)\nprint(score_dataset(X_modified_9_droped_unifomative.loc[d_train.index, :].copy(), d_train_y.copy()))\nmodel_run(X_modified_9_droped_unifomative,d_train,d_test, d_train_y)","662cc1b7":"## Scoring DataSet","fae50f49":"## Mutual infirmation Scores","47f4ff24":"### Impute","b43e315f":"# Get mutual informations without considering onehot encorded features(original features)","f56f7d8a":"### Encode","f1dd0fed":"# According to the above informations we can say,\n**1. Pclass [3 1 2] Sex ['male' 'female'] SibSp [1 0 3 4 2 5 8] Parch [0 1 2 5 3 4 6 9] Embarked ['S' 'C' 'Q' nan] can be more infomative**\n\n**2. Ticket, Name, PassengerId is unique for almost every entry. So it has low infomations to map input to output**\n\n**3. Fare, cabin, age features will be more considered in the feature engineering section**","75704c5a":"# Test preprocess","8fb1bc64":"# Target Encoding","8c64a88d":"#### **SibSp and Parch has a significant principal components. So, I add those two, because adding that we can get familly size of the person. It might help to map input to output**","f8eab083":"# Preprocessing","8956a57a":"### Mutual information check","5d657200":"## Data Loading","587c79d3":"### Clean","4af489e3":"# PCA","bb829d9e":"# Creating Features"}}