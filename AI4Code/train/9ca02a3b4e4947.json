{"cell_type":{"62297687":"code","135a2523":"code","b0fb0950":"code","32f9d883":"code","eb999f78":"code","5dc9161f":"code","180ab5eb":"code","e5ac7f22":"code","f2fa62c0":"code","236f45d8":"code","36467036":"code","b5ab5e34":"code","9bdfaaeb":"code","03718aeb":"code","b7096673":"code","94bc27a9":"code","6bad45f4":"code","58f19f58":"code","44eafd6d":"code","2285f1b7":"code","2300ad4f":"code","36db54bb":"code","e0e6caea":"code","2e4b2cbf":"code","d995261f":"code","f302690b":"code","82ba4f18":"code","c7db4d49":"code","da386842":"code","3b8f6524":"code","493917a8":"code","d0066226":"code","899f741d":"code","42fec080":"markdown","a085924b":"markdown","d7422a81":"markdown","4d25bc5e":"markdown","8833f229":"markdown","d2638c57":"markdown","5fa6f6bd":"markdown","ef33e89d":"markdown","8869dde8":"markdown","c91f8ef3":"markdown","a2c6ed9a":"markdown","6b1c2c8d":"markdown","aa2688e5":"markdown","3aee0a3b":"markdown","86256b7a":"markdown","3fb4dc6c":"markdown","0c6d19d4":"markdown","0accfe44":"markdown","5c46deea":"markdown","74038807":"markdown","7a049dc6":"markdown","72783bf8":"markdown"},"source":{"62297687":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")","135a2523":"# Load dataset\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_train['dataset'] = 'train'\ndf_test = pd.read_csv('..\/input\/test.csv')\ndf_test['dataset'] = 'test'","b0fb0950":"df_train.shape","32f9d883":"# Response (SalePrice)\ndistplot = sns.distplot(df_train.SalePrice, fit=stats.norm)","eb999f78":"probplot = stats.probplot(df_train.SalePrice, plot=plt)","5dc9161f":"# get standard deviation\nresponse = df_train[['SalePrice']].copy()\nresponse['stddev'] = df_train[['SalePrice']].apply(lambda x: abs(x - x.mean())\/x.std())\nresponse[response.stddev > 3].sort_values('SalePrice', ascending=False).min()","180ab5eb":"# SalePrice >= 423000 is outlier\ndf_train = df_train[df_train.SalePrice < 423000]","e5ac7f22":"# Transform response with log function\ndf_train['SalePrice'] = np.log(df_train['SalePrice'])","f2fa62c0":"distplot = sns.distplot(df_train.SalePrice, fit=stats.norm)","236f45d8":"# select all numeric features, including response\nnum_features= df_train.select_dtypes(['float64', 'int64']).keys()\nlen(num_features)","36467036":"# select correlation > 0.5\nnum_corr = abs(df_train[num_features].corr()['SalePrice']).sort_values(ascending=False)\nnum_ok = num_corr[num_corr > 0.5].drop('SalePrice')\nnum_ok","b5ab5e34":"for i in range(0, len(num_ok), 5):\n    sns.pairplot(data=df_train, x_vars=num_ok[i:i+5].keys(), y_vars='SalePrice', kind='reg')","9bdfaaeb":"corr_plot = sns.heatmap(df_train[num_ok.keys()].corr(), cmap=plt.cm.Reds, annot=True)","03718aeb":"# check NaN values on numerical falues\ndf_train[num_ok.keys()].isnull().sum()\ntrain_num_null = df_train[num_ok.keys()].isnull().sum().sort_values(ascending=False)\ntest_num_null = df_test[num_ok.keys()].isnull().sum().sort_values(ascending=False)\nprint('null from train dataset:\\n{}\\n'.format(train_num_null[train_num_null > 0]))\nprint('null from test dataset:\\n{}'.format(test_num_null[test_num_null > 0]))","b7096673":"# fill GarageYrBlt on train dataset\ndf_train['GarageYrBlt'] = df_train[['GarageYrBlt']].applymap(lambda x: 0 if pd.isnull(x) else x)\n# fill GarageYrBlt, TotalBsmtSF, GarageArea, GarageCars on test dataset\nfor i in test_num_null[test_num_null > 0].keys():\n    df_test[i] = df_test[[i]].applymap(lambda x: 0 if pd.isnull(x) else x)","94bc27a9":"cat_features = df_train.select_dtypes(['object'])\nlen(cat_features.keys())","6bad45f4":"# after see all plots, i will choose this features\ncat_ok = ['MSZoning', 'LotShape', 'LotConfig', 'Neighborhood', 'Condition1', 'BldgType', 'HouseStyle', 'MasVnrType', 'ExterQual', 'Foundation', 'BsmtQual', 'BsmtCond','HeatingQC', 'CentralAir', 'KitchenQual', 'GarageType', 'GarageFinish', 'SaleType', 'SaleCondition']","58f19f58":"# this is how its look like\nplt.rcParams['figure.max_open_warning'] = len(cat_ok)\nfor i in cat_ok:\n    fig, ax = plt.subplots(1, 2, sharey=True, figsize=(16, 3))\n    sns.boxplot(x='SalePrice', y=i, data=df_train, ax=ax[0])\n    sns.countplot(y=i, data=df_train, ax=ax[1])","44eafd6d":"# check NaN values on categorical features\ntrain_cat_null = df_train[cat_ok].isnull().sum().sort_values(ascending=False)\ntest_cat_null = df_test[cat_ok].isnull().sum().sort_values(ascending=False)\nprint('null from train dataset:\\n{}\\n'.format(train_cat_null[train_cat_null > 0]))\nprint('null from test dataset:\\n{}'.format(test_cat_null[test_cat_null > 0]))","2285f1b7":"print('GarageFinish:', df_train['GarageFinish'].unique())\nprint('GarageType:', df_train['GarageType'].unique())\nprint('BsmtCond:', df_train['BsmtCond'].unique())\nprint('BsmtQual:', df_train['BsmtQual'].unique())\nprint('MasVnrType:', df_train['MasVnrType'].unique())","2300ad4f":"# Replace NaN value to 'None'\nfor i in ['GarageFinish', 'GarageType', 'BsmtCond', 'BsmtQual', 'MasVnrType']:\n    df_train[i] = df_train[[i]].applymap(lambda x: 'None' if pd.isnull(x) else x)\n    df_test[i] = df_test[[i]].applymap(lambda x: 'None' if pd.isnull(x) else x)","36db54bb":"print('MSZoning:', df_train.groupby(['MSZoning']).size())\nprint('\\nKitchenQual:', df_train.groupby(['KitchenQual']).size())\nprint('\\nSaleType:', df_train.groupby(['SaleType']).size())","e0e6caea":"df_test['MSZoning'] = df_test['MSZoning'].fillna(df_train['MSZoning'].mode()[0])\ndf_test['KitchenQual'] = df_test['KitchenQual'].fillna(df_train['KitchenQual'].mode()[0])\ndf_test['SaleType'] = df_test['SaleType'].fillna(df_train['SaleType'].mode()[0])","2e4b2cbf":"# concat train and test dataset\ndf_test['SalePrice'] = np.nan\ndf = pd.concat([df_train, df_test], sort=True)","d995261f":"# mean and stddev from numeric train dataset\nnum_mean = df_train[num_ok.index].mean()\nnum_std = df_train[num_ok.index].std()\n\n# Standarize numeric features\ndf_num = (df[num_ok.keys()] - num_mean) \/ num_std","f302690b":"# Create dummies for categorical features\ndf_dummies = pd.get_dummies(df[cat_ok])","82ba4f18":"# dataset column + SalePrice + numeric features + categorical features\ndf = pd.concat([df.dataset, df.SalePrice, df_num, df_dummies], axis=1)\ndf.head()","c7db4d49":"# for splitting train and validate dataset\nfrom sklearn.model_selection import train_test_split\n\n# machine learning regressor\nfrom sklearn.linear_model import LinearRegression, Lasso, ElasticNetCV\nfrom sklearn.ensemble import RandomForestRegressor\n\n# for evaluate model using RootMeanSquaredError\nfrom sklearn.metrics import mean_squared_error","da386842":"X = df[df.dataset == 'train'].drop(['dataset', 'SalePrice'], axis=1)\ny = df[df.dataset == 'train'][['SalePrice']]\nX_test = df[df.dataset == 'test'].drop(['dataset', 'SalePrice'], axis=1)","3b8f6524":"# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\nX_train, y_train = X, y","493917a8":"model = ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000)\nmodel.fit(X_train, y_train)\n# y_pred = model.predict(X_valid)\n# mean_squared_error(y_valid, y_pred)","d0066226":"X_test.isnull().sum().sum()","899f741d":"# Create submission file\ny_test_pred = np.exp(model.predict(X_test))\nresult = df_test[['Id']].copy()\nresult['SalePrice'] = y_test_pred\nresult.set_index('Id').to_csv('submission.csv')","42fec080":"38 numeric features. We will select highly correlated features to SalePrice","a085924b":"Now we must transform predictor (Standarize).","d7422a81":"# 1. Load datasets","4d25bc5e":"for NaN value on MSZoning, KitchenQual, SaleType, we can simply replace with most frequent value (mode).","8833f229":"We finish preprocessing data. Now time to train model.","d2638c57":"outlier has been remove and log transform applied, lets see distplot again","5fa6f6bd":"## 2.2 Numerical features","ef33e89d":"Not really normal distribution, we will fix it with log transformation.<br\/>\nNow we have outlier on response. Ideally, we must remove outlier from training dataset.","8869dde8":"This is how its look like features with highly correlated to SalePrice","c91f8ef3":"# 3. Preparing data","a2c6ed9a":"## 2.1 Response","6b1c2c8d":"We have 44 categorical features.<br\/>\nFor selecting categorical features i ever hear about features importance from preliminary Random Forest model,<br\/>\nbut all of features must first convert to numerical.<br\/>\nI also hear about ANOVA test or dimensionality reduction but i haven't ever try it yet.<br\/>\nSo i will selecting based on boxplot and countplot.","aa2688e5":"8 features contain NaN value on test dataset.<br\/>\nfrom data_description.txt we can understand that:<br\/>\nfor GarageFinish and GarageType NaN means No Garage<br\/>\nfor BsmtCond and BsmtQual NaN means No Basement","3aee0a3b":"# 2. Exploratory data analysis","86256b7a":"Now it's just 10 features. Lets check this plot.","3fb4dc6c":"yup, its better now","0c6d19d4":"## 2.3 Categorical features","0accfe44":"Now lets see categorical features.<br\/>","5c46deea":"There is only one feature that has NaN value on train dataset<br\/>\nIssue on test dataset: TotalBsmtSF, GarageArea, GarageCars missing 1 value.<br\/>\nWe assume NaN equal to 0. We fill it.","74038807":"Wow !!! There's a lot of  features. From now lets focus on exploratory data first","7a049dc6":"YearBuilt-GarageYrBlt and TotalBsmtSF-1stFlrSF correlate each other, but leave it be.","72783bf8":"# 4. Build model"}}