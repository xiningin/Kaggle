{"cell_type":{"2395ea38":"code","d7dee77d":"code","cf1f3bbb":"code","31cab36c":"code","a6e35e84":"code","e5a7f3d0":"code","d244f520":"code","f2babc36":"code","0bb21678":"code","29e0afdc":"code","cef1029c":"code","81e1e53a":"code","08600eca":"code","5173f76c":"code","02d6a70e":"code","8b1163cf":"code","c6f7f8c1":"code","614d10d3":"code","76c4409e":"code","df4d99b1":"code","4d81f31f":"code","c7b3ebbc":"code","953bceaf":"code","df77610c":"code","ba2d4919":"code","dbb91873":"code","ae3b32e9":"code","1636db58":"code","d98d730d":"code","7ec25b8a":"code","4502bdbc":"code","2f52b0cc":"code","7bbf85d7":"code","8b7b2633":"code","a62a3325":"markdown","1cb48106":"markdown","aa54ed57":"markdown","eaa20183":"markdown","c2187b19":"markdown","b1a09de5":"markdown","8be382f3":"markdown","de5f7fcc":"markdown","846f3edc":"markdown","0cc192b4":"markdown","f4d9a510":"markdown","66da9254":"markdown","7d20db99":"markdown","01b90eaa":"markdown","43c31e03":"markdown","9ecce8a0":"markdown","6eaf8eb1":"markdown","e399193b":"markdown","f4bd3f1c":"markdown","2a1545cc":"markdown","386a857c":"markdown","cec1bbe7":"markdown","84d37b73":"markdown","7787b57d":"markdown","7e7b4353":"markdown","4bd51ec8":"markdown"},"source":{"2395ea38":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d7dee77d":"# Importing the dataset\nimport pandas as pd\ndataset = pd.read_csv(\"..\/input\/abalone-dataset\/abalone.csv\")","cf1f3bbb":"# Displaying the first 5 row in our dataset dataframe\ndataset.head(5)","31cab36c":"# droping Rings column and adding age column of the abalone\ndataset['age'] = dataset['Rings']+1.5\ndataset.drop('Rings', axis = 1, inplace = True)","a6e35e84":"# Display the shape of the dataset\nprint(dataset.shape)\n\n# Display the Datatype of each attribute\ndataset.dtypes","e5a7f3d0":"# Checking whether we have null or empty data in our dataset\nprint(dataset.isna().sum())","d244f520":"# Descriptive Statitistics describe each attribute.\ndataset.describe()","f2babc36":"# Histogram visualisation for each attribute to know what kind of distribution it is?\ndataset.hist(figsize=(20,10), grid=False, layout=(2, 4), bins = 30)","0bb21678":"# Density visualisation for all attributes\ndataset.plot(kind='density',layout=(2,4),sharex=False,sharey=False,subplots=True,grid=False)\n","29e0afdc":"# Histogram Descriptive visualisation distribution for output attribute age\n\nimport seaborn as sb\nsb.distplot(dataset['age'])","cef1029c":"# Histogram visualisation for age output and Longest shell measurement input attributes.\n\ndata_plot=pd.concat([dataset['age'],dataset['Length']],axis=1)\ndata_plot.plot.scatter(x='Length',y='age')","81e1e53a":"# Histogram visualisation for age output and Diameter input attributes.\n\ndata_plot=pd.concat([dataset['Diameter'],dataset['age']],axis=1)\ndata_plot.plot.scatter(x='Diameter',y='age')","08600eca":"# Histogram visualisation for age output and Height input attributes.\n\ndata_plot=pd.concat([dataset['Height'],dataset['age']],axis=1)\ndata_plot.plot.scatter(x='Height',y='age')","5173f76c":"# Removing the outliers values from our dataset\ndataset=dataset.drop(dataset[(dataset['Height']>0.4) & (dataset['Height']<1.4)].index)\n\n# Visualising again to know those outlier removed or not\ndata_plot=pd.concat([dataset['Height'],dataset['age']],axis=1)\ndata_plot.plot.scatter(x='Height',y='age')","02d6a70e":"# Histogram visualisation for age output and Whole weight input attributes.\n\ndata=pd.concat([dataset['Whole weight'],dataset['age']],axis=1)\ndata.plot.scatter(x='Whole weight',y='age')","8b1163cf":"# Histogram visualisation for age output and Shucked weight input attributes.\n\ndata=pd.concat([dataset['Shucked weight'],dataset['age']],axis=1)\ndata.plot.scatter(x='Shucked weight',y='age')","c6f7f8c1":"# Removing the outlier values for age output and Shucked weight input attribute.\n\ndataset=dataset.drop(dataset[(dataset['Shucked weight']>1.2)&(dataset['Shucked weight']<15)].index)\n\n# Visualising again to know whether those outlier values removed or not\ndata=pd.concat([dataset['Shucked weight'],dataset['age']],axis=1)\ndata.plot.scatter(x='Shucked weight',y='age')","614d10d3":"# Histogram visualisation for Viscera weight input attribute and age output attribute.\ndata=pd.concat([dataset['Viscera weight'],dataset['age']],axis=1)\ndata.plot.scatter(x='Viscera weight',y='age')","76c4409e":"# Removing the outlier value lies in between 0.6 to 15\ndataset=dataset.drop(dataset[(dataset['Viscera weight']>0.6)&(dataset['Viscera weight']<15)].index)\n\n# Visualising again to check whether those outliers removed or not\ndata=pd.concat([dataset['Viscera weight'],dataset['age']],axis=1)\ndata.plot.scatter(x='Viscera weight',y='age')","df4d99b1":"# Histogram Visualisation for Shell weight input attribute and age output attribute.\n\ndata=pd.concat([dataset['Shell weight'],dataset['age']],axis=1)\ndata.plot.scatter(x='Shell weight',y='age')","4d81f31f":"# Removing the outliers for Shell weight input attribute and age output attribute.\n\ndataset=dataset.drop(dataset[(dataset['Shell weight']>0.9)&(dataset['Shell weight']<15)].index)\n\n# Visualising again to check all outlier below the threshold removed or not.\n\ndata=pd.concat([dataset['Shell weight'],dataset['age']],axis=1)\ndata.plot.scatter(x='Shell weight',y='age')","c7b3ebbc":"# Correlation value with each attribute using heatmap\n\nimport seaborn as sb\ncorrelation_values=dataset.corr()\nsb.heatmap(correlation_values,square=True)","953bceaf":"# Zoom in correlation coefficient values in each attribute.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nk=9 #No of Heapmaps values with the best correlation with other variables\n\n# all column  values\ncols=dataset.corr().nlargest(k,'age')['age'].index\n# find correlation coefficient values\ncorrelation_coefficient=np.corrcoef(dataset[cols].values.T)\n# column head name values shape\nsb.set(font_scale=1.0)\n#heat map\nsb.heatmap(correlation_coefficient,cbar=True,annot=True,square=True,\n           fmt='.2f',annot_kws={'size': 10},xticklabels=True,yticklabels=True)\nplt.show()","df77610c":"# Splitting the dataset into input and output attribute.\nx=dataset.iloc[:,:-1].values\ny=dataset.iloc[:,-1].values","ba2d4919":"# Encoding the categorical value into numerical values \n\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder=LabelEncoder()\nx[:,0]=labelencoder.fit_transform(x[:,0])","dbb91873":"train_set=0.80\ntest_set=0.20\nseed=5\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,train_size=train_set,test_size=test_set,random_state=seed)","ae3b32e9":"# Spot Checking and Comparing Algorithms Without MinmaxScaler Scaler\nn_neighbors=5\nmodels=[]\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nmodels.append(('LinearRegression',LinearRegression()))\nmodels.append(('knn',KNeighborsRegressor(n_neighbors=n_neighbors)))\nmodels.append(('SVR',SVR()))\nmodels.append((\"decision_tree\",DecisionTreeRegressor()))\n\n# Evaluating Each model\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nnames=[]\npredictions=[]\nerror='neg_mean_squared_error'\nfor name,model in models:\n    fold=KFold(n_splits=10,random_state=0)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    predictions.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n\n# Visualizing the Model accuracy\nfig=plt.figure()\nfig.suptitle(\"Comparing Algorithms\")\naxis=fig.add_subplot(111)\nplt.boxplot(predictions)\naxis.set_xticklabels(names)\nplt.xticks(rotation='90')\nplt.show()","1636db58":"# Spot Checking and Comparing Algorithms With MinmaxScaler Scaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn. preprocessing import MinMaxScaler\npipelines=[]\npipelines.append(('scaled LinearRegression',Pipeline([('scaler',MinMaxScaler()),('LinearRegression',LinearRegression())])))\npipelines.append(('scaled KNN',Pipeline([('scaler',MinMaxScaler()),('KNN',KNeighborsRegressor(n_neighbors=n_neighbors))])))\npipelines.append(('scaled SVR',Pipeline([('scaler',MinMaxScaler()),('SVR',SVR())])))\npipelines.append(('scaled DecisionTree',Pipeline([('scaler',MinMaxScaler()),('decision',DecisionTreeRegressor())])))\n\n# Evaluating Each model\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nnames=[]\npredictions=[]\nfor name,model in models:\n    fold=KFold(n_splits=10,random_state=0)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    predictions.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n\n# Visualizing the Model accuracy\nfig=plt.figure()\nfig.suptitle(\"Comparing Algorithms\")\nplt.boxplot(predictions)\nplt.show()","d98d730d":"# Linear Regression Algorithm Tuning\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=MinMaxScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nparam_grid=dict()\nmodel=LinearRegression()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","7ec25b8a":"# KNN Regression Tuning\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=MinMaxScaler().fit(x_train)\nrescalex=scaler.transform(x_train)\nn_neighbors=[3,4,5,6,7,8,9,10,15,20]\n# With degree our model fit to training set overfitting so better not use for all algorithms except polyomial\n#degree=[1,2,3,4,5,6,7,8,9]\nparam_grid=dict(n_neighbors=n_neighbors)\nmodel=KNeighborsRegressor()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescalex,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","4502bdbc":"# Ensemble and Boosting algorithm to improve performance\n\n#Ensemble\n# Boosting methods\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n# Bagging methods\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nensembles=[]\nensembles.append(('scaledAB',Pipeline([('scale',MinMaxScaler()),('AB',AdaBoostRegressor())])))\nensembles.append(('scaledGBC',Pipeline([('scale',MinMaxScaler()),('GBc',GradientBoostingRegressor())])))\nensembles.append(('scaledRFC',Pipeline([('scale',MinMaxScaler()),('rf',RandomForestRegressor(n_estimators=10))])))\nensembles.append(('scaledETC',Pipeline([('scale',MinMaxScaler()),('ETC',ExtraTreesRegressor(n_estimators=10))])))\n\n# Evaluate each Ensemble Techinique\nresults=[]\nnames=[]\nfor name,model in ensembles:\n    fold=KFold(n_splits=10,random_state=5)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    results.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n# Visualizing the compared Ensemble Algorithms\nfig=plt.figure()\nfig.suptitle('Ensemble Compared Algorithms')\nplt.boxplot(results)\nplt.show()","2f52b0cc":"# RandomForest Regressor Tuning\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=MinMaxScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nn_estimators=[5,10,15,20,25,30,40,50,75,100]\nparam_grid=dict(n_estimators=n_estimators)\nmodel=RandomForestRegressor()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","7bbf85d7":"# Gradient Boosting Algorithm Tuning\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=MinMaxScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nlearning_rate=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nn_estimators=[10,15,20,25,30,40,50,75,100,150,200]\nparam_grid=dict(learning_rate=learning_rate,n_estimators=n_estimators)\nmodel=GradientBoostingRegressor()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","8b7b2633":"# Finalize Model\n# we finalized the Random Forest Regressor algorithm and evaluate the model for Abalone Physical meansurements\n\nfrom sklearn.metrics import mean_squared_error\nscaler=MinMaxScaler().fit(x_train)\nscaler_x=scaler.transform(x_train)\nmodel=RandomForestRegressor(n_estimators=75)\nmodel.fit(scaler_x,y_train)\n\n#Transform the validation test set data\nscaledx_test=scaler.transform(x_test)\ny_pred=model.predict(scaledx_test)\n\naccuracy=mean_squared_error(y_test,y_pred)\n\nprint(\"accuracy :\",accuracy)","a62a3325":"We listing top two accuracy models\n\n1. LinearRegression : mean -4.857541 (0.499037)\n2. knn : mean:-5.224915 std:(0.714804)\nNow we are applying regularisation tuning to LinearRegression and knn Regression algorithms\n\nBefore going to tuning those 2 algorithms we need to apply feature scaling to our dataset and predicting again then we will take top 2 algorithm which is scaled one.Because scaled algorithm always gives best predictions.","1cb48106":"We Dont have any Outliers for age output and Diameter input attributes as per above visualisation plot.","aa54ed57":"We dont have null or empty cells in our dataset so no need to apply imputer library to handle Empty Cell or empty values","eaa20183":"Though features are not normaly distributed, are close to normality","c2187b19":"We listing tunned two accuracy models\n\nGradient Boosting Algorithm with mean -4.824129 and std(0.622529)\n\nRandom Forest Regressor Algorithm with mean -5.198858 and std(0.586180)\n\nNow we are going to tuning this 2 algorithms","b1a09de5":"\n# Label Encoder","8be382f3":"# Correlation values between each attributes using heatmap.","de5f7fcc":"As per above plot we got normal distribution for discrete ouput age column values.","846f3edc":"We listing top four tunned algorithms.\n\n1. Linear Regression Algorithm Best: -4.857541 using {}\n2. Regression Algorithm Best: -5.125680 using {'n_neighbors': 15}\n3. Random Forest Regressor Algorithm Best: -4.797185 using {'n_estimators': 75} \n4. Gradient Boosting Regressor Algorithm -4.802066 using {'learning_rate': 0.2, 'n_estimators': 75} \n\nAs per above four tunned algorithm Decision Tree Classifier Algorithm.","0cc192b4":"# Analyzing the correlations with output and each input attribute and find outliers.","f4d9a510":"Here we got 2 outlier point in between 0.4 to 1.2 values. Perhaps with this outlier value will effect the performance for our algorithm.\n\nSo now we are going to remove this 2 outlier values.\n\n# Removing Outlier for age output and Height input attributes","66da9254":"# Feature Engineering\n\n# Data Cleaning\n\n\nNo need to apply cleaning to our dataset. Because we dont have any error or empty or null values","7d20db99":"We listing tunned two accuracy models\n\nLinear Regression Algorithm Best: -4.857541 using {}\n\nKNN Regression Algorithm Best: -5.125680 using {'n_neighbors': 15}","01b90eaa":"# Splitting dataset into training and test set","43c31e03":"\n# Classification Modelling","9ecce8a0":"Ouput attribute age confidence interval range starts from 9 to 12 so it is not starting from zero so it will positive sknew","6eaf8eb1":"From problem statement and feature discription, let's first compute the target varible of the problem ' Age' and assign it to the dataset. Age = 1.5+Rings","e399193b":"We are not sure whether those values lies below the threshold 0.9 to 15 values we are going remove those values.","f4bd3f1c":"We got Regression accuracy \n\ntraining set accuracy: 4.797185\n\ntesting accuracy :4.622583966920101","2a1545cc":"We got again 1 outlier value for Viscera weight input attribute and age output attribut. So now we are going to remove in between 0.6 to 15 value box.","386a857c":"# Abalone Age Prediction\nDescription- Predicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age","cec1bbe7":"# Data Analysis","84d37b73":"We didnt got any outliers for Rings output and Whole weight input attribute. So we going further to check.","7787b57d":" As we can see above plot with high value we got less rings, so it was near to outlier so we are removing from 1.2 to 1.6 range values for Rings output and Shucked weight input attribute.\n","7e7b4353":"We Dont have any Outliers for age output and Length input attributes as per above visualisation plot.","4bd51ec8":"We listing top two accuracy models, After applying feature scaling performance get increased \n\n Before Applying Feature Scaling \n1. LinearRegression : mean -4.857541 (0.499037)\n2. knn : mean:-5.224915 std:(0.714804)\n\n After Applying Feature Scaling\n1. LinearRegression : mean -4.857541 (0.499037)\n2. knn : mean:-5.224915 std:(0.714804)\n\nNow we are applying regularisation tuning to decision tree and supoort vector algorithms"}}