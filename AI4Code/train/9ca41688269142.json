{"cell_type":{"5a505e70":"code","9755950e":"code","2a9eabd2":"code","cedbc46e":"code","3131ab7f":"code","be260290":"code","d39bb718":"code","42bdc14a":"code","07aab23f":"code","9d1c81ae":"code","f369a93a":"code","8a58ed84":"code","99929301":"code","d0a9f302":"code","37de5272":"code","cf4bcc59":"code","fa7c90fd":"code","cafa16e3":"code","7dec44db":"code","b6b5496b":"code","45aa2e07":"code","f3695b08":"code","fee05219":"code","f3e3962d":"code","921a4d12":"code","2a23db87":"code","cf2bb461":"code","4b2201f1":"code","15291625":"code","9fd0d20d":"code","dd356f31":"code","c9c6f174":"code","229993ab":"code","8b84d367":"code","0a66db35":"code","16d221e5":"code","083f42bd":"code","d791e1b2":"code","9596a06d":"code","71b2951b":"code","50469684":"code","6889aed5":"code","20f42db5":"code","cd66416f":"code","a93f0111":"code","6ee16d25":"code","fab99658":"code","33b40fd3":"code","1128dc27":"code","4a38d812":"code","367c71e3":"code","7e8b5660":"code","258f5982":"code","8921a932":"code","86f30979":"code","b273d2de":"code","6c197412":"code","4c945fae":"code","b9466ebe":"code","3e50b3b7":"code","11f17aa6":"markdown","49dd6a46":"markdown","20fbe25c":"markdown","02a84e22":"markdown","30c4a6d8":"markdown","1251f362":"markdown","dccb4c8f":"markdown","5048dc38":"markdown","dc0a4e00":"markdown","0f779c0d":"markdown","4f83312a":"markdown","5c1676a2":"markdown","7757e451":"markdown","0a5e14b4":"markdown","75a9a518":"markdown","718fbd82":"markdown","fb5d5bba":"markdown","403d309f":"markdown","3f30d04b":"markdown","8504c1b5":"markdown","b6c94fcf":"markdown","7d30e358":"markdown","1ace5659":"markdown","4dc6b0a5":"markdown","5c642fcf":"markdown","fc496de8":"markdown","01f7aaeb":"markdown","fb9f9032":"markdown","edcf16d8":"markdown","4b2191c5":"markdown","781da036":"markdown","f6f605a0":"markdown","368a4ff0":"markdown","b69c5e58":"markdown","3e22c816":"markdown","e7a776ed":"markdown","8b718664":"markdown","c4713157":"markdown","c4419e86":"markdown","3d7a5e1b":"markdown","e6d7663e":"markdown","3792a087":"markdown","c256b9cc":"markdown","af4504fd":"markdown","6460407f":"markdown","9ab29e27":"markdown","60acfa0d":"markdown","010c5ad9":"markdown","b6ba1734":"markdown","1a192957":"markdown","200c282f":"markdown","5cd0a8ed":"markdown","b5668313":"markdown","ff202fdc":"markdown","2d22c106":"markdown","ae884af2":"markdown","92647d73":"markdown","4a60d5b4":"markdown","63e7ba12":"markdown","4d94ea37":"markdown","adc18ab4":"markdown","4e0bf4e9":"markdown","dffa94b1":"markdown","3fabba87":"markdown","0d4cd08a":"markdown","d4be4f3e":"markdown","d676a9d2":"markdown","5712c5cb":"markdown","f99f3242":"markdown","7843e298":"markdown"},"source":{"5a505e70":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt ","9755950e":"#importing data\n# ds --> dataset\nds = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","2a9eabd2":"# getting the first ten rows of dataset\nds.head(10)","cedbc46e":"ds.info()","3131ab7f":"Y = ds.Survived\n# drop the columns\nX = ds.drop(['PassengerId','Name','Survived'],axis = 1)","be260290":"print(X.isnull().sum())","d39bb718":"# description\nprint(X.Age.describe())\n\n# plus histogram\nfig, ax = plt.subplots(figsize= (5,5))\nax.hist(x= X.Age,color= \"blue\",edgecolor = 'black')\nplt.show()","42bdc14a":"X.columns","07aab23f":"X.Sex.describe()","9d1c81ae":"from sklearn.compose import ColumnTransformer # transforming\nfrom sklearn.preprocessing import OrdinalEncoder # encoding\nX_temp = X\n\nSex_Embarked = {\"Sex\":{\"male\": 1.,\"female\": 0.},\n               \"Embarked\":{\"S\": 0.,\"C\": 1.,\"Q\": 2.}}\nX_temp = X_temp.replace(Sex_Embarked, inplace=False)\n\nCT = ColumnTransformer(transformers = [('encoder',OrdinalEncoder(),['Ticket'])],remainder = 'passthrough')\nX_temp = pd.DataFrame(CT.fit_transform(X_temp.drop(['Cabin'],axis = 1,inplace = False).dropna(subset = ['Age','Embarked'],axis =0 ,inplace = False)),\n                      columns=[ 'Ticket', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare','Embarked'],\n                     dtype = 'float')","f369a93a":"X_temp","8a58ed84":"import seaborn as sb\nfig,ax = plt.subplots(figsize = (10,10))\nsb.heatmap(pd.concat([X_temp,Y],axis=1).dropna().corr(),annot = True,linewidth = 0.5)","99929301":"sb.set(style=\"whitegrid\", palette=\"muted\")\n\nfig = plt.figure(figsize=(10,10))\n\nax = sb.swarmplot(x='Sex', y=\"Age\", hue = \"Survived\",data=ds,size=6)\n\nplt.show()","d0a9f302":"from sklearn.impute import SimpleImputer # missing values\nimputer = SimpleImputer(missing_values=np.nan, strategy = 'mean')\nds.Age = imputer.fit_transform(ds.Age.values.reshape(-1,1))\nX.Age = imputer.fit_transform(X.Age.values.reshape(-1,1))","37de5272":"X.Age","cf4bcc59":"ds.Embarked = SimpleImputer(missing_values=np.nan, strategy = 'most_frequent').fit_transform(ds.Embarked.values.reshape(-1,1))\nX.Embarked = SimpleImputer(missing_values=np.nan, strategy = 'most_frequent').fit_transform(X.Embarked.values.reshape(-1,1))","fa7c90fd":"# get more information of the cabin attribute\nX.Cabin.describe()","cafa16e3":"# what are the unique values of this attribute?\nX.Cabin.unique()","7dec44db":"# for instance, we'll get the not-null values of cabins whose names start with \"C\".\n# which means those cabins belong to deck C.\nX_Cabins = X.Cabin.dropna().str.contains('C',regex = False)\nX_Cabins = X.Cabin[X_Cabins.loc[X_Cabins == True].index]\n# X_Cabins = X.Cabin.dropna().str.extract(r'(^C.+)')\nprint(X_Cabins)","b6b5496b":"X_Cabins.describe()","45aa2e07":"# add a new column called Deck to our dataset\nds = ds.assign(Deck = lambda x: x.Cabin.str.extract(r'(.)'))\n# reassign it to X\nX = ds.drop(['PassengerId','Name','Survived'],axis = 1)\nds.Deck.describe()","f3695b08":"import seaborn as sb\n\n\norder = ['A','B','C','D','E','F','G','T']\n\nfig, ax1 = plt.subplots()\nfig.set_size_inches(11, 8)\n\nsb.set(style = 'darkgrid')\n\nax = sb.countplot(ax =ax1 ,data=ds[ds.Deck.notnull()],x=\"Deck\",hue =\"Survived\",\n                  palette=\"dark\",alpha=.6, order = order)\nbars = ax.patches\nhalf = int(len(bars)\/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()\/2., height_l, '{0:.0%}'.format(height_l\/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()\/2., height_r, '{0:.0%}'.format(height_r\/total), ha=\"center\")\n\n\nplt.show()","fee05219":"X = X.drop(['Cabin'],axis = 1)\nX","f3e3962d":"X_temp = X\n\nSex_Embarked = {\"Sex\":{\"male\": 1.,\"female\": 0.},\n               \"Embarked\":{\"S\": 0.,\"C\": 1.,\"Q\": 2.},\n               \"Deck\":{\"A\": 1.,\"B\": 2.,\"C\": 3.,\n                      \"D\": 4.,\"E\": 5.,\"F\": 6.,\n                      \"G\": 7.,\"T\": 8.}}\n\nX_temp = X_temp.replace(Sex_Embarked, inplace=False)\n\nX_temp = pd.DataFrame(CT.fit_transform(X_temp),\n                      columns=[ 'Ticket', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare','Embarked','Deck'],\n                     dtype = 'float')","921a4d12":"X_temp","2a23db87":"fig,ax = plt.subplots(figsize = (10,10))\nsb.heatmap(pd.concat([X_temp,Y],axis=1).dropna().corr(),annot = True,linewidth = 0.5)","cf2bb461":"X_temp = X_temp.drop(['Deck'],axis = 1)\n","4b2201f1":"X_temp","15291625":"fig,ax = plt.subplots(figsize = (10,10))\nsb.heatmap(pd.concat([X_temp,Y],axis=1).corr(),annot = True,fmt = '0.1',linewidth = 1)","9fd0d20d":"# assign zero if they are alone\nX_temp['family_Size'] = X_temp.Parch + X_temp.SibSp \nX_temp = X_temp.drop(['Parch','SibSp'],axis = 1)","dd356f31":"X_temp","c9c6f174":"#mean of the Fare of the tickets for each class\nmean = ['{0}, {1:.0f}'.format(pclass,np.nanmean(X.where(X.Pclass == pclass,inplace = False).Fare)) for pclass in X.Pclass.unique()]\nprint(mean)","229993ab":"# fig, ax = plt.subplots(1,2,figsize=(10,10))\n# sb.set_theme(style=\"whitegrid\")\nfig, ax1 = plt.subplots(1,2)\nfig.set_size_inches(15, 10)\n\nsb.violinplot(ax = ax1[0],data=ds,x=\"Pclass\",y =\"Fare\",hue=\"Survived\",\n              split=True, inner=\"quart\", linewidth=1,\n              palette={0: \"b\", 1: \"r\"},scale=\"area\",saturation = 1)\n\nsb.countplot(ax =ax1[1] ,data=ds,x=\"Pclass\",hue =\"Survived\",\n                  palette={0: \"b\", 1: \"r\"},alpha=1)\n\n\nplt.show()","8b84d367":"X_temp = X_temp.drop(['Ticket'],axis = 1)","0a66db35":"X_temp","16d221e5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_temp,Y,test_size = 0.25,random_state = 1)\n\n# Set the regularization parameter C=1\nlogistic = LogisticRegression(C=1, penalty=\"l1\", solver='liblinear', random_state=7).fit(X_train,Y_train )\nmodel = SelectFromModel(logistic, prefit=True)\n\nX_new = model.transform(X_train)\nX_new","083f42bd":"# Get back the kept features as a DataFrame with dropped columns as all 0s\nselected_features = pd.DataFrame(model.inverse_transform(X_new), \n                                 index=X_train.index,\n                                 columns=X_train.columns)\n\n# Dropped columns have values of all 0s, keep other columns \nselected_columns = selected_features.columns[selected_features.var() != 0]\nselected_columns","d791e1b2":"from sklearn.metrics import confusion_matrix, accuracy_score # estimating the model\ncm2 = confusion_matrix(Y_valid,logistic.predict(X_valid))\nlog_acc = accuracy_score(Y_valid,logistic.predict(X_valid))\nprint(log_acc,'\\n',cm2)","9596a06d":"from sklearn.preprocessing import  MinMaxScaler# scaling\nsc = MinMaxScaler()\nscaled_X_temp = pd.DataFrame(sc.fit_transform(X_temp),columns = X_temp.columns)\nscaled_X_temp","71b2951b":"from sklearn.neighbors import KNeighborsClassifier\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(scaled_X_temp,Y,test_size = 0.25,random_state = 1)\n\nknn_Classifier = KNeighborsClassifier(n_neighbors = 7, metric = 'minkowski', p=2)\nknn_Classifier.fit(X_train,Y_train)\n\n#predicting\nY_pred_knn = knn_Classifier.predict(X_valid)\n\n#estimating\ncm2 = confusion_matrix(Y_valid,Y_pred_knn)\nknn_acc = accuracy_score(Y_valid,Y_pred_knn)\nprint(knn_acc)\nprint(cm2)","50469684":"from sklearn.model_selection import cross_val_predict # used to predict\nfrom sklearn.model_selection import cross_val_score #use to get the accuracy\ncv_knn_score = cross_val_score(knn_Classifier,scaled_X_temp,Y,cv = 10,scoring='accuracy').mean()\nprint(cv_knn_score)","6889aed5":"from sklearn.svm import SVC\n\nsvm_Classifier = SVC(kernel = 'rbf', random_state = 0)\n\ncv_svm_score = cross_val_score(svm_Classifier,scaled_X_temp,Y,cv = 10,scoring='accuracy').mean()\nprint(cv_svm_score)","20f42db5":"from sklearn.naive_bayes import GaussianNB\nnb_Classifier = GaussianNB()\n\ncv_nb_score = cross_val_score(nb_Classifier,scaled_X_temp,Y,cv = 10,scoring='accuracy').mean()\nprint(cv_nb_score)","cd66416f":"from sklearn.tree import DecisionTreeClassifier\ndt_Classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n\ncv_dt_score = cross_val_score(dt_Classifier,scaled_X_temp,Y,cv = 10,scoring='accuracy').mean()\nprint(cv_dt_score)","a93f0111":"from sklearn.ensemble import RandomForestClassifier\n\nrf_Classifier = RandomForestClassifier(n_estimators=10, max_depth=None, random_state=0)\ncv_rf_score = cross_val_score(rf_Classifier,scaled_X_temp, Y,cv = 8,scoring='accuracy').mean()\nprint(cv_rf_score)","6ee16d25":"from xgboost import XGBClassifier\n\nXGB_Classifier = XGBClassifier(n_estimators = 1000,learning_rate = 0.01)\ncv_XGB_score = cross_val_score(XGB_Classifier,scaled_X_temp, Y,cv = 18,scoring='accuracy').mean()\nprint(cv_XGB_score)","fab99658":"from sklearn.ensemble import VotingClassifier\n\neclf = VotingClassifier(\n    estimators=[('xgb',XGB_Classifier), ('lr', logistic), ('rf', rf_Classifier), ('dt', dt_Classifier),('svm',svm_Classifier)],\n    voting='hard'\n)\n\n# let's see all of our models accuracy til now\nfor clf, label in zip([XGB_Classifier, logistic, rf_Classifier, dt_Classifier,svm_Classifier, eclf], \n                      ['XGBooster Classifier', 'Logistic Regression', 'Random Forest', 'Decision Tree','SVM Classifier', 'Ensemble']):\n    scores = cross_val_score(clf, scaled_X_temp, Y, scoring='accuracy', cv=10)\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n    ","33b40fd3":"from sklearn.model_selection import RandomizedSearchCV\nparams = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200],\n          'xgb__n_estimators': [500,2000],'xgb__learning_rate': [0.01,0.1]}\nrands = RandomizedSearchCV(estimator=eclf, param_distributions=params, cv=5)\nrands = rands.fit(scaled_X_temp, Y)\n#predicting\nY_pred_rands = rands.predict(X_valid)\n\n#estimating\nrands_cm = confusion_matrix(Y_valid,Y_pred_rands)\nrands_acc = accuracy_score(Y_valid,Y_pred_rands)\nprint(rands_acc)\nprint(rands_cm)","1128dc27":"test_set = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_set","4a38d812":"test_set.info()","367c71e3":"test_set.describe()","7e8b5660":"#Create a copy\ntest = test_set.copy()\n\n# Creating family_Size\ntest['family_Size'] = test['SibSp'] + test['Parch']\n\n# Dropping unused features\ntest = test.drop(['Name','SibSp','Parch','Cabin','Ticket'],axis = 1)\n\n# Encoding\nencoded_features = {'Sex' : {'male': 1,'female': 0 },\n                   'Embarked':{'S': 0.,'C': 1.,'Q': 2.}}\ntest.replace(encoded_features, inplace = True)\ntest","258f5982":"fig,ax = plt.subplots(figsize = (10,10))\nsb.heatmap(test.corr(),annot = True,fmt = '0.1',linewidth = 1)","8921a932":"from sklearn.linear_model import LinearRegression\n# Filling null-values\n# Age --> mean\ntest.Age = imputer.fit_transform(test.Age.values.reshape(-1,1))\n# Fare --> predicting its value with linearregression\nnotnull_samples = test[test.columns].dropna()\n\nX_set = notnull_samples.loc[:,['Pclass', 'Sex', 'Age', 'Embarked','family_Size']]\nY_set = notnull_samples.loc[:,['Fare']]\n\nlinreg = LinearRegression()\nlinreg.fit(X_set, Y_set)\nfare_predict = linreg.predict(test.loc[test.Fare.isnull(),X_set.columns])\n\ntest.loc[test.Fare.isnull(),'Fare'] = fare_predict","86f30979":"test.info()","b273d2de":"scaled_test = pd.concat([test.loc[:,['PassengerId']],pd.DataFrame(sc.fit_transform(test.drop(['PassengerId'],axis = 1)),\n                                                        columns = test.drop(['PassengerId'],axis = 1).columns)],axis = 1)\n","6c197412":"scaled_test","4c945fae":"eclf.fit(scaled_X_temp,Y)\nSurvived = pd.DataFrame(eclf.predict(scaled_test.drop(['PassengerId'],axis = 1)),columns = ['Survived'])\nSurvived","b9466ebe":"submission = pd.concat([scaled_test.loc[:,'PassengerId'],Survived],axis =1)\nsubmission","3e50b3b7":"submission.to_csv('submission.csv', index=False)","11f17aa6":"![](https:\/\/s3-prod.adage.com\/s3fs-public\/styles\/width_1024\/public\/RMS_Titanic_3_TedSmith.jpg)","49dd6a46":"* Is there a realtion with Age and Survival?\n* How about with the Sex attribute?","20fbe25c":"<a id='fact_3'><\/a>\n> __FACT__ 3: Each Deck had three classes in Titanic. First, Second, Third. That they have priority over each other, respectively. So, Obviously the Fare of each Tickets depends on the classes. \n> \n> But, do the classes exclusively, belong to an indivisual Deck?\n> (Let's have a look at the true picture of Titanic classes distribution)\n> ![image.png](attachment:image.png)\n> \n> according to the distribution above(showing in the picture), the answer is 'No'. which means we can't judge the survivorial rate through the Deck itself.\n\nBut let's see what the dataset says. According to what we've done so far, there are some problems with keeping the Deck attribute and trying to predict the null values of it. First, we have just 33 percent of its data and maybe there are some more decks we do not know about them ( we just had the dataset itself, not the information about the Titanic of Kaggle). Second, we can't find a relevant relationship between Deck and other attributes. and the reason that we saw the remarkable increase in the relation between Deck and pclass is just because of that we reduced the dimensionality of the Cabin attribute to just 8 unique values of Deck, not because of reaching some relations to decide whether we can predict them with classification methods or not. And also as we see in Fact 3, the classes are separated from the Decks in concept. \nIn another perspective, even if we knew the location of the passengers on the ship, maybe the passengers at that moment were on the board or somewhere else and had easier and faster access to the life-boats.\n\nSo it is rational to drop the feature.\n","02a84e22":"<a id= 'subsection_2_9'><\/a>\n## 8. Voting Classifier","30c4a6d8":"to see correaltion between attributes first let's encode the categorical values.\n\ncategorical values :\n\n1. Sex\n2. Ticket\n3. Cabin\n4. Embarked\n\nWe want, temporarily, encode them by Ordinal encoder, except for Cabin. the reason for separating the Cabin attribute is that we just have about 33% of its data. additionally, cabins are not going to have a relation with the age of the passengers, obviously.","1251f362":"<a id= 'subsection_2_7'><\/a>\n## 6. Random Forest","dccb4c8f":"<a id= 'subsection_1_1_2'><\/a>\n### 2.Embarked Attribute","5048dc38":"That's really nice which means all of our features needed to be in our dataset and are critical.\nLet's evaluate the accuracy of our LogisticRegression.","dc0a4e00":"so we get the cabins of the deck C now we have to do it with all of the decks and group them together to count the survivors for each deck.","0f779c0d":"<a id='subsection_1_1_3'><\/a>\n### 3. Cabin Attribute:","4f83312a":"<a id=\"section_1\"><\/a>\n# Data Analyzing and Visualizing","5c1676a2":"<a id='subsection_1_1_1'><\/a>\n### 1. First looking at the Age attribute:","7757e451":"We've just examined and fit one model. In the following we are going to use other models with cross-validation and compare them with each-others.","0a5e14b4":"### That's insane!! \nBut we have to be aware of overfitting.\nAs a matter of fact, for small datasets like Titanic these methods, highly push the models toward overfitting.\nSo because of this problem we are not going to get predictions out of it. instead, we had more stable methods like voting classification.\n\n#### Last steps: importing test set, cleaning, and making our predictions.","75a9a518":"Let's see the collinearity of the test dataset with heatmap.","718fbd82":"XGBosting stands for __Extreme Gradiant Booting__. So to figure it out, let's first define gradient boosting.\n\nGradient boosting is another ensemble method that iteratively injects models, respectively get the loss of each to learn and correct the new one in a cycle. The learning process benefits from Gradient Descent's grace.\n\n![](https:\/\/i.imgur.com\/MvCGENh.png)\n\nAnd XGBoosting is an implementation of gradient boosting with several additional features focused on performance and speed.","fb5d5bba":"We do not have strong relationships between them, at first glance.\nNow let's plot some relationships we think they may have to have, and answer some pop-up questions.\n\n","403d309f":"Now Let's see the Ticket attribute ralations with the Fare and Survival rate to decide what we can do.","3f30d04b":"As we see, there are 146 unique values over 204 non-null samples of cabin attribute.\nExplicitely, thats too much to compare each cabin and also it's not right to do that so. but what should we do now(?).\n<a id='fact_2'><\/a>\n> __FACT 2:__\n> Each cabin, as we can see in the list, start with a capital alphabet letter.\n> And that is the loaction of each cabin in the decks of the titanic ship.\n> The decks are the layouts and foundations of every ships to separate the segments of the ships and better adress each passengers via their tickets(?).\n>\n> As a matter of fact, Titanic ship had 10 decks in total. From top to bottom they were the Boat Deck, the Promenade Deck (deck A), passenger decks B to G, Orlop Deck, and the Tank Top.(as image below)\n> ![image.png](attachment:image.png)\n> When the Titanic hits an iceberg, a series of holes created below the waterline. And that caused the water to penetrate into the ship. But at that time because they could not alert everyone in the decks below or even above to evacuate the ship to save their lives. \n\nAlthough now, we know the facts, we can not use them to engineer our features. However we can examine this, by separating cabins by their decks. Then plot our data to decide what should we do and see which deck had more survivors.","8504c1b5":"As in the trainset, we have null values in Cabin and Age attributes, but we have one null value in Fares as well.\nSo in the following, we are going to do the same changes for the test set as done in the trainset. additionally, with respect to the observations and analysis of the dataset, we will fill the null Fare value with the help of P-class, Family_size.","b6c94fcf":"__From now and then we cross-validate each algorithm__","7d30e358":"# A Survey on Titanic Irritant Case","1ace5659":"The question here is what happened if we ensemble some models together and get advantagous of each of them to get the best result out of them?\nthe answer is ensemble methods like Voting Classifier.\n\n> A voting classifier isn\u2019t an actual classifier but a wrapper for a set of different ones that are trained and evaluated in parallel in order to exploit the different peculiarities of each algorithm.\n\n![](https:\/\/imgur.com\/eAruUj3.png)\n\nIn the hard voting mode, the voting classifier aggregate each prediction of the models fitted in the estimator and the we get the mode of prediction to conclude th eresult for each sample.","4dc6b0a5":"First of all, I prefer using the catplot to see which Deck(there are about 204 non-null samples) saves more lives.","5c642fcf":"That's nice. We can say the deck B, D, E have more survivors and deck T is the outlier data.\nBut unfortunately, that's not true!! Because we are in lack of data. We plotted just about 33% of our dataset.\n\n__the real question here is how we can fill the null values? Or the better question should be \"do we really need this attribute(Deck)?\"__ \n\n__Lets have a look at the relationships between the attributes to decide what we can do with them.__\n\n*(we can replace cabin attribute with Deck)*","fc496de8":"But Let's do it multiple time instead of just fitting the model one time.","01f7aaeb":"* we have to feature-engineer on the dataset itself without having judge about the background of it and see what the statistics say.\n* do not drop a feature just because of loss of data. have a clear vision of the relationships between the features( using heatmaps alongside the project, highly recommended) maybe you can retrieve the lost data's, which then can make you feel like finding gold under ashes\n* use multiple models to predict do not fear of examining even dumbest things, because the Diamonds of successes are under the water of curiousness with a tar layer of fear over it.","fb9f9032":"Let's have a look at the relations between attributes to decide whether we need to combine them and decide which one of them is more crucial?","edcf16d8":"That was nice to gather more than one classifier and gain the advantages of each. But other critical criteria is the tunning of the parameters of the models, which needs to be set accurately. But it is hard to set all of the possible combinations of each of the parameters in even a single model like XGBoosting.\n\nIn this situation, the Grid and Randomized Search approach and approach help us.\nThey are known as Hypertunner. Which means tune the n numbers of parameters of n numbers models.\n\n![](https:\/\/miro.medium.com\/max\/3266\/1*Xq9OvMKXhrF3W2RBJCWW7w.png)\n\n\nThe difference here is that the Gridsearch approach selects parameters in all possible combinations from first to end. Instead,  Randomized search is much faster than Grid search by using random combinations of parameters to reach the best combination.  So we will use RandomizedSearch.","4b2191c5":"As we can see here, there are 891 reports with the total number of 12's columns. and The Survived attribute seeming to be our label class.\n\nHere also we can decide to drop some attributes such as Name and PassengerId since they aren't going to affect our prediction.\n\nBut the most considerable point here is the missing values of some attributes. They have to be handled.","781da036":"Table of Contents:\n* [Data Analyzing and Visualizing](#section_1)\n    * [Handling Missing Values](#subsection_1_1)\n        * [Age](#subsection_1_1_1)\n        * [Embarked](#subsection_1_1_2)\n        * [Cabin](#subsection_1_1_3)\n    * [Feature Selection and Combination](#subsection_1_2)\n    \n    \n* [Regularization and Models](#section_2)\n    * [LogisticRegression](#subsection_2_1)\n    * [K-Nearest-Neighbor](#subsection_2_2)\n    * [Cross-validation Schema](#subsection_2_3)\n    * [Kernel SVM](#subsection_2_4)\n    * [Naive Bayes](#subsection_2_5)\n    * [Decision Tree](#subsection_2_6)\n    * [Random Forest](#subsection_2_7)\n    * [XGBClassifier](#subsection_2_8)\n    * [Voting Classification](#subsection_2_9)\n    * [Grid\/Randomized Search](#subsection_2_10)\n    \n    \n* [Test Set Preparation](#section_3)\n* [Conclusion](#section_4)\n\n","f6f605a0":"<a id='subsection_1_2'><\/a>\n## Feature Selection and Combination","368a4ff0":"<a id=\"subsection_1_1\"><\/a>\n## Handling Missing Values","b69c5e58":"At first glance at heatmap, we can see that there isn't any high correlation between the Parch and SibSp and the Survival rate, but they have a higher relation with each other. So we can combine them as family_Size.","3e22c816":"<a id= 'subsection_2_1'><\/a>\n## 1. LogisticRegression","e7a776ed":"As we can see here, our prediction was true. the first class is more expensive than from the second, and the second one from the third.\n\nLet's plot it by one of the best one violinplot to have better consideration. and see the relation with the survival rate.\n\n","8b718664":"<a id= 'subsection_2_3'><\/a>\nWe got the best result out of knn by repeating it 10 times and each of the time, we used 0.1 of the data for validation and the rest for the train set.\n![image.png](attachment:image.png)","c4713157":"So now our feature engineering was completed.\nLet's test it with regularization.","c4419e86":"We will use L2 regularization with logistic regression to see the effect of our features.","3d7a5e1b":"So, we have three incomplete columns:\n1. Age,\n2. Embarked,\n3. Cabin\n\nBut how should we treat them?\nThe answer is laid under the correlation of each with the other attributes and most importnatly, the amount of their effect on the output.\n__We can plot them to see these effects.__","e6d7663e":"Our goal is to see which model is going to classify better and get the accuracy of each model to compare.\nI'll use cross-validation for each model and in the end, use XGBoost which is the combination of all the methods we are going to fit.","3792a087":"<a id= 'section_2'><\/a>\n# Regularization, Models, Prediction","c256b9cc":"<a id= 'subsection_2_10'><\/a>\n## 9. Grid\/Randomized Search","af4504fd":"## Submission","6460407f":"# Importing DataSet","9ab29e27":"<a id= 'subsection_2_4'><\/a>\n## 3.Kernel_SVM","60acfa0d":"Our test_set is ready to predict and Submit","010c5ad9":"In this notebook, we are going to have a look at some simple possible approaches for Titanic Challenge with an educational perspective.\n\nAt first, we will use data analyzing methods to clear our data set. Then we are going to implement supervised machine learning models. And at the end, comparing each of the results to conclude which method is better and why?","b6ba1734":"now Let's scale it and then make our prediction.","1a192957":"Let's see if we have relations with other attributes to decide how we can fill the null values of Ages.","200c282f":"Random forest is a method of gathering more than one tree to reach more reliable predictions than DecisionTree which is considered as an ensemble method. By definition, ensemble methods combine the predictions of several models (e.g., several trees, in the case of random forests).\n![image.png](attachment:image.png)\n\nWe set our number of trees by the n_estimators parameter in the codes below.","5cd0a8ed":"Let's now see the Heatmaps with new updates.","b5668313":"<a id= 'subsection_2_5'><\/a>\n## 4.Naive Bayes","ff202fdc":"To answer these two questions, I prefer using swarplot.","2d22c106":"We'll fill na-values with the most-frequent.","ae884af2":"we started a journey from feature engineering to implementing machine learning.\nthe most crucial part was to choose whether we need a feature or not and how we can combine and generate new ones, with the help of the relations of them in our dataset.\nAnd we need to have a clear vision of how to choose different models and importantly how to tune the parameters to get the most out of it, severally not deceiving by the accuracy.","92647d73":"# Importing Basic Libararies","4a60d5b4":"Now we are going to use family_Size and Pclass to predict Fare because they are highly correlated with each other. We ignore the has_Deck for prediction because we are in loss of data of it.","63e7ba12":"Now we can rest assured that, according to the distribution, our prediction mostly lays under the passengers' wealth.(!?)\n\nBut what's about the Tickets?\nin my opinion we need to drop the tickets as we drop the Name and cabins.","4d94ea37":"Table of Facts:\n* [Fact 1](#fact_1)\n* [Fact 2](#fact_2)\n* [Fact 3](#fact_3)","adc18ab4":"<a id= 'subsection_2_2'><\/a>\n## 2. K-Nearest-Neighbor","4e0bf4e9":"#### Let's make it clear. What is inside our dataset?","dffa94b1":"<a id= 'subsection_2_6'><\/a>\n## 5. Decission Tree","3fabba87":"<a id= 'section_3'><\/a>\n# Test Set Preparation","0d4cd08a":"Now we can see there is kind of a relation between Deck, P-class, Fare, Embarked(near 1 or -1) But we cannot rely on this we'll discuss in the next cells!","d4be4f3e":"<a id= 'section_4'><\/a>\n# Conclusion","d676a9d2":"that's cool. we got some information from the plot above. Males have more potential to be condemned to death than females.\n<a id='fact_1'><\/a>\n> FACT 1: As a matter of fact, while the Titanic was sinking under the water, in a humanitarian gesture, first the crew of the ship let the women and children get on the lifeboats. So this enforces us to conclude we had a poor survival rate in men.\n> ![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6e\/St%C3%B6wer_Titanic.jpg)\n\nWe will consider it soon.\nBeyond of the Sex attribute, we can see that Age is not a good factor for seprating calsses.\n\nthe possible solution here is that we can fill the null samples of Age with the 'most-frequent' or by 'mean'.\nI'll used to fill the nulls in this situations, seemingly, with the mean of the whole existing samples.","5712c5cb":"<a id= 'subsection_2_8'><\/a>\n## 7. XGBoosting Classifier","f99f3242":"Using classification models, except for LogisticRegression, such as RandomForest, SVM, KNN and others need to feed scaled data. So at first, we will use a scaler, MinMax scaler preferred, to prepare our dataset.","7843e298":"On top of our crucial attributes, we have the cabins of passengers which can give us better information about the location of each passenger on the ship.\nThe importance of cabins and the position of passengers is obviously because, as we are in the upper level of a ship, we have more chances to be out of it when it's going to drown. But unfortinately we loss much af the data of cabin attribute but seemingly it has kind of arelation with the \"Fare\" of the tickets.\n\nSo, our goal is to have a view on the distribution of deaths and survivors for each cabin to see which cabin saves more lives.\n\nTo reach this goal, first, let's have a look at the unique values of the cabin attribute."}}