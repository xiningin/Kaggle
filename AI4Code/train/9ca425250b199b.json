{"cell_type":{"f938c2b2":"code","29482db7":"code","7733ea9a":"code","2130a6c8":"code","d7107462":"code","7bfe93ef":"code","eef00d89":"code","105113e9":"code","f4fd1161":"code","16acca9b":"code","3f1aabf7":"code","3f121ce8":"code","a7212bbc":"code","4289b162":"code","ba8756bd":"code","e88647ef":"code","e01083b3":"code","d7bbeb4a":"code","cb1094a8":"code","a2425e27":"code","f41924e7":"code","fb8703e8":"code","4a272092":"code","89ca4986":"code","f95f2a25":"code","3c70cbc8":"code","cc0e1081":"code","d6bcc07f":"code","e0fdb4cd":"code","0da8fb53":"code","b9182442":"code","601e9843":"code","65755588":"code","9e45795c":"code","8238ba27":"code","10de1d88":"code","30ea189a":"code","fb090170":"code","af4aa33a":"code","c61c6a6e":"code","2c80eb5f":"code","7d603129":"code","fefe234a":"code","8ee6c219":"code","eb5bc376":"code","7b684585":"code","fd4d14dc":"code","0a5e57c1":"code","34078bce":"code","487229d2":"code","2d9beb7e":"code","67ca2645":"code","3c4fc3c7":"code","4ce30bda":"code","88a24755":"code","a27deb51":"code","00a51c63":"code","9d34276e":"code","57a453c6":"code","99568eae":"code","71d1cbcd":"code","8dd5781b":"code","228effc8":"code","ab3b5b2f":"markdown","187fccae":"markdown","4ca9fb57":"markdown","fe1d6d7b":"markdown","564e2dec":"markdown","b3e7b167":"markdown","de76ca50":"markdown","c2614c21":"markdown","a6d7d873":"markdown","63148156":"markdown","f7c6abee":"markdown","82ff7aa2":"markdown","8482d8a3":"markdown","a48d7db5":"markdown","1d4fd6b5":"markdown","cf6dfeb4":"markdown","ac5a80ac":"markdown","66bb18f8":"markdown","41f3e1da":"markdown","25f1d7d7":"markdown","61bb714f":"markdown","efb4d3f8":"markdown","c5aca18c":"markdown","c5fca3f6":"markdown","c844fe11":"markdown","407af714":"markdown","e40ae21f":"markdown","ca7fc5ee":"markdown","034a487e":"markdown","f53f17c3":"markdown","fc974bbb":"markdown","fa82e039":"markdown","590d3d4d":"markdown","6c0b1dc4":"markdown","e8664b71":"markdown","48b40a3c":"markdown","4b1822e3":"markdown","a5a37498":"markdown","d668cd5e":"markdown","1496c01a":"markdown","35e6722b":"markdown","a4184f24":"markdown","086147de":"markdown","fff53fd7":"markdown","053feb9e":"markdown","55bd65cd":"markdown","83a763e0":"markdown","b345931e":"markdown","60b832e9":"markdown","fd211bef":"markdown","65d926a5":"markdown","cfd6dfcf":"markdown","4df06152":"markdown","dc56123d":"markdown","c39b7c4d":"markdown","4273fb28":"markdown","ee20562e":"markdown","74023ecf":"markdown","65d25c64":"markdown","884fa492":"markdown","74e9ab20":"markdown","6be4c144":"markdown","5a5d593d":"markdown"},"source":{"f938c2b2":"import matplotlib.patches as patch\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom scipy import linalg\nfrom numpy import poly1d\nfrom sklearn import svm\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport glob\nimport sys\nimport os","29482db7":"%matplotlib inline\n%precision 4\nplt.style.use('ggplot')\nnp.set_printoptions(suppress=True)","7733ea9a":"# let see how to create a multi dimentional Array with Numpy\na = np.zeros((2, 3, 4))\n#l = [[[ 0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.],\n     #     [ 0.,  0.,  0.,  0.]],\n     #     [[ 0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.],\n     #     [ 0.,  0.,  0.,  0.]]]\nprint(a)\nprint(a.shape)","2130a6c8":"\n# Declaring Vectors\n\nx = [1, 2, 3]\ny = [4, 5, 6]\n\nprint(type(x))\n\n# This does'nt give the vector addition.\nprint(x + y)\n\n# Vector addition using Numpy\n\nz = np.add(x, y)\nprint(z)\nprint(type(z))\n\n# Vector Cross Product\nmul = np.cross(x, y)\nprint(mul)","d7107462":"# initializing matrices \nx = np.array([[1, 2], [4, 5]]) \ny = np.array([[7, 8], [9, 10]])","7bfe93ef":"# using add() to add matrices \nprint (\"The element wise addition of matrix is : \") \nprint (np.add(x,y)) ","eef00d89":"# using subtract() to subtract matrices \nprint (\"The element wise subtraction of matrix is : \") \nprint (np.subtract(x,y)) ","105113e9":"# using divide() to divide matrices \nprint (\"The element wise division of matrix is : \") \nprint (np.divide(x,y)) ","f4fd1161":"# using multiply() to multiply matrices element wise \nprint (\"The element wise multiplication of matrix is : \") \nprint (np.multiply(x,y))","16acca9b":"x = [1, 2, 3]\ny = [4, 5, 6]\nnp.cross(x, y)","3f1aabf7":"x = np.array([1, 2, 3, 4])\ny = np.array([5, 6, 7, 8])\nprint(\"x:\", x)\nprint(\"y:\", y)","3f121ce8":"np.dot(x, y)","a7212bbc":"np.dot(y, x)","4289b162":"print(\"x:\", x)\nx.shape = (4, 1)\nprint(\"xT:\", x)\nprint(\"y:\", y)\ny.shape = (4, 1)\nprint(\"yT:\", y)","ba8756bd":"x = np.array([1, 2, 3, 4])\ny = np.array([5, 6, 7, 8])\nprint(\"x:\", x)\nprint(\"y:\", y)\nprint(\"xT:\", x.T)\nprint(\"yT:\", y.T)","e88647ef":"x = np.array([[1, 2, 3, 4]])\ny = np.array([[5, 6, 7, 8]])\nprint(\"x:\", x)\nprint(\"y:\", y)\nprint(\"xT:\", x.T)\nprint(\"yT:\", y.T)\n","e01083b3":"print(\"x:\", x)\nprint(\"y:\", y.T)\nnp.dot(x, y.T)","d7bbeb4a":"print(\"x:\", x.T)\nprint(\"y:\", y)\nnp.dot(y, x.T)","cb1094a8":"np.dot(y, x.T)[0][0]","a2425e27":"x = np.array([[1, 2, 3, 4]])\nprint(\"x:\", x)\nprint(\"xT:\", np.reshape(x, (4, 1)))\nprint(\"xT:\", x.T)\nprint(\"xT:\", x.transpose())","f41924e7":"x = np.array([[1, 2, 3, 4]])\ny = np.array([[5, 6, 7, 8]])\nx.T * y","fb8703e8":"np.outer(x, y)","4a272092":"x = np.array([1, 2, 3, 4])\ny = np.array([5, 6, 7, 8])\nnp.outer(x, y)","89ca4986":"a = np.array([[ 5, 1 ,3], [ 1, 1 ,1], [ 1, 2 ,1]])\nb = np.array([1, 2, 3])\nprint (a.dot(b))","f95f2a25":"A = np.array([[4, 5, 6],\n             [7, 8, 9]])\nx = np.array([1, 2, 3])\nA.dot(x)","3c70cbc8":"a = [[1, 0], [0, 1]]\nb = [[4, 1], [2, 2]]\nnp.matmul(a, b)","cc0e1081":"matrix1 = np.matrix(a)\nmatrix2 = np.matrix(b)","d6bcc07f":"matrix1 + matrix2","e0fdb4cd":"matrix1 - matrix2","0da8fb53":"np.dot(matrix1, matrix2)","b9182442":"\nmatrix1 * matrix2","601e9843":"np.identity(3)","65755588":"identy = np.array([[21, 5, 7],[9, 8, 16]])\nprint(\"identy:\", identy)","9e45795c":"identy.shape","8238ba27":"np.identity(identy.shape[1], dtype=\"int\")","10de1d88":"np.identity(identy.shape[0], dtype=\"int\")","30ea189a":"inverse = np.linalg.inv(matrix1)\nprint(inverse)","fb090170":"import numpy as np\nA = np.array([[0,   1,  2,  3],\n              [4,   5,  6,  7],\n              [8,   9, 10, 11],\n              [12, 13, 14, 15]])\nnp.diag(A)","af4aa33a":"np.diag(A, k=1)","c61c6a6e":"np.diag(A, k=-1)","2c80eb5f":"a = np.array([[1, 2], [3, 4]])\na","7d603129":"a.transpose()","fefe234a":"N = 100\nb = np.random.random_integers(-2000,2000,size=(N,N))\nb_symm = (b + b.T)\/2","8ee6c219":"np.trace(np.eye(3))","eb5bc376":"print(np.trace(matrix1))","7b684585":"det = np.linalg.det(matrix1)\nprint(det)","fd4d14dc":"v = np.array([1,2,3,4])\nnorm.median(v)","0a5e57c1":"#How to find linearly independent rows from a matrix\nmatrix = np.array(\n    [\n        [0, 1 ,0 ,0],\n        [0, 0, 1, 0],\n        [0, 1, 1, 0],\n        [1, 0, 0, 1]\n    ])\n\nlambdas, V =  np.linalg.eig(matrix.T)\n# The linearly dependent row vectors \nprint (matrix[lambdas == 0,:])","34078bce":"import numpy as np\nprint(\"np.arange(9):\", np.arange(9))\nprint(\"np.arange(9, 18):\", np.arange(9, 18))\nA = np.arange(9, 18).reshape((3, 3))\nB = np.arange(9).reshape((3, 3))\nprint(\"A:\", A)\nprint(\"B:\", B)","487229d2":"A + B","2d9beb7e":"A - B","67ca2645":"x = np.array([[1,2],[3,4]]) \ny = np.linalg.inv(x) \nprint (x )\nprint (y )\nprint (np.dot(x,y))","3c4fc3c7":"## based on https:\/\/stackoverflow.com\/questions\/38426349\/how-to-create-random-orthonormal-matrix-in-python-numpy\ndef rvs(dim=3):\n     random_state = np.random\n     H = np.eye(dim)\n     D = np.ones((dim,))\n     for n in range(1, dim):\n         x = random_state.normal(size=(dim-n+1,))\n         D[n-1] = np.sign(x[0])\n         x[0] -= D[n-1]*np.sqrt((x*x).sum())\n         # Householder transformation\n         Hx = (np.eye(dim-n+1) - 2.*np.outer(x, x)\/(x*x).sum())\n         mat = np.eye(dim)\n         mat[n-1:, n-1:] = Hx\n         H = np.dot(H, mat)\n         # Fix the last sign such that the determinant is 1\n     D[-1] = (-1)**(1-(dim % 2))*D.prod()\n     # Equivalent to np.dot(np.diag(D), H) but faster, apparently\n     H = (D*H.T).T\n     return H","4ce30bda":"from scipy.linalg import null_space\nA = np.array([[1, 1], [1, 1]])\nns = null_space(A)\nns * np.sign(ns[0,0])  # Remove the sign ambiguity of the vector","88a24755":"a = np.array([[1, 2], [3, 4]])\nnp.linalg.det(a)","a27deb51":"# credits: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/Variable\nA = tf.Variable(np.zeros((5, 5), dtype=np.float32), trainable=False)\nnew_part = tf.ones((2,3))\nupdate_A = A[2:4,2:5].assign(new_part)\nsess = tf.InteractiveSession()\ntf.global_variables_initializer().run()\nprint(update_A.eval())","00a51c63":"##based on this address: https:\/\/stackoverflow.com\/questions\/46511017\/plot-hyperplane-linear-svm-python\nnp.random.seed(0)\nX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\nY = [0] * 20 + [1] * 20\n\nfig, ax = plt.subplots()\nclf2 = svm.LinearSVC(C=1).fit(X, Y)\n\n# get the separating hyperplane\nw = clf2.coef_[0]\na = -w[0] \/ w[1]\nxx = np.linspace(-5, 5)\nyy = a * xx - (clf2.intercept_[0]) \/ w[1]\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx2, yy2 = np.meshgrid(np.arange(x_min, x_max, .2),\n                     np.arange(y_min, y_max, .2))\nZ = clf2.predict(np.c_[xx2.ravel(), yy2.ravel()])\n\nZ = Z.reshape(xx2.shape)\nax.contourf(xx2, yy2, Z, cmap=plt.cm.coolwarm, alpha=0.3)\nax.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.coolwarm, s=25)\nax.plot(xx,yy)\n\nax.axis([x_min, x_max,y_min, y_max])\nplt.show()","9d34276e":"np.mgrid[0:5,0:5]","57a453c6":"a=np.array([1,2,3])\nb=np.array([(1+5j,2j,3j), (4j,5j,6j)])\nc=np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]])","99568eae":"np.transpose(b)","71d1cbcd":"b.flatten()","8dd5781b":"np.hsplit(c,2)","228effc8":"p=poly1d([3,4,5])\np","ab3b5b2f":"Note, in this simple case using the simple arrays for the data structures of the vectors does not affect the result of the *outer* function:","187fccae":"We see the issues when we try to transform the array objects. Usually, we can transform a row vector into a column vector in *numpy* by using the *T* method on vector or matrix objects:\n###### [Go to top](#top)","4ca9fb57":"In fact, in our understanding of Linear Algebra, we take the arrays above to represent **row vectors**. *Numpy* treates them differently.","fe1d6d7b":"We can now add and subtract the two matrices $A$ and $B$:","564e2dec":"### 20-1 Create a dense meshgrid","b3e7b167":"## 20-3 Polynomials","de76ca50":"The order of the arguments is irrelevant:","c2614c21":"We can now compute the **outer product** by multiplying the column vector $x$ with the row vector $y$:","a6d7d873":"Or by reverting to:","63148156":"<a id=\"13\"><\/a> <br>\n## 13- Orthogonal Matrices\nHow to create random orthonormal matrix in python numpy","f7c6abee":"Note that the *numpy* functions *dot* and *outer* are not affected by this distinction. We can compute the dot product using the mathematical equation above in *numpy* using the new $x$ and $y$ row vectors:\n###### [Go to top](#top)","82ff7aa2":"To read the result from this array of arrays, we would need to access the value this way:","8482d8a3":"A [**tensor**](https:\/\/en.wikipedia.org\/wiki\/Tensor) could be thought of as an organized multidimensional array of numerical values. A vector could be assumed to be a sub-class of a tensor. Rows of tensors extend alone the y-axis, columns along the x-axis. The **rank** of a scalar is 0, the rank of a **vector** is 1, the rank of a **matrix** is 2, the rank of a **tensor** is 3 or higher.\n\n###### [Go to top](#top)","a48d7db5":"<a id=\"41\"><\/a> <br>\n## 4-1 Vector-Vector Products\n\nnumpy.cross(a, b, axisa=-1, axisb=-1, axisc=-1, axis=None)[source]\nReturn the cross product of two (arrays of) vectors.[scipy](https:\/\/docs.scipy.org\/doc\/numpy-1.15.0\/reference\/generated\/numpy.cross.html)\n<img src='http:\/\/gamedevelopertips.com\/wp-content\/uploads\/2017\/11\/image8.png'>\n[image-credits](http:\/\/gamedevelopertips.com)","1d4fd6b5":"<a id=\"4\"><\/a> <br>\n## 4- Matrix Multiplication\n<img src='https:\/\/www.mathsisfun.com\/algebra\/images\/matrix-multiply-constant.gif'>\n\n[mathsisfun](https:\/\/www.mathsisfun.com\/algebra\/matrix-multiplying.html)","cf6dfeb4":"<a id=\"14\"><\/a> <br>\n## 14- Range and Nullspace of a Matrix","ac5a80ac":"We define the vectors $x$ and $y$ using *numpy*:","66bb18f8":"We can now calculate the $dot$ or $inner product$ using the *dot* function of *numpy*:","41f3e1da":"<a id=\"3\"><\/a> <br>\n## 3- Notation\n<img src='http:\/\/s8.picofile.com\/file\/8349058626\/la.png'>\n[linear.ups.edu](http:\/\/linear.ups.edu\/html\/notation.html)","25f1d7d7":"Example\n###### [Go to top](#top)","61bb714f":"$C_{ij}=\\sum_{k=1}^n{A_{ij}B_{kj}}$\n<img src='https:\/\/cdn.britannica.com\/06\/77706-004-31EE92F3.jpg'>\n[reference](https:\/\/cdn.britannica.com\/06\/77706-004-31EE92F3.jpg)","efb4d3f8":"The result of the multiplication of two matrixes $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$ is the matrix:","c5aca18c":"## SciPy Cheat Sheet: Linear Algebra in Python\nThis Python cheat sheet is a handy reference with code samples for doing linear algebra with SciPy and interacting with NumPy.\n\n[DataCamp](https:\/\/www.datacamp.com\/community\/blog\/python-scipy-cheat-sheet)","c5fca3f6":"<a id=\"9\"><\/a> <br>\n## 9-The Trace\nReturn the sum along diagonals of the array.","c844fe11":"$C = AB \\in \\mathbb{R}^{m \\times n}$","407af714":"<a id=\"11\"><\/a> <br>\n# 11- Linear Independence and Rank\nHow to identify the linearly independent rows from a matrix?","e40ae21f":"<a id=\"25\"><\/a> <br>\n# 17- Hyperplane","ca7fc5ee":"<a id=\"2\"><\/a> <br>\n# 2- What is Linear Algebra?\nLinear algebra is the branch of mathematics concerning linear equations such as\n<img src='https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/f4f0f2986d54c01f3bccf464d266dfac923c80f3'>\nLinear algebra is central to almost all areas of mathematics. [6]\n<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/2f\/Linear_subspaces_with_shading.svg\/800px-Linear_subspaces_with_shading.svg.png' height=400 width=400>\n[wikipedia](https:\/\/en.wikipedia.org\/wiki\/Linear_algebra#\/media\/File:Linear_subspaces_with_shading.svg)\n","034a487e":"numpy.identity(n, dtype=None)\n\nReturn the identity array.\n[source](https:\/\/docs.scipy.org\/doc\/numpy-1.15.0\/reference\/generated\/numpy.identity.html)","f53f17c3":"<a id=\"5\"><\/a> <br>\n## 5- Identity Matrix","fc974bbb":"## 2-1 What is Vectorization?\nIn mathematics, especially in linear algebra and matrix theory, the vectorization of a matrix is a linear transformation which converts the matrix into a column vector. Specifically, the vectorization of an m \u00d7 n matrix A, denoted vec(A), is the mn \u00d7 1 column vector obtained by stacking the columns of the matrix A on top of one another:\n<img src='https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/30ca6a8b796fd3a260ba3001d9875e990baad5ab'>\n[wikipedia](https:\/\/en.wikipedia.org\/wiki\/Vectorization_(mathematics) )","fa82e039":"That is, we are multiplying the columns of $A$ with the rows of $B$:","590d3d4d":"Note that both vectors are actually **row vectors** in the above code. We can transpose them to column vectors by using the *shape* property:","6c0b1dc4":"<a id=\"43\"><\/a> <br>\n## 4-3 Matrix-Vector Products\nUse numpy.dot or a.dot(b). See the documentation [here](http:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.dot.html).","e8664b71":"<a id=\"121\"><\/a> <br>\n## 12-1 Inverse\nWe use numpy.linalg.inv() function to calculate the inverse of a matrix. The inverse of a matrix is such that if it is multiplied by the original matrix, it results in identity matrix.[tutorialspoint](https:\/\/www.tutorialspoint.com\/numpy\/numpy_inv.htm)","48b40a3c":"The number of columns in $A$ must be equal to the number of rows in $B$.\n\n###### [Go to top](#top)","4b1822e3":"Using *numpy* we can compute $Ax$:","a5a37498":"<a id=\"7\"><\/a> <br>\n## 7- Transpose of a Matrix\nFor reading about Transpose of a Matrix, you can visit [this link](https:\/\/py.checkio.org\/en\/mission\/matrix-transpose\/)","d668cd5e":" # <div style=\"text-align: center\">Linear Algebra for Data Scientists \n<div style=\"text-align: center\">\nHaving a basic knowledge of linear algebra is one of the requirements for any data scientist. In this tutorial we will try to cover all the necessary concepts related to linear algebra.","1496c01a":"<a id=\"44\"><\/a> <br>\n## 4-4 Matrix-Matrix Products","35e6722b":"How to create *identity matrix* in *numpy*  ","a4184f24":"### 20-2 Permute array dimensions","086147de":"Vectors of the length $n$ could be treated like points in $n$-dimensional space. One can calculate the distance between such points using measures like [Euclidean Distance](https:\/\/en.wikipedia.org\/wiki\/Euclidean_distance). The similarity of vectors could also be calculated using [Cosine Similarity](https:\/\/en.wikipedia.org\/wiki\/Cosine_similarity).\n###### [Go to top](#top)","fff53fd7":"<a id=\"21\"><\/a> <br>\n# 21-Conclusion\nIf you have made this far \u2013 give yourself a pat at the back. We have covered different aspects of **Linear algebra** in this Kernel. You are now finishing the **third step** of the course to continue, return to the [**main page**](https:\/\/www.kaggle.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/) of the course.  \n\n###### [Go to top](#top)","053feb9e":"<a id=\"8\"><\/a> <br>\n## 8- Symmetric Matrices\nIn linear algebra, a symmetric matrix is a square matrix that is equal to its transpose. Formally,\n<img src='https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/ad8a5a3a4c95de6f7f50b0a6fb592d115fe0e95f'>\n\n[wikipedia](https:\/\/en.wikipedia.org\/wiki\/Symmetric_matrix)","55bd65cd":"The problem here is that this does not do, what we expect it to do. It only works, if we declare the variables not to be arrays of numbers, but in fact a matrix:","83a763e0":"<a id=\"1\"><\/a> <br>\n#  1-Introduction\nThis is the third step of the [10 Steps to Become a Data Scientist](https:\/\/www.kaggle.com\/mjbahmani\/10-steps-to-become-a-data-scientist).\nwe will cover following topic:\n1. notation\n1. Matrix Multiplication\n1. Identity Matrix\n1. Diagonal Matrix\n1. Transpose of a Matrix\n1. The Trace\n1. Norms\n1. Tensors\n1. Hyperplane\n1. Eigenvalues and Eigenvectors\n## What is linear algebra?\n**Linear algebra** is the branch of mathematics that deals with **vector spaces**. good understanding of Linear Algebra is intrinsic to analyze Machine Learning algorithms, especially for **Deep Learning** where so much happens behind the curtain.you have my word that I will try to keep mathematical formulas & derivations out of this completely mathematical topic and I try to cover all of subject that you need as data scientist.\n<img src='https:\/\/camo.githubusercontent.com\/e42ea0e40062cc1e339a6b90054bfbe62be64402\/68747470733a2f2f63646e2e646973636f72646170702e636f6d2f6174746163686d656e74732f3339313937313830393536333530383733382f3434323635393336333534333331383532382f7363616c61722d766563746f722d6d61747269782d74656e736f722e706e67' height=200 width=700>\n[image credit: https:\/\/hadrienj.github.io\/posts\/Deep-Learning-Book-Series-2.1-Scalars-Vectors-Matrices-and-Tensors\/ ](https:\/\/hadrienj.github.io\/posts\/Deep-Learning-Book-Series-2.1-Scalars-Vectors-Matrices-and-Tensors\/)\n <a id=\"top\"><\/a> <br>","b345931e":"<a id=\"31\"><\/a> <br>\n## 20- Exercises\nlet's do some exercise.","60b832e9":"<a id=\"51\"><\/a> <br>\n### 5-1  Inverse Matrices","fd211bef":"<a id=\"10\"><\/a> <br>\n# 10- Norms\nnumpy.linalg.norm\nThis function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the ord parameter. [scipy](https:\/\/docs.scipy.org\/doc\/numpy-1.13.0\/reference\/generated\/numpy.linalg.norm.html)\n\n <a id=\"top\"><\/a> <br>","65d926a5":"*Numpy* provides an *outer* function that does all that:","cfd6dfcf":"<a id=\"12\"><\/a> <br>\n##  1-2 Setup","4df06152":"In *numpy* we can create a *diagonal matrix* from any given matrix using the *diag* function:","dc56123d":"<a id=\"42\"><\/a> <br>\n## 4-2 Outer Product of Two Vectors\nCompute the outer product of two vectors.","c39b7c4d":" <a id=\"top\"><\/a> <br>\n## Notebook  Content\n1. [Introduction](#1)\n1. [What is Linear Algebra?](#2)\n1. [Notation ](#2)\n1. [Matrix Multiplication](#3)\n    1. [Vector-Vector Products](#31)\n    1. [Outer Product of Two Vectors](#32)\n    1. [Matrix-Vector Products](#33)\n    1. [Matrix-Matrix Products](#34)\n1. [Identity Matrix](#4)\n1. [Diagonal Matrix](#5)\n1. [Transpose of a Matrix](#6)\n1. [Symmetric Metrices](#7)\n1. [The Trace](#8)\n1. [Norms](#9)\n1. [Linear Independence and Rank](#10)\n1. [Subtraction and Addition of Metrices](#11)\n    1. [Inverse](#111)\n1. [Orthogonal Matrices](#12)\n1. [Range and Nullspace of a Matrix](#13)\n1. [Determinant](#14)\n1. [Tensors](#16)\n1. [Hyperplane](#17)\n1. [Eigenvalues and Eigenvectors](#18)\n1. [Exercise](#19)\n1. [Conclusion](#21)\n1. [References](#22)","4273fb28":"<a id=\"15\"><\/a> <br>\n# 15-  Determinant\nCompute the determinant of an array","ee20562e":"<a id=\"12\"><\/a> <br>\n# 12-  Subtraction and Addition of Metrices","74023ecf":"<a id=\"11\"><\/a> <br>\n## 1-1 Import","65d25c64":"Hyperplanes divide an $n$-dimensional space into sub-spaces that might represent clases in a machine learning algorithm.","884fa492":"<a id=\"16\"><\/a> <br>\n# 16- Tensors","74e9ab20":"<a id=\"441\"><\/a> <br>\n### 4-4-1  Multiplication","6be4c144":"The **hyperplane** is a sub-space in the ambient space with one dimension less. In a two-dimensional space the hyperplane is a line, in a three-dimensional space it is a two-dimensional plane, etc.","5a5d593d":"<a id=\"6\"><\/a> <br>\n## 6- Diagonal Matrix"}}