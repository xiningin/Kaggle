{"cell_type":{"98fc7930":"code","d3867cbb":"code","ed2cbb42":"code","24ae9706":"code","0ca299f8":"code","295cb3c6":"code","39d6a351":"code","aef82f77":"markdown","ea9a0721":"markdown","57edd0b2":"markdown"},"source":{"98fc7930":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","d3867cbb":"!git clone https:\/\/github.com\/NVIDIA\/apex\n!cd apex; pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\/","ed2cbb42":"!pip install performer-pytorch","24ae9706":"# Try enwik dataset\nfrom performer_pytorch import PerformerLM\n# Calculates loss\nfrom performer_pytorch.autoregressive_wrapper import AutoregressiveWrapper\n\nimport random\n#import tqdm\nfrom tqdm.notebook import tqdm\nimport gzip\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\n# Mixed Precision in PyTorch: - Throws Apex Error (or not?)\nfrom torch.cuda.amp import autocast, GradScaler\n\n# constants\nNUM_BATCHES = 10#int(1e5)\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATE_EVERY = 4\nLEARNING_RATE = 1e-4\nVALIDATE_EVERY  = 100\nGENERATE_EVERY  = 500\nGENERATE_LENGTH = 2048\nSEQ_LEN = 4096\n\n# helpers\ndef cycle(loader):\n    while True:\n        for data in loader:\n            yield data\n\ndef decode_token(token):\n    \"\"\"\n    chr: returns character from num; e.g. chr(97)) -> a; chr of <=32 -> whitespace\n    \"\"\"\n    return str(chr(max(32, token)))\n\ndef decode_tokens(tokens):\n    return ''.join(list(map(decode_token, tokens)))\n\n\n# instantiate model\n\nmodel = PerformerLM(\n    num_tokens = 256,\n    dim = 512,\n    depth = 6,\n    max_seq_len = SEQ_LEN,\n    heads = 8,\n    causal = True,\n    reversible = True,\n    nb_features = 256,\n    use_scalenorm = True,\n    local_attn_heads = (8, 8, 8, 6, 4, 2) # Attention Heads per layer\n)\n\nmodel = AutoregressiveWrapper(model)\nmodel.cuda()\n","0ca299f8":"# prepare enwik8 data\n\nwith open(\"..\/input\/enwikidataset\/enwikinews-dataset.txt\") as file:\n    \n    file = file.read()\n    X = np.fromstring(file, dtype=np.uint8)\n    trX, vaX = np.split(X, [int(len(file)*0.9)])\n    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n    \nclass TextSamplerDataset(Dataset):\n    def __init__(self, data, seq_len):\n        super().__init__()\n        self.data = data\n        self.seq_len = seq_len\n\n    def __getitem__(self, index):\n        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n        return full_seq.cuda()\n\n    def __len__(self):\n        return self.data.size(0) \/\/ self.seq_len\n    \ntrain_dataset = TextSamplerDataset(data_train, SEQ_LEN)\nval_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\ntrain_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\nval_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n\n# optimizer\noptim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscaler = GradScaler()","295cb3c6":"\nfor i in tqdm(range(NUM_BATCHES), desc='training'):\n    model.train()\n    \n    ###\n    ##if i % 500 == 0:\n    #    print(i)\n\n    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n        with autocast():\n            loss = model(next(train_loader), return_loss = True)\n        #loss.backward()\n        scaler.scale(loss).backward()\n        \n    print(f'training loss: {loss.item()}')\n\n    scaler.unscale_(optim)\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n    scaler.step(optim)\n    scaler.update()\n    #optim.step() # TMP\n    \n    optim.zero_grad()\n\n    if i % VALIDATE_EVERY == 0:\n        model.eval()\n        with torch.no_grad():\n            loss = model(next(val_loader), return_loss = True)\n            print(f'validation loss: {loss.item()}')\n\n    if i % GENERATE_EVERY == 0 and i != 0:\n        model.eval()\n        inp = random.choice(val_dataset)[:-1]\n        prime = decode_tokens(inp)\n        print(f'%s \\n\\n %s', (prime, '*' * 100))\n\n        sample = model.generate(inp, GENERATE_LENGTH)\n        output_str = decode_tokens(sample)\n        print(output_str)","39d6a351":"# W\/ AMP, 60secs\n\ntraining loss: 5.719467639923096\nvalidation loss: 5.679171562194824\ntraining loss: 5.688153266906738\ntraining loss: 5.64174747467041\ntraining loss: 5.6018266677856445\ntraining loss: 5.5692853927612305\ntraining loss: 5.528021335601807\ntraining loss: 5.4963812828063965\ntraining loss: 5.4679107666015625\ntraining loss: 5.419412136077881\ntraining loss: 5.399522304534912\n\n\n# W\/o AMP, 57 secs\n\ntraining loss: 5.67160701751709\nvalidation loss: 5.638372421264648\ntraining loss: 5.621777057647705\ntraining loss: 5.606544494628906\ntraining loss: 5.550500869750977\ntraining loss: 5.5118408203125\ntraining loss: 5.481593132019043\ntraining loss: 5.444958686828613\ntraining loss: 5.410432815551758\ntraining loss: 5.354680061340332\ntraining loss: 5.319333553314209","aef82f77":"## Data Preparation","ea9a0721":"## Performer Model","57edd0b2":"## Training"}}