{"cell_type":{"80e26ef4":"code","2d66e3bd":"code","f58b4267":"code","14cfc117":"code","4e9dbaa3":"code","7e1ec939":"code","a91e4aef":"code","c18659d6":"code","5ac86221":"code","5d0fabb6":"code","e972177f":"code","81734295":"code","dcc0d450":"code","8bd76d9b":"code","7ab620a4":"code","e4d44ee7":"code","27c441c3":"code","1715feb7":"code","3633ff28":"code","d9bc5bd4":"code","a6a41863":"code","a0b7dac6":"code","961c911c":"code","23a60f31":"code","4e56bb2c":"code","2f104367":"code","3fbff2ab":"code","b599fd67":"code","ac59da45":"code","18ecbde0":"code","cc65b05b":"code","461018af":"markdown","be97bb47":"markdown","db0e3234":"markdown","b5f70924":"markdown","896e214b":"markdown","2559a00a":"markdown","cb2232c8":"markdown","65644fbf":"markdown","0d043a65":"markdown","4e9145cc":"markdown","2b2ee4d5":"markdown","09d9b06e":"markdown","ccbbf092":"markdown","c56f7f46":"markdown","a1193c8c":"markdown","6826bd2d":"markdown","891ce4a7":"markdown","2c77a746":"markdown","27cd5225":"markdown","2fd9233f":"markdown","abd5427a":"markdown"},"source":{"80e26ef4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ni = 0\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        i += 1\n        if i < 5:\n            print(os.path.join(dirname, filename))\nprint('Printed 5 filenames out of ', i)\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2d66e3bd":"# Genes from : https:\/\/alsod.ac.uk\/\nurl = 'https:\/\/raw.githubusercontent.com\/chervov\/genes\/main\/genes_ALS_from_alsod_ac_uk.csv'\ndf_genes_alsod = pd.read_csv(url)\n\nif 0: # If inet is not available: \n    \n    # Wikipedia selected list: \n    list_genes = ['NEFL', 'C9orf72', 'SOD1',  'TARDBP','FUS', 'NEFH',\n                  'MATR3', 'TUBA4A' ,\n    'ANXA11', 'NEK1',  'TBK1', 'CCNF', 'TIA1',\n                 'ALS2']\n\n    # Genes from : https:\/\/alsod.ac.uk\/\n    list_definitive_ALS_gene = ['ANXA11' ,'C9orf72' ,'CHCHD10' ,'EPHA4' ,'FUS' ,'HNRNPA1' ,'KIF5A' ,'NEK1' ,'OPTN' ,'PFN1' ,'SOD1' ,'TARDBP' ,'TBK1' ,'UBQLN2' ,'UNC13A' ,'VAPB' ,'VCP' ] \n    print(len( list_definitive_ALS_gene  ))\n    list_strong_evidence_ALS_gene = ['ATXN1','CCNF','CFAP410','HFE','NIPA1','SCFD1','TUBA4A']\n    print(len( list_strong_evidence_ALS_gene  ))\n    list_clinical_modifier_ALS_gene =  ['ATXN2','CAMTA1','ENAH']\n    print(len( list_clinical_modifier_ALS_gene  ))\n    list_moderate_evidence_ALS_gene =  ['ANG','ARHGEF28','CDH22','CHMP2B','CNTN6','CRYM','CSNK1G3','CX3CR1','DAO','DNAJC7','DNMT3A','ERBB4','FIG4','GLE1','GPX3','LMNB1','SARM1','SMN1','SQSTM1','SS18L1','TNIP1']\n    print(len( list_moderate_evidence_ALS_gene  ))\n    list_tenuous_ALS_gene = ['ALS2', 'ALS3', 'ALS7', 'APEX1', 'APOE', 'AR', 'ARPP21', 'B4GALT6', 'BCL11B', 'BCL6', 'CCS', 'CDH13', 'CHGB', 'CNTF', 'CNTN4', 'CRIM1', 'CST3', 'CYP2D6', 'DCTN1', 'DIAPH3', 'DISC1', 'DNMT3B', 'DOC2B', 'DPP6', 'DYNC1H1', 'EFEMP1', 'ELP3', 'EphA3', 'ERLIN1', 'EWSR1', 'FEZF2', 'FGGY', 'GARS', 'GLT8D1', 'GRB14', 'GRN', 'HEXA', 'HNRNPA2B1', 'ITPR2', 'KDR', 'KIFAP3', 'LIF', 'LIPC', 'LOX', 'LUM', 'MAOB', 'MAPT', 'MATR3', 'MOBP', 'MTND2P1', 'NAIP', 'NEFH', 'NEFL', 'NETO1', 'NT5C1A', 'ODR4', 'OGG1', 'OMA1', 'PARK7', 'PCP4', 'PLEKHG5', 'PNPLA6', 'PON1', 'PON2', 'PON3', 'PRPH', 'PSEN1', 'PVR', 'RAMP3', 'RBMS1', 'RFTN1', 'RNASE2', 'RNF19A', 'SCN7A', 'SELL', 'SEMA6A', 'SETX', 'SIGMAR1', 'SLC1A2', 'SLC39A11', 'SLC52A3', 'SMN2', 'SNCG', 'SOD2', 'SOX5', 'SPAST', 'SPG11', 'SPG7', 'SUSD1', 'SYNE1', 'SYT9', 'TAF15', 'TIA1', 'TRPM7', 'UBQLN1', 'VDR', 'VEGFA', 'VPS54', 'VRK1', 'ZFP64', 'ZNF512B']\n    print(len(list_tenuous_ALS_gene))\n    len(set(list_definitive_ALS_gene+list_strong_evidence_ALS_gene+list_clinical_modifier_gene+list_moderate_evidence_gene) & set(list_genes)), len(list_genes)\n\nprint(df_genes_alsod['Category'].value_counts()    )\ndf_genes_alsod","f58b4267":"list_fn = ['\/kaggle\/input\/end-als\/end-als\/transcriptomics-data\/DESeq2\/bulbar_vs_limb.csv', \n'\/kaggle\/input\/end-als\/end-als\/transcriptomics-data\/DESeq2\/median_low_vs_high.csv',\n'\/kaggle\/input\/end-als\/end-als\/transcriptomics-data\/DESeq2\/ctrl_vs_case.csv']\nimport time\nt0 = time.time()\nlist_data_df = []\nfor fn in list_fn:\n    d = pd.read_csv(fn)\n    list_data_df.append(d)\n    print('Loaded:', fn, 'shape:', d.shape)\nlist_data_names = [ 'bulbar_vs_limb'.replace('_',' '), 'median_low_vs_high'.replace('_',' '),  'ctrl_vs_case'.replace('_',' ') ]\n\nprint(time.time()-t0, 'seconds passed')","14cfc117":"list_all_genes_in_data = list( list_data_df[0].columns[2:])\ndf_genes_alsod['In Data'] = df_genes_alsod['Gene symbol'].isin(list_all_genes_in_data)# list_genes)        \nprint('Genes found in data list: ', df_genes_alsod['In Data'].sum() )\nmask = df_genes_alsod['In Data'] == False\nprint()\nprint('Genes not found in data list (seems not very important can exclude them for the moment): ')\ndisplay( df_genes_alsod[mask] )\ndf_genes_alsod = df_genes_alsod[~mask]\ndf_genes_alsod.shape","4e9dbaa3":"\nmask = df_genes_alsod['Category'].isin([ 'Definitive ALS gene', 'Strong evidence', 'Clinical modifier','Moderate evidence' ]  ) \nlist_48_quite_als_related_genes = list( df_genes_alsod[mask]['Gene symbol'] )\nprint(len(list_48_quite_als_related_genes))\n\nlist_all_149_als_related_genes = list( df_genes_alsod['Gene symbol'] )\nprint( len(list_all_149_als_related_genes) )\n\nlist_all_genes_in_data = list( list_data_df[0].columns[2:])\ngenes_subsets = {'48 ALS top related': list_48_quite_als_related_genes, '149 all ALS related':  list_all_149_als_related_genes,\n                'All genes': list_all_genes_in_data }\n\n","7e1ec939":"url = 'https:\/\/raw.githubusercontent.com\/chervov\/genes\/main\/genes_interacting_with_ALS_genes_by_BIOGRID.csv'\ngenes_interacting_with_ALS_genes_by_BIOGRID = pd.read_csv( url)\ngenes_interacting_with_ALS_genes_by_BIOGRID","a91e4aef":"import matplotlib.pyplot as plt","c18659d6":"df = list_data_df[2]\ns = set(df.columns[2:]) & set( genes_interacting_with_ALS_genes_by_BIOGRID['Gene symbol'])\nX = df[s]# .iloc[:,2:][s]\nv = np.sum(X,axis = 0)\nplt.plot(np.log10(1+ np.sort(v)), '*-' )\nplt.show()\nIX = np.argsort(v)\nIX_keep = IX[2000:6000]\nX = X.iloc[:,IX_keep]\nv = np.sum(X,axis = 0)\nplt.plot(np.log10(1+ np.sort(v)), '*-' )\nplt.show()\n\nX","5ac86221":"from sklearn.decomposition import FastICA\nfrom sklearn.decomposition import NMF\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nfrom sklearn.decomposition import FastICA\nn_components = 10\ndim_reducer = FastICA(n_components=n_components, random_state=0)\nX_transformed = dim_reducer.fit_transform(np.random.randn(100,200))\nprint(X_transformed.shape )\nfrom sklearn.decomposition import LatentDirichletAllocation\ndim_reducer = LatentDirichletAllocation(n_components=n_components, random_state=0)\nX_transformed = dim_reducer.fit_transform(1+np.random.rand(100,200))\nprint(X_transformed.shape )\n\nfrom sklearn.decomposition import NMF\ndim_reducer = NMF(n_components=n_components, random_state=0)\nX_transformed = dim_reducer.fit_transform(1+np.random.rand(100,200))\nprint(X_transformed.shape )\n\n","5d0fabb6":"from sklearn.decomposition import PCA\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import NMF\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport umap\n\n\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import KBinsDiscretizer\n\n\nfrom sklearn.preprocessing import StandardScaler\n#from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import StratifiedKFold\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nimport time \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#scaler = StandardScaler()\n#pca = umap.UMAP(n_components=20, random_state = 0 ) # PCA(n_components=20, random_state = 0)\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('PCA', pca ), ('Scaler',  scaler ), ('Logistic', clf )])\n#model = Pipeline([('PCA', pca ), ('Logistic', clf )])\n#from sklearn.neighbors import NearestNeighbors\n#clf = NearestNeighbors(n_neighbors=1)\n#from sklearn.neighbors import KNeighborsClassifier\n#clf = KNeighborsClassifier(n_neighbors=5)\n\n# pca = umap.UMAP(n_components=20, random_state = 0 ) # \n#pca = PCA(n_components=20, random_state = 0)\n\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n\n# param_grid = {\"max_depth\":[1 , 5], \"n_estimators\":[5,20], } \n\n\nt00 = time.time()\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\n\nn_components = 100\nfor model_type in [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]:\n    if model_type == 0:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = PCA(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'PCA'+str(n_components)+'+LR1'\n    elif model_type == 1:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = FastICA(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'ICA'+str(n_components)+'+LR1'\n    elif model_type == 2:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = NMF(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([ ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'NMF'+str(n_components)+'+LR1'\n    elif model_type == 3:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = LatentDirichletAllocation(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([ ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'LDA'+str(n_components)+'+LR1'\n    elif model_type == 4:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = umap.UMAP(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([ ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'umap'+str(n_components)+'+LR1'\n    elif model_type == 5:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = PCA(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', qt) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Quantile10+PCA'+str(n_components)+'+LR1'\n    elif model_type == 6:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = FastICA(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', qt) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Quantile10+ICA'+str(n_components)+'+LR1'\n    elif model_type == 7:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = NMF(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', qt) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Quantile10+NMF'+str(n_components)+'+LR1'\n    elif model_type == 8:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = LatentDirichletAllocation(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', qt) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Quantile10+LDA'+str(n_components)+'+LR1'\n    elif model_type == 9:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = umap.UMAP(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', qt) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Quantile10+umap'+str(n_components)+'+LR1'\n    elif model_type == 10:\n        preproc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #preproc = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = PCA(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', preproc) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Kbin3+PCA'+str(n_components)+'+LR1'\n    elif model_type == 11:\n        preproc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #preproc = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = FastICA(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', preproc) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Kbin3+ICA'+str(n_components)+'+LR1'\n    elif model_type == 12:\n        preproc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #preproc = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = NMF(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', preproc) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Kbin3+NMF'+str(n_components)+'+LR1'\n    elif model_type == 13:\n        preproc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #preproc = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = LatentDirichletAllocation(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', preproc) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Kbin3+LDA'+str(n_components)+'+LR1'\n    elif model_type == 14:\n        preproc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #preproc = QuantileTransformer(n_quantiles=10, random_state=0)\n        dim_reducer = umap.UMAP(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', preproc) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Kbin3+umap'+str(n_components)+'+LR1'\n    else:\n        break\n        \n    for i,df in  enumerate(list_data_df):\n        data_name = list_data_names[i]\n        if i in [0,1]:\n            continue \n        for feature_subset_id in genes_subsets:\n            if feature_subset_id not in ['All genes']: #, '149 all ALS related']:\n                continue\n\n            t0 = time.time()\n\n            list_features = genes_subsets[ feature_subset_id]\n\n            y = df.iloc[:,1]\n            #X = df[list_features]# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n            #X = np.random.randn(X.shape[0], X.shape[1] )\n            print('Data name:', data_name,'shapes:', X.shape, y.shape, 'y.sum:', y.sum(), 'feature_subset_id',feature_subset_id )\n            t0 = time.time()\n\n            #model_cv.fit(X,y)\n            #score_cv = cross_val_score(model, X, y, cv=skf, scoring = 'roc_auc')\n            #print('score_cv', score_cv )\n            #score_cv_mean = np.mean( score_cv )\n            predict_cv = cross_val_predict(model, X, y, cv=skf , method='predict_proba') \n            score_cv_mean = roc_auc_score(y, predict_cv[:,1] )\n            tt1 = np.round(time.time()-t0,1)\n            print('Score:',score_cv_mean, 'seconds passed: ', tt1 )\n\n\n            models_results_info.loc[model_index,'RocAuc CV'] = score_cv_mean # np.round(model_cv.best_score_ ,3)\n            models_results_info.loc[model_index,'Data'] = data_name\n            models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n\n            models_results_info.loc[model_index,'Model'] = str_model_inf # 'PCA + Logistic'\n            # models_results_info.loc[model_index,'Params'] = 'PCA6 LR_C'+str(C)+'L1'#str( model_cv.best_params_)\n            \n            models_results_info.loc[model_index,'n_features'] = X.shape[1]\n            models_results_info.loc[model_index,'Seconds Passed'] = tt1\n            model_index += 1\n    \nprint(np.round(time.time()-t00),'total seconds passed')    \n    \nmodels_results_info","e972177f":"from sklearn.decomposition import PCA\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import NMF\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport umap\n\n\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import KBinsDiscretizer\n\n\nfrom sklearn.preprocessing import StandardScaler\n#from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import StratifiedKFold\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nimport time \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#scaler = StandardScaler()\n#pca = umap.UMAP(n_components=20, random_state = 0 ) # PCA(n_components=20, random_state = 0)\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('PCA', pca ), ('Scaler',  scaler ), ('Logistic', clf )])\n#model = Pipeline([('PCA', pca ), ('Logistic', clf )])\n#from sklearn.neighbors import NearestNeighbors\n#clf = NearestNeighbors(n_neighbors=1)\n#from sklearn.neighbors import KNeighborsClassifier\n#clf = KNeighborsClassifier(n_neighbors=5)\n\n# pca = umap.UMAP(n_components=20, random_state = 0 ) # \n#pca = PCA(n_components=20, random_state = 0)\n\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n\n# param_grid = {\"max_depth\":[1 , 5], \"n_estimators\":[5,20], } \n\n\nt00 = time.time()\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\n\nfor model_type in [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]:\n    if model_type == 0:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = PCA(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'PCA'+str(n_components)+'+LR1'\n    elif model_type == 1:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = FastICA(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'ICA'+str(n_components)+'+LR1'\n    elif model_type == 2:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = NMF(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([ ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'NMF'+str(n_components)+'+LR1'\n    elif model_type == 3:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = LatentDirichletAllocation(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([ ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'LDA'+str(n_components)+'+LR1'\n    elif model_type == 4:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = umap.UMAP(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([ ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'umap'+str(n_components)+'+LR1'\n    elif model_type == 5:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = PCA(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', qt) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Quantile10+PCA'+str(n_components)+'+LR1'\n    elif model_type == 6:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = FastICA(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', qt) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Quantile10+ICA'+str(n_components)+'+LR1'\n    elif model_type == 7:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = NMF(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', qt) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Quantile10+NMF'+str(n_components)+'+LR1'\n    elif model_type == 8:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = LatentDirichletAllocation(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', qt) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Quantile10+LDA'+str(n_components)+'+LR1'\n    elif model_type == 9:\n        #enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        qt = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = umap.UMAP(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', qt) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Quantile10+umap'+str(n_components)+'+LR1'\n    elif model_type == 10:\n        preproc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #preproc = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = PCA(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', preproc) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Kbin3+PCA'+str(n_components)+'+LR1'\n    elif model_type == 11:\n        preproc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #preproc = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = FastICA(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', preproc) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Kbin3+ICA'+str(n_components)+'+LR1'\n    elif model_type == 12:\n        preproc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #preproc = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = NMF(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', preproc) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Kbin3+NMF'+str(n_components)+'+LR1'\n    elif model_type == 13:\n        preproc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #preproc = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = LatentDirichletAllocation(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', preproc) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Kbin3+LDA'+str(n_components)+'+LR1'\n    elif model_type == 14:\n        preproc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #preproc = QuantileTransformer(n_quantiles=10, random_state=0)\n        n_components = 20\n        dim_reducer = umap.UMAP(n_components=n_components, random_state=0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('Preproc', preproc) , ('dim reducer', dim_reducer ), ('Logistic', clf )])\n        str_model_inf = 'Kbin3+umap'+str(n_components)+'+LR1'\n    else:\n        break\n        \n    for i,df in  enumerate(list_data_df):\n        data_name = list_data_names[i]\n        if i in [0,1]:\n            continue \n        for feature_subset_id in genes_subsets:\n            if feature_subset_id not in ['All genes']: #, '149 all ALS related']:\n                continue\n\n            t0 = time.time()\n\n            list_features = genes_subsets[ feature_subset_id]\n\n            y = df.iloc[:,1]\n            #X = df[list_features]# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n            #X = np.random.randn(X.shape[0], X.shape[1] )\n            print('Data name:', data_name,'shapes:', X.shape, y.shape, 'y.sum:', y.sum(), 'feature_subset_id',feature_subset_id )\n            t0 = time.time()\n\n            #model_cv.fit(X,y)\n            #score_cv = cross_val_score(model, X, y, cv=skf, scoring = 'roc_auc')\n            #print('score_cv', score_cv )\n            #score_cv_mean = np.mean( score_cv )\n            predict_cv = cross_val_predict(model, X, y, cv=skf , method='predict_proba') \n            score_cv_mean = roc_auc_score(y, predict_cv[:,1] )\n            tt1 = np.round(time.time()-t0,1)\n            print('Score:',score_cv_mean, 'seconds passed: ', tt1 )\n\n\n            models_results_info.loc[model_index,'RocAuc CV'] = score_cv_mean # np.round(model_cv.best_score_ ,3)\n            models_results_info.loc[model_index,'Data'] = data_name\n            models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n\n            models_results_info.loc[model_index,'Model'] = str_model_inf # 'PCA + Logistic'\n            # models_results_info.loc[model_index,'Params'] = 'PCA6 LR_C'+str(C)+'L1'#str( model_cv.best_params_)\n            \n            models_results_info.loc[model_index,'n_features'] = X.shape[1]\n            models_results_info.loc[model_index,'Seconds Passed'] = tt1\n            model_index += 1\n    \nprint(np.round(time.time()-t00),'total seconds passed')    \n    \nmodels_results_info","81734295":"from sklearn.preprocessing import KBinsDiscretizer\nenc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n#enc = KBinsDiscretizer(n_bins=10, encode='onehot')\nX_binned = enc.fit_transform(np.random.rand(1000).reshape(-1,1))\npd.Series(X_binned.ravel()).value_counts()","dcc0d450":"from sklearn.preprocessing import QuantileTransformer\n\nfrom sklearn.preprocessing import StandardScaler\n#from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nimport time \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport umap\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#scaler = StandardScaler()\n#pca = umap.UMAP(n_components=20, random_state = 0 ) # PCA(n_components=20, random_state = 0)\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('PCA', pca ), ('Scaler',  scaler ), ('Logistic', clf )])\n#model = Pipeline([('PCA', pca ), ('Logistic', clf )])\n#from sklearn.neighbors import NearestNeighbors\n#clf = NearestNeighbors(n_neighbors=1)\n#from sklearn.neighbors import KNeighborsClassifier\n#clf = KNeighborsClassifier(n_neighbors=5)\n\n# pca = umap.UMAP(n_components=20, random_state = 0 ) # \n#pca = PCA(n_components=20, random_state = 0)\n\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n\n# param_grid = {\"max_depth\":[1 , 5], \"n_estimators\":[5,20], } \n\n\nt00 = time.time()\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\n\nfor model_type in [1,2,3 ,4,5,6]:\n    if model_type == 1:\n        enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=100, random_state = 0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('KBinsDiscretizer', enc) , ('PCA', pca ), ('Logistic', clf )])\n        str_model_inf = 'KBins3+PCA100+LR1'\n    elif model_type == 2:\n        enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=100, random_state = 0)\n        C = 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('KBinsDiscretizer', enc) , ('PCA', pca ), ('Logistic', clf )])\n        str_model_inf = 'KBins3+PCA100+LR0.1'\n    elif model_type == 3:\n        enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=100, random_state = 0)\n        C = 0.01\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('KBinsDiscretizer', enc) , ('PCA', pca ), ('Logistic', clf )])\n        str_model_inf = 'KBins3+PCA100+LR0.01'\n    elif model_type == 4:\n        enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=100, random_state = 0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('KBinsDiscretizer', enc) , ('Logistic', clf )])\n        str_model_inf = 'KBins3+LR1'\n    elif model_type == 5:\n        enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=100, random_state = 0)\n        C = 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('KBinsDiscretizer', enc) , ('Logistic', clf )])\n        str_model_inf = 'KBins3+LR0.1'\n    elif model_type == 6:\n        enc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n        #qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=100, random_state = 0)\n        C = 0.01\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('KBinsDiscretizer', enc) , ('Logistic', clf )])\n        str_model_inf = 'KBins3+LR0.01'\n    else:\n        break\n        \n    for i,df in  enumerate(list_data_df):\n        data_name = list_data_names[i]\n        if i in [0,1]:\n            continue \n        for feature_subset_id in genes_subsets:\n            if feature_subset_id not in ['All genes']: #, '149 all ALS related']:\n                continue\n\n            t0 = time.time()\n\n            list_features = genes_subsets[ feature_subset_id]\n\n            y = df.iloc[:,1]\n            #X = df[list_features]# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n            #X = np.random.randn(X.shape[0], X.shape[1] )\n            print('Data name:', data_name,'shapes:', X.shape, y.shape, 'y.sum:', y.sum(), 'feature_subset_id',feature_subset_id )\n            t0 = time.time()\n\n            #model_cv.fit(X,y)\n            #score_cv = cross_val_score(model, X, y, cv=skf, scoring = 'roc_auc')\n            #print('score_cv', score_cv )\n            #score_cv_mean = np.mean( score_cv )\n            predict_cv = cross_val_predict(model, X, y, cv=skf , method='predict_proba') \n            score_cv_mean = roc_auc_score(y, predict_cv[:,1] )\n            tt1 = np.round(time.time()-t0,1)\n            print('Score:',score_cv_mean, 'seconds passed: ', tt1 )\n\n\n            models_results_info.loc[model_index,'RocAuc CV'] = score_cv_mean # np.round(model_cv.best_score_ ,3)\n            models_results_info.loc[model_index,'Data'] = data_name\n            models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n\n            models_results_info.loc[model_index,'Model'] = str_model_inf # 'PCA + Logistic'\n            # models_results_info.loc[model_index,'Params'] = 'PCA6 LR_C'+str(C)+'L1'#str( model_cv.best_params_)\n            \n            models_results_info.loc[model_index,'n_features'] = X.shape[1]\n            models_results_info.loc[model_index,'Seconds Passed'] = tt1\n            model_index += 1\n    \nprint(np.round(time.time()-t00),'total seconds passed')    \n    \nmodels_results_info","8bd76d9b":"from sklearn.preprocessing import QuantileTransformer\nqt = QuantileTransformer(n_quantiles=10, random_state=0)\n\nrng = np.random.RandomState(0)\n\ntmp = np.sort(rng.normal(loc=0.5, scale=0.25, size=(100, 1)), axis=0)\nqt = QuantileTransformer(n_quantiles=10, random_state=0)\ntmp = qt.fit_transform(tmp)\npd.Series(tmp.ravel()).value_counts()\nplt.plot(np.sort(tmp))","7ab620a4":"from sklearn.preprocessing import QuantileTransformer\n\nfrom sklearn.preprocessing import StandardScaler\n#from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nimport time \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport umap\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#scaler = StandardScaler()\n#pca = umap.UMAP(n_components=20, random_state = 0 ) # PCA(n_components=20, random_state = 0)\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('PCA', pca ), ('Scaler',  scaler ), ('Logistic', clf )])\n#model = Pipeline([('PCA', pca ), ('Logistic', clf )])\n#from sklearn.neighbors import NearestNeighbors\n#clf = NearestNeighbors(n_neighbors=1)\n#from sklearn.neighbors import KNeighborsClassifier\n#clf = KNeighborsClassifier(n_neighbors=5)\n\n# pca = umap.UMAP(n_components=20, random_state = 0 ) # \n#pca = PCA(n_components=20, random_state = 0)\n\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n\n# param_grid = {\"max_depth\":[1 , 5], \"n_estimators\":[5,20], } \n\n\nt00 = time.time()\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\n\nfor model_type in [1,2,3,4,5]:\n    if model_type == 1:\n        str_model_inf = 'QT3+PCA100+LR1'\n        qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=100, random_state = 0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n    elif model_type == 2:\n        str_model_inf = 'QT3+PCA100+LR0.1'\n        qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=100, random_state = 0)\n        C = 0.1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n    elif model_type == 3:\n        str_model_inf = 'QT3+PCA100+LR0.01'\n        qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=100, random_state = 0)\n        C = 0.01 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n    elif model_type == 4:\n        str_model_inf = 'QT3+PCA100+LR0.001'\n        qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=100, random_state = 0)\n        C = 0.001 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n    elif model_type == 5:\n        str_model_inf = 'QT3+PCA100+LR0.0001'\n        qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=100, random_state = 0)\n        C = 0.0001 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n    else:\n        break\n        \n    for i,df in  enumerate(list_data_df):\n        data_name = list_data_names[i]\n        if i in [0,1]:\n            continue \n        for feature_subset_id in genes_subsets:\n            if feature_subset_id not in ['All genes']: #, '149 all ALS related']:\n                continue\n\n            t0 = time.time()\n\n            list_features = genes_subsets[ feature_subset_id]\n\n            y = df.iloc[:,1]\n            #X = df[list_features]# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n            #X = np.random.randn(X.shape[0], X.shape[1] )\n            print('Data name:', data_name,'shapes:', X.shape, y.shape, 'y.sum:', y.sum(), 'feature_subset_id',feature_subset_id )\n            t0 = time.time()\n\n            #model_cv.fit(X,y)\n            #score_cv = cross_val_score(model, X, y, cv=skf, scoring = 'roc_auc')\n            #print('score_cv', score_cv )\n            #score_cv_mean = np.mean( score_cv )\n            predict_cv = cross_val_predict(model, X, y, cv=skf , method='predict_proba') \n            score_cv_mean = roc_auc_score(y, predict_cv[:,1] )\n            tt1 = np.round(time.time()-t0,1)\n            print('Score:',score_cv_mean, 'seconds passed: ', tt1 )\n\n\n            models_results_info.loc[model_index,'RocAuc CV'] = score_cv_mean # np.round(model_cv.best_score_ ,3)\n            models_results_info.loc[model_index,'Data'] = data_name\n            models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n\n            models_results_info.loc[model_index,'Model'] = str_model_inf # 'PCA + Logistic'\n            # models_results_info.loc[model_index,'Params'] = 'PCA6 LR_C'+str(C)+'L1'#str( model_cv.best_params_)\n            \n            models_results_info.loc[model_index,'n_features'] = X.shape[1]\n            models_results_info.loc[model_index,'Seconds Passed'] = tt1\n            model_index += 1\n    \nprint(np.round(time.time()-t00),'total seconds passed')    \n    \nmodels_results_info","e4d44ee7":"from sklearn.preprocessing import QuantileTransformer\n\nfrom sklearn.preprocessing import StandardScaler\n#from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nimport time \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport umap\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#scaler = StandardScaler()\n#pca = umap.UMAP(n_components=20, random_state = 0 ) # PCA(n_components=20, random_state = 0)\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('PCA', pca ), ('Scaler',  scaler ), ('Logistic', clf )])\n#model = Pipeline([('PCA', pca ), ('Logistic', clf )])\n#from sklearn.neighbors import NearestNeighbors\n#clf = NearestNeighbors(n_neighbors=1)\n#from sklearn.neighbors import KNeighborsClassifier\n#clf = KNeighborsClassifier(n_neighbors=5)\n\n# pca = umap.UMAP(n_components=20, random_state = 0 ) # \n#pca = PCA(n_components=20, random_state = 0)\n\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n\n# param_grid = {\"max_depth\":[1 , 5], \"n_estimators\":[5,20], } \n\n\nt00 = time.time()\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\n\nfor model_type in [1,2,3,4,5]:\n    if model_type == 1:\n        str_model_inf = 'QT2+LR1'\n        qt = QuantileTransformer(n_quantiles=2, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n    elif model_type == 2:\n        str_model_inf = 'QT2+LR0.1'\n        qt = QuantileTransformer(n_quantiles=2, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 0.1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n    elif model_type == 3:\n        str_model_inf = 'QT2+LR0.01'\n        qt = QuantileTransformer(n_quantiles=2, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 0.01 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n    elif model_type == 4:\n        str_model_inf = 'QT2+LR0.001'\n        qt = QuantileTransformer(n_quantiles=2, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 0.001 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n    elif model_type == 5:\n        str_model_inf = 'QT2+LR0.0001'\n        qt = QuantileTransformer(n_quantiles=2, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 0.0001 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n\n    else:\n        break\n        \n    for i,df in  enumerate(list_data_df):\n        data_name = list_data_names[i]\n        if i in [0,1]:\n            continue \n        for feature_subset_id in genes_subsets:\n            if feature_subset_id not in ['All genes']: #, '149 all ALS related']:\n                continue\n\n            t0 = time.time()\n\n            list_features = genes_subsets[ feature_subset_id]\n\n            y = df.iloc[:,1]\n            #X = df[list_features]# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n            #X = np.random.randn(X.shape[0], X.shape[1] )\n            print('Data name:', data_name,'shapes:', X.shape, y.shape, 'y.sum:', y.sum(), 'feature_subset_id',feature_subset_id )\n            t0 = time.time()\n\n            #model_cv.fit(X,y)\n            #score_cv = cross_val_score(model, X, y, cv=skf, scoring = 'roc_auc')\n            #print('score_cv', score_cv )\n            #score_cv_mean = np.mean( score_cv )\n            predict_cv = cross_val_predict(model, X, y, cv=skf , method='predict_proba') \n            score_cv_mean = roc_auc_score(y, predict_cv[:,1] )\n            tt1 = np.round(time.time()-t0,1)\n            print('Score:',score_cv_mean, 'seconds passed: ', tt1 )\n\n\n            models_results_info.loc[model_index,'RocAuc CV'] = score_cv_mean # np.round(model_cv.best_score_ ,3)\n            models_results_info.loc[model_index,'Data'] = data_name\n            models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n\n            models_results_info.loc[model_index,'Model'] = str_model_inf # 'PCA + Logistic'\n            # models_results_info.loc[model_index,'Params'] = 'PCA6 LR_C'+str(C)+'L1'#str( model_cv.best_params_)\n            \n            models_results_info.loc[model_index,'n_features'] = X.shape[1]\n            models_results_info.loc[model_index,'Seconds Passed'] = tt1\n            model_index += 1\n    \nprint(np.round(time.time()-t00),'total seconds passed')    \n    \nmodels_results_info","27c441c3":"from sklearn.preprocessing import QuantileTransformer\n\nfrom sklearn.preprocessing import StandardScaler\n#from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nimport time \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport umap\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#scaler = StandardScaler()\n#pca = umap.UMAP(n_components=20, random_state = 0 ) # PCA(n_components=20, random_state = 0)\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('PCA', pca ), ('Scaler',  scaler ), ('Logistic', clf )])\n#model = Pipeline([('PCA', pca ), ('Logistic', clf )])\n#from sklearn.neighbors import NearestNeighbors\n#clf = NearestNeighbors(n_neighbors=1)\n#from sklearn.neighbors import KNeighborsClassifier\n#clf = KNeighborsClassifier(n_neighbors=5)\n\n# pca = umap.UMAP(n_components=20, random_state = 0 ) # \n#pca = PCA(n_components=20, random_state = 0)\n\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n\n# param_grid = {\"max_depth\":[1 , 5], \"n_estimators\":[5,20], } \n\n\nt00 = time.time()\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\n\nfor model_type in [1,2,3,4,5]:\n    if model_type == 1:\n        str_model_inf = 'QT3+LR1'\n        qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n    elif model_type == 2:\n        str_model_inf = 'QT3+LR0.1'\n        qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 0.1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n    elif model_type == 3:\n        str_model_inf = 'QT3+LR0.01'\n        qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 0.01 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n    elif model_type == 4:\n        str_model_inf = 'QT3+LR0.001'\n        qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 0.001 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n    elif model_type == 5:\n        str_model_inf = 'QT3+LR0.0001'\n        qt = QuantileTransformer(n_quantiles=3, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 0.0001 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n\n    else:\n        break\n        \n    for i,df in  enumerate(list_data_df):\n        data_name = list_data_names[i]\n        if i in [0,1]:\n            continue \n        for feature_subset_id in genes_subsets:\n            if feature_subset_id not in ['All genes']: #, '149 all ALS related']:\n                continue\n\n            t0 = time.time()\n\n            list_features = genes_subsets[ feature_subset_id]\n\n            y = df.iloc[:,1]\n            #X = df[list_features]# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n            #X = np.random.randn(X.shape[0], X.shape[1] )\n            print('Data name:', data_name,'shapes:', X.shape, y.shape, 'y.sum:', y.sum(), 'feature_subset_id',feature_subset_id )\n            t0 = time.time()\n\n            #model_cv.fit(X,y)\n            #score_cv = cross_val_score(model, X, y, cv=skf, scoring = 'roc_auc')\n            #print('score_cv', score_cv )\n            #score_cv_mean = np.mean( score_cv )\n            predict_cv = cross_val_predict(model, X, y, cv=skf , method='predict_proba') \n            score_cv_mean = roc_auc_score(y, predict_cv[:,1] )\n            tt1 = np.round(time.time()-t0,1)\n            print('Score:',score_cv_mean, 'seconds passed: ', tt1 )\n\n\n            models_results_info.loc[model_index,'RocAuc CV'] = score_cv_mean # np.round(model_cv.best_score_ ,3)\n            models_results_info.loc[model_index,'Data'] = data_name\n            models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n\n            models_results_info.loc[model_index,'Model'] = str_model_inf # 'PCA + Logistic'\n            # models_results_info.loc[model_index,'Params'] = 'PCA6 LR_C'+str(C)+'L1'#str( model_cv.best_params_)\n            \n            models_results_info.loc[model_index,'n_features'] = X.shape[1]\n            models_results_info.loc[model_index,'Seconds Passed'] = tt1\n            model_index += 1\n    \nprint(np.round(time.time()-t00),'total seconds passed')    \n    \nmodels_results_info","1715feb7":"from sklearn.preprocessing import QuantileTransformer\n\nfrom sklearn.preprocessing import StandardScaler\n#from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nimport time \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport umap\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#scaler = StandardScaler()\n#pca = umap.UMAP(n_components=20, random_state = 0 ) # PCA(n_components=20, random_state = 0)\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('PCA', pca ), ('Scaler',  scaler ), ('Logistic', clf )])\n#model = Pipeline([('PCA', pca ), ('Logistic', clf )])\n#from sklearn.neighbors import NearestNeighbors\n#clf = NearestNeighbors(n_neighbors=1)\n#from sklearn.neighbors import KNeighborsClassifier\n#clf = KNeighborsClassifier(n_neighbors=5)\n\n# pca = umap.UMAP(n_components=20, random_state = 0 ) # \n#pca = PCA(n_components=20, random_state = 0)\n\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n\n# param_grid = {\"max_depth\":[1 , 5], \"n_estimators\":[5,20], } \n\n\nt00 = time.time()\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\n\nfor model_type in [1,2,3,4,5]:\n    if model_type == 1:\n        str_model_inf = 'QT30+LR1'\n        qt = QuantileTransformer(n_quantiles=30, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n    elif model_type == 2:\n        str_model_inf = 'QT30+LR0.1'\n        qt = QuantileTransformer(n_quantiles=30, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 0.1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n    elif model_type == 3:\n        str_model_inf = 'QT30+LR0.01'\n        qt = QuantileTransformer(n_quantiles=30, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 0.01 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n    elif model_type == 4:\n        str_model_inf = 'QT30+LR0.001'\n        qt = QuantileTransformer(n_quantiles=30, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 0.001 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n    elif model_type == 5:\n        str_model_inf = 'QT30+LR0.0001'\n        qt = QuantileTransformer(n_quantiles=30, random_state=0)\n        pca = PCA(n_components=1000, random_state = 0)\n        C = 0.0001 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        #model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n        model = Pipeline([('QuantileTransformer', qt) , ('Logistic', clf )])\n\n    else:\n        break\n        \n    for i,df in  enumerate(list_data_df):\n        data_name = list_data_names[i]\n        if i in [0,1]:\n            continue \n        for feature_subset_id in genes_subsets:\n            if feature_subset_id not in ['All genes']: #, '149 all ALS related']:\n                continue\n\n            t0 = time.time()\n\n            list_features = genes_subsets[ feature_subset_id]\n\n            y = df.iloc[:,1]\n            #X = df[list_features]# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n            #X = np.random.randn(X.shape[0], X.shape[1] )\n            print('Data name:', data_name,'shapes:', X.shape, y.shape, 'y.sum:', y.sum(), 'feature_subset_id',feature_subset_id )\n            t0 = time.time()\n\n            #model_cv.fit(X,y)\n            #score_cv = cross_val_score(model, X, y, cv=skf, scoring = 'roc_auc')\n            #print('score_cv', score_cv )\n            #score_cv_mean = np.mean( score_cv )\n            predict_cv = cross_val_predict(model, X, y, cv=skf , method='predict_proba') \n            score_cv_mean = roc_auc_score(y, predict_cv[:,1] )\n            tt1 = np.round(time.time()-t0,1)\n            print('Score:',score_cv_mean, 'seconds passed: ', tt1 )\n\n\n            models_results_info.loc[model_index,'RocAuc CV'] = score_cv_mean # np.round(model_cv.best_score_ ,3)\n            models_results_info.loc[model_index,'Data'] = data_name\n            models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n\n            models_results_info.loc[model_index,'Model'] = str_model_inf # 'PCA + Logistic'\n            # models_results_info.loc[model_index,'Params'] = 'PCA6 LR_C'+str(C)+'L1'#str( model_cv.best_params_)\n            \n            models_results_info.loc[model_index,'n_features'] = X.shape[1]\n            models_results_info.loc[model_index,'Seconds Passed'] = tt1\n            model_index += 1\n    \nprint(np.round(time.time()-t00),'total seconds passed')    \n    \nmodels_results_info","3633ff28":"from sklearn.preprocessing import QuantileTransformer\nqt = QuantileTransformer(n_quantiles=10, random_state=0)\n\nfrom sklearn.preprocessing import StandardScaler\n#from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nimport time \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport umap\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#scaler = StandardScaler()\n#pca = umap.UMAP(n_components=20, random_state = 0 ) # PCA(n_components=20, random_state = 0)\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('PCA', pca ), ('Scaler',  scaler ), ('Logistic', clf )])\n#model = Pipeline([('PCA', pca ), ('Logistic', clf )])\n#from sklearn.neighbors import NearestNeighbors\n#clf = NearestNeighbors(n_neighbors=1)\n#from sklearn.neighbors import KNeighborsClassifier\n#clf = KNeighborsClassifier(n_neighbors=5)\n\n# pca = umap.UMAP(n_components=20, random_state = 0 ) # \n#pca = PCA(n_components=20, random_state = 0)\n\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n\n# param_grid = {\"max_depth\":[1 , 5], \"n_estimators\":[5,20], } \n\n\nt00 = time.time()\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\n\nfor model_type in [1,2,3,4,5]:\n    if model_type == 1:\n        str_model_inf = 'QT10+PCA100+LR1'\n        pca = PCA(n_components=100, random_state = 0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n    elif model_type == 2:\n        str_model_inf = 'QT10+PCA100+LR0.1'\n        pca = PCA(n_components=100, random_state = 0)\n        C = 0.1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n    elif model_type == 3:\n        str_model_inf = 'QT10+PCA100+LR0.01'\n        pca = PCA(n_components=100, random_state = 0)\n        C = 0.01 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n    elif model_type == 4:\n        str_model_inf = 'QT10+PCA100+LR0.001'\n        pca = PCA(n_components=100, random_state = 0)\n        C = 0.001 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n    elif model_type == 5:\n        str_model_inf = 'QT10+PCA100+LR0.0001'\n        pca = PCA(n_components=100, random_state = 0)\n        C = 0.0001 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n    else:\n        break\n        \n    for i,df in  enumerate(list_data_df):\n        data_name = list_data_names[i]\n        if i in [0,1]:\n            continue \n        for feature_subset_id in genes_subsets:\n            if feature_subset_id not in ['All genes']: #, '149 all ALS related']:\n                continue\n\n            t0 = time.time()\n\n            list_features = genes_subsets[ feature_subset_id]\n\n            y = df.iloc[:,1]\n            #X = df[list_features]# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n            #X = np.random.randn(X.shape[0], X.shape[1] )\n            print('Data name:', data_name,'shapes:', X.shape, y.shape, 'y.sum:', y.sum(), 'feature_subset_id',feature_subset_id )\n            t0 = time.time()\n\n            #model_cv.fit(X,y)\n            #score_cv = cross_val_score(model, X, y, cv=skf, scoring = 'roc_auc')\n            #print('score_cv', score_cv )\n            #score_cv_mean = np.mean( score_cv )\n            predict_cv = cross_val_predict(model, X, y, cv=skf , method='predict_proba') \n            score_cv_mean = roc_auc_score(y, predict_cv[:,1] )\n            tt1 = np.round(time.time()-t0,1)\n            print('Score:',score_cv_mean, 'seconds passed: ', tt1 )\n\n\n            models_results_info.loc[model_index,'RocAuc CV'] = score_cv_mean # np.round(model_cv.best_score_ ,3)\n            models_results_info.loc[model_index,'Data'] = data_name\n            models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n\n            models_results_info.loc[model_index,'Model'] = str_model_inf # 'PCA + Logistic'\n            # models_results_info.loc[model_index,'Params'] = 'PCA6 LR_C'+str(C)+'L1'#str( model_cv.best_params_)\n            \n            models_results_info.loc[model_index,'n_features'] = X.shape[1]\n            models_results_info.loc[model_index,'Seconds Passed'] = tt1\n            model_index += 1\n    \nprint(np.round(time.time()-t00),'total seconds passed')    \n    \nmodels_results_info","d9bc5bd4":"from sklearn.preprocessing import QuantileTransformer\nqt = QuantileTransformer(n_quantiles=10, random_state=0)\n\nfrom sklearn.preprocessing import StandardScaler\n#from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nimport time \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport umap\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#scaler = StandardScaler()\n#pca = umap.UMAP(n_components=20, random_state = 0 ) # PCA(n_components=20, random_state = 0)\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('PCA', pca ), ('Scaler',  scaler ), ('Logistic', clf )])\n#model = Pipeline([('PCA', pca ), ('Logistic', clf )])\n#from sklearn.neighbors import NearestNeighbors\n#clf = NearestNeighbors(n_neighbors=1)\n#from sklearn.neighbors import KNeighborsClassifier\n#clf = KNeighborsClassifier(n_neighbors=5)\n\n# pca = umap.UMAP(n_components=20, random_state = 0 ) # \n#pca = PCA(n_components=20, random_state = 0)\n\n#C = 0.1 # 0.1\n#clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n\n# param_grid = {\"max_depth\":[1 , 5], \"n_estimators\":[5,20], } \n\n\nt00 = time.time()\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\n\nfor model_type in [1,2,3]:\n    if model_type == 1:\n        str_model_inf = 'QT10+PCA20+LR1'\n        pca = PCA(n_components=20, random_state = 0)\n        C = 1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n    elif model_type == 2:\n        str_model_inf = 'QT10+PCA20+LR0.1'\n        pca = PCA(n_components=20, random_state = 0)\n        C = 0.1 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n    elif model_type == 3:\n        str_model_inf = 'QT10+PCA20+LR0.01'\n        pca = PCA(n_components=20, random_state = 0)\n        C = 0.01 # 0.1\n        clf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n        model = Pipeline([('QuantileTransformer', qt) , ('PCA', pca ), ('Logistic', clf )])\n    else:\n        break\n        \n    for i,df in  enumerate(list_data_df):\n        data_name = list_data_names[i]\n        if i in [0,1]:\n            continue \n        for feature_subset_id in genes_subsets:\n            if feature_subset_id not in ['All genes']: #, '149 all ALS related']:\n                continue\n\n            t0 = time.time()\n\n            list_features = genes_subsets[ feature_subset_id]\n\n            y = df.iloc[:,1]\n            #X = df[list_features]# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n            #X = np.random.randn(X.shape[0], X.shape[1] )\n            print('Data name:', data_name,'shapes:', X.shape, y.shape, 'y.sum:', y.sum(), 'feature_subset_id',feature_subset_id )\n            t0 = time.time()\n\n            #model_cv.fit(X,y)\n            #score_cv = cross_val_score(model, X, y, cv=skf, scoring = 'roc_auc')\n            #print('score_cv', score_cv )\n            #score_cv_mean = np.mean( score_cv )\n            predict_cv = cross_val_predict(model, X, y, cv=skf , method='predict_proba') \n            score_cv_mean = roc_auc_score(y, predict_cv[:,1] )\n            tt1 = np.round(time.time()-t0,1)\n            print('Score:',score_cv_mean, 'seconds passed: ', tt1 )\n\n\n            models_results_info.loc[model_index,'RocAuc CV'] = score_cv_mean # np.round(model_cv.best_score_ ,3)\n            models_results_info.loc[model_index,'Data'] = data_name\n            models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n\n            models_results_info.loc[model_index,'Model'] = str_model_inf # 'PCA + Logistic'\n            # models_results_info.loc[model_index,'Params'] = 'PCA6 LR_C'+str(C)+'L1'#str( model_cv.best_params_)\n            \n            models_results_info.loc[model_index,'n_features'] = X.shape[1]\n            models_results_info.loc[model_index,'Seconds Passed'] = tt1\n            model_index += 1\n    \nprint(np.round(time.time()-t00),'total seconds passed')    \n    \nmodels_results_info","a6a41863":"from sklearn.preprocessing import StandardScaler\n#from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nimport time \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport umap\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nscaler = StandardScaler()\npca = umap.UMAP(n_components=20, random_state = 0 ) # PCA(n_components=20, random_state = 0)\nC = 0.1 # 0.1\nclf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\n#model = Pipeline([('PCA', pca ), ('Scaler',  scaler ), ('Logistic', clf )])\nmodel = Pipeline([('PCA', pca ), ('Logistic', clf )])\nfrom sklearn.neighbors import NearestNeighbors\nclf = NearestNeighbors(n_neighbors=1)\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=5)\n\nmodel = Pipeline([('PCA', pca ), ('Logistic', clf )])\n\n# param_grid = {\"max_depth\":[1 , 5], \"n_estimators\":[5,20], } \n\n\nt00 = time.time()\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\nfor i,df in  enumerate(list_data_df):\n    data_name = list_data_names[i]\n    if i in [0,1]:\n        continue \n    for feature_subset_id in genes_subsets:\n        if feature_subset_id not in ['All genes']: #, '149 all ALS related']:\n            continue\n        \n        t0 = time.time()\n        \n        list_features = genes_subsets[ feature_subset_id]\n\n        y = df.iloc[:,1]\n        #X = df[list_features]# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n        #X = np.random.randn(X.shape[0], X.shape[1] )\n        print('Data name:', data_name,'shapes:', X.shape, y.shape, 'y.sum:', y.sum(), 'feature_subset_id',feature_subset_id )\n        t0 = time.time()\n\n        #model_cv.fit(X,y)\n        #score_cv = cross_val_score(model, X, y, cv=skf, scoring = 'roc_auc')\n        #print('score_cv', score_cv )\n        #score_cv_mean = np.mean( score_cv )\n        predict_cv = cross_val_predict(model, X, y, cv=skf , method='predict_proba') \n        score_cv_mean = roc_auc_score(y, predict_cv[:,1] )\n        tt1 = np.round(time.time()-t0,1)\n        print('Score:',score_cv_mean, 'seconds passed: ', tt1 )\n        \n\n        models_results_info.loc[model_index,'RocAuc CV'] = score_cv_mean # np.round(model_cv.best_score_ ,3)\n        models_results_info.loc[model_index,'Data'] = data_name\n        models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n\n        models_results_info.loc[model_index,'Model'] = 'PCA + Logistic'\n        models_results_info.loc[model_index,'Params'] = 'PCA6 LR_C'+str(C)+'L1'#str( model_cv.best_params_)\n        models_results_info.loc[model_index,'Seconds Passed'] = tt1\n        model_index += 1\n    \nprint(np.round(time.time()-t00),'total seconds passed')    \n    \nmodels_results_info","a0b7dac6":"from sklearn.preprocessing import StandardScaler\n#from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nimport time \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nscaler = StandardScaler()\npca = PCA(n_components=6, random_state = 0)\nC = 0.1\nclf = LogisticRegression( C = C,  random_state=0, solver = 'liblinear', penalty = 'l1')\nmodel = Pipeline([('PCA', pca ), ('Scaler',  scaler ), ('Logistic', clf )])\n# param_grid = {\"max_depth\":[1 , 5], \"n_estimators\":[5,20], } \n\n\nt00 = time.time()\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\nfor i,df in  enumerate(list_data_df):\n    data_name = list_data_names[i]\n    if i in [0,1]:\n        continue \n    for feature_subset_id in genes_subsets:\n        #if feature_subset_id in ['All genes', '149 all ALS related']:\n        #    continue\n        \n        t0 = time.time()\n        \n        list_features = genes_subsets[ feature_subset_id]\n\n        y = df.iloc[:,1]\n        X = df[list_features]# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n        X = np.random.randn(X.shape[0], X.shape[1] )\n        print('Data name:', data_name,'shapes:', X.shape, y.shape, 'y.sum:', y.sum(), 'feature_subset_id',feature_subset_id )\n        t0 = time.time()\n\n        #model_cv.fit(X,y)\n        #score_cv = cross_val_score(model, X, y, cv=skf, scoring = 'roc_auc')\n        #print('score_cv', score_cv )\n        #score_cv_mean = np.mean( score_cv )\n        predict_cv = cross_val_predict(model, X, y, cv=skf , method='predict_proba') \n        score_cv_mean = roc_auc_score(y, predict_cv[:,1] )\n        tt1 = np.round(time.time()-t0,1)\n        print('Score:',score_cv_mean, 'seconds passed: ', tt1 )\n        \n\n        models_results_info.loc[model_index,'RocAuc CV'] = score_cv_mean # np.round(model_cv.best_score_ ,3)\n        models_results_info.loc[model_index,'Data'] = data_name\n        models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n\n        models_results_info.loc[model_index,'Model'] = 'PCA + Logistic'\n        models_results_info.loc[model_index,'Params'] = 'PCA6 LR_C'+str(C)+'L1'#str( model_cv.best_params_)\n        models_results_info.loc[model_index,'Seconds Passed'] = tt1\n        model_index += 1\n    \nprint(np.round(time.time()-t00),'total seconds passed')    \n    \nmodels_results_info","961c911c":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\n\nscaler = StandardScaler()\n\n\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\n\nfor i,df in  enumerate(list_data_df):\n    for feature_subset_id in genes_subsets:\n        list_features = genes_subsets[ feature_subset_id]\n        \n        data_name = list_data_names[i]\n        y = df.iloc[:,1]\n        X = df[list_features]# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n\n        pca = PCA()# n_components=2)\n        r = pca.fit_transform(X)\n        \n        for k in range(10):\n            roc = roc_auc_score(y, r[:,k])\n            models_results_info.loc[model_index,'RocAuc'] = np.round(roc ,3)\n            models_results_info.loc[model_index,'RocAuc Abs'] = np.round( np.abs(roc-0.5)+0.5 ,3)\n            models_results_info.loc[model_index,'Data'] = data_name\n            models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n\n            models_results_info.loc[model_index,'Model'] = 'Just PCA '\n            models_results_info.loc[model_index,'Params'] = k \n            #models_results_info.loc[model_index,'Seconds Passed'] = tt1\n            model_index += 1\n            \n            \nprint(model_index)    \nmodels_results_info","23a60f31":"models_results_info.sort_values('RocAuc Abs', ascending = False ).head(20)","4e56bb2c":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\n\nscaler = StandardScaler()\n\n\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\n\nfor i,df in  enumerate(list_data_df):\n    for feature_subset_id in genes_subsets:\n        list_features = genes_subsets[ feature_subset_id]\n        \n        data_name = list_data_names[i]\n        y = df.iloc[:,1]\n        X = df[list_features].values# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n        v = np.std(X, axis = 0 )\n        IX = np.argsort(v)\n        X = X[:,IX[-10000:]]\n        X = scaler.fit_transform(X)\n\n        pca = PCA()# n_components=2)\n        r = pca.fit_transform(X)\n        \n        for k in range(10):\n            roc = roc_auc_score(y, r[:,k])\n            models_results_info.loc[model_index,'RocAuc'] = np.round(roc ,3)\n            models_results_info.loc[model_index,'RocAuc Abs'] = np.round( np.abs(roc-0.5)+0.5 ,3)\n            models_results_info.loc[model_index,'Data'] = data_name\n            models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n\n            models_results_info.loc[model_index,'Model'] = 'Scaler PCA  '\n            models_results_info.loc[model_index,'Params'] = k \n            #models_results_info.loc[model_index,'Seconds Passed'] = tt1\n            model_index += 1\n            \n            \nprint(model_index)    \nmodels_results_info","2f104367":"models_results_info.sort_values('RocAuc Abs', ascending = False ).head(20)","3fbff2ab":"from sklearn.ensemble import RandomForestClassifier\n\nprint('Default params:')\nclf = RandomForestClassifier( random_state=0)\nclf.get_params()","b599fd67":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nimport time \nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nskf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\n\nclf = RandomForestClassifier( random_state=0)\n# param_grid = {\"max_depth\":[1 , 5], \"n_estimators\":[5,20], } \n\nparam_grid = {\"max_depth\":[None, 1,2,3,5], \"n_estimators\":[5,20,100], \n            'bootstrap' : [True, False],\n            'max_features' : ['auto', 'sqrt'],\n            'min_samples_split' : [2, 5, 10],\n            'min_samples_leaf' : [1, 2, 4]} #1e-7,1e-6, 5e-6,1e-5, 2e-5,1e-4,1e-3,1e-2,1], \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n\nt00 = time.time()\n\nfor i,df in  enumerate(list_data_df):\n    for feature_subset_id in genes_subsets:\n        list_features = genes_subsets[ feature_subset_id]\n        \n        data_name = list_data_names[i]\n        y = df.iloc[:,1]\n        X = df[list_features]# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n        print('Data name:', data_name,'shapes:', X.shape, y.shape, 'y.sum:', y.sum(), 'feature_subset_id',feature_subset_id )\n        t0 = time.time()\n\n        model_cv=GridSearchCV(clf,param_grid,cv=skf, scoring = 'roc_auc')\n        model_cv.fit(X,y)\n        tt1 = np.round(time.time()-t0,1)\n        print('Grid search finished. ', tt1,'seconds passed')\n        print(\"tuned hyperparameters (best parameters) \",model_cv.best_params_)\n        print(\"best roc_auc :\",model_cv.best_score_)\n        print()\n\n        models_results_info.loc[model_index,'RocAuc CV'] = np.round(model_cv.best_score_ ,3)\n        models_results_info.loc[model_index,'Data'] = data_name\n        models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n        \n        models_results_info.loc[model_index,'Model'] = 'Random Forest'\n        models_results_info.loc[model_index,'Params'] = str( model_cv.best_params_)\n        models_results_info.loc[model_index,'Seconds Passed'] = tt1\n        model_index += 1\n    \nprint(np.round(time.time()-t00),'total seconds passed')    \n    \nmodels_results_info","ac59da45":"pd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\npd.set_option('display.max_columns', None)  \npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', -1)\n\nmodels_results_info","18ecbde0":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nimport time \nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nskf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\nmodels_results_info = pd.DataFrame()\nmodel_index = 0\n\nclf = RandomForestClassifier( random_state=0)\n# param_grid = {\"max_depth\":[1 , 5], \"n_estimators\":[5,20], } \n\nparam_grid = {\"max_depth\":[None, 1,2,3,5], \"n_estimators\":[5,20,100], \n            'bootstrap' : [True, False],\n            'max_features' : ['auto', 'sqrt'],\n            'min_samples_split' : [2, 5, 10],\n            'min_samples_leaf' : [1, 2, 4]} #1e-7,1e-6, 5e-6,1e-5, 2e-5,1e-4,1e-3,1e-2,1], \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n\nt00 = time.time()\n\nfor repeat_index in range(100):\n    for i,df in  enumerate(list_data_df[:1]):\n        for feature_subset_id in genes_subsets:\n            if feature_subset_id in ['All genes', '149 all ALS related']:\n                continue\n            list_features = genes_subsets[ feature_subset_id]\n\n            data_name = list_data_names[i]\n            y = df.iloc[:,1]\n            X = df[list_features]# .iloc[:,2:].values # [list_genes].values # .iloc[:,2:].values\n            X = np.random.randn(X.shape[0], X.shape[1] )\n            print('Data name:', data_name,'shapes:', X.shape, y.shape, 'y.sum:', y.sum(), 'feature_subset_id',feature_subset_id )\n            t0 = time.time()\n\n            model_cv=GridSearchCV(clf,param_grid,cv=skf, scoring = 'roc_auc')\n            model_cv.fit(X,y)\n            tt1 = np.round(time.time()-t0,1)\n            print('Grid search finished. ', tt1,'seconds passed')\n            print(\"tuned hyperparameters (best parameters) \",model_cv.best_params_)\n            print(\"best roc_auc :\",model_cv.best_score_)\n            print()\n\n            models_results_info.loc[model_index,'RocAuc CV'] = np.round(model_cv.best_score_ ,3)\n            models_results_info.loc[model_index,'Data'] = data_name\n            models_results_info.loc[model_index,'Feature susbet'] = feature_subset_id\n\n            models_results_info.loc[model_index,'Model'] = 'Random Forest'\n            models_results_info.loc[model_index,'Params'] = str( model_cv.best_params_)\n            models_results_info.loc[model_index,'Seconds Passed'] = tt1\n            model_index += 1\n    \nprint(np.round(time.time()-t00),'total seconds passed')    \n    \nmodels_results_info","cc65b05b":"models_results_info.describe()","461018af":"# Download list of genes known from literature to be associated with ALS with various degree of confidence","be97bb47":"# Various dimensional reductions - PCA,ICA,NMF,LDA, umap","db0e3234":"## QuantileTransform 30 , EXCLUDE PCA from pipeline, LogReg","b5f70924":"# QuantileTransformer","896e214b":"## QuatileTransform10 + PCA 20 + LogReg","2559a00a":"# Testing modeling scheme on random data - shows that previous modeling scheme is UNRELIABLE - the reason is in GridSearch.\n\nWe take the first dataset and the smallest considered number of features - 48.\nAnd generate completely random features of the same size. And look for the RocAuc we obtain from several trials.\n\nJust the third trial gives us RocAuc 0.68 and nineth trial gives 0.7 which is better than modeling 0.66.\nThus the modeling is unreliable.\n\nActually we can understand what happening without simulation.\nThe problem is that when we make GridSearch with big number of possible params - everytime we get a random RocAuc with std about 0.05 (that std have been estimated https:\/\/www.kaggle.com\/alexandervc\/als-transciptomics-no-markers-beware-overfits ) \nthus taking the best would correspond just taking the best value from  mean = 0.5 , std = 0.05 random variable,\nwhich of course can be easyly go quite high when number of params configurations is high. \n\n**The possible solution** - choose params with one  split(s) to folds, but then estimate the quality of the model\nretraining it on other split fold or even averaging several splits to folds.  (Or even more sophisticated schemes like described here: https:\/\/habr.com\/ru\/company\/ods\/blog\/336168\/ )\n","cb2232c8":"# Look at PCA components as predictors\n\nResults are not so bad - we get auc above 0.67 within top5 componets for two targets - case\/non case, and median\/vs high","65644fbf":"# Load genes list which \"interact\" (according to BIOGRUD database) with known ALS genes\n\n(see script: https:\/\/www.kaggle.com\/alexandervc\/genes-interacting-with-als-genes how the list was obtained). \n","0d043a65":"## QuantileTransform 3 , EXCLUDE PCA from pipeline, LogReg","4e9145cc":"# Random Forest modeling","2b2ee4d5":"## QuatileTransform10 + PCA 100 + LogReg","09d9b06e":"### Remark: Compare with single gene rocaucs which can be found Single gene performances for 149 genes can be found here: https:\/\/www.kaggle.com\/alexandervc\/als-transciptomics-eda?scriptVersionId=59339623&cellId=42\n\nwe can see not a big gain \n","ccbbf092":"# KBinsDiscretizer ","c56f7f46":"## QuantileTransform 2 , EXCLUDE PCA from pipeline, LogReg","a1193c8c":"# What is about ? PRELIMINARY DRAFT TO BE MODIFIED \n\n\n\nNotebook for https:\/\/www.kaggle.com\/alsgroup\/end-als\n\nWe consider predictive models based on transcriptomics data to predict targets given to us.\nThere are three targets which are defined on subsets on the main dataset (see details below). \nSo we are considering three models. \n\nThe number of samples is quite low - at most 163 with more than 53 000 featurs.\nThat means - OVERFIT is the main problem. \n\n\n----------------------------------\n\n## Conclusion\n\nNo good model yet found.\nNeed to research further. \n\n----------------------------------\n\n## Version 12:  same as 11, but dimreduction to 100 dimensions Various dimensional reductions - PCA,ICA,NMF,LDA, umap\n\nauc for PCA - 0.59, for NMF - 0.6\n\n## Version 11: Various dimensional reductions - PCA,ICA,NMF,LDA, umap\n\ndimensional reduction to 20 dimensions\n\nbest auc 0.57 \n\n## Version 10: KBinsDiscretizer  + [PCA] + LR\n\nbest auc 61 - again not good and probably(?) random \n\n\n## Version 8,9: Similar to version 7 - try without PCA , play with quantile_number\n\nSeems without pca it is a little-little better, but auc is too low.\n\n## Version 7: Filter genes , try QuantileTransform + PCA + LogRreg \n\nFiltered genes by considering only those interacting (accordring to BIOGRID database) with known ALS genes and also filtering by expression level. Then pipeline QuantileTransform PCA, LogReg. \n\n\n## Version 6: Filter genes and try umap, knn \n\nFiltered genes by considering only those interacting (accordring to BIOGRID database) with known ALS genes and also filtering by expression level. \n\nThen several schemes with PCA\/UMAP + LogReg\/KNN  - tried  - no good results at least on \"case not case\" dataset.\n(Result saved for one example - but tried several). \n\n\n## Version 5: Logistic + PCA \n\nPreliminary result are not good - prediction are quite bad , despite some single components of pca seems to be not bad predictors as we seen below. That is strange. Also results are randomly changing from run to run, despite random_states seems to be fixed.\n\n\n##  Version 4: Look at PCA components as predictors\n\nResults are not so bad - we get auc above 0.67 within top5 componets for two targets - case\/non case, and median\/vs high\n\n\n## Versions 2,3:  Random Forest and genes subsets from https:\/\/alsod.ac.uk\/ and testing on random \n\n#### Versions 2:  Random Forest and genes subsets from https:\/\/alsod.ac.uk\/ \n\nWe consider random forest models and study models based only on genes subsets which are known to be related to ALS from https:\/\/alsod.ac.uk\/.\n\nNon of the models shows much better results than just best single gene predictor from the list 149 known genes.\nOnly 1-2 rocauc better for the case of the 1,3 datasets.\nHowever it might be hyperparams for random forest should better optimized.\nAnd what is interesting on the first task best performing is the model on the least number of genes - just those top 48 genes from https:\/\/alsod.ac.uk\/ . \n\nSingle gene performances for 149 genes can be found here: https:\/\/www.kaggle.com\/alexandervc\/als-transciptomics-eda?scriptVersionId=59339623&cellId=42\n\n#### Version 3: test on random features - see that previous results unreliable, propose the solution \n\nWe take the first dataset and the smallest considered number of features - 48.\nAnd generate completely random features of the same size. And look for the RocAuc we obtain from several trials.\n\nJust the third trial gives us RocAuc 0.68 and nineth trial gives 0.7 which is better than modeling 0.66.\nThus the modeling is unreliable.\n\nActually we can understand what happening without simulation.\nThe problem is that when we make GridSearch with big number of possible params - everytime we get a random RocAuc with std about 0.05 (that std have been estimated https:\/\/www.kaggle.com\/alexandervc\/als-transciptomics-no-markers-beware-overfits ) \nthus taking the best would correspond just taking the best value from  mean = 0.5 , std = 0.05 random variable,\nwhich of course can be easyly go quite high when number of params configurations is high. \n\n**The possible solution** - choose params with one  split(s) to folds, but then estimate the quality of the model\nretraining it on other split fold or even averaging several splits to folds.  (Or even more sophisticated schemes like described here: https:\/\/habr.com\/ru\/company\/ods\/blog\/336168\/ )\n\n\n## Version 1:  Consider Logistic regression with regularization. \n\n\n#### Briefly conclusions: The outcome is rather negative (i.e. models does not seem to be worthy predictors) by the following reason - linear model with regularization is senstive to scaling of features, and it just selected features with greates values - not just good predictors, but just big ones. In next version we will try to avoid that problem \n\n\nWe get optimal is L1 (Lasso) regularization with C about 1e-5 for all three datasets.\n\nSelected genes are similar in all three tasks, however they include MALAT1 and MT.RNR2 - that are often high expressed genes.\nSo probably that selection does not have a big sense. \n\n(See for example https:\/\/www.kaggle.com\/alexandervc\/sciplex2-part-1-eda?scriptVersionId=57667001 cell 22 - MALAT1 abd MT.RNR2  - on top  , or https:\/\/scanpy-tutorials.readthedocs.io\/en\/latest\/pbmc3k.html cell 8  - MALAT1 - on top ) \n\n\n#### bulbar_vs_limb data:\n\nC = 1e-5, RocAuc - 0.63, 6 feutures selected: \n['COL3A1', 'X7SK.4', 'NEFL', 'MALAT1', 'SNORD3C', 'MT.RNR2']\n\n#### median_low_vs_high data:\n\nC = 1e-5,  RocAuc 0.58, 5 features -  ['EEF1A1', 'NEFL', 'MALAT1', 'SNORD3C', 'MT.RNR2']\n\n#### ctrl_vs_case data:\n\nC = 2e-5, RocAuc  0.57, 13 features ['SOX11', 'X7SK.4', 'XIST', 'DCX', 'NEFL', 'STMN2', 'MALAT1', 'RN7SL2',\n        'SNORD3A', 'ONECUT2', 'MT.RNR2', 'MT.CO1', 'ENSG00000277048']\n\nSubbest: \n\nC = 1e-5, rouauc = 0.56, 5 features ['X7SK.4', 'XIST', 'NEFL', 'MALAT1', 'MT.RNR2']\n\n\n(Actual rocauc may vary from launch to launch - we did not fixed random state for logreg , but fixed for folds generator)\n\n\n\n\nPSPS\n\n\nhttps:\/\/en.wikipedia.org\/wiki\/Amyotrophic_lateral_sclerosis\n\nhttps:\/\/en.wikipedia.org\/wiki\/Amyotrophic_lateral_sclerosis_research\n\nhttps:\/\/en.wikipedia.org\/wiki\/Genetics_of_amyotrophic_lateral_sclerosis\n\n","6826bd2d":"# Load transcriptomics data","891ce4a7":"## Qauntile trasform 3, PCA 100 ","2c77a746":"# Modeling - several schemes with PCA\/UMAP + LogReg\/KNN  - tried  - no good results at least on \"case not case\" dataset\n\n\n","27cd5225":"# Filter genes by interaction list and by expression","2fd9233f":"## Filter out 5 genes not found in our data \n\nmay be they just  have  different symbols in our data - later plan to check it - but seems to so importand for the moment, since they are marked as \"Tenuously\" associated ","abd5427a":"# Logistic + PCA \n\nPreliminary result are not good - prediction are quite bad , despite some single components of pca seems to be not bad predictors as we seen below. That is strange. Also results are randomly changing from run to run, despite random_states seems to be fixed.\n\n"}}