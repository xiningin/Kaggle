{"cell_type":{"0dfbf94e":"code","b1276899":"code","48c94c9b":"code","a06cf3af":"code","ef2e75f6":"code","66fe2ecc":"code","7b8d6576":"code","89764b17":"code","d0400202":"code","4436d3f2":"code","570fcfc6":"markdown","0fa2026f":"markdown"},"source":{"0dfbf94e":"### This notebook is based on Josh Starmer detaled youtube tutorial on XGBoost\n### https:\/\/youtu.be\/GrJP9FLV3FE\nimport pandas as pd, numpy as np\nimport lightgbm\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\n\npath = '..\/input\/telco-customer-churn\/'","b1276899":"df = pd.read_csv(path + 'Telco_customer_churn.csv', sep=';')\ndf.head(3)","48c94c9b":"# Let's delete data leakage columns and columns with 1 unique catigory\ndrop_cols = ['Churn Label', 'Churn Score', 'CLTV', 'Churn Reason', 'Count', 'Country', 'State', 'Lat Long', 'CustomerID']\ndf.drop(drop_cols, axis=1, inplace=True)","a06cf3af":"# We already know that NaNs are not mapped. But we have white spaces in data, so we will replace them with NaNs\n# df.loc[df['Total Charges'] == ' ']\ndf.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n# df.isna().sum()","ef2e75f6":"# Replace ',' with '.' to convert to float\nto_be_numeric = ['Latitude', 'Longitude', 'Monthly Charges', 'Total Charges']\n\nfor col in to_be_numeric:\n    df[col].replace(',', '.', regex=True, inplace=True)\n    df[col] = pd.to_numeric(df[col])","66fe2ecc":"# Let's find catigorical columns\ncat_columns = [cname for cname in df.columns if df[cname].dtype == \"object\"]\n\n# We need 'categorical' type for categorical columns for lightgbm\nfor col in df.columns:\n    if col in cat_columns:\n        df[col] = df[col].astype('category')","7b8d6576":"# Splitting the data \nX = df.drop('Churn Value', axis=1)\ny = df['Churn Value']\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)","89764b17":"# Base model\nlgb_base = lightgbm.LGBMClassifier(random_state=0)\nlgb_base.fit(X_train, y_train, eval_set=(X_valid, y_valid), eval_metric='auc', verbose=0)\n\nprint(f'roc_auc of base model: {roc_auc_score(y_valid, lgb_base.predict(X_valid))}\\n')\nprint(classification_report(y_valid, lgb_base.predict(X_valid)))\nplot_confusion_matrix(lgb_base, X_valid, y_valid, values_format='d', display_labels=['Did not leave', 'Left'])","d0400202":"%%time\nparam_grid = {\n              'max_depth': [4, 9, -1],\n              'num_leaves': [4, 9],\n              'learning_rate': [0.1],\n              'scale_pos_weight': [3],\n              'n_estimators': [50, 100],\n              'reg_lambda': [10, 15],\n              'subsample': [0.9],\n              'colsample_bytree': [0.5, 0.6]\n              }\n\nlgb = lightgbm.LGBMClassifier(random_state=1)\n\nopt_params = GridSearchCV(estimator=lgb,\n                           param_grid=param_grid,\n                           scoring='roc_auc',\n                           cv=3)\n\nopt_params.fit(X_train, y_train, eval_metric='auc', eval_set=(X_valid, y_valid), verbose=0)\nparams = opt_params.best_params_\nprint(params)","4436d3f2":"lgb = lightgbm.LGBMClassifier(**params)\nlgb.fit(X_train, y_train)\n\nprint(f'roc_auc of final model: {roc_auc_score(y_valid, lgb.predict(X_valid))}\\n')\nprint(classification_report(y_valid, lgb.predict(X_valid)))\nplot_confusion_matrix(lgb, X_valid, y_valid, values_format='d', display_labels=['Did not leave', 'Left'])","570fcfc6":"### Now we can indentify 83% of churn and can do something to prevent that :)","0fa2026f":"### With base model we can see that 166 cases (46%) of churn were not identified. In this kind of task you would want lower false negatives than false positives, so we will tune that."}}