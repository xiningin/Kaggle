{"cell_type":{"7ff209d3":"code","366c2a45":"code","f0d80c64":"code","accdda8e":"code","88e085a3":"code","742d0d2f":"code","78758249":"code","aeec8ff5":"code","e226f7d0":"code","4a0c7b5e":"code","afc4a2a0":"code","a055e2db":"code","36f2134e":"code","b46618b4":"code","39c06f4f":"code","110d950c":"code","36512d96":"code","9e62e2ce":"code","99841efc":"code","5f23cffb":"code","537ae1bc":"code","08dbfe7f":"code","40e1464e":"code","8f71aaf1":"code","f919d03e":"code","9c1389fb":"code","9e54b2ea":"code","fffded19":"code","c3161351":"code","e4f21a49":"code","38d01124":"code","1d67a4f1":"code","9a550454":"code","74228ba2":"code","63f9682e":"code","175e1d89":"code","6c287ff7":"markdown","dbcc7ffb":"markdown","eb422fe2":"markdown","2ef795db":"markdown","00a7a813":"markdown","2ec81df4":"markdown","6171c22a":"markdown","6f0c758b":"markdown","d53d1dc1":"markdown","18635cea":"markdown","972c944f":"markdown","a590ca28":"markdown"},"source":{"7ff209d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n!pip install pydub\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy, scipy, matplotlib.pyplot as plt\nimport os\n# Input import numpy, scipy, matplotlib.pyplot as plt files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\n# Any results you write to the current directory are saved as output.","366c2a45":"csv = pd.read_csv('\/kaggle\/input\/urbansound8k\/UrbanSound8K.csv')\ncsv","f0d80c64":"#retorna o caminho completo de cada arquivo do CSV. \ndef path_class(filename):\n    #cria um filtro com a linha onde o arquivo est\u00e1. \n    excerpt = csv[csv['slice_file_name'] == filename]\n    \n    #cria o path completo\n    path_name = os.path.join('\/kaggle\/input\/urbansound8k\/', 'fold'+str(excerpt.fold.values[0]), filename)\n    return path_name, excerpt['class'].values[0]","accdda8e":"#retorna o audio e informa\u00e7\u00f5es do mesmo. \ndef to_dataset (df, fold=[]):\n    # df = dataframe a ser percorrido.\n    # fold = qual\/quais folds ler. \n    # quando fold nulo, significa para ler tudo.     \n    audio = []\n    audio_signals = []\n    label= []\n    labels=[]\n    paths=[]   \n    sampling_rate=[]\n    librosa_sampling_rate = []\n    \n    \n    if fold != []:\n        #filtra somentes os folds que foram enviados.  \n        filter_fold = df.fold.isin(fold)\n        df = df[filter_fold]\n\n        \n    #df = df.head(100)   \n    #para cada fold, pega todos os arquivos de dentro.            \n    for i in (df.fold.unique()):\n        #filtra o cada fold em cada itera\u00e7\u00e3o\n        filter_slice =  df['fold']==i\n        dt_fold = df[filter_slice]\n        \n        #Itera\u00e7\u00e3o para ler os arquivos da pasta fold da vez.\n        for p in dt_fold['slice_file_name']:\n            # Librosa j\u00e1 converte os dois canais para um canal e normaliza os dados entre 1 e -1. \n            audio,librosa_sampling_rate  = librosa.load('\/kaggle\/input\/urbansound8k\/fold'+str(i)+'\/' + p)\n            audio_signals.append(audio)\n            sampling_rate.append(librosa_sampling_rate)\n            \n            #busca as classes e paths.\n            path, label = path_class(p)\n            paths.append(path)\n            labels.append(label)\n\n    \n    print('Reading...')    \n    \n    #audio = cont\u00e9m os arquivos de audio\n    #paths = cont\u00e9m o caminho completo do arquivo.\n    #labels = contem as classifica\u00e7\u00f5es, \n    #librosa_sampling_rate = cont\u00e9m o sampling rate, que pode ser visto adiante. \n    return audio_signals,paths,labels,sampling_rate","88e085a3":"# Para anlisar cada classe, primeiro \u00e9 necess\u00e1rio extrair cada uma e criar um dataframe. \ndt_class = pd.DataFrame()\ndt_fold = (csv[csv['fold']==1])\nfor i in (dt_fold.classID.unique()):\n    dt_class = dt_class.append(dt_fold[dt_fold['classID']==i].head(1))    \ndt_class","742d0d2f":"import librosa\nsample, sample_path, sample_label, sample_S_Rate = to_dataset(dt_class)","78758249":"from pydub import AudioSegment\n\ncombined_sounds = AudioSegment.silent(duration=1000)\n\nfor x in sample_path:\n    sound = AudioSegment.from_wav(x)\n    combined_sounds =  combined_sounds +  AudioSegment.silent(duration=2000)  + sound\n\ncombined_sounds.export(\"joinedFile.wav\", format=\"wav\")","aeec8ff5":"import struct\nimport IPython.display as ipd\nipd.Audio('..\/working\/joinedFile.wav')","e226f7d0":"%matplotlib inline\nimport numpy, scipy, matplotlib.pyplot as plt, IPython.display as ipd\nimport librosa, librosa.display\nplt.rcParams['figure.figsize'] = (14, 5)","4a0c7b5e":"plt.style.use('seaborn-muted')\nplt.rcParams['figure.figsize'] = (14, 5)\nplt.rcParams['axes.grid'] = True\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.bottom'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.xmargin'] = 0\nplt.rcParams['axes.ymargin'] = 0\nplt.rcParams['image.cmap'] = 'gray'\nplt.rcParams['image.interpolation'] = None","afc4a2a0":"# Usando dois exemplos. O que eu criei e o dado para o trabalho.\n#..\/working\/joinedFile.wav\n#..\/input\/exemplo2\/exemplo2.wav\nsignal1, sr1 = librosa.load('..\/input\/exemplos-recebidos\/exemplo.wav')\nsignal2, sr2 = librosa.load('..\/input\/exemplos-recebidos\/exemplo2.wav')\nsignal3, sr3 = librosa.load('..\/input\/exemplos-recebidos\/exemplo3.wav')\nsignaljoined, srjoined = librosa.load('..\/working\/joinedFile.wav')\n","a055e2db":"#Plotando o audio\nlibrosa.display.waveplot(signal1, sr=sr1)\n# Audio lido\nipd.Audio(signal1, rate=sr1)","36f2134e":"#Plotando o audio\nlibrosa.display.waveplot(signal2, sr=sr2)\nipd.Audio(signal2, rate=sr2)","b46618b4":"#Plotando o audio\nlibrosa.display.waveplot(signal3, sr=sr3)\nipd.Audio(signal3, rate=sr3)","39c06f4f":"#Plotando o audio\nlibrosa.display.waveplot(signaljoined, sr=srjoined)\nipd.Audio(signaljoined, rate=srjoined)","110d950c":"# funcao plotar a energia e o RMS\ndef plot_RMSE(signal,sr):\n    # Determina o hop e o frame para come\u00e7ar a calcular a energia e o RMS\n    hop_length = 256\n    frame_length = 512\n    energy = numpy.array([\n        sum(abs(signal[i:i+frame_length]**2))\n        for i in range(0, len(signal), hop_length)\n    ])\n    rmse = librosa.feature.rms(signal, frame_length=frame_length, hop_length=hop_length, center=True)\n    rmse = rmse[0]\n    frames = range(len(energy))\n    t = librosa.frames_to_time(frames, sr=sr, hop_length=hop_length)\n    librosa.display.waveplot(signal, sr=sr, alpha=0.4)\n    plt.plot(t, energy\/energy.max(), 'r--')             # normalized for visualization\n    plt.plot(t[:len(rmse)], rmse\/rmse.max(), color='g') # normalized for visualization\n    plt.legend(('Energy', 'RMSE'))","36512d96":"print('Exemplo.wav')\nplot_RMSE(signal1,sr1)","9e62e2ce":"print('Exemplo2.wav')\nplot_RMSE(signal2,sr2)","99841efc":"print('Exemplo3.wav')\nplot_RMSE(signal3,sr3)","5f23cffb":"print('Arquivo com 10 Classes.wav')\nplot_RMSE(signaljoined,srjoined)","537ae1bc":"#funcao para pegar o inicio de cada aumento de energia e seu final. \ndef strip(signal, frame_length, hop_length, index, thresh):\n\n    # Compute RMSE.\n    rms = librosa.feature.rms(signal, frame_length=frame_length, hop_length=hop_length, center=True)  \n\n    # inicializa um ponteiro na primeira posicao.     \n    frame_index = index\n\n    # Anda ate achar aumento no rms\n    while rms[0][frame_index] < thresh and frame_index < len(rms[0])-1:\n        frame_index += 1\n     \n    # Converte os frames em samples\n    start_sample_index = librosa.frames_to_samples(frame_index, hop_length=hop_length)\n    \n    #remove a parte do vetor que ja foi lida.         \n    frame_index_2 = frame_index\n    \n    #anda ate encontrar queda na energia abaixo do threeshold.\n    while  rms[0][frame_index_2] > thresh and frame_index_2 < len(rms[0])-1:\n        frame_index_2 += 1       \n    \n    #Converte os frames    \n    end_sample_index = librosa.frames_to_samples(frame_index_2, hop_length=hop_length)\n       \n    #signal \u00e9 o peda\u00e7o de audio.\n    # frame_index_2 \u00e9 o ponteiro onde a leitura parou\n    # len(rms...) \u00e9 para que o loop externo pare caso o arquivo tenha acabado. \n    return signal[start_sample_index:end_sample_index], frame_index_2, len(rms[0])-1","08dbfe7f":"# Pegando o primeiro arquivo e colocando em uma vari\u00e1vel unica que vai ser usada pra todos. \nsignal = signaljoined\nsr = srjoined\nipd.Audio(signal, rate=sr)","40e1464e":"\n#loop para pegar cada som do arquivo. \nextracted_signals = []\nindex = 0\nstop=0\nthresh = 0.002\nhop_length = 256\nframe_length = 512\n# Enquanto nao identificar o fim do arquivo...\nwhile stop !=1:\n    y,index,quit = strip(signal, frame_length, hop_length,index,thresh)\n    extracted_signals.append(y)\n    if index >= quit:\n        stop=1\n\n#deleta a ultima posicao que n\u00e3o armazena nada.\nextracted_signals= extracted_signals[:-1]\n\nprint ('Quantidade De Audios Encontrados')\nprint(len(extracted_signals))","8f71aaf1":"#exemplo de audio extraido\nipd.Audio(extracted_signals[1], rate=sr)","f919d03e":"# O top_db varia de acordo com o arquivo. O melhor resultado foi 62 que conseguiu separar um audio com 10 classes.\n# Nos demais arquivos, fica entre 40 e 50. \ndef cut_signal(signal, top_db):\n    y = librosa.effects.split(signal,top_db=top_db)\n    extracted_signals = []\n    for i in y:\n        extracted_signals.append( signal[i[0]:i[1]] )\n        #emphasized_signal = np.concatenate(l,axis=0)\n\n    return extracted_signals","9c1389fb":"#Lembrando de cada arquivo:\n#signal1, sr1 = librosa.load('..\/input\/exemplos-recebidos\/exemplo.wav')\n#signal2, sr2 = librosa.load('..\/input\/exemplos-recebidos\/exemplo2.wav')\n#signal3, sr3 = librosa.load('..\/input\/exemplos-recebidos\/exemplo3.wav')\n\n#Realiza o corte para cada arquivo e armazena em diferentes vari\u00e1veis. \nextracted_signals1 = cut_signal(signal1, 40)\nextracted_signals2 = cut_signal(signal2, 50)\nextracted_signals3 = cut_signal(signal3, 50)\nextracted_signalsjoined = cut_signal(signaljoined, 62)\n\n#exemplo de audio extraido\nipd.Audio(extracted_signals1[0], rate=sr)","9e54b2ea":"#Extrai as features.\ndef extract_features(signal):\n    mfccs = librosa.feature.mfcc(y=signal,  n_mfcc=40)\n    mfccs_processed = np.mean(mfccs.T,axis=0)\n     \n    return mfccs_processed    ","fffded19":"#cria um array para predi\u00e7\u00e3o para cada audio\npredict1 = []\nfor x in extracted_signals1:\n    predict1.append(extract_features(x))    \n\npredict2 = []\nfor x in extracted_signals2:\n    predict2.append(extract_features(x))    \n\npredict3 = []\nfor x in extracted_signals3:\n    predict3.append(extract_features(x))    \n    \npredictjoined = []\nfor x in extracted_signalsjoined:\n    predictjoined.append(extract_features(x))        \n# A extra\u00e7\u00e3o foi bem melhor. \nprint('Classes encontradas no Exemplo.wav')\nprint(len(predict1))\n\nprint('Classes encontradas no Exemplo2.wav')\nprint(len(predict2))\n\nprint('Classes encontradas no Exemplo3.wav')\nprint(len(predict3))\n\nprint('Classes encontradas no Exemplojoined.wav')\nprint(len(predictjoined))\n","c3161351":"#importa o modelo \nfrom keras.models import Sequential, load_model\nmodel = load_model('..\/input\/model-keras\/best_model.h5')\n","e4f21a49":"# realiza a predi\u00e7\u00e3o\nresults1 = model.predict_classes(np.array(predict1))\nresults2 = model.predict_classes(np.array(predict2))\nresults3 = model.predict_classes(np.array(predict3))\nresultsjoined = model.predict_classes(np.array(predictjoined))\n","38d01124":"import pickle\n\ndef translate(results):\n    # Para mostrar as classes como texto, usamos o dicionario criado com base no encoder do modelo.\n    d = pickle.load( open( \"..\/input\/dictionary\/dict\", \"rb\" ) )\n\n    # Converte o dicionario\n    d = {v: k for k, v in d.items()}\n\n    # Cria a coluna de labels com base do dicionario. \n    label = [d[x] for x in results]\n    df = pd.DataFrame({'classID':results, 'label':label})\n    return df","1d67a4f1":"pd_results_1 = translate(results1)\npd_results_2 = translate(results2)\npd_results_3 = translate(results3)\npd_results_joined = translate(resultsjoined)\n","9a550454":"# Compare o dataframe com a predi\u00e7\u00e3o e o audio. \nprint('Arquivo.wav')\nprint(pd_results_1)\nipd.Audio(signal1, rate=sr)\n","74228ba2":"# Compare o dataframe com a predi\u00e7\u00e3o e o audio. \nprint('Arquivo2.wav')\nprint(pd_results_2)\nipd.Audio(signal2, rate=sr2)\n","63f9682e":"# Compare o dataframe com a predi\u00e7\u00e3o e o audio. \nprint('Arquivo3.wav')\nprint(pd_results_3)\nipd.Audio(signal3, rate=sr3)","175e1d89":"# Compare o dataframe com a predi\u00e7\u00e3o e o audio. \nprint('JoinedFile.wav')\nprint(pd_results_joined)\nipd.Audio(signaljoined, rate=srjoined)","6c287ff7":"## Separa\u00e7\u00e3o do Arquivo\n\n1. Manual\n1. Autom\u00e1tica (Librosa)","dbcc7ffb":"O corte se mostrou correto!","eb422fe2":"### Extra\u00e7\u00e3o das Features","2ef795db":"### 1. Manual (Descontinuado neste trabalho...)","00a7a813":"### 2. Autom\u00e1tica (Librosa)\n\nEsta que ser\u00e1 usado para a predi\u00e7\u00e3o, pois se mostrou melhor","2ec81df4":"### Comparando os resultados!","6171c22a":"### 1 Aproveitando as fun\u00e7\u00f5es criadas na parte 1 para criar um novo som composto","6f0c758b":"## Conclus\u00e3o\n\n - No primeiro arquivo Exemplo.wav\n     - Das 3 classes, acertou 2. Confundiu som de sirene com cachorro. \n     - top_db usado foi 40, assim consegui extrair correto. \n - No arquivo Exemplo2.wav\n     - O top_db foi 50, dessa forma trazendo as 3 classes corretas. Como o arquivo anterior, gerou 6 classes. \n     - O modelo acertou 100% \n - No arquivo Exemplo3.wav\n     - O modelo errou apenas um dos 4 audios. Confundiu musica com crian\u00e7as. \n     - Mantive o top_db como 50. \n - No arquivo Joined (10 classes)\n     - Confundiu tiro com latidos.\n     - Confundiu arcondicionado com musica de rua\n     - Errou 20%, ou 2 de 10. Top_db 62 conseguiu separar todos corretamente. \n     \nEmbora a separa\u00e7\u00e3o esteja adequada, e o modelo esteja acertando uma quantidade boa, h\u00e1 como melhorar, principalmente se for aplicar mais pr\u00e9-processamentos. Acredito que alguns sons, embora da mesma classe, estando com diferentes volumes, pode atrapalhar o aprendizado. \n\nOs resultados acima foram obtidos com os dois modelos testados.\n\n","d53d1dc1":"### Observa\u00e7\u00e3o Importante\n** ! Na primeira vers\u00e3o deste trabalho, usando a separa\u00e7\u00e3o manual do arquivo e aplicando no modelo tive as seguintes conclus\u00f5es **\n- Houveram erros na predi\u00e7\u00e3o, mas mesmo em um audio que foi separado em mais partes err\u00f4neamente, o modelo n\u00e3o acusou classes diferentes. \n- \u00c9 poss\u00edvel que eu tenha errado ao buscar o dicion\u00e1rio.\n- No resultado final errou em torno de 20% das predi\u00e7\u00f5es.\n- Ele confundiu o som de Drilling com o de Crian\u00e7as. O resto acertou. Isso \u00e9 um ponto que pode ser melhorado no treinamento do modelo. \n- Na extra\u00e7\u00e3o manual n\u00e3o consegui fazer os latidos sairem no mesmo audio. \n","18635cea":"## Roteiro do trabalho\n1. Criar um som com as 10 categorias do UrbanSounb8k\n1. Separar esse arquivo novamente, identificando cada sim\n1. Carregar os arquivos de exemplo para o trabalho\n1. Aplicar no modelo treinado na primeira parte","972c944f":"## 2 - Separando o arquivo em mais de um","a590ca28":"## Aplica\u00e7\u00e3o do Modelo (Predict)"}}