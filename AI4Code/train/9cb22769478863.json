{"cell_type":{"9c9601f1":"code","1fd0d165":"code","a43f933c":"code","515f9ff5":"code","477aa4ef":"code","a5eb97bf":"code","a7861919":"code","ceb6e295":"code","446b0376":"code","c3ef3178":"code","683c8363":"markdown","eaabba10":"markdown","56d2f756":"markdown","a6d894fe":"markdown","ecc8c8e7":"markdown"},"source":{"9c9601f1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\nsns.set(font_scale=1.6)\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","1fd0d165":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n(market_train_df, news_train_df) = env.get_training_data()\n\nnews_train_df['subjects'] = news_train_df['subjects'].str.findall(f\"'([\\w\\.\/]+)'\")\nnews_train_df['audiences'] = news_train_df['audiences'].str.findall(f\"'([\\w\\.\/]+)'\")\nnews_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\.\/]+)'\")\n\n(market_train_df.shape, news_train_df.shape)","a43f933c":"# We will reduce the number of samples for memory reasons\ntoy = False\n\nif toy:\n    market_train_df = market_train_df.tail(100_000)\n    news_train_df = news_train_df.tail(300_000)\nelse:\n    market_train_df = market_train_df.tail(3_000_000)\n    news_train_df = news_train_df.tail(6_000_000)\n\n(market_train_df.shape, news_train_df.shape)","515f9ff5":"def generator(data, lookback, min_index, max_index,\n              shuffle=False, batch_size=128, step=1):\n    if max_index is None:\n        max_index = data.shape[0] - 1\n        \n    i = min_index + lookback\n    while True:\n        # Select the rows that will be used for the batch\n        if shuffle:\n            rows = np.random.randint(min_index + lookback, size=batch_size)\n        else:\n            rows = np.arange(i, min(i + batch_size, max_index))\n            i += rows.shape[0]\n            if i + batch_size >= max_index:\n                i = min_index + lookback\n                \n        samples = np.zeros((len(rows),\n                   lookback \/\/ step,\n                   data.shape[-1]))\n        targets = np.zeros((len(rows),))\n        for j, row in enumerate(rows):\n            indices = range(rows[j] - lookback, rows[j], step)\n            samples[j] = data[indices]\n            targets[j] = data[rows[j]][1]\n            \n        yield samples, targets","477aa4ef":"from keras.callbacks import Callback\n\nclass EarlyStoppingByLossVal(Callback):\n    def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n        super(Callback, self).__init__()\n        self.monitor = monitor\n        self.value = value\n        self.verbose = verbose\n\n    def on_epoch_end(self, epoch, logs={}):\n        current = logs.get(self.monitor)\n        if current is None:\n            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n\n        if current < self.value:\n            if self.verbose > 0:\n                print(\"Epoch %05d: early stopping THR\" % epoch)\n            self.model.stop_training = True\n","a5eb97bf":"from tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop, Adagrad\n\nlookback = 30 \nbatch_size = 1024\nsteps_per_epoch = 100\nepochs = 10\ndata_split = 0.8\nstep = 1\n\ndef generators(market_data_float):\n    l = market_data_float.shape[0]\n    train_gen = generator(market_data_float,\n                          min_index=0,\n                          max_index=int(data_split * l),\n                          batch_size=batch_size,\n                          lookback=lookback,\n                          step=step)\n    \n    val_gen = generator(market_data_float,\n                        min_index=int(data_split * l),\n                        max_index=None,\n                        batch_size=batch_size,\n                        lookback=lookback,\n                        step=step)\n    \n    return (train_gen, val_gen)\n\ndef learn_model(market_data_float):\n    (train_gen, val_gen) = generators(market_data_float)\n    input_shape = (None, market_data_float.shape[-1])\n    \n    model = Sequential()\n    model.add(layers.GRU(4, input_shape=input_shape))   \n    model.add(layers.Dense(1))\n    model.compile(optimizer=Adagrad(), loss='mae')\n    \n    callbacks = [EarlyStoppingByLossVal(monitor='val_loss', value=0.00001, verbose=1)]\n    history = model.fit_generator(train_gen,\n                                  steps_per_epoch=steps_per_epoch,\n                                  epochs=10,\n                                  validation_data=val_gen,\n                                  validation_steps=100,\n                                  callbacks=callbacks)\n    \n    return (model, history)\n\ndef learn_models(market_train_df, Histories):\n    for asset_code, market_data in tqdm(market_train_df.groupby('assetCode')):\n        # drop the non-numeric columns and handle the nans.\n        market_float_data = market_data.drop(['assetCode', 'assetName', 'time'], axis=1).fillna(0)\n        \n        # normalize the data\n        scaler = StandardScaler().fit(market_float_data)\n        \n        # learn a model using the normalized data\n        (model, history) = learn_model(scaler.transform(market_float_data))\n        \n        # save the history\n        Histories[asset_code] = history\n        yield asset_code, (scaler, model)","a7861919":"n = 5\nn_random_assets = np.random.choice(market_train_df.assetCode.unique(), n)\n\nmarket_train_sampled_df = market_train_df[market_train_df.assetCode.isin(n_random_assets)]","ceb6e295":"(fig, ax) = plt.subplots(figsize=(15, 8))\n\nmarket_train_sampled_df.groupby('assetCode').plot(x='time', y='close', ax=ax)\n\nax.legend(n_random_assets)\nplt.xlabel('time')\nplt.ylabel('close')\nplt.title('Closing Price of %i random assets' % n)\nplt.show()","446b0376":"Histories = {}\nModels = dict(learn_models(market_train_sampled_df, Histories))","c3ef3178":"for asset, history in Histories.items():\n    (fig, ax) = plt.subplots(figsize=(15, 8))\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(loss) + 1) \n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n\n    plt.title(asset)\n    plt.legend()\n    plt.show()","683c8363":"Each model is trained on a particular asset. The time series data for that particular asset needs to be normalized.","eaabba10":"# Learning the Models ","56d2f756":"# Random Sample the Assets","a6d894fe":"# Data Generator","ecc8c8e7":"# Unscaled loss plots"}}