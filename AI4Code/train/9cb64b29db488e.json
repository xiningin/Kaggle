{"cell_type":{"60216bf5":"code","81eda480":"code","5e3cce03":"code","88251c64":"code","aa6e9a54":"code","99d63b32":"code","2bc6c38b":"code","df64374d":"code","95670c57":"code","ce58fdcc":"code","d4acdc26":"code","1b4da4a9":"code","de8209a4":"code","6cfb02e7":"code","1dc87cd9":"code","879c1631":"code","24120fc2":"code","899fd1c7":"code","b8fea41d":"code","b635bcfb":"code","074944cc":"code","92196980":"code","82a937eb":"code","506985a2":"code","9c79ba5f":"code","2c082470":"code","b8c7b7c1":"code","eca3b5a7":"code","7307abdb":"code","5bcdbfe0":"code","519570c0":"code","95aa94f0":"code","179b3fbd":"code","0d69b872":"code","c78267cd":"code","b6b23bde":"code","bf190a47":"code","d5f9137f":"code","af77b00c":"code","347d0d82":"code","7980c754":"code","45dbd55d":"code","e56ca252":"code","f1cba229":"code","53b4b183":"code","d8422d9e":"code","2106b64b":"code","5f25853d":"code","223d8323":"code","efe84663":"code","328def53":"code","78979b7c":"code","4827dfdb":"code","91716e65":"code","08be0183":"code","4e8fe6d2":"code","bcff2f9c":"code","ae0531ae":"code","97174f3f":"code","29672513":"code","efb3c95d":"code","0cbd372f":"code","5e590e7b":"code","8ef252fe":"code","fbb49b27":"code","b66d5d8b":"code","f95c8d5f":"code","3b77504c":"code","823821a2":"code","1d5a6095":"code","97622d3d":"code","1bc9abb6":"markdown","9e357219":"markdown","cd479dba":"markdown","088b49b3":"markdown","57b1d146":"markdown","c797d247":"markdown","b6044f19":"markdown","02268be9":"markdown","0dcc0392":"markdown","5e3d2f04":"markdown","803b01a2":"markdown","7e1824fb":"markdown","93e81d40":"markdown","19a47fb8":"markdown","f20919b7":"markdown","0c953221":"markdown","66577c27":"markdown","37f319e0":"markdown","b8ffcd89":"markdown","6a33897c":"markdown","ef6cd7fb":"markdown","3c1ebad6":"markdown","a2516a8b":"markdown","b7302982":"markdown","6c7b7912":"markdown","1e12e062":"markdown","950cac0a":"markdown","65d99e2b":"markdown","1da14a34":"markdown","9ebcb3a9":"markdown","96217af3":"markdown","68828256":"markdown","4e7abce2":"markdown","811f31a8":"markdown","e8f00bce":"markdown","bc44c178":"markdown","95016f62":"markdown","b9c206ad":"markdown","83d43969":"markdown","4929123d":"markdown","c12fa9a5":"markdown","be7b5785":"markdown","635c15d3":"markdown","53c7ab36":"markdown","feac1678":"markdown","b64a945c":"markdown","e5c91ef9":"markdown","1ff10848":"markdown"},"source":{"60216bf5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","81eda480":"# data analysis and wrangling libraries\nimport pandas as pd\nimport numpy as np\n\n#data visualization libraries\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\n#Scipy libraries (Statistical Analysis on data)\nfrom scipy import stats\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import zscore\nimport math\n\n#Scikit Libraries (Machine Learning)\n\n#preprocessing \nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n#model selection\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n\n#Models (Though a number of algorithms can be tested, we have used only few of them)\n##Linear Models \nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\n##Neighbour Models\nfrom sklearn.neighbors import KNeighborsClassifier\n##Support Vector Machine Model\nfrom sklearn.svm import SVC, LinearSVC\n## Tree\nfrom sklearn.tree import DecisionTreeClassifier\n##Ensemble Models\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, AdaBoostClassifier\n##Neural Networ(sklearn)\nfrom sklearn.neural_network import MLPClassifier\n##XGBoost\nimport xgboost as xgb\n\n\n#Metrics to evaluate the models\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score, roc_curve, roc_auc_score, accuracy_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%timeit \n%matplotlib inline\nsns.set_style('darkgrid')\n\nplt.style.use(style='ggplot')\nplt.rcParams['figure.figsize'] = (10, 6)","5e3cce03":"%%HTML\n<style type=\"text\/css\">\ntable.dataframe td, table.dataframe th {\n    border: 1px  black solid !important;\n  color: black !important;\n}","88251c64":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')","aa6e9a54":"train_df.head()","99d63b32":"#creating a dummy dataset to explore without making any changes to original dataset\ndataset = train_df.copy()","2bc6c38b":"#Variable Distinction\ndisplay(dataset.info())\ndisplay(dataset.describe().transpose())","df64374d":"#Checking if our data is bias or not\nplt.figure(figsize= (4,4))\nsns.countplot(dataset['Survived'])","95670c57":"#selecting categorical categories\ncat_features = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n\n# Ploting the count as well as percetage frequency\nfig, axes = plt.subplots(3, 2, figsize = (12,14))\naxes = axes.flatten()\n    \nfor ax, cat in zip(axes, cat_features) :\n    total = float(len(dataset[cat]))      \n    sns.countplot(dataset[cat], palette = 'viridis', ax =ax)       \n    for p in ax.patches :\n        height = p.get_height()\n        ax.text(p.get_x() + p.get_width()\/2., height + 10,\n                '{:1.0f}%'.format((height\/total) * 100), ha = 'center',)     \n    plt.ylabel('Count')\n","ce58fdcc":"#selecting numerical features \nnum_features = ['Age', 'Fare']\n\n# Ploting the distribution plot\nfig, axes = plt.subplots(1, 2, figsize = (10,6))\naxes = axes.flatten()\n\nfor ax, num in zip(axes, num_features) :\n    sns.distplot(dataset[num], ax= ax, hist = True, kde = True)\n","d4acdc26":"corr = dataset.drop('PassengerId', axis =1).corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\ndisplay(sns.heatmap(data= corr, cmap= \"coolwarm_r\", robust= True, annot= True, square= True, mask= mask, fmt= \"0.2f\"))\ndisplay(train_df.corr()['Survived'].sort_values())","1b4da4a9":"display(sns.countplot(dataset['Pclass'], hue = dataset['Survived'], palette= \"viridis\"))\n\nax = plt.gca()\n# Iterate through the list of axes' patches\ntotal = len(dataset['Pclass'])\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width()\/2., p.get_height(), '{:1.0f}'.format(height), \n            fontsize=12, ha='center', va='bottom')","de8209a4":"dataset.corr()['Pclass'].sort_values()","6cfb02e7":"fig = plt.figure(constrained_layout = True, figsize = (10,5))\n#Creating a grid of 2 cols and 1 rows\ngrid = gridspec.GridSpec(ncols= 2, nrows=1, figure= fig)\n\n#Plot Pclass vs Fare\nax1 = fig.add_subplot(grid[0,0])\nax1.set_title('Pclass vs Fare')\nsns.barplot(x=\"Pclass\",y=\"Fare\",data=dataset, palette = \"viridis\", ax= ax1)\n\n#Plot Pclass vs Age\nax1 = fig.add_subplot(grid[0,1])\nax1.set_title('Pclass vs Age')\nsns.barplot(x=\"Pclass\",y=\"Age\",data=dataset, palette = \"viridis\", ax= ax1)","1dc87cd9":"fig = plt.figure(constrained_layout = True, figsize = (10,5))\n#Creating a grid of 2 cols and 1 rows\ngrid = gridspec.GridSpec(ncols= 2, nrows=1, figure= fig)\n\n#Distribution plot \nax1 = fig.add_subplot(grid[0,0])\nax1.set_title('Distribution')\nsns.distplot(dataset[\"Age\"], label=\"Skewness : %.1f\"%(dataset[\"Age\"].skew()), ax= ax1)\n\n#Box plot of Age with Survival \nax1 = fig.add_subplot(grid[0,1])\nax1.set_title('Pclass vs Age')\nsns.boxplot(x=\"Survived\", y=\"Age\",data=dataset, palette = \"viridis\", ax= ax1)","879c1631":"dataset[dataset['Survived'] == 1]['Age'].hist(alpha = 0.6, bins = 10, color = 'g')\ndataset[dataset['Survived'] == 0]['Age'].hist(alpha = 0.5, bins = 10, color = 'b')","24120fc2":"#let us try to split the age into categories\ndataset['Age_Category'] = pd.cut(dataset['Age'], bins= 8,labels = ['0-10', '10-20', '20-30', '30-40', '40-50', \n                                                                    '50-60','60-70', '80-90'])\nplt.figure(figsize= (12,6))\nsns.countplot(dataset['Age_Category'], hue= dataset['Survived'])","899fd1c7":"fig = plt.figure(constrained_layout = True, figsize = (10,5))\n#Creating a grid of 2 cols and 1 rows\ngrid = gridspec.GridSpec(ncols= 2, nrows=1, figure= fig)\n\n#Box plot of Sex with Survived\nax1 = fig.add_subplot(grid[0,0])\nax1.set_title('Sex vs Survived')\nsns.countplot(dataset['Sex'], hue = dataset['Survived'], palette= \"viridis\", ax= ax1)\n\n#Box plot of Age with Survival \nax1 = fig.add_subplot(grid[0,1])\nax1.set_title('Sex vs Pclass')\nsns.countplot(dataset['Sex'], hue = dataset['Pclass'], palette= \"viridis\", ax= ax1)\n","b8fea41d":"#selecting categorical categories\nfamily_features = ['SibSp', 'Parch']\n\n# Ploting the count as well as percetage frequency\nfig, axes = plt.subplots(1, 2, figsize = (14,6))\naxes = axes.flatten()\n    \nfor ax, cat in zip(axes, family_features) :\n    total = float(len(dataset[cat]))      \n    sns.countplot(dataset[cat], palette = 'viridis', ax =ax, hue= dataset['Survived'])\n    for p in ax.patches :\n        height = p.get_height()\n        ax.text(p.get_x() + p.get_width()\/2., height + 10,\n                '{:1.0f}'.format(height), ha = 'center',)     \n    plt.ylabel('Count')\n","b635bcfb":"dataset['Family'] = dataset.apply(lambda x : x.SibSp + x.Parch, axis = 1)\n\nsns.countplot(dataset['Family'], hue = dataset['Survived'], palette= \"viridis\")\nax = plt.gca()\n\n# Iterate through the list of axes' patches\ntotal = len(dataset['Pclass'])\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width()\/2., p.get_height(), '{:1.0f}'.format(height), \n            fontsize=12, ha='center', va='bottom')\n","074944cc":"fig = plt.figure(constrained_layout = True, figsize= (10,10))\ngrid = gridspec.GridSpec(ncols= 2, nrows= 2, figure= fig)\n\n#Count plot of Embarked with Survival \nax1 = fig.add_subplot(grid[0,0])\nax1.set_title('Embarked with Survival')\nsns.countplot(x=dataset[\"Embarked\"], hue= dataset[\"Survived\"], palette = \"viridis\", ax= ax1)\n\n#Box plot of Embarked with Age \nax1 = fig.add_subplot(grid[0,1])\nax1.set_title('Embarked with Age')\nsns.boxplot(x=\"Embarked\", y=\"Age\", data=dataset, palette = \"viridis\", ax= ax1)\n\n#Box plot of Embarked with Fare \nax1 = fig.add_subplot(grid[1,0])\nax1.set_title('Embarked with Fare')\nsns.boxplot(x=\"Embarked\", y=\"Fare\", data=dataset, palette = \"viridis\", ax= ax1)\n\n#Box plot of Embarked with Pclass \nax1 = fig.add_subplot(grid[1,1])\nax1.set_title('Embarked with Pclass')\nsns.countplot(dataset[\"Embarked\"], hue =dataset['Pclass'], palette = \"viridis\", ax= ax1)","92196980":"fig = plt.figure(constrained_layout = True, figsize= (12,6))\ngrid = gridspec.GridSpec(ncols= 2, nrows= 1, figure= fig)\n\n#Distribution of Fare\nax1 = fig.add_subplot(grid[0,0])\nax1.set_title('Distribution of Fare')\nsns.distplot(dataset[\"Fare\"], label=\"Skewness : %.1f\"%(dataset[\"Fare\"].skew()))\n\n#Box plot of Fare with Survival \nax1 = fig.add_subplot(grid[0,1])\nax1.set_title('Fare with Survival')\nsns.boxplot(x=\"Survived\", y=\"Fare\", data=dataset, palette = \"viridis\", ax= ax1)\n","82a937eb":"dataset[dataset['Pclass']== 1]['Fare'].hist(bins = 20)\ndataset[dataset['Pclass']== 2]['Fare'].hist(bins = 20)\ndataset[dataset['Pclass']== 3]['Fare'].hist(bins = 20)","506985a2":"dataset['Cabin'].fillna('None', inplace = True)\ndataset['Have Cabin'] = dataset['Cabin'].apply(lambda x : \"Yes\" if x != 'None' else \"No\")","9c79ba5f":"sns.countplot(dataset['Have Cabin'], hue = dataset['Survived'])","2c082470":"merged_df = pd.concat([train_df, test_df], sort= False).reset_index(drop= True)\nmerged_df.head(3)","b8c7b7c1":"merged_df.info()\ndisplay(sns.heatmap(merged_df.isna(), cmap= 'binary_r', yticklabels = False))","eca3b5a7":"# get mean, standard deviation and number of null values\nmean = train_df[\"Age\"].mean()\nstd = train_df[\"Age\"].std()\n\ndef impute_age(age) :   \n    if age != age :\n        return np.random.randint(mean - std, mean + std)\n    else :\n        return age\n    \nmerged_df['Age'] = merged_df['Age'].apply(lambda x : impute_age(x))","7307abdb":"#identify the missing Fare value passenger\ndisplay(merged_df[merged_df['Fare'].isna()])\n\n#identify the missing Embarked value passenger\ndisplay(merged_df[merged_df['Embarked'].isna()])\n\n#identify aggregate value of Fare under depending section\ndisplay(merged_df.groupby(['Pclass', 'Embarked']).mean()['Fare'])\n\n#fill the relevant value of the Fare for the Passenger\nmerged_df.loc[1043, 'Fare'] = 14\n\n#fill the relevant value of the Embarked for the Passenger\nmerged_df.loc[61, 'Embarked'] = 'S'\nmerged_df.loc[829, 'Embarked'] = 'S'","5bcdbfe0":"#drop the extra columns\nmerged_df.drop('PassengerId', axis= 1, inplace= True)","519570c0":"merged_df['Name'].head(5)","95aa94f0":"merged_df['Title'] = merged_df['Name'].apply(lambda x : x.split(',')[1].split(' ')[1].strip())\ndisplay(merged_df['Title'].unique())\ndisplay(sns.countplot(merged_df['Title']))","179b3fbd":"merged_df[\"Title\"] = merged_df[\"Title\"].replace(['Don.', 'Rev.', 'Dr.', 'Mme.','Ms.', 'Major.', 'Lady.', \n                                                 'Sir.', 'Mlle.', 'Col.', 'Capt.', 'the','Jonkheer.', 'Dona.'], 'Other')\ndisplay(sns.countplot(merged_df['Title']))","0d69b872":"sns.factorplot(x=\"Title\",y=\"Survived\",data=merged_df,kind=\"bar\", order = ['Mr.', 'Other', 'Master.', 'Miss.', 'Mrs.'])","c78267cd":"#convert them into ordinal values\nmerged_df['Title'] = merged_df['Title'].map({'Mr.' : 0, 'Other' : 1,'Master.' : 2,'Miss.' : 3,'Mrs.' : 4})","b6b23bde":"#name length\nmerged_df['Name Length'] = merged_df['Name'].apply(lambda x : len(x))\ndisplay(sns.factorplot(x ='Survived', y= 'Name Length',kind = 'bar', data = merged_df))","bf190a47":"#drop name variable \nmerged_df.drop('Name', axis= 1, inplace= True)","d5f9137f":"#creating family feature\nmerged_df['Family'] = merged_df.apply(lambda x : x.SibSp + x.Parch, axis=1)\n\n#creating into categorical feature\nmerged_df['Family Size'] = merged_df['Family'].apply(lambda x : \"Alone\" if x == 0 else \"Small\" if x < 4 else \"Large\")\n\ndisplay(sns.factorplot(y ='Survived', x= 'Family Size',kind = 'bar', data = merged_df))","af77b00c":"#converting family size to categorical features\nfam_features = pd.get_dummies(merged_df['Family Size'], drop_first= True, prefix= \"Fam\")\nmerged_df = pd.concat([merged_df, fam_features], axis= 1)\n\n#drop Parch, Sibsp & Family feature\nmerged_df.drop(['Parch', 'SibSp', 'Family', 'Family Size'], axis= 1, inplace = True)","347d0d82":"#imputing NA values with 0\nmerged_df['Cabin'].fillna(0, inplace = True)\n\n#changing other values to 1\nmerged_df['Cabin'] = merged_df['Cabin'].apply(lambda x : 0 if x ==0 else 1)\n\ndisplay(sns.countplot(merged_df['Cabin'], hue = merged_df['Survived']))","7980c754":"merged_df['Ticket'].head()","45dbd55d":"def ticket_prefix(ticket) :\n    #if ticket is alphabetic(not numeric)\n    if not ticket.isdigit() :\n        return ticket.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]\n    else :\n        return 'No'\n\nmerged_df['Ticket prefix int'] = merged_df['Ticket'].apply(lambda x : x.split()[0])\nmerged_df['Ticket prefix'] = merged_df['Ticket prefix int'].apply(lambda x : ticket_prefix(x))","e56ca252":"display(merged_df['Ticket prefix'].nunique())\ndisplay(merged_df['Ticket prefix'].unique())","f1cba229":"#converting to categorical feature\nticket_feature = pd.get_dummies(merged_df['Ticket prefix'], drop_first= True, prefix= \"Tic\")\n\n#merging the tables\nmerged_df = pd.concat([merged_df, ticket_feature], axis=1)\n\n#drop Ticket columns\nmerged_df.drop(['Ticket', 'Ticket prefix int', 'Ticket prefix'], axis =1 ,inplace= True)","53b4b183":"# Create categorical values for Pclass\nmerged_df[\"Pclass\"] = merged_df[\"Pclass\"].astype(\"category\")\npclass_feature = pd.get_dummies(merged_df[\"Pclass\"],prefix=\"Pc\", drop_first= True)\nmerged_df = pd.concat([merged_df, pclass_feature], axis= 1)\n\n# Create categorical values for Sex\nsex_feature = pd.get_dummies(merged_df[\"Sex\"],prefix=\"Sex\", drop_first= True)\nmerged_df = pd.concat([merged_df, sex_feature], axis= 1)\n\n# Create categorical values for Embarked\nembarked_feature = pd.get_dummies(merged_df[\"Embarked\"],prefix=\"Em\", drop_first= True)\nmerged_df = pd.concat([merged_df, embarked_feature], axis= 1)\n\n#drop the duplicate columns\nmerged_df.drop(['Pclass', 'Sex', 'Embarked'],axis= 1, inplace= True)","d8422d9e":"merged_df['Age_Category'] = pd.cut(merged_df['Age'], bins= 8,labels = ['0-10', '10-20', '20-30', '30-40', '40-50', \n                                                                    '50-60','60-70', '80-90'])\n\nfeatures = pd.get_dummies(merged_df['Age_Category'], drop_first= True, prefix= 'Age')\n\nmerged_df = pd.concat([merged_df, features], axis=1)\n\nmerged_df.drop(['Age', 'Age_Category'], axis=1, inplace= True)","2106b64b":"#spliting back the data into training and test data\ntraining_data = merged_df.iloc[:891,:]  \ntest_data = merged_df.iloc[891 :,:] \ntest_data.drop('Survived', axis =1, inplace =True)\n\nX_train = training_data.drop('Survived', axis = 1).values\ny_train = training_data['Survived'].values\n\nX_test = test_data.values\n\ndisplay(X_train.shape)\ndisplay(y_train.shape)\ndisplay(X_test.shape)","5f25853d":"#create a scaling object\nscaler = MinMaxScaler()\n\n#fit the object only on training data and not test data\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","223d8323":"#write a function to evaluate your model\n\ndef evaluate_accuracy(model) :\n    \n    print(str(model))\n    #On Training Data\n    model.fit(X_train, y_train)\n    predict_train = model.predict(X_train)\n    training_accuracy = accuracy_score(y_train, predict_train)\n    print(\"Training Data\")\n    print(confusion_matrix(y_train, predict_train))\n    print(f'Accuracy Score: {training_accuracy}')\n    print('-'*50)\n    print(\"Validation Data\")\n    predict_score = cross_val_score(model, X_train, y_train, cv = 10, scoring = 'accuracy')\n    validation_accuracy = predict_score.mean()\n    print(f'Accuracy Score: {validation_accuracy}')\n    print('')\n    return training_accuracy, validation_accuracy","efe84663":"models = []\nk = 42\n#Linear Class\nlogistic_clf = LogisticRegression(random_state= k)\nridge_clf = RidgeClassifier(random_state= k)\nmodels.append(logistic_clf)\nmodels.append(ridge_clf)\n\n#Neighbor Class\nknn_clf = KNeighborsClassifier()\nmodels.append(knn_clf)\n\n#SVC Class\nsvc_clf = SVC(random_state= k)\nlinearsvc_clf = LinearSVC(random_state= k)\nmodels.append(svc_clf)\nmodels.append(linearsvc_clf)\n\n#Tree Class\ntree_clf = DecisionTreeClassifier(random_state= k)\nmodels.append(tree_clf)\n\n#Ensemble\nrandomforest_clf = RandomForestClassifier(random_state= k)\nbagging_clf = BaggingClassifier(random_state= k)\ngradboosting_clf = GradientBoostingClassifier(random_state= k)\nadaboosting_clf = AdaBoostClassifier(DecisionTreeClassifier(random_state= k),random_state= k)\nmodels.append(randomforest_clf)\nmodels.append(bagging_clf)\nmodels.append(gradboosting_clf)\nmodels.append(adaboosting_clf)\n\n#Neural Network\nmlp_clf = MLPClassifier(random_state = k)\nmodels.append(mlp_clf)\n\n#Xgboost\nxgboost = xgb.XGBClassifier(random_state = k)\nmodels.append(xgboost)","328def53":"training_accuracy = []\nvalidation_accuracy = []\n\nfor model in models :\n    train_acc, val_acc = evaluate_accuracy(model)\n    training_accuracy.append(train_acc)\n    validation_accuracy.append(val_acc)","78979b7c":"result = pd.DataFrame({'Algorithm' : ['Logistic', 'Ridge', 'KNN', 'SVC', 'Lin SVC', 'Tree', \n                                     'Rnd Forest', 'Bagging', 'Grad Boost', 'AdaBoost', 'MLP', 'XGBoost'], \n                       'Training Accuracy': training_accuracy, 'Validation Accuracy' : validation_accuracy})\n\ndisplay(result)","4827dfdb":"sns.barplot(x = 'Algorithm',  y = 'Validation Accuracy', data = result)\nax = plt.gca()\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width()\/2., p.get_height(), '{:1.3f}'.format(height), \n            fontsize=12, ha='center', va='bottom')","91716e65":"sns.scatterplot(x = 'Training Accuracy', y='Validation Accuracy', data = result)","08be0183":"#tuning the model\n\nsvc_classifier = SVC()\nparam_grid = {'C': [0.1, 1, 3], 'kernel': ['rbf', 'linear'], 'gamma' : ['scale', 0.1, 0.01], \n              'degree' : [1 ,3,5], 'break_ties': ['True', 'False']}\n\nsvc_grid = GridSearchCV(svc_classifier, param_grid, scoring= \"accuracy\", cv= 10)\nsvc_grid.fit(X_train, y_train)\nprint(f'Best Score: {svc_grid.best_score_:0.5f}')\nprint(f'Best Parameter: {svc_grid.best_params_}')","4e8fe6d2":"optimized_svc = SVC(C=1.0, break_ties=True, cache_size=200, class_weight=None, coef0=0.0,\n                    decision_function_shape='ovr', degree=1, gamma='scale', kernel='rbf',\n                    max_iter=-1, probability=True, random_state=42, shrinking=True, tol=0.001,verbose=False)\n\nscores = cross_val_score(optimized_svc, X_train, y_train, cv = 10)\nprint(f'Tuned SVC Score: {scores.mean()}')","bcff2f9c":"# Gradient boosting tunning\n\ngb_clf= GradientBoostingClassifier(random_state= 42)\n\n#param_grid = {'loss' : [\"deviance\"], 'n_estimators' : [100,300, 500, 750], 'learning_rate': [0.3 ,0.1],\n#              'max_depth': [8,10,12], 'min_samples_leaf': [50,75,100],'max_features': [0.3, 0.1]  }\n\n''' Best Parameters from first grid are : {'learning_rate': 0.3, 'loss': 'deviance', 'max_depth': 10,\n 'max_features': 0.3, 'min_samples_leaf': 100, 'n_estimators': 500}\n'''\n\nparam_grid_2 = {'loss' : [\"deviance\"], 'n_estimators' : [500, 600], 'learning_rate': [0.3, 0.5, 0.7],\n               'max_depth': [9,10,11], 'min_samples_leaf': [100, 150],'max_features': [0.3, 0.5, 0.7]}\n\nparam_grid_3 = {'loss' : [\"deviance\"], 'n_estimators' : [600], 'learning_rate': [0.3, 0.4],\n               'max_depth': [9], 'min_samples_leaf': [100, 120],'max_features': [0.3, 0.4]}\n\ngb_grid = GridSearchCV(gb_clf, param_grid_3, cv=10, scoring=\"accuracy\", n_jobs=-1)\ngb_grid.fit(X_train,y_train)\nprint(f'Best Score: {gb_grid.best_score_:0.5f}')\nprint(f'Best Parameter: {gb_grid.best_params_}')","ae0531ae":"optimized_gb = GradientBoostingClassifier(learning_rate =  0.3, max_depth = 9, max_features = 0.4, min_samples_leaf =  100, \n                                          n_estimators = 700, random_state= 42)\n\nscores = cross_val_score(optimized_gb, X_train, y_train, cv = 10)\nprint(f'Tuned Gradient Boosting Score: {scores.mean()}')","97174f3f":"# RFC Parameters tunning \nrf_clf = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nparam_grid = {\"max_depth\": [None],\"max_features\": [1,10, 15, 18], \"min_samples_split\": [5, 10, 12],\n              \"min_samples_leaf\": [1, 3],\"bootstrap\": [True, False],\"n_estimators\" :[300, 750],\n              \"criterion\": [\"gini\"]}\n\n\nrf_grid = GridSearchCV(rf_clf,param_grid = param_grid, cv=10, scoring=\"accuracy\")\n\nrf_grid.fit(X_train,y_train)\nrf_grid.best_score_","29672513":"optimized_rf = RandomForestClassifier(bootstrap = False, criterion = 'gini', max_depth = None, max_features = 15, \n                                      min_samples_leaf = 1, min_samples_split = 10, n_estimators = 750, random_state=42)\n\nscores = cross_val_score(optimized_rf, X_train, y_train, cv = 10)\nprint(f'Tuned Random Forest Score: {scores.mean()}')","efb3c95d":"#creating training and validation dataset\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train)","0cbd372f":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import EarlyStopping","5e590e7b":"X_train.shape","8ef252fe":"model = Sequential()\n\n# Adding the input layer and first hidden layer\nmodel.add(Dense(input_dim=54, units=32, activation='relu'))\nmodel.add(Dropout(rate=0.75)) \n\n# Adding the second hidden layer\nmodel.add(Dense(units=16, activation='relu'))\nmodel.add(Dropout(rate=0.5))\n\n# Adding the output layer\nmodel.add(Dense(units=1, activation='sigmoid'))\n\n#Early Stopping\nearly_stop  = EarlyStopping(monitor= 'val_loss', mode = 'min', patience= 25, verbose=1)\n\n# Compiling the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Training the model\nmodel.fit(X_train, y_train, batch_size=10, epochs=500, validation_data = (X_val,y_val), verbose = 3,\n          callbacks = [early_stop])","fbb49b27":"history = pd.DataFrame(model.history.history)\n\nfig, axes = plt.subplots(2,1, figsize = (10,8))\n\naxes[0].plot(history['loss'], 'r', label = 'Training Loss')\naxes[0].plot(history['val_loss'], 'b', label = 'Validation Loss')\nlegend  = axes[0].legend()\n\naxes[1].plot(history['accuracy'], 'r', label = 'Training Accuracy')\naxes[1].plot(history['val_accuracy'], 'b', label = 'Validation Accuracy')\nlegend  = axes[1].legend()","b66d5d8b":"#fit the model on complete data\nrandom_forest = RandomForestClassifier(bootstrap = False, criterion = 'gini', max_depth = None, max_features = 15, \n                                      min_samples_leaf = 1, min_samples_split = 10, n_estimators = 750, random_state=42)\n\nrandom_forest.fit(X_train, y_train)\n\n#predict for the test set\npredictions = random_forest.predict(X_test)","f95c8d5f":"submission = pd.DataFrame(columns= ['PassengerId', 'Survived'])\nsubmission['PassengerId'] = test_df['PassengerId']\nsubmission['Survived'] = predictions\nsubmission['Survived'] = submission['Survived'].astype('int64')","3b77504c":"submission.to_csv(\"Optimized Random Forest.csv\",index=False)","823821a2":"predictions = model.predict_classes(X_test)","1d5a6095":"submission = pd.DataFrame(columns= ['PassengerId', 'Survived'])\nsubmission['PassengerId'] = test_df['PassengerId']\nsubmission['Survived'] = predictions\nsubmission['Survived'] = submission['Survived'].astype('int64')","97622d3d":"submission.to_csv(\"Neural Network.csv\",index=False)","1bc9abb6":"### 4.1.1 Imputing Age\n\nWe can either use other variables to impute the age or use an average value. Here, we are more inclined to use a average value because age cannot be derived from other features which are present here. We will impute the value with a random value within 1 standard deviation of mean","9e357219":"### Fare\n\nFare has already been observed in Pclass as a contributing factor. We can make the following observation from the visualizations:\n1. Fare has heavily skewed with outliers. \n2. Survived passengers had higher fare but have many outliers\n","cd479dba":"## 4.2 Feature Engineering\n\nFeature Engineering involves adding new features, discretize continuous features, decompose features & add promising transformation of features (e.g, log(x),sin(x) etc)","088b49b3":"# 3. Discover and visualize the data to gain insights","57b1d146":"For \"Name\" & \"Ticket\", we have to incorporate feature engineering.","c797d247":"### 4.2.3 Cabin\n\nAs already shown in visualization, we will create a subsitiute value for having cabin or not with 1 and 0","b6044f19":"# 1. Look at the Big Picture","02268be9":"Let us look at the output first. Since this is a classification problem, we might want to look whether our data is biased or not!\n\nBelow plot shows that more people have not survived. It is case where the output is biased towards value zero. This is helpful because we might not want the threshold probablity to be 50% as there is higher chance of not surviving. This is a simply a speculation and moving forward, we will see we can use it or not.","0dcc0392":"We have calculated Training Accuracy and Validation Accuracy. If the difference between Training Error and Validation Error is high, it implies the model is overfitting the data and could perform worse in case real test set. We have obtained the best accuracy(Validation) in \"SVC\" & \"Gradient Boosting\". Both look promising as they have lower training error too which could be improved with tuning.","5e3d2f04":"## Neural Network","803b01a2":"### Age\n\nInitutively thinking, Age should be most determining factor but we have to keep in mind that it is also interacting a lot with other features. Older people might have more wealth and thus can travel in Pclass 1 with higher survival probability. This does not mean that Age is reason behind survival. Keeping this in mind, let us summarize our findings:\n1. Age has a normal distribution with slight skew on right side\n2. Box plot indicates that the distribution of age for survived or not is similar\n3. Spliting the age into category provide a better distinction between survived or not. We can look forward into this prospective","7e1824fb":"### 4.2.4 Ticket\n","93e81d40":"We have null values in Age, Fare & Embarked(currently we will ignore the Cabin value as we deal with later on)","19a47fb8":"# 2. Get the data","f20919b7":"### 4.2.6 Age\n\nAs observed during the visualization, converting the age into a categorical variable provides a clear distinction between survival probabilities","0c953221":"# 4. Prepare the data for Machine Learning Algorithm","66577c27":"Welcome to the Titanic Sink Machine Learning Project. Your task is to build a model which will take demographic features of a passenger and predict whether he\/she had survived or not. To state simply, few \"data points\" related to a passenger will go into your model and it will ouptut 1 or 0. But this is not the big picture! In real life, you would not even have the data, forget about such clean data. To understand the big picture, follow this :\n\n1. *Understand the real world situation* : The Titanic Sink was an unfortunate event, it can be called a disaster. But people have survived because the ship did not sink immediately. Those who were able to get out of ship earlier on life boats have survived and this was manual call. Maybe the women & children were given the chance to save their life first or it might depend on the social status of the passenger. Thus, it is a characterstic of a passenger that majorly defines whether he\/she might survive or not. This view will help you to understand how your model should behave\n\n2. *Think about the features* : Does wearing a white shirt will help the passenger to survive? Does age matters? Are there features which heavily depend on one another? What features could be added or removed to enhance the quality of model? Adding a lot of features that does not guarantee better performance. Always consider what impact the feature can have on real world situation.\n\n3. *Performance Measure* : Since you are working on a Kaggle Competition and your goal is to improve your model's rank, the performace measure must be in coherance with Kaggle Evaluation method. It can be RMSE, log square error, accuracy, precision, recall and any other. The best model to improve f1 score will not give you best acuracy. For this competition, the performace measure is **Accuracy**. We will train our model to improve the accuracy.\n\n4. How should you frame the problem? : Framing the problem helps you decide which algorithms or performace measure to use. Here we have a labeled data and thus it comes under **Supervised Learning**. We have to predict whether the passenger survived or not and thus the problem is a **Classification Problem**.\n\nApart from this, there are many questions you can ask to understand the problem in a better way. Feel free to add points which you think might help to imporve your understanding.","37f319e0":"## 6.3 Random Forest","b8ffcd89":"### 4.2.2 Family\n\nWe have already seen that by combining the \"Parch\" & \"SibSp\" we can generate \"Family\" feature which is more useful. Also Family can be converted into a categorical feature to sustain the meaning.","6a33897c":"We will extract Title from the name. As observed, there are only few dominant title values and others are very less frequent. We will club all the other values in one category named \"Other\".","ef6cd7fb":"## 6.1 SVC\n\nSupport Vector Machine uses kernel design to transform the data and then based on these transformations it finds an optimal boundary between the possible outputs. Thus the choice of \"kernal\" and C (measure of how much error it can handle to fit the data) determines the behaviour of data.","3c1ebad6":"### 4.3 Scaling\n\nOnly fare is a variable which is on a continous varaible and thus will require scaling","a2516a8b":"# 6. Fine-tune your model","b7302982":"### 4.2.5 Converting Pclass, Sex, Embarked into Categorical Variable","6c7b7912":"### Cabin\n\nCabin feature is heavily spare and thus on Cabin value, we might not be able to get any concusion. We will consider that Passenger with Null value in cabin does not have any cabin and other have. Below we can see that, passenger with cabin have higher survival chance.","1e12e062":"### 4.2.1 Name\n\nObservation on the few names listed below tell us that it contains \"Title\" & as well as designation of the person. People with longer name(having some important designation) might have higher chance of survival","950cac0a":"# 5.Select a model and train it","65d99e2b":"# **Table of Contents :**\n\n1. Look at the big picture\n2. Get the data\n3. Discover and visualize the data to gain insights\n4. Prepare the data for Machine Learning algorithms\n5. Select a model and train it\n6. Fine-tune your model\n7. Present your solution","1da14a34":"Selecting a model is an iterative process. There is no free lunch in this process. First, we will cross validate our data on different algorithms and look where it is performing the best. Once identified, we can keep fine tuning our model.","9ebcb3a9":"## 4.1 Impute Null Values","96217af3":"Preparing the data involves imputing the null values, feature engineering & encoding categorical features. For this, we will combine our test data and train data to look at the complete data together. Please note that this is not a good thing to do in real life ML projects. You would never want your test data to be mixed with training data. This lead to \"spilling effect\".","68828256":"\n\nThe best part begins! You are to required to analyze the data and visualize it. These can be done in multiple ways so feel free to use any of the below methods.","4e7abce2":"### Correlation Visualization \n\nPandas \"corr\" method calculates Pearson Correlation. We will look at the correlation between the label and variables and also in-between variables. \n\nFrom the below plot, we can draw the following conclusions ;\n1. Pclass & Fare have highest correlation with the survival \n2. Age as a numerical feature does not have a strong linear correlation. Note this does not mean they are not related. It might happen that they have a different correlation which is non-linear\n3. Passenger Class and Fare have high correlation which is obvious. We have to determine that whether both of these features are required or not.\n4. Passenger Class has also correlation with Age\n5. SibSp & Parch have correaltion which means people having siblings or spouses also have parent and children with them","811f31a8":"## 6.2 Gradient Boosting","e8f00bce":"### 4.1.2 Fare & Embarked values\n\nSince there is only one instance of Fare null value and two instance of Embarked value, we can groupby the data and look at the relevant value for the fare. Ideally we should write a function as we have done in imputing the Age but for simplicity sake, we will simply input the value. On the basis of the pivot table, we can correlate the Fare and Embarked Value and fill the missing value","bc44c178":"## 6.4 Neural Network ","95016f62":"As Kaggle provide readily the data, we are not required to put efforts on this point. But keep this in your mind that today data has lot of restriction and you might want to take permission of the respective facilty before using their data on any kind of analysis. Here, we might not have to look for any data permission or obligations but always keep this in your mind before using data from any external source.","b9c206ad":"### Family\n\nWe have two features which describe the family status of the passenger. It is important to understand that whether both the features are contributing something to the survival status or they can clubbed into one feature. Also does the size of the family is a factor or not. From the below visualization, we can form the required conclusion :\n\n1. Parch & SibSp exhibits a similar behvaiour and thus can be clubbed together into a new feature as \"Family\"\n2. Family provide a interesting behaviour. Passengers with no family have huge disadvantage. Passengers having family size from 1-3 have high survival chance whereas having more than 3 leads to low survival chance. We can use this property to engineer a new feature","83d43969":"### 3.2 Exploratory Data Analysis ","4929123d":"### Frequency and Distribution Plots\nWe have already divided the features into numeric and categorical. Before moving to individual variable, let us take a quick look on overall picture.\n\nFew conclusions from the below plots can be:\n**Categorical Features**\n1. Pclass : More than 50% of passengers below to Class 3. As defined in the dataset, the social status might influnece the survival rate and this could be a determining factor for survival rate.\n2. Sex : There are more men than women\n3. SibSp & Parch : A large porton of people are travelling alone\n4. Embarked : Most people have boarded from \"S\". We might want to look its dependency on other features\n\n**Numerical Feature**\n1. Age : It has almost a normal distribution. We would want to look at the survival rate of people belonging to categories at extreme.\n2. Fare : It has a skewed distribution with outliers at right side. ","c12fa9a5":"### Pclass\n\nWe have already observed that there is correlation between Pclass and Survived. Below are few conclusions that we can get from the visualizations :\n\n1. For Passenger belonging to Class 3, there is only 33% probability that they will survive\n2. Highest survival probability is of Class 1 passengers and then is for Class 2\n3. There is correlation of Pclass with Age and Fare. We want to keep in mind that this correaltion might decrease the feature importance of Age or Fare (or even Pclass)","be7b5785":"### Sex\nIt can concluded that women have a higher chance of survival than men. ","635c15d3":"> **In this notebook, we attempt to predict the survival chance of people on basis of their demographic features. Since it is likely that this is your first project, the notebook is desgined to highlight points where a beginner should focus to maximize their learning. For this, I have taken points from other notebooks as well as approach suggested in the book \"Hands on Machine Learning with Scikit-learn and Tensorflow\"\n**\n\n\n![titanic%20image.jpg](attachment:titanic%20image.jpg)","53c7ab36":"### Individual Feature Visualization\nNow, we are ready to go for a column wise inspection of features. Here we have less features so it is possible to check for all the features. If there are many features, select the few which you suspect might give you strong conclusions","feac1678":"# 7. Present your solution\n","b64a945c":"## **Take a quick look at the data**\n\nEach row represent a passenger. There are 12 columns with \"PassengerId\" as indentifier and \"Survived\" as label. Thus, we have 10 input columns with \"Age\" & \"Fare\" as Numerical Feature and \"Pclass\", \"Name\", \"Sex\", \"SibSp\", \"Parch\", \"Ticket\", \"Cabin\" & \"Embarked\" as Descriptive Feature(Note we still haven't categorized them as Categorical Feature)","e5c91ef9":"### Embarked\n\nThis defines the port of Embarkation of the passenger. A correlation between the survival status of a passenger and the port will depend only if a certain class of passengers embark from a particular port. From the below visualization, it is diffcult to understand a direct correlation between these two.","1ff10848":"### 3.1 Understand the data \n\nIt is important to distinguish between Numerical and Categorical Features because it will allow better visualization of them. Here, we have distinguished between them :\n\n**Numerical Features ** - Age & Fare\n\n*Categorical Features* - Pclass(It is actually an ordinal feature), Sex, SibSp, Parch & Embarked\n\n- Passenger ID is simply an identifier and cannot be regarded as a feature \n- Name & Ticket might help us to extract feature from them\n- Cabin is spare and we might have to drop the entire column "}}