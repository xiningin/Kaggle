{"cell_type":{"5330aee8":"code","ca2427bc":"code","170824a0":"code","430420c9":"code","8dd972b2":"code","f4f75c3b":"code","eae88226":"markdown","abbc22db":"markdown","5f4ef999":"markdown","4de9a037":"markdown","95402bf6":"markdown","012cba50":"markdown"},"source":{"5330aee8":"import numpy as np\nimport pandas as pd\nimport time\nimport csv\nimport os\nimport seaborn as sns\nimport random\nimport gc\n\nfrom sklearn import preprocessing\nfrom matplotlib import pyplot as plt\nimport matplotlib as mpl\nimport scipy.stats as st\nfrom sklearn import ensemble, tree, linear_model\nimport missingno as msno\nimport math\nimport copy\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom matplotlib import pyplot\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import train_test_split \nimport gc\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import datasets, linear_model\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","ca2427bc":"def null_table(df):\n    print(\"Training Data\\n\\n\")\n    print(pd.isnull(df).sum()) \n\ndef some_useful_data_insight(df):\n    Num_of_line = 100\n    print(Num_of_line*'=')\n    print(Num_of_line*'=')\n    print(df.head(5))\n    print(Num_of_line*'=')\n    print(df.dtypes)\n    print(Num_of_line*'=')\n    print(null_table(df))\n    print(Num_of_line*'=')\n    print('data length=', len(df))\n    print(Num_of_line*'=')\n    \ndef Plot_Hist_column(df, x):\n    pyplot.hist(df[x], log = True)\n    pyplot.title(x)\n    pyplot.show()\n    \ndef Plot_Hist_columns(df, xlist):\n    [Plot_Hist_column(df, x) for x in xlist]  \n    pyplot.show()\n    \ndef Make_X_Y(df):\n    Num_of_line = 100\n    Y = pd.DataFrame()\n    Y['is_attributed'] = df['is_attributed']\n    X = df.copy()\n    X.drop(labels = [\"is_attributed\"], axis = 1, inplace = True)\n    print(Num_of_line*'=')\n    print('X=', X.head(5))\n    print(Num_of_line*'=')\n    print('Y=', Y.head(5))\n    return X, Y\n\ndef Train_Test_training_valid(X, Y, ratio):\n    Num_of_line = 100\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=ratio)\n    print ('Train X shape = ', X_train.shape, '-----Train Y shape = ', y_train.shape)\n    print(Num_of_line*'=')\n    print ('Test X shape = ', X_test.shape, '-----Test Y shape = ',y_test.shape)\n    print(Num_of_line*'=')\n    X_training, X_valid, y_training, y_valid = \\\n    train_test_split(X_train, y_train, test_size=ratio, random_state=0)\n    print ('Training X shape = ', X_training.shape, '-----Training Y shape = ', y_training.shape)\n    print(Num_of_line*'=')\n    print ('Valid X shape = ', X_valid.shape, '-----Valid Y shape = ',y_valid.shape)\n    \n    return X_training, y_training, X_valid, y_valid\n\ndef Drop_cols(df, x):\n    Num_of_line = 100\n    print(Num_of_line*'=')\n    print('Before drop =\\n', df.head(3))\n    print(Num_of_line*'=')\n    df.drop(labels = x, axis = 1, inplace = True)\n    print('After drop =\\n', df.head(3))\n    return df\n\ndef Normalized(df):\n\n    df_col_names = df.columns\n    x = df.values \n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    df = pd.DataFrame(x_scaled)\n    df.columns = df_col_names\n    \n    \n    return df\n\n\ndef Parse_time(df):\n    df['day'] = df['click_time'].dt.day.astype('uint8')\n    df['hour'] = df['click_time'].dt.hour.astype('uint8')\n    df['minute'] = df['click_time'].dt.minute.astype('uint8')\n    df['second'] = df['click_time'].dt.second.astype('uint8')\n    \ndef Merge_data(df1, df2):\n    frames = [df1, df2]\n    df = pd.concat(frames)\n    return df\n\n\n\ndef read_csv_random(address_train, address_test, p):\n    \n    \n    #p = 0.01  # 1% of the lines\n    # keep the header, then take only 1% of lines\n    # if random from [0,1] interval is greater than 0.01 the row will be skipped\n    df_train = pd.read_csv(\n         address_train, parse_dates=['click_time'],\n         header=0, \n         skiprows=lambda i: i>0 and random.random() > p)\n    \n    df_test = pd.read_csv(address_test, parse_dates=['click_time'])\n\n    return df_train, df_test\n\n\ndef read_train_test_data(address_train, train_nrows, address_test, test_nrows, Skip_range_low, Skip_range_Up, nrows):\n    \n    df_train = pd.read_csv(address_train, parse_dates=['click_time'], skiprows=range(Skip_range_low,Skip_range_Up), nrows = nrows)\n    df_test = pd.read_csv(address_test, parse_dates=['click_time'])#, nrows = 100)#, nrows = test_nrows)\n    return df_train, df_test\n\n\ndef read_train_test_data_balanced(address_train, address_test):\n    \n    #Read Training data, all class 1 and add same amount 0\n    iter_csv = pd.read_csv(address_train, iterator=True, chunksize=10000000, parse_dates=['click_time'])\n    df_train_1 = pd.concat([chunk[chunk['is_attributed'] > 0] for chunk in iter_csv])\n    iter_csv = pd.read_csv(address_train, iterator=True, chunksize=10000000, parse_dates=['click_time'], nrows=2000000)\n    df_train_0 = pd.concat([chunk[chunk['is_attributed'] == 0] for chunk in iter_csv])\n    #seperate same number values as train data with class 1\n    df_train_0 = df_train_0.head(len(df_train_1))\n    #Merge 0 and 1 data\n    df_train = Merge_data(df_train_1, df_train_0)\n    \n    #Read Test data\n    df_test = pd.read_csv(address_test, parse_dates=['click_time'])\n    return df_train, df_test\n\ndef read_train_test_data_balanced_oversample1(address_train, address_test):\n    \n    #Read Training data, all class 1 and add same amount 0\n    iter_csv = pd.read_csv(address_train, iterator=True, chunksize=10000000, parse_dates=['click_time'])\n    df_train_1 = pd.concat([chunk[chunk['is_attributed'] > 0] for chunk in iter_csv])\n    iter_csv = pd.read_csv(address_train, iterator=True, chunksize=10000000, parse_dates=['click_time'], nrows=5000000)\n    df_train_0 = pd.concat([chunk[chunk['is_attributed'] == 0] for chunk in iter_csv])\n    \n    count_class_0 = len(df_train_0)\n    \n    df_train_1_over = df_train_1.sample(count_class_0, replace=True)\n    df_train_over = pd.concat([df_train_1_over, df_train_0], axis=0)\n    print('Random over-sampling:')\n    print(df_train_over.is_attributed.value_counts())\n    #Read Test data\n    df_test = pd.read_csv(address_test, parse_dates=['click_time'])\n\n    return df_train_over, df_test\n\ndef check_memory():\n    \n    mem=str(os.popen('free -t -m').readlines())\n    T_ind=mem.index('T')\n    mem_G=mem[T_ind+14:-4]\n    S1_ind=mem_G.index(' ')\n    mem_T=mem_G[0:S1_ind]\n    mem_G1=mem_G[S1_ind+8:]\n    S2_ind=mem_G1.index(' ')\n    mem_U=mem_G1[0:S2_ind]\n    mem_F=mem_G1[S2_ind+8:]\n    print('Free Memory = ' + mem_F +' MB')\n\n\ndef Feature_engineering(df, ip_count):\n    \n    # Count the number of clicks by ip\n    #ip_count = df.groupby(['ip'])['channel'].count().reset_index()\n    #ip_count.columns = ['ip', 'clicks_by_ip']\n    df = pd.merge(df, ip_count, on='ip', how='left', sort=False)\n    df['clicks_by_ip'] = df['clicks_by_ip'].astype('uint16')\n    #df.drop('ip', axis=1, inplace=True)\n    return df\n\n\ndef predict_And_Submit_using_xgb(df, Trained_Model):\n    \n\n    Num_of_line = 100\n    print(Num_of_line*'=')\n    #sub = pd.DataFrame()\n    #sub['click_id'] = df['click_id'].astype('int')\n    #df['clicks_by_ip'] = df['clicks_by_ip'].astype('uint16')\n    \n    data_to_submit = pd.DataFrame()\n    data_to_submit['click_id'] = range(0, len(df))\n    dtest = xgb.DMatrix(df)\n    del df\n    predict = Trained_Model.predict(dtest, ntree_limit=Trained_Model.best_ntree_limit)\n    data_to_submit['is_attributed'] = predict\n\n    print(Num_of_line*'=')\n    print('data_to_submit = \\n', data_to_submit.head(5))\n    pyplot.hist(data_to_submit['is_attributed'], log = True)\n    #data_to_submit.to_csv('Amin_csv_to_submit.csv', index = False)\n    return data_to_submit\n\n\ndef predict_And_Submit(df, Trained_Model):\n\n    Num_of_line = 100\n    print(Num_of_line*'=')\n    pred = Trained_Model.predict(df)\n    print('pred Done.')\n    predict = pd.DataFrame(pred)\n    data_to_submit = pd.DataFrame()\n    data_to_submit['click_id'] = range(0, len(df))\n    data_to_submit['is_attributed'] = predict\n    print(Num_of_line*'=')\n    print('data_to_submit = \\n', data_to_submit.head(5))\n    pyplot.hist(data_to_submit['is_attributed'], log = True)\n    #data_to_submit.to_csv('Amin_csv_to_submit.csv', index = False)\n    return data_to_submit","170824a0":"def Train_KNN(X_training, y_training, X_valid, y_valid):\n    knn_clf = KNeighborsClassifier()\n    knn_clf.fit(X_training, y_training)\n    pred_knn = knn_clf.predict(X_valid)\n    KNN_accuracy = accuracy_score(y_valid, pred_knn)\n    print('KNN_accuracy=\\n', KNN_accuracy)\n    Trained_KNN_Model = knn_clf\n    return Trained_KNN_Model, KNN_accuracy\n    \ndef Train_Decision_tree(X_training, y_training, X_valid, y_valid):\n    dt_clf = DecisionTreeClassifier()\n    dt_clf.fit(X_training, y_training)\n    pred_dt = dt_clf.predict(X_valid)\n    Decision_tree_accuracy = accuracy_score(y_valid, pred_dt)\n    print('Decision_tree_accuracy=\\n', Decision_tree_accuracy)\n    Trained_Decision_tree_Model = dt_clf\n    return Trained_Decision_tree_Model, Decision_tree_accuracy\n    \ndef Train_Random_forest(X_training, y_training, X_valid, y_valid):\n    rf_clf = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n    rf_model = rf_clf.fit(X_training, y_training)\n    pred_rf = rf_clf.predict(X_valid)\n    Random_forest_accuracy = accuracy_score(y_valid, pred_rf)\n    print('Random_forest_accuracy=\\n', Random_forest_accuracy)\n    Trained_Random_forest_Model = rf_clf\n    return Trained_Random_forest_Model, Random_forest_accuracy\n    \ndef Train_logistic_regression(X_training, y_training, X_valid, y_valid):\n    logreg_clf = LogisticRegression()\n    logreg_clf.fit(X_training, y_training)\n    pred_logreg = logreg_clf.predict(X_valid)\n    logistic_regression_accuracy = accuracy_score(y_valid, pred_logreg)\n    print('logistic_regression_accuracy=\\n', logistic_regression_accuracy)\n    Trained_logistic_regression_Model = logreg_clf\n    return Trained_logistic_regression_Model, logistic_regression_accuracy\n\ndef Train_Gaussian_Naive_Bayes(X_training, y_training, X_valid, y_valid):\n    gnb_clf = GaussianNB()\n    gnb_clf.fit(X_training, y_training)\n    pred_gnb = gnb_clf.predict(X_valid)\n    Gaussian_Naive_Bayes_accuracy = accuracy_score(y_valid, pred_gnb)\n    print('Gaussian_Naive_Bayes_accuracy=\\n', Gaussian_Naive_Bayes_accuracy)\n    Trained_Gaussian_Naive_Bayes_Model = gnb_clf\n    return Trained_Gaussian_Naive_Bayes_Model, Gaussian_Naive_Bayes_accuracy\n    \ndef Train_support_vector_machine(X_training, y_training, X_valid, y_valid):\n    linsvc_clf = LinearSVC()\n    linsvc_clf.fit(X_training, y_training)\n    pred_linsvc = linsvc_clf.predict(X_valid)\n    support_vector_machine_accuracy = accuracy_score(y_valid, pred_linsvc)\n    print('support_vector_machine_accuracy=\\n', support_vector_machine_accuracy)\n    Trained_support_vector_machine_Model = linsvc_clf\n    return Trained_support_vector_machine_Model, support_vector_machine_accuracy\n    \ndef Train_XGBoost(X_training, y_training, X_valid, y_valid):\n\n    XGB_model = XGBClassifier(learning_rate =0.1,\n                              n_estimators=1000,\n                              max_depth=15,\n                              min_child_weight=1,\n                              gamma=0,\n                              subsample=0.8,\n                              colsample_bytree=0.8,\n                              objective= 'binary:logistic',\n                              nthread=4,\n                              scale_pos_weight=1)\n    \n   \n    \n    \n    XGB_model.fit(X_training, y_training)\n    pred_XGB = XGB_model.predict(X_valid)\n    pred_XGB = [round(value) for value in pred_XGB]\n    XGBoost_accuracy = accuracy_score(y_valid, pred_XGB)\n    print('XGBoost_accuracy=\\n', XGBoost_accuracy)\n    Trained_XGBoost_Model = XGB_model\n    return Trained_XGBoost_Model, XGBoost_accuracy\n\ndef Train_BBC(X_training, y_training, X_valid, y_valid):\n    #Create an object of the classifier.\n    bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n                                    sampling_strategy='auto',\n                                    replacement=False,\n                                    random_state=0)\n\n    BBC = bbc.fit(X_training, y_training)\n    pred_BBC = bbc.predict(X_valid)\n    BBC_accuracy = accuracy_score(y_valid, pred_BBC)\n    print('BBC accuracy = ', BBC_accuracy)\n    return BBC, BBC_accuracy\n\n\ndef xgb2(X_training, y_training, X_valid, y_valid):\n    \n    params = {'eta': 0.3,\n          'tree_method': \"hist\",\n          'grow_policy': \"lossguide\",\n          'max_leaves': 1400,  \n          'max_depth': 0, \n          'subsample': 0.9, \n          'colsample_bytree': 0.7, \n          'colsample_bylevel':0.7,\n          'min_child_weight':0,\n          'alpha':4,\n          'objective': 'binary:logistic', \n          'scale_pos_weight':9,\n          'eval_metric': 'auc', \n          'nthread':8,\n          'random_state': 99, \n          'silent': True}\n    \n    dtrain = xgb.DMatrix(X_training, y_training)\n    dvalid = xgb.DMatrix(X_valid, y_valid)\n    \n    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n    \n    model = xgb.train(params, dtrain, 200, watchlist, maximize=True, early_stopping_rounds = 25, verbose_eval=5)\n\n    return model\n\n\ndef model_performance(Models, Accuracy):\n    model_performance = pd.DataFrame({\n    \"Model\": Models,\n    \"Accuracy\": Accuracy\n    })\n    print(model_performance.sort_values(by=\"Accuracy\", ascending=False))\n    \ndef generate_ip_count(df_train, df_test):\n    \n    \n    df_train2 = df_train.copy()\n    df_test2 = df_test.copy()\n    # Drop the IP and the columns from target\n    y = df_train2['is_attributed']\n    df_train2.drop(['is_attributed'], axis=1, inplace=True)\n    # Drop IP and ID from test rows\n    sub = pd.DataFrame()\n    #sub['click_id'] = test['click_id'].astype('int')\n    df_test2.drop(['click_id'], axis=1, inplace=True)\n    gc.collect()\n    nrow_df_train2 = df_train2.shape[0]\n    merge = pd.concat([df_train2, df_test2])\n\n    del df_train2, df_test2\n    gc.collect()\n    \n    # Count the number of clicks by ip\n    ip_count = merge.groupby(['ip'])['channel'].count().reset_index()\n    ip_count.columns = ['ip', 'clicks_by_ip']\n    merge = pd.merge(merge, ip_count, on='ip', how='left', sort=False)\n    merge['clicks_by_ip'] = merge['clicks_by_ip'].astype('uint16')\n    merge.drop('ip', axis=1, inplace=True)\n\n    df_train2 = merge[:nrow_df_train2]\n    df_test2 = merge[nrow_df_train2:]\n    del df_test2, merge\n    gc.collect()\n    \n    return ip_count","430420c9":"def Run_Kernel(Skip_range_low, Skip_range_Up, nrows):\n    \n    Start_time = time.time()\n    #Address to data\n    address_train = '..\/input\/talkingdata-adtracking-fraud-detection\/train.csv'\n    address_test = '..\/input\/talkingdata-adtracking-fraud-detection\/test.csv'\n    address_train_sample = '..\/input\/talkingdata-adtracking-fraud-detection\/train_sample.csv'\n    address_test_supplement = '..\/input\/talkingdata-adtracking-fraud-detection\/test_supplement.csv'\n    \n    print('Reading data...!'); check_memory()\n    nrows_read_train = 100; nrows_read_test = 100\n    #df_train, df_test = read_train_test_data(address_train, nrows_read_train, address_test, nrows_read_test)\n    #df_train, df_test = read_train_test_data(address_train, nrows_read_train, address_test, nrows_read_test, Skip_range_low, Skip_range_Up, nrows)\n    df_train, df_test = read_train_test_data_balanced(address_train_sample, address_test)\n    #df_train, df_test = read_train_test_data_balanced_oversample1(address_train, address_test)\n    #df_train, df_test = read_csv_random(address_train, address_test, 0.05)\n    print(len(df_train)); print('Reading Done!'); check_memory()\n    \n    some_useful_data_insight(df_train)\n    some_useful_data_insight(df_test)\n    Plot_Hist_columns(df_train, ['ip', 'app','device', 'os', 'channel', 'is_attributed'])\n   \n\n    #Parse time\n    print('Parse, training data...'); check_memory(); Parse_time(df_train); print('Parse, training data, Done!'); \n    check_memory()\n    \n    #Feature_engineering data\n    ip_count = generate_ip_count(df_train, df_test)\n    df_train = Feature_engineering(df_train, ip_count); df_train.head(); null_table(df_train);  df_train.head() #df_train = df_train.dropna()\n    \n    #Drop and normalize \n    print('Drop colum and normalize, training data...!'); check_memory()\n    colmn_names = ['attributed_time','click_time', 'ip']; df_train = Drop_cols(df_train, colmn_names)\n    #df_train = Normalized(df_train)\n    print('Drop colum and normalize, training data, Done!'); check_memory()\n    \n    \n    #Devide training data, X-Y\n    print('Begin devide training data, X_Y...'); check_memory()\n    X, Y = Make_X_Y(df_train); X_training, y_training, X_valid, y_valid = Train_Test_training_valid(X, Y, 0.1)\n    print('Begin devide training data, X_Y, Done!'); check_memory()\n    print('Cleaning before training'); del df_train; gc.collect(); check_memory()\n    print('Begin training...'); check_memory()\n    \n    #Trained_XGBoost_Model, XGBoost_accuracy = Train_XGBoost(X_training, y_training, X_valid, y_valid)\n    Trained_Decision_tree_Model, Decision_tree_accuracy = Train_Decision_tree(X_training, y_training, X_valid, y_valid)\n    #Trained_BBC_Model, BBC_accuracy = Train_BBC(X_training, y_training, X_valid, y_valid)\n    #Trained_support_vector_machine_Model, support_vector_machine_accuracy = Train_support_vector_machine(X_training, y_training, X_valid, y_valid)    \n    #Trained_KNN_Model, KNN_accuracy = Train_KNN(X_training, y_training, X_valid, y_valid)\n    #Trained_Random_forest_Model, Random_forest_accuracy = Train_Random_forest(X_training, y_training, X_valid, y_valid)\n    #Trained_logistic_regression_Model, logistic_regression_accuracy = Train_logistic_regression(X_training, y_training, X_valid, y_valid)\n    #Trained_Gaussian_Naive_Bayes_Model, Gaussian_Naive_Bayes_accuracy = Train_Gaussian_Naive_Bayes(X_training, y_training, X_valid, y_valid)\n    #Trained_xgb2_Model = xgb2(X_training, y_training, X_valid, y_valid)\n    \n    print('training Done!'); check_memory(); print('reading test data')\n    \n    \n    \n    \n    #Parse time\n    print('Parse, test data...'); check_memory(); Parse_time(df_test); df_test.head(); null_table(df_test); print('Parse, test data, Done!'); check_memory()\n    #Feature_engineering data\n    df_test = pd.merge(df_test, ip_count, on='ip', how='left', sort=False)\n    #df_test, ip_count = Feature_engineering(df_test)\n    #Drop and normalize\n    print('Drop colum and normalize, test data...!'); check_memory()\n    colmn_names = [\"click_time\", \"click_id\", \"ip\"]; df_test = Drop_cols(df_test, colmn_names); df_test.head(); null_table(df_test);\n    #df_test = Normalized(df_test)\n    print('Drop colum and normalize, test data, Done!'); check_memory()\n    print('Cleaning before prediction'); del X, Y, X_training, y_training, X_valid, y_valid, ip_count; gc.collect(); check_memory()\n\n    #Begin Prediction\n    print('Begin Prediction...')\n    check_memory()\n    #data_to_submit = predict_And_Submit(df_test, Trained_XGBoost_Model)\n    data_to_submit = predict_And_Submit(df_test, Trained_Decision_tree_Model)\n    #data_to_submit = predict_And_Submit(df_test, Trained_BBC_Model)\n    #data_to_submit = predict_And_Submit(df_test, Trained_support_vector_machine_Model)\n    #data_to_submit = predict_And_Submit(df_test, Trained_KNN_Model)\n    #data_to_submit = predict_And_Submit(df_test, Trained_Random_forest_Model)\n    #data_to_submit = predict_And_Submit(df_test, Trained_logistic_regression_Model)\n    #data_to_submit = predict_And_Submit(df_test, Trained_Gaussian_Naive_Bayes_Model)\n    #data_to_submit = predict_And_Submit_using_xgb(df_test, Trained_xgb2_Model)    \n    \n    print('Prediction Done!'); check_memory(); print('Cleaning RAM'); del df_test; gc.collect(); check_memory()\n    print('Program ran for {} seconds'.format(time.time()-Start_time)); print(50*'=','\\n',50*'=')\n    \n    return data_to_submit","8dd972b2":"                #Run_Kernel(Skip_range_low, Skip_range_Up, nrows)\ndata_to_submit = Run_Kernel(1, 1200000, 3000000) # Note that in case of using balanced data reading the used values in the Run_Kernel function are ignored.\ndata_to_submit.to_csv('Amin_csv_to_submit.csv', index = False)","f4f75c3b":"data_to_submit.head()","eae88226":"In this Kernel, several ML algorithm are applied for fraud detection. For sack of simplicity, all necessary models and function are written using Python function which makes working with data easy and chance for making mistake way less. Several reading functions were written which including reading data base on two classes. It was observed that the data is pretty imbalance and therefore it will affect the ML algorithm performance. The class 1 was read (from train samples) and then the same number of class 0 was read. two classes were merged to come up with 50-50 balance data. To run codes efficiently, those data which are not necessary for the next step in program was delete and garbage collector function was run. The data was feathered based on the following Kernel. (https:\/\/www.kaggle.com\/pranav84\/lightgbm-fixing-unbalanced-data-lb-0-9680) It is observed that the Decision tree algorithm will fit with around 71% accuracy on Training data and 77% on test data, utilizing the 50-50 reading data method. The XGBoost with its associated parameters had the accuracy of 89% (utilizing the 30M data with reading ordinary method). The reading 50-50 class method was not very helpful in this case. A procedure was written to apply the same ML algorithm to different reading intervals and use the voting method for improving the performance. Fraud detection Project, Taking-Data challenge.","abbc22db":"'''\ndef Kernel_several_times():\n    \n    data_to_submit1 = Run_Kernel(1, 2, 40000000)\n    #data_to_submit2 = Run_Kernel(1, 60000000, 20000000)\n    #data_to_submit3 = Run_Kernel(1, 120000000, 20000000)\n    #data_to_submit4 = Run_Kernel(1, 90000000, 30000000)\n    #data_to_submit5 = Run_Kernel(1, 1200000000, 30000000)\n    #data_to_submit6 = Run_Kernel(1, 1500000000, 30000000)\n    return data_to_submit1#, data_to_submit2, data_to_submit3#, data_to_submit4, data_to_submit5, data_to_submit6\n# Vote\ndef take_care_of_data():\n    \n    a1 = pd.DataFrame()\n    a1['click_id'] = data_to_submit1['click_id']\n    a1['is_attributed'] = (data_to_submit1['is_attributed']+\\\n                           data_to_submit2['is_attributed']+\\\n                           data_to_submit3['is_attributed'])\/3\n    a = a1['is_attributed'].values.tolist()\n    #a = [1 if i>0.1 else 0 for i in a]\n    b = pd.DataFrame(a)\n    data_to_submit = pd.DataFrame()\n    data_to_submit['click_id'] = data_to_submit1['click_id']\n    data_to_submit['is_attributed'] = b[0]\n    pyplot.hist(data_to_submit['is_attributed'], log = True)\n    data_to_submit.to_csv('Results_csv_to_submit.csv', index = False)\n    return data_to_submit  \n\n'''","5f4ef999":"# Models","4de9a037":"# Essential Libraries","95402bf6":"# Essential Functions","012cba50":"# Run the Kernel"}}