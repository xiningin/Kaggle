{"cell_type":{"f17429cf":"code","a4dc1f65":"code","13ccb45f":"code","e3b1c9b9":"code","f7d1102e":"code","63445473":"code","e59b94e5":"code","72356e26":"code","716f0eaf":"code","29bb0f4e":"code","8e909c30":"code","7ab54dd8":"code","3067ea73":"code","53664e8f":"code","97151e4b":"code","1761feae":"code","bd2fbfde":"code","8f06ec27":"code","34889be4":"code","7102eea3":"code","d798ce88":"code","93a9152a":"code","2dc5a13d":"code","aed643b7":"code","407d8fcd":"code","f18ae215":"code","59e472a2":"code","ba072998":"code","14f6c564":"code","43b0df8e":"code","7ca4b3f4":"code","2b119aa8":"code","2b7ddd50":"code","de4c412b":"code","44d3d6a4":"code","13521947":"code","11f15027":"code","28d0c10e":"code","8530f834":"code","2a9b32fb":"code","5850fc18":"code","535ffb22":"code","f21d28f9":"code","fc118ec1":"code","5c6fcf79":"code","a8f4e15d":"code","30f6d35c":"code","b8340d1c":"code","00abbb0e":"code","cb24405a":"code","e7524983":"code","fe2e4061":"code","4453b24f":"markdown","e97d6862":"markdown","d0d9bfc4":"markdown","4e659785":"markdown","45bf7817":"markdown","b4f916d7":"markdown","712c3f8d":"markdown","ad520c43":"markdown","0b920805":"markdown","06e77ebd":"markdown","5a8572d3":"markdown","346b4378":"markdown","bf01e7cc":"markdown","1e69e371":"markdown","97082c2e":"markdown","d4c23e61":"markdown","5a33fa78":"markdown","344995c2":"markdown","2003b2e4":"markdown","fcff6436":"markdown","93273c81":"markdown","d8c6e9a7":"markdown","1f723e0e":"markdown","318e1df4":"markdown","b69e7cfc":"markdown","b09292d3":"markdown","06ee3015":"markdown","78698e57":"markdown","c41e8784":"markdown","27576c10":"markdown","c63fd4cc":"markdown","5b2ac2c7":"markdown"},"source":{"f17429cf":"from sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport os \nimport pandas as pd\nimport seaborn as sns","a4dc1f65":"nRowsRead = None\ndf = pd.read_csv('..\/input\/freMTPL2freq.csv', delimiter=',', nrows = nRowsRead)\ndf.dataframeName = 'french_claims_data'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\n\ndf['LogDensity'] = np.log(df['Density']).round(6)\ndf['Freq']=df['ClaimNb']\/df['Exposure']","13ccb45f":"df.head()","e3b1c9b9":"df.describe()","f7d1102e":"#any null values\nall(df.notnull())","63445473":"sns.heatmap(data=df.corr(), cmap = 'viridis', annot=True)","e59b94e5":"#claims as func of age\n#gives sum of exposure per category\nEVY= df.groupby('DrivAge',as_index=False).agg({'Exposure': 'sum'})\n#gives no.claims(weighted by exposure) per category\nFreq= df.groupby('DrivAge',as_index=False).agg({'Freq': 'mean'})\n\nplt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nsns.scatterplot(data=pd.pivot_table(index='DrivAge', values='Freq', data=df))\n#sns.distplot(Freq)\nplt.subplot(1,2,2)\nsns.barplot(x='DrivAge', y='Exposure',data=df.groupby('DrivAge',as_index=False).agg({'Exposure': 'sum'}))","72356e26":"print('min driv age', df.DrivAge.min())\nprint('mean driv age',df.DrivAge.mean())\ndf[(df.DrivAge <20) | (df.DrivAge >90)]['DrivAge'].value_counts()","716f0eaf":"#claims as func of BousMalus, removed values of BonusMalus with low exposure\ndf=df.assign(Freq_BonusMalus=df.groupby('BonusMalus')['BonusMalus'].transform('count')\/df['Exposure'])\nby_bm = df.groupby('BonusMalus')[['Freq','Freq_BonusMalus']].mean()\nby_bm=by_bm.sort_values(by=['Freq_BonusMalus'],ascending=False).drop('Freq_BonusMalus',axis=1).iloc[:56]\n\nplt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nsns.scatterplot(data=by_bm)\n#sns.distplot(Freq)\nplt.subplot(1,2,2)\nsns.barplot(x='BonusMalus', y='Exposure',data=df.groupby('BonusMalus',as_index=False).agg({'Exposure': 'sum'}))","29bb0f4e":"#veh power exposure and claims\nplt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nsns.barplot(x='VehPower', y='Freq',data= df.groupby('VehPower',as_index=False).agg({'Freq': 'mean'}))\n#sns.distplot(Freq)\nplt.subplot(1,2,2)\nsns.barplot(x='VehPower', y='Exposure',data=df.groupby('VehPower',as_index=False).agg({'Exposure': 'sum'}))","8e909c30":"#veh age\nplt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nsns.scatterplot(data=pd.pivot_table(index='VehAge', values='Freq', data=df))\n#sns.distplot(Freq)\nplt.subplot(1,2,2)\nsns.barplot(x='VehAge', y='Exposure',data=df.groupby('VehAge',as_index=False).agg({'Exposure': 'sum'}))","7ab54dd8":"#area\nplt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nsns.barplot(x='Area', y='Freq',data= df.groupby('Area',as_index=False).agg({'Freq': 'mean'}))\n\nplt.subplot(1,2,2)\nsns.barplot(x='Area', y='Exposure',data=df.groupby('Area',as_index=False).agg({'Exposure': 'sum'}))","3067ea73":"#Veh Brand\nplt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nsns.barplot(x='VehBrand', y='Freq',data= df.groupby('VehBrand',as_index=False).agg({'Freq': 'mean'}))\n\nplt.subplot(1,2,2)\nsns.barplot(x='VehBrand', y='Exposure',data=df.groupby('VehBrand',as_index=False).agg({'Exposure': 'sum'}))","53664e8f":"#Veh Gas\nplt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nsns.barplot(x='VehGas', y='Freq',data= df.groupby('VehGas',as_index=False).agg({'Freq': 'mean'}))\n\nplt.subplot(1,2,2)\nsns.barplot(x='VehGas', y='Exposure',data=df.groupby('VehGas',as_index=False).agg({'Exposure': 'sum'}))","97151e4b":"#Density\nplt.figure(figsize=(20,8))\n\nplt.subplot(1,4,1)\nsns.scatterplot(data=pd.pivot_table(index='Density', values='Freq', data=df))\nplt.title('Density')\nplt.subplot(1,4,2)\nsns.distplot(df.Density,bins=30)\n\n\nplt.subplot(1,4,3)\nsns.scatterplot(data=pd.pivot_table(index='LogDensity', values='Freq', data=df))\nplt.title('LogDensity')\n\nplt.subplot(1,4,4)\nsns.distplot(df.LogDensity,bins=30)","1761feae":"#Veh REGION\nplt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nsns.barplot(x='Region', y='Freq',data= df.groupby('Region',as_index=False).agg({'Freq': 'mean'}))\n\nplt.subplot(1,2,2)\nsns.barplot(x='Region', y='Exposure',data=df.groupby('Region',as_index=False).agg({'Exposure': 'sum'}))","bd2fbfde":"df.drop(['Freq_BonusMalus', 'Density'],axis=1,inplace=True)","8f06ec27":"#unproportionally high number of claims at v. low exposure\nprint('low exposure, mean freq {}'.format(df[df['Exposure']<0.01]['Freq'].mean()))\nprint('high exposure, mean freq {}'.format(df[df['Exposure']>0.9]['Freq'].mean()))\nprint('overall, mean freq {}'.format(df['Freq'].mean()))\nprint('low exposure, mean claims {}'.format(df[df['Exposure']<0.01]['ClaimNb'].mean()))\nprint('high exposure, mean claims {}'.format(df[df['Exposure']>0.9]['ClaimNb'].mean()))\nprint('overall, mean claims {}'.format(df['ClaimNb'].mean()))\ndf['Exposure']=df['Exposure'].round(4)\ndf.drop('IDpol',axis=1).duplicated().value_counts()\n","34889be4":"df_dup = df[df.drop(['IDpol','Freq','ClaimNb'],axis=1).duplicated(keep=False)]\nprint(df[df.ClaimNb >5])\nprint(df_dup[df_dup.ClaimNb >5])","7102eea3":"#predict freq of claims, so drop claimsnb and use regressor algorithm\n#remove IDpol and Freq and make dummy variabls of categories\n#use exposure column as weight\n#use ClaimNb as target\ndf_modelling = df\ndf_modelling.head()","d798ce88":"#sort out categorical variables into binary\nArea_dummies = pd.get_dummies(df_modelling['Area'], drop_first=True)\nVehBrand_dummies = pd.get_dummies(df_modelling['VehBrand'], drop_first=True)\nVehGas_dummies = pd.get_dummies(df_modelling['VehGas'], drop_first=True)\nRegion_dummies = pd.get_dummies(df_modelling['Region'], drop_first=True)\ndf_modelling = pd.concat([df_modelling.drop('Area',axis=1),Area_dummies],axis=1)\ndf_modelling = pd.concat([df_modelling.drop('VehBrand',axis=1),VehBrand_dummies],axis=1)\ndf_modelling = pd.concat([df_modelling.drop('VehGas',axis=1),VehGas_dummies],axis=1)\ndf_modelling = pd.concat([df_modelling.drop('Region',axis=1),Region_dummies],axis=1)","93a9152a":"df_modelling.head(0)","2dc5a13d":"#splitting the data (AB)\nfrom sklearn.model_selection import train_test_split\n\n# Get index sorted with ascending IDpol, just in case it is out or order, also remove IDpol\ndf_all = df_modelling.sort_values('IDpol').reset_index(drop=True)\n\n# Split out training data\ndf_train, df_not_train = train_test_split(df_all, test_size=0.3, random_state=51, shuffle=True)\n\n# Split remaining data between validation and holdout\ndf_validation, df_holdout = train_test_split(df_not_train, test_size=0.5, random_state=13, shuffle=True)\n\nX_train = df_train.drop(['Freq','ClaimNb','Exposure','IDpol'],axis=1)\ny_train = df_train['ClaimNb']\nX_validation = df_validation.drop(['Freq','ClaimNb','Exposure','IDpol'],axis=1)\ny_validation = df_validation['ClaimNb']","aed643b7":"#use of neg_mean_square_error in CV to score model\nimport sklearn\nsorted(sklearn.metrics.SCORERS.keys())","407d8fcd":"#simple tree, no bagging, no boosting\n#cv scored on mean squared error\n#hyperoarameters tuned\n#paper suggests min_leaf_sample(weighted) = 10000, max_leaf_nodes=12\n#weighted by exposure, exposure removed as a feature\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\n#instantiate model\ndt= DecisionTreeRegressor()\n#hypertuning parameters\nparams = {'max_leaf_nodes':[8,12,16,\n                            20,24],\n          'min_weight_fraction_leaf':[0.005\n                            ,0.01,0.015,0.02,0.05\n                            ]}\ngrid_dt = GridSearchCV(estimator=dt,param_grid=params,scoring='neg_mean_squared_error',cv=5,verbose=2)\ngrid_dt.fit(X_train,y_train,sample_weight=df_train['Exposure'].values)","f18ae215":"#getting best hyperparameters\n#Best params and model chosen to predict off validation set\nbest_hyperparams = grid_dt.best_params_\nprint(best_hyperparams)\nbest_CV_score = grid_dt.best_score_\nprint('best_CV_score',best_CV_score)\nbest_model = grid_dt.best_estimator_\ny_pred=best_model.predict(X_validation)","59e472a2":"#evaluation of model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error as mse\nprint('rmse from validation data:',np.sqrt(mse(y_validation,y_pred, sample_weight=df_validation['Exposure'].values)))\n#how do you weight on cv?\nprint('rmse from cv of train data (5 folds) unweighted:',np.sqrt(best_CV_score*-1))","ba072998":"importances=pd.Series(data=best_model.feature_importances_,\n                      index=X_train.columns).sort_values(ascending=False).iloc[:8]\nimportances.plot(kind='bar')","14f6c564":"#random forest model withoout grid search\n#include boostrapping, no. features, estimators, using tuned hyperparameters from paper\nfrom sklearn.ensemble import RandomForestRegressor\ngrid_rf=RandomForestRegressor(bootstrap = False, max_features='sqrt',max_leaf_nodes=20, min_weight_fraction_leaf=0.005, n_estimators=250)\ngrid_rf.fit(X_train,y_train,sample_weight=df_train['Exposure'].values)\nbest_model_rf = grid_rf\ny_pred_rf=best_model_rf.predict(X_validation)","43b0df8e":"print('Random Forest rmse from validation data:',np.sqrt(mse(y_validation,y_pred_rf,sample_weight=df_validation['Exposure'].values)))\nprint('Single Tree rmse from validation data:',np.sqrt(mse(y_validation,y_pred, sample_weight=df_validation['Exposure'].values)))","7ca4b3f4":"claimnb_validation = pd.concat((pd.Series(y_pred,index=y_validation.index), pd.Series(y_pred_rf,index=y_validation.index), y_validation,df_validation['Exposure']),axis=1)\nclaimnb_validation.sort_index()\ndef weighted_avg(column,weights=claimnb_validation['Exposure']):\n    column = pd.to_numeric(column)\n    for x in column:\n        weighted_avg = x*weights\/weights.sum()\n        return weighted_avg.sort_index()\n\nclaimnb_validation['y_pred weighted'] = weighted_avg(claimnb_validation[0])\nclaimnb_validation['y_pred_rf weighted'] = weighted_avg(claimnb_validation[1])\nclaimnb_validation['y_validation weighted'] = weighted_avg(pd.to_numeric(claimnb_validation['ClaimNb']))","2b119aa8":"df_validation1 = df_validation[['Exposure','ClaimNb']]\ndf_validation1.index = df_validation['IDpol']","2b7ddd50":"#for random forest, is X_validation, best_model_rf, y_pred_rf \nrf_df = pd.DataFrame(index=df_validation['IDpol'], data = y_pred_rf,columns = ['Random Forest Predictions'])\nrf_df = pd.concat((rf_df,df_validation1),axis=1).sort_index()\npd.DataFrame.to_pickle(rf_df,'Alex_Farquharson_rf_dataframe.gzip')","de4c412b":"rf_df['weighted rf predictions']=rf_df['Random Forest Predictions']*rf_df['Exposure']\nrf_df['weighted actual values']=rf_df['ClaimNb']*rf_df['Exposure']\n\nprint('weighted predicted result over weighted actual result',np.sum(rf_df['weighted rf predictions'])\/np.sum(rf_df['weighted actual values']))\nrf_df.sum()","44d3d6a4":"importances_rf=pd.Series(data=best_model_rf.feature_importances_,\n                      index=X_train.columns).sort_values(ascending=False).iloc[:20]\nimportances_rf.plot(kind='bar')\nplt.title('influence of each feature on the model')","13521947":"#plot of predicted freq and actual freq for the two models ( validation data set)\nfig, _ = plt.subplots(2,4, figsize=(15,8))\nplt.subplot(1,2,1)\nsns.scatterplot(x=y_pred,y=y_validation)\nplt.title('Single tree')\nplt.subplot(1,2,2)\nsns.scatterplot(x=y_pred_rf,y=y_validation)\nplt.title('Random forest')","11f15027":"#plot of predicted freq and actual freq for the two models ( train data set)\nfig, _ = plt.subplots(2,4, figsize=(15,8))\nplt.subplot(1,2,1)\nsns.scatterplot(x=best_model.predict(X_train),y=y_train)\nplt.title('Single tree')\nplt.subplot(1,2,2)\nsns.scatterplot(x=best_model_rf.predict(X_train),y=y_train)\nplt.title('Random forest')","28d0c10e":"def plot_two_histograms(data1, data_source1, data2, data_source2,bins1=20,bins2=20):\n    fig, _ = plt.subplots(2, 2, figsize=(12, 4))\n    plt.subplot(1,2,1)\n    sns.distplot(data1,kde=False,bins =bins1)\n    plt.title('Frequency of values from {}'.format(data_source1))\n    plt.subplot(1,2,2)\n    sns.distplot(data2,kde=False,bins=bins2)\n    plt.title('Frequency of claims from {}'.format(data_source2))\n\nplot_two_histograms(y_pred,'tree model',y_pred_rf, 'random forest model',bins1=100, bins2=100)","8530f834":"#removal of claims >  certain number\nsns.distplot(df[df['ClaimNb']>4]['ClaimNb'],kde=False,bins=20)\ndf_no_fraud = df_modelling\ndf_no_fraud = df_no_fraud[df_no_fraud['ClaimNb']<=4]\ndf[df['ClaimNb']>4]","2a9b32fb":"#re-doing with df_no_fraud data\n# Get index sorted with ascending IDpol, just in case it is out or order, also remove IDpol\ndf_all_no_fraud = df_no_fraud.sort_values('IDpol').reset_index(drop=True)\ndf_all_no_fraud.drop('IDpol',axis=1,inplace=True)\n\n# Split out training data\ndf_train_no_fraud, df_not_train_no_fraud = train_test_split(df_all, test_size=0.3, random_state=51, shuffle=True)\n\n# Split remaining data between validation and holdout\ndf_validation_no_fraud, df_holdout_no_fraud = train_test_split(df_not_train, test_size=0.5, random_state=13, shuffle=True)\n\nX_train_no_fraud = df_train_no_fraud.drop(['Freq','ClaimNb','Exposure'],axis=1)\ny_train_no_fraud  = df_train_no_fraud['ClaimNb']\nX_validation_no_fraud  = df_validation_no_fraud.drop(['Freq','ClaimNb','Exposure'],axis=1)\ny_validation_no_fraud  = df_validation_no_fraud['ClaimNb']","5850fc18":"#random forest model\n#include boostrapping, no. features, estimators, using tuned hyperparameters from paper\nfrom sklearn.ensemble import RandomForestRegressor\nrf_no_fraud=RandomForestRegressor(min_weight_fraction_leaf=0.01,max_leaf_nodes=12, bootstrap = False, n_estimators = 100, max_features ='sqrt')\nrf_no_fraud.fit(X_train_no_fraud,y_train_no_fraud,sample_weight=df_train_no_fraud['Exposure'].values)","535ffb22":"y_pred_no_fraud=rf_no_fraud.predict(X_validation_no_fraud)\nprint(np.sqrt(mse(y_validation_no_fraud,y_pred_no_fraud,sample_weight=df_validation_no_fraud['Exposure'].values)))\nprint(y_pred.sum())\nprint(y_validation_no_fraud.sum())","f21d28f9":"fig, _ = plt.subplots(2,4, figsize=(15,8))\nplt.subplot(1,2,1)\nsns.scatterplot(x=y_pred_no_fraud,y=y_validation_no_fraud)\nplt.subplot(1,2,2)\nsns.scatterplot(x=rf_no_fraud.predict(X_train_no_fraud),y=y_train_no_fraud)","fc118ec1":"#quantile lift (best model is best_model_rf)\n#1 sort validation data by predicted loss\n#1.1 get df with predicted claimnb, actual claimnb id\npredicted_claimnb = best_model_rf.predict(X_validation)\npredicted_claimnb= pd.Series(predicted_claimnb)\npredicted_claimnb.index=X_validation.index\ndf_lift = pd.concat((predicted_claimnb, y_validation, df_validation['Exposure']),axis=1)\ndf_lift.rename(columns={0:'Predicted ClaimNb'},inplace=True)\nassert len(predicted_claimnb)==len(df_lift)==len(y_validation)\nprint('yes')\ndf_lift=df_lift.sort_values(by='Predicted ClaimNb',ascending=False)\ndf_lift.index=np.arange(1,101703)","5c6fcf79":"#2bucket into equally weighted \n#2.1 cum exposure column\n#2.2 make function tos plit by weights\n#2.3 check its done\ndf_lift['Cum Exposure'] = df_lift['Exposure'].cumsum()\n\nfrom pandas._libs.lib import is_integer\n\ndef weighted_qcut(values, weights, q,**kwargs):\n    #Return weighted quantile cuts from a given series\n    if is_integer(q):\n        quantiles = np.linspace(0, 1, q + 1)\n    else:\n        quantiles = q\n    order = weights.iloc[values.argsort()].cumsum() #makes series of cumulative exposure (sorted by values)\n    bins = pd.cut(order \/ order.iloc[-1], quantiles, **kwargs) #cuts into q quantiles along order (cumulative exposure) column\n    return bins.sort_index() #makes column in line with index of original dataframe\n\n\n#add weighted column to dataframe\ndf_lift['weighted_cut'] = weighted_qcut(df_lift['Predicted ClaimNb'],df_lift['Exposure'],10,labels=False)\n#check it worked\n#assert df_lift[df_lift['weighted_cut']== 9]['Exposure'].sum().round(0) == df_lift[df_lift['weighted_cut']== 0]['Exposure'].sum().round(0)\n#print('split by weights correctly')","a8f4e15d":"#3 calculate means on predicted and real for each bucket\npredicted_mean_values = []\nactual_mean_values= []\nfor x in np.arange(10):\n    Predicted_mean = df_lift[df_lift['weighted_cut']==x]['Predicted ClaimNb'].mean()\n    predicted_mean_values.append(Predicted_mean)\n    Actual_mean = df_lift[df_lift['weighted_cut']==x]['ClaimNb'].mean()\n    actual_mean_values.append(Actual_mean)\n\ncolnames=['predicted']\nmeans=pd.DataFrame(columns= colnames,data=predicted_mean_values)\nmeans['actual'] = actual_mean_values\nmeans['index']=np.arange(1,11)\nmeans","30f6d35c":"sns.set_style('darkgrid')\nsns.scatterplot(data=means,x='actual',y='actual')\nsns.scatterplot(data=means, x='actual',y='predicted')\nplt.ylim(0.02,0.12)\nplt.ylabel('predicted and actual')\nplt.title('Predicted (orange) and actual (blue) ClaimsNb')\nprint('model differentitation between top and bottom decile',means.iloc[9]['predicted'] \/ means.iloc[0]['predicted'])\nprint('actual differentitation between top and bottom decile',means.iloc[9]['actual'] \/ means.iloc[0]['actual'])","b8340d1c":"#attempt at double lift\n#make dataframe\npredicted_claimnb_rf = pd.Series(best_model_rf.predict(X_validation))\npredicted_claimnb_tree = pd.Series(best_model.predict(X_validation))\npredicted_claimnb_rf.index=X_validation.index\npredicted_claimnb_tree.index=X_validation.index\ndf_d_lift = pd.concat((predicted_claimnb_rf,predicted_claimnb_tree, y_validation, df_validation['Exposure']),axis=1)\ndf_d_lift.rename(columns={0:'Predicted ClaimNb rf',1:'Predicted ClaimNb tree'},inplace=True)\nassert len(predicted_claimnb_rf)==len(df_d_lift)==len(y_validation)==len(predicted_claimnb_tree)\nprint('yes')\ndf_d_lift['ratio']=df_d_lift['Predicted ClaimNb rf']\/df_d_lift['Predicted ClaimNb tree']\ndf_d_lift=df_d_lift.sort_values(by='ratio',ascending=False)\ndf_d_lift.index=np.arange(1,101703)\ndf_d_lift\n\n#split into quartiles\ndf_d_lift['weighted_cut'] = weighted_qcut(df_d_lift['ratio'],df_d_lift['Exposure'],10,labels=False)\n\n#check it worked\n#assert df_d_lift[df_d_lift['weighted_cut']== 9]['Exposure'].sum().round(0) == df_d_lift[df_d_lift['weighted_cut']== 0]['Exposure'].sum().round(0)\n#print('split by weights correctly')\n\n#calculate means on predicted and real for each bucket\npredicted_rf_mean_values = []\npredicted_tree_mean_values = []\nactual_mean_values = []\nfor x in np.arange(10):\n    Predicted_rf_mean = df_d_lift[df_d_lift['weighted_cut']==x]['Predicted ClaimNb rf'].mean()\n    predicted_rf_mean_values.append(Predicted_rf_mean)\n    Predicted_tree_mean = df_d_lift[df_d_lift['weighted_cut']==x]['Predicted ClaimNb tree'].mean()\n    predicted_tree_mean_values.append(Predicted_tree_mean)\n    Actual_mean = df_d_lift[df_d_lift['weighted_cut']==x]['ClaimNb'].mean()\n    actual_mean_values.append(Actual_mean)\n\ncolnames=['predicted rf']\nmeans=pd.DataFrame(columns= colnames,data=predicted_rf_mean_values)\nmeans['predicted tree'] = predicted_tree_mean_values\nmeans['actual'] = actual_mean_values\nmeans['index']=np.arange(1,11)\n\n#plot it\nsns.scatterplot(data=means,x='actual',y='actual', color ='blue')\nsns.scatterplot(data=means, x='actual',y='predicted rf',color ='orange')\nsns.scatterplot(data = means, x='actual', y='predicted tree',color = \"darkred\")\n\nplt.ylabel('predicted and actual')\nplt.title('Predicted rf (orange), predicted tree (red) and actual (blue) ClaimsNb')\nprint('rf model differentitation between top and bottom decile',means.iloc[0]['predicted rf'] \/ means.iloc[9]['predicted rf'])\nprint('tree model differentitation between top and bottom decile',means.iloc[0]['predicted tree'] \/ means.iloc[9]['predicted tree'])\nprint('model differentitation between top and bottom decile',means.iloc[0]['actual'] \/ means.iloc[9]['actual'])","00abbb0e":"#for random forest, is X_validation, best_model_rf, y_pred_rf \nrf_df = pd.DataFrame(index=X_validation.index, data = y_pred_rf,columns = ['Random Forest Predictions'])\nrf_df = pd.concat((rf_df,y_validation, df_validation['Exposure']),axis=1).sort_index()\npd.DataFrame.to_pickle(rf_df,'Alex_Farquharson_rf_dataframe.gzip')\n\npd.DataFrame.to_pickle(X_train,'Alex_Farquharson_X_train_dataframe.gzip')\npd.DataFrame.to_pickle(y_train,'Alex_Farquharson_y_train_dataframe.gzip')\npd.DataFrame.to_pickle(X_validation,'Alex_Farquharson_X_validation_dataframe.gzip')\npd.DataFrame.to_pickle(y_validation,'Alex_Farquharson_y_validation_dataframe.gzip')","cb24405a":"import pickle\nAlex_Farquharson_rf_model = pickle.dumps(best_model_rf)\nbest_model_rf_2 = pickle.loads(Alex_Farquharson_rf_model)","e7524983":"from joblib import dump,load\ndump(best_model_rf,'rf_model.gzip')\nbest_model_rf_2 = load('rf_model.gzip')","fe2e4061":"#dataframe of models\nmodels_list = [best_model, best_model_rf, rf_no_fraud]\nmodel_name = ['best_model', 'best_model_rf', 'rf_no_fraud']\nweighted = ['Yes','Yes','Yes']\ntrained_on = ['normal dataset', 'normal dataset', 'fraud omitted dataset']\ntarget_variable = ['claimNb','claimNb','claimNb']\nmodels = pd.DataFrame()\nmodels['model']=models_list\nmodels['model_name']=model_name\nmodels['weighted']=weighted\nmodels['trained_on']=trained_on\nmodels['target_variable']=target_variable\nmodels","4453b24f":"## Saving model and dataframe to pickle","e97d6862":"high exposure at BM=50","d0d9bfc4":"# Introduction\nThis kernel goes through loading the data, exploratory analysis, data manipulation, then model building and tuning (RF), and some evaluation (lift charts, rmse) of the final model.","4e659785":"log density better variable to use - in model, normal distribution of variable more logical\n\nbetter definition between cities and rural","45bf7817":"both models fail to model values with very high ClaimNb, good, means anonymous results not affecting model, expected with tree approach\n","b4f916d7":"sums weighted by exposure shows model extremely accurate at calculating overall ClaimNb, factor of 0.99 out","712c3f8d":"0.005 min weight fraction leaf is 0.5% of EVY, which is approx. 1% of rows. To put this into context, this means it is granular enough to split by B12, just about by BonusMalus>100 or age <22","ad520c43":"* random forest model (right) has max Freq at 0.3 and has a more coninuous spread in its values\n* single tree model can only output 20 probabilities ( compared to 100 ** 12) for random forest model\n* Approximately a Poisson distribution with mean 0.065 (makes sense as target variable is independent,discrete,random, count data)\n","0b920805":"df_dup values are of consecutive IDpol - worth noting, same person just next year on? \n\nmaybe new column when next year happens, meaning doesnt matter if we take these out or not, EVY,feautures and target remain the same.\n\n\nor bad\/fraudulent data results?\n\nClaimNB average is smaller than original, however of the 7 results where ClaimNb >5, 5 are from duplicated data and have same features\n","06e77ebd":"## Visual data exploration:","5a8572d3":"## Loading the data","346b4378":"## Evaluation metrics:\n### Lift","bf01e7cc":"## Datamanipulation\npreparation of the data set and its variables\n\nCategorical, repeat, log, train,test,split","1e69e371":"* In order to predict claims accurately one must know the exposure of each row. At the very crudest, it should be left to be the mean (0.5287 for df) or more accurately one could predict exposure.\n* Using exposure in the model (to predict claims) can not use weight in predict method. This leave the possibility of yusing exposure as a feature to predict claims.\n* Past models have indicated that exposure is heavily influential in the model, so using a predicted exposure(with a non negligible error) this would heavily influence the no. claims predicted (propagating its error) leaving a large error in the predicted claims value.\n* \n* Ideally:\n* Use predicted exposure to weight validation dataset.\n* Notes on exposure prediction model - must be predicted off known features only (not claims)\n* Formulation of model to predict exposure resulted in model similar to a null model, (mean), mean exposure should be used in model","97082c2e":"can clearly see higher no of claims per EVY at low exposure values - could use exposure as a feature\nthis would require knowledge of length of policy in advance - unknown for new datasets\nis best to remove exposure as a feature\n\nA lot of duplicated values, ","d4c23e61":"### Double Lift comparing tree with forest","5a33fa78":"small number of claims >=5 the majority of which have identical features - (only 2 dont) all are removed and the model retrained","344995c2":"## Modelling\n\n### Regression Tree model","2003b2e4":"No link between Freq and vehoiwer.\nVehpower poisson distributed: lambda ~ 6.5","fcff6436":"some element of overfitting, CV(val) > CV(train)","93273c81":"exposure removed as a feature here - good - no data leakage\n\nTree only large enough to take into account 6 splits, with BM dominating the result","d8c6e9a7":"### exposure prediction model code\n#data\nX_train_e = df_train.drop(['Freq','ClaimNb','Exposure'],axis=1)\ny_train_e = df_train['Exposure']\nX_validation_e = df_validation.drop(['Freq','ClaimNb','Exposure'],axis=1)\ny_validation_e = df_validation['Exposure']\n\n#model rf_exposure\nrf_exposure = RandomForestRegressor(bootstrap = False, max_features='sqrt',max_leaf_nodes=20, min_samples_leaf = 5000, n_estimators=250)\nrf_exposure.fit(X_train_e,y_train_e)\ny_pred_e=rf_exposure.predict(X_validation_e)\n\n#model rf_exposure evaluation\nnp.sqrt(mse(y_validation_e, y_pred_e))\nsns.scatterplot(x=y_validation_e,y=y_validation_e)\nsns.scatterplot(x=y_validation_e,y=y_pred_e)\n\nfactor = y_pred \/ df[df['Exposure']==1]['ClaimNb'].mean() * df['ClaimNb'].mean()\nfactor.sum()","1f723e0e":"### random forest model code (removed and best params chosen)\n### 'bootstrap': False, 'max_features': 'sqrt', 'max_leaf_nodes': 20, 'min_weight_fraction_leaf': 0.005, 'n_estimators': 250\nfrom sklearn.ensemble import RandomForestRegressor\nrf=RandomForestRegressor()\nparams = {'bootstrap':[False,True],'n_estimators':[100,250],'max_features':['sqrt','log2'],'min_weight_fraction_leaf':[0.005,0.015,0.025], 'max_leaf_nodes':[12,20]}\ngrid_rf = GridSearchCV(estimator=rf,param_grid=params,scoring='neg_mean_squared_error',cv=5,verbose=2)\ngrid_rf.fit(X_train,y_train,sample_weight=df_train['Exposure'].values)\n\nbest_hyperparams = grid_rf.best_params_\nprint('best hyperparams',best_hyperparams)\nbest_CV_score = grid_rf.best_score_\nprint('best_CV_score',best_CV_score)\nbest_model_rf = grid_rf.best_estimator_\ny_pred_rf=best_model_rf.predict(X_validation)","318e1df4":"direct comparison of models","b69e7cfc":"??if exposure is unknown then freq of validation data is unknown so should be predicting claims? not freq??\ncompare this error with that of tree trained on all data\nmodel: 6729, 0.238525 without weighting\n        6729, 0.25974 with weighting\nactual: 5447","b09292d3":"Poisson distributed age, right skewed with low limit cutoff at 18, mean = 45\n\nApparent trend of claims with age - makes logical sense\n\nWorth noting that correlation between drivage ad exposure, but little correaltion between exposure and claimNB to counteract this, which may push freq up for low drivage values","06ee3015":"df is raw data,\n\nexploring the data table:","78698e57":"Strong correaltions between driver age and BonusMalus - expected\n\nCorrelation between vehage,drivage and exposure\n\nSurprisingly little correlation between exposure and ClaimNb","c41e8784":"## Saving of dataset for evaluation later (apologies for placement here)","27576c10":"no more high claims data\nshould leave it in validation data set..","c63fd4cc":"## Modelling on data with removal of fraudulent (high ClaimNb) rows","5b2ac2c7":"* similar errors for both models.\n* both models overpredict no. of claims - will be interesting to see effect of removal of fraaulent claims - see later: it has small effect, but makes model overpredict a little more.\n\n* For a model that predicts ClaimNb, has unknown value of exposure when predicting. Either assumes all values are weighted evenly at average 0.52 or at 1.\n* If the latter is true, all rows will get a predicted no. of claims as if they lasted the whole year - untrue, some finish early lowering their real no. of claims, so overprediction of model should be expected.\n\n* however if upon calculating total no. of claims predicted you times it by the exposure, you will get the models actual prediction which is comparable with data set"}}