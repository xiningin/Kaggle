{"cell_type":{"460604b6":"code","89a501dc":"code","88df8813":"code","2367e34e":"code","1467c031":"code","f0295ca9":"code","05a7b838":"code","fcb9376c":"code","4f54b277":"code","14bc8692":"code","a1f8607f":"code","906b7c66":"code","8ba8a544":"code","fadc4604":"code","a34712fb":"code","4c124ea9":"code","690b27b2":"code","4b6c41b7":"code","d65ab476":"code","d9c1a2dd":"code","ef671223":"code","b094b678":"code","5ef8da82":"code","88e2136c":"code","5d83365b":"code","860d26f1":"code","fd2bf55b":"code","3fa06a41":"code","3fab551f":"code","5ba5c134":"code","d3457c95":"code","21e60f9a":"code","f3630be6":"code","3584eb2e":"code","e6bc07eb":"code","846c7e8e":"markdown","41466e30":"markdown","08ec4957":"markdown","81c7f69c":"markdown","0886042b":"markdown","62bad094":"markdown","05581fa1":"markdown","92afad2e":"markdown","1dc45d0d":"markdown","5d51e54a":"markdown","86b4766c":"markdown","43130455":"markdown","18b07191":"markdown","9fa43b4c":"markdown","389d47ac":"markdown","a3033399":"markdown"},"source":{"460604b6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","89a501dc":"import warnings\nwarnings.filterwarnings('ignore')","88df8813":"from time import time\nimport datetime\nimport gc\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style('white')\nsns.set(font_scale=1.2)","2367e34e":"import tensorflow as tf\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras import layers\n#from tensorflow.keras.backend as K\n\nfrom sklearn.metrics import log_loss\nfrom tensorflow_addons.layers import WeightNormalization","1467c031":"np.random.seed(42)\ntf.random.set_seed(42)","f0295ca9":"df_train = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\nprint('Train data size: {}'.format(df_train.shape))\ndisplay(df_train.head(3))\n\ndf_target_ns = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\nprint('Train target nonscored size: {}'.format(df_target_ns.shape))\ndisplay(df_target_ns.head(3))\n\ndf_target_s = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\nprint('Train target scored size: {}'.format(df_target_s.shape))\ndisplay(df_target_s.head(3))\n\ndf_test = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\nprint('Test data size: {}'.format(df_test.shape))\ndisplay(df_test.head(3))\n\ndf_sample = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\nprint('Sample submission size: {}'.format(df_sample.shape))\ndisplay(df_sample.head(3))","05a7b838":"print(df_train.isnull().sum().any())","fcb9376c":"df_train.info()","4f54b277":"display(df_train.select_dtypes('int64').head(3))\ndisplay(df_train.select_dtypes('object').head(3))","14bc8692":"g_features = [cols for cols in df_train.columns if cols.startswith('g-')]","a1f8607f":"color = ['dimgray', 'navy', 'purple', 'orangered', 'red', 'green', 'mediumorchid', 'khaki', 'salmon', 'blue', 'cornflowerblue', 'mediumseagreen']\n\ncolor_ind = 0\nn_row = 6\nn_col = 3\nn_sub = 1\n\nplt.rcParams['legend.loc'] = 'upper right'\nfig = plt.figure(figsize=(8,14))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\n\nfor i in (np.arange(0,6,1)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.kdeplot(df_train.loc[:,g_features[i]],color=color[color_ind],shade=True,\n                 label=['mean:'+str('{:.2f}'.format(df_train.loc[:,g_features[i]].mean()))\n                        +'  ''std: '+str('{:.2f}'.format(df_train.loc[:,g_features[i]].std()))])\n    \n    plt.xlabel(g_features[i])\n    plt.legend()                    \n    n_sub+=1\n    color_ind+=1\nplt.show()","906b7c66":"c_features = [cols for cols in df_train.columns if cols.startswith('c-')]","8ba8a544":"n_row = 6\nn_col = 3\nn_sub = 1 \nfig = plt.figure(figsize=(8,14))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nplt.rcParams[\"legend.loc\"] = 'upper left'\nfor i in (np.arange(0,6,1)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.kdeplot(df_train.loc[:,c_features[i]],color=color[color_ind],shade=True,\n                 label=['mean:'+str('{:.2f}'.format(df_train.loc[:,c_features[i]].mean()))\n                        +'  ''std: '+str('{:.2f}'.format(df_train.loc[:,c_features[i]].std()))])\n    \n    plt.xlabel(c_features[i])\n    plt.legend()                    \n    n_sub+=1\n    color_ind+=1\nplt.show()","fadc4604":"fig = plt.figure(figsize=(10,4))\nplt.subplots_adjust(right=1.3)\nplt.subplot(1,2,1)\n\nsns.countplot(df_train['cp_time'], palette='nipy_spectral')\nplt.subplot(1,2,2)\n\nsns.countplot(df_train['cp_dose'], palette='nipy_spectral')\nplt.show()","a34712fb":"train_copy = df_train.copy()\ntrain_copy['target_71'] = df_target_s.iloc[:,72]\n\nfig = plt.figure(figsize=(16,8))\nplt.subplots_adjust(right=1.1, top=1.1)\n\nax1 = fig.add_subplot(121)\nsns.stripplot(data=train_copy, x='cp_time', y='g-3', color='red', hue='target_71', ax=ax1)\n\nax2 = fig.add_subplot(122)\nsns.stripplot(data=train_copy, x='cp_dose', y='g-3', color='red', hue='target_71', ax=ax2)\n\nplt.show()","4c124ea9":"fig = plt.figure(figsize=(16,8))\nplt.subplots_adjust(right=1.1, top=1.1)\n\nax1 = fig.add_subplot(121)\nsns.stripplot(data=train_copy, x='cp_time', y='c-3', color='yellow', hue='target_71', ax=ax1)\n\nax2 = fig.add_subplot(122)\nsns.stripplot(data=train_copy, x='cp_dose', y='c-3', color='yellow', hue='target_71', ax=ax2)\n\nplt.show()","690b27b2":"train_copy['g_mean'] = train_copy.loc[:, g_features].mean(axis=1)\n\nfig = plt.figure(figsize=(16,10))\nplt.subplots_adjust(right=1.1, top=1.1)\n\nax1 = fig.add_subplot(121)\nsns.stripplot(data=train_copy, x='cp_time', y= 'g_mean',color='red', hue='target_71',ax=ax1)\n\nax2 = fig.add_subplot(122)\nsns.stripplot(data= train_copy , x='cp_dose', y= 'g_mean', color='red', hue='target_71',ax=ax2)\n\nplt.show()","4b6c41b7":"train_copy['c_mean'] = train_copy.loc[:, c_features].mean(axis=1)\n\nfig = plt.figure(figsize=(16,10))\nplt.subplots_adjust(right=1.1, top=1.1)\n\nax1 = fig.add_subplot(121)\nsns.stripplot(data=train_copy, x='cp_time', y= 'c_mean',color='yellow', hue='target_71',ax=ax1)\n\nax2 = fig.add_subplot(122)\nsns.stripplot(data= train_copy , x='cp_dose', y= 'c_mean', color='yellow', hue='target_71',ax=ax2)\n\nplt.show()","d65ab476":"target_s_copy = df_target_s.copy()\ntarget_s_copy.drop('sig_id', axis=1, inplace=True)\n\nn_row = 20\nn_col = 4 \nn_sub = 1   \n\nfig = plt.figure(figsize=(20,50))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\n\nfor i in np.random.choice(np.arange(0,target_s_copy.shape[1],1),n_row):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.countplot(y=target_s_copy.iloc[:, i],palette='nipy_spectral',orient='h')\n    \n    plt.legend()                    \n    n_sub+=1\n\nplt.show()","d9c1a2dd":"plt.figure(figsize=(10,10))\n\ntarget_s_copy.sum().sort_values()[-20:].plot(kind='barh', color='mediumseagreen')\nplt.show()","ef671223":"target_ns_copy = df_target_ns.copy()\ntarget_ns_copy.drop('sig_id', axis=1, inplace=True)\nn_row = 20\nn_col = 4 \nn_sub = 1   \nfig = plt.figure(figsize=(20,50))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nfor i in np.random.choice(np.arange(0,target_ns_copy.shape[1],1),n_row):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.countplot(y=target_ns_copy.iloc[:, i],palette='magma',orient='h')\n    \n    plt.legend()                    \n    n_sub+=1\nplt.show()","b094b678":"plt.figure(figsize=(10,10))\n\ntarget_ns_copy.sum().sort_values()[-20:].plot(kind='barh',color='purple')\nplt.show()","5ef8da82":"ind_tr = df_train[df_train['cp_type']=='ctl_vehicle'].index\n\nind_te = df_test[df_test['cp_type']=='ctl_vehicle'].index","88e2136c":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\n\ntransformer = QuantileTransformer(n_quantiles=100, random_state=42, output_distribution='normal')\n\ndef preprocess(df):\n    df['cp_time'] = df['cp_time'].map({24:1, 48:2, 72:3})\n    df['cp_dose'] = df['cp_dose'].map({'D1':0, 'D2':1})\n    \n    g_features = [cols for cols in df.columns if cols.startswith('g-')]\n    c_features = [cols for cols in df.columns if cols.startswith('c-')]\n    \n    for col in (g_features + c_features):\n        vec_len = len(df[col].values)\n        raw_vec = df[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n        df[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    \n    return df ","5d83365b":"X = preprocess(df_train)\nX_test = preprocess(df_test)\n\ndisplay(X.head(5))\nprint('Train data size', X.shape)\n\ndisplay(X_test.head(5))\nprint('Test data size', X_test.shape)\n\ny = df_target_s.drop('sig_id', axis=1)\ndisplay(y.head(3))\nprint('Target size', y.shape)\n\ny0 = df_target_ns.drop('sig_id', axis=1)","860d26f1":"g_features = [cols for cols in X.columns if cols.startswith('g-')]\nn_comp = 0.95\n\ndata = pd.concat([pd.DataFrame(X[g_features]), pd.DataFrame(X_test[g_features])])\ndata2 = (PCA(0.95, random_state=42).fit_transform(data[g_features]))\n\ntrain2 = data2[:X.shape[0]]\ntest2 = data2[-X_test.shape[0]:]\ntrain2 = pd.DataFrame(train2, columns=[f'pca_g-{i}' for i in range(data2.shape[1])])\ntest2 = pd.DataFrame(test2, columns=[f'pca_g-{i}' for i in range(data2.shape[1])])\n\nX = pd.concat((X, train2), axis=1)\nX_test = pd.concat((X_test, test2), axis=1)\n\nc_features = [cols for cols in X.columns if cols.startswith('c-')]\nn_comp = 0.95\n\ndata = pd.concat([pd.DataFrame(X[c_features]), pd.DataFrame(X_test[c_features])])\ndata2 = (PCA(0.95, random_state=42).fit_transform(data[c_features]))\n\ntrain2 = data2[:X.shape[0]]\ntest2 = data2[-X_test.shape[0]:]\ntrain2 = pd.DataFrame(train2, columns=[f'pca_c-{i}' for i in range(data2.shape[1])])\ntest2 = pd.DataFrame(test2, columns=[f'pca_c-{i}' for i in range(data2.shape[1])])\n\nX = pd.concat((X, train2), axis=1)\nX_test = pd.concat((X_test, test2), axis=1)\nfrom sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)  \ndata = X.append(X_test)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : X.shape[0]]\ntest_features_transformed = data_transformed[-X_test.shape[0] : ]\n\n\nX = pd.DataFrame(X[['sig_id','cp_type', 'cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\nX = pd.concat([X, pd.DataFrame(train_features_transformed)], axis=1)\n\nX_test = pd.DataFrame(X_test[['sig_id','cp_type', 'cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\nX_test = pd.concat([X_test, pd.DataFrame(test_features_transformed)], axis=1)\n\ndisplay(X.head(2))\nprint(X.shape)\n\ndisplay(X_test.head(2))\nprint(X_test.shape)","fd2bf55b":"from sklearn.cluster import KMeans\n\ndef fe_cluster(train, test, n_clusters_g=35, n_clusters_c=5, SEED=239):\n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    def create_cluster(train, test, features, kind='g', n_clusters=n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        \n        data = pd.concat([train_, test_], axis=0)\n        kmeans = KMeans(n_clusters=n_clusters, random_state=SEED).fit(data)\n        \n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        \n        train = pd.get_dummies(train, columns=[f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns=[f'clusters_{kind}'])\n        \n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind='g', n_clusters=n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind='c', n_clusters=n_clusters_c)\n    \n    return train, test","3fa06a41":"X, X_test = fe_cluster(X, X_test)\n\ndisplay(X.head(2))\nprint(X.shape)\n\ndisplay(X_test.head(2))\nprint(X_test.shape)","3fab551f":"def fe_stats(train, test):\n    \n    features_g = list(train.columns)[4:776]\n    features_c = list(train.columns)[776:876]\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis=1)\n        df['g_mean'] = df[features_g].mean(axis=1)\n        df['g_std'] = df[features_g].std(axis=1)\n        df['g_kurt'] = df[features_g].kurtosis(axis=1)\n        df['g_skew'] = df[features_g].skew(axis=1)\n        \n        df['c_sum'] = df[features_c].sum(axis=1)\n        df['c_mean'] = df[features_c].mean(axis=1)\n        df['c_std'] = df[features_c].std(axis=1)\n        df['c_kurt'] = df[features_c].kurtosis(axis=1)\n        df['c_skew'] = df[features_c].skew(axis=1)\n        \n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n        return train, test","5ba5c134":"X, X_test = fe_stats(X, X_test)\n\ndisplay(X.head(2))\nprint(X.shape)\n\ndisplay(X_test.head(2))\nprint(X_test.shape)","d3457c95":"y0 = y0[X['cp_type'] == 'trt_cp'].reset_index(drop = True)\n\ny = y[X['cp_type'] == 'trt_cp'].reset_index(drop = True)\nX = X[X['cp_type'] == 'trt_cp'].reset_index(drop = True)\n\nX.drop(['cp_type','sig_id'], axis=1, inplace=True)\n\nX_test.drop(['cp_type','sig_id'], axis=1, inplace=True)\n\nprint('New data shape', X.shape)","21e60f9a":"p_min = 0.001\np_max = 0.999\n\nfrom tensorflow.keras import regularizers\n\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    \n    return -K.mean(y_true*K.log(y_pred) + (1-y_true) * K.log(1-y_pred))\n\n\n\ndef create_model(num_cols, hid_layers, activations, dropout_rate, lr, num_cols_y):\n    \n    inp1 = tf.keras.layers.Input(shape=(num_cols, ))\n    x1 = tf.keras.layers.BatchNormalization()(inp1)\n\n    for i, units in enumerate(hid_layers):\n        x1 = tf.keras.layers.Dense(units, activation=activations[i])(x1)\n        x1 = tf.keras.layers.Dropout(dropout_rate[i])(x1)\n        x1 = tf.keras.layers.BatchNormalization()(x1)\n    \n    x1 = tf.keras.layers.Dense(num_cols_y,activation='sigmoid')(x1)\n    model = tf.keras.models.Model(inputs= inp1, outputs= x1)\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                 loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)\n    \n    return model ","f3630be6":"hid_layers = [[2048, 768, 2048],[128, 1152, 1152],[128,1280,896],[1152, 2048, 1152],\n              [128, 1408, 1024], [2048, 512, 1280], [128,1536,1024],[128,2048,1408],\n              [128, 1408, 896],[1048,2048,1792]]\n\ndropout_rate = [[0.55,0.55,0.55],[0.55,0.45,0.55],[0.55,0.45,0.45],[0.55,0.55,0.55],\n                [0.55,0.55,0.55],[0.55,0.4,0.55],[0.55,0.45,0.5],[0.55,0.55,0.5],\n               [0.55, 0.45, 0.5],[0.45, 0.5, 0.55]]\n\nactivations = [['selu', 'swish', 'swish'], ['selu','relu','swish'], ['selu','relu','swish'],\n              ['selu','relu','swish'],['selu','relu','swish'],['selu','relu','swish'],\n               ['selu','relu','elu'],['selu','relu','swish'],['selu','relu','swish'],\n               ['elu','relu','swish']]\n\nlr = [0.00035388197445653164,0.0003,0.0003,0.0003,0.0003,0.0003,0.0003,\n      0.0003, 0.0010958464491213106, 0.0003]\n\nfeats = np.arange(0,X.shape[1],1)\ninp_size = int(np.ceil(1* len(feats)))\nres = y.copy()\ndf_sample.loc[:, y.columns] = 0\nres.loc[:, y.columns] = 0\n\nn_round = 2","3584eb2e":"def callbacks():\n    rlr = ReduceLROnPlateau(monitor='val_logloss', factor=0.2, patience=3, verbose=0, \n                                min_delta=1e-4, min_lr=1e-6, mode='min')\n        \n    ckp = ModelCheckpoint(\"model.h5\", monitor='val_logloss', verbose=0, \n                              save_best_only=True, mode='min')\n        \n    es = EarlyStopping(monitor='val_logloss', min_delta=1e-5, patience=10, mode='min', \n                           baseline=None, restore_best_weights=True, verbose=0)\n    return rlr, ckp, es","e6bc07eb":"def log_loss_metric(y_true, y_pred):\n    metrics = []\n    for _target in y.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    \n    return np.mean(metrics)","846c7e8e":"# Import Dependencies","41466e30":"This part of code is taken from kernel: [Rankgauss scaler and PCA](https:\/\/www.kaggle.com\/kushal1506\/moa-pytorch-0-01859-rankgauss-pca-nn?scriptVersionId=44558776)","08ec4957":"## Cell Viability Features","81c7f69c":"### cp_time and cp_dose\n\n`cp_time` and `cp_dose` indicate treatment duration (24, 48, 72 hours) and dose (high or low which are D1 and D2)","0886042b":"# Model Training","62bad094":"Let's take a look at 20 largest positive number of labels in the scored targets.","05581fa1":"## Missing Values","92afad2e":"# EDA\n* `g-` is gene expression data\n* `c-` is cell viability data\n* `cp_type` is sample treated with a compound (`cp_vehicle`) or with a control perturbations (`ctrl_vehicle`) have no MoA\n* `cp_time` and `cp_dose` indicate time duration and dose","1dc45d0d":"## Relationship between features and target\nLet's now see the relationship of feature and target with respect to dosage and time.  \nHere we will do this for one label `target_71` and two random features `g-3` and `c-3`.","5d51e54a":"Let's visaualize these features\n\n## Gene Expression Features","86b4766c":"## Targets\n### Scored Targets","43130455":"Let's us first understand what is MoA - Mechanism of Action?  \nThe term mechanism of action means the biochemical interactions through which a drug generates its pharmacological effect.  \n\nIn this notebook we are going to train a model that classifies a drug based on their biological activity.  \nThe dataset consists of different features of gene expression data, cell viability data as well as multiple targets of mechanism of action (MoA).  \nThis is a multilabel classification problem which means we have multiple targets (not multiple classes).  \n  \nWe will first perform EDA, and then train a model using deep neural networks with Keras followed by Model Evaluation in the end.","18b07191":"This shows that there are 872 floats, 1 integer, and 3 objects. Let's see them.","9fa43b4c":"# Preprocessing and Feature Engineering\n* The control group is the group which does not produce the desires effect or MoAs and therefore the target labels are zero. So let's drop those data. Also we will keep track of cintrol group (ctl_vehicle) indexes.","389d47ac":"### Non-scored targets","a3033399":"Now let's do the same for mean of `g` and `c` features. i.e., plotting the mean of g and c features with respect to target, dosage and time."}}