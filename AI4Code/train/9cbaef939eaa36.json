{"cell_type":{"304739f1":"code","158d94a8":"code","db270489":"code","dbedaab0":"code","17c6ad1e":"code","66e51091":"code","32505943":"code","8f56daf7":"code","c7e88731":"code","432c0b24":"code","a2f574af":"code","0f19ce08":"code","ac385836":"code","a5922554":"code","dce22525":"markdown","706e419b":"markdown","3ecb97d6":"markdown","93f7e574":"markdown","cb902e1c":"markdown","601fb52c":"markdown","26fb5042":"markdown","344e72c4":"markdown","ef4a841e":"markdown","062138a3":"markdown","2542af83":"markdown","017cdb5d":"markdown","aeb23128":"markdown"},"source":{"304739f1":"import os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","158d94a8":"#Importing libraries.................\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import accuracy_score,f1_score,auc,confusion_matrix\nfrom sklearn.utils import resample\nfrom sklearn.ensemble import RandomForestClassifier","db270489":"#Checking the dataset.....................\ndf=pd.read_csv('..\/input\/diabetes2.csv')","dbedaab0":"df.head()","17c6ad1e":"#Checking missing values..............\ndf.isnull().sum()","66e51091":"#Visualizing the Geography\ndf['Outcome'].value_counts().plot(kind='bar')","32505943":"df['Outcome'].value_counts()","8f56daf7":"# Separate input features (X) and target variable (y)\ny = df.Outcome\nX = df.drop('Outcome', axis=1)\n \n# Train model\nclf_2 = LogisticRegression().fit(X, y)\n \n# Predict on training set\npred_y_2 = clf_2.predict(X)\n \n# Is our model still predicting just one class?\nprint( np.unique( pred_y_2 ) )\n# [0 1]\n \n# How's our accuracy?\nprint( accuracy_score(y, pred_y_2) )\n","c7e88731":"# Separate majority and minority classes\ndf_majority = df[df.Outcome==0]\ndf_minority = df[df.Outcome==1]\n \n# Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=500,    # to match majority class\n                                 random_state=123) # reproducible results\n \n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\n \n\n#Visualizing the Geography\ndf_upsampled['Outcome'].value_counts().plot(kind='bar')\n","432c0b24":"# Separate input features (X) and target variable (y)\ny = df_upsampled.Outcome\nX = df_upsampled.drop('Outcome', axis=1)\n \n# Train model\nclf_2 = LogisticRegression().fit(X, y)\n \n# Predict on training set\ny_pred = clf_2.predict(X)\n \n# Is our model still predicting just one class?\nprint( np.unique( y_pred ) )\n\n# How's our accuracy?\nprint( accuracy_score(y, y_pred) )\n","a2f574af":"# Separate majority and minority classes\ndf_majority = df[df.Outcome==0]\ndf_minority = df[df.Outcome==1]\n \n# Downsample majority class\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=268,     # to match minority class\n                                 random_state=123) # reproducible results\n \n# Combine minority class with downsampled majority class\ndf_downsampled = pd.concat([df_majority_downsampled, df_minority])\n \n# Display new class counts\ndf_downsampled['Outcome'].value_counts().plot(kind='bar')\n\n","0f19ce08":"\n# Separate input features (X) and target variable (y)\ny = df_downsampled.Outcome\nX = df_downsampled.drop('Outcome', axis=1)\n \n# Train model\nclf_2 = LogisticRegression().fit(X, y)\n \n# Predict on training set\npred_y_2 = clf_2.predict(X)\n \n# Is our model still predicting just one class?\nprint( np.unique( pred_y_2 ) )\n# [0 1]\n \n# How's our accuracy?\nprint( accuracy_score(y, pred_y_2) )\n","ac385836":"from sklearn.svm import SVC\nfrom sklearn.metrics import roc_auc_score\n\n# Separate input features (X) and target variable (y)\ny = df.Outcome\nX = df.drop('Outcome', axis=1)\n \n# Train model\nclf_3 = SVC(kernel='linear', \n            class_weight='balanced', # penalize\n            probability=True)\n \nclf_3.fit(X, y)\n \n# Predict on training set\npred_y_3 = clf_3.predict(X)\n \n# Is our model still predicting just one class?\nprint( np.unique( pred_y_3 ) )\n# [0 1]\n \n# How's our accuracy?\nprint(\"Accuracy score\" ,accuracy_score(y, pred_y_3) )\n\n \n# What about AUROC?\nprob_y_3 = clf_3.predict_proba(X)\nprob_y_3 = [p[1] for p in prob_y_3]\nprint(\"ROC acurracy\", roc_auc_score(y, prob_y_3) )","a5922554":"# Separate input features (X) and target variable (y)\ny = df.Outcome\nX = df.drop('Outcome', axis=1)\n \n# Train model\nclf_4 = RandomForestClassifier()\nclf_4.fit(X, y)\n \n# Predict on training set\npred_y_4 = clf_4.predict(X)\n \n# Is our model still predicting just one class?\nprint( np.unique( pred_y_4 ) )\n# [0 1]\n \n# How's our accuracy?\nprint(\"Accuracy_score\", accuracy_score(y, pred_y_4) )\n \n# What about AUROC?\nprob_y_4 = clf_4.predict_proba(X)\nprob_y_4 = [p[1] for p in prob_y_4]\nprint(\"ROC_score\",roc_auc_score(y, prob_y_4) )\n","dce22525":"![](https:\/\/i.imgur.com\/0HNe7RA.gif)","706e419b":"Without balancing the data my accuracy for given dataset is 0.7747","3ecb97d6":"As per above count_plot we can see that Almost 300 Patients have a high chances of getting diabeties and almost 500 patients will have a less chances of geting diabeties....","93f7e574":"# 2. Down-sample Majority Class","cb902e1c":"# 3.Penalize Algorithms (Cost-Sensitive Training)**","601fb52c":"As per dataset we can see that we have total 9 attributes from which 8 are my independent variables and 1 is my dependent variable(**\"Outcome\"**)","26fb5042":"# **The Danger of Imbalanced Classes**","344e72c4":"**After balancing my dependent variable i got accuracy of 0.7750!!**","ef4a841e":"**Model on Diabetic patient Which has to predict whether the patient will get diabeties or not?? **","062138a3":"# You can choose as per your usecase!!!","2542af83":"After balancing my dependent variable i got accuracy of 0.76. ","017cdb5d":"# **1. Up-sample Minority Class**","aeb23128":"# **4.Use Tree-Based Algorithms**"}}