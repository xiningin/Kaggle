{"cell_type":{"cca1131a":"code","c4445281":"code","a86fd3fd":"code","7be56c19":"code","ad5515a1":"code","80103860":"code","a56585c5":"code","8ce125cd":"code","2e82b051":"code","314b6eac":"code","d7f615f4":"code","bcf8465c":"code","4852f5a9":"code","2ae1739e":"code","1d703e34":"code","9b814c17":"markdown","e18bc526":"markdown","89786e3a":"markdown","6489eb3f":"markdown","533292b6":"markdown","18fbe421":"markdown","7bd645e6":"markdown","a8a1f51b":"markdown","e7ae4485":"markdown","0e215450":"markdown","53dc2da6":"markdown"},"source":{"cca1131a":"import torch\nfrom torch import nn\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom typing import Iterator, NamedTuple, Dict, List \nfrom torch.nn import functional as f\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import ToTensor ","c4445281":"plt.rc(\"figure\", autolayout=True)\nplt.rc(\"axes\", titleweight=\"bold\", titlesize=15, titlepad=8)","a86fd3fd":"training = datasets.MNIST(\n    root=\"data\",\n    download=True,\n    train=True,\n    transform=ToTensor ()\n)\ntesting = datasets.MNIST(\n    root=\"data\",\n    download=True,\n    train=True,\n    transform=ToTensor ()\n)","7be56c19":"# Shape of one image from the training data\nimg, label = training[0]\nimg.shape","ad5515a1":"figure = plt.figure(figsize=(8, 8))\nnrow, ncol = 3, 3\nfor i in range(1, nrow * ncol + 1):\n    sample_idx = torch.randint(len(training), size=(1,)).item()\n    figure.add_subplot(nrow, ncol, i)\n    img, label = training[sample_idx]\n    plt.imshow(img.squeeze(), cmap=\"gray\")\n    plt.title(label)\n    plt.axis(\"off\")\nplt.show()","80103860":"BATCHSIZE = 32\ntrain_loader = DataLoader (training, shuffle=True, batch_size=BATCHSIZE)\ntest_loader = DataLoader(testing, shuffle=True, batch_size=BATCHSIZE)","a56585c5":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"{device} device\")","8ce125cd":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.flatten = nn.Flatten()\n        self.func1 = nn.Linear(28*28, 512)\n        self.relu1 = nn.ReLU()\n        self.func2 = nn.Linear(512, 10)\n        self.softmax = nn.Softmax()\n        \n        \n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.func1(x)\n        x = self.relu1(x)\n        x = self.func2(x)\n        logits = self.softmax(x)\n        return logits\n        ","2e82b051":"# Construct model object: model\nmodel = Model().to(device)\nmodel","314b6eac":"# Setting the optimizer and loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nloss = nn.CrossEntropyLoss()","d7f615f4":"class FitModel:\n    def __init__(self):\n        self.train_loss = []\n        self.train_acc = []\n        self.test_loss = []\n        self.test_acc = []\n            \n    def training_fn(self,\n        dataloader: DataLoader, loss_fn: loss = loss,\n        optim: optimizer = optimizer, MODEL: Model = model) -> Iterator:\n        \"\"\"\n        The Function Compute Train accuracy and loss and append loss and accuracy to\n        train_loss and train_acc list\n        :Return: Iterator \n        :Param: dataloader, loss_fn, optim, MODEL\n        \"\"\"\n        size = len(dataloader.dataset)\n        train_loss, train_acc = 0, 0\n        \n        for batch, (X, y) in enumerate(dataloader):\n            x, y = X.to(device), y.to(device)\n            \n            # forward pass\n            target = MODEL(x)\n            losses = loss_fn(target, y)\n            \n            # Back propagation\n            optim.zero_grad()\n            losses.backward()\n            optim.step()\n            \n            # Storing the train loss and train accuracy\n            train_loss += losses.item()\n            train_acc += (target.argmax(1) == y).type(torch.float).sum().item()\n            \n            if batch % 100 == 0:\n                loss, current = losses.item(), batch * len(X)\n                print (f\"Loss: {loss:>7f} [{current:>5d}\/{size:>5d}]\")\n\n        train_loss \/= size\n        train_acc \/= size\n        train_acc *= 100\n        self.train_loss.append(train_loss)\n        self.train_acc.append(train_acc)\n        print(f\"Train Error: \\nTrain Acc: {(train_acc):>0.1f} Train Avg Loss: {train_loss:>.8f} \\n\")\n                \n    def testing_fn(self, dataloader: DataLoader, loss_fn: loss = loss, MODEL: Model = model) -> Iterator:\n        \"\"\"\n        Compute test accuracy and loss and store it in the test_loss and \n        test_accuracy\n        :Return: Iterator\n        :Param: dataloader, loss_fn, MODEL\n        \"\"\"\n        size = len(dataloader.dataset)\n        test_loss, current = 0, 0\n        with torch.no_grad():\n            for x, y in dataloader:\n                pred = MODEL(x)\n                test_loss += loss_fn(pred, y).item()\n                current += (pred.argmax(1) == y).type(torch.float).sum().item()\n        test_loss \/= size\n        current \/= size\n        current *= 100\n        self.test_loss.append(test_loss)\n        self.test_acc.append(current)\n        \n        print(f\"Test Error: \\nAccuracy: {(current):>0.1f} Avg Loss: {test_loss:>.8f} \\n\")\n        \n    def history (self) -> pd.DataFrame:\n        \"\"\"\n        Returns pandas dataframe with Train_acc, Train_loss, Test_acc and Test_loss as\n        columns\n        :Return: pd.DataFram\n        :Param: None\n        \"\"\"\n        return pd.DataFrame({\n            \"Train_acc\" : self.train_acc,\n            \"Train_loss\" : self.train_loss,\n            \"Test_acc\" : self.test_acc,\n            \"Test_loss\" : self.test_loss\n        })","bcf8465c":"train = FitModel()\nfor epochs in range(20):\n    print(f\"epochs: {epochs * 1} ---------------\")\n    train.training_fn(train_loader)\n    train.testing_fn(test_loader)","4852f5a9":"# Check out the history of the model\nhistory = train.history()\nhistory","2ae1739e":"# history index \nidx = history.index\n# plotting the train accuracy and test accuracy\nplt.plot(idx, history.Train_acc, color='b')\nplt.plot(idx, history.Test_acc, color='r')\nplt.title(\"Train And Test Accuracy\")\nplt.legend('best')\nplt.xlabel(\"Epochs\")\nplt.gca().legend((\"Train acc\", \"val acc\"))\nplt.ylabel(\"Accuracy\")\nplt.show()","1d703e34":"# plotting the train loss and test loss\nplt.plot(idx, history.Train_loss, color='b')\nplt.plot(idx, history.Test_loss, color='r')\nplt.title(\"Train And Test Loss\")\nplt.legend('best')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.gca().legend((\"Train loss\", \"val loss\"))\nplt.show()","9b814c17":"# **Mnist Datasets with Pytorch**","e18bc526":"## **Prepare Data For Training With DataLoader**","89786e3a":"## **Data Shape**","6489eb3f":"## **Load Data In The Notebook**","533292b6":"## **Getting Device For Training**","18fbe421":"## **Training And Evaluation**","7bd645e6":"## **Model Building**","a8a1f51b":"## **Import Libraries** ","e7ae4485":"## **Visualizing Images with Label**","0e215450":"## **Set Notebook**","53dc2da6":"## **Loss And The Optimizer**"}}