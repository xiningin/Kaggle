{"cell_type":{"77a9c77f":"code","da0744a9":"code","26e2817b":"code","7b34686b":"code","cdd5f669":"code","af0e90a2":"code","6372609a":"code","3c9cee14":"code","51ecd1f8":"code","2a77d9ba":"code","4f6f93cf":"code","7fd2fb99":"code","50f9f3ba":"code","4ae398ab":"code","412f78a1":"code","15725723":"code","38109f81":"code","426c523e":"code","2339029f":"code","c79bdde9":"code","41dccc39":"code","86eb8c19":"code","d629d766":"code","07624afc":"code","4ad3bacf":"code","0e308d0f":"code","8ef23629":"code","3fdaa0f2":"code","8eddb525":"code","a6f62e90":"code","4de5ba79":"code","a1fa7234":"code","51417488":"code","a4c0e768":"code","d4a8c153":"code","25d3009c":"code","58710c33":"markdown","620f5880":"markdown","8b6c5de2":"markdown"},"source":{"77a9c77f":"# IMPORT MODULES\n# TURN ON the GPU !\n\nimport os\nfrom operator import itemgetter    \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nget_ipython().magic(u'matplotlib inline')\nplt.style.use('ggplot')\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import preprocessing\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, LabelEncoder, MinMaxScaler, OneHotEncoder, LabelBinarizer\nfrom sklearn.metrics import mean_squared_error, accuracy_score, mean_absolute_error\nfrom sklearn.cross_validation import KFold, cross_val_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, KFold, cross_val_predict, StratifiedKFold, train_test_split, learning_curve, ShuffleSplit\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor\n\nimport tensorflow as tf\n\nfrom keras import models, regularizers, layers, optimizers, losses, metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import np_utils\n\nprint(os.getcwd())\nprint(\"Modules imported \\n\")\nimport os\nprint(os.listdir(\"..\/input\"))","da0744a9":"# Load MIMIC2 data \n\ndata = pd.read_csv('..\/input\/mldata.csv')\nprint(\"With id\", data.shape)\n#print(data.head())\n\ndata_full = data.drop('hadm_id', 1)\n\nprint(\"No id\",data_full.shape)\n#print(data_full.head())\n\n# Label = LOS\ny = data_full['los_days']\nX = data_full.drop('los_days', 1)\nX = X.drop('expired_icu', 1)\n\nprint(\"y - Labels\", y.shape)\nprint(\"X - No Label No id No expired\", X.shape)\nprint(X.columns)","26e2817b":"print(data_full.shape)\ndata_full.info()\ndata_full.describe()","7b34686b":"data_full.head(10)","cdd5f669":"data_full.hist(bins=50, figsize=(20,15))\nplt.show()","af0e90a2":"age_histogram = data_full.hist(column='age', bins=20, range=[0, 100])\nfor ax in age_histogram.flatten():\n    ax.set_xlabel(\"Age\")\n    ax.set_ylabel(\"Num. of Patients\")\n    \nage_LOS = data_full.hist(column='los_days', bins=20, range=[0, 100])\nfor ax in age_LOS.flatten():\n    ax.set_xlabel(\"LOS\")\n    ax.set_ylabel(\"Num. of Patients\")","6372609a":"data_full.groupby('insurance').size().plot.bar()\nplt.show()\ndata_full.groupby('admission_type').size().plot.bar()\nplt.show()\ndata_full.groupby('admission_source').size().plot.bar()\nplt.show()","3c9cee14":"# Pearson linear correlation\n\ncorr_matrix = data_full.corr()\ncorr_matrix[\"los_days\"].sort_values(ascending=False)","51ecd1f8":"# Check that all X columns have no missing values\nX.info()\nX.describe()","2a77d9ba":"from pandas.plotting import scatter_matrix\n\nscatter_matrix(data_full.loc[:, data_full.columns] ,  figsize  = [15, 15], diagonal = \"kde\")\nplt.show()","4f6f93cf":"#data_full.plot(kind=\"scatter\", x=\"sofa_max\", xlim=(0,25), y=\"LOS\", alpha=0.1, ylim=(0,50))\ndata_full.plot(kind=\"scatter\", x=\"age\", xlim=(0,80), y=\"los_days\", alpha=0.1, ylim=(0,50))","7fd2fb99":"# MAP Text to Numerical Data\n# Use one-hot-encoding to convert categorical features to numerical\n\nprint(X.shape)\ncategorical_columns = ['expired_icu', \n                       'gender',\n                      'marital_status',\n                      'ethnicity',\n                      'admission_type',\n                      'admission_source',\n                      'insurance',\n                      'religion',\n                      'DiagnosisFirst',\n                      'ProcedureFirst'\n                      ]\n\nfor col in categorical_columns:\n    #if the original column is present replace it with a one-hot\n    if col in X.columns:\n        one_hot_encoded = pd.get_dummies(X[col])\n        X = X.drop(col, axis=1)\n        X = X.join(one_hot_encoded, lsuffix='_left', rsuffix='_right')\n        \nprint(X.shape)","50f9f3ba":"print(X.columns)\nprint(X['VENTRICULOSTOMY          '])","4ae398ab":"\nprint(data_full.shape)\nprint(X.shape)\n\n#XnotNorm = np.array(X.copy())\nXnotNorm = X.copy()\nprint('XnotNorm ', XnotNorm.shape)\n\nyFI = data_full.los_days\nynotNorm = yFI.copy()\nprint('ynotNorm ', ynotNorm.shape)","412f78a1":"# Normalize X\n\nx = XnotNorm.values #returns a numpy array\nscaler = preprocessing.StandardScaler()\nx_scaled = scaler.fit_transform(x)\nXNorm = pd.DataFrame(x_scaled, columns=XnotNorm.columns)\nprint(XNorm)","15725723":"# Normalize y\n\ny = ynotNorm.values #returns a numpy array\ny = y.reshape(-1, 1)\ny_scaled = scaler.fit_transform(y)\nynorm=pd.DataFrame(y_scaled)\nprint(ynorm)","38109f81":"# FEATURE IMPORTANCE Data NOT normalized using Lasso model - NOT the best one ...\n\ntrainFinalFI = XnotNorm\nyFinalFI = ynotNorm\n\nlasso=Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,\n   normalize=False, positive=False, precompute=False, random_state=None,\n   selection='cyclic', tol=0.0001, warm_start=False)\nlasso.fit(trainFinalFI,yFinalFI)\n\nFI_lasso = pd.DataFrame({\"Feature Importance\":lasso.coef_}, index=trainFinalFI.columns)\n\n# Focus on those with 0 importance\n#print(FI_lasso.sort_values(\"Feature Importance\",ascending=False).to_string())\n#print(\"_\"*80)\nFI_lasso[FI_lasso[\"Feature Importance\"] >40].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\nplt.xticks(rotation=90)\nFI_lasso[FI_lasso[\"Feature Importance\"] <-10].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\nplt.xticks(rotation=90)\nplt.show()","426c523e":"# CROSS VALIDATION\n\ndef rmse_cv(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return rmse","2339029f":"# Lin reg ALL models HYPERPARAMS NOT optimized\n\nmodels = [RandomForestRegressor(),GradientBoostingRegressor(), ExtraTreesRegressor()]\nnames = [\"RandomForestRegressor\", \"GradientBoostingRegressor\", \"ExtraTreesRegressor\"]","c79bdde9":"# Run the models and compare\n\nModScores = {}\n\nfor name, model in zip(names, models):\n    score = rmse_cv(model, XNorm, ynorm)\n    ModScores[name] = score.mean()\n    print(\"{}: {:.2f}\".format(name,score.mean()))\n\nprint(\"_\"*100)\nfor key, value in sorted(ModScores.items(), key = itemgetter(1), reverse = False):\n    print(key, round(value,3))","41dccc39":"# Optimize hyper params for one model\n\nmodel = GradientBoostingRegressor()\n\nparam_grid = [{},]\n\ngrid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(XNorm, ynorm)\n\nprint(grid_search.best_estimator_)","86eb8c19":"model = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\n             min_impurity_split=None, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=100, presort='auto', random_state=None,\n             subsample=1.0, verbose=0, warm_start=False)","d629d766":"# FEATURE IMPORTANCE - NORMALIZED - GBR model\n# NOTE GBR - has NO Negative feature importance\n\ntrainFinalFI = XNorm\nyFinalFI = ynorm\n\nGBR = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\n             min_impurity_split=None, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=100, presort='auto', random_state=None,\n             subsample=1.0, verbose=0, warm_start=False)\n\nGBR.fit(trainFinalFI,yFinalFI)\n\nFI_GBR = pd.DataFrame({\"Feature Importance\":GBR.feature_importances_,}, index=trainFinalFI.columns)\nFI_GBR[FI_GBR[\"Feature Importance\"] > 0.009469].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\nplt.xticks(rotation=90)\nplt.xticks(rotation=90)\nplt.show()","07624afc":"# List of important features for GBR\nFI_GBR = pd.DataFrame({\"Feature Importance\":GBR.feature_importances_,}, index=trainFinalFI.columns)\nFI_GBR=FI_GBR.sort_values('Feature Importance', ascending = False)\nprint(FI_GBR[FI_GBR[\"Feature Importance\"] > 0.0025])","4ad3bacf":"#print(FI_GBR)\n\n","0e308d0f":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Error\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = 1-np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = 1-np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","8ef23629":"# LEARNING CURVES Train \/ Validation\n# Note - ynotNorm !!\n\n\ntitle = \"Learning Curves \"\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nplot_learning_curve(model, title, XNorm, ynotNorm, cv=cv, n_jobs=5)\n#plot_learning_curve(model, title, XNorm, y, ylim=(0.01, 0.99), cv=cv, n_jobs=4)","3fdaa0f2":"# Split into Train & Test\n\n#   NOTE - For ed purposes ynotNorm was USED !!!\n\nX_train, X_test, y_train, y_test = train_test_split(XNorm, ynotNorm, test_size=0.2, random_state=42)\nprint ('X_train: ', X_train.shape)\nprint ('X_test: ', X_test.shape)\nprint ('y_train: ', y_train.shape)\nprint ('y_test: ', y_test.shape)","8eddb525":"# Model FINAL fit and evaluation on test\n\nmodel.fit(X_train, y_train)\n\nfinal_predictions = model.predict(X_test)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse) \nprint(\"rmse on test \", round(final_rmse, 4))\n\nfinal_mae = mean_absolute_error(y_test, final_predictions)\nprint(\"mae on test \", round(final_mae, 4))","a6f62e90":"# PLOT True vs Predicted\n\nxChart = [np.array(y_test)]\nyChart = [np.array(final_predictions)]\n\nplt.scatter(xChart,yChart, alpha=0.3)\nplt.xlim(0,60)\nplt.ylim(0,60)\nplt.plot( [0,60],[0,60], 'b')\nplt.show()\n\nplt.scatter(xChart,yChart, alpha=0.3)\nplt.xlim(0,30)\nplt.ylim(0,30)\nplt.plot( [0,30],[0,30], 'b')\nplt.show()","4de5ba79":"# Transfer data to NN format\n\nx_val = X_test\npartial_x_train = X_train\ny_val = y_test\npartial_y_train = y_train\n\nprint(\"partial_x_train \", partial_x_train.shape)\nprint(\"partial_y_train \", partial_y_train.shape)\n\nprint(\"x_val \", x_val.shape)\nprint(\"y_val \", y_val.shape)","a1fa7234":"# NN MODEL\nfrom keras import models\nmodel = models.Sequential()\nmodel.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1(0.001),\n                       input_shape=(partial_x_train.shape[1],)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(8, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\nmodel.add(layers.Dense(1))\nmodel.summary()\n\nmodel.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\nprint(\"model compiled\")","51417488":"history = model.fit(partial_x_train, partial_y_train,\n                    validation_data=(x_val, y_val), \n                    verbose=1,\n                   epochs=100)","a4c0e768":"acc = history.history['mean_absolute_error']\nval_acc = history.history['val_mean_absolute_error']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training error')\nplt.plot(epochs, val_acc, 'r', label='Validation error')\nplt.title('Training and validation ERROR')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation LOSS')\nplt.legend()\nplt.show()","d4a8c153":"# Model fit and evaluation on test\n# Set the num of Epochs and Batch Size according to learning curves\n\nmodel.fit(partial_x_train, partial_y_train, epochs=100)\ntest_mse_score, test_mae_score = model.evaluate(x_val, y_val)\nprint(\"test_mae_score on test \", test_mae_score)\nprint(\"test_mse_score on test \", test_mse_score)","25d3009c":"# PREDICT\nfinal_predictions = model.predict(X_test)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse) \nprint(\"rmse on test \", round(final_rmse, 4))\n\nfinal_mae = mean_absolute_error(y_test, final_predictions)\nprint(\"mae on test \", round(final_mae, 4))","58710c33":"* The original data is from MIMIC2 - Multiparameter Intelligent Monitoring in Intensive Care (deidentified DB) available freely from \n* https:\/\/physionet.org\/mimic2\/\n* Each instance in the mldata.csv attached is one admission\n* Testing a theory I have, that one can predict LOS just by the number of interactions betweeen patient and hospital per day, I've used the following features for the LOS prediction as a REGRESSION problem:\n* Age, Gender, Ethnicity, Insurance, Admission Type, Admission Source, SOFA first score, etc.\n* First Diagnosis on Admission (seq num=1) and first procedure on admission (seq num=1)\n* Number of Diagnosis on Admission, Procedures on Admission\n* Daily average number of: Labs, Micro labs, IV meds, Non-IV meds, Imaging Reports, Notes, Orders, Caregivers, Careunits\n\nThe label is LOS in days\n\nI've compared initially 12 REGRESSOR models. The top was GBR with a **MAE of 1.7 days**\nSurprisingly (or not) no NN I've tried could beat GBR - from small to huge NN - nothing came even close to GBR\n\nWell....the power of ENSEMBLE of weak predictors is evident (again ?) as GBR is an ensemble model...\n\nLet me know *your* results on this (overly simplified) dataset","620f5880":"# Lin reg ALL models HYPERPARAMS NOT optimized\n\nmodels = [LinearRegression(),Ridge(),Lasso(),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),\n          ElasticNet(),SGDRegressor(),BayesianRidge(),KernelRidge(),ExtraTreesRegressor()]\nnames = [\"LinearRegression\", \"Ridge\", \"Lasso\", \"RandomForestRegressor\", \"GradientBoostingRegressor\", \"SVR\", \"LinearSVR\", \n         \"ElasticNet\",\"SGDRegressor\",\"BayesianRidge\",\"KernelRidge\",\"ExtraTreesRegressor\"]\n         \n # Results on 12 Regressor Models\n\nGradientBoostingRegressor 0.32774\nExtraTreesRegressor 0.343198\nRandomForestRegressor 0.37930\nBayesianRidge 0.76652\nRidge 0.83401\nKernelRidge 0.8343\nLinearSVR 0.8392\nSVR 0.85938\nElasticNet 0.980743\nLasso 0.998706\nSGDRegressor 12414.166959306467\nLinearRegression 204379877732011.9","8b6c5de2":"**NN model**  "}}