{"cell_type":{"14f5c3c5":"code","309adfce":"code","8c21267b":"code","1b27032c":"code","601b213a":"code","8bc54b3c":"code","228b732f":"code","7e0834fd":"code","3ef6fee8":"code","c95b7aa4":"code","75796e4b":"code","1ac6aaee":"code","4bbcdefc":"code","6b67f5c4":"code","e5011b76":"code","1fe927ad":"code","6bcd5472":"code","4523b31a":"code","c9ea6b72":"code","7da216bc":"code","30c36d66":"code","c26c4ff8":"code","84b5f88a":"code","083277bc":"code","a0b4d0cc":"markdown","677f92fb":"markdown","b7d39c72":"markdown","84bec082":"markdown","e5a2ed25":"markdown","9ffd8ed5":"markdown","7aeae0c1":"markdown","10708c0d":"markdown","4254aa75":"markdown","14572d41":"markdown","eeb0b06a":"markdown","6e76ae64":"markdown","a3656bb5":"markdown","db908345":"markdown","310a7979":"markdown","f591954c":"markdown","76772892":"markdown","0465457e":"markdown","0d3fb20b":"markdown","286402f6":"markdown","49b05844":"markdown","fef5713d":"markdown","44605271":"markdown","f3c61e1d":"markdown","b974b98d":"markdown","cff11d93":"markdown","d8d2a68f":"markdown"},"source":{"14f5c3c5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.neighbors import KNeighborsClassifier # Classification\nfrom sklearn.preprocessing import StandardScaler # Standardize features\nfrom sklearn.model_selection import train_test_split # Split dataset\nfrom sklearn.metrics import classification_report, accuracy_score # Useful metrics model\n\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns # Visualization\n\nimport warnings # Ignoe warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set_style('whitegrid') # Set plot style\n%matplotlib inline \n\nimport os\nprint(os.listdir(\"..\/input\"))\n","309adfce":"iris_df = pd.read_csv(\"..\/input\/Iris.csv\")\niris_df['Target'] = iris_df['Species'].map({specie:value for value, specie in enumerate(iris_df['Species'].unique())})\niris_df.head()","8c21267b":"iris_df.info()","1b27032c":"iris_df.isnull().sum()","601b213a":"features_on_off = {'Id':False,\n                   'SepalLengthCm':True,\n                   'SepalWidthCm':True,\n                   'PetalLengthCm':True,\n                   'PetalWidthCm':True,\n                   'Species':False,\n                   'Target':False}\n\nfeatures_on = [feature for feature, state in features_on_off.items() if state]","8bc54b3c":"g = sns.PairGrid(data=iris_df, hue='Species', diag_sharey=False, vars=features_on)\ng.map_upper(sns.scatterplot)\ng.map_diag(sns.distplot)\ng.map_lower(sns.kdeplot)\ng.add_legend()","228b732f":"std_scaler = StandardScaler().fit(iris_df[features_on])\nscaled_features = std_scaler.transform(iris_df[features_on])","7e0834fd":"scaled_features = pd.DataFrame(data=scaled_features, columns=features_on)\nscaled_features.head()","3ef6fee8":"scaled_features.describe()","c95b7aa4":"X = scaled_features[features_on]\ny = iris_df['Target']","75796e4b":"n_neighbors = np.arange(1,26)\np_values = np.arange(1,11)\nerror_rate = {p:{'train':[], 'test':[]} for p in p_values}\nn_samples = 200\n\nfor k in n_neighbors:\n    train_error, test_error = 0, 0\n    for p_value in p_values:\n        for s in range(n_samples):\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)        \n            knn = KNeighborsClassifier(n_neighbors=k, p=p_value)\n            knn.fit(X_train, y_train)               \n            train_error += np.mean(knn.predict(X_train) != y_train)        \n            test_error += np.mean(knn.predict(X_test) != y_test)\n        error_rate[p_value]['train'].append(train_error\/n_samples)\n        error_rate[p_value]['test'].append(test_error\/n_samples)","1ac6aaee":"best_k_p = {}\nfor p_value in p_values:    \n    poly_coeff = np.polyfit(1\/n_neighbors[8:], error_rate[p_value]['test'][8:], 2)\n    poly = lambda x: poly_coeff[0]*x**2 + poly_coeff[1]*x + poly_coeff[2]\n    k_min = -poly_coeff[1]\/(2*poly_coeff[0])\n    error_min = poly(k_min)        \n    pair_k_error = list(zip(1\/n_neighbors, error_rate[p_value]['test']))    \n    best_k_p[p_value] = int(1\/sorted(pair_k_error, key=lambda pair: np.sqrt(np.power(pair[0]-k_min,2) + np.power(pair[1]-error_min,2)))[0][0])","4bbcdefc":"for p, k in best_k_p.items():\n    print(f'Power of Minkowski metric: {p}; Best number of neighbors: {k}')","6b67f5c4":"def plot_error(neighbors, train_error, test_error, best_k, title, ax):\n    \n    k_inv = 1.0\/neighbors\n\n    min_x, max_x = min(k_inv), max(k_inv)\n    min_y, max_y = min(min(train_error, test_error)), max(max(train_error, test_error))\n\n    ax.plot(k_inv, train_error, color='black', marker='o', markerfacecolor='blue', markeredgecolor='white', markersize=10, label='Train Error')\n    ax.plot(k_inv, test_error, color='black', marker='o', markerfacecolor='lime', markeredgecolor='white', markersize=10, label='Test Error')\n\n    ax.vlines(1\/best_k, min_y-0.015, max_y+0.025, label=f'The Bias-Variance Trade-O\ufb00 (k={best_k})', color='green', linestyle='--', linewidth=5)\n\n    ax.set_xlim((min_x-0.05, max_x+0.01))\n    ax.set_ylim((min_y-0.005, max_y+0.005))\n\n    x_right = np.linspace(1\/best_k, max_x+0.05, 10)\n    ax.fill_between(x_right, len(x_right)*[min_y-0.015], len(x_right)*[max_y+0.025], alpha=0.5, color='red', label='High Variance Low Bias')\n\n    x_left = np.linspace(min_x-0.15, 1\/best_k, 10)\n    ax.fill_between(x_left, len(x_left)*[min_y-0.015], len(x_left)*[max_y+0.025], alpha=0.5, color='orange', label='Low Variance High Bias')\n\n    ax.set_xlabel('$1\/k$', fontsize=20)\n    ax.set_ylabel('Error', fontsize=20)\n    \n    ax.set_title(title, fontsize=20)\n\n    ax.legend(loc='best', fontsize=15)    ","e5011b76":"fig, ax = plt.subplots(ncols=2, nrows=5, figsize=(20,30), sharey=False)\n\np = 1\nfor row in range(5):\n    for col in range(2):\n        plot_error(n_neighbors, error_rate[p]['train'], error_rate[p]['test'], best_k_p[p], f'$p={p}$', ax[row][col])\n        p+=1\n        \nplt.tight_layout()        ","1fe927ad":"min_error = {'train':[], 'test':[]}\n\nfor p, k in best_k_p.items():\n    min_error['train'].append(error_rate[p]['train'][k])\n    min_error['test'].append(error_rate[p]['test'][k])","6bcd5472":"plt.figure(figsize=(12,8))\n\nplt.plot(1\/np.array(p_values), min_error['train'], marker='o', markersize=10, linestyle='--', label='Train Error')\nplt.plot(1\/np.array(p_values), min_error['test'], marker='o',markersize=10, linestyle='--', label='Test Error')\n\nplt.xlabel('1\/p', fontsize=20)\nplt.ylabel('Error', fontsize=20)\n\nplt.legend(loc='best', fontsize=15)","4523b31a":"best_p = np.array(min_error['test']).argmin() + 1\nbest_k = best_k_p[best_p]","c9ea6b72":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nknn = KNeighborsClassifier(n_neighbors=best_k, p=best_p)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)","7da216bc":"print(f'Accuracy: {100*accuracy_score(y_pred, y_test):.0f}%')\nprint(classification_report(y_test, y_pred))","30c36d66":"from eli5.sklearn import PermutationImportance # Feature importance\nfrom eli5 import show_weights\n\nperm = PermutationImportance(knn).fit(X_test, y_test)\nshow_weights(perm, feature_names = features_on)","c26c4ff8":"xx, yy = np.meshgrid(np.linspace(X['PetalLengthCm'].min()-0.1, X['PetalLengthCm'].max()+0.1, 500),\n                     np.linspace(X['PetalWidthCm'].min()-0.1, X['PetalWidthCm'].max()+0.1, 500))","84b5f88a":"z = knn.predict(np.c_[np.zeros(xx.size), np.zeros(xx.size), xx.ravel(), yy.ravel()])\nz = z.reshape(xx.shape)","083277bc":"plt.figure(figsize=(12,8))\nplt.scatter(x='PetalLengthCm', y='PetalWidthCm', data=X, c=iris_df['Target'], cmap='viridis', edgecolors='black', s=100)\nplt.contourf(xx, yy, z, cmap='viridis', alpha=0.5)\nplt.xlabel('Petal Length', fontsize=20)\nplt.ylabel('Petal Width', fontsize=20)\nplt.title(f'k={best_k}; p={best_p}', fontsize=20)","a0b4d0cc":"Great! There are no missing values.","677f92fb":"## 2.1) Parameter Tuning","b7d39c72":"Now that we have enough information about the tuning parameters, we can train our final model!","84bec082":"## 1.4) Missing Values","e5a2ed25":"Now each feature has mean equal zero and standard deviation equal one, as you can see in the table below.","9ffd8ed5":"## Summary\n\n* 1) Preprocessing and exploring\n    * 1.1) Import Packages\n    * 1.2) Load Dataset\n    * 1.3) Features type\n    * 1.4) Missing Values\n    * 1.5) Visualizing the dataset\n    * 1.6) Standardizing Features    \n* 2) Training and Tuning\n    * 2.1) Parameter Tuning\n    * 2.2) Training the Final Model\n    * 2.3) Feature Importance\n    * 2.4) Decision Boundary\n\n","7aeae0c1":"## 1.5) Visualizing the dataset","10708c0d":"Let's define a dictionary to keep track which features we are going to use or not.","4254aa75":"## 2.4) Decision Boundary\n\nLet's see how the decision boundary looks like using the top two features.","14572d41":"## 1.6) Standardizing Features\n\nIt is a good idea to standardize the features to avoid scale effect.","eeb0b06a":"It seems that petal length and width are good features to be used because the kde curves do not overlap too much (as can be seen in the scatterplot)! We will see later on if this hypothesis is true.","6e76ae64":"As I supposed at the beggining of this notebook, petal width and length are the most important features (as shown on the table above), followed by sepal length and width.","a3656bb5":"Let's see how the error curves for each 'p' looks like.","db908345":"The best number of neighbors is around 11, what about the power of Minkowski metric? We can infer from the graph below and notice that for p = 1, the errors are lower.","310a7979":"As a result we achieved this:","f591954c":"## 1.2) Load Dataset","76772892":" ## 1) Preprocessing and exploring\n ## 1.1) Import Packages","0465457e":"## 2.2) Training the Final Model","0d3fb20b":"# A classification study using k-nearest neighbors algorithm.\n\nI wrote this kernel for my own self-learning, because practice is the best way to learn something. I intend to use k-nearest neighbors to approach the famous Iris dataset classification problem, and try to find out the best number of neighbors and metric. If you find that this notebook to be useful for you, please upvote! And if you have any suggestion or correction, I'd be delighted to hear from you.\n\n**P.S Sorry for my typing, I'm not fluent in English.**","286402f6":"## 1.3) Features Type","49b05844":"## 2) Training and Tuning","fef5713d":"Now we can check what are the most important features to our model. Let's use [permutation importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance-daily) to see the feature's rank.","44605271":"## 2.3) Feature Importance","f3c61e1d":"The following piece of code will take a while to run, because we are gonna take the average of the train and test error shuffling and splitting the dataset 200 times, for each combination of 'k' (number of neighbors) and 'p' (power parameter for the Minkowski metric).","b974b98d":"Now, let's split the dataset into X (features) and y (label).","cff11d93":"There are a bunch of information in the graphs above. As we would expect, the error on the test data is higher than the training data, and as the number of neighbors goes higher (or 1\/k get higher), both errors increase following a u-shape curve. The [bias-variance trade-off](https:\/\/www-bcf.usc.edu\/~gareth\/ISL\/ISLR%20Seventh%20Printing.pdf) is represented by the green dashed line, which is the optimal number of neighbors. The region on the right (red) represents where the model has high variance (overfit the data), and the region on the left (orange) represents where the model has low variance (doesn't represent well).","d8d2a68f":"Now that we have trained the model for several combinations of 'k' and 'p', the next cell will find out the best number of neighbors for each 'p'. My idea is to fit a second degree polynomial on the test error and find the minimun 'k'."}}