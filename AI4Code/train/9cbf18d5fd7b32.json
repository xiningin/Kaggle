{"cell_type":{"84bb70fa":"code","a3d70b21":"code","5531c766":"code","490451a2":"code","13940ef2":"code","ba207281":"code","fa9e4bbc":"code","1bb041b1":"markdown","c33c7951":"markdown","f371fbab":"markdown","e9c28edc":"markdown"},"source":{"84bb70fa":"# Pandas for table processing\nimport pandas as pd\n\n# Pyplot and Seaborn to draw correlation matrix\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# LGBM itself\nfrom lightgbm import LGBMClassifier\n\n# Packages for auxilary data science tasks: dividing the dataset to train and test and metrics summary generation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","a3d70b21":"# Reading and viewing the dataset as Pandas table\ndata = pd.read_csv('..\/input\/detection-of-glass-types\/Glass_Type.csv').drop(columns=['Unnamed: 0','Unnamed: 0.1'])\ndata","5531c766":"# Looking at classes and number of samples corresponding to them\ndata.Type.value_counts()","490451a2":"# Visualizing the correlation heatmap using Seaborn\ncorr = data.corr()\n\nplt.figure(figsize=(10, 10))\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)","13940ef2":"# Let's find the features the absolute correlation of which is more than 0.9\n# Removing of such features can reduce the model dimensionality and avoid information redundancy\ncorr = data.drop(columns=[\"Type\"]).corr()\ncorr_top = corr.abs().unstack().sort_values(kind='quicksort')\ncorr_top = corr_top[corr_top > 0.9][corr_top < 1]\n\ncols_to_drop = [corr_top.index[i][0] for i in range(0, len(corr_top), 2)]\nprint(f\"Highly correlated features: {cols_to_drop}\" if len(cols_to_drop) else 'There are no highly correlated features')\ndata = data.drop(columns=cols_to_drop)","ba207281":"# Dividing the dataset into training and testing sets in 80%\/20% ratio\nX_train, X_test, Y_train, Y_test = train_test_split(data.drop(columns=[\"Type\"]), data.Type,\n                                                    train_size=0.8, stratify=data.Type, random_state=42)","fa9e4bbc":"# Training and testing LightGBM\nlgc = LGBMClassifier()\nlgc.fit(X_train, Y_train)\n\nY_pred = lgc.predict(X_test)\n\nprint(classification_report(Y_test, Y_pred, digits=4))","1bb041b1":"The class labels are textual, and we could use Sklearn's LabelEncoder to transform them to numbers, but LightGBM doesn't require us to do so.","c33c7951":"**Greetings!**\n\nToday we'll try to detect the glass type by its refractive index and chemical chemical composition. There are six glass types, so we'll consider the problem as the multiclass one. The algorithm to help us with the solution is LightGBM as it learns quickly and produces results comparable to other boosting methods like XGBoost and CatBoost in most cases.\n\n**Let's go!**","f371fbab":"There are no highly correlated features, so we'll keep all the columns as there are other tips to improve the model's performance if it's not satisfying.","e9c28edc":"Perfect! Despite the imbalance of our classes, LightGBM gives us a flawless solution (all metrics are equal to 1.0) in just around 10 seconds on Kaggle environment. In more complex case, if the model fails to predict some samples, we can try to use the oversampling algorithms like ADASYN to provide more training samples to our model or use feature selection techniques.\n\n**Thank you for your attention! Hopefully this notebook will be useful to ones who start learning the multiclass calssification problems.**"}}