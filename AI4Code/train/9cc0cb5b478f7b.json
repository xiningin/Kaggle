{"cell_type":{"b041a12c":"code","5f7ba3f2":"code","99d52b49":"code","3e0a2f3b":"code","f8d7cb9b":"code","ff0a21ca":"code","03ef018c":"code","632e868b":"code","bdc4570f":"code","24df15b2":"code","ed9c10c8":"code","6f66919e":"code","5f502868":"code","611667f5":"code","801f6418":"code","f7c41650":"code","9fbe2849":"code","0a881ed4":"code","9c1380a9":"code","790808fd":"code","4186dda1":"code","ac4d06dc":"code","ec90dae8":"code","97568618":"code","7b2c8583":"code","0080a77e":"code","9fde0714":"code","81db0979":"code","6a58b637":"code","f7a36cbd":"code","13667577":"code","4f082a71":"markdown","edd25fec":"markdown","c727140a":"markdown","b7f6cbb3":"markdown","fb117047":"markdown","e258aab0":"markdown","062f5d08":"markdown","de9cbdd5":"markdown","4c500307":"markdown","af3b4858":"markdown","feb30f09":"markdown","382a0eea":"markdown","8b688961":"markdown","d12112d0":"markdown","17313fe8":"markdown","608fff14":"markdown","cbcd2193":"markdown","732a562d":"markdown","5044012d":"markdown","65123000":"markdown","6dcd956a":"markdown","53915016":"markdown","d6cf0921":"markdown"},"source":{"b041a12c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os,gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport eli5\nimport base64\nfrom IPython.display import HTML\nfrom tqdm import tqdm\n\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials \nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5f7ba3f2":"train_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')\ntest_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')\n\n# train_id = pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv')\n# test_id = pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv')\n\nsample = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv')","99d52b49":"print(train_transaction.shape,test_transaction.shape)\n# print(train_id.shape, test_id.shape)\nprint(sample.shape)","3e0a2f3b":"train_transaction.head(10)","f8d7cb9b":"train_transaction.info(max_cols=400)","ff0a21ca":"columns_to_drop = []\nnum_of_rows = train_transaction.shape[0]\nfor i in train_transaction.columns:\n    count_of_null_values = train_transaction[i].isna().sum()\n    if (count_of_null_values >= num_of_rows\/2):\n        columns_to_drop.append(i)\ndel num_of_rows        ","03ef018c":"# Dropping columns with more than 50% missing values.\ntrain_transaction.drop(columns_to_drop, axis=1, inplace=True)\ntest_transaction.drop(columns_to_drop, axis=1, inplace=True)\n\nprint(\"No of columns dropped {}\".format(len(columns_to_drop)))\ndel columns_to_drop\ngc.collect()","632e868b":"object_columns = train_transaction.select_dtypes(include=object).columns\nprint(\"Number of categorical columns: {}\".format(len(object_columns)))","bdc4570f":"for i in object_columns:\n    print(\"Column Name : {}\".format(i))\n    print(\"-------------> No of missing values: {}\".format(train_transaction[i].isna().sum()))\n    print(\"-------------> Unique values: {}\".format(train_transaction[i].unique()))","24df15b2":"fig,ax = plt.subplots(3,3,figsize=(18,18))\nfor k,i in enumerate(object_columns):\n    plt.subplot(3,3,k+1)\n    if(i != 'P_emaildomain'):\n        train_transaction[i].value_counts().plot(kind='bar')\n    else:\n        prob = train_transaction[i].value_counts(normalize=True)\n        threshold = 0.02\n        mask = prob > threshold\n        tail_prob = prob.loc[~mask].sum()\n        prob = prob.loc[mask]\n        prob['other'] = tail_prob\n        prob.plot(kind='bar')\n    plt.title(i)","ed9c10c8":"fig,ax = plt.subplots(3,3,figsize=(18,18))\nfor k,i in enumerate(object_columns):\n    plt.subplot(3,3,k+1)\n    if(i != 'P_emaildomain'):\n        test_transaction[i].value_counts().plot(kind='bar')\n    else:\n        prob = test_transaction[i].value_counts(normalize=True)\n        threshold = 0.02\n        mask = prob > threshold\n        tail_prob = prob.loc[~mask].sum()\n        prob = prob.loc[mask]\n        prob['other'] = tail_prob\n        prob.plot(kind='bar')\n    plt.title(i)","6f66919e":"for i in object_columns:\n    train_transaction[i].fillna(train_transaction[i].mode()[0], inplace=True)\n    test_transaction[i].fillna(test_transaction[i].mode()[0], inplace=True)","5f502868":"# Categorical Features:\n# ProductCD\n# card1 - card6\n# addr1, addr2\n# Pemaildomain Remaildomain\n# M1 - M9\n\n# We handles few M* features above, others were dropped because of >50% missing values.\ncat_num_features = ['addr1','addr2', 'card1', 'card2', 'card3', 'card5']\n","611667f5":"for i in cat_num_features:\n    print(\"Column Name : {}\".format(i))\n    print(\"-------------> No of missing values: {}\".format(train_transaction[i].isna().sum()))\n    print(\"Mode value {} occurred in {} transactions \\n\".format(train_transaction[i].mode()[0], train_transaction[i].value_counts().values[0]))","801f6418":"# Filling the missing values with mode.\nfor i in cat_num_features:\n    train_transaction[i].fillna(train_transaction[i].mode()[0], inplace=True)\n    test_transaction[i].fillna(test_transaction[i].mode()[0], inplace=True)\ndel cat_num_features\ngc.collect()","f7c41650":"all_numeric_columns = train_transaction.select_dtypes(include=np.number).columns\nnumeric_missing = []\nfor i in all_numeric_columns:\n    missing = train_transaction[i].isna().sum()\n    if(missing>0):\n        numeric_missing.append(i)\ndel all_numeric_columns        \nprint(len(numeric_missing))","9fbe2849":"train_transaction[numeric_missing].describe()","0a881ed4":"for k,i in enumerate(numeric_missing):\n    print(k)\n    print(\"Column {} has {} missing values\".format(i, train_transaction[i].isna().sum()))\n    print(\"Mode value {} occurred in {} transactions\".format(train_transaction[i].mode()[0], train_transaction[i].value_counts().values[0]))\n    print(\"Median value {} \\n\".format(train_transaction[i].median(), train_transaction[i].value_counts().values[0]))","9c1380a9":"# Filling the missing values with median.\nfor i in numeric_missing:\n    train_transaction[i].fillna(train_transaction[i].median(), inplace=True)\n    test_transaction[i].fillna(test_transaction[i].median(), inplace=True)\nprint(train_transaction.isna().any().sum(), test_transaction.isna().any().sum())   \ndel numeric_missing\ngc.collect()","790808fd":"# object_columns\nfor f in object_columns:\n    lbl = LabelEncoder()\n    lbl.fit(list(train_transaction[f].values) + list(test_transaction[f].values))\n    train_transaction[f] = lbl.transform(list(train_transaction[f].values))\n    test_transaction[f] = lbl.transform(list(test_transaction[f].values))    ","4186dda1":"train_transaction[object_columns].head()","ac4d06dc":"len(train_transaction.select_dtypes(exclude=np.number).sum())\ndel object_columns\ngc.collect()","ec90dae8":"train_transaction.head()","97568618":"train_transaction.describe()","7b2c8583":"# Let's plot the histogram of isFraud column.\ntrain_transaction['isFraud'].plot(kind='hist')","0080a77e":"X_train,X_val,y_train,y_val = train_test_split(train_transaction.drop(['isFraud'],axis=1), train_transaction['isFraud'], test_size=0.2)","9fde0714":"# Score= .7427\nparams = {\n    'objective': 'binary',\n    'n_estimators':300,\n    'learning_rate': 0.1,\n    'subsample':0.8\n}\n# Score= .7306\nparams1 = {\n    'objective': 'binary',\n    'n_estimators': 200,\n    'learning_rate': 0.1,\n}\n#Score= .7446\nparams2 = {\n    'objective': 'binary',\n    'n_estimators':300,\n    'learning_rate': 0.1,\n}\n# Score=.774\nparams3 = {\n    'objective': 'binary',\n    'n_estimators':600,\n    'learning_rate': 0.1\n}\n#Score= .7666\nparams4 = {\n    'objective': 'binary',\n    'n_estimators':500,\n    'learning_rate': 0.1\n}\n#Score= .7711\nparams5 = {\n    'objective': 'binary',\n    'n_estimators':500,\n    'learning_rate': 0.1,\n    'num_leaves' : 50,\n    'max_depth' : 7,\n    'subsample' : 0.9,\n    'colsample_bytree' : 0.9\n}\n#Score=.78109\nparams6 = {\n    'objective': 'binary',\n    'n_estimators':600,\n    'learning_rate': 0.1,\n    'num_leaves' : 50,\n    'max_depth' : 7,\n    'subsample' : 0.9,\n    'colsample_bytree' : 0.9\n}\n#Score=.7863\nparams7 = {\n    'objective': 'binary',\n    'n_estimators':700,\n    'learning_rate': 0.1,\n    'num_leaves' : 50,\n    'max_depth' : 7,\n    'subsample' : 0.9,\n    'colsample_bytree' : 0.9\n}","81db0979":"clf = LGBMClassifier(**params7, random_state=108)\nclf.fit(X_train,y_train)","6a58b637":"preds = clf.predict(X_val)\nroc_auc_score(y_val, preds)","f7a36cbd":"predictions = clf.predict_proba(test_transaction)\nsample['isFraud'] = predictions[:,1]\nsample.to_csv('submission.csv', index=False)","13667577":"# def create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n#     csv = df.to_csv(index=False)\n#     b64 = base64.b64encode(csv.encode())\n#     payload = b64.decode()\n#     html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n#     html = html.format(payload=payload,title=title,filename=filename)\n#     return HTML(html)\n# create_download_link(sample)","4f082a71":"## Loading Libraries and Data","edd25fec":"## EDA","c727140a":"Now let's create a list of numerical features with missing values.","b7f6cbb3":"Now that our data contains no missing values and no categorical values. We can start plotting some graphs to get intuition about the data. ","fb117047":"Some insights:\n1. Categorical columns with no missing values : ProductCD\n2. Categorical columns with few missing values : card4,card6\n3. Categorical columns with many missing values : P_emaildomain, M6\n4. Categorical columns with huge number of missing values : M1,M2,M3,M4\n\nLets plot the value counts graphs for these columns and see if we can fill the missing values with the mode value.","e258aab0":"There are many interesting things here,\n1. 'W' is the ProductCD in over 400,000 transactions.\n2. card4 type is 'visa' in over 350,000 transactions and 'mastercard' in 200,000 transactions. Other types are rare.\n3. card6 value is 'debit' in approx. 430,000 transactions and 'credit' in approx. 150,000 transactions. Other values are extremely rare.\n4. P_emaildomain type in 'google.com' in over 40% of the transactions and 'yahoo.com' in 20% of the transactions. Some values are comparatively less. Most values are rare.\n5. For M2 and M3, 'T' is the category for most transactions.\n6. For M1, 'F' category is extremely rare.\n7. In case of M4, 'M0' is the most occurred value.\n8. In M6, both 'T' and 'F' occur almost equally. Still, we will fill the missing values with 'F' as it is the mode.\n\nFrom these insights, we can safely fill the missing values with the mode of the object columns.","062f5d08":"Now, there are no missing values in our data. Let's start with handling categorical features.","de9cbdd5":"## REFERENCES\n\n1. I haven't seen any kernels from this competition yet, but I would like to thank artgor for his amazing EDA kernels. I have done a basic analysis of some of his kernels and learned a lot from them.","4c500307":"Above insights hold true for test data too.","af3b4858":"Now, let's see categorical features with numeric values.\n[From the competition host](https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203)","feb30f09":"Let's preprocess the object type columns first.","382a0eea":"Here were can see that for most V_ columns, median and mode values are same.","8b688961":"## DATA PREPROCESSING","d12112d0":"Now, plotting the object columns from test transactions.","17313fe8":"Number of features is over 200, we can't plot pairplots or heatmaps, It will take up all of kernel's ram. Let's skip to training and get a baseline score.","608fff14":"The data is highly imbalanced. We can downsample the '0' class or upsample '1' class, but for now let's continue with the imbalanced data and check the score.","cbcd2193":"I prefer manual tuning for the baseline model, you can use Hyperopt for hyperparamer-tuning.","732a562d":"1. Size of both train and test data is comparable. \n2. Number of features is pretty high.","5044012d":"Now, all columns in the data have numeric values.","65123000":"If you think there are mistakes or improvements can be made, please comment :)","6dcd956a":"Here,\n1. Mean of the 'isFraud' column is 0.034, this tells us that the no. of 0s in the columns is way greater than the number of 1s.\n2. In most of V_ columns, the max value is way greater than the mean and median. Outliers are present.\n","53915016":"Code below can be used to download the .csv generated in your Kaggle kernel, this way you can submit the predictions without having to commit the kernel again and again. ![Thanks to Rachel](https:\/\/www.kaggle.com\/rtatman\/download-a-csv-file-from-a-kernel)","d6cf0921":"There are many columns with large number of missing values.\nWe can drop columns with more than 50% missing values."}}