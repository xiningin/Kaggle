{"cell_type":{"a2bc65c0":"code","d9e5d7f5":"code","bb6a8a7d":"code","fd154f35":"code","5177f416":"code","89bbcedd":"code","144d02d9":"code","5e21839e":"code","c321811b":"code","13890a8d":"code","1b06cdb3":"code","ddf630e2":"code","dbd424bd":"code","f8896ef7":"code","88b0177f":"code","8bbbcb89":"code","3115e02a":"code","8cbc1288":"code","28ef583e":"code","9cff3f54":"code","9e77afb2":"code","6df4672c":"code","cf426444":"code","dce58845":"code","cc03a467":"code","42f3d6d2":"code","ab31334a":"code","37c458a7":"code","4e254015":"code","c14a93dd":"code","44de510a":"code","04fbfb55":"code","8a4f513d":"code","37ef7b1a":"code","8f698d71":"markdown","70f6811b":"markdown","20c5816d":"markdown","2673fad0":"markdown","99e9286d":"markdown","f0cbac77":"markdown","595180f3":"markdown","b6751864":"markdown","25bd60e5":"markdown","45938dbc":"markdown","53fd8210":"markdown","548d38e6":"markdown","f0eb3cf8":"markdown","73d8d77e":"markdown","d142006d":"markdown","fce1e26b":"markdown"},"source":{"a2bc65c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d9e5d7f5":"import os\nimport csv\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","bb6a8a7d":"from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown(string))\n#printmd('**bold**')","fd154f35":"data_path=\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\"","5177f416":"data_raw = pd.read_csv(data_path)","89bbcedd":"data_raw.head()","144d02d9":"print(\"Number of rows in data =\",data_raw.shape[0])\nprint(\"Number of columns in data =\",data_raw.shape[1])\nprint(\"\\n\")\nprintmd(\"**Sample data:**\")\ndata_raw.head()","5e21839e":"\nmissing_values_check = data_raw.isnull().sum()\nprint(missing_values_check)\n","c321811b":"\n# Comments with no label are considered to be clean comments.\n# Creating seperate column in dataframe to identify clean comments.\n\n# We use axis=1 to count row-wise and axis=0 to count column wise\n\nrowSums = data_raw.iloc[:,2:].sum(axis=1)\nclean_comments_count = (rowSums==0).sum(axis=0)\n\nprint(\"Total number of comments = \",len(data_raw))\nprint(\"Number of clean comments = \",clean_comments_count)\nprint(\"Number of comments with labels =\",(len(data_raw)-clean_comments_count))","13890a8d":"categories = list(data_raw.columns.values)\ncategories = categories[2:]\nprint(categories)","1b06cdb3":"# Calculating number of comments in each category\n\ncounts = []\nfor category in categories:\n    counts.append((category, data_raw[category].sum()))\ndf_stats = pd.DataFrame(counts, columns=['category', 'number of comments'])\ndf_stats","ddf630e2":"sns.set(font_scale = 2)\nplt.figure(figsize=(15,8))\n\nax= sns.barplot(categories, data_raw.iloc[:,2:].sum().values)\n\nplt.title(\"Comments in each category\", fontsize=24)\nplt.ylabel('Number of comments', fontsize=18)\nplt.xlabel('Comment Type ', fontsize=18)\n\n#adding the text labels\nrects = ax.patches\nlabels = data_raw.iloc[:,2:].sum().values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n\nplt.show()","dbd424bd":"rowSums = data_raw.iloc[:,2:].sum(axis=1)\nmultiLabel_counts = rowSums.value_counts()\nmultiLabel_counts = multiLabel_counts.iloc[1:]\n\nsns.set(font_scale = 2)\nplt.figure(figsize=(15,8))\n\nax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)\n\nplt.title(\"Comments having multiple labels \")\nplt.ylabel('Number of comments', fontsize=18)\nplt.xlabel('Number of labels', fontsize=18)\n\n#adding the text labels\nrects = ax.patches\nlabels = multiLabel_counts.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","f8896ef7":"from wordcloud import WordCloud,STOPWORDS\n\nplt.figure(figsize=(40,25))\n\n# toxic\nsubset = data_raw[data_raw.toxic==1]\ntext = subset.comment_text.values\ncloud_toxic = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 1)\nplt.axis('off')\nplt.title(\"Toxic\",fontsize=40)\nplt.imshow(cloud_toxic)\n\n\n# severe_toxic\nsubset = data_raw[data_raw.severe_toxic==1]\ntext = subset.comment_text.values\ncloud_severe_toxic = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 2)\nplt.axis('off')\nplt.title(\"Severe Toxic\",fontsize=40)\nplt.imshow(cloud_severe_toxic)\n\n\n# obscene\nsubset = data_raw[data_raw.obscene==1]\ntext = subset.comment_text.values\ncloud_obscene = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 3)\nplt.axis('off')\nplt.title(\"Obscene\",fontsize=40)\nplt.imshow(cloud_obscene)\n\n\n# threat\nsubset = data_raw[data_raw.threat==1]\ntext = subset.comment_text.values\ncloud_threat = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 4)\nplt.axis('off')\nplt.title(\"Threat\",fontsize=40)\nplt.imshow(cloud_threat)\n\n\n# insult\nsubset = data_raw[data_raw.insult==1]\ntext = subset.comment_text.values\ncloud_insult = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 5)\nplt.axis('off')\nplt.title(\"Insult\",fontsize=40)\nplt.imshow(cloud_insult)\n\n\n# identity_hate\nsubset = data_raw[data_raw.identity_hate==1]\ntext = subset.comment_text.values\ncloud_identity_hate = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 6)\nplt.axis('off')\nplt.title(\"Identity Hate\",fontsize=40)\nplt.imshow(cloud_identity_hate)\n\nplt.show()\n","88b0177f":"data = data_raw\ndata = data_raw.loc[np.random.choice(data_raw.index, size=2000)]\ndata.shape","8bbbcb89":"data.shape","3115e02a":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport re\n\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","8cbc1288":"def cleanHtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', str(sentence))\n    return cleantext\n\n\ndef cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    cleaned = cleaned.strip()\n    cleaned = cleaned.replace(\"\\n\",\" \")\n    return cleaned\n\n\ndef keepAlpha(sentence):\n    alpha_sent = \"\"\n    for word in sentence.split():\n        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n        alpha_sent += alpha_word\n        alpha_sent += \" \"\n    alpha_sent = alpha_sent.strip()\n    return alpha_sent","28ef583e":"data['comment_text'] = data['comment_text'].str.lower()\ndata['comment_text'] = data['comment_text'].apply(cleanHtml)\ndata['comment_text'] = data['comment_text'].apply(cleanPunc)\ndata['comment_text'] = data['comment_text'].apply(keepAlpha)\ndata.head()","9cff3f54":"stop_words = set(stopwords.words('english'))\nstop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\nre_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\ndef removeStopWords(sentence):\n    global re_stop_words\n    return re_stop_words.sub(\" \", sentence)\n\ndata['comment_text'] = data['comment_text'].apply(removeStopWords)\ndata.head()","9e77afb2":"\nstemmer = SnowballStemmer(\"english\")\ndef stemming(sentence):\n    stemSentence = \"\"\n    for word in sentence.split():\n        stem = stemmer.stem(word)\n        stemSentence += stem\n        stemSentence += \" \"\n    stemSentence = stemSentence.strip()\n    return stemSentence\n\ndata['comment_text'] = data['comment_text'].apply(stemming)\ndata.head()","6df4672c":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(data, random_state=42, test_size=0.30, shuffle=True)\n\nprint(train.shape)\nprint(test.shape)","cf426444":"\ntrain_text = train['comment_text']\ntest_text = test['comment_text']","dce58845":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\nvectorizer.fit(train_text)\nvectorizer.fit(test_text)","cc03a467":"\nx_train = vectorizer.transform(train_text)\ny_train = train.drop(labels = ['id','comment_text'], axis=1)\n\nx_test = vectorizer.transform(test_text)\ny_test = test.drop(labels = ['id','comment_text'], axis=1)","42f3d6d2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier","ab31334a":"\n%%time\n\n# Using pipeline for applying logistic regression and one vs rest classifier\nLogReg_pipeline = Pipeline([\n                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n            ])\n\nfor category in categories:\n    printmd('**Processing {} comments...**'.format(category))\n    \n    # Training logistic regression model on train data\n    LogReg_pipeline.fit(x_train, train[category])\n    \n    # calculating test accuracy\n    prediction = LogReg_pipeline.predict(x_test)\n    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n    print(\"\\n\")","37c458a7":"%%time\n\n# using binary relevance\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.naive_bayes import GaussianNB\n\n# initialize binary relevance multi-label classifier\n# with a gaussian naive bayes base classifier\nclassifier = BinaryRelevance(GaussianNB())\n\n# train\nclassifier.fit(x_train, y_train)\n\n# predict\npredictions = classifier.predict(x_test)\n\n# accuracy\nprint(\"Accuracy = \",accuracy_score(y_test,predictions))\nprint(\"\\n\")","4e254015":"# using classifier chains\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom sklearn.linear_model import LogisticRegression","c14a93dd":"%%time\n\n# initialize classifier chains multi-label classifier\nclassifier = ClassifierChain(LogisticRegression())\n\n# Training logistic regression model on train data\nclassifier.fit(x_train, y_train)\n\n# predict\npredictions = classifier.predict(x_test)\n\n# accuracy\nprint(\"Accuracy = \",accuracy_score(y_test,predictions))\nprint(\"\\n\")","44de510a":"# using Label Powerset\nfrom skmultilearn.problem_transform import LabelPowerset","04fbfb55":"\n%%time\n\n# initialize label powerset multi-label classifier\nclassifier = LabelPowerset(LogisticRegression())\n\n# train\nclassifier.fit(x_train, y_train)\n\n# predict\npredictions = classifier.predict(x_test)\n\n# accuracy\nprint(\"Accuracy = \",accuracy_score(y_test,predictions))\nprint(\"\\n\")","8a4f513d":"# http:\/\/scikit.ml\/api\/api\/skmultilearn.adapt.html#skmultilearn.adapt.MLkNN\n\nfrom skmultilearn.adapt import MLkNN\nfrom scipy.sparse import csr_matrix, lil_matrix","37ef7b1a":"%%time\n\nclassifier_new = MLkNN(k=10)\n\n# Note that this classifier can throw up errors when handling sparse matrices.\n\nx_train = lil_matrix(x_train).toarray()\ny_train = lil_matrix(y_train).toarray()\nx_test = lil_matrix(x_test).toarray()\n\n# train\nclassifier_new.fit(x_train, y_train)\n\n# predict\npredictions_new = classifier_new.predict(x_test)\n\n# accuracy\nprint(\"Accuracy = \",accuracy_score(y_test,predictions_new))\nprint(\"\\n\")","8f698d71":"## TF-IDF","70f6811b":"## Removing Stop Words\u00b6","20c5816d":"##  Label Powerset","2673fad0":"## Stemming","99e9286d":"## Cleaning Data","f0cbac77":"## Multiple Binary Classifications - (Binary Relevance)","595180f3":"## Checking for missing values","b6751864":"## Train-Test Split","25bd60e5":"## Multi-Label Classification\n## Multiple Binary Classifications - (One Vs Rest Classifier)","45938dbc":"## Calculating number of comments having multiple labels","53fd8210":"## Adapted Algorithm","548d38e6":"## Calculating number of comments under each label","f0eb3cf8":"## Loading the library","73d8d77e":"### **Classifier Chains**","d142006d":" ## WordCloud representation of most used words in each category of comments","fce1e26b":"## Data Pre-Processing"}}