{"cell_type":{"609bdc98":"code","73235a43":"code","9d455234":"code","5ad3cd5f":"code","e2f9697e":"code","126b55c7":"code","63b93095":"code","32d39950":"code","abeef8b7":"code","1fc3e3be":"code","7047bc21":"code","f8cef431":"code","5889ced5":"code","8f45d943":"code","dc008659":"code","a79bf11d":"code","d760d9b8":"code","cde5ac67":"code","6c3c982c":"code","5dcbc193":"code","a81ab4d6":"code","39ebf027":"code","5af91aa6":"code","83c1acf1":"code","161f00fa":"code","b0892ca5":"code","e7aec96b":"code","b73b2c11":"code","bd62958c":"code","5353bd1e":"code","9f05e496":"code","62f6a897":"code","2c0ea118":"code","1de5ae1c":"code","fa338b19":"code","7978a71f":"code","05e0a92f":"code","8d7c73aa":"code","5ddb7130":"code","cb6a666d":"code","78ffff60":"code","0c33accd":"code","faacabb7":"code","c71fb922":"markdown","2bba92e0":"markdown","d7c20e92":"markdown","9910cf60":"markdown"},"source":{"609bdc98":"import os\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.merge import concatenate\n\nSEED = 2018\nimport tensorflow as tf\nnp.random.seed(SEED)\ntf.set_random_seed(SEED)\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport gc,re\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np","73235a43":"#do a clean up\n#del word_index, embeddings_index, all_embs, embedding_matrix, model\n#import gc; gc.collect()\n#time.sleep(10)","9d455234":"#read in the data\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","5ad3cd5f":"#lets look at the data\ntrain_df.head()","e2f9697e":"def clean_text(x):\n    x = str(x)\n    for punct in \"\/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019':\n        x = x.replace(punct, '')\n    return x\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispell_dict = {'colour':'color',\n                'centre':'center',\n                'didnt':'did not',\n                'doesnt':'does not',\n                'isnt':'is not',\n                'shouldnt':'should not',\n                'favourite':'favorite',\n                'travelling':'traveling',\n                'counselling':'counseling',\n                'theatre':'theater',\n                'cancelled':'canceled',\n                'labour':'labor',\n                'organisation':'organization',\n                'wwii':'world war 2',\n                'citicise':'criticize',\n                'instagram': 'social medium',\n                'whatsapp': 'social medium',\n                'snapchat': 'social medium'\n                }\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)","126b55c7":"# Clean the text\ntrain_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n\n# Clean numbers\ntrain_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n\n# Clean speelings\ntrain_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))","63b93095":"train_df.head()","32d39950":"## split to train and val\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n\n## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\n## fill up the missing values\ntrain_X = train_df[\"question_text\"].fillna(\"_na_\").values\nval_X = val_df[\"question_text\"].fillna(\"_na_\").values\ntest_X = test_df[\"question_text\"].fillna(\"_na_\").values","abeef8b7":"## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\n#produce a list of lists- each list is a integer representation of each word in the sentence\ntrain_X = tokenizer.texts_to_sequences(train_X)\n#print((train_X[:10]))\n\n\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values","1fc3e3be":"EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE,errors='ignore'))\n","7047bc21":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nprint(nb_words)\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","f8cef431":"train_X.shape","5889ced5":"#inp = Input(shape=(maxlen,))\n#x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n#x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n#x = GlobalMaxPool1D()(x)\n#x = Dense(16, activation=\"relu\")(x)\n#x = Dropout(0.1)(x)\n#x = Dense(1, activation=\"sigmoid\")(x)\n#model = Model(inputs=inp, outputs=x)\n#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n#print(model.summary())","8f45d943":"# define the model\ndef define_model(length=maxlen, vocab_size=max_features):\n    # channel 1\n    inputs1 = Input(shape=(length,))\n    embedding1 = Embedding(vocab_size, embed_size,weights=[embedding_matrix])(inputs1)\n    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n    drop1 = Dropout(0.5)(conv1)\n    pool1 = MaxPooling1D(pool_size=2)(drop1)\n    flat1 = Flatten()(pool1)\n    # channel 2\n    inputs2 = Input(shape=(length,))\n    embedding2 = Embedding(vocab_size, embed_size,weights=[embedding_matrix])(inputs2)\n    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n    drop2 = Dropout(0.5)(conv2)\n    pool2 = MaxPooling1D(pool_size=2)(drop2)\n    flat2 = Flatten()(pool2)\n    # channel 3\n    inputs3 = Input(shape=(length,))\n    embedding3 = Embedding(vocab_size, embed_size,weights=[embedding_matrix])(inputs3)\n    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n    drop3 = Dropout(0.5)(conv3)\n    pool3 = MaxPooling1D(pool_size=2)(drop3)\n    flat3 = Flatten()(pool3)\n    # merge\n    merged = concatenate([flat1, flat2, flat3])\n    # interpretation\n    dense1 = Dense(10, activation='relu')(merged)\n    outputs = Dense(1, activation='sigmoid')(dense1)\n    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n    # compile\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize\n    model.summary()\n    #plot_model(model, show_shapes=True, to_file='model.png')\n    return model","dc008659":"model = define_model(maxlen, max_features)","a79bf11d":"model.fit([train_X,train_X,train_X], train_y, batch_size=512, epochs=2,validation_data=([val_X,val_X,val_X], val_y),verbose=2)\n","d760d9b8":"#apply to validation set\npred_glove_val_y = model.predict([val_X,val_X,val_X], batch_size=512, verbose=1)","cde5ac67":"#look for a better threshold\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_glove_val_y>thresh).astype(int))))","6c3c982c":"pred_glove_test_y = model.predict([test_X,test_X,test_X], batch_size=512, verbose=1)","5dcbc193":"pred_glove_test_y","a81ab4d6":"#do a clean up\ndel word_index, embeddings_index, all_embs, embedding_matrix, model\nimport gc; gc.collect()\ntime.sleep(10)","39ebf027":"EMBEDDING_FILE = '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE,errors='ignore') if len(o)>100)\n","5af91aa6":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\nprint(all_embs.shape)","83c1acf1":"word_index = tokenizer.word_index\nprint(len(word_index))\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nprint(embedding_matrix.shape)","161f00fa":"for word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n#len(word_index.items())\n#print(len(embedding_vector))","b0892ca5":"# define the model\ndef define_model(length=maxlen, vocab_size=max_features):\n    # channel 1\n    inputs1 = Input(shape=(length,))\n    embedding1 = Embedding(vocab_size, embed_size,weights=[embedding_matrix])(inputs1)\n    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n    drop1 = Dropout(0.5)(conv1)\n    pool1 = MaxPooling1D(pool_size=2)(drop1)\n    flat1 = Flatten()(pool1)\n    # channel 2\n    inputs2 = Input(shape=(length,))\n    embedding2 = Embedding(vocab_size, embed_size,weights=[embedding_matrix])(inputs2)\n    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n    drop2 = Dropout(0.5)(conv2)\n    pool2 = MaxPooling1D(pool_size=2)(drop2)\n    flat2 = Flatten()(pool2)\n    # channel 3\n    inputs3 = Input(shape=(length,))\n    embedding3 = Embedding(vocab_size, embed_size,weights=[embedding_matrix])(inputs3)\n    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n    drop3 = Dropout(0.5)(conv3)\n    pool3 = MaxPooling1D(pool_size=2)(drop3)\n    flat3 = Flatten()(pool3)\n    # merge\n    merged = concatenate([flat1, flat2, flat3])\n    # interpretation\n    dense1 = Dense(10, activation='relu')(merged)\n    outputs = Dense(1, activation='sigmoid')(dense1)\n    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n    # compile\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize\n    model.summary()\n    #plot_model(model, show_shapes=True, to_file='model.png')\n    return model\nmodel = define_model(maxlen, max_features)","e7aec96b":"model.fit([train_X,train_X,train_X], train_y, batch_size=512, epochs=2,validation_data=([val_X,val_X,val_X], val_y),verbose=2)\n","b73b2c11":"pred_fasttext_val_y = model.predict([val_X,val_X,val_X], batch_size=128, verbose=1)","bd62958c":"for thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_fasttext_val_y>thresh).astype(int))))","5353bd1e":"pred_fasttext_test_y = model.predict([test_X,test_X,test_X], batch_size=512, verbose=1)","9f05e496":"#del  embeddings_index, all_embs, embedding_matrix, model, x\n#import gc; gc.collect()\n#time.sleep(10)","62f6a897":"EMBEDDING_FILE = '..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)","2c0ea118":"\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        ","1de5ae1c":"# define the model\ndef define_model(length=maxlen, vocab_size=max_features):\n    # channel 1\n    inputs1 = Input(shape=(length,))\n    embedding1 = Embedding(vocab_size, embed_size,weights=[embedding_matrix])(inputs1)\n    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n    drop1 = Dropout(0.5)(conv1)\n    pool1 = MaxPooling1D(pool_size=2)(drop1)\n    flat1 = Flatten()(pool1)\n    # channel 2\n    inputs2 = Input(shape=(length,))\n    embedding2 = Embedding(vocab_size, embed_size,weights=[embedding_matrix])(inputs2)\n    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n    drop2 = Dropout(0.5)(conv2)\n    pool2 = MaxPooling1D(pool_size=2)(drop2)\n    flat2 = Flatten()(pool2)\n    # channel 3\n    inputs3 = Input(shape=(length,))\n    embedding3 = Embedding(vocab_size, embed_size,weights=[embedding_matrix])(inputs3)\n    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n    drop3 = Dropout(0.5)(conv3)\n    pool3 = MaxPooling1D(pool_size=2)(drop3)\n    flat3 = Flatten()(pool3)\n    # merge\n    merged = concatenate([flat1, flat2, flat3])\n    # interpretation\n    dense1 = Dense(10, activation='relu')(merged)\n    outputs = Dense(1, activation='sigmoid')(dense1)\n    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n    # compile\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize\n    model.summary()\n    #plot_model(model, show_shapes=True, to_file='model.png')\n    return model\nmodel = define_model(maxlen, max_features)","fa338b19":"model.fit([train_X,train_X,train_X], train_y, batch_size=512, epochs=2,validation_data=([val_X,val_X,val_X], val_y),verbose=2)\n","7978a71f":"pred_paragram_val_y = model.predict([val_X,val_X,val_X], batch_size=1024, verbose=1)","05e0a92f":"for thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_paragram_val_y>thresh).astype(int))))","8d7c73aa":"pred_paragram_test_y = model.predict([test_X,test_X,test_X], batch_size=512, verbose=1)","5ddb7130":"#del word_index, embeddings_index, all_embs, embedding_matrix, model, inp, x\n#import gc; gc.collect()\n#time.sleep(10)","cb6a666d":"pred_val_y = 0.33*pred_glove_val_y + 0.33*pred_fasttext_val_y + 0.33*pred_paragram_val_y ","78ffff60":"for thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_val_y>thresh).astype(int))))","0c33accd":"pred_test_y = 0.33*pred_glove_test_y + 0.33*pred_fasttext_test_y + 0.33*pred_paragram_test_y\npred_test_y = (pred_test_y>0.36).astype(int)","faacabb7":"out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","c71fb922":"## Quroa Kaggle Challenge\n\nLatest Attempt at the Quora kaggle challenge.  See if older CNN n-gram can beat LSTM based solution\n\nFor each qid in the test set, you must predict whether the corresponding question_text is insincere (1) or not (0). Predictions should only be the integers 0 or 1.\n\n- https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification","2bba92e0":"**Paragram Embeddings:**\n\nIn this section, we can use the paragram embeddings and build the model and make predictions.","d7c20e92":"Next steps are as follows:\n * Split the training dataset into train and val sample - cross val too expensive\n * Fill up the missing values in the text column with '_na_'\n * Tokenize the text column and convert them to vector sequences\n * Pad the sequence as needed - if the number of words in the text is greater than 'max_len' trunacate them to 'max_len' or if the number of words in the text is lesser than 'max_len' add zeros for remaining values.","9910cf60":"**Wiki News FastText Embeddings:**\n\nNow let us use the FastText embeddings trained on Wiki News corpus in place of Glove embeddings and rebuild the model."}}