{"cell_type":{"90193e7d":"code","4a50980d":"code","b9939eed":"code","5c922119":"code","a7075527":"code","b09b24e3":"code","3e8aa634":"code","212e40cc":"code","6ce623ac":"code","7c2b7ca5":"code","e506b72b":"code","798af7f0":"code","482ebf34":"code","fe345882":"markdown","d36a65c4":"markdown","cf8fa466":"markdown","b488ab91":"markdown","85a9fc8a":"markdown","646a4b58":"markdown","c0247565":"markdown","25830075":"markdown","2dc10531":"markdown"},"source":{"90193e7d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle","4a50980d":"#Reading data set\ndf_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ndf_train.head()","b9939eed":"#Looking at distibution of labels to ensure that there is no imbalance\nsns.countplot(df_train['label'], palette=\"Set3\")","5c922119":"#Getting X and y\nX_train = df_train.iloc[:,1:].values\nY_train = df_train.iloc[:,0].values\nX_test = df_test.iloc[:,:].values\n\nprint(\"X train shape: \",X_train.shape)\nprint(\"Y train shape: \", Y_train.shape)\nprint(\"X test shape: \",X_test.shape)","a7075527":"#Transposing matrix X_train so that 1 column corresponds to all pixel values of a digit\nX_train_new = X_train.reshape(X_train.shape[0], -1).T\nprint(X_train_new.shape)\n\n#Output layer will have 10 units each corresponding to the digits\n#Modifying Y_train accordingly\ntemp = Y_train.reshape(len(Y_train), 1).T\nY_train_new = np.zeros((10,temp.shape[1]))\nfor i in range(temp.shape[1]):\n    Y_train_new[temp[0,i],i]=1 #The respective position in the column is marked 1\n\nprint(Y_train_new.shape)\nprint(\"New Y train: \", Y_train_new[:,0])","b09b24e3":"def sigmoid(X):\n    res = 1 \/ (1 + np.exp(-X))\n    return res\n\ndef initialize_parameters(input_layer, hidden_layer_1, hidden_layer_2, hidden_layer_3, output_layer):\n    #Weights and biases between input layer and hidden layer 1 \n    W1 = np.random.randn(hidden_layer_1, input_layer) * 0.01\n    b1 = np.zeros((hidden_layer_1, 1))\n\n    #Weights and biases between hidden layer 1 and hidden layer 2 \n    W2 = np.random.randn(hidden_layer_2, hidden_layer_1) * 0.01\n    b2 = np.zeros((hidden_layer_2, 1))\n\n    #Weights and biases between hidden layer 1 and hidden layer 2 \n    W3 = np.random.randn(hidden_layer_3, hidden_layer_2) * 0.01\n    b3 = np.zeros((hidden_layer_3, 1))\n\n    #Weights and biases between hidden layer 3 and output layer \n    W4 = np.random.randn(output_layer, hidden_layer_3) * 0.01\n    b4 = np.zeros((output_layer, 1))\n\n    Weights = {\"W1\" : W1, \"b1\" : b1, \"W2\" : W2, \"b2\" : b2, \"W3\" : W3, \"b3\" : b3, \"W4\" : W4, \"b4\" : b4}\n    return Weights\n\ndef Forward_prop(X, Weights):\n    #Extracting weights and biases\n\n    W1 = Weights[\"W1\"]\n    W2 = Weights[\"W2\"]\n    W3 = Weights[\"W3\"]\n    W4 = Weights[\"W4\"]\n\n    b1 = Weights[\"b1\"]\n    b2 = Weights[\"b2\"]\n    b3 = Weights[\"b3\"]\n    b4 = Weights[\"b4\"]\n\n    #Forward propagation - We will be using tanh as activation for hidden layer \n    #and sigmoid for output_layer\n    Z1 = np.matmul(W1, X) + b1\n    A1 = np.tanh(Z1)\n\n    Z2 = np.matmul(W2, A1) + b2\n    A2 = np.tanh(Z2)\n\n    Z3 = np.matmul(W3, A2) + b3\n    A3 = np.tanh(Z3)\n\n    Z4 = np.matmul(W4, A3) + b4\n    A4 = sigmoid(Z4) #Our Y_hat or predicted value\n\n    Units = {\"Z1\" : Z1, \"A1\" : A1, \"Z2\" : Z2, \"A2\" : A2, \"Z3\" : Z3, \"A3\" : A3, \"Z4\" : Z4, \"A4\" : A4}\n    return Units\n\n\ndef cost_compute(Units, y, m):\n    #Extracting Y_hat from Units\n    A4 = Units[\"A4\"]\n    #cross entropy loss formula\n    J = (np.sum((-y * np.log(A4)) - ((1 - y) * (np.log(1 - A4)))) \/ m)\n    return J\n\n\ndef Back_prop(Weights, Units, X, Y):\n    #Extracting Weights\n    W1 = Weights[\"W1\"]\n    W2 = Weights[\"W2\"]\n    W3 = Weights[\"W3\"]\n    W4 = Weights[\"W4\"]\n\n    #Extracting units\n    A1 = Units[\"A1\"]\n    A2 = Units[\"A2\"]\n    A3 = Units[\"A3\"]\n    A4 = Units[\"A4\"]\n\n    #Back propagation\n    dz4 = A4 - Y\n    dw4 = (1\/m) * np.matmul(dz4, A3.T)\n    db4 = (1\/m) * np.sum(dz4, axis = 1, keepdims = True)\n\n    dz3 = np.multiply(np.matmul(W4.T, dz4), 1 - np.power(A3,2)) # derivative of tanh(Z) = 1-A^2\n    dw3 = (1\/m) * np.matmul(dz3, A2.T)\n    db3 = (1\/m) * np.sum(dz3, axis = 1, keepdims = True)\n\n    dz2 = np.multiply(np.matmul(W3.T, dz3), 1 - np.power(A2,2))\n    dw2 = (1\/m) * np.matmul(dz2, A1.T)\n    db2 = (1\/m) * np.sum(dz2, axis = 1, keepdims = True)\n\n    dz1 = np.multiply(np.matmul(W2.T, dz2), 1 - np.power(A1,2))\n    dw1 = (1\/m) * np.matmul(dz1, X.T)\n    db1 = (1\/m) * np.sum(dz1, axis = 1, keepdims = True)\n\n    Grads = {\"dw1\" : dw1, \"db1\" : db1, \"dw2\" : dw2, \"db2\" : db2, \"dw3\" : dw3, \"db3\" : db3, \"dw4\" : dw4, \"db4\" : db4}\n    return Grads\n\ndef Update_Weights(Weights, Grads, alpha):\n    #Extracting weights and biases\n    W1 = Weights[\"W1\"]\n    W2 = Weights[\"W2\"]\n    W3 = Weights[\"W3\"]\n    W4 = Weights[\"W4\"]\n\n    b1 = Weights[\"b1\"]\n    b2 = Weights[\"b2\"]\n    b3 = Weights[\"b3\"]\n    b4 = Weights[\"b4\"]\n\n    #Extracting grads\n    dw1 = Grads[\"dw1\"]\n    dw2 = Grads[\"dw2\"]\n    dw3 = Grads[\"dw3\"]\n    dw4 = Grads[\"dw4\"]\n\n    db1 = Grads[\"db1\"]\n    db2 = Grads[\"db2\"]\n    db3 = Grads[\"db3\"]\n    db4 = Grads[\"db4\"]\n\n    #Updating weights and biases\n    W1 = W1 - alpha * dw1\n    W2 = W2 - alpha * dw2\n    W3 = W3 - alpha * dw3\n    W4 = W4 - alpha * dw4\n\n    b1 = b1 - alpha * db1\n    b2 = b2 - alpha * db2\n    b3 = b3 - alpha * db3\n    b4 = b4 - alpha * db4\n\n    Weights = {\"W1\" : W1, \"b1\" : b1, \"W2\" : W2, \"b2\" : b2, \"W3\" : W3, \"b3\" : b3, \"W4\" : W4, \"b4\" : b4}\n    return Weights","3e8aa634":"#ANN implementation - architecture\ninput_layer = X_train_new.shape[0] #784\nhidden_layer_1 = 180\nhidden_layer_2 = 120\nhidden_layer_3 = 120\noutput_layer = Y_train_new.shape[0] #10\n\n#epochs = 10000\nepochs = 10 #in order to save version it is set it to 10\nm = X_train_new.shape[1] #Length of training set\nalpha = 0.005 #Learning rate\n\n#Initializing weights\n#Weights = initialize_parameters(input_layer, hidden_layer_1, hidden_layer_2, hidden_layer_3, output_layer)\n\n#Loading saved weights my private dataset to continue training\nfile = open(\"..\/input\/weightsdigitrecognizer\/Weights.pickle\", \"rb\")\nWeights = pickle.load(file)\nfile.close()\n\n#Storing cost history for plotting purposes. J is cost\nJ_hist = []\n\n#Batch gradient descent\nfor i in range(epochs):\n    Units = Forward_prop(X_train_new, Weights)\n    J = cost_compute(Units, Y_train_new, m)\n    J_hist.append(J) \n    Grads = Back_prop(Weights, Units, X_train_new, Y_train_new)   \n    Weights = Update_Weights(Weights, Grads, alpha)\n    \n    if i % 500 == 0:\n        print(\"Cost after Iter :\", i, \"Cost :\", J)\n\n#Plotting cost vs epochs\nplt.plot(list(range(epochs)), J_hist, color = 'red')\nplt.title(\"Cost function vs epochs\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"J - cost function\")\nplt.show()","212e40cc":"#Saving weights - Download and then upload into private dataset to access it\nimport pickle\nfile = open(\".\/Weights.pickle\",\"wb\")\npickle.dump(Weights, file)\nfile.close()","6ce623ac":"#Preparing test set\nX_test_new = X_test.reshape(X_test.shape[0], -1).T\nprint(X_test_new.shape)","7c2b7ca5":"#Defining prediction function\ndef predict(X, Weights):\n    #Calling forward propagation and getting y_pred\n    finalUnits = Forward_prop(X, Weights)\n    y_pred = finalUnits[\"A4\"]\n\n    #Obtaining index (= digit) with highest value\n    Y_pred = np.argmax(y_pred, axis = 0)\n    return Y_pred","e506b72b":"#Predicting digits\nY_pred = predict(X_test_new, Weights)\nprint(Y_pred.shape)\nprint(Y_pred[:30])","798af7f0":"list1 = []\nfor i in list(df_test.index):\n    list1.append(i+1)\nprint(len(list1))    ","482ebf34":"submission = pd.DataFrame({\n    \"ImageId\": list1,\n    \"Label\": Y_pred\n})\nsubmission.to_csv(\"submission_digit_recognizer.csv\", index = False)","fe345882":"9 hour session only allows 10k epochs, so I am saving weights using pickle to reuse it. Total training: 30k epochs","d36a65c4":"# **Training**","cf8fa466":"# **Defining ANN model**","b488ab91":"There is no imbalance","85a9fc8a":"# Submission","646a4b58":"Functions needed for ANN:\n\n1. Initialize parameters - weights and biases\n2. Forward propagation\n3. Cost computation\n4. Backward propagation\n5. Updating parameters\n6. Activation function","c0247565":"# **Results**","25830075":"# **Preparing data**","2dc10531":"The test set has unusually large amount of data that could have been used for training the network. But we cannot alter it so we will proceed with this  "}}