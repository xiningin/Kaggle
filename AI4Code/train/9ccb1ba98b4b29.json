{"cell_type":{"e9ce7d07":"code","4fad157d":"code","7afe6350":"code","7381c5d4":"code","f22ac488":"code","066477dc":"code","4fd68b0d":"code","c47d866d":"code","3a8c112c":"code","f8a3d6c0":"code","48b65efd":"code","1fd638d8":"code","13f1aff1":"code","2020f6b8":"code","1d6295eb":"code","b1bff4f8":"code","beda4110":"code","723c48fa":"code","80c5a4af":"code","5dd03a9d":"code","e2a8f5bf":"code","eae5e10a":"code","676a84bd":"markdown","816a13f1":"markdown"},"source":{"e9ce7d07":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4fad157d":"!pip install pytorch-tabnet","7afe6350":"import numpy as np\nimport pandas as pd\n\n\npd.set_option(\"display.max_columns\", None)\n\ntrain_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nsub_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\n\ntrain_df.head()","7381c5d4":"train_df.drop(\"Id\", axis=1, inplace=True)\ntest_df.drop(\"Id\", axis=1, inplace=True)","f22ac488":"cols = [\"Soil_Type7\", \"Soil_Type15\"]\n\ntrain_df.drop(cols, axis=1, inplace=True)\ntest_df.drop(cols, axis=1, inplace=True)","066477dc":"idx = train_df[train_df[\"Cover_Type\"] == 5].index\ntrain_df.drop(idx, axis=0, inplace=True)","4fd68b0d":"new_names = {\n    \"Horizontal_Distance_To_Hydrology\": \"x_dist_hydrlgy\",\n    \"Vertical_Distance_To_Hydrology\": \"y_dist_hydrlgy\",\n    \"Horizontal_Distance_To_Roadways\": \"x_dist_rdwys\",\n    \"Horizontal_Distance_To_Fire_Points\": \"x_dist_firepts\"\n}\n\ntrain_df.rename(new_names, axis=1, inplace=True)\ntest_df.rename(new_names, axis=1, inplace=True)","c47d866d":"from sklearn.preprocessing import LabelEncoder\n\n\nencoder = LabelEncoder()\ntrain_df[\"Cover_Type\"] = encoder.fit_transform(train_df[\"Cover_Type\"])","3a8c112c":"train_df[\"Aspect\"][train_df[\"Aspect\"] < 0] += 360\ntrain_df[\"Aspect\"][train_df[\"Aspect\"] > 359] -= 360\n\ntest_df[\"Aspect\"][test_df[\"Aspect\"] < 0] += 360\ntest_df[\"Aspect\"][test_df[\"Aspect\"] > 359] -= 360","f8a3d6c0":"# Manhhattan distance to Hydrology\ntrain_df[\"mnhttn_dist_hydrlgy\"] = np.abs(train_df[\"x_dist_hydrlgy\"]) + np.abs(train_df[\"y_dist_hydrlgy\"])\ntest_df[\"mnhttn_dist_hydrlgy\"] = np.abs(test_df[\"x_dist_hydrlgy\"]) + np.abs(test_df[\"y_dist_hydrlgy\"])\n\n# Euclidean distance to Hydrology\ntrain_df[\"ecldn_dist_hydrlgy\"] = (train_df[\"x_dist_hydrlgy\"]**2 + train_df[\"y_dist_hydrlgy\"]**2)**0.5\ntest_df[\"ecldn_dist_hydrlgy\"] = (test_df[\"x_dist_hydrlgy\"]**2 + test_df[\"y_dist_hydrlgy\"]**2)**0.5","48b65efd":"soil_features = [x for x in train_df.columns if x.startswith(\"Soil_Type\")]\ntrain_df[\"soil_type_count\"] = train_df[soil_features].sum(axis=1)\ntest_df[\"soil_type_count\"] = test_df[soil_features].sum(axis=1)\n\nwilderness_features = [x for x in train_df.columns if x.startswith(\"Wilderness_Area\")]\ntrain_df[\"wilderness_area_count\"] = train_df[wilderness_features].sum(axis=1)\ntest_df[\"wilderness_area_count\"] = test_df[wilderness_features].sum(axis=1)","1fd638d8":"train_df.loc[train_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ntest_df.loc[test_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ntest_df.loc[test_df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ntest_df.loc[test_df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ntest_df.loc[test_df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\ntrain_df.loc[train_df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ntest_df.loc[test_df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\ntrain_df.loc[train_df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ntest_df.loc[test_df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","13f1aff1":"from sklearn.preprocessing import RobustScaler\n\n\ncols = [\n    \"Elevation\",\n    \"Aspect\",\n    \"mnhttn_dist_hydrlgy\",\n    \"ecldn_dist_hydrlgy\",\n    \"soil_type_count\",\n    \"wilderness_area_count\",\n    \"Slope\",\n    \"x_dist_hydrlgy\",\n    \"y_dist_hydrlgy\",\n    \"x_dist_rdwys\",\n    \"Hillshade_9am\",\n    \"Hillshade_Noon\",\n    \"Hillshade_3pm\",\n    \"x_dist_firepts\",\n    \"soil_type_count\",\n    \"wilderness_area_count\"\n]\n\nscaler = RobustScaler()\ntrain_df[cols] = scaler.fit_transform(train_df[cols])\ntest_df[cols] = scaler.transform(test_df[cols])","2020f6b8":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n \n    return df","1d6295eb":"df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","b1bff4f8":"df.head()","beda4110":"df.Cover_Type.unique()","723c48fa":"from sklearn.model_selection import train_test_split\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nx_train, x_valid, y_train, y_valid = train_test_split(df.drop(columns='Cover_Type'), df['Cover_Type'], test_size=0.3)\nx_train = x_train.values\ny_train = y_train.values\nx_valid = x_valid.values\ny_valid = y_valid.values\nclf = TabNetClassifier()\nclf.fit(\n    X_train=x_train, y_train=y_train,\n    eval_set=[(x_train, y_train), (x_valid, y_valid)],\n    eval_name=['train', 'valid'],\n    max_epochs=200, patience=10,\n    batch_size=16384, virtual_batch_size=256\n) \ny_test = clf.predict(test_df.values)","80c5a4af":"y_test","5dd03a9d":"y_final_sub = encoder.inverse_transform(y_test)","e2a8f5bf":"submission = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\n\nsubmission['Cover_Type'] = y_final_sub","eae5e10a":"submission.to_csv('pred_csv0.csv',index = False)","676a84bd":"### Train.py","816a13f1":"## FEATURE ENGINEERING TAKEN FROM GULSHAN MISHRA \n### https:\/\/www.kaggle.com\/gulshanmishra\/tps-dec-21-tensorflow-nn-feature-engineering#Part-2:-Feature-Engineering"}}