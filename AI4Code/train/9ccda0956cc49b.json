{"cell_type":{"ff20f6d4":"code","70c74f16":"code","9deb81f3":"code","2d08de08":"code","f657f7cd":"code","c642ad0e":"code","92da9d65":"code","d8fe771a":"code","b81df5a2":"code","3ea028f9":"code","8e07b288":"markdown","6ecfeec1":"markdown","5d6c5262":"markdown","df074217":"markdown","aeef0987":"markdown","5baed3a4":"markdown","2db82940":"markdown"},"source":{"ff20f6d4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","70c74f16":"import random\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor\nfrom sklearn.model_selection import KFold\nfrom catboost import CatBoostRegressor\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.neural_network import MLPRegressor\nfrom catboost import CatBoost, Pool\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.preprocessing import RobustScaler,PowerTransformer,QuantileTransformer,Normalizer,MinMaxScaler\nimport matplotlib.pyplot as plt\n%matplotlib inline","9deb81f3":"train = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest  = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')\nsoumaya_sub = pd.read_csv('..\/input\/soumaya-sub\/submission.csv')","2d08de08":"columns = [col for col in train.columns.to_list() if col not in ['id','target']]","f657f7cd":"param1={'tree_method':'gpu_hist','lambda': 0.0045523892919572, 'alpha': 0.005803668702291496,\n            'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.007, 'max_depth': 20,\n            'random_state': 24, 'min_child_weight': 264,'n_estimators': 8200}","c642ad0e":"preds1 = np.zeros(test.shape[0])\nkf = KFold(n_splits=12,random_state=48,shuffle=True)\nrmse=[]  ## list contains rmse for each fold\nn=0\nfor trn_idx, test_idx in kf.split(train[columns],train['target']):\n    X_tr,X_val=train[columns].iloc[trn_idx],train[columns].iloc[test_idx]\n    y_tr,y_val=train['target'].iloc[trn_idx],train['target'].iloc[test_idx]\n    model = xgb.XGBRegressor(**param1)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    preds1+=model.predict(test[columns])\/kf.n_splits\n    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\n    print(n+1,rmse[n])\n    n+=1  ","92da9d65":"param2={'metric': 'rmse','lambda_l1': 6.177646935770319, 'lambda_l2': 8.267630169831212e-05, \n            'min_sum_hessian_in_leaf': 0.06121961712341018, 'min_data_in_leaf': 17, 'num_leaves': 256,'max_bin':456,\n            'feature_fraction': 0.4808176760171774, 'bagging_fraction': 0.8799188937155665, 'learning_rate': 0.005, \n            'bagging_freq': 1, 'n_estimators': 15898, 'max_depth': 120, 'min_data_per_group': 53, 'random_state': 100, 'cat_smooth': 23}","d8fe771a":"preds2 = np.zeros(test.shape[0])      \nkf = KFold(n_splits=12,random_state=48,shuffle=True)   \nrmse=[]     ## list contains rmse for each fold\nn=0\nfor trn_idx, test_idx in kf.split(train[columns],train['target']): \n    X_tr,X_val=train[columns].iloc[trn_idx],train[columns].iloc[test_idx]\n    y_tr,y_val=train['target'].iloc[trn_idx],train['target'].iloc[test_idx] \n    lgb = LGBMRegressor(**param2)\n    lgb.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=200,verbose=False)\n    preds2+=lgb.predict(test[columns])\/kf.n_splits\n    rmse.append(mean_squared_error(y_val, lgb.predict(X_val), squared=False))\n    print(n+1,rmse[n])    \n    n+=1    ","b81df5a2":"sub=0.18*preds1+0.12*preds2+0.7*soumaya_sub['target']","3ea028f9":"sub.to_csv('submission.csv', index=False)","8e07b288":"## Notes :\n* I use ensemble of xgb and lgb (0.4xgb+0.6lgb) which gives me : LB result =  0.69686 and Private = 0.69565 !!! \ud83e\udd37\u200d\n* the LB score improoved \ud83d\ude00 but the private nope \ud83d\ude15\n* I used also the output of [Somayyeh](https:\/\/www.kaggle.com\/somayyehgholami) public kernel [LB result = 0.69652 and Private = 0.69529] and make ensemble with my work. [kernel of Somayyeh](https:\/\/www.kaggle.com\/somayyehgholami\/results-driven-tabular-playground-series-201).\n* So my final submission was : 0.18 * lgb_output + 0.12 * xgb_output + 0.7 * soumayyeh_submission ==> \nLB result =  0.69648 and Private = 0.69525 \ud83c\udfc4\ud83c\udffe\u200d\u2642\ufe0f","6ecfeec1":"# Submission","5d6c5262":"## lgb model","df074217":"### Xgb results : \n* CV = 0.69537\n* LB result = 0.69712\n* Private = 0.69608","aeef0987":"##  This result ranked me 11\/1728 in this competition \ud83c\udfc4\ud83c\udffd\u200d\u2642\ufe0f\n## I shouldn't forget to thank [Somayyeh](https:\/\/www.kaggle.com\/somayyehgholami) \ud83d\udc4f\ud83d\udc4f\ud83d\udc4f","5baed3a4":"### Xgb results : \n* CV = 0.69494\n* LB result = 0.69696\n* Private = 0.69564","2db82940":"## Xgboost model"}}