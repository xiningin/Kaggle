{"cell_type":{"5ded38fa":"code","0177a026":"code","80714516":"code","8b4b4908":"code","65d24af4":"code","6ffbc18a":"code","9d801e13":"code","3bd0d3ce":"code","1dd28718":"code","c1bcf536":"code","3fe2661a":"code","d7f39cb6":"code","7b7d9b80":"code","2d626805":"code","dfe8b3ec":"code","0ff6b361":"code","6056981f":"code","8152d589":"code","e4569a09":"code","03006b44":"code","ea6a4345":"code","48dbd930":"code","c06f98c0":"code","b89ec124":"code","6443f045":"code","13a44f6d":"code","0e5f7d3f":"code","8cc72538":"code","cc6b996a":"markdown","977d45d7":"markdown","eff5f11e":"markdown","f4209ff2":"markdown","b7229816":"markdown","b7bd299c":"markdown","4190c3a1":"markdown","5b12865c":"markdown","6036fe33":"markdown","0e036bdf":"markdown","5e727735":"markdown","083213e6":"markdown","4b7cd068":"markdown"},"source":{"5ded38fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport os\nimport re\nimport gc \nfrom tqdm import tqdm\nfrom datetime import date     #calculating age\nfrom datetime import datetime #converting string to date\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import GridSearchCV , train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score , f1_score , make_scorer\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder , LabelEncoder ,normalize\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# Any results you write to the current directory are saved as output.","0177a026":"df1 = pd.read_csv(\"..\/input\/train.csv\")\ndf2 = pd.read_csv(\"..\/input\/test_bqCt9Pv.csv\")\n#df1 = dff.drop('loan_default' , axis=1)\n#df = pd.concat([df1,df2], axis=0 , sort=True)","80714516":"df1.head()","8b4b4908":"#df.drop(['UniqueID'] , inplace=True , axis=1)","65d24af4":"df1.describe()","6ffbc18a":"df1.isnull().sum()","9d801e13":"df1['Employment.Type'].value_counts()","3bd0d3ce":"df1 = df1.fillna(df1.mode().iloc[0])\ndf2 = df2.fillna(df2.mode().iloc[0])","1dd28718":"df1.shape","c1bcf536":"#Plotting Histogram    \ndf1.hist(bins=50 , figsize=(20,20))\nplt.show()","3fe2661a":"# Checking on Categorical data\nfor col in tqdm(df1.columns):\n    if df1[col].dtype == 'object':\n        print(col , \":\", df1[col].nunique())","d7f39cb6":"# CODE FOR AGE CONVERSION: https:\/\/www.geeksforgeeks.org\/python-program-to-calculate-age-in-year\/\ndef calcAge(born):\n    born = datetime.strptime(born , '%d-%m-%y')\n    today= date.today()\n    age = today.year- born.year - ((today.month,today.day) < (born.month,born.day))\n    return age","7b7d9b80":"#1\ndf1.drop(['DisbursalDate' , 'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH'], inplace=True , axis=1)\ndf2.drop(['DisbursalDate' , 'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH'], inplace=True , axis=1)\n#2\ndf1['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace=['Not Scored: More than 50 active Accounts found', \n                                                         'Not Scored: No Activity seen on the customer (Inactive)',\n                                                         'Not Scored: No Updates available in last 36 months',\n                                                         'Not Enough Info available on the customer','Not Scored: Only a Guarantor',\n                                                         'Not Scored: Sufficient History Not Available',\n                                                         'Not Scored: Not Enough Info available on the customer'], \n                                                   value= 'Not Scored', inplace = True)\ndf2['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace=['Not Scored: More than 50 active Accounts found', \n                                                         'Not Scored: No Activity seen on the customer (Inactive)',\n                                                         'Not Scored: No Updates available in last 36 months',\n                                                         'Not Enough Info available on the customer','Not Scored: Only a Guarantor',\n                                                         'Not Scored: Sufficient History Not Available',\n                                                         'Not Scored: Not Enough Info available on the customer'], \n                                                   value= 'Not Scored', inplace = True)\n\n#3 \ndob = df1['Date.of.Birth']\ndf1['Age'] = dob.map(calcAge)\ndf1.drop(['Date.of.Birth'] , axis=1 , inplace=True)\n\ndob = df2['Date.of.Birth']\ndf2['Age'] = dob.map(calcAge)\ndf2.drop(['Date.of.Birth'] , axis=1 , inplace=True)","2d626805":"df1.shape , df2.shape","dfe8b3ec":"def removeOutlier(df, cols):\n    indexes=[]\n    for col in tqdm(cols):\n        if (df[col].dtypes !='object'):\n            Q1 = df[col].quantile(q=0.001)\n            Q3 = df[col].quantile(q=0.999)        \n            for i in (df.index):\n                if ((df.loc[i,col]< Q1\/5) or (df.loc[i,col] > 5*Q3)):\n                    df = df.drop(index=i)\n                    indexes.append(i)\n    return df, indexes","0ff6b361":"cols_with_outliers=['disbursed_amount', 'asset_cost', 'PRI.NO.OF.ACCTS', 'PRI.ACTIVE.ACCTS','PRI.OVERDUE.ACCTS','PRI.CURRENT.BALANCE', 'PRI.SANCTIONED.AMOUNT','PRI.SANCTIONED.AMOUNT',\n       'PRI.DISBURSED.AMOUNT', 'SEC.NO.OF.ACCTS', 'SEC.ACTIVE.ACCTS','SEC.CURRENT.BALANCE', 'SEC.SANCTIONED.AMOUNT',\n       'SEC.DISBURSED.AMOUNT', 'PRIMARY.INSTAL.AMT', 'SEC.INSTAL.AMT',\n       'NEW.ACCTS.IN.LAST.SIX.MONTHS', 'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS', 'NO.OF_INQUIRIES','Employment.Type', 'PERFORM_CNS.SCORE.DESCRIPTION']\n\nnew_df1, indexex = removeOutlier(df1 , cols_with_outliers)\nnew_df2, indexex2 = removeOutlier(df2 , cols_with_outliers)","6056981f":"new_df1.shape,new_df2.shape","8152d589":"cat_data= df1.select_dtypes(include='object').columns\n#6\ndf = pd.get_dummies(new_df1[cat_data])\ndff = pd.get_dummies(new_df2[cat_data])\nprint(\"The shape of dummy variables : \" , df.shape)\nprint(\"The shape of dummy variables : \" , dff.shape)","e4569a09":"num_data = list(new_df1._get_numeric_data().columns)\nnum_data.remove('loan_default')\nscaler = StandardScaler()\nscaler.fit(new_df1[num_data])\nnormalized = scaler.transform(new_df1[num_data])\nnormalized2 = scaler.transform(new_df2[num_data])\nnormalized = pd.DataFrame(normalized , columns=num_data)\nnormalized2 = pd.DataFrame(normalized2 , columns=num_data)\nprint(\"The shape of normalised numerical data : \" , normalized.shape)\nprint(\"The shape of normalised numerical data : \" , normalized2.shape)","03006b44":"final_df = pd.concat([normalized , df], sort=True , axis=1)\ntest_df = pd.concat([normalized2 , dff], sort=True , axis=1)\nfinal_df= final_df.dropna(axis=0)\ntest_df = test_df.dropna(axis=0)\nprint(\"The shape of the final data:\" , final_df.shape)\nprint(\"The shape of the final data:\" , test_df.shape)","ea6a4345":"# Finding Correlation between features\ndef correlation(df ,column):\n    column = column.iloc[df.index]\n    dff= df.join(column)\n    corr_max = dff.corr()  #create correlation matrix\n    top_15 = corr_max.nlargest(20 , 'loan_default')['loan_default'].index # select top 15 correlate features\n    corr = np.corrcoef(dff[top_15].values.T)    \n    return corr , top_15\n\n#Plot correlation map\ndef plot_heatmap(corr,top_15):\n    plt.figure(figsize=(15,10))\n    sns.heatmap(corr, cbar=True , annot=True , fmt='.2f', yticklabels=top_15.values , xticklabels=top_15.values)\n    plt.title('CORRELATION MATRIX')\n    plt.show()\n    ","48dbd930":"corr , top_15 = correlation(final_df, df1['loan_default'])\nprint(\"The top 15 feature correlated to target variable\", top_15)","c06f98c0":"# def LogReg(X, y):\n#     kwargs ={\n#         'class_weight':None,\n#         'fit_intercept':True,\n#         'max_iter':1000\n#     }\n    \n#     param_grid =[{ 'C': np.random.rand(5) , \n#                  'solver': ['liblinear'],\n#                  'penalty':['l1']\n#                  },\n#                 { 'C': np.random.rand(5) , \n#                  'solver': ['lbfgs'],\n#                  'penalty':['l2']\n#                  }]\n#     scoring = {'accuracy': make_scorer(accuracy_score) , \n#                'f1': make_scorer(f1_score)}\n    \n#     X_train , X_test , y_train , y_test = train_test_split(X,y, test_size=0.3 , random_state=0)\n#     lr = LogisticRegression(**kwargs)\n#     model =GridSearchCV(lr , param_grid=param_grid , scoring=scoring , cv=5 , refit=False , verbose=10)\n#     model.fit(X_train,y_train)\n#     print(model.best_params_)\n#     print(model.score(X_test , y_test))","b89ec124":"X = final_df\ny = new_df1['loan_default'].iloc[final_df.index]\nX_train , X_test , y_train , y_test = train_test_split(X,y, test_size=0.3 , random_state=0)\nc= [0.001 ,0.0001,0.01,1,10]","6443f045":"scores=[]\nfor i in (c):\n    lr = LogisticRegression(penalty='l1' , solver='liblinear' , max_iter=1000 , C=i)\n    top15= list(top_15)\n    top15.remove('loan_default')\n    lr.fit(X_train[top15],y_train)\n    scores.append(lr.score(X_test[top15],y_test))\nprint(\"The score for C= {} is {}\".format(scores.index(max(scores)),max(scores)))","13a44f6d":"for i in (c):\n    lr = LogisticRegression(penalty='l2' , solver='lbfgs' , max_iter=1000 , C=i)\n    top15= list(top_15)\n    top15.remove('loan_default')\n    lr.fit(X_train[top15],y_train)\n    scores.append(lr.score(X_test[top15],y_test))\nprint(\"The score for C= {} is {}\".format(scores.index(max(scores)),max(scores)))","0e5f7d3f":"model = LogisticRegression(penalty='l2' , solver='lbfgs' , C=1 ,max_iter=1000 )\nmodel.fit(X,y)\npredictions = model.predict(test_df)","8cc72538":"predictions","cc6b996a":"# 1. Reading Dataset","977d45d7":"# Feature Selection","eff5f11e":"### Standardising the data","f4209ff2":"## **Dummy Variables**\n>  Creating Dummy Variables","b7229816":"### Helper Functions\n\n> We will perform the following things:\n\n* As we can see the categorical feature like **CREDIT.HISTORY.LENGTH , AVEGRAGE.ACC.AGE , DISBURSALDATE** have a large number of categories. We decide to drop them from our raw dataset. \n\n* The feature **PERFORM_CNS.SCORE.DESCRIPTION** can be reduced by following command.\n\n* Convert DOB of customers into their Age\n\n* [](http:\/\/)Converting Categorical Data into OneHot encoded features","b7bd299c":"> As we can see there are lot of Outliers in our data which we will remove using ***INTERQUARTILE RANGE(IQR)*** \n\n* See here the code snippet from <https:\/\/www.kaggle.com\/matthieu64\/car-loan-default-prediction\/notebook#Feature-Engineering>","4190c3a1":"> **Only One column has Null VALUES : EMPLOYMENT TYPES**","5b12865c":"# **Model**","6036fe33":"### Standardising the data","0e036bdf":"## **The final data**\n> THE FINAL DATA AFTER ALL THE PREPROCESSING","5e727735":"# 2. Data Analysis","083213e6":"# 3. Data Preprocessing","4b7cd068":"> Filling NaN values with most frequent value."}}