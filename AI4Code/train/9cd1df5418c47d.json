{"cell_type":{"dde0e7d3":"code","be4fda3a":"code","1f914c0b":"code","83f31a3b":"code","c7723d7b":"code","944a84c7":"code","b8f3458a":"code","c5c753d1":"code","5de01131":"code","c10cbb02":"code","6af6d7ca":"code","889a74d8":"code","2cd0fa43":"markdown","d614e82e":"markdown","b3aa72f6":"markdown"},"source":{"dde0e7d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","be4fda3a":"import tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom sklearn.model_selection import train_test_split","1f914c0b":"df = pd.read_json('\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json',lines=True)","83f31a3b":"df.head()","c7723d7b":"df = df.drop(columns=['article_link'])\nX = df['headline']\ny = df['is_sarcastic']","944a84c7":"x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)","b8f3458a":"embedding = \"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\"\nhub_layer = hub.KerasLayer(embedding, input_shape=[], \n                           dtype=tf.string, trainable=True)","c5c753d1":"model = tf.keras.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n\nmodel.summary()","5de01131":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","c10cbb02":"earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',patience=0,verbose=1,)","6af6d7ca":"history = model.fit(x_train,y_train,batch_size=256,\n                    epochs=10,\n                    validation_data=(x_test,y_test),\n                    verbose=1,callbacks=[earlystop])","889a74d8":"model_eval = model.evaluate(x_test,y_test)\nprint('Validation_loss: '+str(model_eval[0]))\nprint('Accuracy: '+str(model_eval[1]))","2cd0fa43":"# Model Evaluation","d614e82e":"# Model","b3aa72f6":"tensorflow_hub is a library to import trained models from TFHub. So instead of performing tokenization and then using an embedding layer we can use a pretrained model. For this problem we will use 'universal-sentence-encoder'."}}