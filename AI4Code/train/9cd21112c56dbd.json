{"cell_type":{"d452ed01":"code","1459015a":"code","913fd18a":"code","f46651c9":"code","06a60917":"code","f79cc321":"code","afa5b699":"code","06ce6b4b":"code","f24d09cc":"code","7aacfe87":"code","edacdc9a":"code","246b76e3":"code","968cd4e1":"code","d9d67b21":"code","2e00560e":"code","bd384186":"code","f0d65ba8":"code","2ff5593d":"code","24ceda91":"markdown","27d6c1bd":"markdown","57387342":"markdown","b2146bd2":"markdown","0197f653":"markdown","659e6360":"markdown"},"source":{"d452ed01":"!pip install transformers","1459015a":"import tensorflow as tf\nimport keras\nimport transformers \nfrom transformers import * \nimport pandas as pd\nimport numpy as np\nimport tokenizers","913fd18a":"print(tf.__version__)","f46651c9":"vocab_file = '..\/input\/tf-roberta\/vocab-roberta-base.json'\nmerge_file = '..\/input\/tf-roberta\/merges-roberta-base.txt'\ntokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file, merge_file,lowercase = True)","06a60917":"MAX_LEN = 100\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\ntrain.head()","f79cc321":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        \n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","afa5b699":"test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","06ce6b4b":"ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\natt = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\ntok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\nconfig = RobertaConfig.from_pretrained('..\/input\/tf-roberta\/config-roberta-base.json')\nbert_model = TFRobertaModel.from_pretrained('..\/input\/tf-roberta\/pretrained-roberta-base.h5',config=config)\nx = bert_model(ids,attention_mask=att,token_type_ids=tok)\ndrop1 = tf.keras.layers.Dropout(0.2)(x[0])\nlayer2 = tf.keras.layers.Conv1D(50 ,kernel_size = 1)(drop1)\ndrop2 = tf.keras.layers.Dropout(0.2)(layer2)\nlayer3 = tf.keras.layers.Conv1D(1 ,kernel_size = 1)(drop2)\nlayer4 = tf.keras.layers.Flatten()(layer3)\noutput_1 = tf.keras.layers.Activation('softmax')(layer4)\n\ndrop1_ = tf.keras.layers.Dropout(0.2)(x[0])\nlayer1_ = tf.keras.layers.Conv1D(50 ,kernel_size = 1)(drop1_)\ndrop2_ = tf.keras.layers.Dropout(0.2)(layer1_)\nlayer2_ = tf.keras.layers.Conv1D(1 ,kernel_size = 1)(drop2_)\nlayer3_ = tf.keras.layers.Flatten()(layer2_)\noutput_2 = tf.keras.layers.Activation('softmax')(layer3_)\nmodel = tf.keras.Model(inputs = [ids ,att ,tok] ,outputs = [output_1 ,output_2])\nmodel.summary()","f24d09cc":"def my_loss(alpha ,gamma):\n    '''defining focal loss with gamma and alpha parameters'''\n    def focal_loss(y_true ,y_pred):\n        y_true = tf.cast(y_true ,dtype = tf.float32)\n        y_pred = tf.cast(y_pred ,dtype = tf.float32)\n        log_lik = y_true*tf.keras.backend.log(y_pred)\n        log_lik = alpha*((1-y_pred)**gamma)*log_lik\n        return -tf.keras.backend.sum(log_lik)\n    return focal_loss","7aacfe87":"optimizer = tf.keras.optimizers.Adam(learning_rate=0.000001)\nmodel.compile(loss = my_loss(1.0 ,2.0) ,optimizer = optimizer)","edacdc9a":"history = model.fit([input_ids[800:], attention_mask[800:],token_type_ids[800:]], [start_tokens[800:], end_tokens[800:]] ,epochs = 30 ,\n         validation_split = 0.1 ,batch_size = 32)","246b76e3":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","968cd4e1":"oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\noof_start,oof_end = model.predict([input_ids[:800],attention_mask[:800],token_type_ids[:800]],verbose=1)","d9d67b21":"all = []\njac = []\nfor k in range(800):\n    a = np.argmax(oof_start[k,])\n    b = np.argmax(oof_end[k,])\n    if a>b: \n        st = train.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n    else:\n        text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(jaccard(st,train.loc[k,'selected_text']))\njac.append(np.mean(all))","2e00560e":"print(np.mean(jac))","bd384186":"preds = model.predict([input_ids_t ,attention_mask_t ,token_type_ids_t] ,verbose = 1)\npreds_start = preds[0]\npreds_end = preds[1]","f0d65ba8":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax([preds_start[k ,]])\n    b = np.argmax(preds_end[k ,])\n    if a>b:\n        st = test.loc[k ,'text']\n    else:\n        text1 = \" \" + \" \".join(test.loc[k ,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)","2ff5593d":"test['selected_text'] = all\ntest[['textID' ,'selected_text']].to_csv('submission.csv' ,index = False)\npd.set_option('max_colwidth' ,60)\ntest.sample(25)","24ceda91":"# Importing dependencies","27d6c1bd":"# Building the model","57387342":"### Defining metric\n","b2146bd2":"# Tokenization <br\/>\nThe tokenization logic was inpired from Abhishek Takur","0197f653":"# Printing tf version\n","659e6360":"# Kaggle submission\n"}}