{"cell_type":{"5efb6282":"code","c6dff035":"code","dd984d33":"code","20ef11a4":"code","d0184ed8":"code","56d0f487":"code","f4394d0a":"code","13f95055":"code","5e3f7a82":"code","d25b153a":"code","425c06ad":"code","003b19bf":"code","4425cf29":"code","fd63359d":"code","1066e71d":"code","4642bf6b":"code","5116e9a4":"code","7be4726c":"code","29c33615":"code","78a6f5ae":"code","70751e96":"code","4983fc9c":"code","408f10bb":"code","3fb06a1a":"code","d8195b86":"code","e3d68821":"markdown","230a5e3b":"markdown"},"source":{"5efb6282":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c6dff035":"!ls","dd984d33":"import sklearn.datasets\nimport sklearn.model_selection\nimport keras.preprocessing.image\nimport keras.utils\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom skimage import color\nfrom sklearn.metrics import accuracy_score\nimport keras.callbacks\nimport os\nimport numpy as np\nimport cv2\n\n#def load_data(infDir):\n#    infData=sklearn.datasets.load_files(infDir,load_content=False)\n#    y_inf = np.array(infData['target'])\n#    y_inf_names = np.array(infData['target_names'])\n#    nclasses = len(np.unique(y_inf))\n#    target_size=50\n#    x_inf=[]\n#    for filename in infData['filenames']:\n#        x_inf.append(\n#                keras.preprocessing.image.img_to_array(\n#                        keras.preprocessing.image.load_img(filename,target_size=(target_size, target_size))\n#                )\n#        )\n#    return([x_inf,y_inf])\n    \n    \n\ntrain_dir = '..\/input\/fruits-360_dataset\/fruits-360\/Training'\ntrainData=sklearn.datasets.load_files(train_dir,load_content=False)\n\ntest_dir = '..\/input\/fruits-360_dataset\/fruits-360\/Test'\ntestData=sklearn.datasets.load_files(test_dir,load_content=False)\n\n\ny_train = np.array(trainData['target'])\ny_train_names = np.array(trainData['target_names'])\n\ny_test = np.array(testData['target'])\ny_test_names = np.array(testData['target_names'])\n\nnclasses = len(np.unique(y_train))\ntarget_size=50\n\nx_train=[]\nfor filename in trainData['filenames']:\n    x_train.append(\n            keras.preprocessing.image.img_to_array(\n                    keras.preprocessing.image.load_img(filename,target_size=(target_size, target_size))\n                    )\n            )\n    \n    \nx_test=[]\nfor filename in testData['filenames']:\n    x_test.append(\n            keras.preprocessing.image.img_to_array(\n                    keras.preprocessing.image.load_img(filename,target_size=(target_size, target_size))\n                    )\n            )","20ef11a4":"x_train=np.array(x_train)\nx_train=x_train\/255\ny_train=keras.utils.np_utils.to_categorical(y_train,nclasses)\n\n\nx_test=np.array(x_test)\nx_test=x_test\/255\ny_test=keras.utils.np_utils.to_categorical(y_test,nclasses)","d0184ed8":"x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(\n        x_train, y_train, test_size=0.2\n)\nprint(y_train.shape)\nprint(y_val.shape)","56d0f487":"model = keras.models.Sequential()\n#model.add(keras.layers.Conv2D(filters = 3, kernel_size = 1, input_shape=x_train.shape[1:],activation='tanh'))\n#model.add(keras.layers.Conv2D(filters = 1, kernel_size = 1, padding='same' ,activation='sigmoid'))\n\nmodel.add(keras.layers.Conv2D(filters = 4, kernel_size = (3,3), activation='relu',input_shape=(x_train.shape[1:]), name=\"conv_1\"))\nmodel.add(keras.layers.MaxPooling2D((2,2)))\nmodel.add(keras.layers.Conv2D(filters = 8, kernel_size = (3,3), activation='relu', name=\"conv_2\"))\nmodel.add(keras.layers.MaxPooling2D((2,2)))\nmodel.add(keras.layers.Conv2D(filters = 16, kernel_size = (3,3), activation='relu', name=\"conv_3\"))\n#model.add(keras.layers.Flatten())\nmodel.add(keras.layers.pooling.GlobalAveragePooling2D(name=\"avg_1\"))\nmodel.add(keras.layers.Dense(nclasses,activation = 'softmax', name='output'))\nmodel.summary()","f4394d0a":"from IPython.display import SVG\nimport IPython\nfrom keras.utils import model_to_dot\n\nprint(model.summary())\n\nkeras.utils.plot_model(model, to_file='test_keras_plot_model.png', show_shapes=True)\nIPython.display.Image('test_keras_plot_model.png')\n","13f95055":"model.compile(loss='categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['accuracy'])\ncheckpointer = keras.callbacks.ModelCheckpoint(filepath = 'cnn_from_scratch_fruits.hdf5', verbose = 1, save_best_only = True)\nearlystopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None, restore_best_weights=False)","5e3f7a82":"history=model.fit(x_train, y_train, batch_size=64, epochs=100,validation_data=(x_val, y_val), callbacks = [checkpointer], shuffle=True)","d25b153a":"model.load_weights('cnn_from_scratch_fruits.hdf5')","425c06ad":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","003b19bf":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","4425cf29":"import keras.backend as K","fd63359d":"def deprocess_image(x):\n    x -= x.mean()\n    x \/= (x.std() + 1e-5)\n    x *= 0.1\n    # clip to [0, 1]\n    x += 0.5\n    x = np.clip(x, 0, 1)\n    # convert to RGB array\n    x *= 255\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x\n    \n    \ndef generate_pattern(layer_name, filter_index, size=target_size):\n    \n    layer_output = model.get_layer(layer_name).output\n    loss = K.mean(layer_output[:, :, :, filter_index])\n    grads = K.gradients(loss, model.input)[0]\n    grads \/= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n    iterate = K.function([model.input], [loss, grads])\n    \n    input_img_data = np.random.random((1, target_size, target_size, 3)) * 20 + 128.\n    \n    #input_img_data = np.zeros((1, target_size, target_size, 3)) * 20 + 128.\n    \n    \n    step = 1.\n    for i in range(80):\n        loss_value, grads_value = iterate([input_img_data])\n        input_img_data += grads_value * step\n        \n    img = input_img_data[0]\n    return deprocess_image(img)","1066e71d":"def show_patterns(layer_name):\n    fig = plt.figure(figsize=(50, 50))\n    for img in range(model.get_layer(layer_name).filters):\n        to_show=generate_pattern(layer_name, img)\n        ax = fig.add_subplot(5, 6, img+1)\n        ax = plt.imshow(to_show)\n        plt.xticks([])\n        plt.yticks([])\n        fig.subplots_adjust(wspace=0.05, hspace=0.05) \n    ","4642bf6b":"show_patterns('conv_1')","5116e9a4":"show_patterns('conv_2')","7be4726c":"show_patterns('conv_3')","29c33615":"test_image = keras.preprocessing.image.img_to_array(keras.preprocessing.image.load_img(train_dir+\"\/Apple Braeburn\/0_100.jpg\",target_size=(target_size, target_size)))\ntest_image = test_image\/255\n\nplt.imshow(test_image)","78a6f5ae":"fig = plt.figure(figsize=(8, 8))\nfor img in range(3):\n    ax = fig.add_subplot(1, 3, img+1)\n    ax = plt.imshow(test_image[:, :, img],cmap='gray')\n    plt.xticks([])\n    plt.yticks([])\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)\n\n\ntest_image = np.expand_dims(test_image, axis=0)","70751e96":"hidden_rappresenter = keras.models.Model(inputs=model.input, outputs=model.get_layer('conv_1').output)\nresult=hidden_rappresenter.predict(test_image)\nresult.shape\n\nfig = plt.figure(figsize=(8, 8))\nfor img in range(4):\n    ax = fig.add_subplot(1, 4, img+1)\n    ax = plt.imshow(result[0, :, :, img], cmap='gray')\n    plt.xticks([])\n    plt.yticks([])\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)","4983fc9c":"hidden_rappresenter = keras.models.Model(inputs=model.input, outputs=model.get_layer('conv_2').output)\nresult=hidden_rappresenter.predict(test_image)\nresult.shape\n\nfig = plt.figure(figsize=(8, 8))\nfor img in range(8):\n    ax = fig.add_subplot(2, 4, img+1)\n    ax = plt.imshow(result[0, :, :, img], cmap='gray')\n    plt.xticks([])\n    plt.yticks([])\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)","408f10bb":"hidden_rappresenter = keras.models.Model(inputs=model.input, outputs=model.get_layer('conv_3').output)\nresult=hidden_rappresenter.predict(test_image)\nresult.shape\n\nfig = plt.figure(figsize=(8, 8))\nfor img in range(16):\n    ax = fig.add_subplot(4, 4, img+1)\n    ax = plt.imshow(result[0, :, :, img], cmap='gray')\n    plt.xticks([])\n    plt.yticks([])\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)","3fb06a1a":"y_test_pred = model.predict(x_test)\naccuracy_score(np.argmax(y_test_pred,axis=1), np.argmax(y_test,axis=1))","d8195b86":"#    def visualize_class_activation_map(img, target_class):\n#        \n#       #Get the 512 input weights to the softmax.\n#       class_weights = model.layers[-1].get_weights()[0]\n#        \n#       final_conv_layer = model.get_layer(\"conv2d_258\")\n#        \n#        \n#       get_output = K.function([model.layers[0].input], [final_conv_layer.output, model.layers[-1].output])\n#       [conv_outputs, predictions] = get_output([img.reshape((1,50,50,3))])\n#       conv_outputs = conv_outputs[0, :, :, :]\n#\n#       #Create the class activation map.\n#       cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[0:2])\n#       \n#       for i, w in enumerate(class_weights[:, target_class]):\n#               cam += w * conv_outputs[:, :, i]\n#                \n#       cam \/= np.max(cam)\n#       cam = cv2.resize(cam, (50, 50))\n#       heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n#       heatmap[np.where(cam < 0.2)] = 0\n#       img = heatmap*0.5 + img      \n#       \n#       return(img)","e3d68821":"**Visualization** Internal rappresentation","230a5e3b":"**Visualization** Learned patterns"}}