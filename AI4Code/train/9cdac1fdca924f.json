{"cell_type":{"01cb1ac5":"code","ad839007":"code","297b76a6":"code","51c88b6a":"code","a4ff2766":"code","14088acb":"code","89f00738":"code","e5666d9c":"code","be8a6ce4":"code","2d59574c":"code","c3dc7e70":"code","e9b2be4b":"code","3ce278b3":"code","8e302661":"code","1b788663":"code","f7b80803":"code","06b3c9d5":"code","5ce0f394":"code","7777245b":"code","23c7a8b6":"code","7d8c4d40":"code","fe3eec38":"code","8ecc164c":"code","20735348":"code","9361c132":"code","af20502f":"markdown","cfd86caf":"markdown","f6f92c13":"markdown","9f8d2b9d":"markdown","36021c30":"markdown","0d649ea7":"markdown","0fdf31db":"markdown","e858d5c2":"markdown","c31a4577":"markdown","f8117d8d":"markdown","98996dac":"markdown","a8a4c913":"markdown","f52dc248":"markdown","6febd9a4":"markdown","34a9a631":"markdown","ca04ae1a":"markdown","8e06d31c":"markdown","df836700":"markdown","61a9a21b":"markdown","da0b159a":"markdown","a44bbb19":"markdown","9374e513":"markdown","9cbaf051":"markdown","2f4af540":"markdown"},"source":{"01cb1ac5":"#%% Imports\n\n# Basic Imports \nimport numpy as np\nimport pandas as pd\n\n# Plotting \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n%matplotlib inline\n\n# Preprocessing\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import LabelEncoder\n\n# Metrics \nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# ML Models\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor \nimport xgboost as xg \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm\nfrom sklearn.linear_model import LinearRegression\n# Model Tuning \nfrom bayes_opt import BayesianOptimization\n\n# Feature Importance \nimport shap\n\n# Ignore Warnings \nimport warnings\nwarnings.filterwarnings('ignore')","ad839007":"#%% Read StudentsPerformance.csv\nSP_csv = pd.read_csv(\"..\/input\/students-performance-in-exams\/StudentsPerformance.csv\")\nSP_csv['average score'] = SP_csv[['math score', 'reading score','writing score']].mean(axis=1)\n# Initial glance at StudentsPerformance.csv\nprint(SP_csv.info(verbose = True,null_counts=True))","297b76a6":"#%% PlotMultiplePie \n# Input: df = Pandas dataframe, categorical_features = list of features , dropna = boolean variable to use NaN or not\n# Output: prints multiple px.pie() \n\ndef PlotMultiplePie(df,categorical_features = None,dropna = False):\n    # set a threshold of 30 unique variables, more than 50 can lead to ugly pie charts \n    threshold = 30\n    \n    # if user did not set categorical_features \n    if categorical_features == None: \n        categorical_features = df.select_dtypes(['object','category']).columns.to_list()\n        \n    print(\"The Categorical Features are:\",categorical_features)\n    \n    # loop through the list of categorical_features \n    for cat_feature in categorical_features: \n        num_unique = df[cat_feature].nunique(dropna = dropna)\n        num_missing = df[cat_feature].isna().sum()\n        # prints pie chart and info if unique values below threshold \n        if num_unique <= threshold:\n            print('Pie Chart for: ', cat_feature)\n            print('Number of Unique Values: ', num_unique)\n            print('Number of Missing Values: ', num_missing)\n            fig = px.pie(df[cat_feature].value_counts(dropna = dropna), values=cat_feature, \n                 names = df[cat_feature].value_counts(dropna = dropna).index,title = cat_feature,template='ggplot2')\n            fig.show()\n        else: \n            print('Pie Chart for ',cat_feature,' is unavailable due high number of Unique Values ')\n            print('Number of Unique Values: ', num_unique)\n            print('Number of Missing Values: ', num_missing)\n            print('\\n')","51c88b6a":"#%% Use PlotMultiplePie to see the distribution of the categorical variables \nPlotMultiplePie(SP_csv)","a4ff2766":"#%% Print the continous features in the dataset \ncontinous_features = SP_csv.select_dtypes(['float64','int64']).columns.to_list()\n\nfor cont_feature in continous_features: \n    plt.figure()\n    plt.title(cont_feature)\n    ax = sns.distplot(SP_csv[cont_feature])","14088acb":"#%% Remove outliers below lower% and above upper % quantile\n# https:\/\/nextjournal.com\/schmudde\/how-to-remove-outliers-in-data\ndef remove_outliers(df,column_name,lower,upper):\n    removed_outliers = df[column_name].between(df[column_name].quantile(lower), df[column_name].quantile(upper))\n    \n    print(str(df[column_name][removed_outliers].size) + \"\/\" + str(SP_csv[column_name].size) + \" data points remain.\") \n\n    index_names = df[~removed_outliers].index # INVERT removed_outliers!!\n    return df.drop(index_names)\n\n# SP_csv_clean = remove_outliers(sp_csv_clean,\"average score\",0.05,0.95)","89f00738":"# Seperate features and target \nSP_csv_clean = SP_csv.copy()\nmath_score = SP_csv_clean[\"math score\"]\nreading_score = SP_csv_clean[\"reading score\"]\nwriting_score = SP_csv_clean[\"writing score\"]\naverage_score = SP_csv_clean[\"average score\"]\nX_features = SP_csv_clean.drop([\"math score\",\"reading score\",\"writing score\",\"average score\"],axis = 'columns') ","e5666d9c":"#%% MultiColumnLabelEncoder\n# Code snipet found on Stack Exchange \n# https:\/\/stackoverflow.com\/questions\/24458645\/label-encoding-across-multiple-columns-in-scikit-learn\nfrom sklearn.preprocessing import LabelEncoder\n\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                # convert float NaN --> string NaN\n                output[col] = output[col].fillna('NaN')\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\n# # store the catagorical features names as a list      \n# cat_features = X_features.select_dtypes(['object']).columns.to_list()\n\n# # use MultiColumnLabelEncoder to apply LabelEncoding on cat_features \n# # uses NaN as a value , no imputation will be used for missing data\n# X_features_encoded = MultiColumnLabelEncoder(columns = cat_features).fit_transform(X_features)","be8a6ce4":"##% Manual Encoding \n# X_features_encoded = X_features_clean.copy() \n# print(X_features_encoded.columns)\n# print(X_features[\"race\/ethnicity\"].value_counts())\n\n# def encode_parent_education(string):\n#     if string == \"some high school\":\n#         return 0 \n#     elif string == \"high school\":\n#         return 1 \n#     elif string == \"some college\":\n#         return 2 \n#     elif string == \"associate's degree\":\n#         return 3 \n#     elif string == \"bachelor's degree\":\n#         return 4 \n#     elif string == \"master's degree\":\n#         return 5 \n#     else:\n#         return None\n\n# def encode_race_ethnicity(string):\n#     if string == \"group A\":\n#         return 0 \n#     elif string == \"group B\":\n#         return 1 \n#     elif string == \"group C\":\n#         return 2 \n#     elif string == \"group D\":\n#         return 3 \n#     elif string == \"group E\":\n#         return 4 \n#     else:\n#         return None\n\n# X_features_encoded[\"parental level of education\"] = [encode_parent_education(x) for x in X_features_encoded[\"parental level of education\"]]\n# X_features_encoded[\"race\/ethnicity\"] = [encode_race_ethnicity(x) for x in X_features_encoded[\"race\/ethnicity\"]]\n# X_features_encoded['gender'] = [0 if x == \"female\" else 1 for x in  X_features_encoded['gender']]\n# X_features_encoded['test preparation course'] = [0 if x == \"none\" else 1 for x in  X_features_encoded['test preparation course']]\n# X_features_encoded['lunch'] = [0 if x == \"standard\" else 1 for x in  X_features_encoded['lunch']]","2d59574c":"##% Dummy Encoding\n# https:\/\/towardsdatascience.com\/one-hot-encoding-multicollinearity-and-the-dummy-variable-trap-b5840be3c41a\n\n# convert all features to categorical data type \nX_features_encoded = X_features.apply(lambda x: x.astype('category')) \n# get dummies for all features but remove the first one(avoid multicollinearity) also called Dummy Variable Trap\nX_features_encoded = pd.get_dummies(X_features_encoded,drop_first= True)","c3dc7e70":"##% Before and After LabelEncoding for train.csv \ndisplay(X_features)\ndisplay(X_features_encoded)","e9b2be4b":"# Create test and train set 80-20\n#%%  train-test split using a 80-20 split\ntarget = average_score\ntrain_X, valid_X, train_y, valid_y = train_test_split(X_features_encoded, target, test_size=0.2, shuffle = True, random_state=1)","3ce278b3":"##% evaluateRegressor\n# https:\/\/towardsdatascience.com\/what-are-the-best-metrics-to-evaluate-your-regression-model-418ca481755b\n# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_scor\ndef evaluateRegressor(true,predicted,message = \"Test set\"):\n    MSE = mean_squared_error(true,predicted,squared = True)\n    MAE = mean_absolute_error(true,predicted)\n    RMSE = mean_squared_error(true,predicted,squared = False)\n    R_squared = r2_score(true,predicted)\n    print(message)\n    print(\"MSE:\", MSE)\n    print(\"MAE:\", MAE)\n    print(\"RMSE:\", RMSE)\n    print(\"R-squared:\", R_squared)","8e302661":"##% Plot True vs predicted values. Useful for continuous y \ndef PlotPrediction(true,predicted, title = \"Dataset: \"):\n    fig = plt.figure(figsize=(20,20))\n    ax1 = fig.add_subplot(111)\n    ax1.set_title(title + 'True vs Predicted')\n    ax1.scatter(list(range(0,len(true))),true, s=10, c='r', marker=\"o\", label='True')\n    ax1.scatter(list(range(0,len(predicted))), predicted, s=10, c='b', marker=\"o\", label='Predicted')\n    plt.legend(loc='upper right');\n    plt.show()","1b788663":"##% Initial Models\n# from sklearn.ensemble import RandomForestRegressor\n# from sklearn import svm\n# import lightgbm as lgb\n# import xgboost as xg \n\nRFReg = RandomForestRegressor(random_state = 0).fit(train_X, train_y)\nSVM = svm.SVR().fit(train_X, train_y) \nXGReg = xg.XGBRegressor(objective ='reg:squarederror', seed = 1,verbosity=0).fit(train_X,train_y) \nLGBMReg = lgb.LGBMRegressor(random_state=0).fit(train_X,train_y)\nLinearReg = LinearRegression(normalize = True).fit(train_X,train_y)","f7b80803":"##% Model Metrics\nprint(\"Random Forest Regressor\") \npredicted_train_y = RFReg.predict(train_X)\nevaluateRegressor(train_y,predicted_train_y,\"    Training Set\")\npredicted_valid_y = RFReg.predict(valid_X)\nevaluateRegressor(valid_y,predicted_valid_y,\"    Test Set\")\nprint(\"\\n\")\n    \nprint(\"Support Vector Machine\") \npredicted_train_y = SVM.predict(train_X)\nevaluateRegressor(train_y,predicted_train_y,\"    Training Set\")\npredicted_valid_y = SVM.predict(valid_X)\nevaluateRegressor(valid_y,predicted_valid_y,\"    Test Set\")\nprint(\"\\n\")\n\n\nprint(\"XGBoost Regressor\") \npredicted_train_y = XGReg.predict(train_X)\nevaluateRegressor(train_y,predicted_train_y,\"    Training Set\")\npredicted_valid_y = XGReg.predict(valid_X)\nevaluateRegressor(valid_y,predicted_valid_y,\"    Test Set\")\nprint(\"\\n\")\n\nprint(\"LightGBM Regressor\") \npredicted_train_y = LGBMReg.predict(train_X)\nevaluateRegressor(train_y,predicted_train_y,\"    Training Set\")\npredicted_valid_y = LGBMReg.predict(valid_X)\nevaluateRegressor(valid_y,predicted_valid_y,\"    Test Set\")\nprint(\"\\n\")\n\nprint(\"Linear Regression\") \npredicted_train_y = LinearReg.predict(train_X)\nevaluateRegressor(train_y,predicted_train_y,\"    Training Set\")\npredicted_valid_y = LinearReg.predict(valid_X)\nevaluateRegressor(valid_y,predicted_valid_y,\"    Test Set\")\nprint(\"\\n\")","06b3c9d5":"##% parameter tuning for lightgbm \n# store the catagorical features names as a list      \ncat_features = train_X.select_dtypes(['int64']).columns.to_list()\n\n# Create the LightGBM data containers\n# Make sure that cat_features are used\ntrain_data=lgb.Dataset(train_X,label=train_y, categorical_feature = cat_features,free_raw_data=False)\nvalid_data=lgb.Dataset(valid_X,label=valid_y, categorical_feature = cat_features,free_raw_data=False)","5ce0f394":"# https:\/\/medium.com\/analytics-vidhya\/hyperparameters-optimization-for-lightgbm-catboost-and-xgboost-regressors-using-bayesian-6e7c495947a9\n# from lightgbm import LGBMRegressor \n# from bayes_opt import BayesianOptimization\ndef search_best_param(X,y,cat_features):\n    \n    trainXY = lgb.Dataset(data=X, label=y,categorical_feature = cat_features,free_raw_data=False)\n    # define the lightGBM cross validation\n    def lightGBM_CV(max_depth, num_leaves, n_estimators, learning_rate, subsample, colsample_bytree, \n                lambda_l1, lambda_l2, min_child_weight):\n    \n        params = {'boosting_type': 'gbdt', 'objective': 'regression', 'metric':'rmse', 'verbose': -1,\n                  'early_stopping_round':100}\n        \n        params['max_depth'] = int(round(max_depth))\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params[\"n_estimators\"] = int(round(n_estimators))\n        params['learning_rate'] = learning_rate\n        params['subsample'] = subsample\n        params['colsample_bytree'] = colsample_bytree\n        params['lambda_l1'] = max(lambda_l1, 0)\n        params['lambda_l2'] = max(lambda_l2, 0)\n        params['min_child_weight'] = min_child_weight\n    \n        score = lgb.cv(params, trainXY, nfold=5, seed=1, stratified=False, verbose_eval =False, metrics=['rmse'])\n\n        return -np.min(score['rmse-mean']) # min or max can change best_param\n\n    # use bayesian optimization to search for the best hyper-parameter combination\n    lightGBM_Bo = BayesianOptimization(lightGBM_CV, \n                                       {\n                                          'max_depth': (5, 50),\n                                          'num_leaves': (20, 100),\n                                          'n_estimators': (50, 1000),\n                                          'learning_rate': (0.01, 0.3),\n                                          'subsample': (0.7, 0.8),\n                                          'colsample_bytree' :(0.5, 0.99),\n                                          'lambda_l1': (0, 5),\n                                          'lambda_l2': (0, 3),\n                                          'min_child_weight': (2, 50) \n                                      },\n                                       random_state = 1,\n                                       verbose = 0\n                                      )\n    np.random.seed(1)\n    \n    lightGBM_Bo.maximize(init_points=5, n_iter=15) # 20 combinations \n    \n    params_set = lightGBM_Bo.max['params']\n    \n    # get the params of the maximum target     \n    max_target = -np.inf\n    for i in lightGBM_Bo.res: # loop thru all the residuals \n        if i['target'] > max_target:\n            params_set = i['params']\n            max_target = i['target']\n    \n    params_set.update({'verbose': -1})\n    params_set.update({'metric': 'rmse'})\n    params_set.update({'boosting_type': 'gbdt'})\n    params_set.update({'objective': 'regression'})\n    \n    params_set['max_depth'] = int(round(params_set['max_depth']))\n    params_set['num_leaves'] = int(round(params_set['num_leaves']))\n    params_set['n_estimators'] = int(round(params_set['n_estimators']))\n    params_set['seed'] = 1 #set seed\n    \n    return params_set\n\nbest_params = search_best_param(train_X,train_y,cat_features)","7777245b":"# Print best_params\nfor key, value in best_params.items():\n    print(key, ' : ', value)","23c7a8b6":"# Train lgbm_best using the best params found from Bayesian Optimization\nlgbm_best = lgb.train(best_params,\n                 train_data,\n                 num_boost_round = 2500,\n                 valid_sets = valid_data,\n                 early_stopping_rounds = 200,\n                 verbose_eval = 100\n                 )","7d8c4d40":"##% Feature Importance \n# https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html\nlgb.plot_importance(lgbm_best,figsize=(25,20))","fe3eec38":"##% Feature Importance using shap package \n# import shap\nlgbm_best.params['objective'] = 'regression'\nshap_values = shap.TreeExplainer(lgbm_best).shap_values(train_X)\nshap.summary_plot(shap_values, train_X)","8ecc164c":"predicted_train_y = lgbm_best.predict(train_X)\nevaluateRegressor(train_y,predicted_train_y,\"    Training Set\")\nPlotPrediction(train_y,predicted_train_y, \"Training Set:\")","20735348":"predicted_valid_y = lgbm_best.predict(valid_X)\nevaluateRegressor(valid_y,predicted_valid_y,\"    Test Set\")\nPlotPrediction(valid_y,predicted_valid_y, \"Test Set:\")","9361c132":"predicted_entire_y = lgbm_best.predict(X_features_encoded)\nevaluateRegressor(target,predicted_entire_y,\"    Entire Set\")\nPlotPrediction(target,predicted_entire_y, \"Entire Set:\")","af20502f":"<a id=\"Importing-Libraries\"><\/a>\n# Importing Libraries","cfd86caf":"From both feature importance, we can see that **lunch**, **test preparation course**, and **gender** contributes a lot in prediction of a student performance. The shap package is prefer when finding feature importance as it preservces consistency and accuracy. You can read more about the shap package in the links provided below:\n\n[https:\/\/shap.readthedocs.io\/en\/latest\/example_notebooks\/tabular_examples\/tree_based_models\/Census%20income%20classification%20with%20LightGBM.html](https:\/\/shap.readthedocs.io\/en\/latest\/example_notebooks\/tabular_examples\/tree_based_models\/Census%20income%20classification%20with%20LightGBM.html)\n\n[https:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d](https:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d)  \n\n[https:\/\/towardsdatascience.com\/interpretable-machine-learning-with-xgboost-9ec80d148d27](https:\/\/towardsdatascience.com\/interpretable-machine-learning-with-xgboost-9ec80d148d27)","f6f92c13":"## Dummy Encoding","9f8d2b9d":"# Initial Models\nWe can apply different machine learning algorthims to test which model perform better on this dataset. I've listed below various machine learning techniques applied in this section.\n \n1. RandomForest\n2. Support Vector Machine\n3. XGBoost\n4. LightGBM\n5. Linear Regression","36021c30":"## Manual Encoding","0d649ea7":"<a id=\"StudentsPerformance.csv\"><\/a>\n## StudentsPerformance.csv","0fdf31db":"<a id=\"Tuning-LightGBM\"><\/a>\n## Tuning LightGBM","e858d5c2":"<a id=\"Features-Visualizaiton\"><\/a>\n## Features Visualizaiton","c31a4577":"<a id=\"Label-Encoding\"><\/a>\n# Label Encoding","f8117d8d":"<a id=\"Table-Of-Contents\"><\/a>\n# Table Of Contents\n* [Table Of Contents](#Table-Of-Contents)\n* [Importing Libraries](#Importing-Libraries)\n* [Task Details](#Task-Details)\n* [Feature Description](#Feature-Description)\n* [Read in Data](#Read-in-Data)\n    - [StudentsPerformance.csv](#StudentsPerformance.csv)\n* [Data Visualization](#Data-Visualization)\n    - [Features Visualizaiton](#Features-Visualizaiton)\n    - [Target Visualizaiton](#Target-Visualizaiton)\n* [Preprocessing Data](#Preprocessing-Data)\n    - [Removing Outliers](#Removing-Outliers)\n    - [Label Encoding](#Label-Encoding)\n    - [Dummy Encoding](#Dummy-Encoding)\n    - [Train-Test Split](#Train-Test-Split)\n* [Initial Models](#Initial-Models)\n* [LightGBM Regressor](#LightGBM-Regressor)\n    - [Bayesian Optimization](#Bayesian-Optimization)\n    - [Tuning LightGBM](#Tuning-LightGBM)\n    - [Feature Importance](#Feature-Importance)\n    - [Model Performance](#Model-Performance)\n* [Conclusion](#Conclusion)","98996dac":"<a id=\"Data-Visualization \"><\/a>\n# Data Visualization \n","a8a4c913":"<a id=\"Target-Visualization \"><\/a>\n## Target Visualization ","f52dc248":"## Bayesian Optimization","6febd9a4":"<a id=\"Conclusion\"><\/a>\n# Conclusion\n\n**Conclusion**\n* This dataset was easy to work with due to having few features and observations  \n* Learnt a lot about using different encoding methods on categorical data\n\n**Challenges**\n* The models perform poorly due to have few features. It was difficult to predict low or high scores\n\n**Closing Remarks**  \n* Please comment and like the notebook if it of use to you! Have a wonderful year! \n\n**Other Notebooks** \n* [https:\/\/www.kaggle.com\/josephchan524\/hranalytics-lightgbm-classifier-auc-80](https:\/\/www.kaggle.com\/josephchan524\/hranalytics-lightgbm-classifier-auc-80)\n* [https:\/\/www.kaggle.com\/josephchan524\/bankchurnersclassifier-recall-97-accuracy-95](https:\/\/www.kaggle.com\/josephchan524\/bankchurnersclassifier-recall-97-accuracy-95)\n* [https:\/\/www.kaggle.com\/josephchan524\/housepricesregressor-using-lightgbm](https:\/\/www.kaggle.com\/josephchan524\/housepricesregressor-using-lightgbm)\n* [https:\/\/www.kaggle.com\/josephchan524\/tabularplaygroundregressor-using-lightgbm-feb2021](https:\/\/www.kaggle.com\/josephchan524\/tabularplaygroundregressor-using-lightgbm-feb2021)\n\n2-12-2020\nJoseph Chan ","34a9a631":"<a id=\"Preprocessing-Data\"><\/a>\n# Preprocessing Data","ca04ae1a":"## Removing Outliers\nOutliers can be removed to obtain a better model. Due to this dataset having only 1000 observations, it might not be useful here. I left this section for anyone who needs to remove outliers from their dataset.","8e06d31c":"<a id=\"Task-Details\"><\/a>\n# Task Detail \n\nPredicting student performance with the demographic and socioeconomic information.\n\n## Expected Submission\nThe solution should contain Notebook with prediction accuracy.\nAn informative solution should also include statistical testing and information about what contributes the prediction.\nOf course the script should be easy to read and understand.\n\n## Evaluation\nI will use various prediction accuarcy such as RMSE, MSE and MSAE","df836700":"# <center>StudentPerforamnceRegressor  <\/center>\n<img src= \"https:\/\/blog.edmentum.com\/sites\/blog.edmentum.com\/files\/styles\/blog_image\/public\/images\/MeasuringStudentGrowth.png?itok=dm-KlP3N\" height=\"200\" align=\"center\"\/>","61a9a21b":"<a id=\"Feature-Description\"><\/a>\n# Feature Description \n\n## Target  \n* math score\n* reading score\n* writing score\n* average score (average of math, reading and writing score)\n\n## Features\n* gender\n* race\/ethnicity\n* parental level of education\n* lunch\n* test preparation course","da0b159a":"<a id=\"LightGBM-Regressor\"><\/a>\n# LightGBM Regressor","a44bbb19":"<a id=\"#Train-Test-Split\"><\/a>\n# Train-Test Split","9374e513":"<a id=\"Model-Performance\"><\/a>\n## Model Performance","9cbaf051":"<a id=\"Feature-Importance \"><\/a>\n## Feature Importance ","2f4af540":"<a id=\"Read-in-Data\"><\/a>\n# Read in Data"}}