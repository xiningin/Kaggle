{"cell_type":{"7bf96c9e":"code","e7fd779a":"code","0b77ed35":"code","b76c3359":"code","329e92c2":"code","0414cdea":"code","519298a5":"code","418df5c0":"code","922914ae":"code","0c64153b":"code","2136d28d":"code","9a45d919":"code","335aee36":"code","3b30f3aa":"code","841fa1fc":"code","c29c974e":"code","307a5c01":"code","c36ced4c":"code","33163957":"markdown","17912a76":"markdown","5775ac4b":"markdown","f599953e":"markdown","94583329":"markdown","a33210be":"markdown","20af45a4":"markdown","29065e9e":"markdown","c9c8392a":"markdown"},"source":{"7bf96c9e":"#B\u00e0i t\u1eadp ng\u00e0y 2 - Linear Regression\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e7fd779a":"\n\nimport matplotlib.pyplot as plt\nimport math\n\n","0b77ed35":"\n\nftPath = \"\/kaggle\/input\/housingprice\/ex1data2.txt\"\nlines = [] # Living area & number of bedroom\nwith open(ftPath) as f:\n    for line in f:\n        a = [float(i) for i in line.strip().split(',')]\n        lines.append(a)\ndata = np.array(lines)\n\n","b76c3359":"### Show data\n# re-readData\ndataVisual = pd.read_csv(ftPath, sep = ',', header = None)\ndataVisual.columns = ['Living Area', 'Bedrooms', 'Price']\n\n# Print out first 5 rows to get the imagination of data\ndataVisual.head()","329e92c2":"from sklearn.linear_model import LinearRegression","0414cdea":"X1 = dataVisual['Living Area'].values.reshape(-1, 1)\nX2 = dataVisual['Bedrooms'].values.reshape(-1, 1)\nX = np.column_stack((X1,X2))\nY = dataVisual['Price'].values.reshape(-1, 1)\n\n# Train the model\nmodel = LinearRegression()\nmodel.fit(X, Y)\n\n# Predict with the same input data\nY_pred = model.predict(X)\n\nprint(model.predict([[2000, 2]]))\n\n# Visualize the data\nplt.scatter(X1, Y)\nplt.plot(X, Y_pred, color = 'red')\nplt.show()\nplt.scatter(X2, Y)\nplt.plot(X, Y_pred, color = 'red')\nplt.show()","519298a5":"# X vector: [[living area, Bedrooms]]\nX = data[:,[0, 1]]\n\n# y: [[price]]\ny = data[:,[2]]\n\n# a: [Living area]\na = X[:, 0]\n","418df5c0":"print(a)\nm = len(a)\nA = 1\/(2*m) * (a.T.dot(a))\nB = -1\/(m) * (a.T.dot(y))\nC = 1\/(2*m) * (y.T.dot(y))\nprint(A)\nprint(B)\nprint(C)\n\n# 2310419.212765957 -382104564.09574467 65591548106.45744","922914ae":"def getCostFunc():\n    plt.figure(figsize=(10, 5))\n    theta = np.linspace(-230, 560, 100)\n    J = A*theta**2 + B*theta + C\n\n    # Gives a new shape to an array without changing its data.\n    plt.plot(theta, J.reshape(-1, 1), color='r')\n\n    plt.xlabel(\"Theta\")\n    plt.ylabel(\"Cost function\")\n    plt.grid(True)\n    return plt\n\ngetCostFunc().show()","0c64153b":"# Solve the problem for theta0, theta1\nfrom sklearn.linear_model import LinearRegression \nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nprint(lin_reg.intercept_)\nprint(lin_reg.coef_)","2136d28d":"from sklearn.linear_model import LinearRegression \nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nprint(lin_reg.intercept_, lin_reg.coef_)","9a45d919":"m = len(X)\n# Translates slice objects to concatenation along the second axis.\n# add slice 1..1 to X\nXb = np.c_[np.ones((m, 1)), X]\n","335aee36":"theta_best = np.array([[89597.9], [139.21067402], [-8738.01911233]]) # normally, this should be a random number\nerror = 1\/m * math.sqrt((Xb.dot(theta_best)-y).T.dot(Xb.dot(theta_best)-y))\nprint(\"MSE:\", error)\nprint(theta_best)","3b30f3aa":"c = len(Xb[0])\n# Return a sample\n# (or samples) from the \u201cstandard normal\u201d distribution.\n\ntheta = np.random.randn(c,1)\nprint(c)\nprint(theta)\neta = 2e-7","841fa1fc":"# Each iteration\n#tmp1 = Xb.dot(theta) - y\n#gradients = (2\/m) * Xb.T.dot(Xb.dot(theta) - y)\n#theta = theta - eta * gradients\n#err = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\n#print(\"MSE:\", err) \n#print(theta)","c29c974e":"# TODO: Add early stopping criteria if err doesn't decrease more than epsilon value\n#### DONE: stop condition: err0 - err < epsilon\n# TODO: Check the number of iteration to converge\n#### DONE: i logged at the end of loop\ni = 1\nepsilon_GD=0.00031\nprint(theta)\n# init error must be biggest so we can keep it loop at least 1 time:\nerr = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\nerr0 = err + epsilon_GD + 10 # 10 or any other number is OK, wee just keep it bugger so it can loop at least 1 time\n\ndef plotDataIntoCostFunc(theta):\n    print(theta)\n    thetaA = theta[1][0]\n    J0 = A*thetaA**2 + B*thetaA + C\n\n    newCostFunc = getCostFunc()\n    newCostFunc.scatter(thetaA, J0)\n    newCostFunc.show()\n   \nwhile (True):\n    e0=err\n    tmp1 = Xb.dot(theta) - y\n    gradients = (2\/m) * Xb.T.dot(Xb.dot(theta) - y)\n    theta = theta - eta * gradients\n    plotDataIntoCostFunc(theta)\n    err = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\n    i = i + 1\n    if err0 - err < epsilon_GD:\n        break\n    else:\n        print(i,'- err decreasement: ',(err0 - err))\n        err0 = err\n        \n\n","307a5c01":"#print result\nthetaResult=theta[:, 0]\nprint(\"loop:  \",i)\nprint(\"err:   \",err)\nprint(\"thetaResult: \",thetaResult)\n","c36ced4c":"# predict function h(x) = y\ndef H_func(features = [['Living area', 'bedrooms']]):\n    featuresBar=features[0]\n    featuresBar.insert(0, 1) # add number 1 into features vector\n    print(\"featuresBar\", featuresBar)\n    return thetaResult[0] + thetaResult[1]*featuresBar[1] + thetaResult[2]*featuresBar[2]\n\nprint(\"predict result: \", H_func([[2000, 2]]))","33163957":"y\n\u2248\nf\n(\nx\n)\n=\n^\n","17912a76":"## Using sklearn built-in function","5775ac4b":"## Batch gradient descent","f599953e":"### With chosen init","94583329":"### With dummy init","a33210be":"**What we have to do (:<**","20af45a4":"*What we should achive*","29065e9e":"**Read from file**","c9c8392a":"**Visualize read data**"}}