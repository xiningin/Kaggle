{"cell_type":{"751e22a9":"code","3954b8c4":"code","c12dca13":"code","66981255":"code","de2a7d12":"code","97186858":"code","c9453a09":"code","5dfee604":"code","1f6d6e21":"code","ce7980dd":"code","06d43040":"code","a3e039bd":"code","7b293b74":"code","5e83e188":"code","a831b796":"code","6ce3a4e4":"code","48191ec3":"code","d7df40f1":"code","7321bc7a":"code","cf944d7e":"code","51e416ba":"code","d4483015":"code","b43762df":"code","d79b2cfe":"code","f485db63":"code","aee96f2c":"code","88790abb":"code","cfa44028":"code","3b98743c":"code","b57a8804":"code","4d510b1c":"markdown","82af0944":"markdown","97e46cc7":"markdown","f4b240d4":"markdown","c1102cbf":"markdown","c357f358":"markdown","68755233":"markdown","ddbe86bc":"markdown","828ee7b2":"markdown","145f244d":"markdown","7805f3d5":"markdown","a9b69e3e":"markdown"},"source":{"751e22a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3954b8c4":"os.listdir('..\/input\/sentiment140\/')","c12dca13":"import pandas as pd","66981255":"filepath = '..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv'","de2a7d12":"df = pd.read_csv(filepath,encoding='ISO-8859-1',delimiter=',')","97186858":"df.head(5)","c9453a09":"df.columns","5dfee604":"df.columns = ['target','id','date','flag','user','tweet']","1f6d6e21":"df.head()","ce7980dd":"df['target'].unique()  # finding target classes","06d43040":"df['tweet'][0]","a3e039bd":"stopword = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\ndef stopword_removal(input_text):\n    words = input_text.split()\n    noise_free_words = [word for word in words if word not in stopword]\n    text = \" \".join(noise_free_words)\n    return text","7b293b74":"df['tweet'] = df['tweet'].apply(lambda x: stopword_removal(x))","5e83e188":"import re\ndef remove_regex_hash(input_text):\n    regex_pattern_hash = \"#[\\w]*\" \n    urls = re.finditer(regex_pattern_hash, input_text) \n    for i in urls: \n        input_text = re.sub(i.group().strip(), '', input_text)\n    return input_text","a831b796":"def remove_regex_mention(input_text):\n    regex_pattern_mention = '@[\\w]*'\n    urls = re.finditer(regex_pattern_mention, input_text) \n    for i in urls: \n        input_text = re.sub(i.group().strip(), '', input_text)\n    return input_text","6ce3a4e4":"df['tweet'] = df.tweet.apply(lambda x:remove_regex_hash(x))","48191ec3":"df.tweet = df.tweet.apply(lambda x: remove_regex_mention(x))","d7df40f1":"df.head()","7321bc7a":"from nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\n\ndf['tweet'] = df['tweet'].apply(lambda x: lem.lemmatize(x,\"v\"))","cf944d7e":"vocab_size = 10000\nembedding_dim = 16\nmax_length = 21\ntrunc_type = 'post'\npadding_type = 'post'\noov_tok = '<OOV>'\ntraining_size = 20000","51e416ba":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","d4483015":"import nltk","b43762df":"def tokenize(value):\n    tokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_tok)\n    tokenizer.fit_on_texts(value)\n\n    word_index = tokenizer.word_index\n\n    sequences = tokenizer.texts_to_sequences(value)\n    padded = pad_sequences(sequences,\n                            maxlen=max_length,\n                            padding=padding_type,\n                            truncating=trunc_type\n                           )\n    return padded","d79b2cfe":"tokenize(df['tweet'][0])","f485db63":"df['tweet'] = df.tweet.apply(lambda x: tokenize(x)) ","aee96f2c":"df.head(10)","88790abb":"df.columns","cfa44028":"df['tweet'][0].shape","3b98743c":"from sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df,test_size=0.8,random_state=42)","b57a8804":"print('Size of training data: {}'.format(len(df_train)))\nprint('Size of testing data: {}'.format(len(df_test)))","4d510b1c":"Getting column names","82af0944":"Removing Stop Words","97e46cc7":"Text Lemmatizing","f4b240d4":"Splitting into Training and Testing data\n\nSplitting is done by train_test_split function of scikit-learn\ntraining data is 80% of total data\ntesting data is 20% of total data","c1102cbf":"Text to Sequence","c357f358":"Viewing First tweet\n\nIt seems this one is negitive","68755233":"Removing mentions from tweets\n\nReplacing @ with a blank space ('')","ddbe86bc":"0 = Negative\n\n4 = Positive","828ee7b2":"Fetching target Values","145f244d":"Assigning column names","7805f3d5":"Reading file from specified filepath","a9b69e3e":"Removing hashtags from tweets\n\nReplacing # with blank space ('')"}}