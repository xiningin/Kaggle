{"cell_type":{"2f51c88b":"code","2f499ada":"code","20494691":"code","b2a7593d":"code","881d2c2c":"code","cf3a5086":"code","3ad912a3":"code","dac3a267":"code","cb010101":"code","541489a2":"code","6b24b040":"code","51fe4ded":"code","71d4e939":"code","88c794b3":"code","317f383e":"markdown","9702d3d5":"markdown","09fb15ed":"markdown","7680ad40":"markdown","5d01e4de":"markdown","889c2106":"markdown","e7d1fe30":"markdown","b0e0794c":"markdown","a041f164":"markdown"},"source":{"2f51c88b":"!nvidia-smi","2f499ada":"SEED = 666\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\ntf.random.set_seed(SEED)\n\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import cifar10","20494691":"import os\nimport csv\nos.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n\nimport numpy as np\nnp.random.seed(SEED)\n\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom PIL import Image\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n\nimport librosa as lb \nimport librosa.display\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\n\nfrom skimage.transform import resize\nfrom scipy import stats","b2a7593d":"import wandb\nfrom wandb.keras import WandbCallback\n\nwandb.login()","881d2c2c":"run = wandb.init(project='rainforest', job_type='download_dataset')\n\nartifact = run.use_artifact('wandb\/rainforest\/spectrogram-dataset_nfft_2024_hop_512:v0', type='dataset')\nartifact_dir = artifact.download()\n\nrun.join()","cf3a5086":"IMG_DIR = Path(artifact_dir+'\/')\nIMG_PATH = list(map(str, list(IMG_DIR.glob('*.bmp'))))","3ad912a3":"train_path, valid_path = train_test_split(IMG_PATH, test_size=0.20, shuffle=True, random_state=42)\nlen(IMG_PATH), len(train_path), len(valid_path)","dac3a267":"AUTO = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 32\nIMG_WIDTH = 400\nIMG_HEIGHT = 224\nCHANNELS = 3\nNUM_CLASSES = 24","cb010101":"@tf.function\ndef parse_data(image_path):\n    # parse image\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_image(image)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    # normalize image\n    image = tf.image.per_image_standardization(image)\n    \n    # parse data\n    label = tf.strings.split(image_path, sep='_')[-2]\n    label = tf.strings.to_number(label, out_type=tf.int32)\n    label = tf.one_hot(label, NUM_CLASSES) \n    \n    return image, label","541489a2":"def get_model(base_model_name):\n    if base_model_name=='resnet':\n        base_model = tf.keras.applications.ResNet50V2(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n        \n    elif base_model_name=='efficientnetb0':\n        base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n    elif base_model_name=='efficientnetb1':\n        base_model = tf.keras.applications.EfficientNetB1(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n    elif base_model_name=='efficientnetb2':\n        base_model = tf.keras.applications.EfficientNetB2(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n    \n    elif base_model_name=='xception':\n        base_model = tf.keras.applications.Xception(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n    \n    elif base_model_name=='densenet':\n        base_model = tf.keras.applications.DenseNet169(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n\n    base_model.trainabe = True  \n    \n    inputs = Input((IMG_HEIGHT, IMG_WIDTH, 3))\n    resize = experimental.preprocessing.Resizing(224,224)(inputs) \n    x = base_model(resize, training=True)\n    x = GlobalAveragePooling2D()(x)\n    x = Dropout(0.5)(x)  \n    outputs = Dense(NUM_CLASSES, activation='sigmoid')(x)\n    \n    return Model(inputs, outputs)","6b24b040":"def train():\n    # Specify the hyperparameter to be tuned along with\n    # an initial value\n    config_defaults = {\n        'base_model_name': 'resnet',\n        'dataset_name': 'nfft_2024_hop_512'\n        }\n\n    # Initialize wandb with a sample project name\n    run = wandb.init(config=config_defaults)\n\n    # Specify the other hyperparameters to the configuration, if any\n    wandb.config.epochs = 30\n\n    # download and prepare data\n    data_artifact = run.use_artifact('wandb\/rainforest\/spectrogram-dataset_{}:latest'.format(wandb.config.dataset_name))\n    artifact_dir = data_artifact.download()\n    \n    IMG_DIR = Path(artifact_dir+'\/')\n    IMG_PATH = list(map(str, list(IMG_DIR.glob('*.bmp'))))\n    \n    # train-validation split\n    train_path, valid_path = train_test_split(IMG_PATH, test_size=0.20, shuffle=True, random_state=42)\n    \n    # dataloader\n    trainloader = tf.data.Dataset.list_files((train_path))\n    validloader = tf.data.Dataset.list_files((valid_path))\n\n    trainloader = (\n        trainloader\n        .shuffle(1024)\n        .map(parse_data, num_parallel_calls=AUTO)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n\n    validloader = (\n        validloader\n        .map(parse_data, num_parallel_calls=AUTO)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n\n\n    # Iniialize model with hyperparameters\n    keras.backend.clear_session()\n    model = get_model(wandb.config.base_model_name)\n    \n    # Compile the model\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    model.compile(opt, 'binary_crossentropy', metrics=['acc'])\n    \n    # Train the model\n    _ = model.fit(trainloader,\n                  epochs=wandb.config.epochs, \n                  validation_data=validloader,\n                  callbacks=[WandbCallback()]) # WandbCallback to automatically track metrics\n                            \n    # Evaluate    \n    loss, accuracy = model.evaluate(validloader, callbacks=[WandbCallback()])\n    print('Test Error Rate: ', round((1-accuracy)*100, 2))\n    wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)}) # wandb.log to track custom metrics\n    \n    # save model\n    model.save('model.h5')\n\n    # initialize a new artifact to save the model\n    model_artifact =  wandb.Artifact(\"trained-model\", \n                                     type=\"model\", \n                                     description=\"Simple model trained with spectrogram dataset formed with nfft 2024 and hop length of 512\",\n                                     metadata={'optimizer': 'Adam',\n                                              'Loss': 'Binary Cross Entropy',\n                                              'Learning Rate': 0.001})\n\n    model_artifact.add_file('model.h5')\n    run.log_artifact(model_artifact)\n    \n    run.join()","51fe4ded":"sweep_config = {\n  'method': 'bayes', \n  'metric': {\n      'name': 'val_loss',\n      'goal': 'minimize'\n  },\n  'early_terminate':{\n      'type': 'hyperband',\n      'max_iter': 27,\n      's': 2,\n      'eta': 3\n  },\n  'parameters': {\n      'base_model_name': {\n          'values': ['resnet',\n                     'efficientnetb0',\n                     'efficientnetb1',\n                     'efficientnetb2',\n                     'xception',\n                     'densenet']\n      },\n      'dataset_name': {\n          'values': ['nfft_2024_hop_512',\n                     'nfft_1024_hop_512',\n                     'nfft_1024_hop_256']\n      }\n  }\n}","71d4e939":"sweep_id = wandb.sweep(sweep_config, project=\"rainforest\")","88c794b3":"wandb.agent(sweep_id, function=train)","317f383e":"# Setup Sweep ","9702d3d5":"In this Kaggle kernel, we will perform an ablation study to find the connection between model size and validation accuracy with the dataset that I have created in this [Kaggle kernel](https:\/\/www.kaggle.com\/ayuraj\/rainforest-create-image-dataset-with-w-b). \n\nWe will set up a [Weights and Biases Sweep](https:\/\/docs.wandb.com\/sweeps) to find the best model for our dataset. We will also see the effect of batch size and learning rate along with the model size. ","09fb15ed":"# Download Dataset from W&B Artifacts and Prepare","7680ad40":"# Initialize Sweep and Run Agent","5d01e4de":"### Train-test Split","889c2106":"# Imports and Setups","e7d1fe30":"### Dataloader","b0e0794c":"# Model","a041f164":"# Sweep Config"}}