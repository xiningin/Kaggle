{"cell_type":{"9321fad4":"code","096a7119":"code","f9852448":"code","62054922":"code","044033f6":"code","17f6b42c":"code","b712cee0":"code","47fef4e0":"code","f771315f":"code","f2820616":"code","f4a0ab2e":"code","5a4970a6":"code","db37f657":"code","2f66b4ef":"code","1bd2f8b5":"code","c6c66839":"code","87544515":"code","c6306e49":"code","2a4cec8b":"code","33563cbe":"code","6226f2d2":"code","62812dde":"code","4c9181ff":"code","3d3e640c":"code","67a8ba58":"code","ccd61071":"code","234d8ab6":"code","6070a557":"code","62d1e428":"code","be242486":"code","f1140509":"code","777b3beb":"code","8cde0ee7":"code","e9c51e47":"code","5e4774b7":"code","c743257e":"code","9330400e":"code","b141ed03":"code","f699a0a7":"markdown"},"source":{"9321fad4":"import numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt","096a7119":"path = \"..\/input\/bms-molecular-translation\/\"\n\nlabels_path = path + \"train_labels.csv\"\ndf_train_labels = pd.read_csv(labels_path)\ndf_train_labels.head()","f9852448":"test = pd.read_csv(path + 'sample_submission.csv', index_col=0)","62054922":"df_train_labels.tail()","044033f6":"fully_qualified_path = path + \"train\/{}\/{}\/{}\/{}.png\"\nconvert_image_id_to_path = lambda image_id_details :fully_qualified_path.format(image_id_details[0], image_id_details[1], image_id_details[2], image_id_details) ","17f6b42c":"df_train_labels['image_path']=df_train_labels['image_id'].apply(convert_image_id_to_path)","b712cee0":"df_train_labels.head()","47fef4e0":"def convert_image_id_2_path(image_id: str) -> str:\n    return path + \"test\/{}\/{}\/{}\/{}.png\".format(\n        image_id[0], image_id[1], image_id[2], image_id \n    )","f771315f":"def visualize_image(image_id, label):\n    plt.figure(figsize=(10, 8))\n    \n    image = cv2.imread(convert_image_id_2_path(image_id))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    plt.imshow(image)\n    plt.title(f\"{label}\", fontsize=14)\n    plt.axis(\"off\")\n    \n    plt.show()","f2820616":"def visualize_image_denoise(image_id):\n    plt.figure(figsize=(10, 8))  \n    image = cv2.imread(convert_image_id_2_path(image_id), cv2.IMREAD_GRAYSCALE)\n    _, blackAndWhite = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY_INV)\n    nlabels, labels, stats, centroids = cv2.connectedComponentsWithStats(blackAndWhite, None, None, None, 8, cv2.CV_32S)\n    sizes = stats[1:, -1] #get CC_STAT_AREA component\n    img2 = np.zeros((labels.shape), np.uint8)\n    for i in range(0, nlabels - 1):\n        if sizes[i] >= 2:   #filter small dotted regions\n            img2[labels == i + 1] = 255\n    image = cv2.bitwise_not(img2)\n    plt.imshow(image)    \n    plt.axis(\"off\")\n    plt.show()","f4a0ab2e":"i=0\nvisualize_image(test.index[i], test.index[i])\nvisualize_image_denoise(test.index[i])","5a4970a6":"i=1\nvisualize_image(test.index[i], test.index[i])\nvisualize_image_denoise(test.index[i])","db37f657":"i=3\nvisualize_image(test.index[i], test.index[i])\nvisualize_image_denoise(test.index[i])","2f66b4ef":"i=4\nvisualize_image(test.index[i], test.index[i])\nvisualize_image_denoise(test.index[i])","1bd2f8b5":"def visualize_train_batch(image_ids, labels):\n    plt.figure(figsize=(16, 12))\n    \n    for ind, (image_id, label) in enumerate(zip(image_ids, labels)):\n        plt.subplot(3, 3, ind + 1)\n        image = cv2.imread(convert_image_id_to_path(image_id))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        plt.imshow(image)\n        plt.title(f\"{label[:30]}...\", fontsize=10)\n        plt.axis(\"off\")\n    \n    plt.show()","c6c66839":"tmp_df = df_train_labels[:9]\nimage_ids = tmp_df['image_id']\nlabels = tmp_df[\"InChI\"].values\nvisualize_train_batch(image_ids, labels)","87544515":"tmp_df = df_train_labels[:9]\nimage_ids = tmp_df['image_id']\nlabels = tmp_df[\"InChI\"].values\nvisualize_train_batch(image_ids, labels)","c6306e49":"print('Length of training-data:',len(df_train_labels))\nprint('Number of unique chemical identifier:',len(df_train_labels['InChI'].value_counts().index))\nprint('Max count of any chemical identifier in trainign data:',max(df_train_labels['InChI'].value_counts().values))","2a4cec8b":"h_shape=[]\nw_shape=[]\naspect_ratio=[]\nfor idx,image_id in enumerate(df_train_labels.image_id.values[:1000]):\n    image = cv2.imread(df_train_labels['image_path'][idx])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    h_shape.append(image.shape[0])\n    w_shape.append(image.shape[1])\n    aspect_ratio.append(1.0 * (image.shape[1] \/ image.shape[0]))","33563cbe":"plt.figure(figsize=(12, 12))\nplt.subplots_adjust(top = 0.5, bottom=0.01, hspace=1, wspace=0.4)\nplt.subplot(2, 2, 1)\nplt.hist(np.array(h_shape) * np.array(w_shape), bins=50)\nplt.xticks(rotation=45)\nplt.title(\"Area Image Distribution\", fontsize=14)\nplt.subplot(2, 2, 2)\nplt.hist(h_shape, bins=50)\nplt.title(\"Height Image Distribution\", fontsize=14)\nprint()\nplt.subplot(2, 2, 3)\nplt.hist(w_shape, bins=50)\nplt.title(\"Width Image Distribution\", fontsize=14)\nplt.subplot(2, 2, 4)\nplt.hist(aspect_ratio, bins=50)\nplt.title(\"Aspect Ratio Distribution\", fontsize=14);","6226f2d2":"from pickle import dump\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications import DenseNet121\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.densenet import preprocess_input\nfrom keras.models import Model","62812dde":"# extract features from each image\ndef extract_features():\n    \n # load the model\n    model = DenseNet121()\n    # re-structure the model\n    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n    # summarize\n    print(model.summary())\n # extract features from each image\n    features = dict()\n    for idx,name in enumerate(df_train_labels['image_path'].values[:100]):\n        filename = name\n        image = load_img(filename, target_size=(224, 224))\n         # convert the image pixels to a numpy array\n        image = img_to_array(image)\n         # reshape data for the model\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n         # prepare the image for the DenseNet121 model\n        image = preprocess_input(image)\n         # get features\n        feature = model.predict(image, verbose=0)\n         # store feature\n        features[df_train_labels['image_id'][idx]] = feature\n        #print('>%s' % name)\n    return features","4c9181ff":"features = extract_features()\nprint('Extracted Features: %d' % len(features))\n# save to file\ndump(features, open('features.pkl', 'wb'))","3d3e640c":"# extract texts for images\ndef load_text():\n    mapping = dict()\n    for idx,text in enumerate(df_train_labels['InChI'].values[:101]):\n        mapping[df_train_labels['image_id'][idx]]=text\n    return mapping","67a8ba58":"def to_vocabulary(descriptions):\n    all_desc = set()\n    for key,value in descriptions.items():\n        all_desc.update([value])\n    return all_desc","ccd61071":"texts = load_text()\nvocabulary  = to_vocabulary(texts)","234d8ab6":"print('Loaded: %d ' % len(texts))\nprint('Vocabulary Size: %d' % len(vocabulary))","6070a557":"from tqdm.auto import tqdm\nimport Levenshtein","62d1e428":"tqdm.pandas()","be242486":"test = pd.read_csv(path + 'sample_submission.csv')","f1140509":"train=df_train_labels\ntrain['InChI_list'] = train['InChI'].progress_apply(lambda x: x.split('\/'))\ntrain['InChI_length'] = train['InChI_list'].progress_apply(len)\nInChI_df = train['InChI_list'].progress_apply(pd.Series)\ntrain = pd.concat([train, InChI_df.add_prefix('InChI_')], axis=1)","777b3beb":"display(train)","8cde0ee7":"def get_score(y_true, y_pred):\n    scores = []\n    for true, pred in zip(y_true, y_pred):\n        score = Levenshtein.distance(true, pred)\n        scores.append(score)\n    avg_score = np.mean(scores)\n    return avg_score","e9c51e47":"mode_concat_string = ''\nfor i in range(11):\n    mode_string = train[f'InChI_{i}'].fillna('nan').mode()[0]\n    if mode_string != 'nan':\n        if i == 0:\n            mode_concat_string += mode_string\n        else:\n            mode_concat_string += '\/' + mode_string","5e4774b7":"print(mode_concat_string)","c743257e":"y_true = train['InChI'].values\ny_pred = [mode_concat_string] * len(train)\nscore = get_score(y_true, y_pred)","9330400e":"print(score)","b141ed03":"test['InChI'] = mode_concat_string\noutput_cols = ['image_id', 'InChI']\ndisplay(test[output_cols])\ntest[output_cols].to_csv('submission.csv', index=False)","f699a0a7":"### Denoise images"}}