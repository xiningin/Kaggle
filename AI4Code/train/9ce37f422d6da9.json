{"cell_type":{"e8149bf4":"code","bdcd889d":"code","80abc69f":"code","0b98d1dd":"code","c0a03518":"code","000058fd":"code","f1c5363c":"code","016f9e6c":"code","25011bcb":"code","c9c13bb4":"code","4970f787":"code","42d3b797":"code","f35ef3db":"markdown","bb0994e4":"markdown","86180c43":"markdown","26afe309":"markdown","da9cc9a2":"markdown","8f57923d":"markdown","cb94463f":"markdown","5e6f79bd":"markdown"},"source":{"e8149bf4":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.utils import save_image\nimport datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport cv2\nimport os\nos.listdir('..\/input\/neural-style-transfer')\nplt.rcParams[\"figure.figsize\"] = (20,10)","bdcd889d":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","80abc69f":"class VGG(nn.Module):\n    def __init__(self):\n        super(VGG, self).__init__()\n        # The first number x in convx_y gets added by 1 after it has gone\n        # through a maxpool, and the second y if we have several conv layers\n        # in between a max pool. These strings (0, 5, 10, ..) then correspond\n        # to conv1_1, conv2_1, conv3_1, conv4_1, conv5_1 mentioned in NST paper\n        self.chosen_features = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n\n        # We don't need to run anything further than conv5_1 (the 28th module in vgg)\n        # Since remember, we dont actually care about the output of VGG: the only thing\n        # that is modified is the generated image (i.e, the input).\n        self.model = models.vgg19(pretrained=True).features[:29]\n\n    def forward(self, x):\n        # Store relevant features\n        features = []\n\n        # Go through each layer in model, if the layer is in the chosen_features,\n        # store it in features. At the end we'll just return all the activations\n        # for the specific layers we have in chosen_features\n        for layer_num, layer in enumerate(self.model):\n            x = layer(x)\n\n            if str(layer_num) in self.chosen_features:\n                features.append(x)\n\n        return features\nmodel = VGG().to(device).eval()\nmodel","0b98d1dd":"imsize = 500\nloader = transforms.Compose(\n    [\n        transforms.Resize((imsize, imsize)),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x  )\n        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\ndef load_image(image_name):\n    image = Image.open(image_name)\n    image = loader(image).unsqueeze(0)\n    print(image.shape)\n    \n    return image.to(device)","c0a03518":"content_name = 'tubingen'\nstyle_name='candy'\noriginal_img = load_image(\"..\/input\/neural-style-transfer\/content-images\/\"+content_name+\".png\")\nstyle_img = load_image(\"..\/input\/neural-style-transfer\/style-images\/\"+style_name+\".jpg\")\ngenerated = original_img.clone().requires_grad_(True)\n\n# Hyperparameters\ntotal_steps = 12000\nlearning_rate = 0.0001\nalpha = 1\nbeta = 0.01\noptimizer = optim.Adam([generated], lr=learning_rate)\n\n","000058fd":"npy = original_img[0].cpu().permute(1,2,0).detach().numpy()\nplt.imshow(npy)","f1c5363c":"plt.imshow(style_img[0].cpu().permute(1,2,0).detach().numpy())","016f9e6c":"npy = generated[0].cpu().permute(1,2,0).detach().numpy()\nplt.imshow(npy)","25011bcb":"\nvideo_name = content_name+'_'+style_name+'.avi'\n\nvideo = cv2.VideoWriter(video_name, 0, 1, (imsize,imsize))\n","c9c13bb4":"\nstart = datetime.datetime.now()\nfor step in range(total_steps):\n    # Obtain the convolution features in specifically chosen layers\n    generated_features = model(generated)\n    original_img_features = model(original_img)\n    style_features = model(style_img)\n\n    # Loss is 0 initially\n    style_loss = original_loss = 0\n\n    # iterate through all the features for the chosen layers\n    for gen_feature, orig_feature, style_feature in zip(\n        generated_features, original_img_features, style_features\n    ):\n\n        # batch_size will just be 1\n        batch_size, channel, height, width = gen_feature.shape\n        original_loss += torch.mean((gen_feature - orig_feature) ** 2)\n        # Compute Gram Matrix of generated\n        G = gen_feature.view(channel, height * width).mm(\n            gen_feature.view(channel, height * width).t()\n        )\n        # Compute Gram Matrix of Style\n        A = style_feature.view(channel, height * width).mm(\n            style_feature.view(channel, height * width).t()\n        )\n        style_loss += torch.mean((G - A) ** 2)\n\n    total_loss = alpha * original_loss + beta * style_loss\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n    \n    \n    if step % 200 == 0 or step < 5:\n        stop = datetime.datetime.now()\n        elapsed =  (stop-start).total_seconds()\n        print('At step : '+str(step)+' Loss:'+str(total_loss)+' in time : '+str(elapsed))\n        img = generated[0].cpu().permute(1,2,0).detach().numpy()\n        video.write((img*255).astype(np.uint8))\n        start = datetime.datetime.now()","4970f787":"video.release()\ndef convert_avi_to_mp4(avi_file_path, output_name):\n    os.popen(\"ffmpeg -i '{input}' -ac 2 -b:v 2000k -c:a aac -c:v libx264 -b:a 160k -vprofile high -bf 0 -strict experimental -f mp4 '{output}.mp4'\".format(input = avi_file_path, output = output_name))\n    return True\nconvert_avi_to_mp4('.\/taj_mahal_mosaic.avi','taj_mahal_mosaic')","42d3b797":"npy = generated[0].cpu().permute(1,2,0).detach().numpy()\nplt.imshow(npy)","f35ef3db":"# Working\n\n## LOSS\n![LOSS](https:\/\/miro.medium.com\/max\/875\/1*INwW0Apz4wUpDS9jetJ7xQ.jpeg)\nWhat are the inputs to this loss function displayed above? We have no idea how the final output might look. So the naive approach of supervised learning might not work. The answer lies in the image below.\n![ARCH](https:\/\/miro.medium.com\/max\/875\/0*gq-qNqxE3b37o3dO)\n\n## Content Loss\n![CL](https:\/\/miro.medium.com\/max\/875\/1*34xPuexhGCHT7xZ17wVvDQ.jpeg)\n\n## Style Loss\n\nNow let\u2019s look at the style loss, while calculating the style loss we will consider feature representation of many convolution layers from shallow to deeper layers of the model. Unlike content loss we can\u2019t just find the difference in activation units, What we need is a way to find the correlation between these activations across different channels of the same layer and to do this we need something called as the Gram Matrix.\n\n### Gram Matrix : It calculates a global statistics of feature maps, kinda correlation but globally on the feature map\n\n![SL](https:\/\/miro.medium.com\/max\/875\/1*IoozR3xGzaSqtEqGEKcWMQ.jpeg)\n","bb0994e4":"# References \n[1] https:\/\/www.tensorflow.org\/tutorials\/generative\/style_transfer\n\n\n[2] https:\/\/towardsdatascience.com\/neural-style-transfer-tutorial-part-1-f5cd3315fa7f\n\n\n[3] https:\/\/towardsdatascience.com\/how-to-get-beautiful-results-with-neural-style-transfer-75d0c05d6489\n\n\n[4] https:\/\/medium.com\/tensorflow\/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398\n","86180c43":"# Lets Load the Image first","26afe309":"## Defination\n\n1. Neural style transfer is an optimization technique used to take two images\u2014a content image and a style reference image (such as an artwork by a famous painter)\u2014and blend them together so the output image looks like the content image, but \u201cpainted\u201d in the style of the style reference image.\n\n    This is implemented by optimizing the output image to match the content statistics of the content image and the style statistics of the style reference image. These statistics are extracted from the images using a convolutional network.\n    \n2. During the past few years we\u2019ve seen a slew of apps like prisma and other similar apps popping up which style your photos in a way wherein they look like paintings. Offering you a variety of beautiful styles some of which are paintings by famous artists like Starry Night by Van Gogh. Trying to explain this concept just with words might be difficult.\n\n\n3. As you can see , there are two input images namely content image and style image that are used to a generate a new image called stylized image. A few things to notice about this image is that it has the same content as the content image and has a style similar to that of the style image. It looks good and we are pretty sure it\u2019s not achieved by overlapping these two images so how do we get here what is the math behind this idea? To answer these question we need to take a step back and focus on what does a convolution neural network actually learn? What are these Convolution layers truly encoding in form of feature maps or kernels that lets them do this or stating this in another way What representations do CNNs learn when we input an image, let\u2019s try to understand that first. Convolutional Neural Networks were originally created for classification of images and have lately been used in a variety of other tasks like Image Segmentation, Neural Style and other computer vision and Natural Language Processing tasks as well. CNNs are one of the most interpretable models in Deep Learning because of our ability to visual their representations and understand what they might be learning.\n![Example 1](https:\/\/miro.medium.com\/max\/875\/0*F3xvwBKFhaQ3Mh_k)","da9cc9a2":"# Neural Style Transfer\nThis tutorial uses deep learning to compose one image in the style of another image (ever wish you could paint like Picasso or Van Gogh?). This is known as neural style transfer and the technique is outlined in A Neural Algorithm of Artistic Style ([Gatys et al](https:\/\/arxiv.org\/abs\/1508.06576).)\n\n","8f57923d":"In case you understood something or get excited or even just thought to read till here, a big thank you. Please upvote and comment for any clarification. I know my underatnding isnt complete and in future will try to improve. I belive the lack of readable jupyter tutorial was present in the field due to which i made this. I hope you like my work. If you would like to see more notebooks I have build through please check [this](https:\/\/www.kaggle.com\/tathagatbanerjee\/code) \n\n\nCheers\n\nTathagat","cb94463f":"# Intro to Transfer Learning\n\nFor intro to transfer learning check out:\n* [Blood cell Classification with TF](https:\/\/www.kaggle.com\/tathagatbanerjee\/blood-cell-detection-100-acc-transfer-learning)\n* [Hand Recognition using TF](https:\/\/www.kaggle.com\/tathagatbanerjee\/hand-recognition-cnn-model-plot-feature-plot)\n\nIf your good to go with python and Transfer Learning lets move forward: \nAlthough the use of different transfer learing algorithms, techniques and optimization is vividly used in the domain but in 2016 gatys first thought about what if we freeze the initialised weights and optimise the image. The idea is to tender a loss built over content and style images and optimise the image to minimise the loss. \nChillax if you dont understand and find it latin, lets go slow and see what Transfer learning especially vgg model has for us\n\n## VGG\n![VGG16](https:\/\/miro.medium.com\/max\/875\/0*sgoGO_rnawcajCbH)\n1. Now let\u2019s consider the 10th convolution layer of vgg16 which uses a 3x3 kernel with 512 feature maps to train and finally generates a output of 28X28x512 image representation, just for sake of simplicity let\u2019s assume that there are certain units in this 10th layer which gets activated by an image containing circles like wheel of a car or there might be some which get activated by an image having some pattern similar to three intersecting lines etc.\n2. It\u2019s safe to assume that CNN does not learn to encode what image is but it actually learns to encode what image represents or what contents are visible in the image and due to inherent nonlinear nature of neural networks has we go from shallow layers to deeper layers the hidden units become capable to detect more and more complex feature from a given image.","5e6f79bd":"# Hyperparameter"}}