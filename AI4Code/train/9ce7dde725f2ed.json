{"cell_type":{"f76014ea":"code","47974140":"code","837412a4":"code","71250775":"code","637abd15":"code","4ebe026e":"code","00a2e5fe":"code","21104cf9":"code","1c7dff84":"code","ce871209":"code","6830fa52":"code","f6aabefa":"code","fab4670d":"code","51334708":"code","49f01139":"code","0c13fb59":"code","6e04d3bd":"code","8397c91e":"code","3638af28":"code","d5265461":"code","91eba184":"code","b5773f0b":"code","ca9a971c":"code","1e82c8d1":"markdown","39f53beb":"markdown","ac82acf5":"markdown","b7c1857a":"markdown","4c56ff41":"markdown","fc55ff91":"markdown","a4ca0acc":"markdown","f916042e":"markdown","a526b621":"markdown","9011e961":"markdown","b807a5cc":"markdown"},"source":{"f76014ea":"# Asthetics\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Basic\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport json\nimport os\nimport random\nfrom tqdm.autonotebook import tqdm\nimport string\nimport re\nfrom functools import partial\n\n# Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom wordcloud import WordCloud, STOPWORDS\n\n# NLP\nimport spacy\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nnlp.max_length = 40000000","47974140":"RANDOM_SEED = 42","837412a4":"def seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)","71250775":"seed_everything()","637abd15":"train_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nsample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","4ebe026e":"train_df.head(10)","00a2e5fe":"def read_append_return(filename, train_files_path=train_files_path, output='text'):\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","21104cf9":"tqdm.pandas()\ntrain_df['text'] = train_df['Id'].progress_apply(read_append_return)","1c7dff84":"train_df.head(10)","ce871209":"tqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_files_path))","6830fa52":"sample_sub.head()","f6aabefa":"# train_df.to_csv('Data_with_text.csv')","fab4670d":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text","51334708":"tqdm.pandas()\ntrain_df['text'] = train_df['text'].progress_apply(text_cleaning)","49f01139":"# tqdm.pandas()\n# sample_sub['text'] = sample_sub['text'].progress_apply(text_cleaning)","0c13fb59":"text = ' '.join(train_df['text'].sample(frac=0.3))\nwordcloud = WordCloud(background_color='white', stopwords=STOPWORDS, width=2560, height=1440).generate(text)\n\nbarplot_dim = (15, 15)\nax = plt.subplots(figsize=barplot_dim, facecolor='w')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","6e04d3bd":"def prepare_text(text, nlp=nlp):\n    '''\n    Returns the text after stop-word removal and lemmatization.\n    text - Sentence to be processed\n    nlp - Spacy NLP model\n    '''\n    doc = nlp(text)\n    lemma_list = [token.lemma_ for token in doc if not token.is_stop]\n    lemmatized_sentence = ' '.join(lemma_list)\n    \n    return lemmatized_sentence","8397c91e":"# tqdm.pandas()\n# train_df['text'] = train_df['text'].progress_apply(prepare_text)","3638af28":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","d5265461":"temp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\n'''\nIdea below of also using the 'dataset_title' is burrowed from\nhttps:\/\/www.kaggle.com\/josephassaker\/coleridge-initiative-eda-na-ve-submission\n'''\ntemp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\ntemp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\n\nexisting_labels = set(temp_1 + temp_2 + temp_3)\nid_list = []\nlables_list = []\nfor index, row in tqdm(sample_sub.iterrows()):\n    sample_text = row['text']\n    row_id = row['Id']\n    temp_df = train_df[train_df['text'] == text_cleaning(sample_text)]\n    cleaned_labels = temp_df['cleaned_label'].to_list()\n    for known_label in existing_labels:\n        if known_label in sample_text.lower():\n            cleaned_labels.append(clean_text(known_label))\n    cleaned_labels = [clean_text(x) for x in cleaned_labels]\n    cleaned_labels = set(cleaned_labels)\n    lables_list.append('|'.join(cleaned_labels))\n    id_list.append(row_id)","91eba184":"submission = pd.DataFrame()\nsubmission['Id'] = id_list\nsubmission['PredictionString'] = lables_list","b5773f0b":"submission.head()","ca9a971c":"submission.to_csv('submission.csv', index=False)","1e82c8d1":"Let's get the text data from json files and append them to the table.","39f53beb":"As we can see there are metions of 'et al' pretty significantly in the test of papers. Which is in fact related to quoting papers. This this should be a significant factor in determining the citation titles. Let, hope so...","ac82acf5":"Let's have a look at the training data csv file...","b7c1857a":"Let's save the data now in case we needed that while creating model later.","4c56ff41":"# Model\nThis is a very naive model based on the assumption that topics having names of label or dataset_title in their content most porobaby are citing the same sources.","fc55ff91":"# Data Description\n\ntrain.csv -labels and metadata for the training set\ntrain\/tezt directory - the full text of the training\/test set's publications in JSON format, broken into sections with section titles\n* `id` - publication id - note that there are multiple rows for some training documents, indicating multiple mentioned datasets.\n* `pub_title` - title of the publication (a small number of publications have the same title).\n* `dataset_title` - the title of the dataset that is mentioned within the publication.\n* `dataset_label` - a portion of the text that indicates the dataset.\n* `cleaned_label` - the dataset_label, as passed through the clean_text function from the [Evaluation page](https:\/\/www.kaggle.com\/c\/coleridgeinitiative-show-us-the-data\/overview\/evaluation).\n\nsample_submission.csv - a sample submission file in the correct format.\n* `Id` - publication id.\n* `PredictionString` - To be filled with equivalent of `cleaned_label` of train data.","a4ca0acc":"# Preparing text","f916042e":"This notebook is a work in progress... This is just a first pass through the data to see what is the situation, along with a very Naive model. Even with that, trust me this used to have LB = 1.0 at some point in time. \ud83d\ude06\ud83d\ude1b  \n\n**If you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels.** \ud83d\ude0a","a526b621":"# Data Cleaning","9011e961":"# About this Notebook\nThis is a first run through the compeition to try and understand the datatset and realise the problem at hand.","b807a5cc":"## Generating Word cloud"}}