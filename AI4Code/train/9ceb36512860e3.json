{"cell_type":{"4c9527a9":"code","6f7e4f63":"code","e3826782":"code","0c9c7217":"code","a3a6699c":"code","d9b116f0":"code","c9431aa3":"code","88c2a1a1":"code","6d740470":"code","d0032f1f":"code","6538d73b":"code","c119f959":"code","49328698":"code","0fe535e8":"markdown","bd18cc9b":"markdown","ab46b6ea":"markdown","7841aec5":"markdown","864724c2":"markdown","fc5e97cb":"markdown","6e6759bd":"markdown"},"source":{"4c9527a9":"import pandas as pd\nimport numpy as np\nimport json\nimport csv\nimport gc\n\nfrom collections import OrderedDict\nfrom tqdm import tqdm_notebook as tqdm\n\n# dtypes for pd.read_csv\n# These are help to reduce the memory usage.\nDTYPES_RAW = {\n    'event_id': 'object',\n    'game_session': 'object',\n    'installation_id': 'object',\n    'event_count': np.uint16,\n    'event_code': np.uint16,\n    'game_time': np.uint32,\n    'type': 'category',\n    'world': 'category',\n    'title': 'category',  \n}","6f7e4f63":"# Extract these arguments from JSON.\n# There is not enough memory to extract everything with this method.\n# You should try it whether it can process the private test set too\n# with your selected arguments\nFIELDS = {\n    # Extras from JSON\n    # If you add more data, do not forget\n    # to add default values below.\n    'level': np.uint8,\n    'round': np.uint8,\n    'correct': np.int8,\n    'misses': np.int8,\n    \n    # Nested object separated by '_'\n    # for example: {'coordinates': {'x': 12, 'y': 12}}\n    # 'coordinates_x': np.uint16\n    # 'coordinates_y': np.uint16\n}\n\nDTYPES = OrderedDict( (dt[0], (dt[1], i)) for i, dt in enumerate(FIELDS.items()))","e3826782":"# This only needs if you want to show a TQDM progress bar.\nimport subprocess\n\ndef file_len(fname):\n    \"\"\"Returns the number of lines in a file.\n       @see: https:\/\/www.kaggle.com\/szelee\/how-to-import-a-csv-file-of-55-million-rows\n    \"\"\"\n    p = subprocess.Popen(['wc', '-l', fname], stdout=subprocess.PIPE, \n                                              stderr=subprocess.PIPE)\n    result, err = p.communicate()\n    if p.returncode != 0:\n        raise IOError(err)\n    return int(result.strip().split()[0])+1","0c9c7217":"def flatten(dct, res, separator='_'):\n    \"\"\"Flatten a dictionary.\n       @see: https:\/\/stackoverflow.com\/a\/34094630\/4158850\n    \"\"\"\n    queue = [('', dct)]\n\n    while queue:\n        prefix, d = queue.pop()\n        for k, v in d.items():\n            key = prefix + k\n            if not isinstance(v, dict):\n                if key in FIELDS.keys():\n                    res[0][DTYPES[key][1]] = v\n            else:\n                queue.append((key + separator, v))\n\n    return res\n\ndef records_from_json(fh, n_rows, event_ids_to_drop):\n    \"\"\"Yields the records from a file object.\"\"\"\n    rows = csv.reader(fh, delimiter=',')\n    skip_header = next(rows)\n    \n    # define dtype for more memory-efficiency.\n    dtype = dict(names=list(FIELDS.keys()), formats=list(FIELDS.values()))\n    defrow = np.zeros((1,), dtype=dtype)\n\n    for event_id, game_session, timestamp, event_data, installation_id, event_count, event_code, game_time, title, typ, world in tqdm(rows, total=n_rows):\n        \n        # It is more memory-efficient if we don't use the the train df's columns yet.\n        row = defrow.copy()\n\n        # Default (required because of the copy above) values for the extracted data\n        # you can use np.nan too (in this case the dtype should be np.float64)\n        row[0][DTYPES['level'][1]] = 0\n        row[0][DTYPES['round'][1]] = 0\n        row[0][DTYPES['correct'][1]] = -1\n        row[0][DTYPES['misses'][1]] = -1\n\n        if event_id not in event_ids_to_drop:\n            row = flatten(json.loads(event_data), row)\n\n        yield row[0]\n\ndef from_records(path, event_ids_to_drop):\n    n_rows = file_len(path)\n    with open(path) as fh:\n        return pd.DataFrame.from_records(records_from_json(fh, n_rows, event_ids_to_drop))\n","a3a6699c":"specs = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/specs.csv')\nspecs.args = specs.args.apply(lambda x: json.loads(str(x)))\neventIdsToDrop = []\n\nfor _, spec in specs.iterrows():\n    j = pd.io.json.json_normalize(spec.args)\n    vals = j.loc[(j.name.isin(FIELDS.keys()))].name.values\n\n    if len(vals) == 0:\n        eventIdsToDrop += [spec.event_id]\n\nset(eventIdsToDrop)\nprint(len(eventIdsToDrop))","d9b116f0":"extras_df = from_records('\/kaggle\/input\/data-science-bowl-2019\/train.csv', eventIdsToDrop)","c9431aa3":"train_df = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv', parse_dates=['timestamp'], dtype=DTYPES_RAW, usecols=['timestamp'] + list(DTYPES_RAW.keys()))","88c2a1a1":"train_df = train_df.merge(extras_df, left_index=True, right_index=True)\ntrain_df.info()","6d740470":"train_df.to_csv('train_extras.csv', index=False)","d0032f1f":"del extras_df\ndel train_df\ngc.collect()\n\n%reset -f Out","6538d73b":"extras_df = from_records('\/kaggle\/input\/data-science-bowl-2019\/test.csv', eventIdsToDrop)","c119f959":"test_df = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv', parse_dates=['timestamp'], dtype=DTYPES_RAW, usecols=['timestamp'] + list(DTYPES_RAW.keys()))\ntest_df = test_df.merge(extras_df, left_index=True, right_index=True)\n\ntest_df.to_csv('test_extras.csv', index=False)","49328698":"del extras_df\ndel test_df\ngc.collect()\n\n%reset -f Out","0fe535e8":"### Train","bd18cc9b":"This is [Miika](https:\/\/www.kaggle.com\/taenareus)'s idea, see his comment below.\n> You can speed up processing significantly by not parsing rows that do not have json data of interest.\n> To determine which event_ids to drop, just read the `spec.csv` file!\n>\n> Using this trick allows you to parse all relevant data in a small number of minutes.","ab46b6ea":"-------------------------------------","7841aec5":"# Memory efficient, faster way (<10 min) to extract JSON data\nIn this notebook I'll show you a solution for handling the JSON event-data. The goal is a fast, memory-efficient way to load and prepare the train (or test) dataframe.\n\n\n- It loads and converts the selected JSON arguments into a dataframe less then 10 minutes.\n- It keeps the memory usage as low as possible (the final train dataframe is ~500 Mb)","864724c2":"**Thanks for reading**","fc5e97cb":"#### Extract JSON event data","6e6759bd":"### Test"}}