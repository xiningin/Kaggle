{"cell_type":{"51347420":"code","213584f7":"code","ae4c83af":"code","b2e94fd8":"code","cfecd8fc":"code","5b36bea4":"code","10b259ba":"code","ca179217":"code","cc981e64":"code","b4f5b0fc":"code","66aa1714":"code","b12f2efd":"code","c5c03172":"code","a082cd2b":"code","74033da6":"code","6077e9ca":"code","2af5a7a3":"code","135f831b":"code","606280f4":"code","8447285e":"code","b6490744":"code","b012d7f3":"code","d298bee7":"code","b42552cf":"code","e0f72d1e":"code","9079033c":"code","f1ead0fb":"code","58498df1":"code","042caff0":"code","5a43aebf":"code","00b42251":"code","1d01a840":"code","debe03fc":"code","504f5500":"code","86f843d7":"code","2bf32aeb":"code","7d4823e1":"code","3156726d":"code","94e79683":"code","3ca754ce":"code","b1e7bf7b":"code","7ec4b5d8":"code","904b33a0":"code","0e7b6d61":"code","d3e0d07d":"code","c66989ea":"code","9951a939":"code","3c750aca":"code","3d901e8b":"code","33c7d822":"code","9e63be1f":"code","16eed44b":"code","4787d8e7":"code","6e05f98e":"code","a99c087a":"code","d1d04d82":"code","b8bb73d1":"code","d3a8d3ce":"code","2d088081":"code","7284fce7":"code","cf8cfe87":"code","a88260b2":"code","53f064c1":"code","82113043":"code","77f04ab5":"code","259f99fb":"code","5cb9f53a":"code","a9960e3f":"code","be8fabd6":"code","01b4ed56":"code","0a97ad0d":"code","06bbc780":"code","6d8dff3f":"code","64fbd6cf":"code","156bc4c6":"code","465d5829":"code","a6d8f491":"code","adafa6d6":"code","523d80e2":"code","b891f533":"code","29445562":"code","8f875779":"code","3b41e304":"code","05c59ba3":"code","866d2173":"code","b638968d":"code","94d789b4":"code","a4677f92":"code","5cd67296":"code","78698866":"code","003a9e43":"code","a53f5a26":"code","f0e62bb0":"code","0df1d9fa":"code","3cce90db":"code","2d626145":"code","2480109f":"code","f0c1f859":"code","e2246308":"code","210b43ff":"code","9a36687c":"code","1b64f789":"code","8b2a70ce":"code","c6d822ac":"code","349e8d91":"code","c6b1b7e6":"code","ee0f7b0d":"code","5d00c3f1":"code","b52c2a6c":"code","d259f79d":"code","7a6a787b":"code","3c8ebb79":"code","ad142a7c":"code","14067ea7":"code","4337f5e4":"code","7267adce":"code","465fbb1a":"code","b56fe375":"code","eb03102f":"code","93720a1e":"code","68bb25a2":"code","3cf1c035":"code","4e7ec249":"code","4117727c":"code","a65e3262":"code","2eba5d8d":"code","9bf25a51":"code","9b9943d8":"code","80a7d57a":"code","97a5213c":"code","1b095a10":"code","28507fc4":"code","9e032eaf":"code","ccba5e9a":"code","fe7cb6c6":"code","42072108":"code","b7827dbb":"code","652efbe1":"code","b98adb0f":"code","695dfe63":"code","6033c7a0":"code","30445baf":"code","f6b0deb6":"code","4e3ae4fd":"code","cc248c8d":"code","2fe1afef":"code","23c261c9":"code","77752a12":"code","e43c088a":"code","b4472a3b":"code","9a0a922c":"code","e7a32e86":"code","7119f8af":"code","bde17db3":"code","8e501879":"code","620eb23a":"code","5e206bcc":"code","76e08485":"code","3b83c552":"code","dd7093ac":"code","905f4124":"code","39205308":"code","f286c80a":"code","2a77ed34":"code","3ae456fe":"code","468f6066":"code","66041559":"code","032f010b":"code","5fdd09ca":"code","2c4e46cf":"code","ee12be12":"code","7e63912d":"code","e7eadd14":"code","e18b382f":"code","6327ce1f":"code","9108b2bc":"code","0a641bf7":"code","990947d9":"code","c250e23b":"code","052fcfa9":"code","0aff8105":"code","fca3942a":"code","1da447e6":"code","ce5b7ab1":"code","acf0d85b":"code","2722c3af":"code","064b23b0":"code","270e5e21":"code","abdd1b2e":"code","d05cc6a0":"code","867879d0":"code","bb9d53d6":"code","46c444c8":"code","150d9305":"code","127c3217":"code","53aac988":"code","08dcd286":"code","f71bbb10":"code","c12beda5":"code","f73aa711":"markdown","4979d3f2":"markdown","cf75143f":"markdown","58e3d062":"markdown","39cdb5b5":"markdown","a03040a6":"markdown","4d17e3df":"markdown","1d967cfc":"markdown","3ec09dd0":"markdown","7128edea":"markdown","7db724cd":"markdown","6846b677":"markdown","a8e9ef34":"markdown","a376013a":"markdown","64ca8123":"markdown","3a61fed0":"markdown","7ece6f02":"markdown","e4857ce9":"markdown","886cbd69":"markdown","f52927fd":"markdown","1b5234c9":"markdown","7fd38fc1":"markdown","069b01f5":"markdown","8aa81d16":"markdown","54ee8e15":"markdown","639210e2":"markdown","2387caa5":"markdown","d0b53e3e":"markdown","f0146c8e":"markdown","5471de32":"markdown","c9caaf66":"markdown","82c7d839":"markdown","b816ba0c":"markdown","f804ef0b":"markdown","977fc6d1":"markdown","af290d1e":"markdown","852a8d9f":"markdown","41b986de":"markdown","c19335c2":"markdown","54a8db86":"markdown","7fce0da8":"markdown","25b27551":"markdown","4b4e4df5":"markdown","2d10b553":"markdown","79f1c502":"markdown","0bcad534":"markdown","26ba10ee":"markdown","b00df495":"markdown","395756af":"markdown"},"source":{"51347420":"import numpy as np \nimport pandas as pd\n\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n# Plotly Libraris\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom plotly.subplots import make_subplots\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","213584f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae4c83af":"prog_books=pd.read_csv(\"\/kaggle\/input\/top-270-rated-computer-science-programing-books\/prog_book.csv\")\ndf=prog_books.copy()\ndf.head()","b2e94fd8":"df.info()","cfecd8fc":"# Columns Types\ndf.dtypes","5b36bea4":"# Size Of Data Set\ndf.shape","10b259ba":"# Columns Names\ndf.columns","ca179217":"# Duplicated data\ndf[df.duplicated() == True]","cc981e64":"df.isnull().values.any()","b4f5b0fc":"df.isnull().sum()","66aa1714":"def missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data(df)","b12f2efd":"# Removing , from the Reviews so that reviews can be converted to Int values\n\ndf['Reviews']=df['Reviews'].map(lambda x: x.replace(',',''))\ndf['Reviews']=pd.to_numeric(df['Reviews'])","c5c03172":"df.Reviews.value_counts()","a082cd2b":"df_corr=df.copy()\ndf_Reviews=df_corr[\"Reviews\"]","74033da6":"point_0=(df_Reviews==0)","6077e9ca":"df[point_0]","2af5a7a3":"df_no_0=df[df[\"Reviews\"]>0]\ndf_no_0.mean()","135f831b":"df_Reviews[point_0]","606280f4":"from math import *\ndf_Reviews[point_0]=ceil(df_no_0.Reviews.mean())","8447285e":"df_Reviews[point_0]","b6490744":"df_corr[\"Reviews\"]=df_Reviews\ndf_corr[point_0]","b012d7f3":"plt.rcParams['figure.figsize']=(8,6)\nsns.heatmap(df_corr.corr(),cmap='coolwarm',linewidths=.5,fmt=\".2f\",annot = True);","d298bee7":"df_corr.describe().T","b42552cf":"fig = go.Figure(data=[go.Histogram(x=df['Rating'],  \n                                  marker_color=\"Crimson\",\n                       xbins=dict(\n                      start=0, #start range of bin\n                      end=5,  #end range of bin\n                      size=0.25   #size of bin\n                      ))])\nfig.update_layout(title=\"Distribution Of Rating\",xaxis_title=\"Rating\",yaxis_title=\"Counts\",title_x=0.5)\nfig.show()","e0f72d1e":"fig = px.histogram(df, x=\"Rating\", color=\"Type\", marginal=\"rug\", # can be `box`, `violin`\n                         hover_data=[\"Book_title\",\"Type\",\"Price\"])\nfig.update_layout(title = \"Rating Distribution With Type \",title_x=0.5,\n                 xaxis_title=\"Rating\",yaxis_title=\"Count\")\nfig.show()","9079033c":"min_Rating=df['Rating'].min()\nmax_Rating=df['Rating'].max()\navg_rate_rating=df['Rating'].mean()\nfig = go.Figure()\n\nfig.add_trace(go.Indicator(\n    mode = \"number+gauge\", value =  min_Rating,\n    domain = {'x': [0.25, 1], 'y': [0.4, 0.6]},\n    title = {'text': \"Min Rating\",'font':{'color': 'black','size':15}},\n     number={'font':{'color': 'black'}},\n    gauge = {\n        'shape': \"bullet\",\n        'axis': {'range': [None, 5]},\n        'bar': {'color': \"blue\"}}))\n\nfig.add_trace(go.Indicator(\n    mode = \"number+gauge\", value = max_Rating,\n    domain = {'x': [0.25, 1], 'y': [0.7, 0.9]},\n    title = {'text': \"Max Rating\",'font':{'color': 'black','size':15}},\n    number={'font':{'color': 'black'}},\n    gauge = {\n        'shape': \"bullet\",\n        'axis': {'range': [None,5]},\n        'bar': {'color': \"cyan\"}}))\n\nfig.add_trace(go.Indicator(\n    mode = \"number+gauge\", value = avg_rate_rating,\n    domain = {'x': [0.25, 1], 'y': [0.1, 0.3]},\n    title = {'text' :\"Mean Rating \",'font':{'color': 'black','size':15}},\n     number={'font':{'color': 'black'}},\n    gauge = {\n        'shape': \"bullet\",\n        'axis': {'range': [None,5]},\n        'bar': {'color': \"Gold\"}}\n))\nfig.update_layout(title=\" Statistics Of Ratings Of Books  \",title_x=0.5)\nfig.show()","f1ead0fb":"df_Rating_Book_title=df[[\"Rating\",\"Book_title\",\"Reviews\"]]\ndf_Rating_Book_title=df_Rating_Book_title.sort_values(\"Rating\",ascending=False)[0:10]\n\n\nfig = make_subplots(rows=2, cols=1,\n                   subplot_titles=(\" Fake Top 10 Reviewed Book Counts\",\n                                   \"Fake Top 10 Rating Books \",))  # Subplot titles\n\nfig.add_trace(go.Bar(\n    y=df_Rating_Book_title['Book_title'],x=df_Rating_Book_title['Reviews'],\n    name=\"Review Counts\",\n    orientation=\"h\",\n    marker={'color': df_Rating_Book_title['Rating'], \n    'colorscale': 'solar'},\n    text=df_Rating_Book_title['Reviews'],\n    textposition = \"outside\"),\n    row=1, col=1           \n)\nfig.add_trace(go.Bar(\n    y=df_Rating_Book_title['Book_title'],x=df_Rating_Book_title['Rating'],\n    name=\"Rating Books\",\n    orientation=\"h\",\n    marker={'color': df_Rating_Book_title['Rating'], \n    'colorscale': 'darkmint'},\n    text=df_Rating_Book_title['Rating'],\n    textposition = \"outside\"),\n    row=2, col=1           \n)\n\nfig.update_layout(height=800, width=600,title =\"Books Ratings\",title_x=0.5)\nfig.show()","58498df1":"df_Rating_Book_title=df[[\"Rating\",\"Book_title\",\"Reviews\"]]\n# nop= number of people\nfilter_N_O_P=df_Rating_Book_title[df_Rating_Book_title[\"Reviews\"]>250]\nfilter_N_O_P=filter_N_O_P.sort_values(\"Rating\",ascending=False)[0:10]\n\nfig = make_subplots(rows=2, cols=1,\n                   subplot_titles=(\" Top 10 Reviewed Book Counts\",\n                                   \" Top 10 Rating Books \",))  # Subplot titles\n\nfig.add_trace(go.Bar(\n    y=filter_N_O_P['Book_title'],x=filter_N_O_P['Reviews'],\n    name=\"Review Counts\",\n    orientation=\"h\",\n    marker={'color': filter_N_O_P['Rating'], \n    'colorscale': 'haline'},\n    text=filter_N_O_P['Reviews'],\n    textposition = \"outside\"),\n    row=1, col=1           \n)\nfig.add_trace(go.Bar(\n    y=filter_N_O_P['Book_title'],x=filter_N_O_P['Rating'],\n    name=\"Rating Books\",\n    orientation=\"h\",\n    marker={'color': filter_N_O_P['Rating'], \n    'colorscale': 'tropic'},\n    text=filter_N_O_P['Rating'],\n    textposition = \"outside\"),\n    row=2, col=1           \n)\n\nfig.update_layout(height=800, width=600,title =\" Best Books \",title_x=0.5)\nfig.show()","042caff0":"df_Rating_Book_title=df[[\"Rating\",\"Book_title\",\"Reviews\"]]\n# nop= number of people\nfilter_N_O_P=df_Rating_Book_title[df_Rating_Book_title[\"Reviews\"]>250]\nfilter_N_O_P=filter_N_O_P.sort_values(\"Rating\",ascending=True)[0:5]\n\nfig = make_subplots(rows=2, cols=1,\n                   subplot_titles=(\"  Worst 5 Reviewed Book Counts\",\n                                   \"  Worst 5 Rating Books \",))  # Subplot titles\n\nfig.add_trace(go.Bar(\n    y=filter_N_O_P['Book_title'],x=filter_N_O_P['Reviews'],\n    name=\"Review Counts\",\n    orientation=\"h\",\n    marker={'color': filter_N_O_P['Rating'], \n    'colorscale': 'haline'},\n    text=filter_N_O_P['Reviews'],\n    textposition = \"outside\"),\n    row=1, col=1           \n)\nfig.add_trace(go.Bar(\n    y=filter_N_O_P['Book_title'],x=filter_N_O_P['Rating'],\n    name=\"Rating Books\",\n    orientation=\"h\",\n    marker={'color': filter_N_O_P['Rating'], \n    'colorscale': 'tropic'},\n    text=filter_N_O_P['Rating'],\n    textposition = \"outside\"),\n    row=2, col=1           \n)\n\nfig.update_layout(height=800, width=700,title =\" Worst Books\",title_x=0.5)\nfig.show()","5a43aebf":"df_type=df['Type'].value_counts().to_frame().reset_index().rename(columns={'index':'Type','Type':'count'})\n\n\nfig = go.Figure(go.Bar(\n    x=df_type['Type'],y=df_type['count'],\n    marker={'color': df_type['count'], \n    'colorscale': 'inferno'},  \n    text=df_type['count'],\n    textposition = \"outside\",\n))\nfig.update_layout(title_text='Type Count',xaxis_title=\"Type\",yaxis_title=\"Count\",title_x=0.5)\nfig.update_xaxes(\n        tickangle = 45,\n        )\nfig.show()","00b42251":"df_type=df['Type'].value_counts().to_frame().reset_index().rename(columns={'index':'Type','Type':'count'})\n\ncolors=['Bisque','darkcyan',\"Brown\",\"cyan\",\"coral\",\"darkblue\"]\n\nfig = go.Figure([go.Pie(labels=df_type['Type'], values=df_type['count'])])\nfig.update_traces(hoverinfo='label+percent', textinfo='percent+value', textfont_size=15,\n                 marker=dict(colors=colors, line=dict(color='#000000', width=2)))\nfig.update_layout(title=\"Type Count\",title_x=0.5)\nfig.show()","1d01a840":"df_type_max=df.groupby(by =['Type'])['Rating'].max().to_frame().reset_index().rename(columns={'Type':'Type','Rating':'max'})\ndf_type_mean=df.groupby(by =['Type'])['Rating'].mean().to_frame().reset_index().rename(columns={'Type':'Type1','Rating':'mean'})\ndf_type_min=df.groupby(by =['Type'])['Rating'].min().to_frame().reset_index().rename(columns={'Type':'Type2','Rating':'min'})\ndf_type_count=df.groupby(by =['Type'])['Rating'].count().to_frame().reset_index().rename(columns={'Type':'Type3','Rating':'count'})\n\nresult = pd.concat([df_type_max, df_type_mean,df_type_min,df_type_count], axis=1)\nresult.drop(['Type1','Type2','Type3'],inplace=True,axis=1)\nresult[\"max\"]=result[\"max\"].map(lambda x:round(x,2))\nresult[\"mean\"]=result[\"mean\"].map(lambda x:round(x,2))\nresult[\"min\"]=result[\"min\"].map(lambda x:round(x,2))\n\nfig = make_subplots(rows=4, cols=1,\n                   subplot_titles=(\" Mean Rating\",\n                                   \" Min Rating\",\n                                   \" Max Rating\",\n                                   \" Count Books\"))  # Subplot titles\n\nfig.add_trace(go.Bar(\n    x=result['Type'],y=result['mean'],\n    name=\"Mean\",\n    marker={'color': result['mean'], \n    'colorscale': 'fall'},  \n    text=result['mean'],\n    textposition = \"inside\"),\n    row=1, col=1         \n)\nfig.add_trace(go.Bar(\n    x=result['Type'],y=result['min'],\n    name=\"Min\",\n    marker={'color': result['min'], \n    'colorscale': 'fall'},  \n    text=result['min'],\n    textposition = \"inside\"),\n    row=2, col=1         \n)\nfig.add_trace(go.Bar(\n    x=result['Type'],y=result['max'],\n    name=\"Max\",\n    marker={'color': result['max'], \n    'colorscale': 'fall'},  \n    text=result['max'],\n    textposition = \"inside\"),\n    row=3, col=1           \n)\nfig.add_trace(go.Bar(\n    x=result['Type'],y=result['count'],\n    name=\"Count\",\n    marker={'color': result['count'], \n    'colorscale': 'fall'},  \n    text=result['count'],\n    textposition = \"inside\"),\n    row=4, col=1           \n)\nfig.update_layout(title = \"Type With Rating\",title_x=0.5)\nfig.update_xaxes(\n        tickangle = 0,\n        )\nfig.show()","debe03fc":"df_type_max=df.groupby(by =['Type'])['Price'].max().to_frame().reset_index().rename(columns={'Type':'Type','Price':'max'})\ndf_type_mean=df.groupby(by =['Type'])['Price'].mean().to_frame().reset_index().rename(columns={'Type':'Type1','Price':'mean'})\ndf_type_min=df.groupby(by =['Type'])['Price'].min().to_frame().reset_index().rename(columns={'Type':'Type2','Price':'min'})\nresult = pd.concat([df_type_max, df_type_mean,df_type_min], axis=1)\nresult.drop(['Type1','Type2',],inplace=True,axis=1)\nresult[\"max\"]=result[\"max\"].map(lambda x:round(x,2))\nresult[\"mean\"]=result[\"mean\"].map(lambda x:round(x,2))\nresult[\"min\"]=result[\"min\"].map(lambda x:round(x,2))\n\nfig = make_subplots(rows=3, cols=1,\n                   subplot_titles=(\"  Mean Price\",\n                                   \"  Min Price\",\n                                   \"  Max Price\", ))  # Subplot titles\n                                 \n\nfig.add_trace(go.Bar(\n    x=result['Type'],y=result['mean'],\n    name=\"Mean\",\n    marker={'color': result['mean'], \n    'colorscale': 'fall'},  \n    text=result['mean'],\n    textposition = \"inside\"),\n    row=1, col=1         \n)\nfig.add_trace(go.Bar(\n    x=result['Type'],y=result['min'],\n    name=\"Min\",\n    marker={'color': result['min'], \n    'colorscale': 'fall'},  \n    text=result['min'],\n    textposition = \"inside\"),\n    row=2, col=1         \n)\nfig.add_trace(go.Bar(\n    x=result['Type'],y=result['max'],\n    name=\"Max\",\n    marker={'color': result['max'], \n    'colorscale': 'fall'},  \n    text=result['max'],\n    textposition = \"inside\"),\n    row=3, col=1           \n)\n\nfig.update_layout(title = \"Type With Price\",title_x=0.5)\nfig.update_xaxes(\n        tickangle = 0,\n        )\nfig.show()","504f5500":"df_type_max=df.groupby(by =['Type'])['Number_Of_Pages'].max().to_frame().reset_index().rename(columns={'Type':'Type','Number_Of_Pages':'max'})\ndf_type_mean=df.groupby(by =['Type'])['Number_Of_Pages'].mean().to_frame().reset_index().rename(columns={'Type':'Type1','Number_Of_Pages':'mean'})\ndf_type_min=df.groupby(by =['Type'])['Number_Of_Pages'].min().to_frame().reset_index().rename(columns={'Type':'Type2','Number_Of_Pages':'min'})\nresult = pd.concat([df_type_max, df_type_mean,df_type_min], axis=1)\nresult.drop(['Type1','Type2',],inplace=True,axis=1)\nresult[\"max\"]=result[\"max\"].map(lambda x:round(x,2))\nresult[\"mean\"]=result[\"mean\"].map(lambda x:round(x,2))\nresult[\"min\"]=result[\"min\"].map(lambda x:round(x,2))\nfig = make_subplots(rows=3, cols=1,\n                   subplot_titles=(\" Mean Number Of Pages\",\n                                   \" Min Number Of Pages\",\n                                   \" Max Number Of Pages\", ))  # Subplot titles                          \n\nfig.add_trace(go.Bar(\n    x=result['Type'],y=result['mean'],\n     name='Mean',\n    marker={'color': result['mean'], \n    'colorscale': 'fall'},  \n    text=result['mean'],\n    textposition = \"inside\"),\n    row=1, col=1         \n)\nfig.add_trace(go.Bar(\n    x=result['Type'],y=result['min'],\n    name='Min',\n    marker={'color': result['min'], \n    'colorscale': 'fall'},  \n    text=result['min'],\n    textposition = \"inside\"),\n    row=2, col=1         \n)\nfig.add_trace(go.Bar(\n    x=result['Type'],y=result['max'],\n    name='Max',\n    marker={'color': result['max'], \n    'colorscale': 'fall'},  \n    text=result['max'],\n    textposition = \"inside\"),\n    row=3, col=1           \n)\n\nfig.update_layout(title = \"Type With Number Of Pages\",title_x=0.5)\nfig.update_xaxes(\n        tickangle = 0,\n        )\nfig.show()","86f843d7":"fig = px.histogram(df, x=\"Reviews\", color=\"Type\", marginal=\"rug\", # can be `box`, `violin`\n                         hover_data=[\"Book_title\",\"Type\",\"Number_Of_Pages\"])\nfig.update_layout(title = \"Reviews Distribution With Type \",title_x=0.5,\n                 xaxis_title=\"Reviews\",yaxis_title=\"Count\")\nfig.show()","2bf32aeb":"df_r0=df[df[\"Reviews\"]>0]\nmin_Rating=df_r0[\"Reviews\"].min()\nmax_Rating=df['Reviews'].max()\navg_rate_rating=df['Reviews'].mean()\nfig = go.Figure()\n\nfig.add_trace(go.Indicator(\n    mode = \"number+gauge\", value =  min_Rating,\n    domain = {'x': [0.25, 1], 'y': [0.4, 0.6]},\n    title = {'text': \"Min Reviews\",'font':{'color': 'black','size':15}},\n     number={'font':{'color': 'black'}},\n    gauge = {\n        'shape': \"bullet\",\n        'axis': {'range': [None, 5]},\n        'bar': {'color': \"blue\"}}))\n\nfig.add_trace(go.Indicator(\n    mode = \"number+gauge\", value = max_Rating,\n    domain = {'x': [0.25, 1], 'y': [0.7, 0.9]},\n    title = {'text': \"Max Reviews\",'font':{'color': 'black','size':15}},\n    number={'font':{'color': 'black'}},\n    gauge = {\n        'shape': \"bullet\",\n        'axis': {'range': [None,6000]},\n        'bar': {'color': \"cyan\"}}))\n\nfig.add_trace(go.Indicator(\n    mode = \"number+gauge\", value = avg_rate_rating,\n    domain = {'x': [0.25, 1], 'y': [0.1, 0.3]},\n    title = {'text' :\"Mean Reviews \",'font':{'color': 'black','size':15}},\n     number={'font':{'color': 'black'}},\n    gauge = {\n        'shape': \"bullet\",\n        'axis': {'range': [None,200]},\n        'bar': {'color': \"Gold\"}}\n))\nfig.update_layout(title=\" Statistics Of Reviews Of Books  \",title_x=0.5)\nfig.show()","7d4823e1":"df_r0=df[df[\"Reviews\"]>0]\nfig = go.Figure()\nfig.add_trace(go.Box(\n    y=df_r0[\"Reviews\"],\n    name='Reviews',\n    marker_color='royalblue',\n    boxmean='sd' # represent mean and standard deviation\n))\nfig.update_layout(title = \"Reviews Distribution \",title_x=0.5,\n                 )\nfig.show()","3156726d":"fig = go.Figure()\nfig.add_trace(go.Box(\n    y=df[\"Price\"],\n    name='Price',\n    marker_color='royalblue',\n    boxmean='sd' # represent mean and standard deviation\n))\nfig.update_layout(title = \"Price Distribution \",title_x=0.5,\n                 )\nfig.show()","94e79683":"fig = px.histogram(df, x=\"Price\", color=\"Type\", marginal=\"rug\", # can be `box`, `violin`\n                         hover_data=[\"Book_title\",\"Type\",\"Number_Of_Pages\"])\nfig.update_layout(title = \"Price Distribution With Type \",title_x=0.5,\n                 xaxis_title=\"Price\",yaxis_title=\"Count\")\nfig.show()","3ca754ce":"fig = px.box(df, y=\"Price\", color=\"Type\",\n             notched=True, # used notched shape\n             title=\"Box Plot Of Price With Type\",\n             hover_data=[\"Book_title\"] # add Book_title column to hover data\n            )\nfig.update_layout(title_x=0.5)\n                 \nfig.show()","b1e7bf7b":"fig = go.Figure(data=[go.Histogram(x=df['Price'],  \n                                  marker_color=\"Crimson\",\n                       xbins=dict(\n                      start=0, #start range of bin\n                      end=250,  #end range of bin\n                      size=10   #size of bin\n                      ))])\nfig.update_layout(title=\"Distribution Of Price\",xaxis_title=\"Price\",yaxis_title=\"Counts\",title_x=0.5)\nfig.show()","7ec4b5d8":"min_Rating=df['Price'].min()\nmax_Rating=df['Price'].max()\navg_rate_rating=df['Price'].mean()\nfig = go.Figure()\n\nfig.add_trace(go.Indicator(\n    mode = \"number+gauge\", value =  min_Rating,\n    domain = {'x': [0.25, 1], 'y': [0.4, 0.6]},\n    title = {'text': \"Min Price\",'font':{'color': 'black','size':15}},\n     number={'font':{'color': 'black'}},\n    gauge = {\n        'shape': \"bullet\",\n        'axis': {'range': [None, 10]},\n        'bar': {'color': \"blue\"}}))\n\nfig.add_trace(go.Indicator(\n    mode = \"number+gauge\", value = max_Rating,\n    domain = {'x': [0.25, 1], 'y': [0.7, 0.9]},\n    title = {'text': \"Max Price\",'font':{'color': 'black','size':15}},\n    number={'font':{'color': 'black'}},\n    gauge = {\n        'shape': \"bullet\",\n        'axis': {'range': [None,250]},\n        'bar': {'color': \"cyan\"}}))\n\nfig.add_trace(go.Indicator(\n    mode = \"number+gauge\", value = avg_rate_rating,\n    domain = {'x': [0.25, 1], 'y': [0.1, 0.3]},\n    title = {'text' :\"Mean Price \",'font':{'color': 'black','size':15}},\n     number={'font':{'color': 'black'}},\n    gauge = {\n        'shape': \"bullet\",\n        'axis': {'range': [None,80]},\n        'bar': {'color': \"Gold\"}}\n))\nfig.update_layout(title=\" Statistics Of Price Of Books  \",title_x=0.5)\nfig.show()","904b33a0":"fig = px.scatter_matrix(df,\n    dimensions=[\"Rating\", \"Number_Of_Pages\", \"Price\"],\n    color=\"Type\")\nfig.update_traces(diagonal_visible=False)\nfig.show()","0e7b6d61":"fig = px.treemap(df, path=['Type'], values='Price')\nfig.show()","d3e0d07d":"fig = px.scatter(df, x=\"Price\", y=\"Number_Of_Pages\", trendline=\"ols\")\nfig.update_layout(title=\"Price With Number Of Pages\",\n                  xaxis_title=\"Price\",\n                  yaxis_title=\"Number Of Pages\",\n                  title_x=0.5)\nfig.show()\n\n\n","c66989ea":"fig = px.scatter(df, x=\"Price\", y=\"Rating\", trendline=\"ols\")\nfig.update_layout(title=\"Price With Rating\",\n                  xaxis_title=\"Price\",\n                  yaxis_title=\"Rating\",\n                  title_x=0.5)\nfig.show()\n","9951a939":"fig = go.Figure()\nfig.add_trace(go.Box(\n    y=df[\"Number_Of_Pages\"],\n    name='Number Of Pages',\n    marker_color='royalblue',\n    boxmean='sd' # represent mean and standard deviation\n))\nfig.update_layout(title =\"Number Of Pages Distribution\",title_x=0.5,\n                 )\nfig.show()","3c750aca":"fig = px.box(df, y=\"Number_Of_Pages\", color=\"Type\",\n             notched=True, # used notched shape\n             title=\"Box Plot Of Number Of Pages With Type\",\n             hover_data=[\"Book_title\"] # add Book_title column to hover data\n            )\nfig.update_layout(title_x=0.5,yaxis_title=\"Number Of Pages\")\n                 \nfig.show()","3d901e8b":"fig = px.scatter(df, x=\"Rating\", y=\"Number_Of_Pages\", trendline=\"ols\")\nfig.update_layout(title=\"Rating With Number Of Pages\",\n                  xaxis_title=\"Number Of Pages\",\n                  yaxis_title=\"Rating\",\n                  title_x=0.5)\nfig.show()\n\n","33c7d822":"from wordcloud import WordCloud, STOPWORDS\nwocl=WordCloud(stopwords=STOPWORDS).generate(\" \".join(df[\"Book_title\"].tolist()))\nplt.figure(figsize=(25,10))\nplt.imshow(wocl)","9e63be1f":"from wordcloud import WordCloud, STOPWORDS\nwocl=WordCloud(stopwords=STOPWORDS).generate(\" \".join(df[\"Description\"].tolist()))\nplt.figure(figsize=(25,10))\nplt.imshow(wocl)","16eed44b":"df_final=df_corr.copy()\ndf_final.head()","4787d8e7":"df_final.drop(['Book_title','Description'],inplace=True,axis=1)\ndf_final.head()","6e05f98e":"df_kick=df_final[df_final[\"Type\"]==\"Boxed Set - Hardcover\"]\nindexs=df_kick.index\nindexs","a99c087a":"for i in indexs:\n    df_final.drop(i, axis =0,inplace = True)","d1d04d82":"df_kick=df_final[df_final[\"Type\"]==\"Unknown Binding\"]\nindexs=df_kick.index\nindexs","b8bb73d1":"for i in indexs:\n    df_final.drop(i, axis =0,inplace = True)","d3a8d3ce":"df_final.Type.value_counts()","2d088081":"dums = pd.get_dummies(df_final['Type'],prefix=\"Type\",drop_first=False)\ndums","7284fce7":"dums=pd.DataFrame(data=dums)\ndums2=dums.drop(['Type_ebook'],axis=1)\ndums2","cf8cfe87":"result = pd.concat([df_final, dums2], axis=1, sort=False)\nresult","a88260b2":"result.drop(['Type'],inplace=True,axis=1)\nresult","53f064c1":"result[\"Price\"]=result[\"Price\"].map(lambda x:round(x,2))\n\nresult","82113043":"from sklearn.neighbors import LocalOutlierFactor\nclf = LocalOutlierFactor(n_neighbors = 20, contamination = 0.1)","77f04ab5":"df_out=result.copy()\nclf.fit_predict(df_out)\ndf_scores = clf.negative_outlier_factor_","259f99fb":"np.sort(df_scores)[0:50]","5cb9f53a":"threshold_value = np.sort(df_scores)[7]\nthreshold_value","a9960e3f":"Outlier_df= df_out[df_scores < threshold_value]\nindexs=Outlier_df.index\nOutlier_df","be8fabd6":"# Kick Outliers\nfor i in indexs:\n    result.drop(i, axis = 0,inplace = True)","01b4ed56":"result","0a97ad0d":"sample_df=result.sample(n=8,random_state=35)\nsample_df\n","06bbc780":"sample_df_xtest=sample_df.drop(['Price'],axis=1)\nsample_df_ytest=sample_df[\"Price\"]","6d8dff3f":"sample_df_xtest = (sample_df_xtest - np.min(sample_df_xtest)) \/ (np.max(sample_df_xtest) - np.min(sample_df_xtest)).values\nsample_df_xtest[\"Rating\"]=sample_df_xtest[\"Rating\"].map(lambda x:round(x,2))\nsample_df_xtest[\"Reviews\"]=sample_df_xtest[\"Reviews\"].map(lambda x:round(x,2))\nsample_df_xtest[\"Number_Of_Pages\"]=sample_df_xtest[\"Number_Of_Pages\"].map(lambda x:round(x,2))\nsample_df_xtest","64fbd6cf":"indexs=sample_df.index\nindexs","156bc4c6":"# Kick sample \nfor i in indexs:\n    result.drop(i, axis = 0,inplace = True)","465d5829":"result","a6d8f491":"X = result.drop(['Price'], axis= 1)\ny = result[\"Price\"]","adafa6d6":"#  Normalize\nX = (X - np.min(X)) \/ (np.max(X) - np.min(X)).values\nX","523d80e2":"X[\"Rating\"]=X[\"Rating\"].map(lambda x:round(x,2))\nX[\"Reviews\"]=X[\"Reviews\"].map(lambda x:round(x,2))\nX[\"Number_Of_Pages\"]=X[\"Number_Of_Pages\"].map(lambda x:round(x,2))\nX","b891f533":"from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict","29445562":"X_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y, \n                                                    test_size=0.25, \n                                                    random_state=42)\n\nprint(\"X_train\", X_train.shape)\n\nprint(\"y_train\",y_train.shape)\n\nprint(\"X_test\",X_test.shape)\n\nprint(\"y_test\",y_test.shape)","8f875779":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nmodel = lm.fit(X_train, y_train)","3b41e304":"model.intercept_","05c59ba3":"model.coef_","866d2173":"from sklearn.metrics import mean_squared_error, r2_score\nnp.sqrt(mean_squared_error(y_train, model.predict(X_train)))","b638968d":"np.sqrt(mean_squared_error(y_test, model.predict(X_test)))","94d789b4":"model.score(X_train, y_train)","a4677f92":"cross_val_score(model, X_train, y_train, cv = 10, scoring = \"r2\").mean()","5cd67296":"np.sqrt(-cross_val_score(model, \n                X_train, \n                y_train, \n                cv = 10, \n                scoring = \"neg_mean_squared_error\")).mean()","78698866":"np.sqrt(-cross_val_score(model, \n                X_test, \n                y_test, \n                cv = 10, \n                scoring = \"neg_mean_squared_error\")).mean()","003a9e43":"pred=model.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_L_R_pred = pd.DataFrame(data=frames)\nresult_L_R_pred=result_L_R_pred.T\n\nresult_L_R_pred=result_L_R_pred.rename(columns={0:'Pred_LR_Value',1:'Real_Value'})\nresult_L_R_pred[\"Diff\"]=result_L_R_pred[\"Pred_LR_Value\"]-result_L_R_pred[\"Real_Value\"]\nresult_L_R_pred","a53f5a26":"from sklearn.svm import SVR\nsvr_rbf = SVR(\"rbf\").fit(X_train, y_train)","f0e62bb0":"y_pred = svr_rbf.predict(X_test)","0df1d9fa":"np.sqrt(mean_squared_error(y_test, y_pred))","3cce90db":"pred=svr_rbf.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_SVR_pred = pd.DataFrame(data=frames)\nresult_SVR_pred=result_SVR_pred.T\n\nresult_SVR_pred=result_SVR_pred.rename(columns={0:'Pred_SVR_Value',1:'Real_Value'})\nresult_SVR_pred[\"Diff\"]=result_SVR_pred[\"Pred_SVR_Value\"]-result_SVR_pred[\"Real_Value\"]\nresult_SVR_pred","2d626145":"from sklearn.model_selection import GridSearchCV\nsvr_params = {\"C\": [0.01, 0.1,0.4,5,10,20,30,40,50]}\nsvr_cv_model = GridSearchCV(svr_rbf,svr_params, cv = 10)\nsvr_cv_model.fit(X_train, y_train)","2480109f":"pd.Series(svr_cv_model.best_params_)[0]","f0c1f859":"svr_tuned = SVR(\"rbf\", C = pd.Series(svr_cv_model.best_params_)[0]).fit(X_train,y_train) \n                                                                      ","e2246308":"y_pred = svr_tuned.predict(X_test)\n\nnp.sqrt(mean_squared_error(y_test, y_pred))","210b43ff":"pred=svr_tuned.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_SVR_pred = pd.DataFrame(data=frames)\nresult_SVR_pred=result_SVR_pred.T\n\nresult_SVR_pred=result_SVR_pred.rename(columns={0:'Pred_SVR_Value',1:'Real_Value'})\nresult_SVR_pred[\"Diff\"]=result_SVR_pred[\"Pred_SVR_Value\"]-result_SVR_pred[\"Real_Value\"]\nresult_SVR_pred","9a36687c":"from sklearn.neighbors import KNeighborsRegressor","1b64f789":"knn_model = KNeighborsRegressor().fit(X_train, y_train)","8b2a70ce":"knn_model","c6d822ac":"y_pred = knn_model.predict(X_test)","349e8d91":"np.sqrt(mean_squared_error(y_test, y_pred))","c6b1b7e6":"pred=knn_model.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_KNN_pred = pd.DataFrame(data=frames)\nresult_KNN_pred=result_KNN_pred.T\n\n\nresult_KNN_pred=result_KNN_pred.rename(columns={0:'Pred_KNN_Value',1:'Real_Value'})\nresult_KNN_pred[\"Diff\"]=result_KNN_pred[\"Pred_KNN_Value\"]-result_KNN_pred[\"Real_Value\"]\nresult_KNN_pred","ee0f7b0d":"RMSE = [] \n\nfor k in range(10):\n    k = k+1\n    knn_model = KNeighborsRegressor(n_neighbors = k).fit(X_train, y_train)\n    y_pred = knn_model.predict(X_train) \n    rmse = np.sqrt(mean_squared_error(y_train,y_pred)) \n    RMSE.append(rmse) \n    print(\"k =\" , k , \" RMSE value: \", rmse)","5d00c3f1":"knn_params = {'n_neighbors': np.arange(1,30,1)}","b52c2a6c":"knn = KNeighborsRegressor()","d259f79d":"knn_cv_model = GridSearchCV(knn, knn_params, cv = 10)","7a6a787b":"knn_cv_model.fit(X_train, y_train)","3c8ebb79":"knn_cv_model.best_params_[\"n_neighbors\"]","ad142a7c":"knn_tuned = KNeighborsRegressor(n_neighbors = knn_cv_model.best_params_[\"n_neighbors\"])","14067ea7":"knn_tuned.fit(X_train, y_train)","4337f5e4":"np.sqrt(mean_squared_error(y_test, knn_tuned.predict(X_test)))","7267adce":"pred=knn_tuned.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_KNN_pred = pd.DataFrame(data=frames)\nresult_KNN_pred=result_KNN_pred.T\n\n\nresult_KNN_pred=result_KNN_pred.rename(columns={0:'Pred_KNN_Value',1:'Real_Value'})\nresult_KNN_pred[\"Diff\"]=result_KNN_pred[\"Pred_KNN_Value\"]-result_KNN_pred[\"Real_Value\"]\nresult_KNN_pred","465fbb1a":"from sklearn.neural_network import MLPRegressor\nmlp_model = MLPRegressor(hidden_layer_sizes = (100,20)).fit(X_train, y_train)","b56fe375":"mlp_model","eb03102f":"y_pred = mlp_model.predict(X_test)","93720a1e":"np.sqrt(mean_squared_error(y_test, y_pred))","68bb25a2":"pred=mlp_model.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_MLP_pred = pd.DataFrame(data=frames)\nresult_MLP_pred=result_MLP_pred.T\n\nresult_MLP_pred=result_MLP_pred.rename(columns={0:'Pred_MLP_Value',1:'Real_Value'})\nresult_MLP_pred[\"Diff\"]=result_MLP_pred[\"Pred_MLP_Value\"]-result_MLP_pred[\"Real_Value\"]\nresult_MLP_pred","3cf1c035":"mlp_params = {'alpha': [0.1, 0.01,0.02,0.005],\n             'hidden_layer_sizes': [(20,20),(100,50,150),(300,200,150)],\n             'activation': ['relu','logistic']}","4e7ec249":"mlp_cv_model = GridSearchCV(mlp_model, mlp_params, cv = 10)","4117727c":"mlp_cv_model.fit(X_train, y_train)","a65e3262":"mlp_cv_model.best_params_","2eba5d8d":"mlp_tuned = MLPRegressor(alpha = 0.005,activation='relu',hidden_layer_sizes =(300,200,150))","9bf25a51":"mlp_tuned.fit(X_train, y_train)","9b9943d8":"y_pred = mlp_tuned.predict(X_test)","80a7d57a":"np.sqrt(mean_squared_error(y_test, y_pred))","97a5213c":"pred=mlp_tuned.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_MLP_pred = pd.DataFrame(data=frames)\nresult_MLP_pred=result_MLP_pred.T\n\nresult_MLP_pred=result_MLP_pred.rename(columns={0:'Pred_MLP_Value',1:'Real_Value'})\nresult_MLP_pred[\"Diff\"]=result_MLP_pred[\"Pred_MLP_Value\"]-result_MLP_pred[\"Real_Value\"]\nresult_MLP_pred","1b095a10":"from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier","28507fc4":"cart_model = DecisionTreeRegressor(min_samples_split = 2)","9e032eaf":"cart_model.fit(X_train, y_train)","ccba5e9a":"y_pred =cart_model.predict(X_test)","fe7cb6c6":"np.sqrt(mean_squared_error(y_test, y_pred))","42072108":"pred=cart_model.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_DTR_pred = pd.DataFrame(data=frames)\nresult_DTR_pred=result_DTR_pred.T\n\nresult_DTR_pred=result_DTR_pred.rename(columns={0:'Pred_DTR_Value',1:'Real_Value'})\nresult_DTR_pred[\"Diff\"]=result_DTR_pred[\"Pred_DTR_Value\"]-result_DTR_pred[\"Real_Value\"]\nresult_DTR_pred","b7827dbb":"cart_params = {\"min_samples_split\": range(2,100),\n               \"max_leaf_nodes\": range(2,10)}","652efbe1":"cart_cv_model = GridSearchCV(cart_model, cart_params, cv = 10)","b98adb0f":"cart_cv_model.fit(X_train, y_train)","695dfe63":"cart_cv_model.best_params_","6033c7a0":"cart_tuned = DecisionTreeRegressor(max_leaf_nodes = 3, min_samples_split = 2)","30445baf":"cart_tuned.fit(X_train, y_train)","f6b0deb6":"y_pred = cart_tuned.predict(X_test)","4e3ae4fd":"np.sqrt(mean_squared_error(y_test, y_pred))","cc248c8d":"pred=cart_tuned.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_DTR_pred = pd.DataFrame(data=frames)\nresult_DTR_pred=result_DTR_pred.T\n\nresult_DTR_pred=result_DTR_pred.rename(columns={0:'Pred_DTR_Value',1:'Real_Value'})\nresult_DTR_pred[\"Diff\"]=result_DTR_pred[\"Pred_DTR_Value\"]-result_DTR_pred[\"Real_Value\"]\nresult_DTR_pred","2fe1afef":"from sklearn.ensemble import BaggingRegressor","23c261c9":"bag_model = BaggingRegressor(bootstrap_features = True)\nbag_model.fit(X_train, y_train)","77752a12":"two_y_pred = bag_model.estimators_[1].fit(X_train, y_train).predict(X_test)\nnp.sqrt(mean_squared_error(y_test, two_y_pred))","e43c088a":"y_pred = bag_model.predict(X_test)","b4472a3b":"np.sqrt(mean_squared_error(y_test, y_pred))","9a0a922c":"pred=bag_model.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_BR_pred = pd.DataFrame(data=frames)\nresult_BR_pred=result_BR_pred.T\n\nresult_BR_pred=result_BR_pred.rename(columns={0:'Pred_BR_Value',1:'Real_Value'})\nresult_BR_pred[\"Diff\"]=result_BR_pred[\"Pred_BR_Value\"]-result_BR_pred[\"Real_Value\"]\nresult_BR_pred","e7a32e86":"bag_params = {\"n_estimators\": range(2,20)}","7119f8af":"bag_cv_model = GridSearchCV(bag_model, bag_params, cv = 10)","bde17db3":"bag_cv_model.fit(X_train, y_train)","8e501879":"bag_cv_model.best_params_","620eb23a":"bag_tuned = BaggingRegressor( n_estimators = 18, random_state = 45)","5e206bcc":"bag_tuned.fit(X_train, y_train)","76e08485":"y_pred = bag_tuned.predict(X_test)","3b83c552":"np.sqrt(mean_squared_error(y_test, y_pred))","dd7093ac":"pred=bag_tuned.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_BR_pred = pd.DataFrame(data=frames)\nresult_BR_pred=result_BR_pred.T\n\nresult_BR_pred=result_BR_pred.rename(columns={0:'Pred_BR_Value',1:'Real_Value'})\nresult_BR_pred[\"Diff\"]=result_BR_pred[\"Pred_BR_Value\"]-result_BR_pred[\"Real_Value\"]\nresult_BR_pred","905f4124":"from sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor(random_state = 42)\nrf_model.fit(X_train, y_train)","39205308":"y_pred = rf_model.predict(X_test)","f286c80a":"np.sqrt(mean_squared_error(y_test, y_pred))","2a77ed34":"pred=rf_model.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_RF_pred = pd.DataFrame(data=frames)\nresult_RF_pred=result_RF_pred.T\n\nresult_RF_pred=result_RF_pred.rename(columns={0:'Pred_RF_Value',1:'Real_Value'})\nresult_RF_pred[\"Diff\"]=result_RF_pred[\"Pred_RF_Value\"]-result_RF_pred[\"Real_Value\"]\nresult_RF_pred","3ae456fe":"rf_params = {'max_depth': list(range(1,10)),\n            'max_features': [3,5,10,15],\n            'n_estimators' : [100, 200, 500, 750]}","468f6066":"rf_model = RandomForestRegressor(random_state = 42)","66041559":"rf_cv_model = GridSearchCV(rf_model, \n                           rf_params, \n                           cv = 10, \n                            n_jobs = -1)","032f010b":"rf_cv_model.fit(X_train, y_train)","5fdd09ca":"rf_cv_model.best_params_","2c4e46cf":"rf_tuned = RandomForestRegressor(max_depth  = 3, \n                                 max_features = 3, \n                                 n_estimators =100)","ee12be12":"rf_tuned.fit(X_train, y_train)","7e63912d":"y_pred = rf_tuned.predict(X_test)","e7eadd14":"np.sqrt(mean_squared_error(y_test, y_pred))","e18b382f":"pred=rf_tuned.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_RF_pred = pd.DataFrame(data=frames)\nresult_RF_pred=result_RF_pred.T\n\nresult_RF_pred=result_RF_pred.rename(columns={0:'Pred_RF_Value',1:'Real_Value'})\nresult_RF_pred[\"Diff\"]=result_RF_pred[\"Pred_RF_Value\"]-result_RF_pred[\"Real_Value\"]\nresult_RF_pred","6327ce1f":"from sklearn.ensemble import GradientBoostingRegressor","9108b2bc":"gbm_model = GradientBoostingRegressor()\ngbm_model.fit(X_train, y_train)","0a641bf7":"y_pred = gbm_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","990947d9":"pred=gbm_model.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_GBM_pred = pd.DataFrame(data=frames)\nresult_GBM_pred=result_GBM_pred.T\n\nresult_GBM_pred=result_GBM_pred.rename(columns={0:'Pred_GBM_Value',1:'Real_Value'})\nresult_GBM_pred[\"Diff\"]=result_GBM_pred[\"Pred_GBM_Value\"]-result_GBM_pred[\"Real_Value\"]\nresult_GBM_pred","c250e23b":"gbm_params = {\n    'learning_rate': [0.001, 0.01, 0.1, 0.2],\n    'max_depth': [3, 5, 8,50,100],\n    'n_estimators': [200, 500, 1000],\n    'subsample': [1,0.5,0.75],\n}","052fcfa9":"gbm = GradientBoostingRegressor()\ngbm_cv_model = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2)\ngbm_cv_model.fit(X_train, y_train)","0aff8105":"gbm_cv_model.best_params_","fca3942a":"gbm_tuned = GradientBoostingRegressor(learning_rate = 0.01,  \n                                      max_depth = 3, \n                                      n_estimators = 200, \n                                      subsample = 0.75)\n\ngbm_tuned = gbm_tuned.fit(X_train,y_train)","1da447e6":"y_pred = gbm_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","ce5b7ab1":"pred=gbm_tuned.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_GBM_pred = pd.DataFrame(data=frames)\nresult_GBM_pred=result_GBM_pred.T\n\nresult_GBM_pred=result_GBM_pred.rename(columns={0:'Pred_GBM_Value',1:'Real_Value'})\nresult_GBM_pred[\"Diff\"]=result_GBM_pred[\"Pred_GBM_Value\"]-result_GBM_pred[\"Real_Value\"]\nresult_GBM_pred","acf0d85b":"Importance = pd.DataFrame({\"Importance\": gbm_tuned.feature_importances_*100},\n                         index = X_train.columns)","2722c3af":"Importance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\")\n\nplt.xlabel(\"Variable Significance Levels\")","064b23b0":"import xgboost as xgb\nfrom xgboost import XGBRegressor","270e5e21":"xgb_model = XGBRegressor().fit(X_train, y_train)","abdd1b2e":"y_pred = xgb_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","d05cc6a0":"pred=xgb_model.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_XGB_pred = pd.DataFrame(data=frames)\nresult_XGB_pred=result_XGB_pred.T\n\nresult_XGB_pred=result_XGB_pred.rename(columns={0:'Pred_XGB_Value',1:'Real_Value'})\nresult_XGB_pred[\"Diff\"]=result_XGB_pred[\"Pred_XGB_Value\"]-result_XGB_pred[\"Real_Value\"]\nresult_XGB_pred","867879d0":"xgb_grid = {\n     'colsample_bytree': [0.4, 0.5,0.6,0.9,1], \n     'n_estimators':[100, 200, 500, 750],\n     'max_depth': [2,3,4,5,6],\n     'learning_rate': [0.1, 0.01, 0.5]\n}","bb9d53d6":"xgb = XGBRegressor()\n\nxgb_cv = GridSearchCV(xgb, \n                      param_grid = xgb_grid, \n                      cv = 10, \n                      n_jobs = -1,\n                      verbose = 2)\n\n\nxgb_cv.fit(X_train, y_train)","46c444c8":"xgb_cv.best_params_","150d9305":"xgb_tuned = XGBRegressor(colsample_bytree = 1, \n                         learning_rate = 0.01, \n                         max_depth =2, \n                         n_estimators = 500) \n\nxgb_tuned = xgb_tuned.fit(X_train,y_train)","127c3217":"y_pred = xgb_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","53aac988":"pred=xgb_tuned.predict(sample_df_xtest)\nframes = [pred, sample_df_ytest.values]\nresult_XGB_pred = pd.DataFrame(data=frames)\nresult_XGB_pred=result_XGB_pred.T\n\nresult_XGB_pred=result_XGB_pred.rename(columns={0:'Pred_XGB_Value',1:'Real_Value1'})\nresult_XGB_pred[\"Diff\"]=result_XGB_pred[\"Pred_XGB_Value\"]-result_XGB_pred[\"Real_Value1\"]\nresult_XGB_pred","08dcd286":"result = pd.concat([result_SVR_pred,result_XGB_pred, result_GBM_pred,\n                    result_RF_pred, result_BR_pred, result_L_R_pred,\n                   result_DTR_pred, result_MLP_pred, result_KNN_pred],\n                   axis=1,\n                   sort=False)\nfinal_result=result[\"Real_Value1\"]\nfinal_result=pd.DataFrame(final_result)\nresult.drop(['Diff',\"Real_Value\",\"Real_Value1\"],inplace=True,axis=1)\n","f71bbb10":"final_result=pd.concat([final_result,result],axis=1)\nfinal_result=final_result.rename(columns={\"Real_Value1\":\"Real_Value\"})\nfinal_result[\"Pred_SVR_Value\"]=final_result[\"Pred_SVR_Value\"].map(lambda x:round(x,2))\nfinal_result[\"Pred_XGB_Value\"]=final_result[\"Pred_XGB_Value\"].map(lambda x:round(x,2))\nfinal_result[\"Pred_GBM_Value\"]=final_result[\"Pred_GBM_Value\"].map(lambda x:round(x,2))\nfinal_result[\"Pred_RF_Value\"]=final_result[\"Pred_RF_Value\"].map(lambda x:round(x,2))\nfinal_result[\"Pred_BR_Value\"]=final_result[\"Pred_BR_Value\"].map(lambda x:round(x,2))\nfinal_result[\"Pred_LR_Value\"]=final_result[\"Pred_LR_Value\"].map(lambda x:round(x,2))\nfinal_result[\"Pred_DTR_Value\"]=final_result[\"Pred_DTR_Value\"].map(lambda x:round(x,2))\nfinal_result[\"Pred_MLP_Value\"]=final_result[\"Pred_MLP_Value\"].map(lambda x:round(x,2))\nfinal_result[\"Pred_KNN_Value\"]=final_result[\"Pred_KNN_Value\"].map(lambda x:round(x,2))\n","c12beda5":"colors=['lightblue','lightpink','lightgreen','yellow',\n        'DarkSalmon','Khaki','LightCoral','Bisque',\n        'FireBrick','LightGray']\nfig = go.Figure(data=[go.Table(header=dict(values=['Real Value', 'Pred SVR Value','Pred XGB Value',\n'Pred GBM Value','Pred RF Value','Pred BR Value','Pred LR Value',\n  'Pred DTR Value','Pred MLP Value','Pred KNN Value'],\nline_color='white', fill_color='LightSlateGray',\nalign='center',font=dict(color='white', size=12)\n                           ),\n                               \n cells=dict( values=[final_result['Real_Value'],\n     final_result['Pred_SVR_Value'],\n      final_result['Pred_XGB_Value'],\n      final_result['Pred_GBM_Value'],\n    final_result['Pred_RF_Value'],\n       final_result['Pred_BR_Value'],\n      final_result['Pred_LR_Value'],\n     final_result['Pred_DTR_Value'],\n    final_result['Pred_MLP_Value'],\n    final_result['Pred_KNN_Value']],\n      line_color=colors, fill_color=colors,\n     align='center', font=dict(color='#660033', size=11))\n                  )])\n                      \nfig.show()","f73aa711":" ## <a id='25'> 21.Model Comparison <\/a>","4979d3f2":" ## <a id='17'> 13.SVR<\/a>","cf75143f":"## <a id='26'>22.Conclusion <\/a>\n\n* As you can see our models are not good.<br>\n* We have little data. <br>\n* Learning is less. <br>\n\n**Models**\n\n![ ](https:\/\/media.giphy.com\/media\/l22ysLe54hZP0wubek\/giphy.gif) \n           ","58e3d062":"<font size=\"+2\" color=\"LIGHTSEAGREEN\"><b>My Other Kernels<\/b><\/font><br>\n\n<a href=\"https:\/\/www.kaggle.com\/drfrank\/loan-data-visualisation-eda-machine-learning\" class=\"btn btn-primary\" style=\"color:white;\">Loan Data Visualisation & EDA & Machine Learning<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/drfrank\/lego-transfer-cnn-classification\" class=\"btn btn-primary\" style=\"color:white;\">Lego Transfer-CNN Classification<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/drfrank\/face-image-classification\" class=\"btn btn-primary\" style=\"color:white;\">Face Image Classification<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/drfrank\/book-review-ratings-data-analysis-visualization\" class=\"btn btn-primary\" style=\"color:white;\">Book Review Ratings Analysis & Visualization<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/drfrank\/insurance-prediction-lgbm-gbm-xgboost-eda\" class=\"btn btn-primary\" style=\"color:white;\">Insurance Prediction- LGBM,GBM,XGBoost EDA<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/drfrank\/fish-market-data-visualisation-machine-learning\" class=\"btn btn-primary\" style=\"color:white;\">Fish Market Data Visualisation & Machine Learning<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/drfrank\/seabron-plotly-for-beginners\" class=\"btn btn-primary\" style=\"color:white;\">Seabron & Plotly For Beginners<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/drfrank\/basketball-players-stats-data-visualisation\" class=\"btn btn-primary\" style=\"color:white;\">Basketball Players Stats Data Visualisation<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/drfrank\/women-s-football-results-visualization\" class=\"btn btn-primary\" style=\"color:white;\">Women's Football Results Visualization<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/drfrank\/us-police-shootings-data-visualisation\" class=\"btn btn-primary\" style=\"color:white;\">Us Police Shootings Data Visualisation<\/a>","39cdb5b5":" ## <a id='16'> 12.Linear Regression<\/a>","a03040a6":"## Model Tuning","4d17e3df":" ## <a id='19'> 15.MLP Regressor<\/a>","1d967cfc":"* I will drop it in the data set because there was a low number of data","3ec09dd0":" ## <a id='11'> 8.Number Of Pages <\/a>","7128edea":"## Model Tuning","7db724cd":"## <a id='8'> 5.Type Of The Book <\/a>\n","6846b677":" ## <a id='18'> 14.KNN<\/a>","a8e9ef34":"* Data has only float,object and integer values.\n* Variable column has not missing values.","a376013a":"## <a id='24'> 20.Xgboost<\/a>","64ca8123":"* Some books have no reviewers but have rating .\n* We have **inconsistent data**\n* I will fill it with the **average value** instead of inconsistent data\n","3a61fed0":"## <a id='17'> <font size=\"+2\" color=\"LIGHTSEAGREEN\"><b>Reference<\/b><\/font><br>\n* https:\/\/seaborn.pydata.org\/api.html\n* https:\/\/plotly.com\/python\/\n* https:\/\/stackoverflow.com \n* https:\/\/www.theanalysisfactor.com\/missing-data-mechanism\/ <br>\n* https:\/\/www.statisticssolutions.com\/missing-values-in-data\/ <br>\n* https:\/\/www.displayr.com\/missing-data-handle\/ <br>\n* https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/index.html <br>\n* https:\/\/www.displayr.com\/what-is-a-correlation-matrix\/ <br>\n    \n* https:\/\/towardsdatascience.com\/exploratory-data-analysis-8fc1cb20fd15 <br>\n* https:\/\/r4ds.had.co.nz\/exploratory-data-analysis.html#introduction-3  <br>    \n    \n    ","7ece6f02":"* Some books have no reviewers ","e4857ce9":"# <a id='5'>3.2 Correlation Matrix <\/a>\n\n* A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables.<br>\n* A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses.<br>\n\n* **There are three broad reasons for computing a correlation matrix** <br>\n\n* To summarize a large amount of data where the goal is to see patterns. <br>\n\n* To input into other analyses. For example, people commonly use correlation matrixes as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise. <br>\n\n* As a diagnostic when checking other analyses. For example, with linear regression, a high amount of correlations suggests that the linear regression estimates will be unreliable.<br>","886cbd69":" ## <a id='14'> 10.1 Local Outlier Factor<\/a>\n\n![](http:\/\/upload.wikimedia.org\/wikipedia\/commons\/4\/4e\/LOF-idea.svg)\n*  The local outlier factor is based on a concept of a local density, where locality is given by k nearest neighbors, whose distance is used to estimate the density. By comparing the local density of an object to the local densities of its neighbors, one can identify regions of similar density, and points that have a substantially lower density than their neighbors. These are considered to be outliers.","f52927fd":"## Model Tuning","1b5234c9":"# Table of contents\n<a href='#0'> Dataset Introduction <\/a> <br>\n<a href='#1'>1. Importing Libraries <\/a> <br>\n<a href='#2'>2. Data <\/a> <br>\n<a href='#3'>3. Exploratory Data Analysis <\/a> <br>\n<a href='#4'>3.1 Missing Value <\/a> <br>\n<a href='#5'>3.2 Correlation Matrix <\/a> <br>\n<a href='#6'>3.3 Describe Function <\/a> <br>\n<a href='#7'>4. Ratings <\/a> <br>\n<a href='#8'>5. Type Of The Book  <\/a> <br>\n<a href='#9'>6. Reviews <\/a> <br>\n<a href='#10'>7. Price <\/a> <br>\n<a href='#11'>8. Number Of Pages <\/a> <br>\n<a href='#12'>9. Word Cloud <\/a> <br>\n<a href='#13'>10. Data Preprocessing <\/a> <br>\n<a href='#14'>10.1 Local Outlier Factor <\/a> <br>\n<a href='#15'>11. Sample Data <\/a> <br>\n<a href='#16'>12. Linear Regression  <\/a> <br>\n<a href='#17'>13.SVR  <\/a> <br>\n<a href='#18'>14.KNN <\/a> <br>\n<a href='#19'>15.MLP Regressor <\/a> <br>\n<a href='#20'>16.Decision Tree Regressor <\/a> <br>\n<a href='#21'>17.Bagging Regressor <\/a> <br>\n<a href='#22'>18.Random Forests <\/a> <br>\n<a href='#23'>19.Gradient Boosting Regressor <\/a> <br>\n<a href='#24'>20.Xgboost <\/a> <br>\n<a href='#25'>21.Model Comparison <\/a> <br>\n<a href='#26'>22.Conclusion <\/a> <br>\n<a href='#27'>23.End Note <\/a> <br>","7fd38fc1":"## <a id='9'> 6.Reviews <\/a>","069b01f5":"# <a id='0'> Dataset Introduction<\/a>","8aa81d16":"## <a id='23'> 19.Gradient Boosting Regressor<\/a>","54ee8e15":" # <a id='3'> 3.Exploratory Data Analysis<\/a>\n * Exploratory Data Analysis refers to the critical process of performing \ninitial investigations on data so as to discover patterns,to spot anomalies, \nto test hypothesis and to check assumptions with\nthe help of summary statistics and graphical representations. <br>\n\n* Your goal during EDA is to develop an understanding of your data. The easiest way to do this is to use questions as tools to guide your investigation. When you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make.<br>\n\n* Generate questions about your data.<br>\n\n* Search for answers by visualising, transforming, and modelling your data. <br>\n\n* Use what you learn to refine your questions and\/or generate new questions. <br>\n\n* EDA is not a formal process with a strict set of rules.<br> \n* More than anything, EDA  is a state of mind.<br> \n* During the initial phases of EDA you should feel free to investigate every idea that occurs to you.<br> \n* Some of these ideas will pan out, and some will be dead ends.<br>\n* As your exploration continues, you will home in on a few particularly productive areas that you\u2019ll eventually write up and communicate to others.<br>\n\n* Let's start exploring our data","639210e2":" ## <a id='15'> 11. Sample Data<\/a>\n * We get some data to test models.","2387caa5":" ## <a id='13'> 10.Data Preprocessing <\/a>","d0b53e3e":" ## <a id='12'> 9.Word Cloud <\/a>","f0146c8e":"## <a id='21'> 17.Bagging Regressor<\/a>","5471de32":"## <a id='7'> 4.Rating <\/a>","c9caaf66":"## <a id='27'><font color=\"LIGHTSEAGREEN\" size=+2.5><b>23.End Note<\/b><\/font> <\/a>\n\nI hope you enjoyed my kernel.If you like this notebook, an <font color=\"DARKCYAN\"><b>Upvote<\/b><\/font> would be great ! :)\n\nI am new with data science. Please <font color=\"GREEN\"><b>comments<\/b><\/font> me your <font color=\"GREEN\"><b>feedbacks<\/b><\/font> to help me improve myself. \n    \nThanks for your time","82c7d839":"* We don't have duplicated data ","b816ba0c":"## Model Tuning","f804ef0b":" ## <a id='10'> 7.Price <\/a>","977fc6d1":"# <a id='1'> 1.Importing Libraries<\/a>","af290d1e":"# <a id='2'> 2. Data<\/a>\n## **Data Columns Means**\n* Rating ------- > The user rating for the book. the rating score ranges between 0 and 5 <br>\n* Reviews ------- >  The number of reviews found on this book <br>\n* Book_title ------- > The name of the book<br>\n* Description ------- > A short description of the book <br>\n* Number_Of_Pages ------- > Number of pages in the book <br>\n* Type ------- >The type of the book meaning is it a hardcover book or an ebook or a kindle book etc.<br>\n* Price ------- > The average price of the book in USD where the average is calculated according the 5 web sources <br>","852a8d9f":"## Model Tuning","41b986de":"## Model Tuning","c19335c2":"## Model Tuning","54a8db86":" * We dont have missing data ","7fce0da8":"* I will drop it in the data set because there was a low number of data","25b27551":"# <a id='4'> 3.1 Missing Value<\/a>\n* Missing data, also known as missing values, is where some of the observations in a data set are blank <br>\n* The concept of missing values is important to understand in order to successfully manage data.  If the missing values are not handled properly by the researcher, then he\/she may end up drawing an inaccurate inference about the data.  Due to improper handling, the result obtained by the researcher will differ from ones where the missing values are present.<br>\n\n## **The Missing Data Mechanisms**\n### **Missing Completely at Random(MCAR)**\n* Means there is no relationship between the missingness of the data and any values, observed or missing. Those missing data points are a random subset of the data. There is nothing systematic going on that makes some data more likely to be missing than others.\n\n### **Missing at Random (MAR)**\n* Means there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data.\n\n* Whether an observation is missing has nothing to do with the missing values, but it does have to do with the values of an individual\u2019s observed variables. So, for example, if men are more likely to tell you their weight than women, weight is MAR.\n\n### **Missing Not at Random(MNAR)**\n\n* Means there is a relationship between the propensity of a value to be missing and its values. This is a case where the people with the lowest education are missing on education or the sickest people are most likely to drop out of the study.\n\n* MNAR is called \u201cnon-ignorable\u201d because the missing data mechanism itself has to be modeled as you deal with the missing data. You have to include some model for why the data are missing and what the likely values are.\n","4b4e4df5":" ## <a id='20'> 16.Decision Tree Regressor<\/a>\n","2d10b553":"## Conclusion Rating\n\n* max rating =5 \n* mean rating = 4.07 \n* min rating = 3 \n* Top 10 Books  (Reviews>250)\n* We have inconsistent data","79f1c502":"# <a id='6'> 3.3 Describe Function<\/a>\n* Generate descriptive statistics.<br>\n* Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values.<br>\n* The **describe**() function in pandas is very handy\nin getting various **summary statistics**.<br>\n* This function returns the **count**, **mean**, **standard deviation**,\n**minimum** and **maximum** **values** and the **quantiles of the data**.<br>","0bcad534":"## Model Tuning","26ba10ee":"![](https:\/\/media.giphy.com\/media\/xT8qBt3pdiCZrk3erS\/giphy.gif)\n* This dataset holds a list of 270 books in the field of computer science and programming related topics.\nThe list of books was constructed using many popular websites which provide information on book ratings an of all the book in those websites the 270 most popular were selected.\n\n* Inside that dataset, you will find general information about the book including the number of pages in the book, the book types, the book descriptions, and the book prices.","b00df495":"## <a id='22'> 18.Random Forests<\/a>","395756af":"* Dataset comprises of 271 observations and 7 characteristics.<br>"}}