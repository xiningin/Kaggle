{"cell_type":{"3224740d":"code","24a07eb8":"code","ff8b4875":"code","b1461671":"code","017e0b5b":"code","e76344e1":"code","511f4697":"code","5ad2804f":"code","219819f8":"code","e468a1c0":"code","0c93ad74":"code","42b77bb3":"code","0e9b16f3":"code","bf9db56e":"code","afc6abdd":"code","0bd1d9ce":"code","cd5a8c26":"code","3936f4ca":"code","a61ce0c8":"code","78110fef":"markdown","76bc006e":"markdown","7e05ca2c":"markdown","5c12c9a6":"markdown","136eef32":"markdown","3b182e45":"markdown","3c4f2b11":"markdown","854d579d":"markdown","cf76c34e":"markdown","88dcbabf":"markdown","88de6ade":"markdown","4c6ed405":"markdown","8ed31093":"markdown","ae092d9c":"markdown","7f6effb5":"markdown","4d01b3f0":"markdown","9c7e94da":"markdown","95776692":"markdown","07d16402":"markdown","f2632df5":"markdown","591eaafc":"markdown","4978f2c7":"markdown"},"source":{"3224740d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","24a07eb8":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","ff8b4875":"print('\\n',train.info())\n\nprint('\\nNaN values in the training dataset\\n\\n',train.isna().sum())","b1461671":"train.Age.hist(bins = 20)","017e0b5b":"train.Embarked.value_counts()","e76344e1":"train.Cabin.value_counts()[:10]","511f4697":"# we repeat the same data cleaning for train and test sets\nfor data in [train,test]:\n    \n    #gr = data.groupby(['Sex','Pclass'])\n    #data.Age = gr.Age.apply(lambda x: x.fillna(x.median()))\n    \n    \n    # Lets simply fill NaN in Age by median value\n    data.Age.fillna(data.Age.mean(),inplace = True)\n    # Lets simply fill NaN in Embarked by the most frequent value\n    data.Embarked.fillna('S',inplace=True)\n    # Lets simply fill NaN in Cabin by 0 and by 1 if there is any value\n    data.Cabin.fillna(0,inplace=True)\n    # Also there are missing Fare values in test set. Lets fill it with mean value as well as Age\n    data.Fare.fillna(data.Fare.mean(),inplace=True)\n","5ad2804f":"# We need ids later on for submission\ntest_id = test['PassengerId']\n\nfor data in [train,test]:\n    # We need ids later on for submission\n    data.drop(['PassengerId','Ticket'],axis=1,inplace=True)    # We won`t use this values as features\n    data['Name']     =  data['Name'].apply(lambda x: len(x)) # as suggested in few notebooks\n    data['FamSize'] = data['SibSp']+data['Parch']+1 ","219819f8":"for data in [train,test]:\n    \n    # Lets create new feature Single, which is 0 if FamSize == 1 else 1\n    data['Single']   = data['FamSize'].apply(lambda x: 1 if x == 1 else 0)\n    # Lets simply fill NaN in Cabin by 0 and by 1 if there is any value\n    data['Cabin']    = data['Cabin'].apply(lambda x: 1 if x != 0 else 0)\n    \n    \n    \n    #data['Sex']      = data['Sex'].apply(lambda x: 0 if x == 'female' else 1)\n    #data['Embarked'] = data['Embarked'].apply(lambda x: 0 if x == 'C' else (1 if x == 'S' else 'Q'))","e468a1c0":"for data in [train,test]:\n    \n    data['AgeBin']    = pd.cut(data['Age']     , bins = 5, labels = ['child','teen','adult','old','oldest'])\n    data['FareBin']   = pd.cut(data['Fare']    , bins = 5, labels = ['lowest','low','inter','high','highest'])\n    data['FamBin']   = pd.cut(data['FamSize'], bins = 3, labels = ['small','inter','big'])","0c93ad74":"from sklearn.model_selection import train_test_split,cross_val_score\n\nX = train.drop(['Survived'],axis=1)\ny = train['Survived']\n\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.33, random_state = 42,shuffle=True)\n","42b77bb3":"# Makes life easier later on\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\n\n# We need this moduls for encoding\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# Our Classifiers\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier \nfrom sklearn.svm import SVC\n\n# This is cool thing. Helps to 'ensemble' many classifiers together and make better predictions\nfrom sklearn.ensemble import VotingClassifier\n\n# For hyperparameters optimisation\nfrom sklearn.model_selection import cross_val_score,GridSearchCV\n","0e9b16f3":"transformer_trees = make_column_transformer(\n    #(MinMaxScaler(),['Name','Age','Fare','FamSize','SibSp','Parch']),\n    #(OrdinalEncoder(),['Pclass',]),\n    (OneHotEncoder(),['Sex','Single','Cabin','Embarked','AgeBin','FareBin','FamBin']),\n    remainder = 'passthrough'\n)\n\ntransformer_svc = make_column_transformer(\n    (MinMaxScaler(),['Name','Age','Fare','FamSize','SibSp','Parch']),\n    #(OrdinalEncoder(),['Pclass',]),\n    (OneHotEncoder(),['Sex','Single','Cabin','Embarked','AgeBin','FareBin','FamBin']),\n    remainder = 'passthrough'\n)","bf9db56e":"clf_rfc = RandomForestClassifier(bootstrap=False,random_state=42)\n\nparam_grid_rfc = {'randomforestclassifier__n_estimators':[x for x in range(10,100,10)],\n                  'randomforestclassifier__max_depth':[2,5,10,15],\n                 }\n\n\nline_rfc = make_pipeline(transformer_trees,\n                         clf_rfc)\n\nsearch_rfc = GridSearchCV(line_rfc,param_grid_rfc,\n                      cv=5,\n                      scoring='accuracy',\n                      verbose=1,\n                      n_jobs=-1,)\n\nsearch_rfc.fit(X_train,y_train)\npipe_rfc = search_rfc.best_estimator_\n\nprint(\"Best parameter (CV score=%0.3f):\" % search_rfc.best_score_)\nprint(search_rfc.best_params_)\n\npipe_rfc.fit(X_train,y_train)\nprint('Train score: ', pipe_rfc.score(X_train,y_train))\nprint('Val score: ', pipe_rfc.score(X_val,y_val))","afc6abdd":"clf_gbc = GradientBoostingClassifier(random_state=42)\n\nparam_grid_gbc = {'gradientboostingclassifier__n_estimators':[x for x in range(10,50,10)],\n                  'gradientboostingclassifier__max_depth':[2,5,10],\n                  'gradientboostingclassifier__learning_rate':[0.1,0.5,0.7]\n                 }\n\n\nline_gbc = make_pipeline(transformer_trees,\n                         clf_gbc)\n\nsearch_gbc = GridSearchCV(line_gbc,param_grid_gbc,\n                      cv=5,\n                      scoring='accuracy',\n                      verbose=1,\n                      n_jobs=-1,)\n\nsearch_gbc.fit(X_train,y_train)\npipe_gbc = search_gbc.best_estimator_\n\nprint(\"Best parameter (CV score=%0.3f):\" % search_gbc.best_score_)\nprint(search_gbc.best_params_)\n\npipe_gbc.fit(X_train,y_train)\nprint('Train score: ', pipe_gbc.score(X_train,y_train))\nprint('Val score: ', pipe_gbc.score(X_val,y_val))","0bd1d9ce":"clf_svc = SVC(random_state = 42,probability = True)\n\nparam_grid_svc = {'svc__C':[x for x in np.arange(0.01,0.1,0.01)],\n                  'svc__gamma':[x for x in [0.01,0.05,0.7,0.1]],\n                  'svc__kernel':['rbf']\n                 }\n\nline_svc = make_pipeline(transformer_svc,\n                         clf_svc)\n\nsearch_svc = GridSearchCV(line_svc,param_grid_svc,\n                          cv=5,\n                          scoring='accuracy',\n                          verbose=1,\n                          n_jobs=-1,)\n\nsearch_svc.fit(X_train,y_train)\n\npipe_svc = search_svc.best_estimator_\n\nprint(\"Best parameter (CV score=%0.3f):\" % search_svc.best_score_)\nprint(search_svc.best_params_)\n\npipe_svc.fit(X_train,y_train)\n\nprint('Train score: ', pipe_svc.score(X_train,y_train))\nprint('Val score: ', pipe_svc.score(X_val,y_val))\n","cd5a8c26":"pipe_ens = VotingClassifier(estimators=[('rfc',pipe_rfc),\n                                        ('svc',pipe_svc),\n                                        ('gbc',pipe_gbc)],\n                            voting='soft',n_jobs=-1)\n\npipe_ens.fit(X_train,y_train)\n\nprint('Train score: ', pipe_ens.score(X_train,y_train))\nprint('Val score: ', pipe_ens.score(X_val,y_val))","3936f4ca":"pred = pipe_ens.predict(test)\nsub = pd.DataFrame({'PassengerId': test_id, 'Survived': pred})\nsub.to_csv('submission.csv',index=False)","a61ce0c8":"# You can also consider outlier detection to be used\n#------------------------------------------------------------------------------\n# accept a dataframe, remove outliers, return cleaned data in a new dataframe\n# see http:\/\/www.itl.nist.gov\/div898\/handbook\/prc\/section1\/prc16.htm\n#------------------------------------------------------------------------------\n#def remove_outlier(df_in, col_name):\n    #q1 = df_in[col_name].quantile(0.25)\n    #q3 = df_in[col_name].quantile(0.75)\n    #iqr = q3-q1 #Interquartile range\n    #fence_low  = q1-1.5*iqr\n    #fence_high = q3+1.5*iqr\n    #df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n    #return df_out","78110fef":"### 1. Random Forest Classifier","76bc006e":"Additionaly, we can create new feature as suggested in many notebooks namely Family size (FamSize). That is sum of Siblings and parents on the bord.","7e05ca2c":"### 2. Gradient Boosting Classifier","5c12c9a6":"### Create training and validation splits based on train.csv\n\nFirst of all we divide our initial on X (features) and y(labels-Survived)\n\nAproachin training part, lets divide training set on train and validation. Validation we will use for hyperparameters optimisation with the help of GridSearchCV.","136eef32":"### Filling NaN values\n\nFirst of all let`s fill NaN values for both training and testing dataset.","3b182e45":"### Importing default modules","3c4f2b11":"### Importing Data","854d579d":"And for carbin we just assign int values 1 where cabin presents and 0 if it is NaN","cf76c34e":"### Bin some numerical features\n\nAlso we can try to add new categorical features and try to bin Age,Fare,FamSize in bins.\nWhich later on will be encoded with sklearn tools","88dcbabf":"### Ensemble","88de6ade":"### Initial Data Check","4c6ed405":"### Submission","8ed31093":"We will use two types of column_transformers. One with scalling int\/float values and one without scaling.\nIt was observed that SVC performs poorly is there is no any scaling. That is why we use MinMaxScaller(StandardScaler)\nin it pipline.\n\nFor both we use OneHotEncoding to encode categorical and object data type ","ae092d9c":"### Conclusion","7f6effb5":"### 3. SVC","4d01b3f0":"Let us simply fill NaN in age with its median or mean value.\nNote, there is another aproach in internet, you can group passangers by 'Sex' and 'Pclass' and assign median values.","9c7e94da":"### Values coding","95776692":"### Additional Features","07d16402":"### Importing necessary modules from sklearn","f2632df5":"For 'Embarked' column we fill NaN values with th emost frequent values i.e 'Q'","591eaafc":"We have 891 rows, 12 columns: 7 int or str and 5 of oblect type.\nAlso there are some missing values in 'Age','Cabin' and 'Embarked' columns on which we must work a bit","4978f2c7":"Even though this model looks heavy, however I couldn`t overcome limit of 0.79425 with test data set.\nI would appreciate if you provide me any suggestions."}}