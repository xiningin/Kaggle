{"cell_type":{"fb8aaff2":"code","1b381f0a":"code","dce48d2b":"code","cefdf219":"code","4ce0b02c":"code","2ce5c593":"code","18e1fa91":"code","6899d2a5":"code","496cab69":"code","58de6e78":"code","a70c3716":"code","0be59b93":"code","d2c34c32":"code","753d8df6":"code","e0265668":"code","4bb9850c":"code","9144be46":"code","b16a6353":"code","47d503de":"code","6f862ab8":"code","f4839310":"code","c8d0738e":"code","c4209a3c":"code","1f319a04":"code","891e58ea":"code","e257c5a9":"code","c557cd55":"code","7e280005":"code","62c21424":"code","4773ef43":"code","10a4a5da":"code","38d17b03":"code","3d0504ab":"code","7baa3bb9":"code","7c73f58e":"code","6253edb2":"code","8439fcb6":"code","0341873f":"code","c9fc9096":"code","43e3de29":"code","fa34ba1e":"markdown","90c06957":"markdown","d1ec5f67":"markdown","3dbbe0d7":"markdown","dd10292b":"markdown","d6fd7e11":"markdown","6b740e33":"markdown","5709becb":"markdown","09780531":"markdown","5ce6a92a":"markdown","63898cec":"markdown","7f42d3b6":"markdown","3beae50d":"markdown","286365e7":"markdown","685968d9":"markdown","2b60bde8":"markdown","fe0a28e0":"markdown","6fa3a439":"markdown","205157cb":"markdown","34563eaf":"markdown","666c01d0":"markdown","e47e24dd":"markdown","1de46e30":"markdown","af696153":"markdown","c826faaf":"markdown","ddc9dde7":"markdown","2f99da25":"markdown","f2c3ba17":"markdown"},"source":{"fb8aaff2":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom xgboost import XGBRegressor\nimport optuna.integration.lightgbm as lgb\nimport lightgbm as lgbm\nfrom sklearn.metrics import mean_squared_error\n\npd.set_option(\"display.max_columns\", 30)\npd.set_option(\"display.max_rows\", 30)","1b381f0a":"train_df= pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest_df= pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsub_df= pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")","dce48d2b":"def plot_model_comparison(models,results,title):\n    \"\"\" \n        Compares the results of different models and plots box plots for the algorithms.\n        models: list of names of models\n        results: training results\n        title: title for the graph\n        \n    \"\"\"\n    fig = plt.figure()\n    fig.suptitle(title)\n    ax = fig.add_subplot(111)\n    plt.boxplot(results)\n    ax.set_xticklabels(models)\n    plt.show()\n\ndef timer(start_time=None):\n    \"\"\" \n        Helps  to keep track of time elapsed while training.\n        start time: if none then start time tracking\n                    if not none tracks time from start time         \n    \"\"\"\n    if not start_time:\n        print(datetime.now())\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print(\"Time taken: %i hours %i minutes and %s seconds.\" % (thour, tmin, round(tsec, 2)))\n\ndef getbounds(col):\n    ''' \n    This function returns the upper bound and the lower bound using the IQR for the column \"col\".\n    '''\n    sorted(col)\n    q1,q3 = np.percentile(col,[25,75]) # quartailes\n    iqr = q3-q1 # inter quartile range\n    lb = q1 -(1.5*iqr) # lower bound\n    ub = q3 +(1.5*iqr) # upper bound\n    return lb,ub","cefdf219":"print(train_df.shape, test_df.shape)","4ce0b02c":"train_df.dtypes","2ce5c593":"train_df.columns","18e1fa91":"train_df.isnull().sum()","6899d2a5":"train_df.duplicated().sum()","496cab69":"train_df.describe().T","58de6e78":"train_df.head()","a70c3716":"cat_cols = [\"cat0\", \"cat1\", \"cat2\", \"cat3\", \"cat4\", \"cat5\", \"cat6\",\n\"cat7\",\"cat8\", \"cat9\"]\ncont_cols= [\"cont0\", \"cont1\", \"cont2\", \"cont3\", \"cont4\", \"cont5\",\n\"cont6\", \"cont7\", \"cont8\", \"cont9\", \"cont10\", \"cont11\", \"cont12\",\n\"cont13\"]","0be59b93":"train_df[cont_cols].hist(figsize=(20,12))\nplt.show()","d2c34c32":"for col in cat_cols:\n    print (col,sorted(train_df[col].unique()))","753d8df6":"for col in cat_cols:\n    print (col,sorted(train_df[col].unique()))","e0265668":"train_df[cat_cols].apply(pd.Series.value_counts).plot(kind='bar',figsize=(15,5))\nplt.show()","4bb9850c":"plt.figure(figsize=(15,15))\ncont_hm = train_df[cont_cols].corr()\nsns.heatmap(cont_hm, annot= True)\nplt.show()","9144be46":"plt.figure(figsize=(12,10))\nsns.boxplot(data=train_df[cont_cols])\nplt.show()","b16a6353":"plt.figure(figsize=(12,10))\nsns.boxplot(data=test_df[cont_cols])\nplt.show()","47d503de":"cont0_lb,cont0_ub = getbounds(train_df['cont0'])\ncont6_lb,cont6_ub = getbounds(train_df['cont6'])\ncont8_lb,cont8_ub = getbounds(train_df['cont8'])","6f862ab8":"train_df.loc[train_df['cont0']>=cont0_ub,'cont0']=cont0_ub\ntrain_df.loc[train_df['cont0']<=cont0_lb,'cont0']=cont0_lb\ntrain_df.loc[train_df['cont6']>=cont6_ub,'cont6']=cont6_ub\ntrain_df.loc[train_df['cont6']<=cont6_lb,'cont6']=cont6_lb\ntrain_df.loc[train_df['cont8']>=cont8_ub,'cont8']=cont8_ub\ntrain_df.loc[train_df['cont8']<=cont8_lb,'cont8']=cont8_lb","f4839310":"cont0_lb,cont0_ub = getbounds(test_df['cont0'])\ncont6_lb,cont6_ub = getbounds(test_df['cont6'])\ncont8_lb,cont8_ub = getbounds(test_df['cont8'])\n\ntest_df.loc[test_df['cont0']>=cont0_ub,'cont0']=cont0_ub\ntest_df.loc[test_df['cont0']<=cont0_lb,'cont0']=cont0_lb\ntest_df.loc[test_df['cont6']>=cont6_ub,'cont6']=cont6_ub\ntest_df.loc[test_df['cont6']<=cont6_lb,'cont6']=cont6_lb\ntest_df.loc[test_df['cont8']>=cont8_ub,'cont8']=cont8_ub\ntest_df.loc[test_df['cont8']<=cont8_lb,'cont8']=cont8_lb","c8d0738e":"plt.figure(figsize=(12,10))\nsns.boxplot(data=train_df[cont_cols])\nplt.show()","c4209a3c":"plt.figure(figsize=(12,10))\nsns.boxplot(data=test_df[cont_cols])\nplt.show()","1f319a04":"for col in cat_cols:\n        values=train_df[col].unique()\n        labels_ordered = { k:i for i,k in enumerate(sorted(values),0)}\n        train_df[col]=train_df[col].map(labels_ordered)","891e58ea":"for col in cat_cols:\n        values=test_df[col].unique()\n        labels_ordered = { k:i for i,k in enumerate(sorted(values),0)}\n        test_df[col]=test_df[col].map(labels_ordered)","e257c5a9":"X = train_df.drop(columns=[\"id\",\"target\"])\nY = train_df[\"target\"]\ntest_df=test_df.drop(columns=\"id\")","c557cd55":"x_train,x_test,y_train,y_test=train_test_split(X, Y, test_size = 0.1, random_state = 21)","7e280005":"def tuner(trial):    \n\n    param = {\n        \"objective\": \"regression\",        \n        \"n_jobs\": -1,\n        \"verbose\": -1,\n        \"force_col_wise\": True,  \n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 8300,8500),\n        \"boosting_type\": 'gbdt',\n        \"max_bin\": 251,\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1), \n        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 1.0),\n        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 1.0),        \n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 25),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 40,50),\n        \"colsample_bytree\":trial.suggest_float(\"colsample_bytree\",0.8, 0.9),\n        \"subsample\":trial.suggest_float(\"subsample\", 0.8,0.9),\n        \"subsample_freq\": 100,\n    }    \n    model = lgbm.LGBMRegressor(**param, random_state = 21)\n    model.fit(\n        x_train, y_train,\n        eval_set=[(x_test, y_test)],\n        categorical_feature =cat_cols,\n        eval_metric='rmse',\n        verbose=2000,\n        early_stopping_rounds=1000\n    )\n    preds = model.predict(x_test)\n    rmse =   mean_squared_error(y_test, preds, squared=False)\n    return rmse\n\nstart_time=timer(None)\nstudy = optuna.create_study()\nstudy.optimize(tuner, n_trials = 100)\ntimer(start_time)","62c21424":"study.best_trial","4773ef43":"study.best_params","10a4a5da":"study.best_value","38d17b03":"studydf = study.trials_dataframe()\nstudydf","3d0504ab":"studydf.loc[studydf['value']<=0.717]","7baa3bb9":"optuna.visualization.plot_optimization_history(study)","7c73f58e":"optuna.visualization.plot_slice(study)","6253edb2":"optuna.visualization.plot_param_importances(study)","8439fcb6":"optuna.visualization.plot_param_importances(\n    study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\"\n)","0341873f":"param_final={\n        \"objective\": \"regression\", \n        \"boosting_type\": 'gbdt',\n        \"max_bin\": 251,\n        \"n_jobs\": -1,\n        \"verbose\": -1,\n        \"force_col_wise\": True,  \n        'n_estimators': 8428,\n        'learning_rate': 0.04052031827001172,\n        'reg_alpha': 0.045791986756797375,\n        'reg_lambda': 0.002070681206144435,\n        'max_depth': 2,\n        'num_leaves': 42,\n        'colsample_bytree': 0.8645965377799666,\n        'subsample': 0.8943640005229004,\n        \"subsample_freq\": 100\n}","c9fc9096":"lgb_model = lgbm.LGBMRegressor(**param_final)\nlgb_model.fit(x_train,y_train)","43e3de29":"target_lgb = lgb_model.predict(test_df)\nsub_df[\"target\"] = target_lgb\nsub_df[[\"id\", \"target\"]].to_csv(\"sub_LGB_9.csv\", index=False)\nsub_df.head()","fa34ba1e":"#### Visualize individual hyperparameters as slice plot. ","90c06957":"## 4.a) Split-out validation dataset","d1ec5f67":"#### Visualize parameter importances.","3dbbe0d7":"#### Visualize which hyperparameters are affecting the trial duration with hyperparameter importance.","dd10292b":"#### Visualize the optimization history. ","d6fd7e11":"# 4. Evaluate Algorithms","6b740e33":"#### 'train_test_split' function is used to split the train data into train and test where we use the test to validate our model's performance before finally predicting on the test set\n#### 'random_state' is used to reproduce the results and 'test_size' will specify the fraction of train data we should use as validation","5709becb":"### * To get the details of the best trial","09780531":"### For hyperparameter sampling, Optuna provides the following features:\n* optuna.trial.suggest_categorical() for categorical parameters\n* optuna.trial.suggest_int() for integer parameters\n* optuna.trial.suggest_float() for floating point parameters\n\nWith optional arguments of step and log, we can discretize or take the logarithm of integer and floating point parameters.\n\n* suggest_loguniform - another way to ask suggestions for the log of the value\n\nOther terminology in Optuna is as follows:\n\n* Trial: A single call of the objective function\n* Study: An optimization session, which is a set of trials\n* Parameter: A variable whose value is to be optimized","5ce6a92a":"### * To get the dictionary of parameter name and parameter values of the best trial","63898cec":"## 2.a) Descriptive statistics","7f42d3b6":"### Outlier Treatment","3beae50d":"## Tuning LightGBM using optuna","286365e7":"# 1. Preparation","685968d9":"#### Optuna also provides the study data as a dataframe object","2b60bde8":"# 5. Prepare final paramters and predictions","fe0a28e0":"# Observations - \n* There  are 300000 training rows  and 200000 test rows in the data\n* There are 10 categorical columns \n* There are 14 continuous numerical columns\n* No NULL values in the data\n* No duplicate values present\n* The unique category values are **same** in the train and test dataset\n* Columns Cont0, Cont6, and Cont8 have outliers - we cap the ouliers at the upper \/ lower bounds\n* For the categorical variables we map the values to numbers ","6fa3a439":"### Neither we have any missing values nor any duplicates","205157cb":"#### Some Functions the we would use","34563eaf":"## 1.b) Load Dataset","666c01d0":"## 3.a) Data Transformation","e47e24dd":"### * To get the best observed value of the objective function:","1de46e30":"# 3. Preparing Data","af696153":"## Quick Visualization for Hyperparameter Optimization Analysis","c826faaf":"# 2. Summarize Data","ddc9dde7":"### After Outlier Treatment","2f99da25":"# 6.. Prepare final predictions and Submit","f2c3ba17":"## 1.a) Load Libraries"}}