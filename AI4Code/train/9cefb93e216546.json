{"cell_type":{"ad583c91":"code","d6ccba2d":"code","67caa488":"code","7496a4ed":"code","07b1f8ed":"code","28407ac6":"code","7c542002":"code","3cefe17c":"code","8b1237cd":"code","075d83fa":"code","a1fc15e8":"code","0f9d00f7":"code","09ae15f3":"markdown","ca684aa5":"markdown","3f78bdd8":"markdown","53d09c55":"markdown","4d062588":"markdown"},"source":{"ad583c91":"#Load data from file\nimport pandas as pd\n\ndef loadData(path, sep=',', headerIndex=0):\n    df = pd.read_csv(path, sep=sep, header=headerIndex)\n    matrix = df.values\n    number_of_columns = df.columns.size\n    X = matrix[:, 0:number_of_columns - 1]\n    y = matrix[:, [number_of_columns - 1]]\n\n    return X, y","d6ccba2d":"#compute cost\ndef computeCost(X, y, theta):\n    m = np.shape(y)[0]\n\n    J = (1 \/ (2 * m)) * np.sum(np.power((np.matmul(X, theta) - y), 2))\n    return J","67caa488":"#compute Gradient\ndef computeGradiant(X, y, theta):\n    m = np.shape(y)[0]\n\n    gradiant = (1 \/ m) * (np.matmul(X.T, np.matmul(X, theta) - y))\n    #print(\"Gradiant value : \", gradiant)\n    return gradiant","7496a4ed":"#This is simple gradiant desent algorithm. Here Goal is to minimize the cost function over no of iteration.\ndef gradiant_desent_algo(X,\n                         y,\n                         learningRate=.01,\n                         iteration=100,\n                         should_print_cost=True):\n\n    #Initalize vector theata as zero\n    theta = np.zeros((X.shape[1], 1))\n    \n    J = computeCost(X, y, theta)\n    print(\"Initial cost : \", J)\n\n    #J_History can be used to visualize how cost changs over every iteration\n    J_History = np.full((iteration + 1, 1), J)\n\n    for i in range(iteration):\n        gradiant = computeGradiant(X, y, theta)\n        theta = theta - learningRate * gradiant\n        \n        #Optionally caluate J_History for visualization\n        J = computeCost(X, y, theta)\n        J_History.itemset((i + 1, 0), J)\n        if should_print_cost:\n            print(\"Cost after iteration \", (i + 1), \" : \", J)\n\n    print(\"Final cost : \", J)\n    return theta, J_History","07b1f8ed":"def gradiant_decent_solution(X, y, iterations = 10000):\n    #adding extra column for 0th index\n    new_X = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n\n    theta, J_History = gradiant_desent_algo(\n        new_X, y, learningRate=.01, iteration=iterations, should_print_cost=False)\n    \n    # Equation for 1 feature and 1 bias\n    # y = mx + c\n    m = theta[1]\n    c = theta[0]\n    print(\"Value of m : \", m)\n    print(\"Value of c : \", c)\n\n    #show J_history\n    plotCostFunction(J_History)\n\n    #plot data\n    plotData(X, y, \"Linear Regression with Gradient Descent.\")\n    abline(m, c)","28407ac6":"#noraml equation technique\nimport numpy as np\nfrom numpy.linalg import inv, pinv\n\ndef calculate_theta_with_normal_eqaution(X, y):\n    theta = np.matmul(np.matmul(inv(np.matmul(X.T, X)), X.T), y)\n    #or\n    theta = np.matmul(pinv(X), y)\n    # hypothesis = x0*theta0 + x1*theta1 + x2*theta2 + ...\n    return theta","7c542002":"def noraml_equation_solution(X, y):\n    new_X = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n    theta = calculate_theta_with_normal_eqaution(new_X, y)\n\n    # Equation for 1 feature and 1 bias\n    # y = mx + c\n    m = theta[1]\n    c = theta[0]\n    print(\"Value of m : \", m)\n    print(\"Value of c : \", c)\n\n    #plot data\n    plotData(X, y, \"Linear Regression with Normal Equation.\")\n    abline(m, c)","3cefe17c":"#Visualizting cost function, data and prediction \n%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndef plotData(X, y, title, markersize=12):\n    plt.plot(X, y, 'b+', markersize=markersize)\n    #plt.axis([0, 10, 0, 10])\n    X_label = plt.xlabel('X Values')\n    Y_label = plt.ylabel('Y Values')\n    X_label.set_color(\"#FFFFFF\")\n    Y_label.set_color(\"#FFFFFF\")\n    plt.title(title)\n\ndef abline(slope, intercept):\n    \"\"\"Plot a line from slope and intercept\"\"\"\n    axes = plt.gca()\n    x_vals = np.array(axes.get_xlim())\n    y_vals = intercept + slope * x_vals\n    plt.plot(x_vals, y_vals, 'g--')\n    plt.show()\n\ndef plotCostFunction(J_history):\n    X = np.arange(J_history.shape[0])\n    y = J_history\n    plt.plot(X, y, 'b--')\n    X_label = plt.xlabel('Iteration')\n    Y_label = plt.ylabel('Cost')\n    X_label.set_color(\"#FFFFFF\")\n    Y_label.set_color(\"#FFFFFF\")\n    plt.title('Cost function changes over no of iterations')\n    plt.show()","8b1237cd":"#main\n#X, y = loadData('..\/input\/perfect_points.csv')\nX, y = loadData('..\/input\/points_with_small_noise.csv')\n","075d83fa":"noraml_equation_solution(X,y)","a1fc15e8":"gradiant_decent_solution(X, y, iterations=100)","0f9d00f7":"gradiant_decent_solution(X, y, iterations=100000)","09ae15f3":"## Gradient Descent : $$\\frac{\\partial }{\\partial \u03b8_i}J =  \\frac{1}{m} \\sum_{i=1}^{m} [(h_\u03b8(X_i) - y_i )*(X_i) ]$$\n","ca684aa5":"## Hypothesis : $$h_\u03b8(X) = \u03b8_0 + \u03b8_1*x_1 + \u03b8_2*x_2 + ...$$\n## Cost function : $$J  =  \\frac{1}{2m} \\sum_{i=1}^{m} (h_\u03b8(X_i) - y_i )^2 $$","3f78bdd8":"## This notebook will help you understand Linear Regression and Gradient Descent alogorithm and help you build intuition around it.\n\nThis tutorial is inspired by **Andrew NG** machine learning course.\n","53d09c55":"## Normal Eqation : $$ \u03b8 = (X^TX)^{-1}X^Ty$$","4d062588":"## Gradient Descent algorithm : \n## $$ \\text{ Minimize \u03b8 until converge : } $$\n## $$\u03b8 := \u03b8  - \u03b1*\\frac{\\partial }{\\partial \u03b8_i}J $$\n\n"}}