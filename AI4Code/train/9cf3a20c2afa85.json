{"cell_type":{"d3466a74":"code","49ca07c7":"code","e7e83954":"code","5ba86ba0":"code","52030b2e":"code","999fc04a":"code","a4900752":"code","154caf23":"code","3a213f27":"code","dada1704":"code","bd049c88":"code","842a6eb1":"code","5c0b5007":"code","addb2c2d":"code","b131f6d2":"code","04017ec7":"code","0f4561a6":"code","1d36ff1c":"code","f560deab":"code","afe6c97a":"code","dfbbb501":"code","313caddf":"code","6e4f11d0":"code","76184c06":"code","9c9b03b9":"code","4cd5e06e":"code","7319a594":"code","99c9861d":"code","3ec45a8d":"code","c8f524e4":"code","f3394eb6":"code","7d26012f":"code","99c85b3a":"code","22e1d2a1":"code","ce7e4389":"code","e0e93361":"code","02307399":"code","952e7782":"code","72aee60c":"code","d48359aa":"code","49d5fdb1":"code","83d73b42":"code","ce1fe536":"code","1474c161":"code","e63cbe90":"code","cb261801":"code","9a604fd6":"code","89dab9ed":"code","a79c2a91":"code","09027a3d":"code","d7b80c77":"code","70186bf8":"code","f22581c2":"code","542654ab":"code","85b2ae4d":"code","d8a891b6":"code","bf68cd22":"code","e068a474":"code","e54d57d0":"code","e1f3a9d7":"code","db5e4492":"markdown","e934199a":"markdown","595e36cd":"markdown","c13db970":"markdown","0d076267":"markdown","b98dddb8":"markdown"},"source":{"d3466a74":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/test set\/test set\"))\nimport torch\n# Any results you write to the current directory are saved as output.","49ca07c7":"test_dir=\"..\/input\/test set\/test set\/\"\npath=\"..\/input\/flower_data\/flower_data\/\"\ntrain_dir=path+\"train\/\"\nvalidation_dir=path+\"valid\/\"","e7e83954":"data_size=list()\nclasses=list()\nsizes=list()","5ba86ba0":"for direc in os.listdir(train_dir):\n    data_size.append({direc:len(os.listdir(train_dir+direc))})\n    classes.append(direc)\n    sizes.append(len(os.listdir(train_dir+direc)))\n    ","52030b2e":"import json\ndata=json.loads(open(\"..\/input\/cat_to_name.json\").read())\nnames=data.values()","999fc04a":"import matplotlib.pyplot as plt\nplt.bar(names,sizes)","a4900752":"names=list(names)\nnames","154caf23":"cat_df=pd.DataFrame({\"category\":names,\"train_data\":sizes}).sort_values(\"category\")","3a213f27":"cat_df.head()","dada1704":"cat_df.set_index(\"category\")['train_data'].plot.bar(color=\"r\",figsize=(20,6))","bd049c88":"max(cat_df['train_data'])","842a6eb1":"sum(sizes)","5c0b5007":"classes=list()\ntrain_imgs=list()\nfor direc in os.listdir(train_dir):\n    for img in os.listdir(train_dir+direc):\n        classes.append(direc)\n        train_imgs.append(train_dir+direc+\"\/\"+img)","addb2c2d":"full_dataset=list()\nclasses=pd.Series(classes)\nimages=pd.Series(train_imgs)\n    ","b131f6d2":"full_data=pd.concat([classes,images],axis=1)","04017ec7":"from torchvision import transforms\nimage_transforms={\n   \"train\":\n       transforms.Compose([transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(),\n        transforms.RandomHorizontalFlip(),\n        transforms.CenterCrop(size=224),  # Image net standards\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])    \n        ]),\n     'valid':\n    transforms.Compose([\n        transforms.Resize(size=256),\n        transforms.CenterCrop(size=224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test':\n    transforms.Compose([\n        transforms.Resize(size=256),\n        transforms.CenterCrop(size=224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n}\n     \n  \n             ","0f4561a6":"path","1d36ff1c":"test_dir=\"..\/input\/test set\/\"","f560deab":"from torchvision import datasets\nimage_datasets={x:datasets.ImageFolder(os.path.join(path,x),image_transforms[x]) for x in ['train','valid']}","afe6c97a":"image_datasets['test']=datasets.ImageFolder(test_dir,image_transforms['test'])","dfbbb501":"len(image_datasets['valid'])","313caddf":"from torchvision import datasets\nfrom torch.utils.data import DataLoader\n\ndataloader={\n    'train':DataLoader(image_datasets['train'],batch_size=32,shuffle=True,drop_last=True),\n    'valid':DataLoader(image_datasets['valid'],batch_size=32,shuffle=True,drop_last=True),\n    'test' : DataLoader(image_datasets['test'],batch_size=32,shuffle=True)\n    \n}","6e4f11d0":"trainiter = iter(dataloader['valid'])\nfeatures, labels = next(trainiter)\nfeatures.shape, labels.shape","76184c06":"trainiter = iter(dataloader['test'])\nfeatures, labels = next(trainiter)\nfeatures.shape, labels.shape","9c9b03b9":"from torchvision import models\nmodel = models.resnet34(pretrained=True)","4cd5e06e":"for param in model.parameters():\n    param.requires_grad=False","7319a594":"import torch.nn as nn\nin_features=512\nout_classes=102\nmodel.fc = nn.Sequential(nn.Linear(in_features,256),\n                         nn.ReLU(),\n                         nn.Dropout(0.5),\n                         nn.Linear(256,out_classes),\n                         nn.LogSoftmax(dim=1))\n\n","99c9861d":"model = model.to(\"cuda\")\nmodel = nn.DataParallel(model)","3ec45a8d":"from torch import optim\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(),lr=1e-03)","c8f524e4":"from torch import cuda,optim\ntrain_on_gpu = cuda.is_available()\nprint(f'Train on gpu: {train_on_gpu}')\n\n# Number of gpus\nif train_on_gpu:\n    gpu_count = cuda.device_count()\n    print(f'{gpu_count} gpus detected.')\n    if gpu_count > 1:\n        multi_gpu = True\n    else:\n        multi_gpu = False","f3394eb6":"from timeit import default_timer as timer\nimport torch\ndef train(model,criterion,optimizer,train_loader,valid_loader,save_file_name,max_epochs_stop=3,n_epochs=20\n         ,print_every=2):\n    \n    \n    epochs_no_improve=0\n    valid_loss_min = np.Inf\n    \n    valid_max_acc=0\n    history=[]\n    \n    try:\n        print(f'Model has been trained for: {model.epochs} epochs \\n')\n    except:\n        model.epochs=0\n        print(f'Training from scratch\\n')\n    \n    overall_start=timer()\n    \n    for epoch in range(n_epochs):\n        train_loss = 0.0\n        valid_loss = 0.0\n        \n        train_acc = 0.0\n        valid_acc = 0.0\n        \n        model.train()\n        start=timer()\n        \n        #training loop\n        for ii,(data,target) in enumerate(train_loader):\n            \n            if train_on_gpu:\n                data,target=data.cuda(),target.cuda()\n                \n            optimizer.zero_grad()\n            \n            output = model(data)\n          \n            loss = criterion(output,target)\n            loss.backward()\n            \n            optimizer.step()\n            \n            train_loss+=loss.item() * data.size(0)\n            \n            _,pred = torch.max(output,dim=1)\n            correct_tensor = pred.eq(target.data.view_as(pred))\n            \n            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n            \n            train_acc+=accuracy.item() * data.size(0)\n            print(\n                f'Epoch: {epoch}\\t{100 * (ii + 1) \/ len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n                end='\\r')\n        \n        else:\n            model.epochs+=1\n        \n            with torch.no_grad():\n                model.eval()\n                \n                for data,target in valid_loader:\n                    if train_on_gpu:\n                        data,target=data.cuda(),target.cuda()\n                        \n                        output = model(data)\n                       \n                        loss = criterion(output,target)\n                        \n                        valid_loss+=loss.item() * data.size(0)\n                        \n                        _,pred = torch.max(output,dim=1)\n                        correct_tensor = pred.eq(target.data.view_as(pred))\n                        accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n                        \n                        valid_acc+=accuracy.item() * data.size(0)\n                        \n                train_loss = train_loss \/ len(train_loader.dataset)\n                valid_loss = valid_loss \/ len(valid_loader.dataset)\n                \n                train_acc = train_acc \/ len(train_loader.dataset)\n                valid_acc = valid_acc \/ len(valid_loader.dataset)\n                \n                history.append([train_loss,valid_loss,train_acc,valid_acc])\n                \n                if (epoch + 1) % print_every == 0:\n                    print(\n                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n                    )\n                    print(\n                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n                    )\n                if valid_loss < valid_loss_min:\n                    # Save model\n                    torch.save(model.state_dict(), save_file_name)\n                    # Track improvement\n                    epochs_no_improve = 0\n                    valid_loss_min = valid_loss\n                    valid_best_acc = valid_acc\n                    best_epoch = epoch\n                    \n                else:\n                    epochs_no_improve += 1\n                    # Trigger early stopping\n                    if epochs_no_improve >= max_epochs_stop:\n                        print(\n                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n                        )\n                        total_time = timer() - overall_start\n                        print(\n                            f'{total_time:.2f} total seconds elapsed. {total_time \/ (epoch+1):.2f} seconds per epoch.'\n                        )\n\n                        # Load the best state dict\n                        model.load_state_dict(torch.load(save_file_name))\n                        # Attach the optimizer\n                        model.optimizer = optimizer\n\n                        # Format history\n                        history = pd.DataFrame(\n                            history,\n                            columns=[\n                                'train_loss', 'valid_loss', 'train_acc',\n                                'valid_acc'\n                            ])\n                        return model, history\n                    \n                    \n    model.optimizer = optimizer\n    total_time = timer() - overall_start\n    print(\n        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n    )\n    print(\n        f'{total_time:.2f} total seconds elapsed. {total_time \/ (epoch):.2f} seconds per epoch.'\n    )\n    # Format history\n    history = pd.DataFrame(\n        history,\n        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n    return model, history\n","7d26012f":"save_file_name = 'resnet34-transfer-1.pth'\ncheckpoint_path = 'resnet34-transfer-1.pth'\n\nmodel, history = train(\n    model,\n    criterion,\n    optimizer,\n    dataloader['train'],\n    dataloader['valid'],\n    save_file_name=save_file_name,\n    max_epochs_stop=5,\n    n_epochs=20,\n    print_every=1)","99c85b3a":"plt.figure(figsize=(8, 6))\nfor c in ['train_loss', 'valid_loss']:\n    plt.plot(\n        history[c], label=c)\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Average Negative Log Likelihood')\nplt.title('Training and Validation Losses')","22e1d2a1":"plt.figure(figsize=(8, 6))\nfor c in ['train_acc', 'valid_acc']:\n    plt.plot(\n        100 * history[c], label=c)\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Average Accuracy')\nplt.title('Training and Validation Accuracy')","ce7e4389":"# model.load_state_dict(torch.load(save_file_name))","e0e93361":"def predict(image_path, model, topk=5):\n    \"\"\"Make a prediction for an image using a trained model\n\n    Params\n    --------\n        image_path (str): filename of the image\n        model (PyTorch model): trained model for inference\n        topk (int): number of top predictions to return\n\n    Returns\n\n    \"\"\"\n    real_class = image_path.split('\/')[-2]\n\n    # Convert to pytorch tensor\n    img_tensor = process_image(image_path)\n\n    # Resize\n    if train_on_gpu:\n        img_tensor = img_tensor.view(1, 3, 224, 224).cuda()\n    else:\n        img_tensor = img_tensor.view(1, 3, 224, 224)\n\n    # Set to evaluation\n    with torch.no_grad():\n        model.eval()\n        # Model outputs log probabilities\n        out = model(img_tensor)\n        ps = torch.exp(out)\n\n        # Find the topk predictions\n        topk, topclass = ps.topk(topk, dim=1)\n\n        # Extract the actual classes and probabilities\n        top_classes = [\n            model.idx_to_class[class_] for class_ in topclass.cpu().numpy()[0]\n        ]\n        top_p = topk.cpu().numpy()[0]\n\n        return img_tensor.cpu().squeeze(), top_p, top_classes, real_class","02307399":"train_dir","952e7782":"# def evaluate(model, test_loader, criterion, topk=(1, 5)):\n#     \"\"\"Measure the performance of a trained PyTorch model\n\n#     Params\n#     --------\n#         model (PyTorch model): trained cnn for inference\n#         test_loader (PyTorch DataLoader): test dataloader\n#         topk (tuple of ints): accuracy to measure\n\n#     Returns\n#     --------\n#         results (DataFrame): results for each category\n\n#     \"\"\"\n\n#     classes = []\n#     losses = []\n#     # Hold accuracy results\n#     acc_results = np.zeros((len(test_loader.dataset), len(topk)))\n#     i = 0\n\n#     model.eval()\n#     with torch.no_grad():\n\n#         # Testing loop\n#         for data, targets in test_loader:\n\n#             # Tensors to gpu\n#             if train_on_gpu:\n#                 data, targets = data.to('cuda'), targets.to('cuda')\n\n#             # Raw model output\n#             out = model(data)\n#             # Iterate through each example\n#             for pred, true in zip(out, targets):\n#                 # Find topk accuracy\n#                 acc_results[i, :] = accuracy(\n#                     pred.unsqueeze(0), true.unsqueeze(0), topk)\n#                 classes.append(model.idx_to_class[true.item()])\n#                 # Calculate the loss\n#                 loss = criterion(pred.view(1, n_classes), true.view(1))\n#                 losses.append(loss.item())\n#                 i += 1\n\n#     # Send results to a dataframe and calculate average across classes\n#     results = pd.DataFrame(acc_results, columns=[f'top{i}' for i in topk])\n#     results['class'] = classes\n#     results['loss'] = losses\n#     results = results.groupby(classes).mean()\n\n#     return results.reset_index().rename(columns={'index': 'class'})","72aee60c":"data = {\n    'train':\n    datasets.ImageFolder(root=train_dir, transform=image_transforms['train']),\n    'test':\n    datasets.ImageFolder(root=test_dir, transform=image_transforms['test'])\n}","d48359aa":"model.class_to_idx = data['train'].class_to_idx\nmodel.idx_to_class = {\n    idx: class_\n    for class_, idx in model.class_to_idx.items()\n}\n\nlist(model.idx_to_class.items())[:10]","49d5fdb1":"dataloader","83d73b42":"test_dir='..\/input\/test set\/'","ce1fe536":"test_dir","1474c161":"model.load_state_dict(torch.load(save_file_name))","e63cbe90":"img=test_dir+\"test set\/gc8.jpg\"\nimg=plt.imread(img)\ns=img.shape\nt=torch.tensor(img).view((1,s))\nprint(t)","cb261801":"def process_image(image_path):\n    \"\"\"Process an image path into a PyTorch tensor\"\"\"\n\n    image = Image.open(image_path)\n    # Resize\n    img = image.resize((256, 256))\n    if img.mode == 'RGBA':\n        rgb_im=img.convert('RGB')\n        \n        img = rgb_im\n        \n        \n    # Center crop\n    width = 256\n    height = 256\n    new_width = 224\n    new_height = 224\n\n    left = (width - new_width) \/ 2\n    top = (height - new_height) \/ 2\n    right = (width + new_width) \/ 2\n    bottom = (height + new_height) \/ 2\n    img = img.crop((left, top, right, bottom))\n    \n    # Convert to numpy, transpose color dimension and normalize\n    img = np.array(img).transpose((2, 0, 1)) \/ 256\n\n    # Standardization\n    means = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n    stds = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n    \n    img = img - means\n    img = img \/ stds\n\n    img_tensor = torch.Tensor(img)\n\n    return img_tensor","9a604fd6":"from PIL import Image\nx = process_image(test_dir+\"test set\/gc8.jpg\")","89dab9ed":"\ndef predict(image_path, model, topk=5):\n    \"\"\"Make a prediction for an image using a trained model\n\n    Params\n    --------\n        image_path (str): filename of the image\n        model (PyTorch model): trained model for inference\n        topk (int): number of top predictions to return\n\n    Returns\n\n    \"\"\"\n    real_class = image_path.split('\/')[-2]\n\n    # Convert to pytorch tensor\n    img_tensor = process_image(image_path)\n\n    # Resize\n    if train_on_gpu:\n        img_tensor = img_tensor.view(1, 3, 224, 224).cuda()\n    else:\n        img_tensor = img_tensor.view(1, 3, 224, 224)\n\n    # Set to evaluation\n    with torch.no_grad():\n        model.eval()\n        # Model outputs log probabilities\n        out = model(img_tensor)\n        ps = torch.exp(out)\n\n        # Find the topk predictions\n        topk, topclass = ps.topk(topk, dim=1)\n        \n        # Extract the actual classes and probabilities\n        top_classes = [\n            model.idx_to_class[class_] for class_ in topclass.cpu().numpy()[0]\n        ]\n        top_p = topk.cpu().numpy()[0]\n\n        return img_tensor.cpu().squeeze(),top_classes, top_p,real_class","a79c2a91":"img, top_p, top_classes,_= predict(test_dir+\"test set\"+\"\/\"+\"gc11.png\", model)","09027a3d":"results=list()\n\nfor img in os.listdir(test_dir+\"test set\"):\n    \n   try:\n    imag, top_p, top_classes,_= predict(test_dir+\"test set\"+\"\/\"+img, model)\n    \n    results.append(top_p[0])\n    \n   except:\n    \n    print(img)\n   ","d7b80c77":"image_names=os.listdir(test_dir+\"test set\")","70186bf8":"import json\ncategory_labels=json.loads(open(\"..\/input\/cat_to_name.json\").read())","f22581c2":"results","542654ab":"cat_results=list()\nfor i in results:\n    cat_results.append(category_labels[i])","85b2ae4d":"cat_results","d8a891b6":"df = pd.DataFrame({'image_name': image_names , 'flower_id': results,'flower_name' : cat_results}, columns=['image_name', 'flower_id', 'flower_name']).sort_values('image_name',ascending=True)","bf68cd22":"df.to_csv('submission.csv',index=False)","e068a474":"x = process_image(test_dir+\"test set\/gc8.jpg\")","e54d57d0":"pd.read_csv(\"submission.csv\")","e1f3a9d7":"plt.imshow(plt.imread(test_dir+\"test set\/gc11.png\"))","db5e4492":"# evaluation","e934199a":"# Data Splitting","595e36cd":"# DataLoaders","c13db970":"# Visualization of data","0d076267":"# Training Model ","b98dddb8":"# Building model"}}