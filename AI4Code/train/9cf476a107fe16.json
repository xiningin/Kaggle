{"cell_type":{"091258c9":"code","7428e411":"code","cd326dc9":"code","bfa7af20":"code","c850b7f5":"code","9f7f9e0b":"code","caf2fa50":"code","3722d665":"code","5f08fd83":"code","b49fb67e":"code","6e411ca8":"code","7d1d6280":"code","b08842f7":"code","abf4fd6d":"code","b80a8ea5":"code","a154baed":"markdown","03971486":"markdown","b4a660ed":"markdown","2e4ef13f":"markdown","8263b5a9":"markdown","4d83309c":"markdown","979a6a79":"markdown","1fce9296":"markdown","6e47875c":"markdown","eac2bf88":"markdown","5b93ecbd":"markdown","aabba13e":"markdown","e5018984":"markdown","7f14b5be":"markdown","50db54a5":"markdown","2e223d0c":"markdown","0566a123":"markdown","4f4c08f6":"markdown","942abef6":"markdown","f157ea16":"markdown","2bea089d":"markdown","816999a3":"markdown","9420e842":"markdown","cf737de0":"markdown","6c8a6be7":"markdown","cfe409e7":"markdown","ad304ae4":"markdown","87623993":"markdown","bc02c16f":"markdown","ab3d58f3":"markdown","148744fc":"markdown","89128fe1":"markdown","24f65e91":"markdown","7563427d":"markdown","5b619e07":"markdown","bce43f81":"markdown"},"source":{"091258c9":"import pandas as pd \ndf = pd.read_csv('..\/input\/lecture6drug4\/drug-4instances.csv') #Read data to a data frame\ndf","7428e411":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nsns.scatterplot(x = \"Age\", y = \"Na\/K\", hue  = 'Drug', data = df,s=200) #s=200: set marker sizes =200\nplt.show() ","cd326dc9":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport sklearn.model_selection as model_selection\nfrom sklearn.neighbors import KNeighborsClassifier\n\ndf = pd.read_csv(\"..\/input\/lecture06risk\/ClassifyRisk.csv\")\ndf","bfa7af20":"churn_crosstab = pd.crosstab(df[\"mortgage\"], df[\"risk\"],normalize ='index')\nchurn_crosstab.plot(kind = 'bar', stacked = 'true',grid=True, title = \"risk with and without mortage\")\nplt.xlabel('mortage')\nplt.ylabel('Count')\nplt.show() ","c850b7f5":"from sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder(categories = [['n','y']]) # create an encoder with order\ndf['mortgage'] = encoder.fit_transform(df['mortgage'].values.reshape(-1, 1)) # fit encoder with data and transfer data \ndf","9f7f9e0b":"from sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder(categories = [['bad loss','good risk']]) # create an encoder with order\ndf['risk'] = encoder.fit_transform(df['risk'].values.reshape(-1, 1)) # fit encoder with data and transfer data \ndf","caf2fa50":"df = pd.get_dummies(df,prefix   =['marital_status'], columns = ['marital_status'])\ndf ","3722d665":"column_name = 'risk'\ncolumn_0 = df.pop(column_name)\ndf.insert(0, column_name, column_0)\ndf.head(5)","5f08fd83":"X = df.iloc[:,1:]\ny = df.iloc[:,0] \nprint(\"Features X\\n\",X[0:5])\nprint(\"Target y\\n\", y[0:5])","b49fb67e":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.3, random_state = 7)","6e411ca8":"#normalisation\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler() \nscaler.fit(X_train)\nX_train = scaler.transform(X_train) \nX_test = scaler.transform(X_test)","7d1d6280":"X_train[0:5]","b08842f7":"X_test[0:5]","abf4fd6d":"knn = KNeighborsClassifier(n_neighbors=1,weights=\"uniform\", metric=\"euclidean\")\nknn.fit(X_train, y_train)","b80a8ea5":"from sklearn.metrics import accuracy_score\ny_predict = knn.predict(X_test) \naccuracy = accuracy_score(y_test, y_predict)\nprint(accuracy) ","a154baed":"### OneHot Encoder method `pandas.get_dummies` \nSince categorical feature `marital_status` has three classes without an order, we use OneHot Encoder\n* However, we don't adopt Sklearn's OneHot Encoder because adding columns names is a bit complex \n* Instead, Pandas its own OneHot Encoder method called `pandas.get_dummies`\n* A big advantage is that `pandas.get_dummies` manipulates dataframes. \n* New column names are automatically added with a prefix `'marital_status`  and its class labels `married`, `single` or `other`. ","03971486":"### Voting Parameter in Sklearn\n`sklearn.neighbors.KNeighborsClassifier (weights=\u2019uniform\u2019)`\n\n`weights` : default = \u2018uniform\u2019\n\n1. **\u2018uniform\u2019** : uniform weights (Simple Unweighted Voting). All instances in neighbourhood are weighted equally.\n2. **\u2018distance\u2019** : weight points by the inverse of their distance (Weighted Voting). The closer, the more simlar.\n\n","b4a660ed":"### OneHot Encoder\n* OneHot Encode is used to measure the \u201cdifference\u201d with 0, 1\n\n\"maths = 0001\"\n\n\"physics = 0010\", \n\n\"English = 0100\"\n\n\"chemistry = 1000\"\n","2e4ef13f":"## Part 1  k-Nearest Neighbour Algorithm\n### Classification tasks in real world\n\nClassification is probably the most common data mining task\n\n* **Banking**:\t\tdetermine whether a mortgage application is good\n* **Education**:\tplace a student into a particular track with respect to special needs\n* **Medicine**:\tdiagnose whether a disease is present\n* **Law**:\t\tdetermine if a will is fraudulent\n* **Security**:\t\tidentify whether a certain financial transaction represents a terrorist threat","8263b5a9":"###  k-Nearest Neighbor Algorithm: k=3\n![image.png](attachment:image.png)\n* Compare the new record with 3 instances. \n* This is called 3-nearest neighbour.\n* Each instance (points) in the neighbour of new patient (blue) can be taken to vote drug equally\n* Tie: the probability of assigning drug `AX, Y, BC` is equal = 1\/3. \n* In this case, the simple voting scheme does not help \n* Better strategy: Think about weighting on distance","4d83309c":"## Learning objectives\nLearn the first and simplest classifier\n* What is the k-Nearest Neighbour Algorithm (kNN)? \n* How to measure the similarity between two instances?\n* How to decide the label of a new record based on its similarity to existing instances?\n* What kinds of hyperparameters exist in KNN? ","979a6a79":"### Instance-based learning\nk-Nearest Neighbour algorithm belongs to supervised learning and is a kind of **instance-based** learning\n\n* First, we have a set of instances (Patients 1 to 4)\n* Next, for the new sample (Patient 0), we choose $k$ instances most similar to the new sample. $k$ is a parameter\n* Then, we assignn the class label (drug) to the new sample based on its neighbour instances' label. This dependens on the value of $k$ \nk-Nearest Neighbour is used most often for classification, and also applicable to estimation and prediction tasks","1fce9296":"#### Split data into feature and target parts\n* We separate features and target variables `risk` into `X` and `y` respectively","6e47875c":"### Build and train a model\n#### Split data\n* Split data X, y into training (70%) and test data (30%): `X_train, X_test, y_train, y_test`\n* Splitting is at random. Parameter `random_state =7` controls random number seed for repeating the same splitting","eac2bf88":"### Simple Unweighted Voting\n* First, decide  how many instances  can have a vote in classifying the new record. k=3\n* Then, find the 3 nearest neighbour instances to the new data, measured by the distance  \n* Once the 3 instances are chosen, then taking simple unweighted voting; each instance have one vote, regardless of the distance \n\n* However, simple unweighted voting did not help and resulted in a tie \n* Perhaps weighted voting should be considered?\n","5b93ecbd":"### Manhattan Distance\nManhattan Distance  is commonly-used to measure distance\n\n* Let  $x=(x_1, \\cdots, x_n)$ and $y=(y_1, \\cdots, y_n)$, i.e., two data points with $n$ features\n* $d(x, y)= |x_1-y_1| + \\cdots + |x_n-y_n|$\n\nExample \n* Patient $x$ is 20-years-old and has a Na\/K ratio = 12, i.e., $x=(20,12)$\n* Patient $y$ is 30-years-old and has a Na\/K ratio = 8, i.e., $y=(30,8)$\n* Manhattan distance between these instances \n$$d(x, y)= |x_1-y_1| + \\cdots +|x_n-y_n| = |30-20|+|12-8| =14$$\n* The choice of a distance function will influce the similarity between two data points, and then the classification results","aabba13e":"### Explorateory Data Analysis \n* Explore the relationship between two varaibles `mortgage, risk`\n* Use crosstab and stacked barplot.\n* Findings: higher bad loss for customers with mortgage","e5018984":"### Choosing k\n* What value of k is optimal?\n* There is no an obvious answer\n\n**Smaller k**\n* Choosing a small value for k may lead the algorithm to overfit the data\n* For example, $k=1$, the classification outcome is greatly decided by the the closest instances (perhaps an outlier or exception patient)\n\n**Larger k**\n* Larger values will tend to smooth  data values in the training set\n* It the values become too large (for example, all training instances), locally interesting values will be overlooked\n\nChoosing the appropriate value for k requires balancing these considerations","7f14b5be":"### Weighted Voting\n* Intuitively, an instance that are closer or more similar to the new record should have a larger weight on voting than an instance that is less similar\n* In weighted voting, the vote weight of an instance  is inversely proportional to the distance between the instance and new record \n","50db54a5":"## Part 3 Data preparation in KNN\n### Data Normalization in Distance Function \n* When measuring the distance, one or more attributes can have very large values, relative to the other attributes\n\nExample\n* `income` range is  30,000 - 100,000\n* `years_of_service` range is 0-30\n\nIn this case, the values of `income` will overwhelm the contribution of `years_of_service`\n\nTo avoid this situation, we will need data  normalization\n\n* Min-Max Scaler\n* Standard Scaler","2e223d0c":"### k-Nearest Neighbor Algorithm: k=4\n![image.png](attachment:image.png)\n* Compare with 4 instances: k=4 \n* The 4-nearest neighbour of blue point include 2 red points and 2 other colours\n* Based on voting,  drugs `BC` is chosen\n* Distance is not considered. Voting weights are uniformly distributed over voters\n* Distance is an important factor. Green is closest, so `AX` is more likely to be chosen.","0566a123":"### Evaluate the model\n* Predict labels of test data\n* compare the prediction accuracy on the test data between actual labels `y_test` and predicted labels `y_predict` \n* Result: accuracy is 86.5% on test data","4f4c08f6":"### Categorical features\n\n* The distance function can not be applied directly to categorical values.\n\n* How to measure the distance between two modules \"maths\", \"physics\" or \"English\", \"chemistry\"?\n* Need data transform from a categorical feature to numerical one\n* We may apply Ordinal Encoder to encode each categorical value to a unique a number\n\n\"maths = 1\", \"physics = 2\", \"English = 3\", \"chemistry = 4\"\n\n* But this is not a good solution if the data is not ordinal. What the meaning of distance(English, chemistry) = 1 < distance (chemistry, maths) = 3?\n* Can we find a better solution? \n\n\n","942abef6":"### k-Nearest Neighbor Algorithm: k=1\n* Scatter plot of `Na\/K` against `Age` shows three records in training set  and one record with unknown label.\n* Blue point is closest to (or most similar to) green point (similar `age` and similar `Na\/K` ratio)\n* Thus Green point is taken to vote\n* Because Green point is with label `AX`, this drug is assigned  to blue point\n* Here we compare a new sample with only one instance. This is called 1-nearest neighbour (k=1)","f157ea16":"### k-Nearest Neighbor Algorithm for a toy problem\n**Example:** classify the type of drug a patient should be prescribed\n* The training set consists of only four patients 1-4 with two features: `Na\/K ratio, age`, and one target variable: `drug`  \n* Our task is to classify the type of drug   a new patient 0 should be prescribed, for example, a patient is 25-years-old and has a Na\/K ratio of 13\n","2bea089d":"# k-Nearest Neighbor Algorithm for classification \n\nCOMP20121 Machine Learning for Data Analytics\n\nAuthor: [Jun He](https:\/\/www.ntu.ac.uk\/staff-profiles\/science-technology\/jun-he) ","816999a3":"### Distance metric in Sklearn\n`sklearn.neighbors.DistanceMetric` provide different distance functions, called `metric`\n\n* `KNeighborsClassifier(metric=\u201ceuclidean\u201d)`\n\n* `KNeighborsClassifier(metric=\u201cmanhattan\u201d)`\n","9420e842":"### Euclidean Distance\nEuclidean Distance  is commonly-used to measure distance\n* Let  $x=(x_1, \\cdots, x_n)$ and $y=(y_1, \\cdots, y_n)$, i.e., two data points with $n$ numerical features\n* $d(x, y)= \\sqrt{ (x_1-y_1)^2 + \\cdots (x_n-y_n)^2)}$\n\nExample\n* Patient $x$ is 20-years-old and has a Na\/K ratio = 12, i.e., $x=(20,12)$\n* Patient $y$ is 30-years-old and has a Na\/K ratio = 8, i.e., $y=(30,8)$\n* Euclidean distance between  instances $x, y$\n$$d(x, y)= \\sqrt{ (x_1-y_1)^2 + \\cdots (x_n-y_n)^2)} = \\sqrt{(30-20)^2+(12-8)^2} =10.17$$","cf737de0":"## Part 4 Combination Function\n## Combination Function\n![image.png](attachment:image.png)\n* Given a value of K, a distance function is used to determine which k instances are most similar to the new  record\n* How do the k similar instances combine to provide a classification decision for the new record?\n* Need a combination function. \n* Example: new patient\u2019s neighbour consists of 3 similar instances\n* How to vote?\n\n","6c8a6be7":"### Issues in k-Nearest Neighbor Algorithm \n![image.png](attachment:image.png)\n* How many neighbors should be used? k = ?\n* How is the distance between two points measured?\n* How is the information from neighbors combined when making a classification decision (voting)?\n* Should all points be weighted equally, or should some points have more influence?\n","cfe409e7":"#### Build and train a model\n* build and train a model on training data","ad304ae4":"#### Normalize training data then test data\n* We normalize training data `X_Train` to the range $[0,1]$\n* Create `MinMaxScaler` and fit it with training data `X_Train`\n* In real world, `X_test` is regarded as new data and the scaler is not required to fit with `X_test`  \n* We apply the **same scaler** to `X_test`\n* **No need to normalize target variable `y` because they represent class labels**","87623993":"## Part 5 Case Study: KNN for predicting credit risk\n### Predict Credit Risk\n* Predict credit `risk` based on `mortgage,  loans,  age, marital_status, income` \n* Credit risk data include two categorical features: `Mortgage, marital_status`\n* Categorical data must be converted into numerical one before applying KNN\n","bc02c16f":"#### Target variable location\n* The target variable `risk` is in the middle of the table\n* For the sake of slicing data, we move `risk` to the first column (or the last column)\n* Remove it (use Pandas function `dataframe.pop`) and then insert it to the required locations and target variables `risk` into `X` and `y` respectively","ab3d58f3":"## Part 2 Distance or similarity\n### Distance function\nThe similarity between two instances are measure by a distance function\n* **distance metric** is a real-valued function $d(x,y)$ used to measure the similarity between data points $x,y$ with properties:\n* Property 1: Distance is always non-negative: $d(x,y) \\ge 0$\n* Property 2: Commutative, distance from $x$ to $y$ is distance from $y$ to $x$\n* Property 3: Triangle inequality holds, distance from $x$ to $z$ must be less than or equal to distance from $x$ to $y$ to $z$\n\n ![image.png](attachment:image.png)\n","148744fc":"#### Hyperparameters\nThere are several hyperparameters in KNN.\n1. k: the number of neighbour instances. \n2. distances: Euclidean or Manhattan\n3. Voting methods: simple unweighted voting or weighted voting.\nYou need to manually tune these parameters though different combinations and find out which combination is best, i.e., to achieve the best accuracy.  \n","89128fe1":"### Encode categorical features\n* `mortgage, risk` have two classes. Simply use OrdinalEncoder","24f65e91":"## Summary\n* KNN is a kind of instance learning (supervised learning)\n* Instances in k nearest neighbour vote the class of a new data\n* Similarity is measured by a distance function\n* Instances in neighbour vote the class label: uniform vote and weighted\n* Data may need normalisation in kNN\n* Categorical data should be converted into numerical ones in distance function","7563427d":"###  Classification \n* Classification belongs to **supervised learning**: learning on labelled data\n* Target variable is **categorical** with two or more classes   \n* Examines the relationship between **predictors** (input) and **target variable**  (output)\n","5b619e07":"#### Machine learning\n![image.png](attachment:22f1ad0a-8082-4110-8927-75b6ba368fed.png)","bce43f81":"### Example: data normalization and transformation\n* Let Patient $x$ = 50-year-old male\n* Patient $y$ = 20-year-old male\n* Patient $z$ = 50-year-old female\n* Suppose that Age variable has a range = 50, i.e, minimum = 10, maximum=60\n* MinMax normalize age to\n\n`x_Age = 0.8\ny_Age =0.2\nz_Age = 0.8`\n\n* Encode `Male = 0, 1`, Female = `1, 0`\n* Calculate the distance\n$$d(x,y) =\\sqrt{(0.8-0.2)^2+(0-0)^2+(1-1)^2} =0.6$$\n\n$$d(x,z) =\\sqrt{(0.8-0.8)^2+(1-0)^2+(0-1)^2} =1.4$$\n\n*In this case, Patient $x$ is now closer to Patient $y$ than to Patient $z$"}}