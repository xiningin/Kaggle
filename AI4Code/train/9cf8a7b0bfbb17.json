{"cell_type":{"6b418ae8":"code","33527c9f":"code","220e5c22":"code","3f242ff7":"code","8b824088":"code","a2849864":"code","d1617d7c":"code","8f067140":"code","a72b3ab1":"code","acf641af":"code","77acc909":"code","c4ca16d0":"code","d349bf6f":"code","fd04a991":"code","84e03a56":"code","6dade9c2":"code","16073fd9":"code","9501a77a":"code","10501394":"code","01e84ca0":"code","ee2d72a5":"code","fac96994":"code","088d0be7":"code","f738874b":"code","7718b9e5":"code","f6773efc":"code","33c32c26":"code","a9b28213":"code","d8802df0":"code","d185fdc8":"code","07cd5fc9":"code","af78e14b":"code","edb5ece3":"code","b52e43dd":"code","1ad42afd":"code","7ab7fec0":"code","43034ae1":"code","8a997eac":"code","de59e1ec":"code","dd8c543d":"code","8ab20334":"code","cefe6918":"code","f94e961d":"code","6ce7d7b9":"code","3348e41b":"code","b42983c6":"code","b3342dc5":"code","f2f24be9":"code","55b9269f":"code","0efbc622":"code","d9a1e2e4":"code","feb89922":"code","f1e155e6":"code","3e66d216":"code","49cf8306":"code","b2bbfc7b":"code","5c62cb78":"code","9ea6d050":"code","6025ec0c":"code","cb8bc742":"code","921007d7":"code","61d1362b":"code","14127223":"code","10594c0b":"code","a4a11762":"code","15df9eda":"code","0ef5919f":"code","8d247975":"code","403390ea":"code","86515376":"code","a83a3552":"markdown","510306db":"markdown","05386f85":"markdown","57646015":"markdown","d2267ca0":"markdown","dd9c2f42":"markdown","e96a4d59":"markdown","9fe700a5":"markdown","66506151":"markdown","d768c52d":"markdown","d700669f":"markdown","013b5741":"markdown","276b258d":"markdown","8401a00e":"markdown","e7870c2a":"markdown","c5db1f28":"markdown"},"source":{"6b418ae8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report,accuracy_score,confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\n# Any results you write to the current directory are saved as output.","33527c9f":"df_train=pd.read_csv(\"\/kaggle\/input\/santander-customer-transaction-prediction\/train.csv\")","220e5c22":"df_test=pd.read_csv(\"\/kaggle\/input\/santander-customer-transaction-prediction\/test.csv\")","3f242ff7":"df_test.iloc[:,1:201]=df_test.iloc[:,1:201].astype(np.float32)","8b824088":"df_test.info()","a2849864":"df_train.iloc[:,2:202]=df_train.iloc[:,2:202].astype(np.float32)","d1617d7c":"df_train.info()","8f067140":"df_train.head()","a72b3ab1":"df_test.head()","acf641af":"df_test.isna().values.any()","77acc909":"df_train.isna().values.any()","c4ca16d0":"df_train.describe()","d349bf6f":"df_test.describe()","fd04a991":"li=['var_0', 'var_1','var_2','var_3', 'var_4', 'var_5', 'var_6', 'var_7', 'var_8', 'var_9', 'var_10','var_11','var_12', 'var_13', 'var_14', 'var_15']","84e03a56":"fig, ax = plt.subplots(4,4,figsize=(14,14))\ni=0\nfor il in li:\n    i+=1\n    plt.subplot(4,4,i)\n    plt.scatter(df_test[il],df_train[il],marker=\"*\",alpha=0.6)\n\nplt.tight_layout()\nplt.show()","6dade9c2":"df_train[\"target\"].value_counts()","16073fd9":"df_train.head()","9501a77a":"cor=df_train.corr()\n\ncor_target = abs(cor[\"target\"])\n\nfor i in li:\n    relevant_features = cor_target[cor_target>0.3]\n    relevant_features","10501394":"to_drop=[\"target\",\"ID_code\"]","01e84ca0":"X=df_train.drop(to_drop,1)\ny=df_train[\"target\"]","ee2d72a5":"X","fac96994":"reg = LassoCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)","088d0be7":"imp_coef = coef.sort_values()\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")","f738874b":"imp_coef[imp_coef>0.02]","7718b9e5":"imp_coef[imp_coef<-0.02]","f6773efc":"to_drope=imp_coef[(imp_coef>-0.000625) & (imp_coef<0.000625)].index.tolist()","33c32c26":"X_trains,X_test,y_trains,y_test=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)","a9b28213":"logreg = LogisticRegression(C=1, random_state=42)\nlogreg.fit(X_trains, y_trains)","d8802df0":"y_pred = logreg.predict(X_test)","d185fdc8":"accuracy_score(y_test,y_pred)","07cd5fc9":"print(classification_report(y_test,y_pred))","af78e14b":"cm=confusion_matrix(y_test, y_pred)\ncm_sum = np.sum(cm, axis=1, keepdims=True)\ncm_perc = cm \/ cm_sum.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\nnrows, ncols = cm.shape\nfor i in range(nrows):\n    for j in range(ncols):\n        c = cm[i, j]\n        p = cm_perc[i, j]\n        if i == j:\n            s = cm_sum[i]\n            annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n        elif c == 0:\n            annot[i, j] = ''\n        else:\n            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\ncm = pd.DataFrame(cm, index=np.unique(y_test), columns=np.unique(y_test))\ncm.index.name = 'Actual'\ncm.columns.name = 'Predicted'\n\nfig, ax = plt.subplots(figsize=[5,2])\n\nsns.heatmap(cm, cmap= \"YlGnBu\", annot= annot, fmt='', ax=ax)","edb5ece3":"logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","b52e43dd":"to_drop_beta=[\"target\",\"ID_code\"]\nto_drop_beta=to_drope+to_drop_beta\nX_beta=df_train.drop(to_drop_beta,1)\ny_beta=df_train[\"target\"]","1ad42afd":"X_trains_b,X_test_b,y_trains_b,y_test_b=train_test_split(X_beta,y_beta,test_size=0.2,random_state=42,stratify=y_beta)","7ab7fec0":"logreg = LogisticRegression(C=1, random_state=42)\nlogreg.fit(X_trains_b, y_trains_b)","43034ae1":"y_pred_b = logreg.predict(X_test_b)","8a997eac":"accuracy_score(y_test_b,y_pred_b)","de59e1ec":"print(classification_report(y_test_b,y_pred_b))","dd8c543d":"logit_roc_auc = roc_auc_score(y_test_b, logreg.predict(X_test_b))\nfpr, tpr, thresholds = roc_curve(y_test_b, logreg.predict_proba(X_test_b)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","8ab20334":"clf = svm.SVC(kernel='linear',decision_function_shape='ovr')","cefe6918":"clf = GaussianNB()\nclf.fit(X_trains_b, y_trains_b)","f94e961d":"y_pred_b = clf.predict(X_test_b)","6ce7d7b9":"accuracy_score(y_test_b,y_pred_b)","3348e41b":"print(classification_report(y_test_b,y_pred_b))","b42983c6":"cm=confusion_matrix(y_test_b, y_pred_b)\ncm_sum = np.sum(cm, axis=1, keepdims=True)\ncm_perc = cm \/ cm_sum.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\nnrows, ncols = cm.shape\nfor i in range(nrows):\n    for j in range(ncols):\n        c = cm[i, j]\n        p = cm_perc[i, j]\n        if i == j:\n            s = cm_sum[i]\n            annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n        elif c == 0:\n            annot[i, j] = ''\n        else:\n            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\ncm = pd.DataFrame(cm, index=np.unique(y_test_b), columns=np.unique(y_test_b))\ncm.index.name = 'Actual'\ncm.columns.name = 'Predicted'\n\nfig, ax = plt.subplots(figsize=[5,2])\n\nsns.heatmap(cm, cmap= \"YlGnBu\", annot= annot, fmt='', ax=ax)\n\nsns.heatmap(cm, cmap= \"YlGnBu\", annot= annot, fmt='', ax=ax)","b3342dc5":"logit_roc_auc = roc_auc_score(y_test_b, clf.predict(X_test_b))\nfpr, tpr, thresholds = roc_curve(y_test_b, clf.predict_proba(X_test_b)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","f2f24be9":"#submission_nb = pd.DataFrame({\n#    \"ID_code\": df_test[\"ID_code\"],\n#    \"target\": y_pred_b\n#})\n#submission_nb.to_csv('naive_baise_submission.csv', index=False)","55b9269f":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn import tree\ndt = tree.DecisionTreeClassifier(random_state = 42)\nrf = RandomForestRegressor(n_estimators = 15, random_state = 42)\n","0efbc622":"dt.fit(X_trains_b, y_trains_b)","d9a1e2e4":"y_pred_b = dt.predict(X_test_b)","feb89922":"accuracy_score(y_test_b,y_pred_b)","f1e155e6":"print(classification_report(y_test_b,y_pred_b))","3e66d216":"cm=confusion_matrix(y_test_b, y_pred_b)\ncm_sum = np.sum(cm, axis=1, keepdims=True)\ncm_perc = cm \/ cm_sum.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\nnrows, ncols = cm.shape\nfor i in range(nrows):\n    for j in range(ncols):\n        c = cm[i, j]\n        p = cm_perc[i, j]\n        if i == j:\n            s = cm_sum[i]\n            annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n        elif c == 0:\n            annot[i, j] = ''\n        else:\n            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\ncm = pd.DataFrame(cm, index=np.unique(y_test_b), columns=np.unique(y_test_b))\ncm.index.name = 'Actual'\ncm.columns.name = 'Predicted'\n\nfig, ax = plt.subplots(figsize=[5,2])\n\nsns.heatmap(cm, cmap= \"YlGnBu\", annot= annot, fmt='', ax=ax)\n\nsns.heatmap(cm, cmap= \"YlGnBu\", annot= annot, fmt='', ax=ax)","49cf8306":"logit_roc_auc = roc_auc_score(y_test_b, dt.predict(X_test_b))\nfpr, tpr, thresholds = roc_curve(y_test_b, dt.predict_proba(X_test_b)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()\n","b2bbfc7b":"from xgboost import XGBClassifier\nmodel = XGBClassifier()\nmodel.fit(X_trains_b, y_trains_b)","5c62cb78":"y_pred_b = model.predict(X_test_b)","9ea6d050":"accuracy_score(y_test_b,y_pred_b)","6025ec0c":"print(classification_report(y_test_b,y_pred_b))","cb8bc742":"cm=confusion_matrix(y_test_b, y_pred_b)\ncm_sum = np.sum(cm, axis=1, keepdims=True)\ncm_perc = cm \/ cm_sum.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\nnrows, ncols = cm.shape\nfor i in range(nrows):\n    for j in range(ncols):\n        c = cm[i, j]\n        p = cm_perc[i, j]\n        if i == j:\n            s = cm_sum[i]\n            annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n        elif c == 0:\n            annot[i, j] = ''\n        else:\n            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\ncm = pd.DataFrame(cm, index=np.unique(y_test_b), columns=np.unique(y_test_b))\ncm.index.name = 'Actual'\ncm.columns.name = 'Predicted'\n\nfig, ax = plt.subplots(figsize=[5,2])\n\nsns.heatmap(cm, cmap= \"YlGnBu\", annot= annot, fmt='', ax=ax)\n\nsns.heatmap(cm, cmap= \"YlGnBu\", annot= annot, fmt='', ax=ax)","921007d7":"logit_roc_auc = roc_auc_score(y_test_b, model.predict(X_test_b))\nfpr, tpr, thresholds = roc_curve(y_test_b, model.predict_proba(X_test_b)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","61d1362b":"testik=df_test.drop(['ID_code'],axis=1)\ntestik","14127223":"testik.shape","10594c0b":"testik=testik.drop(to_drope,1)","a4a11762":"cde=df_test[[\"ID_code\"]]\ncde=cde.values.reshape(200000,).shape","15df9eda":"logreg.predict(testik.values).shape","0ef5919f":"submission_log=pd.DataFrame(\n{\n    \"ID_code\":df_test[[\"ID_code\"]].values.reshape(200000,),\n    \"target\":logreg.predict(testik)\n}\n)\nsubmission_log[\"target\"].value_counts()\nsubmission_log.to_csv(\"Logreg.csv\",index=False)","8d247975":"submission_bayise=pd.DataFrame(\n{\n    \"ID_code\":df_test[[\"ID_code\"]].values.reshape(200000,),\n    \"target\":clf.predict(testik)\n}\n)\nsubmission_bayise[\"target\"].value_counts()\nsubmission_bayise.to_csv(\"Bayise.csv\",index=False)","403390ea":"submission_xg=pd.DataFrame(\n{\n    \"ID_code\":df_test[[\"ID_code\"]].values.reshape(200000,),\n    \"target\":model.predict(testik)\n}\n)\nsubmission_xg[\"target\"].value_counts()\nsubmission_xg.to_csv(\"Xgboost.csv\",index=False)","86515376":"submission_tree=pd.DataFrame(\n{\n    \"ID_code\":df_test[[\"ID_code\"]].values.reshape(200000,),\n    \"target\":dt.predict(testik)\n}\n)\nsubmission_tree[\"target\"].value_counts()\nsubmission_tree.to_csv(\"tree.csv\",index=False)","a83a3552":"\u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e embedded \u043c\u0435\u0442\u043e\u0434\u0430 \u043c\u044b \u043d\u0430\u0448\u043b\u0438 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0432\u044b\u0434\u0435\u043b\u044f\u044e\u0449\u0438\u0435\u0441\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f","510306db":"\u042f \u0445\u043e\u0442\u0435\u043b \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0447\u0435\u0440\u0435\u0437 grid search \u043d\u043e \u043e\u043d \u043f\u0440\u0435\u0432\u044b\u0448\u0430\u043b \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438","05386f85":"SVM \u043f\u0440\u043e\u0441\u0442\u043e \u043d\u0435 \u043a\u043e\u043c\u043f\u0438\u043b\u0438\u0442\u0441\u044f \u043f\u0440\u0438\u0448\u043b\u043e\u0441\u044c \u0434\u0432\u0430\u0436\u0434\u044b \u0432\u043e\u0441\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0442\u044c \u0441\u0435\u0441\u0441\u0438\u044e","57646015":"**<center><h1>Random Forest and Decision Tree<\/h1><\/center>**","d2267ca0":"\u0414\u0438\u0437\u0431\u0430\u043b\u0430\u043d\u0441 \u043c\u0435\u0436\u0434\u0443 0 \u0438 1","dd9c2f42":"random forest \u0442\u043e\u0436\u0435 \u043d\u0435 \u043a\u043e\u043c\u043f\u0438\u043b\u0438\u0442\u0441\u044f","e96a4d59":"\u041d\u0435\u0442\u0443 \u043f\u0443\u0441\u0442\u043e\u0442 \u0438\u043b\u0438 \u043a\u0430\u043a\u0438\u0445 \u043b\u0438\u0431\u043e \u043e\u0443\u0442\u043b\u0435\u0435\u0440\u043e\u0432 \u0447\u0442\u043e \u0433\u043e\u0432\u043e\u0440\u0438\u0442 \u043d\u0430\u043c \u043e \u0442\u043e\u043c \u0447\u0442\u043e \u0434\u0430\u043d\u043d\u044b\u0435 \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043b\u0435\u0436\u0430\u0442 \u0434\u0440\u0443\u0433 \u043d\u0430 \u0434\u0440\u0443\u0433\u0435 ","9fe700a5":"**<center><h1>Support Vector Machines<\/h1><\/center>**","66506151":"\u041d\u0443 train \u0438 test \u043e\u0447\u0435\u043d\u044c \u043f\u043e\u0445\u043e\u0436\u0438 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e mean ,max ,min","d768c52d":"\u0417\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0447\u0442\u043e \u044f \u0443\u0431\u0440\u0430\u043b \u043f\u043e\u0432\u044b\u0441\u0438\u043b\u0438 \u0430\u0443\u043a \u0440\u043e\u043a \u043d\u0430 1% \u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u0449\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u044f \u0441\u0434\u0435\u043b\u0430\u044e \u0442\u0430\u043a\u0436\u0435","d700669f":"\u0417\u0434\u0435\u0441\u044c \u043d\u0435\u0442\u0443 \u0432\u044b\u0441\u043e\u043a\u043e corr \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0438 \u043e\u0442\u0440\u0438\u0446\u0435\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0442\u043e\u0436\u0435","013b5741":"ovr \u041f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0443 \u043d\u0430\u0441 \u0431\u0438\u043d\u0430\u0440\u043d\u0430\u044f","276b258d":"**<center><h1>XGboost<\/h1><\/center>**","8401a00e":"\u042d\u0442\u043e \u044f \u0441\u0434\u0435\u043b\u0430\u043b \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0443\u043c\u0435\u043d\u044c\u0448\u0438\u0442\u044c \u043e\u0431\u044c\u0435\u043c \u043f\u0430\u043c\u044f\u0442\u0438 ","e7870c2a":"**<center><h1>Naive Bayes<\/h1><\/center>**","c5db1f28":"\u043d\u0435\u043d\u0443\u0436\u043d\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u043e \u043c\u043e\u0435\u043c\u0443 \u043c\u043d\u0435\u043d\u0438\u044e"}}