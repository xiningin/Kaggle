{"cell_type":{"0ec6181c":"code","dbd08eba":"code","ed445086":"code","1d6e0f24":"code","9a1fa98d":"code","206096ef":"code","297afa1c":"code","27623ff2":"code","083f3f84":"code","23aa8ffa":"code","f052c47e":"code","bd31ea1e":"code","6d3440b5":"code","ee28e842":"code","13c3b58e":"code","4fe52ba8":"code","36b0871f":"code","a04435b3":"code","4c4a91ce":"code","59042f91":"code","077bbc15":"code","36fb6c6d":"code","e617ba9d":"code","31f15925":"code","109be887":"code","37b6bae0":"code","853960c8":"code","33041b31":"code","76cad916":"code","7e0dbcc1":"code","89bf8d63":"code","e6b25c51":"code","a7aff946":"code","84366de2":"code","70d57f24":"code","123742cd":"code","209b6e7c":"code","d24b6e6c":"code","d3fcffca":"code","247ecc10":"code","26689235":"code","b4e8eb66":"code","029327f2":"code","0af77d44":"code","b797861d":"code","5a63699f":"code","d8daf9d6":"code","aff6e69e":"code","05884d83":"code","ad69a578":"code","b9bfb0fa":"code","c6d0f0bb":"code","15ad0141":"code","4f28d6b6":"code","4f9cf718":"code","e9be5005":"code","38931637":"code","403557c9":"code","5dcdddb6":"code","f8002f3d":"code","0ab44c61":"code","694e46d4":"code","44f89e21":"code","c9fb838e":"code","3df5f39a":"code","9e16c015":"code","09c8b4c0":"code","07931a64":"code","b96d8efd":"code","f382c810":"code","c6c3c903":"code","c484710c":"code","6e80ba9b":"markdown","24d606c9":"markdown","f9113794":"markdown","b618ceef":"markdown","86b0096b":"markdown","abe5bf18":"markdown","46b3b84c":"markdown","ba6cd398":"markdown","bcc35cf1":"markdown","94246374":"markdown","34353655":"markdown","df8a3742":"markdown","0d5913d5":"markdown","3d9f06a2":"markdown","894d2134":"markdown","4fc098c1":"markdown","80e8529a":"markdown","c8425468":"markdown"},"source":{"0ec6181c":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn import cross_validation, metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor","dbd08eba":"#Read files:\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","ed445086":"print(train_df.columns.values)","1d6e0f24":"# preview the data\ntrain_df.head(10)","9a1fa98d":"train_df.describe() #Get summary of numerical variables","206096ef":"train_df.describe(include=['O']) #Get summary of categorical variables","297afa1c":"# Here NA is a value for the feature Alley\ntrain_df['Alley'].fillna('No_Alley',inplace=True)\ntest_df['Alley'].fillna('No_Alley',inplace=True)","27623ff2":"# Here NA is a value for the feature BsmtQual\ntrain_df['BsmtQual'].fillna('No_Basement',inplace=True)\ntest_df['BsmtQual'].fillna('No_Basement',inplace=True)","083f3f84":"# Here NA is a value for the feature BsmtCond\ntrain_df['BsmtCond'].fillna('No_Basement',inplace=True)\ntest_df['BsmtCond'].fillna('No_Basement',inplace=True)","23aa8ffa":"# Here NA is a value for the feature BsmtExposure\ntrain_df['BsmtExposure'].fillna('No_Basement',inplace=True)\ntest_df['BsmtExposure'].fillna('No_Basement',inplace=True)","f052c47e":"# Here NA is a value for the feature BsmtFinType1\ntrain_df['BsmtFinType1'].fillna('No_Basement',inplace=True)\ntest_df['BsmtFinType1'].fillna('No_Basement',inplace=True)","bd31ea1e":"# Here NA is a value for the feature BsmtFinType2\ntrain_df['BsmtFinType2'].fillna('No_Basement',inplace=True)\ntest_df['BsmtFinType2'].fillna('No_Basement',inplace=True)","6d3440b5":"# Here NA is a value for the feature FireplaceQu\ntrain_df['FireplaceQu'].fillna('No_Fireplace',inplace=True)\ntest_df['FireplaceQu'].fillna('No_Fireplace',inplace=True)","ee28e842":"# Here NA is a value for the feature GarageType\ntrain_df['GarageType'].fillna('No_Garage',inplace=True)\ntest_df['GarageType'].fillna('No_Garage',inplace=True)","13c3b58e":"# Here NA is a value for the feature GarageFinish\ntrain_df['GarageFinish'].fillna('No_Garage',inplace=True)\ntest_df['GarageFinish'].fillna('No_Garage',inplace=True)","4fe52ba8":"# Here NA is a value for the feature GarageQual\ntrain_df['GarageQual'].fillna('No_Garage',inplace=True)\ntest_df['GarageQual'].fillna('No_Garage',inplace=True)","36b0871f":"# Here NA is a value for the feature GarageCond\ntrain_df['GarageCond'].fillna('No_Garage',inplace=True)\ntest_df['GarageCond'].fillna('No_Garage',inplace=True)","a04435b3":"# Here NA is a value for the feature PoolQC\ntrain_df['PoolQC'].fillna('No_Pool',inplace=True)\ntest_df['PoolQC'].fillna('No_Pool',inplace=True)","4c4a91ce":"# Here NA is a value for the feature Fence\ntrain_df['Fence'].fillna('No_Fence',inplace=True)\ntest_df['Fence'].fillna('No_Fence',inplace=True)","59042f91":"# Here NA is a value for the feature MiscFeature\ntrain_df['MiscFeature'].fillna('None',inplace=True)\ntest_df['MiscFeature'].fillna('None',inplace=True)","077bbc15":"# Here None is a value for the feature MasVnrType\ntrain_df['MasVnrType'].fillna('None',inplace=True)\ntest_df['MasVnrType'].fillna('None',inplace=True)","36fb6c6d":"train_df.describe(include=['O']) #Get summary of categorical variables","e617ba9d":"# plotting the histogram of GrLivArea\ntrain_df['GrLivArea'].hist(bins=50)\nplt.show()","31f15925":"# plotting the histogram of GarageArea\ntrain_df['GarageArea'].hist(bins=50)\nplt.show()","109be887":"#scatter plot BedroomAbvGr\/saleprice\nvar = 'BedroomAbvGr'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), color='red');","37b6bae0":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), color='red');","853960c8":"#scatter plot OverallQual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), color='red');","33041b31":"#scatter plot OverallCond\/saleprice\nvar = 'OverallCond'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), color='red');","76cad916":"#creating new_variable\ntrain_df['Overallscore']=train_df['OverallQual']+train_df['OverallCond']\ntest_df['Overallscore']=test_df['OverallQual']+test_df['OverallCond']\n#dropping irrevalent variables\ntrain_df = train_df.drop(['OverallQual', 'OverallCond'], axis=1)\ntest_df = test_df.drop(['OverallQual', 'OverallCond'], axis=1)","7e0dbcc1":"#scatter plot Overallscore\/saleprice\nvar = 'Overallscore'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), color='green');","89bf8d63":"#creating new_variable\ntrain_df['BsmtFin']=train_df['BsmtFinSF1']+train_df['BsmtFinSF2']\ntest_df['BsmtFin']=test_df['BsmtFinSF1']+test_df['BsmtFinSF2']\n#dropping irrevalent variables\ntrain_df = train_df.drop(['BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF'], axis=1)\ntest_df = test_df.drop(['BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF'], axis=1)","e6b25c51":"# converting some categorical features to ordinal\nGarage_mapping = {\"No_Garage\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\ntrain_df['GarageQual'] = train_df['GarageQual'].map(Garage_mapping)\ntest_df['GarageQual'] = test_df['GarageQual'].map(Garage_mapping)\ntrain_df['GarageCond'] = train_df['GarageCond'].map(Garage_mapping)\ntest_df['GarageCond'] = test_df['GarageCond'].map(Garage_mapping)\n#creating new_variable\ntrain_df['Garagescore']=train_df['GarageQual']+train_df['GarageCond']\ntest_df['Garagescore']=test_df['GarageQual']+test_df['GarageCond']\n#dropping irrevalent variables\ntrain_df = train_df.drop(['GarageQual', 'GarageCond'], axis=1)\ntest_df = test_df.drop(['GarageQual', 'GarageCond'], axis=1)\ntrain_df['Garagescore'].head(10)","a7aff946":"# converting some categorical features to ordinal\nBsmt_mapping = {\"No_Basement\": 0, \"Unf\":1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\":6}\ntrain_df['BsmtFinType1'] = train_df['BsmtFinType1'].map(Bsmt_mapping)\ntest_df['BsmtFinType1'] = test_df['BsmtFinType1'].map(Bsmt_mapping)\ntrain_df['BsmtFinType2'] = train_df['BsmtFinType2'].map(Bsmt_mapping)\ntest_df['BsmtFinType2'] = test_df['BsmtFinType2'].map(Bsmt_mapping)\n#creating new_variable\ntrain_df['BsmtFinType']=train_df['BsmtFinType1']+train_df['BsmtFinType2']\ntest_df['BsmtFinType']=test_df['BsmtFinType1']+test_df['BsmtFinType2']\n#dropping irrevalent variables\ntrain_df = train_df.drop(['BsmtFinType1', 'BsmtFinType2'], axis=1)\ntest_df = test_df.drop(['BsmtFinType1', 'BsmtFinType2'], axis=1)\ntrain_df['BsmtFinType'].head(10)","84366de2":"# converting some categorical features to ordinal\nExter_mapping = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\ntrain_df['ExterQual'] = train_df['ExterQual'].map(Exter_mapping)\ntest_df['ExterQual'] = test_df['ExterQual'].map(Exter_mapping)\ntrain_df['ExterCond'] = train_df['ExterCond'].map(Exter_mapping)\ntest_df['ExterCond'] = test_df['ExterCond'].map(Exter_mapping)\n#creating new_variable\ntrain_df['Exterscore']=train_df['ExterQual']+train_df['ExterCond']\ntest_df['Exterscore']=test_df['ExterQual']+test_df['ExterCond']\n#dropping irrevalent variables\ntrain_df = train_df.drop(['ExterQual', 'ExterCond'], axis=1)\ntest_df = test_df.drop(['ExterQual', 'ExterCond'], axis=1)\ntrain_df['Exterscore'].head(10)","70d57f24":"#dropping irrevalent variables\ntrain_df = train_df.drop(['Condition2'], axis=1)\ntest_df = test_df.drop(['Condition2'], axis=1)","123742cd":"# Determining the years from Original construction date\n# Years:\ntrain_df['Age_of_House'] = 2018 - train_df['YearBuilt']\ntest_df['Age_of_House'] = 2018 - test_df['YearBuilt']\ntrain_df['Age_of_House'].describe()","209b6e7c":"#scatter plot Age_of_House\/saleprice\nvar = 'Age_of_House'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), color='green');","d24b6e6c":"# dropping YearBuilt feature\ntrain_df = train_df.drop(['YearBuilt'], axis=1)\ntest_df = test_df.drop(['YearBuilt'], axis=1)","d3fcffca":"# Determining the years from Remodelling date\n# Years:\ntrain_df['Age_of_Remod_House'] = 2018 - train_df['YearRemodAdd']\ntest_df['Age_of_Remod_House'] = 2018 - test_df['YearRemodAdd']\ntrain_df['Age_of_Remod_House'].describe()","247ecc10":"#scatter plot Age_of_Remod_House\/saleprice\nvar = 'Age_of_Remod_House'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), color='green');","26689235":"# dropping YearRemodAdd feature\ntrain_df = train_df.drop(['YearRemodAdd'], axis=1)\ntest_df = test_df.drop(['YearRemodAdd'], axis=1)","b4e8eb66":"# Determining the years from garage built date\n# Years:\ntrain_df['Age_of_garage'] = 2018 - train_df['GarageYrBlt']\ntest_df['Age_of_garage'] = 2018 - test_df['GarageYrBlt']\ntrain_df['Age_of_garage'].describe()","029327f2":"#scatter plot Age_of_House\/saleprice\nvar = 'Age_of_garage'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), color='green');","0af77d44":"# dropping GarageYrBlt feature\ntrain_df = train_df.drop(['GarageYrBlt'], axis=1)\ntest_df = test_df.drop(['GarageYrBlt'], axis=1)","b797861d":"#scatter plot YrSold\/saleprice\nvar = 'YrSold'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), color='red');","5a63699f":"#dropping irrevalent variables\ntrain_df = train_df.drop(['YrSold', 'MoSold'], axis=1)\ntest_df = test_df.drop(['YrSold', 'MoSold'], axis=1)","d8daf9d6":"#scatter plot LotFrontage\/saleprice\nvar = 'LotFrontage'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), color='red');","aff6e69e":"#dropping irrevalent variables\ntrain_df = train_df.drop(['LotFrontage'], axis=1)\ntest_df = test_df.drop(['LotFrontage'], axis=1)","05884d83":"#scatter plot MasVnrArea\/saleprice\nvar = 'MasVnrArea'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), color='red');","ad69a578":"#dropping irrevalent variables\ntrain_df = train_df.drop(['MasVnrArea'], axis=1)\ntest_df = test_df.drop(['MasVnrArea'], axis=1)","b9bfb0fa":"#scatter plot GarageCars\/saleprice\nvar = 'GarageCars'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), color='red');","c6d0f0bb":"#scatter plot GarageArea\/saleprice\nvar = 'GarageArea'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), color='red');","15ad0141":"#dropping irrevalent variables\ntrain_df = train_df.drop(['GarageArea'], axis=1)\ntest_df = test_df.drop(['GarageArea'], axis=1)","4f28d6b6":"#missing data for training set\ntotal = train_df.isnull().sum().sort_values(ascending=False)\npercent = (train_df.isnull().sum()\/train_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(10)","4f9cf718":"#missing data for test set\ntotal = test_df.isnull().sum().sort_values(ascending=False)\npercent = (test_df.isnull().sum()\/test_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(15)","e9be5005":"# Age_of_garage is missing where there is no garage\ntrain_df['Age_of_garage'].fillna(0,inplace=True)\ntest_df['Age_of_garage'].fillna(0,inplace=True)","38931637":"#Treating missing values\ntrain_df['Electrical'].fillna(train_df['Electrical'].mode()[0],inplace=True)\ntest_df['MSZoning'].fillna(test_df['MSZoning'].mode()[0],inplace=True)\ntest_df['BsmtHalfBath'].fillna(test_df['BsmtHalfBath'].mode()[0],inplace=True)\ntest_df['BsmtFullBath'].fillna(test_df['BsmtFullBath'].mode()[0],inplace=True)\ntest_df['Functional'].fillna(test_df['Functional'].mode()[0],inplace=True)\ntest_df['Utilities'].fillna(test_df['Utilities'].mode()[0],inplace=True)\ntest_df['KitchenQual'].fillna(test_df['KitchenQual'].mode()[0],inplace=True)\ntest_df['SaleType'].fillna(test_df['SaleType'].mode()[0],inplace=True)\ntest_df['Exterior1st'].fillna(test_df['Exterior1st'].mode()[0],inplace=True)\ntest_df['Exterior2nd'].fillna(test_df['Exterior2nd'].mode()[0],inplace=True)\ntest_df['BsmtFin'].fillna(0,inplace=True)\ntest_df['BsmtUnfSF'].fillna(0,inplace=True)\ntest_df['GarageCars'].fillna(0,inplace=True)","403557c9":"# Converting all categorical variables into numeric by encoding the categories\nvar_mod = ['MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Foundation','BsmtQual','BsmtCond','BsmtExposure','Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu','GarageType','GarageFinish','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition']\nle = LabelEncoder()\nfor i in var_mod:\n    train_df[i] = le.fit_transform(train_df[i])\n    test_df[i] = le.fit_transform(test_df[i])\ntrain_df.dtypes","5dcdddb6":"#dropping irrevalent variables\ntrain_df = train_df.drop(['Id'], axis=1)","f8002f3d":"X_train = train_df.drop(\"SalePrice\", axis=1)\nY_train = train_df[\"SalePrice\"]\nX_test  = test_df.drop(\"Id\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","0ab44c61":"pca = PCA(n_components=30)\nX_train_reduced=pca.fit_transform(X_train)\nX_train_reduced.shape","694e46d4":"X_test_reduced=pca.fit_transform(X_test)\nX_test_reduced.shape","44f89e21":"# generic function\ndef modelfit(alg, dtrain_X, dtrain_Y, dtest_X):\n    #Fit the algorithm on the data\n    alg.fit(dtrain_X, dtrain_Y)\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain_X)\n\n    #Perform cross-validation:\n    cv_score = cross_validation.cross_val_score(alg, dtrain_X, dtrain_Y, cv=20, scoring='neg_mean_squared_error')\n    cv_score = np.sqrt(np.abs(cv_score))\n    \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error(dtrain_Y.values, dtrain_predictions)))\n    print (\"CV Score : Mean - %.4g | Std - %.4g | Min - %.4g | Max - %.4g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n    \n    #Predict on testing data:\n    Y_pred = alg.predict(dtest_X)\n    \n    return Y_pred","c9fb838e":"# Linear Regression Model\nalg1 = LinearRegression(normalize=True)\nY_pred=modelfit(alg1, X_train_reduced, Y_train, X_test_reduced)","3df5f39a":"# Ridge Regression Model\nalg2 = Ridge(alpha=0.05,normalize=True)\nY_pred=modelfit(alg2, X_train_reduced, Y_train, X_test_reduced)","9e16c015":"# Decision Tree Model\nalg3 = DecisionTreeRegressor(max_depth=20, min_samples_leaf=300)\nY_pred=modelfit(alg3, X_train_reduced, Y_train, X_test_reduced)","09c8b4c0":"# Random Forest Model\nalg4 = RandomForestRegressor(n_estimators=400,max_depth=20, min_samples_leaf=100, n_jobs=4)\nY_pred=modelfit(alg4, X_train_reduced, Y_train, X_test_reduced)","07931a64":"# XGB regressor\nalg5 = xgb.XGBRegressor(n_estimators=300, max_depth=2, learning_rate=0.1) \nY_pred=modelfit(alg5, X_train_reduced, Y_train, X_test_reduced)","b96d8efd":"# Tuning of parameters\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [2, 3, 5, 7],\n    'learning_rate': [0.05, 0.1, 0.3],\n    'n_estimators': [200, 350, 450, 500]\n}\n# Create a based model\nXGBR = xgb.XGBRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = XGBR, param_grid = param_grid, \n                          cv = 5, n_jobs = -1, verbose = 2)\n# Fit the grid search to the data\ngrid_search.fit(X_train_reduced, Y_train)\ngrid_search.best_params_","f382c810":"# XGB regressor with tuned parameters\nalg6 = xgb.XGBRegressor(n_estimators=500, max_depth=3, learning_rate=0.05) \nY_pred=modelfit(alg6, X_train_reduced, Y_train, X_test_reduced)","c6c3c903":"# Using ANN\n# Initialising the ANN\nalg7 = Sequential()\n\n# Adding the input layer and the first hidden layer\nalg7.add(Dense(units = 256, kernel_initializer = 'normal', activation = 'relu', input_dim = 30))\n\n# Adding the second hidden layer\nalg7.add(Dense(units = 256, kernel_initializer = 'normal', activation = 'relu'))\n\n# Adding the output layer\nalg7.add(Dense(units = 1, kernel_initializer = 'normal'))\n\n# Compiling the ANN\nalg7.compile(optimizer = 'adam', loss = 'mse')\n\n# Fitting the ANN to the Training set\nalg7.fit(X_train_reduced, Y_train, batch_size = 64, epochs = 800)\n\n# Predicting the Test set results\ny_pred = alg7.predict(X_test_reduced)","c484710c":"submission = pd.DataFrame({\n        \"Id\": test_df[\"Id\"],\n        \"SalePrice\": Y_pred\n    })\n\nsubmission.to_csv('submission.csv', index=False)","6e80ba9b":"We will perform some visualization to better understand the distribution of different features and also the relationship of our dependent variable *SalePrice* with other features.","24d606c9":"Now, let's get a detailed distribution of categorical features across the samples. This time we will observe all such features has one more unique value than the previous one.","f9113794":"Next, we convert all categorical variables into numeric by Label Encoding","b618ceef":" Hello everyone, this is my first kernel. I will appreciate any kind of constructive feedback. ","86b0096b":"We will define a generic function for model fitting and then we will fit models like *Linear Regression, Ridge Regression, Decision Tree Regressor, Random Forest Regressor *and *XGB regressor *","abe5bf18":"**Missing data**\nLet's analyze the missing data for training set and test set","46b3b84c":"XGB regressor outperforms other models. So, we will tune its parameters to further improve the results.","ba6cd398":"Next, we will create some new features from the combination of old features and then drop the old features. For example, we can combine *OverallCond* and *OverallQual* to get a new feature as that will be more relevant.","bcc35cf1":"Let's get a detailed distribution of numerical feature values across the samples.","94246374":"There are multiple features that have **year** as feature value. Instead of keeping the entire year as feature value, I have converted it to a number as 2018 - **year**","34353655":"**Age_of_garage** is missing where there is no garage so we can fill the missing values by 0.  Same is the case for **BsmtFin**, **BsmtUnfSF** and **GarageCars**. For rest of the features, we will simply fill the missing values by mode value of that feature.","df8a3742":"Next we will use Artificial Neural Network with two hidden layers.","0d5913d5":"Which features are categorical? Which features are numerical?","3d9f06a2":"Next, let's get a detailed distribution of categorical features across the samples.","894d2134":"Now, if you look at the data_description.txt, you can observe that a lot of categorical features contain **NA**  as a feature value. However, now they are considered as missing values.  For example the categorical feature **Alley** has **3** unique values: *Grvl*, *Pave* and *NA*. But in the above summary of cateforical features, **Alley** is seen to have **2** unique values as *NA* is considered as missing value. We will now rectify all those features.","4fc098c1":"But the neural network has not outperformed our XGB regressor. So, we will submit the predictions made by XGB regressor.","80e8529a":"As you can see, we have 67 features currently which is huge. We will further reduce the dimensionality using *Principal Component Analysis(PCA).  *","c8425468":"Dropping some more irrevalent features"}}