{"cell_type":{"7eecfc4e":"code","7349c97e":"code","4730cc2a":"code","c6ff7b97":"code","46445d49":"code","04174972":"code","7864ffe6":"code","7fe009e3":"code","473a5285":"code","a49fbb0d":"markdown","a223679c":"markdown","0e03231a":"markdown","75046da0":"markdown","7dbcb633":"markdown","73b3113b":"markdown"},"source":{"7eecfc4e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","7349c97e":"# Loading data (feature hp gets dropped since is 85 for each row)\ndata   = pd.read_csv(\"..\/input\/car-price-prediction\/car_price.csv\")\ntarget = data[\"price\"].values\ndata.drop(columns=[\"hp\"], inplace=True)","4730cc2a":"data","c6ff7b97":"# Saving attributes categories for later analysis\nfeatures = [\"Body Color\", \"Gearing Type\", \"body_type\", \"make_model\"]\nfeatures_info = {feature: {\"uniques\": data[feature].unique(),\n                           \"mean\"   : [data.loc[data[feature] == val, \"price\"].mean() for val in data[feature].unique()],\n                           \"std\"    : [data.loc[data[feature] == val, \"price\"].std() for val in data[feature].unique()]\n                          }\n                for feature in features}\n\n# Data preprocessing\ndata = pd.get_dummies(data, columns=[\"make_model\", \"body_type\", \"Body Color\", \"Gearing Type\"])\ncolumns = data.columns.values\ndata.rename({column: column.split(\"_\")[-1] for column in columns}, axis=\"columns\", inplace=True)\n\nextras = data[\"Extras\"].str.get_dummies(\",\")\ndata.drop(columns=[\"Extras\"], inplace=True)\ndata = pd.concat([data, extras], axis=1)","46445d49":"data","04174972":"from math import floor\n\n# Kilometers - Price \nfig, km_to_price = plt.subplots(figsize=(15,5))\nkms = data[\"km\"].values\nkm_to_price.scatter(kms, target, s=3)\nkm_to_price.set_xlabel(\"Kilometers\")\nkm_to_price.set_xlabel(\"Price\")\nkm_to_price.set_title(\"Kilometers -> Price (Used cars?)\")\nplt.show()\n\n# Features - Price\nfig, axes = plt.subplots(2, len(features) \/\/ 2, figsize=(15,15))\nfor i, feature in enumerate(features_info):\n    axes[floor(i \/\/ 2), i % 2].set_title(feature)\n    axes[floor(i \/\/ 2), i % 2].set_xlabel(\"Price\")\n    axes[floor(i \/\/ 2), i % 2].barh(y=features_info[feature][\"uniques\"],\n                 width=features_info[feature][\"mean\"],\n                 xerr=features_info[feature][\"std\"]\n                )\n    ","7864ffe6":"print(f\"Target mean: {target.mean()} target std: {target.std()}\")","7fe009e3":"# Train \/ Test splitting\nfrom sklearn.model_selection import train_test_split\n\nnumpy_data = data.drop(columns=[\"price\"]).values\n\nx_train, x_test, y_train, y_test = train_test_split(numpy_data, target, test_size=0.3, random_state=24)","473a5285":"# Models testing \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics         import r2_score, mean_squared_error, make_scorer\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble     import HistGradientBoostingRegressor                           \n\nestimators = {'HistGradientBoostingRegressor': {'func'  : HistGradientBoostingRegressor(random_state=42),\n                                                'params': {'learning_rate'    : [0.05, 0.1, 0.15],\n                                                           'min_samples_leaf' : [10, 20, 30],\n                                                           'max_leaf_nodes'   : [31, 41, 51]}},\n             }\n\nmodels_to_test = estimators.keys()\nfor name in models_to_test:\n    model = GridSearchCV(estimator=estimators[name][\"func\"],\n                         param_grid=estimators[name][\"params\"],\n                         scoring=make_scorer(r2_score),\n                         n_jobs=-1)\n    model.fit(x_train, y_train)\n    preds   = model.predict(x_test)\n    print(\"{}: \\n R2: {:.3f} \\n RMSE {} \\n BP: {} \\n\".format(name, \n                                                          r2_score(y_test, preds),\n                                                          np.sqrt(r2_score(y_test, preds)),\n                                                          model.best_params_))","a49fbb0d":"This is what we get after the preprocessing steps. Note that some column have been renamed, just for convenience.","a223679c":"# Some EDA\nThe ```km``` column probably refers to used cars. If we look at the scatter plot ```price```-```km```, there is a negative correlation between the two attributes. We can also plot the average price for a car, given a certain body type, color, gearing and model. Looks like orange cars are cheaper! ","0e03231a":"The dataset is quite small, so we can directly use sklearn with grid search fpr hyperparameters optimization. I tried an Histogram-based Gradient Boosting Regression Tree. No particular reason for that, it is possible to try out different models and see how they perform. ","75046da0":"# Building a model\nTrying to predict the car price.","7dbcb633":"# Data preprocessing\nThe dataset has almost exclusively categorical data, (except for ```km``` attribute). Being the numerosity of unique elements (for each categorical feature) not too high, we could think of One-Hot encode each one of them. ```Extras``` is composed by a string of extra accessories for each row. Since they are separated by a comma, they can be easily splitted by using ```DataFrame.str.get_dummies(sep=\",\")```. ","73b3113b":"The target standard deviation give us the error of the trivial predictor (target mean). "}}