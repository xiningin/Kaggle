{"cell_type":{"63fe6b23":"code","330cb6de":"code","36d40919":"code","e0ad8e04":"code","bfb56c22":"code","53a29d10":"code","5e73371d":"code","378b7027":"code","edcd9196":"code","280633cc":"code","b3b0d750":"code","435da4fd":"code","e370b8fb":"code","7a94edde":"code","b6616136":"code","bdf61307":"code","932236bd":"code","4862628b":"code","cd923c08":"code","5636115a":"code","43ad271d":"code","02165c18":"code","b2ca8668":"code","a31528bd":"code","dfe875cf":"code","01d1f9dc":"code","ad899c79":"code","d5008b30":"code","0e219f28":"code","3660db77":"code","3d018d66":"code","84030b1c":"code","8d00139b":"code","74a0d66c":"code","dbbed7a9":"code","94a00c00":"code","fd44e4a5":"code","7dc41e84":"code","f9805d9e":"code","39dd428c":"code","656b7956":"code","6f7bf186":"code","5c153d43":"code","4c8821d6":"code","7640d212":"code","f251de75":"code","657a539e":"code","ebc93142":"code","f3018f67":"code","a5fbe7bf":"code","fc871899":"code","5f9cf657":"code","428f1445":"code","cedc2b85":"code","53e0579b":"code","0fc9949e":"code","862c472c":"code","0a3bd4bc":"markdown","ff20246f":"markdown","bf810187":"markdown","c9f33793":"markdown","a91064a8":"markdown","b4d28cba":"markdown","701e06a7":"markdown","25b579d0":"markdown","ff329f3d":"markdown","1c972c30":"markdown","87019db0":"markdown","925af95b":"markdown","6f28edd1":"markdown","48cc2283":"markdown","a736f254":"markdown","f968a16f":"markdown","eca636bc":"markdown","38631974":"markdown","cac21621":"markdown","c400c0eb":"markdown","dcf484aa":"markdown","4aadf286":"markdown","d075fd9f":"markdown","11f52308":"markdown","0b729fa8":"markdown","250618c9":"markdown","a2e9f0c5":"markdown","a4c05d78":"markdown","7ae5ed78":"markdown"},"source":{"63fe6b23":"#Importing libraries\n\nimport nltk, re\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nimport requests\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport pprint, time","330cb6de":"# note the start time of execution to be used later to get the total time of execution\nstartTimeOfExecution = time.time()","36d40919":"nltk.download('universal_tagset')","e0ad8e04":"# read the Treebank tagged sentences\nnltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\nnltk_data[:2]","bfb56c22":"# Split into train and test\nrandom.seed(1234)\ntrain_set, test_set = train_test_split(nltk_data, test_size=0.05)\n\nprint(\"Length of Training Set: \", len(train_set))\nprint(\"Length of Test Set: \", len(test_set))\nprint(\"-\" * 120)\nprint(\"Training Data Overview:\\n\")\nprint(train_set[:10])","53a29d10":"# Next, get the list of tagged words\ntrain_tagged_words = [tup for sent in train_set for tup in sent]\nlen(train_tagged_words)","5e73371d":"# Now, let's have a look at the tagged words in the training set\ntrain_tagged_words[:]","378b7027":"# get the tokens in the train set - train_tagged_words which will be used later\ntrain_data_tokens = [pair[0] for pair in train_tagged_words]\ntrain_data_tokens[:15]","edcd9196":"# get the POS tags for the tokens in the train set which will be used later\ntrain_data_pos_tags = [pair[1] for pair in train_tagged_words]\ntrain_data_pos_tags[:10]","280633cc":"# transform the train vocabulary to a set\ntraining_vocabulary_set = set(train_data_tokens)","b3b0d750":"# transform the POS tags to a set\ntraining_pos_tag_set = set(train_data_pos_tags)","435da4fd":"print(\"Length : \\nVocabulary: {} \\nTags: {}\".format(len(training_vocabulary_set), len(training_pos_tag_set)))\nprint(\"\\nAvailable Tags :\\n\")\nprint(training_pos_tag_set)","e370b8fb":"# compute P(w\/t) and store in [Tags x Vocabulary] matrix which is a matrix with dimensions of len(training_pos_tag_set) X en(training_vocabulary_set)\n\nlen_pos_tags = len(training_pos_tag_set)\nlen_vocab = len(training_vocabulary_set)\n\nword_given_tag = np.zeros((len_pos_tags, len_vocab))","7a94edde":"# compute word given tag: Emission Probability\n\ndef word_given_tag(word, tag, train_bag = train_tagged_words):\n    tag_list = [pair for pair in train_bag if pair[1]==tag]\n    count_tag = len(tag_list)\n    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n    count_w_given_tag = len(w_given_tag_list)\n    \n    return (count_w_given_tag, count_tag)","b6616136":"# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n\ndef t2_given_t1(t2, t1, train_bag = train_tagged_words):\n    tags = [pair[1] for pair in train_bag]\n    count_t1 = len([t for t in tags if t==t1])\n    count_t2_t1 = 0\n    for index in range(len(tags)-1):\n        if tags[index]==t1 and tags[index+1] == t2:\n            count_t2_t1 += 1\n    return (count_t2_t1, count_t1)","bdf61307":"# create t x t transition matrix of tags\n# each column is t2, each row is t1\n# thus M(i, j) represents P(tj given ti)\n\ntags_matrix = np.zeros((len_pos_tags, len_pos_tags), dtype='float32')\nfor i, t1 in enumerate(list(training_pos_tag_set)):\n    for j, t2 in enumerate(list(training_pos_tag_set)): \n        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]\/t2_given_t1(t2, t1)[1]\n\n# Let's have a glimpse into the transition matrix\ntags_matrix","932236bd":"# convert the matrix to a data frame for better readability\ntags_df = pd.DataFrame(tags_matrix, columns = list(training_pos_tag_set), index=list(training_pos_tag_set))\n\n# dataset glimpse\ntags_df","4862628b":"# heatmap of tags matrix\n# T(i, j) means P(tag j given tag i)\nplt.figure(figsize=(14, 8))\nsns.heatmap(tags_df, annot = True)\nplt.show()","cd923c08":"# frequent tags\n# filter the df to get P(t2, t1) > 0.5\n\ntags_frequent = tags_df[tags_df>0.5]\nplt.figure(figsize=(14, 8))\nsns.heatmap(tags_frequent, annot = True)\nplt.show()","5636115a":"# Viterbi Heuristic\ndef VanillaViterbiPOSTagger(words, train_bag = train_tagged_words):\n    state = []\n    T = list(set([pair[1] for pair in train_bag]))\n    \n    for key, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        p = [] \n        for tag in T:\n            if key == 0:\n                transition_p = tags_df.loc['.', tag]\n            else:\n                transition_p = tags_df.loc[state[-1], tag]\n                \n            # compute emission and state probabilities\n            emission_p = word_given_tag(words[key], tag)[0]\/word_given_tag(words[key], tag)[1]\n            state_probability = emission_p * transition_p    \n            p.append(state_probability)\n            \n        pmax = max(p)\n        # getting state for which probability is maximum\n        state_max = T[p.index(pmax)] \n        state.append(state_max)\n    return list(zip(words, state))","43ad271d":"random.seed(1234)\n\n# list of tagged words in the test set\ntest_run_base = [tup for sent in test_set for tup in sent]\n\n# list of untagged words in the test set\ntest_tagged_words = [tup[0] for sent in test_set for tup in sent]","02165c18":"# Now, let's check the accuracy of our vanilla viterbi algorithm\n\nstart = time.time()\n\n# tag the test sentences\ntagged_seq = VanillaViterbiPOSTagger(test_tagged_words)\n\nend = time.time()\ndifference = end-start\n\nprint(\"Time taken in seconds by Vanilla Viterbi Algorithm: \", difference)\n\ncheck = [i for i, j in zip(tagged_seq, test_run_base) if i == j]\n\nvanilla_viterbi_accuracy = len(check)\/len(tagged_seq)\n\nprint(\"Accuracy of the Vanilla Viterbi Algorithm: \", vanilla_viterbi_accuracy)","b2ca8668":"# Let's have a look at the incorrect tagged cases in the test set\n\nincorrect_tagged_cases = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]\nprint(len(incorrect_tagged_cases))\nincorrect_tagged_cases","a31528bd":"# Unknown words \ntest_vocabulary_set = set([t for t in test_tagged_words])\n\nunknown_words = list(test_vocabulary_set - training_vocabulary_set)\nprint(\"Total Unknown words :\", len(unknown_words))\nprint(\"\\n\")\nprint(\"Unknown Words :\", unknown_words)","dfe875cf":"# Modified Viterbi Heuristic\ndef VanillaViterbiForUnknownWords(words, train_bag = train_tagged_words):\n    state = []\n    T = list(set([pair[1] for pair in train_bag]))\n    \n    for key, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        p = [] \n        for tag in T:\n            if key == 0:\n                transition_p = tags_df.loc['.', tag]\n            else:\n                transition_p = tags_df.loc[state[-1], tag]\n                \n            # compute emission and state probabilities\n            emission_p = word_given_tag(words[key], tag)[0]\/word_given_tag(words[key], tag)[1]\n            \n            # modification to the original vanilla viterbi algorithm \n            # to consider unknown words\n            if word not in training_vocabulary_set:\n                state_probability = transition_p\n            else:\n                state_probability = emission_p * transition_p\n                \n            p.append(state_probability)\n            \n        pmax = max(p)\n        # getting state for which probability is maximum\n        state_max = T[p.index(pmax)] \n        state.append(state_max)\n    return list(zip(words, state))","01d1f9dc":"# tagging the test sentences\nstart = time.time()\n\nunknown_word_tagged_seq = VanillaViterbiForUnknownWords(test_tagged_words)\n\nend = time.time()\ndifference = end-start\n\nprint(\"Time taken in seconds by Vanilla Viterbi Algorithm for unknown words: \", difference)\n\n# accuracy\nunknown_word_check = [i for i, j in zip(unknown_word_tagged_seq, test_run_base) if i == j]\n\noptimized_viterbi_accuracy = len(unknown_word_check)\/len(unknown_word_tagged_seq)\n\nprint(\"The accuracy of the Vanilla Viterbi Algorithm after modification to handle unknown words is -\", optimized_viterbi_accuracy)","ad899c79":"# Unigram Tagger\nunigram_tagger = nltk.UnigramTagger(train_set)\n\nstart = time.time()\n\nunigram_tagger_accuracy = unigram_tagger.evaluate(test_set)\n\nend = time.time()\ndifference = end-start\n\nprint(\"Time taken in seconds by Unigram Tagger: \", difference)\nprint(\"The accuracy of the Unigram Tagger is -\", unigram_tagger_accuracy)","d5008b30":"# patterns for tagging using a rule based regex tagger -\n\npatterns = [\n    (r'^[aA-zZ].*[0-9]+','NOUN'),  # Alpha Numeric\n    (r'.*ness$', 'NOUN'),\n    (r'.*\\'s$', 'NOUN'),              # possessive nouns\n    (r'.*s$', 'NOUN'),                # plural nouns\n    (r'.*', 'NOUN'),    \n    (r'.*ly$', 'ADV'),\n    (r'^(0|([*|-|$].*))','X'), # Any special character combination\n    (r'.*ould$', 'X'), # modals\n    (r'(The|the|A|a|An|an)$', 'DET'),\n    (r'^([0-9]|[aA-zZ])+\\-[aA-zZ]*$','ADJ'),\n    (r'.*able$', 'ADJ'), # adjective like 100-megabytes 237-Seats\n    (r'[aA-zZ]+(ed|ing|es)$', 'VERB'), # Any word ending with 'ing' or 'ed' is a verb\n    (r'[0-9].?[,\\\/]?[0-9]*','NUM')# Numbers \n    ]","0e219f28":"# rule based RegexpTagger\n\nrule_based_tagger = nltk.RegexpTagger(patterns)\n\n# unigram tagger backed up by the rule-based tagger\nrule_based_unigram_tagger = nltk.UnigramTagger(train_set, backoff = rule_based_tagger)\n\nstart = time.time()\n\naccuracy_rule_based_unigram_tagger = rule_based_unigram_tagger.evaluate(test_set)\n\nend = time.time()\ndifference = end-start\n\nprint(\"Time taken in seconds by Regular Expression Tagger: \", difference)\n\n\nprint(\"The accuracy of the Unigram Tagger backed up by the RegexpTagger is -\", accuracy_rule_based_unigram_tagger)","3660db77":"# Bigram tagger\n\nbigram_tagger = nltk.BigramTagger(train_set, backoff = rule_based_unigram_tagger)\n\nstart = time.time()\n\naccuracy_bigram_tagger = bigram_tagger.evaluate(test_set)\n\nend = time.time()\ndifference = end-start\n\nprint(\"Time taken in seconds by Bigram Tagger: \", difference)\n\nprint(\"The accuracy of the Bigram Tagger backed up by the rule_based_unigram_tagger is -\", accuracy_bigram_tagger)","3d018d66":"# trigram tagger\n\ntrigram_tagger = nltk.TrigramTagger(train_set, backoff = bigram_tagger)\n\nstart = time.time()\n\naccuracy_trigram_tagger = trigram_tagger.evaluate(test_set)\n\nend = time.time()\ndifference = end-start\n\n\nprint(\"Time taken in seconds by Trigram Tagger: \", difference)\n\nprint(\"The accuracy of the Trigram Tagger backed up by the bigram_tagger is -\", accuracy_trigram_tagger)","84030b1c":"# A trigram tagger backed off by a rule based tagger.\n\ndef trigram_tagger(word, train_set = train_set):\n    \n    patterns = [\n    (r'[aA-zZ]+(ed|ing|es)$', 'VERB'), # Any word ending with 'ing' or 'ed' is a verb\n\n    (r'.*ly$', 'ADV'),\n        \n    (r'^([0-9]|[aA-zZ])+\\-[aA-zZ]*$','ADJ'),\n    (r'.*able$', 'ADJ'), \n    (r'.*ful$', 'ADJ'),\n    (r'.*ous$', 'ADJ'),\n        \n    (r'^[aA-zZ].*[0-9]+','NOUN'),     # Alpha Numeric\n    (r'.*ness$', 'NOUN'),\n    (r'.*\\'s$', 'NOUN'),              # possessive nouns - words ending with 's\n    (r'.*s$', 'NOUN'),                # plural nouns\n    (r'.*ers$', 'NOUN'),              # eg.- kinderganteners, autobioghapgers\n    (r'.*ment$', 'NOUN'),\n    (r'.*town$', 'NOUN'),\n        \n    (r'^(0|([*|-|$].*))','X'), # Any special character combination\n    (r'.*ould$', 'X'),\n        \n    (r'(The|the|A|a|An|an|That|that|This|this|Those|those|These|these)$', 'DET'), # That\/this\/these\/those belong to the category of Demonstrative determiners\n    (r'[0-9].?[,\\\/]?[0-9]*','NUM'), # Numbers \n        \n    (r'.*', 'NOUN')\n    ]\n\n    regex_based_tagger = nltk.RegexpTagger(patterns)\n\n    # trigram backed up by the regex tagger\n    trigram_regex_tagger = nltk.TrigramTagger(train_set, backoff = regex_based_tagger)\n    return trigram_regex_tagger.tag_sents([[(word)]])","8d00139b":"def ViterbiBackedupByTrigramTagger(words, train_bag = train_tagged_words):\n    state = []\n    tag_set = list(set([pair[1] for pair in train_bag]))\n    \n    # use the trigram tagger backed up by the rule based tagger\n    # for unknown words.\n    for key, word in enumerate(words):\n        if word not in training_vocabulary_set:\n            unknown_word_tag = trigram_tagger(word)\n            for sent in unknown_word_tag:\n                for tup in sent:\n                    state.append(tup[1])\n        else:            \n            p = [] \n            for tag in tag_set:\n                if key == 0:\n                    transition_p = tags_df.loc['.', tag]\n                else:\n                    transition_p = tags_df.loc[state[-1], tag]\n                \n            # compute emission and state probabilities\n                emission_p = word_given_tag(words[key], tag)[0]\/word_given_tag(words[key], tag)[1]\n                state_probability = emission_p * transition_p    \n                p.append(state_probability)\n            \n            pmax = max(p)\n            # getting state for which probability is maximum\n            state_max = tag_set[p.index(pmax)] \n            state.append(state_max)\n    return list(zip(words, state))","74a0d66c":"# tagging the test sentences\n\nstart = time.time()\n\nviterbi_trigram_tagged_seq = ViterbiBackedupByTrigramTagger(test_tagged_words)\n\nend = time.time()\ndifference = end-start\n\nprint(\"Time taken in seconds by Viterbi Algorithm backed by the Trigram Tagger: \", difference)\n\n# accuracy\nviterbi_trigram_word_check = [i for i, j in zip(viterbi_trigram_tagged_seq, test_run_base) if i == j]\n\nviterbi_trigram_accuracy = len(viterbi_trigram_word_check)\/len(viterbi_trigram_tagged_seq)\n\nprint(\"The accuracy of the ViterbiBackedupByTrigramTagger is -\", viterbi_trigram_accuracy)","dbbed7a9":"acccuracy_data = [['Vanilla Viterbi', vanilla_viterbi_accuracy*100], \n                  ['Optimized Vanilla Viterbi', optimized_viterbi_accuracy*100], \n                  ['Unigram Tagger', unigram_tagger_accuracy*100],\n                  ['Unigram + RegexpTagger', accuracy_rule_based_unigram_tagger*100],\n                  ['Bigram Tagger + Unigram_tagger', accuracy_bigram_tagger*100],\n                  ['Trigram Tagger + Bigram_tagger', accuracy_trigram_tagger*100],\n                  ['Viterbi + Trigram_tagger', viterbi_trigram_accuracy*100]]\n\nacccuracy_data_df = pd.DataFrame(acccuracy_data, columns = ['Tagging_Algorithm', 'Tagging_Accuracy'])\n\nacccuracy_data_df.set_index('Tagging_Algorithm', drop = True, inplace = True)\n\nacccuracy_data_df","94a00c00":"acccuracy_data_df.plot.line(rot = 90, legend = False)","fd44e4a5":"f = open('..\/input\/Test_sentences.txt')\ntext = f.read()\nsample_test_sent = text.splitlines()\nf.close()","7dc41e84":"sample_test_sentences = sample_test_sent[:-3]\nsample_test_sent","f9805d9e":"sample_test_tokenized_sents = [word_tokenize(sent) for sent in sample_test_sentences]\nsample_test_corpus_pos_tags = nltk.pos_tag_sents(sample_test_tokenized_sents, tagset = 'universal')\nsample_test_corpus_pos_tags","39dd428c":"# list of untagged words from the sample test data provided\nsample_test_tagged_words = [tup[0] for sent in sample_test_corpus_pos_tags for tup in sent]\n\n\n# list of tagged words\nsample_test_test_run_base = [tup for sent in sample_test_corpus_pos_tags for tup in sent]","656b7956":"sample_test_tagged_words","6f7bf186":"# tagging the test sentences\nstart = time.time()\n\nsample_test_tagged_seq = VanillaViterbiPOSTagger(sample_test_tagged_words)\n\nend = time.time()\ndifference = end-start\n\nprint(\"Time taken in seconds by Vanilla Viterbi Algorithm: \", difference)\n\n\n\n# Let's check the accuracy of the vanilla viterbi algorithm\nsample_test_check = [i for i, j in zip(sample_test_tagged_seq, sample_test_test_run_base) if i == j]\n\nvanilla_viterbi_accuracy_on_sample_test_data = len(sample_test_check)\/len(sample_test_tagged_seq)\n\nprint(\"The accuracy of the Vanilla Viterbi Algorithm on the sample test data is -\", vanilla_viterbi_accuracy_on_sample_test_data)","5c153d43":"# tagging the test sentences\n\nstart = time.time()\n\nsample_test_unknown_word_tagged_seq = VanillaViterbiForUnknownWords(sample_test_tagged_words)\n\nend = time.time()\ndifference = end-start\n\nprint(\"Time taken in seconds by Vanilla Viterbi Algorithm for unknown words: \", difference)\n\n\n# accuracy\nsample_test_unknown_word_check = [i for i, j in zip(sample_test_unknown_word_tagged_seq, sample_test_test_run_base) if i == j]\n\nsample_test_optimized_viterbi_accuracy = len(sample_test_unknown_word_check)\/len(sample_test_unknown_word_tagged_seq)\n\nprint(\"The accuracy of the VanillaViterbiForUnknownWords on the sample test data is -\", sample_test_optimized_viterbi_accuracy)","4c8821d6":"# tagging the test sentences\nstart = time.time()\n\nsample_test_viterbi_trigram_tagged_seq = ViterbiBackedupByTrigramTagger(sample_test_tagged_words)\n\n\nend = time.time()\ndifference = end-start\n\nprint(\"Time taken in seconds by Viterbi Algorithm backed by the Trigram Tagger: \", difference)\n\n\n# accuracy\nsample_test_viterbi_trigram_check = [i for i, j in zip(sample_test_viterbi_trigram_tagged_seq, sample_test_test_run_base) if i == j]\n\nsample_test_viterbi_trigram_accuracy = len(sample_test_viterbi_trigram_check)\/len(sample_test_viterbi_trigram_tagged_seq)\n\nprint(\"The accuracy of the ViterbiBackedupByTrigramTagger on the sample test data is -\", sample_test_viterbi_trigram_accuracy)","7640d212":"acccuracy_on_sample_test_data = [['Vanilla Viterbi', vanilla_viterbi_accuracy_on_sample_test_data*100], \n                  ['Optimized Vanilla Viterbi', sample_test_optimized_viterbi_accuracy*100], \n                  ['Viterbi + Trigram_tagger', sample_test_viterbi_trigram_accuracy*100]]\n\nsample_test_data_acccuracy_df = pd.DataFrame(acccuracy_on_sample_test_data, columns = ['Tagging_Algorithm', 'Tagging_Accuracy'])\n\nsample_test_data_acccuracy_df.set_index('Tagging_Algorithm', drop = True, inplace = True)\n\nsample_test_data_acccuracy_df","f251de75":"sample_test_data_acccuracy_df.plot.line(rot = 90, legend = False)","657a539e":"# test sentences\n\ntest_sentence_1 = 'Android is a mobile operating system developed by Google.'\ntest_sentence_2 = 'Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.'\ntest_sentence_3 = \"Google and Twitter made a deal in 2015 that gave Google access to Twitter's firehose.\"\ntest_sentence_4 = 'Twitter is an online news and social networking service on which users post and interact with messages known as tweets.'\ntest_sentence_5 = 'Before entering politics, Donald Trump was a domineering businessman and a television personality.'\ntest_sentence_6 = 'The 2018 FIFA World Cup is the 21st FIFA World Cup, an international football tournament contested once every four years.'\ntest_sentence_7 = 'This is the first World Cup to be held in Eastern Europe and the 11th time that it has been held in Europe.'\ntest_sentence_8 = 'Show me the cheapest round trips from Dallas to Atlanta'\ntest_sentence_9 = 'I would like to see flights from Denver to Philadelphia.'\ntest_sentence_10 = 'Show me the price of the flights leaving Atlanta at about 3 in the afternoon and arriving in San Francisco.'\ntest_sentence_11 = 'NASA invited social media users to experience the launch of ICESAT-2 Satellite.'","ebc93142":"# test_sentence_2 = 'Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.'\n\nwords_test_sentence_2 = nltk.word_tokenize(test_sentence_2)\n\n# pos tags with VanillaViterbiPOSTagger\npos_tagged_sequence_with_vanilla_viterbi_2 = VanillaViterbiPOSTagger(words_test_sentence_2)\nprint(\"Tagging sequence with VanillaViterbiPOSTagger -\\n\\n\", pos_tagged_sequence_with_vanilla_viterbi_2)\n\nprint('\\n')\nprint('*'*120)\n\n# pos tags with ViterbiBackedupByTrigramTagger\npos_tagged_sequence_with_viterbi_trigram_2 = ViterbiBackedupByTrigramTagger(words_test_sentence_2)\nprint(\"Tagging sequence with ViterbiBackedupByTrigramTagger -\\n\\n\", pos_tagged_sequence_with_viterbi_trigram_2)","f3018f67":"cols = ['vanilla_viterbi_tags', 'viterbi_trigram_tags_with_corrections']\ntags_correction_df_2 = pd.DataFrame(columns = cols)\ntags_correction_df_2 = tags_correction_df_2.fillna(0)\n\ntags_correction_df_2.vanilla_viterbi_tags = [tup for tup in pos_tagged_sequence_with_vanilla_viterbi_2]\ntags_correction_df_2.viterbi_trigram_tags_with_corrections = [tup for tup in pos_tagged_sequence_with_viterbi_trigram_2]\n\ntags_correction_df_2","a5fbe7bf":"# test_sentence_4 = 'Twitter is an online news and social networking service on which users post and interact with messages known as tweets.'\n\nwords_test_sentence_4 = nltk.word_tokenize(test_sentence_4)\n\n# pos tags with VanillaViterbiPOSTagger\npos_tagged_sequence_with_vanilla_viterbi_4 = VanillaViterbiPOSTagger(words_test_sentence_4)\nprint(\"Tagging sequence with VanillaViterbiPOSTagger -\\n\\n\", pos_tagged_sequence_with_vanilla_viterbi_4)\n\nprint('\\n')\nprint('*'*120)\n\n# pos tags with ViterbiBackedupByTrigramTagger\npos_tagged_sequence_with_viterbi_trigram_4 = ViterbiBackedupByTrigramTagger(words_test_sentence_4)\nprint(\"Tagging sequence with ViterbiBackedupByTrigramTagger -\\n\\n\", pos_tagged_sequence_with_viterbi_trigram_4)","fc871899":"tags_correction_df_4 = pd.DataFrame(columns = cols)\ntags_correction_df_4 = tags_correction_df_4.fillna(0)\n\ntags_correction_df_4.vanilla_viterbi_tags = [tup for tup in pos_tagged_sequence_with_vanilla_viterbi_4]\ntags_correction_df_4.viterbi_trigram_tags_with_corrections = [tup for tup in pos_tagged_sequence_with_viterbi_trigram_4]\n\ntags_correction_df_4","5f9cf657":"# test_sentence_1 = 'Android is a mobile operating system developed by Google.'\n\nwords_test_sentence_1 = nltk.word_tokenize(test_sentence_1)\n\n# pos tags with VanillaViterbiPOSTagger\npos_tagged_sequence_with_vanilla_viterbi_1 = VanillaViterbiPOSTagger(words_test_sentence_1)\nprint(\"Tagging sequence with VanillaViterbiPOSTagger -\\n\\n\", pos_tagged_sequence_with_vanilla_viterbi_1)\n\nprint('\\n')\nprint('*'*120)\n\n# pos tags with ViterbiBackedupByTrigramTagger\npos_tagged_sequence_with_viterbi_trigram_1 = ViterbiBackedupByTrigramTagger(words_test_sentence_1)\nprint(\"Tagging sequence with ViterbiBackedupByTrigramTagger -\\n\\n\", pos_tagged_sequence_with_viterbi_trigram_1)","428f1445":"tags_correction_df_1 = pd.DataFrame(columns = cols)\ntags_correction_df_1 = tags_correction_df_1.fillna(0)\n\ntags_correction_df_1.vanilla_viterbi_tags = [tup for tup in pos_tagged_sequence_with_vanilla_viterbi_1]\ntags_correction_df_1.viterbi_trigram_tags_with_corrections = [tup for tup in pos_tagged_sequence_with_viterbi_trigram_1]\n\ntags_correction_df_1","cedc2b85":"# pos tags with VanillaViterbiPOSTagger\nsample_test_tagged_sequence_with_vanilla_viterbi = VanillaViterbiPOSTagger(sample_test_tagged_words)\nprint(\"Tagging sequence with VanillaViterbiPOSTagger -\\n\\n\", sample_test_tagged_sequence_with_vanilla_viterbi)\n\nprint('\\n')\nprint('-'*120)\n\n# Method 1 - pos tags with VanillaViterbiForUnknownWords\nsample_test_tagged_sequence_unknown_words = VanillaViterbiForUnknownWords(sample_test_tagged_words)\nprint(\"Tagging sequence with viterbi using only transition probability for unknown words -\\n\\n\", \n      sample_test_tagged_sequence_unknown_words)\n\nprint('\\n')\nprint('-'*120)\n\n# Method 2 - pos tags with ViterbiBackedupByTrigramTagger\nsample_test_tagged_sequence_with_viterbi_trigram = ViterbiBackedupByTrigramTagger(sample_test_tagged_words)\nprint(\"Tagging sequence with ViterbiBackedupByTrigramTagger -\\n\\n\", sample_test_tagged_sequence_with_viterbi_trigram)","53e0579b":"# Let's get the final results in a dataframe containing the below: \n# 1. The actual tags using nltk.pos_tag_sent\n# 2. Words as tagged by VanillaViterbiPOSTagger\n# 3. Words as tagged by VanillaViterbiForUnknownWords - METHOD - 1\n# 4. Words as tagged by ViterbiBackedupByTrigramTagger - METHOD - 2\n\ncols1 = ['actual_tags', 'as_tagged_by_vanilla_viterbi', 'as_tagged_by_Vanilla_Viterbi_for_Unknown_Words',\n            'as_tagged_by_Vanilla_Viterbi_backed_by_trigram_tagger']\n\ntags_comparison_df = pd.DataFrame(columns = cols1)\ntags_comparison_df = tags_comparison_df.fillna(0)\n\n# Actual tags\ntags_comparison_df.actual_tags = [tup for tup in sample_test_test_run_base]\n\n# tags by VanillaViterbiPOSTagger\ntags_comparison_df.as_tagged_by_vanilla_viterbi = [tup for tup in sample_test_tagged_sequence_with_vanilla_viterbi]\n\n# tags by VanillaViterbiForUnknownWords - METHOD - 1\ntags_comparison_df.as_tagged_by_Vanilla_Viterbi_for_Unknown_Words = [tup for tup in sample_test_tagged_sequence_unknown_words]\n\n# tags by ViterbiBackedupByTrigramTagger - METHOD - 2\ntags_comparison_df.as_tagged_by_Vanilla_Viterbi_backed_by_trigram_tagger = [tup for tup in \n                                                                            sample_test_tagged_sequence_with_viterbi_trigram]\n\ntags_comparison_df[tags_comparison_df.as_tagged_by_vanilla_viterbi != \n                   tags_comparison_df.as_tagged_by_Vanilla_Viterbi_backed_by_trigram_tagger]","0fc9949e":"endTimeOfExecution = time.time()","862c472c":"print(\"Total time of execution of this assignment notebook in seconds: \", endTimeOfExecution - startTimeOfExecution)","0a3bd4bc":"#### As we see in the sample test sentence 1, the vanilla viterbi tagged 'Android' as X. This was corrected by the modified viterbi algorithm (backed up by the trigram tagger) and tagged as NOUN.\n\n#### Now lets see the tags and corrections for all the sample test sentences with both the methods.\n\n#### Method 1 - We used only the transition probability for unknown words.\n\n#### Method 2 - We used the viterbi algorithm and backed it up with the trigram tagger which was inturn backed up by the rule based regex tagger.","ff20246f":"#### As we see above in the sample test sentence 2 the word 'Android' was tagged as X by vanilla viterbi. The modified algorithm of viterbi backed up by trigram tagger corrected this and tagged 'Android' as NOUN.","bf810187":"### Scenario - 2","c9f33793":"#### The accuracy of the Vanilla Viterbi Algorithm after modification to handle unknown words is 93.94%.\n#### We see the modified viterbi algorithm performs better than the original vanilla viterbi algorithm. The modified viterbi achieves an accuracy of (approx.) 93.94% comapred to 91.33% of the vanilla viterbi.\n\n#### Still there is a loss of approx. 6%.\n\n#### Now lets see if we can in reduce this loss further.","a91064a8":"### Evaluating tagging accuracy","b4d28cba":"### STEP 1: Data Preparation","701e06a7":"#### 1. First lets check the tagging accuracy on the test_set data generated using the test_train_split","25b579d0":"#### Let's try with some Lexicon and Rule-Based Models for POS Tagging.","ff329f3d":"#### Let's now try combining the unigram tagger with a rule based regex tagger.","1c972c30":"#### 3. Testing the accuracy of ViterbiBackedupByTrigramTagger on sample test data -","87019db0":"## Syntactic Analysis Assignment - ABHINAV PANDEY\n\n### HMMs and Viterbi algorithm for POS tagging\n\n#### In this assignment, we need to modify the Viterbi algorithm to solve the problem of unknown words using at least two techniques.\n\n#### We need to accomplish the following in this assignment:\n\n1. Write the vanilla Viterbi algorithm for assigning POS tags (i.e. without dealing with unknown words) \n2. Solve the problem of unknown words using at least two techniques. These techniques can use any of the approaches discussed in the class - lexicon, rule-based, probabilistic etc. Note that to implement these techniques, you can either write separate functions and call them from the main Viterbi algorithm, or modify the Viterbi algorithm, or both.\n3. Compare the tagging accuracy after making these modifications with the vanilla Viterbi algorithm.\n4. List down at least three cases from the sample test file (i.e. unknown word-tag pairs) which were incorrectly tagged by the original Viterbi POS tagger and got corrected after your modifications.\n\n### Let's begin!","925af95b":"#### Let's build a Vanilla Veterbi POS Tagger","6f28edd1":"### Conclusion -\n\n#### Thus we see that most of the words that were incorrectly tagged by the vanilla viterbi algorithm were corrected by the viterbi algorithm backed up by the trigram tagger.\nIn this analysis we used 2 modification techniques for the original vanilla viterbi -\n\n#### Method 1 -\n- We used only the transition probability for unknown words.\n- This method showed an increase in the accuracy (93.94%) but still had some incorrect classification.\n- There was a room to increase the accuracy further and we did that using a second modification as explained below.\n\n#### Method 2 -\n- We used the viterbi algorithm and backed it up with the trigram tagger which was inturn backed up by the rule based regex tagger.\n- This gave us an accuracy of 96.35% on the test data.\n- Further, on testing this model on the provided sample test data, it achieved an accuracy of 92.26% which clearly indicates that it is the best model.\n\n#### Please note that the accuracies change every time we run the Viterbi methods. This workbook contains the accuracies as obtained during the last run.","48cc2283":"#### 2. Testing the accuracy of VanillaViterbiForUnknownWords on sample test data -","a736f254":"### Method 2\n\n#### We see that the Trigram Tagger backed up by the bigram tagger gives an accuracy of about 94.08%. Let's now try to modify the viterbi algorithm to use this trigram tagger as a back-off.\n\n#### The cases where the viterbi algorithm is not able to tag an unknown word, we have used the rule-based tagger.","f968a16f":"#### Let's check the unknown words before proceeding. These are the words this algorithm hasn't seen before.","eca636bc":"#### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm","38631974":"#### Exploratory Data Analysis","cac21621":"#### Unigram Tagger","c400c0eb":"### Solve the problem of unknown words\n\n### Method 1\n#### With the default Vanilla viterbi algorithm, we achieved an accuracy of about 91.33%. This approx. 9% loss of accuracy was mainly due to the fact that when the algorithm encountered an unknown word (i.e. not present in the training set, such as 'Norfolk'), it assigned an incorrect tag arbitrarily. This is because, for unknown words, the emission probabilities for all candidate tags are 0, so the algorithm arbitrarily chooses (the first) tag.\n\n#### Now lets see how we can minimize this loss by modifying the vanilla viterbi.\n\n#### If a word is not present in the training vocabulary (unknown word), its emission probability will be ZERO. In that case the state probability will inturn be zero. Thus in cases where the algorithm encounters a word which it has not seen previously, we can omit the emission probability and only consider the transition probability.\n\n#### The algorithm can be modified to deal the case as below:\n\n#### If word is unknown then consider only the transition_p (emission probability being 0) else consider both the emission and transition probabilities.","dcf484aa":"#### Comparing the accuracies of the 3 algorithm on sample test data -","4aadf286":"#### Emission and Transition Probabilities","d075fd9f":"### Scenario - 3","11f52308":"#### As we see above in the sample test sentence 4 the following words were incorrectly tagged by the vanilla viterbi algorithm - 'Twitter' - X, 'online' - X, 'messages' - X, 'tweets' - CONJ, 'networking' - X.\n\n#### The above words were corrected by viterbi backed up by the trigram tagger. The corrections were made as - 'Twitter' - NOUN, 'online' - NOUN, 'messages' - VERB, 'tweets' - NOUN, 'networking' - VERB.","0b729fa8":"#### 1. Testing the accuracy of VanillaViterbiPOSTagger on sample test data -","250618c9":"### Scenario - 1","a2e9f0c5":"#### 2. Next, lets check the tagging accuracies for the algorithm on the sample test sentences provided -","a4c05d78":"#### List down cases which were incorrectly tagged by original POS tagger & got corrected by your modifications -","7ae5ed78":"## This marks the end of this assignment."}}