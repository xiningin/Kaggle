{"cell_type":{"bccd2c0c":"code","7073b57c":"code","31dc02be":"code","37732273":"code","aaad2f2d":"code","3cf89b40":"code","810882c1":"code","a1347076":"code","00fa0034":"code","c6fa8f1e":"code","b2041802":"code","8bd38c00":"code","25f0ec93":"code","c952f2d5":"code","0ff70708":"code","8d21add9":"code","f58c4ddc":"code","af9a005d":"code","0c099207":"code","072befad":"code","6e3e14a5":"code","3f5689e8":"code","4f8d347c":"code","0ffd986b":"code","109c1018":"code","f86b5496":"code","986c9bcc":"code","5bca9488":"code","1fce3154":"code","dd9d6560":"code","830e8a26":"code","c6ef24e4":"code","0e44ecc1":"code","2377a4c0":"code","89d658d6":"code","9aa49a69":"code","6c57869c":"code","b1b2e7c7":"code","d091367b":"code","3c613b2c":"code","31a44094":"code","0d00ef36":"code","f400c74a":"code","867e6f8d":"code","1645fbbe":"code","7249adc5":"code","140590d3":"code","08a1fa1b":"code","3ea8f2ec":"code","b3aa4628":"code","b4b33ea7":"code","ada37372":"code","8f5f6a83":"code","9fe7315b":"code","82900c75":"code","2a199b11":"code","d0d7c38d":"code","34f2a954":"code","17e2b174":"code","592f6f77":"code","3ca1705a":"code","b724158d":"code","a98131b4":"code","d4ae6fe6":"code","8d29d8dd":"code","0414ff3f":"code","f34e2a8b":"code","bf9daf41":"code","3f6f4471":"code","edb80228":"code","7c370fa6":"code","dcd94a15":"code","ceae22bc":"code","00cf1ddc":"code","d92fe8e2":"code","1d0eb0ab":"code","91875b82":"code","8f2229d9":"code","e71c307a":"code","9c46fb36":"code","17f843ce":"code","63f5abf4":"code","5c889a11":"code","62f66af9":"code","d33ce2ff":"code","df409fc9":"code","506f1d07":"markdown","c9e7341c":"markdown","3bff4512":"markdown","225507ab":"markdown","70c00fd8":"markdown","be0db836":"markdown","86999760":"markdown","91612341":"markdown","9a2397d9":"markdown","90c1ebed":"markdown","24b1be56":"markdown","5db69a53":"markdown","eea9a5f6":"markdown","74f8e7e8":"markdown","d9971e8e":"markdown","5f74dbc1":"markdown","cbfe200d":"markdown","36bcc7ad":"markdown","3b60ae84":"markdown","97f3df10":"markdown","993e091d":"markdown","bb71be43":"markdown","02f827f0":"markdown","a9d2badb":"markdown","272e5529":"markdown","a0450fa7":"markdown","0bf02dbe":"markdown","8d41c3b9":"markdown","71c42578":"markdown","c9331e82":"markdown","fda353bd":"markdown","1d340d5f":"markdown","833d6492":"markdown","d7bf5cdf":"markdown","22bc7fa2":"markdown","d9f81f38":"markdown","dbb9e11a":"markdown","89659473":"markdown","b3b41cd9":"markdown","60f35cad":"markdown","bdda9e28":"markdown","a5a7b757":"markdown","af9b4c96":"markdown","fa3d4f90":"markdown","b8c5f09c":"markdown","9b42ef59":"markdown","5e76c990":"markdown","86dfe808":"markdown","54de451d":"markdown","bfaf29ed":"markdown","5833546e":"markdown","e8258074":"markdown","fe16c76e":"markdown","dd74f2eb":"markdown","7961dbe5":"markdown","13c94617":"markdown","e76f7c22":"markdown","bc2f56f5":"markdown","6aa2f587":"markdown","996777a5":"markdown","640903d6":"markdown","26a875e7":"markdown","2569cfdd":"markdown","8c78998b":"markdown","d27e7311":"markdown","23c1e358":"markdown","5105a21b":"markdown","ef287ced":"markdown","c9e8c849":"markdown","eae68f8d":"markdown","d57c9f0f":"markdown","59709f2c":"markdown","ae2bdbf8":"markdown","f6da0554":"markdown","a922725b":"markdown","475a5ddf":"markdown","613d59ac":"markdown","53b44237":"markdown","302988dc":"markdown","dfc37888":"markdown","2977b97d":"markdown","253a5b91":"markdown","e8dc65e9":"markdown","c2798688":"markdown","e9832970":"markdown","7782ec62":"markdown","41888b40":"markdown","c1131a70":"markdown","90cf3a78":"markdown","5e0212f7":"markdown","ac134191":"markdown","e3cb0fd4":"markdown","5b1cdc7d":"markdown","e2f5e16a":"markdown","de7b17af":"markdown","500f0e9f":"markdown","68f7720a":"markdown","6f4c77ea":"markdown","79cc5203":"markdown","cbb6f2d9":"markdown","7c12850d":"markdown","7a81a4a0":"markdown","bba359d0":"markdown","44d2d6c5":"markdown","0b408123":"markdown"},"source":{"bccd2c0c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7073b57c":"!pip install lofo-importance","31dc02be":"!pip install optuna","37732273":"import pandas as pd\nimport numpy as np\nfrom numpy import absolute\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom lofo import LOFOImportance, plot_importance, Dataset\nimport lightgbm as lgbm\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport optuna","aaad2f2d":"train = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\ntest = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\nprint('Train set shape: ', train.shape)\nprint('Test set shape: ', test.shape)","3cf89b40":"original_train = train.copy(deep  = True)","810882c1":"def missing_values(df):\n        \n        missing_values = df.isnull().sum()\n        #compute the percentage\n        missing_values_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # concatenate the two table\n        missing_values_table = pd.concat([missing_values, missing_values_percent], axis=1)\n        #give the columns more meaningful names\n        mis_val_table_ren_columns = missing_values_table.rename(\n        columns = {0 : 'Number of Missing Values', 1 : 'Percentage of Entries Missing'})\n        # Sort the table in descending order to see biggest values first\n        mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0\n                                                             ].sort_values('Percentage of Entries Missing', ascending=False).round(1)\n        \n        # Return the created missing values dataframe\n        return mis_val_table_ren_columns","a1347076":"missing_values(train)","00fa0034":"train_dtypes = pd.DataFrame(train.dtypes.value_counts()).reset_index()\ntrain_dtypes.columns = ['dtypes', 'column count']\n\ntrain_dtypes","c6fa8f1e":"train.loc[:, train.dtypes == np.object].describe()","b2041802":"# Instantiate a label encoder\nlabel_encode = LabelEncoder()\n# Iterate over columns\nfor col in train:\n    if train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(train[col].unique())) <= 2:\n            label_encode.fit(train[col])\n            # apply the transformation to both train and test sets\n            train[col] = label_encode.transform(train[col])\n            test[col] = label_encode.transform(test[col])","8bd38c00":"#one-hot encode the multiclass categoricals\ntrain = pd.get_dummies(train)\ntest = pd.get_dummies(test)\n\nprint('Training Features shape: ', train.shape)\nprint('Testing Features shape: ', test.shape)\ntrain_labels = train['TARGET']\n\n#align the training and testing data, keep only columns present in both dataframes\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\n#add the target back in\ntrain['TARGET'] = train_labels\nprint('Aligned Training Features shape: ', train.shape)\nprint('Aligned Testing Features shape: ', test.shape)","25f0ec93":"train['TARGET'].value_counts()\ntrain['TARGET'].value_counts().plot(kind='bar', figsize=(10,5), color = ['blue', 'red'])\nplt.xlabel('Target Value')\nplt.ylabel('Number of occurances') \nplt.show()","c952f2d5":"#Defining a function to plot KDE plots\n\ndef plot_kde(df, col, reverse_scale = False):\n    \n    plt.figure(figsize = (12, 6))\n    \n    if reverse_scale == True:\n        r = -1\n    else:\n        r = 1\n    \n    #KDE of paid loans (target == 0)\n    sns.kdeplot(df.loc[df['TARGET'] == 0, col] * r, label = 'Target: 0', color = 'green', shade = True)\n\n    #KDE of defaults (target == 1)\n    sns.kdeplot(df.loc[df['TARGET'] == 1, col] * r, label = 'Target: 1', color = 'purple', shade = True)\n\n    plt.xlabel('{}'.format(col)); plt.ylabel('KDE'); plt.title('KDE for column {}'.format(col));\n    plt.show()\n    plt.close()\n\n","0ff70708":"# iterate over all float (continuous) variables and plot KDE\nfor col in original_train.loc[:, (original_train.dtypes == np.float64)].columns.values:\n    # do not plot target \n    if col != 'TARGET':\n        # reverse axis if values are negative\n        if (original_train[col].median() < 0):\n            plot_kde(train,col, reverse_scale = True)\n        else:\n            plot_kde(train,col)","8d21add9":"# Defining function for plotting categorical bar charts for remaining variables\ndef plot_bars(df, col):\n    \n    plt.figure(figsize = (11, 5))\n    \n    df_high = df[df['TARGET'] == 1].groupby(col)['TARGET'].agg('count')\n    df_var = df.groupby(col)['TARGET'].agg('count')\n    categorical = df_high.divide(df_var, fill_value = 0) * 100\n\n    # Convert back to df\n    df_categorical = categorical.to_frame().reset_index().sort_values('TARGET', ascending = True)\n\n    # Create plot in Plotly for interactive visualisation (with some Starling colours)\n    ax = df_categorical.plot(x = col, y = 'TARGET', kind = 'barh', figsize=(10,10), color = 'purple')\n    ax.set_xlabel('Percentage of defaulters %')\n    ax.set_ylabel(col)\n    plt.title('Percentage of defaulters plot for {}'.format(col.lower()));\n    plt.show()\n    plt.close()","f58c4ddc":"# iterate over object\/categorical columns and plot\nfor col in original_train.loc[:, original_train.dtypes == np.object].columns.values:\n    plot_bars(original_train, col)","af9a005d":"# Find correlations with the target(takes a while due to many features)\ncorrelations = train.corr()['TARGET'].sort_values()","0c099207":"print('Most Positive Correlations:\\n')\nprint(correlations.sort_values(ascending = False).head(16))\nprint('\\nMost Negative Correlations:\\n')\nprint(correlations.head(15))","072befad":"ft_list = list(correlations.index)[0:15] + list(correlations.index)[-16:]\nft_list","6e3e14a5":"df = train[ft_list]\ndf","3f5689e8":"sample = df.copy(deep = True)","4f8d347c":"# create X_train, y_train\nX_train = df.drop('TARGET', axis = 1)\ny_train = df['TARGET']\nX_test = test[ft_list[:len(ft_list)-1:]]\ntarget = y_train\nfeatures = X_train\n# Feature names\nfeatures_list = list(X_train.columns)","0ffd986b":"#Impute missing values using median strategy\nimputer = SimpleImputer(strategy = 'median')\n\n#Scale all values in 0-1 range\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n#fit the imputer on training set\nimputer.fit(X_train)\n\n#transform both training and testing data\nX_train = imputer.transform(X_train)\nX_test = imputer.transform(X_test)\n\n#fit scaler and transform\nscaler.fit(X_train)\ntrain = scaler.transform(X_train)\ntest = scaler.transform(X_test)\n","109c1018":"\nlgbm_model = lgbm.LGBMClassifier(n_estimators=500, objective = 'binary', \n                                learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 42)\nridge_model = RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n                max_iter=None, normalize=True, random_state=None, solver='auto',\n                tol=0.001)\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)","f86b5496":"#scoring the lgbm model\n#scores = cross_val_score(lgbm_model, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)\n#print('Mean F1: %.3f (%.3f)' % (mean(scores), std(scores)))","986c9bcc":"#scoring the ridge model\n#scores = cross_val_score(ridge_model, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)\n#print('Mean F1: %.3f (%.3f)' % (mean(scores), std(scores)))","5bca9488":"train_features, test_features, train_labels, test_labels = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)\n","1fce3154":"lgbm_model.fit(train_features, train_labels)","dd9d6560":"ridge_model.fit(train_features, train_labels)","830e8a26":"\nlgbm_predictions = lgbm_model.predict(test_features)\nridge_predictions = ridge_model.predict(test_features)\n","c6ef24e4":"#let's get the scores\ndef get_scores (preds): \n    print('Accuracy: %.3f' % accuracy_score(test_labels.values, preds))\n    print('Precision: %.3f' % precision_score(test_labels.values, preds))\n    print('Recall: %.3f' % recall_score(test_labels.values, preds))\n    print('F1: %.3f' % f1_score(test_labels.values, preds))\nprint('LGBM scores: ')\nget_scores(lgbm_predictions)\nprint('Ridge scores: ')\nget_scores(ridge_predictions)","0e44ecc1":"#out of curiosity, let's take a look at our feature performance\ndef extract_feature_importance(model): \n    model_fi = model.feature_importances_\n    feature_importances = pd.DataFrame({'Feature': features_list, 'Importance': model_fi})\n    return feature_importances\nlgbm_fi = extract_feature_importance(lgbm_model)\n","2377a4c0":"def plot_feature_importance(df):\n\n    # Normalize the feature importances to add up to one\n    df['Importance_normalized'] = df['Importance'] \/ df['Importance'].sum()\n    df = df.sort_values('Importance_normalized', ascending = True).tail(20)\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 16))\n\n    ax = df.plot(x = 'Feature' , y = 'Importance_normalized', kind = 'barh', figsize=(10,10), color = 'blue')\n    \n    # Plot labeling\n    plt.xlabel('Importance')\n    plt.title('Feature Importances')\n    plt.show()\n    \n    # return top 20 features\n    return(df['Feature'])\n","89d658d6":"lgbm_top = plot_feature_importance(lgbm_fi)","9aa49a69":"# define the binary target and the features\ndataset = Dataset(df=sample, target=\"TARGET\",features = features)","6c57869c":"# define the validation scheme and scorer. \nlofo_imp = LOFOImportance(dataset, model = lgbm_model, scoring=\"accuracy\")","b1b2e7c7":"# get the mean and standard deviation of the importances in pandas format\n#importance_df = lofo_imp.get_importance()","d091367b":"# plot the means and standard deviations of the importances\n#plot_importance(importance_df, figsize=(12, 20))","3c613b2c":"#lofo_imp_ridge = LOFOImportance(dataset, model = ridge_model, scoring=\"accuracy\")","31a44094":"#ridge_importance_df = lofo_imp_ridge.get_importance()","0d00ef36":"#plot_importance(ridge_importance_df, figsize=(12, 20))","f400c74a":" #define the CV instance that will be used for hyperparameter optimization\ncv = RepeatedKFold(n_splits = 5, n_repeats = 10, random_state = 42)\n","867e6f8d":"#define LGBM search space\nlgbm_space = dict()\nlgbm_space['num_leaves'] = [50,100,1000]\nlgbm_space['max_depth'] = [5,30,50]\nlgbm_space['min_data_in_leaf'] = [200,5000,10000]\nlgbm_space['learning_rate'] = [0.01,0.1, 0.5]\nlgbm_space['n_estimators'] = [100, 500, 1000]","1645fbbe":"#define ridge search space\nridge_space = dict()\nridge_space['alpha'] = [0.1, 0.5, 1]\nridge_space['solver'] = ['auto', 'svd', 'sag', 'saga', 'lbfgs']\nridge_space['random_state'] = [42]","7249adc5":"#define base models\nlgbm_base = lgbm.LGBMClassifier()\nridge_base = RidgeClassifier()","140590d3":"lgbm_rand_search = RandomizedSearchCV(lgbm_base, lgbm_space, n_iter = 5, scoring = 'f1', n_jobs = -1)","08a1fa1b":"ridge_rand_search = RandomizedSearchCV(ridge_base, ridge_space, n_iter = 5, scoring = 'f1', n_jobs = -1)","3ea8f2ec":"#lgbm_rand_result = lgbm_rand_search.fit(train_features, train_labels)","b3aa4628":"#ridge_rand_result = ridge_rand_search.fit(train_features, train_labels)","b4b33ea7":"#print('LGBM RS Best Score: %s' % lgbm_rand_result.best_score_)\n#print('LGBM RS Best Hyperparameters: %s' % lgbm_rand_result.best_params_)","ada37372":"#print('Ridge RS Best Score: %s' % ridge_rand_result.best_score_)\n#print('Ridge RS Best Hyperparameters: %s' % ridge_rand_result.best_params_)","8f5f6a83":"lgbm_grid_search = GridSearchCV(lgbm_base, lgbm_space, scoring = 'f1', n_jobs = -1)","9fe7315b":"ridge_grid_search = GridSearchCV(ridge_base, ridge_space, scoring = 'f1', n_jobs = -1)","82900c75":"#lgbm_grid_result = lgbm_grid_search.fit(train_features, train_labels)","2a199b11":"#ridge_grid_result = ridge_grid_search.fit(train_features, train_labels)","d0d7c38d":"#print('LGBM GS Best Score: %s' % lgbm_grid_result.best_score_)\n#print('LGBM GS Best Hyperparameters: %s' % lgbm_grid_result.best_params_)","34f2a954":"#print('Ridge GS Best Score: %s' % ridge_grid_result.best_score_)\n#print('Ridge GS Best Hyperparameters: %s' % ridge_grid_result.best_params_)","17e2b174":"\ndef objective_lgbm (trial): \n    print(\"Started Trial...\")\n    #the hyperparameters to tune\n    param_grid = {\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [100, 200, 500]),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.05, 0.1),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 100, 3000, step=100),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 10),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 500, 1000, step=100),\n        \"random_state\": 42,\n        \"n_jobs\":-1\n    }\n    lgbm_optuna_model = lgbm.LGBMClassifier(objective = 'binary', **param_grid)\n    return cross_val_score(lgbm_optuna_model, train_features, train_labels, scoring = 'f1', cv=cv,n_jobs = -1).mean()\n","592f6f77":"lgbm_study = optuna.create_study(direction = 'maximize')\n","3ca1705a":"#lgbm_study.optimize(objective_lgbm, n_trials = 20)","b724158d":"#best_lgbm_optuna = lgbm_study.best_trial\n#print('LGBM F1: {}'.format(best_lgbm_optuna.value))","a98131b4":"#print(\"Best LGBM hyperparameters: {}\".format(best_lgbm_optuna.params))","d4ae6fe6":"def objective_ridge (trial): \n    #the hyperparameters to tune\n    param_grid = {\n    'solver' : trial.suggest_categorical(\"solver\", ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']), \n    'alpha' : trial.suggest_uniform('alpha', 0.5, 1), \n    \"random_state\": 42\n    }\n    ridge_optuna_model = RidgeClassifier(**param_grid)\n    return cross_val_score(ridge_optuna_model, train_features, train_labels, scoring = 'f1', cv=cv,n_jobs = -1).mean()","8d29d8dd":"ridge_study = optuna.create_study(direction = 'maximize')\n","0414ff3f":"#ridge_study.optimize(objective_ridge, n_trials = 10)","f34e2a8b":"#best_ridge_optuna = ridge_study.best_trial\n#print('F1: {}'.format(best_ridge_optuna.value))","bf9daf41":"#print(\"Best Ridge hyperparameters: {}\".format(best_ridge_optuna.params))","3f6f4471":"reduced_ft_list = ['AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE', 'DAYS_ID_PUBLISH', 'DAYS_BIRTH']\nreduced_X_train = df[reduced_ft_list]\nreduced_X_train","edb80228":"#impute and scale reduced_X_train\nimputer = SimpleImputer(strategy = 'median')\nscaler = MinMaxScaler(feature_range = (0, 1))\nimputer.fit(reduced_X_train)\nreduced_X_train = imputer.transform(reduced_X_train)\nscaler.fit(reduced_X_train)\ntrain = scaler.transform(reduced_X_train)","7c370fa6":"#train test split new dataset\nreduced_train_features, reduced_test_features, reduced_train_labels,reduced_test_labels = train_test_split(reduced_X_train, y_train, test_size = 0.2, random_state = 42) ","dcd94a15":"reduced_lgbm_model = lgbm.LGBMClassifier(n_estimators=500, objective = 'binary', \n                                learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 42)\nreduced_ridge_model = RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n                max_iter=None, normalize=True, random_state=None, solver='auto',\n                tol=0.001)","ceae22bc":"reduced_lgbm_model.fit(reduced_train_features,reduced_train_labels)","00cf1ddc":"reduced_ridge_model.fit(reduced_train_features,reduced_train_labels)","d92fe8e2":"reduced_lgbm_predictions = reduced_lgbm_model.predict(reduced_test_features)\nreduced_ridge_predictions = reduced_ridge_model.predict(reduced_test_features)","1d0eb0ab":"print('LGBM scores: ')\nget_scores(reduced_lgbm_predictions)\nprint('Ridge scores: ')\nget_scores(reduced_ridge_predictions)","91875b82":"tuned_lgbm_model = lgbm.LGBMClassifier(n_estimators= 500, learning_rate = 0.09913706634323802, \n                                       num_leaves = 900, max_depth = 8, min_data_in_leaf = 600)","8f2229d9":"tuned_lgbm_model.fit(train_features, train_labels)","e71c307a":"tuned_lgbm_predictions = tuned_lgbm_model.predict(test_features)","9c46fb36":"print('Tuned LGBM scores: ')\nget_scores(tuned_lgbm_predictions)","17f843ce":"reduced_tuned_lgbm_model = lgbm.LGBMClassifier(n_estimators= 500, learning_rate = 0.09913706634323802, \n                                       num_leaves = 900, max_depth = 8, min_data_in_leaf = 600)","63f5abf4":"reduced_tuned_lgbm_model.fit(reduced_train_features, reduced_train_labels)","5c889a11":"reduced_tuned_lgbm_predictions = reduced_tuned_lgbm_model.predict(reduced_test_features)","62f66af9":"print('Tuned LGBM scores: ')\nget_scores(reduced_tuned_lgbm_predictions)","d33ce2ff":"classification_report(test_labels, tuned_lgbm_predictions, output_dict = True)","df409fc9":"confusion_matrix(test_labels, tuned_lgbm_predictions)","506f1d07":"However, just because the model is relying on them the most doesn't mean that they are necessary or that they improve its performance. In order to find out which features are the best, we will use LOFO (Leave One Feature Out). ","c9e7341c":"Let's see what the best parameters are: ","3bff4512":"Now I will start the hyperparameter optimization. This is an essential step to get better performance because the quality of our model necessarily depends on the parameters it was instantiated with. ","225507ab":"# 6. Performance improvement: ","70c00fd8":"There are many tools for hyperparameter optimization. The most famous ones are RandomizedSearch and GridSearch (which I have attempted below). BayesianSearch has also been gaining attention lately because it produces good results in a shorter amount of time. Another new tool in Optuna, which I have also attempted. ","be0db836":"Now, I will start doing some exploratory data analysis. This process has many advantages. It allows us to better understand the dataset, which will certainly lead us to building a better model. Also, it could be useful if we want to later detect outliers and skewed data. ","86999760":"Now, I am optimizing the hyperparameters. Indeed, this cell took some time to run, but it was generally faster than GridSearch and RandomSearch. Also, the intermediate outputs were printed which made it easier to keep up with the progress. Please not that I have used only 20 trials. Optuna is usually run with a far larger number, but again, I did this for the sake of efficiency. ","91612341":"Here I am instantiating two ML models: LightGBM and Sklean Ridge. Since this is a classification problem, I am using the respective classifier of each model. I haven't yet done any hyperparameter tuning, so I will just use simple or default parameters for now. ","9a2397d9":"I am keeping a copy of this dataframe because I will need it later on in LOFO.","90c1ebed":"Defining two models set with the same parameters as my baseline models in order to make comparisons between the two meaningful (i.e. I am comparing how the feature selection impacts the performance all else equal). ","24b1be56":"As a final preprocessing step, we will impute our data and scale it. For Imputing, I used the median strategy, i.e., missing values will be replaced by the median of their respective column. MinMax Scaling is also important to make sure that higher values are not necessarily given greater importance. ","5db69a53":"# 1. Preprocessing: ","eea9a5f6":"Upon running the cell above, I obtained the following plot: \n![Screenshot (163).png](attachment:678e8b36-d27b-4837-a00a-a67541cdc420.png)","74f8e7e8":"These following cells are used to get the score of each model. I have them commented out now because I already ran them once and decided not to redo it since it took about 11mins to run. If you'd like to run it, just uncomment. ","d9971e8e":"Next, I will train the models with the hyperparameters obtained by Optuna but using the original set of features and see how that goes. From now on, I will focus of LGBM since it is the most promising model. ","5f74dbc1":"After facing very slow running for RandomizedSearch, I have moved to GridSearch, because randomized is known to be way slower. ","cbfe200d":"First, the target is very much influenced by EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3, as the two distributions are drastically different.\nSecond, the changes are less apparent in some other features (DAYS_LAST_PHONE_CHANGE for example)\nThird, the target does not seem to be impacted by some features at all (LAND_AREA_MEDI for example). ","36bcc7ad":"# 2. Model Validation: ","3b60ae84":"Let's make a new sub-dataframe that would contain these features only","97f3df10":"A great tool (albeit not always sufficient) to understand relationships between features and target is by using correlations. This can make it easier to spot how a feature affects the target than using plots (although plots often have valuable visual cues). ","993e091d":"Wow! As shown above the percentage of missing values could reach values like 69.9%. That's a whole lot. Later on this notebook these values will be imputed using sklearn's SimpleImputer. Now, let's continue exploring our data. ","bb71be43":"After inspecting the plots, a few features catch my attention: ","02f827f0":"The unique row tells us the number of unique values in each column. It turns out that we have both binary and multiclass categorical features. The binary ones will be label encoded, i.e. they will be given 0 for val1 and 1 for val2, multiclass features will be one-hot encoded, i.e. we will create a new column for each value with each one of these columns being label encoded. Let's start. ","a9d2badb":"What LOFO does is basically what its name says: it takes out one features at a time, and sees how the model performs without it (better or worse). We can then plot the importances in order to see which features are good and which ones are harmful. This step is essential because it's just enough to feed to model some features: we need to feed it the right ones. Because, as the famous data science saying goes: garbage in, garbage out. If we don't take the time to select our features carefully, we will not get good results. ","272e5529":"It turns out the best trial was trial 18, which resulted in an F1 score equal to 0.04969739265934099","a0450fa7":"Keeping this is mind, let's move on to inspect the categorical features. This time, we will use bar charts.","0bf02dbe":"I will make a list of these features because these are the ones I will use to fit my baseline models. There are two reasons behind this decision: first, they are likely to produce better baselines based on which I can make improvements, and second, this can greatly reduce the computational demands (I have tried fitting my model and running LOFO using all features and it was indeed very time-consuming and computationally expensive)","8d41c3b9":"The best parameters for LGBM using optuna are the following: **{'n_estimators': 500, 'learning_rate': 0.09913706634323802, 'num_leaves': 900, 'max_depth': 8, 'min_data_in_leaf': 600}**","71c42578":"In this final section, I will comment of the results of the best performing model so far: tuned_lgbm_model. ","c9331e82":"Let's read the input csv files into pandas dataframes for data preprocessing. I am also making a copy of the train dataframe as I would like to keep an untouched version of the data for visualizations later on in the notebook. ","fda353bd":"# Closing Notes: ","1d340d5f":"One of the many factors that might impact a model's performance is the existence of missing values. If there aren't that many missing values, they could safely be dropped. However, if there are a whole lot, dropping them would lead to significant information loss. In this case they ought to be imputed. ","833d6492":"This concludes our preprocessing. Of course, it could have been more thorough and involved more steps, but for now, and for lack of time, we will proceed to the next steps. ","d7bf5cdf":"Interesting... I would have expected the models' performance to improve but it turns out that it didn't. The accuracy remained almost the same for both models, while the Precision, Recall, and F1 score. Although I followed what I had been told by LOFO, the performance worsened rather than improved. Let's see if hyperparameters can fix this. ","22bc7fa2":"Let's see what Optuna came up with for ridge. ","d9f81f38":"In the preprocessing phase, I have taken a look at what other kagglers have done and used their steps. However, instead of directly importing their preprocessed datasets through pickle or joblib, I have decided to reproduce them myself along with some exploratory data analysis because I believe that it was important for me to spend some time with the dataset and get familiar with it. Nevertheless, I did not perform an important step in preprocessing: balancing the dataset. This resulted in low model performance as I will discuss later on. ","dbb9e11a":"Now I define two instances of RandomizedSearchCV. I am limiting the number of iterations to 10 in order to make the fitting faster. I am using F1 score because it is a good middle ground between precision and recall. ","89659473":"Eventually I obtained that the best parameters for ridge are **{'solver': 'auto', 'alpha': 0.9115035913991532}**, however, the performance of the model remains incredibly low. Apparently, the RidgeClassifier would need further studies that are beyond the scope of this notebook. ","b3b41cd9":"Next, I define an Optuna Study. This takes one parameter, which is direction. I set it to 'maximize' because my scoring metric is F1 score, and this metric is maximized for better results. ","60f35cad":"In the hyperparameter optimization step, I have attempted RandomizedSearch, GridSearch, and Optuna. The first two techniques took very long to run, hence, I did not get the opportunity to benefit from their results. Optuna, on the hand, ran faster and I used its results in the subsequent steps of the notebook. ","bdda9e28":"I begin by defining a cross validation scheme. Again, I use RepeatedKFold. ","a5a7b757":"Let's first start by installing and importing all the necessary dependencies! ","af9b4c96":"I have written down a function to extract feature importances. However, I later discovered that RidgeClassifier does not support feature importances in its API. Hence, we will inspect this for LightGBM only. Later on, I will use LOFO, which is model agnostic and will work on both of them. ","fa3d4f90":"Making predictions with the new models. ","b8c5f09c":"The easiest way to use cross validation using sklearn's API is via cross_val_score, so that's what I used. ","9b42ef59":"Let's interpret the classification report generated above. It shows that for the first label\/class, the metrics seem quite good. Which means the model was able to predict non-defaults very well. On the other hand, the metrics of the second label are very much low, which means the model failed to predict defaulters. This is a common result in machine learning when the dataset is unbalanced. ","5e76c990":"In this section, I will conduct a few experiments and see which ones improve the performance of the model. ","86dfe808":"For model validation, I had the choice between various techniques (KFold, StratifiedKFold, RepeatedKFold, etc.). I have decided to use repeated KFold, and that is for the following reason: A single run of KFold may result in a noisy estimate of the model's performance. Hence, we could get a better and more reliable estimate by repeating the procedure multiple times. ","54de451d":"First, I will train the models with the features favored by LOFO. These features were 'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE', 'DAYS_ID_PUBLISH', and 'DAYS_BIRTH'. I create a new dataframe containing these features only and impute it and scale it. ","bfaf29ed":"This goes on to confirm the fact that the model fails at predicting positives (defaulters.)","5833546e":"Now I will go ahead and use Optuna. Optuna is a new tool for hyperparameter optimization that has become in style lately because of the various advantages it offers, such as not requiring the user to specify a static search space, its use of pruning (i.e. cutting an unpromising trial based on intermediate results), and it also gives the user more control over the model in general.  ","e8258074":"According to the F1 score, the two models seem quite equivalent. ","fe16c76e":"It turns out that most columns are numeric (either float64 or int64). This is in fact good news, because it means that we only have to encode 16 categorical features. ","dd74f2eb":"As shown on the bar plots above, some features, such as NAME_INCOME_TYPE, seem to closely influence that target, whereas this is less pronounced or not at all in other features","7961dbe5":"Let's take a closer look at the categoricals. The most important thing here is that we know whether they are binary of multiclass, because this will impact how we encode them. ","13c94617":"Let's plot the feature importance now to visually inspect it. ","e76f7c22":"Let's inspect our dataset and see how many missing entries it has. This will be done by computing the percentage of missing values in each column. ","bc2f56f5":"Let's get more acquainted to our dataset by inspecting the datatypes of its columns. Most machine learning models perform best with numeric values. Let's see how many numeric (and non-numeric) values we have in our dataset. ","6aa2f587":"# 7. Comments on model variables: ","996777a5":"Now, I will define the search space of my LGBM model. I have decided to tune 5 parameters only. However, I could definitely do more, especially if I had more powerful computational capabilities. ","640903d6":"Last, I will train the models using the LOFO selected features and the hyperparameters obtained by Optuna. ","26a875e7":"Best parameters for ridge: ","2569cfdd":"Making all necessary imports. ","8c78998b":"The optimization process ran for about an hour and 40 mins. Let's see what Optuna came up with. ","d27e7311":"Let's first split the data on train and test. This is done sklearn's train_test_split. ","23c1e358":"We'll do the same for the ridge classifier","5105a21b":"# 5. Hyperparameter Optimization: ","ef287ced":"Now let's make the initial model evaluation for our two models. Since we are dealing with a classification problem, I have chosen to take into consideration the most used classification metrics: Accuracy, Precision, Recall, and F1 score. ","c9e8c849":"Label encoding binary columns: ","eae68f8d":"# 4. Feature Selection with LOFO importance: ","d57c9f0f":"One-hot encoding multiclass columns: ","59709f2c":"I have this cell commented out because it takes too long to run. This is problem that I have faced repeatedly in this challenge. ","ae2bdbf8":"# 3. Baseline Models with simple parameters: ","f6da0554":"Let's first fit our LGBM model previously defined. This cell might take a while to run. ","a922725b":"Those are some quite interesting results. Although the accuracy is quite high (92%), the precision, recall and F1 scores are incredibly low. This reminds us of the fact that accuracy is not always the right metric to use. My assumption here is that the model is almost always predicting that the client will not default (which is usually the case), making it an extremely naive model. This might be due to the fact that our dataset is not balanced: most entries in the target are 0s. This could have been addressed in the beginning by attempting to balance the dataset (using undersampling for example). However, at this point we could fix this, at least in the LGBM model, which seems to be the most promising one, by using is_unbalanced or scale_pos_weight in the class_weight parameter. ","475a5ddf":"It turns out that the model is only making significant use of a handful to features, namely EXT_SOURCE_2, DAYS_REGISTRATION, DAYS_ID_PUBLISH, DAYS_LAST_PHONE_CHANGE, DAYS_BIRTH, EXT_SOURCE_3, DAYS_EMPLOYED, AMT_GOODS_PRICE, REGION_POPULATION_RELATIVE","613d59ac":"Let's generate the classification report and confusion matrix. ","53b44237":"Yet, I have faced the same problem here. The cell ran for well more than an hour when I decided to stop running and use Optuna. Of course, one hour of running is not too much in the machine learning (some deep learning models take days to train), but I unfortunately do not have the luxury of time here. ","302988dc":"Of course, the final ML model I have reached is far from being perfect. In my work, I have focused on completing the taks rather than on producing the best model. Moreover, I was often limited my the computational complexity of the tasks, especially since I was using a Kaggle CPU and not a GPU or TPU (they wouldn't have enhanced the results since they are mostly useful for neural networks). However, in order to improve my results, I would: \n1. Balance the Dataset in the preprocessing step\n2. Tune more parameters and carefully create the search spaces\n3. Do some feature engineering\n4. Make more experimented and record the pros and cons of each\n5. Try out other models (RandomForest for e.g.)\n","dfc37888":"This is my first time using Optuna and I am really glad I got the chance to learn how to use it. I first start by defining the objective function. ","2977b97d":"The confusion matrix is used to showcase the number of True Positive (correctly predicted as defauters), True Negative (correctly predicted as non-defaulters), False Positive (incorrectly predicted as defaulters), False Negative (incorrectly predicted as non-defaulters). ","253a5b91":"I redo the same process for the ridge classifier.","e8dc65e9":"Let's not get upset about the low values of the metrics, we're here to improve them after all! The first thing I would like to do is take a look at the feature importance, this shows the features that the model relied on the most, and could give us some insights about what features were useful and what weren't. ","c2798688":"Throughout the notebook, there are some cells that take very long to run (model validation using RepeatedKFold, get LOFO importance, optuna otimization, etc.) Some of them take about 11 mins (RepeatedKFold), some take 6 hours (LOFO). I have thus commented them out. If you would like to see the output dynamically, please go ahead and run them :)! ","e9832970":"Installing Optuna for hyperparameter tuning\/optimization: ","7782ec62":"Let's make initial predictions from our two models","41888b40":"Now, I will take care of numeric features. For this, I will use a function that plots the KDE (Kernel Density Estimation) of each feature against the target. This will allow us to see how the distribution of the target is influenced by the said feature, and would hence give us insights about the most important features to take into consideration. ","c1131a70":"Although all the models produced so far perform rather poorly, the best one so far is tuned_lgbm_model (i.e. the model in which I used Optuna's optimized hyperparameters and the initial features, not the ones selected by LOFO). This goes to show that LOFO did not really enhance the performance. Of course, this does not mean that it is not useful, but it didn't work in our favor in this context. On the other hand, hyperparameter optimization did serve us some good! ","90cf3a78":"In our case: TN = 56407, TP = 131, FP = 147 , FN = 4818","5e0212f7":"Let's optimize ridge's paramaters. I am using 10 trials for the sake of running the code in shorter time.  ","ac134191":"*Hello B2Metric Team! Here is my project notebook which contains the following sections: Preprocessing, Model Validation, Baseline Models, Feature Selection using LOFO, Hyperparamter Optimization, Performance Improvement, and Comments of Model Variables. I have aimed to always explain the reasoning behind the decisions I make. If anything is unclear, please let me know. Also, as I am still a beginner in the field, I would love to hear your insights, if possible, about how you would have tackled the challenge. Now, without further ado, let's get straight to work*","e3cb0fd4":"The cell below took a very long time to run. Just for 30 features, it took 6 full hours! And that's just for one model (LightGBM). I think that this is drawback of LOFO. Eventhough it gives great insights and is model agnostic, it simply is too slow. That's something that might dissuade a lot of data scientists from using it. ","5b1cdc7d":"I do the same for the ridge model. ","e2f5e16a":"Installing lofo-importance for feature selection.","de7b17af":"After six hours of running, we could finally obtain the LOFO importance plot. It shows that only 4 features had a positive score: **AMT_GOOD_PRICE, REGION_POPULATION_RELATIVE, DAYS_ID_PUBLISH, DAYS_BIRTH.** All other features actually did more harm than good. So, these are the features that we'll keep for the LightGBM model. ","500f0e9f":"First thing I will visualize is the target variable. In applications that involve default\/fraud detection, the dataset tends to be unbalanced. Let's see if that's the case for us. ","68f7720a":"Let's create two new datasets, X_train and y_train, which will be used to later on fit our models, in additin to X_test, used for final predictions","6f4c77ea":"It turns out that our dataset is in fact very much unbalanced. This poses a problem for many machine learning problems. In a more detailed project, I would certainly look into this. But for the sake of completing the tasks now, this will be skipped. ","79cc5203":"I have commented out the LOFO-importance cells because they took too long to run. If you have more powerful hardware and computational resources, please go ahead and run them. ","cbb6f2d9":"If we compare these to the results obtained via feature importance we could say that they are somewhat similar. The four features mentioned are also among the top 10 according to feature importance, but of course, LOFO gives us further insights because it tells us which ones are also harming the model.  ","7c12850d":"# Thank you B2Metric for the project! ","7a81a4a0":"# Introductory Notes: ","bba359d0":"Our features of interest are of course the ones that have the most positive correlations with the target and those with the most negative correlations. Let's find out what they are. ","44d2d6c5":"Now, it's time to build our baseline models! ","0b408123":"With the function defined above, we should just call it on different columns to generate the plots. "}}