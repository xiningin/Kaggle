{"cell_type":{"9b3a984b":"code","6859b7f7":"code","03db6b2a":"code","7325c39f":"code","263c74a0":"code","dd54ee93":"code","260b592c":"code","459ee110":"code","20a7e0f9":"code","0816fcff":"code","cce1e116":"code","adce122f":"code","e7fcdddf":"code","a9232c0f":"code","b1afbdaf":"markdown","ad784b9e":"markdown","854729bd":"markdown","53db1c95":"markdown","910c2da1":"markdown","56f57b5f":"markdown","c4ee2d84":"markdown","f448ccac":"markdown"},"source":{"9b3a984b":"import json\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport torch.nn.functional as F\nimport catalyst.dl as dl\nimport catalyst.dl.utils as utils\nimport numpy as np, pandas as pd","6859b7f7":"def one_hot(categories, string):\n    encoding = np.zeros((len(string), len(categories)))\n    for idx, char in enumerate(string):\n        encoding[idx, categories.index(char)] = 1\n    return encoding\n\ndef featurize(entity):\n    sequence = one_hot(list('ACGU'), entity['sequence'])\n    structure = one_hot(list('.()'), entity['structure'])\n    loop_type = one_hot(list('BEHIMSX'), entity['predicted_loop_type'])\n    features = np.hstack([sequence, structure, loop_type])\n    return features \n\ndef char_encode(index, features, feature_size):\n    half_size = (feature_size - 1) \/\/ 2\n    \n    if index - half_size < 0:\n        char_features = features[:index+half_size+1]\n        padding = np.zeros((int(half_size - index), char_features.shape[1]))\n        char_features = np.vstack([padding, char_features])\n    elif index + half_size + 1 > len(features):\n        char_features = features[index-half_size:]\n        padding = np.zeros((int(half_size - (len(features) - index))+1, char_features.shape[1]))\n        char_features = np.vstack([char_features, padding])\n    else:\n        char_features = features[index-half_size:index+half_size+1]\n    \n    return char_features","03db6b2a":"def augment(X: np.array):\n    \n    X = np.vstack((X, np.flip(X, axis=1)))\n    \n    return X\n\nclass VaxDataset(Dataset):\n    def __init__(self, path, test=False):\n        self.path = path\n        self.test = test\n        self.features = []\n        self.targets = []\n        self.ids = []\n        self.load_data()\n    \n    def load_data(self):\n        with open(self.path, 'r') as text:\n            for line in text:\n                records = json.loads(line)\n                features = featurize(records)\n                \n                for char_i in range(records['seq_scored']):\n                    char_features = char_encode(char_i, features, 21)\n                    self.features.append(augment(char_features))\n                    self.ids.append('%s_%d' % (records['id'], char_i))\n                        \n                if not self.test:\n                    \n                    targets = np.stack([records['reactivity'], records['deg_Mg_pH10'], records['deg_Mg_50C']], axis=1)\n                    self.targets.extend([targets[char_i] for char_i in range(records['seq_scored'])])\n                    \n    def __len__(self):\n        return len(self.features)\n    \n    def targets(self):\n        return self.targets\n    \n    def __getitem__(self, index):\n        if self.test:\n            return self.features[index], self.ids[index]\n        else:\n            return self.features[index], self.targets[index], self.ids[index]","7325c39f":"class Flatten(nn.Module):\n    def forward(self, x):\n        batch_size = x.shape[0]\n        return x.view(batch_size, -1)\n \nclass WaveBlock(nn.Module):\n\n    def __init__(self, in_channels, out_channels, dilation_rates, kernel_size):\n        super(WaveBlock, self).__init__()\n        self.num_rates = dilation_rates\n        self.convs = nn.ModuleList()\n        self.filter_convs = nn.ModuleList()\n        self.gate_convs = nn.ModuleList()\n\n        self.convs.append(nn.Conv1d(in_channels, out_channels, kernel_size=1))\n        dilation_rates = [2 ** i for i in range(dilation_rates)]\n        for dilation_rate in dilation_rates:\n            self.filter_convs.append(\n                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))\/2), dilation=dilation_rate))\n            self.gate_convs.append(\n                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))\/2), dilation=dilation_rate))\n            self.convs.append(nn.Conv1d(out_channels, out_channels, kernel_size=1))\n\n    def forward(self, x):\n        x = self.convs[0](x)\n        res = x\n        for i in range(self.num_rates):\n            x = torch.tanh(self.filter_convs[i](x)) * torch.sigmoid(self.gate_convs[i](x))\n            x = self.convs[i + 1](x)\n            res = res + x\n        return res\n\n\n\"\"\"Modified version of the SED work by Hidehisa Arai.\"\"\"\ndef init_layer(layer):\n    nn.init.xavier_uniform_(layer.weight)\n\n    if hasattr(layer, \"bias\"):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)\n            \n\ndef init_bn(bn):\n    bn.bias.data.fill_(0.)\n    bn.weight.data.fill_(1.0)\n    \nclass AttBlock(nn.Module):\n    def __init__(self,\n                 in_features: int,\n                 out_features: int,\n                 activation=\"linear\",\n                 temperature=1.0):\n        super().__init__()\n\n        self.activation = activation\n        self.temperature = temperature\n        self.att = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True)\n        self.cla = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True)\n\n        self.bn_att = nn.BatchNorm1d(out_features)\n        self.init_weights()\n\n    def init_weights(self):\n        init_layer(self.att)\n        init_layer(self.cla)\n        init_bn(self.bn_att)\n\n    def forward(self, x):\n        # x: (n_samples, n_in, n_time)\n        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n        cla = self.nonlinear_transform(self.cla(x))\n        x = torch.sum(norm_att * cla, dim=2)\n        return x\n\n    def nonlinear_transform(self, x):\n        if self.activation == 'linear':\n            return x\n        elif self.activation == 'sigmoid':\n            return torch.sigmoid(x)\n\nclass VaxModel(nn.Module):\n    def __init__(self):\n        super(VaxModel, self).__init__()\n        self.layers = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Conv1d(14, 32, 1, 1),\n            WaveBlock(32, 64, 1, 1),\n            nn.PReLU(),\n            nn.BatchNorm1d(64),\n            nn.Upsample(scale_factor=2, mode='linear'),\n            nn.Dropout(0.2),\n            nn.Conv1d(64, 1, 1, 1),\n        )\n        self.rnn1 = nn.LSTM(84, 64)\n      \n        \n        self.finalprelu = nn.PReLU()\n        self.finaldrop = nn.Dropout(0.2),\n        self.attn =  AttBlock(16,32),\n        self.final = nn.Sequential(\n        nn.PReLU(),\n        nn.Dropout(0.2),\n        nn.Linear(32, 3)\n        )\n    \n    def forward(self, features):\n        \n        features = self.layers(features)\n        features = features.permute(1, 0, 2)\n        features = self.rnn1(features)\n        features = self.finalprelu(features[0])\n        if features.size() == torch.Size([1, 16, 64]):\n            self.attn =  AttBlock(16,32).cuda().float()\n            features = self.attn(features)\n        else:\n            self.attn = AttBlock(3,32).cuda().float()\n            features = self.attn(features)\n        final = self.final(features)\n        return final","263c74a0":"model = VaxModel().cuda()\noptimizer = torch.optim.SGD(model.parameters(), 0.005, momentum=0.9)\ncriterion = nn.MSELoss()","dd54ee93":"train_dataset = VaxDataset('..\/input\/stanford-covid-vaccine\/train.json')\ntrain_dataloader = DataLoader(train_dataset, 16, shuffle=True, num_workers=4, pin_memory=True)","260b592c":"class CustomRunner(dl.Runner):\n\n    def predict_batch(self, batch):\n        # model inference step\n        return self.model(batch[0].to(self.device).permute(0, 2, 1).float()), batch[1]\n\n    def _handle_batch(self, batch):\n        # model train\/valid step\n        x, y = batch[0], batch[1]\n        x = x.cuda().permute(0,2,1).float()\n        y = y.cuda().float().unsqueeze(0)[:, 0, :]\n        y_hat = self.model(x)\n\n        loss = criterion(y_hat, y)\n        score = mcrmse_loss(y_hat, y)\n        self.batch_metrics.update(\n            {\"loss\": loss, 'metric': score}\n        )\n\n        if self.is_train_loader:\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n","459ee110":"device = utils.get_device()","20a7e0f9":"def mcrmse_loss(y_true, y_pred, N=3):\n    \"\"\"\n    Calculates competition eval metric\n    \"\"\"\n    y_true, y_pred = y_true.detach().cpu().numpy(), y_pred.detach().cpu().numpy()\n    assert len(y_true) == len(y_pred)\n    n = len(y_true)\n    return np.sum(np.sqrt(np.sum((y_true - y_pred)**2, axis=0)\/n)) \/ N","0816fcff":"test_dataset = VaxDataset('..\/input\/stanford-covid-vaccine\/test.json', test=True)\ntest_dataloader = DataLoader(test_dataset, 16, num_workers=4, drop_last=False, pin_memory=True)\nscheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,base_lr=1e-3,max_lr=1e-2,step_size_up=2000)\n\nloaders = {\n    'train': train_dataloader,\n}\nrunner = CustomRunner(device=device)\n# model training\nrunner.train(\n    model=model,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=\"..\/working\",\n    num_epochs=6,\n    scheduler=scheduler,\n    verbose=False,\n    load_best_on_end=True,\n    \n)","cce1e116":"utils.plot_metrics(\n    logdir=\"..\/working\", \n    # specify which metrics we want to plot\n    metrics=[\"loss\", \"metric\"]\n)","adce122f":"sub = pd.read_csv('..\/input\/stanford-covid-vaccine\/sample_submission.csv', index_col='id_seqpos')\n\nfor predictions, ids in runner.predict_loader(loader=test_dataloader):\n    sub.loc[ids, ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']] = predictions.detach().cpu().numpy()","e7fcdddf":"sub.head()","a9232c0f":"sub.to_csv('submission.csv')","b1afbdaf":"# Submission","ad784b9e":"# [Train, infer] Catalyst + PyTorch RNN Baseline\n\n![](https:\/\/raw.githubusercontent.com\/catalyst-team\/catalyst-pics\/master\/pics\/catalyst_logo.png)\n\nThis is mainly a RNN modification of MatthewMasters' CNN baseline (do have a look there and upvote it). The RNN model gives considerable improvements in CV over a CNN, and it seems the same can be said for TensorFlow given Xhlulu's brilliant kernel. This also adds a small attention component (giving 0.12 boost in local CV) but it makes the loss and metric score go yo-yo for a bit.\n\nNow the main part of this notebook is to demonstrate how Catalyst simplifies your training loop in a few ways:-\n+ Makes it much easier to train with PyTorch\n+ Inference too gets simplified drastically.","854729bd":"# Setup Model and Data Processing","53db1c95":"# Imports and helpers","910c2da1":"This involves defining a few basic functions: one-hot encoding, feature engineering and typical preprocessing functions.","56f57b5f":"Check a few of the results post-training.","c4ee2d84":"# Training loop","f448ccac":"Typical Data science\/machine learning stack for Torch with the addition of `catalyst`."}}