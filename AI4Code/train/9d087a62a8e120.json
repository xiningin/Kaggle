{"cell_type":{"12ae5fce":"code","ffc25479":"code","5ebc045e":"code","658dedf9":"code","c364e0e2":"code","0118d431":"code","0014107f":"code","ca6e8c69":"code","90ecd63c":"code","fdcd680a":"code","9a45bec2":"code","814d5c0e":"code","edd8fee2":"code","3b54f5a0":"code","fa6d7b1d":"code","e022880c":"code","9591c7c3":"code","5a57753b":"code","2b6058bd":"code","85b04fda":"code","526db098":"code","d6ba6dd5":"code","fb664f65":"code","b3afb1de":"code","4e763211":"code","c9135e2d":"code","db81d385":"code","8e8d31dd":"code","d189637f":"code","33cae1a3":"code","b773b09b":"code","062b4f8d":"code","1de4bffe":"code","b0f24ef0":"code","8ee3db69":"code","18cca42e":"code","1b375900":"code","27fb1801":"code","cbe4f8e7":"code","093d93df":"code","4f8d8c21":"code","c8fd3a78":"code","740fbb51":"code","7e457800":"code","14d2cc62":"code","5a343420":"code","864f4e7e":"code","2ab93bd0":"code","be4e7842":"markdown","33f9eb36":"markdown","7bf616f1":"markdown","55094b2c":"markdown","8ae7a0b5":"markdown","32489cf4":"markdown","249efb19":"markdown","dc611917":"markdown","a88a2702":"markdown","f6e15964":"markdown","5cbbad87":"markdown","23239d87":"markdown","88656b80":"markdown","cf840a25":"markdown","2d59ac55":"markdown","4aa39408":"markdown","229c9d8e":"markdown","32631a65":"markdown","c3305e02":"markdown","33f28087":"markdown","8d6d25e1":"markdown","d4d2d818":"markdown","9b15e140":"markdown","ef3b5bc5":"markdown","61ba4444":"markdown","06b9fab0":"markdown","4c620a02":"markdown","37ab0848":"markdown","6d9a3cb4":"markdown","018b232f":"markdown","78cb973e":"markdown","20ce85f5":"markdown","aa85c31c":"markdown"},"source":{"12ae5fce":"import os\nimport cv2\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport json\nimport math\nimport PIL\nfrom PIL import ImageOps\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Flatten, Activation, Dropout, GlobalAveragePooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers, applications\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\nfrom keras import backend as K \nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport keras\n\nfrom tqdm.auto import tqdm\ntqdm.pandas()","ffc25479":"!ls -l ..\/input\/","5ebc045e":"!ls -l ..\/input\/aptos2019-blindness-detection\/train_images | wc -l","658dedf9":"!ls -l ..\/input\/aptos2019-blindness-detection\/test_images | wc -l","c364e0e2":"!ls -l ..\/input\/retinopathy-train-2015\/rescaled_train_896\/rescaled_train_896 | wc -l","0118d431":"train_path_2015 = \"..\/input\/retinopathy-train-2015\/rescaled_train_896\/rescaled_train_896\/\"\ntrain_path = \"..\/input\/aptos2019-blindness-detection\/train_images\/\"\ntest_path = \"..\/input\/aptos2019-blindness-detection\/test_imges\/\"\n","0014107f":"df_train = pd.read_csv(\"..\/input\/aptos2019-blindness-detection\/train.csv\")\ndf_train.head()","ca6e8c69":"df_test = pd.read_csv(\"..\/input\/aptos2019-blindness-detection\/test.csv\")\ndf_test.head()","90ecd63c":"df_train_2015 = pd.read_csv(\"..\/input\/retinopathy-train-2015\/rescaled_train_896\/trainLabels.csv\")\ndf_train_2015.head()","fdcd680a":"!ls -lU ..\/input\/aptos2019-blindness-detection\/train_images\/ | head -10","9a45bec2":"!ls -lU ..\/input\/retinopathy-train-2015\/rescaled_train_896\/rescaled_train_896 | head -10","814d5c0e":"n_rows = df_train.shape[0]\nn_rows","edd8fee2":"df_train[\"filename\"] = df_train[\"id_code\"]+\".png\"\ndf_train[\"path\"] = [train_path]*n_rows\n#the year is just to be able to easily separate the past and present datasets later\ndf_train[\"year\"] = [2019]*n_rows\ndf_train.head()","3b54f5a0":"n_rows_2015 = df_train_2015.shape[0]\nn_rows_2015","fa6d7b1d":"df_train_2015[\"filename\"] = df_train_2015[\"image\"]+\".png\"\ndf_train_2015[\"path\"] = [train_path_2015]*n_rows_2015\ndf_train_2015[\"year\"] = [2015]*n_rows_2015\ndf_train_2015.head()","e022880c":"df_train_2015.columns = [\"id_code\", \"diagnosis\", \"filename\", \"path\", \"year\"]\ndf_train_2015.head()","9591c7c3":"df_train_all = pd.concat([df_train,df_train_2015], axis=0, sort=False).reset_index()\ndf_train_all.head()","5a57753b":"df_train_all.tail()","2b6058bd":"#replacing df_train with the full set to calculate features and do visualizations all at once, keeping the original (present) just in case\ndf_train_orig = df_train\ndf_train = df_train_all","85b04fda":"%%time\nimg_sizes = []\nwidths = []\nheights = []\naspect_ratios = []\n\nfor index, row in tqdm(df_train.iterrows(), total=df_train.shape[0]):\n    filename = row[\"filename\"]\n    path = row[\"path\"]\n    img_path = os.path.join(path, filename)\n    with open(img_path, 'rb') as f:\n        img = PIL.Image.open(f)\n        img_size = img.size\n        img_sizes.append(img_size)\n        widths.append(img_size[0])\n        heights.append(img_size[1])\n        aspect_ratios.append(img_size[0]\/img_size[1])\n\ndf_train[\"width\"] = widths\ndf_train[\"height\"] = heights\ndf_train[\"aspect_ratio\"] = aspect_ratios\ndf_train[\"size\"] = img_sizes","526db098":"df_train.head()","d6ba6dd5":"df_sorted = df_train.sort_values(by=\"aspect_ratio\")","fb664f65":"df_sorted.head()","b3afb1de":"df_sorted[df_sorted[\"year\"] == 2015].head()","4e763211":"df_sorted[df_sorted[\"year\"] == 2019].head()","c9135e2d":"df_sorted.tail()","db81d385":"df_sorted[df_sorted[\"year\"] == 2015].tail()","8e8d31dd":"df_sorted[df_sorted[\"year\"] == 2019].tail()","d189637f":"#This just shows a single image in the notebook\ndef show_img(filename, path):\n        img = PIL.Image.open(f\"{path}\/{filename}\")\n        npa = np.array(img)\n        print(npa.shape)\n        #https:\/\/stackoverflow.com\/questions\/35902302\/discarding-alpha-channel-from-images-stored-as-numpy-arrays\n#        npa3 = npa[ :, :, :3]\n        print(filename)\n        plt.imshow(npa)\n","33cae1a3":"import matplotlib\n\nfont = {'family' : 'normal',\n        'weight' : 'normal',\n        'size'   : 22}\n\nmatplotlib.rc('font', **font)","b773b09b":"row = df_sorted[df_sorted[\"year\"] == 2019].iloc[0]\nshow_img(row.filename, row.path)","062b4f8d":"row = df_sorted[df_sorted[\"year\"] == 2015].iloc[0]\nshow_img(row.filename, row.path)","1de4bffe":"def plot_first_9(df_to_plot):\n    plt.figure(figsize=[30,30])\n    for x in range(9):\n        path = df_to_plot.iloc[x].path\n        filename = df_to_plot.iloc[x].filename\n        img = PIL.Image.open(f\"{path}\/{filename}\")\n        print(filename)\n        plt.subplot(3, 3, x+1)\n        plt.imshow(img)\n        title_str = filename+\", diagnosis: \"+str(df_to_plot.iloc[x].diagnosis)\n        plt.title(title_str)","b0f24ef0":"del df_sorted\ndf_sorted = df_train.sort_values(by=\"aspect_ratio\", ascending=True)","8ee3db69":"plot_first_9(df_sorted[df_sorted[\"year\"] == 2019])","18cca42e":"plot_first_9(df_sorted[df_sorted[\"year\"] == 2015])","1b375900":"del df_sorted\ndf_sorted = df_train.sort_values(by=\"aspect_ratio\", ascending=False)","27fb1801":"plot_first_9(df_sorted[df_sorted[\"year\"] == 2019])","cbe4f8e7":"plot_first_9(df_sorted[df_sorted[\"year\"] == 2015])","093d93df":"del df_sorted\ndf_sorted = df_train.sort_values(by=\"diagnosis\", ascending=False)\ndf_sorted.head()","4f8d8c21":"plot_first_9(df_sorted[df_sorted[\"year\"] == 2019])","c8fd3a78":"plot_first_9(df_sorted[df_sorted[\"year\"] == 2015])","740fbb51":"del df_sorted\ndf_sorted = df_train.sort_values(by=\"diagnosis\", ascending=True)\ndf_sorted.head()","7e457800":"plot_first_9(df_sorted[df_sorted[\"year\"] == 2019])","14d2cc62":"plot_first_9(df_sorted[df_sorted[\"year\"] == 2015])","5a343420":"df_train.describe()","864f4e7e":"df_sorted = df_train.sort_values(by=\"width\", ascending=True)\n\nplot_first_9(df_sorted[df_sorted[\"year\"] == 2019])","2ab93bd0":"\nplot_first_9(df_sorted[df_sorted[\"year\"] == 2015])","be4e7842":"### Present","33f9eb36":"## Smallest Aspect Ratio\n\nThere seem to be no images with aspect ratio < 1, so plotting the smallest aspect ratios (practically the ratio is then 1) should show the most \"square\" images:","7bf616f1":"# Look at the Images \/ Eyes","55094b2c":"## Collect all metadata to single dataframe(s)","8ae7a0b5":"## Basic metadata","32489cf4":"### Present","249efb19":"Are the smallest files still valid files?","dc611917":"## Highest Aspect Ratios\n\nThis should be the ones least \"square\":","a88a2702":"I guess the healthier ones look more \"clean\".","f6e15964":"## A Random Eye\n\nVisualize the first image in past and present sets to see if they are at all alike:\n","5cbbad87":"Generally, the past vs present images seem very similar. Some color differences, although some of the later pics will show both have these more \"orange\" and \"greenish\" ones as well. But a deeper investigation of how the color spaces are distributed in different sets could be interesting.","23239d87":"## Calculate Aspect Ratios etc.","88656b80":"First 10 un-ordered files in past and present training sets to see the filenames match the csv columns (\"id_code\" and \"image\"):","cf840a25":"### Past","2d59ac55":"## Aspect Ratios\n\nSee that there are no images that are hugely different in size to others:","4aa39408":"# Introduction\n\nThis kernel is just a quick look at the training dataset image sizes, and a look at some of the images at the lowest and highest diagnosis levels. To see if there is something easily visible to understand what the doctor might be looking at in a classification.\n\nThere is also a [previous competition](https:\/\/www.kaggle.com\/c\/diabetic-retinopathy-detection) on the same topic, with the exact same training labels. It seems to have a much larger training dataset. This set was mentioned multiple times in the [external data thread](). I had trouble adding that competition as a data source (error about loading the data). So I downloaded the data and set it up as a [separate dataset](https:\/\/www.kaggle.com\/donkeys\/retinopathy-train-2015). Had to downscale it quite a bit to max 896x896 pixel sizes, to fit it into the 20GB dataset size limit. But it seems potentially useful.\n\nI am not quite sure how to check exact date of some old competition here on Kaggle, so I just picket the number of years it displays in the past, and went with 2015. So I will call the older set the *2015* set here. Or the *past* set vs the actual current set for the *present* time.\n","229c9d8e":"It's a match.","32631a65":"# Final Size Statistics\n\nOn average, are the files about the same size? Actually might make sense to look at the past and present sets separately since I had to downsize the past significantly. But the idea is there, and it does already show if there are some really small ones..","c3305e02":"### Present","33f28087":"### Past","8d6d25e1":"## 9-Eyes\n\nVisualize 9 images from a set at a time, to learn a bit more about the set at once.","d4d2d818":"The aspect ratios in the past and present seem very close to each other.","9b15e140":"# Conclusions\n\nThe images from both sets seem to be quite similar. Possibly some color differences and other minor differences?","ef3b5bc5":"### Past","61ba4444":"### Past","06b9fab0":"### Lowest \/ Healthiest Diagnosis:","4c620a02":"## Diagnosis Values\n\nA look at the highest vs lowest diagnosis values \/levels given in the training set. Can we spot some differences? \n\n### Highest \/ Most Severe Diagnosis:","37ab0848":"### Past","6d9a3cb4":"## Number of files in train vs test vs the 2015 training set","018b232f":"### Present","78cb973e":"### Present","20ce85f5":"### Past","aa85c31c":"### Present"}}