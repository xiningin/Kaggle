{"cell_type":{"33e426fe":"code","f514f5fd":"code","ee6b6142":"code","9a233b78":"code","cbc7a8ea":"code","8fb6187a":"code","1fab5716":"code","64fd2b97":"code","3a36a065":"code","cc786bd0":"code","37d5f7f1":"code","c9c6feb4":"code","5c1a9445":"code","2a33acc0":"code","76fa6d39":"code","6c8e650a":"code","97a2990e":"code","906667f9":"code","bc46d2aa":"code","15a205bf":"code","91f71803":"code","86872c43":"code","136d0779":"code","ae9b9fbc":"code","5454853a":"code","a6346d54":"code","e28ef663":"code","415c4bb3":"code","4411114b":"markdown","cf9a32b9":"markdown","df21076a":"markdown","be83e5ec":"markdown","aec7dec4":"markdown","04b5a5bb":"markdown","446ed1cb":"markdown","e04979a4":"markdown","07e0397f":"markdown","123f2af9":"markdown","45836c70":"markdown","18761dbc":"markdown","76957fbc":"markdown","9d667d3b":"markdown","66c954c7":"markdown","efa67de3":"markdown","c9c8d5a0":"markdown","df0358af":"markdown","3d0dadbd":"markdown","d3451979":"markdown","67f765bf":"markdown","59e2b598":"markdown"},"source":{"33e426fe":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nimport seaborn as sns\nimport wandb\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\n\n\n#ignore warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\n","f514f5fd":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"api_key\")\n    wandb.login(key=secret_value_0)\n    anony=None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')\n    \nCONFIG = dict(competition = 'TPSJan2022',_wandb_kernel = 'tensorgirl')","ee6b6142":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\", parse_dates=True)\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\", index_col=0, parse_dates=True)\n","9a233b78":"train, val = np.split(train.sample(frac=1), [int(0.8*len(train))])\ntrain['date'] = pd.to_datetime(train['date'])\nval['date'] = pd.to_datetime(val['date'])\ntest['date'] = pd.to_datetime(test['date'])","cbc7a8ea":"# code copied from https:\/\/www.kaggle.com\/vad13irt\/tps-jan-2022-exploratory-data-analysis?scriptVersionId=84140623&cellId=11\ndef hide_spines(ax, spines=[\"top\", \"right\", \"bottom\", \"left\"]):\n    for spine in spines:\n        ax.spines[spine].set_visible(False)\n        \nchart_colors = [\"#2a9d8f\",\"#ff355d\", \"#E4916C\"]\nsns.palplot(chart_colors)\nchart_colors1 = [\"#2a9d8f\",\"#ff355d\"]\nsns.palplot(chart_colors1)","8fb6187a":"# code copied from https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-01-eda-which-makes-sense?scriptVersionId=84561837&cellId=16\n\nplt.figure(figsize=(18, 12))\nfor i, (combi, df) in enumerate(train.groupby(['country', 'store', 'product'])):\n    ax = plt.subplot(6, 3, i+1, ymargin=0.5)\n    ax.hist(train.num_sold, bins=50, color='#2a9d8f')\n    #ax.set_xscale('log')\n    ax.set_title(combi)\nplt.suptitle('Histograms of num_sold', y=1.03)\nplt.tight_layout(h_pad=3.0)\nplt.show()","1fab5716":"# code copied from https:\/\/www.kaggle.com\/vad13irt\/tps-jan-2022-exploratory-data-analysis?scriptVersionId=84140623&cellId=21\n\nfig = plt.figure(figsize=(25, 7))\nfig.set_facecolor(\"#fff\")\nax = fig.add_subplot()\nax.set_facecolor(\"#fff\")\nax.grid(color=\"lightgrey\", alpha=0.7, linewidth=1, axis=\"both\", zorder=0)\nsns.lineplot(x=\"date\", y=\"num_sold\", color=\"#2a9d8f\", err_style=None, data=train, linewidth=1, ax=ax, zorder=2)\nax.yaxis.set_tick_params(color=\"#000\", labelsize=12, pad=5, length=0)\nax.set_ylabel(\"Num Sold\", fontsize=15, fontfamily=\"serif\", labelpad=10)\nax.set_xlabel(\"Date\", fontsize=15, fontfamily=\"serif\", labelpad=10)\nax.xaxis.set_tick_params(color=\"#000\", labelsize=12, pad=5, length=0)\nax.yaxis.set_tick_params(color=\"#000\", labelsize=12, pad=5, length=0)\nax.set_title(\"Number of sales\", loc=\"left\", color=\"#000\", fontsize=25, pad=5, fontweight=\"bold\", fontfamily=\"serif\", y=1.05, zorder=3)\nhide_spines(ax)\nfig.show()","64fd2b97":"# code copied from https:\/\/www.kaggle.com\/vad13irt\/tps-jan-2022-exploratory-data-analysis?scriptVersionId=84140623&cellId=23\n\nfig = plt.figure(figsize=(25, 7))\nfig.set_facecolor(\"#fff\")\nax = fig.add_subplot()\nax.set_facecolor(\"#fff\")\nax.grid(color=\"lightgrey\", alpha=0.7, linewidth=1, axis=\"both\", zorder=0)\nsns.lineplot(x=\"date\", y=\"num_sold\", hue=\"country\", color=\"#FECD00\",palette=chart_colors, data=train, err_style=None, linewidth=1, ax=ax, zorder=2)\nax.yaxis.set_tick_params(color=\"#000\", labelsize=12, pad=5, length=0)\nax.set_ylabel(\"Num Sold\", fontsize=15, fontfamily=\"serif\", labelpad=10)\nax.set_xlabel(\"Date\", fontsize=15, fontfamily=\"serif\", labelpad=10)\nax.xaxis.set_tick_params(color=\"#000\", labelsize=12, pad=5, length=0)\nax.yaxis.set_tick_params(color=\"#000\", labelsize=12, pad=5, length=0)\nax.set_title(\"Countries vs Number of sales\", loc=\"left\", color=\"#000\", fontsize=25, pad=5, fontweight=\"bold\", fontfamily=\"serif\", y=1.05, zorder=3)\nhide_spines(ax)\nax.legend(loc=\"upper right\", ncol=3, fontsize=15, edgecolor=None, facecolor=None, markerscale=2, labelcolor=\"#000\", handlelength=1, title=None)\nfig.show()","3a36a065":"# code copied from https:\/\/www.kaggle.com\/vad13irt\/tps-jan-2022-exploratory-data-analysis?scriptVersionId=84140623&cellId=25\n\nfig = plt.figure(figsize=(25, 7))\nfig.set_facecolor(\"#fff\")\nax = fig.add_subplot()\nax.set_facecolor(\"#fff\")\nax.grid(color=\"lightgrey\", alpha=0.7, linewidth=1, axis=\"both\", zorder=0)\nsns.lineplot(x=\"date\", y=\"num_sold\", data=train, hue=\"store\",palette=chart_colors1, err_style=None, linewidth=1, ax=ax, zorder=2)\nax.yaxis.set_tick_params(color=\"#000\", labelsize=12, pad=5, length=0)\nax.set_ylabel(\"Num Sold\", fontsize=15, fontfamily=\"serif\", labelpad=10)\nax.set_xlabel(\"Date\", fontsize=15, fontfamily=\"serif\", labelpad=10)\nax.xaxis.set_tick_params(color=\"#000\", labelsize=12, pad=5, length=0)\nax.yaxis.set_tick_params(color=\"#000\", labelsize=12, pad=5, length=0)\nax.set_title(\"Stores vs Number of sales\", loc=\"left\", color=\"#000\", fontsize=25, pad=5, fontweight=\"bold\", fontfamily=\"serif\", y=1.05, zorder=3)\nhide_spines(ax)\nax.legend(loc=\"upper right\", ncol=3, fontsize=15, edgecolor=None, facecolor=None, markerscale=2, labelcolor=\"#000\", handlelength=1, title=None)\nfig.show()","cc786bd0":"# code copied from https:\/\/www.kaggle.com\/vad13irt\/tps-jan-2022-exploratory-data-analysis?scriptVersionId=84140623&cellId=27\n\nfig = plt.figure(figsize=(25, 7))\nfig.set_facecolor(\"#fff\")\nax = fig.add_subplot()\nax.set_facecolor(\"#fff\")\nax.grid(color=\"lightgrey\", alpha=0.7, linewidth=1, axis=\"both\", zorder=0)\nsns.lineplot(x=\"date\", y=\"num_sold\", data=train, hue=\"product\",palette=chart_colors, err_style=None, linewidth=1, ax=ax, zorder=2)\nax.yaxis.set_tick_params(color=\"#000\", labelsize=12, pad=5, length=0)\nax.set_ylabel(\"Num Sold\", fontsize=15, fontfamily=\"serif\", labelpad=10)\nax.set_xlabel(\"Date\", fontsize=15, fontfamily=\"serif\", labelpad=10)\nax.xaxis.set_tick_params(color=\"#000\", labelsize=12, pad=5, length=0)\nax.yaxis.set_tick_params(color=\"#000\", labelsize=12, pad=5, length=0)\nax.set_title(\"Products vs Number of sales\", loc=\"left\", color=\"#000\", fontsize=25, pad=5, fontweight=\"bold\", fontfamily=\"serif\", y=1.05, zorder=3)\nhide_spines(ax)\nax.legend(loc=\"upper right\", ncol=3, fontsize=15, edgecolor=None, facecolor=None, markerscale=2, labelcolor=\"#000\", handlelength=1, title=None)\nfig.show()","37d5f7f1":"# code copied from https:\/\/www.kaggle.com\/subinium\/tps-jan-happy-new-year?scriptVersionId=84186421&cellId=22\n\nfig, ax = plt.subplots(1, 1, figsize=(25, 7))\ntrain_monthly = train.set_index('date').groupby([pd.Grouper(freq='M')])[['num_sold']].mean()\n\nsns.lineplot(x=\"date\", y=\"num_sold\", data=train, ax=ax, label='daily',color= '#2a9d8f')\nsns.lineplot(x=\"date\", y=\"num_sold\", data=train_monthly, ax=ax, label='monthly mean', color='black')\nax.set_title('Monthly Trend', fontsize=20, fontweight='bold', loc='left', y=1.03)\nax.grid(alpha=0.5)\nhide_spines(ax)\nax.legend()\nplt.show()","c9c6feb4":"def create_features(df):\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['weekday'] = df['date'].dt.weekday   \n    df['weekend'] = (df['date'].dt.weekday>=5).astype(int)   \n    df.drop(columns=['date'], inplace=True)\n    \ncreate_features(train)\ncreate_features(val)\ncreate_features(test)","5c1a9445":"train = train.drop('row_id',axis =1)\nval = val.drop('row_id',axis =1)","2a33acc0":"# Save train data to W&B Artifacts\ntrain.to_csv(\"train_wandb.csv\", index = False)\nrun = wandb.init(project='TPSJan2022', name='training_data', anonymous=anony,config=CONFIG) \nartifact = wandb.Artifact(name='training_data',type='dataset')\nartifact.add_file(\".\/train_wandb.csv\")\n\nwandb.log_artifact(artifact)\nwandb.finish()","76fa6d39":"def df_to_dataset(dataframe, shuffle=True, batch_size=32 , train = 1):\n  df = dataframe.copy()\n  labels = df.pop('num_sold')\n  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(dataframe))\n  ds = ds.batch(batch_size)\n  ds = ds.prefetch(batch_size)\n  return ds","6c8e650a":"batch_size = 5\ntrain_ds = df_to_dataset(train, batch_size=batch_size )\nval_ds = df_to_dataset(val, batch_size=batch_size )\ntest_ds = tf.data.Dataset.from_tensor_slices(dict(test))","97a2990e":"[(train_features, label_batch)] = train_ds.take(1)\nprint('Every feature:', list(train_features.keys()))\nfor feature in list(train_features.keys()):\n    print(train_features[feature])\nprint('A batch of targets:', label_batch )","906667f9":"def get_normalization_layer(name, dataset):\n  # Create a Normalization layer for the feature.\n  normalizer = tf.keras.layers.Normalization(axis=None)\n\n  # Prepare a Dataset that only yields the feature.\n  feature_ds = dataset.map(lambda x, y: x[name])\n\n  # Learn the statistics of the data.\n  normalizer.adapt(feature_ds)\n\n  return normalizer","bc46d2aa":"all_inputs = []\nencoded_features = []\n\n# Numerical features.\nfor header in ['month', 'year','weekday','weekend']:\n  numeric_col = tf.keras.Input(shape=(1,), name=header)\n  normalization_layer = get_normalization_layer(header, train_ds)\n  encoded_numeric_col = normalization_layer(numeric_col)\n  all_inputs.append(numeric_col)\n  encoded_features.append(encoded_numeric_col)","15a205bf":"def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n  # Create a layer that turns strings into integer indices.\n  if dtype == 'string':\n    index = tf.keras.layers.StringLookup(max_tokens=max_tokens)\n  # Otherwise, create a layer that turns integer values into integer indices.\n  else:\n    index = tf.keras.layers.IntegerLookup(max_tokens=max_tokens)\n\n  # Prepare a `tf.data.Dataset` that only yields the feature.\n  feature_ds = dataset.map(lambda x, y: x[name])\n\n  # Learn the set of possible values and assign them a fixed integer index.\n  index.adapt(feature_ds)\n\n  # Encode the integer indices.\n  encoder = tf.keras.layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n\n  # Apply multi-hot encoding to the indices. The lambda function captures the\n  # layer, so you can use them, or include them in the Keras Functional model later.\n  return lambda feature: encoder(index(feature))","91f71803":"\ncategorical_cols = ['country','store','product']\nfor feature in categorical_cols:\n  categorical_col = tf.keras.Input(shape=(1,), name=feature, dtype='string')\n  encoding_layer = get_category_encoding_layer(name=feature,\n                                               dataset=train_ds,\n                                               dtype='string',\n                                               max_tokens=5)\n  encoded_categorical_col = encoding_layer(categorical_col)\n  all_inputs.append(categorical_col)\n  encoded_features.append(encoded_categorical_col)","86872c43":"all_features = tf.keras.layers.concatenate(encoded_features)\nx = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\nx = tf.keras.layers.Dropout(0.5)(x)\noutput = tf.keras.layers.Dense(1)(x)\n\nmodel = tf.keras.Model( all_inputs, output)","136d0779":"def smape(y_true, y_pred):\n   y_true = tf.cast(y_true, tf.float32)\n   y_pred = tf.cast(y_pred, tf.float32)\n   num = tf.math.abs(tf.math.subtract(y_true, y_pred))\n   denom = tf.math.add(tf.math.abs(y_true), tf.math.abs(y_pred))\n   denom = tf.math.divide(denom,200.0)\n\n   val = tf.math.divide(num,denom)\n   val = tf.where(denom == 0.0, 0.0, val)\n        \n   return tf.reduce_mean(val)","ae9b9fbc":"model.compile(optimizer='rmsprop',loss=smape)","5454853a":"class SMAPE(tf.keras.losses.Loss):\n    def __init__(self,**kwargs):\n        super().__init__(**kwargs)        \n\n    def call(self, y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n        num = tf.math.abs(tf.math.subtract(y_true, y_pred))\n        denom = tf.math.add(tf.math.abs(y_true), tf.math.abs(y_pred))\n        denom = tf.math.divide(denom,200.0)\n\n        val = tf.math.divide(num,denom)\n        val = tf.where(denom == 0.0, 0.0, val)\n        \n        return tf.reduce_mean(val)\n    \n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config}","a6346d54":"model.compile(optimizer='rmsprop',loss=SMAPE())","e28ef663":"# Use `rankdir='LR'` to make the graph horizontal.\ntf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")","415c4bb3":"model.fit(train_ds, epochs=10, validation_data=val_ds)\n","4411114b":"# Work in progress \ud83d\udea7","cf9a32b9":"\ud83d\udccc KaggleRama has higher sales compared to KaggleMart\n\n","df21076a":"\ud83d\udccc KaggleHat has the highest sales followed by KaggleMug and KaggleStickers","be83e5ec":"## **<span style=\"color:#e76f51;\">tf.data.Dataset.take()<\/span>**","aec7dec4":"\ud83d\udccc Norway has the highest sales followed by Sweden and Finland\n","04b5a5bb":"Prebuilt layers can be mixed and matched with custom layers and other tensorflow functions. Preprocessing can be split from training and applied efficiently with tf.data, and joined later for inference.","446ed1cb":"# **<span style=\"color:#e76f51;\">Custom Loss Functions<\/span>**\n\nCustom Loss can be implemented by using functions and the function name has to be passed as value to the loss parameter in the compile() method .","e04979a4":"# **<span style=\"color:#e76f51;\">Exploratory Data Analysis<\/span>**\n\n[Source1](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-01-eda-which-makes-sense) [Source2](https:\/\/www.kaggle.com\/vad13irt\/tps-jan-2022-exploratory-data-analysis)\n\n\ud83d\udccc Histograms in the below graph are skewed with outliers . Hence choosing log(num_sold over num_sold is preferred .\n\n","07e0397f":"# **<span style=\"color:#e76f51;\">Preprocessing<\/span>**\n\nThe training dataset is divided in to training and validation dataset . The date column is converted to pandas datetime object .","123f2af9":"![](https:\/\/drive.google.com\/uc?id=1rDqk8wCX9zJXOvyqSi6e5exmz_x03Ji4)\n\nModels can be created using one of the following API\n\n`Keras Sequential API`\n\n`Keras Functional API`\n\n`Model Subclassing`\n\nIn this tutorial lets explore the usage of Keras Functional API\n\n\n# **<span style=\"color:#e76f51;\">Keras Functional API<\/span>**\n\nThe Keras Functional API gives users more flexibility in model creation by allowing shared layers , non -linear topology and multiple input and output layers. The functional API can be used to be build a graph of layers.\n\n","45836c70":"## **<span style=\"color:#e76f51;\">Feature representation using Keras Preprocessing Layers<\/span>**\n\nFeature representations can be one of the crucial aspect in model developement workflows . It is a experimental process and there is no perfect solution . Keras preprocessing Layers helps us create more flexible preprocessing pipeline where new data transformations can be applied while changing the model architecture .\n\n![](https:\/\/drive.google.com\/uc?id=1248y8JYTwjnxZnIEaTQHr1xV5jUZotLm)\n\n[ImageSource](https:\/\/blog.tensorflow.org\/2021\/11\/an-introduction-to-keras-preprocessing.html)\n\n## **<span style=\"color:#e76f51;\">Keras Preprocessing Layers - Numerical Features<\/span>**\n\nThe Keras preprocessing layers available for numerical features are below \n\n`tf.keras.layers.Normalization`: performs feature-wise normalization of input features.\n  \n`tf.keras.layers.Discretization`: turns continuous numerical features into integer categorical features.\n\n`adapt():`\n\nAdapt is an optional utility function which helps in setting the internal state of layers from input data . adapt() is available on all stateful processing layerrs and it computes mean and variance for the layerrs and stores them as layers weights . adapt() is called before fit() , evaluate or predict()\n\n\nIn this example , we are going to use tf.keras.layers.Normalization for normalizing numeric input features like month , year , weekday and weekend . This normalization layer shifts and scales inputs to a distribution  centered around 0 with standard deviation 1 by precomputing the mean and variance of the data, and calling (input - mean) \/ sqrt(var) at runtime.\n","18761dbc":"# **<span style=\"color:#e76f51;\">References<\/span>**\n\nhttps:\/\/www.tensorflow.org\/tutorials\/structured_data\/preprocessing_layers#categorical_columns\n\nhttps:\/\/www.kaggle.com\/ambrosm\/tpsjan22-01-eda-which-makes-sense\n\nhttps:\/\/www.kaggle.com\/subinium\/tps-jan-happy-new-year\n\nhttps:\/\/www.kaggle.com\/vad13irt\/tps-jan-2022-exploratory-data-analysis","76957fbc":"\ud83d\udccc Monthly trend in the below graph shows the seasonal variations in sales across products","9d667d3b":"The snapshot of the artifact created is below\n\n![](https:\/\/drive.google.com\/uc?id=16biHK189-q2mhyZAhE-cAvxHb3BIAfFq)","66c954c7":"## **<span style=\"color:#e76f51;\">Keras Preprocessing Layers - Categorical Features<\/span>**\n\nThe various keras preprocessing layers available for categorical variables are below .\n\n`tf.keras.layers.CategoryEncoding:` turns integer categorical features into one-hot, multi-hot, or count dense representations.\n\n`tf.keras.layers.Hashing:` performs categorical feature hashing, also known as the \"hashing trick\".\n\n`tf.keras.layers.StringLookup:` turns string categorical values an encoded representation that can be read by an Embedding layer or Dense layer.\n\n`tf.keras.layers.IntegerLookup:` turns integer categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.","efa67de3":"\n[Source](https:\/\/www.tensorflow.org\/guide\/data)\n\n# **<span style=\"color:#e76f51;\">\ud83c\udfaftf.data<\/span>**\n\ntf.data API is used for building efficient input pipelines which can handle large amounts of data and perform complex data transformations . tf.data API has provisions for handling different data formats .\n\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data.png\" \/>\n\n[Image Source](https:\/\/www.kaggle.com\/jalammar\/intro-to-data-input-pipelines-with-tf-data)\n\nData source is essential for building any input pipeline and tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices can be used to construct a dataset from data in memory .The recommended format for the iput data stored in file is TFRecord which can be created using TFRecordDataset() .The different data source formats supported are numpy arrays , python generators , csv files ,image , TFRecords , csv and text files. \n\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data-read-data.png\" \/>\n\n[Image Source](https:\/\/www.kaggle.com\/jalammar\/intro-to-data-input-pipelines-with-tf-data)\n\nConstruction of tf.data input pipeline consists of three phases namely Extract , Transform and Load . The extraction involves the loading of data from different file format and converting it in to tf.data.Dataset object .\n\n## **<span style=\"color:#e76f51;\">\ud83c\udfaftf.data.Dataset<\/span>**\n\ntf.data.Dataset is an abstraction introduced by tf.data API and consists of sequence of elements where each element has one or more components . For example , in a tabular data pipeline , an element might be a single training example , with a pair of tensor components representing the input features and its label \n\ntf.data.Dataset can be created using two distinct ways\n\nConstructing a dataset using data stored in memory by a data source\n\nConstructing a dataset from one or more tf.data.Dataset objects by a data transformation\n\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data-simple-pipeline.png\" \/>\n\n[Image Source](https:\/\/www.kaggle.com\/jalammar\/intro-to-data-input-pipelines-with-tf-data)\n\n\nBasic input data pipeline constructed using tf.data API consists of the following steps .\n\n\ud83d\udccc Reading input data\n\n\ud83d\udccc Processing multiple epochs using **Dataset.repeat()**\n\n\ud83d\udccc Randomly shuffling using **Dataset.shuffle()**\n\n\ud83d\udccc Batching dataset elements using **Dataset.batch()**\n\nAdditionally preprocessing of dataset can be done using **Dataset.map()** transformation .\n\n\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data-pipeline-1.png\" \/>\n\n[Image Source](https:\/\/www.kaggle.com\/jalammar\/intro-to-data-input-pipelines-with-tf-data)\n\nIn the first step, tf.data reads the CSV file and creates a Dataset object representing the dataset. If we're to pass this Dataset to the model, it would take one of the rows in each training iteration. It's important to note that the Dataset object does not make these transformations right away -- if the a dataset is 2 TB in size and the CPU tf.data is running on only has 32GBs of RAM available, we'd be in trouble. The Dataset object acknowledges the processing plan and the transformations required, and then applies them when needed on a batch-by-batch basis.\n\n\n## **<span style=\"color:#e76f51;\">Randomly shuffling using Dataset.shuffle()<\/span>**\n\nDataset.shuffle() transformation shuffles the order of elements in the dataset and uniformly chooses the next element from the buffer.\n\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data-pipeline-2.png\" \/>\n\n[Image Source](https:\/\/www.kaggle.com\/jalammar\/intro-to-data-input-pipelines-with-tf-data)\n\n## **<span style=\"color:#e76f51;\">Repeating for several epochs<\/span>**\n\nNow, models are trained over multiple epochs -- with the training dataset being fed to the model in each epoch. So let's tell tf.data that we want to use the Dataset for two epochs. That's done using the repeat() method:\n\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data-pipeline-3.png\" \/>\n\n[Image Source](https:\/\/www.kaggle.com\/jalammar\/intro-to-data-input-pipelines-with-tf-data)\n\nYou can see that we now have double the number of rows -- the first half would be epoch #1 and the second half is epoch number #2.\n\n## **<span style=\"color:#e76f51;\">Creating batches using Dataset.batch()<\/span>**\n\n\nThe dataset can be broken down in to stacks or batches of consecutive elements using Dataset.batch() API\n\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data-pipeline-4.png\" \/>\n\n[Image Source](https:\/\/www.kaggle.com\/jalammar\/intro-to-data-input-pipelines-with-tf-data)\n\n","c9c8d5a0":"`Keras.utils.plot_model` converts a Keras model to dot format and save to a file.\n\n","df0358af":"# **<span style=\"color:#e76f51;\">W & B Artifacts<\/span>**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https:\/\/docs.wandb.ai\/guides\/artifacts)\n\n![](https:\/\/drive.google.com\/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)","3d0dadbd":"\ud83d\udccc The peaks in the below graph indicates lot of sales happens during January .","d3451979":"# **<span style=\"color:#e76f51;\">Feature Engineering<\/span>**\n\nNew features can be created from the date column like month , year , weekend or weekday .","67f765bf":"# **<span style=\"color:#e76f51;\">Custom Loss Functions as Classes<\/span>**\n\nCustom Loss can also be implemented using class and the class name has to be passed as value to loss parameter in compile() method","59e2b598":"![](https:\/\/drive.google.com\/uc?id=1OAEI8ghsx2CITu_vZVu9X9fnINtp_1oO)\n\nFor Tabular Playground Series - Jan 2022 , the problem deals with sales forecasting for two (fictitious) independent store chains selling Kaggle merchandise that want to become the official outlet for all things Kaggle. .\n\n# **<span style=\"color:#e76f51;\">Goal<\/span>**\n \nThe goal is to which of the store chains (KaggleMart or KaggleRama ) would have the best sales going forward\n\n# **<span style=\"color:#e76f51;\">Data<\/span>**\n\n**Training Data**\n\n> - ```train.csv``` -  the training set, which includes the sales data for each date-country-store-item combination.\n> - ```test.csv``` -  the test set; your task is to predict the corresponding item sales for each date-country-store-item combination. Note the Public leaderboard is scored on the first quarter of the test year, and the Private on the remaining.\n> - ```sample_submission.csv``` - a sample submission file in the correct format\n\n# **<span style=\"color:#e76f51;\">Metric<\/span>**\n\nSubmissions are evaluated on SMAPE between forecasts and actual values. SMAPE = 0 when the actual and predicted values are both 0.\n\n\n\ud83c\udfaf The mean absolute percentage error is one of the most commonly used metrics for forecasting . MAPE is expressed as percentage and is scale independent . MAPE is not easily differentiable and asymmetric . MAPE also puts heavy penalties on the negative errors .\n\n\ud83c\udfaf Symmetric Mean Absolute Percentage Error (sMAPE) overcomes the shortcomings of MAPE and has both lower and upper bounds .\n\n\ud83c\udfaf SMAPE is calculated by taking square root of the squared difference between the forecast and the actual value .\n\n### SMAPE = SquareRoot(Squared(F - A))\n\nResources to understand SMAPE in detail are [Source1](https:\/\/www.brightworkresearch.com\/the-problem-with-using-smape-for-forecast-error-measurement\/) [Source2](https:\/\/towardsdatascience.com\/choosing-the-correct-error-metric-mape-vs-smape-5328dec53fac)\n\n\n<img src=\"https:\/\/camo.githubusercontent.com\/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b\/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\">\n\n> I will be integrating W&B for visualizations and logging artifacts!\n> \n> [TPS Jan 2022 Project on W&B Dashboard]\n(https:\/\/wandb.ai\/usharengaraju\/TPSJan2022)\n> \n> - To get the API key, create an account in the [website](https:\/\/wandb.ai\/site) .\n> - Use secrets to use API Keys more securely "}}