{"cell_type":{"9ccb52a7":"code","5e6e3c66":"code","46a0facb":"code","c473aa66":"code","fc9a8f30":"code","9850ce1e":"code","6035f835":"code","9dcb35e6":"code","d1e3e3d7":"code","a26fa765":"code","d51705d0":"code","044de9b0":"code","95a019e4":"code","2350fea6":"code","019e88a0":"code","0edb8857":"code","242d46a2":"code","a5570d8a":"code","7a917ea5":"code","278dbba7":"code","d8cb8171":"code","2d885659":"code","0586e43d":"code","7ba880db":"code","5a6a9b9a":"code","fa180142":"code","a1a15617":"code","b55351fa":"code","08caaba0":"code","66f7799b":"code","1bcf7abe":"code","5c90e1b7":"code","0108cada":"code","51f5171c":"code","0194a86c":"code","940babce":"code","003cb234":"code","3d903aef":"code","23ce4d2a":"code","af483960":"code","93983df9":"code","2b1a1597":"code","9f1c04b2":"code","7d799ace":"code","106a0017":"code","86f87bb6":"code","36697c66":"code","00eff3ef":"code","5c5a6caa":"code","16c02842":"code","6eb6e37f":"code","d3c0c20a":"code","cbbdb147":"code","c5ddcbbf":"code","5ea989ed":"code","f3067039":"code","29b22af3":"code","99b4ff32":"code","0bf1eb65":"code","69496198":"code","cf3f0e3b":"code","b1f4f40f":"code","8bb043af":"code","bd821551":"code","d501fe7d":"code","f0bcfb5a":"code","ce69d4e3":"code","25cf3590":"code","06759d64":"code","3a3404bf":"code","3937b9f7":"code","bb3d2f89":"code","40f46a81":"markdown","10d6162c":"markdown","84480a60":"markdown","100b7ea8":"markdown","8f4823bd":"markdown","d3154737":"markdown","11a7682d":"markdown","005f64da":"markdown","129be6e9":"markdown","9d722085":"markdown","83411ffa":"markdown","da124eea":"markdown","1b757d43":"markdown","0de1c505":"markdown","f0861e3a":"markdown","81d79d2a":"markdown","6e3a04aa":"markdown","85d9abf7":"markdown","c5d557d8":"markdown","a0701b65":"markdown","0619463a":"markdown","9ff84434":"markdown","b231c1ae":"markdown","13ded0f6":"markdown","52a34cf4":"markdown","72d9bcbf":"markdown","36b61b31":"markdown","accc0920":"markdown","0606cbb1":"markdown","c61a58e6":"markdown","740123af":"markdown","dbfbad26":"markdown","83fb07c0":"markdown","5532c34f":"markdown","84e0c23a":"markdown","c15c3e1d":"markdown","eea2abfe":"markdown","7aff229f":"markdown","023ffaae":"markdown","e55a593e":"markdown","973bf0a1":"markdown","b8b08691":"markdown","c2f0a3e4":"markdown","7dfd2268":"markdown","b34d35fa":"markdown","b47864b0":"markdown"},"source":{"9ccb52a7":"# !pip install fastai\n# import libraries\nimport fastai\nfrom fastai import *\nfrom fastai.text import * \nimport pandas as pd\nimport numpy as np\nfrom functools import partial\nimport io\nimport os","5e6e3c66":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.express as px\nimport cufflinks as cf \nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\n\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected=True)  \nimport plotly.figure_factory as ff\nimport random\n","46a0facb":"def random_colors(number_of_colors):\n    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n                 for i in range(number_of_colors)]\n    return color","c473aa66":"df_imdb = pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')","fc9a8f30":"df_imdb.head()","9850ce1e":"df_imdb.columns\n","6035f835":"df_imdb.shape\n","9dcb35e6":"iplot(ff.create_table(df_imdb.dtypes.to_frame().reset_index().round(3)),filename='jupyter-table2')","d1e3e3d7":"df_imdb.isnull().sum()","a26fa765":"msno.bar(df_imdb, color = 'green', figsize = (10,8))","d51705d0":"msno.matrix(df_imdb)","044de9b0":"species_count = df_imdb['sentiment'].value_counts()\ndata = [go.Bar(\n    x = species_count.index,\n    y = species_count.values,\n    marker = dict(color = random_colors(3),line=dict(color='#000000', width=2))\n)]\n\nlayout = go.Layout(\n   {\n      \"title\":\"Sentimental Reviews\",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","95a019e4":"trace = go.Pie(labels = list(df_imdb.sentiment.unique()), values = list(df_imdb.sentiment.value_counts()),\n                            hole = 0.2,\n               marker=dict(colors = random_colors(2), \n                           line=dict(color='#000000', width=2)\n                           ))\ndata = [trace]\nlayout = go.Layout(\n   {\n      \"title\":\"Review Sentiments\",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","2350fea6":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(df_imdb[(df_imdb['sentiment'] != \"positive\")][\"review\"], title=\"Word Cloud for Negative Reviews\")","019e88a0":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(df_imdb[(df_imdb['sentiment'] != \"negative\")][\"review\"], title=\"Word Cloud for Positive Reviews\")","0edb8857":"from collections import defaultdict\ntrain1_df = df_imdb[(df_imdb['sentiment'] != \"negative\")]\ntrain0_df = df_imdb[(df_imdb['sentiment'] != \"positive\")]\n\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ))\n    return trace\n\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"review\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), random_colors(50))\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"review\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), random_colors(50))\n\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of Positive Reviews\", \n                                          \"Frequent words of Negative Reviews\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\niplot(fig)\n","242d46a2":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"review\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), random_colors(50))\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"review\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50),random_colors(50))\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of Positive Reviews\", \n                                          \"Frequent bigrams of Negative Reviews\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\niplot(fig)","a5570d8a":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"review\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), random_colors(50))\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"review\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), random_colors(50))\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04, horizontal_spacing=0.2,\n                          subplot_titles=[\"Frequent trigrams of Positive Reviews\", \n                                          \"Frequent trigrams of Negative Review\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\niplot(fig)","7a917ea5":"import string\n## Number of words in the text ##\ndf_imdb[\"num_words\"] = df_imdb[\"review\"].apply(lambda x: len(str(x).split()))\ndf_imdb[\"num_words\"] = df_imdb[\"review\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ndf_imdb[\"num_unique_words\"] = df_imdb[\"review\"].apply(lambda x: len(set(str(x).split())))\ndf_imdb[\"num_unique_words\"] = df_imdb[\"review\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ndf_imdb[\"num_chars\"] = df_imdb[\"review\"].apply(lambda x: len(str(x)))\ndf_imdb[\"num_chars\"] = df_imdb[\"review\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ndf_imdb[\"num_stopwords\"] = df_imdb[\"review\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ndf_imdb[\"num_stopwords\"] = df_imdb[\"review\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n## Number of punctuations in the text ##\ndf_imdb[\"num_punctuations\"] =df_imdb['review'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ndf_imdb[\"num_punctuations\"] =df_imdb['review'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ndf_imdb[\"num_words_upper\"] = df_imdb[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ndf_imdb[\"num_words_upper\"] = df_imdb[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ndf_imdb[\"num_words_title\"] = df_imdb[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ndf_imdb[\"num_words_title\"] = df_imdb[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ndf_imdb[\"mean_word_len\"] = df_imdb[\"review\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ndf_imdb[\"mean_word_len\"] = df_imdb[\"review\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","278dbba7":"cols = \"num_words\"\ntrace0 = go.Box(\n    name = cols,\n    y = df_imdb[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","d8cb8171":"Present = df_imdb[(df_imdb['sentiment'] != \"negative\")]\nNot_Present = df_imdb[(df_imdb['sentiment'] != \"positive\")]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","2d885659":"fig = go.Figure(\n    data=[go.Histogram(x=df_imdb[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","0586e43d":"fig = px.violin(df_imdb, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","7ba880db":"cols = \"mean_word_len\"\ntrace0 = go.Box(\n    name = cols,\n    y = df_imdb[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","5a6a9b9a":"Present = df_imdb[(df_imdb['sentiment'] != \"negative\")]\nNot_Present = df_imdb[(df_imdb['sentiment'] != \"positive\")]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","fa180142":"fig = go.Figure(\n    data=[go.Histogram(x=df_imdb[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","a1a15617":"fig = px.violin(df_imdb, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","b55351fa":"cols = \"num_words_title\"\ntrace0 = go.Box(\n    name = cols,\n    y = df_imdb[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","08caaba0":"Present = df_imdb[(df_imdb['sentiment'] != \"negative\")]\nNot_Present = df_imdb[(df_imdb['sentiment'] != \"positive\")]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","66f7799b":"fig = go.Figure(\n    data=[go.Histogram(x=df_imdb[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","1bcf7abe":"fig = px.violin(df_imdb, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","5c90e1b7":"cols = \"num_words_upper\"\ntrace0 = go.Box(\n    name = cols,\n    y = df_imdb[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","0108cada":"Present = df_imdb[(df_imdb['sentiment'] != \"negative\")]\nNot_Present = df_imdb[(df_imdb['sentiment'] != \"positive\")]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","51f5171c":"fig = go.Figure(\n    data=[go.Histogram(x=df_imdb[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","0194a86c":"fig = px.violin(df_imdb, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","940babce":"cols = \"num_punctuations\"\ntrace0 = go.Box(\n    name = cols,\n    y = df_imdb[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","003cb234":"Present = df_imdb[(df_imdb['sentiment'] != \"negative\")]\nNot_Present = df_imdb[(df_imdb['sentiment'] != \"positive\")]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","3d903aef":"fig = go.Figure(\n    data=[go.Histogram(x=df_imdb[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","23ce4d2a":"fig = px.violin(df_imdb, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","af483960":"cols = \"num_stopwords\"\ntrace0 = go.Box(\n    name = cols,\n    y = df_imdb[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","93983df9":"Present = df_imdb[(df_imdb['sentiment'] != \"negative\")]\nNot_Present = df_imdb[(df_imdb['sentiment'] != \"positive\")]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","2b1a1597":"fig = go.Figure(\n    data=[go.Histogram(x=df_imdb[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","9f1c04b2":"fig = px.violin(df_imdb, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","7d799ace":"cols = \"num_chars\"\ntrace0 = go.Box(\n    name = cols,\n    y = df_imdb[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","106a0017":"Present = df_imdb[(df_imdb['sentiment'] != \"negative\")]\nNot_Present = df_imdb[(df_imdb['sentiment'] != \"positive\")]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","86f87bb6":"fig = go.Figure(\n    data=[go.Histogram(x=df_imdb[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","36697c66":"fig = px.violin(df_imdb, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","00eff3ef":"cols = \"num_unique_words\"\ntrace0 = go.Box(\n    name = cols,\n    y = df_imdb[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","5c5a6caa":"Present = df_imdb[(df_imdb['sentiment'] != \"negative\")]\nNot_Present = df_imdb[(df_imdb['sentiment'] != \"positive\")]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","16c02842":"fig = go.Figure(\n    data=[go.Histogram(x=df_imdb[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","6eb6e37f":"fig = px.violin(df_imdb, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","d3c0c20a":"EMOTICONS = {\n    u\":\u2011\\)\":\"Happy face or smiley\",\n    u\":\\)\":\"Happy face or smiley\",\n    u\":-\\]\":\"Happy face or smiley\",\n    u\":\\]\":\"Happy face or smiley\",\n    u\":-3\":\"Happy face smiley\",\n    u\":3\":\"Happy face smiley\",\n    u\":->\":\"Happy face smiley\",\n    u\":>\":\"Happy face smiley\",\n    u\"8-\\)\":\"Happy face smiley\",\n    u\":o\\)\":\"Happy face smiley\",\n    u\":-\\}\":\"Happy face smiley\",\n    u\":\\}\":\"Happy face smiley\",\n    u\":-\\)\":\"Happy face smiley\",\n    u\":c\\)\":\"Happy face smiley\",\n    u\":\\^\\)\":\"Happy face smiley\",\n    u\"=\\]\":\"Happy face smiley\",\n    u\"=\\)\":\"Happy face smiley\",\n    u\":\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\":D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n    u\"X\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n    u\":-\\)\\)\":\"Very happy\",\n    u\":\u2011\\(\":\"Frown, sad, andry or pouting\",\n    u\":-\\(\":\"Frown, sad, andry or pouting\",\n    u\":\\(\":\"Frown, sad, andry or pouting\",\n    u\":\u2011c\":\"Frown, sad, andry or pouting\",\n    u\":c\":\"Frown, sad, andry or pouting\",\n    u\":\u2011<\":\"Frown, sad, andry or pouting\",\n    u\":<\":\"Frown, sad, andry or pouting\",\n    u\":\u2011\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\[\":\"Frown, sad, andry or pouting\",\n    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n    u\">:\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\{\":\"Frown, sad, andry or pouting\",\n    u\":@\":\"Frown, sad, andry or pouting\",\n    u\">:\\(\":\"Frown, sad, andry or pouting\",\n    u\":'\u2011\\(\":\"Crying\",\n    u\":'\\(\":\"Crying\",\n    u\":'\u2011\\)\":\"Tears of happiness\",\n    u\":'\\)\":\"Tears of happiness\",\n    u\"D\u2011':\":\"Horror\",\n    u\"D:<\":\"Disgust\",\n    u\"D:\":\"Sadness\",\n    u\"D8\":\"Great dismay\",\n    u\"D;\":\"Great dismay\",\n    u\"D=\":\"Great dismay\",\n    u\"DX\":\"Great dismay\",\n    u\":\u2011O\":\"Surprise\",\n    u\":O\":\"Surprise\",\n    u\":\u2011o\":\"Surprise\",\n    u\":o\":\"Surprise\",\n    u\":-0\":\"Shock\",\n    u\"8\u20110\":\"Yawn\",\n    u\">:O\":\"Yawn\",\n    u\":-\\*\":\"Kiss\",\n    u\":\\*\":\"Kiss\",\n    u\":X\":\"Kiss\",\n    u\";\u2011\\)\":\"Wink or smirk\",\n    u\";\\)\":\"Wink or smirk\",\n    u\"\\*-\\)\":\"Wink or smirk\",\n    u\"\\*\\)\":\"Wink or smirk\",\n    u\";\u2011\\]\":\"Wink or smirk\",\n    u\";\\]\":\"Wink or smirk\",\n    u\";\\^\\)\":\"Wink or smirk\",\n    u\":\u2011,\":\"Wink or smirk\",\n    u\";D\":\"Wink or smirk\",\n    u\":\u2011P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"X\u2011P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u2011\u00de\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u00de\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u2011\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":\u2011\\|\":\"Straight face\",\n    u\":\\|\":\"Straight face\",\n    u\":$\":\"Embarrassed or blushing\",\n    u\":\u2011x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":\u2011#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":\u2011&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\"O:\u2011\\)\":\"Angel, saint or innocent\",\n    u\"O:\\)\":\"Angel, saint or innocent\",\n    u\"0:\u20113\":\"Angel, saint or innocent\",\n    u\"0:3\":\"Angel, saint or innocent\",\n    u\"0:\u2011\\)\":\"Angel, saint or innocent\",\n    u\"0:\\)\":\"Angel, saint or innocent\",\n    u\":\u2011b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n    u\">:\u2011\\)\":\"Evil or devilish\",\n    u\">:\\)\":\"Evil or devilish\",\n    u\"\\}:\u2011\\)\":\"Evil or devilish\",\n    u\"\\}:\\)\":\"Evil or devilish\",\n    u\"3:\u2011\\)\":\"Evil or devilish\",\n    u\"3:\\)\":\"Evil or devilish\",\n    u\">;\\)\":\"Evil or devilish\",\n    u\"\\|;\u2011\\)\":\"Cool\",\n    u\"\\|\u2011O\":\"Bored\",\n    u\":\u2011J\":\"Tongue-in-cheek\",\n    u\"#\u2011\\)\":\"Party all night\",\n    u\"%\u2011\\)\":\"Drunk or confused\",\n    u\"%\\)\":\"Drunk or confused\",\n    u\":-###..\":\"Being sick\",\n    u\":###..\":\"Being sick\",\n    u\"<:\u2011\\|\":\"Dump\",\n    u\"\\(>_<\\)\":\"Troubled\",\n    u\"\\(>_<\\)>\":\"Troubled\",\n    u\"\\(';'\\)\":\"Baby\",\n    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(~_~;\\) \\(\u30fb\\.\u30fb;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-\\)zzz\":\"Sleeping\",\n    u\"\\(\\^_-\\)\":\"Wink\",\n    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n    u\"\\(\\+o\\+\\)\":\"Confused\",\n    u\"\\(o\\|o\\)\":\"Ultraman\",\n    u\"\\^_\\^\":\"Joyful\",\n    u\"\\(\\^_\\^\\)\/\":\"Joyful\",\n    u\"\\(\\^O\\^\\)\uff0f\":\"Joyful\",\n    u\"\\(\\^o\\^\\)\uff0f\":\"Joyful\",\n    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"\\('_'\\)\":\"Sad or Crying\",\n    u\"\\(\/_;\\)\":\"Sad or Crying\",\n    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n    u\"\\(;_;\":\"Sad of Crying\",\n    u\"\\(;_:\\)\":\"Sad or Crying\",\n    u\"\\(;O;\\)\":\"Sad or Crying\",\n    u\"\\(:_;\\)\":\"Sad or Crying\",\n    u\"\\(ToT\\)\":\"Sad or Crying\",\n    u\";_;\":\"Sad or Crying\",\n    u\";-;\":\"Sad or Crying\",\n    u\";n;\":\"Sad or Crying\",\n    u\";;\":\"Sad or Crying\",\n    u\"Q\\.Q\":\"Sad or Crying\",\n    u\"T\\.T\":\"Sad or Crying\",\n    u\"QQ\":\"Sad or Crying\",\n    u\"Q_Q\":\"Sad or Crying\",\n    u\"\\(-\\.-\\)\":\"Shame\",\n    u\"\\(-_-\\)\":\"Shame\",\n    u\"\\(\u4e00\u4e00\\)\":\"Shame\",\n    u\"\\(\uff1b\u4e00_\u4e00\\)\":\"Shame\",\n    u\"\\(=_=\\)\":\"Tired\",\n    u\"\\(=\\^\\\u00b7\\^=\\)\":\"cat\",\n    u\"\\(=\\^\\\u00b7\\\u00b7\\^=\\)\":\"cat\",\n    u\"=_\\^=\t\":\"cat\",\n    u\"\\(\\.\\.\\)\":\"Looking down\",\n    u\"\\(\\._\\.\\)\":\"Looking down\",\n    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n    u\"\\(\\\u30fb\\\u30fb?\":\"Confusion\",\n    u\"\\(?_?\\)\":\"Confusion\",\n    u\">\\^_\\^<\":\"Normal Laugh\",\n    u\"<\\^!\\^>\":\"Normal Laugh\",\n    u\"\\^\/\\^\":\"Normal Laugh\",\n    u\"\\\uff08\\*\\^_\\^\\*\uff09\" :\"Normal Laugh\",\n    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n    u\"\\(\\^\u2014\\^\\\uff09\":\"Normal Laugh\",\n    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n    u\"\\\uff08\\^\u2014\\^\\\uff09\":\"Waving\",\n    u\"\\(;_;\\)\/~~~\":\"Waving\",\n    u\"\\(\\^\\.\\^\\)\/~~~\":\"Waving\",\n    u\"\\(-_-\\)\/~~~ \\($\\\u00b7\\\u00b7\\)\/~~~\":\"Waving\",\n    u\"\\(T_T\\)\/~~~\":\"Waving\",\n    u\"\\(ToT\\)\/~~~\":\"Waving\",\n    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n    u\"\\(\\*_\\*\\)\":\"Amazed\",\n    u\"\\(\\*_\\*;\":\"Amazed\",\n    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n    u'\\(-\"-\\)':\"Worried\",\n    u\"\\(\u30fc\u30fc;\\)\":\"Worried\",\n    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n    u\"\\(\\\uff3e\uff56\\\uff3e\\)\":\"Happy\",\n    u\"\\(\\\uff3e\uff55\\\uff3e\\)\":\"Happy\",\n    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n    u\"\\(\\^O\\^\\)\":\"Happy\",\n    u\"\\(\\^o\\^\\)\":\"Happy\",\n    u\"\\)\\^o\\^\\(\":\"Happy\",\n    u\":O o_O\":\"Surprised\",\n    u\"o_0\":\"Surprised\",\n    u\"o\\.O\":\"Surpised\",\n    u\"\\(o\\.o\\)\":\"Surprised\",\n    u\"oO\":\"Surprised\",\n    u\"\\(\\*\uffe3m\uffe3\\)\":\"Dissatisfied\",\n    u\"\\(\u2018A`\\)\":\"Snubbed or Deflated\"\n}","cbbdb147":"from collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport string\nlemmatizer = WordNetLemmatizer()\ncnt = Counter()\nPUNCT_TO_REMOVE = string.punctuation\nSTOPWORDS = set(stopwords.words('english'))\nn_rare_words = 10\nstemmer = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\n\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndef remove_rarewords(text):\n    \"\"\"custom function to remove the rare words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ndef lemmatize_words(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n\ndef remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', string)\n\ndef remove_emoticons(text):\n    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n    return emoticon_pattern.sub(r'', text)\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ndef remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\ndf_imdb[\"review\"] = df_imdb[\"review\"].str.lower()\ndf_imdb[\"review\"] = df_imdb[\"review\"].apply(lambda text: remove_punctuation(text))\ndf_imdb[\"review\"] = df_imdb[\"review\"].apply(lambda text: remove_stopwords(text))\ndf_imdb[\"review\"] = df_imdb[\"review\"].apply(lambda text: remove_emoji(text))\ndf_imdb[\"review\"] = df_imdb[\"review\"].apply(lambda text: remove_emoticons(text))\ndf_imdb[\"review\"] = df_imdb[\"review\"].apply(lambda text: remove_urls(text))\ndf_imdb[\"review\"] = df_imdb[\"review\"].apply(lambda text: remove_html(text))\n\nfor text in df_imdb[\"review\"].values:\n    for word in text.split():\n        cnt[word] += 1\n    \nFREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n\ndf_imdb[\"review\"] = df_imdb[\"review\"].apply(lambda text: remove_freqwords(text))\n\nRAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\ndf_imdb[\"review\"] = df_imdb[\"review\"].apply(lambda text: remove_rarewords(text))\ndf_imdb[\"review\"] = df_imdb[\"review\"].apply(lambda text: stem_words(text))\ndf_imdb[\"review\"] = df_imdb[\"review\"].apply(lambda text: lemmatize_words(text))","c5ddcbbf":"data_lm = (TextList.from_df(df_imdb)\n           #Inputs: all the text files in path\n            .split_by_rand_pct(0.20)\n           #We randomly split and keep 20% for validation\n            .label_for_lm()           \n           #We want to do a language model so we label accordingly\n            .databunch(bs=128))\ndata_lm.save('tmp_lm')","5ea989ed":"data_lm.show_batch()","f3067039":"# Language model AWD_LSTM\nlearn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)","29b22af3":"print('Model Summary:')\nprint(learn.layer_groups)","99b4ff32":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","0bf1eb65":"learn.fit_one_cycle(10, 1e-2)\nlearn.save('lm_hyper')","69496198":"learn.unfreeze()\nlearn.fit_one_cycle(10, 1e-3)","cf3f0e3b":"learn.save_encoder('ft_enc')\n","b1f4f40f":"data_clas = (TextList.from_df(df_imdb, cols=[\"review\"], vocab=data_lm.vocab)\n             .split_by_rand_pct(0.20)\n             .label_from_df('sentiment')\n             .databunch(bs=128))\n\ndata_clas.save('tmp_class')\n","8bb043af":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n","bd821551":"learn.load_encoder('ft_enc')","d501fe7d":"learn.freeze_to(-1)\nlearn.summary()","f0bcfb5a":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","ce69d4e3":"learn.fit_one_cycle(10, 1e-3)","25cf3590":"learn.save('stage1')","06759d64":"learn.load('stage1')\nlearn.unfreeze()\nlearn.fit_one_cycle(10, slice(5e-3\/2., 5e-3))\nlearn.save('stage2')","3a3404bf":"from fastai.vision import ClassificationInterpretation\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(6,6), dpi=60)","3937b9f7":"interp = TextClassificationInterpretation.from_learner(learn)\ninterp.show_top_losses(10)","bb3d2f89":"learn.export()\nlearn.model_dir = \"\/kaggle\/working\"\nlearn.save(\"stage-1\",return_path=True)","40f46a81":"<a id=\"5\"><\/a>\n<font color=\"blue\" size=+2.5><b>2.2 Library Import<\/b><\/font>","10d6162c":"<a id=\"23\"><\/a>\n<font color=\"blue\" size=+2.5><b>5.3 Save and Load Model<\/b><\/font>\n<br\/>\n","84480a60":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Number of Stopwords <\/b><\/font>","100b7ea8":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> N-gram Analysis <\/b><\/font>\n","8f4823bd":"<a id=\"3\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Exploratory Data Analysis <\/center><\/h2>","d3154737":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Feature Engineering <\/b><\/font>\n\n* Number of words in the text\n* Number of unique words in the text\n* Number of characters in the text\n* Number of stopwords\n* Number of punctuations\n* Number of upper case words\n* Number of title case words\n* Average length of the words","11a7682d":"![image.png](attachment:image.png)","005f64da":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Data Preprocessing and Data Cleaning <\/b><\/font>\n","129be6e9":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Number of Char <\/b><\/font>","9d722085":"<a id=\"9\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.4 Model Summary <\/b><\/font>\n","83411ffa":"<a id=\"14\"><\/a>\n<font color=\"blue\" size=+2.5><b> 4.1 Loading Data For Text Classification <\/b><\/font>\n","da124eea":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Number of Unique words <\/b><\/font>","1b757d43":"<a id=\"3\"><\/a>\n<font color=\"blue\" size=+2.5><b>1.3  What is Language Model ?<\/b><\/font>\n<br\/>\n<br\/>\n\n![image.png](attachment:image.png)\n**A language model learns to predict the probability of a sequence of words. But why do we need to learn the probability of words? Let\u2019s understand that with an example.**\n\nI\u2019m sure you have used Google Translate at some point. We all use it to translate one language to another for varying reasons. This is an example of a popular NLP application called Machine Translation.\n\nIn Machine Translation, you take in a bunch of words from a language and convert these words into another language. Now, there can be many potential translations that a system might give you and you will want to compute the probability of each of these translations to understand which one is the most accurate.\n[Source](https:\/\/www.analyticsvidhya.com\/blog\/2019\/08\/comprehensive-guide-language-model-nlp-python-code\/)","0de1c505":"<a id=\"5\"><\/a>\n\n\n<font color=\"blue\" size=+2.5><b>2. Library<\/b><\/font>","f0861e3a":"# Objective\nGoal of this kernel is following:\n- Learn how to train FastAI  ULMFIT on custom text data.\n- Learn how to implement language model on custom data.\n- Learn how to use Transfer Learning to get better accuracy.\n- Provide Perfect Guide for all the tips and trick to implement Text Classification model and get better accuracy as a Beginner.\n\nI have learned them from [FastAI](https:\/\/docs.fast.ai\/)","81d79d2a":"<a id=\"10\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.5 Finding LR <\/b><\/font>\n","6e3a04aa":"<a id=\"13\"><\/a>\n<font color=\"blue\" size=+2.5><b> 4. Building and Training a Text Classifier <\/b><\/font>","85d9abf7":"<font size=\"+1\" color=red ><b>Please Upvote my kernel and keep it in your favourite section if you think it is helpful.<\/b><\/font>","c5d557d8":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Word Cloud for a Target Variable <\/b><\/font>\n","a0701b65":"<a id=\"3.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Missing Value Analysis <\/b><\/font>\n","0619463a":"<a id=\"2\"><\/a>\n<font color=\"blue\" size=+2.5><b>1.2  What is Sentiment Analysis ?<\/b><\/font>\n<br\/>\n<br\/>\n![image.png](attachment:image.png)\n\n<br\/>\n**Sentiment analysis refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.**","9ff84434":"<a id=\"6\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.1 Data Loading For training Language Model <\/b><\/font>","b231c1ae":"<a id=\"1\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\"><center><font color=\"black\" size=+2.5><b>Introduction<\/b><\/font><\/center><\/h3>","13ded0f6":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Number of Punc <\/b><\/font>","52a34cf4":"<font size=\"+3\" color=blue><b> <center><u>Sentiment Analysis For Beginner<\/u><\/center><\/b><\/font>","72d9bcbf":"<a id=\"18\"><\/a>\n<font color=\"blue\" size=+2.5><b>Feedback and Support<\/b><\/font>\n<br\/>\n* Your feedback is much appreciated\n* Please UPVOTE if you LIKE this notebook\n* Comment if you have any doubts or you found any errors in the notebook","36b61b31":"<font color=\"blue\" size=+2.5><b> 3. Train a Text Language Model <\/b><\/font>","accc0920":"<a id=\"15\"><\/a>\n<font color=\"blue\" size=+2.5><b> 4.2 Loading Text Classification Model <\/b><\/font>\n","0606cbb1":"<a id=\"11\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.6 HyperParameter Tuning For Model Training <\/b><\/font>","c61a58e6":"<a id=\"top\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"><center>Table of content<\/center><\/h3>\n\n<font color=\"blue\" size=+1><b>Introduction<\/b><\/font>\n* [1. What is Sentiment Analysis ?](#2)\n* [2. What is Transfer Learning ?](#3)    \n\n<font color=\"blue\" size=+1><b>Library<\/b><\/font>\n* [1. Installation](#4)\n* [2. Import Libraries ](#5)   \n    \n<font color=\"blue\" size=+1><b> Train a text language model <\/b><\/font>\n* [1. Data Loading ](#6)\n* [2. Data Explorations ](#8)\n* [3. Model Loading For Language Model training ](#7)\n* [4. Training Language Model ](#9)\n* [5. Model Summary ](#10)\n* [6. Finding LR ](#11)\n* [7. Hyper Parameter Tuning ](#12)\n* [8. Saving Model ](#13)\n\n<font color=\"blue\" size=+1><b> Building a Text Classifier <\/b><\/font>\n* [1. Data Loading ](#14)\n* [2. Data Explorations ](#15)\n* [3. Model Loading ](#16)\n* [4. Training Model ](#17)\n* [5. Model Summary ](#18)\n* [6. Finding LR ](#19)\n* [7. Hyper Parameter Tuning ](#20)\n* [8. Saving Model ](#21)\n\n<font color=\"blue\" size=+1><b> Others <\/b><\/font>\n* [1. Interpret the results](#22)\n* [2. Prediction Using Trained Model](#23)\n* [3. Save and Load Model](#24)\n* [4. Sources](#25)","740123af":"<a id=\"12\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.7 Saving Model After Training <\/b><\/font>","dbfbad26":"<a id=\"8\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.3 Model Loading For Language Model training <\/b><\/font>\n","83fb07c0":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Number of in Title <\/b><\/font>","5532c34f":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Mean words length<\/b><\/font>","84e0c23a":"**Now, the training cycle is repeated: lr_find, freeze except last layer,..., unfreeze the model and saving the final trained model.**","c15c3e1d":"<a id=\"1.1\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\"><center><font color=\"black\" size=+2.5><b>About Data<\/b><\/font><\/center><\/h3><br\/>\n<br\/>\nIMDb is the most popular movie website and it combines movie plot description, Metastore ratings, critic and user ratings and reviews, release dates, and many more aspects.\n\nThe website is well know for storing almost every movie that has ever been released (the oldest is from 1874 - \"Passage de Venus\") or just planned to be released (newest movie is from 2027 - \"Avatar 5\").\n\nIMDb stores information related to more than 6 million titles (of which almost 500,000 are featured films) and it is owned by Amazon since 1998.\n\n\n## The dataset used contains\nIMDB Dataset.csv that has instances (texts from Twitter) 50k rows and atrributes\n\n* Review: the text of the reviews\n* Sentiment : this denotes whether a review is about a positive (1) or negative (0)","eea2abfe":"***Create a databunch for a text language model to get the data ready for training a language model. The text will be processed, tokenized and numericalized by a default processor, if you want to apply a customized tokenizer or vocab, you just need to create them.***","7aff229f":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Number of words <\/b><\/font>","023ffaae":"**Lets train our language model. First, we call lr_find to analyze and find an optimal learning rate for our problem, then we fit or train the model for a few epochs. Finally we unfreeze the model and runs it for a few more epochs. So we have a encoder trained and ready to be used for our classifier and it is recorded on disk.**","e55a593e":"<a id=\"4\"><\/a>\n<font color=\"blue\" size=+2.5><b>2.1 Installation<\/b><\/font>\n* Numpy\n* Pandas\n* Matplotlib\n* Fastai","973bf0a1":"Now we can create a language model based on the architecture \n[AWD_LSTM](https:\/\/docs.fast.ai\/text.models.html#AWD_LSTM)","b8b08691":"<a id=\"24\"><\/a>\n<font color=\"blue\" size=+2.5><b>5.4 Sources<\/b><\/font>\n<br\/>\n* [Fastai MOOC](https:\/\/course.fast.ai\/)\n* [Fastai library](https:\/\/docs.fast.ai\/)","c2f0a3e4":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Number of words Upper <\/b><\/font>","7dfd2268":"**we will have to load the encoder previously trained (the language model).**","b34d35fa":"<a id=\"22\"><\/a>\n<font color=\"blue\" size=+2.5><b>5.1 Interpret the results<\/b><\/font>\n<br\/>","b47864b0":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Target Variable Analysis <\/b><\/font>\n"}}