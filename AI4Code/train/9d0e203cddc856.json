{"cell_type":{"6c56f765":"code","b65db2f9":"code","1d07233d":"code","f113af19":"code","73be339d":"code","f3650f12":"code","ad1d2a20":"code","7451bbfe":"code","11087749":"code","17e2e6f0":"code","e8db5578":"code","d4c26190":"code","dc63b439":"code","e55aae3a":"markdown","0d30f64f":"markdown","65a0b512":"markdown","b5e968de":"markdown","3d32d47e":"markdown","b9a73d1b":"markdown","a205ba00":"markdown","d61900c0":"markdown","a7828d2a":"markdown","eb078f0e":"markdown","36ca5e24":"markdown"},"source":{"6c56f765":"!pip install -q git+git:\/\/github.com\/oke-aditya\/pytorch_cnn_trainer.git","b65db2f9":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport torchvision.transforms as T\nfrom pytorch_cnn_trainer import dataset\nfrom pytorch_cnn_trainer import model_factory\nfrom pytorch_cnn_trainer import utils\nfrom pytorch_cnn_trainer import engine","1d07233d":"import timm\n# To list all the models supported \n# print(timm.list_models())","f113af19":"MODEL_NAME = \"efficientnet_b3\"\nNUM_ClASSES = 10\nIN_CHANNELS = 1\nPRETRAINED = True  # If True -> Fine Tuning else Scratch Training\nEPOCHS = 5\nEARLY_STOPPING = True  # If you need early stoppoing for validation loss\nSAVE_PATH = \"{}.pt\".format(MODEL_NAME)\nSEED = 42","73be339d":"# Train and validation Transforms which you would like\ntrain_transforms = T.Compose([T.ToTensor(), T.Normalize((0.5,), (0.5,))])\nvalid_transforms = T.Compose([T.ToTensor(), T.Normalize((0.5,), (0.5,))])","f3650f12":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","ad1d2a20":"# Sets seed for your entire run\nutils.seed_everything(SEED)\nprint(\"Setting Seed for the run, seed = {}\".format(SEED))","7451bbfe":"# For example I provide ready to use CIFAR10, you can create your own dataset too.\nprint(\"Creating Train and Validation Dataset\")\ntrain_set, valid_set = dataset.create_fashion_mnist_dataset(train_transforms, valid_transforms)\nprint(\"Train and Validation Datasets Created\")","11087749":"print(\"Creating DataLoaders\")\ntrain_loader, valid_loader = dataset.create_loaders(train_set, train_set, train_batch_size=256, valid_batch_size=256)\nprint(\"Train and Validation Dataloaders Created\")","17e2e6f0":"print(\"Creating Model\")\nmodel = model_factory.create_timm_model(MODEL_NAME, num_classes=NUM_ClASSES, in_channels=IN_CHANNELS, pretrained=True)\nif torch.cuda.is_available():\n    print(\"Model Created. Moving it to CUDA\")\nelse:\n    print(\"Model Created. Training on CPU only\")\nmodel.to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)","e8db5578":"criterion = nn.CrossEntropyLoss()  # All classification problems usually need Cross entropy loss\nearly_stopper = utils.EarlyStopping(patience=7, verbose=True, path=SAVE_PATH)","d4c26190":"history = engine.fit(\n    epochs=EPOCHS,\n    model=model,\n    train_loader=train_loader,\n    valid_loader=valid_loader,\n    criterion=criterion,\n    device=device,\n    optimizer=optimizer,\n    early_stopper=early_stopper,\n)\nprint(\"Done !!\")","dc63b439":"for epoch in tqdm(range(EPOCHS)):\n    print()\n    print(\"Training Epoch = {}\".format(epoch))\n    train_metrics = engine.train_step(model, train_loader, criterion, device, optimizer)\n    print()\n    print(\"Validating Epoch = {}\".format(epoch))\n    valid_metrics = engine.val_step(model, valid_loader, criterion, device)\n    validation_loss = valid_metrics[\"loss\"]\n    early_stopper(validation_loss, model=model)\n\n    if early_stopper.early_stop:\n        print(\"Saving Model and Early Stopping\")\n        print(\"Early Stopping. Ran out of Patience for validation loss\")\n        break\n\n    print(\"Done Training, Model Saved to Disk\")","e55aae3a":"That's it !!! Training done.\n- It automates the stuff that you shouldn't worry about. Written in Pure PyTorch it has no extra library requirements too.\n- Train_step is useful for experienced people who want to do something hacky in their loop.\n- The library has only 4 Files !!!. Model Factory, Engine, dataset and utils. \n- I will add more features and support soon.\n","0d30f64f":"- Simple Pip install from github.","65a0b512":"## Install it","b5e968de":"- Model Factory Simplifies the model making, directly creates model you need","3d32d47e":"- The best part. We can simply do a .fit() method as in Keras. It trains the model !!!","b9a73d1b":"You can view the source code of this on GitHub here\nhttps:\/\/github.com\/oke-aditya\/pytorch_cnn_trainer","a205ba00":"- For hacky people, you simply use train_step and val_step to do it and customize the training too !!","d61900c0":"## Time to Use it !!!","a7828d2a":"- If you like this kernel or package please do upvote and share with others.\n- Also checkout on Github as well and let me know what you think in comments !! \n\nPackage Link  \nhttps:\/\/github.com\/oke-aditya\/pytorch_cnn_trainer","eb078f0e":"## Easily Fine Tune Torchvision and Timm models","36ca5e24":"- Introducing PyTorch CNN Trainer.\n- A simple yet powerful trainer, which allows you to easily train over datasets.\n- It is very annoying to write training loop and training code for CNN training. Also to support all the training features it takes massive time.\n\n- Usually we don't need distributed training and it is very uncomfortable to use argparse and get the job done.\n\n- This simplifies the training. It provide you a powerful engine.py which can do lot of training functionalities. Also a dataset.py to load dataset in common scenarios.\n\n- Works for both torchvision and Ross Wightman's models [timm](https:\/\/github.com\/rwightman\/pytorch-image-models\/tree\/master\/timm)\n\n- Note: - Pytorch Trainer is not a distributed training script.\n\n- It will work good for single GPU machine for Google Colab \/ Kaggle."}}