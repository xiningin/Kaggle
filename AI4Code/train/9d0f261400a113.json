{"cell_type":{"a0343945":"code","7f3dd265":"code","6cd1da5f":"code","5e5e8f9f":"code","11d958c1":"code","00cfb6a7":"code","bcd2b01a":"code","6cb33432":"code","9dc18421":"code","3691419e":"code","c24c294f":"code","fe6b299d":"code","d1c93f02":"code","5c1b2a03":"code","e59f10d7":"code","475cbc89":"code","c20b5270":"code","5dd7d871":"code","fb8cc490":"code","61c98499":"code","ef57833c":"code","d355b9a4":"code","1b4d4b6b":"code","858f332a":"code","4e1fc431":"code","3915759e":"code","83269975":"code","58369bbc":"code","d692374c":"markdown","942052d2":"markdown","c97b100c":"markdown","0ad68479":"markdown","fa1a33d9":"markdown","79e2e57e":"markdown"},"source":{"a0343945":"import os\nimport sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom itertools import chain\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.measure import label, regionprops\nfrom PIL import Image, ImageDraw\nfrom ast import literal_eval\nfrom tqdm.notebook import tqdm\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Dropout, Lambda,Conv2D, Conv2DTranspose, MaxPooling2D, MaxPooling2D, concatenate\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K","7f3dd265":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n\ntf.debugging.set_log_device_placement(True)\n\n# Create some tensors\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nc = tf.matmul(a, b)\n\nprint(c)","6cd1da5f":"# # Detect hardware, return appropriate distribution strategy\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     tpu = None\n\n# if tpu:\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# else:\n#     strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\n# print(\"REPLICAS: \", strategy.num_replicas_in_sync)","5e5e8f9f":"# Set some parameters\nIMG_WIDTH = 512\nIMG_HEIGHT = 512\n\n#customization for running on laptop\n# PATH =  os.getcwd() + \"\\\\\"\n# TRAIN_PATH = PATH + \"train\\\\\"\n# TEST_PATH = PATH + \"test\\\\\"\n\nPATH = \"..\/input\/global-wheat-detection\/\"\nTRAIN_PATH = '\/kaggle\/input\/global-wheat-detection\/train\/'\nTEST_PATH = '\/kaggle\/input\/global-wheat-detection\/test\/'\nSC_FACTOR = int(1024 \/ IMG_WIDTH)\n\nwarnings.filterwarnings('ignore')\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)","11d958c1":"# PATH = \"..\/input\/global-wheat-detection\/\"\ntrain_folder = os.path.join(PATH, \"train\")\ntest_folder = os.path.join(PATH, \"test\")\n\ntrain_csv_path = os.path.join(PATH, \"train.csv\")\n\ndf = pd.read_csv(train_csv_path)\nsample_sub = pd.read_csv(PATH + \"sample_submission.csv\")\n\ndf.head()","00cfb6a7":"# Get train and test IDs and paths\ntrain_ids = os.listdir(TRAIN_PATH)\ntest_ids = os.listdir(TEST_PATH)\n\n# train_ids = train_ids[0:1000]","bcd2b01a":"def make_polygon(coords):\n    xm, ym, w, h = coords\n    xm, ym, w, h = xm \/ SC_FACTOR, ym \/ SC_FACTOR, w \/ SC_FACTOR, h \/ SC_FACTOR   # scale values if image was downsized\n    return [(xm, ym), (xm, ym + h), (xm + w, ym + h), (xm + w, ym)]\n\nmasks = dict() # dictionnary containing all masks\n\nfor img_id, gp in tqdm(df.groupby(\"image_id\")):\n    gp['polygons'] = gp['bbox'].apply(eval).apply(lambda x: make_polygon(x))\n\n    img = Image.new('L', (IMG_WIDTH, IMG_HEIGHT), 0)\n    for pol in gp['polygons'].values:\n        ImageDraw.Draw(img).polygon(pol, outline=1, fill=1)\n\n    mask = np.array(img, dtype=np.uint8)\n    masks[img_id] = mask","6cb33432":"im = Image.fromarray(masks[list(masks.keys())[7]])\nplt.imshow(im)","9dc18421":"# Get and resize train images and masks\nX_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\nY_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\nprint('Getting and resizing train images and masks... ')\nsys.stdout.flush()\n\nfor n, id_ in tqdm(enumerate(train_ids[:]), total=len(train_ids)):\n    path = TRAIN_PATH + id_\n    img = imread(path)\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_train[n] = img\n    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n    \n    id_clean = id_.split('.')[0]\n    if id_clean in masks.keys():\n        Y_train[n] = masks[id_clean][:, :, np.newaxis]\n\n# Get and resize test images\nX_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\nsizes_test = list()\nprint('Getting and resizing test images...')\nsys.stdout.flush()\n\nfor n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n    path = TEST_PATH + id_\n    img = imread(path)\n    sizes_test.append([img.shape[0], img.shape[1]])\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_test[n] = img\n\nprint('Done!')","3691419e":"X_train.shape, Y_train.shape","c24c294f":"def show_images(images, num=2):\n    \n    images_to_show = np.random.choice(images, num)\n\n    for image_id in images_to_show:\n\n        image_path = os.path.join(train_folder, image_id + \".jpg\")\n        image = Image.open(image_path)\n\n        # get all bboxes for given image in [xmin, ymin, width, height]\n        bboxes = [literal_eval(box) for box in df[df['image_id'] == image_id]['bbox']]\n\n        # visualize them\n        draw = ImageDraw.Draw(image)\n        for bbox in bboxes:    \n            draw.rectangle([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]], width=3)\n\n        plt.figure(figsize = (15,15))\n        plt.imshow(image)\n        plt.show()\n\n\nunique_images = df['image_id'].unique()\nshow_images(unique_images)","fe6b299d":"#Credits to : https:\/\/www.kaggle.com\/c\/tgs-salt-identification-challenge\/discussion\/63044\n\ndef castF(x):\n    return K.cast(x, K.floatx())\n\ndef castB(x):\n    return K.cast(x, bool)\n\ndef iou_loss_core(true,pred):  #this can be used as a loss if you make it negative\n    intersection = true * pred\n    notTrue = 1 - true\n    union = true + (notTrue * pred)\n\n    return (K.sum(intersection, axis=-1) + K.epsilon()) \/ (K.sum(union, axis=-1) + K.epsilon())\n\ndef competitionMetric2(true, pred):\n\n    tresholds = [0.5 + (i * 0.05)  for i in range(5)]\n\n    #flattened images (batch, pixels)\n    true = K.batch_flatten(true)\n    pred = K.batch_flatten(pred)\n    pred = castF(K.greater(pred, 0.5))\n\n    #total white pixels - (batch,)\n    trueSum = K.sum(true, axis=-1)\n    predSum = K.sum(pred, axis=-1)\n\n    #has mask or not per image - (batch,)\n    true1 = castF(K.greater(trueSum, 1))    \n    pred1 = castF(K.greater(predSum, 1))\n\n    #to get images that have mask in both true and pred\n    truePositiveMask = castB(true1 * pred1)\n\n    #separating only the possible true positives to check iou\n    testTrue = tf.boolean_mask(true, truePositiveMask)\n    testPred = tf.boolean_mask(pred, truePositiveMask)\n\n    #getting iou and threshold comparisons\n    iou = iou_loss_core(testTrue,testPred) \n    truePositives = [castF(K.greater(iou, tres)) for tres in tresholds]\n\n    #mean of thressholds for true positives and total sum\n    truePositives = K.mean(K.stack(truePositives, axis=-1), axis=-1)\n    truePositives = K.sum(truePositives)\n\n    #to get images that don't have mask in both true and pred\n    trueNegatives = (1-true1) * (1 - pred1) # = 1 -true1 - pred1 + true1*pred1\n    trueNegatives = K.sum(trueNegatives) \n\n    return (truePositives + trueNegatives) \/ castF(K.shape(true)[0])","d1c93f02":"### Build U-Net model\n    \ninputs = Input((IMG_HEIGHT, IMG_WIDTH, 3))\ns = Lambda(lambda x: x \/ 255) (inputs)  # rescale inputs\n\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\nc1 = Dropout(0.1) (c1)\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\nc2 = Dropout(0.1) (c2)\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\nc3 = Dropout(0.2) (c3)\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\nc4 = Dropout(0.2) (c4)\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\nc5 = Dropout(0.3) (c5)\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n\nu6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = concatenate([u6, c4])\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\nc6 = Dropout(0.2) (c6)\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n\nu7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c3])\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\nc7 = Dropout(0.2) (c7)\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n\nu8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c2])\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\nc8 = Dropout(0.1) (c8)\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n\nu9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = concatenate([u9, c1])\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\nc9 = Dropout(0.1) (c9)\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n\np5 = Conv2DTranspose(16, (8, 8), strides=(16,16), padding = 'same') (c5)\nu10 = concatenate([p5, c9], axis=3)\nc10 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u10)\nc10 = Dropout(0.1) (c10)\nc10 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c10)\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid') (c10)\n\n\n# instantiating the model in the strategy scope creates the model on the TPU\n\nmodel = Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[competitionMetric2])\n\nmodel.summary()","5c1b2a03":"from tensorflow.keras.utils import plot_model\nplot_model(model, show_shapes=True)","e59f10d7":"# #data augmentation\/ generation\n# data_gen_args = dict(#featurewise_center=True,\n#                  #featurewise_std_normalization=True,\n#                  #rotation_range=10,\n#                  #zca_whitening=True,\n#                  #width_shift_range=0.2,\n#                  #height_shift_range=0.2,\n#                  #zoom_range=0.2,\n#                  #horizontal_flip=True,\n#                  #vertical_flip=True\n#                 )\n# image_datagen = ImageDataGenerator(**data_gen_args)\n# mask_datagen = ImageDataGenerator(**data_gen_args)\n# # Provide the same seed and keyword arguments to the fit and flow methods\n# #seed = 10\n# #image_datagen.fit(X_train, augment=True#, seed=seed)\n# #mask_datagen.fit(Y_train, augment=True#, seed=seed)\n# image_generator = image_datagen.flow(\n#     X_train,\n#     batch_size=4,\n#     #seed=42\n# )\n# mask_generator = mask_datagen.flow(\n#     Y_train,\n#     batch_size=4,\n#     #seed=42\n# )\n# # combine generators into one which yields image and masks\n# # train_generator = (pair for pair in zip(image_generator, mask_generator))","475cbc89":"# Fit model\nearlystop = EarlyStopping(patience=10, verbose=1, restore_best_weights=True)\n\nmodel.fit(x=X_train,y=Y_train,\n            validation_split=0.20,\n            batch_size=16, \n            #steps_per_epoch=6*len(X_train)\/8,\n            epochs=40, \n            callbacks=[earlystop]\n         )","c20b5270":"THRESH = 0.5\n\npreds = model.predict(X_test)[:, :, :, 0]\nmasked_preds = preds > THRESH","5dd7d871":"preds.shape","fb8cc490":"n_rows = 3\nf, ax = plt.subplots(n_rows, 3, figsize=(14, 10))\n\nfor j, idx in enumerate([4,5,6]):\n    for k, kind in enumerate(['original', 'pred', 'masked_pred']):\n        if kind == 'original':\n            img = X_test[idx]\n        elif kind == 'pred':\n            img = preds[idx]\n        elif kind == 'masked_pred':\n            masked_pred = preds[idx] > THRESH\n            img = masked_pred\n        ax[j, k].imshow(img)\n\nplt.tight_layout()","61c98499":"def get_params_from_bbox(coords, scaling_factor=SC_FACTOR):\n    xmin, ymin = coords[1] * scaling_factor, coords[0] * scaling_factor\n    w = (coords[3] - coords[1]) * scaling_factor\n    h = (coords[2] - coords[0]) * scaling_factor\n    \n    return xmin, ymin, w, h","ef57833c":"# Allows to extract bounding boxes from binary masks\nbboxes = list()\n\nfor j in range(masked_preds.shape[0]):\n    label_j = label(masked_preds[j, :, :]) \n    props = regionprops(label_j,intensity_image=preds[j,:,:])   # that's were the job is done\n    bboxes.append(props)","d355b9a4":"bboxes[0][0].bbox","1b4d4b6b":"np.max(bboxes[0][0].intensity_image)","858f332a":"# Here we format the bboxes into the required format\noutput = dict()\nsignificant_scores = list()\nsignificant_bboxes = list()\n\nfor i in range(masked_preds.shape[0]):\n    \n    bboxes_processed = [get_params_from_bbox(bb.bbox, scaling_factor=SC_FACTOR) for bb in bboxes[i]]\n    scores = [np.max(bb.intensity_image) for bb in bboxes[i]]\n    \n    bbareas = [bb.bbox_area for bb in bboxes[i]]\n    \n#     df_test = pd.DataFrame({'bboxe_processed':bboxes_processed,\n#                             'scores':scores,\n#                             'bbares':bbareas})\n#     print(df_test.shape)\n    \n    significant_scores.append([score for score,area in zip(scores,bbareas) if ((score > 0.5) & (area > 100))])\n    significant_bboxes.append([' '.join(map(str, bb_m)) for bb_m,score,area in zip(bboxes_processed,scores,bbareas) if ((score > 0.5) & (area > 100))])\n    \n    print(\"There are \" + str(len(significant_scores[i])) + \" Scores in \"+ test_ids[i])\n    print(\"There are \" + str(len(significant_bboxes[i])) + \" BBoxes in \"+ test_ids[i])\n    \n    assert(len(significant_scores[i]) == len(significant_bboxes[i]))\n    \n    \n    \n    \n    formated_boxes = [str(score) + ' '.join(map(str, bb_m)) for score,bb_m in zip(significant_scores[i],significant_bboxes[i])]\n    \n#     formated_boxes = formated_boxes[i > 100 for i in bbareas]\n    \n    output[str.split(test_ids[i],'.')[0]] = \" \".join(formated_boxes)","4e1fc431":"sub  = pd.DataFrame()\nsub['image_id'] = output.keys()\nsub[\"PredictionString\"] = output.values()\nsub","3915759e":"# for image_id in sub['image_id']:\n\n#         image_path = os.path.join(test_folder, image_id + \".jpg\")\n#         image = Image.open(image_path)\n\n#         # get all bboxes for given image in [xmin, ymin, width, height]\n#         bboxes = significant_bboxes[pd.Index(sub['image_id']).get_loc(image_id)]\n\n#         # visualize them\n#         draw = ImageDraw.Draw(image)\n#         for bbox in bboxes:  \n#             bbox = tuple(map(int ,bbox.split(' ')))\n#             draw.rectangle([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]], width=5)\n\n#         plt.figure(figsize = (5,5))\n#         plt.imshow(image)\n#         plt.show()","83269975":"sub = pd.merge(sample_sub[['image_id']],sub)\nsub","58369bbc":"sub.to_csv('submission.csv', index=False)","d692374c":"Let's quickly look at what the images and mask look like (credits : https:\/\/www.kaggle.com\/devvindan\/wheat-detection-eda)","942052d2":"Here we want to binarize the prediction to get a mask (as the input y_train was also a mask)\n\nLet's visualise predictions !","c97b100c":"Let's load the dataset","0ad68479":"Let's plot a quick example of the mask of the first image :","fa1a33d9":"Now the next step is to translate our masked predictions into several bounding boxes","79e2e57e":"Prediction"}}