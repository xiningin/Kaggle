{"cell_type":{"8b06cae3":"code","dad1878d":"code","64ed1216":"code","a6caa9ed":"code","fe1ed562":"code","2212e581":"code","2bf6b8ac":"code","ab6dba56":"code","43a62cef":"code","9a773d70":"code","159b968c":"code","70f2a68c":"code","428291b2":"code","cf96a047":"code","2c9b7da1":"code","b0ce7db4":"code","f7aeb024":"code","81bc991d":"code","e84b310c":"markdown","9dd52221":"markdown","f53a964d":"markdown","c59e8940":"markdown","75886675":"markdown","80900494":"markdown","2282a7af":"markdown","127a05ee":"markdown","0f9d94c6":"markdown","0ffac362":"markdown","85664dc6":"markdown","cc66a798":"markdown","0f9491a5":"markdown","8fcbf46c":"markdown","3a85bba3":"markdown"},"source":{"8b06cae3":"import numpy as np # linear algebra\nimport pandas as pd # data processing\n\n# for standardization\nfrom sklearn.preprocessing import StandardScaler as ss\n\n# for splitting into train and test datasets\nfrom sklearn.model_selection import train_test_split \n\n# for modelling\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# for balancing dataset by oversampling\nfrom imblearn.over_sampling import SMOTE, ADASYN\n\n# for performance metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_recall_curve\nfrom sklearn.metrics import auc, roc_curve, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report, average_precision_score\n\n# for data visualization\nimport matplotlib.pyplot as plt\n\n#Miscellaneous\nimport time\nimport random\nimport os\n\n\n","dad1878d":"# set number of rows to be displayed\npd.options.display.max_columns = 300\n\n# reading the dataset\ndata = pd.read_csv(\"..\/input\/creditcard.csv\")","64ed1216":"print('Shape: ',data.shape)\n\nprint('\\nColumns: ',data.columns.values)\n\nprint('\\nData types:\\n',data.dtypes.value_counts())","a6caa9ed":"data.Class.value_counts()","fe1ed562":"# check if there are null values in the dataset\ndata.isnull().sum().sum()","2212e581":"# Time & Amount distributions\n\nfig = plt.figure(figsize=(14,5))\nax = fig.add_subplot(1,2,1)\ndata.Time.plot(kind = \"hist\", bins = 40)\nplt.xlabel('Time(in secs)', size='large')\nax = fig.add_subplot(1,2,2)\ndata.Amount.plot(kind = \"hist\", bins = 40)\nplt.xlabel('Amount', size='large')\n","2bf6b8ac":"y = data.iloc[:,30]\nX = data.iloc[:,0:30]\n\nprint(X.columns)\nprint(y.head(3))","ab6dba56":"X_train, X_test, y_train, y_test =   train_test_split(X, y, test_size = 0.3, stratify = y)\n\nX_train.shape\ny_train.value_counts()","43a62cef":"# XGBoost - scale_pos_weight - Control the balance of positive and negative weights, useful for unbalanced classes.\n# A typical value to consider: sum(negative instances) \/ sum(positive instances)\n\nweight = 199020\/344\n\n# Using Random Forest and XGBoost\nrf = RandomForestClassifier(n_estimators=100, class_weight={0:1,1:7})\nxg = XGBClassifier(scale_pos_weight = weight, learning_rate = 0.7,\n                   reg_alpha= 0.8,\n                   reg_lambda= 1)\n\nrf1 = rf.fit(X_train,y_train)\nxg1 = xg.fit(X_train,y_train)","9a773d70":"y_pred_rf = rf1.predict(X_test)\ny_pred_xg= xg1.predict(X_test)\n\ny_pred_rf_prob = rf1.predict_proba(X_test)\ny_pred_xg_prob = xg1.predict_proba(X_test)\n\nprint(\"RF - Accuracy - \",accuracy_score(y_test,y_pred_rf))\nprint(\"XGBoost - Accuracy - \",accuracy_score(y_test,y_pred_xg))\n\nprint(\"RF:\\n\",confusion_matrix(y_test,y_pred_rf))\nprint(\"XGBoost:\\n\",confusion_matrix(y_test,y_pred_xg))\n\nfpr_rf1, tpr_rf1, thresholds = roc_curve(y_test,\n                                 y_pred_rf_prob[: , 1],\n                                 pos_label= 1\n                                 )\n\nfpr_xg1, tpr_xg1, thresholds = roc_curve(y_test,\n                                 y_pred_xg_prob[: , 1],\n                                 pos_label= 1\n                                 )\n\nprint(\"RF - AUC: \",auc(fpr_rf1,tpr_rf1))\nprint(\"XGBoost - AUC: \",auc(fpr_xg1,tpr_xg1))\n\np_rf,r_rf,f_rf,_ = precision_recall_fscore_support(y_test,y_pred_rf)\np_xg,r_xg,f_xg,_ = precision_recall_fscore_support(y_test,y_pred_xg)\n\nprint(\"Random Forest:\\n Precision: \",p_rf, \"Recall: \", r_rf)\nprint(\"XGBoost:\\n Precision: \",p_xg, \"Recall: \", r_xg)","159b968c":"# Oversampling and balancing using SMOTE\nsm = SMOTE(random_state=42)\nX_bal, y_bal = sm.fit_sample(X_train, y_train)\n\ncolumns = X_train.columns\nX_bal = pd.DataFrame(data = X_bal, columns = columns)\n\nprint(X_train.shape)\nprint(X_bal.shape)\nprint(np.unique(y_bal, return_counts=True))","70f2a68c":"# Initialising the models\nrf_sm = RandomForestClassifier(n_estimators=100)\nxg_sm = XGBClassifier(learning_rate=0.7,\n                   reg_alpha= 0.8,\n                   reg_lambda= 1\n                   )\n\n# training the models\nrf_sm1 = rf_sm.fit(X_bal,y_bal)\nxg_sm1 = xg_sm.fit(X_bal,y_bal)","428291b2":"print(\"With Smote: \\n\")\n\n# Making predictions on the test data\ny_pred_rf1 = rf_sm1.predict(X_test)\ny_pred_xg1= xg_sm1.predict(X_test)\n\ny_pred_rf_prob1 = rf_sm1.predict_proba(X_test)\ny_pred_xg_prob1 = xg_sm1.predict_proba(X_test)\n\nprint(\"RF - Accuracy - \",accuracy_score(y_test,y_pred_rf1))\nprint(\"XGBoost - Accuracy - \",accuracy_score(y_test,y_pred_xg1))\n\nprint(\"RF:\\n\",confusion_matrix(y_test,y_pred_rf1))\nprint(\"XGBoost:\\n\",confusion_matrix(y_test,y_pred_xg1))\n\nfpr_rf2, tpr_rf2, thresholds = roc_curve(y_test,\n                                 y_pred_rf_prob1[: , 1],\n                                 pos_label= 1\n                                 )\n\nfpr_xg2, tpr_xg2, thresholds = roc_curve(y_test,\n                                 y_pred_xg_prob1[: , 1],\n                                 pos_label= 1\n                                 )\n\nprint(\"RF - AUC: \",auc(fpr_rf2,tpr_rf2))\nprint(\"XGBoost - AUC: \",auc(fpr_xg2,tpr_xg2))\n\np_rf,r_rf,f_rf,_ = precision_recall_fscore_support(y_test,y_pred_rf1)\np_xg,r_xg,f_xg,_ = precision_recall_fscore_support(y_test,y_pred_xg1)\n\nprint(\"Random Forest:\\n Precision: \",p_rf, \"Recall: \", r_rf)\nprint(\"XGBoost:\\n Precision: \",p_xg, \"Recall: \", r_xg)","cf96a047":"# oversampling and balancing dataset with ADASYN\nad = ADASYN()\nX_ad, y_ad = ad.fit_sample(X_train, y_train)\n \nX_ad = pd.DataFrame(data = X_ad, columns = X_train.columns)\n\nprint(X_train.shape)\nprint(X_ad.shape)\nprint(np.unique(y_bal, return_counts=True))","2c9b7da1":"# Initialising the models\nrf_ad = RandomForestClassifier(n_estimators=100)\nxg_ad = XGBClassifier(learning_rate=0.8,\n                   reg_alpha= 0.8,\n                   reg_lambda= 0.8)\n\n# training the models\nrf_ad1 = rf_ad.fit(X_ad,y_ad)\nxg_ad1 = xg_ad.fit(X_ad,y_ad)","b0ce7db4":"print(\"With ADASYN:\\n\")\n# Making predictions on the test data\ny_pred_rf2 = rf_ad1.predict(X_test)\ny_pred_xg2 = xg_ad1.predict(X_test)\n\ny_pred_rf_prob2 = rf_ad1.predict_proba(X_test)\ny_pred_xg_prob2 = xg_ad1.predict_proba(X_test)\n\nprint(\"RF - Accuracy - \",accuracy_score(y_test,y_pred_rf2))\nprint(\"XGBoost - Accuracy - \",accuracy_score(y_test,y_pred_xg2))\n\nprint(\"RF:\\n\",confusion_matrix(y_test,y_pred_rf2))\nprint(\"XGBoost:\\n\",confusion_matrix(y_test,y_pred_xg2))\n\nfpr_rf3, tpr_rf3, thresholds = roc_curve(y_test,\n                                 y_pred_rf_prob2[: , 1],\n                                 pos_label= 1\n                                 )\n\nfpr_xg3, tpr_xg3, thresholds = roc_curve(y_test,\n                                 y_pred_xg_prob2[: , 1],\n                                 pos_label= 1\n                                 )\n\nprint(\"RF - AUC: \",auc(fpr_rf3,tpr_rf3))\nprint(\"XGBoost - AUC: \",auc(fpr_xg3,tpr_xg3))\n\np_rf,r_rf,f_rf,_ = precision_recall_fscore_support(y_test,y_pred_rf2)\np_xg,r_xg,f_xg,_ = precision_recall_fscore_support(y_test,y_pred_xg2)\n\nprint(\"Random Forest:\\n Precision: \",p_rf, \"Recall: \", r_rf)\nprint(\"XGBoost:\\n Precision: \",p_xg, \"Recall: \", r_xg)","f7aeb024":"fig = plt.figure(figsize=(14,16))   # Create window frame\n\n\nroc = [[\"Imbalanced\", fpr_rf1, tpr_rf1,'rf'],[\"Imbalanced\", fpr_xg1, tpr_xg1,'xg'],[\"SMOTE\", fpr_rf2, tpr_rf2,'rf'],[\"SMOTE\", fpr_xg2, tpr_xg2,'xg'],\n[\"ADASYN\", fpr_rf3, tpr_rf3,'rf'], [\"ADASYN\", fpr_xg3, tpr_xg3,'xg']]\n\nfor i in range(6):\n    #8.1 Connect diagonals\n    ax = fig.add_subplot(3,2,i+1) \n    ax.plot([0, 1], [0, 1], ls=\"--\")  # Dashed diagonal line\n\n    #8.2 Labels \n    ax.set_xlabel('False Positive Rate')  # Final plot decorations\n    ax.set_ylabel('True Positive Rate')\n    ax.set_title(roc[i][0])\n\n    #8.3 Set graph limits\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.0])\n\n    #8.4 Plot each graph now\n    ax.plot(roc[i][1], roc[i][2], label = roc[i][3])\n\n    #8.5 Set legend and show plot\n    ax.legend(loc=\"lower right\")\n    ","81bc991d":"prc = [[\"Imbalanced\", rf1,'rf'],[\"Imbalanced\", xg1,'xg'],[\"SMOTE\", rf_sm1,'rf'],[\"SMOTE\", xg_sm1,'xg'],\n[\"ADASYN\", rf_ad1,'rf'], [\"ADASYN\", xg_ad1,'xg']]\n\nfig = plt.figure(figsize=(14,16))\n\nfor i in range(6):\n    ax = fig.add_subplot(3,2,i+1)\n   \n    precision, recall, _ = precision_recall_curve(y_test,prc[i][1].predict_proba(X_test)[:,-1])\n\n    plt.step(recall, precision, color='b', alpha=0.2,\n         where='post')\n    plt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='b')\n\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    ttl = prc[i][0]+' '+prc[i][2]\n    plt.title(ttl)\n    \n      \n ","e84b310c":"#### Importing Libraries","9dd52221":"#### Splitting data into predictors and target","f53a964d":"## Model Building\n1. -Building model on immbalanced dataset.\n2. -Building model on dataset balanced using SMOTE.\n3. -Building model on dataset balanced using ADASYN.\n","c59e8940":"* <font color=orange>As seen from the results of modelling on imbalanced dataset, when compared to Random Forest XGboost has given better precision and recall values for Fraud cases. XGBoost has performed better. <\/font>","75886675":"* <font color=orange>We now have equal occurences of Fraud and Non Fraud cases in y_bal.<\/font> ","80900494":"#### Splitting predictors and target into train and test data","2282a7af":"### ROC curves","127a05ee":"#### Building model on dataset balanced using ADASYN.","0f9d94c6":"#### Data Exploration","0ffac362":"#### <div id='imm'> Building models on immbalanced dataset. <\/div>","85664dc6":"#### Building model on dataset balanced using SMOTE.\n","cc66a798":"### Context : \n##### The dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\n##### It contains only numerical input variables which are the result of a PCA transformation. Due to confidentiality issues, original features and more background information about the data are not provided. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","0f9491a5":"* <font color=orange>As seen from the results of modelling on SMOTE balanced dataset, when compared to XGBoost Random Forest has given better precision and recall values for Fraud cases. <\/font>","8fcbf46c":"* <font color=orange>As seen from the results of modelling on ADASYN balanced dataset, when compared to XGBoost Random Forest has given better precision and recall values for Fraud cases. <\/font>","3a85bba3":"### Precision - Recall curves"}}