{"cell_type":{"279485d5":"code","7eb5a6b4":"code","a577b653":"code","c1658a9c":"code","a16eed3f":"code","2a460ab0":"code","dca48e79":"code","e5e103e9":"code","0b5c6b38":"code","ec62c8e9":"code","4c04c518":"code","9d9d7e9f":"code","074419d6":"code","4071094a":"code","18e62a7e":"code","346056ad":"code","b3a4f75c":"code","9418e646":"code","03cdc862":"code","e20bc9a7":"markdown","875cb0bf":"markdown"},"source":{"279485d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # visualization\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7eb5a6b4":"df = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndf.head()","a577b653":"df.info()","c1658a9c":"# Checking Null Values \ndf.isnull().values.any()","a16eed3f":"LABELS = [\"Normal\", \"Fraud\"]\ncount_class = pd.value_counts(df['Class'],sort =True)\ncount_class.plot(kind='bar')\nplt.title(\"Class Distribution\")\nplt.xticks(range(2),LABELS)\nplt.xlabel(\"class\")\nplt.ylabel(\"Frequency\")","2a460ab0":"# Fraud and Normal Data\nfraud = df[df[\"Class\"]==1]\nnormal = df[df['Class']==0]\nprint(fraud.shape,normal.shape)","dca48e79":"fraud.Amount.describe()","e5e103e9":"normal.Amount.describe()","0b5c6b38":"f,(ax1,ax2) = plt.subplots(2,1,sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 50\nax1.hist(fraud.Amount,bins = bins)\nax1.set_title('Fraud')\nax2.hist(normal.Amount,bins=bins)\nax2.set_title('Normal')\nplt.xlabel(\"Amount ($)\")\nplt.ylabel(\"Number of Transactions\")\nplt.xlim((0,20000))\nplt.yscale(\"log\")\nplt.show()","ec62c8e9":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Time of transaction vs Amount by class')\nax1.scatter(fraud.Time, fraud.Amount)\nax1.set_title('Fraud')\nax2.scatter(normal.Time, normal.Amount)\nax2.set_title('Normal')\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","4c04c518":"## Take some sample of the data\n\ndata1= df.sample(frac = 0.1,random_state=1)\n\ndata1.shape","9d9d7e9f":"#Determine the number of fraud and valid transactions in the dataset\n\nFraud = data1[data1['Class']==1]\n\nValid = data1[data1['Class']==0]\n\noutlier_fraction = len(Fraud)\/float(len(Valid))","074419d6":"\nprint(outlier_fraction)\n\nprint(\"Fraud Cases : {}\".format(len(Fraud)))\n\nprint(\"Valid Cases : {}\".format(len(Valid)))\n","4071094a":"import seaborn as sns\n#get correlations of each features in dataset\ncorrmat = data1.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","18e62a7e":"#Create independent and Dependent Features\ncolumns = data1.columns.tolist()\n# Filter the columns to remove data we do not want \ncolumns = [c for c in columns if c not in [\"Class\"]]\n# Store the variable we are predicting \ntarget = \"Class\"\n# Define a random state \nstate = np.random.RandomState(42)\nX = data1[columns]\nY = data1[target]\nX_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))\n# Print the shapes of X & Y\nprint(X.shape)\nprint(Y.shape)","346056ad":"from sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM","b3a4f75c":"##Define the outlier detection methods\n\nclassifiers = {\n    \"Isolation Forest\":IsolationForest(n_estimators=100, max_samples=len(X), \n                                       contamination=outlier_fraction,random_state=state, verbose=0),\n    \"Local Outlier Factor\":LocalOutlierFactor(n_neighbors=20, algorithm='auto', \n                                              leaf_size=30, metric='minkowski',\n                                              p=2, metric_params=None, contamination=outlier_fraction),\n    \"Support Vector Machine\":OneClassSVM(kernel='rbf', degree=3, gamma=0.1,nu=0.05, \n                                         max_iter=-1, random_state=state)\n   \n}","9418e646":"type(classifiers)","03cdc862":"n_outliers = len(Fraud)\nfor i, (clf_name,clf) in enumerate(classifiers.items()):\n    #Fit the data and tag outliers\n    if clf_name == \"Local Outlier Factor\":\n        y_pred = clf.fit_predict(X)\n        scores_prediction = clf.negative_outlier_factor_\n    elif clf_name == \"Support Vector Machine\":\n        clf.fit(X)\n        y_pred = clf.predict(X)\n    else:    \n        clf.fit(X)\n        scores_prediction = clf.decision_function(X)\n        y_pred = clf.predict(X)\n    #Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n    n_errors = (y_pred != Y).sum()\n    # Run Classification Metrics\n    print(\"{}: {}\".format(clf_name,n_errors))\n    print(\"Accuracy Score :\")\n    print(accuracy_score(Y,y_pred))\n    print(\"Classification Report :\")\n    print(classification_report(Y,y_pred))","e20bc9a7":"\n**Observations :**\n\nIsolation Forest detected 73 errors versus Local Outlier Factor detecting 97 errors vs. SVM detecting 8516 errors\nIsolation Forest has a 99.74% more accurate than LOF of 99.65% and SVM of 70.09\nWhen comparing error precision & recall for 3 models , the Isolation Forest performed much better than the LOF as we can see that the detection of fraud cases is around 27 % versus LOF detection rate of just 2 % and SVM of 0%.\nSo overall Isolation Forest Method performed much better in determining the fraud cases which is around 30%.\nWe can also improve on this accuracy by increasing the sample size or use deep learning algorithms however at the cost of computational expense.We can also use complex anomaly detection models to get better accuracy in determining more fraudulent cases","875cb0bf":"Model Prediction"}}