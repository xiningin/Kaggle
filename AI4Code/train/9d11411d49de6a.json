{"cell_type":{"256432ee":"code","2c6feb63":"code","387cab76":"code","daefa967":"code","c8df0c55":"code","f529a886":"code","5b078af5":"code","5268590b":"code","ac79e9ae":"code","f3ca9a72":"code","13371213":"code","299254c2":"code","6d9a3d6b":"code","c56e1ca3":"code","02ee91ac":"code","f6cd0ecb":"code","3814c743":"code","df06e00c":"code","bba1a659":"code","7bcc5019":"code","282aa414":"code","abaa06dc":"code","a0fcc7ce":"code","04e26a20":"code","8b84ff90":"code","5c3cea09":"code","ded8afef":"code","549999cc":"code","62b1b6c9":"code","925251ff":"code","2f5b1885":"code","999e706a":"code","5dd9b97a":"code","0565c416":"markdown","d3f823e0":"markdown","c8654451":"markdown","56ba5c36":"markdown","deebd45f":"markdown","536533bd":"markdown","a6f89c01":"markdown","79d58574":"markdown","622b9d58":"markdown","2ee4f3e0":"markdown","4abc4203":"markdown","0a15fcff":"markdown","26967bf4":"markdown","bbe076e8":"markdown","64283745":"markdown","466135ff":"markdown","9f4279b5":"markdown","206995ff":"markdown","79a07529":"markdown"},"source":{"256432ee":"# Import some libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","2c6feb63":"ls \/kaggle\/input\/cord19-data-set-converted-to-csv\/\n","387cab76":"# Read files and load into dataframes\npath = \"\/kaggle\/input\/cord19-data-set-converted-to-csv\/\"\ndf_biorxiv = pd.read_csv(path + \"biorxiv_clean.csv\")\ndf_comm = pd.read_csv(path + \"clean_comm_use.csv\")\ndf_noncomm = pd.read_csv(path + \"clean_noncomm_use.csv\")\ndf_pmc = pd.read_csv(path + \"clean_pmc.csv\")","daefa967":"# Merge all into one big dataframe.\nframes = [df_biorxiv, df_comm, df_noncomm, df_pmc]\ndf = pd.concat(frames, ignore_index=True)","c8df0c55":"df.head()","f529a886":"df.info()","5b078af5":"# Get a new dataframe with the 'title' column only\ndft = pd.DataFrame(df['title'], columns=['CLUSTER_NBR', 'TITLE']) \ndft['TITLE'] = df['title']\ndft","5268590b":"# Drop column CLUSTER_NBR, it will be recreated later \ndft.drop(columns='CLUSTER_NBR', axis=1, inplace=True)\ndft.reset_index(drop=True, inplace=True)\n# Drop nulls\ndft.dropna(axis=0, inplace=True)\n# Drop duplicates\ndft.drop_duplicates(inplace=True)","ac79e9ae":"dft.describe()","f3ca9a72":"%%time\nimport string\nimport spacy.cli\nspacy.cli.download('en_core_web_sm')\n\nnlp = spacy.load('en_core_web_sm')\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\npunctuation = string.punctuation","13371213":"# Create a tokenizer function\ndef my_tokenizer(sentence):\n    # Create token object\n    mytokens = nlp(sentence)\n    #mytokens = parser(sentence)\n\n    # Lemmatize each token and convert it into lowercase\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n\n    # Remove stop words and punctuations\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuation ]\n\n    # return preprocessed list of tokens\n    return mytokens","299254c2":"%%time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Define and set up the vectorizer function\ntfidf_vectorizer = TfidfVectorizer(tokenizer=my_tokenizer, use_idf=True)\n\n# Compute TfIdf values for the samples\ntfidf_vectors = tfidf_vectorizer.fit_transform(dft['TITLE'])","6d9a3d6b":"tfidf_vectors","c56e1ca3":"# Get the first vector out (first title)\nfirst_vector = tfidf_vectors[0]\nsecond_vector = tfidf_vectors[1]\n \n# Place tf-idf values in pandas data frame\ndf_vector_1 = pd.DataFrame(first_vector.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\ndf_vector_1.sort_values(by=[\"tfidf\"],ascending=False).head(15)","02ee91ac":"# And get the vector for the second title\ndf_vector_2 = pd.DataFrame(second_vector.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\ndf_vector_2.sort_values(by=[\"tfidf\"],ascending=False).head(10) ","f6cd0ecb":"# Display model main Tfidf values\ndf_model_tfidf = pd.DataFrame(tfidf_vectorizer.idf_, index=tfidf_vectorizer.get_feature_names(),columns=[\"idf_weights\"])\n \n# sort ascending\ndf_model_tfidf.sort_values(by=['idf_weights']).head(20) # lower value means less unique","3814c743":"%%time\n# Variance vs. Nbr. of Components of 'title' \nfrom sklearn.decomposition import TruncatedSVD\n###############################################\n# Max nbr of components #######################\n###############################################\nsvd = TruncatedSVD(n_components=10000)\nsvd_components = svd.fit(tfidf_vectors)","df06e00c":"%%time\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,6))\nplt.title('Variance vs Nbr. of Components of \"title\"')\nplt.plot(np.cumsum(svd.explained_variance_ratio_))","bba1a659":"# Create a function that calculates the number of components needed for a given variance\ndef calc_nbr_components(var_ratio, goal_var: float) -> int:\n    # Set initial variance explained so far\n    total_variance = 0.0\n    \n    # Set initial number of features\n    n_components = 0\n    \n    # For the explained variance of each feature:\n    for explained_variance in var_ratio:\n        \n        # Add the explained variance to the total\n        total_variance += explained_variance\n        \n        # Add one to the number of components\n        n_components += 1\n        \n        # If we reach our goal level of explained variance\n        if total_variance >= goal_var:\n            # End the loop\n            break\n            \n    # Return the number of components\n    return n_components","7bcc5019":"nbr_components_95 = calc_nbr_components(svd.explained_variance_ratio_, 0.95)\nnbr_components_95","282aa414":"%%time\n# Configure SVD model and apply to TfIdf vectors data\nsvd = TruncatedSVD(n_components=nbr_components_95)\nsvd_tfidf_vectors = svd.fit_transform(tfidf_vectors)","abaa06dc":"# Convert svd tdidf vectors matrix to dataframe\ndf_svd_tfidf_vectors = pd.DataFrame(svd_tfidf_vectors)\ndf_svd_tfidf_vectors.head()","a0fcc7ce":"%%time\nfrom sklearn.cluster import KMeans\n######################################################\n# Nbr of clusters for K-means = 6  ##################\n######################################################\nn_clusters = 6\nkmeans = KMeans(n_clusters=n_clusters, n_init=100, random_state=17, n_jobs=1)\nkmeans.fit(svd_tfidf_vectors)\ncluster_labels = kmeans.labels_","04e26a20":"kmeans.labels_","8b84ff90":"dft['CLUSTER_NBR'] = kmeans.labels_\ndft.head()","5c3cea09":"for i in range(n_clusters):\n \n  print(\"---------------------------------------------------------------------\")\n  print(dft.loc[(dft['CLUSTER_NBR'] == i)][['CLUSTER_NBR', 'TITLE']])\n","ded8afef":"plt.figure(figsize=(8,6))\nplt.title('K-means clusters')\nplt.scatter(df_svd_tfidf_vectors[0], df_svd_tfidf_vectors[1], s=10, alpha=.5, c=kmeans.labels_, cmap='viridis')","549999cc":"plt.figure(figsize=(8,6))\nplt.title('K-means clusters')\nplt.scatter(df_svd_tfidf_vectors[1], df_svd_tfidf_vectors[2], s=10, alpha=.5, c=kmeans.labels_, cmap='viridis');","62b1b6c9":"plt.figure(figsize=(8,6))\nplt.title('K-means clusters')\nplt.scatter(df_svd_tfidf_vectors[0], df_svd_tfidf_vectors[2], s=10, alpha=.5, c=kmeans.labels_, cmap='viridis');","925251ff":"from mpl_toolkits.mplot3d import Axes3D \nfig = plt.figure(figsize=(16,10)) \nax = fig.add_subplot(111, projection = '3d') \nax.scatter(df_svd_tfidf_vectors[0], df_svd_tfidf_vectors[1], df_svd_tfidf_vectors[2], alpha=1, c=kmeans.labels_, s=10)\nax.set_xlabel('$Comp 0$', fontsize=10)\nax.set_ylabel('$Comp 1$', fontsize=10)\nax.set_zlabel('$Comp 2$', fontsize=10)\nplt.show()","2f5b1885":"fig = plt.figure(figsize=(16,10)) \nax = fig.add_subplot(111, projection = '3d') \nax.scatter(df_svd_tfidf_vectors[1], df_svd_tfidf_vectors[2], df_svd_tfidf_vectors[3], alpha=1, c=kmeans.labels_, s=10) \nax.set_xlabel('$Comp 1$', fontsize=10)\nax.set_ylabel('$Comp 2$', fontsize=10)\nax.set_zlabel('$Comp 3$', fontsize=10)\nplt.show()","999e706a":"# Create method that takes a sentence and creates a work cloud from it\nfrom wordcloud import WordCloud\ndef  createWordCloud(df_text):\n  text = \"\"\n  for x in df_text.index:\n    text = text + df_text.TITLE[x] + \" \"\n    text \n  # Generate and display a word cloud \n  wordcloud = WordCloud().generate(text)\n  plt.figure(figsize=(12,8))\n  plt.imshow(wordcloud, interpolation='bilinear')\n  plt.axis(\"off\")\n  plt.show()  ","5dd9b97a":"for x in range(n_clusters):\n  df_text = pd.DataFrame(dft[dft.CLUSTER_NBR==x], columns=['TITLE','CLUSTER_NBR'])\n  print(\"##################\")\n  print(\"CLUSTER NUMBER: \" + str(x))\n  print(\"##################\")\n  createWordCloud(df_text)\n  print('')   \n  print(\"SAMPLE ARTICLES: \")\n  print(df_text.head(12))\n  print('###################################################################')\n  print('')","0565c416":"Interesting to note how the word 'virus' has the lowest TFIDF value in the set, as it appears possibly in each and every of the titles.\n\nThe lower the TFIDF value of a word, the less unique it is to any particular title.","d3f823e0":"## NLP analysis of the titles data: TFIDF vectorization\n\n\n---\n\n","c8654451":"To finish off, I am going to use the WordCloud library to create one cloud of words for each of the clusters identified by K-means.\n\nThis should give a reasonable amount of insight to the main topics, cases, approaches, results, etc. covered by the articles in each cluster, and according to their title.","56ba5c36":"*tfidf_vectors* is a **sparse matrix** where each of the article titles is described by their TFIDF vector.\n\nLet\u2019s have a look at the TFIDF values for a couple of titles in the collection.","deebd45f":"## Clustering of articles by 'title' using K-means\n\n\n---\n\n","536533bd":"Now we can apply K-means to the titles data transformed through TFIDF and T-SVD, using the nbr of components that explains 95% of variance rather than the full number of them.","a6f89c01":"And finally a couple of 3D scatter plot of the article title samples against SVD components 0,1 y 2 first and then 1, 2 and 3. ","79d58574":"We can use 5672 components to meet 95% variance.","622b9d58":"From here, it is easy to tweak the model to specify a different number of clusters, improve the data tokenization by adding custom remove_words or bigrams, or just start all over again grouping the articles by any other column (abstract, text) or combination. \n\nBe warned: computing times will quickly go through the roof, so be careful. \n\nIf you found the notebook of interest, please upvote as that encourages me to keep progressing through my data scientist career.\n\nAlso I would very much appreciate any comments from doctors and other health related professionals, in respect of the accuracy of the algorithms predictions (the meaningfulness of the groupings) and \/ or suggestions on what to implement from here, as I lack the knowledge in the health sciences domain.\n\nThank you for reading!!!\n\n\n---\n\n\n","2ee4f3e0":"The above represents a list of articles in each cluster. \n\nLet's now see some scatter plots of these clusters, represented against their main SVD components. ","4abc4203":"This shows the main TFIDF components and its weights for the first title.\n","0a15fcff":"## Data loading\n\n\n---\n\n","26967bf4":"## Data clean up and preparation\n\n\n---\n\n","bbe076e8":"*kmeans.labels_* is actually an array with the cluster number each article has been asssigned to.","64283745":"## CORD-19: Clustering of articles by their title\n\n---\n\nIn this notebook, I am going to perform a clustering analysis of the CORD-19 articles collection, based on their titles.\n\nI will be using the four .csv files as pre-processed here:\n\nhttps:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\/data\n\nI will first load and merge all data from the four different files into a single dataframe, then process the text in the title column, performe clean-up and compute the TFID vectors matrix, to finally apply techniques of dimension reduction and K-means clustering.\n\nI have done a few iterations with different parameters during the development, but the current configuation and outputs correspond to the following parameters:\n\n- Number of articles = all articles (13202 before any clean-up)\n- Number of clusters = 6\n- Number of T-SVD components (max) = 10000\n- Number of T-SVD components (95% var) = 5672","466135ff":"## Scatter plots of the clusters (in 2D and 3D)\n\n\n---\n\n","9f4279b5":"## One wordcloud for each cluster\n\n\n---\n\n","206995ff":"Similarly for the second title.\n\nFinally, just for educative purposes, let's have a look at the model vocabulary for the whole set of titles.","79a07529":"## Feature reduction with Truncated SVD\n\n---\n\n\nI want to ultimately apply K-means clustering algorithm to the abstract vectors dataframe, in order to try and classify the articles in groups (clusters). But K-means is known to be a slow algorithm so it makes sense to perform some sort of feature reduction before going on.\n\nPCA (Principal Component Analysis) is the usual algorithm to this end, however PCA cannot handle sparse matrixes. Fortunately, we can use **Truncated SVD** to reduce the number of dimensions. \n\nThe objective is to leave only the components necessary to explain 95% or higher the variance of the data. \n"}}