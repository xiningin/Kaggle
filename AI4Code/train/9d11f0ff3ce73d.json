{"cell_type":{"0e18ed8b":"code","60ec3b4d":"code","3a69b1ea":"code","56f5df3b":"code","3fd7a661":"code","c8586636":"code","f00ab85f":"code","183387a8":"code","092d4d45":"code","a54abde6":"code","9554054f":"code","04697057":"code","448e08e9":"code","2e6b8980":"code","a649bccb":"code","c9e8f023":"code","b0cfc274":"code","5641869b":"code","f9ce101f":"code","aeaf7fb0":"code","0077f579":"code","fb6f7802":"code","be9b465f":"code","68cf7375":"code","ece53969":"code","fdabc75d":"code","85acf77b":"code","7bd98d30":"code","3320bc36":"code","fb746644":"code","3ed73209":"code","5fcf2a35":"code","68533fd4":"code","1930c58a":"code","d6c051cf":"code","2c3834eb":"code","0df13cc3":"code","0c4e0490":"code","79163701":"code","a119c50f":"code","b8d4b6d3":"code","350caa85":"code","1d9eb879":"code","cbc2051b":"code","0e8575dd":"code","c7522605":"code","ef2e73de":"code","ba8a343d":"markdown","ad977a1a":"markdown","1cbffeaa":"markdown","f8f3a60e":"markdown","56b2ffd9":"markdown","ffdf057e":"markdown","6b00f6cd":"markdown","8cf4cef3":"markdown","006bad75":"markdown","de05ed4e":"markdown","6290a846":"markdown","860c29cf":"markdown","e62c31c9":"markdown","e243228a":"markdown","ddc0ed57":"markdown","4e87135f":"markdown","77294fea":"markdown","38299c49":"markdown","a232c84c":"markdown","e1dbcaaf":"markdown","52be627e":"markdown"},"source":{"0e18ed8b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","60ec3b4d":"!pip install xlrd","3a69b1ea":"import pandas as pd # module baca file csv\nimport matplotlib.pyplot as plt # module untuk visualisasi (EDA)","56f5df3b":"train_data = pd.read_csv(\"\/kaggle\/input\/GiveMeSomeCredit\/cs-training.csv\", index_col=0)\ntest_data  = pd.read_csv(\"\/kaggle\/input\/GiveMeSomeCredit\/cs-test.csv\", index_col=0)\ndata_description = pd.read_excel(\"\/kaggle\/input\/GiveMeSomeCredit\/Data Dictionary.xls\", header=1)","3fd7a661":"train_data.head() ","c8586636":"test_data.head()","f00ab85f":"print(train_data.shape)\nprint(test_data.shape)\n# Training dan testing data memiliki fitur yang sama","183387a8":"train_data.info()","092d4d45":"test_data.info()","a54abde6":"train_data.isnull().sum()\n# Terdapat 2 fitur yang memiliki nilai null pada training data","9554054f":"test_data.isnull().sum()\n# Terdapat 3 column yang memiliki nilai null pada test data\n# Namun column SeriousDlqin2yrs dibiarkan null karena kita akan memprediksinya","04697057":"import seaborn as s\n\n# dibawah ini adalah berapa banyak orang yang memiliki tunggakan lewat 90 hari\n# 1 artinya memiliki tunggakan\n# 0 artinya tidak memiliki\n\ntarget_count = train_data[\"SeriousDlqin2yrs\"].value_counts()\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\ns.countplot(\"SeriousDlqin2yrs\", data=train_data, ax=axes[0])\n\naxes[1].set_title(\"SeriousDlqin2yrs\")\ntarget_count.plot.pie(explode=[0, 0.1], autopct='%1.1f%%',ax=axes[1])","448e08e9":"fig, ax = plt.subplots(figsize=(10, 10))\ns.heatmap(train_data.corr(), annot=True, cmap=\"Blues\", ax=ax)","2e6b8980":"age_bins = [e for e in range(0, 100, 5)]\n\nplt.hist(train_data[\"age\"], bins=age_bins)\n\n# Dari data berikut kita tau bahwa mayoritas yang mengajukan kartu kredit adalah rentang 45 - 60","a649bccb":"from scipy import stats\nimport numpy as np\n\ndef remove_outliers(df, feature_name, max_scale_from_std):\n    f = df[feature_name]\n    std = f.std()\n    distance_from_mean = f - f.mean()\n    condition = np.abs(distance_from_mean) >= max_scale_from_std * std\n    df.loc[condition, feature_name] = f.mean()\n    \n    return df","c9e8f023":"feature_one = train_data['NumberOfTimes90DaysLate']\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 6))\naxes[0].scatter(feature_one, feature_one)\naxes[0].set_xlabel(\"Sebelum\")\n\ntrain_data = remove_outliers(train_data, \"NumberOfTimes90DaysLate\", 3)\nfeature_one = train_data[\"NumberOfTimes90DaysLate\"]\naxes[1].scatter(feature_one, feature_one)\naxes[1].set_xlabel(\"Sesudah\")\n\nplt.show()","b0cfc274":"feature_two = train_data['NumberOfTime30-59DaysPastDueNotWorse']\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 6))\naxes[0].scatter(feature_two, feature_two)\naxes[0].set_xlabel(\"Sebelum\")\n\ntrain_data = remove_outliers(train_data, \"NumberOfTime30-59DaysPastDueNotWorse\", 3)\nfeature_two = train_data[\"NumberOfTime30-59DaysPastDueNotWorse\"]\naxes[1].scatter(feature_two, feature_two)\naxes[1].set_xlabel(\"Sesudah\")\n\nplt.show()","5641869b":"feature_three = train_data['NumberOfTime60-89DaysPastDueNotWorse']\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 6))\naxes[0].scatter(feature_three, feature_three)\naxes[0].set_xlabel(\"Sebelum\")\n\ntrain_data = remove_outliers(train_data, \"NumberOfTime60-89DaysPastDueNotWorse\", 3)\nfeature_three = train_data[\"NumberOfTime60-89DaysPastDueNotWorse\"]\naxes[1].scatter(feature_three, feature_three)\naxes[1].set_xlabel(\"Sesudah\")\n\nplt.show()","f9ce101f":"import seaborn as s","aeaf7fb0":"# Plot data training\n\nfig, ax = plt.subplots(figsize=(18, 6), ncols=2)\ns.distplot(train_data[\"NumberOfDependents\"], ax=ax[0])\ns.distplot(train_data[\"MonthlyIncome\"], ax=ax[1])","0077f579":"number_of_dependents_train = train_data[\"NumberOfDependents\"]\nnumber_of_dependents_train_mean = number_of_dependents_train.mean()\nnumber_of_dependents_train_median = number_of_dependents_train.median()\nnumber_of_dependents_train_mode = number_of_dependents_train.mode().mean()\n\nmonthly_income_train = train_data[\"MonthlyIncome\"]\nmonthly_income_train_mean = monthly_income_train.mean()\nmonthly_income_train_median = monthly_income_train.median()\nmonthly_income_train_mode = monthly_income_train.mode().mean()\n\nprint(\"Number Of Dependents:\", number_of_dependents_train_mean, number_of_dependents_train_median, number_of_dependents_train_mode)\nprint(\"Monthly Income:\", monthly_income_train_mean, monthly_income_train_median, monthly_income_train_mode)\n\n# Terlihat dari plot diatas serta kondisi mean-median-mode, data condong ke arah kanan","fb6f7802":"# Plot data testing\n\nfig, ax = plt.subplots(figsize=(18, 6), ncols=2)\ns.distplot(test_data[\"NumberOfDependents\"], ax=ax[0]) \ns.distplot(test_data[\"MonthlyIncome\"], ax=ax[1])","be9b465f":"number_of_dependents_test = test_data[\"NumberOfDependents\"]\nnumber_of_dependents_test_mean = number_of_dependents_test.mean()\nnumber_of_dependents_test_median = number_of_dependents_test.median()\nnumber_of_dependents_test_mode = number_of_dependents_test.mode().mean()\n\nmonthly_income_test = test_data[\"MonthlyIncome\"]\nmonthly_income_test_mean = monthly_income_test.mean()\nmonthly_income_test_median = monthly_income_test.median()\nmonthly_income_test_mode = monthly_income_test.mode().mean()\n\nprint(\"Number Of Dependents:\", number_of_dependents_test_mean, number_of_dependents_test_median, number_of_dependents_test_mode)\nprint(\"Monthly Income:\", monthly_income_test_mean, monthly_income_test_median, monthly_income_test_mode)\n\n# Terlihat dari plot diatas serta kondisi mean-median-mode, data condong ke arah kanan","68cf7375":"# Setelah mendapatkan nilai modus dan median, kita bisa langsung mengisi nilai kosong tersebut\n# Sesuai kondisi kemiringan diatas\n\ntrain_data['NumberOfDependents'].fillna(number_of_dependents_train_mode, inplace=True)\ntrain_data['MonthlyIncome'].fillna(monthly_income_train_mode, inplace=True)\n\ntest_data['NumberOfDependents'].fillna(number_of_dependents_test_mode, inplace=True)\ntest_data['MonthlyIncome'].fillna(monthly_income_test_mode, inplace=True)","ece53969":"train_data.isnull().sum()","fdabc75d":"test_data.isnull().sum()","85acf77b":"# Kita tidak perlu menggunakan fitur SeriousDlqin2yrs karena hanya digunakan untuk testing\nX = train_data.drop(\"SeriousDlqin2yrs\", axis=1)\ny = train_data[\"SeriousDlqin2yrs\"]\n\nX.info()","7bd98d30":"from sklearn.model_selection import train_test_split\n\n# Data kita bagi dengan rasio 70\/30 untuk training\/testing\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","3320bc36":"from sklearn.ensemble import RandomForestClassifier\n\nparameter = {\n    'n_estimators': [9,27,36],\n    'max_depth': [3,7,9],\n    'min_samples_leaf': [2, 4]\n}\n\nrf = RandomForestClassifier()","fb746644":"from sklearn.model_selection import RandomizedSearchCV\n\nrandom_forest_search = RandomizedSearchCV(rf, param_distributions=parameter, cv=5)","3ed73209":"random_forest_search.fit(x_train, y_train)","5fcf2a35":"# Mendapatkan parameter terbaik untuk random forest\n\nbest_estimator_rf = random_forest_search.best_estimator_\nbest_estimator_rf","68533fd4":"# Acuracy for random forest\nprint(\"training accuracy: {:.2f}\".format(random_forest_search.score(x_train, y_train) * 100))\nprint(\"validation accuracy: {:.2f}\".format(random_forest_search.score(x_test, y_test) * 100))","1930c58a":"from lightgbm import LGBMClassifier\n\nparameter = {\n    'n_estimators': [100, 250],\n    'max_depth': [8, 24],\n    'num_leaves': [25, 50],\n    'first_metric_only': [True]\n}\n\n\nlgb = LGBMClassifier()","d6c051cf":"from sklearn.model_selection import GridSearchCV\n\ngs = GridSearchCV(\n    estimator=lgb,\n    param_grid=parameter,\n    cv=5,\n)\n\nlgbm_fit = gs.fit(x_train, y_train)","2c3834eb":"print(lgbm_fit.best_score_)\nprint(lgbm_fit.best_params_)","0df13cc3":"# Acuracy for lightgbm\nprint(\"training accuracy: {:.2f}\".format(gs.score(x_train, y_train) * 100))\nprint(\"validation accuracy: {:.2f}\".format(gs.score(x_test, y_test) * 100))","0c4e0490":"from sklearn.linear_model import LogisticRegression\n\nparameter = {\n    'solver': ['newton-cg', 'lbfgs'],\n    'penalty': ['l2', 'elasticnet'],\n    'C': [ 1e-1, 1],\n}\n\nlr = LogisticRegression()","79163701":"%%capture --no-display\n\ngs_lr = GridSearchCV(\n    estimator=lr,\n    param_grid=parameter,\n    cv=5,\n)\n\nlr_fit = gs_lr.fit(x_train, y_train)","a119c50f":"print(lr_fit.best_score_)\nprint(lr_fit.best_params_)","b8d4b6d3":"# Acuracy for logistic regression\nprint(\"training accuracy: {:.2f}\".format(gs_lr.score(x_train, y_train) * 100))\nprint(\"validation accuracy: {:.2f}\".format(gs_lr.score(x_test, y_test) * 100))","350caa85":"# Ambil probabilitas prediksi untuk target 0 dan 1\n# Tidak lupa konversi dari data continouse ke binary\n\ncontinous_proba = lgbm_fit.best_estimator_.predict_proba(x_test)\n\nproba = continous_proba[:, 1]\nproba[proba >= 0.5] = 1\nproba[proba < 0.5] = 0\nproba = proba.astype(int)","1d9eb879":"from sklearn.metrics import confusion_matrix\n\nmatrix = confusion_matrix(y_test, proba) # Mengambil probabilitas keluar angka 1\n\nplt.figure(figsize=(12, 6))\n\ns.heatmap(matrix, annot=True, fmt=\".2f\", lineWidths=5, square=True, cmap=\"PuBuGn_r\")\nplt.show()","cbc2051b":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, proba))","0e8575dd":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_test, proba)","c7522605":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, continous_proba[:, 1])\nplt.plot(fpr, tpr)\nplt.plot(fpr, fpr, linestyle = '--', color = 'b')\nplt.xlabel('Rate of false positive')\nplt.ylabel('Rate of true positive')\nplt.title('ROC')","ef2e73de":"X = test_data.drop([\"SeriousDlqin2yrs\"], axis=1)\ny = lgbm_fit.predict_proba(X)[:, 1]\nids = X.index.values\npredicted = pd.DataFrame({'Id': ids, 'Probability': y})\npredicted.to_csv(\"submission.csv\", index=False)","ba8a343d":"# 3. Modeling dan prediksi\n\n## 3.1 Membuat data testing dari data training","ad977a1a":"### 3.3.2 Classification report","1cbffeaa":"## 3.3 Kesimpulan\n\nDari 3 model diatas terlihat bahwa LightGBM mendapatkan score yang paling baik, jadi kita gunakan itu sebagai final model untuk melakukan prediksi pada submission.\n\n### 3.3.1 Confusion matrix","f8f3a60e":"## 1.2 Membaca data\n### 1.2.1 Menyimpan data ke-variable","56b2ffd9":"# 1. Exploratory Data Analysis\n\n## 1.1 Mengimport module yang diperlukan","ffdf057e":"### 1.3.3 Mencari mean, median, dan modus","6b00f6cd":"# 2. Feature Engineering\n\n### 2.1 Menghapus outliers","8cf4cef3":"### 1.3.2 Korelasi dengan Heatmap\n\nKita visualisasikan hubungan tiap fitur dengan heatmap","006bad75":"Dari data tersebut kita bisa mengambil kesimpulan bahwa data yang kita miliki tidak seimbang, karena rasio dari 2 kelas tersebut adalah 14:1. Umumnya kriteria data yang baik adalah yang memiliki rasio kurang lebih 50:50. Jadi kita tidak bisa terlalu bergantung pada skor akurasi untuk memprediksi kesuksesan model.","de05ed4e":"## 3.4 Submission","6290a846":"### 1.3.3 Age","860c29cf":"## 3.2 Membuat dan mengetest model\n\nUntuk setiap model yang kita test, kita akan menentukan possible parameter dan membuat random search untuk menemukan parameter terbaik.\n\n### 3.2.1 Random Forest","e62c31c9":"Bisa kita lihat 3 feature diatas memiliki nilai outliers dan kita sudah membersihkannya.","e243228a":"## 2.2 Pengecekan kemiringan data\n\nDisini kita akan plot data berdasarkan median, mean, ataupun modus untuk cek kearah mana data itu miring.  \nData yang simetris biasanya ditandai dengan plot mediannya adalah data tertinggi, lalu kiri dan kanan memiliki tinggi yang kurang lebih sama.  \n\nKita mengisi nilai null berdasarkan kondisi berikut:  \nGunakan median jika:\n> (mean < median < mode)\n\nGunakan modus jika:\n> (mean > median > mode)\n","ddc0ed57":"### 3.2.3 Logistic Regression","4e87135f":"## 2.3 Mengisi nilai kosong","77294fea":"## 1.3 Explorasi fitur\n\n### 1.3.1 SeriousDlqin2yrs\n\nData dicolumn ini digunakan sebagai target dari prediksi. Berisi enum antara 0 atau 1, Kita bisa menggunakan Piechart atau Barchart untuk menghitung banyaknya tiap value","38299c49":"### 3.3.3 AUC ROC Score","a232c84c":"### 1.2.2 Pengecekan fitur dan column\n\nDisini kita akan melihat kelengkapan data, misalnya terdapat missing value (null)","e1dbcaaf":"Dari korelasi map diatas kita mengetahu bahwa ada 3 column yang memiliki korelasi yang tinggi yaitu: NumberOfTimes90DaysLate, NumberOfTime30-59DaysPastDueNotWorse, NumberOfTime60-89DaysPastDueNotWorse. Setelah itu kita perlu mendeteksi ouliers dari ketiga column tersebut. Lalu menghapusnya dengan fungsi berikut.","52be627e":"### 3.2.2 LightGBM"}}