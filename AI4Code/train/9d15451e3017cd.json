{"cell_type":{"ded8c78e":"code","992901c5":"code","bc81d92e":"code","488756ed":"code","789d1bd2":"code","e4fee7bb":"code","48d7a306":"code","e9bbbebc":"code","8e8b162b":"code","040e226b":"code","5d5b8aa7":"code","cd80cfda":"code","55355b2b":"code","f86519e1":"markdown"},"source":{"ded8c78e":"import os\nimport gc\nimport copy\nimport time\nimport random\nimport string\n\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\nimport pandas as pd\n\n\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import AutoTokenizer, AutoModel\n\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import lr_scheduler","992901c5":"class Config:\n    \n    num_classes=1\n    epochs=1\n    margin=0.5\n    model_name = 'bert-base-uncased'\n    batch_size = 64\n    lr = 1e-5\n    weight_decay=0.01\n    scheduler = 'CosineAnnealingLR'\n#     scheduler = 'LinearWarmup'\n    max_length = 128\n    accumulation_step = 1\n    patience = 1\n    seed = 11\n    warmup_steps = 10","bc81d92e":"def set_seed(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)","488756ed":"class ToxicDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.less_toxic = df['less_toxic'].values\n        self.more_toxic = df['more_toxic'].values\n        self.tokenizer = tokenizer\n        self.max_len = max_length\n        \n    def __len__(self):\n        return len(self.less_toxic)\n    \n    def __getitem__(self, idx):\n\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                self.more_toxic[idx],\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                self.less_toxic[idx],\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']\n        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        target = 1\n       \n        return {\n            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n            'target': torch.tensor(target, dtype=torch.long)\n        }\n       ","789d1bd2":"class ToxicModel(nn.Module):\n    def __init__(self, model_name, args):\n        super(ToxicModel, self).__init__()\n        self.args = args\n        self.model = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(p=0.2)\n        self.output = nn.LazyLinear(self.args.num_classes)\n    \n        \n    def forward(self, toxic_ids, toxic_mask):\n        \n        out = self.model(\n            input_ids=toxic_ids,\n            attention_mask=toxic_mask,\n            output_hidden_states=False\n        )\n        \n        out = self.dropout(out[1])\n        outputs = self.output(out)\n\n        return outputs\n        ","e4fee7bb":"def train_one_epoch(model, optimizer, scheduler, dataloader, epoch):\n    model.train()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    index=[]\n    lr=[]\n    losses=[]\n    ind = 0\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        more_toxic_ids = data['more_toxic_ids'].cuda()\n        more_toxic_mask = data['more_toxic_mask'].cuda()\n        less_toxic_ids = data['less_toxic_ids'].cuda()\n        less_toxic_mask = data['less_toxic_mask'].cuda()\n        targets = data['target'].cuda()\n        \n        batch_size = args.batch_size\n\n        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n        \n        loss = nn.MarginRankingLoss(margin=args.margin)(more_toxic_outputs, less_toxic_outputs, targets)\n        loss = loss \/ args.accumulation_step\n        loss.backward()\n        \n        losses.append(loss.item())\n        if (step + 1) % args.accumulation_step == 0:\n            optimizer.step()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss \/ dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n        ind=ind+1\n        index.append(ind)\n        lr.append(optimizer.param_groups[0]['lr'])\n        \n    gc.collect()\n    \n    return epoch_loss, index, lr, losses\n","48d7a306":"\ndef valid_one_epoch(model, dataloader, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    losses=[]\n    with torch.no_grad():\n        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for step, data in bar:        \n            more_toxic_ids = data['more_toxic_ids'].cuda()\n            more_toxic_mask = data['more_toxic_mask'].cuda()\n            less_toxic_ids = data['less_toxic_ids'].cuda()\n            less_toxic_mask = data['less_toxic_mask'].cuda()\n            targets = data['target'].cuda()\n\n            batch_size = 2*args.batch_size\n\n            more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n            less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n\n            loss = nn.MarginRankingLoss(margin=args.margin)(more_toxic_outputs, less_toxic_outputs, targets)\n            losses.append(loss.item())\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n\n            epoch_loss = running_loss \/ dataset_size\n\n            bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                            LR=optimizer.param_groups[0]['lr'])   \n\n        gc.collect()\n\n        return epoch_loss, losses","e9bbbebc":"def get_loaders(args, fold, df, tokenizer):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = ToxicDataset(df_train, tokenizer=tokenizer, max_length=args.max_length)\n    valid_dataset = ToxicDataset(df_valid, tokenizer=tokenizer, max_length=args.max_length)\n\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, \n                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=2*args.batch_size, \n                              num_workers=2, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader, len(train_dataset)","8e8b162b":"def run(args, model, optimizer, scheduler, num_epochs, fold):\n    \n    if torch.cuda.is_available():\n        print(\"Model pushed to GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n\n    best_epoch_loss = np.inf\n\n    patience_counter = 0\n    indexes=[]\n    idx=0\n    lrs=[]\n    train_losses=[]\n    valid_losses=[]\n    epoch_train_losses=[]\n    epoch_valid_losses=[]\n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss, index, lr, train_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           epoch=epoch)\n        epoch_train_losses.append(train_epoch_loss)\n        idx = idx+len(index)\n        \n\n        lrs.append(lr)\n        train_losses.append(train_loss)\n        \n        \n        val_epoch_loss, val_loss = valid_one_epoch(model, valid_loader,\n                                         epoch=epoch)\n        epoch_valid_losses.append(val_epoch_loss)\n        valid_losses.append(val_loss)\n        \n        if val_epoch_loss <= best_epoch_loss:\n            \n            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n            best_epoch_loss = val_epoch_loss\n\n            PATH = f\"model_fold_{fold}.bin\"\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            print(f\"Model Saved\")\n            \n        else:\n            patience_counter+=1\n            print('-'*50)\n            print(f'Early stopping counter {patience_counter} of {args.patience}')\n            print('-'*50)\n            if patience_counter == args.patience:\n                print('*'*20,'Early Stopping','*'*20)\n                break\n            \n            \n        print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    \n    indexes = [i for i in range(idx)]\n    \n    fig = make_subplots(rows=2, cols=3, subplot_titles=(\n        'Loss Variation',\n        'Train Loss per step',\n        'Validation Loss per step',\n        'Train Loss per epoch',\n        'Validation Loss per epoch'\n    ))\n        \n    fig.append_trace(go.Scatter(x=np.hstack(indexes), y=np.hstack(lrs), name='LR'), row=1, col=1)\n    fig.append_trace(go.Scatter(x=np.hstack(indexes), y=np.hstack(train_losses), name='Train Loss'), row=1, col=2)\n    fig.append_trace(go.Scatter(x=np.hstack(indexes), y=np.hstack(valid_losses), name='Valid Loss'), row=1, col=3)          \n    fig.append_trace(go.Scatter(x=[i for i in range(num_epochs)], y=epoch_train_losses, name = 'Train loss', mode='lines'), row=2, col=1)\n    fig.append_trace(go.Scatter(x=[i for i in range(num_epochs)], y=epoch_valid_losses, name = 'Valid loss', mode='lines'), row=2, col=2)\n    fig.show()\n    \n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n    \n    # load best model weights\n\n    \n    return model","040e226b":"def fetch_scheduler(args, optimizer, num_training_steps):\n    if args.scheduler == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=500, \n                                                   eta_min=1e-6)\n#     elif args.schedular == 'CosineAnnealingWarmRestarts':\n#         scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n#                                                              eta_min=CONFIG['min_lr'])\n\n    elif args.scheduler == 'LinearWarmup':\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, \n            num_warmup_steps=args.warmup_steps, \n            num_training_steps=num_training_steps\n        )\n    elif args.schedular == None:\n        return None\n        \n    return scheduler","5d5b8aa7":"df = pd.read_csv('..\/input\/fold-is-gold-custom-data\/5folds.csv')","cd80cfda":"# df = df[:50]","55355b2b":"for fold in range(1,2):\n    print('-'*50)\n    print(f\"Fold: {fold}\")\n    print('-'*50)\n    \n    args = Config()\n    set_seed(args.seed)\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    train_loader, valid_loader, train_dataset_len = get_loaders(args, fold, df, tokenizer)\n    \n    model = ToxicModel(args.model_name, args)\n    model = model.cuda()\n    num_training_steps = (train_dataset_len \/ args.batch_size * args.epochs)\n    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n    scheduler = fetch_scheduler(args, optimizer, num_training_steps)\n    \n    model = run(args, model, optimizer, scheduler,\n                                  num_epochs=args.epochs,\n                                  fold=fold)\n    \n    del model, train_loader, valid_loader, train_dataset_len\n    _ = gc.collect()\n    print('-'*100)\n    print()","f86519e1":"# Pytorch FIT Kernel\n\n### Special Points :\n* Shows training and validation losses per step\n* Shows training and validation losses per Epoch\n* Shows variation of Loss w.r.t. each step\n* Has Early stopping enabled to prevent overfitting\n* Added get_linear_schedule_with_warmup scheduler\n\nMost of the code has been taken from []() by []().\n\nThanks to him for such a great kernel and nice approach. This kernel deos not use WandB (T(O.o) Complicated for me) :(\n\nAnyways. Enjoy and Upvote if this helps :)"}}