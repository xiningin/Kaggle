{"cell_type":{"9a329cc7":"code","6a910971":"code","2ce30435":"code","7e705efa":"code","623be460":"code","a358a683":"code","9627cba8":"code","750c5257":"code","a1ecd2f1":"code","d22a84ec":"code","631b27f9":"code","69bc23e6":"code","89c1283e":"code","825e062a":"code","d947aa62":"code","655b7d96":"code","89e1cd61":"code","2ba14735":"code","87ab9897":"code","43a03191":"code","5da47565":"code","737fb652":"code","6e7c818a":"code","53d07f4f":"code","cfa46f2c":"code","66e24081":"code","de554bc1":"code","b8e7170f":"code","bf4d361a":"code","6fbed375":"code","e4bff9e5":"code","5fc78462":"code","007b421c":"markdown","d4f8065d":"markdown","402b8253":"markdown","f459111d":"markdown","a7b091f0":"markdown","ca5c0e04":"markdown","c9ab5bd7":"markdown","bc073ab5":"markdown","d97585f2":"markdown","dbbeb8a8":"markdown","c3ffb5a5":"markdown","28696779":"markdown","14bc233e":"markdown","02d42b50":"markdown","6d4dcb8b":"markdown","ebbea305":"markdown","86584617":"markdown","e2dd0590":"markdown","8790fc2b":"markdown","8ebbe303":"markdown","2aafc889":"markdown"},"source":{"9a329cc7":"import gresearch_crypto\nimport pandas as pd\nimport numpy as np\nimport os\n\nimport time\nfrom datetime import datetime\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.options.display.max_rows = 999\n%config InlineBackend.figure_format = 'retina'\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6a910971":"dtype={'Asset_ID': 'int8', 'Count': 'int32', 'row_id': 'int32', 'Count': 'int32',\n       'Open': 'float32', 'High': 'float32', 'Low': 'float32', 'Close': 'float32',\n       'Volume': 'float32', 'VWAP': 'float32'}\n\ntrain_df = pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv', low_memory=False, dtype=dtype)\nasset_details = pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv')","2ce30435":"# auxiliary function, from datetime to timestamp\ntotimestamp = lambda s: np.int32(time.mktime(datetime.strptime(s, \"%d\/%m\/%Y\").timetuple()))\n\n# define function to compute log returns\ndef log_return(series, periods=1):\n    return np.log(series).diff(periods=periods)","7e705efa":"len(train_df.Asset_ID.unique())","623be460":"asset_details","a358a683":"len(train_df.timestamp.unique())","9627cba8":"print(\"min date\", datetime.fromtimestamp(min(train_df.timestamp.unique())))\nprint(\"max date\", datetime.fromtimestamp(max(train_df.timestamp.unique())))","750c5257":"plt.plot(np.diff(train_df.timestamp.unique()))","a1ecd2f1":"esp = train_df.timestamp.unique()[np.insert(np.diff(train_df.timestamp.unique())>60,False,0)]\nval = np.diff(train_df.timestamp.unique())[np.diff(train_df.timestamp.unique())>60]\n\npd.DataFrame({'day':[datetime.fromtimestamp(timestamp).strftime(\"%d\/%m\/%y\") for timestamp in esp],'delta':val}).groupby('day').agg([np.size,np.mean])","d22a84ec":"esp = train_df.timestamp.unique()[np.insert(np.diff(train_df.timestamp.unique())>120,False,0)]\nval = np.diff(train_df.timestamp.unique())[np.diff(train_df.timestamp.unique())>120]\n\npd.DataFrame({'day':[datetime.fromtimestamp(timestamp).strftime(\"%d\/%m\/%y\") for timestamp in esp],'delta':val}).groupby('day').count()","631b27f9":"#train_df = train_df[~train_df.Target.isna()]","69bc23e6":"train_df['timestamp'].hist(by=train_df['Asset_ID']);","89c1283e":"train_df['Count'].hist(by=train_df['Asset_ID'], log=True);","825e062a":"train_df['Open'].hist(by=train_df['Asset_ID'], log=True);","d947aa62":"train_df['Volume'].hist(by=train_df['Asset_ID'], log=True);","655b7d96":"train_df[~np.isinf(train_df['VWAP'])].hist(by=train_df[~np.isinf(train_df['VWAP'])]['Asset_ID'], log=True);","89e1cd61":"for i in train_df.Asset_ID.unique():\n    df_p = train_df[['Close','timestamp']][train_df.Asset_ID==i]\n    df_p.index = df_p.timestamp\n    df_p.Close  = df_p.Close\/df_p.Close.iloc[0]\n    df_p = df_p.drop(columns=['timestamp'])\n    plt.plot(df_p,label=asset_details[asset_details.Asset_ID == i].Asset_Name.values[0])\n    \nplt.legend(bbox_to_anchor=(1.05, 1),fontsize='x-small')\nplt.show()","2ba14735":"for i in train_df.Asset_ID.unique():\n    df_p = train_df[['Close','Open','timestamp']][train_df.Asset_ID==i]\n    df_p.index = df_p.timestamp\n    df_p.Close  = np.log(df_p.Close\/df_p.Open)\n    df_p = df_p.drop(columns=['timestamp','Open'])\n    plt.plot(df_p,label=asset_details[asset_details.Asset_ID == i].Asset_Name.values[0])\n    \nplt.legend(loc='upper left',fontsize='x-small')\nplt.show()","87ab9897":"for i in train_df.Asset_ID.unique():\n    \n    df_p = train_df[['Close','timestamp']][train_df.Asset_ID==i]\n    df_p.index = df_p.timestamp\n\n    df_p.Close  = np.log(df_p.Close\/df_p.Close.shift())\n    \n    #train_df.loc[train_df.Asset_ID==i,'log_ret_diff'] = df_p.Close \n    \n    df_p = df_p.drop(columns=['timestamp'])\n    plt.plot(df_p,label=asset_details[asset_details.Asset_ID == i].Asset_Name.values[0])\n    \nplt.legend(loc='upper left',fontsize='x-small')\nplt.show()","43a03191":"for i in train_df.Asset_ID.unique():\n    df_p = train_df[['Volume','timestamp']][train_df.Asset_ID==i]\n    df_p.index = df_p.timestamp\n    df_p = df_p.drop(columns=['timestamp'])\n    plt.plot(df_p,label=asset_details[asset_details.Asset_ID == i].Asset_Name.values[0])\n    \nplt.legend(loc='upper left',fontsize='x-small')\nplt.show()","5da47565":"for i in train_df.Asset_ID.unique():\n    df_p = train_df[['Open','High','Low','Close','timestamp']][train_df.Asset_ID==i]\n    df_p.index = df_p.timestamp\n    df_p['GK_vol'] = (1 \/ 2 * np.log(df_p.High \/ df_p.Low) ** 2 - (2 * np.log(2) - 1) * np.log(df_p.Close \/ df_p.Open) ** 2).astype('float32')\n    df_p = df_p.drop(columns=['Open','High','Low','Close','timestamp'])\n    plt.plot(np.log(df_p.rolling(2400).mean()),label=asset_details[asset_details.Asset_ID == i].Asset_Name.values[0])\n    \nplt.legend(loc='upper left',fontsize='x-small')\nplt.show()","737fb652":"import seaborn as sns\n\nsns.color_palette(\"tab10\")\nsns.kdeplot(data=train_df, x='Target', hue='Asset_ID', fill=True, common_norm=False, alpha=0.4)\nplt.show()","6e7c818a":"%%time\n\nfeatures_to_aggregate = ['log_ret']\ntrain_df['log_ret'] = np.log(train_df.Close\/train_df.Open)\n\ndict_weights = {}\n\nfor i in range(asset_details.shape[0]):\n    dict_weights[asset_details.iloc[i,0]] = asset_details.iloc[i,1]\n    \ntrain_df['weights'] = train_df.Asset_ID.map(dict_weights).astype('float32')\n\nt, w, A_id = (train_df[col].values for col in ['timestamp','weights','Asset_ID'])\nids, index = np.unique(t, return_index=True)\n\nValues = train_df[features_to_aggregate].values\nsplits = np.split(Values, index[1:])\nsplits_w = np.split(w, index[1:])\nsplits_A_id = np.split(A_id, index[1:])\n\nret_small = []\nret = []\n\nfor time_id, x, w, A_id in zip(ids.tolist(), splits, splits_w, splits_A_id):\n    outputs = np.float32(np.sum((x.T*w),axis=1)\/sum(w))\n    ret_small.append(outputs)\n    ret.append(np.tile(outputs, (len(w), 1)))\n","53d07f4f":"market_ret = pd.DataFrame({'timestamp':ids,'log_ret_M':np.concatenate(ret_small,axis=0).flatten()})","cfa46f2c":"#train_df['log_ret_M'] = np.concatenate(ret,axis=0)","66e24081":"market_ret.index = market_ret.timestamp","de554bc1":"train_df.head()","b8e7170f":"for i in train_df.Asset_ID.unique():\n    df_p = train_df[['log_ret','timestamp']][train_df.Asset_ID==i]\n    df_p.index = df_p.timestamp\n    df_p = df_p.drop(columns=['timestamp'])\n    plt.plot(df_p.cumsum(),label=asset_details[asset_details.Asset_ID == i].Asset_Name.values[0])\n    \nplt.plot(market_ret.log_ret_M.cumsum(),label='Market',c='Black')\nplt.legend(loc='upper left',fontsize='x-small')\nplt.show()","bf4d361a":"train_df['log_ret_M'] = np.concatenate(ret,axis=0).flatten()\ntrain_df['log_ret_M2'] = train_df['log_ret_M']**2\ntrain_df['log_ret_Mr'] = train_df['log_ret_M'] * train_df['log_ret']","6fbed375":"for i in train_df.Asset_ID.unique():\n    df_p = train_df[['Asset_ID','log_ret_Mr','log_ret_M2','timestamp']][train_df.Asset_ID==i]\n    df_p['mr_rolling'] = df_p['log_ret_Mr'].rolling(window=3750, min_periods=3750).mean()\n    df_p['m2_rolling'] = df_p['log_ret_M2'].rolling(window=3750, min_periods=3750).mean()\n    df_p['beta'] = df_p['mr_rolling'] \/ df_p['m2_rolling']\n    \n    plt.plot(df_p['beta'],label=asset_details[asset_details.Asset_ID == i].Asset_Name.values[0])\n    \nplt.legend(loc='upper left',fontsize='x-small')\nplt.show()","e4bff9e5":"for i in train_df.Asset_ID.unique():\n    \n    print('Asset: '+asset_details[asset_details.Asset_ID == i].Asset_Name.values[0])\n    \n    \n    df_p = train_df[['log_ret','log_ret_M','Target','timestamp']][train_df.Asset_ID==i]\n    \n    df_p.index = df_p.timestamp\n    \n    df_p['log_ret_res'] = df_p['log_ret'] - df_p['log_ret_M']\n    \n    #corr_time = df_p.groupby(df_p.index\/\/(3750*60)).corr().loc[:,\"Target\"].loc[:,'log_ret']\n    #corr_time2 = df_p.groupby(df_p.index\/\/(3750*60)).corr().loc[:,\"Target\"].loc[:,'log_ret_M']\n    corr_time3 = df_p.groupby(df_p.index\/\/(3750*60)).corr().loc[:,'log_ret'].loc[:,'log_ret_M']\n    #corr_time4 = df_p.groupby(df_p.index\/\/(3750*60)).corr().loc[:,\"Target\"].loc[:,'log_ret_res']\n    \n    #print('Asset: Correlation between target and timestamp log returns: '+str(corr_time.mean()))\n    #print('Asset: Correlation between target and timestamp market log returns: '+str(corr_time2.mean()))\n    print('Asset: Beta: '+str(corr_time3.mean()))\n    #print('Asset: Correlation between target and timestamp residualisez log returns: '+str(corr_time4.mean()))\n    \n    \"\"\"\n    \n    corr_time.plot();\n    plt.xticks([])\n    plt.ylabel(\"Correlation\")\n    plt.title(\"Correlation between target and timestamp log returns\");\n    plt.show();\n    \n    corr_time2.plot();\n    plt.xticks([])\n    plt.ylabel(\"Correlation\")\n    plt.title(\"Correlation between target and timestamp market log returns\");\n    plt.show();\"\"\"\n    \n    corr_time3.plot();\n    plt.xticks([])\n    plt.ylabel(\"Correlation\")\n    plt.title(\"Correlation between timestamp log return and timestamp market log return\");\n    #plt.show();\n    \n    \"\"\"\n    corr_time4.plot();\n    plt.xticks([])\n    plt.ylabel(\"Correlation\")\n    plt.title(\"Correlation between target and timestamp residualisez log returns\");\n    plt.show();\"\"\"","5fc78462":"#create dataframe with returns for all assets\nall_assets_2021 = pd.DataFrame([])\nfor asset_id, asset_name in zip(asset_details.Asset_ID, asset_details.Asset_Name):\n  asset = train_df[train_df[\"Asset_ID\"]==asset_id].set_index(\"timestamp\")\n  asset = asset.loc[totimestamp('01\/01\/2021'):totimestamp('01\/05\/2021')]\n  asset = asset.reindex(range(asset.index[0],asset.index[-1]+60,60),method='pad')\n  lret = log_return(asset.Close.fillna(0))[1:]\n  all_assets_2021 = all_assets_2021.join(lret, rsuffix=asset_name, how=\"outer\")\nplt.imshow(all_assets_2021.corr());\nplt.yticks(asset_details.Asset_ID.values, asset_details.Asset_Name.values);\nplt.xticks(asset_details.Asset_ID.values, asset_details.Asset_Name.values, rotation='vertical');\nplt.colorbar();","007b421c":"# time stamps","d4f8065d":"# data distribution","402b8253":"# Returns\n\nReturns in time frame v.s. return between time frames ?","f459111d":"remove target na ?","a7b091f0":"# Target","ca5c0e04":"# Correlation\n\nTO DO: over time","c9ab5bd7":"### Training data is in the competition dataset as usual","bc073ab5":"# Correlation with target - Beta\n\nResidualised Log returns appears generally negatively correlated with the target. I exploit this idea in this ultra fast baseline notebook: https:\/\/www.kaggle.com\/lucasmorin\/minimal-fast-baseline","d97585f2":"# Normalized prices","dbbeb8a8":"# Price on log scale \/ compared to market","c3ffb5a5":"# Cumulated log returns","28696779":"# Volatility","14bc233e":"# Introduction to Crypto Forecasting\n\nOn-going work.","02d42b50":"# volume","6d4dcb8b":"# Asset Details","ebbea305":"# Market","86584617":"Garmann-Klass estimator for volatility - smoothed. Maybe better with inter time volatility ?","e2dd0590":"Seems to have outliers (ultra low volume ?)","8790fc2b":"Some data appears with less history. Esp. Asset #10 and #11.","8ebbe303":"Weird things happened on 16\/10\/2019 and 23\/10\/2019","2aafc889":"# outliers"}}