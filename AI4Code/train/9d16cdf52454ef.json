{"cell_type":{"cf798b32":"code","b208154a":"code","601b991d":"code","a3b6e186":"code","81a4400b":"code","36f43e09":"code","b4ad960e":"code","8f3f5319":"code","242e9d2c":"code","1b1f6b5e":"code","7defb171":"code","3c0a99eb":"code","27679e30":"code","f20aceb0":"code","8a6786b2":"code","a5c358a2":"code","a426cf9f":"code","874cce8a":"code","63742701":"code","192906b0":"code","ec5d9e8f":"code","e635ecae":"code","20581121":"code","737612a2":"code","15c4d4a5":"code","05d04305":"code","0855a7aa":"code","6ebc1085":"code","65c17b70":"code","ed351613":"code","9031ece1":"code","d7cac4f8":"code","0390278f":"code","efd0c73d":"code","54cb0b7a":"code","19243360":"code","393d6d3a":"code","64e32f9f":"code","f47a4877":"code","20e7e52f":"markdown","7101cb3f":"markdown","3835593a":"markdown","09f883fe":"markdown","4fc48596":"markdown","b656427a":"markdown","96c8aa5e":"markdown","de81d96c":"markdown","204fcafb":"markdown","2aba94b3":"markdown","b7bc9a67":"markdown","f530b6c3":"markdown","a8c4e6c3":"markdown","10dd3e74":"markdown","3617070e":"markdown","8355d3a7":"markdown","653119d5":"markdown","a79c4b56":"markdown","2bca1c8b":"markdown","c7a17db7":"markdown","1e767dcd":"markdown","0148d36b":"markdown","5a489d2b":"markdown","1f0e9034":"markdown","1452bde7":"markdown","ca06bc44":"markdown","02065e48":"markdown","935f135a":"markdown","f9e4c99e":"markdown","2e3805b9":"markdown"},"source":{"cf798b32":"# All Package Imports\nimport pandas as pd\nimport numpy as np\nimport os\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport time\n\n# import sklearn packages for modelling\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb","b208154a":"# List files available\nprint(os.listdir(\"..\/input\"))","601b991d":"# load and print head of the a_train dataset\na_train = pd.read_csv(r'..\/input\/application_train.csv')\na_train.head()\n\n# keep original version of a_train for exploratory visualisations\noriginal_a_train = a_train.copy(deep = True)","a3b6e186":"# Train matrix shape\na_train.shape","81a4400b":"# load and print head of the a_train dataset\na_test = pd.read_csv(r'..\/input\/application_test.csv')\na_test.head()","36f43e09":"# Test matrix shape\na_test.shape","b4ad960e":"# function to find missing values in datafame so we can reuse if we look at other data sources\n# including proper docstring\ndef missing_values_table(df):\n        \n        # sum of missing values\n        mis_val = df.isnull().sum()\n        \n        # % of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # concat table\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% Missing Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0\n                                                             ].sort_values('% Missing Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"The dataframe has {} columns.\\n\".format(str(df.shape[1])),      \n                \"There are {} columns that have missing values.\\n\".format(str(mis_val_table_ren_columns.shape[0])),\n               \"There are {} columns that have no missing values\".format(int(df.shape[1]) - int(mis_val_table_ren_columns.shape[0])) )\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n    \n","8f3f5319":"missing_values_table(a_train)","242e9d2c":"# Columns Data Types\ntrain_dtypes = pd.DataFrame(a_train.dtypes.value_counts()).reset_index()\ntrain_dtypes.columns = ['dtypes', 'column count']\n\ntrain_dtypes","1b1f6b5e":"# create dict object from columns and datatypes\ncolumns = a_train.columns.to_series().groupby(a_train.dtypes).groups\nfor key in columns.keys():\n    print('\\nData Type {} Columns:'.format(key))\n    pprint(list(columns[key]))","7defb171":"# desribe the categorical data\na_train.loc[:, a_train.dtypes == np.object].describe()","3c0a99eb":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in a_train:\n    if a_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(a_train[col].unique())) <= 2:\n            print(\"{} was encoded\".format(col))\n            # Train on the training data\n            le.fit(a_train[col])\n            # Transform both training and testing data\n            a_train[col] = le.transform(a_train[col])\n            a_test[col] = le.transform(a_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","27679e30":"# one-hot encoding of categorical variables\na_train = pd.get_dummies(a_train)\na_test = pd.get_dummies(a_test)\n\nprint('Training Features shape: ', a_train.shape)\nprint('Testing Features shape: ', a_test.shape)\ntrain_labels = a_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\na_train, a_test = a_train.align(a_test, join = 'inner', axis = 1)\n\n# Add the target back in\na_train['TARGET'] = train_labels\nprint('Aligned Training Features shape: ', a_train.shape)\nprint('Aligned Testing Features shape: ', a_test.shape)","f20aceb0":"# Plot TARGET distribution\na_train['TARGET'].value_counts()\na_train['TARGET'].value_counts().plot(kind='bar', figsize=(10,5), color = ['grey', 'cornflowerblue'])\nplt.xlabel('Target Class')\nplt.ylabel('Count') \nplt.show()","8a6786b2":"# Create function for plotting kde with scale reversing\ndef plot_kde(df, var, reverse_scale = False):\n    \n    plt.figure(figsize = (12, 6))\n    \n    if reverse_scale == True:\n        r = -1\n    else:\n        r = 1\n    \n    # KDE plot of loans that were repaid on time\n    sns.kdeplot(df.loc[df['TARGET'] == 0, var] * r, label = 'target: negative class', color = 'grey', shade = True)\n\n    # KDE plot of loans which were not repaid on time\n    sns.kdeplot(df.loc[df['TARGET'] == 1, var] * r, label = 'target: positive class', color = 'cornflowerblue', shade = True)\n\n    # Labeling of plot\n    plt.xlabel('{}'.format(var)); plt.ylabel('Density'); plt.title('KDE for {}'.format(var));\n    plt.show()\n    plt.close()\n\n# plot age kde plot\nplot_kde(a_train,'DAYS_BIRTH', True)","a5c358a2":"# iterate over float variables and plot KDE\nfor column in original_a_train.loc[:, (original_a_train.dtypes == np.float64)].columns.values:\n    # do not plot target \n    if column != 'TARGET':\n        # reverse axis if values are negative\n        if (original_a_train[column].median() < 0):\n            plot_kde(a_train,column, reverse_scale = True)\n        else:\n            plot_kde(a_train,column)","a426cf9f":"def analyse_outliers(df, column):\n    \n    # Print Summary Statistics\n    print('Summary Statistics:\\n')\n    print(df[column].describe())\n\n    # find mean and std\n    outlier_df = df[column]\n    std = outlier_df.std()\n    print('\\nStandard Deviation: ', std)\n    mean =  outlier_df.mean()\n    print('Mean: ', mean)\n\n    # how many std is the max\n    max_outlier = int((outlier_df.max() - mean) \/ std)\n\n    # separate outliers over 2 std from mean\n    outliers_l = outlier_df[(outlier_df < mean - 2 * std)]\n    outliers_h = outlier_df[(outlier_df > mean + 2 * std)]\n    print('\\nThere are {} low end outliers in the {} dataset'.format(len(outliers_l), column ))    \n    print('There are {} high end outliers in the {} dataset'.format(len(outliers_h), column ))\n    print('The max value is {} standard deviations from the mean'.format(max_outlier))\n    \n    return mean, std\n\nincome_mean, income_std = analyse_outliers(a_train, 'AMT_INCOME_TOTAL')   ","874cce8a":"# define function for plotting categorical bar charts for remaining variables\ndef categorical_plot(df, variable):\n    \n    plt.figure(figsize = (11, 5))\n    \n    df_high = df[df['TARGET'] == 1].groupby(variable)['TARGET'].agg('count')\n    df_var = df.groupby(variable)['TARGET'].agg('count')\n    categorical = df_high.divide(df_var, fill_value = 0) * 100\n\n    # Convert back to df\n    df_categorical = categorical.to_frame().reset_index().sort_values('TARGET', ascending = True)\n\n    # Create plot in Plotly for interactive visualisation (with some Starling colours)\n    ax = df_categorical.plot(x = variable, y = 'TARGET', kind = 'barh', figsize=(10,10), color = 'cornflowerblue')\n    ax.set_xlabel('Target: Positive %')\n    ax.set_ylabel(variable)\n    plt.title('% postive plot for {}'.format(variable.lower()));\n    plt.show()\n    plt.close()\n","63742701":"# iterate over np.object columns and plot\nfor column in original_a_train.loc[:, original_a_train.dtypes == np.object].columns.values:\n    categorical_plot(original_a_train, column)","192906b0":"# Find correlations with the target(takes a while due to many features)\ncorrelations = a_train.corr()['TARGET'].sort_values()","ec5d9e8f":"# print ordered list of correlations\nprint('Most Positive Correlations:\\n')\nprint(correlations.sort_values(ascending = False).head(16))\nprint('\\nMost Negative Correlations:\\n')\nprint(correlations.head(15))","e635ecae":"# create X_train, y_train\nX_train = a_train.drop('TARGET', axis = 1)\ny_train = a_train['TARGET']\nX_test = a_test\n\nX_train = X_train.drop('SK_ID_CURR', axis = 1)\nX_test = X_test.drop('SK_ID_CURR', axis = 1)\n\n# Feature names\nfeatures = list(X_train.columns)","20581121":"# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(X_train)\n\n# Transform both training and testing data\nX_train = imputer.transform(X_train)\nX_test = imputer.transform(X_test)\n\n# Repeat with the scaler\nscaler.fit(X_train)\ntrain = scaler.transform(X_train)\ntest = scaler.transform(X_test)\n\nprint('Training data shape: ', X_train.shape)\nprint('Testing data shape: ', X_test.shape)","737612a2":"# Using Cross Validation to find a good model\nnum_folds = 5\nseed = 1\nscoring = 'roc_auc'\nmodels = []\n\n# Typical Classifiers\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DTC', DecisionTreeClassifier()))\nmodels.append(('GNB', GaussianNB()))\nmodels.append(('RF', RandomForestClassifier()))\n\n# iterate over models and print cross val scores\nresults = []\nnames = []\nprint('Please wait while models train..')\nfor name, model in models:\n    \n    # start timer\n    start = time.time()\n    \n    # Cross Validation\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    \n    # stop timing\n    end = time.time()\n    time_run = (end - start)\/60\n    output = \"{}--> auroc: {}   (Training Time: {}mins)\".format(name, cv_results.mean(), time_run)\n    \n    print(output)","15c4d4a5":"# Train LR Model\nRF = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\nRF.fit(X_train,y_train)\n\n# Extract feature importances\nfeature_importance_values = RF.feature_importances_\nfeature_importances = pd.DataFrame({'Feature': features, 'Importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = RF.predict_proba(test)[:, 1]\n\n# Make a submission dataframe\nsubmit = a_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)\n","05d04305":"# Function to plot feature importance\ndef plot_feature_importance(df):\n\n    # Normalize the feature importances to add up to one\n    df['Importance_normalized'] = df['Importance'] \/ df['Importance'].sum()\n    df = df.sort_values('Importance_normalized', ascending = True).tail(20)\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 16))\n\n    ax = df.plot(x = 'Feature' , y = 'Importance_normalized', kind = 'barh', figsize=(10,10), color = 'blue')\n    \n    # Plot labeling\n    plt.xlabel('Importance')\n    plt.title('Feature Importances')\n    plt.show()\n    \n    # return top 20 features\n    return(df['Feature'])\n\ntop20 = plot_feature_importance(feature_importances)","0855a7aa":"# import bureau data\nbureau = pd.read_csv(r'..\/input\/bureau.csv')\nbureau.head()","6ebc1085":"# create feature dataframe\nbureau_agg = bureau['SK_ID_CURR'].unique()\nbureau_agg = pd.DataFrame(bureau_agg, columns = ['SK_ID_CURR'])","65c17b70":"# previous loan count\nprevious_loan_counts = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'})\nprevious_loan_counts.head()\n\nbureau_agg = bureau_agg.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')","ed351613":"# active loan count\nactive_loan_counts = bureau[bureau['CREDIT_ACTIVE'] == 'Active'].groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'active_loan_counts'})\nactive_loan_counts.head()\n\n# join new features\nbureau_agg = bureau_agg.merge(active_loan_counts, on = 'SK_ID_CURR', how = 'left')\n\n# fill na\nbureau_agg = bureau_agg.fillna(0)","9031ece1":"# join additional features onto train and test\na_train_features = a_train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\na_test_features = a_test.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\na_train_features = a_train_features.fillna(0)\na_test_features = a_test_features.fillna(0)","d7cac4f8":"# plot kde of new features\nplot_kde(a_train_features, 'previous_loan_counts')\nplot_kde(a_train_features, 'active_loan_counts')","0390278f":"print('Training data shape: ', a_train_features.shape)\nprint('Testing data shape: ', a_test_features.shape)","efd0c73d":"# create X_train, y_train\nX_train = a_train_features.drop('TARGET', axis = 1)\ny_train = a_train_features['TARGET']\nX_test = a_test_features\n\nX_train = X_train.drop('SK_ID_CURR', axis = 1)\nX_test = X_test.drop('SK_ID_CURR', axis = 1)\n\n# Feature names\nfeatures = list(X_train.columns)\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(X_train)\n\n# Transform both training and testing data\nX_train = imputer.transform(X_train)\nX_test = imputer.transform(X_test)\n\n# Repeat with the scaler\nscaler.fit(X_train)\ntrain = scaler.transform(X_train)\ntest = scaler.transform(X_test)\n\nprint('Training data shape: ', X_train.shape)\nprint('Testing data shape: ', X_test.shape)\n\n# Train LR Model\nRF = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\nRF.fit(X_train,y_train)\n\n# Extract feature importances\nfeature_importance_values = RF.feature_importances_\nfeature_importances = pd.DataFrame({'Feature': features, 'Importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = RF.predict_proba(test)[:, 1]\n\n# Make a submission dataframe\nsubmit = a_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_features.csv', index = False)\n\ntop20 = plot_feature_importance(feature_importances)","54cb0b7a":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ \/ k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] \/ k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","19243360":"submission, fi, metrics = model(a_train_features, a_test_features)\nprint('Baseline metrics')\nprint(metrics)","393d6d3a":"def plot_feature_importances(df, num_bars = 15):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n\n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:num_bars]))), \n            df['importance_normalized'].head(num_bars), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:num_bars]))))\n    ax.set_yticklabels(df['feature'].head(num_bars))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","64e32f9f":"fi_sorted = plot_feature_importances(fi, 25)","f47a4877":"submission.to_csv('baseline_lgb_features.csv', index = False)","20e7e52f":"### Iterative Graphing\n\nWhilst not the prettiest visualisations, we can use python to quickly produce multiple iterations over many variables for this type of graph using the function we defined. \n\nWe can use this to quickly observe many metrics, pull out immediate trends and spot any issues with our data **quickly**, which is the purpose of this kind of initial EDA and model building. \n\nIn more detailed studies we could spend time producing more detailed visualisations if there is more to understand.","7101cb3f":"### Trends from Numeric Visualisations\n\n#### Feature Outliers\n\nOne thing that is immediately obvious from these kde plots is that some of the variables appear to have huge outliers, and so the kde plot is highly skewed. This means we see a tall, sharp peak on one part of the axis followed by a thin layer on the bottom. It is hard to visualise data that is this skewed, and for these plot we need to investigate further these outliers and (if possible) determine whether they are legitimate or due to error. I will look at this in the next section.\n\n#### Other intesting trends\n\n* Days since last phone change shows that those who have changed their phones very recently and those who haven't changed their phone is years are more likely to default on a loan. \n* The EXT_SOURCE plots all appear to hold information for predicting default, as we see quite a strong separation in the distributions\n* Many of the variables are hard to understand, and appear to be scaled aggregations (avgs, modes, medians) of more detailed metrics.","3835593a":"## Continued Exploratory Data Analysis\n### Looking at trends in the Categorical Variables\n\nAs mentioned above, we can plot proportions of target being the positiive class against each categorical variable value to see which groups have higher default rates.","09f883fe":"We can see there is not much in it between LR and Random Forest both on the evaluation metric and training time. As we progress our solution we should aim to produce a more optimised model, and as such a more complex model such as Random Forest (which is an ensemble method) will gives us more flexibility when we start training hyperparameters. There are also fair more effective algorithms which I could explore, but they are slightly too complex to cover in this inital notebook, or take a very long time to train on a dataset this large.\n\nSo to create our baseline model I will use Random Forest in this instance. \n\n### Training Model and Test Predictions","4fc48596":"## Using an LGBM Model\n\nLGBM tends to do really well in competitions so I want to try it out here!","b656427a":"## Feature Engineering\n\nWe can now try some quick feature engineering using the other datasets. ","96c8aa5e":"### Age\n\nWe can see straight away that age seems to play a big factor in defaulting on a loan. There is a very clear elevation to default in the younger age groups. This is interesting knowledge to have since in future we could use this kind of knowledge to optimise our model if we were removing features (we would not remove this one) and we could look to engineer further more detailed features using this knowledge.","de81d96c":"## Continued Exploratory Data Analysis\n### Target Variable Distribution\n\nLet's take a look at the distribution of the target variable, ideally we would have a fairly balanced dataset. Many machine learning models produce a undesired fit on highly imbalanced datasets, as most of them train for accuracy. We can see below that the classes are infact quite imbalanced, with the positive class being under represented. There are multiple ways to deal with an unbalanced classification problem which we could go on to explore in a more detailed notebook.","204fcafb":"#### Model Performance\nWe now have a baseline dataset for submission. The ROC score for the model upon submission was: **0.653**. \n\n0.5 is not better than random guessing, and 0.80 was a winning score in the competition. For a baseline model this is quite satisfying. However, this is the simple model which we can now aim to build on and improve by:\n\n* Enriching the dataset with additional data (Adding Features)\n* Create new features from existing data (Feature Engineering)\n* Tuning Hyperparameters to improve the model performance\n* Testing more cutting edge machine learning and deep learning models\n\n#### Feature Importance\n\nBelow is a function I have created to plot feature importances. Looking at the plot we can immediately see that some of our exploratory data analysis had highlighted features which were important.","2aba94b3":"One thing that really stands out above is how much of an outlier the maximum value is at 492 standard deviations from the mean. 177M salary could potentially be accurate, and with more information about the data this may be possible to find out. Regardless, it is possible that this value will really skew a model fit to the data, and could potentially be removed, particularly if it is found to be an error.\n\nThis kind of analysis could be repeated for more of the features, and the written above function could be reused to enable this.","b7bc9a67":"### Re-run Random Forest Model\n\nNow that we have an enriched train and test dataset, lets retrain the RF model under the same conditions.","f530b6c3":"We can now see there split of dtypes across the columns. One thing that jumps out is that a lot of the columns are int or float compared to object (or string) columns. \n\nHowever, when we look closer at the column names, we can see that many of the int columns are infact 'FLAG' and take the value 0 or 1. So these are essentially categorical variables which have already been binary encoded. This is ideal for machine learning, as we would need to convert the data set to a matrix of numeric values to ensure that the data set can be used by the different ML algorithms we will want to explore later in the workbook.\n\nThere are also a large number of float columns, which appear to be a mixture of customer information and derived summary statistics (avg, modes etc). These variables will likely need scaling to prevent assigning to much weight to any variables which take high values.\n\nLets now take a look at the categorical columns.","a8c4e6c3":"### Trends from Categorical Visualisations\n\n* Income type, Education and occupation type appear to have quite a range when it comes to default rate. Which makes sense, as you would imagine that those will lower income jobs are more likely to be unable to repay a loan. \n* Day of application has some very minor variation, but you would hope that the day of the week someone applies on would be less linked to their likelihood of default. Although until obeserving the trend, we can never simply say that for sure.\n* owning car or realty seems to have little affect, which is interesting as you would perhaps link ownership with more financial stability. But the trend shows that once again we should trust the data over our initial intuition.","10dd3e74":"### Columns Summary Statistics\n\nFrom the previous section we can immediately see that we have a lot of columns and it is difficult to understand what they contain by using .head() alone. So it makes sense to get some more summary statistics about the columns.","3617070e":"So there are 121 features in the train and test data, with the train data having an additional target column. We can see that the test set is approximately a sixth of the size of the training set.","8355d3a7":"## Data Transformation (Encoding)\n### Encoding of Categorical Variables\n\nIn order to encode the categorical variables, we need to use two methods. \n\n* Label Encoder - for categorical variables with 2 classes, converting the value to binary (0 or 1)\n* OneHotEncoding - for categorical variables with multiple classes, one hot encoding transformed each class into a new column which is then a binary value.","653119d5":"One thing that jumps out is that there are also some binary categorical columns here, where there are only 2 unique values.  We will need to encode these variables in order to use them in a machine learning model.","a79c4b56":"Despite now seeing that both of the new features are showing in the top list for features importance, this model actually underperforms the original baseline model by approx. 3 AUROC Score. \n\nThat being said feature enrichment and model optimisation will be the path we need to follow to increase the ROC score. With future work, we should be able to increase that ROC score a fair bit more. ","2bca1c8b":"## Exploratory Data Analysis\n\nEDA is useful for us to be able to understand what sort of data we are working with and build up some intuition about trends within the data. \n","c7a17db7":"### Correlations\n\nTo conclude our EDA, we can look at correlation coefficients between features and the target. This gives us a more quantitative measure for the relationships we have just explored visually. Below we will use the encoded variables for the correlations.","1e767dcd":"## Model Fitting\n\nBefore doing any feature engineering, lets run a cross validation against the training set. From this we can determine what the benchmark accuracy metrics are for a variety of models. This can help us narrow down which models are performing well on the data. \n\nOur training and test data has already been encoded suring the EDA steps. One final thing to do before passing the train data into a cross validation step would be to scale the numeric quantities, so that features with large values don't get assigned far greater weight in some machine learning algorithms.","0148d36b":"This agrees with some of the insight we were able to pull from the visualisations. Target correlates perfectly with itself as expected (almost acts as a good check for our code. Then we can see that age has a strong positive correlation, which is actually reversed because age is negative.\n\nWe also see that the EXT_SOURCE variables do seem to hold some information that will help us predict the target class. ","5a489d2b":"TO DO:\n* Visualisations \/ kde\n* Find anomalies (programatically if possible)\n* Feature Engineer some features\n* Fit initial models with and without features\n* Test a final model","1f0e9034":"# Home Credit Default Risk Modelling\n\n\n## Introduction\n\nI am no expert in the field of Risk modelling, but I am aware that this is typical process in many financial services companies. Any organisation that gives Credit to a customer, whether that customer is a typical consumer, business or government organisation, should be able to understand their risk (and potential exposure) should the client default on the loan. This means that Credit Risk modelling is important and is worth learning more about.\n\n## Imports","1452bde7":"There are a lot of missing values in some of the columns (c. 50% or more). We will need to impute these values for many machine learning algorithms to work, we can consider the imputation method later on once we are ready to preprocess the data before loading into the model. Another option would be to drop rows with lots of missing values, but these could be very important to fitting the model. Likewise we can drop the columns which have a high number of missing values, but the information we do have could still be useful to training a good model. For now it makes sense to keep these columns, and work on imputing or removing features once we are trying to optimise fitting a model later.","ca06bc44":"### Missing Values\n\nMany machine learning models are intolerant to missing values, so its important to understand their distribution so that we can come up with an appopriate strategy for handling them.","02065e48":"### Visualising affect on Target variable using Proportion Target Positive plots or Kernal Density Estimation (KDE) plots\n\nIn order to help us understand the affect an independent variable is having on our target of default or not default, we should ideally visualise the data.  A histogram will be dependent on the underlying distribution of the independent variable, i.e. if there are lots of young people in the dataset, then the positive class may appear to be higher in this group simply because there are more of both classes. There are a couple of ways around this which can help us understand the data better.\n\n1.  For categorical values, we can plot the proportion (%) of each category that has a positive class\n2. For continuous variables, we can use a KDE plot. This plot estimates the histogram as a probability density function (pdf) which is essentially a smoothed histogram, where the area under the chart sums to a probability of 1. This is useful for us to see how the probability of a negative or positive target class changes with different ranges of the independent variable. \n\nBelow is an example KDE plotted for Age using a function I have written, which enables you to pick you variable but also reverse the scale when required (say for Age which is in minus days)","935f135a":"First and foremost, we need to understand the data which we have for this model. There are multiple datasets, but we will need to focus on the application train (and application test) datasets first and then use the other data to enrich the train and test with more features to improve the model. \n\nSo the first objective in this notebook will be to:\n* Exploratory Data Analysis\n* Data Transformation (Encoding)\n* Data Transformation (Scaling\/Normalisation)\n* Benchmark Models Cross-validation\n\nThen I can go on to explore (time permitting):\n* Feature engineering using existing dataset\n* Feature engineering using additional data\n\nBut without further ado, lets start by exploring the data to understand intuitively what is contained within it.","f9e4c99e":"## Data Load","2e3805b9":"### Outliers\n\nWe established from the charts above that there are outliers in some of the numeric features above. In a more detailed study I would look to observe some of these in more detail, but for the purposes of this notebook and producing an intial baseline model I want to look at an example. In model optimisation, I could look at how these outliers may affect the model and process them in a way to ensure a model that is not skewed due to collection error.\n\nI will look at total income amount as this is the first plot which displays outlier behaviour above."}}