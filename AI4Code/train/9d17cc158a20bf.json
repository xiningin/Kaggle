{"cell_type":{"80f6c023":"code","e67a4e7c":"code","bee751dd":"code","4102e897":"code","969bd576":"code","f398f683":"code","b80f461f":"code","2463cf87":"code","877a2bf3":"code","6e98494b":"code","4756957f":"code","708b7ec2":"code","0a7cb662":"code","d8c515f4":"code","adfda2f5":"code","c54a5898":"code","3835baac":"code","e857752c":"code","72252348":"code","3ea536f3":"code","b4b8b2b4":"code","0ecd8787":"code","bd945b8f":"code","a58fec24":"code","12b0b2c5":"code","1275b533":"code","c28ed018":"code","5253451b":"code","0b423b7e":"code","72cf370d":"code","75b555b8":"code","10ca1b23":"code","e731168b":"code","aaa8f411":"code","12044392":"code","dd6e3e1b":"code","2d21f85e":"code","b6f1e8e4":"code","24b521d7":"code","4832112f":"code","893798ab":"code","445930ad":"code","184a0998":"code","73f43203":"code","7d95d06c":"code","04b8806f":"code","34b48a6b":"code","54680569":"markdown","294a8a44":"markdown","4df1686e":"markdown","f522a957":"markdown","01931f0a":"markdown","b4b88b76":"markdown","fc63e37d":"markdown","b4a620bd":"markdown","5cfc0f41":"markdown","e553344f":"markdown","14b6d537":"markdown","ebae9c0a":"markdown","36a6c340":"markdown"},"source":{"80f6c023":"# Import the libraries\nimport os\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img","e67a4e7c":"# checking if gpu is available \nimport tensorflow as tf\ntf.test.is_gpu_available()","bee751dd":"def get_default_device():\n    \"\"\"Picking GPU if available or else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","4102e897":"device = get_default_device()","969bd576":"def to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)","f398f683":"data_set = \"dogs-vs-cats\"\n\nimport zipfile \nwith zipfile.ZipFile(\"\/kaggle\/input\/\"+ data_set +\"\/train.zip\",\"r\") as z:\n    z.extractall(\".\")\n    # save all files to kaggle\/files\/images\n    destination = '\/kaggle\/files\/images\/train'\n    z.extractall(destination)\n    \nwith zipfile.ZipFile(\"\/kaggle\/input\/\"+ data_set +\"\/test1.zip\",\"r\") as z:\n    z.extractall(\".\")\n    # save all files to kaggle\/files\/images\n    destination = '\/kaggle\/files\/images\/test'\n    z.extractall(destination)","b80f461f":"def list_full_paths(directory):\n    return [os.path.join(directory, file) for file in os.listdir(directory)]\n\ntrain = pd.DataFrame({'filepath': list_full_paths('\/kaggle\/files\/images\/train\/train')})\ntrain['truth_label'] = np.where(train['filepath'].str.contains('dog'), 'dog', 'cat')\n\ntest = pd.DataFrame({'filepath': list_full_paths('\/kaggle\/files\/images\/test\/test1')})","2463cf87":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test = train_test_split(train, test_size=0.2)","877a2bf3":"train.shape","6e98494b":"test.shape","4756957f":"train.columns","708b7ec2":"test.columns","0a7cb662":"train.head()","d8c515f4":"train_datagen = ImageDataGenerator(\n                    rescale = 1.\/255,\n                    shear_range = 0.2,\n                    zoom_range = 0.2,\n                    rotation_range=40,\n                    width_shift_range=0.2,\n                    height_shift_range=0.2,\n                    horizontal_flip=True,\n                    fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)","adfda2f5":"training_set = train_datagen.flow_from_dataframe(dataframe=X_train, x_col='filepath', y_col='truth_label', class_mode='categorical', target_size = (64, 64), batch_size = 128)\ntest_set = test_datagen.flow_from_dataframe(dataframe=X_test, x_col='filepath', y_col='truth_label', class_mode='categorical', target_size = (64, 64), batch_size = 128)","c54a5898":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n\nbatches_augmented = train_datagen.flow_from_directory('\/kaggle\/files\/images\/', target_size = (512, 512), batch_size = 16, class_mode = 'categorical', seed=1234)\nbatches_real = test_datagen.flow_from_directory('\/kaggle\/files\/images\/', target_size = (512, 512), batch_size = 16, class_mode = 'categorical', seed=1234)\n\nx_batch_augmented, y_batch_augmented = next(batches_augmented)\nx_batch_real, y_batch_real = next(batches_real)\n\nfor i in range(16):\n    image_augmented = x_batch_augmented[i]\n    image_real = x_batch_real[i]\n    \n    title_add_on = \"random image\"\n    if y_batch_augmented[i][1]: title_add_on =  \"dog vs cat\"\n\n    plt.subplot(221)\n    plt.imshow(image_real)\n    plt.title(\"original \" + title_add_on)\n\n        \n    plt.subplot(222)\n    plt.imshow(image_augmented)\n    plt.title(\"augmented \" + title_add_on)\n\n    plt.show()","3835baac":"classifier = Sequential()\n\nclassifier.add(Conv2D(filters=32,kernel_size=(3,3),activation=\"relu\",\n                padding=\"valid\", input_shape = (64,64,3)))\n\nclassifier.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))\n\nclassifier.add(Conv2D(filters=32,kernel_size=(3,3),activation=\"relu\",\n                padding=\"valid\", input_shape = (64,64,3)))\n\nclassifier.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))\n\nclassifier.add(Flatten())\nclassifier.add(Dense(128,activation=\"relu\")) \nclassifier.add(Dense(2,activation=\"sigmoid\")) ","e857752c":"classifier.summary()","72252348":"classifier.compile(optimizer=\"adam\",loss='binary_crossentropy',metrics=['accuracy'])","3ea536f3":"history = classifier.fit(training_set, validation_data = test_set, epochs=20)","b4b8b2b4":"test_set2 = test_datagen.flow_from_dataframe(dataframe=test,\n    directory = '\/kaggle\/files\/images\/test',\n    x_col = 'filepath',\n    y_col = None,\n    class_mode = None,\n    target_size = (64, 64),\n    batch_size = 32,\n    shuffle = False)","0ecd8787":"test_preds = classifier.predict(test_set2, steps = np.ceil(test.shape[0] \/ 32))\n\ntest[\"test_preds\"] = np.argmax(test_preds, axis = 1)\nlabels = dict((v,k) for k,v in training_set.class_indices.items())\n\ntest['test_preds'] = test['test_preds'].map(labels)","bd945b8f":"sample_test = test.sample(64).reset_index(drop = True)\n\nfig = plt.figure(1, figsize = (24, 20))\nfig.suptitle(\"Sample Predictions\")\n\nfor i in range(len(sample_test)):\n    \n    plt.subplot(10, 8, i + 1)\n    image = load_img(sample_test.filepath[i])\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.title(f\"Predicted as {sample_test['test_preds'][i]}\")\n    \nplt.tight_layout()\nplt.show()","a58fec24":"!rm -r .\/*","12b0b2c5":"!unzip ..\/input\/dogs-vs-cats\/train.zip -d .\/new_data","1275b533":"# The path to the directory where the original\n# dataset was uncompressed\noriginal_dataset_dir = '.\/new_data\/train'","c28ed018":"# The directory where we will\n# store our smaller dataset\nbase_dir = '.\/new_data_base'\nos.mkdir(base_dir)\n\n# Directories for our training,\n# validation and test splits\ntrain_dir = os.path.join(base_dir, 'train')\nos.mkdir(train_dir)\nvalidation_dir = os.path.join(base_dir, 'validation')\nos.mkdir(validation_dir)\ntest_dir = os.path.join(base_dir, 'test')\nos.mkdir(test_dir)","5253451b":"# Directory with our training cat pictures\ntrain_cats_dir = os.path.join(train_dir, 'cats')\nos.mkdir(train_cats_dir)\n\n# Directory with our training dog pictures\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\nos.mkdir(train_dogs_dir)\n\n# Directory with our validation cat pictures\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')\nos.mkdir(validation_cats_dir)\n\n# Directory with our validation dog pictures\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\nos.mkdir(validation_dogs_dir)\n\n# Directory with our validation cat pictures\ntest_cats_dir = os.path.join(test_dir, 'cats')\nos.mkdir(test_cats_dir)\n\n# Directory with our validation dog pictures\ntest_dogs_dir = os.path.join(test_dir, 'dogs')\nos.mkdir(test_dogs_dir)","0b423b7e":"import shutil\n# Copy first 1000 cat images to train_cats_dir\nfnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_cats_dir, fname)\n    shutil.copyfile(src, dst)","72cf370d":"# Copy next 500 cat images to validation_cats_dir\nfnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_cats_dir, fname)\n    shutil.copyfile(src, dst)","75b555b8":"# Copy next 500 cat images to test_cats_dir\nfnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(test_cats_dir, fname)\n    shutil.copyfile(src, dst)","10ca1b23":"# Copy first 1000 dog images to train_dogs_dir\nfnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_dogs_dir, fname)\n    shutil.copyfile(src, dst)","e731168b":"# Copy next 500 dog images to validation_dogs_dir\nfnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_dogs_dir, fname)\n    shutil.copyfile(src, dst)","aaa8f411":"# Copy next 500 dog images to test_dogs_dir\nfnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(test_dogs_dir, fname)\n    shutil.copyfile(src, dst)","12044392":"print('total training dog images:', len(os.listdir(train_dogs_dir)))","dd6e3e1b":"from tensorflow.keras import layers\nfrom tensorflow.keras import models","2d21f85e":"from tensorflow.keras.applications import VGG16\n\nconv_base = VGG16(weights='imagenet',\n                  include_top=False, \n                  input_shape=(150, 150, 3))\n\nconv_base.summary()","b6f1e8e4":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(rescale=1.\/255)\nbatch_size = 20","24b521d7":"def extract_features(directory, sample_count):\n    features = np.zeros(shape=(sample_count, 4, 4, 512))\n    labels = np.zeros(shape=(sample_count))\n    \n    generator = datagen.flow_from_directory(\n        directory,\n        target_size=(150, 150),\n        batch_size=batch_size,\n        class_mode='binary')\n    \n    i = 0\n    \n    for inputs_batch, labels_batch in generator:\n        features_batch = conv_base.predict(inputs_batch)\n        \n        features[i * batch_size : (i + 1) * batch_size] = features_batch\n        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n        \n        i += 1\n        if i * batch_size >= sample_count:\n            # Note that since generators yield data indefinitely in a loop, \n            # we must `break` after every image has been seen once.\n            break\n    return features, labels\n","4832112f":"train_features, train_labels = extract_features(train_dir, 2000)\nvalidation_features, validation_labels = extract_features(validation_dir, 1000)\ntest_feature, test_labels = extract_features(test_dir, 1000)","893798ab":"# flattening our input data for dense layers\ntrain_features = np.reshape(train_features, (2000, 4 * 4 * 512))\nvalidation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\ntest_feature = np.reshape(test_feature, (1000, 4 * 4 * 512))","445930ad":"model = models.Sequential()\nmodel.add(layers.Dense(256, activation=\"relu\", input_dim = 4 * 4 * 512))\n\n# adding Dropout layer for regularization\nmodel.add(layers.Dropout(0.5))\n\nmodel.add(layers.Dense(1, activation=\"sigmoid\"))\n\nmodel.summary()","184a0998":"from tensorflow.keras import optimizers\n\nmodel.compile(\n    loss=\"binary_crossentropy\", \n    optimizer=optimizers.RMSprop(lr=2e-5), \n    metrics=[\"acc\"])","73f43203":"history = model.fit(\n    train_features, train_labels, \n    epochs=30, \n    batch_size=20,\n    validation_data=(validation_features, validation_labels))","7d95d06c":"model.save(\".\/cats_and_dogs_vgg16.h5\")","04b8806f":"import matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']","34b48a6b":"epochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc') \nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy') \nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss') \nplt.plot(epochs, val_loss, 'b', label='Validation loss') \nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","54680569":"![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20200219152327\/conv-layers-vgg16.jpg)","294a8a44":"# Cats vs Dogs Classification using CNN Tensorflow.Keras \n- A convolutional neural network (CNN) is a type of artificial neural network used in image recognition and processing that is specifically designed to process pixel data. A CNN uses a system much like a multilayer perceptron that has been designed for reduced processing requirements\n\n- image credits: [Click Here](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/cat-vs-dog.jpp)","4df1686e":"# About Dataset:\n- ### The Asirra (animal species image recognition for restricting access) dataset was introduced in 2013 for a machine learning competition. The dataset includes 25,000 images with equal numbers of labels for cats and dogs.","f522a957":"# We will build a model that takes an image as input and determines whether the image contains a picture of a dog or a cat.","01931f0a":"- Create the neural net model:","b4b88b76":"![](https:\/\/lirp.cdn-website.com\/f499246c\/dms3rep\/multi\/opt\/Convolutional+Neural+Network-637w.jpg)","fc63e37d":"- Prepare dataset for training model:","b4a620bd":"- Model Training ","5cfc0f41":"# Now will be using VGG16\n\n- VGG-16 is a convolutional neural network that is 16 layers deep. You can load a pretrained version of the network trained on more than a million images from the ImageNet database [1]. The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals. \n\n- image credits: [Click Here](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20200219152327\/conv-layers-vgg16.jpg)","e553344f":"# Conclusion:\n- **Both performed Quite well.**","14b6d537":"- plotting images from dataset","ebae9c0a":"image credits: [Click Here](https:\/\/lirp.cdn-website.com\/f499246c\/dms3rep\/multi\/opt\/Convolutional+Neural+Network-637w.jpg)","36a6c340":"![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/cat-vs-dog.jpg)"}}