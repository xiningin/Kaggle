{"cell_type":{"82050c15":"code","571b2495":"code","b3cc4ccd":"code","e6193776":"code","540c60fb":"code","cdcee8fb":"code","7753c294":"code","b9b4c446":"code","a8f41b91":"code","fc0ab756":"code","12e1e1f8":"code","79a52e77":"code","187a6de2":"code","b5e18a71":"code","7a4b0d4c":"code","d3211db3":"code","48555ad9":"code","2fd7b6bd":"markdown","e438aa01":"markdown","9b314815":"markdown","95da6920":"markdown","48870032":"markdown","9acb8e63":"markdown","c4e095b0":"markdown","7005dad0":"markdown","16f16b5b":"markdown","c8dd06be":"markdown","9dcee3d0":"markdown","896ec295":"markdown","fe21293b":"markdown","0bf2625e":"markdown","30662653":"markdown","6908f7ed":"markdown","9e8f46b3":"markdown","1b2bfe47":"markdown","0d648887":"markdown","0f204e27":"markdown","bd4b274c":"markdown"},"source":{"82050c15":"import os\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras import models, layers, regularizers, metrics, optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import VGG19\nfrom scipy import stats\nimport shutil\nimport random\nfrom sklearn.metrics import confusion_matrix","571b2495":"def dataset_basic_info(generator, name):\n        print('The ' + name + ' data set includes ' + str(generator.samples) + ' samples.')\n        print('The ' + name + ' image shapes is ' + str(generator.image_shape))\n        keys = [el for el in generator.class_indices.keys()]\n        print('The ' + name + ' data set includes the following labels: ')\n        print(keys)\n        labels     = generator.labels\n        cat_labels = []\n        for i in range(len(labels)):\n            for j in range(len(keys)):\n                if (labels[i] == j):\n                    cat_labels.append(keys[j])\n                    break\n        occurrences = []\n        for key in keys:\n            counter = 0\n            for i in range(len(cat_labels)):\n                if cat_labels[i] == key:\n                    counter += 1\n            occurrences.append(counter)\n        print(name + ' data set labels frequencies:')\n        weights = {}\n        for i in range(len(keys)):\n            print(keys[i] + ': ' + str(occurrences[i]) + ' (absolute), ' + str(round(occurrences[i]\/float(generator.samples), 3)) + ' (relative).' )\n            weights[i] = generator.samples\/np.array(occurrences[i])*(1.0\/float(len(keys)))\n        \n        return weights\n\n\ndef build_model_cnn(regression_problem, conv_filters, conv_filter_shape, conv_activation_function, conv_padding, conv_pooling_type, conv_pooling_shape, hidden_layers_neurons, hidden_activation_function, L1_coeffs, L2_coeffs, hidden_layers_dropout, final_layer_neurons, final_activation_function, shape, model_optimizer, loss_function, metrics):\n    \n    model = models.Sequential()\n    \n    for i in range(len(conv_activation_function)):\n        \n        if (i == 0):\n            model.add(layers.Conv2D(conv_filters[i],\n                                    (conv_filter_shape[i][0],conv_filter_shape[i][1]), \n                                    activation = conv_activation_function[i], \n                                    padding    = conv_padding[i],\n                                    input_shape = (shape[0],shape[1],shape[2])))             \n        else:\n            model.add(layers.Conv2D(conv_filters[i],\n                                    (conv_filter_shape[i][0],conv_filter_shape[i][1]), \n                                    activation = conv_activation_function[i],\n                                    padding    = conv_padding[i]))\n        \n        if (conv_pooling_type[i] == 'max'):\n            model.add(layers.MaxPooling2D((conv_pooling_shape[i][0],conv_pooling_shape[i][1])))\n        elif (conv_pooling_type[i] == 'avg'):\n            model.add(layers.AveragePooling2D((conv_pooling_shape[i][0],conv_pooling_shape[i][1])))\n        else:\n            'no pooling'\n            \n    model.add(layers.Flatten())\n    \n    for i in range(len(hidden_activation_function)):\n\n        model.add(layers.Dense(hidden_layers_neurons[i], \n                               kernel_regularizer = regularizers.l1_l2(l1 = L1_coeffs[i], l2 =  L2_coeffs[i]),  \n                               activation=hidden_activation_function[i]))\n        if (hidden_layers_dropout[i] > 0.0):\n            model.add(layers.Dropout(hidden_layers_dropout[i]))\n    if regression_problem:\n            model.add(layers.Dense(final_layer_neurons))\n    else:\n            model.add(layers.Dense(final_layer_neurons,activation = final_activation_function))\n            \n    model.compile(optimizer = model_optimizer, loss = loss_function, metrics = metrics)\n    \n    model.summary()\n    \n    return model\n\ndef build_model_pretrained_cnn(pre_trained_model, include_top, regression_problem, hidden_layers_neurons, hidden_activation_function, L1_coeffs, L2_coeffs, hidden_layers_dropout, final_layer_neurons, final_activation_function, model_optimizer, loss_function, metrics):\n    \n    model = models.Sequential()\n    model.add(pre_trained_model)\n    \n    if (include_top == False):\n        \n            model.add(layers.Flatten())\n\n            for i in range(len(hidden_activation_function)):\n\n                model.add(layers.Dense(hidden_layers_neurons[i], \n                                       kernel_regularizer = regularizers.l1_l2(l1 = L1_coeffs[i], l2 =  L2_coeffs[i]),  \n                                       activation=hidden_activation_function[i]))\n                if (hidden_layers_dropout[i] > 0.0):\n                    model.add(layers.Dropout(hidden_layers_dropout[i]))\n            if regression_problem:\n                    model.add(layers.Dense(final_layer_neurons))\n            else:\n                    model.add(layers.Dense(final_layer_neurons,activation = final_activation_function))\n            \n    model.compile(optimizer = model_optimizer, loss = loss_function, metrics = metrics)\n    \n    model.summary()\n    \n    return model\n\ndef display_input_images(generator, max_n_figures, batch_size, grid_size, fig_size):\n    \n    fig_counter = 0\n    for image_batch, label_batch in generator: \n        plt.figure(figsize=(fig_size[0],fig_size[1]))\n        for j in range(batch_size):\n            ax   = plt.subplot(grid_size[0], grid_size[1], j + 1)\n            plt.imshow(image_batch[j])\n            if (label_batch[j][0] == 1):\n                    plt.title('MildDemented')\n            elif (label_batch[j][1] == 1):\n                    plt.title('ModerateDemented')\n            elif (label_batch[j][2] == 1):\n                    plt.title('NonDemented')\n            else:\n                    plt.title('VeryMildDemented')\n            plt.axis(\"off\")\n        plt.show()\n        fig_counter += 1\n        if (fig_counter == max_n_figures): break\n\ndef analyze_performances(hst, epochs):\n    history_dict             = hst.history\n    loss_values              = history_dict['loss']\n    validation_loss_values   = history_dict['val_loss']\n    acc_values               = history_dict['categorical_accuracy']\n    validation_acc_values    = history_dict['val_categorical_accuracy']\n    auc_values               = history_dict['multiclass_AUC']\n    validation_auc_values    = history_dict['val_multiclass_AUC']\n    epochs                   = range(1,len(loss_values) + 1)\n    fig, axes                = plt.subplots(1,3,figsize = (30,10))\n    training_ts              = [loss_values, acc_values, auc_values]\n    validation_ts            = [validation_loss_values, validation_acc_values, validation_auc_values]\n    metric_names             = ['loss', 'categorical accuracy','average multiclass AUC']\n    for i in range(len(axes)):\n        axes[i].plot(epochs,training_ts[i],color = 'r',label = 'training')\n        axes[i].plot(epochs,validation_ts[i],color = 'b',label = 'validation')\n        axes[i].set_xlabel('epoch')\n        axes[i].set_ylabel(metric_names[i])\n        axes[i].set_title(metric_names[i] + ' analysis')\n        axes[i].set_xticks(np.arange(0,epochs[-1] + 1,5))\n        axes[i].set_yticks(np.arange(0,1.1,0.1))\n        axes[i].set_xlim([1,epochs[-1]])\n        axes[i].set_ylim([np.min([np.min(training_ts[i]),np.min(validation_ts[i])]),np.max([np.max(training_ts[i]),np.max(validation_ts[i])])])\n        axes[i].legend()\n    plt.show()\n\n\ndef model_evaluation(model, test_generator):\n    test_loss_1, test_acc_1, test_auc_1 = model.evaluate_generator(test_generator, verbose=0)\n    print('The value of the loss function on the test data set is: ' + str(round(test_loss_1,4)))\n    print('The categorical accuracy of the predictions on the test data set is: ' + str(round(test_acc_1,4)))\n    print('The categorical AUC (i.e., average curve across classes) of the predictions on the test data set is: ' + str(round(test_auc_1,4)))\n    \n    class_labels = list(test_generator.class_indices.keys())\n    predictions = []\n    true        = []\n    ctr         = 0\n    for batch, label in test_generator:\n        prediction = model.predict(batch).argmax(axis = -1)\n        predictions.extend(prediction)\n        true.extend(label.argmax(axis = -1))\n        ctr += len(prediction)\n        if ctr >= len(test_generator.labels):\n            break\n            \n    matrix     = confusion_matrix(true,predictions)\n    rel_matrix = matrix\/np.sum(matrix,axis = 0)\n    fig, axes  = plt.subplots(1,2,figsize = (20,40))\n\n    image1 = axes[0].imshow(matrix, cmap=plt.get_cmap('GnBu'))\n    for (i, j), e in np.ndenumerate(matrix):\n        axes[0].text(j, i, s = str(e), ha='center', va='center')\n    axes[0].set_xticks(np.arange(0,len(class_labels), 1))\n    axes[0].set_xticklabels(class_labels)\n    axes[0].set_yticks(np.arange(0,len(class_labels), 1))\n    axes[0].set_yticklabels(class_labels)\n    axes[0].set_title('Confusion Matrix')\n    \n    image2 = axes[1].imshow(matrix\/np.sum(matrix,axis = 0), cmap=plt.get_cmap('GnBu'))\n    for (i, j), e in np.ndenumerate(rel_matrix):\n        axes[1].text(j, i, s = str(np.round(e,2)), ha='center', va='center')\n    axes[1].set_xticks(np.arange(0,len(class_labels), 1))\n    axes[1].set_xticklabels(class_labels)\n    axes[1].set_yticks(np.arange(0,len(class_labels), 1))\n    axes[1].set_yticklabels(class_labels)\n    plt.subplots_adjust(wspace = 0.5)\n    axes[1].set_title('Confusion Matrix (Relative)')                      \n    plt.show()","b3cc4ccd":"training_path                  =  \"\/kaggle\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/train\/\"\ntest_path                      =  \"\/kaggle\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/test\/\"\nvalidation_split               = 0.20\nregression_problem             = False\ntarget_img_shape_1             = 224\ntarget_img_shape_2             = 224\ntarget_img_channels            = 3\nconv_layers                    = 4\nconv_filters                   = [32,64,128,128]   \nconv_filter_shape              = [[3,3]]*conv_layers\nconv_activation_function       = ['relu']*conv_layers\nconv_padding                   = ['valid']*conv_layers\nconv_pooling_type              = ['max']*conv_layers\nconv_pooling_shape             = [[2,2]]*conv_layers\naugment_data                   = True\nrotation_range                 = 0.1\nwidth_shift_range              = 0.1\nheight_shift_range             = 0.1\nshear_range                    = 0.1\nbrightness_range               = [0.8,1.2]\nzoom_range                     = 0.1\nhorizontal_flip                = False\nfill_mode                      = 'nearest'\nprint_sample_input             = True\nhidden_activation_function     = ['relu']\nhidden_layers_neurons          = [128]\nhidden_layers_L1_coeffs        = [0.00]\nhidden_layers_L2_coeffs        = [0.00]\nhidden_layers_dropout          = [0.00]\nfinal_activation_function      = 'softmax'\nfinal_layer_neurons            = 4\nmodel_optimizer                = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nloss_function                  = 'categorical_crossentropy'\nmetrics                        = [metrics.CategoricalAccuracy(name='categorical_accuracy'),\n                                  metrics.AUC(multi_label = True, name='multiclass_AUC')]\nn_epochs                       = 40\nbatch_size                     = 40\nvalidation_steps               = 50\nvgg_include_top                = False\nvgg_hidden_activation_function = ['relu']\nvgg_hidden_layers_neurons      = [128]\nvgg_hidden_layers_L1_coeffs    = [0.00]\nvgg_hidden_layers_L2_coeffs    = [0.00]\nvgg_hidden_layers_dropout      = [0.00]\nvgg_final_activation_function  = 'softmax'\nvgg_final_layer_neurons        = 4\nvgg_model_optimizer            = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nvgg_n_epochs                   = 40\nvgg_validation_steps           = 50","e6193776":"labels              = ['MildDemented','ModerateDemented','NonDemented', 'VeryMildDemented']\nnew_training_path   = \"..\/files\/alzheimers-dataset-4-class-of-images\/dataset\/training_set\/\"\nnew_validation_path = \"..\/files\/alzheimers-dataset-4-class-of-images\/dataset\/validation_set\/\"\nnew_test_path       = \"..\/files\/alzheimers-dataset-4-class-of-images\/dataset\/test_set\/\"\nshutil.rmtree(new_training_path, ignore_errors=True)\nshutil.rmtree(new_validation_path, ignore_errors=True) \nshutil.rmtree(new_test_path, ignore_errors=True)\n[os.makedirs(new_training_path + label,exist_ok=True) for label in labels]\n[os.makedirs(new_validation_path + label,exist_ok=True) for label in labels]\n[os.makedirs(new_test_path + label,exist_ok=True) for label in labels]\ntraining_label_frequencies   = []\nfor label in labels:\n        training_filenames   = os.listdir(training_path + label + \"\/\") \n        validation_filenames = random.sample(training_filenames, int(len(training_filenames)*validation_split))\n        training_filenames   = [file for file in training_filenames if file not in validation_filenames]\n        test_filenames       = os.listdir(test_path + label + \"\/\") \n        for file in training_filenames:\n            shutil.copy(training_path + label + \"\/\" + file, new_training_path + label + \"\/\" + file)\n        print('Training images transfer complete for label: ' + label + '. # transferred images: ' + str(len(training_filenames)))\n        for file in validation_filenames:\n            shutil.copy(training_path + label + \"\/\" + file, new_validation_path + label + \"\/\" + file)\n        print('Validation images transfer complete for label: ' + label + '. # transferred images: '  + str(len(validation_filenames)))\n        for file in test_filenames:\n            shutil.copy(test_path + label + \"\/\" + file, new_test_path + label + \"\/\" + file)\n        print('Test images transfer complete for label: ' + label + '. # transferred images: '  + str(len(test_filenames)))\n        \n        training_label_frequencies.append(len(training_filenames))\n","540c60fb":"training_label_frequencies = np.array(training_label_frequencies)\ntarget_n_samples           = np.max(training_label_frequencies)\nfor i in range(len(labels)):\n    current_label     = labels[i]\n    n_missing_samples = target_n_samples - training_label_frequencies[i]\n    filenames         = os.listdir(new_training_path + current_label + \"\/\") \n    n_filled          = np.zeros(len(filenames))\n    while (np.sum(n_filled) < n_missing_samples):\n          idx = np.random.randint(0,len(filenames))\n          shutil.copy(new_training_path + current_label + \"\/\" + filenames[idx], new_training_path + current_label + \"\/\" + filenames[idx].replace(\".jpg\", \"_copy_\" + str(int(n_filled[idx] + 1)) + \".jpg\"))\n          n_filled[idx] += 1\n    ","cdcee8fb":"if augment_data:\n    train_datagen   = ImageDataGenerator(rescale            = 1.\/255,\n                                         rotation_range     = rotation_range,\n                                         width_shift_range  = width_shift_range,\n                                         height_shift_range = height_shift_range,\n                                         shear_range        = shear_range,\n                                         brightness_range   = brightness_range,\n                                         zoom_range         = zoom_range,\n                                         horizontal_flip    = horizontal_flip,\n                                         fill_mode          = fill_mode)\nelse:\n    train_datagen   = ImageDataGenerator(rescale = 1.\/255)\n\nvalidation_datagen   = ImageDataGenerator(rescale = 1.\/255)\ntest_datagen         = ImageDataGenerator(rescale = 1.\/255)\ntrain_generator      = train_datagen.flow_from_directory(new_training_path,target_size = (target_img_shape_1, target_img_shape_2), batch_size = batch_size, class_mode = \"categorical\")  \nvalidation_generator = validation_datagen.flow_from_directory(new_validation_path,target_size = (target_img_shape_1, target_img_shape_2), batch_size = batch_size, class_mode = \"categorical\") \ntest_generator       = test_datagen.flow_from_directory(new_test_path,target_size = (target_img_shape_1, target_img_shape_2), batch_size = batch_size, class_mode = \"categorical\") ","7753c294":"if print_sample_input:\n    display_input_images(train_generator, 2, batch_size, [8,5], [30,30])","b9b4c446":"train_labels_weights_dict      = dataset_basic_info(train_generator, 'training')\nvalidation_labels_weights_dict = dataset_basic_info(validation_generator, 'validation')\ntest_labels_weights_dict       = dataset_basic_info(test_generator, 'test')","a8f41b91":"model = build_model_cnn(regression_problem, conv_filters, conv_filter_shape, conv_activation_function, conv_padding, conv_pooling_type, conv_pooling_shape, hidden_layers_neurons, hidden_activation_function, \n                             hidden_layers_L1_coeffs, hidden_layers_L2_coeffs, hidden_layers_dropout, final_layer_neurons, final_activation_function, [target_img_shape_1, target_img_shape_2, target_img_channels], \n                             model_optimizer, loss_function, metrics)","fc0ab756":"early_exit      = EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='min')\nbest_checkpoint = ModelCheckpoint('.best_fit.hdf5', save_best_only=True, monitor='val_multiclass_AUC', mode='max')\n\nhst = model.fit(train_generator, steps_per_epoch = train_generator.samples\/\/batch_size, epochs = n_epochs, validation_data = validation_generator, validation_steps = validation_generator.samples\/\/batch_size, callbacks =[early_exit, best_checkpoint])\n        \nmodel.load_weights(filepath = '.best_fit.hdf5')","12e1e1f8":"analyze_performances(hst, n_epochs)","79a52e77":"model_evaluation(model, test_generator)","187a6de2":"pre_trained_conv = VGG19(weights = 'imagenet', include_top = vgg_include_top, classes = 4, classifier_activation=\"softmax\", input_shape = (target_img_shape_1, target_img_shape_2, target_img_channels))","b5e18a71":"model_pt = build_model_pretrained_cnn(pre_trained_conv, vgg_include_top, regression_problem, vgg_hidden_layers_neurons, vgg_hidden_activation_function, vgg_hidden_layers_L1_coeffs, vgg_hidden_layers_L2_coeffs, vgg_hidden_layers_dropout, vgg_final_layer_neurons, vgg_final_activation_function, vgg_model_optimizer, loss_function, metrics)","7a4b0d4c":"early_exit      = EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='min')\nbest_checkpoint = ModelCheckpoint('.best_fit_pre_trained.hdf5', save_best_only=True, monitor='val_categorical_accuracy', mode='max')\n\nvgg_hst = model_pt.fit(train_generator, steps_per_epoch = train_generator.samples\/\/batch_size, epochs = vgg_n_epochs, validation_data = validation_generator, validation_steps = validation_generator.samples\/\/batch_size, callbacks =[early_exit, best_checkpoint])\n\nmodel_pt.load_weights(filepath = '.best_fit_pre_trained.hdf5')","d3211db3":"analyze_performances(vgg_hst, vgg_n_epochs)","48555ad9":"model_evaluation(model_pt, test_generator)","2fd7b6bd":"## LOAD THE VGG19 PRE-TRAINED CNN","e438aa01":"## DISPLAY BASIC INFORMATION ON TRAIN AND TEST IMAGES\n\nWe use the function \"dataset_basic_info()\" to display some basic information, e.g., # samples, image shape, labels frequencies, for each dataset.","9b314815":"## CLASSIFY THE TEST IMAGES\n\nWe use our trained convolutional neural network to predict the labels of previously unseen images contained in the test folder.","95da6920":"## DISPLAY SOME SAMPLE IMAGES\n\nCredits to Amy Jang, see https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays\/notebook#5.-Correct-for-data-imbalance","48870032":"## REORGANIZE IMAGES IN A NEW FOLDER\n\nWe transfer the available images in three folders (training, validation, testing).\n\nWe are forced to perfom this action because we want an augmentable training set, a non-augmentable validation set which is drawn from training images and an independent, unseen, test set.","9acb8e63":"## ANALYZE THE PERFORMANCE OF THE MODEL\n\nWe display three different metrics to assess the performances of the model\n\n1. loss                  -> the value of the loss function in each epoch.\n2. categorical accuracy  -> \"Calculates how often predictions matches one-hot labels.\", see https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/metrics\/categorical_accuracy\n3. multiclass_AUC        -> \"Computes the approximate AUC (Area under the curve) via a Riemann sum for each label and then takes the average.\", see https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/metrics\/AUC\n\nThese metrics are recorded at the end of each training epoch and plotted in epoch vs. metric diagrams. Red lines represent the pattern of these metrics during training while blue lines represent the pattern of these metrics during the validation phase.","c4e095b0":"## ANALYZE THE PERFORMANCE OF THE MODEL","7005dad0":"## GENERATE TRAINING AND TEST IMAGES\n\nWe create three distinct generators (training, validation, test) that we will use later to train and assess the model.","16f16b5b":"## BUILD A MODEL BY STACKING THE PRE-TRAINED CNN WITH AN USER DEFINED NEURAL NETWORK CLASSIFIER","c8dd06be":"## REFERENCES\n\n1. This script is strongly inspired by the contents of Chapter 5 of \"Deep Learning with Python\", Francois Chollet, Manning.\n2. I also found the notebook \"TensorFlow Pneumonia Classification on X-rays\" by Amy Jang very useful. See https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays\/notebook#5.-Correct-for-data-imbalance","9dcee3d0":"## DEFINE AUXILIARY FUNCTIONS\n\nThe following cells supplies auxiliary functions that can be used to achieve the objectives of the present script\n\n1. dataset_basic_info         -> display basic information on the data sets.\n2. build_model_ann_conv       -> return a Keras model object of a convolutional neural network built according to input specs provided by the user.\n3. build_model_pretrained_cnn -> return a Keras model object of a convolutional neural network made of a pretrained convolutional base (specified by the user) and a dense classifier (standard neural network) built according to input specs provided by the user.\n4. display_input_images       -> display batches of images from the train or test data set, credits to Amy Jang, see https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays\/notebook#5.-Correct-for-data-imbalance\n5. analyze_performances       -> plot the trajectories of four different performance metrics (i.e., loss, accuracy, precision and recall) recorded at the end of each epoch for both the testing and the validation phases.\n6. model_evaluation          -> evaluate the ability of the trained model to predict the labels of previously unseen samples (test set). Display the same metrics examined in the training phase and the confusion matrix.        ","896ec295":"## FIT THE MODEL","fe21293b":"## CORRECT CLASS LABEL IMBALANCES\n\nWe save copies of existing images belonging to the training data set until the frequencies of the available classes are balanced, e.g., 1\/3, 1\/3, 1\/3 if we have three distinct labels.\nLater, we will use data augmentation so that even copies of the same image will end up to be different.\n\nNOTE: validation and test images do not undergo any augmentation procedure.","0bf2625e":"## USE A PRE-TRAINED MODEL TO FURTHER IMPROVE PERFORMANCES\n\nFinally, we use pre trained models and compare how they perform with respect to our trained model.\n\nSee Chapter 5 of \"Deep Learning with Python\", Francois Chollet, Manning, for details on pre-trained convolutional neural networks.\n\n![1_cufAO77aeSWdShs3ba5ndg.jpg](attachment:1_cufAO77aeSWdShs3ba5ndg.jpg)\n\nSchematics of the architecture of the pre-trained VGG19 model.\n\nImage source: https:\/\/www.kaggle.com\/shivamb\/cnn-architectures-vgg-resnet-inception-tl","30662653":"## BUILD A CONVOLUTIONAL NEURAL NETWORK MODEL\n\nThe input parameters set by the user serve as arguments of a function which returns a Keras model object of a convolutional neural network.\n\n![1_vkQ0hXDaQv57sALXAJquxA.jpg](attachment:1_vkQ0hXDaQv57sALXAJquxA.jpg)\n\nSchematics of the typical structure of a neural network model for image recognition.\n\nImage source: https:\/\/towardsdatascience.com\/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53","6908f7ed":"## INPUT DASHBOARD\n\nWe let the user set several variables that affect the estimation of the model:\n\n* training_path -> specify the path of the training images. NOTE: the next level should include k folder, where k is the number of different possible labels (e.g. k = 2, dogs and cats).\n* test_path -> specify the path of the test images. NOTE: the next level should include k folder, where k is the number of different possible labels (e.g. k = 2, dogs and cats).\n* validation_split    -> fraction of the images in the test data folder that should be used as validation subset during the training of the model.\n* regression_problem  -> indicates whether we are facing a regression problem. If = True, the final layer of the densely connected neural network won't have any specified activation function.   \n* target_img_shape_1  -> the first desired dimension for the images; an input for the Keras function flow_from_directory that is applied to an ImageDataGenerator.\n* target_img_shape_2  -> the second desired dimension for the images; an input for the Keras function flow_from_directory that is applied to an ImageDataGenerator.\n* target_img_channels -> the number of desired channels for the images; an input for the Keras function flow_from_directory that is applied to an ImageDataGenerator. E.g. 3 channels for RGB images.\n* conv_filters        -> list including the number of different filters that are used in each convolution.\n* conv_filter_shape   -> list of k lists, where k is the number of desired convolutions. Each of these k lists includes two integers representing the dimensions (in number of cells) of each filter that slides across the picture.\n* conv_activation_function -> list including the names of the activation function that should be used in each convolution. Note: we have a product between a patch of the image and the filter. Then, the entries of the resulting matrix are summed up. This sum serves as the argument of the activation function g().\n* conv_padding -> list including the type of padding that should be used in each convolution. The string 'valid' means that no padding should be applied on the borders of the image. 'same' means that the image is padded such that the output of the convolution retains the same shape.\n* conv_pooling_type   -> list including the type of pooling that must be applied after each convolution. The string 'max' implements max pooling, the string 'avg' implements average pooling, any other string means no pooling after the current convolution.\n* conv_pooling_coeffs -> list of k lists, where k is the number of desired convolutions. Each of these k lists includes two integers representing the dimensions (in number of cells) of the pooling operation region that is applied across the convolved image.\n* augment_data -> boolean indicating whether the images in the training data set should be augmented by applying a broad range of actions, such as rotations, shearing, zooming, flipping etc.\n* rotation_range -> specifies the range (in degrees) of random rotations of the input images.\n* width_shift_range -> specifies the range (in fractions of the image width) of random horizontal shifts of the input images.\n* height_shift_range -> specifies the range (in fractions of the image height) of random vertical shifts of the input images.\n* shear_range -> specifies the symmetric range -x, x of random shears of the input images.\n* brightness_range -> list including the lower and upper extremes of the range of image brightness values that are randomly applied to the input images. \n* zoom_range -> specifies the symmetric range 1 - x, 1 + x of random zooms of the input images.\n* horizontal_flip -> boolean indicating whether the columns of the training images should be randomly flipped.\n* fill_mode -> name of the method that should be used to fill the new empty pixels emerged during data augmentation procedures.\n* print_sample_input -> boolean indicating whether a sample of the input images shall be printed.\n* hidden_activation_function -> list containing the name of the activation functions (available in Keras) used in the hidden layers of the densely connected neural network that follows the sequence of convolutions.\n* hidden_layers_neurons -> list containing the number of neurons forming each hidden layer of the densely connected neural network that follows the sequence of convolutions.\n* hidden_layers_L1_coeffs -> list containing scalars multiplying the L1-penalty terms for each hidden layer weights of the densely connected neural network that follows the sequence of convolutions. Set it to 0 to avoid L1-regularization.\n* hidden_layers_L2_coeffs -> list containing scalars multiplying the L2-penalty terms for each hidden layer weights of the densely connected neural network that follows the sequence of convolutions. Set it to 0 to avoid L2-regularization.\n* hidden_layers_dropouts ->  list containing the fractions of weights that are randomly set to zero in each hidden layer of the densely connected neural network that follows the sequence of convolutions. Set it to 0 to avoid dropout regularization.\n* final_activation_function -> name of the activation function (available in Keras) used in the terminal layer of the densely connected neural network that follows the sequence of convolutions.\n* final_layer_neurons -> number of neurons forming the terminal layer of the densely connected neural network that follows the sequence of convolutions.\n* model_optimizer -> name of the method (available in Keras) to iteratively update the search of the set of parameters that minimize the loss function.\n* loss_function -> name the loss function (available in Keras) that we seek to minimize.\n* metrics -> list containing the name of the metrics (available in Keras) that we use to assess the performances of the model.\n* n_epochs -> the times the optimization algorithm goes through the entire training data set.\n* batch_size -> the number of samples included in a single batch.\n* steps_per_epoch -> the number of batches that form an epoch during the training phase. \n* vgg_hidden_activation_function -> list containing the name of the activation functions (available in Keras) used in the hidden layers of the densely connected neural network classifier that follows the pretrained VGG19 CNN.\n* vgg_hidden_layers_neurons -> list containing the number of neurons forming each hidden layer of the densely connected neural network classifier that follows the pretrained VGG19 CNN.\n* vgg_hidden_layers_L1_coeffs -> list containing scalars multiplying the L1-penalty terms for each hidden layer weights of the densely connected neural network classifier that follows the pretrained VGG19 CNN. Set it to 0 to avoid L1-regularization.\n* vgg_hidden_layers_L2_coeffs -> list containing scalars multiplying the L2-penalty terms for each hidden layer weights of the densely connected neural network classifier that follows the pretrained VGG19 CNN. Set it to 0 to avoid L2-regularization.\n* vgg_hidden_layers_dropouts ->  list containing the fractions of weights that are randomly set to zero in each hidden layer of the densely connected neural network classifier that follows the pretrained VGG19 CNN. Set it to 0 to avoid dropout regularization.\n* vgg_final_activation_function -> name of the activation function (available in Keras) used in the terminal layer of the densely connected neural network classifier that follows the pretrained VGG19 CNN.\n* vgg_final_layer_neurons -> number of neurons forming the terminal layer of the densely connected neural network classifier that follows the pretrained VGG19 CNN.\n* vgg_n_epochs -> the times the optimization algorithm goes through the entire training data set when the VGG19 based CNN is trained.\n* vgg_steps_per_epoch -> the number of batches that form an epoch during the training phase when the VGG19 based CNN is trained. ","9e8f46b3":"## NOTES\n\nAuthor: Alberto Ciacci\n\nE-mail: alberto.ciacci16@imperial.ac.uk\n\nLanguage: python\n\nComments, corrections and suggestions are very much welcome.","1b2bfe47":"## FIT THE MODEL","0d648887":"## SUMMARY\n\nThis notebook implements a neural network model to predict the status (non-demented, moderate demented, very mild demented, mild demented) associated to the brain captured in MRI images.\n\nThe input dataset is available at https:\/\/www.kaggle.com\/tourist55\/alzheimers-dataset-4-class-of-images\n\nThe code lets the user choose the number of convolutional layers and dense layers that follow the convolution processes. \n\nIn this experiment, our model achieves very satisfactory accuracy levels, i.e., > 99%, in predicting the state of previously unseen MRI brain images.\n\n![1280px-Alzheimer's_disease_brain_comparison.jpg](attachment:1280px-Alzheimer's_disease_brain_comparison.jpg)\n\nSchematics of a healthy brain (left) and an Alzheimer affected brain (right).\n\nImage source: https:\/\/en.wikipedia.org\/wiki\/Alzheimer%27s_disease#\/media\/File:Alzheimer's_disease_brain_comparison.jpg\n\n![mri-scan.jpg](attachment:mri-scan.jpg)\n\nAn MRI device.\n\nImage source: https:\/\/www.medicalnewstoday.com\/articles\/146309","0f204e27":"## IMPORT LIBRARIES\n\nWe begin by importing the libraries needed to perform the incoming tasks.","bd4b274c":"## CLASSIFY THE TEST IMAGES\n\nWe use our trained convolutional neural network (including the pre-trained convolutional component) to predict the labels of previously unseen images contained in the test folder."}}