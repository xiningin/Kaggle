{"cell_type":{"b8167229":"code","a819a0f2":"code","090f06b6":"code","91f703c2":"code","53455c21":"code","5bab8246":"code","7d3543dc":"code","62bf6a55":"code","7ac94314":"code","f013ce04":"code","57089656":"code","eeb8d9fd":"code","c88b05c8":"code","f3c7ea20":"code","e01fe52f":"code","d9583d2f":"code","0ce68787":"code","e0b6039b":"code","c1bd86d9":"code","b2bbe9c4":"code","557fdcf2":"code","acc40af5":"code","d0981f56":"code","ca5fa18d":"code","9e766ab7":"code","f61fe1f1":"code","039de54b":"code","a9ba38f6":"code","8fcd0453":"code","e4ea134d":"code","42032e3c":"code","f6194158":"code","0b90a255":"code","3a470f64":"code","a982737b":"code","6e1148a7":"code","9a8ccdaa":"code","caab7eea":"code","1614b220":"code","44c11f51":"code","d49d418e":"code","30ea59c4":"code","c75d64a0":"code","7e43f1b0":"code","45b7dcd2":"code","710f7e2e":"code","7de256b5":"code","f10b8921":"code","2bca78b6":"code","7e285ec7":"code","66c7b029":"code","29b9a429":"code","704e72c4":"code","bc2c64a8":"code","4a1e8f42":"code","33e60287":"code","4eed3eec":"code","9e141ee3":"code","a33e64d7":"code","d63a5084":"code","ed2f79b0":"code","05e49416":"code","5c711ac8":"code","f1a45233":"code","7b51aca0":"code","3cd018cf":"code","4a1cc43d":"code","6c9c225f":"code","b026d65a":"code","d81875f1":"code","036e5b67":"markdown","2ca39bb6":"markdown","84beb828":"markdown","3e892e1c":"markdown","d767c11b":"markdown","3eb83c18":"markdown","ab70727e":"markdown","d2e1c36e":"markdown","5b355a9d":"markdown","88b0534c":"markdown","c6eecbf4":"markdown","e2559c85":"markdown","4b36d357":"markdown","d3f6825b":"markdown","ff14c7e7":"markdown","02753390":"markdown","3180cec9":"markdown","e063db16":"markdown","06cde80f":"markdown","fe92ffdc":"markdown","b0cbe348":"markdown","5c87b90d":"markdown","d7c277e2":"markdown","57d3b0a7":"markdown","55fe6ec2":"markdown","6a59b89f":"markdown","dfbc0082":"markdown","d7fdfe0f":"markdown","31fd5878":"markdown","3096fc97":"markdown","17fd108d":"markdown","1985acbc":"markdown","5724a937":"markdown","4dcbc547":"markdown","950b5a74":"markdown","dc066c15":"markdown","0c1f8e97":"markdown","013de039":"markdown","d870a52b":"markdown","d08f1c26":"markdown","60b249c1":"markdown","e349b910":"markdown","4e9e4106":"markdown","86e22344":"markdown","31ffa811":"markdown"},"source":{"b8167229":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a819a0f2":"# importing libraries\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.ticker as mtick\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom imblearn.combine import SMOTEENN # imbalanced-learn Python library,\n\nimport warnings\nwarnings.filterwarnings('ignore')","090f06b6":"# to get a view of all the columns\npd.set_option(\"display.max_columns\", None)","91f703c2":"# Importing data\ndataset = pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndataset.head()","53455c21":"dataset.Churn.value_counts() # pandas.core.series.Series","5bab8246":"# Visualization of imbalance data\nplt.bar(x = dataset['Churn'].unique(), height = dataset.Churn.value_counts())\nplt.legend()\nplt.xlabel(\"Churn label\")\nplt.ylabel(\"Count\")\nplt.show()","7d3543dc":"plt.pie(dataset.Churn.value_counts(), labels = dataset['Churn'].unique(), explode = [0.1, 0.5])\nplt.legend()\nplt.show()","62bf6a55":"# more understanding on data\ndataset.info()","7ac94314":"# include='all' helps to describe all types of columns, (both categorical and numerical)\ndataset.describe(include='all')","f013ce04":"dataset.TotalCharges.value_counts()\n# Observation: There is one ' ' blank value, which has occurred 11 times; making the data type as object instead of float64","57089656":"dataset[dataset['TotalCharges'] == ' ']\n# rows with total Charges as blank(' ')","eeb8d9fd":"# We will replace the 11 blank places with 0.0 here\nprint(\"Before removing blank values\")\nprint(dataset[dataset['TotalCharges'] == ' '].index) \n# to print those indexes values where variable 'TotalCharges == ' '\n\n# dataset['TotalCharges'] = dataset.TotalCharges.replace(r'^\\s*$', 0.0, regex=True) \n# to replace white spaces (any number of white spaces (' ') and then ending with white space) with 0.0\n# replacing with corresponding Monthly charges, where TotalCharges has blanks\n# df['col1'] = np.where(df['col1'] == ' ', df['col2'], df['col1'])\ndataset['TotalCharges'] = np.where(dataset['TotalCharges'] == ' ', dataset['MonthlyCharges'], dataset['TotalCharges'])\n\nprint(\"After removing full blank values\")\nprint(dataset[dataset['TotalCharges'] == ' '].index)","c88b05c8":"dataset[dataset['TotalCharges'] == dataset['MonthlyCharges']]\n# After replacing blanks","f3c7ea20":"# we need to change the data type of variable 'Totalcharges' to float64\ndataset.info() # still object\ndataset['TotalCharges'] = dataset['TotalCharges'].astype(float)\ndataset.info() # yes now it has changed to float64","e01fe52f":"# The count for each churn categories in this path\ndataset.groupby(['gender', 'Partner', 'Dependents', 'Churn']).size()","d9583d2f":"# instead of keeping multi index, or merged data, changing it to row-wise data to get a better view\ndataset.groupby(['gender', 'Partner', 'Dependents', 'Churn']).size().reset_index(name='Count')","0ce68787":"# visualizing the above info using plotly\n# Treemap charts visualize hierarchical data using nested rectangles\n# all the features here are categorical in nature\nfig = px.treemap(dataset.groupby(['gender', 'Partner', 'Dependents', 'Churn']).size().reset_index(name='Count'), \n                 path=['gender', 'Partner', 'Dependents', 'Churn'], values='Count', color='Churn', \n                 title='1. How gender, partner and dependents are related to churn?')\n\nfig.show()","e0b6039b":"dataset.groupby(['tenure', 'Churn']).size().reset_index(name='count')\n# tenure is a numerical variable, and thus will have large number of unique values","c1bd86d9":"# Here, reset_index is from pandas.Series.reset_index\n# A rug plot is a plot of data for a single quantitative variable, displayed as marks along an axis. \n# Rug plots are used to visualise the distribution of the data\n\n# A marginal distribution is the percentages out of totals\n\nfig = px.histogram(dataset.groupby(['tenure', 'Churn']).size().reset_index(name='count'), \n                   x=\"tenure\", y=\"count\", color=\"Churn\", marginal=\"rug\", color_discrete_map={\"Yes\":\"#E45756\", \"No\":\"#1CBE4F\"}, \n                   title=\"2. Does tenure has any impact on churn?\")\n\nfig.show()\n\n# Observation: customers with low tenure (0-10) has the highest rate of churning","b2bbe9c4":"dataset.groupby(['Churn', 'PhoneService', 'InternetService']).size()","557fdcf2":"# Sunburst plots visualize hierarchical data spanning outwards radially from root to leaves\n# the root starts from the center and children are added to the outer rings.\n\nfig = px.sunburst(dataset.groupby(['Churn', 'PhoneService', 'InternetService']).size().reset_index(name='count'), \n                  path = ['Churn', 'PhoneService', 'InternetService'], values = 'count', \n                  title='3. As the dataset is about telecom industry, we need some insights on phone and internet services!')\n\nfig.show()","acc40af5":"np.unique(dataset.TechSupport)","d0981f56":"# Let me take only \"Yes\" and \"No\" for TechSupport in consideration\ndata_techSupport_yes = dataset[dataset['TechSupport'] == 'Yes']\ndata_techSupport_no = dataset[dataset['TechSupport'] == 'No'] # type == pandas.core.frame.DataFrame\n# same as --> dataset.loc[dataset['TechSupport'] == 'No',:]","ca5fa18d":"# for each tenure, whether Churned or not, and its corresponding count\ndata_techSupport_yes.groupby(['tenure', 'Churn']).size()","9e766ab7":"fig = px.histogram(data_techSupport_yes.groupby(['tenure', 'Churn']).size().reset_index(name='count'), \n                   x=\"tenure\", y=\"count\", marginal=\"rug\", color=\"Churn\", color_discrete_map={\"Yes\":\"#E45756\", \"No\":\"#1CBE4F\"}, \n                   title=\"Statistics of customers opted for tech support with churning\")\n# color=\"Churn\" is vvi to mention, or else it will color 'count'\nfig.show()","f61fe1f1":"# for each tenure, whether Churned or not, and its corresponding count\ndata_techSupport_no.groupby(['tenure', 'Churn']).size()","039de54b":"# plotting histogram for customer churning who took no tech support\nfig = px.histogram(data_techSupport_no.groupby(['tenure', 'Churn']).size().reset_index(name='count'),\n                   x='tenure', y='count',color='Churn', marginal='rug', color_discrete_map={\"Yes\":\"#E45756\", \"No\":\"#1CBE4F\"}, \n                   title=\"Statistics of customers opted for tech support with churning\")\n\nfig.show()","a9ba38f6":"fig = px.sunburst(dataset.groupby(['Churn', 'TechSupport', 'tenure']).size().reset_index(name='count'), \n                            path=['Churn', 'TechSupport', 'tenure'], values='count', \n                            title='Does customers opted for tech support stayed for longer tenure with less churn?')\n\nfig.show()","8fcd0453":"sns.set(rc={'figure.figsize':(26, 8.27)}) # rc - seems row, column\nsns.kdeplot(data = dataset, x=\"MonthlyCharges\", hue=\"Churn\", multiple=\"stack\").set(title=\"5. Is there any relationship of churn with monthly charges?\")","e4ea134d":"sns.set(rc={'figure.figsize':(26,8.27)})\nsns.kdeplot(data=dataset, x=\"TotalCharges\", hue=\"Churn\", multiple=\"stack\").set(title=\"Did customers' total charges relate with churn?\")","42032e3c":"dataset.groupby(['Churn', 'StreamingTV']).size()","f6194158":"ax = sns.barplot(x=\"StreamingTV\", y=\"count\", hue=\"Churn\", data = dataset.groupby(['Churn', 'StreamingTV']).size().reset_index(name='count'), \n                 palette=\"Set2\").set(title = \"6. Is there any relation between churning and customers who opted for streaming?\")","0b90a255":"fig = px.sunburst(dataset.groupby(['Churn', 'InternetService', 'StreamingTV']).size().reset_index(name='count'), \n                            path=['Churn', 'InternetService', 'StreamingTV'], values='count', \n                            title='6. Is there any relation between churning and customers who opted for streaming?')\n\nfig.show()","3a470f64":"fig = px.sunburst(dataset.groupby(['Churn', 'StreamingTV']).size().reset_index(name='count'), \n                            path=['Churn', 'StreamingTV'], values='count', \n                            title='Do customers opted for streaming, faced issue with the service?')\n\nfig.show()","a982737b":"dataset.groupby(['Churn', 'StreamingMovies']).size()","6e1148a7":"# plotting Streaming movies count with Churn to see if there exists any relationship\nax = sns.barplot(x=\"Churn\", y=\"count\", hue=\"StreamingMovies\", \n                 data = dataset.groupby(['Churn', 'StreamingMovies']).size().reset_index(name=\"count\"), palette=\"Set2\").set(title=\"\")","9a8ccdaa":"dataset.groupby(['Churn', 'Contract']).size()","caab7eea":"# plotting graph to get better understanding\nfig = px.sunburst(dataset.groupby(['Churn', 'Contract']).size().reset_index(name='count'), \n                  path=['Churn', 'Contract'], values='count', \n                  title='7. How contract is impacting business?')\n\nfig.show()","1614b220":"# plotting graph to get better understanding\n# 0 - not a senior citizen\n# 1 - senior citizen\nfig = px.sunburst(dataset.groupby(['Churn', 'SeniorCitizen']).size().reset_index(name='count'), \n                  path=['Churn', 'SeniorCitizen'], values='count', \n                  title='How being or non being SeniorCitizen is impacting Churning?')\n\nfig.show()","44c11f51":"# plotting graph to get better understanding\nfig = px.sunburst(dataset.groupby(['Churn', 'MultipleLines']).size().reset_index(name='count'), \n                  path=['Churn', 'MultipleLines'], values='count', \n                  title='How having MultipleLines is impacting Churning?')\n\nfig.show()\n# Observation: Having (yes) multiple lines have almost equal impact as not having (No) multiple lines","d49d418e":"from pandas_profiling import ProfileReport\n\n# generating the profile report by passing the dataframe object to the profiling function\nprofile = ProfileReport(dataset)\nprofile\n# profile.to_file(\"Analysis.html\") --> if analysis is required in html format","30ea59c4":"dataset.isnull().sum()","c75d64a0":"import missingno as msno\nmsno.bar(dataset)\n# All the features have complete data","7e43f1b0":"X = dataset.drop(['customerID', 'Churn'], axis=1)\ny = dataset.Churn","45b7dcd2":"# List of categorical columns\ncat_cols = [col for col in X.columns if X[col].dtype == 'object'] + ['SeniorCitizen'] \n# although Senior Citizen got the 0,1 (int64) values, but basically it is a categorical column\nprint(cat_cols)","710f7e2e":"OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nX_encoded = pd.DataFrame(OH_encoder.fit_transform(X[cat_cols]), index = X.index)\n\nX_num = X.drop(cat_cols, axis=1)\nX_encoded = pd.concat([X_encoded, X_num], axis=1)","7de256b5":"X_encoded.head()","f10b8921":"X_encoded.shape\n# number of features increased","2bca78b6":"cols = X_encoded.columns # keeping the names of all columns\n\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\nx_scaled = scaler.fit_transform(X_encoded)\nX_encoded_scaled = pd.DataFrame(x_scaled, columns=cols)","7e285ec7":"y #pandas.core.series.Series\n# y[0] -- is valid; y['Churn'] is not valid for series data type ","66c7b029":"print(\"Before encoding: \", y.unique())\n\ny = np.where(y.str.contains(\"Yes\"), 1, 0)\n\nprint(\"After encoding: \", y) # Now y is a numpy array","29b9a429":"# Finding out optimum k-value using elbow method\nfrom sklearn.neighbors import KNeighborsClassifier\ndef find_k_KNN(x_train, x_test, y_train, y_test):\n    error_rate = []\n\n    # calculating error rate\n    for i in range(1,40):\n        knn = KNeighborsClassifier(n_neighbors=i)\n        knn.fit(x_train, y_train)\n        pred_i = knn.predict(x_test)\n        error_rate.append(np.mean(pred_i != y_test))\n\n\n    # Plotting elbow graph\n    plt.figure(figsize=(10,6))\n    plt.plot(range(1,40), error_rate, color=\"green\", linestyle=\"dashed\", marker=\"o\",\n             markerfacecolor=\"red\", markersize=10)\n    plt.title(\"Error Rate vs. K Value\")\n    plt.xticks(range(1,40))\n    plt.xlabel(\"K\")\n    plt.ylabel(\"Error Rate\")","704e72c4":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\n\n\n\n\n\nfrom sklearn.model_selection import GridSearchCV # grid search CV was taking hours to calculate all the combinations\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\ndef train_models(x_train, y_train, k_values):\n    # defining models\n    models = [LogisticRegression(penalty = 'l2'), RandomForestClassifier(), \n              XGBClassifier(use_label_encoder=False, verbosity = 0, eval_metric='logloss', tree_method = 'gpu_hist', \n                            predictor = 'gpu_predictor'),\n              KNeighborsClassifier(), tree.DecisionTreeClassifier(), GaussianNB(), svm.SVC()]\n\n    # predictor = 'gpu_predictor', tree_method = 'gpu_hist' --> to use gpu for XGBoost\n    \n    \n    # defining model names\n    model_names = ['Logistic Regression', 'Random Forest', 'Gradient Boosting Classifier', 'KNN', 'Decision Tree', \n                   'Naive Bayes', 'Support Vector Machines']\n\n    # defining parameters\n\n    parameters = [{'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}, # logistic regression\n\n                 {'n_estimators':[100, 300, 500, 600, 700, 1000], 'criterion':['gini', 'entropy'], \n                  'max_depth' : [10, 20, 25, 30, 35, 40], 'min_samples_split': [100, 200, 50, 25]}, # random forest classifier\n\n                 {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] , \"max_depth\": [ 3, 4, 5, 6, 8, 10, 12, 15],\n                  \"min_child_weight\" : [ 1, 3, 5, 7 ], \"gamma\": [ 0.0, 0.1, 0.2 , 0.3, 0.4 ], \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ], \n                  \"n_estimators\": [100, 120, 135, 150, 165, 200]}, # xgb classifier\n\n                 {'n_neighbors' : k_values}, # kNN\n\n                {'criterion':['gini', 'entropy'], 'max_depth' : [10, 20, 25, 30, 35, 40], 'min_samples_split': [100, 200, 50, 25]}, # decision tree\n\n                {}, # naive bayer\n\n                {'kernel': ['linear', 'poly', 'rbf', 'sigmoid']} # svm\n                 ]\n    \n    \n    # training the models\n\n    for model_idx in range(len(models)):\n        \n        \n        if model_names[model_idx] in ['Logistic Regression', 'KNN', 'Naive Bayes', 'Support Vector Machines']:\n            classifier = GridSearchCV(estimator = models[model_idx], param_grid = parameters[model_idx], n_jobs=-1)\n        else:\n            classifier = RandomizedSearchCV(estimator = models[model_idx], param_distributions = parameters[model_idx], random_state=0, n_jobs=-1)\n\n        classifier.fit(x_train, y_train)\n\n        print(model_names[model_idx])\n        print(classifier.best_estimator_)\n        models[model_idx] = classifier.best_estimator_ # updating the model with best hyperparameters as per training data\n        \n        print(f'Best train score: { classifier.best_score_}')\n        print(classifier.best_params_)\n        print(\"\\n******************************************************************************************************************\\n\")\n    \n    return (models, model_names)","bc2c64a8":"from sklearn.metrics import classification_report\n\ndef test_models(x_train, x_test, y_train, y_test, models, model_names):\n    for classifier_idx in range(len(models)):\n        models[classifier_idx].fit(x_train, y_train)\n        y_pred = models[classifier_idx].predict(x_test)\n        \n        print(model_names[classifier_idx])\n        print(f'{models[classifier_idx].score(x_test, y_test)*100}')\n        print(\"\\n####################\")\n        print(classification_report(y_test, y_pred))\n        print(\"\\n\\n******************************************************************************************************************\\n\\n\")\n    ","4a1e8f42":"%%time\n# dependent and independent variables\nX_encoded_scaled.shape, y.shape\n\n# train - test split\nx_train, x_test, y_train, y_test = train_test_split(X_encoded_scaled, y, test_size = 0.10, stratify=y)\n\nfind_k_KNN(x_train, x_test, y_train, y_test) # decide using the pictorial graph","33e60287":"%%time\nk_values = [2, 4, 6]\n\n# for training the models to find the best estimators based on training score\nmodels, model_names = train_models(x_train, y_train, k_values)\n","4eed3eec":"# for testing the models to find the best test score using the best estimated parameters for each algorithm\ntest_models(x_train, x_test, y_train, y_test, models, model_names)","9e141ee3":"# choosing the final model depending on best train and test scores\n\nclassifier = LogisticRegression(solver='liblinear')\n\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\n","a33e64d7":"# classification report for imbalanced data\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","d63a5084":"# Here, I have used SMOTE\n# from imblearn.combine import SMOTEENN (UpSampling + ENN-Edited NearestNeighbors)\n\nfrom imblearn.combine import SMOTEENN\nsm = SMOTEENN(random_state = 42) \n# Sampling_strategy is left 'auto' which is equivalent to 'not majority', i.e., multiple copies of minority class will be generated to match the number of samples in majority class\n# Then ENN  will be used to remove the misclassified majority samples\nX_resample, y_resample = sm.fit_resample(X_encoded, y) # resampling using SMOTE","ed2f79b0":"X_encoded.shape","05e49416":"X_resample.shape\n# we can see first upsampling using SMOTE was done on minority class and then down sampling on majority class using ENN","5c711ac8":"np.unique(y, return_counts=True)","f1a45233":"np.unique(y_resample, return_counts=True)","7b51aca0":"cols = X_resample.columns # keeping the names of all columns\n\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\nx_scaled = scaler.fit_transform(X_resample)\nX_resample_scaled = pd.DataFrame(x_scaled, columns=cols)","3cd018cf":"# Train - test split\n# again spliting train and test data\nxr_train, xr_test, yr_train, yr_test = train_test_split(X_resample_scaled, y_resample, test_size=0.2)\n\nfind_k_KNN(xr_train, xr_test, yr_train, yr_test) # decide using the pictorial graph","4a1cc43d":"k_values = [1]\n\n# for training the models to find the best estimators based on training score\nmodels, model_names = train_models(xr_train, yr_train, k_values)","6c9c225f":"# for testing the models to find the best test score using the best estimated parameters for each algorithm\ntest_models(xr_train, xr_test, yr_train, yr_test, models, model_names)","b026d65a":"# choosing the final model depending on best test scores\n\nclassifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.5, gamma=0.2, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=8,\n              min_child_weight=1, missing=np.nan, monotone_constraints='()',\n              n_estimators=165, n_jobs=4, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', use_label_encoder=False,\n              validate_parameters=1, verbosity = 0, eval_metric='logloss')\n\nclassifier.fit(xr_train, yr_train)\nyr_pred = classifier.predict(xr_test)\n# classifier.score(x_test, y_test)*100","d81875f1":"print(classification_report(yr_test, yr_pred))","036e5b67":"### Model creation and predictions using balanced data (SMOTE + ENN)","2ca39bb6":"#### Senior Citizen vs Churning","84beb828":"### Observations Based on Visualizations\n* Observation-1: Whether male or female, if they do not have partner or dependents, they are more likely to churn!!\u00b6\n* Observation-2: During 0-10 years of tenure, we can see maximum churning. As the customer turns old, they might get habituated using same telecom service\n* Observations-3: People with Phone services (yes) and 'Fiber optic' Internet Service are churning more\n* Observations-4:more churning takes place in first 10 yrs, for customers with or without tech support. But Churning is more in case of \"without tech support\" customers\n* Observations-5: As the monthy charges are incresing, we can see the density increasing too (60-120), which means more churning with increasing monthly charges\n* Observation-6: It is quite opposite of what has been seen for monthly charges. Here high churning occurs in early phase itself, 0-2000 total charges have maximum churning\n* Observation-7: Churning is being observed equally for the 'Yes', 'No' group of whether connected StreamingTv or not!\n* Observation-8: Churning is being observed equally for both the 'Yes', 'No' group of StreamingMovies\n* Observations-9: clearly visible that customers with month-to-month contract are the highest churners\n* Observation-10: Most churners are non-senior citizens. Although we should also consider the fact that, data is more for non-senior citizens (5:1)","3e892e1c":"##### Observation: It is quite opposite of what has been seen for monthly charges. Here high churning occurs when total charges is less,  0-2000 total charges have maximum churning","d767c11b":"#### Observation: Most important piece of info that I can take into consideration is the Warning tab, showing correlations","3eb83c18":"# Visualization \n#### Mostly using Plotly and Seaborn","ab70727e":"### 1. How gender, partner and dependents are related to churn?","d2e1c36e":"### 5. Is there any relationship of churn with monthly charges or total charges?","5b355a9d":"## Pandas Profiling\n[Tutorial by Analyticsvidya on Pandas Profiling](http:\/\/www.analyticsvidhya.com\/blog\/2021\/06\/generate-reports-using-pandas-profiling-deploy-using-streamlit\/)\n\n#### Pandas Profiling consists of 3 tabs:\n* 1. Overview\n* 2. Warnings\n* 3. Reproduction\n\n#### The \"Overview\" consists of overall statistics of our dataframe passed. This includes the \n* - number of variables (features or columns of the dataframe), \n* - Number of observations (rows of dataframe), \n* - Missing cells,  \n* - Percentage of missing cells, \n* - Duplicate rows, \n* - Percentage of duplicate rows, \n* - Total size in memory.\n\n\n#### The \"Warnings\" tab provides waring info regarding cardinality, correlation among variables (features), missing values, skewness in features, etc.\n\n#### The \"reproduction\" tab is on meta data of report. It provides information like start time, end time of the report generation analysis, etc.","88b0534c":"### 7. How contract is impacting business?","c6eecbf4":"### 3. As the dataset is about telecom industry, we need some insights on phone and internet services!","e2559c85":"#### Customers who took tech support","4b36d357":"#### Finding Optimum k-value for k-nearest neighbour","d3f6825b":"### 6. Is there any relation between churning and customers who opted for streaming?","ff14c7e7":"# Understanding the Business Problem\n### To get an understanding of data and give our analysis some direction we can come up with some basic questions as follows:\n#### 1. How gender, partner and dependents are related to churn? \n#### 2. How tenure has an impact on churn?\n#### 3. As the dataset is about telecom industry, we need some insights on phone and internet services!\n#### 4. Does Tech support have any impact on churn?\n#### 5. Is there any relationship of churn with monthly charges or total charges?\n#### 6. How is the service for customers who opted for streaming?\n#### 7. How contract is impacting business the most?","02753390":"##### Observations: more churning takes place in first 10 yrs (max in first year itself), for customers with or without tech support. But Churning is more in case of \"without tech support\" customers","3180cec9":"##### Observations: No missing values observed","e063db16":"##### Observations: Most of the columns are of **object** type, with seemingly no missing values","06cde80f":"#### Splitting data into dependent and indendent variables","fe92ffdc":"##### Observation: During 0-10 years of tenure, we can see maximum churning. As the customer turns old, they might get habituated using same telecom service","b0cbe348":"### Observations: clearly visible that customers with month-to-month contract are the highest churners","5c87b90d":"### 2. Does tenure has any impact on churn?","d7c277e2":"## Data Pre-processing","57d3b0a7":"### 4. Does Tech support have any impact on churn, considering the tenure?","55fe6ec2":"## Final Result: Balancing the data, clearly improved model's performance.","6a59b89f":"### Dealing with data imbalance!!\n* Choosing a learning algorithm that provide weights for every class\n* Data-level approach:\n    1. Under-sampling,\n    2. over-sampling\n    3. Cluster-based over sampling\n    4. Synthetic minority over-sampling technique(SMOTE)\n        * The authors of the technique recommend using SMOTE on the minority class, followed by an undersampling technique on the majority class.\n            * SMOTE + Tomek Links (Tomek links helps to identify paris of Nearest Neighbors that have different classes, and then removing these pairs (from majority class). It helps in making decision boundary less noisy.)\n\n            * So, SMOTE is applied to oversample minority class and then Tomek links from majority classes are identified and removed (undersampling)\n\n            * SMOTE + Edited NearestNeighbors (ENN (Edited Nearest Neighbors)), using k=3 nearest neighbors to find those examples in the dataset that have been misclassified and then remove them. It can be applied to only majority class examples or all the classes.\n","dfbc0082":"#### Converting 'TotalCharges'from categorical to float type","d7fdfe0f":"#### Customers who didn't took tech support","31fd5878":"##### Observations: People with Phone services (yes) and 'Fiber optic' Internet Service are churning more","3096fc97":"## Data Description","17fd108d":"## Missing Values","1985acbc":"### Observations: using .info() and .describe() method we can observe that although 'TotalCharges' have been considered as object, but in reality it is numeric. So we must further investigate","5724a937":"##### Observations: The data is clearly imbalanced (More data of non-churners)\n##### As Data is imbanced, we shouldn't consider \"Accuracy\" to measure the performance of the model. Accuracy will give wrong measure, for eg. lets just print \"No\" for every example, then also majority of times we will be correct.","4dcbc547":"### Model creation and prediction using imbalanced data","950b5a74":"## Model Building","dc066c15":"### About features available:\n  #### 1.customerID - customer id\n  #### 2. gender - client's gender (male \/ female)\n  #### 3. SeniorCitizen - is the client retired (1-yes, 0-no)\n  #### 4. Partner - is the client married (Yes, No)\n  #### 5. tenure - how many months a person has been a client of the company\n  #### 6. PhoneService - is the telephone service connected (Yes, No)\n  #### 7. MultipleLines - are multiple phone lines connected (Yes, No, No phone service)\n  #### 8. InternetService - client's Internet service provider (DSL, Fiber optic, No)\n  #### 9. OnlineSecurity - is the online security service connected (Yes, No, No internet service)\n  #### 10. OnlineBackup - is the online backup service activated (Yes, No, No internet service)\n  #### 11. DeviceProtection - does the client have equipment insurance (Yes, No, No internet service)\n  #### 12. TechSupport - is the technical support service connected (Yes, No, No internet service)\n  #### 13. StreamingTV - is the streaming TV service connected (Yes, No, No internet service)\n  #### 14. StreamingMovies - is the streaming cinema service activated (Yes, No, No internet service)\n  #### 15. Contract - type of customer contract (Month-to-month, One year, Two year)\n  #### 16. PaperlessBilling - whether the client uses paperless billing (Yes, No)\n  #### 17. PaymentMethod - payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n  #### 18. MonthlyCharges - current monthly payment\n  #### 19. TotalCharges - the total amount that the client paid for the services for the entire time\n  #### 20.**Churn** - whether there was a churn (Yes or No)","0c1f8e97":"#### Observation: Whether male or female, if they do not have partner or dependents, they are more likely to churn!!","013de039":"#### Feature Scaling: MinMaxScaler","d870a52b":"##### Observations: As the monthy charges are incresing, we can see the density increasing too (60-120), which means more churning with increasing monthly charges","d08f1c26":"#### Feature Scaling - MinMax Scaler","60b249c1":"##### Observation: Churning is being observed equally for both the 'Yes', 'No' group of StreamingMovies","e349b910":"##### Observation: Churning is being observed equally for the 'Yes', 'No' group of whether connected StreamingTv or not!","4e9e4106":"#### One Hot encoding of categorical columns","86e22344":"#### Label encoding the target column","31ffa811":"#### About KDE\n* kernel density estimate (KDE) plot is a method for visualizing the distribution of observations in a dataset, analagous to a histogram. \n* KDE represents the data using a continuous probability density curve in one or more dimensions."}}