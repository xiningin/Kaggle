{"cell_type":{"e56ac545":"code","a8c2e98f":"code","f2079dc6":"code","a8ee0d1e":"code","8da7e600":"code","fa2134aa":"code","3423d0ac":"code","3845a912":"markdown","40b03c1a":"markdown","00c21293":"markdown","561c5674":"markdown","020ba7b2":"markdown","41bef56b":"markdown","cc8cea86":"markdown"},"source":{"e56ac545":"!pip install av\nimport av\nprint(av.__version__)","a8c2e98f":"import torch\n\nfrom torchvision import transforms\nfrom torchvision.datasets import UCF101","f2079dc6":"ucf_data_dir = \"\/kaggle\/input\/ucf101\/UCF101\/UCF-101\"\nucf_label_dir = \"\/kaggle\/input\/ucf101\/UCF101TrainTestSplits-RecognitionTask\/ucfTrainTestlist\"\nframes_per_clip = 5\nstep_between_clips = 1\nbatch_size = 32","a8ee0d1e":"tfs = transforms.Compose([\n            # TODO: this should be done by a video-level transfrom when PyTorch provides transforms.ToTensor() for video\n            # scale in [0, 1] of type float\n            transforms.Lambda(lambda x: x \/ 255.),\n            # reshape into (T, C, H, W) for easier convolutions\n            transforms.Lambda(lambda x: x.permute(0, 3, 1, 2)),\n            # rescale to the most common size\n            transforms.Lambda(lambda x: nn.functional.interpolate(x, (240, 320))),\n])","8da7e600":"def custom_collate(batch):\n    filtered_batch = []\n    for video, _, label in batch:\n        filtered_batch.append((video, label))\n    return torch.utils.data.dataloader.default_collate(filtered_batch)","fa2134aa":"# create train loader (allowing batches and other extras)\ntrain_dataset = UCF101(ucf_data_dir, ucf_label_dir, frames_per_clip=frames_per_clip,\n                       step_between_clips=step_between_clips, train=True, transform=tfs)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                                           collate_fn=custom_collate)\n# create test loader (allowing batches and other extras)\ntest_dataset = UCF101(ucf_data_dir, ucf_label_dir, frames_per_clip=frames_per_clip,\n                      step_between_clips=step_between_clips, train=False, transform=tfs)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True,\n                                          collate_fn=custom_collate)","3423d0ac":"print(f\"Total number of train samples: {len(train_dataset)}\")\nprint(f\"Total number of test samples: {len(test_dataset)}\")\nprint(f\"Total number of (train) batches: {len(train_loader)}\")\nprint(f\"Total number of (test) batches: {len(test_loader)}\")\nprint()","3845a912":"Define the typical preprocessing transforms to be used for training a neural network:","40b03c1a":"These are some minimal variables used to configure and load the dataset:","00c21293":"We also need a custom collation function in order to deal with videos with different number of audio channels (none, mono, stereo, etc.):","561c5674":"The summary of the loaded dataset is as follows:","020ba7b2":"First we have to install the Python AV dependency in order to be able to load the videos:","41bef56b":"Finally, let's load the dataset training and testing mini-batches:","cc8cea86":"The minimal imports necessary to load the dataset are:"}}