{"cell_type":{"6244a2a9":"code","1f5efb7c":"code","60a1aa2d":"code","32ffa75a":"code","68c09886":"code","2d4cd3e9":"code","48860472":"code","b62bea28":"code","8c24bb0d":"code","193012a2":"code","5a6ffd65":"code","ef8df1cd":"code","755bfc73":"code","f75f769f":"code","d5dc1028":"code","563f2a6f":"code","d3f2fb44":"code","24081060":"code","a3ad2a9d":"code","5faea7ee":"code","06900de1":"code","d1ff8687":"code","968e8190":"code","c02d00bc":"code","15435be9":"code","59812b5a":"code","e38889d9":"code","030a993e":"code","fc93519c":"code","a9da71e8":"code","40b9ac0b":"code","7612f9d0":"code","0be7a555":"code","de3ef08b":"code","18f24b39":"markdown","9b594312":"markdown","03746b9d":"markdown","53a1ad5b":"markdown","4c6d2935":"markdown","65acc1f8":"markdown","45116ccf":"markdown","d854f65c":"markdown","0c0f5199":"markdown","e0fdf73c":"markdown","effe031d":"markdown","4abdbd83":"markdown","0877866b":"markdown","bb72d21f":"markdown","e4f1128b":"markdown","17ab8414":"markdown","f1e65334":"markdown","f1c45662":"markdown","70927f81":"markdown","3c889e47":"markdown","0b21b252":"markdown","aa6948f6":"markdown","a4d158f2":"markdown","e72b1826":"markdown","c90a6b4b":"markdown","b49f681a":"markdown","a90dc1b0":"markdown","e6f432e9":"markdown","b377258d":"markdown","f364629b":"markdown"},"source":{"6244a2a9":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\n\n%matplotlib inline\nplt.style.use('seaborn-darkgrid')\npalette = plt.get_cmap('Set2')","1f5efb7c":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","60a1aa2d":"train.head(10)","32ffa75a":"test.head(10)","68c09886":"train.info()","2d4cd3e9":"msno.bar(train.iloc[:, :40])","48860472":"msno.bar(train.iloc[:, 40:])","b62bea28":"train.iloc[:, :40].describe()","8c24bb0d":"train.iloc[:, 40:-1].describe()","193012a2":"pd.DataFrame(train['SalePrice'].describe())","5a6ffd65":"plt.figure(figsize=(12, 7))\n\nsns.distplot(train['SalePrice']).set(ylabel=None, xlabel=None)\nplt.title('House price distribution histogram', fontsize=18)\nplt.show()","ef8df1cd":"train['SalePrice'] = np.log1p(train['SalePrice'])","755bfc73":"plt.figure(figsize=(12, 7))\n\nsns.distplot(train['SalePrice'])\nplt.title('House price distribution histogram after fix', fontsize=18)\nplt.show()","f75f769f":"corr_train = train.corr()\n\ncolormap = plt.cm.RdBu\n\nplt.figure(figsize=(14,12))\nplt.title('Pearson correlation matrix between features', y=1, size=15)\nsns.heatmap(corr_train, vmax=.8, square=True, cmap=colormap)\nplt.show()","d5dc1028":"train.head()","563f2a6f":"highest_corr_features = corr_train.index[\n    abs(corr_train['SalePrice']) > 0.5\n    ]\n\nplt.figure(figsize=(14,12))\nplt.title('Pearson correlation matrix between features and \"SalePrice\"', y=1, size=15)\nsns.heatmap(train[highest_corr_features].corr(), linewidths=0.1, vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\nplt.show()","d3f2fb44":"SalePrice = pd.DataFrame(corr_train['SalePrice'].sort_values(ascending=False))\nSalePrice","24081060":"features = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n\nsns.pairplot(train[features])\nplt.show()","a3ad2a9d":"y_train = train['SalePrice']\ntest_id = test['Id']\ndata = pd.concat([train, test], axis=0, sort=False)\ndata = data.drop(['Id', 'SalePrice'], axis=1)","5faea7ee":"Total = data.isnull().sum().sort_values(ascending=False)\npercent = (data.isnull().sum() \/ data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([Total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","06900de1":"data.drop((missing_data[missing_data['Total'] > 5]).index, axis=1, inplace=True)\nprint(data.isnull().sum().max())","d1ff8687":"# numeric data\nnumeric_missed = ['BsmtFinSF1',\n                  'BsmtFinSF2',\n                  'BsmtUnfSF',\n                  'TotalBsmtSF',\n                  'BsmtFullBath',\n                  'BsmtHalfBath',\n                  'GarageArea',\n                  'GarageCars']\n\nfor feature in numeric_missed:\n    data[feature].fillna(0, inplace=True)","968e8190":"# categorical data\ncategorical_missed = ['Exterior1st',\n                  'Exterior2nd',\n                  'SaleType',\n                  'MSZoning',\n                   'Electrical',\n                     'KitchenQual']\n\nfor feature in categorical_missed:\n    data[feature].fillna(data[feature].mode()[0], inplace=True)","c02d00bc":"data['Functional'].fillna('Typ', inplace=True)","15435be9":"data.isnull().sum().max() ","59812b5a":"from scipy.stats import skew","e38889d9":"numeric = data.dtypes[data.dtypes != 'object'].index\nskewed = data[numeric].apply(lambda col: skew(col)).sort_values(ascending=False)\nskewed = skewed[abs(skewed) > 0.5]\n\nfor feature in skewed.index:\n    data[feature] = np.log1p(data[feature])","030a993e":"data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']","fc93519c":"data = pd.get_dummies(data)\ndata","a9da71e8":"x_train = data[:len(y_train)]\nx_test = data[len(y_train):]","40b9ac0b":"from sklearn.metrics import make_scorer\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error","7612f9d0":"!pip install xgboost","0be7a555":"import xgboost as XGB\n\nxgb_model = XGB.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, random_state =7, nthread = -1)\nxgb_model.fit(x_train, y_train)","de3ef08b":"y_predict = np.floor(np.expm1(xgb_model.predict(x_test)))\n\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['SalePrice'] = y_predict\nsub.to_csv('submission.csv',index=False)","18f24b39":"#### As we can see, we have a positive skew, we must fix it.","9b594312":"### Good job! Now we know important features","03746b9d":"### Feature Engineering","53a1ad5b":"#### Let's build a Pearson correlation matrix","4c6d2935":"### Converting the categorical to numerical.","65acc1f8":"##### first group","45116ccf":"#### Import required libraries ","d854f65c":"### We will make a work plan","0c0f5199":"### Modeling and predicting","e0fdf73c":"#### Almost done! That is enough","effe031d":"#### We can safely remove these features as they are not important and do not have a high correlation.","4abdbd83":"* #### Saleprice is highly correlated with OverallQual\n* #### GarageArea logically has a great relationship with GarageCars\n* #### Have the smallest connection YearBuilt and TotRmsAbvGrd\n* #### Also highly correlated 1stFirSF and TotalBsmtSF\n* #### TotRmsAbvGrd is highly correlated with GrLivArea","0877866b":"### Let's find and fill in the missing data","bb72d21f":"* #### Analysis of the target\n* #### Filling missing values \n* #### Feature Engineering\n* #### Converting categorical to numerical\n* #### Modeling and predicting\n\n","e4f1128b":"#### Read train and test datasets","17ab8414":"### Let's celebrate","f1e65334":"Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","f1c45662":"#### As we can see there are a lot of missing values in some columns, not missing values do not exceed the mark of 30. We'll fix it after a while!","70927f81":"#### Display information on values in columns","3c889e47":"#### We see the relationship (correlation) between the features, but let's see the correlation between the \"Price of houses\" and features","0b21b252":"## House sales predict","aa6948f6":"### We cleaned the data very well, good job!","a4d158f2":"#### Let's combine training and test datasets for convenience","e72b1826":"#### Fix The Skewness in the other features","c90a6b4b":"#### Display the description of the values in the columns","b49f681a":"#### display a graph of missing values","a90dc1b0":"#### Let's take only strongly related features","e6f432e9":"#### The simplest is to use the function pd.get_dummies()","b377258d":"#### its ok now","f364629b":"#### Display first 10 rows from train and test datasets"}}