{"cell_type":{"9ae3a7cc":"code","57b0a57f":"code","6b179c83":"code","381785e9":"code","a36f7a1d":"code","5e11af2b":"code","95d24aaa":"code","7e99e362":"code","70e46bb7":"code","d122531c":"markdown","e7cf998d":"markdown","40b7b102":"markdown"},"source":{"9ae3a7cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","57b0a57f":"ls '..\/input\/aptos2019-blindness-detection\/'","6b179c83":"DATA_DIR = '..\/input\/aptos2019-blindness-detection\/'\ntrain_label_dir = os.path.join(DATA_DIR, 'train.csv')\ntest_label_dir = os.path.join(DATA_DIR, 'test.csv')\n\ndf_train = pd.read_csv(train_label_dir)\ndf_test = pd.read_csv(test_label_dir)\n\nprint(df_train.head())\ndf_test.head()","381785e9":"os.listdir(DATA_DIR+'train_images')","a36f7a1d":"import numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\n\nfrom torchvision import transforms\n\nfrom PIL import Image\n\nDATA_DIR = '..\/input\/aptos2019-blindness-detection\/'\nMODEL_DIR = '..\/input\/'","5e11af2b":"# class that loads the Train\/Validation dataset\nclass DRDatasetTrain(Dataset):\n    def __init__(self, val_set=False, val_size=0.2, random_state=42):\n        train_label_dir = os.path.join(DATA_DIR, 'train.csv')       # save path to train.csv file - DATA_DIR + 'train.csv'\n        \n        df = pd.read_csv(train_label_dir)                           # load train.csv to a Pandas dataframe \n        \n        train_size = int((1 - val_size) * len(df)   )               # save size of training data to use - has to be an integer\n        df_train = df.sample(train_size, random_state=random_state) # save a sample of size train_size from the df dataframe\n                                                                    # random state allows us to get the same sample data if we run the code again later\n        idx = [i for i in df.index if i not in df_train.index]      # save the indices of data not in df_train\n        \n        if val_set:\n            df_train = df[idx]     # if we have set val_set=True \n        \n        # saves the dataframe to be used later\n        # drop=True avoids keeping the current index as a new column\n        self.data = df_train.reset_index(drop=True) \n        \n    # this function is called when len() function is called on this class' objects\n    # >>> train_data = DRDatasetTrain()   >>> len(train_data) \n    def __len__(self):\n        return len(self.data)\n    \n    # this function is called when this class' object is index\n    # >>> train_data = DRDatasetTrain()   >>> train_data[2]\n    def __getitem__(self, idx):\n        # idx - the index of the datapoint we have to return\n        id_code = str(self.data.loc[idx, 'id_code'])                      # save the 'id_code' field in the row no. 'idx' in the train.csv file\n        file_name = id_code + '.png'\n        \n        img_file = os.path.join(DATA_DIR, 'train_images', file_name)  # save path of the image file\n        img = Image.open(img_file)                                         # open the image file using PIL libraries Image class\n        \n        # transforms to be used on the image data\n        transform = transforms.Compose([transforms.Resize((224, 224)),                    # resize images to 224x224\n                                         transforms.ToTensor(),                           # convert PIL image to torch Tensor\n                                         transforms.Normalize([0.485, 0.456, 0.406],      # normalize image pixels - two lists correspond to mean and std\n                                                              [0.229, 0.224, 0.225])])\n        img_tensor = transform(img) # transform the image to be returned\n        label = self.data.loc[idx, 'diagnosis']  # save the label of the image to be returned\n        \n        return (img_tensor, label)# return the image and label as a dictionary","95d24aaa":"# class that loads the Test dataset\nclass DRDatasetTest(Dataset):\n    def __init__(self, val_set=False, val_size=0.2, random_state=42):\n        test_label_dir = os.path.join(DATA_DIR, 'train.csv') # save path to train.csv file - DATA_DIR + 'test.csv'\n        df = pd.read_csv(test_label_dir)                     # load test.csv to a Pandas dataframe \n        self.data = df.reset_index(drop=True)\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        id_code = str(self.data.loc[idx, 'id_code'])                     # save the 'id_code' field in the row no. 'idx' in the train.csv file\n        file_name = id_code + '.png'\n        \n        img_file = os.path.join(DATA_DIR, 'test_images', file_name)  # save path of the image file\n        img = Image.open(img_file)    # open the image file using PIL libraries Image class\n        \n        # transforms to be used on the image data\n        transform = transforms.Compose([transforms.Resize((224, 224)),\n                                         transforms.ToTensor(),\n                                         transforms.Normalize([0.485, 0.456, 0.406], \n                                                                   [0.229, 0.224, 0.225])])\n        img_tensor = transform(img) # transform the image to be returned\n        \n        return {'image': img_tensor} # return the image as a dictionary","7e99e362":"# DataLoader class is used to return data from the dataset in a controlled manner\n# batch_size - how many datapoints are returned in each iteration(try reducing this if model takes too much time in each epoch)\n# drop_last - should we drop the last batch of data if it's length not equal to batch_size\n# shuffle - whether to shuffle the datapoints when returned in each epoch(one complete iteration over entire data)\ntrainloader = torch.utils.data.DataLoader(DRDatasetTrain(), batch_size=32, drop_last=False, shuffle=False) \ntestloader = torch.utils.data.DataLoader(DRDatasetTest(), batch_size=32, drop_last=False, shuffle=False)","70e46bb7":"# make an iterator out of trainloader and use next() to get a batch(batch_size) of data\nimages, labels = next(iter(trainloader))\nprint(images.shape)\nprint(labels)","d122531c":"The *DATA_DIR* folder contains:\n* train_images - images to train on or images whose category of diabetic retinopathy is provided(in train.csv)\n* test_images - images to test our model on \n* train.csv - csv file with two columns: 1)id_code - referring to the image  2) diagnosis - category of the image\n* test.csv - csv file with one column: 1) id_code - referring to the image\n\nThe labels for test data aren't provided - we upload our model to kaggle for getting the test accuracy","e7cf998d":"We can see that the images are named as value in the *id_code* column of *train.csv* and '.png'","40b7b102":"### Visualize some images"}}