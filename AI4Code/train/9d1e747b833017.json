{"cell_type":{"1b8a1a32":"code","4025c9a2":"code","97560366":"code","3b0e3f02":"code","dd928e38":"code","e7d9c706":"code","8d4984d5":"code","2eef874f":"code","4476e389":"code","611357b4":"code","3f3d6283":"code","c8d22fb7":"code","11bb7a60":"code","d088e646":"code","b6517764":"code","956161ea":"code","3ec0c772":"code","c67b568e":"code","1a260c33":"code","c1415089":"code","ecb802d2":"code","7c4907db":"code","4a7d5966":"code","d3fe3641":"code","b70d96e2":"code","00f1347a":"code","246fe91f":"code","ed59aabf":"code","7c93df5e":"code","5c2180a8":"code","11d74ac5":"code","e69be36e":"code","24b9ad3f":"code","90aa8df1":"code","73133ddd":"code","c8cf1f3e":"code","f4db3111":"code","d9dc9249":"code","112ba92f":"code","4b19d5d8":"code","91a4bcc3":"code","80dab30d":"code","0ebadd9e":"code","d573da15":"code","0a4fa073":"code","7ee4c098":"code","7dc02747":"code","68d6c65f":"code","1e8a3a8b":"code","dd202d07":"code","37fc845a":"code","a95c0517":"code","f98efb6a":"code","9bbbaa6c":"code","985bbfc1":"code","670734bf":"code","ee5488aa":"code","438c0f67":"code","1262b01e":"code","0b48a321":"code","f22c7cff":"code","0851da2b":"code","63406e66":"code","12377830":"code","64b84cae":"code","cbfa33e5":"code","2dfa2409":"code","0a42ad0d":"code","a2ba8e41":"code","d5976759":"code","36f96578":"code","07c2b01a":"markdown","54700915":"markdown","1bc0cb2d":"markdown","43205f92":"markdown","9f807457":"markdown","cab8c000":"markdown","4a27fc35":"markdown","7ab46a05":"markdown","582db1ea":"markdown","1a372ad2":"markdown","5fd861eb":"markdown","80eb1dfd":"markdown","b82401d3":"markdown","27c99bd1":"markdown","4f6de6bc":"markdown","87ead745":"markdown","a2cce277":"markdown","f99b5568":"markdown","25bc5022":"markdown","e61b35d3":"markdown","5c06f365":"markdown","383b4f6a":"markdown","334b4b38":"markdown","3f09a2f7":"markdown","baf842d1":"markdown","7bcc793d":"markdown","eea0d708":"markdown","8f46ea02":"markdown","4a6502d5":"markdown","92d0ff4a":"markdown","c9cf1904":"markdown","f12b034a":"markdown","47038cd6":"markdown","76ca6972":"markdown","eb29068d":"markdown","30e901a3":"markdown","b8508c9c":"markdown","a2bf4c04":"markdown","a0382a62":"markdown","8d7f2241":"markdown","eb517474":"markdown","dc4cdb26":"markdown","188b8019":"markdown","cfd1431a":"markdown","a4031d28":"markdown","e12d8fab":"markdown","966c2ae6":"markdown","d49f00ef":"markdown","79001f2c":"markdown","78989586":"markdown","ab3d19c4":"markdown","16ca1f39":"markdown","244afa7c":"markdown","44a3296a":"markdown","a9ffa8e4":"markdown","00f9a7f8":"markdown","55b85683":"markdown"},"source":{"1b8a1a32":"import numpy as np\nimport pandas as pd \n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sbrn\n\nfrom sklearn.preprocessing import OneHotEncoder, label_binarize\n\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, datasets, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom catboost import CatBoostClassifier, Pool, cv\n\nfrom IPython.display import HTML\nimport base64","4025c9a2":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsubmission = pd.read_csv('..\/input\/gender_submission.csv')","97560366":"train.head(15)","3b0e3f02":"len(train)","dd928e38":"train.describe()","e7d9c706":"train.Age.plot.hist()","8d4984d5":"train.Pclass.plot.hist()","2eef874f":"train.Fare.plot.hist()","4476e389":"missingno.matrix(train)","611357b4":"train.isnull().sum()","3f3d6283":"train.dtypes","c8d22fb7":"sbrn.countplot(y='Survived', data=train);\nprint(train.Survived.value_counts())","11bb7a60":"#Separating our variables into categorical and continuous variables within dataframes, makes exploration easier\n\ndf_cat = pd.DataFrame()\ndf_con = pd.DataFrame()","d088e646":"#To explore the variables as related to the outcome of survival, we'll add the outcome to the empty dataframes\n\ndf_cat['Survived'] = train['Survived']\ndf_con['Survived'] = train['Survived']\n\n#Adding Pclass to the dataframes since no data is missing for that variable\n\ndf_cat['Pclass'] = train['Pclass']\ndf_con['Pclass'] = train['Pclass']\n\n#Adding Sex to the dataframes since no data is missing for that variable\n\ndf_cat['Sex'] = train['Sex']\n#This next line of code will recode the variables into 0\/1; female = 1, male = 0. Male = 0 was decided since the idea of \"women and children first\" would probably lead to higher survival for 'female'\ndf_cat['Sex'] = np.where(df_cat['Sex'] == 'male', 1, 0)\ndf_con['Sex'] = train['Sex']","b6517764":"train.groupby(['Sex', 'Survived']).count()","956161ea":"train.SibSp.value_counts()","3ec0c772":"train.groupby(['SibSp', 'Survived']).count()","c67b568e":"df_cat['SibSp'] = train['SibSp']\n# We will rewrite them as categories where 0=0, 1=1, 2-8=2\ndf_cat['SibSp'] = pd.cut(train['SibSp'], [0,1,2,8], labels=['0','1','2+'], right=False)\n\ndf_con['SibSp'] = train['SibSp']","1a260c33":"train.Parch.value_counts()","c1415089":"train.groupby(['Parch', 'Survived']).count()","ecb802d2":"train.groupby(['Parch', 'SibSp']).count()","7c4907db":"df_cat['Parch'] = train['Parch']\n# We will rewrite them as categories where 0=0, 1=1, 2=2, 3=3-6\ndf_cat['Parch'] = pd.cut(train['Parch'], [0,1,2,3,6], labels=['0','1','2','3+'], right=False)\n\ndf_con['Parch'] = train['Parch']","4a7d5966":"train.Ticket.value_counts()","d3fe3641":"from scipy.stats import boxcox\n\ndf_cat['Fare'] = train['Fare'] + 0.1\ndf_cat['Fare'] = boxcox(df_cat.Fare)[0]\ndf_cat['Fare'] = pd.cut(df_cat['Fare'], bins=5)\n\ndf_con['Fare'] = train['Fare']","b70d96e2":"train['Cabin'].head(15)","00f1347a":"df_cat['Cabin'] = train['Cabin']\ndf_cat['Cabin'] = df_cat['Cabin'].replace(np.nan, '0')\n\ndf_cat['Cabin'] = df_cat['Cabin'].str.replace('.*A+.*', '1')\ndf_cat['Cabin'] = df_cat['Cabin'].str.replace('.*B+.*', '1')\ndf_cat['Cabin'] = df_cat['Cabin'].str.replace('.*C+.*', '1')\ndf_cat['Cabin'] = df_cat['Cabin'].str.replace('.*D+.*', '1')\ndf_cat['Cabin'] = df_cat['Cabin'].str.replace('.*E+.*', '1')\ndf_cat['Cabin'] = df_cat['Cabin'].str.replace('.*F+.*', '1')\ndf_cat['Cabin'] = df_cat['Cabin'].str.replace('.*G+.*', '1')\n","246fe91f":"train[train.Embarked.isna()]","ed59aabf":"train.groupby(['Embarked',train['Cabin'].str.contains(\"B\", na=False)]).count()","7c93df5e":"df_cat['Embarked'] = train['Embarked']\ndf_cat.set_value(61, 'Embarked', 'S')\ndf_cat.set_value(829, 'Embarked', 'S')\n\ndf_con['Embarked'] = df_cat['Embarked']","5c2180a8":"df_cat.head()","11d74ac5":"# One-Hot Encoding employed\nonehot_cols = df_cat.columns.tolist()\nonehot_cols.remove('Survived')\ndf_cat_enc = pd.get_dummies(df_cat, columns=onehot_cols)\n\ndf_cat_enc.head(8)","e69be36e":"df_con.head(15)","24b9ad3f":"df_embarked_onehot = pd.get_dummies(df_con['Embarked'], \n                                     prefix='embarked')\n\ndf_sex_onehot = pd.get_dummies(df_con['Sex'], \n                                prefix='sex')\n\ndf_plcass_onehot = pd.get_dummies(df_con['Pclass'], \n                                   prefix='pclass')","90aa8df1":"# Now lets remove the original variables\ndf_con_enc = pd.concat([df_con, \n                        df_embarked_onehot, \n                        df_sex_onehot, \n                        df_plcass_onehot], axis=1)\n\n# Drop the original categorical columns (because now they've been one hot encoded)\ndf_con_enc = df_con_enc.drop(['Pclass', 'Sex', 'Embarked'], axis=1)","73133ddd":"df_con_enc.head(9)","c8cf1f3e":"chosen_df = df_con_enc","f4db3111":"X_train = chosen_df.drop('Survived', axis=1)\ny_train = chosen_df.Survived","d9dc9249":"def fit_ml_algo(algo, X_train, y_train, cv):\n    \n    # One Pass\n    model = algo.fit(X_train, y_train)\n    acc = round(model.score(X_train, y_train) * 100, 2)\n    \n    # Cross Validation \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  X_train, \n                                                  y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    \n    \n    # Cross-validation accuracy metric\n    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n    confus_matr = metrics.confusion_matrix(y_train, train_pred)\n    precision = precision_score(y_train, train_pred)\n    recall = recall_score(y_train, train_pred)\n    F1 = f1_score(y_train, train_pred)\n    auc = roc_auc_score(y_train, train_pred)\n    \n    return train_pred, acc, acc_cv, confus_matr, precision, recall, F1, auc","112ba92f":"train_pred_log, acc_log, acc_cv_log, cfmt_log, prec_log, rec_log, F1_log, auc_log = fit_ml_algo(LogisticRegression(), \n                                                                                               X_train, \n                                                                                               y_train, \n                                                                                                10)\nprint(\"Accuracy: %s\" % acc_log)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)\nprint(\"Confusion Matrix: %s\" % cfmt_log)\nprint(\"Precision: \", float(\"{0:.3f}\".format(round(prec_log, 3))))\nprint(\"Recall: \", float(\"{0:.3f}\".format(round(rec_log, 3))))\nprint(\"F1 Score: \", float(\"{0:.3f}\".format(round(F1_log, 3))))\nprint(\"AUC: \", float(\"{0:.3f}\".format(round(auc_log, 3))))","4b19d5d8":"train_pred_knn, acc_knn, acc_cv_knn, cfmt_knn, prec_knn, rec_knn, F1_knn, auc_knn = fit_ml_algo(KNeighborsClassifier(), \n                                                                                                X_train, \n                                                                                                y_train, \n                                                                                                10)\nprint(\"Accuracy: %s\" % acc_knn)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)\nprint(\"Confusion Matrix: %s\" % cfmt_knn)\nprint(\"Precision: \", float(\"{0:.3f}\".format(round(prec_knn, 3))))\nprint(\"Recall: \", float(\"{0:.3f}\".format(round(rec_knn, 3))))\nprint(\"F1 Score: \", float(\"{0:.3f}\".format(round(F1_knn, 3))))\nprint(\"AUC: \", float(\"{0:.3f}\".format(round(auc_knn, 3))))","91a4bcc3":"train_pred_svc, acc_linear_svc, acc_cv_linear_svc, cfmt_svc, prec_svc, rec_svc, F1_svc, auc_svc = fit_ml_algo(LinearSVC(),\n                                                                                                                X_train, \n                                                                                                                y_train, \n                                                                                                                10)\nprint(\"Accuracy: %s\" % acc_linear_svc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svc)\nprint(\"Confusion Matrix: %s\" % cfmt_svc)\nprint(\"Precision: \", float(\"{0:.3f}\".format(round(prec_svc, 3))))\nprint(\"Recall: \", float(\"{0:.3f}\".format(round(rec_svc, 3))))\nprint(\"F1 Score: \", float(\"{0:.3f}\".format(round(F1_svc, 3))))\nprint(\"AUC: \", float(\"{0:.3f}\".format(round(auc_svc, 3))))","80dab30d":"train_pred_sgd, acc_sgd, acc_cv_sgd, cfmt_sgd, prec_sgd, rec_sgd, F1_sgd, auc_sgd = fit_ml_algo(SGDClassifier(), \n                                                                                                  X_train, \n                                                                                                  y_train,\n                                                                                                  10)\nprint(\"Accuracy: %s\" % acc_sgd)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_sgd)\nprint(\"Confusion Matrix: %s\" % cfmt_sgd)\nprint(\"Precision: \", float(\"{0:.3f}\".format(round(prec_sgd, 3))))\nprint(\"Recall: \", float(\"{0:.3f}\".format(round(rec_sgd, 3))))\nprint(\"F1 Score: \", float(\"{0:.3f}\".format(round(F1_sgd, 3))))\nprint(\"AUC: \", float(\"{0:.3f}\".format(round(auc_sgd, 3))))","0ebadd9e":"train_pred_gaussian, acc_gaussian, acc_cv_gaussian, cfmt_gaussian, prec_gaussian, rec_gaussian, F1_gaussian, auc_gaussian = fit_ml_algo(GaussianNB(), \n                                                                                                              X_train, \n                                                                                                              y_train, \n                                                                                                               10)\nprint(\"Accuracy: %s\" % acc_gaussian)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gaussian)\nprint(\"Confusion Matrix: %s\" % cfmt_gaussian)\nprint(\"Precision: \", float(\"{0:.3f}\".format(round(prec_gaussian, 3))))\nprint(\"Recall: \", float(\"{0:.3f}\".format(round(rec_gaussian, 3))))\nprint(\"F1 Score: \", float(\"{0:.3f}\".format(round(F1_gaussian, 3))))\nprint(\"AUC: \", float(\"{0:.3f}\".format(round(auc_gaussian, 3))))","d573da15":"train_pred_dt, acc_dt, acc_cv_dt, cfmt_dt, prec_dt, rec_dt, F1_dt, auc_dt = fit_ml_algo(DecisionTreeClassifier(), \n                                                                                        X_train, \n                                                                                        y_train,\n                                                                                        10)\nprint(\"Accuracy: %s\" % acc_dt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_dt)\nprint(\"Confusion Matrix: %s\" % cfmt_dt)\nprint(\"Precision: \", float(\"{0:.3f}\".format(round(prec_dt, 3))))\nprint(\"Recall: \", float(\"{0:.3f}\".format(round(rec_dt, 3))))\nprint(\"F1 Score: \", float(\"{0:.3f}\".format(round(F1_dt, 3))))\nprint(\"AUC: \", float(\"{0:.3f}\".format(round(auc_dt, 3))))","0a4fa073":"train_pred_gbt, acc_gbt, acc_cv_gbt, cfmt_gbt, prec_gbt, rec_gbt, F1_gbt, auc_gbt = fit_ml_algo(GradientBoostingClassifier(), \n                                                                                               X_train, \n                                                                                               y_train,\n                                                                                               10)\nprint(\"Accuracy: %s\" % acc_gbt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gbt)\nprint(\"Confusion Matrix: %s\" % cfmt_gbt)\nprint(\"Precision: \", float(\"{0:.3f}\".format(round(prec_gbt, 3))))\nprint(\"Recall: \", float(\"{0:.3f}\".format(round(rec_gbt, 3))))\nprint(\"F1 Score: \", float(\"{0:.3f}\".format(round(F1_gbt, 3))))\nprint(\"AUC: \", float(\"{0:.3f}\".format(round(auc_gbt, 3))))","7ee4c098":"## Remove non-categorical variables\ncat_features = np.where(X_train.dtypes != np.float)[0]","7dc02747":"train_pool = Pool(X_train, \n                  y_train,\n                  cat_features)","68d6c65f":"# CatBoost model \ncatboost_model = CatBoostClassifier(iterations=1000,\n                                    custom_loss=['Accuracy'],\n                                    loss_function='Logloss')\n\n# Fit CatBoost model\ncatboost_model.fit(train_pool,\n                   plot=False)\n\n# CatBoost accuracy\nacc_catboost = round(catboost_model.score(X_train, y_train) * 100, 2)","1e8a3a8b":"# Set params for cross-validation\ncv_params = catboost_model.get_params()\n\n# Run the cross-validation for 10-folds\ncv_data = cv(train_pool,\n             cv_params,\n             fold_count=10,\n             plot=False)\n\n\n# CatBoost CV results save into a dataframe (cv_data), let's withdraw the maximum accuracy score\nacc_cv_catboost = round(np.max(cv_data['test-Accuracy-mean']) * 100, 2)","dd202d07":"# Print out the CatBoost model metrics\nprint(\"---CatBoost Metrics---\")\nprint(\"Accuracy: {}\".format(acc_catboost))\nprint(\"Accuracy cross-validation 10-Fold: {}\".format(acc_cv_catboost))","37fc845a":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_knn, \n        acc_log,  \n        acc_gaussian, \n        acc_sgd, \n        acc_linear_svc, \n        acc_dt,\n        acc_gbt,\n        acc_catboost\n    ]})\nprint(\"---Reuglar Accuracy Scores---\")\nmodels.sort_values(by='Score', ascending=False)","a95c0517":"cv_models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_cv_knn, \n        acc_cv_log,      \n        acc_cv_gaussian, \n        acc_cv_sgd, \n        acc_cv_linear_svc, \n        acc_cv_dt,\n        acc_cv_gbt,\n        acc_cv_catboost\n    ]})\nprint('---Cross-validation Accuracy Scores---')\ncv_models.sort_values(by='Score', ascending=False)","f98efb6a":"model_gbt = GradientBoostingClassifier()\nmodel_gbt.fit(X_train, y_train)\n# plot feature importance\nprint(model_gbt.feature_importances_)\nplt.bar(range(len(model_gbt.feature_importances_)), model_gbt.feature_importances_)\nplt.show()","9bbbaa6c":"def feature_importance(model, data):\n    \"\"\"\n    Function to show which features are most important in the model.\n    ::param_model:: Which model to use?\n    ::param_data:: What data to use?\n    \"\"\"\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n    _ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))\n    return fea_imp","985bbfc1":"feature_importance(catboost_model, X_train)","670734bf":"\nmetrics = ['Precision', 'Recall', 'F1', 'AUC']\n\neval_metrics = catboost_model.eval_metrics(train_pool,\n                                           metrics=metrics,\n                                           plot=False)\n\nfor metric in metrics:\n    print(str(metric)+\": {}\".format(np.mean(eval_metrics[metric])))","ee5488aa":"test_embarked_onehot = pd.get_dummies(test['Embarked'], \n                                     prefix='embarked')\n\ntest_sex_onehot = pd.get_dummies(test['Sex'], \n                                prefix='sex')\n\ntest_plcass_onehot = pd.get_dummies(test['Pclass'], \n                                   prefix='pclass')","438c0f67":"# Now lets remove the original variables\ntest = pd.concat([test, \n                        test_embarked_onehot, \n                        test_sex_onehot, \n                        test_plcass_onehot], axis=1)","1262b01e":"test.head(3)","0b48a321":"want_test_colum = X_train.columns\nwant_test_colum","f22c7cff":"def create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)","0851da2b":"predict_test_cb = catboost_model.predict(test[want_test_colum])","63406e66":"submission_catboost = pd.DataFrame()\nsubmission_catboost['PassengerId'] = test['PassengerId']\nsubmission_catboost['Survived'] = predict_test_cb\nsubmission_catboost['Survived'] = submission_catboost['Survived'].astype(int)\nsubmission_catboost.head()","12377830":"create_download_link(submission_catboost)","64b84cae":"test[test.Fare.isna()]","cbfa33e5":"df_con.groupby(['Pclass', 'Fare']).count()","2dfa2409":"test.set_value(152, 'Fare', '55.7875')\ntest.iloc[152]","0a42ad0d":"test[want_test_colum].isna().sum()","a2ba8e41":"predict_test_gbt = model_gbt.predict(test[want_test_colum])","d5976759":"submission_gbt = pd.DataFrame()\nsubmission_gbt['PassengerId'] = test['PassengerId']\nsubmission_gbt['Survived'] = predict_test_gbt\nsubmission_gbt['Survived'] = submission_gbt['Survived'].astype(int)\nsubmission_gbt.head()","36f96578":"create_download_link(submission_gbt)","07c2b01a":"I might impute those two values if I can find a relationship with maybe the Cabin or Ticket. The two missing Embarkment examples seem to be related to each other since they have the same ticket number, fare and cabin.","54700915":"I'm going to group Parch as 0, 1, 2, 3+ since 3-6 have similar relationships. ","1bc0cb2d":"We're going to now 0\/1 encode the features in the df_cat dataframe. ","43205f92":"## Decision Tree\n\n**Pros: east to interpet; non-parametric; good for few categorical variables\n\n**Cons: easily overfits","9f807457":"## Preparing Data Set for Training with Feature Encoding","cab8c000":"### Pool() will combine our values from the training data and categories together. ","4a27fc35":"GBT doesn't handle missing data well, so I might need to impute data for the row missing a Fare value (which is odd). ","7ab46a05":"From looking at the variable types, we see which variables are continuous\/numerical and which are categorical. Pclass although an integer, probably makes more sense as a categorical variable. Name and Sex are categorical variables. Age is a continuous (float) variable since children under 1 are coded with one decimal place (e.g. 0.5). SibSp and Parch are continuous but could also make more sense as a categorical variable. Ticket is categorical, but not sure what can be done with this variable yet since I assume the tickets may not follow a pattern thus being unique. Lastly, Cabin and Port of Embarkement are categorical, which makes sense in its current form.","582db1ea":"## Gaussian Naive Bayes\n\n**Pros: very simple; requires less training data; converges quicker; few features\n\n**Cons: multicollinearity can be an issue","1a372ad2":"## Testing Machine Learning Methods Over Clean and Manipulated Datasets\n\nI suspect that a logistic regression would probably do just fine, but let's find out!","5fd861eb":"Given the performance of my Gradient Boosting Trees (AUC=79%) and CatBoost (AUC=88%), I'm going to go with CatBoost as my final model for predicting on the test set.","80eb1dfd":"From the quartiles that we saw above, I estimated that 50% to 75% of the training examples did not survive. Here we see 342 people did survive and 549 people did not survive. Roughly 61% of the examples in the training dataset did not survive.\n\n## Adding Non-missing Variables to Dataframes: PClass, Sex","b82401d3":"This is a weird variable and I'm not sure what to do with it. I'll skip over this variable for now.","27c99bd1":"From the histogram for ticket class, we see that roughly half were in the 3rd class, a quarter in the 2nd class, and a quarter in the 1st class. ","4f6de6bc":"## Features: SibSp ","87ead745":"## K-Nearest Neighbor (KNN)\n\n**Pros: no assumption about data; simple; relative high accuracy; versatile\n\n**Cons: computationally heavy; high memory usage; stores the training data; prediction might be slow; sensitive to unneeeded features and scale","a2cce277":"## Linear Support Vector Machine (SVM)\n\n**Pros: good with high-dimensional space; good for non-linear problems; high accuracy; flexible; multicollinearity is not an issue\n\n**Cons: hard to interpret; inefficient to train; better for smaller problems; not meant for massive sets","f99b5568":"### Gradient Boost Tree","25bc5022":"## Submitting Final Predictions from GBT and CatBoost\n\nWe'll need to transform the Test dataset to look like our Train dataset.","e61b35d3":"This next section, I used a lot of Google searching and helping to figure out how to execute what I wanted to execute. I want to first try the methods I have picked up from my Machine Learning course with Andrew Ng: Logistic Regression, K-Nearest Neighbor, Linear Support Vector Machines, and Stochastic Gradient Descent. There are some other methods that I am not familiar with but will test them out. I'll explain the pros and cons of each method. \n\nI'm going to borrow some code that will streamline the information I want from testing my models. It takes as inputs the mode you wan tot use, your X_train values, your y_train values and cross validation proportion. ","5c06f365":"Age, as calculate above, has 177 missing, while Cabin has 687 missing. ","383b4f6a":"From visual inspection, it seems that those who had 2 or more siblings onboard, likely didn't survive. This cutoff of \"0, 1, 2+\" seems reasonabl given the differences between the groups. ","334b4b38":"From the histogram for fare, we see that the vast majority of people paid less than 100 for their fare. ","3f09a2f7":"## Features: Cabin\nSince there are a lot of missing values for Cabin, I might create a new feature that is 0\/1 to determine whether or not it's there.","baf842d1":"## Features: Fare\n\nGiven the distribution we saw, we might scale the feature. I found a cool new method to deal with very skewed, odd data. It's called the Box-Cox transformation. ","7bcc793d":"## Exploring the Train Dataset\nNow, we want to just preview what the dataset looks like.","eea0d708":"Given his Pclass, I might see if I can impute from the relationship between Pclass and Fare. ","8f46ea02":"## Stochastic Gradient Descent\n\n**Pros: updates parameters example by example; gets to minimum relatively quickly; not as computationally heavy; \n\n**Cons: variance can be an issue","4a6502d5":"## Features: Tickets","92d0ff4a":"My assumption that women were more likely to survive than men can be confirmed by exploring Sex by survival status.","c9cf1904":"From this, we can see that we have full data for all of the examples except for age where roughly 177 examples do not have an age. Although it's not necessary to pinpoint exactly the percentage, but it seems that between 50% to 75% of people did not survive. Most people in the dataset were of lower SES based on their ticket class. Most people did not have a sibling onboard or children with them. And most people paid less than 31 for their fare.","f12b034a":"## Target Feauture\nNow, we'll want to look at survivorshop of our training data set.","47038cd6":"### Creating GBT Prediction CSV","76ca6972":"From this, I'm going to imput the average value between 16.1000 and 39.6875, since it seems like that majority of the Pclass are between there, which is 55.7875.","eb29068d":"## Logistic Regression\n\n**Pros: easy to understand; when features and problem is pretty linear; robust to noise; efficient; can avoid overfitting\n\n**Cons: doesn't do well with categorical variables; multicollinearity can be a problem","30e901a3":"## Model Results","b8508c9c":"## Catboost Algorithm\n\nThis seems to be a new one that deals with categorical variables really well.\n\n**Cons: takes a while to run","a2bf4c04":"Given the distribution, I might group together 2-5 siblings onboard together. Let's see how it might relate to survival.","a0382a62":"Since there are only two missing Cabin information, I'm going to impute them with C simply because it is more likely they embarked from Southhampton than Cherbourg. ","8d7f2241":"From these results, I'd probably pick Gradient Boosting Trees or CatBoost. \n\n## Feature Importance\n\nThis will allow us to evaluate how much features are contributing to the model and whether or not we can remove some to reduce our dimensionality.","eb517474":"We're going to split the dataframe into the target feature and the independent features. ","dc4cdb26":"The variables in the training dataset are: Passenger ID, Survived(0\/1), P Class (ticket class, 1st-2nd-3rd, SES proxy), Name, Sex, Age, Sibling onboard, Parch (children on board), Ticket, Fare, Cabin and Port of Emarkment. More notes on the variables can be found on the Kaggle competition page.\n\nSurvived(0\/1) is our label for each training example. And we have 891 examples to train our model on. \n\nWe now will ask for some basic descriptions of our variables in the dataset using '.describe()'. ","188b8019":"The two variables with the most missing data are Age and Cabin. Cabin seems pretty sparse. ","cfd1431a":"As you can see here, this confirms my inkling about Sex differences in survival. 74% of women survived while only 19% of men survived. This will be important to keep note of as from just the Sex variable, we should predict women to survive more often than men. ","a4031d28":"## Features: Embarkment\n\nFrom previous views, there are a couple of missing values for Embarkment. Let's see what we can do about it. ","e12d8fab":"Parch of 2 and above have different surival dynamics from 0 and 1, so I might group them together. It seems these might be related to SibSp. If they are, we may need an interaction feature.","966c2ae6":"Pull out just the columns that we used for the training set.","d49f00ef":"### CatBoost\n\nI borrowed code to evaluated the performance of the CatBoost","79001f2c":"We'll now encode the categorical variables in the continuous dataset, and remove the original variables. ","78989586":"## Features: Parch\nWe'll do a similar exploration and categorization as SibSp.","ab3d19c4":"From this plot, we see that Fare, Sex, and PClass seem to be significant features.\n\nSibSp, Parch and Embarkment can probably be removed. ","16ca1f39":"# Learning the Process for Machine Learning: First Kaggle Competition, Titanic\n\nI will be learning the process of machine learning from beginning to end by preprocessing data, building machine learning models, and submitting my output\/predictions to my first Kaggle competition, Titanic. In addition, this will give me the opportunity to work with new libraries and troubleshoot preprocessing issues I may encounter in the future to build parismonious machine learning models. \n\nConceptually, the first thing I will need to do is to explore the data in my train set, which involves understanding the format of the variables, distributions, and interactions. Then, given that the output of the machine learning model should be 0\/1, preparing my features to be inserted into a logistic regression classifier. Finally, tweaking the feature parameters as well as inclussion given bias\/variance and performance.","244afa7c":"## Import Datasets\nThe first step is to import my datasets and save them as variables in the memory. I will use the pandas library to exectue this.","44a3296a":"This histogram for age makes sense. Younger people were probably more excited to try out new technologies and be a part of a big event which is why we see that spike at around 18 years old. The low numbers under the age of 18 make sense since minors probably couldn't go unaccompanied and parents may have not wanted them on the trip.\n\n**** We might want to create a feature that lets us know whether they are children, adults, or seniors. We should explore the variable a little more to decide if these groupings could be helpful.**","a9ffa8e4":"### Creating CatBoost Prediction CSV","00f9a7f8":"Although this is not the most elegant coding, I wanted to code those with Cabin information available to be coded as 1 and those without to be coded as 0. ","55b85683":"## Gradient Boost Trees\n\n**Pros: since it builds a tree at a time, errors correct over time; perform better than Random Forest\n\n**Cons: prone ot overfitting; parameters are harder to 'tune'; training takes much longer"}}