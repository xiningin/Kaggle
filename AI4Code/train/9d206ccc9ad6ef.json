{"cell_type":{"a51c9ebb":"code","35b03c16":"code","e163be66":"code","87709583":"code","29b08a72":"code","37183582":"code","8654d1bc":"code","8743759b":"code","f3ae888f":"code","7bbf28bd":"code","6409092c":"code","67b6d7f5":"code","48d446af":"code","a364e78e":"code","13207602":"code","ff5f2d92":"code","d4d52015":"code","4c48ef21":"code","575a9060":"code","d502bbff":"code","6b0d4563":"code","4408eff2":"code","484a6535":"code","ec1ae0b4":"code","94fb5f16":"code","ba287ae0":"code","ff0645be":"code","55a5b4a9":"code","0f9817d3":"code","aef212cb":"code","99d058f7":"code","668618e8":"code","46450f92":"code","50c2ad73":"code","22d87d33":"code","bff1d783":"code","264e4216":"code","92c31082":"code","c1b2c311":"code","2881a5e8":"code","f56b666f":"code","174887bf":"code","c4ccc402":"code","fe6e3f23":"code","1bb7b3c1":"code","a6a347e4":"code","8e37aa0e":"code","70a7f2fb":"code","fd9f53fd":"code","840f52cb":"code","c1d0016f":"code","439b0d65":"code","f7c05aa0":"code","adf12998":"code","4e7f25e5":"code","27826ee5":"markdown","18d0bdb5":"markdown","5168758b":"markdown","a6e40f50":"markdown","46047ca0":"markdown","b340ba81":"markdown","e018e220":"markdown","86a3df5a":"markdown","280213f6":"markdown","0659d35c":"markdown","76c5ed82":"markdown","846703fb":"markdown","0512dfda":"markdown","a5f1a7cc":"markdown","e4b690d5":"markdown","88c64f50":"markdown","aaee481c":"markdown","1f82c367":"markdown","d4ac5da2":"markdown","63b57313":"markdown","0e7e950a":"markdown","1b1cb6c1":"markdown"},"source":{"a51c9ebb":"import pandas as pd\n# This will control the display options of pandas within this notebook only\npd.options.display.max_columns = None\npd.options.display.max_rows = 20","35b03c16":"# Our master table containing main information\ndf = pd.read_csv(\"..\/input\/ai-trainee\/churn_data.csv\")\nprint(\"Input data-frame shape: {}\".format(df.shape))\ndf.head()","e163be66":"# The cutomer-specific data\ncustomer = pd.read_csv(\"..\/input\/ai-trainee\/customer_data.csv\")\ncustomer.head()","87709583":"# Contract data\ncontract = pd.read_csv(\"..\/input\/ai-trainee\/internet_data.csv\")\ncontract.head()","29b08a72":"# Read metadata. \n# If you get an error, just inspect the file manually and remove empty spaces or tabs after each column (' ,'>'')\n# This actually happens very often that the provided data set contains some strange symbols leading to errors. Get custom to it :)\nmeta = pd.read_csv(\"..\/input\/ai-trainee\/Telecom Churn Data Dictionary.csv\")\n# the columns in input data and meta do not match (some are in lowercase, some contain empty spaces, ...)\n# Will create a new column that will unify all. Same will be done in following section for input dataframes\nmeta = meta.assign(name_id=meta[\"Variable Name\"].replace({\" \":\"\",\"\\t\":\"\"},regex=True).str.lower()).set_index(\"name_id\")\nmeta.head()","37183582":"# Set customer ID (=unique ID) as index for further joining\n# This is clearly a repeating task. Imagine doing this ten times or so! => use a for loop\n# In addition, rename the columns to be identical with \"meta\"\nfor i in [df, customer, contract]:\n    i.set_index(\"customerID\",inplace=True)\n    i.rename(columns={j:j.lower() for j in i.columns},inplace=True)\n    \ndf.head()","8654d1bc":"# Join all three data-frames (one after another)\ndf = df.join(customer).join(contract)\n\n# Make sure no 1:N relation = no duplicates, print shape again (compare number of rows with input above)\nprint(\"Joined dataframe shape: {}\".format(df.shape))\ndf.head()","8743759b":"from sklearn.model_selection import train_test_split","f3ae888f":"# to achive the identical result each run (just for this AI trainee lesson), use 'random_state' option\ntrain, test = train_test_split(df,train_size=0.75,shuffle=True,random_state=123)","7bbf28bd":"print(train.shape)\ntrain.head()","6409092c":"def check_stats(df):\n    \"\"\"\n    This function will return a table (dataframe) showing main statistics and additional indicators\n    \"\"\"\n    # We will store the data types in a separate dataframe\n    dfinfo = pd.DataFrame(df.dtypes,columns=[\"dtypes\"])\n    \n    # We are interested if we have any missing data (sum all). \n    # Again, join the result with the dfinfo (append new column). Consider '' or ' ' also as missing\n    dfinfo = dfinfo.join((df.replace({'':None,' ':None}) if \"('O')\" in str(df.dtypes.values) else df).isna().sum().rename(\"isna\"))\n        \n\n    # In the last step, add statistics (will be computed for numerical columns only)\n    # We need to \"T\"ranspose the dataframe to same shape as df.describe() output\n    return dfinfo.T.append(df.describe(),sort=False)","67b6d7f5":"check_stats(train).T.query(\"isna != 0\")","48d446af":"# The missing data are in the input file marked with ' '\n# drop them = overwrite the dataframe\ntest = test[test.totalcharges!=' ']\ntrain = train[train.totalcharges!=' ']","a364e78e":"# for visualization purposes, we will store the results in pandas dataframe and print the result in the next jupyter notebook cell\n# nr of unique: will count the number of unique entries\n# first 5 unique: will show first 5 unique entries (if less, only those)\ndfsummary = pd.DataFrame({\"nr of unique\":[],\"first 5 unique\":[]})\n\n# Run loop over all columns computing length (len) of unique entries in each column and converting first 5 enties to a string\nfor i in train.columns:\n    dfsummary = pd.concat([dfsummary,pd.DataFrame({\"nr of unique\":[len(train[i].unique())],\n                                                   \"first 5 unique\":[str(train[i].unique()[0:5])]},index=[i])],sort=False)","13207602":"# join the result with metadata-column description\n# Will join on column name. However, the provided metadata column names are not identical (e.g, contain empty spaces)\nmeta = meta.assign(join_name=meta[\"Variable Name\"].replace({\" \":\"\",\"\\t\":\"\"},regex=True).str.lower()).set_index(\"join_name\")\n\n# need to do tha same for dfsummary\ndfsummary = dfsummary.assign(join_name=dfsummary.index.astype(str).str.lower()).set_index(\"join_name\")\n\n# now having identical indices, join tables\ndfsummary = dfsummary.join(meta[[\"Meaning\"]])\n# set column with only for the next step (to see the Meaning description)\npd.options.display.max_colwidth = 100\ndfsummary","ff5f2d92":"# set back to normal\npd.options.display.max_colwidth = 50","d4d52015":"train = train.replace({'No phone service':'No','No internet service':'No'})\ntest = test.replace({'No phone service':'No','No internet service':'No'})\ndf = df.replace({'No phone service':'No','No internet service':'No'})","4c48ef21":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder","575a9060":"\n# Tenure, seniorcitizen, and monthlycharges do not need to be converted\n# We will convert \ntrain_e = train[[\"tenure\",\"seniorcitizen\",\"monthlycharges\"]].copy()\ntest_e = test[[\"tenure\",\"seniorcitizen\",\"monthlycharges\"]].copy()\n\n# Convert to float (str because of missing data that was, however, dropped before)\ntrain_e[\"totalcharges\"] = train.totalcharges.astype(\"float64\")\ntest_e[\"totalcharges\"] = test.totalcharges.astype(\"float64\")","d502bbff":"# We want to ensure that 'No' is 0 and 'Yes' is 1. \n# To do that \"fit\" the encoder first, and apply (=transform) afterwards\n# \"fit\" means that the object\/variable \"le_no_yes\" will \"remember\" that no=0, and yes=1\nle_no_yes = LabelEncoder().fit(['No','Yes'])\n\n# now, apply to all columns where 'Yes', 'No' (or inverse order) occurs\nfor i in dfsummary.index:\n    if \"yes\" in dfsummary.loc[i,\"first 5 unique\"].lower() and \"no\" in dfsummary.loc[i,\"first 5 unique\"].lower():\n        print(i)\n        train_e[i] = le_no_yes.transform(train[i])\n        test_e[i] = le_no_yes.transform(test[i])","6b0d4563":"# to ensure the values are \"ordered\" = month-to-month = 0, year=1, 2 years = 2, fit first\nle_contract = LabelEncoder().fit(['Month-to-month','One year','Two year'])\ntrain_e[\"contract\"] = le_contract.transform(train[\"contract\"])\ntest_e[\"contract\"] = le_contract.transform(test[\"contract\"])","4408eff2":"# Here is a for loop that will convert all remaining columns applying OneHotEncoder\n# we will \"declare\" the encoder just to use its 'categories_' attribute \nohe = OneHotEncoder() \nfor i in df.columns:\n    if i not in train_e.columns:\n        print(i)\n        # fit = get new mapping for each column\n        ohe = OneHotEncoder().fit(train[i].unique().reshape(-1,1))\n        # OneHotEncoder (just like ML models) expects\/require a numpy matrix\/array as input\n        temp = pd.DataFrame(ohe.transform(df[i].to_numpy().reshape(-1,1)).toarray(),\n                            index=df.index,\n                            columns=[i+\"_\"+cat.lower().replace(\" \",\"_\") for cat in ohe.categories_[0]])\n        # Check also category-encoders library for easier encoding\n        train_e = train_e.join(temp)\n        test_e = test_e.join(temp)","484a6535":"train_e.head()","ec1ae0b4":"check_stats(train_e)","94fb5f16":"train_e.corr().round(3).style.background_gradient(cmap=\"viridis\")","ba287ae0":"# see https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\nfrom sklearn.preprocessing import StandardScaler","ff0645be":"# It makes, of course, only sense to apply scaling only to numerical values\n# Again, \"fit\" on train and apply\/\"transform\" test\nscaler = StandardScaler()\nfor i in [[\"tenure\",\"monthlycharges\",\"totalcharges\"]]:\n    train_e[i] = scaler.fit_transform(train_e[i])\n    test_e[i] = scaler.transform(test_e[i])","55a5b4a9":"check_stats(train_e)","0f9817d3":"import plotly as py\nimport plotly.graph_objects as go\npy.offline.init_notebook_mode(connected=True)","aef212cb":"fig = go.Figure(data=[go.Bar(\n                            x=train_e.churn.value_counts().index, \n                            y=train_e.churn.value_counts(),\n                            text=train_e.churn.value_counts().index,\n                            name=\"train\"\n                            ),\n                      go.Bar(\n                            x=test_e.churn.value_counts().index, \n                            y=test_e.churn.value_counts(),\n                            text=test_e.churn.value_counts().index,\n                            name=\"test\"\n                            ),\n                     ],\n                layout = go.Layout(\n                                   title=\"Checking target\/churn imbalance\",\n                                   # Reduce default (rather big) margins between Figure edges and axes\n                                   margin=go.layout.Margin(l=50,r=50,b=50,t=50),\n                                   # Set figure size\n                                   width=600,\n                                   height=400,\n                                   xaxis=go.layout.XAxis(\n                                                        showgrid=False,\n                                                        zeroline=False,\n                                                        showticklabels=False\n                                                        )\n                                   )\n               )\n\nfig.show()","99d058f7":"# select features & labels\nX_train, X_test = train_e.drop(columns=[\"churn\"]).to_numpy(), test_e.drop(columns=[\"churn\"]).to_numpy()\ny_train, y_test = train_e[\"churn\"].to_numpy(), test_e[\"churn\"].to_numpy()","668618e8":"# Scikit-learn models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.dummy import DummyClassifier\n\n# Classification metrics\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# cross-validation\nfrom sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV","46450f92":"dt = DecisionTreeClassifier()\nrf = RandomForestClassifier(n_estimators=50)\ndu = DummyClassifier(strategy=\"stratified\")","50c2ad73":"for clf in [du,dt,rf]:\n    clf.fit(X_train,y_train)\n    print(f\"\\n{clf}\")\n    print(classification_report(y_test,clf.predict(X_test)))","22d87d33":"# ## Visualize the tree: https:\/\/www.kaggle.com\/willkoehrsen\/visualize-a-decision-tree-w-python-scikit-learn\n\n# from sklearn.tree import export_graphviz\n# # Export as dot file\n# export_graphviz(dt, out_file='tree.dot', \n#                 feature_names = list(train_e.drop(columns=[\"churn\"]).columns),\n#                 class_names = ['no','yes'],\n#                 rounded = True, proportion = False, \n#                 precision = 2, filled = True)\n\n# # Convert to png > works on Linux\n# from subprocess import call\n# call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# # Display in python\n# import matplotlib.pyplot as plt\n# plt.figure(figsize = (14, 18))\n# plt.imshow(plt.imread('tree5.png'))\n# plt.axis('off');\n# plt.show();\n","bff1d783":"tuning_parameter = {'min_samples_leaf': [1, 3, 6],\n                    'min_samples_split': [2, 10, 15],\n                    'n_estimators': [100,350,600]}","264e4216":"cv = KFold(n_splits=5)\nrf = GridSearchCV(RandomForestClassifier(),\n                  param_grid=tuning_parameter, \n                  scoring=\"f1_weighted\",\n                  cv=cv,\n                  n_jobs = 10,\n                  ) # verbose=10","92c31082":"rf.fit(X_train,y_train)","c1b2c311":"rf.best_estimator_","2881a5e8":"print(classification_report(y_test,rf.best_estimator_.predict(X_test)))","f56b666f":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA","174887bf":"pca = PCA(n_components=0.999)# or set n_components=\"mle\"\n# As always, fit on train, transform test\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\nprint(\"Nr. of features after PCA = {} (input = {})\".format(X_train_pca.shape[1],X_train.shape[1]))","c4ccc402":"tuning_parameter = {\"n_neighbors\":list(range(1,30,2)),\n                    \"weights\":[\"uniform\",\"distance\"]}\nknn = GridSearchCV(KNeighborsClassifier(), \n                   tuning_parameter, \n                   cv=cv,\n                   scoring=\"f1_weighted\",\n                   n_jobs = 10)","fe6e3f23":"knn.fit(X_train_pca,y_train)","1bb7b3c1":"knn.best_estimator_","a6a347e4":"print(classification_report(y_test,knn.best_estimator_.predict(X_test_pca)))","8e37aa0e":"from sklearn.svm import SVC\nimport numpy as np","70a7f2fb":"tuning_parameter = {\"C\":np.logspace(-3, 3, 7),\n                    \"gamma\":np.logspace(-3, 3, 7)}\nsvc = GridSearchCV(SVC(kernel=\"rbf\"), \n                      tuning_parameter, \n                      cv=cv,\n                      scoring=\"f1_weighted\",\n                      n_jobs = 10,\n                      return_train_score=True)","fd9f53fd":"svc.fit(X_train,y_train)","840f52cb":"svc.best_estimator_","c1d0016f":"print(classification_report(y_test,svc.best_estimator_.predict(X_test)))","439b0d65":"# To show how the score varies on parameters, plot the results (in test set!)\n#pd.DataFrame(search.cv_results_)","f7c05aa0":"# ##You can try to train SVM with the reduced data set: the results should be similar\n# svc.fit(X_train_pca,y_train)\n# print(classification_report(y_test,svc.best_estimator_.predict(X_test_pca)))","adf12998":"# ##Try re-sampling to suppres the moderate imbalance\n# from imblearn.over_sampling import RandomOverSampler\n# ros = RandomOverSampler(random_state=0)\n# X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n# tuning_parameter = {'min_samples_leaf': [1, 3, 6],\n#                     'min_samples_split': [2, 10, 15],\n#                     'n_estimators': [100,350,600]}\n# rf = GridSearchCV(RandomForestClassifier(),\n#                   param_grid=tuning_parameter, \n#                   scoring=\"f1_weighted\",\n#                   cv=cv,\n#                   n_jobs = 10,\n#                   )\n# rf.fit(X_resampled, y_resampled)\n# print(classification_report(y_test,rf.best_estimator_.predict(X_test)))","4e7f25e5":"# ## Check if the result changes if using more k-folds\n# search = GridSearchCV(RandomForestClassifier(),\n#                       param_grid={'min_samples_leaf': [1, 3, 6],\n#                                   'min_samples_split': [2, 10, 15],\n#                                   'n_estimators': [100,350,600]}, \n#                       scoring=\"f1_weighted\",\n#                       cv=KFold(n_splits=10),\n#                       n_jobs = 10,\n#                       )\n# rf.fit(X_train,y_train)\n# print(classification_report(y_test,rf.best_estimator_.predict(X_test)))","27826ee5":"# Support Vector Machines (SVMs)\n\nThis Jupyter notebook gives a very brief Introduction to Support Vector Machine (SVM). Please refer to [SVM notebook](https:\/\/www.kaggle.com\/mmdatainfo\/support-vector-machines) for more details. \nSMVs are **supervised learning models** for **classification** and **regression**. The algorithm finds support vectors across which to divide the data into two categories. These support vectors define hyperplanes. Thus, SVM is parametric, non-probabilistic binary classifier. SVM can be also used in classification with non-linear boundaries using \"kernel trick\".   \n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/7\/72\/SVM_margin.png\/1920px-SVM_margin.png\" alt=\"drawing\" width=\"300\"\/> \n\n### How it works\n\nThe aim of SVM is to find a line (hyperplane) that separates two sets of labels into two classes\n* The question is how to set the line: there are many hyperplanes that might classify the data\n* One reasonable choice as the best hyperplane is the one that **represents the largest separation**, or margin, between the two classes. \n    * The bigger margin means higher probability that any new point will correctly classified (more space with correct classification)\n    * Maximum margin also constrains the solution which leads to reduced [VC dimensions](https:\/\/en.wikipedia.org\/wiki\/VC_dimension) (compared to arbitrary lines that would separate the two labels)\n    * The algorithm **chooses the hyperplane so that the distance from it to the nearest data point on each side is maximized**\n* Non-linear boundary: works via non-linear transformation: convert features from one  space to another feature space \n    * The advantage is that the this only affects the features while the actual function to be minimized stays the same\n    * We work in new space, this means that also the **support vectors are in the new space**\n        * So even if the image of the support vector in the original space may look very dramatic (polynomial of 10 degree for example), it is **linear in the new space**\n    * More importantly, **only the number of found support vectors in the new space affects the generalizability**\n        * **Compared to simple regression analysis**: take degree 10 polynomial function for example: that would significantly increase number of estimated parameters (complexity) leading to poor generalization\n    * The transformation is achieved by so-called \"kernel trick\" (far beyond the scope of this lecture)\n\n### Pros\n* Very sufficient\n* Works well with **higher-dimensional data**\n* Intuitive interpretation\n* Can use different kernels (linear, polynomial,...): can **solve non-linear problems**\n* The **absence of local minima**\n\n### Cons\n* Highly **depends on choice of the kernel** and regularization parameter\n* Does not directly provide probability estimates\n* The optimal design for multiclass SVM classifiers is a further area for research (SVM is binary by design)\n* **Scaling** and **regularization** are recommended\n* Parameters of a solved model are difficult to interpret\n","18d0bdb5":"# ML: finally!<a name=\"ml\"\/>\nNow we can finally look at some ML in action! Here, we will show Decision Trees\/Random Forest, k-NN, and SVM models. Check the [sklearn model comparison](https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html) to see how various models perform in different situations (with different decision boundaries)\nFor metrics, see the prepared [notebook](https:\/\/www.kaggle.com\/mmdatainfo\/performance-metrics) and\/or [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall)  \n\n# Decision Tree <a name=\"decision-tree\"\/>\nMuch more detailed description can be found [here](https:\/\/www.kaggle.com\/mmdatainfo\/decision-tree)\n\n### How it works\nAt each step, find the attribute we can use to partition the data set to minimize the [Entropy](https:\/\/en.wikipedia.org\/wiki\/Entropy), or [Gini impurity](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning#Gini_impurity) (not coefficient) of the data a in the next step\n* just picking decision that reduced entropy the most at the current stage (=greedy algorithm)\n    * will not produce optimal tree, just one that will (somehow) work\n    \n#### Entropy<a name=\"entropy\"\/>\n* is a measure of disorder in the data set. A quantity which will measure, in some sense, how much information is \"produced\" by a process (Markoff), or at what rate information is produced\n* Entropy is highest when every outcome is equally likely\n* Every time you remove equally probable outcome you introduce predictability and the entropy will decrease\n* **If the entropy decreases, we can guess the outcome with higher likelihood**\n* Information gain is a measure of the decrease in disorder achieved by partitioning the original data set\n* Information gain\/entropy and **Gini impuritiy** are [almost identical](https:\/\/datascience.stackexchange.com\/questions\/10228\/gini-impurity-vs-entropy)\n    * In decision learning, it only matters in 2% of the cases whether you use gini impurity or entropy.\n    * Entropy (needed for information gain) might be a little slower to compute (because it makes use of the logarithm).\n        \n### Pros<a name=\"pros\"\/> \n* Simple to understand and to interpret. Trees can be visualized\n* Requires little data preparation: **no data normalization** required \n    * however, balancing of classes is recommended (see disadvantages)\n* Able to **handle both numerical and categorical data**\n* Uses a **white box model**. If a given situation is observable in a model, the explanation for the condition is easily explained by Boolean logic \n    * By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret\n* Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model\n* Performs well even if its assumptions are somewhat violated by the true model from which the data were generated\n\n### Cons<a name=\"cons\"\/> \n* Decision-tree learners can create over-complex trees that do not generalize the data well = **overfitting**\n    * **Random decision forests** correct for decision trees habit of overfitting to their training set\n* Decision trees can be **unstable** because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble \n* There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems\n* Decision tree learners create **biased trees if some classes dominate**. \n    * It is therefore recommended to balance the dataset prior to fitting with the decision tree\n\n## Random forest\nThe training algorithm for [random forests](https:\/\/en.wikipedia.org\/wiki\/Random_forest) applies the general technique of bootstrap aggregating, or bagging, to tree learners\n* bagging repeatedly selects a random sample with replacement of the training set and fits trees\n* outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees\n    * Can compute probability of the estimate as we have X trees \"voting\" for one result","5168758b":"## Imbalance<a name=\"imbalance\"\/>\n[Class imbalance](https:\/\/en.wikipedia.org\/wiki\/Oversampling_and_undersampling_in_data_analysis) is a common phenomenon in classification problems. The aim of our ML algorithm is to correctly classify each class member. This includes minority classes. The class imbalance is not to be confused with [outliers in regression](https:\/\/www.kaggle.com\/mmdatainfo\/linear-regression) where we want to suppress their effect. \n\nThe [machine learning mastery](https:\/\/machinelearningmastery.com\/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset\/) summarizes different methods that could be deployed when dealing with class imbalance.  \nHere, we would just inspect our data set for imbalance.\n\nSimplest way to check for imbalance is to use [value_coutns](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.Series.value_counts.html) method in Pandas. Feel free to check the notebook showing how to plot data using [PyPlot](https:\/\/www.kaggle.com\/mmdatainfo\/pyplot-visualization) or interactive [Plotly](https:\/\/www.kaggle.com\/mmdatainfo\/plotly-visualization) libraries\n* Our target is \"churn\" with binary classes (not a [multiclass problem](https:\/\/en.wikipedia.org\/wiki\/Multiclass_classification))\n* As shown below, the imbalance is rather small-to-modes, i.e. around 1:3\n","a6e40f50":"Let\u2019s try to improve the random forest performance via cross-validation hyper-parameter tuning\n\n* Some of the **[important parameters](https:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/beginners-guide-random-forest-hyperparameter-tuning\/)**\n    * `max_depth`: maximum depth of the tree (none by default)\n    * `min_samples_leaf`: minimal number of samples required to be at a leaf node (after split). 1 by default\n        * See a tree visualization, for example, [here](https:\/\/medium.com\/greyatom\/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb)\n    * `min_samples_split`: minimum number of samples required to split an internal node\n        * See a discussion about the difference between split and leaf on [stackoverflow](https:\/\/stackoverflow.com\/questions\/46480457\/difference-between-min-samples-split-and-min-samples-leaf-in-sklearn-decisiontre)\n    * `max_features` how many features used for decision\/split. All by default\n    * `n_estimators`: the number of trees in the forest (10 by default)\n    \n> Keep in mind that tuning more parameters (with more options) will increase the computation time (and RAM demands)","46047ca0":"## Get data<a name=\"get-data\"\/>\nIt is highly unlikely that the customer will hand you over a script to load the data directly. Often, you will receive an access to database, link to data set or simply an archive containing the data.  \n\nIn this example, the customer will ask us to predict [churn rate](https:\/\/en.wikipedia.org\/wiki\/Churn_rate) providing selected data on customers, contracts, and yes\/no churn-left flag for each of them. We will use the [Telecom churn data sets](https:\/\/www.kaggle.com\/vpfahad\/telecom-churn-data-sets) data set hosted on kaggle.com  \n\n1. Download the data zip archive: https:\/\/www.kaggle.com\/vpfahad\/telecom-churn-data-sets\n2. Extract to your data folder\n    * Tip: python contains library [zipfile](https:\/\/docs.python.org\/3.7\/library\/zipfile.html) that allows you to read from archive without extraction!\n    * If you are working on kaggle.com (although I highly recommend to work on your PC):\n        * From dashboard, click on Data > New Dataset > Upload (include duplicates)\n        * Start new notebook (or clone this) and click on + Add data & select your folder (you may need to leave or restart the notebook if some error occurs). Then you will see your data set in \"input\" from the Data drop-down menu. Congrats!","b340ba81":"## Scaling<a name=\"scaling\"\/>\n\nThe aim of scaling is to reduce the range of available numbers to a certain minimum-maximum. One of the most commonly used scaling techniques is [normalization](https:\/\/en.wikipedia.org\/wiki\/Normalization_(statistics), more specifically transforming the data set to unit norm (standard deviation of 1) and zero mean value. This process can be useful especially for ML applying transfer functions (activation functions in neural networks).  \n\nWithout scaling, these transfer functions do no serve their purpose. Imagine [Logistic function](https:\/\/en.wikipedia.org\/wiki\/Logistic_function) converting input into 0 and 1 range output. Without scaling (setting the mean to 0, and 99.7% of range to +\/- 3), all values could be converted to min\/max, i.e., 0 or 1 even if the input vector would show \"normal\" variance but mean would be let's say -100 or +100 respectively. Scaling also improves numerical stability. Without scaling, the program could exceed the float precision (imagine multiplying ```1^10``` with ```1^13``` multipe times)\n\nFor more information, see [scikit-learn preprocessing](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#preprocessing) materials  \n","e018e220":"#### Check correlation\n* See if there is any high (anti)correlation for `churn`\n* See if there are some fully-correlated feature columns (if so, you can drop one-of them)\n\n> Correlation = linear relation!","86a3df5a":"## Inspect data types<a name=\"inspect-types\"\/>\n* What data **types** are the columns?\n    * Keep in mind that \"Object\" can store string, date-time, or even Integer if with some None\/missing entries\n* Do we have any missing data?\n* What are the basic statistics for numerical values?  \n\n\n> We will use this function at least two times >> prepare function. Try using function whenever you know that the task will be repeated!!","280213f6":"## Read<a name=\"read\"\/>\n* We will read the [churn rate](https:\/\/en.wikipedia.org\/wiki\/Churn_rate) data to pandas\/python\n    * **Read** the churn data, customer data, internet contract data, and metadata\n        * All data stored in [csv](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html)\n* If working on kaggle.com the data will be located in `..\/input\/` folder","0659d35c":"## Train-test split<a name=\"train-test-split\"\/>\nThe aim of the [train-test-split](https:\/\/towardsdatascience.com\/3-things-you-need-to-know-before-you-train-test-split-869dfabb7e50) is to assess (estimate) the future performance of the model on \"unseen\" data. It can be simply achieved by putting aside part of the available data (usually 20-30% depending on the data set size) and train the data on the remaining part. It is very important to use the test set as we would not have it. Data Scientists often unconscientiously make their decision after looking at the complete data set. This is called [data snooping](https:\/\/en.wikipedia.org\/wiki\/Data_dredging) and should be avoided. Check the awesome [Caltech ML course CS 156](https:\/\/www.youtube.com\/watch?v=SEYAnnLazMU&list=PLD63A284B7615313A&index=6&t=0s) for very detailed explanation.\n\nHere, we will apply the split before encoding and scaling! Otherwise, we would scale the data using mean of the whole data set! In real-world applications, we will not know what the mean, standard deviation, or range of all features is! On similar note, imagine you would want to encode text data set. Fitting the encoder on the whole is a bit of cheating as we do not know in advance all possible expression that will pop-up in the future data. \nHence, applying train-test-split prior any processing steps, the test accuracy will more likely follow the out-of-sample accuracy. \n\nIn addition, the [train-test-split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html) will automatically re-shuffle the data so we avoid the situation that we will train the model on \"sorted\" data set (for example, if the customer provide the data sorted by tenure)\n\nAfter the train-test-split, we would use \"[pipes](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html)\" to apply the same steps on train and test data set. Here, we will simply apply all steps manually on both data sets.\n","76c5ed82":"# k-Nearest Neighbors (kNN)\n\nIs a non-probabilistic, non-parametric and instance-based learning algorithm:\n* **Non-parametric** means it makes no explicit assumptions about the function form of *h*, avoiding the dangers of mis-modelling the underlying distribution of the data\n    * For example, suppose our data is highly non-Gaussian but the learning model was chosen assumes a Gaussian form. In that case, a parametric algorithm would make extremely poor predictions\n* **Instance-based** learning means that the algorithm does not explicitly learn a model\n    * Instead, it chooses to memorize the training instances which are subsequently used as \"knowledge\" for the prediction phase\n    * Concretely, this means that only when a query to our database is made (i.e., when we ask it to predict a label given an input), will the algorithm use the training instances to predict the result\n        \nSee separate [k-NN notebook](https:\/\/www.kaggle.com\/mmdatainfo\/k-nearest-neighbors) for more details.  \n\n### How it works\nAn object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k-nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor\n\n[Wikpedia example k-NN classification](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm): The test sample (green dot) should be classified either to blue squares or to red triangles. If k = 3 (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/e\/e7\/KnnClassification.svg\/1280px-KnnClassification.svg.png\" alt=\"drawing\" width=\"300\"\/> \n\n### Pros<a name=\"pros\"\/> \n* **simple** to understand and implement\n* kNN **works just as easily with multi-class data** sets whereas other algorithms are hard-coded for the binary setting\n* the non-parametric nature of kNN gives it an edge in certain settings where the data may be highly unusual, thus **without prior knowledge on distribution**\n\n### Cons<a name=\"cons\"\/> \n* **computationally expensive** testing phase\n    * we **need to store the whole data set for each decision**!\n* can **suffer from skewed class distributions**\n    * for example, if a certain class is very frequent in the training set, it will tend to dominate the majority voting of the new example (large number = more common)\n* the accuracy can be severally **degraded with high-dimension data** because of the little difference between the nearest and farthest neighbor\n    * **the curse of dimensionality** refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience\n    * for high-dimensional data (e.g., with number of dimensions more than 10) **scaling** and **dimension reductions** (such as PCA) is usually performed prior applying kNN\n","846703fb":"# Feature engineering<a name=\"feature-engineering\"\/>","0512dfda":"### Missing data\n* Check if any missing data (see the created \"isna\" columns)\n* If found some, first check if the missing data is not caused by incorrect data reading (not in this case)\n* Due to the low number of affected entries (0.2%) we can neglect, drop these rows\n    * Revise this decision if facing imbalanced data set\n    * If you decide not to drop the rows, follow the notebook describing different methods when dealing with missing data: https:\/\/www.kaggle.com\/mmdatainfo\/missing-data","a5f1a7cc":"# Data preparation<a name=\"data-preparation\"\/>","e4b690d5":"# AI Trainee - Level 3 Practical\n\nThe aim of this notebook is to show selected Machine Learning (ML) algorithms in action and highlight their advantages and drawbacks. Furthermore, the aim is to highlight the important points not the coding style. **Do not worry** if you are not able to fully understand some code snippets as long as you understand the idea behind it.   ","88c64f50":"## Encoding<a name=\"encoding\"\/>\n\nThe aim of encoding is to convert available data types to numerical values suitable for machine learning. These non-numerical values are often called categorical values as they describe category such as 'internet contract' or 'cable contract'\n* For example, column containing 'no' & 'yes' values needs to be converted to numeric representation\n* There are also methods to process numerical values, e.g., [discretization](https:\/\/en.wikipedia.org\/wiki\/Discretization_of_continuous_features) but these are applied only in rare cases\n\nThere are numerous ways\/methods to encode the data as shown in the [category-encoders](https:\/\/pypi.org\/project\/category-encoders\/) library. Here is the list of most common ones:\n\n### Label encoding \n\nLabel encoding will convert the categorical value into a vector of integers. The aforementioned 'no' & 'yes' would simply be converted to `[0,1]` list. Accordingly, `['no','maybe','yes']` will be converted to `[0,1,2]`. \n\n* **Advantages**\n    * Simple & memory-efficient (integer\/binary vector requires only minimal RAM)\n        * Will not create a new features (=columns)\n    * Ideal for [ordinal](https:\/\/en.wikipedia.org\/wiki\/Ordinal_number) categorical values\n\n* **Disadvantages**\n    * Not suitable for many [ML scikit-learn estimators](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#encoding-categorical-features), as these expect continuous input, and would interpret the categories as being ordered, which is often not desired\n        * Take for example linear\/logistic regression with input `['red','blue','green']` converted to `[0,1,2]`. Higher number (`green`) would imply higher\/lower effect (assuming positive\/negative slope respectively)\n\n* **Examples**\n    * Experience of a candidate when applying to a job: need to sort `['junior','senior','lead']` would be `[0,1,2]`\n    * Product rating: `['ok-ish','good','very-good','perfect']` would be `[0,1,2,3]`\n        * Keep in mind that the input list is sorted\/ordered!\n        \n### One-Hot Encoding\n\n[One-Hot Encoding](https:\/\/machinelearningmastery.com\/why-one-hot-encode-data-in-machine-learning\/) converts the categorical values into a vector of one-and-zeros. One is used at indices where a particular value exists, and zeros are used for all other columns. \n\n* **Examples**\n   * Colors: If we have a features with three possible colors `['red','green','blue']`, than rows with `green` will be converted to `[0,1,0]`. Similarly, rows with blue will be converted to `[0,0,1]` and consequently, `red` will be encoded as `[1,0,0]`\n       * Is not possible to have more than one color, hence, we will have only one `1` entry per each row\n   * Country codes: imagine we have customers from `US`, `DE`, `SK`, and `PL`. The one-hot encoding will create 4 columns. Rows where the customer is from `US` will look like `[1,0,0,0]`, rows with Germany\/`DE` like `[0,1,0,0]`, and so on\n       * One could think of a different method to encode such column. For example, order the countries according to some criteria (e.g., distance) and use ordinal encoding.\n       \n* **Advantages**\n    * Introduces objective information independent of ordinal or non-ordinal nature of the feature\/input vector\n\n* **Disadvantages**\n    * Suitable only for categorical values with relatively low number of unique entries\n        * Created many new features\/columns which increases RAM demand\n    * May introduce [collinearity](https:\/\/en.wikipedia.org\/wiki\/Multicollinearity) which is not desired in some models (see for example [linear regression](https:\/\/www.kaggle.com\/mmdatainfo\/linear-regression))\n    ","aaee481c":"Looking at the results above we can do following\n* columns with `['No' 'Yes']` can be encoded using [scikit-learn LabelEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html)\n* `['Yes' 'No' 'No internet service']` can be first converted to `['No' 'Yes']` and encoded using [scikit-learn LabelEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html). Same applies to the `['Yes' 'No' 'No phone service']`\n    * We have the information about internet service in `internetservice` and `phoneservice` columns\n* column `totalcharges` should be converted to number\n    * pandas csv method did not convert it because of the ' ' missing values\n* `contract` can be interpreted as ordered list: monthly, yearly, 2-yearly. Hence, apply [LabelEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html)\n* The remaining columns will be encoded applying [scikit-learn OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html)\n* Will create a new data frame with encoded values `train_e` and `test_e` columns\n","1f82c367":"## Data Quest:\n1. try SVM or\/and random forest with reduced data set (`X_train_pca`) to see if the result differs from `X_train`\n2. try running random forest using [over-sampled](https:\/\/imbalanced-learn.org\/stable\/over_sampling.html) data to further suppress the moderate class imbalance (solution in commented code below)\n3. try to change the [scoring metric](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter) and number of [KFold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html)s in [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) just the see how the results change\n4. try to change the hyper-parameter range (`tuning_parameter`)\n    * Keep in mind: more = longer run time","d4ac5da2":"Before applying the encoding method, have a look at the unique values available in each of the columns\n* Number of unique entries affects directly the shape of output\/encoded matrix, see one-hot-encoding\/disadvantages section","63b57313":"## Join dataframes<a name=\"join\"\/>\n* The aim of data [join](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.join.html) is to get all required columns into one dataframe. We have 3 files\/dataframe and need to combine them into one\n* If you look closer at the dataframes, we have a unique `customerID` column that is suitable for joining\n* Here, we will do a [left join](https:\/\/www.w3schools.com\/sql\/sql_join_left.asp). This is easy as we have [1:1 ralation](https:\/\/hackernoon.com\/mysql-tutorial-example-relation-foreign-key-database-funtion-join-table-query-one-namy-nest-41dd09648fbd) (no duplicates = one customer one contract)","0e7e950a":"Let\u2019s compare the performance of one tree (`dt`), to a whole forest (`rf`). In addition, use a [dummy model](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.dummy) as benchmark (if your model should always outperform the dummy model)","1b1cb6c1":"Parameters for [SVC](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html) using [grid search](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html)\n* `C`: Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty (see [Regularization](https:\/\/www.kaggle.com\/mmdatainfo\/regularization-and-validation) and [RBF theory](https:\/\/www.kaggle.com\/mmdatainfo\/support-vector-machines))\n* `gamma`: Kernel coefficient (see [RBF theory](https:\/\/www.kaggle.com\/mmdatainfo\/support-vector-machines))"}}