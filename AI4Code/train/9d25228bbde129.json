{"cell_type":{"2a046a8a":"code","ebdc0e62":"code","8ce70241":"code","48ccbac6":"code","47502cc5":"code","6b6b631f":"code","d0955ef0":"code","48d07afa":"code","e42ff127":"code","f19df847":"code","d1705f6b":"code","f1298260":"code","674a531e":"code","1aed51bb":"code","33c2b2d0":"code","d6692734":"code","8a01b1f2":"code","e6e31463":"code","64e60735":"code","fd66a995":"code","3b904876":"code","6177330c":"code","6bd63542":"code","6db6e881":"code","2223dbce":"code","c058f2a2":"code","1dbfb7e9":"code","0426d6df":"code","d89774f2":"code","f45356f6":"code","20ea1769":"code","5dae8010":"code","5a32c4d3":"code","5055ca78":"code","4dbbfd89":"code","82a4cddc":"code","ffcdcf9b":"code","61fbcd5c":"code","3852f169":"code","f485826f":"code","b30124a3":"code","31bd73db":"code","28e9cc6c":"code","07e2c867":"code","ee17f69f":"code","9661dffe":"code","d09010f3":"code","92aba348":"code","e280544e":"code","bc469c6a":"code","c1080c6d":"code","429a1c8f":"code","1ec97801":"code","25b12725":"code","d2d19e8b":"code","0de8f440":"code","651908db":"code","25a6fc00":"code","33eea04a":"code","1e44aaf0":"code","0b6040d0":"code","c426f8d8":"code","83ed19c8":"code","6edc77cc":"code","0c302827":"code","2d1f79bb":"code","3a826114":"code","0ee89c16":"code","c78340a2":"code","5c43c652":"code","c61d576a":"code","a74b74e2":"code","0f008a43":"code","9bfce227":"code","3166fbe2":"code","dd0bff70":"code","d4207bf4":"code","ed7205fd":"code","6c7c66ed":"code","a93d2121":"code","4cabc98a":"code","20eba599":"markdown","c10ed335":"markdown","7b33a712":"markdown","cc75e671":"markdown","ef805006":"markdown","6f243503":"markdown","4d83a94e":"markdown","0e2300e8":"markdown","d3d4807a":"markdown","0514dd0c":"markdown","64aae2ba":"markdown","d255b991":"markdown","80e7ddbc":"markdown","262c7f97":"markdown","46b622e6":"markdown","ad0eda60":"markdown","a6d7ba13":"markdown","b078512d":"markdown","e456f52f":"markdown","88fd8665":"markdown","df18ae62":"markdown","fa69c271":"markdown","b25bf57c":"markdown","e9f4f50e":"markdown","1bd495a6":"markdown","665c0aea":"markdown","8f7d256d":"markdown","30c3cdd1":"markdown","8d21ebc7":"markdown","07790219":"markdown","709abffd":"markdown","e8bacd9b":"markdown"},"source":{"2a046a8a":"import numpy as np \nimport pandas as pd\nimport json\nimport ast\nfrom collections import Counter, OrderedDict\nimport time\nimport datetime\nimport random\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nimport eli5\n\nimport os\nprint(os.listdir(\"..\/input\"))","ebdc0e62":"df_train = pd.read_csv('..\/input\/tmdb-box-office-prediction\/train.csv')\ndf_test = pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv')","8ce70241":"features=df_train.drop(['revenue'],axis=1).append(df_test).reset_index()","48ccbac6":"# from this kernel: https:\/\/www.kaggle.com\/gravix\/gradient-in-a-box\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']","47502cc5":"def text_to_dict(df, columns_to_parse):\n    for column in columns_to_parse:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df","6b6b631f":"f_clean = text_to_dict(features,dict_columns)","d0955ef0":"def fix_date(x):\n    \"\"\"\n    Fixes dates which are in 20xx\n    \"\"\"\n    year = x.split('\/')[2]\n    if int(year) <= 19:\n        return x[:-2] + '20' + year\n    else:\n        return x[:-2] + '19' + year","48d07afa":"f_clean.loc[f_clean['release_date'].isnull() == True, 'release_date'] = '01\/01\/98' ","e42ff127":"f_clean['release_date'] = f_clean['release_date'].apply(lambda x: fix_date(x))\nf_clean['release_date'] = pd.to_datetime(f_clean['release_date'])","f19df847":"f_clean['year']=pd.DatetimeIndex(f_clean['release_date']).year\nf_clean['month']=pd.DatetimeIndex(f_clean['release_date']).month\nf_clean['yr_mth']=f_clean['year']*100+f_clean['month']","d1705f6b":"min_date_months = f_clean[\"year\"].min()*12 + f_clean[\"month\"].min()\n\ndef change_time_to_num(year_month, min_date):\n    date_to_months = year_month.apply(lambda x: int(str(x)[:4]) * 12 + int(str(x)[-2:]))\n    return date_to_months.apply(lambda x: x - min_date)\n\nf_clean['timediff'] = change_time_to_num(f_clean['yr_mth'], min_date_months)","f1298260":"f_clean['original_title'][f_clean.duplicated('original_title')].shape","674a531e":"f_clean['edited_title'] = f_clean['original_title'].copy()\nf_clean['edited_title'][f_clean.duplicated('original_title')] = f_clean['edited_title'].map(str) + ' (' + f_clean['year'].map(str) + ')'","1aed51bb":"f_clean['edited_title'][f_clean.duplicated('edited_title')].shape","33c2b2d0":"movie_index={v: k for k, v in f_clean['edited_title'].to_dict().items()}\nindex_movie=f_clean['edited_title'].to_dict()","d6692734":"f_clean['list_keywords']=f_clean['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values","8a01b1f2":"len(set([i for j in f_clean['list_keywords'] for i in j])) # check number of unique keywords","e6e31463":"count_keywords=Counter([i for j in f_clean['list_keywords'] for i in j]).most_common()","64e60735":"count_keywords[0:10] # check most common keywords","fd66a995":"for i in range(1,5):\n    print(f'There are {len([t[0] for t in count_keywords if t[1] == i])} keywords that appear in {i} movies')\n\nprint(f'There are {len([t[0] for t in count_keywords if t[1] > 4])} keywords that appear in 5 or more movies')\nprint(f'In total, {len([t[0] for t in count_keywords if t[1] > 1])} keywords appear more than once')","3b904876":"keywords = [t[0] for t in count_keywords if t[1] > 1]","6177330c":"kcount=pd.concat([f_clean['edited_title'],\n                  f_clean['list_keywords'].apply(lambda x: [i for i in x if i in keywords]),\n                  f_clean['list_keywords'].apply(lambda x: len([i for i in x if i in keywords]))],\n                 axis=1)\nkcount.columns=['movie','keywords','kcount']","6bd63542":"kcount.sort_values(by='kcount',ascending=False)[0:10]","6db6e881":"kcount['movie'].loc[kcount['kcount']==0].count() # check how many movies have zero keywords","2223dbce":"kword_index = {kword: idx for idx, kword in enumerate(keywords)}\nindex_kword = {idx: kword for kword, idx in kword_index.items()}","c058f2a2":"pairs = []\n\nfor movie in movie_index.values():\n    pairs.extend((movie,kword_index[kword]) for kword in kcount['keywords'][kcount.index==movie].iloc[0]) ","1dbfb7e9":"random.seed(100)\n\npairs_set = set(pairs)\n\ndef generate_batch(pairs, n_positive = 50, negative_ratio = 1.0, classification = False):\n    \"\"\"Generate batches of samples for training\"\"\"\n    batch_size = n_positive * (1 + negative_ratio)\n    batch = np.zeros((batch_size, 3))\n    \n    # Adjust label based on task\n    if classification:\n        neg_label = 0\n    else:\n        neg_label = -1\n    \n    # This creates a generator\n    while True:\n        # randomly choose positive examples\n        for idx, (movie_id, kword_id) in enumerate(random.sample(pairs, n_positive)):\n            batch[idx, :] = (movie_id, kword_id, 1)\n\n        # Increment idx by 1\n        idx += 1\n        \n        # Add negative examples until reach batch size\n        while idx < batch_size:\n            \n            # random selection\n            random_movie = random.randrange(len(index_movie))\n            random_kword = random.randrange(len(index_kword))\n            \n            # Check to make sure this is not a positive example\n            if (random_movie, random_kword) not in pairs_set:\n                \n                # Add to batch and increment index\n                batch[idx, :] = (random_movie, random_kword, neg_label)\n                idx += 1\n                \n        # Make sure to shuffle order\n        np.random.shuffle(batch)\n        yield {'movie': batch[:, 0], 'kword': batch[:, 1]}, batch[:, 2]","0426d6df":"from keras.layers import Input, Embedding, Dot, Reshape, Dense\nfrom keras.models import Model","d89774f2":"\ndef embedding_model(embedding_size = 50, classification = False):\n      \n    # Layer 1: 1-dimensional inputs\n    movie = Input(name = 'movie', shape = [1])\n    kword = Input(name = 'kword', shape = [1])\n    \n    # Layer 2: Embedding the movie (shape will be (None, 1, 50))\n    movie_embedding = Embedding(name = 'movie_embedding',\n                               input_dim = len(movie_index),\n                               output_dim = embedding_size)(movie)\n    \n    # Layer 2: Embedding the keyword (shape will be (None, 1, 50))\n    kword_embedding = Embedding(name = 'kword_embedding',\n                               input_dim = len(kword_index),\n                               output_dim = embedding_size)(kword)\n    \n    # Layer 3: Merge the layers with a dot product along the second axis (shape will be (None, 1, 1))\n    merged = Dot(name = 'dot_product', normalize = True, axes = 2)([movie_embedding, kword_embedding])\n    \n    # Layer 4: Reshape to be a single number (shape will be (None, 1))\n    merged = Reshape(target_shape = [1])(merged)\n    \n    # If classifcation, add extra layer and loss function is binary cross entropy\n    if classification:\n        merged = Dense(1, activation = 'sigmoid')(merged) # layer 5: for classification\n        model = Model(inputs = [movie, kword], outputs = merged)\n        model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    # Otherwise loss function is mean squared error\n    else:\n        model = Model(inputs = [movie, kword], outputs = merged)\n        model.compile(optimizer = 'Adam', loss = 'mse')\n    \n    return model","f45356f6":"model = embedding_model()\nmodel.summary()","20ea1769":"n_positive = 1024\n\ngen = generate_batch(pairs, n_positive, negative_ratio = 2)\n\n# Train\nh = model.fit_generator(gen, epochs = 50, \n                        steps_per_epoch = len(pairs) \/\/ n_positive,\n                        verbose = 2)","5dae8010":"movie_layer = model.get_layer('movie_embedding')\nmovie_weights = movie_layer.get_weights()[0]\nmovie_weights.shape","5a32c4d3":"movie_weights = movie_weights \/ np.linalg.norm(movie_weights, axis = 1).reshape((-1, 1))","5055ca78":"def similar_movies(name, n=10):\n    \n    dists = np.dot(movie_weights, movie_weights[movie_index[name]])\n    sorted_dists = np.argsort(dists)\n    closest = sorted_dists[-n:]\n    max_width = max([len(index_movie[c]) for c in closest])\n    \n    for c in reversed(closest):\n        print(f'Movie: {index_movie[c]:{max_width + 2}} Similarity: {dists[c]:.{2}}')","4dbbfd89":"similar_movies('Avatar')","82a4cddc":"col_names = ['membed_'+str(i) for i in range(1,51)]\n\nmovie_embeds = pd.DataFrame(movie_weights,columns=col_names)","ffcdcf9b":"f_clean = pd.concat([f_clean,movie_embeds],axis=1)","61fbcd5c":"train = pd.concat([f_clean.iloc[0:df_train.shape[0]],df_train['revenue']],axis=1)\ntest = f_clean.iloc[df_train.shape[0]:]","3852f169":"train['revenue'].describe().T","f485826f":"train[['original_title','revenue']][train['revenue']<300].sort_values(by=['revenue'])","b30124a3":"train['revenue'][train['revenue']<300] = 300","31bd73db":"train['director'] = train['crew'].apply(lambda x: [i['name'] for i in x if i['job'] == 'Director']).apply(pd.Series).iloc[:,0]","28e9cc6c":"train['revenue'].groupby(train['director']).count().describe().T","07e2c867":"dir_rev= pd.concat([train['revenue'].groupby(train['director']).count(),\n           train['revenue'].groupby(train['director']).sum(),\n           train['revenue'].groupby(train['director']).mean(),\n           train['revenue'].groupby(train['director']).max(),\n           train['revenue'].groupby(train['director']).min()], axis=1).reset_index()\n\ndir_rev.columns = ['director','N_movies','Total_rev','Average_rev','Highest_rev','Lowest_rev']","ee17f69f":"dir_rev[dir_rev['N_movies']>1].sort_values(by=['Average_rev'],ascending=False)[0:10]","9661dffe":"dir_rev['Hi_lo_rev'] = (dir_rev['Highest_rev'] - dir_rev['Lowest_rev']) \/ dir_rev['Average_rev']","d09010f3":"f_clean['director'] = f_clean['crew'].apply(lambda x: [i['name'] for i in x if i['job'] == 'Director']).apply(pd.Series).iloc[:,0]","92aba348":"f_clean = f_clean.merge(dir_rev[['director','Average_rev','Hi_lo_rev']],how='left',on='director')\nf_clean = f_clean.rename(columns={'Average_rev': 'Dir_avg_rev', 'Hi_lo_rev': 'Dir_HL_rev'})","e280544e":"cast_list = train['cast'].apply(lambda x: [i['name'] for i in x])","bc469c6a":"len(set([i for j in cast_list for i in j])) # over 38,000 unique cast members in the training set","c1080c6d":"cast_revenue = []\n\nfor i,r in enumerate(train['revenue'].values):\n    cast_revenue.extend((act,r) for act in cast_list[cast_list.index==i].iloc[0])\n    \ncast_revenue = pd.DataFrame(list(cast_revenue), columns=['Name','Revenue'])\n\ncast_rev_sum = pd.concat([cast_revenue.groupby(['Name']).count(),\n                          cast_revenue.groupby(['Name']).sum(),\n                          cast_revenue.groupby(['Name']).mean(),\n                          cast_revenue.groupby(['Name']).max(),\n                          cast_revenue.groupby(['Name']).min()], axis=1)\n\ncast_rev_sum.columns = ['N_movies','Total_rev','Average_rev','Highest_rev','Lowest_rev']","429a1c8f":"cast_rev_sum.sort_values(by=['Highest_rev'],ascending=False)[0:10]","1ec97801":"cast_rev_sum['rev99p'] = (cast_rev_sum['Highest_rev'] >cast_revenue['Revenue'].quantile(0.99))*1\ncast_rev_sum['rev20p'] = (cast_rev_sum['Highest_rev'] <cast_revenue['Revenue'].quantile(0.20))*1","25b12725":"full_cast_list = f_clean['cast'].apply(lambda x: [i['name'] for i in x])","d2d19e8b":"id_cast = []\n\nfor i,r in enumerate(f_clean['id'].values):\n    id_cast.extend((r,act) for act in full_cast_list[full_cast_list.index==i].iloc[0])\n    \nid_cast = pd.DataFrame(list(id_cast), columns=['id','Name'])","0de8f440":"cast_rev_movie = id_cast.merge(cast_rev_sum,how='left',on='Name')","651908db":"cast_rev_summary = pd.concat([cast_rev_movie.groupby(['id']).sum()['rev99p'],\n                             cast_rev_movie.groupby(['id']).sum()['rev20p'],\n                             cast_rev_movie.groupby(['id']).min()['Highest_rev']],\n                             axis=1).reset_index()","25a6fc00":"f_clean = f_clean.merge(cast_rev_summary,how='left',on='id')\nf_clean = f_clean.rename(columns={'rev99p': 'N_cast_99p', 'rev20p': 'N_cast_20p','Highest_rev':'Cast_low_bound'})","33eea04a":"companies_list = train['production_companies'].apply(lambda x: [i['name'] for i in x])","1e44aaf0":"comp_revenue = []\n\nfor i,r in enumerate(train['revenue'].values):\n    comp_revenue.extend((comp,r) for comp in companies_list[companies_list.index==i].iloc[0])\n    \ncomp_revenue = pd.DataFrame(list(comp_revenue), columns=['Company','Revenue'])\n\ncomp_rev_sum = pd.concat([comp_revenue.groupby(['Company']).count(),\n                          comp_revenue.groupby(['Company']).sum(),\n                          comp_revenue.groupby(['Company']).mean(),\n                          comp_revenue.groupby(['Company']).max(),\n                          comp_revenue.groupby(['Company']).min()], axis=1)\n\ncomp_rev_sum.columns = ['N_movies','Total_rev','Average_rev','Highest_rev','Lowest_rev']","0b6040d0":"comp_rev_sum['rev75p'] = (comp_rev_sum['Highest_rev'] >comp_revenue['Revenue'].quantile(0.75))*1\ncomp_rev_sum['rev25p'] = (comp_rev_sum['Highest_rev'] <comp_revenue['Revenue'].quantile(0.25))*1","c426f8d8":"full_comp_list = f_clean['production_companies'].apply(lambda x: [i['name'] for i in x])","83ed19c8":"id_comp = []\n\nfor i,r in enumerate(f_clean['id'].values):\n    id_comp.extend((r,comp) for comp in full_comp_list[full_comp_list.index==i].iloc[0])\n    \nid_comp = pd.DataFrame(list(id_comp), columns=['id','Company'])","6edc77cc":"comp_rev_movie = id_comp.merge(comp_rev_sum,how='left',on='Company')","0c302827":"comp_rev_summary = pd.concat([comp_rev_movie.groupby(['id']).sum()['rev75p'],\n                             comp_rev_movie.groupby(['id']).sum()['rev25p'],\n                             comp_rev_movie.groupby(['id']).min()['Highest_rev']],\n                             axis=1).reset_index()","2d1f79bb":"f_clean = f_clean.merge(comp_rev_summary,how='left',on='id')\nf_clean = f_clean.rename(columns={'rev75p': 'N_comp_75p', 'rev25p': 'N_comp_25p','Highest_rev':'Comp_low_bound'})","3a826114":"med_budget = train['budget'].median()","0ee89c16":"f_clean['budget'] = f_clean['budget'].replace(0, med_budget)","c78340a2":"f_clean['runtime'] = f_clean['runtime'].fillna(train['runtime'].median())\nf_clean['Dir_avg_rev'] = f_clean['Dir_avg_rev'].fillna(f_clean['Dir_avg_rev'].median())\nf_clean['Dir_HL_rev'] = f_clean['Dir_HL_rev'].fillna(f_clean['Dir_HL_rev'].median())\nf_clean['N_cast_99p'] = f_clean['N_cast_99p'].fillna(0)\nf_clean['N_cast_20p'] = f_clean['N_cast_20p'].fillna(0)\nf_clean['Cast_low_bound'] = f_clean['Cast_low_bound'].fillna(f_clean['Cast_low_bound'].median())\nf_clean['N_comp_75p'] = f_clean['N_comp_75p'].fillna(1)\nf_clean['N_comp_25p'] = f_clean['N_comp_25p'].fillna(0)\nf_clean['Comp_low_bound'] = f_clean['Comp_low_bound'].fillna(f_clean['Comp_low_bound'].median())\n","5c43c652":"extra_train = pd.read_csv('..\/input\/moviestmdb-datapreparation\/train_prep.csv')\nextra_test = pd.read_csv('..\/input\/moviestmdb-datapreparation\/test_prep.csv')","c61d576a":"cols_add = ['has_collection','num_cast','num_crew','genres_name_Drama','genres_name_Comedy','genres_name_Thriller',\n            'genres_name_Action','genres_name_Romance','genres_name_Crime','genres_name_Adventure',\n            'genres_name_Horror','genres_name_Science Fiction','genres_name_Family',\n            'production_countries_name_United States of America',\n            'spoken_languages_name_English','spoken_languages_name_Fran\u00e7ais','spoken_languages_name_Espa\u00f1ol']","a74b74e2":"train = pd.concat([f_clean.iloc[0:df_train.shape[0]],extra_train[cols_add],df_train['revenue']],axis=1)\ntest = pd.concat([f_clean.iloc[df_train.shape[0]:].reset_index(),extra_test[cols_add]],axis=1)","0f008a43":"cols_to_drop = ['index','id','belongs_to_collection','genres','homepage','imdb_id','original_language',\n                'original_title','overview','poster_path','production_companies','production_countries',\n               'release_date','spoken_languages','status','tagline','title','Keywords','cast','crew',\n               'edited_title','list_keywords','director']","9bfce227":"X = train.drop(['revenue'],axis=1).drop(cols_to_drop,axis=1)\ny = np.log1p(train['revenue'])\nX_test = test.drop(['level_0'],axis=1).drop(cols_to_drop,axis=1)","3166fbe2":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)","dd0bff70":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\n\nlgbm = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\nlgbm.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=200)","d4207bf4":"eli5.show_weights(lgbm, feature_filter=lambda x: x != '<BIAS>')","ed7205fd":"lasso = linear_model.Lasso(alpha=0.1)\nprint(np.sqrt(-cross_val_score(lasso, X, y, cv=10, scoring='neg_mean_squared_error')))","6c7c66ed":"lasso.fit(X,y)","a93d2121":"preds_lasso = lasso.predict(X_test)","4cabc98a":"sub = pd.read_csv('..\/input\/tmdb-box-office-prediction\/sample_submission.csv')\nsub['revenue'] = np.expm1(preds_lasso)\nsub.to_csv(\"lasso_sub.csv\", index=False)","20eba599":"### Add movie and keyword index dictionaries","c10ed335":"- let us create keyword indices","7b33a712":"### Neural network embedding model","cc75e671":"# FE: Now we will again separate the feature data set into train and test sets and work on feature engineering","ef805006":"### Let us now create the embedding learning task","6f243503":" - some movies with revenue of 1\\$, which seems very unusual. Let's see how many movies are under 300\\$ revenue","4d83a94e":" - each movie can now be represented on the 50-dimensional vector obtained based on keywords\n - to be able to calculate similarities based on cosine similarity, we should first normalize embeddings so that they have the dot product of two movie embeddings is the cosine similarity\n - this normalization is achieved by dividing each vector by the square root of the sum of squared components","0e2300e8":" - start by examining the target variable:","d3d4807a":"# Modelling","0514dd0c":" - some of these values may be correct. However, let us assume that a minimum 300\\$ is more acceptable","64aae2ba":" - examining `production_companies` and `revenue`\n - we will treat this variable similarly to the `cast` variable","d255b991":" - perhaps having a high-profile director helps a movie becoming more successful. We could try to extract some features from this\n - let's add summary variables such as average revenue of the director and (High-Low)\/Average to give a sense of revenue dispersion\n - we will calculate these measures based on cast members in the training set, but we will then trickle that information down to the test set","80e7ddbc":"The neural network will have the following layers:\n1. Input layer (movie and keyword inputs)\n2. Embedding layer (embeddings for movies and keywords. These will be trained to map our inputs into a 50-dimensional vector)\n3. Dot product layer\n4. Reshape layer (to correct the shape)\n5. Dense (in classification): fully connected layer with sigmoid activation to generate output for classification","262c7f97":" - we will disregard keywords that appear only once","46b622e6":" - what about the relation between `cast` and revenue?","ad0eda60":" - before creating movie indices, we should transform the `original_title`, since there are some duplicates that may mix up our results","a6d7ba13":" - now that these 133 duplicates were taken care of, we can use `edited_title` to produce the movie indices","b078512d":" - let us now produce the indices for keywords","e456f52f":"### Extract the embeddings","88fd8665":"(closely following this method: https:\/\/github.com\/WillKoehrsen\/wikipedia-data-science\/blob\/master\/notebooks\/Book%20Recommendation%20System.ipynb)****","df18ae62":" - there are 1,857 directors in the training set, which means most directors only show up once. These will not be very informative to discriminate revenue\n  - how many directors show up twice or more?","fa69c271":" - we will also add date variables, since they will be useful right next after this","b25bf57c":" - there are still some `NaN` values in some of the created columns and in `runtime`. We will replace them by the mode or median","e9f4f50e":" - Setting up a Random training example generator (as in the wikipedia book example)\n - Since the neural network will be trained one batch at a time, the Random training example generator is made in a way that it yields batches of samples each time it is called (which will happen during training of the network)","1bd495a6":" - we will investigate which movies have greater and lower keyword count","665c0aea":" - the relation between cast members and revenue could be slightly trickier than that of directors and revenue? Should try to summarize this somehow\n - we will try to use summary measures such as number of actors which have participated in movies with extremly high or relatively low revenue, or the minimum highest revenue any cast member has achieved in the past\n - we will calculate these measures based on cast members in the training set, but we will then trickle that information down to the test set","8f7d256d":" - `director` might be important?","30c3cdd1":" - there seem to be some zero values for `budget`, which is odd. We will simply replace them by the training set median, due to time constraints","8d21ebc7":"### Training the model","07790219":" - we will extract some features from this notebook https:\/\/www.kaggle.com\/joanalpinto\/moviestmdb-datapreparation","709abffd":"# We will try to learn embeddings from movie-keyword pairs to generate extra features","e8bacd9b":"## Import data, append train and test sets for feature engineering and transform some strings into dictionaries"}}