{"cell_type":{"32638a9e":"code","4bea560e":"code","a69269b2":"code","614fc61a":"code","e151a672":"code","38cc5693":"code","42833f1f":"code","1bad4977":"code","cee86a17":"code","6467ad1a":"code","32bcaef8":"code","9065de74":"markdown","de04a495":"markdown","66da3e59":"markdown","0fb04587":"markdown","03ecc273":"markdown","da937c12":"markdown","f798b003":"markdown","b3cda451":"markdown","0e83c16e":"markdown","c35e9ca5":"markdown"},"source":{"32638a9e":"# Importing Necessary Libraries\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.options.display.max_columns = 999","4bea560e":"dtypes = {\n    'date':'int16',\n    'weight':'float16',\n    'resp_1':'float16',\n    'resp_2':'float16',\n    'resp_3':'float16',\n    'resp_4':'float16',\n    'resp':'float16',\n    'feature_0':'int8',\n    'feature_1':'float16',\n    'feature_2':'float16',\n    'feature_3':'float16',\n    'feature_4':'float16',\n    'feature_5':'float16',\n    'feature_6':'float16',\n    'feature_7':'float16',\n    'feature_8':'float16',\n    'feature_9':'float16',\n    'feature_10':'float16',\n    'feature_11':'float16',\n    'feature_12':'float16',\n    'feature_13':'float16',\n    'feature_14':'float16',\n    'feature_15':'float16',\n    'feature_16':'float16',\n    'feature_17':'float16',\n    'feature_18':'float16',\n    'feature_19':'float16',\n    'feature_20':'float16',\n    'feature_21':'float16',\n    'feature_22':'float16',\n    'feature_23':'float16',\n    'feature_24':'float16',\n    'feature_25':'float16',\n    'feature_26':'float16',\n    'feature_27':'float16',\n    'feature_28':'float16',\n    'feature_29':'float16',\n    'feature_30':'float16',\n    'feature_31':'float16',\n    'feature_32':'float16',\n    'feature_33':'float16',\n    'feature_34':'float16',\n    'feature_35':'float16',\n    'feature_36':'float16',\n    'feature_37':'float16',\n    'feature_38':'float16',\n    'feature_39':'float16',\n    'feature_40':'float16',\n    'feature_41':'float16',\n    'feature_42':'float16',\n    'feature_43':'float16',\n    'feature_44':'float16',\n    'feature_45':'float16',\n    'feature_46':'float16',\n    'feature_47':'float16',\n    'feature_48':'float16',\n    'feature_49':'float16',\n    'feature_50':'float16',\n    'feature_51':'float16',\n    'feature_52':'float16',\n    'feature_53':'float16',\n    'feature_54':'float16',\n    'feature_55':'float16',\n    'feature_56':'float16',\n    'feature_57':'float16',\n    'feature_58':'float16',\n    'feature_59':'float16',\n    'feature_60':'float16',\n    'feature_61':'float16',\n    'feature_62':'float16',\n    'feature_63':'float16',\n    'feature_64':'float16',\n    'feature_65':'float16',\n    'feature_66':'float16',\n    'feature_67':'float16',\n    'feature_68':'float16',\n    'feature_69':'float16',\n    'feature_70':'float16',\n    'feature_71':'float16',\n    'feature_72':'float16',\n    'feature_73':'float16',\n    'feature_74':'float16',\n    'feature_75':'float16',\n    'feature_76':'float16',\n    'feature_77':'float16',\n    'feature_78':'float16',\n    'feature_79':'float16',\n    'feature_80':'float16',\n    'feature_81':'float16',\n    'feature_82':'float16',\n    'feature_83':'float16',\n    'feature_84':'float16',\n    'feature_85':'float16',\n    'feature_86':'float16',\n    'feature_87':'float16',\n    'feature_88':'float16',\n    'feature_89':'float16',\n    'feature_90':'float16',\n    'feature_91':'float16',\n    'feature_92':'float16',\n    'feature_93':'float16',\n    'feature_94':'float16',\n    'feature_95':'float16',\n    'feature_96':'float16',\n    'feature_97':'float16',\n    'feature_98':'float16',\n    'feature_99':'float16',\n    'feature_100':'float16',\n    'feature_101':'float16',\n    'feature_102':'float16',\n    'feature_103':'float16',\n    'feature_104':'float16',\n    'feature_105':'float16',\n    'feature_106':'float16',\n    'feature_107':'float16',\n    'feature_108':'float16',\n    'feature_109':'float16',\n    'feature_110':'float16',\n    'feature_111':'float16',\n    'feature_112':'float16',\n    'feature_113':'float16',\n    'feature_114':'float16',\n    'feature_115':'float16',\n    'feature_116':'float16',\n    'feature_117':'float16',\n    'feature_118':'float16',\n    'feature_119':'float16',\n    'feature_120':'float16',\n    'feature_121':'float16',\n    'feature_122':'float16',\n    'feature_123':'float16',\n    'feature_124':'float16',\n    'feature_125':'float16',\n    'feature_126':'float16',\n    'feature_127':'float16',\n    'feature_128':'float16',\n    'feature_129':'float16',\n    'ts_id':'int32'\n}","a69269b2":"train = pd.read_feather('..\/input\/fast-reading-w-pickle-feather-parquet-jay\/jane_street_train.feather')\ntrain = train.astype(dtypes)","614fc61a":"train","e151a672":"train['numNull'] = train.isnull().sum(axis=1)\ntrain","38cc5693":"train['numNull'].unique()","42833f1f":"b = train.groupby('numNull').count()\nb = b.reset_index()\nb","1bad4977":"a = train.groupby('numNull').mean()\na = a.reset_index()\nsignificantNulls = [0, 1, 2, 3, 4, 6, 7, 12, 13, 14, 15, 16, 18, 19, 21, 25, 26, 27, 35, 36, 39, 40, 42]\na = a.loc[significantNulls]\na","cee86a17":"fig, ax = plt.subplots(3, 2, figsize=(15, 15))\nj = 0\nfor col in ['weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']:\n    ax[int(j \/ 2), j%2].bar(a['numNull'], a[col])\n    ax[int(j \/ 2), j%2].set_title(\"Average \" + col + \" for Different Null Counts\")\n    j += 1\nplt.show()","6467ad1a":"for col in ['weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']:\n    fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n    label = []\n    for null in a['numNull'].values:\n        nullData = train[train['numNull'] == null]\n        sns.distplot(nullData[col], ax=ax)\n        label.append(null)\n    fig.legend(labels=label)\n    plt.show()","32bcaef8":"cols = list(train.columns)\nfor removeCol in ['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp', 'ts_id', 'numNull']:\n    cols.remove(removeCol)\n\nX = train[cols]\nX = X.fillna(0)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\ny = pca.fit_transform(X)\n\nfig, ax = plt.subplots(6, 4, figsize=(15,15))\ni = 0\nfor null in a['numNull'].values:\n    c = y[train['numNull'] == null]\n    ax[int(i \/ 4), i%4].scatter(c[:, 0], c[:, 1])\n    ax[int(i \/ 4), i%4].set_title(\"NumNull = \" + str(null))\n    i += 1\nplt.show()","9065de74":"So there is some variability in each of the null counts, especially with the brown line (numNull = 28). That has an interesting flattish hump. Also, these do look like the returns of different securities, as they follow a similar distribution, but have different skewness. This supports the notion that it might be something to do with different markets of securities.\n\n# Variations in Train\nNow let's see if the training data has any general distinction based on null counts. I will use PCA here since the data is quite high dimensional.","de04a495":"Wow! Now inspecting this, we already see that there is a difference in distribution for some features, based on the number of null values in a row. \n\n# Investigating Avg Resp and Weight\n\nLet's make some plots and see this further. The first set of plots I am going to make is just the average weight and Resp (1, 2, 3, 4 and normal) for each null count. Let's see if there is any difference in the average return.","66da3e59":"If you scroll to the end, you will see a new column 'numNull', which is just the number of null values in the row. Now we can already see that there is some order, as we see that the first few rows all have null values of 31, and the last few have 0s. Let's see what are all of the unique number of null values per row.","0fb04587":"So the average weight does have some variation, but doesn't seem to related to the null counts. \nThe resp features on the other hand are quite interesting. Some null counts have a significantly negative return on average, whereas some have a positive return. This could be a very helpful bit of information when training a model. \n\nNow, let's just look at the distribution of the resp features in the train per null count (right now we did the average, now we're doing the distribution from train). I deliberately made the plots big so that you could see lines of all colors.","03ecc273":"# Final Notes\n\nSo this was a short look at the missing values. The most important piece of information that this gives us (I think) is that we are probably dealing with multiple securities \/ markets over here, rather than a single homogeneous index.\n\nSome other things that could be added to the feature engineering stage:\n- Add a feature that is just the number of null values in the row. Complex models might be understand the variation in training data due to null counts.\n- Add more features for the average weight and resp for that null count (beware that this is leaking some information, so may lead to some amount of overfitting. You could try adding some noise here to make it more robust).\n\nThere is one more interpretation of the null counts, which is that it tells us the time of the day (link to discussion: https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/201264). I am not too sure about this, since there are many days where such a pattern is not repeated, which I think can only be explained by the fact that there are multiple markets \/ securites, but these are just my thoughts. \n\nThanks for going through it and please do upvote if you liked it.\nIf you want a more general understanding and analysis of the data, check out my EDA notebook: https:\/\/www.kaggle.com\/yushg123\/a-walk-down-jane-street-eda-baseline\n","da937c12":"Now, let's first create a column that has the number of missing values in that row, and then see a summary of train data.","f798b003":"So it seems like most of the data is leaning towards a similar trend, but there are significant variations. NumNull = 1, 76, 16 look like blobs, rather than having the tail that many others have. \n\nSo it may even make sense to make separate models for each null count, but it doesn't seem plausible due to the timing constraints and the fact that some null counts have low number of data points.","b3cda451":"# Mystery Behind Null Values\n\nSo it turns out that null values are not entirely random throughout the data, but seem to have a pattern, so I am going to explore this a little further.\n\nNow this kind of makes sense as well, since when you access market data, you either have the data for almost every time period, or not, but rarely have any uncertainty. Unless you have multiple markets or securities. Different markets and securities may not have some data available, so in this notebook, I try to categorize these different markets or securities based on the null values, and see if they have an significantly different distribution.","0e83c16e":"The number of unique elements is very low, considering the massive size of the dataset, but this is still more than what I expected. Let's see what we can do. I think a few of these might just be outliers, having a handful of points in them, so would be insignificant. Let's first find the number of unique rows in each of the null values","c35e9ca5":"Just what we expcted. There are a lot of null counts that have less than 3 digit null values. What I am going to do now is remove all of the nullcounts that have a null count less than 100. Anything more is significant and of interest.\n\nSide note: We can see that most rows have no null values. So any information that comes out of this null value analysis would probably only lead to a small score boost, nothing like a major breakthrough!"}}