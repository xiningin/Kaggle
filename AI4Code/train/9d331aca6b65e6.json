{"cell_type":{"883a31bd":"code","1464eae6":"code","a16876fa":"code","8298e2a4":"code","66cff1b3":"code","f4a0cbd4":"code","99248f57":"code","4fa9748c":"code","53d80936":"code","8e3385fe":"code","0d9bac14":"code","cf301d37":"code","c64e28e6":"code","7246150b":"code","3e4da76a":"code","1e2675f6":"code","27b3c21c":"code","d233d1bb":"code","39de429f":"code","fc3810ad":"code","a6d2b374":"code","c5b9e2d9":"code","bbd842d5":"code","d184087a":"code","0d6ae789":"code","78ea9dd8":"code","de6151a9":"code","5e4bfd3b":"code","cc73b7df":"code","3219ac0b":"code","44e746a8":"code","de8f688f":"code","9e95b2bc":"code","b752e294":"code","f8fc27a7":"code","9838e4ff":"code","07f05068":"markdown","50fd9913":"markdown","d498c759":"markdown","da305d48":"markdown","dfb1649a":"markdown","b2248e9a":"markdown","73917f42":"markdown","4881a033":"markdown","b167bf2c":"markdown","42767991":"markdown","5177bc74":"markdown","417965a1":"markdown","c62db2ee":"markdown","e5ede164":"markdown","77fc2abc":"markdown","9ef5388e":"markdown","1a045f47":"markdown","82cbb086":"markdown","7cb07cea":"markdown","7ae2da1e":"markdown","108964f2":"markdown","92ba0d50":"markdown","94952994":"markdown","152aa49f":"markdown","4644994c":"markdown","e01cd02f":"markdown","9bfafa00":"markdown","a33deaa6":"markdown","63737b12":"markdown","a651a709":"markdown","715a09d3":"markdown","b40e9982":"markdown","845f29bb":"markdown","4f22419e":"markdown","0f7daca7":"markdown","16b24f48":"markdown","096a7740":"markdown","55a2e1d0":"markdown","d4d52222":"markdown","e2b07683":"markdown","ebc57f21":"markdown","36c9c738":"markdown","85b8e4bc":"markdown","83b1c26a":"markdown","119c5d17":"markdown","6d13be1a":"markdown","5e1bcc44":"markdown","af166022":"markdown","e6eda1c2":"markdown","f1df256c":"markdown"},"source":{"883a31bd":"#here ","1464eae6":"#start","a16876fa":"# imports \n\nimport re, time\nimport os, sys, math\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\n\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\nAUTOTUNE = tf.data.AUTOTUNE","8298e2a4":"# #TPUs\n\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n# strategy = tf.distribute.TPUStrategy(tpu)\n\n# print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n","66cff1b3":"# # mixed precision\n# # On TPU, bfloat16\/float32 mixed precision is automatically used in TPU computations.\n# # Enabling it in Keras also stores relevant variables in bfloat16 format (memory optimization).\n# # On GPU, specifically V100, mixed precision must be enabled for hardware TensorCores to be used.\n# # XLA compilation must be enabled for this to work. (On TPU, XLA compilation is the default)\n# MIXED_PRECISION = False\n# if MIXED_PRECISION:\n#     policy = tf.keras.mixed_precision.Policy('mixed_bfloat16')\n#     tf.keras.mixed_precision.set_global_policy(policy)\n#     print('Mixed precision enabled')","f4a0cbd4":"#here","99248f57":"#break","4fa9748c":"#here","53d80936":"#here","8e3385fe":"#here","0d9bac14":"import pandas as pd\n\ndf = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv',usecols = ['Id','Pawpularity'])\ndf.head()","cf301d37":"!mkdir Data","c64e28e6":"import os\n\nfor i in range(100):\n\n  path = os.path.join('.\/Data', str(i+1))\n  os.mkdir(path)\n","7246150b":"import shutil\n\ns_path = '..\/input\/petfinder-pawpularity-score\/train\/'\nd_path = '.\/Data\/' \n\nfor id_, class_ in zip(df['Id'],df['Pawpularity']):  \n    \n  shutil.copy(s_path+str(id_)+'.jpg',d_path+str(class_))\n","3e4da76a":"#break","1e2675f6":"def display_images_from_dataset(dataset):\n  plt.figure(figsize=(13,13))\n  subplot=331\n  for i, (image, label) in enumerate(dataset):\n    plt.subplot(subplot)\n    plt.axis('off')\n    plt.imshow(image.numpy().astype(np.uint8))\n    plt.title(label.numpy().decode(\"utf-8\"), fontsize=16)\n    subplot += 1\n    if i==8:\n      break\n  #plt.tight_layout()\n  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n  plt.show()","27b3c21c":"# We are taking small size for quick demostration, You can take any size \nTARGET_SIZE = [256,256] \n\nLOCAL_INPUT ='.\/Data\/*\/*.jpg'\nGCS_OUTPUT = 'gs:\/\/exampel_bucket\/example_folder_\/'  # prefix for output file names\n\nSHARDS = 16\n","d233d1bb":"#  maps to the labels in the data (folder names)\nCLASSES = [str(i) for i in range(1,101) ] # simply maps the image to Pawpularity \nprint(CLASSES[:5])","39de429f":"nb_images = len(tf.io.gfile.glob(LOCAL_INPUT))\nshard_size = math.ceil(1.0 * nb_images \/ SHARDS)\nprint(\"Pattern matches {} images which will be rewritten as {} .tfrec files containing {} images each.\".format(nb_images, SHARDS, shard_size))\n","fc3810ad":"def decode_jpeg_and_label(filename):\n    \n  bits = tf.io.read_file(filename) # this will not allow the reading from local file system, changing to normal read\n  image = tf.io.decode_jpeg(bits)\n    \n  # parse flower name from containing directory\n  label = tf.strings.split(tf.expand_dims(filename, axis=-1), sep='\/')\n  label = label.values[-2]\n  return image, label","a6d2b374":"filenames = tf.data.Dataset.list_files(LOCAL_INPUT, seed=35155) # This also shuffles the images\n\ndataset1 = filenames.map(decode_jpeg_and_label, num_parallel_calls=AUTOTUNE)\n","c5b9e2d9":"display_images_from_dataset(dataset1)","bbd842d5":"#here","d184087a":"def resize_and_crop_image(image, label):\n  # Resize and crop using \"fill\" algorithm:\n  # always make sure the resulting image\n  # is cut out from the source image so that\n  # it fills the TARGET_SIZE entirely with no\n  # black bars and a preserved aspect ratio.\n  w = tf.shape(image)[0]\n  h = tf.shape(image)[1]\n  tw = TARGET_SIZE[1]\n  th = TARGET_SIZE[0]\n  resize_crit = (w * th) \/ (h * tw)\n  image = tf.cond(resize_crit < 1,\n                  lambda: tf.image.resize(image, [w*tw\/w, h*tw\/w]), # if true\n                  lambda: tf.image.resize(image, [w*th\/h, h*th\/h])  # if false\n                 )\n  nw = tf.shape(image)[0]\n  nh = tf.shape(image)[1]\n  image = tf.image.crop_to_bounding_box(image, (nw - tw) \/\/ 2, (nh - th) \/\/ 2, tw, th)\n  return image, label\n  \n","0d6ae789":"dataset2 = dataset1.map(resize_and_crop_image, num_parallel_calls=AUTOTUNE)","78ea9dd8":"display_images_from_dataset(dataset2)","de6151a9":"#here","5e4bfd3b":"def recompress_image(image, label):\n  height = tf.shape(image)[0]\n  width = tf.shape(image)[1]\n  image = tf.cast(image, tf.uint8)\n  image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)\n  return image, label, height, width\n\n","cc73b7df":"dataset3 = dataset2.map(recompress_image, num_parallel_calls=AUTOTUNE)\n\n# sharding: there will be one \"batch\" of images per file \ndataset3 = dataset3.batch(shard_size) ","3219ac0b":"def _bytestring_feature(list_of_bytestrings):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))","44e746a8":"def _int_feature(list_of_ints): # int64\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))","de8f688f":"def _float_feature(list_of_floats): # float32\n  return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))","9e95b2bc":"def to_tfrecord(tfrec_filewriter, img_bytes, label, height, width):  \n\n  # work around for the problme of leabeling all the image to zero \n  class_num = int(label)-1#np.argmax(np.array(CLASSES)==int(label)) # 'roses' => 2 (order defined in CLASSES)\n\n\n  one_hot_class = np.eye(len(CLASSES))[class_num]     # [0, 0, 1, 0, 0 ... 0] for class #2\n\n  feature = {\n      \"image\": _bytestring_feature([img_bytes]), # one image in the list\n      \"class\": _int_feature([class_num]),        # one class in the list\n      \n      # additional (not very useful) fields to demonstrate TFRecord writing\/reading of different types of data\n      \"label\":         _bytestring_feature([label]),          # fixed length (1) list of strings, the text label\n      \"size\":          _int_feature([height, width]),         # fixed length (2) list of ints\n      \"one_hot_class\": _float_feature(one_hot_class.tolist()) # variable length  list of floats, n=len(CLASSES)\n  }\n  return tf.train.Example(features=tf.train.Features(feature=feature))","b752e294":"print(\"Writing TFRecords\")\n\n\nfor shard, (image, label, height, width) in enumerate(dataset3):\n  # batch size used as shard size here\n  shard_size = image.numpy().shape[0]\n  # good practice to have the number of records in the filename\n  filename = GCS_OUTPUT + \"{:02d}-{}.tfrec\".format(shard, shard_size)\n  \n  with tf.io.TFRecordWriter(filename) as out_file:\n    for i in range(shard_size):\n      example = to_tfrecord(out_file,\n                            image.numpy()[i], # re-compressed image: already a byte string\n                            label.numpy()[i],\n                            height.numpy()[i],\n                            width.numpy()[i])\n      out_file.write(example.SerializeToString())\n    print(\"Wrote file {} containing {} records\".format(filename, shard_size))\n    ","f8fc27a7":"#break","9838e4ff":"#end","07f05068":"## GCS SET UP","50fd9913":"#### Writing integers","d498c759":"#### Writing floats","da305d48":"## Creating the Dataset ","dfb1649a":"#### Recompress ","b2248e9a":"### What are GCP and GCS?","73917f42":"Now let\u2019s move all the images ","4881a033":"Finally, we have successfully converted the image labels into tfrec and saved all the files into the GCS bucket.\n\nIn a nutshell, we have done the following:\n\n* Rearrange the data in multiple folders \n* Read the data and perform certain operations ( we can have any type of processing we want )\n* Convert the data into the tfrec files \n* Write the tfrec files into the GCS bucket \n\nI hope the journey was interesting, in the next part we will read the data from the GCS bucket in the form of tf.data.dataset And train the deep learning model using TPU.\n","b167bf2c":"The first option is not possible as we don\u2019t have that much memory available to load all the data.\nThe second option however seems possible because our data can be loaded in batches, but remember the problem we talked about that TPUs are very fast, and the problem is with feeding the data to the TPU. local storage won\u2019t work even if we are using TFRecord file format.\n\nGoogle Cloud Storage (GCS) is capable of sustaining very high throughput. And that is why we need to use GCP with TPU.\n","42767991":"Display utility ","5177bc74":"Let's see the images in the dataset.","417965a1":"![](data:image\/png;base64,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)","c62db2ee":"\nTensorflow has its own file format called as TFRecord or tfrec file format for storing a sequence of binary records. Using Tfrecord has many advantages especially when dealing with large datasets and TPU. \n\nThe TFRecord files are stored in binary which means it takes very little space compared to standard storage, As well as many TensorFlow functionalities that are specially modified to run faster when used with the TFRecoed. Using TFRecord also increases your training performance. \n","e5ede164":"Tensor Processing Unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning, particularly using Google's own TensorFlow software.\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/be\/Tensor_Processing_Unit_3.0.jpg\/220px-Tensor_Processing_Unit_3.0.jpg)\n\nWell, we know that TPU is incredibly useful when we have large neural networks. Cloud TPU resources accelerate the performance of linear algebra computation and thus give us high-speed processing. \n\n","77fc2abc":"We can see that the Pawpularity ranges from 1 to 100, It would be simple if we have 100 folders each containing the images belonging to that particular category.","9ef5388e":"## Why do we need GCS?","1a045f47":"\n![](data:image\/jpeg;base64,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)\n","82cbb086":"#### We are going to set up the directory and have all the images put into their respective labeled folder ","7cb07cea":"## BASIC INFO\n\nIn this section we will be dissusing about the basic information need to continue with the notebook ","7ae2da1e":"TPUs pair a classic vector processor with a dedicated matrix multiply unit and excel at any task where large matrix multiplications dominate, such as neural networks.\nYou can learn more [here](https:\/\/codelabs.developers.google.com\/codelabs\/keras-flowers-data#2).\n","108964f2":"Here in this competition, we have a huge dataset that can not directly be loaded into the memory. We load the batches of the data into the memory and then process it, for such applications TFReocrd files are very helpful. The only downside is we have to convert the data into TFRecord before using and conversion is not really straightforward. \n\n\nUsing TFRecord is very useful when we are dealing with TPU, As TPUs are very fast, often the problem is to feed the data to the TPU fast enough to keep the TPU busy and that\u2019s why using TFRcord files stored in GCS is ideal for such tasks. We will talk more about this soon\u2026\n\n","92ba0d50":"### Setting up your GCS bucket and adding permissions ","94952994":"We are done with the basic setup, We can start converting ","152aa49f":"### Let's see the data once ( brushing up a little ) \n\nWe have seen the complete EDA and all about that so we will not be going through everything again, but let\u2019s just see the data and its structure\n","4644994c":"#### Resize and crop ","e01cd02f":"As this is an computer vision compitation we need to process a lot of visula information and the model that we use for such applications can be very large and complicated. \n\nIn such compitations we need to constatly conduct experiemtnts to find the best technique\/model. In order to do so we need a fine TPU set up ready to use and experiemnt \n\n","9bfafa00":"Now, this looks pretty much it, But we need to do some processing.","a33deaa6":"### What is Tfrec files and how it is usefull ?","63737b12":"Now we have understood a lot about the tfrec files and tf.data.\n\nIt\u2019s time for writing all that tfrec files into the GCS bucket.\n\nNote: So far we have defined the structure only, now we will be utilizing this structure to do the actual action, the below cell can take some time depending on the image size and processing power. \n","a651a709":"### Hello reders,\nIn this notebook we will be dissusing about the Tfrecord files and fully utilizing the TPU. \n\nWhile we all know that the TPU is awesome and the fetures it provides can help us in the high processing tasks specially when dealing with large nureal networks and image data. \n\nIn this compitation we are using image data and task is fairly complicated.Utilizing TPU not only provides Us the high processing power but also makes the experimenting with the new model very easy. \n\nHere in this notebook we will learn many things about the tfrecord files and TPU.\n","715a09d3":"\nGCP stands for Google cloud platform and it is a public cloud vendor. GCP provides a lot of clouds functionalities like compute engines, virtual machines, cloud storage, and many more.\nMore on GCP [here](https:\/\/en.wikipedia.org\/wiki\/Google_Cloud_Platform).  \n\nGCS stands for google cloud storage. Google Cloud Storage is a RESTful online file storage web service for storing and accessing data on Google Cloud Platform infrastructure. We will be using GCS for our data storage.\nMore on GCS [here](https:\/\/en.wikipedia.org\/wiki\/Google_Cloud_Storage). \n\n","b40e9982":"Its time for writing the TFrec files, To do so we use the above functions and then convert the image labels into the tfrec files.","845f29bb":"#### Writing byte strings","4f22419e":"Now we are going to produce the first dataset \n\n\nNote: just a thing, We define the dataset structure, what to do with the image, and all about the image processing.  the real images are not stored in the dataset object, it happens  when we fetch the batch from the dataset object.\n","0f7daca7":"Three types of data can be stored in TFRecords: byte strings (list of bytes), 64 bit integers and 32 bit floats. They are always stored as lists, a single data element will be a list of size 1. You can use the following helper functions to store data into TFRecords.\n","16b24f48":"### Let's begin ","096a7740":"# Utilizing_full_TPU : (Part - 1) A Journey from image\/label to tfrecord in GCS","55a2e1d0":"Here we simply decode the image label in the dataset1. \n\nWe will be experimenting more. ","d4d52222":"### Following is the overview of the notebook\n\n### Part - 1\n\n#### BASIC INFO \n* What is TPU and why should we use it here?\n* What are Tfrec files and how it is useful?\n\n\n#### GCS SET UP  \n* What are GCP and GCS?\n* Why do we need GCS?\n* Setting up your GCS bucket and permissions\n\n#### From IMAGES-LABELS to tfrec in GCP \n* looking in the data\n* Converting image labels to tfrec\n* uploading to GCP bucket\n\n\n\n### part - 2\n\n\n#### From GCP bucket to train\/Val dataset \n* loading and preprocessing the tfrec files \n* generating the train\/val dataset\n\n#### Setup the deep learning model \n* Transfer learning block\n* Fine tunning block \n* CNN and FNN blocks \n* compile and set up the model \n\n#### Training the model ( Experiments ) \n* training the model \n* saving the best \n\n#### Inference \n* Using the saved model \n* making the submition file \n\n#### Final Points \n* What can be modified \n...\n","e2b07683":"### What is TPU and why should we use it here ?","ebc57f21":"### Thank you, Happy to hear any thoughts\/suggestions :) ...","36c9c738":"NOTE: We are not going to convert every single image label into its respective tfrec file because of the I\/O problem. We have discussed the problem of fast feeding the data to TPU, Even though the GCS bucket is very fast but we can not trust the internet connection or any networking issue, and thus instead of putting soo many tfrec files in the GCS, we simply generate a relatively small amount of the tfrec files containing all the data. \n\nThis makes the input pipeline more efficient.\n","85b8e4bc":"#### Note: Kaggel TPU is in high demand these days and thus we may not get the TPU, but we can always use the Google colab TPU it's easily available.  \n\n","83b1c26a":"The data is simple, We have a lot of images in the train folder and we have one train.csv file wich contains the label for each image and we want to have the pair of the image and label so that we can convert it into the respective tfrec file ","119c5d17":"Well, one might wonder why do we need GCS. AS we know The Tensor Processing Unit (TPU) hardware accelerators are very fast. Problem is to feed the data fast enough to keep the TPU busy. \n\nNote: I imagine that reader is using cloud TPU only.\n\nWhen dealing with the Clod TPU we have two options, \n* load all the data into the memory to use with TPU \n* load the batches of the data while training.\n\n","6d13be1a":"Let's set up a few things before we move to the conversion \nflowing are the information about the attributes  we are using:\n\n* TARGET_SIZE: the size of the final image in the dataset ( i.e the final dataset will contain the images of this size in the tfrec format )\n* LOCAL_INPUT: location of the input images \n* GCS_OUTPUT: location of the output GCS bucket \n* SHARDS: no. of tfrec files produced as output and stored in the GCS bucket \n","5e1bcc44":"First, off all You need to create the GCP billing account (a simple free account is good enough ) \nYou can learn all about creating the GCP project [here](https:\/\/cloud.google.com\/resource-manager\/docs\/creating-managing-projects).\n\nOnce you have the project ready with a valid billing account, you can move to creating the GCS bucket. Creating and managing a GCS bucket is easy if you have experience with cloud services. If you don\u2019t have much experience you can always learn !!! \nMore information on creating the GCS bucket [here](https:\/\/cloud.google.com\/storage\/docs\/creating-buckets). \n\nOnce you have the GCS bucket ready, You need to manage the permissions to access and modify the GCS bucket\n[here](https:\/\/cloud.google.com\/storage\/docs\/access-control\/using-iam-permissions).  \n","af166022":"Here in this section, we will convert the image\/label pair to the tf record files \n","e6eda1c2":"#### Note: You can notice that I haven\u2019t included any defined steps for GCP actions because in my personal experience it\u2019s always best to look into the updated documentation for solving the problem.\n\n","f1df256c":"## A journey from IMAGES-LABELS to tfrec in GCS"}}