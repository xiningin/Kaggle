{"cell_type":{"8dd0c7f0":"code","82caa913":"code","7aff098a":"code","91e96d72":"code","852987e3":"code","8972fcd7":"code","f27eeea0":"code","babf1eab":"code","5742b5b9":"code","bd3f09d2":"code","89262a33":"code","03e99cf6":"code","67069357":"code","c00590d5":"code","4183a19d":"code","5dbd6259":"code","6b89c5c2":"code","baedc7e7":"code","61f3aa60":"code","e61655a2":"code","725185c9":"code","ba22cfcb":"code","4c526439":"code","9562c75a":"code","a041c47e":"code","4ec2feb4":"code","2d3960fa":"code","a72e72f7":"code","31e642ae":"code","65677d9b":"code","836910ab":"code","b9c3ea58":"code","d6abf8b8":"code","a2f874f7":"code","16b24c0e":"code","b5efc301":"code","ab3408d0":"code","5227b437":"code","b747fe35":"code","ccd90fd4":"code","c85d7919":"code","4ad29943":"code","f7a45e7f":"code","c61940b0":"code","2faf467a":"code","7aa58c6b":"code","00edec11":"code","2bd19f74":"code","35874d3c":"code","f507efd9":"code","ec8a5daf":"code","c67cfe66":"code","1061d9d2":"code","29c7baed":"markdown","5cf6240d":"markdown","96de1ae2":"markdown","0efd8145":"markdown","1677f3f9":"markdown","ee38f65d":"markdown","3f28be85":"markdown","08a5c4f6":"markdown","44384761":"markdown","e5b63590":"markdown","08cd42dc":"markdown","77a3f3e3":"markdown","1572b94f":"markdown","77f60e81":"markdown","f687c63d":"markdown","bade189d":"markdown","1b1614d8":"markdown","9486695a":"markdown","9f841fa5":"markdown","ba6130d8":"markdown","b5a4c902":"markdown","9614a5a9":"markdown","6ed14151":"markdown","288145e5":"markdown","a5317b23":"markdown","a784f8d7":"markdown","a2601518":"markdown","f2f300f2":"markdown","18ac57ea":"markdown","c819e6d5":"markdown","22cbcd2f":"markdown","5131b05a":"markdown","f294e6b3":"markdown","29a24d5c":"markdown","dffa41f6":"markdown","3ba5cd08":"markdown","817e977c":"markdown","33bcb28e":"markdown","60c7e84c":"markdown","2593e66a":"markdown","ef9a4951":"markdown","a55bb617":"markdown","1e66039c":"markdown"},"source":{"8dd0c7f0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport optuna\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import (\n    ElasticNet, \n    Lasso,\n    LinearRegression,\n    Ridge\n)\nfrom scipy.stats import norm, skew, boxcox_normmax #for some statistics\nfrom sklearn import utils\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import RFE, f_regression\nfrom scipy.special import boxcox1p\n\nimport datetime\nimport time","82caa913":"# Note:  We can't set this here due to https:\/\/github.com\/mlflow\/mlflow\/issues\/608\n#tracking_uri='file:\/\/\/mnt\/pipelineai\/users\/experiments'\nfrom mlflow import log_metric, log_param, log_artifact\nimport mlflow.sklearn\nimport mlflow.xgboost\n\nexperiment_name = 'house_price'\nmlflow.set_experiment(experiment_name)\n# Forcing an end_run() to prevent \n#    https:\/\/github.com\/mlflow\/mlflow\/issues\/1335 \n#    https:\/\/github.com\/mlflow\/mlflow\/issues\/608\nmlflow.end_run()\n\nartifact_path = mlflow.get_artifact_uri()\nuri = mlflow.tracking.get_tracking_uri()\nprint(artifact_path)\nprint(uri)","7aff098a":"def log_mlflow(model):\n    # Track params and metrics \n    with mlflow.start_run() as run:\n        mlflow.set_tag(\"model_name\", name)\n        mlflow.log_param(\"CV_n_folds\", CV_n_folds)\n        mlflow.log_param(\"TEST_PART\", TEST_PART)\n        mlflow.log_param(\"Train size\", X_train.shape)\n        mlflow.log_param(\"Colums\", str(X_train.columns.values.tolist()))\n        mlflow.log_metrics({'rmse_cv': score_cv.mean(), 'rmse': score})\n        mlflow.log_metric(\"rmse_train\", score_train)\n        # Save model to artifacts\n        mlflow.sklearn.log_model(model, name)\n    mlflow.end_run()","91e96d72":" mlflow.end_run()","852987e3":"# Import data\npath =  '\/kaggle\/input\/house-prices-advanced-regression-techniques\/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nprint (\"Size ntrain= {} \/ ntest = {}\".format(train.shape, test.shape))\n# train.head()","8972fcd7":"# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","f27eeea0":"# Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4600)].index)\ntrain = train.drop(train[(train['TotalBsmtSF']>5900)].index)\ntrain = train.drop(train[(train['1stFlrSF']>4000)].index)\ntrain = train.drop(train[(train['MasVnrArea']>1500)].index)\ntrain = train.drop(train[(train['GarageArea']>1230)].index)\ntrain = train.drop(train[(train['TotRmsAbvGrd']>13)].index)","babf1eab":"ntrain = train.shape[0]\nntest = test.shape[0]\nprint (\"Size ntrain= {} \/ ntest = {}\".format(ntrain, ntest))\n# all_data = pd.concat((train, test)).reset_index(drop=True)\nall_data = train.append(test, sort=False).reset_index(drop=True)\n#To save original ID for final submission\norig_test = test.copy() \nlog_y_train = train['SalePrice']\n# To avoid normalization of SalesPrice - drop it from All data \nall_data.drop(['SalePrice'], axis=1, inplace=True)\n# Id no need for traning\nall_data.drop(['Id'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","5742b5b9":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","bd3f09d2":"threshold = 0.90\n# Absolute value correlation matrix\ncorr_matrix = train.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# upper.head(50)\n# Select columns with correlations above threshold\ncollinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\nprint('There are %d features to remove.' % (len(collinear_features)))\nprint(collinear_features)","89262a33":"# Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n    \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n    \n# Raplace null with None value.\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")    \nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\nall_data = all_data.drop(['Utilities'], axis=1)","03e99cf6":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","67069357":"def convert_str_to_int(data, features, score):\n    all_data[features] = all_data[features].applymap(lambda s: score.get(s) if s in score else s)\n\nfeaturesQualCond = [\"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"KitchenQual\",\n                    \"FireplaceQu\", \"HeatingQC\", \"GarageQual\", \"GarageCond\", \"PoolQC\"]\nqual_score_QualCond = {\"None\":0, \"NA\":1, \"Po\":2, \"Fa\":3, \"TA\":4, \"Gd\":5, \"Ex\":6}\nconvert_str_to_int(all_data, featuresQualCond, qual_score_QualCond)\n\nfeaturesExposure = [\"BsmtExposure\"]\nqual_score = {\"None\":0, \"NA\":1, \"No\":2, \"Mn\":3, \"Av\":4, \"Gd\":5}\nconvert_str_to_int(all_data, featuresExposure, qual_score)\n\nfeaturesFinType = [\"BsmtFinType1\", \"BsmtFinType2\"]\nqual_score = {\"None\":0, \"NA\":1, \"Unf\":2, \"LwQ\":3, \"Rec\":4, \"BLQ\":5, \"ALQ\":6, \"GLQ\":7}\nconvert_str_to_int(all_data, featuresFinType, qual_score)\n\nfeaturesGarageFin = [\"GarageFinish\"]\nqual_score = {\"None\":0, \"NA\":1, \"Unf\":2, \"RFn\":3, \"Fin\":4}\nconvert_str_to_int(all_data, featuresGarageFin, qual_score)","c00590d5":"#MSSubClass=The building class represented as int but it's category\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","4183a19d":"cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","5dbd6259":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskew_features = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"Skew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skew_features})\n\nskewness = skewness[abs(skew_features) > 0.05]\nprint(\"There are {} high skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nskewed_features = skewness.index\nfor i in skewed_features:\n    all_data[i] = boxcox1p(all_data[i], 0.15)\nprint('Shape all_data: {}'.format(all_data.shape))","6b89c5c2":"# Getting Dummies from Condition1 and Condition2\nconditions = set([x for x in all_data['Condition1']] + [x for x in all_data['Condition2']])\ndummies = pd.DataFrame(data=np.zeros((len(all_data.index), len(conditions))),\n                       index=all_data.index, columns=conditions)\nfor i, cond in enumerate(zip(all_data['Condition1'], all_data['Condition2'])):\n#     dummies.ix[i, cond] = 1\n    dummies.ix[i, cond] = 1\nall_data = pd.concat([all_data, dummies.add_prefix('Condition_')], axis=1)\nall_data.drop(['Condition1', 'Condition2'], axis=1, inplace=True)","baedc7e7":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","61f3aa60":"cols = ('Condition_RRNn', 'Condition_RRAe', \n        'Condition_Artery', 'Condition_Feedr', 'Condition_Feedr', 'Condition_RRNe', \n        'Condition_PosA', 'Condition_Norm', 'Condition_RRAn', \n        'Condition_PosN')\n# process columns, apply LabelEncoder to categorical features\nlbl = LabelEncoder()\nfor c in cols:\n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(all_data[c].values)       \nprint('Shape all_data: {}'.format(all_data.shape))","e61655a2":"todrop = ['GarageType_BuiltIn', 'MiscFeature_None', 'PoolQC', 'MiscVal', 'Condition_RRAn', 'Neighborhood_BrDale', 'GarageType_2Types', 'Exterior1st_Stucco', 'Neighborhood_Blmngtn', 'LotConfig_FR3', 'Neighborhood_Timber', 'SaleType_ConLI', 'Condition_PosA', 'LandContour_Bnk', 'Alley_None', 'Street_Pave', 'Street_Grvl', 'Condition_Norm', 'Condition_RRNn', '3SsnPorch', 'BldgType_TwnhsE', 'RoofMatl_Membran', 'RoofMatl_WdShake', 'RoofMatl_Roll', 'RoofMatl_Metal', 'Exterior2nd_Stone', 'Exterior2nd_MetalSd', 'MasVnrType_None', 'Exterior2nd_ImStucc', 'LowQualFinSF', 'RoofMatl_Tar&Grv', 'Exterior2nd_AsphShn', 'Heating_GasA', 'HouseStyle_2.5Unf', 'Exterior2nd_AsbShng', 'Exterior1st_WdShing', 'BldgType_Duplex', 'Exterior2nd_CBlock', 'SaleType_Oth', 'Condition_PosN', 'Neighborhood_Veenker', 'BldgType_2fmCon', 'MiscFeature_TenC', 'Neighborhood_Blueste', 'RoofStyle_Mansard', 'Foundation_Slab', 'HouseStyle_SFoyer', 'Heating_Floor', 'HouseStyle_2.5Fin', 'Exterior1st_Stone', 'Exterior1st_CBlock']\nprint (\"Before Drop = \", all_data.shape)\nall_data.drop(todrop, axis=1, inplace=True)\nprint (\"After Drop = \", all_data.shape)\n","725185c9":"scaler = RobustScaler()\ndf_all = pd.DataFrame(scaler.fit_transform(all_data))","ba22cfcb":"# all_data = all_data.iloc[:, 1:10]\n# data.iloc[:, 0:2] # first two columns of data frame with all rows\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]\ncolnames = train.columns\n\n# Check remaining missing values if any \ntrain_na = (train.isnull().sum() \/ len(all_data)) * 100\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :train_na})\nmissing_data.head()","4c526439":"# Log into a file train shape and columns names\nf = open(\"models_training_log.txt\", \"a+\")\nprint(\"\\n-------------------\" + str(datetime.datetime.now().isoformat()) + \"-------------------\", file=f)\nprint(\"Train shape:\" + str(train.shape) , file=f)\nprint(\"feature names:\" + str(list(colnames)) , file=f)\nf.close()","9562c75a":"def rmsle_cv(model, X_train, y_train, cv_n_folds):\n    kf = KFold(cv_n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef print_rmse_score(y, y_pred):\n    score = rmse(y, y_pred)\n    print(\"RMSE score: {:.8f}\".format(score))\n    return score\n\ndef print_rmse_cv_score(model, X_train, y_train, cv_n_folds, prefix=\"\"):\n    score = rmsle_cv(model, X_train, y_train, cv_n_folds)\n    print(prefix + \"CV RMSE score: {:.8f} ({:.4f})\".format(score.mean(), score.std()))\n    return score\n\ndef prepare_datasets(X_matrix, Y_vector, test_part):\n    X_train = X_matrix\n    y_train = Y_vector\n    X_test = np.array([])\n    y_test = np.array([])\n    if test_part > 0:\n        X_train, X_test, y_train, y_test = train_test_split(X_matrix, Y_vector, random_state = 0, test_size = test_part)\n            \n    print (\"\\nTEST_PART = \",  test_part)\n    print (\"Train X| \" + str(X_train.shape) + \" Y| \" + str(y_train.shape))\n    print (\"Test X| \" + str(X_test.shape) + \" Y| \" + str(y_test.shape))\n    return [X_train, y_train, X_test, y_test]\n","a041c47e":"X = train.copy()\nY = log_y_train.copy()\nX_submit = test.copy()\n\nTEST_PART = 0.1\nCV_n_folds = 3\n[X_train, y_train, X_test, y_test] = prepare_datasets(X, Y, TEST_PART)","4ec2feb4":"classifiers=set([])\nmodels={}\nscores_cv={}\nscores={}\nscores_train={}\nsubmits={}","2d3960fa":"# Define dictionary to store our rankings\nranks = {}\n# Create our function which stores the feature rankings to the ranks dictionary\ndef ranking(ranks, names, order=1):\n    minmax = MinMaxScaler()\n    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n    ranks = map(lambda x: round(x,2), ranks)\n    return dict(zip(names, ranks))","a72e72f7":"%%time\nmodel_ridge = Ridge(alpha=5.75)\nmodel = model_ridge.fit(X_train, y_train)\nscore_cv = score_ridge_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_ridge = print_rmse_score(y_test, model.predict(X_test))\nridge_submit = np.expm1(model.predict(X_submit))\n\nname = \"Ridge\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = ridge_submit\nranks[name] = ranking(np.abs(model.coef_), colnames)\n\nlog_mlflow(model)","31e642ae":"%%time\nparams = {\n    'learning_rate': 0.01,\n    \"n_estimators\":500,\n    'max_depth': 3,\n    'max_features': \"sqrt\",\n    \"loss\":\"huber\",\n    'min_samples_leaf': 12,\n    'min_samples_split': 11,\n    \"random_state\":5\n}\nmodel_GBoost = GradientBoostingRegressor(**params)\nmodel = model_GBoost.fit(X_train, y_train)\nscore_cv = score_GBoost_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_GBoost = print_rmse_score(y_test, model.predict(X_test))\ngboost_submit = np.expm1(model.predict(X_submit))\n\nname = \"GBoost\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = gboost_submit\nranks[name] = ranking(np.abs(model.feature_importances_), colnames)\n\nlog_mlflow(model)","65677d9b":"%%time\nmodel_KRR = KernelRidge(alpha=0.03525, \n                        kernel='polynomial', \n                        degree=1, coef0=1e-6)\nmodel = model_KRR.fit(X_train, y_train)\nscore_cv = score_KRR_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_KRR = print_rmse_score(y_test, model.predict(X_test))\nkrr_submit = np.expm1(model.predict(X_submit))\n\nname = \"KRR\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = krr_submit\n\nlog_mlflow(model)","836910ab":"%%time\nparams = {'learning_rate': 0.01, 'num_leaves': 3, 'max_bin': 84,\n          'bagging_freq': 1, 'bagging_seed': 2, 'feature_fraction_seed': 97, \n          'bagging_fraction': 0.745, \"verbose\":-1,\n          'objective': 'regression',\"n_estimators\":1000\n         }\nmodel_lgbm = lgb.LGBMRegressor(**params)\nmodel = model_lgbm.fit(X_train, y_train)\nscore_cv = score_lgbm_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_lgbm = print_rmse_score(y_test, model.predict(X_test))\nlgbm_submit = np.expm1(model.predict(X_submit))\n\nname = \"lgbm\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = lgbm_submit\nranks[name] = ranking(np.abs(model.feature_importances_), colnames)\nlog_mlflow(model)","b9c3ea58":"%%time\nmodel_xgb = xgb.XGBRegressor(tree_method=\"hist\",\n                             colsample_bytree=0.4603, gamma=0.01468, \n                             learning_rate=0.05187, max_depth=3, \n                             min_child_weight=0.0817, n_estimators=200,\n                             reg_alpha=0.4640, reg_lambda=0.6571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel = model_xgb.fit(X_train, y_train)\nscore_cv = score_xgb_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_xgb = print_rmse_score(y_test, model.predict(X_test))\nxgb_submit = np.expm1(model.predict(X_submit))\n\nname = \"xgb\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = xgb_submit\nranks[name] = ranking(np.abs(model.feature_importances_), colnames)\n\nlog_mlflow(model)\n","d6abf8b8":"%%time\nmodel_lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.00055419, random_state=1, max_iter=50000))\nmodel = model_lasso.fit(X_train, y_train)\nscore_cv = score_lasso_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_lasso = print_rmse_score(y_test, model.predict(X_test))\nlasso_submit = np.expm1(model.predict(X_submit))\n\nname = \"lasso\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = lasso_submit\nranks[name] = ranking(np.abs(model[1].coef_), colnames)\n\nlog_mlflow(model)","a2f874f7":"%%time\nparams = {\n    \"alpha\":0.00185,\n    \"max_iter\":10000,\n    \"l1_ratio\":0.224,\n    \"random_state\":1\n}\nmodel_ENet = make_pipeline(RobustScaler(), ElasticNet(**params))\nmodel = model_ENet.fit(X_train, y_train)\nscore_cv = score_ENet_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_ENet = print_rmse_score(y_test, model.predict(X_test))\nENet_submit = np.expm1(model.predict(X_submit))\n\nname = \"ENet\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = ENet_submit\nranks[name] = ranking(np.abs(model[1].coef_), colnames)\n\nlog_mlflow(model)","16b24c0e":"class CustomEnsembleRegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, regressors=None):\n        self.regressors = regressors\n\n    def fit(self, X, y):\n        for regressor in self.regressors:\n            regressor.fit(X, y)\n        return self\n\n    def predict(self, X):\n        self.predictions_ = list()\n        for regressor in self.regressors:\n            self.predictions_.append(np.exp(regressor.predict(X).ravel()))\n\n        return np.log1p(np.mean(self.predictions_, axis=0))","b5efc301":"# Averaged base models class\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","ab3408d0":"%%time\nmodel_averaged = AveragingModels(models = (model_ENet, \n                                            model_lasso, \n                                            model_GBoost,\n                                            model_KRR))\n\nmodel = model_averaged.fit(X_train, y_train)\nscore_cv = score_averaged_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_averaged = print_rmse_score(y_test, model.predict(X_test))\naveraged_models_submit = np.expm1(model.predict(X_submit))\n\nname = \"Averaged\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = averaged_models_submit\n\nlog_mlflow(model)","5227b437":"# https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=False, random_state=111)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                \n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","b747fe35":"%%time\nmodel_stacked_averaged = StackingAveragedModels(base_models = (model_ENet, model_GBoost, model_KRR),\n                                                 meta_model = model_lasso)\n\nmodel = model_stacked_averaged.fit(X_train.values, y_train.values)\nscore_cv = score_stacked_cv = print_rmse_cv_score(model, X_train, y_train.values, CV_n_folds)\nscore = score_stacked = print_rmse_score(y_test, model.predict(X_test.values))\nstacked_averaged_models_submit = np.expm1(model.predict(X_submit.values))\n\nname = \"Stacked\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = stacked_averaged_models_submit\n\nlog_mlflow(model)","ccd90fd4":"CV_score=[]\ntrain_score=[]\nPred_score=[]\nscore_list=[]\nlog_y_test = np.expm1(y_test)\ny_pred_df = pd.DataFrame({\"test\":log_y_test}).reset_index(drop=True)\nstd=[]\n\nfor i, name in enumerate(classifiers):\n    model = models[name]\n    if name != \"Stacked\":\n        y_pred = np.expm1(model.predict(X_test))\n    else:\n        y_pred = np.expm1(model.predict(X_test.values))\n    CV_score.append(scores_cv[name].mean())\n    std.append(scores_cv[name].std())\n    Pred_score.append(scores[name])\n    train_score.append(scores_train[name])\n    score_list.append(scores_cv[name])\n    \n    new_y_pred = pd.DataFrame({name:y_pred})\n    y_pred_df = pd.concat([y_pred_df, new_y_pred], axis=1).reset_index(drop=True)\n    y_pred_df[\"del_\"+name] = y_pred_df[\"test\"] - y_pred_df[name]\n   \n    \nnew_models_dataframe2=pd.DataFrame({\"RMSE\":Pred_score, \"Train - RMSE\":train_score,'CV Mean':CV_score,'Std':std}, index=classifiers) \nprint(\"\\nTEST_PART=\", TEST_PART)\nprint(\"CV_n_folds=\", CV_n_folds)\nprint(new_models_dataframe2)\nprint(\"\\nMin RMSE score: {:.5f}\".format(new_models_dataframe2[\"RMSE\"].min()))\nprint(\"Min CV mean score: {:.5f}\".format(new_models_dataframe2[\"CV Mean\"].min()))\n\n# Log into a file iteration score\nf = open(\"models_training_log.txt\", \"a\")\nprint(\"TEST_PART=\", TEST_PART, file=f)\nprint(\"CV_n_folds=\", CV_n_folds, file=f)\nprint(new_models_dataframe2, file=f)\nprint(\"\\nMin RMSE score: {:.5f}\".format(new_models_dataframe2[\"RMSE\"].min()), file=f)\nprint(\"Min CV mean score: {:.5f}\".format(new_models_dataframe2[\"CV Mean\"].min()), file=f)\nf.close()\n\n","c85d7919":"if CV_n_folds > 2:\n    plt.subplots(figsize=(12,6))\n    sns.boxplot(list(classifiers), score_list)","4ad29943":"# Create empty dictionary to store the mean value calculated from all the scores\nr = {}\nfor name in colnames:\n    r[name] = round(np.mean([ranks[method][name] for method in ranks.keys()]), 2)\nmethods = sorted(ranks.keys())\nranks[\"Mean\"] = r\nmethods.append(\"Mean\")","f7a45e7f":"meanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])\n# Sort the dataframe\nmeanplot = meanplot.sort_values('Mean Ranking', ascending=False)\n# Let's plot the ranking of the features\nsns.factorplot(x=\"Mean Ranking\", y=\"Feature\", data = meanplot, kind=\"bar\", \n               size=14, aspect=1.9, palette='coolwarm')","c61940b0":"todrop = list(meanplot[meanplot[\"Mean Ranking\"]<0.02][\"Feature\"])\nprint (\"Features to drop: {}\".format(len(todrop)))\nprint (todrop)","2faf467a":"plt.figure(figsize=(16, 8))\n# multiple line plot\nplt.plot(y_pred_df.index,'test', data=y_pred_df, marker='o', markerfacecolor='blue', markersize=6, color='skyblue', linewidth=4)\n\ncolors = ['red', 'yellow', 'green', 'black', 'cyan', 'magenta', 'black', 'green', 'black', 'cyan', 'magenta', 'black']\nfor i, name in enumerate(classifiers):\n    plt.plot(y_pred_df.index, name, data=y_pred_df, marker='o', markerfacecolor=colors[i], markersize=3)\n\nplt.legend()","7aa58c6b":"plt.figure(figsize=(16, 8))\n# multiple line plot\nplt.plot(y_pred_df.index,'test', data=y_pred_df, marker='o', markerfacecolor='blue', markersize=6, color='skyblue', linewidth=4)\n# plt.plot(y_pred_df.index, \"lasso\", data=y_pred_df, marker='o', markerfacecolor=\"red\", markersize=3)\nplt.plot(y_pred_df.index, \"Stacked\", data=y_pred_df, marker='o', markerfacecolor=\"green\", markersize=3)\n# plt.plot(y_pred_df.index, \"xgb\", data=y_pred_df, marker='X', markerfacecolor=\"yellow\", markersize=6)\nplt.plot(y_pred_df.index, \"lgbm\", data=y_pred_df, marker='X', markerfacecolor=\"yellow\", markersize=6)\n\nplt.legend()","00edec11":"plt.figure(figsize=(16, 8))\n# multiple line plot\n# plt.plot(y_pred_df.index,'test', data=y_pred_df, marker='o', markerfacecolor='blue', markersize=6, color='skyblue', linewidth=4)\nplt.plot(y_pred_df.index, \"del_Averaged\", data=y_pred_df, marker='o', markerfacecolor=\"red\", markersize=3)\nplt.plot(y_pred_df.index, \"del_Stacked\", data=y_pred_df, marker='o', markerfacecolor=\"green\", markersize=3)\n# plt.plot(y_pred_df.index, \"del_xgb\", data=y_pred_df, marker='X', markerfacecolor=\"yellow\", markersize=6)\nplt.plot(y_pred_df.index, \"del_lgbm\", data=y_pred_df, marker='X', markerfacecolor=\"yellow\", markersize=6)\n# plt.plot(y_pred_df.index, \"del_lasso\", data=y_pred_df, marker='X', markerfacecolor=\"red\", markersize=6)\n\nplt.legend()","2bd19f74":"fail_idx = y_pred_df[abs(y_pred_df[\"del_xgb\"]) > 60000].index\ny_pred_df[abs(y_pred_df[\"del_xgb\"]) >60000]","35874d3c":"df = X.iloc[fail_idx, :]\ndf[['OverallQual', 'MasVnrArea', 'TotalSF', '1stFlrSF', '2ndFlrSF',  \n    'LotArea', 'LotFrontage', 'GrLivArea', \"TotRmsAbvGrd\"]]","f507efd9":"print(y_pred_df[\"del_Stacked\"].mean())\nprint(y_pred_df[\"del_xgb\"].mean())\nprint(y_pred_df[\"del_lgbm\"].mean())","ec8a5daf":"def blend_models_predict(names, coeff, X):\n    pred = 0;\n    for i, name in enumerate(names):\n        if type(models[name]) == StackingAveragedModels:\n            pred += coeff[i] * models[name].predict(X.values)\n        else:\n            pred += coeff[i] * models[name].predict(X)\n              \n    return pred","c67cfe66":"def log_to_file_stackconfig(models, coeff, rmse, rmse_train):\n    f = open(\"models_training_log.txt\", \"a\")\n    print(\"Stacking config:\" +str(models)+\" \"+str(coeff), file=f)\n    print(\"TEST={:0.5f}\".format(rmse) + \" TRAIN={:0.5f}\".format(rmse_train), file=f)\n    f.close()","1061d9d2":"compare_models = [\"Stacked\",\n                  \"Averaged\", \n#                   \"xgb\", \n                  \"lgbm\"\n                 ]\ncoeff = [0.35, 0.35, 0.3]\n\ny_train_pred = blend_models_predict(compare_models, coeff, X_train)\ny_test_pred = blend_models_predict(compare_models, coeff, X_test)\n# Final result convert from log \nY_pred = np.expm1(blend_models_predict(compare_models, coeff, X_submit))\n\nprint(\"[TRAIN]:\")\ntrain_rmse_ = print_rmse_score(y_train, y_train_pred)\nprint(\"[TEST]:\")\ntest_score = print_rmse_score(y_test, y_test_pred)\nlog_to_file_stackconfig(compare_models, coeff, test_score, train_rmse_)\n\ndate = datetime.datetime.now().isoformat()\nsubmit_name = str(int(round(time.time() * 1000)))\nfor i, name in enumerate(compare_models):\n    submit_name += \"_\"+str(coeff[i])+\"_\" + name \nsubmit_name += \"_TEST_\" + str(TEST_PART) \nsubmit_name += \"_rmse_{:0.5f}\".format(test_score) + \".csv\"\nprint(\"=\"*80)\nsubmission = pd.DataFrame({\n        \"Id\": orig_test[\"Id\"],\n        \"SalePrice\": Y_pred\n    })\nsubmission.to_csv(submit_name, index=False)\nprint(submit_name)","29c7baed":"## Load data ","5cf6240d":"# Features engineering","96de1ae2":"## Drop low importance features","0efd8145":"## Model Stacking","1677f3f9":"# -------  Models  ---------\n","ee38f65d":"### Convert str in Quality features to int","3f28be85":"## Remove outliers","08a5c4f6":"# How automate and log your experiments","44384761":"## convert categorical variable into dummy","e5b63590":"### Generate new features","08cd42dc":"## Normalize data","77a3f3e3":"## ElasticNet","1572b94f":"## Import mlflow\n- First of all you should create your new experiment. \n- Call get_tracking_uri return you a path to local folder with meta files\/results of your experiments\n- Call get_artifact_uri return path to your artifacts","77f60e81":"## Separate back train and test set","f687c63d":"## LightGBM","bade189d":"## XGBoost","1b1614d8":"## Lasso","9486695a":"### Imputing missing values","9f841fa5":"## Sales Price ","ba6130d8":"### Data Correlation","b5a4c902":"**Box Cox Transformation of (highly) skewed features**","9614a5a9":"## GradientBoostingRegressor","6ed14151":"Show just selected models","288145e5":"If your code crashed and you didn't finish mlflow you shoud run the following code:","a5317b23":"- We will TAGs for more easy filtering between different models\n- Parameters help us to track training configuration \/ conditions\n- Metrics helps us filter models by score\n- Log_model will save model to the artifacts","a784f8d7":"## Skewed features","a2601518":"# Generate Submit file","f2f300f2":"### Show statistic how much data is missing","18ac57ea":"## Categorial and quantative features investigation","c819e6d5":"# All model comparison owerview\nIn this section compare model perforamnce of singe notebook run.","22cbcd2f":"## KernelRidge","5131b05a":"The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.","f294e6b3":"### Let's try to see the biggest missprediction","29a24d5c":"### Show how real prise from Test set corelated with Predicted price by models","dffa41f6":"## Automated experiment tracking with MLflow\n\nDuring the competition I did many experiments and quit often I submited results with some best score and after thet can't reproduce it. So, I used my personal help tool set:\n- Git repository. After submission I commit code that generate that submission file\n- Local log file to record all usefull information in parallel with printf\n- And MLflow. It help me to compare many different experiments and did aditional analyse on how feature engeeniring impact model perforamnce\n\nOfficial site: https:\/\/mlflow.org\/\n\nInstalation tutorial: https:\/\/www.mlflow.org\/docs\/latest\/tutorials-and-examples\/tutorial.html\n\n- But you can quickly install: **pip install mlflow**\n- After installation go to folder with this notebook and run cmd **mlflow ui** in terminal\n- It will run server at http:\/\/localhost:5000\n- More examples here: https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\n\nTIP: Some times mlflow could fail, so just run **mlflow.end_run()**","3ba5cd08":"## Label Encoding \n\n### some categorical variables that may contain information in their ordering set","817e977c":"### Show delta between real price and predicted","33bcb28e":"## Ridge","60c7e84c":"## Feature importance","2593e66a":"### Some features should be categorial","ef9a4951":"Before start training log into file all features that we will use for training. Some feature droped due to feature importance to improve model performance. So it nice to have. \nWe will use simple local file for this purpose. ","a55bb617":"### Group train and test datasets","1e66039c":"# ====== Train model ======"}}