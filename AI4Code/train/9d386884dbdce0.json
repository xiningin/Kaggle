{"cell_type":{"5ab49fa5":"code","b616fb7c":"code","f06543e9":"code","115d9f1d":"code","5ff38bd5":"code","4c62e43c":"code","630ff9a6":"code","3df03401":"code","9a5c36c3":"code","73538ee6":"code","7f2d291f":"code","4c4ddd89":"code","3a4adf16":"code","49ce43f2":"code","9974f1f4":"code","6f4089bc":"code","841021a8":"code","979eecae":"code","e94a9251":"code","4ede4abe":"code","cce5fe3c":"code","e0e56997":"code","9ca43bfd":"code","ded586fa":"code","5a583f1d":"code","9daf4005":"code","fc927217":"code","e3c9b78f":"code","2e7b45aa":"code","0d0e391b":"code","f2392dea":"code","c6a61de3":"code","7fd1a03b":"code","81c0f64b":"code","987de9cb":"code","550ed527":"code","30daf76a":"code","64d6cb40":"code","d9172d55":"code","8775f9c8":"code","3d450bde":"code","9cd9ceb7":"code","fb7d47d8":"markdown","ee914785":"markdown","689417d0":"markdown","75a6f7d0":"markdown","82b023b0":"markdown","179809ed":"markdown","a1068cfe":"markdown","a9a35897":"markdown","378eae46":"markdown","a8f39b78":"markdown","3a5e3e3a":"markdown","be456fb9":"markdown","88ce59bb":"markdown"},"source":{"5ab49fa5":"## Hi,\n\n## This is my first notebook. Please let me know the changes required for this.","b616fb7c":"# importing required libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom  matplotlib.pyplot import subplot","f06543e9":"%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","115d9f1d":"# Read the dataset\n\ndf = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndf.head()","5ff38bd5":"df.shape","4c62e43c":"df.quality.value_counts()","630ff9a6":"df.isnull().sum()","3df03401":"# check the number of records for all quality wine\n\nsns.countplot(df['quality'])\n#plt.xticks(rotation=90)","9a5c36c3":"# check the relationship between each variables\nplt.figure(figsize=(20,10))\nsns.heatmap(df.corr(), annot=True,cmap='Blues')\nplt.show()","73538ee6":"df.corr()","7f2d291f":"\nsns.barplot(y='fixed acidity', x='quality', data=df)","4c4ddd89":"\nsns.barplot(y='volatile acidity', x='quality', data=df)\n","3a4adf16":"\nsns.barplot(y='citric acid', x='quality', data=df)\n","49ce43f2":"\nsns.barplot(y='residual sugar', x='quality', data=df)\n","9974f1f4":"\nsns.barplot(y='chlorides', x='quality', data=df)\n","6f4089bc":"\nsns.barplot(y='free sulfur dioxide', x='quality', data=df)\n","841021a8":"\nsns.barplot(y='total sulfur dioxide', x='quality', data=df)\n","979eecae":"\nsns.barplot(y='density', x='quality', data=df)\n","e94a9251":"\nsns.barplot(y='pH', x='quality', data=df)\n","4ede4abe":"\nsns.barplot(y='sulphates', x='quality', data=df)","cce5fe3c":"sns.barplot(y='alcohol', x='quality', data=df)","e0e56997":"# making binary classification for the dependent variable Quality\n\n# 0 -> bad\n# 1 -> good\n\nbins = (2, 6.5, 8)\ngroup_names = [0, 1]\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)\ndf['quality'].value_counts()\n\n# or\n\n#df['quality']= df['quality'].replace(df[df['quality']<=6.5]['quality'],'bad')\n#df['quality']= df['quality'].replace(df[df['quality']>6.5]['quality'],'good')","9ca43bfd":"d=df.groupby('quality').mean().reset_index().T\nd=d.reset_index().iloc[1:,:]\nd.columns=['independent_variables','bad','good']\nd","ded586fa":"# check the number of records with bad and good quality wine\n\nsns.countplot(df['quality'])","5a583f1d":"x=df.drop('quality', axis = 1)\ny= df['quality']","9daf4005":"from sklearn.preprocessing import MinMaxScaler","fc927217":"# Normalize using MinMaxScaler to constrain values to between 0 and 1.\n\nscaler = MinMaxScaler(feature_range = (0,1))\n\nscaler.fit_transform(x)\n\nx.head()","e3c9b78f":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)","2e7b45aa":"#fit logistic regression model\n\nfrom sklearn.linear_model import LogisticRegression\nclassifier_log = LogisticRegression()\nmodel = classifier_log.fit(x_train,y_train)\n\ny_pred_log = classifier_log.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_pred_log, y_test)*100)","0d0e391b":"probs = classifier_log.predict_proba(x_test)\n\nfrom sklearn import metrics\nprob_positive = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, prob_positive)\n\nroc_auc = metrics.auc(fpr, tpr)\nprint('Area under the curve:', roc_auc)\n\nplt.title('Receiver Operating characteristics')\n\nplt.plot(fpr,tpr, 'orange', label= 'auc %0.2f'  )\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","f2392dea":"# fit decision tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# doing pruning to avoid overfitting\nclassifier_tree=DecisionTreeClassifier(criterion ='gini', splitter = 'random',\n                         max_leaf_nodes = 10, min_samples_leaf = 5, \n                         max_depth = 6)\nmodel = classifier_tree.fit(x_train, y_train)\n\ny_pred_tree = classifier_tree.predict(x_test)\n\nprint(accuracy_score(y_pred_tree, y_test)*100)","c6a61de3":"probs = classifier_tree.predict_proba(x_test)\n\nfrom sklearn import metrics\nprob_positive = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, prob_positive)\n\nroc_auc = metrics.auc(fpr, tpr)\nprint('Area under the curve:', roc_auc)\n\nplt.title('Receiver Operating characteristics')\n\n#plt.title('receiver operating characteristics')\nplt.plot(fpr, tpr, 'orange', label='AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\n\nplt.plot([0,1],[0,1],color='darkblue', linestyle='--')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","7fd1a03b":"# fit random forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_forest = RandomForestClassifier(n_estimators=100,\n                                           criterion = 'entropy',\n                                           random_state = 0,max_leaf_nodes = 10, min_samples_leaf = 5, \n                         max_depth = 6)\n\nmodel = classifier_forest.fit(x_train,y_train)\n\ny_pred_tree = classifier_forest.predict(x_test)\n\nprint(accuracy_score(y_pred_tree, y_test)*100)","81c0f64b":"probs = classifier_forest.predict_proba(x_test)\n\nfrom sklearn import metrics\nprob_positive = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, prob_positive)\n\nroc_auc = metrics.auc(fpr, tpr)\nprint('Area under the curve:', roc_auc)\n\nplt.title('Receiver Operating characteristics')\n\n#plt.title('receiver operating characteristics')\nplt.plot(fpr, tpr, 'orange', label='AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\n\nplt.plot([0,1],[0,1],color='darkblue', linestyle='--')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","987de9cb":"print(df['quality'].value_counts())\ndf_majority= df[df['quality'] == 0]\ndf_minority = df[df['quality']== 1]\n","550ed527":"import sklearn.utils as ut\n\ndf_minority_upsampled = ut.resample(df_minority, \n                                   replace = True, # sample with replacement\n                                   n_samples = 1382, # to match majority class\n                                   random_state = 1)  # reproducible results","30daf76a":"df_upsampled = pd.concat([df_majority, df_minority_upsampled])\ndf_upsampled.head()","64d6cb40":"df_upsampled.quality.value_counts()","d9172d55":"x_upsampled = df_upsampled.drop('quality', axis = 1)\ny_upsampled = df_upsampled['quality']\n\n\n# Normalize using MinMaxScaler to constrain values to between 0 and 1.\n\nscaler = MinMaxScaler(feature_range = (0,1))\n\nscaler.fit_transform(x_upsampled)","8775f9c8":"x_train, x_test, y_train, y_test = train_test_split(\n                                                    x_upsampled, y_upsampled,\n                                                    test_size=0.2, random_state=0)","3d450bde":"# Random forest\n\nclassifier_forest = RandomForestClassifier(n_estimators=100,\n                                           criterion = 'entropy',\n                                           random_state = 0)\n\nmodel = classifier_forest.fit(x_train,y_train)\n\ny_pred_tree = classifier_forest.predict(x_test)\n\nprint(accuracy_score(y_pred_tree, y_test)*100)","9cd9ceb7":"probs = classifier_forest.predict_proba(x_test)\n\nfrom sklearn import metrics\nprob_positive = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, prob_positive)\n\nroc_auc = metrics.auc(fpr, tpr)\nprint('Area under the curve:', roc_auc)\n\nplt.title('Receiver Operating characteristics')\n\n#plt.title('receiver operating characteristics')\nplt.plot(fpr, tpr, 'orange', label='AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\n\nplt.plot([0,1],[0,1],color='darkblue', linestyle='--')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","fb7d47d8":"## Data Analysis","ee914785":"### Random forest classifier","689417d0":"insight: Data is imbalanced","75a6f7d0":"\"https:\/\/www.kaggle.com\/uciml\/red-wine-quality-cortez-et-al-2009\"\n","82b023b0":"## Data pre processing","179809ed":"### random forest","a1068cfe":"Insight: records with wine quality 5 & 6 are more","a9a35897":"### Decision tree classifier","378eae46":"### Logistic regression","a8f39b78":"# Model building","3a5e3e3a":"Insights:\n- All are numerical fields\n- Quaity is the dependent variable(discrete). All other fields are continuous.\n- Data is free from missing values","be456fb9":"#### From the Details of \"https:\/\/www.kaggle.com\/uciml\/red-wine-quality-cortez-et-al-2009\"\n\nWhat might be an interesting thing to do, is aside from using regression modelling, is to set an arbitrary cutoff for your dependent variable (wine quality) at e.g. 7 or higher getting classified as 'good\/1' and the remainder as 'not good\/0'. This allows you to practice with hyper parameter tuning on e.g. decision tree algorithms looking at the ROC curve and the AUC value. Without doing any kind of feature engineering or overfitting you should be able to get an AUC of .88 (without even using random forest algorithm)\n\nKNIME is a great tool (GUI) that can be used for this.\n1. - File Reader (for csv) to linear correlation node and to interactive histogram for basic EDA.\n2. - File Reader to 'Rule Engine Node' to turn the 10 point scale to dichtome variable (good wine and rest), the code to put in the rule engine is something like this:\n\n$quality$ > 6.5 => \"good\"\nTRUE => \"bad\"\n3. - Rule Engine Node output to input of Column Filter node to filter out your original 10point feature (this prevent leaking)\n4. - Column Filter Node output to input of Partitioning Node (your standard train\/tes split, e.g. 75%\/25%, choose 'random' or 'stratified')\n5. - Partitioning Node train data split output to input of Train data split to input Decision Tree Learner node and\n6. - Partitioning Node test data split output to input Decision Tree predictor Node\n7. - Decision Tree learner Node output to input Decision Tree Node input\n8. - Decision Tree output to input ROC Node.. (here you can evaluate your model base on AUC value)","88ce59bb":"## Upsampling"}}