{"cell_type":{"2cf69100":"code","69ab40b1":"code","77493c60":"code","641cd7e9":"code","da1d4e15":"code","4d8ce41e":"code","5dfa158c":"code","300bc932":"code","a224498c":"code","2216b556":"code","44bc17e6":"code","63f77b9d":"code","bcf8fa63":"code","a7b8b0e4":"code","856810d4":"code","f7135874":"code","ecaf03b5":"code","3e73bbc1":"code","af91fbf1":"code","835305ce":"code","ecf05348":"code","57e2b411":"code","a248e09b":"code","ae43f52a":"code","25a16ee1":"code","b717cbca":"code","33ab7b0d":"code","a310b809":"code","d8e9a9c0":"code","b8ee3f32":"code","6c9d18b2":"code","f4e58026":"code","8c621ccd":"code","65901d0e":"code","1fd71418":"code","8624705f":"code","b9d05bd2":"code","53f0211a":"code","7a35e6bc":"code","db7b8697":"code","9639befc":"code","75f78b5b":"code","364c6a08":"code","37c2129a":"code","f2ee07c8":"code","163e6b24":"code","9bde07cc":"code","6c3a46c5":"code","5d8d218b":"code","32b10936":"code","c47eed63":"code","a76a0ae0":"code","53a0dbf0":"code","d7728837":"code","39a3f996":"code","44736374":"code","e219c152":"code","80a6d2df":"code","ffa1fb63":"code","71abe075":"code","30ba665c":"code","929f8113":"code","16300c91":"code","2e52f218":"code","2f6e7e0a":"code","588d370d":"code","8b253847":"code","a7ba9cd0":"code","f568ea66":"code","0dade878":"code","621a6653":"code","0533b87b":"code","7f15ea43":"code","6bfc3907":"code","2c26b0ee":"code","6b88d0dc":"code","a690e334":"code","9b2203fd":"code","a6b30acc":"code","59d3f680":"code","99de3030":"code","7af3317d":"code","d68701e0":"code","678440fa":"code","fb874edc":"code","2ef844db":"markdown","16c977f0":"markdown","4cf089a0":"markdown","fff2a602":"markdown","34d13c34":"markdown","5d43abe4":"markdown","40ce2a43":"markdown","a3a3f967":"markdown","78e12fb8":"markdown","19b0ac24":"markdown","84248ca3":"markdown","ebd62f11":"markdown","44233309":"markdown","e0b763fa":"markdown","cc3d95de":"markdown","fc31ed37":"markdown","224cacd8":"markdown","8c311ee8":"markdown","39d25df6":"markdown","4b1b5264":"markdown","dba3cdb7":"markdown","4c9845a4":"markdown","04f0b645":"markdown","9de68ea8":"markdown","af6e8cbc":"markdown","f07b041d":"markdown","4946567f":"markdown","b0b1419d":"markdown","27c62c30":"markdown","5cfce853":"markdown","999b90db":"markdown","0e89e389":"markdown","ad9cf13d":"markdown","b578e097":"markdown","7ce47d46":"markdown","261c9967":"markdown","d1042281":"markdown","c5f11e9c":"markdown","73a8122e":"markdown","c8d0ce79":"markdown","14746c9d":"markdown","7174aaa2":"markdown","7c5e86cb":"markdown","615c169a":"markdown","83c7c256":"markdown","5198e98c":"markdown","d6041dd1":"markdown","fd413563":"markdown","f8584def":"markdown","f6949cd4":"markdown","e0c5ec0e":"markdown","1047f0e4":"markdown","72010aa9":"markdown","d04e0ad7":"markdown","4ccbe324":"markdown","6b8b5bd1":"markdown","d2a0ecfe":"markdown","b3585e93":"markdown","5ea26c99":"markdown","77c6d829":"markdown","c18fff8c":"markdown","85f31405":"markdown","b8622963":"markdown"},"source":{"2cf69100":"!pip install scikit-learn==1.0.2","69ab40b1":"import numpy as np\n\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\n\nimport seaborn as sns\n\nfrom sklearn import set_config\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedGroupKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn.utils import resample\n\nfrom sklearn_pandas import DataFrameMapper, gen_features\n\nfrom catboost import CatBoostClassifier\n\nfrom lightgbm import LGBMClassifier\n\nfrom xgboost import XGBClassifier\n\nfrom imblearn.over_sampling import SMOTENC\nfrom imblearn.pipeline import Pipeline\n\nfrom IPython.display import HTML, display\n\nfrom typing import Tuple\n\nimport plotly.express as px\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\nsns.set_theme()","77493c60":"PALETTE = sns.color_palette(\"Set2\")\nRANDOM_STATE = 42","641cd7e9":"sns.set_context(\"paper\", rc={\"font.size\":12, \n                             \"figure.titlesize\":18, \n                             \"axes.titlesize\":15, \n                             \"axes.labelsize\":13, \n                             \"xtick.labelsize\": 13,\n                             \"ytick.labelsize\": 13,\n                             \"legend.fontsize\": 9,\n                             \"legend.title_fontsize\": 11}) ","da1d4e15":"set_config(display='diagram')","4d8ce41e":"data = pd.read_csv('..\/input\/parkinsons-disease-speech-signal-features\/pd_speech_features.csv')","5dfa158c":"data.info()","300bc932":"data.head()","a224498c":"sizes = dict(data['class'].value_counts())\n\nplt.figure(figsize=(12, 8))\nplt.title(\"Does the person has Parkinson's Disease\")\nplt.pie(sizes.values(), labels=['Yes', 'No'], autopct=\"%.1f%%\", pctdistance=0.85, colors=PALETTE)\n\nplt.show()","2216b556":"sns.heatmap(pd.crosstab(data['class'], data['gender']).divide(3).astype('int64'), \n            yticklabels=['No PD', 'PD'],\n            xticklabels=['Female', 'Male'],\n            annot=True,\n            fmt='d')\nplt.title('Number of males and females in each class')\nplt.show()","44bc17e6":"X = data.drop(columns='class')\ny = data['class']","63f77b9d":"corr_matr = X.drop(columns=['id', 'gender']).corr(method='pearson')\nplt.figure(figsize=(10,10))\nsns.heatmap(corr_matr, cmap='coolwarm', square=True)\nplt.title(\"Pearson's correlation heatmap on raw dataset\")\nplt.show()","bcf8fa63":"g = sns.pairplot(data=X.iloc[:, 2:23], \n                 kind='scatter')\n\nplt.tight_layout()","a7b8b0e4":"scaler = gen_features(\n    columns = [[c] for c in X.columns.values if c not in ['gender', 'id']],\n    classes=[{'class': QuantileTransformer, 'output_distribution': 'normal'}]\n)","856810d4":"scaling_mapper = DataFrameMapper(scaler, default=None, df_out=True)\nX_scaled = scaling_mapper.fit_transform(X)","f7135874":"g = sns.pairplot(data=X_scaled.iloc[:, 2:23], \n                 kind='scatter')\n\nplt.tight_layout()","ecaf03b5":"def cross_validate(estimator, \n                   X: pd.DataFrame, \n                   y: pd.Series, \n                   print_fold_scores=False, \n                   plot_cm=False, \n                   scaling=False,\n                   upsampling=False, \n                   resampling=False, \n                   pca=False) -> pd.DataFrame:\n    \"\"\"Calculates estimators's cross-validation scores on (X, y) dataset \n    \n    Parameters\n    ----------\n    estimator : estimator to evaluate\n    X : Data set to cross-validate on\n    y : Data set target labels\n    print_fold_scores : Set to True to print scores for each fold in cv\n    plot_cm : Set to True to plot cofusion matrix\n    scaling : DataFrameMapper with a scaler in it, used to scale X_train and X_test.\n              If False - scaling is not used\n    upsampling : Set to True to upsample train data in each fold\n    resampling : Set to True to resample train data in each fold\n    pca : Used as n_components parameter in PCA. \n          If False - pca is not used \n    \n    Returns\n    -------\n    mean_cv_scores_df : DataFrame with mean cross validation scores for estimator\n    \"\"\"\n    # defining scores to evaluate\n    cv_scores = {'Accuracy': [],\n                 'Recall': [],\n                 'Precision': [],\n                 'F1': []}\n    \n    estimator_name = type(estimator).__name__\n    \n    # Stratify by target and group by id in order to prevent getting records \n    # of one person in train and test set\n    fold = StratifiedGroupKFold(5, shuffle=True, random_state=RANDOM_STATE)\n    \n    for train_index, test_index in fold.split(X, y, groups=X['id']):\n        X_train, X_test = X.iloc[train_index].drop(columns='id'), X.iloc[test_index].drop(columns='id')\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n        # transformations before training\n        if scaling:\n            scaling.fit(X_train)\n            X_train = scaling.transform(X_train)\n            X_test = scaling.transform(X_test)\n        if upsampling:\n            X_train, y_train = upsample(X_train, y_train)\n        if resampling:\n            X_train, y_train = resample_gender(X_train, y_train)\n        if pca:\n            X_train, X_test = perform_pca(X_train, X_test, explained_variance=pca)\n\n        estimator.fit(X_train, y_train)\n\n        predictions = estimator.predict(X_test)\n        \n        cv_scores['Accuracy'].append(accuracy_score(y_test, predictions))\n        cv_scores['Recall'].append(recall_score(y_test, predictions, pos_label=1))\n        cv_scores['Precision'].append(precision_score(y_test, predictions, pos_label=1))\n        cv_scores['F1'].append(f1_score(y_test, predictions, average='binary'))\n    \n    # prints scores for each fold if True\n    if print_fold_scores:\n        for item in cv_scores.items():\n            print(item)\n    \n    mean_cv_scores = {k: np.mean(v) for k, v in cv_scores.items()}\n    mean_cv_scores_df = pd.DataFrame.from_dict(data={estimator_name: mean_cv_scores.values()}, \n                                               orient='index', \n                                               columns=mean_cv_scores.keys())\n    \n    if plot_cm:\n        plot_confusion_matrix(y_test, predictions, estimator_name)\n        \n    return mean_cv_scores_df\n\ndef plot_confusion_matrix(y_true: pd.Series, \n                          y_pred: pd.Series, \n                          estimator_name: str):\n    \"\"\"Plots confusion matrix for the last fold\n    \n    Parameters\n    ----------\n    y_true : True class labels\n    y_pred : Predicted class labels\n    \"\"\"\n    conf = confusion_matrix(y_true, y_pred)\n    sns.heatmap(conf,         \n                yticklabels=['No PD', 'PD'],\n                xticklabels=['No PD', 'PD'],\n                annot=True,\n                fmt='d')\n    plt.title(f'{estimator_name} confusion matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()\n    \ndef upsample(X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n    \"\"\"Upsamples dataset with SOMTENC \n    \n    Parameters\n    ----------\n    X : Data set to uspsample\n    y : Data set class labels\n    \n    Returns\n    -------\n    X_upsampled : upsampled dataset\n    y_upsampled : upsampled dataset class labels\n    \"\"\"\n    smotenc = SMOTENC(categorical_features=[X.columns.get_loc(\"gender\")], \n                      random_state=RANDOM_STATE, \n                      sampling_strategy=1)\n    \n    X_upsampled, y_upsampled = smotenc.fit_resample(X, y)\n    \n    return X_upsampled, y_upsampled\n    \ndef resample_gender(X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n    \"\"\"Resamples gender proportions in each class\n    \n    Parameters\n    ----------\n    X : Data set to resample\n    y : Data set class labels\n    \n    Returns\n    -------\n    X_resampled : resampled dataset\n    y_resampled : resampled dataset class labels\n    \"\"\"\n    \n    X_full = X.copy()\n    X_full['class_'] = y\n\n    # resampling gender proportions in 1 class\n    df_majority_1 = X_full.query('class_ == 1 and gender == 1')\n    df_minority_1 = X_full.query('class_ == 1 and gender == 0')\n\n    df_minority_resampled_1 = resample(df_minority_1, \n                                       replace=True,\n                                       n_samples=len(df_majority_1),\n                                       random_state=RANDOM_STATE)\n\n    df_resampled_1 = pd.concat([df_majority_1, df_minority_resampled_1])\n\n    # resampling gender proportions in 0 class\n    df_majority_0 = X_full.query('class_ == 0 and gender == 0')\n    df_minority_0 = X_full.query('class_ == 0 and gender == 1')\n\n    df_minority_resampled_0 = resample(df_minority_0, \n                                       replace=True,\n                                       n_samples=len(df_majority_0),\n                                       random_state=RANDOM_STATE)\n\n    df_resampled_0 = pd.concat([df_majority_0, df_minority_resampled_0])\n\n    # Combining two resampled subsets\n    df_resampled = pd.concat([df_resampled_1, df_resampled_0], ignore_index=True)\n\n    X_resampled = df_resampled.drop(columns='class_')\n    y_resampled = df_resampled['class_']\n    \n    return X_resampled, y_resampled\n\ndef perform_pca(X_train, X_test, explained_variance) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Performs PCA on X_train and X_test\n    \n    Parameters\n    ----------\n    X_train : Train data set to fit PCA\n    X_test : Test data set to tranform with PCA  \n    \n    Returns\n    -------\n    X_train_pca : PCA-transformed train data set \n    y_test_pca : PCA-transformed test data set\n    \"\"\"\n    pca = PCA(n_components=explained_variance).fit(X_train.drop(columns='gender'))\n    pca_train_data = pca.transform(X_train.drop(columns='gender'))\n    pca_test_data = pca.transform(X_test.drop(columns='gender'))\n    \n    X_train_pca = pd.DataFrame.from_records(data=pca_train_data)\n    \n    #reset index to map id and gender to pca data (train)\n    X_train.reset_index(inplace=True)\n    X_train_pca['gender'] = X_train['gender']\n    \n    X_test_pca = pd.DataFrame.from_records(data=pca_test_data)\n    \n    #reset index to map id and gender to pca data (test)\n    X_test.reset_index(inplace=True)\n    X_test_pca['gender'] = X_test['gender']\n    \n    return X_train_pca, X_test_pca\n\ndef display_side_by_side(dfs: list, titles: list):\n    \"\"\"Displays dataframes side by side\n    \n    Parameters\n    ----------\n    dfs : list of pandas.DataFrame\n    titles : list of dataframe titles\n    \"\"\"\n    output = \"\"\n    combined = dict(zip(titles, dfs))\n    for title, df in combined.items():\n        output += df.style.set_table_attributes(\"style='display:inline'\").set_caption(title)._repr_html_()\n        output += \"\\xa0\\xa0\\xa0\"\n    display(HTML(output))","3e73bbc1":"models_results = cross_validate(KNeighborsClassifier(n_jobs=-1), X, y, scaling=scaling_mapper, plot_cm=True)","af91fbf1":"models_results","835305ce":"lg_cv = cross_validate(LogisticRegression(random_state=RANDOM_STATE, n_jobs=-1), X, y, scaling=scaling_mapper, plot_cm=True)","ecf05348":"lg_cv","57e2b411":"models_results = models_results.append(lg_cv)","a248e09b":"dt_cv = cross_validate(DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE), X, y, plot_cm=True)","ae43f52a":"dt_cv","25a16ee1":"models_results = models_results.append(dt_cv)","b717cbca":"rf_cv = cross_validate(RandomForestClassifier(max_depth=6, random_state=RANDOM_STATE, n_jobs=-1), X, y, plot_cm=True)","33ab7b0d":"rf_cv","a310b809":"models_results = models_results.append(rf_cv)","d8e9a9c0":"catboost_cv = cross_validate(CatBoostClassifier(depth=6, cat_features=['gender'], verbose=False, random_seed=RANDOM_STATE), X, y, plot_cm=True)","b8ee3f32":"catboost_cv","6c9d18b2":"models_results = models_results.append(catboost_cv)","f4e58026":"lgbm_cv = cross_validate(LGBMClassifier(max_depth=6, random_state=RANDOM_STATE), X, y, plot_cm=True)","8c621ccd":"lgbm_cv","65901d0e":"models_results = models_results.append(lgbm_cv)","1fd71418":"xgb_cv = cross_validate(XGBClassifier(max_depth=6, verbosity=0, random_state=RANDOM_STATE), X, y, plot_cm=True)","8624705f":"xgb_cv","b9d05bd2":"models_results = models_results.append(xgb_cv)","53f0211a":"models_results.sort_index(inplace=True)","7a35e6bc":"models_results","db7b8697":"setting = {\n    'upsampling': True,\n    'X': X,\n    'y': y\n}\n\nmodels = [\n    dict({'estimator': KNeighborsClassifier(n_jobs=-1),\n          'scaling': scaling_mapper}, \n         **setting),\n    dict({'estimator': LogisticRegression(random_state=RANDOM_STATE, n_jobs=-1),\n          'scaling': scaling_mapper}, \n         **setting),\n    dict({'estimator': DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE)}, \n         **setting),\n    dict({'estimator': RandomForestClassifier(max_depth=6, random_state=RANDOM_STATE, n_jobs=-1)}, \n         **setting),\n    dict({'estimator': CatBoostClassifier(depth=6, cat_features=['gender'], verbose=False, random_seed=RANDOM_STATE)}, \n         **setting),\n    dict({'estimator': LGBMClassifier(max_depth=6, random_state=RANDOM_STATE)}, \n         **setting),\n    dict({'estimator': XGBClassifier(max_depth=6, verbosity=0, random_state=RANDOM_STATE)}, \n         **setting)\n]","9639befc":"models_results_upsampling = pd.DataFrame()\nfor model in models:\n    models_results_upsampling = models_results_upsampling.append(cross_validate(**model))\n    \nmodels_results_upsampling.sort_index(inplace=True)","75f78b5b":"display_side_by_side([models_results, models_results_upsampling], \n                     titles=['original data cv scores', 'upsampled data cv scores'])","364c6a08":"X_resampled, y_resampled = resample_gender(X, y)\n\nsns.heatmap(pd.crosstab(y_resampled, X_resampled['gender']).divide(3).astype('int64'), \n            yticklabels=['No PD', 'PD'],\n            xticklabels=['Female', 'Male'],\n            annot=True,\n            fmt='d')\n\nplt.title('Number of males and females in each class after resampling')\nplt.show()","37c2129a":"del X_resampled, y_resampled","f2ee07c8":"setting = {\n    'resampling': True,\n    'X': X,\n    'y': y\n}\n\nmodels = [\n    dict({'estimator': KNeighborsClassifier(n_jobs=-1),\n          'scaling': scaling_mapper}, \n         **setting),\n    dict({'estimator': LogisticRegression(random_state=RANDOM_STATE, n_jobs=-1),\n          'scaling': scaling_mapper}, \n         **setting),\n    dict({'estimator': DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE)}, \n         **setting),\n    dict({'estimator': RandomForestClassifier(max_depth=6, random_state=RANDOM_STATE, n_jobs=-1)}, \n         **setting),\n    dict({'estimator': CatBoostClassifier(depth=6, cat_features=['gender'], verbose=False, random_seed=RANDOM_STATE)}, \n         **setting),\n    dict({'estimator': LGBMClassifier(max_depth=6, random_state=RANDOM_STATE)}, \n         **setting),\n    dict({'estimator': XGBClassifier(max_depth=6, verbosity=0, random_state=RANDOM_STATE)}, \n         **setting)\n]","163e6b24":"models_results_resampling = pd.DataFrame()\nfor model in models:\n    models_results_resampling = models_results_resampling.append(cross_validate(**model))\n    \nmodels_results_resampling.sort_index(inplace=True)","9bde07cc":"display_side_by_side([models_results, models_results_resampling], \n                     titles=['original data cv scores', 'resampled data cv scores'])","6c3a46c5":"display_side_by_side([models_results, models_results_upsampling, models_results_resampling], \n                     titles=['original data cv scores', 'upsampled data cv scores', 'resampled data cv scores'])","5d8d218b":"pca_data = PCA(n_components=3).fit_transform(X_scaled.drop(columns=['id', 'gender']))\nplot_df = pd.DataFrame.from_records(data=pca_data,columns=['pc1','pc2', 'pc3'])\nplot_df['target'] = y\nfig = px.scatter_3d(plot_df, x='pc1', y='pc2', z='pc3', color='target', width=800, height=800)\nfig.show()\n","32b10936":"EXPLAINED_VARIANCE = 0.99\n\npca = PCA(n_components=EXPLAINED_VARIANCE).fit(X_scaled.drop(columns=['id', 'gender']))","c47eed63":"plt.figure(figsize=(15, 10))\n\nplt.bar(range(len(pca.explained_variance_)), pca.explained_variance_ratio_, align='center',\n        label='Component explained variance ratio', edgecolor = \"none\")\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal component')\nplt.title('Explained variance ratio for each principal component')\nplt.legend()\nplt.tight_layout()\n","a76a0ae0":"n_components = len(pca.explained_variance_ratio_)\n\nfig, ax = plt.subplots(figsize=(24, 8))\nx_ticks = np.arange(1, n_components + 1, step=1)\ny_values = np.cumsum(pca.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(x_ticks, y_values, marker='.', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, n_components + 1, step=10))\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=EXPLAINED_VARIANCE, color='r', linestyle='-')\n\nplt.axvline(x=n_components, color='r', linestyle='--')\n\nplt.text(0.5, 1.01, f'{EXPLAINED_VARIANCE*100}% threshold', color = 'red')\nplt.text(n_components + 1, 0.1, f'{n_components}', color = 'red')\n\nax.grid(axis='x')\nplt.xticks(rotation=0)\nplt.show()","53a0dbf0":"pca.explained_variance_ratio_[:150].sum()","d7728837":"train_pca, test_pca = perform_pca(X_scaled.drop(columns=['id'])[:600], X_scaled.drop(columns=['id'])[600:], 150)","39a3f996":"test_pca","44736374":"del train_pca, test_pca","e219c152":"setting = {\n    'pca': 150,\n    'X': X,\n    'y': y\n}\n\nmodels = [\n    dict({'estimator': KNeighborsClassifier(n_jobs=-1),\n          'scaling': scaling_mapper}, \n         **setting),\n    dict({'estimator': LogisticRegression(random_state=RANDOM_STATE, n_jobs=-1),\n          'scaling': scaling_mapper}, \n         **setting),\n    dict({'estimator': DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE)}, \n         **setting),\n    dict({'estimator': RandomForestClassifier(max_depth=7, random_state=RANDOM_STATE, n_jobs=-1)}, \n         **setting),\n    dict({'estimator': CatBoostClassifier(depth=6, verbose=False, random_seed=RANDOM_STATE)}, \n         **setting),\n    dict({'estimator': LGBMClassifier(max_depth=6, random_state=RANDOM_STATE)}, \n         **setting),\n    dict({'estimator': XGBClassifier(max_depth=6, verbosity=0, random_state=RANDOM_STATE)}, \n         **setting)\n]","80a6d2df":"models_results_pca = pd.DataFrame()\nfor model in models:\n    models_results_pca = models_results_pca.append(cross_validate(**model))\n    \nmodels_results_pca.sort_index(inplace=True)","ffa1fb63":"display_side_by_side([models_results, models_results_pca], \n                     titles=['original data cv scores', 'pca data cv scores'])","71abe075":"class CustomResamplingTransformer():\n    \n    def fit_resample(self, X, y):\n        return resample_gender(X, y)","30ba665c":"pipeline = Pipeline([\n    ('resample', CustomResamplingTransformer()),\n    ('estimator', LGBMClassifier(random_state=RANDOM_STATE))\n])","929f8113":"params_1 = {\n    'estimator__num_leaves':[10, 20, 30, 40, 60, 80, 100],\n    'estimator__n_estimators': [200, 250, 300, 350],\n    'estimator__max_depth':[-1, 4, 6, 8, 10, 15]}","16300c91":"gs_1 = GridSearchCV(pipeline,\n                    param_grid=params_1,\n                    cv=StratifiedGroupKFold(5, shuffle=True, random_state=RANDOM_STATE).split(X, y, groups=X['id']),\n                    scoring='f1',\n                    n_jobs=-1)","2e52f218":"gs_1.fit(X.drop(columns='id'), y)","2f6e7e0a":"gs_1.best_score_","588d370d":"gs_1.best_params_","8b253847":"def extract_estimator_params(gs_params: dict) -> dict:\n    '''Extracts estimator params from GridSearchCV best_params_\n    \n    Parameters\n    ----------\n    gs_params : GridSearchCV best_params_ attribute\n    \n    Returns\n    -------\n    estimator_params : dict, containing estimator's params\n    '''\n    estimator_params = {k.split('__')[-1]: v for k, v in gs_params.items()}\n    return estimator_params","a7ba9cd0":"estimator_1_params = extract_estimator_params(gs_1.best_params_)","f568ea66":"estimator_1_params","0dade878":"cross_validate(LGBMClassifier(random_state=RANDOM_STATE, **estimator_1_params), X, y, resampling=True, print_fold_scores=True, plot_cm=True)","621a6653":"params_2 = {\n    'estimator__num_leaves':[16, 18, 20, 22, 24],\n    'estimator__n_estimators': [330, 340, 350, 360, 370],\n    'estimator__max_depth':[3, 4, 5]}","0533b87b":"gs_2 = GridSearchCV(pipeline,\n                    param_grid=params_2,\n                    cv=StratifiedGroupKFold(5, shuffle=True, random_state=RANDOM_STATE).split(X, y, groups=X['id']),\n                    scoring='f1',\n                    n_jobs=-1)","7f15ea43":"gs_2.fit(X.drop(columns='id'), y)","6bfc3907":"gs_2.best_score_","2c26b0ee":"gs_2.best_params_","6b88d0dc":"estimator_2_params = extract_estimator_params(gs_2.best_params_)","a690e334":"cross_validate(LGBMClassifier(random_state=RANDOM_STATE, **estimator_2_params), X, y, print_fold_scores=True, plot_cm=True, resampling=True)","9b2203fd":"params_3 = {\n    'estimator__num_leaves':[20],\n    'estimator__n_estimators': [315, 320, 325, 330, 335],\n    'estimator__max_depth':[5]}","a6b30acc":"gs_3 = GridSearchCV(pipeline,\n                    param_grid=params_3,\n                    cv=StratifiedGroupKFold(5, shuffle=True, random_state=RANDOM_STATE).split(X, y, groups=X['id']),\n                    scoring='f1',\n                    n_jobs=-1)","59d3f680":"gs_3.fit(X.drop(columns='id'), y)","99de3030":"gs_3.best_score_","7af3317d":"gs_3.best_params_","d68701e0":"estimator_3_params = extract_estimator_params(gs_3.best_params_)","678440fa":"cross_validate(LGBMClassifier(random_state=RANDOM_STATE, **estimator_3_params), X, y, print_fold_scores=True, plot_cm=True, resampling=True)","fb874edc":"estimator_3_params","2ef844db":"There are 756 rows with a lot of columns - 755. Each person has 3 records, so there are 252 patients overall","16c977f0":"# Import required libraries","4cf089a0":"# Upsampling with SMOTE","fff2a602":"Target feature is unbalanced, like in most medical data, but this time we have 0 class underrepresented (no Parkinsons's Disease)","34d13c34":"# EDA","5d43abe4":"# Feature scaling","40ce2a43":"## KNN","a3a3f967":"## LogReg","78e12fb8":"First two features are `id` and `gender`, we don't need to tranform them","19b0ac24":"If we look on feature distributions, we will see somewhere near to normal skewed distributions and hardly skewed distributions","84248ca3":"__CV scores on original data, upsampled data and resampled data compared:__","ebd62f11":"What has been done in this work:\n- Simple EDA (features are already extracted)\n- Custom cross-validation scheme with stratifying by `class` and grouping by `id` \n- Out-of-box models comparison (LGBMClassifier is the best)\n- Upsampling using SMOTENC (badly affects the scores)\n- Resampling gender proportions in each class (used in the result model) \n- PCA (badly affects the scores)\n- Model tuning (LGBM)","44233309":"# Dimensionality Reduction","e0b763fa":"Let's find the optimal number of components","cc3d95de":"So we have the following model","fc31ed37":"The scores didn't change, so the final params are","224cacd8":"As the result model I would choose LGBMClassifier with resampling.\n- LGBMClassifier provides us with the best recall\/precision tradeoff out-of-box\n- LGBM learning is relatively fast (in comparison with other boostings), so the tuning will not take a lot of time and we can experiment more with that\n- The intuition about resampling gender proportions in each class: training on resampled data will be more robust to test sets, that not look like train set (in terms of class\/gender proportions). And i assume that the model will be used equally on males and females","8c311ee8":"Seems like the scores are a little lower than on raw data.","39d25df6":"# Dataset description","4b1b5264":"We know that there are correlated features in this dataset, so non-robust to multicollinearity models might suffer","dba3cdb7":"LGBMClassifier on resampled data with the parameters:\n- max_depth: 5\n- n_estimators: 320\n- num_leaves: 20","4c9845a4":"I will scale all data, just to show how this scaling works","04f0b645":"I guess gender is important feature, because vocal features of male and female may vary a lot.  \nSo let's look on gender proportions in each class","9de68ea8":"Correlations in the dataset:","af6e8cbc":"Now let's check the scores for diferent models out-of-box","f07b041d":"That's how `perform_pca` method works on our data (just an example to see what's the output of PCA):","4946567f":"Cross-validation in our data set requires stratifying by `class` and also grouping by `id`. Records from one person can be very similar, so to prevent data leakage, we should group person's records","b0b1419d":"# Resampling ","27c62c30":"This is how resampling method works:","5cfce853":"# Model tuning","999b90db":"__The first lap of GridSearch__","0e89e389":"## PCA","ad9cf13d":"## DT","b578e097":"As we see, the data is not very separable even on 3 dimensions.","7ce47d46":"Now let's cross-validate on resampled data, so gender proportion in each class are equal","261c9967":"I will use F1 score in GridSearch, because it takes into account both Recall and Precision for 1 class.  \nI do not use first class Recall for tuning, because the model will just classify almost all objects as 1 and that is a bad model","d1042281":"LGBMClassifier gives the best Recall - Precision ratio and the best accuracy out-of-box","c5f11e9c":"## CatBoost","73a8122e":"## LGBMClassifier","c8d0ce79":"Let's look how the whole data is distributed in 3 dimensions (using PCA)","14746c9d":"We have:\n- 41 Females and 23 Males without PD\n- 81 Females and 107 Males with PD\n\nMales are underrepresented in No PD group\n\nFemales are underrepresented in PD group","7174aaa2":"These are the first 20 features, but i checked features in each attribute type (see attribute descriptin) and the distributions are pretty similar","7c5e86cb":"__Context__\n\nThis dataset is collected from UCI Machine Learning Repository through the following link: [click](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Parkinson%27s+Disease+Classification#)\n\n__Data Set Information:__\n\nThe data used in this study were gathered from 188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 (65.1\u00b110.9) at the Department of Neurology in Cerrahpaya Faculty of Medicine, Istanbul University. The control group consists of 64 healthy individuals (23 men and 41 women) with ages varying between 41 and 82 (61.1\u00b18.9). During the data collection process, the microphone is set to 44.1 KHz and following the physician's examination, the sustained phonation of the vowel \/a\/ was collected from each subject with three repetitions.\n\n__Attribute Information:__\n\nVarious speech signal processing algorithms including Time Frequency Features, Mel Frequency Cepstral Coefficients (MFCCs), Wavelet Transform based Features, Vocal Fold Features and TWQT features have been applied to the speech recordings of Parkinson's Disease (PD) patients to extract clinically useful information for PD assessment. [Related paper](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S1568494618305799?via%3Dihub)\n\nAttribute description:\n\n- Baseline Features: Col3 to Col23\n- Intensity Parameters: Col24 to Col26\n- Formant Frequencies: Col27 to Col30\n- Bandwidth Parameters: Col31 to Col34\n- Vocal Fold: Col35 to Col56\n- MFCC: Col57 to Col140\n- Wavelet Features: Col141 to Col322\n- TQWT Features: Col323 to Col754\n- Class: Col755\n\n[Similar paper](https:\/\/arxiv.org\/ftp\/arxiv\/papers\/1905\/1905.00377.pdf)","615c169a":"__The third lap of GridSearch:__","83c7c256":"From the dataset description, attributes are extracted using:\n\n> Various speech signal processing algorithms including Time Frequency Features, Mel Frequency Cepstral Coefficients (MFCCs), Wavelet Transform based Features, Vocal Fold Features and TWQT features have been applied to the speech recordings of Parkinson's Disease (PD) patients to extract clinically useful information for PD assessment.\n\nWithout diving into the domain area, I cannot say aything about this features and their impact","5198e98c":"## LightGBM","d6041dd1":"## XGBoost","fd413563":"## RF","f8584def":"As expected, recall decreased, precision increased, that is not really what we want","f6949cd4":"__Models comparison:__","e0c5ec0e":"Pairplot of features after scaling:","1047f0e4":"Let's check how PCA affects our models. This time, even trees models are trained on scaled data, because we must scale the data before PCA","72010aa9":"__Spplitting the data__","d04e0ad7":"So the results are not good, PCA negatively affects F1 score and accuracy overall. Some models are definetly overfitted (RFC, CatBoost)","4ccbe324":"As long as I use grouping, resampling and stratifying, I have to write my own wrapper transformer with `fit_resample` method","6b8b5bd1":"Now let's try to cross validate with SMOTE upsampling","d2a0ecfe":"__The second lap of GridSearch (now we specify parameters in smaller limits):__","b3585e93":"Both precision and recall has increased\n\nWe can also see that, for example, the third and the last fold scores differs a lot. And that is happend because of small dataset, i guess ","5ea26c99":"# Cross-validation scheme","77c6d829":"I would choose 150 number of components, that's  5 times less features, but they still explain most of the variance (around 95%)","c18fff8c":"I will use QuantileTransformer for feature scaling. This method transforms the features to follow a uniform or a normal distribution. This transformation tends to spread out the most frequent values. It also reduces the impact of outliers","85f31405":"And the mean cv scores of this model are:\n- Accuracy: 0.858719\n- Recall: 0.959212\n- Precision: 0.865519\n- F1: 0.909701","b8622963":"# Results"}}