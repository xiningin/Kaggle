{"cell_type":{"f153f1a2":"code","d05f2a7a":"code","4c84a2d9":"code","4bda1ee9":"code","c07d5c99":"code","6aecd2bb":"code","e4e39643":"code","bca35890":"code","17c2f730":"code","f3bf86bb":"code","4eed9056":"code","cfb18317":"code","10d22c3f":"code","a579afe0":"code","6dfbf47b":"code","772b600d":"code","fe3260cf":"code","77c2709b":"code","9f7404ad":"code","bc5edad3":"code","d55a09e9":"code","1c66f591":"code","1a438f79":"code","bb16b907":"code","6f1b2124":"code","85b011ac":"code","ee69952e":"code","72e801c9":"code","1d7c805f":"code","0f3e4bb8":"code","630e5db6":"code","5e30bb19":"code","46c98461":"code","8e1b20ca":"code","17949351":"code","cdaa1354":"code","8d20bcec":"code","9c54af76":"code","5d81d09b":"code","baf844f1":"code","528a9b12":"code","bd6b2883":"code","8e41ee11":"code","9eeb5172":"code","2a7c3f9b":"code","8b4f1508":"markdown","146335e6":"markdown","bcf38066":"markdown","9472dfea":"markdown","242e0972":"markdown","4e2578bd":"markdown","667c5a72":"markdown","b30b8d11":"markdown","f12dfd76":"markdown","bc588c28":"markdown","66ef4e37":"markdown","b4991a5d":"markdown","13f99ebf":"markdown","91b07d85":"markdown","c57fcede":"markdown","c901e9f2":"markdown","939e5556":"markdown","2946f7b7":"markdown"},"source":{"f153f1a2":"#Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sn","d05f2a7a":"#data_clean = pd.read_csv(\"drive\/My Drive\/Colab Notebooks\/CpcProyect\/data\/data_cleaned.csv\")\ndata_mining = pd.read_csv('MiningProcess_Flotation_Plant_Database.csv',decimal=\",\",parse_dates=[\"date\"],infer_datetime_format=True).drop_duplicates()\n#data_mining = pd.read_csv('drive\/My Drive\/Colab Notebooks\/CpcProyect\/data\/Copia de MiningProcess_Flotation_Plant_Database.csv')","4c84a2d9":"data_mining.head().transpose() #transpose the data","4bda1ee9":"#change the order of columns (the last column) for do the job more easy\n\ndata_mining = data_mining[['date', '% Iron Feed', '% Silica Feed', 'Starch Flow', 'Amina Flow',\n       'Ore Pulp Flow', 'Ore Pulp pH', 'Ore Pulp Density',\n       'Flotation Column 01 Air Flow', 'Flotation Column 02 Air Flow',\n       'Flotation Column 03 Air Flow', 'Flotation Column 04 Air Flow',\n       'Flotation Column 05 Air Flow', 'Flotation Column 06 Air Flow',\n       'Flotation Column 07 Air Flow', 'Flotation Column 01 Level',\n       'Flotation Column 02 Level', 'Flotation Column 03 Level',\n       'Flotation Column 04 Level', 'Flotation Column 05 Level',\n       'Flotation Column 06 Level', 'Flotation Column 07 Level', \n        '% Silica Concentrate','% Iron Concentrate']]","c07d5c99":"data_mining.shape","6aecd2bb":"#create corrmatrix\nplt.figure(figsize=(30, 25))\ncorr_matrix = data_mining.corr()\n\nsn.heatmap(corr_matrix , annot=True)\nplt.savefig(\"corr_matrix.eps\")\nplt.show()","e4e39643":"df_corr = pd.DataFrame(corr_matrix)\n\n\ncorr_values = np.array(df_corr.iloc[22]) #drop IRON\ncorr_values = corr_values[0:len(corr_values)-1]\n\ncorr_names = np.array(df_corr.columns)\ncorr_names = corr_names[0:len(corr_names)-1] #drop iron\n#corr_names = corr_names + corr_names[len(corr_names)] #add silica \n\nplt.figure(figsize=(55,25))\nplt.barh(corr_names, corr_values, color=\"salmon\")\nplt.title(\"Correlation Values for % Iron Concentrate\", fontsize=45)\nplt.xticks(fontsize=30,rotation=90)\nplt.yticks(fontsize=30)\n#plt.savefig(\"corr_silicia.eps\")\n#plt.savefig(\"corr_silicia.svg\")\nplt.show()","bca35890":"#resampling with mean \n\nnames = ['% Iron Feed', '% Silica Feed', 'Starch Flow', 'Amina Flow',\n       'Ore Pulp Flow', 'Ore Pulp pH', 'Ore Pulp Density',\n       'Flotation Column 01 Air Flow', 'Flotation Column 02 Air Flow',\n       'Flotation Column 03 Air Flow', 'Flotation Column 04 Air Flow',\n       'Flotation Column 05 Air Flow', 'Flotation Column 06 Air Flow',\n       'Flotation Column 07 Air Flow', 'Flotation Column 01 Level',\n       'Flotation Column 02 Level', 'Flotation Column 03 Level',\n       'Flotation Column 04 Level', 'Flotation Column 05 Level',\n       'Flotation Column 06 Level', 'Flotation Column 07 Level',\n       '% Iron Concentrate', '% Silica Concentrate']\n \n \nts = 175\nfor x in range(1,data_mining.shape[1]):\n  data = [] #clear data \n  n = 0 #set counter\n \n  while n*ts <= round(data_mining.shape[0]):\n    sample_mean_value = data_mining.iloc[(ts*n):(ts*(n+1)),x].mean()  #sampling data and save mean of n*ts elements\n    data += [sample_mean_value]   #save sampling into a list\n    data_array = np.array(data)   #list to array\n    data_array= np.transpose(data_array)  #transpose\n    df = pd.DataFrame(data_array)   #array to dataframe\n    n += 1  #counter up \n \n  if x == 1:\n    df_first = df\n  if x == 2:\n    new_df = pd.concat([df_first, df], axis = 1)\n  if x > 2: \n    new_df = pd.concat([new_df, df], axis=1) \n \nnew_df.columns = names","17c2f730":"new_df.to_csv('resampling_data.csv')","f3bf86bb":"#remove output %Silica concentrate for post ML analysis \nnew_df = new_df.drop(labels=[ '% Silica Concentrate'], axis = 1)","4eed9056":"from sklearn.preprocessing import MinMaxScaler","cfb18317":"names = new_df.columns #store the names","10d22c3f":"scaler = MinMaxScaler()\nscaled_df = scaler.fit_transform(new_df)\nscaled_df = pd.DataFrame(scaled_df, columns=names)","a579afe0":"scaled_df","6dfbf47b":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","772b600d":"data_mining = scaled_df  #change the name... \nX = data_mining.drop(['% Iron Concentrate'], axis=1) #drop target \ny = data_mining['% Iron Concentrate']  #select target \n\n#make split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)","fe3260cf":"# Set the parameters by cross-validation and GridSearch\npara_grids = {\n            \"n_estimators\" : [10,50,100,200],\n            \"max_features\" : [\"auto\", \"log2\", \"sqrt\"],\n            \"bootstrap\"    : [True, False]\n        }\n\nscores = ['precision', 'recall']\n\nfor score in scores:\n    print(\"# Tuning hyper-parameters for %s\" % score)\n    print()\n    estimator = RandomForestRegressor()\n    clf = GridSearchCV(estimator, para_grids)\n    clf.fit(X_train, y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(clf.best_params_)\n\n\n","77c2709b":"model = RandomForestRegressor(n_estimators=200, max_features='sqrt', bootstrap = False) #utilizamos los parametros anteriors","9f7404ad":"model.fit(X_train, y_train)","bc5edad3":"model.score(X_test,y_test)","d55a09e9":"#PLOT PREDICT VALUES VS REAL VALUES\npred = model.predict(X_test)\nplt.figure(figsize=(20, 10))\nplt.plot(y_test.values[1:600]) #JUST SOME DATA\nplt.plot(pred[1:600])\nplt.savefig(\"regressionRF.pdf\")\nplt.show()","1c66f591":"from sklearn.svm import SVR","1a438f79":"#WARNING: be careful with this code. Take  A LOT OF TIME in finish!\n#param = { ,\n\n#model = SVR()\n\n#grids = grid_search = GridSearchCV(estimator = model, param_grid = param, \n                    #  cv = 3, n_jobs = 4, verbose = 2)\n\n#grids.fit(X_train, y_train)\n# Tuning hyper-parameters for precision\n\n\n#print(\"Best parameters set found on development set:\")\n#print()\n#print(grid_search.best_params_)\n\n","bb16b907":"regressor = SVR(kernel = 'poly', C = 50, degree=8, coef0=0.5, gamma='auto')\nregressor.fit(X_train, y_train)\nregressor.score(X_test,y_test)","6f1b2124":"pred = model.predict(X_test)\nplt.figure(figsize=(20, 10))\nplt.plot(y_test.values[1:600])\nplt.plot(pred[1:600])\nplt.savefig(\"regressionSVRF.pdf\")\nplt.show()","85b011ac":"#define x's and y \nx = data_mining.drop(labels='% Iron Concentrate', axis=1)\ny = data_mining['% Iron Concentrate']","ee69952e":"# Create train\/test\nfrom sklearn.model_selection import train_test_split\n \nx_train, x_test, y_train, y_test = train_test_split(    \n    x, y, test_size=0.3)","72e801c9":"#Import TensorFlow and keras\n \ntry:\n    %tensorflow_version 2.x\n    COLAB = True\n    print(\"Note: using Google CoLab\")\nexcept:\n    print(\"Note: not using Google CoLab\")\n    COLAB = False\n    \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom scipy.stats import zscore","1d7c805f":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom scipy.stats import zscore","0f3e4bb8":"monitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, \n                        patience=5, verbose=1, mode='auto', \n                        restore_best_weights=True)","630e5db6":" \n#Build a NN\n \nmodel = Sequential()\nmodel.add(Dense(22, input_dim=x.shape[1], activation='relu')) # Hidden 1\nmodel.add(Dense(18, activation='relu')) # Hidden 2\nmodel.add(Dense(1)) # Output\nmodel.compile(loss='mae', optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(x_train,y_train,batch_size = 10,validation_data=(x_test,y_test),\n          callbacks=[monitor],verbose=2,epochs=100)","5e30bb19":"model.evaluate(x_test,y_test)","46c98461":"plt.figure(figsize=(10, 4))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","8e1b20ca":"# summarize history for loss\nplt.figure(figsize=(10, 4))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","17949351":"pred = model.predict(x_test)\nplt.figure(figsize=(20, 10))\nplt.plot(y_test.values[1:600])\nplt.plot(pred[1:600])\nplt.savefig(\"regression.pdf\")\nplt.show()","cdaa1354":"from sklearn import metrics\n \n# Predict\npred = model.predict(x_test)\n \n# Measure MSE error.  \nscore = metrics.mean_squared_error(pred,y_test)\nprint(\"Final score (MSE): {}\".format(score))","8d20bcec":" \nimport numpy as np\n \n# Measure RMSE error.  RMSE is common for regression.\nscore = np.sqrt(metrics.mean_squared_error(pred,y_test))\nprint(\"Final score (RMSE): {}\".format(score))\n\n","9c54af76":"\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n \n# convert series to supervised learning \ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = concat(cols, axis=1)\n\tagg.columns = names\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg\n\n\n\nvalues = new_df.values\n\n# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\n# frame as supervised learning\nreframed = series_to_supervised(scaled, 1, 1)\n\n# drop columns we don't want to predict\nreframed.drop(reframed.columns[(reframed.shape[1]-21):(reframed.shape[1]-1)], axis=1, inplace=True)\n\n# split into train and test sets\nvalues = reframed.values\nn_train_hours = round(reframed.shape[0] * 0.8) #70% of data for training\ntrain = values[:n_train_hours, :]\ntest = values[n_train_hours:, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\n\n# design network\nmodel = Sequential()\nmodel.add(LSTM(5, activation='sigmoid', input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dense(1))\nmodel.compile(loss='mae', optimizer='adam')\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n                        patience=5, verbose=1, mode='auto', \n                        restore_best_weights=True)\n# fit network\nhistory = model.fit(train_X, train_y, epochs=100, batch_size=20, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n\n ","5d81d09b":"# plot history\nplt.figure(figsize=(8, 5))\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend(fontsize=15)\nplt.xlabel('epochs', fontsize = 15)\nplt.ylabel('Loss value', fontsize = 15)\n\n\n\nplt.savefig(\"lossLSTM.svg\")\nplt.show()","baf844f1":"pred = model.predict(test_X)\nplt.figure(figsize=(15, 8))\n\n#subplots\n\nplt.subplot(211)\n\n\nplt.plot(pred[1:int(818\/2),0], label='predict', color='black')\nplt.plot(test_y[:int(818\/2)], label='test', color = 'salmon')\nplt.title('LSTM. Predicted values and test values', fontsize=20)\nplt.ylabel('%Iron Concentrate', fontsize = 15)\nplt.legend(fontsize = 15)\n\n\nplt.subplot(212)\nplt.plot(np.arange(int(820\/2),841),pred[int(820\/2):,0], label='predict', color='black')\nplt.plot(np.arange(int(820\/2),842),test_y[int(818\/2):], label='test', color = 'salmon')\n\n\nplt.xlabel('Data[Hours]', fontsize = 15)\nplt.ylabel('%Iron Concentrate', fontsize = 15)\nplt.legend(fontsize = 15)\n\nplt.savefig(\"regressionLSTM.svg\")\n\nplt.show()","528a9b12":"#error plot \n\npred = model.predict(test_X)\nplt.figure(figsize=(11, 8))\n\nerror = (test_y[:818]-pred[1:819,0]) * 100\n\n#error_percent = error*100\/test_y.max()\n\nplt.plot(range(error.shape[0]) ,error, color = 'red')\n\nplt.title('LSTM. Error predicted values vs test values', fontsize=20)\nplt.xlabel('Data[Hours]', fontsize = 15)\nplt.ylabel('%Error', fontsize = 15)\n#plt.legend(fontsize = 15)\n\nplt.grid(b=True, which='major', color='#999999', linestyle='-', alpha=0.2)\nplt.minorticks_on()\nplt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n\nplt.savefig(\"LSTM_error.pdf\")\nplt.show()","bd6b2883":"error.max()","8e41ee11":"from sklearn import metrics\n\n# Predict\npred = model.predict(test_X)\n\n# Measure MSE error.  \nscore = metrics.mean_squared_error(pred,test_y)\nprint(\"Final score (MSE): {}\".format(score))","9eeb5172":"import numpy as np\n\n# Measure RMSE error.  RMSE is common for regression.\nscore = np.sqrt(metrics.mean_squared_error(pred,test_y))\nprint(\"Final score (RMSE): {}\".format(score))","2a7c3f9b":"#mae\nfrom sklearn.metrics import mean_absolute_error\npred = model.predict(test_X)\nscore = mean_absolute_error(test_y, pred, multioutput='raw_values')\nprint(\"Final score (MAE): {}\".format(score))","8b4f1508":"# First view of the data","146335e6":"# METRICS LSTM","bcf38066":"# Tarea 2 IA\n\n> Mauricio Montanares\n\n> Sebastian Mendoza","9472dfea":"\n\n\n# Test Model and some plots ","242e0972":"# Resampling.\n\n> La fuente del dataset especifica que existen variables medidas cada 1 minuto y otras (silica y concentracion de hierro) que son medidas cada 1 hora aproximadamente.\n\n> Nuestro resampling considera un promedio de las muestras tomadas cada 1 minuto para generar un nuevo dato, este dato es almacenado en nuevo dataset.","4e2578bd":"# Random Forest Regressor","667c5a72":"> SCORE 0.3217","b30b8d11":"# Support Vector Machines","f12dfd76":"# CLASSIC NN","bc588c28":"# MinMaxScaler","66ef4e37":"> Se observa una disminucion pero no es rapida ni constante","b4991a5d":"Este dataset contiene datos de una planta procesadora de minerales. \n\nEn la industria minera es de vital importancia conocer el nivel de impurezas de la pulpa final de la etapa de flotaci\u00f3n. Los m\u00e9todos tradicionales consisten en realizar pruebas qu\u00edmicas en base a muestras de est\u00e1 pulpa al final del proceso de flotaci\u00f3n, no hay m\u00e9todos que permitan una estimaci\u00f3n previa de los niveles de contaminantes.\n\nConocer el % de contaminaci\u00f3n de la pulpa puede ayudar a reducir p\u00e9rdidas en la calidad del producto final y posibles p\u00e9rdidas de dinero.\n\nEn este trabajo se implementan modelos Min Max Scaler, Random Forest Regressor, Support Vector Machines, Classic NN y LSTM Implementation.\n\nSe recalca que este es un problema de regresi\u00f3n, ya que se busca predecir valores continuos, y no clasificar.\n\n","13f99ebf":"> Now data it's normalized","91b07d85":"# LSTM IMPLEMENTATION with mean()\n\n> Al ser un dataset del tipo temporal(dependencia temporal) los datos en t, t+1, dependen de t-n\n\n> Existen arquitecturas especiales para estos tipos de problemas, una de ellas es la LSTM (Long short-term memory)\n\n","c57fcede":"> No observamos ninguna correlacion evidente ademas de la correlacion inversa entre la concentracion de silice y la de hierro. Esto ultimo coincide con la relacion inversamente proporcional de la cual la fuente del dataset hacia mencion.","c901e9f2":"> *We will work with this new dataset!*","939e5556":"> SCORE 0.48","2946f7b7":"> Not so good..."}}