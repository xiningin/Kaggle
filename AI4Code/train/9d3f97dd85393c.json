{"cell_type":{"8de0d31a":"code","cdf08a81":"code","ebe376e8":"code","d648867e":"code","3b52d7e7":"code","d2cc7416":"code","f4dc22dd":"code","0c707d7e":"code","1731905f":"code","30f3e3c0":"code","3ab17664":"code","3d223970":"code","a7e60dc8":"code","2cf0c424":"code","2dd93767":"code","8f6644ff":"code","211386a9":"code","afe8e572":"code","e0bcb523":"code","d928f28c":"code","b893f34e":"code","53ee3dec":"code","53944704":"markdown","208a8037":"markdown","6c3df3dc":"markdown","c89c48b4":"markdown","7c43411e":"markdown","f0aa6d5e":"markdown","62389bf4":"markdown","00ad2d91":"markdown","a49257e7":"markdown","b47a8a64":"markdown","65c36846":"markdown","78bf2111":"markdown","f547dc23":"markdown","26aa840c":"markdown","dc6c5012":"markdown","372b6a9e":"markdown","dc9b618d":"markdown","e2c03088":"markdown"},"source":{"8de0d31a":"from torchvision import datasets","cdf08a81":"from torchvision import transforms\n\ndir(transforms)","ebe376e8":"normalize = transforms.Normalize((0.4915, 0.4823, 0.4468),\n                         (0.2470, 0.2435, 0.2616))","d648867e":"train_transform = transforms.Compose(\n                    [transforms.RandomCrop(32,32),\n                     transforms.ToTensor(),\n                     normalize]\n)","3b52d7e7":"data_path = '.\/'\n\ncifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=train_transform)\ncifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=train_transform)","d2cc7416":"len(cifar10), len(cifar10_val)","f4dc22dd":"img, label = cifar10[30000]\n# img, label","0c707d7e":"import matplotlib.pyplot as plt","1731905f":"def get_cropped_image(index, number):\n    columns = number\n    rows = 1\n    fig = plt.figure(figsize=(32,32))\n    for i in range(1, columns*rows +1):\n        img, label = cifar10[index]\n        fig.add_subplot(rows, columns, i)\n        plt.imshow(img.permute(1,2,0))\n    plt.show() ","30f3e3c0":"get_cropped_image(550, 5)","3ab17664":"get_cropped_image(20345,5)","3d223970":"cifar10.classes, len(cifar10.classes)","a7e60dc8":"import torch.nn as nn\nimport torch.optim as optim","2cf0c424":"# model = nn.Sequential(\n#         nn.Linear(3072, 1024),\n#         nn.Tanh(),\n#         nn.Linear(1024, 512),\n#         nn.Tanh(),\n#         nn.Linear(512, 128),\n#         nn.Tanh(),\n#         nn.Linear(128, 10),\n#         nn.LogSoftmax(dim=1)\n# )\n\n#simpler\nmodel = nn.Sequential(\n    nn.Linear(3072, 512),\n    nn.Tanh(),\n    nn.Linear(512, 10),\n    nn.LogSoftmax(dim=1)\n)","2dd93767":"loss_fn = nn.NLLLoss()\nn_epochs = 30\nlearning_rate = 1e-2\n\noptimizer = optim.SGD(model.parameters(),\n                      lr = learning_rate)","8f6644ff":"import torch","211386a9":"for epoch in range(n_epochs):\n    for img, label in cifar10:\n        print(\"Current label: \", label, end=\"\\r\")\n        out = model(img.view(-1).unsqueeze(0))\n        loss = loss_fn(out, torch.tensor([label]))\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    print(\"Epoch: \", epoch, \"Loss: \", float(loss))","afe8e572":"correct = 0\ntotal = 0\n\nwith torch.no_grad():\n    for imgs, labels in cifar10_val:\n        outputs = model(img.view(-1).unsqueeze(0))\n        _, predicted = torch.max(outputs, dim=1)\n        total += labels.shape[0]\n        correct += int((predicted==labels).sum())\n\nprint(\"Accuracy: \", correct\/total)","e0bcb523":"loss_fn = nn.MSELoss()\nn_epochs = 1\nlearning_rate = 1e-2\noptimizer = optim.SGD(model.parameters(), lr = learning_rate)","d928f28c":"train_loader = torch.utils.data.DataLoader(cifar10, batch_size=10,shuffle=True )","b893f34e":"for epoch in range(n_epochs):\n    for imgs, labels in train_loader:\n        batch_size = imgs.shape[0]\n        outputs = model(imgs.view(batch_size, -1))\n        loss = loss_fn(outputs, labels.type(torch.FloatTensor))\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    print(\"Epoch: \", epoch, \" loss: \", float(loss))","53ee3dec":"val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=10, shuffle=False)\n\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        batch_size = imgs.shape[0]\n        outputs = model(imgs.view(batch_size, -1))\n        _, predicted = torch.max(outputs, dim=1)\n        total += labels.shape[0]\n        correct += int((predicted==labels).sum())\n\nprint(\"Accuracy: \", correct\/total)","53944704":"defining hyperparameters","208a8037":"Random resized crop is definitly as random as it professes out to be. Is it possible to create a classfiier with this? lets find out. We will create a simple classfier that checks between a plane and a bird.","6c3df3dc":"the classes in cifar10.","c89c48b4":"# Q2. Switch Loss function","7c43411e":"We will use MSELoss for testing","f0aa6d5e":"Lets try and implement random crop using transforms. Lets first see what transforms are available.","62389bf4":"you can compose a transformation pipeline using transforms.Compose.","00ad2d91":"lets download the data","a49257e7":"The dataset is zipped, but we can use the cifar10 dataset available in public domain.","b47a8a64":"Checking on validation set","65c36846":"Lets create a simple model.","78bf2111":"we have 50000 images for training and 10000 for validation","f547dc23":"we will create a data loader this time.","26aa840c":"We can also could have used data loaders for creating batches for iterating over in every epoch.","dc6c5012":"lets initialize a normalize function with values given from the book itself.","372b6a9e":"Solving exercises in ch7 Deep Learning with Pytorch book","dc9b618d":"## Q1.  Use torchvision to implement random cropping of the data.","e2c03088":"training"}}