{"cell_type":{"7c8ab92d":"code","c4d146bd":"code","e200dc3a":"code","dd6dceba":"code","9bd1caab":"code","b930e737":"code","43e1eeb7":"code","c52658a5":"code","f4065561":"code","1a505522":"code","53d2958c":"code","de52e4c6":"code","70b8a3aa":"code","910fa69b":"code","faa33205":"code","358db17e":"code","1811df70":"code","d243a3f4":"code","1ab5ac0f":"code","dba92076":"code","b04657c4":"code","f724b56a":"markdown","299747f9":"markdown","88f2ee72":"markdown","296ecd97":"markdown","33fa7642":"markdown","12c4ca14":"markdown","cc154d92":"markdown","175ea3d9":"markdown","cb2a4b32":"markdown","f5ae0d9b":"markdown","533b6ed6":"markdown","e2912481":"markdown","b3c43625":"markdown","2d493a38":"markdown","da560724":"markdown","47c5dc6c":"markdown","6a243a08":"markdown","81ccec2d":"markdown","64ec8276":"markdown"},"source":{"7c8ab92d":"!pip install transformers datasets --upgrade --quiet","c4d146bd":"import math\nimport os\nimport pickle\nimport re\nfrom dataclasses import dataclass\n\nimport datasets\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport torch\nimport torch.nn.functional as F\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (accuracy_score, classification_report,\n                             confusion_matrix, precision_recall_fscore_support)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom tqdm.notebook import tqdm\nfrom transformers import (AdamW, BertForSequenceClassification, BertTokenizer,\n                          DataCollatorWithPadding,\n                          get_linear_schedule_with_warmup)\n\ndatasets.logging.set_verbosity_error()\n","e200dc3a":"def load_data():\n    no_theme = pd.read_csv(\n        '\/kaggle\/input\/portuguese-tweets-for-sentiment-analysis\/NoThemeTweets.csv', \n        index_col=0)\n    # the `type` column will be important in the future to stratify the splits\n    no_theme['type'] = 'no_theme-'\n\n    with_theme = pd.read_csv(\n        '\/kaggle\/input\/portuguese-tweets-for-sentiment-analysis\/TweetsWithTheme.csv', \n        index_col=0)\n    with_theme['type'] = 'with_theme-'\n\n    data = pd.concat([no_theme, with_theme])\n    data['type'] = data['type'] + data['sentiment']\n    # Remove duplicate tweets\n    data = data[~data.index.duplicated(keep='first')]\n    \n    return data\n\ndata = load_data()\ndata","dd6dceba":"data.info()","9bd1caab":"sentiments = data.sentiment.value_counts()\nprint('Class ratio:', sentiments['Positivo']\/sentiments['Negativo'])\nsentiments","b930e737":"def create_splits(data):\n    test_validation_size = int(0.01*data.shape[0])\n    train_validation, test = train_test_split(data, test_size=test_validation_size, random_state=42, stratify=data['type'])\n    train, validation = train_test_split(train_validation, test_size=test_validation_size, random_state=42, stratify=train_validation['type'])\n    return train, validation, test\ntrain, validation, test = create_splits(data)\nprint('Training samples:  ', train.shape[0])\nprint('Validation samples:', validation.shape[0])\nprint('Test samples:      ', test.shape[0])","43e1eeb7":"def build_dataset(tokenizer, splits):\n    train, validation, test = splits\n    # I could create the dataset directly from pandas, but I will save and load from disk so Datasets com cache it\n    # on disk. This is specially useful when you have a very large dataset that does not fit in memory, which is not\n    # the case, but I will leave here this way as a demonstration. \n    train.to_csv('train_split.csv')\n    validation.to_csv('validation_split.csv')\n    test.to_csv('test_split.csv')\n    dataset = datasets.load_dataset('csv', data_files={'train': 'train_split.csv',\n                                                       'validation':'validation_split.csv',\n                                                       'test': 'test_split.csv'})\n    dataset = dataset.map(lambda example: {'unbiased_text': re.sub(r':[\\)\\(]+', '', example['tweet_text'])}, batched=False)\n    dataset = dataset.map(lambda examples: tokenizer(examples['unbiased_text']), batched=True)\n    dataset = dataset.map(lambda example: {'labels': 1 if example['sentiment'] == 'Positivo' else 0}, batched=False)\n    dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n    \n    return dataset","c52658a5":"tokenizer = BertTokenizer.from_pretrained('neuralmind\/bert-base-portuguese-cased')\ndataset = build_dataset(tokenizer, (train, validation, test))","f4065561":"def compute_metrics(preds, labels):\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\ndef send_inputs_to_device(inputs, device):\n    return {key:tensor.to(device) for key, tensor in inputs.items()}","1a505522":"train_loader = torch.utils.data.DataLoader(dataset['train'], batch_size=16, collate_fn=DataCollatorWithPadding(tokenizer))\nvalidation_loader = torch.utils.data.DataLoader(dataset['validation'], batch_size=32, collate_fn=DataCollatorWithPadding(tokenizer))\ntest_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=32, collate_fn=DataCollatorWithPadding(tokenizer))\n","53d2958c":"num_epochs = 1\nnum_warmup_steps = 5000\n\nmodel = BertForSequenceClassification.from_pretrained(\"neuralmind\/bert-base-portuguese-cased\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel.train().to(device)\n\noptimizer = AdamW(model.parameters(), lr=5e-6)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_epochs*len(train_loader))","de52e4c6":"def predict(model, validation_loader, device):\n    with torch.no_grad():\n        model.eval()\n        preds = []\n        labels = []\n        validation_losses = []\n        for inputs in validation_loader:\n            labels.append(inputs['labels'].numpy())\n            \n            inputs = send_inputs_to_device(inputs, device)\n            loss, scores = model(**inputs)[:2]\n            validation_losses.append(loss.cpu().item())\n\n            _, classifications = torch.max(scores, 1)\n            preds.append(classifications.cpu().numpy())\n        model.train()\n    return np.concatenate(preds), np.concatenate(labels)\n        ","70b8a3aa":"epoch_bar = tqdm(range(num_epochs))\nloss_acc = 0\nalpha = 0.95\nfor epoch in epoch_bar:\n    batch_bar = tqdm(enumerate(train_loader), desc=f'Epoch {epoch}', total=len(train_loader))\n    for idx, inputs in batch_bar:\n        inputs = send_inputs_to_device(inputs, device)\n        optimizer.zero_grad()\n        loss, logits = model(**inputs)[:2]\n        \n        loss.backward()\n        optimizer.step()\n        \n        # calculate a simplified ewma to the loss\n        if epoch == 0 and idx == 0:\n            loss_acc = loss.cpu().item()\n        else:\n            loss_acc = loss_acc * alpha + (1-alpha) * loss.cpu().item()\n        \n        batch_bar.set_postfix(loss=loss_acc)\n        \n        if idx%5000 == 0:\n            preds, labels = predict(model, validation_loader, device)\n            metrics = compute_metrics(preds, labels)\n            print(metrics)\n            \n\n        scheduler.step()\n    os.makedirs('\/kaggle\/working\/checkpoints\/epoch'+str(epoch))\n    model.save_pretrained('\/kaggle\/working\/checkpoints\/epoch'+str(epoch))  ","910fa69b":"preds, labels = predict(model, test_loader, device)\nmetrics = compute_metrics(preds, labels)\nprint(metrics)","faa33205":"stemmer = nltk.stem.snowball.PortugueseStemmer()\nanalyzer = TfidfVectorizer().build_analyzer()\n\ndef stemmed_words(doc):\n    return (stemmer.stem(w) for w in analyzer(doc) if w[0]!='@')\n\nvectorizer = TfidfVectorizer(\n    stop_words=nltk.corpus.stopwords.words('portuguese'), \n    analyzer=stemmed_words,\n    min_df=0.0001, \n    max_features=100000, \n    max_df=0.8)\n\nX_train = vectorizer.fit_transform(train['tweet_text'].apply(lambda s: re.sub(r':[\\)\\(]+', '', s)))\nX_validation = vectorizer.transform(validation['tweet_text'].apply(lambda s: re.sub(r':[\\)\\(]+', '', s)))\nX_test = vectorizer.transform(test['tweet_text'].apply(lambda s: re.sub(r':[\\)\\(]+', '', s)))\n\ny_train = (train['sentiment']=='Positivo').astype(int).values\ny_validation = (validation['sentiment']=='Positivo').astype(int).values\ny_test = (test['sentiment']=='Positivo').astype(int).values","358db17e":"' | '.join(vectorizer.get_feature_names()[100:150])","1811df70":"lr = LogisticRegression(random_state=0, class_weight='balanced', max_iter=500, verbose=True)\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_validation)\n\nprint(classification_report(y_validation, y_pred))","d243a3f4":"nb = MultinomialNB()\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_validation)\nprint(classification_report(y_validation, y_pred))","1ab5ac0f":"imdb_dataset = datasets.load_dataset('csv', data_files={'test': '\/kaggle\/input\/imdb-ptbr\/imdb-reviews-pt-br.csv'})\nimdb_dataset = imdb_dataset.map(lambda examples: tokenizer(examples['text_pt']), batched=True)\nimdb_dataset = imdb_dataset.filter(lambda example: len(example['input_ids']) <= 512)\nimdb_dataset = imdb_dataset.map(lambda example: {'labels': 1 if example['sentiment'] == 'pos' else 0}, batched=False)\n\nimdb_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\nimdb_loader = torch.utils.data.DataLoader(imdb_dataset['test'], batch_size=16, collate_fn=DataCollatorWithPadding(tokenizer))","dba92076":"preds, labels = predict(model, imdb_loader, device)\nmetrics = compute_metrics(preds, labels)\nprint(metrics)","b04657c4":"ax = sns.heatmap(confusion_matrix(labels, preds), cmap='Greens_r', annot=True, fmt='d')\n_ = ax.set(xlabel='Predicted', ylabel='Truth', title='Confusion Matrix')","f724b56a":"As expected, our BERT model does much better than these baselines. Now let's evaluated our model on another related dataset","299747f9":"Let's train and evaluate naive bayes","88f2ee72":"# Evaluate our model on the imdb pt-br dataset\n","296ecd97":"First lets define some helper functions","33fa7642":"And finally run the training loop","12c4ca14":"Let's train and evaluate logistic regression","cc154d92":"# Training BERT","175ea3d9":"Hugginface ships its models with its tokenizers. This makes it really simple to use.","cb2a4b32":"Define an evaluation function","f5ae0d9b":"Now we evaluate or model on the holdout test set.","533b6ed6":"# Load Tweets Dataset\n","e2912481":"Now we load our model, define an optimizer, an scheduler for controlling the learning rate, and set everything up for training","b3c43625":"## Prepare data for training","2d493a38":"Thanks for checking up this notebook! \n\n**Cheers!**","da560724":"Now let's create our dataloaders","47c5dc6c":"Let's take a look on some of those features","6a243a08":"# Introduction\n\nThis notebook is very similar (and somehow an improvement) to the work I have done on this other [kaggle notebook](https:\/\/www.kaggle.com\/viniciuscleves\/an-lise-de-sentimento-com-bert). My motivation behind sentiment analysis on these datasets is to apply the trained model on another dataset I have of tweet-like data in portuguese.\n\nTherefore, on this notebook I will:\n\n1. Load positive and negative tweet samples. I will not use the neutral ones, as I believe that treating news tweets as neutral is a really strong assumption. \n2. Train and evaluate a BERT classifier.\n3. Evaluate the model on the IMDB dataset translated to portuguese and compare the results with the ones obtained on my other notebook mentioned before. \n\n### Reasons to read this notebook:\n\n1. Learn to use state-of-the-art BERT model to sentiment analysis. We will use Hugginface's implementation, which makes it very easy to apply these model in practice.\n2. Learn to use the `Datasets` library from Hugginface. It allows you to cache your dataset on disk, so you can seamlessly deal with huge datasets. \n","81ccec2d":"# Conclusion\n\nThe BERT model fits very well on the tweet data. When carrying the model to the imdb dataset the results are modest, though. There are a few explanations for why this happens. First, the length and subject of the messages are essentially different. Second, the tweet data is only distantly supervised, so the labels are not very reliable, which introduces uncertainty to the model. To be very precise, our model is trying to predict whether the autor would put a happy emoji or a sad one. Maybe the model is capable of identifying some sarcasm pattern that degrades the quality of the predictions on the imdb dataset.\n\nTaking the differences between data distribuition on the datasets in consideration, the model is capable of delivering a modest results that might be useful on some applications. The negative predictions are fairly precise and reliable, for example. \n\nIn the future, I would like to evaluate the performance of the model trained on the imdb dataset on the tweet dataset, and whether mixing both datasets together boost performance on both. ","64ec8276":"Just for the sake of stablishing a baseline, we run some bag-of-word models on the same dataset"}}