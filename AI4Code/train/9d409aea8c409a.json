{"cell_type":{"03ee7140":"code","d39a7297":"code","59174554":"code","8b9198f3":"code","2d307cfe":"code","ff6f82d1":"code","af802801":"code","7e6987b2":"code","ba8552e2":"code","100ae799":"code","2c32502a":"code","7f21b9ca":"code","ab9db48f":"code","b24c74b0":"code","dc1234d5":"code","769f89aa":"code","880f92a0":"code","3a3d1153":"code","4c33a070":"code","8ba2b933":"code","04425c0c":"code","79f3f1b3":"code","263890fe":"code","4eaf2b0e":"code","6acc55e1":"markdown","6bed10bc":"markdown","fe314a40":"markdown","22fc229d":"markdown","5888a73a":"markdown","21a7e09e":"markdown","12644385":"markdown","3d66e96a":"markdown","1fa95704":"markdown","a413825e":"markdown","4ae406cd":"markdown","0eab2881":"markdown"},"source":{"03ee7140":"import pandas as pd\nimport numpy as np\nimport random\nimport os\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n\nimport lightgbm as lgb\nimport catboost as ctb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nimport graphviz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')","d39a7297":"TARGET = 'Survived'\n\nN_ESTIMATORS = 1000\nN_SPLITS = 10\nSEED = 2021\nEARLY_STOPPING_ROUNDS = 100\nVERBOSE = 100","59174554":"def set_seed(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \nset_seed(SEED)","8b9198f3":"train_df = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')\ntest_df[TARGET] = pd.read_csv(\"..\/input\/tps-apr-2021-label\/pseudo_label.csv\")[TARGET]\n\nall_df = pd.concat([train_df, test_df]).reset_index(drop=True)","2d307cfe":"# Age fillna with mean age for each class\nall_df['Age'] = all_df['Age'].fillna(all_df['Age'].mean())\n\n# Cabin, fillna with 'X' and take first letter\nall_df['Cabin'] = all_df['Cabin'].fillna('X').map(lambda x: x[0].strip())\n\n# Ticket, fillna with 'X', split string and take first split \nall_df['Ticket'] = all_df['Ticket'].fillna('X').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\n# Fare, fillna with mean value\nfare_map = all_df[['Fare', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\nall_df['Fare'] = all_df['Fare'].fillna(all_df['Pclass'].map(fare_map['Fare']))\nall_df['Fare'] = np.log1p(all_df['Fare'])\n\n# Embarked, fillna with 'X' value\nall_df['Embarked'] = all_df['Embarked'].fillna('X')\n\n# Name, take only surnames\nall_df['Name'] = all_df['Name'].map(lambda x: x.split(',')[0])","ff6f82d1":"label_cols = ['Name', 'Ticket', 'Sex']\nonehot_cols = ['Cabin', 'Embarked']\nnumerical_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']","af802801":"def label_encoder(c):\n    le = LabelEncoder()\n    return le.fit_transform(c)\n\nscaler = StandardScaler()\n\nonehot_encoded_df = pd.get_dummies(all_df[onehot_cols])\nlabel_encoded_df = all_df[label_cols].apply(label_encoder)\nnumerical_df = pd.DataFrame(scaler.fit_transform(all_df[numerical_cols]), columns=numerical_cols)\ntarget_df = all_df[TARGET]\n\nall_df = pd.concat([numerical_df, label_encoded_df, onehot_encoded_df, target_df], axis=1)","7e6987b2":"params = {\n    'metric': 'binary_logloss',\n    'n_estimators': N_ESTIMATORS,\n    'objective': 'binary',\n    'random_state': SEED,\n    'learning_rate': 0.01,\n    'min_child_samples': 150,\n    'reg_alpha': 3e-5,\n    'reg_lambda': 9e-2,\n    'num_leaves': 20,\n    'max_depth': 16,\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n    'subsample_freq': 2,\n    'max_bin': 240,\n}","ba8552e2":"lgb_oof = np.zeros(train_df.shape[0])\nlgb_preds = np.zeros(test_df.shape[0])\nfeature_importances = pd.DataFrame()\n\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(all_df, all_df[TARGET])):\n    print(f\"===== FOLD {fold} =====\")\n    oof_idx = np.array([idx for idx in valid_idx if idx < train_df.shape[0]])\n    preds_idx = np.array([idx for idx in valid_idx if idx >= train_df.shape[0]])\n\n    X_train, y_train = all_df.iloc[train_idx].drop(TARGET, axis=1), all_df.iloc[train_idx][TARGET]\n    X_valid, y_valid = all_df.iloc[oof_idx].drop(TARGET, axis=1), all_df.iloc[oof_idx][TARGET]\n    X_test = all_df.iloc[preds_idx].drop(TARGET, axis=1)\n    \n    pre_model = lgb.LGBMRegressor(**params)\n    pre_model.fit(\n        X_train, y_train,\n        eval_set=[(X_train, y_train),(X_valid, y_valid)],\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n        verbose=VERBOSE\n    )\n\n    params2 = params.copy()\n    params2['learning_rate'] = params['learning_rate'] * 0.1\n    model = lgb.LGBMRegressor(**params2)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_train, y_train),(X_valid, y_valid)],\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n        verbose=VERBOSE,\n        init_model=pre_model\n    )\n    \n    fi_tmp = pd.DataFrame()\n    fi_tmp[\"feature\"] = model.feature_name_\n    fi_tmp[\"importance\"] = model.feature_importances_\n    fi_tmp[\"fold\"] = fold\n    fi_tmp[\"seed\"] = SEED\n    feature_importances = feature_importances.append(fi_tmp)\n    \n    lgb_oof[oof_idx] = model.predict(X_valid)\n    lgb_preds[preds_idx-train_df.shape[0]] = model.predict(X_test)\n    \n    acc_score = accuracy_score(y_valid, np.where(lgb_oof[oof_idx]>0.5, 1, 0))\n    print(f\"===== ACCURACY SCORE {acc_score:.6f} =====\\n\")\n    \nacc_score = accuracy_score(all_df[:train_df.shape[0]][TARGET], np.where(lgb_oof>0.5, 1, 0))\nprint(f\"===== ACCURACY SCORE {acc_score:.6f} =====\")","100ae799":"# just to get ideas to improve\norder = list(feature_importances.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importances, order=order)\nplt.title(\"{} importance\".format(\"LGBMRegressor\"))\nplt.tight_layout()","2c32502a":"params = {\n    'bootstrap_type': 'Poisson',\n#     'bootstrap_type': 'Bayesian',\n\n    'loss_function': 'Logloss',\n    'eval_metric': 'Logloss',\n    'random_seed': SEED,\n    'task_type': 'GPU',\n    'max_depth': 8,\n    'learning_rate': 0.01,\n    'n_estimators': N_ESTIMATORS,\n    'max_bin': 280,\n    'min_data_in_leaf': 64,\n    'l2_leaf_reg': 0.01,\n    'subsample': 0.8\n}","7f21b9ca":"ctb_oof = np.zeros(train_df.shape[0])\nctb_preds = np.zeros(test_df.shape[0])\nfeature_importances = pd.DataFrame()\n\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(all_df, all_df[TARGET])):\n    print(f\"===== FOLD {fold} =====\")\n    oof_idx = np.array([idx for idx in valid_idx if idx < train_df.shape[0]])\n    preds_idx = np.array([idx for idx in valid_idx if idx >= train_df.shape[0]])\n\n    X_train, y_train = all_df.iloc[train_idx].drop(TARGET, axis=1), all_df.iloc[train_idx][TARGET]\n    X_valid, y_valid = all_df.iloc[oof_idx].drop(TARGET, axis=1), all_df.iloc[oof_idx][TARGET]\n    X_test = all_df.iloc[preds_idx].drop(TARGET, axis=1)\n    \n    model = ctb.CatBoostClassifier(**params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_valid, y_valid)],\n              use_best_model=True,\n              early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n              verbose=VERBOSE\n              )\n    \n    fi_tmp = pd.DataFrame()\n    fi_tmp[\"feature\"] = X_test.columns.to_list()\n    fi_tmp[\"importance\"] = model.get_feature_importance()\n    fi_tmp[\"fold\"] = fold\n    fi_tmp[\"seed\"] = SEED\n    feature_importances = feature_importances.append(fi_tmp)\n    \n    ctb_oof[oof_idx] = model.predict(X_valid)\n    ctb_preds[preds_idx-train_df.shape[0]] = model.predict(X_test)\n    \n    acc_score = accuracy_score(y_valid, np.where(ctb_oof[oof_idx]>0.5, 1, 0))\n    print(f\"===== ACCURACY SCORE {acc_score:.6f} =====\\n\")\n    \nacc_score = accuracy_score(all_df[:train_df.shape[0]][TARGET], np.where(ctb_oof>0.5, 1, 0))\nprint(f\"===== ACCURACY SCORE {acc_score:.6f} =====\")","ab9db48f":"# just to get ideas to improve\norder = list(feature_importances.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importances, order=order)\nplt.title(\"{} importance\".format(\"CatBoostClassifier\"))\nplt.tight_layout()","b24c74b0":"# Tuning the DecisionTreeClassifier by the GridSearchCV\nparameters = {\n    'max_depth': np.arange(2, 5, dtype=int),\n    'min_samples_leaf':  np.arange(2, 5, dtype=int)\n}\n\nclassifier = DecisionTreeClassifier(random_state=2021)\n\nmodel = GridSearchCV(\n    estimator=classifier,\n    param_grid=parameters,\n    scoring='accuracy',\n    cv=10,\n    n_jobs=-1)\nmodel.fit(X_train, y_train)\n\nbest_parameters = model.best_params_\nprint(best_parameters)","dc1234d5":"dtm_oof = np.zeros(train_df.shape[0])\ndtm_preds = np.zeros(test_df.shape[0])\nfeature_importances = pd.DataFrame()\n\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(all_df, all_df[TARGET])):\n    print(f\"===== FOLD {fold} =====\")\n    oof_idx = np.array([idx for idx in valid_idx if idx < train_df.shape[0]])\n    preds_idx = np.array([idx for idx in valid_idx if idx >= train_df.shape[0]])\n\n    X_train, y_train = all_df.iloc[train_idx].drop(TARGET, axis=1), all_df.iloc[train_idx][TARGET]\n    X_valid, y_valid = all_df.iloc[oof_idx].drop(TARGET, axis=1), all_df.iloc[oof_idx][TARGET]\n    X_test = all_df.iloc[preds_idx].drop(TARGET, axis=1)\n    \n    model = DecisionTreeClassifier(\n        max_depth=best_parameters['max_depth'],\n        min_samples_leaf=best_parameters['min_samples_leaf'],\n        random_state=SEED\n    )\n    model.fit(X_train, y_train)\n    \n    dtm_oof[oof_idx] = model.predict(X_valid)\n    dtm_preds[preds_idx-train_df.shape[0]] = model.predict(X_test)\n    \n    acc_score = accuracy_score(y_valid, np.where(dtm_oof[oof_idx]>0.5, 1, 0))\n    print(f\"===== ACCURACY SCORE {acc_score:.6f} =====\\n\")\n    \nacc_score = accuracy_score(all_df[:train_df.shape[0]][TARGET], np.where(dtm_oof>0.5, 1, 0))\nprint(f\"===== ACCURACY SCORE {acc_score:.6f} =====\")","769f89aa":"# plot tree\ndot_data = export_graphviz(\n    model,\n    out_file=None,\n    feature_names=X_train.columns,\n    class_names=['0', '1'],\n    filled=True,\n    rounded=False,\n    special_characters=True,\n    precision=3\n)\ngraph = graphviz.Source(dot_data)\ngraph ","880f92a0":"submission['submit_lgb'] = np.where(lgb_preds>0.5, 1, 0)\nsubmission['submit_ctb'] = np.where(ctb_preds>0.5, 1, 0)\nsubmission['submit_dtm'] = np.where(dtm_preds>0.5, 1, 0)","3a3d1153":"submission[[col for col in submission.columns if col.startswith('submit_')]].sum(axis = 1).value_counts()","4c33a070":"submission[TARGET] = (submission[[col for col in submission.columns if col.startswith('submit_')]].sum(axis=1) >= 2).astype(int)\nsubmission.drop([col for col in submission.columns if col.startswith('submit_')], axis=1, inplace=True)","8ba2b933":"submission['submit_1'] = submission[TARGET].copy()\nsubmission['submit_2'] = pd.read_csv(\"..\/input\/tps-apr-2021-label\/dae.csv\")[TARGET]\nsubmission['submit_3'] = pd.read_csv(\"..\/input\/tps-apr-2021-label\/pseudo_label.csv\")[TARGET]","04425c0c":"submission[[col for col in submission.columns if col.startswith('submit_')]].sum(axis = 1).value_counts()","79f3f1b3":"submission[TARGET] = (submission[[col for col in submission.columns if col.startswith('submit_')]].sum(axis=1) >= 2).astype(int)","263890fe":"submission[['PassengerId', TARGET]].to_csv(\"voting_submission.csv\", index = False)","4eaf2b0e":"submission[TARGET].hist()","6acc55e1":"# Load data","6bed10bc":"# Filling missing values","fe314a40":"# Ensemble\/Submission","22fc229d":"# CatBoost","5888a73a":"# Libraries","21a7e09e":"### Feature importance","12644385":"# DecisionTreeModel","3d66e96a":"# Ensemble","1fa95704":"### Feature importance","a413825e":"# Encoding","4ae406cd":"### Plot tree","0eab2881":"# LightGBM"}}