{"cell_type":{"95c1f6eb":"code","59321767":"code","fe46ef58":"code","e5b39a63":"code","577c3151":"code","a44ad197":"code","87ac136f":"code","3ab81af8":"code","66516a4e":"code","0bd984ee":"code","a2f7f673":"code","ec881071":"code","cd50e691":"code","d1f4b630":"code","f0d5788f":"code","d2a4237c":"code","5eefa4cf":"code","d5e62e84":"code","d43d75dd":"code","8bf65241":"markdown","e6528ec9":"markdown","84287554":"markdown","c26ab566":"markdown","268597c0":"markdown","77db2706":"markdown","ffb5929f":"markdown","693d6f87":"markdown","62dc8d2d":"markdown","d52f561d":"markdown","cb402606":"markdown","b880df5f":"markdown","6b609849":"markdown","1e3dfb2f":"markdown"},"source":{"95c1f6eb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport pickle\nimport math\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_squared_error\n# Global setting\nplt.style.use('seaborn')\nfile = \"\/kaggle\/input\/traffic-counting-using-cameras\/telraam.csv\"","59321767":"# Describe Data\ndef describeData(data):\n    print(\"shape = {}\".format(data.shape))\n    description = data.describe().T\n    description[\"isNull\"] = data.isnull().sum()\n    print(description)\n# root mean squared error or rmse\ndef measure_rmse(actual, predicted):\n\treturn math.sqrt(mean_squared_error(actual, predicted))","fe46ef58":"# Load the data\n##########################################\ndf = pd.read_csv(file, parse_dates = False, header = None,sep=',')\ndf.columns=['time','id','timezone','pct_up','pedestrian','bike','car','lorry','pedestrian_lft','bike_lft','car_lft','lorry_lft','pedestrian_rgt','bike_rgt','car_rgt','lorry_rgt','car_speed_00','car_speed_10','car_speed_20','car_speed_30','car_speed_40','car_speed_50','car_speed_60','car_speed_70']\ndf['time'] = pd.to_datetime(df['time'],errors='coerce',utc=True)  \ndf['time'] = df['time'].fillna(method = 'ffill')\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head(5)\n#describeData(df)","e5b39a63":"# Skip the first row\n########################################### \nminDate = '2020-06-16 00:00:00+00'\ndf = df.loc[df['time']>minDate]\n\n# Reindex the data\n########################################### \ndf.set_index(['time'],drop=False,inplace=True)\nall_freq = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\ndf = df.reindex(all_freq)\ndf['timezone'].fillna('Europe\/Paris',inplace=True)\ndf['time'] = df.index\ndf['localtime']=df.time.map(lambda t: t.tz_convert(tz = 'Europe\/Paris'))\ndf['dayofweek'] = df['localtime'].dt.dayofweek\ndf['day'] = df['localtime'].dt.day\ndf['hour'] = df['localtime'].dt.hour","577c3151":"# Fill missing values\n###########################################\nfor col in ['car','pedestrian','lorry','bike','pedestrian_lft','bike_lft','car_lft','lorry_lft','pedestrian_rgt','bike_rgt','car_rgt','lorry_rgt']:\n    df['adjust_'+col] = df.groupby(['dayofweek','hour'])[col].transform(lambda x: x.fillna(x.median()))\n    df[col].fillna(0,inplace=True)\ndf.fillna(0,inplace=True)\n\n# See adjusted data versus row data\n###########################################\ndf['car'].plot(title='Number of car')\nplt.show()\ndf['adjust_car'].plot(title='Adjusted number of car ')\nplt.show()","a44ad197":"# Estimate the average speed\n###########################################\ndf['speed']=df['car_speed_00']*5+df['car_speed_10']*15+df['car_speed_20']*25+df['car_speed_30']*35+df['car_speed_40']*45+df['car_speed_50']*55+df['car_speed_60']*65+df['car_speed_70']*75\ndf['speed'] = df['speed'] \/ (df['car_speed_00']+df['car_speed_10']+df['car_speed_20']+df['car_speed_30']+df['car_speed_40']+df['car_speed_50']+df['car_speed_60']+df['car_speed_70'])\ndf['speed'].plot(title='Speed')\nplt.show()","87ac136f":"left = pd.pivot_table(df, index=\"hour\", values=['car_lft'],columns=['dayofweek'], aggfunc=np.sum,margins=False,dropna=False)\nright = pd.pivot_table(df, index=\"hour\", values=['car_rgt'],columns=['dayofweek'], aggfunc=np.sum,margins=False,dropna=False)\nleft.columns = ['Monday','Tuesday','Wenesday','Thursday','Friday','Saturday','Sunday']\nright.columns = ['Monday','Tuesday','Wenesday','Thursday','Friday','Saturday','Sunday']\n\nbarWidth = 0.45\n\nfor day in ['Monday','Tuesday','Wenesday','Thursday','Friday','Saturday','Sunday']:\n    r1 = np.arange(len(left[day]))\n    r2 = [x + barWidth for x in r1]\n    plt.bar(r1, left[day], color='#d35400', width=barWidth, edgecolor='white', label='car count left')\n    plt.bar(r2, right['Monday'], color='#2e86c1', width=barWidth, edgecolor='white', label='car count right')\n    plt.xlabel('Hour', fontweight='bold')\n    plt.legend()\n    plt.title(day)\n    plt.show()","3ab81af8":"# max \/ median average speed by hour\n###########################################\nresult = pd.pivot_table(df, index=\"hour\", values=['speed'],columns=['dayofweek'], aggfunc=np.max,margins=False,dropna=False)\nresult.columns = ['Monday','Tuesday','Wenesday','Thursday','Friday','Saturday','Sunday']\nresult.plot(title='{} : max average speed by hour'.format('Speed'))\nplt.show()\n\nresult = pd.pivot_table(df, index=\"hour\", values=['speed'],columns=['dayofweek'], aggfunc=np.median,margins=False,dropna=False)\nresult.columns = ['Monday','Tuesday','Wenesday','Thursday','Friday','Saturday','Sunday']\nresult.plot(title='{} : median average speed by hour'.format('Speed'))\nplt.show()","66516a4e":"# Mean number by hour\n###########################################\nfor col in ['car','pedestrian','lorry','bike']:\n    result = pd.pivot_table(df, index=\"hour\", values=[col],columns=['dayofweek'], aggfunc=np.mean,margins=False,dropna=False)\n    result.columns = ['Monday','Tuesday','Wenesday','Thursday','Friday','Saturday','Sunday']\n    result.plot(title='{} : mean number by hour'.format(col))\n    plt.show()","0bd984ee":"df['car'].plot(title='Number of car')\nplt.show()","a2f7f673":"# Decomposition\n###########################################\n \nd=sm.tsa.seasonal_decompose(df.adjust_car,period=24)\nfigure = d.plot()\nplt.show()","ec881071":"# Select a window of time with clean data \n###########################################\n\nminDate = '2020-06-16 10:00:00+00'\nmaxDate = '2020-07-21 00:00:00+00'\n\n# Select a window of time with clean data \n# AND\n# Split train and test\n###########################################\ntrain = df.adjust_car.loc[df.index<'2020-07-19 00:00:00+00'].copy()\ntest = df.adjust_car.loc[(df.index>='2020-07-19 00:00:00+00')&(df.index<'2020-07-21 00:00:00+00')].copy().iloc[:48]\ntrain_and_test = pd.concat([train,test])\nprint(train.shape)\nprint(test.shape)","cd50e691":"# SARIMA\n###########################################\nmodel = sm.tsa.statespace.SARIMAX(train, trend='n', order=(1,0,0), seasonal_order=(2,1,0,24))\nmodel_sarima = model.fit()\nprint(model_sarima.summary())","d1f4b630":"forecast_sarima = model_sarima.predict(start = test.index[0], end= test.index[-1], dynamic= True) \nplt.plot(test,label=\"true\")\nplt.plot(forecast_sarima,label=\"forecast\")\nplt.legend(loc='upper left')\nplt.title(\"SARIMA - Forecast of number of car\")\nplt.show()\n\n# root mean squared error or rmse\nprint(measure_rmse(test, forecast_sarima))","f0d5788f":"# ExponentialSmoothing\n###########################################\n\nfrom statsmodels.tsa.api import ExponentialSmoothing\nmodel_exp = ExponentialSmoothing(train.values, trend='add', seasonal='add', seasonal_periods=24).fit()","d2a4237c":"forecast_exp = model_exp.forecast(len(test))\nforecast_exp = pd.DataFrame(data=forecast_exp,index=test.index)\nplt.plot(test,label=\"true\")\nplt.plot(test.index,forecast_exp,label=\"forecast\")\nplt.legend(loc='upper left')\nplt.title(\"ExponentialSmoothing - Forecast of number of car\")\nplt.show()\n\n# root mean squared error or rmse\nprint(measure_rmse(test, forecast_exp))","5eefa4cf":"# LSTM\n###########################################\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.layers import Dense, LSTM\nfrom tensorflow.keras.models import Sequential\n\n\n# Window size = number of previous values to predict the next value\nWINDOW_SIZE = 10\n\ntrain_serie = train.values.reshape(-1, 1)\nall_serie = train_and_test.values.reshape(-1, 1)\n\n#  MinMaxScaler\nscaler = MinMaxScaler()\nscaled_all_serie = scaler.fit_transform(all_serie)\nscaled_train_serie = scaler.transform(train_serie)                     \n#scaled_close = scaled_close[~np.isnan(scaled_close)]\n#scaled_close = scaled_close.reshape(-1, 1)\nprint(\"Train shape = {}\".format(scaled_train_serie.shape))\nprint(\"All shape = {}\".format(scaled_all_serie.shape))\nprint(\"nan values ? {}\".format(np.isnan(scaled_train_serie).any()))\n\ndef generateSequence(sequence,backward):\n    X, y = list(), list()\n    for i in range(sequence.shape[0]-backward):\n        seq_x, seq_y = sequence[i:i+backward], sequence[i+backward]\n        X.append(seq_x)\n        y.append(seq_y)\n    X=np.array(X)\n    y=np.array(y)\n    X = X.reshape((X.shape[0], X.shape[1], 1))\n    return X,y\n    \nX,y = generateSequence(scaled_train_serie,WINDOW_SIZE)\nprint(\"X shape = {}\".format(X.shape))\nprint(\"y shape = {}\".format(y.shape))\n\n\nmodel = Sequential()\nmodel.add(LSTM(12, activation='relu', input_shape=(WINDOW_SIZE, 1)))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1))\n\n\n#  Compile\nmodel.compile(\n    loss='mse', \n    optimizer='adam'\n)\n\nBATCH_SIZE = 64\n\n#  Compile\nhistory = model.fit(\n    X, \n    y, \n    epochs=50, \n    batch_size=BATCH_SIZE, \n    shuffle=False,\n    validation_split=0.1\n)\n\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n","d5e62e84":"X,y = generateSequence(scaled_all_serie,WINDOW_SIZE)\ny_predicted = model.predict(X)\ny_inverse = scaler.inverse_transform(y)\ny_predicted_inverse = scaler.inverse_transform(y_predicted)\n\nforecast_lstm = y_predicted_inverse[-48:].ravel()\nplt.plot(test,label=\"true\")\nplt.plot(test.index,forecast_lstm,label=\"forecast\")\nplt.legend(loc='upper left')\nplt.title(\"LSTM - Forecast of number of car\")\nplt.show()\n\n# root mean squared error or rmse\nprint(measure_rmse(test, forecast_lstm))","d43d75dd":"## Comparison\nplt.plot(test,label=\"true\",linestyle='dashed')\nplt.plot(test.index,forecast_lstm,label=\"forecast_lstm\")\nplt.plot(test.index,forecast_exp,label=\"forecast_exp\")\nplt.plot(test.index,forecast_sarima,label=\"forecast_sarima\")\nplt.legend(loc='upper left')\nplt.title(\"Forecast of number of car\")\nplt.show()\n\n# root mean squared error or rmse\nprint(\"root mean squared error of SARIMA model     = {}\".format(measure_rmse(test, forecast_sarima)))\nprint(\"root mean squared error of EXP SMOOTH model = {}\".format(measure_rmse(test, forecast_exp)))\nprint(\"root mean squared error of LSTM model       = {}\".format(measure_rmse(test, forecast_lstm)))","8bf65241":"# Columns description\n\nEach hour the camera records the following data:\n\n- Timestamp : ['time','id','timezone']\n- Percentage of camera activity : ['pct_up']\n- Counting of pedestrians, cars, bicycles, trucks (total, left and right of the street)\n['pedestrian','bike','car','lorry','pedestrian_lft','bike_lft','car_lft','lorry_lft','pedestrian_rgt','bike_rgt','car_rgt','lorry_rgt']\n- Histogram of car speeds for the intervals [0-10 [[10-20 [[20- 30 [..... [70 and more [\n['car_speed_00','car_speed_10','car_speed_20','car_speed_30','car_speed_40','car_speed_50','car_speed_60','car_speed_70']\n\n# Load the data","e6528ec9":"## Filling Missing Values\nWith reindexing, we have to fill NaN values. New columns, adjust_#column#, are created where NaN values are replaced by the mean at the same hour and the same day of week. Once it's done,all NaN are replaced by zero. So if we sum the original columns, the counts are OK.\n\n* original columns : car, bike, ... can be used for counting\n* adjust_#column# can be used for time series analysis.","84287554":"# Introduction\n\nThe data analyzed were collected between the 06\/16\/2020 and the 08\/07\/2020.\n\nThis camera is located place general de gaulle in Mouans Sartoux in France\n\nThis camera is equipped with image recognition to count pedestrians, cars, bicycles, trucks\n\nThis implies periods of inactivity, at night for example.\n\n> &#171; The question is to find what knowledge can be extracted from this data in order to provide the relevant information to users. These users can be pedestrians, drivers but also actors of the municipalities, the town hall, etc... &#187;\n\n![telraam.png](attachment:telraam.png)\n","c26ab566":"## SARIMA\n\nHow to configure a SARIMA(p,d,q)(P,D,Q) ?\n- Plotting ACF, PACF, etc... OR\n- You can use function auto_arima from pmdarima that automatically discover the optimal order for an ARIMA model.\n\nThis function auto_arima is time consuming and the model is big (553Mo), so we've ran it once and kept the result :\n`order=(1,0,0), seasonal_order=(2,1,0,24)`\n```\nimport pmdarima as pm\ngrid_model = pm.auto_arima(train, start_p=1, start_q=1,\n                          test='adf',\n                          max_p=4, max_q=4, m=24,\n                          start_P=0, seasonal=True,\n                          d=0, D=1, trace=True,\n                          error_action='ignore',  \n                          suppress_war*n*ings=True, \n                          stepwise=True)\n```","268597c0":"## ExponentialSmoothing","77db2706":"# Average daily trends\n\n> &#171; The idea is to group measures by hour and weekday in order to have daily trends. &#187;","ffb5929f":"### Utility functions used in this notebook","693d6f87":"## Comparison","62dc8d2d":"### Decomposition with the function seasonal_decompose of statsmodels\n**This is a naive decomposition.**\nIt's just to illustrate the decomposition:  \n* a base level\n* a trend\n* a saisonality\n* a noise","d52f561d":"# Prediction of future flows\n\n> &#171; In view of count observations there is a pattern of daily data.\nThe idea is to generate from a series of counts, for example car counts, two days in advance to see if we can capture this structure.&#187;\n\n> &#171; On the graph below we see that the data is hieratic from 07\/23. There is not enough data to know if this is normal, the activity in August would be very low, or if there is a sensing anomaly. To resolve this point, we would need data for at least one year. &#187;\n\n3 models are tested :\n- SARIMA\n- ExponentialSmoothing\n- LSTM","cb402606":"# Exploratory data analysis\n\n## Estimate the average speed\n\n> &#171; From the data of the histogram of car speeds, we can do a rough estimation of the average speed over the time. &#187;","b880df5f":"# Conclusion\n\n> &#171;These traffic counts generated by a camera equipped with image recognition can indeed generate daily trends and flow predictions.&#187;\n\n> &#171;However the results should be much more reliable if the historical data were sufficient to observe all the seasonality in these series, more than 1 year.&#187;\n\n> &#171;The LTSM algorithm that predicts one or more future values from a window into the past would be more reliable.&#187;\n\n> &#171;And with the help of SARIMA or EXP_SMOO, more data in the future could be generated to capture trends.\nFinally with more data, we could create sub-series that have very similar daily patterns to make targeted predictive models: working Mondays, Sundays and holidays etc.&#187;\n\n**So, we are going to publish more data from this camera in a few month, and try again**","6b609849":"## Reindexing the data\n\nThe camera does not count continuously, there are periods of inactivity.\nSo the data are reindexed in order to have a record every hour.\n\nThere is a big step between the first row and the second (2020-06-11 16:00:00+00:00 to 2020-06-16 10:00:00+00:00), so we skip this first row\n","1e3dfb2f":"## Balance between right and left\n> &#171; We can also be interested in which direction and at what time of the day is the street more busy, by comparing the count of cars on the right and on the left. &#187;"}}