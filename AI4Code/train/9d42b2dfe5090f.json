{"cell_type":{"0bd442e5":"code","2c348a5a":"code","6653f4d8":"code","36b5380f":"code","93bc9ad3":"code","9f54f3df":"code","9bcfdcc7":"code","1d1c4500":"code","d6d3ca4f":"code","c24314b3":"code","8e24efbd":"code","f4d73280":"code","44ecbdab":"code","c8ba918a":"code","53f5777a":"code","7351dab0":"code","1e3e8d9e":"code","125f77a2":"code","314b48aa":"code","61040719":"code","89fe2766":"code","5b9344ac":"code","242bef19":"code","c729efe6":"code","35837368":"code","39672afe":"code","2da0e8d1":"code","f0e03d35":"code","37a57b71":"markdown","1fa7edfa":"markdown","02bcbf4f":"markdown","06512db6":"markdown","60dc7409":"markdown","2432ad7c":"markdown","b22bd926":"markdown","e931261c":"markdown","f262edce":"markdown","dd2b939f":"markdown","30e88ea6":"markdown","1ddc5668":"markdown"},"source":{"0bd442e5":"import numpy as np\nimport pandas as pd\nimport os\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\nfrom fastai import *\nfrom fastai.vision import *","2c348a5a":"import fastai\nprint('The version of FastAI being used is:',fastai.__version__)","6653f4d8":"img = open_image(Path('\/\/kaggle\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/train\/VeryMildDemented\/verymildDem972.jpg'))\nprint(img.shape)\nimg","36b5380f":"PATH = Path('\/kaggle\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/')\n","93bc9ad3":"transform = get_transforms(max_rotate=7.5,\n                           max_zoom=1.15,\n                           max_lighting=0.15,\n                           max_warp=0.15,\n                           p_affine=0.8, p_lighting = 0.8, \n                           xtra_tfms= [\n                               pad(mode='zeros'),\n                               symmetric_warp(magnitude=(-0.1,0.1)),\n                               cutout(n_holes=(1,3), length=(5,5))\n                           ])","9f54f3df":"data = ImageDataBunch.from_folder(PATH, train=\"train\/\",\n                                  test=\"test\/\",\n                                  valid_pct=.4,\n                                  ds_tfms=transform,\n                                  size=112,bs=64, \n                                  ).normalize(imagenet_stats)","9bcfdcc7":"data.show_batch(rows=3, figsize=(10,10))","1d1c4500":"Category.__eq__ = lambda self, that: self.data == that.data\nCategory.__hash__ = lambda self: hash(self.obj)\nCounter(data.train_ds.y)","d6d3ca4f":"import torch.nn as nn\n\nlearn = cnn_learner(data, models.vgg16_bn, metrics=[FBeta(average='weighted'),accuracy], wd=1e-1, callback_fns=ShowGraph)\n\n#learn = cnn_learner(data, models.vgg16_bn,loss_func=catagoricalcrossentropy , metrics=error_rate, wd=1e-1)#,pretrained=False)\nlearn.fit_one_cycle(4)","c24314b3":"Model_Path = Path('\/kaggle\/working\/Alzheimer-stage-classifier-model\/')\nlearn.model_dir = Model_Path\nlearn.save('checkpoint-1')","8e24efbd":"learn.model\nlearn.recorder.plot_losses()","f4d73280":"learn.load('checkpoint-1');","44ecbdab":"learn.unfreeze()\n#learn.fit_one_cycle(8, max_lr=slice(1e-6,3e-4))\nlearn.fit_one_cycle(4, max_lr=slice(1e-6,3e-4))","c8ba918a":"learn.model\nlearn.recorder.plot_losses()","53f5777a":"learn.save('checkpoint-2')","7351dab0":"learn.destroy()","1e3e8d9e":"transform = get_transforms(max_rotate=7.5,\n                           max_zoom=1.15,\n                           max_lighting=0.15,\n                           max_warp=0.15,\n                           p_affine=0.8,\n                           p_lighting = 0.8,\n                           xtra_tfms= [\n                               pad(mode='zeros'),\n                               symmetric_warp(magnitude=(-0.1,0.1)),\n                               cutout(n_holes=(1,6),length=(5,20))])","125f77a2":"data = ImageDataBunch.from_folder(PATH, train=\"train\/\",\n#                                  valid=\"train\/\",\n                                  test=\"test\/\",\n                                  valid_pct=.2,\n                                  ds_tfms=transform,\n                                  size=224,bs=32, \n                                  ).normalize(imagenet_stats)","314b48aa":"learn = cnn_learner(data, models.vgg16_bn, pretrained=False, metrics=[error_rate, FBeta(average='weighted')], wd=1e-1, callback_fns=ShowGraph)\nModel_Path = Path('\/kaggle\/working\/Alzheimer-stage-classifier-model\/')\nlearn.model_dir = Model_Path\nlearn.load('checkpoint-2');","61040719":"learn.lr_find()\nlearn.recorder.plot()","89fe2766":"learn.fit_one_cycle(17, max_lr=5e-4)","5b9344ac":"learn.save('checkpoint-3')","242bef19":"learn.model\nlearn.recorder.plot_losses()","c729efe6":"interp = ClassificationInterpretation.from_learner(learn)\nlosses,idxs = interp.top_losses()\nlen(data.valid_ds)==len(losses)==len(idxs)\ninterp.plot_confusion_matrix(figsize=(8,8))","35837368":"transform = get_transforms()","39672afe":"def random_seed(seed_value, use_cuda):\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    random.seed(seed_value) # Python\n    if use_cuda: \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False\nrandom_seed(42, True)","2da0e8d1":"data_test =  ImageDataBunch.from_folder(PATH,\n                                        #ignore_empty=True,\n                                  train=\"test\/\",#\"train\/\",\n                                  valid=\"test\/\",\n                                  valid_pct=.95,\n#                                  ds_tfms=transform,\n                                  size=224,bs=32,\n                                  num_workers=0\n                                  ).normalize(imagenet_stats)\nev = learn.validate(data_test.train_dl,metrics=[error_rate, FBeta(average='weighted')])\nprint('Results from test set \\tError rate:', float(ev[1]), '\\tF Beta Score: ', float(ev[2]))","f0e03d35":"# The code below is slighty modified from https:\/\/www.kaggle.com\/daisukelab\/verifying-cnn-models-with-cam-and-etc-fast-ai\n# and that code was a hevily modified version of https:\/\/nbviewer.jupyter.org\/github\/fastai\/course-v3\/blob\/master\/nbs\/dl1\/lesson6-pets-more.ipynb\n\nfrom fastai.callbacks.hooks import *\n\ndef visualize_cnn_by_cam(learn, data_index):\n    x, _y = learn.data.valid_ds[data_index]\n    y = _y.data\n    if not isinstance(y, (list, np.ndarray)): # single label -> one hot encoding\n        y = np.eye(learn.data.valid_ds.c)[y]\n\n    m = learn.model.eval()\n    xb,_ = learn.data.one_item(x)\n    xb_im = Image(learn.data.denorm(xb)[0])\n    xb = xb.cuda()\n\n    def hooked_backward(cat):\n        with hook_output(m[0]) as hook_a: \n            with hook_output(m[0], grad=True) as hook_g:\n                preds = m(xb)\n                preds[0,int(cat)].backward()\n        return hook_a,hook_g\n    def show_heatmap(img, hm, label):\n        _,axs = plt.subplots(1, 2)\n        axs[0].set_title(label)\n        img.show(axs[0])\n        axs[1].set_title(label)\n        img.show(axs[1])\n        axs[1].imshow(hm, alpha=0.6, extent=(0,img.shape[1],img.shape[1],0),\n                      interpolation='bilinear', cmap='magma');\n        plt.show()\n\n    for y_i in np.where(y > 0)[0]:\n        hook_a,hook_g = hooked_backward(cat=y_i)\n        acts = hook_a.stored[0].cpu()\n        grad = hook_g.stored[0][0].cpu()\n        grad_chan = grad.mean(1).mean(1)\n        mult = (acts*grad_chan[...,None,None]).mean(0)\n        show_heatmap(img=xb_im, hm=mult, label=str(learn.data.valid_ds.y[data_index]))\n\n\nidx_list = [0,1,2,31,3,63, 142, 207]        \n#idx_list = range(200,220)\nfor idx in idx_list:# range(10):\n    visualize_cnn_by_cam(learn, idx)","37a57b71":"<a id=\"pihds\"><\/a>\n# Potholes in high dimensional space:\n\nIf you look at the training epochs below you will notice that the model hit a series of local minima on the way towards the optimal weights. I experimented with many combinations of weight decays, training cycles, learning rates, fine tuning, and the combination below worked the best. Even with these parameters the model still fumbles and falls on its face a little as it tries to find the optimal point.You can get an idea of how bunpy a ride it is with the added visual below.\n\nSmaller learning rates kept getting stuck in local minima then overfitting, higher learning rates kept overshooting and getting worse without overfitting. Fine tuning the model by unfreezing it and retraining the early layers with a mild learning rate caused the error rate to go through the roof. Weight decay worked just fine at 1e-1, did some experimenting but the accuracy never got much higher than 80% with other weight decay settings. As a funny side note I tested this with multiple different models, and the learning rate curves optimal point stayed the same, but the steepness of the slope was different (the larger models had a flatter curve while the smaller models had a sharper one). ","1fa7edfa":"# Task: Improve Accuracy\n\nThe task is to improve the accuracy from 90% to 98-99%. Densenet161 gives 90% accuracy. The task is to optimize for accuracy but I will also show the F Beta score of the model as well. This notebook will feature a vgg16 model, trained in fastAI, and using progressive resizing and cutout to attain better results. Updated for september, I added some additional visuals to help understand the training and added a new section at the end for practical applications in healthcare.\n\nThe statement below was from the original 'completed' version of this notebook about 5 months ago. I am thrilled to say that this is no longer the case as an academic team improved upon my work to achieve truly phonomenal results\n`\nThe model only manages to achieve 95% accuracy, so I would consider this model a large step in the right direction, but not quite reaching the destination. Feel free to fork this notebook and play around, maybe you will find the missing link to reach the 98%+ mark. Check my final thoughts section for ideas for further improvement\n`\nThe current best performing model is now getting 99% on a custom test set, I highly recommend checking it out in the link to the paper in the special thanks section.\n\nSpecial Thanks to:\n* Sarvesh Dubey for both the dataset and the Task https:\/\/www.kaggle.com\/tourist55\n* Zachary Burns, Derrick Cosmas, and Bryce Smith for taking the project I started here and improving upon it (I think its my first time being cited in an acedemic paper) http:\/\/noiselab.ucsd.edu\/ECE228\/projects\/Report\/52Report.pdf","02bcbf4f":"I explored many different models and found VGG16 to have more consistent performance. Other similar performing models were resnet50, resnet101, resnet152, densenet161 and vgg19_bn. I experimented with squeezenet but the performance was awful. I had also tried googlenet and inception v3, but these methods I was unable to make compatible with progressive resizing. ","06512db6":"This project uses FastAI version 1.0.60, the new version have several syntax differences and new features.","60dc7409":"<a id=\"conclusions\"><\/a>\n\n # Final Thoughts:\n\nThe goal of the Task was to hit an accuracy of 98% or greater, looks like the approach I chose to take just gets me to around 95% accuracy with an f_beta score of around .95 (Didn't set any random states so those number can be off by a percentage point or two when you rerun the notebook). The one nitpick about the model I have is that it will generate a small number of false negatives (the model would predict someone does not have alzheimers when they do).\n\nIn training and validation the model is able to get an accuracy score as high as 99% and an F_beta of around .98, however on test data the model is only able to achieve 95%. The final piece in getting this model's performance up is to focus on how to tone the overfitting down a smidge (In my personal experience, the best performing models are always slightly overfit). There is probably room for improvement with the transforms that I chose for the images. There may also be room for improvement by adding another resizing step or changing some of the dimensions in the current step. It's also possible that combining the results from this model with another would be necessary to close the gap for 98%.\n\nIn 3\/21\/2020, this was the most accurate model for this dataset that I'm aware of. Now this model isn't the most accurate, but the heatmap interptiability adds a lot of value to this project\n\nThis year I've challenged myself to complete one task on Kaggle per week, in order to develop a larger Data Science portfolio. If you found this notebook useful or interesting please give it an upvote. I'm always open to constructive feedback. If you have any questions, comments, concerns, or if you would like to collaborate on a future task of the week feel free to leave a comment here or message me directly. For past TOTW check out the link to my page on github for this ongoing project\nhttps:\/\/github.com\/Neil-Kloper\/Weekly-Kaggle-Task\/wiki\n\n[Back to table of contents](#TOC) ","2432ad7c":"<a id=\"part-four\"><\/a>\n# Part 4: Test sets in FastAI\n\nFastAI's data bunch doesn't attempt to compare test data with any labels, it assumes that the test set is only there to be labelled. As such, the cell below allows me to test the dataset on the test set and view the results. The results are output in an n+1 size list, where n refers to the number of evaluation metrics the model is given. I'm not sure what the first entry in the list does, each of the following entries are the scores of the model, output in the order in which they are received.\n\n[Back to table of contents](#TOC) ","b22bd926":"Below is a nifty function to figure out the number of cases per class, written by a fellow by the name of James Briggs.\n\ncopy pasted from here: https:\/\/forums.fast.ai\/t\/get-value-counts-from-a-imagedatabunch\/38784","e931261c":"<a id=\"part-two\"><\/a>\n### Part 2: Pint sized model\n\nProgressive Resizing involves pretraining a model on shrunken versions of images (in this case, down to size 112x112) and then retraining the model on the full sized images (or possibly a few intermediate sized images in between if we want to get excessive).\n\n[Back to table of contents](#TOC) ","f262edce":"<a id=\"TOC\"><\/a>\n## Table of Contents\n\n* [Part 2: Pint sized model](#part-two) \n* [part 3: Full sized fun](#part-three) \n    * [Potholes in high dimensional space](#pihds) \n* [Part 4: Test sets in FastAI](#part-four) \n    * [What does the model see](#wdtms) \n* [Conclusions](#Conclusions) \n\n\n# Part one: feeding in the data","dd2b939f":"<a id=\"wdtms\"><\/a>\n### What does the model see?\nBelow is where the real meat of this notebook is. We have a function which shows what areas of the image contributed the most to the appropriate diagnostic. With this method, one can view an image, see what the algorithmn predicted and why. This can help humans involved in the diagnostic process get a second opinion or bring attention high risk areas.\n\nAs a side note, I find it interesting that for alzheimers free and early stage MRI images the areas that are most highlighted are around the ventricals (the tubes in the center) while the late stage images often have the outer regions highlighted (which appear to be less dense).","30e88ea6":"I'm relatively new to DeepLearning with FastAI, from my limited experience with Progressive Resizing, the smaller training phases are most useful for the final product when they are still a bit underfit to the dataset. Further experimentation with this could, and possibly should be done to confirm.\n<a id=\"part-three\"><\/a>\n## Part 3: Full sized fun!\n\nNow that there is a reasonably good model for 112x112 images, now I will train the model on full sized images. Since I am now creating a final model, I will include the F Beta score to get a more complete view of the models performance. I also will modify the cutout section of the transforms to upscale the cutout sizes. \n\n[Back to table of contents](#TOC) ","1ddc5668":"### Cutout\nCutout is a method for removing small chunks of an image at random (see the batch images below to get an idea). This was one of the transforms that I experimented the most with and it helped close the gap a little between validation accuracy and test accuracy. "}}