{"cell_type":{"d52d5b72":"code","ac934ce6":"code","d2c246b9":"code","c6c68e94":"code","37030977":"code","ec729557":"code","090e222f":"code","0ee1843b":"code","8d830040":"code","3b501450":"code","1bb2f8db":"code","9c3e5bb5":"code","b233b9de":"code","c291c6c0":"code","c3d689f1":"code","dfbf2176":"code","e50f6474":"code","43f6676d":"code","cf0cf7db":"code","897c7899":"code","cd5bd2f8":"code","19859aec":"markdown","1937f578":"markdown","634d3bb2":"markdown","d40aa7b6":"markdown","222798e3":"markdown","e53c6679":"markdown","3fd11c1a":"markdown","00ac38cc":"markdown","f8c0d0db":"markdown","0a60c341":"markdown","5bef58c5":"markdown","44b9470b":"markdown","6bb1acc0":"markdown","0de6405f":"markdown","cc3c2afa":"markdown","aa7fda73":"markdown","ed65b3fd":"markdown","bdb03cd1":"markdown","cc7ada43":"markdown"},"source":{"d52d5b72":"# importando bibliotecas necess\u00e1rias\r\n%matplotlib inline\r\n\r\nimport pandas as pd\r\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\r\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.model_selection import cross_val_score\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport seaborn as sns","ac934ce6":"# carregando dados de treino\rtrainData = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\", na_values = '?')\r\ntrainData","d2c246b9":"# Colunas que tem dados faltantes\r\ntrainData.isna().any()","c6c68e94":"trainData[\"workclass\"].value_counts(normalize=True).head(1)","37030977":"trainData[\"occupation\"].value_counts(normalize=True).head(1)","ec729557":"trainData[\"native.country\"].value_counts(normalize=True).head(1)","090e222f":"replacements = {\"workclass\": \"Private\", \"occupation\": \"Prof-specialty\", \"native.country\": \"United-States\"}\r\ntrainData = trainData.fillna(value=replacements)\r\n\r\n# Colunas que tem dados faltantes\r\ntrainData.isna().any()","0ee1843b":"trainData[\"native.country\"]=trainData[\"native.country\"].replace(to_replace=r'^(?!United).*', value=\"Others\", regex=True)","8d830040":"trainData=trainData.drop(columns=[\"Id\", \"fnlwgt\", \"education\"])","3b501450":"# vetor de vari\u00e1veis categ\u00f3ricas\r\ncategorical_features = [\"occupation\", \"relationship\", \"sex\", \"race\", \"native.country\", \"workclass\", \"marital.status\"]\r\n\r\n# ajustando os dados de treino\r\ntrainData[categorical_features] = OrdinalEncoder().fit_transform(trainData[categorical_features])\r\ntrainData[\"income\"] = LabelEncoder().fit_transform(trainData[\"income\"])","1bb2f8db":"trainData.head(10)","9c3e5bb5":"trainData.insert(loc=8, column=\"capital\", value=trainData[\"capital.gain\"] - trainData[\"capital.loss\"])\r\ntrainData = trainData.drop(columns=[\"capital.gain\", \"capital.loss\"])","b233b9de":"plt.figure(figsize=(11,1))\r\n\r\nsns.heatmap(trainData.corr().iloc[[-1]], annot = True, fmt='.2g', vmax = 0.5, vmin = -0.5, center = 0, cmap = 'seismic')\r\nplt.show()","c291c6c0":"fig, axes = plt.subplots(1, 4, figsize=(20,4))\r\nfig.tight_layout(pad=3.0)\r\nsns.histplot(ax=axes[0], kde=True, data=trainData['capital'])\r\nsns.histplot(ax=axes[1], kde=True, data=trainData['age'])\r\nsns.histplot(ax=axes[2], kde=True, data=trainData['education.num'])\r\nsns.histplot(ax=axes[3], kde=True, data=trainData['hours.per.week'])","c3d689f1":"# normalizando age, education.num e hours.per.week\r\nfeaturesConcentradas = [\"age\", \"education.num\", \"hours.per.week\"]\r\nscalerConc = StandardScaler()\r\ntrainData[featuresConcentradas] = scalerConc.fit_transform(trainData[featuresConcentradas])\r\n\r\n# normaliza\u00e7\u00e3o robusta que diminui efeito de outliers em capital\r\nscalerSparse = RobustScaler()\r\ntrainData[[\"capital\"]] = scalerSparse.fit_transform(trainData[[\"capital\"]])\r\n\r\ntrainData.head()","dfbf2176":"X = trainData.drop(columns=['income']).values\r\nY = trainData['income']","e50f6474":"kMax = None\r\naccMax=0\r\nfor k in range(25, 36):\r\n    \r\n    KNNclf = KNeighborsClassifier(n_neighbors=k)\r\n    \r\n    score = cross_val_score(KNNclf, X, Y, cv = 10).mean()\r\n    \r\n    print(k,\" vizinhos | Score:\", round(score,6))\r\n\r\n    if score > accMax:\r\n        kMax = k\r\n        accMax = score\r\n        \r\nprint('\\nMelhor K: '+str(kMax)+\" | Score:\", round(accMax,6))","43f6676d":"# carregando dados de treino\rtestData = pd.read_csv('..\/input\/adult-pmr3508\/test_data.csv', na_values = '?')\r\n\r\n# substituindo NA pela moda\r\nreplacements = {\"workclass\": \"Private\", \"occupation\": \"Prof-specialty\", \"native.country\": \"United-States\"}\r\ntestData = testData.fillna(value=replacements)\r\n\r\n# substituindo pa\u00edses diferentes de United-States por Others\r\ntestData[\"native.country\"]=testData[\"native.country\"].replace(to_replace=r'^(?!United).*', value=\"Others\", regex=True)\r\n\r\n# retirando colunas n\u00e3o utilizadas\r\ntestData=testData.drop(columns=[\"Id\", \"fnlwgt\", \"education\"])\r\n\r\n# vetor de vari\u00e1veis categ\u00f3ricas\r\ncategorical_features = [\"occupation\", \"relationship\", \"sex\", \"race\", \"native.country\", \"workclass\", \"marital.status\"]\r\n\r\n# ajustando os dados d\r\ntestData[categorical_features] = OrdinalEncoder().fit_transform(testData[categorical_features])\r\n\r\n# criando coluna de capital\r\ntestData.insert(loc=8, column=\"capital\", value=testData[\"capital.gain\"] - testData[\"capital.loss\"])\r\ntestData = testData.drop(columns=[\"capital.gain\", \"capital.loss\"])\r\n\r\n# normalizando age, education.num e hours.per.week\r\nfeaturesConcentradas = [\"age\", \"education.num\", \"hours.per.week\"]\r\nscalerConc = StandardScaler()\r\ntestData[featuresConcentradas] = scalerConc.fit_transform(testData[featuresConcentradas])\r\n\r\n# normaliza\u00e7\u00e3o robusta que diminui efeito de outliers em capital\r\nscalerSparse = RobustScaler()\r\ntestData[[\"capital\"]] = scalerSparse.fit_transform(testData[[\"capital\"]])","cf0cf7db":"bestKNN = KNeighborsClassifier(n_neighbors=32)\r\nbestKNN.fit(X,Y)\r\nX_test = testData.values","897c7899":"resultado = bestKNN.predict(X_test)\r\nresultado","cd5bd2f8":"submissao = pd.DataFrame(data=resultado).replace(to_replace=[0,1], value=['<=50K','>50K'])\r\nsubmissao.columns = ['income']\r\nsubmissao.to_csv(\"submission.csv\", index = True, index_label = 'Id')","19859aec":"# 4. Predi\u00e7\u00e3o da base de teste\r\n\r\nAgora devemos utilizar o nosso classificador constru\u00eddo na base de testes, ent\u00e3o faremos o mesmo processo que fizemos com a base de treino com ela:","1937f578":"# 2. Tratamento dos dados\r\n\r\nCom os dados preparados, podemso realizar analises para entender como tratamos eles antes de receber os modelos. Esse processo visa diminuir os outlies normalizando algumas colunas e poupar trabalho do modelo, retirando colunas pouco relacionadas da an\u00e1lise.","634d3bb2":"Agora vamos normalizar esses dados, usando o StandartScaler() para dados mais concentrados e o RobustScaler() para os mais dispersos","d40aa7b6":"Finalmente estamos com os dados preparados e podemos aplicar o KNN para treinar o modelo e obter um classificador.\r\nO primeiro passo \u00e9 construir as nossas vari\u00e1veis X e Y:","222798e3":"Aqui \u00e9 interessante notar que na coluna \"native.country\" mais de 90% das pessoas s\u00e3o dos Estados-Unidos. Para facilitar a analise, vamos substituir a nacionalidade dos n\u00e3o estado unidenses por \"outros\".","e53c6679":"# PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es (2021)\r\n# An\u00e1lise e predi\u00e7\u00e3o utilizando a base de dados Adult\r\n\r\n**Autor:** Bernardo Moredo Rocco","3fd11c1a":"Podemos ent\u00e3o transformar as vari\u00e1veis categ\u00f3ricas em valores num\u00e9ricos","00ac38cc":"# 3. Aplica\u00e7\u00e3o de KNN","f8c0d0db":"Agora vamos preparar o nosso classificador (j\u00e1 treinado anteriormente) e o valor de X","0a60c341":"# 1. Prepara\u00e7\u00e3o dos dados\r\n\r\nNo primeiro passo vamos preparar os dados para a analise. Devemos importar as bibliotecas e carregar as bases de treino e teste, bem como selecionar os atributos que ser\u00e3o utilizados na classifica\u00e7\u00e3o e tratar os dados faltantes.","5bef58c5":"Verificando como ficaram os dados ap\u00f3s a prepara\u00e7\u00e3o","44b9470b":"Analisando as correla\u00e7\u00f5es entre as vari\u00e1veis, vemos que as que menos influenciam o income s\u00e3o \"workclass\", \"occupation\" e \"native.country\". Apesar disso, foi testado o modelo retirando essas colunas e os resultado obtidos foram piores. Ent\u00e3o foi decidido realizar o treinamento com elas","6bb1acc0":"Vemos que as colunas \"workclass\", \"occupation\" e \"native.country\" tem dados faltantes. Vamos popular eles com a moda de cada coluna.","0de6405f":"Vamos aproveitar e tirar as colunas \"Id\", \"fnlwgt\" e \"education\". As informa\u00e7\u00f5es de \"Id\" e \"fnlwgt\" n\u00e3o acrescentam em nada na an\u00e1lise e a informa\u00e7\u00f5es de \"education\" j\u00e1 est\u00e1 encodada em \"education.num\"","cc3c2afa":"Primeiro vamos substituir as colunas \"capital.gain\" e \"capital.loss\" por \"capital\", que ser\u00e1 a diferen\u00e7a entre as duas","aa7fda73":"Agora iremos testar o KNN para valores entre 25 e 36 vizinhos. Usaremos cross validation para atribuir um score a cada valor e seguiremos usando o n\u00famero de K que obtiver o maior score.","ed65b3fd":"E, finalmente, realizar a predi\u00e7\u00e3o dos dados:","bdb03cd1":"Seguiremos usando o valor de K=32, j\u00e1 que foi o n\u00famero de vizinhos que obteve a maior acur\u00e1cia","cc7ada43":"Agora precisamos preparar esses dados em um arquivo CSV para realizar a submiss\u00e3o no Kaggle"}}