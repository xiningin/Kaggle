{"cell_type":{"230cfdd2":"code","ecd0b8f4":"code","ca84d231":"code","ac27a36e":"code","d303bab5":"code","e3cee550":"code","81c4e487":"code","a4320f0e":"code","24258f37":"code","93ac6018":"code","63739d52":"code","be7ab422":"code","4fbaac10":"code","a849e204":"code","c8f45f3d":"code","29d3ab5d":"code","b399e9bb":"code","ed34e0dc":"code","0b8c3a7f":"code","9b7d306e":"code","b72800d8":"code","9d561ae0":"code","fa298b53":"code","3b950d5b":"code","d8895766":"code","55542d09":"code","70296745":"code","8039f9eb":"code","aa5b51a6":"code","e9552f3b":"code","d2f64fb0":"code","f7076394":"code","bfc59362":"code","9c879e39":"code","17cb8e08":"code","2ae0d001":"code","c1ac80e8":"code","b89d3599":"code","f1abd27e":"code","2f2fb03f":"code","91f3bfb6":"code","44150519":"code","1c115bff":"code","75a72bb8":"code","2012d244":"markdown","24274786":"markdown","d2afb056":"markdown","375a695e":"markdown","783dd44a":"markdown","329b3b08":"markdown","fc30d0fa":"markdown","0b06aab6":"markdown","e4659968":"markdown","adcb3040":"markdown","a4a143b1":"markdown","ed460c06":"markdown","403403b6":"markdown","ec70f1a2":"markdown","ee045177":"markdown","701363ac":"markdown","cd2b7aea":"markdown","287bb3ad":"markdown","5dd752a9":"markdown","7bfdeb1d":"markdown","4251f266":"markdown","c59c72a4":"markdown","8e31e5f2":"markdown","86a33a96":"markdown","c6277972":"markdown","637ffc11":"markdown"},"source":{"230cfdd2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if 'train' in os.path.join(dirname, filename):\n            train = pd.read_csv(os.path.join(dirname, filename))\n        elif 'test' in os.path.join(dirname, filename):\n            test = pd.read_csv(os.path.join(dirname, filename))\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ecd0b8f4":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nimport pylab\nfrom scipy.stats import shapiro","ca84d231":"train.head()","ac27a36e":"test.head()","d303bab5":"train.isnull().values.any()","e3cee550":"test.isnull().values.any()","81c4e487":"train.describe()","a4320f0e":"test.describe()","24258f37":"\ndef normality_plots(df,rb=0):\n    plt.figure()\n    fig, ax = plt.subplots(5,5,figsize=(15,14))\n    \n    for i in range(1,26):\n        plt.subplot(5,5,i)\n        plt.hist(train['var_'+str(i+rb)])\n    \n    plt.show()\n    #almost works...except I need to create separate plots","93ac6018":"normality_plots(train)","63739d52":"normality_plots(train, 27)","be7ab422":"normality_plots(train,52)","4fbaac10":"normality_plots(train, 77)","a849e204":"normality_plots(train, 102)","c8f45f3d":"normality_plots(train, 127)","29d3ab5d":"normality_plots(train, 152)","b399e9bb":"stats.probplot(train['var_1'], dist='norm',plot=pylab)\npylab.show()","ed34e0dc":"sns.countplot(train['target'])","0b8c3a7f":"train25=train.iloc[:,1:26]\ntrain25","9b7d306e":"train25=train.iloc[:,1:26]\ncorr=train25.corr()\n\nsns.heatmap(corr,xticklabels=corr.columns,\n        yticklabels=corr.columns)","b72800d8":"\n# Sigmoid function\n#\ndef sigmoid(z):\n    return 1 \/ (1 + np.exp(-z))\n# Creating sample Z points\n#\nz = np.arange(-5, 5, 0.1)\n \n# Invoking Sigmoid function on all Z points\n#\nphi_z = sigmoid(z)\n \n# Plotting the Sigmoid function\n#\nplt.plot(z, phi_z)\nplt.axvline(0.0, color='k')\nplt.xlabel('z')\nplt.ylabel('$\\phi(z)$')\nplt.yticks([0.0, 0.5, 1.0])\nax = plt.gca()\nax.yaxis.grid(True)\nplt.tight_layout()\nplt.show()","9d561ae0":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)\n\n\n#Big problem here is that this calculation is soooo damn slow","fa298b53":"#mu_co=train.iloc[:,2:-1]\n#calc_vif(mu_co)\n\nmu_co=train.iloc[0:25,2:10]\ncalc_vif(mu_co)","3b950d5b":"X=train.drop(['target','ID_code'], axis=1)\ny=train.target","d8895766":"y","55542d09":"X","70296745":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val=train_test_split(X, y, test_size=0.2,random_state=0)","8039f9eb":"X_train.head()","aa5b51a6":"len(X_train)","e9552f3b":"X_val.head()\nlen(X_val)","d2f64fb0":"y_train","f7076394":"y_val.head()","bfc59362":"from sklearn.linear_model import LogisticRegression","9c879e39":"logreg = LogisticRegression(solver='sag')","17cb8e08":"#Fit applies the logistic regression model to the X_train input data values and compares the y_train values to the predicted.\nlogreg.fit(X_train, y_train)","2ae0d001":"y_pred=logreg.predict(X_val)","c1ac80e8":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_val,y_pred)\ncnf_matrix","b89d3599":"print(\"Accuracy:\",metrics.accuracy_score(y_val, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_val, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_val, y_pred))","f1abd27e":"y_pred_proba = logreg.predict_proba(X_val)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_val,  y_pred_proba)\nauc = metrics.roc_auc_score(y_val, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","2f2fb03f":"my_test=test.drop(['ID_code'],axis=1)\nmy_test","91f3bfb6":"my_submission=logreg.predict(my_test)","44150519":"my_submission","1c115bff":"output= pd.DataFrame({'ID_code': test.ID_code,\n                       'target': my_submission})\noutput.to_csv('submission.csv', index=False)","75a72bb8":"output","2012d244":"## Assumptions with Logistic Regression\n\nNow the question is, what assumptions does logistic regression make about our model? After all, all models make some sort of assumptions.\n\nSome big differences from other GLMs (generalized linear models) right off the bat.\n\n    1) Residuals do not need to be normally distributed\n\n    2) No homoscedasticity required\n\n\nAssumptions for **binary** logistic regression\n\n    1) Target variable must be binary\n\n    2) Independent observations\n\n    3) Little multicollinearity\n\n    4) Linearity of independent variables & log odds\n\n    5) Relatively large sample size\n\n\nFor us\n\n    1) Not a problem\n\n    2) Unsure, but it doesn't really seem like this should be an issue. At the very least, I'm not diving into the anonymous data to see if we have matched or repeated data, although it would strike me as odd if we did.\n    \n    3) This trips people up. Multicollinearity details a linear relationship between multiple predictors. We don't really know about this yet.\n    \n    4) We're good here.\n    \n    5) Excellent here\n    ","24274786":"The fit part of scikit is a little bit weird. Essentially, you're training the model on whatever algorithm you have chosen. The scikit model essentially vectorizes\u2014performs the algorithm on each part of\u2014the logistic regression model and chooses your classifications according to the parameters you input.","d2afb056":"Prediction tells you how well your trained model matches to your validation model here. If there is a wide disparity, it is a good place to tell that you overfit.","375a695e":"I originally used the liblinear solver here due to an issue I was having with the default lfbgs solver. I think it is fixed, but I didn't want to mess with updating scikit right now in case other things break. Sag should work a little faster. No real need to go into the details of what exactly this does (for me, right now... I could be wrong and it could drastically improve performance later).","783dd44a":"**The Model - Logistic Regression**\n\nGoing to be focusing on the scikit version of logistic regression here. Hoping to explain how things work along the way.\n\nFirst thing: despite the name, logistic regression is a *classification* algorithm. The reason it has the name regression is due to the algebraic equation nestled snugly in there. There are, of course, variations on this. I'll be focusing on the most typical logistic regression here. Binary Logistic Regression\n\n\n# Binary Logistic Regression\n\n$\\huge p(X) = \\frac{e^{\\large\\beta_{\\large0}+\\beta_{\\large1}X}}{1+e^{\\large\\beta_{\\large0}+\\beta_{\\large1}X}} $\n\nwhere\n\n$\\large p(X) = $ The probability of being X\n\n$\\large e =$ Euler's number\n\n$\\large \\beta_{0} = $ Intercept Term\n\n$\\large \\beta_{1}= $ Slope Term\n\nThe formula above does some great things for us. First and foremost, it reduces our output to a value between 0 and 1. In this circumstance, that is perfect. We are looking for a yes\/no answer, 0 or 1 gives us that binary response\n\nBefore we go on, it is worth pointing out, from a mathematical position, where the name logistic regression comes from. Quick, I'll put the linear regression formula.\n\n$\\large y = \\beta_{0}+\\beta_{1}X $\n\nI hope you see it. If you don't, look at the raised values in the numerator and denominator of the binary logistic regression formula. For those that are more visually minded, it may help to see what the plot looks like.","329b3b08":"I'm separating my X from my y values and turning them into a sci-kit approved train-test (or validation in this case) set.\n\nA lot of people can run into problems here (I know I can as well). Scikit gets pretty picky about having everything in the EXACT right format. Pay attention to your columns","fc30d0fa":"For my EDA (Exploratory Data Analysis), I need to start of by seeing what is actually going on with my data. At this point, I'm assuming that I have no idea what is in it. Currently, my framework is as follows:\n\nEDA\n1. Check the raw data & data dictionary (if applicable)\n2. Check for nulls\n3. Check to make sure that quantitative and qualitative data are in the correct format\n4. Check data distributions & related data transformations\n5. If applicable, do some feature tinkering.\n6. Picking my model. Assessing assumptions of model & if it fits with data. (If assumptions do not hold, go back to 4 or decide why it is ok to continue).\n\n\nTraining\n\n7. Training the model\n8. Submitting my test\n\nI'm always open and tweaking this model, but overall it is general enough to get me through the day. You could easily add things like factor analysis in the EDA if you wanted to. I'll refrain for now.","0b06aab6":"I'm going to make the not so bold decision to believe that none of these really have much correlation to speak of here.\n\nAt this point, I'm going to go forward with my logistic regression model and just see what happens. I'll likely come back and adjust. I saw some of the feature engineering in the most popular EDA for this, but I'm going to avoid that this time. I'll approach it in a later edition once I work through it and see what gems it has.","e4659968":"Generally trending towards CLT. I don't *need* a normal distribution here, so I'm not going to hammer down on this. I'll illustrate a probability plot on one variable just to see how close to normal we are.","adcb3040":"Since these two isnull() tests end up as false, I don't have to worry about checking where any null data might be. No need to spend the time (computationally or programmatically) before doing an easy check.\n\nTime for some descriptive statistics","a4a143b1":"Accuracy is basic. Exactly what you would think you were looking for. Just the $\\huge\\frac{TP+TN}{TP+TN+FP+FN}$ Overall, I'm relatively pleased with this.\n\n_____________________________________________\n\nPrecision goes down substantially. Precision is a little more difficult for people to grasp. $\\huge\\frac{TP}{TP+FP}$ or in other words: the percentage of the time the model correctly predicts when something is positive (or classified as a 1 in our case).\n\n______________________________________________\n\n\nOur recall score...isn't good. It also tends to be the hardest one for people to grasp. Recall tells us how often we correctly predicts a positive when a positive exists.\n$\\huge\\frac{TP}{TP+FN}$\n","ed460c06":"Printing these out so you can see what is going on in between all of these. In essence:\n\nX_train: Large dataset with 80% of all data & no response\n\nX_val: Smaller dataset with 20% of the data & no response\n\ny_train: Corresponding 80% response values\n\ny_val: Corresponding 20% response values.\n\nThe reason that I am labeling these as 'val' instead of 'test' is because I'm technically testing on the unseen data. Labeling it as validation keeps everything a little bit more clear than it otherwise would be. This is pretty common practice, although unfortunately I don't think it is quite common enough.","403403b6":"I'll be doing a series of different classification methods throughout my Santander series (with explanations). For each model, I will attempt to over-explain rather than to under-explain. It is my hope that this can be approachable. \n\nBelow, I'll be adding links to the different classifications as I get to them. For logistic regression, I'm currently thinking that I'll have a very simple model & and few more complex models. I'll link to those here and at the bottom.\n\n\n# **SCIKIT Classification**\n\n## ***Linear Models***\n\n### **Logistic Regression**\n\n#### 1) Simple model. This one.\n\n\n2) Adding things like k fold & perhaps playing around with loss models & SGDClassifier\n\n\n\n*Ridge Classifier*\n\n1) Sometimes referred to as a Least Squares Support Vector Machines with a linear kernel (according to scikit)\n\n\n\n*Perceptron*\n\n \n*Passive Aggressive Classifier*\n \n \n\n \n***Linear & Quadratic Discriminant Analysis***\n\n\n\n***Support Vector Machines***\n\n\n\n***Nearest Neighbors***\n\n\n*K neighbors Classification*\n\n\n*Radius Neighbors*\n\n\n*Nearest Centroid*\n\n\n*Neighborhood Components Analysis*\n\n\n***Gaussian Process Classifier***\n\n\n***Naive Bayes***\n\n\n*Gaussian Naive Bayes*\n\n\n*Multinomial Naive Bayes*\n\n\n*Bernoulli Naive Bayes*\n\n*Categorical Naive Bayes*\n\n***Decision Tree***\n\n***Ensemeble Methods***\n\n***Probability Calibration***\n\n***Neural Networks***\n\n*Multi layer perceptron*\n\n\n\nI'll mostly keep it to one type per. I might do some ensemble and other combinations in separate posts if feasible.","ec70f1a2":"This is slightly chaotic to read. Essentially, what I really want from all of this is to see the differences between variables in terms of mean, standard deviation, min and max numbers. From a glance, it seems like standardizing these numbers could be useful here.\n\n\nI should probably check for normality and scedasticity of the raw data. Normality assumption test first.","ee045177":"The line, which we call a sigmoid curve,is allowing us to see the growing *probability* of the input z giving us either a 0 or 1 output.","701363ac":"Some code & information used from the following sources:\n\n1) https:\/\/vitalflux.com\/logistic-regression-sigmoid-function-python-code\/\n\n2) Datacamp\n\n3) \"An Introduction to Statistical Learning in R\" by Gareth James, Daniela Witten, et al.\n\n4) \"Pattern Recognition & Machine Learning\" by Bishop, et al.\n\n5) https:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/what-is-multicollinearity\/","cd2b7aea":"ROC curves give us a tradeoff. In this circumstance, the tradeoff is between sensitivity(the true positive rate) and specificity (1 - false positive rate). The AUC (Area Under Curve) is pretty decent here. Actually better than I expected.","287bb3ad":"Looks like we have some pretty terrible multicollinearity throughout all this. I'm not going to attempt fixing it right now, I just want to see how the model performs as is. However, these seem like pretty bad numbers. I'll likely want to do something with these on my next go around","5dd752a9":"Ok, ok. So now you've seen the logistic regression formula & you've seen why it is misleadingly called regression when it is in fact a classification algorithm. But...what does it actually mean? How to convert the abstract notation into something generally usable? In order to make the above notation a bit more understandable, we can manipulate this formula in order to shows us *odds*.\n\n$\\huge \\frac{p(X)}{1-p(x)} = e^{\\beta_{0}+\\beta_{1}X} $\n\n\nIf you know anything about odds, you know that you can now read the formula by saying. \"If I have a p(X) = 0.4, then that means that I have an odds ratio of .4\/.6\" Or an odds of 2:3. It is important to note here what odds is *not*. Odds is not saying that your chance is 2\/3. Instead, it is saying that that you will, probabilistically, arrive at answer X 2 times out of 5 and answer Y 3 times out of 5.\n\n\nThink about it this way. An odds of 1:1 would just be...50% probability, right? If you ever get confused, just remember the odds of 1:1 and then work your interpretation from there.\n\nBut, of course, that isn't it with odds. You still have to add the logarithm (this is essentially done to make everything easier and to get rid of the intercept, slope, & x being in the exponential).\n\n\nNow the question is, how on earth do we use this? Well, it ends up being pretty simple here. Once you have all the more complicated stuff out of the way, then you check on your probability switch. What I mean by that is that you decide what your decision boundary is for moving your outputs to 0 or 1.\n\nFor this modeling practice, we won't mess with that, it'll remain at a 50% probability for our decision line. In certain business practices, you may want to shift this in order to be more conservative or more aggressive.","7bfdeb1d":"## Checking Raw Data & Data Dictionary\n\nIn this circumstance, there is no data dictionary. The data is anonymized financial data & thus pretty difficult to glean any real world information. Not ideal, but workable as there is so much financial data.","4251f266":"Doing an 80\/20 split here. Not concerned about having too little data at all.","c59c72a4":"**Train**\n\nID_code\n\nTarget\n\n200 variables\n\n________________________________________________________\n\n**Test**\n\nID_code\n\n200 variables\n\n________________________________________________________\n\nThe obvious question here is: how do you comfortably work with anonymized data? You have to get comfortable working with *the numbers*. This can be a little intimidating, but it doesn't have to be. In some ways, when you work with just the numbers you are working with less abstraction. What can you do with just numbers?\n\nTwo things come to mind:\n\n1) You can always check for nulls\n\n2) You can check data distributions, mean distances, etc. Essentially, you're looking for summary statistics. Heck, you could do some factor analysis or clustering on this stuff and see what comes up. At this point, I'm going to keep it more on the basic summary statistics, but I might come back to some multivariate methods later.","8e31e5f2":"# **Basic Exploratory Analysis**","86a33a96":"I'm going to add a confusion matrix in here & start seeing how well I did with my logistic regression model.","c6277972":"Now, I'll finish up by putting everything into my test dataset and then submitting this simplest of logistic regressions. I'll make a more complicated logistic regression model which introduces k-folds and some other things in my following model, which I'll link here when it is available.","637ffc11":"Any correlated variables?"}}