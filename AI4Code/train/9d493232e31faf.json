{"cell_type":{"4d9b6f10":"code","8b55caef":"code","09b13c08":"code","80d599a3":"code","55ec786d":"code","0250163e":"code","99992423":"code","d7b459ea":"code","367ecc6b":"code","d101428c":"code","d0f9b782":"code","6faafdab":"code","de99b6c3":"code","e068e14d":"code","8710b0c3":"code","1bcf70d4":"code","7c3c273a":"code","55985959":"code","6d257a71":"code","4c4ed9d1":"code","25fc06a0":"code","98cbca46":"code","9d3361cd":"code","1cc2cfa7":"code","f914552b":"code","79f5bdd5":"code","9ab65363":"code","e19c2775":"code","a9fdb61a":"code","a59d76c9":"code","f6eb8355":"code","46649264":"code","feb4490b":"code","18761225":"code","9cf00909":"code","24367bc7":"code","a828827c":"code","f384b7e7":"code","14725489":"code","dab87bbd":"code","d71518a9":"code","a2c3c927":"markdown","73e48e1d":"markdown","203d16cd":"markdown","5f405b19":"markdown","ea419940":"markdown","c69d8492":"markdown","f12edb75":"markdown","36e4927e":"markdown","d4966f6c":"markdown","d2cdc128":"markdown","586f4bec":"markdown","b903d74f":"markdown"},"source":{"4d9b6f10":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b55caef":"import pandas as pd\n\n\ndf = pd.read_csv('\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv',encoding='latin-1')\ndf.head()","09b13c08":"df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\ndf = df.rename(columns={\"v1\":\"labels\", \"v2\":\"text\"})\ndf.head()","80d599a3":"df.describe()","55ec786d":"df.labels.value_counts()","0250163e":"df.labels.value_counts().plot.bar()","99992423":"# Replacing spam with 1 and ham with 0\ndf['spam']=df['labels']\nfor i,j in df.iterrows():\n    # i is index\n    # j is (labels, text)\n    if j['labels']=='ham':\n        j['spam'] = 0\n    else:\n        j['spam']=1","d7b459ea":"df.head()","367ecc6b":"import string\nprint(string.punctuation)","d101428c":"from nltk.corpus import stopwords\nprint(stopwords.words('english')[10:15])","d0f9b782":"def punctuation_stopwords_removal(sms):\n    # filters charecter-by-charecter : ['h', 'e', 'e', 'l', 'o', 'o', ' ', 'm', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'p', 'u', 'r', 'v', 'a']\n    remove_punctuation = [ch for ch in sms if ch not in string.punctuation]\n    # convert them back to sentences and split into words\n    remove_punctuation = \"\".join(remove_punctuation).split()\n    filtered_sms = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n    return filtered_sms","6faafdab":"print(punctuation_stopwords_removal(\"Hello we need to send this report by EOD.!!! yours sincerely, Purva\"))","de99b6c3":"print(df.head())","e068e14d":"from collections import Counter\n\ndata_ham = df[df['spam']==0].copy()\ndata_spam = df[df['spam']==1].copy()\n","8710b0c3":"print(data_ham[:2])\nprint(data_spam[:2])","1bcf70d4":"data_ham.loc[:, 'text'] = data_ham['text'].apply(punctuation_stopwords_removal)\nprint(data_ham[:1])","7c3c273a":"words_data_ham = data_ham['text'].tolist()","55985959":"words_data_ham[:3]","6d257a71":"data_spam.loc[:, 'text']=data_spam['text'].apply(punctuation_stopwords_removal)\nprint(data_spam[:1])\n#words_data_spam = data_spam['text'].tolist()","4c4ed9d1":"words_data_spam = data_spam['text'].tolist()\nprint(words_data_spam[:2])","25fc06a0":"ham_list = []\nfor sublist in words_data_ham:\n    for word in sublist:\n        ham_list.append(word)\n\nspam_list = []\nfor sublist in words_data_spam:\n    for word in sublist:\n        spam_list.append(word)","98cbca46":"ham_count = Counter(ham_list)\nspam_count = Counter(spam_list)\n\nham_top_30_words = pd.DataFrame(ham_count.most_common(30), columns=['word', 'count'])\nspam_top_30_words = pd.DataFrame(spam_count.most_common(30), columns=['word', 'count'])","9d3361cd":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=ham_top_30_words, ax=ax)\nplt.title(\"Top 30 Ham words\")\nplt.xticks(rotation='vertical');","1cc2cfa7":"\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=spam_top_30_words, ax=ax)\nplt.title(\"Top 30 Spam words\")\nplt.xticks(rotation='vertical');","f914552b":"from sklearn.feature_extraction.text import CountVectorizer\nbow_transformer = CountVectorizer(analyzer=punctuation_stopwords_removal).fit(df['text'])","79f5bdd5":"len(bow_transformer.vocabulary_)","9ab65363":"sample_spam = df['text'][8]\nbow_sample_spam = bow_transformer.transform([sample_spam])\nprint(sample_spam)\nprint(bow_sample_spam)","e19c2775":"print('Printing bag of words for sample 1')\nrow, cols = bow_sample_spam.nonzero()\nfor col in cols:\n    print(bow_transformer.get_feature_names()[col])","a9fdb61a":"import numpy as np\nprint(np.shape(bow_sample_spam))","a59d76c9":"sample_ham = df['text'][4]\nbow_sample_ham = bow_transformer.transform([sample_ham])\nprint(sample_ham)\nprint(bow_sample_ham)\nrows, cols = bow_sample_ham.nonzero()\nprint('Printing ')\nfor col in cols:\n    print(bow_transformer.get_feature_names()[col])","f6eb8355":"from sklearn.feature_extraction.text import TfidfTransformer\n\n# bag of words in vectorized format\nbow_data = bow_transformer.transform(df['text'])\nprint(bow_data[:1])\ntfidf_transformer = TfidfTransformer().fit(bow_data)","46649264":"tfidf_sample_ham = tfidf_transformer.transform(bow_sample_ham)\nprint('Sample HAM : ')\nprint(tfidf_sample_ham)\n\ntfidf_sample_spam = tfidf_transformer.transform(bow_sample_spam)\nprint('Sample SPAM : ')\nprint(tfidf_sample_spam)","feb4490b":"final_data_tfidf = tfidf_transformer.transform(bow_data)\nprint(final_data_tfidf)\nprint(np.shape(final_data_tfidf))","18761225":"from sklearn.model_selection import train_test_split\n\ndata_tfidf_train, data_tfidf_test, label_train, label_test = train_test_split(final_data_tfidf, df[\"spam\"], test_size=0.3, random_state=5)","9cf00909":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport seaborn as sns\n\ndef plot_confusion_matrix(y_true, y_pred):\n    mtx = confusion_matrix(y_true, y_pred)\n    #fig, ax = plt.subplots(figsize=(4,4))\n    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n                cmap=\"Blues\", square=True, cbar=False)\n    #  \n    plt.ylabel('true label')\n    plt.xlabel('predicted label')","24367bc7":"data_tfidf_train = data_tfidf_train.A\ndata_tfidf_test = data_tfidf_test.A","a828827c":"print(data_tfidf_train.dtype)\nprint(label_train.dtype)","f384b7e7":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nspam_detect_model_MNB = MultinomialNB()\nspam_detect_model_MNB.fit(data_tfidf_train, np.asarray(label_train, dtype=\"float64\"))\npred_test_MNB = spam_detect_model_MNB.predict(data_tfidf_test)\nacc_MNB = accuracy_score(np.asarray(label_test, dtype=\"float64\"), pred_test_MNB)\nprint(acc_MNB)","14725489":"from sklearn.metrics import roc_curve, auc\n\nfpr, tpr, thr = roc_curve(np.asarray(label_test, dtype=\"float64\"), spam_detect_model_MNB.predict_proba(data_tfidf_test)[:,1])\nplt.figure(figsize=(5, 5))\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Plot')\nauc_knn4 = auc(fpr, tpr) * 100\nplt.legend([\"AUC {0:.3f}\".format(auc_knn4)]);","dab87bbd":"def plot_confusion_matrix(y_true, y_pred):\n    mtx = confusion_matrix(y_true, y_pred)\n    #fig, ax = plt.subplots(figsize=(4,4))\n    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n                cmap=\"Blues\", square=True, cbar=False)\n    #  \n    plt.ylabel('true label')\n    plt.xlabel('predicted label')","d71518a9":"plot_confusion_matrix(np.asarray(label_test, dtype=\"float64\"), pred_test_MNB)","a2c3c927":"### Results Visualization Methods\n","73e48e1d":"# Analysis of most common words in spam and ham SMS\n\nHere, we will be making use of `collections.Counter`.","203d16cd":"#### Confusion Matrix\n","5f405b19":"### TfidfTransformer from sklearn\n\nBoth tf and tf\u2013idf can be computed as follows using sklearn's [TfidfTransformer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfTransformer.html)","ea419940":"#### ROC Curve","c69d8492":"# TF-IDF on BOW\n\nTF-IDF expects a bag-of-words (integer values) training corpus during initialization. During transformation, it will take a vector and return another vector of the same dimensionality.<br>\n\nTF-IDF stands for \"Term Frequency, Inverse Document Frequency\".<br>\n\n* It is a way to score the importance of words (or \"terms\") in a document based on how frequently they appear across multiple documents.\n* If a word appears frequently in a document, it's important. Give the word a high score. But if a word appears in many documents, it's not a unique identifier. Give the word a low score.<br>\n\n* Therefore, common words like *\"the\"* and *\"for\"*, which appear in many documents, will be scaled down. Words that appear frequently in a single document will be scaled up.<br>\n\nIn other words:\n* TF(w) = `(Number of times term w appears in a document) \/ (Total number of terms in the document).`\n* IDF(w) = `log_e(Total number of documents \/ Number of documents with term w in it).`\nFor example\nConsider a document containing 100 words wherein the word 'tiger' appears 3 times.\n* The term frequency (i.e., tf) for 'tiger' is then:<br>\n    TF = (3 \/ 100) = 0.03.\n* Now, assume we have 10 million documents and the word 'tiger' appears in 1000 of these. Then, the inverse document frequency (i.e., idf) is calculated as:<br>\n`IDF = log(10,000,000 \/ 1,000) = 4.`\nThus, the Tf-idf weight is the product of these quantities:\nTF-IDF = 0.03 * 4 = 0.12.","f12edb75":"### Train test split\n","36e4927e":"### Stop-words\n\nStop words are words like \u201cand\u201d, \u201cthe\u201d, \u201chim\u201d, which are presumed to be uninformative in representing the content of a text,\nand which may be removed to avoid them being construed as signal for prediction.","d4966f6c":"# Exploratory Data Analysis (EDA)\n\nHere I will be performing EDA on our spam\/ham dataset.","d2cdc128":"# BOW with CountVectorizer\n\nIn this scheme, features and samples are defined as follows: each individual token occurrence frequency (normalized or not) is treated as a feature.\nthe vector of all the token frequencies for a given document is considered a multivariate sample.<br>\nA corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\nWe call vectorization the general process of turning a collection of text documents into numerical feature vectors.<br>\nThis specific strategy (tokenization, counting and normalization) is called the Bag of Words or \u201cBag of n-grams\u201d representation.\nDocuments are described by word occurrences while completely ignoring the relative position information of the words in the document.\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/word2vec_architectures.png?raw=1\" width=\"500\"><\/img>\n\nIn this kernel we apply the CountVectorizer from sklearn as BOW model. : [CountVectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html)","586f4bec":"### Naive Bayes Classifier for Spam\/Ham Classification \n\nHere we will be using Naive Bayes' [MultinomialNB](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html) to classify emails into spam\/ham category. <br>One important thing to note in this part of coding section is that numpy didnt manage to figure out that datatype of `label_train` was float64 and by default it set the datatypt to a generic object.<br>\nIn order to solve this issue, we need to explicitly define dataype of `label_train` as `np.asarray(label_train, dtype=\"float64\")`.\n\n#### Results\n\nUpon applying NaiveBayes Classifier, we have achieved 96.5% accuracy.<br>\nUpon analysis of ROC charecterstics, we have achieved 97.698 as area under the curve (auc)","b903d74f":"# Pre-processing of SMS\n\nThis task involves :<br>\n1. Tokenization\n2. Vectorization\n3. TF-IDF resemblency\n\n## Removal of punctuations and stop-words\n\n### Punctuations"}}