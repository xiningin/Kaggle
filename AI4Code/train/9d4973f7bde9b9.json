{"cell_type":{"b8a3a550":"code","0e74ac53":"code","5da51149":"code","ad34ba74":"code","8dfc7f7b":"code","f352e40d":"code","87d7d0bd":"code","d88524be":"code","45429c63":"code","6b9ce4d5":"code","8076e4fc":"code","ae6d759f":"code","003a74ff":"code","b8c77341":"code","a5a14056":"code","2ef26a88":"code","f7b0c2e0":"code","9e2e5b07":"code","0454d6ca":"code","d2a0a584":"code","8dd0c46e":"code","8e3cba8d":"code","d9f171f3":"code","7c6fdc75":"code","bba62336":"code","4780f053":"code","78c114e2":"code","31451a56":"code","6faf3b70":"code","2442b134":"code","afb03c0a":"markdown","db9e9bc1":"markdown","7ac5d59a":"markdown","7d3d5134":"markdown","bdc70bfb":"markdown","cdd7760b":"markdown"},"source":{"b8a3a550":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0e74ac53":"df = pd.read_excel('\/kaggle\/input\/nhl-salary-v2\/NHL_Salary_Final_v2.xlsx')","5da51149":"df.head()\n","ad34ba74":"X = df.values[:, 1:176]\nY = df['Salary']","8dfc7f7b":"from sklearn.model_selection import train_test_split\n\n\nx_train, x_test, y_train, y_test = train_test_split (X, Y, test_size = 0.2, random_state = 42)\nx_train, x_val, y_train, y_val = train_test_split (x_train, y_train, test_size = 0.1, random_state = 42)\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\nprint(x_val.shape)\nprint(y_val.shape)\n","f352e40d":"#We will do this if we decide to standardize our data\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x_train)\n\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)\nx_val = scaler.transform(x_val)","87d7d0bd":"import statsmodels\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nfrom statsmodels.formula.api import ols\n\nformula = ''' Salary ~ Year + Month + Day + Pr_St_Val + Country_Val + Nat_Val + Ht + Wt + DftYr + DftRd + Ovrl + Left + Right + LW + RW + C + D + GP + G + A + A1 + A2 + PTS + Plus_Minus + E_Plus_Minus + PIM + Shifts + TOI + TOIX + TOI_GP + TOI% + IPP% + SH% + SV% + PDO + F_60 + A_60 + Pct% + Diff + Diff_60 + iCF + iFF + iSF + ixG + iSCF + iRB + iRS + iDS + sDist + sDist + Pass + iHF + iHF_2 + iHA + iHDf + iMiss + iGVA + iTKA + iBLK + iGVA + iTKA + iBLK + BLK% + iFOW + iFOL + FO% + %FOT + dzFOW + dzFOL + nzFOW + nzFOL + ozFOW + ozFOL + FOW.Up + FOL.Up + FOW.Down + FOL.Down + FOW.Close + FOL.Close + OTG + 1G + GWG + ENG + PSG + PSA + G.Bkhd + G.Dflct + G.Slap + G.Snap + G.Tip + G.Wrap + G.Wrst + CBar + Post + Over + Wide + S.Bkhd + S.Dflct + S.Slap + S.Snap + S.Tip + S.Wrap + S.Wrst + iPenT + iPenD + iPENT_2 + iPEND_2 + iPenDf + NPD + Mins + Maj + Match + Misc + Game + CF + CA + FF + FA + SF + SA + xGF + xGA + SCF + SCA + GF + GA + RBF + RBA + RSF + RSA + DSF + DSA + FOW + FOL + HF + HA + GVA + TKA + PENT + PEND + PS + DPS + PS + OTOI + Grit + DAP + Pace + GS + GS_G + ANA + ARI + BOS + BUF + CAR + CBJ + CGY + CHI + COL + DAL + DET + EDM + FLA + L.A + MIN + MTL + N.J + NSH + NYI + NYR + OTT + PHI + PIT + S.J + STL + T.B + TOR + VAN + WPG + WSH\n'''\n\nX = sm.add_constant(x_train) # adding a constant\n \nmodel = sm.OLS(y_train, X)\n\nresults = model.fit()\n \nprint_model = results.summary()\nprint(print_model)","d88524be":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncorr = df.corr(method ='pearson') \nplt.figure(figsize=(15, 10))\nsns.heatmap(corr)\nplt.show()","45429c63":"df2 = df[['Salary','Year','Wt','DftYr','RW','G','A1','A2','PTS','TOI%','iRB','iRS','iDS',\n'FOW.Up','FOL.Up','S.Bkhd','Game','xGF','RBA','FOL','GS_G','COL','DAL','PIT','TOR',]]\n\ndf2","6b9ce4d5":"X = df2.values[:, 1:25]\nY = df2['Salary']","8076e4fc":"x_train, x_test, y_train, y_test = train_test_split (X, Y, test_size = 0.2, random_state = 42)\nx_train, x_val, y_train, y_val = train_test_split (x_train, y_train, test_size = 0.1, random_state = 42)\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\nprint(x_val.shape)\nprint(y_val.shape)","ae6d759f":"X = sm.add_constant(x_train) # adding a constant\n \nmodel2 = sm.OLS(y_train, X)\n\nresults2 = model2.fit()\n \nprint_model2 = results2.summary()\nprint(print_model2)","003a74ff":"df3 = df[['Salary','Year','DftYr','RW','G','A1','A2','PTS','TOI%',\n'FOW.Up','S.Bkhd','TOR']]\n\ndf3","b8c77341":"X = df3.values[:, 1:12]\nY = df3['Salary']","a5a14056":"x_train, x_test, y_train, y_test = train_test_split (X, Y, test_size = 0.2, random_state = 42)\nx_train, x_val, y_train, y_val = train_test_split (x_train, y_train, test_size = 0.1, random_state = 42)\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\nprint(x_val.shape)\nprint(y_val.shape)","2ef26a88":"X = sm.add_constant(x_train) # adding a constant\n \nmodel3 = sm.OLS(y_train, X)\n\nresults3 = model3.fit()\n \nprint_model3 = results3.summary()\nprint(print_model3)","f7b0c2e0":"from patsy import dmatrices\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","9e2e5b07":"from patsy import dmatrices\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX = df3[['Salary','Year','DftYr','RW','G','A1','A2','PTS','TOI%','FOW.Up','S.Bkhd','TOR']]\nX['Intercept'] = 1\n\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['Variables'] = X.columns","0454d6ca":"vif.round(1)","d2a0a584":"df4 = df[['Salary','Year','DftYr', 'RW', 'TOI%','FOW.Up','S.Bkhd','TOR']]\n\ndf4","8dd0c46e":"X = df4.values[:, 1:8]\nY = df4['Salary']","8e3cba8d":"x_train, x_test, y_train, y_test = train_test_split (X, Y, test_size = 0.2, random_state = 42)\nx_train, x_val, y_train, y_val = train_test_split (x_train, y_train, test_size = 0.1, random_state = 42)\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\nprint(x_val.shape)\nprint(y_val.shape)","d9f171f3":"X = sm.add_constant(x_train) # adding a constant\n \nmodel4 = sm.OLS(y_train, X)\n\nresults4 = model4.fit()\n \nprint_model4 = results4.summary()\nprint(print_model4)","7c6fdc75":"X = df4[['Salary','Year','DftYr', 'RW', 'TOI%','FOW.Up','S.Bkhd','TOR']]\nX['Intercept'] = 1\n\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['Variables'] = X.columns\n\nvif.round(1)","bba62336":"X = sm.add_constant(x_test) # adding a constant\n \nmodel5 = sm.OLS(y_test, X)\n\nresults5 = model5.fit()\n \nprint_model5 = results5.summary()\nprint(print_model5)","4780f053":"df5 = df[['Salary','Year','TOI%','FOW.Up']]\nX = df5.values[:, 1:4]\nY = df5['Salary']","78c114e2":"x_train, x_test, y_train, y_test = train_test_split (X, Y, test_size = 0.2, random_state = 42)\nx_train, x_val, y_train, y_val = train_test_split (x_train, y_train, test_size = 0.1, random_state = 42)\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\nprint(x_val.shape)\nprint(y_val.shape)","31451a56":"X = sm.add_constant(x_train) # adding a constant\n \nmodel5 = sm.OLS(y_train, X)\n\nresults5 = model5.fit()\n \nprint_model5 = results5.summary()\nprint(print_model5)","6faf3b70":"#Let's run against our test dataset\nX = sm.add_constant(x_test) # adding a constant\n \nmodel5 = sm.OLS(y_test, X)\n\nresults5 = model5.fit()\n \nprint_model5 = results5.summary()\nprint(print_model5)","2442b134":"X = sm.add_constant(x_val) # adding a constant\n \nmodel5 = sm.OLS(y_val, X)\n\nresults5 = model5.fit()\n \nprint_model5 = results5.summary()\nprint(print_model5)","afb03c0a":"Let's finally test against our validation dataset","db9e9bc1":"**Values > 10 indicate presence of significant multicollinearity**","7ac5d59a":"The model is still having problems with multicollinearity. Let's reduce the dataset further ","7d3d5134":"The problem with the dataset is that there is a multicollinearity problem. This is because there are predictor variables that are highly correlated with each other. This is seen with variables like country and nationality (0.97), height and weight (0.72). Looking at the heat map above there are sections that are mostly white, which is where there are high correlations. It is possible for us to calculate the VIF (Variance Inflation Factor) score between each variable and our target, which would determine which variables should be removed. It is also possible for us to run a PCA (Principle Component Analysis), which is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n\nTo perform a more simple approach I have decided to bring in only variables that are significant based on our first regression model, and variables that have a high correlation score back to salary. Again, this is not a perfect solution however it is a statistically sound approach. If this model were to be used in production I would suggest performing a PCA analysis or doing some feature engineering.","bdc70bfb":"Based on this let's drop G, A1, A2, Pts","cdd7760b":"As one can see our model dropped in significance as we dropped variables, however our model had a problem with multicollinearity. This is because many of the variables are related or dependent on each other. A VIF test was able to show that this problem wasn't happening once we dropped the majority of the variables, and is shown in the last VIF Factor test\n\nThe process that we performed was very manual however it is possible to do more automated regression modeling and fitting using techniques like the stepwise regression which adds and takes out variables in a more automated process. There are additional advanced techniques such as the PCA or feature engineering that can be done. It is sometimes easy to work with the data when there are only a small number of variables however this isn't always the case. The most important thing is to have a working knowledge of the dataset going into modeling. This way the analyst or data expert can help determine what will be relevant to add into the model and what is simply noise. There is also the possibility of us doing more data transformations to make sure our data is more uniform. We could also transform our target variable salary to account for higher salaries, this was an issue when working with our training data set. Our model was overfitting towards the higher salaries, which is why variables like Right Wing were pulling into this analysis. Overall, I think this was a great exercise to practice working with a large dataset, however there is a lot of work that needs to be done before the model is predicting at a high enough accuracy to be usable."}}