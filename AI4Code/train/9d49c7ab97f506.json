{"cell_type":{"41bd17c0":"code","4e644369":"code","0e25f7a0":"code","27560b3b":"code","a4b3a0d2":"code","2a704081":"code","60cc4b6a":"code","d83f42ec":"code","17ac2485":"code","f3b97799":"code","8307d684":"code","74f044ff":"code","6fcc5c71":"code","d690c402":"code","07e9298c":"code","08cedc6c":"code","2c75d9a0":"code","71a00576":"code","e0e89cf1":"code","89f4e198":"code","627ded34":"code","7a37bc8a":"code","ece26d03":"markdown","018645c4":"markdown","5bcc469b":"markdown"},"source":{"41bd17c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e644369":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport glob","0e25f7a0":"COVID_LABEL_DATA_PATH=\"\/kaggle\/input\/covid-xray-dataset\/labels_Covid.csv\"\nDATA_PATH=\"\/kaggle\/input\/covid-xray-dataset\/DATA\/DATA\/\"\nNORMAL_DATA_PATH=DATA_PATH + '0\/'\nCOVID_DATA_PATH = DATA_PATH + '1\/'","27560b3b":"data=pd.read_csv(COVID_LABEL_DATA_PATH)\ndata.head()","a4b3a0d2":"# X-Ray of Covid patient\nplt.imshow(plt.imread(COVID_DATA_PATH+'person861_virus_1506.jpeg'))","2a704081":"plt.imread(COVID_DATA_PATH+'person861_virus_1506.jpeg').shape","60cc4b6a":"# X-Ray of Normal patient\nplt.imshow(plt.imread(NORMAL_DATA_PATH+'NORMAL2-IM-0856-0001.jpeg'))","d83f42ec":"plt.imread(NORMAL_DATA_PATH+'NORMAL2-IM-0856-0001.jpeg').shape","17ac2485":"files = []\n[files.extend(glob.glob(NORMAL_DATA_PATH + '*.jpeg'))]\n\nif (len(files) == len(os.listdir(NORMAL_DATA_PATH))):\n    print(\"All images for normal patient are of jpeg type and the number of images are {}\".format(len(os.listdir(NORMAL_DATA_PATH))))\nelse:\n    print(\"Some image of normal patient are not of jpeg type\")","f3b97799":"files = []\n[files.extend(glob.glob(COVID_DATA_PATH + '*.jpeg'))]\n\nif (len(files) == len(os.listdir(COVID_DATA_PATH))):\n    print(\"All images for covid patient are of jpeg type and the number of images are {}\".format(len(os.listdir(COVID_DATA_PATH))))\nelse:\n    print(\"Some image of covid patient are not of jpeg type\")","8307d684":"target_image_size = (180,180)","74f044ff":"training_ds=tf.keras.preprocessing.image_dataset_from_directory(DATA_PATH,\n                                                                label_mode='binary',\n                                                                seed=100, \n                                                                validation_split=0.2, \n                                                                subset='training',\n                                                                image_size=target_image_size\n                                                                )","6fcc5c71":"for images, _ in training_ds.take(1):\n    for i in range(1):\n        print(images[i].shape)\n        ","d690c402":"validation_ds=tf.keras.preprocessing.image_dataset_from_directory(DATA_PATH,\n                                                                label_mode='binary',\n                                                                seed=100, \n                                                                validation_split=0.2, \n                                                                subset='validation',\n                                                                image_size=target_image_size\n                                                                )","07e9298c":"for images, _ in validation_ds.take(1):\n    for i in range(1):\n        print(images[i].shape)","08cedc6c":"data_augmentation = tf.keras.Sequential([\n                        tf.keras.layers.RandomFlip(\"horizontal\"),\n                        tf.keras.layers.RandomRotation(0.1)\n                    ])","2c75d9a0":"def make_model(input_shape):\n    inputs=tf.keras.Input(shape=input_shape)\n    x=data_augmentation(inputs)\n    x=tf.keras.layers.Rescaling(1.0\/255)(x)\n    \n    x=tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation='relu')(x)\n    x=tf.keras.layers.BatchNormalization()(x)\n    \n    x=tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation='relu')(x)\n    x=tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    \n    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    return tf.keras.Model(inputs, output)","71a00576":"model = make_model(input_shape=(180,180,3))","e0e89cf1":"model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])","89f4e198":"history = model.fit(training_ds, validation_data=validation_ds, epochs=20)","627ded34":"loss_train = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1,21)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","7a37bc8a":"loss_train = history.history['accuracy']\nloss_val = history.history['val_accuracy']\nepochs = range(1,21)\nplt.plot(epochs, loss_train, 'g', label='Training accuracy')\nplt.plot(epochs, loss_val, 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","ece26d03":"**We can conclude that at the epoch = 10 the model performs well without overfitting and underfitting problem.**","018645c4":"**Import Necessary libraries**","5bcc469b":"**Check whether all the available images of jpeg type**"}}