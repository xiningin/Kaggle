{"cell_type":{"784a7321":"code","592940f9":"code","8f4bf304":"code","aebedcb7":"code","58312920":"code","c45d56b3":"code","7800a887":"code","eea35165":"code","86c2ac04":"code","4b8d741d":"code","3b9702dd":"code","d00cd023":"code","b030036a":"code","7cb7ad03":"code","7be3f102":"code","999c0fc4":"code","ef23fb26":"markdown"},"source":{"784a7321":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom keras.layers import CuDNNLSTM, CuDNNGRU, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.optimizers import Adam\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler","592940f9":"EMBEDDING_FILES = [\n    '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec',\n    '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n]\nNUM_MODELS = 2\nMAX_FEATURES = 100000\nBATCH_SIZE = 512*3\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 1\nMAX_LEN = 220\n","8f4bf304":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')","aebedcb7":"def load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)","58312920":"def build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix","c45d56b3":"def build_model(embedding_matrix, num_aux_targets):\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.3)(x)\n    x2 = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x2 = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x2)\n    x1 = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\n    x1 = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x1)\n    x = add([x1, x2])\n\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(\n        loss='binary_crossentropy',\n        optimizer=Adam(clipnorm=0.1),\n        metrics=['accuracy'])\n\n    return model","7800a887":"train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')","eea35165":"x_train = train['comment_text'].fillna('').values\ny_train = np.where(train['target'] >= 0.5, 1, 0)\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\nx_test = test['comment_text'].fillna('').values","86c2ac04":"tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))","4b8d741d":"x_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","3b9702dd":"embedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)","d00cd023":"checkpoint_predictions = []\nweights = []","b030036a":"for model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            x_train,\n            [y_train, y_aux_train],\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            callbacks=[\n                LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n            ]\n        )\n        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n        weights.append(2 ** global_epoch)","7cb7ad03":"predictions = np.average(checkpoint_predictions, weights=weights, axis=0)","7be3f102":"submission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': predictions\n})\n","999c0fc4":"submission.to_csv('submission.csv', index=False)","ef23fb26":"# Reference\n1. https:\/\/www.kaggle.com\/thousandvoices\/simple-lstm"}}