{"cell_type":{"68b2b52f":"code","7a4b22e3":"code","abf7d3ba":"code","62508ec6":"code","ed533869":"code","d9b5c996":"code","29260c1c":"code","83acb656":"code","221d419d":"code","aae220ba":"code","875edd2d":"code","989bc201":"code","be4f08fd":"code","ceae3eb4":"code","00cde723":"code","adef9b4e":"code","40e9ea33":"code","96b06db0":"code","8b08458f":"code","6a9a323c":"code","672d8788":"code","36ddcc45":"code","8108f999":"code","02b9d543":"code","ffe46e50":"code","e72468eb":"code","a2d0b264":"code","b38ef42f":"code","f9cea2ef":"code","9e165f99":"code","8da448a9":"code","6af30d99":"code","11ea3a84":"code","dbb6dca6":"code","89d19a0d":"code","0bdb44e3":"code","dcf2fe67":"code","669f7faa":"code","74ddd66b":"code","db10603a":"code","4b8b216e":"code","5455b7ee":"code","e461b806":"code","471de3b4":"code","0695f8aa":"code","ef62e4c2":"code","a5e1d70b":"code","2e42eab9":"code","5322aba5":"code","b9b72f6f":"code","6d470e91":"code","4f169310":"code","bd51546f":"code","cef09313":"code","e65ccd54":"code","c583290e":"code","ece9bc50":"code","de726dbb":"code","c4cf5291":"code","417ab9f6":"code","4cb4a61e":"code","3c563665":"code","75e6b71b":"code","4feb2b4e":"code","dff4438a":"code","773aa664":"code","83e9733c":"code","8a585a86":"code","1a4b4f46":"code","35c1f293":"code","91b82ed6":"code","bee6b903":"code","cc322d36":"code","35877cb1":"code","9d3d56fe":"code","ca3893e1":"code","a4c7706e":"code","68952dae":"code","aef4bc10":"code","8ed18fca":"code","0ddbfd13":"code","a7100780":"code","4254d92a":"code","91b55362":"code","0c2192ad":"code","de5723da":"code","97f5451d":"code","07e9c085":"code","ed4a90c7":"code","9a51feb2":"code","c3ca94e1":"code","5cabb051":"code","0a79431a":"code","5a3832f8":"code","bc403a63":"code","19de8796":"code","e60fcf02":"code","dacfa499":"code","08e367c8":"code","7b815e8a":"code","9d9c25ee":"code","21ca599f":"code","75b3a8a1":"code","3f6d3f46":"code","56c10d01":"code","f825c345":"code","933d3a43":"code","eed86531":"code","f45d2960":"code","0925cfd1":"code","74d74788":"code","ac60726a":"code","1324c10f":"code","0f8c9b71":"code","cd592ffb":"code","879cb646":"code","14ba5f47":"code","da9aa829":"code","1808aaff":"markdown","3859aab0":"markdown","f22c5f6a":"markdown","333517d5":"markdown","e56ccd4a":"markdown","98e04abf":"markdown","5aa29ed6":"markdown","2cc330bc":"markdown","742f6c42":"markdown","87ba1df0":"markdown","6281fb65":"markdown","1f7da6d2":"markdown","66e233cd":"markdown","c0804e24":"markdown","5319fea9":"markdown","b4ac634d":"markdown","4df1caec":"markdown","5a5a2bd1":"markdown","72fc1ec1":"markdown","12368884":"markdown","ab4f83e2":"markdown","c25dce46":"markdown","103d3c96":"markdown","d389e5c7":"markdown","1048c51a":"markdown","d500d114":"markdown","857bd55d":"markdown","59e70193":"markdown","f038fdf0":"markdown","0b5f6368":"markdown","6156744f":"markdown","652e5840":"markdown","ba646fd6":"markdown","ddd35a6f":"markdown","9898bac0":"markdown","0ad8f9eb":"markdown","074c9711":"markdown","8e079245":"markdown","dfd42623":"markdown","160450c9":"markdown","631bb34e":"markdown","4b740147":"markdown","bc15e7de":"markdown","20b12b9d":"markdown","b17d9880":"markdown","7129e5e2":"markdown","c3652136":"markdown","0ee3cc7a":"markdown","83013070":"markdown","85c51ccb":"markdown","aac4457e":"markdown","865fc62d":"markdown","a6c35685":"markdown","3c385aa1":"markdown","b2cdf6b1":"markdown","4f439a34":"markdown","d0531dd1":"markdown","438f74f0":"markdown","7d7ce2b6":"markdown","dd944d48":"markdown","8c01492e":"markdown","f3b23a06":"markdown","0cb5af61":"markdown","83d34b5c":"markdown","3fb99152":"markdown","793de7f7":"markdown","f917beb4":"markdown","ed1a9c77":"markdown","f104af6a":"markdown","d7747e88":"markdown","a6073150":"markdown","bd393e74":"markdown","d482f7bd":"markdown","361db707":"markdown","d5e8eb85":"markdown","20b1abd1":"markdown","a84db58c":"markdown","834294bf":"markdown","b5c69852":"markdown","cabe956e":"markdown","a2164dd3":"markdown","b58a0767":"markdown","20e167ec":"markdown","47c5f6ec":"markdown","76427880":"markdown","52e17e43":"markdown"},"source":{"68b2b52f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a4b22e3":"# !pip install pyforest\n# !pip install ipython\n# !pip install pyclustertend\n# !pip install xlrd\n# !pip install Autoviz\n# !pip3 install termcolor\n# !pip install termcolor\n# !pip install pandas-profiling\n# !pip install lightgbm\n# !pip3 install catboost","abf7d3ba":"!pip install pyforest","62508ec6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport scipy.stats as stats\nimport pyforest\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, StandardScaler, PowerTransformer, MinMaxScaler, LabelEncoder, RobustScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold, KFold, cross_val_predict, train_test_split, GridSearchCV, cross_val_score, cross_validate\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge,ElasticNet\nfrom sklearn.metrics import plot_confusion_matrix, r2_score, mean_absolute_error, mean_squared_error, classification_report, confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import make_scorer, precision_score, precision_recall_curve, plot_precision_recall_curve, plot_roc_curve, roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostClassifier\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import plot_tree\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option('max_colwidth',200)\n\n# pd.set_option('display.max_rows', 100) # if you wish to see more rows rather than default, just uncomment this line.\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nimport colorama\nfrom colorama import Fore, Style  # maakes strings colored\n# !pip3 install termcolor\nfrom termcolor import colored\n\n# Import Pandas Profiling\nimport pandas_profiling","ed533869":"###############################################################################\n\ndef missing(df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values\n\ndef missing_values(df):\n    return missing(df)[missing(df)['Missing_Number']>0]\n\n###############################################################################\n\ndef first_looking(df):\n    print(colored(\"Shape:\", attrs=['bold']), df.shape,'\\n',\n          f\"There is \", df.shape[0], \" observation and \", df.shape[1], \" columns in the dataset.\", '\\n',\n          colored('-'*79, 'red', attrs=['bold']),\n          colored(\"\\nInfo:\\n\", attrs=['bold']), sep='')\n    print(df.info(), '\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"Number of Uniques:\\n\", attrs=['bold']), df.nunique(),'\\n',\n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"Missing Values:\\n\", attrs=['bold']), missing_values(df),'\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"All Columns:\", attrs=['bold']), list(df.columns),'\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n\n    df.columns= df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n\n    print(colored(\"Columns after rename:\", attrs=['bold']), list(df.columns),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n\ndef duplicate_values(df):\n    duplicate_values = df.duplicated(subset=None, keep='first').sum()\n    if duplicate_values > 0:\n        df.drop_duplicates(keep='first', inplace=True)\n        print(duplicate_values, colored(\"duplicates were dropped\", attrs=['bold']),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n    else:\n        print(colored(\"No duplicates\", attrs=['bold']),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n        \ndef drop_columns(df, drop_columns):\n    if drop_columns !=[]:\n        df.drop(drop_columns, axis=1, inplace=True)\n        print(drop_columns, 'were dropped')\n    else:\n        print(colored('We will now check the missing values and if necessary drop some columns!!!', attrs=['bold']),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n        \ndef drop_null(df, limit):\n    print('Shape:', df.shape)\n    for i in df.isnull().sum().index:\n        if (df.isnull().sum()[i]\/df.shape[0]*100)>limit:\n            print(df.isnull().sum()[i], 'percent of', i ,'null and were dropped')\n            df.drop(i, axis=1, inplace=True)\n            print('new shape:', df.shape)\n        else:\n            print(df.isnull().sum()[i], '%, percentage of missing values of', i ,'less than limit', limit, '%, so we will keep it.')\n    print('New shape after missing value control:', df.shape)\n\n###############################################################################","d9b5c996":"# Function for determining the number and percentages of missing values\n\ndef missing (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values","29260c1c":"# To view summary information about the column\n\ndef first_look(col):\n    print(\"column name    : \", col)\n    print(\"--------------------------------\")\n    print(\"per_of_nulls   : \", \"%\", round(df[col].isnull().sum()\/df.shape[0]*100, 2))\n    print(\"num_of_nulls   : \", df[col].isnull().sum())\n    print(\"num_of_uniques : \", df[col].nunique())\n    print(df[col].value_counts(dropna = False))","83acb656":"df0 = pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf = df0.copy()","221d419d":"df.head()","aae220ba":"first_looking(df)","875edd2d":"df.profile_report()","989bc201":"df.head(3)","be4f08fd":"df.tail(3)","ceae3eb4":"df.sample(10)","00cde723":"df.columns","adef9b4e":"print(\"There is\", df.shape[0], \"observation and\", df.shape[1], \"columns in the dataset\")","40e9ea33":"df.info()","96b06db0":"df.describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","8b08458f":"df.describe(include=object).T","6a9a323c":"df.nunique()","672d8788":"# to find how many unique values numerical features have\n\nfor col in df.select_dtypes(include=[np.number]).columns:\n  print(colored(f\"{col}\", 'green', attrs=['bold']), f\"feature has\", colored(f\"{df[col].nunique()}\", 'green', attrs=['bold']), f\"unique values.\")","36ddcc45":"# to find how many unique values object features have\n\nfor col in df.select_dtypes(include=\"object\").columns:\n  print(colored(f\"{col}\", 'green', attrs=['bold']), f\"feature has\", colored(f\"{df[col].nunique()}\", 'green', attrs=['bold']), f\"unique values.\")","8108f999":"df.duplicated().value_counts()","02b9d543":"duplicate = df[df.duplicated()]","ffe46e50":"duplicate","e72468eb":"df.shape","a2d0b264":"df.drop(\"customerid\", axis=1, inplace=True)","b38ef42f":"df.drop_duplicates(keep=False, inplace=True)","f9cea2ef":"df.shape","9e165f99":"missing(df)","8da448a9":"df.isnull().melt(value_name=\"missing\")","6af30d99":"plt.figure(figsize=(4, 6))\n\nsns.displot(\n    data=df.isnull().melt(value_name=\"missing\"),\n    y=\"variable\",\n    hue=\"missing\",\n    multiple=\"fill\",\n    height=9.25\n)\n\nplt.axvline(0.3, color=\"r\");","11ea3a84":"first_look('totalcharges')","dbb6dca6":"print(\"Before dealing with blank values\")\nprint(df[df['totalcharges'] == ' '].index) ","89d19a0d":"df['totalcharges'][488]","0bdb44e3":"df['totalcharges']= df['totalcharges'].apply(lambda x: x if x!= ' ' else np.nan).astype(float)","dcf2fe67":"df['totalcharges'][488]","669f7faa":"first_look('totalcharges')","74ddd66b":"# LE = LabelEncoder()\n# df['churn'] = LE.fit_transform(df['churn'])","db10603a":"df['churn']= df['churn'].apply(lambda x: 0 if x==\"No\" else 1).astype(float)","4b8b216e":"# Let's take a quick look at Target variable\n\nfirst_look('churn')","5455b7ee":"y = df['churn']\n\nprint(f'Percentage of \\033[1m\"being churn\"\\033[0m: % {round(y.value_counts(normalize=True)[1]*100,2)} --> \\\n({y.value_counts()[1]} churn cases out of {len(df)})\\nPercentage of \\033[1m\"Not being churn\"\\033[0m: % {round(y.value_counts(normalize=True)[0]*100,2)} --> ({y.value_counts()[0]} NOT churn cases out of {len(df)})')\n\ndf[\"churn\"].value_counts().plot(kind=\"pie\", autopct='%1.1f%%', figsize=(8, 8));","e461b806":"df['churn'].iplot(kind='hist')","471de3b4":"df[df['churn']==0].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","0695f8aa":"df[df['churn']==1].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","ef62e4c2":"print( f\"Skewness: {df['churn'].skew()}\")","a5e1d70b":"print( f\"Kurtosis: {df['churn'].kurtosis()}\")","2e42eab9":"df_skew_temp = df.skew()\ndf_skew_temp = pd.DataFrame(df_skew_temp, columns=[\"skewness_value\"])\ndf_skew_temp","5322aba5":"symetric_features = []\nmoderate_skewed_features = []\nhighly_skewed_features = []\n\nfor col, skew in df_skew_temp.iterrows():\n        if -0.5 < skew[0] < 0.5:\n            symetric_features.append(col)\n            print(f\"The skewness value of\", colored(f\"{skew[0]}\", 'green', attrs=['bold']), \"for\", colored(f\"{col}\", 'green', attrs=['bold']), \"feature means that the distribution is approx.\", colored(f\"symmetric\", 'green', attrs=['bold']))\n        elif (-0.5 < skew[0] < -1.0) or (0.5 < skew[0] < 1.0):  \n            moderate_skewed_features.append(col)\n            print(f\"The skewness value of\", colored(f\"{skew[0]}\", 'yellow', attrs=['bold']), \"for\", colored(f\"{col}\", 'yellow', attrs=['bold']), \"feature means that the distribution is approx.\", colored(f\"moderately symmetric\", 'yellow', attrs=['bold']))\n        else:\n            highly_skewed_features.append(col)\n            print(f\"The skewness value of\", colored(f\"{skew[0]}\", 'red', attrs=['bold']), \"for\", colored(f\"{col}\", 'red', attrs=['bold']), \"feature means that the distribution is approx.\", colored(f\"highly symmetric\", 'red', attrs=['bold']))\n\nprint(colored('*'*120, 'cyan', attrs=['bold']))\nprint(\"\\033[1mThe number of symetric features:\\033[0m\", len(symetric_features))\nprint(\"\\033[1mThe number of moderately skewed features:\\033[0m\", len(moderate_skewed_features)) \nprint(\"\\033[1mThe number of highly skewed features:\\033[0m\", len(highly_skewed_features)) ","b9b72f6f":"#Calculating Kurtosis \n\nkurtosis_limit = 1 \nkurtosis_vals = df.kurtosis()\nkurtosis_cols = kurtosis_vals[abs(kurtosis_vals) > kurtosis_limit].sort_values(ascending=False)\nkurtosis_cols","6d470e91":"plt.figure(figsize=(14, 10))\n\n# Getting the Upper Triangle of the co-relation matrix\nmatrix = np.triu(df.corr())\n\n# using the upper triangle matrix as mask \nsns.heatmap(df.corr(), annot=True, cmap = sns.cubehelix_palette(8), mask=matrix)\n\nplt.xticks(rotation=45);","4f169310":"sns.scatterplot(data=df, x=\"tenure\", y=\"totalcharges\", hue=\"churn\");","bd51546f":"df_temp = df.corr()\n\nfeature =[]\ncollinear=[]\n\nfor col in df_temp.columns:\n    for i in df_temp.index:\n        if (df_temp[col][i]> .85 and df_temp[col][i] < 1) or (df_temp[col][i]< -.85 and df_temp[col][i] > -1) :\n                feature.append(col)\n                collinear.append(i)\n                print(Fore.RED + f\"\\033[1mmulticolinearity alert\\033[0m between {col} - {i}\")\n        else:\n            print(f\"For {col} and {i}, there is \\033[1mNO multicollinearity problem\\033[0m\") \n\nunique_list = list(set(feature+collinear))\n\nprint(colored('*'*80, 'cyan', attrs=['bold']))\nprint(\"\\033[1mThe total number of strong corelated features:\\033[0m\", len(unique_list)) ","cef09313":"df.dtypes","e65ccd54":"df['seniorcitizen']= df['seniorcitizen'].astype(float)\ndf['tenure']= df['tenure'].astype(float)","c583290e":"numerical= df.drop(['churn'], axis=1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns\n\nprint(colored(\"Numerical Columns:\", attrs=['bold']), list(df[numerical].columns),'\\n',\n              colored('-'*124, 'red', attrs=['bold']), sep='')\nprint(colored(\"Categorical Columns:\", attrs=['bold']), list(df[categorical].columns),'\\n',\n              colored('-'*124, 'red', attrs=['bold']), sep='')","ece9bc50":"df[numerical].head().T","de726dbb":"df[numerical].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu').format('{:.2f}')","c4cf5291":"df[numerical].drop(\"seniorcitizen\", axis=1).iplot(kind='hist');","417ab9f6":"df[numerical].drop(\"seniorcitizen\", axis=1).iplot(kind='histogram', subplots=True, bins=50)","4cb4a61e":"for i in df[numerical].drop(\"seniorcitizen\", axis=1):\n    df[i].iplot(kind=\"box\", title=i, boxpoints=\"all\", color='lightseagreen')","3c563665":"index = 0\nplt.figure(figsize=(20,20))\nfor feature in numerical:\n    if feature != \"churn\":\n        index += 1\n        plt.subplot(4, 3, index)\n        sns.boxplot(x='churn', y=feature, data=df)","75e6b71b":"fig = px.scatter_3d(df, \n                    x='monthlycharges',\n                    y='seniorcitizen',\n                    z='tenure',\n                    color='churn')\nfig.show();","4feb2b4e":"sns.pairplot(df, hue=\"churn\", palette=\"inferno\", corner=True);","dff4438a":"df[categorical].head().T","773aa664":"df[categorical].describe().T.style.background_gradient(subset=['unique','freq','count'], cmap='RdPu')","83e9733c":"df[categorical].nunique()","8a585a86":"df[\"onlinesecurity\"].value_counts()","1a4b4f46":"df[\"multiplelines\"]= df[\"multiplelines\"].replace(\"No phone service\", \"No\")","35c1f293":"df[[\"onlinesecurity\", \"onlinebackup\", \"deviceprotection\", \"techsupport\", \"streamingtv\", \"streamingmovies\"]] = df[[\"onlinesecurity\", \"onlinebackup\", \"deviceprotection\", \"techsupport\", \"streamingtv\", \"streamingmovies\"]].replace(\"No internet service\", \"No\")","91b82ed6":"df[categorical].nunique()","bee6b903":"for i, col in enumerate(df[categorical].columns):\n    fig = px.histogram(df[col], color=df[\"churn\"], width=800, height=800, title=col, pattern_shape=df[\"contract\"], pattern_shape_sequence=[\"x\", \".\", \"+\"])\n    fig.show()","cc322d36":"df.head(3)","35877cb1":"sns.swarmplot(y=\"totalcharges\", x=\"contract\", hue=\"churn\", data=df, palette=\"husl\");","9d3d56fe":"sns.swarmplot(y=\"monthlycharges\", x=\"contract\", hue=\"churn\", data=df, palette=\"husl\");","ca3893e1":"sns.swarmplot(y=\"monthlycharges\", x=\"gender\", hue=\"churn\", data=df, palette=\"husl\");","a4c7706e":"for i, col in enumerate(df[categorical].columns):\n    xtab = pd.crosstab(df[col], df[\"churn\"], normalize=True)\n    print(colored('-'*40, 'red', attrs=['bold']), sep='')\n    print(xtab*100)","68952dae":"fig = px.histogram(df, x=df.contract, animation_frame=\"tenure\", facet_col=\"churn\")\nfig.show()","aef4bc10":"fig = px.histogram(df, x=df.tenure, animation_frame=df.contract, facet_col=\"churn\")\nfig.show()","8ed18fca":"plt.figure(figsize=(8, 8))\n\nexplode = [0, 0.1]\nplt.pie(df['churn'].value_counts(), explode=explode, autopct='%1.1f%%', shadow=True, startangle=140)\nplt.legend(labels=['1', '0'])\nplt.title('Churn Distribution')\nplt.axis('off');","0ddbfd13":"df.describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","a7100780":"df.head()","4254d92a":"X = df.drop(\"churn\", axis=1)\ny = df[\"churn\"]","91b55362":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","0c2192ad":"y_train.value_counts(normalize=True)*100","de5723da":"y_test.value_counts(normalize=True)*100","97f5451d":"X_train","07e9c085":"X_train.isnull().sum()","ed4a90c7":"X_test.isnull().sum()","9a51feb2":"from sklearn.impute import SimpleImputer\n\n# Imputation\n\nimputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n\nX_train.totalcharges = imputer.fit_transform(X_train[\"totalcharges\"].values.reshape(-1, 1))\nX_test.totalcharges = imputer.transform(X_test[\"totalcharges\"].values.reshape(-1, 1))","c3ca94e1":"X_train.isnull().sum()","5cabb051":"X_test.isnull().sum()","0a79431a":"df[categorical].head()","5a3832f8":"OHE = OneHotEncoder(sparse=False, drop=\"if_binary\")\n\nOHE_df = OHE.fit_transform(df[categorical])\nOHE_df = pd.DataFrame(OHE_df, columns=OHE.get_feature_names(df[categorical].columns))\n\nOHE_df.head()","bc403a63":"categorical","19de8796":"# Libraries Needed in This Section\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, recall_score, classification_report, roc_auc_score, f1_score, accuracy_score","e60fcf02":"transformer = ColumnTransformer([(\"OHE\", OneHotEncoder(drop=\"if_binary\"), categorical)], \n                                remainder=\"passthrough\")","dacfa499":"transformer","08e367c8":"models = []\n\nmodels.append((\"LR\", LogisticRegression()))\nmodels.append((\"SVC\", SVC()))\nmodels.append((\"KNN\", KNeighborsClassifier()))\nmodels.append((\"DT\", DecisionTreeClassifier()))\nmodels.append((\"RF\", RandomForestClassifier()))\nmodels.append((\"ADA\", AdaBoostClassifier()))\nmodels.append((\"GB\", GradientBoostingClassifier()))\nmodels.append((\"XGB\", XGBClassifier(eval_metric='mlogloss')))\n\n# evaluate each model in turn\n\nresults = []\nnames = []\nf1_scores = []\nrecall_scores = []\nroc_auc_scores = []\nacc_scores = []\n\nfor name, model in models:\n    pipe = Pipeline([(\"transformer\", transformer),\n                     (\"model\", model)])\n    kfold = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n    cv_results = cross_val_score(pipe, X_train, y_train, cv=kfold, scoring=\"recall\")\n    \n    results.append(cv_results)\n    names.append(name)\n\n    y_pred = pipe.fit(X_train, y_train).predict(X_test)\n    \n    f1_scores.append(f1_score(y_test, y_pred))\n    recall_scores.append(recall_score(y_test, y_pred))\n    roc_auc_scores.append(roc_auc_score(y_test, y_pred))\n    acc_scores.append(accuracy_score(y_test, y_pred))    \n    \n\nresult_df = pd.DataFrame(results, columns=[i for i in range(1, 11)], index=names).T\nresult_df.iplot(kind=\"box\", boxpoints=\"all\", title=\"CV Results\")\n\ncompare = pd.DataFrame({\"F1\": f1_scores,\n                        \"Recall\": recall_scores,\n                        \"ROC AUC\": roc_auc_scores, \n                        \"Accuracy\": acc_scores \n                       }, index=names)\n\ncompare\n\nfor score in compare.columns:\n    compare[score].sort_values().iplot(kind=\"barh\", title=f\"{score} Scores\")","7b815e8a":"result_df = pd.DataFrame(results, columns=[i for i in range(1, 11)], index=names).T\nModel_CVscores_Mean = pd.DataFrame(result_df.mean()[:]).T\nframes = [result_df, Model_CVscores_Mean]\ncompared_CVscores = pd.concat(frames)\ncompared_CVscores = compared_CVscores.set_axis(['1st CV', '2nd CV', '3rd CV', '4th CV', '5th CV', '6th CV', '7th CV', '8th CV', '9th CV', '10th CV', \"Model CVscores Mean\"]) \ncompared_CVscores.style.apply(lambda x: ['background: yellow' if x.name in [\"Model CVscores Mean\"] else '' for i in x], axis=1)\\\n                       .highlight_max(color = 'lightgreen', axis = 0)","9d9c25ee":"comp_model_results1 = pd.DataFrame(compared_CVscores.iloc[10]).rename(columns={'Model CVscores Mean': 'With Default Values1'})\ncomp_model_results1.style.highlight_max(color = 'lightgreen', axis = 0)","21ca599f":"transformer = ColumnTransformer([(\"OHE\", OneHotEncoder(drop=\"if_binary\"), categorical)], \n                                remainder=\"passthrough\")","75b3a8a1":"models = []\n\nmodels.append((\"LR\", LogisticRegression(class_weight=\"balanced\")))\nmodels.append((\"SVC\", SVC(class_weight=\"balanced\")))\nmodels.append((\"KNN\", KNeighborsClassifier()))\nmodels.append((\"DT\", DecisionTreeClassifier(class_weight=\"balanced\")))\nmodels.append((\"RF\", RandomForestClassifier(class_weight=\"balanced\")))\nmodels.append((\"ADA\", AdaBoostClassifier()))\nmodels.append((\"GB\", GradientBoostingClassifier()))\nmodels.append((\"XGB\", XGBClassifier(eval_metric='mlogloss')))\n\n# evaluate each model in turn\n\nresults = []\nnames = []\nf1_scores = []\nrecall_scores = []\nroc_auc_scores = []\nacc_scores = []\n\nfor name, model in models:\n    pipe = Pipeline([(\"transformer\", transformer),\n                     (\"model\", model)])\n    kfold = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n    cv_results = cross_val_score(pipe, X_train, y_train, cv=kfold, scoring=\"recall\")\n    \n    results.append(cv_results)\n    names.append(name)\n   \n    y_pred = pipe.fit(X_train, y_train).predict(X_test)\n    \n    f1_scores.append(f1_score(y_test, y_pred))\n    recall_scores.append(recall_score(y_test, y_pred))\n    roc_auc_scores.append(roc_auc_score(y_test, y_pred))\n    acc_scores.append(accuracy_score(y_test, y_pred))    \n    \n\nresult_df = pd.DataFrame(results, columns=[i for i in range(1, 11)], index=names).T\nresult_df.iplot(kind=\"box\", boxpoints=\"all\", title=\"CV Results\")\n\ncompare = pd.DataFrame({\"F1\": f1_scores,\n                        \"Recall\": recall_scores,\n                        \"ROC AUC\": roc_auc_scores, \n                        \"Accuracy\": acc_scores \n                       }, index=names)\n\ncompare\n\nfor score in compare.columns:\n    compare[score].sort_values().iplot(kind=\"barh\", title=f\"{score} Score\")","3f6d3f46":"result_df = pd.DataFrame(results, columns=[i for i in range(1, 11)], index=names).T\nModel_CVscores_Mean = pd.DataFrame(result_df.mean()[:]).T\nframes = [result_df, Model_CVscores_Mean]\ncompared_CVscores = pd.concat(frames)\ncompared_CVscores = compared_CVscores.set_axis(['1st CV', '2nd CV', '3rd CV', '4th CV', '5th CV', '6th CV', '7th CV', '8th CV', '9th CV', '10th CV', \"Model CVscores Mean\"]) \ncompared_CVscores.style.apply(lambda x: ['background: yellow' if x.name in [\"Model CVscores Mean\"] else '' for i in x], axis=1)\\\n                       .highlight_max(color = 'lightgreen', axis = 0)","56c10d01":"comp_model_results2 = pd.DataFrame(compared_CVscores.iloc[10]).rename(columns={'Model CVscores Mean': 'With Default Values2'})\n\ncomp_model_results2 = pd.concat([comp_model_results1, comp_model_results2], axis=1)\ncomp_model_results2.style.highlight_max(color = 'lightgreen', axis = 0)","f825c345":"transformer = ColumnTransformer([\n    (\"OHE\", OneHotEncoder(drop=\"if_binary\"), categorical),\n    (\"scaler\", StandardScaler(), numerical)\n], remainder=\"passthrough\")  # We added StandardScaler","933d3a43":"models = []\n\nmodels.append((\"LR\", LogisticRegression(class_weight=\"balanced\")))\nmodels.append((\"SVC\", SVC(class_weight=\"balanced\")))\nmodels.append((\"KNN\", KNeighborsClassifier()))\nmodels.append((\"DT\", DecisionTreeClassifier(class_weight=\"balanced\")))\nmodels.append((\"RF\", RandomForestClassifier(class_weight=\"balanced\")))\nmodels.append((\"ADA\", AdaBoostClassifier()))\nmodels.append((\"GB\", GradientBoostingClassifier()))\nmodels.append((\"XGB\", XGBClassifier(eval_metric='mlogloss')))\n\n# evaluate each model in turn\n\nresults = []\nnames = []\nf1_scores = []\nrecall_scores = []\nroc_auc_scores = []\nacc_scores = []\n\nfor name, model in models:\n    pipe = Pipeline([(\"transformer\", transformer),\n                     (\"model\", model)])\n    kfold = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n    cv_results = cross_val_score(pipe, X_train, y_train, cv=kfold, scoring=\"recall\")\n    \n    results.append(cv_results)\n    names.append(name)\n   \n    y_pred = pipe.fit(X_train, y_train).predict(X_test)\n    \n    f1_scores.append(f1_score(y_test, y_pred))\n    recall_scores.append(recall_score(y_test, y_pred))\n    roc_auc_scores.append(roc_auc_score(y_test, y_pred))\n    acc_scores.append(accuracy_score(y_test, y_pred))    \n    \n\nresult_df = pd.DataFrame(results, columns=[i for i in range(1, 11)], index=names).T\nresult_df.iplot(kind=\"box\", boxpoints=\"all\", title=\"CV Results\")\n\ncompare = pd.DataFrame({\"F1\": f1_scores,\n                        \"Recall\": recall_scores,\n                        \"ROC AUC\": roc_auc_scores, \n                        \"Accuracy\": acc_scores \n                       }, index=names)\n\ncompare\n\nfor score in compare.columns:\n    compare[score].sort_values().iplot(kind=\"barh\", title=f\"{score} Score\")","eed86531":"result_df = pd.DataFrame(results, columns=[i for i in range(1, 11)], index=names).T\nModel_CVscores_Mean = pd.DataFrame(result_df.mean()[:]).T\nframes = [result_df, Model_CVscores_Mean]\ncompared_CVscores = pd.concat(frames)\ncompared_CVscores = compared_CVscores.set_axis(['1st CV', '2nd CV', '3rd CV', '4th CV', '5th CV', '6th CV', '7th CV', '8th CV', '9th CV', '10th CV', \"Model CVscores Mean\"]) \ncompared_CVscores.style.apply(lambda x: ['background: yellow' if x.name in [\"Model CVscores Mean\"] else '' for i in x], axis=1)\\\n                       .highlight_max(color = 'lightgreen', axis = 0)","f45d2960":"comp_model_results3 = pd.DataFrame(compared_CVscores.iloc[10]).rename(columns={'Model CVscores Mean': 'With Default Values3'})\n\ncomp_model_results3 = pd.concat([comp_model_results2, comp_model_results3], axis=1)\ncomp_model_results3.style.highlight_max(color = 'lightgreen', axis = 0)","0925cfd1":"transformer = ColumnTransformer([\n    (\"OHE\", OneHotEncoder(drop=\"if_binary\"), categorical),\n    (\"scaler\", StandardScaler(), numerical)\n], remainder=\"passthrough\")","74d74788":"pipe = Pipeline([(\"preprocess\", transformer),\n                 (\"classifier\", LogisticRegression(class_weight=\"balanced\"))])\n\npenalty = [\"l1\", \"l2\"]\nl1_ratio = np.linspace(0, 1, 20)\nC = [0.001 ,0.01, 0.1, 1, 10, 100, 1000]\n\nparam_grid = {\"classifier__penalty\" : penalty,\n             \"classifier__l1_ratio\" : l1_ratio,\n             \"classifier__C\" : C}\n\nkfold = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\nLR_grid = GridSearchCV(pipe, param_grid=param_grid, cv=kfold, scoring=\"recall\", verbose=1)\n\nLR_grid.fit(X_train, y_train)","ac60726a":"print(colored('\\033[1mBest Parameters of GridSearchCV for LR Model:\\033[0m', 'blue'), colored(LR_grid.best_params_, 'cyan'))\nprint(\"--------------------------------------------------------------------------------------------------------------------\")\nprint(colored('\\033[1mBest Estimator of GridSearchCV for LR Model:\\033[0m', 'blue'), colored(LR_grid.best_estimator_, 'cyan'))","1324c10f":"y_pred = LR_grid.predict(X_test)\n\nlog_grid_f1 = f1_score(y_test, y_pred)\nlog_grid_acc = accuracy_score(y_test, y_pred)\nlog_grid_recall = recall_score(y_test, y_pred)\nlog_grid_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")","0f8c9b71":"cf_matrix = confusion_matrix(y_test, y_pred)\n\ngroup_names = [\"True Negatives (TN)\", \"False Positives (FP)\\n(Type I Error)\", \"False Negatives (FN)\\n(Type II Error)\", \"True Positives (TP)\"]\n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\n\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names, group_counts, group_percentages)]\n\nlabels = np.asarray(labels).reshape(2, 2)\n\nax = sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap=\"Blues\")\nax.set(xlabel=\"Predicted Class\", ylabel = \"Actual Claass\");","cd592ffb":"%matplotlib inline\nplot_roc_curve(LR_grid, X_test, y_test, response_method='auto');","879cb646":"f1_scores.append(log_grid_f1)\nrecall_scores.append(log_grid_recall)\nroc_auc_scores.append(log_grid_auc)\nacc_scores.append(log_grid_acc)","14ba5f47":"compare = pd.DataFrame({\"Model\": [\"Logistic Regression\", \"SVM\", \"KNN\", \"Decision Tree\", \"Random Forest\", \"AdaBoost\", \"GradientBoost\", \"XGBoost\", \"Logistic Regression with GridSearch\"],\n                        \"F1 Scores\": f1_scores,\n                        \"Recall Scores\": recall_scores,\n                        \"ROC AUC Scores\": roc_auc_scores,\n                        \"Accuracy Scores\": acc_scores})\n\ncompare.style.highlight_max(color = 'lightgreen', subset=[\"Recall Scores\"], axis = 0)","da9aa829":"def labels(ax):\n    for p in ax.patches:\n        width = p.get_width()                        # get bar length\n        ax.text(width,                               # set the text at 1 unit right of the bar\n                p.get_y() + p.get_height() \/ 2,      # get Y coordinate + X coordinate \/ 2\n                '{:1.3f}'.format(width),             # set variable to display, 2 decimals\n                ha = 'left',                         # horizontal alignment\n                va = 'center')                       # vertical alignment\n    \nfig = plt.figure(figsize=(14, 14))\n\nplt.subplot(411)\ncompare = compare.sort_values(by=\"Recall Scores\", ascending=False)\nax=sns.barplot(x=\"Recall Scores\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\n\nplt.subplot(412)\ncompare = compare.sort_values(by=\"F1 Scores\", ascending=False)\nax=sns.barplot(x=\"F1 Scores\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\n\nplt.subplot(413)\ncompare = compare.sort_values(by=\"ROC AUC Scores\", ascending=False)\nax=sns.barplot(x=\"ROC AUC Scores\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\n\nplt.subplot(414)\ncompare = compare.sort_values(by=\"Accuracy Scores\", ascending=False)\nax=sns.barplot(x=\"Accuracy Scores\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\n\ndisplay(fig)","1808aaff":"**Let's first create our models with default parameters WITH class_weight AND Scaling and examine their scores visually.**","3859aab0":"**In our given study, the AUC score is 0.86 meaning that it can be considered as excellent since it's between 0.8 to 0.9.** ","f22c5f6a":"<a id=\"6.4\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.4 Modelling Logistic Regression (class_weight AND Scaling) With GridSearchCV Best Parameters<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","333517d5":"**In the 'totalcharges' feature there have been 11 missing values coded as \" \".**","e56ccd4a":"**Let's handle with missing values using SimpleImputer before modelling to prevent data leakage.**","98e04abf":"<a id=\"7.2\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">7.2 Visual Comparison of Model Scores<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","5aa29ed6":"**Let's remove \"customerID\" column from the given dataset since it is NOT useful for any classification in our analysis.**","2cc330bc":"**It can be concluded that there has been weak correlations between the numerical features and the target variable while a strong correlation between \"tenure\" and \"totalcharges\". Nevertheless, if we accept the level of 0.85 as criteria for multicollinearity, it can be concluded that there is no multi-colliniearity problem between \"tenure\" and \"totalcharges\".** \n\nBased on the matrix, we can observe weak level correlation between the numerical features and the target variable\nThere is strong correlation between tenure and total charges.\nBeing senior citizen and increasing monthly charges have a positive correlation with the churn.\nSenior citizen more likely churn than younger customers.\nCustomer with higher monthly charges also more likely churn than lesser monthly charges customers.\nBeing long term with the company, customer less likely churn than customer with lesser time with the company.\nTotal charges has negative correlation with the churn.","742f6c42":"**Additionally, as you remember from the previous session, even if data type of \"totalcharges\" column is object type, it has numerical values. So we also need to change its data type to float type.**","87ba1df0":"**You can use first_looking(df) user defined function above for getting a general insight before going further in the analysis**","6281fb65":"<a id=\"4.6.3\"><\/a>\n#### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.6.3 The Examination of Categorical Features<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","1f7da6d2":"**You can plot confusion matrix directly via:**  plot_confusion_matrix(LR_grid, y_test, y_pred)\n\n**However, we will use seaborn library for plotting this time for getting a better visualization.**","66e233cd":"<a id=\"2.1\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">2.1 Context<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**\"Predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\" [IBM Sample Data Sets]**\n\n**Each row represents a customer, each column contains customer\u2019s attributes described on the column Metadata.**\n\n**The data set includes information about:**\n\n- **Customers who left within the last month \u2013 the column is called Churn**\n- **Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device -protection, tech support, and streaming TV and movies**\n- **Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges**\n- **Demographic info about customers \u2013 gender, age range, and if they have partners and dependents**\n\n**To explore this type of models and learn more about the subject.**\n\n**[New version](https:\/\/community.ibm.com\/community\/user\/businessanalytics\/blogs\/steven-macko\/2019\/07\/11\/telco-customer-churn-1113) from IBM.**","c0804e24":"**We will encode the target variable of \"churn\" to binary variable here. However, some of you, as a second option, prefer to perform label encoding using the following code. It's up to you**","5319fea9":"**Let's first create our models with default parameters WITHOUT Scaling & class_weight and examine their scores visually.**","b4ac634d":"**Even if they are in small size, we have some missing values in X_train and X_test sets. Now let's handle with them by filling mean.**","4df1caec":"**Now, let's examine crosstab outputs for each variable.**","5a5a2bd1":"## If you like this kernel PLEASE upvote it \ud83d\ude4f and feel free to use and reference it  \u2764\ufe0f","72fc1ec1":"**When we focus on \"churn\" part (color=1) in each category, it's easy captured that \"Month-to-month\" contract type comprises most of being churn.**","12368884":"**Let's examine Cross Validation scores for each model.**","ab4f83e2":"<a id=\"4.7\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.7 Descriptive Statistics<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","c25dce46":"**Interpreting Kurtosis**\n\n1.0 is our threshold-limit to evaluate kurtosis.","103d3c96":"**Having applied for default parameters WITH scalling AND class_weight, Logistic Regression (LR) still have the best score of 0.799107 among the results of other models with a little increase from a value of 0.791058 to 0.799107; however, it's obvious that Support Vector Machine (SVM) score obtained a considerable increase from a value of 0.557174 to 0.793044.**","d389e5c7":"<a id=\"6.2\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.2 Modelling Default Parameters WITH class_weight BUT Without Scaling<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","1048c51a":"**It looks like there has NOT been a high cardinality issue.**","d500d114":"**Now we can compare each categorical variable with target variable for the bivariate analysis.**","857bd55d":"<a id=\"1.1\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">1.1 User Defined Functions<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**We have defined some useful user defined functions**","59e70193":"<a id=\"7.1\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">7.1 Tabular Comparison of Model Scores<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","f038fdf0":"<a id=\"toc\"><\/a>\n\n## <h3 style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\" class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tablist\" aria-controls=\"home\">TABLE OF CONTENTS<\/h3>\n\n* [   PREFACE](#0)\n* [1) LIBRARIES NEEDED IN THE STUDY](#1)\n    * [1.1 User Defined Functions](#1.1)\n* [2) DATA](#2)\n    * [2.1 Context](#2.1)\n    * [2.2 About the Features](#2.2) \n    * [2.3 What the Problem is](#2.3) \n* [3) ANALYSIS](#3)\n    * [3.1) Reading the Data](#3)\n* [4) EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION](#4)\n    * [4.1 - A General Looking at the Data](#4.1)\n    * [4.2 - Convert Multi-Index Columns To One Level](#4.2)\n    * [4.3 - Handling with Missing Values](#4.3)\n    * [4.4 The Examination of Target Variable](#4.4)\n    * [4.5 - The Examination of Skewness, Kurtosis & Multicollinearity](#4.5)\n    * [4.6 Numerical vs. Categorical](#4.6) \n        * [4.6.1 Spliting Dataset into Numeric & Categoric Features](#4.6.1)\n        * [4.6.2 The Examination of Numerical Features](#4.6.2)\n        * [4.6.3 The Examination of Categorical Features](#4.6.3)\n    * [4.7 Descriptive Statistics](#4.7) \n* [5) DATA PREPROCESSING](#5)     \n    * [5.1 The Implementation of Scaling](#5.1)\n* [6) 6 - MODELLING WITH MULTIPLE ALGORITHMS](#6)        \n    * [6.1 Modelling Default Parameters WITHOUT Scalling & class_weight](#6.1)\n    * [6.2 Modelling Default Parameters WITH class_weight BUT Without Scalling](#6.2)\n    * [6.3 Modelling Default Parameters WITH class_weight & Scaling](#6.3)\n    * [6.4 Modelling Logistic Regression (class_weight AND Scaling) With GridSearchCV Best Parameters](#6.4)\n* [7) THE VISUAL\/TABULAR COMPARISON & EVALUATION OF MODELS](#7)\n    * [7.1 Tabular Comparison of Model Scores](#7.1)\n    * [7.2 Visual Comparison of Model Scores](#7.2)\n* [8) CONLUSION](#8)\n* [9) REFERENCES](#9)\n* [10 FURTHER READINGS](#10)","0b5f6368":"**Let's examine Cross Validation scores for each model.**","6156744f":"Kurtosis are of three types:\n\nMesokurtic: When the tails of the distibution is similar to the normal distribution then it is mesokurtic. The kutosis for normal distibution is 3.\n\nLeptokurtic: If the kurtosis is greater than 3 then it is leptokurtic. In this case, the tails will be heaviour than the normal distribution which means lots of outliers are present in the data. It can be recognized as thin bell shaped distribution with peak higher than normal distribution.\n\nPlatykurtic: Kurtosis will be less than 3 which implies thinner tail or lack of outliers than normal distribution.In case of platykurtic, bell shaped distribution will be broader and peak will be lower than the mesokurtic. Hair et al. (2010) and Bryne (2010) argued that data is considered to be normal if Skewness is between \u20102 to +2 and Kurtosis is between \u20107 to +7.\n\nMulti-normality data tests are performed using leveling asymmetry tests (skewness < 3), (Kurtosis between -2 and 2) and Mardia criterion (< 3). Source Chemingui, H., & Ben lallouna, H. (2013).\n\nSkewness and kurtosis index were used to identify the normality of the data. The result suggested the deviation of data from normality was not severe as the value of skewness and kurtosis index were below 3 and 10 respectively (Kline, 2011). Source Yadav, R., & Pathak, G. S. (2016).","652e5840":"**On the other hand, when we consider the descriptions about features in the \"2.2 About The Features\" of this study, some features such as \"multiplelines\", \"onlinesecurity\", \"onlinebackup\", \"deviceprotection\", \"techsupport\", \"streamingtv\" and \"streamingmovies\" have more categories than expected. Therefore, we have to catagorize them to \"No\" and \"Yes\" in consistent with their orginal form.**","ba646fd6":"<a id=\"4.4\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.4 The Examination of Target Variable<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","ddd35a6f":"**Let's first create our models with default parameters WITH class_weight BUT without Scaling and examine their scores visually.**","9898bac0":"**Overall Distribution of Target (\"Churn\") Variable.**","0ad8f9eb":"<a id=\"2.3\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">2.3 What The Problem Is<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**- In the given study, we have a classification problem.**\n\n- We will try to create classifications on the target variable in the given dataset.\n- The Target variable in the given dataset is \"Churn\"\n- To make the best classification possible on the target variable, we will focus on if the target variable is balanced or not.\n- As you will be familiar with later, the target variable in this study is an imblanced data so this researcher will concentrate Recall score on evaluating the results rather than Accuracy score used in the evaluation of balanced data.\n- Lastly we will build a variety of Classification models and compare the models giving the best prediction on Churn.","074c9711":"<a id=\"4.6\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.6 Numerical vs. Categorical<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","8e079245":"**However, when we try to change its data type to float type, we have grasped that \"totalcharges\" feature has some missing values of \" \". So we will take care of both issues in the \"Handling with Missing Values\" section.**","dfd42623":"**Considering the results with default parameters without scalling & class_weight when we examine, it's interesting that Support Vector Machine (SVM) could NOT learn anything (what a shame) while Logistic Regression (LR) provided the best score of 0.538350 among the results of other models.**\t","160450c9":"**We don't see any missing values as we examine the dataset in general; however, when we look over each feature one by one in detail, we have grasped that \"totalcharges\" feature has some missing values since while converting its data type.**","631bb34e":"**The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.**\n\n**According to [Academic Literature](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1556086415306043), in general, an AUC of 0.5 suggests no discrimination (i.e., ability to diagnose patients with and without the disease or condition based on the test), 0.7 to 0.8 is considered acceptable, 0.8 to 0.9 is considered excellent, and more than 0.9 is considered outstanding.**","4b740147":"<a id=\"8\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">8 - CONCLUSION<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","bc15e7de":"<a id=\"7\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7 - THE VISUAL\/TABULAR COMPARISON & EVALUATION OF MODELS<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","20b12b9d":"- Kline, R.B. (2011). Principles and practice of structural equation modeling (5th ed., pp. 3-427). New York:The Guilford Press.\n- Edwards, A. (1976). An introduction to linear regression and correlation. W. H. Freeman\n- Everitt, B. S.; Skrondal, A. (2010), The Cambridge Dictionary of Statistics, Cambridge University Press.\n- https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1\n- https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W\n- https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1\n- https:\/\/www.amazon.com\/Introduction-Machine-Learning-Python-Scientists\/dp\/1449369413\n- Neural Networks from Scratch in Python (by Kinsley \u00a7 Kukiela) [external link text](https:\/\/nnfs.io\/)\n- Practical Statistics for Data Scientists (by Bruce & Gedeck) [external link text](https:\/\/www.amazon.com\/Practical-Statistics-Data-Scientists-Essential\/dp\/149207294X\/ref=sr_1_1?dchild=1&keywords=Practical+Statistics+for+Data+Scientists&qid=1627662007&sr=8-1)\n- Applications of Deep Neural Networks(by Jeff Heaton) [external link text](https:\/\/arxiv.org\/abs\/2009.05673)\n- Applied Predictive Modeling (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Applied-Predictive-Modeling-Max-Kuhn\/dp\/1461468485\/ref=pd_sbs_3\/141-4288971-3747365?pd_rd_w=AOIS7&pf_rd_p=3676f086-9496-4fd7-8490-77cf7f43f846&pf_rd_r=MCCHJXWK39VD6VW7RVAR&pd_rd_r=4ffcd1ea-44b9-4f33-b9b3-dc02ee159662&pd_rd_wg=nU1Ex&pd_rd_i=1461468485&psc=1:)\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (by Aur\u00e9lien G\u00e9ron) [external link text](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646\/ref=sr_1_1?crid=2GV554Q2EKD1E&dchild=1&keywords=hands-on+machine+learning+with+scikit-learn%2C+keras%2C+and+tensorflow&qid=1627628294&s=books&sprefix=hands%2Cstripbooks-intl-ship%2C309&sr=1-1)\n- Master Machine Learning Algorithms (by Brownlee, ML algorithms are very well explained ) [external link text](https:\/\/machinelearningmastery.com\/master-machine-learning-algorithms\/)\n- Python Feature Engineering Cookbook (by Galli) [external link text](https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1)\n- Feature Engineering Made Easy (by Ozdemir & Susarla) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W)\n- Feature Engineering and Selection (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1)\n- Imbalanced Classification with Python(by Brownlee) [external link text](https:\/\/machinelearningmastery.com\/imbalanced-classification-with-python\/)\n- https:\/\/www.analyticsvidhya.com\/blog\/2021\/06\/generate-reports-using-pandas-profiling-deploy-using-streamlit\/\n- https:\/\/towardsdatascience.com\/exploratory-data-analysis-with-pandas-profiling-de3aae2ddff3\n- https:\/\/analyticsindiamag.com\/why-data-scaling-is-important-in-machine-learning-how-to-effectively-do-it\/\n- https:\/\/www.analyticsvidhya.com\/blog\/2021\/01\/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning\/\n- https:\/\/www.analyticsvidhya.com\/blog\/2021\/01\/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning\/\n- https:\/\/www.kaggle.com\/kadirduran\/military-power-clustering\n- https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/feature-scaling-machine-learning-normalization-standardization\/\n- https:\/\/machinelearningmastery.com\/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling\/\n- https:\/\/machinelearningmastery.com\/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\/#:~:text=Early%20stopping%20is%20a%20method,deep%20learning%20neural%20network%20models.\n- https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1556086415306043","b17d9880":"**Similar to patterns in histogram, it's clearly obvious that \"churn\" is prevalent in \"Month-to-month\" contract by \"totalcharges\" and \"monthlycharges\" and among female \"monthlycharges\".**","7129e5e2":"<a id=\"4.5\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.5 - The Examination of Skewness, Kurtosis & Multicollinearity<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","c3652136":"**SPECIAL NOTE: In the given dataset, there have been 11 missing values in the column of \"totalcharges\". These missing values will be handled with after Train & Split process for preventing data leakage.**","0ee3cc7a":"**As easily understood from the plots, we have an imbalanced data.**\n\n**% 26.35 of customers, who represent 1845 people out of 7001, didn't show loyalty to the company and got churned while % 73.65 of which representing 5156 people out of 7001 showed loyalty to the company.**\n","83013070":"**Let's examine Cross Validation scores for each model.**","85c51ccb":"Let's check if there is a multicollinearity problem among the features.","aac4457e":"<a id=\"4.3\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.3 Handling With Missing Values<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Missing data (or missing values) is defined as the data value that is not stored for a variable in the observation of interest. The problem of missing data is relatively common in almost all research and can have a significant effect on the conclusions that can be drawn from the data. Accordingly, all studies need to focus on handling the missing data, problems caused by missing data, and the methods to avoid or minimize their effects on the analysis. Otherwise, missing data can reduce the statistical power of a study and can produce biased estimates, leading to invalid conclusions.**\n\n**For a better understanding and more information how to handle with missing values in Machine Learning, please refer to [external link text](https:\/\/machinelearningmastery.com\/handle-missing-data-python\/) & [external link text](https:\/\/www.kaggle.com\/kaanboke\/the-most-used-methods-to-deal-with-missing-values)**","865fc62d":"**Let's find best parameters with GridSearchCV for our Logistic Regression model WITH class_weight AND Scaling and examine their scores visually.**","a6c35685":"<a id=\"6\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">6 - MODELLING WITH MULTIPLE ALGORITHMS<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","3c385aa1":"<a id=\"5\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">5 - DATA PREPROCESSING<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","b2cdf6b1":"**Having created our models with default and optional parameters, Logistic Regression (LR) had the best score of 0.799107 among the other models; however, it's obvious that Support Vector Machine (SVM) score obtained a considerable increase from a value of 0.557174 to 0.793044 when we scaled our data.**\n\n**In our given study, the Logistic Regression with the best parameters of GridSearchCV revealed a value of 0.86 meaning that it can be considered as excellent since it's between 0.8 to 0.9.**\n\n**The examination of F1, Recall, ROC AUC and Accuracy scores for each model we created demonstrated that except the \"Accuracy\" score, Logistic Regression seems to the best model for the evaluation of \"churn\" specific to this study at hand.**\n\n**Moreover, in this study we also experienced how scaling and using class_weight parameter is crucial in some cases such as imbalaned data to enhance model credibility.**","4f439a34":"<a id=\"3\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">3 - ANALYSIS<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","d0531dd1":"**A General Look At The Missing Values**","438f74f0":"**Plotly library is very useful for data visualization and understanding the data simply and easily. One of the visualization functions is scatter_3d() which is used to create a 3D scatter plot and can be used with pandas dataframes.** \n\n**3D scatter plots are used to plot data points on three axes in an attempt to show the relationship between three variables. Each row in the data table is represented by a marker whose position depends on its values in the columns set on the X, Y, and Z axes. A fourth variable can be set to correspond to the color or size of the markers; thus, adding yet another dimension to the plot.**","7d7ce2b6":"<a id=\"2\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">2 - DATA<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**This is the Telco customer churn data that contains information about a fictional telco company providing home phone and internet services to 7043 customers in California in Q3. It indicates which customers have left, stayed, or signed up for their service. Multiple important demographics are included for each customer, as well as a Satisfaction Score, Churn Score, and Customer Lifetime Value (CLTV) index.**\n\n**For a better understanding and more information, please refer to [Cognos Analytics with Watson](https:\/\/community.ibm.com\/community\/user\/businessanalytics\/blogs\/steven-macko\/2019\/07\/11\/telco-customer-churn-1113) and [Kaggle Website](https:\/\/www.kaggle.com\/blastchar\/telco-customer-churn)**","dd944d48":"Pandas profiling is an open source Python module with which we can quickly do an exploratory data analysis with just a few lines of code. Similarly, for EDA, profile_report() is One-Line Magical Code creating reports in the interactive HTML format which is quite easy to understand and analyze the data. In short, at the first hand, what pandas profiling does is to save us all the work of visualizing and understanding the distribution of each variable.\n\n**For a better understanding and more information, please refer to [external link text](https:\/\/www.analyticsvidhya.com\/blog\/2021\/06\/generate-reports-using-pandas-profiling-deploy-using-streamlit\/) & [external link text](https:\/\/towardsdatascience.com\/exploratory-data-analysis-with-pandas-profiling-de3aae2ddff3)**","8c01492e":"**Let's begin importing libraries needed in the given analysis**","f3b23a06":"<a id=\"9\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">9 - REFERENCES<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","0cb5af61":"**Let's convert categorical variables by using OneHotEncoder.**","83d34b5c":"<a id=\"4.6.1\"><\/a>\n#### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.6.1 Spliting Dataset into Numeric & Categoric Features<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","3fb99152":"**Kurtosis**","793de7f7":"# <p style=\"background-color:#9c2162;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">TELCO CUSTOMER CHURN<\/p>\n\n![image-3.png](attachment:image-3.png)\n\n<p style=\"background-color:#9c2162;font-family:newtimeroman;color:#FFF9ED;font-size:140%;text-align:left;border-radius:10px 10px;\">Image credit : https:\/\/technologyadvice.com\/blog\/sales\/crm-stop-customer-churn\/<\/p> ","f917beb4":"<a id=\"1\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">1 - LIBRARIES NEEDED IN THE STUDY<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**First, if you do NOT have the following libraries in your kernel, just install them since you have to need them to import some libraries.**","ed1a9c77":"<a id=\"4.1\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.1 A General Looking at the Data<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","f104af6a":"- Kline, R.B. (2011). Principles and practice of structural equation modeling (5th ed., pp. 3-427). New York:The Guilford Press.\n- Edwards, A. (1976). An introduction to linear regression and correlation. W. H. Freeman\n- Everitt, B. S.; Skrondal, A. (2010), The Cambridge Dictionary of Statistics, Cambridge University Press.\n- https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1\n- https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W\n- https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1\n- https:\/\/www.amazon.com\/Introduction-Machine-Learning-Python-Scientists\/dp\/1449369413\n- Neural Networks from Scratch in Python (by Kinsley \u00a7 Kukiela) [external link text](https:\/\/nnfs.io\/)\n- Practical Statistics for Data Scientists (by Bruce & Gedeck) [external link text](https:\/\/www.amazon.com\/Practical-Statistics-Data-Scientists-Essential\/dp\/149207294X\/ref=sr_1_1?dchild=1&keywords=Practical+Statistics+for+Data+Scientists&qid=1627662007&sr=8-1)\n- Applications of Deep Neural Networks(by Jeff Heaton) [external link text](https:\/\/arxiv.org\/abs\/2009.05673)\n- Applied Predictive Modeling (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Applied-Predictive-Modeling-Max-Kuhn\/dp\/1461468485\/ref=pd_sbs_3\/141-4288971-3747365?pd_rd_w=AOIS7&pf_rd_p=3676f086-9496-4fd7-8490-77cf7f43f846&pf_rd_r=MCCHJXWK39VD6VW7RVAR&pd_rd_r=4ffcd1ea-44b9-4f33-b9b3-dc02ee159662&pd_rd_wg=nU1Ex&pd_rd_i=1461468485&psc=1:)\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (by Aur\u00e9lien G\u00e9ron) [external link text](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646\/ref=sr_1_1?crid=2GV554Q2EKD1E&dchild=1&keywords=hands-on+machine+learning+with+scikit-learn%2C+keras%2C+and+tensorflow&qid=1627628294&s=books&sprefix=hands%2Cstripbooks-intl-ship%2C309&sr=1-1)\n- Master Machine Learning Algorithms (by Brownlee, ML algorithms are very well explained ) [external link text](https:\/\/machinelearningmastery.com\/master-machine-learning-algorithms\/)\n- Python Feature Engineering Cookbook (by Galli) [external link text](https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1)\n- Feature Engineering Made Easy (by Ozdemir & Susarla) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W)\n- Feature Engineering and Selection (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1)\n- Imbalanced Classification with Python(by Brownlee) [external link text](https:\/\/machinelearningmastery.com\/imbalanced-classification-with-python\/)\n- https:\/\/www.kaggle.com\/kaanboke\/the-most-common-evaluation-metrics-a-gentle-intro","d7747e88":"<a id=\"4.6.2\"><\/a>\n#### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.6.2 The Examination of Numerical Features<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","a6073150":"Let's first plot the heatmap and examine the correlations among the features visually. ","bd393e74":"**Let's examine F1, Recall, ROC AUC and Accuracy scores for each model we created.**","d482f7bd":"**Skewness**","361db707":"**Interpreting Skewness**\n\n0.5 is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models.","d5e8eb85":"**Even if data type of \"totalcharges\" column is object type, it has numerical values. So we need to change its data type to float type.**","20b1abd1":"<a id=\"3.1\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">3.1 Reading The Data<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**How to read and assign the dataset as df, you can visit [pandas official Website](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_csv.html) (You can define it as what you want instead of df)**","a84db58c":"**Considering the results with default parameters WITHOUT scalling BUT WITH class_weight when we examine, Support Vector Machine (SVM) score caught a big jump from a value of 0.000000 to 0.557174; however, Logistic Regression (LR) still have the best score of 0.799107 among the results of other models by increasing from a value of 0.538350 to 0.791058.**\t","834294bf":"<a id=\"4\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">4 - EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n\n**Before performing Supervised Classification Algorithms, you need to know the data well in order to label the observations correctly. You need to analyze frequency distributions of features, relationships and correlations between the independent variables and the dependent variable. It is recommended to apply data visualization techniques. Observing breakpoints helps you to internalize the data.**\n\n**So, in this Exploratory Data Analysis section the data will be analyzed by summarizing its main characteristics, using statistical graphics and other data visualization methods. As such the reader will be familiar with what the data can tell.**","b5c69852":"<a id=\"6.1\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.1 Modelling Default Parameters WITHOUT Scaling & class_weight<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","cabe956e":"**When we compare the number of unique values with the description of each feature in \"2.2 About The Features\" section, it can be easily captured that some features should be fixed.**\n\n- multiplelines          3\n- internetservice        3\n- onlinesecurity         3\n- onlinebackup           3\n- deviceprotection       3\n- techsupport            3\n- streamingtv            3\n- streamingmovies        3\n- contract               3\n\n**This issue will be solved in the section of \"The Examination of Categorical Variables\".**","a2164dd3":"## Hope to meet in new studies and have fun while... \ud83d\udc4b","b58a0767":"<a id=\"0\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">PREFACE<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Churn analysis is the evaluation of a company\u2019s customer loss rate in order to reduce it. Also referred to as customer attrition rate, churn can be minimized by assessing your product and how people use it. In fact, acquiring new customers is considerably more expensive than maintaining and upgrading existing customer relationships. The more customers you churn, the more money you must spend to recoup the loss of business by finding new ones. Therefore, companies nowadays are more concerned than past and want to identify the causes of churn and implement effective strategies for retaining their customers.**\n\n**In this Exploratory Data Analysis (EDA) and and a variety of Model Classifications including Logistic Regression (LR), Support Vector Machine (SVM), AdaBoosting (AB), GradientBoosting (GB), K-Nearest Neighbors (KNN), Random Forest (RF), Desicion Tree (DT), XGBoost (XGB), this study will examine the dataset named as \"Telco Customer Churn\" under the 'WA_Fn-UseC_-Telco-Customer-Churn.csv' file at Kaggle website [at Kaggle website](https:\/\/www.kaggle.com\/blastchar\/telco-customer-churn).**\n\n**This study, in general, will cover what any beginner in Machine Learning can do as much as possible for a better understanding with the given dataset not only by examining its various aspects but also visualising it. Later Later S\/he will be familiar with eight (8) Classification Algorithms in Machine Learning.**","20e167ec":"<a id=\"2.2\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">2.2 About The Features<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**The features in the given dataset are:**\n\n- **CustomerID:** A unique ID that identifies each customer.\n\n- **Gender:** The customer\u2019s gender: Male, Female\n\n- **Age:** The customer\u2019s current age, in years, at the time the fiscal quarter ended.\n\n- **Senior Citizen:** Indicates if the customer is 65 or older: Yes, No\n\n- **Married (Partner):** Indicates if the customer is married: Yes, No\n\n- **Dependents:** Indicates if the customer lives with any dependents: Yes, No. Dependents could be children, parents, grandparents, etc.\n\n- **Number of Dependents:** Indicates the number of dependents that live with the customer.\n\n- **Phone Service:** Indicates if the customer subscribes to home phone service with the company: Yes, No\n\n- **Multiple Lines:** Indicates if the customer subscribes to multiple telephone lines with the company: Yes, No\n\n- **Internet Service:** Indicates if the customer subscribes to Internet service with the company: No, DSL, Fiber Optic, Cable.\n\n- **Online Security:** Indicates if the customer subscribes to an additional online security service provided by the company: Yes, No\n\n- **Online Backup:** Indicates if the customer subscribes to an additional online backup service provided by the company: Yes, No\n\n- **Device Protection Plan:** Indicates if the customer subscribes to an additional device protection plan for their Internet equipment provided by the company: Yes, No\n\n- **Premium Tech Support:** Indicates if the customer subscribes to an additional technical support plan from the company with reduced wait times: Yes, No\n\n- **Streaming TV:** Indicates if the customer uses their Internet service to stream television programing from a third party provider: Yes, No. The company does not charge an additional fee for this service.\n\n- **Streaming Movies:** Indicates if the customer uses their Internet service to stream movies from a third party provider: Yes, No. The company does not charge an additional fee for this service.\n\n- **Contract:** Indicates the customer\u2019s current contract type: Month-to-Month, One Year, Two Year.\n\n- **Paperless Billing:** Indicates if the customer has chosen paperless billing: Yes, No\n\n- **Payment Method:** Indicates how the customer pays their bill: Bank Withdrawal, Credit Card, Mailed Check\n\n- **Monthly Charge:** Indicates the customer\u2019s current total monthly charge for all their services from the company.\n\n- **Total Charges:** Indicates the customer\u2019s total charges, calculated to the end of the quarter specified above.\n\n- **Tenure:** Indicates the total amount of months that the customer has been with the company.\n\n- **Churn:** Yes = the customer left the company this quarter. No = the customer remained with the company. Directly related to Churn Value.","47c5f6ec":"**Multicollinearity**","76427880":"<a id=\"6.3\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.3 Modelling Default Parameters WITH class_weight & Scaling<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","52e17e43":"<a id=\"10\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">10 - FURTHER READINGS<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>"}}