{"cell_type":{"c764fc80":"code","65eb2531":"code","1fcf48ce":"code","24348897":"code","338befb4":"code","9c8c4c97":"code","9cf360bd":"code","088f7ebb":"code","b3ebf899":"code","cca4af31":"code","683f190f":"code","7d90f351":"code","06d2f212":"code","3b52e867":"markdown","e1a97177":"markdown","4b53b985":"markdown","8ccae1fa":"markdown","e02c7b6c":"markdown","06f50edb":"markdown","377c4a0d":"markdown","c83c705f":"markdown","cd9772a8":"markdown","912a08f0":"markdown","cb9a3158":"markdown","d974704f":"markdown","ae21acfc":"markdown","2d0ce85f":"markdown","d85d9a48":"markdown","418ba306":"markdown","30628190":"markdown","77986836":"markdown","630b9fb1":"markdown","a5c7a3fd":"markdown","5ce4e8ab":"markdown","2c0822ce":"markdown","6d461328":"markdown","3f8970ef":"markdown","6bc1fc6d":"markdown","17b2844f":"markdown","3ed348ef":"markdown","f37eb343":"markdown","8f038359":"markdown","1700ee25":"markdown","7d1d875a":"markdown","b134744c":"markdown","e3523761":"markdown","3937a9ef":"markdown","c5c08d8c":"markdown","de47afea":"markdown","a72abd81":"markdown","8e22974c":"markdown","87c9721a":"markdown","ece2e785":"markdown","2c70a2f6":"markdown","2693e00e":"markdown","16e77338":"markdown","ed571993":"markdown","5d97761f":"markdown","afc608bd":"markdown","ef820d18":"markdown","9854e3d6":"markdown","570f2c6c":"markdown","ddfb644f":"markdown","c23e74f4":"markdown","846dce21":"markdown","a249f61f":"markdown","02aab0aa":"markdown","c46755af":"markdown","02015a1e":"markdown","31cc4a92":"markdown","14196576":"markdown","8d72a57f":"markdown","6eb51089":"markdown","7d49e7b8":"markdown","61217839":"markdown","323f8b25":"markdown","0ff5b010":"markdown","7351be1a":"markdown","686d48d9":"markdown","3318fbbe":"markdown","c18a16fb":"markdown","5f12d701":"markdown","15221402":"markdown","b1d45339":"markdown","bf6b6930":"markdown","694154dc":"markdown","b62cc9db":"markdown","bc0b3a2b":"markdown","202a772a":"markdown","e116c055":"markdown","9fba404d":"markdown","4f662dc0":"markdown","7a081933":"markdown","43c5d93a":"markdown","fadd58b0":"markdown","87a4ac2c":"markdown","3dd51865":"markdown","3737278e":"markdown","61e5c7a3":"markdown"},"source":{"c764fc80":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \n\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,PowerTransformer\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold,cross_val_score,GridSearchCV\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom sklearn.metrics import mean_squared_error,classification_report,make_scorer,accuracy_score,plot_roc_curve,auc,roc_curve\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import KFold\n\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","65eb2531":"df_credit = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf_credit.head()","1fcf48ce":"df_credit['Class'].value_counts(normalize=True)","24348897":"\ndf_credit = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\nX = df_credit.drop('Class', axis=1)\ny = df_credit['Class']\n\nmodel =DummyClassifier(strategy='most_frequent')\n\n#pipeline = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),('s',MinMaxScaler()),('m', model)]) \n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n\nresult = cross_val_score(model, X, y,  scoring='accuracy',cv=cv, n_jobs=-1)\n\nprint(f'{round(np.mean(result),6)}')","338befb4":"\ndef classification_report_with_validation(y_true, y_pred):\n    real_values.extend(y_true)\n    predicted_values.extend(y_pred)\n    return accuracy_score(y_true, y_pred)\n\n\ndf_credit = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\nX = df_credit.drop('Class', axis=1)\ny = df_credit['Class']\n\nreal_values = []\npredicted_values = []\n\nmodel =LogisticRegression(solver='liblinear')\n\npipeline = Pipeline(steps=[('s',MinMaxScaler()),('m', model)]) \n\ncv = KFold(n_splits=10, random_state=42)\n\nresult = cross_val_score(pipeline, X, y,  scoring=make_scorer(classification_report_with_validation),cv=cv)\n\nprint(classification_report(real_values, predicted_values)) ","9c8c4c97":"df_credit = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\nX = df_credit.drop('Class', axis=1)\ny = df_credit['Class']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)\n\npipeline = make_pipeline(MinMaxScaler(), LogisticRegression(solver='liblinear'))\n\npipeline.fit(X_train,y_train)\nprobs = pipeline.predict_proba(X_test)\nfpr1, tpr1, thresholds = roc_curve(y_test, probs[:, 1], pos_label=1)\nroc_auc1 = auc(fpr1, tpr1)\n\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\n \nplt.plot(fpr1, tpr1, label='ROC Curve 1 (AUC = %0.2f)' % (roc_auc1))\nplt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Random Classifier')   \nplt.plot([0, 0, 1], [0, 1, 1], linestyle=':', color='green', label='Perfect Classifier')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend(loc=\"lower right\")\nplt.show()","9cf360bd":"# log loss for naive probability predictions.\nfrom sklearn.metrics import log_loss\n# generate 2 class dataset\ndf_credit = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\nX = df_credit.drop('Class', axis=1)\ny = df_credit['Class']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n\n\n# no skill prediction 0\nprobabilities = [[1, 0] for _ in range(len(y_test))]\navg_logloss = log_loss(y_test, probabilities)\nprint('P(class0=1): Log Loss=%.3f' % (avg_logloss))\n# no skill prediction 1\nprobabilities = [[0, 1] for _ in range(len(y_test))]\navg_logloss = log_loss(y_test, probabilities)\nprint('P(class1=1): Log Loss=%.3f' % (avg_logloss))\n# baseline probabilities\nprobabilities = [[0.99, 0.01] for _ in range(len(y_test))]\navg_logloss = log_loss(y_test, probabilities)\nprint('Baseline: Log Loss=%.3f' % (avg_logloss))\n# perfect probabilities\navg_logloss = log_loss(y_test, y_test)\nprint('Perfect: Log Loss=%.3f' % (avg_logloss))","088f7ebb":"column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndf_boston= pd.read_csv('..\/input\/boston-house-prices\/housing.csv',header=None, delimiter=r\"\\s+\", names=column_names)\ndf_boston = df_boston.drop('CHAS', axis=1)\ndf_boston.head()","b3ebf899":"df_boston['MEDV'].describe()","cca4af31":"X = df_boston.drop('MEDV',axis=1)\ny = df_boston['MEDV']\npipeline = make_pipeline(PowerTransformer(method='yeo-johnson'), LinearRegression())\ncv = KFold(n_splits=10, random_state=42)\nresults = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_absolute_error')\nprint(f'MAE: {round(results.mean()*-1,3)}, ({round(results.std(),3)})')","683f190f":"X = df_boston.drop('MEDV',axis=1)\ny = df_boston['MEDV']\npipeline = make_pipeline(PowerTransformer(method='yeo-johnson'), LinearRegression())\ncv = KFold(n_splits=10, random_state=42)\nresults = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_squared_error')\nprint(f'MSE: {round(results.mean()*-1,3)}, ({round(results.std(),3)})')","7d90f351":"X = df_boston.drop('MEDV',axis=1)\ny = df_boston['MEDV']\npipeline = make_pipeline(PowerTransformer(method='yeo-johnson'), LinearRegression())\ncv = KFold(n_splits=10, random_state=42)\nresults = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_squared_error')\nprint(f'RMSE: {round(np.sqrt(results.mean()*-1),3)}, ({round(results.std(),3)})')","06d2f212":"X = df_boston.drop('MEDV',axis=1)\ny = df_boston['MEDV']\npipeline = make_pipeline(PowerTransformer(method='yeo-johnson'), LinearRegression())\ncv = KFold(n_splits=10, random_state=42)\nresults = cross_val_score(pipeline, X, y, cv=cv, scoring='r2')\nprint(f'R Squared: {round(results.mean(),3)}, ({round(results.std(),3)})')","3b52e867":"- Before starting the evaluation metrics, we should be on the same page.\n- Let's refresh the basics.","e1a97177":"image credit: https:\/\/giphy.com","4b53b985":"<img src=\"https:\/\/els-jbs-prod-cdn.jbs.elsevierhealth.com\/cms\/attachment\/36cdb4ec-0c7d-48cb-9a4d-7cb463f8b7c3\/gr1.jpg\" width=\"600\">","8ccae1fa":"<img src=\"https:\/\/www.publichealthnotes.com\/wp-content\/uploads\/2020\/04\/slide_9.jpg\" width=\"600\">\n","e02c7b6c":"![](https:\/\/www.magazine.etnfocus.com\/wp-content\/uploads\/2017\/08\/metrics.jpg)","06f50edb":"- It would be good idea to refresh our knowledge on the regression evaluation metrics.\n- We will look at \n    - Mean Absolute Error, \n    - Mean Squared Error\n    - Root Mean Squared Error\n    - R2\n- First we will see their definitions and formulas and then see them in the action.\n- In this study, we will use Boston House  prices dataset.","377c4a0d":"<a id=\"5\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>F Score (F Measure)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","c83c705f":"- Most of the applications in default uses R squared as a metric for the regression problems.\n- R squared gives us the proportion of the target variable is explained by the feature(s).\n- R squared provides an indication of the goodness of fit of a set of predictions to the actual values.","cd9772a8":"<div class=\"alert alert-block alert-info\">\n<b>Precision & Recall --> Which one to use and When ?<\/b> \n     <ul style=\"list-style-type:none\">\n         <li><b>Precision:<\/b>  When our aim is to minimize false positive (Only fraud case, not include non-fraud transaction as a fraud transaction)<\/li>\n         <li><b>Recall : <\/b> When our aim is to minimize false negative (Every cancer patient should be classified as  a cancer patient, not classified as a healthy one)<\/li>\n      <\/ul>\n    \n   \n<\/div>\n","912a08f0":"image credit: https:\/\/cdn-images-1.medium.com","cb9a3158":"![](https:\/\/cdn-images-1.medium.com\/max\/959\/1*WDKhO-z7rti70ZTv59yJ9A.jpeg)","d974704f":"- **Enjoy** \ud83e\udd18","ae21acfc":"Image Credit: https:\/\/miro.medium.com\/","2d0ce85f":"#### **By the way, when you like the topic, you can show it by supporting** \ud83d\udc4d\n\n####  **Feel free to leave a comment in the notebook**. \n\n#### All the best \ud83e\udd18","d85d9a48":"image credit: https:\/\/en.wikipedia.org","418ba306":"<a id=\"13\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>Conclusion<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","30628190":"![](https:\/\/i.stack.imgur.com\/UN1Pk.png)","77986836":"- We didn't make any extensive exploratory analysis with the data. \n- We have just used it for showing the usage of the classification metrics on the imbalanced data.\n- For having said that precision: .93 , recall : 77 and f1 score: .83\n- And accuracy is 1.00 !!!","630b9fb1":"image credit : https:\/\/stackoverflow.com\/questions\/56401346\/mean-absolute-error-in-tensorflow-without-built-in-functions\/56401550","a5c7a3fd":"- Formula for the accuracy is easy one: Total number of correct predictions divided by the total number of predictions.","5ce4e8ab":"#### **Precision**\n- Precision gives us accuracy of the positive classs (fraud case, cancer case, malign case, etc).\n- Main aim of the precision is the minimize the false positive (Type 1 error)","2c0822ce":"- **False Positive**: Our prediction is positive but actual value is negative. Our prediction is false.\n   - We predict as fraud or malign but actual value is non-fraud or benign.\n   - Which is also called as **Type 1 error**.","6d461328":"image credit: https:\/\/www.magazine.etnfocus.com","3f8970ef":"![](https:\/\/www.negotiations.com\/wp-content\/uploads\/2017\/05\/negotiation-success.jpg)","6bc1fc6d":" #### **What is the problem with the accuracy metric for the imbalanced data?**\n\n- Accuracy metric with imbalanced data gives us the accuracy on the majority class (non-fraud)\n- We can reach to 99.8% accuracy without building a machine leraning model, by always predicting the non-fraud.\n- The problem here is that accuracy is an inadequate measure for quantifying predictive performance in this imbalanced setting.\n- Accuracy does not report the correct score for the imbalanced data.\n- As we have seen overwhelming number of non-fraud instances (99.8%) surprass the fraud instances.\n- Even Dummy Classifier can get the 99.8% accuracy score.","17b2844f":"- When we look at the formula above, if we put 100 cases as non-fraud, based on the equation we can get 99.8% accuracy\n-  99.8% accuracy !!!!\n- It is great isn't it ?\n- Let's see all of this by using Dummy Classifier.","3ed348ef":"- **True Positive**: Predicted vale is positive and we predicted correctly. \n- Real Transaction --> Fraud  and our model correctly predict as fraud.","f37eb343":"<a id=\"3\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Accuracy<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","8f038359":"image credit: https:\/\/slidetodoc.com\/class-5-thurs-sep-23-example-of-using","1700ee25":"- Ok we have 99.8% accuracy on the credit card fraud, without learning anything, why we are bothering ourselves to build a model?\n\n- We can easily select every case as a non-fraud and 99.8 out of 100 times, we are right.\n\n- Why aren't we celebrating it?","7d1d875a":"- What we are looking for  is the lowest level loss. The best possible log loss is 0.0\n- Any model with lower log-loss value brings us better predictions.","b134744c":"- RMSE is basically square root of the MSE\n- By taking the square root of the MSE, units are converted  back to the original units of the target variable.","e3523761":"<a id=\"2\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Confusion Matrix<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","3937a9ef":"image credit: https:\/\/www.superheuristics.com","c5c08d8c":"- As shown below, MAE is average absolute differences between  our predicitions and the real value.\n- MAE is easily interpretable\n- Lower the MAE, better the prediction.","de47afea":"- We have used log loss() function of the scikit-learn.\n- It took the predicted probability for each class as input and returned the average log loss.\n- Predicting certainty for fraud and non fraud label is punished with large log loss scores.\n- Since dataset has .5% minority class instances,  being certain for the minority class in all cases results in a much larger log loss score.\n- Baseline did better job by using target distribution.\n- Any model brings us lower than baseline log loss score would make prediction with skill.\n- Perfect log loss score: 0.0 means that there is no difference between prediction and the real values.","a72abd81":"image credit: https:\/\/www.youtube.com\/channel\/UCeoF_5Kw0YyWOqhAbQGrxJQ","8e22974c":"<a id=\"12\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>R Squared (R2)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","87c9721a":"<div class=\"alert alert-block alert-info\">\n<b>Rule of Thumb:<\/b> Do not use accuracy score metric with the imbalanced data.\n<\/div>","ece2e785":"- We have covered one of the most important concepts of the Machine Learning.\n- We have looked at both classification metrics and regression metrics.\n- We have talked about the misusages of the metrics and the correct ones.\n\n- Evaluation metrics are crucial. Based on the model performance we are giving decisions.\n- We should remember that we are not just only looking for a better model, also looking for our end goal.\n- Before deciding evaluation metrics, it would be a good idea to talk your customer, stakeholders and relevant people to clarify their goals and what they realy want.\n- And please remember that most of the classification problems in the real life have imbalanced data.\n","2c70a2f6":"- MSE is the squarred average squared differences between predicted value and the real value.\n- Lower the MSE, better the prediction.","2693e00e":"#### Hi all.  \ud83d\ude4b\u2022\u2642\ufe0f \n\n#### We continue our **Beginner-Intermediate Friendly Machine Learning series**, which would help anyone who wants to learn or refresh the basics of ML.\n\n#### What we have covered: \n\n#### [Beginner Friendly Detailed Explained EDAs \u2013 For anyone at the beginnings of DS\/ML journey](https:\/\/www.kaggle.com\/general\/253911#1393015) \u2714\ufe0f\n\n#### [BIAS & VARIANCE TRADEOFF](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-bias-variance-tradeoff) \u2714\ufe0f\n\n#### [LINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-linear-algorithms)  \u2714\ufe0f\n\n#### [NONLINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/nonlinear-algorithms)  \u2714\ufe0f\n\n#### [The Most Used Methods to Deal with MISSING VALUES](https:\/\/www.kaggle.com\/kaanboke\/the-most-used-methods-to-deal-with-missing-values)  \u2714\ufe0f\n\n#### [Beginner Friendly End to End ML Project- Classification with Imbalanced Data](https:\/\/www.kaggle.com\/kaanboke\/beginner-friendly-end-to-end-ml-project-enjoy)  \u2714\ufe0f\n\n#### [How to Prevent the Data Leakage ?](https:\/\/www.kaggle.com\/kaanboke\/how-to-prevent-the-data-leakage) \u2714\ufe0f\n\n#### In this notebook we will  cover one of the important concepts of the **Machine Learning Evaluation Metrics**\n#### Enjoy \ud83e\udd18","16e77338":"![](https:\/\/slidetodoc.com\/presentation_image\/7d85c6a301ba5b97b7d3b73273b073d0\/image-13.jpg)","ed571993":"<a id=\"11\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Root Mean Squared Error (RMSE)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","5d97761f":"![](https:\/\/media.giphy.com\/media\/l2JJsJQY6yj9HLaZW\/giphy.gif)","afc608bd":"- ROC Curve (receiver operating characteristic curve- AUC) measures model's ability to make distinction between two classes (positive & negative).\n- ROC Curve score close to 1, represents better model.\n- ROC Curve shows false positive rate against the true positive rate (recall)\n- What we are looking for : **High recall and low false positive rate**\n- ROC Curve should be as close as possible to the top left corner.\n- No matter how imbalanced  data we have,predicting randomly always produces an AUC of 0.5.","ef820d18":"- In real life we want to get perfect prediction on the positive class\n- Which means that we are looking high recall and high precision.\n- We have to balance them to get what we want.\n- F score provides us a score which combines precision and recall into a single measure without losing  their properties.","9854e3d6":"- Below code-snippet is generated by using code recipe in the [Imbalanced Classification with Python](https:\/\/machinelearningmastery.com\/imbalanced-classification-with-python\/). I have made changes and modified it to adjust to the problem at hand.","570f2c6c":"image credit: https:\/\/www.publichealthnotes.com","ddfb644f":"- Higher value of True Positive Rate (TPR) means that false negative is very low. Model correctly predicted positive class.\n\n- Lower value of False Positive Rate means that false positive is very low. Model correctly predicted negative class.","c23e74f4":"![](https:\/\/www.superheuristics.com\/wp-content\/uploads\/2021\/03\/Blog_image_confusion-matrix.png)","846dce21":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/26\/Precisionrecall.svg\/600px-Precisionrecall.svg.png)","a249f61f":"<img src=\"https:\/\/programmerah.com\/wp-content\/uploads\/2020\/11\/20190714113817886.png\" width=\"600\">","02aab0aa":"<a id=\"1\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Classification Evaluation Metrics<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","c46755af":"![](https:\/\/miro.medium.com\/max\/1400\/1*FUZS9K4JPqzfXDcC83BQTw.png)","02015a1e":"- Logarithmic loss or log loss metric is based on probabilities.\n- The log loss function calculates the negative log likelihood for probability predictions made by the binary classification model.\n\n","31cc4a92":" <a id=\"9\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Mean Absolute Error (MAE)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","14196576":"<a id=\"14\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>References & Further Reading<\/b><\/font>\n\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\n\n[Machine Learning - Beginner &Intermediate-Friendly BOOKS](https:\/\/www.kaggle.com\/general\/255972)","8d72a57f":"- As we have mentioned before, deciding which metric to use very crucial step on the Machine Learning projects.\n- Stakeholders \/ customers concerns should be taken into consideration before deciding which metric to use.\n\n- In fraud detection case, if our customer aims to reduce false negative:\n    - Which means every fraud case should be defined as a fraud case\n    - Missing the prediction of the fraud case should be minimum\n    - We have to focus on how to reduce wrongly classified non-fraud cases.\n    - In that case we are looking for minimizin type 2 error and increasing the positive rate.\n    - We are looking for higher score recall for fraud case.\n    \n    \n- In fraud detection case, if our customer aims to reduce false positive:\n    - Which means we want to be sure that positive case should be positive case, not the others\n    - We do not want to classify our loyal customer's transaction as a fraud transaction and block his\/her account.\n    - We have to focus on wrongly classified positive case.\n    - In that case we are looking for minimizin type 1 error and decreasing the false positive rate.\n    - We are looking for higher score precision for fraud case.\n \n \n- If we want to reduce the risk of the fraud without losing our customer:\n    - We want to make a balance between precision and recall\n    - It would be good idea to focus on F score\n","6eb51089":"#### **Recall**\n- Recall gives us the score of the number of correct positive predictions made out of all correct positive predictions.\n- Main aim ofthe recall is the minimize the false negative (Type 2 error).","7d49e7b8":"- One of the most common evaluation metrics, we can see in the real world and also in the Kaggle.\n- Accuracy is better to use with balanced classification problem and when all predictions and prediction errors are equally important (for example using iris dataset. Every class has equal instances).\n\n- Balanced Data: Target has equal or almost equal number of instances.\n- Prediction Errors are  equally important: Predicting  Class A, Class B or Probability of detecting fraud or detecting non fraud\n- Is it really possible in the real life?\n- I can't say, it is impossible, but fair to say it is rare.\n- Most of the classification problem, we handle in ML, has imbalanced data and consequences of the prediction errors are rarely same.\n- When we have the imbalanced data, accuracy is not a good evaluation metric to use.\n","61217839":"<a id=\"10\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Mean Squared Error (MSE)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","323f8b25":"image credit: https:\/\/programmerah.com","0ff5b010":"- **False Negative**: Predicted value is negative, but actual value is positive. Our prediction is false.\n   - We predict non-fraud, benign, but actual value is fraud or malign.\n   - Which is also called **Type 2 error**.","7351be1a":"<a id=\"0\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>What is Evaluation Metrics? & Why We Need Them?<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","686d48d9":"<div class=\"alert alert-block alert-info\">\n<b>Note:<\/b>  ROC Curve (AUC) is often more meaningful than using accuracy metric for classification problems with imbalanced data.\n<\/div>","3318fbbe":"<a id=\"toc\"><\/a>\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n    \n* [What is Evaluation Metrics?](#0)\n* [Classification Evalution Metrics](#1)\n    * [Confusion Matrix](#2)\n    * [Accuracy](#3)\n    * [Precision & Recall](#4)\n    * [F Score (F Measure)](#5)\n    * [ROC Curve (AUC)](#6)\n    * [Log Loss](#7)\n  \n  \n* [Regression Evaluation Metrics](#8)    \n    * [Mean Absolute Error(MAE)](#9)\n    * [Mean Squared Error (MSE)](#10)    \n    * [Root Mean Squared Error (RMSE)](#11)\n    * [R Squared (R2)](#12)\n\n\n* [Conclusion](#13)\n* [References & Further Reading](#14)","c18a16fb":"image credit: https:\/\/www.mydatamodels.com","5f12d701":"- **True Negative** : We predict as negative and our prediction is correct. Actual value is negative ( non-fraud, benign, etc.)","15221402":"<a id=\"7\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Log Loss<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","b1d45339":"image credit: https:\/\/stackoverflow.com","bf6b6930":"<a id=\"4\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Precision & Recall<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","694154dc":"![](https:\/\/i.ytimg.com\/vi\/fcO9820wCXE\/hqdefault.jpg)","b62cc9db":"- It is clearly seen in the formula, why accuracy is not a good measure for the imbalanced data.\n- Imagine we have  a data...\n- Just kidding, you don't need to imagine we have real data to see.\n- We will use credit card fraud data to estimate fraud cases.\n- This is imbalanced data. Be careful !!!","bc0b3a2b":"![](https:\/\/www.mydatamodels.com\/wp-content\/uploads\/2020\/10\/2.-Accuracy-formula-machine-learning-algorithms.png)","202a772a":"<a id=\"8\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Regression Evaluation Metrics<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","e116c055":"image credit: https:\/\/www.pinterest.com","9fba404d":"image credit: https:\/\/www.jtcvs.org\/article\/S0022-5223(18)32875-7","4f662dc0":"#### **By the way, when you like the topic, you can show it by supporting** \ud83d\udc4d\n\n####  **Feel free to leave a comment in the notebook**. \n\n#### All the best \ud83e\udd18","7a081933":"![](https:\/\/i.imgur.com\/19LNbyQ.jpg)","43c5d93a":"- Classification problems are the most common problems in the Machine Learning.\n- It would be a good idea to refresh our knowledge on the classification evaluation metrics.\n- In the classification part of the study, we will use Credit Card Fraud dataset.","fadd58b0":"![](https:\/\/i.pinimg.com\/originals\/aa\/91\/7a\/aa917a42422eaedb18224224519e48f0.jpg)","87a4ac2c":"- In this study we will divide our evaluation metrics into two categories.\n    - Classification Evaluation Metrics\n    - Regression Evaluation Metrics\n \n - Ok let's start.","3dd51865":"<a id=\"6\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>ROC Curve (AUC)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","3737278e":"image credit: https:\/\/www.negotiations.com","61e5c7a3":"- In machine learning, evaluation metrics are used to measure the performance of machine learning models\/algorithms.\n- Evaluation metrics are crucial. Based on the model performance we are giving decisions.\n- We should remember that we are not just only looking for a better model, also looking for our end goal.\n- Let's imagine our end goal is to make an application to detect fraud.\n- We develop our model based on the data in hand, which contains 99.5% non-fraud cases and %.5 fraud cases.\n- Without using the correct evaluation metric on this imbalanced data we will deploy the model with poor performance and prediction on the real data."}}