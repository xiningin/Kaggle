{"cell_type":{"2ea389e4":"code","96857120":"code","27e4e27f":"code","edf0184b":"code","55f083d6":"code","5ab6bc70":"code","728ebcd7":"code","1e4e5eac":"code","d4b894be":"code","07e31623":"code","6fb9c408":"code","90fb3afb":"code","e0521e5e":"code","8542e6ef":"markdown","bec8fa54":"markdown","a70df134":"markdown","ca8205c8":"markdown","5aa7d7f9":"markdown","7d3fed31":"markdown","e9acf36f":"markdown"},"source":{"2ea389e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom utils import WeightReader, decode_netout, draw_boxes\nfrom yolo1_preprocessing import parse_annotation, BatchGenerator\nimport sys\nprint(sys.version)\n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","96857120":"\n#lets check what all images we have to check the performance of the model. \nimport os\nfolder=os.listdir('\/kaggle\/input\/yolo-weights-inputs\/downloaded_images\/downloaded_images')\nfolder","27e4e27f":"from keras.models import Sequential, Model\nfrom keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nfrom keras.optimizers import SGD, Adam, RMSprop\nfrom keras.layers.merge import concatenate\nimport matplotlib.pyplot as plt\nimport keras.backend as K\nimport tensorflow as tf\nimport imgaug as ia\nfrom tqdm import tqdm\nfrom imgaug import augmenters as iaa\nimport numpy as np\nimport pickle\nimport os, cv2\n# from preprocessing import parse_annotation, BatchGenerator\n# from utils import WeightReader, decode_netout, draw_boxes #normalize\n# The above two imports are imported seperately by first creating a script and then adding te scipt to our notebook\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n%matplotlib inline","edf0184b":"LABELS = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n\nIMAGE_H, IMAGE_W = 416, 416\nGRID_H,  GRID_W  = 13 , 13\nBOX              = 5\nCLASS            = len(LABELS)\nCLASS_WEIGHTS    = np.ones(CLASS, dtype='float32')\nOBJ_THRESHOLD    = 0.3#0.5\nNMS_THRESHOLD    = 0.3#0.45\nANCHORS          = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n\nNO_OBJECT_SCALE  = 1.0\nOBJECT_SCALE     = 5.0\nCOORD_SCALE      = 1.0\nCLASS_SCALE      = 1.0\n\nBATCH_SIZE       = 16\nWARM_UP_BATCHES  = 0\nTRUE_BOX_BUFFER  = 50","55f083d6":"print(F\"Total number of classes :{len(LABELS)}\")\nprint(f\"some of the labels: {LABELS[10:20]}\")","5ab6bc70":"wt_path = '..\/input\/yolo-weights-inputs\/yolo.weights'     \n# the function to implement the orgnization layer (thanks to github.com\/allanzelener\/YAD2K)\ndef space_to_depth_x2(x):\n    import tensorflow as tf\n    return tf.nn.space_to_depth(x, block_size=2)","728ebcd7":"import sys\ninput_image = Input(shape=(IMAGE_H, IMAGE_W, 3))\ntrue_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))\n\n# Layer 1\nx = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input_image)\nx = BatchNormalization(name='norm_1')(x)\nx = LeakyReLU(alpha=0.1)(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Layer 2\nx = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(x)\nx = BatchNormalization(name='norm_2')(x)\nx = LeakyReLU(alpha=0.1)(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Layer 3\nx = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False)(x)\nx = BatchNormalization(name='norm_3')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 4\nx = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4', use_bias=False)(x)\nx = BatchNormalization(name='norm_4')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 5\nx = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False)(x)\nx = BatchNormalization(name='norm_5')(x)\nx = LeakyReLU(alpha=0.1)(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Layer 6\nx = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(x)\nx = BatchNormalization(name='norm_6')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 7\nx = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(x)\nx = BatchNormalization(name='norm_7')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 8\nx = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False)(x)\nx = BatchNormalization(name='norm_8')(x)\nx = LeakyReLU(alpha=0.1)(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Layer 9\nx = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(x)\nx = BatchNormalization(name='norm_9')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 10\nx = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(x)\nx = BatchNormalization(name='norm_10')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 11\nx = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(x)\nx = BatchNormalization(name='norm_11')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 12\nx = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(x)\nx = BatchNormalization(name='norm_12')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 13\nx = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(x)\nx = BatchNormalization(name='norm_13')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\nskip_connection = x\n\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Layer 14\nx = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(x)\nx = BatchNormalization(name='norm_14')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 15\nx = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(x)\nx = BatchNormalization(name='norm_15')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 16\nx = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(x)\nx = BatchNormalization(name='norm_16')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 17\nx = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(x)\nx = BatchNormalization(name='norm_17')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 18\nx = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(x)\nx = BatchNormalization(name='norm_18')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 19\nx = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(x)\nx = BatchNormalization(name='norm_19')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 20\nx = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(x)\nx = BatchNormalization(name='norm_20')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 21\nskip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\nskip_connection = BatchNormalization(name='norm_21')(skip_connection)\nskip_connection = LeakyReLU(alpha=0.1)(skip_connection)\nskip_connection = Lambda(space_to_depth_x2)(skip_connection)\n\nx = concatenate([skip_connection, x])\n\n# Layer 22\nx = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(x)\nx = BatchNormalization(name='norm_22')(x)\nx = LeakyReLU(alpha=0.1)(x)\n\n# Layer 23\nx = Conv2D(BOX * (4 + 1 + CLASS), (1,1), strides=(1,1), padding='same', name='conv_23')(x)\noutput = Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS))(x)\n\n# small hack to allow true_boxes to be registered when Keras build the model \n# for more information: https:\/\/github.com\/fchollet\/keras\/issues\/2790\noutput = Lambda(lambda args: args[0])([output, true_boxes])\n\nmodel = Model([input_image, true_boxes], output)","1e4e5eac":"model.summary()","d4b894be":"weight_reader = WeightReader(wt_path)","07e31623":"weight_reader.reset()\nnb_conv = 23\n\nfor i in range(1, nb_conv+1):\n    conv_layer = model.get_layer('conv_' + str(i))\n    \n    if i < nb_conv:\n        norm_layer = model.get_layer('norm_' + str(i))\n        \n        size = np.prod(norm_layer.get_weights()[0].shape)\n\n        beta  = weight_reader.read_bytes(size)\n        gamma = weight_reader.read_bytes(size)\n        mean  = weight_reader.read_bytes(size)\n        var   = weight_reader.read_bytes(size)\n\n        weights = norm_layer.set_weights([gamma, beta, mean, var])       \n        \n    if len(conv_layer.get_weights()) > 1:\n        bias   = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n        kernel = kernel.transpose([2,3,1,0])\n        conv_layer.set_weights([kernel, bias])\n    else:\n        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n        kernel = kernel.transpose([2,3,1,0])\n        conv_layer.set_weights([kernel])","6fb9c408":"dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))","90fb3afb":"images=os.listdir('..\/input\/yolo-weights-inputs\/downloaded_images\/downloaded_images')\nimages=images[4:]","e0521e5e":"images=os.listdir('..\/input\/yolo-weights-inputs\/downloaded_images\/downloaded_images')\nimages=images[3:]\nfor file in images:\n#     image=os.path.join()\n    image = cv2.imread(os.path.join('..\/input\/yolo-weights-inputs\/downloaded_images\/downloaded_images' ,file))\n    dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\n\n    plt.figure(figsize=(10,10))\n\n    input_image = cv2.resize(image, (416, 416))\n    input_image = input_image \/ 255.\n    input_image = input_image[:,:,::-1]\n    input_image = np.expand_dims(input_image, 0)\n\n    netout = model.predict([input_image, dummy_array])\n\n    boxes = decode_netout(netout[0], \n                          obj_threshold=OBJ_THRESHOLD,\n                          nms_threshold=NMS_THRESHOLD,\n                          anchors=ANCHORS, \n                          nb_class=CLASS)\n\n    image = draw_boxes(image, boxes, labels=LABELS)\n\n    plt.imshow(image[:,:,::-1]); plt.show()","8542e6ef":"So,as usual lets first import the required libraries. I have uploaded the YOLO weights, some scripts and the test data set comprising of images downloaded from internet into the input folder.","bec8fa54":"Lets define some parameters:\n* Labels: There are \n* Image height\n* Image Width\n* Classes\n* Batch size","a70df134":"This is my first computer vision notebook.I truly believe that understanding some beautifull work already done by some brilliant minds help us immenesly in developing our skills.\n\nIf you are reading this, I thank you for giving me your time and going through the notebook. Your comments and feedbacks are valuable and I will appreciate you to leave one feedback and if you liked let me know by voting.\n\nThank you again! ","ca8205c8":"# Load pretrained weights\n**Load the weights originally provided by YOLO**","5aa7d7f9":"# Perform detection on image","7d3fed31":"![](http:\/\/)![image.png](attachment:image.png)","e9acf36f":"# My First notebook on YOLO: Hope you will like it.\n\nThe pic above summarizes the objective of this notebook which is to detect and locate objects in images. For this  I am using the pretrained weights from the famous YOLO(You only look once) model.This notebook is a stripped-down version of Huynh Ngoc Anh's. notebook. I have removed the training part.\nYou can find the original notebook here:\n\nhttps:\/\/github.com\/experiencor\/basic-yolo-keras\/blob\/master\/Yolo%20Step-by-Step.ipynb\n\nThis notebook is a demonstration of what we can do utilizing the work already done by some of the brightest mind in this field.We can reinitialize the weights of the last fully connected layer and train the model to detect objects that are specific to our case and similar to the original output classes.Before we move ahead, a brief about **YOLO**** and **COCO****\n\n### YOLO(You Only Look Once):\nYOLO (\u201cYou Only Look Once\u201d) is a real-time object recognition algorithm, first described in 2015 by Joseph Redmon et al in the paper **You Only Look Once: Unified, Real-Time Object Detection.** The paper can be found here https:\/\/arxiv.org\/abs\/1506.02640.\n\n### COCO(Common Objects in Context) : \nCOCO is a large-scale object detection, segmentation, and captioning dataset.Its a huge dataser of more than 200K images with labels,bounding boxes and annotations.The dataset has these details for more than 80 output classes. The dataset is divided in train, val and test sets. We can retrain the YOLO model on this dataset with the last layer's weight re-initialized to tune performance as per our specific business case."}}