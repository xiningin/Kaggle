{"cell_type":{"b25b89c9":"code","9415d6f7":"code","07d35a14":"code","9e1edf75":"code","4d5099bb":"code","d994e6b1":"code","c872652a":"code","34fbff15":"code","1f8a5cc6":"code","c879702a":"code","4baa0c51":"code","ae92fff2":"code","60411bcc":"code","39ea8825":"code","2201fa7d":"code","e0f9d21d":"code","cd95d294":"code","1b8a068d":"code","04f545fa":"code","a922f4c2":"code","36b19880":"code","434b4b5f":"code","fc417c40":"code","d92f5049":"code","df565f1c":"code","0ff39c43":"code","c292f885":"code","f88b2dcb":"code","0a4c643f":"code","955aef24":"code","20981571":"code","7576d10d":"code","ccb0866b":"code","7e06e422":"code","e519b2ca":"code","7460154b":"code","bfc9098f":"code","056a7112":"code","36a5e31f":"code","70af71b6":"code","45aed99e":"code","ba391122":"markdown"},"source":{"b25b89c9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport warnings\nimport gc\n\nimport pyarrow.parquet as pq\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\".\/\"))","9415d6f7":"BATCH_SIZE = 64\nMERGE_SIZE = 400","07d35a14":"metadata_train = pd.read_csv('..\/input\/metadata_train.csv')","9e1edf75":"def read_wave_data(parquet_path,col_nums,end_col_num, merge_size=800):\n    df_diff = None\n    for i, col_num in tqdm(enumerate(col_nums)):\n        start = col_num\n        if i == len(col_nums) - 1:\n            end = end_col_num\n        else:\n            end = col_nums[i + 1]\n        columns = [str(j) for j in range(start,end)]\n        tmp_df = pq.read_pandas(parquet_path, columns=columns).to_pandas()\n        group_id = np.repeat(range(len(tmp_df) \/\/ merge_size), merge_size)\n        tmp_df['group_id'] = pd.Series(group_id)\n        tmp_diff = (tmp_df.groupby('group_id').max() - tmp_df.groupby('group_id').min()) \/ 256\n        if df_diff is None:\n            df_diff = tmp_diff\n        else:\n            df_diff = pd.concat([df_diff, tmp_diff], axis=1)\n    df_diff = df_diff.astype('float16')\n    return df_diff","4d5099bb":"train_parquet_path = '..\/input\/train.parquet'\nend_col_num = metadata_train['signal_id'].values[-1] + 1\ncol_nums = metadata_train['signal_id'].values[::500].tolist()\ntrain_diff = read_wave_data(train_parquet_path,col_nums,end_col_num,merge_size=MERGE_SIZE)\nprint(train_diff.shape)","d994e6b1":"def train_data_gen(metadata_train, train_diff, batch_size=128, is_reverse=False):\n    np.random.seed(1)\n    while True:\n        x_train = []\n        y_train = []\n        true_sample = metadata_train[metadata_train['target']==1].sample(batch_size \/\/ 2)\n        neg_sample = metadata_train[metadata_train['target']==0].sample(batch_size \/\/ 2)\n    \n        sample_signal_id = np.concatenate([true_sample['signal_id'].values,neg_sample['signal_id'].values])\n        np.random.shuffle(sample_signal_id)\n        for signal_id in sample_signal_id:\n            diffs = train_diff[str(signal_id)].values.T\n            \n            if is_reverse:\n                diffs = diffs[::-1]\n            \n            data = diffs[:, np.newaxis]\n            x_train.append(data)\n            y_train.append(metadata_train[metadata_train['signal_id']==signal_id]['target'].values[0])\n            \n        x_train = np.array(x_train)\n        y_train = np.array(y_train)\n        yield x_train, y_train","c872652a":"metadata_train, metadata_val = train_test_split(metadata_train, test_size=0.2, random_state=42)\nprint(metadata_train.shape)\nprint(metadata_val.shape)","34fbff15":"x_val = []\ny_val = []\nfor signal_id in metadata_val['signal_id'].values:\n    diffs = train_diff[str(signal_id)].values.T\n    data = diffs[:, np.newaxis]\n    x_val.append(data)\n    y_val.append(metadata_val[metadata_val['signal_id']==signal_id]['target'].values[0])\nx_val = np.array(x_val)\ny_val = np.array(y_val)    \nprint(x_val.shape)\nprint(y_val.shape)","1f8a5cc6":"from keras.models import Sequential\nfrom keras import layers\n\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nimport keras.models as models\nimport keras.backend as K","c879702a":"from sklearn.metrics import confusion_matrix\n\ndef mcc(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    TP = cm[0][0]\n    FP = cm[0][1]\n    FN = cm[1][0]\n    TN = cm[1][1]\n    val = ((TP * TN) - (FP * FN)) \/ ((TP + FP)*(TP + FN)*(TN + FP)*(TN + FN))**0.5\n    return val","4baa0c51":"def matthews_corr_coeff(y_true, y_pred):\n    y_pos_pred = K.round(K.clip(y_pred, 0, 1))\n    y_pos_true = K.round(K.clip(y_true, 0, 1))\n    \n    y_neg_pred = 1 - y_pos_pred\n    y_neg_true = 1 - y_pos_true\n\n    tp = K.sum(y_pos_true * y_pos_pred)\n    tn = K.sum(y_neg_true * y_neg_pred)\n    fp = K.sum(y_neg_true * y_pos_pred)\n    fn = K.sum(y_pos_true * y_neg_pred)\n    return (tp * tn - fp * fn) \/ (K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) + K.epsilon())","ae92fff2":"length_of_sequence = train_diff.shape[0]\ndrop_out_rate = 0.2\nrecurrent_dropout = 0.5\nSTEPS_PER_EPOCH = 100\nEPOCHS = 50","60411bcc":"# Create Model\n\nmodel = Sequential()\nmodel.add(layers.Conv1D(32, 8, \n                 padding='same',\n                 input_shape=(length_of_sequence, 1),\n                 activation='relu'))\nmodel.add(layers.MaxPooling1D(2, padding='same'))\nmodel.add(layers.Conv1D(64, 8, padding='same', activation='relu'))\nmodel.add(layers.MaxPooling1D(2, padding='same'))\nmodel.add(layers.Conv1D(128, 8, padding='same', activation='relu'))\nmodel.add(layers.MaxPooling1D(2, padding='same'))\nmodel.add(layers.Conv1D(256, 8, padding='same', activation='relu'))\nmodel.add(layers.LSTM(64, \n#               return_sequences=True,\n               dropout = drop_out_rate,\n               recurrent_dropout = recurrent_dropout\n              ))\n               #batch_input_shape=(None, 2, length_of_sequence)))\n#model.add(layers.LSTM(128, \n#               dropout = drop_out_rate,\n#               recurrent_dropout = recurrent_dropout\n#              ))\n#model.add(layers.Dense(100,activation='relu'))\nmodel.add(layers.Dense(1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy',matthews_corr_coeff])\nmodel.summary()","39ea8825":"weight_path=\"{}_weights.best.hdf5\".format('lstm_model')\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=10) # probably needs to be more patient, but kaggle time is limited\nlr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.001)\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\ncallbacks_list = [checkpoint, early, lr]","2201fa7d":"train_gen = train_data_gen(metadata_train, train_diff, batch_size=BATCH_SIZE)","e0f9d21d":"history = model.fit_generator(\n                train_gen,\n                steps_per_epoch=STEPS_PER_EPOCH,\n                epochs=EPOCHS,\n                validation_data=(x_val,y_val),\n                callbacks=callbacks_list)","cd95d294":"model.load_weights('lstm_model_weights.best.hdf5')","1b8a068d":"y_val_pred = model.predict(x_val)","04f545fa":"y_val_pred = y_val_pred.flatten()\ny_val_pred[y_val_pred >= 0.5] = 1\ny_val_pred[y_val_pred < 0.5] = 0\ny_val_pred.sum()","a922f4c2":"y_val.sum()","36b19880":"mcc(y_val,y_val_pred)","434b4b5f":"metadata_test = pd.read_csv('..\/input\/metadata_test.csv')\nmetadata_train = pd.read_csv('..\/input\/metadata_train.csv')","fc417c40":"test_parquet_path = '..\/input\/test.parquet'\nend_col_num = metadata_test['signal_id'].values[-1] + 1\ncol_nums = metadata_test['signal_id'].values[::500].tolist()\ntest_diff = read_wave_data(test_parquet_path,col_nums,end_col_num,merge_size=MERGE_SIZE)\nprint(test_diff.shape)","d92f5049":"x_test = []\nfor c in test_diff.columns:\n    diffs = test_diff[c].values.T\n    data = diffs[:, np.newaxis]\n    x_test.append(data)\n    \nx_test = np.array(x_test)\nprint(x_test.shape)","df565f1c":"y_test = model.predict(x_test)\ny_test = y_test.flatten()\ny_test[y_test >= 0.5] = 1\ny_test[y_test < 0.5] = 0\nprint(y_test.sum())","0ff39c43":"#y_test = np.array(y_test,dtype='bool')\nmetadata_test['target'] = pd.Series(y_test)","c292f885":"metadata_all = pd.concat([metadata_train, metadata_test])\nprint(metadata_all.shape)\nmetadata_all.head()","f88b2dcb":"all_diff = pd.concat([train_diff,test_diff],axis=1)\nprint(all_diff.shape)","0a4c643f":"metadata_train, metadata_val = train_test_split(metadata_all, test_size=0.2, random_state=42)\nprint(metadata_train.shape)\nprint(metadata_val.shape)","955aef24":"x_val = []\ny_val = []\nfor signal_id in metadata_val['signal_id'].values:\n    diffs = all_diff[str(signal_id)].values.T\n    data = diffs[:, np.newaxis]\n    x_val.append(data)\n    y_val.append(metadata_val[metadata_val['signal_id']==signal_id]['target'].values[0])\nx_val = np.array(x_val)\ny_val = np.array(y_val)    \nprint(x_val.shape)\nprint(y_val.shape)","20981571":"# Create Model\n\nmodel_2 = Sequential()\nmodel_2.add(layers.Conv1D(32, 8, \n                 padding='same',\n                 input_shape=(length_of_sequence, 1),\n                 activation='relu'))\nmodel_2.add(layers.MaxPooling1D(2, padding='same'))\nmodel_2.add(layers.Conv1D(64, 8, padding='same', activation='relu'))\nmodel_2.add(layers.MaxPooling1D(2, padding='same'))\nmodel_2.add(layers.Conv1D(128, 8, padding='same', activation='relu'))\nmodel_2.add(layers.MaxPooling1D(2, padding='same'))\nmodel_2.add(layers.Conv1D(256, 8, padding='same', activation='relu'))\nmodel_2.add(layers.LSTM(64, \n               return_sequences=True,\n               dropout = drop_out_rate,\n               recurrent_dropout = recurrent_dropout\n              ))\n               #batch_input_shape=(None, 2, length_of_sequence)))\nmodel_2.add(layers.LSTM(128, \n               dropout = drop_out_rate,\n               recurrent_dropout = recurrent_dropout\n              ))\n#model.add(layers.Dense(100,activation='relu'))\nmodel_2.add(layers.Dense(1,activation='sigmoid'))\n\nmodel_2.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy',matthews_corr_coeff])\nmodel_2.summary()","7576d10d":"weight_path=\"{}_weights.best.hdf5\".format('lstm_model_2')\nlr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.001)\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\ncallbacks_list = [checkpoint, lr]","ccb0866b":"train_gen = train_data_gen(metadata_all, all_diff, batch_size=BATCH_SIZE)","7e06e422":"history = model_2.fit_generator(\n                train_gen,\n                steps_per_epoch=STEPS_PER_EPOCH,\n                epochs=EPOCHS,\n                validation_data=(x_val,y_val),\n                callbacks=callbacks_list)","e519b2ca":"model_2.load_weights('lstm_model_2_weights.best.hdf5')","7460154b":"y_val_pred = model_2.predict(x_val)\ny_val_pred = y_val_pred.flatten()\ny_val_pred[y_val_pred >= 0.5] = 1\ny_val_pred[y_val_pred < 0.5] = 0\nprint(y_val_pred.sum())\nprint(y_val.sum())\nprint(mcc(y_val,y_val_pred))","bfc9098f":"y_test_1 = model.predict(x_test)\ny_test_2 = model_2.predict(x_test)\n\ny_test = (y_test_1.flatten()) * 0.5 + (y_test_2.flatten()) * 0.5\n#y_test = y_test_2.flatten()\ny_test[y_test >= 0.5] = 1\ny_test[y_test < 0.5] = 0\nprint(y_test.sum())","056a7112":"submit_df = pd.read_csv('..\/input\/sample_submission.csv')","36a5e31f":"y_test = np.array(y_test,dtype='bool')","70af71b6":"submit_df['target'] = pd.Series(y_test)\nsubmit_df['target'].astype('bool')\nsubmit_df.head()","45aed99e":"submit_df.to_csv('submission.csv',index=False)","ba391122":"Add predict label to test data and train again."}}