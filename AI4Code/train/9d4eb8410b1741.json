{"cell_type":{"91147a04":"code","47c79d79":"code","e9bde785":"code","86d8e964":"code","bbece794":"code","31825587":"code","f2d97560":"code","e50b7ac5":"code","e029a9ce":"code","98b7da92":"code","a47a8db1":"code","f979e689":"code","3c949097":"code","62fb992e":"code","b53d149c":"markdown","7b175f67":"markdown","017fd660":"markdown","aa5181ff":"markdown","d373600d":"markdown","03c228d7":"markdown","e1491bb6":"markdown","e4afa0c9":"markdown","82deec75":"markdown"},"source":{"91147a04":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns; sns.set()\nfrom matplotlib import pyplot as plt\n%config InlineBackend.figure_format = 'retina'","47c79d79":"xx = np.linspace(0,1,50)\nplt.plot(xx, [2 * x * (1-x) for x in xx], label='gini')\nplt.plot(xx, [4 * x * (1-x) for x in xx], label='2*gini')\nplt.plot(xx, [-x * np.log2(x) - (1-x) * np.log2(1 - x)  for x in xx], label='entropy')\nplt.plot(xx, [1 - max(x, 1-x) for x in xx], label='missclass')\nplt.plot(xx, [2 - 2 * max(x, 1-x) for x in xx], label='2*missclass')\nplt.xlabel('p+')\nplt.ylabel('criterion')\nplt.title('Criteria of quality as a function of p+ (binary classification)')\nplt.legend();","e9bde785":"# first class\nnp.random.seed(17)\ntrain_data = np.random.normal(size=(100, 2))\ntrain_labels = np.zeros(100)\nprint(train_data.shape)\n# adding second class\ntrain_data = np.r_[train_data, np.random.normal(size=(100, 2), loc=2)]\ntrain_labels = np.r_[train_labels, np.ones(100)]","86d8e964":"print(train_data.shape)","bbece794":"plt.figure(figsize=(10,8))\nplt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, \ncmap='autumn', edgecolors='black', linewidth=1.5);\nplt.plot(range(-2,5), range(4,-3,-1));","31825587":"from sklearn.tree import DecisionTreeClassifier\n\n# Let\u2019s write an auxiliary function that will return grid for further visualization.\ndef get_grid(data):\n    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n    return np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n\nclf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, \n                                  random_state=17)\n\n# training the tree\nclf_tree.fit(train_data, train_labels)\n\n# some code to depict separating surface\nxx, yy = get_grid(train_data)\npredicted = clf_tree.predict(np.c_[xx.ravel(), \n                                   yy.ravel()]).reshape(xx.shape)\nplt.pcolormesh(xx, yy, predicted, cmap='autumn')\nplt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, \n            cmap='autumn', edgecolors='black', linewidth=1.5);","f2d97560":"from sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf_tree, out_file='tree_limited.dot',  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n!dot -Tpng tree_limited.dot -o tree_limited.png -Gdpi=600\n\n\n","e50b7ac5":"from IPython.display import Image\nImage(filename = 'tree_limited.png')\n","e029a9ce":"data = pd.DataFrame({'Age': [17,64,18,20,38,49,55,25,29,31,33],\n                     'Loan Default': [1,0,1,0,1,0,0,1,1,0,1]})\ndata","98b7da92":"data.sort_values('Age')","a47a8db1":"age_tree = DecisionTreeClassifier(random_state=17)\nage_tree.fit(data['Age'].values.reshape(-1, 1), data['Loan Default'].values)\n\nexport_graphviz(age_tree, out_file='tree_limited.dot',  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n!dot -Tpng tree_limited.dot -o tree_limited.png -Gdpi=600\n\nfrom IPython.display import Image\nImage(filename = 'tree_limited.png')\n","f979e689":"data2 = pd.DataFrame({'Age':  [17, 64, 18, 20, 38, 49, 55, 25, 29, 31, 33], \n                      'Salary': [25, 80, 22, 36, 37, 59, 74, 70, 33, 102, 88],\n                      'Loan Default': [1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1]})\ndata2.sort_values('Age')\ndata2","3c949097":"age_sal_tree = DecisionTreeClassifier(random_state=17)\nage_sal_tree.fit(data2[['Age', 'Salary']].values, data2['Loan Default'].values);\nexport_graphviz(age_sal_tree, out_file='tree_limited.dot',  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n!dot -Tpng tree_limited.dot -o tree_limited.png -Gdpi=600\n\nfrom IPython.display import Image\nImage(filename = 'tree_limited.png')","62fb992e":"n_train = 150        \nn_test = 1000       \nnoise = 0.1\n\ndef f(x):\n    x = x.ravel()\n    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n\ndef generate(n_samples, noise):\n    X = np.random.rand(n_samples) * 10 - 5\n    X = np.sort(X).ravel()\n    y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 2) ** 2) + \\\n    np.random.normal(0.0, noise, n_samples)\n    X = X.reshape((n_samples, 1))\n    return X, y\n\nX_train, y_train = generate(n_samples=n_train, noise=noise)\nX_test, y_test = generate(n_samples=n_test, noise=noise)\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nreg_tree = DecisionTreeRegressor(max_depth=5, random_state=17)\n\nreg_tree.fit(X_train, y_train)\nreg_tree_pred = reg_tree.predict(X_test)\n\nplt.figure(figsize=(10, 6))\nplt.plot(X_test, f(X_test), \"b\")\nplt.scatter(X_train, y_train, c=\"b\", s=20)\nplt.plot(X_test, reg_tree_pred, \"g\", lw=2)\nplt.xlim([-5, 5])\nplt.title(\"Decision tree regressor, MSE = %.2f\" % (np.sum((y_test - reg_tree_pred) ** 2) \/ n_test))\nplt.show()\n","b53d149c":"Let's plot the data. Informally, the classification problem in this case is to build some \"good\" boundary separating the two classes (the red dots from the yellow). Machine learning for this case boils down to choosing a good separating border. A straight line will be too simple while some complex curve snaking by each red dot will be too complex and will lead us to making mistakes on new samples. Intuitively, some smooth boundary, or at least a straight line or a hyperplane, would work well on new data.","7b175f67":"Entropy\nShannon's entropy is defined for a system with N possible states as follows:\n\nS=\u2212\u2211pilog2pi,\n \nwhere  pi  is the probability of finding the system in the  i -th state. This is a very important concept used in physics, information theory, and other areas. Entropy can be described as the degree of chaos in the system. The higher the entropy, the less ordered the system and vice versa. ","017fd660":"Other Quality Criteria for Splits in Classification Problems\nWe discussed how entropy allows us to formalize partitions in a tree. But this is only one heuristic; there exists others:\n\nGini uncertainty (Gini impurity):  G=1\u2212\u2211k(pk)^2 . Maximizing this criterion can be interpreted as the maximization of the number of pairs of objects of the same class that are in the same subtree (not to be confused with the Gini index).\nMisclassification error:  E=1\u2212maxkpk \nIn practice, misclassification error is almost never used, and Gini uncertainty and information gain work similarly.\n\n\nIf we plot these two functions against the argument  p+ , we will see that the entropy plot is very close to the plot of Gini uncertainty, doubled. Therefore, in practice, these two criteria are almost identical.","aa5181ff":"Example\nLet's generate some data distributed by the function  f(x)=e\u2212x^2+1.5\u2217e\u2212(x\u22122)^2  with some noise. Then we will train a tree with this data and predictions that the tree makes.","d373600d":"Example\nLet's consider fitting a decision tree to some synthetic data. We will generate samples from two classes, both normal distributions but with different means.","03c228d7":"Crucial Tree Parameters\nTechnically, you can build a decision tree until each leaf has exactly one instance, but this is not common in practice when building a single tree because it will be overfitted, or too tuned to the training set, and will not predict labels for new data well.","e1491bb6":"Class DecisionTreeClassifier in Scikit-learn\nThe main parameters of the sklearn.tree.DecisionTreeClassifier class are:\n\nmax_depth \u2013 the maximum depth of the tree;\nmax_features - the maximum number of features with which to search for the best partition (this is necessary with a large number of features because it would be \"expensive\" to search for partitions for all features);\nmin_samples_leaf \u2013 the minimum number of samples in a leaf. This parameter prevents creating trees where any leaf would have only a few members.","e4afa0c9":"How to Build a Decision Tree\nEarlier, we saw that the decision to grant a loan is made based on age, assets, income, and other variables. But what variable to look at first? Let's discuss a simple example where all the variables are binary.","82deec75":"In our next case, we solve a binary classification problem (approve\/deny a loan) based on the \"Age\", \"Home-ownership\", \"Income\", and \"Education\" features.\n\nThe decision tree as a machine learning algorithm is essentially the same thing as the diagram shown above; we incorporate a stream of logical rules of the form \"feature  a  value is less than  x  and feature  b  value is less than  y  ... => Category 1\" into a tree-like data structure. The advantage of this algorithm is that they are easily interpretable. For example, using the above scheme, the bank can explain to the client why they were denied for a loan: e.g the client does not own a house and her income is less than 5,000."}}