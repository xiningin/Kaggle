{"cell_type":{"26de8393":"code","9f6dd02a":"code","f16cbd71":"code","0426aef6":"code","c5c725e2":"code","f978c2a8":"code","edbc7a92":"code","0d4ba4b4":"code","54e17b78":"code","f00eba72":"code","d74bbc3e":"code","10474432":"code","f3894d9c":"code","ef4e4844":"code","8841e439":"code","2e916db8":"code","f0e51dbb":"code","eb92f65d":"code","c4f7d6fc":"code","927ee3ff":"code","a088b051":"code","d31a6536":"code","973bcc9d":"code","8bc4657e":"code","6dbc4b8e":"code","8d9f1f1a":"code","8fff2404":"code","1fb0db44":"code","5ebe0f2b":"code","2d42beaf":"code","98dd5bde":"markdown","fb2a0f1c":"markdown","7d0be471":"markdown","b239a015":"markdown","d2c055e2":"markdown","6f4d5136":"markdown","9bd0b6f7":"markdown","e5497636":"markdown","890bb2b3":"markdown","6ce989bd":"markdown","f803b618":"markdown","e960c306":"markdown","c41e8d65":"markdown","5d91beb6":"markdown","e79ec381":"markdown"},"source":{"26de8393":"import torch\nimport torch.nn as nn\nimport torchvision \nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nimport numpy as np\nimport matplotlib.pyplot as plt","9f6dd02a":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","f16cbd71":"transform = transforms.Compose([transforms.RandomVerticalFlip(),\n                                transforms.RandomRotation(0.3),\n                                transforms.ToTensor(),\n                                transforms.Resize((80,80))\n                               ])\n\ndataset = torchvision.datasets.ImageFolder(root = '..\/input\/flowers-recognition\/flowers\/flowers',\n                                           transform = transform)\nprint(\"No of Classes: \", len(dataset.classes))\n\ntrain, val = torch.utils.data.random_split(dataset, [3000, 1323])\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train,\n                                           batch_size = 32, \n                                           shuffle = True)\n\nval_loader = torch.utils.data.DataLoader(dataset = val,\n                                         batch_size = 32, \n                                         shuffle = True)","0426aef6":"examples = enumerate(val_loader)\nbatch_idx, (example_data, example_targets) = next(examples)\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nfor i in range(6):\n  plt.subplot(2,3,i+1)\n  plt.tight_layout()\n  plt.imshow(example_data[i].numpy().transpose())\n  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n  plt.xticks([])\n  plt.yticks([])","c5c725e2":"Accuracies = []","f978c2a8":"class ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        \n        self.layer3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        \n        self.layer4 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        \n        self.layer5 = nn.Sequential(\n            nn.Conv2d(512, 1024, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        \n        self.fc1 = nn.Linear(2*2*1024, 256)\n        self.fc2 = nn.Linear(256, 512)\n        self.fc3 = nn.Linear(512, 5)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc1(out)\n        out = F.dropout(out, training=self.training)\n        out = self.fc2(out)\n        out = F.dropout(out, training=self.training)\n        out = self.fc3(out)\n        return F.log_softmax(out,dim=1)","edbc7a92":"model = ConvNet().to(device)","0d4ba4b4":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","54e17b78":"total_step = len(train_loader)\nLoss = []\nAcc = []\nVal_Loss = []\nVal_Acc = []\nn=15\n\n\nfor epoch in range(n):\n  acc = 0\n  val_acc = 0\n  for i, (images, labels) in enumerate(train_loader):\n    model.train()\n    images = images.to(device)\n    labels = labels.to(device)\n    \n    # Forward pass\n    outputs = model(images)\n    loss = criterion(outputs, labels)\n    \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    # Checking accuracy\n    preds = outputs.data.max(dim=1,keepdim=True)[1]\n    acc += preds.eq(labels.data.view_as(preds)).cpu().sum()\n    \n  acc = acc\/len(train_loader.dataset) * 100\n    \n  for i, (images, labels) in enumerate(val_loader):\n    model.eval()\n    images = images.to(device)\n    labels = labels.to(device)\n    \n    # Forward pass\n    outputs = model(images)\n    val_loss = criterion(outputs, labels)\n    \n    # Checking accuracy\n    preds = outputs.data.max(dim=1,keepdim=True)[1]\n    val_acc += preds.eq(labels.data.view_as(preds)).cpu().sum()\n    \n  val_acc = val_acc\/len(val_loader.dataset) * 100\n    \n  print(\"Epoch {} =>  loss : {loss:.2f};   Accuracy : {acc:.2f}%;   Val_loss : {val_loss:.2f};   Val_Accuracy : {val_acc:.2f}%\".format(epoch+1, loss=loss.item(), acc=acc, val_loss=val_loss.item(), val_acc=val_acc))\n  \n  Loss.append(loss)\n  Acc.append(acc)\n\n  Val_Loss.append(val_loss)\n  Val_Acc.append(val_acc)","f00eba72":"plt.plot(range(n),Loss)\nplt.plot(range(n),Val_Loss)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.title(\"Loss\")\nplt.legend([\"Training Loss\", \"Validation Loss\"])\nplt.show()","d74bbc3e":"plt.plot(range(n),Acc)\nplt.plot(range(n),Val_Acc)\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.title(\"Accuracy\")\nplt.legend([\"Training Accuracy\", \"Validation Accuracy\"])\nplt.show()","10474432":"model.eval()  \nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in val_loader:\n        y_pred = []\n        Images = images.to(device)\n        Labels = labels.to(device)\n        outputs = model(Images)\n        prediction_array = outputs.data\n        \n        _, predicted = torch.max(prediction_array, 1)\n        y_pred += predicted\n        total += Labels.size(0)\n        correct += (predicted == Labels).sum().item()\n        \n    acc = 100 * correct \/ total\n    Accuracies.append(acc)\n    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct \/ total))","f3894d9c":"transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n                                transforms.RandomRotation(0.2),\n                                transforms.ToTensor(),\n                                transforms.Resize((224,224))\n                               ])\n\ndataset = torchvision.datasets.ImageFolder(root = '..\/input\/flowers-recognition\/flowers\/flowers',\n                                           transform = transform)\nprint(\"No of Classes: \", len(dataset.classes))\n\ntrain, val = torch.utils.data.random_split(dataset, [3000, 1323])\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train,\n                                           batch_size = 32, \n                                           shuffle = True)\n\nval_loader = torch.utils.data.DataLoader(dataset = val,\n                                         batch_size = 32, \n                                         shuffle = True)","ef4e4844":"vgg = torchvision.models.vgg19(pretrained=True)","8841e439":"vgg","2e916db8":"vgg.classifier[6].out_features = 5\nfor param in vgg.features.parameters(): \n    param.requires_grad = False\n\nvgg = vgg.cuda()","f0e51dbb":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(vgg.parameters(), lr=0.001)","eb92f65d":"total_step = len(train_loader)\nLoss = []\nAcc = []\nVal_Loss = []\nVal_Acc = []\n\n\nfor epoch in range(n):\n  acc = 0\n  val_acc = 0\n  for i, (images, labels) in enumerate(train_loader):\n    vgg.train()\n    images = images.to(device)\n    labels = labels.to(device)\n    \n    # Forward pass\n    outputs = vgg(images)\n    loss = criterion(outputs, labels)\n    \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    # Checking accuracy\n    preds = outputs.data.max(dim=1,keepdim=True)[1]\n    acc += preds.eq(labels.data.view_as(preds)).cpu().sum()\n    \n  acc = acc\/len(train_loader.dataset) * 100\n    \n  for i, (images, labels) in enumerate(val_loader):\n    vgg.eval()\n    images = images.to(device)\n    labels = labels.to(device)\n    \n    # Forward pass\n    outputs = vgg(images)\n    val_loss = criterion(outputs, labels)\n    \n    # Checking accuracy\n    preds = outputs.data.max(dim=1,keepdim=True)[1]\n    val_acc += preds.eq(labels.data.view_as(preds)).cpu().sum()\n    \n  val_acc = val_acc\/len(val_loader.dataset) * 100\n    \n  print(\"Epoch {} =>  loss : {loss:.2f};   Accuracy : {acc:.2f}%;   Val_loss : {val_loss:.2f};   Val_Accuracy : {val_acc:.2f}%\".format(epoch+1, loss=loss.item(), acc=acc, val_loss=val_loss.item(), val_acc=val_acc))\n  \n  Loss.append(loss)\n  Acc.append(acc)\n\n  Val_Loss.append(val_loss)\n  Val_Acc.append(val_acc)","c4f7d6fc":"plt.plot(range(n),Loss)\nplt.plot(range(n),Val_Loss)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.title(\"Loss\")\nplt.legend([\"Training Loss\", \"Validation Loss\"])\nplt.show()","927ee3ff":"plt.plot(range(n),Acc)\nplt.plot(range(n),Val_Acc)\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.title(\"Accuracy\")\nplt.legend([\"Training Accuracy\", \"Validation Accuracy\"])\nplt.show()","a088b051":"vgg.eval()  \nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in val_loader:\n        y_pred = []\n        Images = images.to(device)\n        Labels = labels.to(device)\n        outputs = vgg(Images)\n        prediction_array = outputs.data\n        \n        _, predicted = torch.max(prediction_array, 1)\n        y_pred += predicted\n        total += Labels.size(0)\n        correct += (predicted == Labels).sum().item()\n\n    acc = 100 * correct \/ total\n    Accuracies.append(acc)\n    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct \/ total))","d31a6536":"resnet = torchvision.models.resnet18(pretrained=True)","973bcc9d":"resnet","8bc4657e":"ftr = resnet.fc.in_features\nresnet.fc = nn.Linear(ftr, 5)\n\nresnet = resnet.cuda()","6dbc4b8e":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(resnet.parameters(), lr=0.001)","8d9f1f1a":"total_step = len(train_loader)\nLoss = []\nAcc = []\nVal_Loss = []\nVal_Acc = []\nn=15\n\nfor epoch in range(n):\n  acc = 0\n  val_acc = 0\n  for i, (images, labels) in enumerate(train_loader):\n    resnet.train()\n    images = images.to(device)\n    labels = labels.to(device)\n    \n    # Forward pass\n    outputs = resnet(images)\n    loss = criterion(outputs, labels)\n    \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    # Checking accuracy\n    preds = outputs.data.max(dim=1,keepdim=True)[1]\n    acc += preds.eq(labels.data.view_as(preds)).cpu().sum()\n    \n  acc = acc\/len(train_loader.dataset) * 100\n    \n  for i, (images, labels) in enumerate(val_loader):\n    resnet.eval()\n    images = images.to(device)\n    labels = labels.to(device)\n    \n    # Forward pass\n    outputs = resnet(images)\n    val_loss = criterion(outputs, labels)\n    \n    # Checking accuracy\n    preds = outputs.data.max(dim=1,keepdim=True)[1]\n    val_acc += preds.eq(labels.data.view_as(preds)).cpu().sum()\n    \n  val_acc = val_acc\/len(val_loader.dataset) * 100\n    \n  print(\"Epoch {} =>  loss : {loss:.2f};   Accuracy : {acc:.2f}%;   Val_loss : {val_loss:.2f};   Val_Accuracy : {val_acc:.2f}%\".format(epoch+1, loss=loss.item(), acc=acc, val_loss=val_loss.item(), val_acc=val_acc))\n  \n  Loss.append(loss)\n  Acc.append(acc)\n\n  Val_Loss.append(val_loss)\n  Val_Acc.append(val_acc)\n","8fff2404":"plt.plot(range(n),Loss)\nplt.plot(range(n),Val_Loss)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.title(\"Loss\")\nplt.legend([\"Training Loss\", \"Validation Loss\"])\nplt.show()\n","1fb0db44":"plt.plot(range(n),Acc)\nplt.plot(range(n),Val_Acc)\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.title(\"Accuracy\")\nplt.legend([\"Training Accuracy\", \"Validation Accuracy\"])\nplt.show()","5ebe0f2b":"model.eval()  \nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in val_loader:\n        y_pred = []\n        Images = images.to(device)\n        Labels = labels.to(device)\n        outputs = resnet(Images)\n        prediction_array = outputs.data\n        \n        _, predicted = torch.max(prediction_array, 1)\n        y_pred += predicted\n        total += Labels.size(0)\n        correct += (predicted == Labels).sum().item()\n\n    acc = 100 * correct \/ total\n    Accuracies.append(acc)\n    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct \/ total))","2d42beaf":"plt.plot(range(3), Accuracies, color='green', linestyle='dashed', linewidth = 3, \n         marker='o', markerfacecolor='blue', markersize=12) \nplt.ylabel('Acc')\nplt.xlabel('Models')\nplt.title(\"Accuracies\")\nplt.xticks(range(3), ['Custom CNN', 'Vgg19', 'Resnet18'])\nplt.show()","98dd5bde":"### Model Evaluation","fb2a0f1c":"## ResNet18","7d0be471":"### Training model ","b239a015":"## Custom CNN","d2c055e2":"### Model Evaluation ","6f4d5136":"### Training Model","9bd0b6f7":"### Model Evaluation ","e5497636":"## Transfer Learning","890bb2b3":"## VGG19","6ce989bd":"### Comparison of accuracies of different models","f803b618":"### Model Training","e960c306":"### Loss and Optimizer ","c41e8d65":"### Loss and optimizer","5d91beb6":"### Loss and Optimizer","e79ec381":"### Import necessary libraries"}}