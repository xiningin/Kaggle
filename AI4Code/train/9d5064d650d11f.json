{"cell_type":{"1709c90c":"code","3d97e154":"code","2eec7c33":"code","500b9fc4":"code","5d6f40e1":"code","b9d128ca":"code","1dd3713e":"code","832faad0":"code","4b7a9f55":"code","230ca219":"code","98e3acec":"code","12301a70":"code","3b8a6bb0":"code","171354ff":"code","e0945ea7":"code","c50f0a5c":"code","d36d4003":"code","b47a538e":"code","73f7f607":"code","bfebc67e":"code","1df26615":"code","9f5d2d1f":"code","34645b83":"code","02d44185":"code","bca04f6a":"code","ee236ff0":"code","b08de068":"markdown","3cce3cf7":"markdown","011eed7a":"markdown"},"source":{"1709c90c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nimport torch.utils.data\nimport collections\nfrom scipy.spatial import KDTree\nfrom tqdm import tqdm_notebook\nimport pdb\n\nfrom pathlib import Path\npath = Path(\"..\/input\")\nlist(path.iterdir())","3d97e154":"#Make sure GPU is on \ntorch.cuda.device_count()","2eec7c33":"#Read in training data\nfullDF = pd.read_json(path\/\"train.json\")","500b9fc4":"#Create a list of all unique ingredients\nvocab = list({s for l in fullDF.ingredients for s in l})\nstoi = collections.defaultdict(lambda: len(vocab),{s:i for i,s in enumerate(vocab)})\npadIdx = len(vocab)+1","5d6f40e1":"#Set cuisine type as a categorical column\nfullDF.cuisine = fullDF.cuisine.astype(\"category\")\n\n#Create new column, to be passed to the model, which contains an array of ingredients by position in the vocab array\nfullDF[\"x\"] = fullDF.ingredients.apply(lambda l: np.array([stoi[s] for s in l]))","b9d128ca":"nCuisines = len(fullDF.cuisine.cat.categories)\n#Dictionary which converts cuisine index to string value\nitosCuisine = {i:c for i,c in enumerate(fullDF.cuisine.cat.categories)}","1dd3713e":"#Our processed dataframe\nfullDF.head()","832faad0":"#Extremely basic dataset class\nclass RecipeDataset(torch.utils.data.Dataset):\n    def __init__(self,x,y):\n        self.x, self.y = x,y\n        \n    def __len__(self): return len(self.x)\n    \n    def __getitem__(self,idx): return self.x[idx], self.y[idx]","4b7a9f55":"#Split train\/valid sets\nnp.random.seed(5342)\nvalDF = fullDF.sample(frac=0.15,replace=False)\ntrainDF = fullDF[~fullDF.index.isin(valDF.index)]\nassert len(valDF) + len(trainDF) == len(fullDF)","230ca219":"#Create three different datasets. fullDS contains all rows in training data\nfullDS = RecipeDataset(fullDF.x.values,fullDF.cuisine.cat.codes.values)\ntrainDS = RecipeDataset(trainDF.x.values,trainDF.cuisine.cat.codes.values)\nvalDS = RecipeDataset(valDF.x.values,valDF.cuisine.cat.codes.values)","98e3acec":"#Custom collate function which takes a batch of samples and embeds them in a tensor (sequence length,batch size), padded out to the max ingredient list length of the batch\ndef collate(samples):\n    bs = len(samples)\n    maxLen = max(len(s[0]) for s in samples)\n    out = torch.zeros(maxLen,bs,dtype=torch.long) + padIdx\n    for i,s in enumerate(samples):\n        out[:len(s[0]),i] = torch.tensor(s[0],dtype=torch.long)\n    return out.cuda(), torch.tensor([s[1] for s in samples],dtype=torch.long).cuda()\n\n#Create the dataloaders\nbs = 64\ntrainDL = torch.utils.data.DataLoader(trainDS,bs,shuffle=True,collate_fn=collate)\nvalDL = torch.utils.data.DataLoader(valDS,bs,collate_fn=collate)\nfullDL = torch.utils.data.DataLoader(fullDS,bs,collate_fn=collate,shuffle=True)","12301a70":"#copied from https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/layers.py#L116, implements He initialization for the embedding layer\ndef trunc_normal_(x:torch.tensor, mean:float=0., std:float=1.) -> torch.tensor:\n    \"Truncated normal initialization.\"\n    # From https:\/\/discuss.pytorch.org\/t\/implementing-truncated-normal-initializer\/4778\/12\n    return x.normal_().fmod_(2).mul_(std).add_(mean)","3b8a6bb0":"class CuisineNet(torch.nn.Module):\n    def __init__(self,nIngred,embSize,hiddenSize,nCuisines):\n        super().__init__()\n        self.hiddenSize = hiddenSize\n        self.ingredEmb = torch.nn.Embedding(nIngred,embSize,padding_idx = padIdx)\n        self.embDropout = torch.nn.Dropout(0.5)\n        with torch.no_grad(): trunc_normal_(self.ingredEmb.weight, std=0.01) # Use He initilization on the embedding layer\n        self.ingredEnc = torch.nn.GRU(embSize,hiddenSize,2,dropout=0.90,bidirectional=True)\n        self.encDropout = torch.nn.Dropout(0.5)\n        self.out = torch.nn.Linear(hiddenSize*2,nCuisines)\n        \n    def forward(self,inp):\n        sl, bs = inp.size()\n        inp = self.embDropout(self.ingredEmb(inp))\n        enc,h = self.ingredEnc(inp,torch.zeros(4,bs,self.hiddenSize).cuda())\n        #Since we are using a bidrectional GRU, we need to concat the forward state to the backward state, then pass it to the output layer\n        return self.out(self.encDropout(torch.cat([h[-2],h[-1]],dim=1)))","171354ff":"#initialize the model. The number of embeddings it two larger than the vocab size, since we need embeddings for padding and unknown. The embedding dimension is 100, and the \n#hidden size of the GRU is 400 (since it is bidrectional, we end up with an output of size 800).\nmodel = CuisineNet(len(vocab)+2,100,400,nCuisines).cuda()","e0945ea7":"#Grab a batch from the dataloader, and pass it through the model to make sure the output shape is correct\nx,y = next(iter(trainDL))\nmodel(x)","c50f0a5c":"#function to calculate the average accuracy of a batch\ndef batchAccuracy(preds,target):\n    preds = torch.softmax(preds,dim=1)\n    preds = torch.argmax(preds,dim=1)\n    o = (preds == target).sum().item()\n    return o \/ len(preds)","d36d4003":"#fit function\ndef learn(model,epochs,lr,trainDL,valDL=None):\n    lossFn = torch.nn.functional.cross_entropy\n    optimizer = torch.optim.Adam(model.parameters(),lr=lr,amsgrad=True,weight_decay=5e-4)\n\n    for e in tqdm_notebook(range(epochs)):\n        model.train()\n        with tqdm_notebook(iter(trainDL),leave=False) as t:\n            bloss, n = 0.0,0\n            for x,y in t:\n                pred = model(x)\n                loss = lossFn(pred,y)\n                bloss += loss.item()\n                n += 1\n                t.set_postfix({\"loss\": bloss \/ n})\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            print(f\"Epoch {e+1} Training Set Loss: {bloss \/ n}\")\n        if valDL is not None:\n            model.eval()\n            with torch.no_grad():\n                loss,accuracy,n =0.0,0.0,0\n                for x,y in tqdm_notebook(iter(valDL),leave=False):\n                    pred = model(x)\n                    loss += lossFn(pred,y)\n                    accuracy += batchAccuracy(pred,y)\n                    n += 1\n                print(f\"Validation Set Loss: {loss \/ n}, Accuracy: {accuracy \/ n}\")","b47a538e":"#For some reason tqdm progress bars don't display correctly in the kernel (bar is missing).\n\n#The model is trained for 18 epochs, lowering the learning rate every 6 epochs\nlearn(model,6,1e-3,fullDL)","73f7f607":"learn(model,6,1e-4,fullDL)","bfebc67e":"learn(model,6,1e-5,fullDL)","1df26615":"torch.save(model.state_dict(),\"model.pth\")","9f5d2d1f":"testDF = pd.read_json(path\/\"test.json\")\ntestDF[\"x\"] = testDF.ingredients.apply(lambda l: np.array([stoi[s] for s in l]))","34645b83":"testDS = RecipeDataset(testDF.x.values,testDF.id.values)\ntestDL = torch.utils.data.DataLoader(testDS,bs,collate_fn=collate)","02d44185":"def testModel():\n    o = []\n    model.eval()\n    with torch.no_grad():\n        for x,ids in tqdm_notebook(iter(testDL),leave=False):\n            preds = model(x)\n            preds = torch.softmax(preds,dim=1)\n            preds = torch.argmax(preds,dim=1)\n            for c,id in zip(preds,ids): o.append([id.item(),itosCuisine[c.item()]])\n    return pd.DataFrame(o,columns=[\"id\",\"cuisine\"])","bca04f6a":"t = testModel()\nt.head()","ee236ff0":"t.to_csv(\"out.csv\",index=False)","b08de068":"> This code uses only Pytorch, but is heavily inspired by fast.ai.","3cce3cf7":"# What's Cooking with bidirectional GRU in Pytorch","011eed7a":"This is the class for the model. The first layer is an embedding matrix, which maps each ingredient to a vector. Next is the GRU, which encodes the list of ingredients. The last timestep of output from the GRU is then put through a linear layer, which outputs a vector of size nCuisines. Dropout is added at each layer. \n\nThe approach is to use a large number of parameters, while also utilizing large amounts of regularization, specifically dropout and weight decay. "}}