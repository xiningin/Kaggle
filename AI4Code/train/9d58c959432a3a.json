{"cell_type":{"66fb308b":"code","67d30b11":"code","c37811f9":"code","a47611dd":"code","875e8ff4":"code","3d88490b":"code","9be3af38":"code","705c489d":"code","97c412c6":"markdown","a31be829":"markdown","3046b10b":"markdown","ee31fa66":"markdown","dc082390":"markdown"},"source":{"66fb308b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # basic plotting\nimport seaborn as sns # somewhat fancier plotting\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","67d30b11":"def rmsle(y_true, y_pred):\n    \"\"\"return root mean squared log error between true and predicted value lists\"\"\"\n    return np.sqrt(np.mean(np.power(np.log1p(y_true) - np.log1p(y_pred),2)))\n\n# load the data and do some preliminary column engineering\n\ntrain_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-2\/train.csv\", header=0, parse_dates=['Date'])\ntest_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-2\/test.csv\", header=0, parse_dates=['Date'])\nhs_df = pd.read_csv(\"..\/input\/world-bank-wdi-212-health-systems\/2.12_Health_systems.csv\", header=0)\n\nhs_numeric_cols = ['Health_exp_pct_GDP_2016', 'Health_exp_public_pct_2016',\n       'Health_exp_out_of_pocket_pct_2016', 'Health_exp_per_capita_USD_2016',\n       'per_capita_exp_PPP_2016', 'External_health_exp_pct_2016',\n       'Physicians_per_1000_2009-18', 'Nurse_midwife_per_1000_2009-18',\n       'Specialist_surgical_per_1000_2008-18',\n       'Completeness_of_birth_reg_2009-18',\n       'Completeness_of_death_reg_2008-16']\n\n# make a singluar area that will be country and subdivision together\n\ntrain_df['Area'] = train_df['Country_Region'].str.cat(train_df['Province_State'], sep=\"\/\", na_rep='').str.replace('\\\/$', '')\ntest_df['Area'] = test_df['Country_Region'].str.cat(test_df['Province_State'], sep=\"\/\", na_rep='').str.replace('\\\/$', '')\nhs_df['Area'] = hs_df['Country_Region'].str.cat(hs_df['Province_State'], sep='\/', na_rep='').str.replace('\\\/$', '')\nhs_df.set_index('Area', inplace=True)\ntrain_df.set_index('Area', inplace=True)\n\n# drop training dates on or after the first testing date to prevent data leakage\nmin_test_date = test_df['Date'].min()\ntrain_df = train_df.loc[train_df['Date'] < min_test_date]\n\nprint(\"{0} unique dates in training\".format(len(list(train_df['Date'].unique()))))\nprint(\"{0} unique dates in testing\".format(len(list(test_df['Date'].unique()))))\n\ntrain_df = train_df.join(hs_df, on='Area', rsuffix='_hs')\n\n# calculate the case:fatality ratio\ntrain_df['FatalityRatio'] = train_df.apply(lambda x: x['Fatalities'] \/ x['ConfirmedCases'] if x['ConfirmedCases'] > 0 else np.nan, axis=1)\n\n# fill in NaN values in the health system columns of the test and train set with the median value calculated in the test set\nfor c in hs_numeric_cols:\n    median = np.nanmedian(train_df[c])\n    train_df[c].fillna(value=median, axis=0, inplace=True)\n\nprint(train_df[hs_numeric_cols].isnull().sum())\ntest_df['ConfirmedCases'] = pd.Series()\ntest_df['Fatalities'] = pd.Series()\nprint(test_df.columns)\n","c37811f9":"from scipy.stats import linregress\n\npolyfit_degree = 2\n\nfitted_lin_slopes = []\nfatality_slopes = []\nfatality_ratios = []\n\n# to build prediction\npredicted_ConfirmedCases = []\npredicted_Fatalities = []\n\ndocs = []\nnurses = []\npub_spend = []\nper_cap_ppp = []\n\ntrain_df['Area'] = train_df.index\n#print(train_df['Area'])\n\n#area_list = list(train_df.index.unique())\narea_list = list(train_df['Area'].unique())\n\n# get the number of days we need to predict\ndays_to_predict = len(list(test_df['Date'].unique()))\ndays_of_data = len(list(train_df['Date'].unique()))\n\nsub_df = pd.DataFrame({'ForecastId':[],\n                      'ConfirmedCases': [],\n                      'Fatalities': []\n                      },dtype=np.int64)\n\nprint(\"will predict {0} days from {1} days of data\".format(days_to_predict, days_of_data))\n\nfor one_area in area_list:\n    \n    # build model off of days since 100th case, as this is more stable\n    area_df = train_df.loc[(train_df['Area'] == one_area) & (train_df['ConfirmedCases'] >= 100)]\n    \n    # if that isn't possible, use everything\n    if area_df.shape[0] < 3:\n    \n        area_df = train_df.loc[train_df['Area'] == one_area]\n    \n    # log transform the cases, model growth as log linear, predict, and assess accuracy\n    area_df['ConfirmedCases_log1p'] = area_df.apply(lambda x: np.log1p(x['ConfirmedCases']), axis=1)\n    ys = list(area_df['ConfirmedCases_log1p'])\n    xs = range(1, area_df.shape[0] + 1)\n    logm, logb, logr, logp, logstd_err = linregress(xs, ys)\n    y_case_pred_lin = np.maximum(np.zeros(len(xs)), np.round(np.expm1((xs * logm) + logb), 0))\n    case_logrmsle = rmsle(list(area_df['ConfirmedCases']), y_case_pred_lin)\n    print(\"{0} case loglin log1p(y) = {1:.3f}x + {2:.3f} r={3:.3f} sterr={4:.3f} rmsle={5:.3f}\".format(one_area, logm, logb, logr, logstd_err, case_logrmsle))\n\n    # model fatalities as mean case:fatality ratio for this area\n    ratios = list(area_df['FatalityRatio'])\n    mean_fatality_ratio = np.nanmean(ratios)\n    \n    if np.isnan(mean_fatality_ratio):\n        \n        # make global mean fatality ratio\n        print(\"-->{0} had no fatalities, going with global mean of {1:.3f}\".format(one_area, np.nanmean(list(train_df['FatalityRatio']))))\n        mean_fatality_ratio = np.nanmean(list(train_df['FatalityRatio']))\n    \n    xs = list(area_df['ConfirmedCases'])\n    y_fat_pred = np.round(np.array(xs) * mean_fatality_ratio, 0)\n    fat_rmsle = rmsle(list(area_df['Fatalities']), y_fat_pred)\n    print(\"{0} mean fatality ratio fat:cases={1:.3f} rmsle={2:.3f}\".format(one_area, mean_fatality_ratio, fat_rmsle))\n\n    # model fatalities as regressed from fatality data\n    ys = list(area_df['Fatalities'])\n    fm, fb, fr, fp, fstd_err = linregress(xs, ys)\n    y_fat_lin_pred = np.maximum(np.zeros(len(xs)), np.round((np.array(xs) * fm) + fb, 0))\n    fat_lin_rmsle = rmsle(list(area_df['Fatalities']), y_fat_lin_pred)\n    print(\"{0} fatality lin reg y = {1:.3f}x + {2:.3f} r={3:.3f} sterr={4:.3f} rmsle={5:.3f}\".format(one_area, fm, fb, fr, fstd_err, fat_lin_rmsle))\n   \n    # predict the ConfirmedCases for this area\n    pred_xs = range(area_df.shape[0], area_df.shape[0] + days_to_predict)\n    predicted_ConfirmedCases = np.maximum(np.zeros(len(pred_xs)), np.round(np.expm1((pred_xs * logm) + logb), 0))\n    print(predicted_ConfirmedCases)\n    # predict fatalities from the better of the two metrics for this area\n    \n    if fat_lin_rmsle < fat_rmsle:\n        \n        # use linear regression of raw data\n        predicted_Fatalities = np.maximum(np.zeros(len(pred_xs)), np.round((np.array(pred_xs) * fm) + fb, 0))\n    \n    else:\n        \n        # use case:fatality ratio\n        predicted_Fatalities = np.round(np.array(predicted_ConfirmedCases) * mean_fatality_ratio, 0)\n\n    # add data to submission dataframe\n    ids = test_df.loc[test_df['Area'] == one_area, 'ForecastId']\n    sub_df = pd.concat([sub_df, pd.DataFrame({'ForecastId' : ids,\n                                            'ConfirmedCases' : predicted_ConfirmedCases,\n                                            'Fatalities' : predicted_Fatalities\n                                            },dtype=np.int64)])\n    \n    # catalog data points for eventual modeling\n    docs.append(train_df.loc[(train_df['Area'] == one_area),'Physicians_per_1000_2009-18'].unique()[0])\n    nurses.append(train_df.loc[(train_df['Area'] == one_area),'Nurse_midwife_per_1000_2009-18'].unique()[0])\n    pub_spend.append(train_df.loc[(train_df['Area'] == one_area),'Health_exp_public_pct_2016'].unique()[0])\n    per_cap_ppp.append(train_df.loc[(train_df['Area'] == one_area),'per_capita_exp_PPP_2016'].unique()[0])\n    fitted_lin_slopes.append(logm)\n    fatality_ratios.append(mean_fatality_ratio)\n    fatality_slopes.append(fm)\n\nprint(\"Submission dataframe:\")\nprint(sub_df.describe())\n\nmodeling_df = pd.DataFrame({'Area' : area_list, \n                            'ConfirmedCases_fitted_lin_slope' : fitted_lin_slopes, \n                            'Fatalities_slope' : fatality_slopes,\n                            'Fatality_ratio' : fatality_ratios, \n                            'Doctor_per_1k' : docs,\n                            'Nurse_midwife_per_1k' : nurses, \n                            'Pct_health_spending_public_money' : pub_spend, \n                            'Per_capita_health_spend_PPP' : per_cap_ppp\n                           })","a47611dd":"# write it out\nprint(sub_df.dtypes)\nprint(sub_df.head())\nprint(sub_df.describe())\nsub_df.to_csv('submission.csv', header=True, index=False)\n","875e8ff4":"#print(modeling_df['Doctor_per_1k'])\nprint(modeling_df.describe())\nprint(modeling_df.isnull().sum())\nprint(sns.pairplot(modeling_df))\n","3d88490b":"# We've got 28 null values in the fatality slope column; let's fill them in with the mean column value.\n\nmean_fat_slope = np.nanmean(modeling_df['Fatalities_slope'])\nmodeling_df['Fatalities_slope'].fillna(mean_fat_slope, inplace=True)\nprint(modeling_df.isnull().sum())\n                            ","9be3af38":"# split out our target values and drop them plus the row label Area\n\ny_case_slope = modeling_df['ConfirmedCases_fitted_lin_slope']\ny_fat_slope = modeling_df['Fatalities_slope']\ny_fat_ratio = modeling_df['Fatality_ratio']\nX = modeling_df.drop(['Area','ConfirmedCases_fitted_lin_slope', 'Fatalities_slope','Fatality_ratio'], axis=1)\nprint(X.columns)\nprint(X.shape)\n","705c489d":"# define a seed for repeatability\nrand_seed = 112358\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nkf = KFold(n_splits=5, shuffle=True, random_state=rand_seed)\nerrors = []\n\ntarget_dict = {'case_slope': y_case_slope,\n              'fat_slope' : y_fat_slope,\n              'fat_ratio' : y_fat_ratio\n              }\n\n# for each of the targets\nfor name, y in target_dict.items():\n    \n    # do cross-val splitting\n    for train_index, test_index in kf.split(X):\n\n        X_train, X_test, y_train, y_test = train_test_split(X.loc[train_index], y.loc[train_index], test_size=0.25, random_state=rand_seed)\n        #X_train, X_test = np.array(X.loc[train_index]), np.array(X.loc[test_index])\n        #y_train, y_test = np.array(y.loc[train_index]), np.array(y.loc[test_index])\n        \n        rfr = RandomForestRegressor()\n        rfr.fit(X_train, y_train)\n        print(rfr.estimators_[0])\n        \n        y_pred = rfr.predict(X_test)\n        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n        print(\"{0} mse: {1:.3f}\".format(name, mean_squared_error(y_test, y_pred)))\n        print(\"{0} mae: {1:.3f}\".format(name, mean_absolute_error(y_test, y_pred)))\n        print(\"{0} rmse: {1:.3f}\".format(name, rmse))\n        print(\"{0} score: {1:3f}\".format(name, rfr.score(X_test, y_test)))\n        \n        errors.append(mean_absolute_error(y_test, y_pred))\n        \n        print(\"Feature importances: \")\n        impdict = dict(list(zip(X.columns, rfr.feature_importances_)))\n        print(impdict)\n        \n    print(\"{0} average mae: {1:.3f}\".format(name, np.mean(errors)))","97c412c6":"# Okay! Time for some modeling.\nNow that we have isolated per-area metrics for the Health Systems data in relation to the slope of their case and fatality metrics, let's see if we can regress those growth metrics based on the information from the World Bank.\n\nBut first, some EDA and data cleaning.","a31be829":"# How we'll start\nI did the California Week 1 challenge and got a score a bit over .08. Let's see if I can build on that success.\n- log1p transform the case data to predict its log-linear growth\n- check the case:fatality ratio for predicting fatalities\n- start bringing in some external data to see if we can get any data on rate of spread per country\/area\n","3046b10b":"So let's now start looking at each area individually and doing forecasting based on the available data.\n\nModeling from various groups has started looking at models \"since the 100th case\", since models before that point are very sensitive\/fragile. Therefore, if an area has at least three days with more than 100 cases, we will use just that data to model. Otherwise, we'll do our best with what we've got.\n\nAlso, let's compare linregress on transformed data vs polyfit on non-transformed to see which is more accurate.","ee31fa66":"# Regressing the slopes of various lines doesn't seem to work overly well, but should be investigated more fully.\n\nLet's write out our testing values for the information we've defined above.\n","dc082390":"# Write sub_df as submission.csv "}}