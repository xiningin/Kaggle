{"cell_type":{"3faff67e":"code","5f4640dc":"markdown","70802103":"markdown","a0e3ee57":"markdown","42b4acb5":"markdown","34b0a0e4":"markdown","dbf0f2c9":"markdown","9b94057c":"markdown","463087b0":"markdown"},"source":{"3faff67e":"import numpy as np\nimport matplotlib.pyplot as plt\n\nxs = np.linspace(0.00000001, 1, 1000)\nys = np.log(xs)\n\nplt.plot(xs, ys, label='Actual Log')\nplt.plot(xs, -ys, label='Neg. Log')\n\nplt.xlabel('Probabilities [0+e, 1]')\nplt.legend()\nplt.show()","5f4640dc":"$$ - \\sum{y_{true} \\log{P(x_i)}} \\,\\,\\,\\, \\text{For every class where } P(x_i) \\text{ is } \\hat{y}$$\n\n> - $y_{true}$ is ground truth `1` for expected class `0` for others (Hence works as **NLL above!!!**)\n> - $P(x_i)$ i.e $\\hat{y}$ is presented by [softmax](link) \/ [sigmoid](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/nn\/weighted_cross_entropy_with_logits) at final layer\n\n- We **use distributions**        \n\nAlso known as **SPARSE CROSS ENTROPY LOSS**<br>\nOnly difference is, instead of using sparse one-hot-encoded vector $y$ as `[0, 0, 0, 0, 1]` for `class E`, we use numeric values such as `4` ","70802103":"# 00. Negative Log Liklihood Loss (NLL)","a0e3ee57":"\n- Probability distributions (not simple yes\/no)\n- As we are minimizing in optimisation, negative log is taken\n   - **Significance of Neg. sign:** if $P \\uparrow \\Rightarrow L\\downarrow$. Hence **minimizes loss** \n       - By mirror imaging actual log vals as in fig above\n   - **Significance of Log:** small change in P $\\Rightarrow$ **Large change in $L$**\n       -  Loss is improved\/penalized exponentially w\/ change in P\n- Computed <span style='color:red'>only for CORRECT LABEL<\/span>\n- Always positive! ([A loss val should always be](https:\/\/www.kaggle.com\/l0new0lf\/why-loss-always-positive))\n   - Log of P is taken and P $\\in$ [0, 1] => log(P) is always Negative => **-** log(P) is always Positive\n\n![image.png](attachment:image.png)\n\n[Image sorce](https:\/\/ljvmiranda921.github.io\/notebook\/2017\/08\/13\/softmax-and-the-negative-log-likelihood\/#:~:text=The%20negative%20log-likelihood%20becomes,less%20unhappy%20at%20larger%20values.)\n","42b4acb5":"# 01. Cross Entropy Loss (Multinomial Log Loss)","34b0a0e4":"# 02. Log Loss \/ Logistic Loss\n\n> Cross entropy loss when num of classes is `2`","dbf0f2c9":"> - *Log is not suitable for optimisation but neg. log is!*\n> - *Additionally Loss is improved\/penalized exponentially w\/ change in P*","9b94057c":"> Based on Negative Log Liklihood loss","463087b0":"# 03. Weighted and Balanced Cross Entropy\n\ncoming soon"}}