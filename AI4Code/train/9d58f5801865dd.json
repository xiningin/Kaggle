{"cell_type":{"cf0545d8":"code","162b21cc":"code","abf2bb25":"code","fff1ed9e":"code","3fabe042":"code","25a0d709":"code","af18c278":"code","c8b27008":"code","c058de02":"code","ba34bafa":"code","4015ec8b":"code","11231446":"code","2354d351":"code","25d3fb8a":"code","ec7ab624":"code","1aa64706":"code","bfd26585":"code","f8ae13f7":"markdown","f285be7a":"markdown"},"source":{"cf0545d8":"import pandas as pd \nimport numpy as np","162b21cc":"DATA_DIR = \"\/kaggle\/input\/summeranalytics2020\/\"\n\ntrain = pd.read_csv(DATA_DIR+\"train.csv\", index_col=\"Id\")\ntest = pd.read_csv(DATA_DIR+\"test.csv\", index_col=\"Id\")\ntrain.head()","abf2bb25":"train.info()","fff1ed9e":"from sklearn.preprocessing import LabelEncoder\n\ndef train_cats(train_df, test_df):\n    for col in train_df.columns:\n        if train_df[col].dtype == 'O' :\n            le = LabelEncoder()\n            train_df[col] = le.fit_transform(train_df[col])\n            test_df[col] = le.transform(test_df[col])\n    return train_df, test_df\n            \ntrain, test = train_cats(train, test)\ntrain.info()","3fabe042":"X = train.drop(columns=\"Attrition\")\ny = train.Attrition\nX.shape, y.shape, test.shape","25a0d709":"from sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_score","af18c278":"rfc = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=7, min_samples_leaf=5)\ncross_val_score(rfc, X, y, cv=5, n_jobs=-1, verbose=1, scoring='roc_auc').mean()","c8b27008":"DATA_DIR = \"\/kaggle\/input\/summeranalytics2020\/\"\n\ntrain = pd.read_csv(DATA_DIR+\"train.csv\", index_col=\"Id\")\ntest = pd.read_csv(DATA_DIR+\"test.csv\", index_col=\"Id\")\ntrain.head()","c058de02":"print(f\"Number of duplicates in the training data are {train.duplicated().sum()} of {len(train)}, ie {(100* train.duplicated().sum()\/len(train)).round(2)} % of data duplicated\")\nprint(f\"Number of duplicates in the testing data are {test.duplicated().sum()} of {len(test)}, ie {(100* test.duplicated().sum()\/len(test)).round(2)} % of data duplicated\")","ba34bafa":"train.drop_duplicates(inplace=True)\nprint(f\"Number of duplicates in the training data are {train.duplicated().sum()} of {len(train)}, ie {(100* train.duplicated().sum()\/len(train)).round(2)} % of data duplicated\")","4015ec8b":"from sklearn.preprocessing import LabelEncoder\n\ndef train_cats(train_df, test_df):\n    for col in train_df.columns:\n        if train_df[col].dtype == 'O' :\n            le = LabelEncoder()\n            train_df[col] = le.fit_transform(train_df[col])\n            test_df[col] = le.transform(test_df[col])\n    return train_df, test_df\n            \ntrain, test = train_cats(train, test)\ntrain.info()","11231446":"X = train.drop(columns=\"Attrition\")\ny = train.Attrition\nX.shape, y.shape, test.shape","2354d351":"rfc = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=7, min_samples_leaf=5)\ncross_val_score(rfc, X, y, cv=5, n_jobs=-1, verbose=1, scoring='roc_auc').mean()","25d3fb8a":"lgbmc = LGBMClassifier(random_state=7, n_estimators=100, colsample_bytree=0.5, \n                       max_depth=2, learning_rate=0.1, boosting_type='gbdt')\ncross_val_score(lgbmc, X, y, cv=5, n_jobs=-1, verbose=1, scoring='roc_auc').mean()","ec7ab624":"xgbc = XGBClassifier(seed=7, n_jobs=-1, n_estimators=100, random_state=7, max_depth=2, learning_rate=0.1)\ncross_val_score(xgbc, X, y, cv=5, n_jobs=-1, verbose=1, scoring='roc_auc').mean()","1aa64706":"from sklearn.ensemble import VotingClassifier\nensemble = VotingClassifier(estimators=[('rfc', rfc), ('xgbc', xgbc), ('lgbmc', lgbmc)],\n                                         voting='soft', n_jobs=-1)\ncross_val_score(ensemble, X, y, cv=5, n_jobs=-1, verbose=1, scoring='roc_auc').mean()","bfd26585":"ensemble.fit(X, y)\ny_pred = ensemble.predict_proba(test)[:, 1]\nsub_df = pd.DataFrame({\"Id\":test.index, \"Attrition\": y_pred})\nsub_df.to_csv(\"SA_submission_2.csv\", index=False)","f8ae13f7":"# The Common Approach","f285be7a":"# What Else Could Have been done Instead?"}}