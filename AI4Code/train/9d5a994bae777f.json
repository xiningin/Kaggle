{"cell_type":{"9b81ad0b":"code","43f95a3d":"code","da4b9c7c":"code","9b7d626a":"code","a899f790":"code","966d30f2":"code","57c38b9c":"code","d0ab2679":"code","e31e61b7":"markdown","35ab0b40":"markdown","a7c55641":"markdown","9d597c69":"markdown","95b331d0":"markdown","1fac8eb3":"markdown","c8983fb7":"markdown"},"source":{"9b81ad0b":"pip install pickledb","43f95a3d":"import json\nimport pandas as pd\n\ndef extractData(csvfile, jsonfile):\n    data = pd.read_csv(csvfile) # add exception handling incase filepath it wrong or does not exists\n    data_with_abstract = data[data.abstract.notnull()]\n    with open(jsonfile, 'a+') as json_out_file:\n        for index, row in data_with_abstract.iterrows():\n            json_data = {}\n            json_data[\"id\"] = index\n            json_data['articleID'] = row['cord_uid']\n            json_data[\"text\"] = row['abstract']\n            if json_data[\"text\"] == \"Unknown\":\n                continue\n            json.dump(json_data, json_out_file)\n            json_out_file.write('\\n')\n    print(\"Done !!!\")\n            \ninputfile = \"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\"\noutputfile = \"covid-19.json\"\n\nextractData(inputfile, outputfile)","da4b9c7c":"import tensorflow_hub as hub\nimport numpy as np\nimport pickle\nimport pickledb\nimport random\n\nclass EmbeddingStore:\n    embed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\")\n\n    def __init__(self, filename):\n        self.embedding_file = filename\n        self.all_docs_embeddings = self.read_embeddings()\n        if self.all_docs_embeddings is None:\n            print(\"all_docs initialized to None\")\n\n    def save_embeddings(self):\n        f = open(self.embedding_file, 'wb')\n        pickle.dump(self.all_docs_embeddings, f)\n\n    def read_embeddings(self):\n        try:\n            f = open(self.embedding_file, 'rb')\n        except FileNotFoundError:\n            return None\n        try:\n            embeddings_ = pickle.load(f)\n        except EOFError:\n            embeddings_ = None\n        return embeddings_\n\n    def add_embeddings(self, new_embeddings_):\n        if self.all_docs_embeddings is None:\n            combined_embeddings_ = new_embeddings_\n        else:\n            combined_embeddings_ = np.concatenate((self.all_docs_embeddings, new_embeddings_))\n        self.all_docs_embeddings = combined_embeddings_\n\n    def dump(self):\n        self.save_embeddings()\n\n    def add_para(self, new_para_):\n        new_embeddings_ = self.embed([new_para_])\n        self.add_embeddings(new_embeddings_)\n\n    def find_top_k_match(self, query, k):\n        #print(\"Query received = {}\".format(query))\n        query_embedding_ = self.embed(query)\n        #print(query_embedding_.shape)\n        #print(query_embedding_[0].shape)\n        #print(self.all_docs_embeddings.shape)\n        corr = np.inner(query_embedding_[0], self.all_docs_embeddings)\n        #print(corr)\n        values = np.argpartition(corr, -k)[-k:]\n        #print(\"Top K matches for = {} at {}\".format(query, values))\n        return values\n\nclass ParaStore:\n\n    def __init__(self, docsfilename, embeddingsfile):\n        self.docfile = docsfilename\n        self.embeddingstore = EmbeddingStore(embeddingsfile)\n        self.all_docs = self.read_para()\n        self.nos_docs = self.all_docs.totalkeys()\n        print(\"There are {} docs in the store\".format(self.nos_docs))\n\n    def save_para(self):\n        self.all_docs.dump()\n\n    def read_para(self):\n        docs = pickledb.load(self.docfile, False)\n        return docs\n\n    # Changed this function to use the aid instead of text\n    def already_exists(self, new_para):\n        allkeys = self.all_docs.getall()\n        for key in allkeys:\n            if new_para[\"aid\"] == self.all_docs.get(key)[\"aid\"]:\n                print(\"{} already in the doc store\".format(new_para[\"aid\"]))\n                return True\n        return False\n\n    def add_para(self, new_para):\n        text = new_para[\"text\"]\n        if not self.already_exists(new_para):\n            self.all_docs.set(str(self.nos_docs), new_para)\n            self.embeddingstore.add_para(text)\n            self.nos_docs = self.nos_docs+1\n            if self.nos_docs % 1000 == 0:\n                self.dump()\n    def dump(self):\n        self.all_docs.dump()\n        self.embeddingstore.dump()\n\n    def get_para(self, nos):\n        return self.all_docs.get(str(nos))\n\n    def get_matching_para(self, query):\n        pos = self.embeddingstore.find_top_match(query)\n        match = self.get_para(pos)\n        return match\n\n    def get_matching_k_para(self, query, k):\n        positions = self.embeddingstore.find_top_k_match(query, k)\n        matches = {}\n        for i in positions:\n            match = self.get_para(i)\n            matches[str(i)] = match\n        return matches\n\n    def filter_k_randomly(self, matches, k):\n        kmatches = {}\n        #print(\"Filtering {} from {} matches\".format(k, len(matches)))\n        if len(matches) <= k:\n            return matches\n        else:\n           indexes = random.sample(matches.keys(), k)\n        for i in indexes:\n            kmatches[i] = matches[i]\n        return kmatches\n\n    def applyFilter(self, filt, text):\n        for name in filt:\n            if text and name.lower() in text.lower():\n                return True\n        return False\n        \n    def get_including_k_para(self, query, filt, k):\n        matches = {}\n        allkeys = self.all_docs.getall()\n        for key in allkeys:\n            candidate = self.all_docs.get(key)\n            if (query[0] in candidate[\"text\"]) and (self.applyFilter(filt, candidate[\"text\"])):\n                matches[key] = candidate\n        kmatches = self.filter_k_randomly(matches, k)\n        return kmatches\n\n","9b7d626a":"def openStore(docfile, embedding_file):\n    docstore = ParaStore(docfile, embedding_file)\n    return docstore\n\ndef importFromDrqa(filename, dbfile, embedfile):\n    docstore = openStore(dbfile, embedfile)\n    with open(filename) as f:\n        line = f.readline()\n        while line:\n            para = {}\n            #print(line)\n            data = json.loads(line)\n            #print(data[\"id\"])\n            para[\"aid\"] = data[\"articleID\"]\n            para[\"text\"] = data[\"text\"]\n            docstore.add_para(para)\n            line = f.readline()\n        docstore.dump()\n        \nfilename = \"covid-19.json\"\npickledbfile = \"cord19.db\"\nembedfile = \"cord19.pkl\"\n\nimportFromDrqa(filename, pickledbfile, embedfile)\nprint(\"All Document Imported\")","a899f790":"import csv\nimport json\n\ndef openStore(docfile, embedding_file):\n    docstore = ParaStore(docfile, embedding_file)\n    return docstore\n\ndef getTopKMatch(dbfile, embedfile, query, k=1):\n    # open the store\n    query_vector = [query]\n    print(\"The query vector is {}\".format(query_vector))\n    docstore = openStore(dbfile, embedfile)\n    matches = docstore.get_matching_k_para(query_vector, k)\n    return matches\n\ndef getTopKStringMatch(dbfile, embedfile, filt, query, k=1):\n    # open the store\n    query_vector = [query]\n    print(\"The query vector is {}\".format(query_vector))\n    docstore = openStore(dbfile, embedfile)\n    matches = docstore.get_including_k_para(query_vector, filt, k)\n    return matches\n\ndef createBigRow(med, indexes, aids, abstracts):\n    print(\"Creating a big row of {}, {}, {}\".format(med,indexes,aids))\n    big_row = list()\n    big_row.append(med)\n    for i in indexes:\n        big_row.append(i)\n    for i in aids:\n        big_row.append(i)\n    for abs in abstracts:\n        big_row.append(abs)\n    return big_row\n\ndef pad2K(data, k):\n    if len(data) >= k:\n        return data\n    else:\n        for i in range(len(data), k):\n            data.append(\"None\")\n    return data\n\ncovid19_names = {\n    'COVID19',\n    'COVID-19',\n    '2019-nCoV',\n    '2019-nCoV.',\n    'coronavirus disease 2019',\n    'Corona Virus Disease 2019',\n    '2019-novel Coronavirus',\n    'SARS-CoV-2',\n}\n\ndef evalQueryDetailCSV(filename, outfile, match):\n    with open(filename) as f, open(outfile, 'w') as output:\n        filewriter = csv.writer(output, delimiter=',')\n        line = f.readline()\n        while line:\n            line = line.replace(\"\\'\", \"\\\"\")\n            data = json.loads(line)\n            q_id = data['id']\n            q = data['text']\n            med = q\n            if match == 'USE':\n                topk = getTopKMatch(paraFile, embeddingFile, q, 5)\n            else:\n                topk = getTopKStringMatch(paraFile, embeddingFile, covid19_names, q, 5)\n            indexes = []\n            aids = []\n            abstracts = []\n            for key in topk.keys():\n                indexes.append(key)\n                para = topk.get(key)\n                aids.append(para[\"aid\"])\n                #print(para[\"aid\"])\n                abstracts.append(para[\"text\"].encode('utf-8'))\n                #print(para[\"text\"].encode('utf-8'))\n            indexes = pad2K(indexes, 5)\n            aids = pad2K(aids, 5)\n            abstracts = pad2K(abstracts, 5)\n            big_row = createBigRow(med, indexes, aids, abstracts)\n            filewriter.writerow(big_row)\n            line = f.readline()\n\n# if using the embeddings computed by the notebook uncomment these 4 lines\n#queryFile = \"\/kaggle\/input\/medicinenames\/drugs-only.json\"\n#paraFile = \"cord19.db\"  \n#embeddingFile = \"cord19.pkl\" \n#evalQueryDetailCSV(queryFile, \"top-5.csv\", 'string')\n\n# if using the precomputed embeddings uncomment these 4 lines (and comment out the above 4 lines)\nqueryFile = \"\/kaggle\/input\/medicinenames\/drugs-only.json\"\nparaFile = \"\/kaggle\/input\/embeddings\/cord19.db\"  # Use precomputed embeddings, this is to save time\nembeddingFile = \"\/kaggle\/input\/embeddings\/cord19.pkl\" # Use precomputed embeddings, this is to save time\nevalQueryDetailCSV(queryFile, \"top-5.csv\", 'string')\n\n","966d30f2":"import pandas as pd\nimport csv\n\ndef findDocIds(row):\n    data = row.tolist()\n    return data[6:11]\n\nmetadatafile = \"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\"\ninputfile = \"top-5.csv\"\noutputfile = \"top5-with-abstract.csv\"\n\noutput = open(outputfile, \"w\")\nfilewriter = csv.writer(output, delimiter=',')\nmetadata = pd.read_csv(metadatafile)\ninput_data = pd.read_csv(inputfile, header=None)\nfor index, row in input_data.iterrows():\n    docids = findDocIds(row)\n    print(docids)\n    titles = list()\n    for docid in docids:\n        if docid != 'None':\n            cand = metadata[metadata[\"cord_uid\"] == docid]\n            title = cand[\"title\"].values[0]\n        else:\n            title = 'None'\n        titles.append(title)\n    x = pd.Series(titles)\n    print(x)\n    newrow = row.append(x)\n    filewriter.writerow(newrow)","57c38b9c":"pip install MulticoreTSNE","d0ab2679":"import pickle\nimport pickledb\nimport argparse\nimport sys\nimport time\nimport numpy as np\nimport scipy as sp\nimport scipy.sparse\nimport matplotlib.pyplot as plt\nimport csv\nimport pandas\n \n \nfrom sklearn.neighbors import kneighbors_graph\nfrom scipy import sparse\nfrom scipy.sparse.linalg import eigs\nfrom MulticoreTSNE import MulticoreTSNE as TSNE\nfrom sklearn.manifold import SpectralEmbedding\n \n \n \ndef read_embeddings(file):\n        try:\n            f = open(file, 'rb')\n        except FileNotFoundError:\n            return None\n        try:\n            embeddings_ = pickle.load(f)\n        except EOFError:\n            embeddings_ = None\n        return embeddings_\n \ndef read_para(self):\n        docs = pickledb.load(self.docfile, False)\n        return docs\n \ndef get_para(self, nos):\n        return self.docs.get(str(nos))\n \ndef make_graph(x, n_neighbors, mode, sparse_mode, sigma, knnfile, nrmd):\n    '''\n    ***\n   \n    inputs:\n \n    x: the data (np.array)\n    args: args: the input arguments to the script\n    nrmd: if we want the graph matrix to be normalized\n    outputs:\n \n    a: the diffusion matrix (scipy.csr_matrix)\n \n    ***\n    '''\n    n = x.shape[0]\n    print(knnfile)\n    # construct kneighbors graph from data\n \n    if (id(knnfile) == id('')):\n        print('computing knn\\n')\n \n        start_time = time.time()\n        a = kneighbors_graph(x, n_neighbors, mode='distance')\n        print(\"--- %s seconds ---\" % (time.time() - start_time))\n        print(a.shape)\n        with open('knn'+str(n_neighbors), 'wb') as wfile:\n            pickle.dump(a,wfile)\n    else:\n        with open('knn'+str(n_neighbors), 'rb') as rfile:\n            a = pickle.load(rfile)                          \n    # simmetrize it\n \n#print(a[[1,2],:])\n    a = a + a.transpose()\n#a = np.expm1(-a)\n#a = a-a.sign()\n#if args.sparse_mode: a = a + sparse.eye(n)\n \n#print(a[[1,2],:])\n \n    if (nrmd):\n \n        # get the un-normalized weight matrix\n        norm = (a * sigma).max(axis=1).todense()\n \n        norm = 1. \/ norm\n        a = - a.multiply(norm)\n        a = a.expm1()\n        a = a - a.sign()\n        if sparse_mode: a = a + sparse.eye(n)\n \n        # get the normalized weight matrix\n        p = a.sum(axis=1)\n        p = 1. \/ p\n        a = a.multiply(p)\n        assert a.sum() == n # sanity check\n    return a\n \n \nclass diff_vecs: \n    def __init__(self, name, roll): \n        self.name = name \n        self.vec = vec\n   \n \n \ndef diffuse_labels(y=None, train_indices=None, g=None, t=1, class_=1):\n    '''\n    ***\n   \n    inputs:\n \n    y: the labels (np.array)\n    train_indices: the indices of the train dataset (list)\n    g: the graph (scipy.csr_matrix)\n    t: how many diffusion steps (int)\n \n    outputs:\n \n    signal: the soft labels from diffusion (np.array)\n \n    ***\n    '''\n    n = len(y)\n \n    # get training data labels and normalize in [-1,1]\n    y = y[train_indices]\n    y = 2 * (y == class_).astype(np.float) - 1\n    # get the signal to diffuse\n    signal = np.zeros((n))\n    signal[train_indices] = y\n \n    # diffuse t times\n    for _ in range(t):\n        signal = g.dot(signal)\n        signal[train_indices] = y\n    return signal\n \ndef plot_diff(title,fig_ax, x, signal,training_idx,text,color):\n    print('Plotting results')\n    fig_ax.scatter(x[:, 0], x[:, 1], s=2**2)\n    heatmap = signal[signal>0]\n    cmap = 'bwr'\n \n \n    for ix in training_idx:\n        print(ix)\n        fig_ax.text(x[ix,0],x[ix,1], str(ix))\n \n    fig_ax.set_title(title, fontsize='x-large')\n    fig_ax.scatter(x[signal>0,0], x[signal>0,1],color = 'red')\n \n    fig_ax.scatter(x[training_idx,0], x[training_idx,1], marker='s',  color = color)\n \n    return(fig_ax)\n \n# Function to lookup title for notebook\ndef findTitle(pos, paraFile=None, metadataFile=None):\n    if not paraFile:\n        paraFile = \"\/kaggle\/input\/embeddings\/cord19.db\"\n    if not metadataFile:\n        metadataFile = \"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\"\n       \n    docs = pickledb.load(paraFile, False)\n    para = docs.get(str(pos))\n    docid = para[\"aid\"]\n    metadata = pandas.read_csv(metadataFile)\n    cand = metadata[metadata[\"cord_uid\"] == docid]\n    title = cand[\"title\"].values[0]\n    return docid, title\n \n \ndef main(embeddingFile, seedFile, n_neighbors, mode, sparse_mode, sigma, knnfile, TSNE_done):\n    data = read_embeddings(embeddingFile)\n    #print(data[[0,1],:])\n    n = data.shape[0]\n    print(n)\n \n    #try TSNE\n \n    print('starting TSNE')\n    print(TSNE_done)\n    if (TSNE_done == 0):\n        print('Doing TSNE')\n        X_embedded = TSNE(n_jobs=4).fit_transform(data)\n        #with open('\/kaggle\/input\/tsnedata\/tsne'+str(n_neighbors), 'wb') as wfile:\n        with open('tsne'+str(n_neighbors), 'wb') as wfile:\n            pickle.dump(X_embedded,wfile)\n            plt.scatter(X_embedded[:, 0], X_embedded[:, 1])\n            plt.show()\n    else:\n        with open('tsne'+str(n_neighbors), 'rb') as rfile:\n            X_embedded = pickle.load(rfile)\n \n \n    #creating the graph and intiating diffusion:\n    print('Creating graph, and starting diffusion process\\n')\n \n    g=make_graph(X_embedded, n_neighbors, mode, sparse_mode, sigma, knnfile, nrmd = 1)\n    df = pandas.read_csv(seedFile,header=None)\n    meds = ['chloroquine','hydroxychloroquine','ritonavir','Nucleocapsid']#,'lopinavir\/ritonavir','lopinavir']\n    titles = ['drug: chloroquine','drug derivative: hydroxychloroquine','drug: ritonavir','ritonavir target: Nucleocapsid']#,'lopinavir\/ritonavir','lopinavir']\n    colors = ['green','yellow','pink','black','brown','grey']\n    count = 0\n \n    fig, ax_array = plt.subplots(1, len(meds), sharex=True, sharey=True)\n \n \n    print('diffusion loop')\n    a = []\n    for med in meds:\n        med_idx = df.index[df[0] == med]\n        #med_idx = np.where(df[0] == med)\n        print('med is:' + med)\n        print('med_idx:' + str(med_idx))\n        seed_idx = df.values[med_idx,[1,2,3,4,5]]\n        seed_idx = seed_idx.astype(int)\n        med_name = med\n        print('seed_idx:' + str(seed_idx))\n        y = np.ones(n, dtype=int)\n        y.astype(int)\n        signal = diffuse_labels(y=y, train_indices=seed_idx, g=g, t=titer, class_=1)\n        a.append(np.asarray(np.where(signal>0)))\n        # list = []\n        # list.append(diff_vec(med,np.where(signal>0)))\n        #print(np.where(signal>0))\n        color = colors[count]\n        ax_array[count] = plot_diff(titles[count],ax_array[count], X_embedded, signal,seed_idx,med_name,color)\n        count = count+1\n        # print out the interactions\n        \n    print(len(a))\n        \n    for i in range(len(meds)):\n        vec1 = a[i]\n        # print('vec1_len is:' + str(vec1.shape[1]))\n        # print('vec1:' + str(vec1))\n        for j in range(i+1,len(meds)):\n            vec2 = a[j]\n            #print('vec2:' + str(len(vec2)))\n            insct = np.intersect1d(vec1,vec2)\n            print('----------------------------------------------------------------------------------------------------------\\n')\n            print('intersection of concept ' + meds[i] + ' and concept ' + meds[j] + '\\n')\n            for pos in insct:\n                #id, tit = findTitle(pos, paraFile='cord19.db', metadataFile='metadata.csv')\n                id, tit = findTitle(pos)\n                print('id: '+ id + ' and title: ' + tit +  '\\n')\n \n    plt.show()\n    print('done')\n   \n# Data files\n# If using precomputed embeddings then uncomment these 2 lines\n#paraFile = \"\/kaggle\/input\/embeddings\/cord19.db\"  # Use precomputed embeddings, this is to save time\n#embeddingFile = \"\/kaggle\/input\/embeddings\/cord19.pkl\" # Use precomputed embeddings, this is to save time\n\n# If using embeddings computed by the notebook then uncomment these 2 lines (and comment out the above 2 lines)\nparaFile = \"cord19.db\"  # Use precomputed embeddings, this is to save time\nembeddingFile = \"cord19.pkl\" # Use precomputed embeddings, this is to save time\n\nseedFile = 'top5-with-abstract.csv'\n# Default parameters\nsigma = 1\nmode = 0\nsparse_mode = 0\nknnfile = ''\nn_neighbors = 14\nTSNE_done = 0\ntiter = 3\n \nmain(embeddingFile, seedFile, n_neighbors, mode, sparse_mode, sigma, knnfile, TSNE_done)\n \n#              parser.add_argument('--n-neighbors', default=10, type=int)\n#              parser.add_argument('--mode', default=0, type=int)\n#              parser.add_argument('--sparse-mode', default=0, type=int)\n#              parser.add_argument('--sigma', default=1., type=float)\n#              parser.add_argument('--knnfile', default='', type=str)\n#              parser.add_argument('--TSNE_done', default=0, type=int)","e31e61b7":"We are displaying titles in the contextual graph therefore we add the title to the result obtained above i.e. the seed file.","35ab0b40":"![nlp.jpg](attachment:nlp.jpg)\n\nThis is a joint project by Avinash Vyas and Dan Kushnir with contributions from Joerg Abendroth\n\n\u00a9 2020 Nokia\nLicensed under the BSD 3-Clause License\nSPDX-License-Identifier: BSD-3-Clause","a7c55641":"# GOAL\nCOVID-19 Open Research Dataset Challenge has 10 Tasks where each task has several sub tasks. For each subtask or question related to the subtask we aim to find the most relevant papers\/articles thus saving time and effort of researchers and doctors. For e.g. the tool can be used to find documents that are contextually associated with specific medical questions for any of the task such as, What is the target of a drug x?, Which drugs have similar effect?, Does cold weather cause faster spread?, Which protective equipment reduce covid transmission?, Does smoking increase fatality? Are pets susceptible to spill over? or Does increased testing enhance containment?\n\n\n\n# Approach\nWe will be using Universal Sentence Encoder (USE) to compute embeddings\/representation of all the abstracts and tasks (and subtasks) and then match a subtask (or a related question) to abstracts in this embeddings space using cosine similarity. Next, we build a graph using the USE-embeddings. The graph G(V,E,w), whose nodes correspond to the documents and its edges are constructed according to a K-nearest neighbor search in a further reduced space. Specifically we use TSNE to reduce the encoded dimension and then we assign the edges with weights according to the proximity in the reduced space.\n\nAt the basis of our approach is the assumption that the reduced space preserves the semantic relationship encoded by the USE. To this end, we can introduce a search mechanism that uses the graph to learn association between different concepts (e.g. therapeutics) to answer various question, as specific as \"what is the target of drug x?\" or \"which drugs are used in combination to treat covid-19 patients?\"\n\nThe search mechanism employs several stages:\n1. Concept Match:A match document (or batch of documents) for each prescribed concept of interest is found among the documents which cover covid-19 as well. The corresponding match is found by similarity in the encoding space.\n2. Diffusion: the corresponding matching nodes in the graph are labeled with a weight and the weights is diffused to the neighboring nodes, and further from those neighboring nodes to the neighbors and so on. The weighted are transduces using a Markov process that uses transition probabilities from the normalized edge weights.\n3. Concept intersection: We use the set of nodes that have been reached with weighted labels from the original concept matching node(s) to represent the that concept. We then find all intersection between such concept sets to infer target documents that represent the interaction between the concepts. \n4. Report and visualization: We report the pairwise concept intersection sets. Specifically, we report the article id (aid) and its title. We also plot two-dimensional visualization of the TSNE-embedding for each concept and the diffusion sets around each concept match. \n \n\nTo Demonstrate our approach we present below a visualized output of 4 concepts that were matched and diffused in the TSNE embedding of the USE representation. In this case we selected rather specific concepts including the two drugs Chloroquine and Hydroxycloroquine (its derivative) which known for Malaria treatment. The drug Ritonovire and the Viral target protein Nucelocapsid. In each sub figure the diffused sets are observed for concept. We used 5-top-rankging matches for each concept and diffused their labels for 4 edge-hops. \n\nThe following observations are noted:\n1. Chloroquine (CHL) and Hydroxycloroquine (HCHL) (a drug and its derivative) are found with a strong intersection for most matches. Typically, they are found in clusters representing both combined therapies and in proximity in regions of articles focusing on the specific drug therapy.\n2. We also find  CHL in a separate match representing combination with Chinese medicine.\n3. Ritonovir is found in the regions where combined therapies are used, intersecting with CHL and HCHL, and also in proximity and some intersection with its target the Nucleocpasid.\n4. we observe some anomalous match for the Nucleocpasid concept whose abstract contains a single line.\n\nWe conclude that the USE embedding preserves semantics and the diffusion process employed can be used to answer various questions related to various tasks in the challenge scope regarding interactions between concepts, and general search for relevant literature on the concept.\n\n![reults1_1.jpg](attachment:reults1_1.jpg)\n\n# Description\nWe have built a pipeline of the following processing modules \n1. **ETL** - extract a representation of each article in the CORD-19 data corpus,\n2. **Encoding** - compute the Universal Sentence Encoder (USE) encoding of the extracted representation of each article. (This is a computationaly expensive step which takes 2-3 hours. To save time we have precomputed embeddings for 37k abstracts and uploaded them as input data source. To use them instead of computing them, please skip the step marked as optional below and uncomment the filename pointing to the input data)\n3. **Representation** - use the deep-encoding to create a contextual graph\n4. **Seed Generation** - use the sub-task or a related question to find top-k matching articles from the CORD-19 data corpus\n5. **Diffusion** - using the top-k matches as seed, use the process of diffusion on the graph to find contextually similar document. Reports the sets of intersection, and a visualization.","9d597c69":"# Compute Embeddings (Optional)\nThe next step computes the USE embeddings for all the abstracts and store them. Its take considerable time (2-3 hours) to compute these embeddings therefore we have pre-computed the embeddings of 37K abstracts and uploaded them as a dataset. **This is a optional step****.","95b331d0":"# Seed Generation\nNext we find top-k match to a input query text, in the USE emebedding space (using cosine similarity). Here we use as input a list of concepts extracted from the CORD-19 dataset (including medications and proteins of the virus) and find the top-5 matching abstracts to these concepts. The output is saved in a CSV file that is used in the next step as a seed for diffusion process on the knowledge graph. \n\nIf the optional step of computing the embeddings have been executed there is no change to made and the next step can run as-is. If one wants to use the pre-computed embeddings then it needs to uncomment the ","1fac8eb3":"Next we compute the USE embeddings of all the abstracts and store them in a pickle file. We will also store the corresponding abstract in a pickleDB file. For this we are going to use two classes (shown below) that provides the abstractions to manage the embeddings (EmbeddingStore) and the abstracts (ParaStore). The two classes allow us to save, retrive and compute top-k matching abstract for any input sentence.","c8983fb7":"# Contextual Graph and Diffusion\nWe compute the contextual graph and use diffusion with the seed obtained from top-k match. Several parameters are set within the program which can be further tuned per user.\n\nsigma (= 1) - distance normalization for the weights\n\nmode (= 0): controls the use of sparse data structure\n\nsparse_mode (= 0): \n\nknnfile = (''): if set to 'knn' we look for the file 'knn n_neighbors' with prior-computed KNN data to save computing time (avoiding computing KNN is typically not needed unless the reduced data is a high dimension which may take more time to compute if program is invoked several times). If n_neighbors value is changing to new value the user has to use '' to compute the graph from scratch.\n\nn_neighbors (= 14): number of nearest neighbors for graph construction (KNN). Since currently graph is constructed at low dimension can be set between 10-30\n\nTSNE_done (= 0): if 1 TSNE is attmpted to be read from a precomputed file, assuming that it exists, to save on the TSNE computation. \n\ntiter = 3: number of diffusion iterations (hops). These critical parameters controls the rate of expansion of labels around the seeded matches. if too high may collect documents that might have little contextual link to the matches. Yet, has a higher chance of intersecting with other concepts.\n\nPlease note that in order to assign concepts to the program you need to do it in the code below in the array 'meds'. Right now it is configured as\n\n    meds = ['chloroquine','hydroxychloroquine','ritonavir','Nucleocapsid']\n\n"}}