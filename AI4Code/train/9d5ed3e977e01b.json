{"cell_type":{"23036406":"code","0d8913d7":"code","1de9e6c2":"code","81be6368":"code","19dbd573":"code","18d0557f":"code","c810b762":"code","c7531694":"code","bc964de8":"code","3043d104":"code","e11f19be":"markdown","8a30f750":"markdown","9498e8a3":"markdown","d249e517":"markdown","2952a958":"markdown"},"source":{"23036406":"ls ..\/input\/finetune-of-tensorflow-bilstm-eda-about\/","0d8913d7":"import numpy as np, os\nimport pandas as pd\n\nimport gc\n\nimport optuna\n\n# https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction\/discussion\/274717 \nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\nfrom IPython.display import display\n\nDEBUG = False\nTRAIN_MODEL = False\nINFER_TEST = False\nONE_FOLD_ONLY = True\nCOMPUTE_LSTM_IMPORTANCE = True\n\nif DEBUG:\n    train = train[:80*1000]","1de9e6c2":"%%time\ntrain = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\npressure_values = np.sort( train.pressure.unique() )\ntest = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\nsubmission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')","81be6368":"def add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n\n    #######################################\n    # fast area calculation\n    df['time_delta'] = df['time_step'].diff()\n    df['time_delta'].fillna(0, inplace=True)\n    df['time_delta'].mask(df['time_delta'] < 0, 0, inplace=True)\n    df['tmp'] = df['time_delta'] * df['u_in']\n    df['area_true'] = df.groupby('breath_id')['tmp'].cumsum()\n    \n    #u_in_max_dict = df.groupby('breath_id')['u_in'].max().to_dict()\n    #df['u_in_max'] = df['breath_id'].map(u_in_max_dict)\n    #u_in_min_dict = df.groupby('breath_id')['u_in'].min().to_dict()\n    #df['u_in_min'] = df['breath_id'].map(u_in_min_dict)\n    u_in_mean_dict = df.groupby('breath_id')['u_in'].mean().to_dict()\n    df['u_in_mean'] = df['breath_id'].map(u_in_mean_dict)\n    del u_in_mean_dict\n    u_in_std_dict = df.groupby('breath_id')['u_in'].std().to_dict()\n    df['u_in_std'] = df['breath_id'].map(u_in_std_dict)\n    del u_in_std_dict\n    \n    # u_in_half is time:0 - time point of u_out:1 rise (almost 1.0s)\n    df['tmp'] = df['u_out']*(-1)+1 # inversion of u_out\n    df['u_in_half'] = df['tmp'] * df['u_in']\n    \n    # u_in_half: max, min, mean, std\n    u_in_half_max_dict = df.groupby('breath_id')['u_in_half'].max().to_dict()\n    df['u_in_half_max'] = df['breath_id'].map(u_in_half_max_dict)\n    del u_in_half_max_dict\n    u_in_half_min_dict = df.groupby('breath_id')['u_in_half'].min().to_dict()\n    df['u_in_half_min'] = df['breath_id'].map(u_in_half_min_dict)\n    del u_in_half_min_dict\n    u_in_half_mean_dict = df.groupby('breath_id')['u_in_half'].mean().to_dict()\n    df['u_in_half_mean'] = df['breath_id'].map(u_in_half_mean_dict)\n    del u_in_half_mean_dict\n    u_in_half_std_dict = df.groupby('breath_id')['u_in_half'].std().to_dict()\n    df['u_in_half_std'] = df['breath_id'].map(u_in_half_std_dict)\n    del u_in_half_std_dict\n    \n    gc.collect()\n    \n    # All entries are first point of each breath_id\n    first_df = df.loc[0::80,:]\n    # All entries are first point of each breath_id\n    last_df = df.loc[79::80,:]\n    \n    # The Main mode DataFrame and flag\n    main_df= last_df[(last_df['u_in']>4.8)&(last_df['u_in']<5.1)]\n    main_mode_dict = dict(zip(main_df['breath_id'], [1]*len(main_df)))\n    df['main_mode'] = df['breath_id'].map(main_mode_dict)\n    df['main_mode'].fillna(0, inplace=True)\n    del main_df\n    del main_mode_dict\n\n    # u_in: first point, last point\n    u_in_first_dict = dict(zip(first_df['breath_id'], first_df['u_in']))\n    df['u_in_first'] = df['breath_id'].map(u_in_first_dict)\n    del u_in_first_dict\n    u_in_last_dict = dict(zip(first_df['breath_id'], last_df['u_in']))\n    df['u_in_last'] = df['breath_id'].map(u_in_last_dict)\n    del u_in_last_dict\n    # time(sec) of end point\n    time_end_dict = dict(zip(last_df['breath_id'], last_df['time_step']))     \n    df['time_end'] = df['breath_id'].map(time_end_dict)\n    del time_end_dict\n    del last_df\n    \n    # u_out1_timing flag and DataFrame: speed up\n    # \u9ad8\u901f\u7248 uout1_df \u4f5c\u6210\n    df['u_out_diff'] = df['u_out'].diff()\n    df['u_out_diff'].fillna(0, inplace=True)\n    df['u_out_diff'].replace(-1, 0, inplace=True)\n    uout1_df = df[df['u_out_diff']==1]\n    \n    gc.collect()\n    \n    #main_uout1 = uout1_df[uout1_df['main_mode']==1]\n    #nomain_uout1 = uout1_df[uout1_df['main_mode']==1]\n    \n    # Register Area when u_out becomes 1\n    uout1_area_dict = dict(zip(first_df['breath_id'], first_df['u_in']))\n    df['area_uout1'] = df['breath_id'].map(uout1_area_dict)\n    del uout1_area_dict\n    \n    # time(sec) when u_out becomes 1\n    uout1_dict = dict(zip(uout1_df['breath_id'], uout1_df['time_step']))\n    df['time_uout1'] = df['breath_id'].map(uout1_dict)\n    del uout1_dict\n    \n    # u_in when u_out becomes1\n    u_in_uout1_dict = dict(zip(uout1_df['breath_id'], uout1_df['u_in']))\n    df['u_in_uout1'] = df['breath_id'].map(u_in_uout1_dict)\n    del u_in_uout1_dict\n    \n    # Dict that puts 0 at the beginning of the 80row cycle\n    first_0_dict = dict(zip(first_df['id'], [0]*len(uout1_df)))\n\n    del first_df\n    del uout1_df   \n    \n    gc.collect()\n    \n    # Faster version u_in_diff creation, faster than groupby\n    df['u_in_diff'] = df['u_in'].diff()\n    df['tmp'] = df['id'].map(first_0_dict) # put 0, the 80row cycle\n    df.iloc[0::80, df.columns.get_loc('u_in_diff')] = df.iloc[0::80, df.columns.get_loc('tmp')]\n\n    # Create u_in vibration\n    df['diff_sign'] = np.sign(df['u_in_diff'])\n    df['sign_diff'] = df['diff_sign'].diff()\n    df['tmp'] = df['id'].map(first_0_dict) # put 0, the 80row cycle\n    df.iloc[0::80, df.columns.get_loc('sign_diff')] = df.iloc[0::80, df.columns.get_loc('tmp')]\n    del first_0_dict\n    \n    # Count the number of inversions, so take the absolute value and sum\n    df['sign_diff'] = abs(df['sign_diff']) \n    sign_diff_dict = df.groupby('breath_id')['sign_diff'].sum().to_dict()\n    df['diff_vib'] = df['breath_id'].map(sign_diff_dict)\n    \n    if 'diff_sign' in df.columns:\n        df.drop(['diff_sign', 'sign_diff'], axis=1, inplace=True)\n    if 'tmp' in df.columns:\n        df.drop(['tmp'], axis=1, inplace=True)\n    \n    gc.collect()\n    #######################################\n    '''\n    '''\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    #df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    #df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    #df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    #df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    #df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    #df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    #df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    #df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    df = df.fillna(0)\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    #df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    #df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    #df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    #df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    df['cross']= df['u_in']*df['u_out']\n    #df['cross2']= df['time_step']*df['u_out']\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    \n    df = pd.get_dummies(df)\n    return df","19dbd573":"%%time\ntrain = add_features(train)","18d0557f":"%%time\ntest = add_features(test)","c810b762":"print('Train dataframe shape',train.shape)\ntrain.head()","c7531694":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['pressure'], axis=1, inplace=True)\ntrain.drop(['id', 'breath_id'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id'], axis=1)\n\nCOLS = ['BASELINE'] + list(train.columns)\nprint('Number of feature columns =', len(COLS)-1 )\n\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)\n\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","bc964de8":"np.array(targets).shape","3043d104":"EPOCH = 300\nBATCH_SIZE = 1024\nNUM_FOLDS = 10\n\nTPU = False\n\nif TPU:\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n    ## instantiate a distribution strategy\n    xpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # GET GPU STRATEGY\n    xpu_strategy = tf.distribute.get_strategy()\n\nwith xpu_strategy.scope():\n    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n    test_preds = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        #print(fold)\n        if (fold != 0) and (ONE_FOLD_ONLY):\n            break\n        \n        K.clear_session()\n        \n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        \n        checkpoint_filepath = f\"folds{fold}.hdf5\"\n        if TRAIN_MODEL:\n            model = keras.models.Sequential([\n                keras.layers.Input(shape=train.shape[-2:]),\n                keras.layers.Bidirectional(keras.layers.LSTM(1024, return_sequences=True)),\n                keras.layers.Bidirectional(keras.layers.LSTM(512, return_sequences=True)),\n                keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True)),\n                keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True)),\n                keras.layers.Dense(128, activation='selu'),\n                keras.layers.Dense(1),\n            ])\n            model.compile(optimizer=\"adam\", loss=\"mae\")\n\n            lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n            es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n            sv = keras.callbacks.ModelCheckpoint(\n                checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n                save_weights_only=False, mode='auto', save_freq='epoch',\n                options=None\n            )\n            model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr, es, sv])\n            \n        else:\n            #model = keras.models.load_model('..\/input\/finetune-of-tensorflow-bidirectional-lstm\/'+checkpoint_filepath)\n            model = keras.models.load_model('..\/input\/finetune-of-tensorflow-bilstm-eda-about\/'+checkpoint_filepath)\n\n        if INFER_TEST:\n            print(' Predicting test data...')\n            test_preds.append(model.predict(test,verbose=0).squeeze().reshape(-1, 1).squeeze())\n                    \n        if COMPUTE_LSTM_IMPORTANCE:\n            results = []\n            print(' Computing LSTM feature importance...')\n\n            for k in tqdm(range(len(COLS))):\n                if k>0: \n                    save_col = X_valid[:,:,k-1].copy()\n                    np.random.shuffle(X_valid[:,:,k-1])\n                        \n                oof_preds = model.predict(X_valid, verbose=0).squeeze() \n                mae = np.mean(np.abs( oof_preds-y_valid ))\n                results.append({'feature':COLS[k],'mae':mae})\n        \n                if k>0: \n                    X_valid[:,:,k-1] = save_col\n         \n            # DISPLAY LSTM FEATURE IMPORTANCE\n            print()\n            df = pd.DataFrame(results)\n            df = df.sort_values('mae')\n            plt.figure(figsize=(10,20))\n            plt.barh(np.arange(len(COLS)),df.mae)\n            plt.yticks(np.arange(len(COLS)),df.feature.values)\n            plt.title('LSTM Feature Importance',size=16)\n            plt.ylim((-1,len(COLS)))\n            plt.show()\n                               \n            # SAVE LSTM FEATURE IMPORTANCE\n            df = df.sort_values('mae',ascending=False)\n            df.to_csv(f'lstm_feature_importance_fold_{fold}.csv',index=False)\n                               \n        # ONLY DO ONE FOLD\n        #if ONE_FOLD_ONLY:\n        #    #break\n        #    exit()","e11f19be":"\n# LSTM Feature Importance (aka permutation importance)\nWhen using an XGB model (gradient boosted trees), we have the advantage of displaying XGB Feature Importance. With Neural Networks, we don't have that advantage. However, we can use a technique called [Permutation Feature Importance][1] to compute the Feature Importance for any model. Therefore we can compute LSTM Feature Importance using permuation importance. Permutation importance has the advantage that we only need to train 1 model versus training multiple models for each feature we wish to evaluate!\n\nOther types of feature importances that can be applied to any model are [SHAP Feature Importance][3] and [LOFO Feature Importance][4].\n\nWe will compute LSTM feature importance for Zhangxin's notebook [here][2]. (Thus our notebook here has been forked from his notebook and we will use his saved model weights).\n\n[1]: https:\/\/christophm.github.io\/interpretable-ml-book\/feature-importance.html#feature-importance\n[2]: https:\/\/www.kaggle.com\/tenffe\/finetune-of-tensorflow-bidirectional-lstm\n[3]: https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html\n[4]: https:\/\/www.kaggle.com\/aerdem4\/google-ventilator-lofo-feature-importance","8a30f750":"# This notebook shows the importance of the features introduced in the \"EDA about\" series below.\n- [EDA about time_step and u_out](https:\/\/www.kaggle.com\/marutama\/eda-about-time-step-and-u-out)\n- [EDA about u_in](https:\/\/www.kaggle.com\/marutama\/eda-about-u-in)\n- [EDA about Pressure Part 1](https:\/\/www.kaggle.com\/marutama\/eda-about-pressure-part-1)\n- [EDA about Pressure Part 2](https:\/\/www.kaggle.com\/marutama\/eda-about-pressure-part-2)\n\nThe characteristic features are as follows.\n- u_in_last : Separate main mode from others\n- u_in_mean, u_in_half_mean : Classify pressure layers. It seems that u_in_mean is enough instead of u_in_half_mean.\n- area_true : Area_true is higher than the wrong area\n- u_in_std, u_in_half_std : Either one is fine\n- time_end : Other than the main mode, the mode differs depending on whether it is 2.65s or more or less.\n- time_delta : The time interval. The part where the time interval is long seems to be the part where data is missing.\n- u_in_uout1 : This is u_in when u_out rises to 1 with this breath_id.\n- diff_vib : Indicates the vibration of u_in. The number of times the u_in diff coding is inverted.\n\nIf you find it useful, please upvote this notebook and the \"EDAabout\" series.\n\nMany thanks to Zhangxin-san and Chris Deotte-san for sharing the following two notebooks.\n- [finetune of Tensorflow Bidirectional LSTM](https:\/\/www.kaggle.com\/tenffe\/finetune-of-tensorflow-bidirectional-lstm)\n- [LSTM Feature Importance](https:\/\/www.kaggle.com\/cdeotte\/lstm-feature-importance)\n","9498e8a3":"# Engineer Features","d249e517":"# Load Libraries and Data ","2952a958":"# Compute LSTM Feature Importance\nAfter we train (or load) each fold model, we will compute LSTM feature importance for all of our features. We do this with a for-loop of size `N` where `N` is the number of features we have. For each feature we wish to evaluate, we infer our OOF with that feature column randomly shuffled. If this feature column is important to our LSTM model, then the OOF MAE will become worse for that for-loop step. After our for-loop, we display bars equal to the size of how much MAE worsened without each feature, which is the importance of each feature.\n\nNote that computing LSTM feature importance after each fold will add about 1 minute for every 5 features."}}