{"cell_type":{"aaa82772":"code","f0554444":"code","b8de4a2d":"code","db4ff1fa":"code","b2206c63":"code","8837713d":"code","5655981a":"code","32b171ec":"code","db72804d":"code","cb8f6f0a":"code","14d05b11":"code","8baef68d":"code","3f7d078b":"code","caa1f074":"code","46f81e38":"code","044335c9":"code","b2242240":"code","1c9b3e05":"code","8adc6969":"code","f111eb50":"code","55ca1397":"code","7395e1ec":"code","f9d3acd6":"code","986cfcea":"code","5158c0ec":"code","122434a7":"code","a6850a76":"code","52e937bb":"markdown","ee024897":"markdown","1354238c":"markdown","0e799c10":"markdown","a28d5dee":"markdown","964a31eb":"markdown","a8590c0c":"markdown","e42fc2e6":"markdown","3edcb4ee":"markdown","01322d35":"markdown","138c79f8":"markdown","4472a465":"markdown","85db7b21":"markdown","1444823e":"markdown","e24ca36f":"markdown","82d18a34":"markdown","7d292cf8":"markdown"},"source":{"aaa82772":"# Setting up\n%matplotlib inline\n \nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n \nprint('Imports successful')","f0554444":"file_path = '..\/input\/spotify-dataset-19212020-160k-tracks\/data.csv'\ndf = pd.read_csv(file_path)","b8de4a2d":"df.shape","db4ff1fa":"df.head()","b2206c63":"df.describe()","8837713d":"df.info()","5655981a":"oldest_year = df['release_date'].min()\nprint(f'The oldest year in this dataset is {oldest_year}.')","32b171ec":"most_popular = df[['artists','year','name', 'release_date', 'popularity']]\nmost_popular = most_popular.sort_values(by=['popularity', 'name'], ascending=[False, True])\nmost_popular.artists = most_popular.artists.str.strip('[]').str.replace(\"'\", \"\")\nmost_popular.head(10)","db72804d":"# Remove non-numeric columns, that I may begin numerical analysis:\ndf.drop(['artists', 'id', 'name', 'release_date'], axis=1, inplace=True)\nprint(df.shape)\ndf.head(2)","cb8f6f0a":"sns.heatmap(df.corr(), cmap='icefire');","14d05b11":"df.corr()['popularity'].sort_values(ascending=False)","8baef68d":"# Set the target variable\ny = df.popularity\n \nfig,axs = plt.subplots(2,1, figsize=(7,7))\nfig.suptitle('Observations of popularity')\n \n# Observe the distribution of 'popularity'axs[0].set_title('Distribution of popularity')\naxs[0].set_title('Distribution of popularity')\nsns.distplot(df['popularity'], ax=axs[0], kde=False)\n \naxs[1].set_title('Relationship between popularity and year')\nsns.lineplot(x='year', y='popularity', data=df, ax=axs[1])\n \nfig.tight_layout(pad=3.0)","3f7d078b":"# Set the predictor variables\nfeatures = ['valence', 'acousticness', 'danceability',\n       'duration_ms', 'energy', 'explicit', 'year', 'instrumentalness', 'key',\n       'liveness', 'loudness', 'mode', 'speechiness', 'tempo']\n\nX = df[features]\nX.head()","caa1f074":"#create a 4 by 4 grid of subplots\nfig, axs = plt.subplots(4, 4, sharey=True, figsize=(15,15))\nfig.suptitle('Correlation of several features with popularity', y=1.01)\n \n#I only need 14 subplots (there are 14 features), might as well remove the last three subplots\nfig.delaxes(axs[3][2])\nfig.delaxes(axs[3][3])\n \n#I need to be able to loop through each subplot, so I make a 1-d array I can loop through easily. \n#I also exclude the deleted axes\nloopable_axs = axs.reshape(-1)[:-2]\n \nfor n in range(len(loopable_axs)):\n  loopable_axs[n].set_title(f'{features[n]} against popularity')\n  sns.regplot(ax=loopable_axs[n], x=features[n], y='popularity', data=df, line_kws={'color':'orange'})\n \nfig.tight_layout(pad=2.0)","46f81e38":"train_X, test_X, train_y, test_y = train_test_split(X,y, test_size= 0.1, random_state=0)","044335c9":"# Uncomment this line to update the yellowbrick module\n# !pip install yellowbrick --upgrade","b2242240":"# Import the Yellowbrick Feature Importance visualizer\nfrom yellowbrick.model_selection import FeatureImportances\nmodel = RandomForestRegressor()\nviz = FeatureImportances(model)\nviz.fit(train_X, train_y)\nviz.show()","1c9b3e05":"dec_tree = DecisionTreeRegressor(random_state=0)\ndec_tree.fit(train_X, train_y)","8adc6969":"val_preds1 = dec_tree.predict(test_X)\nval_mae1 = mean_absolute_error(test_y, val_preds1)\nprint(f'Mean absolute error of this model: {val_mae1:.3f}')\n \n# print(f'Training Coefficient of R^2 : {dec_tree.score(train_X, train_y):.2f}')\n# print(f'Test Coefficient of R^2 : {dec_tree.score(test_X, test_y):.2f}')","f111eb50":"# Import the GridSearchCV module\nfrom sklearn.model_selection import GridSearchCV\nprint('Imports successful')","55ca1397":"n_features = X.shape[1]\nn_samples = X.shape[0]\n \ngrid = GridSearchCV(DecisionTreeRegressor(random_state=0), cv=3, n_jobs=-1, verbose=5,\n                    param_grid ={\n                    'max_depth': [None,5,6,7,8,9,10,11],\n                    'max_features': [None, 'sqrt', 'auto', 'log2', 0.3,0.5,0.7, n_features\/\/2, n_features\/\/3, ],\n                    'min_samples_split': [2,0.3,0.5, n_samples\/\/2, n_samples\/\/3, n_samples\/\/5],\n                    'min_samples_leaf':[1, 0.3,0.5, n_samples\/\/2, n_samples\/\/3, n_samples\/\/5]},\n                    )\n \ngrid.fit(train_X, train_y)\nprint('Train R^2 Score : %.3f'%grid.best_estimator_.score(train_X, train_y))\nprint('Test R^2 Score : %.3f'%grid.best_estimator_.score(test_X, test_y))\nprint('Best R^2 Score Through Grid Search : %.3f'%grid.best_score_)\nprint('Best Parameters : ',grid.best_params_)","7395e1ec":"# Decision tree with tuned hyperparameters using GridSearchCV\ndec_tree2 = DecisionTreeRegressor(max_depth=8, max_features=None, \n                                  min_samples_leaf=1, min_samples_split=2, random_state=0)\ndec_tree2.fit(train_X, train_y)\nval_preds2 = dec_tree2.predict(test_X)\nval_mae2 = mean_absolute_error(test_y, val_preds2)\nprint(f'Mean absolute error of this model: {val_mae2:.3f}')","f9d3acd6":"# This function returns the mean absolute error (MAE) of a decision tree\n# given the max_leaf_nodes argument, the train dataset and the test dataset\n \ndef return_mae(mln, train_X, val_X, train_y, val_y):\n  music_model = DecisionTreeRegressor(max_leaf_nodes=mln, random_state=0)\n  music_model.fit(train_X, train_y)\n  pred_y = music_model.predict(val_X)\n  mae = mean_absolute_error(val_y, pred_y)\n  return mae","986cfcea":"# This cell runs the return_mae function on a range of max_leaf_node values and returns the best MLN\n \nmln_values = range(5,1000,50) #max leaf nodes for consideration\n \nmln_dict = {x:return_mae(x, train_X, test_X, train_y, test_y) for x in mln_values}\nbest_mln = min(mln_dict, key=mln_dict.get)\n \nprint('max_leaf_node values and their Mean Absolute Error values:')\nfor mln in mln_dict:\n  print(f'{mln:-<6}{mln_dict[mln]:.4f}')\nprint(f'\\nBest value for max_leaf_nodes : {best_mln}')","5158c0ec":"# Decision tree with tuned max_leaf_node using get_mae()\ndec_tree3 = DecisionTreeRegressor(max_leaf_nodes=best_mln, random_state=0)\ndec_tree3.fit(train_X, train_y)\nval_preds3 = dec_tree3.predict(test_X)\nval_mae3 = mean_absolute_error(test_y, val_preds3)\nprint(f'Mean absolute error of this model: {val_mae3:.3f}')","122434a7":"rf_model = RandomForestRegressor(random_state=0)\nrf_model.fit(train_X, train_y)","a6850a76":"val_preds4 = rf_model.predict(test_X)\nval_mae4 = mean_absolute_error(test_y, val_preds4)\nprint(f'Mean absolute error of this model: {val_mae4:.3f}')","52e937bb":"\n-----------------\n## Questions I want answered from the data","ee024897":"#### Using Grid Search to tune the hyperparameters\nIn the next three cells I will be using GridSearchCV to find the optimal hyperparameters for the decision tree to predict song popularity.","1354238c":"##### The tuned decision tree model: max_leaf_nodes","0e799c10":"\n------\n## Predicting Popularity","a28d5dee":"### Approach 2: Using Random Forest","964a31eb":"##### The tuned decision tree model: GridSearchCV\n\n","a8590c0c":"### What are the most popular tracks in the entire dataset when it was sourced (and when were they released)?","e42fc2e6":"### What features are the most important to our model?","3edcb4ee":" The plot above shows that the features `mode`and `explicit` are not really important to the model. However, because they contribute (a little) to the reduction of the Mean Absolute Error of my models, I will not drop them.","01322d35":"### What is the correlation between the other features and the target variable (popularity)?","138c79f8":"# Spotify Popularity\n## Introduction\nI set out to practise concepts I learned in statistical learning, particularly regression(with decision trees and random forest) and exploratory data analysis. The goal is to **develop a model that predicts the popularity (measured on a scale of 1 to 100) of a track on spotify**.  Along the way I'll ask some questions:\n \n1. How far back do the tracks in the dataset go?\n2. What are the most popular songs in the entire dataset (and when were they released)?\n3. What is the relationship between the other features and the target variable (popularity)?\n \n","4472a465":"#### Tweaking just the max_leaf_nodes argument\nI get better results from just tweaking the max_leaf_nodes argument than from running GridSearchCV (my guess is I didn't have the right parameters for it).","85db7b21":"## Observations and Summary\nIn the course of this project, I have completed the objectives I intended to check off:\n- The oldest year in consideration in the dataset is **1921**.\n- The most popular song in the dataset is **Dakiti** by **Bad Bunny** & **Jhay Cortez**. The next two popular songs are **Mood** by **24kGoldn** & **Iann Dior** and **Dynamite** by **BTS**.\n- The five best predictors of popularity in order of importance are\n  - year;\n  - song duration (the scale of the regression plot of `popularity` against `duration_ms` may be confusing, but it shows that most songs are shorter than 16 minutes and that the longer a song is, the more popular it is. (Untrained opinion: this is given the song is less than 16 minutes.);\n  - loudness; \n  - danceability;\n  - speechiness; and\n  - acousticness.\n \nFurthermore, I attempted different   models in order to get one with a favorable Mean Absolute Error.  \n \nHere are the models with their Mean Absolute Error (MAE):  \n \nModel | MAE\n--- | ---\nSimple decision tree | 9.196\nDecision tree (GridSearchCV) | 6.915  \nDecision tree (max_leaf_nodes) | 6.829  \nRandom Forest | 6.750  \n \n*Note: These insights are based on the version of the dataset I am currently working with. If I use an updated  version of the dataset in the future,  I should update these insights too.*","1444823e":" The next two code cells are simply to perform one task: visualize feature importances when predicting song popularity. To do this, I use the `yellowbrick`  library. The first cell simply updates the library so I can use the `FeatureImportances` module.","e24ca36f":"Note: The Spotify API documentation states  that [popularity is calculated with respect to the current year](https:\/\/developer.spotify.com\/documentation\/web-api\/reference\/#objects-index). Hence, the rise in popularity as the years go by does not mean tracks were not popular in their time; they just are not as  popular **now**.  \n> The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are.\nGenerally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past.","82d18a34":"### Approach 1: Using Decision Trees","7d292cf8":"### How far back do the tracks go?"}}