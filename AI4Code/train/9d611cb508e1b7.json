{"cell_type":{"77077f01":"code","8a6a48e4":"code","50b88ce8":"code","ca383e96":"code","75a1df7e":"code","675e6bff":"code","0b099fbe":"code","ab8c160d":"code","212c6feb":"code","b651964f":"code","fe3e41cc":"code","93b429b9":"code","4638cf4b":"code","ca0b1d99":"code","569cce25":"code","90ffb1f4":"code","52302e6a":"code","a8b7b254":"code","42a8ab56":"code","7b914c23":"code","7b40b58a":"code","3fd9ae73":"code","34805ad1":"code","14fe95ba":"code","036ada3a":"code","97a96663":"code","cb065f50":"code","969a7f6d":"code","c90e405d":"code","c5205be0":"code","dd556d51":"code","80f47005":"code","6787953f":"code","14f66b38":"code","56a0d8b0":"code","a68f0e9f":"code","90c04dc3":"code","8173e222":"code","5ffe120d":"code","3e0e014a":"code","dfeb0631":"markdown","982e0ea6":"markdown","9a05ae9a":"markdown","894ea313":"markdown","fd85617f":"markdown","e326b331":"markdown","68cab948":"markdown","7e330fa5":"markdown","8bd88027":"markdown","c5be11ed":"markdown","48e9e282":"markdown","39b6631e":"markdown","59b5eeb6":"markdown","69f31299":"markdown","af06480d":"markdown","af110ca9":"markdown","b3f74230":"markdown","184829f6":"markdown","f96af0c1":"markdown","3bf17a5f":"markdown","2ce40f65":"markdown","02754850":"markdown","c8c25678":"markdown","eb1ed3e6":"markdown","154a0116":"markdown","1c047f7b":"markdown","baf671c8":"markdown","30104429":"markdown","d7187f19":"markdown","2a7b89a6":"markdown","d703130f":"markdown"},"source":{"77077f01":"#IMPORTING NECESSARY LIBRARIES\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\ntorch.manual_seed(1)\nimport matplotlib.pyplot as plt\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","8a6a48e4":"train = pd.read_csv(\"\/kaggle\/input\/mnist-data\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/mnist-data\/test.csv\")\ntrain_array = train.to_numpy() #pandas to numpy\ntest_array = test.to_numpy() #pandas to numpy \ntrain_images = train_array[:,1:].reshape(42000,1,28,28) #images in numpy array\ntrain_output = train_array[:,0] #label\nprint(train_images.shape)\nprint(train_output.shape)\n","50b88ce8":"'''\nHere random vector is of shape (m,1,5,5) and aim to generate an image of shape (m,1,28,28).\n'''\nclass Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ConvT1 = nn.ConvTranspose2d(in_channels = 1,out_channels = 3,stride = 1, padding = 0,kernel_size = (3,3)) # input : (m,1,5,5) output : (m,3,7,7)\n        self.ConvT2 = nn.ConvTranspose2d(in_channels = 3,out_channels = 5,stride = 1, padding = 0,kernel_size = (5,5)) # input : (m,3,7,7) output : (m,5,12,12)\n        self.ConvT3 = nn.ConvTranspose2d(in_channels = 5,out_channels = 2,stride = 2, padding = 1,kernel_size = (2,2)) # input : (m,5,12,12) output : (m,2,24,24)\n        self.ConvT4 = nn.ConvTranspose2d(in_channels = 2,out_channels = 1,stride = 2, padding = 6,kernel_size = (2,2)) # input : (m,2,24,24) output : (m,1,28,28)\n    \n    def forward(self,x):\n        x = nn.functional.relu(self.ConvT1(x))\n        x = nn.functional.relu(self.ConvT2(x))\n        x = nn.functional.relu(self.ConvT3(x))\n        x = nn.functional.sigmoid(self.ConvT4(x))\n        return x\ngen = Generator()\n","ca383e96":"# input : (m,1,28,28) output : (m,1)\nclass Discriminator(nn.Module):\n    def __init__(self,channel = 1):\n        super().__init__()\n        self.Conv1 = nn.Conv2d(in_channels = channel,out_channels = 2,stride = 2, padding = 7,kernel_size = (4,4)) \n        self.Conv2 = nn.Conv2d(in_channels = 2,out_channels = 5,stride = 2, padding = 2,kernel_size = (4,4))\n        self.Conv3 = nn.Conv2d(in_channels = 5,out_channels = 10,stride = 1, padding = 0,kernel_size = (5,5))\n        self.fc1 = nn.Linear(in_features = 490, out_features = 256)\n        self.fc2 = nn.Linear(in_features = 256, out_features = 64)\n        self.fc3 = nn.Linear(in_features = 64, out_features = 16)\n        self.fc4 = nn.Linear(in_features = 16, out_features = 1)\n        \n    def forward(self,x):\n        m = nn.LeakyReLU(0.01)\n        x = m(self.Conv1(x))\n        x = m(self.Conv2(x))\n        x = m(self.Conv3(x))\n        x = torch.flatten(x,start_dim = 1)\n        x = m(self.fc1(x))\n        x = m(self.fc2(x))\n        x = m(self.fc3(x))\n        x = self.fc4(x)\n        return x\n        \ndisc = Discriminator()","75a1df7e":"from torch.optim import Adam\n#empty cuda components\ntorch.cuda.empty_cache()\n\n#transfer model to GPU\ngen.to(device)\ndisc.to(device)\n\n#initialize optimizers\nopt1 = Adam(gen.parameters(),lr = 0.001)\nopt2 = Adam(disc.parameters(),lr = 0.001)\n\n#number of epochs\nEPOCHS = 50\n\n#loss function\nloss = nn.BCEWithLogitsLoss()\n\n#the random array which will be passed ot generator\nrandom_array = torch.rand((42000,1,5,5))\n\n#passing data into GPU\nrandom_array = random_array.to(device)\ntrain_images = torch.tensor(train_images).to(device)\n\n#list to store losses after each iteration\ngen_loss_list = []\ndisc_loss_list = []\n","675e6bff":"for epoch in range(EPOCHS):\n    \n    #We will be running in batches of 1000\n    for i in range(0,42000,1000):\n        \n        #zeroing gradients\n        gen.zero_grad()\n        disc.zero_grad()\n\n        #generating fakes\n        gen_out = gen(random_array[i:i+1000,:,:,:])\n\n        #classifying fakes and reals\n        disc_out_fakes = disc(gen_out.detach())\n        disc_out_reals = disc(torch.tensor(train_images[i:i+1000,:,:,:]).float())\n\n        #calculating loss form discriminator\n        fakes_loss = loss(disc_out_fakes, torch.zeros_like(disc_out_fakes))\n        reals_loss = loss(disc_out_reals, torch.ones_like(disc_out_reals))\n        net_loss = (fakes_loss + reals_loss)\/2\n\n        #optimizing discriminator\n        net_loss.backward(retain_graph=True)\n        opt2.step()\n\n        #calculating loss form generator\n        gen_loss = loss(disc_out_fakes, torch.ones_like(disc_out_fakes))\n\n        #optimizing generator\n        gen_loss.backward(retain_graph=True)\n        opt1.step()\n    gen_loss_list.append(gen_loss)\n    disc_loss_list.append(net_loss)\n        ","0b099fbe":"X = torch.rand((1,1,5,5))\n# print(X)\nplt.imshow(gen(X.to(device)).cpu().detach().reshape(28,28))\n# print(gen(X.to(device)).cpu().detach().reshape(28,28))\nplt.show()","ab8c160d":"def gen_loss(pred):\n    return -1*torch.mean(pred)","212c6feb":"# Gradient Penalty is like regularization, we add it to the loss function to make it more stable\ndef Gradient_penalty(fake_images,real_images,disc,alpha):\n    mixed_images = alpha*fake_images + (1 - alpha)*real_images\n    mixed_scores = disc(mixed_images)\n    gradients = torch.autograd.grad(inputs = mixed_images, outputs = mixed_scores,\n        grad_outputs = torch.ones_like(mixed_scores), \n        create_graph = True,\n        retain_graph = True)[0]\n    gradients = gradients.view(len(gradients), -1)\n#     print(gradients.shape)\n    gradients_norm = gradients.norm(2,dim = 1)\n#     print(gradients_norm.shape)\n    penalty = ((gradients_norm - 1)**2).mean()\n    return penalty\nGradient_penalty(torch.rand(2,1,28,28,requires_grad = True).to(device),torch.rand(2,1,28,28,requires_grad = True).to(device),disc,0.1)","b651964f":"def critic_loss(fake_preds,real_preds,gradient_penalty,alpha):\n    return torch.mean(fake_preds - real_preds) + alpha*gradient_penalty\n    ","fe3e41cc":"#transfer model to GPU\ngen_wei = Generator()\ndisc_wei = Discriminator()\ngen_wei.to(device)\ndisc_wei.to(device)\n\nEPOCHS = 50\n\nopt_wei1 = Adam(gen_wei.parameters(),lr = 0.001)\nopt_wei2 = Adam(disc_wei.parameters(),lr = 0.001)\n\n#the random array which will be passed ot generator\nrandom_array1 = torch.rand((42000,1,5,5))\n\n#passing data into GPU\nrandom_array1 = random_array1.to(device)\n\n#list to store losses after each iteration\ngen_loss_list1 = []\ndisc_loss_list1 = []\n\n#parameters for gradient penalty and crit loss\nweight = 0.5*torch.ones(1000, 1, 1, 1, device=device, requires_grad=True)\nalpha = 0.001\n","93b429b9":"for epoch in range(EPOCHS):\n     for i in range(0,42000,1000):\n        \n        print(i)\n        #zeroing gradients\n        gen_wei.zero_grad()\n        disc_wei.zero_grad()\n\n        #generating fakes\n        gen_wei_out = gen_wei(random_array1[i:i+1000,:,:,:])\n\n        #classifying fakes and reals\n        disc_wei_out_fakes = disc_wei(gen_wei_out.detach())\n        disc_wei_out_reals = disc_wei(torch.tensor(train_images[i:i+1000,:,:,:]).float())\n\n        #calculating loss form discriminator\n        grad_pnlty = Gradient_penalty(gen_wei_out,torch.tensor(train_images[i:i+1000,:,:,:]).float(),disc_wei,weight)\n        net_wei_loss = critic_loss(disc_wei_out_fakes,disc_wei_out_reals,grad_pnlty ,alpha)\n       \n        #optimizing discriminator\n        net_wei_loss.backward(retain_graph=True)\n        opt_wei2.step()\n\n        #calculating loss form generator\n        gen_wei_loss = gen_loss(disc_wei_out_fakes)\n      \n        #optimizing generator\n        gen_wei_loss.backward(retain_graph=True)\n        opt_wei1.step()\n    gen_loss_list1.append(gen_wei_loss)\n    disc_loss_list1darkl.append(net_wei_loss)","4638cf4b":"# Let Real_images_FID be the matrix of real images of size (228,28) i.e 228 examples each with 28 features.\n# Let Fake_images_FID be the matrix of fake images of size (228,28) i.e 228 examples each with 28 features.\n\nReal_images_FID = torch.rand((228,28))\nFake_images_FID = torch.rand((228,28))","ca0b1d99":"from scipy.linalg import sqrtm\n# This function calculates the FID between 2 sets of images\ndef FID(real_imgs, fake_imgs):\n    cov_real = torch.Tensor(np.cov(real_imgs.detach().numpy(), rowvar=False))\n    cov_fake = torch.Tensor(np.cov(fake_imgs.detach().numpy(), rowvar=False))\n    mean_real = real_imgs.mean(0)\n    mean_fake = fake_imgs.mean(0)\n    sqrt_mat = sqrtm(cov_real@cov_fake)\n    \n    fid = (mean_real - mean_fake).dot(mean_real - mean_fake) + torch.trace(cov_real) + torch.trace(cov_fake) - 2*torch.trace(torch.tensor(sqrtm(cov_real@cov_fake)))\n    return fid","569cce25":"FID(Real_images_FID,Fake_images_FID)","90ffb1f4":"torch.manual_seed(1)\n\nEPOCHS = 50\n\n#Making dataset\none_hot_output = pd.get_dummies(train_output)\none_hot_output = torch.tensor(np.array(one_hot_output),dtype = torch.float32)\nchannel = one_hot_output.shape[1]\ninput_withlabel = torch.rand(42000,15)\ninput_withlabel = torch.cat((input_withlabel, one_hot_output),1)\n\n#training with labeled inputs\n#transfer model to GPU\ngen_labels = Generator()\ndisc_labels = Discriminator(channel)\ngen_labels.to(device)\ndisc_labels.to(device)\n\n#optimizers\n\nopt_labels1 = Adam(gen_labels.parameters(),lr = 0.001)\nopt_labels2 = Adam(disc_labels.parameters(),lr = 0.001)\n\n\n#passing data into GPU\ninput_withlabel = input_withlabel.to(device)\n\n#list to store losses after each iteration\ngen_loss_list2 = []\ndisc_loss_list2 = []\n\n","52302e6a":"for epoch in range(EPOCHS):\n    print(epoch)\n    for i in range(0,42000,1000):\n        \n        print(i)\n        #zeroing gradients\n        gen_labels.zero_grad()\n        disc_labels.zero_grad()\n\n        #generating fakes\n        gen_labels_out = gen_labels(input_withlabel[i:i+1000,:,:,:])\n\n        #classifying fakes and reals\n        disc_labels_out_fakes = disc_labels(gen_labels_out.detach())\n        disc_labels_out_reals = disc_labels(torch.tensor(input_withlabel[i:i+1000,:,:,:]).float())\n\n        disc_labels_out_fakes = torch.cat((disc_labels_out_fakes, one_hot_output),1)\n        disc_labels_out_reals = torch.cat((disc_labels_out_reals, one_hot_output),1)\n        \n        #calculating loss form discriminator\n        fakes_labels_loss = loss(disc_labels_out_fakes, torch.zeros_like(disc_labels_out_fakes))\n        reals_labels_loss = loss(disc_labels_out_reals, torch.ones_like(disc_labels_out_reals))\n        net_labels_loss = (fakes_labels_loss + reals_labels_loss)\/2\n\n        #optimizing discriminator\n        net_labels_loss.backward(retain_graph=True)\n        opt_labels2.step()\n\n        #calculating loss form generator\n        gen_labels_loss = loss(disc_labels_out_fakes, torch.ones_like(disc_labels_out_fakes))\n\n        #optimizing generator\n        gen_labels_loss.backward(retain_graph=True)\n        opt_labels1.step()\n\n    gen_loss_list2.append(gen_labels_loss)\n    disc_loss_list2.append(net_labels_loss)\n\n","a8b7b254":"class Z2W(nn.Module):\n    \n    '''\n    Takes in input of size(1,in_fea) and gives 'distanged' output of size(1,in_fea)\n    '''\n    def __init__(self,in_fea = 1,hidden_fea = 100,out_fea):\n        '''\n        in_fea : the number of features in 1D input\n        hidden_fea : the number of features in intermediate output\n        out_fea : the number of features in output\n        '''\n        super().__init__()\n        \n        self.fc1 = nn.Linear(in_features = in_fea, out_features = hidden_fea)\n        self.fc2 = nn.Linear(in_features = hidden_fea, out_features = hidden_fea)\n        self.fc3 = nn.Linear(in_features = hidden_fea, out_features = hidden_fea)\n        self.fc4 = nn.Linear(in_features = hidden_fea, out_features = in_fea)\n        \n    def forward(self,x):\n        m = nn.function.relu()\n        x = m(self.fc1(x))\n        x = m(self.fc2(x))\n        x = m(self.fc3(x))\n        x = self.fc4(x)\n        return x\n        ","42a8ab56":"class AdaIn(nn.module):\n    '''\n    Takes image and w as input, returns style changed image\n    image = [:,:,512], w = [1,512]\n    '''\n    def __init__(self,in_dim, channels):\n        super().__init__()\n        self.norm = nn.InstanceNorm2d(channels)\n        self.style1 = nn.Linear(in_features = in_dim, out_features = channels)\n        self.style2 = nn.Linear(in_features = in_dim, out_features = channels)\n        \n    def forward(self,image,w):\n        image = self.norm(image)\n        z1 = self.style1(w)[:,:,None,None] # z1 = [1,512] \n        z2 = self.style2(w)[:,:,None,None] # z1 = [1,512]\n        return image*z1 + z2\n        \n","7b914c23":"class OneBlockStyleGan(nn.Module):\n    def __init__(self, in_chan, out_chan, final_size, kernel):\n        super().__init__()\n        self.upsample = nn.Upsample(final_size, mode='bilinear')\n        self.conv1 = nn.Conv2d(in_features = in_chan, out_features = out_chan, kernel_size = kernel)\n        self.conv2 = nn.Conv2d(in_features = in_chan, out_features = out_chan, kernel_size = kernel)\n        self.adain1 = AdaIn(w_dim,out_chan)\n        self.adain2 = AdaIn(w_dim,out_chan)\n        self.act = nn.LeakyReLU(0.3)\n        \n    def forward(self,image,w):\n        image = self.upsample(image) # Image is now upsampled, hereafter the size and shpae of image will not change\n        image_conv = self.conv1(image)\n        image_conv = nn.functional.relu(image_conv)\n        image_conv = self.adain1(image_conv,w)\n        image_conv = self.conv2(image)\n        image_conv = nn.functional.relu(image_conv)\n        image_conv = self.adain2(image_conv,w)\n        image_conv = self.act(image_conv)\n        return [image_conv,image]","7b40b58a":"class STYLEGAN(nn.Module):\n    def __init__(self,in_chan,hidden_chan,out_chan):\n        super().__init__()\n        \n        self.map = Z2W(in_chan, hidden_chan, out_chan)\n        self.constant = nn.Parameter(torch.randn(1, out_chan, 4, 4))\n        self.block1 = OneBlockStyleGan(in_chan, out_chan, (8,8,512), 1)\n        self.block2 = OneBlockStyleGan(in_chan, out_chan, (16,16,512), 1)\n        self.block3 = OneBlockStyleGan(in_chan, out_chan, (32,32,512), 1)\n        self.block4 = OneBlockStyleGan(in_chan, out_chan, (64,64,512), 1)\n        self.block5 = OneBlockStyleGan(in_chan, out_chan, (128,128,512), 1)\n        self.block6 = OneBlockStyleGan(in_chan, out_chan, (256,256,512), 1)\n        self.make_image = nn.Conv2d(in_channels = 512,out_channels = 3, kernel_size = 1)\n    def forward(self,EPOCH,z):\n        w = self.map(z)\n        c = self.constant\n        if EPOCH <= 100:\n            [image_conv,image] = self.block1(c,w)\n            mixed_image = (1\/EPOCH)*image + (1 - 1\/EPOCH)*(image_conv)\n            mixed_image = self.make_image(mixed_image)\n            return mixed_image\n        else if EPOCH <= 200:\n            [image_conv,image] = self.block1(c,w)\n            [image_conv2,image2] = self.block2(image_conv,w)\n            EPOCH = EPOCH - 100\n            mixed_image = (1\/EPOCH)*image2 + (1 - 1\/EPOCH)*(image_conv2)\n            EPOCH = EPOCH + 100\n            mixed_image = self.make_image(mixed_image)\n            return mixed_image\n        else if EPOCH <= 300:\n            [image_conv,image] = self.block1(c,w)\n            [image_conv2,image2] = self.block2(image_conv,w)\n            [image_conv3,image3] = self.block3(image_conv2,w)\n            EPOCH = EPOCH - 200\n            mixed_image = (1\/EPOCH)*image3 + (1 - 1\/EPOCH)*(image_conv3)\n            EPOCH = EPOCH + 200\n            mixed_image = self.make_image(mixed_image)\n            return mixed_image\n        else if EPOCH <= 400:\n            [image_conv,image] = self.block1(c,w)\n            [image_conv2,image2] = self.block2(image_conv,w)\n            [image_conv3,image3] = self.block3(image_conv2,w)\n            [image_conv4,image4] = self.block4(image_conv3,w)\n            \n            EPOCH = EPOCH - 300\n            mixed_image = (1\/EPOCH)*image4 + (1 - 1\/EPOCH)*(image_conv4)\n            EPOCH = EPOCH + 300\n            mixed_image = self.make_image(mixed_image)\n            return mixed_image\n        else if EPOCH <= 500:\n            [image_conv,image] = self.block1(c,w)\n            [image_conv2,image2] = self.block2(image_conv,w)\n            [image_conv3,image3] = self.block3(image_conv2,w)\n            [image_conv4,image4] = self.block4(image_conv3,w)\n            [image_conv5,image5] = self.block5(image_conv4,w)\n            \n            EPOCH = EPOCH - 400\n            mixed_image = (1\/EPOCH)*image5 + (1 - 1\/EPOCH)*(image_conv5)\n            EPOCH = EPOCH + 400\n            mixed_image = self.make_image(mixed_image)\n            return mixed_image\n        else if EPOCH <= 600:\n            [image_conv,image] = self.block1(c,w)\n            [image_conv2,image2] = self.block2(image_conv,w)\n            [image_conv3,image3] = self.block3(image_conv2,w)\n            [image_conv4,image4] = self.block4(image_conv3,w)\n            [image_conv5,image5] = self.block5(image_conv4,w)\n            [image_conv6,image6] = self.block6(image_conv5,w)\n            \n            EPOCH = EPOCH - 500\n            mixed_image = (1\/EPOCH)*image6 + (1 - 1\/EPOCH)*(image_conv6)\n            EPOCH = EPOCH + 500\n            mixed_image = self.make_image(mixed_image)\n            return mixed_image\n        \n        ","3fd9ae73":"class STYLEDISC(nn.Module)\n    def __init__(self):\n    super().__init__()\n    self.downsample = nn.MaxPool2d(kernel_size = 2, stride = 2,dilation = 1, padding = 0)\n    self.linear = nn.Linear(in_features = 192,out_features = 1 )\n    self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 1)\n    self.conv2 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 1)\n    self.conv3 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 1)\n    self.conv4 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 1)\n    self.conv5 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 1)\n    \n    def forward(self,EPOCH,image):\n        if EPOCH <= 100:    \n            image = torch.flatten(image)\n            value = self.linear(image)\n            return value\n    \n        else if EPOCH <= 200:\n            image_conv = nn.relu(self.conv1(image))\n            image = self.downsample(image)\n            image_conv = self.downsample(image_conv)\n            EPOCH = EPOCH - 100\n            mixed_image = (1\/EPOCH)*(image) + (1 - 1\/EPOCH)*(image_conv)\n            EPOCH = EPOCH - 100\n            mixed_image = torch.flatten(mixed_image)\n            value = self.linear(mixed_image)\n            return value\n        \n        else if EPOCH <= 300:\n            \n            image_conv = nn.relu(self.conv2(image))\n            image_conv = self.downsample(image_conv)\n            image = self.downsample(image)\n            EPOCH = EPOCH - 200\n            mixed_image = (1\/EPOCH)*(image) + (1 - 1\/EPOCH)*(image_conv)\n            EPOCH = EPOCH + 200\n            \n            mixed_image = nn.relu(self.conv1(mixed_image))\n            mixed_image = self.downsample(mixed_image)\n            mixed_image = torch.flatten(mixed_image)\n            value = self.linear(mixed_image)\n            return value\n        \n        else if EPOCH <= 400:\n            \n            \n            image_conv = nn.relu(self.conv3(image))\n            image_conv = self.downsample(image_conv)\n            image = self.downsample(image)\n            EPOCH = EPOCH - 300\n            mixed_image = (1\/EPOCH)*(image) + (1 - 1\/EPOCH)*(image_conv)\n            EPOCH = EPOCH + 300\n            \n            mixed_image = nn.relu(self.conv2(mixed_image))\n            mixed_image = self.downsample(mixed_image)\n            mixed_image = nn.relu(self.conv1(mixed_image))\n            mixed_image = self.downsample(mixed_image)\n            mixed_image = torch.flatten(mixed_image)\n            value = self.linear(mixed_image)\n            return value\n        else if EPOCH <= 500:\n            \n            \n            image_conv = nn.relu(self.conv4(image))\n            image_conv = self.downsample(image_conv)\n            image = self.downsample(image)\n            EPOCH = EPOCH - 400\n            mixed_image = (1\/EPOCH)*(image) + (1 - 1\/EPOCH)*(image_conv)\n            EPOCH = EPOCH + 400\n            \n            mixed_image = nn.relu(self.conv3(mixed_image))\n            mixed_image = self.downsample(mixed_image)\n            mixed_image = nn.relu(self.conv2(mixed_image))\n            mixed_image = self.downsample(mixed_image)\n            mixed_image = nn.relu(self.conv1(mixed_image))\n            mixed_image = self.downsample(mixed_image)\n            mixed_image = torch.flatten(mixed_image)\n            value = self.linear(mixed_image)\n            return value\n        \n        else if EPOCH <= 600:\n                \n            image_conv = nn.relu(self.conv5(image))\n            image_conv = self.downsample(image_conv)\n            image = self.downsample(image)\n    \n            EPOCH = EPOCH - 500\n            mixed_image = (1\/EPOCH)*(image) + (1 - 1\/EPOCH)*(image_conv)\n            EPOCH = EPOCH + 500\n            \n            mixed_image = nn.relu(self.conv4(mixed_image))\n            mixed_image = self.downsample(mixed_image) \n            mixed_image = nn.relu(self.conv3(mixed_image))\n            mixed_image = self.downsample(mixed_image) \n            mixed_image = nn.relu(self.conv2(mixed_image))\n            mixed_image = self.downsample(mixed_image)\n            mixed_image = nn.relu(self.conv1(mixed_image))\n            mixed_image = self.downsample(mixed_image)\n            mixed_image = torch.flatten(mixed_image)\n            value = self.linear(mixed_image)\n            return value\n        \n        \n        ","34805ad1":"#INITIALIZATIONS\nEPOCHS = 600\nz = torch.rand(1000,512)\n\nstyle_gan = STYLEGAN(512,100,512)\nopt_gan = Adam(style_gan.parameters(), lr = 0.001)\nstyle_disc = STYLEDISC()\nopt_disc = Adam(style_disc.parameters(), lr = 0.001)\n\n\nstyle_loss = nn.BCEWithLogitsLoss()\n\n#LET\nreal_images = torch.rand(1000,3,256,256)","14fe95ba":"for epoch in range(1,epochs):\n    fake_images = style_gan(epoch,z) # fake_images = (1000,3,:,:)\n    fake_out = style_disc(epoch,fake_images)\n    fake_real = style_disc(epoch,real_images)\n\n    fakes_style_loss = style_loss(fake_out, torch.zeros_like(fake_out))\n    reals_style_loss = style_loss(fake_real, torch.ones_like(fake_real))\n    net_style_loss = (fakes_style_loss + reals_style_loss)\/2\n    print(\"net_style loss \" + str(net_style_loss.item()))\n\n    #optimizing discriminator\n    net_style_loss.backward(retain_graph=True)\n    opt_gan.step()\n\n    #calculating loss form generator\n    gen_style_loss = style_loss(fake_out, torch.ones_like(fake_out))\n    print(\"gen_style loss \" + str(gen_style_loss.item()))\n\n    #optimizing generator\n    gen_style_loss.backward(retain_graph=True)\n    opt_disc.step()\n\n    print(\"\\n\")\n\n    ","036ada3a":"# to crop the image centrally to a particular size so that skip connections can be formed\ndef Crop(image,image_shape):\n    middle_height = image.shape[2] \/\/ 2\n    middle_width = image.shape[3] \/\/ 2\n    starting_height = middle_height - image_shape[2] \/\/ 2\n    final_height = starting_height + image_shape[2]\n    starting_width = middle_width - image_shape[3] \/\/ 2\n    final_width = starting_width + image_shape[3]\n    cropped_image = image[:, :, starting_height:final_height, starting_width:final_width]\n    return cropped_image","97a96663":"# SINGLE UNET CONTRACTING BLOCK\n# input : (m,n,h,h)\n# output: (m,n*2,(h-4)\/\/2,(h-4)\/\/2)\nclass UNet_con_Block(nn.Module):\n    def __init__(self,in_chan):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels = in_chan, out_channels = in_chan*2, kernel_size = 3)\n        self.conv2 = nn.Conv2d(in_channels = in_chan*2, out_channels = in_chan*2, kernel_size = 3)\n        self.pool = nn.MaxPool2d(kernel_size = 2,stride = 2)\n    def forward(self,image,num):\n        \n        image = nn.functional.relu(self.conv1(image))\n        image = nn.functional.relu(self.conv2(image))\n        image_s = image\n        if num == 1:\n            image = self.pool(image)\n        return [image,image_s]","cb065f50":"# SINGLE UNET EXPANDING BLOCK\n# input : (m,n,h,h)\n# output: (m,n\/\/2,2h-4,2h-4)\nclass UNet_exp_Block(nn.Module):\n    def __init__(self,in_chan):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels = in_chan, out_channels = in_chan\/\/2, kernel_size = 3)\n        self.conv2 = nn.Conv2d(in_channels = in_chan, out_channels = in_chan\/\/2, kernel_size = 3)\n        self.upsamp = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n    def forward(self,image,skip_image):\n        image = self.upsamp(image)\n        image = nn.functional.relu(self.conv1(image))\n        skip_image = Crop(skip_image,image.shape)\n        image = torch.cat([image, skip_image], axis=1)\n        image = nn.functional.relu(self.conv2(image))\n        return image","969a7f6d":"\nclass UNET(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.con_block1 = UNet_con_Block(32)\n        self.con_block2 = UNet_con_Block(64)\n        self.con_block3 = UNet_con_Block(128)\n        self.con_block4 = UNet_con_Block(256)\n        self.con_block5 = UNet_con_Block(512)\n        self.exp_block1 = UNet_exp_Block(1024)\n        self.exp_block2 = UNet_exp_Block(512)\n        self.exp_block3 = UNet_exp_Block(256)\n        self.exp_block4 = UNet_exp_Block(128)\n        \n        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 32,kernel_size = 1)\n        self.trans = nn.ConvTranspose2d(in_channels = 64, out_channels = 64, kernel_size = 19, output_padding = 8 , dilation = 10)\n        self.img = nn.Conv2d(in_channels = 64, out_channels = 3,kernel_size = 1)\n        \n    def forward(self,image):\n        \n        image1 = self.conv1(image)\n        print(image1.shape)\n        [image2,image2s] = self.con_block1(image1,1)\n        [image3,image3s] = self.con_block2(image2,1)\n        [image4,image4s] = self.con_block3(image3,1)\n        [image5,image5s] = self.con_block4(image4,1)\n        [image6,image6s] = self.con_block5(image5,0)\n        # image6 : (m,1024,24,24)\n        image7 = self.exp_block1(image6,image5s)\n        image8 = self.exp_block2(image7,image4s)\n        image9 = self.exp_block3(image8,image3s)\n        image10 = self.exp_block4(image9,image2s)\n        image10 = nn.ReLU(self.trans(image10))\n        image10 = nn.ReLU(self.img(image10))\n        return image10\n    \n    ","c90e405d":"UNet = UNET()\nfrom prettytable import PrettyTable\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad: continue\n        param = parameter.numel()\n        table.add_row([name, param])\n        total_params+=param\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\")\n    return total_params","c5205be0":"count_parameters(UNet)","dd556d51":"class Patch_Disc(nn.Module):\n    \n    def __init__(self, input_channels, hidden_channels=8):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels = input_channels, out_channels = hidden_channels,kernel_size = 3,stride = 2)\n        self.conv2 = nn.Conv2d(in_channels = hidden_channels, out_channels = hidden_channels\/\/2,kernel_size = 3,stride = 2)\n        self.conv3 = nn.Conv2d(in_channels = hidden_channels, out_channels = 1,kernel_size = 3)\n        \n    def forward(self, image, real_image):\n        image = torch.cat([image, real_image], axis=1)\n        image = self.conv1(image)\n        image = self.conv2(image)\n        image = self.conv3(image)\n        \n        return image","80f47005":"advesarial_loss = nn.BCEWithLogitsLoss() \npixel_loss = nn.L1Loss()\n\ndef gan_loss(gan_image,real_image,disc_out,lambda_):\n    adv_loss = advesarial_loss(disc_out, torch.ones_like(disc_out))\n    pix_loss = pixel_loss(real_image, gan_image)\n    gen_loss = adv_loss + lambda_ * pix_loss\n    return gen_loss\n\n","6787953f":"patch_disc = Patch_Disc(3,128)\nopt_unet = Adam(UNet.parameters(), lr = 0.001)\nopt_disc = Adam(patch_disc.parameters(), lr= 0.001)\nEPOCHS = 100\n\npix_real_input = torch.rand(100, 3,512,512)\npix_rand_input = torch.rand(100, 3,512,512)","14f66b38":"#training\nEPOCHS = 100\nfor epoch in range(1,epochs):\n    patch_disc.zero_grad()\n    UNet.zero_grad()\n    \n    pix_fake_output = UNet(pix_rand_input)\n    pix_fake_disc_out = patch_disc(pix_fake_output,pix_rand_input)\n    pix_fake_disc_loss = advesarial_loss(pix_fake_disc_out, torch.zeros_like(pix_fake_disc_out))\n    \n    pix_real_disc_out = patch_disc(pix_real_input,pix_rand_input)\n    pix_real_disc_loss = advesarial_loss(pix_real_disc_out, torch.ones_like(pix_real_disc_out))\n    \n    pix_net_disc_loss = (pix_fake_disc_loss + pix_real_disc_loss)\/2\n    pix_net_disc_loss.backward()\n    opt_disc.step()\n\n    pix_gen_loss = gan_loss(pix_fake_output, pix_real_input, pix_fake_disc_out, 0.01)\n    pix_gen_loss.backward()\n    opt_unet.step()\n","56a0d8b0":"class Residual(nn.Module):\n    def __init__(self,in_chan):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_chan, in_chan, kernel_size=3, padding=1, padding_mode='reflect')\n        self.conv2 = nn.Conv2d(in_chan, in_chan, kernel_size=3, padding=1, padding_mode='reflect')\n        self.innorm = nn.InstanceNorm2d(in_chan)\n        \n    def forward(self, image):\n        image_orig = image.clone()\n        image = nn.functional.relu(self.innorm(self.conv1(image)))\n        image = self.innorm(self.conv2(image))\n        return image_orig + image\n\nres = Residual(10)\nres(torch.rand(3,10,28,28)).shape","a68f0e9f":"class CYCLEGAN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.con_block1 = UNet_con_Block(32)\n        self.con_block2 = UNet_con_Block(64)\n        self.con_block3 = UNet_con_Block(128)\n        self.con_block4 = UNet_con_Block(256)\n        self.con_block5 = UNet_con_Block(512)\n        self.exp_block1 = UNet_exp_Block(1024)\n        self.exp_block2 = UNet_exp_Block(512)\n        self.exp_block3 = UNet_exp_Block(256)\n        self.exp_block4 = UNet_exp_Block(128)\n        self.res_block1 = Residual(1024)\n        self.res_block2 = Residual(1024)\n        self.res_block3 = Residual(1024)\n        \n        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 32,kernel_size = 1)\n        self.trans = nn.ConvTranspose2d(in_channels = 64, out_channels = 64, kernel_size = 19, output_padding = 8 , dilation = 10)\n        self.img = nn.Conv2d(in_channels = 64, out_channels = 3,kernel_size = 1)\n        \n    def forward(self,image):\n        \n        image1 = self.conv1(image)\n        print(image1.shape)\n        [image2,image2s] = self.con_block1(image1,1)\n        [image3,image3s] = self.con_block2(image2,1)\n        [image4,image4s] = self.con_block3(image3,1)\n        [image5,image5s] = self.con_block4(image4,1)\n        [image6,image6s] = self.con_block5(image5,0)\n        image6 = self.res_block1(image6)\n        image6 = self.res_block2(image6)\n        image6 = self.res_block3(image6)\n        \n        # image6 : (m,1024,24,24)\n        image7 = self.exp_block1(image6,image5s)\n        image8 = self.exp_block2(image7,image4s)\n        image9 = self.exp_block3(image8,image3s)\n        image10 = self.exp_block4(image9,image2s)\n        image10 = nn.ReLU(self.trans(image10))\n        image10 = nn.ReLU(self.img(image10))\n        return image10\n","90c04dc3":"class CycleDisc(nn.Module):\n    \n    def __init__(self, input_channels, hidden_channels=8):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels = input_channels, out_channels = hidden_channels,kernel_size = 3,stride = 2)\n        self.conv2 = nn.Conv2d(in_channels = hidden_channels, out_channels = hidden_channels\/\/2,kernel_size = 3,stride = 2)\n        self.conv3 = nn.Conv2d(in_channels = hidden_channels, out_channels = 1,kernel_size = 3)\n        \n    def forward(self, image):\n        image = nn.functional.relu(self.conv1(image))\n        image = nn.functional.relu(self.conv2(image))\n        image = self.conv3(image)\n        \n        return image","8173e222":"gen_1 = CYCLEGAN()\ngen_2 = CYCLEGAN()\ngen_opt = Adam(list(gen_1.parameters()) + list(gen_2.parameters()), lr=0.001)\ndisc_1 = CycleDisc(3)\ndisc_1_opt = Adam(disc_1.parameters(), lr=0.001)\ndisc_2 = Discriminator(dim_B).to(device)\ndisc_2_opt = Adam(disc_2.parameters(), lr=0.001)\nadv_crit = nn.BCEWithLogitsLoss()\noth_crit = nn.L1Loss()\nEPOCHS = 100\nreal1 = torch.rand(1000,3,512,512)\nreal2 = torch.rand(1000,3,512,512)","5ffe120d":"def gen_loss(real1,real2,disc1,disc2,gen1,gen2,adv,oth):\n    #ADV LOSS\n    disc_out1 = disc1(real1)\n    disc_out2 = disc2(real2)\n    fake1 = gen1(real1)\n    fake2 = gen2(real2)\n    disc_fake_out1 = disc1(fake1)\n    disc_fake_out2 = disc2(fake2)\n    adv_loss = adv(disc_fake_out1, torch.ones_like(disc_fake_out1)) + adv(disc_fake_out2, torch.ones_like(disc_fake_out2)) \n    #CYCLE LOSS\n    real_1 = gen2(disc_out1)\n    real_2 = gen1(disc_out2)\n    cycle_loss = oth(real_1,real1) + oth(real_2,real2)\n    \n    #identity loss\n    disc_1 = disc1(real2)\n    disc_2 = disc2(real1)\n    iden_loss = oth(disc_1,real2) + oth(disc_2,real1)\n    return adv_loss + cycle_loss + iden_loss","3e0e014a":"for epoch in range(1,EPOCHS):\n    \n    disc_out1 = disc1(real1)\n    disc_out2 = disc2(real2)\n    fake1 = gen1(real1)\n    fake2 = gen2(real2)\n    disc_fake_out1 = disc1(fake1.detach())\n    disc_fake_out2 = disc2(fake2.detach())\n    \n    disc1_loss1 = adv_loss(disc_fake_out1,torch.zeros_like(disc_fake_out1))\n    disc1_loss2 = adv_loss(real2,torch.zeros_like(real2))\n    disc1_loss = (disc1_loss1 + disc1_loss2)\/2\n    disc1_loss.backward(retain_graph=True)\n    disc_1_opt.step()\n    \n    disc2_loss1 = adv_loss(disc_fake_out2,torch.zeros_like(disc_fake_out2))\n    disc2_loss2 = adv_loss(real2,torch.zeros_like(real2))\n    disc2_loss = (disc2_loss1 + disc2_loss2)\/2\n    disc2_loss.backward(retain_graph=True)\n    disc_2_opt.step()\n    \n    cyclegen_loss = gen_loss(real1,real2,disc_1,disc2,gen_1,gen_2,adv_crit,oth_crit)\n    cyclegen_loss.backward(retain_graph=True)\n    gen_opt.step()\n    ","dfeb0631":"<h2>Discriminator<h2>\n<h3>Here the discriminator is a Patch Gan, instead of giving a scalar output (real\/fake) it gives a matrix of outputs, each for a given section of image.<h3>","982e0ea6":"<h2>INITIALIZING FOR WGAN<h2>","9a05ae9a":"<h2>INITIALIZATIONS<h2>\n<h3>We will be using Adam optimizer and BCE Loss<h3>","894ea313":"<h2>TRAINING STYLEGAN<h2>","fd85617f":"<h2>Problems with the previous ones<h3> \n<h3>a) The discriminator learns a lot faster as its task is easy<br\/> \nb) The loss function used is not appropriate<br\/>\nc) The design of the ConvNets might not suit our needs<h3>","e326b331":"<h2>TRAINING Pix2Pix GAN<h2>","68cab948":"<h2>Comparing and Evaluating GANS using FID<h2>\n<h3>We use a pretrained ConvNet to convert the images to smaller dimensional vectors, So now we have 2 set of images, one real and one fake, Now we can compare these 2 distributions by using **Frechet Inception distance**, this smaller the FID the closer the 2 distributions are.<h3> Importing a pre-trained network can be done using torch vision but thats not the point here, hence we will be using random vectors and checkout FID","7e330fa5":"<h2>PIX2PIX NETWORK FOR IMAGE TRANSLATION<h2>","8bd88027":"<h2>STYLE GAN Discriminator<h2>\n","c5be11ed":"<h2>TRAINING BASIC GAN<h2>","48e9e282":"<h1>Wasserstein GANS and Gradient Penalty<h1>\n<h3>We do not have to write separate code for <strong>Critic\/Discriminator<strong> as we didnt apply sigmoid to the last layer of discriminator (we used BCEWithLogitsLoss instead)<h3>","39b6631e":"<h2> Training<h2>\n    ","59b5eeb6":"<h2>Initialization with Target Labels<h2>\n<h3> This is a case of conditional gan, i.e we pass some info about the generated image along with the random vector, so that the generator learns that whenever we pass a particular piece of info it should generate a particular kind of image.<br\/>In case of MNIST data we can pass the number we would like to generate, for that we convert it into a one hot vector and concatenate with the random vector.<h3>","69f31299":"<h2> Adaptive Initialization <h2>","af06480d":"<h2>DISCRIMINATOR<h2>\n<h3>It takes an (28,28) image and gives an scalar output, Its job is to distinguish real and fakes<h3>","af110ca9":"<h2>VISUALIZE THE NETWORK<h2>","b3f74230":"<h2>STYLE GAN<h2>\n<h3>This is complicated GAN with lots of parameters, all subsequent gan code cannot be run here due to insufficient memory, hence outputs aren't available, however the architecture here is very similar to STYLE GAN, and is helpful in understand the working of STYLE GAN.<h3>\n\n<h3>z -> w function<h3>","184829f6":"<h2>CYCLEGAN Generator<h2>","f96af0c1":"<h2>CYCLEGAN Discriminator<h2>","3bf17a5f":"<h2>Initializations and Loss Functions<h2>","2ce40f65":"<h1>1. BASIC GAN<h1>\n<h2>IMPORTING DATA <br\/> The data is MNIST Handwritten digits by Yann Lecun, we will be using this data for the basic gan.<h2>","02754850":"<h2>CROPPING FUNCTION<h2>","c8c25678":"<h2>STYLE GAN Generator<h2>\n<h3>At the end of training, It takes an random vector of size (m,512) and gives an output of size (m,3,256,256) <h3>","eb1ed3e6":"<h2>Discriminator Loss and Gradient Penalty<h2>","154a0116":"<h2>GENERATOR<h2>\n<h3>The generator's job is to take a random noise vector and generate an image from it.<h3>","1c047f7b":"<h1> GANs : Generative Advesarial Networks <h1>\n<h2> This notebook introduces the concept of GANs, I have written this notebook while doing the coursera specialization, so many concepts may overlap. We will begin with basic gan, then improve upon it. Finally we will se some SOTA Gan Architecture.<h2>\n<h3> Note 1: The Architecture of the Named GANs will not be exactly the same, this notebook is just for learning purposes not for importing the code.<br \/>\n     Note 2: Few codes do not have an output as the memory capacity in Kaggle is not sufficient to execute it.<h3>\n     ","baf671c8":"<h2>New Generator Loss<h2>","30104429":"<h2>Progressive Growing<h2>","d7187f19":"<h2>TRAINING WGAN<h2>","2a7b89a6":"<h2>CYCLE GAN<h2>\n<h3>We will be using the same Contracting Block and Expanding Block code as in style gan, we need to the code for residual block.<h3>","d703130f":"<h2>PIX2PIX GENERATOR<h2>"}}