{"cell_type":{"c16622c6":"code","222a4260":"code","38d44174":"code","43fd41c5":"code","d5760b6e":"code","f382959e":"code","9d0d8fa1":"code","1eda4c11":"code","cef7f40f":"code","6a5f9e39":"code","01723a77":"code","08afa7a4":"code","7ddba8f1":"code","c021f855":"code","2e0521ee":"code","f773d853":"code","f1328a10":"code","90284d0b":"code","04dab604":"code","efc9eb61":"code","ddbfb5fa":"code","2cdbde1c":"code","2ab634a7":"code","4593c033":"code","ef15f44b":"code","c957bf0a":"code","33f82743":"code","81402cc6":"code","3383e043":"code","93d56766":"code","1b2464f2":"code","87c3adab":"code","21f5a839":"code","513ce93a":"markdown","df758293":"markdown","51407f0c":"markdown","d6ed7496":"markdown","fe08faf7":"markdown","2b9a6384":"markdown","f3a9b1a4":"markdown","56ae41f5":"markdown","ec2c15e2":"markdown","51928018":"markdown","754c0542":"markdown","971269b2":"markdown","bb1dd9f9":"markdown"},"source":{"c16622c6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport re\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix, f1_score, classification_report, accuracy_score","222a4260":"df = pd.read_csv('..\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv')","38d44174":"df.head(3)","43fd41c5":"df.shape","d5760b6e":"df.info()","f382959e":"# Checking if any duplicate records are present\n\nduplicate=df[df.duplicated()] \nduplicate","9d0d8fa1":"# Removing duplicate records\n\ndf.drop_duplicates(inplace=True)","1eda4c11":"# Again check if any duplicate records are left\n\nduplicate = df[df.duplicated()] \nduplicate","cef7f40f":"df.describe(include='object')","6a5f9e39":"# Checking for null values\n\ndf.isnull().sum()","01723a77":"# Visualizing the disribution of ham and spam messages\n\nsns.countplot(df['Category'])","08afa7a4":"# Expanding contractions\n\n# Dictionary of English Contractions\ncontractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n                     \"you've\": \"you have\"}\n\n# Regular expression for finding contractions\ncontractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n\n# Function for expanding contractions\ndef expand_contractions(text,contractions_dict=contractions_dict):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, text)\n\n# Expanding Contractions in the reviews\ndf['Message'] = df['Message'].apply(lambda x:expand_contractions(x))","7ddba8f1":"# Converting text to lowercase\n\ndf['Message'] = df['Message'].apply(lambda x:x.lower())","c021f855":"# Removing digits and words containing digits\n\ndf['Message'] = df['Message'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))","2e0521ee":"# Removing punctuations\n\ndf['Message'] = df['Message'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))","f773d853":"# Removing extra spaces\n\ndf['Message']=df['Message'].apply(lambda x: re.sub(' +',' ',x))","f1328a10":"# Displaying the text after cleaning\n\nfor index,text in enumerate(df['Message'][0:3]):\n    print('Review %d:\\n'%(index+1), text)","90284d0b":"w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    rev = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text) if w not in stopwords.words('english')]\n    rev = ' '.join(rev)\n    return rev\n\ndf['Message'] = df.Message.apply(lemmatize_text)","04dab604":"# Creating text variable for ham\n\ndf_ham = df[df.Category == 'ham']\n\ntext_ham = \" \".join(text for text in df_ham['Message'])","efc9eb61":"# Creating text variable for spam\n\ndf_spam = df[df.Category == 'spam']\n\ntext_spam = \" \".join(text for text in df_spam['Message'])","ddbfb5fa":"# Creating wordcloud for ham\n\nham_cloud = WordCloud(collocations = False, background_color = 'white').generate(text_ham)\nplt.imshow(ham_cloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","2cdbde1c":"# Creating wordcloud for spam\n\nspam_cloud = WordCloud(collocations = False, background_color = 'white').generate(text_spam)\nplt.imshow(spam_cloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","2ab634a7":"# Creating a Bag of Words model\n\ncv = CountVectorizer()\nX = cv.fit_transform(df['Message']).toarray()","4593c033":"X","ef15f44b":"# Encoding dependent variable\n\nle = LabelEncoder()\ndf[\"Category\"] = le.fit_transform(df[\"Category\"])    # ham->0 and spam->1","c957bf0a":"y = df[\"Category\"]","33f82743":"# Splitting the dataset into train and test \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"X_train:\", X_train.shape)\nprint(\"X_test:\", X_test.shape)\nprint(\"y_train:\", y_train.shape)\nprint(\"y_test:\", y_test.shape)","81402cc6":"# Training the model using Naive Bayes classifier\n\nnb = MultinomialNB().fit(X_train, y_train)","3383e043":"print(\"Score of train data:\", nb.score(X_train, y_train))\nprint(\"Score of test data:\", nb.score(X_test, y_test))","93d56766":"y_pred = nb.predict(X_test)\ny_pred","1b2464f2":"# F1 score and accuracy\n\nf1_score = f1_score(y_test, y_pred, average='weighted')\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(\"F1 Score:\", f1_score)\nprint(\"Accuracy Score:\", accuracy)","87c3adab":"# Classification Report\n\nprint(classification_report(y_test, y_pred))","21f5a839":"cm = confusion_matrix(y_test, y_pred)\n\ngroup_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cm.flatten()\/np.sum(cm)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cm, annot=labels, fmt='', cmap='Dark2')","513ce93a":"## Text Preprocessing","df758293":"<b>There are 415 duplicate records in the dataset<\/b>","51407f0c":"<b> call, free, now, txt, reply, text, claim, prize, mobile, new, service, stop, etc. are some of the most common words in spam messages. <\/b> ","d6ed7496":"<b>The dataset doesn't have any missing values<\/b>","fe08faf7":"## Loading the dataset","2b9a6384":"<b>Hence, all duplicate records are removed.<\/b>","f3a9b1a4":"<b> The model performs well on train as well as test data <\/b>","56ae41f5":"<b> u, go, know, come, good, got, ok, time, ur, need, call etc. are some of the most common words in ham messages. <\/b> ","ec2c15e2":"<b>The dataset has 5572 records and 2 columns<\/b>","51928018":"<b> It can be seen that the count of spam messages is much less than ham. <\/b>","754c0542":"## Importing libraries","971269b2":"<b>Both columns are of object datatype<\/b>","bb1dd9f9":"## Model"}}