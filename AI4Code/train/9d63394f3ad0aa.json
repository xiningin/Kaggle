{"cell_type":{"aa1b6521":"code","22d0eff8":"code","bc18db44":"code","20653db8":"code","037ac5f7":"code","5868bccf":"code","4d556a5f":"code","90d34abc":"code","ed8533d6":"code","95ebecf7":"code","44a7ddd3":"code","200bfc01":"code","21db62d6":"code","801dd495":"code","cbbd5650":"code","bf3ba0a1":"code","70a15bd7":"code","70aa9998":"code","7065e337":"code","f13cfc8f":"code","6c4901c6":"code","ea8c68b2":"code","8119b5c5":"code","23acd1da":"code","349d6741":"code","6ec646ed":"code","2b8b5cb5":"code","febcf617":"code","2587241b":"code","a855b661":"code","a20a58a1":"code","a69134bc":"code","4b7f17f3":"code","d43454b3":"code","d705c264":"code","63882881":"code","98235427":"code","53f48824":"code","36e49434":"code","1c968da8":"code","875f4dea":"code","788869d6":"code","8a936d8f":"code","bbf4e932":"code","49c51c4d":"code","41792190":"code","1c5208f0":"code","35b4c31d":"code","bd6d4663":"code","7b1268a9":"code","b95a01dd":"code","9ceb3587":"code","86b067c3":"code","af681978":"code","0e069bcc":"code","0f89e433":"code","6b2d8752":"code","b4946008":"code","1b84b69d":"code","cbf5c8b4":"code","478bac4a":"code","dfd1d396":"code","bae0cbe9":"code","59f4645d":"markdown","9cbf9f20":"markdown","65f95f62":"markdown","1ed389f8":"markdown","0ba81461":"markdown","1245d034":"markdown","de403f22":"markdown","170bd9fc":"markdown","04a3bf42":"markdown","0938e253":"markdown","f4f4038f":"markdown","34e735b3":"markdown","fd94ba5f":"markdown","0938422c":"markdown","82166056":"markdown","d1658c3c":"markdown","6a6dc0bb":"markdown","fb5d09d3":"markdown","f4934d95":"markdown","8ac5b541":"markdown","6f15ba13":"markdown","a9e3aad6":"markdown","a9c8f154":"markdown","d96f71aa":"markdown","9afe3d83":"markdown"},"source":{"aa1b6521":"import sys\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","22d0eff8":"#path = '\/Users\/ridleyleisy\/Documents\/lambda\/unit_two\/DS-Unit-2-Classification-1\/ds4-predictive-modeling-challenge\/'","bc18db44":"train = pd.read_csv('..\/input\/train_features.csv')\ntest = pd.read_csv('..\/input\/test_features.csv')\nlabels = pd.read_csv('..\/input\/train_labels.csv')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')","20653db8":"def drop_cols_rows(df):\n    '''\n    Function takes in a pandas Dataframe and drops columns: num_private and recorded_by\n    Returns: df\n    '''\n    df.drop('num_private',axis=1,inplace=True)\n    df.drop('recorded_by',axis=1,inplace=True)\n    return df","037ac5f7":"#applying changes to train and test data\ntrain = drop_cols_rows(train)\ntest = drop_cols_rows(test)","5868bccf":"#test whether you want to keep longitude\nlong_mean = train.loc[train['longitude'] !=0]['longitude'].mean()\ntrain['longitude'].replace(0,long_mean,inplace=True)\ntest['longitude'].replace(0,long_mean,inplace=True)","4d556a5f":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport seaborn as sns","90d34abc":"# replacing 0 construction year with NaN values\ntrain['construction_year'] = train['construction_year'].replace(0,np.nan)\ntest['construction_year'] = test['construction_year'].replace(0,np.nan)","ed8533d6":"def transform_construction(df):\n    '''\n    Function that takes in pandas Dataframe and returns predicted values for construction year\n    Returns: np.array of predicted values\n    '''\n    df = df.select_dtypes(include=np.number)\n    X = df.loc[~df['construction_year'].isna()]\n    \n    # can only use these featuers since they differ \n    features = ['amount_tsh', 'gps_height', 'longitude', 'latitude',\n       'region_code', 'district_code', 'population']\n    target = 'construction_year'\n    \n    X_train = X[features]\n    y_train = X[target]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X_train,y_train)\n    m = RandomForestRegressor(n_estimators=20,max_depth=25)\n    m.fit(X_train, y_train)\n    \n    vals = m.predict(df.loc[df['construction_year'].isna()][features])\n    \n    return  vals","95ebecf7":"# applying random forest to np.nan values\ntrain.loc[train['construction_year'].isna(),'construction_year'] = transform_construction(train)\ntest.loc[test['construction_year'].isna(),'construction_year'] = transform_construction(test)\n# rounding construction year so it aligns with existing data\ntrain['construction_year'] = round(train['construction_year'])\ntest['construction_year'] = round(test['construction_year'])","44a7ddd3":"def add_construction_diff(df):\n    '''\n    Returns pandas Dataframe with time_since_construction year column added\n    '''\n    # convert series to datetime objects\n    df['date_recorded'] = pd.to_datetime(df['date_recorded'])\n    df['construction_year'] = pd.to_datetime(df['construction_year'].astype(int),format=\"%Y\")\n    \n    # difference of date recorded and construction year\n    df['time_since_construction'] = (df['date_recorded'] - df['construction_year']).dt.days\n    \n    # removing time_since_construction data that's less than 0 \n    df.loc[df['time_since_construction'] < 0,'time_since_construction'] = 0    \n    df['construction_year'] = df['construction_year'].dt.year\n    return df","200bfc01":"test = add_construction_diff(test)\ntrain = add_construction_diff(train)","21db62d6":"sns.distplot(train['construction_year'])","801dd495":"def add_ratios(df):\n    '''\n    Returns pandas Dataframe that includes tsh ratios\n    '''\n    df['tsh_by_longitude'] = df['amount_tsh'] \/ df['longitude']\n    df['tsh_by_latitude'] = df['amount_tsh'] \/ abs(df['latitude'])\n    df['tsh_by_height'] =  df['amount_tsh'] \/ df['gps_height']\n    df['tsh_by_height'] = df['tsh_by_height'].replace(np.inf,0).replace(np.nan,0)\n    return df","cbbd5650":"test = add_ratios(test)\ntrain = add_ratios(train)","bf3ba0a1":"from mpl_toolkits.mplot3d import Axes3D","70a15bd7":"%matplotlib notebook\nfig = plt.figure()\nax = fig.gca(projection='3d')\nax.plot_trisurf(train['longitude'], train['latitude'], train['gps_height'], cmap=plt.cm.viridis, linewidth=0.2)\nplt.show()","70aa9998":"labels = labels.merge(train,on='id')[['id','status_group']]","7065e337":"import category_encoders as ce\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n","f13cfc8f":"# grabbing unique counts for categorical variables\nunique = train.describe(exclude=np.number).T.sort_values(by='unique')","6c4901c6":"# appending categorical features with less than 130 values to list\ncat_features = list(unique.loc[unique['unique'] < 130].index)","ea8c68b2":"# numerical features to list\nnumeric_features = list(train.describe().columns[1:])","8119b5c5":"encode_features = cat_features\nfeatures = numeric_features + encode_features","23acd1da":"def transform_data_hot(features:list):\n    '''\n    Function that processes training data so it's ready for an ml model\n    Args: list of features to process\n    \n    Returns:\n    1 - X_train encoded \n    2 - X_val encoded\n    3 - y_train\n    4 - y_val\n    5 - X_train_sub\n    6 - X_val_sub\n    7 - Encoded column names\n    8 - Test dataset encoded\n    \n    '''\n    # using one hot encoding for categorical features\n    encoder = ce.OneHotEncoder(use_cat_names=True)\n    scaler = RobustScaler()\n    \n    X_train = train[features]\n    y_train = labels['status_group']\n    \n    # train test splitting the data\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train, y_train, train_size=0.80, test_size=0.20, \n        stratify=y_train, random_state=42)\n\n    X_train_sub = X_train[features]\n    X_val_sub = X_val[features]\n    \n    #creating pipeline that includes encoder and scaler\n    pipeline1 = Pipeline([('encoder',encoder),('scaler',scaler)])\n    # apply pipeline to train and val datasets\n    X_train_sub_encoded = pipeline1.fit_transform(X_train_sub)\n    X_val_sub_encoded = pipeline1.transform(X_val_sub)\n    \n    # test encoded is encoded and scaled dataframe for us to run our model on test dataset\n    test_encoded = pipeline1.transform(test[features])\n    \n    # X_encode_cols is dataframe \n    X_encode_cols = encoder.transform(X_train_sub).columns\n    \n    return X_train_sub_encoded, X_val_sub_encoded, y_train, y_val, X_train_sub, X_val_sub, X_encode_cols, test_encoded","349d6741":"from sklearn.ensemble import RandomForestClassifier","6ec646ed":"X_train_sub_encoded, X_val_sub_encoded, y_train, y_val, X_train_sub, X_val_sub, X_encode_cols, test_encoded = transform_data_hot(features)","2b8b5cb5":"# creating random forest model \nm = RandomForestClassifier(n_estimators=300,max_depth=28,max_features='auto',n_jobs=-1)","febcf617":"# fitting our training data to the model\nm.fit(X_train_sub_encoded,y_train)","2587241b":"print(f'Our Random Forest Score for Training is {m.score(X_train_sub_encoded,y_train)}')\nprint(f'Our Random Forest Score for Validation is {m.score(X_val_sub_encoded,y_val)}')","a855b661":"val_preds = m.predict(X_val_sub_encoded)","a20a58a1":"train_preds = m.predict(X_train_sub_encoded)","a69134bc":"# grabbing predictions for our test dataset \npreds = m.predict(test_encoded)","4b7f17f3":"import seaborn as sns\nimport matplotlib.pyplot as plt","d43454b3":"# setting feature importance to a pandas Dataframe\nfeat_impt = pd.DataFrame(m.feature_importances_,X_encode_cols).sort_values(by=0)","d705c264":"# Cumulative summing feature importance\ncumsum = np.cumsum(feat_impt[::-1])\n# Selecting all features that sum up to 95%\nsub_sum = cumsum.loc[cumsum[0] < .95]","63882881":"# Distplot on feature importance \nsns.distplot(feat_impt.iloc[len(sub_sum.index):-1])\nplt.text(x=.084,y=100,s='longitude',rotation=90,fontsize=12)\nplt.text(x=.041,y=120,s='gps_height',rotation=90,fontsize=12)\nplt.text(x=.068,y=230,s='time_since_construction',rotation=90,fontsize=12);","98235427":"# plotting cumulative importance on a line graph\nfig, ax1 = plt.subplots(nrows=1,ncols=1)\nfig.set_figheight(8)\nfig.set_figwidth(20)\n\nsub_sum.plot(ax=ax1)\nax1.set_xticklabels(np.arange(0,150,20))\nax1.set_title('Cumulative Importance by Number of Features',fontsize=16)\nax1.set_xlabel('Number of Features',fontsize=14)\nax1.set_ylabel('Cumlative Importance',fontsize=14)\nax1.get_legend().remove()\n","53f48824":"from sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.metrics import classification_report","36e49434":"confusion_matrix(val_preds,y_val)","1c968da8":"def plot_confusion_matrix(y_true, y_pred):\n    labels = unique_labels(y_true)\n    columns = [f'Predicted {label}' for label in labels]\n    index = [f'Actual {label}' for label in labels]\n    table = pd.DataFrame(confusion_matrix(y_true, y_pred), \n                         columns=columns, index=index)\n    return sns.heatmap(table, annot=True, fmt='d', cmap='viridis')\n    \nplot_confusion_matrix(val_preds,y_val);","875f4dea":"print(classification_report(val_preds,y_val))","788869d6":"#hard coded but numbers can be accessed from the confusion matrix above\nnon_fun_precision = 0.78 \nrepair_precision = 0.33","8a936d8f":"# let's merge our predicted data with the original dataset to get population stats","bbf4e932":"X_val_sub['preds'] = val_preds\ndf = X_val_sub[['population','preds']]","49c51c4d":"# let's split the dataframe into non functional wells and wells that need repair for populations above 1000\nnon_func = df.loc[(df['population'] > 1000) & (df['preds'] == 'non functional')].reset_index()\nrepairs = df.loc[(df['population'] > 1000) & (df['preds'] == 'functional needs repair')].reset_index()","41792190":"# let's add a actual column where the worker shows up and sees the condition of the well\n#allocating our error in prediction from non_fun_precision\nactual_func = round(len(non_func) * (1-non_fun_precision) \/ 2) \nactual_repair = round(len(non_func) * (1-non_fun_precision) \/ 2)\nactual_non_func = len(non_func) - actual_func - actual_repair","1c5208f0":"actual_func + actual_non_func + actual_repair","35b4c31d":"non_func['actual'] = 0\nnon_func['days'] = 0\nnon_func['cost'] = 0","bd6d4663":"non_func.loc[0:actual_func,'actual'] = 'functioning'\n# allocating 2 days to relocate to another well\nnon_func.loc[0:actual_func,'days'] = 2\nnon_func.loc[0:actual_func,'cost'] = 0","7b1268a9":"non_func.loc[actual_func:actual_func+actual_repair-1,'actual'] = 'repair'\n#allocating 2 days of relocating and 5 days of repairing\nnon_func.loc[actual_func:actual_func+actual_repair-1,'days'] = 7\nnon_func.loc[actual_func:actual_func+actual_repair-1,'cost'] = 300","b95a01dd":"non_func['actual'] = non_func['actual'].replace(0,'non_functional')\n# allocating 2 days of relocating and 7 days to fix\nnon_func.loc[non_func['actual'] == 'non_functional','days'] = 9\nnon_func.loc[non_func['actual'] == 'non_functional','cost'] = 500","9ceb3587":"non_func.head()","86b067c3":"# let's add a actual column where the worker shows up and sees the condition of the well\nactual_repair = round(len(repairs) * .33)\nactual_non_func = len(repairs) -  actual_repair","af681978":"actual_repair + actual_non_func","0e069bcc":"repairs['actual'] = 0\nrepairs['days'] = 0\nrepairs['cost'] = 0","0f89e433":"repairs.loc[0:actual_repair,'actual'] = 'repair'\n#allocating 2 days of relocating and 5 days of repairing\nrepairs.loc[0:actual_repair,'days'] = 7\nrepairs.loc[0:actual_repair,'cost'] = 300","6b2d8752":"repairs['actual'] = repairs['actual'].replace(0,'non_functional')\n# allocating 2 days of relocating and 7 days to fix\nrepairs.loc[repairs['actual'] == 'non_functional','days'] = 9\nrepairs.loc[repairs['actual'] == 'non_functional','cost'] = 500","b4946008":"repairs","1b84b69d":"f\"It will take {repairs.sum()['days'] + non_func.sum()['days']} days to complete\"","cbf5c8b4":"f\"It will cost {repairs.sum()['cost'] + non_func.sum()['cost']} dollars\"","478bac4a":"f\"We lost {non_func.loc[non_func['actual'] == 'functioning']['days'].sum()} days scouting out functioning wells\"","dfd1d396":"# submission = pd.DataFrame(test['id'])\n# submission['status_group'] = preds\n# submission.to_csv('submission-01.csv',index=False)","bae0cbe9":"# import pandas as pd\n\n# # Filenames of your submissions you want to ensemble\n# files = ['submission-01.csv', 'submission-02.csv', 'submission-03.csv']\n\n# submissions = (pd.read_csv(file)[['status_group']] for file in files)\n# ensemble = pd.concat(submissions, axis='columns')\n# majority_vote = ensemble.mode(axis='columns')[0]\n\n# sample_submission = pd.read_csv('sample_submission.csv')\n# submission = sample_submission.copy()\n# submission['status_group'] = majority_vote\n# submission.to_csv('my-ultimate-ensemble-submission.csv', index=False)","59f4645d":"## Predicting for Kaggle","9cbf9f20":"# ReadMe\nThis Notebook is broken into the following sections\n1. Loading Data\n2. Wrangling Data\n3. Feature Engineering\n4. Encoding with Pipelines\n5. Modeling\n6. Confusion Matrix \n7. Hypothetical Consultant Project from the Government\n8. Kaggle Submission ","65f95f62":"### Pipeline","1ed389f8":"### Reporting","0ba81461":"## Wrangle Data","1245d034":"## Feature Engineering\nFeatures we're adding\n1. time_since_construction = date_recorded - construction year\n2. Ratios:\n    <br>a. tsh_by_longitude\n    <br>b. tsh_by_latitude\n    <br>c. tsh_by_height","de403f22":"## Encoding and Scaling Categorical Data","170bd9fc":"### Ratios","04a3bf42":"Our model predicts that out of the need repair wells, we expect the well to actually be in a state of repair 33% of the time. Since we aren't perfect, we expect that the other 67% are in non functional state","0938e253":"## Load Data","f4f4038f":"### Fill in Missing Construction Year Data\nSince we have missing construction year data, we will create a random forest to predict the missing values","34e735b3":"### Time Since Construction","fd94ba5f":"#### Random Forest","0938422c":"### Feature Importance","82166056":"### Reduce label size to fit new features","d1658c3c":"Our model predicts that out of the non functional wells, we expect the well to actually be non functional 78% of the time. Since we aren't perfect, we expect that 11% are in need of repair and the other 11% are functional","6a6dc0bb":"### Grabbing data for modeling","fb5d09d3":"### Replace longitude\nReplacing missing longitude data with mean of longitude","f4934d95":"## Random Forest","8ac5b541":"### Dropping Cols\n1. num_private: only one unique observation\n2. recorded_by: only one unique observation","6f15ba13":"### How many days did we waste on scouting functional wells?","a9e3aad6":"#### 3d Visualization","a9c8f154":"### Model","d96f71aa":"## Confusion Matrix...Confusion? Don't be","9afe3d83":"## You are now a consultant for the Tanzanian Government\n\nYou are instructed by the government to assess total costs to fix their damaged water wells. The government informs you that\n1. On average, it costs \\\\$300 to repair a well and \\\\$500 to fix a non functional well\n2. On average, it takes 7 days to fix and 5 days to repair a well\n3. The government can allocate 1 worker for the job. Yup only one!\n4. It takes 2 days for the worker to relocate to a new well (this includes showing up to a well that was predicted as broken but is actually in working order\n\n\nThe government prioritizes the following wells \n1. Non functional wells with populations greater than 1,000\n2. Needs repair wells with populations greater than 1,000\n\nQuestions to Answer\n1. How long will this project take?\n2. What are the expected costs of this project?\n\n**Dataset to use -- Validation** <br>\n**The historical confusion matrix for their previous model is given. Note, it is different than the matrix above**"}}