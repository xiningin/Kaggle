{"cell_type":{"04c7aaa3":"code","30414553":"code","f69b09bc":"code","48cb6508":"code","4d4ba664":"code","59f553ba":"code","921dcf5f":"code","f4bd0f5b":"code","af0f2f2b":"code","b8d96a02":"code","28acc7db":"code","988b2a04":"code","71891ae0":"code","2507500f":"code","4ab8cc86":"code","48649cb6":"code","b7863db9":"code","7ba093ca":"code","2abc55f1":"code","9dfbffa0":"code","7406d78e":"markdown","65f95a29":"markdown","621c6ba0":"markdown","6886c5d4":"markdown","69a261aa":"markdown","27332414":"markdown","404f8c9a":"markdown","65e505ca":"markdown","7c1d4a8a":"markdown","e2e22c1b":"markdown","70a04a70":"markdown","c0168121":"markdown","9f1c9b47":"markdown","c6d84f5d":"markdown"},"source":{"04c7aaa3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n#from keras.optimizers import SGD, Adam\nfrom tensorflow.keras.optimizers import SGD\nfrom keras import optimizers\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.utils.np_utils import to_categorical","30414553":"my_data = pd.read_csv('..\/input\/az-handwritten-alphabets-in-csv-format\/A_Z Handwritten Data.csv').astype('float32')\nmy_data.head()","f69b09bc":"my_frame = pd.DataFrame(my_data)\nmy_frame.head()","48cb6508":"my_frame.shape","4d4ba664":"my_frame.info()","59f553ba":"my_frame.describe()","921dcf5f":"my_frame.isnull().sum()","f4bd0f5b":"x = my_frame.drop('0', axis = 1)\ny = my_frame['0']","af0f2f2b":"# we are reshaping the train & test image data so that they can be displayed as an image, \n# as initially in the CSV file they were present as 784 columns of pixel data. So we convert it to 28\u00d728 pixels.\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\nx_train = np.reshape(x_train.values, (x_train.shape[0], 28, 28))\nx_test = np.reshape(x_test.values, (x_test.shape[0], 28, 28))\n\nprint('Train Data Shape:', x_train.shape)\nprint('Test Data Shape:', x_test.shape)","b8d96a02":"# All the labels are present in the form of floating point values, that we convert to integer values, \n# & so we create a dictionary word_dict to map the integer values with the characters.\n\nword_dict = {\n    0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',11:'L',12:'M',13:'N',14:'O',15:'P',16:'Q',17:'R',18:'S',19:'T',20:'U',21:'V',22:'W',23:'X', 24:'Y',25:'Z'\n}","28acc7db":"# Plotting the number of alphabets in the dataset\n# 1): Firstly we convert the labels into integer values and append into the count list according to the label. \n#     This count list has the number of images present in the dataset belonging to each alphabet.\n# 2): Now we create a list \u2013 alphabets containing all the characters using the values() function of the dictionary.\n# 3): Now using the count & alphabets lists we draw the horizontal bar plot.\n\nplt.style.use('fivethirtyeight')\nplt.xkcd()\n\ny_integer = np.int0(y)\ncount = np.zeros(26, dtype = 'int')\n\nfor i in y_integer:\n    count[i] += 1\n    \nalphabets = []\n\nfor i in word_dict.values():\n    alphabets.append(i)\n    \nfig, ax = plt.subplots(1, 1, figsize = (15, 15))\nax.barh(alphabets, count)\n\nplt.xlabel('Number Of Elements..!!', fontsize = 20, fontweight = 'bold', color = 'green')\nplt.ylabel('Alphabets..!!', fontsize = 30, fontweight = 'bold', color = 'green')\nplt.grid()\nplt.show()","988b2a04":"# Now we shuffle the data with the shuffle() function to show the random images..\n\nplt.style.use('fivethirtyeight')\nplt.xkcd()\n\nshuff = shuffle(x_train[:100])\nfig, ax = plt.subplots(3, 3, figsize = (15, 15))\naxes = ax.flatten()\n\nfor i in range(9):\n    shu = cv2.threshold(shuff[i], 30, 200, cv2.THRESH_BINARY)\n    axes[i].imshow(np.reshape(shuff[i], (28, 28)), cmap = 'Greys')\nplt.show()\n","71891ae0":"# Reshaping the training & test dataset so that it can be put in the model\n# Now we reshape the train & test image dataset so that they can be put in the model.\n# New shape of train data: (297960, 28, 28, 1)\n# New shape of test data: (74490, 28, 28, 1)\n\nx_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\nprint(\"New shape of train data:\", x_train.shape)\n\nx_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\nprint(\"New shape of test data:\", x_test.shape)","2507500f":"# Here we convert the single float values to categorical values. \n# This is done as the CNN model takes input of labels & generates the output as a vector of probabilities.\n\n# What is CNN?\n#   CNN stands for Convolutional Neural Networks \n#   that are used to extract the features of the images using several layers of filters.\n\ncategorical_train = to_categorical(y_train, num_classes = 26, dtype = 'int')\nprint(\"New shape of train labels:\", categorical_train.shape)\n\ncategorical_test = to_categorical(y_test, num_classes = 26, dtype = 'int')\nprint(\"New shape of test labels:\", categorical_test.shape)","4ab8cc86":"# The convolution layers are generally followed by maxpool layers that are used to reduce the number of features extracted and\n# ultimately the output of the maxpool and layers and convolution layers are flattened into a vector of single dimension and\n# are given as an input to the Dense layer (The fully connected network).","48649cb6":"# We have the Sequential model that we designed for training the model over the training dataset.\n# The model created is as follows:\n\n# Sequential() Model: A Sequential model is appropriate for a plain stack of layers \n# where each layer has exactly one input tensor and one output tensor.\n\n# MaxPool2D: The input along its spatial dimensions (height and width) by taking the maximum value over an input window\n#(of size defined by pool_size) for each channel of the input. The window is shifted by strides along each dimension.\n\nmy_model = Sequential()\n\nmy_model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', input_shape = (28, 28, 1)))\nmy_model.add(MaxPool2D(pool_size = (2, 2), strides = 2))\n\nmy_model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmy_model.add(MaxPool2D(pool_size = (2, 2), strides = 2))\n\nmy_model.add(Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', padding = 'valid'))\nmy_model.add(MaxPool2D(pool_size = (2, 2), strides = 2))\n\nmy_model.add(Flatten())\n\nmy_model.add(Dense(64, activation = \"relu\"))\nmy_model.add(Dense(128, activation = \"relu\"))\n\nmy_model.add(Dense(26, activation = \"softmax\"))","b7863db9":"# 1): Here we are compiling the model, where we define the optimizing function & the loss function to be used for fitting.\n# 2): The optimizing function used is Adam, that is a combination of RMSprop & Adagram optimizing algorithms.\n# 3): The dataset is very large so we are training for only a single epoch, \n#     however, as required we can even train it for multiple epochs \n#     (which is recommended for character recognition for better accuracy).\n\nmy_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\nhistory = my_model.fit(x_train, categorical_train, epochs = 1, validation_data = (x_test, categorical_test))\n","7ba093ca":"# Now we are getting the model summary that tells us what were the different layers defined in the model &\n# also we save the model using model.save() function.\n\nmy_model.summary()\nmy_model.save(r'model_hand.h5')","2abc55f1":"# we print out the training & validation accuracies along with the training & validation losses for character recognition.\n\nprint(\"The validation accuracy is :\", history.history['val_accuracy'])\nprint(\"The training accuracy is :\", history.history['accuracy'])\nprint(\"The validation loss is :\", history.history['val_loss'])\nprint(\"The training loss is :\", history.history['loss'])","9dfbffa0":"# Here we are creating 9 subplots of (3,3) shape & visualize some of the test dataset alphabets along with their predictions, \n# that are made using the model.predict() function for text recognition.\n\nplt.style.use('fivethirtyeight')\nplt.xkcd()\n\nfig, axes = plt.subplots(3, 3, figsize = (12, 15))\naxes = axes.flatten()\n\nfor i, ax in enumerate(axes):\n    img = np.reshape(x_test[i], (28, 28))\n    ax.imshow(img, cmap = 'Greys')\n    \n    pred = word_dict[np.argmax(categorical_test[i])]\n    ax.set_title(\"Prediction: \" + pred, fontsize = 20, fontweight = 'bold', color = 'red')\n    ax.grid()","7406d78e":"## Convert float values into categorical values","65f95a29":"## Getting summary of Model","621c6ba0":"## Finally, make predictions on the test data","6886c5d4":"## Reshaping images from dataset","69a261aa":"## Import Libraries","27332414":"## Compiling the Model","404f8c9a":"## Plotting number of alphabets from dataset","65e505ca":"## Reshaping train & test images from dataset to put in Deep Learning Model","7c1d4a8a":"## Implementation of Deep Learning Model","e2e22c1b":"## Find accuracy, losses of Model","70a04a70":"## Shuffle images from dataset with shuffle() function to show random images","c0168121":"## Split x and y column from dataset","9f1c9b47":"## Read dataset with pandas (python)","c6d84f5d":"## Creating dictionary  of alphabets with indexes"}}