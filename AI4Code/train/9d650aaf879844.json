{"cell_type":{"09b47a7c":"code","1cdef72a":"code","4d81c13e":"code","6567244c":"code","db06849e":"code","e6e47238":"code","3ec75d83":"code","4ad8b007":"code","c271e8d5":"code","5706a563":"code","e8228951":"code","d032fc2e":"code","dfc3fe9f":"code","abfda5fb":"code","7e81b144":"code","90057897":"code","6fc825d4":"code","98465ed0":"code","edf4c436":"code","5e99bf91":"code","2d7cea3e":"code","148ae0ec":"code","6df30a84":"code","cc683c6e":"code","153e607c":"code","a2454218":"code","e79d4d38":"code","bf2d39bd":"code","48a9aa63":"code","f20b332b":"code","70d0abdd":"code","ec5bc327":"code","ff16febc":"code","014d7681":"code","013556c1":"code","4e12e9a9":"code","87a80227":"code","f4c57fa1":"markdown","f3893686":"markdown","1cd72169":"markdown","9460caf3":"markdown","d9ddbe85":"markdown","2c73064f":"markdown","e4c0af87":"markdown","ea937816":"markdown","46fdbd60":"markdown","8210e311":"markdown","8d02c2f7":"markdown","3dba225a":"markdown","4112a88c":"markdown","f4ceace0":"markdown","d5443a5a":"markdown","999d7a84":"markdown","92d62592":"markdown","6d3b8fc6":"markdown","016615bd":"markdown","897793fd":"markdown","52dd641b":"markdown","25a17a39":"markdown","0991eac1":"markdown"},"source":{"09b47a7c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt","1cdef72a":"matplotlib inline","4d81c13e":"train = pd.read_csv(\"\/kaggle\/input\/sf-crime\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/sf-crime\/test.csv\")","6567244c":"train.head()","db06849e":"train.dtypes","e6e47238":"test.head()","3ec75d83":"np.setdiff1d(test.columns,train.columns)","4ad8b007":"np.setdiff1d(train.columns,test.columns).tolist()","c271e8d5":"train.isnull().sum()","5706a563":"train.nunique()","e8228951":"train['Resolution'].value_counts().plot.barh();","d032fc2e":"train[train['Resolution']!='NONE']['Resolution'].value_counts().plot.barh();","dfc3fe9f":"train['PdDistrict'].value_counts().plot.barh();","abfda5fb":"train['DayOfWeek'].value_counts().plot.barh();","7e81b144":"train['Category'].value_counts().plot.barh(figsize = (5,18));","90057897":"train_feats = train.drop(labels = ['Descript','Resolution'],axis = 1)\ntrain_feats.head()","6fc825d4":"train_dummies = pd.get_dummies(train_feats[['PdDistrict','DayOfWeek']])\ntrain_feats_dummies = pd.merge(train_feats.drop(['PdDistrict','DayOfWeek'],1),train_dummies,left_index = True, right_index = True)\ntrain_feats_dummies.columns","98465ed0":"test_dummies = pd.get_dummies(test[['PdDistrict','DayOfWeek']])\ntest_feats_dummies = pd.merge(test.drop(['PdDistrict','DayOfWeek'],1),test_dummies,left_index = True, right_index = True)\ntest_feats_dummies = test_feats_dummies.drop(['Dates','Address'],1)\ntest_feats_dummies.columns","edf4c436":"X = train_feats_dummies.drop('Category',1)\ny = train_feats_dummies[['Category']]","5e99bf91":"X = X.drop(['Dates','Address'],1)\nX.head(2)","2d7cea3e":"y.head(2)","148ae0ec":"pd.get_dummies(y,prefix = None,prefix_sep = '')","6df30a84":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=5)","cc683c6e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import GaussianNB","153e607c":"#clf = GaussianNB().fit(X_train, y_train['Category']) #worked (maybe)\n#clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X_train, y_train['Category']) #worked\nclf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='ovr').fit(X_train, y_train['Category']) #worked","a2454218":"clf.coef_.shape","e79d4d38":"clf.classes_","bf2d39bd":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score","48a9aa63":"y_pred = clf.predict(X_test)\n\nprint(np.unique(y_pred,return_counts = True))\n#print(classification_report(y_test,y_pred))\nprint(accuracy_score(y_test, y_pred))","f20b332b":"X_train.head(2)","70d0abdd":"test_feats_dummies.head(2)","ec5bc327":"test_predictions = clf.predict(test_feats_dummies.drop('Id',1))\ntest_predictions","ff16febc":"test_probability_predictions = clf.predict_proba(test_feats_dummies.drop('Id',1))","014d7681":"test_probability_predictions.shape","013556c1":"clf.classes_","4e12e9a9":"submission = pd.merge(test['Id'],pd.DataFrame(data = test_probability_predictions,columns = clf.classes_), \\\n         left_index = True, right_index = True)","87a80227":"submission.to_csv('submission.csv', index = False)","f4c57fa1":"## Model Evaluation","f3893686":"## Closing Thoughts","1cd72169":"I'm just creating a quick notebook for this competition to try to keep my skills sharp. I've been using mostly SQL and Tableau at work lately, and I didn't want my python and data science skills to get too rusty. I'm not spending much time improving my score or going into much detail.  I'm just doing some very basic EDA, Feature Engineering, and getting a basic, functioning model up and running.","9460caf3":"Looking for prediction probabilities for each category.\nCan see below that there are unique coefficients for each of the 39 categories.","d9ddbe85":"Resolution excluding NONE","2c73064f":"Will create dummy variables for PdDistrict and DayOfWeek","e4c0af87":"## Predictions","ea937816":"No Null Values","46fdbd60":"# Feature Engineering","8210e311":"## Modeling","8d02c2f7":"Unique Value Counts","3dba225a":"## Exploratory Data Analysis","4112a88c":"Set up to predict probability for all categories, so get dummies","f4ceace0":"y_train.Category.tolist()\npd.get_dummies(y_train)\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(y_train.Category)\nX_train.pop('X')\nle = LabelEncoder()\nle.fit_transform(y_train)\nenc.transform(y_train).toarray()","d5443a5a":"I could add a lot more to make this better. I really didn't try many models. I didn't even train on the full train data, which I should have done after deciding which model to use. I also would have found the 50 or so most common addresses and created dummy variables from them and added them as features. I expect this would have been a significant added value.","999d7a84":"Columns in test not in train","92d62592":"## Intro","6d3b8fc6":"## Train\/Test Split","016615bd":"### Columns in train not in test","897793fd":"I'm just leaving this as is, since my objective was just to keep my skills sharp. I've been using mostly SQL and Tableau these days, so I didn't want my python and data science skills to get too rusty.","52dd641b":"dropping \"Descript\" and \"Resolution\" from train data since those features aren't in the test data.","25a17a39":"### A closer look at a few select counts","0991eac1":"Dropping \"Dates\" and \"Address\" for now...\nI might come up with something to do for those but not sure..."}}