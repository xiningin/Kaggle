{"cell_type":{"b9224857":"code","a2721b82":"code","9f63b446":"code","8893c66c":"code","555b83a5":"code","608d36d1":"code","2efc951d":"code","35c74b8d":"code","1b63c6f1":"code","73b5ded2":"code","cc0ed1f2":"code","400165b5":"code","75d6c857":"code","fb03d4a1":"code","5de2d472":"code","272d8c6e":"code","404f9913":"code","5e7ac0c8":"code","7fae9f66":"code","0abf271e":"code","405ae5ad":"code","3b0c8179":"code","ad6f45d8":"code","de52c248":"code","5ccd5fb7":"code","d4d9cf59":"code","8e7ab53e":"code","f84b1165":"code","1dc8f6c9":"markdown","a3fa971c":"markdown","268c3960":"markdown","434c8e7a":"markdown","b8ce8d6d":"markdown","8ec4c029":"markdown","eebc7a24":"markdown","564cc6b4":"markdown","3d59df31":"markdown","ceaab71a":"markdown","a6c68e12":"markdown","8b783cd5":"markdown","6caf0898":"markdown","db2d5646":"markdown","27155981":"markdown","6a058625":"markdown","f7fdad49":"markdown","29edf288":"markdown","031b7641":"markdown","2ff70150":"markdown","3d8a530f":"markdown","798b4687":"markdown","f50f67f1":"markdown","146d1aeb":"markdown","26a0ea2d":"markdown"},"source":{"b9224857":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2721b82":"test_set = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntrain_set = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")","9f63b446":"import seaborn as sns\nfrom matplotlib import pyplot as plt","8893c66c":"print(\"train set\")\nprint(train_set.head())\nprint(\"test set\")\nprint(test_set.head())","555b83a5":"print(\"train set\")\nprint(train_set.count())\nprint(\"test set\")\nprint(test_set.count())","608d36d1":"train_set.groupby(\"target\")[\"id\"].count().reset_index().plot.bar(figsize=(10,5) ,x=\"target\", y=\"id\", rot=0, xlabel=\"is disaster?\", ylabel=\"tweet number\", legend=False, title=\"Number of tweets by categories\")","2efc951d":"train_set[\"words_count\"] = train_set.text.apply(lambda x: len(x.split(\" \")))\n\nfig = plt.figure(figsize=(10,5))\n\nax1 = fig.add_subplot(211)\n\nax2 = fig.add_subplot(212)\n\ntrain_set.groupby(\"words_count\")[\"id\"].count().reset_index().plot.bar(ax=ax1,x=\"words_count\", y=\"id\", rot=0, xlabel=\"number of words in sentence\", ylabel=\"tweet number\", legend=False, title=\"Number of tweets by words count\")\n\ntrain_set.groupby([\"words_count\", \"target\"])[\"id\"].count().reset_index().pivot(columns='target', index='words_count').plot.bar(ax=ax2, rot=0, xlabel=\"number of words in sentence\", ylabel=\"tweet number\", legend=True, title=\"Number of tweets by words count for each target\")","35c74b8d":"test_set[\"words_count\"] = test_set.text.apply(lambda x: len(x.split(\" \")))\ntest_set.groupby(\"words_count\")[\"id\"].count().reset_index().plot.bar(figsize=(10,5) ,x=\"words_count\", y=\"id\", rot=0, xlabel=\"number of words in sentence\", ylabel=\"tweet number\", legend=False, title=\"Number of tweets by words count\")","1b63c6f1":"!pip install emoji","73b5ded2":"import emoji\nimport re\n\ndef strip_emoji(text: str) -> str:\n    \"\"\"\n    Strip emoji from an input text\n    \n    Args:\n        text: Input text with emoji\n    \n    Returns:\n        str: Ouptut string without emoji\n    \"\"\"\n    return re.sub(emoji.get_emoji_regexp(), r\"\", text)","cc0ed1f2":"train_set[\"text\"] = train_set.text.str.strip().str.replace(\"\\n\", \"\")\ntrain_set[\"text\"] = train_set.text.str.strip().str.replace(\"\\r\", \"\")\ntrain_set[\"text\"] = train_set.text.apply(strip_emoji)\ntrain_set[\"text\"] = train_set.text.str.strip().str.replace(\"#\", \"\")\ntrain_set[\"text\"] = train_set.text.str.lower()","400165b5":"test_set[\"text\"] = test_set.text.str.strip().str.replace(\"\\n\", \"\")\ntest_set[\"text\"] = test_set.text.str.strip().str.replace(\"\\r\", \"\")\ntest_set[\"text\"] = test_set.text.apply(strip_emoji)\ntest_set[\"text\"] = test_set.text.str.strip().str.replace(\"#\", \"\")\ntest_set[\"text\"] = test_set.text.str.lower()","75d6c857":"!pip install torch>=1.6.0 transformers==3.3.1","fb03d4a1":"import random\nfrom typing import Sequence\nfrom transformers import BertForMaskedLM, BertTokenizer\nimport torch","5de2d472":"# Load a pre-trained model to tokenize text\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\ntext = train_set.text[0]\n\n# Transform input text into list of tokens\ntokenized_text = tokenizer.tokenize(text)\nprint(tokenized_text)","272d8c6e":"# Insert mask token at a choosed place\nindex = 4\ntokenized_text.insert(index, \"[MASK]\")\nprint(\"Text Tokens\")\nprint(tokenized_text)\n\n# Convert tokens into ids\ntokenized_text_ids = torch.LongTensor(tokenizer.encode(tokenized_text, max_length=512, truncation=True))\nprint(\"Text Tokens ids\")\nprint(tokenized_text_ids)","404f9913":"# Load a pre-trained model to predict masked word\nmodel = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n\n# Pass Bert model in eval mode \nmodel.eval()\n# Deactivate the autograd engine \nwith torch.no_grad():\n    # Get masked output\n    predictions = model(tokenized_text_ids.unsqueeze(0))[0]\n# Get word at the mask token index\npredicted_word = tokenizer.convert_ids_to_tokens([torch.argmax(predictions[0, index+1]).item()])[0]\nprint(predicted_word)","5e7ac0c8":"# Insert the word into the original text\ntokenized_text[index] = predicted_word\n\n# Convert tokens into text\naugmented_text = \" \".join(tokenized_text)\n\nprint(augmented_text)","7fae9f66":"class WordAugmentation:\n    \"\"\"\n    Class used to augment an input sentence inserting or replacing contextual word(s)\n    \n    _mask_token: string representing mask token for Bert model\n    \n    Attributes:\n        _tokenizer: tokenizer to convert string sentence into tokens and tokens ids\n        _device: define running environment: cpu or cuda (gpu)\n        _model: model to predict word to insert\n    \"\"\"\n    \n    _mask_token: str = \"[MASK]\"\n    \n    def __init__(self,\n                model_path: str=\"bert-base-uncased\",\n                device: str=\"cuda\"):\n        \"\"\"\n        \n        Args:\n            model_path: path of pre-trained Bert model\n            device: cpu for cpu processing or cuda for gpu processing\n        \"\"\"\n        self._tokenizer = BertTokenizer.from_pretrained(model_path)\n        self._device = device\n        self._model = BertForMaskedLM.from_pretrained(model_path).to(self._device)\n        \n    def _predict(self,\n                 inputs_ids: torch.Tensor,\n                 mask_index: int) -> str:\n        \"\"\"\n        Word prediction method\n        \n        Args:\n            inputs_ids: text token ids\n            mask_index: index of mask token\n        \n        Returns:\n            str: predicted word\n        \"\"\"\n        self._model.eval()\n        inputs_ids = inputs_ids.to(self._device).unsqueeze(0)\n        with torch.no_grad():\n            predictions = self._model(inputs_ids)[0]\n        return self._tokenizer.convert_ids_to_tokens([torch.argmax(predictions[0, mask_index]).item()])[0]  \n\n    def apply(self,\n              text: str,\n              n_word=1,\n              action='insert') -> str:\n        \"\"\"\n        apply method with original text as input and augmented text as output\n        \n        Args:\n            text: original text to augment\n            n_word: number of word to insert\/repalce\n            action: insert for inserting words or replace for replacing words\n        \n        Returns:\n            str: augmented text\n        \n        Raises:\n            ValueError: if bad action parameter passed\n        \"\"\"\n        if isinstance(text, str) and text != \"\":\n            text = self._tokenizer.tokenize(text)\n            masks_index = []\n            for i in range(n_word):\n                random_index = random.choice([idx for idx in range(0, len(text)) if idx not in masks_index])\n                masks_index.insert(i, random_index)\n                \n                if action == \"insert\":\n                    text.insert(masks_index[i], self._mask_token)\n                elif action == \"replace\":\n                    text[masks_index[i]] = self._mask_token\n                else:\n                    raise ValueError(f\"action {action} not recognized. Please use: insert or replace\")\n                    \n                text_ids = torch.LongTensor(self._tokenizer.encode(text, max_length=512, truncation=True))\n                predicted_word = self._predict(text_ids, random_index+1)\n                \n                # Get previous and next words\n                if (random_index != 0) and (random_index != len(text) - 1):\n                    previous_word = text[random_index - 1]\n                    next_word = text[random_index + 1]\n                elif (random_index == 0) and (len(text) > 1):\n                    previous_word = \"\"\n                    next_word = text[random_index + 1]\n                elif (random_index == len(text) - 1) and (len(text) > 1):\n                    previous_word = text[random_index - 1] \n                    next_word = \"\"  \n                else:\n                    previous_word = \"\"\n                    next_word = \"\"\n                \n                if predicted_word in [previous_word, next_word, \" \"]:\n                    del text[random_index]\n                else:\n                    text[random_index] = predicted_word\n            text = \" \".join(text)\n        else:\n            text = \"\"\n        return text","0abf271e":"augmenter = WordAugmentation(device='cuda')","405ae5ad":"import tqdm\n\ntqdm.tqdm.pandas()\n\nnew_sentences_replace = train_set.progress_apply(lambda x: augmenter.apply(x[\"text\"], n_word=max(int(len(x[\"text\"])*0.05), 1), action='replace'), axis=1)","3b0c8179":"new_train_set_replace = pd.DataFrame({\"text\": new_sentences_replace.to_numpy(), \"target\": train_set.target.to_numpy()})","ad6f45d8":"augmented_train_set = pd.concat([train_set[[\"text\", \"target\"]], new_train_set_replace], ignore_index=True)","de52c248":"augmented_train_set.drop_duplicates(subset=\"text\", inplace=True)","5ccd5fb7":"!pip install fastai==2.0.16","d4d9cf59":"from fastai.text.all import TextDataLoaders, language_model_learner, AWD_LSTM, text_classifier_learner, \\\n    accuracy, Perplexity, F1Score","8e7ab53e":"# Define train and validate dataset\ntrain_target_0, validate_target_0 = np.split(augmented_train_set[augmented_train_set.target == 0].sample(frac=1,\n                                                                                              random_state=1),\n                                                            [int(.85 * len(augmented_train_set[augmented_train_set.target == 0]))])\n\ntrain_target_1, validate_target_1 = np.split(augmented_train_set[augmented_train_set.target == 1].sample(frac=1,\n                                                                                              random_state=1),\n                                                            [int(.85 * len(augmented_train_set[augmented_train_set.target == 1]))])\n\ntrain = pd.concat([train_target_0, train_target_1])\nvalidate = pd.concat([validate_target_0, validate_target_1])\ntrain[\"is_valid\"] = 0\nvalidate[\"is_valid\"] = 1\ntrain = pd.concat([train, validate])\nseed=42\n\n# Language model data\ndata_lm = TextDataLoaders.from_df(train,\n                                  path=\"fastai_files\/\",\n                                  is_lm=True,\n                                  valid_col=\"is_valid\",\n                                  text_col=\"text\",\n                                  seed=seed,\n                                  bs=32)\n# Classifier model data\ndata_class = TextDataLoaders.from_df(train,\n                                  path=\"fastai_files\/\",\n                                  is_lm=False,\n                                  valid_col=\"is_valid\",\n                                  label_col=\"target\",\n                                  text_col=\"text\",\n                                  seed=seed,\n                                  bs=32)\n\nlearn = language_model_learner(data_lm, arch=AWD_LSTM, pretrained=True, drop_mult=0.5, metrics=[accuracy, Perplexity()])\nlearn.fit_one_cycle(1, 1e-2)\n\nlearn.unfreeze()\nlearn.fit_one_cycle(10, 1e-3)\n\nlearn.save_encoder('processed_ft_enc')\n\nlearn = text_classifier_learner(data_class, arch=AWD_LSTM, pretrained=True, drop_mult=0.5, metrics=[accuracy, F1Score()])\nlearn.load_encoder('processed_ft_enc')\n\nlearn.fit_one_cycle(1, 1e-2)\n\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2\/(2.6**4),1e-2))\n\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3\/(2.6**4),5e-3))\n\nlearn.unfreeze()\nlearn.fit_one_cycle(3, slice(1e-3\/(2.6**4),1e-3))","f84b1165":"test_set[\"target\"] = test_set.text.apply(lambda x : learn.predict(x)[0])\ntest_set[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)","1dc8f6c9":"# Classifier\nThen we classified tweets using ULMFiT method with [Fastai](https:\/\/docs.fast.ai\/quick_start.html#Natural-language-processing)","a3fa971c":"### Bert prediction","268c3960":"Then, we insert a special token named mask. This token will indicate the place of the word to predict by Bert. We'll also transform tokens into ids to fit with Bert model input","434c8e7a":"### Mask token insertion","b8ce8d6d":"## Training","8ec4c029":"## Predictions","eebc7a24":"Concat original and new train set","564cc6b4":"## Word Augmentation class\nThis is the class use to augment a text with N words","3d59df31":"In order to augment the train set with fake tweets, I will improve a little bit my process:\n\n* Adding the possibility to insert or replace a word\n* Checking if the new word is not empty or equal to the next or previous word\n* Defining how many new words to add\n\nA lot of improvments could be done to perform better augmentation, but for this notebook only points above will be implemented.","ceaab71a":"## Construct final train set\nInitialization of my augmenter object","a6c68e12":"# Quick overview","8b783cd5":"### Sentence reconstruction","6caf0898":"Construct a new train set","db2d5646":"# Train set augmentation","27155981":"# Data Augmentation - Writing a new tweet from an original one","6a058625":"# Quick pre processing","f7fdad49":"### Tokenization","29edf288":"First, we need to tokenize the text. To do so, we will use [BertTokenizer](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#berttokenizer) from tranformers library.","031b7641":"## Word Augmentation Process","2ff70150":"We perform a quick pre-processing on text: lowercase, removing emoji, removing \"#\".. ","3d8a530f":"Just following fastai v2 documentation to train a [text classifier](https:\/\/docs.fast.ai\/tutorial.text)","798b4687":"In this Notebook, we will focus on a technique using semantic augmentation. This is inspired and based from this [article](https:\/\/towardsdatascience.com\/data-augmentation-in-nlp-2801a34dfc28).\n\nThe goal here is to build a simple word augmenter based on PyTorch and Bert from [Transformers](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html). To build it, we will used Bert Model with a language modeling head on top ([BertForMaskedLM](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#bertformaskedlm)), in order to predict best words to insert\/replace into original sentence.\n\nIf you want a complete package implemented multiples nlp techniques, please take a look to this [github repository](https:\/\/github.com\/makcedward\/nlpaug).","f50f67f1":"Make prediction on test set","146d1aeb":"Now, we have to pass these ids into BertForMaskedLM model and predict the word replacing mask token","26a0ea2d":"Finally, we reconstruct the sentence with the predicted word from Bert output"}}