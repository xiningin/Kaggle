{"cell_type":{"e6389172":"code","a0413fb8":"code","e63d703b":"code","a9ba6637":"code","f8335fb2":"code","c1fe0849":"code","5dd28725":"code","fa818a9b":"code","4ae3ac30":"code","3beacc61":"code","540fda2f":"code","7859904a":"code","0ae12307":"code","68f1aa83":"code","fff90085":"code","7d871014":"code","f3426070":"code","2f19afde":"code","f0ebeb52":"code","55a2095d":"code","6ddfcf0b":"code","eabcbde3":"code","12839f59":"code","340f0dfd":"code","830a4b6b":"code","bac19e3a":"code","5b6b38b4":"code","b972e294":"code","5715709c":"code","1f14f14b":"code","85d68cf0":"code","22f3ff4b":"code","a603e30f":"code","a4d602e1":"code","b5654a5a":"code","ea899ada":"code","f07c4e4f":"code","29e67d28":"code","9e8980fd":"code","a73c8c7d":"code","8745fa82":"code","da2a65a2":"code","b6a236f1":"code","f26bf790":"code","a6a06320":"code","1d110824":"code","c3336d88":"code","3d986e8b":"code","0ea439df":"code","1c7066be":"code","c92929e6":"code","7a3d7fb3":"code","969f0982":"code","a743a255":"code","826724c7":"code","6f9ac757":"code","a640434f":"code","07c9c881":"code","b907068f":"code","c1a1e7d3":"code","51543641":"code","5591ed35":"code","9446d696":"code","9b700d31":"code","6254ac3c":"code","77407542":"code","660249ba":"code","f223035e":"code","76e4be11":"code","6a604050":"code","cbc7b7b7":"code","3478a4cf":"code","63e79600":"code","11848d23":"code","5c271407":"code","17bc6935":"code","a3be5b7d":"code","272e55a3":"code","784a9935":"markdown","60d63c36":"markdown","7a8bbfeb":"markdown","c6208c79":"markdown","9afd51a9":"markdown","cb1665f0":"markdown","082f49b0":"markdown","de491adb":"markdown","b4f7b713":"markdown","b2a93532":"markdown","a542e224":"markdown","93688b56":"markdown","b6ff5668":"markdown","c0046438":"markdown","b2281132":"markdown","ccf3836e":"markdown","21419f34":"markdown","1b5ff136":"markdown","911a753d":"markdown","03841213":"markdown","3ec55dc3":"markdown","608d038c":"markdown","17b434df":"markdown","162ad4f1":"markdown","7384efcd":"markdown","f9ba0936":"markdown","3c2d743a":"markdown","f6ca5c24":"markdown","956001c1":"markdown","73d3176c":"markdown","1cc9c914":"markdown","709a9a03":"markdown","f964d322":"markdown","09e4beda":"markdown","657642ab":"markdown","9e76630b":"markdown","bea4d5ab":"markdown","33a6b26a":"markdown","8f8c1651":"markdown","1c97badc":"markdown","45fda7ab":"markdown","88b7bae0":"markdown","444f4efc":"markdown","63161852":"markdown","ff451456":"markdown","194ff53b":"markdown","8d1f9c5b":"markdown","6e91d6c4":"markdown","16e9ba6c":"markdown","d127b40a":"markdown","147a8df3":"markdown","2082b815":"markdown"},"source":{"e6389172":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a0413fb8":"#importing libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score","e63d703b":"df = pd.read_csv(\"\/kaggle\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv\")\ndf.head()","a9ba6637":"df.shape","f8335fb2":"df.describe()","c1fe0849":"df.info()","5dd28725":"#listing categorical variables as per the data definition and converting them to category\ncatvar=['sex','cp','fbs','restecg','exang','ca','thal', 'slope']\ndf[catvar]=df[catvar].astype('category')","fa818a9b":"df.dtypes","4ae3ac30":"df.describe()","3beacc61":"#create boxplots to inspect the presence of outliers\nnum_cols=['trestbps','chol','thalach','oldpeak']\n#  plot Numerical Data\na = 2  # number of rows\nb = 2  # number of columns\nc = 1  # initialize plot counter\n\nfig = plt.figure(figsize=(20,30))\nfor i in num_cols:\n    plt.subplot(a, b, c)\n    plt.title('{} (box)'.format(i, a, b, c))\n    plt.xlabel(i)\n    plt.boxplot(x = df[i])\n    c = c + 1\nplt.show()","540fda2f":"#function to remove outliers using IQR\ndef subset_by_iqr(df, column, whisker_width=1.5):\n   # Calculate Q1, Q2 and IQR\n    q1 = df[column].quantile(0.25)                 \n    q3 = df[column].quantile(0.75)\n    iqr = q3 - q1\n    filter = (df[column] >= q1 - whisker_width*iqr) & (df[column] <= q3 + whisker_width*iqr)\n    return df.loc[filter]                                                     \n","7859904a":"for i in num_cols:\n    df = subset_by_iqr(df, i)","0ae12307":"df.shape","68f1aa83":"df.head()","fff90085":"#creating dummy variable and dropping the first one\ndf1=pd.get_dummies(df[['sex','cp','fbs','restecg','exang','ca','thal','slope']], drop_first=True)\n#adding results to original df\ndf=pd.concat([df,df1],axis=1)\ndf.head()\ndf.columns","7d871014":"#Dropping the repeated variables\ndf=df.drop(['sex','cp','fbs','restecg','exang','ca','thal','slope'],1)","f3426070":"df.shape\ndf.columns","2f19afde":"#dropping target variable from X\nX=df.drop('target', 1)","f0ebeb52":"X.head()","55a2095d":"#putting target variable to y\ny=df['target']\ny.head()","6ddfcf0b":"#Splitting data into train and test set\nX_train,X_test, y_train,y_test=train_test_split(X,y, train_size=0.7,test_size=0.3, random_state=100)","eabcbde3":"scaler=StandardScaler()","12839f59":"numeric_col=['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\nX_train[numeric_col]=scaler.fit_transform(X_train[numeric_col])\nX_train.head()","340f0dfd":"##Checking the rate of heart attack\nrisk=(sum(df['target'])\/len(df['target'].index))*100\nrisk","830a4b6b":"#lets see the correlation matrix for the entire dataset\nplt.figure(figsize=(20,10))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","bac19e3a":"X_train=X_train.drop(['exang_1','thal_2','slope_2'],1)\nX_test=X_test.drop(['exang_1','thal_2','slope_2'],1)","5b6b38b4":"plt.figure(figsize=(20,10))\nsns.heatmap(X_train.corr(),annot=True)\nplt.show()","b972e294":"#Logistic Regression Model\nlogm=sm.GLM(y_train,(sm.add_constant(X_train)), family=sm.families.Binomial())\nlogm.fit().summary()","5715709c":"logreg=LogisticRegression()","1f14f14b":"rfe=RFE(logreg,13)\nrfe=rfe.fit(X_train, y_train)","85d68cf0":"rfe.support_","22f3ff4b":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","a603e30f":"col=X_train.columns[rfe.support_]\ncol","a4d602e1":"X_train_sm=sm.add_constant(X_train[col])","b5654a5a":"logm1=sm.GLM(y_train, X_train_sm,famiily=sm.families.Binomial())\nres=logm1.fit()\nres.summary()","ea899ada":"vif=pd.DataFrame()\nvif['Features']=X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f07c4e4f":"col=col.drop('ca_4',1)\ncol","29e67d28":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","9e8980fd":"col=col.drop('thal_1',1)\ncol","a73c8c7d":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","8745fa82":"col=col.drop('ca_3',1)\ncol","da2a65a2":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","b6a236f1":"col=col.drop('restecg_1',1)\ncol","f26bf790":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","a6a06320":"vif=pd.DataFrame()\nvif['Features']=X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","1d110824":"y_train_pred=res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","c3336d88":"y_train_pred_final = pd.DataFrame({'Risk':y_train.values, 'Risk_Prob':y_train_pred})\n\ny_train_pred_final.head()","3d986e8b":"y_train_pred_final['predicted'] = y_train_pred_final.Risk_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","0ea439df":"confusion=metrics.confusion_matrix(y_train_pred_final.Risk,y_train_pred_final.predicted)\nprint(confusion)","1c7066be":"print(metrics.accuracy_score(y_train_pred_final.Risk,y_train_pred_final.predicted))","c92929e6":"TP=confusion[1,1]\nTN=confusion[0,0]\nFP=confusion[0,1]\nFN= confusion[1,0]","7a3d7fb3":"TP\/float(TP+FN)","969f0982":"TN\/float(TN+FP)","a743a255":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","826724c7":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Risk, y_train_pred_final.Risk_Prob, drop_intermediate = False )","6f9ac757":"draw_roc(y_train_pred_final.Risk, y_train_pred_final.Risk_Prob)","a640434f":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Risk_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","07c9c881":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Risk, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","b907068f":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","c1a1e7d3":"precision_score(y_train_pred_final.Risk, y_train_pred_final.predicted)","51543641":"recall_score(y_train_pred_final.Risk, y_train_pred_final.predicted)","5591ed35":"X_test[numeric_col]=scaler.transform(X_test[numeric_col])\nX_test = X_test[col]\nX_test.head()","9446d696":"X_test_sm = sm.add_constant(X_test)","9b700d31":"y_test_pred = res.predict(X_test_sm)","6254ac3c":"y_test_pred[:10]","77407542":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","660249ba":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","f223035e":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","76e4be11":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","6a604050":"y_pred_final.head()","cbc7b7b7":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Risk_Prob','target' : 'Risk'})","3478a4cf":"# Let's see the head of y_pred_final\ny_pred_final.head()","63e79600":"y_pred_final['final_predicted'] = y_pred_final.Risk_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_pred_final.head()","11848d23":"metrics.accuracy_score(y_pred_final.Risk, y_pred_final.final_predicted)","5c271407":"confusion2 = metrics.confusion_matrix(y_pred_final.Risk, y_pred_final.final_predicted )\nconfusion2","17bc6935":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","a3be5b7d":"TP\/float(TP+FN)","272e55a3":"TN\/float(TN+FP)","784a9935":"##### Assessing the model with statsmodels","60d63c36":"##### From the above we see that cutoff of 0.5 which we had chosen earlier is the optimum cutof point","7a8bbfeb":"#### 2.4 :Feature Scaling","c6208c79":"##### Overall Accuracy of the model","9afd51a9":"##### Confusion Matrix on Test Set","cb1665f0":"##### Recall","082f49b0":"There is almost 56% risk of suffering heart attack in the given dataset and this looks like a fairly balanced dataset","de491adb":"Let us perform one-hot encoding for these variables with multiple levels","b4f7b713":"### Step2:Data Preparation","b2a93532":"### Step4: Feature Selection Using RFE","a542e224":"Since the p-values of all the variables are less than 0.05 and also the VIF are pretty low we can consider this model as final model and free from any multicollinearity","93688b56":"##### From above we see that there are no missing values and all the columns have correct datatype","b6ff5668":"#### 2.2 :Dummy Variable Creation for categorical variables","c0046438":"##### Sensitivity","b2281132":"##### Confusion Matrix","ccf3836e":"Optimal cutoff probability is that prob where we get balanced sensitivity and specificity","21419f34":"##### Specificity","1b5ff136":"##### Columns suspected to have outliers are restbps,chol,thalach,oldpeak","911a753d":"##### Precision","03841213":"Since the problem at hand is to predict the probability of heart attack, we would like to maximise the sensitivity. We would not like to classify a person having high risk as one having low risk","3ec55dc3":"### Step8:Precision and Recall","608d038c":"##### Sensitivity","17b434df":"the variable thal_1 has a significantly high p-value and hence can be dropped","162ad4f1":"### Step7:Checking for Optimal Cut-off Point","7384efcd":"the variable ca_3 has a significantly high p-value and hence can be dropped","f9ba0936":"##### Creating a dataframe with actual risk probabilities and the predicted probabilities","3c2d743a":"The features that affect the probability of a heart attack are <br>\noldpeak,sex_1,cp_1,cp_2,cp_3,ca_1,ca_2,thal_3,slope_1","f6ca5c24":"All the variables have pretty low VIF and thus do not indicate multicollinearity. However the variable ca_4 has a significantly high p-value and hence can be dropped","956001c1":"##### Checking the correlation matrix again","73d3176c":"We see that the p-values for all the variables are significantly low.Let us again check the VIF","1cc9c914":"##### Clearly sensitivity is higher than specificity with our current model and threshold for prediction defined at 0.5","709a9a03":"94% of the area is under the ROC curve .","f964d322":"##### 2.5: Looking at correlations","09e4beda":"In the training set sensitivity is 89% and specificity is 85% <br>\nIn the test set sensistivity is 84% and specificity is 81%","657642ab":"### Conclusion","9e76630b":"### Step9:Making Predictions on the Test Set","bea4d5ab":"##### Accuracy","33a6b26a":"##### Checking VIF","8f8c1651":"##### Dropping highly correlated dummy variables","1c97badc":"#### Making predictions on the test set","45fda7ab":"the variable restecg_1 has a significantly high p-value and hence can be dropped","88b7bae0":"An ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","444f4efc":"##### Specificity","63161852":"2.1: Outlier Treatment","ff451456":"### Step 1:Inspecting dataframe","194ff53b":"##### Creating new column 'predicted' with 1 if Risk_Prob > 0.5 else 0","8d1f9c5b":"### Step5:Model Evaluation","6e91d6c4":"### Step6:Plotting the ROC Curve","16e9ba6c":"##### Predicting on training set","d127b40a":"#### 2.3: Test-Train Split","147a8df3":"### Step3 :Model Building","2082b815":"After dropping the highly correlated dummy variables let us again check the correlation matrix"}}