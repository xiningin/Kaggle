{"cell_type":{"c875b568":"code","f64dbf09":"code","7c4e8254":"code","afec1dcb":"code","a37c597a":"code","8a3d3ec6":"code","f10eef4f":"code","22fde5a9":"code","27b790f3":"code","2c82d065":"code","7e0220d5":"code","8b8c1b7b":"code","6c2f631c":"code","a3d5fbb1":"code","a4452a32":"code","5cfdae0b":"code","66fadc1b":"code","1f343175":"code","89db40a6":"code","eb1d94da":"code","71aca308":"code","c6d85892":"code","31215552":"code","7c74f668":"code","4c0e57da":"code","c2127db1":"code","d36ed1f3":"code","a5450699":"code","c4011d3b":"code","8822286a":"code","4e8c926c":"code","af0992a0":"code","ba5def9c":"code","5485670e":"code","cfcc5018":"code","64e51953":"code","8b0cf60d":"code","7c66f1f4":"code","f2b2fecd":"code","54279ca5":"code","5581d340":"code","e08cc056":"code","b7b203e6":"code","d154f3c9":"code","96c1a72e":"code","fc4bc887":"code","c80b71ea":"code","eda19f83":"code","fc3328d7":"code","86c492fc":"code","5e0aefb5":"code","957ac5c8":"code","7a238a92":"code","efc83474":"code","d0518cd3":"code","fe7d21ce":"code","3b7fca8a":"code","8bb5451e":"code","be81a16e":"code","0d0bd8b3":"code","4d3a49eb":"code","acd7cb8c":"code","e35450ae":"code","a616eab6":"code","d67737ea":"code","5df74e82":"code","2f565b6b":"code","a2102fa1":"code","a7c0f3cc":"code","7eabbd17":"code","5392005f":"code","c5e3b2fe":"code","52f40309":"code","d8e89e3a":"code","2743d0b4":"code","1bbe0e9f":"code","cc6e24cd":"code","567edb27":"code","033e23b6":"code","5844d67c":"code","37b5d4ea":"code","5625e7ed":"code","d583de12":"code","7954eaa9":"markdown","5b2fefa5":"markdown","526ea78e":"markdown","c159364c":"markdown","aedecec4":"markdown","9bdf262d":"markdown","59ba25ad":"markdown","7776157a":"markdown","9bad518c":"markdown","bbf3e02e":"markdown","28e1c059":"markdown","6324cc4f":"markdown","a9a30371":"markdown","c3b6eed0":"markdown","fb5cfe9b":"markdown"},"source":{"c875b568":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","f64dbf09":"# linspace is an in-built function in Python\u2019s NumPy library. \n# It is used to create an evenly spaced sequence in a specified interval.\n\n# np.linspace(start, stop, num = 50, endpoint = True, retstep = False, dtype = None)\n\n# start  : [optional] start of interval range. By default start = 0\n# stop   : end of interval range\n# retstep : If True, return (samples, step). By deflut restep = False\n# num    : [int, optional] No. of samples to generate\n# dtype  : type of output array\n\n# Return :\n# ndarray\n# step : [float, optional], if restep = True\n","7c4e8254":"x = np.linspace(0, 50, num = 501)\ny = np.sin(x)","afec1dcb":"#x","a37c597a":"#y","8a3d3ec6":"plt.plot(x, y)\nplt.show()","f10eef4f":"# df = pd.DataFrame(data = x ,columns = ['x'])\n# df\n# df['Sine'] = pd.DataFrame(data = y)\n# df","22fde5a9":"df = pd.DataFrame(data = y, index = x, columns = ['Sine'])\ndf.head(5)","27b790f3":"# take last points","2c82d065":"len(df)","7e0220d5":"test_percent = 0.1 #10%","8b8c1b7b":"len(df)*test_percent","6c2f631c":"test_point = np.round(len(df)*test_percent)\ntest_point\n# 10% of indices of tatal dataset are 50 indices","a3d5fbb1":"#test_point = np.round((len(df)*10)\/100)","a4452a32":"# Now lets take last points\ntest_ind = int(len(df)-test_point)","5cfdae0b":"test_ind","66fadc1b":"# Now split into Training and Test data\ntrain = df.iloc[:test_ind]\ntest = df.iloc[test_ind:]","1f343175":"train.head(5)","89db40a6":"test.head(5)","eb1d94da":"from sklearn.preprocessing import MinMaxScaler","71aca308":"scaler =  MinMaxScaler()\nscaled_train = scaler.fit_transform(train)\nscaled_test = scaler.transform(test)","c6d85892":"#scaler = MinMaxScaler()","31215552":"# IGNORE WARNING ITS JUST CONVERTING TO FLOATS\n# WE ONLY FIT TO TRAININ DATA, OTHERWISE WE ARE CHEATING ASSUMING INFO ABOUT TEST SET\n#scaler.fit(train)","7c74f668":"# scaled_train = scaler.transform(train)\n# scaled_test = scaler.transform(test)","4c0e57da":"from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator","c2127db1":"# define generator\nlength = 2 # Length of the output sequences (in number of timesteps)\nbatch_size = 1 #Number of timeseries samples in each batch\ngenerator = TimeseriesGenerator(scaled_train, #X_train\n                                scaled_train, #y_train\n                                length = length, \n                                batch_size = batch_size)","d36ed1f3":"len(scaled_train)","a5450699":"len(generator) # n_input = 2","c4011d3b":"# scaled_train","8822286a":"X, y = generator[0]","4e8c926c":"# What does the first batch look like?\nX,y = generator[0]","af0992a0":"X","ba5def9c":"y","5485670e":"print(f'Given the Array: \\n{X.flatten()}')\nprint(\"\\n\")\nprint(f'Predict this y: \\n {y}')","cfcc5018":"# Let's redefine to get 10 steps back and then predict the next step out\nlength = 10 # Length of the output sequences (in number of timesteps)\ngenerator = TimeseriesGenerator(scaled_train, scaled_train, length = length, batch_size = 1)","64e51953":"# What does the first batch look like?\nX,y = generator[0]","8b0cf60d":"print(f'Given the Array: \\n{X.flatten()}')\nprint(\"\\n\")\nprint(f'Predict this y: \\n {y}')","7c66f1f4":"length = 50 # Length of the output sequences (in number of timesteps)\ngenerator = TimeseriesGenerator(scaled_train, scaled_train, length = length, batch_size = 1)","f2b2fecd":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM,SimpleRNN","54279ca5":"# We're only using one feature in our time series\nn_features = 1","5581d340":"# define model\nmodel = Sequential()\n\n# Simple RNN layer\nmodel.add(SimpleRNN(50,input_shape = (length, n_features)))\n\n# Final Prediction\nmodel.add(Dense(1))\n\nmodel.compile(optimizer = 'adam', loss = 'mse')","e08cc056":"model.summary()","b7b203e6":"# fit model\nmodel.fit_generator(generator,epochs = 5)","d154f3c9":"model.history.history.keys()","96c1a72e":"losses = pd.DataFrame(model.history.history)\nlosses.plot()\nplt.show()","fc4bc887":"first_eval_batch = scaled_train[-length:]","c80b71ea":"first_eval_batch[:6]","eda19f83":"first_eval_batch = first_eval_batch.reshape((1, length, n_features))","fc3328d7":"model.predict(first_eval_batch)","86c492fc":"scaled_test[0]","5e0aefb5":"test_predictions = []\n\nfirst_eval_batch = scaled_train[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))","957ac5c8":"current_batch.shape","7a238a92":"current_batch[:,45:,:]","efc83474":"#np.append(current_batch[:,1:,:],[[[99]]],axis=1) #[99] a random value","d0518cd3":"test_predictions = []\n\nfirst_eval_batch = scaled_train[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))\n#current_batch.shape\n#np.append(current_batch[:,1:,:],[[[99]]],axis=1)\n\nfor i in range(len(test)):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    test_predictions.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis = 1)","fe7d21ce":"test_predictions[:6]","3b7fca8a":"scaled_test[:6]","8bb5451e":"true_predictions = scaler.inverse_transform(test_predictions)","be81a16e":"true_predictions[:6]","0d0bd8b3":"test.head(5)","4d3a49eb":"# IGNORE WARNINGS\ntest['Predictions'] = true_predictions","acd7cb8c":"test.plot(figsize = (10,6))\nplt.show()","e35450ae":"from tensorflow.keras.callbacks import EarlyStopping","a616eab6":"early_stop = EarlyStopping(monitor = 'val_loss',patience = 2)","d67737ea":"length = 40\ngenerator = TimeseriesGenerator(scaled_train,scaled_train,\n                               length = length,batch_size = 1)\n\n\nvalidation_generator = TimeseriesGenerator(scaled_test,scaled_test,\n                                          length = length,batch_size = 1)","5df74e82":"# define model\nmodel = Sequential()\n\n# Simple RNN layer\nmodel.add(LSTM(50,input_shape=(length, n_features)))\n\n# Final Prediction\nmodel.add(Dense(1))\n\nmodel.compile(optimizer = 'adam', loss = 'mse')","2f565b6b":"model.fit_generator(generator,epochs=20,\n                   validation_data = validation_generator,\n                   callbacks = [early_stop])","a2102fa1":"test_predictions = []\n\nfirst_eval_batch = scaled_train[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))\n\nfor i in range(len(test)):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    test_predictions.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","a7c0f3cc":"# IGNORE WARNINGS\ntrue_predictions = scaler.inverse_transform(test_predictions)\ntest['LSTM Predictions'] = true_predictions\ntest.plot(figsize = (12,8))\nplt.show()","7eabbd17":"df.plot()\n# plt.plot(x, y)\nplt.show()","5392005f":"full_scaler = MinMaxScaler()\nscaled_full_data = full_scaler.fit_transform(df)","c5e3b2fe":"length = 50 # Length of the output sequences (in number of timesteps)\ngenerator = TimeseriesGenerator(scaled_full_data, scaled_full_data, length = length, batch_size = 1)","52f40309":"model = Sequential()\nmodel.add(LSTM(50, input_shape = (length, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer = 'adam', loss = 'mse')\nmodel.fit_generator(generator, epochs = 6)","d8e89e3a":"forecast = []\n\nfirst_eval_batch = scaled_full_data[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))\n\nfor i in range(100):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    forecast.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","2743d0b4":"forecast = scaler.inverse_transform(forecast)","1bbe0e9f":"forecast[:6]","cc6e24cd":"df.head(3)","567edb27":"len(forecast)","033e23b6":"100*0.1 #0.1 is the step size in original data","5844d67c":"50.1+10\n","37b5d4ea":"#np.arange(start, stop, step)\n\nforecast_index = np.arange(50.1,60.1,step=0.1)\n\n#50.1=is starting point if we have to plot on original data \n#since 50 is the last point on original data with '0.1' step size, then add 5.0 (as calculated in 50*0.1) for stope point, \n#step size is 0.1","5625e7ed":"len(forecast_index)","d583de12":"plt.plot(df.index,df['Sine'])\nplt.plot(forecast_index,forecast)\nplt.show()","7954eaa9":"### Create the Model","5b2fefa5":"Now let's put this logic in a for loop to predict into the future for the entire test range.\n\n----","526ea78e":"## Scale Data","c159364c":"## Evaluate on Test Data","aedecec4":"# Time Series Generator\n\nThis class takes in a sequence of data-points gathered at\nequal intervals, along with time series parameters such as\nstride, length of history, etc., to produce batches for\ntraining\/validation.\n\n#### Arguments\n    data: Indexable generator (such as list or Numpy array)\n        containing consecutive data points (timesteps).\n        The data should be at 2D, and axis 0 is expected\n        to be the time dimension.\n    targets: Targets corresponding to timesteps in `data`.\n        It should have same length as `data`.\n    length: Length of the output sequences (in number of timesteps).\n    sampling_rate: Period between successive individual timesteps\n        within sequences. For rate `r`, timesteps\n        `data[i]`, `data[i-r]`, ... `data[i - length]`\n        are used for create a sample sequence.\n    stride: Period between successive output sequences.\n        For stride `s`, consecutive output samples would\n        be centered around `data[i]`, `data[i+s]`, `data[i+2*s]`, etc.\n    start_index: Data points earlier than `start_index` will not be used\n        in the output sequences. This is useful to reserve part of the\n        data for test or validation.\n    end_index: Data points later than `end_index` will not be used\n        in the output sequences. This is useful to reserve part of the\n        data for test or validation.\n    shuffle: Whether to shuffle output samples,\n        or instead draw them in chronological order.\n    reverse: Boolean: if `true`, timesteps in each output sample will be\n        in reverse chronological order.\n    batch_size: Number of timeseries samples in each batch\n        (except maybe the last one).","9bdf262d":"#Let's turn this into a DataFrame","59ba25ad":"Now you will be able to edit the length so that it makes sense for your time series!","7776157a":"## Adding in Early Stopping and Validation Generator","9bad518c":"# Forecasting\n\nForecast into unknown range. We should first utilize all our data, since we are now forecasting! Meaning beyond 50 (data we took upto 50 earlier, see df.plot()","bbf3e02e":"# LSTMS","28e1c059":"## Train Test Split\n\nNote! This is very different from our usual test\/train split methodology!","6324cc4f":"**NOTE: PAY CLOSE ATTENTION HERE TO WHAT IS BEING OUTPUTED AND IN WHAT DIMENSIONS. ADD YOUR OWN PRINT() STATEMENTS TO SEE WHAT IS TRULY GOING ON!!**","a9a30371":"## Inverse Transformations and Compare","c3b6eed0":"# Sine Wave Example","fb5cfe9b":"## Data\n\nLet's use Numpy to create a simple sine wave."}}