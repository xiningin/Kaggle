{"cell_type":{"47b0490f":"code","681b82b0":"code","24dc6b25":"code","ad0e4ff4":"code","641f3a91":"code","f990628d":"code","3af4074e":"code","1c491e50":"code","95d23c7f":"code","528401f5":"code","594b0d06":"code","76ebe431":"code","a50be4f2":"code","21d42247":"code","a4b3c2e8":"code","68b2ac00":"code","534f1f95":"code","fa6b60f7":"code","8979440c":"code","2cf45fbe":"code","608ec9e0":"code","9450424a":"code","bffe0dd2":"code","3d62779c":"code","ddd0a0c3":"code","df7efb85":"code","bbf179e9":"code","354a4bfa":"code","7b9aac38":"code","ebe488db":"code","03335679":"code","c9a835fb":"code","db3a7ad2":"code","932720f4":"code","91f76d25":"code","a7d466b7":"code","43631ef2":"code","5a668b3e":"code","0a743d64":"code","edf40264":"code","1121b16a":"code","b02449f7":"code","e7d0603f":"code","c194bf32":"code","7ee3a265":"code","6b810dc4":"code","c64adb2e":"code","6856b379":"code","33685b30":"code","7cc9f4bf":"code","c4da598e":"code","a6b10647":"code","6b5f2a7b":"code","50197ec4":"code","03888fed":"code","e7aaadee":"code","8b4d4200":"code","e0266ac4":"code","6d4ee98f":"code","be00a513":"code","be223602":"code","82a62da3":"code","13614fbd":"code","5269d713":"code","15434a7b":"code","a80ad7d7":"code","8d74c3b7":"code","f5d6ab3c":"code","0e87bdd0":"code","c605b0fe":"code","94331083":"code","bf98e058":"code","62cf93e7":"code","7437caa1":"code","a7088b55":"code","eb72d694":"markdown","4cac9245":"markdown","97300bb7":"markdown","c6f38ad2":"markdown","16195a70":"markdown","839eb0d8":"markdown","d24cad3a":"markdown","3da8b3b4":"markdown","b115d59b":"markdown","2d34c83a":"markdown","d6f17742":"markdown","5e3daa1a":"markdown","f8b15c61":"markdown","56012f69":"markdown","b7fbe401":"markdown","1ebbcc24":"markdown","42d1b921":"markdown","03e061a5":"markdown","bf465885":"markdown","997026c1":"markdown","8f5afdf2":"markdown","be66a308":"markdown","db002eb6":"markdown","cd07e811":"markdown","f5511812":"markdown","eb00520a":"markdown","563f77e9":"markdown","6cba8e61":"markdown","1910b7c3":"markdown","8586bc0a":"markdown","5bf8cc45":"markdown","4317357b":"markdown","91d2340e":"markdown","dcee6a90":"markdown","676582c6":"markdown","ec1ad27f":"markdown","22ca86a2":"markdown","b9fdd491":"markdown","1925dc56":"markdown","45fbe942":"markdown","c4714797":"markdown","87e1089a":"markdown","19abeeeb":"markdown","27ccfb0d":"markdown","64737463":"markdown","ca06073e":"markdown","0be5bf8d":"markdown","99430b15":"markdown","6f03e7f8":"markdown","fe625281":"markdown","374de6e0":"markdown","cab6786b":"markdown"},"source":{"47b0490f":"pip install emoji","681b82b0":"##################################################\n# Imports\n##################################################\n\nimport emoji\nimport numpy as np\nimport cv2\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport re\nimport spacy\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras import backend\nfrom keras.initializers import Constant\n\n\n##################################################\n# Params\n##################################################\n\nDATA_BASE_FOLDER = '\/kaggle\/input\/emojify-challenge'\n\n\n##################################################\n# Utils\n##################################################\n\ndef label_to_emoji(label):\n    \"\"\"\n    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n    \"\"\"\n    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)","24dc6b25":"#### Function for plotting confusion matrix ######\n\ndef plot_cf_matrix(y_true, y_pred):\n  cf_matrix= confusion_matrix(y_true, y_pred);\n  heatmap = sns.heatmap(cf_matrix, annot=True, cmap='YlOrRd')\n  plt.xlabel(\"Predicted value\") \n  plt.ylabel(\"True value\") \n  return plt.show()\n","ad0e4ff4":"###################################################\n# Plots accuracy and loss in Train and Validation #\n###################################################\n\ndef plot_accuracy_loss(history_l, K, parameters = None):\n    plt.figure(figsize=(20, 7))\n    colors = [\"red\", \"blue\", \"orange\", \"green\", \"cyan\",\"brown\",\"grey\"]\n    #Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.title('Train and Validation Accuracy by Epochs')\n    plt.xlabel('Epochs', fontweight='bold', fontsize=15)\n    plt.ylabel('Accuracy', fontweight='bold', fontsize=15)\n    if parameters == None:\n      for i in range(0, K):\n          plt.plot(history_l[i].history['accuracy'], label = 'Train Accuracy Fold ' + str(i+1), color = colors[i])\n          plt.plot(history_l[i].history['val_accuracy'], label = 'Validation Accuracy Fold ' + str(i+1), color = colors[i], linestyle = \":\")\n          plt.legend(fontsize='small')\n    else:\n      for i in range(0, K):\n          plt.plot(history_l[i].history['accuracy'], label = 'Train Accuracy ' + str(parameters[i]), color = colors[i])\n          plt.plot(history_l[i].history['val_accuracy'], label = 'Validation Accuracy ' + str(parameters[i]), color = colors[i], linestyle = \":\")\n          plt.legend(fontsize='medium')\n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.title('Train and Validation Loss by Epochs')\n    plt.xlabel('Epochs', fontweight='bold', fontsize=15)\n    plt.ylabel('Loss', fontweight='bold', fontsize=15)\n    if parameters == None:\n      for i in range(0, K):\n          plt.plot(history_l[i].history['loss'], label = 'Train Accuracy Loss Fold' + str(i+1), color = colors[i])\n          plt.plot(history_l[i].history['val_loss'], label = 'Validation Accuracy Loss Fold' + str(i+1), color = colors[i], linestyle = \":\")\n          plt.legend(fontsize='small')\n    else:\n      for i in range(0, K):\n          plt.plot(history_l[i].history['loss'], label = 'Train Accuracy Loss ' + str(parameters[i]), color = colors[i])\n          plt.plot(history_l[i].history['val_loss'], label = 'Validation Accuracy Loss ' + str(parameters[i]), color = colors[i], linestyle = \":\")\n          plt.legend(fontsize='medium')\n    plt.show()","641f3a91":"\n##################################################\n# Load dataset\n##################################################\n\ndf_train = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'train.csv'))\ny_train = df_train['class']\ndf_validation = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'validation.csv'))\ny_validation = df_validation['class']\nprint(df_validation.columns)\ndf_test = pd.read_csv(os.path.join('\/kaggle\/input\/emojify', 'test.csv'))\ny_test = df_test['class']\n\nemoji_dictionary = {\n    '0': '\\u2764\\uFE0F',\n    '1': ':baseball:',\n    '2': ':smile:',\n    '3': ':disappointed:',\n    '4': ':fork_and_knife:'\n}\n\n# See some data examples\nprint('EXAMPLES:\\n####################')\nfor idx in range(10):\n  print(f'{df_train[\"phrase\"][idx]} -> {label_to_emoji(y_train[idx])}')\n\nprint(df_train.head()) \n","f990628d":"# Load phrase representation gloVe\nx_train = np.load(\n    os.path.join(DATA_BASE_FOLDER, \n                 'train.npy')).reshape(len(df_train), -1)\nx_validation = np.load(\n    os.path.join(DATA_BASE_FOLDER, \n                 'validation.npy')).reshape(len(df_validation), -1)\n\n                \nprint(f'Word embedding size: {x_train.shape[-1]}')\n","3af4074e":"# pooled dati per GloVe\nnew_x_pooled = np.vstack((x_train,x_validation))\nnew_y_pooled = np.hstack((y_train, y_validation))\n\n# pooled string data\ndf_new_train = np.vstack((df_train,df_validation))         \ndf_string_train = [\"\".join(x[0]) for x in df_new_train]\ndf_string_test = [\"\".join(x[0]) for x in pd.DataFrame.to_numpy(df_test)]\n","1c491e50":"# words occurrences\nword_frequency = {}\n\nnew_str = ' '.join(df_string_train);\nfor word in new_str.lower().split():\n    if word not in word_frequency:\n      word_frequency[word] = 0    \n    word_frequency[word] += 1\n\nnewd = dict(sorted(word_frequency.items(), key=lambda item: item[1], reverse=True))\nprint(len(newd))\n\n\n# plot \nplt.plot(list(newd.keys()),list(newd.values()))\nplt.xticks(list(newd.keys())[::10], rotation = \"vertical\");\nplt.title(\"Word frequency\");","95d23c7f":"sns.countplot(new_y_pooled).set_title('Frequency of target variables in pooled dataset')\nplt.xlabel('Class');","528401f5":"vect = CountVectorizer().fit(df_string_train)\nstandCV_comp = vect.transform(df_string_train)","594b0d06":"# Using Stopwords\nprint(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\nprint(\"Every 10th stopwords: \\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))","76ebe431":"#the comparison between using stop_words and not\nvect_sw = CountVectorizer(min_df=1, stop_words='english').fit(df_string_train)\nstopw_x_train = vect_sw.transform(df_string_train)\nprint('X_train shape using Stop words: {}'.format(stopw_x_train.shape[1]))\nprint('Standard CountVectorizer shape: {}'.format(standCV_comp.shape[1]))\nprint('Thank to Stopwords we discard {} features (words).'.format(standCV_comp.shape[1]-stopw_x_train.shape[1]))","a50be4f2":"#Convert each document into a list of tokens using the tokenizer parameter\n#regexp used in CounterVector\nregexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n\nen_nlp = spacy.load('en')\nold_tokenizer = en_nlp.tokenizer\n#replace the tokenizer with regexp\nen_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(regexp.findall(string))\n#create a custom tokenizer using the spacy document pipeline\ndef our_tokenizer(document):\n  doc_spacy = en_nlp(document)\n  return [token.lemma_ for token in doc_spacy]\n\n#define a count vectorizer with the custom tokenizer to do Lemmatization\nlemma_vect = CountVectorizer(tokenizer = our_tokenizer, min_df = 1, stop_words = 'english')\nlemma_vect_tfidf = TfidfVectorizer(tokenizer = our_tokenizer, min_df = 1, stop_words= 'english')","21d42247":"x_train_lemma = lemma_vect.fit_transform(df_string_train);\nx_train_lemma_tfidf = lemma_vect_tfidf.fit_transform(df_string_train);\n#standard CountVectorizer for comparison\n\nprint('Standard CountVectorizer shape: {}'.format(standCV_comp.shape))\nprint('X_train_lemma shape: {}'.format(x_train_lemma.shape))\nprint('Thank to Lemmatization we discard {} features'.format(standCV_comp.shape[1]-x_train_lemma.shape[1]))\n","a4b3c2e8":"#\npipe_bof_lr = make_pipeline(CountVectorizer(min_df = 1), LogisticRegression())\n#\nparam_grid = {'logisticregression__C': [0.01, 0.1, 1, 5, 10, 50,100],\n              'countvectorizer__ngram_range': [(1,1),(1,2)],\n              'countvectorizer__stop_words':['english', None]}\ngrid_bof_lr = GridSearchCV(pipe_bof_lr, param_grid , cv = 5)\ngrid_bof_lr.fit(df_string_train, new_y_pooled)\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_lr.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_lr.best_params_))","68b2ac00":"param_grid = {'C': [0.01, 0.1, 1, 5, 10, 50,100]}\ngrid_bof_lr = GridSearchCV(LogisticRegression(), param_grid= param_grid, cv = 5)\ngrid_bof_lr.fit(x_train_lemma, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_lr.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_lr.best_params_))","534f1f95":"pipe_tfidf_lr = make_pipeline(TfidfVectorizer(min_df = 1), LogisticRegression())\n#\nparam_grid = {'logisticregression__C': [0.1, 1, 5, 10, 25, 50,100],\n              'tfidfvectorizer__ngram_range': [(1,1),(1,2)],\n              'tfidfvectorizer__stop_words':['english', None]}\ngrid_tfidf_lr = GridSearchCV(pipe_tfidf_lr, param_grid , cv = 5)\ngrid_tfidf_lr.fit(df_string_train, new_y_pooled)\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_tfidf_lr.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_lr.best_params_))","fa6b60f7":"param_grid = {'C': [0.01, 0.1, 1, 5, 10, 50,100]}\ngrid_tfidf_lr = GridSearchCV(LogisticRegression(), param_grid= param_grid, cv = 5)\ngrid_tfidf_lr.fit(x_train_lemma_tfidf, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_tfidf_lr.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_lr.best_params_))","8979440c":"#\nparam_grid = {'logisticregression__C': [0.01, 0.05, 0.1, 0.5, 1]}\n#pipe_glove_lr = make_pipeline(MinMaxScaler(feature_range= (np.min(new_x_pooled),np.max(new_x_pooled))),LogisticRegression(max_iter=1000))\npipe_glove_lr = make_pipeline(LogisticRegression(max_iter=1000))\n#pipe_glove_lr = make_pipeline(StandardScaler(),LogisticRegression(max_iter=1000))\n\ngrid_glove_lr = GridSearchCV(pipe_glove_lr, param_grid , cv = 5)\ngrid_glove_lr.fit(new_x_pooled, new_y_pooled)\nprint(\"Best cross-validation score: {:.4f}\".format(grid_glove_lr.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_glove_lr.best_params_))\n\n","2cf45fbe":"vect_lr = TfidfVectorizer(min_df = 1, stop_words = 'english', ngram_range = (1, 1)).fit(df_string_train)\nx_train_vect_lr = vect_lr.transform(df_string_train)\nx_test_vect_lr =  vect_lr.transform(df_string_test)\nbest_lr = LogisticRegression(C = 10).fit(x_train_vect_lr, new_y_pooled)\ny_pred_lr = best_lr.predict(x_test_vect_lr)\n#accuracy(y_pred_lr,y_test)\nprint(\"Best accuracy score for Logistic Regression: {:.4f}\".format(best_lr.score(x_test_vect_lr,y_test)))\nprint(classification_report(y_test, y_pred_lr ))\nplot_cf_matrix(y_test, y_pred_lr)\n\n# Using lemmatization --> ending up with the same accuracy score\n# x_train_lemma_lr = lemma_vect_tfidf.fit_transform(df_string_train)\n# x_test_lemma_lr = lemma_vect_tfidf.transform(df_string_test)\n# best_lr = LogisticRegression(C = 10).fit(x_train_lemma_lr, new_y_pooled)\n# y_pred_lr = best_lr.predict(x_test_lemma_lr)\n# print(\"Best accuracy score for Logistic Regression: {:.4f}\".format(best_lr.score(x_test_lemma_lr,y_test)))\n# print(classification_report(y_test, y_pred_lr ))\n# plot_cf_matrix(y_test, y_pred_lr)","608ec9e0":"\npipe_bof_rf = make_pipeline(CountVectorizer(min_df = 1), RandomForestClassifier())\n\nparam_grid = {'randomforestclassifier__n_estimators': [500],\n              'randomforestclassifier__max_features': [15,20,30],\n              'countvectorizer__ngram_range': [(1,1),(1,2)],\n              'countvectorizer__stop_words':['english', None]}\ngrid_bof_rf = GridSearchCV(pipe_bof_rf, param_grid , cv = 5)\ngrid_bof_rf.fit(df_string_train, new_y_pooled)\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_rf.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_rf.best_params_))","9450424a":"param_grid = {'n_estimators': [500],\n              'max_features': [20,25 ,30]}\ngrid_bof_rf = GridSearchCV(RandomForestClassifier(), param_grid= param_grid, cv = 5)\ngrid_bof_rf.fit(x_train_lemma, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_rf.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_rf.best_params_))","bffe0dd2":"pipe_tfidf_rf = make_pipeline(TfidfVectorizer(min_df = 1), RandomForestClassifier())\n#\nparam_grid = {'randomforestclassifier__n_estimators': [500],\n              'randomforestclassifier__max_features': [15,20,30],\n              'tfidfvectorizer__ngram_range': [(1,1),(1,2)],\n              'tfidfvectorizer__stop_words':['english', None]}\ngrid_tfidf_rf = GridSearchCV(pipe_tfidf_rf, param_grid , cv = 5)\ngrid_tfidf_rf.fit(df_string_train, new_y_pooled)\nprint(\"Best cross-validation score: {:.7f}\".format(grid_tfidf_rf.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_rf.best_params_))    ","3d62779c":"param_grid = {'n_estimators': [500],\n              'max_features': [20,25 ,30]}\ngrid_tfidf_rf = GridSearchCV(RandomForestClassifier(), param_grid= param_grid, cv = 5)\ngrid_tfidf_rf.fit(x_train_lemma_tfidf, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_tfidf_rf.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_rf.best_params_))","ddd0a0c3":"#  Decision Tree model don't need rescaling of the data\nparam_grid = {'randomforestclassifier__n_estimators': [500],\n              'randomforestclassifier__max_features': [20,25 ,30]}\npipe_glove_rf = make_pipeline(RandomForestClassifier())\ngrid_glove_rf = GridSearchCV(pipe_glove_rf, param_grid , cv = 5)\ngrid_glove_rf.fit(new_x_pooled, new_y_pooled)\nprint(\"Best cross-validation score: {:.4f}\".format(grid_glove_rf.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_glove_rf.best_params_))\n","df7efb85":"vect_rf = CountVectorizer(min_df = 1, stop_words = 'english', ngram_range = (1, 1)).fit(df_string_train)\nx_train_vect_rf = vect_rf.transform(df_string_train)\nx_test_vect_rf =  vect_rf.transform(df_string_test)\nbest_rf = RandomForestClassifier(n_estimators=500,max_features=15).fit(x_train_vect_rf, new_y_pooled)\ny_pred_rf = best_rf.predict(x_test_vect_rf)\nprint(\"Best accuracy score for Random Forest: {:.4f}\".format(best_rf.score(x_test_vect_rf,y_test)))\nprint(classification_report(y_test, y_pred_rf ))\nplot_cf_matrix(y_test, y_pred_rf)\n","bbf179e9":"pipe_bof_knn = make_pipeline(CountVectorizer(min_df = 1), KNeighborsClassifier())\n\nparam_grid = {'kneighborsclassifier__n_neighbors':[2,3,4,5,7,9],\n              'kneighborsclassifier__metric': ['minkowski', 'euclidean'],\n              'countvectorizer__ngram_range': [(1,1),(1,2)],\n              'countvectorizer__stop_words':['english', None]}\ngrid_bof_knn = GridSearchCV(pipe_bof_knn, param_grid , cv = 5)\ngrid_bof_knn.fit(df_string_train, new_y_pooled)\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_knn.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_knn.best_params_))","354a4bfa":"param_grid = {'n_neighbors':[2,3,4,5,7,9],\n             'metric': ['minkowski', 'euclidean']}\ngrid_bof_knn = GridSearchCV(KNeighborsClassifier(), param_grid= param_grid, cv = 5)\ngrid_bof_knn.fit(x_train_lemma, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_knn.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_knn.best_params_))","7b9aac38":"pipe_tfidf_knn = make_pipeline(TfidfVectorizer(min_df = 1), KNeighborsClassifier())\n#\nparam_grid = {'kneighborsclassifier__n_neighbors':[2,3,4,5,7,9],\n              'kneighborsclassifier__metric': ['minkowski', 'euclidean'],\n              'tfidfvectorizer__ngram_range': [(1,1),(1,2)],\n              'tfidfvectorizer__stop_words':['english', None]}\ngrid_tfidf_knn = GridSearchCV(pipe_tfidf_knn, param_grid , cv = 5)\ngrid_tfidf_knn.fit(df_string_train, new_y_pooled)\nprint(\"Best cross-validation score: {:.7f}\".format(grid_tfidf_knn.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_knn.best_params_))   ","ebe488db":"param_grid = {'n_neighbors':[2,3,4,5,7,9],\n             'metric': ['minkowski', 'euclidean']}\ngrid_tfidf_knn = GridSearchCV(KNeighborsClassifier(), param_grid= param_grid, cv = 5)\ngrid_tfidf_knn.fit(x_train_lemma_tfidf, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_tfidf_knn.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_knn.best_params_))","03335679":"param_grid = {'kneighborsclassifier__n_neighbors':[2,3,4,5,7,9],\n             'kneighborsclassifier__metric': ['minkowski', 'euclidean']}\npipe_glove_knn= make_pipeline(MinMaxScaler(),KNeighborsClassifier())\n#pipe_glove_knn = make_pipeline(KNeighborsClassifier())\n#pipe_glove_knn = make_pipeline(StandardScaler(),KNeighborsClassifier())\n\ngrid_glove_knn = GridSearchCV(pipe_glove_knn, param_grid , cv = 5)\ngrid_glove_knn.fit(new_x_pooled, new_y_pooled)\nprint(\"Best cross-validation score: {:.4f}\".format(grid_glove_knn.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_glove_knn.best_params_))","c9a835fb":"vect_knn = TfidfVectorizer(min_df = 1, stop_words = 'english', ngram_range = (1, 1)).fit(df_string_train)\n#vect_knn = lemma_vect_tfidf.fit(df_string_train)\nx_train_vect_knn = vect_knn.transform(df_string_train)\nx_test_vect_knn =  vect_knn.transform(df_string_test)\nbest_knn = KNeighborsClassifier(metric = 'minkowski', n_neighbors =  3).fit(x_train_vect_knn, new_y_pooled)\n#best_knn = KNeighborsClassifier(metric = 'minkowski', n_neighbors =  4).fit(x_train_vect_knn, new_y_pooled)\n\ny_pred_knn = best_knn.predict(x_test_vect_knn)\nprint(\"Best accuracy score for K-NN: {:.4f}\".format(best_knn.score(x_test_vect_knn,y_test)))\nprint(classification_report(y_test, y_pred_knn))\nplot_cf_matrix(y_test, y_pred_knn)\n","db3a7ad2":"pipe_bof_svm = make_pipeline(CountVectorizer(min_df = 1), SVC())\n\nparam_grid = {'svc__C':[0.1,1,10,100, 1000],\n             'svc__gamma':[0.001,0.01,0.1,1,10,100],\n             'svc__kernel': ['linear', 'rbf'],\n              'countvectorizer__ngram_range': [(1,1),(1,2)],\n              'countvectorizer__stop_words':['english', None]}\ngrid_bof_svm = GridSearchCV(pipe_bof_svm, param_grid , cv = 5)\ngrid_bof_svm.fit(df_string_train, new_y_pooled)\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_svm.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_svm.best_params_))","932720f4":"param_grid = {'C':[0.01,0.1,1,10,100],\n             'gamma':[0.001,0.01,0.1,1,10,100],\n              'kernel': ['linear', 'rbf']}\ngrid_bof_svm = GridSearchCV(SVC(), param_grid= param_grid, cv = 5)\ngrid_bof_svm.fit(x_train_lemma, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_svm.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_svm.best_params_))","91f76d25":"pipe_tfidf_svm = make_pipeline(TfidfVectorizer(min_df = 1), SVC())\n#\nparam_grid = {'svc__C':[1,10,25,50],\n             'svc__gamma':[0.01,0.05,0.1,0.5,1],\n              'svc__kernel': ['linear', 'rbf'],\n              'tfidfvectorizer__ngram_range': [(1,1),(1,2)],\n              'tfidfvectorizer__stop_words':['english', None]}\ngrid_tfidf_svm = GridSearchCV(pipe_tfidf_svm, param_grid , cv = 5)\ngrid_tfidf_svm.fit(df_string_train, new_y_pooled)\nprint(\"Best cross-validation score: {:.4f}\".format(grid_tfidf_svm.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_svm.best_params_))","a7d466b7":"param_grid = {'C':[0.01,0.1,1,10,100],\n             'gamma':[0.001,0.01,0.1,1,10,100],\n              'kernel': ['linear', 'rbf'],}\ngrid_tfidf_svm = GridSearchCV(SVC(), param_grid= param_grid, cv = 5)\ngrid_tfidf_svm.fit(x_train_lemma_tfidf, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_tfidf_svm.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_svm.best_params_))","43631ef2":"param_grid = {'svc__C':[0.01,0.1,1,10,50,100],\n             'svc__gamma':[0.001,0.005,0.01,0.1,1,10],\n              'svc__kernel': ['linear', 'rbf']}\n#pipe_glove_svm = make_pipeline(MinMaxScaler(),SVC())\n#pipe_glove_svm = make_pipeline(SVC())\npipe_glove_svm = make_pipeline(StandardScaler(),SVC())\n\ngrid_glove_svm = GridSearchCV(pipe_glove_svm, param_grid , cv = 5)\ngrid_glove_svm.fit(new_x_pooled, new_y_pooled)\nprint(\"Best cross-validation score: {:.4f}\".format(grid_glove_svm.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_glove_svm.best_params_))","5a668b3e":"vect_svm = TfidfVectorizer(min_df = 1, stop_words = 'english', ngram_range = (1, 2)).fit(df_string_train)\nx_train_vect_svm = vect_svm.transform(df_string_train)\nx_test_vect_svm =  vect_svm.transform(df_string_test)\nbest_svm = SVC(C = 10, gamma = 0.1, kernel = 'rbf').fit(x_train_vect_svm, new_y_pooled)\ny_pred_svm = best_svm.predict(x_test_vect_svm)\n# can't increase the result even if using lemmatization\nprint(\"Best accuracy score for Supported Vector Machinie: {:.4f}\".format(best_svm.score(x_test_vect_svm,y_test)))\nprint(classification_report(y_test, y_pred_svm ))\nplot_cf_matrix(y_test, y_pred_svm)","0a743d64":"# one hot-encoding for train and validation\nfrom keras.utils.np_utils import to_categorical\n\n\ncategorical_y_train = to_categorical(y_train)\ncategorical_y_validation = to_categorical(y_validation)\n\n# merge one hot-encoding\ntargets = np.concatenate((categorical_y_train, categorical_y_validation), axis = 0)","edf40264":"parameters = ['relu', 'tanh', 'sigmoid']\nevolution_activation = []\nfor activation in parameters:\n  naive_model = Sequential()\n  naive_model.add(Dense(64, activation = activation, input_dim=250))\n  naive_model.add(Dropout(0.5))\n  naive_model.add(Dense(5, activation='softmax'))\n  naive_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  history = naive_model.fit(x_train, categorical_y_train, batch_size=32, epochs=75,validation_data=(x_validation, categorical_y_validation))\n  evolution_activation.append(history)\n\n","1121b16a":"plot_accuracy_loss(evolution_activation, len(parameters), parameters)","b02449f7":"parameters = [0.001, 0.25, 0.35, 0.5]\nevolution_dropout = []\nfor dropout in parameters:\n  naive_model = Sequential()\n  naive_model.add(Dense(64, activation = \"relu\", input_dim=250))\n  naive_model.add(Dropout(dropout))\n  naive_model.add(Dense(5, activation='softmax'))\n  naive_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  history = naive_model.fit(x_train, categorical_y_train, batch_size=32, epochs=75,validation_data=(x_validation, categorical_y_validation))\n  evolution_dropout.append(history)\n\n","e7d0603f":"plot_accuracy_loss(evolution_dropout, len(parameters), parameters)","c194bf32":"\ndef get_model_name(k):\n    return 'model_'+str(k)+'.h5'\n\nK = 7   # numbers of folds\nskf = StratifiedKFold(n_splits = K, random_state = 7, shuffle = True) # creating the stratified k-fold cross validation\n","7ee3a265":"validation_accuracy = []\nvalidation_loss = []\nk_fold_evolution = []\n\nsave_dir = '\/saved_models\/'\nfold_var = 1\n\nfor train_index, val_index in skf.split(np.zeros(132),new_y_pooled):\n  training_data = new_x_pooled[train_index]\n  training_targets = targets[train_index]\n  validation_data = new_x_pooled[val_index]\n  validation_targets = targets[val_index]\n  # create and compile new model\n  naive_model = Sequential()\n  naive_model.add(Dense(64, activation = \"relu\", input_dim=250))\n  naive_model.add(Dropout(0.35))\n  naive_model.add(Dense(5, activation='softmax'))\n  naive_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n\n  print('------------------------------------------------------------------------')\n  print('------------------------ history for fold {} ----------------------------'.format(fold_var))\n  print('------------------------------------------------------------------------')\n  history = naive_model.fit(training_data, training_targets, batch_size=32, epochs=75,validation_data=(validation_data, validation_targets));\n\n  # CREATE CALLBACKS\n  checkpoint = ModelCheckpoint(save_dir+get_model_name(fold_var), \n\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n  callbacks_list = [checkpoint]\n\t# This saves the best model\n  # Generate generalization metrics\t\n  results = naive_model.evaluate(validation_data,validation_targets,verbose=0)\n  results = dict(zip(naive_model.metrics_names,results))\n  validation_accuracy.append(results['accuracy'])\n  validation_loss.append(results['loss'])\n  k_fold_evolution.append(history)\n\t\n  backend.clear_session()\n  fold_var += 1 # Increase fold number\n\n    \n\n","6b810dc4":"# print useful result of stratified k-fold cross validation\nprint('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(validation_accuracy)*100}% (+- {np.std(validation_accuracy)*100}%)')\nprint(f'> Loss: {np.mean(validation_loss)}\\n\\n')\n# plotting resuls of stratified k-fold cross validation\nplot_accuracy_loss(k_fold_evolution,K)\n\n","c64adb2e":"first_layer_dim = [128,64]\nsecond_layer_dim = [64,32,16]\nevolution_architecture = []\nfor dim_1 in first_layer_dim:\n  for dim_2 in second_layer_dim:\n    dnn_model = Sequential()\n    dnn_model.add(Dense(dim_1, activation = \"relu\", input_dim=250))\n    dnn_model.add(Dropout(0.5))\n    dnn_model.add(Dense(dim_2, activation = \"relu\"))\n    dnn_model.add(Dense(5, activation='softmax'))\n    dnn_model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n    history = dnn_model.fit(x_train, categorical_y_train, batch_size=32, epochs=75,validation_data=(x_validation, categorical_y_validation))\n    evolution_architecture.append(history)","6856b379":"plot_accuracy_loss(evolution_architecture,6)","33685b30":"parameters = ['relu', 'tanh', 'sigmoid']\nevolution_activation = []\nfor activation in parameters:\n  dnn_model = Sequential()\n  dnn_model.add(Dense(128, activation = activation, input_dim=250))\n  dnn_model.add(Dropout(0.5))\n  dnn_model.add(Dense(32, activation = activation))\n  dnn_model.add(Dense(5, activation='softmax'))\n  dnn_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  history = dnn_model.fit(x_train, categorical_y_train, batch_size=32, epochs=75,validation_data=(x_validation, categorical_y_validation))\n  evolution_activation.append(history)\n\n","7cc9f4bf":"plot_accuracy_loss(evolution_activation, len(parameters), parameters)","c4da598e":"parameters = [0.001, 0.25, 0.35, 0.5]\nevolution_dropout = []\nfor dropout in parameters:\n  dnn_model = Sequential()\n  dnn_model.add(Dense(128, activation = \"sigmoid\", input_dim=250))\n  dnn_model.add(Dropout(dropout))\n  dnn_model.add(Dense(32, activation = \"sigmoid\"))\n  dnn_model.add(Dense(5, activation='softmax'))\n  dnn_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  history = dnn_model.fit(x_train, categorical_y_train, batch_size=32, epochs=75,validation_data=(x_validation, categorical_y_validation))\n  evolution_dropout.append(history)\n\n","a6b10647":"plot_accuracy_loss(evolution_dropout, len(parameters), parameters)","6b5f2a7b":"def get_model_name(k):\n    return 'model_'+str(k)+'.h5'\n\nK = 7   # numbers of folds\nskf = StratifiedKFold(n_splits = K, random_state = 7, shuffle = True) # creating the stratified k-fold cross validation\n","50197ec4":"validation_accuracy = []\nvalidation_loss = []\nk_fold_evolution = []\n\nsave_dir = '\/saved_models\/'\nfold_var = 1\n\nfor train_index, val_index in skf.split(np.zeros(132),new_y_pooled):\n  training_data = new_x_pooled[train_index]\n  training_targets = targets[train_index]\n  validation_data = new_x_pooled[val_index]\n  validation_targets = targets[val_index]\n  # create and compile new model\n  dnn_model = Sequential()\n  dnn_model.add(Dense(128, activation = \"sigmoid\", input_dim=250))\n  dnn_model.add(Dropout(0.25))\n  dnn_model.add(Dense(32, activation = \"sigmoid\"))\n  dnn_model.add(Dense(5, activation='softmax'))\n  dnn_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  print('------------------------------------------------------------------------')\n  print('------------------------ history for fold {} ----------------------------'.format(fold_var))\n  print('------------------------------------------------------------------------')\n  history = dnn_model.fit(training_data, training_targets, batch_size=32, epochs=75,validation_data=(validation_data, validation_targets));\n\n  # CREATE CALLBACKS\n  checkpoint = ModelCheckpoint(save_dir+get_model_name(fold_var), \n\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n  callbacks_list = [checkpoint]\n\t# This saves the best model\n  # Generate generalization metrics\t\n  results = dnn_model.evaluate(validation_data,validation_targets,verbose=0)\n  results = dict(zip(dnn_model.metrics_names,results))\n  validation_accuracy.append(results['accuracy'])\n  validation_loss.append(results['loss'])\n  k_fold_evolution.append(history)\n\t\n  backend.clear_session()\n  fold_var += 1 # Increase fold number\n\n    \n","03888fed":"# print useful result of stratified k-fold cross validation\nprint('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(validation_accuracy)*100}% (+- {np.std(validation_accuracy)*100}%)')\nprint(f'> Loss: {np.mean(validation_loss)}\\n\\n')\n# plotting resuls of stratified k-fold cross validation\nplot_accuracy_loss(k_fold_evolution,K)","e7aaadee":"# creating the matrix of total words with gloVe values\nindex_words = {}\nembeddings_index = {}\n\ni = 0\nfn = open('..\/input\/glove-global-vectors-for-word-representation\/glove.twitter.27B.25d.txt','r')\n\nword_arr = set()\nfor line in fn:\n    values = line.split()\n    word = values[0]\n    word_arr.add(word)\n    w_value = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = w_value\nfn.close()\nprint('Library composed of {} word vectors.'.format(len(embeddings_index)))\n\nfor w in sorted(word_arr):\n    index_words[w] = i\n    i += 1\n\n","8b4d4200":"# creating the matrix of our sentences populated with gloVe values\ndef index_sentences(txt, idx_word, maxlen):\n  n_rows = txt.shape[0]\n  sentence_indices = np.zeros((n_rows, maxlen))\n\n  for row in range(n_rows):\n      sentence_words = txt[row].lower().split()\n      col_sentence = 0\n      for w in sentence_words:\n          sentence_indices[row, col_sentence] = idx_word[w]\n          col_sentence += 1\n  return sentence_indices\n            ","e0266ac4":"feat_train = index_sentences(df_train['phrase'].values, index_words, 10)\nfeat_validation = index_sentences(df_validation['phrase'].values, index_words, 10)\n\nfeatures = np.concatenate((feat_train, feat_validation), axis = 0)\ntargets = np.concatenate((to_categorical(y_train), to_categorical(y_validation)), axis = 0)\n","6d4ee98f":"sequence_length = 10\nembedding_dim = 25 \nnumber_of_words = len(index_words.values())\n\n# first create a matrix of zeros, this is our embedding matrix\nembedding_matrix = np.zeros((len(embeddings_index), embedding_dim))\n\nprint(embedding_matrix.shape)\n\n# then fill the matrix in order to be processed by the model\nfor word, i in index_words.items():\n    try:\n      embedding_matrix[i, :] = embeddings_index[word]\n    except:\n      continue\n","be00a513":"parameters = [0.01, 0.1, 0.25, 0.5]\nevolution_dropout = []\n\nfor dropout in parameters:\n  lstm_model = Sequential()\n  lstm_model.add(Embedding(number_of_words,\n                      25,\n                      embeddings_initializer=Constant(embedding_matrix),\n                      input_length=sequence_length,\n                      trainable=True))\n  lstm_model.add(SpatialDropout1D(dropout))\n  lstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n  lstm_model.add(Bidirectional(LSTM(32)))\n  lstm_model.add(Dropout(0.25))\n  lstm_model.add(Dense(units=5, activation='softmax'))\n  lstm_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  history = lstm_model.fit(feat_train, categorical_y_train, batch_size=32, epochs=75, verbose=1, validation_data=(feat_validation ,categorical_y_validation))\n  evolution_dropout.append(history)\n","be223602":"plot_accuracy_loss(evolution_dropout, len(parameters), parameters)","82a62da3":"parameters = [0.1, 0.25, 0.5]\nevolution_dropout = []\n\nfor dropout in parameters:\n  lstm_model = Sequential()\n  lstm_model.add(Embedding(number_of_words,\n                      25,\n                      embeddings_initializer=Constant(embedding_matrix),\n                      input_length=sequence_length,\n                      trainable=True))\n  lstm_model.add(SpatialDropout1D(0.25))\n  lstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n  lstm_model.add(Bidirectional(LSTM(32)))\n  lstm_model.add(Dropout(dropout))\n  lstm_model.add(Dense(units=5, activation='softmax'))\n  lstm_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  history = lstm_model.fit(feat_train, categorical_y_train, batch_size=32, epochs=75, verbose=1, validation_data=(feat_validation ,categorical_y_validation))\n  evolution_dropout.append(history)","13614fbd":"plot_accuracy_loss(evolution_dropout, len(parameters), parameters)","5269d713":"def get_model_name(k):\n    return 'model_'+str(k)+'.h5'\n\nK = 7   # numbers of folds\nskf = StratifiedKFold(n_splits = K, random_state = 7, shuffle = True) # creating the stratified k-fold cross validation\n","15434a7b":"validation_accuracy = []\nvalidation_loss = []\nk_fold_evolution = []\n\nsave_dir = '\/saved_models\/'\nfold_var = 1\n\nfor train_index, val_index in skf.split(np.zeros(132),new_y_pooled):\n  training_data = features[train_index]\n  training_targets = targets[train_index]\n  validation_data = features[val_index]\n  validation_targets = targets[val_index]\n  # create and compile new model  \n  lstm_model = Sequential()\n  lstm_model.add(Embedding(number_of_words,\n                      25,\n                      embeddings_initializer=Constant(embedding_matrix),\n                      input_length=sequence_length,\n                      trainable=True))\n  lstm_model.add(SpatialDropout1D(0.25))\n  lstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n  lstm_model.add(Bidirectional(LSTM(32)))\n  lstm_model.add(Dropout(0.25))\n  lstm_model.add(Dense(units=5, activation='softmax'))\n  lstm_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  print('------------------------------------------------------------------------')\n  print('------------------------ history for fold {} ----------------------------'.format(fold_var))\n  print('------------------------------------------------------------------------')\n  history = lstm_model.fit(training_data, training_targets, batch_size=32, epochs=75,validation_data=(validation_data, validation_targets));\n\n  # CREATE CALLBACKS\n  checkpoint = ModelCheckpoint(save_dir+get_model_name(fold_var), \n\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n  callbacks_list = [checkpoint]\n\t# This saves the best model\n  # Generate generalization metrics\t\n  results = lstm_model.evaluate(validation_data,validation_targets,verbose=0)\n  results = dict(zip(lstm_model.metrics_names,results))\n  validation_accuracy.append(results['accuracy'])\n  validation_loss.append(results['loss'])\n  k_fold_evolution.append(history)\n\t\n  backend.clear_session()\n  fold_var += 1 # Increase fold number\n","a80ad7d7":"# print useful result of stratified k-fold cross validation\nprint('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(validation_accuracy)*100}% (+- {np.std(validation_accuracy)*100}%)')\nprint(f'> Loss: {np.mean(validation_loss)}\\n\\n')\n# plotting resuls of stratified k-fold cross validation\nplot_accuracy_loss(k_fold_evolution,K)","8d74c3b7":"##################################################\n# Evaluate the model here\n##################################################\n\n# Use this function to evaluate your model\ndef accuracy(y_pred, y_true):\n    '''\n    input y_pred: ndarray of shape (N,)\n    input y_true: ndarray of shape (N,)\n    '''\n    return (1.0 * (y_pred == y_true)).mean()\n\n# Report the accuracy in the train and validation sets.","f5d6ab3c":"\ny_test_pred = None\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\nx_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy')).reshape(len(submission), -1)\nsubmission[\"phrase\"] = submission[\"phrase\"].str.replace(\"\\t\", \"\")\n# print(submission[\"phrase\"]) \nstring_submission = [\"\".join(x) for x in submission[\"phrase\"]]\n\n","0e87bdd0":"vect_svm = TfidfVectorizer(min_df = 1, stop_words = 'english', ngram_range = (1, 2)).fit(df_string_train)\nx_train_vect_svm = vect_svm.transform(df_string_train)\nx_test_vect_svm =  vect_svm.transform(string_submission)\nbest_svm = SVC(C = 10, gamma = 0.1, kernel = 'rbf').fit(x_train_vect_svm, new_y_pooled)\ny_pred_svm = best_svm.predict(x_test_vect_svm)\nprint(y_pred_svm)","c605b0fe":"vect_lr = TfidfVectorizer(min_df = 1, stop_words = 'english', ngram_range = (1, 1)).fit(df_string_train)\nx_train_vect_lr = vect_lr.transform(df_string_train)\nx_test_vect_lr =  vect_lr.transform(string_submission)\nbest_lr = LogisticRegression(C = 10).fit(x_train_vect_lr, new_y_pooled)\ny_pred_lr = best_lr.predict(x_test_vect_lr)\nprint(y_pred_lr)","94331083":"##################################################\n# Save your test prediction in y_test_pred\n##################################################\n\ny_test_pred = y_pred_lr\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\n\nx_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy')).reshape(len(submission), -1)\n\nif y_test_pred is not None:\n     submission['class'] = y_test_pred\n\nsubmission = submission[['Unnamed: 0', 'class']]\nsubmission = submission.rename(columns={'Unnamed: 0': 'id'})\nsubmission.to_csv('my_submission.csv', index=False)","bf98e058":"\ny_test_pred = None\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\nx_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy')).reshape(len(submission), -1)\nsubmission[\"phrase\"] = submission[\"phrase\"].str.replace(\"\\t\", \"\")\n# print(submission[\"phrase\"]) \nstring_submission = [\"\".join(x) for x in submission[\"phrase\"]]\n","62cf93e7":"\n# create and compile new model  \nlstm_model = Sequential()\nlstm_model.add(Embedding(number_of_words,\n                    25,\n                    embeddings_initializer=Constant(embedding_matrix),\n                    input_length=sequence_length,\n                    trainable=True))\nlstm_model.add(SpatialDropout1D(0.25))\nlstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))\nlstm_model.add(Bidirectional(LSTM(32)))\nlstm_model.add(Dropout(0.25))\nlstm_model.add(Dense(units=5, activation='softmax'))\nlstm_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n\nhistory = lstm_model.fit(features, targets, batch_size=32, epochs=75);\n\n","7437caa1":"\nfeat_submission = index_sentences(submission['phrase'].values, index_words, 10)\ny_pred_lstm = lstm_model.predict(feat_submission)\ny_pred_lstm = np.argmax(y_pred_lstm, axis=1)\n\nprint(y_pred_lstm)\n","a7088b55":"##################################################\n# Save your test prediction in y_test_pred\n##################################################\n\ny_test_pred = y_pred_lstm\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\n\nx_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy')).reshape(len(submission), -1)\n\nif y_test_pred is not None:\n     submission['class'] = y_test_pred\n\nsubmission = submission[['Unnamed: 0', 'class']]\nsubmission = submission.rename(columns={'Unnamed: 0': 'id'})\nsubmission.to_csv('my_submission.csv', index=False)\n","eb72d694":"\n\n# Visualizzazione Dati\n\n\n","4cac9245":"**Best Logistic Regression model**","97300bb7":"**GloVe**","c6f38ad2":"Test different value of SpatialDropout","16195a70":"**Tf-idf**","839eb0d8":"# Neural Networks","d24cad3a":"K-Fold cross-validation\n","3da8b3b4":"**Tf-idf**","b115d59b":"\n# Pre-processing data\n\n\n\n\n\n\n\n","2d34c83a":"Test different type of architecture","d6f17742":"*K*-Fold cross-validation\n","5e3daa1a":"# Dataset","f8b15c61":"# Deep Neural Network\n","56012f69":"Test different value of activation function","b7fbe401":"LSTM Submission","1ebbcc24":"# LSTM\n","42d1b921":"Test different value of dropout rate","03e061a5":"**GloVe**","bf465885":"# Welcome to the Emojify Challenge!","997026c1":"Test different value of activation function","8f5afdf2":"Logistic Regression Submission","be66a308":"# Support Vector Machine","db002eb6":"Transform data with Lemmatization\n\n","cd07e811":"**Standard Bag-of-words**","f5511812":"SVM Submission","eb00520a":"**Standard Bag-of-words**","563f77e9":"**GloVe**","6cba8e61":"# Evaluation","1910b7c3":"**Standard Bag-of-words**","8586bc0a":"**Best SVM model**","5bf8cc45":"# Word embeddings\n\nWords can be represented as n-dimentional vectors where the distance between points has a correspondence respect to similarity between word semantics (similar words are closer, while dissimilar ones are distant). This representation is known as word embeddings and here is extrapolated and pre-computed from the [GloVe](https:\/\/nlp.stanford.edu\/projects\/glove\/) model. \n\nHere is depicted an example of bi-dimensional word embeddings:\n![word embedding](https:\/\/shanelynnwebsite-mid9n9g1q9y8tt.netdna-ssl.com\/wp-content\/uploads\/2018\/01\/word-vector-space-similar-words.jpg)\n\nIn our case a single word is represented by a vector of length 25.\n\n# Phrase representation\n\nAll the phrases are padded to the phrase of maximum length, in this case `max_len = 10`, and each phrase is represented by the concatenation of his word embeddings (each phrase thus is a 10 * 25 = 250 dimentional vector).","4317357b":"**TF-IDF** \n\nInstead of dropping features that are unimportant, another approach is to rescale features by how informative we expect them to be. To do this we can use term frequency-inverse document frequency (Tf-IDF). The intuition of this method is to give high weight to any term that appears often in a particular document, in this case phrase, but not in many documents in the corpus.\n","91d2340e":"processing data for *Bag-of-Words*","dcee6a90":"# Naive Neural Networks","676582c6":"For GloVe we don't use standard preprocessing steps like stemming, lowercasing or stopword removal. The problem is that we are working with pre-trained embeddings and so, using standard preprocessing, we could loose valuable information, which would help our neural networks to figure things out.","ec1ad27f":"**Tf-idf**","22ca86a2":"**Best K-NN model**","b9fdd491":"**GloVe**","1925dc56":"**Tf-idf**","45fbe942":"Test different value of dropout rate","c4714797":"**Best Random Forest model**","87e1089a":"Test different value of Dropuot","19abeeeb":"*K*-Fold cross-validation\n","27ccfb0d":"# Random Forest \\ Decision tree","64737463":"# Logistic Regression\n","ca06073e":"# Model\n\nHere you have to implement a model (or more models, for finding the most accurate) for classification.\n\nYou can use the sklearn (or optionally other more advanced frameworks such as pytorch or tensorflow) package that contains a pool of models already implemented that perform classification. (SVMs, NNs, LR, kNN, ...)","0be5bf8d":"Class count","99430b15":"**Standard Bag-of-words**","6f03e7f8":"# K-nearest neighbors","fe625281":"Convert a collection of text documents to a matrix of token counts\n\n","374de6e0":"Using Stopwords to reduce number of features\n","cab6786b":"# Send the submission for the challenge"}}