{"cell_type":{"5eac87c7":"code","d7652b65":"code","29959cd4":"code","0c891c77":"code","98942e3f":"code","328cdf85":"code","027faee1":"code","d5740d4a":"code","387b9c5d":"code","97dd330c":"code","f926c6de":"code","3113ac58":"code","fecb7e24":"code","80a2d20c":"code","128e6dfa":"code","d5807d16":"code","43b8a788":"code","57cd1691":"code","fbd4c775":"code","b72ea201":"markdown","dea8eaf2":"markdown","9f2dfb51":"markdown","b6ac7383":"markdown","7239c286":"markdown","c9dd4807":"markdown","04f9c7a8":"markdown","08a517d2":"markdown","2c012e67":"markdown","7ee28940":"markdown","fea7c6d9":"markdown","1ec92eb7":"markdown","57ed748e":"markdown","b71a33d5":"markdown","27e316a6":"markdown","e88b79ab":"markdown","a8cd4e5b":"markdown","66bbc43a":"markdown","835f94e9":"markdown","5027beca":"markdown","86e4ce72":"markdown","f4c7aeab":"markdown","d021d5b7":"markdown"},"source":{"5eac87c7":"# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Plotting\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Finance related operations\nfrom pandas_datareader import data\n\n# Import this to silence a warning when converting data column of a dataframe on the fly\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\n%matplotlib inline","d7652b65":"# Load data\ndf = pd.read_csv('..\/input\/200-financial-indicators-of-us-stocks-20142018\/2014_Financial_Data.csv', index_col=0)\n\n# Drop rows with no information\ndf.dropna(how='all', inplace=True)","29959cd4":"# Get info about dataset\ndf.info()\n\n# Describe dataset variables\ndf.describe()","0c891c77":"# Plot class distribution\ndf_class = df['Class'].value_counts()\nsns.barplot(np.arange(len(df_class)), df_class)\nplt.title('CLASS COUNT', fontsize=20)\nplt.show()\n\n# Plot sector distribution\ndf_sector = df['Sector'].value_counts()\nsns.barplot(np.arange(len(df_sector)), df_sector)\nplt.xticks(np.arange(len(df_sector)), df_sector.index.values.tolist(), rotation=90)\nplt.title('SECTORS COUNT', fontsize=20)\nplt.show()","98942e3f":"# Extract the columns we need in this step from the dataframe\ndf_ = df.loc[:, ['Sector', '2015 PRICE VAR [%]']]\n\n# Get list of sectors\nsector_list = df_['Sector'].unique()\n\n# Plot the percent price variation for each sector\nfor sector in sector_list:\n    \n    temp = df_[df_['Sector'] == sector]\n\n    plt.figure(figsize=(30,5))\n    plt.plot(temp['2015 PRICE VAR [%]'])\n    plt.title(sector.upper(), fontsize=20)\n    plt.show()","328cdf85":"# Get stocks that increased more than 500%\ngain = 500\ntop_gainers = df_[df_['2015 PRICE VAR [%]'] >= gain]\ntop_gainers = top_gainers['2015 PRICE VAR [%]'].sort_values(ascending=False)\nprint(f'{len(top_gainers)} STOCKS with more than {gain}% gain.')\nprint()\n\n# Set\ndate_start = '01-01-2015'\ndate_end = '12-31-2015'\ntickers = top_gainers.index.values.tolist()\n\nfor ticker in tickers:\n    \n    # Pull daily prices for each ticker from Yahoo Finance\n    daily_price = data.DataReader(ticker, 'yahoo', date_start, date_end)\n    \n    # Plot prices with volume\n    fig, (ax0, ax1) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 1]})\n    \n    ax0.plot(daily_price['Adj Close'])\n    ax0.set_title(ticker, fontsize=18)\n    ax0.set_ylabel('Daily Adj Close $', fontsize=14)\n    ax1.plot(daily_price['Volume'])\n    ax1.set_ylabel('Volume', fontsize=14)\n    ax1.yaxis.set_major_formatter(\n            matplotlib.ticker.StrMethodFormatter('{x:.0E}'))\n\n    fig.align_ylabels(ax1)\n    fig.tight_layout()\n    plt.show()","027faee1":"# Drop those stocks with inorganic gains\ninorganic_stocks = tickers[:-2] # all except last 2\ndf.drop(inorganic_stocks, axis=0, inplace=True)","d5740d4a":"# Check again for gain-outliers\ndf_ = df.loc[:, ['Sector', '2015 PRICE VAR [%]']]\nsector_list = df_['Sector'].unique()\n\nfor sector in sector_list:\n    \n    temp = df_[df_['Sector'] == sector] # get all data for one sector\n\n    plt.figure(figsize=(30,5))\n    plt.plot(temp['2015 PRICE VAR [%]'])\n    plt.title(sector.upper(), fontsize=20)\n    plt.show()","387b9c5d":"# Drop columns relative to classification, we will use them later\nclass_data = df.loc[:, ['Class', '2015 PRICE VAR [%]']]\ndf.drop(['Class', '2015 PRICE VAR [%]'], inplace=True, axis=1)\n\n# Plot initial status of data quality in terms of nan-values and zero-values\nnan_vals = df.isna().sum()\nzero_vals = df.isin([0]).sum()\nind = np.arange(df.shape[1])\n\nplt.figure(figsize=(50,10))\n\nplt.subplot(2,1,1)\nplt.title('INITIAL INFORMATION ABOUT DATASET', fontsize=22)\nplt.bar(ind, nan_vals.values.tolist())\nplt.ylabel('NAN-VALUES COUNT', fontsize=18)\n\nplt.subplot(2,1,2)\nplt.bar(ind, zero_vals.values.tolist())\nplt.ylabel('ZERO-VALUES COUNT', fontsize=18)\nplt.xticks(ind, nan_vals.index.values, rotation='90')\n\nplt.show()","97dd330c":"# Find count and percent of nan-values, zero-values\ntotal_nans = df.isnull().sum().sort_values(ascending=False)\npercent_nans = (df.isnull().sum()\/df.isnull().count() * 100).sort_values(ascending=False)\ntotal_zeros = df.isin([0]).sum().sort_values(ascending=False)\npercent_zeros = (df.isin([0]).sum()\/df.isin([0]).count() * 100).sort_values(ascending=False)\ndf_nans = pd.concat([total_nans, percent_nans], axis=1, keys=['Total NaN', 'Percent NaN'])\ndf_zeros = pd.concat([total_zeros, percent_zeros], axis=1, keys=['Total Zeros', 'Percent Zeros'])\n\n# Graphical representation\nplt.figure(figsize=(15,5))\nplt.bar(np.arange(30), df_nans['Percent NaN'].iloc[:30].values.tolist())\nplt.xticks(np.arange(30), df_nans['Percent NaN'].iloc[:30].index.values.tolist(), rotation='90')\nplt.ylabel('NAN-Dominance [%]', fontsize=18)\nplt.grid(alpha=0.3, axis='y')\nplt.show()\n\nplt.figure(figsize=(15,5))\nplt.bar(np.arange(30), df_zeros['Percent Zeros'].iloc[:30].values.tolist())\nplt.xticks(np.arange(30), df_zeros['Percent Zeros'].iloc[:30].index.values.tolist(), rotation='90')\nplt.ylabel('ZEROS-Dominance [%]', fontsize=18)\nplt.grid(alpha=0.3, axis='y')\nplt.show()","f926c6de":"# Find reasonable threshold for nan-values situation\ntest_nan_level = 0.5\nprint(df_nans.quantile(test_nan_level))\n_, thresh_nan = df_nans.quantile(test_nan_level)\n\n# Find reasonable threshold for zero-values situation\ntest_zeros_level = 0.6\nprint(df_zeros.quantile(test_zeros_level))\n_, thresh_zeros = df_zeros.quantile(test_zeros_level)","3113ac58":"# Clean dataset applying thresholds for both zero values, nan-values\nprint(f'INITIAL NUMBER OF VARIABLES: {df.shape[1]}')\nprint()\n\ndf_test1 = df.drop((df_nans[df_nans['Percent NaN'] > thresh_nan]).index, 1)\nprint(f'NUMBER OF VARIABLES AFTER NaN THRESHOLD {thresh_nan:.2f}%: {df_test1.shape[1]}')\nprint()\n\ndf_zeros_postnan = df_zeros.drop((df_nans[df_nans['Percent NaN'] > thresh_nan]).index, axis=0)\ndf_test2 = df_test1.drop((df_zeros_postnan[df_zeros_postnan['Percent Zeros'] > thresh_zeros]).index, 1)\nprint(f'NUMBER OF VARIABLES AFTER Zeros THRESHOLD {thresh_zeros:.2f}%: {df_test2.shape[1]}')","fecb7e24":"# Plot correlation matrix\nfig, ax = plt.subplots(figsize=(20,15)) \nsns.heatmap(df_test2.corr(), annot=False, cmap='YlGnBu', vmin=-1, vmax=1, center=0, ax=ax)\nplt.show()","80a2d20c":"# New check on nan values\nplt.figure(figsize=(50,10))\n\nplt.subplot(2,1,1)\nplt.title('INFORMATION ABOUT DATASET - CLEANED NAN + ZEROS', fontsize=22)\nplt.bar(np.arange(df_test2.shape[1]), df_test2.isnull().sum())\nplt.ylabel('NAN-VALUES COUNT', fontsize=18)\n\nplt.subplot(2,1,2)\nplt.bar(np.arange(df_test2.shape[1]), df_test2.isin([0]).sum())\nplt.ylabel('ZERO-VALUES COUNT', fontsize=18)\nplt.xticks(np.arange(df_test2.shape[1]), df_test2.columns.values, rotation='90')\n\nplt.show()","128e6dfa":"# Analyze dataframe\ndf_test2.describe()","d5807d16":"# Cut outliers\ntop_quantiles = df_test2.quantile(0.97)\noutliers_top = (df_test2 > top_quantiles)\n\nlow_quantiles = df_test2.quantile(0.03)\noutliers_low = (df_test2 < low_quantiles)\n\ndf_test2 = df_test2.mask(outliers_top, top_quantiles, axis=1)\ndf_test2 = df_test2.mask(outliers_low, low_quantiles, axis=1)\n\n# Take a look at the dataframe post-outliers cut\ndf_test2.describe()","43b8a788":"# Replace nan-values with mean value of column, considering each sector individually.\ndf_test2 = df_test2.groupby(['Sector']).transform(lambda x: x.fillna(x.mean()))","57cd1691":"# Plot correlation matrix of output dataset\nfig, ax = plt.subplots(figsize=(20,15)) \nsns.heatmap(df_test2.corr(), annot=False, cmap='YlGnBu', vmin=-1, vmax=1, center=0, ax=ax)\nplt.show()","fbd4c775":"# Add the sector column\ndf_out = df_test2.join(df['Sector'])\n\n# Add back the classification columns\ndf_out = df_out.join(class_data)\n\n# Print information about dataset\ndf_out.info()\ndf_out.describe()","b72ea201":"# STEP 7: ADD TARGET DATA\n\nAs you recall, we dropped the target data from the dataframe. However, we need it back in order to use this dataset with ML algorithms. This can be easily achieved thanks to a couple `.join()` lines.\n\nFinally, we can wrap this notebook up by printing both `.info()` and `.describe()` of the final dataframe `df_out`.","dea8eaf2":"So, it is worth to check again the target data in the column `2015 PRICE VAR [%]`, in order to assess the impact of dropping the *fake* top gainers. ","9f2dfb51":"Now that's much better! We don't have any major peak, and the remaining ones are somewhat reasonable values.\n\nStill, even if we removed all those fake top gainers, *we cannot be fully certain that the remaining stocks have undergone an organic trading process during 2015*. ","b6ac7383":"# STEP 4: CORRELATION MATRIX, CHECK MISSING VALUES AGAIN\n\nThe correlation matrix is an important tool that can be used to quickly evaluate the linear correlation between variables, in this case financial indicators. As clearly explained [here](https:\/\/www.investopedia.com\/ask\/answers\/032515\/what-does-it-mean-if-correlation-coefficient-positive-negative-or-zero.asp), a positive linear correlation value between two variables means that they move in a similar way; a negative linear correlation value between two variables means that they move in opposite ways. Finally, if the correlation value is close to 0, then their trends are not related.\n\nLooking at the figure below, we can see that there is a chunk of financial indicators that show no linear correlation whatsoever. Those financial indicators are the heavy nan-dominant ones (as highlighted in the barplot below). This means that this chart will change once we will fill the `nan` values.","7239c286":"Thanks to this check, we can clearly see that there are indeed some major peaks in the following sectors:\n\n* Consumer Defensive\n* Basic Materials\n* Healthcare\n* Consumer Cyclical\n* Real Estate\n* Energy\n* Financial Services\n* Technology\n\nThis means that, for one reason or another, some stocks experienced incredible gains. However, how can be sure that each of these gains is organic (i.e. due to trading activity)?\n\nWe can take a closer look at this situation by plotting the price trend for those **stocks that increased their value by more than 500% during 2015**. While it is possible for a stock to experience such gains, I'd still like to verify it with my eyes.\n\nHere, we will use `pandas_datareader` to pull the *Adjusted Close* daily price, during 2015, of the required stocks. To further investigate these stocks, I think it is worth to plot the *Volume* too.","c9dd4807":"Once that's done, we can plot again the correlation matrix in order to evaluate the impact of our choices.","04f9c7a8":"# STEP 3: HANDLE MISSING VALUES, 0-VALUES\n\nThe next check we need to perform concerns the presence of missing values (`NaN`). At the same time, I think it is also useful to check the quantity of `0`-valued entries. What I like to do is simply plot a bar chart of the count of both missing values and 0-valued entries, in order to take a first look at the situation. (Due to the large quantity of financial indicators available, I will make quite a big plot)\n\nBefore doing that, we can drop the categorical columns from the dataframe `df`, since we won't be needing them now.","08a517d2":"In this notebook, we explored the financial indicators for a list of stocks relative to 2014. After an initial investigation regarding general aspects of the dataset, we performed some data cleaning steps in order to improve the usability of the dataset.\n\nFeel free to fork this notebook and add your own touch.","2c012e67":"# STEP 1: LOAD DATA\n\nFirst things first, we need to load the data from the `.csv` file. This is easily done with `pandas`. I like to keep the ticker of the stocks as index of the dataframe, so I specify `index_col=0` when loading the data.\n\nFurthermore, it is convenient to immediately drop those rows where *all* the values are `NaN`.","7ee28940":"We can evaluate the impact of our choices in terms of threshold levels by plotting again the count of missing values and 0-valued entries occurring in the remaining financial indicators. The situation has clearly improved, even if a few financial indicators mantain high levels of nan-dominance, which is evident when looking at the correlation matrix above.","fea7c6d9":"The two plots above clearly show that to improve the quality of the dataframe `df` we need to:\n1. fill the missing data\n2. fill or drop those indicators that are heavy zeros-dominant.\n\n**What levels of nan-dominance and zeros-dominance are we going to tolerate?**\n\nI usually determine a threshold level for both nan-dominance and zeros-dominance, which corresponds to a given percentage of the total available samples (rows): **if a column has a percentage of nan-values and\/or zero-valued entries higher than the threshold, I drop it**.\n\nFor this specific case we know that we have about 3800 samples, so I reckon we can set:\n* nan-dominance threshold = 5-7%\n* zeros-dominance threshold = 5-10%\n\nOnce the threshold levels have been set, I iteratively compute the `.quantile()` of both `df_nans` and `df_zeros` in order to find the number of financial indicators that I will be dropping. In this case, we can see that:\n* We need to drop the top 50% (`test_nan_level=1-0.5=0.5`) nan-dominant financial indicators in order to not have columns with more than 226 `nan` values, which corresponds to a nan-dominance threshold of 5.9% (aligned with our initial guess).\n* We need to drop the top 40% (`test_zeros_level=1-0.4=0.6`) zero-dominant financial indicators in order to not have columns with more than 283 `0` values, which corresponds to a zero-dominance threshold of 7.5% (aligned with our initial guess).","1ec92eb7":"# STEP 2: FIRST LOOK AT THE DATASET\n\n## STEP 2.1: general info, categorical variables\n\nIt is useful to take a quick look at the initial state of the dataset:\n\n1. use `.info()`, `.describe()` method to get a first sense of the dimensions of the dataset and value of numeric variables;\n2. focus on the categorical variables and also the class of the samples, to see if they are balanced.","57ed748e":"The plots above show that:\n1. **the samples are not balanced in terms of class**. Indeed, 2174 samples belong to class `0`, which as explained in the documentation of the dataset correspond to stocks that are *not buy-worthy*. At the same time, 1634 samples belong to class `1`, meaning they are *buy-worthy* stocks. This should be accounted for when splitting the data between training and testing data (it is useful to use the `stratify` option available within `sklearn.model_selection.train_test_split`).\n2. there is a total of 11 sectors, 5 of them with about 500+ stocks each, while the remaining 6 sectors have less than 300 stocks. In particular, the sectors *Utilities* and *Communication Services* have around 100 samples. This has to be kept in mind if we want to use this data with ML algorithms: there are very few samples, which could lead to overfitting, etc.","b71a33d5":"We can see that:\n1. There are quite a lot of missing values\n2. There are also a lot of 0-valued entries. For some financial indicators, almost every entry is set to 0.\n\nTo understand the situation from a more quantitative perspective, it is useful to count the occurrences of both missing-values and 0-valued entries, and sort them in descending order. This allows us to establish the *dominance* level for both missing values and 0-valued entries.","27e316a6":"As we can see, most of the `top_gainers` stocks did not experienced an organic growth during 2015. This is highlighted by a portion of the price trend being completely flat, due to the absence of trading activity.\n\nSo, I reckon that only the last 2 stocks from the `top_gainers`, namely **NYMX** and **AVXL**, should be kept in the dataframe and we should drop the others.","e88b79ab":"## STEP 2.2: price variation, look for outliers\/errors\n\nIt is always important to make sure that the target data *makes sense*. I am particularly curious to see if the column `2015 PRICE VAR [%]`, which lists the percent price variation of each stock during the year 2015, contains any *mistake* (for instance mistypings or unreasonable values). A quick plot of this column (for each sector), will allow us to assess the situation.\n\nIn layman's terms, here we are looking for major peaks\/valleys, which indicate stocks that increased\/decreased in value by an incredible amount with respect to the overall sector's trend.","a8cd4e5b":"We now know that we have:\n- 3808 samples\n- 224 columns\n - 222 numeric --> they are the financial indicators\n - 1 int       --> this is the class column\n - 1 object    --> this is categorical (`Sector`)\n\nNext, we will take a look at the distribution of the class values, and the distribution of the categorical variable `Sector`.","66bbc43a":"Once the threshold levels have been set, I can proceed and drop from `df` those columns (financial indicators) that show dominance levels higher than the threshold levels, in terms of both missing values and 0-valued entries.\n\nSo, we reduced the number of financial indicators available in the dataframe `df` to 62. By doing so, we removed all those columns characterized by heavy nan-dominance and zeros-dominance. \n\n*We should always keep in mind that this is quite a brute force approach, and there is the possibilty of having dropped useful information.*","835f94e9":"# EXPLORATORY DATA ANALYSIS\n\nIn this notebook, I will show some quick and easy exploratory steps that I usually take when working on these types of dataset (namely finance-related datasets). Hopefully you'll find this notebook somewhat useful.\n\nI will be focusing my attention on the file `2014_Financial_Data.csv`, which is available as past of the dataset **200+ Financial Indicators of US stocks (2014-2018)**. Needless to say, the reported steps can be applied to every `.csv` file.\n\nThe Python packages that we will be using are:\n\n1. for data manipulation\n - pandas\n - numpy\n2. for plotting purposes\n - matplotlib\n - seaborn\n3. for finance related operations\n - [pandas_datareader](https:\/\/pandas-datareader.readthedocs.io\/en\/latest\/)\n\nWith that being said, let's begin by importing the required packages.","5027beca":"# STEP 5: HANDLE EXTREME VALUES\n\nAnalyzing the `df` with the method `.describe()` we can see that some financial indicators show a large discrepancy between max value and 75% quantile. Furthermore, we also have standard deviation values that are very large! This could be a sign of the presence of outliers: to be conservative, I will drop the top 3% and bottom 3% of the data for each financial indicator.","86e4ce72":"As we can see, the chunk of financial indicators that was characterized by linear correlation equal to 0 is now more organic (i.e. correlation is either positive or negative), thanks to the fact that we replaced the missing values with the respective mean value of the column (per-sector).","f4c7aeab":"# STEP 6: FILL MISSING VALUES\n\nWe can now fill the missing values, but how? There are several methods we could use to fill the missing values:\n* fill `nan` with 0\n* fill `nan` with mean value of column\n* fill `nan` with mode value of column\n* fill `nan` with previous value\n* ...\n\nIn this case, I think it is appropriate to fill the missing values with the mean value of the column. However, we must not forget the intrinsic characteristics of the data we are working with: **we have a many stocks from many different sectors**. It is fair to expect that each sector is characterized by macro-trends and macro-factors that may influence some financial indicators in different ways. So, I reckon that we should keep this separation somehow.\n\nFrom a practical perspective, this translates into **filling the missing value with the mean value of the column, grouped by each sector**.","d021d5b7":"Looking at the statistical description of the dataframe `df` post outliers removal, we can see that **we managed to decrease the standard deviation values considerably**, and also the discrepancy between max value and 75% quantile is smaller."}}