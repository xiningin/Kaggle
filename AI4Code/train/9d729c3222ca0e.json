{"cell_type":{"c2a903ca":"code","e3e81254":"code","157aa9ce":"code","419c19a1":"code","9804b6d9":"code","3eff4338":"code","11df2969":"code","6cbbe038":"code","9124e663":"code","0c5e61eb":"code","fcd25d33":"code","8b2267e1":"code","15fdaf28":"code","cb407a9d":"code","d0772f78":"code","4b0779de":"code","49d66dfb":"code","dff2e70a":"code","de70e56c":"code","53e7fad6":"code","279d41cd":"code","ff944637":"code","9be3ef19":"code","4291976e":"code","1924f1ca":"code","3495ece3":"code","eb190577":"code","67e0169d":"code","5933651e":"code","6be5ef98":"code","52792b4b":"code","998381bb":"code","43cdfaf4":"code","974ae354":"code","7a66f05c":"code","8c53ccb6":"code","bc4a8ff7":"code","6dd70bcc":"code","a86bb11f":"code","4c01df45":"markdown","293ad655":"markdown","43b9e83d":"markdown","afe815dc":"markdown","a717cc3e":"markdown","a2cd69b7":"markdown","f294fb25":"markdown","9c26c2aa":"markdown","063c1d85":"markdown","65aa624a":"markdown","af8ad032":"markdown","12dd71d2":"markdown","4f8b4fd1":"markdown","4b81fd52":"markdown","dcfdfb3d":"markdown","44f2386b":"markdown","097b9802":"markdown","6d929013":"markdown","8ecc6bba":"markdown","87bd8e34":"markdown","b0bbf5f2":"markdown","c4fddad8":"markdown","06bc8d8a":"markdown","31105452":"markdown","8dd76bca":"markdown","926a1486":"markdown","eec6347b":"markdown","e075911b":"markdown","94b1fb4d":"markdown","881c556a":"markdown","86fe2541":"markdown","c7bb4647":"markdown","26b37608":"markdown","a71a3529":"markdown","fcf50a16":"markdown","d96e9f31":"markdown"},"source":{"c2a903ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e3e81254":"def write_submissions(file_name, test_df, predictions):\n    test_df.Id = test_df.Id.astype('int32')\n\n    output = pd.DataFrame({\n        'Id': test_df.Id, 'SalePrice': predictions\n    })\n    output.to_csv(file_name, index=False)\n    \ndef get_categorical_columns(data_df):\n    return list(data_df.select_dtypes(include=['category', 'object']))\n\n\ndef get_numeric_columns(data_df):\n    return list(data_df.select_dtypes(exclude=['category', 'object']))\n\n\ndef read_train_test_data():\n    train_df = pd.read_csv('..\/input\/train.csv')\n    test_df = pd.read_csv('..\/input\/test.csv')\n    \n    print(\"Shape of Train Data: \" + str(train_df.shape))\n    print(\"Shape of Test Data: \" + str(test_df.shape))\n    \n    categorical_columns = get_categorical_columns(train_df)\n    print(\"No of Categorical Columns: \" + str(len(categorical_columns)))\n    numeric_columns = get_numeric_columns(train_df)\n    print(\"No of Numeric Columns: \" + str(len(numeric_columns)))\n\n    return train_df, test_df","157aa9ce":"train_df, test_df = read_train_test_data()","419c19a1":"numeric_columns = get_numeric_columns(train_df)\ntrain_df[numeric_columns].describe()","9804b6d9":"categorical_columns = get_categorical_columns(train_df)\ntrain_df[categorical_columns].describe()","3eff4338":"from sklearn.tree import DecisionTreeRegressor\n    \ndef simple_model(X, y, X_test):\n    model = DecisionTreeRegressor(random_state=1)\n    predictions = None\n    try:\n        model.fit(X, y)\n        predictions = model.predict(X_test)\n    except Exception as exception:\n        print(exception)\n        pass\n\n    return predictions","11df2969":"### Simple Model \nX = train_df[numeric_columns].copy()\nX.drop(columns=[\"SalePrice\"], axis=1, inplace=True)\ny = train_df.SalePrice\nprint(\"Shape of Modified Train Data: \" + str(X.shape))\nsimple_model(X, y, test_df[X.columns])","6cbbe038":"X = X.dropna(axis=1)\nprint(X.shape)\nsimple_model(X, y, test_df[X.columns])","9124e663":"cols_with_missing_values = [col for col in X.columns if X[col].isnull().any()]\nX = X.drop(cols_with_missing_values, axis=1)\nprint(X.shape)\nsimple_model(X, y, test_df[X.columns])","0c5e61eb":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='median')\nimputed_numeric_X = imputer.fit_transform(X)\n\nimputed_numeric_test = imputer.transform(test_df[X.columns])","fcd25d33":"predictions = simple_model(imputed_numeric_X, y, imputed_numeric_test)\ndel imputed_numeric_X, imputed_numeric_test","8b2267e1":"print(\"No. of Numeric Features in Source: {0}, in Model: {1}\".format(len(numeric_columns), len(X.columns)))\nfaulty_columns = set(numeric_columns) - set(X.columns)\nprint(faulty_columns)","15fdaf28":"for a_faulty_column in X.columns:\n    if(a_faulty_column !=  'SalePrice'):\n        train_df[a_faulty_column].loc[train_df[a_faulty_column].isnull()] = train_df[a_faulty_column].mean()\n        train_df[a_faulty_column].loc[~np.isfinite(train_df[a_faulty_column])] = train_df[a_faulty_column].mean()\n        \n        test_df[a_faulty_column].loc[test_df[a_faulty_column].isnull()] = test_df[a_faulty_column].mean()\n        test_df[a_faulty_column].loc[~np.isfinite(test_df[a_faulty_column])] = test_df[a_faulty_column].mean()\n        \n        train_df[a_faulty_column].astype('float32', inplace=True)\n        test_df[a_faulty_column].astype('float32', inplace=True)","cb407a9d":"train_df, test_df = read_train_test_data()\n","d0772f78":"X = train_df[categorical_columns]\ny = train_df.SalePrice\nprint(X.shape)\ntest_X = test_df[categorical_columns]","4b0779de":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfailed_features = []\nfor aFeature in categorical_columns:\n    try:\n        X[aFeature] = le.fit_transform(X[aFeature])\n        test_X[aFeature] = le.transform(test_X[aFeature])\n    except:\n        failed_features.append(aFeature)\n        \nX.drop(columns=failed_features, inplace=True)\ntest_X.drop(columns=failed_features, inplace=True)\n\nprint(failed_features)","49d66dfb":"imputer = SimpleImputer(strategy='median')\nimputed_X = imputer.fit_transform(X)\n\nimputed_test = imputer.transform(test_X)","dff2e70a":"predictions = simple_model(imputed_X, y, imputed_test)\ndel imputed_X, imputed_test","de70e56c":"write_submissions(\"simple_model_s2.csv\", test_df, predictions)","53e7fad6":"train_df, test_df = read_train_test_data()","279d41cd":"le = LabelEncoder()\n\nfailed_features = [] # Placeholder to store the failed features.\nfor aFeature in categorical_columns:\n    try:\n        train_df[aFeature] = le.fit_transform(train_df[aFeature])\n        test_df[aFeature] = le.transform(test_df[aFeature])\n    except:\n        failed_features.append(aFeature)\n        \ntrain_df.drop(columns=failed_features, inplace=True)\ntest_df.drop(columns=failed_features, inplace=True)","ff944637":"imputer = SimpleImputer(strategy='median')\nimputer.fit(train_df)\nimputed_data = imputer.transform(train_df)\ntrain_df = pd.DataFrame(imputed_data, columns=train_df.columns)","9be3ef19":"imputer = SimpleImputer(strategy='median')\nimputer.fit(test_df)\nimputed_data = imputer.transform(test_df)\ntest_df = pd.DataFrame(imputed_data, columns=test_df.columns)","4291976e":"features = test_df.columns\nX = train_df[features]\ny = train_df.SalePrice\npredictions = simple_model(X, y, test_df)","1924f1ca":"write_submissions(\"simple_model_s3.csv\", test_df, predictions)","3495ece3":"from sklearn.ensemble import RandomForestRegressor\nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(X, y)\nforest_predictions = forest_model.predict(test_df)","eb190577":"write_submissions(\"simple_model_s4.csv\", test_df, forest_predictions)","67e0169d":"from xgboost import XGBRegressor\nmodel_xgb = XGBRegressor()\nmodel_xgb.fit(X, y, verbose=False)\nxgb_predictions = model_xgb.predict(test_df)","5933651e":"write_submissions(\"simple_model_s5.csv\", test_df, xgb_predictions)","6be5ef98":"print(\"Shape of Train Data: {0}\".format(X.shape))\nprint(\"Shape of Test Data: {0}\".format(test_df.shape))\nprint(\"No. of Failed Categorical Features: {0}\".format(len(failed_features)))\nprint(\"No. of Failed Numeric Features: {0}\".format(len(faulty_columns)))","52792b4b":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n","998381bb":"from sklearn.metrics import mean_absolute_error\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)","43cdfaf4":"max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n# Write loop to find the ideal tree size from max_leaf_nodes\nmaes = [[get_mae(a, train_X, val_X, train_y, val_y), a] for a in max_leaf_nodes]\nprint(maes)","974ae354":"best_tree_size = min(maes)[1]\nprint(best_tree_size)","7a66f05c":"optimal_tree_size_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)\n\n# fit the final model and uncomment the next two lines\noptimal_tree_size_model.fit(train_X, train_y)\noptimal_tree_predictions = optimal_tree_size_model.predict(test_df)\n# step_2.check()","8c53ccb6":"xgb_model = XGBRegressor(n_estimators=1000, learning_Rate=0.05)\nxgb_model.fit(train_X, train_y, early_stopping_rounds=15, eval_set=[(val_X, val_y)], verbose=True)","bc4a8ff7":"from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngbr_model = GradientBoostingRegressor()\ngbr_model.fit(train_X, train_y)\nplots = plot_partial_dependence(gbr_model, features=[1, 2, 8], X=train_X, feature_names=train_X.columns, grid_resolution=200)","6dd70bcc":"from sklearn.pipeline import make_pipeline\npipeline = make_pipeline(SimpleImputer(), XGBRegressor())\npipeline.fit(X, y)\nvalidation_predictions = pipeline.predict(val_X)\n\nprint(\"MAE \" + str(mean_absolute_error(validation_predictions, val_y)))","a86bb11f":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(pipeline, X,y, scoring='neg_mean_absolute_error')\nprint(scores)","4c01df45":"## <a id='section19'>Train and Validation Data Set<\/a>\nWe made set of predictions using various models and improved the quality of our predictions. Until now the progress is mostly linear in terms of learning but this happy path is not promised always. Few fundamental concepts are the key to success,one such thing is underfitting and overfitting.\n\nIn this process, we shall split the train data into train set and validation set. Using validation set we can identify the accuracy well ahead.\n\nWithin the scope of Decision Trees, let us build a model that takes care of this.\n- Split data into train and validation sets to calculate error in advance\n- Depth of the tree is a key hyperparameter to play with\n\nReference: An introduction to data splitting.\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/oJzDsnPq4vU?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>'\n\n","293ad655":"**Observe the reduction of RMSE value and the iterations stops when there was no longer an improvement.**\n\nSimilar to n_estimators, learning rate... there are many other attributes that determines the model accuracy. To name few\n- **max_depth**: Maximum depth of the tree\n- **min_child_weight**: Minimum sum of weights of all observations required\n- **lambda**: L2 Regularization term on weights(Reduce overfitting)\n- **alpha**: L1 regularization term on weights(Used during high dimensionality)\n\n## <a id='section23'>Partial Dependence Plots<\/a>\nPartial dependence plots show how each variable or predictor afftect the model's accuracy. It is similar to coefficients in a linear or logistic regression model.","43b9e83d":"We have missed 3 features GarageYrBlt, LotFrontage and MasVnrArea.We shall revisit the numerical data once again and impute them.","afe815dc":"## <a id='#section9'>Missed Features - Examination<\/a>\nOur source data had 80 features and we\n- Considered 34 features only\n- Dropped 43 of the categorical value\n- 3 Features are dropped due to NaN, Infinity etc\n\nUsing the dropped columns could have made significant impact on the accuracy on the final outcome.\nLet us figure out ways to incorporate them as well.","a717cc3e":"## <a id='section15'>Submission 4<\/a>\n- Score: **17465.83305**\n- Rank: **~966**\n- Date: **16th April, 2019**\n\n### Wow, that's a leap. From 4500 to 966 jump just by using Random Forest","a2cd69b7":"* MSSubClass, LotFrontAge, OverallQual, OverallConditions are looking like a categorical value\n* YearBuilt, YearRemodAdd, GarageYrBlt, MoSold, YrSold are definitely not numeric but time period. ","f294fb25":"** Examine Numerical Features that are failed**\n\nTo ensure all features are taken into consideration, examine the features in the source data and imputed data used to build our first model.","9c26c2aa":"#### Examine the Features once again","063c1d85":"The problem still persists. We shall drop the columns where null values are present.\n\n** Remove Null Value Columns **","65aa624a":"This naturally triggers the curiosity to try other models and see how things work.\n\n## <a id='section16'>XGBoost<\/a>\nXGBoost expands Extreme Gradient Boosting, This is the most important technic to learn in ML.\n\nReference: A deep down into boosting concept, an MIT video\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/UHBmv7qCey4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>'\n\n\n","af8ad032":"## <a id='section6'>Simple Imputer<\/a>\nImputation fills the missing values with data which is determined during the imputation process\nSome of the most common ways of imputation are\n- **Hot Deck**: A missing value is replaced with a randomly selected similar record.\n- **LOCF**: Stands Last Observation Carry Forward, a most popular way of imputation but it has very high risk.\n- **Cold Deck**: Select the missing value from another dataset.\n- **Mean Substitution**: Replacing the missing value with mean of the sample set. It attenuates the correlation of the missing variable\n- **Regression**: A regression model is estimated to predict observed values of a variable based on other variables. Rather than linear, a stochastic regression model works much better.\n\n\nReference: Simple Imputer is a class from sklearn, It allows us to handle missing data very efficiently. Following video explains in detail how to use them\n\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/ENvSybznF_o?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>'","12dd71d2":"## <a id='section25'>Cross Validation<\/a>\nIn cross validation we run our modeling process on different subsets of the data to get multiple measures of model quality. A five fold experiment scheme from **Dan Becker**'s course\n![5 Fold CV Scheme](https:\/\/i.stack.imgur.com\/1fXzJ.png)","4f8b4fd1":"## <a id='section5'>Missing Data<\/a>\n\nInput contains NaN - Not A Number or Infinity or Larger value beyond 'float32' space is found.\nThis has caused an exception while fitting the data. It's time to treat them to get our model going.\n\nReference: Our dataset has significant number of anamolies in the form of missing data or deformed data. This video gives much insight into how to handle them.\n\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/fCMrO_VzeL8?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>'","4b81fd52":"## <a id='section13'>Submission 3 - Results<\/a>\n- Score: **26223.10562**\n- Rank: **4511**\n- Date: **16th April, 2019**\n\n## <a id='section13'>Random Forest Regressor<\/a>\n\nWith the same dataset we shall try Random Forest Regressor\n\nReference: A cool video on random forest algorithm.\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/D_2LkhMJcfY?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>'\n\n","dcfdfb3d":"**Read the Data**:\nLet's call our utility methods and read the data from the storage","44f2386b":"One hot encode the categorical features and identify features that are failed to encode.","097b9802":"## <a id='section2'>Reusable Methods - SET 1<\/a>\n\nWe will be using the following methods repeatedly while progressing through the course. I suggest to skip this section and refer as and when it is used for the first time.","6d929013":"## <a id='section10'>Categorical Features - Label Encoding<\/a>\nWe saw the model performance with numeric variables, let us examine how to make use of categorical values.\nHere I am loading the data again to mimic fresh start\n\nReference: Data are 2 kind, Categorical and Numerial. This video briefs them\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/IffHV_AS_Do?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>'\n","8ecc6bba":"## <a id='section8'>Submission 1<\/a>\n- Score: **26450.16598**\n- Rank: **~4500**\n- Date: **16th April, 2019**","87bd8e34":"## Objective\nObjective of this kernel is \n- To bring in a quick reference that covers the key concepts of ML.\n- Target Audience: Beginners\n- Best way to learn is to learn from the best, this kernel introduces the key personalities of Machine Learning world\n- Video tutorials are the best and only next to a class room session, bunch of videos included\n- Beyond theory... implementation, validation and evaluation gives significant confidence to the learner. Every progress or new concept is validated as a competition submission \n\n\n#### Some of My Other Works\n[Game of Thrones - Systematic Analysis of GoT Battles(Kaggle Script of the Week)](https:\/\/www.kaggle.com\/gowrishankarin\/analysis-on-got-battles-kaggle-script-of-the-week)  \n[Space and Time Complexity - 5 Governing Rules](https:\/\/www.kaggle.com\/gowrishankarin\/time-and-space-complexity-5-governing-rules)  \n\n## What to expect\nThis kernel is expected to answer the following questions...\n- How to look at data by doing an exploratory data analysis?\n- How to handle missing data in a dataset?\n- What are numerical and categorical data?\n- How to identify numerical and categorical data?\n- What are machine learning models?\n- How to prepare datasets feed into some of the famous ML models?\n- What is Bias and Variance?\n- How to handle Bias and Variance?\n- What are hyperparameters, how to optimize hyperparameters?\n- What are data pipelines?","b0bbf5f2":"Let us figure out a way to treat the columns with above said discrepancies.\n\n** Drop All Columns where NaN occurs**","c4fddad8":"## <a id='section22'>Model Tuning<\/a>\n**n_estimators**: n_estimators specifies how many times to go throught the modeling cycle described above. To low value of n_estimators leads to underfitting and larger values ends up in overfitting.\n\n**early_stopping_rounds**: Early stopping rounds is a mechanism through which the right estimaters are found automatically and training stops when it reaches optimal level.\nIt is an iterative process so there is a chance of models taking more time to attain convergence.\n\n**Validation Data**: Validation data obtained during splitting process is fed as evaluation dataset to make the model fit for optimal estimators.\n\nReference: Without Siraj's videos, ML tutorials will be incomplete. Siraj speaks about hyperparameter optimization\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/ttE0F7fghfk?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>'\n","06bc8d8a":"> ## <a id='section18'>Observations<\/a>\n\n- Out of 80 features available, our current model could utilize only 57\n- Total 23 of categorical variables are failed during one hot encoding\n- 3(-SalePrice) of the numerical variables failed due to failure of imputing.\n- There is a significant scope for improvement, if the 33% of the features are accounted while building the model","31105452":"## <a id='section20'>Underfitting and Overfitting<\/a>\n**Overfitting** : The phenomenon where a model matches the training data almost perfectly, but does poorly in validation and other new data.\nThis can overcome by making deeper tree.\nFew thoughts on Overfitting\n- It occurs when a machine learning model learns the noise in the dataset\n- It is a phenomenon where the model portrays **low bias** and **high variance**\n- The model is complex\n- The cause of overfitting could be non parametric and non linear due to the degree of freedom the model builder\n- The models could be unrealistic\n\nMethods that help us avoid overfitting\n- **Cross Validation**\n- **Early Stopping**\n- **Pruning**\n- **Regularization**\n\n**Underfitting** : When a model fails to capture important distinctions and patterns in the data, it performs poorly in training data. This phenomenon is called as underfitting.\nFew thoughts on Underfitting\n- It occurs when the ML model fail to capture the underlying trend of the dataset\n- It is a phenomenon where the model portrays **high bias** and **low variance**\n- The model is simple\n- By avoiding more data and reducing features, underfitting can be dealt\n\n\n\nReference: Here Dr.Andrew Ng explains Bias and Variance in detail\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/SjQyLhQIXSM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>'","8dd76bca":"## <a id='section21'>Optimal Tree Structure<\/a>\n\nReference: Some deep thoughts on tree pruning\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/t56Nid85Thg?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>'\n","926a1486":"Reference: We used Decision Tree Regressor, A cool video on Decision Trees\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/DCZ3tsQIoGU?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>'","eec6347b":"1. [Inspiration](#section1)\n1. [Reusable Methods - Set 1](#section2)\n1. [Exploratory Data Analysis](#section3)\n\n**Preprocessing**\n1. [Simple Model - Catch Failures](#section4)\n1. [Missing Data - NaN, Null](#section5)\n1. [Simple Imputer](#section6)\n1. [Predictions - Numeric Data](#section7)\n1. [Submission - 1](#section8)\n1. [Missed Features - Examination](#section9)\n1. [Categorical Features - Label Encoding](#section10)\n\n**Model Building**\n1. [Submission - 2](#section11)\n1. [Imputing by Strategy](#section12)\n1. [Submission 3](#section13)\n1. [Random Forest](#section14)\n1. [Submission 4](#section15)\n1. [XGBoost](#section16)\n1. [Submission 5](#section17)\n1. [Observations](#section18)\n\n**Key Concepts**\n1. [Train and Validation Data Set](#section19)\n1. [Underfitting and Overfitting](#section20)\n1. [Optimal Tree Structure](#section21)\n1. [Model Tuning](#section22)\n1. [Partial Dependence Plots](#section23)\n1. [Data Pipeline](#section24)\n1. [Cross Validation](#section25)","e075911b":"## <a id='section1'>Inspiration<\/a>\n**The inspiration is to bring in what I learned in Kaggle Learn - Machine Learning Course into One Kernel**\n\nI thought the whole course can be fit into one Jupyter notebook and started building this.\nDuring initial periods of learning, I struggled a lot in bringing in a process. By then there was no such course available and I believe this notebook along with the course will help the beginners significantly.\n\nFurther, I have included learning videos from the following channels\/ML experts for clarity and depth.\n\n- [Dmitry Ulyanov, HSE - EDA](https:\/\/www.youtube.com\/watch?v=3Xisy5EsPWA&feature=youtu.be)\n- [Socratica - Python Exceptions](https:\/\/www.youtube.com\/watch?v=nlCKrKGHSSk&feature=youtu.be)\n- [Augmented Startups - Decision Trees](https:\/\/youtu.be\/DCZ3tsQIoGU)\n- [Data School - Handling Missing Values](https:\/\/youtu.be\/fCMrO_VzeL8)\n- [ML with Python - Imputing](https:\/\/www.youtube.com\/watch?v=ENvSybznF_o&feature=youtu.be)\n- [Ian Bailey - Kinds of Data](https:\/\/www.youtube.com\/watch?v=7bsNWq2A5gI&feature=youtu.be)\n- [Cristi Vlad - Label Encoding](https:\/\/www.youtube.com\/watch?v=IffHV_AS_Do&feature=youtu.be)\n- [Augmented Startups - Random Forest](https:\/\/youtu.be\/D_2LkhMJcfY)\n- [MIT Open Courseware - Boosting](https:\/\/youtu.be\/UHBmv7qCey4)\n- [Udacity - Split Data](https:\/\/youtu.be\/oJzDsnPq4vU)\n- [Dr.Andrew Ng - Bias and Variance](https:\/\/youtu.be\/SjQyLhQIXSM)\n- [Ritvikmath - Decision Tree Pruning](https:\/\/youtu.be\/t56Nid85Thg)\n- [Siraj Raval - Hyperparameter Optimization](https:\/\/youtu.be\/ttE0F7fghfk)\n- [Mark Weiss, Beeswax - Data Pipelines](https:\/\/youtu.be\/C6Abv87D5dU)\n\nThanks to all of them for their awesome content.\n\nThis kernel consists of approximately 25-30 steps, that can be reproduced within an hour or 2. **I suggest everyone to program by yourself rather than copy\/paste.**\nWhile learning, I could improve from my rank around 4500 to 424 in just 6-8 submissions. \nI felt that was really cool and I should share the same with everyone.\n\nThanks to **Dan Becker** of Kaggle for his course and insights.\nReference: [Dan Becker's Learn ML Course](https:\/\/www.kaggle.com\/learn\/machine-learning)\n\n### Python Imports\nI decided to import the relevant packages\/libraries as and when required rather than doing it in the beginning.","94b1fb4d":"## <a id='section7'>Prediction 1 - Selected Numeric Data<\/a>\n\nReference: Data are 2 kind, Categorical and Numerial. This video briefs them\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/7bsNWq2A5gI?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>'\n","881c556a":"## <a id='section11'>Submission 2 - Results<\/a>\n- Score: **32873.91538**\n- Rank: **4543**\n- Date: **16th April, 2019**\n\nLet us start from the beginning, combine both numeric and categorical features to train the model.","86fe2541":"**Find the tree size where mean absolute error is minimum.**","c7bb4647":"## <a id='section4'>Simple Model - Catch Failures, Decision Trees<\/a>\n- Your first Machine Learning model should take very minimal time to build\n- Building the first model should ensure the bare minimal requirements for solving the problem\n- A minimalist approach will help us at benchmarking\n- The number of features in the dataset might be overwhelming, to overcome a fail fast approach should be taken\n- To get quick results, choosing the numerical features alone to start is recommended\n- Building a simple model involves 4 steps\n    - **Model Definition** - Linear Regression, Decision Trees, Random Forest etc\n    - **Train the Dataset** - In python, fit is the common function that trains to capture the patters\n    - **Predict** - Predict the unknown\n    - **Validate** - Determine how accurate the proposed model is. \n\nLet us build a simple model with very few of number properties and see how it works.\n\nReference: Using exceptions I tried to understand what kind of failures happens. A nice video on exception.\n\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/nlCKrKGHSSk?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>'","26b37608":"The above PDP indicates no influence of the feature **MSSubClass**, but significant influence of **LotFrontage** feature\n\n## <a id='section24'>Data Pipeline<\/a>\nData pipelines ensures cleaner code, fewer bugs, easier to productionize and offers more options for model testing.\n\nLife of a data has the following steps \n\n- **Getting or Collection of Data** \u2013 Applications, APIs, File Transfers etc\n- **Cleaning Data** \u2013 Data Transformers, Data Marts etc\n- **Exploratory Analysis** \u2013 Analysis and Intelligence tools like Tableau, Qlik etc and Technologies like R, Python, GGPlot2, MatPlotLib etc.\n- **Statistical Inferences** \u2013 Identifying statistically significant attributes and properties from the data\n- **Reproducible Research** \u2013 Repeat the analysis and inferences on different sample set\n- **Regression Models** \u2013 Find the nature of the data. Linear, Logistic, Bayesian etc\n- **Machine Learning** \u2013 Classification and Clustering of data. Feature creation and selection. Nai\u0308ve Bayes, Random Forests etc.\n- **Recommendation Systems** \u2013 Demand predictions, Route optimization, Log analysis, Share of Shelf etc.\n\nAll the above activities are to be done in tandem. Some involves ETL processes, visualization tools, external storage units, external data sources independent of a single project etc.\nTo ensure data integrity and reproducibility, data pipeline processes and procedures are achieved in various ways. One such way is explained in this video.\n\nReference: We got to productionize what we build and almost always the results are expected at realtime. This video gives an idea on the big picture\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/C6Abv87D5dU?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>'\n\n\n\n","a71a3529":"## <a id='section12'>Impute by Strategy<\/a>\n","fcf50a16":"## <a id='section17'>Submission 5<\/a>\n\n- Score: **15257.60080**\n- Rank: **~467**\n- Date: **16th April, 2019**\n\n### Hurrah, another significant progress. From 966 to 467 jump by using XGBoost","d96e9f31":"## <a id='section3'>Exploratory Data Analysis<\/a>\n\nExploratory data analysis is a critical process of data science during the initial stages,\n- It is conducted to preliminary investigations on nature of the data\n- Discover patterns via visualizations\n- Identify outliers, anomalies to make certain assumptions\n- Acquire knowledge to conduct hypothesis testing\n- Learn and spot statistically significant variables\n\nAlmost all datasets are Multivariate, It is a usual practice to plot the data graphically. For example\n- **Line Plot** - to identify trend(ex. time series)\n- **Scatter Plot** - to identify density(ex. classification, clustering)\n- **Heatmap** - to visualize correlation matrix\n- **Box Plot** - to view inter quartile ranges(IQR)\n- **Historgram** - to see the skewness\n- **Pie Chart** - to examine numerical proportion\n\nLet us examine numeric and categorical columns independently. There is a chance some of categorical columns are represented with numeric value and vice versa\n\nReference: This video explains the importance of EDA\n\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/3Xisy5EsPWA?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>'\n"}}