{"cell_type":{"bed64285":"code","e003b73a":"code","fc1c8deb":"code","3c671656":"code","e9316a8a":"code","d06d9827":"code","6ce5a8e8":"code","27c74c32":"code","513629af":"code","2bbab474":"code","168b1e8e":"code","610635d9":"code","8ac11f7a":"code","709ff35d":"code","a86ec3f1":"markdown","e78e1b90":"markdown","2ea092a2":"markdown","321032d2":"markdown","d9ad1611":"markdown","a15fb08f":"markdown","425a325e":"markdown","8f52dc41":"markdown","1144374e":"markdown","8602acde":"markdown","3727c36f":"markdown","0dc777f1":"markdown","23800560":"markdown","e4808692":"markdown","c317d84d":"markdown","192a646d":"markdown"},"source":{"bed64285":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))","e003b73a":"train = pd.read_csv('..\/input\/mnist_train.csv')\ntest = pd.read_csv('..\/input\/mnist_test.csv')","fc1c8deb":"X_train = train.drop(['label'], axis=1)\ny_train = train['label']\n\nX_test = test.drop(['label'], axis=1)\ny_test = test['label']","3c671656":"X_train = np.array(X_train)\nX_test = np.array(X_test)","e9316a8a":"import matplotlib.pyplot as plt\n\ndigit = X_train[0]\ndigit_pixels = digit.reshape(28, 28)\nplt.imshow(digit_pixels)\n\nprint(y_train[0])","d06d9827":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(loss='hinge', random_state=42)\nsgd_clf.fit(X_train, y_train)","6ce5a8e8":"from sklearn.model_selection import cross_val_predict, cross_val_score\n\nprint(cross_val_predict(sgd_clf, X_train, y_train, cv=3))\nprint(cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy'))\n","27c74c32":"from sklearn.metrics import accuracy_score\n\nsgd_svm_pred = sgd_clf.predict(X_test)\nsgd_svm_pred","513629af":"sgd_accuracy = accuracy_score(y_test, sgd_svm_pred)\nsgd_accuracy","2bbab474":"sgd_clf = SGDClassifier(loss='log', random_state=42)\nsgd_clf.fit(X_train, y_train)","168b1e8e":"sgd_log_pred = sgd_clf.predict(X_test)\nsgd_log_pred","610635d9":"sgd_log_score = accuracy_score(y_test, sgd_log_pred)\nsgd_log_score","8ac11f7a":"from sklearn.neighbors import KNeighborsClassifier\n\ndef knn_fit(n_neighbors=3, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=-1):\n    knn = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n    knn.fit(X_train, y_train)\n    \n#     print(n_neighbors, weights)\n\n    knn_pred = knn.predict(X_test)\n    \n    print('KNN Score: ', knn.score(X_test, y_test))\n    print('Accuracy:', accuracy_score(y_test, knn_pred))\n    \n","709ff35d":"knn_fit(n_neighbors=5)\n","a86ec3f1":"WOW! we are getting over 97% accuracy by using KNN, and that too without using any hyperparameter tuning. Now, it is all up to you to make the model even better. Try using **`GridSearchCV`** and make some changes to the hyperparameters.","e78e1b90":"## Preparing the Training and Testing Data","2ea092a2":"## Visualizing the first written digit.","321032d2":"* **The cross validation scores seem just okay. Not too bad, not too good either. So, let us look at the scores.**\n\nNow, let us predict using the test set and also see the accuracy scores on the test set.","d9ad1611":"Looks like it is workign alright. We are getting the same answer for both.","a15fb08f":"## Converting into arrays.\n\nConverting the objects into arrays will help us further on while making them into 2D arrays and also accessing the pixel values.","425a325e":"This is a very simple kernel for the famous MNIST dataset. Here, we ar going to see how three different classifiers perform on the data set. \n\nFirst, we will be using the `SGDClassifier` and see how it performs for `loss` = 'hinge' (linear SVM) and `loss` = 'log' (Logistic Regression). \n\nThen, we will train the set using the K-Nearest Neighbor Classifier. We will try to get above 95% accuracy by the end of training.","8f52dc41":"Interesting, we got less score than cross validation. So, the model is generalizing a bit worse with the test set.","1144374e":"So, we are getting better results by using Logistic Regression.  \n**But can we achieve above 95% accuracy somehow? Let us see whether using `KNeighborsClassifier` can give us above 90% accuracy (or maybe we should try for more than 95% accuracy).**","8602acde":"Let us cross-validate our results first by predicting from the data set and also by looking at the scores.","3727c36f":"## Using KNN","0dc777f1":"* Here we will be classifying using the linear SVM classifier.\n* To do so we have to set the `loss` parameter to 'hinge'. This is also set by default, but because we will be comparing it with Logistic Regression, so, it is better to set it manually as well.","23800560":"In this part we will be using KNN classifier to **train** the model, get the **cross-validation** scores, **predict the probabilities** and also get the **final score**.","e4808692":"## Using `loss='log'` for Logistic Regression","c317d84d":"So, for cross-validation scores, we are getting around 87% accuracy.","192a646d":"### Using SGDClassifier with loss = 'hinge'"}}