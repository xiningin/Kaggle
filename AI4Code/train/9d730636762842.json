{"cell_type":{"51508ed1":"code","6755b86f":"code","bd96f73a":"code","cb067348":"code","d151ea6a":"code","f49f154e":"code","59980132":"code","be510a3e":"code","a5d5401e":"code","8dfa824b":"code","f24742ae":"code","06fafbd6":"code","856d8b57":"code","0de70d7a":"code","cb761745":"code","9a89b805":"code","9aa3d9e3":"code","445b732e":"code","e336b10c":"code","406d95d3":"code","ff5cc6cb":"code","0efcab3e":"code","b5cdb653":"code","5c873ef7":"code","ed901eb1":"code","acfcc388":"code","64e4b4bc":"code","50dae0e2":"code","d6644363":"code","fc799d83":"code","ed049f61":"code","c6ad9c30":"code","2328084a":"code","4e24876f":"markdown"},"source":{"51508ed1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6755b86f":"df_train = pd.read_csv('\/kaggle\/input\/ods-mlclass-dubai-2019-03-lecture3-hw\/train.csv')\nprint(df_train.shape)\n\ndf_test = pd.read_csv('\/kaggle\/input\/ods-mlclass-dubai-2019-03-lecture3-hw\/test.csv')\nprint(df_test.shape)","bd96f73a":"df_train.head()","cb067348":"df_test['target'] = np.nan\n\ndf = pd.concat([df_train, df_test])\n\nprint(df.shape)","d151ea6a":"df.head()","f49f154e":"df.tail()","59980132":"df.dtypes","be510a3e":"cats = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n\nnums = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']","a5d5401e":"import itertools as it","8dfa824b":"for i in range(1, 4):\n    print(i)\n    for g in it.combinations(cats, i):\n        df = pd.concat(\n            [\n                df, \n                df.groupby(list(g))[nums].transform('mean').rename(\n                    columns=dict([(s, ':'.join(g) + '__' + s + '__mean') for s in nums])\n                )\n            ], \n            axis=1\n        )","f24742ae":"df.drop(columns=cats, inplace=True)\ndf.shape","06fafbd6":"cols = [c for c in df.columns if c != 'uid' and c != 'target']","856d8b57":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\ndf[cols] = MinMaxScaler().fit_transform(df[cols])","0de70d7a":"df_m = df[cols].corr()","cb761745":"cor = {}\nfor c in cols:\n    cor[c] = set(df_m.loc[c][df_m.loc[c] > 0.5].index) - {c}\n    \nlen(cor)","9a89b805":"for c in cols:\n    if c not in cor:\n        continue\n    for s in cor[c]:\n        if s in cor:\n            cor.pop(s)","9aa3d9e3":"cols = list(cor.keys())\n\nlen(cols)","445b732e":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier","e336b10c":"'''\nX=df.loc[df['target'].notna()][cols]\ny=df.loc[df['target'].notna()]['target']\n# range of k we want to try\nk_range = range(150, 200,3)\n# empty list to store scores\nk_scores = []\n\n# 1. we will loop through reasonable values of k\nfor k in k_range:\n    print(k)\n    # 2. run KNeighborsClassifier with k neighbours\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # 3. obtain cross_val_score for KNeighborsClassifier with k neighbours\n    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n    # 4. append mean of scores for k neighbors to k_scores list\n    k_scores.append(scores.mean())\n\n# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n# plt.plot(x_axis, y_axis)\nplt.plot(k_range, k_scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-validated accuracy')\n'''","406d95d3":"'''\nX=df.loc[df['target'].notna()][cols]\ny=df.loc[df['target'].notna()]['target']\n\n#List Hyperparameters to tune\nleaf_size = list(range(20,50))\nn_neighbors = [100,148,151,153]\np=[1,2]\nweights = ['distance','uniform']\nalgorithm = ['auto','brute']\nmetric = ['minkowski']\nn_jobs = [-1]\n#convert to dictionary\nhyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p, weights=weights, \n                       algorithm=algorithm, metric=metric,n_jobs=n_jobs)\n\nknn = KNeighborsClassifier()\ngridsearch = RandomizedSearchCV(knn, hyperparameters, cv=10)\ngridsearch.fit(X,y)\nprint(\"Best parameter for RandomSearch:\\n\")\nprint(gridsearch.best_params_)\n'''","ff5cc6cb":"'''\nX=df.loc[df['target'].notna()][cols]\ny=df.loc[df['target'].notna()]['target']\n\nparameters = {'criterion': ['gini','entropy'],\n             'max_depth': range (1,100,3)}\n\ndtc = DecisionTreeClassifier()\ngridsearch = GridSearchCV(dtc, param_grid=parameters, cv=10)\ngridsearch.fit(X,y)\nprint(\"Best parameter for GridSearch:\\n\")\nprint(gridsearch.best_params_)\n'''","0efcab3e":"'''\nX=df.loc[df['target'].notna()][cols]\ny=df.loc[df['target'].notna()]['target']\n# range of k we want to try\nd_range = range(10,50,1)\n# empty list to store scores\nd_scores = []\n# 1. we will loop through reasonable values of d\nfor d in d_range:\n    print(d)\n    # 2. run DecisionTreeClassifier with different max_depth\n    dtc = DecisionTreeClassifier(criterion='entropy',max_depth=d)\n    # 3. obtain cross_val_score for DecisionTreeClassifier with different max_depth\n    scores = cross_val_score(dtc, X, y, cv=10, scoring='accuracy')\n    # 4. append mean of scores for max_depth to d_scores list\n    d_scores.append(scores.mean())\n\n# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n# plt.plot(x_axis, y_axis)\nplt.plot(d_range, d_scores)\nplt.xlabel('Value of d for max_depth')\nplt.ylabel('Cross-validated accuracy')\n'''","b5cdb653":"knn_model = KNeighborsClassifier(\n    n_neighbors=153,\n    weights='distance',\n    algorithm='auto',\n    leaf_size=36,\n    p=1,\n    metric='minkowski',\n    n_jobs=-1\n)\n\ndtc_model = DecisionTreeClassifier(\n    criterion='entropy',\n    max_depth=9,\n    splitter='best',\n    max_features=6,\n    min_samples_split=45,\n    min_samples_leaf=10)\n\nknn_model = knn_model.fit(df.loc[df['target'].notna()][cols], df.loc[df['target'].notna()]['target'])\ndtc_model = dtc_model.fit(df.loc[df['target'].notna()][cols], df.loc[df['target'].notna()]['target'])","5c873ef7":"knn_p = knn_model.predict_proba(df.loc[df['target'].isna()][cols])\ndtc_p = dtc_model.predict_proba(df.loc[df['target'].isna()][cols])\nfinalpred=(knn_p*0.25+dtc_p*0.75)","ed901eb1":"%matplotlib inline\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"dark\")","acfcc388":"sns.distplot(knn_p[:, 1])","64e4b4bc":"sns.distplot(dtc_p[:, 1])","50dae0e2":"sns.distplot(finalpred[:, 1])","d6644363":"from sklearn.ensemble import VotingClassifier\nvot_model = VotingClassifier(estimators=[('knn', knn_model), ('dt', dtc_model)], voting='soft')\n\nvot_model.fit(df.loc[df['target'].notna()][cols], df.loc[df['target'].notna()]['target'])\nvot_p = vot_model.predict_proba(df.loc[df['target'].isna()][cols])","fc799d83":"sns.distplot(vot_p[:, 1])","ed049f61":"df_submit_knn = pd.DataFrame({\n    'uid': df.loc[df['target'].isna()]['uid'],\n    'target': knn_p[:, 1]\n})\n\ndf_submit_dtc = pd.DataFrame({\n    'uid': df.loc[df['target'].isna()]['uid'],\n    'target': dtc_p[:, 1]\n})\n\ndf_submit_vot = pd.DataFrame({\n    'uid': df.loc[df['target'].isna()]['uid'],\n    'target': vot_p[:, 1]\n})\n\ndf_submit_mix = pd.DataFrame({\n    'uid': df.loc[df['target'].isna()]['uid'],\n    'target': finalpred[:, 1]\n})","c6ad9c30":"df_submit_knn.to_csv('\/kaggle\/working\/submit_knn.csv', index=False)\ndf_submit_dtc.to_csv('\/kaggle\/working\/submit_dtc.csv', index=False)\ndf_submit_vot.to_csv('\/kaggle\/working\/submit_vop.csv', index=False)\ndf_submit_mix.to_csv('\/kaggle\/working\/submit_mix.csv', index=False)","2328084a":"!head \/kaggle\/working\/submit.csv","4e24876f":"Ikenna Anigbogu - kennason212@gmail.com"}}