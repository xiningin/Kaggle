{"cell_type":{"401b3f98":"code","c5b51075":"code","766872e0":"code","e3912d97":"code","23318f8d":"code","f3853cfc":"code","fb18c4e8":"code","14df3465":"code","1e64c51d":"code","415c6a11":"code","f8c8ca87":"code","fb4b7558":"code","36dd9fd1":"code","250d4f29":"code","00d79527":"code","c9ac113a":"code","3f1cf6c8":"code","570c8c87":"code","e80c08c5":"code","61b64a13":"code","5e0e7899":"code","82e21f99":"code","a1c5445c":"markdown","3be1ec8c":"markdown","a96273f3":"markdown"},"source":{"401b3f98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas_profiling as pp\nimport xgboost as xgb\n# import lightgbm as lgb\nfrom pandas import get_dummies\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import linear_model\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\n# from catboost import CatBoostClassifier,Pool\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\ntrain_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\n\n# Any results you write to the current directory are saved as output.","c5b51075":"train_data.info()","766872e0":"pp.ProfileReport(train_data)","e3912d97":"train_data.shape","23318f8d":"train_data.isnull().any().any()","f3853cfc":"train_data['target'].value_counts()","fb18c4e8":"plt.figure(figsize=(15,6))\nsns.countplot(train_data['target'])","14df3465":"train_data['target'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',shadow=True)\nplt.title('target')","1e64c51d":"X_train = train_data.drop(['id','target'],axis=1)\ny_train = train_data['target']\nX_test = test_data.drop(['id'],axis=1)\n\n#Scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","415c6a11":"ridge = linear_model.Ridge()\nlasso = linear_model.Lasso()\nelastic = linear_model.ElasticNet()\nlasso_lars = linear_model.LassoLars()\nbayesian_ridge = linear_model.BayesianRidge()\nlogistic = linear_model.LogisticRegression(solver='liblinear')\nsgd = linear_model.SGDClassifier()","f8c8ca87":"models = [ridge, lasso, elastic, lasso_lars, bayesian_ridge, logistic, sgd]","fb4b7558":"def get_cv_scores(model):\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n    print('CV Mean: ', np.mean(scores))\n    print('STD: ', np.std(scores))\n    print('\\n')","36dd9fd1":"for model in models:\n    print(model)\n    get_cv_scores(model)","250d4f29":"penalty = ['l1', 'l2']\nC = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\nclass_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\nsolver = ['liblinear', 'saga']\n\nparam_grid = dict(penalty=penalty,\n                  C=C,\n                  class_weight=class_weight,\n                  solver=solver)\n\ngrid = GridSearchCV(estimator=logistic, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1)\ngrid_result = grid.fit(X_train, y_train)\n\nprint('Best Score: ', grid_result.best_score_)\nprint('Best Params: ', grid_result.best_params_)","00d79527":"logistic = linear_model.LogisticRegression(C=1, class_weight={1:0.6, 0:0.4}, penalty='l1', solver='liblinear')\nget_cv_scores(logistic)","c9ac113a":"predictions = logistic.fit(X_train, y_train).predict_proba(X_test)","3f1cf6c8":"predictions","570c8c87":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = predictions\nsubmission.head()","e80c08c5":"loss = ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']\npenalty = ['l1', 'l2', 'elasticnet']\nalpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\nlearning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']\nclass_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\neta0 = [1, 10, 100]\n\nparam_distributions = dict(loss=loss,\n                           penalty=penalty,\n                           alpha=alpha,\n                           learning_rate=learning_rate,\n                           class_weight=class_weight,\n                           eta0=eta0)\n\nrandom = RandomizedSearchCV(estimator=sgd, param_distributions=param_distributions, scoring='roc_auc', verbose=1, n_jobs=-1, n_iter=1000)\nrandom_result = random.fit(X_train, y_train)\n\nprint('Best Score: ', random_result.best_score_)\nprint('Best Params: ', random_result.best_params_)","61b64a13":"sgd = linear_model.SGDClassifier(alpha=0.1,\n                                 class_weight={1:0.7, 0:0.3},\n                                 eta0=100,\n                                 learning_rate='optimal',\n                                 loss='log',\n                                 penalty='elasticnet')\nget_cv_scores(sgd)","5e0e7899":"predictions = sgd.fit(X_train, y_train).predict_proba(X_test)","82e21f99":"predictions","a1c5445c":"Regularization","3be1ec8c":"**Stochastic Gradient Descent and Random Search**","a96273f3":"**Logistic Regression and Grid Search**"}}