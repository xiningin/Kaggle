{"cell_type":{"3ca79dd4":"code","086dc399":"code","42389b02":"code","f8c7b8e1":"code","b29b1ad2":"code","7ef48e7d":"code","a9cfe76d":"code","4d2bbedd":"code","2326a686":"code","6f4bdd4b":"code","70f4d476":"code","e159e5b4":"code","3f6d0a7b":"code","5e275b01":"code","251c73c3":"code","18e8a2df":"code","ad823db7":"code","62e38e2f":"code","470289b8":"code","6dd67d90":"code","cbda21e9":"code","d8191d8a":"code","bc3b179f":"code","58c89b6a":"code","b0135e55":"code","a8d2ef23":"code","84d4cc9f":"code","9ded7d08":"code","e30474f4":"code","94a3c235":"code","12d5fe33":"code","a9f2961b":"code","1e0a6027":"code","d6ddb146":"code","4fb34aa8":"code","2e9a9e92":"code","b9e31cd5":"code","df966e59":"code","03c033e6":"code","617ca179":"markdown","63b68f01":"markdown","c1ba4df0":"markdown","99d39297":"markdown","011c17a8":"markdown","69ec662f":"markdown","f1881904":"markdown","0f1bc01c":"markdown","27a4ae0b":"markdown","f72aa557":"markdown","fbcf522c":"markdown","1fa1ee3b":"markdown","17f34d36":"markdown","102f30c9":"markdown","5fd15a56":"markdown","515cb79a":"markdown","b22d03bd":"markdown","6e9f5362":"markdown","b117ce68":"markdown","6577da67":"markdown","453e7d11":"markdown","126d86c3":"markdown","eaa6a408":"markdown","15d7f87f":"markdown","b91a4fb8":"markdown","6c7f0fe3":"markdown","0b8ca17e":"markdown","17d2a990":"markdown","560ebc0d":"markdown","017dfc8b":"markdown","e7985a63":"markdown","fb6860e9":"markdown","c1e933b8":"markdown","8b694696":"markdown","a59c38d5":"markdown","39d5bc62":"markdown"},"source":{"3ca79dd4":"import pandas as pd\nimport numpy as np\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","086dc399":"df = pd.read_csv(\"\/kaggle\/input\/telecom-churn\/telecom_churn.csv\")","42389b02":"df.head()","f8c7b8e1":"label_ratio = df.Churn.value_counts()\nprint('There are {} churn and {} current customers, or {}% have churned.'.format(label_ratio[1], label_ratio[0], np.round((label_ratio[1]\/label_ratio[0])*100, 1)))","b29b1ad2":"df.describe()","7ef48e7d":"df.info()","a9cfe76d":"df_hue = df.copy()\ndf_hue[\"Churn\"] = np.where(df_hue[\"Churn\"] == 0, \"S\", \"C\")\nsn.pairplot(df_hue.drop(['DataPlan', 'ContractRenewal'], axis=1), hue=\"Churn\", palette=\"husl\")","4d2bbedd":"def calc_vif(X):\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    return(vif)\ncalc_vif(df)","2326a686":"df['average_call_dura'] = df.apply(lambda x: x['DayMins'] \/ x['DayCalls'] if x['DayCalls'] > 0 else 0,axis=1)\n# df['total_monthly_charge'] = df.apply(lambda x: x['MonthlyCharge'] + x['OverageFee'],axis=1)\ndf2 = df.drop(['MonthlyCharge', 'DayCalls', 'DataUsage', 'RoamMins', 'DayMins'],axis=1)\ncalc_vif(df2)","6f4bdd4b":"final_model_cols = ['AccountWeeks', 'ContractRenewal', 'DataPlan', 'CustServCalls', 'OverageFee', 'average_call_dura']","70f4d476":"plt.figure(figsize=(12,9))\ncorrMatrix = df.drop('average_call_dura', axis=1).corr()\nsn.heatmap(corrMatrix, annot=True)\nplt.show()","e159e5b4":"X0 = df[df.DataPlan==0].MonthlyCharge.values.reshape(-1,1)\ny0 = df[df.DataPlan==0].DayMins.values.reshape(-1,1)\nX1 = df[df.DataPlan==1].MonthlyCharge.values.reshape(-1,1)\ny1 = df[df.DataPlan==1].DayMins.values.reshape(-1,1)\n\nX0_train, X0_test, y0_train, y0_test = train_test_split(X0, y0, test_size=0.33, random_state=42)\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.33, random_state=42)\n\nregr0 = LinearRegression()\nregr1 = LinearRegression()\nregr0.fit(X0_train, y0_train)\nregr1.fit(X1_train, y1_train)\ny0_pred = regr0.predict(X0_test)\ny1_pred = regr1.predict(X1_test)\nprint('Coefficients: \\n No data plan y = {}x + {} \\n With data plan y = {}x + {}'.format(regr0.coef_[0][0], regr0.intercept_[0], regr1.coef_[0][0], regr1.intercept_[0]))\nprint('Mean squared error for no data plan: {}; Mean squared error for data plan: {}'.format(mean_squared_error(y0_test, y0_pred), mean_squared_error(y1_test, y1_pred)))\nprint('Coefficient of determination for no data plan: {}; Coefficient of determination for data plan: {}'.format(r2_score(y0_test, y0_pred), r2_score(y1_test, y1_pred)))\n\n\nfig, ax = plt.subplots()\nax.scatter(X0_test, y0_test, color='red', alpha=0.6, label='No data plan train')\nax.plot(X0_test, y0_pred, color='orange', linewidth=2, label='No data plan pred')\nax.scatter(X1_test, y1_test, color='green', alpha=0.6, label='Data plan train')\nax.plot(X1_test, y1_pred, color='blue', linewidth=2, label='Data plan pred')\nax.set_xlabel('MonthlyCharge')\nax.set_ylabel('DayMins')\nplt.legend()\nplt.show()","3f6d0a7b":"X = df.drop('Churn', axis=1)\ny = df.Churn\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nX_filt = X[final_model_cols]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nX_filt_train, X_filt_test, y_train, y_test = train_test_split(X_filt, y, test_size=0.33, random_state=42)\n\nfeature_names = list(X_train.columns.values)","5e275b01":"from sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, auc, roc_curve","251c73c3":"maj_class = DummyClassifier(strategy=\"most_frequent\")","18e8a2df":"maj_class.fit(X_train, y_train)\npreds = maj_class.predict(X_test)\nmaj_class.score(X_test,y_test)\nprint('Score: {}'.format(maj_class.score(X_test, y_test)))\nprint('Cross Val score: {}'.format(np.round(cross_val_score(maj_class, X_test, y_test).mean(),2)))\nprint('ROC AUC Score: {}'.format(roc_auc_score(y_test, preds)))\nfpr, tpr, thresholds = roc_curve(y_test, preds)\nprint('AUC Score: {}'.format(auc(fpr, tpr)))","ad823db7":"confusion_matrix(y_test, preds)","62e38e2f":"from sklearn.ensemble import RandomForestClassifier","470289b8":"RF_class = RandomForestClassifier(random_state=42, class_weight='balanced').fit(X_train, y_train)\npreds = RF_class.predict(X_test)\nprint('Score: {}'.format(RF_class.score(X_test, y_test)))\nprint('Cross Val score: {}'.format(np.round(cross_val_score(RF_class, X_test, y_test).mean(),2)))\nprint('ROC AUC Score: {}'.format(roc_auc_score(y_test, preds)))\nfpr, tpr, thresholds = roc_curve(y_test, preds)\nprint('AUC Score: {}'.format(auc(fpr, tpr)))\nprint(\"\\nFeature Importantce ranking \"+ str(sorted(zip(map(lambda x: round(x, 4), RF_class.feature_importances_), feature_names), \n             reverse=True)))","6dd67d90":"confusion_matrix(y_test, preds)","cbda21e9":"from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score","d8191d8a":"SVC_class = make_pipeline(StandardScaler(), SVC(class_weight='balanced')).fit(X_train, y_train)\npreds = SVC_class.predict(X_test)\nprint('Score: {}'.format(SVC_class.score(X_test, y_test)))\nprint('Cross Val score: {}'.format(np.round(cross_val_score(SVC_class, X_test, y_test).mean(),2)))\nprint('ROC AUC Score: {}'.format(roc_auc_score(y_test, preds)))\nfpr, tpr, thresholds = roc_curve(y_test, preds)\nprint('AUC Score: {}'.format(auc(fpr, tpr)))","bc3b179f":"confusion_matrix(y_test, preds)","58c89b6a":"from sklearn.ensemble import ExtraTreesClassifier","b0135e55":"ET_class = ExtraTreesClassifier(random_state=42, class_weight='balanced').fit(X_train, y_train)\npreds = ET_class.predict(X_test)\nprint('Score: {}'.format(ET_class.score(X_test, y_test)))\nprint('Cross Val score: {}'.format(np.round(cross_val_score(ET_class, X_test, y_test).mean(),2)))\nprint('ROC AUC Score: {}'.format(roc_auc_score(y_test, preds)))\nfpr, tpr, thresholds = roc_curve(y_test, preds)\nprint('AUC Score: {}'.format(auc(fpr, tpr)))\nprint(\"\\nFeature Importantce ranking \"+ str(sorted(zip(map(lambda x: round(x, 4), ET_class.feature_importances_), feature_names), \n             reverse=True)))","a8d2ef23":"confusion_matrix(y_test, preds)","84d4cc9f":"from xgboost import XGBClassifier","9ded7d08":"XG_class = XGBClassifier(eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric='auc').fit(X_train, y_train)\npreds = XG_class.predict(X_test)\nprint('Score: {}'.format(XG_class.score(X_test, y_test)))\nprint('Cross Val score: {}'.format(np.round(cross_val_score(XG_class, X_test, y_test).mean(),2)))\nprint('ROC AUC Score: {}'.format(roc_auc_score(y_test, preds)))\nfpr, tpr, thresholds = roc_curve(y_test, preds)\nprint('AUC Score: {}'.format(auc(fpr, tpr)))\nprint(\"\\nFeature Importantce ranking \"+ str(sorted(zip(map(lambda x: round(x, 4), XG_class.feature_importances_), feature_names), \n             reverse=True)))","e30474f4":"import scipy.stats as ss\n        \ndef compare_filt_scores(classifier):\n    clf = classifier.fit(X_train, y_train)\n    print('no filter: {}'.format(np.round(clf.score(X_test, y_test), 3)))\n    print('Cross Val score: {}'.format(np.round(cross_val_score(clf, X_test, y_test).mean(),3)))\n    feature_names = list(X_train.columns.values)\n    try:\n        print(\"Feature Importantce ranking \"+ str(sorted(zip(map(lambda x: round(x, 4), clf.feature_importances_), feature_names), reverse=True))+\"\\n\")\n    except AttributeError as a:\n        pass\n    \n    clf_filt = classifier.fit(X_filt_train, y_train)\n    print('with filter: {}'.format(np.round(clf_filt.score(X_filt_test, y_test), 3)))\n    print('Cross Val score: {}'.format(np.round(cross_val_score(clf_filt, X_filt_test, y_test).mean(),3)))\n    feature_names = list(X_filt_train.columns.values)\n    try:\n        print(\"Feature Importantce ranking \"+ str(sorted(zip(map(lambda x: round(x, 4), clf.feature_importances_), feature_names), reverse=True))+\"\\n\")\n    except AttributeError as a:\n        pass\n    ","94a3c235":"print('majority class: 0.85')\nprint('')\nprint('RF classifier')\ncompare_filt_scores(RandomForestClassifier(random_state=42, class_weight='balanced'))\nprint('')\nprint('SVC classifier')\ncompare_filt_scores(make_pipeline(StandardScaler(), SVC(class_weight='balanced')))\nprint('')\nprint('ET classifier')\ncompare_filt_scores(ExtraTreesClassifier(random_state=42, class_weight='balanced'))\nprint('')\nprint('XG classifier')\ncompare_filt_scores(XGBClassifier())\nprint('')","12d5fe33":"df.Churn.value_counts()","a9f2961b":"56.22 * 258","1e0a6027":"threshold = 0.2\n\npredicted_proba = XG_class.predict_proba(X_test)\npredicted = (predicted_proba [:,1] >= threshold).astype('int')\n\naccuracy = accuracy_score(y_test, predicted)\nprint(accuracy)\ncm = confusion_matrix(y_test, predicted)\nprint(cm)\n\nprint(\"correctly classified {} churners and {} non churners.\".format(cm[0][0], cm[1][1]))\nprint(\"{} classified as churners when not, represents {} in potential churn driven loss\".format(cm[1][0], cm[1][0]*56.22))\nprint(\"{} classified as not churners when they were, represents {} missed retention opppertunity loss\".format(cm[0][1], cm[0][1]*56.22))","d6ddb146":"predicted_proba = XG_class.predict_proba(X_test)\npos = [x[1] for x in predicted_proba]\nns_probs = [0 for _ in range(len(y_test))]\n\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, pos)\n\nlr_auc = roc_auc_score(y_test, pos)\nprint('XGBoost: ROC AUC=%.3f' % (lr_auc))\n\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\nplt.plot(lr_fpr, lr_tpr, marker='.', label='XGBoost')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","4fb34aa8":"from sklearn.metrics import precision_recall_curve, f1_score","2e9a9e92":"yhat = XG_class.predict(X_test)\n\nlr_precision, lr_recall, _ = precision_recall_curve(y_test, pos)\nlr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\n\nprint('XGBoost: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\nplt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()\nplt.show()","b9e31cd5":"def to_labels(pos_probs, threshold):\n\treturn (pos_probs >= threshold).astype('int')","df966e59":"predicted_proba = XG_class.predict_proba(X_test)\nprobs = predicted_proba[:, 1]\nthresholds = np.arange(0, 1, 0.001)\nscores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]\nix = np.argmax(scores)\nprint('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))\nplt.plot(thresholds, scores)\nplt.vlines(thresholds[ix], 0, scores[ix]+0.05*scores[ix])\nplt.hlines(scores[ix], 0, 1)\nplt.xlabel('Thresholds')\nplt.ylabel('F1 score')","03c033e6":"threshold = 0.614\n\npredicted_proba = XG_class.predict_proba(X_test)\npredicted = (predicted_proba [:,1] >= threshold).astype('int')\n\naccuracy = accuracy_score(y_test, predicted)\nprint(accuracy)\ncm = confusion_matrix(y_test, predicted)\nprint(cm)\n\nprint('potential to save: {}; ${}'.format(len(y_test[y_test==1]), len(y_test[y_test==1])*56.22))\nprint(\"correctly classified TN: {} churners and TP: {} non churners.\".format(cm[0][0], cm[1][1]))\nprint(\"FP: {} classified as churners when not, represents {} in potential churn driven loss\".format(cm[1][0], cm[1][0]*56.22))\nprint(\"FN: {} classified as not churners when they were, represents {} missed retention opppertunity loss\".format(cm[0][1], cm[0][1]*56.22))\nprint('If we assume you always save 50% of TP, drive 20% of FP customers away, and lose all FN we can see the new loss')\nsaved = (int(0.5*cm[1][1]) - int(0.2*cm[1][0]) - cm[0][1])\nsaved_d = (int(0.5*cm[1][1]) - int(0.2*cm[1][0]) - cm[0][1])*56.22\nprint('Saved: {}; ${}; {}%'.format(saved, saved_d, (saved\/len(y_test[y_test==1]))*100))","617ca179":"Each row represents a customer so the index can be used here as their ID","63b68f01":"import libraries","c1ba4df0":"#### Extra-trees Classifier","99d39297":"Looks like the filtered results perform worse than the entire set.","011c17a8":"Churn is typically a highly imbalenced classification problem with many people not churning and few churners. The dataset here is imbalenced but only ~ 1:6, which is not too bad.","69ec662f":"The features distributions are largely gaussian, nice, no need to log transform","f1881904":"As this is an imbalenced problem we will use the Presision Recall plot to get the optimal threshold","0f1bc01c":"## Churn of Telecom Customers","27a4ae0b":"We'll that's better than the SVC but not quite as good as the RFC. As RFC is the more widely used model I would recommend using that in this case.","f72aa557":"We need to find the optimal threshold here, lets look at the ROC and AUC curves","fbcf522c":"#### Take a look","1fa1ee3b":"a few binaries (Churn, ContractRenewal, DataPlan), ints(AccountWeeks, CustServCalls, DayCalls) and floats(the rest). The max term is nearly 5 years, but most people spend ~2 years in an account. Most customers use 0.82 GB of data and 179 minutes over 100 seperate calls monthly, this also means the average call duration is 1.8 minutes.","17f34d36":"Well that improved things, 92%, not too bad. the confusion matrix could be balanced a bit better.","102f30c9":"From the above we can se that 'AccountWeeks', 'ContractRenewal', 'DataPlan', 'CustServCalls', 'OverageFee', 'average_call_dura' are the features we should use in our final model to avoid a large amount of multicollinearlity.","5fd15a56":"Read in data","515cb79a":"In our example, you can use a majority class classifier to get 85% accuracy. This means you will, without your knowledge, ignore 15%, or 483 customers in this dataset, that will churn every month. \n\nA customer is worth on average at least \\\\$56.22, the churn then represents a monthly loss of 56.22 * 483 = \\\\$27,155.52 revenue. \n\nThis loss is what we hope to recover. By informing a business of potential churners, that business can then take action to retain those customers.\n\nOur best model (XGBoost Classifier) here has an accuracy of 93%. The increase of 8% accuracy over the major class classifier means that 8\/15 * 483 ~ 258 churning customers of the 483 that do churn could have been identified. \n\nThese identified customers represent at least 56.22 * 258 = \\$14,504.76 of revenue that could be potentially retained by \"saving\" these customers from churning. ","b22d03bd":"### Begin Modelling","6e9f5362":"We can see that we have a number of features with very high multicollinearity. let's try dropping a few and see if that makes things better.","b117ce68":"#### Let's compare results with the filtered columns","6577da67":"#### Majority Class Classifier","453e7d11":"So let try a Random Forest Classifier, it's basic, easy to understand, and is used widely for this kind of problem.","126d86c3":"#### Random Forest Classifier","eaa6a408":"Random forest was successful on just default, let's see if we can increase accuracy using a SVC.","15d7f87f":"Looks like there is a very strong (+)ve correlation with DataUsage and DataPlan, which makes sense, without a data plan you can't use data. The next strongest correlation is (+)ve between Usage (DataPlan, DataUsage, DayMins) and MonthlyCharge, there are two seperate groups in DayMins v MonthlyCharge, likely caused by those with a data plan and those without, see below for the plot of these two groups in this context. We also have strong (+) correlation between OverageFee and MonthlyCharge, this could be caused by higher spending customers being more likely to incur and overage charge, this is weakly seen in the scatter plot between these two.","b91a4fb8":"#### Support Vector Classifier","6c7f0fe3":"#### Takeaways from above","0b8ca17e":"Just as a benchline I'll use a majority class classifier. This type of classifier for churn can have suprising levels of accuracy due to the fact that Churn data is highly unbalenced. This type of classifier also largely reflects business understanding of churn when no model is present, i.e. assume no one churns. This dataset has approximately 16.9% of customers labelled as churn, we are therefore expecting an accuracy below of approximately 83.1%, this value will vary based on the test sample.","17d2a990":"#### Take a peek","560ebc0d":"#### Predict Probabilities to get a list of most likely to churn","017dfc8b":"I've seen some people talking about using an Extra-trees Classifier for problems like this one, let's try it.","e7985a63":"From the above plot, we can say that generally data plan users have lower daily call minutes for the same monthly charge when compared to customers with a data plan. We can also see that the daily call minutes for customers with no plan increase faster as the monthly charge increase than customers with a data plan.","fb6860e9":"Lets find the best threshold using the F1 score","c1e933b8":"The above results use the entire feature list, including any \"engineered\" features. ","8b694696":"Well, not as good as the RF classifier but still better than the majority class classifier.","a59c38d5":"The above analysis shows us that this is an imbalenced classification problem, but the imbalence is not too large, I will work with this data as though it is balenced data unless I find a reason to think it's not. \nThe data is all binary or numeric.\nA new feature was created, average_call_dura which is just the average minutes over the average call count.\nThere are is a lot of multicollinearality between features. By iterativily removing those with the highest varience inflation factor. This left AccountWeeks, DataUsage, CustServCalls, average_call_dura as the only features in the data set. This may be too few to train with but we'll see moving forward.","39d5bc62":"The binary label for churn is not exactly the greatest in this example, we really only want people who are very likely (>~80%) to churn not those who are 51% likely too. We can look at the probablities for this and create a new threshold to label churn."}}