{"cell_type":{"8f97c263":"code","2f60c09f":"code","1a1f61aa":"code","9a26fe08":"code","4fffc0be":"code","a559ca7b":"code","c6cf5793":"code","babdf2a3":"code","9379a1c3":"code","a6bb91fb":"code","70a6c47e":"code","ebcbb9cd":"code","887ae983":"markdown","d7562caf":"markdown","c19b9818":"markdown","5e9423e9":"markdown","72c8b9a6":"markdown","8ad8479a":"markdown","111b5914":"markdown","900b2ac0":"markdown","7dcc18ca":"markdown","69de3d3c":"markdown","5461d613":"markdown","29d05bcf":"markdown"},"source":{"8f97c263":"import pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# Don't Show Warning Messages\nimport warnings\nwarnings.filterwarnings('ignore')","2f60c09f":"os.listdir('..\/input')","1a1f61aa":"# set up the canvas for the subplots\nplt.figure(figsize=(20,20))\nplt.tight_layout()\nplt.axis('Off')\n\n# Our subplot will contain 1 row and 2 columns\n# plt.subplot(nrows, ncols, plot_number)\n\n# style image\nplt.subplot(2,2,1)\npath = '..\/input\/art-by-ai-neural-style-transfer\/style_image.jpg'\nimage = plt.imread(path)\nplt.title('Style Image', fontsize=20)\nplt.imshow(image)\nplt.axis('off')\n\n\n# content image\nplt.subplot(2,2,2)\nimage_id = '1803270348-00000112.jpg'\n\nfolder_id = 1803270348\npath = '..\/input\/aisegmentcom-matting-human-datasets\/matting_human_half\/clip_img\/' + \\\nstr(folder_id) + '\/clip_00000000\/' + image_id\nmask = plt.imread(path)\nplt.title('Content Image\/Target Image', fontsize=20)\nplt.imshow(mask)\nplt.axis('off')\n\nplt.show()","9a26fe08":"# ==================================== #\n\ntotal_variation_weight = 1e-4\nstyle_weight = 1.0\ncontent_weight = 0.025\n\niterations = 10\n\n# ==================================== #","4fffc0be":"from keras.preprocessing.image import load_img, img_to_array\n\n# Path to the image you want to transform\nimage_id = '1803270348-00000112.jpg'\nfolder_id = 1803270348\ntarget_image_path = '..\/input\/aisegmentcom-matting-human-datasets\/matting_human_half\/clip_img\/' + \\\nstr(folder_id) + '\/clip_00000000\/' + image_id\n\n# Path to the style image\nstyle_reference_image_path = '..\/input\/art-by-ai-neural-style-transfer\/style_image.jpg'\n\n# Dimensions of the generated picture\nwidth, height = load_img(target_image_path).size\nimg_height = 400\nimg_width = int(width * img_height \/ height)\n\nprint('img_height: ', img_height)\nprint('img_width: ', img_width)","a559ca7b":"# Auxiliary functions for loading, preprocessing and\n# postprocessing the images that go in and out of VGG19.\n\nimport numpy as np\nfrom keras.applications import vgg19\n\ndef preprocess_image(image_path):\n    img = load_img(image_path, target_size=(img_height, img_width))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    \n    # This step pre-processes the image in the exact same way as\n    # the original Imagenet images were preprocessed when VGG19 was pre-trained.\n    img = vgg19.preprocess_input(img)\n    return img\n\ndef deprocess_image(x):\n    # Zero-centering by removing the mean pixel value from ImageNet. \n    # This reverses a transformation done by vgg19.preprocess_input.\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    \n    # Converts images from 'BGR' to 'RGB'.\n    # This is also part of the reversal of vgg19.preprocess_input.\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x\n\n","c6cf5793":"from keras import backend as K\n\ntarget_image = K.constant(preprocess_image(target_image_path)) \nstyle_reference_image = K.constant(preprocess_image(style_reference_image_path)) \n\n# placeholder that will contain the generated image\ncombination_image = K.placeholder((1, img_height, img_width, 3))\n\n# Combines the three images in a single batch\ninput_tensor = K.concatenate([target_image, style_reference_image,\n                              combination_image], axis=0)\n\n# Builds the VGG19 network with the batch of three images as input. \n# The model will be loaded with pretrained ImageNet weights.\nmodel = vgg19.VGG19(input_tensor=input_tensor, \n                    weights='imagenet',include_top=False) \n\nprint('Model loaded.')","babdf2a3":"# Content loss\n\ndef content_loss(base, combination):\n    return K.sum(K.square(combination - base))\n\n\n\n# Style loss\n\ndef gram_matrix(x):\n    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n    gram = K.dot(features, K.transpose(features))\n    return gram\n\ndef style_loss(style, combination):\n        S = gram_matrix(style)\n        C = gram_matrix(combination)\n        channels = 3\n        size = img_height * img_width\n        return K.sum(K.square(S - C)) \/ (4. * (channels ** 2) * (size ** 2))\n\n\n    \n# Total variation loss\n\ndef total_variation_loss(x):\n    a = K.square(\n        x[:, :img_height - 1, :img_width - 1, :] - \\\n        x[:, 1:, :img_width - 1, :])\n    b = K.square(\n        x[:, :img_height - 1, :img_width - 1, :] - \\\n        x[:, :img_height - 1, 1:, :])\n    return K.sum(K.pow(a + b, 1.25))\n\n\n\n\n# Final loss\n\n# Dictionary that maps layer names to activation tensors\noutputs_dict = dict([(layer.name, layer.output) for layer in model.layers]) \n\n# layer used for content loss\ncontent_layer = 'block5_conv2'\n\n# layers used for style loss\nstyle_layers = ['block1_conv1',\n            'block2_conv1',\n            'block3_conv1',\n            'block4_conv1',\n            'block5_conv1']\n\n\n# Weights and weighted average of the loss components.\n# I've moved these lines of code to the top of this notebook.\n#total_variation_weight = 1e-4\n#style_weight = 1.\n#content_weight = 0.025\n\n\n# You\u2019ll define the loss by adding all components to this scalar variable.\nloss = K.variable(0.)\n\n# adds the content loss\nlayer_features = outputs_dict[content_layer]\ntarget_image_features = layer_features[0, :, :, :]\ncombination_features = layer_features[2, :, :, :]\nloss += content_weight * content_loss(target_image_features,\n                                      combination_features)\n\n# adds the style loss component for each target layer\nfor layer_name in style_layers:\n    layer_features = outputs_dict[layer_name]\n    style_reference_features = layer_features[1, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    sl = style_loss(style_reference_features, combination_features)\n    loss += (style_weight \/ len(style_layers)) * sl\n    \n# adds the total variation loss\nloss += total_variation_weight * total_variation_loss(combination_image)\n\n","9379a1c3":"# Gets the gradients of the generated image with regard to the loss\ngrads = K.gradients(loss, combination_image)[0]\n\n# Function to fetch the values of the current loss and the current gradients\nfetch_loss_and_grads = K.function([combination_image], [loss, grads])\n\n\n# This class wraps fetch_loss_and_grads in a way that lets you retrieve the \n# losses and gradients via two separate method calls, \n# which is required by the SciPy optimizer you'll use.\nclass Evaluator(object):\n    def __init__(self):\n        self.loss_value = None\n        self.grads_values = None\n    def loss(self, x):\n        assert self.loss_value is None\n        x = x.reshape((1, img_height, img_width, 3))\n        outs = fetch_loss_and_grads([x])\n        loss_value = outs[0]\n        grad_values = outs[1].flatten().astype('float64')\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        return self.loss_value\n    def grads(self, x):\n        assert self.loss_value is not None\n        grad_values = np.copy(self.grad_values)\n        self.loss_value = None\n        self.grad_values = None\n        return grad_values\n    \nevaluator = Evaluator()\n","a6bb91fb":"from scipy.optimize import fmin_l_bfgs_b\n\n# imsave is not working now, possibly due to a Kaggle package update.\n# Use PIL instead.\n#from scipy.misc import imsave\n\nfrom PIL import Image\nimport time\n\nresult_prefix = 'my_result'\n\n# I've moved this variable to the top of this notebook.\n# iterations = 20\n\n# This is the initial state: the target image.\nx = preprocess_image(target_image_path) \n\n# You flatten the image because scipy.optimize.fmin_l_bfgs_b \n# can only process flat vectors.\nx = x.flatten()\n\nfor i in range(iterations):\n    \n    print('Start of iteration', i)\n    start_time = time.time()\n    \n    # Runs L-BFGS optimization over the pixels of the generated image to \n    # minimize the neural style loss. Note that you have to pass the function \n    # that computes the loss and the function that computes the gradients \n    # as two separate arguments.\n    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x, fprime=evaluator.grads, maxfun=20)\n    \n    print('Current loss value:', min_val)\n    \n    # Saves the current generated image.\n    img = x.copy().reshape((img_height, img_width, 3))\n    img = deprocess_image(img)\n    fname = result_prefix + '_at_iteration_%d.png' % i\n    \n    # not working anymore\n    # imsave(fname, img)\n    \n    # save the images using PIL\n    result = Image.fromarray(img.astype(np.uint8))\n    result.save(fname)\n    print('Image saved as', fname)\n    \n    end_time = time.time()\n    print('Iteration %d completed in %ds' % (i, end_time - start_time))","70a6c47e":"!ls","ebcbb9cd":"# set up the canvas for the subplots\nplt.figure(figsize=(20,20))\nplt.tight_layout()\nplt.axis('Off')\n\n# Our subplot will contain 2 rows and 2 columns\n# plt.subplot(nrows, ncols, plot_number)\n\n# style image\nplt.subplot(2,2,1)\n\n# change this path to see an image from a different iteration\npath = 'my_result_at_iteration_9.png'\n\nimage = plt.imread(path)\nplt.title('Art Image', fontsize=20)\nplt.imshow(image)\nplt.axis('off')\n\n# style image\nplt.subplot(2,2,3)\npath = '..\/input\/art-by-ai-neural-style-transfer\/style_image.jpg'\nimage = plt.imread(path)\nplt.title('Style Image', fontsize=20)\nplt.imshow(image)\nplt.axis('off')\n\n\n# content image\nplt.subplot(2,2,4)\nimage_id = '1803270348-00000112.jpg'\n\nfolder_id = 1803270348\npath = '..\/input\/aisegmentcom-matting-human-datasets\/matting_human_half\/clip_img\/' + \\\nstr(folder_id) + '\/clip_00000000\/' + image_id\nmask = plt.imread(path)\nplt.title('Content Image\/Target Image', fontsize=20)\nplt.imshow(mask)\nplt.axis('off')\n\nplt.show()","887ae983":"This art image is the output from iteration 9.","d7562caf":"### Define helper functions","c19b9818":"### Define the Losses","5e9423e9":"### Set up VGG19","72c8b9a6":"<hr>","8ad8479a":"### Display One Art Image","111b5914":"### Set the output width and height","900b2ac0":"### Set up gradient descent","7dcc18ca":"I copied this Neural Style Transfer workflow from chapter 8 of Francois Chollet's book, Deep Learning with Python. Later, I discovered that this [style transfer code](https:\/\/keras.io\/examples\/neural_style_transfer\/) is also included in the Keras docs.\n\nIf you would like to understand Neural Style Transfer theory, Andrew Ng's deep learning course explains the fundamnental concepts like style image, content image and losses.\n\nThe code in this notebook is set up to run for 10 iterations. It saves the image that is generated at the end of each iteration. Before running the code please ensure that the GPU and Internet are switched on in your kernel.\n\n**Helpful Style Transfer Resources**\n\n1. Course<br>\nAndrew Ng Deep Learning Specialization<br>\nCourse 4: Convolutional Neural Networks<br>\nhttps:\/\/www.coursera.org\/specializations\/deep-learning\n\n2. Book<br>\nFrancois Chollet<br>\nDeep Learning with Python<br>\nhttps:\/\/www.manning.com\/books\/deep-learning-with-python\n\n\n**Image Credit**\n\n*Style Image*<br>\nPhoto by Jean-Philippe Delberghe on Unsplash\n","69de3d3c":"## Introduction","5461d613":"### List the art image file names","29d05bcf":"### Set Parameters"}}