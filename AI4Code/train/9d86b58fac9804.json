{"cell_type":{"9d00cf26":"code","fc82b8ed":"code","e6c13597":"code","982437ea":"code","0c9cbd15":"code","121dfda5":"code","c8e7fc58":"code","32c45a2a":"code","bf2143ae":"code","c585728e":"code","f2b26951":"code","2a88045d":"code","5316541f":"code","351a726f":"code","93f73659":"code","76db23b5":"code","954cfa67":"code","64a635ea":"code","4e306aa6":"code","78e9a366":"code","1c83c056":"code","8ca1cd55":"code","c79b476a":"code","cb01c39b":"code","ff487e4b":"code","01de28b0":"code","c784762e":"code","c1d70fbd":"code","d16754e2":"code","da4fe52a":"code","1db5004c":"code","038a333b":"code","20123da1":"code","7d5afd6d":"code","372cc1ac":"code","534ef467":"code","ca6c5717":"code","992fd654":"code","e2d9187f":"code","27b8f8e9":"code","c183311d":"code","0f40795e":"code","26dba3da":"code","7a40f969":"markdown","25cb69fb":"markdown","deeb8308":"markdown","644ee635":"markdown","8932605b":"markdown","1ef34820":"markdown","02c12f8b":"markdown","1df985a9":"markdown","7c9286e2":"markdown","ecbe6455":"markdown","a4e72e67":"markdown"},"source":{"9d00cf26":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.models import resnet18\n\nimport string\nfrom tqdm.notebook import tqdm\nimport cv2\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport multiprocessing as mp","fc82b8ed":"cpu_count = mp.cpu_count()\nprint(cpu_count)","e6c13597":"data_path = \"\/kaggle\/input\/captcha-version-2-images\/samples\"","982437ea":"image_fns = os.listdir(data_path)\nprint(len(image_fns))\nprint(np.unique([len(image_fn.split(\".\")[0]) for image_fn in image_fns]))","0c9cbd15":"for idx, image_fn in enumerate(image_fns):\n    if len(image_fn.split(\".\")[0]) != 5:\n           print(idx, image_fn)","121dfda5":"image_fns.remove('samples')\nprint(len(image_fns))","c8e7fc58":"image_fns_train, image_fns_test = train_test_split(image_fns, random_state=0)\nprint(len(image_fns_train), len(image_fns_test))","32c45a2a":"image_ns = [image_fn.split(\".\")[0] for image_fn in image_fns]\nimage_ns = \"\".join(image_ns)\nletters = sorted(list(set(list(image_ns))))\nprint(len(letters))\nprint(letters)","bf2143ae":"vocabulary = [\"-\"] + letters\nprint(len(vocabulary))\nprint(vocabulary)\nidx2char = {k:v for k,v in enumerate(vocabulary, start=0)}\nprint(idx2char)\nchar2idx = {v:k for k,v in idx2char.items()}\nprint(char2idx)","c585728e":"batch_size = 16","f2b26951":"class CAPTCHADataset(Dataset):\n    \n    def __init__(self, data_dir, image_fns):\n        self.data_dir = data_dir\n        self.image_fns = image_fns\n        \n    def __len__(self):\n        return len(self.image_fns)\n    \n    def __getitem__(self, index):\n        image_fn = self.image_fns[index]\n        image_fp = os.path.join(self.data_dir, image_fn)\n        image = Image.open(image_fp).convert('RGB')\n        image = self.transform(image)\n        text = image_fn.split(\".\")[0]\n        return image, text\n    \n    def transform(self, image):\n        \n        transform_ops = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n        ])\n        return transform_ops(image)","2a88045d":"trainset = CAPTCHADataset(data_path, image_fns_train) \ntestset = CAPTCHADataset(data_path, image_fns_test)\ntrain_loader = DataLoader(trainset, batch_size=batch_size, num_workers=cpu_count, shuffle=True)\ntest_loader = DataLoader(testset, batch_size=batch_size, num_workers=cpu_count, shuffle=False)\nprint(len(train_loader), len(test_loader))","5316541f":"image_batch, text_batch = iter(train_loader).next()\nprint(image_batch.size(), text_batch)","351a726f":"num_chars = len(char2idx)\nprint(num_chars)\nrnn_hidden_size = 256","93f73659":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","76db23b5":"resnet = resnet18(pretrained=True)\n#print(resnet)","954cfa67":"class CRNN(nn.Module):\n    \n    def __init__(self, num_chars, rnn_hidden_size=256, dropout=0.1):\n        \n        super(CRNN, self).__init__()\n        self.num_chars = num_chars\n        self.rnn_hidden_size = rnn_hidden_size\n        self.dropout = dropout\n        \n        # CNN Part 1\n        resnet_modules = list(resnet.children())[:-3]\n        self.cnn_p1 = nn.Sequential(*resnet_modules)\n        \n        # CNN Part 2\n        self.cnn_p2 = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=(3,6), stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True)\n        )\n        self.linear1 = nn.Linear(1024, 256)\n        \n        # RNN\n        self.rnn1 = nn.GRU(input_size=rnn_hidden_size, \n                            hidden_size=rnn_hidden_size,\n                            bidirectional=True, \n                            batch_first=True)\n        self.rnn2 = nn.GRU(input_size=rnn_hidden_size, \n                            hidden_size=rnn_hidden_size,\n                            bidirectional=True, \n                            batch_first=True)\n        self.linear2 = nn.Linear(self.rnn_hidden_size*2, num_chars)\n        \n        \n    def forward(self, batch):\n        \n        batch = self.cnn_p1(batch)\n        # print(batch.size()) # torch.Size([-1, 256, 4, 13])\n        \n        batch = self.cnn_p2(batch) # [batch_size, channels, height, width]\n        # print(batch.size())# torch.Size([-1, 256, 4, 10])\n        \n        batch = batch.permute(0, 3, 1, 2) # [batch_size, width, channels, height]\n        # print(batch.size()) # torch.Size([-1, 10, 256, 4])\n         \n        batch_size = batch.size(0)\n        T = batch.size(1)\n        batch = batch.view(batch_size, T, -1) # [batch_size, T==width, num_features==channels*height]\n        # print(batch.size()) # torch.Size([-1, 10, 1024])\n        \n        batch = self.linear1(batch)\n        # print(batch.size()) # torch.Size([-1, 10, 256])\n        \n        batch, hidden = self.rnn1(batch)\n        feature_size = batch.size(2)\n        batch = batch[:, :, :feature_size\/\/2] + batch[:, :, feature_size\/\/2:]\n        # print(batch.size()) # torch.Size([-1, 10, 256])\n        \n        batch, hidden = self.rnn2(batch)\n        # print(batch.size()) # torch.Size([-1, 10, 512])\n        \n        batch = self.linear2(batch)\n        # print(batch.size()) # torch.Size([-1, 10, 20])\n        \n        batch = batch.permute(1, 0, 2) # [T==10, batch_size, num_classes==num_features]\n        # print(batch.size()) # torch.Size([10, -1, 20])\n        \n        return batch","64a635ea":"def weights_init(m):\n    classname = m.__class__.__name__\n    if type(m) in [nn.Linear, nn.Conv2d, nn.Conv1d]:\n        torch.nn.init.xavier_uniform_(m.weight)\n        if m.bias is not None:\n            m.bias.data.fill_(0.01)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)","4e306aa6":"crnn = CRNN(num_chars, rnn_hidden_size=rnn_hidden_size)\ncrnn.apply(weights_init)\ncrnn = crnn.to(device)","78e9a366":"#crnn","1c83c056":"text_batch_logits = crnn(image_batch.to(device))\nprint(text_batch)\nprint(text_batch_logits.shape)","8ca1cd55":"criterion = nn.CTCLoss(blank=0)","c79b476a":"def encode_text_batch(text_batch):\n    \n    text_batch_targets_lens = [len(text) for text in text_batch]\n    text_batch_targets_lens = torch.IntTensor(text_batch_targets_lens)\n    \n    text_batch_concat = \"\".join(text_batch)\n    text_batch_targets = [char2idx[c] for c in text_batch_concat]\n    text_batch_targets = torch.IntTensor(text_batch_targets)\n    \n    return text_batch_targets, text_batch_targets_lens","cb01c39b":"def compute_loss(text_batch, text_batch_logits):\n    \"\"\"\n    text_batch: list of strings of length equal to batch size\n    text_batch_logits: Tensor of size([T, batch_size, num_classes])\n    \"\"\"\n    text_batch_logps = F.log_softmax(text_batch_logits, 2) # [T, batch_size, num_classes]  \n    text_batch_logps_lens = torch.full(size=(text_batch_logps.size(1),), \n                                       fill_value=text_batch_logps.size(0), \n                                       dtype=torch.int32).to(device) # [batch_size] \n    #print(text_batch_logps.shape)\n    #print(text_batch_logps_lens) \n    text_batch_targets, text_batch_targets_lens = encode_text_batch(text_batch)\n    #print(text_batch_targets)\n    #print(text_batch_targets_lens)\n    loss = criterion(text_batch_logps, text_batch_targets, text_batch_logps_lens, text_batch_targets_lens)\n\n    return loss","ff487e4b":"compute_loss(text_batch, text_batch_logits)","01de28b0":"num_epochs = 50\nlr = 0.001\nweight_decay = 1e-3\nclip_norm = 5","c784762e":"optimizer = optim.Adam(crnn.parameters(), lr=lr, weight_decay=weight_decay)\nlr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=5)","c1d70fbd":"crnn = CRNN(num_chars, rnn_hidden_size=rnn_hidden_size)\ncrnn.apply(weights_init)\ncrnn = crnn.to(device)","d16754e2":"epoch_losses = []\niteration_losses = []\nnum_updates_epochs = []\nfor epoch in tqdm(range(1, num_epochs+1)):\n    epoch_loss_list = [] \n    num_updates_epoch = 0\n    for image_batch, text_batch in tqdm(train_loader, leave=False):\n        optimizer.zero_grad()\n        text_batch_logits = crnn(image_batch.to(device))\n        loss = compute_loss(text_batch, text_batch_logits)\n        iteration_loss = loss.item()\n\n        if np.isnan(iteration_loss) or np.isinf(iteration_loss):\n            continue\n          \n        num_updates_epoch += 1\n        iteration_losses.append(iteration_loss)\n        epoch_loss_list.append(iteration_loss)\n        loss.backward()\n        nn.utils.clip_grad_norm_(crnn.parameters(), clip_norm)\n        optimizer.step()\n\n    epoch_loss = np.mean(epoch_loss_list)\n    print(\"Epoch:{}    Loss:{}    NumUpdates:{}\".format(epoch, epoch_loss, num_updates_epoch))\n    epoch_losses.append(epoch_loss)\n    num_updates_epochs.append(num_updates_epoch)\n    lr_scheduler.step(epoch_loss)","da4fe52a":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\nax1.plot(epoch_losses)\nax1.set_xlabel(\"Epochs\")\nax1.set_ylabel(\"Loss\")\n\nax2.plot(iteration_losses)\nax2.set_xlabel(\"Iterations\")\nax2.set_ylabel(\"Loss\")\n\nplt.show()","1db5004c":"def decode_predictions(text_batch_logits):\n\n    text_batch_tokens = F.softmax(text_batch_logits, 2).argmax(2) # [T, batch_size]\n    text_batch_tokens = text_batch_tokens.numpy().T # [batch_size, T]\n\n    text_batch_tokens_new = []\n    for text_tokens in text_batch_tokens:\n        text = [idx2char[idx] for idx in text_tokens]\n        text = \"\".join(text)\n        text_batch_tokens_new.append(text)\n\n    return text_batch_tokens_new","038a333b":"results_train = pd.DataFrame(columns=['actual', 'prediction'])\ntrain_loader = DataLoader(trainset, batch_size=16, num_workers=1, shuffle=False)\nwith torch.no_grad():\n    for image_batch, text_batch in tqdm(train_loader, leave=True):\n        text_batch_logits = crnn(image_batch.to(device)) # [T, batch_size, num_classes==num_features]\n        text_batch_pred = decode_predictions(text_batch_logits.cpu())\n        #print(text_batch, text_batch_pred)\n        df = pd.DataFrame(columns=['actual', 'prediction'])\n        df['actual'] = text_batch\n        df['prediction'] = text_batch_pred\n        results_train = pd.concat([results_train, df])\nresults_train = results_train.reset_index(drop=True)","20123da1":"results_test = pd.DataFrame(columns=['actual', 'prediction'])\ntest_loader = DataLoader(testset, batch_size=16, num_workers=1, shuffle=False)\nwith torch.no_grad():\n    for image_batch, text_batch in tqdm(test_loader, leave=True):\n        text_batch_logits = crnn(image_batch.to(device)) # [T, batch_size, num_classes==num_features]\n        text_batch_pred = decode_predictions(text_batch_logits.cpu())\n        #print(text_batch, text_batch_pred)\n        df = pd.DataFrame(columns=['actual', 'prediction'])\n        df['actual'] = text_batch\n        df['prediction'] = text_batch_pred\n        results_test = pd.concat([results_test, df])\nresults_test = results_test.reset_index(drop=True)","7d5afd6d":"print(results_train.shape)\nresults_train.head()","372cc1ac":"print(results_test.shape)\nresults_test.head()","534ef467":"def remove_duplicates(text):\n    if len(text) > 1:\n        letters = [text[0]] + [letter for idx, letter in enumerate(text[1:], start=1) if text[idx] != text[idx-1]]\n    elif len(text) == 1:\n        letters = [text[0]]\n    else:\n        return \"\"\n    return \"\".join(letters)\n\ndef correct_prediction(word):\n    parts = word.split(\"-\")\n    parts = [remove_duplicates(part) for part in parts]\n    corrected_word = \"\".join(parts)\n    return corrected_word","ca6c5717":"results_train['prediction_corrected'] = results_train['prediction'].apply(correct_prediction)\nresults_train.head()","992fd654":"results_test['prediction_corrected'] = results_test['prediction'].apply(correct_prediction)\nresults_test.head()","e2d9187f":"mistakes_df = results_test[results_test['actual'] != results_test['prediction_corrected']]\nmistakes_df","27b8f8e9":"print(mistakes_df['prediction_corrected'].str.len().value_counts())","c183311d":"mask = mistakes_df['prediction_corrected'].str.len() == 5\nmistakes_df[mask]","0f40795e":"mistake_image_fp = os.path.join(data_path, mistakes_df[mask]['actual'].values[0] + \".png\")\nprint(mistake_image_fp)\nmistake_image = Image.open(mistake_image_fp)\nplt.imshow(mistake_image)\nplt.show()","26dba3da":"train_accuracy = accuracy_score(results_train['actual'], results_train['prediction_corrected'])\nprint(train_accuracy)\ntest_accuracy = accuracy_score(results_test['actual'], results_test['prediction_corrected'])\nprint(test_accuracy)","7a40f969":"## 6. Train model","25cb69fb":"## 5. Define loss","deeb8308":"## 2. Define character maps","644ee635":"\n**Project Repository:** https:\/\/github.com\/GokulKarthik\/deep-learning-projects-pytorch","8932605b":"### References:\n[1] https:\/\/github.com\/carnotaur\/crnn-tutorial\/","1ef34820":"## 7. Make predictions","02c12f8b":"**Data Link**: https:\/\/www.kaggle.com\/shawon10\/captcha-recognition","1df985a9":"## 4. Define model","7c9286e2":"## 1. Make train-test split","ecbe6455":"## 8. Evaluate the model","a4e72e67":"## 3. Define data loader"}}