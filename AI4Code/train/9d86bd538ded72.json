{"cell_type":{"3c047cc7":"code","e3d4f8f7":"code","e3e637ee":"code","5263243f":"code","4f9dee8f":"code","4df3ddd9":"code","0f1ebd60":"code","8b6c52a8":"code","c0e25dee":"code","d5d4c118":"code","853fe56b":"code","d8f3551d":"code","b037ed79":"code","b10c3333":"markdown","b2340c47":"markdown"},"source":{"3c047cc7":"import numpy as np\nimport pandas as pd\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM, Dropout\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.backend import clear_session\n","e3d4f8f7":"## Fix random seed for reproducibility\nnp.random.seed(7)","e3e637ee":"# load the dataset but only keep the top n words, zero the rest\ntop_words= 5000\n(X_train, y_train), (X_test, y_test)= imdb.load_data(num_words=top_words)","5263243f":"## Features\nX_train","4f9dee8f":"## Labels\ny_train","4df3ddd9":"len(X_train), len(y_train), len(X_test), len(y_test)","0f1ebd60":"# Truncate and pad input sequences\nmax_review_length= 500\nX_train= sequence.pad_sequences(X_train, maxlen = max_review_length)\nX_test= sequence.pad_sequences(X_test, maxlen=max_review_length)","8b6c52a8":"X_train","c0e25dee":"## Model\nembedding_vector_length = 32\nclear_session()\nmodel= Sequential()\nmodel.add(Embedding(top_words, embedding_vector_length, input_length= max_review_length))\nmodel.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","d5d4c118":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","853fe56b":"history= model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))","d8f3551d":"import matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'y', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy Graph')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","b037ed79":"Train_Loss = history.history['loss']\nVal_Loss = history.history['val_loss']\nEpochs = range(1, len(Train_Loss) + 1)\nplt.plot(Epochs, Train_Loss, 'r', label='Training Loss')\nplt.plot(Epochs, Val_Loss, 'g', label='Validation Loss')\nplt.title('Training and Validation Loss Graph')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","b10c3333":"**The IMDB review data does have a one-dimensional spatial structure in the sequence of words in reviews and the CNN may be able to pick out invariant features for good and bad sentiment. This learned spatial features may then be learned as sequences by an LSTM layer.**","b2340c47":"# **Build Model (CNN + LSTM)**"}}