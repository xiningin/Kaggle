{"cell_type":{"937d78de":"code","46e678c9":"code","4f35b10e":"code","4504fcaf":"code","60b50ce3":"code","74aeed51":"code","d68e02c2":"code","f55482e2":"code","c821c074":"markdown","9b66ffbb":"markdown","699675dd":"markdown","57753a29":"markdown","5b831973":"markdown","4c30d15a":"markdown","c35c02a8":"markdown","08163f1f":"markdown","bc9269ad":"markdown","2ca31969":"markdown","e8c6c311":"markdown"},"source":{"937d78de":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\nimport math\nimport warnings\nfrom sklearn.preprocessing import PowerTransformer\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_colwidth', -1) \n\nclass DataConstants(object):\n           \n    NORMALISE_SCALE = 1\n    KEY_COLS = [ 'Country_Region', 'Province_State', 'Day', 'ConfirmedCases', 'ConfirmedCases_log', 'ConfirmedCases_Max', 'Fatalities', 'Fatalities_log', 'Fatalities_Max' ]\n    HISTORICAL_STEPS = 7\n    INPUT_FEAT_C = [ 'ConfirmedCases_log', 'ConfirmedCases_acc' ]  \n    INPUT_FEAT_F = [ 'Fatalities_log' ]   \n    TARGET_C = [ 'ConfirmedCases_log' ]\n    TARGET_F = [ 'Fatalities_log' ]\n    TEST_DAY_FROM = 71\n    TEST_DAY_TO = 79\n    \n\nclass Util(object):\n    '''\n        Helper class to contain util function\n    '''\n    class BackColors:\n        MAGENTA = '\\033[35m'\n        CYAN    = '\\033[36m'  \n        ENDC = '\\033[0m'\n        UNDERLINE = '\\033[4m'\n\n    @staticmethod\n    def display(title, mesg=''):\n        print('\\n{}{}{}\\n{}'.format(Util.BackColors.CYAN, title, Util.BackColors.ENDC, mesg))\n    \n    @staticmethod\n    def to_datetime(dt):\n        return datetime.datetime.strptime(dt, '%Y-%m-%d')\n    \n    @staticmethod\n    def to_isoweekday(dt):\n        return datetime.datetime.strptime(dt, '%Y-%m-%d').isoweekday()\n    \n    @staticmethod\n    def count_days(dt):\n        return (dt - datetime.datetime.strptime('2020-01-22', \"%Y-%m-%d\")).days\n\n\n'''\nRepository to read and retrieve dataframe from source\n'''\nclass DataFrameReader(object):\n    \n    country_states = [\n        ('Australia', 'New South Wales'),\n        ('Australia', 'Victoria'),\n        ('Australia', 'Queensland'),\n        ('Korea, South', 'Unknown'),\n        ('US', 'New York'),\n        ('Iran', 'Unknown'),\n        ('Italy', 'Unknown'),\n        ('China', 'Hubei'),\n        ('Brazil', 'Unknown'),\n        ('China', 'Beijing'),\n        ('Lebanon', 'Unknown'),\n        ('Russia', 'Unknown'),\n        ('US', 'Kansas'),\n        ('US', 'Florida'),\n        ('US', 'Illinois'),\n        ('US', 'Maryland'),\n        ('US', 'Michigan'),\n        ('US', 'Mineesota'),\n        ('United Kingdom', 'Bermuda'),\n        ('United Kingdom', 'Gibraltar'),\n        ('United Kingdom', 'Montserrat'),\n        ('United Kingdom', 'Unknown'),\n        ('Malaysia', 'Unknown'),\n        ('Mongolia', 'Unknown'),\n        ('New Zealand', 'Unknown'),\n    ]\n\n    n_states = 0    \n    power_transformer = None\n    \n    def __init__(self):\n        \n        self.power_transformer = PowerTransformer()\n    \n    def read(self):   \n        \n        df = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv')\n        df_test = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv')  \n        \n        df, df_test = self._filter(df, df_test)\n        df, df_test = self._preprocess(df, df_test)\n        df, df_test = self._transform(df, df_test)\n        \n        return df, df_test\n    \n    def _filter(self, df, df_test):\n        \n        df = df[((df.Country_Region != 'France') & (df.Province_State != 'Saint Barthelemy')) & ((df.Country_Region != 'Guyana') & (df.Province_State != 'Unknown'))]\n        \n        return df, df_test\n        \n    def _preprocess(self, df, df_test):\n        \n        # Fill empty values\n        df = df.fillna({'Province_State': 'Unknown'})\n        df_test = df_test.fillna({'Province_State': 'Unknown'})\n        \n        # Replace special char in fields\n        df['Country_Region']= df['Country_Region'].str.replace(\"'\", \"\")\n        df['Province_State']= df['Province_State'].str.replace(\"'\", \"\")\n        df_test['Country_Region']= df_test['Country_Region'].str.replace(\"'\", \"\")\n        df_test['Province_State']= df_test['Province_State'].str.replace(\"'\", \"\")\n\n        # Convert dates to day count (from the starting date)\n        df['Date_dt'] = df['Date'].map(f.to_datetime)\n        df_test['Date_dt'] = df_test['Date'].map(f.to_datetime)\n        df['Day'] = df['Date_dt'].map(f.count_days)\n        df_test['Day'] = df_test['Date_dt'].map(f.count_days)\n\n        # Correct outliers\n        df.loc[((df.Country_Region == 'Iceland') & (df.Province_State == 'Unknown') & (df.Day == 53)), 'Fatalities'] = 0\n        df.loc[((df.Country_Region == 'Kazakhstan') & (df.Province_State == 'Unknown') & (df.Day == 58)), 'Fatalities'] = 0\n        df.loc[df.index == 4874, 'ConfirmedCases'] = 146\n        df.loc[df.index == 19733, 'Fatalities'] = 0\n        df.loc[(df.index >= 17199) & (df.index <= 17202), 'Fatalities'] = 0  \n        \n        return df, df_test\n   \n    def _transform(self, df, df_test):     \n        \n        # Generate delta and log-transform\n        df['ConfirmedCases_log'] = 0.0\n        df['Fatalities_log'] = 0.0\n        \n        self.n_states = len(self.country_states)\n        \n        df = df[(df.Country_Region.isin([ g[0] for g in self.country_states ]) ) & (df.Province_State.isin([ g[1] for g in self.country_states ]))]\n        df_test = df_test[(df_test.Country_Region.isin([ g[0] for g in self.country_states ]) ) & (df_test.Province_State.isin([ g[1] for g in self.country_states ]))]        \n#         for c, s in df.groupby(['Country_Region', 'Province_State']): self.n_states += 1            \n        f.display('Total Countries', self.n_states)\n\n        from sklearn.preprocessing import LabelEncoder\n        lbl_encoder = LabelEncoder()\n        df['Geo_Code'] = df['Country_Region'].astype(str) + '_' + df['Province_State'].astype(str)\n        df['Geo_One_Hot'] = None\n    \n        df['Geo_Code'] = lbl_encoder.fit_transform(df['Geo_Code'])\n        \n        def one_hot(x):\n            n = np.empty(self.n_states)\n            n.fill(0)\n            n[x]=1\n            return n\n        df['Geo_One_Hot'] = df.Geo_Code.apply(one_hot)\n       \n#         for c, s in df[(df.Country_Region=='Australia')].groupby(['Country_Region', 'Province_State']):\n            \n#             df_a = df[(df.Country_Region==c[0]) & (df.Province_State==c[1])]\n#             #print(c[0], c[1], df_a.Geo_Code.values[0], df_a.Geo_One_Hot.values[0])\n#             one_hot = df_a.Geo_One_Hot.values[0]\n#             [ print(i) for i in range(len(one_hot)) if one_hot[i] == 1 ]\n                \n        # Normalise regardless of Group        \n        for c, s in df.groupby(['Country_Region', 'Province_State']):\n            \n            df_temp = df[(df.Country_Region == c[0]) & (df.Province_State == c[1])]\n            \n            df_case = df_temp.ConfirmedCases.rolling(3).sum()\n            df_fat = df_temp.Fatalities.rolling(3).sum()\n            \n            # Convert to rolling sum\n            idx = df_temp.head(1).index.values[0]\n            for i in range(len(df_temp)):                \n                df.at[idx + i, 'ConfirmedCases_acc'] = df_case[idx + i] if not math.isnan(df_case[idx + i]) else 0\n                df.at[idx + i, 'Fatalities_acc'] = df_fat[idx + i] if not math.isnan(df_fat[idx + i]) else 0\n                \n        df['ConfirmedCases_log'] = np.log1p(df.ConfirmedCases)#self.power_transformer.fit_transform(df[['ConfirmedCases']]) #np.log1p(df.ConfirmedCases) #df.ConfirmedCases \/ df.ConfirmedCases.max() * DataConstants.NORMALISE_SCALE\n        df['Fatalities_log'] = df.Fatalities \/ df.Fatalities.max() * DataConstants.NORMALISE_SCALE#np.log1p(df.Fatalities) #df.Fatalities \/ df.Fatalities.max() * DataConstants.NORMALISE_SCALE\n        df['ConfirmedCases_Max'] = df.ConfirmedCases.max()\n        df['Fatalities_Max'] = df.Fatalities.max()\n        \n        # continuous to discrete\n#         df['ConfirmedCases_bin'] = pd.cut(df.ConfirmedCases_log, bins=20, labels=np.arange(20), right=False)        \n        \n#         # Normalise by Group\n#         df[['ConfirmedCases_log']] = df.groupby(['Geo_Code'])[['ConfirmedCases']].transform(lambda x: (x \/ x.max() * DataConstants.NORMALISE_SCALE) if x.max() != 0 else 0)\n#         df[['Fatalities_log']] = df.groupby(['Geo_Code'])[['Fatalities']].transform(lambda x: (x \/ x.max() * DataConstants.NORMALISE_SCALE) if x.max() != 0 else 0)\n#         df = pd.merge(df, df.groupby(['Geo_Code'])[['ConfirmedCases', 'Fatalities']].max().reset_index()\\\n#                       .rename(columns={ 'ConfirmedCases': 'ConfirmedCases_Max',  'Fatalities': 'Fatalities_Max'}), on='Geo_Code', how='left')\n        \n        \n        return df, df_test\n    \nconst = DataConstants()\nf = Util()  \ndr = DataFrameReader()\ndf, df_test = dr.read()\n\nf.display('Columns', df.columns.tolist())\nf.display('Fill blank fields', df[ DataConstants.KEY_COLS ].isna().sum())\nf.display('Confirm no inf')\ndf[(df.ConfirmedCases_log == np.inf)  | (df.Fatalities_log == np.inf)][ DataConstants.KEY_COLS ]\n","46e678c9":"df[(df.Country_Region == 'Australia') & (df.Province_State == 'New South Wales')]","4f35b10e":"df_arr = []\n\nfor g in dr.country_states:    \n    df_arr.append(df[(df.Country_Region==g[0]) & (df.Province_State==g[1]) & (df.ConfirmedCases > 10)])   \n\nfig, ax = plt.subplots(nrows=3, ncols=3,figsize=(10,10))\n\nfor a in ax.flat:    a.set(xlabel='Total Cases (Log)', ylabel='7 Days Increase (Log)')\nfor a in ax.flat:    a.label_outer()\n    \ncounter = 0\nfor row in ax:\n    for col in row:\n        if len(df_arr) > counter:\n            plot_df = df_arr[counter]\n            col.title.set_text('{}, {}'.format(plot_df.Country_Region.values[0], plot_df.Province_State.values[0]))\n            col.grid()\n            col.plot(plot_df.Day, plot_df.ConfirmedCases_log, label='xxx')\n        counter += 1\nplt.show()","4504fcaf":"import time\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Sequential, backend as K\nfrom tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, Dropout, Reshape, Concatenate, Embedding, Flatten, TimeDistributed, RepeatVector, Conv1D, Conv2D, MaxPooling1D, AveragePooling1D, MaxPooling2D, ConvLSTM2D, BatchNormalization, LayerNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\n\nclass TimeSeriesModelRepo(object):\n    '''\n        Repository to create an instance for LSTM model\n    '''\n\n    verbose = 0\n    \n    input_shape = None\n    input_shape_geo = None\n\n    def __init__(self):\n        pass\n\n    def make_sequential_input(self, input_df):\n        \n        # historical time steps in last 7 days at time t\n        historical_steps = DataConstants.HISTORICAL_STEPS\n        \n        # ConfirmedCases and Fatalities columns\n        features_c = DataConstants.INPUT_FEAT_C \n        features_f = DataConstants.INPUT_FEAT_F \n        \n        tic = time.perf_counter()   \n        inputs_c, inputs_ca, inputs_f, inputs_geo, inputs_hot, targets_c, targets_f = [], [], [], [], [], [], []\n\n        # Iterate through each geo location\n        for c, s in input_df.groupby(['Country_Region', 'Province_State']):\n            country = c[0]\n            state = c[1]\n\n            print('Country: {}, State: {},'.format(country, state), end=' ')\n\n            # Country df\n            co_df = input_df.loc[(input_df.Country_Region==country) & (input_df.Province_State==state)]        \n            co_start_idx = co_df.index.values[0]\n\n            activate_tracking = False\n            activated_i = 0\n\n            for i in range(0, len(co_df)):\n                idx = co_start_idx + i     \n                last_idx = idx + historical_steps\n\n                if co_df.tail(1).index[0] >= last_idx:\n\n                    delta_df = co_df['ConfirmedCases'].diff()\n                    delta_1 = delta_df.at[idx + 1]\n                    delta_2 = delta_df.at[idx + 2]\n\n                    # Start tracking when a data point starts moves up and down\n                    if not activate_tracking \\\n                    and co_df.at[idx, 'ConfirmedCases'] >= 10 \\\n                    and (not math.isnan(delta_1) and delta_1 > 0) \\\n                    and (not math.isnan(delta_2) and delta_2 > 0):\n                        \n                        activate_tracking = True\n                        activated_i = i + historical_steps\n                        print('Train from Day: {}'.format(co_df.at[last_idx, 'Day']))\n\n                    if activate_tracking == True and i >= activated_i:\n\n                        data = co_df.loc[idx : last_idx]\n                        steps = data.iloc[-(historical_steps + 1):-1]\n                        last = data.tail(1)  \n                        \n                        for i in range(1):\n                            # inputs for geo location, confirmed case sequence, and fatality sequence (from time t - 7 to t - 1)\n                            inputs_geo.append(np.array(last[['Geo_Code']]).reshape(-1).tolist())  \n                            inputs_hot.append(np.array(last[['Geo_One_Hot']]).reshape(-1).tolist())  \n                            inputs_c.append(np.array(steps[['ConfirmedCases_log']]).tolist()) \n                            inputs_ca.append(np.array(steps[['ConfirmedCases_acc']]).tolist()) \n                            inputs_f.append(np.array(steps[features_f]).tolist())        \n\n                            # outpus for confirmed case and fatality at time t\n                            targets_c.append(np.array(last[DataConstants.TARGET_C]).reshape(-1).tolist()) \n                            targets_f.append(np.array(last[DataConstants.TARGET_F]).reshape(-1).tolist())   \n                            \n\n        toc = time.perf_counter()\n        f.display('Prepare historical steps', '{:.2f} sec(s) taken'.format((toc-tic)))\n\n        return inputs_c, inputs_ca, inputs_f, inputs_geo, inputs_hot, targets_c, targets_f\n    \n    def split_train_test(self, train_inputs, train_inputs_a, train_inputs_f, train_inputs_geo, train_inputs_hot, train_targets_c, train_targets_f):\n        \n        max_index = np.array(train_inputs).shape[0] - 1\n        indices = []\n\n        for i in range(int(max_index*0.1)):\n            indices.append(random.randint(0, max_index))\n\n        val_inputs = [ train_inputs[i] for i in indices ]\n        val_inputs_a = [ train_inputs_a[i] for i in indices ]\n        val_inputs_f = [ train_inputs_f[i] for i in indices ]\n        val_inputs_geo = [ train_inputs_geo[i] for i in indices  ] \n        val_inputs_hot = [ train_inputs_hot[i] for i in indices  ] \n        val_targets_c = [ train_targets_c[i] for i in indices ]\n        val_targets_f = [ train_targets_f[i] for i in indices ]\n        \n        return val_inputs, val_inputs_a, val_inputs_f, val_inputs_geo, val_inputs_hot, val_targets_c, val_targets_f\n    \n    def create_ae(self, n_nodes, lr, inputs=None, inputs_geo=None, targets=None, v_inputs=None, v_inputs_geo=None, v_targets=None):\n        \n        self.input_shape=np.array(inputs).shape[-2:]\n        self.input_shape_geo=np.array(inputs_geo).shape\n        dim = int(dr.n_states)\n        \n        input_geo = Input(shape=(1,), name='input_geo')    \n        embed_h = Embedding(dim, int(dim), trainable=True)(input_geo)\n        embed_c = Embedding(dim, int(dim), trainable=True)(input_geo)\n        state_h_r = Flatten()(embed_h)\n        state_c_r = Flatten()(embed_c)\n        state_h = Dense(n_nodes, activation='relu')(state_h_r)\n        state_c = Dense(n_nodes, activation='relu')(state_c_r)\n                        \n        input_ts = Input(shape=self.input_shape, name='input_ts')\n        lstm = LSTM(n_nodes, activation='relu')(input_ts, initial_state=[ state_h, state_c ])\n        lnorm = LayerNormalization()(lstm)\n        main_output = Dense(1, name='output_main')(lnorm) \n        model = Model(inputs=[ input_geo, input_ts ], outputs=main_output)  \n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=[ 'mape', 'mae' ])\n\n        history = model.fit([ inputs_geo, inputs ], targets, \\\n                    epochs=self.epochs, \\\n                    batch_size=self.n_batch, \\\n                    verbose=self.verbose)\n        \n        print(\"Model Accuracy (LSTM AE): {}\".format(model.evaluate([ inputs_geo, inputs ], targets)))\n        \n        return model\n\n    \n    def create_cnn_lstm(self, epoch, n_batch, n_nodes, f_size, lr, inputs=None, inputs_a=None, inputs_geo=None, targets=None, v_inputs=None, v_inputs_geo=None, v_targets=None):\n        \n        self.input_shape = np.array(inputs).shape[-2:]  \n        dim = int(dr.n_states)   \n        \n        # cnn encoder\n        input_ts = Input(shape=self.input_shape, name='input_ts')\n        conv = Conv1D(filters=f_size, kernel_size=3, activation='relu', input_shape=(DataConstants.HISTORICAL_STEPS, 1))(input_ts)  \n#         conv_2 = Conv1D(filters=f_size, kernel_size=3, activation='relu')(conv)  \n        pool = MaxPooling1D(pool_size=2)(conv)        \n        flat = Flatten()(pool)\n        vector = RepeatVector(1)(flat)  \n        \n        # embedding 1               \n        input_geo = Input(shape=(1,), name='input_geo')    \n        embed_h = Embedding(dim, dim*4, trainable=True)(input_geo)\n        embed_c = Embedding(dim, dim*4, trainable=True)(input_geo)\n        state_h_r = Flatten()(embed_h)\n        state_c_r = Flatten()(embed_c) \n        state_h = Dense(n_nodes, activation='tanh')(state_h_r)\n        state_c = Dense(n_nodes, activation='tanh')(state_c_r)\n        \n        # embedding 2\n        embed_h_2 = Embedding(dim, dim*4, trainable=True)(input_geo)\n        embed_c_2 = Embedding(dim, dim*4, trainable=True)(input_geo)\n        state_h_r_2 = Flatten()(embed_h_2)\n        state_c_r_2 = Flatten()(embed_c_2) \n        state_h_2 = Dense(n_nodes, activation='tanh')(state_h_r_2)\n        state_c_2 = Dense(n_nodes, activation='tanh')(state_c_r_2)  \n        \n        # stacked lstm\n        decoder = LSTM(n_nodes, activation='relu', return_sequences=True)(vector, initial_state=[ state_h, state_c ]) # TODO: recurrent_dropout          \n        decoder_2 = LSTM(n_nodes, activation='relu', return_sequences=True)(decoder, initial_state=[ state_h_2, state_c_2 ])\n        lnorm = LayerNormalization()(decoder_2)                 \n        main_output = TimeDistributed(Dense(1, name='output_main'))(lnorm)              \n        \n        model = Model(inputs=[ input_geo, input_ts ], outputs=main_output)  \n        \n        model.compile(optimizer='adam', loss='mse', metrics=[ 'mape', 'mae' ])\n        \n#         reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n#                               patience=5, min_lr=0.0001)\n        \n        history = model.fit([ inputs_geo, inputs ], targets, \\\n                    epochs=epoch, \\\n                    batch_size=n_batch, \\\n                    verbose=self.verbose, \\\n                    validation_data=([ v_inputs_geo, v_inputs ], v_targets))\n        \n        print(\"Model Accuracy (CNN LSTM): {}\".format(model.evaluate([ inputs_geo, inputs ], targets)))\n        model.summary()\n\n        return model\n\n\nmr = TimeSeriesModelRepo()\n\n# Make sequential input for training and validation\ntrain_inputs, train_inputs_a, train_inputs_f,  train_inputs_geo, train_inputs_hot, train_targets_c, train_targets_f = mr.make_sequential_input(df[df.Day < 79])\nf.display('Input data shape', 'Train input shape: {} \\nTrain input geo shape: {} \\nTrain target shape: {}'.format(np.shape(train_inputs), np.shape(train_inputs_geo), np.shape(train_targets_c)))\n\nval_inputs, val_inputs_a, val_inputs_f, val_inputs_geo, val_inputs_hot, val_targets_c, val_targets_f = \\\n    mr.split_train_test(train_inputs, train_inputs_a, train_inputs_f, train_inputs_geo, train_inputs_hot, train_targets_c, train_targets_f)\nf.display('Split train & validation data', 'No. train data: {} \\nNo. validation data: {}'.format(len(train_inputs), len(val_inputs)))","60b50ce3":"model_cases = mr.create_cnn_lstm(2000, 100, 100, 64, 0.001, np.array(train_inputs), np.array(train_inputs_a), np.array(train_inputs_geo), np.array(train_targets_c), np.array(val_inputs), np.array(val_inputs_geo), np.array(val_targets_c))","74aeed51":"import time\n\ndf_test['ConfirmedCases'] = None\ndf_test['Fatalities'] = None\n\ntemp_df = df[df.Day <= 79].copy()\ntemp_df = pd.concat([ temp_df, df_test[df_test.Day > 79] ], ignore_index=True)\n\nday_predicting_from = 71\nday_predicting_to = 113\nhistorical_steps = DataConstants.HISTORICAL_STEPS\nfeatures_c = DataConstants.INPUT_FEAT_C\nfeatures_f = DataConstants.INPUT_FEAT_F\n\ncurrent_row = None\nhist_rows = None\n\ntic = time.perf_counter()\ncounter = 0\n    \n# For each country and state\nfor c, s in df_test.groupby(['Country_Region', 'Province_State']):  \n    \n    toc = time.perf_counter()\n    \n    country = c[0]\n    state = c[1]\n    \n    # Traverse from Day 71 to the end\n    for day in range(day_predicting_from, day_predicting_to + 1):   \n        current_row = temp_df[ (temp_df.Country_Region == country) & (temp_df.Province_State == state) & (temp_df.Day == day)]\n        \n        if day == day_predicting_from:\n            first_row = current_row\n            \n        hist_rows = temp_df[(temp_df.Country_Region == country) & (temp_df.Province_State == state) & (temp_df.Day >= (day - historical_steps)) & (temp_df.Day < day)]        \n        \n        # Only predict when historical steps exist\n        if not current_row.empty and not hist_rows.empty and hist_rows.shape[0] == historical_steps:    \n            \n            input_geo = np.array(hist_rows.iloc[-historical_steps]['Geo_Code']).reshape(1, 1)\n#             input_geo = np.array(hist_rows.iloc[-historical_steps]['Geo_One_Hot']).reshape(-1, dr.n_states)\n            \n            input_c = np.array(hist_rows.iloc[-historical_steps:,]['ConfirmedCases_log']).reshape(1, mr.input_shape[0], mr.input_shape[1])\n#             input_ca = np.array(hist_rows.iloc[-historical_steps:,]['ConfirmedCases_acc']).reshape(1, mr.input_shape[0], mr.input_shape[1])\n#             input_f = np.array(hist_rows.iloc[-historical_steps:,][ features_f ]).reshape(1, mr.input_shape[0], mr.input_shape[1])\n            \n            pred = model_cases.predict([ tf.convert_to_tensor(input_geo, np.float64), tf.convert_to_tensor(input_c, np.float64) ])\n\n            current_idx = current_row.index.values[0]       \n            temp_df.at[current_idx, 'ConfirmedCases_inversed_predicted'] = np.expm1(float(pred[0][0])) #dr.power_transformer.inverse_transform([[float(pred[0][0])]])[0][0] #np.expm1(float(pred[0][0])) #float(pred[0][0]) \/ DataConstants.NORMALISE_SCALE * float(first_row.ConfirmedCases_Max)\n            \n            temp_df.at[current_idx, 'Geo_Code'] = hist_rows.iloc[-1,]['Geo_Code']    \n#             temp_df.at[current_idx, 'Geo_One_Hot'] = hist_rows.iloc[-1,]['Geo_One_Hot']    \n    \n            # Update the existing fields if empty\n            if (current_row['ConfirmedCases'].values[0] == None) and (current_row['Fatalities'].values[0] == None):\n                temp_df.at[current_idx, 'ConfirmedCases_log'] = float(pred[0][0])\n\n    toc = time.perf_counter()\n    counter = counter + 1\n    print('{:.2f} sec(s) taken - Geo: {}, {}, No. Records: {}'.format((toc-tic), country, state, counter))        ","d68e02c2":"hist_arr = []\npred_arr = []\n\nfor country, state in dr.country_states:\n    hist_arr.append(temp_df[(temp_df.Country_Region == country) & (temp_df.Province_State == state) & (temp_df.Day <= const.TEST_DAY_TO)].groupby(['Country_Region', 'Province_State', 'Day']).agg({'ConfirmedCases': 'first'}).reset_index())\n    pred_arr.append(temp_df[(temp_df.Country_Region == country) & (temp_df.Province_State == state) & (temp_df.Day >= const.TEST_DAY_FROM)].groupby(['Country_Region', 'Province_State', 'Day']).agg({'ConfirmedCases_inversed_predicted': 'first'}).reset_index())\n\nfig, ax = plt.subplots(nrows=5, ncols=5,figsize=(17,17))\nfig.tight_layout(pad=3.0)\n\nfor a in ax.flat:    a.set(xlabel='Day', ylabel='No. ConfirmedCases')\n    \ncounter = 0\nfor row in ax:\n    for col in row:\n        if len(hist_arr) > counter and len(hist_arr[counter]) > 0:\n\n            hist_df = hist_arr[counter]\n            pred_df = pred_arr[counter]                       \n            \n            col.title.set_text('{}, {}'.format(hist_df.Country_Region.values[0], hist_df.Province_State.values[0]))\n            col.grid()\n            col.plot(hist_df.Day, hist_df.ConfirmedCases, label='Historical')\n            col.plot(pred_df.Day, pred_df.ConfirmedCases_inversed_predicted, label='Predictive')               \n            \n            y = np.array(hist_df[(hist_df.Day >= const.TEST_DAY_FROM) & (hist_df.Day <= const.TEST_DAY_TO)].ConfirmedCases)\n            y_hat = np.array(pred_df[(pred_df.Day >= const.TEST_DAY_FROM) & (pred_df.Day <= const.TEST_DAY_TO)].ConfirmedCases_inversed_predicted)\n            \n            sum_errs = np.sum((y-y_hat )**2)\n            std = np.sqrt(1\/(len(y)-2) * sum_errs)\n            interval = 1.96 * std            \n            fill_df = pred_df[pred_df.Day >= const.TEST_DAY_TO]\n            col.fill_between(fill_df.Day, (fill_df.ConfirmedCases_inversed_predicted - interval), (fill_df.ConfirmedCases_inversed_predicted + interval), color='b', alpha=.1)\n            \n            col.axhline(y=pred_df.ConfirmedCases_inversed_predicted.max(), color='r', linestyle='--', label=round(pred_df.ConfirmedCases_inversed_predicted.max(), 2))\n            col.legend(loc=\"best\")\n            counter += 1\n        \nplt.show()","f55482e2":"from sklearn.metrics import mean_squared_log_error\n\nmsle = mean_squared_log_error(temp_df[(temp_df.Day >= const.TEST_DAY_FROM) & (temp_df.Day <= const.TEST_DAY_TO)].ConfirmedCases, \\\n                             temp_df[(temp_df.Day >= const.TEST_DAY_FROM) & (temp_df.Day <= const.TEST_DAY_TO)].ConfirmedCases_inversed_predicted)\n\nprint('RMSLE: ', math.sqrt(msle))","c821c074":"# 7. Calculate the RMSE (Error)\n\nHere we calculate errors to see how much deviated the predictions are from the actual results during the given period (Day 71 ~ 79). The lower the metric value is the more accurate the model is. RMSLE is used","9b66ffbb":"One of the most remarkable patterns I can find was that a time series of Covid-19 is similar to common growth curve, for example new cases of AIDS in a country and child growth curve, in a sense that the observed data points grow expoentially and flatten out afterward. This growth curve looks a lot like ***Logistic \/ Sigmoid*** curve.","699675dd":"# 2. Exploratory Data Analysis (EDA)\n\nThe list of geo location is selected to shape a shape of trend over time, and identify common patterns in a time series.","57753a29":"# 8. Conclusion\n\nNext time, this type of time series which can be grouped by context e.g. geo location can be forecasted by a group after clustering time series to mutiple clusters which are based on their patterns in growth curve.","5b831973":"# 3. Prepare Model Input Data","4c30d15a":"* y: normalised, x: day\n* Not all start from the same day\n* but has deviation in shape which can be stated as custom weights and bias in LSTM\n","c35c02a8":"# 6. Visualise the Forecasted Result","08163f1f":"# 5. Forecast the Future Result","bc9269ad":"# 4. Train LSTM Model","2ca31969":"# Predicting COVID-19 Infections with LSTM\n> Yohan Chung\n\nThe pandemic of Coronavirus (COVID-19) became reality and many countries around the globe strive to contain further spread of the virus with social distancing as well as qurantining of those who potentially contact the infected. At this moment, the project aims to predict future of the infected by geo location and identify which country needs more attention. The prediction of the newly infected starts with reading dataset which are provided by Johns Hopkins University, and the dataset includes the number of cases confirmed with Covid-19 as well as fatalities by country and state. I would like to thank for sharing the dataset so that data scientists can contribute to modelling data.\n\n# 1. Feature Engineering\n\nAfter reading dataset, there are two main actions that I make:\n\n* Country and state fields are combined as called Geo_Code in order to easily filter and sort dataset by geo location.\n* The scale of ConfirmedCases and Fatalities is normalised to the range from 0 to 1\n\nThe normalisation is typically performed in order to reduce computation time due to that the numbers become small. In addition, it is normalised that a model can learn potential patterns in data in the same scale. For example, 1% increase in the infected from countries with large and small population can be regarded as same impact on the model.","e8c6c311":"![New cases of AIDS in the United Stated over Years](http:\/\/www.nlreg.com\/aids.jpg)"}}