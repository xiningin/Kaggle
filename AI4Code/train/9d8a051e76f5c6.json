{"cell_type":{"76ba9f3b":"code","8762f87a":"code","595a10ab":"code","6681fd25":"code","092f4b1b":"code","9c7ccef6":"code","e6ee9ce3":"code","b3466294":"code","fe6af3c5":"code","86f6c3c0":"code","a4b192ed":"code","9a5789bd":"code","e6c4edca":"code","a77394d5":"code","246b10dc":"code","1927dc96":"code","942abf25":"code","f1f8614b":"code","94936596":"code","d3588420":"code","b679ac46":"code","011651a4":"code","bddf8827":"code","c50af471":"code","3ff718b6":"code","c1c9623d":"code","2747f1de":"markdown","5be91560":"markdown","ad00e9a9":"markdown","375243c6":"markdown","7d3125a3":"markdown","7b01e22e":"markdown","5c0dd7c7":"markdown","3affb7b4":"markdown","e088314c":"markdown","7984e8c2":"markdown","dc7fee12":"markdown","b35f94bf":"markdown","f8f44405":"markdown","776db51a":"markdown","c8779320":"markdown","4cd6bfaa":"markdown"},"source":{"76ba9f3b":"import os\n\nimport re\nimport numpy as np\nimport pandas as pd\nimport nltk\nnltk.download('punkt')\nfrom nltk import sent_tokenize\nimport transformers\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, concatenate, GlobalMaxPooling1D, GlobalAveragePooling1D, Dropout, Conv1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\n\nfrom transformers import TFAutoModel, AutoTokenizer, AutoConfig\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\ndef seed_everything(seed=0):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nSEED = 40\nseed_everything(SEED)","8762f87a":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=True, \n        return_token_type_ids=True,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    return np.array(enc_di['input_ids'],dtype=np.int32),np.array(enc_di['attention_mask'],dtype=np.int32),np.array(enc_di['token_type_ids'],dtype=np.int32)","595a10ab":"LANGS = {\n    'en': 'english',\n    'it': 'italian', \n    'fr': 'french', \n    'es': 'spanish',\n    'tr': 'turkish', \n    'ru': 'russian',\n    'pt': 'portuguese'\n}\n\ndef get_sentences(text, lang='en'):\n    return sent_tokenize(text, LANGS.get(lang, 'english'))\n\ndef exclude_duplicate_sentences(text, lang='en'):\n    sentences = []\n    for sentence in get_sentences(text, lang):\n        sentence = sentence.strip()\n        if sentence not in sentences:\n            sentences.append(sentence)\n    return ' '.join(sentences)\n\ndef clean_text(text, lang='en'):\n    text = str(text)\n    text = re.sub(r'[0-9\"]', '', text)\n    text = re.sub(r'#[\\S]+\\b', '', text)\n    text = re.sub(r'@[\\S]+\\b', '', text)\n    text = re.sub(r'https?\\S+', '', text)\n    text = re.sub(r'\\s+', ' ', text)\n    text = exclude_duplicate_sentences(text, lang)\n    return text.strip()","6681fd25":"#last 3 layers\ndef build_model(transformer, max_len=512):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n#     sequence_output = transformer((input_ids,input_mask,segment_ids))[0] #the last layer [batch, seq_len, dim] dim=1024\n#     cls_token = sequence_output[:, 0, :]\n    _, _, hs = transformer((input_ids,input_mask,segment_ids)) #[batch, seq_len, dim]\n    x = tf.stack([hs[-1],hs[-2],hs[-3]])    #[3, batch, seq_len, dim]\n    sequence_output = tf.reduce_mean(x, axis = 0)         #[batch, seq_len, dim]\n    \n    pool_output = []\n    kernel_sizes = [3, 4, 5] \n    for kernel_size in kernel_sizes:\n        c = Conv1D(filters=64, kernel_size=kernel_size, padding='same',activation='relu',strides=1)(sequence_output) #[batch, seq_len, 2]\n        p = GlobalMaxPooling1D()(c) #[batch, 2]\n        pool_output.append(p)\n    pool_output = concatenate([p for p in pool_output])    #[batch, 8]\n    \n#     sequence_output = pool_output\n#     gp = GlobalMaxPooling1D()(sequence_output) #[batch,dim]\n#     gp = Dropout(0.3)(gp)\n#     ap = GlobalAveragePooling1D()(sequence_output) #[batch,dim]\n#     ap = Dropout(0.3)(ap)\n#     stack = concatenate([gp,ap],axis=1) #[batch,2*dim]\n#     out = Dropout(0.3)(stack)\n    out = pool_output\n    out = Dropout(0.3)(out)\n    out = Dense(64, activation='relu')(out) #[batch,1]\n    out = Dropout(0.3)(out)\n    out = Dense(1, activation='sigmoid')(out) #[batch,1]\n    \n    model = Model(inputs=[input_ids,input_mask,segment_ids], outputs=out)\n    model.compile(Adam(lr=0.2e-5), loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.AUC()])\n    \n    return model","092f4b1b":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","9c7ccef6":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\n# GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\n# Configuration\nEPOCHS = 2\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 210\nMODEL = 'jplu\/tf-xlm-roberta-large'\nPRETRAINED_MODEL = '\/kaggle\/input\/jigsaw-mlm-finetuned-xlm-r-large'","e6ee9ce3":"# Load the real tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","b3466294":"train1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\ntrain2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\ntrain2.toxic = train2.toxic.round().astype(int)\n\nvalid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')","fe6af3c5":"train3 = pd.read_csv(\"\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-tr-cleaned.csv\")\ntrain4 = pd.read_csv(\"\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-it-cleaned.csv\")\ntrain5 = pd.read_csv(\"\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-pt-cleaned.csv\")\ntrain6 = pd.read_csv(\"\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-ru-cleaned.csv\")\ntrain7 = pd.read_csv(\"\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-fr-cleaned.csv\")\ntrain8 = pd.read_csv(\"\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-es-cleaned.csv\")","86f6c3c0":"import seaborn as sns\nsns.countplot(valid['lang'])","a4b192ed":"sns.countplot(test['lang'])","9a5789bd":"train1['toxic'].value_counts()","e6c4edca":"train = pd.concat([\n    train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1').sample(n=60000, random_state=12312),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=60000, random_state=42323),\n    ])\ntrain_tr = pd.concat([\n    train3[['comment_text', 'toxic']].query('toxic==1'),\n    train3[['comment_text', 'toxic']].query('toxic==0').sample(n=50000, random_state=23412),\n])\ntrain_it = pd.concat([\n    train4[['comment_text', 'toxic']].query('toxic==1'),\n    train4[['comment_text', 'toxic']].query('toxic==0').sample(n=50000, random_state=23412),\n])\ntrain_pt = pd.concat([\n    train5[['comment_text', 'toxic']].query('toxic==1'),\n    train5[['comment_text', 'toxic']].query('toxic==0').sample(n=50000, random_state=75293),\n])\ntrain_ru = pd.concat([\n    train6[['comment_text', 'toxic']].query('toxic==1'),\n    train6[['comment_text', 'toxic']].query('toxic==0').sample(n=50000, random_state=17479),\n])\ntrain_fr = pd.concat([\n    train7[['comment_text', 'toxic']].query('toxic==1'),\n    train7[['comment_text', 'toxic']].query('toxic==0').sample(n=50000, random_state=56321),\n])\ntrain_es = pd.concat([\n    train8[['comment_text', 'toxic']].query('toxic==1'),\n    train8[['comment_text', 'toxic']].query('toxic==0').sample(n=50000, random_state=45874),\n])","a77394d5":"train['comment_text'] = train['comment_text'].apply(lambda x : clean_text(x))\nvalid['comment_text'] = valid['comment_text'].apply(lambda x : clean_text(x))\n\ntrain_tr['comment_text'] = train_tr['comment_text'].apply(lambda x : clean_text(x,lang='tr'))\ntrain_pt['comment_text'] = train_pt['comment_text'].apply(lambda x : clean_text(x,lang='pt'))\ntrain_it['comment_text'] = train_it['comment_text'].apply(lambda x : clean_text(x,lang='it'))\ntrain_ru['comment_text'] = train_ru['comment_text'].apply(lambda x : clean_text(x,lang='ru'))\ntrain_fr['comment_text'] = train_fr['comment_text'].apply(lambda x : clean_text(x,lang='fr'))\ntrain_es['comment_text'] = train_es['comment_text'].apply(lambda x : clean_text(x,lang='es'))\ntest['content'] = test['content'].apply(lambda x : clean_text(x))\n","246b10dc":"%%time \n\nx_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)\nx_train_tr = regular_encode(train_tr.comment_text.values, tokenizer, maxlen=MAX_LEN)\nx_train_pt = regular_encode(train_pt.comment_text.values, tokenizer, maxlen=MAX_LEN)\nx_train_it = regular_encode(train_it.comment_text.values, tokenizer, maxlen=MAX_LEN)\nx_train_ru = regular_encode(train_ru.comment_text.values, tokenizer, maxlen=MAX_LEN)\nx_train_fr = regular_encode(train_fr.comment_text.values, tokenizer, maxlen=MAX_LEN)\nx_train_es = regular_encode(train_es.comment_text.values, tokenizer, maxlen=MAX_LEN)\n\nx_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\nx_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\n\ny_train = train.toxic.values\ny_train_tr = train_tr.toxic.values\ny_train_pt = train_pt.toxic.values\ny_train_it = train_it.toxic.values\ny_train_ru = train_ru.toxic.values\ny_train_fr = train_fr.toxic.values\ny_train_es = train_es.toxic.values\n\ny_valid = valid.toxic.values","1927dc96":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .shuffle(len(train))\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(AUTO)\n)\ntrain_tr_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train_tr, y_train_tr))\n    .shuffle(len(train_tr))\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(AUTO)\n)\ntrain_it_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train_it, y_train_it))\n    .shuffle(len(train_it))\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(AUTO)\n)\ntrain_pt_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train_pt, y_train_pt))\n    .shuffle(len(train_pt))\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(AUTO)\n)\ntrain_ru_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train_ru, y_train_ru))\n    .shuffle(len(train_ru))\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(AUTO)\n)\ntrain_fr_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train_fr, y_train_fr))\n    .shuffle(len(train_fr))\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(AUTO)\n)\ntrain_es_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train_es, y_train_es))\n    .shuffle(len(train_es))\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(AUTO)\n)\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n# test_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices(x_test)\n#     .batch(BATCH_SIZE)\n# )","942abf25":"%%time\nwith strategy.scope():\n    config = AutoConfig.from_pretrained(PRETRAINED_MODEL)\n    config.output_hidden_states = True\n    transformer_layer = TFAutoModel.from_pretrained(PRETRAINED_MODEL,config=config)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","f1f8614b":"n_steps = train_tr.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_tr_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS+1,\n    shuffle=False,\n)","94936596":"n_steps = train_it.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_it_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS,\n    shuffle=False,\n)","d3588420":"n_steps = train_pt.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_pt_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS,\n    shuffle=False,\n)","b679ac46":"n_steps = train_ru.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_ru_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS,\n    shuffle=False,\n)","011651a4":"n_steps = train_fr.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_fr_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS,\n    shuffle=False,\n)","bddf8827":"n_steps = train_es.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_es_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS,\n    shuffle=False,\n)","c50af471":"#test dataset do not include Engish reviews\n# n_steps = train.shape[0] \/\/ BATCH_SIZE\n# train_history = model.fit(\n#     train_dataset,\n#     steps_per_epoch=n_steps,\n#     validation_data=valid_dataset,\n#     epochs=EPOCHS,\n#     shuffle=False,\n# )","3ff718b6":"%%time\nn_steps = valid.shape[0] \/\/ BATCH_SIZE\ntrain_history_2 = model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=5,\n    verbose=2,\n    shuffle=False,\n)","c1c9623d":"sub['toxic'] = model.predict(x_test, verbose=1)\nsub.to_csv('submission.csv', index=False)\nsub.head(10)","2747f1de":"## Data Processing","5be91560":"## Build datasets objects","ad00e9a9":"## Build model","375243c6":"### About this notebook\n#### Hi everyone! I'm a beginner in NLP and happy to win a medal in this comp. This notebook is one part of my single model notebooks before blending with others' results. I think this single model performs well(public lb 0.9448). The main highlight is the downstream structure of the model, which is more suitable for novices like me for reference.\n\nThe entire notebook is based on the [baseline](https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-xlm-roberta) provided by [@xhlulu](https:\/\/www.kaggle.com\/xhlulu\/).\n\nFor the model, I used the [MLM finetuned XLM-R large](https:\/\/www.kaggle.com\/riblidezso\/jigsaw-mlm-finetuned-xlm-r-large) provided by [@riblidezso](https:\/\/www.kaggle.com\/riblidezso).\n\nAnd it trains on the different translated data in [translated data(Google API)](https:\/\/www.kaggle.com\/miklgr500\/jigsaw-train-multilingual-coments-google-api) provided by [@miklgr500](https:\/\/www.kaggle.com\/miklgr500).\n\n### References\n* Original Author:  [@xhlulu](https:\/\/www.kaggle.com\/xhlulu\/)\n\n* Original notebook:  [Link](https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras)\n\n* some functions for cleaning data: [link](https:\/\/www.kaggle.com\/shonenkov\/tpu-inference-super-fast-xlmroberta) by [@shonenkov](https:\/\/www.kaggle.com\/shonenkov)\n\n* model:  [Link](https:\/\/www.kaggle.com\/riblidezso\/jigsaw-mlm-finetuned-xlm-r-large) by [@riblidezso](https:\/\/www.kaggle.com\/riblidezso)\n\n* dataset:  [Link](https:\/\/www.kaggle.com\/miklgr500\/jigsaw-train-multilingual-coments-google-api) by [@miklgr500](https:\/\/www.kaggle.com\/miklgr500)\n\nMany Thanks for their nice work!","7d3125a3":"## Create tokenizer","7b01e22e":"## Train Model","5c0dd7c7":"## Import What We Need","3affb7b4":"## Submission","e088314c":"#### We train it for five more epochs on the `validation` set, because the `validation` set seems to be similar to the `test` set.","7984e8c2":"## Helper Functions","dc7fee12":"#### We sample subsets from each datasets.","b35f94bf":"#### We train on the different translated training set respectively.","f8f44405":"## Load model into the TPU","776db51a":"## What's more\n### My final result is blend of awesome kernels [notebooks](https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/notebooks) and all my results. The flowing image shows how I change the model structure and use [single language dataset](https:\/\/www.kaggle.com\/kashnitsky\/jigsaw-multilingual-toxic-test-translated) and [multilingual dataset](https:\/\/www.kaggle.com\/miklgr500\/jigsaw-train-multilingual-coments-google-api) to get more single-model results.\n![%E8%B0%83%E5%8F%82%E8%AE%B0%E5%BD%95.png](attachment:%E8%B0%83%E5%8F%82%E8%AE%B0%E5%BD%95.png)","c8779320":"## TPU Configs","4cd6bfaa":"#### I tried to combine various model structures in the downstream of xlm-roberta, and found that **stacking the last three layers and combining with CNN** brings the most outstanding effect (after parameter selection)"}}