{"cell_type":{"417e2348":"code","a4ecb91b":"code","89245e63":"code","989ddbab":"code","aaf758d0":"code","bced9e8a":"code","5d499a95":"code","037d5f7c":"code","43bbbd28":"code","9d9165fb":"code","559152a7":"code","31475985":"code","c1144bd9":"code","6c90bd8d":"code","204ad1de":"code","2c4885c9":"code","0225ca51":"code","ddba1ca2":"code","bbe1fa46":"code","0ea2a72a":"code","eaf4347f":"code","2f776356":"code","54012381":"code","4e624c0e":"code","44b3f2ff":"code","f067da50":"code","09eec3d1":"code","a7727239":"code","6192ade0":"code","c081c2d7":"code","cc32584a":"code","66be7ce8":"code","6e13cd6a":"code","ed0dcead":"code","6b399c19":"code","83906a36":"code","d43e3f82":"code","096fbec3":"code","c3833cd5":"code","e1b824e9":"code","dbc1a050":"code","d16cc0be":"code","5ee2308e":"code","3c878bee":"code","77e800bd":"code","cce5a8e0":"code","75861dd6":"code","6ce2eb11":"code","ba3b356e":"code","3777936e":"code","d03faa8e":"code","eb43b12c":"code","a1b79441":"markdown","63e8f1a3":"markdown","3734ff07":"markdown","0490f660":"markdown","2159c315":"markdown","bf902abb":"markdown","d9499798":"markdown","a700696e":"markdown","89df480a":"markdown","eacfca86":"markdown","7e64bf92":"markdown","7116a5dc":"markdown","c1f1a246":"markdown","3a9a75ff":"markdown","ce2b5d9e":"markdown","d5faa0c5":"markdown","90ac4500":"markdown","ad892b45":"markdown","51356586":"markdown","629a72fe":"markdown","88263159":"markdown","9e69bee3":"markdown","9a2efb25":"markdown","91f7c6c9":"markdown","ca62020a":"markdown","0ab04129":"markdown","95ef9175":"markdown","2ff7ed28":"markdown","44ae9930":"markdown","70c1ed25":"markdown","2c3af9e7":"markdown","7dcaeb21":"markdown","472f12a0":"markdown","d7e3293a":"markdown","cf0e533e":"markdown","4fdf06a0":"markdown","5b5a8ac1":"markdown","a5767446":"markdown","860f61d7":"markdown","8858e39d":"markdown","bbc6f520":"markdown","4d48537a":"markdown","328c445f":"markdown","efed39fa":"markdown","3765029b":"markdown","d0b63d43":"markdown","76ecedfa":"markdown","ace97a63":"markdown","d98a62f7":"markdown","7f6cb413":"markdown","e85db77a":"markdown","027e8fb6":"markdown","a71db7d5":"markdown","211f673c":"markdown","cfb18492":"markdown","63f5ddd7":"markdown","5e186da6":"markdown","57b5fddb":"markdown","c6e5b110":"markdown","77883466":"markdown","76f96721":"markdown","58b24bdb":"markdown","aeff6165":"markdown","cda1d117":"markdown"},"source":{"417e2348":"import pandas as pd\nimport numpy as np\n\n#Machine Learning\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import learning_curve\nimport sklearn.metrics as metrics \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\n\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, \\\n                            roc_curve, roc_auc_score, average_precision_score, auc, f1_score\n\n\n#MatPlotLib\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\n# pip install dash\n\n# import dash\n# import dash_core_components as dc# c\n# import dash_html_components as ht# ml\n# from dash.dependencies import Input, Output\n\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots","a4ecb91b":"def count_duplicated(df):\n  df_values = {}\n  for column in df.columns:\n      df_values[column] = df.duplicated(subset=[column]).sum()\n  df_values = pd.DataFrame(df_values, index=[\"Duplicated Values\"])\n  return df_values","89245e63":"def check_duplicated_count(df, df_values):\n\n  #To confirm the result, we count the number of unique values in each column\n  df_values2 = {}\n  for column in df.columns:\n      df_values2[column] = df[column].nunique()\n\n  #And, then, we check if the results are the same for every variable\n  df_shape = df.shape\n  for column in df.columns:\n    if (int(df_values[column]) + int(df_values2[column]) - int(df_shape[0])) != 0 :\n      raise 'There is an error'\n  return 'Everthing is good!'\n\n\ndef check_for_outliers(df):\n  outliers = {}\n  for column in df.columns:\n      q1 = df[column].quantile(0.25)\n      q3 = df[column].quantile(0.75)\n      iqr = q3 - q1\n      lower_bound = q1 - (iqr * 1.5)\n      upper_bound = q3 + (iqr * 1.5)\n      outliers[column] = len(df) - df[column].between(lower_bound,upper_bound).sum()   #Compute the difference between the total rows and the rows between the bounds\n  return pd.DataFrame(outliers, index=['Outliers'])","989ddbab":"def histogram_plots (df_good, df_fraud, features):\n  plt.figure(figsize=(25, 25)) \n\n  for i in range(0,len(features)):\n    plt.subplot(11, 3, i+1)\n    ax=plt.hist(df_good[features[i]], density='TRUE', bins=100, label='Good transactions', alpha=0.5)\n    ax=plt.hist(df_fraud[features[i]], density='TRUE', bins=100, label='Fraud transactions', alpha=0.5)\n    plt.grid()\n    plt.xlabel(features[i]+\" value\")\n    plt.ylabel(\"Density\")\n    plt.legend()\n    plt.tight_layout()\n    \n\ndef boxplot_plots (df, target_column, features):\n  plt.figure(figsize=(25, 25)) \n\n  for i in range(0,len(features)):\n    plt.subplot(10, 3, i+1)\n    ax = sns.boxplot(x=df[target_column], y=df[features[i]])\n    plt.ylabel(features[i]+' value')\n    plt.tight_layout()\n    plt.grid()\n    \n    \ndef violinplots (df, target_column, exclude_this_column):\n  features_v = df.copy()\n  features_v.set_index(target_column, inplace=True)\n  stacked = features_v.stack().reset_index(level=target_column).reset_index().iloc[1:]\n\n  stacked.columns = ['Variable',target_column,'Normalized_Value']\n  stacked = stacked[stacked['Variable']!= exclude_this_column][stacked['Variable']!= 'Amount']\n  print(stacked[\"Variable\"])\n\n\n  fig = go.Figure()\n\n  fig.add_trace(go.Violin(x=stacked[\"Variable\"][ stacked[target_column] == 1 ],\n                          y=stacked[\"Normalized_Value\"][ stacked[target_column] == 1 ],\n                          legendgroup='Positive Class', \n                          scalegroup='Yes', \n                          name='Yes',\n                          side='negative',\n                          line_color='blue')\n              )\n  fig.add_trace(go.Violin(x=stacked[\"Variable\"][ stacked[target_column] == 0 ],\n                          y=stacked[\"Normalized_Value\"][ stacked[target_column] == 0 ],\n                          legendgroup='Negative Class', \n                          scalegroup='No', \n                          name='No',\n                          side='positive',\n                          line_color='orange')\n              )\n\n  fig.update_layout(\n      autosize=False,\n      width=3000,\n      height=600,\n      title='Interactive Violinplots'\n  )\n\n\n  fig.update_traces(meanline_visible=False)\n  fig.update_layout(violingap=0, violinmode = 'overlay')\n  fig.show()","aaf758d0":"def create_df(set_):\n  df_training=set_[0:round(0.7*len(set_))]\n  df_cv=set_[round(0.7*len(set_)):round(0.9*len(set_))]\n  df_test=set_[round(0.9*len(set_)):]\n\n  #df_appo, df_test = train_test_split(set_, test_size=0.3, random_state=1, shuffle=True )\n  #df_training, df_cv = train_test_split(df_appo, test_size=0.3, random_state=1, shuffle=True )\n\n  return(df_training,df_cv,df_test)\n\n\ndef print_info_df(df_training,df_cv,df_test):\n  print(\"The trainig dataset has: \"+str(len(df_training))+\" events, and there are \"+str(len(df_training[df_training['Class']==1]))+\" fraud events and \"+str(len(df_training[df_training['Class']==0]))+\" good events.\")\n  print(\"The cv dataset has: \"+str(len(df_cv))+\" events, and there are \"+str(len(df_cv[df_cv['Class']==1]))+\" fraud events and \"+str(len(df_cv[df_cv['Class']==0]))+\" good events.\")\n  print(\"The test dataset has: \"+str(len(df_test))+\" events, and there are \"+str(len(df_test[df_test['Class']==1]))+\" fraud events and \"+str(len(df_test[df_test['Class']==0]))+\" good events.\\n\")\n\n\ndef plot_heat_map(set_, features,):\n  heat_df = set_.loc[:, features ].corr()\n  if len(set_) == len(set_1):\n    heat= go.Heatmap( z = heat_df,x =features ,y =features, hoverongaps = False) #We plot the legend only one time\n    return heat\n  else:\n    heat= go.Heatmap( z = heat_df,x =features ,y =features, hoverongaps = False, showscale=False)\n    return heat\n\ndef heatmap_matrix (datasets, plot_titles, features):\n  fig = make_subplots(rows=1, cols=4, subplot_titles=plot_titles, shared_xaxes=True, shared_yaxes=True,)\n\n  fig.add_trace(plot_heat_map(datasets[1], features,), 1,1)\n  fig.add_trace(plot_heat_map(datasets[2], features ),1,2)\n  fig.add_trace(plot_heat_map(datasets[3], features ),1,3)\n  fig.add_trace(plot_heat_map(datasets[0], features ),1,4)\n\n  fig.update_layout(\n      autosize=False,\n      width=1400,\n      height=450,\n      title={'text':' Correlation matrix by Set',\n          'y':0.95,\n          'x':0.5,\n          'xanchor': 'center',\n          'yanchor': 'top'}\n    )\n  fig.show()\n\ndef barplots (variable_names, coefficients, title, y_axis_name):\n  fig = go.Figure()\n  fig.add_trace(go.Bar(x=variable_names,\n                  y=coefficients[0].flatten(),\n                  name='Set 1',\n                  marker_color='#0892A5'\n                  ))\n  fig.add_trace(go.Bar(x=variable_names,\n                  y=coefficients[1].flatten(),\n                  name='Set 2',\n                  marker_color='#A1E5AB'\n                  ))\n  fig.add_trace(go.Bar(x=variable_names,\n                  y=coefficients[2].flatten(),\n                  name='Set 3',\n                  marker_color='#A23E48'\n                  ))\n\n  fig.update_layout(\n      title=title,\n      xaxis_tickfont_size=14,\n      yaxis=dict(\n          title=y_axis_name,\n          titlefont_size=16,\n          tickfont_size=14,\n      ),\n      legend=dict(\n          x=0,\n          y=1.0,\n          bgcolor='rgba(255, 255, 255, 0)',\n          bordercolor='rgba(255, 255, 255, 0)'\n      ),\n      barmode='group',\n      bargap=0.15, # gap between bars of adjacent location coordinates.\n      bargroupgap=0.1 # gap between bars of the same location coordinate.\n  )\n  fig.show()","bced9e8a":"def model_evaluation(df_training,df_cv,df_test, features, model):\n\n  X_train=df_training.loc[:, features].to_numpy() #create the variables for the training\n  Y_train=df_training.loc[:, 'Class'].to_numpy() #separate the target variable used for the classification\n\n  X_cv=df_cv.loc[:, features].to_numpy()\n  Y_cv=df_cv.loc[:, 'Class'].to_numpy()\n\n  X_test=df_test.loc[:, features].to_numpy()\n  Y_test=df_test.loc[:, 'Class'].to_numpy()\n\n  model.fit(X_train,Y_train) #fit the model with the training data \n\n  #print the report for the train and cv sets\n  #print(classification_report(Y_train, model.predict(X_train))) \n\n  print(classification_report(Y_test, model.predict(X_test))) \n\n  Y_test_predict = model.predict(X_test)\n  Y_test_predict_proba = model.predict_proba(X_test)[:, 1]\n\n  #We use the average precision to evaluate the performance of the model trained on the test sample\n  print(\"Modello finale score (avg precision): \" + str(average_precision_score(Y_test, Y_test_predict)))\n\n  return (X_train,Y_train,X_cv,Y_cv,X_test,Y_test,Y_test_predict_proba)\n\ndef calculate_plot_cv_LR(X_train,Y_train,X_cv,Y_cv,values):\n  train_scores=[]\n  cv_scores=[]\n\n  for value in values: #train and evaluate the score for every value in the list values \n  \n    model=LogisticRegression(C=value,solver='liblinear',random_state=42, penalty='l2',  class_weight='balanced', max_iter=1000)\n    model.fit(X_train, Y_train)\n    Y_train_predict = model.predict(X_train)\n    Y_cv_predict = model.predict(X_cv)\n\n    train_scores.append(average_precision_score(Y_train, Y_train_predict))\n    cv_scores.append(average_precision_score(Y_cv, Y_cv_predict))\n\n  plt.figure(figsize=(10,5))\n\n  plt.plot(C_values, train_scores, label='Training', marker='o', lw=0, ms=10)\n  plt.plot(C_values, cv_scores, label='Cross-Validation', marker='*', lw=0, ms=12)\n\n  plt.xlabel(\"C\", fontsize=20)\n  plt.ylabel(\"average precision\", fontsize=20)\n  plt.xlim(1e-9, 1e9)\n\n  plt.xscale('log')\n\n  plt.grid()\n  plt.legend(loc='best',fontsize=20)\n  plt.tick_params( length=2, width=2, grid_alpha=1, labelsize=20)\n\n  return (train_scores,cv_scores)\n\n\ndef precision_recall_plot (precision_list, recall_list, labels):\n  plt.figure(figsize=(10,6))\n\n  plt.plot(precision_list[0], recall_list[0], label=labels[0])\n  plt.plot(precision_list[1], recall_list[1], label=labels[1])\n  plt.plot(precision_list[2], recall_list[2], label=labels[2])\n\n  plt.xlabel('precision')\n  plt.ylabel('recall')\n  plt.legend()\n  plt.grid()\n    \n    \ndef plot_confusion_matrix(Y, X, model, ax):\n  cm = confusion_matrix(Y, model.predict(X))\n  #fig, ax = plt.subplots(figsize=(4, 4))\n  ax.imshow(cm)\n  ax.grid(False)\n  ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n  ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n  ax.set_ylim(1.5, -0.5)\n  for i in range(2):\n      for j in range(2):\n         ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n            \n            \ndef confusion_matrix_plots (Y, X, models, title):\n  fig,ax=plt.subplots(1,3, figsize=(18,8))\n\n  plot_confusion_matrix(Y[0], X[0], models[0], ax[0])\n  plot_confusion_matrix(Y[1], X[1], models[1], ax[1])\n  plot_confusion_matrix(Y[2], X[2], models[2], ax[2])\n\n  ax[0].title.set_text('Set 1')\n  ax[1].title.set_text('Set 2')\n  ax[2].title.set_text('Set 3')\n\n  plt.subplots_adjust(wspace=0.5)\n  fig.suptitle(title)\n\n  plt.show()\n    \n    \n    \ndef plot_proba(model,X_test,ax,title):\n  predict_proba=model.predict_proba(X_test)\n  ax.hist(predict_proba[:,0], density=True, label='Fraud',alpha=0.5, bins=10)\n  ax.hist(predict_proba[:,1], density=True, label='Good', alpha=0.5, bins=10)\n  ax.set_xlabel('Predicted probabilities')\n  ax.set_ylabel('Counts')\n  ax.set_title(title)\n  ax.grid()\n  ax.legend()\n    \n    \ndef plot_learning_curves(model,title,ax,x_train,y_train):\n  train_size,train_scores,val_scores=learning_curve(model,x_train,y_train,cv=3)\n\n  train_scores_plot=np.mean(train_scores,axis=1)\n  val_scores_plot=np.mean(val_scores,axis=1)\n\n  ax.plot(train_size,train_scores_plot,marker='*',label='Train')\n  ax.plot(train_size,val_scores_plot,marker='.',label='Validation')\n  ax.set_xlabel('Size training sample')\n  ax.set_ylabel('Accuracy of the model')\n  ax.grid()\n  ax.legend()\n  ax.set_title('Learning Curves '+str(title))","5d499a95":"df=pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","037d5f7c":"if (df.isna().sum().sum() == 0) == True:\n  print(\"There are no missing data, we're lucky!\")\nelse:\n  print(\"This need further investigation\")","43bbbd28":"if (df.duplicated().any()) == True:\n  duplicated_rows = df.duplicated().sum()\n  print ('There are {} rows duplicated'.format(str(duplicated_rows)))","9d9165fb":"print(\"Original Number of Rows:\" + str(df.shape[0]))\ndf = df.drop_duplicates()\nprint(\"Filtered Number without Duplicated Rows:\" + str(df.shape[0]))","559152a7":"df_values = count_duplicated(df)\ndf_values","31475985":"check_duplicated_count(df, df_values)","c1144bd9":"check_for_outliers(df)","6c90bd8d":"df.describe()","204ad1de":"#Divide the main dataset (df) in others two to distinguish the good and fraud transactions\ndf_good=df[df['Class']==0]\ndf_fraud=df[df['Class']==1]\n\nprint(\"In the dataset there are \"+str(len(df_good))+\" good transactions against \"+str(len(df_fraud))+\" fraudolent transactions\")\nprint(\"The percentage of fraud transactions agains the total is \"+ str((round((len(df_fraud)*100\/len(df)),3)))+\"%\")","2c4885c9":"features=df.columns\nfeatures","0225ca51":"df.head()","ddba1ca2":"#print (\"Avaiable Variables:\")\n#print(*features)\n#plot_this_feature = input(\"Select one of the variables:\")\n\nplot_this_feature='V3'\n\nplt.figure(figsize=(23, 8)) \n\nax=plt.hist(df_good[plot_this_feature], density='TRUE', bins=100, label='Good transactions', alpha=0.5)\nax=plt.hist(df_fraud[plot_this_feature], density='TRUE', bins=100, label='Fraud transactions', alpha=0.5)\nplt.grid()\nplt.title('Histogram of the Feature ' + str(plot_this_feature),  fontsize = 22, pad= 25)\nplt.xlabel(plot_this_feature+\" value\", fontsize = 18, labelpad=10)\nplt.ylabel(\"Density\",  fontsize = 18, labelpad=10)\nplt.legend(fontsize = 18)\n\nplt.show()","bbe1fa46":"random_state_seed=42 #\"The answer\"","0ea2a72a":"fraud_len=len(df_fraud)","eaf4347f":"#This set is composed by the same number of fraud and good transactions\nset_1=pd.concat([df_fraud.sample(frac=1, random_state=random_state_seed), df_good.sample(frac=1, random_state=random_state_seed)[:fraud_len]], axis=0)\n\n#This set is slightly unbalanced towards the negative class\nset_2=pd.concat([df_fraud.sample(frac=1, random_state=random_state_seed), df_good.sample(frac=1, random_state=random_state_seed)[:int(fraud_len*3)]])\n\n#This set is heavily unbalanced towards the negative class\nset_3=pd.concat([df_fraud.sample(frac=1, random_state=random_state_seed), df_good.sample(frac=1, random_state=random_state_seed)[:int(fraud_len*9)]])","2f776356":"#Standardize the values of all the variables in the dataset except for the target one\nfor elem in (set_1,set_2,set_3):\n  scaler=MinMaxScaler((-1,1))\n  for column in elem.columns:\n      if column!='Class': #target class\n      #scaler= StandardScaler()\n        elem[column]=scaler.fit_transform(elem[[column]])","54012381":"use_this_set=set_1\nplot_this_feature = 'V5'\n\nplt.figure(figsize=(25, 8)) \nax = sns.boxplot(x=use_this_set['Class'], y=use_this_set[plot_this_feature])\nplt.ylabel(plot_this_feature+' value', fontsize = 18, labelpad=10)\nplt.xlabel('Class', fontsize = 18, labelpad=10)\nplt.title('Boxplot of the Feature ' + str(plot_this_feature),  fontsize = 22, pad= 25)\n\n\nplt.show()","4e624c0e":"# #@title Violin Plot by Variable { run: \"auto\" }\n# # use_this_set = set_1 #@param [\"set_1\", \"set_2\", \"set_3\"] {type:\"raw\"}\n# # plot_this_feature = \"V2\" #@param ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n\n# print (\"Avaiable Variables:\")\n# print(*features)\n# use_this_set = input(\"Choose between Original_Dataset, Set_1, Set_2 and Set_3:\")\n# plot_this_feature = input(\"Select one of the variables:\")\n\n# if use_this_set == \"Original_Dataset\":\n#     use_this_set = df\n#     set_label = \"Original DataSet\"\n# elif use_this_set == \"Set_1\":\n#     use_this_set = set_1    \n#     set_label = \"Set 1\"\n# elif use_this_set == \"Set_2\":\n#     use_this_set = set_2\n#     set_label = \"Set 2\"\n# elif use_this_set == \"Set_3\":\n#     use_this_set = set_3\n#     set_label = \"Set 3\"\n\n\nvp_df=set_1\nplot_this_feature='V6'\n\nfig = go.Figure()\n\nfig.add_trace(go.Violin(x=vp_df['Class'][vp_df['Class'] == 0],\n                        y=vp_df[plot_this_feature][vp_df['Class'] == 0],\n                        name=\"0 - Not Fraud\",\n                        box_visible=True,\n                        meanline_visible=True, x0=\"0 - Good\"))\n\nfig.add_trace(go.Violin(x=vp_df['Class'][vp_df['Class'] == 1],\n                        y=vp_df[plot_this_feature][vp_df['Class'] == 1],\n                        name=\"1 - Fraud\",\n                        box_visible=True,\n                        meanline_visible=True))\n\nfig.update_traces(meanline_visible=True,\n                  points='all') # show all points\n\nfig.update_layout(\n    title='Violin Plot of the Feature ' + plot_this_feature,\n    xaxis_tickfont_size=14,\n    yaxis=dict(\n        title='Normalized Value',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),\n    legend=dict(\n        x=0.40,\n        y = 0.95,\n        bgcolor='rgba(255, 255, 255, 0)',\n        bordercolor='rgba(255, 255, 255, 0)',\n    )\n)\n\nfig.show()","44b3f2ff":"heatmap_matrix(datasets = [df, set_1, set_2, set_3], plot_titles = [\"Set 1 (50\/50)\", \"Set 2 (25\/75)\" , \"Set 3 (10\/90)\", \"Total Dataset\"], features = features)","f067da50":"set_1=set_1.sample(frac=1, random_state=random_state_seed)\nset_2=set_2.sample(frac=1, random_state=random_state_seed)\nset_3=set_3.sample(frac=1, random_state=random_state_seed)","09eec3d1":"(df_training1,df_cv1,df_test1)=create_df(set_1)\nprint_info_df(df_training1,df_cv1,df_test1)\n\n(df_training2,df_cv2,df_test2)=create_df(set_2)\nprint_info_df(df_training2,df_cv2,df_test2)\n              \n(df_training3,df_cv3,df_test3)=create_df(set_3)\nprint_info_df(df_training3,df_cv3,df_test3)","a7727239":"#We add to the new dataframe the \"amount\" and \"time\" scaled columns, removing the old ones\nfeatures_train=features.drop('Class')\nfeatures_train=['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'Amount']","6192ade0":"C_values=np.array([1.e-08, 1.e-07, 1.e-06,1.e-05, 1.e-04,1.e-03, 1.e-02,1.e-01, 1.e+00,1.e+01, 1.e+02,1.e+03, 1.e+04,1.e+05, 1.e+06,1.e+07,\n       1.e+08,1.e+09, 1.e+10])","c081c2d7":"print('################   Logistic Regression with set 1:   ################')\nmodel_LR1=LogisticRegression(solver='liblinear',random_state=random_state_seed, penalty='l2',  class_weight='balanced',C= 1.e-04)\n(X_train1,Y_train1,X_cv1,Y_cv1,X_test1,Y_test1,Y_test_predict_proba1)=model_evaluation(df_training1,df_cv1,df_test1, features_train, model_LR1)\nprint('===========================================\\n')\n\nprint('################   Logistic Regression with set 2:   ################')\nmodel_LR2=LogisticRegression(solver='liblinear',random_state=random_state_seed, penalty='l2',  class_weight='balanced',C= 1.e-04)\n(X_train2,Y_train2,X_cv2,Y_cv2,X_test2,Y_test2,Y_test_predict_proba2)=model_evaluation(df_training2,df_cv2,df_test2, features_train, model_LR2)\nprint('===========================================\\n')\n\nprint('################   Logistic Regression with set 3:   ################')\nmodel_LR3=LogisticRegression(solver='liblinear',random_state=random_state_seed, penalty='l2',  class_weight='balanced',C= 1.e-04)\n(X_train3,Y_train3,X_cv3,Y_cv3,X_test3,Y_test3,Y_test_predict_proba3)=model_evaluation(df_training3,df_cv3,df_test3, features_train, model_LR3)","cc32584a":"coef1=np.abs(model_LR1.coef_)\ncoef2=np.abs(model_LR2.coef_)\ncoef3=np.abs(model_LR3.coef_)","66be7ce8":"set_1 = \"Yes\" #@param [\"Yes\", \"No\"]\nset_2 = \"Yes\" #@param [\"Yes\", \"No\"]\nset_3 = \"Yes\" #@param [\"Yes\", \"No\"]\nimport plotly.graph_objects as go\n\nvariable_names = features_train\n\nfig = go.Figure()\nif set_1 == 'Yes':\n  fig.add_trace(go.Bar(x=variable_names,\n                  y=coef1.flatten(),\n                  name='Set 1',\n                  marker_color='#0892A5'\n                  ))\n\nif set_2 == 'Yes':\n  fig.add_trace(go.Bar(x=variable_names,\n                  y=coef2.flatten(),\n                  name='Set 2',\n                  marker_color='#A23E48'\n                  )) \n\nif set_3 == 'Yes':\n  fig.add_trace(go.Bar(x=variable_names,\n                  y=coef3.flatten(),\n                  name='Set 3',\n                  marker_color='#A1E5AB'\n                  ))  \n\nfig.update_layout(\n    title='Coefficient Plot of the Set',\n    xaxis_tickfont_size=14,\n    yaxis=dict(\n        title='Weight',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),\n    legend=dict(\n        x=0,\n        y=1.0,\n        bgcolor='rgba(255, 255, 255, 0)',\n        bordercolor='rgba(255, 255, 255, 0)'\n    ),\n    barmode='group',\n    bargap=0.15, # gap between bars of adjacent location coordinates.\n    bargroupgap=0.1 # gap between bars of the same location coordinate.\n)\nfig.show()","6e13cd6a":"print('Recall LR1: ' +str(round(metrics.recall_score(Y_test1,model_LR1.predict(X_test1)),3)))\nprint('Recall LR2: ' +str(round(metrics.recall_score(Y_test2,model_LR2.predict(X_test2)),3)))\nprint('Recall LR3: ' +str(round(metrics.recall_score(Y_test3,model_LR3.predict(X_test3)),3)))","ed0dcead":"fig,ax=plt.subplots(1,3,figsize=(20,6))\n\nplot_proba(model_LR1,X_test1,ax[0],'Model LR1')\nplot_proba(model_LR2,X_test2,ax[1],'Model LR2')\nplot_proba(model_LR3,X_test3,ax[2],'Model LR3')\nfig.suptitle('Predicted probabilities by the model on the test sample')","6b399c19":"confusion_matrix_plots (Y = [Y_test1, Y_test2, Y_test3], X = [X_test1, X_test2, X_test3], models = [model_LR1, model_LR2, model_LR3],title='Confusion matrixes for the Logistic Regression models')","83906a36":"precision1, recall1, thresholds1 = precision_recall_curve(Y_test1, Y_test_predict_proba1)\nprecision2, recall2, thresholds2 = precision_recall_curve(Y_test2, Y_test_predict_proba2)\nprecision3, recall3, thresholds3 = precision_recall_curve(Y_test3, Y_test_predict_proba3)\n\nprecision_recall_plot(precision_list = [precision1, precision2, precision3], recall_list = [recall1, recall2, recall3], labels = [\"Set1\", \"Set2\", \"Set3\"])\nplt.title('Logistic Regression Precision Recall curves by Set')","d43e3f82":"print(\"Modello 1 (AUC_ROC): \" + str(metrics.roc_auc_score(Y_test1, Y_test_predict_proba1)))\nprint(\"Modello 2 (AUC_ROC): \" + str(metrics.roc_auc_score(Y_test2, Y_test_predict_proba2)))\nprint(\"Modello 3 (AUC_ROC): \" + str(metrics.roc_auc_score(Y_test3, Y_test_predict_proba3)))","096fbec3":"print(\"Modello 1 (AUC_PR): \" + str(metrics.average_precision_score(Y_test1, Y_test_predict_proba1)))\nprint(\"Modello 2 (AUC_PR): \" + str(metrics.average_precision_score(Y_test2, Y_test_predict_proba2)))\nprint(\"Modello 3 (AUC_PR): \" + str(metrics.average_precision_score(Y_test3, Y_test_predict_proba3)))","c3833cd5":"from sklearn.ensemble import RandomForestClassifier\n\n\nprint('################   Random forest with set 1:   ################')\n\nmodel_RF1=RandomForestClassifier(n_estimators=100)\n(X_train1,Y_train1,X_cv1,Y_cv1,X_test1,Y_test1,YRF_test_predict_proba1)=model_evaluation(df_training1,df_cv1,df_test1, features_train, model_RF1)\nprint('===========================================\\n')\n\nprint('################   Random forest with set2:   ################')\n\nmodel_RF2=RandomForestClassifier(n_estimators=100)\n(X_train2,Y_train2,X_cv2,Y_cv2,X_test2,Y_test2,YRF_test_predict_proba2)=model_evaluation(df_training2,df_cv2,df_test2, features_train, model_RF2)\nprint('===========================================\\n')\n\nprint('################   Random forest with set3:   ################')\n\nmodel_RF3=RandomForestClassifier(n_estimators=100)\n(X_train3,Y_train3,X_cv3,Y_cv3,X_test3,Y_test3,YRF_test_predict_proba3)=model_evaluation(df_training3,df_cv3,df_test3, features_train, model_RF3)\n","e1b824e9":"from sklearn.model_selection import GridSearchCV\n\n\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n\nrf = RandomForestClassifier()\n\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","dbc1a050":"################################################################################################\n################################################################################################\n################################################################################################\n\n##############################            ATTENTION           ##################################\n\n################################################################################################\n################################################################################################\n################################################################################################\n\n#Just a fast application of the grid search to looking for the best parameters\n#Around 10 min to compile this section, if you have no time please go on the next cells\n#grid_search.fit(X_train1, Y_train1)","d16cc0be":"#grid_search.best_params_","5ee2308e":"#df_grid_search=pd.DataFrame(grid_search.cv_results_)\n#df_grid_search.sort_values('rank_test_score').head()","3c878bee":"confusion_matrix_plots (Y = [Y_test1, Y_test2, Y_test3], X = [X_test1, X_test2, X_test3], models = [model_RF1, model_RF2, model_RF3],title='Confusion matrixes for the Random Forest models')","77e800bd":"coef1_RF=np.abs(model_RF1.feature_importances_)\ncoef2_RF=np.abs(model_RF2.feature_importances_)\ncoef3_RF=np.abs(model_RF3.feature_importances_)","cce5a8e0":"barplots (variable_names = features_train, coefficients = [coef1_RF, coef2_RF, coef3_RF], title = 'Coefficient Plot for the 3 Sets infor the Random Forest models', y_axis_name = 'Weight')","75861dd6":"precision1RF, recall1RF, thresholds1RF = precision_recall_curve(Y_test1, YRF_test_predict_proba1)\nprecision2RF, recall2RF, thresholds2RF = precision_recall_curve(Y_test2, YRF_test_predict_proba2)\nprecision3RF, recall3RF, thresholds3RF = precision_recall_curve(Y_test3, YRF_test_predict_proba3)\n\nprecision_recall_plot(precision_list=[precision1RF, precision2RF, precision3RF], recall_list=[recall1RF, recall2RF, recall3RF], labels=[\"Set1\", \"Set2\", \"Set3\"])\nplt.title('Random Forest Precision Recall curves by Set')","6ce2eb11":"print('Recall RF1: ' +str(round(metrics.recall_score(Y_test1,model_RF1.predict(X_test1)),3)))\nprint('Recall RF2: ' +str(round(metrics.recall_score(Y_test2,model_RF2.predict(X_test2)),3)))\nprint('Recall RF3: ' +str(round(metrics.recall_score(Y_test3,model_RF3.predict(X_test3)),3)))","ba3b356e":"fig,ax=plt.subplots(3,1,figsize=(15,15))\n\nplot_learning_curves(model_RF1,'Random Forest 1',ax[0], X_train1,Y_train1)\nplot_learning_curves(model_RF1,'Random Forest 2',ax[1], X_train2,Y_train2)\nplot_learning_curves(model_RF1,'Random Forest 3',ax[2], X_train3,Y_train3)","3777936e":"train_scores1,cv_scores1=calculate_plot_cv_LR(X_train1,Y_train1,X_cv1,Y_cv1,C_values)\nplt.title('Model LR1')","d03faa8e":"train_scores2,cv_scores2=calculate_plot_cv_LR(X_train2,Y_train2,X_cv2,Y_cv2,C_values)\nplt.title('Model LR2')","eb43b12c":"train_scores3,cv_scores3=calculate_plot_cv_LR(X_train3,Y_train3,X_cv3,Y_cv3,C_values)\nplt.title('Model LR3')","a1b79441":"## Loading Functions\n<a id=\"functions\"><\/a>","63e8f1a3":"We can start to see the first indicators of fraudolent transactions. For example, they tend to have higher V4 and V11 values","3734ff07":"### Logistic Regression \n\n**What is a Logistic Regression Model?**\n\nThe LR Model is a predictive model used mainly to study phenomenons with a binary event outcome (fail\/pass, fraud\/no fraud, 0\/1, and so on)\nThe model has a categorical dependent variable with two possible values (like 0 and 1). The logarithm of the odds is computed using a linear combination of one or more independent variables, called \"predictors\".\n\n**How we set up our Logistic Regression Model**\n\nWe first define the train sets, the cross validation sets and the test sets.\n\nThen, we inizialize the Logistic Regressor of SkLearn using the liblinear solver (ideal for \"small\" dataset like this one) without specifing nothing about the weight of the classes.\n\nFinally, we evaluate the model using different methods.","0490f660":"As we've said in the introduction, the dataset is strongly unbalanced towards the negative class (legit transactions).\n\nFutrthermore, the values of the features aren't very relevant, considering the PCA-transformation.\n\nCreating a predictive model with this type of dataset is very difficult and can result in overfitting.\n\n<br>\n\n**Data Selection**\n\nOur goal was to create an easily scalable model while avoiding overfitting. To do that we have designed tre scenarios:\n\n*   One with a **balanced dataset** (50% of legit transactions and 50% of fraud transactions)\n*   One **slighlty unbalanced** (75% of legit transactions and 25% of fraud transactions)\n*   One **heavily unbalanced** (90% of legit transactions and 10% of fraud transactions)\n\nAfter that, we've removed the variables that weren't strong predictors of the target class (from feature V20 up to the feature V28). \n\nWe've computed the relation between the variables and the target class using a correlation matrix and an heatmap to visualize that.\n\nYou can see from the heatmaps that increasingly unbalanced datasets lead to weaker relations between the variables and the target class.\n\n<br>\n\n**Models Comparison**\n\nFor the actual machine learning part, we've chosen a simple yet powerful model called \"Logistic Regressor\" that has yielded good results.\nEven with the third dataset, the most unbalanced one, the model still had good performances.\n\nThe results are consistent in the three models, as displayed by the interactive barplot in the notebook, because the coefficients are proportional to the dataset.\n\nLastly, we've used a more complex model, called \"Random Forest\", to have a term of comparison with the Logistic Regressor.\n\nIn order to tune the hyperparameters at their best, we've used a simple grid search over the most important features (it wasn't feasibile doing that for every feature because of the computational cost of the operation).\n\nThe Random Forest model showed far better results than the Logistic Regressor.\n\nFrom the learning curve it's possible to see that increasing the sample size leads to better performances, although narrowly.\n\nAs a final note, we should be aware that the we have few data for the test part. So, the confusion matrix and the precision\/recall plots aren't very reliable, they gives us just a general idea of the performance.\n\n<br>\n\nIn conclusion, we can say that the best models are the Logistic Regression and the Random Forest applied over the third set.\n\n<br>\n<br>\n\nThanks for the reading,\n\n*Luca Pessina e Gabriele Carbone*","2159c315":"<h2>Data Selection<\/h2>\n\n\nIn this section we will scale the features that are not scaled (Time and Amount) and we will create the dataset that we will use for the training, cross-validation and test pf our classification models. We have to decide which features we will use, which kind of data and the composition of our dataset. A general idea could be:\n\n**Set 1** \u27f6 50% Class 1 + 50% Class 0\n\n**Set 2** \u27f6 25% Class 1 + 75% Class 0\n\n**Set 3** \u27f6 10% Class 1 + 90% Class 0\n\nThet we will split these 3 sets in 3 different subset each for the training (+cv) and test part (90%+10%). During the project we will study the performance of the model changing the training sample to understand which one gives the best perfomance.","bf902abb":"Standardizing the features using a standard deviation is important when we compare measurements that have different units. Variables that are measured at different scales do not contribute equally to the analysis and might end up creating a bias.\n\nFor example, a variable that ranges between 0 and 1000 will outweight a variable that ranges between 0 and 1. Using these variables without standardization will give the variable with the larger range weight of 1000 in the analysis. Transforming the data to comparable scales can prevent this problem. Typical data standardization procedures equalize the range and\/or data variability.\n\nIn our case, we have standardized all the columns minus the target class using the scikitlearn integrated functions *MinMaxScaler* and *fit_transform*","d9499798":"<h3>Variable Weights<\/h3>","a700696e":"# DATA MODELLING","89df480a":"Used to create the three dataset studied, they are different in the number of Class 0 events","eacfca86":"All the plots are interactive but is not possible to display that on kaggle so I selected some features and datasets. The code to make the plots interactive is commented into the cell code. ","7e64bf92":"## Describing the Dataset\n\nFirst, we explore the dataset using the built-in pandas method .describe()","7116a5dc":"It is possible to see the plot obtained from the cross validation on the parameter C for the logistic regression. Are shown the validation and training scores and the relative value of C. The datasets are very small and we select the data in a random way so these graphs don't give us a good information. It is possible to see the high overfitting status of the model for values of C>1 and underfitting status for C< 1 given by the hight regularitation. ","c1f1a246":"We're ready to summarize what we've learned until now:\n\n\n*   The \"amount\" column is highly unbalanced\n*   Variables V4 and V11 seem somewhat useful in detecting frauds\n*   The amount and the time don't seem to correlate very well with the target class\n*   We can not really say anything about the distribution of the variables because they have not real meaning (mathematical of physical).\n","3a9a75ff":"## Data Visualization\n\nIn our dataset there are 2 types of classes, one is the fraudolent transiction (labeled as 1) the other is the good transaction class (labeled as 0). We will use two different dataset to explore the different charateristics among the classes and the features.","ce2b5d9e":"Then, we check the number of duplicate rows in the dataset","d5faa0c5":"**Finding Duplicates**","90ac4500":"The plots gives us addictional information about the variables that influence the target class. We can see, as we thought, that V4, V11 and V18 are strongly correlated in some way with a fraudlent transaction. But there are many more indicators that show up (like V12 and V14), this means that our model can be more precise. \n\nFrom these plots we can uderstand how much is important the data selection, if we didn't, we would have less correlation between the Class and the features, as the third plot shows. For example the correlation between the 'Class' and V4 is 0.70 for the set 1 but is only 0.13 for the whole dataset. \n","ad892b45":"We're ready to summarize what we've learned so far:\n\n*   The dataset is very unbalanced towards the negative class\n*   There are no missing  values\n*   There were more than one thousand rows duplicated\n*   There are some features with many outliers (like V27 and V28). In the next steps we will remove the variable from 20 to 28 because they're poorly correlated with the target class so they offer minimal information with an high degree of risk.<br>\nYou can see in the photo below the absolute values of weight of the coefficients in the logistic regression, method used in our preparation study. The higher the coefficient, the greater the discriminating power of the relative variable. So features between 20 and 28 are not very important related to V3 or V4 for example. \n\n![Weight_Plot.PNG](data:image\/png;base64,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)\n\n","51356586":"**Missing Data**\n\nFirst, we check if there are any columns with missing data","629a72fe":"<h3>Violin Plot by Variable<\/h3>","88263159":"Read the dataset in a cvs format and create the variable 'df', it will be used in all the future steps.\n\n<strong> THE DATASET MUST BE IN THE SAME DIRECTORY AS THE NOTEBOOK<\/strong>","9e69bee3":"<h3>Data Visualization<\/h3>","9a2efb25":"# APPENDIX\n\nIn this short section we will post all the non essential material, it could be used to better udestand some parts of the previuos sections. ","91f7c6c9":"In this section we report the distributions of our variables distincted among the two classes, this plots give us a general idea about the distributions. We will repeat the plot using the training data.\n\n*In our preliminary study, we have seen that these histograms (and the boxplots) weren't much descriptive of the data. So we have chosen instead to use them in the later sections of the notebook.*","ca62020a":"Finally, we display the tradeoff between recall and precision mentioned before.\n\nAs we can see, our model sligthly favors recall against precision. ","0ab04129":"## Loading Dataset","95ef9175":"In order to have a consistent story-telling, we will set a random state seed so the \"random shuffling\" will return each time the same outputs.","2ff7ed28":"#### Model Evaluation","44ae9930":"<h3>Machine Learning<\/h3>","70c1ed25":"In this function we summarize all the steps for train and evaluate the performace of a model starting from the datasets used for training, cross validation and test.","2c3af9e7":"In this section we want to have a look into the grid search method to looking for the best parameters for the model. Found the best classifier goes further the aim of this project so is it not investigated more. We will use the classifier with the predefinite parameters. ","7dcaeb21":"# Introduction to the DataSet\n\nThe dataset contains over 284,000 transactions between european cardholders in September 2013.\n\nThere are over 30 variables, most of them (28) are crypted. The only three columns not crypted are the time, the amount of the transaction and the target class.\n\nOnly 492 over 248,807 transaction, the 0.172%, are classified as frauds. Therefore, the dataset is highly unbalanced towards the negative class.\n\nThe main risks of an unbalanced dataset are **overfitting** and detecting the right correlations (we are limited by the PCA transformation in the majority of columns) ","472f12a0":"Outliers can be very dangerous: even a single one (with very large value or a small one) can greatly reduce the precision of our models.\n\nTo identify them, we use the IQR Rule.\n\n>**INTERQUARTILE RULE**<br>\n1.   Calculate the interquartile range for the data\n2.   Multiply the interquartile range (IQR) by 1.5 (a constant used to discern outliers).\n3.   *Upper_Bound:* Add 1.5 x (IQR) to the third quartile. Any number greater than this is a suspected outlier.\n4.   *Lower_Bound:* Subtract 1.5 x (IQR) from the first quartile. Any number less than this is a suspected outlier.\n","d7e3293a":"# DATA EXPLORING","cf0e533e":"Hello everyone,\n\nThe aim of the project is to create a machine learning model to predict whether a transaction it\u2019s a fraud or not, the focus of the project is based on unbalanced datasets and how to manage them. \n\nThe project follows the OSEMN framework:\n\n> **O** - Data Obtain\n<br>**S** - Data Scrubbing\n<br>**E** - Data Exploring\n<br>**M** - Data Modelling and Machine Learning\n<br>**N** - Data Interpreting\n\nWe skip the first step because we already have the dataset\n\n<br>\n\n*To start the project, please load the libraries, the functions and the dataset in the sections below*","4fdf06a0":"# DATA INTERPRETATION","5b5a8ac1":"Luckily, there is no feature with this characteristic","a5767446":"Next, we need to check if there are columns that have only one unique value. In that case, we can safely remove them (and simplify the computations) because they don't give any additional information.","860f61d7":"#### Learning curves","8858e39d":"In this section we will understand the quality and the integrity of the database.\nWe will try to detect and remove not relevant data in order to improve the quality of the analysis and the prediction of the machine learning models deployed. ","bbc6f520":"Compare the number of duplicated values with the number of unique values and the total number of values, for each column.\n\nCreate the interquartile range and the bounds for the interquartile rule.\n\nAfter that, for each column computes the number of values outside the bounds","4d48537a":"Now we are ready for some exploring!\n\nIn the two cell below you can see how differently boxplots and violinplots supplies information to the viewer.\n\nClearly, violinplots bear more information at a glance, but they're certainly a more difficult to read. They're more suited for features were minimal differences in the distribution may be important.\n\nOn the other hand, the boxplots give a clear view of the main statistical measures (quartiles, iqr and outliers) while tossing away the kernel distribution of the feature.","328c445f":"#### Cross Validation (grid search)","efed39fa":"We can take a look inside our model using the *.coef* attribute of the regressor.\n\nWe can display the \"contribution\" that each variable give in the evaluation of the target class.","3765029b":"The classification report gives us a numerical evaluation of the model:\n\n\n*   The first column, **\"Precision\"**, is defined as ratio between True Positives and (True Positives + False Positives)\n*   The second column, **\"Recall\"**, is defined as the ratio between True Positives and (True Positives + False Negatives)\n*   The third column, **\"F1-Score\"**, is the harmonic mean of the precedent measures. It's computed as 2 x (Precision x Recall) \/ (Precision + Recall)  \n\n","d0b63d43":"<h3>Data Selection<\/h3>","76ecedfa":"<h3>Data Preparation<\/h3>","ace97a63":"<h2>DATA SCRUBBING<\/h2>","d98a62f7":"And we remove them.","7f6cb413":"## Machine Learning\n\nIn this section we will discuss about machine learning application, in particular this project is about a classification task. We want to create a model that can classify the different classes, in this case fraud or good trasactions. We will try a linear model, the logistic regression on different datasets (set1,set2 and set3) and a Random Forest, a more complex model. \n\nThe dataset will be divided into three parts: one for the training, one for the cross validation (to avoid overfitting) and the latter for the test or rather to calculate the performance of the model. \n\nThis is a very unbalanced dataset so we must be careful about the metric used to calculate the perfomance, for example accuracy is a bad one, so we will use the average precision. This metrics calculates the area under the precision recall curve (approximately).\n\nWe will use other matrices like ROC curves, precision-recall curve or other times of score.\n\nWe will use the models available on scikit-learn and we will find the best values for their hyperarameters. \n\nThe datasets that we will create are:\n\n\n**df_training** \u27f6 70% tot\n\n**df_cv** \u27f6 20% tot\n\n**df_test** \u27f6 10% tot ","e85db77a":"<h3>Histogram by Target Class<\/h3>","027e8fb6":"## Loading Libraries \n[<a id=\"libraries\"><\/a>](http:\/\/)","a71db7d5":"<h3>Box Plot by Variable<\/h3>","211f673c":"Count the duplicated values for each column","cfb18492":"**What is a Random Forest Model?**\n\nRandom forest is a flexible, easy to use machine learning algorithm that produces, even without hyper-parameter tuning, a great result most of the time. It is also one of the most used algorithms, because of its simplicity and diversity (it can be used for both classification and regression tasks).\n\nA Random Forest model builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n\nInstead of searching for the most important feature while splitting a node, the random forest searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model.\nTherefore, in random forest, only a random subset of the features is taken into consideration by the algorithm for splitting a node.\n\n**How we set up our Random Forest Model**\n\nWe've used the integrated function of scikitlearn called \"*RandomForestClassifier*\" on the three datasets created at the start of the section (\"df_training_1\", \"df_cv1\", \"df_test1\"). We repeat this operation for each set.\n\nIn order to increase the efficiency of our model, we setted up a grid search (a function that compute the outputs while trying different combinations of hyperparameters) and used the first ranked combination.","63f5ddd7":"**Detecting Outliers**","5e186da6":"#### Model Evaluation\n\nWe evaluate the results of the model in several ways.\n\nFirst, we use a confusion matrix. \nIt's a matrix with four quadrant, one for  each combination of class and prediction.\n\nThen we compute the score of the model using the dedicated sklearn function *Average_precision_score*\n\nFinally, we plot the precision\/recall curve which show the tradeoff between these two measure. A very useful indicator when the classes are unbalanced.","57b5fddb":"Next, we can explore the relations between the different features of the dataset.\n\nWe have plotted correlation matrices using heatmaps.\nBelow, you can see an heatmap for every set we've generated at the start of the session and an heatmap for the original dataset.\n\nNote that the matrices look *very* similar.\n\nThis makes sense.\n\nThe 3 sets were random sampled from the original one, we would have a problem if one of the matrices looked much different from the others.","c6e5b110":"We can see that the features V27 and V28 have many outliers. So, we must decide if these variable are so important that we should remove around 15% of our database or we should simply ignore them.\n\nOur preliminary study suggests us that the group of variables from V20 to V28 offer little to no information regarding the prediction of the target class.\nAs a consequence, we decided to drop them.","77883466":"![credit_cards.jpg](attachment:credit_cards.jpg)<h1 align=\"center\"> <\/h1>\n# Is it possible to detect fraud transactions?\n","76f96721":"#### Cross Validation","58b24bdb":"Looking at the \"Amount\" column we can see that the dataset has a very wide range of transactions, starting from as little as 5.6\u20ac and going over 25.000\u20ac in at least one case.\n\nWe can also see that the average transaction is much more close to the minimum than to the maximum and the median transaction is 22\u20ac. Therefore, we can expect a positive skewness in the curve.","aeff6165":"### Random Forest","cda1d117":"In this section we will conclude our studies on data visualitation and machine learning models applied to a classification task with an high unbalanced dataset. We will discuss the main different among the datasets and the models used and their perfomance on the test sample.\n"}}