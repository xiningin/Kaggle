{"cell_type":{"9daa6537":"code","d79e5355":"code","761547d5":"code","8660404a":"code","d3d412cb":"code","b7982074":"code","2a7f00ef":"code","512c99a7":"code","9fcda1d1":"code","c115413f":"code","d276ea30":"code","82e392e8":"markdown","0d352fb9":"markdown","78f24268":"markdown","d4c0be97":"markdown","6a4c90a4":"markdown","a97d8d73":"markdown","ca9ed81a":"markdown","d6d50d25":"markdown","25e0c6aa":"markdown","eeeed552":"markdown"},"source":{"9daa6537":"# Import Some Libraries Needed\n# If you are actually running the notebook uncomment the last\n# 'mkdir' line to make the directory\n# then comment it back after running\n\nimport numpy as np # Vital for Math\nprint('Numpy Import Success')\nimport pandas as pd  # Data Reading\nprint('Pandas Import Success')\nimport tensorflow as tf # Main Model Library\nprint('Tensorflow Import Success')\nfrom keras.models import Sequential # Keras\nfrom keras.layers import Dense # Keras\nfrom keras import optimizers # Keras\nfrom keras import losses # Keras\nprint('Keras Import Success')\nfrom tqdm import tqdm # Progress Bar\nprint('Tqdm Import Success')\nimport matplotlib.pyplot as plt # Data Visualization\nprint('Matplotlib Import Success')\nimport os # Creating file to save model\nos.mkdir(\"\/kaggle\/working\/models\")\n","d79e5355":"# Load the data from training\ndf = pd.read_csv('..\/input\/titanic\/train.csv')\ndf.columns","761547d5":"# Passenger Id and Name would have no effect\n# drop them\nX = df.drop(['PassengerId','Name','Ticket','Survived'], axis=1)\ny = df['Survived']\nprint(X)","8660404a":"# A Bunch of preprocessing functions\n# from this helpful notebook: https:\/\/www.kaggle.com\/jameskhoo\/deep-learning-with-keras-and-tensorflow\n\ndef simplify_ages(df):\n    #df['Age'] = df['Age'].fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n    categories = pd.cut(df['Age'], bins, labels=group_names)\n    df['Age'] = categories.cat.codes \n    return df\n\ndef simplify_cabins(df):\n    df['Cabin'] = df['Cabin'].fillna('N')\n    df['Cabin'] = df['Cabin'].apply(lambda x: x[0])\n    df['Cabin'] =  pd.Categorical(df['Cabin'])\n    df['Cabin'] = df['Cabin'].cat.codes \n    return df\n\ndef simplify_fares(df):\n    df['Fare'] = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    group_names = ['Unknown', '1_quartile', '2_quartile', '3_quartile', '4_quartile']\n    categories = pd.cut(df['Fare'], bins, labels=group_names)\n    df['Fare'] = categories.cat.codes \n    return df\n\ndef simplify_sex(df):\n    df['Sex'] = pd.Categorical(df['Sex'])\n    df['Sex'] = df['Sex'].cat.codes \n    return df\n\ndef simplify_embarked(df):\n    df['Embarked'] = pd.Categorical(df['Embarked'])\n    df['Embarked'] = df['Embarked'].cat.codes + 1\n    return df\n\ndef transform_features(df):\n    df = simplify_ages(df)\n    df = simplify_cabins(df)\n    df = simplify_fares(df)\n    df = simplify_sex(df)\n    df = simplify_embarked(df)\n    return df\n\nX = transform_features(X).to_numpy()\ny = y.to_numpy()","d3d412cb":"# Defining and building the model\ndef build_keras_model():\n    model = Sequential([\n        Dense(32, activation='relu'),\n        Dense(32, activation='relu'),\n        Dense(16, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n    return model\nmodel = build_keras_model()","b7982074":"# Initializing\nmodel(tf.constant(X, dtype=tf.float32))","2a7f00ef":"# Getting a Batch of Data\ndef get_batch(x, y, batch_size):\n    idx = np.random.choice(len(x), batch_size)\n    input_batch = [x[i] for i in idx]\n    label_batch = [y[i] for i in idx]\n    return input_batch, label_batch\n# Making 1D array to 2D matrix\nnew_y = np.reshape(y, (-1, 1))","512c99a7":"# Binary Crossentropy Function\ndef compute_loss(labels, logits):\n  loss = tf.keras.losses.binary_crossentropy(labels, logits, from_logits=True)\n  return loss","9fcda1d1":"# IMPORTANT:\n# parameters for training\n# changing them may change the model a lot\n\n# Learning Rate\nlearning_rate = 1e-2\n# Iterations\nepochs = 50000\n# Batch Size\nbatch_size = 16\n\n# Defining our optimizer\n# Adam and Adagrad are also suitable\noptimizer = tf.keras.optimizers.RMSprop(learning_rate)\n\n# Defining a checkpoint to save model\ncheckpoint_dir = '\/kaggle\/working\/models\/model.ckpt'\n\n# Train Step Function\n@tf.function\ndef train_step(x, y): \n  # Use tf.GradientTape()\n  with tf.GradientTape() as tape:\n    # Prediciting labels\n    y_hat = model(x) \n    # Computing Loss\n    loss = compute_loss(y, y_hat) \n  # Backprogating through landscape\n  grads = tape.gradient(loss, model.weights) # TODO\n  # Descending to local minimum\n  optimizer.apply_gradients(zip(grads, model.weights))\n  return loss\n\n# History for finding minimum\nhistory = []\n\n# Clearing Bar\nif hasattr(tqdm, '_instances'): tqdm._instances.clear()\n    \n\nfor iter in tqdm(range(epochs)):\n  # Get a Batch\n  x_batch, y = get_batch(X, new_y, batch_size)\n  y_batch = tf.constant(np.reshape(y, (-1, 1)), dtype=tf.float32)\n  x_batch = tf.constant(x_batch, dtype=tf.float32)\n  loss = train_step(x_batch, y_batch)\n    \n  # Update the progress bar\n  history.append(loss.numpy().mean())\n  num_iterations = [i for i in range(len(history))]\n  # Update the model with the changed weights!\n  if iter % 100 == 0:     \n    model.save_weights(checkpoint_dir)\n    \n# Save the trained model and the weights\nplt.plot(num_iterations, history)\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nmodel.save_weights(checkpoint_dir)","c115413f":"print(f'Local Minimum: {min(history)}')\nprint(f'Final Loss: {history[len(history) -1]}')","d276ea30":"# Submitting Predictions\ntesting_set = pd.read_csv('..\/input\/titanic\/test.csv')\nx_test = testing_set.drop(['PassengerId','Name','Ticket'], axis=1)\nx_test = transform_features(x_test)\n\npredictions = model.predict_classes(x_test)\nids = testing_set['PassengerId'].copy()\nnew_output = ids.to_frame()\nnew_output[\"Survived\"]=predictions\nnew_output.to_csv(\"another_submission3.csv\",index=False)\nprint(new_output)","82e392e8":"I hope this notebook will help you implement the really powerful `tf.GradientTape()`. It allows us more control over training than `model.fit()`, and is very easy to use. ","0d352fb9":"Passing some test data","78f24268":"Getting a random batch to boost gradient finding","d4c0be97":"## Building the Model\n\nNow we can get to the actual focus: building the model. We will use one input, two hidden, and one output 'sigmoid' layer, to make everything into some kind of probability between 0 and 1.","6a4c90a4":"## Titanic Competition with Tensorflow Keras\nThis a notebook of my solution that got a 79% score with just the basic mainframe. By cleaning up the data, and changing some function, you can achieve a score with a lot more points.\nI am a bit new to Kaggle, and found this Notebook: 'https:\/\/www.kaggle.com\/jameskhoo\/deep-learning-with-keras-and-tensorflow', really helpful for preprocessing the data.\n\nIn this we will go more into more of building a model with the highly useful function Gradient Tape, rather than spending much tine re-engineering data.","a97d8d73":"## Preprocessing\n\nI was stuck preprocessing the information to make it the best, then I found this notebook: https:\/\/www.kaggle.com\/jameskhoo\/deep-learning-with-keras-and-tensorflow. This had really helpful functions for preprocessing. Listed Here:","ca9ed81a":"## Loading in the Data","d6d50d25":"Computing a loss between predictions and labels","25e0c6aa":"Looking at the columns, we can say that Passenger Id and Name would not have any affect on the output, and would cause **unwanted bias** in the model.","eeeed552":"This is the part that is the reason this is more accurate with less engineering. We start by definining a few parameters, such as learning rate (size of steps to reach minimum), epochs (number of iterations), and batch size (amount of data taken). We then define our optimizer, which in this case is RMSprop. Adam and Adagrad will work well too. Next we create a tensorflow functions to simulate a train step, given a set up inputs and labels. `tf.GradientTape()` allows us to use this loss and backpropagate through the landscape. As you can see, we calculate gradients by taking a loss, and series of weights. Then we feed that into the optimizer which will continue descending. The last section is the actual training, and plotting to see how are model did.\n\nWe can see how the model did on the plot. It looks that the values are all over the place, but the since the batch size was very small, and there are 890 samples it took time to understand the patterns. Training it more will most likely overfit it."}}