{"cell_type":{"79378717":"code","2e670578":"code","0acba637":"code","08ed6435":"code","d0105dd6":"code","5ac6d9c3":"code","caa5d048":"code","cb39d9b7":"code","a495bef8":"code","1ec7290b":"code","72f1826c":"code","9d715f79":"code","1ecba3f1":"code","2b9de020":"code","4b47ed3b":"code","9a2e162a":"code","0df18bfb":"code","cb28a4c4":"code","db90a0f5":"code","39376ccf":"code","b8e21f37":"code","079771d8":"markdown","30a73b60":"markdown","f1b30248":"markdown","88696b74":"markdown","d54ca490":"markdown","b75b570a":"markdown"},"source":{"79378717":"# Ahora haremos una tareas de modeling sobre los 5665 textos que vann a entrenar un modelo LDA\n# Primero hay que limpiarlos y preprocesarlos, con la finalidad de bajar el ruido, se usar\u00e1 la librer\u00eda NLTK\n\nimport json, re\nimport pandas as pd \nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import ToktokTokenizer\n\n# Ahora se se carga el archivo de noticias y se lo convierte en un objeto DataFrame\n\nwith open('..\/input\/corpusnoticiaslda\/noticias.txt') as json_file:\n    datos = json.load(json_file)\ntuplas = list(zip([noticia.get(\"titular\") for noticia in datos],\n                  [noticia.get(\"texto\") for noticia in datos]))\ndf = pd.DataFrame(tuplas, columns =['Titular', 'Noticia'])\n\n# Me quedo con una peque\u00f1a muestra, borrar l\u00ednea para procesar todos los registros\n#  df = df.drop(df.index[500:5565])\n\nprint(df.shape) # muestra la dimensionalidad del dataframe creado, 2 atributos, 5665 registros\n\ndf.head() # muestra las primeras noticias\n","2e670578":"# Operaci\u00f3n de limpieza de los textos de cada noticia, se crea una funci\u00f3n\n# Se eliminan los caracteres especiales, las palabras con un solo car\u00e1cter y se pasa todo a min\u00fasculas\n \ndef limpiar_texto(texto): # Funci\u00f3n para realizar la limpieza de un texto dado.\n    # Eliminamos los caracteres especiales\n    texto = re.sub(r'\\W', ' ', str(texto))\n    # Eliminado las palabras que tengo un solo caracter\n    texto = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', texto)\n    # Sustituir los espacios en blanco en uno solo\n    texto = re.sub(r'\\s+', ' ', texto, flags=re.I)\n    # Convertimos textos a minusculas\n    texto = texto.lower()\n    return texto\n\n# se crea un atributo nuevo que va a ser el resultado de la limpieza\n\ndf[\"Tokens\"] = df.Noticia.apply(limpiar_texto)\n\ndf.head()","0acba637":"# Se  tokeniza el texto procesad. Consiste en dividir los textos en tokens o terminos individuales. \n\ntokenizer = ToktokTokenizer() \ndf[\"Tokens\"] = df.Tokens.apply(tokenizer.tokenize)\n\ndf.head()\n","08ed6435":"# Se eliminan las stopwords (palabras vac\u00edas o comunes) \n \nSTOPWORDS = set(stopwords.words(\"spanish\"))\n\ndef filtrar_stopword_digitos(tokens): # Filtra stopwords y digitos de una lista de tokens.\n    return [token for token in tokens if token not in STOPWORDS \n            and not token.isdigit()]\n\ndf[\"Tokens\"] = df.Tokens.apply(filtrar_stopword_digitos)\n\ndf.head()","d0105dd6":"# Se lleva a la forma de ra\u00edz a cada palabra usando el proceso de stemming\n\nstemmer = SnowballStemmer(\"spanish\")\n\ndef stem_palabras(tokens): #     Reduce cada palabra de una lista dada a su ra\u00edz.\n    return [stemmer.stem(token) for token in tokens]\n\ndf[\"Tokens\"] = df.Tokens.apply(stem_palabras)\n\ndf.head()\n","5ac6d9c3":"# Fin de la primera parte del proceso\n\n# A continuaci\u00f3n mostramos las primeras diez palabras procesadas de una noticia, la primera \n\nprint(df.Tokens[0][0:10])\n","caa5d048":"# Se debe construir un diccionario con todas las palabras y un corpus con la frecuencia de las palabras\n# en cada documento. Estas ser\u00e1n las principales entradas de nuestro modelo LDA. \n\n# Importamos las librer\u00edas que vamos a necesitar para este tutorial pr\u00e1ctico:\n\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n%matplotlib inline\n","cb39d9b7":"# Los diccionarios son objetos que asignan un identificador num\u00e9rico a cada palabra \u00fanica y \n# se pueden usar para obtener el identificador a partir de la palabra y viceversa.\n\n# La generaci\u00f3n de estos diccionarios con la librer\u00eda Gensim es tan f\u00e1cil como crear un objeto de tipo Dictionary \n# pas\u00e1ndole como argumento las listas de palabras que est\u00e1 en el atributo del dataframe llamado \"tokens\"\n\n\ndiccionario = Dictionary(df.Tokens)\n\nprint(f'N\u00famero de tokens: {len(diccionario)}')","a495bef8":"# Se limpian t\u00e9rminos del diccionario.Por ejemplo las palabras m\u00e1s raras o demasiado frecuentes. \n# Se aplica la funci\u00f3n filter_extremes, que nos dejar\u00e1 \u00fanicamente aquellos tokens que se encuentran \n# en al menos 2 documentos (no_below) y los que est\u00e1n contenidos en no m\u00e1s del 80% de documentos (no_above). \n\ndiccionario.filter_extremes(no_below=2, no_above = 0.8)\n\nprint(f'N\u00famero de tokens: {len(diccionario)}') # cantidad de tokens restantes luego del filtro\n","1ec7290b":"#  Se inicializa el corpus en base al diccionario creado. \n# Cada documento se transformar\u00e1 en una bolsa de palabras (BOW) con las frecuencias de aparici\u00f3n.\n\n# Al aplicar esta t\u00e9cnica veremos que cada documento est\u00e1 representado como una lista de tuplas \n# donde el primer elemento es el identificador num\u00e9rico de la palabra y el segundo es el n\u00famero \n# de veces que esa palabra aparece en el documento.\n\n# Creamos el corpus \ncorpus = [diccionario.doc2bow(noticia) for noticia in df.Tokens]\n\n# Mostramos el BOW de una noticia cualquiera, la seis en el ejemplo\nprint(corpus[6]) \n\n# Vemos que los primeros 5 elementos son: [(3, 1), (25, 1), (26, 6), (29, 1), (40, 1) \u2026].\n# La primera tupla (3, 1) indica que, para el primer documento, la palabra con el identificador 3 aparece una vez. \n# Mediante el comando (diccionario[3]) se puede ver que el identificador 3 corresponde al token \"afirm\".\n\nprint(diccionario[3]) ","72f1826c":"lda = LdaModel(corpus=corpus, id2word=diccionario, \n               num_topics=50, random_state=42, \n               chunksize=1000, passes=10, alpha='auto')\n\n# Para visualizar los t\u00f3picos extra\u00eddos se usa print_topics\n# Se indic\u00e1 el n\u00famero de t\u00f3picos y el n\u00famero de palabras por t\u00f3pico que queremos que se muestre:\n\ntopicos = lda.print_topics(num_words=5, num_topics=20)\n\nfor topico in topicos:\n    print(topico)\n","9d715f79":"for i in range(1, 5):\n    plt.figure()\n    plt.imshow(WordCloud(background_color='white', prefer_horizontal=1.0)\n               .fit_words(dict(lda.show_topic(i, 20))))\n    plt.axis(\"off\")\n    plt.title(\"T\u00f3pico \" + str(i))\n    plt.show() # Muestra nube de palabras de cada t\u00f3pico","1ecba3f1":"# Se empieza escogiendo una noticia del corpus al azar y mostramos su contenido y titular:\n\nindice_noticia = random.randint(0,len(df))\nnoticia = df.iloc[indice_noticia]\n\nprint(\"Titular: \" + noticia.Titular)\nprint(noticia.Noticia)\n\n# Primero debemos obtener la representaci\u00f3n BOW del documento y la distribuci\u00f3n de los t\u00f3picos:\n\nbow_noticia = corpus[indice_noticia]\ndistribucion_noticia = lda[bow_noticia]\n\n# Luego, sacamos los \u00edndices y la contribuci\u00f3n (proporci\u00f3n) de los t\u00f3picos m\u00e1s significativos para \n# nuestra noticia y los mostramos en un gr\u00e1fico de barras para un mejor entendimiento.\n\n# Indices de los topicos mas significativos\ndist_indices = [topico[0] for topico in lda[bow_noticia]]\n\n# Contribuci\u00f3n de los topicos mas significativos\ndist_contrib = [topico[1] for topico in lda[bow_noticia]]\ndistribucion_topicos = pd.DataFrame({'Topico':dist_indices,\n                                     'Contribucion':dist_contrib })\ndistribucion_topicos.sort_values('Contribucion', \n                                 ascending=False, inplace=True)\nax = distribucion_topicos.plot.bar(y='Contribucion',x='Topico', \n                                   rot=0, color=\"orange\",\n                                   title = 'T\u00f3picos mas importantes'\n                                   'de noticia ' + str(indice_noticia))","2b9de020":"\n# Se observan los t\u00f3picos m\u00e1s predominante en la noticia\n# Ahora imprimimos las palabras m\u00e1s significativas de estos.\n\nfor ind, topico in distribucion_topicos.iterrows():\n    print(\"*** T\u00f3pico: \" + str(int(topico.Topico)) + \" ***\")\n    palabras = [palabra[0] for palabra in lda.show_topic(\n        topicid=int(topico.Topico))]\n    palabras = ', '.join(palabras)\n    print(palabras, \"\\n\")\n","4b47ed3b":"\n# En una segunda prueba, asignaremos los t\u00f3picos a un nuevo documento no utilizado \n# en el entrenamiento del modelo LDA. La noticia escogida trata sobre la violencia de g\u00e9nero en Cantabria.\n\narticulo_nuevo = \"Igualdad admite que el crimen machista de Cantabria es un error del sistema La v\u00edctima hab\u00eda presentado denuncia y el agresor contaba con una orden de alejamiento. El Ministerio de Igualdad ha reconocido este lunes que el crimen machista del pasado 17 de diciembre en Cantabria, en el que un hombre mat\u00f3 a su expareja,  con orden de protecci\u00f3n, y a su peque\u00f1a de once meses, es evidentemente un fallo del sistema que tanto el Gobierno  como el poder judicial deben analizar. Lo ha dicho la delegada del Gobierno contra la Violencia de G\u00e9nero, Victoria Rosell,  en la rueda de prensa que ha ofrecido junto a la ministra de Igualdad, Irene Montero, y la secretaria de Estado, \u00c1ngela  Rodr\u00edguez, para hacer balance de las pol\u00edticas que el departamento ha llevado a cabo durante este a\u00f1o y las que  tiene previsto emprender en 2022. Preguntada por lo que fall\u00f3 en este asesinato machista en la localidad c\u00e1ntabra  de Lia\u00f1o de Villaescusa de la mujer de 40 a\u00f1os y su beb\u00e9 de once meses, ya que la v\u00edctima hab\u00eda presentado denuncia  y el agresor contaba con una orden de alejamiento, Rosell ha querido comenzar diciendo que con este caso tiene el  aliento contenido y el coraz\u00f3n en un pu\u00f1o. identemente es un fallo del sistema que tenemos que analizar no solo el  poder ejecutivo, tambi\u00e9n el poder judicial, ha dicho Rosell, que ha insistido en la necesidad de revisar la eficacia de las \u00f3rdenes de alejamiento sin dispositivo y promover los medios t\u00e9cnicos disponibles que funcionan a la hora de detectar los  quebrantamiento de las \u00f3rdenes de protecci\u00f3n.\"\n\narticulo_nuevo = limpiar_texto(articulo_nuevo)\narticulo_nuevo = tokenizer.tokenize(articulo_nuevo)\narticulo_nuevo = filtrar_stopword_digitos(articulo_nuevo)\narticulo_nuevo = stem_palabras(articulo_nuevo)\n\n#articulo_nuevo\n","9a2e162a":"# Se convierte a bow el art\u00edculo nuevo a testear\n\nbow_articulo_nuevo = diccionario.doc2bow(articulo_nuevo)\n\n# Se muestran los resultados:\n\ndist_indices = [topico[0] for topico in lda[bow_articulo_nuevo]] # Indices de los topicos mas significativos\n\n# Contribucion de los topicos mas significativos\ndist_contrib = [topico[1] for topico in lda[bow_articulo_nuevo]]\n\ndistribucion_topicos = pd.DataFrame({'Topico':dist_indices,\n                                     'Contribucion':dist_contrib })\n\ndistribucion_topicos.sort_values('Contribucion', \n                                 ascending=False, inplace=True)\n\nax = distribucion_topicos.plot.bar(y='Contribucion',x='Topico', \n                                   rot=0, color=\"green\",\n                                   title = 'T\u00f3picos m\u00e1s importantes' \n                                   'para documento nuevo')\n","0df18bfb":"# El t\u00f3pico 7 es el m\u00e1s significante con diferencia en la noticia. Imprimimos de nuevo las palabras m\u00e1s significativas:\n\nfor ind, topico in distribucion_topicos.iterrows():\n    print(\"*** T\u00f3pico: \" + str(int(topico.Topico)) + \" ***\")\n    palabras = [palabra[0] for palabra in lda.show_topic(\n        topicid=int(topico.Topico))]\n    palabras = ', '.join(palabras)\n    print(palabras, \"\\n\")\n","cb28a4c4":"# Observando las principales palabras del t\u00f3pico 7 (mujer, sexual, violencia, gener y hombr) \n# podemos decir que coincide claramente con la tem\u00e1tica principal de la noticia, es decir, la violencia de g\u00e9nero.\n\n# Se guarda el modelo y el diccionario para utilizarlo m\u00e1s adelante.  \n\n# lda.save(\"..\/kaggle\/working\/articulos.model\")\n# diccionario.save(\"..\/kaggle\/working\/articulos.dictionary\")\n \n# Fin. En este tutorial hemos visto que el modelo LDA es una buena herramienta para clasificar textos.\n# Conviene utilizar muchas m\u00e1s noticias para entrenar el modelo y de esta forma obtener un mejor rendimiento.\n","db90a0f5":"\n# Ejemplo: noticas m\u00e1s similares a una noticia sobre violencia de g\u00e9nero\n\n# Original: https:\/\/elmundodelosdatos.com\/topic-modeling-gensim-similitud-textos\/\n\nfrom gensim.matutils import jensen_shannon\n\n# Despu\u00e9s, le aplicamos el modelo LDA a una nueva noticia para obtener su \n# distribuci\u00f3n de t\u00f3picos con las proporciones de pertenencia de cada t\u00f3pico en el documento.\n\n# Usamos la misma noticia sobre violencia de g\u00e9nero en Cantabria que ya usamos en la anterior publicaci\u00f3n (noticia1.txt). Para la obtenci\u00f3n de los t\u00f3picos debemos realizar el preprocesamiento al texto de la noticia y obtener su representaci\u00f3n BOW. En el siguiente c\u00f3digo pode\u00eds ver como abrimos la noticia en Python y realizamos las anterior dos operaciones:\n\narticulo_nuevo = \"Igualdad admite que el crimen machista de Cantabria es un error del sistema La v\u00edctima hab\u00eda presentado denuncia y el agresor contaba con una orden de alejamiento. El Ministerio de Igualdad ha reconocido este lunes que el crimen machista del pasado 17 de diciembre en Cantabria, en el que un hombre mat\u00f3 a su expareja,  con orden de protecci\u00f3n, y a su peque\u00f1a de once meses, es evidentemente un fallo del sistema que tanto el Gobierno  como el poder judicial deben analizar. Lo ha dicho la delegada del Gobierno contra la Violencia de G\u00e9nero, Victoria Rosell,  en la rueda de prensa que ha ofrecido junto a la ministra de Igualdad, Irene Montero, y la secretaria de Estado, \u00c1ngela  Rodr\u00edguez, para hacer balance de las pol\u00edticas que el departamento ha llevado a cabo durante este a\u00f1o y las que  tiene previsto emprender en 2022. Preguntada por lo que fall\u00f3 en este asesinato machista en la localidad c\u00e1ntabra  de Lia\u00f1o de Villaescusa de la mujer de 40 a\u00f1os y su beb\u00e9 de once meses, ya que la v\u00edctima hab\u00eda presentado denuncia  y el agresor contaba con una orden de alejamiento, Rosell ha querido comenzar diciendo que con este caso tiene el  aliento contenido y el coraz\u00f3n en un pu\u00f1o. identemente es un fallo del sistema que tenemos que analizar no solo el  poder ejecutivo, tambi\u00e9n el poder judicial, ha dicho Rosell, que ha insistido en la necesidad de revisar la eficacia de las \u00f3rdenes de alejamiento sin dispositivo y promover los medios t\u00e9cnicos disponibles que funcionan a la hora de detectar los  quebrantamiento de las \u00f3rdenes de protecci\u00f3n.\"\n\narticulo_nuevo = limpiar_texto(articulo_nuevo)\narticulo_nuevo = tokenizer.tokenize(articulo_nuevo)\narticulo_nuevo = filtrar_stopword_digitos(articulo_nuevo)\narticulo_nuevo = stem_palabras(articulo_nuevo)\nbow_articulo_nuevo = diccionario.doc2bow(articulo_nuevo)\n\n# Con el m\u00e9todo get_document_topics de Gensim obtenemos la distribuci\u00f3n de t\u00f3picos y \n# ajustando el par\u00e1metro minimum_probability a 0 nos aseguraremos de que ning\u00fan t\u00f3pico \n# sea descartado.\n\ndistribucion_noticia = lda.get_document_topics(bow_articulo_nuevo, \n                                               minimum_probability=0)\n\n# Esa ser\u00e1 la distribuci\u00f3n que necesitamos para comparar las noticias. \n# Creamos un m\u00e9todo que se encarga de calcular la distancia entre dos distribuciones. \n# Gensim nos facilitar\u00e1 el trabajo al contar con la funci\u00f3n jensen_shannon que har\u00e1 \n# este c\u00e1lculo por nosotros.\n\ndef calcular_jensen_shannon_sim_doc_doc(doc_dist1, doc_dist2):\n      return jensen_shannon(doc_dist1, doc_dist2)\n\n# Tambi\u00e9n implementamos un m\u00e9todo que se encarga de calcular las distancias entre \n# las distribuciones de t\u00f3picos de nuestra noticia con las del resto y mostrar el \n# titular de aquellas m\u00e1s similares, es decir, las que tengan la distancia m\u00e1s baja\n\ndef mostrar_n_mas_similares(distribucion_noticia, n):\n    distancias = [calcular_jensen_shannon_sim_doc_doc(\n        distribucion_noticia, lda[noticia]) for noticia in corpus]\n    mas_similares = np.argsort(distancias)\n    for i in range(0,n):\n        titular = df.iloc[int(mas_similares[i])].Titular\n        print(f'{i + 1}: {titular} ({distancias[mas_similares[i]]})')\n\n# Ahora simplemente usamos el m\u00e9todo implementado pas\u00e1ndole como entrada la \n# distribuci\u00f3n de nuestra noticia y el n\u00famero de noticias m\u00e1s similares que queremos mostrar.\n\nmostrar_n_mas_similares(distribucion_noticia, 10)\n\n# Como podemos ver, las 10 noticias m\u00e1s similares calculadas mediante la distancia \n","39376ccf":"#Noticas m\u00e1s similares a una noticia sobre Trump\n\n# Se realiza una segunda prueba y esta vez usamos para evaluar el modelo una noticia\n# del ex presidente Trump y las elecciones de 2020 en Estados Unidos. \n\n# articulo_nuevo =  'El Servicio Postal de Estados Unidos (USPS, por sus siglas en ingl\u00e9s) se ha convertido en los \u00faltimos d\u00edas en protagonista de la pelea entre Donald Trump y los dem\u00f3cratas de cara a las elecciones del pr\u00f3ximo 3 de noviembre.  El presidente clama contra los riesgos de fraude... '\n\narticulo_nuevo =  \"el banco financiara la moneda y los dolares para activar el prestamo financiero\"\n\n# Realizamos el preprocesamiento del texto y obtenemos la distribuci\u00f3n de t\u00f3picos:\n\narticulo_nuevo = limpiar_texto(articulo_nuevo)\narticulo_nuevo = tokenizer.tokenize(articulo_nuevo)\narticulo_nuevo = filtrar_stopword_digitos(articulo_nuevo)\narticulo_nuevo = stem_palabras(articulo_nuevo)\n\narticulo_nuevo\n\nbow_articulo_nuevo = diccionario.doc2bow(articulo_nuevo)\ndistribucion_noticia = lda.get_document_topics(bow_articulo_nuevo, \n                                               minimum_probability=0)\n\n# Ya solo nos queda mostrar las 10 noticias m\u00e1s similares a la nuestra:\n\nmostrar_n_mas_similares(distribucion_noticia, 10)\n","b8e21f37":"# Visualizaci\u00f3n de los datos, con reducci\u00f3n de dimensionalidad, por m\u00e9todo TSNE \n\n# The t-distributed stochastic neighbor embedding (t-SNE) algorithm projects high-dimensional vectors to 2-D space. \n\nfrom gensim.test.utils import common_texts\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Anda pero demora much\u00edsmo, hay que tunearlo con los t\u00e9rminos m\u00e1s importantes\n\nmodel = Word2Vec(sentences=df[\"Tokens\"], window=5, min_count=400)\n\nlabels = [i for i in model.wv.key_to_index.keys()]\ntokens = model.wv[labels]\n\ntsne_model = TSNE(init='pca' )\n\nnew_values = tsne_model.fit_transform(tokens)\n\nx = []\ny = []\nfor value in new_values:\n    x.append(value[0])\n    y.append(value[1])\n    \nplt.figure(figsize=(20, 35)) \nfor i in range(new_values.shape[0]):\n    plt.scatter(x[i],y[i])\n    plt.annotate(labels[i],\n                 xy=(x[i], y[i]),\n                 xytext=(5, 2),\n                 textcoords='offset points',\n                 ha='right',\n                 va='bottom')\nplt.plot()\n\n","079771d8":"# An\u00e1lisis por LDA aplicado a un corpus de noticias\nOriginal en https:\/\/elmundodelosdatos.com\/topic-modeling-gensim-fundamentos-preprocesamiento-textos\/\n\nEl topic modeling es una t\u00e9cnica no supervisada de NLP, capaz de detectar y extraer de manera autom\u00e1tica relaciones sem\u00e1nticas latentes de grandes vol\u00famenes de informaci\u00f3n.\n\nEstas relaciones son los llamados t\u00f3picos, que son un conjunto de palabras que suelen aparecer juntas en los mismos contextos y nos permiten observar relaciones que ser\u00edamos incapaces de observar a simple vista.\n\nExisten diversas t\u00e9cnicas que pueden ser usadas para obtener estos t\u00f3picos. El principal algoritmo y que adem\u00e1s ser\u00e1 el que utilizaremos en esta publicaci\u00f3n, es el modelo latent dirichlet allocation (LDA), propuesto por David Blei, que nos devuelve por un lado los diferentes t\u00f3picos que componen la colecci\u00f3n de documentos y por otro lado cu\u00e1nto de cada t\u00f3pico est\u00e1 presente en cada documento. Los t\u00f3picos consisten en una distribuci\u00f3n de probabilidades de aparici\u00f3n de las distintas palabras del vocabulario.\n\nPaper base: https:\/\/www.jmlr.org\/papers\/volume3\/blei03a\/blei03a.pdf\n\n**Corpus**\n\nEn este tutorial utilizaremos como corpus 5665 noticias extra\u00eddas de distintos peri\u00f3dicos digitales espa\u00f1oles como El Diario o El Mundo. Para este ejemplo y para que pod\u00e1is ver el poder de esta t\u00e9cnica, esas 5665 noticias nos ser\u00e1n m\u00e1s que suficientes, pero si quer\u00e9is aplicarlo en un problema real, cuantos m\u00e1s documentos le metamos al modelo, mejor.","30a73b60":"# Segunda parte: construcci\u00f3n de un dicccionario y entrenamiento del modelo LDA.","f1b30248":"# Similitud de textos con Gensim\n\nExisten diversas t\u00e9cnicas para obtener la similitud entre textos.  Las t\u00e9cnicas m\u00e1s b\u00e1sicas para calcular la similitud entre textos solo tienen en cuenta la **similitud l\u00e9xica**, esto es la semejanza en las palabras contenidas en los textos comparados. Una de estas t\u00e9cnicas consiste en representar los textos como bolsas de palabras (BOW) y aplicar la similitud de coseno para comparar la similitud entre dos documentos. En este caso, la semejanza entre los dos textos depender\u00e1 del n\u00famero de palabras que compartan. \nHay varios problemas evidentes con este tipo de t\u00e9cnicas. Por ejemplo, una palabra puede tener varios significados completamente diferentes y, al carecer de contexto estos m\u00e9todos, no pueden saber a que significado se refiere. Adem\u00e1s, es posible que dos frases que no compartan ninguna palabra tengan un significado muy similar, como, por ejemplo: \u201cFui en coche a su casa y compramos algo para comer\u201d y \u201cEl otro d\u00eda conduje hacia su piso y pedimos unas hamburguesas\u00bb. \nPor ello, es importante que los modelos tengan tambi\u00e9n en cuenta la **similitud sem\u00e1ntica**, es decir, que puedan entender el significado real de las palabras o de la frase en cada contexto. Existen varios m\u00e9todos para extraer esta similitud sem\u00e1ntica de los textos, como los ampliamente utilizados modelos de word embeddings que permiten representar cada palabra como un vector de n\u00fameros reales y capturar de esta forma su informaci\u00f3n sem\u00e1ntica.\n\n**En esta pr\u00e1ctica sobre  topic modeling se ense\u00f1ar\u00e1 como calcular la similitud de textos aplicando la m\u00e9trica de distancia de Jensen-Shannon a las distribuciones de los t\u00f3picos del modelo LDA.** De esta forma, estaremos teniendo en cuenta las estructuras sem\u00e1nticas de los documentos extra\u00eddas mediante el modelo LDA.\n\nEn el caso pr\u00e1ctico que encontrareis en esta publicaci\u00f3n seguiremos utilizando el mismo corpus que consiste en 5665 noticias extra\u00eddas de distintos peri\u00f3dicos digitales espa\u00f1oles.\n\nRealizaremos dos pruebas distintas. En primer lugar, calcularemos las noticas m\u00e1s similares a otra noticia nueva sobre violencia de g\u00e9nero. En la segunda prueba vamos a obtener las m\u00e1s similiares a una noticas sobre el ex presidente Trump.\n\n**Usando la m\u00e9trica Jensen Shannon para calcular la distancia entre textos**\n\nPara calcular la distancia entre dos distribuciones utilizaremos la m\u00e9trica de distancia de Jensen-Shannon, una medida de distancia estad\u00edstica entre distribuciones de probabilidad. Esta m\u00e9trica nos devolver\u00e1 un valor comprendido entre 0 y 1, donde cuanto menor sea este valor significa una mayor similitud entre las dos distribuciones.\n\nLa distancia de Jensen-Shannon se calcula mediante la ra\u00edz cuadrada de la divergencia de Jensen-Shannon, m\u00e9trica basada en la divergencia de Kullback-Leiber. La diferencia entre ambas es que Jensen-Shannon es sim\u00e9trica y siempre tiene un valor finito","88696b74":"En cada t\u00f3pico se observan  las cinco palabras que m\u00e1s contribuyen a ese t\u00f3pico y sus pesos en el mismo. El peso nos indica como de importante es la palabra en ese t\u00f3pico. \n\nEsto significa que, por ejemplo, las cinco principales palabras en el t\u00f3pico 32 son: azul, eolic, sue\u00f1, arroy y luz. Por lo tanto, esta claro que este t\u00f3pico se centra en el medio ambiente y la energ\u00eda renovable.\n\nEs f\u00e1cil tambi\u00e9n interpretar que el t\u00f3pico 26 tiene relaci\u00f3n con la econom\u00eda al contener palabras como mill\u00f3n, eur y banc y el t\u00f3pico 38 se centra en elecciones, ya que sus palabras m\u00e1s significativas son encuest, votant y dat.\n\nTambi\u00e9n podemos visualizar las palabras m\u00e1s importantes de cada t\u00f3pico mediante nube de palabras, donde el tama\u00f1o de cada palabra corresponde con su contribuci\u00f3n en el t\u00f3pico. ","d54ca490":"# Evaluaci\u00f3n del modelo\n\nPara evaluar el rendimiento del modelo, estimaremos los t\u00f3picos en dos documentos diferentes. En el primer caso, escogeremos un documento de entre las noticias utilizadas en el corpus para entrenar el modelo y para la segunda prueba utilizaremos una nueva noticia.","b75b570a":"# Construcci\u00f3n del modelo LDA\n\nSe crea un objeto **LdaModel** de la librer\u00eda Gensim pas\u00e1ndole como argumento el corpus y el diccionario y le indicamos adicionalmente los siguientes par\u00e1metros:\n\n* num_topics: n\u00famero de t\u00f3picos. Para este ejemplo se extraen 50 t\u00f3picos.\n* random_state: par\u00e1metro para controlar la aleatoriedad del proceso de entrenamiento y que nos devuelva siempre los mismos resultados.\n* chunksize: n\u00famero de documentos que ser\u00e1 utilizado en cada pasada de entrenamiento.\n* passes: n\u00famero de pasadas por el corpus durante el entrenamiento.\n* alpha: representa la densidad de t\u00f3picos por documento. Un mayor valor de este par\u00e1metro implica que los documentos est\u00e9n compuestos de m\u00e1s t\u00f3picos. En este caso, fijamos el valor en auto.\n\n"}}