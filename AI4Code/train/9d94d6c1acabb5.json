{"cell_type":{"da1a18bc":"code","cd045054":"code","e8cf69a2":"code","08b7d1f0":"code","aee413d4":"code","733fd9ec":"code","db26401e":"code","36157cc4":"code","fa599c5e":"code","8211ee3b":"code","f23e2e52":"code","9b79b726":"code","f16f9c56":"code","65c63102":"code","be4c7842":"code","c62ae3af":"code","466659fe":"code","d3acaf06":"code","e1981bc1":"code","31fe94ae":"code","4756cdb8":"code","ff7e2bd3":"code","15954973":"code","04f6f667":"code","44961bd8":"code","a4e53b99":"code","045c3ef2":"code","2f7c1e09":"code","64cd972b":"code","5bf900de":"code","19e95319":"code","33d90116":"code","dbce0e28":"code","35df8887":"code","9a872380":"code","03a47601":"code","dec9e153":"code","ac319fce":"code","78bdcf31":"code","88462b7e":"code","d57f6f04":"code","ac5e9502":"code","8a6c48f1":"code","c1eb9c9c":"code","940ef4a5":"code","615d1c13":"code","1099dc28":"code","9dac74b0":"code","1312fb9a":"code","95cdbb78":"code","41d19a89":"code","4b83144d":"code","a2526cac":"markdown","13a5b1aa":"markdown","11da08e6":"markdown","9c816bfc":"markdown","c0ac3997":"markdown","b2ebfec1":"markdown","4b3b9afd":"markdown","32d049d7":"markdown","feb32827":"markdown","78fe5ba2":"markdown","03ba38b5":"markdown","e41331bf":"markdown","f0089c27":"markdown","31b7c565":"markdown","c6e6415e":"markdown","dfdf109d":"markdown","067e2939":"markdown","f6f39da9":"markdown","2bfd99c5":"markdown","0303a062":"markdown","88951f1e":"markdown","0ff88fba":"markdown","4360700d":"markdown","bdadcad3":"markdown","ec514d86":"markdown","a8bc902a":"markdown","99553d9d":"markdown","a1fcd83d":"markdown"},"source":{"da1a18bc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.metrics import f1_score\nimport graphviz\nfrom sklearn import tree\nfrom sklearn.model_selection import KFold","cd045054":"# prettify plots\nplt.rcParams['figure.figsize'] = [20.0, 5.0]","e8cf69a2":"!ls ..\/input\/**","08b7d1f0":"%%time\ntest = pd.read_csv('..\/input\/54-features-datasets-clean-drift-noise-free\/54-features-from-test_clean_removed_drift_noise.csv')","aee413d4":"test = test.fillna(0.0)","733fd9ec":"%%time\ntrain = pd.read_csv('..\/input\/54-features-datasets-clean-drift-noise-free\/54-features-from-train_clean_removed_drift_noise.csv')","db26401e":"train = train.fillna(0.0)","36157cc4":"print(\"train\", train.shape)\nprint(\"test\", test.shape)","fa599c5e":"train.head()","8211ee3b":"test.head()","f23e2e52":"res = 1000\nbatch_size=500000\nsub_sample_size = batch_size\/5\nmargin=200000\n\ndef plot_data(column, column_name):\n    plt.figure(figsize=(20,5))\n    plt.plot(range(0, column.shape[0], res), column[0::res])\n    for i in range(11): plt.plot([i*batch_size,i*batch_size],[-5,12.5],'r')\n    for j in range(10): plt.text(j*batch_size+margin,10,str(j+1),size=20)\n    plt.xlabel('Row',size=16); plt.ylabel(column_name,size=16); \n    plt.title(f'Training Data {column_name} - 10 batches',size=20)\n    plt.show()","9b79b726":"plot_data(train.signal, 'Signal')","f16f9c56":"plot_data(train.open_channels, 'Open Channels')","65c63102":"columns_to_keep_but_are_not_features = [\n    'time',\n    'batch_index',\n    'signal',\n    'open_channels'\n]\n\nfeature_cols = list(set(train.columns) - set(columns_to_keep_but_are_not_features))\nprint(f\"{len(feature_cols)} features: {feature_cols}\")","be4c7842":"def chain_to_previous_range(indices: tuple, folds: int):\n    return indices[1] + folds, indices[1] + folds\n    \ndef generate_range_of_indices(indices: tuple, fold_size: int, max_allowed_value: int):\n    start = fold_size * indices[0]\n    end = fold_size * (indices[1] + 1)\n    if abs(max_allowed_value - end) <= 3:\n        end = max_allowed_value\n\n    return np.array(range(start, end))","c62ae3af":"def generate_nested_folds_batch_ranges(total_folds_size: int, num_of_training_folds: int = 3,\n                                       num_of_validation_folds: int = 1, num_of_test_folds: int = 0):\n    total_folds_size = int(total_folds_size)\n    total_folds = num_of_training_folds + num_of_validation_folds + num_of_test_folds\n    each_fold_size = int(round(total_folds_size \/ total_folds))\n\n    nested_folds_indices = []\n    min_training_index = 0\n    max_training_index = total_folds - num_of_validation_folds - num_of_test_folds\n    for max_training_index_this_fold in range(min_training_index, max_training_index):\n        training_indices = (min_training_index, max_training_index_this_fold)\n        validation_indices = chain_to_previous_range(training_indices, num_of_validation_folds)\n        test_indices = (0, 0)\n        if num_of_test_folds > 0:\n            test_indices = chain_to_previous_range(validation_indices, num_of_test_folds)\n        nested_folds_indices.append([training_indices, validation_indices, test_indices])\n\n    nested_batch_indices = []\n    for each_nested_fold_indices in nested_folds_indices:\n        training_indices = each_nested_fold_indices[0]\n        validation_indices = each_nested_fold_indices[1]\n        test_indices = each_nested_fold_indices[2]\n\n        indices = [\n            generate_range_of_indices(training_indices, each_fold_size, total_folds_size),\n            generate_range_of_indices(validation_indices, each_fold_size, total_folds_size),\n        ]\n        if num_of_test_folds > 0:\n            indices.append(\n                generate_range_of_indices(test_indices, each_fold_size, total_folds_size),\n            )\n        nested_batch_indices.append(indices)\n\n    return nested_batch_indices","466659fe":"print(\"training,      validation,        test\")\ngenerate_nested_folds_batch_ranges(train.shape[0], 5)","d3acaf06":"print(\"training,      validation,        test\")\ngenerate_nested_folds_batch_ranges(train.shape[0], 5, 1, 1)","e1981bc1":"training_folds = 5","31fe94ae":"def get_kfold_enumerator(dataset: pd.DataFrame, folds: int = training_folds):\n    return enumerate(KFold(n_splits=folds).split(dataset))","4756cdb8":"def get_nestedcv_enumerator(dataset: pd.DataFrame, folds: int = training_folds):\n    return enumerate(generate_nested_folds_batch_ranges(dataset.shape[0], folds))","ff7e2bd3":"def train_with_cross_validation(params, model, X_train_, y_train_, cv_enumerator=get_nestedcv_enumerator):\n    total_f1_macro_score = 0.0\n    models = []\n    best_model = None\n    best_f1_macro_score = 0.0\n    \n    for fold_index, (training_index, validation_index) in cv_enumerator(X_train_):\n        X_training_set = X_train_[training_index]\n        y_training_set = y_train_[training_index]\n        X_validation_set = X_train_[validation_index]\n        y_validation_set = y_train_[validation_index]\n        model = model.fit(X_training_set, y_training_set)\n        models.append(model)\n        predictions = model.predict(X_validation_set)\n        f1_macro_score = f1_score(y_validation_set, predictions, average='macro')\n        if best_f1_macro_score < f1_macro_score:\n            best_f1_macro_score = f1_macro_score\n            best_model = model\n        print(f'fold {fold_index + 1}: macro f1 validation score: {f1_macro_score}, best macro f1 validation score: {best_f1_macro_score}')\n        total_f1_macro_score += f1_macro_score\n\n    return models, best_model, total_f1_macro_score\/training_folds\n\ndef train_model_by_batch(train_df, feature_cols_, first_batch, second_batch, model_type, \n                         class_names=['0', '1'], params={'max_depth':1}, cv_enumerator=get_nestedcv_enumerator):\n    a = batch_size * (first_batch - 1); b = (batch_size * first_batch); \n    c = batch_size * (second_batch - 1); d = (batch_size * second_batch)\n    left_batch = train_df[a:b];     right_batch = train_df[a:b];\n\n    X_train = np.concatenate([left_batch[feature_cols_].values, right_batch[feature_cols_].values]).reshape((-1,len(feature_cols)))\n    y_train = np.concatenate([left_batch.open_channels.values, left_batch.open_channels.values]).reshape((-1,1))\n    \n    print(f'Training model {model_type} channel')\n    model = tree.DecisionTreeClassifier(**params)\n    models, best_model, f1_macro_score = train_with_cross_validation(params, model, X_train, y_train, cv_enumerator=cv_enumerator)\n    print(f'model {model_type}, average macro f1 validation score = {f1_macro_score}')\n    \n    tree_graph = tree.export_graphviz(best_model, out_file=None, max_depth = 10, impurity = False, \n                                      feature_names = feature_cols_, class_names = class_names, rounded = True, filled= True)\n    return models, f1_macro_score, graphviz.Source(tree_graph) ","15954973":"import numpy as np\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras.callbacks import Callback\n\n\ndef macro_f1(y_true, y_pred):\n    \"\"\"\n    The Macro F1 metric used in this competition\n    :param y_true: The ground truth labels given in the dataset\n    :param y_pred: Our predictions\n    :return: The Macro F1 Score\n    \"\"\"\n    return f1_score(y_true, y_pred, average=\"macro\", labels=np.unique(y_true))","04f6f667":"nestedcv_f1_macro_scores = []\nkfold_f1_macro_scores = []","44961bd8":"%%time\nnestedcv_clf1s, f1_macro_score, graph = train_model_by_batch(train, feature_cols, 1, 2, '1s', cv_enumerator=get_nestedcv_enumerator)\nnestedcv_f1_macro_scores.append(f1_macro_score)\ngraph","a4e53b99":"%%time\nkfold_clf1s, f1_macro_score, graph = train_model_by_batch(train, feature_cols, 1, 2, '1s', cv_enumerator=get_kfold_enumerator)\nkfold_f1_macro_scores.append(f1_macro_score)\ngraph","045c3ef2":"%%time\nnestedcv_clf1f, f1_macro_score, graph = train_model_by_batch(train, feature_cols, 3, 7, '1f', cv_enumerator=get_nestedcv_enumerator)\nnestedcv_f1_macro_scores.append(f1_macro_score)\ngraph","2f7c1e09":"%%time\nkfold_clf1f, f1_macro_score, graph = train_model_by_batch(train,feature_cols, 3, 7, '1f', cv_enumerator=get_kfold_enumerator)\nkfold_f1_macro_scores.append(f1_macro_score)\ngraph","64cd972b":"%%time\nprint(\"Training using NestedCV cross-validation method\")\nnestedcv_clf3, f1_macro_score, graph = train_model_by_batch(train, feature_cols, 4, 8, '3', \n                                                            class_names=['0','1','2','3'], params={'max_leaf_nodes': 4}, \n                                                            cv_enumerator=get_nestedcv_enumerator)\nnestedcv_f1_macro_scores.append(f1_macro_score)\ngraph","5bf900de":"%%time\nprint(\"Training using KFold cross-validation method\")\nkfold_clf3, f1_macro_score, graph = train_model_by_batch(train, feature_cols, 4, 8, '3', \n                                                         class_names=['0','1','2','3'], params={'max_leaf_nodes': 4}, \n                                                         cv_enumerator=get_kfold_enumerator)\nkfold_f1_macro_scores.append(f1_macro_score)\ngraph","19e95319":"%%time\nprint(\"Training using NestedCV cross-validation method\")\nnestedcv_clf5, f1_macro_score, graph = train_model_by_batch(train, feature_cols, 6, 9, '5', \n                                                            class_names=['0','1','2','3','4','5'], params={'max_leaf_nodes': 6}, \n                                                            cv_enumerator=get_nestedcv_enumerator)\nnestedcv_f1_macro_scores.append(f1_macro_score)\ngraph","33d90116":"%%time\nprint(\"Training using KFold cross-validation method\")\nkfold_clf5, f1_macro_score, graph = train_model_by_batch(train, feature_cols, 6, 9, '5', \n                                                            class_names=['0','1','2','3','4','5'], params={'max_leaf_nodes': 6}, \n                                                           cv_enumerator=get_kfold_enumerator)\nkfold_f1_macro_scores.append(f1_macro_score)\ngraph","dbce0e28":"%%time\nprint(\"Training using NestedCV cross-validation method\")\nnestedcv_clf10, f1_macro_score, graph = train_model_by_batch(train, feature_cols, 5, 10, '10', \n                                                             class_names=[str(x) for x in range(11)], params={'max_leaf_nodes': 255}, \n                                                             cv_enumerator=get_nestedcv_enumerator)\nnestedcv_f1_macro_scores.append(f1_macro_score)\ngraph","35df8887":"%%time\nprint(\"Training using KFold cross-validation method\")\nkfold_clf10, f1_macro_score, graph = train_model_by_batch(train, feature_cols, 5, 10, '10', \n                                                             class_names=[str(x) for x in range(11)], params={'max_leaf_nodes': 255}, \n                                                             cv_enumerator=get_kfold_enumerator)\nkfold_f1_macro_scores.append(f1_macro_score)\ngraph","9a872380":"%%time\nnestedcv_sub = pd.read_csv('..\/input\/liverpool-ion-switching\/sample_submission.csv')\nkfold_sub = nestedcv_sub.copy()","03a47601":"\"\"\"\nTraining Batches mapped to sub-model types \n1,  2 ==>  1 Slow Open Channel\n3,  7 ==>  1 Fast Open Channel\n4,  8 ==>  3 Open Channels\n6,  9 ==>  5 Open Channels\n5, 10 ==> 10 Open Channels\n\"\"\"\n\nf1_macro_scores = nestedcv_f1_macro_scores\nnestedcv_params = [\n    [ (0, 1), \"Subsample A\",     \"Model 1s (1 Slow Open Channel)\", nestedcv_clf1s, f1_macro_scores[0]],\n    [ (1, 2), \"Subsample B\",     \"Model 3  (3 Open Channels)\",     nestedcv_clf3,  f1_macro_scores[2]],\n    [ (2, 3), \"Subsample C\",     \"Model 5  (5 Open Channels)\",     nestedcv_clf5,  f1_macro_scores[3]],\n    [ (3, 4), \"Subsample D\",     \"Model 1s (1 Slow Open Channel)\", nestedcv_clf1s, f1_macro_scores[0]],\n    [ (4, 5), \"Subsample E\",     \"Model 1f (1 Fast Open Channel)\", nestedcv_clf1f, f1_macro_scores[1]],\n    [ (5, 6), \"Subsample F\",     \"Model 10 (10 Open Channels)\",    nestedcv_clf10, f1_macro_scores[4]],\n    [ (6, 7), \"Subsample G\",     \"Model 5  (5 Open Channels)\",     nestedcv_clf5,  f1_macro_scores[3]],\n    [ (7, 8), \"Subsample H\",     \"Model 10 (10 Open Channels)\",    nestedcv_clf10, f1_macro_scores[4]],\n    [ (8, 9), \"Subsample I\",     \"Model 1s (1 Slow Open Channel)\", nestedcv_clf1s, f1_macro_scores[0]],\n    [ (9,10), \"Subsample J\",     \"Model 3  (3 Open Channels)\",     nestedcv_clf3,  f1_macro_scores[2]],\n    [(10,20), \"Batches 3 and 4\", \"Model 1s (1 Slow Open Channel)\", nestedcv_clf1s, f1_macro_scores[0]]\n]\n\nf1_macro_scores = kfold_f1_macro_scores\nkfold_params = [\n    [ (0, 1), \"Subsample A\",     \"Model 1s (1 Slow Open Channel)\", kfold_clf1s, f1_macro_scores[0]],\n    [ (1, 2), \"Subsample B\",     \"Model 3  (3 Open Channels)\",     kfold_clf3,  f1_macro_scores[2]],\n    [ (2, 3), \"Subsample C\",     \"Model 5  (5 Open Channels)\",     kfold_clf5,  f1_macro_scores[3]],\n    [ (3, 4), \"Subsample D\",     \"Model 1s (1 Slow Open Channel)\", kfold_clf1s, f1_macro_scores[0]],\n    [ (4, 5), \"Subsample E\",     \"Model 1f (1 Fast Open Channel)\", kfold_clf1f, f1_macro_scores[1]],\n    [ (5, 6), \"Subsample F\",     \"Model 10 (10 Open Channels)\",    kfold_clf10, f1_macro_scores[4]],\n    [ (6, 7), \"Subsample G\",     \"Model 5  (5 Open Channels)\",     kfold_clf5,  f1_macro_scores[3]],\n    [ (7, 8), \"Subsample H\",     \"Model 10 (10 Open Channels)\",    kfold_clf10, f1_macro_scores[4]],\n    [ (8, 9), \"Subsample I\",     \"Model 1s (1 Slow Open Channel)\", kfold_clf1s, f1_macro_scores[0]],\n    [ (9,10), \"Subsample J\",     \"Model 3  (3 Open Channels)\",     kfold_clf3,  f1_macro_scores[2]],\n    [(10,20), \"Batches 3 and 4\", \"Model 1s (1 Slow Open Channel)\", kfold_clf1s, f1_macro_scores[0]]\n]","dec9e153":"def ensemble_by_geometric_mean(sets_of_predictions,\n                               number_of_predictions_per_set: int,\n                               min_label_value: int,\n                               max_label_value: int) -> np.ndarray:\n    result = np.ones(number_of_predictions_per_set)\n    for index, each_set_of_predictions in enumerate(sets_of_predictions):\n        result *= each_set_of_predictions\n    result = result ** (1 \/ len(sets_of_predictions))\n    \n    return np.nan_to_num(result, nan=min_label_value, posinf=max_label_value, neginf=min_label_value)\n    \ndef predict_using(models, data):\n    predictions = []\n    if isinstance(models, list):\n        for each_model in models:\n            predictions.append(each_model.predict(data))\n        return ensemble_by_geometric_mean(predictions, len(data), 0, 10)\n    else:\n        return np.round(models.predict(data))\n    \ndef create_prediction(reference_dataframe, feature_cols, results_dataframe, params):\n    total_score = 0.0\n    for each_param in params:\n        begin_index, end_index = each_param[0]\n        start_batch = int(sub_sample_size * begin_index)\n        end_batch = int(sub_sample_size * end_index)\n        batch_or_sample_models = each_param[3]\n        f1_macro_score = each_param[4]\n        X_batch = reference_dataframe[feature_cols]\n        X_batch = X_batch.iloc[start_batch:end_batch].values.reshape((-1,len(feature_cols)))\n        results_dataframe.iloc[start_batch:end_batch, 1] = predict_using(batch_or_sample_models, X_batch)\n        print(f\"Predicting for {each_param[1]} ({start_batch} to {end_batch}) of submission with predictions from {each_param[2]} with a F1 Macro score of {f1_macro_score}\")\n        total_score = total_score + f1_macro_score\n\n    print()\n    average_f1_macro_score = total_score\/len(params)\n    print(f\"Average F1 Macro across the {len(params)} subsamples\/batches: {average_f1_macro_score}\")\n    results_dataframe.open_channels = results_dataframe.open_channels.astype(int)\n    return results_dataframe, average_f1_macro_score","ac319fce":"%%time\nnestedcv_sub, nestedcv_average_f1_macro_score = create_prediction(test, feature_cols, nestedcv_sub, nestedcv_params)","78bdcf31":"%%time\nkfold_sub, kfold_average_f1_macro_score = create_prediction(test, feature_cols, kfold_sub, kfold_params)","88462b7e":"res = 1000\nletters = ['A','B','C','D','E','F','G','H','I','J']\n\ndef plot_results(reference_dataframe, results_dataframe):\n    plt.figure(figsize=(20,5))\n    plt.plot(range(0,reference_dataframe.shape[0],res),results_dataframe.open_channels[0::res])\n    for i in range(5): plt.plot([i*batch_size,i*batch_size],[-5,12.5],'r')\n    for i in range(21): plt.plot([i*sub_sample_size, i*sub_sample_size],[-5,12.5],'r:')\n    for k in range(4): plt.text(k*batch_size + (batch_size\/2),10,str(k+1),size=20)\n    for k in range(10): plt.text(k*sub_sample_size + 40000,7.5,letters[k],size=16) # \n    plt.title('Test Data Predictions',size=16)\n    plt.show()","d57f6f04":"plot_results(test, nestedcv_sub)","ac5e9502":"nestedcv_sub.describe()","8a6c48f1":"print(nestedcv_sub.open_channels.describe())\nnestedcv_sub.open_channels.hist()","c1eb9c9c":"nestedcv_sub","940ef4a5":"nestedcv_sub[100000:200000]","615d1c13":"plot_results(test, kfold_sub)","1099dc28":"kfold_sub.describe()","9dac74b0":"print(kfold_sub.open_channels.describe())\nkfold_sub.open_channels.hist()","1312fb9a":"kfold_sub","95cdbb78":"kfold_sub[100000:200000]","41d19a89":"%%time\n!rm sub*nestedcv*.csv || true\nsubmission_filename = f'submission-1-54-features-nestedcv-DecisionTree-f1-macro.csv'\nnestedcv_sub.to_csv(submission_filename, index=False, float_format='%0.4f')\nprint(f'Saved {submission_filename} with Macro F1 validation score of {nestedcv_average_f1_macro_score}')\n!ls sub*nestedcv*.csv","4b83144d":"%%time\n!rm sub*kfold*.csv || true\nsubmission_filename = f'submission-2-54-features-kfold-DecisionTree-f1-macro.csv'\nnestedcv_sub.to_csv(submission_filename, index=False, float_format='%0.4f')\nprint(f'Saved {submission_filename} with Macro F1 validation score of {kfold_average_f1_macro_score}')\n!ls sub*kfold*.csv","a2526cac":"### Ensembling sub-models using Geometric mean","13a5b1aa":"#### Ensembled models through KFold cross-validation","11da08e6":"#### Ensembled models through KFold cross-validation","9c816bfc":"## 5 Open Channels","c0ac3997":"### NestedCV implementation","b2ebfec1":"### Feature columns","4b3b9afd":"## 3 Open Channels","32d049d7":"#### _Removed cells about **Reflection**, **Test Data**, and **Remove Test Data Drift**, see [original notebook](https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930) by [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte\/)._","feb32827":"### Define the CV to be used for training: KFold and NestedCV","78fe5ba2":"#### Example usage of the NestCV generator function","03ba38b5":"# One Feature Model Scores LB 0.930!\nIn this notebook, we will explore the Kaggle Ion Comp data and explore a one feature model. The LB result of 0.930 is enlightening.\n\nHere we manually remove signal drift. Note that it is better to use machine learning to remove drift, but doing it by hand once allows us to understand its nature and build better models later.","e41331bf":"### Full credits to the author [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte) of the [Original notebook](https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930)\n\n#### I have reorganised and refactored the code for my curiosity and learnings. Added console logs, and re-wrote datastructure to understand how the splits and batching is occuring. Also added two differentCV methods **KFold** and **NestedCV** methods, these are ideal for using with Timeseries-like problems. The focus of the changes in the notebook was to focus a bit more on the training aspects (CV methods), the original notebook covered a lot on analysis and cleaning, please refer to it for those goodies.\n\nYou can read more about **NestedCV** here:\n- https:\/\/www.elderresearch.com\/blog\/nested-cross-validation\n- https:\/\/towardsdatascience.com\/time-series-nested-cross-validation-76adba623eb9\n- https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_nested_cross_validation_iris.html?highlight=nested%20cross%20validation\n\nThanks for Marcus for the clean datasets (drift and noise removed), and also [Carlo Lepelaars](https:\/\/www.kaggle.com\/carlolepelaars) for sharing with me the **54 features**, that were generated fron the `signal` column as part of a Feature Engineering step after cleaning the datasets and before building the model.\n\n### This notebook has been forked from [One Feature Model - clean datasets (refactored)](https:\/\/www.kaggle.com\/neomatrix369\/one-feature-model-clean-datasets-refactored\/), find other such refactored notebooks [here](https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/discussion\/153653).","f0089c27":"#### Plot results for NestedCV cross-validation","31b7c565":"### Define the Macro F1 function to be used during training","c6e6415e":"# Display Test Predictions","dfdf109d":"# Make Five Simple Models\nWe will make one model for each different type of signal we observed above.","067e2939":"#### _Removed cells about **Reflection**, **Correlation Between Signal and Open Channels**, Test Data, and Remove Training Data Drift, see [original notebook](https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930) by [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte\/)._","f6f39da9":"# Load Libraries and Data","2bfd99c5":"#### Saving submission results for KFold cross-validation","0303a062":"#### Saving submission results for NestedCV cross-validation","88951f1e":"# Predict Test\n","0ff88fba":"**Note:** This channel produce low Macro F1 scores (in the ranges of`0.78nnnn`) using the configuration mentioned in the [original notebook](https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930). So a set of hyperparameters were tried and the one that worked best was passing in `params={'max_leaf_nodes': 255}` to the `DecisionTreeClassifier`.\n\nOther results from using different values for `max_leaf_nodes` are as below:\n```\nOriginal: max_leaf_nodes: 8 => scores: 0.78nnnnn (no folds)\nmax_leaf_nodes:     100 => oof score: 0.843581 (5 folds)\nmax_leaf_nodes:     250 => oof score: 0.871766 (5 folds)\nmax_leaf_nodes:     255 => oof score: 0.871819 (5 folds) -- best so far, but you may get other values for this\nmax_leaf_nodes:     500 => oof score: 0.87133  (5 folds)\nmax_leaf_nodes:   1_000 => oof score: 0.870259 (5 folds)\nmax_leaf_nodes:   2_000 => oof score: 0.864435 (5 folds)\nmax_leaf_nodes:   4_000 => oof score: 0.861501 (5 folds)\nmax_leaf_nodes:   5_000 => oof score: 0.859787  (5 folds)\nmax_leaf_nodes:   5_000 => F1 Macro Score: 0.9025314007624331 (no folds)\nmax_leaf_nodes:  10_000 => oof score: 0.827114 (5 folds)\nmax_leaf_nodes:  10_000 => F1 Macro Score: 0.9177560091220058 (no folds)\nmax_leaf_nodes:  25_000 => oof score: 0.815225 (5 folds)\nmax_leaf_nodes:  25_000 => F1 Macro Score: 0.9435734746197743 (bit away from overfit zone) (no folds)\nmax_leaf_nodes:  50_000 => oof score: 0.78893 (5 folds)\nmax_leaf_nodes:  50_000 => F1 Macro Score: 0.9687715940722476 (nearing overfit zone) (no folds)\nmax_leaf_nodes: 100_000 => F1 Macro Score: 1.0 (fully in overfit zone) (no folds)\n```\n\nFeel free to play with this parameter to improve the scores further, although the scores seem to slowly plateau and go down past the **255**-**300** range.","4360700d":"#### Ensembled models through NestedCV cross-validation","bdadcad3":"# Description of Data\nThe training data is recordings in time. At each 10,000th of a second, the strength of the signal was recorded and the number of ion channels open was recorded. It is our task to build a model that predicts the number of open channels from signal at each time step. Furthermore we are told that the data was recorded in batches of 50 seconds. Therefore each 500,000 rows is one batch. The training data contains 10 batches and the test data contains 4 batches. Let's display the number of open channels and signal strength together for each training batch.","ec514d86":"## 10 Open Channels","a8bc902a":"## 1 Slow Open Channel","99553d9d":"Below code cells show the smaller building blocks that make up the NestedCV generator (the data structure of the output created by `generate_nested_folds_batch_ranges()` can be wrapped with `enumerate()` and used as an iterator and swapped with other CV methods i.e. `KFold`, etc...","a1fcd83d":"## 1 Fast Open Channel"}}