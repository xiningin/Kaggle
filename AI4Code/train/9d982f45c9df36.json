{"cell_type":{"a41a0649":"code","f6cdb4f0":"code","44dc0f63":"code","691379a6":"code","81b0c778":"code","bd757a10":"code","c98e2fcf":"code","6b1f2f16":"code","ea03dab3":"code","5a9782e2":"code","69075e82":"code","d3c979c6":"code","32f2c7c8":"code","a3894154":"code","fc6a99f9":"code","d6777094":"code","0e19ebe5":"code","b1c3bec0":"code","dfa8acc9":"code","962ee341":"code","3c60ffe1":"code","f4e23614":"code","69ea1cb4":"code","8bf6f2c4":"code","0675a9d5":"code","cd96878e":"code","30a18a33":"code","88392728":"code","8389f5db":"markdown","8480c617":"markdown","27abfea8":"markdown"},"source":{"a41a0649":"def find_sentiment(sentence, pos, neg):\n    \n    \"\"\"\n    This function returns sentiment of sentence\n    :param sentence: sentence, a string\n    :param pos: set of positive words\n    :param neg: set of negative words\n    :return: returns positive, negative or neutral sentiment\n    \"\"\"\n    \n    # split sentence by a space\n    sentence = sentence.split()\n    \n    sentence = set(sentence)\n    \n    # check number of common words with positive\n    num_common_pos = len(sentence.intersection(pos))\n    \n    num_common_neg = len(sentence.intersection(neg))\n    \n    if num_common_pos > num_common_neg:\n        return(\"positive\")\n    if num_common_pos < num_common_neg:\n        return(\"negative\")\n    return(\"neutral\")\n    ","f6cdb4f0":"# tokenization splits sentence into list of words\nfrom nltk.tokenize import word_tokenize\n\nsentence = \"hi, how are you?\"\n\nprint(sentence.split())\n\nprint(word_tokenize(sentence))","44dc0f63":"# classification problem : Bag of Words\n# bag of words we create a sparse matrix that stores counts of all words in our corpus\n# (corpus = all the documents = all the sentences)\n#  CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# create a corpus of sentences\ncorpus = [\n    \"hello, how are you?\",\n    \"im getting bored at home. And you? What do you think?\",\n    \"did you know about counts\",\n    \"let's see if this works\",\n    \"YES!!!!\"\n]\n\nctv = CountVectorizer()\n\n# fit the vectorizer on corpus\nctv.fit(corpus)\n\ncorpus_transformed = ctv.transform(corpus)\n\n","691379a6":"print(corpus_transformed)","81b0c778":"print(ctv.vocabulary_)","bd757a10":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import word_tokenize\n\n# create a corpus of sentences\ncorpus = [\n \"hello, how are you?\",\n \"im getting bored at home. And you? What do you think?\",\n \"did you know about counts\",\n \"let's see if this works!\",\n \"YES!!!!\"\n]\n# initialize CountVectorizer with word_tokenize from nltk\n# as the tokenizer\nctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n# fit the vectorizer on corpus\nctv.fit(corpus)\ncorpus_transformed = ctv.transform(corpus)\nprint(ctv.vocabulary_)\n","c98e2fcf":"# import what we need \nimport pandas as pd\n\nfrom nltk.tokenize import word_tokenize\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nif __name__==\"__main__\":\n    \n    df = pd.read_csv(\"..\/input\/cleaned-imdb\/imdb_clean.csv\")\n    \n   \n    # we create a new column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    df = df.sample(frac=1).reset_index(drop=True)\n    y = df.sentiment.values\n    \n    kf = model_selection.StratifiedKFold(n_splits=5)\n    # fill the new kfold column\n    for f, (t_,v_) in enumerate(kf.split(X=df, y=y)):\n        df.loc[v_,\"kfold\"] = f\n    for fold_ in range(5):\n        # temporary dataframes for train and test\n        train_df = df[df.kfold != fold_ ].reset_index(drop=True)\n        test_df = df[df.kfold == fold_ ].reset_index(drop=True)\n        \n        count_vec = CountVectorizer(\n        tokenizer=word_tokenize,\n        token_pattern=None\n        )\n        count_vec.fit(train_df.reviews)\n        \n        xtrain = count_vec.transform(train_df.reviews)\n        xtest = count_vec.transform(test_df.reviews)\n        \n        model = linear_model.LogisticRegression()\n        \n        model.fit(xtrain, train_df.sentiment)\n        preds = model.predict(xtest)\n        \n        accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n        print(f\"Fold: {fold_}\")\n        print(f\"Accuracy = {accuracy}\")\n        print(\"\")","6b1f2f16":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import word_tokenize\n# create a corpus of sentences\ncorpus = [\n    \"hello, how are you?\",\n \"im getting bored at home. And you? What do you think?\",\n \"did you know about counts\",\n \"let's see if this works!\",\n \"YES!!!!\"\n]\n# initialize TfidfVectorizer with word_tokenize from nltk\n# as the tokenizer\ntfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n# fit the vectorizer on corpus\ntfv.fit(corpus)\ncorpus_transformed = tfv.transform(corpus)\nprint(corpus_transformed)\n","ea03dab3":"\"\"\"\nWe see that instead of integer values, this time we get floats. Replacing\nCountVectorizer with TfidfVectorizer is also a piece of cake. Scikit-learn also offers\nTfidfTransformer. If you have count values, you can use TfidfTransformer and get\nthe same behaviour as TfidfVectorizer. \n\"\"\"\n# import what we need\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n # we go over the folds created\nfor fold_ in range(5):\n # temporary dataframes for train and test\n    train_df = df[df.kfold != fold_].reset_index(drop=True)\n    test_df = df[df.kfold == fold_].reset_index(drop=True)\n # initialize TfidfVectorizer with NLTK's word_tokenize\n # function as tokenizer\n    tfidf_vec = TfidfVectorizer(\n    tokenizer=word_tokenize,\n    token_pattern=None\n )\n # fit tfidf_vec on training data reviews\n    tfidf_vec.fit(train_df.reviews)\n # transform training and validation data reviews\n    xtrain = tfidf_vec.transform(train_df.reviews)\n    xtest = tfidf_vec.transform(test_df.reviews)\n # initialize logistic regression model\n    model = linear_model.LogisticRegression()\n # fit the model on training data reviews and sentiment\n    model.fit(xtrain, train_df.sentiment)\n # make predictions on test data\n # threshold for predictions is 0.5\n    preds = model.predict(xtest)\n # calculate accuracy\n    accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n    print(f\"Fold: {fold_}\")\n    print(f\"Accuracy = {accuracy}\")\n    print(\"\")\n","5a9782e2":"\"\"\"\nAnother interesting concept in NLP is n-grams. N-grams are combinations of\nwords in order. N-grams are easy to create. You just need to take care of the order.\nTo make things even more comfortable, we can use n-gram implementation from\nNLTK.\n\"\"\"","69075e82":"from nltk import ngrams\nfrom nltk.tokenize import word_tokenize\n# let's see 3 grams\nN = 3\n# input sentence\nsentence = \"hi, how are you?\"\n# tokenized sentence\ntokenized_sentence = word_tokenize(sentence)\n# generate n_grams\nn_grams = list(ngrams(tokenized_sentence, N))\nprint(n_grams)\n","d3c979c6":"import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n # we go over the folds created\nfor fold_ in range(5):\n # temporary dataframes for train and test\n    train_df = df[df.kfold != fold_].reset_index(drop=True)\n    test_df = df[df.kfold == fold_].reset_index(drop=True)\n # initialize TfidfVectorizer with NLTK's word_tokenize\n # function as tokenizer\n    tfidf_vec = TfidfVectorizer(\n    tokenizer=word_tokenize,\n    token_pattern=None,\n    ngram_range=(1,3)\n )\n # fit tfidf_vec on training data reviews\n    tfidf_vec.fit(train_df.reviews)\n # transform training and validation data reviews\n    xtrain = tfidf_vec.transform(train_df.reviews)\n    xtest = tfidf_vec.transform(test_df.reviews)\n # initialize logistic regression model\n    model = linear_model.LogisticRegression()\n # fit the model on training data reviews and sentiment\n    model.fit(xtrain, train_df.sentiment)\n # make predictions on test data\n # threshold for predictions is 0.5\n    preds = model.predict(xtest)\n # calculate accuracy\n    accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n    print(f\"Fold: {fold_}\")\n    print(f\"Accuracy = {accuracy}\")\n    print(\"\")","32f2c7c8":"# Stemming and lemmatization\n#  lemmatization is more aggressive than stemming and stemming is\n# more popular and widely used.\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.snowball import SnowballStemmer\n# initialize lemmatizer\nlemmatizer = WordNetLemmatizer()\n# initialize stemmer\nstemmer = SnowballStemmer(\"english\")\nwords = [\"fishing\", \"fishes\", \"fished\"]\nfor word in words:\n    print(f\"word={word}\")\n    print(f\"stemmed_word={stemmer.stem(word)}\")\n    print(f\"lemma={lemmatizer.lemmatize(word)}\")\n    print(\"\")\n\n","a3894154":"## One more topic that you should be aware of is topic extraction. Topic extraction\n## can be done using non-negative matrix factorization (NMF) or latent semantic\n## analysis (LSA), which is also popularly known as singular value decomposition or\n## SVD. These are decomposition techniques that reduce the data to a given number\n## of components. You can fit any of these on sparse matrix obtained from\n## CountVectorizer or TfidfVectorizer. \n##\n##\n##\n##\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom sklearn import decomposition\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# create a corpus of sentences\n# we read only 10k samples from training data\n# for this example\ncorpus = pd.read_csv(\"..\/input\/cleaned-imdb\/imdb_clean.csv\", nrows=10000)\ncorpus = corpus.review.values\n# initialize TfidfVectorizer with word_tokenize from nltk\n# as the tokenizer\ntfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n# fit the vectorizer on corpus\ntfv.fit(corpus)\n# transform the corpus using tfidf\ncorpus_transformed = tfv.transform(corpus)\n# initialize SVD with 10 components\nsvd = decomposition.TruncatedSVD(n_components=10)\n# fit SVD\ncorpus_svd = svd.fit(corpus_transformed)\nsample_index = 0\nfeature_scores = dict(\n zip(\n tfv.get_feature_names(),\n corpus_svd.components_[sample_index]\n )\n)\nN = 5\nprint(sorted(feature_scores, key=feature_scores.get, reverse=True)[:N])\n","fc6a99f9":"N = 5\nfor sample_index in range(5):\n feature_scores = dict(\n zip(\n tfv.get_feature_names(),\n corpus_svd.components_[sample_index]\n )\n )\n print(\n sorted(\n feature_scores,\n key=feature_scores.get,\n reverse=True\n )[:","d6777094":"# clean data to make sense\nimport re\nimport string\ndef clean_text(s):\n \"\"\"\n This function cleans the text a bit\n :param s: string\n :return: cleaned string\n \"\"\"\n# split by all whitespaces\n s = s.split()\n\n # join tokens by single space\n # why we do this?\n # this will remove all kinds of weird space\n # \"hi. how are you\" becomes\n # \"hi. how are you\"\n s = \" \".join(s)\n\n # remove all punctuations using regex and string module\n s = re.sub(f'[{re.escape(string.punctuation)}]', '', s)\n\n # you can add more cleaning here if you want\n # and then return the cleaned string\n return s\n","0e19ebe5":"# \u201chi, how are you????\u201d to \u201chi how are you\u201d","b1c3bec0":"import pandas as pd\n\ncorpus = pd.read_csv(\"..\/input\/cleaned-imdb\/imdb_clean.csv\", nrows=10000)\ncorpus.loc[:, \"reviews\"] = corpus.review.apply(clean_text)","dfa8acc9":"#make it even better by removing stopwords in your cleaning function. What are\n#stopwords? These are high-frequency words that exist in every language. For\n#example, in the English language, these words are \u201ca\u201d, \u201can\u201d, \u201cthe\u201d, \u201cfor\u201d, etc.","962ee341":"import numpy as np\ndef sentence_to_vec(s, embedding_dict, stop_words, tokenizer):\n    \"\"\"\n     Given a sentence and other information,\n     this function returns embedding for the whole sentence\n     :param s: sentence, string\n     :param embedding_dict: dictionary word:vector\n     :param stop_words: list of stop words, if any\n     :param tokenizer: a tokenization function\n     \"\"\"\n     # convert sentence to string and lowercase it\n    words = str(s).lower()\n\n     # tokenize the sentence\n    words = tokenizer(words)\n\n     # remove stop word tokens\n    words = [w for w in words if not w in stop_words]\n\n     # keep only alpha-numeric tokens\n    words = [w for w in words if w.isalpha()]\n\n     # initialize empty list to store embeddings\n    M = []\n    for w in words:\n     # for evert word, fetch the embedding from\n     # the dictionary and append to list of\n     # embeddings\n        if w in embedding_dict:\n            M.append(embedding_dict[w])\n\n     # if we dont have any vectors, return zero\n    if len(M) == 0:\n        return np.zeros(300)\n     # convert list of embeddings to array\n    M = np.array(M)\n\n     # calculate sum over axis=0\n    v = M.sum(axis=0)\n     # return normalized vector\n    return v \/ np.sqrt((v ** 2).sum())\n","3c60ffe1":"# fasttext.py\nimport io\nimport numpy as np\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef load_vectors(fname):\n    # taken from: https:\/\/fasttext.cc\/docs\/en\/english-vectors.html\n    fin = io.open(\n     fname,\n     'r',\n    encoding='utf-8',\n     newline='\\n',\n     errors='ignore'\n     )\n    n, d = map(int, fin.readline().split())\n    data = {}\n    for line in fin:\n        tokens = line.rstrip().split(' ')\n        data[tokens[0]] = list(map(float, tokens[1:]))\n    return data","f4e23614":"def sentence_to_vec(s, embedding_dict, stop_words, tokenizer):\n\nif __name__ == \"__main__\":\n    # read the training data\n    df = pd.read_csv(\"..\/input\/cleaned-imdb\/imdb_clean.csv\")\n    # map positive to 1 and negative to 0\n    df.sentiment = df.sentiment.apply(\n  lambda x: 1 if x == \"positive\" else 0\n )\n # the next step is to randomize the rows of the data\n    df = df.sample(frac=1).reset_index(drop=True)\n # load embeddings into memory\n    print(\"Loading embeddings\")\n    embeddings = load_vectors(\"..\/input\/crawl-300d-2M.vec\")\n # create sentence embeddings\n    print(\"Creating sentence vectors\")\n    vectors = []\n    for review in df.review.values:\n        vectors.append(\n         sentence_to_vec(\n         s = review,\n         embedding_dict = embeddings,\n         stop_words = [],\n         tokenizer = word_tokenize\n     )\n )\n\n    vectors = np.array(vectors)\n    # fetch labels\n    y = df.sentiment.values\n\n # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=5)\n\n # fill the new kfold column\n    for fold_, (t_, v_) in enumerate(kf.split(X=vectors, y=y)):\n        print(f\"Training fold: {fold_}\")\n # temporary dataframes for train and test\n        xtrain = vectors[t_, :]\n        ytrain = y[t_]\n        xtest = vectors[v_, :]\n        ytest = y[v_]\n # initialize logistic regression model\n        model = linear_model.LogisticRegression()\n # fit the model on training data reviews and sentimen\n        model.fit(xtrain, ytrain)\n # make predictions on test data\n # threshold for predictions is 0.5\n        preds = model.predict(xtest)\n # calculate accuracy\n        accuracy = metrics.accuracy_score(ytest, preds)\n        print(f\"Accuracy = {accuracy}\")\n        print(\"\")\n","69ea1cb4":"# import pandas and model_selection module of scikit-learn\nimport pandas as pd\nfrom sklearn import model_selection\nif __name__ == \"__main__\":\n # Read training data\n df = pd.read_csv(\"..\/input\/cleaned-imdb\/imdb_clean.csv\")\n # map positive to 1 and negative to 0\n df.sentiment = df.sentiment.apply(\n lambda x: 1 if x == \"positive\" else 0\n )\n # we create a new column called kfold and fill it with -1\n df[\"kfold\"] = -1\n# the next step is to randomize the rows of the data\n df = df.sample(frac=1).reset_index(drop=True)\n\n # fetch labels\n y = df.sentiment.values\n\n # initiate the kfold class from model_selection module\n kf = model_selection.StratifiedKFold(n_splits=5)\n\n # fill the new kfold column\n for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n df.loc[v_, 'kfold'] = f\n\n # save the new csv with kfold column\n df.to_csv(\"..\/input\/imdb_folds.csv\", index=False)\n","8bf6f2c4":"import torch\nclass IMDBDataset:\n def __init__(self, reviews, targets):\n \"\"\"\n :param reviews: this is a numpy array\n :param targets: a vector, numpy array\n \"\"\"\n self.reviews = reviews\n self.target = targets\n def __len__(self):\n # returns length of the dataset\n return len(self.reviews)\n\n def __getitem__(self, item):\n # for any given item, which is an int,\n # return review and targets as torch tensor\n # item is the index of the item in concern\n review = self.reviews[item, :]\n target = self.target[item]\n return {\"review\": torch.tensor(review, dtype=torch.long),\n \"target\": torch.tensor(target, dtype=torch.float)\n }\n","0675a9d5":"import torch\nimport torch.nn as nn\nclass LSTM(nn.Module):\n def __init__(self, embedding_matrix):\n \"\"\"\n :param embedding_matrix: numpy array with vectors for all words\n \"\"\"\n super(LSTM, self).__init__()\n # number of words = number of rows in embedding matrix\n num_words = embedding_matrix.shape[0]\n # dimension of embedding is num of columns in the matrix\n embed_dim = embedding_matrix.shape[1]\n # we define an input embedding layer\n self.embedding = nn.Embedding(\n num_embeddings=num_words,\n embedding_dim=embed_dim\n )\n # embedding matrix is used as weights of\n # the embedding layer\n self.embedding.weight = nn.Parameter(\n torch.tensor(\n embedding_matrix,\n dtype=torch.float32\n )\n )\n # we dont want to train the pretrained embeddings\n self.embedding.weight.requires_grad = False\n # a simple bidirectional LSTM with\n # hidden size of 128\nself.lstm = nn.LSTM(\n embed_dim,\n 128,\n bidirectional=True,\n batch_first=True,\n )\n # output layer which is a linear layer\n # we have only one output\n # input (512) = 128 + 128 for mean and same for max pooling\n self.out = nn.Linear(512, 1)\n def forward(self, x):\n # pass data through embedding layer\n # the input is just the tokens\n x = self.embedding(x)\n # move embedding output to lstm\n x, _ = self.lstm(x)\n # apply mean and max pooling on lstm output\n avg_pool = torch.mean(x, 1)\n max_pool, _ = torch.max(x, 1)\n\n # concatenate mean and max pooling\n # this is why size is 512\n # 128 for each direction = 256\n # avg_pool = 256 and max_pool = 256\n out = torch.cat((avg_pool, max_pool), 1)\n # pass through the output layer and return the output\n out = self.out(out)\n # return linear output\n return out\n","cd96878e":"import torch\nimport torch.nn as nn\ndef train(data_loader, model, optimizer, device):\n    # set model to training mode\n model.train()\n # go through batches of data in data loader\n for data in data_loader:\n # fetch review and target from the dict\n reviews = data[\"review\"]\n targets = data[\"target\"]\n # move the data to device that we want to use\n reviews = reviews.to(device, dtype=torch.long)\n targets = targets.to(device, dtype=torch.float)\n # clear the gradients\n optimizer.zero_grad()\n # make predictions from the model\n predictions = model(reviews)\n # calculate the loss\n loss = nn.BCEWithLogitsLoss()(\n predictions,\n targets.view(-1, 1)\n )\n # compute gradient of loss w.r.t.\n # all parameters of the model that are trainable\n loss.backward()\n # single optimization step\n optimizer.step()\ndef evaluate(data_loader, model, device):\n # initialize empty lists to store predictions\n # and targets\n final_predictions = []\n final_targets = []\n # put the model in eval mode\nmodel.eval()\n # disable gradient calculation\n with torch.no_grad():\n for data in data_loader:\n reviews = data[\"review\"]\n targets = data[\"target\"]\n reviews = reviews.to(device, dtype=torch.long)\n targets = targets.to(device, dtype=torch.float)\n # make predictions\n predictions = model(reviews)\n # move predictions and targets to list\n # we need to move predictions and targets to cpu too\n predictions = predictions.cpu().numpy().tolist()\n targets = data[\"target\"].cpu().numpy().tolist()\n final_predictions.extend(predictions)\n final_targets.extend(targets)\n # return final predictions and targets\n return final_predictions, final_targets\n    ","30a18a33":"import io\nimport torch\nimport numpy as np\nimport pandas as pd\n# yes, we use tensorflow\n# but not for training the model!\nimport tensorflow as tf\nfrom sklearn import metrics\nimport config\nimport dataset\nimport engine\nimport lstm\ndef load_vectors(fname):\n # taken from: https:\/\/fasttext.cc\/docs\/en\/english-vectors.html\nfin = io.open(\n fname,\n 'r',\n encoding='utf-8',\n newline='\\n',\n errors='ignore'\n )\n n, d = map(int, fin.readline().split())\n data = {}\n for line in fin:\n tokens = line.rstrip().split(' ')\n data[tokens[0]] = list(map(float, tokens[1:]))\n return data\ndef create_embedding_matrix(word_index, embedding_dict):\n \"\"\"\n This function creates the embedding matrix.\n :param word_index: a dictionary with word:index_value\n :param embedding_dict: a dictionary with word:embedding_vector\n :return: a numpy array with embedding vectors for all known words\n \"\"\"\n # initialize matrix with zeros\n embedding_matrix = np.zeros((len(word_index) + 1, 300))\n # loop over all the words\n for word, i in word_index.items():\n # if word is found in pre-trained embeddings,\n # update the matrix. if the word is not found,\n # the vector is zeros!\n if word in embedding_dict:\n embedding_matrix[i] = embedding_dict[word]\n # return embedding matrix\n return embedding_matrix\ndef run(df, fold):\n \"\"\"\n Run training and validation for a given fold\n and dataset\n :param df: pandas dataframe with kfold column\n :param fold: current fold, int\n \"\"\"\n # fetch training dataframe\n train_df = df[df.kfold != fold].reset_index(drop=True)\n # fetch validation dataframe\nvalid_df = df[df.kfold == fold].reset_index(drop=True)\n print(\"Fitting tokenizer\")\n # we use tf.keras for tokenization\n # you can use your own tokenizer and then you can\n # get rid of tensorflow\n tokenizer = tf.keras.preprocessing.text.Tokenizer()\n tokenizer.fit_on_texts(df.review.values.tolist())\n # convert training data to sequences\n # for example : \"bad movie\" gets converted to\n # [24, 27] where 24 is the index for bad and 27 is the\n # index for movie\n xtrain = tokenizer.texts_to_sequences(train_df.review.values)\n # similarly convert validation data to\n # sequences\n xtest = tokenizer.texts_to_sequences(valid_df.review.values)\n # zero pad the training sequences given the maximum length\n # this padding is done on left hand side\n # if sequence is > MAX_LEN, it is truncated on left hand side too\n xtrain = tf.keras.preprocessing.sequence.pad_sequences(\n xtrain, maxlen=config.MAX_LEN\n )\n # zero pad the validation sequences\n xtest = tf.keras.preprocessing.sequence.pad_sequences(\n xtest, maxlen=config.MAX_LEN\n )\n # initialize dataset class for training\n train_dataset = dataset.IMDBDataset(\n reviews=xtrain,\n targets=train_df.sentiment.values\n )\n # create torch dataloader for training\n # torch dataloader loads the data using dataset\n # class in batches specified by batch size\n train_data_loader = torch.utils.data.DataLoader(\n train_dataset,\n batch_size=config.TRAIN_BATCH_SIZE,\n num_workers=2\n )\n # initialize dataset class for validation\nvalid_dataset = dataset.IMDBDataset(\n reviews=xtest,\n targets=valid_df.sentiment.values\n )\n\n # create torch dataloader for validation\n valid_data_loader = torch.utils.data.DataLoader(\n valid_dataset,\n batch_size=config.VALID_BATCH_SIZE,\n num_workers=1\n )\n print(\"Loading embeddings\")\n # load embeddings as shown previously\n embedding_dict = load_vectors(\"..\/input\/crawl-300d-2M.vec\")\n embedding_matrix = create_embedding_matrix(\n tokenizer.word_index, embedding_dict\n )\n # create torch device, since we use gpu, we are using cuda\n device = torch.device(\"cuda\")\n # fetch our LSTM model\n model = lstm.LSTM(embedding_matrix)\n # send model to device\n model.to(device)\n\n # initialize Adam optimizer\n optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n print(\"Training Model\")\n # set best accuracy to zero\n best_accuracy = 0\n # set early stopping counter to zero\n early_stopping_counter = 0\n # train and validate for all epochs\n for epoch in range(config.EPOCHS):\n # train one epoch\n engine.train(train_data_loader, model, optimizer, device)\n # validate\n outputs, targets = engine.evaluate(\n valid_data_loader, model, device\n )\n # use threshold of 0.5\n # please note we are using linear layer and no sigmoid\n# you should do this 0.5 threshold after sigmoid\n outputs = np.array(outputs) >= 0.5\n # calculate accuracy\n accuracy = metrics.accuracy_score(targets, outputs)\n print(\n f\"FOLD:{fold}, Epoch: {epoch}, Accuracy Score = {accuracy}\"\n )\n # simple early stopping\n if accuracy > best_accuracy:\n best_accuracy = accuracy\n else:\n early_stopping_counter += 1\n if early_stopping_counter > 2:\n break\nif __name__ == \"__main__\":\n # load data\n df = pd.read_csv(\"..\/input\/imdb_folds.csv\")\n # train for all folds\n run(df, fold=0)\n run(df, fold=1)\n run(df, fold=2)\n run(df, fold=3)\n run(df, fold=4)\n","88392728":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 8\nEPOCHS = 10","8389f5db":"## word embeddings","8480c617":"You have seen that till now we\nconverted the tokens into numbers. So, if there are N unique tokens in a given\ncorpus, they can be represented by integers ranging from 0 to N-1. Now we will\nrepresent these integer tokens with vectors. This representation of words into\nvectors is known as word embeddings or word vectors. Google\u2019s Word2Vec is one\nof the oldest approaches to convert words into vectors. We also have FastText from\nFacebook and GloVe (Global Vectors for Word Representation) from Stanford.\nThese approaches are quite different from each other. ","27abfea8":"# LSTM"}}