{"cell_type":{"7f8c43db":"code","7a452bdb":"code","0de3cb7c":"code","011bca0e":"code","802e4030":"code","0ba14014":"code","2e465b08":"code","6e6c411c":"code","0270156e":"code","56d5e8fb":"code","012bdbac":"code","70cbe0ba":"code","cfd170dd":"code","f02f5930":"code","b5d50b74":"code","e2ffc1a7":"code","abb20557":"code","c0631366":"code","e3558c7b":"code","da42fa9f":"code","3c05c753":"code","4bee321b":"code","037d500f":"code","b641b333":"code","08f7a7e5":"code","ef1d7208":"code","6d8eb247":"code","53aaa700":"markdown","8cf12218":"markdown","16aa44d1":"markdown","6a43da0c":"markdown","f6bc7a48":"markdown","d5ee685f":"markdown","71d24423":"markdown","3ae458f1":"markdown","2e2e9240":"markdown","2f12ef1f":"markdown","4a0e6d3b":"markdown","c690cb7f":"markdown","bcd0daaf":"markdown","92c4223e":"markdown","4d1a33a6":"markdown","707b75b2":"markdown"},"source":{"7f8c43db":"import os\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns # plotting\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import Pool \nfrom sklearn import metrics\nimport shap as shap\nfrom tqdm import tqdm\n\npd.set_option('display.max_rows', 120)\npd.set_option('display.max_columns', 200)\n\nSEED = 91 # random seed","7a452bdb":"PATH = '\/kaggle\/input\/tabular-playground-series-sep-2021\/' # you can use your own local path\n\nprint(f\"Files in directory {PATH.split('\/')[-2]}:\")\nfor _, _, filenames in os.walk(PATH):\n    for filename in filenames:\n        print('  '+os.path.join(filename))","0de3cb7c":"try:\n    df_train = pd.read_csv(PATH+'train.csv', index_col=0)\n    df_test = pd.read_csv(PATH+'test.csv', index_col=0)\n    print('All data has been loaded successfully!')\nexcept Exception as err:\n    print(repr(err))","011bca0e":"#df_train = df_train.sample(frac=0.15, random_state=SEED)","802e4030":"full_lenght_data = len(df_train) + len(df_test)\nprint(f\"train: {len(df_train)} ({100*len(df_train)\/full_lenght_data:.0f}%)\")\nprint(f\"test:  {len(df_test)} ({100*len(df_test)\/full_lenght_data:.0f}%)\")","0ba14014":"df_train.info()","2e465b08":"df_test.info()","6e6c411c":"df_train.isnull().sum().sort_values(ascending=False)","0270156e":"print(f\"claim = 0: {len(df_train[df_train['claim'] == 0])} ({100*len(df_train[df_train['claim'] == 0])\/len(df_train):.2f}%)\")\nprint(f\"claim = 1: {len(df_train[df_train['claim'] == 1])} ({100*len(df_train[df_train['claim'] == 1])\/len(df_train):.2f}%)\")","56d5e8fb":"def add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df_new = df.copy()\n    features = [x for x in df.columns.values if x not in 'claim']\n\n    df_new['num_missing'] = df_new[features].isna().sum(axis=1)\n    df_new['num_missing_std'] = df_new[features].isna().std(axis=1).astype('float')\n    df_new['abs_sum'] = df_new[features].abs().sum(axis=1)\n    df_new['median'] = df_new[features].median(axis=1)\n    df_new['std'] = df_new[features].std(axis=1)\n    df_new['min'] = df_new[features].abs().min(axis=1)\n    df_new['max'] = df_new[features].abs().max(axis=1)\n    #df_new['sem'] = df_new[features].sem(axis=1)\n    df_new['avg'] = df_new[features].mean(axis=1)\n    \n    return df_new","012bdbac":"def preprocess_na(df: pd.DataFrame) -> pd.DataFrame:\n    df_new = df.copy()\n    features = df.columns.tolist()\n    \n    fill_value_dict = {\n        'f1': 'Mean', \n        'f2': 'Median', \n        'f3': 'Median', \n        'f4': 'Median', \n        'f5': 'Mode', \n        'f6': 'Mean', \n        'f7': 'Median', \n        'f8': 'Median', \n        'f9': 'Median', \n        'f10': 'Median', \n        'f11': 'Mean', \n        'f12': 'Median', \n        'f13': 'Mean', \n        'f14': 'Median', \n        'f15': 'Mean', \n        'f16': 'Median', \n        'f17': 'Median', \n        'f18': 'Median', \n        'f19': 'Median', \n        'f20': 'Median', \n        'f21': 'Median', \n        'f22': 'Mean', \n        'f23': 'Mode', \n        'f24': 'Median', \n        'f25': 'Median', \n        'f26': 'Median', \n        'f27': 'Median', \n        'f28': 'Median', \n        'f29': 'Mode', \n        'f30': 'Median', \n        'f31': 'Median', \n        'f32': 'Median', \n        'f33': 'Median', \n        'f34': 'Mean', \n        'f35': 'Median', \n        'f36': 'Mean', \n        'f37': 'Median', \n        'f38': 'Median', \n        'f39': 'Median', \n        'f40': 'Mode', \n        'f41': 'Median', \n        'f42': 'Mode', \n        'f43': 'Mean', \n        'f44': 'Median', \n        'f45': 'Median', \n        'f46': 'Mean', \n        'f47': 'Mode', \n        'f48': 'Mean', \n        'f49': 'Mode', \n        'f50': 'Mode', \n        'f51': 'Median', \n        'f52': 'Median', \n        'f53': 'Median', \n        'f54': 'Mean', \n        'f55': 'Mean', \n        'f56': 'Mode', \n        'f57': 'Mean', \n        'f58': 'Median', \n        'f59': 'Median', \n        'f60': 'Median', \n        'f61': 'Median', \n        'f62': 'Median', \n        'f63': 'Median', \n        'f64': 'Median', \n        'f65': 'Mode', \n        'f66': 'Median', \n        'f67': 'Median', \n        'f68': 'Median', \n        'f69': 'Mean', \n        'f70': 'Mode', \n        'f71': 'Median', \n        'f72': 'Median', \n        'f73': 'Median', \n        'f74': 'Mode', \n        'f75': 'Mode', \n        'f76': 'Mean', \n        'f77': 'Mode', \n        'f78': 'Median', \n        'f79': 'Mean', \n        'f80': 'Median', \n        'f81': 'Mode', \n        'f82': 'Median', \n        'f83': 'Mode', \n        'f84': 'Median', \n        'f85': 'Median', \n        'f86': 'Median', \n        'f87': 'Median', \n        'f88': 'Median', \n        'f89': 'Median', \n        'f90': 'Mean', \n        'f91': 'Mode', \n        'f92': 'Median', \n        'f93': 'Median', \n        'f94': 'Median', \n        'f95': 'Median', \n        'f96': 'Median', \n        'f97': 'Mean', \n        'f98': 'Median', \n        'f99': 'Median', \n        'f100': 'Mode', \n        'f101': 'Median', \n        'f102': 'Median', \n        'f103': 'Median', \n        'f104': 'Median', \n        'f105': 'Median', \n        'f106': 'Median', \n        'f107': 'Median', \n        'f108': 'Median', \n        'f109': 'Mode', \n        'f110': 'Median', \n        'f111': 'Median', \n        'f112': 'Median', \n        'f113': 'Mean', \n        'f114': 'Median', \n        'f115': 'Median', \n        'f116': 'Mode', \n        'f117': 'Median', \n        'f118': 'Mean'\n    }\n\n\n    for col in tqdm(features):\n        if fill_value_dict.get(col)=='Mean':\n            fill_value = df_new[col].mean()\n        elif fill_value_dict.get(col)=='Median':\n            fill_value = df_new[col].median()\n        elif fill_value_dict.get(col)=='Mode':\n            fill_value = df_new[col].mode().iloc[0]\n    \n        df_new[col].fillna(fill_value, inplace=True)\n    \n    return df_new","70cbe0ba":"print(f\"Number of features before preprocess: train_df={df_train.shape[1]} test_df={df_test.shape[1]}\")\n\ndf_train = add_features(df_train)\ndf_train = preprocess_na(df_train)\ndf_test = add_features(df_test)\ndf_test = preprocess_na(df_test)\n\nprint(f\"After: train_df={df_train.shape[1]} test_df={df_test.shape[1]}\")\ndf_train.head()","cfd170dd":"TARGET = 'claim'\n\nX = df_train.copy()\ny = X.pop(TARGET)","f02f5930":"scaler = MinMaxScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns=list(df_train.columns).remove(TARGET))\nX_test = pd.DataFrame(scaler.transform(df_test), columns=list(df_train.columns).remove(TARGET))","b5d50b74":"model_results = {'model': [], 'score': [], 'training_time': []}\n\ndef add_model_result(dic, model, score, time=None, fi=None):\n    '''Save results of every model'''\n    dic['model'].append(model)\n    dic['score'].append(score)\n    if time:\n        dic['training_time'].append(time)","e2ffc1a7":"models = {\n    'XGB 1': { # https:\/\/www.kaggle.com\/mlanhenke\/tps-09-simple-blend-stacking-xgb-lgbm-catb\n        'model': XGBClassifier(\n            eval_metric='auc',\n            max_depth=4,\n            alpha=10,\n            subsample=0.65,\n            colsample_bytree=0.7,\n            colsample_bylevel = 0.8675692743597421,\n            objective='binary:logistic',\n            use_label_encoder=False,\n            learning_rate=0.012,\n            n_estimators=10000,\n            min_child_weight = 366,\n            tree_method='gpu_hist',\n            gpu_id=0,\n            predictor='gpu_predictor',\n            n_jobs=-1,\n        ),\n        'feature_importance': 0\n    },\n    \n    'XGB 2': { # https:\/\/www.kaggle.com\/mlanhenke\/tps-09-simple-blend-stacking-xgb-lgbm-catb\n        'model': XGBClassifier(\n            eval_metric='auc',\n            max_depth=3,\n            subsample=0.5,\n            colsample_bytree=0.5,\n            learning_rate=0.01187431306013263,\n            objective='binary:logistic',\n            use_label_encoder=False,\n            n_estimators=10000,\n            tree_method='gpu_hist',\n            gpu_id=0,\n            predictor='gpu_predictor',\n            n_jobs=-1,\n            seed=SEED\n        ),\n        'feature_importance': 0\n    },\n\n#     'LGBM 1': {\n#         'model': LGBMClassifier(\n#             num_leaves = 28,\n#             n_estimators = 3000,\n#             max_depth = 8,\n#             min_child_samples = 202,\n#             learning_rate = 0.11682677767413432,\n#             bagging_fraction = 0.5036513634677549,\n#             colsample_bytree = 0.7519268943195143,\n#             n_jobs = 4,\n#             random_seed = SEED\n#         ),\n#         'feature_importance': 0\n#     },\n\n#     'LGBM 2': { # https:\/\/www.kaggle.com\/tensorchoko\/tabular-sep-2021-lightgbm\n#         'model': LGBMClassifier(\n#             learning_rate = 0.03,\n#             num_iterations = 30000,\n#             objective ='binary',\n#             metric = 'binary_logloss',\n#             feature_pre_filter = False,\n#             lambda_l1 = 0.0,\n#             lambda_l2 = 0.0,\n#             num_leaves = 123,\n#             feature_fraction = 1.0,\n#             bagging_fraction = 1.0,\n#             bagging_freq = 0,\n#             min_child_samples = 20,\n#             n_jobs = 4,\n#             random_seed = SEED+1\n#         ),\n#         'feature_importance': 0\n#     },\n\n#     'LGBM 3': { # https:\/\/www.kaggle.com\/mlanhenke\/tps-09-simple-blend-stacking-xgb-lgbm-catb\n#         'model': LGBMClassifier(\n#             max_depth = 4,\n#             objective = 'binary',\n#             metric = 'auc',\n#             n_estimators = 5000,\n#             learning_rate = 0.1,\n#             reg_alpha = 18,\n#             reg_lambda = 17,\n#             num_leaves = 7,\n#             colsample_bytree = 0.3,\n#             device = 'gpu',\n#             n_jobs = 4,\n#             random_seed = SEED\n#         ),\n#         'feature_importance': 0\n#     },\n    \n    'LGBM 4': { # https:\/\/www.kaggle.com\/realtimshady\/single-simple-lightgbm\n        'model': LGBMClassifier(\n            max_depth = 4,\n            objective = 'binary',\n            metric = 'auc',\n            n_estimators = 30000,\n            learning_rate = 0.02,\n            reg_alpha = 25.2,\n            reg_lambda = 90,\n            num_leaves = 148,\n            subsample = 0.71,\n            subsample_freq = 1,\n            colsample_bytree = 0.98,\n            min_child_samples = 99,\n            min_child_weight = 152,\n            #n_jobs = 4,\n            device = 'gpu',\n            random_seed = 3407\n        ),\n        'feature_importance': 0\n    },\n\n    'LGBM 5': { # https:\/\/www.kaggle.com\/hiro5299834\/tps-sep-2021-single-lgbm\n        'model': LGBMClassifier(\n            objective = 'binary',\n            metric = 'AUC',\n            n_estimators = 20000, #20000,\n            learning_rate = 0.01, #5e-3,\n            subsample = 0.6,\n            subsample_freq = 1,\n            colsample_bytree = 0.4,\n            reg_alpha = 10.0,\n            reg_lambda = 1e-1,\n            min_child_weight = 256,\n            min_child_samples = 20,\n            importance_type = 'gain',\n            random_seed = SEED\n        ),\n        'feature_importance': 0\n    },\n\n    'LGBM 6': { # https:\/\/www.kaggle.com\/towhidultonmoy\/tuned-lightgbm\n        'model': LGBMClassifier(\n            objective = 'binary',\n            boosting_type = 'gbdt', #gbdt\n            num_leaves = 6, #6 #2^(max_depth)\n            max_depth = 2, #2  \n            learning_rate = 0.1, #0.1\n            n_estimators = 41000, #40000\n            reg_alpha = 25.0,\n            reg_lambda = 76.7,\n            bagging_seed = 7014, #42\n            feature_fraction_seed = 7014, #42\n            subsample = 0.985,\n            subsample_freq = 1,\n            colsample_bytree = 0.69,\n            min_child_samples = 54,\n            min_child_weight = 256,\n            device = 'gpu',\n            random_seed = SEED\n        ),\n        'feature_importance': 0\n    },\n    \n    'CatBoost 1': {\n        'model': CatBoostClassifier(\n            class_weights = [1,1.15],\n            depth = 7,\n            learning_rate = 0.02,\n            iterations = 16000,\n            bootstrap_type = 'Bernoulli',\n            subsample = 0.98,\n            task_type = 'GPU',\n            #thread_count = 4,\n            random_seed = 3407\n        ),\n        'feature_importance': 0\n    },\n\n    'CatBoost 2': { # https:\/\/www.kaggle.com\/brendanartley\/sep-21-tab-series-lgbm-optuna\n        'model': CatBoostClassifier(\n            iterations = 15585, \n            objective = 'CrossEntropy', \n            bootstrap_type = 'Bernoulli', \n            od_wait = 1144, \n            learning_rate = 0.023575206684596582, \n            reg_lambda = 36.30433203563295, \n            random_strength = 43.75597655616195, \n            depth = 7, \n            min_data_in_leaf = 11, \n            leaf_estimation_iterations = 1, \n            subsample = 0.8227911142845009,\n            task_type = 'GPU',\n            #thread_count = 4,\n            random_seed = SEED\n        ),\n        'feature_importance': 0\n    },\n    \n#     'CatBoost 3': { # https:\/\/www.kaggle.com\/kennethquisado\/xgboost-10fold-cv-blend\n#         'model': CatBoostClassifier(\n#             eval_metric = 'AUC',\n#             #n_estimators = 10000,\n#             max_depth = 6,\n#             learning_rate = 0.04,\n#             grow_policy = \"SymmetricTree\",\n#             l2_leaf_reg = 3.0,\n#             random_strength = 1.0,\n#             task_type = 'GPU',\n#             #thread_count = 4,\n#             random_seed = SEED+2\n#         ),\n#         'feature_importance': 0\n#     },\n}","abb20557":"N_FOLD =  5\nkfold = KFold(n_splits = N_FOLD, random_state = SEED, shuffle = True)\n\nmodel_results_level0 = {'model': [], 'score': [], 'training_time': []}\npredicted_probabilities = pd.DataFrame(X.index, columns=['id'])\ntest_predicted_probabilities = pd.DataFrame(X_test.index, columns=['id'])\n\nfor m in models:\n    print(f\"{m}:\")\n    predictions_valid  = np.zeros(X.shape[0])\n    probabilities_valid = np.zeros(X.shape[0])\n    test_predicted_probabilities[m] = np.zeros(X_test.shape[0])\n    score = 0\n    \n    start_time = time.time()\n    # Iterate through each fold\n    for fold, (train_idx, valid_idx) in enumerate(kfold.split(X)):\n        X_train = X.iloc[train_idx]\n        X_valid = X.iloc[valid_idx]\n        y_train = y.iloc[train_idx]\n        y_valid = y.iloc[valid_idx]        \n\n        model = models[m]['model']\n        model.fit(X_train, y_train,\n                  eval_set = [(X_valid, y_valid)],\n                  early_stopping_rounds = 120,\n                  verbose = False\n                 )\n\n        # Mean of the predictions\n        test_predicted_probabilities[m] += model.predict_proba(X_test)[:,1] \/ N_FOLD\n\n        # Mean of feature importance\n        models[m]['feature_importance'] += model.feature_importances_ \/ N_FOLD\n\n        # Out of Fold predictions\n        predictions_valid[valid_idx] = model.predict(X_valid)\n        probabilities_valid[valid_idx] = model.predict_proba(X_valid)[:,1]\n        fold_score = metrics.roc_auc_score(y_valid, predictions_valid[valid_idx])\n        print(f\"Fold {fold} | ROC-AUC: {fold_score:.3f}\")\n\n        score += fold_score \/ N_FOLD\n\n    predicted_probabilities[m] = probabilities_valid\n    add_model_result(model_results_level0, m, score, time.time()-start_time)\n    print(f\"Overall ROC-AUC: {score:.6f}\\n\")","c0631366":"model_results = pd.DataFrame(model_results_level0).sort_values('score', ascending=False)\nmodel_results","e3558c7b":"predicted_probabilities[TARGET] = y.reset_index()[TARGET]\npredicted_probabilities = predicted_probabilities.drop('id', axis=1)\npredicted_probabilities","da42fa9f":"df_fi = pd.concat([pd.DataFrame(models[m]['feature_importance'], index=df_test.columns, columns=[m]) for m in models],\n                  axis=1)\ndf_fi = df_fi.fillna(0).apply(lambda x: x\/sum(x)*100)\ndf_fi['overall'] = df_fi.apply(lambda x: sum(x), axis=1)\ndf_fi = df_fi.apply(lambda x: x\/sum(x)*100)\ndf_fi.sort_values('overall', ascending=False)","3c05c753":"TRESHOLD = 0.5 # treshold to decide claim or not","4bee321b":"plt.figure(figsize=(17, 11))\nplt.subplots_adjust(hspace=0.5, wspace=0.3)\n#sns.set_palette(\"Spectral\")\nfor i, m in enumerate(models):\n    plt.subplot(3, 3, i+1)\n    predictions = predicted_probabilities[m].apply(lambda x: 1 if x > TRESHOLD else 0)\n    df_cm = pd.DataFrame(metrics.confusion_matrix(y, predictions), columns=np.unique(y), index = np.unique(y))\n    df_cm.index.name = 'Actual'\n    df_cm.columns.name = 'Predicted'\n    sns.heatmap(df_cm, cmap=\"Blues\", annot=True, fmt='g')\n    plt.title(f\"{m} (acc={metrics.accuracy_score(y, predictions):.3f})\")\nplt.show()","037d500f":"# Plot ROC curve\ndef plot_roc_curve(fpr=None, tpr=None):\n    \"\"\"Plot custom histogram\"\"\"\n    plt.figure(figsize=(5,5))\n    plt.title('ROC-curve', fontsize=16)\n    plt.xlabel('False Positive Rate', fontsize=14)\n    plt.ylabel('True Positive Rate', fontsize=14)\n    \n    plt.plot(fpr, tpr)\n\n    # ROC-curve of random model\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    \n    plt.ylim([0.0, 1.0])\n    plt.xlim([0.0, 1.0])\n    plt.grid(True)\n    \n    plt.show()","b641b333":"%%time\n\npredictions_valid = np.zeros((predicted_probabilities.shape[0],))\nprobabilities_valid = np.zeros(X.shape[0])\nfinal_predicted_probabilities = 0\nscore = 0\n\nX = predicted_probabilities[models.keys()]\ny = predicted_probabilities[TARGET]\n\nN_FOLD = 7\nkf = KFold(n_splits = N_FOLD, random_state = 99, shuffle = True)\n\n# Iterate through each fold\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X)):\n    X_train = X.iloc[train_idx]\n    X_valid = X.iloc[valid_idx]\n    y_train = y.iloc[train_idx]\n    y_valid = y.iloc[valid_idx] \n\n    model = LogisticRegression(C=0.55, solver='saga', penalty='elasticnet', l1_ratio=.15, max_iter=150, n_jobs=-1)\n    model.fit(X_train, y_train)\n    \n    # Mean of the predictions\n    final_predicted_probabilities += model.predict_proba(test_predicted_probabilities[models.keys()])[:,1] \/ N_FOLD\n    \n    # Out of Fold predictions\n    predictions_valid[valid_idx] = model.predict(X_valid)\n    probabilities_valid[valid_idx] = model.predict_proba(X_valid)[:,1]\n    fold_score = metrics.roc_auc_score(y_valid, predictions_valid[valid_idx])\n    print(f\"Fold {fold} | ROC-AUC: {fold_score:.3f}\")\n\n    score += fold_score \/ N_FOLD\n    \nprint(f\"Overall ROC-AUC: {score:.6f}\")","08f7a7e5":"fpr, tpr, thresholds = metrics.roc_curve(y, probabilities_valid)\nplot_roc_curve(fpr, tpr)\nplt.show()","ef1d7208":"import plotly.figure_factory as ff\nfig = ff.create_distplot([probabilities_valid], ['LogisticRegression'], bin_size=0.1, show_hist=False, show_rug=False)\nfig.show()","6d8eb247":"output = pd.DataFrame({'id': df_test.index,\n                        'claim': final_predicted_probabilities})\noutput.to_csv('submission.csv', index=False)","53aaa700":"### Feature importance","8cf12218":"# 1. Load data and first look","16aa44d1":"### Plot final AUC-ROC","6a43da0c":"# 2. Data preprocessing","f6bc7a48":"### Preprocess nan values","d5ee685f":"### Import libraries","71d24423":"## <center>Tabular Playground Series - Sep 2021<\/center>\n### <center>Stacking solution (LightGBM + CatBoost + XGBoost)<\/center>\n\nThis notebook contains full solution to building stacking pipeline + evaluation of predictions.  \nIn this competition we predict whether a customer made a claim upon an insurance policy.\n\n#### Dataset:\nThe dataset is used for this competition is synthetic (and generated using a CTGAN), but based on a real dataset. The original dataset deals with predicting whether a claim will be made on an insurance policy.\n* 'f1' - 'f118' continuous features\n* 'claim' - binary valued target, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim.","3ae458f1":"I test StandartScaler, RobustScaler and MinMaxScaler. And last one gives better score. If you have thoughts why, please tell in comments.","2e2e9240":"Idea taken from www.kaggle.com\/dlaststark\/tps-sep-single-xgboost-model  \n\nI have modified the choices using the following rationale:\n* Mean: normal distribution  \n* Median: unimodal and skewed  \n* Mode: all other cases  ","2f12ef1f":"# 5. Submit predictions","4a0e6d3b":"# 3. Model","c690cb7f":"### _If you find it useful please upvote_\n### _Thank you!_","bcd0daaf":"# 4. Evaluation","92c4223e":"Check if exist missing value","4d1a33a6":"Save the probabilities of predictions to a CSV file","707b75b2":"### Feature engineering"}}