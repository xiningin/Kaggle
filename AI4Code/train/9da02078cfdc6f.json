{"cell_type":{"45f04a97":"code","0e6a5623":"code","696825cd":"code","2b4d3046":"code","3931cf62":"code","baaacb8e":"code","fd0227e9":"code","9e5ab46d":"code","07f997a2":"code","02b89e03":"code","b1f6b623":"code","85ca8564":"code","f666deeb":"code","ce1b5e52":"code","4d3b2d83":"code","29741f58":"code","4d401439":"code","56d30760":"code","c19ad25d":"code","f30c46bb":"code","2ac442aa":"code","9472f634":"code","3fcff626":"code","d82b71d6":"code","8c449840":"code","c541a461":"code","1e4dbcb2":"code","6a2cb797":"code","12d2cad8":"code","6f2d2ba6":"code","25b5090e":"code","e90775fc":"code","94b4e79a":"code","f4330fc4":"code","5a8353ca":"code","c7b696c7":"code","65e9cec9":"code","16692411":"code","079eeb65":"code","0edc30ca":"code","48889719":"code","1d2d1351":"code","e04d5f99":"code","6aefe3c0":"code","b5730b6c":"code","d1f3747f":"code","7b92b067":"code","e7b2d94f":"code","3d8312d2":"code","8bb90436":"code","7c94534b":"code","bfcb4dbf":"code","beeaa75e":"code","3fde568d":"code","2ffb3b7f":"code","4a3f9431":"code","b19525bf":"code","6b457737":"code","a15f26be":"code","14f2aeea":"code","de61359a":"code","c9aa156f":"code","e3a91476":"code","7fa0217f":"code","163db6a9":"code","a2a9351c":"code","0bcc498c":"code","c7cd5947":"code","cab6b6c4":"code","a51168fa":"code","4df26fca":"code","fa99df84":"code","2efb2eaa":"code","9dc36ea5":"code","7b10236b":"code","375050b2":"code","b3e6763f":"code","9522cb02":"code","80ee4df3":"code","2e8fe326":"code","a160ba02":"code","e4a4c3d2":"code","15b09f8b":"code","c04847d8":"code","9027d6a3":"code","7893f8d0":"markdown","f3a901c8":"markdown","5b71507b":"markdown","db194d75":"markdown","d153ce2b":"markdown","42fd02bd":"markdown","5cdeeb3b":"markdown","0017d949":"markdown","1f160f1f":"markdown","b9f5d4ec":"markdown","87d9406e":"markdown","d6aa4266":"markdown","6c561c36":"markdown","fa0029ed":"markdown","a0dc9d4e":"markdown","8461f055":"markdown","b5158d77":"markdown","0a079751":"markdown","63678a98":"markdown","62f2ba62":"markdown","0a9b9af9":"markdown","c3a33965":"markdown","42a9d723":"markdown","20e89939":"markdown","4f6416a5":"markdown"},"source":{"45f04a97":"#GENERAL\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom keras.preprocessing import image\nimport skimage\nfrom skimage.feature import hessian_matrix, hessian_matrix_eigvals\nfrom scipy.ndimage.filters import convolve\nfrom skimage import data, io, filters\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN, LSTM, GlobalAveragePooling2D, SeparableConv2D,\\\nZeroPadding2D, Convolution2D, ZeroPadding2D,AveragePooling2D,Input, GlobalMaxPooling2D, Conv2DTranspose, Reshape\nfrom keras import models\nfrom keras import layers\nfrom keras import Input\nfrom keras.models import Model\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\nfrom keras.models import load_model\nfrom keras.regularizers import l1,l2,L1L2\nfrom tensorflow.keras import regularizers\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","0e6a5623":"Mars_Train_CSV = pd.read_csv(\"..\/input\/mars-surface-and-curiosity-image-set-nasa\/Mars Surface and Curiosity Image\/Train_CSV.csv\")\nMars_Test_CSV = pd.read_csv(\"..\/input\/mars-surface-and-curiosity-image-set-nasa\/Mars Surface and Curiosity Image\/Test_CSV.csv\")\nMars_Validation_CSV = pd.read_csv(\"..\/input\/mars-surface-and-curiosity-image-set-nasa\/Mars Surface and Curiosity Image\/Validation_CSV.csv\")","696825cd":"print(Mars_Train_CSV.head(-1))","2b4d3046":"print(Mars_Test_CSV.head(-1))","3931cf62":"print(Mars_Validation_CSV.head(-1))","baaacb8e":"def new_path_function(jpg_path,jpg_labels,new_jpg_list,new_label_list,splitting_string = \"calibrated\"):\n    \n    for image_path, path_label in zip(jpg_path,jpg_labels):\n        ID_pathing,exporting_type = image_path.split(splitting_string)\n        New_File_Path_Name = \"..\/input\/mars-surface-and-curiosity-image-set-nasa\/Mars Surface and Curiosity Image\/images\" + str(exporting_type)\n        new_jpg_list.append(New_File_Path_Name)\n        new_label_list.append(path_label)","fd0227e9":"New_JPG_Path_Train = []\nNew_Labels_Train = []\nsplitting_string = \"calibrated\"\n\nnew_path_function(Mars_Train_CSV.JPG,Mars_Train_CSV.LABELS,New_JPG_Path_Train,New_Labels_Train,splitting_string)","9e5ab46d":"New_JPG_Path_Test = []\nNew_Labels_Test = []\nsplitting_string = \"calibrated\"\n\nnew_path_function(Mars_Test_CSV.JPG,Mars_Test_CSV.LABELS,New_JPG_Path_Test,New_Labels_Test,splitting_string)","07f997a2":"New_JPG_Path_Validation = []\nNew_Labels_Validation = []\nsplitting_string = \"calibrated\"\n\nnew_path_function(Mars_Validation_CSV.JPG,Mars_Validation_CSV.LABELS,New_JPG_Path_Validation,New_Labels_Validation,splitting_string)","02b89e03":"Train_JPG_Series = pd.Series(New_JPG_Path_Train,name=\"JPG\").astype(str)\nTrain_Labels_Series = pd.Series(New_Labels_Train,name=\"CATEGORY\")","b1f6b623":"Test_JPG_Series = pd.Series(New_JPG_Path_Test,name=\"JPG\").astype(str)\nTest_Labels_Series = pd.Series(New_Labels_Test,name=\"CATEGORY\")","85ca8564":"Validation_JPG_Series = pd.Series(New_JPG_Path_Validation,name=\"JPG\").astype(str)\nValidation_Labels_Series = pd.Series(New_Labels_Validation,name=\"CATEGORY\")","f666deeb":"Main_Train_Data = pd.concat([Train_JPG_Series,Train_Labels_Series],axis=1)","ce1b5e52":"print(Main_Train_Data.head(-1))","4d3b2d83":"Main_Test_Data = pd.concat([Test_JPG_Series,Test_Labels_Series],axis=1)","29741f58":"print(Main_Test_Data.head(-1))","4d401439":"Main_Validation_Data = pd.concat([Validation_JPG_Series,Validation_Labels_Series],axis=1)","56d30760":"print(Main_Validation_Data.head(-1))","c19ad25d":"frame_list = [Main_Train_Data,Main_Test_Data,Main_Train_Data]\n\nMain_Mars_Data = pd.concat(frame_list)","f30c46bb":"print(Main_Mars_Data.head(-1))","2ac442aa":"Main_Mars_Data = Main_Mars_Data.sample(frac=1).reset_index(drop=True)","9472f634":"print(Main_Mars_Data.head(-1))","3fcff626":"def threshold_function(image_path):\n    \n    Picking_IMG = image_path\n    Picking_IMG = cv2.cvtColor(cv2.imread(Picking_IMG),cv2.COLOR_BGR2RGB)\n    _,Threshold_IMG = cv2.threshold(Picking_IMG,200,255,cv2.THRESH_BINARY_INV)\n    \n    plt.xlabel(Threshold_IMG.shape)\n    plt.ylabel(Threshold_IMG.size)\n    plt.imshow(Threshold_IMG)","d82b71d6":"def simple_vision(image_path):\n    \n    Picking_IMG = image_path\n    Picking_IMG = cv2.cvtColor(cv2.imread(Picking_IMG),cv2.COLOR_BGR2RGB)\n    \n    plt.xlabel(Picking_IMG.shape)\n    plt.ylabel(Picking_IMG.size)\n    plt.imshow(Picking_IMG)","8c449840":"def just_vision(image_path):\n    \n    plt.xlabel(image_path.shape)\n    plt.ylabel(image_path.size)\n    plt.imshow(image_path)","c541a461":"def just_threshold(image_path):\n    \n    _,threshold_IMG = cv2.threshold(image_path,220,255,cv2.THRESH_BINARY_INV)\n    \n    plt.xlabel(threshold_IMG.shape)\n    plt.ylabel(threshold_IMG.size)\n    plt.imshow(threshold_IMG)","1e4dbcb2":"def just_canny(image_path):\n    \n    Canny_Image = cv2.Canny(image_path,10,100)\n    \n    plt.xlabel(Canny_Image.shape)\n    plt.ylabel(Canny_Image.size)\n    plt.imshow(Canny_Image)","6a2cb797":"def just_drawing_contour(image_path):\n    \n    Canny_Image = cv2.Canny(image_path,10,100)\n    contour,_ = cv2.findContours(Canny_Image,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n    Drawing_Contour = cv2.drawContours(image_path,contour,-1,(255,0,0),2)\n    \n    plt.xlabel(Drawing_Contour.shape)\n    plt.ylabel(Drawing_Contour.size)\n    plt.imshow(Drawing_Contour)","12d2cad8":"simple_vision(Main_Mars_Data[\"JPG\"][3])","6f2d2ba6":"simple_vision(Main_Mars_Data[\"JPG\"][30])","25b5090e":"simple_vision(Main_Mars_Data[\"JPG\"][4560])","e90775fc":"figure,axis = plt.subplots(5,5,figsize=(10,10))\n\nfor indexing,operation in enumerate(axis.flat):\n    \n    Picking_Image = Main_Mars_Data[\"JPG\"][indexing]\n    Reading_Image = cv2.cvtColor(cv2.imread(Picking_Image),cv2.COLOR_BGR2RGB)\n    \n    operation.set_xlabel(Reading_Image.shape)\n    operation.set_ylabel(Reading_Image.size)\n    operation.set_title(Main_Mars_Data[\"CATEGORY\"][indexing])\n    operation.imshow(Reading_Image)\n    \nplt.tight_layout()\nplt.show()","94b4e79a":"threshold_function(Main_Mars_Data[\"JPG\"][3])","f4330fc4":"threshold_function(Main_Mars_Data[\"JPG\"][44])","5a8353ca":"threshold_function(Main_Mars_Data[\"JPG\"][400])","c7b696c7":"threshold_function(Main_Mars_Data[\"JPG\"][827])","65e9cec9":"Another_Mars_Path = Path(\"..\/input\/mars-surface-and-curiosity-image-set-nasa\/Mars Surface and Curiosity Image\/additional_images\")","16692411":"Another_JPG_List = list(Another_Mars_Path.glob(r\"*.jpg\"))","079eeb65":"Another_JPG_Series = pd.Series(Another_JPG_List,name=\"JPG\").astype(str)","0edc30ca":"print(Another_JPG_Series.head(-1))","48889719":"Another_JPG_Series = Another_JPG_Series[0:2000]","1d2d1351":"print(Another_JPG_Series.head(-1))","e04d5f99":"Transformed_X = []\n\nfor X_image in Another_JPG_Series:\n    \n    One_Image = cv2.cvtColor(cv2.imread(X_image),cv2.COLOR_BGR2RGB)\n    One_Image = cv2.resize(One_Image,(180,180))\n    One_Image = One_Image \/ 255.0\n    Transformed_X.append(One_Image)","6aefe3c0":"X_AE_Train = np.array(Transformed_X)","b5730b6c":"print(X_AE_Train.shape)","d1f3747f":"just_vision(X_AE_Train[0])","7b92b067":"just_vision(X_AE_Train[20])","e7b2d94f":"just_vision(X_AE_Train[100])","3d8312d2":"Reading_IMG = cv2.cvtColor(cv2.imread(Another_JPG_Series[0]),cv2.COLOR_BGR2RGB)\n\njust_canny(Reading_IMG)","8bb90436":"Reading_IMG = cv2.cvtColor(cv2.imread(Another_JPG_Series[200]),cv2.COLOR_BGR2RGB)\n\njust_canny(Reading_IMG)","7c94534b":"Reading_IMG = cv2.cvtColor(cv2.imread(Another_JPG_Series[600]),cv2.COLOR_BGR2RGB)\n\njust_canny(Reading_IMG)","bfcb4dbf":"Reading_IMG = cv2.cvtColor(cv2.imread(Another_JPG_Series[600]),cv2.COLOR_BGR2RGB)\n\njust_drawing_contour(Reading_IMG)","beeaa75e":"Reading_IMG = cv2.cvtColor(cv2.imread(Another_JPG_Series[1600]),cv2.COLOR_BGR2RGB)\n\njust_drawing_contour(Reading_IMG)","3fde568d":"X_Train,X_Test = train_test_split(Main_Mars_Data,train_size=0.9,random_state=42,shuffle=True)","2ffb3b7f":"print(X_Train.shape)\nprint(X_Test.shape)","4a3f9431":"Validation_Set = X_Train[6000:7917]","b19525bf":"X_Train = X_Train[0:2000]","6b457737":"Validation_Set = Validation_Set.reset_index()","a15f26be":"print(X_Train.shape)\nprint(X_Test.shape)\nprint(Validation_Set.shape)","14f2aeea":"print(X_Train.CATEGORY.value_counts())","de61359a":"Transformed_Y_Train = []\nTransformed_Y_Train_Labels = []\n\nfor Y_image, Y_labels in zip(X_Train.JPG,X_Train.CATEGORY):\n    \n    Y_image = cv2.cvtColor(cv2.imread(Y_image),cv2.COLOR_BGR2RGB)\n    Y_image = cv2.resize(Y_image,(180,180))\n    Y_image = Y_image \/ 255.0\n    Transformed_Y_Train.append(Y_image)\n    Transformed_Y_Train_Labels.append(Y_labels)","c9aa156f":"Y_S_Train_Img = np.array(Transformed_Y_Train)\nY_S_Train_Labels = to_categorical(Transformed_Y_Train_Labels)","e3a91476":"print(Y_S_Train_Img.shape)\nprint(Y_S_Train_Labels.shape)\nprint(X_AE_Train.shape)","7fa0217f":"Early_Stopper = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3,mode=\"min\")\nCheckpoint_Model = tf.keras.callbacks.ModelCheckpoint(monitor=\"val_accuracy\",\n                                                      save_best_only=True,\n                                                      save_weights_only=True,\n                                                      filepath=\".\/modelcheck\")\nReduce_Model = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"accuracy\",\n                                                   factor=0.1,\n                                                   patience=7)","163db6a9":"Input_Layer = tf.keras.Input(shape=(180,180,3))\n#\nx = Conv2D(32,(3,3),activation=\"relu\",padding=\"same\")(Input_Layer)\nx = MaxPooling2D((2,2))(x)\nx = Conv2D(64,(3,3),activation=\"relu\",padding=\"same\")(x)\nx = MaxPooling2D((2,2))(x)\nx = Conv2D(128,(2,2),activation=\"relu\",padding=\"same\")(x)\nx = MaxPooling2D((2,2))(x)\nx = Conv2D(256,(2,2),activation=\"relu\",padding=\"same\")(x)\nx = GlobalMaxPooling2D()(x)\nx = Dense(128,activation=\"relu\")(x)\nx = Dropout(0.5)(x)\nclass_prediction_layer = Dense(25,activation=\"softmax\",name=\"CLASS_PREDICTION\")(x)\n#\nencoder = Dense(128,activation=\"relu\")(Input_Layer)\nencoder = Dense(64,activation=\"relu\")(encoder)\nencoder = Dense(32,activation=\"relu\")(encoder)\n#\ndecoder = Dense(64,input_shape=[32],activation=\"relu\")(encoder)\ndecoder = Dense(128,activation=\"relu\")(decoder)\nae_output = Dense(3,activation=\"sigmoid\",name=\"AE_OUTPUT\")(decoder)","a2a9351c":"Configure_Model = Model(Input_Layer,[class_prediction_layer,ae_output])","0bcc498c":"print(Configure_Model.summary())","c7cd5947":"plot_model(Configure_Model, to_file='Conf.png', show_shapes=True, show_layer_names=True)","cab6b6c4":"Configure_Model.compile(optimizer=\"adam\",loss={\"CLASS_PREDICTION\":\"categorical_crossentropy\",\n                                              \"AE_OUTPUT\":\"binary_crossentropy\"},metrics=[\"accuracy\"])","a51168fa":"Model_Configure_Total = Configure_Model.fit(Y_S_Train_Img,\n                    [Y_S_Train_Labels,X_AE_Train],\n                    epochs=50,\n                    batch_size=64,\n                    callbacks=[Early_Stopper,Checkpoint_Model,Reduce_Model])","4df26fca":"plt.style.use(\"dark_background\")","fa99df84":"Grap_Data = pd.DataFrame(Model_Configure_Total.history)\nGrap_Data.plot()","2efb2eaa":"plt.plot(Model_Configure_Total.history[\"CLASS_PREDICTION_loss\"])\nplt.plot(Model_Configure_Total.history[\"AE_OUTPUT_loss\"])\nplt.ylabel(\"ACCURACY\")\nplt.legend()\nplt.show()","9dc36ea5":"plt.plot(Model_Configure_Total.history[\"CLASS_PREDICTION_accuracy\"])\nplt.plot(Model_Configure_Total.history[\"AE_OUTPUT_accuracy\"])\nplt.ylabel(\"ACCURACY\")\nplt.legend()\nplt.show()","7b10236b":"Test_Prediction = Configure_Model(Y_S_Train_Img[0:10])","375050b2":"print(type(Test_Prediction))","b3e6763f":"print(Test_Prediction[0])","9522cb02":"print(Test_Prediction[0].argmax(axis=-1)) # this is first class(label) prediction","80ee4df3":"fig, axes = plt.subplots(nrows=2,\n                         ncols=5,\n                         figsize=(20, 20),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(Transformed_Y_Train[i])\n    ax.set_title(f\"PREDICTION:{Test_Prediction[0].argmax(axis=-1)[i]}\")\nplt.tight_layout()\nplt.show()","2e8fe326":"Transformed_Y_Test = []\nTransformed_Y_Test_Labels = []\n\nfor Y_image, Y_labels in zip(X_Test.JPG,X_Test.CATEGORY):\n    \n    Y_image = cv2.cvtColor(cv2.imread(Y_image),cv2.COLOR_BGR2RGB)\n    Y_image = cv2.resize(Y_image,(180,180))\n    Y_image = Y_image \/ 255.0\n    Transformed_Y_Test.append(Y_image)\n    Transformed_Y_Test_Labels.append(Y_labels)","a160ba02":"Y_S_Test_Img = np.array(Transformed_Y_Test)\nY_S_Test_Labels = to_categorical(Transformed_Y_Test_Labels)","e4a4c3d2":"Other_Test_Prediction = Configure_Model.predict(Y_S_Test_Img[0:10])","15b09f8b":"fig, axes = plt.subplots(nrows=2,\n                         ncols=5,\n                         figsize=(20, 20),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(Transformed_Y_Test[i])\n    ax.set_title(f\"PREDICTION:{Other_Test_Prediction[0].argmax(axis=-1)[i]}\")\nplt.tight_layout()\nplt.show()","c04847d8":"print(\"NORMAL\")\nplt.imshow(X_AE_Train[0])\nplt.show()\nprint(\"Auto Encoder\")\nplt.imshow(Test_Prediction[1][1])","9027d6a3":"print(\"NORMAL\")\nplt.imshow(Y_S_Test_Img[0])\nplt.show()\nprint(\"Auto Encoder\")\nplt.imshow(Other_Test_Prediction[1][1])","7893f8d0":"#### CALLBACK","f3a901c8":"# MODEL","5b71507b":"# PACKAGES AND LIBRARIES","db194d75":"# HISTORY\n\n#### Mars surface image (Curiosity rover) labeled data set\n-----------------------------------------------------\n\nAuthors: Alice Stanboli and Kiri L. Wagstaff\nContact: kiri.l.wagstaff@jpl.nasa.gov\n\nThis data set consists of 6691 images that were collected by the Mars\nScience Laboratory (MSL, Curosity) rover by three instruments (Mastcam\nRight eye, Mastcam Left eye, and MAHLI).  These images are the\n\"browse\" version of each original data product, not full resolution.\nThey are roughly 256x256 pixels each.  Full-size images can be\nobtained from the PDS at https:\/\/pds-imaging.jpl.nasa.gov\/search\/ .\n\nWe divided the MSL images into train, validation, and test data sets\naccording to their sol (Martian day) of acquisition.  This strategy\nwas chosen to model how the system will be used operationally with an\nimage archive that grows over time.  The images were collected from\nsols 3 to 1060 (August 2012 to July 2015).  The exact\ntrain\/validation\/test splits are given in individual files.","d153ce2b":"# IMAGE PROCESS FOR SUPERVISED","42fd02bd":"# PATH, LABEL, TRANSFORMATION FOR SUPERVISED","5cdeeb3b":"# VISION FOR UNSUPERVISED PROCESS","0017d949":"# VISION FOR SUPERVISED PROCESS","1f160f1f":"#### NEW PATH PROCESS","b9f5d4ec":"#### TO DATAFRAME","87d9406e":"# VISION FUNCTION","d6aa4266":"#### TO SHUFFLE","6c561c36":"* 0       apxs\n* 1       apxs cal target\n* 2       chemcam cal target\n* 3       chemin inlet open\n* 4       drill\n* 5       drill holes\n* 6       drt front\n* 7       drt side\n* 8       ground\n* 9       horizon\n* 10      inlet\n* 11      mahli\n* 12      mahli cal target\n* 13      mastcam\n* 14      mastcam cal target\n* 15      observation tray\n* 16      portion box\n* 17      portion tube\n* 18      portion tube opening\n* 19      rems uv sensor\n* 20      rover rear deck\n* 21      scoop\n* 22      sun\n* 23      turret\n* 24      wheel","fa0029ed":"#### BASIC PREDICTION","a0dc9d4e":"#### AUTO-ENCODER OUTPUT","8461f055":"#### THRESHOLD","b5158d77":"#### TO SERIES","0a079751":"# PATH, LABEL, TRANSFORMATION FOR UNSUPERVISED","63678a98":"#### STRUCTURE","62f2ba62":"#### MAIN CSV","0a9b9af9":"#### CHECKING","c3a33965":"#### NON-SEEN PREDICTION","42a9d723":"#### SPLITTING","20e89939":"#### TO MERGE","4f6416a5":"#### SIMPLE"}}