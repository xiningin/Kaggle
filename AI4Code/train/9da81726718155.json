{"cell_type":{"eee3d108":"code","c7260596":"code","61303394":"code","9e4b1687":"code","d835e45c":"code","74c6f6ca":"code","a84aa8f8":"code","cf53d3ef":"code","7119da3b":"code","7e821bad":"code","d8fb59da":"code","d1de4491":"code","5a101aaa":"code","552efc12":"code","3c456242":"code","6db7b23f":"code","4bf3010e":"code","fd7d5d7e":"code","8b42bdf5":"code","8dbb9c45":"code","24db0c08":"code","14784e19":"code","f1834ff1":"code","7e002f7c":"code","240e69e3":"code","032962bc":"code","0b65aa4b":"code","207a66be":"code","8cbd32bc":"code","c3fdbe38":"code","60069aaa":"code","01f2d202":"code","4cb8d904":"code","f81ff4db":"code","ecf02f3e":"code","cfb0bd89":"code","09ec47f6":"code","7440733c":"code","bb5424f4":"code","a8325398":"code","909d9839":"code","cae8d5b5":"code","77cbfaf4":"code","4f1d0b8c":"code","2fe64bdf":"code","d2fc348d":"code","3ee08c7a":"code","52fe98ff":"code","643899a9":"code","4032b95e":"code","b8a03e31":"code","3161b6b5":"code","631136b5":"code","61cc1db0":"code","9307be5b":"code","a3cd28fa":"code","9f97cc2c":"code","787bbd20":"code","3738ae7c":"code","e9205e1b":"code","dbb9bcfc":"code","d793daae":"code","9f452900":"code","1b5a8d89":"code","1f8bd351":"code","69674fdd":"code","41713eed":"code","2d9944cd":"code","3145f6c3":"code","a51c6b80":"code","843bf215":"code","7f637d12":"code","c861f0ce":"code","70512885":"code","3df5ed4e":"code","80d043f1":"code","63b42873":"code","865eeddf":"code","2fd6d0a9":"code","dacc61f4":"code","bf3605d6":"code","7c076a29":"code","d9f07f33":"code","76a76217":"code","d05258b5":"code","deacdb84":"code","9efb19c7":"code","d424d00b":"code","599df62b":"code","1132fb16":"code","d548df24":"code","4e798602":"code","e0f41fcb":"code","2cdebecc":"code","f2af4f64":"code","98a27acb":"code","6b1c19ea":"code","240c0c7b":"code","d8d1d66b":"code","8859228c":"code","7be00d6a":"code","5ee135aa":"code","309c432e":"code","175a6bc5":"code","8f57254b":"code","f9b85cab":"code","1d45c754":"code","7518a85c":"code","c4830a91":"code","d288eba0":"code","446d7a28":"code","ad2451e5":"code","4fbfe3ab":"code","e18a6e0f":"code","fda5ae88":"code","72b9ddc5":"code","b940991d":"code","abf2b224":"code","b2f8ea49":"code","ea88b77e":"code","c282e369":"code","526095f2":"code","d11c601a":"code","78383e4b":"code","0b5d5971":"code","af6d5fde":"code","57349918":"code","fcbf406e":"code","581239e1":"code","e3fa637d":"code","c191156e":"code","6abddcde":"code","024c8671":"code","41c064a8":"code","bd711454":"code","645a7577":"code","3c980486":"code","4a538e4e":"code","c56f92e9":"code","73216ffa":"code","f5654690":"code","76dadd7c":"code","98fa59b0":"code","105d44bb":"code","05d885d0":"code","ccf7b291":"code","aa803d61":"code","fae7987c":"code","8617705f":"code","3f4ff3b2":"code","38243dbb":"code","6854d5b5":"code","9a456ae6":"code","b0b8f0b1":"code","47adcdbf":"code","7bc8e9d5":"code","1b2abf80":"code","9aca1f3f":"code","26713ef4":"code","8b5efe0d":"code","00c6720a":"code","7db84140":"code","78b0d474":"code","16648323":"code","2b40e9ab":"code","77b5d77e":"code","387a57b8":"code","4f39a541":"code","b95476a0":"code","f5d9dd00":"code","7dd1d39e":"code","7a64939c":"code","62d47b78":"code","58933646":"code","86e196bd":"code","44ded32a":"code","cdb7d82b":"code","c959d2f8":"code","133c9795":"code","76adf5d8":"markdown","97f210e2":"markdown","0823b033":"markdown","444962f5":"markdown","1f98efea":"markdown","d27af62f":"markdown","795b92f4":"markdown","0e5a1bc7":"markdown","f0c80218":"markdown","34c7bcbd":"markdown","72838a3e":"markdown","05771836":"markdown","94f2ca42":"markdown","eb8f7769":"markdown","9a4d4585":"markdown","74fa86df":"markdown","13de370c":"markdown","b5eaeb54":"markdown","bfc18a80":"markdown","a98dba9a":"markdown","5209cc54":"markdown","f4f88de7":"markdown","434ce2e3":"markdown","4159997f":"markdown","f33400a7":"markdown","383bf91a":"markdown","48cd6b78":"markdown","339998e0":"markdown","b30fb8a5":"markdown","df4ed13b":"markdown","b9dcec4a":"markdown","22eb58c7":"markdown","2de35bc1":"markdown","6a8e9c08":"markdown","97d9e2e1":"markdown","a853443e":"markdown","ee8bb40c":"markdown","f6c5ba8e":"markdown","39a0a51e":"markdown","10d191c5":"markdown","42bf71a5":"markdown","6ac68285":"markdown","d7f39a48":"markdown","9ac9e6d2":"markdown","39fa36b1":"markdown","66610837":"markdown","85274d17":"markdown","c88ba020":"markdown","3abe3f6d":"markdown","61ee7cb2":"markdown","aed160f5":"markdown","a9ff7592":"markdown","2d3e2488":"markdown","46a14e6b":"markdown","e26de459":"markdown","27e807e6":"markdown","6c1a2316":"markdown","f5eb1d94":"markdown","f71856af":"markdown","36857623":"markdown","91cb9d4c":"markdown","499a97f8":"markdown","ec83ca87":"markdown","61a064ea":"markdown","5135f6c5":"markdown","851e22f6":"markdown","68db7c45":"markdown","d3001d05":"markdown"},"source":{"eee3d108":"import numpy as np\nimport pandas as pd","c7260596":"loan_data_backup = pd.read_csv('\/kaggle\/input\/loan-data-2007-2014\/loan_data_2007_2014\/loan_data_2007_2014.csv')","61303394":"loan_data = loan_data_backup.copy()","9e4b1687":"loan_data.head()","d835e45c":"pd.options.display.max_columns = None\npd.options.display.max_rows = None\n# Sets the pandas dataframe options to display all columns\/ rows.","74c6f6ca":"loan_data.head()","a84aa8f8":"loan_data.columns.values\n# Displays all column names.","cf53d3ef":"loan_data.info()\n# Displays column names, complete (non-missing) cases per column, and datatype per column.","7119da3b":"loan_data['emp_length'].unique()","7e821bad":"loan_data['emp_length_int'] = loan_data['emp_length'].str.replace('\\+ years', '')\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('< 1 year', str(0))\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('n\/a', str(0))\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('years', '')\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('year', '')","d8fb59da":"type(loan_data['emp_length_int'][0])","d1de4491":"loan_data['emp_length_int'] = pd.to_numeric(loan_data['emp_length_int'])","5a101aaa":"type(loan_data['emp_length_int'][0])","552efc12":"loan_data['term'].unique()","3c456242":"loan_data['term_int'] = loan_data['term'].str.replace('\\+ months', '')\nloan_data['term_int'] = loan_data['term'].str.replace('months', '')","6db7b23f":"type(loan_data['term_int'][0])","4bf3010e":"loan_data['term_int'] = pd.to_numeric(loan_data['term_int'])","fd7d5d7e":"loan_data['earliest_cr_line'].head()","8b42bdf5":"loan_data['earliest_cr_line_date'] = pd.to_datetime(loan_data['earliest_cr_line'], format = '%b-%y')\nloan_data['earliest_cr_line_date'] = loan_data['earliest_cr_line_date'].mask(loan_data['earliest_cr_line_date'].dt.year > 2017, \n                                         loan_data['earliest_cr_line_date'] - pd.offsets.DateOffset(years=100))","8dbb9c45":"type(loan_data['earliest_cr_line_date'][0])","24db0c08":"loan_data['mths_since_earliest_cr_line'] = round((pd.to_datetime('2017-12-01') - loan_data['earliest_cr_line_date'])\/np.timedelta64(1, 'M'))","14784e19":"loan_data['mths_since_earliest_cr_line'].describe()","f1834ff1":"loan_data.loc[:, ['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']][loan_data['mths_since_earliest_cr_line'] < 0]","7e002f7c":"loan_data['issue_d'].head()","240e69e3":"loan_data['issue_date'] = pd.to_datetime(loan_data['issue_d'], format = '%b-%y')\n#loan_data['issue_date'] = loan_data['issue_date'].mask(loan_data['issue_date'].dt.year > 2017, loan_data['issue_date'] - pd.offsets.DateOffset(years=100))","032962bc":"loan_data['issue_date'].head()","0b65aa4b":"loan_data['mths_since_issue_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - loan_data['issue_date']) \/ np.timedelta64(1, 'M')))","207a66be":"loan_data['mths_since_issue_d'].describe()","8cbd32bc":"loan_data_dummies = [pd.get_dummies(loan_data['grade'], prefix= 'grade', prefix_sep = \":\"),\n                     pd.get_dummies(loan_data['sub_grade'], prefix= 'sub_grade', prefix_sep = \":\"),\n                     pd.get_dummies(loan_data['home_ownership'], prefix= 'home_ownership', prefix_sep = \":\"),\n                     pd.get_dummies(loan_data['verification_status'], prefix= 'verification_status', prefix_sep = \":\"),\n                     pd.get_dummies(loan_data['loan_status'], prefix= 'loan_status', prefix_sep = \":\"),\n                     pd.get_dummies(loan_data['purpose'], prefix= 'purpose', prefix_sep = \":\"),\n                     pd.get_dummies(loan_data['addr_state'], prefix= 'addr_state', prefix_sep = \":\"),\n                     pd.get_dummies(loan_data['initial_list_status'], prefix= 'initial_list_status', prefix_sep = \":\")]","c3fdbe38":"loan_data_dummies = pd.concat(loan_data_dummies, axis = 1)","60069aaa":"type(loan_data_dummies)","01f2d202":"loan_data = pd.concat([loan_data, loan_data_dummies], axis = 1)","4cb8d904":"loan_data.columns","f81ff4db":"loan_data.isnull().sum()","ecf02f3e":"loan_data['total_rev_hi_lim'].fillna(loan_data['funded_amnt'], inplace = True)\nloan_data['annual_inc'].fillna(loan_data['annual_inc'].mean(), inplace=True)\nloan_data['mths_since_earliest_cr_line'].fillna(0, inplace = True) \nloan_data['acc_now_delinq'].fillna(0, inplace = True) \nloan_data['total_acc'].fillna(0, inplace = True) \nloan_data['pub_rec'].fillna(0, inplace = True) \nloan_data['open_acc'].fillna(0, inplace = True) \nloan_data['inq_last_6mths'].fillna(0, inplace = True) \nloan_data['delinq_2yrs'].fillna(0, inplace = True) \nloan_data['emp_length_int'].fillna(0, inplace = True) ","cfb0bd89":"loan_data['loan_status'].unique()","09ec47f6":"loan_data['loan_status'].value_counts()\/loan_data['loan_status'].count()","7440733c":"loan_data['good_bad'] = np.where(loan_data['loan_status'].isin(['Charged Off', 'Default',\n                                                                   'Does not meet the credit policy. Status:Charged Off',\n                                                                   'Late (31-120 days)']), 0, 1)","bb5424f4":"loan_data['good_bad'].value_counts()","a8325398":"from sklearn.model_selection import train_test_split","909d9839":"loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'], test_size = 0.2, random_state = 42)","cae8d5b5":"data_cleaner = [loan_data_inputs_train, loan_data_inputs_test]","77cbfaf4":"loan_data_inputs_train['grade'].unique()","4f1d0b8c":"df1 = pd.concat([loan_data_inputs_train['grade'], loan_data_targets_train], axis = 1)","2fe64bdf":"df1.head()","d2fc348d":"df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count()","3ee08c7a":"df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()","52fe98ff":"df1 = pd.concat([df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count(),\n                 df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()], axis = 1)","643899a9":"df1.head()","4032b95e":"df1 = df1.iloc[: , [0, 1, 3]]","b8a03e31":"df1.columns = [df1.columns.values[0], 'n_obs', 'prop_good']","3161b6b5":"df1['prop_n_obs'] = df1['n_obs'] \/ df1['n_obs'].sum()\ndf1['n_good'] = df1['prop_good'] * df1['n_obs']\ndf1['n_bad'] = (1 - df1['prop_good']) * df1['n_obs']","631136b5":"df1.head()","61cc1db0":"df1['prop_n_good'] = df1['n_good'] \/ df1['n_good'].sum()\ndf1['prop_n_bad'] = df1['n_bad'] \/ df1['n_bad'].sum()","9307be5b":"df1.head()","a3cd28fa":"df1['WoE'] = np.log(df1['prop_n_good'] \/ df1['prop_n_bad'])","9f97cc2c":"df1.head()","787bbd20":"df1 = df1.sort_values(['WoE'])\ndf1 = df1.reset_index(drop = True)","3738ae7c":"df1.head()","e9205e1b":"df1['diff_prop_good'] = df1['prop_good'].diff().abs()\ndf1['diff_WoE'] = df1['WoE'].diff().abs()","dbb9bcfc":"df1","d793daae":"df1['IV'] = (df1['prop_n_good'] - df1['prop_n_bad']) * df1['WoE']\ndf1['IV'] = df1['IV'].sum()","9f452900":"df1","1b5a8d89":"def woe_discrete(df, discrete_variable_name, good_bad_variable_df):\n    df = pd.concat([df[discrete_variable_name], good_bad_variable_df], axis = 1)\n    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n    df = df.iloc[: , [0, 1, 3]]\n    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n    df['prop_n_obs'] = df['n_obs'] \/ df['n_obs'].sum()\n    df['n_good'] = df['prop_good'] * df['n_obs']\n    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n    df['prop_n_good'] = df['n_good'] \/ df['n_good'].sum()\n    df['prop_n_bad'] = df['n_bad'] \/ df['n_bad'].sum()\n    df['WoE'] = np.log(df['prop_n_good'] \/ df['prop_n_bad'])\n    df = df.sort_values(['WoE'])\n    df = df.reset_index(drop = True)\n    df['diff_prop_good'] = df['prop_good'].diff().abs()\n    df['diff_WoE'] = df['WoE'].diff().abs()\n    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n    df['IV'] = df['IV'].sum()\n    return(df)","1f8bd351":"df_temp = woe_discrete(loan_data_inputs_train, 'grade', loan_data_targets_train)","69674fdd":"df_temp","41713eed":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","2d9944cd":"def plot_by_woe(df_WOE, rotation_of_xaxis_labels = 90):\n    # reading the first column from the WOE dataset, and coercing to string\n    x = np.array(df_WOE.iloc[:, 0].apply(str))\n    y = df_WOE['WoE']\n    plt.figure(figsize = (18, 6))\n    plt.plot(x, y, marker = 'o', linestyle = '--', color = 'k')\n    plt.xlabel(df_WOE.columns[0])\n    plt.ylabel('Weight of Evidence')\n    plt.title('Weight of Evidence by' + df_WOE.columns[0])\n    plt.xticks(rotation = rotation_of_xaxis_labels)","3145f6c3":"plot_by_woe(df_temp)","a51c6b80":"df_temp = woe_discrete(loan_data_inputs_train, 'home_ownership', loan_data_targets_train)\ndf_temp","843bf215":"plot_by_woe(df_temp)","7f637d12":"for dataset in data_cleaner:  \n    dataset['home_ownership:RENT_OTHER_ANY_NONE'] = sum([dataset['home_ownership:RENT'], \n                                                            dataset['home_ownership:OTHER'], \n                                                            dataset['home_ownership:ANY'],\n                                                            dataset['home_ownership:NONE']])","c861f0ce":"loan_data_inputs_train['addr_state'].unique()","70512885":"df_temp = woe_discrete(loan_data_inputs_train, 'addr_state', loan_data_targets_train )\ndf_temp","3df5ed4e":"plot_by_woe(df_temp)","80d043f1":"for dataset in data_cleaner: \n    if ['addr_state:ND'] in dataset.columns.values:\n        pass\n    else:\n        dataset['addr_state:ND'] = 0","63b42873":"plot_by_woe(df_temp.iloc[2: -2, :])","865eeddf":"plot_by_woe(df_temp.iloc[6: -6, :])","2fd6d0a9":"for dataset in data_cleaner: \n    dataset['addr_state:NE_IA_NV_FL_HI_AL'] = sum([dataset['addr_state:NE'], dataset['addr_state:IA'],\n                                                     dataset['addr_state:NV'], dataset['addr_state:FL'], \n                                                     dataset['addr_state:HI'], dataset['addr_state:AL']])\n    \n    dataset['addr_state:NM_VA'] = sum([dataset['addr_state:NM'], dataset['addr_state:VA']])\n    \n    dataset['addr_state:OK_TN_MO_LA_MD_NC'] = sum([dataset['addr_state:OK'], dataset['addr_state:TN'],\n                                                     dataset['addr_state:MO'], dataset['addr_state:LA'], \n                                                     dataset['addr_state:MD'], dataset['addr_state:NC']])\n    \n    dataset['addr_state:UT_KY_AZ_NJ'] = sum([dataset['addr_state:UT'], dataset['addr_state:KY'],\n                                                     dataset['addr_state:AZ'], dataset['addr_state:NJ']])\n    \n    dataset['addr_state:AR_MI_PA_OH_MN']= sum([dataset['addr_state:AR'], dataset['addr_state:MI'],\n                                                  dataset['addr_state:PA'], dataset['addr_state:OH'], \n                                                  dataset['addr_state:MN']])\n    \n    dataset['addr_state:RI_MA_DE_SD_IN']= sum([dataset['addr_state:RI'], dataset['addr_state:MA'],\n                                                  dataset['addr_state:DE'], dataset['addr_state:SD'],\n                                                  dataset['addr_state:IN']])\n    \n    dataset['addr_state:GA_WA_OR'] = sum([dataset['addr_state:GA'], dataset['addr_state:WA'], \n                                          dataset['addr_state:OR']])\n    \n    dataset['addr_state:WI_MT'] = sum([dataset['addr_state:WI'], dataset['addr_state:MT']])\n    \n    dataset['addr_state:IL_CT'] = sum([dataset['addr_state:IL'], dataset['addr_state:CT']])\n    \n    dataset['addr_state:KS_SC_CO_VT_AK_MS'] = sum([dataset['addr_state:KS'], dataset['addr_state:SC'],\n                                                     dataset['addr_state:CO'], dataset['addr_state:VT'], \n                                                     dataset['addr_state:AK'], dataset['addr_state:MS']])\n    \n    dataset['addr_state:WV_NH_WY_DC_ME_ID'] = sum([dataset['addr_state:WV'], dataset['addr_state:NH'],\n                                                     dataset['addr_state:WY'], dataset['addr_state:DC'], \n                                                     dataset['addr_state:ME'], dataset['addr_state:ID']])","dacc61f4":"loan_data_inputs_train['verification_status'].unique()","bf3605d6":"df_temp = woe_discrete(loan_data_inputs_train, 'verification_status', loan_data_targets_train)","7c076a29":"df_temp","d9f07f33":"plot_by_woe(df_temp)","76a76217":"loan_data_inputs_train['purpose'].unique()","d05258b5":"df_temp = woe_discrete(loan_data_inputs_train, 'purpose', loan_data_targets_train)","deacdb84":"df_temp","9efb19c7":"plot_by_woe(df_temp)","d424d00b":"#dataset['purpose:debt_consolidation']\n#dataset['purpose:credit_card']\nfor dataset in data_cleaner: \n    dataset['purpose:educ__sm_b__wedd__ren_en__mov__house'] = sum([dataset['purpose:educational'], \n                                                                       dataset['purpose:small_business'],\n                                                                       dataset['purpose:wedding'], \n                                                                       dataset['purpose:renewable_energy'],\n                                                                       dataset['purpose:moving'], \n                                                                       dataset['purpose:house']])\n    dataset['purpose:oth__med__vacation'] = sum([dataset['purpose:other'], \n                                                     dataset['purpose:medical'],\n                                                     dataset['purpose:vacation']])\n    dataset['purpose:major_purch__car__home_impr'] = sum([dataset['purpose:major_purchase'],\n                                                              dataset['purpose:car'],\n                                                              dataset['purpose:home_improvement']])","599df62b":"loan_data_inputs_train['initial_list_status'].unique()","1132fb16":"df_temp = woe_discrete(loan_data_inputs_train, 'initial_list_status', loan_data_targets_train)","d548df24":"df_temp","4e798602":"plot_by_woe(df_temp)","e0f41fcb":"def woe_ordered_continuous(df, discrete_variabe_name, good_bad_variable_df):\n    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)\n    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n    df = df.iloc[:, [0, 1, 3]]\n    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n    df['prop_n_obs'] = df['n_obs'] \/ df['n_obs'].sum()\n    df['n_good'] = df['prop_good'] * df['n_obs']\n    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n    df['prop_n_good'] = df['n_good'] \/ df['n_good'].sum()\n    df['prop_n_bad'] = df['n_bad'] \/ df['n_bad'].sum()\n    df['WoE'] = np.log(df['prop_n_good'] \/ df['prop_n_bad'])\n    #df = df.sort_values(['WoE'])\n    #df = df.reset_index(drop = True)\n    df['diff_prop_good'] = df['prop_good'].diff().abs()\n    df['diff_WoE'] = df['WoE'].diff().abs()\n    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n    df['IV'] = df['IV'].sum()\n    return df","2cdebecc":"loan_data_inputs_train['term_int'].unique()","f2af4f64":"df_temp = woe_ordered_continuous(loan_data_inputs_train, 'term_int', loan_data_targets_train)\ndf_temp","98a27acb":"plot_by_woe(df_temp)","6b1c19ea":"for dataset in data_cleaner:\n    dataset['term:36'] = np.where((dataset['term_int'] == 36), 1, 0)\n    dataset['term:60'] = np.where((dataset['term_int'] == 60), 1, 0)","240c0c7b":"loan_data_inputs_train['emp_length_int'].unique()","d8d1d66b":"df_temp = woe_ordered_continuous(loan_data_inputs_train, 'emp_length_int', loan_data_targets_train)\ndf_temp","8859228c":"plot_by_woe(df_temp)","7be00d6a":"for dataset in data_cleaner: \n    dataset['emp_length:0'] = np.where(dataset['emp_length_int'].isin([0]), 1, 0)\n    dataset['emp_length:1'] = np.where(dataset['emp_length_int'].isin([1]), 1, 0)\n    dataset['emp_length:2-4'] = np.where(dataset['emp_length_int'].isin(range(2, 5)), 1, 0)\n    dataset['emp_length:5-6'] = np.where(dataset['emp_length_int'].isin(range(5, 7)), 1, 0)\n    dataset['emp_length:7-9'] = np.where(dataset['emp_length_int'].isin(range(7, 10)), 1, 0)\n    dataset['emp_length:10'] = np.where(dataset['emp_length_int'].isin([10]), 1, 0)","5ee135aa":"loan_data_inputs_train['mths_since_issue_d'].unique()","309c432e":"loan_data_inputs_train['mths_since_issue_factor'] = pd.cut(loan_data_inputs_train['mths_since_issue_d'], 50)","175a6bc5":"loan_data_inputs_train['mths_since_issue_factor'].unique()","8f57254b":"df_temp = woe_ordered_continuous(loan_data_inputs_train, 'mths_since_issue_factor', loan_data_targets_train)\ndf_temp","f9b85cab":"plot_by_woe(df_temp)","1d45c754":"plot_by_woe(df_temp.iloc[3:, :])","7518a85c":"for dataset in data_cleaner: \n    dataset['mths_since_issue_d:<38'] = np.where(dataset['mths_since_issue_d'].isin(range(38)), 1, 0)\n    dataset['mths_since_issue_d:38-39'] = np.where(dataset['mths_since_issue_d'].isin(range(38, 40)), 1, 0)\n    dataset['mths_since_issue_d:40-41'] = np.where(dataset['mths_since_issue_d'].isin(range(40, 42)), 1, 0)\n    dataset['mths_since_issue_d:42-48'] = np.where(dataset['mths_since_issue_d'].isin(range(42, 49)), 1, 0)\n    dataset['mths_since_issue_d:49-52'] = np.where(dataset['mths_since_issue_d'].isin(range(49, 53)), 1, 0)\n    dataset['mths_since_issue_d:53-64'] = np.where(dataset['mths_since_issue_d'].isin(range(53, 65)), 1, 0)\n    dataset['mths_since_issue_d:65-84'] = np.where(dataset['mths_since_issue_d'].isin(range(65, 85)), 1, 0)\n    dataset['mths_since_issue_d:>84'] = np.where(dataset['mths_since_issue_d'].isin(range(85, int(dataset['mths_since_issue_d'].max()))), 1, 0)","c4830a91":"loan_data_inputs_train['int_rate'].unique()","d288eba0":"loan_data_inputs_train['int_rate_factor'] = pd.cut(loan_data_inputs_train['int_rate'], 50)","446d7a28":"df_temp = woe_ordered_continuous(loan_data_inputs_train, 'int_rate_factor', loan_data_targets_train)\ndf_temp","ad2451e5":"plot_by_woe(df_temp)","4fbfe3ab":"for dataset in data_cleaner: \n    dataset['int_rate:<9.548'] = np.where((dataset['int_rate'] <= 9.548), 1, 0)\n    dataset['int_rate:9.548-12.025'] = np.where((dataset['int_rate'] > 9.548) & (dataset['int_rate'] <= 12.025), 1, 0)\n    dataset['int_rate:12.025-15.74'] = np.where((dataset['int_rate'] > 12.025) & (dataset['int_rate'] <= 15.74), 1, 0)\n    dataset['int_rate:15.74-20.281'] = np.where((dataset['int_rate'] > 15.74) & (dataset['int_rate'] <= 20.281), 1, 0)\n    dataset['int_rate:>20.281'] = np.where((dataset['int_rate'] > 20.281), 1, 0)","e18a6e0f":"loan_data_inputs_train['funded_amnt_factor'] = pd.cut(loan_data_inputs_train['funded_amnt'], 50)\ndf_temp = woe_ordered_continuous(loan_data_inputs_train, 'funded_amnt_factor', loan_data_targets_train)\ndf_temp","fda5ae88":"plot_by_woe(df_temp)","72b9ddc5":"loan_data_inputs_train['mths_since_earliest_cr_line'].unique()","b940991d":"loan_data_inputs_train['mths_since_earliest_cr_line_factor'] = pd.cut(loan_data_inputs_train['mths_since_earliest_cr_line'], 50)\ndf_temp = woe_ordered_continuous(loan_data_inputs_train, 'mths_since_earliest_cr_line_factor', loan_data_targets_train)\ndf_temp","abf2b224":"loan_data_inputs_train['mths_since_earliest_cr_line'].min()","b2f8ea49":"plot_by_woe(df_temp)","ea88b77e":"plot_by_woe(df_temp.iloc[6:, :])","c282e369":"for dataset in data_cleaner: \n    dataset['mths_since_earliest_cr_line:<140'] = np.where(dataset['mths_since_earliest_cr_line'].isin(range(140)), 1, 0)\n    dataset['mths_since_earliest_cr_line:141-164'] = np.where(dataset['mths_since_earliest_cr_line'].isin(range(140, 165)), 1, 0)\n    dataset['mths_since_earliest_cr_line:165-247'] = np.where(dataset['mths_since_earliest_cr_line'].isin(range(165, 248)), 1, 0)\n    dataset['mths_since_earliest_cr_line:248-270'] = np.where(dataset['mths_since_earliest_cr_line'].isin(range(248, 271)), 1, 0)\n    dataset['mths_since_earliest_cr_line:271-352'] = np.where(dataset['mths_since_earliest_cr_line'].isin(range(271, 353)), 1, 0)\n    dataset['mths_since_earliest_cr_line:>352'] = np.where(dataset['mths_since_earliest_cr_line'].isin(range(353, int(dataset['mths_since_earliest_cr_line'].max()))), 1, 0)","526095f2":"loan_data_inputs_train['installment'].unique()","d11c601a":"loan_data_inputs_train['installment_factor'] = pd.cut(loan_data_inputs_train['installment'], 50)\ndf_temp = woe_ordered_continuous(loan_data_inputs_train, 'installment_factor', loan_data_targets_train)\ndf_temp","78383e4b":"plot_by_woe(df_temp)","0b5d5971":"loan_data_inputs_train['delinq_2yrs'].unique()","af6d5fde":"df_temp = woe_ordered_continuous(loan_data_inputs_train, 'delinq_2yrs', loan_data_targets_train)\ndf_temp","57349918":"plot_by_woe(df_temp)","fcbf406e":"for dataset in data_cleaner: \n    dataset['delinq_2yrs:0'] = np.where((dataset['delinq_2yrs'] == 0), 1, 0)\n    dataset['delinq_2yrs:1-3'] = np.where((dataset['delinq_2yrs'] >= 1) & (dataset['delinq_2yrs'] <= 3), 1, 0)\n    dataset['delinq_2yrs:>=4'] = np.where((dataset['delinq_2yrs'] >= 9), 1, 0)","581239e1":"loan_data_inputs_train['inq_last_6mths'].unique()","e3fa637d":"df_temp = woe_ordered_continuous(loan_data_inputs_train, 'inq_last_6mths', loan_data_targets_train)\ndf_temp","c191156e":"plot_by_woe(df_temp)","6abddcde":"for dataset in data_cleaner: \n    dataset['inq_last_6mths:0'] = np.where((dataset['inq_last_6mths'] == 0), 1, 0)\n    dataset['inq_last_6mths:1-2'] = np.where((dataset['inq_last_6mths'] >= 1) & (dataset['inq_last_6mths'] <= 2), 1, 0)\n    dataset['inq_last_6mths:3-6'] = np.where((dataset['inq_last_6mths'] >= 3) & (dataset['inq_last_6mths'] <= 6), 1, 0)\n    dataset['inq_last_6mths:>6'] = np.where((dataset['inq_last_6mths'] > 6), 1, 0)","024c8671":"loan_data_inputs_train['open_acc'].unique()","41c064a8":"df_temp = woe_ordered_continuous(loan_data_inputs_train, 'open_acc', loan_data_targets_train)\ndf_temp","bd711454":"plot_by_woe(df_temp)","645a7577":"for dataset in data_cleaner: \n    dataset['open_acc:0'] = np.where((dataset['open_acc'] == 0), 1, 0)\n    dataset['open_acc:1-3'] = np.where((dataset['open_acc'] >= 1) & (dataset['open_acc'] <= 3), 1, 0)\n    dataset['open_acc:4-12'] = np.where((dataset['open_acc'] >= 4) & (dataset['open_acc'] <= 12), 1, 0)\n    dataset['open_acc:13-17'] = np.where((dataset['open_acc'] >= 13) & (dataset['open_acc'] <= 17), 1, 0)\n    dataset['open_acc:18-22'] = np.where((dataset['open_acc'] >= 18) & (dataset['open_acc'] <= 22), 1, 0)\n    dataset['open_acc:23-25'] = np.where((dataset['open_acc'] >= 23) & (dataset['open_acc'] <= 25), 1, 0)\n    dataset['open_acc:26-30'] = np.where((dataset['open_acc'] >= 26) & (dataset['open_acc'] <= 30), 1, 0)\n    dataset['open_acc:>=31'] = np.where((dataset['open_acc'] >= 31), 1, 0)","3c980486":"loan_data_inputs_train['pub_rec'].unique()","4a538e4e":"df_temp = woe_ordered_continuous(loan_data_inputs_train, 'pub_rec', loan_data_targets_train)\ndf_temp","c56f92e9":"plot_by_woe(df_temp)","73216ffa":"for dataset in data_cleaner: \n    dataset['pub_rec:0-2'] = np.where((dataset['pub_rec'] >= 0) & (dataset['pub_rec'] <= 2), 1, 0)\n    dataset['pub_rec:3-4'] = np.where((dataset['pub_rec'] >= 3) & (dataset['pub_rec'] <= 4), 1, 0)\n    dataset['pub_rec:>=5'] = np.where((dataset['pub_rec'] >= 5), 1, 0)","f5654690":"loan_data_inputs_train['total_acc'].unique()","76dadd7c":"loan_data_inputs_train['total_acc_factor']= pd.cut(loan_data_inputs_train['total_acc'], 50)\ndf_temp = woe_ordered_continuous(loan_data_inputs_train, 'total_acc_factor', loan_data_targets_train)\ndf_temp","98fa59b0":"plot_by_woe(df_temp)","105d44bb":"for dataset in data_cleaner: \n    dataset['total_acc:<=27'] = np.where((dataset['total_acc'] <= 27), 1, 0)\n    dataset['total_acc:28-51'] = np.where((dataset['total_acc'] >= 28) & (dataset['total_acc'] <= 51), 1, 0)\n    dataset['total_acc:>=52'] = np.where((dataset['total_acc'] >= 52), 1, 0)","05d885d0":"loan_data_inputs_train['acc_now_delinq'].unique()","ccf7b291":"df_temp = woe_ordered_continuous(loan_data_inputs_train, 'acc_now_delinq', loan_data_targets_train)\ndf_temp","aa803d61":"plot_by_woe(df_temp)","fae7987c":"for dataset in data_cleaner: \n    dataset['acc_now_delinq:0'] = np.where((dataset['acc_now_delinq'] == 0), 1, 0)\n    dataset['acc_now_delinq:>=1'] = np.where((dataset['acc_now_delinq'] >= 1), 1, 0)","8617705f":"loan_data_inputs_train['annual_inc_factor'] = pd.cut(loan_data_inputs_train['annual_inc'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(loan_data_inputs_train, 'annual_inc_factor', loan_data_targets_train)\n# We calculate weight of evidence.\ndf_temp","3f4ff3b2":"loan_data_inputs_train['annual_inc_factor'] = pd.cut(loan_data_inputs_train['annual_inc'], 100)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 100 categories by its values.\ndf_temp = woe_ordered_continuous(loan_data_inputs_train, 'annual_inc_factor', loan_data_targets_train)\n# We calculate weight of evidence.\ndf_temp","38243dbb":"# Initial examination shows that there are too few individuals with large income and too many with small income.\n# Hence, we are going to have one category for more than 150K, and we are going to apply our approach to determine\n# the categories of everyone with 140k or less.\nloan_data_inputs_train_temp = loan_data_inputs_train.loc[loan_data_inputs_train['annual_inc'] <= 140000, : ]\n#loan_data_temp = loan_data_temp.reset_index(drop = True)\n#loan_data_inputs_train_temp","6854d5b5":"loan_data_inputs_train_temp[\"annual_inc_factor\"] = pd.cut(loan_data_inputs_train_temp['annual_inc'], 50)\nloan_data_inputs_train_temp[\"annual_inc_factor\"].unique()","9a456ae6":"df_temp = woe_ordered_continuous(loan_data_inputs_train_temp, 'annual_inc_factor', \n                                 loan_data_targets_train[loan_data_inputs_train_temp.index])\ndf_temp","b0b8f0b1":"plot_by_woe(df_temp) ","47adcdbf":"# WoE is monotonically increasing with income, so we split income in 12 categories\nfor dataset in data_cleaner: \n    dataset['annual_inc:<20K'] = np.where((dataset['annual_inc'] <= 20000), 1, 0)\n    dataset['annual_inc:20K-30K'] = np.where((dataset['annual_inc'] > 20000) & (dataset['annual_inc'] <= 30000), 1, 0)\n    dataset['annual_inc:30K-40K'] = np.where((dataset['annual_inc'] > 30000) & (dataset['annual_inc'] <= 40000), 1, 0)\n    dataset['annual_inc:40K-50K'] = np.where((dataset['annual_inc'] > 40000) & (dataset['annual_inc'] <= 50000), 1, 0)\n    dataset['annual_inc:50K-60K'] = np.where((dataset['annual_inc'] > 50000) & (dataset['annual_inc'] <= 60000), 1, 0)\n    dataset['annual_inc:60K-70K'] = np.where((dataset['annual_inc'] > 60000) & (dataset['annual_inc'] <= 70000), 1, 0)\n    dataset['annual_inc:70K-80K'] = np.where((dataset['annual_inc'] > 70000) & (dataset['annual_inc'] <= 80000), 1, 0)\n    dataset['annual_inc:80K-90K'] = np.where((dataset['annual_inc'] > 80000) & (dataset['annual_inc'] <= 90000), 1, 0)\n    dataset['annual_inc:90K-100K'] = np.where((dataset['annual_inc'] > 90000) & (dataset['annual_inc'] <= 100000), 1, 0)\n    dataset['annual_inc:100K-120K'] = np.where((dataset['annual_inc'] > 100000) & (dataset['annual_inc'] <= 120000), 1, 0)\n    dataset['annual_inc:120K-140K'] = np.where((dataset['annual_inc'] > 120000) & (dataset['annual_inc'] <= 140000), 1, 0)\n    dataset['annual_inc:>140K'] = np.where((dataset['annual_inc'] > 140000), 1, 0)\n","7bc8e9d5":"loan_data_inputs_train['mths_since_last_delinq'].unique()","1b2abf80":"loan_data_inputs_train_temp = loan_data_inputs_train[pd.notnull(loan_data_inputs_train['mths_since_last_delinq'])]\nloan_data_inputs_train_temp['mths_since_last_delinq_factor'] = pd.cut(loan_data_inputs_train_temp['mths_since_last_delinq'], 50)\n","9aca1f3f":"df_temp = woe_ordered_continuous(loan_data_inputs_train_temp, 'mths_since_last_delinq_factor', \n                                 loan_data_targets_train[loan_data_inputs_train_temp.index])\n# We calculate weight of evidence.\ndf_temp","26713ef4":"plot_by_woe(df_temp, 90)","8b5efe0d":"for dataset in data_cleaner: \n    dataset['mths_since_last_delinq:Missing'] = np.where((dataset['mths_since_last_delinq'].isnull()), 1, 0)\n    dataset['mths_since_last_delinq:0-3'] = np.where((dataset['mths_since_last_delinq'] >= 0) & (dataset['mths_since_last_delinq'] <= 3), 1, 0)\n    dataset['mths_since_last_delinq:4-30'] = np.where((dataset['mths_since_last_delinq'] >= 4) & (dataset['mths_since_last_delinq'] <= 30), 1, 0)\n    dataset['mths_since_last_delinq:31-56'] = np.where((dataset['mths_since_last_delinq'] >= 31) & (dataset['mths_since_last_delinq'] <= 56), 1, 0)\n    dataset['mths_since_last_delinq:>=57'] = np.where((dataset['mths_since_last_delinq'] >= 57), 1, 0)","00c6720a":"loan_data_inputs_train['dti'].unique()","7db84140":"loan_data_inputs_train['dti'].max()","78b0d474":"# dti\nloan_data_inputs_train['dti_factor'] = pd.cut(loan_data_inputs_train['dti'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 100 categories by its values.\ndf_temp = woe_ordered_continuous(loan_data_inputs_train, 'dti_factor', loan_data_targets_train)\n# We calculate weight of evidence.\ndf_temp","16648323":"plot_by_woe(df_temp, 90)","2b40e9ab":"loan_data_inputs_train_temp = loan_data_inputs_train.loc[loan_data_inputs_train['dti'] <= 35, : ]","77b5d77e":"loan_data_inputs_train_temp['dti_factor'] = pd.cut(loan_data_inputs_train_temp['dti'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(loan_data_inputs_train_temp, 'dti_factor',\n                                 loan_data_targets_train[loan_data_inputs_train_temp.index])\n# We calculate weight of evidence.\ndf_temp","387a57b8":"plot_by_woe(df_temp, 90)","4f39a541":"for dataset in data_cleaner: \n    dataset['dti:<=1.4'] = np.where((dataset['dti'] <= 1.4), 1, 0)\n    dataset['dti:1.4-3.5'] = np.where((dataset['dti'] > 1.4) & (dataset['dti'] <= 3.5), 1, 0)\n    dataset['dti:3.5-7.7'] = np.where((dataset['dti'] > 3.5) & (dataset['dti'] <= 7.7), 1, 0)\n    dataset['dti:7.7-10.5'] = np.where((dataset['dti'] > 7.7) & (dataset['dti'] <= 10.5), 1, 0)\n    dataset['dti:10.5-16.1'] = np.where((dataset['dti'] > 10.5) & (dataset['dti'] <= 16.1), 1, 0)\n    dataset['dti:16.1-20.3'] = np.where((dataset['dti'] > 16.1) & (dataset['dti'] <= 20.3), 1, 0)\n    dataset['dti:20.3-21.7'] = np.where((dataset['dti'] > 20.3) & (dataset['dti'] <= 21.7), 1, 0)\n    dataset['dti:21.7-22.4'] = np.where((dataset['dti'] > 21.7) & (dataset['dti'] <= 22.4), 1, 0)\n    dataset['dti:22.4-35'] = np.where((dataset['dti'] > 22.4) & (dataset['dti'] <= 35), 1, 0)\n    dataset['dti:>35'] = np.where((dataset['dti'] > 35), 1, 0)","b95476a0":"loan_data_inputs_train['mths_since_last_record'].unique()","f5d9dd00":"loan_data_inputs_train_temp = loan_data_inputs_train[pd.notnull(loan_data_inputs_train['mths_since_last_record'])]\nloan_data_inputs_train_temp['mths_since_last_record_factor'] = pd.cut(loan_data_inputs_train_temp['mths_since_last_record'], 50)\ndf_temp = woe_ordered_continuous(loan_data_inputs_train_temp, 'mths_since_last_record_factor',\n                                 loan_data_targets_train[loan_data_inputs_train_temp.index])\ndf_temp","7dd1d39e":"plot_by_woe(df_temp, 90)","7a64939c":"# Categories: 'Missing', '0-2', '3-20', '21-31', '32-80', '81-86', '>86'\nfor dataset in data_cleaner: \n    dataset['mths_since_last_record:Missing'] = np.where((dataset['mths_since_last_record'].isnull()), 1, 0)\n    dataset['mths_since_last_record:0-2'] = np.where((dataset['mths_since_last_record'] >= 0) & (dataset['mths_since_last_record'] <= 2), 1, 0)\n    dataset['mths_since_last_record:3-20'] = np.where((dataset['mths_since_last_record'] >= 3) & (dataset['mths_since_last_record'] <= 20), 1, 0)\n    dataset['mths_since_last_record:21-31'] = np.where((dataset['mths_since_last_record'] >= 21) & (dataset['mths_since_last_record'] <= 31), 1, 0)\n    dataset['mths_since_last_record:32-80'] = np.where((dataset['mths_since_last_record'] >= 32) & (dataset['mths_since_last_record'] <= 80), 1, 0)\n    dataset['mths_since_last_record:81-86'] = np.where((dataset['mths_since_last_record'] >= 81) & (dataset['mths_since_last_record'] <= 86), 1, 0)\n    dataset['mths_since_last_record:>86'] = np.where((dataset['mths_since_last_record'] > 86), 1, 0)","62d47b78":"loan_data_inputs_train['mths_since_last_record:>86'].isnull().sum()\n","58933646":"loan_data_inputs_train['total_rev_hi_lim'].unique()","86e196bd":"loan_data_inputs_train_temp = loan_data_inputs_train[pd.notnull(loan_data_inputs_train['total_rev_hi_lim'])]\nloan_data_inputs_train_temp['total_rev_hi_lim_factor'] = pd.cut(loan_data_inputs_train_temp['total_rev_hi_lim'], 50)\ndf_temp = woe_ordered_continuous(loan_data_inputs_train_temp, 'total_rev_hi_lim_factor',\n                                 loan_data_targets_train[loan_data_inputs_train_temp.index])\ndf_temp","44ded32a":"loan_data_inputs_train_temp = loan_data_inputs_train.loc[loan_data_inputs_train['total_rev_hi_lim'] <= 200000, : ]\nloan_data_inputs_train_temp['total_rev_hi_lim_factor'] = pd.cut(loan_data_inputs_train_temp['total_rev_hi_lim'], 50)\ndf_temp = woe_ordered_continuous(loan_data_inputs_train_temp, 'total_rev_hi_lim_factor',\n                                 loan_data_targets_train[loan_data_inputs_train_temp.index])\ndf_temp","cdb7d82b":"plot_by_woe(df_temp, 90)","c959d2f8":"for dataset in data_cleaner:\n    dataset['total_rev_hi_lim:Missing'] = np.where((dataset['total_rev_hi_lim'].isnull()), 1, 0)\n    dataset['total_rev_hi_lim:<=5K'] = np.where((dataset['total_rev_hi_lim'] <= 5000), 1, 0)\n    dataset['total_rev_hi_lim:5K-10K'] = np.where((dataset['total_rev_hi_lim'] > 5000) & (dataset['total_rev_hi_lim'] <= 10000), 1, 0)\n    dataset['total_rev_hi_lim:10K-20K'] = np.where((dataset['total_rev_hi_lim'] > 10000) & (dataset['total_rev_hi_lim'] <= 20000), 1, 0)\n    dataset['total_rev_hi_lim:20K-30K'] = np.where((dataset['total_rev_hi_lim'] > 20000) & (dataset['total_rev_hi_lim'] <= 30000), 1, 0)\n    dataset['total_rev_hi_lim:30K-40K'] = np.where((dataset['total_rev_hi_lim'] > 30000) & (dataset['total_rev_hi_lim'] <= 40000), 1, 0)\n    dataset['total_rev_hi_lim:40K-55K'] = np.where((dataset['total_rev_hi_lim'] > 40000) & (dataset['total_rev_hi_lim'] <= 55000), 1, 0)\n    dataset['total_rev_hi_lim:55K-95K'] = np.where((dataset['total_rev_hi_lim'] > 55000) & (dataset['total_rev_hi_lim'] <= 95000), 1, 0)\n    dataset['total_rev_hi_lim:>95K'] = np.where((dataset['total_rev_hi_lim'] > 95000), 1, 0)\n            \n             ","133c9795":"loan_data_inputs_train.to_csv('loan_data_inputs_train.csv')\nloan_data_targets_train.to_csv('loan_data_targets_train.csv')\nloan_data_inputs_test.to_csv('loan_data_inputs_test.csv')\nloan_data_targets_test.to_csv('loan_data_targets_test.csv')","76adf5d8":"#### Address State","97f210e2":"This 99% of observations lie in the first bin, we need to dissect this into further bins to calculate WoE. ","0823b033":"### Weight of Evidence Calculation for 1 variable","444962f5":"The values equal to 0 need to be a separate category. Bins from 1-3 can be combined. Then the next cutoff can be around 6. The rest of the bins have very few values and can be combined together. ","1f98efea":"NY and CA have the highest number of observations. Most borrowers. These can be in a separate category dummy of their own. \nThe other states can be in one category\/ grouped together. But leaving out NY and CA because these need to be in a separate category. \n\nNM and VA could be combined\nOK, TN, MO, LA, MD, NC could be combined to one. \nThese can't be combined together because they are separated by NY. (10% of observations come from NY)\n\nUT, KY, AZ and NJ can be comibined into one category. They have similar WOE and neither has very high number of values. \n\nAR, MI, PA, OH and MN have similar WOE. RI, MA, DE, SD and IN belong to one category. \nGA, WA and OR belong together. WI and MT need to be combined.\n\nTX, IL and CT. have similar WOE but Texas has too many values. It can stay a separate category. \nIL and CT can be combined\n\nKS, SC, CO, VT, AK and MS combined into one category\nThey have very high WOE but too few observations. This makes WoE less reliable. They can be combined together.","d27af62f":"#### Months since Issue","795b92f4":"Debt consolidation and credit card can remain a separate value. It has too many values. \n","0e5a1bc7":"#### purpose","f0c80218":"Calculating the diff in proportion of good\/bad as we move across categories","34c7bcbd":"## Import Libraries","72838a3e":"#### mths_since_last_record","05771836":"#### Annual income","94f2ca42":"Since data preparation binning will be same for both training and test data, combining both dataframes to a list so that changes can be applied uniformly.","eb8f7769":"There are a large number of values in the first interval (~94% of values). In this case, it might be worthwhile to split the variable into more classes. Eg. 100","9a4d4585":"This makes things better, but still the first category ends up with 60% of observations. This is logical, because there are very few people with high incomes. We can increate one dummy variable for rich , while exploring the rest of the categories separately. Let the cutoff for rich start at the 3rd bin (144693.64). ","74fa86df":"### Preprocessing Discrete Variables: Creating dummy variables, Part 1","13de370c":"#### Delinquency in the last 2 years","b5eaeb54":"The first three bins need to be kept separate from the rest of the categories. They have higher WOEs than the rest of the categories. Plotting the remaining bins to see which ones can be binned together:","bfc18a80":"This variable has too many values to use as is. It can be converted to categorical using the cut function","a98dba9a":"Consecutive intervals have very different weights of evidence. All of these variations are around a trend that is almost horizontal. Also, the IV of the variable is low. This is variable with weak predictive power (WoE varies greatly, and there is no association w\/ the independent variable. )\n\nThis variable can be left out of the model. ","5209cc54":"#### Months since last delinquency","f4f88de7":"#### interest rate","434ce2e3":"The first 4 bins have similar weights of evidence (41.4, 48.6]. The next two categories can be grouped together.(48.6, 52.2]. The next 7 variables have similar WOE and can be grouped together (52.2, 64.8]. \n\nThe rest of the values have highly varying values. This is a warning to check their bin sizes. Usually small bin sizes could contribute to such variation. These can be combined into one dummy variable till there is an upward trend, and the rest of them together. (64.8, 84.60] & (84.60, Inf)","4159997f":"The WOE of all these values is sufficiently different. All categories have enough observations. This variable can remain as is. The lowest WOE belongs to verified. That can become the reference category","f33400a7":"## Preprocessing - I","383bf91a":"## Train\/Test Split","48cd6b78":"#### initial_list_status","339998e0":"There are fewer than 50 states in the results. There are no borrowers from 1 of the states. (North Dakota)","b30fb8a5":"### Good\/bad(default) definition, Default and non-default accounts","df4ed13b":"## Import Data\nThe dataset contains all available data for more than 800,000 consumer loans issued from 2007 to 2015 by Lending Club: a large US peer-to-peer lending company. There are several different versions of this dataset. We have used a version available on kaggle.com. You can find it here: https:\/\/www.kaggle.com\/wendykan\/lending-club-loan-data\/version\/1\nWe divided the data into two periods because we assume that some data are available at the moment when we need to build Expected Loss models, and some data comes from applications after. Later, we investigate whether the applications we have after we built the Probability of Default (PD) model have similar characteristics with the applications we used to build the PD model.","b9dcec4a":"#### Installment","22eb58c7":"Nevada has a lower WOE than the states after. It could be clubbed with the low proportion states we had excluded. The state with no records (North Dakota) can be included here. This would be a conservative approach to take. For states with unknown values, assignin them the worst WOE value. The first dummy variable will include NE, IA, NV, FL, HI and AL. The states at the end will include WV, NH, WY, DC, ME and ID. Plotting the remaining states: \n","2de35bc1":"## Defining Dependent variable","6a8e9c08":"## Creating Dummy variables","97d9e2e1":"### Preprocessing Continuous Variables: Automating Calculations and Visualizing Results","a853443e":"## Weight of Evidence Calculation","ee8bb40c":"## Explore Data","f6c5ba8e":"#### Employment Length","39a0a51e":"## Checking for missing values","10d191c5":"Weight of evidence for the first category is considerably lower than the next one. So that can be a separate category (0-3]. The next jumps in WoE happen around 30 an then 56. The values after 56 have very few observations in each bin and hence can be combined together. ","42bf71a5":"The variable has a largely downward trend around WoE. But there are very few values greater than 35.","6ac68285":"This is variable with null values. ","d7f39a48":"#### Term","9ac9e6d2":"Initial examination shows that there are too few individuals with large income and too many with small income. Hence, we are going to have one category for more than 150K, and we are going to apply our approach to determine the categories of everyone with 140k or less.","39fa36b1":"### Automating WOE calculation","66610837":"### Preprocessing Continuous Variables: Creating Dummy Variables","85274d17":"Other, none and any categories represent very few observations. We combine them to not lose the info they bring. Combining underrepresented categories. They are combined w\/ the riskiest category w\/ enough number of observations. In this case: rent","c88ba020":"#### Number of accounts","3abe3f6d":"This variable has null values. The binning exercise should not consider those. However, instead  of dropping these variables, we can introduce a new dummy variable that takes on a value 1 when the value is nan and 0 when its not. Examining the WoE of the remaining values:","61ee7cb2":"### Preprocessing discrete variables: Visualising results","aed160f5":"## Saving Output","a9ff7592":"#### Inquiries in the last 6 months","2d3e2488":"Largely the relationship of interest rate w\/ WOE is monotonic. The boundaries of the bins will not have an impact on the monotonicity of this relationship. \n\nIdentifying cases where WOE differs significantly from the previous categories (huge jumps or lows).\nThe first ten bins have very large WOEs have low bin size. They can be combined together. This is till 9.548. \nThen the bins till 12.025 see a reduction in WOE.The other points where a huge change in WOE is seen are 15.74, 20.281, 23.583. \n\nThe bins after that have some variation but have very small bin sizes. \n\nFor a continuous variables, the condition will not look up a list. Instead we will use greater than\/less than conditions. ","46a14e6b":"#### months since credit line","e26de459":"## Preprocessing few continuous variables","27e807e6":"#### Funded amount","6c1a2316":"#### Number of public records","f5eb1d94":"#### Accounts now delinquent","f71856af":"### Total Revolving Limit","36857623":"#### Debt to income ratio","91cb9d4c":"Installement has very low IV, and a highly erratic WoE graph. ","499a97f8":"#### verification_status","ec83ca87":"There are only 2 categories and they both bins have enough values- no need to combine these. ","61a064ea":"Weight of evidence is not calculated for 2 states. There are no bads. The first two states have very few observations. These can be clubbed w\/ the next riskiest category","5135f6c5":"Most accounts have 0 delinquencies. They can remain in a separate category. Then the values from 1-3 delinquencies. The bins after that have very few values, so they can all be clubbed together. ","851e22f6":"#### Number of trades","68db7c45":"0 years of emplyment can be separate category. 1 year of employment\n2-4\n5-6\n7-9\n10","d3001d05":"This variable can have only positive values and has values running upto millions."}}