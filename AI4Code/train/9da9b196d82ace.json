{"cell_type":{"90a85ce6":"code","f7892268":"code","4ed396c2":"code","99cdd94e":"code","f24a96df":"code","1ed6525b":"code","bfac4ea9":"code","48c3c91e":"code","a8808a00":"code","316335e5":"code","39180f13":"code","5aa1c03c":"code","79594430":"code","546e7ddd":"code","1287cfa2":"code","837d81ab":"code","649bb8d7":"code","804a5bbe":"code","f5ee2c82":"code","8444dbf2":"code","6c783c6c":"code","9726120c":"code","cdb3dd77":"code","9eb5039a":"code","c5d709ee":"code","18636518":"markdown","0aaabb9d":"markdown","0d4273f5":"markdown","47dfa8ab":"markdown","99c8a8c3":"markdown","5c969055":"markdown","5d3aa34d":"markdown","c99eeed2":"markdown","45894746":"markdown","f6a9f59a":"markdown","5d1aee7c":"markdown","226ee0ea":"markdown","39f55b9f":"markdown","d429fcb5":"markdown"},"source":{"90a85ce6":"# Dataframes\nimport pandas as pd\n\n#\u00a0SK-Learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\n#NLTK\nfrom nltk.corpus import stopwords\nfrom nltk import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\n\n# String manipulation\nimport re\nimport string\n\n# Random\nimport random\n\n# Keras\nfrom keras_preprocessing.text import Tokenizer\nfrom keras import Sequential, initializers, regularizers, layers, utils, Input\n\n# Plotting\nimport matplotlib.pyplot as plt\n\n# Numpy\nimport numpy as np\n\n# Itertools\nimport itertools\n","f7892268":"# Function to remove html links & mentions\ndef remove_html_mentions(text):\n    text = re.sub(r\"(?:\\@|https?\\:\/\/)\\S+\", \"\", text)\n    return text\n\n# Function to remove numbers\ndef remove_numbers(text):\n    text = ''.join([''.join([i for i in word if not i.isdigit()]) for word in text])\n    return text\n\n# Function to remove punctuation\ndef remove_punctuation(text):\n    text = ''.join([symbol for symbol in text if symbol not in string.punctuation])\n    return text\n\ntokenizer = RegexpTokenizer('\\s+', gaps=True)\n# Function to tokenize text\ndef tokenize_text(text):\n    tokenized_text = tokenizer.tokenize(text)\n    return tokenized_text\n\n# Function to remove stopwords\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\nlemmatizer = WordNetLemmatizer()\n# Function to lemmatize words:\ndef lemmatize_words(text):\n    lem_text = [lemmatizer.lemmatize(word) for word in text]\n    return lem_text\n\nstemmer = PorterStemmer()\n# Function to stem words\ndef stem_words(text):\n    stem_text = ' '.join([stemmer.stem(word) for word in text])\n    return stem_text","4ed396c2":"def clean(data):\n    data = data.apply(lambda x: remove_html_mentions(x))\n    data = data.apply(lambda x: remove_numbers(x))\n    data = data.apply(lambda x: remove_punctuation(x))\n    data = data.apply(lambda x: tokenize_text(x.lower()))\n    data = data.apply(lambda x: remove_stopwords(x))\n    data = data.apply(lambda x: lemmatize_words(x))\n    data = data.apply(lambda x: stem_words(x))\n    return data","99cdd94e":"def train_test_validate(x, y, t_size=.25, validation_size=.25):\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=t_size, random_state=1)\n\n    val_size = round((1-t_size)*validation_size,2)\n    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_size, random_state=1)\n\n    train = pd.concat([x_train, y_train], axis=1)\n    test = pd.concat([x_test, y_test], axis=1)\n    validate = pd.concat([x_val, y_val], axis=1)\n\n    return train, test, validate","f24a96df":"f = '..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv'\n\n# Total number of data points\nnum_lines = sum(1 for _ in open(f, encoding='ISO-8859-1'))\n\n# Number of data points to use\nsize = int(num_lines \/ 50)\nskip_idx = random.sample(range(1, num_lines), num_lines - size)\ntweet_data = pd.read_csv(f, skiprows=skip_idx, encoding='ISO-8859-1')","1ed6525b":"# Set column names\ntweet_data.columns = ['target', 'id', 'date', 'flag', 'user', 'text']","bfac4ea9":"# Reset target columns\ntweet_data.loc[tweet_data['target'] == 0, 'target'] = 0\ntweet_data.loc[tweet_data['target'] == 4, 'target'] = 1","48c3c91e":"target, text = tweet_data.iloc[:,0], tweet_data.iloc[:,5]","a8808a00":"# Clean tweets\ntext = clean(text)","316335e5":"# Create train, validation and test data\ntrain, test, validate = train_test_validate(text, target)","39180f13":"# Create tokenizer\nnum_words_keep = 1000\ntokenizer = Tokenizer(num_words=num_words_keep,filters='',lower=False,split=' ',\n                      char_level=False, oov_token=None)","5aa1c03c":"# Fit tokenizer on training data\nx_train = train.iloc[:,0]\n\ntokenizer.fit_on_texts(texts=x_train)\nmodes = ['binary', 'count', 'tfidf', 'freq']","79594430":"# Training data\ny_train = train.iloc[:,1]\n\nx_train = tokenizer.texts_to_matrix(x_train, mode=modes[1])\ny_train = utils.to_categorical(y_train, num_classes=2)","546e7ddd":"# Validation data\nx_validate = validate.iloc[:,0]\nx_validate = tokenizer.texts_to_matrix(x_validate, mode=modes[1])\n\ny_validate = validate.iloc[:,1]\ny_validate = utils.to_categorical(y_validate, num_classes=2)","1287cfa2":"# Test data\nx_test = test.iloc[:,0]\nx_test = tokenizer.texts_to_matrix(x_test, mode=modes[1])\n\ny_test = test.iloc[:,1]","837d81ab":"# Class Names for labels 0:Negative, 1:Positive\nclass_names = ['Negative', 'Positive']","649bb8d7":"# Implement model\nmodel = Sequential()\n\nactivation_functions = ['relu','sigmoid','tanh']\n\n# Kernel\/weight initialiser\ninitialiser = initializers.GlorotNormal(seed=1) # Mitigating risk of vanishing\/exploding gradients\n\n# Kernel\/weight regulariser\nreg_constant1 = 0.01\nl2_regulariser = regularizers.l2(l=reg_constant1)\n\n# Dropout 20% of input variables at random each pass\ndropout = layers.Dropout(.2, input_shape=(num_words_keep,))\n\nlayer1 = layers.Dense(\n    units = num_words_keep\/2,\n    activation= activation_functions[0],\n    use_bias=True,\n    kernel_initializer=initialiser,\n    bias_initializer='zeros',\n    kernel_regularizer=l2_regulariser\n)\n\nlayer2 = layers.Dense(\n    units = 2,\n    activation= activation_functions[1],\n    use_bias=True,\n    kernel_initializer=initialiser,\n    bias_initializer='zeros'\n)\n\nmodel.add(Input(shape=(num_words_keep,)))\nmodel.add(dropout)\nmodel.add(layer1)\nmodel.add(layer2)\n\nmodel.compile(optimizer='Adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","804a5bbe":"# Train model\nhistory = model.fit(x=x_train,\n                    y=y_train,\n                    batch_size=512,\n                    epochs=10,\n                    verbose=2, #one line per epoch\n                    validation_data=(x_validate,y_validate),\n                    shuffle=True,\n                    validation_freq=1)","f5ee2c82":"# Plot accuracy\nplt.plot(history.history['accuracy'], label = 'Training Accuracy')\nplt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\nplt.title('Training & Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('No. epoch')\nplt.legend(loc=\"upper left\")\nplt.show()","8444dbf2":"# Plot loss\nplt.plot(history.history['loss'], label = 'Training Loss')\nplt.plot(history.history['val_loss'], label = 'Validation Loss')\nplt.title('Training & Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('No. epoch')\nplt.legend(loc=\"upper left\")\nplt.show()\nplt.close()","6c783c6c":"# Function to plot confusion matrix\n# Credit: (https:\/\/www.kaggle.com\/paoloripamonti\/twitter-sentiment-analysis\/notebook)\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=16)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90, fontsize=16)\n    plt.yticks(tick_marks, classes, fontsize=16)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=16)\n    plt.xlabel('Predicted label', fontsize=16)","9726120c":"# Make new predictions:\ny_pred = np.argmax(model.predict(x_test), axis=-1)","cdb3dd77":"# Plot confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8,8))\nplot_confusion_matrix(cm=cnf_matrix, classes=class_names)\nplt.show()\nplt.close()","9eb5039a":"print(classification_report(y_test, y_pred))","c5d709ee":"print(accuracy_score(y_test, y_pred))","18636518":"**Objective:**\n* Predict the sentiment of tweets using a multilayer percepetron neural network. There are no neutral tweets in this dataset, only positive and negative tweets, hence this is a binary classification problem.","0aaabb9d":"**Accuracy Report**","0d4273f5":"**Import Libraries**","47dfa8ab":"**Select a random subset of 32k tweets for training, testing & validating**","99c8a8c3":"**Functions to clean data**","5c969055":"**Implement tokenizer**\n* Keep 1000 words\n* Split each word between spaces","5d3aa34d":"**Concluding Thoughts:**\n* Implementing EarlyStopping (from keras.callbacks) would have stopped the training at epoch 4.\n* Accuracy of 71% is not bad accuracy given that the NN was only trained on around 16k tweets.\n**Improvements:**\n* Use more data.\n* Using word2vec to create embedding matrix and implementing this into an Embedding layer.\n* Using a recurrent neural network which is designed to receive inputs as sequences and \"remember\" previous sequences. A popular choice would be a Long-Short-Term-Memory (LSTM) network.","c99eeed2":"**Function to create train, test & validation data**","45894746":"# Tweet sentiment prediction","f6a9f59a":"**Overview of features**\n1. target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n\n2. ids: The id of the tweet ( 2087)\n\n3. date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n\n4. flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n\n5. user: the user that tweeted (robotickilldozr)\n\n6. text: the text of the tweet (Lyx is cool)","5d1aee7c":"**Classification Report**","226ee0ea":"Implement a dense multi-layer percepetron with 2 hidden layers & a dropout layer.","39f55b9f":"**Function that applies all above functions to pandas column**","d429fcb5":"Use the counts of each word in the tweet when fitting the tokenizer."}}