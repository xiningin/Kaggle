{"cell_type":{"0523414b":"code","62435022":"code","393f999d":"code","8da0ec2f":"code","d69e81c7":"code","16397747":"code","86ee4762":"code","37701ad3":"code","d685ff69":"code","6057ebfc":"code","a3320a98":"code","620f054a":"code","9e8a4420":"code","f1e1469c":"code","5395c660":"code","5b09d911":"code","8f6263a4":"code","bb01e4b3":"code","d90040a3":"code","9c1f3f66":"markdown","5a205b90":"markdown"},"source":{"0523414b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport time, random, math, string","62435022":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchtext.legacy.datasets import Multi30k\nfrom torchtext.legacy.data import Field, BucketIterator","393f999d":"tokenizer = lambda x: str(x).translate(str.maketrans('', '', string.punctuation)).strip().split() \nreverse_tokenizer = lambda x: tokenizer(x)[::-1]\n\nSRC = Field(tokenize=reverse_tokenizer, init_token='<sos>', eos_token='<eos>', lower=True)\nTRG = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>', lower=True)\n\ntrain_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'),\n                                                   fields=(SRC, TRG))","8da0ec2f":"print(f\"Number of training examples: {len(train_data)}\")\nprint(f\"Number of validation examples: {len(valid_data)}\")\nprint(f\"Number of test examples: {len(test_data)}\")","d69e81c7":"SRC.build_vocab(train_data, min_freq=2)\nTRG.build_vocab(train_data, min_freq=2)\n\nprint(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\nprint(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")","16397747":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nBATCH_SIZE = 128\n\ntrain_iter, valid_iter, test_iter = BucketIterator.splits((train_data, valid_data, test_data),\n                                                          batch_size=BATCH_SIZE, device=device)\n#train_iter = BucketIterator(train_data,batch_size=BATCH_SIZE, device=device)","86ee4762":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, src):\n        # src : [sen_len, batch_size]\n        embedded = self.dropout(self.embedding(src))\n        # embedded : [sen_len, batch_size, emb_dim]\n        outputs, (hidden, cell) = self.rnn(embedded)\n        # outputs = [sen_len, batch_size, hid_dim * n_directions]\n        # hidden = [n_layers * n_direction, batch_size, hid_dim]\n        # cell = [n_layers * n_direction, batch_size, hid_dim]\n        return hidden, cell","37701ad3":"class Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.output_dim = output_dim\n        self.emb_dim = emb_dim\n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=self.n_layers, dropout=dropout)\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, input, hidden, cell):\n        \n        # input = [batch_size]\n        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n        # cell = [n_layers * n_dir, batch_size, hid_dim]\n        \n        input = input.unsqueeze(0)\n        # input : [1, ,batch_size]\n        \n        embedded = self.dropout(self.embedding(input))\n        # embedded = [1, batch_size, emb_dim]\n        \n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        # output = [seq_len, batch_size, hid_dim * n_dir]\n        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n        # cell = [n_layers * n_dir, batch_size, hid_dim]\n        \n        # seq_len and n_dir will always be 1 in the decoder\n        prediction = self.fc_out(output.squeeze(0))\n        # prediction = [batch_size, output_dim]\n        return prediction, hidden, cell","d685ff69":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n        assert encoder.hid_dim == decoder.hid_dim, \\\n            'hidden dimensions of encoder and decoder must be equal.'\n        assert encoder.n_layers == decoder.n_layers, \\\n            'n_layers of encoder and decoder must be equal.'\n        \n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        # src = [sen_len, batch_size]\n        # trg = [sen_len, batch_size]\n        # teacher_forcing_ratio : the probability to use the teacher forcing.\n        batch_size = trg.shape[1]\n        trg_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        \n        # tensor to store decoder outputs\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        \n        # last hidden state of the encoder is used as the initial hidden state of the decoder\n        hidden, cell = self.encoder(src)\n        \n        # first input to the decoder is the <sos> token.\n        input = trg[0, :]\n        for t in range(1, trg_len):\n            # insert input token embedding, previous hidden and previous cell states \n            # receive output tensor (predictions) and new hidden and cell states.\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            \n            # replace predictions in a tensor holding predictions for each token\n            outputs[t] = output\n            \n            # decide if we are going to use teacher forcing or not.\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            # get the highest predicted token from our predictions.\n            top1 = output.argmax(1)\n            # update input : use ground_truth when teacher_force \n            input = trg[t] if teacher_force else top1\n            \n        return outputs","6057ebfc":"# First initialize our model.\nINPUT_DIM = len(SRC.vocab)\nOUTPUT_DIM = len(TRG.vocab)\nENC_EMB_DIM = 256\nDEC_EMB_DIM = 256\nHID_DIM = 512\nN_LAYERS = 2\nENC_DROPOUT = 0.5\nDEC_DROPOUT = 0.5\n\nencoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\ndecoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n\nmodel = Seq2Seq(encoder, decoder, device).to(device)","a3320a98":"def init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.uniform_(param.data, -0.08, 0.08)\n        \nmodel.apply(init_weights)","620f054a":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","9e8a4420":"optimizer = optim.Adam(model.parameters())\n\nTRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n\ncriterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)","f1e1469c":"def train(model, iterator, optimizer, criterion, clip):\n    \n    model.train()\n    \n    epoch_loss = 0\n    \n    for i, batch in enumerate(iterator):\n        src = batch.src\n        trg = batch.trg\n        \n        optimizer.zero_grad()\n        # trg = [sen_len, batch_size]\n        # output = [trg_len, batch_size, output_dim]\n        output = model(src, trg)\n        output_dim = output.shape[-1]\n        \n        # transfrom our output : slice off the first column, and flatten the output into 2 dim.\n        output = output[1:].view(-1, output_dim) \n        trg = trg[1:].view(-1)\n        # trg = [(trg_len-1) * batch_size]\n        # output = [(trg_len-1) * batch_size, output_dim]\n        loss = criterion(output, trg)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","5395c660":"def evaluate(model, iterator, criterion): \n    model.eval()\n    epoch_loss = 0\n    with torch.no_grad():\n        for i, batch in enumerate(iterator):\n            src = batch.src\n            trg = batch.trg\n            output = model(src, trg, 0) # turn off teacher forcing.\n            # trg = [sen_len, batch_size]\n            # output = [sen_len, batch_size, output_dim]\n            output_dim = output.shape[-1]\n            output = output[1:].view(-1, output_dim)\n            trg = trg[1:].view(-1)\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n            \n    return epoch_loss \/ len(iterator)","5b09d911":"# a function that used to tell us how long an epoch takes.\ndef epoch_time(start_time, end_time):    \n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time  \/ 60)\n    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n    return  elapsed_mins, elapsed_secs","8f6263a4":"N_EPOCHS = 10\n\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, valid_iter, criterion)\n    \n    end_time = time.time()\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'Seq2SeqModel.pt')\n    print(f\"Epoch: {epoch+1:02} | Time {epoch_mins}m {epoch_secs}s\")\n    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}\")","bb01e4b3":"def test():\n    best_model = Seq2Seq(encoder, decoder, device).to(device)\n    best_model.load_state_dict(torch.load('Seq2SeqModel.pt'))\n    \n    test_loss = evaluate(model, test_iter, criterion)\n    \n    print(f\"Test Loss : {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}\")\n    \ntest()","d90040a3":"tokenizer(\"Meine name ist Vanya\")","9c1f3f66":"## Preparing Data","5a205b90":"## Training the Seq2Seq model\n"}}