{"cell_type":{"7ee396dd":"code","ae81c53d":"code","69cbc946":"code","d7e98ba2":"code","fea16155":"code","153f3bfe":"code","0bfe9291":"code","c1b4862b":"code","ed77cda4":"code","6cb1783a":"code","7659f855":"code","6477a135":"code","f63c5ed4":"code","8cadb474":"code","bb357a45":"code","656e853b":"code","24c6d500":"code","2ef0af26":"markdown","1f7a116e":"markdown","a3858ad5":"markdown","a629966b":"markdown","f769c3df":"markdown","0f140e5c":"markdown","88d2005a":"markdown","fb476898":"markdown","6ff3448d":"markdown","1a14647c":"markdown","f7c02aae":"markdown"},"source":{"7ee396dd":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nimport json\nimport os\nimport random\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.utils import plot_model\n\nimport tensorflow as tf\nprint(\"Tensorflow version:\", tf.__version__)","ae81c53d":"path = \"..\/input\/face-mask-dataset\/DATASET\"\nlabels = [\"Mask\", \"NoMask\"]","69cbc946":"# Randomly view 5 images in each category\n\nnum = 10\nfig, axs = plt.subplots(len(labels), num, figsize = (15, 15))\n\nclass_len = {}\nfor i, c in enumerate(labels):\n    class_path = os.path.join(path, c)\n    all_images = os.listdir(class_path)\n    sample_images = random.sample(all_images, num)\n    class_len[c] = len(all_images)\n    \n    for j, image in enumerate(sample_images):\n        img_path = os.path.join(class_path, image)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        axs[i, j].imshow(img)\n        axs[i, j].set(xlabel = c, xticks = [], yticks = [])\n\nfig.tight_layout()","d7e98ba2":"\n# Make a pie-chart to visualize the percentage contribution of each category.\nfig, ax = plt.subplots()\nax.pie(\n    class_len.values(),\n    labels = class_len.keys(),\n    autopct = \"%1.1f%%\"\n)\nfig.show()\n# The dataset is imbalance so we will have to take care of that later.","fea16155":"# We do not have separate folders for training and validation. \n# We need to read training and validation images from the same folder such that:\n# 1. There is no data leak i.e. Training images should not appear as validation images.                 \n# 2. We must be able to apply augmentation to training images but not validation images.  \n# We shall adopt the following strategy:\n# 1. Use the same validation_split in ImageDataGenerator for training and validation.\n# 2. Use the same seed when using flow_from_directory for training and validation. \n# To veify the correctness of this approach, you can print filenames from each generator and check for overlap.\n\n# Note that we use simple augmentation to avoid producing unsuitable images.\n\nimg_size = 100\nbatch = 64\n\ndatagen_train = ImageDataGenerator(\n    rescale = 1.\/255, \n    validation_split = 0.2,\n    rotation_range = 5,\n    width_shift_range = 0.05,\n    height_shift_range = 0.05,\n    zoom_range = 0.01\n)\n\ndatagen_val = ImageDataGenerator(\n    rescale = 1.\/255, \n    validation_split = 0.2 \n)    \n\ntrain_generator = datagen_train.flow_from_directory(\n    directory = path,\n    classes = labels,\n    target_size=(img_size, img_size),\n    seed = 42,\n    batch_size = batch, \n    shuffle = True,\n    subset = 'training'\n)\n\nval_generator = datagen_val.flow_from_directory(\n    directory = path,\n    classes = labels,\n    target_size=(img_size, img_size),\n    seed = 42,\n    batch_size = batch, \n    shuffle = True,\n    subset = 'validation'\n)","153f3bfe":"# To veify the correctness of this approach (empty set is expected)\nset(val_generator.filenames).intersection(set(train_generator.filenames))","0bfe9291":"# Check out labeling\nLabelMap = val_generator.class_indices\nLabelMap","c1b4862b":"# Previously we found that there was class imbalance. \n# We shall use class weights to tackle this before moving to training.\n\ntotal_wt = sum(class_len.values())\n\nweights = {\n    0: 1 - class_len[labels[0]]\/total_wt,\n    1: 1 - class_len[labels[1]]\/total_wt\n}\nweights","ed77cda4":"# Initialising the CNN\n\n# Use base model\nbasemodel = MobileNetV2(\n    weights=\"imagenet\", \n    include_top=False,\n    input_tensor=Input(shape=(img_size, img_size, 3))\n)\n\n# Fine tuning\nbasemodel.trainable = True\n\n# Add classification head to the model\nheadmodel = basemodel.output\nheadmodel = GlobalAveragePooling2D()(headmodel)\nheadmodel = Flatten()(headmodel) \nheadmodel = Dense(256, activation = \"relu\")(headmodel)\nheadmodel = Dropout(0.3)(headmodel)\nheadmodel = Dense(128, activation = \"relu\")(headmodel)\nheadmodel = Dropout(0.3)(headmodel)\nheadmodel = Dense(len(labels), activation = \"softmax\")(headmodel) \n\nmodel = Model(inputs = basemodel.input, outputs = headmodel)\n\nmodel.compile(\n    optimizer=Adam(lr=0.0005), \n    loss='categorical_crossentropy', \n    metrics=['accuracy']\n)\nmodel.summary()","6cb1783a":"%%time\n\nepochs = 32\nsteps_per_epoch = train_generator.n\/\/train_generator.batch_size\nval_steps = val_generator.n\/\/val_generator.batch_size\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.1,\n    patience=2, \n    min_lr=0.00001, \n    mode='auto'\n)\n\ncheckpoint = ModelCheckpoint(\n    \"model_weights.h5\", \n    monitor='val_accuracy',\n    save_weights_only=True, \n    mode='max', \n    verbose=1\n)\n\ncallbacks = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(\n    train_generator,\n    validation_data = val_generator,\n    validation_steps = val_steps,\n    class_weight = weights,\n    steps_per_epoch = steps_per_epoch,\n    epochs = epochs,\n    callbacks = callbacks\n)","7659f855":"model_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)","6477a135":"# Plotting training and validation accuracy per epoch\n\ntrain_acc = history.history[\"accuracy\"]\nvalid_acc = history.history[\"val_accuracy\"]\n\nepochs = range(len(train_acc)) \n\nplt.plot(epochs, train_acc)\nplt.plot(epochs, valid_acc)\nplt.legend([\"Training Accuracy\", \"Validation Accuracy\"])\nplt.title(\"Training and Validation Accuracy\")","f63c5ed4":"# Plotting training and validation loss per epoch\n\ntrain_loss = history.history[\"loss\"]\nvalid_loss = history.history[\"val_loss\"]\n\nepochs = range(len(train_loss)) \n\nplt.plot(epochs, train_loss)\nplt.plot(epochs, valid_loss)\nplt.legend([\"Training Loss\", \"Validation Loss\"])\nplt.title(\"Training and Validation Loss\")","8cadb474":"# Confusion Matrix \n\n# Since we do not have a lot of data, we did not split into training-validation-testing.\n# Instead we split into training-validation.\n# Strictly speaking, we should verify performance against new images from testing dataset.\n# However, we shall use images in validation dataset for testing. \n\n# There is one problem. Previously, we set shuffle = True in our generator.\n# This makes it difficult to obtain predictions and their corresponding ground truth labels.\n# Thus, we shall call the generator again, but this time set shuffle = False.\n\nval_generator = datagen_val.flow_from_directory(\n    directory = path,\n    classes = labels,\n    target_size=(img_size, img_size),\n    seed = 42,\n    batch_size = batch, \n    shuffle = False,\n    subset = 'validation'\n)\n\n# Obtain actual labels\nactual = val_generator.classes\n\n# Obtain predictions\npred = model.predict_generator(val_generator) # Gives class probabilities\npred = np.round(pred) # Gives one-hot encoded classes\npred = np.argmax(pred, axis = 1) # Gives class labels\n    \n# Now plot matrix\ncm = confusion_matrix(actual, pred, labels = list(LabelMap.values()))\nsns.heatmap(\n    cm, \n    cmap=\"Blues\",\n    annot = True, \n    fmt = \"d\"\n)\nplt.show()","bb357a45":"# Classification Report\nprint(classification_report(actual, pred))","656e853b":"# Store 10 images with their predicted and actual lables\nimg_list = []\ntitle_list = []\nfor i in range(10):    \n    X, y = next(val_generator)\n    image = X[0]\n    actual = labels[np.argmax(y[0])]\n    \n    img = image.reshape(1, img_size, img_size, 3) # Simple pre-processing. Sometimes it can be more complex than this.\n    predict = model.predict(img)\n    predict = np.argmax(predict)\n    predict = labels[predict]\n        \n    img_list.append(img)\n    title_list.append(\"Actual: \" + actual + \" , Predicted: \" + predict)","24c6d500":"# Plot images with their predicted and actual lables\nimg_list = np.array(img_list)\ntitle_list = np.array(title_list)\n\nimg_list = img_list.reshape(2, 5, 100, 100, 3)\ntitle_list = title_list.reshape(2, 5)\n\nfig, axs = plt.subplots(2, 5, figsize = (20, 20))\nfor i in range(2):\n    for j in range(5):\n        axs[i, j].imshow(img_list[i, j])\n        axs[i, j].set(xlabel = title_list[i, j], xticks = [], yticks = [])\n        \nfig.tight_layout()","2ef0af26":"### View performance","1f7a116e":"#### CREDITS: \n#### 1. Snehan Kekre - Facial Expression Recognition with Keras on Coursera Project Network\n#### 2. Chandrika Deb - https:\/\/github.com\/chandrikadeb7\/Face-Mask-Detection\n#### 3. Prajna Bhandary - https:\/\/github.com\/prajnasb\/observations\n#### 4. Adrian Rosebrock - https:\/\/www.pyimagesearch.com\/2020\/05\/04\/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning\/","a3858ad5":"### Train and Evaluate Model","a629966b":"### Plot Sample Images","f769c3df":"### Plot accuracy and loss","0f140e5c":"### Model Evaluation","88d2005a":"### Represent Model as JSON String","fb476898":"<h2 align=center> Face Mask Detection<\/h2>","6ff3448d":"### Import Libraries","1a14647c":"### Generate Training and Validation Batches","f7c02aae":"### Create CNN Model"}}