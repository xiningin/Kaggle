{"cell_type":{"09ee81bd":"code","832e0d77":"code","1cc2da9e":"code","d0c86d92":"code","942efdfd":"code","ee05a810":"code","d2102708":"code","ce74b1c4":"code","fb45aeb4":"code","cba178c7":"code","ce9b56a3":"code","626f4b29":"code","2672ba9c":"code","dbec5a82":"code","ef78dd98":"code","d59e7606":"code","e4228335":"code","8d4487c7":"code","51d45f3d":"code","31fcede4":"code","ad886fd3":"code","7f0dea24":"code","e1439391":"code","5cf3570a":"code","82aafe14":"code","d30f56f3":"code","fd9ac64d":"code","05b64166":"code","0a2d7f59":"code","8e75a35d":"code","3d8b5d29":"markdown","f77e7c7c":"markdown","d0e8f866":"markdown","a7c05762":"markdown","934d2735":"markdown","c84e2877":"markdown","066dd2e8":"markdown"},"source":{"09ee81bd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport warnings\nimport os \nwarnings.filterwarnings(\"ignore\")\nimport datetime\n","832e0d77":"data=pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\n","1cc2da9e":"data.head()      #displaying the head of dataset they gives the 1st to 5 rows of the data","d0c86d92":"data.describe()      #description of dataset ","942efdfd":"data.info()","ee05a810":"data.shape       #569 rows and 33 columns","d2102708":"data.columns     #displaying the columns of dataset","ce74b1c4":"data.value_counts","fb45aeb4":"data.dtypes","cba178c7":"data.isnull().sum()","ce9b56a3":"data.drop('Unnamed: 32', axis = 1, inplace = True)\n","626f4b29":"data","2672ba9c":"data.corr()","dbec5a82":"plt.figure(figsize=(18,9))\nsns.heatmap(data.corr(),annot = True, cmap =\"Accent_r\")\n\n\n\n","ef78dd98":"sns.barplot(x=\"id\", y=\"diagnosis\",data=data[160:190])\nplt.title(\"Id vs Diagnosis\",fontsize=15)\nplt.xlabel(\"Id\")\nplt.ylabel(\"Diagonis\")\nplt.show()\nplt.style.use(\"ggplot\")\n","d59e7606":"sns.barplot(x=\"radius_mean\", y=\"texture_mean\", data=data[170:180])\nplt.title(\"Radius Mean vs Texture Mean\",fontsize=15)\nplt.xlabel(\"Radius Mean\")\nplt.ylabel(\"Texture Mean\")\nplt.show()\nplt.style.use(\"ggplot\")\n","e4228335":" \nmean_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nsns.pairplot(data[mean_col],hue = 'diagnosis', palette='Accent')\n","8d4487c7":"sns.violinplot(x=\"smoothness_mean\",y=\"perimeter_mean\",data=data)","51d45f3d":"plt.figure(figsize=(14,7))\nsns.lineplot(x = \"concavity_mean\",y = \"concave points_mean\",data = data[0:400], color='green')\nplt.title(\"Concavity Mean vs Concave Mean\")\nplt.xlabel(\"Concavity Mean\")\nplt.ylabel(\"Concave Points\")\nplt.show()\n\n","31fcede4":"worst_col = ['diagnosis','radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\n\nsns.pairplot(data[worst_col],hue = 'diagnosis', palette=\"CMRmap\")","ad886fd3":"# Getting Features\n\nx = data.drop(columns = 'diagnosis')\n\n# Getting Predicting Value\ny = data['diagnosis']\n","7f0dea24":"\n#train_test_splitting of the dataset\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n","e1439391":"print(len(x_train))\n","5cf3570a":"print(len(x_test))","82aafe14":"print(len(y_train))","d30f56f3":"print(len(y_test))","fd9ac64d":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(x_train,y_train)                         \n","05b64166":"y_pred=reg.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",reg.score(x_train,y_train)*100)\n\n\n","0a2d7f59":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata\n\n\n\n\n","8e75a35d":"print(accuracy_score(y_test,y_pred)*100)","3d8b5d29":"# 1. Logistic Regression","f77e7c7c":"# TRAINING AND TESTING DATA","d0e8f866":"**So we get a accuracy score of 58.7 % using logistic regression**","a7c05762":"**So we have to drop the Unnamed: 32 coulumn which contains NaN values**","934d2735":"# LOADING THE DATASET","c84e2877":"# MODELS","066dd2e8":"# VISUALIZING THE DATA"}}