{"cell_type":{"c9bd36e0":"code","05cb1ebf":"code","3ec78c94":"code","68c0b226":"code","ed590fd5":"code","dbeab779":"code","59404b1e":"code","b37c99f4":"code","eb7e02f8":"markdown","04018e08":"markdown","76569ce3":"markdown","9de4dfa4":"markdown","50a3201b":"markdown","4dcaea31":"markdown","092f7c4c":"markdown","61bf206f":"markdown","2ce90169":"markdown","cd324126":"markdown"},"source":{"c9bd36e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","05cb1ebf":"dataset = pd.read_csv(r\"..\/input\/classification-of-malwares\/ClaMP_Integrated-5184.csv\")\n\n#------------------------------------------------------------------------------------------------\n#Summary\nprint('Total Shape :',dataset.shape)\ndataset.head()","3ec78c94":"type_df = pd.DataFrame(dataset.dtypes).reset_index()\ntype_df.columns=['cols','type']\ntype_df[type_df['type']=='object']['cols'].unique()\n\n#------------------------------------------------------------------------------------\nprint('Total unique values in \"packer_type\":',dataset['packer_type'].nunique())\n#------------------------------------------------------------------------------------\n#Extracting the required levels only, based on value counts. \npacker_unique_df = pd.DataFrame(dataset['packer_type'].value_counts()).reset_index()\npacker_unique_df.columns = ['packer_type','unique_count']\ncatg = packer_unique_df[packer_unique_df['unique_count']>10]['packer_type'].unique()\n#------------------------------------------------------------------------------------\nencoded = pd.get_dummies(dataset['packer_type'])\nencoded = encoded[[col for col in list(encoded.columns) if col in catg]]\nprint('Shape of encode :',encoded.shape)\n#------------------------------------------------------------------------------------\n#Concatenating the encoded columns\nif set(catg).issubset(set(dataset.columns))==False: #Conditional automation \n    dataset = pd.concat([dataset,encoded],axis=1)\n    dataset.drop(columns='packer_type',inplace=True)\n\ndataset.shape","68c0b226":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Test Train Split for modelling purpose\nX = dataset.loc[:,[cols for cols in dataset.columns if ('class' not in cols)]] #Removing time since its a level column\ny = dataset.loc[:,[cols for cols in dataset.columns if 'class' in cols]]\n\n#----------------------------------------------------------------------------------------------------\n#Scaling the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n#Splitting data into train-test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,random_state=100)\n\n#----------------------------------------------------------------------------------------------------\nprint('Total Shape of Train X:',X_train.shape)\nprint('Total Shape of Train Y:',y_train.shape)\nprint('Total Shape of Test X:',X_test.shape)\n\n#----------------------------------------------------------------------------------------------------\n\nX_arr = np.array(X_train)\nX_test_arr = np.array(X_test)\n\ny_arr = np.array(y_train).reshape(len(y_train),1)\ny_test_arr = np.array(y_test).reshape(len(y_test),1)\n\n#----------------------------------------------------------------------------------------------------\nprint(X_arr.shape)\nprint(X_test_arr.shape)\nprint(y_arr.shape)","ed590fd5":"#distance calculation udf\ndef minkowski_(point_a,point_b,p=2):\n    \n    if p==1:\n        print('----> Manhattan')\n        dist = np.sum(abs(point_a-point_b))\n        print('Manual Distance :',dist)\n    elif p==2:\n        #print('----> Euclidean')\n        dist = np.sqrt(np.sum(np.square(point_a-point_b)))\n        #print('Manual Distance :',dist)\n        \n    return dist\n\n#------------------------------------------------------------------\n#Calculate distance from one point to all other points including itself\ndef distance_to_all(curr_vec,data,p_=2):\n\n    distance_list = []\n\n    for vec_idx in range(len(data)):\n        dist = minkowski_(point_a=curr_vec,point_b=data[vec_idx],p=p_)\n        distance_list.append(dist)\n\n    return distance_list","dbeab779":"predictions = []\nprobabilities = []\n\ndef knn_model(data_x=X_arr,data_y=y_arr,k=10,curr_vec_=X_test_arr[34],mode='predict',threshold=0.5):\n\n    #print('#--------------------------------------------------------------------------------')\n    #Calculating distance of that point to every other point\n    distance_list = distance_to_all(curr_vec=curr_vec_,data=data_x,p_=2)\n    distance_list_reshaped = np.array(distance_list).reshape(len(distance_list),1)\n\n    #print('#--------------------------------------------------------------------------------')\n    #Creating a unified array for ease of indexing\n    array_final = data_x\n    array_final = np.append(array_final,data_y,axis=-1)\n    array_final = np.append(array_final,distance_list_reshaped,axis=-1) #Appending distances\n    \n    #Sorting the datapoints by the distance column\n    array_final_argsorted = array_final[array_final[:, -1].argsort()]\n\n    if mode=='train':\n\n        array_final_argsorted_top_k = array_final_argsorted[1:k+1,-2] #k+1 as the minimum distance is always 0 (with itself)\n        ratio_ = np.sum(array_final_argsorted_top_k)\/k #Total density around the point\n\n        if ratio_>threshold:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n            \n    elif mode=='predict':\n\n        array_final_argsorted_top_k = array_final_argsorted[0:k,-2] #Not k+1 since test data is not present in the training data (0 dist doesnt occur)\n        ratio_ = np.sum(array_final_argsorted_top_k)\/k\n\n        if ratio_>threshold:\n            pred = 1\n        else:\n            pred = 0\n\n    return pred,ratio_    ","59404b1e":"predictions = [] #Initializing predictions tray for each test datapoint\nprobabilities = [] #Initializing prediction probability tray for each test datapoint\n\nfor idx in range(len(X_test)): #Iterating for datapoint in test data\n    #print('#-------------- ',idx,' --------------#')\n    pred,prob = knn_model(data_x=X_arr,data_y=y_arr,\n                          k=5,curr_vec_=X_test_arr[idx],\n                          mode='predict',threshold=0.5)\n    \n    predictions.append(pred) #Appending into the tray\n    probabilities.append(prob) #Appending into the tray\n    \n    \n#-----------------------------------------------\n#Evaluating the predictions from the KNN model\nscore = roc_auc_score(y_test_arr, predictions)\nprint('1. ROC AUC: %.3f' % score)\nprint('2. Accuracy :',accuracy_score(y_test_arr, predictions))\nprint('3. Classification Report -\\n',classification_report(y_test_arr, predictions))\nprint('4. Confusion Matrix - \\n',confusion_matrix(y_test_arr, predictions))","b37c99f4":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_arr,y_arr)\nsklearn_preds = knn.predict(X_test_arr)\n\n#------------------------------------------------------------------------------------\nscore = roc_auc_score(y_test_arr, sklearn_preds)\nprint('1. ROC AUC: %.3f' % score)\nprint('2. Accuracy :',accuracy_score(y_test_arr, sklearn_preds))\nprint('3. Classification Report -\\n',classification_report(y_test_arr, sklearn_preds))\nprint('4. Confusion Matrix - \\n',confusion_matrix(y_test_arr, sklearn_preds))","eb7e02f8":"## UDF for KNN operation","04018e08":"# Separating the target column for our analysis (Since it is an dependent column in this use-case) and scaling the data (standard scaler)","76569ce3":"# Testing the major dimensionality reduction techniques on a new dataset - The malware classification dataset from the location :\nhttps:\/\/www.kaggle.com\/saurabhshahane\/classification-of-malwares","9de4dfa4":"# END","50a3201b":"## Utility UDF's regarding distance calculation","4dcaea31":"## Insights : \n1. The manual implementation gives exact result as the sklearn implementation suggesting that the implementation is correct","092f7c4c":"## Invoking the UDF for KNN for test data","61bf206f":"# Sklearn implementation for benchmarking","2ce90169":"# Pre-Processing the data to enable one-hot encoding on the categorical columns","cd324126":"# KNN from scratch"}}