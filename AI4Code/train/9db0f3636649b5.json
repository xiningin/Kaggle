{"cell_type":{"2d77146e":"code","d6641833":"code","d4475444":"code","b3af95d0":"code","1f3bcbbb":"code","abd186e7":"code","1b82f6c5":"code","5299d186":"code","c9b7d1c6":"code","3fb287cd":"markdown","2dfde498":"markdown","fdf73e96":"markdown","c83a216f":"markdown"},"source":{"2d77146e":"!pip install pytorch-transformers","d6641833":"import random\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nfrom pytorch_transformers import BertModel, BertTokenizer, BertConfig, WarmupLinearSchedule\nimport re\nimport pandas as pd\nimport json\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom tqdm import tqdm, trange\n\ndef seed_everything(seed = 42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n# For reproducible results\nseed_everything()","d4475444":"# Constants\n\nSEP_TOKEN = '[SEP]'\nCLS_TOKEN = '[CLS]'\nTRAIN_FILE_PATH = '..\/input\/Sarcasm_Headlines_Dataset_v2.json'\nMAX_SEQ_LENGTH = 512\nBATCH_SIZE = 4\nNUM_EPOCHS = 6\nGRADIENT_ACCUMULATION_STEPS = 8\nWARMUP_STEPS = 3\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(DEVICE)","b3af95d0":"class BertClassifier(nn.Module):\n\n    def __init__(self, config):\n        super(BertClassifier, self).__init__()\n        # Binary classification problem (num_labels = 2)\n        self.num_labels = config.num_labels\n        # Pre-trained BERT model\n        self.bert = BertModel(config)\n        # Dropout to avoid overfitting\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        # A single layer classifier added on top of BERT to fine tune for binary classification\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        # Weight initialization\n        torch.nn.init.xavier_normal_(self.classifier.weight)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None,\n                position_ids=None, head_mask=None):\n        # Forward pass through pre-trained BERT\n        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n                            attention_mask=attention_mask, head_mask=head_mask)\n        \n        # Last layer output (Total 12 layers)\n        pooled_output = outputs[-1]\n\n        pooled_output = self.dropout(pooled_output)\n        return self.classifier(pooled_output)","1f3bcbbb":"class SequenceDataset(Dataset):\n    def __init__(self, dataset_file_path, tokenizer, regex_transformations={}):\n        # Read JSON file and assign to headlines variable (list of strings)\n        df = pd.read_json(dataset_file_path, lines=True)\n        df = df.drop(['article_link'], axis=1)\n        self.headlines = df.values\n        # Regex Transformations can be used for data cleansing.\n        # e.g. replace \n        #   '\\n' -> ' ', \n        #   'wasn't -> was not\n        self.regex_transformations = regex_transformations\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.headlines)\n\n    def __getitem__(self, index):\n        headline, is_sarcastic = self.headlines[index]\n        for regex, value_to_replace_with in self.regex_transformations.items():\n            headline = re.sub(regex, value_to_replace_with, headline)\n\n        # Convert input string into tokens with the special BERT Tokenizer which can handle out-of-vocabulary words using subgrams\n        # e.g. headline = Here is the sentence I want embeddings for.\n        #      tokens = [here, is, the, sentence, i, want, em, ##bed, ##ding, ##s, for, .]\n        tokens = self.tokenizer.tokenize(headline)\n\n        # Add [CLS] at the beginning and [SEP] at the end of the tokens list for classification problems\n        tokens = [CLS_TOKEN] + tokens + [SEP_TOKEN]\n        # Convert tokens to respective IDs from the vocabulary\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n\n        # Segment ID for a single sequence in case of classification is 0. \n        segment_ids = [0] * len(input_ids)\n\n        # Input mask where each valid token has mask = 1 and padding has mask = 0\n        input_mask = [1] * len(input_ids)\n\n        # padding_length is calculated to reach max_seq_length\n        padding_length = MAX_SEQ_LENGTH - len(input_ids)\n        input_ids = input_ids + [0] * padding_length\n        input_mask = input_mask + [0] * padding_length\n        segment_ids = segment_ids + [0] * padding_length\n\n        assert len(input_ids) == MAX_SEQ_LENGTH\n        assert len(input_mask) == MAX_SEQ_LENGTH\n        assert len(segment_ids) == MAX_SEQ_LENGTH\n\n        return torch.tensor(input_ids, dtype=torch.long, device=DEVICE), \\\n               torch.tensor(segment_ids, dtype=torch.long, device=DEVICE), \\\n               torch.tensor(input_mask, device=DEVICE), \\\n               torch.tensor(is_sarcastic, dtype=torch.long, device=DEVICE)","abd186e7":"# Load BERT default config object and make necessary changes as per requirement\nconfig = BertConfig(hidden_size=768,\n                    num_hidden_layers=12,\n                    num_attention_heads=12,\n                    intermediate_size=3072,\n                    num_labels=2)\n\n# Create our custom BERTClassifier model object\nmodel = BertClassifier(config)\nmodel.to(DEVICE)\n\n# Initialize BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","1b82f6c5":"# Load Train dataset and split it into Train and Validation dataset\ntrain_dataset = SequenceDataset(TRAIN_FILE_PATH, tokenizer)\n\nvalidation_split = 0.2\ndataset_size = len(train_dataset)\nindices = list(range(dataset_size))\nsplit = int(np.floor(validation_split * dataset_size))\nshuffle_dataset = True\n\nif shuffle_dataset :\n    np.random.shuffle(indices)\ntrain_indices, val_indices = indices[split:], indices[:split]\n\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalidation_sampler = SubsetRandomSampler(val_indices)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\nval_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=validation_sampler)\n\nprint ('Training Set Size {}, Validation Set Size {}'.format(len(train_indices), len(val_indices)))","5299d186":"# Loss Function\ncriterion = nn.CrossEntropyLoss()\n\n# Adam Optimizer with very small learning rate given to BERT\noptimizer = torch.optim.Adam([\n                {'params': model.bert.parameters(), 'lr' : 1e-5},\n                {'params': model.classifier.parameters(), 'lr': 3e-4}\n            ])\n\n# Learning rate scheduler\nscheduler = WarmupLinearSchedule(optimizer, warmup_steps=WARMUP_STEPS, t_total=len(train_loader) \/\/ GRADIENT_ACCUMULATION_STEPS * NUM_EPOCHS)\n\nmodel.zero_grad()\nepoch_iterator = trange(int(NUM_EPOCHS), desc=\"Epoch\")\ntraining_acc_list, validation_acc_list = [], []\n\nfor epoch in epoch_iterator:\n    epoch_loss = 0.0\n    train_correct_total = 0\n\n    # Training Loop\n    train_iterator = tqdm(train_loader, desc=\"Train Iteration\")\n    for step, batch in enumerate(train_iterator):\n        model.train(True)\n        # Here each element of batch list refers to one of [input_ids, segment_ids, attention_mask, labels]\n        inputs = {\n            'input_ids': batch[0].to(DEVICE),\n            'token_type_ids': batch[1].to(DEVICE),\n            'attention_mask': batch[2].to(DEVICE)\n        }\n\n        labels = batch[3].to(DEVICE)\n        logits = model(**inputs)\n\n        loss = criterion(logits, labels) \/ GRADIENT_ACCUMULATION_STEPS\n        loss.backward()\n        epoch_loss += loss.item()\n\n        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n            scheduler.step()\n            optimizer.step()\n            model.zero_grad()\n\n        _, predicted = torch.max(logits.data, 1)\n        correct_reviews_in_batch = (predicted == labels).sum().item()\n        train_correct_total += correct_reviews_in_batch\n    \n    print('Epoch {} - Loss {:.2f}'.format(epoch + 1, epoch_loss \/ len(train_indices)))\n\n    # Validation Loop\n    with torch.no_grad():\n        val_correct_total = 0\n        model.train(False)\n        val_iterator = tqdm(val_loader, desc=\"Validation Iteration\")\n        for step, batch in enumerate(val_iterator):\n            inputs = {\n                'input_ids': batch[0].to(DEVICE),\n                'token_type_ids': batch[1].to(DEVICE),\n                'attention_mask': batch[2].to(DEVICE)\n            }\n\n            labels = batch[3].to(DEVICE)\n            logits = model(**inputs)\n\n            _, predicted = torch.max(logits.data, 1)\n            correct_reviews_in_batch = (predicted == labels).sum().item()\n            val_correct_total += correct_reviews_in_batch\n\n        training_acc_list.append(train_correct_total * 100 \/ len(train_indices))\n        validation_acc_list.append(val_correct_total * 100 \/ len(val_indices))\n        print('Training Accuracy {:.4f} - Validation Accurracy {:.4f}'.format(train_correct_total * 100 \/ len(train_indices), val_correct_total * 100 \/ len(val_indices)))","c9b7d1c6":"import matplotlib.pyplot as plt\n\nepochs_list = list(range(1, NUM_EPOCHS + 1))\nplt.plot(epochs_list, training_acc_list, color='g')\nplt.plot(epochs_list, validation_acc_list, color='orange')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training vs Validation Accuracy')\nplt.show()","3fb287cd":"Fortunately, the implementation of BERT and other transformer architectures in PyTorch is already done by a group of amazing people at HuggingFace.\n\n> https:\/\/github.com\/huggingface\/pytorch-transformers\n\nWe just need to install the library **pytorch-transformers** and we can get started with the state-of-the-art pre-trained networks in no time.","2dfde498":"**Github Repository**\n> https:\/\/github.com\/mabdullah1994\/Text-Classification-with-BERT-PyTorch","fdf73e96":"**Bidirectional Encoder Representations from Transformers (BERT)**\n\nBERT is the state-of-the-art pre-trained model for Natural Language Processing which combines context from both directions along with the famous self-attention based architecture, Transformers, while processing a text sequence. BERT has turned out to be extremely powerful and can outperform almost all other models merely by adding and fine tuning a shallow network on top of it. \n\n> https:\/\/arxiv.org\/abs\/1810.04805","c83a216f":"**Conclusion and Key Points**\n\n1. Shallow network on top of BERT for reasonable accuracy.\n2. No pre-processing done.\n3. Performs well even with less data.\n4. Bidirectional Context\n5. Sub-grams Tokenizer\n6. Self Attention"}}