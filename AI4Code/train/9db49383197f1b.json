{"cell_type":{"e5948caa":"code","1c560bcc":"code","35f60488":"code","78208ccb":"code","f9c7503f":"code","0533faec":"code","6a80b19a":"code","1c7f9d06":"markdown","9dd6c053":"markdown","a5ab77f7":"markdown","0de5ac00":"markdown","086fde9b":"markdown","79f5ba9b":"markdown","da59962c":"markdown"},"source":{"e5948caa":"import pyarrow as pa\nimport pyarrow.parquet as pq","1c560bcc":"table = pq.read_table('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0\/c439ef22282f412ba39e9137a3fdabac.parquet')\ndf = table.to_pandas()\ndf.head()\n","35f60488":"%%time\ndataset = pq.ParquetDataset('..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/')  \ntable = dataset.read()\ntrades = table.to_pandas()\ntrades.info()","78208ccb":"dataset = pq.ParquetDataset('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/') \nbooks = dataset.read()\nbooks = books.to_pandas()  # I overwrite the pyarrow table object here to save memory\nbooks.info()","f9c7503f":"print(f'Found {books.time_id.nunique()} unique time ids')\nprint(f'Found {books.stock_id.nunique()} unique stock ids.')","0533faec":"dataset = pq.ParquetDataset('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/', \n                            filters =[('stock_id', '=', '5')]) \ntable = dataset.read()\nstock = table.to_pandas()\nprint(f'Found {stock.time_id.nunique()} unique time ids')\nprint(f'Found {stock.stock_id.nunique()} unique stock ids.')","6a80b19a":"dataset = pq.ParquetDataset('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/', \n                            use_legacy_dataset=False,\n                            filters =[('time_id', '=', '5')]) \ntable = dataset.read()\nstock = table.to_pandas()\nprint(f'Found {stock.time_id.nunique()} unique time ids')\nprint(f'Found {stock.stock_id.nunique()} unique stock ids.')","1c7f9d06":"# I \u2764 parquet\n\nParquet is my favorite file format for large amounts of columnar data.  Here are a few patterns to help you get started!  The [Apache parquet docs](https:\/\/arrow.apache.org\/docs\/python\/parquet.html) are also helpful.","9dd6c053":"We just loaded ALL the trades for every stock!  The *partition key* became the categorical column `stock_id` automatically! You can do this for the book data too.","a5ab77f7":"Note that I used the option `use_legacy_dataset=False`.  This is required to filter on something other than *partition keys*. ","0de5ac00":"My kernel shows 7 out of 16GB used which might cause problems when you start unleashing your pandas magic.  One way to save memory, which you already know, is to load one stock at a time.  But you might not know there's a way to get a single stock using the `filters` option!","086fde9b":"When reading parquet files I like to use `pq.read_table`.  You can also use `pd.read_table` as shown in the [competition introduction notebook](https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data]).","79f5ba9b":"The competition has been structured with *partitioned* datasets.  That means the value for `stock_id` is in the path and not in the file.  Its called the *partition key*.  The `pyarrow.parquet` library has a `ParquetDataset` that takes advantage of partitions.","da59962c":"You could have also iterated though the files, but this way the `stock_id` column is already in the dataframe.  The `filter` option can be used to select rows for any column.  Here we get every stock for `time_id=5`."}}