{"cell_type":{"0d8448f9":"code","250418af":"code","b0829a0d":"code","cf7612e4":"code","bbf154cf":"code","42ca414b":"code","0040a793":"code","0e7aa8d9":"code","d391e375":"code","647e8077":"code","a89675fe":"code","87267a6a":"code","288641db":"code","09760b6a":"code","8e5e6f8c":"code","39e278a8":"code","eb5a2eee":"code","4793eaae":"code","5d4d8579":"code","86083b3e":"code","4d9089e2":"code","412edb89":"code","3d42167e":"code","2616b4e7":"code","98d3bb78":"code","bc908479":"code","1932466d":"code","5dc5700d":"code","f63ca99b":"code","fd0354de":"code","c2d8a3d7":"code","cbd1de4b":"code","dbc24432":"code","3f263caf":"markdown","23e12964":"markdown","3ed93b2c":"markdown","481e3336":"markdown","598cf922":"markdown","4c098f3d":"markdown","576f3637":"markdown","086c3c09":"markdown","d5c5f400":"markdown","4c32f431":"markdown","29882d73":"markdown","c1e7bc41":"markdown","840e4138":"markdown","e3d7ed1a":"markdown","f682e0a1":"markdown","8f215fd4":"markdown","d2b4d893":"markdown","56ee89cd":"markdown"},"source":{"0d8448f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport gc\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n\n# Any results you write to the current directory are saved as output.\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler\n\nfrom tqdm import tqdm\n\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.utils import class_weight\n\nfrom sklearn.metrics import accuracy_score, make_scorer\nfrom sklearn.metrics import roc_curve, auc, accuracy_score, cohen_kappa_score\nfrom sklearn.metrics import mean_squared_error, f1_score, confusion_matrix","250418af":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        if col != 'time':\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef get_stats(df):\n    stats = pd.DataFrame(index=df.columns, columns=['na_count', 'n_unique', 'type', 'memory_usage'])\n    for col in df.columns:\n        stats.loc[col] = [df[col].isna().sum(), df[col].nunique(dropna=False), df[col].dtypes, df[col].memory_usage(deep=True, index=False) \/ 1024**2]\n    stats.loc['Overall'] = [stats['na_count'].sum(), stats['n_unique'].sum(), None, df.memory_usage(deep=True).sum() \/ 1024**2]\n    return stats\n\ndef print_header():\n    print('col         conversion        dtype    na    uniq  size')\n    print()\n    \ndef print_values(name, conversion, col):\n    template = '{:10}  {:16}  {:>7}  {:2}  {:6}  {:1.2f}MB'\n    print(template.format(name, conversion, str(col.dtypes), col.isna().sum(), col.nunique(dropna=False), col.memory_usage(deep=True, index=False) \/ 1024 ** 2))","b0829a0d":"def display_set(df, column, n_sample, figsize ):\n    f, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = figsize )\n    sns.lineplot(x= df.index[::n_sample], y = df[column][::n_sample], ax=ax1)\n","cf7612e4":"# Showing Confusion Matrix\n# Thanks to https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title):\n    figsize=(14,14)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","bbf154cf":"def get_class_weight(classes, exp=1):\n    '''\n    Weight of the class is inversely proportional to the population of the class.\n    There is an exponent for adding more weight.\n    '''\n    hist, _ = np.histogram(classes, bins=np.arange(12)-0.5)\n    class_weight = hist.sum()\/np.power(hist, exp)\n    \n    return class_weight","42ca414b":"PATH = '\/kaggle\/input\/data-without-drift\/'\n#PATH = '\/kaggle\/input\/liverpool-ion-switching\/'\n\ntrain = pd.read_csv(PATH + 'train_clean.csv')\ntest = pd.read_csv(PATH + 'test_clean.csv')\n\ntrain.head()","0040a793":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","0e7aa8d9":"## Most of the FE have been taken from: \n## - https:\/\/www.kaggle.com\/vbmokin\/ion-switching-advanced-fe-lgb-xgb-confmatrix","d391e375":"WINDOW_SIZES = [3, 5, 10, 50, 100, 500]","647e8077":"%%time\n\ndef gen_roll_features(full, win_sizes = WINDOW_SIZES):\n    for window in tqdm(win_sizes):\n        full[\"rolling_mean_\" + str(window)] = full['signal'].rolling(window=window).mean()\n        full[\"rolling_std_\" + str(window)] = full['signal'].rolling(window=window).std()\n        full[\"rolling_var_\" + str(window)] = full['signal'].rolling(window=window).var()\n        full[\"rolling_min_\" + str(window)] = full['signal'].rolling(window=window).min()\n        full[\"rolling_max_\" + str(window)] = full['signal'].rolling(window=window).max()\n\n        a = (full['signal'] - full['rolling_min_' + str(window)]) \/ (full['rolling_max_' + str(window)] - full['rolling_min_' + str(window)])\n        full[\"norm_\" + str(window)] = a * (np.floor(full['rolling_max_' + str(window)]) - np.ceil(full['rolling_min_' + str(window)]))\n    \n    full = full.replace([np.inf, -np.inf], np.nan)    \n    full.fillna(0, inplace=True)\n    return full\n\ntrain = gen_roll_features(train)\ntest = gen_roll_features(test)","a89675fe":"%%time\n\nGROUP_BATCH_SIZE = 8000\n\n# create batches of GROUP_BATCH_SIZE observations\ndef batching(df, batch_size, gr_name='group'):\n    df[gr_name] = df.groupby(df.index\/\/batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df[gr_name] = df[gr_name].astype(np.uint16)\n    return df\n\ndef run_feat_engineering(df, batch_size, gr_name='group'):\n    df = batching(df, batch_size = batch_size, gr_name=gr_name)\n    df['signal_2'] = df['signal'] ** 2\n    df['signal_2-7500-mean'] = df['signal_2'] - df['signal_2'].rolling(window=7500).mean()    \n    return df\n\ntrain = run_feat_engineering(train, batch_size = GROUP_BATCH_SIZE, gr_name='group')\ntest = run_feat_engineering(test, batch_size = GROUP_BATCH_SIZE, gr_name='group')","87267a6a":"%%time\n\n## add some noise\n\nSTD = 0.01\n\nold_data = train['signal']\nnew_data = old_data + np.random.normal(0,STD,size=len(train)) \ntrain['signal'] = new_data\n\nold_data = test['signal']\nnew_data = old_data + np.random.normal(0,STD,size=len(test)) \ntest['signal'] = new_data\n\ndel old_data, new_data","288641db":"%%time\n\ndef gen_roll_features(full, win_sizes = WINDOW_SIZES):\n    for window in tqdm(win_sizes):\n        full[\"rolling_mean_\" + str(window)] = full['signal'].rolling(window=window).mean()\n        full[\"rolling_std_\" + str(window)] = full['signal'].rolling(window=window).std()\n        full[\"rolling_var_\" + str(window)] = full['signal'].rolling(window=window).var()\n        full[\"rolling_min_\" + str(window)] = full['signal'].rolling(window=window).min()\n        full[\"rolling_max_\" + str(window)] = full['signal'].rolling(window=window).max()\n\n        a = (full['signal'] - full['rolling_min_' + str(window)]) \/ (full['rolling_max_' + str(window)] - full['rolling_min_' + str(window)])\n        full[\"norm_\" + str(window)] = a * (np.floor(full['rolling_max_' + str(window)]) - np.ceil(full['rolling_min_' + str(window)]))\n    return full\n\ntrain = gen_roll_features(train)\ntest = gen_roll_features(test)","09760b6a":"%%time\n\ndef gen_sig_features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index \/\/ 25_000\n    df['batch_index'] = df.index  - (df.batch * 25_000)\n    df['batch_slices'] = df['batch_index']  \/\/ 2500\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in tqdm(['batch','batch_slices2']):\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal'].mean()\n        d['median'+c] = df.groupby([c])['signal'].median()\n        d['max'+c] = df.groupby([c])['signal'].max()\n        d['min'+c] = df.groupby([c])['signal'].min()\n        d['std'+c] = df.groupby([c])['signal'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n        d['range'+c] = d['max'+c] - d['min'+c]\n        d['maxtomin'+c] = d['max'+c] \/ d['min'+c]\n        d['abs_avg'+c] = (d['abs_min'+c] + d['abs_max'+c]) \/ 2\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n    df = reduce_mem_usage(df)\n    gc.collect()\n    return df\n\ntrain = gen_sig_features(train)\ntest = gen_sig_features(test)","8e5e6f8c":"%%time\n\ndef gen_shift_features(df):\n    # add shifts\n    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-1'][i] = np.nan\n    \n    df['signal_shift_+2'] = [0,] + [1,] + list(df['signal'].values[:-2])\n    df['signal_shift_-2'] = list(df['signal'].values[2:]) + [0] + [1]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==1].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-2'][i] = np.nan\n    for i in df[df['batch_index']==49998].index:\n        df['signal_shift_-2'][i] = np.nan\n    \n    df.drop(columns=['batch', 'batch_index', 'batch_slices', 'batch_slices2'], inplace=True)\n    gc.collect()\n\n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'group' 'category', 'index']]:\n        df[c+'_msignal'] = df[c] - df['signal']\n        \n    df = df.replace([np.inf, -np.inf], np.nan)    \n    df.fillna(0, inplace=True)\n    df = reduce_mem_usage(df)\n    gc.collect()\n    return df\n\ntrain = gen_shift_features(train)\ntest = gen_shift_features(test)","39e278a8":"ALL_FEATURES = [c for c in train.columns if c not in ['time', 'signal', 'open_channels', 'group' 'category', 'index']]\ntrain.info()","eb5a2eee":"DATA_BATCH_SIZE = 500000\n\nTRAIN_SAMPLE_RATE = 100 ## for display\nTRAIN_BATCH_SIZE = int(len(train)\/TRAIN_SAMPLE_RATE)\n\nf, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = (20,4))\nsns.lineplot(data=train.signal[::TRAIN_SAMPLE_RATE], ax=ax1, hue=\"size\", size=\"size\")\nsns.lineplot(data=train.open_channels[::TRAIN_SAMPLE_RATE], ax=ax1, hue=\"size\", size=\"size\")\nax1.set_title(f'Full train signal')\n\nf, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = (10,4))\nsns.lineplot(data=test.signal[::TRAIN_SAMPLE_RATE], ax=ax1, hue=\"size\", size=\"size\")\nax1.set_title(f'Full test signal')","4793eaae":"f, axes = plt.subplots(nrows = 2, ncols = 5, figsize = (26,12))\nfor i in range(10):\n    XX = train.signal[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1]\n    yy = train.open_channels[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1]\n    sns.scatterplot(data=XX[::TRAIN_SAMPLE_RATE], ax=axes[int(i\/5), i%5], hue=\"size\", size=\"size\")\n    sns.scatterplot(data=yy[::TRAIN_SAMPLE_RATE], ax=axes[int(i\/5), i%5], hue=\"size\", size=\"size\")\n    axes[int(i\/5), i%5].set_title(f'Train Batch# {i+1}')\n    \nf, axes = plt.subplots(nrows = 1, ncols = 5, figsize = (26,6))\nfor i in range(4):\n    XX = test.signal[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1]\n    sns.scatterplot(data=XX[::TRAIN_SAMPLE_RATE], ax=axes[i], hue=\"size\", size=\"size\")\n    axes[i].set_title(f'Test Batch# {i+1}')\n","5d4d8579":"f, ax = plt.subplots(figsize=(15, 6))\nsns.countplot(x=\"open_channels\", data=train, ax=ax)","86083b3e":"f, axes = plt.subplots(nrows = 2, ncols = 5, figsize = (26,12))\nfor i in range(10):\n    y = pd.DataFrame()\n    sns.countplot( x = train.open_channels[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1], ax=axes[int(i\/5), i%5])\n    axes[int(i\/5), i%5].set_title(f'Train Batch# {i+1}')","4d9089e2":"print(f'Original sizes: train: {train.shape}, test: {test.shape}' )","412edb89":"%%time\n\n## this may take a lot of time\ncorr = train[ALL_FEATURES][::20].corr('spearman')\n\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.9:\n            if columns[j]:\n                columns[j] = False\n                \nselected_columns = train[ALL_FEATURES].columns[columns]\nprint(len(selected_columns))            \n#print(selected_columns)            ","3d42167e":"%%time\n\nfrom sklearn.feature_selection import VarianceThreshold\n\nvt = VarianceThreshold(0.5)\ny = train['open_channels'].values\nX = train[selected_columns].values\nvt.fit(X, y)\n\n## let's take top 25\ntop_idx = np.argpartition(vt.variances_, -25)[-25:]\nSELECTED_FEATURES = [selected_columns[i] for i in top_idx]\nprint(SELECTED_FEATURES)     ","2616b4e7":"## reduce amount of data to speed things up\nX_train = train[SELECTED_FEATURES]\ny_train = train['open_channels'].values\n\nprint(f'Original sizes: train: {train.shape}, test: {test.shape}' )\nprint(f'Reduced train sizes: X_train: {X_train.shape}, y_train: {y_train.shape}' )","98d3bb78":"# Thanks to https:\/\/www.kaggle.com\/siavrez\/simple-eda-model\ndef MacroF1Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average = 'macro')\n    return ('MacroF1Metric', score, True)","bc908479":"## started from here:\n## https:\/\/www.kaggle.com\/vbmokin\/ion-switching-advanced-fe-lgb-xgb-confmatrix\n\nNUM_BOOST_ROUND = 2000 \nEARLY_STOPPING_ROUNDS = 40\nVERBOSE_EVAL = 100\nRANDOM_SEED = 13\nLEARNING_RATE = 0.02\nMAX_DEPTH = -1\nNUM_LEAVES = 200\n\n\nX_train1, X_valid1, y_train1, y_valid1 = train_test_split(X_train, y_train, test_size=0.3, random_state=RANDOM_SEED)\n\nparams = {\n    'learning_rate': LEARNING_RATE, \n    'max_depth': MAX_DEPTH, \n    'num_leaves': NUM_LEAVES,\n    'metric': 'logloss', \n    'random_state': RANDOM_SEED, \n    'n_jobs':-1, \n    'sample_fraction':0.33\n    }\n\nevals_result = {}\nmodel = lgb.train(\n    params, \n    train_set=lgb.Dataset(X_train1, y_train1), \n    num_boost_round = NUM_BOOST_ROUND,\n    valid_sets = lgb.Dataset(X_valid1, y_valid1), \n    verbose_eval = VERBOSE_EVAL,\n    evals_result = evals_result,\n    early_stopping_rounds = EARLY_STOPPING_ROUNDS, \n    feval = MacroF1Metric)\n\ngc.collect()","1932466d":"f, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize=(15, 6))\nlgb.plot_metric(evals_result, metric='MacroF1Metric', ax=ax1)\nplt.show()","5dc5700d":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(model,ax = axes,height = 0.5)\nplt.show();plt.close()","f63ca99b":"y_pred_train_lgb = model.predict(X_train, num_iteration=model.best_iteration)\nprint('LGB score {0:.4f}'.format(np.mean(f1_score(y_train, np.round(np.clip(y_pred_train_lgb,0,10)).astype(int), average=\"macro\"))))\ngc.collect()","fd0354de":"plot_cm(y_train, y_pred_train_lgb, 'LGB Confusion Matrix')","c2d8a3d7":"y_hat = model.predict(test[SELECTED_FEATURES], num_iteration=model.best_iteration)\ny_pred = np.round(np.clip(y_hat,0,10)).astype(int)","cbd1de4b":"f, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = (15,6))\nsns.scatterplot(x=test[SELECTED_FEATURES].index[::1000], y=test.signal[::1000], ax=ax1)\nsns.scatterplot(x=test[SELECTED_FEATURES].index[::1000], y=y_pred[::1000], ax=ax1 )\nax1.set_title(f'Full test signal with predictions')","dbc24432":"sub = pd.read_csv('..\/input\/liverpool-ion-switching\/sample_submission.csv')\nsub['open_channels'] = y_pred\nsub.to_csv('submission.csv', index=False, float_format='%.4f')\n\nsub.head(20)","3f263caf":"## Feature selection","23e12964":"## Load train and test datasets","3ed93b2c":"## Acknowledgements\n\n### This kernel used ideas and some code from excellent notebook:\n### https:\/\/www.kaggle.com\/vbmokin\/ion-switching-advanced-fe-lgb-xgb-confmatrix\n### and from this discussion: \n### https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/discussion\/143390\n\n\n### data from\n### https:\/\/www.kaggle.com\/cdeotte\/data-without-drift","481e3336":"## LGB feature importance","598cf922":"## Distributions for open channels for each batch","4c098f3d":"## LGB Confusion Matrix","576f3637":"**IMPORTANT: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.**","086c3c09":"## By batch","d5c5f400":"## Display train and test signals","4c32f431":"Identify the number of channels open at each time point\n\nMany diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.\n\nWhen ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data.\n\nThe University of Liverpool\u2019s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for your help. In this competition, you\u2019ll use ion channel data to better model automatic identification methods. If successful, you\u2019ll be able to detect individual ion channel events in noisy raw signals. The data is simulated and injected with real world noise to emulate what scientists observe in laboratory experiments.","29882d73":"## LGB Model","c1e7bc41":"## Submission","840e4138":"### remove high corr values","e3d7ed1a":"### remove all low-variance features","f682e0a1":"## Plot test signal with predictions","8f215fd4":"## Utils","d2b4d893":"## Open channels value count","56ee89cd":"## Introduction"}}