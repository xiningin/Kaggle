{"cell_type":{"8ebd5667":"code","46cd3bfe":"code","f32ae18d":"code","581cd3fc":"code","d133032a":"code","679088b7":"code","dfe8ce23":"code","86eb1f35":"code","00c4eea1":"code","16494b3a":"code","712f2e46":"code","25a1ad31":"code","dd839134":"code","bea85bcf":"code","473d51ec":"code","e78773ee":"code","03d7d128":"code","e1a44735":"code","fd15aec0":"code","9fd8d994":"code","f3e45d73":"code","1b082f94":"code","3e59469b":"code","73db0532":"code","672c3d2f":"code","11a077b7":"code","6d2fbcd3":"code","14840ea9":"code","ab9510de":"code","fdfc2a47":"code","2b59390b":"code","b6a67759":"code","feb45e1c":"code","2b34906b":"code","a833a6bd":"code","e1eefa65":"code","37d68266":"code","269e9af4":"code","ca9a402d":"code","c1274ae1":"code","19860ab8":"code","09cd6c7b":"code","cea5c7ca":"code","6fc44fd5":"code","d821f0d9":"code","45ec9cd4":"code","789ed159":"code","dcbbe18a":"code","44d245ae":"code","a6822f78":"code","36da8a57":"code","e41f63fa":"markdown","34bcddab":"markdown","8180ae89":"markdown","22c94066":"markdown","80329051":"markdown","173d6ba7":"markdown","8187ea6f":"markdown","7398ce3e":"markdown","e17e76d0":"markdown","734e4d7a":"markdown","e60db5fb":"markdown","d125bfc6":"markdown","af5b6439":"markdown","4078e940":"markdown","e504e3eb":"markdown","7208b9e8":"markdown","22660a69":"markdown","2ba72d67":"markdown"},"source":{"8ebd5667":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","46cd3bfe":"sample=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ntrain=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","f32ae18d":"train_copy=train.copy()\ntest_copy=test.copy()","581cd3fc":"numeric=train.select_dtypes(include=[np.number]) #selecting the data asociate with the numerical variables","d133032a":"categorical=train.select_dtypes(exclude=[np.number]) #selecting the data asociate with the categorical variables","679088b7":"train['SalePrice'].hist()","dfe8ce23":"y=train['SalePrice']","86eb1f35":"train.drop(['SalePrice','Id'], axis=1, inplace=True)","00c4eea1":"numeric.drop(['SalePrice','Id'], axis=1, inplace=True)\nnum_columns=numeric.columns","16494b3a":"def select_skew_index(df):\n    numeric=df.select_dtypes(include=[np.number])\n    num_columns=numeric.columns\n    skew_features = df[num_columns].skew(axis = 0, skipna = True)\n    high_skewness = skew_features[skew_features > 0.5]\n    skew_index = high_skewness.index\n    return skew_index","712f2e46":"skew_index=select_skew_index(train)","25a1ad31":"train[skew_index].hist(figsize=(15,15))","dd839134":"kurt_features = train[num_columns].kurtosis(axis = 0, skipna = True)","bea85bcf":"high_kurt = kurt_features[kurt_features > 3]\nkurt_index = high_kurt.index\nhigh_kurt","473d51ec":"fig=plt.figure(figsize=(15,20))\nfor i in range(1,18):\n    ax=fig.add_subplot(6,3,i)\n    ax.scatter(x=train[kurt_index[i-1]], y=y)\n    ax.set_xlabel(kurt_index[i-1])","e78773ee":"train[train['BsmtFinSF1']>5000].index","03d7d128":"train[train['TotalBsmtSF']>6000].index","e1a44735":"train[train['GrLivArea']>5000].index\n","fd15aec0":"train.shape","9fd8d994":"train=train.drop(train.index[1298])\ny=y.drop(y.index[1298])","f3e45d73":"train.shape","1b082f94":"y=np.log1p(y)","3e59469b":"y.hist()","73db0532":"def correct_skew(df,skew_index):\n    for i in skew_index:\n        df[i] = np.log1p(df[i])","672c3d2f":"correct_skew(train,skew_index)","11a077b7":"train[skew_index].hist(figsize=(15,15))","6d2fbcd3":"train.isnull().sum().sort_values(ascending=False)[0:25]","14840ea9":"def fill_miss(df):\n    Nvalues=['FireplaceQu','GarageFinish','BsmtCond','Alley','BsmtExposure','GarageCond','PoolQC','BsmtQual',\n             'MiscFeature','MasVnrType','BsmtFinType1','GarageType','Fence','GarageQual','BsmtFinType2']\n    GarBsmt=['GarageYrBlt','GarageCars','GarageArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF',\n'BsmtFullBath','BsmtHalfBath']\n    df_cat=df.select_dtypes(exclude=[np.number])\n    stats_df=df_cat.describe() \n    for i in df.columns:\n        if(i in Nvalues):\n            df[i].replace(np.nan,\"None\", inplace=True )\n        elif(i in GarBsmt):\n            df[i].replace(np.nan,0, inplace=True )\n        \n    for i in df_cat.columns:\n        top = stats_df[i].iloc[2]\n        if(df[i].isnull().sum()!=0):\n            df[i].replace(np.nan,top, inplace=True )\n            \n    df.interpolate(inplace=True)","ab9510de":"fill_miss(train)","fdfc2a47":"train.isnull().sum()","2b59390b":"def uniq_cat(df):\n    categoric=df.select_dtypes(exclude=[np.number])\n    cat_col=categoric.columns\n    high_val=[]\n    for i in cat_col:\n        for j in range(df[i].unique().shape[0]):\n            if ((df[i].value_counts()[j])\/1459 > 0.99):\n                 high_val.append(i) \n    return high_val","b6a67759":"uniq_cat(train)","feb45e1c":"train = train.drop(['Utilities', 'Street', 'PoolQC',], axis=1)","2b34906b":"def filling_ordinal(df):\n    feat=['ExterQual','ExterCond','BsmtQual','BsmtCond','KitchenQual','HeatingQC','KitchenQual'\n          ,'HeatingQC','GarageQual','GarageCond']\n    for x in feat:  \n        df[x][df[x] == 'Ex'] = 5\n        df[x][df[x] == 'Gd'] = 4\n        df[x][df[x] == 'TA'] = 3\n        df[x][df[x] == 'Fa'] = 2\n        df[x][df[x] == 'Po'] = 1\n        df[x][df[x] == 'None'] = 0\n        \n    df['LandSlope'][df['LandSlope'] == 'Sev'] = 3\n    df['LandSlope'][df['LandSlope'] == 'Mod'] = 2\n    df['LandSlope'][df['LandSlope'] == 'Gtl'] = 1\n    \n    df['BsmtExposure'][df['BsmtExposure'] == 'Gd'] = 4\n    df['BsmtExposure'][df['BsmtExposure'] == 'Av'] = 3\n    df['BsmtExposure'][df['BsmtExposure'] == 'Mn'] = 2\n    df['BsmtExposure'][df['BsmtExposure'] == 'No'] = 1\n    df['BsmtExposure'][df['BsmtExposure'] == 'None'] = 0\n    \n    feat1=['BsmtFinType1','BsmtFinType2']\n    \n    for x in feat1:\n        df[x][df[x] == 'GLQ'] = 6\n        df[x][df[x] == 'ALQ'] = 5\n        df[x][df[x] == 'BLQ'] = 4\n        df[x][df[x] == 'Rec'] = 3\n        df[x][df[x] == 'LwQ'] = 2\n        df[x][df[x] == 'Unf'] = 1\n        df[x][df[x] == 'None'] = 0\n        \n    df['CentralAir'][df['CentralAir'] == 'Y'] = 1\n    df['CentralAir'][df['CentralAir'] == 'N'] = 0\n    ","a833a6bd":"filling_ordinal(train)","e1eefa65":"def feat_ing(X):\n    X['TotalBath']=X['BsmtFullBath']+ (1\/2)*X['BsmtHalfBath']+X['FullBath']+ (1\/2)*X['HalfBath']\n    X['TotalSF']=X['TotalBsmtSF']+X['1stFlrSF']+X['2ndFlrSF']","37d68266":"feat_ing(train)","269e9af4":"X=pd.get_dummies(train)","ca9a402d":"def clean(rtrain,rtest):\n    y=rtrain['SalePrice']\n    testId=rtest['Id']\n    rtrain.drop(['Id','SalePrice'],axis=1,inplace=True)\n    rtest.drop(['Id'],axis=1,inplace=True)\n    \n    # selecting the indexes of the skew features\n    skew_index=select_skew_index(rtrain)\n    \n    # Eliminate the outier\n    rtrain=rtrain.drop(rtrain.index[1298])\n    y=y.drop(rtrain.index[1298])\n    \n    # Drop the columns in the test data with all values equal to na\n    rtest=rtest.dropna(axis=1,how='all')\n    \n    # preparing features and target values\n    y=np.log1p(y)\n\n    #Correct the skewness\n    #correct_skew(rtrain,skew_index)\n    #correct_skew(rtest,skew_index)\n    \n    #Filling missing values\n    fill_miss(rtrain)\n    fill_miss(rtest)\n    \n    #Drop the features with low information\n    #rtrain = rtrain.drop(['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'PoolQC'], axis=1)\n    #rtest = rtest.drop(['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'PoolQC'], axis=1)\n    \n    # Correcting categorical values that are ordinal\n    filling_ordinal(rtrain)\n    filling_ordinal(rtest)\n    \n    #Feature ingeneering\n    feat_ing(rtrain)\n    feat_ing(rtest)\n    \n    #One hot encoding\n    rtrain=pd.get_dummies(rtrain)\n    rtest=pd.get_dummies(rtest)\n    \n    # Update the training set\n    rtrain=rtrain[rtest.columns]\n    \n    \n    return(rtrain,rtest,y,testId)","c1274ae1":"X,Xtest,y,TestId=clean(train_copy,test_copy)","19860ab8":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.33, random_state=77)","09cd6c7b":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nparameters= {'alpha':[0.0001,0.0002,0.0003,0.0004,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100]}\n\nlasso=Lasso()\nlasso_reg=GridSearchCV(lasso, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\nlasso_reg.fit(X,y)\n\nprint('The best value of Alpha is: ',lasso_reg.best_params_,'neg_mean_squared_error',lasso_reg.best_score_)","cea5c7ca":"from sklearn import linear_model\nfrom sklearn.linear_model import Lasso\n\nbest_alpha=0.0009\nlasso = Lasso(alpha=best_alpha,max_iter=10000)\nlasso.fit(X,y)","6fc44fd5":"ytest=lasso.predict(Xtest)\n#ytest_ridge=ridge.predict(Xtest)\n#ytest=(ytest_ridge+ytest_lasso)\/2","d821f0d9":"\nytest=np.expm1(ytest)","45ec9cd4":"#FI_lasso = pd.DataFrame({\"Feature\":X.columns, 'Importance':lasso.coef_})","789ed159":"#FI_lasso=FI_lasso.sort_values(\"Importance\",ascending=False)","dcbbe18a":"#import seaborn as sns\n#sns.barplot(x='Importance', y='Feature', data=FI_lasso.head(10),color='b')","44d245ae":"prediction=pd.DataFrame({'Id': TestId, 'SalePrice': ytest})","a6822f78":"prediction","36da8a57":"prediction.to_csv('submission.csv', index=False)","e41f63fa":"### Feature Ingeneering","34bcddab":"Also we want to look for variables which have the majority of its elements in only one category, for this we have the following function:","8180ae89":"#### Kurtosis\nIt describe the extreme values in one versus the other tail and measures the outliers present in the distribution.\n\nHigh kurtosis in a data set is an indicator that data has heavy tails or outliers. In this case we should investigate wrong data entry or other things.\n","22c94066":"### Skewness\n\nIt is the degree of distortion from the symmetrical bell curve or the normal distribution. It measures the lack of symmetry in data distribution.\n\nIf the skewness is between -0.5 and 0.5, the data are fairly symmetrical.\n\nIf the skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed), the data are moderately skewed.\n\nIf the skewness is less than -1(negatively skewed) or greater than 1(positively skewed), the data are highly skewed.\n\nAs it was expected our target variable is right skewed, therefore we have to normalize its distribution. There are many ways to approaching this but we have selected a simple one: logarithm transformation, but we are going to do this later.","80329051":"We change the variables that are ordinal according to the importance of the category. We combine the variables Condition1 and Condition2 into dummies to avoid duplicates later","173d6ba7":"First we have to look at the data description for a better understanding of the data. There are variables like Alley for which NA values means No alley access and variables like 'GarageCars' for which NA means 0.","8187ea6f":"As we can see there are 38 numerical variables including SalePrice and Id:\n\n['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n       'MiscVal', 'MoSold', 'YrSold', 'SalePrice']\n       \n Similarly there are 43 categorical variables:\n \n   ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n       'SaleType', 'SaleCondition']\n","7398ce3e":"Now it looks much better. ","e17e76d0":"### EDA: Exploratory Data Analysis\n\nAs the description suggest we probably want to have a better undestanding of the data before diving into a complex Machine Learning algorithm. We start by reading our data into a Pandas dataframe and finding the type of the variables in it.","734e4d7a":"We also eliminate the  target variable and the Id to form the feature set.","e60db5fb":"### Categorical variables\n\nNow we have to work with the categorical variables and select the ones that are ordinal and nominal to find a correct approach for them. For this we have to look the data description and find similarities between categories. We have notice that the variable MSSubClass is in fact categorical therefore we hace to change its type to object.","d125bfc6":"From these graphics we can see that there is one value in 'BsmtFinSF1','GrLivArea' and 'TotalBsmtSF' each, with high values of square feets and low sale price. That has no sense, moreover, when find this value for 'BsmtFinSF1','GrLivArea' and 'TotalBsmtSF' it turns to be the same index 1298. Therefore, 1298 its our first candidate\nto outlier.","af5b6439":"We use the skew function to find the features variables wich are skewed and select between them the ones with high values based on the criteria presented before.","4078e940":"#### Outliers\n\nAs we saw before it is a good approach to look for outlier in the variables with high values of kurtosis. To have a better idea let's draw the scatter plot of this variables.","e504e3eb":"### Exploring the Target variable; SalePrice\n\nNormally most of the people can buy houses with low prices. Now let's see if this is reflected in the histogram of our target variable.","7208b9e8":"Now we want to correct the skewness and kurtosis by applying a logarithm transformation. We start with our target variable and then continue with the features","22660a69":"#### Filling missing values\nWe start by finding the variables with nan values","2ba72d67":"## House Prices: Advanced Regression Techniques\n\nThe Notebook is based on a Kaggle competition with the following description. Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home."}}