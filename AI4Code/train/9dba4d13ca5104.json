{"cell_type":{"d15c73ec":"code","05c429fb":"code","57fadd1c":"code","78863f90":"code","55e947f1":"code","8fbdf1eb":"code","13e5a652":"code","677081aa":"code","7ae9bb86":"code","7170d753":"code","fcc1e076":"code","e71f4ef8":"code","44082e9a":"code","6474fa43":"code","865c1b71":"code","66c7b33e":"code","7e444ef7":"code","c095ec72":"code","2304cae0":"code","644ec494":"code","b1c4dd1d":"code","d4afd989":"code","95baf287":"code","0a82ac29":"code","5c9dedf9":"code","74dcb13a":"code","82ae11b3":"code","aec6d088":"code","2d1ee75c":"code","890949ee":"code","f5918d84":"markdown","cbae820a":"markdown","6375826f":"markdown","a7b941b2":"markdown","07bdc00f":"markdown","c3bdd385":"markdown","0486b250":"markdown","a1e1d98d":"markdown","1585ba89":"markdown","07e332e4":"markdown","e3fe7216":"markdown","94b22a06":"markdown","384ac4b9":"markdown","55e1fb41":"markdown","d509b420":"markdown","f269a93f":"markdown","d34a8245":"markdown","081f5594":"markdown","7e33bbcf":"markdown","ee83eb30":"markdown"},"source":{"d15c73ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport sys\nimport re\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport scipy as sp\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Python version: {}\". format(sys.version))\nprint(\"pandas version: {}\". format(pd.__version__))\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\nprint(\"NumPy version: {}\". format(np.__version__))\nprint(\"SciPy version: {}\". format(sp.__version__)) \nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\nprint('-'*25)\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\nRANDOM_SEED = 19901129 # for reproducibility\nnp.random.seed(RANDOM_SEED)","05c429fb":"train = pd.read_csv('..\/input\/train.csv', header = 0, dtype={'Age': np.float64})\ntest  = pd.read_csv('..\/input\/test.csv' , header = 0, dtype={'Age': np.float64})\nfull_data = [train, test]\nprint (train.info())","57fadd1c":"# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n# Create a New feature CategoricalAge\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n    \n    # Age * Class\n    dataset['Age*Class'] = dataset['Age'] * dataset['Pclass']","78863f90":"# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\n# Feature selection\nfeature_columns = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone', 'Age*Class']\ntrain = train[feature_columns + ['Survived']]\ntest  = test[feature_columns]\ntrain.head()","55e947f1":"print('enabled features'.upper())\nfor column_name in list(train.columns):\n    print('\\t' + column_name)","8fbdf1eb":"# visualization\ng = sns.pairplot(train[list(train.columns)], hue='Survived', palette = 'seismic',height=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","13e5a652":"# Some useful parameters which will come in handy later on\nclassifiers = []\nntrain = train.shape[0]\nntest = test.shape[0]\n\n# Cross validate model with Kfold stratified cross val\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=10)\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\nx_train = train.drop(['Survived'], axis=1).values # Creates an array of the train data\nx_train_columns = train.drop(['Survived'], axis=1).columns\nx_test = test.values # Creates an array of the test data\n\ndef classifier_name(clf):\n    if 'ABCMeta' == clf.__class__:\n        return clf.__name__\n    else:\n        return clf.__class__.__name__ ","677081aa":"from sklearn.ensemble import RandomForestClassifier\n# parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n    'warm_start': False, \n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0,\n    'random_state': RANDOM_SEED\n}\nrf = RandomForestClassifier(**rf_params)\nclassifiers.append(rf)","7ae9bb86":"from sklearn.svm import SVC\nsvm_params = {'probability': True,\n              'random_state': RANDOM_SEED}\nsvm = SVC(**svm_params)\nclassifiers.append(svm)","7170d753":"from sklearn.linear_model import LogisticRegression\n\nlr_params = {\n    'solver': 'liblinear',\n    'random_state': RANDOM_SEED\n}\n\nlr = LogisticRegression(**lr_params)\nclassifiers.append(lr)","fcc1e076":"from sklearn.neighbors import KNeighborsClassifier\nkn_params = {'n_neighbors': 3}\nkn = KNeighborsClassifier(**kn_params)\nclassifiers.append(kn)","e71f4ef8":"from sklearn.ensemble import GradientBoostingClassifier\ngb_params = {'random_state':RANDOM_SEED}\ngb = GradientBoostingClassifier(**gb_params)\nclassifiers.append(gb)","44082e9a":"from sklearn.tree import DecisionTreeClassifier\ndt_params = {'random_state':RANDOM_SEED}\ndt = DecisionTreeClassifier(**dt_params)\nclassifiers.append(dt)","6474fa43":"from sklearn.ensemble import AdaBoostClassifier\nab_params = {'random_state':RANDOM_SEED}\nab = AdaBoostClassifier(**ab_params)\nclassifiers.append(ab)","865c1b71":"from sklearn.naive_bayes import GaussianNB\ngnb_params = {}\ngnb = GaussianNB(**gnb_params)\nclassifiers.append(gnb)","66c7b33e":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nld_params = {}\nld = LinearDiscriminantAnalysis(**ld_params)\nclassifiers.append(ld)","7e444ef7":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nqd_params = {}\nqd = QuadraticDiscriminantAnalysis(**qd_params)\nclassifiers.append(qd)","c095ec72":"from xgboost import XGBClassifier\nxgb_params = {'random_state':RANDOM_SEED}\nxgb = XGBClassifier(**xgb_params)\nclassifiers.append(xgb)","2304cae0":"import keras \nfrom keras.models import Sequential # intitialize the ANN\nfrom keras.layers import Dense      # create layers\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n# Create function returning a compiled network\n_, NUMBER_OF_FEATURES = x_train.shape\n\ndef create_network():\n    # Start neural network\n    network = Sequential()\n    # layers\n    network.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_shape=(NUMBER_OF_FEATURES,)))\n    network.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\n    network.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n    network.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n    # Compile neural network\n    network.compile(loss='binary_crossentropy', # Cross-entropy\n                    optimizer='rmsprop', # Root Mean Square Propagation\n                    metrics=['accuracy']) # Accuracy performance metric\n    # Return compiled network\n    return network\n\n# Wrap Keras model so it can be used by scikit-learn\nNN_classifier = KerasClassifier(build_fn=create_network, \n                                epochs=200, \n                                batch_size=32, \n                                verbose=1)\nclassifiers.append(NN_classifier)","644ec494":"from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm_notebook as tqdm\n\ndef evaluate_with_cross_validation(classifiers, test_size, split_num, X, y):\n    ret_results = {}\n    cv_results = []\n    for classifier in tqdm(classifiers) :\n        clf_name = classifier_name(classifier)\n        print('cross validate with {0}'.format(clf_name))\n        n_jobs = None if clf_name in ['KerasClassifier'] else -1\n        cv_results.append(cross_val_score(classifier, X, y = y, scoring = \"accuracy\", cv = kfold, n_jobs=n_jobs))\n            \n    cv_means = []\n    cv_std = []\n    for index, cv_result in enumerate(cv_results):\n        mean_accuracy = cv_result.mean()\n        std = cv_result.std()\n        cv_means.append(mean_accuracy)\n        cv_std.append(std)\n        classifier = classifiers[index]\n        clf_name = classifier_name(classifier)\n        ret_results[clf_name] = (mean_accuracy, classifier)\n        print('{0:30s}: {1:.4f} (\u00b1{2:.4f})'.format(clf_name, mean_accuracy, std))\n    cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\": list(ret_results.keys())})\n\n    g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\n    g.set_xlabel(\"Mean Accuracy\")\n    g = g.set_title(\"Cross validation scores\")\n    return ret_results\n\nsplit_num = 10\ntest_size = 0.2\nclassifier_results = evaluate_with_cross_validation(classifiers, test_size, split_num, x_train, y_train)","b1c4dd1d":"from sklearn.model_selection import GridSearchCV\n# Adaboost\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=-1, verbose = 1)\n\ngsadaDTC.fit(x_train,y_train)\n\nada_best = gsadaDTC.best_estimator_\nada_accuracy = gsadaDTC.best_score_\nclassifier_results['ada_best'] = (ada_accuracy, ada_best)\nada_accuracy","d4afd989":"#ExtraTrees\nfrom sklearn.ensemble import ExtraTreesClassifier\nExtC = ExtraTreesClassifier()\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=-1, verbose = 1)\n\ngsExtC.fit(x_train,y_train)\n\next_best = gsExtC.best_estimator_\n\n# Best score\next_accuracy = gsExtC.best_score_\nclassifier_results['ext_best'] = (ext_accuracy, ext_best)\next_accuracy","95baf287":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=-1, verbose = 1)\n\ngsRFC.fit(x_train,y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\nrfc_accuracy = gsRFC.best_score_\nclassifier_results['RFC_best'] = (rfc_accuracy, RFC_best)\nrfc_accuracy","0a82ac29":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=-1, verbose = 1)\n\ngsGBC.fit(x_train,y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngs_accuracy = gsGBC.best_score_\nclassifier_results['GBC_best'] = (gs_accuracy, GBC_best)\ngs_accuracy","5c9dedf9":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=-1, verbose = 1)\n\ngsSVMC.fit(x_train,y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC_accuracy = gsSVMC.best_score_\n\nclassifier_results['SVMC_best'] = (gsSVMC_accuracy, SVMC_best)\n\ngsSVMC_accuracy","74dcb13a":"from sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",x_train,y_train,cv=kfold)","82ae11b3":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ext_best), (\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n\nnclassifier = 0\nx_train_columns = train.drop(['Survived'], axis=1).columns\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=x_train_columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","aec6d088":"# get TOP N classifiers to use in Voting Classifier\nenable_classifiers_num = 5\nenable_classifiers = sorted(classifier_results.items(), key=lambda kv: kv[1][0], reverse=True)[:enable_classifiers_num]\nprint('Top {0:,} classifiers'.format(enable_classifiers_num).upper())\nfor clf_name, tup in enable_classifiers:\n    print('\\t{0:30s}: {1:.5f}'.format(clf_name, tup[0]))","2d1ee75c":"from sklearn.ensemble import VotingClassifier\nestimators = [(n, tup[1]) for n, tup in enable_classifiers]\nvc_params = {\n    'estimators': estimators,\n    'voting': 'soft',\n    'n_jobs': -1,\n    'flatten_transform': True\n}\nvoting_classifier = VotingClassifier(**vc_params)\nvoting_classifier = voting_classifier.fit(x_train, y_train)\ncv_result = cross_val_score(voting_classifier, x_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=-1)\nprint('{0:30s}: {1:.4f} (\u00b1{2:.4f})'.format(classifier_name(voting_classifier), cv_result.mean(), cv_result.std()))","890949ee":"# best_classifier_name = 'VotingClassifier'\n# best_classifier = [_clf for _clf in classifiers if best_classifier_name == _clf.classifier_name()][0]\nbest_classifier = voting_classifier\nbest_classifier.fit(x_train, y_train)\npredictions = best_classifier.predict(x_test)\n# Generate Submission File \nsubmission_file = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nfrom datetime import datetime\nts = datetime.now().strftime(\"%Y%m%d%H%M\")\nfilename = 'submission_file_{0}.csv'.format(ts)\nsubmission_file.to_csv(filename, index=False)\nsubmission_file","f5918d84":"## QuadraticDiscriminantAnalysis\ncf. [sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html)","cbae820a":"## SVC (Support Vector Classification)\nC-Support Vector Classification\n\ncf. [sklearn.svm.SVC](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html)","6375826f":"## GaussianNB\ncf. [sklearn.naive_bayes.GaussianNB](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html)","a7b941b2":"# Modelling\nUse following methods to predict survivors.\n\n## Simple Model\n1. SVM\n2. Logistic Regression\n3. KNeighborsClassifier\n4. GaussianNB\n5. LinearDiscriminantAnalysis\n6. QuadraticDiscriminantAnalysis\n\n## TreeBased Model\n1.  RandomForest\n2. GradientBoostingClassifier\n3. DecisionTreeClassifier\n4. AdaBoostClassifier\n5. XGBoost Classifier\n\n## Neural Model\n1. Neural Network (Sequential)\n\n## Ensemble Model\n1. Voting Classifier\n\nThe feature engineering ideas are based on Sina's [Titanic best working Classifier\n](https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier)","07bdc00f":"## XGBoost Classifier\ncf. https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_intro.html","c3bdd385":"# Prediction","0486b250":"# Parameter Settings","a1e1d98d":"## LinearDiscriminantAnalysis\ncf. [sklearn.discriminant_analysis.LinearDiscriminantAnalysis](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)","1585ba89":"## Voting Classifier\ncf. [sklearn.ensemble.VotingClassifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html)","07e332e4":"## LogisticRegression\ncf. [sklearn.linear_model.LogisticRegression](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)","e3fe7216":"## Neural Network (Sequential)","94b22a06":"# Feature Engineering","384ac4b9":"## Random Forest Classifier\n ensemble methods\n\ncf. [sklearn.ensemble.RandomForestClassifier.](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)","55e1fb41":"# Ensemble modeling","d509b420":" ## GradientBoostingClassifier\n ensemble methods\n \n cf. [sklearn.ensemble.GradientBoostingClassifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html)","f269a93f":"# Training and Evaluation with Cross-validation (Stratified)","d34a8245":"## KNeighborsClassifier\ncf. [sklearn.neighbors.KNeighborsClassifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html)","081f5594":"# Titanic with basic and ensembles methods\n## Introduction\nThis notebook has been made by beginner (me) for kaggle beginners with Titanic competition dataset.\nFor the first thing, simple feature engineering based on the other kernels is conducted before modeling the basic and simple unsembles methods.\n\nThe final submittion file generated with this notebook achieves the score of appropriately 0.78.\n\nI'm so eager to imporove both this notebook and my predictions. Therefore, All your suggestions or comments are very welcome. ","7e33bbcf":" ## DecisionTreeClassifier\n \n \n cf. [sklearn.tree.DecisionTreeClassifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)","ee83eb30":"## AdaBoostClassifier\ncf. [sklearn.ensemble.AdaBoostClassifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html)"}}