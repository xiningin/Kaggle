{"cell_type":{"c0e385b5":"code","b484c4c5":"code","46bd8fb5":"code","31fc1fed":"code","422ea3a9":"code","10c77ddd":"code","132bc3a8":"code","aadd85a8":"code","3203af5c":"code","c8562282":"code","6fe70b6f":"code","ce25baa2":"code","c08910b3":"code","d8747727":"code","e7ab7beb":"code","194a3de1":"code","9f495bbb":"code","0a15b951":"code","625ab27d":"code","c40dd5ed":"code","da783052":"code","41318caa":"code","a32539f4":"code","94d740bf":"code","f97848ca":"code","1e3bd701":"code","80f91d4e":"code","89b69215":"code","bc54be4b":"code","438e242a":"code","7f3b11fc":"code","fb42cc14":"code","1875db2f":"code","fe5ae979":"code","4618f3b0":"code","8f8b25fe":"code","a35b2187":"code","97227e40":"code","19c66aa3":"code","84903a32":"code","8d39c415":"code","474c19f1":"code","5fdb7b63":"code","a87ce638":"code","f5e216c2":"code","7c48a57d":"code","626a990e":"markdown","e88a9cff":"markdown","2bfe0409":"markdown","b6908c09":"markdown","0175691d":"markdown","24b6e02c":"markdown","2741eba0":"markdown","0fdb58cd":"markdown","af58143f":"markdown","cfcc8d98":"markdown","049f527f":"markdown","dc1abddc":"markdown","f08b1296":"markdown","4c66f7c3":"markdown","3aba4264":"markdown","752e9b51":"markdown","8c9246f5":"markdown","61487e93":"markdown","3fcd44ee":"markdown"},"source":{"c0e385b5":"#import the initial libraries and packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b484c4c5":"#The csv file is loaded as a dtaframe.\ndataset = pd.read_csv(\"\/kaggle\/input\/insurance\/insurance.csv\")","46bd8fb5":"len(dataset)","31fc1fed":"dataset.head()","422ea3a9":"dataset.dtypes","10c77ddd":"dataset.isnull().sum()","132bc3a8":"#correlation between columns with respect to charges\ndataset.corr()[\"charges\"].sort_values()","aadd85a8":"#Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndataset.sex = le.fit_transform(dataset.sex)\ndataset.smoker = le.fit_transform(dataset.smoker)\ndataset.region = le.fit_transform(dataset.region)","3203af5c":"#checking data type\ndataset.dtypes","c8562282":"#checking correlation again\ndataset.corr()[\"charges\"].sort_values()","6fe70b6f":"#heatmap\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (20,10))\nsns.heatmap(dataset.corr(), annot = True)","ce25baa2":"#dividing the dataset into X and y matrix\n#X has the features needed to predict charges\n#y has the values of charges\nX = dataset.iloc[:,0:-1].values\ny = dataset.iloc[:,6:7].values","c08910b3":"X","d8747727":"y","e7ab7beb":"import statsmodels.api as sm","194a3de1":"#creating a new matrix with the first column consisting of all ones\nX = np.append(arr = np.ones((1338,1)).astype(int), values = X, axis = 1)","9f495bbb":"#selecting all the features initially to determine the p value\nX_opt = X[:,[0,1,2,3,4,5,6]]","0a15b951":"#Using Ordinary Least Square (OLS) to determine the p value and other features\nreg_OLS = sm.OLS(y,X_opt).fit()\nreg_OLS.summary()","625ab27d":"#After removing the x2 column\nX_opt = X[:,[0,1,3,4,5,6]]\nreg_OLS = sm.OLS(y,X_opt).fit()\nreg_OLS.summary()","c40dd5ed":"#reinitializing the matrix X and y\nX = dataset.iloc[:,0:-1].values\ny = dataset.iloc[:,6:7].values","da783052":"#split into train and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)","41318caa":"#checking length of training set\nlen(X_train)","a32539f4":"#checking length of test set\nlen(X_test)","94d740bf":"#importing sklearn for Multivariate Regression\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\n\n#fit the model with the training set\nregressor.fit(X_train, y_train)","f97848ca":"#calculating the score of the model\nregressor.score(X_test, y_test)","1e3bd701":"#reinitializing the X and y data \nX = dataset.iloc[:,0:-1].values\ny = dataset.iloc[:,6:7].values","80f91d4e":"#importing the PolynomialFeatures from sklearn.preprocessing\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree = 2)","89b69215":"#fitting the polynomial equation model\nX_poly = poly.fit_transform(X)","bc54be4b":"#split into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_poly,y, test_size = 0.2, random_state = 0)","438e242a":"#train the regressor model\nregressor.fit(X_train, y_train)","7f3b11fc":"#score the model\nregressor.score(X_test, y_test)","fb42cc14":"#reinitialize X and y\nX = dataset.iloc[:,0:-1].values\ny = dataset.iloc[:,6:7].values","1875db2f":"#train and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)","fe5ae979":"#importing Decision Tree Regressor\nfrom sklearn.tree import DecisionTreeRegressor","4618f3b0":"#using the criterion of mean square error\nregressor = DecisionTreeRegressor(criterion = \"mse\", random_state = 0)\nregressor.fit(X_train,y_train)","8f8b25fe":"#importing metrics to calculate the r2 score and mse\nfrom sklearn.metrics import r2_score,mean_squared_error","a35b2187":"#r2 score for the training set\nr2_score(y_train, regressor.predict(X_train))","97227e40":"#mse for the training set\nmean_squared_error(y_train, regressor.predict(X_train))","19c66aa3":"#r2 score for the test set\nr2_score(y_test, regressor.predict(X_test))","84903a32":"#mse for the test set\nmean_squared_error(y_test, regressor.predict(X_test))","8d39c415":"#importing the Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor","474c19f1":"#using 100 estimators for Random Forest Calculation\n#takes average of 100 predictions and trains model\nregressor = RandomForestRegressor(n_estimators = 300, random_state = 0)\nregressor.fit(X_train, y_train)","5fdb7b63":"#r2 score of training set\nr2_score(y_train, regressor.predict(X_train))","a87ce638":"#mse of training set\nmean_squared_error(y_train, regressor.predict(X_train))","f5e216c2":"#r2 score of test set\nr2_score(y_test, regressor.predict(X_test))","7c48a57d":"#mse of test set\nmean_squared_error(y_test, regressor.predict(X_test))","626a990e":"Now to determine if the dataset has any null values or any NaN values. ","e88a9cff":"Now from these values it can be determined that the column that affects the charges most is the smoker feature followed by the age and bmi at third.\n\nTo confirm this we shall visualize it with the help of a heatmap.","2bfe0409":"The length or the number of rows in the dataset is obtained.","b6908c09":"As you can see the accuracy of the model has improved, it now has an accuracy of 86.48% which is better than the Multivariate Regression which had an accuracy of 79.98%.\n\nNext we try the Decision Tree Regression Model.","0175691d":"In this regression model we can clearly notice the biggest drawback of Decision Trees which is Overfitting. \n\nThe r2 score of the training set is 99.98% while the r2 score of the test set is only 64.86%. This big drop in accuracy is only because the the model was overfitted.\n\nThis can be overcome by either pruning or by using a better model which is the Random Forest Regression.","24b6e02c":"Here we observe that only the children, bmi, age and charges column are being considered for checking of the correlation. This is because the corr() function performs operation only on numeric values.\n\nThus the columns that have data type object need to be converted to either int or float.\n\nThis can be done by using Label Encoding.","2741eba0":"Lets begin performing the regression by using the sklearn library.","0fdb58cd":"In Random Forest Regressor we notice a way better performance than the Decision Tree. This is because it is an ensemble method which takes multiple prediction and takes their average value to train the model, thus preventing overfitting.\n\nThe accuracy of the training and the test model are 97.49% and 87.32% respectively which is a massive gain over Decision Tree.\n\nBy checking each of the regression models it can be determined that the regression model having the highest accuracy for this dataset is the Random Forest Regressor.","af58143f":"![Screen%20Shot%202020-04-05%20at%206.13.07%20AM.png](attachment:Screen%20Shot%202020-04-05%20at%206.13.07%20AM.png)\n\nThe goal of this notebook is to explore the dataset, determine the features that most affect the cost of medical care by using a heatmap and also by using the Backward Elimination Model (checking p value with respect to significance value).\n\nAfter which different regression models are run to determine the best model to predict the charges given the features. The models that are trained are,\n1. Multivariate Linear regression\n2. Polynomial Regression\n3. Decision Tree Regression\n4. Random Forest Regression","cfcc8d98":"By using the head function the first five rows of the dataset are displayed. These values are displayed for all the columns.","049f527f":"The datatype of each row is obtained using dtypes function.\nAs we can see here the columns of sex, smoker and region have data type object. This is because they have string values in them.\n\nSex - Male, Female.\n\nSmoker - Yes, No.\n\nRegion - Northeast, Northwest, Southeast, Southwest.","dc1abddc":"Since all the features have a p value less than the significance value we can conclude that these features are important in determining the charge.\n\nNext we proceed to the main regression part.\n\nFirst up is Multivariate Linear Regression.","f08b1296":"Now that the X and y values have been reinitialized, the X matrix is used to develop a polynomial equation for charges. \n\nThe degree chosen here is 2, higher degrees result in more number of columns.","4c66f7c3":"From this summary we notice that the p value of feature x2 is above the the significance level of 0.05. Hence we shall remove it and run the summary again.","3aba4264":"This gives a accuracy of 79.98%. Not bad!\n\nThis is a good accuracy but lets try other regression methods to check if we can get a better model to predict charges.\n\nNext up, Polynomial Regression!","752e9b51":"Luckily this dataset has no values that means we can go ahead and check the correlation between the features and our desired column. \n\nThe column of interest here is the \"charges\" column.","8c9246f5":"Now, to calculate the p values of each of the feature the statsmodels.api feature is used.","61487e93":"It can be confirmed from this visual representation that the correlation between charges and smoker is the highest.\n\nThe next step what we will be doing is determining which features are the best to train a model. This is done by using the Backward Elimination method where the significance level will be 0.05.","3fcd44ee":"Now to predict the values for the test set and calculate the accuracy of the model."}}