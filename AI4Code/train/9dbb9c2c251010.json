{"cell_type":{"60cf6519":"code","4cf19bc3":"code","32e9f8fd":"code","9bc0eeca":"code","28b0ac2b":"code","1134ed2c":"code","3905748c":"code","11d8ea55":"code","3b6eeb6d":"code","76c194d9":"code","e4cae48e":"code","9e951718":"code","72d5df2d":"code","6da95f2c":"code","0ef542fd":"code","08a8f3af":"code","de68fa88":"code","4d7a9dc2":"code","3b9247a1":"code","53528904":"code","02ce4d5f":"code","70728afd":"code","3e5f10dd":"code","d1435bd0":"code","5080f228":"code","da9bee3e":"code","4d76836d":"code","0931c2fc":"code","85a76095":"code","a8728465":"code","9f3762e7":"code","bb7b201b":"code","cc093712":"code","52469445":"code","4d8a7584":"code","305bdf55":"code","25a30fab":"code","83cd3055":"code","7579ed78":"code","bf9c34ff":"code","9f3209df":"code","17da9425":"code","0cf30e50":"code","c8941bc0":"code","dee45d90":"code","a645717d":"markdown","e6dc1af8":"markdown","02841e19":"markdown","39803bc2":"markdown","b85c49bf":"markdown","244c68f9":"markdown","865aa39e":"markdown","a7722aeb":"markdown","377eddd9":"markdown","190d768a":"markdown","cc10642b":"markdown","14a1818f":"markdown","18fafc8c":"markdown","f007b47e":"markdown","92695bcf":"markdown","96ffa1e9":"markdown","c6898130":"markdown","2e45b5f9":"markdown","cb11fb78":"markdown","aafd9120":"markdown","80c85695":"markdown"},"source":{"60cf6519":"from IPython.display import clear_output\n!pip install imutils\n!pip install -q efficientnet\nclear_output()","4cf19bc3":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport sys\nimport cv2\nimport math\nimport umap\nimport keras\nimport shutil\nimport random\nimport imutils\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport efficientnet.tfkeras as efn\n\n\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom os import listdir\nfrom scipy import misc\nfrom keras import layers\nfrom plotly import tools\nfrom random import shuffle\nfrom itertools import chain\nfrom numpy import expand_dims\nfrom matplotlib import pyplot \nfrom keras import backend as K\nfrom keras.layers import Input\nfrom collections import Counter\nfrom os.path import isfile, join\nfrom sklearn.manifold import TSNE\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nfrom sklearn.metrics import f1_score\nfrom sklearn.decomposition import PCA\nfrom IPython.display import clear_output\nfrom sklearn.metrics import recall_score\nfrom sklearn.datasets import make_circles\nfrom sklearn.metrics import roc_auc_score\nfrom keras.layers.merge import concatenate\nfrom keras.optimizers import Adam, RMSprop\nfrom sklearn.metrics import precision_score\nfrom keras.layers.core import Dropout, Lambda\nfrom sklearn.metrics import cohen_kappa_score\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.preprocessing.image import load_img\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.preprocessing.image import img_to_array\nfrom sklearn.model_selection import train_test_split\nfrom plotly.offline import init_notebook_mode, iplot\nfrom keras.models import Model, Sequential, load_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.applications.vgg19 import VGG19, preprocess_input\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\n\n\ninit_notebook_mode(connected=True)\nRANDOM_SEED = 1998\nEPOCHS = 128","32e9f8fd":"!apt-get install tree\nclear_output()\n# create new folders\n!mkdir TRAIN TEST VAL TRAIN\/YES TRAIN\/NO TEST\/YES TEST\/NO VAL\/YES VAL\/NO\n!tree -d","9bc0eeca":"IMG_PATH = '..\/input\/brain-tumor-detection\/'\n# split the data by train\/val\/test\nfor CLASS in os.listdir(IMG_PATH):\n    if (not CLASS.startswith('.') and (not CLASS.startswith('pred'))):\n        print(IMG_PATH + '     ' + CLASS)\n        IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n            img = IMG_PATH + CLASS + '\/' + FILE_NAME\n            if n < 200:\n                shutil.copy(img, 'TEST\/' + CLASS.upper() + '\/' + FILE_NAME)\n            elif n < 1200:\n                shutil.copy(img, 'TRAIN\/'+ CLASS.upper() + '\/' + FILE_NAME)\n            else:\n                shutil.copy(img, 'VAL\/'+ CLASS.upper() + '\/' + FILE_NAME)","28b0ac2b":"def load_data(dir_path, img_size=(100,100)):\n    \"\"\"\n    Load resized images as np.arrays to workspace\n    \"\"\"\n    X = []\n    y = []\n    i = 0\n    labels = dict()\n    for path in tqdm(sorted(os.listdir(dir_path))):\n        if not path.startswith('.'):\n            labels[i] = path\n            for file in os.listdir(dir_path + path):\n                if not file.startswith('.'):\n                    img = cv2.imread(dir_path + path + '\/' + file)\n                    X.append(img)\n                    y.append(i)\n            i += 1\n    X = np.array(X)\n    y = np.array(y)\n    print(f'{len(X)} images loaded from {dir_path} directory.')\n    return X, y, labels\n\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    cm = np.round(cm,2)\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","1134ed2c":"TRAIN_DIR = 'TRAIN\/'\nTEST_DIR = 'TEST\/'\nVAL_DIR = 'VAL\/'\nIMG_SIZE = (224,224)\n\n# use predefined function to load the image data into workspace\nX_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\nX_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\nX_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)","3905748c":"y = dict()\ny[0] = []\ny[1] = []\nfor set_name in (y_train, y_val, y_test):\n    y[0].append(np.sum(set_name == 0))\n    y[1].append(np.sum(set_name == 1))\n\ntrace0 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[0],\n    name='No',\n    marker=dict(color='#33cc33'),\n    opacity=0.7\n)\ntrace1 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[1],\n    name='Yes',\n    marker=dict(color='#ff3300'),\n    opacity=0.7\n)\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title='Count of classes in each set',\n    xaxis={'title': 'Set'},\n    yaxis={'title': 'Count'}\n)\nfig = go.Figure(data, layout)\niplot(fig)","11d8ea55":"def plot_samples(X, y, labels_dict, n=50):\n    \"\"\"\n    Creates a gridplot for desired number of images (n) from the specified set\n    \"\"\"\n    for index in range(len(labels_dict)):\n        imgs = X[np.argwhere(y == index)][:n]\n        j = 10\n        i = int(n\/j)\n\n        plt.figure(figsize=(15,6))\n        c = 1\n        for img in imgs:\n            plt.subplot(i,j,c)\n            plt.imshow(img[0])\n\n            plt.xticks([])\n            plt.yticks([])\n            c += 1\n        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n        plt.show()","3b6eeb6d":"plot_samples(X_train, y_train, labels, 10)","76c194d9":"RATIO_LIST = []\nfor set in (X_train, X_test, X_val):\n    for img in set:\n        RATIO_LIST.append(img.shape[1]\/img.shape[0])\n        \nplt.hist(RATIO_LIST)\nplt.title('Distribution of Image Ratios')\nplt.xlabel('Ratio Value')\nplt.ylabel('Count')\nplt.show()","e4cae48e":"def crop_imgs(set_name, add_pixels_value=0):\n    \"\"\"\n    Finds the extreme points on the image and crops the rectangular out of them\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # threshold the image, then perform a series of erosions +\n        # dilations to remove any small regions of noise\n        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n        thresh = cv2.erode(thresh, None, iterations=2)\n        thresh = cv2.dilate(thresh, None, iterations=2)\n\n        # find contours in thresholded image, then grab the largest one\n        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts = imutils.grab_contours(cnts)\n        c = max(cnts, key=cv2.contourArea)\n\n        # find the extreme points\n        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n        extRight = tuple(c[c[:, :, 0].argmax()][0])\n        extTop = tuple(c[c[:, :, 1].argmin()][0])\n        extBot = tuple(c[c[:, :, 1].argmax()][0])\n\n        ADD_PIXELS = add_pixels_value\n        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n        set_new.append(new_img)\n    return np.array(set_new)","9e951718":"img = cv2.imread('..\/input\/brain-tumor-detection\/yes\/y66.jpg')\nimg = cv2.resize(\n            img,\n            dsize=IMG_SIZE,\n            interpolation=cv2.INTER_CUBIC\n        )\ngray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\ngray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n# threshold the image, then perform a series of erosions +\n# dilations to remove any small regions of noise\nthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\nthresh = cv2.erode(thresh, None, iterations=2)\nthresh = cv2.dilate(thresh, None, iterations=2)\n\n# find contours in thresholded image, then grab the largest one\ncnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\nc = max(cnts, key=cv2.contourArea)\n\n# find the extreme points\nextLeft = tuple(c[c[:, :, 0].argmin()][0])\nextRight = tuple(c[c[:, :, 0].argmax()][0])\nextTop = tuple(c[c[:, :, 1].argmin()][0])\nextBot = tuple(c[c[:, :, 1].argmax()][0])\n\n# add contour on the image\nimg_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n\n# add extreme points\nimg_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\nimg_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n\n# crop\nADD_PIXELS = 0\nnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()","72d5df2d":"plt.figure(figsize=(15,6))\nplt.subplot(141)\nplt.imshow(img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 1. Get the original image')\nplt.subplot(142)\nplt.imshow(img_cnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 2. Find the biggest contour')\nplt.subplot(143)\nplt.imshow(img_pnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 3. Find the extreme points')\nplt.subplot(144)\nplt.imshow(new_img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 4. Crop the image')\nplt.show()","6da95f2c":"# apply this for each set\nX_train_crop = crop_imgs(set_name=X_train)\nX_val_crop = crop_imgs(set_name=X_val)\nX_test_crop = crop_imgs(set_name=X_test)","0ef542fd":"plot_samples(X_train_crop, y_train, labels, 10)","08a8f3af":"def save_new_images(x_set, y_set, folder_name):\n    i = 0\n    for (img, imclass) in zip(x_set, y_set):\n        if imclass == 0:\n            cv2.imwrite(folder_name+'NO\/'+str(i)+'.jpg', img)\n        else:\n            cv2.imwrite(folder_name+'YES\/'+str(i)+'.jpg', img)\n        i += 1","de68fa88":"# saving new images to the folder\n!mkdir TRAIN_CROP TEST_CROP VAL_CROP TRAIN_CROP\/YES TRAIN_CROP\/NO TEST_CROP\/YES TEST_CROP\/NO VAL_CROP\/YES VAL_CROP\/NO\n\nsave_new_images(X_train_crop, y_train, folder_name='TRAIN_CROP\/')\nsave_new_images(X_val_crop, y_val, folder_name='VAL_CROP\/')\nsave_new_images(X_test_crop, y_test, folder_name='TEST_CROP\/')","4d7a9dc2":"def preprocess_imgs(set_name, img_size):\n    \"\"\"\n    Resize and apply VGG-15 preprocessing\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        img = cv2.resize(\n            img,\n            dsize=img_size,\n            interpolation=cv2.INTER_CUBIC\n        )\n        set_new.append(preprocess_input(img))\n    return np.array(set_new)","3b9247a1":"X_train_prep = preprocess_imgs(set_name=X_train_crop, img_size=IMG_SIZE)\nX_test_prep = preprocess_imgs(set_name=X_test_crop, img_size=IMG_SIZE)\nX_val_prep = preprocess_imgs(set_name=X_val_crop, img_size=IMG_SIZE)","53528904":"# plot_samples(X_train_prep, y_train, labels, 30)","02ce4d5f":"# set the paramters we want to change randomly\ndemo_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    rescale=1.\/255,\n    shear_range=0.05,\n    brightness_range=[0.1, 1.5],\n    horizontal_flip=True,\n    vertical_flip=True\n)","70728afd":"os.mkdir('preview')\nx = X_train_crop[0]  \nx = x.reshape((1,) + x.shape) \n\ni = 0\nfor batch in demo_datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='aug_img', save_format='jpg'):\n    i += 1\n    if i > 20:\n        break ","3e5f10dd":"plt.imshow(X_train_crop[0])\nplt.xticks([])\nplt.yticks([])\nplt.title('Original Image')\nplt.show()\n\nplt.figure(figsize=(15,6))\ni = 1\nfor img in os.listdir('preview\/'):\n    img = cv2.cv2.imread('preview\/' + img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.subplot(3,7,i)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    i += 1\n    if i > 3*7:\n        break\nplt.suptitle('Augemented Images')\nplt.show()","d1435bd0":"!rm -rf preview\/","5080f228":"TRAIN_DIR = 'TRAIN_CROP\/'\nVAL_DIR = 'VAL_CROP\/'\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    brightness_range=[0.5, 1.5],\n    horizontal_flip=True,\n    vertical_flip=True,\n    preprocessing_function=preprocess_input\n)\n\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input\n)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=32,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)\n\n\nvalidation_generator = test_datagen.flow_from_directory(\n    VAL_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=16,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)","da9bee3e":"# load base model\nvgg = VGG16(\n    weights='imagenet',\n    include_top=False, \n    input_shape=IMG_SIZE + (3,)\n)","4d76836d":"f = plt.figure(figsize=(16,16))\n# load the modelf = plt.figure(figsize=(10,3))\nmodel = VGG16()\n# redefine model to output right after the first hidden layer\nmodel = Model(inputs=model.inputs, outputs=model.layers[1].output)\nmodel.summary()\n# load the image with the required shape\n# convert the image to an array\nimg = img_to_array(X_val_prep[43])\n# expand dimensions so that it represents a single 'sample'\nimg = expand_dims(img, axis=0)\n# prepare the image (e.g. scale pixel values for the vgg)\nimg = preprocess_input(img)\n# get feature map for first hidden layer\nfeature_maps = model.predict(img)\n# plot all 64 maps in an 8x8 squares\nsquare = 8\nix = 1\nfor _ in range(square):\n\tfor _ in range(square):\n\t\t# specify subplot and turn of axis\n\t\tax = pyplot.subplot(square, square, ix)\n\t\tax.set_xticks([])\n\t\tax.set_yticks([])\n\t\t# plot filter channel in grayscale\n\t\tpyplot.imshow(feature_maps[0, :, :, ix-1], cmap='viridis')\n\t\tix += 1\n# show the figure\npyplot.show()\n","0931c2fc":"NUM_CLASSES = 1\n\nvgg16 = Sequential()\nvgg16.add(vgg)\nvgg16.add(layers.MaxPooling2D(pool_size=(2,2)))\nvgg16.add(layers.Flatten())\nvgg16.add(layers.Dropout(0.2))\nvgg16.add(layers.Dense(NUM_CLASSES, activation='sigmoid'))\n\nvgg16.layers[0].trainable = False\n\nvgg16.compile(\n    loss='binary_crossentropy',\n    optimizer=RMSprop(lr=1e-4),\n    metrics=['accuracy']\n)\nvgg16.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False), metrics=[\"accuracy\"])\n\nvgg16.summary()","85a76095":"# load the model\nmodel = VGG16()\n# redefine model to output right after the first hidden layer\nixs = [2, 5, 9, 13, 17]\noutputs = [model.layers[i].output for i in ixs]\nmodel = Model(inputs=model.inputs, outputs=outputs)\n# load the image with the required shape\n# convert the image to an array\nimg = img_to_array(X_val_prep[43])\n# expand dimensions so that it represents a single 'sample'\nimg = expand_dims(img, axis=0)\n# prepare the image (e.g. scale pixel values for the vgg)\nimg = preprocess_input(img)\n# get feature map for first hidden layer\nfeature_maps = model.predict(img)\n# plot the output from each block\nsquare = 8\nfor fmap in feature_maps:\n\t# plot all 64 maps in an 8x8 squares\n\tix = 1\n\tfor _ in range(square):\n\t\tplt.figure(figsize=(64,64))\n\t\tfor _ in range(square):\n           \n\n\t\t\t# specify subplot and turn of axis\n\t\t\tax = pyplot.subplot(square, square, ix)\n\t\t\tax.set_xticks([])\n\t\t\tax.set_yticks([])\n\t\t\t\n\t\t\t# plot filter channel in grayscale\n\t\t\tplt.imshow(fmap[0, :, :, ix-1], cmap='viridis')\n\t\t\tix += 1\n\t# show the figure\n       \n\tplt.show()","a8728465":"vgg16.load_weights('..\/input\/brain-tumor-detection-br35h\/VGG16_model.h5')","9f3762e7":"es = EarlyStopping(\n    monitor='val_acc', \n    mode='max',\n    patience=6\n)\n\n\nimport time\nstart = time.time()\n\nvgg16_history = vgg16.fit_generator(\n    train_generator,\n    steps_per_epoch=64,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=30,\n    callbacks=[es]\n)\n\nend = time.time()\nprint(end - start)","bb7b201b":"# plot model performance\nacc = vgg16_history.history['accuracy']\nval_acc = vgg16_history.history['val_accuracy']\nloss = vgg16_history.history['loss']\nval_loss = vgg16_history.history['val_loss']\nepochs_range = range(1, len(vgg16_history.epoch) + 1)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Train Set')\nplt.plot(epochs_range, val_acc, label='Val Set')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Train Set')\nplt.plot(epochs_range, val_loss, label='Val Set')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Model Loss')\n\nplt.tight_layout()\nplt.show()","cc093712":"# validate on val set\npredictions = vgg16.predict(X_val_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_val, predictions)\nprint('VGG-16 Val Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_val, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","52469445":"# validate on test set\npredictions = vgg16.predict(X_test_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_test, predictions)\nprint('VGG-16 Test Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_test, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","4d8a7584":"ind_list = np.argwhere((y_test == predictions) == False)[:, -1]\nif ind_list.size == 0:\n    print('There are no missclassified images.')\nelse:\n    for i in ind_list:\n        plt.figure()\n        plt.imshow(X_test_crop[i])\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(f'Actual class: {y_val[i]}\\nPredicted class: {predictions[i]}')\n        plt.show()","305bdf55":"def build_lrfn(lr_start=0.0005, lr_max=0.001, \n               lr_min=0.000001, lr_rampup_epochs=20, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn","25a30fab":"import tensorflow.keras.layers as L","83cd3055":"NUM_CLASSES = 1\n\nefficientnet = tf.keras.Sequential([\n    efn.EfficientNetB2(\n        input_shape=(224, 224, 3),\n        weights='imagenet',\n        include_top=False\n    ),\n    L.GlobalMaxPooling2D(),\n    L.Dropout(rate=0.2),\n    L.Dense(NUM_CLASSES, activation='sigmoid')\n    ])\n        \nefficientnet.compile(\n    optimizer='adam',\n    loss = 'binary_crossentropy',\n    metrics=['accuracy']\n)\n    \nefficientnet.summary()","7579ed78":"efficientnet.load_weights('..\/input\/brain-tumor-detection-br35h\/efficientnetb2_model.h5')","bf9c34ff":"import time\n\nstart = time.time()\n\nefficientnet_history = efficientnet.fit_generator(\n    train_generator,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=30,\n)\n\nend = time.time()\nprint(end - start)","9f3209df":"# plot model performance\nacc = efficientnet_history.history['accuracy']\nval_acc = efficientnet_history.history['val_accuracy']\nloss = efficientnet_history.history['loss']\nval_loss = efficientnet_history.history['val_loss']\nepochs_range = range(1, len(efficientnet_history.epoch) + 1)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Train Set')\nplt.plot(epochs_range, val_acc, label='Val Set')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Train Set')\nplt.plot(epochs_range, val_loss, label='Val Set')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Model Loss')\n\nplt.tight_layout()\nplt.show()","17da9425":"# validate on val set\npredictions = efficientnet.predict(X_val_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_val, predictions)\nprint('Val Accuracy = %.2f' % accuracy)\nprint(\"Nice,\\nBut how many wrong cases really?! And what kind of mistake is that ?!\\n# let's view F1-Score:\")\n\nconfusion_mtx = confusion_matrix(y_val, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","0cf30e50":"# validate on test set\npredictions = efficientnet.predict(X_test_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_test, predictions)\nprint('Test Accuracy = %.2f' % accuracy)\nprint(\"Nice,\\nBut how many wrong cases really?! And what kind of mistake is that ?!\\n# let's view F1-Score:\")\n\nconfusion_mtx = confusion_matrix(y_test, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","c8941bc0":"ind_list = np.argwhere((y_test == predictions) == False)[:, -1]\nprint\nif ind_list.size == 0:\n    print('There are no missclassified images.')\nelse:\n    for i in ind_list:\n        plt.figure()\n        plt.imshow(X_test_crop[i])\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(f'Actual class: {y_val[i]}\\nPredicted class: {predictions[i]}')\n        plt.show()","dee45d90":"# clean up the space\n!rm -rf TRAIN TEST VAL TRAIN_CROP TEST_CROP VAL_CROP\n# save the model\nvgg16.save('VGG16_model.h5')\nefficientnet.save('efficientnetb2_model.h5')","a645717d":"Let's take a look at the distribution of classes among sets:","e6dc1af8":"# VGG16","02841e19":"# <a id='intro'>1. Project Overview and Objectives<\/a>\n\nThe main purpose of this project was to build a CNN model that would classify if subject has a tumor or not base on MRI scan. I used the [VGG-16 & Efficientnet-B2](https:\/\/www.kaggle.com\/ahmedhamada0\/brain-tumor-detection-br35h) models architecture and weights to train the model for this binary problem. I used `accuracy` as a metric to justify the model performance which can be defined as:\n\n$\\textrm{Accuracy} = \\frac{\\textrm{Number of correclty predicted images}}{\\textrm{Total number of tested images}} \\times 100\\%$\n\nFinal results look as follows:\n\n| Set | Accuracy |\n|:-:|:-:|\n| Validation Set* | ~99% |\n| Test Set* | ~99% |\n<br>\n\\* *Note: there might be some misunderstanding in terms of set names so I want to describe what do I mean by `test` and `validation` set:*\n* *`validation set` - is the set used during the model training to adjust the hyperparameters. *\n* *`test set` - is the small set that I don't touch for the whole training process at all. It's been used for final model performance evaluation.*\n\n## <a id='dataset'>1.1. Data Set Description<\/a>\n\nThe image data that was used for this problem is [Br35H :: Brain Tumor Detection 2020](https:\/\/www.kaggle.com\/ahmedhamada0\/brain-tumor-detection). It conists of MRI scans of two classes:\n\n* `NO` - no tumor, encoded as `0`\n* `YES` - tumor, encoded as `1`\n\nUnfortunately, the data set description doesn't hold any information where this MRI scans come from and so on.\n\n## <a id='tumor'>1.2. What is Brain Tumor?<\/a>\n\n> A brain tumor occurs when abnormal cells form within the brain. There are two main types of tumors: cancerous (malignant) tumors and benign tumors. Cancerous tumors can be divided into primary tumors, which start within the brain, and secondary tumors, which have spread from elsewhere, known as brain metastasis tumors. All types of brain tumors may produce symptoms that vary depending on the part of the brain involved. These symptoms may include headaches, seizures, problems with vision, vomiting and mental changes. The headache is classically worse in the morning and goes away with vomiting. Other symptoms may include difficulty walking, speaking or with sensations. As the disease progresses, unconsciousness may occur.\n>\n> ![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/5\/5f\/Hirnmetastase_MRT-T1_KM.jpg)\n>\n> *Brain metastasis in the right cerebral hemisphere from lung cancer, shown on magnetic resonance imaging.*\n\nSource: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Brain_tumor)","39803bc2":"> ## And that's it, we hope you enjoyed the kernel and see you around!","b85c49bf":"Let's look at example what this function will do with MRI scans:","244c68f9":"Now let's take a look at the images from the `test set` that were misclassified:","865aa39e":"As you can see, images have different `width` and `height` and diffent size of \"black corners\". Since the image size for VGG-16 imput layer is `(224,224)` some wide images may look weird after resizing. Histogram of ratio distributions (`ratio = width\/height`):","a7722aeb":"The next step would be resizing images to `(224,224)` and applying preprocessing needed for VGG-16 model input.","377eddd9":"Now let's take a look at the images from the `test set` that were misclassified:","190d768a":"# <a id='cnn'>4. CNN Model<\/a>\n\n## <a id='aug'>4.1. Data Augmentation<\/a>\n\nSince I had small data set I used the technique called [Data Augmentation](https:\/\/blog.keras.io\/building-powerful-image-classification-models-using-very-little-data.html) which helps to \"increase\" the size of training set.\n\n### <a id='demo'>4.1.1. Demo<\/a>\n\nThat's the example from one image how does augmentation look like.","cc10642b":"# <a id='env'>2. Setting up the Environment<\/a>","14a1818f":"First scan looks a bit misleading - what is that light spot in the middle? \ud83e\udd14 I can clearly see why model classified it as `Tumor`.","18fafc8c":"# Efficientnet-B2","f007b47e":"## <a id='build'>4.2. Model Building<\/a>","92695bcf":"### <a id='apply'>4.1.2. Apply<\/a>","96ffa1e9":"# <a id='import'>3. Data Import and Preprocessing<\/a>","c6898130":"The first step of \"normalization\" would be to crop the brain out of the images. I used technique which was perfectly described in [pyimagesearch](https:\/\/www.pyimagesearch.com\/2016\/04\/11\/finding-extreme-points-in-contours-with-opencv\/) blog and I highly suggest to looks deeper into it.","2e45b5f9":"## <a id='perf'>4.3. Model Performance<\/a>","cb11fb78":"## <a id='perf'>4.3. Model Performance<\/a>","aafd9120":"**<center><font size=5>|| Br35H || Brain Tumor Detection<\/font><\/center>**\n***\n**Author**: Ahmed Hamada\n\n**date**: 8th June, 2021\n\n**Table of Contents**\n- <a href='#intro'>1. Project Overview and Objectives<\/a> \n    - <a href='#dataset'>1.1. Data Set Description<\/a>\n    - <a href='#tumor'>1.2. What is Brain Tumor?<\/a>\n- <a href='#env'>2. Setting up the Environment<\/a>\n- <a href='#import'>3. Data Import and Preprocessing<\/a>\n- <a href='#cnn'>4. CNN Model<\/a>\n    - <a href='#aug'>4.1. Data Augmentation<\/a>\n        - <a href='#demo'>4.1.1. Demo<\/a>\n        - <a href='#apply'>4.1.2. Apply<\/a>\n    - <a href='#build'>4.2. Model Building<\/a>\n    - <a href='#perf'>4.3. Model Performance<\/a>\n- <a href='#concl'>5. Conclusions<\/a>","80c85695":"Right now all images are in one folder with `yes` and `no` subfolders. I will split the data into `train`, `val` and `test` folders which makes its easier to work for me. The new folder heirarchy will look as follows:"}}