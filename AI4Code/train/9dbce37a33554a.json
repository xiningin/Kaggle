{"cell_type":{"e1812cf5":"code","e0301c03":"code","50e13565":"code","614a7275":"code","ec72bc6c":"code","286feeff":"code","1a3f26fb":"code","3a0ccac5":"code","f7ad16a8":"code","85429da2":"code","3f9cdbb0":"code","9802e99f":"code","5f34c7c6":"code","3841af1b":"code","b2dab521":"code","e9a0a2bf":"code","f7ee34c5":"code","3a983c43":"markdown","2047989f":"markdown","72f8fc6a":"markdown","30046f4b":"markdown","09541261":"markdown","67498d38":"markdown","49de2ea5":"markdown","42db3699":"markdown","c12f9445":"markdown","c9194eff":"markdown","1ffe7522":"markdown","56c2db25":"markdown","be96e7a7":"markdown","747f7366":"markdown","9b2e3b91":"markdown","3511fe4c":"markdown","4459cbfa":"markdown","c12f8090":"markdown","10d59dba":"markdown","8dfc76d1":"markdown","4293f252":"markdown","e74e8dce":"markdown","8fbecac6":"markdown"},"source":{"e1812cf5":"# importing necessary libraries\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split","e0301c03":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain.sample(5)\n","50e13565":"#basic information about our dataset\ntrain.info()","614a7275":"survive = train.Survived.values\ntrain.drop(['PassengerId', 'Survived'], axis=1, inplace=True)\n\ntrain['FamSize'] = train[\"SibSp\"] + train[\"Parch\"] #combining 'SibSp' and 'Parch' column into one 'FamSize' column\n\n\n# We will use the function below to determine the deck letter for each passenger:\ndef set_deck(cabin):\n    if str(cabin) == 'nan':\n        return 'Missing'\n    return cabin[0]\n\n\ntrain['Deck'] = train['Cabin'].map(set_deck)","ec72bc6c":"\nnum_features = ['Age','FamSize','Fare'] #numerical features\n\ncat_features = ['Sex','Pclass','Deck','Embarked'] #categorical features\n\n\nfeatures = num_features + cat_features\n\n# create a pipeline to fill missing values and standardize them in numerical features\nnum_transformer = Pipeline(\n    steps = [\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('scaler', StandardScaler())  \n    ]\n)\n\n# create a pipeline to fill missing values and encode categorical variables\n\ncat_transformer = Pipeline(\n    steps = [\n        ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ]\n)\n\n# combining both pipelines\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', num_transformer, num_features),\n        ('cat', cat_transformer, cat_features)\n    ]\n)","286feeff":"\npreprocessor.fit(train)\ntrain2 = preprocessor.transform(train)\nprint('Train shape:', train2.shape)\nprint('y_train shape:', survive.shape)\n","1a3f26fb":"# we are gonna do a 80\/20 split on our training dataset\nX_train, X_test, y_train, y_test = train_test_split(train2, survive, test_size=0.2, stratify=survive, random_state=1)\n\nprint('Train shape:',X_train.shape)\nprint('Test shape:',X_test.shape)","3a0ccac5":"# creating classification report function\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nscore=[]\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n     if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        accuracies = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 10)\n        print(\"Train Result:\\n================================================\")\n        print(clf,\":Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        score.append(accuracies.mean()*100)\n        \n     elif train==False:\n         pred = clf.predict(X_test)\n         clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n         print(\"Test Result:\\n================================================\")        \n         print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n         print(\"_______________________________________________\")\n         print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n         print(\"_______________________________________________\")\n         print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","f7ad16a8":"from sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nfrom sklearn.model_selection import cross_val_score\n\nlog_clf = LogisticRegression()\nrnd_clf = RandomForestClassifier()\nxgb_clf = XGBClassifier()\nlgb_clf = lgb.LGBMClassifier()\nfrom sklearn.metrics import accuracy_score\nfor clf in (log_clf, rnd_clf, lgb_clf, xgb_clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)","85429da2":"print_score(log_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(log_clf, X_train, y_train, X_test, y_test, train=False)","3f9cdbb0":"print_score(rnd_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(rnd_clf, X_train, y_train, X_test, y_test, train=False)","9802e99f":"print_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)","5f34c7c6":"print_score(lgb_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(lgb_clf, X_train, y_train, X_test, y_test, train=False)","3841af1b":"!pip install pytorch_tabnet","b2dab521":"from pytorch_tabnet.tab_model import TabNetClassifier\nimport torch\n\n# define the model\nclf= TabNetClassifier(optimizer_fn=torch.optim.Adam,\n                       scheduler_params={\"step_size\":10, \n                                         \"gamma\":0.9},\n                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                      )\n\n# fit the model \nclf.fit(\n    X_train,y_train,\n    eval_set=[(X_train, y_train), (X_test, y_test)],\n    eval_name=['train', 'test'],\n    eval_metric=['auc','balanced_accuracy'],\n    max_epochs=200, patience=60,\n    batch_size=512, virtual_batch_size=512,\n    num_workers=0,\n    weights=1,\n    drop_last=False\n)            ","e9a0a2bf":"pred=clf.predict(X_test)\nprint(classification_report(y_test, y_pred))","f7ee34c5":"log_test_score = round(accuracy_score(y_test, log_clf.predict(X_test)) * 100,2)\nlog_accuracies = cross_val_score(estimator = log_clf, X = X_train, y = y_train, cv = 10)\nlog_train_score=round(log_accuracies.mean()*100,2)\n\nrnd_test_score = round(accuracy_score(y_test, rnd_clf.predict(X_test)) * 100,2)\nrnd_accuracies = cross_val_score(estimator = rnd_clf, X = X_train, y = y_train, cv = 10)\nrnd_train_score=round(rnd_accuracies.mean()*100,2)\n\nxgb_test_score = round(accuracy_score(y_test, xgb_clf.predict(X_test)) * 100,2)\nxgb_accuracies = cross_val_score(estimator = xgb_clf, X = X_train, y = y_train, cv = 10)\nxgb_train_score = round(xgb_accuracies.mean()*100,2)\n\nlgb_test_score = round(accuracy_score(y_test, lgb_clf.predict(X_test)) * 100,2)\nlgb_accuracies = cross_val_score(estimator = lgb_clf, X = X_train, y = y_train, cv = 10)\nlgb_train_score = round(lgb_accuracies.mean()*100,2)\n\n\nresults_df = pd.DataFrame(data=[[\"Logistic Regression\", log_train_score, log_test_score],\n                                ['Random Forrest',rnd_train_score, rnd_test_score],\n                                ['XGB',xgb_train_score,xgb_test_score],['LGB',lgb_train_score,lgb_test_score]],\n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df.index += 1 \nresults_df","3a983c43":"![](https:\/\/miro.medium.com\/max\/1400\/1*HFCK0uTXA6HcXzg-VfGXLQ.png)","2047989f":"Now finally let's get started to code!!","72f8fc6a":"\nwe have 890 rows and 12 columns present in our dataset with columns 'Age','Cabin' and 'Embarked' having some missing values.\n","30046f4b":"# 1. Loading the dataset","09541261":"Now before implementing the Tabnet code let's have a look at the parameters that are present in the tabnet model:\n\n## Model parameters\n### n_d : int (default=8)\n\nWidth of the decision prediction layer. Bigger values gives more capacity to the model with the risk of overfitting. Values typically range from 8 to 64.\n\n### n_a: int (default=8)\n\nWidth of the attention embedding for each mask. According to the paper n_d=n_a is usually a good choice. (default=8)\n\n### n_steps : int (default=3)\n\nNumber of steps in the architecture (usually between 3 and 10)\n\n### gamma : float (default=1.3)\n\nThis is the coefficient for feature reusage in the masks. A value close to 1 will make mask selection least correlated between layers. Values range from 1.0 to 2.0.\n\n### cat_idxs : list of int (default =[])\n\nList of categorical features indices.\n\n### cat_emb_dim : list of int\n\nList of embeddings size for each categorical features. (default =1)\n\n### n_independent : int (default=2)\n\nNumber of independent Gated Linear Units layers at each step. Usual values range from 1 to 5.\n\n### n_shared : int (default=2)\n\nNumber of shared Gated Linear Units at each step Usual values range from 1 to 5\n\n### epsilon : float (default 1e-15)\n\nShould be left untouched.\n\n### seed : int (default=0)\n\nRandom seed for reproducibility\n\n### momentum : float\n\nMomentum for batch normalization, typically ranges from 0.01 to 0.4 (default=0.02)\n\n### clip_value : float (default None)\n\nIf a float is given this will clip the gradient at clip_value.\n\n### lambda_sparse : float (default = 1e-3)\n\nThis is the extra sparsity loss coefficient as proposed in the original paper. The bigger this coefficient is, the sparser your model will be in terms of feature selection. Depending on the difficulty of your problem, reducing this value could help.\n\n### optimizer_fn : torch.optim (default=torch.optim.Adam)\n\nPytorch optimizer function\n\n### optimizer_params: dict (default=dict(lr=2e-2))\n\nParameters compatible with optimizer_fn used initialize the optimizer. Since we have Adam as our default optimizer, we use this to define the initial learning rate used for training. As mentionned in the original paper, a large initial learning of 0.02  with decay is a good option.\n\n### scheduler_fn : torch.optim.lr_scheduler (default=None)\n\nPytorch Scheduler to change learning rates during training.\n\n### scheduler_params : dict\n\nDictionnary of parameters to apply to the scheduler_fn. Ex : {\"gamma\": 0.95, \"step_size\": 10}\n\n### model_name : str (default = 'DreamQuarkTabNet')\n\nName of the model used for saving in disk, you can customize this to easily retrieve and reuse your trained models.\n\n### saving_path : str (default = '.\/')\n\nPath defining where to save models.\n\n### verbose : int (default=1)\n\nVerbosity for notebooks plots, set to 1 to see every epoch, 0 to get None.\n\n### device_name : str (default='auto') 'cpu' for cpu training, 'gpu' for gpu training, 'auto' to automatically detect gpu.\n\n### mask_type: str (default='sparsemax') Either \"sparsemax\" or \"entmax\" : this is the masking function to use for selecting features\n\n## Fit parameters\n\n### X_train : np.array\n\nTraining features\n\n### y_train : np.array\n\nTraining targets\n\n### eval_set: list of tuple\n\nList of eval tuple set (X, y).\nThe last one is used for early stopping\n\n### eval_name: list of str\nList of eval set names.\n\n### eval_metric : list of str\nList of evaluation metrics.\nThe last metric is used for early stopping.\n\n### max_epochs : int (default = 200)\n\nMaximum number of epochs for trainng.\n\n### patience : int (default = 15)\n\nNumber of consecutive epochs without improvement before performing early stopping.\n\n### weights : int or dict (default=0)\n\n\/!\\ Only for TabNetClassifier Sampling parameter 0 : no sampling 1 : automated sampling with inverse class occurences dict : keys are classes, values are weights for each class\n\n### loss_fn : torch.loss or list of torch.loss\n\nLoss function for training (default to mse for regression and cross entropy for classification) When using TabNetMultiTaskClassifier you can set a list of same length as number of tasks, each task will be assigned its own loss function\n\n### batch_size : int (default=1024)\n\nNumber of examples per batch, large batch sizes are recommended.\n\n### virtual_batch_size : int (default=128)\n\nSize of the mini batches used for \"Ghost Batch Normalization\"\n\n### num_workers : int (default=0)\n\nNumber or workers used in torch.utils.data.Dataloader\n\n### drop_last : bool (default=False)\n\nWhether to drop last batch if not complete during training\n\n### callbacks : list of callback function\nList of custom callbacks","67498d38":"We are getting an accuracy around 84.5% on the test set","49de2ea5":"# Models","42db3699":"In this notebook we will walkthrough and implement Google\u2019s TabNet for a classification problem ","c12f9445":"# TabNet \u2014 Deep Neural Network for Structured, Tabular data","c9194eff":"You don't have to be scared of that many parameters since we dont even have to change most of them.\nNow let's finally implement the Tabnet .","1ffe7522":"You guys can see the full code of the Tabnet of how it was made by clicking this [Github](https:\/\/github.com\/dreamquark-ai\/tabnet) link.","56c2db25":"## Conclusion\n\nWe can conclude that Tabnet is performing better than the mainstream models that generally give us the best accuracies.\nOf course we can increase the performance by preprocessing our data better and performing hyperparameter tuning to our models but since our main focus was to implement the Tabnet Classifier and have a basic idea of its performance in comparison to others models we didn't do that.\n","be96e7a7":"![](https:\/\/thumbor.forbes.com\/thumbor\/960x0\/https%3A%2F%2Fspecials-images.forbesimg.com%2Fdam%2Fimageserve%2F966248982%2F960x0.jpg%3Ffit%3Dscale)","747f7366":"We can see that XGBoost classifier is giving the best test accuracy among all the tree based models with 83.2% but our Tabnet classifier is performing better than that with 84.5% accuracy.","9b2e3b91":"# Tabnet","3511fe4c":"You guys can upvote if you liked the notebook and felt it was helpful , any suggestions are welcome since i'm still learning , i'll try to come up with more interesting and better notebooks ,thanks for reading and have a nice day :)","4459cbfa":"TabNet is a modern Neural Network architecture for tabular data.It is specifically made to adequately manage all those situations in which we work on data in tabular format.It was presented at the end of 2020 and its best description can be found in the article available on the usual ArXiv: https:\/\/arxiv.org\/pdf\/1908.07442.pdf","c12f8090":"# Preprocessing","10d59dba":"We know that neural networks give exceptional results on unstructured data, such as in the field of Image Recognition and Object Detection (Images), as in the field of Natural Language Processing (texts), in \u201cSpeech-to-text\u201d, often proving to be able to reach and overcome the \u201cHuman-Level Performance\u201d.\nBut the generalized idea, until now, is that on tabular data it is often easier and faster to obtain results using algorithms not based on Neural Networks, such as Gradient Boosting algorithms. In fact, it is sufficient to analyze the competition leaderboards on Kaggle for these types of data: the best results are obtained using implementations, which can also run quickly on GPUs, such as XGBoost, LightGBM, CatBoost.","8dfc76d1":"# What Is Tabnet","4293f252":"## Installation:\n\nYou can follow this [Link](https:\/\/dreamquark-ai.github.io\/tabnet\/generated_docs\/README.html#) for full installation and implementation instructions.\n\nThe implementation and installation of Tabnet is very easy , you can simply install it by '!pip install command'","e74e8dce":" #### In this notebook we will implement Tabnet Classifier on the good old Titanic Dataset since most of us are fimiliar with it and it is easily understandable","8fbecac6":"Now here comes the moment we all were waiting for , modelling the ML models and selecting the best one"}}