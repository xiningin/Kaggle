{"cell_type":{"1d8434b5":"code","432407ea":"code","7841fa3c":"code","db6e478f":"code","3edc3e8d":"code","58b67956":"code","28361809":"markdown","b98cc2b9":"markdown"},"source":{"1d8434b5":"import numpy as np \nimport pandas as pd \nfrom tqdm.notebook import tqdm\nfrom collections import deque\nfrom operator import itemgetter\n\n\n__all__ = ['hamming_distance', 'BKTree']\n\n__version__ = '1.1'\n\n_getitem0 = itemgetter(0)\n\n### BKTree\n### source https:\/\/github.com\/benhoyt\/pybktree\n\nclass BKTree(object):\n    \"\"\"BK-tree data structure that allows fast querying of matches that are\n    \"close\" given a function to calculate a distance metric (e.g., Hamming\n    distance or Levenshtein distance).\n    Each node in the tree (including the root node) is a two-tuple of\n    (item, children_dict), where children_dict is a dict whose keys are\n    non-negative distances of the child to the current item and whose values\n    are nodes.\n    \"\"\"\n    def __init__(self, distance_func, items=[]):\n        \"\"\"Initialize a BKTree instance with given distance function\n        (which takes two items as parameters and returns a non-negative\n        distance integer). \"items\" is an optional list of items to add\n        on initialization.\n        >>> tree = BKTree(hamming_distance)\n        >>> list(tree)\n        []\n        >>> tree.distance_func is hamming_distance\n        True\n        >>> tree = BKTree(hamming_distance, [])\n        >>> list(tree)\n        []\n        >>> tree = BKTree(hamming_distance, [0, 4, 5])\n        >>> sorted(tree)\n        [0, 4, 5]\n        \"\"\"\n        self.distance_func = distance_func\n        self.tree = None\n\n        _add = self.add\n        for item in items:\n            _add(item)\n\n    def add(self, item):\n        \"\"\"Add given item to this tree.\n        >>> tree = BKTree(hamming_distance)\n        >>> list(tree)\n        []\n        >>> tree.add(4)\n        >>> sorted(tree)\n        [4]\n        >>> tree.add(15)\n        >>> sorted(tree)\n        [4, 15]\n        \"\"\"\n        node = self.tree\n        if node is None:\n            self.tree = (item, {})\n            return\n\n        # Slight speed optimization -- avoid lookups inside the loop\n        _distance_func = self.distance_func\n\n        while True:\n            parent, children = node\n            distance = _distance_func(item, parent)\n            node = children.get(distance)\n            if node is None:\n                children[distance] = (item, {})\n                break\n\n    def find(self, item, n):\n        \"\"\"Find items in this tree whose distance is less than or equal to n\n        from given item, and return list of (distance, item) tuples ordered by\n        distance.\n        >>> tree = BKTree(hamming_distance)\n        >>> tree.find(13, 1)\n        []\n        >>> tree.add(0)\n        >>> tree.find(1, 1)\n        [(1, 0)]\n        >>> for item in [0, 4, 5, 14, 15]:\n        ...     tree.add(item)\n        >>> sorted(tree)\n        [0, 0, 4, 5, 14, 15]\n        >>> sorted(tree.find(13, 1))\n        [(1, 5), (1, 15)]\n        >>> sorted(tree.find(13, 2))\n        [(1, 5), (1, 15), (2, 4), (2, 14)]\n        >>> sorted(tree.find(0, 1000)) == [(hamming_distance(x, 0), x) for x in tree]\n        True\n        \"\"\"\n        if self.tree is None:\n            return []\n\n        candidates = deque([self.tree])\n        found = []\n\n        # Slight speed optimization -- avoid lookups inside the loop\n        _candidates_popleft = candidates.popleft\n        _candidates_extend = candidates.extend\n        _found_append = found.append\n        _distance_func = self.distance_func\n\n        while candidates:\n            candidate, children = _candidates_popleft()\n            distance = _distance_func(candidate, item)\n            if distance <= n:\n                _found_append((distance, candidate))\n\n            if children:\n                lower = distance - n\n                upper = distance + n\n                _candidates_extend(c for d, c in children.items() if lower <= d <= upper)\n\n        found.sort(key=_getitem0)\n        return found\n\n    def __iter__(self):\n        \"\"\"Return iterator over all items in this tree; items are yielded in\n        arbitrary order.\n        >>> tree = BKTree(hamming_distance)\n        >>> list(tree)\n        []\n        >>> tree = BKTree(hamming_distance, [1, 2, 3, 4, 5])\n        >>> sorted(tree)\n        [1, 2, 3, 4, 5]\n        \"\"\"\n        if self.tree is None:\n            return\n\n        candidates = deque([self.tree])\n\n        # Slight speed optimization -- avoid lookups inside the loop\n        _candidates_popleft = candidates.popleft\n        _candidates_extend = candidates.extend\n\n        while candidates:\n            candidate, children = _candidates_popleft()\n            yield candidate\n            _candidates_extend(children.values())\n\n    def __repr__(self):\n        \"\"\"Return a string representation of this BK-tree with a little bit of info.\n        >>> BKTree(hamming_distance)\n        <BKTree using hamming_distance with no top-level nodes>\n        >>> BKTree(hamming_distance, [0, 4, 8, 14, 15])\n        <BKTree using hamming_distance with 3 top-level nodes>\n        \"\"\"\n        return '<{} using {} with {} top-level nodes>'.format(\n            self.__class__.__name__,\n            self.distance_func.__name__,\n            len(self.tree[1]) if self.tree is not None else 'no',\n        )","432407ea":"data_dir = '..\/input\/shopee-product-matching'\ntrain_dir = '{}\/train.csv'.format(data_dir)\ndf_train = pd.read_csv(train_dir)\ndf_train.head()","7841fa3c":"def hex_to_hash(hexstr):\n    # modified function from imagehash\n    \n    hash_size = int(np.sqrt(len(hexstr)*4))\n    #assert hash_size == np.sqrt(len(hexstr)*4)\n    binary_array = '{:0>{width}b}'.format(int(hexstr, 16), width = hash_size * hash_size)\n    bit_rows = [binary_array[i:i+hash_size] for i in range(0, len(binary_array), hash_size)]\n    hash_array = np.array([[bool(int(d)) for d in row] for row in bit_rows])\n    return hash_array.flatten().astype(int)\n\ndef simple_hamming(x1, x2, idx =0):\n    #hamming distance between integer arrays\n    \n    return np.count_nonzero(x1[idx]!=x2[idx])\n\ndf_train['phash_vec'] = df_train['image_phash'].apply(hex_to_hash)\n\ntree_items = list(df_train[['phash_vec', 'label_group', 'title', 'image', 'posting_id']].values)\n\ntree = BKTree(simple_hamming, tree_items)\n\n# consider using different threshold\nhamming_distance = 1\n\nsuspicious_groups = []\n\n# O(nlogn)\nfor i in tqdm(range(len(tree_items))):\n    \n    neighbor_list = tree.find(tree_items[i], hamming_distance)\n    \n    if len(neighbor_list)>1:\n        \n        neighbor_labels = [neighbor_list[i][1][1] for i in range(len(neighbor_list))]\n    \n        # if all items in the neighbor_list don't have the same label then we assume\n        # that some samples have wrong label\n        \n        if len(set(neighbor_labels))>1:\n            print('>', end='')\n            suspicious_groups.append(neighbor_list)","db6e478f":"sus_ids = []\nfor group in suspicious_groups:\n    for item in group:\n        sus_ids.append(item[1][4])\n\nn_sus = len(set(sus_ids))\nprint('Total number of suspicious samples: {} or in percent: {}%'.format(n_sus, round(n_sus\/len(df_train)*100,2)))\nprint('Consider removing them from the training set')","3edc3e8d":"#sample\nsuspicious_groups[0]","58b67956":"import cv2\nimport matplotlib.pyplot as plt\n#plt.rcParams[\"figure.figsize\"] = (10,10)\n\ndef image_viz(image_path, title, ax):\n    \"\"\"\n    Function for visualization.\n    Takes path to image as input.\n    \"\"\"\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    \n    ax.imshow(img)\n    #plt.axis('off')\n    ax.set_title(title)\n    \ntrain_img_dir = '..\/input\/shopee-product-matching\/train_images'\n\n# show random 15 suspicious groups\n\nprint('Very similar images that have different label')\n\nn_cols=4\n\nfor i in range(15):\n    \n    sample = np.random.choice(suspicious_groups)\n    \n    n_rows = int((len(sample)-0.01)\/n_cols)+1\n    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols,figsize=(15, 5*n_rows))\n    labels = []\n    \n    for j, ax in enumerate(axes.flatten()):\n        \n        if j >= len(sample):\n            ax.plot(list(range(1000)), color = 'red')\n            continue\n            \n        img_path = f'{train_img_dir}\/{sample[j][1][3]}'\n        img_label_group = sample[j][1][1]\n        labels.append(img_label_group)\n        image_viz(img_path, img_label_group, ax)\n        \n    fig.suptitle(\"labels: \"+\"_\".join(str(x) for x in set(labels)))\n    plt.show()","28361809":"### Idea\n* Calculate phash matrices for images. Flatten them into vectors.\n* Based on the Hamming distance between vectors, decide what images might have the wrong label. The assumption is that if the group of very similar images has different labels, then some of the images might be wrong labeled. \n* Use BKTree to reduce time complexity (we can use this data structure since Hamming distance is metric).\n\nBKTree can be used also with other metrics like Levenshtein distance, Euclidean, L1, etc.\n\nGenerally, we call function $d$ metric if it satisfies following conditions\n$$d(x, y) >= 0\\\\ \n  d(x, y) = 0 \\iff x=y\\\\\n  d(x, y) = d(y, x)\\\\\n  d(x, y) <= d(x, z) + d(z, y),\n$$\nfor all $x, y$ and $z$ from the domain of the function $d$. \n\nNotice that cosine similarity can't be used in BKTree since it doesn't satisfy triangle inequality and it is not metric by definition.","b98cc2b9":"References:\n* https:\/\/www.kaggle.com\/maksymshkliarevskyi\/shopee-before-we-start-eda-phash-baseline\n"}}