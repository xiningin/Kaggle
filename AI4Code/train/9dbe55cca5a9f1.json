{"cell_type":{"edc62942":"code","793bc07b":"code","e7e83e6a":"code","bc7633b7":"code","c87c1a72":"code","766d2428":"code","abf27e0b":"code","245ce8b0":"code","db288ef0":"code","5264b0c3":"code","0b55c3b1":"code","9e9f416b":"code","33064042":"code","f628c62d":"code","0dbb3ce4":"code","3a77ddde":"code","bb3ea787":"markdown","97096a0e":"markdown","d14fd611":"markdown","0292f5f9":"markdown","7faf6ad2":"markdown"},"source":{"edc62942":"import keras.backend as K\nimport tensorflow.keras as keras\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.applications.vgg16 import VGG16\nimport random\nimport seaborn as sns\nimport os\nfrom PIL import Image\nimport cv2\nfrom keras.applications.vgg16 import decode_predictions\n# Visualizating filters\n# https:\/\/blog.keras.io\/how-convolutional-neural-networks-see-the-world.html\nfrom keras import backend as K\nK.set_learning_phase(1)\nimport tensorflow as tf","793bc07b":"train_dir='..\/input\/dogs-cats-images\/dataset\/training_set'\ntest_dir='..\/input\/dogs-cats-images\/dataset\/test_set'","e7e83e6a":"#Create Data Generator Since all our images need to be preprocessed using vgg16.preprocess _input\ndatagen=tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.vgg16.preprocess_input, \n)","bc7633b7":"original_img=plt.imread(\"..\/input\/dogs-cats-images\/dataset\/training_set\/cats\/cat.1.jpg\")\nprocessed_img=tf.keras.applications.vgg16.preprocess_input(original_img)\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,10))\nax1.imshow(original_img)\nax1.set_title(\"Original\")\nax2.imshow(processed_img)\nax2.set_title(\"Processed \")","c87c1a72":"#Images created from generator are pocessed using vgg16 preprocess function we have to deprocess them\ndef deprocess_img(processed_img):\n  x = processed_img.copy()\n  if len(x.shape) == 4:\n    x = np.squeeze(x, 0)\n  assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n  if len(x.shape) != 3:\n    raise ValueError(\"Invalid input to deprocessing image\")\n  \n  # perform the inverse of the preprocessiing step\n  x[:, :, 0] += 103.939\n  x[:, :, 1] += 116.779\n  x[:, :, 2] += 123.68\n  x = x[:, :, ::-1]\n\n  x = np.clip(x, 0, 255).astype('float32')\n  return x","766d2428":"#Create train and test sets(even though we don't need test here)\nbatch_size=6\ntrain_gen=datagen.flow_from_directory(train_dir,\n                                      target_size=(224,224),\n                                      batch_size=batch_size,\n                                      )\ntest_gen=datagen.flow_from_directory(test_dir,\n                                      target_size=(224,224),\n                                      batch_size=batch_size,\n                                      )","abf27e0b":"#import vgg16\nmodel = VGG16(weights='imagenet')","245ce8b0":"#utility Cell ,name of layers from the last conv layer untill the prediction layer\nlast_classifier_layers=[\n            \"block5_pool\",\n            \"flatten\",\n            \"fc1\",\n            \"fc2\",\n            \"predictions\",\n        ]","db288ef0":"#Create The Grad Cam function\ndef get_top_predicted_indices(predictions, top_n):\n    return np.argsort(-predictions).squeeze()[:top_n]\n\ndef make_gradcam_heatmap(\n    img_array, model, \n    last_conv_layer_name, \n    classifier_layer_names,\n    top_n,\n    class_indices\n):\n    #1. Create a model that maps the input image to the activations of the last convolution layer - Get last conv layer's output dimensions\n    last_conv_layer = model.get_layer(last_conv_layer_name)\n    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n    \n    #2. Create another model, that maps from last convolution layer to the final class predictions - This is the classifier model that calculated the gradient\n    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n    x = classifier_input\n    for layer_name in classifier_layer_names:\n        x = model.get_layer(layer_name)(x)\n    classifier_model = keras.Model(classifier_input, x)\n    \n    heatmaps = []\n    #5. Iteratively calculate heatmaps for all classes of interest using GradientTape\n    for index in (class_indices):\n    \n        #6. Watch the last convolution output during the prediction process to calculate the gradients\n        #7. Compute the activations of last conv layer and make the tape to watch\n        with tf.GradientTape() as tape:\n            # Compute activations of the last conv layer and make the tape watch it\n            last_conv_layer_output = last_conv_layer_model(img_array)\n            tape.watch(last_conv_layer_output)\n\n            #8. Get the class predictions and the class channel using the class index\n            preds = classifier_model(last_conv_layer_output)\n            class_channel = preds[:, index]\n#             print(class_channel)\n            \n        #9. Using tape, Get the gradient for the predicted class wrt the output feature map of last conv layer    \n        grads = tape.gradient(\n            class_channel,\n            last_conv_layer_output\n        )\n        \n        #10. Calculate the mean intensity of the gradient over its feature map channel\n        pooled_grads = tf.reduce_mean(grads, axis=(1,2,3))    \n        last_conv_layer_output = last_conv_layer_output.numpy()[0]\n        pooled_grads = pooled_grads.numpy()\n        \n        #11. Multiply each channel in feature map array by weight importance of the channel\n        for i in range(pooled_grads.shape[-1]):\n            last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n        #12. The channel-wise mean of the resulting feature map is our heatmap of class activation\n        heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n        #13. Normalize the heatmap between [0, 1] for ease of visualization\n        heatmap = np.maximum(heatmap, 0) \/ np.max(heatmap)\n\n        heatmaps.append({\n            \"class_id\": index,\n            \"heatmap\": heatmap\n        })\n\n    return heatmaps","5264b0c3":"imgs,_=train_gen.next()\nlabels=np.argmax(model.predict(imgs),axis=-1)\nheat=make_gradcam_heatmap(imgs,model,\"block5_conv3\",last_classifier_layers,1,labels)","0b55c3b1":"i=1\nfig=plt.figure(figsize=(16, 16))\nfor img in imgs:\n    fig.add_subplot(8,4,i)\n    plt.imshow(np.uint8(deprocess_img(img)))\n    i=i+1\nplt.show()","9e9f416b":"#used to put the heatmap on top of image\ndef superimpose_heatmap(image, heatmap):\n    img = image\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    heatmap = keras.preprocessing.image.array_to_img(heatmap)\n    heatmap = heatmap.resize((img.shape[1], img.shape[0]))\n    heatmap = keras.preprocessing.image.img_to_array(heatmap)\n    superimposed_img = cv2.addWeighted(heatmap, 0.4, img, 0.6,0)\n    superimposed_img = np.uint8(superimposed_img)\n    \n    return superimposed_img","33064042":"\nheat=make_gradcam_heatmap(imgs,model,\"block5_conv3\",last_classifier_layers,1,labels)","f628c62d":"i=1\nlabels=decode_predictions(model.predict(imgs),top=1)\nfig=plt.figure(figsize=(16, 16))\nfor i in range(len(imgs)):\n    fig.add_subplot(8,4,i+1)\n    plt.imshow(superimpose_heatmap(deprocess_img(imgs[i]),heat[i]['heatmap']))\n    plt.title(labels[i][0][1])\nfig.tight_layout() \nplt.show()","0dbb3ce4":"def create_saliency_maps(model,imgs):\n    maps=[]\n    for image in imgs:\n        image=tf.Variable(np.expand_dims(image,axis=0),dtype=float)    \n        with tf.GradientTape() as tape:\n            pred = model(image, training=False)\n            class_idxs_sorted = np.argsort(pred.numpy().flatten())\n            loss = pred[0][class_idxs_sorted[-1]]\n    \n        grads = tape.gradient(loss, image)\n        dgrad_abs = tf.math.abs(grads)\n        dgrad_max_ = np.max(dgrad_abs, axis=-1)[0]\n        ## normalize to range between 0 and 1\n        arr_min, arr_max  = np.min(dgrad_max_), np.max(dgrad_max_)\n        grad_eval = (dgrad_max_ - arr_min) \/ (arr_max - arr_min + 1e-18)\n        maps.append(grad_eval)\n\n    return maps","3a77ddde":"saliency_maps = create_saliency_maps(model,imgs)\nfig=plt.figure(figsize=(16, 16))\nfor i in range(len(imgs)):\n    fig.add_subplot(8,4,i+1)\n    plt.imshow(saliency_maps[i])\nplt.show()","bb3ea787":"we'll be using vgg16 as our baseline Model for this interpretability showcasing of visual Systems","97096a0e":"# Data Preperation","d14fd611":"# How Neural Network take their decision?\none of The big problems of Neural Network is the lack of inference ,we know they do well with variety of problems ,classification\/regression ... but a big question is how they do so ? answering this question can deepen our understanding of NN and help us create even better Networks .\nin This Kernel we'll be using VGG16 trained on ImageNet and we'll use different interpretabilty Techniques to see how vgg16 decide which class to assign to a certain image , what features are more important in the decision making process \n![image.png](attachment:63a909b2-d497-435b-b9f9-c1ba836e14d3.png)","0292f5f9":"# Grad-CAM implementation\nThe basic idea behind Grad-CAM is the same as the basic idea behind CAM: we want to exploit the spatial information that is preserved through convolutional layers, in order to understand which parts of an input image were important for a classification decision.\n\nThe idea is to Get the gradient for the predicted class wrt the output feature map of last conv layer,then use the calcuated gradient as weights for the output of the last conv layer , the result of this operation create ceartian heatmap serving as our visual reference of pixel importance    \n","7faf6ad2":"# Saliency Maps createion\nThe idea here is to see how the change value of each pixel would affect the the value of the loss , the bigger the effect (abs(gradient)) the more important the pixel is , so we take take the gradient of the loss of the predicted class wrt to the input image , take the absoulte value later we take the maximum value \nExample : point(n,n,c) ,point(n,n,c+1) (the bigger value will be taken ) \nand of course later on we should normalize ,in order to have good visual representation"}}