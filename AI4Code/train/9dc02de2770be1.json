{"cell_type":{"78305fb0":"code","6e6cb403":"code","84f50da2":"code","aa80d043":"code","fce6f004":"code","6d237d28":"code","3c861e53":"code","9d635b1c":"code","2b48ebd1":"code","a54dfb48":"code","4f449d9e":"code","4cb28dbd":"code","10e6401b":"code","dfc504b7":"code","eab6fb38":"code","b28c3e3a":"code","597024e7":"code","a6596f90":"code","af9cafe0":"code","2d5fb386":"code","dfc97270":"code","b004f8b0":"code","9af92fc5":"code","f87cc97f":"code","30215252":"code","bad1396b":"code","41ee344e":"code","a5a30671":"code","27ce6779":"code","7dc9364c":"code","d87740fe":"code","c0638b79":"code","ff7799ad":"code","fc028559":"code","eeba0771":"code","f2e38950":"code","679dd72f":"code","a070b408":"code","4dabe038":"code","1f44a994":"code","4189f105":"code","df02ac86":"code","32747967":"code","cd1f048c":"code","4825912e":"code","f51d82b4":"code","d6e8b11a":"code","fc0014b7":"code","68b7d5a2":"code","176e7a23":"code","606f29c5":"code","938fda15":"code","e3cd74e2":"code","2ffefb7a":"code","0fc703e0":"code","7692c7d0":"code","6849a9e5":"code","6292b523":"code","2211747e":"code","e43aa117":"code","fda52b71":"code","2a005604":"code","b4dc3a94":"code","1a662631":"code","93e2029a":"code","7e81ddd1":"code","6f46985e":"code","3e2f8a46":"code","d34aa2fb":"code","8827166a":"code","f3c2c5fd":"code","f48bd316":"code","0aa97c6f":"code","c2aeafd8":"code","e0b588de":"code","5d8810c8":"code","d9947953":"code","808f9a5f":"code","660b1ce8":"code","a8691daf":"code","72acf535":"code","13ed30cf":"code","b6a3d1d2":"code","5f188cf6":"code","894c0507":"code","30334370":"code","fd31a509":"code","b2b38621":"code","09339def":"code","2695997c":"code","a6632374":"code","5f0c6cd4":"code","881f0120":"code","73d8cb5d":"code","1dc666c1":"markdown","f834f897":"markdown","7d5ae67c":"markdown","b222d6b7":"markdown","9e3d2429":"markdown","e56f4f37":"markdown","d4660a9b":"markdown","29c52aae":"markdown","c78db928":"markdown","901e4a33":"markdown","968f75da":"markdown","26e0cad7":"markdown","3365854f":"markdown","c4fb81e1":"markdown","71ec5c7a":"markdown","6fd33a35":"markdown","911db19b":"markdown","4c7735c1":"markdown","60483e08":"markdown","cc203cb8":"markdown","e0f86dc3":"markdown","d6fb291a":"markdown","09f2fd0c":"markdown","d2c44170":"markdown","1e23fa7a":"markdown","d7df1e8e":"markdown","e6ccd349":"markdown","d42647be":"markdown","50735c72":"markdown","df40f22b":"markdown","b0babf33":"markdown","ceee25fa":"markdown","e6270961":"markdown","9ff32b45":"markdown","8dbea828":"markdown","9ad64c30":"markdown","f6eec8cc":"markdown","3eb2df10":"markdown","8ac13953":"markdown","8b89ea2b":"markdown","fca8d94b":"markdown","0f39b3f2":"markdown","9d0f89a7":"markdown","8c68418d":"markdown","c26ef548":"markdown","01c62eed":"markdown","eb974602":"markdown","61cbce9d":"markdown","8fece573":"markdown","daf6fcc4":"markdown","ed0bc229":"markdown","57792a07":"markdown","6e517c2b":"markdown","7a159858":"markdown","c3f85b12":"markdown","73b05397":"markdown","f354a9d1":"markdown","c3b3340c":"markdown","7410c183":"markdown","c010d47a":"markdown","f0e393c7":"markdown","96a8d86b":"markdown","27a1d62e":"markdown","4848e20b":"markdown","166fea65":"markdown","f65fe860":"markdown","9ccb5459":"markdown","0b0e0943":"markdown","47923b47":"markdown","81243d05":"markdown","d276a357":"markdown","4f063f6a":"markdown","6a5bfdc0":"markdown","6b2460ea":"markdown","25a53b85":"markdown","90a96e7e":"markdown","836d6497":"markdown","5bfc6c40":"markdown","f41226fb":"markdown"},"source":{"78305fb0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","6e6cb403":"df = pd.read_csv(\"..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\")","84f50da2":"df.shape","aa80d043":"df.isnull().sum()","fce6f004":"df.head()","6d237d28":"df.drop(['Serial No.'], axis = 1, inplace = True)","3c861e53":"df.head()","9d635b1c":"plt.figure(figsize=(25,10))\nsns.heatmap(df.corr(), annot=True, linewidth=0.5, cmap='coolwarm')","2b48ebd1":"sns.pairplot(df)","a54dfb48":"x = df['Chance of Admit ']\nsns.distplot(x , kde= True,rug = False, bins = 30)","4f449d9e":"x = df['GRE Score']\nsns.distplot(x , kde= True,rug = False, bins = 30)","4cb28dbd":"x = df['TOEFL Score']\nsns.distplot(x , kde= True,rug = False, bins = 30)","10e6401b":"x = df['CGPA']\nsns.distplot(x , kde= True,rug = False, bins = 30)","dfc504b7":"sns.lineplot(x = 'GRE Score', y = 'CGPA', data = df)","eab6fb38":"sns.lineplot(x = 'TOEFL Score', y = 'CGPA', data = df)","b28c3e3a":"sns.jointplot(x = 'GRE Score', y = 'CGPA', data=df)","597024e7":"sns.jointplot(x = 'TOEFL Score', y = 'CGPA', data=df)","a6596f90":"sns.jointplot(x = 'TOEFL Score', y = 'University Rating', data=df)","af9cafe0":"sns.jointplot(x = 'GRE Score', y = 'University Rating', data=df)","2d5fb386":"sns.relplot(x ='SOP', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')","dfc97270":"sns.relplot(x ='LOR ', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')","b004f8b0":"sns.relplot(x ='Research', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')","9af92fc5":"from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn import linear_model\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n\n\nfrom sklearn_pandas import DataFrameMapper\nfrom numpy import asarray\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler","f87cc97f":"X = df.drop(['Chance of Admit '], axis = 1)\nX","30215252":"y = df['Chance of Admit ']\ny","bad1396b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,shuffle = False)","41ee344e":"model1 = LinearRegression()\nmodel1.fit(X_train, y_train)\n\naccuracy1 = model1.score(X_test,y_test)\nprint(accuracy1*100,'%')","a5a30671":"y_pred1 = model1.predict(X_test)\n\nval = mean_squared_error(y_test, y_pred1, squared=False)\nval1 = str(round(val, 4))\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))\n","27ce6779":"model2 = DecisionTreeRegressor()\nmodel2.fit(X_train, y_train)\n\naccuracy2 = model2.score(X_test,y_test)\nprint(accuracy2*100,'%')","7dc9364c":"y_pred2 = model2.predict(X_test)\n\nval = mean_squared_error(y_test, y_pred2, squared=False)\nval2 = str(round(val, 4))\n\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))\n","d87740fe":"n_estimators = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]\n\nRF = RandomForestRegressor()\n\nparameters = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}\n\nRFR = GridSearchCV(RF, parameters,scoring='neg_mean_squared_error', cv=5)\n\nRFR.fit(X_train, y_train)\n\nRFR.best_params_\n","c0638b79":"model3 = RandomForestRegressor(n_estimators = 190)\nmodel3.fit(X_train, y_train)\n\naccuracy3 = model3.score(X_test,y_test)\nprint(accuracy3*100,'%')","ff7799ad":"y_pred3 = model3.predict(X_test)\n\nval = mean_squared_error(y_test, y_pred3, squared=False)\nval3 = str(round(val, 4))\n\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred3)))\n","fc028559":"lasso = Lasso()\n\nparameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n\nlasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv = 100)\n\nlasso_regressor.fit(X_train, y_train)\n\nlasso_regressor.best_params_","eeba0771":"model4 = linear_model.Lasso(alpha=.001)\nmodel4.fit(X_train,y_train)\n\naccuracy4 = model4.score(X_test,y_test)\nprint(accuracy4*100,'%')","f2e38950":"y_pred4 = model4.predict(X_test)\n\nval= mean_squared_error(y_test, y_pred4, squared=False)\nval4 = str(round(val, 4))\n\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred4)))\n","679dd72f":"alpha = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n\nridge = Ridge()\n\nparameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n\nridge_regressor = GridSearchCV(ridge, parameters,scoring='neg_mean_squared_error', cv=100)\n\nridge_regressor.fit(X_train, y_train)\nridge_regressor.best_params_","a070b408":"model5 = linear_model.Ridge(alpha=1)\nmodel5.fit(X_train,y_train)\n\naccuracy5 = model5.score(X_test,y_test)\nprint(accuracy5*100,'%')","4dabe038":"y_pred5 = model5.predict(X_test)\n\nval = mean_squared_error(y_test, y_pred5, squared=False)\nval5 = str(round(val, 4))\n\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred5)))\n","1f44a994":"Elasticnet = ElasticNet()\n\nparameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n\nen_regressor = GridSearchCV(Elasticnet, parameters, scoring='neg_mean_squared_error', cv = 100)\n\nen_regressor.fit(X_train, y_train)\nen_regressor.best_params_","4189f105":"model6 = linear_model.ElasticNet(alpha=0.001)\nmodel6.fit(X_train,y_train)\n\naccuracy6 = model6.score(X_test,y_test)\nprint(accuracy6*100,'%')","df02ac86":"y_pred6 = model6.predict(X_test)\n\nval = mean_squared_error(y_test, y_pred6, squared=False)\nval6 = str(round(val, 4))\n\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred6)))\n","32747967":"data1 = [['Linear Regression ',val1],['Decision Tree',val2],['Random Forest',val3],['Lasso Regression',val4],['Ridge Regression',val5],['ElasticNet Regression',val6]]\nd1 = pd.DataFrame(data1,columns = ['Without Scaling Models ','RMSE Error'])\nHalf1RMSE = d1.copy()\nHalf1RMSE\n","cd1f048c":"mapper = DataFrameMapper([(df.columns, StandardScaler())])\nscaled_features = mapper.fit_transform(df.copy(), 4)\ndata = pd.DataFrame(scaled_features, index=df.index, columns=df.columns)","4825912e":"data.head()","f51d82b4":"x = data.drop(['Chance of Admit '], axis = 1)\nx","d6e8b11a":"Y = data['Chance of Admit ']\nY","fc0014b7":"x_train, x_test, Y_train, Y_test = train_test_split(x, Y, test_size = 0.20,shuffle = False)","68b7d5a2":"model7 = LinearRegression()\nmodel7.fit(x_train, Y_train)\n\naccuracy7 = model7.score(x_test,Y_test)\nprint(accuracy7*100,'%')","176e7a23":"y_pred7 = model7.predict(x_test)\n\nval = mean_squared_error(Y_test, y_pred7, squared=False)\nval7 = str(round(val, 4))\n\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred7)))\n","606f29c5":"model8 = DecisionTreeRegressor()\nmodel8.fit(x_train, Y_train)\n\naccuracy8 = model8.score(x_test,Y_test)\nprint(accuracy8*100,'%')","938fda15":"y_pred8 = model8.predict(x_test)\n\nval = mean_squared_error(Y_test, y_pred8, squared=False)\nval8 = str(round(val, 4))\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred8)))\n","e3cd74e2":"n_estimators = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]\n\nrf = RandomForestRegressor()\n\nparameters = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}\n\nrfr = GridSearchCV(rf, parameters,scoring='neg_mean_squared_error', cv=10)\n\nrfr.fit(x_train, Y_train)\n\nrfr.best_params_","2ffefb7a":"model9 = RandomForestRegressor(n_estimators = 220)\nmodel9.fit(x_train, Y_train)\n\naccuracy9 = model9.score(x_test,Y_test)\nprint(accuracy9*100,'%')","0fc703e0":"y_pred9 = model9.predict(x_test)\n\nval = mean_squared_error(Y_test, y_pred9, squared=False)\nval9 = str(round(val, 4))\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred9)))\n","7692c7d0":"L = Lasso()\n\nparameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n\nLR = GridSearchCV(L, parameters, scoring='neg_mean_squared_error', cv = 100)\n\nLR.fit(x_train, Y_train)\nLR.best_params_","6849a9e5":"model10 = linear_model.Lasso(alpha=.01)\nmodel10.fit(x_train,Y_train)\n\naccuracy10 = model10.score(x_test,Y_test)\nprint(accuracy10*100,'%')","6292b523":"y_pred10 = model10.predict(x_test)\n\nval = mean_squared_error(Y_test, y_pred10, squared=False)\nval10 = str(round(val, 4))\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred10)))\n","2211747e":"EN = ElasticNet()\n\nparameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n\nENR = GridSearchCV(Elasticnet, parameters, scoring='neg_mean_squared_error', cv = 100)\n\nENR.fit(x_train, Y_train)\nENR.best_params_","e43aa117":"model11 = linear_model.Lasso(alpha=.01)\nmodel11.fit(x_train,Y_train)\n\naccuracy11 = model11.score(x_test,Y_test)\nprint(accuracy11*100,'%')","fda52b71":"y_pred11 = model11.predict(x_test)\n\nval = mean_squared_error(Y_test, y_pred11, squared=False)\nval11 = str(round(val, 4))\n\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred11)))\n","2a005604":"SVR = SVR()\n\nparameters = {'C':[.0001 ,.001 ,0.1, 1, 10, 100, 1000],\n              'epsilon':[0.001, 0.01, 0.1, 0.5, 1, 2, 4]\n             }\n\nENR = GridSearchCV(SVR, parameters, scoring='neg_mean_squared_error', cv = 10)\n\nENR.fit(x_train, Y_train)\nENR.best_params_","b4dc3a94":"from sklearn.svm import SVR\nmodel12 = SVR(C=1, epsilon=0.1)\nmodel12.fit(x_train,Y_train)\n\nmodel12 = model12.score(x_test,Y_test)\nprint(model12*100,'%')","1a662631":"R = Ridge()\n\nparameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n\nR = GridSearchCV(R, parameters, scoring='neg_mean_squared_error', cv = 100)\n\nR.fit(x_train, Y_train)\nR.best_params_","93e2029a":"model13 = linear_model.Ridge(alpha=10)\nmodel13.fit(x_train,Y_train)\n\naccuracy13 = model13.score(x_test,Y_test)\nprint(accuracy13*100,'%')","7e81ddd1":"y_pred13 = model13.predict(x_test)\n\nval = mean_squared_error(Y_test, y_pred13, squared=False)\nval13 = str(round(val, 4))\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred13)))\n","6f46985e":"data2 = [['Scaled Linear Regression',val7],['Scaled Decision Tree',val8],['Scaled Random Forest',val9],['Scaled Lasso Regression',val10],['Scaled Ridge Regression',val13],['Scaled ElasticNet Regression',val11]]\nd2 = pd.DataFrame(data2,columns = ['Standard Scaler - Model ','RMSE Error'])\nHalf2RMSE = d2.copy()\n","3e2f8a46":"Half2RMSE","d34aa2fb":"from pandas import DataFrame\ntrans = MinMaxScaler()\ndat = trans.fit_transform(df)\ndataset = DataFrame(dat)\n\ndf.head()","8827166a":"dataset.columns = ['GRE Score','TOEFL Score','University Rating','SOP','LOR','CGPA','Research','Chance of Admit']\n","f3c2c5fd":"ex = dataset.drop(['Chance of Admit'], axis = 1)\nex","f48bd316":"ey = dataset['Chance of Admit']\ney","0aa97c6f":"x_t, x_es, Y_t, Y_es = train_test_split(ex, ey, test_size = 0.20,shuffle = False)","c2aeafd8":"model14 = LinearRegression()\nmodel14.fit(x_t, Y_t)\n\naccuracy14 = model14.score(x_es,Y_es)\nprint(accuracy14*100,'%')","e0b588de":"y_pred14 = model14.predict(x_es)\n\nval = mean_squared_error(Y_es, y_pred14, squared=False)\nval14 = str(round(val, 4))\n\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred14)))\n","5d8810c8":"l = Lasso()\n\nparameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n\nlr = GridSearchCV(l, parameters, scoring='neg_mean_squared_error', cv = 100)\n\nlr.fit(x_t, Y_t)\nlr.best_params_","d9947953":"model15 = linear_model.Lasso(alpha=.001)\nmodel15.fit(x_t,Y_t)\n\naccuracy15 = model15.score(x_es,Y_es)\nprint(accuracy15*100,'%')","808f9a5f":"y_pred15 = model15.predict(x_es)\n\nval = mean_squared_error(Y_es, y_pred15, squared=False)\nval15 = str(round(val, 4))\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred15)))\n","660b1ce8":"r = Ridge()\n\nparameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n\nr = GridSearchCV(r, parameters, scoring='neg_mean_squared_error', cv = 100)\n\nr.fit(x_t, Y_t)\nr.best_params_","a8691daf":"model16 = linear_model.Ridge(alpha=0.01)\nmodel16.fit(x_t,Y_t)\n\naccuracy16 = model16.score(x_es,Y_es)\nprint(accuracy16*100,'%')","72acf535":"y_pred16 = model16.predict(x_es)\n\nval = mean_squared_error(Y_es, y_pred16, squared=False)\nval16 = str(round(val, 4))\n\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred16)))\n","13ed30cf":"en = ElasticNet()\n\nparameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n\nenr = GridSearchCV(en, parameters, scoring='neg_mean_squared_error', cv = 100)\n\nenr.fit(x_t, Y_t)\nenr.best_params_","b6a3d1d2":"model17 = linear_model.Lasso(alpha=.001)\nmodel17.fit(x_t,Y_t)\n\naccuracy17 = model17.score(x_es,Y_es)\nprint(accuracy17*100,'%')","5f188cf6":"y_pred17 = model17.predict(x_es)\n\nval = mean_squared_error(Y_es, y_pred17, squared=False)\nval17 = str(round(val, 4))\n\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred17)))\n","894c0507":"model18 = DecisionTreeRegressor()\nmodel18.fit(x_t, Y_t)\n\naccuracy18 = model18.score(x_es,Y_es)\nprint(accuracy18*100,'%')","30334370":"y_pred18 = model18.predict(x_es)\n\nval = mean_squared_error(Y_es, y_pred8, squared=False)\nval18 = str(round(val, 4))\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred18)))\n","fd31a509":"n_estimators = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]\n\nRf = RandomForestRegressor()\n\nparameters = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}\n\nRfr = GridSearchCV(Rf, parameters,scoring='neg_mean_squared_error', cv=10)\n\nRfr.fit(x_t, Y_t)\n\nRfr.best_params_","b2b38621":"model19 = RandomForestRegressor(n_estimators = 100)\nmodel19.fit(x_t, Y_t)\n\naccuracy19 = model19.score(x_es,Y_es)\nprint(accuracy19*100,'%')","09339def":"y_pred19 = model19.predict(x_es)\n\nval = mean_squared_error(Y_es, y_pred19, squared=False)\nval19 = str(round(val, 4))\n\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred19)))\n","2695997c":"from sklearn.svm import SVR\n\nSvr = SVR()\n\nparameters = {'C':[.0001 ,.001 ,0.1, 1, 10, 100, 1000],\n              'epsilon':[0.001, 0.01, 0.1, 0.5, 1, 2, 4]\n             }\n\nEnr = GridSearchCV(Svr, parameters, scoring='neg_mean_squared_error', cv = 10)\n\nEnr.fit(x_t, Y_t)\nEnr.best_params_","a6632374":"from sklearn.svm import SVR\nmodel20 = SVR(C=1, epsilon=0.1)\nmodel20.fit(x_t,Y_t)\n\nmodel20 = model20.score(x_es,Y_es)\nprint(model20*100,'%')","5f0c6cd4":"data3 = [['Scaled Linear Regression',val14],['Scaled Lasso Regression',val15],['Scaled Ridge Regression',val16],['Scaled ElasticNet Regression',val17],['Scaled Decision Tree',val18],['Scaled Random Forest',val19]]\nd3 = pd.DataFrame(data3,columns = ['Min Max Scaler - Model ','RMSE Error'])\nHalf3RMSE = d3.copy()\nHalf3RMSE","881f0120":"frames = [Half1RMSE,Half2RMSE,Half3RMSE] \nFullRMSE = pd.concat(frames, axis = 1)","73d8cb5d":"FullRMSE","1dc666c1":"**6.B) ELASTICNET REGRESSION WITHOUT SCALING**","f834f897":"**19.B) MIN MAX SCALER RANDOM FOREST**","7d5ae67c":"**13.C)RMSE VALUE FOR STANDARD SCALER RIDGE REGRESSION**","b222d6b7":"**5.b) RIDGE REGRESSION WITHOUT SCALING**","9e3d2429":"**18.A) SCALED DECISION TREE MIN MAX SCALER**","e56f4f37":"**If the notebook was helpful, drop an upvote. Much appreciated and thank you!**","d4660a9b":"**4.b) LASSO REGRESSION WITHOUT SCALING**","29c52aae":"Higher number of admits are there for SOP from Univesitites having rating 4 and 5. This gradually declines as ratings go down.","c78db928":"The TOEFL scores are also a bit higher for the students from higher rated Universities. ","901e4a33":"**15.B) LASSO REGRESSION FOR MIN MAX SCALER**","968f75da":"**4.) GRID SEARCH FOR ALPHA FOR LASSO REGRESSION**","26e0cad7":"**11.B) ELASTICNET REGRESSION WITH SCALING**","3365854f":"Maximum of the students have a CPGA between 8 and 9 with peak around 8.5","c4fb81e1":"Students having good CGPA also have a good score in TOEFL.\n","71ec5c7a":"The 1st column represents datasets which have not been scaled and the models are used on them.\n\n\nThe 3rd column 'Standard Scaler - Model' means that StandardScaler library was used on the dataset. The successive RMSE Error are included in the 4th column for the Models.\n\n\nThe 5th column 'Min Max Scaler - Model' means that MinMaxScaler library was used on the dataset. The successive RMSE Error are included in the 6th columns for the Models.\n\n","6fd33a35":"**5.c) RMSE FOR RIDGE REGRESSION**","911db19b":"# Therefore, ElasticNet Regression without Scaler Transform provides the lowest value for RMSE.","4c7735c1":"**5.a) GRID SEARCH FOR ALPHA FOR RIDGE REGRESSION**","60483e08":"The students having higher CGPA also have high GRE Scores. Most of the students have a CGPA of 8.5 and GRE Score from the range of 310-330","cc203cb8":"**19.A) GRID SEARCH FOR RANDOM FOREST MIN MAX SCALER**","e0f86dc3":"**13.B)RIDGE REGRESSION WITH STANDARD SCALER**","d6fb291a":"**17.A) GRID SEARCH FOR APLHA FOR ELASTICNET**","09f2fd0c":"**6.C) RMSE VALUE FOR ELASTICNET REGRESSION**","d2c44170":"**17.B) ELASTICNET REGRESSION FOR MIN MAX SCALER**","1e23fa7a":"**2.a) DECISION TREE RMSE WITHOUT SCALING**","d7df1e8e":"**9.B) RANDOM FOREST WITH SCALING**","e6ccd349":"**10.B) LASSO REGRESSION WITH SCALING**","d42647be":"**16.B) RIDGE REGRESSION FOR MIN MAX SCALER**","50735c72":"Most of the students have an Admit Chance of 0.6-0.8 when they fulfill certain criterias.","df40f22b":"**12.B) SVR WITH SCALING**","b0babf33":"**8.B) RMSE VALUE FOR DECISION TREE WITH SCALING**","ceee25fa":"**15.A) GRID SEARCH FOR ALPHA FOR LASSO REGRESSION**","e6270961":"**This shows the correlation between several features.**\n\n1. Chance of Admit increases with GRE Score, TOEFL Score and CGPA.\n2. Chance of Admit is also dependent on University Rating, SOP and LOR.Atlhough, the dependency is relatively less than GRE, TOEFL and CGPA.\n3. Chance of Admit is moderately dependent on Research. \n\n**Correlation is positive here for majority of the features. This means as the Value for a certain feature increases, the Chance of Admit also increases. They are lineraly dependent on each other. Generally, when correlation is 0, it means that there is no relationship between the two features you are looking at.**\n\n1 shows high dependece. Values near to -1 shows inverse relationship.","9ff32b45":"This shows a linear relationship between CPGA and GRE Score.","8dbea828":"**20.B) SVR WITH MIN MAX SCALER**","9ad64c30":"**9.C) RMSE VALUE FOR RANDOM FOREST WITH SCALING**","f6eec8cc":"**13.A) GRID SEARCH FOR SCALER RIDGE REGRESSION**","3eb2df10":"**1.a) RMSE VALUE FOR LINEAR REGRESSION WITHOUT SCALING**","8ac13953":"The higher the University Rating, the better is the LOR and the chances of admit are better.","8b89ea2b":"**10.A) GRID SEARCH FOR ALPHA FOR LASSO REGRESSION WITH SCALING**","fca8d94b":"**15.C) RMSE VALUE FOR SCALED LASSO REGRESSION**","0f39b3f2":"**7.A) LINEAR REGRESSION WITH SCALING**","9d0f89a7":"**17.C) RMSE VALUE FOR ELASTICNET REGRESSION FOR MIN MAX SCALER**","8c68418d":"GRE scores are much better for students from Higher rated Universities.","c26ef548":"**18.B) RMSE ERROR FOR SCALED DECISION TREE**\n","01c62eed":"# APPLYING STANDARD SCALER TO DATASET","eb974602":"**16.C) RMSE VALUE FOR SCALED MIN MAX SCALER**","61cbce9d":"This shows the relationship between all the features in the form of ScatterPlot.","8fece573":"**14.A) LINEAR REGRESSION WITH SCALING**","daf6fcc4":"**7.B) RMSE VALUE FOR LINEAR REGRESSION WITH SCALING**","ed0bc229":"The GRE Scores are distributed across many Scores. They start at 290 and maximizes at around 310-325 and then decreases for majority of the students.","57792a07":"Serial Number can be dropped here as it used for indexing and it shows no relationship with 'Chance of Admit '.","6e517c2b":"**1.) LINEAR REGRESSION WITHOUT SCALING**","7a159858":"**3.a) RANDOM FOREST WITHOUT SCALING**","c3f85b12":"**2.) DECISION TREE WITHOUT SCALING**","73b05397":"**8.A) DECISION TREE WITH SCALING**","f354a9d1":"**6.A) GRID SEARCH FOR ELASTICNET REGRESSION**","c3b3340c":"**3.) GRID SEARCH FOR N_ESTIMATORS VALUE**","7410c183":"# MODEL SELECTION\n","c010d47a":"**11.A) GRID SEARCH FOR ELASTICNET REGRESSION WITH SCALING**","f0e393c7":"**14.B) RMSE LINEAR REGRESSION FOR MIN MAX SCALER**","96a8d86b":"**12.A) GRID SEARCH FOR SVR WITH SCALING**","27a1d62e":"**11.C) RMSE FOR ELASTICNET REGRESSION WITH SCALING**","4848e20b":"**Shuffle = False means that values are not randomized and used as they are given in the .CSV file. The first 400 values are used for Training and the last 100 for testing\/CV.** ","166fea65":"Maximum students have TOEFL Score between 100 and 110 with peak at 105","f65fe860":"**19.C) RMSE VALUE FOR MIN MAX SCALER RANDOM FOREST**","9ccb5459":"# DATA ANALYSIS","0b0e0943":"**4.c) RMSE FOR LASSO REGRESSION**","47923b47":"**20.A) GRID SEARCH FOR SVR WITH MIN MAX SCALER**","81243d05":"This shows a linear relationship between CGPA and TOEFL Score.","d276a357":"# **MinMax Scaler Transform**","4f063f6a":"In this notebook, the following models are used.\n1. Linear Regression\n2. Decision Tree\n3. Random Forest\n4. Lasso Regression\n5. Ridge Regression\n6. ElasticNet Regression\n7. SVR\n\n\n\nI have also used **Grid Search** for HyperParameter Tuning.\n\nI have also done scaling of the dataset with **StandardScaler and MinMaxScaler**.\n\nI have evaluated model thrice.\n\n1. Evaluating models without Scaling\n2. Evaluating models with Standard Scaling\n3. Evaluating models with MinMax Scaling\n\n\nThe result of the lowest RMSE values are attached at the end in the form of DataFrame.","6a5bfdc0":"**9.A) GRID SEARCH FOR N_ESTIMATORS FOR RANDOM FOREST WITH SCALING**","6b2460ea":"**16.A) GRID SEARCH FOR ALPHA FOR RIDGE REGRESSION**","25a53b85":"**Shuffle = False means that values are not randomized and used as they are given in the .CSV file. The first 400 values are used for Training and the last 100 for testing\/CV.** \n","90a96e7e":"As the University Rating increases so does the Research and this increases the Chance of Admit.","836d6497":"**3.b) RANDOM FOREST RMSE WITHOUT SCALING**","5bfc6c40":"**10.C) RMSE FOR LASSO REGRESSION WITH SCALING**","f41226fb":"**This shows that LASSO REGRESSION and ELASTICNET REGRESSION has the minimum RMSE for the dataset. Moreover, they both have the same Accuracy which is 90.15 after scaling the dataset**"}}