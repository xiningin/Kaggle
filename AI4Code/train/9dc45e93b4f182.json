{"cell_type":{"878acd4b":"code","fe0444d5":"code","29663eeb":"code","a453703f":"code","87c39138":"code","7e9e1223":"code","a8dba96b":"code","5bdfd335":"code","1fe141dc":"code","98333f67":"code","2835a99f":"code","cb88dc09":"code","06cce5c4":"code","d04b4ef9":"code","5d4df65a":"code","44462c65":"code","796bfd7f":"code","a94618e8":"code","874659eb":"code","aee16b24":"code","b2d18bd5":"code","047d2133":"code","c78cdec2":"code","58700898":"code","a8e5ae4d":"code","88cb4d6d":"code","e48413a9":"code","50429ab1":"code","5e3fe47a":"code","5734a9ca":"code","fc5992a5":"code","1208ccbd":"code","86b5c9b6":"markdown","92cbdb7d":"markdown","7396b0ed":"markdown","4e9cde5a":"markdown","667e2c81":"markdown","1b6139ca":"markdown","9fe0b816":"markdown","fada9cf0":"markdown","a878905e":"markdown","1e06feec":"markdown","614e1360":"markdown","e75a6496":"markdown","6388ea97":"markdown","e74c764b":"markdown","03daa819":"markdown","745d354d":"markdown","b5abfb4d":"markdown","3e823d99":"markdown","3a68e821":"markdown","e44a9e32":"markdown","1e97334f":"markdown","3a8adf85":"markdown","fd4ee70b":"markdown","117c98aa":"markdown","85949580":"markdown","031dcdbd":"markdown","71edbfa3":"markdown","84217448":"markdown","a14b7f18":"markdown","a3dcac15":"markdown","4df1ffc6":"markdown","0840c54f":"markdown","f28b2d84":"markdown","ad10cfff":"markdown","12264877":"markdown","499ddfbc":"markdown","41d9e2c0":"markdown","a65203eb":"markdown","56ddf540":"markdown","493f42ea":"markdown","70458e94":"markdown","5be56572":"markdown","271b7226":"markdown","e63bb192":"markdown","9f62fb17":"markdown","66126ea1":"markdown","07018a04":"markdown","f79c3d2e":"markdown","25eab717":"markdown","e62d2948":"markdown","bd6d905e":"markdown","e0f63584":"markdown","c6fcee95":"markdown","25301f12":"markdown"},"source":{"878acd4b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.model_selection import train_test_split\nfrom nltk.stem.snowball import SnowballStemmer","fe0444d5":"train=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ndataset=pd.concat([train,test])\nprint(f'train:{train.shape}\\ntest:{test.shape}\\ndataset:{dataset.shape}')","29663eeb":"train.head()","a453703f":"test.head()","87c39138":"(train.isnull().sum()[train.isnull().sum()>0]\/len(train))*100","7e9e1223":"pd.DataFrame({'Test Data Missing':(test.isnull().mean()*100).sort_values(ascending=False)})","a8dba96b":"non_dis = train[train.target==0]['text']\nnon_dis.values[7]","5bdfd335":"dis=train[train.target==1]['text']\ndis.values[7]","1fe141dc":"train.target.value_counts()","98333f67":"plt.figure(figsize=(6,6))\nsns.barplot(train.target.value_counts().index,train.target.value_counts())","2835a99f":"train.keyword.nunique()","cb88dc09":"plt.figure(figsize=(12,12))\nsns.barplot(y=train.keyword.value_counts().index[:15],x=train.keyword.value_counts()[:15])","06cce5c4":"print(train.location.nunique())","d04b4ef9":"plt.figure(figsize=(12,12))\nsns.barplot(y=train.location.value_counts().index[:15],x=train.location.value_counts()[:15])","5d4df65a":"plt.figure(figsize=(12,12))\nsns.barplot(y=train.location.value_counts().index[-10:],x=train.location.value_counts()[-10:])","44462c65":"def lowercase_text(text):\n    return text.lower()\n\ntrain.text=train.text.apply(lambda x: lowercase_text(x))\ntest.text=test.text.apply(lambda x: lowercase_text(x))","796bfd7f":"train.text.head(5)","a94618e8":"def remove_noise(text):\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","874659eb":"train.text=train.text.apply(lambda x: remove_noise(x))\ntest.text=test.text.apply(lambda x: remove_noise(x))","aee16b24":"train.text.head(5)","b2d18bd5":"!pip install nlppreprocess\nfrom nlppreprocess import NLP\n\nnlp = NLP()\n\ntrain['text'] = train['text'].apply(nlp.process)\ntest['text'] = test['text'].apply(nlp.process)  ","047d2133":"train.text.sample(10)","c78cdec2":"stemmer = SnowballStemmer(\"english\")\n\ndef stemming(text):\n    text = [stemmer.stem(word) for word in text.split()]\n    return ' '.join(text)\n\ntrain['text'] = train['text'].apply(stemming)\ntest['text'] = test['text'].apply(stemming)","58700898":"from wordcloud import WordCloud\nfig , ax1 = plt.subplots(1,figsize=(12,12))\nwordcloud=WordCloud(background_color='white',width=600,height=600).generate(\" \".join(train.text))\nax1.imshow(wordcloud)\nax1.axis('off')\nax1.set_title('Frequent Words',fontsize=24)","a8e5ae4d":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer=CountVectorizer(analyzer='word',binary=True)\ncount_vectorizer.fit(train.text)\n\ntrain_vec = count_vectorizer.fit_transform(train.text)\ntest_vec = count_vectorizer.transform(test.text)\n\nprint(train_vec[7].todense())\nprint(test_vec[7].todense())","88cb4d6d":"y=train.target","e48413a9":"from sklearn import model_selection\nmodel =MultinomialNB(alpha=1)\nscores= model_selection.cross_val_score(model,train_vec,y,cv=6,scoring='f1')\nscores","50429ab1":"model.fit(train_vec,y)","5e3fe47a":"sample_submission=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","5734a9ca":"sample_submission.target= model.predict(test_vec)","fc5992a5":"sample_submission.head()","1208ccbd":"sample_submission.to_csv('submission.csv',index=False)","86b5c9b6":"<a id=\"sec14\"><\/a>\n#  References","92cbdb7d":"So let's create our bag of words then ! If you do not know about bag of words , you can read about it here >>\n[BAG OF WORDS](https:\/\/machinelearningmastery.com\/gentle-introduction-bag-words-model\/#:~:text=A%20bag%2Dof%2Dwords%20is,the%20presence%20of%20known%20words.)","7396b0ed":"<a id=\"sec2\"><\/a>\n## Exploring the data\n","4e9cde5a":"Let's fill the target column !","667e2c81":"<a id=\"sec13\"><\/a>\n## Modelling","1b6139ca":"# <a id=\"top_section\"><\/a>\n\n<div align='center'><font size=\"5\" color=\"#000000\"><b>NLP with disaster tweets!-Starter modelling , data cleaning and explanation <br>(~80% accuracy)<\/b><\/font><\/div>\n<hr>\n<div align='center'><font size=\"5\" color=\"#000000\">About the problem<\/font><\/div>\n<hr>\n\nIn this competition, you\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t.<br>\nI have two notebooks on this competition , the first one is using basic naive-bayes model whereas the second is by using BERT pre-trained model. If you're a beginner I highly recommend you to to start with this notebook! After that if you want to enhance your accuracy and read about how we can implement this model using BERT then do check out the second notebook here : <br><br>\n<a class=\"nav-link active\"  style=\"background-color:; color:Blue\"  href=\"https:\/\/www.kaggle.com\/friskycodeur\/nlp-with-disaster-tweets-bert-explained\" role=\"tab\">NLP with disaster tweets!(BERT explained)<\/a>\n\n<br>\n<a href=\"https:\/\/ibb.co\/nm4kTk1\"><img src=\"https:\/\/i.ibb.co\/54Ccdcj\/Aquamarine-and-Orange-Pixel-Games-Collection-You-Tube-Icon.png\" alt=\"Aquamarine-and-Orange-Pixel-Games-Collection-You-Tube-Icon\" border=\"0\" height=300 width=300><\/a>\n\n\n### Here are the things I will try to cover in this Notebook:\n\n- Basic EDA of the text data.\n- Data cleaning\n- Transforming text into vectors\n- Building our model \n\n### If you like this kernel feel free to upvote and leave feedback, thanks!","9fe0b816":"Now that we have seen how our data is , how much it is missing and some counts, let's visualize our data so that we can to more explore and make better of it!","fada9cf0":"Now we will use NLP preprocessing to process our data ! This actually gave me better results so , let's use it !","a878905e":"Up until now , we have done all the processing to the texts , but you and I both know that our system cannot really read any language(English in this case) so how do we train it on this data ?","1e06feec":"<img src='https:\/\/media.tenor.com\/images\/0bf00f08e5e5cce9bb1ec5899cbc046b\/tenor.gif'>","614e1360":"So we have seen how some locations have very high tweeting activity whereas some have very low , and how alot of keywords were highly used and how many of them were alot hinting towards the nature of the tweet(i.e disastarious or non-disastarious).","e75a6496":"<img src='https:\/\/i.pinimg.com\/originals\/2f\/08\/84\/2f088410e696203853ecf91a3fbcd0f4.gif'>","6388ea97":"<img src='https:\/\/i.gifer.com\/EP97.gif'>","e74c764b":"We will start with cleaning basic text noises such as URLS , Email IDS , punctautions etc.","03daa819":" Mind take a sneak peak at our data set ! ;)\n\n<img src='https:\/\/media1.tenor.com\/images\/41597f32f2989333d14515fb1b7a9b4f\/tenor.gif?itemid=13480143'>","745d354d":"Finally ,let's convert our predictions into .csv file and submit it !","b5abfb4d":"Now comes one of the most important parts of any Natural Language Processing Problem ! Let's clean our data !","3e823d99":"<img src=\"https:\/\/media1.giphy.com\/media\/j2ersR5s9rDnUpMDBI\/giphy.gif\" alt=\"Thank you!\" width=\"500\" height=\"600\">","3a68e821":"Let's see how much of the keywords were actualy unique ! We will use the nunique function of pandas for this !","e44a9e32":"<a id=\"sectionlst\"><\/a>\n#  Submission\n\n<a href=\"#toc_section\" class=\"btn btn-primary\" style=\"color:white;\" >Back to Table of Content<\/a>","1e97334f":"Let's see how many disaster and non-disaster tweets are actually there in our data !","3a8adf85":"First we will store the target data into a variable !","fd4ee70b":"Now let's import our datasets , both train and test.","117c98aa":"<a id=\"sec8\"><\/a>\n### Stemming","85949580":"Let's see the top 15 locations where the most tweets come from !","031dcdbd":"Now we will use the sample_submission csv file as reference and fill the target column with our predictions !","71edbfa3":"- [Basic EDA,Cleaning and GloVe](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove)\n- [NLP with Disaster Tweets - EDA, Cleaning and BERT](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n- [Disaster NLP: Keras BERT using TFHub](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub)\n","84217448":"<a id=\"toc_section\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> Table of Content<\/h3>\n\n* [Introduction](#top_section)\n* [Importing the Required Libraries and Data](#sec1)\n* [Exploring the Data](#sec2)\n    - [Visualizing given dataset](#sec3)\n* [Text Pre-processing](#sec4)\n    - [Data cleaning](#sec5)\n    - [Using NLP processing](#sec7)\n    - [Stemming](#sec8)\n    - [Frequent words using WordCloud](#sec9)\n* [Transform token in vectors](#sec10)\n    - [Bag of words](#sec11)\n* [Modelling](#sec13)\n* [Submission & Some Last Words](#sectionlst)\n* [References](#sec14)\n","a14b7f18":"We have pre-processed our data , converted it so that our machine can actually process and use it ! <br>\nNow comes the final step , let's get our model ready !","a3dcac15":"<a id=\"sec11\"><\/a>\n### Using Bag of words","4df1ffc6":"Simple we will convert the text data into numerical vectors ! ;) <br>\nFor this we can use two approaches , the first one being Bag-of-Words and the second one being TFIDF.<br>\nFor this model I will be using bag of words !","0840c54f":"# Some last words:\n\nThank you for reading! I'm still a beginner and want to improve myself in every way I can. So if you have any ideas to feedback please let me know in the comments section!\n\n\n<div align='center'><font size=\"3\" color=\"#000000\"><b>And again please star if you liked this notebook so it can reach more people, Thanks!<\/b><\/font><\/div>","f28b2d84":"Some highly used keywords are fatalities , sinking , harm , damage , etc which can actually be very helpful in finding either the given tweet is disaster related or not !","ad10cfff":"Let us start with importing all the required libraries ! We will use the basic libraries to play with data(numpy,pandas,etc),some text related libraries (re,string,nltk,etc) and various model libraries.","12264877":"<a id=\"sec9\"><\/a>\n### Frequent words using wordcloud","499ddfbc":"Do you want to increase your accuracy ? Do you want to know how to get to 84-85 % accuracy ? Do you want to know how BERT can help attain that accuract? Do you want to know if it is possible to get to 100% accuracy ?If yes , then Check out my other notebook on the same problem here :\n<a class=\"nav-link active\"  style=\"background-color:; color:Blue\"  href=\"https:\/\/www.kaggle.com\/friskycodeur\/nlp-with-disaster-tweets-bert-explained\" role=\"tab\">NLP with disaster tweets!(BERT explained)<\/a>","41d9e2c0":"<a id=\"sec4\"><\/a>\n## Text Pre-processing","a65203eb":"Now let's train our model !","56ddf540":"First let's see the count of disaster and non-disaster tweets !","493f42ea":"<a id=\"sec3\"><\/a>\n## Visualizing the data !\n","70458e94":"All the functions are below and quiet basic !","5be56572":"Now we have to stem our text , will be using SnowballStemmer as it is quite good for the job ! So let's just get to the code !","271b7226":"<a id=\"sec1\"><\/a>\n## Importing the required libraries and data\n","e63bb192":"<a id=\"sec10\"><\/a>\n##  Transform token in vectors","9f62fb17":"What are the places where the least tweets were tweeted from ? Let's find out !","66126ea1":"<a id=\"sec7\"><\/a>\n### Using NLP processing","07018a04":"Mind taking a sneak-peak? :P","f79c3d2e":"Let's see the top 15 most used keywords ! Maybe we can get some insights from this !","25eab717":"Let's see how much of our data is missing !","e62d2948":"We will use a multinomial Naive Bayes model for this notebook ! You can go ahead and choose your own model as per you like , can also play with this model's parameters so as to increase it's accuracy! But for me this gave a accuracy of around 79.6% ","bd6d905e":"This is just a fun part , I loved this thing i found in one of the notebooks so i added it in mine ! <br>\nThis is a wordcloud of the frequent words in our text and it's actually quite cool to look at !","e0f63584":"We will deal with the missing data a bit later. But first let's look at some examples of disaster and non-disaster tweets !","c6fcee95":"Now let's see the unique locations that the tweets in our dataset were tweeted from !","25301f12":"<a id=\"sec5\"><\/a>\n### Data cleaning"}}