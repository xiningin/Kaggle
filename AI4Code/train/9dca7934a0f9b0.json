{"cell_type":{"06533e7b":"code","739560b7":"code","56eb8116":"code","adae3e6b":"code","c7d83e7a":"code","ae025a2f":"code","de976514":"code","51988118":"code","68209a81":"code","3a94491a":"code","c3fe6dfc":"code","d8eccbbb":"code","0c2c9c39":"code","c14dee5d":"code","5bc394b8":"code","e81d2895":"code","78a74383":"code","49f375a3":"code","d5370b66":"code","7e2dd52a":"code","10ad069f":"code","a6301efd":"code","40880e5c":"code","634436f3":"code","cda8e591":"code","7661d0f0":"code","e038cef5":"code","20ecdc57":"code","0fbb238d":"code","02a45b7e":"code","af1fc4e4":"code","2fa9ccee":"code","d99fb7ae":"code","61c77b84":"code","1d96c062":"code","0a962a7e":"markdown","64e262f1":"markdown","6fc16be6":"markdown","fb9ebbbd":"markdown","10c37cfe":"markdown","cdceb5a5":"markdown","0b9bbd6b":"markdown","520c895c":"markdown"},"source":{"06533e7b":"import numpy as np\nimport re\nfrom scipy.spatial.distance import cosine\nnp.random.seed(13)\nfrom keras.models import Sequential, Model\nfrom keras.layers import Embedding, Reshape, Activation, Input\nfrom keras.layers.merge import Dot\nfrom keras.utils import np_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import skipgrams\nimport numpy as np\nnp.random.seed(13)\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Reshape\nfrom IPython.display import SVG\nfrom keras.utils import np_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.preprocessing.text import Tokenizer\n# from keras.utils.visualize_util import model_to_dot, plot\n# from gensim.models.doc2vec import Word2Vec\n\nimport gensim\n","739560b7":"path = '..\/input\/alicedataset\/alice.txt'\ncorpus = open(path).readlines()\ncorpus = [sentence for sentence in corpus if sentence.count(' ') >= 2]\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\nV = len(tokenizer.word_index) + 1\nV","56eb8116":"dim_embedddings = 128\n\n# inputs\nw_inputs = Input(shape=(1, ), dtype='int32')\nw = Embedding(V, dim_embedddings)(w_inputs)\n\n# context\nc_inputs = Input(shape=(1, ), dtype='int32')\nc  = Embedding(V, dim_embedddings)(c_inputs)\no = Dot(axes=2)([w, c])\no = Reshape((1,), input_shape=(1, 1))(o)\no = Activation('sigmoid')(o)\n\nSkipGram = Model(inputs=[w_inputs, c_inputs], outputs=o)\nSkipGram.summary()\nSkipGram.compile(loss='binary_crossentropy', optimizer='adam')","adae3e6b":"for _ in range(5):\n    loss = 0.\n    for i, doc in enumerate(tokenizer.texts_to_sequences(corpus)):\n        data, labels = skipgrams(sequence=doc, vocabulary_size=V, window_size=5, negative_samples=5.)\n        x = [np.array(x) for x in zip(*data)]\n        y = np.array(labels, dtype=np.int32)\n        if x:\n            loss += SkipGram.train_on_batch(x, y)\n\n    print(loss)","c7d83e7a":"d={}\n# f = open('vectors.txt' ,'w')\n# f.write('{} {}\\n'.format(V-1, dim_embedddings))\nvectors = SkipGram.get_weights()[0]\nfor word, i in tokenizer.word_index.items():\n    d[word]=list(vectors[i, :])\n# f.close()","ae025a2f":"def avg_sentence_vector(words, model, num_features):\n    featureVec = np.zeros((num_features,), dtype=\"float32\")\n    nwords = 0\n    for word in words:\n        if word in model.keys():\n            featureVec = np.add(featureVec, model[word])\n\n    return featureVec","de976514":"sentence1 = ['So','she','was','considering','in' , 'her' , 'own' , 'mind']\nsentence2 = ['So','she','was','considering','the'  , 'truth']\n\n","51988118":"feature1 = avg_sentence_vector(sentence1,d,128)\nfeature2 = avg_sentence_vector(sentence2,d,128)\nprint(sentence1)","68209a81":"from scipy.spatial.distance import cosine\ncosine(feature1,feature2)","3a94491a":"feature1","c3fe6dfc":"sentence3 = ['remarkable' , 'in' , 'that']\nsentence4 = ['remarkable' , 'in' , 'the' , 'game']","d8eccbbb":"feature3 = avg_sentence_vector(sentence3,d,128)\nfeature4 = avg_sentence_vector(sentence4,d,128)\n","0c2c9c39":"print(cosine(feature3,feature4))","c14dee5d":"def tokenize(text):\n    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n    return pattern.findall(text.lower())","5bc394b8":"tokens = tokenize(\"Hello what are you doing\")\nprint(tokens)","e81d2895":"id_to_word = {i:x for (i, x) in enumerate(tokens)}\nword_to_id = {x:i for (i, x) in enumerate(tokens)}","78a74383":"print(word_to_id)\nprint(id_to_word)","49f375a3":"def generate_training_data(tokens, word_to_id, window_size):\n    X, Y = [], []\n\n    for i in range(len(tokens)):\n        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n                   list(range(i + 1, min(len(tokens), i + window_size + 1)))\n        for j in nbr_inds:\n            X.append(word_to_id[tokens[i]])\n            Y.append(word_to_id[tokens[j]])\n            \n    return np.array(X), np.array(Y)","d5370b66":"def expand_dims(x, y):\n    x = np.expand_dims(x, axis=0)\n    y = np.expand_dims(y, axis=0)\n    return x, y","7e2dd52a":"x, y = generate_training_data(tokens, word_to_id, 3)\nx, y = expand_dims(x, y)","10ad069f":"# generated training data\nx, y","a6301efd":"def init_parameters(vocab_size, emb_size):\n    wrd_emb = np.random.randn(vocab_size, emb_size) * 0.01\n    w = np.random.randn(vocab_size, emb_size) * 0.01\n    \n    return wrd_emb, w","40880e5c":"def softmax(z):\n    return np.divide(np.exp(z), np.sum(np.exp(z), axis=0, keepdims=True) + 0.001)","634436f3":"def forward(inds, params):\n    wrd_emb, w = params\n    word_vec = wrd_emb[inds.flatten(), :].T\n    z = np.dot(w, word_vec)\n    out = softmax(z)\n    \n    cache = inds, word_vec, w, z\n    \n    return out, cache\n","cda8e591":"def cross_entropy(y, y_hat):\n    m = y.shape[1]\n    cost = -(1 \/ m) * np.sum(np.sum(y_hat * np.log(y + 0.001), axis=0, keepdims=True), axis=1)\n    return cost","7661d0f0":"def dsoftmax(y, out):\n    dl_dz = out - y\n    \n    return dl_dz","e038cef5":"def backward(y, out, cache):\n    inds, word_vec, w, z = cache\n    wrd_emb, w = params\n    \n    dl_dz = dsoftmax(y, out)\n    # deviding by the word_vec length to find the average\n    dl_dw = (1\/word_vec.shape[1]) * np.dot(dl_dz, word_vec.T)\n    dl_dword_vec = np.dot(w.T, dl_dz)\n    \n    return dl_dz, dl_dw, dl_dword_vec\n","20ecdc57":"def update(params, cache, grads, lr=0.03):\n    inds, word_vec, w, z = cache\n    wrd_emb, w = params\n    dl_dz, dl_dw, dl_dword_vec = grads\n    \n    wrd_emb[inds.flatten(), :] -= dl_dword_vec.T * lr\n    w -= dl_dw * lr\n    \n    return wrd_emb, w","0fbb238d":" y = y.astype(int)","02a45b7e":"y.shape","af1fc4e4":"vocab_size = len(id_to_word)\n\nm = y.shape[1]\ny_one_hot = np.zeros((vocab_size, m))\ny_one_hot[y.flatten(), np.arange(m)] = 1\n\ny = y_one_hot","2fa9ccee":"batch_size=256\nembed_size = 50\n\nparams = init_parameters(vocab_size, 50)\n\ncosts = []\n\nfor epoch in range(10000):\n    epoch_cost = 0\n    \n    batch_inds = list(range(0, x.shape[1], batch_size))\n    np.random.shuffle(batch_inds)\n    \n    for i in batch_inds:\n        x_batch = x[:, i:i+batch_size]\n        y_batch = y[:, i:i+batch_size]\n        \n        pred, cache = forward(x_batch, params)\n        grads = backward(y_batch, pred, cache)\n        params = update(params, cache, grads, 0.03)\n        cost = cross_entropy(pred, y_batch)\n        \n        epoch_cost += np.squeeze(cost)\n        \n    costs.append(epoch_cost)\n    \n    if(epoch % 250 == 0):\n        print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n","d99fb7ae":"x_test = np.arange(vocab_size)\nx_test = np.expand_dims(x_test, axis=0)\nsoftmax_test, _ = forward(x_test, params)\ntop_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]\n","61c77b84":"for input_ind in range(vocab_size):\n    input_word = id_to_word[input_ind]\n    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n    print(\"{}'s skip-grams: {}\".format(input_word, output_words))\n ","1d96c062":"print(output_words)\n\ninput_word ","0a962a7e":"## Training","64e262f1":"## TEST","6fc16be6":"###### Built IN","fb9ebbbd":"## Back Propgration","10c37cfe":"## Genrating traning Data","cdceb5a5":"###### Self made \n","0b9bbd6b":"## Cost Function","520c895c":"## Genrate data"}}