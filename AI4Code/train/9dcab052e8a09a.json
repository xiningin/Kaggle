{"cell_type":{"36f6acf3":"code","1050bc98":"code","d9ff75f8":"code","86dbe89c":"code","df69935a":"code","43e58f2d":"code","9cebdcf2":"code","bbcddd03":"code","0c4de253":"code","34b166fb":"code","dc98a612":"code","00d81ae6":"code","71ddbee8":"code","c3040ea8":"code","c69f047e":"code","b2fadf21":"code","65ff02a9":"code","8deb452b":"code","3e097e5f":"code","bbeb7f50":"code","97e3bf50":"code","4acf4f9c":"code","feff0132":"code","8647a137":"code","f78c9a9a":"code","9c56ae74":"code","39708bf7":"code","8f49dab8":"code","3e7c1048":"code","55554c27":"code","08cf4dd4":"code","7ca14414":"code","a515b1f2":"code","3b324a98":"code","26e0be0d":"code","cc92e78b":"code","a2ed86c8":"code","6bf44c6d":"code","ed9bf6bb":"code","e1b133a0":"code","9f2fb3d1":"code","a7c967b0":"code","7253fca6":"code","b8d969a9":"code","5fa7a0fa":"code","955d91a1":"code","74a3cb7f":"code","509dfc46":"code","3e5a856a":"code","201df710":"code","225a94a7":"code","727be757":"code","b51b1958":"code","134a203f":"code","a44258d5":"code","3ff4ebdb":"code","b70d219c":"code","e0c27a22":"code","b6eea39c":"code","4b715d89":"code","aa60e36a":"code","4f2bb74d":"code","6aa82148":"code","e87305ba":"code","c5d46b1a":"code","43a04ebd":"code","ae43d561":"code","92be3ef9":"code","1c0fd33f":"code","59bb2355":"code","6378d046":"code","bd1eac35":"code","863535af":"code","0ef2cc0c":"code","8124c024":"code","281b5777":"code","ac36ca4f":"code","06c15e59":"code","fc99fb98":"code","e69dfea3":"code","b204097b":"code","06a11724":"code","30f36b68":"code","37a9d3b3":"code","c654bc20":"code","5f971362":"code","87023daf":"code","beafc024":"code","3be5d50c":"code","b9cd0c89":"code","23d9cd08":"code","9ff11e0e":"code","558c3724":"code","e42e3151":"code","f8535502":"code","c3ec357a":"code","86802586":"code","8bc5dd3f":"code","8d89d1be":"code","7ce12e3e":"code","68d3dda8":"code","005ca4b7":"code","afc1b0fc":"code","19441304":"code","281ed8b3":"code","dd42cec2":"code","d8002f15":"code","9494bd1e":"code","3e1b2618":"code","62c540bd":"code","e17c8dec":"code","f9eac4d2":"code","905c9949":"code","8fff6296":"code","996bbd4b":"code","a19c997e":"code","cb43dbd8":"code","c150bc31":"code","978c31bb":"code","c5727af1":"code","34ab0b2c":"code","489249c9":"code","565fc367":"markdown","286d7296":"markdown","70fb1ec6":"markdown","b8403e99":"markdown","2bf4f94f":"markdown","6449ddce":"markdown","63b95af1":"markdown","45a768e7":"markdown","11a35603":"markdown","3420ad18":"markdown","49cef07e":"markdown","af27827f":"markdown","86f551b9":"markdown","dc30dfa4":"markdown","d00e8e67":"markdown","ef6a3bf1":"markdown","e39c943a":"markdown","64ef1462":"markdown","d685fe57":"markdown","47f0690c":"markdown","2f3a1ae2":"markdown","6e0b5851":"markdown","2133fc8a":"markdown","12b893bc":"markdown","e92ba7f0":"markdown","041ac594":"markdown","4335b4b5":"markdown","f1ea5a2a":"markdown","949cc5ce":"markdown","998af896":"markdown","0cb7d607":"markdown","c0d891c9":"markdown","b495499c":"markdown","f316a338":"markdown","69d2419f":"markdown","5bcd61ad":"markdown","febec17f":"markdown","4a7ffa30":"markdown","0f87957c":"markdown","9ddef984":"markdown","2e7cd7d3":"markdown","02918697":"markdown","cfb4c9db":"markdown","9ed774bf":"markdown","0b7ff82a":"markdown","abf8f31e":"markdown","508d8869":"markdown","49de00d0":"markdown","3ee48d01":"markdown","d695c1ca":"markdown","24bec89e":"markdown","9d2bd61a":"markdown","237e6516":"markdown","b1f127fe":"markdown","a8b96913":"markdown","c42828d0":"markdown","031b67a9":"markdown","e5e153c3":"markdown","5694cc57":"markdown"},"source":{"36f6acf3":"!pip install scikit-optimize --quiet","1050bc98":"import os\nfrom datetime import datetime\nimport math\nimport warnings\nfrom zipfile import ZipFile\n\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nimport pickle","d9ff75f8":"# Folder com as bases dispon\u00edveis\nos.listdir('..\/input\/walmart-recruiting-store-sales-forecasting')","86dbe89c":"# Descompactando os arquivos\n\nwith ZipFile('..\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip') as f:\n    f.extractall(path='walmart-recruiting-store-sales-forecasting')\n\nwith ZipFile('..\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip') as f:\n    f.extractall(path='walmart-recruiting-store-sales-forecasting')\n    \nwith ZipFile('..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip') as f:\n    f.extractall(path='walmart-recruiting-store-sales-forecasting')\n\nwith ZipFile('..\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip') as f:\n    f.extractall(path='walmart-recruiting-store-sales-forecasting')\n\nos.listdir('walmart-recruiting-store-sales-forecasting')","df69935a":"# Carregando as bases em dataframes panda\nfeatures = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/features.csv\")\nstores = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv\")\ntrain = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/train.csv\")\ntest = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/test.csv\")\nsubmission = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv\")","43e58f2d":"# drive.mount('\/content\/gdrive')","9cebdcf2":"# Definindo caminho para os arquivos:\n# 'gdrive\/My Drive\/wallmart_sales_forecasting\/'\n\n# features = pd.read_csv('gdrive\/My Drive\/wallmart_sales_forecasting\/features.csv')\n# stores = pd.read_csv('gdrive\/My Drive\/wallmart_sales_forecasting\/stores.csv')\n# train = pd.read_csv('gdrive\/My Drive\/wallmart_sales_forecasting\/train.csv')\n# test = pd.read_csv('gdrive\/My Drive\/wallmart_sales_forecasting\/test.csv')\n# submission = pd.read_csv('gdrive\/My Drive\/wallmart_sales_forecasting\/submission.csv')","bbcddd03":"print(\"features.shape\", features.shape)\nprint(\"stores.shape\", stores.shape)\nprint(\"train.shape\", train.shape)\nprint(\"test.shape\", test.shape)\nprint(\"submission.shape\", submission.shape)","0c4de253":"stores.head()","34b166fb":"stores.Store.unique()","dc98a612":"stores.Type.unique()","00d81ae6":"train.head()","71ddbee8":"test.head()","c3040ea8":"submission.head()","c69f047e":"features.head()","b2fadf21":"df_data = train.merge(features\n                       ,on = ['Store','Date','IsHoliday']\n                       ,how = 'inner').merge(stores\n                                            ,on = ['Store']\n                                            ,how = 'inner')","65ff02a9":"df_data.head()","8deb452b":"plt.figure()\nplt.title ('Volume de vendas semanais em fun\u00e7\u00e3o dos feriados')\nfig = sns.boxplot(x = 'IsHoliday'\n                  ,y = 'Weekly_Sales'\n                  ,data = df_data[['Weekly_Sales','IsHoliday']]\n                  ,showfliers = False)","3e097e5f":"# Fun\u00e7\u00e3o para expandir o campo data em intervalos que podem ser relevantes no futuro\n\ndef split_date(df,date):\n\n    '''\n    Transforma o campo indicado na entrada em data e extrai valores que podem ser relevantes, como ano, m\u00eas e dia. \n    A saida da fun\u00e7\u00e3o concatena esses novos campos no dataframe indicado na entrada\n    '''\n    \n    df['dt_ref'] = pd.to_datetime(df[date])\n    df['year'] = df.dt_ref.dt.year\n    df['month'] = df.dt_ref.dt.month\n    df['day'] = df.dt_ref.dt.day\n    df['week_of_year'] = df.dt_ref.dt.isocalendar().week\n    df['period_month'] = df_data.dt_ref.dt.to_period('M')","bbeb7f50":"split_date(df_data, 'Date')\n\n# df_data[['month','Weekly_Sales','IsHoliday']].head()\n\nplt.figure(figsize = (20,6))\nplt.title ('Varia\u00e7\u00e3o do volume de vendas em rela\u00e7\u00e3o aos meses do ano')\nfig = sns.boxplot(x = 'month'\n                  ,y = 'Weekly_Sales'\n                  ,data = df_data[['month','Weekly_Sales','IsHoliday']]\n                  ,showfliers = False\n                  ,hue = 'IsHoliday')","97e3bf50":"plt.figure(figsize = (20,6))\nplt.title ('Varia\u00e7\u00e3o do volume de vendas em rela\u00e7\u00e3o \u00e0s lojas e impacto dos feriados')\nfig = sns.boxplot(x = 'Store'\n                  ,y = 'Weekly_Sales'\n                  ,data = df_data[['Store','Weekly_Sales','IsHoliday']]\n                  ,showfliers = False\n                  ,hue = 'IsHoliday')","4acf4f9c":"plt.figure(figsize = (20,8))\nplt.title ('Varia\u00e7\u00e3o do volume de vendas em rela\u00e7\u00e3o aos departamentos e impacto dos feriados', fontsize=16)\nfig = sns.boxplot(x = 'Dept'\n                  ,y = 'Weekly_Sales'\n                  ,data = df_data[['Dept','Weekly_Sales','IsHoliday']]\n                  ,showfliers = False\n                  ,hue = 'IsHoliday')","feff0132":"weekly_sales_2010 = df_data[df_data.year==2010].groupby('week_of_year')['Weekly_Sales'].mean()\nweekly_sales_2011 = df_data[df_data.year==2011].groupby('week_of_year')['Weekly_Sales'].mean()\nweekly_sales_2012 = df_data[df_data.year==2012].groupby('week_of_year')['Weekly_Sales'].mean()\n\nplt.figure(figsize=(20,6))\nplt.plot(weekly_sales_2010.index, weekly_sales_2010.values)\nplt.plot(weekly_sales_2011.index, weekly_sales_2011.values)\nplt.plot(weekly_sales_2012.index, weekly_sales_2012.values)\n\nplt.xticks(np.arange(1, 53, step=1), fontsize=16)\nplt.yticks( fontsize=16)\nplt.xlabel('Week of Year', fontsize=16, labelpad=20)\nplt.ylabel('Sales', fontsize=20, labelpad=20)\n\nplt.title(\"Volume de vendas por semana, por ano\", fontsize=16)\nplt.legend(['2010', '2011', '2012'], fontsize=14);","8647a137":"df_data.isna().sum()","f78c9a9a":"df_data.tail()","9c56ae74":"df_data.columns","39708bf7":"# Vari\u00e1veis para o modelo\ninput_cols = ['Store', 'Dept', 'Weekly_Sales', 'IsHoliday', 'Temperature',\n       'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4',\n       'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'year',\n       'month', 'day', 'week_of_year']\n\n# Vari\u00e1veis num\u00e9ricas\nvar_num = ['Store', 'Dept', 'Size','Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n       'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'year', 'month', 'day',\n       'week_of_year']\n\n# Vari\u00e1veis categ\u00f3ricas\nvar_cat = ['IsHoliday', 'Type']\n\n# Vari\u00e1vel alvo\ntarget_col = 'Weekly_Sales'","8f49dab8":"def Metadados(dataframe, \n              target_var, \n              id_vars = [], \n              vars_quanti = [], \n              vars_quali = [], \n              vars_bin = []): \n    '''\n    Cria um novo df com pricipais caracteristicas do df inicial,\n    como tipo de variaveis, quantidade de valores diferentes, \n    quantidade de missings, etc.\n    \n    Usa como entrada o df original, a variavel alvo, as vars utilizadas como ID, \n    a lista de variaveis quantitativas, ou continuas e as variaveis qualitativas, \n    binarias ou nao\n    '''\n\n    \n    '''\n    \n    Entradas: DF original\n              var resposta\n              vars ID\n              lista de vars quanti\n              lista de vars quali\n              lista de vars bin\u00e1rias\n              \n    Saida:    DF com proprieades do DF original\n    \n    '''\n    \n    train = dataframe\n\n    missing = []\n    data = []\n    dtype = []\n    card = []\n    lista_mod = []\n    \n    for f in train.columns:\n\n        missing=train[f].isnull().sum()\n        \n        # Definindo o papel das vari\u00e1veis:\n        if f == target_var: #Vari\u00e1vel Resposta\n            role = 'target'\n        elif f in id_vars:\n            role = 'id'          \n        else:\n            role = 'input'\n\n        # Definindo o tipo das vari\u00e1veis da tabela de entrada\n        dtype = train[f].dtype\n        \n        # Quantidade de dom\u00ednios distintos para cada vari\u00e1vel do tipo ordinal e nominal\n        card = train[f].value_counts().shape[0]\n        \n        # Definindo lista que a var se encaixa('quanti' ou 'quali')\n        if f in list(vars_quanti):\n            lista_mod = 'quanti'\n        elif f in list(vars_quali):\n            lista_mod= 'quali'\n        else: \n            lista_mod = 'outra'\n\n        # Criando a lista com todo metadados\n        f_dict = {'Variaveis': f\n                  ,'Role': role\n                  ,'Type': dtype\n                  ,'Missing': missing\n                  ,'Cardinalidade': card\n                  ,'Lista_mod': lista_mod}\n        data.append(f_dict)\n\n    meta = pd.DataFrame(data, columns=['Variaveis'\n                                       ,'Role'\n                                       ,'Type'\n                                       ,'Missing'\n                                       ,'Cardinalidade'\n                                       ,'Lista_mod'])\n\n\n    return meta ","3e7c1048":"Meta_raw = Metadados(df_data, target_col, vars_quanti = var_num, vars_quali = var_cat)","55554c27":"Meta_raw","08cf4dd4":"df_data['MarkDown1'].fillna(0, inplace=True)\ndf_data['MarkDown2'].fillna(0, inplace=True)\ndf_data['MarkDown3'].fillna(0, inplace=True)\ndf_data['MarkDown4'].fillna(0, inplace=True)\ndf_data['MarkDown5'].fillna(0, inplace=True)","7ca14414":"df_var_num = df_data[['Weekly_Sales']]\n\ndf_bivar = pd.DataFrame()\naux_concent_num = pd.DataFrame()\naux_drop_num = []\n\nfor i in var_num:\n    \n#         print (aux_var)\n    aux_var = 'cat_' + i\n    df_var_num[aux_var] = pd.qcut(df_data[i], 10,labels=False, duplicates = 'drop')\n    \n    if df_data[i].nunique() < 2:\n        aux_drop_num.append(i)\n\n    else:\n        max_concent = df_var_num[aux_var].value_counts(normalize = True).max()\n        aux_df = pd.DataFrame({'vars':[i]\n                               ,'concentracao':max_concent})\n        aux_concent_num = pd.concat([aux_concent_num,aux_df], axis = 0)\n    \n        aux_bivar = pd.DataFrame()\n        aux_bivar['Weekly_Sales'] = df_var_num[[aux_var,'Weekly_Sales']].groupby([aux_var]).mean().Weekly_Sales    \n        aux_bivar['pop'] = df_var_num[[aux_var]].groupby(df_var_num[aux_var]).count()\n        aux_bivar.loc[:,'taxa_evento'] = (aux_bivar.loc[:,'Weekly_Sales'] \/ aux_bivar.loc[:,'pop'])*100\n        aux_bivar['taxa_med'] = (aux_bivar.Weekly_Sales.sum()\/aux_bivar['pop'].sum())*100\n        aux_bivar['var'] = i\n        aux_bivar['var_cat']= aux_var\n        aux_bivar['var_tipo'] = 'quanti'\n        aux_bivar.reset_index(drop = False, inplace = True)\n\n        aux_bivar.columns = ['cat'\n                            ,'evento'\n                            ,'pop'\n                            ,'taxa_evento'\n                            ,'taxa_med' \n                            ,'var'\n                            ,'var_cat'\n                            ,'var_tipo']\n\n        #print()\n        #print(aux_bivar)\n        #print()\n\n\n        df_bivar = pd.concat([df_bivar,aux_bivar], axis = 0)\n\n        # Cria a janela\n        fig, ax1 = plt.subplots()\n\n        # Taxa de evento por categorias\n        color = 'tab:red'\n        plt.title(aux_var)\n        ax1.set_xlabel('faixas')\n        ax1.set_ylabel('Weekly Sales', color=color)\n        ax1.scatter(aux_bivar['cat'], aux_bivar['taxa_evento'], color=color, label = 'taxa evento')\n        ax1.tick_params(axis='y', labelcolor=color)\n\n\n\n        #ylim\n        if aux_bivar['taxa_evento'].min() < 10:\n            bottom = 0\n        else:\n            bottom = round(aux_bivar['taxa_evento'].min() - 10)\n\n        if aux_bivar['taxa_evento'].max() > 90:\n            up = 100\n        else:\n            up = round(aux_bivar['taxa_evento'].max() + 10)\n\n        plt.ylim(bottom,up)\n\n\n        for x,y in zip(aux_bivar['cat'], aux_bivar['taxa_evento']):\n\n            label = \"{:.2f}\".format(y)\n\n            # aqui incluimos o rotulo para cada ponto dos graficos\n            plt.annotate(label, # texto\n                         (x,y), # ponto\n                         textcoords=\"offset points\", # posicao do rotulo\n                         xytext=(0,10), # distancia do texto ao ponto (x,y)\n                         ha='center') # 'horizontal alignment' pode ser left, right or center\n\n        # Taxa de evento m\u00e9dia\n        color = 'tab:green'\n        ax1.plot(aux_bivar['cat'], aux_bivar['taxa_med'], color=color, linestyle='dashed', label='taxa media de evento')\n\n        # Publico em cada categoria\n        ax2 = ax1.twinx()  # segundo eixo instanciado, com o primeiro \n        color = 'tab:grey'\n        ax2.set_ylabel('pop', color=color)  \n        ax2.bar(aux_bivar['cat'], aux_bivar['pop'], color=color, alpha = 0.2, label = 'pulico por categoria')\n        ax2.tick_params(axis='y', labelcolor=color)\n\n        #fig.tight_layout()  # otherwise the right y-label is slightly clipped\n        plt.show()            \n            \naux_concent_num = aux_concent_num.reset_index(drop=True)","a515b1f2":"aux_drop_num","3b324a98":"aux_concent_num","26e0be0d":"plt.figure(figsize=(16, 16))\n\ncorr = df_data[var_num].corr(method = 'pearson')\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nsns.heatmap(corr, mask = mask,annot = True)","cc92e78b":"var_cat","a2ed86c8":"df_var_cat = df_data[['Weekly_Sales']]\n\naux_concent_cat = pd.DataFrame()\naux_drop_cat = []\n\nfor i in var_cat:\n    \n    aux_var = 'cat_' + i\n    df_var_cat[aux_var] = df_data[i]\n    if df_var_cat[aux_var].nunique() < 2:\n        aux_drop_cat.append(i)\n\n    else:\n        max_concent = df_var_cat[aux_var].value_counts(normalize = True).max()\n        aux_df = pd.DataFrame({'vars':[i]\n                               ,'concentracao':max_concent})\n        aux_concent_cat = pd.concat([aux_concent_cat,aux_df], axis = 0)    \n    \n        \n        aux_bivar = pd.DataFrame()\n    #print (aux_var)\n    \n    \n\n        aux_bivar['Weekly_Sales'] = df_var_cat[[aux_var,'Weekly_Sales']].groupby([aux_var]).mean().Weekly_Sales\n        aux_bivar['pop'] = df_var_cat[[aux_var]].groupby(df_var_cat[aux_var]).count()\n        aux_bivar.loc[:,'taxa_evento'] = (aux_bivar.loc[:,'Weekly_Sales'] \/ aux_bivar.loc[:,'pop'])*100\n        aux_bivar['taxa_med'] = (aux_bivar.Weekly_Sales.sum()\/aux_bivar['pop'].sum())*100\n        aux_bivar['var'] = i\n        aux_bivar['var_cat']= aux_var\n        aux_bivar['var_tipo'] = 'quali'\n        aux_bivar.reset_index(drop = False, inplace = True)\n\n        aux_bivar.columns = ['cat'\n                            ,'evento'\n                            ,'pop'\n                            ,'taxa_evento'\n                            ,'taxa_med' \n                            ,'var'\n                            ,'var_cat'\n                            ,'var_tipo']\n\n        #print()\n        #print(aux_bivar)\n        #print()\n\n\n        df_bivar = pd.concat([df_bivar,aux_bivar], axis = 0)\n\n\n        # Cria a janela\n        fig, ax1 = plt.subplots()\n\n        # Taxa de evento por categorias\n        color = 'tab:red'\n        plt.title(aux_var)\n        ax1.set_xlabel('categorias')\n        ax1.set_ylabel('Weekly Sales', color=color)\n        ax1.scatter(aux_bivar['cat'], aux_bivar['taxa_evento'], color=color, label = 'taxa evento')\n        ax1.tick_params(axis='y', labelcolor=color)\n\n        #ylim\n        if aux_bivar['taxa_evento'].min() < 10:\n            bottom = 0\n        else:\n            bottom = round(aux_bivar['taxa_evento'].min() - 10)\n\n        if aux_bivar['taxa_evento'].max() > 90:\n            up = 100\n        else:\n            up = round(aux_bivar['taxa_evento'].max() + 10)\n\n        plt.ylim(bottom,up)\n\n        for x,y in zip(aux_bivar['cat'], aux_bivar['taxa_evento']):\n\n            label = \"{:.2f}\".format(y)\n\n            # aqui incluimos o rotulo para cada ponto dos graficos\n            plt.annotate(label, # texto\n                         (x,y), # ponto\n                         textcoords=\"offset points\", # posicao do rotulo\n                         xytext=(0,10), # distancia do texto ao ponto (x,y)\n                         ha='center') # 'horizontal alignment' pode ser left, right or center\n\n        # Taxa de evento m\u00e9dia\n        color = 'tab:green'\n        ax1.plot(aux_bivar['cat'], aux_bivar['taxa_med'], color=color, linestyle='dashed', label='taxa media de evento')\n\n        # Publico em cada categoria\n        ax2 = ax1.twinx()  # segundo eixo instanciado, com o primeiro \n        color = 'tab:grey'\n        ax2.set_ylabel('pop', color=color)  \n        ax2.bar(aux_bivar['cat'], aux_bivar['pop'], color=color, alpha = 0.2, label = 'pulico por categoria')\n        ax2.tick_params(axis='y', labelcolor=color)\n\n        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n        plt.show()\n        \naux_concent_cat = aux_concent_cat.reset_index(drop=True)","6bf44c6d":"aux_drop_cat","ed9bf6bb":"aux_concent_cat","e1b133a0":"df_data[input_cols].head()","9f2fb3d1":"mean_list = ['Temperature','Fuel_Price','CPI','Unemployment']","a7c967b0":"def variaveis_relativas(df, lista_vars, var_group):\n    \n    '''\n    O objetivo desta fun\u00e7\u00e3o \u00e9 criar 'valores relativos', que indicam o quanto o valor mais atual de distancia do valor m\u00e9dio de um per\u00edodo de interesse.\n    Aqui foi testado o valor da vari\u00e1vel em rela\u00e7\u00e3o \u00e0 m\u00e9dia mensal, que \u00e9 indicada pela vari\u00e1vel 'var_group'.\n    Entradas: DataFrame com todas as informa\u00e7\u00f5es necess\u00e1rias para calcular os valores consolidados, lista dos campos que ser\u00e3o transformados e a vari\u00e1vel sobre a qual os valores ser\u00e3o agrupados\n    Sa\u00eddas: A fun\u00e7\u00e3o devolve o mesmo DataFrame, com as novas vari\u00e1veis concatenadas \n    '''\n    \n    # Calculando valores m\u00e9dios para servir de referencial \n    aux_mean = pd.DataFrame()\n    aux_names = [var_group]\n    \n    for var in lista_vars:\n        #print(var)\n        aux_mean[var] = df[[var_group,var]].groupby([var_group]).mean()\n        aux_name_2 = 'mean_' + var\n        aux_names.append(aux_name_2)\n\n    aux_mean.reset_index(inplace = True)\n\n    # Ajustando o nome dos campos\n    aux_mean.columns = aux_names\n\n    df = df.merge(aux_mean\n                  ,on = [var_group]\n                  ,how = 'inner')\n    \n    #Incluindo as vari\u00e1veis no dataframe\n    for var in lista_vars:\n        var_name = 'relative_' + var\n        mean_var = 'mean_' + var\n        df[var_name] = (df[var] + 0.0001)\/df[mean_var]\n        \n    return df","7253fca6":"df_train = variaveis_relativas(df_data, mean_list, 'month')","b8d969a9":"df_train.head()","5fa7a0fa":"df_train.columns","955d91a1":"# Incluindo as novas vari\u00e1veis na lista\nvar_num_2 = var_num + ['relative_Temperature'\n                       ,'relative_Fuel_Price'\n                       ,'relative_CPI'\n                       ,'relative_Unemployment']","74a3cb7f":"plt.figure(figsize=(16, 16))\n\ncorr = df_train[var_num_2].corr(method = 'pearson')\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nsns.heatmap(corr, mask = mask,annot = True)","509dfc46":"# Binarizando as vari\u00e1veis categ\u00f3ricas para evitar lidar com campos de texto\ndf_train = pd.get_dummies(df_train, columns = var_cat)","3e5a856a":"df_train.head()","201df710":"df_train.period_month.unique()","225a94a7":"# Selecionando m\u00eas mais recente na base para valida\u00e7\u00e3o\ndf_val = df_train[df_train.period_month == '2012-10'].copy()\n\ndf_train = df_train[df_train.period_month != '2012-10']","727be757":"# Inclus\u00e3o das vari\u00e1veis categ\u00f3ricas, j\u00e1 binarizadas, na lista geral de vari\u00e1veis.\nvars_model = var_num_2 + ['IsHoliday_True', 'Type_A', 'Type_B', 'Type_C']\ndf_train[vars_model].head()","b51b1958":"# Dividindo a base de treino em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(df_train.loc[:,df_train.columns != 'Weekly_Sales'], # Vari\u00e1veis Explicativas\n                                                    df_train.Weekly_Sales,  # Vari\u00e1vel Resposta\n                                                    test_size = 0.3, # Propor\u00e7\u00e3o entre treino e teste\n                                                    random_state = 0)","134a203f":"# Salvando as informa\u00e7\u00e7\u00f5es originais para an\u00e1lises futuras\nviz_result_test = X_test[['Store', 'Dept', 'Date', 'year', 'week_of_year']].copy()\nviz_result_valida = df_val[['Store', 'Dept', 'Date', 'year','week_of_year']].copy()","a44258d5":"viz_result_test.head()","3ff4ebdb":"scaler = StandardScaler().fit(X_train[var_num_2])\nX_train[var_num_2] = scaler.transform(X_train[var_num_2])\nX_test[var_num_2] = scaler.transform(X_test[var_num_2])\ndf_val[var_num_2] = scaler.transform(df_val[var_num_2])","b70d219c":"scaler_Pkl = pickle.dumps(scaler)","e0c27a22":"X_train[var_num_2].head()","b6eea39c":"def ml_error(model_name, y, yhat):\n    '''\n    Esta fun\u00e7\u00e3o auxilia na avalia\u00e7\u00e3o dos modelos utilizando as m\u00e9tricas escolhidas\n    Entradas: Nome dado ao modelo, valores reais e valores preditos \n    Sa\u00eddas: Dataframe com nome do modelo, erro m\u00e9dio absoluto e erro m\u00e9dio quadrado\n    '''\n    mae = mean_absolute_error(y, yhat)\n    rmse = np.sqrt(mean_squared_error(y, yhat))\n    \n    return pd.DataFrame({'Model Name' : model_name,\n                        'MAE' : mae,\n                        'RMSE' : rmse}, index = [0])","4b715d89":"# DataFrame para guardar a informa\u00e7\u00e3o sobre o desempenho dos modelos\naux_compara = pd.DataFrame()","aa60e36a":"LR = LinearRegression()\n\nLR.fit(X_train[vars_model],y_train)\n","4f2bb74d":"y_pred = LR.predict(X_test[vars_model])\ny_val = LR.predict(df_val[vars_model])\n\n#performance oos\nlr_result_oos = ml_error('Linear Regression OOS', y_test, y_pred)\nlr_result_oos","6aa82148":"# performance oot\nlr_result_oot = ml_error('Linear Regression OOT', df_val.Weekly_Sales, y_val)\nlr_result_oot","e87305ba":"aux_compara = pd.concat([aux_compara, lr_result_oos],axis = 0)\naux_compara = pd.concat([aux_compara, lr_result_oot],axis = 0)\n\n\naux_compara","c5d46b1a":"from sklearn.model_selection import GridSearchCV\n\nparameters = {'alpha':[0.01, 0.1, 0.5, 2, 5]}\n\n# model\nlrr = Lasso()\nlrr_clf = GridSearchCV(lrr, parameters)\nlrr_clf.fit(X_train[vars_model], y_train)","43a04ebd":"lrr_clf.best_params_","ae43d561":"# prediction\ny_pred_lrr = lrr_clf.predict(X_test[vars_model])\ny_val_lrr = lrr_clf.predict(df_val[vars_model])\n\n#performance\nlrr_result_oos = ml_error('Linear Regression - Lasso OOS', y_test, y_pred_lrr)\nlrr_result_oos","92be3ef9":"lrr_result_oot = ml_error('Linear Regression - Lasso OOT', df_val.Weekly_Sales, y_val_lrr)\nlrr_result_oot","1c0fd33f":"aux_compara = pd.concat([aux_compara, lrr_result_oos],axis = 0)\naux_compara = pd.concat([aux_compara, lrr_result_oot],axis = 0)\n\n\naux_compara","59bb2355":"# parameters = {'n_estimators': [100, 250, 500]\n              # ,'max_depth': [None, 3, 4, 5]\n              # ,'max_features': [0.25, 0.5, 0.75]}\n\n# model\n# rf = RandomForestRegressor()\n# rf_reg = GridSearchCV(rf, parameters)\n# rf_reg.fit(X_train[vars_model], y_train)\n\n# rf_reg.best_params_","6378d046":"# model\nrf = RandomForestRegressor(n_estimators = 100\n                           ,max_depth = None\n                           ,max_features = 0.75)\n\nrf.fit(X_train[vars_model], y_train)","bd1eac35":"# prediction\ny_pred_rf = rf.predict(X_test[vars_model])\ny_val_rf = rf.predict(df_val[vars_model])\n\n# Performance\nrf_result_oos = ml_error('Random Forest Regressor OOS', y_test, y_pred_rf)\nrf_result_oos","863535af":"rf_result_oot = ml_error('Random Forest Regressor OOT', df_val.Weekly_Sales, y_val_rf)\nrf_result_oot","0ef2cc0c":"aux_compara = pd.concat([aux_compara, rf_result_oos],axis = 0)\naux_compara = pd.concat([aux_compara, rf_result_oot],axis = 0)\n\n\naux_compara","8124c024":"# from skopt.space import Real, Integer\n# from skopt.utils import use_named_args\n\n# n_features = X_train[vars_model].shape[1]\n\n# XGB_opt = GradientBoostingClassifier(n_estimators=100, random_state=0, criterion = 'mse')\n\n# space  = [Integer(5,15, name='max_depth')\n#          ,Real(10**-5, 0.1, \"log-uniform\", name='learning_rate')\n#          ,Real(0.25, 0.75,'log-unifom', name = 'subsample')\n#          ,Integer(2, 5, name='min_samples_split')\n#          ,Integer(1, 4, name='min_samples_leaf')]\n\n\n#@use_named_args(space)\n#def objective(**params):\n#   XGB_opt.set_params(**params)\n\n#    return -np.mean(cross_val_score(XGB_opt, X_train[vars_model], y_train, cv=5, n_jobs=-1,\n#                                    scoring=\"neg_root_mean_squared_error\"))","281b5777":"#from skopt import gp_minimize\n#res_gp = gp_minimize(objective, space, n_calls=50, random_state=0)\n\n#\"Best score=%.4f\" % res_gp.fun","ac36ca4f":"#print(\"\"\"Best parameters:\n#- max_depth=%d\n#- learning_rate=%.6f\n#- subsample=%.2f\n#- min_samples_split=%d\n#- min_samples_leaf=%d\"\"\" % (res_gp.x[0], res_gp.x[1],\n#                            res_gp.x[2], res_gp.x[3],\n#                            res_gp.x[4]))","06c15e59":"# model\nxgb_model = GradientBoostingRegressor(n_estimators= 100\n                                     ,learning_rate=0.1\n                                     ,max_depth=15\n                                     ,subsample=0.75)\n\nxgb_model.fit(X_train[vars_model], y_train)","fc99fb98":"# prediction\ny_pred_xgb = xgb_model.predict(X_test[vars_model])\ny_val_xgb = xgb_model.predict(df_val[vars_model])\n\n# performance\nxgb_result_oos = ml_error('XGBoost Regressor OOS', y_test, y_pred_xgb)\nxgb_result_oos","e69dfea3":"# performance\nxgb_result_oot = ml_error('XGBoost Regressor OOT', df_val.Weekly_Sales, y_val_xgb)\nxgb_result_oot","b204097b":"aux_compara = pd.concat([aux_compara, xgb_result_oos],axis = 0)\naux_compara = pd.concat([aux_compara, xgb_result_oot],axis = 0)\n\n\naux_compara","06a11724":"Import_vars_rf = pd.DataFrame({'Var':vars_model,\n                               'Importancia': rf.feature_importances_})\n\n#Ordena\u00e7\u00e3o das vari\u00e1veis pela import\u00e2ncia\nImport_vars_rf.sort_values('Importancia',ascending = False, inplace = True)\nImport_vars_rf.reset_index(drop=True,inplace = True)","30f36b68":"Import_vars_rf","37a9d3b3":"plt.rcdefaults()\nfig, ax = plt.subplots(figsize = (6,8))\n\n# Example data\nvar = Import_vars_rf['Var']\ny_pos = np.arange(len(var))\nimportance = Import_vars_rf['Importancia']\n\nax.barh(y_pos, importance, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(var)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('importance')\nax.set_ylabel('variaveis')\nax.set_title('Import\u00e2ncia Vari\u00e1veis')\n\nplt.show()","c654bc20":"Import_vars_rf.Var.unique()","5f971362":"vars_model_opt = ['Dept', \n                  'Size', \n                  'Store', \n                  'week_of_year', \n                  'CPI', \n                  #'relative_CPI',\n                  'Type_A', \n                  'Type_B', \n                  'day', \n                  'month', \n                  'Temperature', \n                  'Unemployment',\n                  #'relative_Unemployment', \n                  #'relative_Temperature', \n                  'MarkDown3',\n                  #'Type_C', \n                  'Fuel_Price', \n                  #'relative_Fuel_Price', \n                  'MarkDown4',\n                  'MarkDown5', \n                  'IsHoliday_True'\n                  #,'MarkDown1', 'MarkDown2', 'year'\n                ]","87023daf":"# model\nrf = RandomForestRegressor(n_estimators = 100\n                           ,max_depth = None\n                           ,max_features = 0.75)\n\nrf.fit(X_train[vars_model_opt], y_train)","beafc024":"# prediction\ny_pred_rf_opt = rf.predict(X_test[vars_model_opt])\ny_val_rf_opt = rf.predict(df_val[vars_model_opt])\n\n# Performance\nrf_result_opt_oos = ml_error('Random Forest Regressor Opt OOS', y_test, y_pred_rf_opt)\nrf_result_opt_oos","3be5d50c":"rf_result_opt_oot = ml_error('Random Forest Regressor Opt OOT', df_val.Weekly_Sales, y_val_rf_opt)\nrf_result_opt_oot","b9cd0c89":"aux_compara = pd.concat([aux_compara, rf_result_opt_oos],axis = 0)\naux_compara = pd.concat([aux_compara, rf_result_opt_oot],axis = 0)\n\n\naux_compara","23d9cd08":"LR_opt = LinearRegression()\n\nLR_opt.fit(X_train[vars_model_opt],y_train)\n","9ff11e0e":"y_pred_opt = LR_opt.predict(X_test[vars_model_opt])\ny_val_opt = LR_opt.predict(df_val[vars_model_opt])\n\n#performance oos\nlr_result_opt_oos = ml_error('Linear Regression - OPT - OOS', y_test, y_pred_opt)\nlr_result_opt_oos","558c3724":"# performance oot\nlr_result_opt_oot = ml_error('Linear Regression - OPT - OOT', df_val.Weekly_Sales, y_val_opt)\nlr_result_opt_oot","e42e3151":"aux_compara = pd.concat([aux_compara, lr_result_opt_oos],axis = 0)\naux_compara = pd.concat([aux_compara, lr_result_opt_oot],axis = 0)\n\n\naux_compara","f8535502":"# model\nlrr_rff_opt = Lasso(alpha = 0.01)\nlrr_rff_opt.fit(X_train[vars_model_opt], y_train)","c3ec357a":"# prediction\ny_pred_lrr_opt = lrr_rff_opt.predict(X_test[vars_model_opt])\ny_val_lrr_opt = lrr_rff_opt.predict(df_val[vars_model_opt])\n\n#performance\nlrr_result_opt_oos = ml_error('Linear Regression - Lasso - OPT - OOS', y_test, y_pred_lrr_opt)\nlrr_result_opt_oos","86802586":"lrr_result_opt_oot = ml_error('Linear Regression - Lasso - OPT - OOT', df_val.Weekly_Sales, y_val_lrr_opt)\nlrr_result_opt_oot","8bc5dd3f":"aux_compara = pd.concat([aux_compara, lrr_result_opt_oos],axis = 0)\naux_compara = pd.concat([aux_compara, lrr_result_opt_oot],axis = 0)\n\n\naux_compara","8d89d1be":"X_test.head()","7ce12e3e":"df_erros = pd.DataFrame({'is_holiday':X_test.IsHoliday_True\n                         ,'y_test':y_test\n                         ,'y_pred_LR':y_pred_opt\n                         ,'y_pred_Lasso':y_pred_lrr_opt\n                         ,'y_pred_RF':y_pred_rf_opt\n                         ,'y_pred_XGB':y_pred_xgb})","68d3dda8":"df_erros.head()","005ca4b7":"df_erros['w'] = df_erros['is_holiday'].apply(lambda x: 5 if x == 1 else 1)\n\ndf_erros.loc[:,'prod_LR'] = df_erros.loc[:,'y_test'] - df_erros.loc[:,'y_pred_LR']\ndf_erros.loc[:,'prod_Lasso'] = df_erros.loc[:,'y_test'] - df_erros.loc[:,'y_pred_Lasso']\ndf_erros.loc[:,'prod_RF'] = df_erros.loc[:,'y_test'] - df_erros.loc[:,'y_pred_RF']\ndf_erros.loc[:,'prod_XGB'] = df_erros.loc[:,'y_test'] - df_erros.loc[:,'y_pred_XGB']\n\ndf_erros.loc[:,'w_prod_LR'] = df_erros.loc[:,'prod_LR'] * df_erros.loc[:,'w']\ndf_erros.loc[:,'w_prod_Lasso'] = df_erros.loc[:,'prod_Lasso'] * df_erros.loc[:,'w']\ndf_erros.loc[:,'w_prod_RF'] = df_erros.loc[:,'prod_RF'] * df_erros.loc[:,'w']\ndf_erros.loc[:,'w_prod_XGB'] = df_erros.loc[:,'prod_XGB'] * df_erros.loc[:,'w']\n\nerro_LR = np.abs(df_erros.w_prod_LR.sum() \/ df_erros.w.sum()).round(2)\nerro_Lasso = np.abs(df_erros.w_prod_Lasso.sum() \/ df_erros.w.sum()).round(2)\nerro_RF = np.abs(df_erros.w_prod_RF.sum() \/ df_erros.w.sum()).round(2)\nerro_XGB = np.abs(df_erros.w_prod_XGB.sum() \/ df_erros.w.sum()).round(2)","afc1b0fc":"print('erro_LR', erro_LR)\nprint('erro_Lasso', erro_Lasso)\nprint('erro_RF', erro_RF)\nprint('erro_XGB', erro_XGB)","19441304":"df_val.head()","281ed8b3":"df_erros_val = pd.DataFrame({'is_holiday':df_val.IsHoliday_True\n                             ,'y_val':df_val.Weekly_Sales\n                             ,'y_val_LR':y_val_opt\n                             ,'y_val_Lasso':y_val_lrr_opt\n                             ,'y_val_RF':y_val_rf_opt\n                             ,'y_val_XGB':y_val_xgb})\n\ndf_erros_val['w'] = df_erros_val['is_holiday'].apply(lambda x: 5 if x == 1 else 1)\n\ndf_erros_val.loc[:,'prod_LR'] = df_erros_val.loc[:,'y_val'] - df_erros_val.loc[:,'y_val_LR']\ndf_erros_val.loc[:,'prod_Lasso'] = df_erros_val.loc[:,'y_val'] - df_erros_val.loc[:,'y_val_Lasso']\ndf_erros_val.loc[:,'prod_RF'] = df_erros_val.loc[:,'y_val'] - df_erros_val.loc[:,'y_val_RF']\ndf_erros_val.loc[:,'prod_XGB'] = df_erros_val.loc[:,'y_val'] - df_erros_val.loc[:,'y_val_XGB']\n\ndf_erros_val.loc[:,'w_prod_LR'] = df_erros_val.loc[:,'prod_LR'] * df_erros_val.loc[:,'w']\ndf_erros_val.loc[:,'w_prod_Lasso'] = df_erros_val.loc[:,'prod_Lasso'] * df_erros_val.loc[:,'w']\ndf_erros_val.loc[:,'w_prod_RF'] = df_erros_val.loc[:,'prod_RF'] * df_erros_val.loc[:,'w']\ndf_erros_val.loc[:,'w_prod_XGB'] = df_erros_val.loc[:,'prod_XGB'] * df_erros_val.loc[:,'w']\n\n\n\nerro_LR_val = np.abs(df_erros_val.w_prod_LR.sum() \/ df_erros_val.w.sum()).round(2)\nerro_Lasso_val = np.abs(df_erros_val.w_prod_Lasso.sum() \/ df_erros_val.w.sum()).round(2)\nerro_RF_val = np.abs(df_erros_val.w_prod_RF.sum() \/ df_erros_val.w.sum()).round(2)\nerro_XGB_val = np.abs(df_erros_val.w_prod_XGB.sum() \/ df_erros_val.w.sum()).round(2)","dd42cec2":"df_erros_val.head()","d8002f15":"df_err_consolid = pd.DataFrame({'Modelo': ['Baseline - Reg. Lin.', 'Baseline - Reg. Lin.', 'Reg. Lin. - Lasso', 'Reg. Lin. - Lasso', 'Random Forest', 'Random Forest', 'Gradient Boost', 'Gradient Boost']\n                               ,'Etapa': ['Teste - OOS', 'Valida\u00e7\u00e3o - OOT', 'Teste - OOS', 'Valida\u00e7\u00e3o - OOT', 'Teste - OOS', 'Valida\u00e7\u00e3o - OOT', 'Teste - OOS', 'Valida\u00e7\u00e3o - OOT']\n                               ,'Erro':[erro_LR, erro_LR_val, erro_Lasso, erro_Lasso_val, erro_RF, erro_RF_val, erro_XGB, erro_XGB_val]})","9494bd1e":"plt.figure()\nplt.title ('Erro Absoluto Ponderado pelos Feriados por Modelo')\nfig = sns.barplot(x = 'Modelo'\n                  ,y = 'Erro'\n                  ,data = df_err_consolid\n                  ,hue = 'Etapa')","3e1b2618":"print('erro_LR_val:', erro_LR_val)\nprint('erro_Lasso_val:', erro_Lasso_val)\nprint('erro_RF_val:', erro_RF_val)\nprint('erro_XGB_val:', erro_XGB_val)","62c540bd":"df_err_consolid","e17c8dec":"df_result = pd.DataFrame({'Store':viz_result_test['Store']\n                          ,'Dept':viz_result_test['Dept']\n                          ,'Date':viz_result_test['Date']\n                          ,'Year':viz_result_test['year']\n                          ,'Week':viz_result_test['week_of_year']\n                          ,'Weekly Sales': y_test\n                          ,'Weekly Sales Estimated': y_pred_rf_opt})\n\ndf_plot_store = pd.melt(df_result, id_vars = \"Store\", value_vars = [\"Weekly Sales\", 'Weekly Sales Estimated'], var_name = 'real_vs_estimated', value_name=\"Sales\")\ndf_plot_dept = pd.melt(df_result, id_vars = \"Dept\", value_vars = [\"Weekly Sales\", 'Weekly Sales Estimated'], var_name = 'real_vs_estimated', value_name=\"Sales\")","f9eac4d2":"df_plot_store.head()","905c9949":"plt.figure(figsize = (20,6))\nplt.title ('Avalia\u00e7\u00e3o das vendas estimadas em rela\u00e7\u00e3o ao real separado por loja')\nfig = sns.boxplot(x = 'Store'\n                  ,y = 'Sales'\n                  ,data = df_plot_store[['Store','Sales','real_vs_estimated']]\n                  ,showfliers = False\n                  ,hue = 'real_vs_estimated')","8fff6296":"df_plot_dept.head()","996bbd4b":"plt.figure(figsize = (20,6))\nplt.title ('Avalia\u00e7\u00e3o das vendas estimadas em rela\u00e7\u00e3o ao real separado por loja')\nfig = sns.boxplot(x = 'Dept'\n                  ,y = 'Sales'\n                  ,data = df_plot_dept[['Dept','Sales','real_vs_estimated']]\n                  ,showfliers = False\n                  ,hue = 'real_vs_estimated')","a19c997e":"real_weekly_sales_2010 = df_result[df_result.Year==2010].groupby('Week')['Weekly Sales'].mean()\nreal_weekly_sales_2011 = df_result[df_result.Year==2011].groupby('Week')['Weekly Sales'].mean()\nreal_weekly_sales_2012 = df_result[df_result.Year==2012].groupby('Week')['Weekly Sales'].mean()\n\nestimated_weekly_sales_2010 = df_result[df_result.Year==2010].groupby('Week')['Weekly Sales Estimated'].mean()\nestimated_weekly_sales_2011 = df_result[df_result.Year==2011].groupby('Week')['Weekly Sales Estimated'].mean()\nestimated_weekly_sales_2012 = df_result[df_result.Year==2012].groupby('Week')['Weekly Sales Estimated'].mean()\n","cb43dbd8":"plt.figure(figsize=(20,6))\n# plt.plot(weekly_sales_2010.index, weekly_sales_2010.values)\n# plt.plot(weekly_sales_2011.index, weekly_sales_2011.values)\nplt.plot(real_weekly_sales_2010.index, real_weekly_sales_2010.values)\nplt.plot(estimated_weekly_sales_2010.index, estimated_weekly_sales_2010.values)\n\nplt.xticks(np.arange(1, 53, step=1), fontsize=16)\nplt.yticks( fontsize=16)\nplt.xlabel('Week of Year', fontsize=16, labelpad=20)\nplt.ylabel('Sales', fontsize=20, labelpad=20)\n\nplt.title(\"Volume de vendas por semana de 2010, real versus estimado\", fontsize=16)\nplt.legend(['Real', 'Estimado'], fontsize=14);","c150bc31":"plt.figure(figsize=(20,6))\n# plt.plot(weekly_sales_2010.index, weekly_sales_2010.values)\n# plt.plot(weekly_sales_2011.index, weekly_sales_2011.values)\nplt.plot(real_weekly_sales_2011.index, real_weekly_sales_2011.values)\nplt.plot(estimated_weekly_sales_2011.index, estimated_weekly_sales_2011.values)\n\nplt.xticks(np.arange(1, 53, step=1), fontsize=16)\nplt.yticks( fontsize=16)\nplt.xlabel('Week of Year', fontsize=16, labelpad=20)\nplt.ylabel('Sales', fontsize=20, labelpad=20)\n\nplt.title(\"Volume de vendas por semana de 2011, real versus estimado\", fontsize=16)\nplt.legend(['Real', 'Estimado'], fontsize=14);","978c31bb":"plt.figure(figsize=(20,6))\n# plt.plot(weekly_sales_2010.index, weekly_sales_2010.values)\n# plt.plot(weekly_sales_2011.index, weekly_sales_2011.values)\nplt.plot(real_weekly_sales_2012.index, real_weekly_sales_2012.values)\nplt.plot(estimated_weekly_sales_2012.index, estimated_weekly_sales_2012.values)\n\nplt.xticks(np.arange(1, 53, step=1), fontsize=16)\nplt.yticks( fontsize=16)\nplt.xlabel('Week of Year', fontsize=16, labelpad=20)\nplt.ylabel('Sales', fontsize=20, labelpad=20)\n\nplt.title(\"Volume de vendas por semana de 2012, real versus estimado\", fontsize=16)\nplt.legend(['Real', 'Estimado'], fontsize=14);","c5727af1":"regressor_Pkl = pickle.dumps(rf)","34ab0b2c":"test.head()\n","489249c9":"# Para transformar esta c\u00e9lula em um arquivo .py \n\n# %%writefile Forecasting_Walmart_PyScore.py\n\n# ***********************************************************\n# *********** Imports ***************************************\n# ***********************************************************\n\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom sklearn.ensemble import RandomForestRegressor\n\n# ***********************************************************\n# ******* Fun\u00e7\u00f5es Utilizadas ********************************\n# ***********************************************************\n\ndef split_date(df,date):\n\n    '''\n    Transforma o campo indicado na entrada em data e extrai valores que podem ser relevantes, como ano, m\u00eas e dia. \n    A saida da fun\u00e7\u00e3o concatena esses novos campos no dataframe indicado na entrada\n    '''\n    \n    df['dt_ref'] = pd.to_datetime(df[date])\n    df['year'] = df.dt_ref.dt.year\n    df['month'] = df.dt_ref.dt.month\n    df['day'] = df.dt_ref.dt.day\n    df['week_of_year'] = df.dt_ref.dt.isocalendar().week\n    df['period_month'] = df_data.dt_ref.dt.to_period('M')\n    \ndef variaveis_relativas(df, lista_vars, var_group):\n    \n    '''\n    O objetivo desta fun\u00e7\u00e3o \u00e9 criar 'valores relativos', que indicam o quanto o valor mais atual de distancia do valor m\u00e9dio de um per\u00edodo de interesse.\n    Aqui foi testado o valor da vari\u00e1vel em rela\u00e7\u00e3o \u00e0 m\u00e9dia mensal, que \u00e9 indicada pela vari\u00e1vel 'var_group'.\n    Entradas: DataFrame com todas as informa\u00e7\u00f5es necess\u00e1rias para calcular os valores consolidados, lista dos campos que ser\u00e3o transformados e a vari\u00e1vel sobre a qual os valores ser\u00e3o agrupados\n    Sa\u00eddas: A fun\u00e7\u00e3o devolve o mesmo DataFrame, com as novas vari\u00e1veis concatenadas \n    '''\n    \n    # Calculando valores m\u00e9dios para servir de referencial \n    aux_mean = pd.DataFrame()\n    aux_names = [var_group]\n    \n    for var in lista_vars:\n        #print(var)\n        aux_mean[var] = df[[var_group,var]].groupby([var_group]).mean()\n        aux_name_2 = 'mean_' + var\n        aux_names.append(aux_name_2)\n\n    aux_mean.reset_index(inplace = True)\n\n    # Ajustando o nome dos campos\n    aux_mean.columns = aux_names\n\n    df = df.merge(aux_mean\n                  ,on = [var_group]\n                  ,how = 'inner')\n    \n    #Incluindo as vari\u00e1veis no dataframe\n    for var in lista_vars:\n        var_name = 'relative_' + var\n        mean_var = 'mean_' + var\n        df[var_name] = (df[var] + 0.0001)\/df[mean_var]\n        \n    return df\n\n# ***********************************************************\n# ******* Listas Importantes ********************************\n# ***********************************************************\n\n\n# Vari\u00e1veis para o modelo\ninput_cols = ['Store', 'Dept', 'Weekly_Sales', 'IsHoliday', 'Temperature',\n       'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4',\n       'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'year',\n       'month', 'day', 'week_of_year']\n\n# Vari\u00e1veis num\u00e9ricas\nvar_num = ['Store', 'Dept', 'Size','Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n       'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'year', 'month', 'day',\n       'week_of_year']\n\nvar_num_2 = var_num + ['relative_Temperature'\n                       ,'relative_Fuel_Price'\n                       ,'relative_CPI'\n                       ,'relative_Unemployment']\n\n\n# Vari\u00e1veis categ\u00f3ricas\nvar_cat = ['IsHoliday', 'Type']\n\n# Vari\u00e1vel alvo\ntarget_col = 'Weekly_Sales'\n\nvars_model = ['Store',\n             'Dept',\n             'Size',\n             'Temperature',\n             'Fuel_Price',\n             'MarkDown1',\n             'MarkDown2',\n             'MarkDown3',\n             'MarkDown4',\n             'MarkDown5',\n             'CPI',\n             'Unemployment',\n             'year',\n             'month',\n             'day',\n             'week_of_year',\n             'relative_Temperature',\n             'relative_Fuel_Price',\n             'relative_CPI',\n             'relative_Unemployment',\n             'IsHoliday_True',\n             'Type_A',\n             'Type_B',\n             'Type_C']\n\nvars_model_opt = ['Dept', \n                  'Size', \n                  'Store', \n                  'week_of_year', \n                  'CPI', \n                  'Type_A', \n                  'Type_B', \n                  'day', \n                  'month', \n                  'Temperature', \n                  'Unemployment',\n                  'MarkDown3',\n                  'Fuel_Price',  \n                  'MarkDown4',\n                  'MarkDown5', \n                  'IsHoliday_True']\n\n# ***********************************************************\n# ******* Carregando os Datasetes ***************************\n# ***********************************************************\n\n# Carregando as bases em dataframes panda\nfeatures = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/features.csv\")\nstores = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv\")\ntest = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/test.csv\")\n\n# Combinando o dataset de teste com as outras informa\u00e7\u00f5es dispon\u00edveis para conseguir geraras previs\u00f5es\ndf_test = test.merge(features\n                     ,on = ['Store','Date','IsHoliday']\n                     ,how = 'inner').merge(stores\n                                           ,on = ['Store']\n                                           ,how = 'inner')\n\n# ***********************************************************\n# ******* Prepara\u00e7\u00e3o da base ********************************\n# ***********************************************************\n\n# Incluindo formas diferentes de imputar valores para os campos missing nas vari\u00e1veis\n\n# Aqui escolhi inputar o valor m\u00e9dio das vari\u00e1veis para todo o dataset. \n# Em produ\u00e7\u00e3o, este valor deveria ser fixo, pois qualquer varia\u00e7\u00e3o no comportamento da popula\u00e7\u00e3o pode influenciar no comportamento da vari\u00e1vel\nmean_list = ['Temperature','Fuel_Price','CPI','Unemployment']\n\n# Aqui mantive a ideia inicial, utilizada para o dataset de treino, substituindo por zero.\nmarkdowns = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n\nfor var in mean_list:\n    df_test[var] = df_test[var].fillna(df_test[var].mean())\n\nfor var in markdowns:\n    df_test[var] = df_test[var].fillna(0)\n    \n# Aplicando as fun\u00e7\u00f5es para constru\u00e7\u00e3o, primeiro das vari\u00e1veis relacionadas com as datas e per\u00edodos de interesse, e depois para constru\u00e7\u00e3o das vari\u00e1veis relativas.\nsplit_date(df_test, 'Date')\ndf_test = variaveis_relativas(df_test, mean_list, 'month')\n\n# Normalizando as vari\u00e1veis cont\u00ednuas com o scaler que foi ajustado com o dataset de treino e armazenado no pickle.\nscaler = pickle.loads(scaler_Pkl) \ndf_test[var_num_2] = scaler.transform(df_test[var_num_2])\n\n# Inclus\u00e3o das vari\u00e1veis categ\u00f3ricas, j\u00e1 binarizadas, na lista geral de vari\u00e1veis.\n# Como s\u00e3o poucas vari\u00e1veis, decidi fazer o ajuste manual.\ndf_test['IsHoliday_True'] = np.where(df_test.IsHoliday == True, 1,0)\ndf_test['Type_A'] = np.where(df_test.Type == 'A', 1,0)\ndf_test['Type_B'] = np.where(df_test.Type == 'B', 1,0)\ndf_test['Type_C'] = np.where(df_test.Type == 'C', 1,0)\n\n# A defini\u00e7\u00e3o desta lista, assim como as outras no in\u00edcio da c\u00e9lula, \u00e9 redundante no notebook, mas se considerarmos que a id\u00e9ia desta c\u00e9lula \u00e9 ter um c\u00f3digo que poderia ser levado para produ\u00e7\u00e3o, estas defini\u00e7\u00f5es s\u00e3o importantes.\nvars_model = var_num_2 + ['IsHoliday_True', 'Type_A', 'Type_B', 'Type_C']\n\n# ***********************************************************\n# ********* Escorando a base ********************************\n# ***********************************************************\n\n\nrf_pickle = pickle.loads(regressor_Pkl) \n\n# Escoragem da popula\u00e7\u00e3o na base teste\ny_pred_rf_test = rf.predict(df_test[vars_model_opt])","565fc367":"### Concentra\u00e7\u00e3o de informa\u00e7\u00f5es faltantes 'Missings'","286d7296":"* Aqui podemos perceber que o volume de vendas varia muito entre as lojas. No entanto, ainda n\u00e3o \u00e9 poss\u00edvel perceber claramente o impacto dos feriados nas vendas.","70fb1ec6":"* Como esperado, com exce\u00e7\u00e3o da temperatura, as vari\u00e1veis constru\u00eddas s\u00e3o altamente correlacionadas com as vari\u00e1veisl originais.\n* Se formos trabalhar com um modelo de regress\u00e3o, \u00e9 importante tratar de forma espec\u00edfica estas vari\u00e1veis. Como os modelo n\u00e3o lineares n\u00e3o s\u00e3o t\u00e3o impactados por vari\u00e1veis correlacionadas, podemos seguir coma s vari\u00e1veis e depois fazemos a escolha da vari\u00e1vel que resultar em um valor de import\u00e2ncia maior e aplicamos este conceito para os modelos lineares tamb\u00e9m.\n* Adicionamentel, este ponto poderia ser minimizado aumentando o per\u00edodp m\u00e9dio de compara\u00e7\u00e3o, antes de construir o valor relavito \u00e0 m\u00e9dia.\n* Isso pode ser testado para uma segunda vers\u00e3o do modelo final.\n","b8403e99":"* De forma similar ao que observamos na an\u00e1lise do volume de vendas por m\u00eas, podemos notar um aumento expressivo no n\u00famero de vendas nas semanas referentes aos feriados de a\u00e7\u00e3o de gra\u00e7as e natal.\n* Este ponto indica que talvez seja importante criar vari\u00e1veis adicionais para separar os feriados, como uma combina\u00e7\u00e3o de per\u00edodo do ano por exemplo.","2bf4f94f":"# An\u00e1lise dos resultados sob a perspectiva do WMAE\n\n* A avalia\u00e7\u00e3o do desempenho do modelo sob a perspectiva do erro m\u00e9dio ponderado pelos feriados trouxe uma vis\u00e3o bem diferente das outras m\u00e9tricas analisadas no desenvolvimento. \n* Quando olhamos para os valores de erro dentro do mesmo per\u00edodo em que o modelo foi desenvolvido, observamos valores semelhantes de erro.\n* No entanto, o desempenho do modelo em um per\u00edodo diferente do que foi treinado foi muito inferior para as t\u00e9cinas de regress\u00e3o. \n* \u00c9 importante salientar aqui que, por raz\u00e3o do tempo, s\u00f3 avaliei um m\u00eas, que pode ser um m\u00eas at\u00edpico, mesmo possibilitando a an\u00e1lise de 4 semanas distintas. \n* Ainda assim, tanto o random forest quanto o gradient boost foram superiores.\n* Entre os dois \u00faltimos, temos valores pr\u00f3ximos de desempenho. O Randon Forest (RF) mostrou valores maiores de erro na base de teste e valores menores no dataset de valida\u00e7\u00e3o fora do tempo.\n* Neste caso, como os valores s\u00e3o pr\u00f3ximos, o RF mostrou melhor desempenho fora do per\u00edodo de desenvolvimento e \u00e9 uma t\u00e9cnica mais simples, isto \u00e9, mais barata do ponto de vista computacional, escolhi seguir com o Random Forest para a an\u00e1lise da base de Teste.\n* Quando comparamos os valores reais com os previstos, podemos perceber qu eo modelo funcionou bem para a previs\u00e3o de todas as lojas, com pequenas varia\u00e7\u00f5es.\n* O modelo tamb\u00e9m mostrou boa performance para todos os departamentos, com exce\u00e7\u00e3o do 39.\n* Por fim, quando avaliamos as previs\u00f5es semana a semana, podemos observar que o modelo consegue prever de forma eficiente as vendas, inclusive nos per\u00edodos de feriados","6449ddce":"Como citado no enunciado do problema, as informa\u00e7\u00f5es dos campos 'markdown' s\u00f3 est\u00e3o dispon\u00edveis a partir de um per\u00edodo espec\u00edfico.","63b95af1":"# Walmart Recruiting - Store Sales Forecasting\n\n* ***Objetivo:*** Prever o volume de vendas semanal das lojas Walmart\n* ***Relev\u00e2ncia do Problema:*** Ser capaz de estimar as vendas em cada loja pode trazer benef\u00edcios significativos nos planejamentos de estoque e log\u00edstica, alem de aloca\u00e7\u00e3o de funcion\u00e1rios e gest\u00e3o de recursos e campanhas de marketing.\n* ***Import\u00e2ncia dos Feriados no Problema:*** Todos os aspectos citados anteriormente podem ser impactados significativamente por datas que influenciam de maneira extraordin\u00e1ria no volume de vendas.","45a768e7":"# An\u00e1lise Explorat\u00f3ria\n\n* O Objetivo desta etapa \u00e9 entender o dataset um pouco mais a fundo.\n* O primeiro passo \u00e9 entender o evento, como est\u00e1 suaa distribui\u00e7\u00e3o em rela\u00e7\u00e3o ao tempo, se existe sazonalidade e em que granularidade.\n* Como o enunciado d\u00e1 bastante \u00eanfase aos feriados, podemos partir deste par\u00e2metro, e depois podemos aprofundar as an\u00e1lises.","11a35603":"# Instalando pacotes adicionais necess\u00e1rios","3420ad18":"## Baseline - Regress\u00e3o Linear","49cef07e":"### Train\n* Cont\u00e9m os campos:\n* **'Store'** e **'Depto'**, que ajudam a identificar o volume de vendas por loja e por departamento;\n* **'Date'** e **'Weekly_Sales'**, que trazer a combina\u00e7\u00e3o entre semana e volume de vendas naquela semana, respectivamente e;\n* **'IsHoliday'**, que diz se existe um feriado naquela semana.","af27827f":"* Quando avaliamos o volume de vendas m\u00eas a m\u00eas, percebemos que os feriados influenciam sim nas vendas, mas que essa influ\u00eancia \u00e9 diferente entre os feriados. ","86f551b9":"# Divis\u00e3o da base em treino, teste e valida\u00e7\u00e3o\n\n* Aqui dividimos a base em treino e teste, antes de utilizar a base 'test' de fato.\n* Al\u00e9m disso, podemos separar uma data de refer\u00eancia mais recente para avaliar o modelo em um intervalo de tempo em que ele ainda n\u00e3o foi treinado. \n* Desta forma minimizamos a chance de overfit e podemos avaliar a estabilidade do modelo ao longo do tempo.","dc30dfa4":"# Carregando Bibliotecas necess\u00e1rias para o desenvolvimento do trabalho","d00e8e67":"# Upload dados\n\nNesta etapa deixei duas vers\u00f5es poss\u00edveis, uma para o upload dos arquivos dentro do kaggle e outra para o uso na m\u00e1quina local.","ef6a3bf1":"# An\u00e1lise sobre o modelo final\n\n* Nesta etapa vale explorar as vari\u00e1veis com um pouco mais de cuidado. \n* Baseado no valor da import\u00e2cia, escolhi entre as vari\u00e1veis absolutas e relativas, alem de remover as vari\u00e1veis com import\u00e2ncia menor que 0,001.\n","e39c943a":"## Varia\u00e7\u00e3o do volume de vendas em rela\u00e7\u00e3o aos feriados","64ef1462":"* Dentre as vari\u00e1veis analisadas, chamou aten\u00e7\u00e3o MarkDown4 e 1, al\u00e9m do ano com o preco do pre\u00e7o do combust\u00edvel.\n","d685fe57":"# An\u00e1lise Explorat\u00f3ria das vari\u00e1veis explicativas\n\n* Ap\u00f3s entender um pouco melhor o comportamento das vendas, \u00e9 importante entender e avaliar as vari\u00e1veis que podem ajudar na predi\u00e7\u00e3o do volume de vendas.\n* O primeiro passo \u00e9 dividir as vari\u00e1veis em num\u00e9ricas e categ\u00f3ricas.\n* Outro ponto imporante \u00e9 a consist\u00eancia desses campos, que pode influenciar consideravelmente na estabilidade do modelo ao longo do tempo.\n\n","47f0690c":"## Gradient Boosting - XGB\n\n* Da mesma forma que o Random Forest, tive que rodar a etapa de otimiza\u00e7\u00e3o dos hiperpar\u00e2metros na minha m\u00e1quina local.\n* Como os modelos xgb demoram ainda mais para rodar e, consequentemente, testar os par\u00e2metros, utilizei um m\u00e9todo n\u00e3o-exaustivo para acelerar o processo de otimiza\u00e7\u00e3o dos par\u00e2metros.\n* O c\u00f3digo utilizado est\u00e1 comentado aqui.","2f3a1ae2":"* Podemos observar um comportamento linear em relacao ao evento para algumas vari\u00e1veis, como o tamanho das lojas (Size)\n* Outras vari\u00e1veis parecem n\u00e3o estar em nada associadas ao volume de vendas, como o pre\u00e7o do combust\u00edvel.\n* De forma geral, as vari\u00e1veis n\u00e3o demonstram um comportamento linear em rela\u00e7\u00e3o ao volume de vendas.\n* Isso indica que, provavelmente, um modelo n\u00e3o linear ser\u00e1 a melhor op\u00e7\u00e3o para resolver esse problema.\n* Outra possivel solu\u00e7\u00e3o seria utilizar essas vari\u00e1veis divididas em faixas, agrupando valores de acordo com a rela\u00e7\u00e3o com o volume de vendas.\n","6e0b5851":"### Stores\n* Traz os campos 'Store', com valores variando de 1 a 45, e 'Type', com valores entre 1 e 3","2133fc8a":"### Varia\u00e7\u00e3o do volume de vendas entre os departamentos e influ\u00eancia dos feriados \n\n* Aqui repetimos a an\u00e1lise realizada por loja, mas olhando para os departamentos.","12b893bc":"## Feature Engeneering\n\n* Aqui podemos utilizar um valor relativo para algumas vari\u00e1veis cont\u00ednuas. \n* Por exemplo, podemos comparar o valor da Temperatura na semana em quest\u00e3o com o valor m\u00e9dio do m\u00eas, dos \u00f9ltimos 3, 6 ou at\u00e9 12 meses. Desta forma conseguimos avaliar se a mudan\u00e7a na temperatura em rela\u00e7\u00e3o a um per\u00edodo anterior impacta o volume de vendas.\n* O racional \u00e9 o mesmo para todos os campos aqui.","e92ba7f0":"* Aqui \u00e9 poss\u00edvel notar o impacto do feriado em departamentos espec\u00edficos, como o 55 e o 72.\n* No entanto, tamb\u00e9m \u00e9 possivel observar influ\u00eancia negativa do feriado no departamento 65.","041ac594":"## Acessando arquivos via Google Colab ou na m\u00e1quina local","4335b4b5":"### Varia\u00e7\u00e3o do volume de vendas por loja e influ\u00eancia dos feriados nas vendas de cada loka\n\n* Com essa visualiza\u00e7\u00e3o podemos investigar a varia\u00e7\u00e3o do volume de vendas entre as lojas e tamb\u00e9m o quanto os feriados impactam as vendas em cada loja.","f1ea5a2a":"### Volume de vendas por semana ao longo do ano\n\n* Por fim, podemos avaliar o volume de vendas por semana, se existe alguma sazonalidade e\/ou influ\u00eancia dos feriados que possa nos direcionar na constru\u00e7\u00e3o de novas vari\u00e1veis e no desenvolvimento do  modelo.","949cc5ce":"### Volume de vendas por semana ao longo do ano\n\n* Por fim, podemos avaliar o volume de vendas por semana, se existe alguma sazonalidade e\/ou influ\u00eancia dos feriados que possa nos direcionar na constru\u00e7\u00e3o de novas vari\u00e1veis e no desenvolvimento do  modelo.","998af896":"# Cria\u00e7\u00e3o da Base inicial\n\nAqui \u00e9 importante agrupar as informa\u00e7\u00f5es em uma \u00fanica base para facilitar a preparacao e o tratamento necess\u00e1rio das vari\u00e1veis.","0cb7d607":"## Separando as vari\u00e1veis em num\u00e9ricas e categ\u00f3ricas","c0d891c9":"## Random Forrest\n\n* Como o tempo de execu\u00e7\u00e3o dos notebooks na plataforma \u00e9 limitada, acabei rodando essa etapa do GridSearchCV fora deste notebok.\n* De qualquer forma, mantive aqui o c\u00f3digo, como ele foi utilizado, por\u00e9m comentado. ","b495499c":"## Regress\u00e3o Linear - l1 (Lasso)","f316a338":"# Armazenando modelo final\n\n* Os resultados acima mostraram que o melhor modelo foi o Random Forest.\n* Assim, aqui armazenamos o modelo em um pickle para poder us\u00e1-lo na base de teste, de forma que este processo seja reprodut\u00edvel em produ\u00e7\u00e3o.","69d2419f":"## Regress\u00e3o Linear - l1 (Lasso) - Com vari\u00e1veis selecionadas pelo Random Forest","5bcd61ad":"# Vari\u00e1veis Num\u00e9ricas ou Cont\u00ednuas\n\n* Aqui as vari\u00e1veis num\u00e9ricas foram 'quebradas' em faixas de valor e comparadas com o volume de vendas.\n* O Objetivo aqui \u00e9 avaliar o comportamento das vari\u00e1veis em rela\u00e7\u00e3o ao evento.\n* Existem algumas formas diferentes de realizar essa an\u00e1lise, mas desta forma conseguimos avaliar se existe uma rela\u00e7\u00e3o linear entre as vari\u00e1veis e o evento, se existe um pico de vendas relacionado com alguma faixa de alguma vari\u00e1vel, ou se alguns campos parecem n\u00e3o trazer informa\u00e7\u00f5es relevantes para o modelo.","febec17f":"# Compara\u00e7\u00e3o entre os modelos utilizando a m\u00e9trica proposta pela competi\u00e7\u00e3o\n\n* **M\u00e9trica:** Weighted Mean Absolute Error (WMAE)\n* A WMAE \u00e9 semelhante ao Mean Absolute Error (MAE), que foi avaliada no desenvolvimento. \n* A grande diferen\u00e7a aqui \u00e9 que as semanas que contem feriados tem um peso maior na avalia\u00e7\u00e3o do modelo","4a7ffa30":"* Quando comparamos apenas o volume de vendas em rela\u00e7\u00e3o aos feriados, n\u00e3o encontramos grande diferen\u00e7a.\n* Deste dado, podemos supor que nem todos os feriados influenciam no volume de vendas, ou que a pr\u00f3pria variacao do volume de vendas \u00e9 muito grande.","0f87957c":"# Aplicando o modelo na base de teste\n\n* Nesta etapa, podemos aproveitar para avaliar o desempenho do modelo e tamb\u00e9m para garantir que o modelo, bem como as transforma\u00e7\u00f5es necess\u00e1rias nas vari\u00e1veis, podem ser aplicadas em uma nova base.\n* Para esta etapa, podemos utilizar o modelo e a fun\u00e7\u00e3o de normaliza\u00e7\u00e3o (scaler) que foram armazenadas em arquivos pickle anteriormente, durante o desenvolvimento.","9ddef984":"# Vari\u00e1veis Categ\u00f3ricas\n\n* **Seguindo o mesmo processo para as vari\u00e1veis categ\u00f3ricas**","2e7cd7d3":"## Avalia\u00e7\u00e3o do erro no conjunto de valida\u00e7\u00e3o (OOT)","02918697":"## Avalia\u00e7\u00e3o do erro no conjunto de teste (OOS)","cfb4c9db":"### Fun\u00e7\u00e3o para consolidar as principais informa\u00e7\u00f5es referentes \u00e0s vari\u00e1veis","9ed774bf":"# Incluindo as vari\u00e1veis categ\u00f3ricas no DataFrame normalizado","0b7ff82a":"## Correla\u00e7\u00e3o entre Vari\u00e1veis Num\u00e9ricas - Incluindo novas vari\u00e1veis\n\nMesmo processo realizado anteriormente, mas adicionando as novas vari\u00e1veis que, possivelmente, estar\u00e3o correlacionadas com suas origens (valores absolutos)","abf8f31e":"## An\u00e1lise sobre os modelos testados\n\n* Como esperado, os modelos Random Forest (RF) e Gradient Boost (XGB) demonstraram desenpenho muito superior ao modelo Baseline e tamb\u00e9m em rela\u00e7\u00e3o \u00e0 regress\u00e3o com regulariza\u00e7\u00e3o.\n* Os modelos RF e XGB mostraram desempenho semelhante. No entanto, dada a maior compledixade do XGB, faz sentido seguir com o Random Forest.","508d8869":"## Correla\u00e7\u00e3o entre Vari\u00e1veis Num\u00e9ricas\n\nEmbora a correla\u00e7\u00e3o n\u00e3o impacte muito os modelos n\u00e3o-lineares, \u00e9 importante ter conhecimento do comportamento e da correla\u00e7\u00e3o entre as vari\u00e1veis para conseguir avaliar sua import\u00e2ncia no modelo final.","49de00d0":"# An\u00e1lise das predi\u00e7\u00f5es do modelo para o volume de vendas no per\u00edodo analisado\n\n* Aqui podemos avaliar como o modelo se comportou por loja, por departamento e por per\u00edodo.\n* Isto nos permite avaliar se \u00e9 necess\u00e1rio ter um modelo separado por loja ou por departamento.\n* Tamb\u00e9m podemos verificar se o modelo consegue prever os picos de vendas que ocorrem em feriados espec\u00edficos. ","3ee48d01":"### Input de missings\n\n* Aqui tomei a decis\u00e3o de substituir os valores faltantes por 0, indicando que a falta do registro poder\u00e1 ser entendida como aus\u00eancia de a\u00e7\u00f5es promocionais.","d695c1ca":"## Baseline - Regress\u00e3o Linear com vari\u00e1veis selecionadas pelo Random Forest\n\n* Aqui removi as var\u00e1veis correlacionadas, optando pela vari\u00e1vel com maior import\u00e2ncia dada pelo modelo anterior.","24bec89e":"## Upload dentro do Kaggle","9d2bd61a":"* Aqui achei interessante construir o c\u00f3digo de forma que seria poss\u00edvel utilizar esta c\u00e9lula como um todo para implanta\u00e7\u00e3o do modelo em produ\u00e7\u00e3o, uma vez que ele traz todas as bibliotecas, fun\u00e7\u00f5es e listas necess\u00e1rias para escoragem da base pelo modelo.","237e6516":"# Treinando os Modelos","b1f127fe":"# Normaliza\u00e7\u00e3o das vari\u00e1veis num\u00e9ricas\n\n* Embora os modelos n\u00e3o lineares n\u00e3o sejam sens\u00edveis \u00e0 diferen\u00e7as de amplitude entre as vari\u00e1veis, a ideia \u00e9 testar diversas t\u00e9cnicas diferentes. ","a8b96913":"### Avalia\u00e7\u00e3o do volume de vendas por m\u00eas\n\n* Como n\u00e3o encontramos uma diferen\u00e7a significativa entre 'Feriados' e 'N\u00e3o Feriados', decidi investigar a relacao entre feriados e vendas um pouco mais a fundo.\n* Aqui podemos avaliar o volume de vendas por m\u00eas, com os meses que possuem feriados destacados","c42828d0":"## Salvando Scaler para utilizar em produ\u00e7\u00e3o","031b67a9":"## Import\u00e2ncia das Vari\u00e1veis","e5e153c3":"## Visualizando os dataframes\n\nForam disponibilizadas bases separadas, com informa\u00e7\u00f5es distribuidas entre elas. No total s\u00e3o 5 arquivos diferentes.\n* **Features:** Contem as vari\u00e1veis explicativas que deve ajudar no desenvolvimento do modelo;\n* **Stores:** Cont\u00e9m a informa\u00e7\u00e3o das lojas em rela\u00e7\u00e3o ao tipo e ao tamanho (Size);\n* **Train:** Traz as informa\u00e7\u00f5es de venda por loja, departamento e data de refer\u00eancia, al\u00e9m da informa\u00e7\u00e3o de feriados, que parece ser bem relevante pelo enunciado do problema;\n* **Test:** Cont\u00e9m as mesmas informa\u00e7\u00f5es disponibilizadas na base de treino, mas para datas diferentes e;\n* **Submission:** Cont\u00e9m a base para ser escorada e submetida para avalia\u00e7\u00e3o na competi\u00e7\u00e3o","5694cc57":"\n* No caso das vari\u00e1veis categ\u00f3ricas, podemos observar valores m\u00e9dios de vendas bem distintos entre as categorias, o que indica que essas vari\u00e1veis podem ser relevantes para o modelo."}}