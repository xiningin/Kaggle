{"cell_type":{"078ad94e":"code","e6c4adf0":"code","c6ddf088":"code","4600df7b":"code","868c214a":"code","0e942635":"code","4b86b419":"code","f3ef2125":"code","724cc5c7":"code","b680f8ea":"code","a9a86e2c":"code","ec993a5f":"code","8833a61f":"code","7811cafa":"code","9c567f9f":"code","6532c0e9":"code","06deca16":"code","d71a7dbb":"code","ed92d598":"code","6bda36fa":"code","03796774":"code","fe6ad06f":"markdown","a3201a9b":"markdown","708acc48":"markdown","7d9dd098":"markdown","86242bcb":"markdown","9756424d":"markdown","e8909547":"markdown","274f6816":"markdown","aa0b0664":"markdown","34c0c667":"markdown","242c7c0e":"markdown","6462d220":"markdown","7b42fe76":"markdown","415f6a3f":"markdown","e2151a37":"markdown"},"source":{"078ad94e":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","e6c4adf0":"df=pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf.head()","c6ddf088":"df.info()","4600df7b":"print(df.isnull().sum())","868c214a":"df.describe()","0e942635":"from sklearn.preprocessing import StandardScaler\nsc= StandardScaler()\ndf_copy=df.copy()\n\n\ndf_copy=sc.fit_transform(df_copy)\n\nfrom sklearn.decomposition import PCA\npca=PCA(n_components=2)\ndf_pca=pca.fit_transform(df_copy)\n\nprincipaldF=pd.DataFrame(data=df_pca,columns=['PC1','PC2'])\nfinaldf = pd.concat([principaldF, df[['DEATH_EVENT']]], axis = 1)\n\nfinaldf.head()","4b86b419":"import plotly.express as px\nexp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n\npx.area(\n    x=range(1, exp_var_cumul.shape[0] + 1),\n    y=exp_var_cumul,\n    labels={\"x\": \"Components\", \"y\": \"Explained Variance\"}\n)","f3ef2125":"sns.set_style(\"darkgrid\", {\"axes.facecolor\": \"0.95\"})\nfig,ax = plt.subplots(1, 1,figsize = (15,6))\nsns.countplot(x='sex',\n              hue = 'DEATH_EVENT', \n              data=df,\n              palette=[\"cornflowerblue\", \"khaki\"])\n\n\n\nax.legend([\"No\",\"Yes\"], \n              bbox_to_anchor=(1,1), \n              title='Survival')\n\nax.set_xticklabels(['Male','Female'],fontdict= { 'fontsize': 10, 'fontweight':'bold'})\n# Customize the axes and title\nax.set_title(\"Death by heart failure amoung genders\",fontdict= { 'fontsize': 20, 'fontweight':'bold'})\nax.set_ylabel(\"Amount\",fontdict= { 'fontsize': 15, 'fontweight':'bold'})\nax.set_xlabel(\"Gender\",fontdict= { 'fontsize': 15, 'fontweight':'bold'})\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2., height + .3,height ,ha=\"center\")\n","724cc5c7":"fig,ax = plt.subplots(1, 2,figsize = (15,6))\nsns.countplot(x=\"smoking\",data=df,ax = ax[0], palette=[\"cornflowerblue\", \"khaki\"])\nsns.countplot(x=\"DEATH_EVENT\",hue = 'smoking', data=df,ax = ax[1], palette=[\"cornflowerblue\", \"khaki\"])\n\n\n#annotatinos\nfor i in range(2):\n    for p in ax[i].patches:\n        height = p.get_height()\n        ax[i].text(p.get_x()+p.get_width()\/2., height + .3,height ,ha=\"center\")\n        \n\nax[0].set_xticklabels(['Non smoker','Smoker'],fontdict= { 'fontsize': 10, 'fontweight':'bold'})\nax[0].set_title(\"Distrubution of smokers\",fontdict= { 'fontsize': 20, 'fontweight':'bold'})\n\n\nax[1].legend([\"Non smoker\",\"Smoker\"], \n              bbox_to_anchor=(1,1))\n\nax[1].set_xticklabels(['Didnt survive','Survived'],fontdict= { 'fontsize': 10, 'fontweight':'bold'})\nax[1].set_title(\"Heart failure due to smoking\",fontdict= { 'fontsize': 20, 'fontweight':'bold'})\n\n","b680f8ea":"fig,ax = plt.subplots(1, 2,figsize = (15,6))\n\nsns.countplot(x=\"smoking\",hue = 'DEATH_EVENT',data=df[df['sex'] == 1],ax = ax[0],palette=[\"cornflowerblue\", \"khaki\"])\nsns.countplot(x=\"smoking\",hue = 'DEATH_EVENT', data=df[df['sex'] == 0],ax = ax[1],palette=[\"cornflowerblue\", \"khaki\"])\n\n\nax[0].set_title('Male')\n\nax[0].legend([\"Didnt Survive\",\"Survived\"], \n              loc=\"upper right\")\n\nax[1].legend([\"Didnt Survive\",\"Survived\"], \n              loc=\"upper right\")\nax[1].set_title('Female')\n\n#annotatinos\nfor i in np.arange(2):\n    for p in ax[i].patches:\n        height = p.get_height()\n        ax[i].text(p.get_x()+p.get_width()\/2., height + .3,height ,ha=\"center\")\nfig.suptitle('Amount of deaths and smoking among genders', fontsize =15)","a9a86e2c":"age_counts = df[\"age\"].value_counts()\nfig = px.bar(age_counts, title=\"Age distribution\")\nfig.update_layout(\n    xaxis_title = \"Age\",\n    yaxis_title = \"Frequency\",\n    title_x = 0.5, \n    showlegend = False\n)\nfig.show()\n\nage = pd.cut(df['age'], 8)\nfig, axs = plt.subplots(figsize=(15, 8))\nsns.countplot(x=age,hue='DEATH_EVENT', \n              data=df,palette=[\"cornflowerblue\", \"khaki\"]).set_title(\"Age distrubation with deaths\",\n                                                                { 'fontsize': 20, 'fontweight':'bold'});\naxs.legend([\"Didnt Survive\",\"Survived\"], \n              loc=\"upper right\")","ec993a5f":"df['ejection_fraction'] = df['ejection_fraction'].div(100).round(2)","8833a61f":"age_counts = df[\"ejection_fraction\"].value_counts()\nfig = px.bar(age_counts, title=\"Distribution of how much blood is pumped out (ejection) as a percentage (normal 50-70%)\")\nfig.update_layout(\n    xaxis_title = \"Ejection percentage\",\n    yaxis_title = \"Frequency\",\n    title_x = 0.5, \n    showlegend = False\n)\nfig.show()\n\nfig, axs = plt.subplots(figsize=(15, 8))\nsns.countplot(x='ejection_fraction',hue='DEATH_EVENT', \n              data=df,palette=[\"cornflowerblue\", \"khaki\"]).set_title(\"Ejection distribution and heart failure\",\n                                                                { 'fontsize': 20, 'fontweight':'bold'});\naxs.legend([\"Didnt Survive\",\"Survived\"], \n              loc=\"upper right\")","7811cafa":"X = df.iloc[:, :11].values\ny = df.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","9c567f9f":"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Lasso\n\npipeline = Pipeline([\n                     ('scaler',StandardScaler()),\n                     ('model',Lasso())\n])\n\nsearch = GridSearchCV(pipeline,\n                      {'model__alpha':np.arange(0.1,10,0.1)},\n                      cv = 5, scoring=\"neg_mean_squared_error\",verbose=3\n                      )\nsearch.fit(X_train,y_train)\nsearch.best_params_","6532c0e9":"coefficients = search.best_estimator_.named_steps['model'].coef_\nimportance = np.abs(coefficients)\nimportance","06deca16":"X=df.iloc[:,[0,4,7]].values\ny=df.iloc[:, -1].values\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","d71a7dbb":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, np.ravel(y_train))\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accurecy score of CM: \",round(accuracy_score(y_test, y_pred),2))\n\n\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"\\nAccuracy with 10-kfold: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","ed92d598":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accurecy score of CM: \",round(accuracy_score(y_test, y_pred),2))\n\n\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"\\nAccuracy with 10-kfold: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","6bda36fa":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\n\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accurecy score of CM: \",round(accuracy_score(y_test, y_pred),2))\n\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"\\nAccuracy with 10-kfold: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","03796774":"import lightgbm as lgb\nclf = lgb.LGBMClassifier()\nclf.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accurecy score of CM: \",round(accuracy_score(y_test, y_pred),2))\n\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"\\nAccuracy with 10-kfold: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","fe6ad06f":"#### Before preceeding with lasso and model prediction a exploatry analysis is done\n\n#### First check for amount of deaths between genders","a3201a9b":"#### Naive bayes","708acc48":"#### Optimal shrinkage level is 0.1","7d9dd098":"#### More people died having an ejection rate of <50%. Still alot of people survived which shows inconsistency in dataset\n\n\n#### Perform lasso regularization to shrink variables to zero and choose the most important features","86242bcb":"#### Decision trees","9756424d":"#### Naive bayes gave a good accurecy according to the score of crossvalidation. With time added as feature the accurecy is improved more, but i excluded it from dataset \n\n#### Future work: A more throughful exploatry analysis could further simply the relationships in dataset or clear up my assumptions about time and this dataset having wrong data","e8909547":"**I found out from this dataset that \"time\" feature should not be used as a predictor because it causes problem when applying different feature selection techniques. When using lasso the variables are affected by this feature, but when using correlation technique it shows that this variable is negativly correlated with target variable. Although my model gets a better prediction accurecy when using time as a predictor, i still decided to remove it**","274f6816":"#### Only the age, ejection_fraction and serum_creatinine are important here\n\n#### Split data to training and testing set and scale","aa0b0664":"#### Randomforest and 10-kfold crossvalidation","34c0c667":"#### the variance captured by pca is low, this shows that variables are not correlated with each other and the reduced dimensions dont capture the variance in dataset","242c7c0e":"#### 67 males that didnt smoke died\n\n#### 3 females that did smoke survived\n\n#### Age might be an important factor","6462d220":"#### Use pca to check for multicollinearity in dataset and reduce the dimensions to only two variables\n\n#### Scale data, create a copy of df and perform pca","7b42fe76":"#### More people died in lower spectrums, this is a bit strange. Dataset could be wrong\n\n#### Even if we look at the probability distribution then it would show that you have a higher probability to die the younger you are....","415f6a3f":"#### Light gradient boosting machine","e2151a37":"#### Amount of deaths due to smoking and its distribution among genders"}}