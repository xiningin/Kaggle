{"cell_type":{"4efde776":"code","1190cd59":"code","baaa8ad6":"code","17537b70":"code","8ff1e23a":"code","000a4d0b":"code","b7bc6c81":"code","288f3740":"code","2db4c636":"markdown","8ef4fdcf":"markdown","e3e8d84d":"markdown","d93c4f6e":"markdown","61bf234c":"markdown","85ec8d52":"markdown","22beaf6e":"markdown","9d9342cb":"markdown"},"source":{"4efde776":"import pandas as pd\nimport numpy as np\nimport spacy\nfrom nltk import ngrams\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics","1190cd59":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n\ntrain_df.head()","baaa8ad6":"print(train_df['target'].value_counts())","17537b70":"train_df['TextLenght'] = train_df['text'].apply(lambda x: len(x))\ntrain_df['TextLenght'].plot.hist()","8ff1e23a":"nlp = spacy.load('en_core_web_sm')\n\n#Generate spacy docs for each tweet.\ntrain_df['Spacy doc'] = [nlp(text.lower()) for text in train_df[\"text\"]]\n#New column of tokens without stopwords, punctuation and URLs\ntrain_df['Filtered Tokens'] = train_df['Spacy doc'].apply(lambda doc: [token for token in doc if not token.is_stop and not token.is_punct and not token.like_url])\n# Generate lemmatized strings\ntrain_df['Lemma Strings'] = train_df['Filtered Tokens'].apply(lambda tokenArray: [token.lemma_ for token in tokenArray])\n# Generate lemmatized text with the lemmatized strings\ntrain_df['Lemma Text'] = train_df['Lemma Strings'].apply(lambda x: ' '.join(x))\n\nvectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,3))\ntfidf_model = vectorizer.fit(train_df['Lemma Text'])\n\ntrain_tfidf = tfidf_model.transform(train_df['Lemma Text'])\n\n#Generate spacy docs for each tweet.\ntest_df['Spacy doc'] = [nlp(text.lower()) for text in test_df[\"text\"]]\n#New column of tokens without stopwords, punctuation and URLs\ntest_df['Filtered Tokens'] = test_df['Spacy doc'].apply(lambda doc: [token for token in doc if not token.is_stop and not token.is_punct and not token.like_url])\n# Generate lemmatized strings\ntest_df['Lemma Strings'] = test_df['Filtered Tokens'].apply(lambda tokenArray: [token.lemma_ for token in tokenArray])\n# Generate lemmatized text with the lemmatized strings\ntest_df['Lemma Text'] = test_df['Lemma Strings'].apply(lambda x: ' '.join(x))\n\ntest_tfidf = tfidf_model.transform(test_df['Lemma Text'])","000a4d0b":"NBClassifier = MultinomialNB()\nNBClassifier.fit(train_tfidf, train_df['target'])","b7bc6c81":"test_df['predictions'] = NBClassifier.predict(test_tfidf)","288f3740":"submission_df = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsubmission_df['target'] = test_df['predictions']\nsubmission_df.to_csv('submission.csv', index=False)","2db4c636":"# What about text lenght?","8ef4fdcf":"# Let's generate those juicy attributes for both sets","e3e8d84d":"# Read the CSV files","d93c4f6e":"# And to be done with it...","61bf234c":"# Import Stuff","85ec8d52":"# Let's train that bad boy, A.K.A Naive Bayes","22beaf6e":"# Time to predict stuff","9d9342cb":"# Let's check the target count"}}