{"cell_type":{"c38ebc48":"code","4acbfeb1":"code","970e1c06":"code","c76cfc5b":"code","53484252":"code","84f259de":"code","2f50e91b":"code","ac58add8":"code","d3d1f1c6":"code","757ba08a":"code","6b466603":"code","bf9aafaa":"code","161a5f26":"code","adf2ea66":"code","522c9a7f":"code","a59b421d":"code","b822cb8f":"code","d9eb1929":"code","883b34dd":"code","5916d55e":"code","fb6303b2":"code","32cc7f07":"code","ed47eb12":"markdown","9b81768f":"markdown","342d42f5":"markdown","b88d9480":"markdown","99dfb031":"markdown","9fa2654f":"markdown"},"source":{"c38ebc48":"import time\nimport datetime\nstart_time = time.time()\nprint(datetime.datetime.now())","4acbfeb1":"### Google Colab\n\nimport sys\n\nif 'google.colab' in sys.modules:\n    !nvidia-smi\n    !cat \/proc\/meminfo | head -2\n    from google.colab import drive\n    mount_dir = '\/content\/drive'\n    drive.mount(mount_dir)\n    TRAIN_CSV = '.\/train.csv'\n    !test -e '.\/setup_flag' || pip install git+https:\/\/github.com\/huggingface\/transformers.git\n    !test -e '.\/setup_flag' || pip install sentencepiece\n    #!test -e '.\/setup_flag' || pip install -U git+https:\/\/github.com\/ildoonet\/pytorch-gradual-warmup-lr.git\n    !test -e '.\/setup_flag' || unzip -o \"\/content\/drive\/MyDrive\/Python\/kaggle\/Shopee\/input\/shopee-product-matching.zip\" -d . > \/dev\/null\n    !touch setup_flag\nelse:\n    TRAIN_CSV = '..\/input\/shopee-product-matching\/train.csv'","970e1c06":"import os\nimport gc\nimport math\nimport random\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.neighbors import NearestNeighbors\n\nimport torch\nfrom torch import nn \nimport torch.nn.functional as F \nfrom transformers import AutoTokenizer, AutoModel\n\nimport warnings\nwarnings.filterwarnings('ignore')","c76cfc5b":"class CFG:\n    compute_cv = True  # set False to train model for submission\n\n    ### BERT\n    if 'kaggle_web_client' in sys.modules:  # for kaggle notebook\n        bert_model_name = '..\/input\/bertmodel\/paraphrase-xlm-r-multilingual-v1'\n        #bert_model_name = '..\/input\/bertmodel\/distilbert-base-indonesian'\n    else:\n        bert_model_name = 'sentence-transformers\/paraphrase-xlm-r-multilingual-v1'\n        #bert_model_name = 'cahya\/distilbert-base-indonesian'\n    max_length = 128\n\n    ### ArcFace\n    scale = 30\n    margin = 0.5\n    fc_dim = 768\n    seed = 412\n    classes = 11014\n    \n    ### Training\n    n_splits = 4  # GroupKFold(n_splits)\n    batch_size = 16\n    accum_iter = 1  # 1 if use_sam = True\n    epochs = 7\n    min_save_epoch = epochs \/\/ 3\n    use_sam = True  # SAM (Sharpness-Aware Minimization for Efficiently Improving Generalization)\n    use_amp = True  # Automatic Mixed Precision\n    num_workers = 2  # On Windows, set 0 or export train_fn and TitleDataset as .py files for faster training.\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    print(device)\n    \n    ### NearestNeighbors\n    bert_knn = 50\n    bert_knn_threshold = 0.4  # Cosine distance threshold\n    \n    ### GradualWarmupSchedulerV2\uff08lr_start -> lr_max -> lr_min\uff09\n    scheduler_params = {\n        \"lr_start\": 7.5e-6,\n        \"lr_max\": 1e-4,\n        \"lr_min\": 2.74e-5, # 1.5e-5,\n    }\n    multiplier = scheduler_params['lr_max'] \/ scheduler_params['lr_start']\n    eta_min = scheduler_params['lr_min']  # last minimum learning rate\n    freeze_epo = 0\n    warmup_epo = 2\n    cosine_epo = epochs - freeze_epo - warmup_epo\n    \n    ### save_model_path\n    if 'google.colab' in sys.modules:  # for Google Colab\n        save_model_path = f\"{mount_dir}\/MyDrive\/Python\/kaggle\/Shopee\/output\/{bert_model_name.rsplit('\/', 1)[-1]}_epoch{epochs}-bs{batch_size}x{accum_iter}.pt\"\n    elif 'kaggle_web_client' in sys.modules:  # for kaggle notebook\n        save_model_path = f\".\/{bert_model_name.rsplit('\/', 1)[-1]}_epoch{epochs}-bs{batch_size}x{accum_iter}.pt\"\n    else:  # for local PC\n        save_model_path = f\"..\/input\/shopee-arcface-models\/{bert_model_name.rsplit('\/', 1)[-1]}_epoch{epochs}-bs{batch_size}x{accum_iter}.pt\"","53484252":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True # set True to be faster\n\nseed_everything(CFG.seed)","84f259de":"### Dataset\n\nclass TitleDataset(torch.utils.data.Dataset):\n    def __init__(self, df, text_column, label_column):\n        texts = df[text_column]\n        self.labels = df[label_column].values\n        \n        self.titles = []\n        for title in texts:\n            title = title.encode('utf-8').decode(\"unicode_escape\")\n            title = title.encode('ascii', 'ignore').decode(\"unicode_escape\")\n            title = title.lower()\n            self.titles.append(title)\n\n    def __len__(self):\n        return len(self.titles)\n\n    def __getitem__(self, idx):\n        text = self.titles[idx]\n        label = torch.tensor(self.labels[idx])\n        return text, label","2f50e91b":"### SAM Optimizer 2020\/1\/16\n# https:\/\/github.com\/davda54\/sam\/blob\/main\/sam.py\n\nclass SAM(torch.optim.Optimizer):\n    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n\n        defaults = dict(rho=rho, **kwargs)\n        super(SAM, self).__init__(params, defaults)\n\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n        self.param_groups = self.base_optimizer.param_groups\n\n    @torch.no_grad()\n    def first_step(self, zero_grad=False):\n        grad_norm = self._grad_norm()\n        for group in self.param_groups:\n            scale = group[\"rho\"] \/ (grad_norm + 1e-12)\n\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                e_w = p.grad * scale.to(p)\n                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n                self.state[p][\"e_w\"] = e_w\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def second_step(self, zero_grad=False):\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n\n        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n\n        self.first_step(zero_grad=True)\n        closure()\n        self.second_step()\n\n    def _grad_norm(self):\n        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n        norm = torch.norm(\n                    torch.stack([\n                        p.grad.norm(p=2).to(shared_device)\n                        for group in self.param_groups for p in group[\"params\"]\n                        if p.grad is not None\n                    ]),\n                    p=2\n               )\n        return norm","ac58add8":"### GradualWarmupScheduler\n# https:\/\/github.com\/ildoonet\/pytorch-gradual-warmup-lr\n\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n\nclass GradualWarmupScheduler(_LRScheduler):\n    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n        total_epoch: target learning rate is reached at total_epoch, gradually\n        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n    \"\"\"\n\n    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n        self.multiplier = multiplier\n        if self.multiplier < 1.:\n            raise ValueError('multiplier should be greater thant or equal to 1.')\n        self.total_epoch = total_epoch\n        self.after_scheduler = after_scheduler\n        self.finished = False\n        super(GradualWarmupScheduler, self).__init__(optimizer)\n\n    def get_lr(self):\n        if self.last_epoch > self.total_epoch:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_last_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch) \/ self.total_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * self.last_epoch \/ self.total_epoch + 1.) for base_lr in self.base_lrs]\n\n    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n        if epoch is None:\n            epoch = self.last_epoch + 1\n        self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n        if self.last_epoch <= self.total_epoch:\n            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch \/ self.total_epoch + 1.) for base_lr in self.base_lrs]\n            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n                param_group['lr'] = lr\n        else:\n            if epoch is None:\n                self.after_scheduler.step(metrics, None)\n            else:\n                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n\n    def step(self, epoch=None, metrics=None):\n        if type(self.after_scheduler) != ReduceLROnPlateau:\n            if self.finished and self.after_scheduler:\n                if epoch is None:\n                    self.after_scheduler.step(None)\n                else:\n                    self.after_scheduler.step(epoch - self.total_epoch)\n                self._last_lr = self.after_scheduler.get_last_lr()\n            else:\n                return super(GradualWarmupScheduler, self).step(epoch)\n        else:\n            self.step_ReduceLROnPlateau(metrics, epoch)","d3d1f1c6":"### GradualWarmupSchedulerV2\n\nclass GradualWarmupSchedulerV2(GradualWarmupScheduler):\n    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n    def get_lr(self):\n        if self.last_epoch > self.total_epoch:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch) \/ self.total_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * self.last_epoch \/ self.total_epoch + 1.) for base_lr in self.base_lrs]","757ba08a":"### Train one epoch\n\ndef train_fn(model, data_loader, optimizer, scheduler, use_sam, accum_iter, epoch, device, use_amp):\n    model.train()\n    if use_amp:\n        scaler = torch.cuda.amp.GradScaler()\n    fin_loss = 0.0\n    tk = tqdm(data_loader, desc = \"Training epoch: \" + str(epoch+1), ncols=100)\n\n    for t, (texts, labels) in enumerate(tk):\n        texts = list(texts)\n\n        if use_sam:\n            if use_amp:\n                with torch.cuda.amp.autocast():\n                    _, loss = model(texts, labels)\n                loss.mean().backward()\n                optimizer.first_step(zero_grad=True)\n                fin_loss += loss.item() \n                with torch.cuda.amp.autocast():\n                     _, loss_second = model(texts, labels)\n                loss_second.mean().backward()\n                optimizer.second_step(zero_grad=True)\n                optimizer.zero_grad()\n            else:\n                _, loss = model(texts, labels)\n                loss.mean().backward()\n                optimizer.first_step(zero_grad=True)\n                fin_loss += loss.item() \n                _, loss_second = model(texts, labels)\n                loss_second.mean().backward()\n                optimizer.second_step(zero_grad=True)\n                optimizer.zero_grad()\n\n        else:  # if use_sam == False\n            if use_amp:\n                with torch.cuda.amp.autocast():\n                    _, loss = model(texts, labels)\n                scaler.scale(loss).backward()\n                fin_loss += loss.item() \n                # mini-batch accumulation\n                if (t + 1) % accum_iter == 0:\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n            else:\n                _, loss = model(texts, labels)\n                loss.backward()\n                fin_loss += loss.item() \n                # mini-batch accumulation\n                if (t + 1) % accum_iter == 0:\n                    optimizer.step() \n                    optimizer.zero_grad()\n                \n        tk.set_postfix({'loss' : '%.6f' %float(fin_loss\/(t+1)), 'LR' : optimizer.param_groups[0]['lr']})\n\n    scheduler.step()\n    return model, fin_loss \/ len(data_loader)","6b466603":"### Validation\n\ndef getMetric(col):\n    def f1score(row):\n        n = len(np.intersect1d(row.target, row[col]))\n        return 2 * n \/ (len(row.target) + len(row[col]))\n    return f1score\n\n\ndef get_bert_embeddings(df, column, model, chunk=32):\n    model.eval()\n    \n    bert_embeddings = torch.zeros((df.shape[0], 768)).to(CFG.device)\n    for i in tqdm(list(range(0, df.shape[0], chunk)) + [df.shape[0]-chunk], desc=\"get_bert_embeddings\", ncols=80):\n        titles = []\n        for title in df[column][i : i + chunk].values:\n            try:\n                title = title.encode('utf-8').decode(\"unicode_escape\")\n                title = title.encode('ascii', 'ignore').decode(\"unicode_escape\")\n            except:\n                pass\n            #title = text_punctuation(title)\n            title = title.lower()\n            titles.append(title)\n            \n        with torch.no_grad():\n            if CFG.use_amp:\n                with torch.cuda.amp.autocast():\n                    model_output = model(titles)\n            else:\n                model_output = model(titles)\n            \n        bert_embeddings[i : i + chunk] = model_output\n    \n    del model, titles, model_output\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return bert_embeddings\n\n\ndef get_neighbors(df, embeddings, knn=50, threshold=0.0):\n\n    model = NearestNeighbors(n_neighbors=knn, metric='cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    preds = []\n    for k in range(embeddings.shape[0]):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[ids].values\n        preds.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return preds","bf9aafaa":"### ArcFace\nclass ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n        \n        self.criterion = nn.CrossEntropyLoss()\n                \n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        if CFG.use_amp:\n            cosine = F.linear(F.normalize(input), F.normalize(self.weight)).float()  # if CFG.use_amp\n        else:\n            cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.out_features\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n        return output, self.criterion(output,label)","161a5f26":"### BERT\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings \/ sum_mask\n\n\nclass ShopeeBertModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.bert_model_name,\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True\n    ):\n\n        super(ShopeeBertModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.backbone = AutoModel.from_pretrained(model_name).to(CFG.device)\n\n        in_features = 768\n        self.use_fc = use_fc\n        \n        if use_fc:\n            self.dropout = nn.Dropout(p=0.0)\n            self.classifier = nn.Linear(in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            in_features = fc_dim\n            \n        self.final = ArcMarginProduct(\n            in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.classifier.weight)\n        nn.init.constant_(self.classifier.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, texts, labels=torch.tensor([0])):\n        features = self.extract_features(texts)\n        if self.training:\n            logits = self.final(features, labels.to(CFG.device))\n            return logits\n        else:\n            return features\n        \n    def extract_features(self, texts):\n        encoding = self.tokenizer(texts, padding=True, truncation=True,\n                             max_length=CFG.max_length, return_tensors='pt').to(CFG.device)\n        input_ids = encoding['input_ids']\n        attention_mask = encoding['attention_mask']\n        embedding = self.backbone(input_ids, attention_mask=attention_mask)\n        x = mean_pooling(embedding, attention_mask)\n        \n        if self.use_fc and self.training:\n            x = self.dropout(x)\n            x = self.classifier(x)\n            x = self.bn(x)\n        \n        return x","adf2ea66":"### Create Dataloader\n\nprint(\"Compute CV =\", CFG.compute_cv)\n\ndf = pd.read_csv(TRAIN_CSV)\ndf['target'] = df.label_group.map(df.groupby('label_group').posting_id.agg('unique').to_dict())\n\ngkf = GroupKFold(n_splits=CFG.n_splits)\ndf['fold'] = -1\nfor i, (train_idx, valid_idx) in enumerate(gkf.split(X=df, groups=df['label_group'])):\n    df.loc[valid_idx, 'fold'] = i\n\nlabelencoder= LabelEncoder()\ndf['label_group'] = labelencoder.fit_transform(df['label_group'])\n\nif CFG.compute_cv:\n    fold = 0\n    train_df = df[df['fold']!=fold].reset_index(drop=True)\n    valid_df = df[df['fold']==fold].reset_index(drop=True)\n    print(\"train_df length =\", len(train_df))\n    print(\"train_df classes =\", len(train_df['label_group'].unique()))\n    print(\"valid_df length =\", len(valid_df))\n    print(\"valid_df classes =\", len(valid_df['label_group'].unique()))\nelse:\n    train_df = df.copy()\n    valid_df = pd.DataFrame()\n    print(\"train_df length =\", len(train_df))\n    print(\"train_df classes =\", len(train_df['label_group'].unique()))\n\ntrain_dataset = TitleDataset(train_df, 'title', 'label_group')\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size = CFG.batch_size,\n    num_workers = CFG.num_workers,\n    pin_memory = True,\n    shuffle = True,\n    drop_last = True\n)\n\nif CFG.compute_cv:\n    valid_dataset = TitleDataset(valid_df, 'title', 'label_group')\n    valid_dataloader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size = CFG.batch_size,\n        num_workers = CFG.num_workers,\n        pin_memory = True,\n        shuffle = False,\n        drop_last = False\n    )","522c9a7f":"### Create Model\n\nmodel = ShopeeBertModel()\nmodel.to(CFG.device);","a59b421d":"### Create Optimizer\n\noptimizer_grouped_parameters = [\n    {'params': model.backbone.parameters(), 'lr': CFG.scheduler_params['lr_start']},\n    {'params': model.classifier.parameters(), 'lr': CFG.scheduler_params['lr_start'] * 2},\n    {'params': model.bn.parameters(), 'lr': CFG.scheduler_params['lr_start'] * 2},\n    {'params': model.final.parameters(), 'lr': CFG.scheduler_params['lr_start'] * 2},\n]\n\nif CFG.use_sam:\n    from transformers import AdamW\n    optimizer = AdamW\n    optimizer = SAM(optimizer_grouped_parameters, optimizer)\n\nelse:\n    from transformers import AdamW\n    optimizer = AdamW(optimizer_grouped_parameters)\n\nprint(\"lr_start\")\nprint(\"-\" * 30)\nfor i in range(len(optimizer.param_groups)):\n    print('Parameter Group ' + str(i) + ' :', optimizer.param_groups[i][\"lr\"])","b822cb8f":"### Create Scheduler\n\nscheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.cosine_epo-2, eta_min=CFG.eta_min, last_epoch=-1)\nscheduler = GradualWarmupSchedulerV2(optimizer, multiplier=CFG.multiplier, total_epoch=CFG.warmup_epo,\n                                     after_scheduler=scheduler_cosine)","d9eb1929":"print(\"Training epochs =\", CFG.epochs)","883b34dd":"max_f1_valid = 0.\n\nfor epoch in range(CFG.epochs):\n    model, avg_loss_train = train_fn(model, train_dataloader, optimizer, scheduler,\n                                     CFG.use_sam, CFG.accum_iter, epoch, CFG.device, CFG.use_amp)\n\n    if CFG.compute_cv:\n        valid_embeddings = get_bert_embeddings(valid_df, 'title', model)\n        valid_predictions = get_neighbors(valid_df, valid_embeddings.detach().cpu().numpy(),\n                                          knn=CFG.bert_knn if len(df) > 3 else 3, threshold=CFG.bert_knn_threshold)\n        \n        valid_df['oof'] = valid_predictions\n        valid_df['f1'] = valid_df.apply(getMetric('oof'), axis=1)\n        valid_f1 = valid_df.f1.mean()\n        print('Valid f1 score =', valid_f1)\n\n        if (epoch >= CFG.min_save_epoch) and (valid_f1 > max_f1_valid):\n            print(f\"[{datetime.datetime.now()}] Valid f1 score improved. Saving model weights to {CFG.save_model_path}\")\n            max_f1_valid = valid_f1\n            torch.save(model.state_dict(), CFG.save_model_path)\n\n    else:\n        # no validation, save weights after last epoch\n        if (epoch == CFG.epochs - 1):\n            print(f\"[{datetime.datetime.now()}] Saving model weights to {CFG.save_model_path}\")\n            torch.save(model.state_dict(), CFG.save_model_path)","5916d55e":"if CFG.compute_cv:\n    \n    print(\"Searching best threshold...\")\n    \n    search_space = np.arange(30, 50, 1)\n\n    model.load_state_dict(torch.load(CFG.save_model_path, map_location=CFG.device))\n    valid_embeddings = get_bert_embeddings(valid_df, 'title', model)\n\n    best_f1_valid = 0.\n    best_threshold = 0.\n\n    for i in search_space:\n        threshold = i \/ 100\n        valid_predictions = get_neighbors(valid_df, valid_embeddings.detach().cpu().numpy(),\n                                          knn=CFG.bert_knn if len(df) > 3 else 3, threshold=threshold)\n\n        valid_df['oof'] = valid_predictions\n        valid_df['f1'] = valid_df.apply(getMetric('oof'), axis=1)\n        valid_f1 = valid_df.f1.mean()\n        print(f\"threshold = {threshold} -> f1 score = {valid_f1}\")\n\n        if (valid_f1 > best_f1_valid):\n            best_f1_valid = valid_f1\n            best_threshold = threshold\n\n    print(\"Best threshold =\", best_threshold)\n    print(\"Best f1 score =\", best_f1_valid)\n    BEST_THRESHOLD = best_threshold","fb6303b2":"if CFG.compute_cv:\n    \n    print(\"Searching best knn...\")\n\n    search_space = np.arange(40, 80, 2)\n\n    best_f1_valid = 0.\n    best_knn = 0\n\n    for knn in search_space:\n\n        valid_predictions = get_neighbors(valid_df, valid_embeddings.detach().cpu().numpy(),\n                                          knn=knn, threshold=BEST_THRESHOLD)\n\n        valid_df['oof'] = valid_predictions\n        valid_df['f1'] = valid_df.apply(getMetric('oof'), axis=1)\n        valid_f1 = valid_df.f1.mean()\n        print(f\"knn = {knn} -> f1 score = {valid_f1}\")\n\n        if (valid_f1 > best_f1_valid):\n            best_f1_valid = valid_f1\n            best_knn = knn\n\n    print(\"Best knn =\", best_knn)\n    print(\"Best f1 score =\", best_f1_valid)","32cc7f07":"time_elapsed = time.time() - start_time\nprint('Elapsed time: {:.0f} min {:.0f} sec'.format(time_elapsed \/\/ 60, time_elapsed % 60))\nprint(datetime.datetime.now())","ed47eb12":"End","9b81768f":"# Classes and Functions","342d42f5":"# Training and Validation","b88d9480":"# Best threshold Search","99dfb031":"# Shopee Training BERT","9fa2654f":"# Setup"}}