{"cell_type":{"4b8e753e":"code","117977bb":"code","7fb15fa2":"code","88b21cf8":"code","0c69c37f":"code","0ea651c7":"code","6335b67c":"code","848d88d3":"code","785428fd":"code","b8411692":"code","70f642cb":"code","b936ebb9":"code","9c540b04":"code","960b4b0a":"code","67ef1280":"code","1565bd9e":"code","c208e533":"code","84ebbc95":"code","f34692d9":"code","668d57db":"code","3caaf83a":"code","e400f840":"code","fa285115":"code","bc63cbda":"code","bbfc84e1":"code","357cedc4":"markdown","4e592e5a":"markdown","520f7351":"markdown","97d59ae8":"markdown","9101c839":"markdown","6e40c93d":"markdown"},"source":{"4b8e753e":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nimport optuna","117977bb":"train = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")\ntest = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")","7fb15fa2":"train.shape, test.shape","88b21cf8":"train.head()","0c69c37f":"train.isnull().sum()","0ea651c7":"test.head()","6335b67c":"train['Pawpularity'].hist()","848d88d3":"from sklearn.preprocessing import PowerTransformer\n\npower = PowerTransformer(method='box-cox')\nx = power.fit_transform(train[['Pawpularity']]).flatten()\npd.Series(x).hist()","785428fd":"# 1. A picture with both eyes and face would be considered as cuter\ntrain['eye_face'] = train['Eyes'] * train['Face']\ntest['eye_face'] = test['Eyes'] * test['Face']","b8411692":"# 2. A picture in which pets in a group feels to be nereby would be considered as cuter\ntrain['near_group'] = train['Near'] * train['Group']\ntest['near_group'] = test['Near'] * test['Group']","70f642cb":"X_train = train.drop(['Id', 'Pawpularity'], axis=1)\ny_train = train['Pawpularity']\ntest_id = test['Id']\ntest = test.drop(['Id'], axis=1)\n\n# transform y_train\npower = PowerTransformer()\ny_trans = pd.Series(power.fit_transform(train[['Pawpularity']]).flatten())","b936ebb9":"X_train","9c540b04":"cat_features = list(X_train.columns)","960b4b0a":"lr = LinearRegression()\nenet = ElasticNet()\nrf = RandomForestRegressor()\nada = AdaBoostRegressor()\ngbr = GradientBoostingRegressor()\nlgbm = LGBMRegressor()\nxgb = XGBRegressor()\ncat = CatBoostRegressor()","67ef1280":"models = [lr, enet, rf, ada, gbr, lgbm, xgb, cat]\nfold = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor model in models:\n    scores = []\n    name = model.__class__.__name__\n    scores = cross_val_score(model, X=X_train, y=y_train, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n    mse = (-1) * np.mean(scores)\n    \n    print('Model %s - RMSE: %.4f'%(name, np.sqrt(mse)))","1565bd9e":"# LGBM, Ada, GBR\n## ElasticNet Optimization\nfrom optuna.samplers import TPESampler\n\ndef objective(trial):\n    param = {\n        'alpha': trial.suggest_loguniform(\"alpha\", 0.5, 2.0),\n        'l1_ratio': trial.suggest_uniform('l1_ratio', 0.0, 1.0),\n        'max_iter': trial.suggest_int('max_iter', 500, 2000),\n        'random_state': 42\n    }\n    \n    model = ElasticNet(**param)\n    scores = cross_val_score(model, X=X_train, y=y_train, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n    mse = (-1) * np.mean(scores)\n    rmse = np.sqrt(mse)\n    \n    return rmse\n\nenet_study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\nenet_study.optimize(objective, n_trials=200)\n\nenet_best = enet_study.best_trial\nenet_best_params = enet_best.params\nprint('score: {0}, params: {1}'.format(enet_best.value, enet_best_params))","c208e533":"## Gradient Boosting\ndef objective(trial):\n    param = {\n      'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 3, 13),\n      'max_depth': trial.suggest_int('max_depth', 3, 13),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-07, 0.5),\n      'n_estimators': trial.suggest_int('n_estimators', 100, 4000),\n      'min_samples_split': trial.suggest_int('min_samples_split', 2, 13),\n      'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 13),\n      'random_state': 42\n    }\n    \n    model = GradientBoostingRegressor(**param)\n    scores = cross_val_score(model, X=X_train, y=y_train, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n    mse = (-1) * np.mean(scores)\n    rmse = np.sqrt(mse)\n    \n    return rmse\n\ngbc_study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\ngbc_study.optimize(objective, n_trials=200)\n\ngbc_best = gbc_study.best_trial\ngbc_best_params = gbc_best.params\nprint('score: {0}, params: {1}'.format(gbc_best.value, gbc_best_params))","84ebbc95":"## LGBM Boosting\ndef objective(trial):\n    param = {\n      'objective': 'regression',\n      'n_jobs': -1,\n      'num_leaves': trial.suggest_int('num_leaves', 15, 45),\n      'max_depth': trial.suggest_int('max_depth', 3, 15),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-07, 0.5),\n      'n_estimators': trial.suggest_int('n_estimators', 300, 4000),\n      'min_child_samples': trial.suggest_int('min_child_samples', 2, 20),\n      'random_state': 42\n  }\n\n    model = LGBMRegressor(**param)\n    scores = cross_val_score(model, X=X_train, y=y_train, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n    mse = (-1) * np.mean(scores)\n    rmse = np.sqrt(mse)\n    \n    return rmse\n\nlgbm_study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\nlgbm_study.optimize(objective, n_trials=200)\n\nlgbm_best = lgbm_study.best_trial\nlgbm_best_params = lgbm_best.params\nprint('score: {0}, params: {1}'.format(lgbm_best.value, lgbm_best_params))","f34692d9":"enet_final = ElasticNet(**enet_best_params)\ngbr_final = GradientBoostingRegressor(**gbc_best_params)\nlgbm_final = LGBMRegressor(**lgbm_best_params)","668d57db":"# ElasticNet\nscores = []\nname = enet_final.__class__.__name__\nfor train_idx, val_idx in fold.split(X_train):\n    train_x, val_x = X_train.loc[train_idx, :], X_train.loc[val_idx]\n    train_y, val_y = y_train.loc[train_idx], y_train.loc[val_idx]\n\n    enet_final.fit(train_x, train_y)\n\n    val_pred = enet_final.predict(val_x)\n    rmse = np.sqrt(mean_squared_error(val_y, val_pred))\n    scores.append(rmse)\n\nprint('%s - RMSE: %.4f' % (name, np.mean(scores)))","3caaf83a":"# Gradient Boosting\nscores = []\nname = gbr_final.__class__.__name__\nfor train_idx, val_idx in fold.split(X_train):\n    train_x, val_x = X_train.loc[train_idx, :], X_train.loc[val_idx]\n    train_y, val_y = y_train.loc[train_idx], y_train.loc[val_idx]\n\n    gbr_final.fit(train_x, train_y)\n\n    val_pred = gbr_final.predict(val_x)\n    rmse = np.sqrt(mean_squared_error(val_y, val_pred))\n    scores.append(rmse)\n\nprint('%s - RMSE: %.4f' % (name, np.mean(scores)))","e400f840":"# LGBM\nscores = []\nname = lgbm_final.__class__.__name__\nfor train_idx, val_idx in fold.split(X_train):\n    train_x, val_x = X_train.loc[train_idx, :], X_train.loc[val_idx]\n    train_y, val_y = y_train.loc[train_idx], y_train.loc[val_idx]\n\n    lgbm_final.fit(train_x, train_y)\n\n    val_pred = lgbm_final.predict(val_x)\n    rmse = np.sqrt(mean_squared_error(val_y, val_pred))\n    scores.append(rmse)\n\nprint('%s - RMSE: %.4f' % (name, np.mean(scores)))","fa285115":"enet_pred = enet_final.predict(test)\ngbr_pred = gbr_final.predict(test)\nlgbm_pred = lgbm_final.predict(test)\n\nlgbm_pred","bc63cbda":"# gbr_pred_df = np.reshape(gbr_pred, (gbr_pred.shape[0], 1))\n# new_gbr_pred = power.inverse_transform(gbr_pred_df).flatten()\n\n# lgbm_pred_df = np.reshape(lgbm_pred, (lgbm_pred.shape[0], 1))\n# new_lgbm_pred = power.inverse_transform(lgbm_pred_df).flatten()","bbfc84e1":"sub = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/sample_submission.csv\")\n\nsub['Pawpularity'] = np.round(lgbm_pred, 0).astype(np.uint8)\nsub.to_csv(\"submission.csv\", index=False)\nsub","357cedc4":"# Data Preparation and Model Selection","4e592e5a":"# Optimization","520f7351":"# EDA","97d59ae8":"# Distribution of the target variable","9101c839":"# Feature Engineering","6e40c93d":"# 1. Import data and library"}}