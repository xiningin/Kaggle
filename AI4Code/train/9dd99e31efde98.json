{"cell_type":{"8b087e6b":"code","7c71508e":"code","7b64a907":"code","061f75f0":"code","cb600d96":"code","1c5752b6":"code","77c4620d":"code","2096e502":"code","ae287e2d":"code","301a1385":"code","0b917c98":"code","9ab8291f":"code","426d6314":"code","865f1423":"code","7c65d661":"code","46a4c3fd":"code","16f8c605":"code","c13aaf64":"code","12baae6c":"code","de4e5b92":"code","d2b22d91":"code","8cf9ce08":"code","0fd98f0a":"code","ce793f4c":"code","7662bf8c":"code","8943c23e":"code","9b530ad4":"code","cfd893ec":"code","6d233927":"code","50ccc535":"code","f4631e67":"code","f62c920a":"code","b3a1b306":"code","bba5aa8d":"code","0deb21ff":"code","2ea40beb":"code","ef624765":"code","8d42b3fc":"code","f0840790":"code","bd001cfa":"code","000b756b":"code","b353f70c":"code","b52aab53":"code","8cd856ad":"code","d67b7e3d":"code","6f4469e6":"code","22769eca":"code","f5ee5e91":"code","dd49a1b7":"code","22b797af":"code","cfa0f999":"code","801aa7ff":"code","8a0ad6be":"code","27c1f464":"code","b60d05ed":"code","f2193a1c":"code","1340dd16":"code","113add4a":"code","212bf615":"code","4a7bc8f9":"code","3ed6a604":"markdown","8cb39776":"markdown","87584dd7":"markdown","cac2e8ed":"markdown","97dc6ad5":"markdown","b012a6c5":"markdown","a18f2fc8":"markdown","5f4b1741":"markdown","04c1c043":"markdown","2f1e24ff":"markdown","97d563dc":"markdown","604b31b8":"markdown","aaf779c0":"markdown"},"source":{"8b087e6b":"# Importing Important Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder,  KBinsDiscretizer\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.pipeline import Pipeline","7c71508e":"Train = pd.read_csv('\/kaggle\/input\/black-friday-sales\/Black_Friday_sale\/train.csv')\nTest = pd.read_csv('\/kaggle\/input\/black-friday-sales\/Black_Friday_sale\/test.csv')\nSub = pd.read_csv('\/kaggle\/input\/black-friday-sales\/Black_Friday_sale\/sample_submission_V9Inaty.csv')","7b64a907":"Train.head(10)","061f75f0":"Test.head(10)","cb600d96":"sns.pairplot(Train)","1c5752b6":"Train.info()","77c4620d":"# Numerical Columns\nnum = []\na = Train.describe()\nfor i in a:\n    i = i\n    num.append(i)\nprint('List of Numerical Columns \\n',num)","2096e502":"# Categorical Columns\ncat = []\na = Train.describe(include='O')\nfor i in a:\n    i = i\n    cat.append(i)\nprint('List of Categorical Columns \\n',cat)","ae287e2d":"Train.skew()","301a1385":"for i in Train.columns:\n    a = Train[i].isnull().sum()\n    if a > 0:\n        print('This {} column has '.format(i),a,' null values')\n        \n# It has lot of Null Values so dropping them is not a Solution , so lets do first basic feature engineering","0b917c98":"# Train['Product_Category_2'].dropna()\n# Train['Product_Category_3'].dropna()","9ab8291f":"gender_dict = {'F':0, 'M':1}\nage_dict = {'0-17':0, '18-25':1, '26-35':2, '36-45':3, '46-50':4, '51-55':5, '55+':6}\ncity_dict = {'A':0, 'B':1, 'C':2}\nstay_dict = {'0':0, '1':1, '2':2, '3':3, '4+':4}\n \nTrain[\"Gender\"] = Train[\"Gender\"].apply(lambda x: gender_dict[x])\nTest[\"Gender\"] = Test[\"Gender\"].apply(lambda x: gender_dict[x])\n \nTrain[\"Age\"] = Train[\"Age\"].apply(lambda x: age_dict[x])\nTest[\"Age\"] = Test[\"Age\"].apply(lambda x: age_dict[x])\n \nTrain[\"City_Category\"] = Train[\"City_Category\"].apply(lambda x: city_dict[x])\nTest[\"City_Category\"] = Test[\"City_Category\"].apply(lambda x: city_dict[x])\n \nTrain[\"Stay_In_Current_City_Years\"] = Train[\"Stay_In_Current_City_Years\"].apply(lambda x: stay_dict[x])\nTest[\"Stay_In_Current_City_Years\"] = Test[\"Stay_In_Current_City_Years\"].apply(lambda x: stay_dict[x])","426d6314":"\ncolumns_list = [\"Product_ID\"]\nfor var in columns_list:\n    lb = LabelEncoder()\n    full_var_data = pd.concat((Train[var],Test[var]),axis=0).astype('str')\n    temp = lb.fit_transform(np.array(full_var_data))\n    Train[var] = lb.transform(np.array( Train[var] ).astype('str'))\n    Test[var] = lb.transform(np.array( Test[var] ).astype('str'))","865f1423":"# Data after basic Feature enigneering\nTrain.head(10)","7c65d661":"X = Train.drop(columns='Purchase')\ny = Train['Purchase']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","46a4c3fd":"# evaluate each strategy on the dataset\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nresults = list()\nstrategies = ['mean', 'median', 'most_frequent', 'constant']\nfor s in strategies:\n# create the modelin g pipeline\n    pipeline = Pipeline(steps=[('i', SimpleImputer(strategy=s)), ('m', RandomForestRegressor())])\n    # evaluate the model\n    pipeline.fit(X_train, y_train)\n    scores = pipeline.score(X_test, y_test)\n    # store results\n    results.append(scores)\n    print('>%s %.3f' % (s, np.mean(scores)))","16f8c605":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=10, random_state=0)\npipeline = Pipeline(steps=[('i', imp), ('m', RandomForestRegressor())])\npipeline.fit(X_train, y_train)\nscores = pipeline.score(X_test, y_test)\nprint('%.3f' % (scores))","c13aaf64":"a = X_train.fillna(-999)\nb = X_test.fillna(-999)\npipeline = Pipeline(steps=[('m', RandomForestRegressor())])\npipeline.fit(a, y_train)\nX_test.fillna(-999)\nscores = pipeline.score(b, y_test)\nprint('%.3f' % (scores))","12baae6c":"dtr = RandomForestRegressor()\ndtr.fit(a,y_train)\ny_pred = dtr.predict(b)\n\nmse = mean_squared_error(y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\nr2 = r2_score(y_test, y_pred)\nprint(\"R2 Score:\", r2)","de4e5b92":"dtr = xgb.XGBRegressor()\ndtr.fit(a,y_train)\ny_pred = dtr.predict(b)\n\nmse = mean_squared_error(y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\nr2 = r2_score(y_test, y_pred)\nprint(\"R2 Score:\", r2)","d2b22d91":"feature_important = dtr.feature_importances_\nfeature_important\ntotal = sum(feature_important)\nnew = [value * 100. \/ total for value in feature_important]\nnew = np.round(new,2)\nkeys = list(X_train.columns)\nfeature_importances = pd.DataFrame()\nfeature_importances['Features'] = keys\nfeature_importances['Importance (%)'] = new\nfeature_importances = feature_importances.sort_values(['Importance (%)'],ascending=False).reset_index(drop=True)\nfeature_importances","8cf9ce08":"Train['User_ID']","0fd98f0a":"Train['Product_Category_2'] = Train['Product_Category_2'].fillna(-999)\nTrain['Product_Category_3'] = Train['Product_Category_3'].fillna(-999)\nTest['Product_Category_2'] = Test['Product_Category_2'].fillna(-999)\nTest['Product_Category_3'] = Test['Product_Category_3'].fillna(-999)","ce793f4c":"Train[\"User_ID_MeanPrice\"]  = Train.groupby(['User_ID'])['Purchase'].transform('mean')\nTrain","7662bf8c":"userID_mean_dict = Train.groupby(['User_ID'])['Purchase'].mean().to_dict()\n# for i, j in userID_mean_dict.items():\n#     Test[\"User_ID_MeanPrice\"] = Test[\"User_ID_MeanPrice\"].replace(i, j) #test['User_ID'].apply(lambda x:userID_mean_dict.get(x,0))\n","8943c23e":"Train[\"Product_ID_MeanPrice\"]  = Train.groupby(['Product_ID'])['Purchase'].transform('mean')\nTrain","9b530ad4":"# userID_mean_dict = Train.groupby(['Product_ID'])['Purchase'].mean().to_dict()\n# Test['Product_ID_MeanPrice'] = Test['Product_ID']\n# for i, j in userID_mean_dict.items():\n#     Test[\"Product_ID_MeanPrice\"] = Test[\"Product_ID_MeanPrice\"].replace(i, j) #test['User_ID'].apply(lambda x:userID_mean_dict.get(x,0))\n","cfd893ec":"X = Train.drop(columns='Purchase')\ny = Train['Purchase']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","6d233927":"dtr = RandomForestRegressor()\ndtr.fit(X_train,y_train)\ny_pred = dtr.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\nr2 = r2_score(y_test, y_pred)\nprint(\"R2 Score:\", r2)\n\n# Earlier RMSE Was RMSE Error: 2756.2351610303654\n#                    R2 Score: 0.6985656174193418","50ccc535":"feature_important = dtr.feature_importances_\nfeature_important\ntotal = sum(feature_important)\nnew = [value * 100. \/ total for value in feature_important]\nnew = np.round(new,2)\nkeys = list(X_train.columns)\nfeature_importances = pd.DataFrame()\nfeature_importances['Features'] = keys\nfeature_importances['Importance (%)'] = new\nfeature_importances = feature_importances.sort_values(['Importance (%)'],ascending=False).reset_index(drop=True)\nfeature_importances","f4631e67":"\nTrain[\"User_ID_MinPrice\"] = Train.groupby(['User_ID'])['Purchase'].transform('min')\nTrain[\"User_ID_MaxPrice\"] = Train.groupby(['User_ID'])['Purchase'].transform('max')\nTrain[\"Product_ID_MinPrice\"] = Train.groupby(['Product_ID'])['Purchase'].transform('min')\nTrain[\"Product_ID_MaxPrice\"] = Train.groupby(['Product_ID'])['Purchase'].transform('max')","f62c920a":"X = Train.drop(columns='Purchase')\ny = Train['Purchase']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","b3a1b306":"dtr = RandomForestRegressor()\ndtr.fit(X_train,y_train)\ny_pred = dtr.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\nr2 = r2_score(y_test, y_pred)\nprint(\"R2 Score:\", r2)\n\n# Earlier RMSE Was RMSE Error: 2535.0195136408065\n#                           R2 Score: 0.7450101673389087","bba5aa8d":"feature_important = dtr.feature_importances_\nfeature_important\ntotal = sum(feature_important)\nnew = [value * 100. \/ total for value in feature_important]\nnew = np.round(new,2)\nkeys = list(X_train.columns)\nfeature_importances = pd.DataFrame()\nfeature_importances['Features'] = keys\nfeature_importances['Importance (%)'] = new\nfeature_importances = feature_importances.sort_values(['Importance (%)'],ascending=False).reset_index(drop=True)\nfeature_importances","0deb21ff":"Train[\"Product_Cat1_MaxPrice\"] = Train.groupby(['Product_Category_1'])['Purchase'].transform('max')\nTrain[\"Product_Cat1_MeanPrice\"] = Train.groupby(['Product_Category_1'])['Purchase'].transform('mean')\nTrain[\"Age_Count\"] = Train.groupby(['Age'])['Age'].transform('count')\nTrain[\"Occupation_Count\"] = Train.groupby(['Occupation'])['Occupation'].transform('count')\nTrain[\"Product_Category_1_Count\"] = Train.groupby(['Product_Category_1'])['Product_Category_1'].transform('count')\nTrain[\"Product_Category_2_Count\"] = Train.groupby(['Product_Category_2'])['Product_Category_2'].transform('count')\nTrain[\"Product_Category_3_Count\"] = Train.groupby(['Product_Category_3'])['Product_Category_3'].transform('count')\nTrain[\"User_ID_Count\"] = Train.groupby(['User_ID'])['User_ID'].transform('count')\nTrain[\"Product_ID_Count\"] = Train.groupby(['Product_ID'])['Product_ID'].transform('count')\nTrain[\"Occupation__Mean_Price\"]  = Train.groupby(['Occupation'])['Purchase'].transform('mean')","2ea40beb":"Train","ef624765":"X = Train.drop(columns='Purchase')\ny = Train['Purchase']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","8d42b3fc":"dtr = RandomForestRegressor()\ndtr.fit(X_train,y_train)\ny_pred = dtr.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\nr2 = r2_score(y_test, y_pred)\nprint(\"R2 Score:\", r2)\n\n# Earlier RMSE Was RMSE Error: 2535.0195136408065\n#                           R2 Score: 0.7450101673389087","f0840790":"\ndtr =  xgb.XGBRegressor()\ndtr.fit(X_train,y_train)\ny_pred = dtr.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\nr2 = r2_score(y_test, y_pred)\nprint(\"R2 Score:\", r2)   ","bd001cfa":"feature_important = dtr.feature_importances_\nfeature_important\ntotal = sum(feature_important)\nnew = [value * 100. \/ total for value in feature_important]\nnew = np.round(new,2)\nkeys = list(X_train.columns)\nfeature_importances = pd.DataFrame()\nfeature_importances['Features'] = keys\nfeature_importances['Importance (%)'] = new\nfeature_importances = feature_importances.sort_values(['Importance (%)'],ascending=False).reset_index(drop=True)\nfeature_importances","000b756b":"Train[\"Stay_In_Current_City_Years_Mean_price\"]  = Train.groupby(['Stay_In_Current_City_Years'])['Purchase'].transform('mean')\n\n# Another Method of Advance Feature engineering\n# Bin the ID column and add as feature\nest = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans')\nTrain[\"User_ID_ID_Bin\"] = est.fit_transform(np.reshape(Train[\"User_ID\"].values, (-1,1)))\nTrain[\"Product_ID_ID_Bin\"] = est.fit_transform(np.reshape(Train[\"Product_ID\"].values, (-1,1)))","b353f70c":"Train","b52aab53":"X = Train.drop(columns='Purchase')\ny = Train['Purchase']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","8cd856ad":"dtr = RandomForestRegressor()\ndtr.fit(X_train,y_train)\ny_pred = dtr.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\nr2 = r2_score(y_test, y_pred)\nprint(\"R2 Score:\", r2)\n\n# Earlier result\n    # RMSE Error: 2480.985464001923\n    # R2 Score: 0.7557645559747941","d67b7e3d":"\ndtr =  xgb.XGBRegressor()\ndtr.fit(X_train,y_train)\ny_pred = dtr.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\nr2 = r2_score(y_test, y_pred)\nprint(\"R2 Score:\", r2)   ","6f4469e6":"Train[\"User_ID_Product_mean\"] = Train.groupby(['User_ID'])['Product_ID_MeanPrice'].transform('mean')","22769eca":"Train[\"User_ID_Product_max\"] = Train.groupby(['User_ID'])['Product_ID_MeanPrice'].transform('max')","f5ee5e91":"feature_important = dtr.feature_importances_\nfeature_important\ntotal = sum(feature_important)\nnew = [value * 100. \/ total for value in feature_important]\nnew = np.round(new,2)\nkeys = list(X_train.columns)\nfeature_importances = pd.DataFrame()\nfeature_importances['Features'] = keys\nfeature_importances['Importance (%)'] = new\nfeature_importances = feature_importances.sort_values(['Importance (%)'],ascending=False).reset_index(drop=True)\nfeature_importances","dd49a1b7":"a = Train.drop(columns=['User_ID_ID_Bin','Product_ID_ID_Bin','Product_ID_MinPrice','Product_Category_3_Count','Product_Category_2'])","22b797af":"cor = a.corr()\nplt.figure(figsize=(20,15))\nsns.heatmap(cor<-0.9, annot=True)\n","cfa0f999":"X = a.drop(columns='Purchase')\ny = a['Purchase']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","801aa7ff":"dtr =  xgb.XGBRegressor()\ndtr.fit(X_train,y_train)\ny_pred = dtr.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\nr2 = r2_score(y_test, y_pred)\nprint(\"R2 Score:\", r2)  \n\n#RMSE Error: 2409.370507700712\n#R2 Score: 0.7696610247065712\n\n# RMSE Error: 2407.6520541081927\n# R2 Score: 0.7699894803583995","8a0ad6be":"from xgboost import plot_importance\n\nplot_importance(dtr)","27c1f464":"test_model = xgb.XGBRegressor(\n            eta = 0.03,\n            n_estimators = 1500 \n)\n#model.fit(X_train, y_train)\ntest_model.fit(X_train, y_train, eval_metric='rmse', \n          eval_set=[(X_test, y_test)], early_stopping_rounds=500, verbose=100)","b60d05ed":"dtr =  xgb.XGBRegressor(\n                eta = 0.03,\n                n_estimators=400,\n                max_depth=8,\n                min_child_weight=0.9\n)\ndtr.fit(X_train,y_train)\ny_pred = dtr.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\nr2 = r2_score(y_test, y_pred)\nprint(\"R2 Score:\", r2) \n\n# RMSE Error: 2407.9961756684083\n# R2 Score: 0.7699237256450062\n# RMSE Error: 2406.639751382179\n# R2 Score: 0.7701828565786376\n# 2393","f2193a1c":"from sklearn.model_selection import GridSearchCV\ntest_model = xgb.XGBRegressor(\n                        eta = 0.03,\n#                         max_depth=5,\n                        min_child_weight=1,\n                        gamma=0,\n                        subsample=0.8,\n                        colsample_bytree=0.8,\n                        seed=27,\n                        n_estimators = 600\n)\n\nparam_grids = {\n           'max_depth':[3,5,7,9,10]\n#  'min_child_weight':[1,2,3,5]\n}\n\ngrid = GridSearchCV(estimator=test_model,\n            \n                    param_grid=param_grids, n_jobs=-1)\n\ngrid.fit(X_train, y_train)","1340dd16":"grid.best_params_","113add4a":"test_model = xgb.XGBRegressor(\n            max_depth=5,\n                        min_child_weight=1,\n                        gamma=0,\n                     eta = 0.03,\n            n_estimators = 1000 ,\n                  subsample=0.8,\n                        colsample_bytree=0.8,\n                        seed=27\n)\n#model.fit(X_train, y_train)\ntest_model.fit(X_train, y_train, eval_metric='rmse', \n          eval_set=[(X_test, y_test)], early_stopping_rounds=500, verbose=100)","212bf615":"test_model = xgb.XGBRegressor(\n            max_depth=8,\n                        min_child_weight=1,\n                        gamma=5,\n                     eta = 0.3,\n            n_estimators = 1000 ,\n                  subsample=0.8,\n                        colsample_bytree=0.8,\n                        seed=27\n)\n#model.fit(X_train, y_train)\ntest_model.fit(X_train, y_train, eval_metric='rmse', \n          eval_set=[(X_test, y_test)], early_stopping_rounds=500, verbose=100)","4a7bc8f9":"test_model = xgb.XGBRegressor(\n            max_depth=6,\n                        min_child_weight=1,\n                        gamma=0,\n                     eta = 0.3,\n            n_estimators = 1000 ,\n                  subsample=0.8,\n                        colsample_bytree=0.8,\n                        seed=27\n)\n#model.fit(X_train, y_train)\ntest_model.fit(X_train, y_train, eval_metric='rmse', \n          eval_set=[(X_test, y_test)], early_stopping_rounds=500, verbose=100)","3ed6a604":"## Step 5-: Basic Feature Engineering","8cb39776":"## Step 8 -: Hypertuning Our Base Model","87584dd7":"# Step 1-:  Reading Data","cac2e8ed":"# Step 3-: Checking basic Data information","97dc6ad5":"## Steps which we will follow\n\n1. Reading Data\n2. Checking numerical and Categorical Data\n3. Checking basic Data information\n4. Checking for Null VAlues and Dealing with them\n5. Basic Feature Engineering\n6. Applying BaseLine Model\n7. Advance Feature Engineering\n8. Hypertuning Our Base Model\n9. Finalizing Predicition.","b012a6c5":"## Step 7 -: Advance Feature Engineering","a18f2fc8":"# How to Ace Data Science Hackathons.","5f4b1741":"## Now lets Deal with Null Values","04c1c043":"**Here we used simple imputer that consider mean, mean....**","2f1e24ff":"## Step -:6 Applying BaseLine Models","97d563dc":"# Step 2-: Checking numerical and Categorical Data","604b31b8":"### Lets check Feature importance","aaf779c0":"# Step 4-: Checking for Null Values and Dealing with them"}}