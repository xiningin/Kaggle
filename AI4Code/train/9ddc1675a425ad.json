{"cell_type":{"1c6b5950":"code","a2795839":"code","76e03f12":"code","8fe6969a":"code","0f29799a":"code","d0c17c6c":"code","9f93157c":"code","3cf02f00":"code","1ef30539":"code","01d94baa":"code","d541fff2":"code","50a007a2":"code","e413b359":"code","152af7df":"code","06874488":"code","aeda98bb":"code","253abd1c":"code","061b3986":"code","69746e31":"code","b534f9d8":"code","46b2a23e":"code","fe88b904":"code","60de3e35":"code","81eec2af":"code","c2d55be6":"code","e4ef6774":"code","9c334c3c":"code","32469776":"code","6c7f8fa0":"code","d9730031":"code","1d92060d":"code","b608ad5d":"code","2fd4d1b5":"code","bf16cc2a":"code","d88f3816":"code","79913da7":"code","3c927b20":"code","bd7631da":"markdown","2c15ddf1":"markdown","f32a9e6f":"markdown","29ea7b28":"markdown","0adbf2b8":"markdown","2ffa5388":"markdown","ad9f2ef9":"markdown","44759928":"markdown","81e9e6e3":"markdown","b03669e5":"markdown","5cf71f1f":"markdown","a3e1d3dd":"markdown","11a4160e":"markdown","42cfc39a":"markdown","cc1ea5e9":"markdown","dc77263c":"markdown","cfbbd7f2":"markdown","873dd244":"markdown","1d6d0670":"markdown","2ad207b0":"markdown","503cf515":"markdown"},"source":{"1c6b5950":"# Let's import all the necessary libraries for this project\n\nimport requests\nimport numpy as np\nimport pandas as pd\nimport json\nfrom datetime import datetime\nimport plotly.express as px\nimport textblob\nfrom textblob.classifiers import NaiveBayesClassifier\nimport nltk\nimport sklearn\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns","a2795839":"df_final = pd.read_csv('..\/input\/covid19-reddit-comments\/covid19_comments.csv')\ndf_final.head()","76e03f12":"df_final.isnull().values.any()","8fe6969a":"df_final.groupby('score')['score'].count().sort_values(ascending=False)","0f29799a":"def clean_negatives(dataset):\n    for i in range(-34, 0):\n        dataset.replace(to_replace=i, value=1, inplace=True)\n    return dataset\n\ndf_final['score'] = clean_negatives(df_final['score'])","d0c17c6c":"def groupbydate(dataset):\n    newdataset = pd.DataFrame()\n    dates = []\n    sums = []\n    dataset['date'] = pd.to_datetime(dataset['date']).dt.date\n    for date in dataset['date']:\n        if date not in dates:\n            dates.append(date)\n            sums.append((dataset['date'] == date).sum())\n    newdataset['date'] = dates\n    newdataset['comments'] = sums\n    return newdataset\n\nprogression = groupbydate(df_final)\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=progression['date'], y=progression['comments']))\n\nfig.update_layout(title='Number of comments about COVID-19 on Reddit per day',\n                   xaxis_title='Date',\n                   yaxis_title='Number of Comments')\n\nfig.show()","9f93157c":"df_final['month'] = pd.DatetimeIndex(df_final['date']).month\ndf_final.groupby('month')['month'].count().sort_values(ascending=False)","3cf02f00":"january_comments = df_final.loc[df_final['month'] == 1]\njanuary_comments.to_csv(path_or_buf='january_comments.csv')\n\nfebruary_comments = df_final.loc[df_final['month'] == 2]\nfebruary_comments.to_csv(path_or_buf='february_comments.csv')\n\nmarch_comments = df_final.loc[df_final['month'] == 3]\nmarch_comments.to_csv(path_or_buf='march_comments.csv')\n\napril_comments = df_final.loc[df_final['month'] == 4]\napril_comments.to_csv(path_or_buf='april_comments.csv')","1ef30539":"fig = px.scatter(january_comments, x=\"date\", \n           y=\"sentiment_polarity\",\n           hover_data=[\"author\", \"permalink\", \"preview\"], \n           color_discrete_sequence=[\"green\", \"red\"],\n           color=\"sentiment\", \n           size=\"score\", \n           size_max=50,\n           labels={\"sentiment_polarity\": \"Comment positivity\", \"date\": \"Date comment was posted\"}, \n           title=f\"Positivity of Reddit Comments about COVID-19 on January 2020\")\nfig.update_layout(\n    autosize=False,\n    width=800,\n    height = 800,\n    plot_bgcolor = 'white'\n)\nfig.show()","01d94baa":"fig = px.scatter(february_comments, x=\"date\", \n           y=\"sentiment_polarity\",\n           hover_data=[\"author\", \"permalink\", \"preview\"],\n           color_discrete_sequence=[\"green\", \"red\"], \n           color=\"sentiment\", \n           size=\"score\", \n           size_max=50,\n           labels={\"sentiment_polarity\": \"Comment positivity\", \"date\": \"Date comment was posted\"}, \n           title=f\"Positivity of Reddit Comments about COVID-19 on February 2020\",\n          )\nfig.update_layout(\n    autosize=False,\n    width=800,\n    height = 800,\n    plot_bgcolor = 'white'\n)\nfig.show()","d541fff2":"fig = px.scatter(march_comments, x=\"date\",\n           y=\"sentiment_polarity\",\n           hover_data=[\"author\", \"permalink\", \"preview\"], \n           color_discrete_sequence=[\"green\", \"red\"], \n           color=\"sentiment\", \n           size=\"score\", \n           size_max=50,\n           labels={\"sentiment_polarity\": \"Comment positivity\", \"date\": \"Date comment was posted\"}, \n           title=f\"Positivity of Reddit Comments about COVID-19 on March 2020\",\n          )\nfig.update_layout(\nautosize=False,\n    width=800,\n    height = 800,\n    plot_bgcolor = 'white')\nfig.show()","50a007a2":"fig = px.scatter(april_comments, x=\"date\",\n           y=\"sentiment_polarity\",\n           hover_data=[\"author\", \"permalink\", \"preview\"], \n           color_discrete_sequence=[\"green\", \"red\"], \n           color=\"sentiment\", \n           size=\"score\", \n           size_max=50,\n           labels={\"sentiment_polarity\": \"Comment positivity\", \"date\": \"Date comment was posted\"}, \n           title=f\"Positivity of Reddit Comments about COVID-19 on April 2020\",\n          )\nfig.update_layout(\n    autosize=False,\n    width=800,\n    height = 800,\n    plot_bgcolor = 'white'\n)\nfig.show()","e413b359":"df_km = pd.DataFrame(df_final['score'])\ndf_km['sentiment_polarity'] = df_final['sentiment_polarity']\n\n# ELBOW METHOD\n\n# calculate distortion for a range of number of cluster\ndistortions = []\nfor i in range(1, 11):\n    km = KMeans(\n        n_clusters=i, init='random',\n        n_init=10, max_iter=300,\n        tol=1e-04, random_state=0\n    )\n    km.fit(df_km)\n    distortions.append(km.inertia_)\n\n# plot\nplt.plot(range(1, 11), distortions, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Distortion')\nplt.show()","152af7df":"kclusters = 4\n\nkm = KMeans(n_clusters=kclusters, init='random',n_init=10, max_iter=300, \n    tol=1e-04, random_state=0).fit(df_km)\n\nkm_labels = km.labels_\n\n# Let's change label numbers so they go from highest scores to lowest\n\nreplace_labels = {0:2, 1:0, 2:3, 3:1}\n\nfor i in range(len(km_labels)):\n    km_labels[i] = replace_labels[km_labels[i]]\n    \ndf_km['Cluster'] = km_labels\ndf_km.head()","06874488":"import matplotlib.ticker as ticker\n\nfig, axes = plt.subplots(1, kclusters, figsize=(20, 10), sharey=True)\n\naxes[0].set_ylabel('Score & Sentiment Polarity', fontsize=25)\n\nfor k in range(kclusters):\n    # We are going to set same y axis limits\n    axes[k].set_ylim(-1,500)\n    axes[k].xaxis.set_label_position('top')\n    axes[k].set_xlabel('Cluster ' + str(k), fontsize=25)\n    axes[k].tick_params(labelsize=20)\n    plt.sca(axes[k])\n    plt.xticks(rotation='vertical')\n    sns.boxplot(data = df_km[df_km['Cluster'] == k].drop('Cluster',1), ax=axes[k])\n\nplt.show()","aeda98bb":"df_final['Cluster'] = df_km['Cluster']\n\ndf_cluster1 = df_final.loc[df_final['Cluster'] == 1]\n\n#df_cluster1['date'] = df_cluster1['date'].apply(lambda x: float(x))\n\nfig = px.scatter(df_cluster1, x=\"date\",\n           y=\"sentiment_polarity\", \n           trendline = 'lowess',\n           hover_data=[\"author\", \"permalink\", \"preview\"], \n           color_discrete_sequence=[\"green\", \"red\"], \n           color=\"sentiment\", \n           size=\"score\", \n           size_max=50,\n           labels={\"sentiment_polarity\": \"Comment positivity\", \"date\": \"Date comment was posted\"}, \n           title=f\"Sentiment of Reddit's most-voted comments on COVID-19\",\n          )\nfig.update_layout(\n    autosize=False,\n    width=800,\n    height = 800,\n    plot_bgcolor = 'white'\n)\nfig.show()","253abd1c":"data = pd.read_csv('..\/input\/figure-eight-labelled-textual-dataset\/text_emotion.csv')\n\n# Let's drop unnecessary columns from our dataset\ndata = data.drop('tweet_id', axis=1)\ndata = data.drop('author', axis=1)","061b3986":"data['sentiment'].unique()","69746e31":"data = data.drop(data[data.sentiment == 'boredom'].index)\ndata = data.drop(data[data.sentiment == 'surprise'].index)\ndata = data.drop(data[data.sentiment == 'enthusiasm'].index)\ndata = data.drop(data[data.sentiment == 'empty'].index)\ndata = data.drop(data[data.sentiment == 'fun'].index)\ndata = data.drop(data[data.sentiment == 'relief'].index)\ndata = data.drop(data[data.sentiment == 'love'].index)\ndata = data.drop(data[data.sentiment == 'neutral'].index)","b534f9d8":"# We are going to merge the category 'hate' to the 'anger' category to make a data augmentation\ndata['sentiment'].replace(to_replace='hate', value='anger', inplace=True)\n# Let's replace 'worry' for 'fear'\ndata['sentiment'].replace(to_replace='worry', value='fear', inplace=True)\n# Let's see how many tweets we have for each sentiment\ndata.groupby('sentiment')['sentiment'].count().sort_values(ascending=False)","46b2a23e":"# Synonym replacement\n\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nimport random\n\ndef get_synonyms(word):\n    \"\"\"\n    Get synonyms of a word\n    \"\"\"\n    synonyms = set()\n    \n    for syn in wordnet.synsets(word): \n        for l in syn.lemmas(): \n            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n            synonyms.add(synonym) \n    \n    if word in synonyms:\n        synonyms.remove(word)\n    \n    return list(synonyms)\n\ndef synonym_replacement(words, n):\n    \n    words = words.split()\n    \n    new_words = words.copy()\n    random_word_list = list(set([word for word in words if word not in stop]))\n    random.shuffle(random_word_list)\n    num_replaced = 0\n    \n    for random_word in random_word_list:\n        synonyms = get_synonyms(random_word)\n        \n        if len(synonyms) >= 1:\n            synonym = random.choice(list(synonyms))\n            new_words = [synonym if word == random_word else word for word in new_words]\n            num_replaced += 1\n        \n        if num_replaced >= n: #only replace up to n words\n            break\n\n    sentence = ' '.join(new_words)\n\n    return sentence\n\n# Random Insertion\n\ndef random_insertion(words, n):\n    \n    words = words.split()\n    new_words = words.copy()\n    \n    for _ in range(n):\n        add_word(new_words)\n        \n    sentence = ' '.join(new_words)\n    return sentence\n\ndef add_word(new_words):\n    \n    synonyms = []\n    counter = 0\n    \n    while len(synonyms) < 1:\n        random_word = new_words[random.randint(0, len(new_words)-1)]\n        synonyms = get_synonyms(random_word)\n        counter += 1\n        if counter >= 10:\n            return\n        \n    random_synonym = synonyms[0]\n    random_idx = random.randint(0, len(new_words)-1)\n    new_words.insert(random_idx, random_synonym)","fe88b904":"anger = data.loc[data['sentiment'] == 'anger']\nnew_anger_comments = []\nfor content in anger['content']:\n    new_anger_comments.append(synonym_replacement(content, 4))\n    new_anger_comments.append(random_insertion(content, 4))\nnew_anger = pd.DataFrame()\nnew_anger['content'] = new_anger_comments\nnew_anger['sentiment'] = 'anger'\nanger = anger.append(new_anger)","60de3e35":"anger.shape","81eec2af":"data = data.append(new_anger)","c2d55be6":"#Making all letters lowercase\ndata['content'] = data['content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n\n#Removing Punctuation, Symbols\ndata['content'] = data['content'].str.replace('[^\\w\\s]',' ')\n\n#Removing urls, links\ndata['content'] = data['content'].str.replace('(www|http)\\S+', ' ')\n\n#Removing Stop Words using NLTK\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ndata['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))","e4ef6774":"#Lemmatisation\nfrom textblob import Word\ndata['content'] = data['content'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n\n#Correcting Letter Repetitions\nimport re\n\ndef de_repeat(text):\n    pattern = re.compile(r\"(.)\\1{2,}\")\n    return pattern.sub(r\"\\1\\1\", text)\n\ndata['content'] = data['content'].apply(lambda x: \" \".join(de_repeat(x) for x in x.split()))","9c334c3c":"# Code to find the top 1000 rarest words appearing in the data\nfreq = pd.Series(' '.join(data['content']).split()).value_counts()[-1000:]\n\n# Removing all those rarely appearing words from the data\nfreq = list(freq.index)\ndata['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))","32469776":"#Encoding output labels\nfrom sklearn import preprocessing\nlbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(data.sentiment.values)\n\n# Splitting into training and testing data in 90:10 ratio\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)","6c7f8fa0":"print(lbl_enc.classes_)","d9730031":"# Extracting TF-IDF parameters\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=1000, analyzer='word',ngram_range=(1,3))\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_val_tfidf = tfidf.fit_transform(X_val)","1d92060d":"# Extracting Count Vectors Parameters\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(analyzer='word')\ncount_vect.fit(data['content'])\nX_train_count =  count_vect.transform(X_train)\nX_val_count =  count_vect.transform(X_val)","b608ad5d":"from sklearn.metrics import accuracy_score\n# Model 1: Multinomial Naive Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train_tfidf, y_train)\ny_pred = nb.predict(X_val_tfidf)\nprint('naive bayes tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n\n# Model 2: Linear SVM\nfrom sklearn.linear_model import SGDClassifier\nlsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\nlsvm.fit(X_train_tfidf, y_train)\ny_pred = lsvm.predict(X_val_tfidf)\nprint('svm using tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n\n# Model 3: logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1, max_iter=21000)\nlogreg.fit(X_train_tfidf, y_train)\ny_pred = logreg.predict(X_val_tfidf)\nprint('log reg tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n\n# Model 4: Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=500)\nrf.fit(X_train_tfidf, y_train)\ny_pred = rf.predict(X_val_tfidf)\nprint('random forest tfidf accuracy %s' % accuracy_score(y_pred, y_val))","2fd4d1b5":"## Model 1: Multinomial Naive Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train_count, y_train)\ny_pred = nb.predict(X_val_count)\nprint('naive bayes count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n\n# Model 2: Linear SVM\nfrom sklearn.linear_model import SGDClassifier\nlsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\nlsvm.fit(X_train_count, y_train)\ny_pred = lsvm.predict(X_val_count)\nprint('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n\n# Model 3: Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1, max_iter=21000)\nlogreg.fit(X_train_count, y_train)\ny_pred = logreg.predict(X_val_count)\nprint('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n\n# Model 4: Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=500)\nrf.fit(X_train_count, y_train)\ny_pred = rf.predict(X_val_count)\nprint('random forest with count vectors accuracy %s' % accuracy_score(y_pred, y_val))","bf16cc2a":"comments = df_final['body']\n\n# Doing some preprocessing on these comments as done before\ncomments = comments.apply(lambda x: \" \".join(x.lower() for x in x.split()))\ncomments = comments.str.replace('[^\\w\\s]',' ')\ncomments = comments.str.replace('(www|http)\\S+', ' ')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ncomments = comments.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\nfrom textblob import Word\ncomments = comments.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ncomments = comments.apply(lambda x: \" \".join(de_repeat(x) for x in x.split()))\n# Extracting Count Vectors feature from our Reddit comments\ncomment_count = count_vect.transform(comments)\n\n#Predicting the emotion of the comment using our already trained logistic regression\ncomment_pred = lsvm.predict(comment_count)","d88f3816":"df_final['emotion'] = comment_pred.tolist()\ndf_final['emotion'].replace(to_replace=0, value='anger', inplace=True)\ndf_final['emotion'].replace(to_replace=1, value='fear', inplace=True)\ndf_final['emotion'].replace(to_replace=2, value='happiness', inplace=True)\ndf_final['emotion'].replace(to_replace=3, value='sadness', inplace=True)\ndf_final.groupby('emotion')['emotion'].count().sort_values(ascending=False)","79913da7":"# Let's group comments by emotion\nanger_comments = df_final.loc[df_final['emotion'] == 'anger']\nfear_comments = df_final.loc[df_final['emotion'] == 'fear']\nhappiness_comments = df_final.loc[df_final['emotion'] == 'happiness']\nsadness_comments = df_final.loc[df_final['emotion'] == 'sadness']\n\n# Let's find out progressions of each emotion\n\nanger_progression = groupbydate(anger_comments)\nfear_progression = groupbydate(fear_comments)\nhappiness_progression = groupbydate(happiness_comments)\nsadness_progression = groupbydate(sadness_comments)","3c927b20":"fig = go.Figure() \n\nfig.add_trace(go.Scatter(x=anger_progression[\"date\"], y=anger_progression['comments'], \n                        name= 'Anger'))\nfig.add_trace(go.Scatter(x=fear_progression[\"date\"], y=fear_progression['comments'],\n                        name = 'Fear'))\nfig.add_trace(go.Scatter(x=happiness_progression[\"date\"], y=happiness_progression['comments'],\n                        name = 'Happiness'))\nfig.add_trace(go.Scatter(x=sadness_progression[\"date\"], y=sadness_progression['comments'],\n                        name = 'Sadness'))\n\nfig.update_layout(title='Emotions towards COVID-19 on Reddit',\n                   xaxis_title='Date',\n                   yaxis_title='Number of Comments')\n\nfig.show()","bd7631da":"<a id=\"1\"><\/a>\n## Aknowledgements\n\nAs a new-comer in the field of data science I humbly observe all the work that has been done before and thank those who created it as it has helped me to learn so much and start to work on my own projects.\n\nSpecial thanks to [Duarte O.Carmo](https:\/\/pbpython.com\/interactive-dashboards.html),  whose analysis using Pushshift API has inspired this analysis, [Lorraine Li](https:\/\/towardsdatascience.com\/k-means-clustering-with-scikit-learn-6b47a369a83c) for her excellent article on K-means clustering,  [Jason Wei](https:\/\/github.com\/jasonwei20\/eda_nlp) for the implementation of data augmenting techniques, which I used to enrich the dataset used for training the classification model and [Aditya Vivek Thota](https:\/\/medium.com\/the-research-nest\/applied-machine-learning-part-3-3fd405842a18) for his great article on machine learning classification for NLP.\n\n","2c15ddf1":"Let's group comments by emotion and figure out how many have been made each day.","f32a9e6f":"The index of each label is the value given by the encoder: \n\n**anger = 0, fear = 1, happiness = 2, sadness = 3**","29ea7b28":"Let's find out the unique sentiment values in our dataset.","0adbf2b8":"Linear SMV seems to be the algorithm with highest accuracy with a 60% using count vectors. Let's use it for our Reddit data and see how many comments fit to every emotion.","2ffa5388":"'Anger' category has now tripled in size which will help to have more accuracy for the machine learning classification algorithm.","ad9f2ef9":"Here is a preview of the dataset we will use for our analysis.","44759928":"As we have now the most rated comments, we can plot a graphic to see if there have been more positive or negative comments regarding the coronavirus.","81e9e6e3":"Let's see which cluster has the comments with highest scores, regardless of their sentiment polarity.","b03669e5":"# Emotions Towards COVID-19 on Reddit\n***This is an ongoing project*** -- \n*Last update: 18th of April 2020* \n\n+400k comments have been retrieved for the analysis so this notebook might take a while to charge.\n\n### Table of contents\n\n1. [Aknowledgements](#1)\n2. [Introduction](#2)\n3. [Data](#3)\n4. [Analysis](#4)","5cf71f1f":"For the sake of simplicity, we will only keep 4 of the emotions that align with psychologist Paul Ekman's classification of basic emotions: 'happiness', 'sadness', 'anger' and 'fear' (fear equates to 'worry' in our dataset).","a3e1d3dd":"Let's replace negative values for a positive value of 1.","11a4160e":"Let's visualize the distribution of positive and negative comments in each month of 2020.","42cfc39a":"<a id=\"2\"><\/a>\n## Introduction\n\nThis is a simple analysis of the sentiment of Reddit user's comments related to the disease starting from the 31st of December 2019, when the first cases were declared in China.\n\nAs the coronavirus has been expanding all over the world, citizens of every country have given their opinion of the current situation and events. This notebook is a first approach to the question of how the people's opinions have been changing over time.\n\nReddit, a platform of social sharing, which contents and comments are voted by its community, might show some light to answer this question.","cc1ea5e9":"<a id=\"4\"><\/a>\n## Analysis\n\n### Exploratory Data Analysis\n\nLet's visualize the increase in interest among users of Reddit on COVID-19.","dc77263c":"### Clustering\n\n\nLet's use K-means clustering to clasify the comments in different clusters. First we are going to use the Elbow Method to determine the best value for *k*.","cfbbd7f2":"Elements in 'anger' category are few in comparison to other categories. Let's do a data augmentation in that category in order to balance all the categories.","873dd244":"<a id=\"3\"><\/a>\n## Data","1d6d0670":"### Emotion detection\nWe are going to analyse in more precision the emotions we can detect from the comments made regarding the coronavirus training a machine learning model.  ","2ad207b0":"As it might happen that some comments may have a negative score and this would have bad consequences in the visualization of our data, we are going to check if some values are negative within our dataframe.","503cf515":"### Data cleaning\n\nLet's check if there are any null values in our dataset"}}