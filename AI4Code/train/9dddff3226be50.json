{"cell_type":{"81d90573":"code","04cb101b":"code","1886b5f3":"code","6d46fd63":"code","75a7e0d3":"code","2a0165fd":"code","955b2059":"code","2e48515d":"code","42f65289":"code","371d627e":"code","8135973d":"markdown","a8543be2":"markdown","1179fa44":"markdown","9c8ed3ac":"markdown","ebd2ad68":"markdown","f6db298c":"markdown","bf00e5d6":"markdown","7df98f55":"markdown","624d82a0":"markdown","8e6e5c02":"markdown","7d477584":"markdown","37a57ecc":"markdown","4bc2a597":"markdown","f2f3771e":"markdown","5381a735":"markdown","3759f828":"markdown","fd743901":"markdown"},"source":{"81d90573":"import numpy as np","04cb101b":"v = np.array([[1, 2, 3]])\nv, v.shape","1886b5f3":"from numpy.linalg import norm\nnorm(v)","6d46fd63":"u = np.array([[1, 3, 5]])\nnp.arccos(np.dot(u, v.T) \/ (norm(u) * norm(v)))","75a7e0d3":"u \/ norm(u)","2a0165fd":"a, b = np.array([[1, 3, 2]]), np.array([[3, -1, 0]])\nnp.dot(a, b.T)","955b2059":"A = np.array([[1,2,3], [4,5,6], [7,8,9]])\nA, A.shape","2e48515d":"B = np.random.rand(3,5)\nC = np.dot(A, B)\nC, C.shape","42f65289":"A = np.random.randint(5, size=(3,3))\nB = np.random.randint(5, size=(3,3))\nC = np.multiply(A, B)\n\nprint(A)\nprint(B)\nprint(C)","371d627e":"from numpy.linalg import matrix_rank as rank\nrank(A)","8135973d":"**Dot Product**: The dot product between two equally-sized vectors $u$ and $v$ is denoted by $u \\cdot v$ and is equal to $\\sum u_i v_i$. Note that $u \\cdot v = v \\cdot u$. Geometrically, the dot product represents a projection of $u$ onto $v$ (and vice-versa), with the scalar result representing the distance of $u$ from the origin point.\n\n**Angle between vectors**: The angle $\\theta$ between two vectors $u$ and $v$ can be calculated as $cos(\\theta) = \\dfrac{u \\cdot v}{|u| \\cdot |v|}$. That is, the dot product of $u$ and $v$ normalized by the product of their lengths.","a8543be2":"**Singular Matrix**: A matrix that:\n\n1. Has no inverse\n2. Its determinant is 0\n3. If its dimensions are $n \\times n$, then its rank is less than $n$\n\n**Orthogonal Matrix**: A matrix whose rows and columns are orthogonal unit vectors (ie. orthonormal), $AA^T = A^T A = I$. For these matrices, the transpose is equal to their inverse, $A^T = A^{\u22121}$.\n\n**Adjugate Matrix**: A matrix $B$ after we swap elements of a matrix $A$ across its diagonal.\n\n**Conjugate of a Matrix**: Take the transpose of $A$ and replace its complex number elements by their conjugate (the conjugate of a complex number is the real number and the negative of the imaginary part). Example:\n\n$$A = \\begin{bmatrix} 1 & -2 - i & 5 \\\\ 1 + i & i & 4-2i \\end{bmatrix}$$\n\nWe first transpose the matrix:\n\n$$A^T = \\begin{bmatrix} 1 & 1 + i \\\\ -2 - i & i \\\\ 5 & 4-2i\\end{bmatrix}$$\n\nThen we conjugate every entry of the matrix:\n\n$$A^H = \\begin{bmatrix} 1 & 1 - i \\\\ -2 + i & -i \\\\ 5 & 4+2i\\end{bmatrix}$$","1179fa44":"**Unit Vector**: A vector with magnitude of $1$. Can be computed by dividing a vector with its norm.","9c8ed3ac":"**Hadamard Product**: The element-wise multiplication of matrices $A$ and $B$, resulting in $C$, where $C_{i,j}$ is equal to $A_{i,j} \\cdot B_{i,j}$. Denoted by $A \\odot B$.","ebd2ad68":"**Span**: The span of a set of vectors $V$ is a set of vectors that can be represented by the linear combination of vectors in $V$. To find the span of a vector set $V$, we use the following:\n\n$$cV = X$$\n\nand we solve for $c$ relative to $X$.\n\n**Minor**: The minor of matrix $A$ is the determinant of $A$ if we remove one or more rows and columns.","f6db298c":"## Vectors\n\nVectors are simply a list of numbers, or matrices of $1 \\times n$ dimensions.","bf00e5d6":"## Matrices\n\nMatrices are 2D lists of elements, of `row x cols` dimensions.","7df98f55":"**Linear Subspace of $R^n$**: A subset $V$ of $R^n$ is a subspace of $R^n$ if:\n\n1. $V$ contains the $0$-vector (a vector of $n$ zeros)\n2. Closure under scalar multiplication: If a vector $v \\in V$ is multiplied by a scalar $a$, $a v$ is still in $V$\n3. Closure under addition: If $u, v \\in V$, then $u + v$ is in $V$\n\nNote that a subspace of $R^n$ is the span of any vectors.\n\n**Basis**: A basis of a subspace is a set of vectors, where the vectors are linearly independent and represent\/define the span of the subspace. A basis is essentially the minimum set of linearly independent vectors that define a subspace.\n\n**Eigenvalues\/Eigenvectors**: We want to find $\\lambda$ (eigenvalue) and $v$ (eigenvector) so that $\\lambda A = \\lambda v$. The characteristic function is $(A - \\lambda I) v = 0$. We equate $det(A-\\lambda I)$ to 0 and solve for $\\lambda$. Then, for each eigenvalue we find the corresponding eigen vector from the characteristic function.\n\n**Matrix Decomposition**: We decompose $A$ into $PDP^{-1}$, where $P$ is a matrix with columns the eigenvectors and $D$ is a diagonal matrix with the eigenvalues as the values on the diagonal.","624d82a0":"**Row Echelon Form (ref)**: Matrix is said to be in *ref* if the following conditions hold:\n\n* The first non-zero element in each row, called the leading coefficient, is $1$.\n* Each leading coefficient is in a column to the right of the previous row's leading coefficient.\n* Rows with all zeros are below rows with at least one non-zero element.\n\n**Reduced Row Echelon Form (rref)**: Matrix is said to be in *rref* if the following conditions hold:\n\n* All the conditions for ref.\n* The leading coefficient in each row is the only non-zero entry in its column.\n\n**Elementary Row Operations**:\n\n* Swapping two rows\n* Multiplying a row by a non-zero number\n* Adding a multiple of one row to another row","8e6e5c02":"**Diagonal Matrix:** A matrix $A$ is diagonal if all the elements outside the diagonal are zero. The values of the diagonal are given by vector $v$, and $A$ can be written as $diag(v)$. Diagonal matrices are useful because they are efficient for multiplication and finding their inverse. To multiply a vector $x$ with $A = diag(v)$, we only need to get the Hadamard product between $v$ and $x$. Also, the inverse of $A$ is $diag(\\dfrac{1}{v_0}, ..., \\dfrac{1}{v_n})$, if every $v_i$ is non-zero.","7d477584":"**Orthonormal Vectors**: Two vectors are orthonormal if they are unit vectors and they are *orthogonal*. Two vectors are orthogonal if their dot product is equal to $0$.","37a57ecc":"**Rank**: The maximum number of linearly independent columns\/rows. Can be computed by finding the ref of the matrix and counting the non-zero rows.","4bc2a597":"**Solving Linear Equations**: $A \\cdot  X = B$\n\nOne way is to solve for $X$, where $X = B \\cdot A^{-1}$. This is computationally expensive though, since finding the inverse of a matrix is resource-intensive.\n\nUsing Gauss elimination we reduce a matrix to ref. We start by augmenting $A$ with $B$. From $[A | B]$, we convert the left-hand side to ref (using elementary row operations) and from that via a series of substitutions we can solve the linear system.\n\n**Finding Inverse of Matrix**: Gauss-Jordan elimination is a variant of Gauss elimination where we convert the matrix to rref. To find the inverse of a matrix, we do the following:\n\nWe are given matrix $A$:\n\n$$\n \\begin{bmatrix}\n\t2 & -1 &  0 \\\\\n\t-1 &  2 & -1 \\\\\n\t0 & -1 &  2\n\\end{bmatrix}\n$$\n\nWe augment the matrix with the identity matrix ($[A | I]$):\n\n$$\n\\left[\\begin{array}{rrr|rrr}\n\t2 & -1 &  0 & 1 & 0 & 0 \\\\\n\t-1 &  2 & -1 & 0 & 1 & 0 \\\\\n\t0 & -1 &  2 & 0 & 0 & 1\n\\end{array}\\right].\n$$\n\nVia elementary row operations, we convert the left-hand side to the identity matrix:\n\n$$\n \\left[\\begin{array}{rrr|rrr}\n1 & 0 & 0 & \\frac34 & \\frac12 & \\frac14 \\\\\n0 & 1 & 0 & \\frac12 &      1  & \\frac12 \\\\\n0 & 0 & 1 & \\frac14 & \\frac12 & \\frac34\n\\end{array}\\right]\n$$\n\nThe reason why this works is that the right-hand side operates as a record of the row operations (starting from the identity matrix). By converting the left-hand side from $A$ to the identity matrix, we are essentially performing matrix multiplication via the row operations. These operations though are stored in the right-hand side! Therefore, what remains on the right-hand side is the inverse of the matrix (remember, $AA^{-1} = I$).","f2f3771e":"# Linear Algebra Notes\n\nFor a set of PhD interviews I recently went through, I had to brush up on my Linear Algebra knowledge. To make life easier, I hastily compiled a list to help me with revision. I wanted to write this list down in a neat format for future reference, so this kernel came about. This serves as a quick refresher of some key concepts that I kept forgetting, using some `numpy` code here and there.\n\nI thought I would share this little project with others in case this may help, so, enjoy!","5381a735":"**Norm**: The norm of a vector is a function that assigns strictly positive values to vectors, except the zero-vector to which a value of $0$ is assigned. In the Euclidean ($R_n$) space, the norm of a vector is its length. The Euclidean norm is also called *magnitude*.\n\n**Magnitude**: The magnitude of a vector is the square root of the squared sums of its elements: $\\sqrt{v_1^2 + v_2^2 + ... + v_n^2}$","3759f828":"**Matrix Multiplication**: Multiplying matrices $A$ and $B$ results in matrix $C$, where $C_{i,j}$ is the dot product of the $i$th row of $A$ and the $j$th column of $B$. That is, $C_{i,j} = \\sum{A_{i,.} \\cdot B_{.,j}}$.","fd743901":"**Hermitian Matrix**: A complex-numbered square matrix that is equal to its own conjugate matrix.\n\n**Skew-Hermitian Matrix**: A complex-numbered square matrix that is equal to the negative of its own conjugate matrix.\n\n**Normal Matrix**: A matrix that commutes with its conjugate, $A^H$. That is, $AA^H = A^HA$.\n\n**Unitary Matrix**: A matrix whose conjugate is also its inverse: $AA^H = I$\n\n**Triangular Matrix**: All elements above diagonal are $0$ (lower triangular) or all below are $0$ (upper triangular)."}}