{"cell_type":{"33766788":"code","ca1014ae":"code","0278d131":"code","4cfbc7d8":"code","c81889d8":"code","bc2691a6":"code","79c39392":"code","6db056ca":"code","8208c199":"code","fe540282":"code","42ce5f0e":"code","19994e21":"code","194e77b7":"code","e676289a":"code","126e3db5":"code","dcaaefcc":"code","8414675e":"code","8ad53f98":"markdown","c730fdde":"markdown","9529bef9":"markdown","6ca622b3":"markdown","cec08cd2":"markdown","35fb0c87":"markdown","afe759db":"markdown","024e36e4":"markdown"},"source":{"33766788":"\n#   .o88b.  .d88b.  db    db d888888b d8888b.         db .d888b. \n#  d8P  Y8 .8P  Y8. 88    88   `88'   88  `8D        o88 88' `8D \n#  8P      88    88 Y8    8P    88    88   88         88 `V8o88' \n#  8b      88    88 `8b  d8'    88    88   88 C8888D  88    d8'  \n#  Y8b  d8 `8b  d8'  `8bd8'    .88.   88  .8D         88   d8'   \n#   `Y88P'  `Y88P'     YP    Y888888P Y8888D'         VP  d8'    \n#                                                                \n# This machine learning pipeline is a fork \n#   from a work in progress (to be published in 2021):\n#   BRHIM - Base de Registros Hospitalares \n#   para Informa\u00e7\u00f5es e Metadados - By: Vaz, Dora, Lamb e Camey\n#***************************************************************\n# The COVID-19 Supicius Cases Classifier is an analysis done \n#   with the help of a multidisciplinary team  \n#   from Hospital de Cl\u00ednicas de Porto Alegre (HCPA), \n#   that holds a permanent data science study group \n#\n#   Contact to authors   \n#\n#           mhbarbian@gmail.com , \n#           scamey@hcpa.edu.br, \n#           tvaz@hcpa.edu.br , \n#           vhirakata@hcpa.edu.br \n#\n#\n#\n#  DATA CULTURE                \/\/ ______________________________________________\n#     SCIENCE                 \/\/ _____________________________________________\n#       MULTIDISCIPLINARITY  \/\/ ____________________________________________\n#\n#=\nimport pandas as pd\nimport numpy as np \nimport seaborn as sns \n\n#from sklearn.model_selection import \n#from sklearn import metrics\n\nfrom sklearn import datasets\nfrom sklearn import preprocessing\n#from sklearn.preprocessing import LabelEncoder\n#from sklearn.preprocessing import OneHotEncoder\n#from sklearn.preprocessing import label_binarize\n\nfrom sklearn import model_selection\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\nfrom sklearn.feature_selection import RFE, VarianceThreshold\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso,  LogisticRegression, LinearRegression\nfrom sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier, GradientBoostingClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\nfrom sklearn import metrics\nfrom sklearn.metrics import (accuracy_score, brier_score_loss, precision_score, recall_score,f1_score, classification_report, confusion_matrix, roc_auc_score, log_loss)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n#from matplotlib.ticker import StrMethodFormatter\n\n\npd.options.display.max_columns = 50","ca1014ae":"df = pd.read_csv('\/kaggle\/input\/covideinsteinn598-prep\/dataset_n598.xlsx - BD_COVID_N598.tsv', \n                  encoding='utf8' ,\n                  index_col=0, \n                  sep='\\t',  lineterminator='\\r', decimal=\",\", \n                  keep_default_na=False,\n                  na_values=['-1.#IND', '1.#QNAN', '1.#IND','-1.#QNAN', '#N\/A','N\/A', '#NA', 'NA','#NULL!', 'NaN', '-NaN', 'nan', '-nan'])\ndf.reset_index(inplace=True)\ndf.rename(columns={'SARSCov2examresult':'outcome'}, inplace=True)\ndf.rename(columns={'PatientID':'id'}, inplace=True)\ndf.rename(columns={'Patientaddmitedtoregularward1yes0no':'regular'}, inplace=True)\ndf.rename(columns={'Patientaddmitedtosemiintensiveunit1yes0no':'semi'}, inplace=True)\ndf.rename(columns={'Patientaddmitedtointensivecareunit1yes0no':'icu'}, inplace=True)\ndf.columns = df.columns.str.replace(\" \", \"_\")\ndf.rename(columns={'Patientagequantile':'age'}, inplace=True)\ntarget_combo = ['outcome','regular','semi', 'icu']\nprint('Dataset shape:', df.shape)\n#print('Dataset shape:', df.dtypes)\n\n\n","0278d131":"df","4cfbc7d8":"df.describe()","c81889d8":"#declare dataframe for features\nX = df.copy()\n\n#declare dataframe for targets\n#task 1\ny= X['outcome']\n#task 2 : not implemented\ny_r= X['regular']\ny_s= X['semi']\ny_i= X['icu']\n\nprint('Quali work')\nquali = ['object']\nquali_columns = list(X.select_dtypes(include=quali).columns)\nquali_columns.remove('id')\nprint(\"Sample and feature selection done during descriptive analysis.\")\nfor col in quali_columns:\n    X[col] = X[col].astype('float64')\n    #X[col] = X[col].cat.add_categories('Unknown')\n    #X[col].fillna('Unknown', inplace =True)\nX.drop(labels='id', axis=1, inplace=True)\n\n\nprint('Quanti work')\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'uint8']\nX = X.round(decimals=12)\nprint('Cleanning ...empty and duplicates')\nX = X.drop_duplicates()\nX.dropna(axis=1, how='all', thresh=None, subset=None, inplace=True)\nprint('Cleanning ...constants')\nconstant_filter = VarianceThreshold(threshold=0)\nconstant_filter.fit(X.select_dtypes(include=numerics))\nlen(X.select_dtypes(include=numerics).columns[constant_filter.get_support()])\nconstant_columns = [column for column in X.select_dtypes(include=numerics).columns\n                    if column not in X.select_dtypes(include=numerics).columns[constant_filter.get_support()]]\nX.drop(labels=constant_columns, axis=1, inplace=True)\nprint(constant_columns)\nprint('Cleanning ...quasi-constants')\nqconstant_filter = VarianceThreshold(threshold=0.0001)\nqconstant_filter.fit(X.select_dtypes(include=numerics))\nlen(X.select_dtypes(include=numerics).columns[qconstant_filter.get_support()])\nqconstant_columns = [column for column in X.select_dtypes(include=numerics).columns\n                    if column not in X.select_dtypes(include=numerics).columns[qconstant_filter.get_support()]]\nX.drop(labels=qconstant_columns, axis=1, inplace=True)\nprint(qconstant_columns)\n\nprint('Cleanning ... correlated :off')\ncorrelated_features = set()\ncorrelation_matrix = X.select_dtypes(include=numerics).corr()\nfor i in range(len(correlation_matrix.columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > 0.99999:\n            colname = correlation_matrix.columns[i]\n            correlated_features.add(colname)\n#X.drop(labels=correlated_features, axis=1, inplace=True)\nprint(correlated_features)\n\n","bc2691a6":"sns.heatmap(X.isnull(), cbar=False)\nprint('Count missings')\nprint(X.select_dtypes(include=numerics).isnull().sum().sum())\nprint('Plot missings ... heatmap: marron are missing values')\n","79c39392":"#prepare targets for task 1\ny= X['outcome']\ndel X['outcome']\n\n#task 2 (maybe)\n#y_r= df_prepared['regular'].astype('category')\n#y_s= df_prepared['semi'].astype('category')\n#y_i= df_prepared['icu'].astype('category')\n\n#prepare datasate with meaningfull features\n\nexperiment_selection = ['age', 'Hematocrit',\n       'Hemoglobin', 'Platelets', 'Meanplateletvolume', 'RedbloodCells',\n       'Lymphocytes', 'MeancorpuscularhemoglobinconcentrationMCHC',\n       'Leukocytes', 'Basophils', 'MeancorpuscularhemoglobinMCH',\n       'Eosinophils', 'MeancorpuscularvolumeMCV', 'Monocytes',\n       'RedbloodcelldistributionwidthRDW']\n\nexperiment_imputation = ['age', 'Hematocrit',\n       'Hemoglobin', 'Platelets', 'Meanplateletvolume', 'RedbloodCells',\n       'Lymphocytes', 'MeancorpuscularhemoglobinconcentrationMCHC',\n       'Leukocytes', 'Basophils', 'MeancorpuscularhemoglobinMCH',\n       'Eosinophils', 'MeancorpuscularvolumeMCV', 'Monocytes',\n       'RedbloodcelldistributionwidthRDW', 'Neutrophils', 'Urea',\n       'ProteinaCreativamgdL', 'Creatinine', 'Potassium', 'Sodium']\n\nexperiment_complete = ['age', 'regular', 'semi', 'icu', 'Hematocrit',\n       'Hemoglobin', 'Platelets', 'Meanplateletvolume', 'RedbloodCells',\n       'Lymphocytes', 'MCHC',\n       'Leukocytes', 'Basophils', 'MeancorpuscularhemoglobinMCH',\n       'Eosinophils', 'MeancorpuscularvolumeMCV', 'Monocytes',\n       'RedbloodcelldistributionwidthRDW', 'Neutrophils', 'Urea',\n       'ProteinaCreativamgdL', 'Creatinine', 'Potassium', 'Sodium',\n       'RespiratorySyncytialVirus_n', 'InfluenzaA_n', 'InfluenzaB_n',\n       'Parainfluenza1_n', 'CoronavirusNL63_n', 'RhinovirusEnterovirus_n',\n       'CoronavirusHKU1_n', 'Parainfluenza3_n', 'Chlamydophilapneumoniae_n',\n       'Adenovirus_n', 'Parainfluenza4_n', 'Coronavirus229E_n',\n       'CoronavirusOC43_n', 'InfAH1N12009_n', 'Bordetellapertussis_n',\n       'Metapneumovirus_n', 'Parainfluenza2_n', 'InfluenzaBrapidtest_n',\n       'InfluenzaArapidtest_n', 'StreptoA_n']\n\nX_experiment = X[experiment_imputation]","6db056ca":"X_train,X_test,y_train,y_test = train_test_split(X_experiment, y , test_size=0.25,random_state=0, stratify=y)\nprint('  X_train  x_test: ', len(X_train), len(X_test))\nprint('  y_train  y_test: ', len(y_train), len(y_test))\n\n#print('Somethin missing?')\n#print(X_train.select_dtypes(include=numerics).isnull().sum().sum())\n\nprint('impute mean:')\nX_experiment =  X_experiment.select_dtypes(include=numerics).apply(lambda x: x.fillna(x.mean())) \nX_train =    X_train.select_dtypes(include=numerics).apply(lambda x: x.fillna(x.mean())) \nX_test =    X_test.select_dtypes(include=numerics).apply(lambda x: x.fillna(x.mean())) \n\nprint('Count missings: ')\nprint(X_experiment.select_dtypes(include=numerics).isnull().sum().sum())\n\n","8208c199":"sns.heatmap(X_experiment.isnull(), cbar=False)","fe540282":"# calculate the correlation matrix\ncorr = X_experiment.corr()\n\n# plot the heatmap\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns)","42ce5f0e":"#Classifier Showdown\n\n#time and details matters\nimport time\nstart_time = time.time()\n\n#select classifier candidates\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('QDA', QuadraticDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('ABC', AdaBoostClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('RFb', RandomForestClassifier(class_weight='balanced')))\nmodels.append(('GBC', GradientBoostingClassifier()))\n#models.append(('NVC', NuSVC(probability=True)))\n#models.append(('NB', GaussianNB()))\n#models.append(('SVM', SVC()))\n\nsplits = 3\n\nprint(\"AUC-ROC\")\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'roc_auc'\nfor name, model in models:\n    kfold = model_selection.StratifiedKFold(n_splits=splits)    \n    #cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    cv_results = model_selection.cross_val_score(model, X_experiment, y,  cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison (AUC-ROC)')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()\n\n\n","19994e21":"# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nprint(\"ACCURACY\")\nfor name, model in models:\n    kfold = model_selection.StratifiedKFold(n_splits=splits)    \n    #cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    cv_results = model_selection.cross_val_score(model, X_experiment, y,  cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison (ACCURACY)')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","194e77b7":"\nresults = []\nnames = []\nscoring = 'f1_weighted'\nprint(\"F1-SCORE\")\n\nfor name, model in models:\n    kfold = model_selection.StratifiedKFold(n_splits=splits)    \n    #cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    cv_results = model_selection.cross_val_score(model, X_experiment, y,  cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison (F1-SCORE)')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","e676289a":"\nresults = []\nnames = []\nscoring = 'recall_weighted'\nprint(\"SENSIBILITY\")\n\nfor name, model in models:\n    kfold = model_selection.StratifiedKFold(n_splits=splits)    \n    #cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    cv_results = model_selection.cross_val_score(model, X_experiment, y,  cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison (SENSIBILITY)')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","126e3db5":"\nresults = []\nnames = []\nscoring = 'precision_weighted'\nprint(\"PRECISION\")\n\nfor name, model in models:\n    kfold = model_selection.StratifiedKFold(n_splits=splits)    \n    #cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    cv_results = model_selection.cross_val_score(model, X_experiment, y,  cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison (PRECISION)')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","dcaaefcc":"#time and details matters\nimport time\nstart_time = time.time()\n\nfor name, model in models:\n    if name =='GBC':  clf_champ = model\n\n#FIT with TRAIN DATA \nclf_champ.fit(X_train,y_train)\n\n\n#VALIDATION OF SCORES WITH ALL DATA (AS WE DONT HAVE A VALIDATION SET)\ny_validation = y\ny_validation_prob=clf_champ.predict_proba(X_experiment)\ny_validation_pred=clf_champ.predict(X_experiment)\n\nprint(\"Validation\")\nprint('Accuracy: ', metrics.accuracy_score(y_validation, y_validation_pred))\nprint('F1-score: ',f1_score(y_validation, y_validation_pred, pos_label=1, average='weighted'))\nprint('Recall: ',recall_score(y_validation, y_validation_pred, pos_label=1, average='weighted'))\nprint('Precision: ',precision_score(y_validation, y_validation_pred, average='weighted'))\nprint('AUC-ROC: ',roc_auc_score(y_validation, y_validation_prob[:,1]))\n\nfpr, tpr, threshold = metrics.roc_curve(y_validation, y_validation_prob[:,1],pos_label=1)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n#OPTIMIZA TRESHOLD\n#sensibility = tpr = 1-fnr\n#specificity= tnr = 1-fpr\n\nbest_threshold = threshold[np.argmax(tpr + (1-fpr) -1)]  \n\ny_validation_pred_treshold = y_validation_prob[:,1] >= best_threshold\n\nprint(\"Best threshold ROC-AUC: %.2f%%\" % (round(best_threshold, 3)))     \n#print(y_validation_pred_treshold)\n\nconfusion_matrix_validation = pd.crosstab(y_validation, y_validation_pred, rownames=['Actual'], colnames=['Predicted'])\nconfusion_matrix_treshold= pd.crosstab(y_validation, y_validation_pred_treshold, rownames=['Actual'], colnames=['Predicted'])\n\n\n#plot results\nplt.rc(\"font\", size=11)\nplt.rcParams['font.family'] = \"cursive\"\nsns.set_style(\"darkgrid\")\nsns.set(color_codes=True)\nsns.set(style=\"whitegrid\", font_scale=0.8)\nsns.set_context(\"paper\")\n\n\nplt.title('Confusion Matrix')\nplt.xlabel('')\ng1 = sns.heatmap(confusion_matrix_validation, annot=True,cmap='Blues', fmt='d')\ng1.set_ylabel('Total Nbr. of Patients')\n\n\nconfusion_matrix_treshold\nplt.show()\n\n\nplt.title('Confusion Matrix (Youden index )')\nplt.xlabel('')\ng1 = sns.heatmap(confusion_matrix_treshold, annot=True,cmap='Blues', fmt='d')\ng1.set_ylabel('Total Nbr. of Patients')\nplt.show()\n\n\nprint('todo: random search and grid optimization')\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","8414675e":"#feature_importances = pd.DataFrame(clf.feature_importances_,index = X_train.columns,columns=['importance']).sort_values('importance',ascending=False)\nfeat_importances = pd.Series(clf_champ.fit(X_train,y_train).feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(21).plot(kind='barh')\n#print(feature_importances)","8ad53f98":"\n# **K-folding cross-validation **\n\nSistematic experimentation with 2, 3, 4 and 10 folds\n\n> Classifiers included in the experiment.\n> \n* models.append(('LR', LogisticRegression()))\n* models.append(('LDA', LinearDiscriminantAnalysis()))\n* models.append(('QDA', QuadraticDiscriminantAnalysis()))\n* models.append(('KNN', KNeighborsClassifier()))\n* models.append(('CART', DecisionTreeClassifier()))\n* models.append(('ABC', AdaBoostClassifier()))\n* models.append(('RF', RandomForestClassifier()))\n* models.append(('RFb', RandomForestClassifier(class_weight='balanced')))\n* models.append(('GBC', GradientBoostingClassifier()))\n\n\n> Experimet #1: 2 splits\n> \n![](http:\/\/)AUC-ROC\n![](http:\/\/)LR: 0.863867 (0.032010)\n![](http:\/\/)LDA: 0.854647 (0.033153)\n![](http:\/\/)QDA: 0.690249 (0.049012)\n![](http:\/\/)KNN: 0.723097 (0.004394)\n![](http:\/\/)CART: 0.678903 (0.008628)\n![](http:\/\/)ABC: 0.751737 (0.010757)\n![](http:\/\/)RF: 0.866745 (0.034888)\n![](http:\/\/)RFb: 0.872363 (0.024544)\n![](http:\/\/)GBC: 0.835223 (0.019244)\n\n![image.png](attachment:image.png)\n","c730fdde":"# FEARTURE IMPORTANCE AND MODEL EXPLICABILITY ","9529bef9":"# Load data (rows and columns selected from statistical analysis in SPSS) and fix some names","6ca622b3":"> > ![](http:\/\/)Experimet #2: 3 splits\n> > \n![](http:\/\/)AUC-ROC\n![](http:\/\/)LR: 0.851095 (0.046127)\n![](http:\/\/)LDA: 0.842125 (0.043182)\n![](http:\/\/)QDA: 0.793706 (0.036553)\n![](http:\/\/)KNN: 0.741218 (0.074779)\n![](http:\/\/)CART: 0.652972 (0.089412)\n![](http:\/\/)ABC: 0.720850 (0.020271)\n![](http:\/\/)RF: 0.853738 (0.059801)\n![](http:\/\/)RFb: 0.851550 (0.069130)\n**GBC: 0.870903 (0.026426)**\n![image.png](attachment:image.png)","cec08cd2":"# QUALI WORK","35fb0c87":"This is a group effort to help understand covid-19 hospitalization data. \nWe are very commited to this achievment and we are glad that Hospital Israelita Albert Einstein promoted this datathon, inspiring and showing that data is fundamental for healthcare now on.\n\nSharing is caring and we scientists are working very hard to help frontline healthcare professionals in their duty.\nAt the end, we hope that algorithms (this and others) can help our hospitals to solve real decision problemns.\n\n    \"in the spirit of solidarity\"\n\n\n**Step 1 - Fast assessment of data, using Tableau Public. Follows link to our Patient Data Explorer.**\nThis is the link, where you can navigate and insipre yourself in our analysis.\nhttps:\/\/public.tableau.com\/views\/DatathonCOVID-19ClinicalSpectrumSuspiciousCasesClassifier\/Painel1?:display_count=y&:origin=viz_share_link\n\n**Step 2 - Hard work and statistical experience applied do sub-sample the original dataset into something without (or with less) misssing values.**\n\nWe managed to find an optimal subsample using age, and blood features, for 598 patients. This is the SPSS synthax to achieve our results.\nhttps:\/\/docs.google.com\/document\/d\/1A-_GB5JGAJuDd2fdEm_YuGExjrqR946zDNqygdOFSKk\/edit?usp=sharing\n\n\n**Step 3 - Comparisson of classifiers, tools and aproaches.**\nI think that is important to document our decision process. First we used tools that we are very experienced.\nSPSS has a more traditional approach to this kind of problem with this kind of dataset, with few records and missing values. We have implemented a solution end to end in SPSS with similar results from here. But we faced the \"kaggle aproach\" to be more suited for a machine learning pipeline in python. \n\n\n**Step 4 - Machine Learning pipeline.**\nThis pipeline is a fork from our WIP (work in progress - BRHIM project) and its source code was modified to adapt to this data. But our pipeline was only half implemented here. Some sleepery points in this python implementation must be clear: there is no validation dataset, so we handled all scores without what we consider a proper machine learning pipeline for this situation. Generalization is important and the current state of this solution is poorly tested for this as we dont have VALIDATION data, and only used a 2,3,4 and 10 k-fold cross validation in all data to achieve our results and final scores.\n\n","afe759db":"# DESCRIBE DATAFRAME, EXAMPLES AND STATISTICS","024e36e4":"> ![](http:\/\/)![](http:\/\/)Experimet #3: 4 splits\n> \n\n![](http:\/\/)![](http:\/\/)AUC-ROC\n![](http:\/\/)![](http:\/\/)LR: 0.854653 (0.046991)\n![](http:\/\/)![](http:\/\/)LDA: 0.844877 (0.044586)\n![](http:\/\/)![](http:\/\/)QDA: 0.758062 (0.073650)\n![](http:\/\/)![](http:\/\/)KNN: 0.752001 (0.034744)\n![](http:\/\/)![](http:\/\/)CART: 0.668798 (0.085402)\n![](http:\/\/)![](http:\/\/)ABC: 0.771755 (0.012080)\n![](http:\/\/)![](http:\/\/)RF: 0.854028 (0.052260)\n![](http:\/\/)![](http:\/\/)**RFb: 0.877025 (0.036332)**\n![](http:\/\/)![](http:\/\/)GBC: 0.862311 (0.049466)\n![](http:\/\/)![](http:\/\/)![image.png](attachment:image.png)\n![](http:\/\/)![](http:\/\/)\n![](http:\/\/)![](http:\/\/)\n![](http:\/\/)![](http:\/\/)Experimento #4: 10 splits (warning: lr interruption)\n![](http:\/\/)![](http:\/\/)AUC-ROC\n![](http:\/\/)![](http:\/\/)LR: 0.867065 (0.078275)\n![](http:\/\/)![](http:\/\/)LDA: 0.855270 (0.081305)\n![](http:\/\/)![](http:\/\/)QDA: 0.786482 (0.091805)\n![](http:\/\/)![](http:\/\/)KNN: 0.769623 (0.106863)\n![](http:\/\/)![](http:\/\/)CART: 0.686297 (0.097079)\n![](http:\/\/)![](http:\/\/)ABC: 0.779889 (0.122759)\n![](http:\/\/)![](http:\/\/)RF: 0.862205 (0.091670)\n![](http:\/\/)![](http:\/\/)RFb: 0.884726 (0.071832)\n![](http:\/\/)![](http:\/\/)GBC: 0.853109 (0.079106)\n![](http:\/\/)![](http:\/\/)\n![](http:\/\/)![](http:\/\/)![image.png](attachment:image.png)"}}