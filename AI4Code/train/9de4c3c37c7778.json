{"cell_type":{"e28b508f":"code","6332fd3d":"code","87869130":"code","e4770b06":"code","d040b505":"code","b2dfea97":"code","4ed69951":"code","8c31f4ff":"code","26091a47":"code","12bbdbf4":"code","4c45a76d":"code","b5b2960d":"code","b184d3f5":"code","e858df91":"code","e0293cc1":"code","0712e369":"code","b1c68140":"code","fbc5d7a4":"code","4170b54f":"code","b19576c5":"code","3f47bc2e":"code","591fc4d1":"code","925d42ae":"code","1d74b0f9":"code","6fdb0e6c":"code","55ebb8e8":"code","81cfe5fd":"code","345e696f":"code","91199e19":"code","a71b7801":"code","641a5cf0":"code","3697fc10":"code","5208e0a6":"code","f104ad79":"code","7f554161":"code","0bdfc586":"code","a8da52b4":"code","cc12b796":"code","9d7f835f":"code","08b0d8b7":"code","74d22fd9":"code","bc0a478a":"code","62d04533":"code","e64384ef":"code","cb59fe86":"code","e5ed4495":"code","5b4c03d5":"code","04219aec":"code","88ccebb2":"code","fd794de7":"code","6576e0e5":"code","52da1a5e":"code","b3d0be30":"code","ffb4d6a4":"code","24f0c53d":"code","9942f9fb":"code","4394771f":"code","c74fbb76":"code","527ce258":"markdown","9e641e4c":"markdown","38b4de61":"markdown","3ee0099b":"markdown","9fcfef65":"markdown","11305467":"markdown","e754e1e8":"markdown","38d16ef3":"markdown","111e5f78":"markdown","39a39337":"markdown","0bb0e8bd":"markdown","a65c465d":"markdown","d60011ef":"markdown","dec3c5bf":"markdown","50859ae8":"markdown","92726f4e":"markdown","a5feb9f0":"markdown","4be6b198":"markdown","05d0f780":"markdown","89a0cd51":"markdown","b8e10a57":"markdown","3fd7c8ff":"markdown","6bd4c9ef":"markdown","ec6647a4":"markdown","96bc97de":"markdown","1b983c14":"markdown","ce1a133b":"markdown","bc6c0050":"markdown","cb88890d":"markdown","4d633c04":"markdown","f2cfc90c":"markdown","5b8f6e72":"markdown","f75d9758":"markdown","20ab320b":"markdown","07c16652":"markdown","7f564fe7":"markdown","4d50f55a":"markdown","12c927b5":"markdown","544317e4":"markdown","477448b7":"markdown","e18319d6":"markdown","8df5f15e":"markdown","585f711e":"markdown","d958ea19":"markdown","d2f4a0ce":"markdown","01fa970c":"markdown","08231796":"markdown","e51ed088":"markdown","ed34f85a":"markdown","4a419084":"markdown","8eda1677":"markdown","d86ccf75":"markdown","a61fff8f":"markdown","625adfbc":"markdown","c47b88a8":"markdown","56f45dfe":"markdown","211368b1":"markdown","6965dd50":"markdown","fbe2ca84":"markdown","1ba7ba3d":"markdown","bb788791":"markdown","b384a06f":"markdown","e6808762":"markdown","9e1dc0f0":"markdown","c03d4d54":"markdown","6dada652":"markdown","df6079fc":"markdown","810e4cfe":"markdown","121077b6":"markdown","a8d60ccf":"markdown","fae6f509":"markdown"},"source":{"e28b508f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n%matplotlib inline\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","6332fd3d":"blocks = pd.read_csv('\/kaggle\/input\/alldatasets\/census_block_loc.csv')\nincome_df = pd.read_csv('\/kaggle\/input\/alldatasets\/nyc_census_tracts.csv', index_col=0)\nblocks = blocks[blocks.County.isin(['Bronx','Kings','New York','Queens','Richmond'])]\nblocks['Tract'] = blocks.BlockCode \/\/ 10000 \nblocks = blocks.merge(income_df,how='left',right_index=True,left_on='Tract')\n\nblocks.info()\nblocks.Income = pd.to_numeric(blocks.Income,errors='coerce')\nblocks=blocks.dropna(how=\"any\")\n","87869130":"def convert_to_2d(lats,lons,values):\n    latmin = 40.48\n    lonmin = -74.28\n    latmax = 40.93\n    lonmax = -73.65\n    lon_vals = np.mgrid[lonmin:lonmax:200j]\n    lat_vals = np.mgrid[latmin:latmax:200j]\n    map_values = np.zeros([200,200])\n    dlat = lat_vals[1] - lat_vals[0]\n    dlon = lon_vals[1] - lon_vals[0]\n    for lat,lon,value in zip(lats,lons,values):\n        lat_idx = int(np.rint((lat - latmin) \/ dlat))\n        lon_idx = int(np.rint((lon-lonmin) \/ dlon ))        \n        if not np.isnan(value):\n            map_values[lon_idx,lat_idx] = value\n    return lat_vals,lon_vals,map_values","e4770b06":"def make_plot(data_values,title='',colors='Greens'):\n    lat_vals,lon_vals,values = convert_to_2d(blocks.Latitude,blocks.Longitude,data_values)\n    fig = plt.figure(1,figsize=[10,10])\n    limits = np.min(lon_vals),np.max(lon_vals),np.min(lat_vals),np.max(lat_vals)\n    \n    im = plt.imshow(values.T,origin='lower',cmap=colors,extent=limits)\n    plt.xlabel('Longitude [degrees]')\n    plt.ylabel('Latitude [degrees]')\n    plt.title(title)\n    plt.colorbar(im,fraction=0.035, pad=0.04)\n    \n    plt.show()\nmake_plot(blocks.IncomePerCap,colors='inferno',title='IncomePerCap')","d040b505":"crime_df = pd.read_csv('\/kaggle\/input\/alldatasets\/NYPD_Complaint_Data.csv', index_col=False)\ncrime_df=crime_df[crime_df.OFNS_DESC == \"BURGLARY\"]\ncrime_df.drop(columns = [\"SUSP_AGE_GROUP\",\"SUSP_RACE\",\"SUSP_SEX\",\"TRANSIT_DISTRICT\",\"STATION_NAME\",\"PARKS_NM\",\"HADEVELOPT\",\"HOUSING_PSA\"],inplace = True)\n\ncrime_df = crime_df[crime_df['Latitude'].notna()]\ncrime_df = crime_df[crime_df['Longitude'].notna()]\n\n\ncrime_df.head()\n\n","b2dfea97":"import folium\n\nstart = crime_df[['Latitude', 'Longitude']]\n\n\nstart.drop_duplicates(inplace=True)\n\nm_latitude = crime_df['Latitude'].mean()\nm_longitude = crime_df['Longitude'].mean()\nm = folium.Map(location=[m_latitude, m_longitude])\n\nfor i, key in start.iterrows():\n  latitude = key['Latitude']\n  longitue = key['Longitude']\n  folium.CircleMarker(\n    location=[latitude, longitue],\n    radius=4,\n    popup='Burglary',\n    color='green',\n    fill=True,\n    fill_color='green'\n    ).add_to(m)\nm","4ed69951":"from folium import plugins\n\nstationArr = start[['Latitude', 'Longitude']].values\nm = folium.Map(location=[m_latitude, m_longitude])\n\n\nplugins.HeatMap(stationArr,radius= 18).add_to(m)\nm","8c31f4ff":"crime =crime_df[\"BORO_NM\"].str.split('|', expand=True).stack().value_counts()\nprint(crime)\n\ndef Average(lst): \n    return np.sum(lst) \/ len(lst) \n\nmeancrime = Average(crime)\nprint(\"Average Crime in NYC : \",meancrime)","26091a47":"names= ['Brooklyn','Queens','Manhattan','Bronx','Staten Island']\nvalues = [crime[0],crime[1],crime[2],crime[3],crime[4]]\nplt.figure(figsize=(20, 4))\nplt.subplot(131)\nplt.bar(names,values)\nplt.subplot(132)\nax_1 = plt.pie(values,labels=names, autopct='%1.0f%%', pctdistance=0.75, labeldistance=1.2)\n\nplt.show()","12bbdbf4":"income_df = pd.read_csv('\/kaggle\/input\/alldatasets\/nyc_census_tracts.csv', index_col=0)\nincome_df = income_df[income_df['IncomePerCap'].notna()]\n\nincome_df.head()","4c45a76d":"def county(geography):\n  if \"Manhattan\" in geography:\n    return \"Manhattan\"\n  elif \"Brooklyn\" in geography:\n    return \"Brooklyn\"\n  elif \"Queens\" in geography:\n    return \"Queens\"\n  elif \"Staten Island\" in geography:\n    return \"Staten Island\"\n  elif \"Bronx\" in geography:\n    return \"Bronx\"\nincome_df[\"Borough\"]=income_df[\"Borough\"].apply(county)\nfrequency =income_df[\"Borough\"].str.split('|', expand=True).stack().value_counts()\nprint(frequency)\n","b5b2960d":"names= ['Brooklyn','Queens','Bronx','Manhattan','Staten Island']\nvalues = [frequency[0],frequency[1],frequency[2],frequency[3],frequency[4]]\nplt.figure(figsize=(20, 4))\nplt.subplot(131)\nplt.bar(names,values)\nplt.subplot(132)\nax_1 = plt.pie(values,labels=names, autopct='%1.0f%%', pctdistance=0.75, labeldistance=1.2)\n\nplt.show()","b184d3f5":"def Average(lst): \n    return np.sum(lst) \/ len(lst) \nbrooklyn=np.array(income_df[[\"IncomePerCap\"]][income_df[\"Borough\"]==\"Brooklyn\"])\nbronx=np.array(income_df[[\"IncomePerCap\"]][income_df[\"Borough\"]==\"Bronx\"])\nmanhattan=np.array(income_df[[\"IncomePerCap\"]][income_df[\"Borough\"]==\"Manhattan\"])\nqueens=np.array(income_df[[\"IncomePerCap\"]][income_df[\"Borough\"]==\"Queens\"])\nstatenisland=np.array(income_df[[\"IncomePerCap\"]][income_df[\"Borough\"]==\"Staten Island\"])\n\nprint(\"Mean of Brooklyn: \",Average(brooklyn))\nprint(\"Mean of Bronx: \",Average(bronx))\nprint(\"Mean of Manhattan: \",Average(manhattan))\nprint(\"Mean of Queens: \",Average(queens))\nprint(\"Mean of Staten Island: \",Average(statenisland))\nNycmean = (Average(brooklyn)+ Average(bronx)+Average(manhattan)+Average(queens)+Average(statenisland))\/5\nprint(\"NYC Mean : \" , Nycmean)","e858df91":"names= ['Brooklyn','Bronx','Manhattan','Queens','Staten Island']\nvalues = [Average(brooklyn),Average(bronx),Average(manhattan),Average(queens),Average(statenisland)]\nplt.figure(figsize=(20, 4))\nplt.subplot(131)\nplt.bar(names,values)\nplt.subplot(132)\nax_1 = plt.pie(values,labels=names, autopct='%1.0f%%', pctdistance=0.75, labeldistance=1.2)\n\nplt.show()","e0293cc1":"df = pd.read_csv('\/kaggle\/input\/alldatasets\/AB_NYC_2019.csv', index_col=False)\ndf = df[df['latitude'].notna()]\ndf = df[df['longitude'].notna()]\ndf.head()\n","0712e369":"def county(geography):\n  if \"Manhattan\" in geography:\n    return \"Manhattan\"\n  elif \"Brooklyn\" in geography:\n    return \"Brooklyn\"\n  elif \"Queens\" in geography:\n    return \"Queens\"\n  elif \"Staten Island\" in geography:\n    return \"Staten Island\"\n  elif \"Bronx\" in geography:\n    return \"Bronx\"\ndf[\"neighbourhood_group\"]=df[\"neighbourhood_group\"].apply(county)\nfrequency =df[\"neighbourhood_group\"].str.split('|', expand=True).stack().value_counts()\nprint(frequency)\n\ndef Average(lst): \n    return np.sum(lst) \/ len(lst) \nprice_brooklyn=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Brooklyn\"])\nprice_bronx=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Bronx\"])\nprice_manhattan=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Manhattan\"])\nprice_queens=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Queens\"])\nprice_statenisland=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Staten Island\"])\n\nprint(\"Price mean of Brooklyn: \",Average(price_brooklyn))\nprint(\"Price mean of Bronx: \",Average(price_bronx))\nprint(\"Price mean of Manhattan: \",Average(price_manhattan))\nprint(\"Price mean of Queens: \",Average(price_queens))\nprint(\"Price mean of Staten Island: \",Average(price_statenisland))\nNYCpricemean = (Average(price_brooklyn)+ Average(price_bronx)+Average(price_manhattan)+Average(price_queens)+Average(price_statenisland))\/5\nprint(\"NYC price mean : \" , NYCpricemean)\n\n","b1c68140":"names= ['Brooklyn','Queens','Manhattan','Bronx','Staten Island']\nvalues = [Average(price_brooklyn),Average(price_queens),Average(price_manhattan),Average(price_bronx),Average(price_statenisland)]\nplt.figure(figsize=(20, 4))\nplt.subplot(131)\nplt.bar(names,values)\nplt.subplot(132)\nax_1 = plt.pie(values,labels=names, autopct='%1.0f%%', pctdistance=0.75, labeldistance=1.2)\n\nplt.show()","fbc5d7a4":"start_airbnb = df[['latitude', 'longitude']]\nstart_airbnb.drop_duplicates(inplace=True)\nm_latitude = df['latitude'].mean()\nm_longitude = df['longitude'].mean()\n\nstationArr = start_airbnb[['latitude', 'longitude']].values\nm = folium.Map(location=[m_latitude, m_longitude])\n\n\nplugins.HeatMap(stationArr,radius= 15).add_to(m)\nm","4170b54f":"df= df.iloc[1:40000:40]\nprint(df.shape)\nstart_airbnb = df[['latitude', 'longitude']]\n\n\nstart_airbnb.drop_duplicates(inplace=True)\n\nm_latitude = df['latitude'].mean()\nm_longitude = df['longitude'].mean()\nm = folium.Map(location=[m_latitude, m_longitude])\n\nfor i, key in start_airbnb.iterrows():\n  latitude = key['latitude']\n  longitue = key['longitude']\n  folium.CircleMarker(\n    location=[latitude, longitue],\n    radius=4,\n    popup='AirBnB',\n    color='green',\n    fill=True,\n    fill_color='blue'\n    ).add_to(m)\nm","b19576c5":"df = pd.read_csv('\/kaggle\/input\/alldatasets\/AB_NYC_2019.csv',index_col=False)\ndf = df[df['latitude'].notna()]\ndf = df[df['longitude'].notna()]\ndf.head()","3f47bc2e":"def county(geography):\n  if \"Manhattan\" in geography:\n    return \"Manhattan\"\n  elif \"Brooklyn\" in geography:\n    return \"Brooklyn\"\n  elif \"Queens\" in geography:\n    return \"Queens\"\n  elif \"Staten Island\" in geography:\n    return \"Staten Island\"\n  elif \"Bronx\" in geography:\n    return \"Bronx\"\ndf[\"neighbourhood_group\"]=df[\"neighbourhood_group\"].apply(county)\nfrequency =df[\"neighbourhood_group\"].str.split('|', expand=True).stack().value_counts()\nprint(frequency)\n\ndef Average(lst): \n    return np.sum(lst) \/ len(lst) \nprice_brooklyn=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Brooklyn\"])\nprice_bronx=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Bronx\"])\nprice_manhattan=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Manhattan\"])\nprice_queens=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Queens\"])\nprice_statenisland=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Staten Island\"])\n\nprint(\"Price mean of Brooklyn: \",Average(price_brooklyn))\nprint(\"Price mean of Bronx: \",Average(price_bronx))\nprint(\"Price mean of Manhattan: \",Average(price_manhattan))\nprint(\"Price mean of Queens: \",Average(price_queens))\nprint(\"Price mean of Staten Island: \",Average(price_statenisland))\nNYCpricemean = (Average(price_brooklyn)+ Average(price_bronx)+Average(price_manhattan)+Average(price_queens)+Average(price_statenisland))\/5\nprint(\"NYC price mean : \" , NYCpricemean)\n","591fc4d1":"sample_mean = (price_bronx[0]+price_statenisland[0])\/2\nmean_test = NYCpricemean\n\ndf=df[df.neighbourhood_group != \"Manhattan\"]\ndf=df[df.neighbourhood_group != \"Queens\"]\ndf=df[df.neighbourhood_group != \"Brooklyn\"]\n\n\n\nsamplearray = []\nsamplearray.append(price_bronx[0])\nsamplearray.append(price_statenisland[0])\n\nn = 3\n\nS = np.std(df[\"price\"])\nt_score = (sample_mean-mean_test)\/(S\/np.sqrt(n))\nt_score = t_score\/2\nprint(t_score)\n","925d42ae":"from scipy.stats import t\n\nfig, ax = plt.subplots(1, 1)\ndf = n-1\nx = np.linspace(t.ppf(0.01, df),t.ppf(0.99, df), 100)\nfirst = t_score\nlast = max(x)\nax.plot(x, t.pdf(x, df),'k-', lw=5, alpha=0.6, label='t pdf')\nax.legend(loc='best', frameon=False)\nplt.fill_between(x, t.pdf(x,df), where=(x>=first)&(x<last), color='b')\n\nplt.show()","1d74b0f9":"from scipy import stats\n# your code\ntemp=[]\ntemp = stats.ttest_1samp(samplearray, mean_test)\nprint(temp)\n","6fdb0e6c":"df = pd.read_csv('\/kaggle\/input\/alldatasets\/AB_NYC_2019.csv', index_col=False)\ndf = df[df['latitude'].notna()]\ndf = df[df['longitude'].notna()]\ndf.head()","55ebb8e8":"def county(geography):\n  if \"Manhattan\" in geography:\n    return \"Manhattan\"\n  elif \"Brooklyn\" in geography:\n    return \"Brooklyn\"\n  elif \"Queens\" in geography:\n    return \"Queens\"\n  elif \"Staten Island\" in geography:\n    return \"Staten Island\"\n  elif \"Bronx\" in geography:\n    return \"Bronx\"\ndf[\"neighbourhood_group\"]=df[\"neighbourhood_group\"].apply(county)\nfrequency =df[\"neighbourhood_group\"].str.split('|', expand=True).stack().value_counts()\nprint(frequency)\n\ndef Average(lst): \n    return np.sum(lst) \/ len(lst) \nprice_brooklyn=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Brooklyn\"])\nprice_bronx=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Bronx\"])\nprice_manhattan=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Manhattan\"])\nprice_queens=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Queens\"])\nprice_statenisland=np.array(df[[\"price\"]][df[\"neighbourhood_group\"]==\"Staten Island\"])\n\nprint(\"Price mean of Brooklyn: \",Average(price_brooklyn))\nprint(\"Price mean of Bronx: \",Average(price_bronx))\nprint(\"Price mean of Manhattan: \",Average(price_manhattan))\nprint(\"Price mean of Queens: \",Average(price_queens))\nprint(\"Price mean of Staten Island: \",Average(price_statenisland))\nNYCpricemean = (Average(price_brooklyn)+ Average(price_bronx)+Average(price_manhattan)+Average(price_queens)+Average(price_statenisland))\/5\nprint(\"NYC price mean : \" , NYCpricemean)\n","81cfe5fd":"samp_mean = (price_brooklyn[0]+price_queens[0]+price_bronx[0]+price_statenisland[0])\/4\n\nmean_te = NYCpricemean\n\n\ndf=df[df.neighbourhood_group != \"Manhattan\"]\n\n\nsamp = []\nsamp.append(price_brooklyn[0])\nsamp.append(price_bronx[0])\nsamp.append(price_queens[0])\nsamp.append(price_statenisland[0])\n\nn = 3\n\nSt = np.std(df[\"price\"])\n\nt_value = (samp_mean-mean_te)\/(St\/np.sqrt(n))\nt_value = t_value\/2\nprint(t_value)\n","345e696f":"from scipy.stats import t\nfig, ax = plt.subplots(1, 1)\ndf = n-1\nx = np.linspace(t.ppf(0.01, df),t.ppf(0.99, df), 100)\nfirst = t_value\nlast = max(x)\nax.plot(x, t.pdf(x, df),'k-', lw=5, alpha=0.6, label='t pdf')\nax.legend(loc='best', frameon=False)\nplt.fill_between(x, t.pdf(x,df), where=(x>=first)&(x<last), color='b')\n\nplt.show()","91199e19":"from scipy import stats\n# your code\nk=[]\nk = stats.ttest_1samp(samp, mean_te)\nprint(k)\n","a71b7801":"df = pd.read_csv('\/kaggle\/input\/alldatasets\/AB_NYC_2019.csv', index_col=False)\nnewdata = pd.read_csv('\/kaggle\/input\/alldatasets\/listings-3.csv')\nnewdata = newdata[['id','review_scores_rating']]\nnewdata2=df.merge(newdata, on='id')\nnewdata2=newdata2.dropna(how=\"any\")\ndf=newdata2\n","641a5cf0":"def pricecategories(price):\n  pricem=price['price']\n  for i in range(5):\n    if (i *200 < pricem) & ( pricem<=i*200+200):\n      return i\n      \ndf = df[df['latitude'].notna()]\ndf = df[df['longitude'].notna()]\ndf=df.loc[df.price<1000]\ndf = df.loc[df.price > 0]\n\n\ndf[\"label\"]=df.apply(pricecategories,axis=1)\ndf.head()","3697fc10":"minlat = df[\"latitude\"].min(axis=0)\nminlong= df[\"longitude\"].min(axis=0)\nmaxlat = df[\"latitude\"].max(axis=0)\nmaxlong = df[\"longitude\"].max(axis=0)\nfarlat = (maxlat-minlat)\/10 \nfarlong=(maxlong-minlong)\/10\nprint(\"The min latitude is {}\".format(df[\"latitude\"].min(axis=0)))\nprint(\"The min longitude is {}\".format(df[\"longitude\"].min(axis=0)))\nprint(\"The max latitude is {}\".format(df[\"latitude\"].max(axis=0)))\nprint(\"The max longitude is {}\".format(df[\"longitude\"].max(axis=0)))\nprint(\"Differences between latitudes is {}\".format(farlat*10))\nprint(\"Differences between longitudes is {}\".format(farlong*10))\n\n\nsizelist=[]\nfor i,j in zip(df[\"latitude\"],df[\"longitude\"]):\n  size=crime_df.loc[(crime_df[\"Latitude\"]> i-farlat) &(crime_df[\"Latitude\"]<i+farlat) &(crime_df[\"Longitude\"]>j-farlong) &(crime_df[\"Longitude\"]<j+farlong)].shape[0]\n  sizelist.append(size)\ndf[\"near_crime\"] = sizelist","5208e0a6":"incomelist=[]\nfor i,j in zip(df[\"latitude\"],df[\"longitude\"]):\n  size=blocks.loc[(blocks[\"Latitude\"]>i-farlat) &(blocks[\"Longitude\"]<i+farlat) &(blocks[\"Longitude\"]>j-farlong) &(blocks[\"Longitude\"]<j+farlong)]\n  meansize=size[\"IncomePerCap\"].mean(axis=0)\n  incomelist.append(meansize)\ndf[\"near_income\"] = incomelist","f104ad79":"plt.scatter(df[\"near_crime\"], df[\"near_income\"], c=df[\"label\"])\nplt.xlabel(\"Near Crime\")\nplt.ylabel(\"Near Income\")\nplt.grid()\nplt.show()\n","7f554161":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX =df[[\"near_crime\",\"near_income\"]]\ny =df[\"label\"]\nX_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.20, random_state=0)\nK_range = np.arange(1, 15)\nbefore = []\nfor k in K_range:\n  model = KNeighborsClassifier(k)\n  model.fit(X_train, y_train)\n  y_pred = model.predict(X_remaining)\n  accuracy = accuracy_score(y_pred,y_remaining)\n  before.append(accuracy)\nplt.figure(figsize=(12, 6))  \nplt.plot(K_range, before, color='black', linestyle='dashed', marker='o',  \n         markerfacecolor='red', markersize=10)\nplt.title('Accuracy of the Validation w\/ K')  \nplt.xlabel('K')  \nplt.ylabel('Accuracy')\nplt.xticks(K_range)\nplt.grid()\nplt.show()\n\n\nprint(before)\n","0bdfc586":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nleaf_size = list(range(1,10))\nn_neighbors = list(range(1,15))\np=[1,2]\n#Convert to dictionary\nhyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n#Create new KNN object\nknn_2 = KNeighborsClassifier()\n#Use GridSearch\nclf = GridSearchCV(knn_2, hyperparameters, cv=10)\n#Fit the model\nbest_model = clf.fit(X,y)\n#Print The value of best Hyperparameters\nprint('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\nprint('Best p:', best_model.best_estimator_.get_params()['p'])\nprint('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n","a8da52b4":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=14,p=1,leaf_size=14)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_remaining)\naccuracy = accuracy_score(y_pred,y_remaining)\naccuracy","cc12b796":"scaler = StandardScaler()\nX2 = scaler.fit_transform(X)\ny2 = LabelEncoder().fit_transform(y)\nX_train2, X_remaining2, y_train2, y_remaining2 = train_test_split(X2, y2, test_size=0.20, random_state=0)\n\n\nafter = []\n\nmodel = KNeighborsClassifier(k)\nmodel.fit(X_train2, y_train2)\ny_pred2 = model.predict(X_remaining2)\naccuracy = accuracy_score(y_pred2,y_remaining2)\nprint(accuracy)\n","9d7f835f":"from sklearn import tree\nfrom sklearn.model_selection import cross_val_score\n\naccuracy_list = []\naccuracy_SD_list = []\n\nfor depth in range (1, 10):\n  model = tree.DecisionTreeClassifier(max_depth=depth)\n  accuracies_CV = cross_val_score(model, X_train, y_train, cv=5)\n  accuracy_list.append(accuracies_CV.mean())\n  accuracy_SD_list.append(accuracies_CV.std())","08b0d8b7":"plt.figure(figsize=(12, 6))  \nplt.plot(range(1, 10), accuracy_list, color='black', linestyle='solid')\nplt.plot(range(1, 10), np.array(accuracy_list) + np.array(accuracy_SD_list),color='black', linestyle='dashed')\nplt.plot(range(1, 10), np.array(accuracy_list) - np.array(accuracy_SD_list),color='black', linestyle='dashed' )\nplt.fill_between(range(1, 10), np.array(accuracy_list) + np.array(accuracy_SD_list),\n                 np.array(accuracy_list) - np.array(accuracy_SD_list), alpha=0.2, facecolor ='b')\nplt.plot()\nplt.title('5-fold cross validated accuracy w\/ max_depth')  \nplt.xlabel('Max depth')  \nplt.ylabel('CV Accuracy +\/- sd') \nplt.show()\nprint(accuracy_list)","74d22fd9":" model = tree.DecisionTreeClassifier(max_depth=4)\n accuracies_CV = cross_val_score(model, X_train2, y_train2, cv=5)\n accuracies_CV.mean()","bc0a478a":"from sklearn.ensemble import RandomForestClassifier\nmodel_rf = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel_rf.fit(X_train, y_train)\nrf_predictions = model_rf.predict(X_remaining)\nrf_acc = accuracy_score(y_remaining, rf_predictions)\nprint(\"Random Forest Accuracy:\"+str(rf_acc))\n","62d04533":"model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel_rf.fit(X_train2, y_train2)\nrf_predictions = model_rf.predict(X_remaining2)\nrf_acc = accuracy_score(y_remaining2, rf_predictions)\nprint(\"Random Forest Accuracy:\"+str(rf_acc))\n","e64384ef":"from sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_remaining)\naccuracy_score(y_remaining, y_pred)","cb59fe86":"clf = GaussianNB()\nclf.fit(X_train2, y_train2)\ny_pred = clf.predict(X_remaining2)\naccuracy_score(y_remaining2, y_pred)\n","e5ed4495":"import math\ndef euclideandistance(x,y,m1,m2):\n  return math.sqrt(sum([(x - m1) ** 2+(y-m2)**2]))\n\n","5b4c03d5":"euclideandistance=[euclideandistance(i,j,40.7831,-73.9712) for i,j in zip(df[\"latitude\"],df[\"longitude\"])]\na=np.array(euclideandistance)\ndf[\"Center Distance\"]=a\ndf.neighbourhood_group.unique()\nimport numpy as np\n\ndef selection_sort(x):\n    for i in range(len(x)):\n        swap = i + np.argmin(x[i:])\n        (x[i], x[swap]) = (x[swap], x[i])\n    return x\nselection_sort(a)\n\nmanhattandistances=df[\"Center Distance\"][df.neighbourhood_group==\"Manhattan\"]\nbrooklyndistances=df[\"Center Distance\"][df.neighbourhood_group==\"Brooklyn\"]\nqueensdistances=df[\"Center Distance\"][df.neighbourhood_group==\"Queens\"]\nStatenIslanddistances=df[\"Center Distance\"][df.neighbourhood_group==\"Staten Island\"]\nBronxdistances=df[\"Center Distance\"][df.neighbourhood_group==\"Bronx\"]\n\nmanhattandistancesarray=np.array(manhattandistances)\nbrooklyndistancessarray=np.array(brooklyndistances)\nqueensdistancessarray=np.array(queensdistances)\nStatenIslanddistancessarray=np.array(StatenIslanddistances)\nBronxdistancessarray=np.array(Bronxdistances)\n\nselection_sort(manhattandistancesarray)\nselection_sort(brooklyndistancessarray)\nselection_sort(queensdistancessarray)\nselection_sort(StatenIslanddistancessarray)\nselection_sort(Bronxdistancessarray)\n\nprint(manhattandistancesarray.shape)\nprint(brooklyndistancessarray.shape)\nprint(queensdistancessarray.shape)\nprint(StatenIslanddistancessarray.shape)\nprint(Bronxdistancessarray.shape)\n\ndef categorizedistance(row):\n  centerdistance=row[\"Center Distance\"]\n  neighbourhoodgrp=row[\"neighbourhood_group\"]\n  if neighbourhoodgrp==\"Manhattan\":\n    if centerdistance<0.03049026729957918:\n      return 0\n    elif (0.03049026729957918< centerdistance) and (centerdistance<0.05446495019734937):\n      return 1\n    else:\n      return 2\n  if neighbourhoodgrp==\"Brooklyn\":\n    if centerdistance<0.0937867463984107:\n      return 0\n    elif (0.0937867463984107< centerdistance) and (centerdistance<0.11028837880755411):\n      return 1\n    else:\n      return 2\n  if neighbourhoodgrp==\"Queens\":\n    if centerdistance<0.06712938924197284:\n      return 0\n    elif (0.06712938924197284< centerdistance) and (centerdistance<0.1396300845090239):\n      return 1\n    else:\n      return 2\n  if neighbourhoodgrp==\"Staten Island\":\n    if centerdistance<0.1915125909698905:\n      return 0\n    elif (0.1915125909698905< centerdistance) and (centerdistance<0.21802457132167005):\n      return 1\n    else:\n      return 2\n  if neighbourhoodgrp==\"Bronx\":\n    if centerdistance<0.09245849068635675:\n      return 0\n    elif (0.09245849068635675< centerdistance) and (centerdistance<0.13281655657333968):\n      return 1\n    else:\n      return 2\ndf[\"Categorized Distance\"]=df.apply(categorizedistance,axis=1)","04219aec":"df[\"room_type\"].unique()","88ccebb2":"def classifyfunc(room_type):\n  if room_type==\"Entire home\/apt\":\n    return 0\n  elif room_type==\"Private room\":\n    return 1\n  elif room_type==\"Shared room\":\n    return 2\ndf[\"room_type\"]=df[\"room_type\"].apply(classifyfunc)","fd794de7":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX =df[[\"near_crime\",\"near_income\",\"number_of_reviews\",\"review_scores_rating\",\"Categorized Distance\",\"room_type\"]]\ny =df[\"label\"]\nX_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.20, random_state=0)\nK_range = np.arange(1, 15)\nbefore = []\nfor k in K_range:\n  model = KNeighborsClassifier(k)\n  model.fit(X_train, y_train)\n  y_pred = model.predict(X_remaining)\n  accuracy = accuracy_score(y_pred,y_remaining)\n  before.append(accuracy)\nplt.figure(figsize=(12, 6))  \nplt.plot(K_range, before, color='black', linestyle='dashed', marker='o',  \n         markerfacecolor='red', markersize=10)\nplt.title('Accuracy of the Validation w\/ K')  \nplt.xlabel('K')  \nplt.ylabel('Accuracy')\nplt.xticks(K_range)\nplt.grid()\nplt.show()\n\n\nprint(before)","6576e0e5":"scaler = StandardScaler()\nX2 = scaler.fit_transform(X)\ny2 = LabelEncoder().fit_transform(y)\nX_train2, X_remaining2, y_train2, y_remaining2 = train_test_split(X2, y2, test_size=0.20, random_state=0)\n\n\nafter = []\n\nmodel = KNeighborsClassifier(k)\nmodel.fit(X_train2, y_train2)\ny_pred2 = model.predict(X_remaining2)\naccuracy = accuracy_score(y_pred2,y_remaining2)\nprint(accuracy)\n","52da1a5e":"from sklearn import tree\nfrom sklearn.model_selection import cross_val_score\n\naccuracy_list = []\naccuracy_SD_list = []\n\nfor depth in range (1, 10):\n  model = tree.DecisionTreeClassifier(max_depth=depth)\n  accuracies_CV = cross_val_score(model, X_train, y_train, cv=5)\n  accuracy_list.append(accuracies_CV.mean())\n  accuracy_SD_list.append(accuracies_CV.std())\nprint(accuracy_list)","b3d0be30":"plt.figure(figsize=(12, 6))  \nplt.plot(range(1, 10), accuracy_list, color='black', linestyle='solid')\nplt.plot(range(1, 10), np.array(accuracy_list) + np.array(accuracy_SD_list),color='black', linestyle='dashed')\nplt.plot(range(1, 10), np.array(accuracy_list) - np.array(accuracy_SD_list),color='black', linestyle='dashed' )\nplt.fill_between(range(1, 10), np.array(accuracy_list) + np.array(accuracy_SD_list),\n                 np.array(accuracy_list) - np.array(accuracy_SD_list), alpha=0.2, facecolor ='b')\nplt.plot()\nplt.title('5-fold cross validated accuracy w\/ max_depth')  \nplt.xlabel('Max depth')  \nplt.ylabel('CV Accuracy +\/- sd') \nplt.show()","ffb4d6a4":" model = tree.DecisionTreeClassifier(max_depth=6)\n accuracies_CV = cross_val_score(model, X_train2, y_train2, cv=5)\n accuracies_CV.mean()","24f0c53d":"from sklearn.ensemble import RandomForestClassifier\nmodel_rf = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel_rf.fit(X_train, y_train)\nrf_predictions = model_rf.predict(X_remaining)\nrf_acc = accuracy_score(y_remaining, rf_predictions)\nprint(\"Random Forest Accuracy:\"+str(rf_acc))\n","9942f9fb":"model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel_rf.fit(X_train2, y_train2)\nrf_predictions = model_rf.predict(X_remaining2)\nrf_acc = accuracy_score(y_remaining2, rf_predictions)\nprint(\"Random Forest Accuracy:\"+str(rf_acc))\n","4394771f":"from sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_remaining)\naccuracy_score(y_remaining, y_pred)","c74fbb76":"clf = GaussianNB()\nclf.fit(X_train2, y_train2)\ny_pred = clf.predict(X_remaining2)\naccuracy_score(y_remaining2, y_pred)\n#standardization etkilemedi gaussianNB","527ce258":"### Creating 2D Array for Spatial Data","9e641e4c":"#### Standarization of Random Forest Algorithm \n\nStandarization does not have big effect on our accuracy score.It is because same reason which explained in Decision Tree standarization.","38b4de61":"#### Standarization of decision tree for max_depth \n\nWe observe that standarization should have no impact on the performance of a decision tree. It is generally useful, when you are solving a system of equations, least squares, etc, where we can have serious issues due to rounding errors. In decision tree, we are just comparing stuff and branching down the tree, so standarization would not help.\n","3ee0099b":"## Introduction\n\nThis project aims to find a correlation between burglary rates, income rates and Airbnb host prices in New York City. ","9fcfef65":"#### Random Forest Algorithm \n\nRandom Forest Algorithm has the power to handle a large data set with higher dimensionality.When we compare with first machine learning part which has only 2 features,now we have better accuracy score , it has higher dimensionality because Random forest model works better with more features.","11305467":"### Implementation\nWe used KNN model with normalization and find the best features for KNN model.Also,We used Decision Tree,Random Forest Algorithm and Gaussian Naive Bayes.","e754e1e8":"#### Decision Tree\nWe used the Decision Tree because we think that it is proper for our classification problem.We use cross validation method for 1-10 max depth values and visualize accuracy scores into a graph to show best max_depth parameter in range 10.\n\nWe found the best max_depth is 4 because we think our features are not complicated and simplicity is best so that we can keep our tree small.In each split step,it gives highest information.So that,max_depth did not grow so much.\n","38d16ef3":"#### Gaussian Naive Bayes\nWhen the predictors take up a continuous value and are not discrete, we assume that these values are sampled from a gaussian distribution.Normalization does not effect the our accuracy score because  it will just rescale the featues in this case but the probabilities will remain the same.\n\nWhen we compare with first machine learning part with 2 features now our accuracy score decreased because model lose efficiency in high dimensional spaces \u2013 namely when the number of features increases.\n\n","111e5f78":"### Creating NYC Map by Colorizing the Income Per Capita Values 2D","39a39337":"#### Random Forest Algorithm \n\nRandom forest method does not have big impact for the accuracy score when we compare with other ML models.Because Random Forest Algorithm has the power to handle a large data set with higher dimensionality.We do not provide high dimensional data to our model.","0bb0e8bd":"###Crime Close to Host\n\nWe realize that when we give all neighbourhood the same crime rate it will not fit our project because we want to investigate crimes which are close to our host. In a neighbourhood there can be diversity of their crime rates because of that we do not generalize it.\n\nWe divide difference of latitudes and difference of longitudes with 10. So that we create a elliptical shape to find crimes which are close that host. Farlong represents our y-radius of elliptical shape and farlat represents our x-radius of elliptical shape. So we acquire a data which is extracted from \"crime data\" and then we create a column with using this data to add inside to airbnb data.\n","a65c465d":"### Visualize Relation of Crime and Average Income\n\nWe plot the relation of crime and average income into a scatter. We conclude that they are well fit for classification model and we think there will no underfitting or overfitting problem, it resembles appropriate fitting.","d60011ef":"#### Gaussian Naive Bayes\nWhen the predictors take up a continuous value and are not discrete, we assume that these values are sampled from a gaussian distribution.We observe the best accuracy score in Bayes model.Normalization does not effect the our accuracy score because  it will just rescale the features in this case but the probabilities will remain the same.","dec3c5bf":"# Analysis and Prediction of Change in Price Based on The Crime Rates and Income Rates        ","50859ae8":"###Label Price\n\nWe will use supervised machine learning technique which is called classification. So that we converted our continious data type to discrete data type. When we examine our data we see that price which are bigger than 1000 $ are outliers. The outliers are 0.4% of our data and we prevent manipulation of our data. \n\nWe divide Airbnb prices into 5 categories (1-200, 201-400, 401-600, 601-800, 801-1000).\n","92726f4e":"## Block Data","a5feb9f0":"##  Airbnb Data\n\nWe check that which neighbourhood group is more preferred and then we calculate price mean of each borough.","4be6b198":"Calculate one tailed t test for our hypothesis.","05d0f780":"#### Decision Tree\nWe used the Decision Tree because we think that it is proper for our classification problem.We use cross validation method for 1-10 max depth values and visualize accuracy scores into a graph to show best max_depth parameter in range 10.\n\nWe found the best max_depth is 4 because we think our features are not complicated and simplicity is best so that we can keep our tree small.In each split step,it gives highest information.So that,max_depth did not grow so much.","89a0cd51":"We use folium library to mark locations with circle in NYC map.","b8e10a57":"## Null and Alternative Hypotheses(Income - Airbnb)\n\n$ H_0: $ Income of borough smaller than 35263, Airbnb prices will equals the mean of Airbnb prices.\n\n$ H_1: $  Income of borough smaller than 35263, Airbnb prices will smaller than mean of Airbnb prices.\n","3fd7c8ff":"### P value Calculate p value for our hypothesis.\n\nP value\n\n","6bd4c9ef":"We created NYC Map by Colorizing the Income Per Capita Values in 2D. When the color get brigther it shows that Income Per Capita is increasing and when the color get darker it shows that Income Per Capita is decreasing.\n\nReference: https:\/\/www.kaggle.com\/muonneutrino","ec6647a4":"## Machine Learning Algorithms with more features\n\nWe add 4 more features to our previous machine learning models.Our aim is to find more accurate result.Also,we want to see that how the new features affect in determining price category.\n\nWe use extra 3 datasets in addition to our original Airbnb dataset for new features.These datasets are Crime dataset,Income dataset,modified Airbnb dataset which is more detailed version of our Airbnb dataset.Categorized distance column was created by our distance calculation.It explained in detail below.Also room types are converted into integer values.\n\n\n","96bc97de":"## Use New and Old Data for ML\n","1b983c14":"### Plotting Average Prices on Graphs","ce1a133b":"## Income Data\n","bc6c0050":"Block data have latitude and longitude values and income data have income per capita data. Also, both of them have the id of that block so we merge them by ids then we create a map of income per capita by using longitude and latitude.\n\nThere were some bad values in median household income column so we change the Income column to take floating point numbers.","cb88890d":"### Create Heat Map for Airbnb Hosts\n\nWe use heat map to observe Airbnb hosts density. ","4d633c04":"### Create Heat Map for Crime Locations","f2cfc90c":"Show which borough is repated more by plotting on a pie and bar chart.\n\n---\n\n","5b8f6e72":"#### Grid Search\n\nWe use grid search in our knn algorithm so that we found best leaf size, best distance calculation algorithm(manhattan) and best k value for knn model but our leaf leaf range is 1 to 10 we can not find the best leaf size(actually best is 30).","f75d9758":"In KNN model we get good accuracy score because our classes are quite separable.We used scatter plot and so it is clear that our classes are more linearly seperable.Standarization of KNN doesnt effect so much because  differing ranges are not high between 2 data feature.In 2 features,Decision tree worked well because of our simplicity of features.There is no high dimensinality only two features effect on our label.It worked better than Random forest because  decision tree is better when the dataset have a feature that is really important to take a decision and Random Forest model worked well when more number of feature existed.In Decision Tree and Random forest tree , we are just comparing stuff and branching down the tree, so standarization would not have big impact on our Standarized feature's accuracy score.We find the our crime rate and income datas are independent from each other in our Null Hypothesis tests.So that,we expect the Naive bayes will give good accuracy score because it works well in independent features.We observed the highest accuracy score in Gaussian Naive Bayes as expected result.","20ab320b":"After we find the frequencies we find the income means for each boroughs and visualize it with bar and pie charts.\n\n---","07c16652":"### Add New Columns to Airbnb Data\n\nFor getting better result in machine learning algorithms we use income and crime data and in addition to that we find the same Airbnb 2019 data's more detailed form and merge some columns from that data to our Airbnb data.\n\nWe dropped rows which has a column with NaN value for make it usable in our machine learning model.\n","7f564fe7":"### Create Map for Airbnb Host Locations\n\nWe pick 1000 of that locations because data is too huge to mark on map so we pick randomly.","4d50f55a":"## Null and Alternative Hypotheses (Crime - Airbnb)\n\n$ H_0: $ The number of burglary committed in the borough smaller than 102.8,Airbnb prices will equals the mean of Airbnb prices.\n\n$ H_1: $  The number of burglary committed in the borough smaller than 102.8  will smaller than the mean of Airbnb prices.\n","12c927b5":"### Plotting Boroughs Income Per Cap Difference","544317e4":"#### Hyperparameter Tunning with KNN\n\nNumber of neighbors will effect of our model preformance so we want to optimize our model. So that we plot it into a grap for 15 K value and we realize that after k = 4 it does not have a big impact our accuracy score.","477448b7":"We read data again because we changed it at on earlier t test.","e18319d6":"We see that Manhattan has the highest average prices for Airbnb hosts with 32% and Bronx is the lowest average income  borough with 14%.","8df5f15e":"#### Standarization of decision tree for max_depth \n\nWe observe that standarization should have no impact on the performance of a decision tree. It is generally useful, when you are solving a system of equations, least squares, etc, where we can have serious issues due to rounding errors. In decision tree, we are just comparing stuff and branching down the tree, so standarization would not help.\n\n","585f711e":"## Machine Learning Algorithms with 2 Features\n\nIn this machine learning part, we investigated the effect of crime and income features with different classification models and determining our categorized price label.\n","d958ea19":"\n\nP value is 0.13 which is bigger than 0.05 = \ud835\udefc. 0.13 > 0.05 so we fail to reject null hypothesis, we accept it.\n\n\n","d2f4a0ce":"### Ploting Borough's Crime Frequencies on Graphs","01fa970c":"We read data again because we changed its' size earlier when we mapping.","08231796":"#### Standart Scaler of KNN \nIt arranges the data in a standard normal distribution.\nWe think that it is useful in our classification problem which preventing differing ranges may cause a problem. \n\nStandardization is only effects 0.002 because we think our features are not obtained from differing ranges due to their definitions.","e51ed088":"#### Encoding Room Type Column\n\nThere are 3 different types of room type.So we converted them into integer form to use in model which expect numeric inputs.","ed34f85a":"#### Standart Scaler\nIt arranges the data in a standard normal distribution.\nWe think that it is useful in our classification problem which preventing differing ranges may cause a problem. \n\nStandardization is only effects 0.0034 because we think our features are not obtained from differing ranges due to their definitions.","4a419084":"We see that Brooklyn is the most burglary comitted borough with 31% and Staten Island is the least burglary comitted borough with 4%. So we see that Staten Island is the safest borough in NYC and Brooklyn is the most unsafe borough in NYC.","8eda1677":"### Implementation\nWe used KNN model with normalization and find the best features for KNN model we used Grid Search method.Also,We used Decision Tree,Random Forest Algorithm and Gaussian Naive Bayes.","d86ccf75":"In KNN model we get good accuracy score because our classes are quite separable.It is clear that our classes are more linearly seperable.Standarization of KNN doesnt effect so much because differing ranges are not high between 6 data feature.In 6 features.When KNN model used with more features, the differening between the values increased, so the standarization gave a more effective result compared to 2 features in 6 features.Decision tree worked well because of our simplicity of features.There is no high dimensinality only six features effect on our label.It worked better than Random forest because decision tree is better when the dataset have a feature that is really important to take a decision and Random Forest model worked well when more number of feature existed.In Decision Tree and Random forest tree , we are just comparing stuff and branching down the tree, so standarization would not have big impact on our Standarized feature's accuracy score.With the increasing number of feature,Random forest model gives better accuracy score because it work better with higher number of feature.More number of features decreasing the independence between the features so that with comparing 2 features Naive Bayes the accuracy score decreased as we expected.","a61fff8f":"Calculate one tailed t test for our hypothesis.","625adfbc":"We see that Manhattan has the highest average income   borough with 39% and Bronx is the lowest average income  borough with 11%. So we see that Bronx is the poorest borough in NYC and Manhattan is the richest borough in NYC.","c47b88a8":"### Results & Discussion\n<table>\n  <thead>\n    <tr>\n      <th>Machine Learning Model<\/th>\n      <th>Raw Features<\/th>\n      <th>Standardized Features<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <td>KNN<\/td>\n      <td>0.862114<\/td>\n      <td>0.865868<\/td>\n    <\/tr>\n    <tr>\n      <td>Decision Tree<\/td>\n      <td>0.860729<\/td>\n      <td>0.862491<\/td>\n    <\/tr> \n    <tr>\n      <td>Random Forest<\/td>\n      <td>0.849791<\/td>\n      <td>0.850828<\/td>\n    <\/tr>\n    <tr>\n      <td>Gaussian Naive Bayes<\/td>\n      <td>0.850866<\/td>\n      <td>0.850866<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>","56f45dfe":"### Plotting Boroughs' Frequencies on Graphs","211368b1":"##Conclusion\nIn our first machine learning part, we achieved a high accuracy score with only 2 features.With our hypothesis tests, we understood that our income and crime data are not dependent with our Airbnb data. However, when using these 2 datas together, airbnb prices can predict the price categories with an accuracy score of up to 86%.Then we added 4 more features to 2 features which we think might affect the price category but it does not give big impact on accuracy score.\n\nWe think our solution is applicable because we have good accuracy score.\n\nOur one of the most important advantages of our solution is high accuracy score which can be used in Airbnb Company.However,we have 14% margin of error so it can be considered as a disadvantage that it does not give a definite result. \n\n\n\n","6965dd50":"### P value\n\nCalculate p value for our hypothesis.","fbe2ca84":"### Create Map for Crime Locations","1ba7ba3d":"### Average Income for the Area Close to Host\n\nWe realize that when we give all neighbourhood the same average income rate it will not fit our project because we want to investigate average income which are close to our host. In a neighbourhood there can be diversity of their average income because of that we do not generalize it.\n\nWe divide difference of latitudes and difference of longitudes with 10. So that we create a elliptical shape to find average income which are close that host. Farlong represents our y-radius of elliptical shape and farlat represents our x-radius of elliptical shape. So we acquire a data which is extracted from \"income data\" and then we create a column with using this data to add inside to airbnb data.","bb788791":"We use this data to determine latitude and longitude of NY BlockCode.\n\nBrooklyn Borough = Kings County\n\nBronx Borough = Bronx County\n\nQueens Borough = Queens County\n\nManhattan Borough = New York County\n\nStaten Island Borough = Richmond County\n\n\n---\n","b384a06f":"#### Categorized Distance Column Creation\n\nWe aim to see effect of distance to center in determining the Airbnb prices.So that we categorized the our Airbnb house into 3 different categories.0 represent closest points to our borough's center.2 represents the farest point to our borough's center.\n\n\nCenter of Manhattan =40.7831\u00b0 latitude, -73.9712\u00b0 longitude \n\nCenter of Brooklyn =40.6782\u00b0 latitude, -73.9442\u00b0 longitude\n\nCenter of Bronx =40.8448\u00b0 latitude, -73.8648\u00b0 longitude \n\nCenter of Queens =40.7282\u00b0 latitude, -73.7949\u00b0 longitude \n\nCenter of Staten Island =40.5795\u00b0 latitude, -74.1502\u00b0 longitude ","e6808762":"### Results & Discussion\n<table>\n  <thead>\n    <tr>\n      <th>Machine Learning Model<\/th>\n      <th>Raw Features<\/th>\n      <th>Standardized Features<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <td>KNN<\/td>\n      <td>0.86318<\/td>\n      <td>0.86426<\/td>\n    <\/tr>\n    <tr>\n      <td>Decision Tree<\/td>\n      <td>0.86058<\/td>\n      <td>0.86093<\/td>\n    <\/tr> \n    <tr>\n      <td>Random Forest<\/td>\n      <td>0.84479<\/td>\n      <td>0.84497<\/td>\n    <\/tr>\n    <tr>\n      <td>Gaussian Naive Bayes<\/td>\n      <td>0.86604<\/td>\n      <td>0.86604<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>","9e1dc0f0":"We create a dense 2D array so that we can make plots of our spatial data. We make a 2D array of the census tract numbers for copying some of the code in the for loop to obtain it in constant time.","c03d4d54":"## Crime Data\n\nWe use this data to find just Burglary crimes because we thought that other crimes like murder will not give us a reliable result because murder can happen any location and not related with that location. Burglary crimes must be related with that location so we use it. The data so huge so we filter this data just for 2019-2020 crimes.\n\n---","6dada652":"We see that Brooklyn has the most frequent   borough in the data with 35% and Staten Island is the least frequent borough with 5%.","df6079fc":"\nP value is 0.36 which is bigger than 0.05 = \ud835\udefc. 0.36 > 0.05 so we fail to reject null hypothesis, we accept it.","810e4cfe":"#### Hyperparameter Tunning with KNN\n\nNumber of neighbors will effect of our model preformance so we want to optimize our model. So that we plot it into a grap for 15 K value and we realize that after k = 6 it does not have a big impact our accuracy score.","121077b6":" We find borough's crime frequencies to reach which borogh's are more unsafe and plot on a graph","a8d60ccf":"We find the frequencies of Boroughs in our income data.\n\n---\n\n","fae6f509":"We use heat map to observe crime's density. So we can easily see that which location of NYC is more unsafe.\n\n\n\n"}}