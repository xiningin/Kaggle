{"cell_type":{"33e71a63":"code","552497ba":"code","b292df5e":"code","76d1458a":"code","49b9c7fe":"code","010936ed":"code","ec1611b6":"code","fb0f3d3d":"code","06844f63":"code","e9aedce9":"code","326a36eb":"code","2ab9d43c":"code","58d02ac9":"code","4eff1f12":"code","1a182874":"code","49d9b73a":"code","65e8010d":"code","5b5f3b0f":"code","a05e8454":"code","e5b2c1d0":"code","17936000":"code","25c6c197":"code","cbeee95e":"code","fab6aa21":"code","c9e14ec5":"code","7f41745b":"code","58f507cb":"markdown","89868291":"markdown","c12b9a8d":"markdown","6890a919":"markdown","82264a5b":"markdown","e730f6c2":"markdown","0d7186fc":"markdown","0e845ca8":"markdown","ef059f07":"markdown","80ba2dea":"markdown","bf9fa119":"markdown","5f841d0e":"markdown","7eac69af":"markdown","2bfa5c3f":"markdown","c5a56c2a":"markdown","7d3a2cf7":"markdown","3bbd1839":"markdown","7df11833":"markdown","21a61655":"markdown","94d33053":"markdown","216a0924":"markdown","5fe704d7":"markdown","1bd8061e":"markdown"},"source":{"33e71a63":"!pip install {\"..\/input\/kapree\/kapre-0.1.7-py3-none-any.whl\"}\n\n# This file is used to download audio from various datasets and converts them audio into numpy \n!wget -q https:\/\/raw.githubusercontent.com\/douglas125\/SpeechCmdRecognition\/master\/SpeechDownloader.py\n\n# A generator for reading and serving audio files\n!wget -q https:\/\/raw.githubusercontent.com\/douglas125\/SpeechCmdRecognition\/master\/SpeechGenerator.py\n\n# Utility functions for audio files\n!wget -q https:\/\/raw.githubusercontent.com\/douglas125\/SpeechCmdRecognition\/master\/audioUtils.py\n\n# The actual speech model\n!wget -q https:\/\/raw.githubusercontent.com\/douglas125\/SpeechCmdRecognition\/master\/SpeechModels.py\n\nimport SpeechDownloader\nimport SpeechGenerator\nimport SpeechModels\nimport audioUtils","552497ba":"!wget pip install tensorflow>=2\n!wget pip install kapre==0.2\n!wget pip install pandas>=0.25\n!wget pip install librosa>=0.8\n!wget pip install tqdm\n!wget pip install matplotlib","b292df5e":"import matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt  \nimport pickle","76d1458a":"# this indicates that we want to use a pretrained model\nversion = 2\n\n# specify the amount of commands that should be recognized by the model\ngscInfo, nCategs = SpeechDownloader.PrepareGoogleSpeechCmd(version=version, task='35word')\n\n# gscInfo: path to trainig, test and validation files \n# nCategs: the amount of classes\nprint(\"gscInfo: \", gscInfo)\nprint(\"nCategs: \", nCategs)\n\nwith open('gscInfo','wb') as f: \n  pickle.dump(gscInfo, f)\n\nwith open('nCategs','wb') as f: \n  pickle.dump(nCategs, f)","49b9c7fe":"with open('gscInfo','rb') as f: gscInfo = pickle.load(f)\n\nwith open('nCategs','rb') as f: nCategs = pickle.load(f)\n\n# test if it worked \n#print(numpy.array_equal(gscInfo,gscInfo2))\n#print(numpy.array_equal(nCategs,nCategs2))","010936ed":"# to handla that the number of samples in validation may not be multiple of batch_size\nshuffle = True  \n\n# Create the speech generators, one for validation and one for testing\nvalidation_speech_generator   = SpeechGenerator.SpeechGen(gscInfo['val']['files'], gscInfo['val']['labels'], shuffle=shuffle)\ntest_speech_generator = SpeechGenerator.SpeechGen(gscInfo['test']['files'], gscInfo['test']['labels'], shuffle=False, batch_size=len(gscInfo['test']['files']))\n\nprint(\"validation generator: \", validation_speech_generator)\nprint(\"test generator: \", test_speech_generator)\n\nprint(\"Length of validation generator: \", validation_speech_generator.__len__())\nprint(\"Length of test generator: \", test_speech_generator.__len__())","ec1611b6":"X, Y = validation_speech_generator.__getitem__(5)\nprint(\"Features: \", X)\nprint(\"Labels: \", Y)","fb0f3d3d":"# varf\u00f6r \u00e4r input length None? Kan det vara vad som helst d\u00e5? \nspeech_model = SpeechModels.AttRNNSpeechModel(nCategs, inputLength = None)\n\n# vad g\u00f6r denna?? trodde vi hade tr\u00e4nat redan?\nspeech_model.compile(optimizer='adam', loss=['sparse_categorical_crossentropy'], metrics=['sparse_categorical_accuracy'])\n\n# gissar att det \u00e4r h\u00e4r vi laddar in vikterna som de tr\u00e4nat fram \n# \u00c4NDRA L\u00c4NKEN SEN N\u00c4R ALLT \u00c4R NEDLADDAT \nspeech_model.load_weights('..\/input\/pre-trained-speech-model\/model-attRNN_pre_trained.h5')","06844f63":"X_test, Y_test = test_speech_generator.__getitem__(0)","e9aedce9":"predictions = np.argmax(speech_model.predict(X_test, verbose = 1), 1)","326a36eb":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nclasses = ['unknown', 'nine', 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go',\n           'zero', 'one', 'two', 'three', 'four', 'five', 'six', \n           'seven',  'eight', 'backward', 'bed', 'bird', 'cat', 'dog',\n           'follow', 'forward', 'happy', 'house', 'learn', 'marvin', 'sheila', 'tree',\n           'visual', 'wow']\n\ncm = confusion_matrix(Y_test, predictions)\naudioUtils.plot_confusion_matrix(cm, classes, normalize = False)\n","2ab9d43c":"import tensorflow as tf\n\nimport matplotlib.pyplot as plt\n\n# regex\nimport re\n\n# merge paths\nimport os\n\nimport numpy as np \n\n# create datasets\nimport pandas as pd \n\n# to get file names in a folder \nfrom glob import glob\n\n# The Image module provides a class with the same name which is used to represent a PIL image. The module also provides a number of factory functions, including functions to load images from files, and to create new images.\nfrom PIL import Image\n\n# to save a model as a file \nfrom pickle import load, dump\n\n# to print a model \nfrom keras.utils import plot_model\n\n# one-hot encoding \nfrom keras.utils import to_categorical\n\n# It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).\nfrom keras.layers.merge import add\n\n# Loads a model saved via model.save(), Model is the actual model \nfrom keras.models import Model, load_model\n\n# Dense: fully connected layer that perform classification on the features extracted by the convolutional layers and down-sampled by the pooling layers in the CNN and that are used in the RNN. \n# \n# LSTM: RNN may suffer from the vanishing gradient problem - LSTM solves this problem, LSTM knows what to store and what to throw away. \n# Dropout: The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1\/(1 - rate) such that the sum over all inputs is unchanged.\n# Embedding: Use to create our own word embeddings using a tokenizer \n# Input: is used to instantiate a Keras tensor.\nfrom keras.layers import Dense, LSTM, Dropout, Embedding, Input\n\n# Pads sequences to the same length.\nfrom keras.preprocessing.sequence import pad_sequences","58d02ac9":"start = \"<start>\"\nend = \"<end>\"\npathToImageFolder = '..\/input\/flickr-image-dataset\/flickr30k_images\/flickr30k_images\/'\npathToImageCaptionCSV = '..\/input\/flickr-image-dataset\/flickr30k_images\/results.csv'\n\n# training params \npictures_used_when_training = 100\n\n# In terms of artificial neural networks, an epoch refers to one cycle through the full training dataset\nepochs = 10\n\n# dimentions of the pictures\nxdim = 299\nydim = 299","4eff1f12":"# returns a list with the name of all vectors and a dictionary with the name as key and caption as value\ndef fetchData(): \n    \n    # read the captions to a panda datastructure \n    captions = pd.read_csv(pathToImageCaptionCSV, delimiter='|')\n    captions.columns = ['imageName', 'captionNumber', 'caption']\n\n    # stores all the file names in a list \n    all_img_name_vector = []\n    \n    # stores all the captions in a dictionary with filenames as key \n    all_captions = {}\n    \n    # parse the panda and fill the structures above \n    for index, row in captions.head(n=pictures_used_when_training).iterrows():\n        caption = start + \" \" + row[2].replace(\".\", \"\").strip() + \" \" + end\n        im_ID = row[0]\n        all_img_name_vector.append(im_ID)\n        \n        if im_ID not in all_captions:\n            all_captions[im_ID] = []\n        all_captions[im_ID].append(caption)\n    \n    all_img_name_vector = list(set(all_img_name_vector))\n    return all_img_name_vector, all_captions\n\n# creates the feature dictionary (feature representation of every picture)\ndef createFeatures(img_name_vector, CNN_model):\n    \n    # create a features.p file if it does not exist\n    if not os.path.exists('features.p'):\n        \n        # load the pretrained Xception model \n        # the pooling layers downsomples the image data extracted by the convolutional layers to reduce the dimentionality of the feaature map inorder to decrease processing time \n        # include_top false to excklude the top layer \n        \n        # dictionary used to store the features images \n        features = {}       \n        \n        # create the feature of each picture (represent the picture as a vector) using the CNN\n        for im_ID in img_name_vector:       \n            image = loadPicture(im_ID)    \n            \n            \n            feature = CNN_model.predict(image)\n            features[im_ID] = feature\n\n        # store the feature representation fo every picture in the file feature.p\n        dump(features, open(\"..\/input\/long-training-overnight\/results\/features.p\",\"wb\"))  \n\n    features = load(open(\"features.p\", \"rb\"))\n   \n    # only return the images that are in the name vector \n    features = {im_ID:features[im_ID] for im_ID in img_name_vector}\n    \n    return features\n\n# transform picture to standard dimention  (299x299)\ndef loadPicture(im_ID):\n    imagePath = pathToImageFolder + im_ID\n    image = Image.open(imagePath).resize((xdim,ydim))\n    image = np.expand_dims(image, axis=0) \n    return normalize(image) \n\n# normalising the values to -1 to 1 \ndef normalize(image):\n    return (image\/127.5) - 1.0\n\n# create and return tokenizer \n# This class allows to vectorize a text corpus, by turning each text into a sequence of integers \n\ndef extractTokens(all_captions, num_words):\n    tkz = tf.keras.preprocessing.text.Tokenizer(num_words = num_words, oov_token = \"<unk>\", filters='!\"#$%&()*+.,-\/:;=?@[\\]^_`{|}~ ')\n    # Updates internal vocabulary based on a list of texts.\n    tkz.fit_on_texts(all_captions)\n    dump(tkz, open('tokenizer.p', 'wb'))\n    return tkz \n\n# flatten dictionary\ndef flatten(dictionary):\n    flat = list()\n    for item in dictionary.keys():\n        [flat.append(d) for d in dictionary[item]]\n    return flat","1a182874":"all_img_name, all_captions = fetchData()\n\n# feature representation of the images\nCNN_model = tf.keras.applications.Xception(pooling = 'avg', include_top = False)\n\n# The output of our CNN is a 2048 feature vector\nimage = loadPicture(\"10002456.jpg\")    \nprint(CNN_model.predict(image).shape)\n\nfeatures = createFeatures(all_img_name, CNN_model)\n\n# tokenizer\nflat_all_captions = flatten(all_captions)\ntkz = extractTokens(flat_all_captions, 5000)\nvocabSize = len(tkz.word_index) + 1\n\n# max length used for the caption length\ncaption_max_length = max(len(t.split()) for t in flat_all_captions)","49d9b73a":"# Build the RNN \ndef buildRNN(vocabSize, caption_max_length):\n\n    # specify the shape (the number of features that each input sample has) \n    # 2048 because that if the shape that Imagenet uses \n    # Output from Imagenets is the input for the RNN model \n    inputs1 = Input(shape=(2048,))\n    \n    # the dropout layer randomly sets input units to 0 with a frequency of 50%\n    # to avoid overfitting \n    fe1 = Dropout(0.5)(inputs1)\n    \n    # create the fully connected layer with 256 neurons and use the relu as the activation function\n    fe2 = Dense(256, activation='relu')(fe1)\n    \n    # Create the LSTM layer\n    # shape of the caption input \n    inputs2 = Input(shape=(caption_max_length,))\n    se1 = Embedding(vocabSize, 256, mask_zero=True)(inputs2)\n    # drop 50% of all neurons \n    se2 = Dropout(0.5)(se1)\n    # the layer for the picture \n    se3 = LSTM(256)(se2)\n    \n    # Merg the two models \n    decoder1 = add([fe2, se3])\n    decoder2 = Dense(256, activation='relu')(decoder1)\n    outputs = Dense(vocabSize, activation='softmax')(decoder2)\n    \n    # tie it together [image, seq] [word]\n    RNN_model = Model(inputs=(inputs1, inputs2), outputs=outputs)\n    RNN_model.compile(loss='categorical_crossentropy', optimizer='adam')\n    \n    return RNN_model\n\n# generator is used so that we dont have to give all data to the model at once (that will exhaust the model)\ndef data_generator(all_captions, features, tkz, caption_max_length):\n    \n    while True:\n        for im_ID, captions in all_captions.items():   \n            \n            # vector representation of the picture \n            feature = features[im_ID][0]\n            \n            # create the sequence \n            input_image, input_sequence, output_word = createSequences(tkz, caption_max_length, captions, feature)\n            \n            yield ([input_image, input_sequence], output_word)\n            \ndef createSequences(tkz, caption_max_length, captions, feature):\n    X1 = []\n    X2 = []\n    y = []\n    \n    for caption in captions:\n        # transform text to vector using the tokenizor\n        seq = tkz.texts_to_sequences([caption])[0]\n        \n        # loop every word in the sequence \n        for i in range(1, len(seq)):\n            \n            # all the words in the caption until letter i\n            inputSequence = seq[:i]\n            \n            # pad so that it always is the same length\n            inputSequence = pad_sequences([inputSequence], maxlen = caption_max_length)[0]\n\n            # the new word \n            outputSequence = seq[i]\n            \n            # go from words to indexes \n            outputSequence = to_categorical([outputSequence], num_classes=vocabSize)[0]\n            \n            # append to inputs and outputs \n            X1.append(feature)\n            X2.append(inputSequence)\n            y.append(outputSequence)\n    \n    X1 = np.array(X1)\n    X2 = np.array(X2)\n    y = np.array(y)\n    return X1, X2 , y","65e8010d":"model = buildRNN(vocabSize, caption_max_length)\nsteps_per_epoch = len(all_captions)\n\nfor i in range(epochs):\n    filename = \"model_\" + str(i) + \".h5\"\n\n    gen = data_generator(all_captions, features, tkz, caption_max_length)\n    model.fit_generator(gen, epochs = epochs, steps_per_epoch = steps_per_epoch, verbose = 1)\n    \n    model.save(filename)","5b5f3b0f":"tkz = load(open(\"..\/input\/long-training-overnight\/results\/tokenizer.p\",\"rb\"))\n\nCNN_model = tf.keras.applications.Xception(pooling = 'avg', include_top = False)\nRNN_model = load_model('..\/input\/long-training-overnight\/results\/model_8.h5')\n\ntkz = load(open(\"..\/input\/long-training-overnight\/results\/tokenizer.p\",\"rb\"))\nRNN_model = load_model('..\/input\/long-training-overnight\/results\/model_8.h5')\n\n# .\/model_8.h5\n# .\/tokenizer.p\n","a05e8454":"# takes an index and returns a word\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word    \n    return None\n    \ndef generate_caption(model, tkz, img_feature, caption_max_length):    \n    # first word is always the start symbol \n    caption = start\n    \n    # loop until we get \"<end>\" or reached the max length\n    for i in range(caption_max_length):\n        \n        # convert the text to a vector using the tokenizer\n        seq = tkz.texts_to_sequences([caption])[0]\n        \n        # transform to the same length \n        seq = pad_sequences([seq], maxlen = caption_max_length)\n        \n        # gives the probability of each word given the picture and the previous sequence\n        predicted_id_list = model.predict([img_feature, seq], verbose=0)\n        \n        # gives the index of the word with the highest probability\n        pred_id = np.argmax(predicted_id_list)\n        \n        # gives the word of the above index \n        word = word_for_id(pred_id, tkz)\n        \n        # bug-fix when a tokenizer that does not match the captions have been used \n        if word is None:\n            break\n            \n        caption += ' ' + word\n        \n        # end if the word is end or it it wants to write start again \n        if word == end or word == start:\n            break\n            \n    return caption\n\ndef test_model(CNN_model, RNN_model, tkz, im_ID):        \n    \n    # create the vector representation of the image \n    image = loadPicture(im_ID)\n    feature = CNN_model.predict(image)\n    \n    # generate caption\n    print(generate_caption(RNN_model, tkz, feature, caption_max_length))\n    \n    # display image \n    img = Image.open(pathToImageFolder + im_ID)\n    plt.imshow(img)\n\ndef test_model2(CNN_model, RNN_model, tkz, im_ID):        \n    \n    # create the vector representation of the image \n    image = loadPicture(im_ID)\n    feature = CNN_model.predict(image)\n    \n    # generate caption\n    caption = generate_caption(RNN_model, tkz, feature, caption_max_length)\n\n    return caption","e5b2c1d0":"test_model(CNN_model, RNN_model, tkz, '1355703632.jpg')","17936000":"test_model(CNN_model, RNN_model, tkz, '1032122270.jpg')","25c6c197":"test_model(CNN_model, RNN_model, tkz, '1009434119.jpg')","cbeee95e":"test_model(CNN_model, RNN_model, tkz, '1714937792.jpg')","fab6aa21":"# use the speech model to predict the classes of all test sound \nword_predictions= np.argmax(speech_model.predict(X_test, verbose = 1), 1)","c9e14ec5":"# To quickly be able to go fram an index to a word\nclasses = ['nine', 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go',\n           'zero', 'one', 'two', 'three', 'four', 'five', 'six', \n           'seven',  'eight', 'backward', 'bed', 'bird', 'cat', 'dog',\n           'follow', 'forward', 'happy', 'house', 'learn', 'marvin', 'sheila', 'tree',\n           'visual', 'wow']","7f41745b":"# now use the CNN and RNN model to create a caption for each picture and return the picture if the \n# caption contains the word we choose \n\n# Lets say we are looking for a picture for sound 5 in the test data;\nsound_index = 900\nprint(word_predictions[sound_index])\npredicted_word = classes[word_predictions[sound_index]]\nreal_word = classes[Y_test[sound_index]]\n\nprint(\"We are looking for the sound with index\", sound_index, \". We classified this sound as the word:\", predicted_word,\"(the real class for this word is:\",real_word,\")\")\n\npictures_Wanted = 3\npictures_looked_at = 0\nfound_images = 0\nfound_images_ID = []\nfound_pictures = 0\n\nfor img_name in all_img_name:\n    pictures_looked_at +=1\n    caption = test_model2(CNN_model, RNN_model, tkz, img_name) \n    \n    print(\"\\n\\n\\nLooking at picture: \", pictures_looked_at)\n    \n    if predicted_word in caption:\n        print(\"found an image!\")\n        found_pictures += 1\n        found_images_ID.append(img_name)\n    \n    if found_pictures == pictures_Wanted:\n        break\n\nif found_pictures == 0:\n    print(\"\\n\\nCould not find a picture with the word\")\nelse:\n    print(\"\\n\\nWe found these pictures for sound : \",sound_index, \"(\",predicted_word, \")\",found_images_ID)\n    print(\"One of them looks like this; \")\n    # display image \n    img = Image.open(pathToImageFolder + found_images_ID[found_pictures-1])\n    plt.imshow(img)\n    \n    ","58f507cb":"## Look at validation data","89868291":"## Test the RNN model","c12b9a8d":"# Sound to text model\n\nFor this part of the project we used this notebook as inspiration: https:\/\/github.com\/douglas125\/SpeechCmdRecognition\n\nCite: \n@ARTICLE{2018arXiv180808929C,\n   author = {{Coimbra de Andrade}, D. and {Leo}, S. and {Loesener Da Silva Viana}, M. and \n\t{Bernkopf}, C.},\n    title = \"{A neural attention model for speech command recognition}\",\n  journal = {ArXiv e-prints},\narchivePrefix = \"arXiv\",\n   eprint = {1808.08929},\n keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},\n     year = 2018,\n    month = aug,\n   adsurl = {http:\/\/adsabs.harvard.edu\/abs\/2018arXiv180808929C},\n  adsnote = {Provided by the SAO\/NASA Astrophysics Data System}\n}","6890a919":"## Create the speech model and load the pre-trained weights","82264a5b":"# Combine the models to find pictures ","e730f6c2":"## Explanation of how the pre-trained model we use was trained\n\nIt was trained using ...","0d7186fc":"## Load pre-saved model (so we dont have to train it every time)","0e845ca8":"### Pickle dump the previous datastructures so that we dont have to download them again ","ef059f07":"## Define functions used when testing the model","80ba2dea":"## Download and prepare Google Speech Dataset","bf9fa119":"## Define functions used when building and training","5f841d0e":"## Preprocess data","7eac69af":"2## Download and import the files needed from the github project","2bfa5c3f":"## Test if the model works by looking at the test data\n'unknown': 0,\n'silence': 0,\n'_unknown_': 0,\n'_silence_': 0,\n'_background_noise_': 0,\n'yes': 2,\n'no': 3,\n'up': 4,\n'down': 5,\n'left': 6,\n'right': 7,\n'on': 8,\n'off': 9,\n'stop': 10,\n'go': 11,\n'zero': 12,\n'one': 13,\n'two': 14,\n'three': 15,\n'four': 16,\n'five': 17,\n'six': 18,\n'seven': 19,\n'eight': 20,\n'nine': 1,\n'backward': 21,\n'bed': 22,\n'bird': 23,\n'cat': 24,\n'dog': 25,\n'follow': 26,\n'forward': 27,\n'happy': 28,\n'house': 29,\n'learn': 30,\n'marvin': 31,\n'sheila': 32,\n'tree': 33,\n'visual': 34,\n'wow': 35}\n\n['nine', 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go',\n           'zero', 'one', 'two', 'three', 'four', 'five', 'six', \n           'seven',  'eight', 'backward', 'bed', 'bird', 'cat', 'dog',\n           'follow', 'forward', 'happy', 'house', 'learn', 'marvin', 'sheila', 'tree',\n           'visual', 'wow']\n\n\n","c5a56c2a":"## Install the needed requirements","7d3a2cf7":"## Import Everything ","3bbd1839":"## Import other libraries that we need ","7df11833":"## Build the test and validation generator (dont need train since we use a pre-trained model)","21a61655":"## Define functions used when preprocessing data\n\nWe use the pretrained CNN model \"Xception\": https:\/\/keras.io\/api\/applications\/xception\/\n\nQuote from webbsite: \nOptionally loads weights pre-trained on ImageNet. \nthe default input image size for this model is 299x299.","94d33053":"## Build and train the model","216a0924":"# Use the code from Lab 2 to create the model that generates captions ","5fe704d7":"## Define global variables","1bd8061e":"# Our project uses 3 models\n1. Speech_model: takes a sound and returns a word \n2. CNN_model (Xception): Takes a pictures and returns a vector representation of the image \n3. RNN_model: Takes a vector representation of an image and a sequence and returns a caption\n\n"}}