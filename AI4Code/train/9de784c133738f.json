{"cell_type":{"ae6673bc":"code","20ad1700":"code","8f1c1868":"code","591dd173":"code","bbc40b74":"code","1333d87b":"code","1a529c8b":"code","7ab8b8a9":"code","dcfce316":"code","3bf50871":"code","c201064a":"code","044664ab":"code","a5cc7702":"code","20c2a20a":"code","f3835730":"code","5bd50aaf":"code","14a8ebd1":"code","538e159a":"code","14526f65":"code","5c6f74d7":"code","3a7f589d":"code","a43cb488":"code","c97e62fd":"code","8b38e2ca":"code","43200c69":"code","f56d310a":"code","3bc29469":"code","c04fab2f":"code","4a3a924f":"code","5b4ba0d7":"code","ce3fa91f":"code","98716749":"code","0a121af2":"code","3ed1c4b2":"code","5d00ac23":"code","c146ee9c":"code","8374ba9e":"code","6db02cc4":"code","90f6f29b":"code","2d4d8602":"code","20173576":"code","54e72351":"code","f1075be4":"code","dd158e28":"code","16c36d55":"code","dd84f8b0":"code","8e388ce2":"code","a1023367":"code","4b408247":"code","423ace62":"code","e2ae1cd4":"code","54ecf94d":"code","45b6ef11":"code","b43b46d3":"code","683a3fbc":"code","3d62d5fb":"code","f3a9850b":"code","2964fcd7":"code","f8516794":"code","5f690a20":"code","68b3f03c":"code","6bc4f960":"code","bc5b695c":"code","fe3c3c0b":"code","043458aa":"code","f380c9cf":"code","cca90028":"code","d110ffbd":"code","a0e37d6f":"code","ab97b53a":"code","9cfd2a38":"code","cf7ba9d8":"code","dc76d22b":"markdown","6e523100":"markdown","62cfc6ea":"markdown","962c6c8a":"markdown","82061434":"markdown","09d811fc":"markdown","28748e16":"markdown","63fa1378":"markdown","5eb6ec39":"markdown","f86982f9":"markdown","37d1282b":"markdown","ab59784b":"markdown","921f36ef":"markdown","e7582798":"markdown","6cd1ebd8":"markdown","871e4d3d":"markdown","06bc40fb":"markdown","6434a31b":"markdown","b5e055c5":"markdown","4a28743d":"markdown","26a57af9":"markdown","c05f1008":"markdown","121a25c4":"markdown","20107ce9":"markdown","49bca536":"markdown","dbf09798":"markdown","97fe3491":"markdown","41e22fa3":"markdown","7cda0878":"markdown","8940d1a0":"markdown","1c0edd11":"markdown","c51f76c8":"markdown","a13215f3":"markdown","241562cd":"markdown","3e267700":"markdown","af278231":"markdown","2afff29b":"markdown"},"source":{"ae6673bc":"from IPython.core.display import display, HTML\nfrom IPython.display import Image\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import normalize","20ad1700":"# Document and Word Vectors\nImage('https:\/\/mlwhiz.com\/images\/countvectorizer.png',width=800, height=400)","8f1c1868":"# create a list of documents\n# text = ['This is the first document'\n#         , 'This is the second second document'\n#         , 'And the third one'\n#         , 'Is it the first document again']\n\ntext=['This is good',\n     'This is bad',\n     'This is awesome']","591dd173":"from sklearn.feature_extraction.text import CountVectorizer","bbc40b74":"# create an instance of countvectorizer\nvect = CountVectorizer()  # shift tab ","1333d87b":"# when we print vect, we see its hyperparameters\nprint(vect)","1a529c8b":"# The vectorizer learns the vocabulary when we fit it with our documents. \n# This means it learns the distinct tokens (terms) in the text of the documents. \n# We can observe these with the method get_feature_names\n\nvect.fit(text)","7ab8b8a9":"print('ORIGINAL_SENTENCES: \\n {} \\n'.format(text))\nprint('FEATURE_NAMES: \\n {}'.format(vect.get_feature_names()))","dcfce316":"# Transform creates a sparse matrix, identifying the indices where terms are stores in each document\n# This sparse matrix has 4 rows and 11 columns\n\npd.DataFrame(vect.transform(text).toarray(),columns= ['awesome', 'bad', 'good', 'is', 'this'])[ ['this','is','good','bad','awesome']]","3bf50871":"print(vect.transform(text))","c201064a":"sparse_matrix_url = 'https:\/\/op2.github.io\/PyOP2\/_images\/csr.svg'\niframe = '<iframe src={} width=1000 height=200><\/iframe>'.format(sparse_matrix_url)\nHTML(iframe)","044664ab":"# This is easier to understand when we covert the sparse matrix into a dense matrix or pandas DataFrame\nvect.transform(text).toarray()","a5cc7702":"import pandas as pd\n\n# store the dense matrix\ndata = vect.transform(text).toarray()\n\n# store the learned vocabulary\ncolumns = vect.get_feature_names()\n\n# combine the data and columns into a dataframe\npd.DataFrame(data, columns=columns)[['this','is','good','bad','awesome']]","20c2a20a":"example_text = ['again we observe a document'\n               , 'the second time we have see this text']","f3835730":"# TODO\n","5bd50aaf":"vect = CountVectorizer()\nvect.fit_transform(text).toarray()","14a8ebd1":"Image('http:\/\/karlrosaen.com\/ml\/learning-log\/2016-06-20\/pipeline-diagram.png')","538e159a":"text = ['This is the first document'\n        , 'This is the second second document'\n        , 'And the third one'\n        , 'Is it the first document again']","14526f65":"vect = CountVectorizer()","5c6f74d7":"# by instantiating CountVectorizer with differnt parameters, we can change the vocabulary\n# lowercase determines if all words should be lowercase, setting it to False includes uppercase words\n\nvect = CountVectorizer(lowercase=False)\nvect.fit(text)\nprint(vect.get_feature_names())","3a7f589d":"# stops words determine if we should include common words (e.g. and, is, the) which show up in most documents\nvect = CountVectorizer(stop_words='english')\nvect.fit(text)\nprint(vect.get_feature_names())","a43cb488":"# stops words determine if we should include common words (e.g. and, is, the) which show up in most documents\nvect = CountVectorizer(stop_words=['first','second','third'])\nvect.fit(text)\nprint(vect.get_feature_names())","c97e62fd":"# stops words determine if we should include common words (e.g. and, is, the) which show up in most documents\nvect = CountVectorizer(vocabulary=['first','second','third'])\nvect.fit(text)\nprint(vect.get_feature_names())","8b38e2ca":"vect.transform(text).toarray()","43200c69":"vect = CountVectorizer(max_features=5)\nvect.fit(text)\nprint(vect.get_feature_names())","f56d310a":"vect = CountVectorizer(max_df=.5)\nvect.fit(text)\nprint(vect.get_feature_names())","3bc29469":"vect = CountVectorizer(min_df=.5)\nvect.fit(text)\nprint(vect.get_feature_names())","c04fab2f":"# max features determines the maximum number of features to display\nvect = CountVectorizer(ngram_range=(1,2), max_features=5)\nvect.fit(text)\nprint(vect.get_feature_names())","4a3a924f":"# max features determines the maximum number of features to display\nvect = CountVectorizer(binary=True)\nvect.fit_transform(['Two Two different words words']).toarray()","5b4ba0d7":"# max features determines the maximum number of features to display\nvect = CountVectorizer(analyzer='char', ngram_range=(2,2))\nvect.fit(text)\nprint(vect.get_feature_names())","ce3fa91f":"vect = CountVectorizer(max_features=5)\nvect.fit(text)\nprint(vect.get_feature_names())","98716749":"vect.vocabulary_","0a121af2":"vect.stop_words_","3ed1c4b2":"import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer","5d00ac23":"text","c146ee9c":"tfidf_vect = TfidfVectorizer()\npd.DataFrame(tfidf_vect.fit_transform(text).toarray(), columns=tfidf_vect.get_feature_names())","8374ba9e":"vect = CountVectorizer()\ntf = vect.fit_transform(text).toarray()\npd.DataFrame(tf,columns= vect.get_feature_names())","6db02cc4":"Image('http:\/\/www.science4all.org\/wp-content\/uploads\/2013\/10\/Graph-of-Logarithm-and-Exponential1.png')","90f6f29b":"tf","2d4d8602":"len(tf)","20173576":"vect = CountVectorizer(binary=True)\ncount_vec = vect.fit_transform(text).toarray()\npd.DataFrame(count_vec,columns= vect.get_feature_names())","54e72351":"len(count_vec)","f1075be4":"\n# idf calculation\nprint( np.log(len(count_vec) \/ count_vec.sum(axis=0)) )","dd158e28":"list(zip(vect.get_feature_names(),np.log(len(count_vec) \/ count_vec.sum(axis=0))))","16c36d55":"# when we use sum(axis=0) we take the sum of each column\n# as opposed to a scalar sum (single # result) of all values\ncount_vec.sum(axis=0)","dd84f8b0":"idf = np.log( (len(count_vec)+1) \/ (count_vec.sum(axis=0)+1) ) + 1\nprint(idf)","8e388ce2":"# value as stored from sklearn in tfidf_vect\nprint(tfidf_vect.idf_)","a1023367":"tfidf = pd.DataFrame(tf*idf,columns=tfidf_vect.get_feature_names())\ntfidf","4b408247":"# tf*idf is equivalent to using TfidfVectorizer without a norm\ntfidf_vect = TfidfVectorizer(norm=None)\npd.DataFrame(tfidf_vect.fit_transform(text).toarray())","423ace62":"from sklearn.preprocessing import normalize\n\npd.DataFrame(normalize(tfidf, norm='l2'))","e2ae1cd4":"# normalize()","54ecf94d":"# TFIDF Weighting in Sklearn\n'http:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#tfidf-term-weighting'\n\n# tf*idf is equivalent to using TfidfVectorizer without a norm\ntfidf_vect = TfidfVectorizer(norm='l2')\npd.DataFrame(tfidf_vect.fit_transform(text).toarray())","45b6ef11":"import os\nos.listdir('..\/input')","b43b46d3":"path='..\/input\/usi-nlp-practicum-2\/imdb_train.csv'\ndata= pd.read_csv(path)\ndata.head()","683a3fbc":"labels=data['sentiment'].unique().tolist()\nlabel2id={ lbl:i for i,lbl in enumerate(labels)}\nid2label={ i:lbl for i,lbl in enumerate(labels)}\nprint(label2id), print(id2label)","3d62d5fb":"data['label']=data['sentiment'].map(label2id)\ndata.head()","f3a9850b":"data.shape","2964fcd7":"import spacy, string\nnlp = spacy.load('en')\npunctuations = string.punctuation\nfrom spacy.lang.en.stop_words import STOP_WORDS\ndef cleanup_text(doc):\n    doc = nlp(doc, disable=['parser', 'ner'])\n    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n#   print (tokens)\n    tokens = \" \".join([i for i in tokens if i not in STOP_WORDS and len(i)>2]) \n#     tokens = ' '.join(tokens)\n    return tokens","f8516794":"print(cleanup_text(data['review'][1]))","5f690a20":"data= data.sample(1000).reset_index(drop=True)","68b3f03c":"from tqdm import tqdm\ntqdm.pandas()","6bc4f960":"data['clean_review']=data['review'].progress_apply(lambda x:cleanup_text(x))\ndata.head()","bc5b695c":"# split the dataset into training and validation datasets \nfrom sklearn import model_selection\ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(data['clean_review'], data['label'],test_size=0.2, random_state=42)","fe3c3c0b":"train_x.head()","043458aa":"# word level tf-idf\ntfidf_vect = TfidfVectorizer(analyzer='word',max_df=0.95, min_df=5,ngram_range=(1,1),\n                             max_features=1500)\n\ntfidf_vect.fit(train_x) #--- -\nxtrain_tfidf =  tfidf_vect.transform(train_x)\nxvalid_tfidf =  tfidf_vect.transform(valid_x)","f380c9cf":"xtrain_tfidf.shape,xvalid_tfidf.shape","cca90028":"features_name=tfidf_vect.get_feature_names()\nfeatures_name[:30]","d110ffbd":"from sklearn import ensemble\n# RF on Word Level TF IDF Vectors\nmodel=ensemble.RandomForestClassifier(n_estimators=50, random_state=0)\nmodel.fit(xtrain_tfidf, train_y)\n","a0e37d6f":"from sklearn import metrics\npredictions = model.predict(xvalid_tfidf)\naccuracy=metrics.accuracy_score(predictions, valid_y)\nprint (\"Random Forest accuracy for validation: \", accuracy)","ab97b53a":"Image('http:\/\/cs.carleton.edu\/cs_comps\/0910\/netflixprize\/final_results\/knn\/img\/knn\/cos.png')","9cfd2a38":"from sklearn.metrics.pairwise import linear_kernel\n\ndef find_similar(tfidf_matrix, index, top_n = 5):\n    cosine_similarities = linear_kernel(tfidf_matrix[index:index+1], tfidf_matrix).flatten()\n    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n    return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]","cf7ba9d8":"find_similar(xtrain_tfidf, 1)","dc76d22b":"### min_df\n\n- float in range [0.0, 1.0] or int, default=1\n- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.","6e523100":"## Customize the Transformer\n\nDuring the process of vectorizing the text, we can apply numerous transformations to modify the text and resulting vectors. ","62cfc6ea":"### fit_transform\n\n- we can combine the training and transformation into a single method. This is a common process in the sklearn api, as we often want to learn something from a training data set and apply the results to testing or production data\n\nfit().transform()","962c6c8a":"#### inverse document frequency (idf)\n\nCalculation: log(\\# document in the corpus \/ # documents where the term appears)\n\n- Numerator: **The # of documents in the corpus has no effect** as it is the same for all terms\n- Denominator: **As the # of documents in which the term appears increases, the idf decreases**; thus terms that show up in many different documents (e.g. stop words) recieve low tfidf scores as they are not important terms to define the meaning of the document \n- As a sub-linear function, we take the **log because the relevance does not increase proportionally with the term frequency**. As an example if a term shows up in 1M docs or in 2M docs, the effect is not the same as if it has shown up in 1 doc or 2 docs times respectively. In other words there is a relative threshold.","82061434":"Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n\nIn order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n\n- **tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n\n- **counting** the occurrences of tokens in each document.\n\n- **normalizing** and weighting with diminishing importance tokens that occur in the majority of samples \/ documents.\n\nSources: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html","09d811fc":"### Step 4 - transform ","28748e16":"### Bag of Words\n\nWe call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or \u201cBag of n-grams\u201d representation. Documents are described by word occurrences while `completely ignoring the relative position information of the words in the document.`\n\nA corpus of documents can thus be represented by a **matrix with one row per document and one column per token (e.g. word)** occurring in the corpus.","63fa1378":"#### scikit-learn calculation modifications\n\nscikit-learn further modifies the caluclation for adding one to the numerator, denominator, and log to avoid divide by zero errors","5eb6ec39":"### TFIDF\n\nIn a large text corpus, some words will be very present (e.g. \u201cthe\u201d, \u201ca\u201d, \u201cis\u201d in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n\nIn order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf\u2013idf transform.\nTf means term-frequency while tf\u2013idf means term-frequency times inverse document-frequency: \n\n- tf-idf(t,d) = tf(t,d) * idf(t)","f86982f9":"**tf-idf on imdb dataset**","37d1282b":"### Sparsity\n\nAs most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them).\n\nFor instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.","ab59784b":"### Limitations of the Bag of Words representation\n\nA collection of unigrams (what bag of words is) cannot capture phrases and multi-word expressions, effectively disregarding any word order dependence. Additionally, the bag of words model doesn\u2019t account for potential misspellings or word derivations.\n\nN-grams to the rescue! Instead of building a simple collection of unigrams (n=1), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted.\n\nOne might alternatively consider a collection of character n-grams, a representation resilient against misspellings and derivations.\n\nFor example, let\u2019s say we\u2019re dealing with a corpus of two documents: ['words', 'wprds']. The second document contains a misspelling of the word \u2018words\u2019. A simple bag of words representation would consider these two as very distinct documents, differing in both of the two possible features. A character 2-gram representation, however, would find the documents matching in 4 out of 8 features, which may help the preferred classifier decide better:","921f36ef":"### stop\\__words_\\_ \n- set\n- Terms that were ignored because they either:\n - occurred in too many documents (max_df)\n - occurred in too few documents (min_df)\n - were cut off by feature selection (max_features)","e7582798":"**preprocessing function**","6cd1ebd8":"#### term frequency (tf)\n\nHow often does each term exist in each document. \n\nTerm frequency is the numerator; thus, the tfidf score for a term increases in documents where it is frequent.","871e4d3d":"#### term frequency * inverse document frequency (tf*idf)","06bc40fb":"Cosine Similarity\nInformation Retrieval","6434a31b":"#### term vector normalization\n\nThe use of the simple tfidf does not account for the length of the document. Additionally it provides opportunities for spammers to repeat the term many times to make it seem more important in a specific document (e.g. to improve google search ranking)\n\nTo solve these issues, we normalize each vector. By default TfidfVectorizer uses an 'l2' normalization.","b5e055c5":"## Text Vectorization\n\n##### Author: Alex Sherman (alsherman@deloitte.com) |  Vikas Kumar (vikkumar@deloitte.com)","4a28743d":"### max_features\n- int or None, default=None\n- If not None, build a vocabulary that only consider the top  max_features ordered by term frequency across the corpus.","26a57af9":"In scikit-learn attributes are often provided to store information of the instance of the transformer or model. \n\nMany attributes are only available after the model is fit. For instance the learned vocabulary does not exist in Countvectorizer until text data has been provided with the fit method. Until the data is provided these attributes do not exist. The notation for these learned attributes is a trailing underscore after the attribute name (e.g. vocabulary_). ","c05f1008":"### Step 1 - import from sklearn","121a25c4":"### binary\n\n- boolean, default=False\n- If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.","20107ce9":"### TFIDF Analysis\nAs we look at the tfidf score (which have a range of 0-1), high score occur for words that show up frequently in specific sentence but infrequenty overall. Low score occur in words that show up frequenty across all documents.\n\n- **'Second' has a high score** as it shows up twice in document two and not in any other documents\n- **'The' has a low score** as it show up in all documents","49bca536":"### analyzer\n\n- String, {\u2018word\u2019, \u2018char\u2019, \u2018char_wb\u2019} or callable\n- Specifies whether to use n_grams of words or characters\n- Character n_grams are useful in certain content, such as genomics with DNA sequences (e.g. GCTATCAFF...)","dbf09798":"### vocabulary\n\n- Mapping or iterable, optional\n- Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. ","97fe3491":"## Exercise\n\nUse the trained CountVectorizer to vectorize the following sentences. Create a dataframe with the dense results.","41e22fa3":"### vocabulary_\n\n- dict\n- A mapping of terms to feature indices.","7cda0878":"### Term-Frequency Problems\n\n\"The **main problem with the term-frequency approach is that it scales up frequent terms and scales down rare terms which are empirically more informative than the high frequency terms.**\nThe basic intuition is that a term that occurs frequently in many documents is not a good discriminator; the important question here is: why would you, in a classification problem for instance, emphasize a term which is almost present in the entire corpus of your documents ?\n\nThe tf-idf weight comes to solve this problem. **What tf-idf gives is how important is a word to a document**\nin a collection, and that\u2019s why tf-idf incorporates local and global parameters, because it takes in consideration not only the isolated term but also the term within the document collection. **What tf-idf then does to solve that problem, is to scale down the frequent terms while scaling up the rare terms; a term that occurs 10 times more than another isn\u2019t 10 times more important than it, that\u2019s why tf-idf uses the logarithmic scale to do that.\"**\n\nSource: http:\/\/blog.christianperone.com\/2011\/10\/machine-learning-text-feature-extraction-tf-idf-part-ii\/","8940d1a0":"# Attributes","1c0edd11":"### lowercase\n- boolean, True by default\n- Convert all characters to lowercase before tokenizing.","c51f76c8":"### ngram_range\n\n- tuple (min_n, max_n)\n\n- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.","a13215f3":"### Step 3 - fit ","241562cd":"### stop_words\n\n- string {\u2018english\u2019}, list, or None (default)\n - If None, no stop words will be used. \n - If \u2018english\u2019, a built-in stop word list for English is used.\n - If list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n- max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words \n","3e267700":"### max_df\n- float in range [0.0, 1.0] or int, default=1.0\n- When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.","af278231":"### Step 2 - instantiate","2afff29b":"## TFIDF Calculation"}}