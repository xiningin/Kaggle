{"cell_type":{"2813d05f":"code","9a3b3989":"code","d3e4dcb4":"code","2229a4ae":"code","f84c89d2":"code","f0221c53":"code","71f889a2":"code","1438c6af":"code","077b1c8e":"code","2f6ff7d9":"markdown","1b700e4b":"markdown","5d8d25b3":"markdown","69c22ab7":"markdown","013929d1":"markdown","076d37a3":"markdown","5e578460":"markdown","ec529a90":"markdown","d4b115d0":"markdown","53aeeb26":"markdown","7f6da398":"markdown","50f22656":"markdown","9afb15c9":"markdown","86949bc8":"markdown","05bad6fd":"markdown","34d5cf7d":"markdown"},"source":{"2813d05f":"import pandas as pd\nfrom sklearn.feature_selection import VarianceThreshold\nX_train = pd.read_csv(\"..\/input\/santaner\/train.csv\", nrows=35000)\nX_test = pd.read_csv(\"..\/input\/santaner\/test.csv\", nrows=15000)\n\n# drop TARGET label from X_train\nX_train.drop(labels=['TARGET'], axis=1, inplace = True)\n\n# check shape of training and test sets\nX_train.shape, X_test.shape\n\n\nvar_thresh = VarianceThreshold(threshold=0)  # 0.1 indicates 99% of observations approximately\nvar_thresh.fit(X_train)\n# get_support is a boolean vector that indicates which features \n# are retained. If we sum over get_support, we get the number\n# of features that are not quasi-constant\nsum(var_thresh.get_support())\n\"\"\"\nalternative way of doing the above operation:\nlen(X_train.columns[sel.get_support()])\n\"\"\"\n# finally we can print the quasi-constant features\nprint(\n    len([x for x in X_train.columns if x not in X_train.columns[var_thresh.get_support()]])\n)\n\n[x for x in X_train.columns if x not in X_train.columns[var_thresh.get_support()]]\n","9a3b3989":"# we can then drop these columns from the train and test sets\nX_train = var_thresh.transform(X_train)\nX_test = var_thresh.transform(X_test)","d3e4dcb4":"# check the shape of training and test set\nX_train.shape, X_test.shape","2229a4ae":"\"\"\"\nWe can also remove\nfeatures which have a high correlation.\nFor calculating the correlation \nbetween different numerical features,\nyou can use the Pearson correlation.\n\"\"\"\nimport pandas as pd \nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\n\n# fetch a regression dataset\ndata = fetch_california_housing()\nX = data[\"data\"]\ncol_names = data[\"feature_names\"]\ny = data[\"target\"]\n\n# convert to pandas dataframe\ndf = pd.DataFrame(X, columns=col_names)\n# introduce a highly correlated column\ndf.loc[:, \"MedInc_Sqrt\"] = df.MedInc.apply(np.sqrt)\n\n# get correlation matrix (pearson)\ndf.corr()","f84c89d2":"from sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import SelectPercentile\n\n\nclass UnivariateFeatureSelection:\n    def __init__(self, n_features, problem_type, scoring):\n        \"\"\"\n        Custom univariate feature selection wrapper on\n        different univariate feature selection models from scikit-learn.\n        :param n_features: SelectPercentile if float else SelectKBest \n        :param problem_type: classification or regression\n        :param scoring: scoring function, string\n        \"\"\"\n        # for a given probelm type, there are only\n        # a few valid scoring methods\n        # methods if you wish\n        if problem_type == \"classification\":\n            valid_scoring = {\n                \"f_classif\" : f_classif,\n                \"chi2\" : chi2,\n                \"mutual_info_classif\" : mutual_info_classif\n            }\n        else:\n            valid_scoring = {\n                \"f_regression\" : f_regression,\n                \"mutual_info_regression\" : mutual_info_regression\n            }\n            \n        # raise exception if we do not have a valid scoring method\n        if scoring not in valid_scoring:\n            raise Exception(\"Invalid scoring function\")\n            \n        # if n_features is int, we use selectKbest\n        # if n_features is float, we use selectpercentile\n        # please note that is is int in both cases in sklearn\n        if isinstance(n_features, int):\n            self.selection = SelectKBest(\n                valid_scoring[scoring],\n                k=n_features\n            )\n        elif isinstance(n_features, float):\n            self.selection = SelectPercentile(\n                valid_scoring[scoring],\n                percentile=int(n_features * 100)\n            )\n        else:\n            raise Exception(\"Invalid type of feature\")\n            \n    # same fit function\n    def fit(self, X, y):\n        return self.selection.fit(X, y)\n    \n    # same transform function\n    def transform(self, X):\n        return self.selection.transform(X)\n    \n    # same fit_transform function\n    def fit_transform(self, X, y):\n        return self.selection.fit_transform(X, y)\n    \nufs = UnivariateFeatureSelection(\n    n_features=0.1,\n    problem_type=\"regression\",\n    scoring=\"f_regression\"\n)\nufs.fit(X,y)\nX_transformed = ufs.transform(X)\nprint(X_transformed)","f0221c53":"# Let's see how it workds by looking at how its implemented.\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn.datasets import make_classification\n\nclass GreddyFeatureSelection:\n    \"\"\"\n    A simple and custom class \n    for greedy feature selection.\n    You will need to modify it quite \n    a bit to make it suitable for your dataset.\n    \"\"\"\n    def evaluate_score(self, X, y):\n        \"\"\"\n        This function evaluates model on data and returns Area Under ROC Curve (AUC)\n        NOTE: We fit the data and calculate AUC on same data. \n        WE ARE OVERFITTING HERE.\n        But this is also a way to achieve greedy selection.\n        k-fold will take k times longer.\n        If you want to implement it in really correct way, \n        calculate OOF AUC and return mean AUC over k folds.\n        This requires only a few lines of change and has been\n        shown a few times in this book.\n        \n        :param X: training data\n        :param y: targets\n        :return: overfitted area under the roc curve\n        \"\"\"\n        # fit the logistic regression model,\n        # and calculate AUC on same data\n        # again: BEWARE\n        # you can choose any model that suits your data.\n        \n        model = linear_model.LogisticRegression()\n        model.fit(X, y)\n        predictions = model.predict_proba(X)[:, 1]\n        auc = metrics.roc_auc_score(y, predictions)\n        return auc\n    \n    def feature_selection(self, X, y):\n        \"\"\"\n        This function does the actual greedy selection \n        :param X: data, numpy array\n        :param y: targets, numpy array\n        :return: (best scores, best features)\n        \"\"\"\n        # initialize good features list\n        # and best scores to keep track of both\n        good_features = []\n        best_scores = []\n        \n        # calculate the number of features\n        num_features = X.shape[1]\n        \n        # infinite loop\n        while True:\n            # initialize best feature and score of this loop\n            this_feature = None\n            best_score = 0\n            \n            # loop over all faetures\n            for feature in range(num_features):\n                # if feature is already in good features,\n                # skip this for loop\n                if feature in good_features:\n                    continue\n                # selected features are all good features till now\n                # and current feature\n                selected_features = good_features + [feature]\n                # remove all other features from data\n                xtrain = X[:, selected_features]\n                # calculate the score, in our case, AUC\n                score = self.evaluate_score(xtrain, y)\n                # if score is greater than the best score\n                # of this loop, change best score and best feature\n                if score > best_score:\n                    this_feature = feature\n                    best_score = score\n                    \n                # if we have selected a feature, add it\n                # to the good feature list and update best scores list\n                if this_feature != None:\n                    good_features.append(this_feature)\n                    best_scores.append(best_score)\n                    \n                # if we didn't improve during the previous round,\n                # exit the while loop\n                if len(best_scores) > 2:\n                    if best_scores[-1] < best_scores[-1]:\n                        break\n            # return best scores and good features\n            # why do we remove the last data point?\n            return best_scores[:-1], good_features[:-1]\n        \n    def __call__(self, X, y):\n        \"\"\"\n        call function will call the class on a set of arguments\n        \"\"\"\n        # select features, return scores and selected indices\n        scores, features = self.feature_selection(X, y)\n        # transform data with selected features\n        return X[:, features], scores\n        \n\n        \nif __name__ == \"__main__\":\n    # generate binary classification data\n    X, y = make_classification(\n        n_samples=1000,\n        n_features=100\n    )\n    \n    X_transformed, scores = GreddyFeatureSelection()(X, y)\n\n    print(X_transformed.shape)\n    print(list(scores))\n        ","71f889a2":"import pandas as pd\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import fetch_california_housing\n\n# fetch a regression dataset\ndata = fetch_california_housing()\nX = data[\"data\"]\ncol_names = data[\"feature_names\"]\ny = data[\"target\"]\n\n# initiliaze the model\nmodel = LinearRegression()\n# initialize RFE\nrfe = RFE(\n    estimator=model,\n    n_features_to_select=3\n)\n\n# fit RFE\nrfe.fit(X, y)\n\n# get the transformed data with\n# selected columns\nX_transformed = rfe.transform(X)\nX_transformed","1438c6af":"# Let'see how we can get feature importance from a model\n# like Random Forest\n\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt\nimport numpy as np\n# fetch a regression dataest\n# in diabetes data we predict diavetes progression\n# after one year based on some features\ndata = load_diabetes()\nX = data[\"data\"]\ncol_names = data[\"feature_names\"]\ny = data[\"target\"]\nprint(col_names)\n# initialize the model\nmodel = RandomForestRegressor()\n\n# fit the model\nmodel.fit(X, y)\n\n# Feature importances from random forest(or any model) and be plotted as follows.\nimportances = model.feature_importances_\nidxs = np.argsort(importances)\nplt.title('Feature Importances')\nplt.barh(range(len(idxs)), importances[idxs], align='center')\nplt.yticks(range(len(idxs)), [col_names[i] for i in idxs])\nplt.xlabel('Random Forest Feature Importances')\nplt.show()","077b1c8e":"# we select the features using default parameters in SelectFromModel.\n\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import SelectFromModel\n# fetch a regression dataset\n# in diabetes data we predict diabetes progression # after one year based on some features\ndata = load_diabetes()\nX = data[\"data\"]\ncol_names = data[\"feature_names\"]\ny = data[\"target\"]\n# initialize the model\nmodel = RandomForestRegressor()\n\n# select from the model\nsfm = SelectFromModel(estimator=model)\nX_transformed = sfm.fit(X, y)\n\n# see what features were selected\nsupport = sfm.get_support()\n\n# get features names\nprint([\n    x for x, y in zip(col_names, support) if y == True\n])","2f6ff7d9":"Note: That should take care of most of your univariate feature selection needs. Please note that it\u2019s usually better to create less and important features than to create hundreds of features in the first place. Univariate feature selection may not always perform well. Most of the time, people prefer doing feature selection using a machine learning model. Let\u2019s see how that is done.","1b700e4b":"- Well, selecting the best features from the model is nothing new. You can choose features from one model and use another model to train.","5d8d25b3":"- We can see how by removing constant features, we managed to reduced the feature space quite a bit.\n","69c22ab7":"When we are doing recursive feature elimination, in each iteration, we remove the feature which has the feature importance or the feature which has a coefficient close to 0. Please remember that when you use a model like logistic regression for binary classification, the coefficients for features are more positive if they are important for the positive class and more negative if they are important for the negative class.","013929d1":"In greedy feature selection, <\/br>\nthe first step is to choose a model.<\/br> The second step is to select a loss\/scoring function. <\/br>And the third and final step is to iteratively evaluate each feature and add it to the list of \u201cgood\u201d features if it improves loss\/score.","076d37a3":"- We can see that there are 51 columns \/ variables that are constant. This means that 51 variables show the same value, just one value, for all the observations of the training set.\n<\/br><\/br>\n- We then use the transform function to reduce the training and testing sets.","5e578460":"One more thing that we are missing here is feature selection using models that have L1 (Lasso) penalization. When we have L1 penalization for regularization, most coefficients will be 0 (or close to 0), and we select the features with non-zero coefficients. You can do it by just replacing random forest in the snippet of selection from a model with a model that supports L1 penalty, e.g. lasso regression. All tree-based models provide feature importance so all the model-based snippets shown in this chapter can be used for XGBoost, LightGBM or CatBoost. The feature importance function names might be different and may produce results in a different format, but the usage will remain the same. In the end, you must be careful when doing feature selection. Select features on training data and validate the model on validation data for proper selection of features without overfitting the model.","ec529a90":"We see that the feature MedInc_Sqrt has a very high correlation with MedInc. We can thus remove one of them.\n","d4b115d0":"#  ***The simplest form of feature selection that uses a model for selection is known as greedy feature selection.***","53aeeb26":"Having too many features pose a problem well known as the curse of dimensionality. <\/br><\/br>\nThe simplest form of selecting features would be to <b>remove features with very low variance.<\/b><\/br><\/br>\nIf the features have a very low variance (i.e. very close to 0), they are close to being constant and thus, do not add any value to any model at all. It would just be nice to get rid of them and hence lower the complexity. Please note that the variance also depends on scaling of the data.\n","7f6da398":"In the previous method, we started with one feature and kept adding new features, but in RFE, we start with all features and keep removing one feature in every iteration that provides the least value to a given model. But how to do we know which feature offers the least value? Well, if we use models like linear support vector machine (SVM) or logistic regression, we get a coefficient for each feature which decides the importance of the features. In case of any tree-based models, we get feature importance in place of coefficients. In each iteration, we can eliminate the least important feature and keep eliminating it until we reach the number of features needed.","50f22656":"But you must keep in mind that this is known as greedy feature selection for a reason. This feature selection process will fit a given model each time it evaluates a feature. The computational cost associated with this kind of method is very high. It will also take a lot of time for this kind of feature selection to finish. And if you do not use this feature selection properly, then you might even end up overfitting the model.","9afb15c9":"# **Another greedy approach is known as recursive feature elimination (RFE).**","86949bc8":"- we see that these are the top- 2 features.","05bad6fd":"We saw two different greedy ways to select features from a model. But you can also fit the model to the data and select features from the model by the feature coefficients or the importance of features. If you use coefficients, you can select a threshold, and if the coefficient is above that threshold, you can keep the feature else eliminate it.","34d5cf7d":"And now we can move to some univariate ways of feature selection. Univariate feature selection is nothing but a scoring of each feature against a given target. Mutual information, ANOVA F-test and chi2 are some of the most popular methods for univariate feature selection. There are two ways of using these in scikit- learn.\n- SelectKBest: It keeps the top-k scoring features\n- SelectPercentile: It keeps the top features which are in a percentage\nspecified by the user\n<\/br><\/br>\n\nIt must be noted that you can use chi2 only for data which is non-negative in nature.\n"}}