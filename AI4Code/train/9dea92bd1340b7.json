{"cell_type":{"f05787ce":"code","b08f5603":"code","d00ee708":"code","f9341ff1":"code","efb0b9ac":"code","abb58a1d":"code","da4f54b4":"code","88294d7a":"code","d300cb96":"code","0a4d4e6b":"code","05843843":"code","1f91d052":"code","8760b83a":"code","03f36f9d":"code","33b7277c":"code","51189607":"code","e36459d8":"code","6516f1fb":"code","b4d51f51":"code","8b1cfa84":"code","1416d368":"code","99b5c9a2":"code","5d23b6aa":"code","ddc66ac8":"code","1ebd5577":"code","64d9c5a6":"code","aaaa1914":"code","11765c32":"code","08e85a8b":"code","93155b89":"code","8f3029fb":"code","7782ffd2":"code","3b847ce1":"code","ab8be33f":"code","feb18cf7":"code","e9c75595":"code","96dc2fa7":"code","93212320":"code","e83cf2c3":"code","be29880d":"code","556a1dc0":"code","9fbe99b6":"code","f574d0c2":"code","ed63beee":"code","5e05f8db":"code","d294539a":"code","584f23c8":"code","2b74c034":"code","199c320a":"code","22841b18":"code","d5eb9d40":"code","e539558d":"code","bc6fc73b":"code","31e6c726":"code","fcbb2270":"code","62687d98":"code","9253a084":"code","03b82d00":"code","f4e3f118":"code","ecf41c55":"code","abb9b96c":"code","fbaadc59":"code","08eb019e":"code","dae6ee7f":"code","7e11e927":"code","0efe6e72":"code","9de6631a":"code","1b545893":"code","93ba4bd4":"markdown","6d73aa1b":"markdown","c13456bb":"markdown","98bd1e19":"markdown","2d74433d":"markdown","45dbe5a9":"markdown","2b20ccfc":"markdown","3b77370b":"markdown","95780cb8":"markdown","fec01080":"markdown","c9e21fb9":"markdown","f335190b":"markdown","53e14a7e":"markdown","2d718375":"markdown","1b94234c":"markdown","6dc7d673":"markdown","677c4ec5":"markdown","6d7e8d46":"markdown","90456365":"markdown","f5e7511d":"markdown","c5232947":"markdown","a31ec814":"markdown","934970a9":"markdown","5559733a":"markdown","ab992eab":"markdown","638a628f":"markdown","a0da291c":"markdown","2bca09a8":"markdown"},"source":{"f05787ce":"from catboost import CatBoostClassifier\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder, StandardScaler, MinMaxScaler\nimport pandas as pd\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\nfrom pandas.tseries.holiday import *\nfrom pandas.tseries.offsets import CustomBusinessDay\n\nfrom sklearn.model_selection import train_test_split, GroupShuffleSplit\nimport numpy as np\nfrom tensorflow.keras.metrics import TopKCategoricalAccuracy, Precision, SparseTopKCategoricalAccuracy # @4\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Import Keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, BatchNormalization, Dropout, Embedding, Flatten , Input , Concatenate ,Dense ,  Reshape \nfrom keras.models import Model\nfrom keras.optimizers import Adam, SGD\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.regularizers import l2\nfrom keras.utils import np_utils\nfrom time import time\n\n\nimport os\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom numpy.random import seed\nseed(42)\ntf.random.set_seed(42)\nfrom keras.callbacks import *\nfrom keras import backend as K\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\npd.set_option(\"display.max_columns\", 90)","b08f5603":"# !nvidia-smi -L","d00ee708":"## https:\/\/www.tensorflow.org\/guide\/mixed_precision ## TF mixed precision - pytorch requires other setup\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\n\npolicy = mixed_precision.Policy('mixed_float16')\nmixed_precision.set_policy(policy)\n## will need to correct in places, e.g.: \n## outputs = layers.Activation('softmax', dtype='float32', name='predictions')(x)","f9341ff1":"MIN_TARGET_FREQ = 50 # drop target\/city_id values that appear less than this many times, as final step's target \nKEEP_TOP_K_TARGETS = 1500 # keep K most frequent city ID targets (redundnat with the above, )\n\n## (some) categorical variables that appear less than this many times will be replaced with a placeholder value!\n## Includes CITY id (but done after target filtering, to avoid creating a \"rare class\" target:\nLOW_COUNT_THRESH = 9\n\nRUN_TABNET = True\nmax_epochs = 50\n\n\nFAST_RUN = True\nif FAST_RUN:\n    KEEP_TOP_K_TARGETS = 40\n    max_epochs = 2","efb0b9ac":"# most basic categorical columns , without 'user_id', , 'utrip_id' ordevice_class - used for count encoding\/filtering\nBASE_CAT_COLS = ['city_id',  'affiliate_id', 'booker_country', 'hotel_country']\n\n\n### features to get lags for. Not very robust. May want different feats for lags before -1\nLAG_FEAT_COLS = ['city_id', 'device_class',\n       'affiliate_id', 'booker_country', 'hotel_country', \n       'duration', 'same_country', 'checkin_day', 'checkin_weekday',\n       'checkin_week',\n        'checkout_weekday','checkout_week',\n       'city_id_count', 'affiliate_id_count',\n       'booker_country_count', 'hotel_country_count', \n       'checkin_month_count', 'checkin_week_count', 'city_id_nunique',\n       'affiliate_id_nunique', 'booker_country_nunique',\n       'hotel_country_nunique', 'city_id_rank_by_hotel_country',\n       'city_id_rank_by_booker_country', 'city_id_rank_by_affiliate',\n       'affiliate_id_rank_by_hotel_country',\n       'affiliate_id_rank_by_booker_country', \n       'booker_country_rank_by_hotel_country',\n       'booker_country_rank_by_affiliate',\n       'hotel_country_rank_by_hotel_country',\n       'hotel_country_rank_by_booker_country',\n       'hotel_country_rank_by_affiliate',\n       'checkin_month_rank_by_hotel_country',\n       'checkin_month_rank_by_booker_country',\n       'checkin_month_rank_by_affiliate']\n\n## smaller subset of the more important lag features - may not need affiliate_id. And booker country doesn't change ? \nMINI_LAG_FEAT_COLS = ['city_id','hotel_country',\"duration\"]\n","abb58a1d":"# https:\/\/stackoverflow.com\/questions\/33907537\/groupby-and-lag-all-columns-of-a-dataframe\n# https:\/\/stackoverflow.com\/questions\/62924987\/lag-multiple-variables-grouped-by-columns\n## lag features with groupby over many columns: \ndef groupbyLagFeatures(df:pd.DataFrame,lag:[]=[1,2],group=\"utrip_id\",lag_feature_cols=[]):\n    \"\"\"\n    lag features with groupby over many columns\n    https:\/\/stackoverflow.com\/questions\/62924987\/lag-multiple-variables-grouped-by-columns\"\"\"\n    if len(lag_feature_cols)>0:\n        df=pd.concat([df]+[df.groupby(group)[lag_feature_cols].shift(x).add_prefix('lag'+str(x)+\"_\") for x in lag],axis=1)\n    else:\n         df=pd.concat([df]+[df.groupby(group).shift(x).add_prefix('lag'+str(x)+\"_\") for x in lag],axis=1)\n    return df\n\ndef groupbyFirstLagFeatures(df:pd.DataFrame,group=\"utrip_id\",lag_feature_cols=[]):\n    \"\"\"\n    Get  first\/head value lag-like of features with groupby over columns. Assumes sorted data!\n    \"\"\"\n    if len(lag_feature_cols)>0:\n        df=pd.concat([df]+[df.groupby(group)[lag_feature_cols].transform(\"first\").add_prefix(\"first_\")],axis=1)\n    else:\n        df=pd.concat([df]+[df.groupby(group).transform(\"first\").add_prefix(\"first_\")],axis=1)\n    return df\n\n\n######## Get n most popular items, per group\ndef most_popular(group, n_max=4):\n    \"\"\"Find most popular hotel clusters by destination\n    Define a function to get most popular hotels for a destination group.\n\n    Previous version used nlargest() Series method to get indices of largest elements. But the method is rather slow.\n    Source: https:\/\/www.kaggle.com\/dvasyukova\/predict-hotel-type-with-pandas\n    \"\"\"\n    relevance = group['relevance'].values\n    hotel_cluster = group['hotel_cluster'].values\n    most_popular = hotel_cluster[np.argsort(relevance)[::-1]][:n_max]\n    return np.array_str(most_popular)[1:-1] # remove square brackets\n\n# A utility method to create a tf.data dataset from a Pandas Dataframe\ndef df_to_dataset(dataframe, shuffle=True, batch_size=64,target_col=\"target\"):\n    \"\"\"\n    Wrap dataframes with tf.data. \n    This will enable us to use feature columns as a bridge to map from the columns in a dataframe to features used to train the model.\n    https:\/\/www.tensorflow.org\/tutorials\/structured_data\/feature_columns#create_an_input_pipeline_using_tfdata\n    \"\"\"\n    dataframe = dataframe.copy()\n    labels = dataframe.pop(target_col)\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    return ds\n","da4f54b4":"if FAST_RUN:\n    df = pd.read_csv(\"\/kaggle\/input\/booking\/booking_train_set.csv\",nrows= 123456,\n                     parse_dates=[\"checkin\",\"checkout\"],infer_datetime_format=True).drop([\"Unnamed: 0\"],axis=1)\nelse:\n    df = pd.read_csv(\"\/kaggle\/input\/booking\/booking_train_set.csv\",\n                     parse_dates=[\"checkin\",\"checkout\"],infer_datetime_format=True).drop([\"Unnamed: 0\"],axis=1)\ndf_test_set = pd.read_csv(\"\/kaggle\/input\/booking\/sample_test_set.csv\", parse_dates=[\"checkin\",\"checkout\"],infer_datetime_format=True)\ndf_sample = pd.read_csv(\"\/kaggle\/input\/booking\/sample_truth.csv\")\ndf_submission = pd.read_csv(\"\/kaggle\/input\/booking\/submission.csv\")\n\ndf.sort_values([\"user_id\",\"checkin\",\"utrip_id\"],inplace=True)\n\ndf","88294d7a":"# Country of the hotel \nprint(df['hotel_country'].nunique())\ndf['hotel_country'].value_counts().head(30).plot.bar()","d300cb96":"df[['hotel_country', 'city_id']].value_counts().head(30)","0a4d4e6b":"df.groupby('hotel_country').count()['affiliate_id'].plot()","05843843":"df[\"duration\"] = (df[\"checkout\"] - df[\"checkin\"]).dt.days\ndf[\"same_country\"] = (df[\"booker_country\"]==df[\"hotel_country\"]).astype(int)\n\ndf[\"checkin_day\"] = df[\"checkin\"].dt.day\ndf[\"checkin_weekday\"] = df[\"checkin\"].dt.weekday\ndf[\"checkin_week\"] = df[\"checkin\"].dt.isocalendar().week.astype(int) ## week of year\ndf[\"checkin_month\"] = df[\"checkin\"].dt.month\ndf[\"checkin_year\"] = df[\"checkin\"].dt.year-2015\n\ndf[\"checkout_weekday\"] = df[\"checkout\"].dt.weekday\ndf[\"checkout_week\"] = df[\"checkout\"].dt.isocalendar().week.astype(int) ## week of year\ndf[\"checkout_day\"] = df[\"checkout\"].dt.day ## day of month\n\ndf[\"checkin_quarter\"] = df[\"checkin\"].dt.quarter\n\nseason = []\nfor _ , s in enumerate(df.checkin_month):\n    if    3 < s < 5 :\n        season.append(2) # spring \n    elif 6 < s < 8:\n        season.append(3) # summer\n    elif 9 < s < 11:\n        season.append(4) # fall\n    else:\n        season.append(1) # winter\n        \n        \n    \n#   !  add Booleans for whether it\u2019s the start\/end of a month\/quarter\/year). \n    \ndf['season'] = season\n\n## cyclical datetime embeddings\n## drop originakl variables? \n## TODO:L add for other variables, +- those that we'll embed (week?)\n\ndf['checkin_weekday_sin'] = np.sin(df[\"checkin_weekday\"]*(2.*np.pi\/7))\ndf['checkin_weekday_cos'] = np.cos(df[\"checkin_weekday\"]*(2.*np.pi\/7))\ndf['checkin_month_sin'] = np.sin((df[\"checkin_month\"]-1)*(2.*np.pi\/12))\ndf['checkin_month_cos'] = np.cos((df[\"checkin_month\"]-1)*(2.*np.pi\/12))\n\ndf['checkin_week_sin'] = np.sin((df[\"checkin_week\"]-1)*(2.*np.pi\/53))\ndf['checkin_week_cos'] = np.cos((df[\"checkin_week\"]-1)*(2.*np.pi\/53))\n\n#############\n# last number in utrip id - probably which trip number it is:\ndf[\"utrip_number\"] = df[\"utrip_id\"].str.split(\"_\",expand=True)[1].astype(int)\n\n### encode string columns - must be consistent with test data \n### IF we can concat test with train, we can just do a single transformation  for the NON TARGET cols\n# obj_cols_list = df.select_dtypes(\"O\").columns.values\nobj_cols_list = ['device_class','booker_country','hotel_country'] # we could also define when loading data, dtype\nfor c in obj_cols_list:\n    df[c] = df[c].astype(\"category\")\n    df[c] = df[c].cat.codes.astype(int)\n\n## view steps of a trip per user & trip, in order. ## last step == 1.\n## count #\/pct step in a trip (utrip_id) per user. Useful to get the \"final\" step per trip - for prediction\n## note that the order is ascending, so we would need to select by \"last\" . (i.e \"1\" is the first step, 2 the second, etc') , or we could use pct .rank(ascending=True,pct=True)\n#### this feature overlaps with the count of each trip id (for the final row)\n##  = df.sort_values([\"checkin\",\"checkout\"])... - df already sorted above\ndf[\"utrip_steps_from_end\"] = df.groupby(\"utrip_id\")[\"checkin\"].rank(ascending=True,pct=True) #.cumcount(\"user_id\")\n# print(df[\"utrip_steps_from_end\"].describe())\ndf[[\"user_id\",\"utrip_steps_from_end\",\"checkin\"]].sort_values([\"user_id\",\"utrip_steps_from_end\"])\n\n","1f91d052":"df[\"row_num\"] = df.groupby(\"utrip_id\")[\"checkin\"].rank(ascending=True,pct=False).astype(int)\nutrip_counts = df[\"utrip_id\"].value_counts()\ndf[\"total_rows\"] = df[\"utrip_id\"].map(utrip_counts)\n\n### last step in trip\ndf[\"last\"] = (df[\"total_rows\"]==df[\"row_num\"]).astype(int)\nprint(df[[\"row_num\",\"total_rows\"]].describe())\n\ndf[\"total_rows\"].hist(); ","8760b83a":"df[\"affiliate_id\"].min() ## smallest value is 5, so we can impute easily","03f36f9d":"### replace rare variables (under 2 occurrences) with \"-1\" dummy\n\naffiliates_counts = df[\"affiliate_id\"].value_counts()\nprint(\"before:\", affiliates_counts)\nprint(\"uniques\",df[\"affiliate_id\"].nunique())\naffiliates_counts = affiliates_counts.to_dict()\n# df[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].apply(lambda x: x.map(x.value_counts()))>=3, -1)\ndf[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].map(affiliates_counts)>=3, -2)\n# df[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].map(lambda x: x.map(affiliates_counts)>=4), affiliates_counts[x]) # replace rare variables with value_count, doesn't work for now\n\ndf[\"affiliate_id\"] = df[\"affiliate_id\"].astype(int)\n\nprint(\"after\\n\",df[\"affiliate_id\"].value_counts())\nprint(\"uniques\",df[\"affiliate_id\"].nunique())\nprint(df[\"affiliate_id\"].value_counts().describe())","33b7277c":"## add the \"first\" place visited\/values\n### nopte - will need to drop first row in trip, or impute nans when using this feature \n\n### first by user results in too much sparsity\/rareness for our IDs purposes\n# df = groupbyFirstLagFeatures(df,group=\"user_id\",lag_feature_cols=[\"hotel_country\"]) # [\"hotel_country\",\"city_id\"]\n\n## alt - messy, but maybe good enough : \ndf = groupbyFirstLagFeatures(df,group=[\"utrip_id\"],lag_feature_cols=[\"hotel_country\",\"city_id\",'duration','affiliate_id','same_country'])\n\nprint(df[[\"first_hotel_country\",'first_city_id','first_duration','first_affiliate_id',\"first_same_country\",\"hotel_country\",\"city_id\",]].nunique())\ndf","51189607":"#US Holidays \ndr = pd.date_range(start='2015-12-31', end='2018-02-28')\ndf2 = pd.DataFrame()\ndf2['Holiday'] = dr\ncal = calendar()\nholidays = cal.holidays(start=dr.min(), end=dr.max())\ndf['US_Holiday'] = df['checkin'].isin(holidays)*1","e36459d8":"#eur holiday\nclass GothamBusinessCalendar(AbstractHolidayCalendar):\n    rules = [\n     Holiday('New Year', month=1, day=1, observance=sunday_to_monday),\n     Holiday('Groundhog Day', month=1, day=6, observance=sunday_to_monday),\n     Holiday('St. Patricks Day', month=3, day=17, observance=sunday_to_monday),\n     Holiday('April Fools Day', month=4, day=1),\n     Holiday('Good Friday', month=1, day=1, offset=[Easter(), Day(-2)]),\n     Holiday('Labor Day', month=5, day=1, observance=sunday_to_monday),\n     Holiday('Canada Day', month=7, day=1, observance=sunday_to_monday),\n     Holiday('July 4th', month=7, day=4, observance=nearest_workday),\n     Holiday('All Saints Day', month=11, day=1, observance=sunday_to_monday),\n     Holiday('Christmas', month=12, day=25, observance=nearest_workday)\n   ]\n\ncal = GothamBusinessCalendar()\n# Getting the holidays (off-days) between two dates\neur_holidays = cal.holidays(start='2015-12-31', end='2018-02-28')\ndf['EU_Holiday'] = df['checkin'].isin(eur_holidays)*1","6516f1fb":"#china holiday\nclass ChinaCalendar(AbstractHolidayCalendar):\n    \n    \"\"\"\n    1 Jan\tWed\tNew Year Holiday\n24 Jan to 2 Feb\tFri to Sun\tSpring Festival\n4 Apr to 6 Apr\tSat to Mon\tChing Ming Festival\n1 May to 5 May\tFri to Tue\tLabour Day Holiday\n25 Jun to 27 Jun\tThu to Sat\tDragon Boat Festival\n1 Oct to 8 Oct\tThu to Thu\tNational Day Holiday\n1 Oct\tThu\tMid-Autumn Festival\n    \n    \"\"\"\nrules = [\n Holiday('New Year', month=1, day=1, observance=nearest_workday),\n Holiday('Spring Festival', month=1, day=24, observance=sunday_to_monday),\n Holiday('Ching Ming Festival', month=4, day=6, observance=sunday_to_monday),\n Holiday('Labour Day Holiday', month=5, day=1, observance=sunday_to_monday),\n Holiday('Dragon Boat Festival', month=6, day=25, observance=sunday_to_monday),\n Holiday('National Day Holiday', month=8, day=1, observance=sunday_to_monday),\n Holiday('Mid-Autumn Festival', month=8, day=1, observance=nearest_workday)\n]\n\ncal = GothamBusinessCalendar()\n# Getting the holidays (off-days) between two dates\neur_holidays = cal.holidays(start='2015-12-31', end='2018-02-28')\ndf['ch_Holiday'] = df['checkin'].isin(eur_holidays)*1","b4d51f51":"%%time\n\n#check stationarity\n\"\"\"\np-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\np-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\n\"\"\"\n\nfrom statsmodels.tsa.stattools import adfuller\nX = df.checkin.unique()\nresult = adfuller(X)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n","8b1cfa84":"##### Following aggregation features - would be best to use time window (sort data) to generate, otherwise they will LEAK! (e.g. nunique countries visited)\n\n### count features (can also later add rank inside groups).\n### Some may be leaks (# visits in a trip should use time window?) , and do users repeat? \n### can add more counts of group X time period (e.g. affiliate X month of year)\n\ncount_cols = ['user_id',  'city_id','affiliate_id', 'booker_country', 'hotel_country', 'utrip_id',\n \"checkin_month\",\"checkin_week\"]\nfor c in count_cols:\n    df[f\"{c}_count\"] = df.groupby([c])[\"duration\"].transform(\"size\")\n    \n########################################################\n## nunique per trip\nnunique_cols = [ 'city_id','affiliate_id', 'booker_country', 'hotel_country']\n# df[\"nunique_booker_countries\"] = df.groupby(\"utrip_id\")[\"booker_country\"].nunique()\n# df[\"nunique_hotel_country\"] = df.groupby(\"utrip_id\")[\"hotel_country\"].nunique()\nfor c in nunique_cols:\n    df[f\"{c}_nunique\"] = df.groupby([\"utrip_id\"])[c].transform(\"nunique\")\nprint(df.nunique())\n\n########################################################\n## get frequency\/count feature's rank within a group - e.g. within a country (or affiliate) \n## add \"_count\" to column name to get count col name, then add rank col \n\n### ALT\/ duplicate feat - add percent rank (instead or in addition)\n\nrank_cols = ['city_id','affiliate_id', 'booker_country','hotel_country',\n \"checkin_month\"]\n### what is meaning of groupby and rank of smae variable by same var? Surely should be 1 \/ unary? \nfor c in rank_cols:\n    df[f\"{c}_rank_by_hotel_country\"] = df.groupby(['hotel_country'])[f\"{c}_count\"].transform(\"rank\")\n    df[f\"{c}_rank_by_booker_country\"] = df.groupby(['booker_country'])[f\"{c}_count\"].transform(\"rank\")\n    df[f\"{c}_rank_by_affiliate\"] = df.groupby(['affiliate_id'])[f\"{c}_count\"].transform(\"rank\")\n    \ndf","1416d368":"df[\"utrip_number\"].value_counts().describe()","99b5c9a2":"## counts of each val\n# df.groupby(['hotel_country']).size() # same thing as value counts only without ordering by values\ndf['hotel_country'].value_counts()","5d23b6aa":"df.isna().sum().max()","ddc66ac8":"df[[ 'checkin', 'checkout','booker_country', 'hotel_country', 'duration']].describe(include=\"all\",datetime_is_numeric=True)","1ebd5577":"df2 = df[[\"user_id\",\"city_id\"]].drop_duplicates()\nprint(df2.shape)\nprint(df2.nunique())\n\nprint(\"cities with at least 9:\",(df2[\"city_id\"].value_counts()>9).sum())\nprint(\"cities with at least 30:\",(df2[\"city_id\"].value_counts()>30).sum())\nprint(\"cities with at least 100:\",(df2[\"city_id\"].value_counts()>100).sum())\nprint(\"cities with at least 300:\",(df2[\"city_id\"].value_counts()>300).sum())\n# dropping very rares leaves us still with thousands of outputs","64d9c5a6":"### According to the contest description - each user should have at least 4 trips?\ndf[\"user_id\"].value_counts().describe()#.hist()","aaaa1914":"if KEEP_TOP_K_TARGETS > 0 :\n    df_end = df.loc[df[\"utrip_steps_from_end\"]==1].drop_duplicates(subset=[\"city_id\",\"hotel_country\",\"user_id\"])[[\"city_id\",\"hotel_country\"]].copy()\n    print(df_end.shape[0])\n    \n    TOP_TARGETS = df_end.city_id.value_counts().head(KEEP_TOP_K_TARGETS).index.values\n    print(f\"top {KEEP_TOP_K_TARGETS} targets \\n\",TOP_TARGETS)\n    \n#     assert df.loc[df[\"city_id\"].isin(TOP_TARGETS)][\"city_id\"].nunique() == KEEP_TOP_K_TARGETS\n\n####        \n# replace low frequency categoircal features    \n\n# ##replace with count encoding if have at least k, group rarest as \"-1\":# df[BASE_CAT_COLS] = df[BASE_CAT_COLS].where(df[BASE_CAT_COLS].apply(lambda x: x.map(x.value_counts()))>=LOW_COUNT_THRESH, -1)   \n# ## replace\/group only the rare variables : \n# df[BASE_CAT_COLS] = df[BASE_CAT_COLS].where(df[BASE_CAT_COLS].apply(lambda x: x.map(x.value_counts()))>=LOW_COUNT_THRESH, -1)\n# df[BASE_CAT_COLS].head()","11765c32":"df_end.city_id.value_counts().describe()\n","08e85a8b":"## check distribution from \"midpoint\" (50%) of trips, onwards\ndf.loc[df[\"utrip_steps_from_end\"]>=0.4].drop_duplicates(subset=[\"city_id\",\"hotel_country\",\"user_id\"])[\"city_id\"].value_counts().describe()\n","93155b89":"# df[\"c\"] = df[\"city_id\"].map(df[\"city_id\"].value_counts())\n# df[BASE_CAT_COLS+ [\"c\"]]","8f3029fb":"df[\"utrip_id\"].value_counts().describe()","7782ffd2":"df_locations = df[[\"hotel_country\",\"city_id\"]].drop_duplicates()\nprint(df_locations.shape[0])\nprint(df_locations.nunique())\nprint(\"After filtering countries with 4 or less unique hotels\/cities:\")\ndf_locations = df_locations.loc[df_locations.groupby([\"hotel_country\"])[\"city_id\"].transform(\"nunique\")>4]\nprint(df_locations.shape[0])\nprint(df_locations.nunique())\n\nprint(df.groupby([\"hotel_country\"])[\"city_id\"].nunique().describe())","3b847ce1":"print(df.shape[0])\n### unsure about this filtering - depends if data points are real or mistake\n\nprint(\"dropping users with less than 4 trips\")\ndf2 = df.loc[df[\"utrip_id_count\"]>=4]\nprint(df2.shape[0])\n\n# print(\"dropping countries+Data with less than 4 unique cities in them:\")\n# df2 = df2.loc[df2.groupby([\"hotel_country\"])[\"city_id\"].transform(\"nunique\")>=4]\n# print(df2.shape[0])\n\nprint(f\"dropping cities  with less than {MIN_TARGET_FREQ} occurences:\")\ndf2 = df2.loc[df2.groupby([\"city_id\"])[\"hotel_country\"].transform(\"count\")>=MIN_TARGET_FREQ]\nprint(df2.shape[0])\n# df2 = df2.loc[df2.groupby([\"hotel_country\"])[\"city_id\"].transform(\"count\")>=MIN_TARGET_FREQ]\n# print(df2.shape[0])\n\nprint(\"nunique cities after freq filt\",df2[\"city_id\"].nunique())\nprint(\"nunique city_id per hotel_country:\")\ndf2.groupby([\"hotel_country\"])[\"city_id\"].nunique().describe()","ab8be33f":"df2[[\"hotel_country\",\"city_id\",\"affiliate_id\",\"user_id\"]].nunique()","feb18cf7":"df_cities = df[[\"city_id\",\"hotel_country\",\"city_id_count\"]] ## +- drop duplicates by tripid? \nprint(df_cities.nunique())\ndf_cities = df_cities.loc[df_cities.groupby(\"hotel_country\")[\"city_id\"].transform(\"nunique\")>4]\ndf_cities = df_cities.loc[df_cities[\"city_id_count\"]>=5].sort_values(\"city_id_count\",ascending=False)\nprint(df_cities.nunique())\nprint(df_cities.shape[0])\n\n\n# ### 5 most frequent overall\n# df_city_samples = df_cities.drop_duplicates().sort_values(\"city_id_count\",ascending=False).groupby(\"city_id\").head(5) \n# df_city_samples","e9c75595":"### features to drop - not usable, or leaks (e.g. aggregations on target)\n\nTARGET_COL = 'city_id'\nDROP_FEATS = [\n#     'user_id', ## drop this later, after using it for train\/test split\n    'checkin', 'checkout',\n              'hotel_country','city_id_count','same_country',\n              'utrip_id',\n#               'utrip_steps_from_end',\n             'city_id_count','hotel_country_count',\n              'city_id_nunique', 'hotel_country_nunique',\n              'city_id_rank_by_hotel_country','city_id_rank_by_booker_country', 'city_id_rank_by_affiliate',\n              'affiliate_id_rank_by_hotel_country','affiliate_id_rank_by_booker_country', 'affiliate_id_rank_by_affiliate',\n              'hotel_country_rank_by_hotel_country',\n       'hotel_country_rank_by_booker_country','hotel_country_rank_by_affiliate',\n              'booker_country_rank_by_hotel_country','booker_country_rank_by_booker_country',\n              'checkin_month_rank_by_hotel_country',\n             ]\n\n# df2.drop(DROP_FEATS,axis=1).columns","96dc2fa7":"print(df2.shape)\n# ### lag features - last n visits\ndf_feat = groupbyLagFeatures(df=df2.copy(), \n                   lag=[1,2],group=\"utrip_id\",lag_feature_cols=LAG_FEAT_COLS)\ndf_feat = groupbyLagFeatures(df=df_feat, \n                   lag=[3],group=\"utrip_id\",lag_feature_cols=MINI_LAG_FEAT_COLS)\n\ndf_feat = df_feat.dropna(subset=[\"lag3_city_id\"]).sample(frac=1) ## could dropna by lag 1\/2 , but would then require imputing nans (must be careful when done with categoircals)\n\n\ndf_feat = df_feat.drop(DROP_FEATS,axis=1,errors=\"ignore\")\nprint(df_feat.shape)\n\n# df_feat.sort_values([\"user_id\",\"utrip_steps_from_end\"])","93212320":"# #nm of cities per utrip\n# cites_per_user = df2.groupby('utrip_id').apply(lambda x: [[a] for a in (x['city_id'])])\n# cites_per_user","e83cf2c3":"g1 = df2.groupby([\"user_id\", 'city_id']).size().reset_index(name='Number of same city')\nprint(g1[g1['Number of same city'] ==95])\n# max(g1['Number of city'])\nprint(g1['Number of same city'].describe())","be29880d":"### filter for most frequent targets\/cities \n\nif KEEP_TOP_K_TARGETS > 0 :\n    print(df_feat.shape[0])\n    df_feat = df_feat.loc[df_feat[\"city_id\"].isin(TOP_TARGETS)]\n    print(df_feat.shape[0])    \n    assert df_feat[\"city_id\"].nunique() == KEEP_TOP_K_TARGETS","556a1dc0":"#label encode city to take less memory in nn , remember to decode it after\nle = LabelEncoder()\ndf_feat.city_id = le.fit_transform(df_feat.city_id)","9fbe99b6":"# ########################\n# ## stratified train\/test split by class - then drop any train rows with overlap wit htest IDs.  Could also stratify by users, but risks some classes being non present in test\n# ### split could maybe be by utrip ID ? \n# ### orig - split by group : \n\n# # train_inds, test_inds = next(GroupShuffleSplit(test_size=.2, n_splits=2, random_state = 7).split(df_feat, groups=df_feat['user_id']))\n# # X_train = df_feat.iloc[train_inds].drop(DROP_FEATS,axis=1,errors=\"ignore\")\n# # X_test = df_feat.iloc[test_inds].drop(DROP_FEATS,axis=1,errors=\"ignore\")\n# # assert (set(X_train[TARGET_COL].unique()) == set(X_test[TARGET_COL].unique()))\n# #################\n# ## alt: split by class. May be leaky! \n# X_train, X_test = train_test_split(df_feat,stratify=df_feat[TARGET_COL])\n\n# ########################\n# print(\"X_train\",X_train.shape[0])\n# ## get last row in trip only in test\/eval set: \n# print(\"X_test\",X_test.shape[0])\n# X_test = X_test.loc[X_test[\"utrip_steps_from_end\"]==1] # last row per trip\n# print(\"X_test after filtering for last instance per trip\",X_test.shape[0])\n\n# y_train = X_train.pop(TARGET_COL)\n# y_train = y_train.values\n\n# y_test = X_test.pop(TARGET_COL)\n# y_test = y_test.values\n# # print(\"# classes\",y_train.nunique())\n\n# # ## check that same classes in train and test - \n# # assert (set(y_train.unique()) == set(y_test.unique()))","f574d0c2":"df_feat.columns\n","ed63beee":"# df_feat[\"Group_id\"] = df_feat[['city_id', 'device_class', 'affiliate_id', 'booker_country',\n#        'checkin_quarter']].astype(str).sum(1)\n# print(df_feat[\"Group_id\"].nunique())","5e05f8db":"train_inds, test_inds = next(GroupShuffleSplit(test_size=.2, n_splits=2, random_state = 7).split(df_feat, groups=df_feat[ 'user_id']))\n\nX_train = df_feat.iloc[train_inds].drop(\"user_id\",axis=1)\ntest = df_feat.iloc[test_inds]\n\n## split test into validation and final test (only last stage of trip). We could also spliut by group here as well\n\nvalid_inds, test_inds  = next(GroupShuffleSplit(test_size=.6, n_splits=2, random_state = 7).split(test, groups=test['user_id']))\nX_valid = test.iloc[valid_inds].drop('user_id',axis=1)\nX_test  = test.iloc[test_inds].drop('user_id',axis=1)\nX_test  = test.loc[test[\"last\"]==1]\n\nprint(\"X_train\",X_train.shape[0],\"X_valid\",X_valid.shape[0],\"X_test\",X_test.shape[0])","d294539a":"y_train = X_train.pop('city_id')\ny_train = y_train.values\n\ny_test = X_test.pop('city_id')\ny_test = y_test.values\n\ny_valid = X_valid.pop('city_id')\ny_valid = y_valid.values","584f23c8":"NUM_CLASSES = len(set(y_train))\nprint(\"unique classes in y_train:\",NUM_CLASSES)","2b74c034":"## train data should contain all possible classes, without \"new ones\" being in valid\/test\nassert NUM_CLASSES >= len(set(y_test)) ","199c320a":"# from sklearn.model_selection import TimeSeriesSplit\n# X = df_feat.drop(columns= ['city_id']).values\n# y = df_feat.city_id.values\n# tscv = TimeSeriesSplit()\n# print(tscv)\n\n# for train_index, test_index in tscv.split(X):\n#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n#     X_train, X_test = X[train_index], X[test_index]\n#     y_train, y_test = y[train_index], y[test_index]","22841b18":"CAT_FEAT_NAMES = [\"booker_country\", \"device_class\",\"affiliate_id\",\n#                   \"user_id\", ## ? could use lower dim - depends on train\/test overlap\n                  \"checkin_week\",#\"checkout_week\",\n                    \"checkin_weekday\",\n    \"lag1_city_id\",\"lag1_booker_country\",\"lag1_hotel_country\",\"lag1_affiliate_id\", \"lag1_device_class\",\n     \"lag2_city_id\",\"lag2_booker_country\",\"lag2_hotel_country\",\"lag2_affiliate_id\",\"lag2_device_class\",\n      \"lag3_city_id\",\"lag3_hotel_country\",\n#                   \"lag3_booker_country\",\"lag3_affiliate_id\",\"lag3_device_class\",\n#                   'checkin_quarter',\n                  'season', \n#                 \"US_Holiday\",\"EU_Holiday\",\"ch_Holiday\", 'total_rows'\n                  \"first_hotel_country\",'first_city_id','first_affiliate_id'\n                 ]","d5eb9d40":"NUMERIC_COLS = [item for item in list(df_feat.columns.drop([TARGET_COL,\"user_id\"],errors=\"ignore\"))  if item not in CAT_FEAT_NAMES]\nprint(len(NUMERIC_COLS))\nprint(\"numeric cols\",NUMERIC_COLS)\n\nfor c in NUMERIC_COLS:\n    l_enc =   StandardScaler() # MinMaxScaler()#\n    l_enc.fit(df_feat[c].values.reshape(-1,1))\n    X_train[c] = l_enc.transform(X_train[c].values.reshape(-1,1))\n    X_test[c] = l_enc.transform(X_test[c].values.reshape(-1,1))","e539558d":"# for c in CAT_FEAT_NAMES:\n# #     l_enc = LabelEncoder().fit(df_feat[c])\n\n#     X_train[c] = l_enc.transform(X_train[c])\n#     X_test[c] = l_enc.transform(X_test[c])\n\nl_enc = OrdinalEncoder().fit(df_feat[CAT_FEAT_NAMES])\nX_train[CAT_FEAT_NAMES] = l_enc.transform(X_train[CAT_FEAT_NAMES])\nX_valid[CAT_FEAT_NAMES] = l_enc.transform(X_valid[CAT_FEAT_NAMES])\nX_test[CAT_FEAT_NAMES] = l_enc.transform(X_test[CAT_FEAT_NAMES])","bc6fc73b":"# from sklearn.ensemble import RandomForestClassifier\n\n# tss = TimeSeriesSplit(n_splits=5).split(X)\n\n# # instantiate the classifier\n# rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=5, min_samples_split=2,\n#   min_samples_leaf=1, max_features='auto',    bootstrap=False, oob_score=False, n_jobs=1, random_state=42,\n#   verbose=0)\n# # fit the model\n# rfc.fit(X_train, y_train)\n# # Predict the Test set results\n# y_pred = rfc.predict(X_test)\n# y_pred_proba  =  rfc.score(X_test) \n# # Check accuracy score \n# from sklearn.metrics import accuracy_score\n\n# print('Model accuracy score with 70 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","31e6c726":"# pipeline=Pipeline([ ('rfc',rfc) ])\n# param_grid = dict( )\n# grid_search = GridSearchCV(pipeline,param_grid=param_grid,verbose=3,scoring='accuracy',\n#                            cv =tss).fit(X_train, y_train)","fcbb2270":"# print(\"Best score: %0.3f\" % grid_search.best_score_)\n# print(grid_search.best_estimator_)\n# report(grid_search.grid_scores_)\n \n# print('-----grid search end------------')\n# print ('on all train set')\n# scores = cross_val_score(grid_search.best_estimator_, x_train, y_train,cv=3,scoring='accuracy')\n# print (scores.mean(),scores)\n# print ('on test set')\n# scores = cross_val_score(grid_search.best_estimator_, X_test, Y_test,cv=3,scoring='accuracy')\n# print (scores.mean(),scores)","62687d98":"# m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)\n# m.update_state(y_true=y_test.values, y_pred=y_pred_proba)    \n# print(m.result().numpy())","9253a084":"from sklearn.utils import check_array, column_or_1d , assert_all_finite , check_consistent_length\nfrom sklearn.utils.multiclass import type_of_target\n\n# class InTopK(tf.keras.metrics.Mean):\n#     def __init__(self, k, name='in_top_k', **kwargs):\n#         super(InTopK, self).__init__(name=name, **kwargs)\n#         self._k = k\n\n#     def update_state(self, y_true, y_pred, sample_weight=None):\n#         matches = tf.nn.in_top_k(\n#             # flatten tensors\n#             tf.reshape(tf.cast(y_true, tf.int32), [-1]),\n#             tf.reshape(y_pred, [-1, y_pred.shape[-1]]),\n#             k=self._k)\n\n#         return super(InTopK, self).update_state(\n#             matches, sample_weight=sample_weight)\n    \n\n\n## from sklearn.metrics import top_k_accuracy_score\n## from experimental sklearn \ndef top_k_accuracy_score(y_true, y_score, *, k=4, normalize=True,\n                         sample_weight=None, labels=None):\n    \"\"\"Top-k Accuracy classification score.\n    This metric computes the number of times where the correct label is among\n    the top `k` labels predicted (ranked by predicted scores). Note that the\n    multilabel case isn't covered here.\n    Read more in the :ref:`User Guide <top_k_accuracy_score>`\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        True labels.\n    y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n        Target scores. These can be either probability estimates or\n        non-thresholded decision values (as returned by\n        :term:`decision_function` on some classifiers). The binary case expects\n        scores with shape (n_samples,) while the multiclass case expects scores\n        with shape (n_samples, n_classes). In the nulticlass case, the order of\n        the class scores must correspond to the order of ``labels``, if\n        provided, or else to the numerical or lexicographical order of the\n        labels in ``y_true``.\n    k : int, default=2\n        Number of most likely outcomes considered to find the correct label.\n    normalize : bool, default=True\n        If `True`, return the fraction of correctly classified samples.\n        Otherwise, return the number of correctly classified samples.\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights. If `None`, all samples are given the same weight.\n    labels : array-like of shape (n_classes,), default=None\n        Multiclass only. List of labels that index the classes in ``y_score``.\n        If ``None``, the numerical or lexicographical order of the labels in\n        ``y_true`` is used.\n    Returns\n    -------\n    score : float\n        The top-k accuracy score. The best performance is 1 with\n        `normalize == True` and the number of samples with\n        `normalize == False`.\n    See also\n    --------\n    accuracy_score\n    Notes\n    -----\n    In cases where two or more labels are assigned equal predicted scores,\n    the labels with the highest indices will be chosen first. This might\n    impact the result if the correct label falls after the threshold because\n    of that.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.metrics import top_k_accuracy_score\n    >>> y_true = np.array([0, 1, 2, 2])\n    >>> y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n    ...                     [0.3, 0.4, 0.2],  # 1 is in top 2\n    ...                     [0.2, 0.4, 0.3],  # 2 is in top 2\n    ...                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n    >>> top_k_accuracy_score(y_true, y_score, k=2)\n    0.75\n    >>> # Not normalizing gives the number of \"correctly\" classified samples\n    >>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\n    3\n    \"\"\"\n    y_true = check_array(y_true, ensure_2d=False, dtype=None)\n    y_true = column_or_1d(y_true)\n    y_type = type_of_target(y_true)\n    y_score = check_array(y_score, ensure_2d=False)\n    y_score = column_or_1d(y_score) if y_type == 'binary' else y_score\n    check_consistent_length(y_true, y_score, sample_weight)\n\n    if y_type not in {'binary', 'multiclass'}:\n        raise ValueError(\n            f\"y type must be 'binary' or 'multiclass', got '{y_type}' instead.\"\n        )\n\n    y_score_n_classes = y_score.shape[1] if y_score.ndim == 2 else 2\n\n    if labels is None:\n        classes = unique(y_true)\n        n_classes = len(classes)\n\n        if n_classes != y_score_n_classes:\n            raise ValueError(\n                f\"Number of classes in 'y_true' ({n_classes}) not equal \"\n                f\"to the number of classes in 'y_score' ({y_score_n_classes}).\"\n            )\n    else:\n        labels = column_or_1d(labels)\n        classes = unique(labels)\n        n_labels = len(labels)\n        n_classes = len(classes)\n\n        if n_classes != n_labels:\n            raise ValueError(\"Parameter 'labels' must be unique.\")\n\n        if not np.array_equal(classes, labels):\n            raise ValueError(\"Parameter 'labels' must be ordered.\")\n\n        if n_classes != y_score_n_classes:\n            raise ValueError(\n                f\"Number of given labels ({n_classes}) not equal to the \"\n                f\"number of classes in 'y_score' ({y_score_n_classes}).\"\n            )\n\n        if len(np.setdiff1d(y_true, classes)):\n            raise ValueError(\n                \"'y_true' contains labels not in parameter 'labels'.\"\n            )\n\n    if k >= n_classes:\n        warnings.warn(\n            f\"'k' ({k}) greater than or equal to 'n_classes' ({n_classes}) \"\n            \"will result in a perfect score and is therefore meaningless.\",\n            UndefinedMetricWarning\n        )\n\n    y_true_encoded = encode(y_true, uniques=classes)\n\n    if y_type == 'binary':\n        if k == 1:\n            threshold = .5 if y_score.min() >= 0 and y_score.max() <= 1 else 0\n            y_pred = (y_score > threshold).astype(np.int64)\n            hits = y_pred == y_true_encoded\n        else:\n            hits = np.ones_like(y_score, dtype=np.bool_)\n    elif y_type == 'multiclass':\n        sorted_pred = np.argsort(y_score, axis=1, kind='mergesort')[:, ::-1]\n        hits = (y_true_encoded == sorted_pred[:, :k].T).any(axis=0)\n\n    if normalize:\n        return np.average(hits, weights=sample_weight)\n    elif sample_weight is None:\n        return np.sum(hits)\n    else:\n        return np.dot(hits, sample_weight)","03b82d00":"# # embedding 3 categorical values\n# no_of_unique_city_id  = df.city_id.nunique()\n# no_of_unique_hotel_country  = df.hotel_country.nunique()\n# no_of_unique_affiliate_id  = df.affiliate_id.nunique()\n\n\n# # #Jeremy Howard provides the following rule of thumb; embedding size = min(50, number of categories\/2).\n# embedding_size_city = min(np.ceil((no_of_unique_city_id)\/2), 50 )\n# embedding_size_city = int(embedding_size_city)\n# embedding_size_cntry = min(np.ceil((no_of_unique_hotel_country)\/2), 50 )\n# embedding_size_cntry = int(embedding_size_cntry)\n# embedding_size_aff = min(np.ceil((no_of_unique_affiliate_id)\/2), 50 )\n# embedding_size_aff = int(embedding_size_aff)\n\n\n# model = Sequential()\n# model.add(Embedding(input_dim = no_of_unique_hotel_country, output_dim = embedding_size_cntry, input_length = 1, name=\"city\"))\n# model.add(Flatten())\n# model.add(Dense(50, activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Dense(15, activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Dense(1, activation='softmax'))\n# model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])\n# model.fit(X_train.lag1_booker_country, y_train , epochs = 50, batch_size = 32)\n\n# model = Sequential()\n# model.add(Embedding(input_dim = no_of_unique_affiliate_id, output_dim = embedding_size_aff, input_length = 1, name=\"affiliate\"))\n# model.add(Flatten())\n# model.add(Dense(50, activation=\"relu\"))\n# model.add(Dropout(0.5)\n# model.add(Dense(15, activation=\"relu\"))\n# model.add(Dropout(0.5)\n# model.add(Dense(1 , activation='sigmoid'))\n# model.compile(loss = \"binary_ca\", optimizer = \"adam\", metrics=[\"accuracy\"])\n# model.fit(X_train.lag1_booker_country, y_train , epochs = 50, batch_size = 32)\n\n# layer = model.get_layer('city')\n# output_embeddings = layer.get_weights()\n# output_embeddings\n\n# output_embeddings_df = pd.DataFrame(output_embeddings[0])\n# output_embeddings_df = output_embeddings_df.reset_index()\n# output_embeddings_df.columns = ['cntry', 'embedding_1',\t'embedding_2',\t'embedding_3',\t'embedding_4',\t'embedding_5',\t'embedding_6',\t'embedding_7',\t'embedding_8',\t'embedding_9',\t'embedding_10',\t'embedding_11',\t'embedding_12',\t'embedding_13',\t'embedding_14',\t'embedding_15',\t'embedding_16',\t'embedding_17',\t'embedding_18',\t'embedding_19',\t'embedding_20',\t'embedding_21',\t'embedding_22',\t'embedding_23',\t'embedding_24',\t'embedding_25',\t'embedding_26',\t'embedding_27',\t'embedding_28',\t'embedding_29',\t'embedding_30',\t'embedding_31',\t'embedding_32',\t'embedding_33',\t'embedding_34',\t'embedding_35',\t'embedding_36',\t'embedding_37',\t'embedding_38',\t'embedding_39',\t'embedding_40',\t'embedding_41',\t'embedding_42',\t'embedding_43',\t'embedding_44',\t'embedding_45',\t'embedding_46',\t'embedding_47',\t'embedding_48',\t'embedding_49',\t'embedding_50']\n# output_embeddings_df\n","f4e3f118":"embed_cols = []\nfor c in CAT_FEAT_NAMES:\n    if len(df_feat[c])>2:\n        embed_cols.append(c)\n        print(c + ': %d values' % len(df_feat[c]))#look at value counts to know the embedding dimensions\n\nprint('\\n')\n","ecf41c55":"import functools\ntop4_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=4)\n\ntop4_acc.__name__ = 'top4_acc'","abb9b96c":"#nn\ndef build_embedding_network():\n    \n    \n    inputs = []\n    embeddings = []\n    \n    input_ps_01_cat = Input(shape=(1,))\n    embedding = Embedding(5,3, input_length=1, name=\"booker_country\")(input_ps_01_cat)\n    embedding = Reshape(target_shape=(3,))(embedding)\n    inputs.append(input_ps_01_cat)\n    embeddings.append(embedding)\n    \n    input_ps_02_cat = Input(shape=(1,))\n    embedding = Embedding(3,2, input_length=1, name=\"device_class\")(input_ps_02_cat)\n    embedding = Reshape(target_shape=(2,))(embedding)\n    inputs.append(input_ps_02_cat)\n    embeddings.append(embedding)\n    \n    input_ps_03_cat = Input(shape=(1,))\n    embedding = Embedding(286,50,  input_length=1, name=\"affiliate_id\")(input_ps_03_cat)\n    embedding = Reshape(target_shape=(50,))(embedding)\n    inputs.append(input_ps_03_cat)\n    embeddings.append(embedding)\n    \n    input_ps_04_cat = Input(shape=(1,))\n    embedding = Embedding(52, 26, input_length=1, name=\"checkin_week\")(input_ps_04_cat)\n    embedding = Reshape(target_shape=(26,))(embedding)\n    inputs.append(input_ps_04_cat)\n    embeddings.append(embedding)\n    \n    input_ps_05_cat = Input(shape=(1,))\n    embedding = Embedding(7, 4, input_length=1, name=\"checkin_weekday\")(input_ps_05_cat)\n    embedding = Reshape(target_shape=(4,))(embedding)\n    inputs.append(input_ps_05_cat)\n    embeddings.append(embedding)\n    \n    input_ps_06_cat = Input(shape=(1,))\n    embedding = Embedding(333, 50, input_length=1, name=\"lag1_city_id\")(input_ps_06_cat)\n    embedding = Reshape(target_shape=(50,))(embedding)\n    inputs.append(input_ps_06_cat)\n    embeddings.append(embedding)\n    \n    input_ps_07_cat = Input(shape=(1,))\n    embedding = Embedding(5,3, input_length=1, name=\"lag1_booker_country\")(input_ps_07_cat)\n    embedding = Reshape(target_shape=(3,))(embedding)\n    inputs.append(input_ps_07_cat)\n    embeddings.append(embedding)\n    \n    input_ps_08_cat = Input(shape=(1,))\n    embedding = Embedding(49,25, input_length=1, name=\"lag1_hotel_country\")(input_ps_08_cat)\n    embedding = Reshape(target_shape=(25,))(embedding)\n    inputs.append(input_ps_08_cat)\n    embeddings.append(embedding)\n    \n    input_ps_09_cat = Input(shape=(1,))\n    embedding = Embedding(299, 50, input_length=1, name=\"lag1_affiliate_id\")(input_ps_09_cat)\n    embedding = Reshape(target_shape=(50,))(embedding)\n    inputs.append(input_ps_09_cat)\n    embeddings.append(embedding)\n    \n    input_ps_10_cat = Input(shape=(1,))\n    embedding = Embedding(3,2 , input_length=1, name=\"lag1_device_class\")(input_ps_10_cat)\n    embedding = Reshape(target_shape=(2,))(embedding)\n    inputs.append(input_ps_10_cat)\n    embeddings.append(embedding)\n    \n    input_ps_11_cat = Input(shape=(1,))\n    embedding = Embedding(356,50 , input_length=1, name=\"lag2_city_id\")(input_ps_11_cat)\n    embedding = Reshape(target_shape=(50,))(embedding)\n    inputs.append(input_ps_11_cat)\n    embeddings.append(embedding)\n    \n    input_ps_12_cat = Input(shape=(1,))\n    embedding = Embedding(5,3 , input_length=1, name=\"lag2_booker_country\")(input_ps_12_cat)\n    embedding = Reshape(target_shape=(3,))(embedding)\n    inputs.append(input_ps_12_cat)\n    embeddings.append(embedding)\n    \n    input_ps_13_cat = Input(shape=(1,))\n    embedding = Embedding(49, 25, input_length=1, name=\"lag2_hotel_country\")(input_ps_13_cat)\n    embedding = Reshape(target_shape=(25,))(embedding)\n    inputs.append(input_ps_13_cat)\n    embeddings.append(embedding)\n    \n    input_ps_14_cat = Input(shape=(1,))\n    embedding = Embedding(304,50 , input_length=1, name=\"lag2_affiliate_id\")(input_ps_14_cat)\n    embedding = Reshape(target_shape=(50,))(embedding)\n    inputs.append(input_ps_14_cat)\n    embeddings.append(embedding)\n    \n    input_ps_15_cat = Input(shape=(1,))\n    embedding = Embedding(3, 2, input_length=1, name=\"lag2_device_class\")(input_ps_15_cat)\n    embedding = Reshape(target_shape=(2,))(embedding)\n    inputs.append(input_ps_15_cat)\n    embeddings.append(embedding)\n    \n    input_ps_16_cat = Input(shape=(1,))\n    embedding = Embedding(359,50 , input_length=1, name=\"lag3_city_id\")(input_ps_16_cat)\n    embedding = Reshape(target_shape=(50,))(embedding)\n    inputs.append(input_ps_16_cat)\n    embeddings.append(embedding)\n    \n    input_ps_17_cat = Input(shape=(1,))\n    embedding = Embedding(51,26 , input_length=1, name=\"lag3_hotel_country\")(input_ps_17_cat)\n    embedding = Reshape(target_shape=(26,))(embedding)\n    inputs.append(input_ps_17_cat)\n    embeddings.append(embedding)\n    \n    input_ps_18_cat = Input(shape=(1,))\n    embedding = Embedding(4,2 , input_length=1, name=\"season\")(input_ps_18_cat)\n    embedding = Reshape(target_shape=(2,))(embedding)\n    inputs.append(input_ps_18_cat)\n    embeddings.append(embedding)\n    \n    input_ps_19_cat = Input(shape=(1,))\n    embedding = Embedding(56, 28, input_length=1 , name=\"first_hotel_country\")(input_ps_19_cat)\n    embedding = Reshape(target_shape=(28,))(embedding)  \n    inputs.append(input_ps_19_cat)\n    embeddings.append(embedding)\n    \n    input_ps_20_cat = Input(shape=(1,))\n    embedding = Embedding(731,50 , input_length=1 , name=\"first_city_id\")(input_ps_20_cat)\n    embedding = Reshape(target_shape=(50,))(embedding)\n    inputs.append(input_ps_20_cat)\n    embeddings.append(embedding)\n    \n    input_ps_21_cat = Input(shape=(1,))\n    embedding = Embedding(312, 50, input_length=1 , name=\"first_affiliate_id\" )(input_ps_21_cat)\n    embedding = Reshape(target_shape=(50,))(embedding)\n    inputs.append(input_ps_21_cat)\n    embeddings.append(embedding)\n    \n  \n    \n  \n    input_numeric = Input(shape=(96,))\n    embedding_numeric = Dense(16)(input_numeric) \n    inputs.append(input_numeric)\n    embeddings.append(embedding_numeric)\n\n    x = Concatenate()(embeddings)\n#     x = Dense(150, kernel_regularizer=l2(0.02) ,activation='relu')(x)\n#     x = Dropout(.35)(x)\n#     x = Dense(120 , activation='relu' , kernel_initializer='normal',\n#                     kernel_regularizer=l2(0.001))(x)\n#     x = Dropout(.2)(x)\n#     x = BatchNormalization()(x)\n    x = Dense(128 , activation='relu' , kernel_initializer='normal',\n                    kernel_regularizer=l2(0.001))(x)\n    x = Dropout(.2)(x)\n    x = BatchNormalization()(x)\n    \n    output1 = Dense(NUM_CLASSES, activation='softmax',dtype=\"float32\")(x) \n#     output2 = Dense(200, activation='sigmoid')(x)\n    # create a multioutput ?   \n    model = Model(inputs, [output1])\n\n    model.compile(  loss='sparse_categorical_crossentropy',\n           metrics=['accuracy','top_k_categorical_accuracy',top4_acc], optimizer=Adam(lr=LR))   #sparse_categorical_accuracy if not label encoder\n    print(model.summary())\n    \n    return model","fbaadc59":"#converting data to list format to match the network structure\ndef preproc(X_train, X_val, X_test):\n\n    input_list_train = []\n    input_list_val = []\n    input_list_test = []\n    \n    #the cols to be embedded: rescaling to range [0, # values)\n    for c in embed_cols:\n        raw_vals = np.unique(X_train[c])\n        val_map = {}\n        for i in range(len(raw_vals)):\n            val_map[raw_vals[i]] = i       \n        input_list_train.append(X_train[c].map(val_map).values)\n        input_list_val.append(X_val[c].map(val_map).fillna(0).values)\n        input_list_test.append(X_test[c].map(val_map).fillna(0).values)\n     \n    #the rest of the columns\n    other_cols = [c for c in X_train.columns if (not c in embed_cols)]\n    input_list_train.append(X_train[other_cols].values)\n    input_list_val.append(X_val[other_cols].values)\n    input_list_test.append(X_test[other_cols].values)\n    \n    return input_list_train, input_list_val, input_list_test    \n","08eb019e":"#network training\nn_epochs = max_epochs\nLR =  3e-03 #7e-5   # learning rate of the gradient descent\n\n\n# Define a learning rate decay method:\nlr_decay = ReduceLROnPlateau(monitor='val_loss',  # was \"loss\"\n                             patience=2, verbose=0, \n                             factor=0.5, min_lr=1e-9)\n# Define Early Stopping:\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0, \n                           patience=5, verbose=1, mode='auto',\n                            restore_best_weights=True)\n\n#preprocessing\nproc_X_train_f, proc_X_val_f, proc_X_test_f = preproc(X_train,X_valid , X_test)\n\nval_preds , y_preds =  0,0","dae6ee7f":"class CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https:\/\/arxiv.org\/abs\/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w\/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi\/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.009, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1\/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary\/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations\/(2*self.step_size))\n        x = np.abs(self.clr_iterations\/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())","7e11e927":"clr_triangular = CyclicLR(mode='triangular')","0efe6e72":"start = time()\nNN = build_embedding_network()\nNN.fit(proc_X_train_f, y_train, epochs=n_epochs, validation_data=(proc_X_val_f, y_valid),  batch_size=128,\n                    shuffle=True,verbose=2,\n                    callbacks=[clr_triangular, early_stop]) #lr_decay\nprint('-'*65)\nprint(f'Training was completed in {time() - start:.2f} secs')\nprint('-'*65)\n","9de6631a":"score = NN.evaluate(proc_X_train_f, y_train,verbose=0) #4 results\nprint(f'Train acc1: {score[0]} \/ Train acc2: {score[1]}\/ Train acc3: {score[2]} \/ Train acc4: {score[3]} ')\nprint('-'*130)\nscore = NN.evaluate(proc_X_test_f, y_test,verbose=0) #4 results\nprint(f'Test acc1: {score[0]} \/ Test acc2: {score[1]}\/ Test acc3: {score[2]} \/ Test acc4: {score[3]}')\nprint('-'*130)","1b545893":"pred = NN.predict(proc_X_test_f)\n# city_pred =  le.inverse_transform(# don;t forget to inverse)\n","93ba4bd4":"https:\/\/github.com\/titu1994\/keras-one-cycle\/tree\/master\/models\/mobilenet","6d73aa1b":"### replace rare categorical variable(s) - affiliates\n* replace rare variables (under 2 occurrences) with \"-1\" dummy - saves on embedding \"cost\"","c13456bb":"**build_embedding_network**","98bd1e19":"* If country is known, then we need to rank within a given country.  How many cities\/points per country? :\n\n(Note - Later, need to consider f eatures about multi country trips) \n","2d74433d":"#### https:\/\/www.kaggle.com\/aquatic\/entity-embedding-neural-net","45dbe5a9":"**CyclicLR**","2b20ccfc":"global most popular destination : cobra island, fook island, gondal ","3b77370b":"data is non-stationary. It has some time dependent structure.","95780cb8":"# Plots","fec01080":"## Model\n* For now - simple multiclass model (Tabnet? LSTM?) ; +- subsample - only most frequent classes\/cities\n\n    * Tabnet: `pip install pytorch-tabnet`\n        * https:\/\/github.com\/dreamquark-ai\/tabnet\/blob\/develop\/forest_example.ipynb\n    * TensorFlow Tabmet: https:\/\/github.com\/ostamand\/tensorflow-tabnet\/blob\/master\/examples\/train_mnist.py\n\n* split train\/test by user id. \n    * Test - only last trip. \n    \n* Try multiclass models","c9e21fb9":"* Try tabnet models (tabular with attention)\n    * + Lag feats\n    * Note - the embedding here is not aware that the same IDs are the same (unlike TF's )! ","f335190b":"#### Features to add:\n* Lag \n* Rank (popularity) of city, country (in general, +- given booker country)\n* Count of hotel; user, trip size ? (may be leaky )\n* Seasonal features - Holidays? , datetime\n\nAggregate feats:\n* user changed country? last booking (lag 1) country change? \n* max\/min\/avg popularity rank of previous locations visited\n\n\n\nWe should create a dictionary of the rank, count, city\/country etc' feats, so we can easily merge them when making more \"negative\" samples\/feats for ranking.\n\n\n* Consider using a df2 of df without dates + drop_duplicates, +- without user\/trip id (After calcing that) .\n\n\nLeaky or potentially leaky (Dependso n test set): \n* Target freq features - frequency of target city, given source county +- affiliate +- month of year +- given country (and interactions of target freq). \n    * Risk of leaks - depends of test data has temporal split or not. \n    * cartboost can do target encode, but this lets us do it for interactions, e.g. target city freq given the 2 countries and affiliate.\n    * beware overfitting! ","53e14a7e":"**Cycling lr**","2d718375":"# embeddings","1b94234c":"# Trees, rf, xgboost, adaboost****","6dc7d673":"-****--until here-------------------------------------------------------------------------------","677c4ec5":"#### get a DF of all cities per country\n* +- get from original DF, +- remove cities that appear less than 4? times , and countries with less than 4 hotels? (Or keep - to avoid messing up training?)\n* Weighted Sample from it, for negatives, +- most freq by country\/affiliate\/etc\n* Don't drop duplicates by user, keep orig freq? ","6d7e8d46":"##### Long tail of targets warning!\n* 75% of cities appear less than 4 times in the data (as a final destination!) \n    * Dropping them will mean a maximum accuracy of 25% at best!!\n    * training on intermediates may help overcome improve this. \n* Using ~2d step+ , still leaves us with 75% appearing less than 7 times\n\n* Unsure how to handle this - too amny targets to learn, and no auxiliary data to help learn it? ","90456365":"* cat_idxs : list of int (default=[] - Mandatory for embeddings)\n    * List of categorical features indices.\n\n* cat_dims : list of int (default=[] - Mandatory for embeddings)\n\n    * List of categorical features number of modalities (number of unique values for a categorical feature) \/!\\ no new modalities can be predicted\n\n* cat_emb_dim : list of int (optional)\n\n    * List of embeddings size for each categorical features. (default =1)\n    \n    \n    \nAll the categorical vals must be known from train (demo used label encoder). Consider doing so also here at late step, to avoid unknown vals ? ","f5e7511d":"* EDA on city popularity by country\n* Drop rare hotels to simplify","c5232947":"embedding all cat","a31ec814":"## Frequent city target List + City count encoding\n* Get the K most frequent target city IDs - selected based on frequency as final destination (not just overall)\n* +- Also after this, replace rare city IDs categorical features with count encoding to reduce dimensionality\n    * Keep them as count, or aggregate all of them as \"under_K\"?\n\n##### Output  : `TOP_TARGETS` - filter data by this *after* creation of lag features ! \n\n* Drop duplicates by the same user (reduce possible bias of frequent users? Only relevant if test is seperater from \"frequent travellers\") \n    * results in 216,633 , vs 217,686 without dropping duplicates by users\n    * ~19.9k unique cities\n    \n* Could do other encodings - https:\/\/contrib.scikit-learn.org\/category_encoders\/count.html\n\n* Note that all this is after we've added rank, count features beforehand, so that information won't be lost for these variables, despite these transforms","934970a9":"# LSTM","5559733a":"* Continue with EDA ","ab992eab":"* https:\/\/www.bookingchallenge.com\/\n\n* Predict `city_id`\n        * Metric: P@4\n\n##### Dataset\nThe training dataset consists of over a million of anonymized hotel reservations, based on real data, with the following features:\n*    user_id - User ID\n*    check-in - Reservation check-in date\n*    checkout - Reservation check-out date\n*    affiliate_id - An anonymized ID of affiliate channels where the booker came from (e.g. direct, some third party referrals, paid search engine, etc.)\n*    device_class - desktop\/mobile\n*    booker_country - Country from which the reservation was made (anonymized)\n*    hotel_country - Country of the hotel (anonymized)\n*    city_id - city_id of the hotel\u2019s city (anonymized)\n*    utrip_id - Unique identification of user\u2019s trip (a group of multi-destinations bookings within the same trip)\n\n\n* Each reservation is a part of a customer\u2019s trip (identified by utrip_id) which includes at least 4 consecutive reservations. The check-out date of a reservation is the check-in date of the following reservation in their trip.\n\n* The evaluation dataset is constructed similarly, however the city_id of the final reservation of each trip is concealed and requires a prediction.\n\n \n###### Evaluation criteria\nThe goal of the challenge is to predict (and recommend) the final city (city_id) of each trip (utrip_id). We will evaluate the quality of the predictions based on the top four recommended cities for each trip by using Precision@4 metric (4 representing the four suggestion slots at Booking.com website). When the true city is one of the top 4 suggestions (regardless of the order), it is considered correct.\n\n------------------------------------------------------\n\n* If we are given  the country in question, then this problem is maybe more of a _learning to rank_ problem. (Rather than massively multiclass). \n    * CatBoost learning to rank on ms dataset (0\/1):  https:\/\/colab.research.google.com\/github\/catboost\/tutorials\/blob\/master\/ranking\/ranking_tutorial.ipynb\n        * https:\/\/catboost.ai\/docs\/concepts\/loss-functions-ranking.html\n        * for CB ranking,  all objects in dataset must be grouped by group_id - this would be user\/trip id X country, in our case. (Still need to add negatives, within each such subgroup\/group\/\"query\"). \n\n    * lightFM - ranking (implicit interactions)\n        * https:\/\/github.com\/qqwjq\/lightFM\n\n    * lstm\/w2v - next item recomendation\n    * dot product between different factors as features (recc.)\n    * xgboost ap - https:\/\/www.kaggle.com\/anokas\/xgboost-2\n* Relevant: Kaggle expedia hotel prediction: https:\/\/www.kaggle.com\/c\/expedia-hotel-recommendations\/discussion  \n\n* ALSO: `implicit interaction` - reccommendation problem (We have only positive feedback, no ranked\/negative explicit feedback)'\n\n\n* __BASELINE__ to beat: 4 most popular by country ; 4 most popular by affiliate_id X booker_country X hotel_country (X month?)\n    * Ignore\/auto answer the 4 most popular for countries with less than 4 unique cities in data\n \n \n* Likely approach : build a model (and targets\/negatives) per country.\n\n-----------\n#### Data notes:\n* Long tail of cities and countries\n* Some (31%) countries have 4 or less unique cities - for those return fixed answer\/prediction ?  -\n    * CAN'T! In test set, we will not have the country ID :(","638a628f":"### add lag features + Train\/test\/data split\n* Lag feats (remember for categorical)\n* Drop leak features (target values - country, city)\n\n* drop instances  that lack history (e.g. at least 3d step and onwards) - by dropna in lag feat\n* fill nans\n* Split train\/test by `user id` \/ split could maybe be by `utrip ID` ? ? \n    * Test - only last trip\n    *  stratified train\/test split by class - then drop any train rows with overlap with tests' IDs.  \n        * Could also stratify by users, but risks some classes being non present in test\n        \n###### Big possible improvement to lag features: Have \"first location\" (starting point) \"lag\" feature","a0da291c":"### lag like - first location visited & properties: ","2bca09a8":"single embedding"}}