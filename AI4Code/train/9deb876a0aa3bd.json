{"cell_type":{"62081969":"code","3c2fc946":"code","121fd2be":"code","e013a74e":"code","a9ad56d3":"code","cc0f8e27":"code","a80baa7a":"code","807a1621":"code","dc76ff29":"code","217f0cc5":"code","8cf5bee1":"code","961ef831":"code","7358daa1":"code","d0f873b0":"code","0276383d":"code","274ae456":"code","5d680924":"code","406e95a2":"code","9f69db19":"code","51ef725b":"code","a1a8307d":"code","5dcfcc15":"code","a455dc7b":"code","7a1bc316":"code","90281b69":"code","0e91480a":"code","0da64a3b":"code","7fd92571":"code","1045cdd6":"code","11f11d9b":"code","2bc2ea45":"code","57b400b9":"code","0e4cacfa":"code","a663c300":"code","eded40f2":"code","b6f15bc6":"code","a468fa18":"code","a7bb256f":"code","881aefba":"code","667359d5":"code","128f1d18":"code","d6472c46":"code","f5ee8b5e":"code","dfb1bab2":"code","a6e92a1a":"code","e7f5cc93":"code","9c4b6664":"code","bab4f60a":"code","10628397":"code","b922e6df":"code","881f0262":"code","762d1e44":"code","e1157b44":"code","7dd8ade8":"code","33c53cd2":"code","dede679e":"code","265f316d":"code","9628da67":"code","35390815":"code","3804e336":"code","92654dd9":"code","9dbac7c4":"code","acf69033":"code","6be571ef":"code","3f590436":"code","48e9b620":"code","1ad78af2":"code","3bf870fa":"code","3b38be62":"code","d2e53b78":"code","fff0a4f8":"code","6485fc82":"code","dae41b4f":"code","87d7ab08":"code","c9672d3b":"code","99441a70":"code","68a316ae":"code","d600812a":"code","65ddea59":"code","e78b6b11":"code","47eafdb8":"code","a12847d5":"code","58b2ee92":"code","d99d2555":"code","de77f87a":"code","38add47c":"code","26908523":"markdown","b6a7f700":"markdown","8eca8f2f":"markdown","e8030476":"markdown","f84d82c5":"markdown","62e0e68f":"markdown","b2c8c802":"markdown","9bfd31c3":"markdown","73388516":"markdown","c8deecf7":"markdown","15d08b79":"markdown","a82f4825":"markdown","ae066b9c":"markdown","faaaf34f":"markdown","4850e9de":"markdown","39e0d4cc":"markdown","fe0bbc9f":"markdown","091acbb7":"markdown","f7ef4779":"markdown","c82e0395":"markdown","9fa8c554":"markdown","6dc36c8c":"markdown","a6304d2d":"markdown","0223df1e":"markdown","6ed3bcf9":"markdown","51b5f28a":"markdown","1cf3ba79":"markdown","ae74ad8f":"markdown","67dad2ed":"markdown","2ea055bc":"markdown","9683b075":"markdown","50f8747c":"markdown","c0c2ec19":"markdown","e224f09d":"markdown","507bc272":"markdown","f7dd1e39":"markdown","ed849d4b":"markdown","8527ad28":"markdown","54aad3f1":"markdown","5d5ccc3b":"markdown","6e5fe750":"markdown","9cb2fff0":"markdown","f0a3f94d":"markdown","7073c796":"markdown","ed6bf718":"markdown","e4a91ef1":"markdown","33d8a3d8":"markdown","fece7cbe":"markdown","3b03403f":"markdown","c270eb9e":"markdown","27dd6e57":"markdown","4b196dc7":"markdown","2e338e17":"markdown","7156d7bb":"markdown","efd1ff97":"markdown","3530dc02":"markdown","2896dc1c":"markdown","3373c434":"markdown","5e2fe1c9":"markdown","067b313e":"markdown","a79c649d":"markdown","d834ef74":"markdown","02b6e4b3":"markdown","4d35c595":"markdown"},"source":{"62081969":"!pip install torchdata\n!pip install imblearn","3c2fc946":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as image\n\nimport torch\n\n\nfrom torchvision import transforms\n\n\n%matplotlib inline","121fd2be":"# below variables are established to be used throughout the notebook without being subjected to any change\nBATCH_SIZE     = 64\nHEIGHT = WIDTH = 48\nNCHANNELS      =  3\n\nOPTIMIZER      = \"adam\"\n\nTRANSFORMER    = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(.485, .456, .406), std=(.229, .224, .225)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(degrees=90),\n])\n\nEPOCHS         = 5\n\nEMOTIONS       = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n\nLIMIT          = 1000\n\nDEVICE         = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","e013a74e":"import os\nimport re\n\nimport numpy as np\nimport pandas as pd\n\nimport cv2\nimport PIL.Image as Image\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","a9ad56d3":"paths   = []\nlabels  = []\n\nbase = f\"..\/input\/fer2013\/\"\n\nif os.path.exists(path=base) and os.path.isdir(base):\n    for level_1_dir in os.listdir(path=base): # train\/test\n        level_1_path = f\"{level_1_dir}\"\n        if os.path.exists(path=base + level_1_path) and os.path.isdir(base + level_1_path):\n            for level_2_dir in os.listdir(path=base + level_1_path): # 7 emotions\n                level_2_path =f\"{level_1_path}\/{level_2_dir}\"\n                if os.path.exists(path=base + level_2_path) and os.path.isdir(base + level_2_path):\n                    for file in os.listdir(path=base + level_2_path): # files\n                        file_path = f\"{level_2_path}\/{file}\"\n                        if os.path.isfile(base + file_path):\n                            paths.append(file_path)\n                            labels.append(level_2_dir)\n\ndf = pd.DataFrame({'path': [base + path for path in paths], 'label': labels})","cc0f8e27":"# create a new column to store the images as ndarray\ndf['img_as_matrix'] = df['path'].apply(lambda path: cv2.imread(path))\n\n# view firts 5 rows of the newly created dataframe\ndf.head()","a80baa7a":"# set plotting options\nfig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, _)) = plt.subplots(2, 4, figsize=(10, 5))\n_.set_visible(False)\n\nfor index, label in enumerate(df['label'].unique()):\n    img = df[df['label'] == label]['img_as_matrix'].iloc[0]\n    \n    exec(f\"ax{index + 1}.imshow(img)\")\n    exec(f\"ax{index + 1}.set_title(label.title())\")\n    \nfig.tight_layout()","807a1621":"# setplotting options\nfig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, _)) = plt.subplots(2, 4, figsize=(10, 5))\n_.set_visible(False)\n\nfor index, label in enumerate(df['label'].unique()):\n    avg_img = np.stack(df[df['label'] == label]['img_as_matrix']).mean(axis=0).astype(np.uint8)\n    \n    exec(f\"ax{index + 1}.imshow(avg_img)\")\n    exec(f\"ax{index + 1}.set_title(label.title())\")\n    \nfig.tight_layout()","dc76ff29":"# set plotting options - equalized images for more transparent view\nfig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, _)) = plt.subplots(2, 4, figsize=(10, 5))\n_.set_visible(False)\n\nfor index, label in enumerate(df['label'].unique()):\n    avg_img   = np.stack(df[df['label'] == label]['img_as_matrix']).mean(axis=0).astype(np.uint8)\n    \n    R, G, B   = cv2.split(avg_img)\n\n    output1_R = cv2.equalizeHist(R)\n    output1_G = cv2.equalizeHist(G)\n    output1_B = cv2.equalizeHist(B)\n\n    equ = cv2.merge((output1_R, output1_G, output1_B))\n    \n    exec(f\"ax{index + 1}.imshow(equ)\")\n    exec(f\"ax{index + 1}.set_title(label.title())\")\n    \nfig.tight_layout()","217f0cc5":"plt.title('Value Counts per Emotion')\nplt.barh(y = df.label.value_counts().index, width = df.label.value_counts().values)","8cf5bee1":"fig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, ax8)) = plt.subplots(2, 4, figsize=(10, 5))\n\nfor index, img in enumerate(df['img_as_matrix'][:8]):\n        \n    # calculate mean value from RGB channels and flatten to 1D array\n    img_1_chan = img.mean(axis=2).flatten().astype(np.uint8)\n    \n    # gett same \n    equ = cv2.equalizeHist(img_1_chan)\n    \n    exec(f\"ax{index + 1}.hist(img_1_chan, bins=100, range=(0, 255))\")\n    exec(f\"ax{index + 1}.hist(equ, bins=100, range=(0, 255))\")","961ef831":"# store the average image per emotion\naverage_image_per_label = {}\n\n# compute the average image per label\nfor index, label in enumerate(df['label'].unique()):\n    avg_img = np.stack(df[df['label'] == label]['img_as_matrix']).mean(axis=0).astype(np.int8)\n    \n    # store the result\n    average_image_per_label[label] = avg_img","7358daa1":"# store deltas between each image and its label's average\ndeltas = []\n\nfor i in range(len(df)):\n    label = df.loc[i, 'label']\n    # find the delta between each image and its label's average\n    delta = np.absolute(average_image_per_label[label] - df.loc[i, 'img_as_matrix'].astype(np.int8))\n    deltas.append(delta)\n    \ndf['delta_vs_avg_image'] = deltas","d0f873b0":"# find the mean error across all channels\ndf['mean_error'] = df['delta_vs_avg_image'].apply(lambda x: x.mean())","0276383d":"# find an average mean error per group\nmean_errors_per_label = df.groupby('label')['mean_error'].mean()\n\n# store standard deviations\nerror_stds = []\n\nfor i in range(len(df)):\n    label = df.loc[i, 'label']\n    # compute variance between mean error of each image and average error for label\n    var = abs(df.loc[i, 'mean_error'] - mean_errors_per_label[label])**2\n    # compute standard deviation from variance\n    std = np.sqrt(var)\n    # store the result\n    error_stds.append(std)\n    \ndf['error_std'] = error_stds","274ae456":"df.head()","5d680924":"# compute standard deviation boundaries\nmin_std = df['error_std'].min()\nmax_std = df['error_std'].max()\n\n\n# treat the image as anomalous if its mean error is below or above standard deviation boundaries.\ndef get_anomalous_point(mean_error: float, min_std: float, max_std: float): \n    return mean_error < min_std or mean_error > max_std\n    \n# mark images as anomalous or not\ndf['check_for_anomalies'] = df['mean_error'].apply(\n    lambda err: get_anomalous_point(err, min_std, max_std)\n)","406e95a2":"# retrieve indices of the anomalous images to drop\nimage_indices_to_drop = list(df[df['check_for_anomalies'] == True].index)\n\n# drop anomalous image indices,reset index, and update dataframe\ndf = df.drop(image_indices_to_drop).reset_index(drop=True)","9f69db19":"# print planned to be dropped indices\nprint(f\"How many images gets dropped: {len(image_indices_to_drop)}\\nSome samples: {image_indices_to_drop[:20]}\\n\")\n\nprint(\"Printing first 5 rows from dataframe\")\ndf.head()","51ef725b":"# label encoding\ndf['labels_numeric'] = [EMOTIONS.index(label) for label in df['label']]","a1a8307d":"# assigning labels to y\ny = df['labels_numeric']\n\n# X is a list of read-in images\nX = np.stack(df['img_as_matrix'])\n\n# get all dimensions of the resulting X\nn_samples, height, width, n_channels = [X.shape[index] for index in range(4)]\n\nprint(f\"Shape of X before reshape: {X.shape}\")\n\n# reshape X because SMOTE accepts only (n_samples, n_channels*height*weight)-type data\nX_reshaped = X.reshape(n_samples, n_channels*height*width)\n\nprint(f\"Shape of X before reshape: {X.shape}\")","5dcfcc15":"# initialize the SMOTE model\nsmote = SMOTE(random_state=62)\n\n# perform re-sampling on modified X given y\nX_smote, y_smote = smote.fit_resample(X_reshaped, y)","a455dc7b":"print(f'Before re-sampling, the amount of images:  {len(X):6}')\nprint(f'After re-sampling, the amount of images :{len(X_smote):8}')","7a1bc316":"# view the resulting balanced data(modified distribution)\nplt.title('Value Counts per Emotion After Re-sampling')\nplt.barh(y = y_smote.value_counts().index, width = y_smote.value_counts().values)","90281b69":"# set plotting options\nfig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, ax8)) = plt.subplots(2, 4, figsize=(10, 5))\n_.set_visible(False)\n\n# reshape each SMOTE image into (h, w, n_channels) format\nfor index, (image, label) in enumerate(zip(X_smote[-9:-1], y_smote[-9:-1])):\n    img_to_plot = image.reshape((HEIGHT, WIDTH, NCHANNELS))\n    \n    exec(f\"ax{index + 1}.imshow(img_to_plot)\")\n    exec(f\"ax{index + 1}.set_title(EMOTIONS[label].title())\")\n    \nfig.tight_layout()","0e91480a":"# each category is limited to 1000 samples for easy training\nlimit_counter = {i:0 for i, _ in enumerate(EMOTIONS)}\n\nX_smote_limited = []\ny_smote_limited = []\n\nfor x, y in zip(X_smote, y_smote):\n    if limit_counter[y] < LIMIT:\n        X_smote_limited.append(x)\n        y_smote_limited.append(y)\n        \n        limit_counter[y] += 1\n\nX_smote_limited = np.array(X_smote_limited)\ny_smote_limited = pd.Series(data=y_smote_limited)","0da64a3b":"# split data into train, valid and test for KERAS\nX_keras_train, X_keras_remained, y_keras_train, y_keras_remained = train_test_split(\n    X_smote_limited, y_smote_limited, test_size=0.20, random_state=42\n)\n\nX_keras_valid, X_keras_trial, y_keras_valid, y_keras_trial = train_test_split(\n    X_keras_remained, y_keras_remained, test_size=0.50, random_state=42\n)","7fd92571":"# convert X and y to Tensors\nX_torch_smote = torch.Tensor(X_smote_limited)\ny_torch_smote = torch.as_tensor(y_smote_limited, device=torch.device('cuda'))\n\n# split data into train, valid and test\nX_torch_train, X_torch_remained, y_torch_train, y_torch_remained = train_test_split(\n    X_torch_smote, y_torch_smote, test_size=0.20, random_state=42\n)\nX_torch_valid, X_torch_trial, y_torch_valid, y_torch_trial = train_test_split(\n    X_torch_remained, y_torch_remained, test_size=0.50, random_state=42\n)","1045cdd6":"def change_shape(key, X, y, shape):\n    # print initial shape of the datasets\n    print(f'Before {key.upper()} datasets:\\n\\tX dataset: {X[key].shape}\\n\\ty dataset: {y[key].shape}\\n')\n\n    # reshape to remove the requirements of SMOTE that do not suit standard model training\n    X[key] = X[key].reshape((X[key].shape[0], shape[0], shape[1], shape[2]))\n\n    # print latest shape of the datasets\n    print(f'After {key.upper()} datasets:\\n\\tX {key} dataset: {X[key].shape}\\n\\ty {key} dataset: {y[key].shape}\\n')","11f11d9b":"# store datasets in a dict for easy use \nX_keras_dict = {\n    \"train\": X_keras_train, \n    \"valid\": X_keras_valid, \n    \"trial\": X_keras_trial\n}\n\ny_keras_dict = {\n    \"train\": y_keras_train, \n    \"valid\": y_keras_valid, \n    \"trial\": y_keras_trial\n}","2bc2ea45":"# reshape KERAS datasets\ninput_keras_shape = (HEIGHT, WIDTH, NCHANNELS)\n\nchange_shape(\"train\", X_keras_dict, y_keras_dict, input_keras_shape)\nprint()\nchange_shape(\"valid\", X_keras_dict, y_keras_dict, input_keras_shape)\nprint()\nchange_shape(\"trial\", X_keras_dict, y_keras_dict, input_keras_shape)","57b400b9":"# store datasets in a dict for easy use \nX_torch_dict = {\n    \"train\": X_torch_train, \n    \"valid\": X_torch_valid, \n    \"trial\": X_torch_trial, \n    \"total\": X_torch_smote\n}\n\ny_torch_dict = {\n    \"train\": y_torch_train, \n    \"valid\": y_torch_valid, \n    \"trial\": y_torch_trial, \n    \"total\": y_torch_smote\n}","0e4cacfa":"# reshape PYTORCH datasets\ninput_torch_shape = (NCHANNELS, HEIGHT, WIDTH)\n\nchange_shape(\"train\", X_torch_dict, y_torch_dict, input_torch_shape)\nprint()\nchange_shape(\"valid\", X_torch_dict, y_torch_dict, input_torch_shape)\nprint()\nchange_shape(\"trial\", X_torch_dict, y_torch_dict, input_torch_shape)","a663c300":"# create train, test and validation datasets for the data loader\ntrain_dataset = TensorDataset(X_torch_dict[\"train\"], y_torch_dict[\"train\"])\nvalid_dataset = TensorDataset(X_torch_dict[\"valid\"], y_torch_dict[\"valid\"])\ntrial_dataset = TensorDataset(X_torch_dict[\"trial\"], y_torch_dict[\"trial\"])","eded40f2":"# apply transformations to the train set\ntrain_dataset.transform = TRANSFORMER\n\n# apply transformations to the validation set\nvalid_dataset.transform = TRANSFORMER\n\n# apply transformations to the test set\ntrial_dataset.transform = TRANSFORMER\n\n# create a dataloader for each subset of data\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\ntrial_loader = DataLoader(trial_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)","b6f15bc6":"# count number of times a color appears\ncolor_counts = {}\n\nfor img in df.loc[:, 'img_as_matrix']:\n    # flatten the image into 1 channel\n    img_1_channel = img.mean(axis=2).flatten().astype(np.uint8)\n    # count each color occurrence\n    for color in img_1_channel:\n        try:\n            color_counts[color] += 1\n        except:\n            color_counts[color] = 1","a468fa18":"# view pixel distribution across the original dataset\nplt.bar(color_counts.keys(), color_counts.values(), color='b')","a7bb256f":"import torchvision\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\n\nimport tensorflow as tf\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D","881aefba":"def build_classifier(optimizer=\"adam\"):\n    \"\"\"\n    The model is based on 4 convolutional layers.\"\"\"\n    cnn4 = Sequential()\n    cnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_keras_shape))\n    cnn4.add(BatchNormalization())\n\n    cnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n    cnn4.add(BatchNormalization())\n    cnn4.add(MaxPooling2D(pool_size=(2, 2)))\n    cnn4.add(Dropout(0.25))\n\n    cnn4.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n    cnn4.add(BatchNormalization())\n    cnn4.add(Dropout(0.25))\n\n    cnn4.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n    cnn4.add(BatchNormalization())\n    cnn4.add(MaxPooling2D(pool_size=(2, 2)))\n    cnn4.add(Dropout(0.25))\n\n    cnn4.add(Flatten())\n\n    cnn4.add(Dense(512, activation='relu'))\n    cnn4.add(BatchNormalization())\n    cnn4.add(Dropout(0.5))\n\n    cnn4.add(Dense(128, activation='relu'))\n    cnn4.add(BatchNormalization())\n    cnn4.add(Dropout(0.5))\n\n    cnn4.add(Dense(len(EMOTIONS), activation='softmax'))\n\n    cnn4.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"categorical_accuracy\"])\n    \n    return cnn4\n\n# establish estimator\nestimator = KerasClassifier(build_fn=build_classifier)","667359d5":"# # choose the hyperparameter values to try\n# params = {\n#     \"batch_size\" : [32],        \n#     \"epochs\"     : [1],        \n#     \"optimizer\"  : [\"adam\"], \n# }","128f1d18":"# choose the hyperparameter values to try\nparams = {\n    \"batch_size\" : [25, 32, 64],        \n    \"epochs\"     : [2, 3, 4],        \n    \"optimizer\"  : [\"adam\", \"rmsprop\"], \n}","d6472c46":"grid = GridSearchCV(estimator=estimator, param_grid=params, cv=5, scoring=\"accuracy\")","f5ee8b5e":"# fit the grid on our train data and print the best scores\ngrid.fit(X_keras_dict[\"train\"], y_keras_dict[\"train\"])\nprint(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))","dfb1bab2":"# compute predictions on the holdout\ny_pred = grid.predict(X_keras_dict[\"trial\"])\ny_true = y_keras_dict[\"trial\"].values","a6e92a1a":"# print classification report\nprint(classification_report(y_true, y_pred))","e7f5cc93":"# compute the confusion matrix\nconf_mat = metrics.confusion_matrix(y_true, y_pred)\n\n# plot confusion matrix\nmetrics.ConfusionMatrixDisplay(conf_mat, display_labels=EMOTIONS).plot()","9c4b6664":"# one-hot-encode y dataset for the actual training\ny_keras_dict[\"train_categorical\"] = tf.keras.utils.to_categorical(y_keras_dict[\"train\"], len(EMOTIONS))\ny_keras_dict[\"valid_categorical\"] = tf.keras.utils.to_categorical(y_keras_dict[\"valid\"], len(EMOTIONS))\ny_keras_dict[\"trial_categorical\"] = tf.keras.utils.to_categorical(y_keras_dict[\"trial\"], len(EMOTIONS))","bab4f60a":"def build_and_run_GridSearchCV(builder, params, cv=5, scoring=\"accuracy\"):\n    # build estimator through Keras Classifier\n    estimator = KerasClassifier(build_fn=builder, verbose=2)\n    \n    # establish gridSearchCV\n    grid = GridSearchCV(estimator=estimator, param_grid=params, cv=cv, scoring=scoring)\n\n    grid.fit(X_keras_dict[\"train\"], y_keras_dict[\"train\"])\n    print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n    \n    # set best parameters\n    BATCH_SIZE, EPOCHS, OPTIMIZER = grid.best_params_\n    \n    # compute predictions on the holdout\n    y_pred = grid.predict(X_keras_dict[\"trial\"])\n    y_true = y_keras_dict[\"trial\"].values\n    \n    # compute the confusion matrix\n    conf_mat = metrics.confusion_matrix(y_true, y_pred)\n\n    # plot confusion matrix\n    metrics.ConfusionMatrixDisplay(conf_mat, display_labels=EMOTIONS).plot()","10628397":"def build_resnet50_model(optimizer=\"adam\"):\n    \n    # create the base pre-trained model\n    base_model = ResNet50(input_shape=input_keras_shape, weights=\"imagenet\", include_top=False)\n\n    # add a global spatial average pooling layer\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n\n    # let's add a fully-connected layer\n    x = Dense(1024, activation='relu')(x)\n\n    # and a logistic layer -- let's say we have 200 classes\n    predictions = Dense(len(EMOTIONS), activation='softmax')(x)\n\n    # create model\n    model = Model(inputs=base_model.input, outputs=predictions)\n    \n    # compile the model (usually should be done *after* setting layers to non-trainable)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","b922e6df":"def build_grid_search_resnet50_model(optimizer=\"adam\"):\n    \n    model = Sequential()\n    \n    base_model = ResNet50(input_shape=input_keras_shape, weights=\"imagenet\", include_top=False)\n\n    # add a global spatial average pooling layer\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n\n    # let's add a fully-connected layer\n    x = Dense(1024, activation='relu')(x)\n    \n    # and a logistic layer -- let's say we have 200 classes\n    predictions = Dense(len(EMOTIONS), activation='softmax')(x)\n\n    # this is the model we will train\n    model.add(Model(inputs=base_model.input, outputs=predictions))\n    \n    model.add(Dense(len(EMOTIONS), activation=\"softmax\"))\n    \n    # compile the model (usually should be done *after* setting layers to non-trainable)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","881f0262":"def train_and_evaluate(model):\n    \n    # train the model on the new data for a few epochs\n    print(\"Training on the TRAIN set:\")\n    model.fit(X_keras_dict[\"train\"], y_keras_dict[\"train_categorical\"], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\n    # evaluate the model on the test data\n    print(\"\\n\\nEvaluation on the HOLDOUT set:\")\n    model.evaluate(X_keras_dict[\"trial\"], y_keras_dict[\"trial_categorical\"], batch_size=BATCH_SIZE)","762d1e44":"# establish and running gridsearchCV to get best parameters for RESNET50 model\nbuild_and_run_GridSearchCV(build_grid_search_resnet50_model, params)","e1157b44":"# build model\nmodel = build_resnet50_model(OPTIMIZER)\n\n# print details\nmodel.summary()","7dd8ade8":"# start training and evaluating the model\ntrain_and_evaluate(model)","33c53cd2":"def build_grid_search_vgg16_model(optimizer=\"adam\"):\n    \n    # initiate sequential model wrapper\n    model = Sequential()\n    \n    # create the base pre-trained model\n    base_model = VGG16(input_shape=input_keras_shape, weights=\"imagenet\", include_top=False)\n\n    # add a global spatial average pooling layer\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n\n    # let's add a fully-connected layer\n    x = Dense(1024, activation='relu')(x)\n\n    # and a logistic layer -- let's say we have 200 classes\n    predictions = Dense(len(EMOTIONS), activation='softmax')(x)\n\n    # add real model to sequential wrapper  \n    model.add(Model(inputs=base_model.input, outputs=predictions))\n    \n    # add last dense layer\n    model.add(Dense(len(EMOTIONS), activation=\"softmax\"))\n\n    # compile the model (usually should be done *after* setting layers to non-trainable)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","dede679e":"def build_vgg16_model(optimizer=\"adam\"):\n    \n    # create the base pre-trained model\n    base_model = VGG16(input_shape=input_keras_shape, weights=\"imagenet\", include_top=False)\n\n    # add a global spatial average pooling layer\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n\n    # let's add a fully-connected layer\n    x = Dense(1024, activation='relu')(x)\n\n    # and a logistic layer -- let's say we have 200 classes\n    predictions = Dense(len(EMOTIONS), activation='softmax')(x)\n\n    # create model\n    model = Model(inputs=base_model.input, outputs=predictions)\n\n    # compile the model (usually should be done *after* setting layers to non-trainable)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","265f316d":"# establishing and running gridsearchCV to get best parameters for RESNET50 model\nbuild_and_run_GridSearchCV(build_grid_search_vgg16_model, params)","9628da67":"# build model\nmodel = build_vgg16_model(OPTIMIZER)\n\n# print details\nmodel.summary()","35390815":"# start training and evaluating the model\ntrain_and_evaluate(model)","3804e336":"def build_torch_resnet50(pretrained=True):\n    \n    # initiate Resnet50 model\n    model = torchvision.models.resnet50(pretrained=pretrained)\n\n    # replace the last fully connected layer to suit the classification problem\n    model.fc = torch.nn.Sequential(\n        torch.nn.Linear(\n            in_features  = 2048,\n            out_features = len(EMOTIONS)\n        ),\n\n        torch.nn.Softmax(dim=1)\n    )\n    \n    return model","92654dd9":"# build model\nmodel = build_torch_resnet50()\n\n# print details \nmodel","9dbac7c4":"def freeze_layers(model, layers = ['layer1', 'layer2']):\n    # freeze \/ unfreeze all layers by toggling the below\n    for (name, module) in model.named_children():\n        if name in layers:\n            for layer in module.children():\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n                print('Layer in module \"{}\" was frozen!'.format(name))\n        else:\n            for layer in module.children():\n                for param in layer.parameters():\n                    param.requires_grad = True","acf69033":"freeze_layers(model)","6be571ef":"# instantiate an SGD optimizer for the chosen parameters\noptimizer = optim.SGD(filter(lambda param: param.requires_grad, model.parameters()), lr=0.1)","3f590436":"# negative log likelihood useful for multiclass problems\ncriterion = nn.NLLLoss()","48e9b620":"# send the model to chosen device\nmodel.to(DEVICE)","1ad78af2":"def compute_running_loss(model, loader):\n    \n    running_loss = 0\n    \n    for images, labels in loader: # processing one batch at a time\n        \n        # send the batch inputs and targets to the selected device\n        images      = images.to(DEVICE)\n        labels      = labels.to(DEVICE)\n        \n        predictions = model(images)                  # predict labels\n        \n        loss        = criterion(predictions, labels) # calculate the loss\n        \n        # BACK PROPAGATION OF LOSS to generate updated weights\n        optimizer.zero_grad() \n                \n        loss.backward()       \n                    \n        optimizer.step()  \n        \n        running_loss += loss.item()\n        \n    return running_loss \/ len(loader)","3bf870fa":"def calculate_accuracy_score(model, loader):\n    # Calculate accuracy score on loader\n    \"\"\"\n    The below code is written to answer the below questions:\n    - Feed in the entire test dataset to the model, to make predictions? \n    - Could you write code to do this, and measure your model performance?\n    \"\"\"\n    results = []\n\n    for tensor_image_batch in iter(loader):\n        tensor_images, labels = tensor_image_batch\n        \n        # send inputs and targets to device\n        tensor_images         = tensor_images.to(DEVICE)\n        labels                = labels.to(DEVICE)\n        \n        for tensor_image, label in zip(tensor_images, labels):\n            prediction        = model(tensor_image.unsqueeze(0).cuda()).detach().cpu().numpy()\n            max_prob          = max(list(np.array(prediction)[0]))\n            predicted_label   = list(prediction[0]).index(max_prob)\n            \n            results.append(label == predicted_label)\n\n    return sum(results)\/len(results) * 100","3b38be62":"def train_and_validate_torch_model(model, epochs, train_loader=None, valid_loader=None):\n\n    # storing all running losses\n    train_losses = []\n    valid_losses = []\n\n    # storing all accuracy scores\n    train_accuracies  = []\n    valid_accuracies  = []\n\n    for epoch in epochs:\n        print(\"In epoch\", epoch)\n\n        # ----------------------------- Training\n        model.train()\n        train_loss = compute_running_loss(model, train_loader)\n        print(f\"Training Loss: {train_loss}\")\n        train_losses.append(train_loss)\n\n        train_accuracy = calculate_accuracy_score(model, train_loader)\n        print(f\"Training Accuracy: {train_accuracy}\")\n        train_accuracies.append(train_accuracy)\n\n        # ----------------------------- Validation\n\n        model.eval()\n        valid_loss = compute_running_loss(model, valid_loader)\n        print(f\"Validation Loss: {valid_loss}\")\n        valid_losses.append(valid_loss)\n\n        valid_accuracy = calculate_accuracy_score(model, valid_loader)\n        print(f\"Validation Accuracy: {valid_accuracy}\")\n        valid_accuracies.append(valid_accuracy)\n\n        print()\n\n    return train_losses, train_accuracies, valid_losses, valid_accuracies","d2e53b78":"# setting number of epochs\nepochs = range(1, 1 + 2)\ntrain_losses, train_accuracies, valid_losses, valid_accuracies = train_and_validate_torch_model(model, epochs, train_loader=train_loader, valid_loader=valid_loader)","fff0a4f8":"plt.plot(epochs, train_losses, 'g', label='Training Loss')\nplt.plot(epochs, valid_losses, 'b', label='Validation Loss')\nplt.title('Training and Validation Losses')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","6485fc82":"plt.plot(epochs, train_accuracies, 'g', label='Training Accuracy')\nplt.plot(epochs, valid_accuracies, 'b', label='Validation Accuracy')\nplt.title('Training and Validation Accuracies')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracies')\nplt.legend()\nplt.show()","dae41b4f":"import random\nfrom PIL import Image","87d7ab08":"single_sample_for_each_emotion_dataset = df.groupby(\"label\").first()\n\n# set plotting options\nfig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, _)) = plt.subplots(2, 4, figsize=(10, 5))\n_.set_visible(False)\n\nfor index, label in enumerate(EMOTIONS):\n    img = single_sample_for_each_emotion_dataset['img_as_matrix'][label]\n    \n    exec(f\"ax{index + 1}.imshow(img)\")\n    exec(f\"ax{index + 1}.set_title(label.title())\")\n    \nfig.tight_layout()","c9672d3b":"labels      = []\npredictions = []\n    \nfor image, label in zip(single_sample_for_each_emotion_dataset['img_as_matrix'], single_sample_for_each_emotion_dataset['labels_numeric']):\n\n    image = TRANSFORMER(image)\n    \n    image = image.to(DEVICE)\n    \n    label = torch.as_tensor(label, device=torch.device('cuda'))\n\n    image = image.reshape(3, 48, 48)\n\n    model.eval()\n    \n    pred        = model(image.unsqueeze(0).cuda()).detach().cpu().numpy()\n    max_prob    = pred.flatten().max()\n    pred_label  = np.where(pred.flatten() == max_prob)[0][0]\n    \n    labels.append(EMOTIONS[label.item()])\n    predictions.append(EMOTIONS[pred_label])","99441a70":"for label, prediction in zip(labels, predictions):\n    print(f\"{label} {'==' if label == prediction else '!='} {prediction}\")","68a316ae":"# finding what amount is equal to 20%\nmislabel_limit = LIMIT \/ 5\n\n# start to corrupt labels for 200 of the data\nlimit_counter  = {i:0 for i, _ in enumerate(EMOTIONS)}\n\nX_smote_corrupted = []\ny_smote_corrupted = []\n\nfor x, y in zip(X_smote, y_smote):\n    if limit_counter[y] < LIMIT:\n        X_smote_corrupted.append(x)\n        y_smote_corrupted.append(y)\n        \n        limit_counter[y] += 1\n\nX_smote_corrupted = np.array(X_smote_corrupted)\ny_smote_corrupted = pd.Series(data=y_smote_corrupted)","d600812a":"# convert X and y to Tensors\nX_torch_smote_corrupted = torch.Tensor(X_smote_corrupted)\ny_torch_smote_corrupted = torch.as_tensor(y_smote_corrupted, device=torch.device('cuda'))\n\n# split data into train, valid and test\nX_torch_train_corrupted, X_torch_remained_corrupted, y_torch_train_corrupted, y_torch_remained_corrupted = train_test_split(\n    X_torch_smote_corrupted, y_torch_smote_corrupted, test_size=0.20, random_state=42\n)\nX_torch_valid_corrupted, X_torch_trial_corrupted, y_torch_valid_corrupted, y_torch_trial_corrupted = train_test_split(\n    X_torch_remained_corrupted, y_torch_remained_corrupted, test_size=0.50, random_state=42\n)","65ddea59":"# reshaping for modeling\nshape = (NCHANNELS, HEIGHT, WIDTH)\n\nX_torch_train_corrupted = X_torch_train_corrupted.reshape((X_torch_train_corrupted.shape[0], shape[0], shape[1], shape[2]))\nX_torch_valid_corrupted = X_torch_valid_corrupted.reshape((X_torch_valid_corrupted.shape[0], shape[0], shape[1], shape[2]))\nX_torch_trial_corrupted = X_torch_trial_corrupted.reshape((X_torch_trial_corrupted.shape[0], shape[0], shape[1], shape[2]))","e78b6b11":"# create train, test and validation datasets for the data loader\ntrain_corrupted_dataset = TensorDataset(X_torch_train_corrupted, y_torch_train_corrupted)\nvalid_corrupted_dataset = TensorDataset(X_torch_valid_corrupted, y_torch_valid_corrupted)\ntrial_corrupted_dataset = TensorDataset(X_torch_trial_corrupted, y_torch_trial_corrupted)","47eafdb8":"# apply transformations to the train set\ntrain_corrupted_dataset.transform = TRANSFORMER\n\n# apply transformations to the validation set\nvalid_corrupted_dataset.transform = TRANSFORMER\n\n# apply transformations to the test set\ntrial_corrupted_dataset.transform = TRANSFORMER\n\n# create a dataloader for each subset of data\ntrain_corrupted_loader = DataLoader(train_corrupted_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\nvalid_corrupted_loader = DataLoader(valid_corrupted_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\ntrial_corrupted_loader = DataLoader(trial_corrupted_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)","a12847d5":"# building model\nmodel = build_torch_resnet50()\n\n# assigning model to cuda\nmodel.to(DEVICE)\n\n# freezing layers\nfreeze_layers(model)\n\n# start training\ntrain_and_validate_torch_model(model, epochs, train_loader=train_corrupted_loader, valid_loader=valid_corrupted_loader)","58b2ee92":"# building model\nmodel = build_torch_resnet50(pretrained=False)\n\n# assigning model to cuda\nmodel.to(DEVICE)\n\n# freezing layers\nfreeze_layers(model)\n\n# start training\ntrain_and_validate_torch_model(model, epochs, train_loader=train_loader, valid_loader=valid_loader)","d99d2555":"saliencies = []\n\nfor path in df['path'][:100]:\n    \n    image = Image.open(path).convert('RGB')\n\n    # use global usage created transformer\n    image = TRANSFORMER(image)\n    \n    image = image.reshape(1, 3, 48, 48)\n\n    model.eval()\n\n    image = image.to(DEVICE)\n\n    image.requires_grad_()\n\n    output = model(image)\n\n    output_idx = output.argmax()\n    output_max = output[0, output_idx]\n\n    output_max.backward()\n\n    saliency, _ = torch.max(image.grad.data.abs(), dim=1) \n    saliency    = saliency.reshape(48, 48)\n    \n    saliencies.append(saliency)","de77f87a":"def plot_img_vs_saliency(image, saliency):\n    fig, ax = plt.subplots(1, 2)\n    ax[0].imshow(image.cpu().detach().numpy().transpose(1, 2, 0))\n    ax[0].axis('off')\n    ax[1].imshow(saliency.cpu(), cmap='hot')\n    ax[1].axis('off')\n    plt.tight_layout()\n    fig.suptitle('The Transformed Image vs Saliency Map')\n    plt.show()","38add47c":"# this was initially written for all\n# random_selected_index = random.randint(0, LIMIT * len(EMOTIONS) - 1)\n\n# after running into memory issue updated to this one\nrandom_selected_index = random.randint(0, 100 - 1)\n\nimage    = Image.open(df['path'][random_selected_index]).convert('RGB')\nimage    = TRANSFORMER(image)\n\nsaliency = saliencies[random_selected_index]\n\nplot_img_vs_saliency(image, saliency)","26908523":"**Task 54:** If you noted some potential biases in the modeling\/dataset above, discuss how you could help mitigate these biases (you don't need to implement, just discuss). If you didn't note any biases in this dataset, discuss what biases there could have been, and how the dataset designers might have helped mitigate them.\n\n> As already described, we tried to mitigate the bias with SMOTE but the results were not satisfactory due to low quality generated. What can be done instead is to remove the `disgust` label, as already mentioned, and equalize the size of other labels to the second lowest representative. ","b6a7f700":"**Task 57:** Discussion of saliency mapping results.\n\n> The saliency map showcases which pixels are the most important in image classification decision. For instance, on the image above, we can see that the parts of mouth and eye contours are colored darker, which means these parts are more active in the neural network decisions. At the same time, the size of our image is rather small (48x48), hence it's difficult to see these pixels on a larger scale.","8eca8f2f":"**Task 35:** Write code that places the model on the GPU, if it exists, otherwise using the CPU.","e8030476":"**Task 45:** Graph training versus validation loss using matplotlib.pyplot (or other). Was your model overfitting, underfitting, or neither?\n\n> The model is underfitting because we have trained the model for only 3 epochs. The reason why we chose to train for just this number of epochs is due to the Kaggle memory allocation issue, as copied below.\n\n*RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.90 GiB total capacity; 613.11 MiB already allocated; 21.75 MiB free; 646.00 MiB reserved in total by PyTorch)*","f84d82c5":"**Task 17:** Set up a gridsearchCV with 5-fold cross validation (scikit-learn) or equivalent in PyTorch. Discuss what accuracy metric you chose and why.\n    \n    In our task our main desire it to find the image labeling correct most of the time, but if we fail it is not  critical. Therefore, most of the time the accuracy is a right metric.\n> 1. F1 Score would be suitable for the case, since it tries to find the balance between precision and recall.\n2. Recall can also be suitable for the case, since it answers the question \"how many relevant items has been choosen?\"","62e0e68f":"**Task 9**: If you are using images\/text, discuss whether you are performing classification or regression on your dataset and why (instead of the other one).\n\n> We are performing a classification on our dataset and the reason is that we have non-continuous values as our y-labels (in this instance, emotion classes). Regression could have been used if we were trying to predict a continuous value but here the prediction target is discrete.","b2c8c802":"**Task 26:** Correctly set up DataLoaders for the three folders (train, validation, holdout). Discuss what options you chose for these loaders, and why (including batch size, shuffling, and dropping last).\n> DONE ABOVE","9bfd31c3":"**Task 22:** Train and tune another type of model on your training dataset. Using the best performing hyperparameters, test this model on your holdout. How did it perform, compared to your earlier model? Do you think your results will generalize?\n\n> Since we are using SMOTE technique, the results are not expected to generalize well due to the fact that there is a very low representation of the `disgust` label in our dataset, and hence SMOTE method can be ineffective in model fine-tuning. If, however, we were to drop the `disgust` label instead, and equalize the other labels in terms of their size to the second lowest label (around 4,000), then the results could have been generalizable. So, we traded-off the better accuracy score for the implementation of a new method not widely tried before.\n\nThe model below has a higher accuracy score on the holdout set (vs the score achieved on an earlier model) and the results are not expected to generalize due to the different structures of models and the impact that SMOTE has on them.","73388516":"**Task 16:** Define a grid to tune at least three different hyperparameters with at least two different values each. Discuss why you think these parameter values might be useful for this dataset.\n\n> Upon researching, we found that the below chosen hyper-parameters are the most commonly used and the most effective in fine-tuning the modelling process. Therefore, these parameters are of interest to see how our model performs with each of them and which set is the best.","c8deecf7":"**Task 34:** Choose and instantiate a loss function. Discuss your choice.\n> `Negative log-likelihood` is used as the loss criterion, due to the reason that we're dealing with a multi-class problem + it helps with handling small probabilities by the computer.","15d08b79":"**Task 8:** Discuss (and implement if applicable) whether or not you need to scale\/normalize your features, and which ones, if any.\n\n> 1. Scaling is important to ensure that all images are of the same size and hence, can be accepted by the model. Our image dataset is already well-scaled, with a fixed height and weight parameters as defined at the beginning of this notebook.\n2. Normalization in our image dataset is needed so that each image has similar pixel distribution. This way we reduce the skewness in our dataset.","a82f4825":"### Reshape Datasets","ae066b9c":"**Task 12**: What customized dataset augmentation you would use (not required to implement) for images?\n\n> 1. improving the contrast on the images (adaptive equalization)\n2. applying flipping, rotations to stabilize model performance regardless of angle\n3. fill in the gaps on the image to the nearest pixel","faaaf34f":"### Notes\n1. For visualization purposes **test** key word has been replaced with **trial** in variable naming to keep character numbers equal to `5`\n2. **UPPER CASE** words are global variables. They are declared and maintaned only above.","4850e9de":"**Task 50:** Generate three datasets of our inputs, where each has only two of the classes. What do you predict the performance should be for three binary classifiers trained on these three datasets? Re-train your model on these three datasets, and discuss your results.\n\n> We believe this task is not applicable to our project since we are performing emotion recognition which requires to classify all the different variations of facial expressions, not just two. Performing binary classification does not yield any value for the project analysis, and hence, we forego this step.","39e0d4cc":"The dataloaders are set up in the cell above for all three data subsets: train, validation and test. `BATCH_SIZE` was chosen to be 64, which is rather small, however, it means the model can start learning better since the number of parameters it needs to update per epoch is now higher. `shuffle` is set to `True` so that at each epoch the data gets re-shuffled which helps avoid under- or over-fitting. `drop_last` is set to `True` to avoid cases where a batch contains just one sample image.","fe0bbc9f":"**Task 52:** Take a look at each of the items in all classes individually. What aspects of the item (such as backgrounds) might be influencing the decision-making of the model, besides the salient parts themselves?\n\n> *Face position* might be influencing the decision-making (i.e. if a face is turned side-ways, the emotion can be less recognizable than if the face is staring straight ahead). Another aspect could be the presence of `logos` on some images (e.g. when the image is copyrighted and has a label on it). This latter aspect creates extra noise and can skew the model performance. Another feature affecting model decisions could be the way *eyebrows are shaped*, *contours of eyes* (e.g. eye socket size) and *mouth appearance*. For example, a surprise label is usually associated with raised eyebrows and\/or open mouth. ","091acbb7":"**Task 3:** Correct explanation generalization from such a holdout split.\n> For the train \/ test split the following percentages were chosen: 80%-20%. Of the 80% coming from the train set, 20% is the validation set. Since the data is already balanced, the choice of a random split is a reasonable one.","f7ef4779":"**Task 51:** Generate a dataset from your original dataset where 20% of the classes in one class are mis-labelled as the remaining two classes. How do you think your model performance will be impacted? Re-train your model on this test dataset, and discuss your results.\n\n> The expectation is that the performance will be slightly higher if we mislabel some items due to the fact that the original model already misclassifies a lot of the data. We can see the performance of the model slightly inreases but overall stays the same due to the fact that it was not generalizable originally anyway. ","c82e0395":"**Task 46:** Make a list of reasons why your model may have under-performed.\n\n> The first reason is low number of epochs and reduced dataset size due to Kaggle resource limitations. The second reason is the usage of SMOTE technique that generated lower quality images and hence could have impacted the model results. This especially affects those labels that have lower representation, increasing the size of the noisy data. The third reason is the fact that we haven't tried more hyper-parameters with GridSearch - if the range chosen were larger, the results could have been stronger.\n\n> The reason we chose to use Kaggle is due to the availability of CUDA and faster processing than on our local laptop.","9fa8c554":"**Task 49:** Generate a dataset of just three items, one for each class, and show your model correctly labels them. (display each item in your notebook, pass it to your model, and then print the prediction).\n> In our case this will be 7 items since having not 3 but 7 classes.","6dc36c8c":"**Task 56:** Correctly implement saliency maps for all images. If doing NLP, discuss feature importances or other metric.\n> Due to Kaggle memory allocation issue, we limit the saliency mapping to first 100 instances\n\n*RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.90 GiB total capacity; 519.98 MiB already allocated; 21.75 MiB free; 646.00 MiB reserved in total by PyTorch)*\n","a6304d2d":"**PAPER 1:**\n\n**Title:**\n\nFacial Emotion Recognition: State of the Art Performance on FER2013 \n\n**Link:**\n\nhttps:\/\/arxiv.org\/abs\/2105.03588\n\n**Summary:**\n\nThis research is aimed at recognizing various emotions on the FER-2013 image dataset. The context is the same as our project.\n\nThe methodology applied by the authors of this paper includes:\n\nImage augmentations applied randomly to each image, in particular:\n- image rescaling, \n- horizontal and vertical shifts, \n- rotations,\n- reduction of image size to 40 x 40 to speed up the training process,\n- normalization of image by dividing each pixel by 255.\n\nTraining process uses the VGGNet model with 300 epochs, a learning rate of 0.9 and a cross-entropy loss. The VGGNet architecture is composed of 4 convolutional stages and 3 fully connected layers. Each convolution stage is made up of two convolutional blocks and a max-pool layer. The activation function in use is ReLU. \n\nTo prevent vanishing gradient problems, batch normalization is applied. \n\nGridSearch is used to ensure that the parameters chosen are the best ones. SGD is used as the optimizer, however, other types of optimizers, such as Ada, Adadelta, etc. are also evaluated. \n\nVarious learning rate schedulers are tested for performance, such as Reducing Learning Rate on Plateau, constant rates (OneCycleLR), Cosine Annealing, etc.\n\nThe model is fine-tuned in the following way:\n\n- The initially trained parameters are re-loaded.\n- Re-training is performed for 50 epochs using a very small learning rate of 0.0001 so that the step update is small.\n- Cosine Annealing, Cosine Annealing with warm restarts is used to sway the learning rate in both directions without the need for a significant parameter weight change. \nThis first way helps with understanding how effective the initial tuning is.\nAnother way of fine-tuning used is to include validation set into training set to allow for better learning.\n\nThe results of this paper are as follows:\n\n- Out of all optimizers, SGD performed the best, attaining a validation accuracy of circa 73%.\n- Out of all learning rate schedulers, Reducing Learning Rate on Plateau performed best, due to the fact that it makes a decision on the next step to take dynamically, as opposed to statically.\n- Fine-tuning improves the accuracy score by 0.05%-0.42%, with the final best accuracy score at 73,28%. \n- Disgust and anger classes are the most confusion-causing ones.\n- The obtained scores outperform previously tried single-network models.","0223df1e":"**Task 15:** Instantiate a model of your choosing.","6ed3bcf9":"# MILESTONE - II\n### Import Libraries","51b5f28a":"**Task 18:** Train your model using grid search, and report the best performing hyperparameters.","1cf3ba79":"**Task 14:** Discuss and implement how you will handle any dataset imbalance.\n\nThere are a number of ways to deal with data imbalance:\n> 1. *Over-sampling the under-represented class*. However, given the big discrepancy in the counts between such classes as `surprise` and `happy`, this method might cause the model to overfit.\n2. *Under-sampling the over-represented class*. However, this can result in a loss of big amounts of useful data.\n3. ***SMOTE (Synthetic Minority Oversampling Technique)***. This method selects examples of data in the feature space (a data point and its nearest neighbor), and then creates a synthetic data point in-between them. In essence, this is an augmentation technique to add varying copies of existing data. We chose to go with this option since we do not want to lose data and do not want to experience the effects of over-fitting.","ae74ad8f":"**Task 53:** Is the data biased in any way that could impact your results? Why or why not?\n\n> The data is originally biased since it has a higher representation of such labels as `happy` and a lower representation of such labels as `disgust` and `surprise`. Hence, this data skewness significantly affects the results in a way that the model learns more about the labels more heavily represented than those that have a lower representation. This imbalance prompted us to use SMOTE technique, which however can be still ineffective due to the low quality images generated.\n\n> Another potential bias could be a lower representation of racial or cultural minorities in this dataset.","67dad2ed":"**Task 1:** Load data correctly and show contents in a cell.\n\n> 1. Database is already divided into test and train in the emotion named folders but it is required to have label data to start working. While doing this, all the data has been combined under unique set which will be seperated later according to the **20\/80** rule.\n2. A sample of each emotion is illustrated by looping through each emotion sub-folder and looking up the first image to plot as an example. Each example image is annotated with the emotion label assigned to it.","2ea055bc":"# MILESTONE - III","9683b075":"### Globals Variables","50f8747c":"### Limiting Dataset Size","c0c2ec19":"**Task 27:** Instantiate any pre-trained model. Discuss why you chose it amongst the others.\n\nA ResNet50 model was chosen for training due to the following reasons:\n1. it does not sacrifice network depth for the sake of reducing the vanishing gradient problem, and in fact\n2. it solves the vanishing gradient problem with \"shortcut skip connections\" - identity maps (residual blocks). \n  \n*Note that the last fully connected layer is replaced to suit our classification problem, i.e. the number of labels. `Softmax` activation function is used because we are dealing with a multi-class problem.*\n\n*Dropout* was not used in the last fully connected layer because we want the model to learn the features as intricately as possible, and if we drop a layer, our training loss can end up being higher and lead to underfitting. Also, batch normalization is used throughout the model structure which already regularizes our model, so adding a dropout will double-penalize the training.\n\n*BatchNormalization* is usually used between a convolutional layer and an activation function, for regularization purposes, to make sure that an input is distributed around approximately the same mean and standard deviation amongst layers. For the purpose of regularization (i.e. to make sure the model does not overfit), it's already quite prevalent in the standard resnet50 model, therefore, we don't see a need to add it in at this stage.","e224f09d":"**Task 11**: For imagery\/text: Show a histogram of the distribution of pixels or word embeddings across your dataset.","507bc272":"**Task 55:** Correctly train your model without pre-training (and discussion how this affects performance)\n\n> The results without pre-training are higher than those with pre-training. This could be due to the fact that we're not using pre-trained weights (trained on another dataset, the weights of which may not be generalizable to our dataset).","f7dd1e39":"**Task 6 and 7:** Handle any missing data. For imagery datasets, discuss what images you might drop and why. The holdout dataset also contains missing data\/bad images. Discuss how you handled this in your holdout.\n> 1. There are images that are far off from the way an average image looks like in an emotion category. To solve this, we find an average image per emotion class, compute the mean error for each image and find whether the mean error is within standard deviation limits. If the mean error is below or above the standard deviation boundaries, then we consider the image as anomalous and drop it.\n2. We perform this for the whole dataset, before splitting it into train \/ holdout.\n3. In the future, we plan to apply augmentations to images to increase their quality \/ contrast. For example, adding salt and pepper noise to the train dataset will help to train the model to predict more or less accurately on the bad holdout images.","ed849d4b":"**Task 5:** Discussion of how the dataset distribution can\/will affect your modeling.\n\n> Depending on how over- or under-represented some emotions are, the model can learn a skewed pattern and hence, not generalize well enough when it comes to test data. For instance, in the cell below, we're plotting the number of images per emotion class to understand the underlying distribution. As can be observed, the representation of different classes is dramatically different from others: e.g., `disgust` is under-represented, while `happy` is over-represented. This means that our model can be skewed in its predictions due to such discrepancies and hence, imbalance will have to be handled.","8527ad28":"**Task 19:** Calculate accuracy, precision and recall on the holdout dataset. Discuss which metric you think is most meaningful for this dataset, and why\n\n> *Precision* and *Recall*, both are very important in the content. But in any case if you desire to have high precision then you sacrifice recall and vice versa. In the particular usage, precision is slightly over weigting since having any false negative does not have a high cost contrast to sick patients example. However, in the particular dataset, we are looking for getting as much correct answer as possible and therefore, *Accuracy* is the correct metric to go with.","54aad3f1":"# MILESTONE - I\n### Import Libraries","5d5ccc3b":"**Group Members:**\n\n- Aydin Bagiyev (abagiyev@gwu.edu)\n- Narmin Jamalova (Aydin's awesome!!! lovely girlfriend! njamalova54@gwu.edu)","6e5fe750":"**Task 25:** Repeat the step above for test and validation transformations.\n> DONE ABOVE","9cb2fff0":"### Import Libraries","f0a3f94d":"**Task 48:** Graph training versus validation accuracy using matplotlib.pyplot (or other). Score your model on its predictions on the holdout. Discuss why you think your results will or will not generalize.\n\n> To repeat, we believe the results will not generalize due to the same reason already listed above for other models (i.e. usage of SMOTE, low number of epochs).","7073c796":"**Task 29:** Replace the head of the model with sequential layer(s) to predict our three classes.\n> DONE ABOVE","ed6bf718":"**Task 3:** For imagery datasets, provide the \"average image\" for each class.\n\n\n> In this section, an average image per emotion class is displayed. The average image is computed by adding together the array representations of each image in the emotion sub-folder and dividing that sum by the total number of images in the sub-folder to find the mean.\n\n*The resulting average images per emotion class are then displayed.*","e4a91ef1":"**Task 33:** Choose and instantiate an optimizer. Discuss your choice.\n\n>`SGD` (Stochastic Gradient Descent) was chosen as an optimizer to enable faster training since it approximates true gradient descent by considering only random samples of the dataset, instead of the whole. Also, since our dataset is large, SGD can converge faster than other types of optimizers.","33d8a3d8":"**Task 31:** Did you use dropout in the step above? Why or why not?\n> In the original implementation of the ResNet50, dropout has not been used. The model has a huge implementation in many cases with the current structure and therefore, adding additional modifications is not necessary.","fece7cbe":"**Task 20:** Discuss how the model performance on holdout compares to the model performance during training. Do you think your model will generalize well? Why or why not?\n\n> Since we are using SMOTE technique, the results are not expected to generalize well due to the fact that there is a very low representation of the `disgust` label in our dataset, and hence SMOTE method can be ineffective in model fine-tuning. If, however, we were to drop the `disgust` label instead, and equalize the other labels in terms of their size to the second lowest label (around 4,000), then the results could have been generalizable. So, we traded-off the better accuracy score for the implementation of a new method not widely tried before.\n\nThe model above has a slightly lower accuracy score during training (vs the score achieved on hold-out) due to possible underfitting as a result of the paragraph above. Moreover, due to using Kaggle, we had resource limitations in terms GPU and memory and hence, could not train for more epochs and had to limit the dataset size to 1,000 samples per category.","3b03403f":"**PAPER 2:**\n\n**Title:**\n\nAn Emotion Recognition Model Based on Facial Recognition in Virtual Learning Environment \n\n**Link:**\nhttps:\/\/reader.elsevier.com\/reader\/sd\/pii\/S1877050917327679?token=CC8074C46A774783A2387453DFE409E307F7DFD6366145F3086C272BF7E058D43130283E68C5A624DC3E41FDEF266224&originRegion=us-east-1&originCreation=20211209212305 \n\n**Summary:**\n\nThis paper is particularly focused on recognizing emotions in virtual settings to enable a better virtual educational environment, more effective human-computer and student-teacher interactions. The specificity of this research is that the recognition should take place in real time. The database used is JAFFE database containing 213 images of size 256x256 of Japanese women\u2019s faces. \n\nThe methodology of this paper is as follows:\n\n- The Haar Cascades classifier method is used to determine if a face exists on a given image. If it does, eyes and mouth regions are identified and cropped. The aforementioned method sums pixel intensities in each image region and calculates the differences between sums. This helps identify where in the image the face is located.\n\n- Filters are applied to remove unwanted noise and edges are detected. Feature extraction is then used to grab the most important pixels.\n\n- There are 6 emotion classes, including sadness, happiness, fear, surprise, anger and disgust. \n\n- The model used is a neural network with 1 hidden layer with 20 neurons and 7 output neurons. \n\n- The accuracy achieved on each emotion category varies between 76%-94%. \n\nHowever, this research does have gaps such as:\n\na) the position of the image is not taken into account, and no significant augmentations are performed.\nb) more complex neural network architectures might yield better accuracy scores to reduce misclassifications.","c270eb9e":"**Task 9**: If you are using images\/text, discuss whether you are performing classification or regression on your dataset and why (instead of the other one).\n\n> We are performing a classification on our dataset and the reason is that we have non-continuous values as our y-labels (in this instance, emotion classes). Regression could have been used if we were trying to predict a continuous value but here the prediction target is discrete.","27dd6e57":"**Task 28:** Write code to freeze\/unfreeze the pretrained model layers.\n> Freezing and unfreezing is simply done by manupulating `requires_grad` parameter of the model. Setting it True makes it unfreeze and on the otherhand False reverse the process. When a pretrained model is used, it is advised to freeze early layer which are basic edge detections, and unfreeze later one which focus more to details.","4b196dc7":"**Task 2:** Holdout dataset split as specified.","2e338e17":"**Task 24:** Define a list of image transformations to be used during training, passing them to transforms.Compose(). Discuss why you think these transformations might help.\n\nWe will use `torchvision.transforms` to apply scaling\/normalization techniques.\n\n> 1. `.ToTensor()`: converts an image from array to tensor format and adjusts pixel intensities to be between 0 and 1. This is essentially a pixel normalization technique.   \n2. `.Normalize()`: normalizes the image to be between -1 and 1 (given the provided 0.5 input parameter values). This helps reduce skewness and hence, contributes to faster training.   \n3. `.RandomHorizontalFlip()` and `.RandomRotation()`: this creates synthetic augmented images which can help the model make correct predictions regardless of image position. ","7156d7bb":"**Task 13**: Separate your training data into features and labels.\n> For the latter task both PYTORCH and KERAS packages are used and therefore, splitting phase has been forked into 2 parts for each distribution.","efd1ff97":"# Emotion Recognition Using Deep Learning","3530dc02":"**Tasks Combined: 36, 37, 38, 39, 40, 41, 42, 43, 44**\n1. Correctly set up your model to train over 20 epochs.\n2. Correctly set up your model to use your batches for training.\n3. Correctly make predictions with your model (the predictions can be wrong).\n4. Correctly call your loss function and back-propagate its results.\n5. Use the optimizer correctly to update weights\/gradients.\n6. Correctly record training losses for each epoch.\n7. Correctly set up validation at each epoch.\n8. Correctly record validation losses for each epoch.\n9. Correctly record training and validation accuracies for each epoch\n**Note:** *The occurence of the task description is relative to the about index sequence.*","2896dc1c":"**Task 30:** What activation function did you use in the step above? Why?\n> The basic structure of the resnet50 model has not been changed. Only the last layer is modified to suit to needs. Since the task is not multi target detection, softmax activation function is being utilized.","3373c434":"**Task 10:** Give an example of an ordinal feature that you've seen used by others, when it should have been treated as a categorical.\n\n> Example: `color` feature (red, green, blue). People might assume there's a color order, but in fact this is a categorical feature, since there's no particular ordering (and if there is, it's a rather complex one). We can treat different pixel intensities within `green` as an ordinal feature, but not when it comes to differing colors such as `green` vs `red`.","5e2fe1c9":"**Task 21:** Generate a confusion matrix and discuss your results.\n> In confusion matrix the desired output is to have all zero except diagonal. `happy` and `neutral` labels are the most misclassified ones, due to the fact that `happy` is more representative in the database and hence the model learns more about that label and overfits, whilst `neutral` is hard to identify by image features and often gets mistaken for other emotions. The best classified ones are `surprise`, `angry` and `fear`.\n","067b313e":"**Task 32:** Did you use batch normalization in the step above? Why or why not?\n> In the original implementation of the ResNet50, batch normalization(BatchNorm2d) has been used in almost all layers. Batch normalization accelerates training, in some cases by halving the epochs or better, and provides some regularization, reducing generalization error.","a79c649d":"**Task 23:** Next, repeat training and tuning on the same data with a third model, dissimilar from the other two. Do you need to do any additional feature cleaning or scaling here? Why or why not?\n\n> The next model tried is VGG16, which is different from the ones above only in terms of its internal structure, i.e. the depth of its layers. Hence, no extra cleaning or scaling is needed.","d834ef74":"# General Setups\n### Install Libraries","02b6e4b3":"# MILESTONE - IV\n\n> Below is the summary of two papers amongst others we've researched for this project.","4d35c595":"**Task 47:** Make a list of ways you could improve your model performance (you don't have to implement these unless you want to).\n\n> First, we can improve performance by removing SMOTE technique and simply remove the `disgust` label, while  decreasing the size of the other categories to fit the size of the second smallest representation. \n\n> Secondly, we can improve performance by increasing the number of epochs for training. In some of the available researches, training has been done for as many as 300 epochs, and therefore could yield better results for us, too."}}