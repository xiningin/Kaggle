{"cell_type":{"ff72b2bb":"code","d31fdbfa":"code","87a0a4b4":"code","9a873a5d":"code","1f16b4b6":"code","12161a3b":"code","f684bcf3":"code","a4533d9f":"code","1976582c":"code","adbb2e12":"code","819a0483":"code","449456cc":"code","6c88f689":"code","6b820736":"code","95600249":"code","48d99396":"code","5c3000d3":"code","fb0bb648":"code","68f260c4":"code","d483ed80":"code","2169fe36":"code","d3bc479e":"code","293c5a93":"code","40a84e95":"code","6544d45e":"code","338d0124":"code","d61224b1":"code","aea0a515":"code","ac177950":"code","c1f5d6e6":"code","cc079f62":"code","61cef5d4":"code","e2f64fd3":"code","3bffb138":"code","07f6cd07":"markdown","ba08cb68":"markdown","3cc4d10c":"markdown","3a160eaa":"markdown","52e60a90":"markdown","54d218d4":"markdown","dae12f62":"markdown","19d81432":"markdown","d1a1e395":"markdown","deb48b7a":"markdown","f3280cb0":"markdown","f06c830b":"markdown","49747b4b":"markdown"},"source":{"ff72b2bb":"import pandas as pd\nimport numpy as np\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n","d31fdbfa":"df = pd.read_csv(\"..\/input\/netflix-shows\/netflix_titles.csv\")\ndf.head()","87a0a4b4":"cols = ['title', 'type', 'listed_in', 'director', 'cast', 'rating', 'description']\ndf['combined'] = df[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n\n# remove non-english words. Reference: https:\/\/datascience.stackexchange.com\/questions\/46705\/to-remove-chinese-characters-as-features\ndf['combined'] = df['combined'].map(lambda x: re.sub(\"([^\\x00-\\x7F])+\",\"\", x))","9a873a5d":"documents = df['combined']\n\n# Create the Document Term Matrix\ncount_vectorizer = CountVectorizer(stop_words='english') # convert all words to lowercase and remove stop words\nsparse_matrix = count_vectorizer.fit_transform(documents)\n\n# Convert Sparse Matrix to Pandas Dataframe \ndoc_term_matrix = sparse_matrix.todense()\nmatrix_df = pd.DataFrame(doc_term_matrix, \n                  columns=count_vectorizer.get_feature_names(), index=df.index)\n","1f16b4b6":"similarity_scores = cosine_similarity(sparse_matrix, sparse_matrix) \n\nscores_df = pd.DataFrame(similarity_scores )\nscores_df","12161a3b":"def recommend(title,scores_df, df):\n    recommended = []\n    \n    # getting title's index \n    title = title.lower()\n    df['title'] = df['title'].str.lower()\n    index = df[df['title']==title].index[0]\n    \n    top10_list = list(scores_df.iloc[index].sort_values(ascending = False).iloc[1:11].index)\n    \n    \n    for each in top10_list:\n        recommended.append(df.iloc[each].title)\n    \n    return recommended\n    \n    ","f684bcf3":"recommend('Avengers: Infinity War',scores_df, df)","a4533d9f":"recommend('Naruto Shippuden : Blood Prison',scores_df, df)","1976582c":"# Importing modules\nimport pandas as pd\nimport os\nimport re\n\n# LDA Model\nimport gensim\nfrom gensim.utils import simple_preprocess\nimport gensim.corpora as corpora\nfrom pprint import pprint\nfrom gensim.models import CoherenceModel\nimport spacy\n\n# NLTK Stop words\n# import nltk\n# nltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\n#plotting libraries!\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Import the wordcloud library\nfrom wordcloud import WordCloud\n\n# Visualize the topics\nimport pyLDAvis.gensim\nimport pickle \nimport pyLDAvis\n\n","adbb2e12":"df = pd.read_csv(\"..\/input\/netflix-shows\/netflix_titles.csv\")\ndf.head()","819a0483":"# remove non-english words. Reference: https:\/\/datascience.stackexchange.com\/questions\/46705\/to-remove-chinese-characters-as-features\ndf['description'] = df['description'].map(lambda x: re.sub(\"([^\\x00-\\x7F])+\",\"\", x))","449456cc":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations and special characters\n\ndata_words = list(sent_to_words(df['description']))\n\n# print(data_words)","6c88f689":"stop_words = stopwords.words('english')\n# stop_words.extend(['']) #extend existing stop word list if needed\n# print(stop_words)\n\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n","6b820736":"# Build the bigram \nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=10) # higher threshold fewer phrases.\nbigram_mod = gensim.models.phrases.Phraser(bigram)\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]","95600249":"def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","48d99396":"# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\nnlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\nprint(data_lemmatized[:1])","5c3000d3":"# spotted bigrams such as high_school, civil_war, martial_arts\nprint(data_words_bigrams)","fb0bb648":"# Import the wordcloud library\nfrom wordcloud import WordCloud\nimport itertools\n\n# Join the different processed titles together\nlist_of_words = list(itertools.chain.from_iterable(data_lemmatized))\nlong_string = ','.join(word for word in list_of_words)\nlong_string\n\n# # Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue', collocations = False,  random_state=1)\n\n# # Generate a word cloud\nwordcloud.generate(long_string)\n\n# # Visualize the word cloud\nwordcloud.to_image()","68f260c4":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Filter out tokens that appear in only 1 documents and appear in more than 90% of the documents\nid2word.filter_extremes(no_below=2, no_above=0.9)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:10])\n# print(id2word[:10])","d483ed80":"import operator\n\n\nword_freq = []\nword_list = []\ncount_dict = {}\nfor n in id2word.values():\n    count_dict[n] = list_of_words.count(n)\nsorted_dict = sorted(count_dict.items(), key=operator.itemgetter(1), reverse=True)[0:10]\n\nplt.figure(figsize=(16,6))\nplt.bar(range(len(sorted_dict)), [val[1] for val in sorted_dict], align='center')\nplt.xticks(range(len(sorted_dict)), [val[0] for val in sorted_dict])\nplt.xticks(rotation=70)\nplt.xlabel('words')\nplt.ylabel('counts')\n\n# setting data labels\nax = plt.gca()\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/2., p.get_height(), '%d' % int(p.get_height()), \n            fontsize=12, color='grey', ha='center', va='bottom')\n    \nplt.show()\n","2169fe36":"# Build LDA model\nlda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                       id2word=id2word,\n                                       num_topics=10, \n                                       random_state=100,\n                                       chunksize=100,\n                                       passes=10,\n                                       per_word_topics=True)","d3bc479e":"# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","293c5a93":"# Evaluate base model- Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","40a84e95":"# supporting function\ndef compute_coherence_values(corpus, dictionary, k):\n    \n    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=10, \n                                           random_state=100,\n                                           chunksize=100,\n                                           passes=10,\n                                           per_word_topics=True)\n    \n    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n    \n    return coherence_model_lda.get_coherence()","6544d45e":"# Iterate over possible number of topics. takes about 2-3mins\ntopic_param = [0,5,10,15,20,25,30,35,40]\ncoherence_score = []\n\nfor k in topic_param:\n    cv = compute_coherence_values(corpus=corpus, dictionary=id2word, k=k)\n    coherence_score.append(cv)\n\nparam_tuning_coherence = pd.DataFrame()\nparam_tuning_coherence[\"topic_param\"] = topic_param\nparam_tuning_coherence['coherence_score'] = coherence_score\nparam_tuning_coherence","338d0124":"# Show graph\nplt.figure(figsize=(16, 8))\nplt.plot(topic_param, param_tuning_coherence['coherence_score'])\n\nplt.title(\"Choosing Optimal LDA Model\")\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence Scores\")\nplt.show()\n\n","d61224b1":"lda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=15, \n                                           random_state=100,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha=0.01,\n                                           eta=0.9)","aea0a515":"# Print the Keyword in the 15 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","ac177950":"def Sort_Tuple(tup):  \n    return(sorted(tup, key = lambda x: x[1], reverse = True))   ","c1f5d6e6":"doc_num, topic_num, prob = [], [], []\nprint(lda_model.get_document_topics(corpus))\nfor n in range(len(df)):\n    get_document_topics = lda_model.get_document_topics(corpus[n])\n    doc_num.append(n)\n    sorted_doc_topics = Sort_Tuple(get_document_topics)\n    topic_num.append(sorted_doc_topics[0][0])\n    prob.append(sorted_doc_topics[0][1])\ndf['Doc'] = doc_num\ndf['Topic'] = topic_num\ndf['Probability'] = prob\ndf.to_csv(\"doc_topic_matrix.csv\", index=False)","cc079f62":"pyLDAvis.enable_notebook()\nLDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, sort_topics=False)\nLDAvis_prepared","61cef5d4":"def recommend_by_storyline(title, df):\n    recommended = []\n    top10_list = []\n    \n    title = title.lower()\n    df['title'] = df['title'].str.lower()\n    topic_num = df[df['title']==title].Topic.values\n    doc_num = df[df['title']==title].Doc.values    \n    \n    output_df = df[df['Topic']==topic_num[0]].sort_values('Probability', ascending=False).reset_index(drop=True)\n\n    index = output_df[output_df['Doc']==doc_num[0]].index[0]\n    \n    top10_list += list(output_df.iloc[index-5:index].index)\n    top10_list += list(output_df.iloc[index+1:index+6].index)\n    \n    output_df['title'] = output_df['title'].str.title()\n    \n    for each in top10_list:\n        recommended.append(output_df.iloc[each].title)\n        \n    return recommended\n","e2f64fd3":"recommend_by_storyline(\"Avengers: Infinity War\", df)","3bffb138":"recommend_by_storyline(\"Naruto Shippuden : Blood Prison\", df)","07f6cd07":"#### 1.2 Creating Document Vectors","ba08cb68":"#### 2.5 Visualize Topics","3cc4d10c":"#### 1.4 Top 10 recommended tv shows\/movies based on Cosine Similarity","3a160eaa":"### 1. Cosine Similarity\n\nCosine similarity is computed based on the features - 'title', 'type', 'listed_in', 'director', 'cast', 'rating' and 'description'.","52e60a90":"#### 1.3 Computing Cosine Similarity between each document","54d218d4":"# Building a simple Content-Based Recommender System for Netflix Movies and TV Shows\n\n### Objective\nWith over 6000 movie and tv shows in a sample dataset, there is an overwhelming number of entertainment options available to Netflix users. As such, the purpose of this project aims to make a TV show \/ movie recommendation using a simple content based recommender system. We would take in an input which is a user's personal favourite show\/movies and pick up the top 10 films that are most similar to the personal favourite. Here, we explore 2 possible ways to identify similar items: (1) a simple similarity measure - Cosine Similarity (2) Clustering Algorithm - Latent Dirichlet Allocation (LDA).\n\n### Basics on Recommender System\nRecommender Systems can be generally divided into 2 categories: Collaborative Filtering System and Content-based Recommender System. A Collaborative Filtering recommends an item that other users of similar characteristics have liked in the past. A content-based recommender system recommends an item which are similar to the ones the user has liked in the past. Since the given dataset only contains item data, we would focus on creating a basic content-based recommender system. \n\n### Basics on Text Similarity\n\nThere are various text similarity metrics and one of the popular metrics is Cosine Similarity. \nConsine Similarity measures the similarity between 2 documents by measuring the consine of angle between two vectors. Here's a simple example to illustrate the calculation of cosine similarity:\n\n\n\n![image.png](attachment:image.png)\n\n### Basics on Topic Modelling\nTopic Modelling is an unsupervised learning technique which groups documents based on the content similarity. One popular algorithm is Latent Dirichlet Allocation (LDA). In LDA, each topic is a probability distribution of words and each document is a probability distribution of topics. The more similar the documents are, the closer they are to each other in the multi-dimensional vector space, thus forming clusters. \n\n\n\n\n### References\nhttps:\/\/www.cse.iitk.ac.in\/users\/nsrivast\/HCC\/Recommender_systems_handbook.pdf\n\nhttps:\/\/towardsdatascience.com\/how-to-build-from-scratch-a-content-based-movie-recommender-with-natural-language-processing-25ad400eb243\n","dae12f62":"#### 1.1 Data Pre-Processing","19d81432":"#### 2.4 Final LDA Model","d1a1e395":"#### 2.3 Hyperparameter Tuning for LDA Model","deb48b7a":"### 2. Topic Modelling with LDA\n\nI have chosen to explore the use of LDA on the 'description' textual data to detect similar documents because I speculate that there might be previously unknown underlying topics in the movie\/tv shows' storyline which differs from the typical genre classification such as Adventure, Romance.  ","f3280cb0":"#### 2.1 Data Pre-Processing","f06c830b":"#### 2.6  Top 10 recommended tv shows\/movies based on topic modelling","49747b4b":"#### 2.2 EDA"}}