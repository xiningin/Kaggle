{"cell_type":{"d6f94a2b":"code","e5f91f14":"code","2a170116":"code","1f791a61":"code","a923ca49":"code","279ca0da":"code","9472e04c":"code","1d54e154":"code","e926558a":"code","5f6320ac":"code","c8a046e7":"code","7f55516d":"code","d096a15f":"code","3e7b5ae4":"code","09132749":"code","be85a19f":"code","fff9e4bc":"code","567ff7fd":"code","ebd844d7":"code","56694c95":"code","b97e63a7":"code","38ae14c9":"code","99a781e4":"code","378c3aed":"code","4ed9be31":"code","e1b15b5c":"code","f241fb1b":"code","2e426b78":"code","d96bb7b3":"code","628318ca":"code","0b7c01fe":"code","0071b4be":"code","320cf5e6":"code","465ee379":"code","d787e951":"code","04efa966":"code","9c9c271d":"code","4ad997e2":"code","cb358792":"code","3526ddc9":"code","63a86a7c":"code","71c74855":"code","9ca65b8a":"code","422df524":"code","454eed02":"code","cc17d1d2":"code","efbfc4c8":"code","c7e31f9f":"code","750581f3":"code","ce351e66":"code","8ef9b928":"code","852a5e6b":"code","78e38f5b":"code","5795c1a8":"code","af045aa4":"code","45372ea4":"code","061d8883":"code","32636c60":"code","00849f4f":"code","f9714063":"code","31669dd7":"code","3c42b6f8":"code","e6e47c0a":"code","0be3537a":"code","984b36a5":"code","e534968f":"code","97fcd25c":"code","1059d1c0":"code","5a81d31a":"code","bed50e78":"markdown","6e34389a":"markdown","7d8ab7a7":"markdown","654a80c6":"markdown","c9aed6e4":"markdown","11cc47d7":"markdown","c19d9fa2":"markdown","b7afe9ae":"markdown","625af38e":"markdown","e6fe4035":"markdown","7c90e7d4":"markdown","52e851ef":"markdown","65f4b5ab":"markdown","198ae135":"markdown","10056230":"markdown","bff67b59":"markdown","ffea3de5":"markdown","9ab73192":"markdown","5f032058":"markdown","5bafaf07":"markdown","69b0f068":"markdown","22b55481":"markdown","37ecf836":"markdown","bcc9ae4f":"markdown","45aee938":"markdown","1c10a732":"markdown","4c530025":"markdown","1f828aa9":"markdown","baaa6e12":"markdown","2e0c5a96":"markdown","f16417db":"markdown","e1c0c8c6":"markdown","7c6877ca":"markdown","90bda6fc":"markdown","0bda9864":"markdown","c7cb7443":"markdown","fe235398":"markdown","418edf66":"markdown","5160cd24":"markdown","f64a2c58":"markdown","cde5e42c":"markdown","438d371f":"markdown","f1e8399c":"markdown","5afcbeae":"markdown","569afd52":"markdown","1fc34dc4":"markdown","53ffc9fc":"markdown","5ab726f2":"markdown","76fce0a7":"markdown","fd12b000":"markdown","543a1b36":"markdown","5a0172f3":"markdown","e994e39e":"markdown","9f8af8d9":"markdown","2f78d839":"markdown","d4df889c":"markdown","4bed2de2":"markdown","c75f3c72":"markdown","6df30c37":"markdown","880251ac":"markdown","e51188e2":"markdown","7757582b":"markdown","cd8e169c":"markdown","84919197":"markdown","23958a2d":"markdown","0ebb97cf":"markdown","846bd76c":"markdown","f1dd7bde":"markdown","1f635f99":"markdown","97647746":"markdown","58ec1861":"markdown","895357c9":"markdown"},"source":{"d6f94a2b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","e5f91f14":"color = sns.color_palette()\nsns.set_style('darkgrid')","2a170116":"path = '\/kaggle\/input\/house-prices-advanced-regression-techniques'\npath = Path(path)\ntrain_raw = pd.read_csv(path\/'train.csv')\ntest_raw = pd.read_csv(path\/'test.csv')\n\ntrain = train_raw.copy(deep=True)\ntest = test_raw.copy(deep=True)\ndata_clean = [train_raw,test_raw]","1f791a61":"print(\"Dataset dimensions: \")\nprint(\"Training Set: \" ,train.shape)\nprint(\"Test Set: \" ,test.shape)","a923ca49":"train.head(n=10)","279ca0da":"test.head(n=10)","9472e04c":"print(\"Variables with missing values in the dataset:\")\nprint(\"Training set: \" ,train.isnull().any().sum())\nprint(\"Test set: \", test.isnull().any().sum())","1d54e154":"y_train = train['SalePrice']\nx_train = train.drop('SalePrice',axis = 1)\n\ndata = pd.concat([x_train,test],ignore_index= True, verify_integrity = True,copy = True)\nprint(data.shape)","e926558a":"train.plot.scatter(x = 'LotArea',y = 'SalePrice')\ntrain.plot.scatter(x = 'GrLivArea',y = 'SalePrice')","5f6320ac":"idx_outliers =train[['GrLivArea','SalePrice']][(train['GrLivArea']>4000) & (train['SalePrice']<300000)]","c8a046e7":"pd.options.display.max_rows = 50\nprice_corr = train[train.notnull()].corr(method='pearson')['SalePrice'].abs()\nprice_corr = pd.DataFrame(price_corr)\nprice_corr.sort_values(by = 'SalePrice',ascending = False)","7f55516d":"mv = data.isnull().sum()\/data.shape[0]*100\nmv = mv[mv>0]\nmv = mv.sort_values(axis = 0,ascending = False)","d096a15f":"f, ax = plt.subplots(figsize=(15, 10))\nplt.xticks(rotation='90')\nsns.barplot(mv.index,mv)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nplt.show()","3e7b5ae4":"garage = ['GarageArea', 'GarageCars', 'GarageCond', 'GarageFinish', 'GarageQual', 'GarageType', 'GarageYrBlt']","09132749":"idx = data['GarageType'].isnull()\nprint(\"Total properties without a garage: \",idx.sum())\nprint(\"Non-NaN entries in properties without a garage: \")\nprint(data[garage][idx].notnull().sum())","be85a19f":"idx = data['GarageType'].isnull() & data['GarageArea'].notnull() & data['GarageCars'].notnull()\nprint(\"Total Garage Area and Cards in properties without a garage:\")\nprint(data[['GarageCars','GarageArea']][idx].sum())","fff9e4bc":"idx = data['GarageArea'].isnull()\ndata[['GarageArea','GarageCars','GarageQual']][idx]","567ff7fd":"bsmt = ['BsmtCond', 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFinType1', 'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtQual', 'BsmtUnfSF','TotalBsmtSF']\nidx = data['BsmtQual'].isnull()\ndata[bsmt][idx].notnull().sum()","ebd844d7":"idx = data['BsmtQual'].isnull() & data['BsmtCond'].notnull()\ndata[bsmt][idx]","56694c95":"idx = data['BsmtQual'].isnull() & data['BsmtFinSF1'].notnull()\ndata[bsmt][idx]","b97e63a7":"print(data[['BsmtFinSF1','BsmtFinSF2']][idx].sum())","38ae14c9":"idx = data['BsmtFinSF2'].isnull()\ndata[['BsmtCond','BsmtFinSF1','BsmtFinSF2']][idx]","99a781e4":"idx = data['BsmtQual'].isnull() & data['BsmtFullBath'].notnull()\ndata[['BsmtFullBath','BsmtHalfBath']][idx].sum()","378c3aed":"idx = data['BsmtQual'].isnull() & data['BsmtUnfSF'].notnull()\ndata[['BsmtUnfSF','TotalBsmtSF']][idx].sum()","4ed9be31":"idx = data['BsmtQual'].isnull() & data['BsmtUnfSF']>0\ndata[['BsmtUnfSF','TotalBsmtSF']][idx]","e1b15b5c":"pd.options.display.max_rows = 15\nidx = data['BsmtExposure'].isnull() & data['BsmtCond'].notnull()\ndata[bsmt][idx]","f241fb1b":"idx = data['BsmtCond'].notnull() & data['BsmtFinType1'].isnull()\ndata[bsmt][idx]","2e426b78":"idx = data['BsmtCond'].notnull() & data['BsmtFinType2'].isnull()\ndata[bsmt][idx]","d96bb7b3":"idx = data['BsmtCond'].notnull() & data['BsmtFullBath'].isnull()\nprint(data[bsmt][idx])\nidx = data['BsmtCond'].notnull() & data['BsmtHalfBath'].isnull()\nprint(data[bsmt][idx])","628318ca":"idx = data['BsmtCond'].notnull() & data['BsmtUnfSF'].isnull()\ndata[bsmt][idx]","0b7c01fe":"idx = data['BsmtCond'].notnull() & data['TotalBsmtSF'].isnull()\ndata[bsmt][idx]","0071b4be":"dict_ext = {'Brk Cmn':'BrkComm',\n            'CmentBd':'CemntBd',\n            'Wd Shng': 'WdShing'\n           }\ndata['Exterior2nd'] = data['Exterior2nd'].replace(dict_ext)","320cf5e6":"v = ['Exterior1st','Exterior2nd']\nidx = data['Exterior1st']!=data['Exterior2nd']\ndata[v][idx]","465ee379":"pd.options.display.max_columns = 100\n\nidx = data['Exterior1st'].isnull()\ndata[idx]","d787e951":"idx = data['MasVnrType'].isnull()\ndata[['MasVnrArea','MasVnrType']][idx]","04efa966":"data[data['MSZoning'].isnull()]","9c9c271d":"data[data['SaleType'].isnull()]","4ad997e2":"train = train.drop(idx_outliers.index, axis = 0)","cb358792":"train.plot.scatter(x = 'GrLivArea',y = 'SalePrice')","3526ddc9":"y_train = train['SalePrice']\nx_train = train.drop('SalePrice',axis = 1)\n\ndel data\ndata = pd.concat([x_train,test],ignore_index= True, verify_integrity = True,copy=True)","63a86a7c":"dict_ext = {'Brk Cmn':'BrkComm',\n            'CmentBd':'CemntBd',\n            'Wd Shng': 'WdShing'\n           }\ndata['Exterior2nd'] = data['Exterior2nd'].replace(dict_ext)","71c74855":"def set_value(df,value, variables):\n    assert type(variables)==list,\"variables must be passed on as list\"\n    var0,var1 = variables\n    idx = df[var0].notnull() & data[var1].isnull()\n    loc = df[var1][idx].index[0]\n    df.at[loc,var1] = value\n    return df\n\ndef findStringMostCommon(d,target,conds, tvals=None):\n    assert type(conds) == list, \"Targetvars must be passed on as a list\"\n    assert len(conds)<3\n    if tvals:\n        if len(conds)>1:\n            cond0,cond1 = conds\n            tval0,tval1 = tvals    \n            selected_data = d[target][(d[cond0]==tval0) & (d[cond1]==tval1)].sort_values()\n        else:\n            selected_data = d[target][d[conds]==tvals].sort_values()\n    else:\n        conds = conds[0]\n        selected_data = d[target].groupby(conds).value_counts()\n    \n    return selected_data.value_counts().index[0]\n\ndef set_conditional(df,target,cond):\n    idx = df[target].isnull()\n    for i in idx.index:\n        if idx[i]:\n            stats = df[target][df[cond]==df.loc[i,cond]].value_counts()\n            df.at[i,target] = stats.index[0]\n    return df","9ca65b8a":"def fill_and_fix(df):\n    df_cp = df.copy(deep= True)\n    vars_cat = ['Alley','BsmtCond','BsmtFinType1','BsmtFinType2','Fence','FireplaceQu','GarageCond','GarageFinish','GarageQual','GarageType','MiscFeature','PoolQC','BsmtQual']\n    \n    vars_num = ['BsmtFinSF1','BsmtFinSF2','BsmtFullBath','BsmtHalfBath','BsmtUnfSF','GarageArea','GarageCars','LotFrontage', 'TotalBsmtSF','MasVnrArea','GarageYrBlt']\n    df_cp = set_value(df_cp,'Unf',['BsmtQual','BsmtFinType2'])\n    df_cp = set_value(df_cp,findStringMostCommon(df_cp,'BsmtQual',['BsmtExposure','BsmtFinType1'],['No','Unf']),['BsmtCond','BsmtQual'])\n    \n    for v in ['Exterior1st','Exterior2nd','MSZoning','Utilities']:\n        df_cp = set_conditional(df_cp,v,'Neighborhood')\n        \n    for v in ['Electrical','KitchenQual']:\n        df_cp = set_conditional(df_cp,v,'MSZoning')\n     \n    idx = df_cp['MasVnrType'].isnull() & df_cp['MasVnrArea'].notnull()\n    df_cp.at[idx[idx==True].index[0],'MasVnrArea']= 0 \n    \n    for var in vars_cat:\n        df_cp[var].fillna(value = '0',inplace = True)\n        \n    for var in vars_num:\n        df_cp[var].fillna(value = 0,inplace = True)\n        \n    df_cp['BsmtExposure'].fillna(value = '0',inplace = True)\n    df_cp['Functional'].fillna(value = 'Typ',inplace = True) \n    df_cp['MasVnrType'].fillna(value = 'None',inplace = True)     \n    \n    return df_cp","422df524":"data  = fill_and_fix(data)\nprint(\"Missing values after cleaning: \",data.isnull().sum().sum())","454eed02":"data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']","cc17d1d2":"data['hasGarage'] =  np.where(data['GarageQual']!='0', 1, 0)\ndata['hasBsmt'] =  np.where(data['BsmtCond']!='0', 1, 0)\n\ndata['GarageAge'] = data['YrSold']-data['GarageYrBlt']\ndata['GarageAge'] = np.where(data['GarageAge']<0,100,0)\ndata['HouseAge'] = data['YrSold']-data['YearBuilt']\ndata['RemodAge'] = data['YrSold']-data['YearRemodAdd']\n\ndata_clean = data.drop(labels = ['SaleType','GarageYrBlt','YearBuilt','YearRemodAdd'],axis = 1)","efbfc4c8":"ordinal = ['Alley','BldgType','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','BsmtQual','CentralAir','Condition1','Condition2','Electrical','ExterCond','Exterior1st','Exterior2nd',\\\n           'ExterQual', 'Fence','FireplaceQu','Foundation','Functional','GarageCond','GarageFinish','GarageQual','GarageType','Heating','HeatingQC','HouseStyle','KitchenQual','LandContour',\\\n           'LandSlope','LotConfig','LotShape','MasVnrArea','MasVnrType','MiscFeature','MSSubClass','MSZoning','Neighborhood','OverallCond','OverallQual','PavedDrive','PoolQC','RoofMatl',\\\n           'RoofStyle','SaleCondition','Street','Utilities']","c7e31f9f":"def categorify(df, var, d = None):\n    df_cp = df.copy(deep=True)\n    codebook = d if d else dict()  \n    for v in var:\n        if v not in codebook.keys():\n            df_cp[v] = df_cp[v].astype('category')\n            keys = np.sort(df[v].unique())\n            if np.array_equal(keys,np.arange(len(keys))):\n                assert bool(codebook),\"No dictionary provided. Please provide a dictionary to avoid overwriting values\"\n            else:\n                df_cp[v] = df_cp[v].cat.reorder_categories(keys,ordered=True)\n                values = df_cp[v].cat.codes\n                df_cp[v] = values\n                codebook[v] = list(zip(keys,np.arange(len(keys))))\n    return df_cp,codebook\n","750581f3":"data,codebook = categorify(data_clean,ordinal)","ce351e66":"\nneed_norm = ['1stFlrSF','2ndFlrSF','3SsnPorch','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','EnclosedPorch','GarageArea','MasVnrArea','OpenPorchSF','PoolArea','ScreenPorch','TotalBsmtSF','WoodDeckSF','LotArea','LotFrontage','TotalSF']","8ef9b928":"def normalize(df,need_norm):\n    df = df.astype('float64')\n    for v in need_norm:\n        df[v] = (df[v]-df[v].mean())\/df[v].std()\n    return df\ndata_final = normalize(data,need_norm)\ndata_final","852a5e6b":"sns.distplot(y_train)","78e38f5b":"y_train_log = np.log(y_train)\nsns.distplot(y_train_log)","5795c1a8":"data_final = data\nm = train_raw.shape[0]-2\nx_train = data_final.loc[:(m-1),:]\nx_test = data_final.loc[m:,:]","af045aa4":"xtrain  = x_train.drop(labels = 'Id', axis = 1)\nxtest = x_test.drop(labels = 'Id',axis = 1)","45372ea4":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,Ridge\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","061d8883":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(xtrain)\n    rmse= np.sqrt(-cross_val_score(model, xtrain, y_train_log, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","32636c60":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","00849f4f":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","f9714063":"RR = Ridge(alpha=0.8)","31669dd7":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","3c42b6f8":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","e6e47c0a":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","0be3537a":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(RR)\nprint(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","984b36a5":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","e534968f":"averaged_models = AveragingModels(models = (ENet, GBoost, RR, lasso,model_xgb,model_lgb))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","97fcd25c":"%%capture\naveraged_models.fit(xtrain, y_train_log)","1059d1c0":"train_pred = np.exp(averaged_models.predict(xtrain))\ntest_pred = np.exp(averaged_models.predict(xtest))","5a81d31a":"submission = pd.DataFrame()\nsubmission['Id'] = x_test['Id'].astype('int32')\nsubmission['SalePrice'] = test_pred\nsubmission.to_csv('submission.csv',index=False)","bed50e78":"- **Ridge Regression** :\n\nNormal Ridge regression","6e34389a":"## 4.1 Stacking  models","7d8ab7a7":"# 3. Data Cleaning","654a80c6":"The idea of the model is to use 6 different regression techniques and to average over their result. The algorithms used are:\n* Lasso\n* ElasticNet\n* Ridge Regression\n* Gradient Boosting Regression\n* XGBoost\n* LightGBM","c9aed6e4":"**Averaged base models score**","11cc47d7":"# 2. Data Exploration","c19d9fa2":"The same applies for baths in the basement.","b7afe9ae":"Base Models:","625af38e":"Outliers in regression models can skew the performance significantly, since many advanced models are very sensitive to them. Therefore, we are looking for houses that were sold at a price lower than the expected value of the dataset. To achieve that, we plot the SalePrice of each house in the training set over the Lot Area and the above-ground living area.","e6fe4035":"In the one case where BsmtFinType2 is missing when a basement exists, we see that the basement has 1600 square feet of unfinished space. Therefore, BsmtFinType2 shall be set to unfinished.","7c90e7d4":"# 4. Training stacked regressors","52e851ef":"In this project we are going to try to predict the house prices in Ames, Iowa. The dataset consists of roughly 3000 houses, split equally in training and test set. Each house in the dataset is described by 80 variables that include any information that a house listing could possibly include. \n\nThe structure of this notebook is the following:\nIn the first section, the development environment is set up, the train and test sets are loaded and we are taking a quick look at the dataframes. <br\/>\n\nIn the second section, the datasets get explored.We are looking for possible outliers that will skew the models during training, and are investigating what is the best way to fill in each missing value.<br\/>\n\nSection 3 is handling data cleaning. This includes removing outliers from the data, filling in missing values and categorifying ordinal variables.<br\/>\n\nFinally, sections 4 and 5 revolve around developing a model, training it on the data and evaluating the results. The idea of using multiple regressors and averaging over their results comes from the following kernel: \n\n[Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard).","65f4b5ab":"Next we shall take a look at the variables BsmtFinSF1 and BsmtFinSF2.","198ae135":"Before we convert the data to datatypes fit for training, the missing values should be filled in according to the analysis presented above. At the same time, any \"wrong\" data will be fixed. The whole process will be encapsulated into a function to ease readability and enable code reproducability.","10056230":"This is weird and probably means that there may be something wrong with the data. Let's examine this further. First we consider the BsmtCond variable.","bff67b59":"Now that all missing values have been explored, we are in a position to start cleaning the dataset.","ffea3de5":"There is one property where GarageArea and GarageCars are not entered, and since there is no Garage, they shall be set to 0.","9ab73192":"Finally, we are normalizing the numerical variables in the dataset to ease the regression fitting process.","5f032058":"5 of the 7 variables have no values entered in the aforementioned categories, while GarageArea and GarageCars are entried for all those properties.","5bafaf07":"We use the **cross_val_score** function of Sklearn to create a 5-fold cross validation function that returns the logarithmic Root Mean Squared Error for each model.","69b0f068":"Moving to the Exterior 1st and 2nd variables that describe the exterior of the house, we would like to check whether the Exterior2nd variable is obsolete. Therefore, we would like to see in how many properties Exterior2nd differs from Exterior1st. However, Exterior2nd values have in some cases a different name than in Exterior1st for the same thing. This should be fixed. These values are:\n\n* Brk Cmn --> BrkComm\n* CmentBd --> CemntBd\n* Wd Shing  --> WdShing","22b55481":"# 1. Setting up environment","37ecf836":"- **XGBoost** :","bcc9ae4f":"## 3.5 Normalizing numerical values","45aee938":"## 3.3 Feature Engineering","1c10a732":"The missing MSZoning values can be will with the median MSZoning value grouped by neighborhood, as properties in the same neighborhood should fall into the same zoning classification.","4c530025":"## 2.3 Missing values","1f828aa9":"Since there is no basement here either, they also should be set to 0.","baaa6e12":"- **LightGBM** :","2e0c5a96":"The most important data cleaning procedure is to understand where missing values lie in the data and decide how to fill them.","f16417db":"## 3.1 Dropping outliers","e1c0c8c6":"By computing the correlation between the numerical features of the data to the sale price, we get a first rough idea of which numerical variables are the most relevant to the target.","7c6877ca":"## 2.1 Outliers","90bda6fc":"Splitting data back into train and test set and droppign 'Id' variable:","0bda9864":"-  **LASSO  Regression**  : \n\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's  **Robustscaler()**  method on pipeline.","c7cb7443":"### 2.3.3 Other variables","fe235398":"Looking at the dataset description, these 2 entries have an unfinished basement without exposure, while the basement condition is fair in one case and typical in the other.Fair and typical are defined as:\n\n* Typical - slight dampness allowed\n* Fair - dampness or some cracking or settling\n\nSince these 2 properties do have a basement, even though it is small and unfinished, the hasBsmt variable should be set to 1. Additionally, it seems that these 2 basements are actually more fit to be used as storage space rather than as a functional part of the house. Therefore, they shall be filled with the median quality of unfinished basements with no exposure.\n\n","418edf66":"Now the sale prices have a much clearer correlation and fitting a regression model on it will not be skewed by outliers. We can now recreate the 'data' dataframe and start cleaning the data.","5160cd24":"About ~9% of the properties have different exterior materials, which is enough to keep the variable. Moving on to see what is going on with the missing values.","f64a2c58":"Most variables in the dataset are categorical. In order to train a model, these should be converted to ordinal, meaning that the categories of each variable will be indicated through a number. The conversion procedure is saved in a dictionary, in case we would like to know the original category of each variable.","cde5e42c":"As we can see, the entries are just zeros. Nothing out of the ordinary here as well.","438d371f":"### 2.3.1 Garage-related\n\nThere are 7 variables in the dataset with missing values related to the garage. We shall examine them all together. According to the documentation, if GarageType is NaN,then the property does not contain any garage space. ","f1e8399c":"Some missing values may be dependent on each other. For example, GarageQual is logically missing if there is no garage and pool quality is also logically missing when the proberty does not have a pool. This explains why some features have such a high percentage of missing values. Therefore, we have to go through each missing value variable one at a time to create a strategy on filling the missing values in a meaningful way.","5afcbeae":"We see that those values were set to 0 even though no garage was mentioned, which makes sense. Therefore, no abnormality in the data is detected. ","569afd52":"The only missing value in the data is in the SaleType variable, which will later be dropped since it should make no difference on the house prices.","1fc34dc4":"- **Elastic Net Regression** :\n\nagain made robust to outliers","53ffc9fc":"## 3.4 Converting categorical to ordinal data","5ab726f2":"Before we continue, we are going to crease some features and revamp some others. We are adding 3 variables: \n\n* TotalSF: The living area of a property, including 1st floor, 2nd floor and basement.\n* hasGarage: Variable indicating whether the property has a garage\n* hasBsmt: Variable indicating whether the property has a basement\n\nWe are also changing variables that indicate the age of some features of the house.\n* GarageAge: Replaces GarageYrBlt and equals to YearSold-GarageYrBlt\n* HouseAge: Replaces YearBlt, equals to YearSold-YearBuilt\n* RemodAge: Replaces YearRemodADd, equals to YearSold-YearRemodAdd\n","76fce0a7":"MasVnrArea's missing values will be set to 0, while MasVnrType will be set to None. In the one case where a veneer area is entered without a veneer type, the area will be corrected to 0.","fd12b000":"These are the same 2 properties that we saw before. We can safely leave them as they are, as there is no abnormality here either.","543a1b36":"## 2.2 Correlation of variables to house price\n","5a0172f3":"There are also some variables where, according to the documentation, a missing value denotes the absence of the corresponding feature. For example, if Fence is NaN for a property, then the property has no fence. These aren't really missing values and we will deal with them later when we categorify ordinal variables. ","e994e39e":"Finally, we are going to take a look at the distribution of SalePrice","9f8af8d9":"# Results","2f78d839":"### 2.3.2 Basement-related","d4df889c":"Bath variables related to basements are only missing when there is no basement. Therefore, nothing special has to be done for them.","4bed2de2":"## 3.6 Fixing target and creating train, test set.\n","c75f3c72":"In the one property with missing exterior values, they will be set according to the most frequent exterior cover for this neighborhood.","6df30c37":"Finally, the missing sale type value should be irrelevant to the final price of each property and will be dropped.","880251ac":"BsmtUnfSF is also only missing when there is no basement, therefore its missing values are set to 0.","e51188e2":"Since no basement has a missing TotalBsmtSF value, it is only missing when no basement is present. Therefore, the missing values will be set to 0.","7757582b":"We begin with this simple approach of averaging base models.  We build a new **class**  to extend scikit-learn with our model and also to laverage encapsulation and code reuse ([inheritance][1]) \n\n\n  [1]: https:\/\/en.wikipedia.org\/wiki\/Inheritance_(object-oriented_programming)","cd8e169c":"To avoid  any error, we will first drop the outliers from the training set and then re-initialize the 'data' dataframe to fill in the missing values and convert to ordinal data.","84919197":"The second major category of missing values are related to basements. We will follow the same procedure as above to examine them. We will also later create a variable called hasBsmt to denote whether the building has a basement or not. This information can be extracted from the BsmtQual variable.","23958a2d":"In the first plot, it cannot be concluded whether the points on the right part should be considered outliers. A bigger Lot Area is usually found in agricultural areas where the prices are lower than in urban areas. However, the 2 points on the lower right part of the second figure are definitely outliers, sold at a lower price than they should have been, and will be removed from the dataset.","0ebb97cf":"The prices resemble a left-skewed normal distribution. Since regression models work better with normally distributed data, we are going to apply a logarithmic function to them to normalize them.","846bd76c":"For these properties, BsmtExposure is set to indicate that no basement is present, even though there is a basement that is just unfinished. We are going to assume that the basement has no exposure. ","f1dd7bde":"## 3.2 Filling in missing values and fix wrong entries","1f635f99":"We shall now save the house prices from the training set and unify train and test set to explore and fill in missing values.","97647746":"Model performance on the data by evaluating the  cross-validation rmsle error[](http:\/\/)","58ec1861":"The documentation instructs to assume typical functionality unless deductions are warranted. Therefore, the missing values in Functional will be filled in with Typ. As for the Electrical and KitchenQual variables, they will be filled with the median of the corresponding house zoning (found in MSZoning). Finally, Utilities will be filled in with the median according to Functional. When LotFrontage is missing, it is assumed that no lot frontage is present and therefore is set to 0.","895357c9":"- **Gradient Boosting Regression** :\n\nWith **huber**  loss that makes it robust to outliers\n    "}}