{"cell_type":{"682bdee0":"code","4de20fe6":"code","d5ac059a":"code","a4159ce5":"code","f598286d":"code","b6804eff":"code","214e7b1d":"code","c0837f60":"code","76ff1668":"code","3c0801e1":"code","3e6affa3":"code","d52ad939":"code","cedd02d0":"code","4183997d":"code","c48be909":"code","cd15a7e0":"code","06c6499d":"code","50346a0c":"code","783fbad8":"code","41147198":"code","b7b37f2e":"code","9a31e9dd":"code","c3774075":"code","0266c560":"code","3a93133e":"markdown","9b34aecf":"markdown","41f954e0":"markdown","c59e2c99":"markdown","5b52bb7b":"markdown","8c45331c":"markdown","e984b5bf":"markdown","a8badd60":"markdown","f8f7115a":"markdown","fc66f1fe":"markdown","13f3aded":"markdown","bd914143":"markdown","542040e1":"markdown","1585f361":"markdown","c0afd619":"markdown","9102c3fe":"markdown","7b93a364":"markdown","e4429e33":"markdown","db8a7347":"markdown","d94aa147":"markdown","952c5905":"markdown","31cd0317":"markdown"},"source":{"682bdee0":"# This Python 3 environment comes with many helpful analytics libraries installed\nimport re\nimport regex\nimport pandas as pd\nimport numpy as np\nimport emoji\nimport plotly.express as px\nfrom collections import Counter\nimport nltk\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n%matplotlib inline","4de20fe6":"# Read txt data file\n\ndata = pd.read_table('..\/input\/whatsapp-chat-momtxt\/whatsapp_chat_mom.txt', names=['text'], header=None)\n\n#get rid of top 2 rows (e.g. \"this chat is encrypted\" etc.)\ndata = data.iloc[3:]\n\n# parse the data into the columns 'date', 'person', and 'message'\ndata[['DateTime','rest']] = data['text'].str.split(']', expand=True, n=1)\ndata[['person','message']] = data['rest'].str.split(':',expand=True, n=1)\n\ndata.drop(['text','rest'], axis=1, inplace=True)","d5ac059a":"data.head()","a4159ce5":"data.info()","f598286d":"# get rid of all the \"image omitted\"\ndata = data[~data[\"message\"].str.contains(\"omitted\", na=False)]\n\n# sort of the date into a proper date column\ndata['DateTime'] = data['DateTime'].map(lambda x: x.replace(',',''))\ndata['DateTime'] = data['DateTime'].map(lambda x: x.strip('['))\ndata['DateTime'] = pd.to_datetime(data['DateTime'], format=\"%d\/%m\/%Y %I:%M:%S %p\", errors='coerce')\n\n# remove null rows \nprint('Number of rows before dropping null: {}'.format(data.shape[0]))\ndata = data[data['DateTime'].notnull()]\nprint('Number of rows after dropping null: {}'.format(data.shape[0]))","b6804eff":"#Create 4 new columns to sort datetime\n#weekday\ndata['weekday'] = data['DateTime'].apply(lambda x: x.day_name())\n\n#month sent\ndata['month_sent'] = data['DateTime'].apply(lambda x: x.month_name())\n\n#date\ndata['date'] = [d.date() for d in data['DateTime']]\n\n#hour\ndata['hour'] = [d.time().hour for d in data['DateTime']]","214e7b1d":"#column urlcount\nURLPATTERN = r'(https?:\/\/\\S+)'\ndata['urlcount'] = data.message.apply(lambda x: re.findall(URLPATTERN, x)).str.len()\n\n#column Letter_Count\ndata['Letter_Count'] = data['message'].apply(lambda s : len(s))\n\n#column Word_Count\ndata['Word_Count'] = data['message'].apply(lambda s : len(s.split(' ')))\n\n#create emoji column\ndef split_count(text):\n    emoji_list = []\n    data = regex.findall(r'\\X', text)\n    for word in data:\n        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n            emoji_list.append(word)  \n    return emoji_list\n\ndata[\"emoji\"] = data[\"message\"].apply(lambda x: split_count(x))","c0837f60":"data.head()","76ff1668":"date_grouped = data.groupby('date')['message'].count().plot(kind='line', figsize=(20,10), color='#A26360')","3c0801e1":"weekday_grouped_msg =  (data.set_index('weekday')['message']\n                          .groupby(level=0)\n                          .value_counts()\n                          .groupby(level=0)\n                          .sum()\n                          .reset_index(name='count'))\nweekday_grouped_msg\n\nfig = px.line_polar(weekday_grouped_msg, r='count', theta='weekday', line_close=True)\nfig.update_traces(fill='toself')\nfig.update_layout(\n  polar=dict(\n    radialaxis=dict(\n      visible=True,\n    )),\n  showlegend=False\n)\nfig.show()","3e6affa3":"hour_grouped_msg =  (data.set_index('hour')['message']\n                          .groupby(level=0)\n                          .value_counts()\n                          .groupby(level=0)\n                          .sum()\n                          .reset_index(name='count'))\nfig = px.bar(hour_grouped_msg, x='hour', y='count',\n                 labels={'hour':'24 Hour Period'}, \n                 height=400)\nfig.update_traces(marker_color='#EDCC8B', marker_line_color='#D4A29C',\n                  marker_line_width=1.5, opacity=0.6)\nfig.update_layout(title_text='Total Messages by Hour of the Day')\nfig.show()","d52ad939":"#which day of the week of each month had the greatest number of messages sent\ngrouped_by_month_and_day = data.groupby(['month_sent','weekday'])['message'].value_counts().reset_index(name='count')\n\ngrouped_by_month_and_day","cedd02d0":"months= ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\ndays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\npt = grouped_by_month_and_day.pivot_table(index= 'month_sent', columns= 'weekday', values='count').reindex(index=months, columns= days)\npt","4183997d":"fig = px.imshow(pt,\n                labels=dict(x=\"Day of Week\", y=\"Months\", color=\"Count\"),\n                x=days,\n                y=months\n               )\nfig.update_layout(\n    width = 700, height = 700)\nfig.show()","c48be909":"total_messages = data.shape[0]\nmedia_messages = data[data['message'] == '<Multimedia omitido>'].shape[0]\naverage_message_words = data['Word_Count'].mean()\naverage_message_letters = data['Letter_Count'].mean()\naverage_message_day = data.groupby('date')['message'].count().mean()\nprint('Total Messages ',total_messages)\nprint('Media Message', media_messages)\nprint('Average Words by Messages', round(average_message_words, 2))\nprint('Average Letters by Messages', round(average_message_letters, 2))\nprint('Average Message Per Day', round(average_message_day, 2))","cd15a7e0":"#coefficient of variation\na = data.groupby('date')['message'].count()\nCV = np.std(a)\/a.mean()\nprint('CV of message frequency is : {}'.format(CV))\n\nif CV >= 1:\n    print('Avg message per day has large variance')\nelse:\n    print('Avg message per day has small variance')","06c6499d":"qty_message_author = data['person'].value_counts()\nqty_message_author.plot(kind='barh',figsize=(20,10), color=['#D4A29C', '#E8B298', '#EDCC8B', '#BDD1C5', '#9DAAA2'])\nqty_message_author","50346a0c":"commond_words = data[['person','message']].copy()\n\nfrom nltk.corpus import stopwords\nSTOPWORDS = stopwords.words('english')\n\nstopwords = list(STOPWORDS)\nextra = [\"<multimedia\", \"omitido>\", \"k\", \"d\",\"si\",\"multimedia\", \"omitido\"]\nstopwords = stopwords + extra\ncommond_words[\"message\"] = (commond_words[\"message\"]\n                           .str.lower()\n                           .str.split()\n                           .apply(lambda x: [item for item in x if item not in stopwords])\n                           .explode()\n                           .reset_index(drop=True)\n                 )\n\n#commond_words['message']= commond_words['message'].apply(remove_emoji)\ncommond_words['message']= commond_words['message'].replace('nan', np.NaN)\ncommond_words['message']= commond_words['message'].replace('', np.NaN)\ncommond_words['message']= commond_words.message.str.replace(r\"(a|h)?(ha)+(a|h)?\", \"haha\")\ncommond_words['message']= commond_words.message.str.replace(r\"(a|h)?(haha)+(a|h)?\", \"haha\")\n\n\nwords_dict = dict(Counter(commond_words.message))\nwords_dict = sorted(words_dict.items(), key=lambda x: x[1], reverse=True)\n\nwords_dict = pd.DataFrame(words_dict, columns=['words', 'count'])\n\nfig = px.bar(words_dict.head(10).dropna(), x='words', y='count',\n                 labels={'words':'Common Words'}, \n                 height=400)\nfig.update_traces(marker_color='#EDCC8B', marker_line_color='#D4A29C',\n                  marker_line_width=1.5, opacity=0.6)\nfig.update_layout(title_text='Commond Words Chart')\nfig.show()","783fbad8":"TopTen =5\nauthor_commond_words =  (commond_words.set_index('person')['message']\n                          .dropna()\n                          .groupby(level=0)\n                          .value_counts()\n                          .groupby(level=0)\n                          .head(TopTen)\n                          .rename_axis(('person','words'))\n                          .reset_index(name='count'))\n\nl = author_commond_words.person.unique()\nfor i in range(len(l)):\n    dummy_df = author_commond_words[author_commond_words['person'] == l[i]]\n    print(dummy_df)\n    print('Most Commond Words by', l[i])\n    fig = px.bar(dummy_df, x='words', y='count',\n                 labels={'words':'By Person Common Words'}, \n                 height=380)\n    fig.update_traces(marker_color='#EDCC8B', marker_line_color='#D4A29C',\n                  marker_line_width=1.5, opacity=0.6)\n    fig.update_layout(title_text='By Person Commond Words Chart')\n    fig.show()","41147198":"#function to display wordcloud\ndef plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(40, 30))\n    # Display image\n    plt.imshow(wordcloud) \n    # No axis details\n    plt.axis(\"off\");\n\n#function to remove urls from text\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)","b7b37f2e":"chat_word_cloud = data[['message']].copy()\n#chat_word_cloud['message']= chat_word_cloud['message'].apply(remove_emoji)\nchat_word_cloud['message']= chat_word_cloud['message'].apply(remove_urls)\nchat_word_cloud['message']= chat_word_cloud['message'].replace('nan', np.NaN)\nchat_word_cloud['message']= chat_word_cloud['message'].replace('', np.NaN)\nchat_word_cloud['message']= chat_word_cloud.message.str.replace(r\"(a|j)?(ja)+(a|j)?\", \"jaja\")\nchat_word_cloud['message']= chat_word_cloud.message.str.replace(r\"(a|j)?(jaja)+(a|j)?\", \"jaja\")\ntext = \" \".join(review for review in chat_word_cloud.message.dropna())\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, \nbackground_color='black', colormap='Set2', collocations=False,\nstopwords = stopwords).generate(text)\n# Plot\nplot_cloud(wordcloud)","9a31e9dd":"total_emojis_list = list(set([a for b in data['emoji'] for a in b]))\ntotal_emojis = len(total_emojis_list)\nprint('Sum of all used Emojis', total_emojis)","c3774075":"total_emojis_list = list([a for b in data['emoji'] for a in b])\nemoji_dict = dict(Counter(total_emojis_list))\nemoji_dict = sorted(emoji_dict.items(), key=lambda x: x[1], reverse=True)\nemoji_df = pd.DataFrame(emoji_dict, columns=['emoji', 'count'])\nemoji_df.head(10)","0266c560":"fig = px.treemap(emoji_df, path= ['emoji'],\n    values = emoji_df['count'].tolist(),\n)\nfig.show()","3a93133e":"# Potential Next Step\nLet's do sentiment analysis on the messages too!","9b34aecf":"We can have more information on behavior of authors, analyzing data such as\nNumber of messages sent per person","41f954e0":"Let's take a look at our dataset!","c59e2c99":"The most common word by person?","5b52bb7b":"A new DataFrame shows the top ten emojis ordered from highest to lowest based on the number of repetitions.","8c45331c":"# I'm bored so let's look at more numbers.\n\nThe DataFrame has 3294 rows, 0 of them are multimedia messages, average of words per message is 7.21, average of letters per message is 31.66 and average of daily messages is 137.25.","e984b5bf":"# WordCloud\nWordCloud is a cool visualization to see all \"chat\" words. We will use the WordCloud library for this.\nUsing a couple of new functions to plot our chart and to eliminate links to web sites that could be found in messages.","a8badd60":"What time of day is it most common to send messages in this chat?","f8f7115a":"# Data Analysis\nWe would like to create a series of data visualisation to understand the data better.\n\nThis chart shows the number of messages per day:","fc66f1fe":"# Emojis\nWhat data can we get from emojis in group chat?\n\n*A sum of all different used emojis.*","13f3aded":"Using another pandas method called info(), we can have a lot of information about columns of DataFrames, such as format, number of nan and the count of records per column, in this way we found that format of DateTime column is string, we will transform this column to a suitable date and time format to be able to analyze these aspects.","bd914143":"# **Importing Libraries**\nThe following analysis uses a series of tools to develop and easy mechanism to generate charts and graphics. \n\nThe imported libraries where:","542040e1":"# Load DataFrame from txt file\nParse and read the chat messages into 3 columns (date, person, message).","1585f361":"If want to know which day of the week of each month had the greatest number of messages sent, to obtain these data\n1. Group data by month and day of the week, in addition to counting the messages sent\n2. Make a pivot_table with DataFrame obtained from in the previous step, having as columns\ndays of the week and as rows the months of the year and the count carried out as the values to evaluate.\n3. Using Plotly, perform a HeatMap with the ImShow function","c0afd619":"# Are these numbers reliable?\nWe can find this out by checking the coefficient of variation (CV).\nCV is defined by standard deviation divided by mean\n* If CV >1, the sample data has a relatively large variance\n* If CV <1, the sample data has a relatively small variance","9102c3fe":"**To find out the most used words in chat, following this steps**\n\n1. Using nltk stopwords, to remove some words from our data\n2. Generate a new DataFrame copying chat DataFrame selecting author and message columns\n3. Separate each word of each message to make a row with each of them\n4. Use \"remove_emoji\" function, to not consider emojis as common words\n5. Remove empty or NaN rows\n6. Unifying every laughing text just in a \"haha\"\n8. Grouping most common words then count their repetitions\n9. Using plotly make a nice chart with top 10 common words","7b93a364":"Let's find out which weekday has the highest message frequency!","e4429e33":"# **Data preparation**\n\nThe first step of this process is to obtain the conversation on TXT format. The easiest way to export an entire chat history, not including video or photos from your cell phone, is to use the built-in \u201cExport Chat\u201d feature, following these steps.\n1. Open an individual or a group chat.\n2. Tap the Menu button shown in the image below.\n3. Tap the More button.\n4. Select Export Chat.\n5. Tap Without Media from the options that are given.\n6. Select an option to share TXT file.","db8a7347":"Let's see what the cleaned dataframe looks like now","d94aa147":"# **Data Cleaning**\n1. Remove all the \"image omitted\"\n2. Remove all rows with null values\n3. Convert column DateTime into datatime object\n4. With DateTime column, add 4 new columns (weekday, month_sent, date, hour)\n5. With Messages column, add 4 new columns (emojis, urlcount, Letter_Count & Word_Count)","952c5905":"# Conclusion\nI got very bored and decide to analyse whatsapp chat history...\n* What are the highest chat frequency weekday?\n* What is the time trend of texting?\n* What are the most commonly used words?\nAnd so on...\nTurns out it was a very fun analytics!\n","31cd0317":"A *Plotly TreeMap chart* shows variation in the amounts of use of each emoji."}}