{"cell_type":{"4152082d":"code","5f84d93e":"code","20fb451e":"code","f7f4a3b6":"code","6960f19e":"code","ff4f0c56":"code","08b5279b":"code","0cb0c7a8":"code","e720538e":"code","19d2b94e":"code","769fce38":"code","1b8fb47a":"code","e920d5e5":"code","99221a22":"code","f72aeb69":"markdown","7ecc4d84":"markdown","824142c5":"markdown","a7be3cfe":"markdown","4d7c51bb":"markdown","2401c84c":"markdown","6427317d":"markdown","2c80d964":"markdown","25c8e424":"markdown","49b77062":"markdown","0af963dd":"markdown"},"source":{"4152082d":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport folium\nimport IPython\nimport matplotlib.pyplot as plt","5f84d93e":"print('\\n'.join(os.listdir('\/kaggle\/input\/birdsong-recognition')))\npath = '\/kaggle\/input\/birdsong-recognition'\nTRAIN_PATH = os.path.join(path, 'train_audio')\nTEST_PATH = os.path.join(path, 'example_test_audio')","20fb451e":"train_meta = pd.read_csv(os.path.join(path, 'train.csv'))\ntest_meta = pd.read_csv(os.path.join(path, 'test.csv'))","f7f4a3b6":"rating_count = train_meta.groupby('rating')['xc_id'].count().reset_index()\nrating_count.rename(columns={'xc_id': 'Count', 'rating': 'Rating'}, inplace=True)\nfig = px.bar(rating_count, x='Rating', y='Count', title='Bar Plot of Number of Audio Recordings in Each Rating Category')\nfig.show()","6960f19e":"labels = train_meta.groupby(['species'])['xc_id'].count().reset_index()\nlabels.rename(columns={'xc_id': 'Count', 'species': 'Species'}, inplace=True)\nlabels.sort_values(by=['Count'], inplace=True, ascending=True)\nfig = px.bar(labels, x='Species', y='Count', hover_data=['Species'], color='Species')\nfig.show()","ff4f0c56":"fig = px.histogram(train_meta['duration'].reset_index(), x='duration', labels={'count': 'Count', 'duration': 'Duration'},\n                   title='Distribution of Duration of Audio Files')\nfig.show()","08b5279b":"train_meta.sampling_rate = train_meta.sampling_rate.apply(lambda sr: int(sr.split(' ')[0])).astype(np.uint16)\nfig = px.histogram(train_meta['sampling_rate'].reset_index(), x='sampling_rate', labels={'count': 'Count', 'sampling_rate': 'Sampling Rate'},\n                   title='Distribution of Sampling Rate of Audio Files')\nfig.show()","0cb0c7a8":"sample1 = train_meta.iloc[528]","e720538e":"IPython.display.Audio(filename=os.path.join(TRAIN_PATH, \n                                            os.path.join(sample1['ebird_code'],sample1['filename'])))","19d2b94e":"sample1","769fce38":"y, sr = librosa.load(os.path.join(os.path.join(TRAIN_PATH, sample1['ebird_code']), sample1['filename']), \n                     sr=sample1['sampling_rate'])\nsample1_audio, _ = librosa.effects.trim(y)\ntime = [v\/sample1['sampling_rate'] for v in range(len(sample1_audio))]\nfig = px.line({'Time': time, 'Intensity': sample1_audio}, x='Time', y='Intensity', title='Wave Plot of Sample1 Audio')\nfig.show()","1b8fb47a":"n_fft = 2048\nD = np.abs(librosa.stft(sample1_audio[:n_fft], n_fft=n_fft, hop_length=n_fft+1))\nfig = px.line({'Frequency': np.array(range(len(D))), 'Magnitude': D.flatten()}, x='Frequency', y='Magnitude', \n              title='Fourier Transformation of Sample1 Audio')\nfig.show()","e920d5e5":"hop_length = 512\nn_fft = 2048\nD = np.abs(librosa.stft(sample1_audio, n_fft=n_fft,  hop_length=hop_length))\nlibrosa.display.specshow(D, sr=sr, x_axis='time', y_axis='linear');\nplt.colorbar();","99221a22":"DB = librosa.amplitude_to_db(D, ref=np.max)\nlibrosa.display.specshow(DB, sr=sr, hop_length=hop_length, x_axis='time', y_axis='log');\nplt.colorbar(format='%+2.0f dB');","f72aeb69":"# EDA of Meta Data","7ecc4d84":"## Distribution of Labels and Training Data","824142c5":"## Sampling Rate","a7be3cfe":"# Sample Breakdown","4d7c51bb":"As the duration of audio file increases there is more probability that there will be ambient noise. If we train a model by splitting the audio files into 5 second partitions there is a possibility that some 5 second partitions will just be ambient noise. ","2401c84c":"## Rating\nThe rating of the audio of the quality of the audio file and the confidence of the label on the birdcall. This depends on if the researcher is able to see the bird or the amount of ambient noise present. A lower rating means a softer label.","6427317d":"It's really hard to see anything because the sounds that most humans hear are concentrated in the lower frequencies. We can adjust the intensity to decibles which is the log-scale for intensity; we can also use the [mel-scale](https:\/\/towardsdatascience.com\/getting-to-know-the-mel-spectrogram-31bca3e2d9d0) to represent frequencies. ","2c80d964":"Let's see what the frequency domain looks like after we apply a fourier transformation. A fourier transformation takes an audio clip that is in the time domain and transforms it into the frequency domain. Here is a great video on [it](https:\/\/www.youtube.com\/watch?v=spUNpyF58BY). <---","25c8e424":"We can see that most classes have almost the exact same amount of data: 100 files. However, for a minority of classes there is data imbalance present. This could possible result in underfitting in these categories; namely, `Redhead`, `Bufflehead`, etc.","49b77062":"## Spectrogram of Sample 1","0af963dd":"This is our spectrogram ! By using spectrogram representations of our audio, we can perform image classification on these spectrograms. This makes it easy to use pre-existing DNN architectures that have been tried and proven."}}