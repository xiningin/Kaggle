{"cell_type":{"37745e18":"code","6d300503":"code","9a95fb4b":"code","b18294c0":"code","9cf58307":"code","e289fcc7":"code","81de1341":"code","525b7860":"code","24d0cdf7":"code","0f534bc9":"code","625c7fb3":"code","6a3d2ed8":"code","5700dfe8":"code","feb96761":"code","b5f8ba15":"code","ca4f1dc7":"code","bb6ce56d":"code","4e870054":"code","b77feac1":"code","703aa9b8":"code","e44b7dd2":"code","299f61f5":"code","a790429a":"code","a1d8ba7f":"code","8578a0b8":"code","c48f2476":"code","8100facc":"code","cd83553b":"code","32ac4930":"code","50ce5d79":"markdown","72a38c4d":"markdown","b6720e69":"markdown","741c6738":"markdown","f0d83d32":"markdown","3e9f0ff2":"markdown","b5be9044":"markdown","47c614ac":"markdown","5cf3528c":"markdown","6fbad91a":"markdown","6365b32a":"markdown","96449f69":"markdown"},"source":{"37745e18":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6d300503":"# Gaussian Naive Bayes Classification\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport scipy.stats as stats\n\n%matplotlib inline\ndata = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\n\nX = data.drop(columns=['Outcome'],axis=1)\nY = data['Outcome']\n","9a95fb4b":"data.head()","b18294c0":"data.isin([0]).sum()","9cf58307":"def univariateAnalysis_numeric1(column,nbins):\n    print(\"\\nDescription of \" + column)\n    print(\"----------------------------------------------------------------------------\")\n    print(data[column].describe(),end=' ')\n\n    print(\"\\nCentral values of \" + column)\n    print(\"----------------------------------------------------------------------------\")\n    #Central values \n    print('\\nMinimum : ', data[column].min(),end=' ')\n    print('\\nMaximum : ', data[column].max(),end=' ')\n    print('\\nMean value : ', data[column].mean(),end=' ')\n    print('\\nMedian value : ', data[column].median(),end=' ')\n    print('\\nStandard deviation : ', data[column].std(),end=' ')\n    print('\\nNull values : ', data[column].isnull().any(),end=' ')\n    print('\\nNull values : ', data[column].isnull().sum().sum(),end=' ')\n\n    print(\"\\nQuartile of \" + column)\n    print(\"----------------------------------------------------------------------------\")\n    #Quartiles\n    Q1=data[column].quantile(q=0.25)\n    Q3=data[column].quantile(q=0.75)\n    print('1st Quartile (Q1) is: ', Q1)\n    print('3st Quartile (Q3) is: ', Q3)\n    print('Interquartile range (IQR) is ', stats.iqr(data[column]))\n\n    print(\"\\nOutlier detection from Interquartile range (IQR) \" + column)\n    print(\"----------------------------------------------------------------------------\")\n    L_outliers=Q1-1.5*(Q3-Q1)\n    U_outliers=Q3+1.5*(Q3-Q1)\n    print('\\nLower outliers range: ', L_outliers)\n    print('\\nUpper outliers range: ', U_outliers)\n    print('Number of outliers in upper : ', data[data[column]>U_outliers][column].count())\n    print('Number of outliers in lower : ', data[data[column]<L_outliers][column].count())\n    print('% of Outlier in upper: ',round(data[data[column]>U_outliers][column].count()*100\/len(data)), '%')\n    print('% of Outlier in lower: ',round(data[data[column]<L_outliers][column].count()*100\/len(data)), '%')\n\n    #boxplot\n    plt.figure()\n    print(\"\\nBoxPlot of \" + column)\n    print(\"----------------------------------------------------------------------------\")\n    ax = sns.boxplot(x=data[column])\n    plt.show()\n    \n    #distplot\n    plt.figure()\n    print(\"\\ndistplot of \" + column)\n    print(\"----------------------------------------------------------------------------\")\n    sns.distplot(data[column])\n    plt.show()\n    \n    #histogram\n    plt.figure()\n    print(\"\\nHistogram of \" + column)\n    print(\"----------------------------------------------------------------------------\")\n    sns.distplot(data[column], kde=False, color='red')\n    plt.show()\n\n    # Plotting mean, median and mode\n    plt.figure()\n    print(\"\\nHistogram with mean, median and mode of \" + column)\n    print(\"----------------------------------------------------------------------------\")\n    mean=data[column].mean()\n    median=data[column].median()\n    mode=data[column].mode()\n\n    print('Mean: ',mean,'\\nMedian: ',median,'\\nMode: ',mode[0])\n    plt.hist(data[column],bins=100,color='lightblue') #Plot the histogram\n    plt.axvline(mean,color='green',label='Mean')     # Draw lines on the plot for mean median and the two modes we have in GRE Score\n    plt.axvline(median,color='blue',label='Median')\n    plt.axvline(mode[0],color='red',label='Mode1')\n    plt.legend()              # Plot the legend\n    plt.show()\n\n    print(\"\\nSkewness of \" + column)\n    print(\"----------------------------------------------------------------------------\")\n\n    print(data[column].skew())\n    \n    fig, (ax1)=plt.subplots(1,0,figsize=(13,5))","e289fcc7":"df_num = data.select_dtypes(include = ['float64', 'int64'])\nlstnumericcolumns = list(df_num.columns.values)\nlen(lstnumericcolumns)\n","81de1341":"for x in lstnumericcolumns:\n    univariateAnalysis_numeric1(x,20)","525b7860":"sns.pairplot(data,diag_kind='kde',kind='reg')","24d0cdf7":"#correlation matrix\ndata.corr().T","0f534bc9":"corr = df_num.corr(method='pearson')\nmask = np.triu(np.ones_like(corr, dtype=np.bool)) \nfig = plt.subplots(figsize=(25, 15))\nsns.heatmap(df_num.corr(), annot=True,fmt='.2f',mask=mask)\nplt.show()","625c7fb3":"# Let us see the significant correlation either negative or positive among independent attributes..\nc = data.corr().abs() # Since there may be positive as well as -ve correlation\ns = c.unstack() # \nso = s.sort_values(ascending=False) # Sorting according to the correlation\nso=so[(so<1) & (so>0.3)].drop_duplicates().to_frame() # Due to symmetry.. dropping duplicate entries.\nso.columns = ['correlation']\nso","6a3d2ed8":"model = GaussianNB()\ncv_scores = cross_val_score(model, X, Y, cv=5)\n    \nprint(model, ' mean accuracy: ', round(cv_scores.mean()*100, 3), '% std: ', round(cv_scores.var()*100, 3),'%')","5700dfe8":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n","feb96761":"# Creating dataframe with 0 values in any cell\ndata1 = data.copy()\ndata1 = data1.drop(columns=['Outcome'],axis=1)\ndata1 = data1[(data1 == 0).any(axis=1)]\ndata1","b5f8ba15":"#Conditional Formatting in pandas - Colouring cells having 0 (zero) values\n\ndata1.style.applymap(lambda x: 'background-color : red' if x==0 else '')","ca4f1dc7":"from sklearn.impute import SimpleImputer\nrep_0 = SimpleImputer(missing_values=0, strategy=\"mean\")\ncols = X_train.columns\nX_train = pd.DataFrame(rep_0.fit_transform(X_train))\nX_test = pd.DataFrame(rep_0.fit_transform(X_test))\n\nX_train.columns = cols\nX_test.columns = cols\n\nX_train.head()","bb6ce56d":"y_pred = model.fit(X_train, y_train).predict(X_test)\nprint(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))","4e870054":"predict_train = model.fit(X_train, y_train).predict(X_train)\n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracy_train)\n\n\n# predict the target on the test dataset\npredict_test = model.predict(X_test)\n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_test,predict_test)\nprint('accuracy_score on test dataset : ', accuracy_test)\n","b77feac1":"from sklearn import metrics\nf,a =  plt.subplots(1,2,sharex=True,sharey=True,squeeze=False)\n\n#Plotting confusion matrix for the different models for the Training Data\n\nplot_0 = sns.heatmap((metrics.confusion_matrix(y_train,predict_train)),annot=True,fmt='.5g',cmap=\"YlGn\",ax=a[0][0]);\na[0][0].set_title('Training Data')\n\nplot_1 = sns.heatmap((metrics.confusion_matrix(y_test,predict_test)),annot=True,fmt='.5g',cmap=\"YlGn\",ax=a[0][1]);\na[0][1].set_title('Test Data');","703aa9b8":"from sklearn.metrics import roc_auc_score,roc_curve,classification_report,confusion_matrix,plot_confusion_matrix\nprint(classification_report(y_train,predict_train))\nprint(classification_report(y_test,predict_test))","e44b7dd2":"np.logspace(0,-9, num=10)","299f61f5":"from sklearn.model_selection import RepeatedStratifiedKFold\n\ncv_method = RepeatedStratifiedKFold(n_splits=5, \n                                    n_repeats=3, \n                                    random_state=999)","a790429a":"\nfrom sklearn.preprocessing import PowerTransformer\nparams_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n\ngs_NB = GridSearchCV(estimator=model, \n                     param_grid=params_NB, \n                     cv=cv_method,\n                     verbose=1, \n                     scoring='accuracy')\n\nData_transformed = PowerTransformer().fit_transform(X_test)\n\ngs_NB.fit(Data_transformed, y_test);","a1d8ba7f":"gs_NB.best_params_","8578a0b8":"gs_NB.best_score_","c48f2476":"results_NB = pd.DataFrame(gs_NB.cv_results_['params'])\nresults_NB['test_score'] = gs_NB.cv_results_['mean_test_score']","8100facc":"plt.plot(results_NB['var_smoothing'], results_NB['test_score'], marker = '.')    \nplt.xlabel('Var. Smoothing')\nplt.ylabel(\"Mean CV Score\")\nplt.title(\"NB Performance Comparison\")\nplt.show()","cd83553b":"# predict the target on the test dataset\npredict_test = gs_NB.predict(Data_transformed)\n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_test,predict_test)\nprint('accuracy_score on test dataset : ', accuracy_test)\n","32ac4930":"\nsns.heatmap((metrics.confusion_matrix(y_test,predict_test)),annot=True,fmt='.5g',cmap=\"YlGn\").set_title('Test Data');","50ce5d79":"## Understanding Statistics behind Naive Bayes\n\nGaussian Naive Bayes is based on Bayes\u2019 Theorem and has a strong assumption that predictors should be independent of each other. For example, Should we give a Loan applicant would depend on the applicant's income, age, previous loan, location, and transaction history. In real life scenario, it is most unlikely that data points don't interact with each other but surprisingly Gaussian Naive Bayes performs well in that situation. Hence, this assumption is called class conditional independence.\n\nLet's understand with an example of 2 dice:\n\nGaussian Naive Bayes says that events should be mutually independent and to understand that let's start with basic statistics. \n\nEvent A -> Roll 1 on 1st Dice\n\nEvent B -> Roll 1 on 2nd Dice\n\nLet A and B be any events with probabilities P(A) and P(B). Both the events are mutually independent. So if we have to calculate the probability of both the events then:\n\nP(A) = 1\/6 and,\n\nP(B) = 1\/6\n\nP(A and B) = P(A)P(B) = 1\/36\n\nIf we are told that B has occurred, then the probability of A might change. The new probability of A is called the conditional probability of A given B.\n\nConditional Probability:\n\nP(A|B) = 1\/6\nP(B|A) = 1\/6\n\nWe can say that:\n\nP(A|B) = P(A and B)\/P(B) It can also be written as,\n\nP(A|B) = P(A) or,\n\nP(A and B) = P(A)P(B)\n\nOR\n\nMultiplication rule: P(A and B) = P(A|B) P(B) OR,\n\nMultiplication rule: P(B and A) = P(B|A)P(A)\n\nWe can also write the equation as:\n\nP(A|B) P(B) = P(B|A)P(A)\n\nThis gives us the Bayes theorem:\n\nP(A|B) = P(B|A)P(A)\/P(B)\n\n* P(A|B) is the posterior probability of class (A, target) given predictor (B, attributes).\n\n* P(A) is the prior probability of class.\n\n* P(B|A) is the probability of the predictor given class.\n\n* P(B) is the prior probability of the predictor.\n\nPosterior Probability = (Conditional Probability x Prior probability)\/ Evidence\n","72a38c4d":"## Understanding how Algorithm works\nLet's understand the working of Naive Bayes with an example. consider a use case where we want to predict if a flight would land in the time given weather conditions on that specific day using the Naive Bayes algorithm. Below are the steps which algorithm follows:\n\nCalculate prior probability for given class labels\nCreate a frequency table out of given historical data\nFind likelihood probability with each attribute of each class. For example, given it was Sunny weather, was the flight in time.\nNow put these values into the Bayes formula and calculate posterior probability.\nThe class with the highest probability will be the outcome.\n\n<b>Problem:<\/b> Given the historical data, we want to predict if the flight will land in time if the weather is Dusty?\n\n![flight%20in%20time.PNG](attachment:flight%20in%20time.PNG)\n\n<b>Probability of Flight arriving in time:<\/b>\n\nP(Yes | Dusty) = P( Dusty | Yes) * P(Yes) \/ P(Dusty) \n\n1. Calculating Prior probability\n\n    P(Dusty) = 6\/16 = 0.375 \n    P(Yes)= 9\/16 = 0.563\n\n2. Calculating Posterior probability\n    P (Dusty | Yes) = 4\/9 = 0.444\n    Putting Prior and Posterior in our equation:\n    P (Yes | Dusty) = 0.444 * 0.563 \/ 0.375 = 0.666\n\n<b>Probability of Flight not arriving in time:<\/b>\n\nP(No | Dusty) = P( Dusty | No) * P(No) \/ P(Dusty)\n\n1. Calculating Prior probability\n    P(Dusty) = 6\/16 = 0.375\n    P(No) = 7\/16 = 0.438\n\n2. Calculating Posterior probability\n    P(Dusty | No) = 2\/7 = 0.285\n\nPutting Prior and Posterior in our equation\n\nP(No | Dusty) = 0.285*0.438 \/ 0.375 = 0.332\n\nGiven its Dusty weather flight will land in time. Here probability of flight arriving in time (0.666) is greater than flight not arriving in time (0.332), So the class assigned will be 'In Time'.","b6720e69":"## Accuracy","741c6738":"## Zero Probability Phenomena\nSuppose we are predicting if a newly arrived email is spam or not. The algorithm predicts based on the keyword in the dataset. While analyzing the new keyword \"money\" for which there is no tuple in the dataset, in this scenario, the posterior probability will be zero and the model will assign 0 (Zero) probability because the occurrence of a particular keyword class is zero. This is referred to as \"Zero Probability Phenomena\".\n\nWe can get over this issue by using smoothing techniques. One of the techniques is Laplace transformation, which adds 1 more tuple for each keyword class pair. In the above example, let's say we have 1000 keywords in the training dataset. Where we have 0 tuples for keyword \"money\", 990 tuples for keyword \"password\" and 10 tuples for keyword \"account\" for classifying an email as spam. Without Laplace transformation the probability will be: 0 (0\/1000), 0.990 (990\/1000) and 0.010 (10\/1000).\n\nNow if we apply Laplace transformation and add 1 more tuple for each keyword then the new probability will be 0.001 (1\/1003), 0.988 (991\/1003), and 0.01 (11\/1003). ","f0d83d32":"## Confusion Matrix","3e9f0ff2":"## EDA","b5be9044":"# Gaussian Naive Bayes - Explained","47c614ac":"## Hyperparameter Tuning to improve Accuracy\n\nVar_smoothing (Variance smoothing) parameter specifies the portion of the largest variance of all features to be added to variances for stability of calculation. \n\nGaussian Naive Bayes assumes that features follows normal distribution which is most unlikely in real world.So solve this problem we can perform \"power transformation\" on each feature to make it more or less normally distributed. By default, PowerTransformer results in features that have a 0 mean and 1 standard deviation.","5cf3528c":"# Pros and Cons\n\n### Pros\n* Simple, Fast in processing, and effective in predicting the class of test dataset. So you can use it to make real-time predictions for example to check if an email is spam or not. Email services use this excellent algorithm to filter out spam emails.\n* Effective in solving a multiclass problem which makes it perfect for identifying Sentiment. Whether it belongs to the positive class or the negative class.\n* Does well with few samples for training when compared to other models like Logistic Regression.\n* Easy to obtain the estimated probability for a prediction. This can be obtained by calculating the mean, for example, print(result.mean()). \n* It performs well in case of text analytics problems. \n* It can be used for multiple class prediction problems where we have more than 2 classes.\n\n### Cons\n* Relies on and often an incorrect assumption of independent features. In real life, you will hardly find independent features. For example, Loan eligibility analysis would depend on the applicant's income, age, previous loan, location, and transaction history which might be interdependent. \n* Not ideal for data sets with a large number of numerical attributes. If the number of attributes is larger then there will be high computation cost and it will suffer from the Curse of dimensionality.\n* If a category is not captured in the training set and appears in the test data set then the model is assign 0 (zero) probability which leads to incorrect calculation. This phenomenon is referred to as 'Zero frequency' and to overcome 'Zero frequency' phenomena you will have to use smoothing techniques.","6fbad91a":"## Introduction\nNaive Bayes is a classification technique based on the Bayes theorem. It is a simple but powerful algorithm for predictive modeling under supervised learning algorithms. The technique behind Naive Bayes is easy to understand.  Naive Bayes has higher accuracy and speed when we have large data points.\n\nThere are three types of Naive Bayes models: Gaussian, Multinomial, and Bernoulli.\n\n* Gaussian Naive Bayes - This is a variant of Naive Bayes which supports continuous values and has an assumption that each class is normally distributed. \n* Multinomial Naive Bayes - This is another variant which is an event-based model that has features as vectors where sample(feature) represents frequencies with which certain events have occurred.\n* Bernoulli - This variant is also event-based where features are independent boolean which are in binary form.","6365b32a":"## Classification Report","96449f69":"## Imputing 0 (zero) values with mean"}}