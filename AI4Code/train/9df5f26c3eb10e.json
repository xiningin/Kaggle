{"cell_type":{"9bf1b7fb":"code","9f2e0616":"code","28f1fac3":"code","7bfb3879":"code","0715c567":"code","424c2dc6":"code","c55005a1":"code","763c9c71":"code","bbc62d59":"code","7086cf02":"code","25e2ae9a":"code","69ce1c88":"code","6f2754f2":"code","f2d88d38":"code","a3eb4f94":"code","ebf1034f":"code","6891a110":"code","fadabff8":"code","62abe868":"code","13806899":"markdown","614f32ff":"markdown","6475be1a":"markdown","b8ae6613":"markdown","7d5df0a0":"markdown","396b842b":"markdown","0ef95bbf":"markdown","ff2fb567":"markdown","7c64be63":"markdown","e1d1eb35":"markdown","a5bf6ef9":"markdown","0ac9b67d":"markdown"},"source":{"9bf1b7fb":"# !pip install -U tensorflow_addons==0.6.0\n\n## Prepara o dataset\n! rm -rf datasets && mkdir datasets\n! tar -C datasets -xzf ..\/input\/airportalunos\/airport-alunos.tgz\n\n## Baixa um .py com fun\u00e7\u00f5es auxiliares\n\n! wget https:\/\/gist.githubusercontent.com\/allexlima\/3d3c8fc751d94142cf08bdb8e89c0d2e\/raw\/e28ab4ec4e4af60eca72a2cb9be4611045300fc7\/ammd_aux.py","9f2e0616":"! ls -l ..\/input\/airportalunos","28f1fac3":"import tensorflow as tf\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scikitplot as skplt\nfrom sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n\nfrom ammd_aux import (person_to_img, combine_cam_files, get_batch, \n                      reconstruction_error_loss, contrastive_loss,\n                      euclidean_distance, accuracy)","7bfb3879":"print(\"TF Version:\", tf.__version__)\nprint(\"TF Using GPU:\", tf.test.is_gpu_available())","0715c567":"file_names_treino = !find datasets\/airport-alunos\/treino -name '???.png' | sort\nfile_names_val = !find datasets\/airport-alunos\/val -name '???.png' | sort","424c2dc6":"# carrega imagens para mem\u00f3ria\ncam_img_dict_treino, pids1_treino, pids2_treino = person_to_img(file_names_treino)\n\n# gera listas de similares e dissimilares na mem\u00f3ria\nd_combination_treino = combine_cam_files(pids1_treino, pids2_treino)\n\n# obtem pares de batches apartir das imagens em mem\u00f3ria\nX_a_train, X_b_train, y_train = get_batch(d_combination_treino, cam_img_dict_treino)","c55005a1":"# carrega imagens para mem\u00f3ria\ncam_img_dict_val, pids1_val, pids2_val = person_to_img(file_names_val)\n\n# gera listas de similares e dissimilares na mem\u00f3ria\nd_combination_val = combine_cam_files(pids1_val, pids2_val)\n\n# obtem pares de batches apartir das imagens em mem\u00f3ria\nX_a_val, X_b_val, y_val = get_batch(d_combination_val, cam_img_dict_val)","763c9c71":"class ConvBlock(tf.keras.layers.Layer):\n    def __init__(self, filters, kernel_size=(3, 3), pool_size=(2, 2), **kwards):\n        super(ConvBlock, self).__init__(name='ConvBlock', **kwards)\n\n        self.conv = tf.keras.layers.Conv2D(filters, kernel_size, padding='same', \n                                           kernel_initializer='glorot_normal', \n                                           kernel_regularizer=tf.keras.regularizers.l2(2e-4))\n        self.btnorm = tf.keras.layers.BatchNormalization()\n        self.relu = tf.keras.layers.Activation('relu')\n        self.pooling = tf.keras.layers.MaxPooling2D(pool_size)\n\n    def call(self, inputs):\n        x = self.conv(inputs)\n        x = self.btnorm(x)\n        return self.pooling(x)\n\n\nclass BaseNet(tf.keras.models.Model):\n    def __init__(self, name='CNN', **kwards):\n        super(BaseNet, self).__init__(name=name, **kwards)\n        self.h1 = ConvBlock(8)\n        self.h2 = ConvBlock(16)\n        self.h3 = ConvBlock(32)\n        self.h4 = ConvBlock(64)\n        self.flatten = tf.keras.layers.Flatten()\n        self.embedding = tf.keras.layers.Dense(64, activation='elu', \n                                               kernel_initializer='he_normal')\n\n    def call(self, inputs):\n        x = self.h1(inputs)\n        x = self.h2(x)\n        x = self.h3(x)\n        x = self.h4(x)\n        x = self.flatten(x)\n        return self.embedding(x)","bbc62d59":"base_cnn = BaseNet()\n\ninput_shape = (128, 64, 3)\n\nx_left = tf.keras.layers.Input(input_shape, name='A')\nx_right = tf.keras.layers.Input(input_shape, name='B')\n\nembedding_left = base_cnn(x_left)\nembedding_right = base_cnn(x_right)\n\ndist = tf.keras.layers.Lambda(euclidean_distance, \n                              output_shape=(1, 1),\n                              name='EuclideanDistance')([embedding_left, \n                                                         embedding_right])\n\nmodel = tf.keras.models.Model(inputs=[x_left, x_right], outputs=[dist])\n\ntf.keras.utils.plot_model(model, show_shapes=True)","7086cf02":"model.summary()","25e2ae9a":"es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, \n                                      patience=20)\n\nmodel.compile(learning_rate=0.001,\n              loss=contrastive_loss, \n              optimizer=tf.keras.optimizers.RMSprop(),\n              callbacks=[es],\n              metrics=[accuracy])\n\nhist = model.fit([X_a_train, X_b_train], y_train, batch_size=64, epochs=100,\n                 validation_data=([X_a_val, X_b_val], y_val))","69ce1c88":"pd.DataFrame(hist.history).plot()","6f2754f2":"class Encoder(tf.keras.layers.Layer):\n  def __init__(self, intermediate_dims, **kwargs):\n    super(Encoder, self).__init__(**kwargs)\n    self.h1 = tf.keras.layers.Dense(\n      units=intermediate_dims[0],\n      activation=tf.nn.elu,\n      kernel_initializer='he_uniform'\n    )\n\n    self.h2 = tf.keras.layers.Dense(\n      units=intermediate_dims[1],\n      activation=tf.nn.elu,\n      kernel_initializer='he_uniform'\n    )\n        \n    self.h3 = tf.keras.layers.Dense(\n      units=intermediate_dims[2],\n      activation=tf.nn.elu,\n      kernel_initializer='he_uniform'\n    )\n\n    \n  def call(self, input_features):\n    x = self.h1(input_features)\n    x = self.h2(x)\n    x = self.h3(x)\n    return x\n\nclass Decoder(tf.keras.layers.Layer):\n  def __init__(self, intermediate_dims, original_dim, **kwargs):\n    super(Decoder, self).__init__(**kwargs)\n    self.h1 = tf.keras.layers.Dense(\n      units=intermediate_dims[2],\n      activation=tf.nn.elu,\n      kernel_initializer='he_uniform'\n    )\n\n    self.h2 = tf.keras.layers.Dense(\n      units=intermediate_dims[1],\n      activation=tf.nn.elu,\n      kernel_initializer='he_uniform'\n    )\n        \n    self.h3 = tf.keras.layers.Dense(\n      units=intermediate_dims[0],\n      activation=tf.nn.elu,\n      kernel_initializer='he_uniform'\n    )\n\n    self.output_layer = tf.keras.layers.Dense(\n      units=original_dim,\n      kernel_initializer='he_uniform'\n    )\n    \n  def call(self, input_features):\n    x = self.h1(input_features)\n    x = self.h2(x)\n    x = self.h3(x)\n    # x = self.output_layer(x)\n    return x\n\nclass Autoencoder(tf.keras.Model):\n  def __init__(self, intermediate_dims, original_dim, **kwargs):\n    super(Autoencoder, self).__init__(**kwargs)\n    self.input_dim = original_dim\n    self.encoder = Encoder(intermediate_dims=intermediate_dims)\n    self.decoder = Decoder(intermediate_dims=intermediate_dims, original_dim=original_dim)\n  \n  def call(self, input_features):\n    code = self.encoder(input_features)\n    reconstructed = self.decoder(code)\n    return reconstructed","f2d88d38":"# autoencoder = Autoencoder(intermediate_dims=[64, 32, 16], original_dim=8192*3)\n# opt = tf.optimizers.Adam(learning_rate=learning_rate)\n\n# autoencoder.compile(loss=reconstruction_error_loss, optimizer=opt)\n\n# writer = tf.summary.create_file_writer('tmp')\n\n# with writer.as_default():\n#   with tf.summary.record_if(True):\n#     for epoch in range(epochs):\n#       # for step, batch_features in enumerate(X_a_treino_norm):\n#       X_a_treino, X_b_treino, y_treino = get_batch(d_combination_treino, cam_img_dict_treino)\n#       print(X_a_treino.shape)\n#       X_a_treino_norm = X_a_treino\/255\n#       X_a_treino_norm = X_a_treino_norm.reshape(X_a_treino.shape[0], X_a_treino.shape[1] * X_a_treino.shape[2] * X_a_treino.shape[3])\n#       autoencoder.fit(X_a_treino_norm, X_a_treino_norm, batch_size=16, epochs=1, validation_data=(X_a_treino_norm, X_a_treino_norm), shuffle=True)\n      ","a3eb4f94":"class BaseNetAE(BaseNet):\n    def __init__(self, name='SiameseNetwork_AE', **kwards):\n        super(BaseNetAE, self).__init__(name=name, **kwards)\n        self.ae = Autoencoder(intermediate_dims=[64, 32, 16], original_dim=8192*3)\n\n    def call(self, inputs):\n        x = self.h1(inputs)\n        x = self.h2(x)\n        x = self.h3(x)\n        x = self.h4(x)\n        x = self.flatten(x)\n        x = self.embedding(x)\n        return self.ae(x)","ebf1034f":"base_cnn_ae = BaseNetAE()\n\ninput_shape = (128, 64, 3)\n\nx_left = tf.keras.layers.Input(input_shape, name='A')\nx_right = tf.keras.layers.Input(input_shape, name='B')\n\nembedding_left = base_cnn_ae(x_left)\nembedding_right = base_cnn_ae(x_right)\n\ndist = tf.keras.layers.Lambda(euclidean_distance, \n                              output_shape=(1, 1),\n                              name='EuclidianDistance')([embedding_left, \n                                                         embedding_right])\n\nmodel = tf.keras.models.Model(inputs=[x_left, x_right], outputs=[dist])\n\ntf.keras.utils.plot_model(model, show_shapes=True)","6891a110":"model.summary()","fadabff8":"es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, \n                                      patience=20)\n\nmodel.compile(learning_rate=0.001,\n              loss=contrastive_loss, \n              optimizer=tf.keras.optimizers.RMSprop(),\n              callbacks=[es],\n              metrics=[accuracy])\n\nhist = model.fit([X_a_train, X_b_train], y_train, batch_size=64, epochs=100,\n                 validation_data=([X_a_val, X_b_val], y_val))","62abe868":"pd.DataFrame(hist.history).plot()","13806899":"# Pretrained AE (S3)\n","614f32ff":"Validation set:","6475be1a":"Training set:","b8ae6613":"# Setup","7d5df0a0":"# Siamese Network + Autoencoder (S2)\n","396b842b":"## Autoencoder Section","0ef95bbf":"## Data importing","ff2fb567":"## Train\n","7c64be63":"# Siamese Network (S1)\n","e1d1eb35":"# T\u00f3picos Especiais em Aprendizado de M\u00e1quina e Minera\u00e7\u00e3o de Dados\n\nNeste trabalho, nosso objetivo \u00e9 re-identificar pessoas capturadas por c\u00e2meras de seguran\u00e7a. Em particular, dadas duas imagens capturadas por diferentes c\u00e2meras, queremos determinar se a pessoa observada nas imagens \u00e9 a mesma. \n\nNa figura abaixo temos exemplos de dez pares de imagens obtidas pelas c\u00e2meras, rotuladas como iguais e diferentes, dependendo se elas capturaram a mesma pessoa ou n\u00e3o:\n\n\n---\n\n\n**Professores:** Marco Cristo e Eulanda Miranda\n\n**Alunos:** Allex Lima e Nicoli Ara\u00fajo\n","a5bf6ef9":"## Combination\n","0ac9b67d":"## Base model"}}