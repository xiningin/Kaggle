{"cell_type":{"bccdc5c5":"code","bc2ec4e4":"code","9677a03d":"code","205b0dbd":"code","3490f68e":"code","881d0a3c":"code","75b30b21":"code","9b6c2f49":"code","7b205ec2":"code","4a4ab58d":"code","0fdedbef":"code","7ba1b409":"code","98ca01b7":"code","9db73025":"code","c532ae23":"code","5f16f52a":"code","a60f47b7":"code","acd5038f":"code","126ee795":"code","94964520":"code","c2c0acc9":"code","688aef84":"code","3e09fcff":"code","ef659b34":"code","090d2234":"code","63ac1db0":"code","105aa725":"code","a3a0a8d7":"code","b01cc770":"code","72d7f5d0":"code","741bb7cd":"code","934cc74b":"code","a64aca60":"code","00cd2fda":"code","815b652b":"code","f2b37684":"code","49b820a0":"code","2eb21dc2":"code","a2f90b51":"code","062b7e89":"code","10325a4f":"code","3f1d008c":"code","8e19b815":"code","65c29807":"code","c9c69d88":"code","04495a07":"code","cc33ae75":"markdown","61db238d":"markdown","a850c224":"markdown","5269a8ee":"markdown","88dd62cc":"markdown","f249aa20":"markdown","5cddc37a":"markdown","6d2653ad":"markdown","1fbff0ae":"markdown","1c93fe3b":"markdown","8e9346fb":"markdown","79032fe9":"markdown","38085403":"markdown"},"source":{"bccdc5c5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import confusion_matrix\n\nsns.set_style('darkgrid')","bc2ec4e4":"df = pd.read_csv('..\/input\/bbcnewsarchive\/bbc-news-data.csv', sep='\\t')\ndf","9677a03d":"df.info()","205b0dbd":"df.category.value_counts()","3490f68e":"stopwords = set(stopwords.words('english'))","881d0a3c":"set(list(stopwords)[0:15]) #showing only the first 15 elements of the stopwords set","75b30b21":"len(stopwords)","9b6c2f49":"contents_clean = []\nfor content_clean in df.content:\n    for word in stopwords:\n        token = \" \" + word + \" \"\n        content_clean = content_clean.replace(token, \" \")\n    contents_clean.append(content_clean)","7b205ec2":"df['content_clean'] = np.array(contents_clean)","4a4ab58d":"df.head()","0fdedbef":"len(df.content_clean[2224])","7ba1b409":"plt.figure(figsize = (12, 6))\nsns.countplot(df.category)","98ca01b7":"X = list(df.content_clean)\ny = list(df.category)","9db73025":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 42, shuffle = True)","c532ae23":"unique_elements_train, counts_elements_train = np.unique(y_train, return_counts=True)\nprint(\"Frequency of unique labels in the train set:\")\nprint(np.asarray((unique_elements_train, counts_elements_train)))","5f16f52a":"unique_elements_test, counts_elements_test = np.unique(y_test, return_counts=True)\nprint(\"Frequency of unique labels in the test set:\")\nprint(np.asarray((unique_elements_test, counts_elements_test)))","a60f47b7":"print(len(X_train))\nprint(len(y_train))\nprint(len(X_test))\nprint(len(y_test))","acd5038f":"vocab_size = 15000\nembedding_dim = 32\nmax_length = 256\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"","126ee795":"tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index","94964520":"X_train_sqncs = tokenizer.texts_to_sequences(X_train)\nX_train_padded = pad_sequences(X_train_sqncs, padding=padding_type, maxlen=max_length)\n\nX_test_sqncs = tokenizer.texts_to_sequences(X_test)\nX_test_padded = pad_sequences(X_test_sqncs, padding=padding_type, maxlen=max_length)\n\nprint(len(X_test_sqncs))\nprint(X_test_padded.shape)","c2c0acc9":"label_tokenizer = Tokenizer()\nlabel_tokenizer.fit_on_texts(df.category)\nlabel_index = label_tokenizer.word_index\n\ny_train_label_sqncs = np.array(label_tokenizer.texts_to_sequences(y_train))\ny_test_label_sqncs = np.array(label_tokenizer.texts_to_sequences(y_test))\n\nprint(y_train_label_sqncs[0])\nprint(y_train_label_sqncs[1])\nprint(y_train_label_sqncs[2])\nprint(y_train_label_sqncs.shape)\n\nprint(y_test_label_sqncs[0])\nprint(y_test_label_sqncs[1])\nprint(y_test_label_sqncs[2])\nprint(y_test_label_sqncs.shape)","688aef84":"len(word_index)","3e09fcff":"dict(list(word_index.items())[0:15]) #showing only the first 15 elements of the word_index dictionary","ef659b34":"label_index","090d2234":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(6, activation='softmax')\n])\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","63ac1db0":"num_epochs = 35\nhistory = model.fit(X_train_padded, y_train_label_sqncs, epochs=num_epochs, validation_data=(X_test_padded, y_test_label_sqncs), verbose=2)","105aa725":"y_pred = model.predict_classes(X_test_padded)","a3a0a8d7":"X_test_padded","b01cc770":"y_pred","72d7f5d0":"plt.figure(figsize=(12,6))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.xlabel(\"Epochs\")\nplt.ylabel('Accuracy')\nplt.legend(['accuracy', 'val_accuracy'])","741bb7cd":"plt.figure(figsize=(12,6))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel(\"Epochs\")\nplt.ylabel('Loss')\nplt.legend(['loss', 'val_loss'])","934cc74b":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_content(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n\nreverse_label_index = dict([(value, key) for (key, value) in label_index.items()])\n\ndef decode_labels(text):\n    text = np.array([text])\n    return ' '.join([reverse_label_index.get(i, '?') for i in text])","a64aca60":"X_test[1]","00cd2fda":"y_test[1]","815b652b":"decode_content(X_test_padded[1])","f2b37684":"decode_labels(y_pred[1])","49b820a0":"X_test[34]","2eb21dc2":"y_test[34]","a2f90b51":"decode_content(X_test_padded[34])","062b7e89":"decode_labels(y_pred[34])","10325a4f":"X_test[400]","3f1d008c":"y_test[400]","8e19b815":"decode_content(X_test_padded[400])","65c29807":"decode_labels(y_pred[400])","c9c69d88":"df.category.drop_duplicates().values","04495a07":"plt.figure(figsize=(14,10))\nconf_mat = confusion_matrix(y_test_label_sqncs, y_pred)\nsns.heatmap(conf_mat, annot=True, fmt='d', cmap=\"YlGnBu\",\n            xticklabels=list(label_index.keys()), yticklabels=list(label_index.keys()))\nplt.ylabel('Actual')\nplt.xlabel('Predicted')","cc33ae75":"Importing the required modules:","61db238d":"#### News Article 3:","a850c224":">You may have already noticed and used the categories in the news websites for accessing the topics that you would like to learn the associated recent developments. Categorizing a new content can be actually done autonomously using Supervised Learning, looking at the past news content and the labels: our ML model can predict the category of a given recent news, this method is particularly called the text classification.\n\n>To illustrate, you can see above the individual news on the BBC News website and their labels such as Business, Health, World, Europe, etc. Using an open dataset called \"BBC News Archive\", we will be training a neural network predicting the labels of unseen news based on this labeled data set.\n\n>We can treat this as a general ML problem by generating word indexes for each word in the news content, or we can be more spesific and consider the interrelations of the words by using pretrained word embeddings.\n\nThe original source of the data may be accessed through [this link ](http:\/\/mlg.ucd.ie\/datasets\/bbc.html)and it might be interesting to read the associated [research article](http:\/\/mlg.ucd.ie\/files\/publications\/greene06icml.pdf). Before giving further details regarding the data, let's discover it together through an Exploratory Data Analysis (EDA).","5269a8ee":"#### News Article 1","88dd62cc":"# Model","f249aa20":"# Train-Test Split","5cddc37a":"The result seems quite satisfying, of course we had a pretty clean text data set with plain English written by the professional journalists, however this is not always the case. For example, if we were to categorize indivudual items in a retail website, where the content was created by distinct sellers, then we might have needed to perform a detalied text processing! However, for the purposes of this notebook, we do not need much of a preprocessing, nevertheless it could be good practice to utilize pretrained word embeddings that is trained on a larger corpus with much fair generalization of the word vector representations! ","6d2653ad":"# CLASSIFYING NEWS TEXT INTO THE PREDEFINED LABELS","1fbff0ae":"# Transforming Article Text to Feature Vector","1c93fe3b":"#### News Article 2:","8e9346fb":"# Exploratory Data Analysis (EDA)","79032fe9":"### Checking a few articles and their category:","38085403":"![resim.png](attachment:resim.png)"}}