{"cell_type":{"021a561b":"code","848b9e42":"code","098d1b06":"code","28078cc2":"code","b867f44f":"code","3163a9b2":"code","02386a98":"code","7d9aa631":"code","b31c8e4e":"code","b8a3957d":"code","9ca6e249":"code","759d6299":"code","0a101655":"code","18b99ffe":"code","29789bd6":"code","ff6d6699":"code","6ef7d708":"code","76f41b9c":"code","4048fede":"code","c903d5fd":"code","469f8f85":"code","966b0f5e":"code","bf98b286":"code","1e9e9472":"code","0aaaf165":"code","6c069274":"code","b634bbc6":"code","64166066":"code","3eec0518":"code","214f253a":"code","8bcdcaa2":"code","0be37b1d":"code","34555cbf":"code","f8cdbead":"code","185ab5bd":"code","55481103":"code","a93e70ff":"code","0bfa1715":"code","65f0f337":"code","471f80bf":"code","31c99ceb":"code","f7ad9ec7":"code","97629342":"code","014643aa":"code","72822fa9":"code","ce07b4ec":"code","0b713611":"code","1b838e6e":"code","5e7ca2fe":"code","846a00f6":"code","0d89974e":"code","72d1d480":"code","e0537aa0":"code","504cc12b":"code","e5fe11a8":"code","80a63299":"code","babf58a5":"code","8536688d":"code","ff42702a":"code","8b674f85":"code","d86b48ba":"code","0c9ee68f":"code","89b3e010":"code","5be4ad6b":"code","5649a44b":"code","a65b9327":"code","541348e9":"code","03715e60":"code","d41ac51f":"code","ddb141cf":"code","fd091e47":"code","6f90fafe":"code","16a09d98":"code","8b476ef5":"code","9ea58443":"code","ea078088":"code","b382ad8e":"code","1895fbd1":"code","2df1e266":"code","80dbc3f2":"code","c050295c":"code","8e7dc729":"code","e45ca41d":"code","25e36798":"code","2763271e":"code","309bd23e":"code","356ef493":"code","1f8c7f3c":"code","705a2372":"code","3d79d43a":"code","c8a58803":"code","9c13effb":"code","c14b4aaf":"code","33bb8e71":"code","6ead6ccd":"code","71e0d4a5":"code","b0947c5f":"code","ce0fff35":"code","b2e3dcfe":"code","6a120932":"code","2f6dcecb":"code","fa647099":"code","e0e77c01":"code","6e8f39b8":"code","210d5cf1":"code","1ba9d6ab":"code","215d5f0d":"code","2fd51574":"code","36c9579e":"code","50fde61d":"code","1bfa0ca0":"code","d95ed6da":"code","facc1f96":"code","7caf487f":"code","c16afc7f":"code","8ca6b84e":"code","e6d50c73":"code","ae979ce3":"code","85f31897":"code","7a17100b":"code","7ee19fd0":"code","fddb63e9":"code","d44f427e":"code","7b8d5e65":"code","abee2af5":"code","7a6ff00e":"code","a035c9a8":"code","c761b9ac":"code","7179a907":"code","519cbe44":"code","e7a4d71e":"code","c4c851bf":"code","a7b3aa01":"code","d3493b76":"markdown","1e55c573":"markdown","4e59aa93":"markdown","345d65cb":"markdown","988eee3e":"markdown","1f44a490":"markdown","1f8c5b81":"markdown","ae2b88e9":"markdown","d03058c5":"markdown","4637729c":"markdown","a8649c3c":"markdown","ae2173d3":"markdown","114f577d":"markdown","1465a86a":"markdown","f2426656":"markdown","f5077e1a":"markdown","2d96e806":"markdown","10758ef5":"markdown","d9bc9999":"markdown","50cd1d49":"markdown","5671f649":"markdown","f4ce5d0b":"markdown","3e16cd1f":"markdown","b1e0c89b":"markdown"},"source":{"021a561b":"import pandas as pd\nimport os\nimport warnings\nwarnings.filterwarnings(action='ignore')\nimport IPython.display as ipd\nimport numpy as np\nimport folium\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport descartes\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\nimport geopandas as gpd\nimport geopy.distance\nfrom shapely.geometry import Point\n\nfrom datetime import datetime, timedelta\n\nfrom bokeh.layouts import column, row\nfrom bokeh.models import ColumnDataSource, LinearAxis, Range1d\nfrom bokeh.models.tools import HoverTool\nfrom bokeh.palettes import BuGn4\nfrom bokeh.plotting import figure, output_notebook, show\nfrom bokeh.transform import cumsum\n\noutput_notebook()","848b9e42":"import time\nimport librosa\nimport sklearn\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import Audio\nfrom tqdm import tqdm\ntqdm.pandas()","098d1b06":"## configuring setup, constants and parameters\nPATH_TRAIN = \"..\/input\/birdclef-2021\/train_metadata.csv\"\nPATH_TRAIN_SOUNDSCAPE = \"..\/input\/birdclef-2021\/train_soundscape_labels.csv\"\nPATH_TEST = \"..\/input\/birdclef-2021\/test.csv\"\nPATH_TEST_RECORDING_DATE_LOC = \"..\/input\/birdclef-2021\/test_soundscapes\/test_set_recording_dates.csv\"\n#PATH_EBIRD_CLEMENTS = \"..\/input\/ebirdclements-checklist\/eBird-Clements-v2019-integrated-checklist-August-2019.csv\"\nPATH_AUDIO = \"..\/input\/birdclef-2021\/train_short_audio\"","28078cc2":"train = pd.read_csv(PATH_TRAIN,)\ntrain.head()","b867f44f":"print(\" The data has \",train.shape[0],\" rows\")\nprint(\"\\n The data has \",train.shape[1],\" columns. \\n The columns are: \",train.columns.values)","3163a9b2":"train.info()","02386a98":"# train[\"date\"] = train[\"date\"].astype('datetime64[ns]')","7d9aa631":"train.dtypes","b31c8e4e":"train.groupby(\"date\")[\"common_name\"].count().reset_index()","b8a3957d":"train.dtypes","9ca6e249":"# Create some time features\n\ntrain['year'] = train['date'].apply(lambda x: x.split('-')[0]).astype(int)\ntrain['month'] = train['date'].apply(lambda x: x.split('-')[1]).astype(int)\ntrain['day_of_month'] = train['date'].apply(lambda x: x.split('-')[2]).astype(int)\ntrain[\"hour\"] = pd.to_numeric(train.time.str.split(\":\", expand = True)[0], errors = \"coerce\")","759d6299":"train[\"year\"].value_counts()","0a101655":"train[\"year\"][train[\"year\"] == 0] = 2015\ntrain[\"year\"][train[\"year\"] == 199] = 1990\ntrain[\"year\"][train[\"year\"] == 201] = 2010\ntrain[\"year\"][train[\"year\"] == 202] = 2020\ntrain[\"year\"][train[\"year\"] == 2104] = 2014","18b99ffe":"train[\"month\"].value_counts()","29789bd6":"train[\"month\"][train[\"month\"] == 0] = 6","ff6d6699":"train[\"day_of_month\"].value_counts()","6ef7d708":"train[\"day_of_month\"][train[\"day_of_month\"] == 0] = 16","76f41b9c":"train","4048fede":"#Recreate train[\"date\"] with imputed values\ntrain['date'] = pd.to_datetime(pd.DataFrame({'year':train['year'],\n                             'month':train['month'],\n                             'day':train['day_of_month']}))","c903d5fd":"train['latitude'].value_counts()","469f8f85":"train['longitude'].value_counts()","966b0f5e":"train.head()","bf98b286":"train.dtypes","1e9e9472":"test_csv = pd.read_csv(PATH_TEST)\nsample_sub= pd.read_csv(\"..\/input\/birdclef-2021\/sample_submission.csv\")","0aaaf165":"test_csv.columns","6c069274":"test_csv.head()","b634bbc6":"sample_sub.head()","64166066":"# df_ebird_clements = pd.read_csv(PATH_EBIRD_CLEMENTS, encoding = \"ISO-8859-1\")\n# df_ebird_clements","3eec0518":"#df_ebird_clements.columns","214f253a":"#Use this information later if required https:\/\/www.kaggle.com\/jmreuter\/a-birder-s-eye-view-of-the-metadata-with-taxonomy\/","8bcdcaa2":"# latitude, longitude\nCOL = ['COL', 5.57,-75.85,200,'Jard\u00edn, Departamento de Antioquia','Colombia']\nCOR = ['COR',10.12,-84.51,200,'Alajuela, San Ram\u00f3n','Costa Rica']\nSNE = ['SNE', 38.49,-119.95,200,'Sierra Nevada, California','USA'] \nSSW = ['SSW', 42.47,-76.45,200,'Ithaca, New York','USA']\nalias = ['COL','COR','SNE','SSW']\ncolumns = ['alias','latitude','longitude','size','location','country']\ndata = [COL, COR, SNE, SSW]\n\ndf_recording_loc = pd.DataFrame(data,columns=columns)\ndf_recording_loc","0be37b1d":"test_recording_date_loc = pd.read_csv(PATH_TEST_RECORDING_DATE_LOC)\ntest_recording_date_loc[\"date\"] = pd.to_datetime(test_recording_date_loc[\"date\"].astype(str), format=\"%Y%m%d\")\ntest_recording_date_loc[\"month\"] = test_recording_date_loc[\"date\"].apply(lambda x: x.month).astype(int)\n\ntest_recording_date_loc.head()","34555cbf":"df_recording_loc.values","f8cdbead":"site_params = dict([(site, []) for site in test_recording_date_loc[\"site\"].unique()])\nfor row in test_recording_date_loc.iterrows():\n    site_params[row[1][\"site\"]].append(row[1][\"month\"])\n\nfor site in site_params:\n    site_params[site] = {\"months\" : list(set(site_params[site]))}\n\nfor spatial in df_recording_loc.values:\n    site_params[spatial[0]][\"latlon\"] = (spatial[1], spatial[2])\n    site_params[spatial[0]][\"R\"] = 200.0\n    \nsite_params","185ab5bd":"site_params.items()","55481103":"def right_place_time(lat, lon, month):\n    \"\"\"\n    Calculate if an observation was made within test site parameters (coordinates and time)\n    \"\"\"\n    check = False\n    for site, params in site_params.items():\n        # Check within site\n        check_site = (geopy.distance.distance(params[\"latlon\"], (lat, lon)).km < params[\"R\"]) and (month in params[\"months\"])\n        check = check or (check_site > 0)\n\n    return check","a93e70ff":"right_place_time(42.3005, -72.5877, 9)","0bfa1715":"right_place_time(42.47, -76.5877, 1)","65f0f337":"right_place_time(42.47, -76.5877, 2)","471f80bf":"train.columns","31c99ceb":"train.dtypes","f7ad9ec7":"train.head()","97629342":"train[\"latitude\"].isna().any()","014643aa":"train[\"hour\"].isna().any()","72822fa9":"train[\"right_place_time\"] = train.progress_apply(lambda r: right_place_time(r['latitude'], r['longitude'], r[\"month\"]), axis=1)","ce07b4ec":"train.head()","0b713611":"train[\"right_place_time\"].value_counts()","1b838e6e":"print(\"Percentage of records withing test sites at matching times of year: {:.2f}%\".format(100*len(train[train[\"right_place_time\"]])\/len(train)))","5e7ca2fe":"print(\"Of {} species {} were observed within sites at the same time of year\".format(train[\"primary_label\"].nunique(), train[train[\"right_place_time\"]][\"primary_label\"].nunique()))","846a00f6":"# Training Given Environment Recordings\ntrain_soundscape = pd.read_csv(PATH_TRAIN_SOUNDSCAPE)\ntrain_soundscape.head()","0d89974e":"train_soundscape.shape","72d1d480":"#tdf0 = pd.read_csv('..\/input\/birdclef-2021\/train_metadata.csv')\nval = train_soundscape.birds.value_counts() \ny = val.to_list() \nx = val.index.to_list()\n\nprint('CALLS vs NOCALLS INFO in All Recordings')\nprint('*****************************************')\nprint(f\"Training Soundscape Identifiers: {train_soundscape[train_soundscape.birds!='nocall'].shape[0]}\")\nprint(f\"Training Soundscapes Nocalls: {train_soundscape[train_soundscape.birds=='nocall'].shape[0]}\")\n\nprint('\\nTRAINING SOUNDSCAPE RECORDINGS:')\nprint('***********************************')\ntrain_soundscape.site.value_counts() # 2\/4 TEST LOCATIONS","e0537aa0":"train['primary_label'].value_counts()","504cc12b":"len(train['primary_label'].value_counts())","e5fe11a8":"df_bird = train.groupby(\"common_name\")[\"filename\"].count().reset_index().rename(columns = {\"filename\": \"recordings\"}).sort_values(\"recordings\")\n\nsource = ColumnDataSource(df_bird)\ntooltips = [\n    (\"Bird Species\", \"@common_name\"),\n    (\"Recordings\", \"@recordings\")\n]\n\nv = figure(plot_width = 1000, plot_height = 6000, y_range = df_bird.common_name.values, tooltips = tooltips, title = \"Count of Bird Species\")\nv.hbar(\"common_name\", right = \"recordings\", source = source, height = 0.75, color = \"steelblue\", alpha = 0.6)\n\nv.xaxis.axis_label = \"Count\"\nv.yaxis.axis_label = \"Species\"\n\nshow(v)\n# df_bird.head()","80a63299":"train['secondary_labels'].value_counts()","babf58a5":"# Code adapted from: https:\/\/www.kaggle.com\/andradaolteanu\/birdcall-recognition-eda-and-audio-fe\n# Make sure to ckeck out the entire nootebook. It's brilliant.\n\n\n\n# SHP file\nworld_map = gpd.read_file(\"..\/input\/world-shapefile\/world_shapefile.shp\")\n\n# Coordinate reference system\ncrs = {\"init\" : \"epsg:4326\"}\n\n# Lat and Long need to be of type float, not object\nspecies_list = ['norcar', 'houspa', 'wesblu', 'banana']\ndata = train[train['primary_label'].isin(species_list)]\ndata[\"latitude\"] = data[\"latitude\"].astype(float)\ndata[\"longitude\"] = data[\"longitude\"].astype(float)\n\n# Create geometry\ngeometry = [Point(xy) for xy in zip(data[\"longitude\"], data[\"latitude\"])]\n\n# Geo Dataframe\ngeo_df = gpd.GeoDataFrame(data, crs=crs, geometry=geometry)\n\n# Create ID for species\nspecies_id = geo_df[\"primary_label\"].value_counts().reset_index()\nspecies_id.insert(0, 'ID', range(0, 0 + len(species_id)))\n\nspecies_id.columns = [\"ID\", \"primary_label\", \"count\"]\n\n# Add ID to geo_df\ngeo_df = pd.merge(geo_df, species_id, how=\"left\", on=\"primary_label\")\n\n# === PLOT ===\nfig, ax = plt.subplots(figsize = (16, 10))\nworld_map.plot(ax=ax, alpha=0.4, color=\"grey\")\n\npalette = iter(sns.hls_palette(len(species_id)))\nfor i in range(len(species_list)):\n    geo_df[geo_df[\"ID\"] == i].plot(ax=ax, \n                                   markersize=20, \n                                   color=next(palette), \n                                   marker=\"o\", \n                                   label = species_id['primary_label'].values[i]);\n    \nax.legend()","8536688d":"plt.figure(figsize=(16, 6))\ntrain = train.sort_values(['year']).reset_index(drop=True)\nax = sns.countplot(train['year'], palette=\"hls\")\n\n\nplt.title(\"Audio Files Registration per Year Made\", fontsize=16)\nplt.xticks(rotation=90, fontsize=13)\nplt.yticks(fontsize=13)\nplt.ylabel(\"Frequency\", fontsize=14)\nplt.xlabel(\"\");","ff42702a":"plt.figure(figsize=(16, 6))\n\ntrain = train.sort_values(['month']).reset_index(drop=True)\nax = sns.countplot(train['month'], palette=\"hls\")\n\nplt.title(\"Audio Files Registration per Month Made\", fontsize=16)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.ylabel(\"Frequency\", fontsize=14)\nplt.xlabel(\"\");","8b674f85":"df_date = train.groupby(\"date\")[\"common_name\"].count().reset_index().rename(columns = {\"common_name\": \"recordings\"})\ndf_date.head(10)","d86b48ba":"df_date.date = pd.to_datetime(df_date.date, errors = \"coerce\")\ndf_date.dropna(inplace = True)\ndf_date[\"weekday\"] = df_date.date.dt.day_name()\nsource_1 = ColumnDataSource(df_date)","0c9ee68f":"# Code adapted from https:\/\/www.kaggle.com\/shahules\/bird-watch-complete-eda-fe\n# Again, make sure to check out the entire notebook.\nimport plotly.graph_objects as go\n\nhist_data = train['rating'].values.tolist()\nfig = go.Figure(data=[go.Histogram(x=hist_data)], \n                layout=go.Layout(margin=go.layout.Margin(l=0, r=0, b=10, t=50)))\nfig.update_layout(title='Number of recordings per rating')\n\nfig.show()","89b3e010":"# df_bird_map = train[[\"primary_label\", \"common_name\"]].drop_duplicates()\n\n# for primary_label in os.listdir(PATH_AUDIO)[:20]:\n#     species = df_bird_map[df_bird_map.primary_label == primary_label].common_name.values[0]\n#     audio_file = os.listdir(f\"{PATH_AUDIO}\/{primary_label}\")[0]\n#     audio_path = f\"{PATH_AUDIO}\/{primary_label}\/{audio_file}\"\n#     ipd.display(ipd.HTML(f\"<h2>{primary_label} ({species})<\/h2>\"))\n#     ipd.display(ipd.Audio(audio_path))","5be4ad6b":"train_soundscapes = pd.read_csv('..\/input\/birdclef-2021\/train_soundscape_labels.csv',)\ntrain_soundscapes","5649a44b":"train_soundscapes[\"site\"].value_counts()","a65b9327":"train_soundscapes.shape, train_soundscapes.describe()","541348e9":"print(train_soundscapes['birds'].value_counts())","03715e60":"train","d41ac51f":"print(\"Of {} species {} were observed within sites at the same time of year\".format(train[\"primary_label\"].nunique(), train[train[\"right_place_time\"]][\"primary_label\"].nunique()))","ddb141cf":"list_of_test_site_birds = train[train[\"right_place_time\"]][\"primary_label\"].unique()","fd091e47":"list_of_test_site_birds, len(list_of_test_site_birds)","6f90fafe":"train.loc[train['primary_label'].isin(list_of_test_site_birds)]","16a09d98":"train_filtered = train.loc[train['primary_label'].isin(list_of_test_site_birds)]","8b476ef5":"train_filtered.head()","9ea58443":"train_filtered[\"primary_label\"].nunique()","ea078088":"train_filtered[train_filtered.rating.isin([0, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])][\"primary_label\"].nunique()","b382ad8e":"train_filtered = train_filtered[train_filtered.rating.isin([0, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])]","1895fbd1":"hist_data = train_filtered['rating'].values.tolist()\nfig = go.Figure(data=[go.Histogram(x=hist_data)], \n                layout=go.Layout(margin=go.layout.Margin(l=0, r=0, b=10, t=50)))\nfig.update_layout(title='Number of recordings per rating')\n\nfig.show()","2df1e266":"df_bird = train_filtered.groupby(\"common_name\")[\"filename\"].count().reset_index().rename(columns = {\"filename\": \"recordings\"}).sort_values(\"recordings\")\n\nsource = ColumnDataSource(df_bird)\ntooltips = [\n    (\"Bird Species\", \"@common_name\"),\n    (\"Recordings\", \"@recordings\")\n]\n\nv = figure(plot_width = 1000, plot_height = 4000, y_range = df_bird.common_name.values, tooltips = tooltips, title = \"Count of Bird Species\")\nv.hbar(\"common_name\", right = \"recordings\", source = source, height = 0.75, color = \"steelblue\", alpha = 0.6)\n\nv.xaxis.axis_label = \"Count\"\nv.yaxis.axis_label = \"Species\"\n\nshow(v)","80dbc3f2":"train_filtered[\"primary_label\"].nunique()","c050295c":"!nvidia-smi","8e7dc729":"!pip install -q pysndfx SoundFile audiomentations pretrainedmodels efficientnet_pytorch resnest","e45ca41d":"import numpy as np\nimport librosa as lb\nimport librosa.display as lbd\nimport soundfile as sf\nfrom  soundfile import SoundFile\nimport pandas as pd\nfrom  IPython.display import Audio\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn, optim\nfrom  torch.utils.data import Dataset, DataLoader\n\nfrom resnest.torch import resnest50\n\nfrom matplotlib import pyplot as plt\n\nimport os, random, gc\nimport re, time, json\nfrom  ast import literal_eval\n\n\nfrom IPython.display import Audio\nfrom sklearn.metrics import label_ranking_average_precision_score\n\nfrom tqdm.notebook import tqdm\nimport joblib","25e36798":"from efficientnet_pytorch import EfficientNet\nimport pretrainedmodels\nimport resnest.torch as resnest_torch","2763271e":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","309bd23e":"NUM_CLASSES = 273\nSR = 32_000\nDURATION = 7\n\nMAX_READ_SAMPLES = 7 # Each record will have 10 melspecs at most, you can increase this on Colab with High Memory Enabled\nDATA_ROOT = Path(\"..\/input\/birdclef-2021\")\nMEL_PATHS = sorted(Path(\"..\/input\").glob(\"kkiller-birdclef-mels-computer-d7-part?\/rich_train_metadata.csv\"))\nTRAIN_LABEL_PATHS = sorted(Path(\"..\/input\").glob(\"kkiller-birdclef-mels-computer-d7-part?\/LABEL_IDS.json\"))\n\nMODEL_ROOT = Path(\".\")","356ef493":"MEL_PATHS, TRAIN_LABEL_PATHS","1f8c7f3c":"TRAIN_BATCH_SIZE = 50\nTRAIN_NUM_WORKERS = 2\n\nVAL_BATCH_SIZE = 64\nVAL_NUM_WORKERS = 2\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Device:\", DEVICE)","705a2372":"temp = pd.read_csv(str('..\/input\/kkiller-birdclef-mels-computer-d7-part1\/rich_train_metadata.csv'), index_col=0)\ntemp.head()","3d79d43a":"def get_df(mel_paths=MEL_PATHS, train_label_paths=TRAIN_LABEL_PATHS):\n  df = None\n  LABEL_IDS = {}\n    \n  for file_path in mel_paths:\n    temp = pd.read_csv(str(file_path), index_col=0)\n    temp[\"impath\"] = temp.apply(lambda row: file_path.parent\/\"audio_images\/{}\/{}.npy\".format(row.primary_label, row.filename), axis=1) \n    df = temp if df is None else df.append(temp)\n    \n  df[\"secondary_labels\"] = df[\"secondary_labels\"].apply(literal_eval)\n\n  for file_path in train_label_paths:\n    with open(str(file_path)) as f:\n      LABEL_IDS.update(json.load(f))\n\n  return LABEL_IDS, df","c8a58803":"LABEL_IDS, df = get_df()\n\nprint(df.shape)\ndf.head()","9c13effb":"df","c14b4aaf":"train_filtered","33bb8e71":"# Create some time features\n\ndf['year'] = df['date'].apply(lambda x: x.split('-')[0]).astype(int)\ndf['month'] = df['date'].apply(lambda x: x.split('-')[1]).astype(int)\ndf['day_of_month'] = df['date'].apply(lambda x: x.split('-')[2]).astype(int)\ndf[\"hour\"] = pd.to_numeric(df.time.str.split(\":\", expand = True)[0], errors = \"coerce\")\n\ndf[\"year\"][df[\"year\"] == 0] = 2015\ndf[\"year\"][df[\"year\"] == 199] = 1990\ndf[\"year\"][df[\"year\"] == 201] = 2010\ndf[\"year\"][df[\"year\"] == 202] = 2020\ndf[\"year\"][df[\"year\"] == 2104] = 2014\n\ndf[\"month\"][df[\"month\"] == 0] = 6\ndf[\"day_of_month\"][df[\"day_of_month\"] == 0] = 16\n#Recreate train[\"date\"] with imputed values\ndf['date'] = pd.to_datetime(pd.DataFrame({'year':df['year'],\n                             'month':df['month'],\n                             'day':df['day_of_month']}))\n","6ead6ccd":"df[\"right_place_time\"] = df.progress_apply(lambda r: right_place_time(r['latitude'], r['longitude'], r[\"month\"]), axis=1)","71e0d4a5":"df[\"right_place_time\"].value_counts()","b0947c5f":"df_filtered = df.loc[df['primary_label'].isin(list_of_test_site_birds)]","ce0fff35":"df_filtered[\"primary_label\"].nunique()","b2e3dcfe":"df_filtered = df_filtered[df_filtered.rating.isin([0, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])]","6a120932":"hist_data = df_filtered['rating'].values.tolist()\nfig = go.Figure(data=[go.Histogram(x=hist_data)], \n                layout=go.Layout(margin=go.layout.Margin(l=0, r=0, b=10, t=50)))\nfig.update_layout(title='Number of recordings per rating')\n\nfig.show()\n","2f6dcecb":"df_bird = df_filtered.groupby(\"common_name\")[\"filename\"].count().reset_index().rename(columns = {\"filename\": \"recordings\"}).sort_values(\"recordings\")\n\nsource = ColumnDataSource(df_bird)\ntooltips = [\n    (\"Bird Species\", \"@common_name\"),\n    (\"Recordings\", \"@recordings\")\n]\n\nv = figure(plot_width = 1000, plot_height = 4000, y_range = df_bird.common_name.values, tooltips = tooltips, title = \"Count of Bird Species\")\nv.hbar(\"common_name\", right = \"recordings\", source = source, height = 0.75, color = \"steelblue\", alpha = 0.6)\n\nv.xaxis.axis_label = \"Count\"\nv.yaxis.axis_label = \"Species\"\n\nshow(v)","fa647099":"LABEL_IDS = {label: label_id for label_id,label in enumerate(sorted(df_filtered[\"primary_label\"].unique()))}\nINV_LABEL_CODE = {val: key for key,val in LABEL_IDS.items()}","e0e77c01":"LABEL_IDS","6e8f39b8":"LABEL_IDS['acafly']","210d5cf1":"import gc\ndel [[df, train, train_filtered, df_bird]]\ngc.collect()","1ba9d6ab":"df_filtered[\"label_id\"] = df_filtered['primary_label'].map(LABEL_IDS)","215d5f0d":"df_filtered[\"label_id\"].min(), df_filtered[\"label_id\"].max()","2fd51574":"df_filtered[[\"label_id\",\"primary_label\"]]","36c9579e":"df_filtered.reset_index(drop=True, inplace=True)","50fde61d":"df_filtered.head()","1bfa0ca0":"df_filtered[\"fold\"].value_counts()","d95ed6da":"def get_model(name, num_classes=NUM_CLASSES):\n    \"\"\"\n    Loads a pretrained model. \n    Supports ResNest, ResNext-wsl, EfficientNet, ResNext and ResNet.\n\n    Arguments:\n        name {str} -- Name of the model to load\n\n    Keyword Arguments:\n        num_classes {int} -- Number of classes to use (default: {1})\n\n    Returns:\n        torch model -- Pretrained model\n    \"\"\"\n    if \"resnest\" in name:\n        #model = getattr(resnest_torch, name)(pretrained=True)\n        pretrained_weights = torch.load('..\/input\/timm-resnest-weights\/resnest50-528c19ca.pth')\n        model = getattr(resnest_torch, name)(pretrained=False)\n        model.load_state_dict(pretrained_weights)\n    elif \"wsl\" in name:\n        model = torch.hub.load(\"facebookresearch\/WSL-Images\", name)\n    elif name.startswith(\"resnext\") or  name.startswith(\"resnet\"):\n        model = torch.hub.load(\"pytorch\/vision:v0.6.0\", name, pretrained=True)\n    elif name.startswith(\"tf_efficientnet_b\"):\n        model = getattr(timm.models.efficientnet, name)(pretrained=True)\n    elif \"efficientnet-b\" in name:\n        model = EfficientNet.from_pretrained(name)\n    else:\n        model = pretrainedmodels.__dict__[name](pretrained='imagenet')\n\n    if hasattr(model, \"fc\"):\n        nb_ft = model.fc.in_features\n        model.fc = nn.Linear(nb_ft, num_classes)\n    elif hasattr(model, \"_fc\"):\n        nb_ft = model._fc.in_features\n        model._fc = nn.Linear(nb_ft, num_classes)\n    elif hasattr(model, \"classifier\"):\n        nb_ft = model.classifier.in_features\n        model.classifier = nn.Linear(nb_ft, num_classes)\n    elif hasattr(model, \"last_linear\"):\n        nb_ft = model.last_linear.in_features\n        model.last_linear = nn.Linear(nb_ft, num_classes)\n\n    return model","facc1f96":"np.load(str(\"..\/input\/kkiller-birdclef-mels-computer-d7-part1\/audio_images\/acafly\/XC109605.ogg.npy\"))","7caf487f":"len(np.load(str(\"..\/input\/kkiller-birdclef-mels-computer-d7-part1\/audio_images\/acafly\/XC109605.ogg.npy\")))","c16afc7f":"def load_data(df):\n    def load_row(row):\n        # impath = TRAIN_IMAGES_ROOT\/f\"{row.primary_label}\/{row.filename}.npy\"\n        return row.filename, np.load(str(row.impath))[:MAX_READ_SAMPLES]\n    pool = joblib.Parallel(4)\n    mapper = joblib.delayed(load_row)\n    tasks = [mapper(row) for row in df.itertuples(False)]\n    res = pool(tqdm(tasks))\n    res = dict(res)\n    return res","8ca6b84e":"#Save a copy of df_filtered and use it for Inference\ndf_filtered.to_csv(\"train_metadata_filtered_rich.csv\", index=False)","e6d50c73":"# We cache the train set to reduce training time\n\naudio_image_store = load_data(df_filtered)\nlen(audio_image_store)","ae979ce3":"print(\"shape:\", next(iter(audio_image_store.values())).shape)\nlbd.specshow(next(iter(audio_image_store.values()))[0])","85f31897":"print(\"shape:\", next(iter(audio_image_store.values())).shape)\nlbd.specshow(next(iter(audio_image_store.values()))[1])","7a17100b":"class BirdClefDataset(Dataset):\n\n    def __init__(self, audio_image_store, meta, sr=SR, is_train=True, num_classes=NUM_CLASSES, duration=DURATION):\n        \n        self.audio_image_store = audio_image_store\n        self.meta = meta.copy().reset_index(drop=True)\n        self.sr = sr\n        self.is_train = is_train\n        self.num_classes = num_classes\n        self.duration = duration\n        self.audio_length = self.duration*self.sr\n    \n    @staticmethod\n    def normalize(image):\n        image = image.astype(\"float32\", copy=False) \/ 255.0\n        image = np.stack([image, image, image])\n        return image\n\n    def __len__(self):\n        return len(self.meta)\n    \n    def __getitem__(self, idx):\n        row = self.meta.iloc[idx]\n        image = self.audio_image_store[row.filename]\n\n        image = image[np.random.choice(len(image))]\n        image = self.normalize(image)\n        \n        \n        t = np.zeros(self.num_classes, dtype=np.float32) + 0.0025 # Label smoothing\n        t[row.label_id] = 0.995\n        \n        return image, t","7ee19fd0":"ds = BirdClefDataset(audio_image_store, meta=df_filtered, sr=SR, duration=DURATION, is_train=True)\nlen(ds)","fddb63e9":"ds","d44f427e":"x, y = ds[np.random.choice(len(ds))]\n# x, y = ds[0]\nx.shape, y.shape, np.where(y >= 0.5)","7b8d5e65":"y[:5]","abee2af5":"lbd.specshow(x[0])","7a6ff00e":"def one_step( xb, yb, net, criterion, optimizer, scheduler=None):\n  xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n        \n  optimizer.zero_grad()\n  o = net(xb)\n  loss = criterion(o, yb)\n  loss.backward()\n  optimizer.step()\n  \n  with torch.no_grad():\n      l = loss.item()\n\n      o = o.sigmoid()\n      yb = (yb > 0.5 )*1.0\n      lrap = label_ranking_average_precision_score(yb.cpu().numpy(), o.cpu().numpy())\n\n      o = (o > 0.5)*1.0\n\n      prec = (o*yb).sum()\/(1e-6 + o.sum())\n      rec = (o*yb).sum()\/(1e-6 + yb.sum())\n      f1 = 2*prec*rec\/(1e-6+prec+rec)\n\n  if  scheduler is not None:\n    scheduler.step()\n\n  return l, lrap, f1.item(), rec.item(), prec.item()","a035c9a8":"@torch.no_grad()\ndef evaluate(net, criterion, val_laoder):\n    net.eval()\n\n    os, y = [], []\n    val_laoder = tqdm(val_laoder, leave = False, total=len(val_laoder))\n\n    for icount, (xb, yb) in  enumerate(val_laoder):\n\n        y.append(yb.to(DEVICE))\n\n        xb = xb.to(DEVICE)\n        o = net(xb)\n\n        os.append(o)\n\n    y = torch.cat(y)\n    o = torch.cat(os)\n\n    l = criterion(o, y).item()\n    \n    o = o.sigmoid()\n    y = (y > 0.5)*1.0\n\n    lrap = label_ranking_average_precision_score(y.cpu().numpy(), o.cpu().numpy())\n\n    o = (o > 0.5)*1.0\n\n    prec = ((o*y).sum()\/(1e-6 + o.sum())).item()\n    rec = ((o*y).sum()\/(1e-6 + y.sum())).item()\n    f1 = 2*prec*rec\/(1e-6+prec+rec)\n\n    return l, lrap, f1, rec, prec, \n","c761b9ac":"def one_epoch(net, criterion, optimizer, scheduler, train_laoder, val_laoder):\n  net.train()\n  l, lrap, prec, rec, f1, icount = 0.,0.,0.,0., 0., 0\n  train_laoder = tqdm(train_laoder, leave = False)\n  epoch_bar = train_laoder\n  \n  for (xb, yb) in  epoch_bar:\n      # epoch_bar.set_description(\"----|----|----|----|---->\")\n      _l, _lrap, _f1, _rec, _prec = one_step(xb, yb, net, criterion, optimizer)\n      l += _l\n      lrap += _lrap\n      f1 += _f1\n      rec += _rec\n      prec += _prec\n\n      icount += 1\n        \n      if hasattr(epoch_bar, \"set_postfix\") and not icount%10:\n          epoch_bar.set_postfix(\n            loss=\"{:.6f}\".format(l\/icount),\n            lrap=\"{:.3f}\".format(lrap\/icount),\n            prec=\"{:.3f}\".format(prec\/icount),\n            rec=\"{:.3f}\".format(rec\/icount),\n            f1=\"{:.3f}\".format(f1\/icount),\n          )\n  \n  scheduler.step()\n\n  l \/= icount\n  lrap \/= icount\n  f1 \/= icount\n  rec \/= icount\n  prec \/= icount\n  \n  l_val, lrap_val, f1_val, rec_val, prec_val = evaluate(net, criterion, val_laoder)\n  \n  return (l, l_val), (lrap, lrap_val), (f1, f1_val), (rec, rec_val), (prec, prec_val)","7179a907":"class AutoSave:\n  def __init__(self, top_k=2, metric=\"f1\", mode=\"min\", root=None, name=\"ckpt\"):\n    self.top_k = top_k\n    self.logs = []\n    self.metric = metric\n    self.mode = mode\n    self.root = Path(root or MODEL_ROOT)\n    assert self.root.exists()\n    self.name = name\n\n    self.top_models = []\n    self.top_metrics = []\n\n  def log(self, model, metrics):\n    metric = metrics[self.metric]\n    rank = self.rank(metric)\n\n    self.top_metrics.insert(rank+1, metric)\n    if len(self.top_metrics) > self.top_k:\n      self.top_metrics.pop(0)\n\n    self.logs.append(metrics)\n    self.save(model, metric, rank, metrics[\"epoch\"])\n\n\n  def save(self, model, metric, rank, epoch):\n    t = time.strftime(\"%Y%m%d%H%M%S\")\n    name = \"{}_epoch_{:02d}_{}_{:.04f}_{}\".format(self.name, epoch, self.metric, metric, t)\n    name = re.sub(r\"[^\\w_-]\", \"\", name) + \".pth\"\n    path = self.root.joinpath(name)\n\n    old_model = None\n    self.top_models.insert(rank+1, name)\n    if len(self.top_models) > self.top_k:\n      old_model = self.root.joinpath(self.top_models[0])\n      self.top_models.pop(0)      \n\n    torch.save(model.state_dict(), path.as_posix())\n\n    if old_model is not None:\n      old_model.unlink()\n\n    self.to_json()\n\n\n  def rank(self, val):\n    r = -1\n    for top_val in self.top_metrics:\n      if val <= top_val:\n        return r\n      r += 1\n\n    return r\n  \n  def to_json(self):\n    # t = time.strftime(\"%Y%m%d%H%M%S\")\n    name = \"{}_logs\".format(self.name)\n    name = re.sub(r\"[^\\w_-]\", \"\", name) + \".json\"\n    path = self.root.joinpath(name)\n\n    with path.open(\"w\") as f:\n      json.dump(self.logs, f, indent=2)","519cbe44":"def one_fold(model_name, fold, train_set, val_set, epochs=20, save=True, save_root=None):\n\n  save_root = Path(save_root) or MODEL_ROOT\n\n  saver = AutoSave(root=save_root, name=f\"birdclef_{model_name}_fold{fold}\", metric=\"f1_val\")\n\n  net = get_model(model_name).to(DEVICE)\n\n  criterion = nn.BCEWithLogitsLoss()\n\n  optimizer = optim.Adam(net.parameters(), lr=8e-4)\n  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-5, T_max=epochs)\n\n  train_data = BirdClefDataset(audio_image_store, meta=df_filtered.iloc[train_set].reset_index(drop=True),\n                           sr=SR, duration=DURATION, is_train=True)\n  train_laoder = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, num_workers=TRAIN_NUM_WORKERS, shuffle=True, pin_memory=True)\n\n  val_data = BirdClefDataset(audio_image_store, meta=df_filtered.iloc[val_set].reset_index(drop=True),  sr=SR, duration=DURATION, is_train=False)\n  val_laoder = DataLoader(val_data, batch_size=VAL_BATCH_SIZE, num_workers=VAL_NUM_WORKERS, shuffle=False)\n\n  epochs_bar = tqdm(list(range(epochs)), leave=False)\n  for epoch  in epochs_bar:\n    epochs_bar.set_description(f\"--> [EPOCH {epoch:02d}]\")\n    net.train()\n\n    (l, l_val), (lrap, lrap_val), (f1, f1_val), (rec, rec_val), (prec, prec_val) = one_epoch(\n        net=net,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        train_laoder=train_laoder,\n        val_laoder=val_laoder,\n      )\n\n    epochs_bar.set_postfix(\n    loss=\"({:.6f}, {:.6f})\".format(l, l_val),\n    prec=\"({:.3f}, {:.3f})\".format(prec, prec_val),\n    rec=\"({:.3f}, {:.3f})\".format(rec, rec_val),\n    f1=\"({:.3f}, {:.3f})\".format(f1, f1_val),\n    lrap=\"({:.3f}, {:.3f})\".format(lrap, lrap_val),\n    )\n\n    print(\n        \"[{epoch:02d}] loss: {loss} lrap: {lrap} f1: {f1} rec: {rec} prec: {prec}\".format(\n            epoch=epoch,\n            loss=\"({:.6f}, {:.6f})\".format(l, l_val),\n            prec=\"({:.3f}, {:.3f})\".format(prec, prec_val),\n            rec=\"({:.3f}, {:.3f})\".format(rec, rec_val),\n            f1=\"({:.3f}, {:.3f})\".format(f1, f1_val),\n            lrap=\"({:.3f}, {:.3f})\".format(lrap, lrap_val),\n        )\n    )\n\n    if save:\n      metrics = {\n          \"loss\": l, \"lrap\": lrap, \"f1\": f1, \"rec\": rec, \"prec\": prec,\n          \"loss_val\": l_val, \"lrap_val\": lrap_val, \"f1_val\": f1_val, \"rec_val\": rec_val, \"prec_val\": prec_val,\n          \"epoch\": epoch,\n      }\n\n      saver.log(net, metrics)","e7a4d71e":"def train(model_name, epochs=20, save=True, n_splits=5, seed=177, save_root=None, suffix=\"\", folds=None):\n  gc.collect()\n  torch.cuda.empty_cache()\n\n  save_root = save_root or MODEL_ROOT\/f\"{model_name}{suffix}\"\n  save_root.mkdir(exist_ok=True, parents=True)\n\n  #pdb.set_trace()\n\n  fold_bar = tqdm(df_filtered.reset_index().groupby(\"fold\").index.apply(list).items(), total=df_filtered.fold.max()+1)\n  \n  for fold, val_set in fold_bar:\n      if folds and not fold in folds:\n        continue\n      \n      print(f\"\\n############################### [FOLD {fold}]\")\n      fold_bar.set_description(f\"[FOLD {fold}]\")\n      train_set = np.setdiff1d(df_filtered.index, val_set)\n        \n      one_fold(model_name, fold=fold, train_set=train_set , val_set=val_set , epochs=epochs, save=save, save_root=save_root)\n    \n      gc.collect()\n      torch.cuda.empty_cache()","c4c851bf":"MODEL_NAMES = [\n      \"efficientnet-b5\",\n]","a7b3aa01":"for model_name in MODEL_NAMES:\n  print(\"\\n\\n###########################################\", model_name.upper())\n  try:\n    train(model_name, epochs=20, suffix=f\"_sr{SR}_d{DURATION}_v1_v1\", folds=[0])\n  except Exception as e:\n    # print(f\"Error {model_name} : \\n{e}\")\n    raise ValueError() from  e","d3493b76":"## TRAINING SOUNDSCAPE : COUNTS INFORMATION  \n\nLet's view the value counts difference between nocall and call labels in the training soundscape data.\nSplitting the whole soundscape recording into 5 second segments, let's also see how many segments exist in each recording, in the training soundscape data.","1e55c573":"## 4.Time of the Recording","4e59aa93":"We can see that the majority of recordings does not have an annotation of background species. Yet, it is highly likely that most of them actually contain one or more additional species. The data also shows us that the Red-winged Blackbird (rewbla), American Robin (amerob), House Sparrow (houspa), and Northern Cardinal (norcar) appear to be some of the most common background species.\n\n**Please note, secondary lables only contain labels of species that are actually represented in the data set.**\n\n## 3. Location, location, location\n\nEach recording comes with a recording location specified in the metadata. Data fields \u201c*latitude*\u201d and \u201c*longitude*\u201d contain GPS coordinates as provided by the recordist. In combination with the recording data (data field \u201c*date*\u201d), this information can be very useful to map distribution and migration patterns. Why is it important? Not all birds occur at all locations at all times! \n\nLet's look at a few examples:\n","345d65cb":"## Recording quality ratings by species\n\nHere's the description for ratings from xeno-canto:\n\nUse the following general guidelines when rating recordings on xeno-canto. Ratings are obviously subjective, and will inevitably vary slightly between different individuals, but these guidelines should improve consistency.  \n  \nA: Loud and Clear  \nB: Clear, but bird a bit distant, or some interference with other sound sources  \nC: Moderately clear, or quite some interference  \nD: Faint recording, or much interference  \nE: Barely audible  \nNote that the A-E character classifications described on the xeno-canto website were sensibly converted to numeric classifications for the metadata, with 1 being the worst and 5 being the Best.","988eee3e":"Overall, the training data contains high-quality recordings and the majority of samples is rated with 3.5 or higher. Whenever we had to limit the amount of recordings per species for the training data, we used the 500 top-rated samples. Sub-sampling training data based on user rating might help to extract high-quality training samples.\n\nOther data fields of the metadata might be of value at some point during development, here is a brief description for each of them:\n\n* **type**: Represents the type of the vocalization, with \u201csong\u201d and \u201ccall\u201d as the most common. Excluding or including recordings of certain call types might help to diversify training data. Learn more about how and why birds vocalize here: https:\/\/academy.allaboutbirds.org\/birdsong\/\n\n* **author**: Acknowledgement to the recordists who contributed the recording. Some recordists focus on a specific subsets of species, so there might be some value in these data.\n\n* **filename**: A reference to the sound file in the training data.\n\n* **license**: All recordings have an open source license which is noted in this field. Make sure to respect the license when sharing the data.\n\n* **time**: Time of recording as stated by the recordist. Might be of value to distinguish between birds that vocalize during the day and those which only vocalize during the night. Can be used to diversify the training data.\n\n* **url**: A link to the original recording on Xeno-canto.\n\n","1f44a490":"Our dataset contains recordings for **397** different *primary* species, all of them defined by their **eBird code** (the codes that we use as primary label). Just as Xeno-canto is a digital platform that collects audio recordings, eBird (https:\/\/ebird.org) is a citizen science project that collects observations of birds. eBird uses unique species codes to reference birds. You can access additional information on each bird species by combining the base URL \u201chttps:\/\/ebird.org\/species\/\u201d with a species code from the *primary_label* columns of the metadata.\n\nHere are a few examples:\n\nGolden-crowned Kinglet: https:\/\/ebird.org\/species\/gockin  \nRed-winged Blackbird: https:\/\/ebird.org\/species\/rewbla  \nAmerican Goldfinch: https:\/\/ebird.org\/species\/amegfi\n\nLet\u2019s take a look at the number of recordings for each species in the training data:","1f8c5b81":"# Training data (Short audio)\n\nThe training data for this competition consists of a collection of so-called \u201cfocal recordings\u201d. These recordings were made using semi-professional equipment (often using highly directional microphones) and primarily focus on one single species. All recordings were contributed by Xeno-canto (https:\/\/www.xeno-canto.org), one of the largest digital archives for bird sounds. Each recording comes with metadata specifying things like recording date, recording location, and (of course) the bird species that was recorded.\n\nTo get a better understanding of the metadata, let\u2019s look at a few entries.","ae2b88e9":"\u201cNocall\u201d seems to be the most common, which is no surprise: Birds only vocalize occasionally during a recording. Yet, some recordings contain very dense acoustic scenes with multiple birds vocalizing at the same time. Why is \u201cnocall\u201d important? There\u2019s a simple reason: Your classifier should be able to suppress false positives for these segments, which is important for ornithologists when confronted with the detections. One of the core challenges of this competition is to reduce the number of false positives (precision) without losing too many true positives (recall).\n\nIt is up to you if you use training soundscapes for validation (since they represent the hidden test set) or if you use annotated segments for training (to cope with the shift in acoustic domains). But be aware: Training with soundscape data for a few species might introduce unwanted biases when overfitting to one recording site.","d03058c5":"\n\n## 2. Background species\n\nThe metadata for each recording lists the number of audible background species. The data field \u201c*seconday_labels*\u201d contains lists of eBird codes (i.e., primary labels) that recordists annotated. It is important to note that these lists might be incomplete, and you might be able to hear background species, although none are specified in the metadata. Therefore, lists of secondary labels are not very reliable, but they might still be useful for multi-label training (e.g., through loss masking for background species).\n\nLet's look at some values:","4637729c":"# Training data (Soundscapes)\n\nOne of the major obstacles in this competition is the significant gap between training and test recordings. There is a distinct shift in acoustic domains between the two and it can be very challenging to train classifiers that generalize well enough to bridge the gap. Yet, training with target samples (i.e., soundscapes) is often not possible - somebody has to annotate the data for each new deployment, for each new recording location. However, we decided to include some examples of soundscape recordings (i.e., test recordings) that can be used for validation, or even for training. These 20 recordings represent 2 of the 4 test recording locations. Yet, they might not be 100% representative, some species might be missing and only audible in the hidden test set, recording equipment might differ. But they should nonetheless provide a good overview of what to expect in the hidden test data.\n\nLet\u2019s take a look at the label data for this set of recordings.","a8649c3c":"\n\n# Test data\n\nIf you\u2019re already familiar with the training soundscapes, the hidden test set should not be a surprise. It contains 20 soundscape recordings of 10-minute duration for each of the four recording sites. Again, you need to predict audible species for 5-second chunks of the audio data. The submission file needs to contain the ID of the processed audio chunk  (fileID_site_time) and all audible species as a list of space-delimited eBird codes.\n\nLet\u2019s look at one example:\n\nWhen analyzing file \u201c*1234_SSW_20170101.ogg*\u201d (that\u2019s a mock filename), the audio chunk ending at second *00:00:35* of the entire file would have the unique ID \u201c*1234_SSW_35*\u201d. If your classifier thinks that species \u201cbluwa1\u201d and \u201credwa2\u201d (again, mock codes) vocalize during this time, the final submission entry should look like this:\n\n*1234_SSW_35 bluwa1 redwa2*\n\nA submission for this file should include **ALL** segments, starting at 5 seconds. Like this:\n \n*1234_SSW_5 nocall*  \n*1234_SSW_10 bluwa1*  \n*1234_SSW_15 nocall*  \n*1234_SSW_20 bluwa1 redwa2*  \n*1234_SSW_25 nocall*  \n*1234_SSW_30 nocall* \n\nAnd so on...\n\n\nMake yourself familiar with the training and test data, also make sure to check out our other notebooks, let us know if you have any comments and - of course - don\u2019t hesitate to start a forum thread if you have any questions.","ae2173d3":"Let's retrieve all months, when the recording took place, in the **test set**. Together with location, lets check all species that were observed in the right place and in the right time.","114f577d":"## Datetime  \nLet's look at when these recordings were taken.","1465a86a":"## 1. Primary species\n\nMost importantly, the metadata specifies the audible species for each recording. The primary species annotation consists of three data fields: *primary_label, scientific_name, and common_name*[](http:\/\/). All labels have to be considered as \u201cweak labels\u201d since we do know which species is audible in the recording, but we do not know the exact timestamps of the vocalizations. Training with weakly labeled data is one of the core challenges of this competition.\n\nLet\u2019s look at the number of different species.","f2426656":"Majority of the data was registered between 2013 and 2020, and from March to July\n\n0000, 0199, 0201, 0202, 2104 are likley wrong years","f5077e1a":"## 5. Rating\n\nXeno-canto has a rating system for the quality of each recording. Ratings are assigned by users, and we adapted this rating scheme for the training data. In our case, ratings range from 0.5 to 5.0 (the latter being the best possible rating) and reflect the overall quality assigned by users and the number of background species. A value of \u201c0\u201d means that this particular recording does not have a rating, and it is by that the fallback value.\n\nLet's see how rating values are distributed across the training data:","2d96e806":"We can see a few data fields and here\u2019s a brief description for each of them:\n\n* **row_id**: Unique identifier of a 5-second segment of each soundscape file. Use this create the submission file entry.\n\n* **site**: Recording site of the soundscape data. In this competition, we included recordings from 4 different sites (COL = Colombia, COR = Costa Rica, SNE = Sierra Nevada, SSW = Sapsucker Woods). **Make sure to take a look at the \u201ctest_soundscape_metadata\u201d which contains more information on each location**. Training soundscapes only represent two of the four locations (COR and SSW).\n\n* **audio_id**: Identifier used to reference audio recordings. Filenames contain the file ID, recording site and recording date (yyyymmdd).\n\n* **seconds**: End time of the 5-second segment for which this entry states the label. A value of 85 would mean that this particular segment starts at 00:01:20 and lasts until 00:01:25 of the audio file.\n\n* **birds**: primary label (i.e., eBird code) of the audible species of this segment. \u201cnocall\u201d references a segment without any bird vocalization. Segments can have more than one bird, in that case, eBird codes are separated by space. \u201cnocall\u201d can never appear together with other codes.\n\nLet\u2019s look at the most common entries for \u201cbirds\u201d:","10758ef5":"## TODO Correct\/Impute Date if aything is wrong.","d9bc9999":"**Recordings**  \nThe main data is the audio files of the bird recordings. Let's hear the first sample from few of the species.","50cd1d49":"Majority of the data was registered between March and July\n\n00 is months which are not known","5671f649":"As we can see, different species occur over different spatial scales. According to the recording locations, the House Sparrow (houspa) has occurrences around the globe, the Northern Cardinal (norcar) appears to be a typical East coast species of the U.S., the Western Bluebird (wesblu) a West coast species. The Bananaquit (banana) seems to only occur in Central and South America. \n\nLocation data can help us to create subsets of the training data for each of the four test data recording locations (which we will explore later). But be aware: The range of certain species may not be fully reflected by recording location data, and the actual range may differ from what we can see in the data. Yet, recording locations are a good starting point.\n\nPlease note that the training data only contains species that are likely to occur at the recording locations of the test data, even though sometimes the majority of the recordings were made in Europe. If you want to know more about the range of a certain species, please take a look at the associated eBird entry.\n\n","f4ce5d0b":"## TRAINING & TEST RECORDING LOCATIONS  \n\nWe can extract location data from txt located in the test_soundscapes folder.","3e16cd1f":" ## Test recording location with date  \n Description of test site parameters, such as location and date.  \n Explore which birds were at the right place and in the right time to have a chance of making into a test set recording.\n All birds were likely to be observed at the test sites, but were they there at the right time?  \n \n Answer : Each recording site is assigned a circular region of 200 km in radius. If any of the birds were previously \nrecorded in the same month as the test recordings on that site, the bird could potentially be present in test recordings.\n","b1e0c89b":"## Bird (or Species)  \nprimary_label is the target variable that needs to be predicted. Let's look at its distribution. There are two features with the same information: common_name is just a prettier (and complete) version of primary_name. Note that the values from primary_name are used for predictions."}}