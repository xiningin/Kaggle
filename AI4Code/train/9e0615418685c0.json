{"cell_type":{"660c1011":"code","d6e16e95":"code","5a806ff6":"code","6325ab13":"code","3291c9e5":"code","6df70e19":"code","90371d15":"code","06dd32a4":"code","65d3efd8":"code","ce782e6a":"code","d6f69317":"code","7480b52f":"code","10443d6e":"code","3ffe72c6":"code","3e8ae832":"code","0e6fdc41":"code","2cb19cbe":"code","dab4b73c":"code","c606ec4d":"code","371ba758":"code","5dc51841":"code","51d078a6":"code","9cca0a7b":"code","d55b1646":"code","9ff284a4":"code","bc7648bb":"code","cf88689e":"markdown","f68fd1af":"markdown","c538a536":"markdown","f51c909b":"markdown","16e4f2d5":"markdown","509d61c7":"markdown","e1790ba7":"markdown","3dfe7f9f":"markdown","760486a0":"markdown","4d8bc7f6":"markdown","00104674":"markdown","40bec918":"markdown","3a04fed7":"markdown"},"source":{"660c1011":"import pandas as pd\nimport pyarrow.parquet as pq\nimport os\n\nos.listdir('..\/input')","d6e16e95":"subset_train = pq.read_pandas('..\/input\/train.parquet').to_pandas() #, columns=[str(i) for i in range(10)]).to_pandas()","5a806ff6":"subset_train = subset_train.iloc[:400000,:]\nsubset_train.info()","6325ab13":"metadata_train = pd.read_csv('..\/input\/metadata_train.csv')\nmetadata_train.info()","3291c9e5":"metadata_train.head()","6df70e19":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#plt.hist(metadata_train['target'])\nsns.countplot(metadata_train['target'])","90371d15":"fig = plt.figure(figsize=(10,8))\n\nplt.subplot(431)\nplt.plot(subset_train['0'])\nplt.subplot(432)\nplt.boxplot(subset_train['0'])\nplt.subplot(433)\nplt.hist(subset_train['0'])\n    \nplt.subplot(434)\nplt.plot(subset_train['1'])\nplt.subplot(435)\nplt.boxplot(subset_train['1'])\nplt.subplot(436)\nplt.hist(subset_train['1'])\n\nplt.subplot(437)\nplt.plot(subset_train['3'])\nplt.subplot(438)\nplt.boxplot(subset_train['3'])\nplt.subplot(439)\nplt.hist(subset_train['3'])\n\nplt.subplot(4,3,10)\nplt.plot(subset_train['4'])\nplt.subplot(4,3,11)\nplt.boxplot(subset_train['4'])\nplt.subplot(4,3,12)\nplt.hist(subset_train['4'])","06dd32a4":"import numpy as np\n\n# Temporarily reduce data size to build the pipeline\nsmall_subset_train = subset_train.iloc[:25000,:]\nsmall_subset_train = small_subset_train.transpose()\nsmall_subset_train.index = small_subset_train.index.astype(np.int32)\ntrain_dataset = metadata_train.join(small_subset_train, how='right')\n\n# Uncomment the following to train on the full dataset\n#subset_train = subset_train.transpose()\n#subset_train.index = subset_train.index.astype(np.int32)\n#train_dataset = metadata_train.join(subset_train, how='right')","65d3efd8":"positive_samples = train_dataset[train_dataset['target']==1]\npositive_samples = positive_samples.iloc[:,3:]\npositive_samples.info()","ce782e6a":"positive_samples.head()","d6f69317":"plt.figure(figsize=(10,4))\nplt.subplot(151)\nplt.boxplot(positive_samples.iloc[3,1:])\nplt.subplot(152)\nplt.boxplot(positive_samples.iloc[4,1:])\nplt.subplot(153)\nplt.boxplot(positive_samples.iloc[5,1:])\nplt.subplot(154)\nplt.boxplot(positive_samples.iloc[201,1:])\nplt.subplot(155)\nplt.boxplot(positive_samples.iloc[202,1:])","7480b52f":"# Normalize the data set\nfrom sklearn.preprocessing import StandardScaler\ny_train_pos = positive_samples.iloc[:, 0]\nX_train_pos = positive_samples.iloc[:, 1:]\nscaler = StandardScaler()\nscaler.fit(X_train_pos.T)\nX_train_pos = scaler.transform(X_train_pos.T).T","10443d6e":"plt.figure(figsize=(10,4))\nplt.subplot(151)\nplt.boxplot(X_train_pos[0,:])\nplt.subplot(152)\nplt.boxplot(X_train_pos[1,:])\nplt.subplot(153)\nplt.boxplot(X_train_pos[2,:])\nplt.subplot(154)\nplt.boxplot(X_train_pos[3,:])\nplt.subplot(155)\nplt.boxplot(X_train_pos[4,:])","3ffe72c6":"negative_samples = train_dataset[train_dataset['target']==0]\nnegative_samples = negative_samples.iloc[:,3:]\nnegative_samples.info(), negative_samples.head()\n\ny_train_neg = negative_samples.iloc[:, 0]\nX_train_neg = negative_samples.iloc[:, 1:]\nscaler.fit(X_train_neg.T)\nX_train_neg = scaler.transform(X_train_neg.T).T\n\nplt.figure(figsize=(10,4))\nplt.subplot(151)\nplt.boxplot(X_train_neg[0,:])\nplt.subplot(152)\nplt.boxplot(X_train_neg[1,:])\nplt.subplot(153)\nplt.boxplot(X_train_neg[2,:])\nplt.subplot(154)\nplt.boxplot(X_train_neg[3,:])\nplt.subplot(155)\nplt.boxplot(X_train_neg[4,:])","3e8ae832":"from sklearn.model_selection import train_test_split\n\nX_train_pos, X_valid_pos, y_train_pos, y_valid_pos = train_test_split(X_train_pos, y_train_pos, \n                                                                    test_size=0.2,\n                                                                    random_state = 0,\n                                                                    shuffle=True)\n\nX_train_neg, X_valid_neg, y_train_neg, y_valid_neg = train_test_split(X_train_neg, y_train_neg, \n                                                                    test_size=0.2,\n                                                                    random_state = 0,\n                                                                    shuffle=True)","0e6fdc41":"X_train_pos.shape, X_train_neg.shape","2cb19cbe":"# Combine positive and negative samples for training...\ndef combine_positive_and_negative_samples(pos_samples, neg_samples, y_pos, y_neg):\n    X_combined = np.concatenate((pos_samples, neg_samples)) \n                                                    # don't select all negative samples, to\n                                                    # keep the samples balanced\n    y_combined = np.concatenate((y_pos, y_neg))\n    #X_train_combined.shape, y_train_combined.shape\n    combined_samples = np.hstack((X_combined, y_combined.reshape(y_combined.shape[0],1)))\n    np.random.shuffle(combined_samples)\n    return combined_samples\n\n# Only use 500 negative samples, to create a balanced dataset with the positive samples...\ntrain_samples = combine_positive_and_negative_samples(X_train_pos, X_train_neg[:500, :], y_train_pos, y_train_neg[:500])\nX_train = train_samples[:,:-1]\ny_train = train_samples[:,-1]\nX_train.shape, y_train.shape","dab4b73c":"# Create the validation set\n#X_valid_combined = np.concatenate((X_valid_pos, X_valid_neg[:500,:])) # don't select all negative samples, to\n                                                  # keep the samples balanced\n#y_valid_combined = np.concatenate((y_valid_pos, y_valid_neg[:500]))\n#X_valid_combined.shape, y_valid_combined.shape\n#validation_samples = np.hstack((X_valid_combined, y_valid_combined.reshape(y_valid_combined.shape[0],1)))\n#np.random.shuffle(validation_samples)\n\nvalidation_samples = combine_positive_and_negative_samples(X_valid_pos, X_valid_neg[:500,:], y_valid_pos, y_valid_neg[:500])\nX_valid = validation_samples[:,:-1]\ny_valid = validation_samples[:,-1]\nX_valid.shape, y_valid.shape","c606ec4d":"# Reshape training and validation data for keras input layer\nX_train = X_train.reshape(-1, X_train.shape[1], 1)\nX_valid = X_valid.reshape(-1, X_valid.shape[1], 1)\n\nX_train.shape, X_valid.shape","371ba758":"import tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras import Input, layers\nfrom tensorflow.keras import backend as K\n\n#Conv1D Model\ndrop_out_rate = 0.2\nlr = 0.0005\ninput_tensor = Input(shape=([X_train_pos.shape[1],1]))\n\nx = layers.Conv1D(8, 11, padding='valid', activation='relu', strides=1)(input_tensor)\nx = layers.MaxPooling1D(2)(x)\nx = layers.Dropout(drop_out_rate)(x)\nx = layers.Conv1D(16, 7, padding='valid', activation='relu', strides=1)(x)\nx = layers.MaxPooling1D(2)(x)\nx = layers.Dropout(drop_out_rate)(x)\nx = layers.Conv1D(32, 5, padding='valid', activation='relu', strides=1)(x)\nx = layers.MaxPooling1D(2)(x)\nx = layers.Dropout(drop_out_rate)(x)\nx = layers.Conv1D(64, 5, padding='valid', activation='relu', strides=1)(x)\nx = layers.MaxPooling1D(2)(x)\nx = layers.Dropout(drop_out_rate)(x)\nx = layers.Conv1D(128, 3, padding='valid', activation='relu', strides=1)(x)\nx = layers.MaxPooling1D(2)(x)\nx = layers.Flatten()(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(drop_out_rate)(x)\nx = layers.Dense(128, activation='relu')(x)\nx = layers.Dropout(drop_out_rate)(x)\noutput_tensor = layers.Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.Model(input_tensor, output_tensor)\n\nmodel.compile(loss=keras.losses.binary_crossentropy,\n             optimizer=keras.optimizers.Adam(lr = lr),\n             metrics=['accuracy'])\n\nmodel.summary()","5dc51841":"from keras.callbacks import ModelCheckpoint\n\nweights_file=\"best_weights.hdf5\"\ncheckpoint = ModelCheckpoint(weights_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks = [checkpoint]","51d078a6":"batch_size = 100\nhistory = model.fit(X_train, y_train, validation_data=[X_valid, y_valid],\n          batch_size=batch_size, \n          epochs=25,\n          verbose=1, callbacks=callbacks)","9cca0a7b":"# plot history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# plot history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","d55b1646":"os.listdir('.\/')","9ff284a4":"model.load_weights(\"best_weights.hdf5\")","bc7648bb":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\ny_pred = model.predict(X_valid).ravel()\nfpr, tpr, thresholds = roc_curve(y_valid, y_pred)\nauc = auc(fpr, tpr)\n\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='Conv1D (area = {:.3f})'.format(auc))\n#plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()\n# Zoom in view of the upper left corner.\nplt.figure(2)\nplt.xlim(0, 0.5)\nplt.ylim(0.6, 1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='Conv1D (area = {:.3f})'.format(auc))\n#plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve (zoomed in at top left)')\nplt.legend(loc='best')\nplt.show()","cf88689e":"Now let's visualize the positive (faulty) signals using a boxplot for several of them.","f68fd1af":"Let's visualize the boxplots again using this normalized data.","c538a536":"This is a plot of the target values. As expected, a faulty power line is a kind of rare event. Let's visualize some negative and positive (faulty) signals.","f51c909b":"At least from these couple of plots, we notice that the faulty signals (last 2) have relatively more outliers than the non-faulty ones. We will analyze this further with more data.\n\nLet's separate the positive and negative signals for further analysis. I'm going to reduce the sample sizes to make sure we don't run out of memory limits.","16e4f2d5":"A 1-D ConvNet would be an interesting model to try out on this signal. Earlier we saw that there are a lot of outliers in fauty signals. Since the actual signal value differs at different times, the outliers are relative to this mean signal value. A 1-D ConvNet can analyze the signal in various windows of increasing lengths and create high-level features out of that to classify on.","509d61c7":"Read the parquet file. The full length of each signal is 800000. We will halve it to 400000 readings to create the pipeline.","e1790ba7":"This kernel uses 1D convolutions on signals from power lines to identify partial faults","3dfe7f9f":"As we know, the positive samples are fewer, so we will only select a subset of negative samples for training.","760486a0":"We see that the data values differ a lot. Let's normalize the data first, this will also be needed for training some type of models later.","4d8bc7f6":"Now read the metadata file.","00104674":"Again we notice that there are a lot of outliers in the positive (faulty) signals.\n\nNow let's extract the negative (non-faulty) samples and visualize the same boxplots, and see if we can notice any apparent difference.","40bec918":"The negative (non-faulty) signals have much fewer outliers, and their magnitudes also seem to be very low. Seems like the number of outliers could be a promising feature.\n\nNow let's create the test\/train split for training a Conv1D model.","3a04fed7":"Import plotting libraries and create some basic plots."}}