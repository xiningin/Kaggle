{"cell_type":{"69d24be1":"code","10ad5875":"code","6b8d6cee":"code","f59d1833":"code","d28234be":"code","6a704825":"code","91277dea":"code","22c0cf23":"code","19e7311b":"code","90f51311":"code","771ff695":"code","25a32875":"code","6cb10ead":"code","ca7fbdd8":"code","0a0b8d05":"code","c2b90b03":"code","77d132d7":"code","dae7fae6":"code","fb579ba7":"code","aa5b6e78":"code","1a640994":"code","d681d803":"code","69ebee13":"code","2c9bd0eb":"code","778db9a5":"code","b08d71b0":"code","e7d7fe16":"code","c0beefad":"code","2cc9c508":"code","7bba750f":"markdown","e381b769":"markdown","37cd6cb8":"markdown","8d7ee0b4":"markdown","357fca8e":"markdown","0fb8f778":"markdown","3f9465e5":"markdown","1c46de5c":"markdown","9f0cad16":"markdown","3905e6bb":"markdown","8324b7e9":"markdown","51eb2480":"markdown","fa6fb397":"markdown","856a7616":"markdown","cc5ee25e":"markdown","74dc6226":"markdown","9e0cce88":"markdown","6a3461ee":"markdown","15f58e73":"markdown","0f82c744":"markdown","99bb9bc1":"markdown","7895d044":"markdown","c12f3fa5":"markdown","cf938e33":"markdown"},"source":{"69d24be1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nfrom matplotlib.lines import Line2D\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", color_codes=True)","10ad5875":"#load data\ndf = pd.read_csv('\/kaggle\/input\/invehicle-coupon-recommendation\/in-vehicle-coupon-recommendation.csv')\n\ndf.shape #lets check the dimensionality of the raw data","6b8d6cee":"#load the data to understand the attributes and data types\ndf.head()","f59d1833":"#let's look at the data types\ndf.dtypes ","d28234be":"#change temperature into a category as its an ordinal datatype\ndf['temperature']=df['temperature'].astype('category')","6a704825":"#check for empty values\ndf.info()","91277dea":"df[\"car\"].value_counts()","22c0cf23":"df.drop('car', inplace=True, axis=1)","19e7311b":"for x in df.columns[df.isna().any()]:\n    df = df.fillna({x: df[x].value_counts().idxmax()})","90f51311":"#change Object datatypes to Categorical datatypes)\n\ndf_obj = df.select_dtypes(include=['object']).copy()\n\nfor col in df_obj.columns:\n    df[col]=df[col].astype('category')\n    \ndf.dtypes","771ff695":"#lets do some statistcal analysis\ndf.describe(include='all')","25a32875":"df.select_dtypes('int64').nunique()","6cb10ead":"df.drop(columns=['toCoupon_GEQ5min'], inplace=True)","ca7fbdd8":"fig, axes = plt.subplots(9, 2, figsize=(20,50))\naxes = axes.flatten()\n\nfor ax, col in zip(axes, df.select_dtypes('category').columns):\n    sns.countplot(y=col, data=df, ax=ax, \n                  palette=\"ch:.25\", order=df[col].value_counts().index);\n\nplt.tight_layout()\nplt.show()","0a0b8d05":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nenc = OneHotEncoder(dtype='int64')\n\ndf_cat = df.select_dtypes(include=['category']).copy()\ndf_int = df.select_dtypes(include=['int64']).copy()\n\ndf_enc = pd.DataFrame()\nfor col in df_cat.columns:\n    enc_results = enc.fit_transform(df_cat[[col]])\n    df0 = pd.DataFrame(enc_results.toarray(), columns=enc.categories_)\n    df_enc = pd.concat([df_enc,df0], axis=1)\n    \ndf_final = pd.concat([df_enc, df_int], axis=1)\n\n#source: https:\/\/pbpython.com\/categorical-encoding.html","c2b90b03":"df_final","77d132d7":"import sklearn as sk\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","dae7fae6":"#split data into training and test set\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(df_final, test_size=.3, random_state=42, shuffle=True, stratify=df_final['Y'])","fb579ba7":"#Creating the Dependent Feature Matrix\nX_train = train_set.iloc[:, :-1].values\nX_test = test_set.iloc[:, :-1].values\n\n#Creating the Independent Vector\ny_train = train_set.iloc[:, -1].values\ny_test = test_set.iloc[:, -1].values","aa5b6e78":"LR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred_LR = LR.predict(X_test)\n\nprint(classification_report(y_test, y_pred_LR))","1a640994":"DTC = DecisionTreeClassifier().fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred_DTC = DTC.predict(X_test)\n\nprint(classification_report(y_test, y_pred_DTC))","d681d803":"KNN = KNeighborsClassifier().fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred_KNN = KNN.predict(X_test)\n\nprint(classification_report(y_test, y_pred_KNN))","69ebee13":"LDA = LinearDiscriminantAnalysis().fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred_LDA = LDA.predict(X_test)\n\nprint(classification_report(y_test, y_pred_LDA))","2c9bd0eb":"GNB = GaussianNB().fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred_GNB = GNB.predict(X_test)\n\nprint(classification_report(y_test, y_pred_GNB))","778db9a5":"SVM = SVC(kernel=\"rbf\", random_state=None, probability=True, cache_size=500, gamma=0.1).fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred_SVM = SVM.predict(X_test)\n\nprint(classification_report(y_test, y_pred_SVM))","b08d71b0":"from sklearn.metrics import confusion_matrix\n\ncm_SVM = confusion_matrix(y_test,y_pred_SVM)\npd.crosstab(y_test, y_pred_SVM, rownames = ['Truth'], colnames =['Predicted'], margins = True)","e7d7fe16":"def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('Truth')\n    plt.xlabel('Prediction')\n\nprint_confusion_matrix(cm_SVM,[\"Coupon Not Accepted\", \"Coupon Accepted\"])","c0beefad":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\n\ny_pred_proba = SVM.predict_proba(X_test)[:,1]\nroc_auc = metrics.roc_auc_score(y_test, y_pred_proba)\n#print(f'Receiver Operating Characteristic AUC = {roc_auc}')\n\nplt.figure(figsize = (10,8))\n# plot no skill roc curve\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n# calculate roc curve for model\nfpr, tpr, thresholds_ = roc_curve(y_test, y_pred_proba)\n# plot model roc curve\nplt.plot(fpr, tpr, marker='.', label='AUC = '+str(roc_auc))\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Curve')\n# show the legend\nplt.legend(loc=4)\n# show the plot\nplt.show()","2cc9c508":"precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n\nplt.figure(figsize = (10,8))\nplt.plot([0, 1], [0.5, 0.5],'k--', label = 'No Skill')\nprc_auc = auc(recall, precision)\nplt.plot(recall, precision, label = 'Precision-Recall AUC = '+str(prc_auc))\nplt.legend(loc=1)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.show()","7bba750f":"I just finished my Hon degree in IT with York University and this dataset it what I chose for project and decided to share the experience I had with you, so I am certainly open to new ideas and learning opportunities. \n\nInformation Wo Wiase!","e381b769":"## ROC Curves(Receiver Operating Characteristic Curve)","37cd6cb8":"Let's plot the distribution charts of all the categorical datatypes.","8d7ee0b4":"For machine learning models, achieving a good prediction model is extremely crucial. It involves achieving a balance between underfitting and overfitting or ie. a trade-off between bias and variance.\n<p>\nWhen it comes to classification, the precision-recall trade-off is a fundamentally important metric to investigate.\n<\/p>\n<p>\nPrecision is the ratio between the True Positive and all Positives. For this problem it would be the measure of drivers that were correctly identified using a coupon out of all the drivers actually using it.\n<\/p>\n<p>\nRecall is the measure of our model correctly identifying True Positives. Thus all the drivers who actually accepted a coupon, recall tells how many we correctly identified as accepting a coupon.\n<\/p>","357fca8e":"A confusion matrix helps us gain an insight into how correct our predictions were and how they hold up against the actual values.","0fb8f778":"Empty values in categorical data can be removed or replaced with the most frequent value in each column.","3f9465e5":"SVM has the highest accuracy amongst the other models. I chose to use the RBF kernel as we do not know if the data is linear separable or not.  ","1c46de5c":"# Data Exploration\nLet's take a peek into the data and explore the data and its variables. The dataset is a supervised learning dataset with over 12000 instances and 26 attributes; this mean there is an input variable X and an out variable y.  ","9f0cad16":"Lets iterate through the pandas table and get all the columns with empty or NaN values, and then for each column the code is going to find the largest variable count and fill the empty values with the corresponding variable with maximum count.","3905e6bb":"It seems that the data has some few numberical datatypes and the rest are string objects, however all the data can be categorized as being categorical datatypes with a mix of binary and ordinal datatypes.","8324b7e9":"# Data Modeling","51eb2480":"## Decision Tree","fa6fb397":"<ul>\n<li>At the lowest point (0,0), the threshold is set to 1 which means the model make no distinctions between drivers who use coupons and drivers who do not use coupons.\n<li>The highest point (1,1), the threshold is set at 0 which means that both precision and recall are high and the model makes distinctions perfectly. \n<li>The rest of the curve is the values of Precision and Recall for the threshold values between 0 and 1. Our aim is to make the curve as close to (1, 1) as possible- meaning a good precision and recall.\n<li>Similar to ROC, the area with the curve and the axes as the boundaries is the Area Under Curve(AUC). Consider this area as a metric of a good model. The AUC ranges from 0 to 1.Therefore, we should aim for a high value of AUC.\n<\/ul>","856a7616":"From our train and test data, we already know that our test data consisted of 3806 data points. That is the 3rd row and 3rd column value at the end. We also notice that there are some actual and predicted values. The actual values are the number of data points that were originally categorized into 0 or 1. The predicted values are the number of data points SVM model predicted as 0 or 1.<br>\n\nThe actual values are:\n<ul>\n    <li>The drivers who actually did not use a coupon = 1643<\/li>\n    <li>The drivers who actually did use a coupon = 2163<\/li>\n<\/ul><br>\nThe predicted values are:\n<ul>\n    <li>Number of drivers who were predicted as not using a coupon = 1401\n    <li>Number of drivers who were predicted as using a coupon = 2396\n<\/ul>\n\nAll the values we obtain above have a term. Let\u2019s go over them one by one:\n<ol>\n    <li>The cases in which the drivers actually did not use a coupon and our model also predicted as not using one is called the <b>True Negatives<\/b>. For our matrix, True Negatives = 1056.\n    <li>The cases in which the drivers actually used a coupon and our model also predicted as using one are called the <b>True Positives<\/b>. For our matrix, True Positives = 1809\n    <li>However, there are are some cases where the drivers actually did not use a coupon, but our model has predicted that they did use one. This kind of error is the <b>Type I Error<\/b> and we call the values as <b>False Positives<\/b>. For our matrix, False Positives = 587\n    <li>Similarly, there are are some cases where the drivers actually used a coupon, but our model has predicted that they did not use one. This kind of error is the <b>Type II Error<\/b> and we call the values as <b>False Negatives<\/b>.  For our matrix, False Negatives = 354\n<\/ol>","cc5ee25e":"From the decription above we can tell that 'toCoupon_GEQ5min' has only one unique variable which won't help much in the encoding of the categorical variables. Therefore, its better to drop that column. ","74dc6226":"## Gaussian Naive Bayes","9e0cce88":"# Cleaning The Data","6a3461ee":"We are going to create feature vectors for our modeling by using the LabelEnconder and OneHotEncoder.","15f58e73":"## Logistic Regression","0f82c744":"## Support Vector Machine","99bb9bc1":"## Linear Discriminant Analysis","7895d044":"There are some missing values in several columns, and the 'car' variable has only 108 non-null values, more than 99% of the values are NaN. We can just drop it off. These variables are insufficient so its best to remove it completely from the data to avoid inaccuracies in the modeling.","c12f3fa5":"## K-Nearest Neighbors","cf938e33":"## Precision-Recall Curve (PRC)"}}