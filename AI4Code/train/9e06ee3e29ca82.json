{"cell_type":{"03226fe7":"code","61188d2f":"code","cf40889c":"code","554eb347":"code","3bc349e2":"code","4574cef6":"code","936dc7ab":"code","b6b34336":"code","7ea61d27":"markdown","a76e7bf3":"markdown","7baf16fa":"markdown","3281d60b":"markdown","49a420c2":"markdown","aab3f5e2":"markdown","4dc81b5a":"markdown","c0095171":"markdown","cf9c2a8f":"markdown"},"source":{"03226fe7":"# Set up code checking\nimport os\nif not os.path.exists(\"..\/input\/train.csv\"):\n    os.symlink(\"..\/input\/home-data-for-ml-course\/train.csv\", \"..\/input\/train.csv\")  \n    os.symlink(\"..\/input\/home-data-for-ml-course\/test.csv\", \"..\/input\/test.csv\") \nfrom learntools.core import binder\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nbinder.bind(globals())\nfrom learntools.ml_intermediate.ex6 import *\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nprint(\"Setup Complete\")","61188d2f":"\n\n# Read the data\nX = pd.read_csv('..\/input\/train.csv', index_col='Id')\n\nX_test_full = pd.read_csv('..\/input\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice              \nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n#print(len(low_cardinality_cols))\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n#print(len(numeric_cols))\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","cf40889c":"plt.figure(figsize=(12,6))\n\nsns.kdeplot(data=y, shade=True)\n\nprint(\"The Average House price in melbourn is: 200000 dollars.\")","554eb347":"from xgboost import XGBRegressor\n\n# Define the model\nmy_model_1 = XGBRegressor()\n\n# Fit the model\nmy_model_1.fit(X_train, y_train)\n\n\n# Get predictions\npredictions_1 = my_model_1.predict(X_valid)\n\n#print(len(X_train.columns))\n","3bc349e2":"# Calculate MAE\nmae_1 = mean_absolute_error(y_valid, predictions_1)\n\nprint(\"Mean Absolute Error:\" , mae_1)\n\n","4574cef6":"# Define the model\nmy_model_2 = XGBRegressor(n_estimators = 5000, learning_rate = 0.05) # Your code here\n\n# Fit the model\nmy_model_2.fit(X_train, y_train,early_stopping_rounds=5,eval_set=[(X_valid, y_valid)],\n             verbose=False) # Your code here\n\n# Get predictions\npredictions_2 = my_model_2.predict(X_valid)# Your code here\n\n# Calculate MAE\nmae_2 = mean_absolute_error(y_valid, predictions_2)# Your code here\n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\" , mae_2)\n","936dc7ab":"model = keras.Sequential([\n    layers.Dense(512, activation='relu', input_shape= (227,)),\n    layers.Dense(1024, activation='relu'),    \n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1024, activation='relu'),\n\n    layers.Dense(1, activation = 'linear'),\n])\n\nmodel.compile(optimizer='adam', loss = 'mae', metrics=['mean_squared_error'])\n\nhistory = model.fit(X_train, y_train, epochs = 10, batch_size = 128)\n\n","b6b34336":"model = keras.Sequential([\n    layers.Dense(512, activation='relu', input_shape= (227,)),\n    layers.Dense(1024, activation='relu'),    \n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1024, activation='relu'),\n\n    layers.Dense(1, activation = 'linear'),\n])\n\nmodel.compile(optimizer='adam', loss = 'mae', metrics=['mean_squared_error'])\n\nhistory = model.fit(X_train, y_train, epochs = 30, batch_size = 128)","7ea61d27":"# Pre-Processing the Data (Melbourn Housing).","a76e7bf3":"# Let's Check the accuracy of our Model.","7baf16fa":"# The mean squared error is 35660083200 which is very huge as compared to Our previous model.\n\n# Lets Train this model a bit longer to reduce the error","3281d60b":"# Tuning the Hyperparameters","49a420c2":"# So, Training the Model for a longer period doesn't making that much differnce. \n\n# Here XGBoost is clearly outperforming DNN.\n\n**If you can furthur reduce the mse of the DNN. Then Please do it. It will help the community!**","aab3f5e2":"# Let's Visualize the Data.","4dc81b5a":"# The XGBoost","c0095171":"# Set UP\n","cf9c2a8f":"**So the error of our XGBoost Model in close to 16802 dollars.**\n\n**Let's check our other Model.**\n\n# The Deep Neural Network."}}