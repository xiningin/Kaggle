{"cell_type":{"3ebc2519":"code","55e03e9a":"code","769841b9":"code","740908de":"code","a15258de":"code","545cc71e":"code","420919f0":"code","6004473d":"code","10c41b86":"code","976ecdf2":"code","0247a523":"code","6cc3a27e":"code","bacc182a":"code","0c84d8d9":"code","3368fe72":"code","44e45579":"code","f8002ccf":"code","8ef28c70":"code","ad8aace2":"code","03e74238":"code","864cfeb6":"code","060eb991":"code","dafed8bd":"code","aaadb87f":"code","f4d3245e":"code","86889f57":"code","a69f30cb":"code","11d50d75":"markdown","f7f8f6e5":"markdown","cea6aa7c":"markdown","31aca7a2":"markdown","29f9fafe":"markdown","76bac751":"markdown","aa63f372":"markdown","a6541e92":"markdown","8703b5fc":"markdown","e50f0357":"markdown","9976fb9d":"markdown","34ba7a5c":"markdown","2cc38b2b":"markdown","95f7b853":"markdown"},"source":{"3ebc2519":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import regularizers\nimport operator\nimport math\nfrom functools import reduce\nfrom sklearn.model_selection import train_test_split\nimport os\nimport json\n","55e03e9a":"df = pd.read_csv('..\/input\/reviews\/5556-ar-reviews.csv')\ndf.head()","769841b9":"df['label'].value_counts(normalize=True)","740908de":"df = df.sample(frac = 1) ","a15258de":"stopwords = {'\u0641\u0625\u0630\u0627', '\u0623\u0646\u0649', '\u0628\u0645\u0646', '\u062d\u062a\u0649', '\u0644\u0645', '\u0623\u0646\u062a\u0645\u0627', '\u0647\u0646\u0627\u0643', '\u062a\u064a\u0646\u0643', '\u0628\u0644', '\u0625\u064a', '\u0639\u0646', '\u0648\u0644\u0643\u0646', '\u0648\u0625\u0630\u0627', '\u062f\u0648\u0646', '\u0625\u0646\u0627', '\u0625\u0630\u0646', '\u0628\u0643\u0645', '\u062d\u064a\u0646', '\u0639\u0646\u062f', '\u0647\u0644', '\u0625\u0644\u0627', '\u0647\u0627\u062a\u0647', '\u0630\u064a\u0646\u0643', '\u0627\u0644\u0644\u0648\u0627\u062a\u064a', '\u0643\u0630\u0627', '\u0644\u0633\u062a\u0645\u0627', '\u0647\u064a', '\u0627\u0644\u0644\u062a\u0627\u0646', '\u0623\u0643\u062b\u0631', '\u0643\u0644\u062a\u0627', '\u0644\u0643\u0646', '\u0644\u064a\u0633\u062a\u0627', '\u0647\u0643\u0630\u0627', '\u0639\u0633\u0649', '\u0625\u0630', '\u0625\u0646', '\u0627\u0644\u0644\u0627\u062a\u064a', '\u0625\u0630\u0627', '\u0628\u0647\u0645', '\u0646\u062d\u0646', '\u0641\u064a\u0645\u0627', '\u0630\u0627\u0643', '\u0628\u0643\u0646', '\u0628\u064a\u062f', '\u0644\u0647\u0646', '\u0647\u0630\u064a', '\u0643\u0623\u064a', '\u0630\u0648\u0627', '\u0623\u064a', '\u0643\u0644\u0627\u0647\u0645\u0627', '\u0647\u0630\u064a\u0646', '\u0623\u064a\u0646\u0645\u0627', '\u0643\u064a', '\u0625\u0644\u064a\u0643\u0646', '\u0645\u0627\u0630\u0627', '\u0647\u064a\u0627', '\u0647\u0646\u0627\u0644\u0643', '\u0628\u064a', '\u0628\u0645\u0627', '\u062a\u0644\u0643\u0645\u0627', '\u0628\u0639\u0636', '\u0628\u0647\u0646', '\u062a\u064a\u0646', '\u0631\u064a\u062b', '\u0639\u0644\u0649', '\u063a\u064a\u0631', '\u062d\u064a\u062b\u0645\u0627', '\u0643\u0623\u0646', '\u0628\u062e', '\u0647\u0627\u062a\u0627\u0646', '\u0647\u0627\u0647\u0646\u0627', '\u0645\u0627', '\u0647\u064a\u0647\u0627\u062a', '\u0644\u062f\u0649', '\u0634\u062a\u0627\u0646', '\u0644\u0633\u0646\u0627', '\u0643\u064a\u0641\u0645\u0627', '\u0645\u0639', '\u0645\u0645\u0646', '\u0643\u0645\u0627', '\u0625\u0646\u0645\u0627', '\u064a\u0627', '\u0639\u0644\u064a\u0647', '\u0644\u0643', '\u0630\u0647', '\u0630\u0627\u0646', '\u0644\u0647\u0645\u0627', '\u0644\u064a\u0633\u062a', '\u0644\u0646\u0627', '\u0645\u0647', '\u0623\u0646\u062a\u0646', '\u0641\u064a', '\u0644\u0648\u0644\u0627', '\u0628\u0633', '\u0644\u0647\u0627', '\u0623\u0642\u0644', '\u0639\u0644\u064a\u0643', '\u0641\u0644\u0627', '\u0645\u0647\u0645\u0627', '\u0644\u064a\u0633\u0627', '\u0630\u064a\u0646', '\u0630\u0627\u062a', '\u0643\u0644\u0645\u0627', '\u0630\u0627', '\u0630\u0648', '\u0641\u064a\u0647', '\u062a\u064a', '\u0647\u0646\u0627', '\u0647\u0627\u062a\u064a\u0646', '\u0647\u0627', '\u0647\u0645', '\u0623\u0644\u0627', '\u0644\u0627', '\u0633\u0648\u0649', '\u0648\u0625\u0630', '\u0643\u0645', '\u0644\u0633\u062a', '\u062d\u064a\u062b', '\u0625\u0644\u064a\u0643\u0645\u0627', '\u0644\u0648\u0645\u0627', '\u0627\u0644\u0630\u064a\u0646', '\u0643\u0644\u0627', '\u0627\u0644\u062a\u064a', '\u0643\u0623\u064a\u0646', '\u0630\u0648\u0627\u062a\u064a', '\u0644\u0633\u062a\u0645', '\u0647\u0630\u0627', '\u0641\u0645\u0646', '\u0630\u0644\u0643\u0645', '\u0648\u0645\u0627', '\u0643\u064a\u0641', '\u0644\u0643\u0645', '\u062d\u0627\u0634\u0627', '\u0628\u0643', '\u0648\u0627\u0644\u0630\u064a', '\u0623\u0646', '\u0644\u0647\u0645', '\u0644\u0633\u0646', '\u062b\u0645\u0629', '\u0630\u064a', '\u0648\u0625\u0646', '\u0648\u0645\u0646', '\u0623\u064a\u0647\u0627', '\u0644\u0647', '\u0645\u062a\u0649', '\u0628\u0644\u0649', '\u0627\u0644\u0644\u062a\u064a\u0646', '\u0644\u0633\u062a\u0646', '\u0628\u0643\u0645\u0627', '\u0642\u062f', '\u0643\u0644\u064a\u0643\u0645\u0627', '\u0644\u0643\u0645\u0627', '\u0647\u0644\u0627', '\u0622\u064a', '\u0644\u0643\u0646\u0645\u0627', '\u0627\u0644\u0644\u0630\u064a\u0646', '\u0627\u0644\u0644\u0627\u0626\u064a', '\u0630\u0644\u0643\u0646', '\u0644\u0627\u0633\u064a\u0645\u0627', '\u0630\u0644\u0643', '\u0645\u0630', '\u0627\u0644\u0644\u062a\u064a\u0627', '\u0647\u0645\u0627', '\u0625\u0644\u064a\u0643', '\u0633\u0648\u0641', '\u0645\u0646\u0647\u0627', '\u0648\u0627\u0644\u0630\u064a\u0646', '\u0623\u0646\u062a\u0645', '\u0647\u0627\u062a\u064a', '\u0644\u0643\u064a', '\u0627\u0644\u0644\u0630\u0627\u0646', '\u0630\u0648\u0627\u062a\u0627', '\u0639\u0645\u0627', '\u0641\u064a\u0647\u0627', '\u0625\u0644\u0649', '\u062a\u0644\u0643', '\u0643\u0644', '\u0644\u064a', '\u0647\u0648', '\u0641\u064a\u0645', '\u0625\u0644\u064a\u0643\u0645', '\u0628\u0647\u0627', '\u0630\u0627\u0646\u0643', '\u0625\u0646\u0647', '\u0647\u0624\u0644\u0627\u0621', '\u0623\u0648\u0644\u0626\u0643', '\u0625\u0630\u0645\u0627', '\u0628\u0646\u0627', '\u0645\u0646', '\u062e\u0644\u0627', '\u0644\u064a\u0633\u0648\u0627', '\u062b\u0645', '\u0644\u0639\u0644', '\u0648\u0647\u0648', '\u0646\u062d\u0648', '\u0623\u064a\u0646', '\u0644\u0626\u0646', '\u0639\u062f\u0627', '\u0622\u0647', '\u0643\u0623\u0646\u0645\u0627', '\u0643\u0644\u064a\u0647\u0645\u0627', '\u0627\u0644\u0630\u064a', '\u0644\u0646', '\u0646\u0639\u0645', '\u0647\u0630\u0647', '\u0628\u0647\u0645\u0627', '\u0644\u064a\u062a', '\u062a\u0644\u0643\u0645', '\u0623\u0645\u0627', '\u0645\u0646\u0630', '\u0623\u0648', '\u0647\u0627\u0643', '\u0628\u0645\u0627\u0630\u0627', '\u0643\u0630\u0644\u0643', '\u0623\u0646\u0627', '\u0622\u0647\u0627', '\u0641\u0625\u0646', '\u0639\u0644', '\u0645\u0646\u0647', '\u0647\u064a\u062a', '\u0623\u0641', '\u0623\u0645', '\u0625\u064a\u0647', '\u0643\u064a\u062a', '\u062a\u0647', '\u0644\u0643\u064a\u0644\u0627', '\u0644\u064a\u0633', '\u0645\u0645\u0627', '\u0647\u0630\u0627\u0646', '\u0623\u0646\u062a', '\u062d\u0628\u0630\u0627', '\u0648\u0644\u0648', '\u0623\u0648\u0647', '\u0625\u0645\u0627', '\u0644\u0648', '\u0628\u064a\u0646', '\u0628\u0647', '\u0648\u0644\u0627', '\u0644\u0645\u0627', '\u0628\u0639\u062f', '\u0647\u0646', '\u0630\u0644\u0643\u0645\u0627', '\u0623\u0648\u0644\u0627\u0621','\u0648'}\n\nmaxDictionaryLength = 8000\n\ndef tokenize(sentence, isCreateDict=False):\n    tmpTokens = sentence.lower().split()\n    tokens = [token for token in tmpTokens if ((token not in stopwords) and (len(token)> 0)) ]\n     \n    if isCreateDict:\n        for token in tokens:\n            if token in dictionary_dict:\n                dictionary_dict[token] += 1\n            else:\n                dictionary_dict[token] = 1\n    documentTokens.append(tokens)\n    return tokens\n\n\ndef getInverseDocumentFrequency(documentTokens, dictionary):\n    return list(map(lambda word : 1 + math.log(len(documentTokens) \/ reduce(lambda acc,curr: (1 if (word in curr) else 0) + acc, documentTokens,0)),dictionary))\n\n\n  \ndef encoder(sentence, dictionary, idfs):\n    tokens = tokenize(sentence)\n    tfs = getTermFrequency(tokens, dictionary)\n    tfidfs = getTfIdf(tfs,idfs)\n    return tfidfs\n\n\ndef getTermFrequency(tokens, dictionary):\n    return  list(map(lambda token: reduce(lambda acc,curr : (acc + 1 if (curr == token) else acc), tokens,0), dictionary))\n\n\n\ndef getTfIdf(tfs, idfs):\n    return [tf * idf for (tf,idf) in zip(tfs,idfs)]\n\n","545cc71e":"dictionary_dict = {}\ndocumentTokens = []\ntestComments = ['\u0644\u0644\u0631\u0627\u062d\u0629 \u0639\u0646\u0648\u0627\u0646 . \u0643\u0644 \u0634\u064a. \u0644\u0627 \u0634\u064a', '\u0634\u064a\u0621 \u062c\u0645\u064a\u0644']\n\nfor comment in testComments:\n    documentTokens.append(tokenize(comment,True))\n\n\ndictionary = sorted(dictionary_dict, key=dictionary_dict.get, reverse=True)\nidfs = getInverseDocumentFrequency(documentTokens, dictionary);\n\ntfidfs = []\n\nfor comment in testComments:\n    tfidfs.append(encoder(comment, dictionary, idfs))\n\nprint(dictionary_dict)\nprint(dictionary)\nprint(idfs)\nprint(tfidfs)","420919f0":"dictionary_dict = {}\ndocumentTokens = []\ndf['tokens'] = df['text'].apply(lambda x : tokenize(x, True))","6004473d":"df.head()","10c41b86":"dictionary = sorted(dictionary_dict, key=dictionary_dict.get, reverse=True)\ndictionary = dictionary[:maxDictionaryLength]\nprint('Length of dictionary : {0}'.format(len(dictionary)))\nprint(dictionary[:10])","976ecdf2":"idfs = getInverseDocumentFrequency(documentTokens, dictionary)\nlen(idfs)","0247a523":"df['features'] = df['text'].apply(lambda x : encoder(x,dictionary, idfs))\ndf['features'].head()","6cc3a27e":"df_new = df['features'].apply(lambda x : pd.Series(x))\ndf_new['label'] = df['label']","bacc182a":"train, test = train_test_split(df_new, test_size=0.2)\ntrain, val = train_test_split(train, test_size=0.1)\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')","0c84d8d9":"print(train.shape, test.shape, val.shape)","3368fe72":"def df_to_dataset(dataframe, shuffle=True, batch_size=16):\n    dataframe = dataframe.copy()\n    labels = dataframe.pop('label')\n    ds = tf.data.Dataset.from_tensor_slices((dataframe.values, labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    return ds","44e45579":"batch_size = 100\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)","f8002ccf":"numOfFeatures = len(dictionary)","8ef28c70":"def get_build_model():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Dense(15, activation='relu', input_shape=(numOfFeatures,)),\n    tf.keras.layers.Dropout(0.3),    \n    tf.keras.layers.Dense(15, activation='relu'),  \n    tf.keras.layers.Dropout(0.3),   \n    tf.keras.layers.Dense(1,activation='sigmoid')\n  ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                metrics=['accuracy'])\n    return model","ad8aace2":"model = get_build_model()\nmodel.summary()\nmodel.fit(train_ds,epochs=20 ,validation_data=val_ds)","03e74238":"model.evaluate(test_ds)","864cfeb6":"## make predictions\ntestComments = ['\u0635\u0628\u0627\u062d \u0627\u0644\u062e\u064a\u0631\u0627\u062a', '\u0644\u0644\u0631\u0627\u062d\u0629 \u0639\u0646\u0648\u0627\u0646 . \u0643\u0644 \u0634\u064a. \u0644\u0627 \u0634\u064a']\ntfidfs = []\nfor comment in testComments:\n    tfidfs.append(encoder(comment, dictionary, idfs))\nprint(f'predicted probabliities : {model.predict(tfidfs)}')\nprint(f'predicted classes : {tf.round(model.predict(tfidfs))}')","060eb991":"model.save('ar_reviews.h5')","dafed8bd":"# write dictionary and IDFs\n\nwith open('dictionary.json', 'w', encoding='utf-8') as outfile:\n    json.dump(dictionary, outfile,  ensure_ascii=False, indent=4)\n\nwith open('idfs.json', 'w', encoding='utf-8') as outfile:\n    json.dump(idfs, outfile, ensure_ascii=False, indent=4)","aaadb87f":"from sklearn.ensemble import RandomForestClassifier","f4d3245e":"rf_model = RandomForestClassifier(n_jobs=-1,n_estimators=180)\nrf_model.fit(train.loc[:, train.columns != 'label'], train['label'])","86889f57":"rf_model.score(train.loc[:, train.columns != 'label'], train['label'])","a69f30cb":"rf_model.score(test.loc[:, test.columns != 'label'],test['label'])","11d50d75":"# sklearn.ensemble import RandomForestClassifier","f7f8f6e5":"    Generating features from Text by: Vector space model (VSM)\n    Including Inverse Document frequency (IDF)","cea6aa7c":"# Build Model","31aca7a2":"**Sample Test** Code used in the slides ( Module : preparing data for machine learning model )","29f9fafe":"# Train Test Split","76bac751":"# Evaluate Model","aa63f372":"# Testing","a6541e92":"# Make Predictions","8703b5fc":"first approach has some limitations:\n1. dictionary size (dimensions) can became huge\n2. context not preserved","e50f0357":"# Export Model","9976fb9d":"the solution: using Universal Sentence Encoder (USE). it will be the second approach ","34ba7a5c":"Text -> Numaric representation -> Model\n# First Approach using Vector Space Model VSM\nin two steps:\n* document vector and remove stopword\n* convert document vector to numbers by:\n1. Term Frequency TF\n2. Invers Document Frequency IDF\n\nIDF=Log(N\/DF)+1 Number of documents\n","2cc38b2b":"# Arabic Sentiment using Tensorflow","95f7b853":"# Process Data"}}