{"cell_type":{"399a90fd":"code","fbd8ff38":"code","b529ff81":"code","2570a94d":"code","39aeeacc":"code","79795086":"code","0be9fbe8":"code","a895c152":"code","d25990ea":"code","073466df":"code","efd9cb10":"code","73119e5a":"code","49dbffc9":"code","5df67057":"code","2d0f2261":"code","0214ce8e":"code","69c0c028":"code","8d7d023f":"code","e2496995":"code","e19341d9":"code","356b9042":"code","215e4fb5":"code","ff73e822":"code","2f439cee":"code","796273ec":"code","3cac476f":"code","ba10ddb9":"code","e7698193":"code","1ac5bfb4":"code","0d4611fc":"code","d5326f1c":"code","e613f498":"code","8aabc569":"code","2dc6fcd7":"code","0aacfe87":"code","ab8eb2d4":"code","d71712b4":"code","dd9ed74d":"code","271d9fb6":"markdown","8f1095e4":"markdown","830d967f":"markdown","fa7416af":"markdown","72dd1291":"markdown","df739024":"markdown","cbadc981":"markdown","828bbc7d":"markdown","33f14cd9":"markdown","aed862a1":"markdown"},"source":{"399a90fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/participants_data_final\/Participants_Data_Final\"))\n\n# Any results you write to the current directory are saved as output.","fbd8ff38":"train=pd.read_excel('..\/input\/participants_data_final\/Participants_Data_Final\/Data_Train.xlsx')\ntest=pd.read_excel('..\/input\/participants_data_final\/Participants_Data_Final\/Data_Test.xlsx')\ns=pd.read_excel('..\/input\/participants_data_final\/Participants_Data_Final\/Sample_submission.xlsx')\ntrain.head()","b529ff81":"train.describe(include='all').T","2570a94d":"train[train['RESTAURANT_ID']==6571]","39aeeacc":"train[train.duplicated()==True]","79795086":"train[train['RESTAURANT_ID'].isin(test['RESTAURANT_ID'])]","0be9fbe8":"test.describe(include='all').T","a895c152":"train[train['RATING']=='NEW']['TITLE'].value_counts()","d25990ea":"train[train['TITLE']=='QUICK BITES']['COST'].describe()","073466df":"train[train['VOTES'].isnull()==True].describe(include='all')","efd9cb10":"train[train['CITY']=='Kochi']['RATING'].value_counts()","73119e5a":"train.groupby('TITLE')['COST'].describe().sort_values('mean',ascending=False)","49dbffc9":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(12,10))\nsns.boxplot(train['COST'])","5df67057":"train.iloc[246,4]='Kochi'\ntrain.iloc[817,4]='Mumbai'\ntrain.iloc[5149,4]='Navi Mumbai'\ntrain.iloc[5297,4]='Kochi'\ntrain.iloc[6397,4]='Mumbai'\ntrain.iloc[6451,4]='Chennai'\ntrain.iloc[8456,4]='Bangalore'\ntrain.iloc[8735,4]='Chennai'\ntrain.iloc[9121,4]='Bangalore'\ntrain.iloc[9268,4]='Kochi'\ntrain.iloc[10200,4]='Mumbai'\ntrain.iloc[10704,4]='Chennai'\ntrain.iloc[11491,4]='Chennai'\ntrain.iloc[12314,4]='Hyderabad'\ntrain.iloc[12447,4]='Hyderabad'\n\ntest.iloc[169,4]='Chennai'\ntest.iloc[710,4]='Chennai'\ntest.iloc[1354,4]='Uttar Pradesh'\ntest.iloc[3621,4]='Mumbai'\ntest.iloc[4167,4]='Chennai'\n\ntrain.iloc[1365,0]='Kerela'\ntrain=train[train.duplicated()==False]\ndf=train.append(test,ignore_index=True)\ndf.head()","2d0f2261":"# Feature Engineering\ntrain['COST'].describe()","0214ce8e":"train.isnull().sum()","69c0c028":"train['RATING'].unique()","8d7d023f":"train.isnull().sum()","e2496995":"import re\ndef timeclean(v):\n    return re.sub(\"[^a-zA-Z0-9:]\", \" \",v)\ndf['TITLE']=df['TITLE'].apply( lambda x :\" \".join(x.split(\",\")))\ndf['CUISINES']=df['CUISINES'].apply( lambda x :\" \".join(x.split(\",\")))\ndf['CITY'].fillna('NOTFOUND',inplace=True)\ndf['LOCALITY'].fillna('NOTFOUND',inplace=True)\ndf['LOCATION']=df['CITY']+' '+df['LOCALITY']\ndf['CITY']=df['CITY'].apply(lambda x : re.sub(\"[^0-9a-zA-Z]\", \" \",x))\ndf['Serves2Time']=df['TIME'].apply(lambda x :1 if len(x.split(','))>1 else 0)\ndf['TIME']=df['TIME'].apply(timeclean)\ndf['RATING'].replace({'NEW': '3.7',np.nan:'0.0','-':'3.7'},inplace=True)\ndf['RATING']=df['RATING'].astype(np.float64)\ndf['RATING.BINS']=pd.cut(df['RATING'],4,labels=['Sly','Fair','Good','Great']).astype(np.object)\n\ndf['VOTES'].fillna('0 votes',inplace=True)\ndf['VOTES']=df['VOTES'].apply(lambda x : int(x.split(' ')[0]))\ndf['VOTES.BINS']=pd.cut(df['VOTES'],5,labels=['Sly','Fair','Good','Great','Awesome']).astype(np.object)\ndf=pd.get_dummies(df,columns=['RATING.BINS','VOTES.BINS'],drop_first=True)\n\ndf.drop(['RESTAURANT_ID'],axis=1,inplace=True)\ndf.head()","e19341d9":"df=df.merge(df.groupby('TITLE')['RATING'].agg(['mean','sum','min','max','quantile']).rename(columns={'mean':'trating_mean','sum':'trating_sum',\n                                                                                       'min':'trating_min','max':'trating_max','quantile':'trating_quant'}).reset_index(),on='TITLE',how='left')\n\ndf=df.merge(df.groupby('TITLE')['VOTES'].agg(['mean','sum','min','max','quantile']).rename(columns={'mean':'tVOTES_mean','sum':'tVOTES_sum',\n                                                                                       'min':'tVOTES_min','max':'tVOTES_max','quantile':'tVOTES_quant'}).reset_index(),on='TITLE',how='left')\n\ndf=df.merge(df.groupby('CUISINES')['VOTES'].agg(['mean','sum','min','max','quantile']).rename(columns={'mean':'cVOTES_mean','sum':'cVOTES_sum',\n                                                                                       'min':'cVOTES_min','max':'cVOTES_max','quantile':'cVOTES_quant'}).reset_index(),on='CUISINES',how='left')\ndf=df.merge(df.groupby('CUISINES')['RATING'].agg(['mean','sum','min','max','quantile']).rename(columns={'mean':'cRATING_mean','sum':'cRATING_sum',\n                                                                                       'min':'cRATING_min','max':'cRATING_max','quantile':'cRATING_quant'}).reset_index(),on='CUISINES',how='left')\n\n\ndf=df.merge(df.groupby('CITY')['RATING'].agg(['mean','sum','min','max','quantile']).rename(columns={'mean':'CITYrating_mean','sum':'CITYrating_sum',\n                                                                                       'min':'CITYrating_min','max':'CITYrating_max','quantile':'CITYrating_quant'}).reset_index(),on='CITY',how='left')\n\ndf=df.merge(df.groupby('CITY')['VOTES'].agg(['mean','sum','min','max','quantile']).rename(columns={'mean':'CITYVOTES_mean','sum':'CITYVOTES_sum',\n                                                                                       'min':'CITYVOTES_min','max':'CITYVOTES_max','quantile':'CITYVOTES_quant'}).reset_index(),on='CITY',how='left')\n\ndf=df.merge(df.groupby('LOCALITY')['VOTES'].agg(['mean','sum','min','max','quantile']).rename(columns={'mean':'LOCALITYVOTES_mean','sum':'LOCALITYVOTES_sum',\n                                                                                       'min':'LOCALITYVOTES_min','max':'LOCALITYVOTES_max','quantile':'LOCALITYVOTES_quant'}).reset_index(),on='LOCALITY',how='left')\ndf=df.merge(df.groupby('LOCALITY')['RATING'].agg(['mean','sum','min','max','quantile']).rename(columns={'mean':'LOCALITYRATING_mean','sum':'LOCALITYRATING_sum',\n                                                                                       'min':'LOCALITYRATING_min','max':'LOCALITYRATING_max','quantile':'LOCALITYRATING_quant'}).reset_index(),on='LOCALITY',how='left')\ndf=df.merge(df.groupby('TIME')['VOTES'].agg(['mean','sum','min','max','quantile']).rename(columns={'mean':'TIMEVOTES_mean','sum':'TIMEVOTES_sum',\n                                                                                       'min':'TIMEVOTES_min','max':'TIMEVOTES_max','quantile':'TIMEVOTES_quant'}).reset_index(),on='TIME',how='left')\ndf=df.merge(df.groupby('TIME')['RATING'].agg(['mean','sum','min','max','quantile']).rename(columns={'mean':'TIMERATING_mean','sum':'TIMERATING_sum',\n                                                                                       'min':'TIMERATING_min','max':'TIMERATING_max','quantile':'TIMERATING_quant'}).reset_index(),on='TIME',how='left')","356b9042":"df.head()","215e4fb5":"df_train = df[df['COST'].isnull()==False]\ndf_test = df[df['COST'].isnull()==True]\nprint(df_train.shape,df_test.shape)","ff73e822":"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nv_city = TfidfVectorizer(ngram_range=(1,1),stop_words=\"english\", analyzer='word')\ncity_tr =v_city.fit_transform(df_train['CITY'])\ncity_ts =v_city.transform(df_test['CITY'])\n\nc_city = CountVectorizer(ngram_range=(1,1),stop_words=\"english\", analyzer='word')\nccity_tr =c_city.fit_transform(df_train['CITY'])\nccity_ts =c_city.transform(df_test['CITY'])\n\nv_cuisine = TfidfVectorizer(ngram_range=(1,1),stop_words=\"english\", analyzer='word')\ncui_tr =v_cuisine.fit_transform(df_train['CUISINES'])\ncui_ts =v_cuisine.transform(df_test['CUISINES'])\n\nv_local = TfidfVectorizer(ngram_range=(1,2),stop_words=\"english\", analyzer='word')\nlocal_tr =v_local.fit_transform(df_train['LOCALITY'])\nlocal_ts =v_local.transform(df_test['LOCALITY'])\n\nv_time = TfidfVectorizer(ngram_range=(1,3),stop_words=\"english\", analyzer='word')\ntime_tr =v_time.fit_transform(df_train['TIME'])\ntime_ts =v_time.transform(df_test['TIME'])\n\nv_title = TfidfVectorizer(ngram_range=(1,3),stop_words=\"english\", analyzer='word')\ntitle_tr =v_title.fit_transform(df_train['TITLE'])\ntitle_ts =v_title.transform(df_test['TITLE'])\n\nv_loc = TfidfVectorizer(ngram_range=(1,2),stop_words=\"english\", analyzer='word')\nloc_tr =v_loc.fit_transform(df_train['LOCATION'])\nloc_ts =v_loc.transform(df_test['LOCATION'])","2f439cee":"df.columns","796273ec":"\ncol=[ 'RATING', \n       'VOTES', 'RATING.BINS_Good',\n       'RATING.BINS_Great', 'RATING.BINS_Sly', 'VOTES.BINS_Fair',\n       'VOTES.BINS_Good', 'VOTES.BINS_Sly', 'trating_mean', 'trating_sum',\n       'trating_min', 'trating_max', 'trating_quant', 'tVOTES_mean',\n       'tVOTES_sum', 'tVOTES_min', 'tVOTES_max', 'tVOTES_quant', 'cVOTES_mean',\n       'cVOTES_sum', 'cVOTES_min', 'cVOTES_max', 'cVOTES_quant',\n       'cRATING_mean', 'cRATING_sum', 'cRATING_min', 'cRATING_max',\n       'cRATING_quant', 'CITYrating_mean', 'CITYrating_sum', 'CITYrating_min',\n       'CITYrating_max', 'CITYrating_quant', 'CITYVOTES_mean', 'CITYVOTES_sum',\n       'CITYVOTES_min', 'CITYVOTES_max', 'CITYVOTES_quant',\n       'LOCALITYVOTES_mean', 'LOCALITYVOTES_sum', 'LOCALITYVOTES_min',\n       'LOCALITYVOTES_max', 'LOCALITYVOTES_quant', 'LOCALITYRATING_mean',\n       'LOCALITYRATING_sum', 'LOCALITYRATING_min', 'LOCALITYRATING_max',\n       'LOCALITYRATING_quant', 'TIMEVOTES_mean', 'TIMEVOTES_sum',\n       'TIMEVOTES_min', 'TIMEVOTES_max', 'TIMEVOTES_quant', 'TIMERATING_mean',\n       'TIMERATING_sum', 'TIMERATING_min', 'TIMERATING_max',\n       'TIMERATING_quant']\n","3cac476f":"df_train[col].head()","ba10ddb9":"from scipy.sparse import csr_matrix\nfrom scipy import sparse\nfinal_features = sparse.hstack((df_train[col],city_tr,cui_tr,local_tr,loc_tr,time_tr,title_tr,  ccity_tr )).tocsr()\nfinal_featurest = sparse.hstack((df_test[col],city_ts,cui_ts,local_ts,loc_ts,time_ts,title_ts,  ccity_ts )).tocsr()","e7698193":"from sklearn.model_selection import train_test_split\nimport math\nfrom sklearn.metrics import accuracy_score,f1_score,mean_squared_error,mean_squared_log_error\nX=final_features\ny=np.log1p(df_train['COST'].astype(np.float64))\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.25,random_state = 1994)","1ac5bfb4":"from sklearn.ensemble import AdaBoostRegressor,GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge,RidgeCV,BayesianRidge\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.metrics import mean_squared_log_error\nimport math\ndef rmsle(real, predicted):\n    real=np.expm1(real)\n    predicted=np.expm1(predicted)\n    return np.sqrt(mean_squared_log_error(real,predicted))\n    \ndef rmsle_lgb(labels, preds):\n    return 'rmsle', rmsle(preds,labels), False","0d4611fc":"m=LGBMRegressor(n_estimators=4000,random_state=1994,learning_rate=0.01,objective='regression',reg_alpha=1,reg_lambda=10,colsample_bytree=0.2,min_child_samples=20,feature_fraction=0.2)\nm.fit(X_train,y_train,eval_set=[(X_val, y_val.values)],eval_metric='rmse', early_stopping_rounds=100,verbose=100)\np=m.predict(X_val)\nprint(rmsle_lgb(y_val.values,p))","d5326f1c":"from xgboost import XGBRegressor\nm=XGBRegressor(n_estimators=6000,learning_rate=0.02,random_state=1994,max_depth=8,reg_alpha=1,colsample_bytree=0.3,max_delta_step=0.5,seed=1994,colsample_bylevel=0.5)\nm.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_val, y_val.values)],eval_metric='rmse', early_stopping_rounds=200,verbose=100)\np1=m.predict(X_val)\nprint(rmsle_lgb(y_val.values,p1))","e613f498":"print(rmsle_lgb(y_val,(p*0.5+p1*0.5)))","8aabc569":"errlgb=[]\ny_pred_totlgb=[]\ni=0\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfold=KFold(n_splits=20,shuffle=True,random_state=1994)\nfor train_index, test_index in fold.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    lgbm_params = {'n_estimators': 4000,\n                   'n_jobs': -1,'learning_rate':0.01,'random_state':1994,'reg_lambda':10,'reg_alpha':1,'colsample_bytree':0.2\n                  ,'min_child_samples':20,'feature_fraction':0.2}\n    rf=LGBMRegressor(**lgbm_params)\n    rf.fit(X_train,y_train,eval_set=[(X_test, y_test.values)],\n         eval_metric=rmsle_lgb,\n         verbose=200\n         , early_stopping_rounds=200\n          )\n    pr=rf.predict(X_test)\n    print(\"errlgb: \",rmsle_lgb(y_test.values,pr)[1])\n    \n    errlgb.append(rmsle_lgb(y_test.values,pr)[1])\n    p = rf.predict(final_featurest)\n    y_pred_totlgb.append(p)","2dc6fcd7":"errxgb=[]\ny_pred_totxgb=[]\ni=0\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfold=KFold(n_splits=20,shuffle=True,random_state=1994)\nfor train_index, test_index in fold.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    lgbm_params = {'n_estimators': 6000,\n                   'n_jobs': -1,'learning_rate':0.02,'random_state':1994,'max_depth':8,'reg_alpha':1,'colsample_bytree':0.3\n                  ,'max_delta_step':0.5,'colsample_bylevel':0.5,'seed':1994}\n    rf=XGBRegressor(**lgbm_params)\n    rf.fit(X_train,y_train,eval_set=[(X_test, y_test.values)],\n         eval_metric='rmse',\n         verbose=200\n         , early_stopping_rounds=100\n          )\n    pr=rf.predict(X_test)\n    print(\"errlgb: \",rmsle_lgb(y_test.values,pr)[1])\n    \n    errxgb.append(rmsle_lgb(y_test.values,pr)[1])\n    p = rf.predict(final_featurest)\n    y_pred_totxgb.append(p)","0aacfe87":"np.mean(errxgb),np.mean(errlgb)","ab8eb2d4":"np.mean(errxgb)*0.5+np.mean(errlgb)*0.5","d71712b4":"np.mean(y_pred_totlgb,0)*0.5+np.mean(y_pred_totxgb,0)*0.5","dd9ed74d":"s=pd.DataFrame({'COST':np.expm1(np.mean(y_pred_totlgb,0)*0.5+np.mean(y_pred_totxgb,0)*0.5)})\ns.to_excel('MH-Predict_food_pricesv9_stack_final.xlsx',index=False)\ns.head()\n\ns=pd.DataFrame({'COST':np.expm1(np.mean(y_pred_totlgb,0))})\ns.to_excel('MH-Predict_food_pricesv9_lgb_final.xlsx',index=False)\ns.head()\n\ns=pd.DataFrame({'COST':np.expm1(np.mean(y_pred_totxgb,0))})\ns.to_excel('MH-Predict_food_pricesv9_xgb_final.xlsx',index=False)\ns.head()","271d9fb6":"## Finding\/Removing duplicates","8f1095e4":"## K-folds","830d967f":"## Some insights","fa7416af":"## Modelling and Evaluating","72dd1291":"### Light-GBM","df739024":"### Xgboost","cbadc981":"### Eval Metric","828bbc7d":"### TF-ID","33f14cd9":"## Missing values and Feature engineering","aed862a1":"# Importing Data"}}