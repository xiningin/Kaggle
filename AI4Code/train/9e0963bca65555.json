{"cell_type":{"cc16822e":"code","e050fb26":"code","e3c97470":"code","0aa92ea6":"code","a1ea4a7a":"code","bfdc3b56":"code","b850298e":"code","42881a1b":"code","5e74af7c":"code","130b788c":"code","5e81b249":"code","998c3a3d":"code","6e3a66eb":"code","35a99ccb":"code","a7fcd593":"code","d2a42823":"code","9f4936eb":"code","6e819ade":"code","ba0452d1":"code","da9e706a":"code","f895b48b":"code","fe1fb0f8":"code","aa3adeae":"code","9db50e4f":"code","58728e91":"code","61dce857":"code","459cd3a9":"code","25e08758":"code","ab8f8211":"code","cd0a783b":"code","c6470074":"code","9c3e5dce":"code","eb710b8a":"code","37eeb37b":"code","c52451a0":"code","c37fc7d1":"code","68fa0d4b":"code","266cf8a1":"code","95edc890":"code","a008a20e":"markdown","3d7073bc":"markdown","90dcfc49":"markdown","1ce7bb36":"markdown","0897fae6":"markdown","70704c2b":"markdown","442feca3":"markdown","6e6b4554":"markdown","82ce770e":"markdown","48916263":"markdown","77a65881":"markdown","4b79f84b":"markdown","c92747e8":"markdown"},"source":{"cc16822e":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\n\n#import lightgbm as lgb \n#import xgboost as xgb\nfrom catboost import CatBoostRegressor\n#from sklearn.linear_model import LinearRegression,HuberRegressor,SGDRegressor\n#from sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\nimport optuna\nimport math","e050fb26":"LINEAR_DATE_AUG = False\n\nOPTUNA = False\nNUM_TRIALS = 400\nSEED = 200\n\n##Add DATA\nADD_2014 = False \n\n#Holidays\nHOLIDAYS = True     \nNEXT_HOLIDAY = True  \n\nPOST_PROCESSING = False\nMODEL_TYPE = \"Catboost\"\n\nVAL_SPLIT = \"2017-12-31\" #\"2018-05-31\"","e3c97470":"EPOCHS = 10000     \nEARLY_STOPPING = 30\n\n###########Come back to this   ---Doesnt work in CATBOOST (need to apply it)\nSCALER_NAME = \"MinMax\"  #None MinMax  \nSCALER = MinMaxScaler() ","0aa92ea6":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\",index_col = 0)\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\",index_col = 0)\ngdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\ngdp_df.set_index('year', inplace=True)\n\nif HOLIDAYS:\n    holidays = pd.read_csv(\"..\/input\/holidays-finland-norway-sweden-20152019\/Holidays_Finland_Norway_Sweden_2015-2019.csv\",usecols = [\"Date\",\"Country\",\"Name\"]                      )\n    holidays.rename(columns = {\"Date\":\"date\",\"Country\":\"country\",\"Name\":\"holiday\"},inplace= True)\n    holidays[\"holiday\"]= 1\n    holidays[\"holiday\"]= holidays[\"holiday\"].astype(\"int32\")\n    holidays[\"date\"] = pd.to_datetime(holidays[\"date\"])","a1ea4a7a":"#Make date\ntrain[\"date\"] = pd.to_datetime(train[\"date\"])\ntest[\"date\"] = pd.to_datetime(test[\"date\"])","bfdc3b56":"if ADD_2014:\n    train_2014= train [train[\"date\"]<\"2016-01-01\"]\n    train_2014[\"date\"] = train_2014[\"date\"] - pd.DateOffset(years=1)\n    train_2014[\"num_sold\"] = train_2014[\"num_sold\"]*0.98\n    train = pd.concat([train_2014,train],axis=0,ignore_index=True)","b850298e":"train.head()","42881a1b":"def public_hols(df):\n    df = pd.merge(df, holidays, how='left', on=['date', 'country'])\n    df.fillna(value = 0,inplace=True)\n    \n    return df\n\nif HOLIDAYS:\n    train = public_hols(train)\n    test = public_hols(test)","5e74af7c":"def get_gdp(row):\n    country = 'GDP_' + row.country\n    return gdp_df.loc[row.date.year, country]","130b788c":"def engineer(df):\n    #get GDP from file \n    df[\"gdp\"] = df.apply(get_gdp, axis=1)   #improves Huber & Tweedie & catboost\n    \n    df[\"day\"] = df[\"date\"].dt.day\n    df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\n    df[\"month\"] = df[\"date\"].dt.month\n    df[\"year\"] = df[\"date\"].dt.year\n    \n    #play around with if Tree model - each varies \n    #df['dayofyear'] = df['date'].dt.dayofyear                ### This can cause noise \n    df['inverse_dayofyear'] = 365 - df['date'].dt.dayofyear    # good for all  except catboost\n    df.loc[df[\"year\"] == 2016 , \"inverse_dayofyear\"] = df.loc[df[\"year\"] == 2016 , \"inverse_dayofyear\"]+1     #Leap year in 2016\n    df['quarter'] = 'Q' + df['date'].dt.quarter.astype(str)      # Good for lightgbm & Huber, bad for Tweedie & catboost\n    #df['daysinmonth'] = df['date'].dt.days_in_month          ## Bad for all except Lightgbm\n\n    #df[\"Friday\"] = df[\"dayofweek\"] ==4\n    #df[\"Sat_sun\"] = (df[\"dayofweek\"] ==5) |(df[\"dayofweek\"] ==6)\n    \n    if LINEAR_DATE_AUG:\n        for country in ['Finland', 'Norway']:\n            df[country] = df.country == country\n        df['KaggleRama'] = df.store == 'KaggleRama'\n        for product in ['Kaggle Mug', 'Kaggle Sticker']:\n            df[product] = df['product'] == product\n\n        df.drop([\"country\",\"store\",\"product\"], axis =1, inplace = True)\n\n        # Seasonal variations (Fourier series)\n        dayofyear = df.date.dt.dayofyear\n        for k in range(1, 20):\n            df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n            df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n            df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n            df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n            df[f'sticker_sin{k}'] = df[f'sin{k}'] * df['Kaggle Sticker']\n            df[f'sticker_cos{k}'] = df[f'cos{k}'] * df['Kaggle Sticker']\n\n    return df","5e81b249":"train = engineer(train)\ntest = engineer(test)\n\n#Sometimes this helps sometimes is doesnt - catboost encodes within model\ncategorical_feats = [\n    \"country\",\"store\",\"product\",\n                     \"quarter\", \n                     #\"weekend_ind\"\n                    ]","998c3a3d":"def next_holiday(x):\n    i=1\n    while sum(holidays[\"date\"] == pd.Timestamp(x) + pd.DateOffset(days=i)) ==0:\n        i+=1\n        if i >200:\n            i=0\n            break\n            break\n    return i\n\nif NEXT_HOLIDAY:\n    holidays[\"date\"] = pd.to_datetime(holidays[\"date\"])\n    train[\"to_holiday\"] = train[\"date\"].apply(lambda x : next_holiday(x))\n    test[\"to_holiday\"] = test[\"date\"].apply(lambda x : next_holiday(x))","6e3a66eb":"def SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","35a99ccb":"def scale_data(X_train, X_test, test):\n    scaler= SCALER\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    test = scaler.transform(test)\n    \n    return X_train, X_test, test","a7fcd593":"train.head()","d2a42823":"# Catboost does encoding in training \ntrain = pd.get_dummies(train,columns= categorical_feats)\ntest = pd.get_dummies(test,columns= categorical_feats)","9f4936eb":"train.head()","6e819ade":"prior_2017 = train[train[\"date\"]<=VAL_SPLIT].index\nafter_2017 = train[train[\"date\"]>VAL_SPLIT].index","ba0452d1":"train.index = train[\"date\"]\ntrain.drop(\"date\",axis=1,inplace=True)\n\ntest.index = test[\"date\"]\ntest.drop(\"date\",axis=1,inplace=True)","da9e706a":"X = train.drop(\"num_sold\", axis=1)\ny= train[\"num_sold\"]","f895b48b":"X_train = train.iloc[prior_2017,:].drop(\"num_sold\", axis=1)\nX_test = train.iloc[after_2017,:].drop(\"num_sold\", axis=1)\ny_train= train.iloc[prior_2017,:][\"num_sold\"]\ny_test= train.iloc[after_2017,:][\"num_sold\"]","fe1fb0f8":"X_train.head(2)","aa3adeae":"# Removing  scaling with Catboost as there are categorical columns in training (and Im too lazy to scale only numerical and concat the categories)\n\nX_train,X_test,test =scale_data(X_train,X_test,test)","9db50e4f":"def objective(trial):\n    # 2. Suggest values of the hyperparameters using a trial object.\n    \n    params = {\n        \"loss_function\": trial.suggest_categorical(\"loss_function\",  [\"Poisson\",'RMSE',\"Tweedie\"]),\n        \"eval_metric\": trial.suggest_categorical(\"eval_metric\",  [\"Poisson\",'RMSE',\"Tweedie\",\"SMAPE\"]),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e0),\n        \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-2, 1e0),\n        #\"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 2, 20),\n        #\"one_hot_max_size\": trial.suggest_int(\"one_hot_max_size\", 2, 20),  \n    }\n    \n    # Conditional Hyper-Parameters\n    if params[\"bootstrap_type\"] == \"Bayesian\":\n        params[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif params[\"bootstrap_type\"] == \"Bernoulli\":\n        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n        \n    #Tweedie needs variance power   \n    if params[\"loss_function\"] == \"Tweedie\":\n        params[\"loss_function\"] = \"Tweedie:variance_power=\" + str(trial.suggest_float(\"loss_function_tweedie\", 1, 2))\n    if params[\"eval_metric\"] == \"Tweedie\":\n        params[\"eval_metric\"] = \"Tweedie:variance_power=\" + str(trial.suggest_float(\"eval_metric_tweedie\", 1, 2))\n\n    \n    ## CREATE catboost model\n    model = CatBoostRegressor(**params,\n                              iterations=EPOCHS,\n                              cat_features=categorical_feats\n                         )\n    model.fit(\n        X_train, y_train, \n        eval_set=(X_test, y_test),\n        early_stopping_rounds=EARLY_STOPPING,\n        verbose=0,\n        use_best_model=True,\n        )\n    \n    #Predictions\n    test_predictions = model.predict(X_test)\n    smape = SMAPE(y_test,test_predictions)\n    \n    print(\"SMAPE:\",smape)\n    \n    return smape","58728e91":"if OPTUNA:\n    print(\"RUNINING OPTUNA CATBOOST\")\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=NUM_TRIALS)\n    trial = study.best_trial","61dce857":"if OPTUNA:\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    print(\"Best trial num :\",trial.number)\n    print(\" SMAPE Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","459cd3a9":"if OPTUNA: \n    params = trial.params\n    \nelse:\n    #OPTUNA 300 - 7.604\n    params = {\n        \"loss_function\": \"Poisson\",\n        \"eval_metric\": \"Poisson\",\n        \"learning_rate\": 0.15643124768390415,\n        \"l2_leaf_reg\": 0.034286809582276825,\n        \"depth\": 4,\n        \"boosting_type\": \"Ordered\",\n        \"bootstrap_type\": \"Bernoulli\",\n        \"min_data_in_leaf\": 9,\n        \"subsample\": 0.5632840359532664\n    }\n        \n        \n    #Optuna 200\n    \"\"\"params = {\n        #only if tweedie\n        \"loss_function\":\"RMSE\",\n        \"eval_metric\":\"SMAPE\",\n        \"learning_rate\" : 0.08328012000638386,\n        \"l2_leaf_reg\":0.011187384806520727,\n        \"depth\": 10,\n        \"boosting_type\": \"Plain\",\n        'bootstrap_type': 'Bayesian',\n        \"min_data_in_leaf\": 18,\n        \"bagging_temperature\": 3.2583638997300923\n        }\"\"\"\n\n    #400 OPTUNA \n    \"\"\"params = {\n        #only if tweedie\n        \"loss_function\":\"Tweedie:variance_power=1.5811093354225167\",\n        \"eval_metric\":\"RMSE\",\n        \"learning_rate\" : 0.5283210615574262,\n        \"l2_leaf_reg\":0.9740971300008016,\n        \"depth\": 5,\n        'bootstrap_type': 'MVS',\n        \"boosting_type\": \"Ordered\",\n        \"min_data_in_leaf\": 15\n        }\"\"\"","25e08758":"\ndef fit_model(X_train,y_train,X_test,y_test,test_df, seed_num):\n    \n    model = CatBoostRegressor(**params,\n                              iterations=EPOCHS,\n                          #cat_features=categorical_feats, \n                              random_seed=seed_num\n                         )\n    model.fit(\n        X_train, y_train, \n        eval_set=(X_test, y_test),\n        early_stopping_rounds=EARLY_STOPPING,\n        verbose=0,\n        use_best_model=True\n        )\n\n\n    train_preds = model.predict(X_test)\n    #predict test data\n    test_preds = model.predict(test_df)\n    \n    smape = SMAPE(y_test,train_preds)\n    print(\"SMAPE:\", smape)\n    \n    return train_preds, test_preds, smape","ab8f8211":"train_preds, test_preds, smape = fit_model(X_train,y_train,X_test,y_test,test, 0)","cd0a783b":"def seed_CV(X_train,y_train, X_test, y_test, test_df):\n    \n    smape_score = []\n    seed_test_preds = np.zeros((SEED, len(test_df)))\n    seed_train_preds = np.zeros((SEED, len(y_test)))\n    \n    for i in range(SEED):\n        print(f\"##### SEED {i} #######\")\n        \n        train_preds, test_preds, smape = fit_model(X_train,y_train,X_test,y_test,test_df, i)\n\n        smape_score.append(smape)\n        seed_test_preds[i] = test_preds\n        seed_train_preds[i] = train_preds\n\n    \n    return np.mean(smape_score) , np.mean(seed_test_preds,axis =0) , np.mean(seed_train_preds,axis =0)\n\nfinal_smape, final_predictions, train_preds  = seed_CV(X_train,y_train, X_test, y_test, test)","c6470074":"print(\"SMAPE :\", final_smape)","9c3e5dce":"final_predictions","eb710b8a":"print(\"SMAPE :\",final_smape )\nprint(f\"\\n EPOCHS: {EPOCHS}\")\nprint(f\"\\n SCALER: {SCALER_NAME}\")\nprint(f\"\\n PARAMS: { params}\")\nprint(f\"\\n Holidays : {HOLIDAYS}\")\nprint(f\"\\n Next Holiday : {NEXT_HOLIDAY}\")\nprint(f\"\\n Linear Date Augmentation : {LINEAR_DATE_AUG}\")\nprint(f\"\\n POST_PROCESSING: {POST_PROCESSING}\")","37eeb37b":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\",index_col = 0)","c52451a0":"if POST_PROCESSING:\n    # from previous run we are under predicting, lets scale the values upwards\n    print(\"Scaling predictions \")\n    print(\"preds_prior:\", final_predictions)\n    \n    sub[\"num_sold\"] = final_predictions*1.143\n    \n    print(\"preds after:\", np.array(sub[\"num_sold\"]))\nelse:\n    sub[\"num_sold\"] = final_predictions","c37fc7d1":"sub.to_csv(\"submission.csv\")","68fa0d4b":"sub.head()","266cf8a1":"#for visual only\ntest1 = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\",index_col = 0)\ntest1[\"date\"] = pd.to_datetime(test1[\"date\"])\n\nfig,ax = plt.subplots(2,1, figsize=(25,20),sharey= True)\n\ndiff = y_test - train_preds\nsns.lineplot(ax=ax[0], data= y_test, label=\"Train Actual\",ci=None)\nsns.lineplot(ax=ax[0], data = y_test,x = y_test.index , y = final_predictions, label =\"Validation Prediction\" ,ci=None)\nsns.lineplot(ax=ax[0],data =sub, x= test1[\"date\"], y = \"num_sold\",label=\"Final Prediction\" ,ci=None) \n\nax[0].set_title(f\"Actual and Predicted Sales for {MODEL_TYPE}\")\n\nsns.lineplot(ax=ax[1], data = diff, label =\"Residuals\" )\nax[1].set_title(f\"Residuals for {MODEL_TYPE} for 2018\")\n\nplt.show()","95edc890":"plt.figure(figsize=(25,10))\n\nsns.lineplot(data= train[\"num_sold\"] ,label=\"Train Actual\",ci=None)\nsns.lineplot(data =sub, x= test1[\"date\"], y = \"num_sold\",label=\"Final Prediction\" ,ci=None) \nplt.title(\"Actual and Predicted Sales\")\n\nplt.show()","a008a20e":"### One HotEncoder","3d7073bc":"# Libraries","90dcfc49":"# Training Visualization","1ce7bb36":"## Random Seed CV\nVariation of [adamwurdits](https:\/\/www.kaggle.com\/adamwurdits\/tps-01-2022-catboost-w-optuna-seed-averaging) \nI removed the CV as I wasnt having good scores with TimeSeriesSplit","0897fae6":"# Optuna","70704c2b":"1. Add fake 2014 data with gDP\n1. Run a model for each product\n1. Recursive model\n1. Add Further Feature engineering - ambrosm https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model\/\n1. SARIMAX model\n\n* **Outliers**? - check for linear and Trees\n\n* Catboost minmax scaling ","442feca3":"obj is the objective function of the algorithm, i.e. what it's trying to maximize or minimize, e.g. \"regression\" means it's minimizing squared residuals.\n\nMetric and eval are essentially the same. They are used for Early stopping ","6e6b4554":"# Run model","82ce770e":"# Split and Scale","48916263":"# Functions ","77a65881":"# Post Processing & Submission ","4b79f84b":"# Load Data","c92747e8":"## Previous Rns \n#### Best Run\n7.793541049903422  loss_function': 'RMSE', 'eval_metric': 'SMAPE', 'learning_rate': 0.08328012000638386, 'l2_leaf_reg': 0.011187384806520727, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'min_data_in_leaf': 18, 'bagging_temperature': 3.2583638997300923\n\n####Best Summited\n7.993  ---Optuna 200   - --Submitted as 5.75958    ---no holidays, \n\ndef engineer(df):\n    #get GDP from file \n    df[\"gdp\"] = df.apply(get_gdp, axis=1)   #improves Huber & Tweedie & catboost\n    \n    df[\"day\"] = df[\"date\"].dt.day\n    df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\n    df[\"month\"] = df[\"date\"].dt.month\n    df[\"year\"] = df[\"date\"].dt.year\n    \n    #play around with if Tree model - each varies \n    df['dayofyear'] = df['date'].dt.dayofyear                ### This can cause noise \n    #df['inverse_dayofyear'] = 365 - df['date'].dt.dayofyear    # good for all  except catboost\n    #df.loc[df[\"year\"] == 2016 , \"inverse_dayofyear\"] = df.loc[df[\"year\"] == 2016 , \"inverse_dayofyear\"]+1     #Leap year in 2016\n    #df['quarter'] = 'Q' + df['date'].dt.quarter.astype(str)      # Good for lightgbm & Huber, bad for Tweedie & catboost\n    #df['daysinmonth'] = df['date'].dt.days_in_month           ## Also reduces performance\n     \n    #encoding\n    for country in ['Finland', 'Norway']:\n        df[country] = df.country == country\n    df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Sticker']:\n        df[product] = df['product'] == product\n    \n    #df[\"Friday\"] = df[\"dayofweek\"] ==4\n    #df[\"Sat_sun\"] = (df[\"dayofweek\"] ==5) |(df[\"dayofweek\"] ==6)\n    \n    df.drop([\"country\",\"store\",\"product\"], axis =1, inplace = True)\n    \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 20):\n        df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'sticker_sin{k}'] = df[f'sin{k}'] * df['Kaggle Sticker']\n        df[f'sticker_cos{k}'] = df[f'cos{k}'] * df['Kaggle Sticker']\n\n    return df"}}