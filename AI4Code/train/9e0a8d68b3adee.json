{"cell_type":{"e26a9b98":"code","68deb634":"code","7cdac3eb":"code","f2bf540e":"code","7bdad8cc":"code","94e8e815":"code","ec3a9e05":"code","e2d2bdba":"code","3a46423f":"code","827aaf71":"code","ee75569e":"markdown","aec18509":"markdown","7ad258df":"markdown","4a71913b":"markdown","9c04a7b7":"markdown","04f2d38d":"markdown","958bd264":"markdown","2ff478f2":"markdown","96b7377a":"markdown","70f4b387":"markdown","655ad6db":"markdown","8d89ab0b":"markdown","e4750aa7":"markdown"},"source":{"e26a9b98":"import numpy as np  # linear algebra\nimport sys\nimport os\nimport tensorflow as tf\nimport logging\nimport random\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action\nfrom tensorflow.keras import Model\n\nfrom kaggle_environments import make\nfrom collections import deque\nimport tqdm\nimport pickle\nimport os as os\n\nfrom tensorflow.keras.callbacks import Callback","68deb634":"Action_dict, Action_invdict = {}, {}\nfor index, action in enumerate(Action):\n    Action_dict[index] = action.name\n    Action_invdict[action.name] = index\n\n\ndef centerize(b):\n    dy, dx = np.where(b[:, :, 0])\n    centerize_y = (np.arange(0, 7) - 3 + dy[0]) % 7\n    centerize_x = (np.arange(0, 11) - 5 + dx[0]) % 11\n\n    b = b[centerize_y, :, :]\n    b = b[:, centerize_x, :]\n\n    return b\n\n\ndef observation_matrix(obs_prev, obs, idx):\n    b = np.zeros((7 * 11, 17), dtype=np.float32)\n\n    # if len(obs['geese'][idx]) == 0:\n    #     return b.reshape(7, 11, -1)\n\n    for p, pos_list in enumerate(obs['geese']):\n        # head position\n        for pos in pos_list[:1]:\n            b[pos, 0 + (p - idx) % 4] = 1\n        # tip position\n        for pos in pos_list[-1:]:\n            b[pos, 4 + (p - idx) % 4] = 1\n        # whole position\n        for pos in pos_list:\n            b[pos, 8 + (p - idx) % 4] = 1\n\n        # previous head position\n        for p, pos_list in enumerate(obs_prev['geese']):\n            for pos in pos_list[:1]:\n                b[pos, 12 + (p - idx) % 4] = 1\n\n                # food\n    for pos in obs['food']:\n        b[pos, 16] = 1\n\n    b = b.reshape(7, 11, -1)\n    b = centerize(b)\n\n    return b","7cdac3eb":"def action_matrix(actions, current_index):\n    action_one_hot = np.zeros((4, 4))\n    for idx, act in enumerate(actions):\n        if act:\n            if idx == current_index:\n                action_one_hot[(idx - current_index) % 4, Action_invdict[act]] = 1\n            else:\n                action_one_hot[(idx - current_index) % 4, Action_invdict[act]] = -1\/4\n\n    return action_one_hot\n\n\ndef get_info(agent, prev_state, state, step, idx=0, train=False, epsilon=0.1):\n    act = state[idx][\"action\"]\n    if state[idx][\"status\"] != \"ACTIVE\":\n        return None, None\n\n    matrix = observation_matrix(prev_state[0].observation, state[0].observation, idx)\n\n    act_index = Action_invdict[act]\n    if epsilon == 2:\n        return matrix, Action_dict[np.random.choice(4)]\n\n    if train and (np.random.random() < epsilon):\n        return matrix, Action_dict[((act_index - 2) + (1 + np.random.choice(2))) % 4]\n\n    Q = tf.squeeze(agent.actor([matrix.reshape(1, 7, 11, 17), step.reshape(1, 1) \/ 200])).numpy()\n    return matrix, Action_dict[np.argmax(Q[0, :])]","f2bf540e":"def reward(prev_state, state):\n    fruit = 0.1#0.2\n    rewards  = np.zeros(4)*0.01#surviving is good\n    rewards2 = np.zeros(4)*0.0\n\n    prev_guesses = prev_state[0][\"observation\"][\"geese\"]\n    glen = np.array([len(prev_guesses[idx])for idx in range(4)])\n    guesses = state[0][\"observation\"][\"geese\"]\n\n    active = np.array([s.status == \"ACTIVE\" for s in prev_state])\n    n_active = sum(active)\n\n    done = all(s[\"status\"] != \"ACTIVE\" for s in state)\n\n    if not done:\n        for idx in range(4):\n            if not active[idx]:  # dead\n                continue\n\n            if len(guesses[idx]) == 0 and active[idx]:  # died on this turn\n                rewards[idx] += -1\n                rewards2[:idx] += 1 #\/ n_active\n                rewards2[idx + 1:] += 1 #\/ n_active\n\n            if len(guesses[idx]) - len(prev_guesses[idx]) > 0:  # ate a fruit\n                rewards[idx] += fruit\n                rewards2[:idx] += -fruit #\/ n_active\n                rewards2[idx + 1:] += -fruit #\/ n_active\n\n    if done:\n        f = lambda s: s.reward if s.reward else 0\n        env_reward = np.array([f(s) for s in state])\n        \n        rewards +=  3*(env_reward == max(env_reward))  # 1 to winner\n        rewards +=  -0.5*(env_reward != max(env_reward))\n\n    return rewards,rewards2\n","7bdad8cc":"def snakeConv():\n    i = tf.keras.layers.Input(shape=(7, 11, 17), name=\"matrix\")\n    istep = tf.keras.layers.Input(shape=(1), name=\"step\")\n\n    x = tf.keras.layers.Conv2D(200, 2, activation=\"relu\")(i)\n    x = tf.keras.layers.AveragePooling2D((2,2))(x)\n    x = tf.keras.layers.Conv2D(100, 2, activation=\"relu\")(x)\n    x = tf.keras.layers.AveragePooling2D((2,2))(x)\n\n    x = tf.keras.layers.Flatten()(x)\n    y = tf.reduce_mean(i, axis=[1, 2])\n\n    z = tf.keras.layers.concatenate([x,y, istep])\n    z = tf.keras.layers.Dense(100, activation=\"relu\")(z)\n    z = tf.keras.layers.Dense(100, activation=\"relu\")(z)\n\n    z = tf.keras.layers.Dense(4 * 4)(z)\n    qvalues = tf.keras.layers.Reshape((4, 4))(z)\n\n    #qvalues = tf.clip_by_value(qvalues, -2, 3)\n\n    actor = Model(inputs=[i, istep], outputs=qvalues)\n    actor.compile()\n\n    return actor\n\n\ndef model_compile(actor):\n    i = tf.keras.layers.Input(shape=(7, 11, 17), name=\"matrix\")\n    istep = tf.keras.layers.Input(shape=(1), name=\"step\")\n    act_onehot = tf.keras.layers.Input(shape=(4, 4), name=\"onehot\")\n\n    x = tf.reduce_sum(actor([i, istep]) * act_onehot, axis=[1, 2])\n\n    model_to_fit = Model(inputs=[i, istep, act_onehot], outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    model_to_fit.compile(opt, \"MSE\")\n    return model_to_fit\n\n\ndef model_target(target):\n    i = tf.keras.layers.Input(shape=(7, 11, 17), name=\"matrix\")\n    istep = tf.keras.layers.Input(shape=(1), name=\"step\")\n    i_active_one = tf.keras.layers.Input(shape=(4), name=\"active_one\")\n    \n    x = target([i, istep])\n    \n    #constant = tf.constant([1.,-1.\/4, -1.\/4, -1.\/4])\n    \n    x = tf.reduce_max(x, axis=2)\n    logits = tf.reduce_sum(x*i_active_one, axis=1)\n\n\n    model_to_target = Model(inputs=[i, istep,i_active_one], outputs=logits)\n    return model_to_target\n\n\nclass agent_class():\n    def __init__(self, alpha):\n        self.actor = snakeConv()\n        self.target = tf.keras.models.clone_model(self.actor)\n        self.model_fit = model_compile(self.actor)\n        self.label = model_target(self.target)\n\n        self.alpha = alpha\n\n        self.prevState = None\n        self.step = None\n        self.opt = tf.keras.optimizers.Adam(0.001, clipnorm=0.25)#0.001\n\n    def agent_call(self, state, step, index):\n        if self.prevState == None:\n            matrix = observation_matrix(state, state, index)\n        else:\n            matrix = observation_matrix(self.prevState, state)\n\n        logits = np.array(self.actor(matrix.reshape(1, 7, 11, 17), step.reshape(1, 1) \/ 200))[0]\n        action = np.argmax(logits[index])\n        action_name = Action_dict[action]\n        self.prevState = state\n\n        return action_name\n\n    def reset(self):\n        self.prevState = None\n\n    def update(self):\n        self.target.set_weights(self.actor.get_weights())","94e8e815":"def one_path(epsilon, train=True):\n    done = False\n    random_agent = [agents[random.randint(0, nb_of_agents - 1)] for _ in range(4)]\n\n    state = env.reset(4)\n    prev_state = state\n\n    epoch = 0\n    while not done:\n        # get action\n        epoch += 1\n        active = np.array([s.status == \"ACTIVE\" for s in state])\n        infos = [get_info(random_agent[idx], prev_state, state, np.array(epoch), idx, train=train, epsilon=epsilon)\n                 for idx in range(4)]\n\n        ## STEPS INTO\n        actions = [info[1] for info in infos]\n        prev_state = state\n        state = env.step(actions)\n        prev_dones = [s[\"status\"] != \"ACTIVE\" for s in prev_state]\n        dones = [s[\"status\"] != \"ACTIVE\" for s in state]\n        done = all(dones)\n        rewards,rewards2 = reward(prev_state, state)\n\n        ## Memories\n        if train:\n            temp = []\n            for idx in range(4):\n                \n                #active indicator that the opponent is alive after actions\n                active_one = np.zeros(4)\n                active_one[0] = 1\n                for opponent in range(4):\n                    if opponent != idx and not dones[opponent]:\n                        active_one[(opponent-idx)%4] = -1\/4\n                \n                \n                if epoch > 1 and active[idx] and not prev_dones[idx]:\n                    if not dones[idx]:\n                        temp.append((infos[idx][0],\n                                       action_matrix(actions, idx), (rewards[idx],rewards2[idx]),\n                                       observation_matrix(prev_state[0].observation, state[0].observation, idx)\n                                       , dones[idx], epoch,active_one))\n                    else:\n                        temp.append((infos[idx][0],\n                                       action_matrix(actions, idx), (rewards[idx],rewards2[idx]),\n                                       infos[idx][0]\n                                       , dones[idx], epoch,active_one))\n            memory.append(temp) # add all the transitions in the same tuples\n            \n    return epoch\n\n\ndef explorer(episodes, epsilon):\n    with tqdm.trange(episodes) as t:\n        for j in t:\n            epoch = one_path(epsilon)\n            t.set_description(f'Episode {j}')\n            t.set_postfix(len=epoch, memory_start=len(memory))\n\nclass EarlyStoppingByLossVal(Callback):\n    def __init__(self, monitor='loss', value=0.05, verbose=0):\n        super(Callback, self).__init__()\n        self.monitor = monitor\n        self.value = value\n        self.verbose = verbose\n\n    def on_epoch_end(self, epoch, logs={}):\n        current = logs.get(self.monitor)\n\n        if current < self.value:\n            if self.verbose > 0:\n                print(\"Epoch %05d: early stopping\" % epoch)\n            self.model.stop_training = True\n\n\ndef all_fit(agents_list, epochs=1):\n    for agent in agents_list:\n        if len(memory) \/\/ 4 > batch_size:\n            samples_Lists = random.sample(memory, min(max_batch,len(memory)))\n            samples = sum(samples_Lists, [])\n            \n            matrix = tf.cast(tf.stack(np.array([sample[0] for sample in samples])), tf.float32)\n            action_one_hot = tf.cast(tf.stack(np.array([sample[1] for sample in samples])), tf.float32)\n            \n            reward1 = tf.cast(tf.stack(np.array([sample[2][0] for sample in samples])), tf.float32)\n            reward2 = tf.cast(tf.stack(np.array([sample[2][1] for sample in samples])), tf.float32)\n            reward = reward1 + agent.alpha*reward2\n            \n            matrix_ = tf.cast(tf.stack(np.array([sample[3] for sample in samples])), tf.float32)\n            done = tf.cast(tf.stack(np.array([sample[4] for sample in samples])), tf.float32)\n            step = tf.cast(tf.stack(np.array([sample[5] for sample in samples])), tf.float32)\/200\n            \n            active_one = tf.cast(tf.stack(np.array([sample[6] for sample in samples])), tf.float32)\n\n            labels_TD = tf.clip_by_value(agent.label([matrix_, step,active_one]) * (1 - done) + reward, tf_clip_min, tf_clip_max) \n\n            agent.model_fit.fit([matrix, step, action_one_hot]\n                          , labels_TD, epochs=epochs, verbose=1, batch_size=batch_size, shuffle=False)#no shuffle to keep actions in the same gradient\n\n\ndef all_fit_update(agents_list, epochs=1000):\n    for agent in agents_list:\n        if len(memory) \/\/ 4 > batch_size:\n            samples_Lists = random.sample(memory, min(max_batch,len(memory)))\n            samples = sum(samples_Lists, [])\n            \n            matrix = tf.cast(tf.stack(np.array([sample[0] for sample in samples])), tf.float32)\n            action_one_hot = tf.cast(tf.stack(np.array([sample[1] for sample in samples])), tf.float32)\n           \n            reward1 = tf.cast(tf.stack(np.array([sample[2][0] for sample in samples])), tf.float32)\n            reward2 = tf.cast(tf.stack(np.array([sample[2][1] for sample in samples])), tf.float32)\n            reward = reward1 + agent.alpha*reward2\n            \n            matrix_ = tf.cast(tf.stack(np.array([sample[3] for sample in samples])), tf.float32)\n            done = tf.cast(tf.stack(np.array([sample[4] for sample in samples])), tf.float32)\n            step = tf.cast(tf.stack(np.array([sample[5] for sample in samples])), tf.float32)\/200\n            \n            active_one = tf.cast(tf.stack(np.array([sample[6] for sample in samples])), tf.float32)\n\n            labels_TD = tf.clip_by_value(agent.label([matrix_, step,active_one]) * (1 - done)+ reward , tf_clip_min, tf_clip_max) \n\n            agent.model_fit.fit([matrix, step, action_one_hot]\n                          , labels_TD, epochs=epochs, verbose=1, batch_size=batch_size, shuffle=False)#no shuffle to keep actions in the same gradient\n\n            agent.update()\n\n            labels_TD = tf.clip_by_value(agent.label([matrix_, step,active_one]) * (1 - done) + reward, tf_clip_min, tf_clip_max) \n            agent.model_fit.fit([matrix, step, action_one_hot]\n                                , labels_TD, epochs=epochs, verbose=1, batch_size=batch_size, shuffle=False)#no shuffle to keep actions in the same gradient\n            agent.update()","ec3a9e05":"env = make(\"hungry_geese\", debug=False)\n\nmemory = deque(maxlen=100_000)\nnb_of_agents = 4\nbatch_size = 256\n\nconvergence = 0.05\nmax_batch = 20_000\/\/batch_size*batch_size\/\/3 #around 20K transitions\ntf_clip_min = -100\ntf_clip_max =  100\n\nagents = []\nalphas = np.linspace(0,0.5,nb_of_agents)\n#alphas = [0.0,0.0,0.0]\n\nfor i in range(nb_of_agents):\n    agents.append(agent_class(alphas[i]))","e2d2bdba":"explorer(100, 0.50)\nall_fit(agents, 1)\nEpoch = 1\n\nfor i in range(1_000):\n    epsilon = 0.25\n    if i > 100: epsilon = 0.10\n    if i > 200: epsilon = 0.05\n    if i > 500: epsilon = 0.02\n    if i > 750: epsilon = 0.01\n\n    explorer(10, epsilon)\n    all_fit(agents, Epoch)\n\n    if i % 50 == 0 and i!= 0:\n        all_fit_update(agents, Epoch)\n\n            \n    if i % 100 == 99:  \n        try:\n            os.mkdir(str(i))\n            for p, agent in enumerate(agents):\n                agent.actor.save_weights(str(i) + \"\/agent.ckp\" + str(p))\n        except:\n            pass\n    \n    if i % 5 == 0:\n        try:\n            for p, agent in enumerate(agents):\n                agent.actor.save_weights('agent.ckp' + str(p))\n        except:\n            print(\"save problem\")\n\n    \n        runlen = one_path(0, True)\n        print(\"\\ \")\n        print(\"episode\" + str(i) + \"--\" + str(runlen))","3a46423f":"%%writefile main.py\n\nimport numpy as np  # linear algebra\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nimport os as os\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action\nimport sys\n\next_folder = '\/kaggle_simulations\/agent\/'\nsys.path.append(ext_folder)\nmodel_name = 'agent.ckp'+ str(1)\n\nAction_dict, Action_invdict = {}, {}\nfor index, action in enumerate(Action):\n    Action_dict[index] = action.name\n    Action_invdict[action.name] = index\n\n\ndef centerize(b):\n    dy, dx = np.where(b[:, :, 0])\n    centerize_y = (np.arange(0, 7) - 3 + dy[0]) % 7\n    centerize_x = (np.arange(0, 11) - 5 + dx[0]) % 11\n\n    b = b[centerize_y, :, :]\n    b = b[:, centerize_x, :]\n\n    return b\n\n\ndef observation_matrix(obs_prev, obs, idx):\n    b = np.zeros((7 * 11, 17), dtype=np.float32)\n\n    # if len(obs['geese'][idx]) == 0:\n    #     return b.reshape(7, 11, -1)\n\n    for p, pos_list in enumerate(obs['geese']):\n        # head position\n        for pos in pos_list[:1]:\n            b[pos, 0 + (p - idx) % 4] = 1\n        # tip position\n        for pos in pos_list[-1:]:\n            b[pos, 4 + (p - idx) % 4] = 1\n        # whole position\n        for pos in pos_list:\n            b[pos, 8 + (p - idx) % 4] = 1\n\n        # previous head position\n        for p, pos_list in enumerate(obs_prev['geese']):\n            for pos in pos_list[:1]:\n                b[pos, 12 + (p - idx) % 4] = 1\n\n                # food\n    for pos in obs['food']:\n        b[pos, 16] = 1\n\n    b = b.reshape(7, 11, -1)\n    b = centerize(b)\n\n    return b\n\n############## MODEL\n\n\ndef snakeConv():\n    i = tf.keras.layers.Input(shape=(7, 11, 17), name=\"matrix\")\n    istep = tf.keras.layers.Input(shape=(1), name=\"step\")\n\n    x = tf.keras.layers.Conv2D(200, 2, activation=\"relu\")(i)\n    x = tf.keras.layers.AveragePooling2D((2,2))(x)\n    x = tf.keras.layers.Conv2D(100, 2, activation=\"relu\")(x)\n    x = tf.keras.layers.AveragePooling2D((2,2))(x)\n\n    x = tf.keras.layers.Flatten()(x)\n    y = tf.reduce_mean(i, axis=[1, 2])\n\n    z = tf.keras.layers.concatenate([x,y, istep])\n    z = tf.keras.layers.Dense(100, activation=\"relu\")(z)\n    z = tf.keras.layers.Dense(100, activation=\"relu\")(z)\n\n    z = tf.keras.layers.Dense(4 * 4)(z)\n    qvalues = tf.keras.layers.Reshape((4, 4))(z)\n\n    #qvalues = tf.clip_by_value(qvalues, -2, 3)\n\n    actor = Model(inputs=[i, istep], outputs=qvalues)\n    actor.compile()\n\n    return actor\n\nclass agent_class():\n    def __init__(self, alpha):\n        self.actor = snakeConv()\n        self.alpha = alpha\n        self.prevState = None\n        self.step = None\n\n\n    def agent_call(self,state,index):\n        if self.prevState == None:matrix      = observation_matrix(state,state,index)\n        else:                     matrix      = observation_matrix(self.prevState,state,index)\n\n        if self.step == None:step=1\n        else:self.step += 1\n        qvalues        = agent1.actor( [matrix.reshape(1,7,11,17), np.array(step).reshape(1, 1) \/ 200] )[0]\n        self.prevState = state\n        return Action_dict[np.argmax(qvalues[0, :])]\n                 \nagent1 = agent_class(0)                                    \nagent1.actor.load_weights(os.path.join(ext_folder, model_name))\n\ndef agent(obs_dict, config_dict):\n    return agent1.agent_call(obs_dict,obs_dict.index)\n","827aaf71":"!tar -czvf submission.tar.gz main.py agent.ckp1.*","ee75569e":"# Main loop","aec18509":"The base of the agent explorer function is from the APEX notebook\nThe base of the observation_matrix, i have seen in many notebook so would not be able to point its origin.","7ad258df":"# The agent class and state encoder","4a71913b":"The idea is then instead of using not only the actions of the active agent but also the one of the 4 others.\n\nTherefore , we are not only looking at the expectation of reward knowing our action but the expectation of reward knowing the actions of the 4 agents. \n\n**Mathematicly, we are estimating the joint probilities of E(state \/ a1,a2,a3,a4) instead of just E(state,a1) over the average of a2,a3,a4**\n\n\n\n","9c04a7b7":"We are still using temporal differences to estimate the transition using a dual deep q learning of 4x4 output.\nThe target network will assume the other agents as well as our will go for the argmax. This time this will be an argmax over 4 actions summed up.\nThe action network will fit to that target using the 4x4 output mulitplied by the action matrix.","04f2d38d":"# Abstract","958bd264":"# reward function","2ff478f2":"Hello fellow kagglers,\n\nThe main point of this workbook, when evaluating transition expectation using off policy method, **if we evaluate only using the action of the current agent, we dont know if the reward is due to our choice or the choice of the other agents.**\n\nFor example,are we winning because the opposite agent went to a wall or because of our agent did something good ?\n\nSo instead of outputting a 4 dimension array , we will evaluate a 4x4 matrix\n\nThere s a few genuine method here. The aggresivity method of the different agent , the method of using a .fit on a reinforcment learning (instead of a custom loop using autogradient method) and the Partial Q learning I want to present.","96b7377a":"# agent Class","70f4b387":"it ouput two values , one for your action and one for the one of your opponent","655ad6db":"# action encoder","8d89ab0b":"# The Import","e4750aa7":"# Explorer and Fit Method"}}