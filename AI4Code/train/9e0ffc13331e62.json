{"cell_type":{"113c0979":"code","a48aa1e1":"code","6869e23c":"code","0d89caf6":"code","dd2a7f5e":"code","f6b9695d":"code","b4deca39":"code","5808626d":"code","9bdc72b4":"code","3ef20a37":"code","69047112":"code","96822773":"code","6f211f52":"code","de8cba08":"code","7d02b546":"code","b47e909f":"code","62bb1bce":"code","d3b7c302":"code","71ffebd3":"code","c5fe205e":"code","173bdbfa":"code","d8715c3f":"code","03d7788a":"code","db134202":"markdown","f8ae1247":"markdown","c1f0ccd3":"markdown","5e9b711a":"markdown","0c6b083c":"markdown","5c264800":"markdown","4a3a5ec7":"markdown","9a77cacf":"markdown","813fb22b":"markdown","c6df7f79":"markdown","14d988bc":"markdown","efb494cd":"markdown","38021011":"markdown"},"source":{"113c0979":"%matplotlib inline\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sqlite3\nimport nltk\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os","a48aa1e1":"import os\nfrom IPython.core.display import display, HTML\n    \nif not os.path.isfile('..\/input\/database.sqlite'):\n    display(HTML(\"\"\"\n        <h3 style='color: red'>Dataset database missing!<\/h3><h3> Please download it\n        <a href='https:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews'>from here on Kaggle<\/a>\n        and extract it to the current directory.\n          \"\"\"))\n    raise(Exception(\"missing dataset\"))\n        ","6869e23c":"con = sqlite3.connect('..\/input\/database.sqlite')\n\nfilter_data=pd.read_sql_query(\"SELECT * FROM Reviews Where Score!=3\", con)","0d89caf6":"filter_data.shape","dd2a7f5e":"#Give reviews 1 (Score>3)\/ 0 (Score<3) \n\ndef partition(x):\n    if x>3:\n        return 1\n    else:\n        return 0\nscore=filter_data['Score']\npositiveNegative = score.map(partition) \n'''\nexplation of map\nlist=[m,n,p]\nfunction=f()\nlist,function----->(map)----->new list,[f(n),f(m),f(p)]\n'''\nfilter_data['Score']=positiveNegative\n\nfilter_data.head(3)","f6b9695d":"final_data=filter_data.drop_duplicates(subset={'UserId','ProfileName','Time','Text'},keep='first', inplace=False)","b4deca39":"final_data.columns\nfinal_data.shape ","5808626d":"final_data=final_data[final_data.HelpfulnessNumerator<=final_data.HelpfulnessDenominator]","9bdc72b4":"final_data.shape ","3ef20a37":"text_0=final_data['Text'][0]\nprint(text_0)\nprint(50*'=')\ntext_1000=final_data['Text'][1000]\nprint(text_1000)\nprint(50*'=')\ntext_10000=final_data['Text'][10000]\nprint(text_10000)\nprint(50*'=')\ntext_15000=final_data['Text'][15000]\nprint(text_15000)\nprint(50*'=')","69047112":"def deconstruted_text(text):\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\",text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\",text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    return text","96822773":"#Set of Stopwords\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","6f211f52":"import re \nfrom tqdm import tqdm\nfrom bs4 import BeautifulSoup\npreprocessed_reviews = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(final_data['Text'].values):\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = deconstruted_text(sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n    preprocessed_reviews.append(sentance.strip())","de8cba08":"count_vect= CountVectorizer()\ncount_vect.fit(preprocessed_reviews)","7d02b546":"print(\"some feature names \", count_vect.get_feature_names()[:10])\nprint('='*50)\n\nfinal_counts = count_vect.transform(preprocessed_reviews)\nprint(\"the type of count vectorizer \",type(final_counts))\nprint(\"the shape of out text BOW vectorizer \",final_counts.get_shape())\nprint(\"the number of unique words \", final_counts.get_shape()[1])","b47e909f":"count_vect = CountVectorizer(ngram_range=(1,2), min_df=10, max_features=5000)\nfinal_bigram_counts = count_vect.fit_transform(preprocessed_reviews)\nprint(\"the type of count vectorizer \",type(final_bigram_counts))\nprint(\"the shape of out text BOW vectorizer \",final_bigram_counts.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_bigram_counts.get_shape()[1])","62bb1bce":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10)\ntf_idf_vect.fit(preprocessed_reviews)\nprint(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names()[0:10])\nprint('='*50)\n\nfinal_tf_idf = tf_idf_vect.transform(preprocessed_reviews)\nprint(\"the type of count vectorizer \",type(final_tf_idf))\nprint(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])","d3b7c302":"# Using Google News Word2Vectors\ni=0\nlist_of_sentance=[]\nfor sentance in preprocessed_reviews:\n    list_of_sentance.append(sentance.split())\n    \nis_your_ram_gt_16g=False\nwant_to_use_google_w2v = False\nwant_to_train_w2v = True\n\nif want_to_train_w2v:\n    # min_count = 5 considers only words that occured atleast 5 times\n    w2v_model=Word2Vec(list_of_sentance,min_count=5,size=50, workers=4)\n    print(w2v_model.wv.most_similar('great'))\n    print('='*50)\n    print(w2v_model.wv.most_similar('worst'))\n    \nelif want_to_use_google_w2v and is_your_ram_gt_16g:\n    if os.path.isfile('GoogleNews-vectors-negative300.bin'):\n        w2v_model=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n        print(w2v_model.wv.most_similar('great'))\n        print(w2v_model.wv.most_similar('worst'))\n    else:\n        print(\"you don't have gogole's word2vec file, keep want_to_train_w2v = True, to train your own w2v \")","71ffebd3":"w2v_words = list(w2v_model.wv.vocab)\nprint(\"number of words that occured minimum 5 times \",len(w2v_words))\nprint(\"sample words \", w2v_words[0:50])","c5fe205e":"# average Word2Vec\n# compute average word2vec for each review.\nsent_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sent in tqdm(list_of_sentance): # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        if word in w2v_words:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n    if cnt_words != 0:\n        sent_vec \/= cnt_words\n    sent_vectors.append(sent_vec)\nprint(len(sent_vectors))\nprint(len(sent_vectors[0]))","173bdbfa":"# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\nmodel = TfidfVectorizer()\nmodel.fit(preprocessed_reviews)\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(model.get_feature_names(), list(model.idf_)))\n# TF-IDF weighted Word2Vec\ntfidf_feat = model.get_feature_names() # tfidf words\/col-names\n# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n\ntfidf_sent_vectors = []; # the tfidf-w2v for each sentence\/review is stored in this list\nrow=0;\nfor sent in tqdm(list_of_sentance): # for each review\/sentence \n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    weight_sum =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        if word in w2v_words and word in tfidf_feat:\n            vec = w2v_model.wv[word]\n#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n            # to reduce the computation we are \n            # dictionary[word] = idf value of word in whole courpus\n            # sent.count(word) = tf valeus of word in this review\n            tf_idf = dictionary[word]*(sent.count(word)\/len(sent))\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n    if weight_sum != 0:\n        sent_vec \/= weight_sum\n    tfidf_sent_vectors.append(sent_vec)\n    row += 1","d8715c3f":"from sklearn.preprocessing import StandardScaler\nprint(final_bigram_counts.shape)\nstd_data = StandardScaler(with_mean = False).fit_transform(final_bigram_counts)\nprint(std_data.shape)\ntype(std_data)\nstd_data=std_data.todense()\nprint(type(std_data))\n","03d7788a":"from sklearn.manifold import TSNE\nfrom sklearn import datasets\nimport pandas as pd\nimport matplotlib.pyplot as plt\nscore=filter_data['Score']\nmodel = TSNE(n_components=2, perplexity=45, learning_rate=100, n_iter = 500, random_state=0)\n\nfor_tsne = model.fit_transform(std_data)\n# if x is a sparse matrix you need to pass it as X_embedding = tsne.fit_transform(x.todense()) , .toarray() will convert the sparse matrix into dense matrix\n\nfor_tsne = np.vstack((for_tsne.T, score)).T\nfor_tsne_df = pd.DataFrame(data=for_tsne, columns=['Dimension_x','Dimension_y','Score'])\nsns.FacetGrid(for_tsne_df, hue=\"Score\", size=10).map(plt.scatter, 'Dimension_x', 'Dimension_y').add_legend()\nplt.title(\"TSNE for Bag of Words\")","db134202":"Let's select only what's of interest to us:","f8ae1247":"# Making predictions over amazon recommendation dataset\n\n## Predictions\nThe purpose of this analysis is to make up a prediction model where we will be able to predict whether a recommendation is positive or negative. In this analysis, we will not focus on the Score, but only the positive\/negative sentiment of the recommendation. \n\nTo do so, we will work on Amazon's recommendation dataset, we will build a Term-doc incidence matrix using term frequency and inverse document frequency ponderation. When the data is ready, we will load it into predicitve algorithms, mainly na\u00efve Bayesian and regression.\n\nIn the end, we hope to find a \"best\" model for predicting the recommendation's sentiment.\n\n## Loading the data\nIn order to load the data, we will use the SQLITE dataset where we will only fetch the Score and the recommendation summary. \n\nAs we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score id above 3, then the recommendation wil be set to \"postive\". Otherwise, it will be set to \"negative\". \n\nThe data will be split into an training set and a test set with a test set ratio of 0.2","c1f0ccd3":"Let's first check whether we have the dataset available:","5e9b711a":"### Applying TNSE on Text BOW vectors\n\n","0c6b083c":"<font size=\"3\">**2-Bi-Grams and n-Grams** <\/font>","5c264800":"<font size=\"5\">**Featurization** <\/font><br \/>\n<br \/><br \/>\n\n<font size=\"3\">**1-Bags of words** <\/font>","4a3a5ec7":"####  TFIDF weighted W2v","9a77cacf":"# Applying TSNE\n<ol> \n    <li> you need to plot 4 tsne plots with each of these feature set\n        <ol>\n            <li>Review text, preprocessed one converted into vectors using (BOW)<\/li>\n            <li>Review text, preprocessed one converted into vectors using (TFIDF)<\/li>\n            <li>Review text, preprocessed one converted into vectors using (AVG W2v)<\/li>\n            <li>Review text, preprocessed one converted into vectors using (TFIDF W2v)<\/li>\n        <\/ol>\n    <\/li>\n   \n<\/ol>","813fb22b":"<font size=\"4\">Text Preprocessing <\/font>\n\nNow that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n\nHence in the Preprocessing phase we do the following in the order below:-\n\n1 Begin by removing the html tags\n\n2 Remove any punctuations or limited set of special characters like , or . or # etc.\n\n3 Check if the word is made up of english letters and is not alpha-numeric\n\n4 Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n\n5 Convert the word to lowercase\n\n6 Remove Stopwords\n\n7 Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)\n\nAfter which we collect the words used to describe positive and negative reviews","c6df7f79":"Drop Duplicate on the basis of 'UserId','ProfileName','Time','Text'","14d988bc":"# Word2Vec \nwe are using pretrained model by google\n ","efb494cd":"We have checked that there is some <br \/br> tags and we have to convertd short word into full meaning word  (eg.\" I'm \"--->I am) ","38021011":"## TF-IDF\n"}}