{"cell_type":{"20bd5abb":"code","84eae458":"code","2ebeb584":"code","f88e56f2":"code","9ab7a6a1":"code","f003b271":"code","6da3a130":"code","f916c7d4":"code","6cd5889c":"code","f9700aa5":"markdown"},"source":{"20bd5abb":"from pathlib import Path\n\nDATA_PATH = Path('\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/')\nJUST_SOME = True # helpful for testing the code with small data","84eae458":"from tqdm.auto import tqdm\n\nimport json\n\ndef iter_texts():\n    \"\"\"\n    Iterate over all directories, all file names, and yield all elements on body_text and abstract\n    \"\"\"\n    dirs = 'comm_use_subset noncomm_use_subset pmc_custom_license biorxiv_medrxiv'.split()\n    for dir in dirs:\n        fnames = (DATA_PATH \/ dir \/ dir).glob('*')\n        for fname in fnames:\n            with fname.open() as f:\n                content = json.load(f)\n                \n            for key in 'abstract body_text'.split():\n                for row in content[key]:\n                    yield row['text']","2ebeb584":"import spacy\n\n# make sure to run python3 -m spacy download en_core_web_sm\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef iter_sents(just_some=False):\n    \"\"\"\n    Use spacy to tokenize what's yielded by iter_sents\n    \"\"\"\n    for i, text in enumerate(iter_texts()):\n        if just_some and i == 1000: break\n        doc = nlp(text)\n        for sent in doc.sents:\n            yield [t.text.lower() for t in sent if not t.is_stop and t.is_alpha and len(t.text) > 1]","f88e56f2":"import json\n\n# dump on a jsonlines file so we do the tokenization just once\nwith open('all_sentences.jl', 'w') as f:\n    for i, sent in enumerate(tqdm(iter_sents(just_some=JUST_SOME))):\n        if i > 0: f.write('\\n')\n        f.write(json.dumps(sent))","9ab7a6a1":"class CachedSentenceIterator:\n    \"\"\"\n    An iterator that is compatible with gensim (you need to be able to iterate this more than once for the epochs to work)\n    \"\"\"\n    def __init__(self, just_some=False, fname='all_sentences.jl'): \n        self.just_some = just_some\n        self.fname = fname\n    \n    def __iter__(self):\n        with open(self.fname) as f:\n            for line in f:\n                yield json.loads(line)\n        ","f003b271":"from gensim.models import Word2Vec\n\nsi = CachedSentenceIterator(just_some=JUST_SOME)\n\nmodel = Word2Vec()\nmodel.build_vocab(sentences=tqdm(si))\nmodel.train(tqdm(si), total_examples=model.corpus_count, epochs=3)","6da3a130":"model.save('covid.w2v')","f916c7d4":"model.wv.most_similar('virus')","6cd5889c":"model.wv.most_similar('coronavirus')","f9700aa5":"# Purpose\n\nI have no background whatsoever on medicine. So just to approach the dataset, I decided to train a Word2Vec so I can get around the jargon\n\nI'm training the model. Will share the trained binaries once it finishes training\n\n# Pretrained model\n\nYou can download the pretrained model from [here](https:\/\/www.kaggle.com\/elsonidoq\/covid19-challenge-trained-w2v-model)\n\nLoad the model with [this code](https:\/\/www.kaggle.com\/elsonidoq\/checkout-the-covid-19-word2vec-model)"}}