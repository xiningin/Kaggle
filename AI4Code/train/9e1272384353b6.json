{"cell_type":{"e7dd1895":"code","507a87d3":"code","07361157":"code","34f8bd31":"code","84eeb6ec":"code","82b7e767":"code","39783112":"code","8dafc49a":"code","f0382880":"code","280082cf":"code","4c12a940":"code","9e854b50":"code","3c349725":"code","9a47bf2d":"code","68ef4e6b":"code","241b0fb9":"code","a6dab3b0":"code","13d5596e":"code","aec4179a":"code","b641b5ab":"code","711d6fee":"code","4d62bbd4":"code","686d5fcc":"code","dc8461cd":"code","96ec2738":"code","acf5d48e":"code","1eea5b78":"code","86e937c8":"code","dee06416":"code","dbcdf32e":"code","5c81d0af":"code","c634faf1":"code","e821d56c":"code","c07c3e84":"code","711726c7":"code","7f8cc196":"code","283d4ad9":"code","85cc99d3":"code","a60bab07":"code","8651d937":"code","eebb757a":"markdown","8057ca4a":"markdown","3fc0892a":"markdown","8facd23b":"markdown","9d582512":"markdown","dc0d71f2":"markdown","2f0632f9":"markdown","d7576575":"markdown","2ad4cf94":"markdown","26eb7372":"markdown","06462403":"markdown","046b33e8":"markdown","216738f5":"markdown","1722ea01":"markdown","9c8d37b7":"markdown"},"source":{"e7dd1895":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.impute import IterativeImputer\nfrom sklearn.utils import resample\n\n#from sklearn.datasets import make_classification","507a87d3":"pd.set_option('display.max_rows', 10)","07361157":"df = pd.read_csv(\"\/kaggle\/input\/widsdatathon2021\/TrainingWiDS2021.csv\", index_col=0)\ndf_test = pd.read_csv(\"\/kaggle\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\", index_col=0)\ndf.info()","34f8bd31":"print(df.describe(include='O').T)","84eeb6ec":"frames = [df,df_test]\njoin_df = pd.concat(frames, keys=['x', 'y'], sort=False)\nassert len(join_df) == len(df) + len(df_test)\nlst = join_df.isna().sum()\/len(join_df)\nto_delete = list(lst[lst>0.6].index)\nprint(to_delete)","82b7e767":"join_df_dropped = join_df.drop(to_delete,axis=1)\n# join_df_dropped","39783112":"df = join_df_dropped.loc['x']\ndf_test = join_df_dropped.loc['y']","8dafc49a":"df_test = df_test.drop('diabetes_mellitus', axis=1)","f0382880":"df.info()","280082cf":"df_test.info()","4c12a940":"def label_encoder_train_test(train, test, additional_cols, prints=True):\n    '''\n    Encode object columns in Train and Test, leaving NAs to be imputed later.\n    Uses Label Encoder from Sklearn\n    Train and Test need to Pandas dataframes.\n    Returns the train and test dataframes with encoded column\n    & a list of the encoded columns\n    '''\n    \n    # Select all columns with type == \"object\"\n    categorical_mask = (train.dtypes == object)\n    train_categorical_columns = train.columns[categorical_mask].tolist()\n    categorical_mask = (test.dtypes == object)\n    test_categorical_columns = test.columns[categorical_mask].tolist()\n    if prints:\n        print(f\"Train: {len(train_categorical_columns)} columns encoded.\",\"\\n\"\n              f\"Test: {len(test_categorical_columns)} columns encoded.\", \"\\n\")\n    \n    for df, cols in zip([train, test], [train_categorical_columns, test_categorical_columns]):\n        for col in cols:\n            ## Encoder\n            encoder = LabelEncoder()\n            \n            ### select all values to encode but NAs (we'll impute these later)\n            fit_by = pd.Series([i for i in df.loc[:,col].unique() if type(i) == str])\n            encoder.fit(fit_by)\n            \n            ### encode the column, leaving NAs untouched\n            df.loc[:,col] = df.loc[:,col].apply(lambda x: encoder.transform([x])[0] \n                                                if type(x) == str else x)\n    \n    \n    ## Encode additional columns (Int)\n    for df, cols in zip([train, test], [additional_cols, additional_cols]):\n        for col in cols:\n            encoder = LabelEncoder()\n            fit_by = list(set(df[col].values))\n            encoder.fit(fit_by)\n\n            df.loc[:,col] = df.loc[:,col].apply(lambda x: encoder.transform([x])[0]\n                                                if type(x) == int else x)\n    \n    train_categorical_columns.extend(additional_cols)\n    test_categorical_columns.extend(additional_cols)\n    if prints:\n        print(f\"Train: {len(additional_cols)} columns additionally encoded.\",\"\\n\"\n              f\"Test: {len(additional_cols)} columns additionally encoded.\", \"\\n\")\n    \n    string_cols = list(set(train_categorical_columns+test_categorical_columns))\n            \n    print(\"Encoding finished.\")\n    return train, test, string_cols","9e854b50":"additional_cols = ['hospital_id','icu_id'] # these are technically categorical values","3c349725":"df_dummies, df_test_dummies, string_cols = label_encoder_train_test(df, df_test, additional_cols, prints=True)","9a47bf2d":"df_dummies.set_index('encounter_id', inplace=True)\ndf_test_dummies.set_index('encounter_id', inplace=True)\nprint(df_dummies.sort_index().info())\nprint(df_test_dummies.sort_index().info())","68ef4e6b":"df_dummies_columns = set(df_dummies.columns)\ndf_test_dummies_columns = set(df_test_dummies.columns)\nmissing_columns_in_test = df_dummies_columns - df_test_dummies_columns\nprint(missing_columns_in_test)\nfor col in missing_columns_in_test:\n    df_test_dummies[col] = 0\n\ndf_dummies_for_imput = df_dummies.drop('diabetes_mellitus', axis=1)\ndf_diabetes_mellitus = df_dummies['diabetes_mellitus']\nprint(df_dummies_for_imput.shape)\n\ndf_test_dummies_for_imput = df_test_dummies.drop('diabetes_mellitus',axis=1)\nprint(df_test_dummies_for_imput.shape)","241b0fb9":"string_cols # Categorical variables that we encoded","a6dab3b0":"from fancyimpute import KNN, IterativeImputer","13d5596e":"def fancy_imputer_train_test(train, test, string_cols, impute_type=\"mice\"):\n    '''Imputes the train and test datasets and returns the result.\n    impute_type: could be 'mice' or 'knn' '''\n    \n    print(\"Imputation has started...\")\n    if impute_type == \"mice\":\n        mice_imputer = IterativeImputer(verbose=2, n_nearest_features=15)\n        train_imputed = mice_imputer.fit_transform(train)\n        test_imputed = mice_imputer.transform(test)\n    \n    elif impute_type == \"knn\":\n        knn_imputer = KNN(verbose=2)\n        train_imputed = knn_imputer.fit_transform(train)\n        test_imputed = knn_imputer.transform(test)\n        \n    train_finished = adjust_imputed_data(imputed_data=train_imputed,\n                                         column_names=train.columns,\n                                         index = train.index,\n                                         string_cols=string_cols)\n    test_finished = adjust_imputed_data(imputed_data=test_imputed,\n                                        column_names=test.columns,\n                                        index = test.index,\n                                        string_cols=string_cols)    \n     \n    print(\"Imputation has finished.\")\n    return train_finished, test_finished\n\ndef adjust_imputed_data(imputed_data, column_names, index, string_cols):\n    '''Adjusts the output of the imputed data.'''\n    data = pd.DataFrame(imputed_data, columns=column_names, index=index)\n\n    # Transform to int all columns that were int\/string in the beginning\n    int64_cols = [col for col in data.columns if data[col].dtype == \"int64\"]\n    int64_cols.extend(string_cols)\n    \n    for col in int64_cols:\n        data[col] = data[col].astype(int)\n        \n    return data","aec4179a":"%time\ntrain, test = fancy_imputer_train_test(train=df_dummies_for_imput,\n                                       test=df_test_dummies_for_imput,\n                                       string_cols=string_cols,\n                                       impute_type=\"mice\")","b641b5ab":"df_dummies = train\ndf_test_dummies = test","711d6fee":"df_dummies['diabetes_mellitus'] = df_diabetes_mellitus","4d62bbd4":"# print(df.isnull().sum())\ndf.loc[:,\"diabetes_mellitus\"].plot(kind='hist')\nplt.show()\nprint(df.diabetes_mellitus.value_counts())\n# Slighly imbalanced, but let's let it go\n\ndef upsampling(df, n_samples):\n    df_majority = df[df['diabetes_mellitus']==0]\n    df_minority = df[df['diabetes_mellitus']==1]\n\n    df_minority_upsampled = resample(df_minority, \n                                     replace=True,     # sample with replacement\n                                     n_samples=n_samples,    # to match majority class\n                                     random_state= 303) # reproducible results\n\n    # Combine majority class with upsampled minority class\n    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n\n    # Display new class counts\n    print(df_upsampled.diabetes_mellitus.value_counts())\n    return df_upsampled","686d5fcc":"df_dummies","dc8461cd":"# df_dummies = upsampling(df_dummies, 62006)","96ec2738":"to_drop = ['hospital_id', 'icu_id']\ndf_dummies = df_dummies.drop(to_drop, axis=1)\ndf_test_dummies = df_test_dummies.drop(to_drop, axis=1)","acf5d48e":"df_dummies","1eea5b78":"X = df_dummies.iloc[:,:-1]\ny = df_dummies.iloc[:,-1]","86e937c8":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)","dee06416":"# xgb_clf = xgb.XGBClassifier()\nxgb_clf = xgb.XGBClassifier(objective='binary:logistic') #tree_method='hist'","dbcdf32e":"xgb_clf.fit(X.values, y.values)","5c81d0af":"df_test_dummies","c634faf1":"preds = xgb_clf.predict_proba(df_test_dummies.values)[:, 1]\noutput = pd.DataFrame(index=df_test_dummies.index)\noutput['diabetes_mellitus'] = preds","e821d56c":"output = output.sort_index()\npd.DataFrame(output).to_csv('output.csv')","c07c3e84":"preds = xgb_clf.predict(X_test.values)\naccuracy = float(np.sum(preds==y_test))\/y_test.shape[0]\nprint(\"accuracy: %f\" % (accuracy))","711726c7":"# predict probability estimates\ny_score = xgb_clf.predict_proba(X_test.values)[:, 1]  # get for positive labels only\nauroc_scores = roc_auc_score(y_test.values, y_score)\nprint(auroc_scores)","7f8cc196":"preds = xgb_clf.predict_proba(X_test.values)[:,1]\nfpr, tpr, _ = roc_curve(y_test, preds)\nauc_score = auc(fpr, tpr)","283d4ad9":"plt.title('ROC Curve')\nplt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([-0.1,1.1])\nplt.ylim([-0.1,1.1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","85cc99d3":"# CV result\n# params_final = {\n#     'max_depth' : 6,\n#     'max_leaves' : 14,\n#     'tree_method' : 'hist',\n#     'objective' : 'reg:logistic',\n#     'grow_policy' : 'lossguide',\n#     'eta' : 0.6,\n# }","a60bab07":"# def train_xgb_model(X_train, X_test, y_train, y_test, params, details=\"default\", prints=True):\n#     '''Trains an XGB and returns the trained model + ROC value.'''\n    \n#     # Create DMatrix - is optimized for both memory efficiency and training speed.\n#     train_matrix = xgb.DMatrix(data = X_train, label = y_train)\n    \n#     # Create & Train the model\n#     model = xgb.train(params, dtrain = train_matrix)\n\n#     # Make prediction\n#     predicts = model.predict(xgb.DMatrix(X_test))\n#     roc = roc_auc_score(y_test.astype('int32'), predicts)\n\n#     if prints:\n#         print(details + \" - ROC: {:.5}\".format(roc))\n    \n#     return model, roc","8651d937":"# model1, roc1 = train_xgb_model(X_train, X_test, y_train, y_test, \n#                                params_final, details=\"Final Model\")","eebb757a":"Let's delete the columns with too many missing values.","8057ca4a":"# Read the data","3fc0892a":"# Split train & test data","8facd23b":"These are categorical variables.","9d582512":"## Categorical Features Encoding","dc0d71f2":"Upsampling","2f0632f9":"# Predict test data","d7576575":"This prediction is using 'real' test data. This result is meant to be for the datathon. The first place on the leaderboard has an auc score of 0.87. This result currently shows an auc score of 0.842.","2ad4cf94":"The 'real' test data of WiDS is unknown. We can check them only by submitting a predicted result on Kaggle. For now, we can think of X_test, y_test more like validation data. We use this train-test split to assess our model internally and plot a ROC curve for the Data Incubator Interview.","26eb7372":"## Missing Value Handling (Column deletion)","06462403":"The below predictions are for internal assessment! Look at the roc curve!","046b33e8":"# Train a XGBoost classifier","216738f5":"# Preprocessing","1722ea01":"## Missing Value Imputation","9c8d37b7":"ROC curve"}}