{"cell_type":{"9ade86ba":"code","fe0eb3fe":"code","4ea91e17":"code","ac4e6ff7":"code","c2313b7e":"code","0d595984":"code","d1fceeec":"code","2df205e9":"code","5e04c3e7":"code","7fde45cc":"code","8117b64f":"code","fa5f1c33":"code","200b885e":"code","82b6833b":"code","51525831":"code","b1d1cf44":"code","52cdd830":"code","c012be73":"code","e3785c72":"code","8636e8f9":"code","8a35e447":"code","c6a81323":"code","d7bb588c":"markdown","9b8c8fd2":"markdown","8c940616":"markdown","926d06d4":"markdown","21ebde81":"markdown","aec9b0b3":"markdown","7a4fa62a":"markdown","28e6be71":"markdown","711d9a60":"markdown","bd0bf5be":"markdown","a5cc07be":"markdown","1e7cb21d":"markdown","5abeee23":"markdown","ba4fc6ef":"markdown","b808e0f9":"markdown","9b8e884d":"markdown","4523e1e4":"markdown","7b003b02":"markdown","6d4096c4":"markdown","bb520dd3":"markdown"},"source":{"9ade86ba":"import numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport random\nimport pandas as pd\nimport scipy.stats as stat\nimport seaborn as sns\nimport os\nimport pandas\nimport sklearn\n\nfrom IPython.display import Image\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Para ter repetibilidade nos resultados\nrandom_state = 1\n\n# Tratar valores infinitos como np.NaN\npandas.options.mode.use_inf_as_na = True\n\n# IMPORTANTE para tornar figuras interativas\n%matplotlib notebook\n\n# Tamanho padr\u00e3o das figuras\nfigsize=(10,6)\n\n# Verifica\u00e7\u00e3o do local para carga de dados\npath = os.environ['PATH']\n\nif path.startswith('C'):\n    IN_KAGGLE = False\nelse:\n    IN_KAGGLE = True\n    \n\n# Bibliotecas espec\u00edficas do livro Introduction to Machine Learning with Python\n# https:\/\/github.com\/amueller\/introduction_to_ml_with_python\n# pip install mglearn\n\nimport mglearn\n\n\n# Configura\u00e7\u00e3o do n\u00famero de linhas e colunas a serem apresentadas em listagens\npd.set_option('display.max_row', 1000)\n\npd.set_option('display.max_columns', 50)\n","fe0eb3fe":"os.listdir('..\/input')","4ea91e17":"# Fun\u00e7\u00e3o de convers\u00e3o de dados copiada de https:\/\/github.com\/shakedzy\/dython\/blob\/master\/dython\/_private.py\n# Autor Shaked Zychlinski\n\ndef convert(data, to):\n    converted = None\n    if to == 'array':\n        if isinstance(data, np.ndarray):\n            converted = data\n        elif isinstance(data, pd.Series):\n            converted = data.values\n        elif isinstance(data, list):\n            converted = np.array(data)\n        elif isinstance(data, pd.DataFrame):\n            converted = data.as_matrix()\n    elif to == 'list':\n        if isinstance(data, list):\n            converted = data\n        elif isinstance(data, pd.Series):\n            converted = data.values.tolist()\n        elif isinstance(data, np.ndarray):\n            converted = data.tolist()\n    elif to == 'dataframe':\n        if isinstance(data, pd.DataFrame):\n            converted = data\n        elif isinstance(data, np.ndarray):\n            converted = pd.DataFrame(data)\n    else:\n        raise ValueError(\"Unknown data conversion: {}\".format(to))\n    if converted is None:\n        raise TypeError('cannot handle data conversion of type: {} to {}'.format(type(data),to))\n    else:\n        return converted","ac4e6ff7":"from sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\n\ndef redes_neurais_regressao(X_, Y_, to_scale=True):\n\n    X_ = convert(X_, 'array')\n        \n    Y_ = convert(Y_, 'array')\n    \n    # Transforma Y em array 1-D\n    #Y_ = np.ravel(Y_)\n    \n    if to_scale:\n        # Escala vari\u00e1veis\n        scaler = MinMaxScaler(feature_range=(0, 1))\n\n        X_escale = scaler.fit_transform(X_) \n        Y_escale = scaler.fit_transform(Y_) \n    else:\n        X_escale = X_\n        Y_escale = Y_\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        X_escale, Y_escale, test_size=0.1, random_state=random_state,shuffle =True)\n\n    estimatorNN = MLPRegressor(\n                              learning_rate = 'adaptive',\n                              random_state = random_state,\n                              verbose=False,\n                                max_iter = 200,\n                            hidden_layer_sizes = [100,50,40,30,20,10],   \n                    solver = 'adam',\n                    alpha = 0.0001,\n                    activation = 'relu'\n                            )\n\n    estimatorNN.fit(x_train,y_train)\n    \n    plt.subplots(figsize=figsize)\n    plt.plot(range(len(y_test)), y_test,'ro')\n    plt.plot(range(len(y_test)), estimatorNN.predict(x_test),'b*')\n    \n\n    plt.ylabel('Estimativa')\n    plt.title('Estimativa (*) X real (o)')\n    plt.grid(True)\n    plt.show()\n    \n    mean_error = mean_absolute_error(y_test, estimatorNN.predict(x_test))\n    print('\\nErro {}'.format(mean_error))\n    \n    mean_s_error = mean_squared_error(y_test, estimatorNN.predict(x_test))\n    print('\\nErro {}'.format(mean_s_error))\n    \n    r2 = r2_score(y_test, estimatorNN.predict(x_test)) \n    print('\\nR2 Score {}'.format(r2))\n    \n    return estimatorNN,r2","c2313b7e":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\n\ndef redes_neurais_classificacao(X_, Y_, to_scale=True):\n\n    X_ = convert(X_, 'array')\n        \n    Y_ = convert(Y_, 'array')\n    \n    # Transforma Y em array 1-D\n    Y_ = np.ravel(Y_)\n    \n    if to_scale:\n        # Escala vari\u00e1veis\n        scaler = MinMaxScaler(feature_range=(0, 1))\n\n        X_escale = scaler.fit_transform(X_) \n        #Y_escale = scaler.fit_transform(Y_) \n    else:\n        X_escale = X_\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        X_escale, Y_, test_size=0.1, random_state=random_state,shuffle =True)\n\n    estimatorNN = MLPClassifier(\n                              learning_rate = 'adaptive',\n                              random_state = random_state,\n                              verbose=False,\n                                max_iter = 200,\n                            hidden_layer_sizes = [100,50,40,30,20,10],   \n                    solver = 'adam',\n                    alpha = 0.0001,\n                    activation = 'relu'\n                            )\n\n    estimatorNN.fit(x_train,y_train)\n    \n    plt.subplots(figsize=figsize)\n    plt.plot(range(len(y_test)), y_test,'ro')\n    plt.plot(range(len(y_test)), estimatorNN.predict(x_test),'b*')\n    \n\n    plt.ylabel('Estimativa')\n    plt.title('Estimativa (*) X real (o)')\n    plt.grid(True)\n    plt.show()\n    \n    # TN FP\n    # FN TP\n    confusion = confusion_matrix(y_test, estimatorNN.predict(x_test))\n    print(\"\\nConfusion matrix:\\n{}\".format(confusion))\n    \n    f1 = f1_score(y_test, estimatorNN.predict(x_test), average ='micro')\n    print(\"\\nf1 score: {:.2f}\".format( f1   ))\n    \n    erro = np.sum(np.abs(estimatorNN.predict(x_test)-y_test))\/len(y_test)\n    print('\\nErro {}'.format(erro))\n    \n    \n    print(classification_report(y_test, estimatorNN.predict(x_test),\n        target_names=[\"Falso\", \"Positivo\"]))\n    \n    return estimatorNN,erro","0d595984":"from sklearn.tree import DecisionTreeRegressor\n\ndef arvore_regressao(X_, Y_, to_scale=True):\n    \n    X_ = convert(X_, 'array')\n        \n    Y_ = convert(Y_, 'array')\n    \n    # Transforma Y em array 1-D\n    Y_ = np.ravel(Y_)\n    \n    if to_scale:\n        # Escala vari\u00e1veis\n        scaler = MinMaxScaler(feature_range=(0, 1))\n\n        X_escale = scaler.fit_transform(X_) \n        #Y_escale = scaler.fit_transform(Y_) \n    else:\n        X_escale = X_\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        X_escale, Y_, test_size=0.1, random_state=random_state,shuffle =True)\n    \n    estimatorTree = DecisionTreeRegressor(max_depth=5, random_state = random_state)\n    estimatorTree.fit(x_train,y_train)\n    \n    plt.subplots(figsize=figsize)\n    plt.plot(range(len(y_test)), y_test,'ro')\n    plt.plot(range(len(y_test)), estimatorTree.predict(x_test),'b*')\n    \n\n    plt.ylabel('Estimativa')\n    plt.title('Estimativa (*) X real (o)')\n    plt.grid(True)\n    plt.show()\n    \n    print('Import\u00e2ncias {}'.format(estimatorTree.feature_importances_))\n    \n    mean_error = mean_absolute_error(y_test, estimatorTree.predict(x_test))\n    print('\\nErro {}'.format(mean_error))\n    \n    mean_s_error = mean_squared_error(y_test, estimatorTree.predict(x_test))\n    print('\\nErro {}'.format(mean_s_error))\n    \n    r2 = r2_score(y_test, estimatorTree.predict(x_test)) \n    print('\\nR2 Score {}'.format(r2))\n    \n    return estimatorTree,r2\n    \n","d1fceeec":"from sklearn.tree import DecisionTreeClassifier\n\ndef arvore_classificacao(X_, Y_, to_scale=True):\n    \n    X_ = convert(X_, 'array')\n        \n    Y_ = convert(Y_, 'array')\n    \n    # Transforma Y em array 1-D\n    Y_ = np.ravel(Y_)\n    \n    if to_scale:\n        # Escala vari\u00e1veis\n        scaler = MinMaxScaler(feature_range=(0, 1))\n\n        X_escale = scaler.fit_transform(X_) \n        #Y_escale = scaler.fit_transform(Y_) \n    else:\n        X_escale = X_\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        X_escale, Y_, test_size=0.1, random_state=random_state,shuffle =True)\n    \n    estimatorTree = DecisionTreeClassifier(max_depth=5, random_state = random_state)\n    estimatorTree.fit(x_train,y_train)\n    \n    plt.subplots(figsize=figsize)\n    plt.plot(range(len(y_test)), y_test,'ro')\n    plt.plot(range(len(y_test)), estimatorTree.predict(x_test),'b*')\n    \n\n    plt.ylabel('Estimativa')\n    plt.title('Estimativa (*) X real (o)')\n    plt.grid(True)\n    plt.show()\n\n    \n    \n    print('Import\u00e2ncias {}'.format(estimatorTree.feature_importances_))\n    \n    confusion = confusion_matrix(y_test, estimatorTree.predict(x_test))\n    print(\"\\nConfusion matrix:\\n{}\".format(confusion))\n    \n    f1 = f1_score(y_test, estimatorNN.predict(x_test), average ='micro')\n    print(\"\\nf1 score: {:.2f}\".format( f1   ))\n    \n    erro = np.sum(np.abs(estimatorTree.predict(x_test)-y_test))\/len(y_test)\n    print('\\nErro {}'.format(erro))\n    \n    \n    print(classification_report(y_test, estimatorTree.predict(x_test),\n        target_names=[\"Falso\", \"Positivo\"]))\n    \n    return estimatorTree,erro\n    \n    ","2df205e9":"if IN_KAGGLE:\n    world_happiness = pd.read_csv(\"..\/input\/world-happiness\/2016.csv\")\nelse:\n    world_happiness = pd.read_csv(\"2016.csv\")\n\n# Conjunto completo\nworld_happiness = world_happiness.loc[:,['Country', 'Region', 'Happiness Rank', 'Happiness Score',\n       'Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)',\n       'Freedom', 'Trust (Government Corruption)', 'Generosity',\n       'Dystopia Residual']]\n\n\n\n#world_happiness = shuffle(world_happiness).reset_index(drop=True)\n\n# Conjunto resumido para treinamento de modelos\nworld_happiness_resumido = world_happiness.loc[:,[ 'Happiness Score',\n       'Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)',\n       'Freedom', 'Trust (Government Corruption)', 'Generosity']]\n\n# Cria vari\u00e1veis para treinamento de modelos\n\ncolunas_fonte = [ \n       'Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)',\n       'Freedom', 'Trust (Government Corruption)', 'Generosity'\n]\n\ncolunas_objetivo = [ \n       'Happiness Score'\n]\n\nworld_happiness_resumido_X = world_happiness_resumido.loc[:,colunas_fonte] \nworld_happiness_resumido_Y = world_happiness_resumido.loc[:,colunas_objetivo]\n\n\nworld_happiness.head(35)","5e04c3e7":"if IN_KAGGLE:\n    tips = pd.read_csv('..\/input\/snstips\/tips.csv')\n    if 'Unnamed: 0' in tips.columns:\n        tips.drop(['Unnamed: 0'], inplace=True, axis=1)\nelse:\n    tips = sns.load_dataset('tips')\n\ntips.head()\n","7fde45cc":"from sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer()\n\ncancer_data = cancer['data']\n# 1 benigno, 0 maligno\ncancer_target = cancer['target']\ncancer_target_names  = cancer['target_names']\ncancer_feature_names = cancer['feature_names']","8117b64f":"cancer_data_DF = pd.DataFrame(cancer_data,columns=cancer_feature_names) \ncancer_data_DF.head()","fa5f1c33":"cancer_target_DF = pd.DataFrame(cancer_target,columns=['target']) \ncancer_target_DF.head()","200b885e":"# fonte Introduction to Machine Learning with Python\n# by Andreas C. M\u00fcller and Sarah Guido\n\nmglearn.plots.plot_kmeans_algorithm()\n","82b6833b":"from sklearn.cluster import KMeans\nscaler = StandardScaler()\nscaler.fit(world_happiness_resumido_X)\nworld_happiness_resumido_X_scaled = scaler.transform(world_happiness_resumido_X)\n\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(world_happiness_resumido_X_scaled)\n\n\nworld_happiness['Cluster'] = y_pred\n\nworld_happiness.head(10)","51525831":"\n\nworld_happiness.plot.scatter(x='Cluster',y='Happiness Score')","b1d1cf44":"f, (ax) = plt.subplots(1, 1, figsize=(12, 4))\nf.suptitle(' ', fontsize=14)\n\nsns.boxplot(x=\"Cluster\", y=\"Happiness Score\", data=world_happiness,  ax=ax)\nax.set_xlabel(\" \",size = 12,alpha=0.8)\nax.set_ylabel(\" \",size = 12,alpha=0.8)","52cdd830":"from pandas.tools.plotting import parallel_coordinates\nfig, ax = plt.subplots(1, 1, figsize=(19,10))\nparallel_coordinates(frame=world_happiness, class_column='Cluster', color = ('r','g','b','y'), ax = ax, cols=['Happiness Score',\n       'Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)',\n       'Freedom', 'Trust (Government Corruption)', 'Generosity',\n       'Dystopia Residual'])","c012be73":"from sklearn.metrics.cluster import silhouette_score\n\nsilhouette_score(world_happiness_resumido_X_scaled, y_pred)","e3785c72":"# C\u00f3digo copiado de https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n# com altera\u00e7\u00f5es\nfrom sklearn.metrics import silhouette_samples\nimport matplotlib.cm as cm\n\nX = world_happiness_resumido_X_scaled\n\nrange_n_clusters = [2, 3, 4, 5, 6]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) \/ n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\nplt.show()","8636e8f9":"from sklearn.cluster import DBSCAN\n","8a35e447":"dbscan = DBSCAN()\nclusters = dbscan.fit_predict(world_happiness_resumido_X)\nworld_happiness['Cluster_DBSCAN'] = clusters\nworld_happiness.plot.scatter(x='Cluster_DBSCAN',y='Happiness Rank')","c6a81323":"from sklearn.cluster import KMeans\nfrom sklearn.cluster import KMeans\nscaler = StandardScaler()\nscaler.fit(cancer_data_DF)\ncancer_data_DF_scaled = scaler.transform(cancer_data_DF)\ny_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(cancer_data_DF_scaled)\n\n\nplt.subplots(figsize=figsize)\nplt.plot(cancer_target_DF.values, y_pred,'b.')\nplt.plot(range(len(y_pred)), cancer_target_DF.values,'r.')\n\n\nplt.ylabel('Estimativa')\nplt.title('Estimativa (*) X real (o)')\nplt.grid(True)\nplt.show()\n\nconfusion = confusion_matrix(y_pred, cancer_target_DF.values)\nprint(\"\\nConfusion matrix:\\n{}\".format(confusion))","d7bb588c":"## Dados de exemplo\n\nWorld happiness report (http:\/\/worldhappiness.report\/).\n\nSomente vari\u00e1veis num\u00e9ricas","9b8c8fd2":"# Refer\u00eancias\n\nLivros usados como refer\u00eancia:\n\nIntroduction to Machine Learning with Python\n\nPython Data Science Handbook (https:\/\/www.oreilly.com\/library\/view\/python-data-science\/9781491912126\/)\n\nVisualiza\u00e7\u00e3o:\n\nhttps:\/\/python-graph-gallery.com\/\n\nhttp:\/\/www.apnorton.com\/blog\/2016\/12\/19\/Visualizing-Multidimensional-Data-in-Python\/\n\nhttps:\/\/towardsdatascience.com\/the-art-of-effective-visualization-of-multi-dimensional-data-6c7202990c57\n\nhttps:\/\/www.oreilly.com\/library\/view\/python-data-science\/9781491912126\/ch04.html\n\nhttps:\/\/matplotlib.org\/mpl_toolkits\/mplot3d\/tutorial.html","8c940616":"# 2- Carga de dados\n\n\n","926d06d4":"Data set de gorgetas com vari\u00e1veis categ\u00f3ricas","21ebde81":"## K-Means\n\nCria agrupamentos de forma a minimizar as dist\u00e2ncias entreo os pontos pertencentes a cada cluster e seu centroide (ponto central do cluster).\n\nAbaixo exemplo do funcionamento do algoritmo. Pontos iniciais s\u00e3o aleatoriamente escolhidos como centros dos clusteres e os demais s\u00e3o atribu\u00eddos aos custers por proximidade. Os centros dos clusters formados s\u00e3o usados como novos centros e nova rodada de reposicionamento ocorre. O processo continua at\u00e9 que a diferen\u00e7a de dist\u00e2ncia entre os novos centros e os centros anteriores seja menos que um limite.","aec9b0b3":"# 1- Importa\u00e7\u00e3o de bibliotecas e fun\u00e7\u00f5es gerais usadas no caderno","7a4fa62a":"## O que ser\u00e1 tratado no curso\n\n- Clustering\n\n\n","28e6be71":"## Treinamento de \u00e1rvore de decis\u00e3o para regress\u00e3o\n\nUsada ao longo do caderno para testar efeitos da redu\u00e7\u00e3o de dimensionalidade\n\nhttp:\/\/scikit-learn.org\/stable\/auto_examples\/tree\/plot_tree_regression.html","711d9a60":"## 10- Clusteriza\u00e7\u00e3o\n\nClusteriza\u00e7\u00e3o \u00e9 o processo de forma\u00e7\u00e3o de agrupamentos (clusters) de dados baseados em suas caracter\u00edsticas, normalmente a geometria.\n\nS\u00e3o algoritmos em geral n\u00e3o supervisionados, que recebem como par\u00e2metro o n\u00famero de clusters a serem formados e dividem os dados em agrupamentos que maximizam algum crit\u00e9rio de separa\u00e7\u00e3o.\n\nUma vez treinado o algoritmo, pode ser usado para estudo das caracter\u00edsticas de cada cluster (ponto central, dispers\u00e3o, etc) e para estimar a qual cluster novos pontos, n\u00e3o usados no treinamento, pertencem.\n\nSendo, em geral, n\u00e3o supervisionados, torna-se dif\u00edcil medir a qualidade dos clusters formados. O link a seguir traz a compara\u00e7\u00e3o de alguns algoritmos X diferentes geometrias de pontos\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/clustering.html","bd0bf5be":"## Treinamento de \u00e1rvore de decis\u00e3o para classifica\u00e7\u00e3o\n\nUsada ao longo do caderno para testar efeitos da redu\u00e7\u00e3o de dimensionalidade\n\nhttp:\/\/scikit-learn.org\/stable\/auto_examples\/tree\/plot_tree_Classifier.html","a5cc07be":"## Avalia\u00e7\u00e3o de clusteriza\u00e7\u00e3o\n\nSe houver algum crit\u00e9rio (coluna) que possa ser usada para julgar a clusteriza\u00e7\u00e3o, a plotagem dos clusters X vari\u00e1vel de an\u00e1lise \u00e9 uma \u00f3tima forma de avalia\u00e7\u00e3o.\n\nNo problema em quest\u00e3o, h\u00e1 um crit\u00e9rio para julgamento da clusteriza\u00e7\u00e3o: a medida de felicidade.\n\nAssim, a compara\u00e7\u00e3o das medidas de felicidade com os clusters formados d\u00e1 uma ideia da qualidade do algoritmo.\n\nA clusteriza\u00e7\u00e3o deve ter sido capaz de separar pa\u00edses com n\u00edveis semelhantes de felicidade nos mesmos clusters. A quest\u00e3o aqui \u00e9 a separa\u00e7\u00e3o dos clusters, dado que n\u00e3o h\u00e1 crit\u00e9rio claro de separa\u00e7\u00e3o no \u00edndice.\n\n","1e7cb21d":"## Treinamento de rede neural para classifica\u00e7\u00e3o\n\nUsada ao longo do caderno para testar efeitos da redu\u00e7\u00e3o de dimensionalidade\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html","5abeee23":"Dados sobre tumores (somente informa\u00e7\u00f5es num\u00e9ricas)\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_breast_cancer.html","ba4fc6ef":"## Avalia\u00e7\u00e3o de clusteriza\u00e7\u00e3o\n\n\nSilhouette score calcula a reala\u00e7\u00e3o entre a dist\u00e2ncia das amostras para os demais pontos de seu cluster X a dist\u00e2ncia da amostra para o cluster mais pr\u00f3ximo.\n\nVaria entre -1 (pior) e 1 (melhor)","b808e0f9":"## Avalia\u00e7\u00e3o de clusteriza\u00e7\u00e3o\n\nCaso n\u00e3o haja crit\u00e9rio para medir a qualidade da separa\u00e7\u00e3o, um recurso \u00e9 analisar a separabilidade dos clusters pela an\u00e1lise de suas caracter\u00edsticas","9b8e884d":"## Treinamento de rede neural para regress\u00e3o\n\nUsada ao longo do caderno para testar efeitos da redu\u00e7\u00e3o de dimensionalidade\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPRegressor.html","4523e1e4":"#### <br>\n<font size=\"8\" color=\"red\">EXERC\u00cdCIO<\/font>\n\nRealize a an\u00e1lise acima com clusteriza\u00e7\u00e3o KMeans para cancer_data_DF_scaled\n","7b003b02":"<font size=\"10\" color=\"black\">Clusteriza\u00e7\u00e3o<\/font>\n\nEduardo Chaves Ferreira\n\n","6d4096c4":"## Carrega dados para exerc\u00edcio\n","bb520dd3":"## DBSCAN\n\nDBScan procura \u00e1reas de maior e menor densidade de pontos, utilizando tal crit\u00e9rio para forma\u00e7\u00e3o de clusters.\n\nDiferente de KMeans, que busca proximidade, DBScan se preocupa somente com densidade, o que o torna excelente para defini\u00e7\u00e3o de cluster irregulares.\n\nTamb\u00e9m diferente de KMeans, n\u00e3o \u00e9 passada informa\u00e7\u00e3o sobre o n\u00famero de clusters, o que pode levar a resultados n\u00e3o desejados, como no pr\u00f3ximo exemplo."}}