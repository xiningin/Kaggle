{"cell_type":{"2e17b5bf":"code","5f64f458":"code","936b0b7f":"code","b664f253":"code","e4093f9a":"code","c30b4dcc":"code","fc9013ad":"code","9ec43d9e":"code","a989ad89":"code","861f1480":"code","d2d9305b":"code","d5e64923":"code","784091a8":"code","507a2e88":"code","0d13e5c9":"code","32039981":"code","8b537000":"code","6276ab67":"code","0ce695df":"code","ef0821cc":"code","432f1785":"code","6d3c4461":"code","45ebc0e7":"code","85f019b0":"code","3493b28c":"code","3de98e71":"code","c8e8854a":"code","9f9536d2":"markdown","5485e9af":"markdown","db7e97e6":"markdown","27800981":"markdown","d9bd1938":"markdown","41c82698":"markdown","52adbc4e":"markdown","8659e788":"markdown","c092d885":"markdown","36c422de":"markdown","62c5b4a9":"markdown","d8121b8a":"markdown","78621227":"markdown","a5c10d8d":"markdown","3e1ff9b7":"markdown","181a5236":"markdown","b7a84245":"markdown","d52b7b47":"markdown","f70de212":"markdown","a441b1b6":"markdown","0d9d9283":"markdown","3ecadd4b":"markdown","5686afeb":"markdown","c40c4f5a":"markdown","2acd4b40":"markdown"},"source":{"2e17b5bf":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\n\ntrain = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv', index_col = 'Id')\ntest = pd.read_csv('\/kaggle\/input\/learn-together\/test.csv', index_col = 'Id')","5f64f458":"train.head()","936b0b7f":"train.describe().T","b664f253":"%matplotlib inline \nimport matplotlib.pyplot as plt\n\ntrain.loc[:,'Elevation':'Horizontal_Distance_To_Fire_Points'].hist(bins = 50, figsize=(20, 15))\nplt.show()","e4093f9a":"train.loc[:,'Wilderness_Area1':'Wilderness_Area4'].hist(bins = 50, figsize=(20, 15))\nplt.show()","c30b4dcc":"train.loc[:,'Soil_Type1':'Soil_Type40'].hist(bins = 50, figsize=(20, 15))\nplt.show()","fc9013ad":"from sklearn.model_selection import train_test_split\n\n# Split into validation and training data, set to random_state 1\ntrain_set, test_set = train_test_split(train, test_size = 0.20, random_state = 1)","9ec43d9e":"## make training set\n# Create target object and call it y\ny_train = train_set.Cover_Type\nX_train = train_set.drop('Cover_Type', axis = 1)\n\n# make test set\ny_test = test_set.Cover_Type\nX_test = test_set.drop('Cover_Type', axis = 1)\n\n# make final test set\nX_final_test = test.copy()","a989ad89":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\n    \n# make a function to extract columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y = None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values","861f1480":"# here you can select your favorite numerical features\nnum_attribs = list(train_set.iloc[:,0:10].columns)\n\n# features that should be removed, they were not present in train set\nmy_list = ['Soil_Type7', 'Soil_Type15']\n\n# here you can select your favorite binary features\ncat_attribs = list(train_set.iloc[:,10:54].columns)\ncat_attribs = [e for e in cat_attribs if e not in (my_list)]","d2d9305b":"# make pipeline for numerical features\nnum_pipeline = Pipeline([('selector', DataFrameSelector(num_attribs)),\n                         ('std_scaler', StandardScaler(),\n                        )])\n\ncat_pipeline = Pipeline([('Selector', DataFrameSelector(cat_attribs))])\n\n# combine both pipelines\nfrom sklearn.pipeline import FeatureUnion\n\nfull_pipeline = FeatureUnion(transformer_list = [('num_pipeline', num_pipeline),\n                                                 ('cat_pipeline', cat_pipeline)])","d5e64923":"# run the pipeline to prepare the train data\nX_train_prepared = full_pipeline.fit_transform(X_train)\n\n# run the pipeline to prepare the test data\nX_test_prepared = full_pipeline.transform(X_test)\n\n# run the pipeline to prepare the final test data\nX_final_test_prepared = full_pipeline.transform(X_final_test)","784091a8":"X_train_prepared.shape","507a2e88":"X_test_prepared.shape","0d13e5c9":"X_test_prepared.shape","32039981":"from sklearn.ensemble import RandomForestClassifier\nimport warnings\n\n# to remove warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# fit the model with default options\nrf = RandomForestClassifier(random_state = 0)\nrf.fit(X_train_prepared, y_train)","8b537000":"from sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\nscores_rf = cross_val_score(rf, X_train_prepared, y_train, cv=10, scoring='accuracy')\n\n# Get the mean accuracy score\nprint(\"Average accuracy score random forest model (across experiments):\")\nprint(scores_rf.mean())","6276ab67":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\ny_test_predict = rf.predict(X_test_prepared)\n\n# make a confusion matrix\nconf_mx_test = confusion_matrix(y_test, y_test_predict)\n\n# make a normalized confusion matrix\nrow_sums = conf_mx_test.sum(axis = 1, keepdims = True)\nnorm_conf_mx = conf_mx_test \/ row_sums","0ce695df":"import seaborn as sns\n\nf, axes = plt.subplots(1, 2, figsize=(16, 8), sharex=True, sharey=True)\n\n# names for labeling\nalpha = ['Spruce\/Fir', 'Lodgehole Pine', 'Ponderosa Pine', 'Cottonwood\/Willow', 'Aspen', 'Douglas\/Fir', 'Krummholz']\n\nsns.heatmap(conf_mx_test, annot=True, xticklabels=alpha, yticklabels = alpha, cbar=False, ax=axes[0])\n\nsns.heatmap(norm_conf_mx, annot=True, xticklabels=alpha, yticklabels = alpha, cbar=False, ax=axes[1])","ef0821cc":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rf, random_state=1).fit(X_train_prepared, y_train)\n\neli5.show_weights(perm, feature_names = num_attribs + cat_attribs, top = 60)","432f1785":"#from sklearn.model_selection import GridSearchCV\n\n#param_grid = [\n#    {'n_estimators': [150, 200, 250, 300], 'max_features': [4, 8, 16, 32]},\n#    {'bootstrap': [False], 'n_estimators':[150, 200, 250, 300], 'max_features':[4, 8, 16, 32]}\n#]\n\n#rf_final = RandomForestClassifier()\n\n#grid_search = GridSearchCV(rf_final, param_grid, cv = 5, scoring = 'accuracy')\n\n#grid_search.fit(X_train_prepared, y_train)","6d3c4461":"#grid_search.best_params_\n\n#{'bootstrap': False, 'max_features': 16, 'n_estimators': 300}","45ebc0e7":"# fit the model with optimized parameters\nrf_final = RandomForestClassifier(bootstrap=False, n_estimators = 300, \n                                      max_features = 16, random_state = 0)\n\n#making the model using cross validation\nscores_rf = cross_val_score(rf_final, X_train_prepared, y_train, cv=10, scoring='accuracy')\n\n# and get scores\nprint(\"Average accuracy score (across experiments):\")\nprint(scores_rf.mean())\n\n# 0.783\n# 0.816","85f019b0":"rf_final.fit(X_train_prepared, y_train)\n\n# make predictions using our model\ny_test_predict = rf_final.predict(X_test_prepared)","3493b28c":"# evaluate the results\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(y_test_predict, y_test) # 0.850","3de98e71":"# make a confusion matrix\nconf_mx_test = confusion_matrix(y_test, y_test_predict)\n\n# make a normalized confusion matrix\nrow_sums = conf_mx_test.sum(axis = 1, keepdims = True)\nnorm_conf_mx = conf_mx_test \/ row_sums\n\n# visualize confusion matrices\nf, axes = plt.subplots(1, 2, figsize=(16, 8), sharex=True, sharey=True)\n\n# names for labeling\nalpha = ['Spruce\/Fir', 'Lodgehole Pine', 'Ponderosa Pine', 'Cottonwood\/Willow', 'Aspen', 'Douglas\/Fir', 'Krummholz']\n\nsns.heatmap(conf_mx_test, annot=True, xticklabels=alpha, yticklabels = alpha, cbar=False, ax=axes[0])\n\nsns.heatmap(norm_conf_mx, annot=True, xticklabels=alpha, yticklabels = alpha, cbar=False, ax=axes[1])","c8e8854a":"# make predictions using the model\npredictions_test_final = rf_final.predict(X_final_test_prepared)\n\n# Save test predictions to file\noutput = pd.DataFrame({'ID': test.index,\n                       'Cover_Type': predictions_test_final})\n\noutput.to_csv('submission.csv', index=False)","9f9536d2":"Elevation and horizontal distance to roadways, fire points and hydroloy are the most important features. Features with an permutation score of zero or zero have no impact. ","5485e9af":"In the code cell below you can select your favorite columns:","db7e97e6":"# 5. evaluating the random forest model","27800981":"The first confusion matrix represents the actual numbers and the second confusion matrix represents the error rates. Each row in a confusion matrix represents an actual Cover_Type, while each column represents a predicted Cover_Type. \n\nFrom the plot it can be observed that the model has difficulties distinguising between Spruce\/Fir and Lodgehole Pine. It can also be observed that sometimes Douglas\/Fir gets confused with Ponderosa Pine.  The model also has some difficulties distinguising between Spruce\/Fir and Krummholz.\n\nConclusion: for improving the model it is important to find features that distinguish between Spruce\/Fir and Lodgehole Pine.","d9bd1938":"## 5.3 Features importance","41c82698":"For data preprocessing I made a pipeline:","52adbc4e":"Soil_Type attributes are also binary features. In some cases there is no patch with a particular Soil type. This is the case for Soil_Type7 and Soil_Type15. These attributes need to be removed from the data before analysis. ","8659e788":"The results are: {'bootstrap': False, 'max_features': 16, 'n_estimators': 300} Use these parameters to build a model with optimized parameters.","c092d885":"# 2. Import data and have quick look","36c422de":"Compared to the default model, you see that the optimized model shows some improvement in classification.","62c5b4a9":"The wilderness attributes are binary features, they are coded 0 and 1. A 1 means that the patch is present in that particular wilderness area.  ","d8121b8a":"From the histograms it can be observed that the attributes have different scales. For example the attribute Aspect ranges from 0 to 350 degrees, and the attribute elevation ranges from 1800 to 3800 meters. This problem can be addressed with feature scaling. Many histograms are also tail heavy, they extend much farther to to right of the median than to the left. Later these attributes will be transformed to achieve a more bell-shaped distribution.  ","78621227":"# 8. Make final predictions","a5c10d8d":"# 4 Running the model","3e1ff9b7":"# 6. Grid search for optimizing the parameters\n\nTo optimize the random forest classifier model it is needed to find the optimal set of parameters. This process is called [hyperparameter optimization](http:\/\/https:\/\/en.wikipedia.org\/wiki\/Hyperparameter_optimization). Here I used grid search to find the set of optimal parameters. \n\nThe following parameters can be tuned in a random forest classifier:\n\n* n_estimators = number of trees in the forest\n* max_features = max number of features considered for splitting a node\n* max_depth = max number of levels in each decision tree\n* min_samples_split = minimum number of data points placed in a node before the node is split\n* min_samples_leaf = minimum number of data points allowed in a leaf node\n* bootstrap = method for sampling data points (with or without replacement)\n\nThe param_grid below will tells SciLearn to first evaluate all 4 x 4 = 16 combinations of n_estimators and max_features specified in the first dict, then try all  4 x 4 = 16 combinations in the second dict, but this time bootstrap is set on False instead of true. Thus in total GridSearch will test 16 + 16 = 32 combinations, and it will try each model 5 times (because we are using 5 times cross-validation). Thus in total 32 x 5 = 160 rounds of training. Here I commented the lines of code with #, otherwise it will take a long time to run.","181a5236":"# 3. Prepare data\n\nWhen preparing the data we start with dividing the dataset in a train and validation set. The train set is used to train the model and the test set is used to evaluate the model. When we have a good model it will be used on the final test set provided by Kaggle and hopefully get us a good score. ","b7a84245":"## 5.2 Confusion matrix","d52b7b47":"According to the accuracy score, 83 percent is correctly predicted.","f70de212":"The obtained accuracy score was 0.86, which is an improvement compared to the random forest model with default options (0.83). Lets fit our optimized model and test how accurate it predicts in the Cover_Types in the test set.","a441b1b6":"The first 10 attributes are continuous. The attributes from Wilderness_area1 to Soil_Type40 are binary. Lets have a look at the distribution for each attribute","0d9d9283":"First we need to make a target object (which is Cover_Type the attribute we have to predict) and a dataframe with all predictor attributes. ","3ecadd4b":"# 1. Introduction\n\nThe goal of the 'Learn with other Kaggle Users' competition is to predict what types of trees (Cover_Type) there are in an area based on various geographic features. All the areas are located in the Roosevelt National Forest and each observation is a 30m x 30m patch. The goal is to predict an integer classification for the forest cover type. The seven types are:\n\n1. Spruce\/Fir\n2. Lodgepole Pine\n3. Ponderosa Pine\n4. Cottonwood\/Willow\n5. Aspen\n6. Douglas-fir\n7. Krummholz\n\nThis is a multiclassification problem. To make predictions I use a random forest classifier. In this kernel I do not perform feature selection\/engineering to improve the model. However I do perform a grid search to find the optimal parameters for the random classifier.\n\nHere are the steps I did to make this model:\n\n* Import data\n* Split the trainset in a train and test set (the testset is only used to make predictions for the competition)\n* Prepare the data using a pipeline\n* Fit random forest model using default options\n* Evaluate results using accuracy score and confusion matrices\n* Search for optimal parameters using grid search\n* Fit the optimized random forest model and evaluate results\n* Predict the Cover_Types in the test set using the final random forest model\n\nThe categorization accuracy score obtained on the test data with the random forest model was 0.76026. For me this is a benchmark which I will hopefully improve using feature selection\/engineering. \n\nThe following book was very usefull for making this notebook: Hands-on Machine Learning with Skikit-Learn and TensorFlow, Aurelie Geron, 2017.","5686afeb":"# 7. Make final model and evaluate it in the test set","c40c4f5a":"## 5.1 Accuracy score","2acd4b40":"Each row represents one patch. There are 55 attributes. The last attribute is called Cover_Type, and this is the attribute we have to predict. "}}