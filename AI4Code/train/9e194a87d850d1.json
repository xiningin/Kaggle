{"cell_type":{"b1d154b7":"code","1347e8f8":"code","ba6e02cd":"code","258a7911":"code","b994fe02":"code","5c7b2225":"code","00dd9b7d":"code","a0e4abd1":"code","c02bfa87":"code","1dfc90f4":"code","fd5ac46d":"code","4474c01f":"code","ebfcbebe":"code","d13f4601":"code","a90e23e6":"code","a879c789":"code","ffae4f5d":"code","18d0b86e":"code","1b774d9c":"code","d02aae74":"code","0548e177":"code","3c8bffed":"code","283a19dd":"code","19376a10":"code","d0d44df0":"code","2c1173b3":"code","9dcb2c15":"code","2251d6b5":"code","b90334d1":"code","6908c553":"code","b28d7150":"code","86692af2":"code","8278e09c":"code","8afab202":"code","50089822":"code","2a527adc":"code","e41f2050":"code","4fa1b6ce":"code","7adb865f":"code","958f73ff":"code","40d83144":"code","36abad49":"code","f5ed2432":"code","95f3a04c":"code","6c2e2064":"code","3c3b01bd":"code","5bfeff2b":"code","4fc62a74":"code","5afecbcf":"code","f97e264f":"code","41af5512":"code","8bf266b0":"code","a8491ea0":"code","453f1725":"code","76353ada":"code","2e086d1c":"code","cb3cd729":"code","4cf8a8d0":"code","50b10d17":"code","9fa8330f":"code","8cac89df":"code","4cd97e55":"code","c9aba9f8":"code","77e8a8ed":"code","d751a7e6":"code","de3f6211":"code","421f17ab":"code","c872fc8d":"code","9d8a6e3a":"code","6f8f14e6":"code","f5b1d05f":"code","a8fba357":"code","3d590b34":"code","4a55e8ca":"code","22f6bcf0":"code","2dc59d5e":"code","55ae535e":"code","530bf522":"code","8caef783":"code","e9d81198":"code","d63947f1":"code","02f813d5":"code","ff331fb6":"code","b0eb6e5e":"code","80f08053":"code","89784792":"code","994e75ad":"code","b6368981":"code","4991ae16":"code","dd81badd":"code","793472ca":"code","d56a9fc8":"code","0786e0e7":"code","f2db87f5":"code","6c53f353":"code","35eaadda":"code","cd2593a9":"code","c9c2dabe":"code","d20aa6fd":"code","c9bb89fa":"code","51e25282":"code","dcf8a725":"code","61e8427d":"code","2c78495c":"code","403805eb":"code","a41b6919":"code","343c82c9":"code","6fb1b27b":"code","8cf447d4":"code","01e45950":"code","81c7ae9a":"code","190a2caf":"code","aa599dda":"code","8ec905b8":"code","e0a5e611":"code","b8c7e574":"code","c73bf99e":"code","ee8d69b9":"code","cd1a9455":"code","25f2daf4":"code","2beaa2e9":"markdown","89b35a68":"markdown","9caf66db":"markdown","f8983380":"markdown","9c36b10f":"markdown","2af064ae":"markdown","146a0d06":"markdown","b4e09f0a":"markdown","f0974797":"markdown","788ba037":"markdown","bd06fae2":"markdown","c200c305":"markdown","6fb34953":"markdown","bfe804a6":"markdown","b816eae7":"markdown","466a46df":"markdown","68542620":"markdown","1cb61b61":"markdown","8b4c00a8":"markdown","ad91defb":"markdown","d7f04e1e":"markdown","57e58554":"markdown","8a7efe2d":"markdown","882830a6":"markdown","c158e8ab":"markdown","136426ba":"markdown","fae22c35":"markdown","4c825385":"markdown","232564e3":"markdown","b37ea588":"markdown","5e4bc850":"markdown","5a556ffe":"markdown","3492c2a1":"markdown","c82f25f4":"markdown","5d2bc349":"markdown","c1e7ad46":"markdown","a513ff21":"markdown","c0c26a21":"markdown","2d741ceb":"markdown","f4c46574":"markdown","b4f4b5e1":"markdown"},"source":{"b1d154b7":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\ncolor = sns.color_palette()\n\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\nimport missingno as msno \nimport pandas_profiling\nimport datetime","1347e8f8":"import matplotlib.font_manager as fm\n[(f.name, f.fname) for f in fm.fontManager.ttflist if 'Apple' in f.name]","ba6e02cd":"import matplotlib.pyplot as plt\nplt.rc('font', family=\"AppleGothic\")","258a7911":"from pandas import DataFrame\ndata = pd.read_csv(r\"\/kaggle\/input\/ecommerce-data\/data.csv\", encoding = 'ISO-8859-1')\ndata.head()","b994fe02":"data.info()","5c7b2225":"data.shape","00dd9b7d":"data.describe()","a0e4abd1":"# Checked where the missing values are = On Description and CustomerID\n\ndata.isnull().sum() \/ data.shape[0]","c02bfa87":"# Checked where and how much null datas are.\n\ndata.isnull().sum().sort_values(ascending=False)","1dfc90f4":"# Cheking where Description is NaN\n\ndata[data.Description.isnull()].head()","fd5ac46d":"# Checking where CustomerID is NaN\n\ndata[data.CustomerID.isnull()].head","4474c01f":"# 'Description' NaN => 'CustomerID' Nan :Conirmed that the NaN values of the 'Description' are the NaN value of the 'CustomerID'","ebfcbebe":"# Number of 'Customer'ID NaN values: 1454\n\ndata[data.Description.isnull()].CustomerID.isnull().value_counts()","d13f4601":"# Removed the NAN value of the data.\n\ndata_n= data.dropna()","a90e23e6":"data_n.isnull().sum() \/ data.shape[0]","a879c789":"data_n.isnull().sum().sort_values(ascending=False)","ffae4f5d":"# Checking again to make sure there are no null values\n\ndata_n.info()","18d0b86e":"data_ntype = {'CustomerID': str,'InvoiceNo': str}","1b774d9c":"# Checked the duplicate values in the data : 5225\uac1c\n\nprint('number of duplicates: {}'.format(data_n.duplicated().sum()))","d02aae74":"# 'Description' length => Des_len\n\ndata_n['Des_len'] = data_n.Description.apply(lambda x: len(x))\ndata_n.head()","0548e177":"data_n.Des_len.describe()","3c8bffed":"# 'InvoiceNo' length => Invo_len\n\ndata_n['Invo_len'] = data_n.InvoiceNo.apply(lambda x: len(x))\ndata_n.head()","283a19dd":"data_n.Invo_len.describe()","19376a10":"# After confirming that there is no problem with 'Description' and 'InvoiceNo', drop it.\n\ndata_n = data_n.drop(columns = ['Des_len', 'Invo_len'])\ndata.head()","d0d44df0":"# When checking the 'Quantity' values, we found out that the minimum value exceeds - 8 million.\n# If you check the data yourself, you can see that this is because the refunds are recorded as - values.\n\ndata_n.Quantity.describe()","2c1173b3":"# Remove 'Quantity' - value and refine it to data_n.\n\ndata_n = data_n[data_n.Quantity > 0]","9dcb2c15":"# Cheking again if the minimum purchase quantity is succesfully refined.\n\ndata_n.Quantity.describe()","2251d6b5":"data_n.head()","b90334d1":"# Visualized the quantities that received the most orders by counting 'Quantity' 'by the number of 'InvoiceNo'.\n# The most ordered Quantity is 1.\n\nqt=data_n.groupby('Quantity')['InvoiceNo'].count().sort_values(ascending=False).iloc[0:30]\nplt.figure(figsize=(40,10))\nsns.barplot(qt.index, qt.values, palette=\"YlOrRd\")\nplt.xlabel('Quantity',fontsize=15)\nplt.ylabel(\"Number of Orders\",fontsize=15)\nplt.title(\"Quantity\",fontsize=20);\nplt.xticks(fontsize=15);","6908c553":"# As showed on the upper graph, you can see that there are many times when there were huge orders, but since their order frequencies were low, we focused on ~25.\n# Distribution of less than Quantity 25: Order in quantities of 1 to 15 were high.\n# As for the quantity, it can be seen that 1-13 quantity is receiving the most order.\n\nplt.figure(figsize=(20,5))\nsns.distplot(data_n[data_n['Quantity'] < 25]['Quantity'].values, kde=True, bins=10,color='orange')","b28d7150":"# Checked the values that are not 'StockCode' numbers.\n# Since the dataset was a transaction data, it includes not only trading transactions, but also parcels and money to be paid to banks.\n\ndata_n[data_n['StockCode'].str.contains('^[ba-zA-Z]+', regex=True)]['StockCode'].unique()","86692af2":"#POST            -> POSTAGE                      \n#D               -> Discount                     \n#C2              -> CARRIAGE                    \n#M               -> Manual                     \n#BANK CHARGES    -> Bank Charges            \n#PADS            -> PADS TO MATCH ALL CUSHIONS \n#DOT             -> DOTCOM POSTAGE ","8278e09c":"#Delete the lines on the 'StockCode' that are texts, since they are not transaction details.\n\ndata_n=data_n[~data_n['StockCode'].isin(['POST', 'C2', 'M', 'BANK CHARGES', 'PADS', 'DOT'])].copy()\ndata_n","8afab202":"# Checked again if it was perfectly excluded from 'StockCode'.\n\ndata_n[data_n['StockCode'].str.contains('^[ba-zA-Z]+', regex=True)]['StockCode'].unique()","50089822":"# Visualized the number of 'StockCodes' that sold the most.\n\nstockcode_c = data.StockCode.value_counts().sort_values(ascending=False)\nplt.figure(figsize=(20,5))\nsns.barplot(stockcode_c.iloc[0:30].index,\n            stockcode_c.iloc[0:30].values,\n            palette=\"Greens_r\")\nplt.ylabel(\"Counts\")\nplt.xlabel(\"Stockcode\")\nplt.title(\"Stockcode\");","2a527adc":"# Looked at the data of the 10 most ordered StockCodes.\n# Found out the 'StockCode's' most ordered countries are all UK.\n\nprint('The TOP 10 Stockcodes with most number of orders') \nstockCode_best = data_n.groupby(by=['StockCode','Country'], as_index=False)['InvoiceNo'].count()\nstockCode_best.sort_values(by='InvoiceNo', ascending=False).head(10)","e41f2050":"# Sorted the 'StockCodes' with the most orders in ascending order of 'Quantity'.\n# Listed the items with the most purchases, in the order of the most purchases. Of the five 'StockCodes', you can see that 85123A and 85099B sell a lot at once.\n\ndata_n[data_n['StockCode'].isin(['85123A','22423','85099B','47566','20725'])].sort_values(by=['Quantity'], axis=0, ascending=False).head(20)","4fa1b6ce":"data_n.CustomerID=data_n.CustomerID.astype('int64')","7adb865f":"# Visualized the data of the customers who made the most purchases.\n\ncustomer_c = data.CustomerID.value_counts().sort_values(ascending=False).iloc[0:30] \nplt.figure(figsize=(20,5))\nsns.barplot(customer_c.index, customer_c.values, order=customer_c.index,palette=\"Reds_r\")\nplt.ylabel(\"Counts\")\nplt.xlabel(\"CustomerID\")\nplt.title(\"Which customers are most common?\");","958f73ff":"# Found out the 'CustomerID' that placed the most orders.\n\nprint('The TOP 10 customers with most number of orders') \ncustomer_best = data_n.groupby(by=['CustomerID','Country'], as_index=False)['InvoiceNo'].count()\ncustomer_best.sort_values(by='InvoiceNo', ascending=False).head(10)","40d83144":"# Sorted the 'CustomerID' with the most orders in ascending order by 'Quantity'\n# Found out the most of the customers who placed many orders bought the ones that the price is low, but are purchasing in a large quantity.\n\ndata_n[data_n['CustomerID'].isin(['17841','14911','14096','12748','14606'])].sort_values(by=['Quantity'], axis=0, ascending=False).head(20)","36abad49":"# Changed the whole 'Description' into uppercase\n\ndata_n['Description'] = data.Description.str.upper()","f5ed2432":"#Checked out the 10 most sold products.\n\ndata.Description.value_counts()[:10]","95f3a04c":"# Visualized what products have sold the most.\n\ndescription_c = data.Description.value_counts().sort_values(ascending=False).iloc[0:30]\nplt.figure(figsize=(20,5))\nsns.barplot(description_c.index, description_c.values, palette=\"Blues_r\")\nplt.ylabel(\"Counts\")\nplt.title(\"Description\");\nplt.xticks(rotation=90);","6c2e2064":"# Found out the most ordered product was \"WHITE HANGING HEART T-LIGHT HOLDER\"\n# And that most of them were ordered from the UK.\n\nprint('The TOP 10 Description with most number of orders') \ncustomer_best = data_n.groupby(by=['Description','Country'], as_index=False)['InvoiceNo'].count()\ncustomer_best.sort_values(by='InvoiceNo', ascending=False).head(10)","3c3b01bd":"# 'Description': Analyzed the product name.\n\ndescription=[data_n.Description.value_counts().index]\ndescription","5bfeff2b":"# Split the 'Description' by spaces.\n\ndescription_most=data_n['Description'].str.split(expand=True).stack().value_counts()\ndf=pd.DataFrame(description_most)\ndf","4fc62a74":"# Visualized the keywords of the 'Description'.\n# Various keywords such as SET, BAG, RETROSPOT, and VINTAGE are searched,\n# And can be seen that this is a e-commerce that sells various group of products.\n\ndf1=df[0].sort_values(ascending=False).iloc[0:50]\nplt.figure(figsize=(20,5))\nsns.barplot(df1.index, df1.values, palette=\"autumn_r\")\nplt.ylabel(\"Counts\")\nplt.title(\"Description Frequency\");\nplt.xticks(rotation=90);","5afecbcf":"# Change the date format by 12\/1\/2010 11:52 to 2010-12-01 11:52:00 \n# Change 'InvoiceDate' to be more user-friendly\n\n\ndata_n['InvoiceDate'] = pd.to_datetime(data.InvoiceDate, format='%m\/%d\/%Y %H:%M')","f97e264f":"# Cut the InvoiceDate into year, month, day, and hour, and then prepare the  statistics for each classification.\n# Since two years exist,2010 and 2011, the 'Year' was changed by combined year\/month.\n\n\ndata_n.insert(loc=2, column='year',value=data_n['InvoiceDate'].map(lambda x: 100*x.year + x.month))\ndata_n.insert(loc=3, column='month', value=data_n.InvoiceDate.dt.month)\n# +1 to make Monday=1.....until Sunday=7\ndata_n.insert(loc=4, column='day', value=(data_n.InvoiceDate.dt.dayofweek)+1)\ndata_n.insert(loc=5, column='hour', value=data_n.InvoiceDate.dt.hour)","41af5512":"data_n.drop(['InvoiceDate'], axis=1)","8bf266b0":"# Visualized the time zone where the product sold the most: 12 o'clock and between.\n\ndf4=data_n['hour'].value_counts().sort_values(ascending=False).iloc[0:50]\nplt.figure(figsize=(20,5))\nsns.barplot(df4.index, df4.values, palette=\"coolwarm_r\")\nplt.ylabel(\"Counts\")\nplt.title(\"Hour\");","a8491ea0":"# Visualized the month where the product sold the most:\n# Found out November sold the most, and as the year-end approaches, sales increases.\n\ndf4=data_n['month'].value_counts().sort_values(ascending=False).iloc[0:50]\nplt.figure(figsize=(20,5))\nsns.barplot(df4.index, df4.values, palette=\"coolwarm_r\")\nplt.ylabel(\"Counts\")\nplt.title(\"Month\");","453f1725":"# Checked out the unit price of the item, and In this process found out that there is a price of zero.\n\ndata_n.UnitPrice.describe()","76353ada":"# Total of 33 products with 'UnitPrice' of 0.\n\ndata_n.loc[data_n.UnitPrice == 0].sort_values(by=\"Quantity\", ascending=False).count()","2e086d1c":"# 'UnitPrice' of 0: Is unknown whether it is a free product or a promotion.\n\ndata_n.loc[data_n.UnitPrice == 0].sort_values(by=\"Quantity\", ascending=False).head()","cb3cd729":"# Visualized distribution of UnitPrice less than 10: The lower the price, the more the sells.\n# Can see that why wholesellers are coming for this E-commerce\n\nplt.figure(figsize=(12,4))\nsns.distplot(data_n[data_n['UnitPrice'] < 10]['UnitPrice'].values, kde=True, bins=10,color='red')","4cf8a8d0":"# Since the 'Quantity' and the 'Unitprice are' are separated, it is hard to know the actual moneyflow for one transaction.\n# Added a column of the sales item called 'Amount' with 'Quantity'*'UnitPrice' that can show the actual one-time purchase.\n\ndata_n['Amount'] = data_n['Quantity'] * data_n['UnitPrice']","50b10d17":"data_n.head()","9fa8330f":"data_n.info()","8cac89df":"# Checked how much sales it made per month.\n# Caution: The Dataset's date was from December 1, 2010 to December 9, 2011 are not perfect one-year data.\n# And 2011 of December is not a perfect one-month data.\n\ndata_n.groupby('year')['Amount'].sum()","4cd97e55":"# Visualized monthly sales: November 2011 shows the highest sales.\n# Since the data of 2011-December was about 1\/3of the month, by multiplying simply 3 times\n# It can be said the monthly sales was constantly getting higher as the year grew older.\n\ndf2=data_n.groupby('year')['Amount'].sum()\nplt.figure(figsize=(20,5))\nax = plt.subplot()\nsns.barplot(df2.index, df2.values, palette='PRGn_r', ax=ax)\nax.get_yaxis().get_major_formatter().set_scientific(False)\nax.set_xlabel\nax.set_ylabel('Amount')\nax.set_title('Monthly Sales')","c9aba9f8":"# Found the outliers of 'Amount' through scatterplot\n\nplt.figure(figsize=(20,5))\nplt.scatter(x=data_n.index, y=data_n['Amount'])","77e8a8ed":"# Removed more than 25000 outliers from data (for average analysis of later modeling)\n\ndata_n = data_n[data_n['Amount'] < 25000]\nplt.figure(figsize=(20,5))\nplt.scatter(x=data_n.index, y=data_n['Amount'])\nplt.xticks(rotation=90)","d751a7e6":"# Check what 'Countries' are in the data.\n\ndata_n.Country.unique()","de3f6211":"# Check how many purchases were made by 'Country'.\n# UK was the most purchasers.\n\ndata_n.Country.value_counts()","421f17ab":"#data_n = data_n.replace({'United Kingdom':'UK','France':'FR','Germany':DE}) ","c872fc8d":"# Number of orders by country: The UK is the highest.\n\ndf7=data_n.groupby('Country')['InvoiceNo'].count().sort_values(ascending=False)\nplt.figure(figsize=(30,10))\nsns.barplot(df7.index, df7.values, palette=\"inferno_r\")\nplt.xlabel('Country',fontsize=15)\nplt.ylabel(\"Number of Orders\",fontsize=15)\nplt.title(\"Country\",fontsize=20);\nplt.xticks(rotation=90,fontsize=20);","9d8a6e3a":"# When deleted UK, the top 3 number of orders by country were Germany, France, and Ireland.\n\ndf7=data_n.groupby('Country')['InvoiceNo'].count().sort_values(ascending=False)\ndel df7['United Kingdom']\nplt.figure(figsize=(30,10))\nsns.barplot(df7.index, df7.values, palette=\"inferno_r\")\nplt.xlabel('Country',fontsize=15)\nplt.ylabel(\"Number of Orders\",fontsize=15)\nplt.title(\"Country\",fontsize=20);\nplt.xticks(rotation=90,fontsize=20);","6f8f14e6":"# Total sales by country: The UK is the highest.\n\ndf3=data_n.groupby('Country')['Amount'].sum().sort_values(ascending=False)\nplt.figure(figsize=(30,10))\nsns.barplot(df3.index, df3.values, palette=\"inferno_r\")\nplt.xlabel('Country',fontsize=15)\nplt.ylabel(\"Amount\",fontsize=15)\nplt.title(\"Average amount by Country\",fontsize=20);\nplt.xticks(rotation=90,fontsize=20);","f5b1d05f":"# The countries with the highest sales average are the Netherlands, Australia and Japan.\n# Here we can see that the gross sales and averages are irrelevant.\n\ndf8=data_n.groupby('Country')['Amount'].mean().sort_values(ascending=False)\nplt.figure(figsize=(30,10))\nsns.barplot(df8.index, df8.values, palette=\"inferno_r\")\nplt.xlabel('Country',fontsize=15)\nplt.ylabel(\"Amount\",fontsize=15)\nplt.title(\"Average amount by Country\",fontsize=20);\nplt.xticks(rotation=90,fontsize=20);","a8fba357":"# The percentage of the UK on which placing the most orders. => A total of 89.19%.\n\nuk_count = data_n[data['Country'] == 'United Kingdom']['Country'].count()\nall_count = data_n['Country'].count()\nuk_perc = uk_count\/all_count\nprint(str('UK : {0:.2f}%').format(uk_perc*100))","3d590b34":"# Identified the most repurchased items.\n\ndf_sort = data_n.sort_values(['CustomerID', 'StockCode', 'InvoiceDate'])\ndf_sort_shift1 = df_sort.shift(1)\ndf_sort_reorder = df_sort.copy()\ndf_sort_reorder['Reorder'] = np.where(df_sort['StockCode'] == df_sort_shift1['StockCode'], 1,0)\ndf_sort_reorder.head(5)","4a55e8ca":"# The most reordered product is \"WHITE HANGING HEART T-LIGHT HOLDER\".\n\npd.DataFrame((df_sort_reorder.groupby(['Description'])['Reorder'].sum())).sort_values('Reorder', ascending = False).head(10)","22f6bcf0":"# Visualized of monthly reorder products: November 2011 has the most reorders.\n# As the period passes, the number of reorders increases, and the regular purchase rate increases.\n\nnotreorder = (df_sort_reorder[df_sort_reorder['Reorder'] == 0 ].groupby(['year'])['Amount'].sum())\nreorder = (df_sort_reorder[df_sort_reorder['Reorder'] == 1 ].groupby(['year'])['Amount'].sum())\nyearmonth = pd.DataFrame([notreorder , reorder], index=['First Buy', 'Reorder']).transpose()\nyearmonth.plot.bar(stacked=True)","2dc59d5e":"data_n.info()","55ae535e":"# Create data_2 to use for data mining.\n# Drop unnecessary columns from data_2.\n\ndata2 = data_n.groupby(['InvoiceNo','InvoiceDate','CustomerID']).sum()\ndata2 = data2.drop(columns = 'UnitPrice')\ndata2 = data2.drop(columns = 'year')\ndata2 = data2.drop(columns = 'month')\ndata2 = data2.drop(columns = 'day')\ndata2 = data2.drop(columns = 'hour')\ndata2.head()","530bf522":"data2.describe()","8caef783":"# To eliminate outliers, check the skew value.","e9d81198":"#If skewness is -0.5 to 0,5, the data is quite symmetric.\n# If skewness is -1~-0.5 or 0.5~1, the data is moderately skewed.\n# If skewness is less than -1 or greater than 1, the data is quite skewed.","d63947f1":"from scipy.stats import skew","02f813d5":"skew(data2.Amount)","ff331fb6":"# mean-stddev <= data <= mean+stddev: Because the data is skewed, use the formula to remove outliers\n# Amount skew reduction\n\ndata2 = data2.query('Amount >= 0 and Amount <= 458.583140 + 939.357035') #mean+std","b0eb6e5e":"data2.describe()","80f08053":"# Visualized data_2 by boxplot.\n\nplt.figure(figsize=(20,5))\nsns.boxplot(data2.Amount)","89784792":"# Visualised data_2 by displot.\n\nsns.displot(ax=ax,data=data2.Amount,height = 5,aspect = 3)","994e75ad":"# Since the result of skewness is very skewed to the left, it needs to be screwed again.\n\nskew(data2.Quantity)","b6368981":"# 'Quantity' skew reduction\n\ndata2 = data2.query('Quantity >= 0  and Quantity <= 204.244463 + 231.298757')\ndata2.describe()","4991ae16":"print(skew(data2.Amount))\nprint(skew(data2.Quantity))","dd81badd":"plt.figure(figsize=(20,5))\nsns.boxplot(data2.Amount)","793472ca":"# Use data_2 with reduced skewness (removed outliers).","d56a9fc8":"#Look'CustomerID' data with data2\n\ndata2 = data2.reset_index()\ninvoice = data2['InvoiceNo'].tolist()","0786e0e7":"data_n = data_n[data_n.InvoiceNo.isin(invoice)]","f2db87f5":"data_n.head()","6c53f353":"data_n.describe()","35eaadda":"# CustomerID: Guest(null) => When customerid is null, it is classified as guest, otherwise, it is classified as customer.\nvalue = {'CustomerID':'Guest'}\ndata = data_n.fillna(value = value)\ndata[data_n.CustomerID == 'Guest'].head()","cd2593a9":"data_n.CustomerID.nunique()","c9c2dabe":"user_month = data_n.groupby('year').CustomerID.nunique().reset_index()\nuser_month.columns = ['month','total_user']\nuser_month\n# Note: Again, December is in both 2010 and 2011, and since there are overlapping days it is divided into year columns, not month.","d20aa6fd":"# Check that the data is between 2010-12-01 08:26:00 and 2011-12-09 12:50:00\ndata_n.InvoiceDate ","c9bb89fa":"# Unique User=Guest's user trend\n# The last month's sharp decline in customers are due to the insufficient data from January 1, 2020 to January 9, 2011.\n\nf, ax = plt.subplots(figsize=(20, 5))\nsns.lineplot(data=user_month.total_user)\nplt.xlabel('Month')\nplt.ylabel('Unique User')\nplt.title('Unique User by Month')\nplt.xticks([1,2,3,4,5,6,7,8,9,10,11,12])","51e25282":"# 'cust_id': customer ID\n# 'total_product': Total transaction volume per customer\n# 'total_trx': Total transaction amount per customer\n# 'recent_trx': Date from the last transaction date\n# 'freq': Transaction frequency within the data period","dcf8a725":"# Saved in data_cust which has the following 6 items.\n\ndata_cust = data_n[['CustomerID','InvoiceDate','Quantity','UnitPrice','Amount','StockCode']]\ndata_cust.head()","61e8427d":"# Checked total_product by customer.\n\ntotal_bought = data_cust.groupby('CustomerID').StockCode.nunique().reset_index()\ntotal_bought.columns = ['cust_id','total_product']\ntotal_bought.head()","2c78495c":"# Made the total transaction price per customer as 'total_trx'.\n\ntotal_trx = data_cust.groupby('CustomerID').Amount.sum().reset_index()\ntotal_trx.columns = ['cust_id','total_trx']\ntotal_trx.head()","403805eb":"data_n.InvoiceDate.max()","a41b6919":"# Made the interval between the last day of the order and the InvoiceDate LastTrx. (Day since last transactions happen)\n\ndata_n['LastTrx'] = (pd.to_datetime('2011\/12\/09 12:50:00') -data_n.InvoiceDate).dt.days\ndata_n.tail()","343c82c9":"# Identify 'freq' as the frequency of purchase per data date range.\ncus_frequency = data_cust.groupby('CustomerID').InvoiceDate.nunique().reset_index()\ncus_frequency.columns = ['cust_id','freq']\ncus_frequency.head()","6fb1b27b":"data_n.describe()","8cf447d4":"# Set recent purchases as 'recent_trx'. If the number is low, it is recent, and if it is high, it's been quite a while since it was purchased.\ncus_recent_trx = data_n.groupby('CustomerID').LastTrx.min().reset_index()\ncus_recent_trx.columns = ['cust_id','recent_trx']\ncus_recent_trx.head()","01e45950":"# Merged with 'cust_id'\n\ncust = pd.DataFrame()\ncust['cust_id'] = cus_recent_trx.cust_id\ncust = cust.merge(total_bought, on='cust_id')\ncust = cust.merge(total_trx, on='cust_id')\ncust = cust.merge(cus_recent_trx, on='cust_id')\ncust = cust.merge(cus_frequency, on='cust_id')\ncust.head()","81c7ae9a":"from sklearn.cluster import KMeans","190a2caf":"# Used the sum of squares of the distance\n\nssd = []\nK = range(1,10)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(cust)\n    ssd.append(km.inertia_)","aa599dda":"# elbow method: A method to find the number of clusters where the variability within the cluster decreases sharply as additional clusters are increased.\n# The fact that the intra-cluster volatility has dropped sharply means that similar people are well tied together.\n\nplt.figure(figsize=(20,5))\nplt.plot(K, ssd, 'bx-')\nplt.xlabel('k')\nplt.ylabel('ssd')\nplt.title('Elbow Method For Optimal k')\nplt.show()","8ec905b8":"kmeans = KMeans(n_clusters=4)\nmodel = kmeans.fit(cust)","e0a5e611":"pred = model.labels_\ncust['Cluster'] = pred\ncust.head()","b8c7e574":"# Checked the distribution of 4 clusters: how recently and how many purchases\n\nplt.figure(figsize=(20,5))\n\nsns.scatterplot(data=cust, x=\"total_trx\", y=\"recent_trx\", hue=\"Cluster\")\nplt.title('Cluster by Total Transaction and Recencys')\nplt.show()","c73bf99e":"# Checked 'total_trx' in customer cluster\n\ncustomers = cust.groupby('Cluster').mean().reset_index()\ncustomers.sort_values('total_trx')","ee8d69b9":"contribution = cust.groupby('Cluster').total_trx.sum().reset_index()\ncontribution['Contribution (%)'] = (contribution.total_trx\/contribution.total_trx.sum())*100\ncontribution","cd1a9455":"# Group 0 purchasesd less frequently (low freq), spends less (low total_trx), purchased long ago (high recent_trx), and occupies a large percentage (high contribution).\n# Seasonal Customer\n\n# Group 1 purchased less frequently (low freq), spends less (low total_trx), purchased long ago (high recent_trx), and occupies a large percentage (high contribution)\n# Seasonal Customer\n\n# Group 2 purchased bit frequently (medium freq), spends okay (medium total_trx), purchased recently(low recent_trx), and occupies a low percentage(low contribution)\n# Loyal Customer\n\n# Group 3 purchased frequently (high freq), spends a lot (very high total_trx), purchased recently (low recent_trx), and occupies a low percentage(low contribution).\n# Dropshipper","25f2daf4":"# Visualized the number of people distributed in each cluster\n\nsns.displot(ax=ax,data=cust,x='Cluster',height = 5,aspect = 3)","2beaa2e9":"Quantity : Purchased Quantity by each person","89b35a68":"## Business\n\n1. In the original data, it was excluded from the pre-processing process, but there were cases where the purchase was refunded for 80,000 pieces per purchase. If this situation is repeated, a request for confirmation of the refund is proposed to the purchasing customer.\n2. Through the EDA process, we saw the state that the repurchase rate and the regular purchase rate were maintained well, and we can see that the site (shopping mall) is operating well. However, in order to develop, we need to run promotions for seasonal customers, which are 99% of our customers. Here are some options.\n   - In addition to widening the coverage of products for seasonal customers in a specific month, the purchase width is increased by gradually introducing products similar to those purchased by seasonal customers.\n   - Increase the discount range applied per product, and if a large purchase is made by more than a certain quantity, through contacts such as email with customers, give out various services such as shipping costs, discounts, seasonal promotion coupons, individual promotions to provide increased loyalty to the site.\n3. Give benefits for promotions such as coupons and discount events for each rating by creating a rating for each customer. However,start the basic group as a VIP so the lower group is not alienated. When a purchase is made as a customer who has a ID, give the VIP level immediately. Also make the first purchaser also proceed with promotions such as a discount on the first purchase and a free shipping coupon. This classifies the frequency of purchase by class, which can be a strategy for customized promotions for each customer. Also, since regular customers who are Silver VIPs are important customers who account for 30% of sales, they provide many discounts and services. In addition, since the number of Silver VIP and Gold VIP is small, communication with dedicated staff per customer can be provided.One thing to be aware of is the consignment customers who will become Gold VIPs, however, since they are doing a consignment business, they aim for profits between purchases, and if there is a cheaper seller, they can move to the place of purchase immediately. For them, thorough delivery times, delivery conditions must be kept at the best, and periodic price comparisons must be made to ensure that cheaper products are not listed in tight, and unique and special items must be maintained on the current site.\n   - Group 0,1(Seasonal Customer) : VIP\n   - Group 2 (Loyal Customer) : Silver VIP\n   - Group 3 (Dropshipper) : Gold VIP\n4. Promote Seasonal, holiday, and anniversary promotions by country. If the data is set in advance on the delivery date and sales start date of the products necessary for the national event day, It will be able to establish itself as the product group that generates the largest sales of the month even it is not a product that generates continuous sales. This will give you a corporate image that you can quickly and easily purchase anniversary event products for the site at low prices through the same promotion every year.\n5. Since there are many customers who are in the wholesale business as a whole, keep records of previously purchased items display data-based UI\/UX on the site to increase convenience by and make repurchases easier.\n","9caf66db":"### 2.9 Reorder Item","f8983380":"   - It categorizes customers as customer clustering and similar to the RFM customer value analysis commonly used in marketing. It has three factors. Recency, Frequency and Monetary.\n\n   - Recency: It is a variable indicating when a customer's last purchase time is, and the current relationship is more significant for customers who have recently purchased.\n   - Frequency: It's a variable as to how often a customer purchases during a specified period, and the higher the number of purchases during the same period, the higher the score is charged, and it is possible to judge the customer's purchase\/use activity.\n   - Monetary: A variable representing the total purchase amount of a customer over a certain period of time. Higher purchase amount can result in a higher score, but if excessively high purchase amount exists, an upper limit is placed when measuring the RFM index to prevent distortion of the overall index.\n   \n##### The RFM score is given by a*Recency + b*Frequency + c * Monetary, and weighting a,b,c wheter which factor is important on the industry. However, only customer classification was performed using those factors. The following five items were used for customer clustering and classification.","9c36b10f":"Checking the missing values and dropping the Nulls.","2af064ae":"# 2. EDA (Exploratory Data Analysis)","146a0d06":"### 3.1 Data Mining","b4e09f0a":"### 2.6 UnitPrice","f0974797":"### 2.3 CustomerID","788ba037":"### 3.3 Customer Classification","bd06fae2":"Description : Product name of the product being sold","c200c305":"### 2.4 Description","6fb34953":"### 3.2 Customer Data","bfe804a6":"### 2.1 Quantity","b816eae7":"**Preprocessing**\n\nImporting libraries and data.csv file.","466a46df":"Review the data of CustomerID and check the trend of user usage.","68542620":"Checked the products with the most reorders through the items of'CustomerID','StockCode', and'InvoiceDate'.","1cb61b61":"StockCode : Code for each product","8b4c00a8":"## Modeling\n#### Through the EDA, various analyzes such as the sales measure of customers and the purchase rate of products were able to analyze. Based on this EDA, customers can be classified into four groups for the purpose of seasonal customers, regular customers, and consignment sales by classifying customers by frequency purchase volume, sales volume per purchase, and recent purchase level through customer data.\n1.\tGroup 0 : Low frequency, low consumption, and purchased long ago: Seasonal customer group\n2.  Group 1 : Low frequency, low consumption, and purchased long ago: Seasonal customer group\n3.  Group 2 : Medium frequency, medium consumption, and recently purchased customer:  Loyal customer group\n4.  Group 3 : High frequency, high consumption, and recently purchased customer:Dropshipper\n\n   - Seasonal Customer : 99% of all customers, 60% of all sales: Group 0, Group 1\n   - Loyal Customer : 0.4%of all customers, 30% of all sales : Group 2\n   - Dropshipper : 0.4%of all customers, 10% of all sales : Group 3","ad91defb":"Rearrange for model the data afterwards.","d7f04e1e":"UnitPrice : Price per product","57e58554":"### Dropping the NaN","8a7efe2d":"### Missing values","882830a6":"### 3.4 K-means Clustering","c158e8ab":"InvoiceDate : Order date and time","136426ba":"### 2.8 Country","fae22c35":"### 2.5 InvoiceDate","4c825385":"# E-Commerce EDA & Modeling\n____\n\n**E-Commerce Data**\n\nWe've used a a set of data that a company actually sold online to wholesalers (customers) from 2010.12.01 to 2011.12.09.\n\nThe E-Commerce Data contains 8 data columns: Transaction Number(InvoiceNo), Product Code(StockCode), Product Name(Description), Purchase Quantity(Quantity), Purchase Date(InvoiceDate), Product Price(UnitPrice), Customer ID(CustomerID), and Purchases's Country(Country).\n\n___\n**1. Data Preparation**\n\n**2. EDA (Exploratory Data Analysis)**\n\n   - 2.1 Quantity\n   - 2.2 StockCode\n   - 2.3 CustomerID\n   - 2.4 Description\n   - 2.5 InvoiceDate\n   - 2.6 UnitPrice\n   - 2.7 Amount\n   - 2.8 Country\n   - 2.9 Reorder Item  \n\n**3. Modeling**\n\n   - 3.1 Data Mining\n   - 3.2 Customer Data \n   - 3.3 Customer Classification \n   - 3.2 K-means Clustering\n\n**4. Conclusion**\n___","232564e3":"   **Each Item meanings**\n   - **InvoiceNo**: Invoice number.<br>\n   - **StockCode**: Product (item) code.<br>\n   - **Description**: Product (item) name. <br>\n   - **Quantity**: The quantities of each product (item) per transaction.<br>\n   - **InvoiceDate**: Invice Date and time.<br>\n   - **UnitPrice**: Unit price.<br>\n   - **CustomerID**: Customer number.<br>\n   - **Country**: Country name. <br>","b37ea588":"### 2.2 StockCode","5e4bc850":"### 2.7 Amount","5a556ffe":"#  3. Modeling","3492c2a1":"# 4.1 Conclusion","c82f25f4":"Check how the datas are","5d2bc349":"It was possible to change the country name, but didn't proceed as it is meaningless to change all those. If someone wants to, I recommend to do only UK, or top 3 countries.","c1e7ad46":"CustomerID : Customer's ID","a513ff21":"Country : Country of purchaser","c0c26a21":"Checked that most of our customers are wholesalers (resellers), and confirmed that there are many Guest (CustomerID null).","2d741ceb":"# 1. Data Preparation","f4c46574":"## EDA (Exploratory Data Analysis)\n\n 1. By classifying the selling price range of products, it was possible to know that what were sold on the site, and the frequency of the prices.\n \n \n 2. Found out that his data includes not only product transaction details but also other things such as POSTAGE, Discount, CARRIAGE, Manual, Bank Charges, PADS TO MATCH ALL CUSHIONS, DOTCOM POSTAGE, Amazon fee an etc.\n \n \n 3. WHITE HANGING HEART T-LIGHT HOLDER was the bestseller and also had a lot of resale. It was possible to categorize the main products through the sales ranking, high purchase rate and reorganize products with a low purchase rate and also reorganize the product composition plan\n\n\n 4. As a result of analyzing the product names by word unit, the popular product names were such as'SET','RETROSPOT', \"VINTAGE\", \"DESIGN\", and \"CHRISTMAS.\" It may not be able to identify the exact product category, but it can analyzed that this E-Commerce does not only sell specific product group but also various products groups.\n \n \n 5. Most of the transactions took place around 12 o'clock, and November was the most trading volume and sales, and the number of transactions increased with the end of the year. Over time, the rate of repurchases and regular purchases also increased.\n \n \n 6.  We don't know whether it is an online business based in the UK or neighboring countries, but we can guess as the UK accounted for 89% of the volume of transactions, with the highest total sales. The average of the transaction amount per item purchased was the highest in the Netherlands, which indicates that the Netherlands is purchasing products at a high price, although the total sales are not high.\n \n \n 7. It can be seen that more than 50% of the sold products' price are less than $$2, and less than 70% are less than $3.\n \n \n 8. It was possible to divide the purchase rate of customers who made the first purchase and customers who made reorders (reorders) on a monthly basis. From January to June, the first half of the year, many customers make their first purchases, and from July to December, the second half of the year, both first-time customers and re-orders are high.","b4f4b5e1":"Check whether there are outliers of Description and InvoiceNo by counting the number of letters."}}