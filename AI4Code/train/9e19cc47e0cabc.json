{"cell_type":{"87bbcc02":"code","79157b5d":"code","5d63c8e1":"code","c4b2ba78":"code","14029440":"code","ad0ddb52":"code","8cd1c7e6":"code","dc49f7e1":"code","fe3a4fc5":"code","22da6048":"markdown","61d06c2f":"markdown","3b084db0":"markdown","a63c8870":"markdown","6f570220":"markdown","6361c452":"markdown","a7231b98":"markdown","c336fff5":"markdown","9686d297":"markdown","7812a748":"markdown","27f45349":"markdown","fe190343":"markdown","69725ec0":"markdown","f9d1750e":"markdown","462081d3":"markdown"},"source":{"87bbcc02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79157b5d":"import matplotlib.pyplot as plt\nplt.rcParams['figure.figsize']=[20,8]\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\n##\ndf=pd.read_csv('..\/input\/student-study-hour-v2\/Student Study Hour V2.csv')\ndisplay(df.shape)\ndisplay(df.corr())\ndisplay(df.describe())\n\n##\n# Let consider 50% of Score as pass mark\npass_mark=df.Scores.quantile(0.5)\ndf['Pass']=(df.Scores>pass_mark).astype(int)\nplt.title('Student Studies',fontsize=24)\nplt.xlabel('Hours',fontsize=24)\nplt.ylabel('Pass',fontsize=24)\nsns.scatterplot(data=df,x='Hours',y='Pass',hue=\"Pass\",style=\"Pass\",size=\"Pass\",sizes=(200, 200))\ny=df['Pass']\nX=df['Hours']","5d63c8e1":"def gradient_descent(all_X,all_y,m,b):\n    total_error=0;y_pred=list()\n    for x,y in zip(all_X,all_y):\n        y_predict=m*x+b\n        y_pred.append(y_predict)\n        error=y_predict-y\n        total_error+=error\n        delta_m=1\n        delta_b=1\n        m=m+delta_m\n        b=b+delta_b\n    return m,b,y_pred,total_error\n# m=0;b=0\n# m,b,y_pred,total_error=gradient_descent(X,y,m,b)\n# print('m:',m,'b:',b,'total_error:',total_error)\n# plt.title('Student Studies',fontsize=24)\n# plt.xlabel('Hours',fontsize=24)\n# plt.ylabel('Pass',fontsize=24)\n# sns.scatterplot(x=X,y=y_pred)\n# plt.plot(X,y_pred)\nplt.title('Student Studies',fontsize=24)\nplt.xlabel('Hours',fontsize=24)\nplt.ylabel('Pass',fontsize=24)\n\nm=0;b=0\niteration=20\nfor i in range(iteration):\n    m,b,y_pred,total_error=gradient_descent(X,y,m,b)\n    print('m:',m,'b:',b,'total_error:',total_error)\n    sns.scatterplot(x=X,y=y_pred)\n    plt.plot(X,y_pred)","c4b2ba78":"def gradient_descent(all_X,all_y,m,b):\n    total_error=0;y_pred=list()\n    for x,y in zip(all_X,all_y):\n        y_predict=m*x+b\n        y_pred.append(y_predict)\n        error=y_predict-y\n        total_error+=error\n        delta_m=error*x\n        delta_b=error\n        m=m+delta_m\n        b=b+delta_b\n    return m,b,y_pred,total_error\nplt.title('Student Studies',fontsize=24)\nplt.xlabel('Hours',fontsize=24)\nplt.ylabel('Pass',fontsize=24)\n\nm=0;b=0\niteration=20\nfor i in range(iteration):\n    m,b,y_pred,total_error=gradient_descent(X,y,m,b)\n    print('m:',m,'b:',b,'total_error:',total_error)\n    sns.scatterplot(x=X,y=y_pred)\n    plt.plot(X,y_pred)","14029440":"def gradient_descent(all_X,all_y,m,b):\n    total_error=0;y_pred=list()\n    for x,y in zip(all_X,all_y):\n        y_predict=m*x+b\n        y_pred.append(y_predict)\n        error=y_predict-y\n        total_error+=error\n        delta_m=error*x\n        delta_b=error\n        m=m + delta_m * 0.001\n        b=b + delta_b * 0.001\n    return m,b,y_pred,total_error\n\nplt.title('Student Studies',fontsize=24)\nplt.xlabel('Hours',fontsize=24)\nplt.ylabel('Pass',fontsize=24)\n\nm=0;b=0\niteration=20\nfor i in range(iteration):\n    m,b,y_pred,total_error=gradient_descent(X,y,m,b)\n    print('m:',m,'b:',b,'total_error:',total_error)\n    sns.scatterplot(x=X,y=y_pred)\n    plt.plot(X,y_pred)","ad0ddb52":"def gradient_descent(all_X,all_y,m,b):\n    total_error=0;y_pred=list()\n    for x,y in zip(all_X,all_y):\n        y_predict=m*x+b\n        y_pred.append(y_predict)\n        error=y_predict-y\n        total_error+=error\n        delta_m=error*x\n        delta_b=error\n        m=m - delta_m * 0.001\n        b=b - delta_b * 0.001\n    return m,b,y_pred,total_error\n\nplt.title('Student Studies',fontsize=24)\nplt.xlabel('Hours',fontsize=24)\nplt.ylabel('Pass',fontsize=24)\n\nm=0;b=0\niteration=20\nfor i in range(iteration):\n    m,b,y_pred,total_error=gradient_descent(X,y,m,b)\n    print('m:',m,'b:',b,'total_error:',total_error)\n    sns.scatterplot(x=X,y=y_pred)\n    plt.plot(X,y_pred)","8cd1c7e6":"def gradient_descent(x,y,iteration=30):\n    m_current=b_current=0  #intialize m and b \n    learning_rate =0.0004 # step's \n    n=len(x)\n    for i in range(iteration):\n        y_predict = (m_current*x)+b_current #y=mx+b\n        cost = (1\/n) * sum( [val**2  for val in ( y-y_predict )]) #https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_squared_error.html\n        m_next = -(2\/n)*sum(x*(y-y_predict)) #(\u2202\/\u2202m) refer above pic\n        b_next = -(2\/n)*sum(y-y_predict) #(\u2202\/\u2202b)\n        m_current = m_current - (learning_rate *m_next)\n        b_current = b_current - (learning_rate *b_next)\n        print(\"m  {}  ,b  {}  ,cost  {}  ,iteration  {} \".format(m_current,b_current,cost,i))\n        sns.scatterplot(x=x,y=y_pred)\n        plt.plot(x,y_pred)\n\nplt.title('Student Studies',fontsize=24)\nplt.xlabel('Hours',fontsize=24)\nplt.ylabel('Pass',fontsize=24)\ngradient_descent(X,y)","dc49f7e1":"##\ndf=pd.read_csv('..\/input\/student-study-hour-v2\/Student Study Hour V2.csv')\nsns.scatterplot(x='Hours',y='Scores',data=df)\n\n##\nfrom sklearn.cluster import KMeans\nindiviual_cluster=[]\nnum_cluster =5\nfor i in range(1,num_cluster):\n    kmeans = KMeans(n_clusters=i, random_state=0)\n    kmeans.fit(df)\n    indiviual_cluster.append(kmeans.inertia_)\n##    \nplt.figure(figsize=(16,5))\nplt.plot(range(1,num_cluster),indiviual_cluster)\nplt.title('Elbow Method',fontsize=20)\nplt.ylabel(\"Cluster Score\",fontsize=14)\nplt.xlabel('Number of cluster',fontsize=14)\n##\nprint('indiviual_cluster ',indiviual_cluster)","fe3a4fc5":"n_clusters=2\nkmeans = KMeans(n_clusters=n_clusters,init='random',random_state=0).fit(df)\npred=kmeans.predict(df)\n\n##\nprint('predict\\n',pred)\ndf['Cluster']=pd.DataFrame(pred,columns=['cluster'])\n##\nsns.lmplot(x='Hours',y='Scores',data=df,hue='Cluster')\nsns.lmplot(x='Hours',y='Scores',data=df,fit_reg=False,hue='Cluster',legend=True)\n##\nprint('Labels\\n',kmeans.labels_,'\\ncluster_centers_\\n',kmeans.cluster_centers_,'\\ninertia_\\n',kmeans.inertia_,'in n_clusters ',n_clusters)","22da6048":"# Conclusion\n\nfrom above diagram we get that slope and  y-intercept  \n\nPoint 1 : Delta varies dynamic to error and x value\n\nPoint 2:  On each Step Slope and y-intecept are decrases by small delta value\n\nfor\ndelta_m=error*x\n\ndelta_b=error\n\nm=m - delta_m * 0.001\n        \nb=b - delta_b * 0.001","61d06c2f":"# Gradient descent - Correct\n\ndelta_m=error*x\n\ndelta_b=error\n\nm=m - delta_m * 0.001\n        \nb=b - delta_b * 0.001","3b084db0":"# Gradient descent Try-1\n\ndelta_m=1\n\ndelta_b=1\n\nm=m+delta_m\n\nb=b+delta_b\n","a63c8870":"# Load","6f570220":"**What does Kmeans inertia mean?**\n\nInertia measures how well a dataset was clustered by K-Means. It is calculated by measuring the distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster. A good model is one with low inertia AND a low number of clusters ( K ).\n\n![](https:\/\/editor.analyticsvidhya.com\/uploads\/62725cluster0.PNG)","6361c452":"# Result 2\n\nfrom above diagram we get that slope and  y-intercept  **Negative**\n\nPoint 1 : Delta varies dynamic to error and x value\n\nPoint 2:  On each Step Slope and y-intecept increases by delta value\n\nfor\n\ndelta_m=error*x  # result dynamic to x and error\n\ndelta_b=error\n\nm=m+delta_m\n\nb=b+delta_b","a7231b98":"# Gradient descent Try-2.1\n\ndelta_m=error*x\n\ndelta_b=error\n\nm=m + delta_m * 0.001\n        \nb=b + delta_b * 0.001","c336fff5":"# 2. II K Means Clustering\n\n1.Convergence\n\nWe should take care of Outlier and Intialization of centroid (which randomly selected) may centroid is introduced to be a \"far away\" point  in K mean \n\ninit='k-means++' > smart way to Intialization of centroid\n\ninit{\u2018k-means++\u2019, \u2018random\u2019}, callable or array-like of shape (n_clusters, n_features), default=\u2019k-means++\u2019\n\nK-means++ is the algorithm which is used to overcome the drawback posed by the k-means algorithm.\n(likelihood of picking a point as centroid is corresponding to the distance from the closest, recently picked centroid.)\n\nK  > select by elbow method\n\nTake care of all point in one cluster are close together(distance small) and distance between two cluster is large\n\nAs it is unsupervised there is no target and we can measure it by silhoutte score \n\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html\n\n\n![](https:\/\/www.unioviedo.es\/compnum\/labs\/new\/d1.png)\n\n\nSilhouette Coefficient \n\nSilhouette Coefficient or silhouette score is a metric used to calculate the goodness of a clustering technique. Its value ranges from -1 to 1. \n\n![](https:\/\/miro.medium.com\/max\/712\/1*cUcY9jSBHFMqCmX-fp8BvQ.jpeg)\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.KMeans.html","9686d297":"# Gradient Descent","7812a748":"# Gradient descent Try-2\n\ndelta_m=error*x\n\ndelta_b=error\n\nm=m+delta_m\n\nb=b+delta_b","27f45349":"# Elbow Method","fe190343":"# Result 1\n\nfrom above diagram we get that slope and  y-intercept  **Increases Largely**\n\nPoint 1 : Delta become constant\n\nPoint 2:  On each Step Slope and y-intecept increases by 1 \n\nfor\n\ndelta_m=1  # constant\n\ndelta_b=1\n\nm=m+delta_m\n\nb=b+delta_b","69725ec0":"# Changing slope and  y-intercept in Each Try (Hours Vs Pass)","f9d1750e":"# Reference\n\nhttps:\/\/youtu.be\/PONM8A7Gwl4\n\nhttps:\/\/youtu.be\/vsWrXfO3wWw\n\nhttps:\/\/youtu.be\/XtE7hqFsYc4\n\nhttps:\/\/youtu.be\/_jg1UFoef1c","462081d3":"# Result 2.1\n\nfrom above diagram we get that slope and  y-intercept  **Negative**\n\nPoint 1 : Delta varies dynamic to error and x value\n\nPoint 2:  On each Step Slope and y-intecept are increases small by delta value\n\nfor\n\ndelta_m=error*x  \n\ndelta_b=error\n\nm=m + delta_m * 0.001\n        \nb=b + delta_b * 0.001"}}