{"cell_type":{"66417677":"code","904d245b":"code","2c2c9ca4":"code","c8d26323":"code","9626211b":"code","f79558bc":"code","f6945c1b":"code","08132739":"code","4ec96741":"code","aa9b235a":"code","6dbfbef6":"code","69748ea0":"code","cc88c79e":"code","3c03cc00":"code","df79bdaa":"code","7bc4ebab":"code","6cf5d912":"code","59ad40b0":"code","0d0cf066":"code","2ed9b642":"code","527ebc51":"code","b37ad65c":"code","aba1e02a":"code","cfab905f":"code","d1b1ccf1":"code","b495cf8a":"code","3bc7783d":"code","3049e118":"code","10812379":"code","e5679092":"code","da4fe070":"code","c346eed6":"code","06332605":"code","3b75d106":"code","10c2c52e":"code","26762ad9":"code","5c77ced2":"code","54e7676a":"markdown","0f593d59":"markdown","cfb00062":"markdown","9e2668c0":"markdown","08f462b7":"markdown","2226e79a":"markdown","965a175e":"markdown","5b46341c":"markdown","59fe0eac":"markdown","7aef9a6b":"markdown","7b508b5f":"markdown","a9a73582":"markdown","9c71336c":"markdown","31dcd915":"markdown","069f1408":"markdown","9113eb69":"markdown","5bde8948":"markdown","8cbc39b5":"markdown","17526611":"markdown","52e2013c":"markdown","0a8e112c":"markdown","e9c5c944":"markdown","d6a4df47":"markdown","2a074cc4":"markdown","83cbf02a":"markdown","5f0c2d71":"markdown","4caf5725":"markdown","0335358c":"markdown","a4fd8858":"markdown","40fa20b6":"markdown","c52fa0f2":"markdown","61d11f8a":"markdown","149f296d":"markdown","93da2aba":"markdown","8d5093f1":"markdown","426879f9":"markdown","cb7516e3":"markdown","5e71f51f":"markdown","f1a65322":"markdown","10ed35ed":"markdown","562fa140":"markdown","6f3bb43b":"markdown","b1ef1d6a":"markdown","db668216":"markdown","361482d6":"markdown","4d98dc97":"markdown","1c9369f5":"markdown","7dd6a0fd":"markdown","882beaaa":"markdown","7691389c":"markdown","6f2a75e4":"markdown","6d1d7701":"markdown","ce554cde":"markdown","f79fd740":"markdown","1c145873":"markdown","8bbdd43f":"markdown","2bcf0e1c":"markdown","f69ef1be":"markdown","2f102a6c":"markdown","6692eb80":"markdown","1ebad7ce":"markdown","c3fee406":"markdown","d6592901":"markdown","011a5101":"markdown","2bb2da27":"markdown","d18664c1":"markdown","57bcda63":"markdown","17ee2ff7":"markdown","6024d420":"markdown","4408eb81":"markdown","47834e98":"markdown","fdfcb3fb":"markdown","961f2614":"markdown","9313ba58":"markdown","e5991e33":"markdown","9ca38c37":"markdown","32284544":"markdown","d961a983":"markdown","58c928bb":"markdown"},"source":{"66417677":"import warnings\n\nimport numpy as np\nfrom numpy import array\nimport pandas as pd\nfrom pandas import concat\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.model_selection import ParameterGrid\n\npd.plotting.register_matplotlib_converters()\n\nwarnings.filterwarnings(\"ignore\")\n\nimport logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.CRITICAL)\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","904d245b":"!pip install pyramid-arima","2c2c9ca4":"import seaborn as sns\ndfatc_daily = pd.read_csv('..\/input\/salesdaily.csv')\nfig, axes = plt.subplots(8, 1, figsize=(10, 30), sharex=True)\nfor name, ax in zip(['M01AB','M01AE','N02BA','N02BE', 'N05B','N05C','R03','R06'], axes):\n    sns.boxplot(data=dfatc_daily, x='Month', y=name, ax=ax)","c8d26323":"fig, axes = plt.subplots(8, 1, figsize=(10, 30), sharex=True)\nfor name, ax in zip(['M01AB','M01AE','N02BA','N02BE', 'N05B','N05C','R03','R06'], axes):\n    sns.boxplot(data=dfatc_daily, x='Weekday Name', y=name, ax=ax)","9626211b":"dfatc_daily=pd.read_csv('..\/input\/salesdaily.csv')\ncols_plot = ['M01AB','M01AE','N02BA','N02BE', 'N05B','N05C','R03','R06']\ndfatc_365d = dfatc_daily[cols_plot].rolling(window=365, center=True).mean()\ndfatc_30d = dfatc_daily[cols_plot].rolling(30, center=True).mean()\ndfatc_std = dfatc_daily[cols_plot].rolling(30, center=True).std()\nsubplotindex=0\nnumrows=4\nnumcols=2\nfig, ax = plt.subplots(numrows, numcols, figsize=(18, 12))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\n\nfor x in cols_plot:\n    rowindex=math.floor(subplotindex\/numcols)\n    colindex=subplotindex-(rowindex*numcols)\n    ax[rowindex,colindex].plot(dfatc_daily.loc[:,x], linewidth=0.5, label='Daily sales')\n    ax[rowindex,colindex].plot(dfatc_30d.loc[:,x], label='30-d Rolling Mean')\n    ax[rowindex,colindex].plot(dfatc_365d.loc[:,x], color='0.2', linewidth=3, label='365-d Rolling Mean')\n    ax[rowindex,colindex].plot(dfatc_std.loc[:,x], color='0.5', linewidth=3, label='30-d Rolling Std')\n    ax[rowindex,colindex].set_ylabel('Sales')\n    ax[rowindex,colindex].legend()\n    ax[rowindex,colindex].set_title('Trends in '+x+' drugs sales');   \n    subplotindex=subplotindex+1\nplt.show()","f79558bc":"fig, ax = plt.subplots(figsize=(30, 10))\nfor nm in cols_plot:\n    ax.plot(dfatc_365d[nm], label=nm, marker='.', linestyle='-', linewidth=0.5)\n    ax.legend()\n    ax.set_ylabel('Drug sales')\n    ax.set_title('Trends in Drug Sales for different groups (365-d Rolling Means)');","f6945c1b":"from statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(dfatc_daily['M01AB'].rolling(30, center=True).mean().dropna(), freq=365, filt=None)\nplt.rcParams[\"figure.figsize\"] = (20,20)\nresult.plot()\nplt.show()","08132739":"df = pd.read_csv('..\/input\/salesweekly.csv')\nfor x in ['M01AB','M01AE','N02BA','N02BE','N05B','N05C','R03','R06']:\n    result = seasonal_decompose(df[x], freq=52, model='additive')\n    dfs = pd.concat([result.trend, result.seasonal, result.resid, result.observed], axis=1)\n    dfs.columns = ['trend', 'seasonal', 'residuals', 'observed']\n    dfs=dfs.dropna()\n    res=dfs['residuals'].values\n    obs=dfs['observed'].values\n    resmean=np.mean(np.abs(res))\n    obsmean=np.mean(np.abs(obs))\n    perc=resmean*100\/obsmean\n    print(x+' RESMEAN:'+str(resmean)+', OBSMEAN:'+str(obsmean)+', PERC:'+str(perc)+'%')","4ec96741":"df=pd.read_csv('..\/input\/salesweekly.csv')\nfrom statsmodels.tsa.stattools import adfuller\n\nfor x in ['M01AB','M01AE','N02BA','N02BE', 'N05B','N05C','R03','R06']:\n    dftest = adfuller(df[x], regression='ct', autolag='AIC')\n    print(\"ADF test for \"+x)\n    print(\"-----------------------------\")\n    print(\"Test statistic = {:.3f}\".format(dftest[0]))\n    print(\"P-value = {:.3f}\".format(dftest[1]))\n    print(\"Critical values :\")\n    for k, v in dftest[4].items():\n        print(\"\\t{}: {} - The data is {} stationary with {}% confidence\".format(k, v, \"not\" if v<dftest[0] else \"\", 100-int(k[:-1])))","aa9b235a":"from statsmodels.tsa.stattools import kpss\nwarnings.filterwarnings(\"ignore\")\ndf=pd.read_csv('..\/input\/salesweekly.csv')\nfor x in ['M01AB','M01AE','N02BA','N02BE', 'N05B','N05C','R03','R06']:\n    print(\" > Is \"+x+\" data stationary ?\")\n    dftest = kpss(np.log(df[x]), 'ct')\n    print(\"Test statistic = {:.3f}\".format(dftest[0]))\n    print(\"P-value = {:.3f}\".format(dftest[1]))\n    print(\"Critical values :\")\n    for k, v in dftest[3].items():\n        print(\"\\t{}: {}\".format(k, v))","6dbfbef6":"df = pd.read_csv('..\/input\/salesweekly.csv')    \ndef ApEn(U, m, r):\n    def _maxdist(x_i, x_j):\n        return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n    def _phi(m):\n        x = [[U[j] for j in range(i, i + m - 1 + 1)] for i in range(N - m + 1)]\n        C = [len([1 for x_j in x if _maxdist(x_i, x_j) <= r]) \/ (N - m + 1.0) for x_i in x]\n        return (N - m + 1.0)**(-1) * sum(np.log(C))\n    N = len(U)\n    return abs(_phi(m+1) - _phi(m))\n\nfor x in ['M01AB','M01AE','N02BA','N02BE','N05B','N05C','R03','R06']:\n    print(x + ': ' + str(ApEn(df[x].values, m=2, r=0.2*np.std(df[x].values))))","69748ea0":"from statsmodels.graphics.tsaplots import plot_acf\ndf = pd.read_csv('..\/input\/salesweekly.csv')\nsubplotindex=0\nnumrows=4\nnumcols=2\nfig, ax = plt.subplots(numrows, numcols, figsize=(18,12))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\nwith plt.rc_context():\n    plt.rc(\"figure\", figsize=(18,12))\n    for x in ['M01AB','M01AE','N02BA','N02BE', 'N05B','N05C','R03','R06']:\n        rowindex=math.floor(subplotindex\/numcols)\n        colindex=subplotindex-(rowindex*numcols)\n        plot_acf(df[x], lags=300, title=x, ax=ax[rowindex,colindex])\n        subplotindex=subplotindex+1\n","cc88c79e":"from statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.graphics.tsaplots import plot_acf\ndf = pd.read_csv('..\/input\/salesweekly.csv')\nsubplotindex=0\nnumrows=4\nnumcols=2\nfig, ax = plt.subplots(numrows, numcols, figsize=(18,12))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\nwith plt.rc_context():\n    plt.rc(\"figure\", figsize=(14,6))\n    for x in ['M01AB','M01AE','N02BA','N02BE', 'N05B','N05C','R03','R06']:\n        rowindex=math.floor(subplotindex\/numcols)\n        colindex=subplotindex-(rowindex*numcols)\n        plot_pacf(df[x], lags=100, title=x, ax=ax[rowindex,colindex])\n        subplotindex=subplotindex+1","3c03cc00":"dfatch=pd.read_csv('..\/input\/saleshourly.csv')\ndfatch['datum']= pd.to_datetime(dfatch['datum']) \n\ngrp1=dfatch.groupby(dfatch.datum.dt.hour)['M01AB'].mean()\ngrp2=dfatch.groupby(dfatch.datum.dt.hour)['M01AE'].mean()\ngrp3=dfatch.groupby(dfatch.datum.dt.hour)['N02BA'].mean()\ngrp6=dfatch.groupby(dfatch.datum.dt.hour)['N05C'].mean()\ngrp7=dfatch.groupby(dfatch.datum.dt.hour)['R03'].mean()\ngrp8=dfatch.groupby(dfatch.datum.dt.hour)['R06'].mean()\n\nplt.title('Daily average sales')\nplt.xlabel('Time of day')\nplt.ylabel('Quantity of sale')\n\ngrp1.plot(figsize=(8,6))\ngrp2.plot(figsize=(8,6))\ngrp3.plot(figsize=(8,6))\ngrp6.plot(figsize=(8,6))\ngrp7.plot(figsize=(8,6))\ngrp8.plot(figsize=(8,6))\n\nplt.legend(['M01AB', 'M01AE', 'N02BA', 'N05C', 'R03', 'R06'], loc='upper left')\n\nplt.show()","df79bdaa":"dfatcw=pd.read_csv('..\/input\/salesdaily.csv')\ndfatcw['datum']= pd.to_datetime(dfatcw['datum'])\ndays = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday', 'Sunday']\nplt.rcParams.update({'font.size': 10})\n\ngrp1=dfatcw.loc[dfatcw.datum>'2018-01-01'].groupby(dfatch.datum.dt.weekday_name)['M01AB'].mean().reindex(days)\ngrp2=dfatcw.loc[dfatcw.datum>'2018-01-01'].groupby(dfatch.datum.dt.weekday_name)['M01AE'].mean().reindex(days)\ngrp3=dfatcw.loc[dfatcw.datum>'2018-01-01'].groupby(dfatch.datum.dt.weekday_name)['N02BA'].mean().reindex(days)\ngrp6=dfatcw.loc[dfatcw.datum>'2018-01-01'].groupby(dfatch.datum.dt.weekday_name)['N05C'].mean().reindex(days)\ngrp7=dfatcw.loc[dfatcw.datum>'2018-01-01'].groupby(dfatch.datum.dt.weekday_name)['R03'].mean().reindex(days)\ngrp8=dfatcw.loc[dfatcw.datum>'2018-01-01'].groupby(dfatch.datum.dt.weekday_name)['R06'].mean().reindex(days)\n\ngrp1.plot(figsize=(8,6))\ngrp2.plot(figsize=(8,6))\ngrp3.plot(figsize=(8,6))\ngrp6.plot(figsize=(8,6))\ngrp7.plot(figsize=(8,6))\ngrp8.plot(figsize=(8,6))\n\nplt.legend(['M01AB', 'M01AE', 'N02BA', 'N05C', 'R03', 'R06'], loc='upper right')\nplt.title('Weekly average sales in 2018-2019')\nplt.xlabel('Day of week')\nplt.ylabel('Quantity of sale')\n\nplt.show()","7bc4ebab":"resultsRolling={'M01AB':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n     'M01AE':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n     'N02BA':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n     'N02BE':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n     'N05B':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n     'N05C':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n     'R03':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n     'R06':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]}\nresultsRollingdf = pd.DataFrame(resultsRolling)\nresultsRollingdf.index = ['Naive MSE', 'Naive MAPE', 'Seasonal Naive MSE', 'Seasonal Naive MAPE', \n                          'ARIMA MSE', 'ARIMA MAPE', 'AutoARIMA MSE', 'AutoARIMA MAPE',\n                         'Prophet MSE', 'Prophet MAPE']\nresultsLongterm={'M01AB':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n     'M01AE':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n     'N02BA':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n     'N02BE':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n     'N05B':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n     'N05C':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n     'R03':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n     'R06':[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]}\nresultsLongtermdf = pd.DataFrame(resultsLongterm)\nresultsLongtermdf.index = ['Average MSE', 'Average MAPE', 'ARIMA MSE', 'ARIMA MAPE', \n                           'AutoARIMA MSE', 'AutoARIMA MAPE', 'Prophet MSE', 'Prophet MAPE',\n                          'Vanilla LSTM MSE', 'Vanilla LSTM MAPE', 'Stacked LSTM MSE', 'Stacked LSTM MAPE',\n                          'Bidirectional LSTM MSE', 'Bidirectional LSTM MAPE']","6cf5d912":"df=pd.read_csv('..\/input\/salesweekly.csv')\nsubplotindex=0\nnumrows=4\nnumcols=2\nfig, ax = plt.subplots(numrows, numcols, figsize=(18,15))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\nfor x in ['M01AB','M01AE','N02BA','N02BE','N05B','N05C','R03','R06']:\n    rowindex=math.floor(subplotindex\/numcols)\n    colindex=subplotindex-(rowindex*numcols)\n    ds=df[x]\n    dataframe = concat([ds.shift(1), ds], axis=1)\n    dataframe.columns = ['t+1', 't-1']\n    size = len(dataframe)-50\n    X=dataframe['t-1']\n    Y=dataframe['t+1']\n    test, predictions = X[size:len(X)], Y[size:len(Y)]\n    error = mean_squared_error(test, predictions)\n    perror = mean_absolute_percentage_error(test, predictions)\n    resultsRollingdf.loc['Naive MSE',x]=error\n    resultsRollingdf.loc['Naive MAPE',x]=perror\n    ax[rowindex,colindex].set_title(x+' (MSE=' + str(round(error,2))+', MAPE='+ str(round(perror,2)) +'%)')\n    ax[rowindex,colindex].legend(['Real', 'Predicted'], loc='upper left')\n    ax[rowindex,colindex].plot(test)\n    ax[rowindex,colindex].plot(predictions, color='red')\n    subplotindex=subplotindex+1\nplt.show()","59ad40b0":"df=pd.read_csv('..\/input\/salesweekly.csv')\nsubplotindex=0\nnumrows=4\nnumcols=2\nfig, ax = plt.subplots(numrows, numcols, figsize=(18,15))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\nfor x in ['M01AB','M01AE','N02BA','N02BE','N05B','N05C','R03','R06']:\n    rowindex=math.floor(subplotindex\/numcols)\n    colindex=subplotindex-(rowindex*numcols)\n    X=df[x].values\n    size = len(X)-50\n    test = X[size:len(X)] \n    mean = np.mean(X[0:size])\n    predictions = np.full(50,mean)\n    error = mean_squared_error(test, predictions)\n    perror = mean_absolute_percentage_error(test, predictions)\n    resultsLongtermdf.loc['Average MSE',x]=error\n    resultsLongtermdf.loc['Average MAPE',x]=perror\n    ax[rowindex,colindex].set_title(x+' (MSE=' + str(round(error,2))+', MAPE='+ str(round(perror,2)) +'%)')\n    ax[rowindex,colindex].legend(['Real', 'Predicted'], loc='upper left')\n    ax[rowindex,colindex].plot(test)\n    ax[rowindex,colindex].plot(predictions, color='red')\n    subplotindex=subplotindex+1\nplt.show()","0d0cf066":"df=pd.read_csv('..\/input\/salesweekly.csv')\nsubplotindex=0\nnumrows=1\nnumcols=3\nfig, ax = plt.subplots(numrows, numcols, figsize=(18,4))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\nfor x in ['N02BE','R03','R06']:\n    rowindex=math.floor(subplotindex\/numcols)\n    colindex=subplotindex-(rowindex*numcols)\n    X=df[x].values\n    size = len(X)-52\n    test = X[size:len(X)]\n    train = X[0:size]\n    predictions=list()\n    history = [x for x in train]\n    for i in range(len(test)):\n        obs=list()\n        for y in range(1,5):\n            obs.append(train[-(y*52)+i])\n        yhat = np.mean(obs)\n        predictions.append(yhat)\n        history.append(test[i])\n    error = mean_squared_error(test, predictions)\n    perror = mean_absolute_percentage_error(test, predictions)\n    resultsRollingdf.loc['Seasonal Naive MSE',x]=error\n    resultsRollingdf.loc['Seasonal Naive MAPE',x]=perror\n    ax[colindex].set_title(x+' (MSE=' + str(round(error,2))+', MAPE='+ str(round(perror,2)) +'%)')\n    ax[colindex].legend(['Real', 'Predicted'], loc='upper left')\n    ax[colindex].plot(test)\n    ax[colindex].plot(predictions, color='red')\n    subplotindex=subplotindex+1\nplt.show()    ","2ed9b642":"import statsmodels.api as sm\ndf=pd.read_csv('..\/input\/salesweekly.csv')\nwarnings.filterwarnings(\"ignore\")\nfor x in ['M01AB','M01AE','N02BA','N02BE', 'N05B','N05C','R03','R06']:\n    resDiff = sm.tsa.arma_order_select_ic(df[x], max_ar=5, max_ma=5, ic='aic', trend='c')\n    print('ARMA(p,q,'+x+') =',resDiff['aic_min_order'],'is the best.')","527ebc51":"def evaluate_arima_model(X, arima_order):\n    train_size = int(len(X) * 0.66)\n    train, test = X[0:train_size], X[train_size:]\n    history = [x for x in train]\n    predictions = list()\n    for t in range(len(test)):\n        model = ARIMA(history, order=arima_order)\n        model_fit = model.fit(disp=0)\n        yhat = model_fit.forecast()[0]\n        predictions.append(yhat)\n        history.append(test[t])\n    error = mean_squared_error(test, predictions)\n    return error\n\ndef evaluate_models(f, dataset, p_values, d_values, q_values):\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    mse = evaluate_arima_model(dataset, order)\n                    if mse < best_score:\n                        best_score, best_cfg = mse, order\n                except:\n                    continue\n    print(f+' - Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))\n\np_values = range(0, 6)\nd_values = range(0, 2)\nq_values = range(0, 6)\n\nwarnings.filterwarnings(\"ignore\")\n\ndf=pd.read_csv('..\/input\/salesweekly.csv')\n\nfor f in ['M01AB','M01AE','N02BA','N02BE','N05B','N05C','R03','R06']:\n    evaluate_models(f, df[f].values, p_values, d_values, q_values)","b37ad65c":"def evaluate_arima_model(X, arima_order):\n    train_size = int(len(X) - 50)\n    train, test = X[0:train_size], X[train_size:]\n    model = ARIMA(train, order=arima_order)\n    model_fit = model.fit()\n    forecast = model_fit.predict(1,len(test))\n    error = mean_squared_error(test, forecast)\n    return error\n\ndef evaluate_models(f, dataset, p_values, d_values, q_values):\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    mse = evaluate_arima_model(dataset, order)\n                    if mse < best_score:\n                        best_score, best_cfg = mse, order\n                except:\n                    continue\n    print(f+' - Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))\n\np_values = range(0, 6)\nd_values = range(0, 2)\nq_values = range(0, 6)\n\nwarnings.filterwarnings(\"ignore\")\n\nfor f in ['M01AB','M01AE','N02BA','N02BE','N05B','N05C','R03','R06']:\n    evaluate_models(f, df[f].values, p_values, d_values, q_values)","aba1e02a":"df=pd.read_csv('..\/input\/salesweekly.csv')\n\nM01AB= {'series':'M01AB','p':0,'d':0,'q':0}\nM01AE= {'series':'M01AE','p':2,'d':0,'q':0}\nN02BA= {'series':'N02BA','p':5,'d':1,'q':1}\nN02BE= {'series':'N02BE','p':2,'d':0,'q':0}\nN05B= {'series':'N05B','p':0,'d':0,'q':5}\nN05C= {'series':'N05C','p':0,'d':0,'q':1}\nR03= {'series':'R03','p':5,'d':1,'q':1}\nR06= {'series':'R06','p':1,'d':0,'q':1}\n\nsubplotindex=0\nnumrows=4\nnumcols=2\nfig, ax = plt.subplots(numrows, numcols, figsize=(18,15))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\n\nwarnings.filterwarnings(\"ignore\")\n\nfor x in [M01AB,M01AE,N02BA,N02BE,N05B,N05C,R03,R06]:\n    rowindex=math.floor(subplotindex\/numcols)\n    colindex=subplotindex-(rowindex*numcols)\n    X = df[x['series']].values\n    size = len(X)-50\n    train, test = X[0:size], X[size:len(X)]\n    history = [x for x in train]\n    predictions = list()\n    for t in range(len(test)):\n        model = ARIMA(history, order=(x['p'],x['d'],x['q']))\n        model_fit = model.fit(disp=0)\n        output = model_fit.forecast()\n        yhat = output[0]\n        predictions.append(yhat)\n        obs = test[t]\n        history.append(obs)\n    error = mean_squared_error(test, predictions)\n    perror = mean_absolute_percentage_error(test, predictions)\n    resultsRollingdf.loc['ARIMA MSE',x['series']]=error\n    resultsRollingdf.loc['ARIMA MAPE',x['series']]=perror\n    ax[rowindex,colindex].set_title(x['series']+' (MSE=' + str(round(error,2))+', MAPE='+ str(round(perror,2)) +'%)')\n    ax[rowindex,colindex].legend(['Real', 'Predicted'], loc='upper left')\n    ax[rowindex,colindex].plot(test)\n    ax[rowindex,colindex].plot(predictions, color='red')\n    subplotindex=subplotindex+1\nplt.show()","cfab905f":"df=pd.read_csv('..\/input\/salesweekly.csv')\n\nM01AB= {'series':'M01AB','p':0,'d':0,'q':0}\nM01AE= {'series':'M01AE','p':4,'d':0,'q':4}\nN02BA= {'series':'N02BA','p':0,'d':0,'q':0}\nN02BE= {'series':'N02BE','p':0,'d':0,'q':0}\nN05B= {'series':'N05B','p':0,'d':0,'q':0}\nN05C= {'series':'N05C','p':2,'d':0,'q':2}\nR03= {'series':'R03','p':0,'d':0,'q':0}\nR06= {'series':'R06','p':2,'d':0,'q':2}\n\nsubplotindex=0\nnumrows=3\nnumcols=3\nfig, ax = plt.subplots(numrows, numcols, figsize=(18,12))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\n\nwarnings.filterwarnings(\"ignore\")\n\nfor x in [M01AB,M01AE,N02BA,N02BE,N05B,N05C,R03,R06]:\n    rowindex=math.floor(subplotindex\/numcols)\n    colindex=subplotindex-(rowindex*numcols)\n    X = df[x['series']].values\n    size = int(len(X) - 50)\n    train, test = X[0:size], X[size:len(X)]\n    model = ARIMA(train, order=(x['p'],x['d'],x['q']))\n    model_fit = model.fit()\n    forecast = model_fit.predict(1,len(test))\n    error = mean_squared_error(test, forecast)\n    perror = mean_absolute_percentage_error(test, forecast)\n    resultsLongtermdf.loc['ARIMA MSE',x['series']]=error\n    resultsLongtermdf.loc['ARIMA MAPE',x['series']]=perror\n    ax[rowindex,colindex].set_title(x['series']+' (MSE=' + str(round(error,2))+', MAPE='+ str(round(perror,2)) +'%)')\n    ax[rowindex,colindex].legend(['Real', 'Predicted'], loc='upper left')\n    ax[rowindex,colindex].plot(test)\n    ax[rowindex,colindex].plot(forecast, color='red')\n    subplotindex=subplotindex+1\nplt.show()","d1b1ccf1":"from pyramid.arima import auto_arima\n\ndf=pd.read_csv('..\/input\/salesweekly.csv')\nsubplotindex=0\nnumrows=2\nnumcols=2\nfig, ax = plt.subplots(numrows, numcols, figsize=(18,15))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\n\nwarnings.filterwarnings(\"ignore\")\n\nfor x in ['M01AB','M01AE','N05B','N05C']:\n    rowindex=math.floor(subplotindex\/numcols)\n    colindex=subplotindex-(rowindex*numcols)\n    X = df[x].values\n    size = len(X)-50\n    train, test = X[0:size], X[size:len(X)]\n    history = [c for c in train]\n    predictions = list()\n    for t in range(len(test)):\n        if (x=='N02BA' or x=='N02BE' or x=='R03' or x=='R06'):\n            model = auto_arima(X, start_p=1, start_q=1,\n                           max_p=5, max_q=5, m=52, max_d=1, max_D=1,\n                           start_P=0, start_Q=0, max_P=5, max_Q=5, seasonal=True,\n                           trace=False,\n                           error_action='ignore',\n                           suppress_warnings=True, \n                           stepwise=True)\n        else:\n            model = auto_arima(X, start_p=1, start_q=1,\n                           max_p=5, max_q=5, max_d=1,\n                           trace=False, seasonal=False,\n                           error_action='ignore',\n                           suppress_warnings=True, \n                           stepwise=True)\n        model_fit = model.fit(history)\n        output = model_fit.predict(n_periods=1)\n        yhat = output[0]\n        predictions.append(yhat)\n        obs = test[t]\n        history.append(obs)\n    error = mean_squared_error(test, predictions)\n    perror = mean_absolute_percentage_error(test, predictions)\n    resultsRollingdf.loc['AutoARIMA MSE',x]=error\n    resultsRollingdf.loc['AutoARIMA MAPE',x]=perror\n    ax[rowindex,colindex].set_title(x+' (MSE=' + str(round(error,2))+', MAPE='+ str(round(perror,2)) +'%)')\n    ax[rowindex,colindex].legend(['Real', 'Predicted'], loc='upper left')\n    ax[rowindex,colindex].plot(test)\n    ax[rowindex,colindex].plot(predictions, color='red')\n    subplotindex=subplotindex+1\nplt.show()","b495cf8a":"from pyramid.arima import auto_arima\n\ndf=pd.read_csv('..\/input\/salesweekly.csv')\nsubplotindex=0\nnumrows=4\nnumcols=2\nfig, ax = plt.subplots(numrows, numcols, figsize=(18,15))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\n\nwarnings.filterwarnings(\"ignore\")\n\nfor x in ['M01AB','M01AE','N02BA','N02BE','N05B','N05C','R03','R06']:\n    rowindex=math.floor(subplotindex\/numcols)\n    colindex=subplotindex-(rowindex*numcols)\n    X = df[x].values\n    size = int(len(X) - 50)\n    train, test = X[0:size], X[size:len(X)]\n    if (x=='N02BE' or x=='R03' or x=='R06'):\n        model = auto_arima(X, start_p=1, start_q=1,\n                           max_p=5, max_q=5, m=52, max_d=1, max_D=1,\n                           start_P=0, start_Q=0, max_P=5, max_Q=5, seasonal=True,\n                           trace=False,\n                           error_action='ignore',  \n                           suppress_warnings=True, \n                           stepwise=True)\n    else:\n        model = auto_arima(X, start_p=1, start_q=1,\n                           max_p=5, max_q=5, max_d=1,\n                           trace=False, seasonal=False,\n                           error_action='ignore',\n                           suppress_warnings=True, \n                           stepwise=True)\n    model_fit = model.fit(train)\n    forecast = model_fit.predict(n_periods=len(test))\n    error = mean_squared_error(test, forecast)\n    perror = mean_absolute_percentage_error(test, predictions)\n    resultsLongtermdf.loc['AutoARIMA MSE',x]=error\n    resultsLongtermdf.loc['AutoARIMA MAPE',x]=perror\n    ax[rowindex,colindex].set_title(x+' (MSE=' + str(round(error,2))+', MAPE='+ str(round(perror,2)) +'%)')\n    ax[rowindex,colindex].legend(['Real', 'Predicted'], loc='upper left')\n    ax[rowindex,colindex].plot(test)\n    ax[rowindex,colindex].plot(forecast, color='red')\n    subplotindex=subplotindex+1\nplt.show()","3bc7783d":"import fbprophet","3049e118":"df=pd.read_csv('..\/input\/salesweekly.csv')\n\nM01AB= {\n    'series':'M01AB',\n    'params_grid':{'growth':['linear'],'changepoint_prior_scale':[10,30,50],\n               'interval_width':[0.0005]\n              }\n}\nM01AE= {\n    'series':'M01AE',\n    'params_grid':{'growth':['linear'],'changepoint_prior_scale':[0.01,0.05,0.1],\n               'interval_width':[0.0005]\n              }\n}\nN02BA= {\n    'series':'N02BA',\n    'params_grid':{'growth':['linear'],'changepoint_prior_scale':[0.005,0.01,0.05,0.1],\n                   'interval_width':[0.0005]\n              }\n}\nN02BE= {\n    'series':'N02BE',\n    'params_grid':{'growth':['linear'],'changepoint_prior_scale':[5,10,50],'seasonality_prior_scale':[150,170,200],\n               'interval_width':[0.0005]\n              }\n}\nN05B= {\n    'series':'N05B',\n    'params_grid':{'growth':['linear'],'changepoint_prior_scale':[1,5,10],\n               'interval_width':[0.0005]\n              }\n}\nN05C= {\n    'series':'N05C',\n    'params_grid':{'growth':['linear'],'changepoint_prior_scale':[0.05,0.08,0.1,0.5],\n               'interval_width':[0.0005]\n              }\n}\nR03= {\n    'series':'R03',\n    'params_grid':{'growth':['linear'],'changepoint_prior_scale':[0.01,0.05,0.1],'seasonality_prior_scale':[120,160,200],\n               'interval_width':[0.0005]\n              }\n}\nR06= {\n    'series':'R06',\n    'params_grid':{'growth':['linear'],'changepoint_prior_scale':[0.01,0.05,0.1],'seasonality_prior_scale':[100,120,160,200],\n               'interval_width':[0.0005]\n              }\n}\n\nr=[M01AB,M01AE,N02BA,N02BE,N05B,N05C,R03,R06]\nwarnings.filterwarnings(\"ignore\")\n\nfor x in r:\n    dfg=df[['datum',x['series']]]\n    dfg = dfg.rename(columns={'datum': 'ds', x['series']: 'y'})\n    size = int(len(dfg) - 50)\n    dfgtrain=dfg.loc[0:size,:]\n    dfgtest=dfg.loc[size+1:len(dfg),:]\n    predictions = list()\n    minError=0\n    grid = ParameterGrid(x['params_grid'])\n    for p in grid:\n        model = fbprophet.Prophet(**p, daily_seasonality=False, weekly_seasonality=False)\n        if(x['series']=='N02BE' or x['series']=='R03' or x['series']=='R06'):\n            model=model.add_seasonality(\n                                name='yearly',\n                                period=365.25,\n                                fourier_order=13)\n        model_fit = model.fit(dfgtrain)\n        future = model.make_future_dataframe(periods=50, freq='W')\n        output = model.predict(future)\n        predictions=output.loc[size+2:len(dfg),:]['yhat'].values\n        error = mean_squared_error(dfgtest['y'].values, predictions)\n        if(minError>0):\n            if(error<minError):\n                minError=error\n                minP=p\n        else:\n            minError=error\n            minP=p\n\n    print(minP)\n    print('Test MSE ('+x['series']+'): %.3f' % minError)","10812379":"df=pd.read_csv('..\/input\/salesweekly.csv')\n\nsubplotindex=0\nnumrows=4\nnumcols=2\nfig, ax = plt.subplots(numrows, numcols, figsize=(18,15))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\n\nwarnings.filterwarnings(\"ignore\")\n\nM01AB= {'series':'M01AB','params_grid':{'changepoint_prior_scale':30,'interval_width':0.0005}}\nM01AE= {'series':'M01AE','params_grid':{'changepoint_prior_scale':0.05,'interval_width':0.0005}}\nN02BA= {'series':'N02BA','params_grid':{'changepoint_prior_scale':0.005,'interval_width':0.0005}}\nN02BE= {'series':'N02BE','params_grid':{'changepoint_prior_scale':10,'seasonality_prior_scale':170,'interval_width':0.0005}}\nN05B= {'series':'N05B','params_grid':{'changepoint_prior_scale':5,'interval_width':0.0005}}\nN05C= {'series':'N05C','params_grid':{'changepoint_prior_scale':0.5,'interval_width':0.005}}\nR03= {'series':'R03','params_grid':{'changepoint_prior_scale':0.05,'seasonality_prior_scale':160,'interval_width':0.0005}}\nR06= {'series':'R06','params_grid':{'changepoint_prior_scale':0.05,'seasonality_prior_scale':120,'interval_width':0.0005}}\n\nr=[M01AB,M01AE,N02BA,N02BE,N05B,N05C,R03,R06]\n\nfor x in r:\n    rowindex=math.floor(subplotindex\/numcols)\n    colindex=subplotindex-(rowindex*numcols)\n    dfg=df[['datum',x['series']]]\n    dfg = dfg.rename(columns={'datum': 'ds', x['series']: 'y'})\n    size = len(dfg) - 50\n    dfgtrain=dfg.loc[0:size,:]\n    dfgtest=dfg.loc[size+1:len(dfg),:]\n    history = dfgtrain.copy()\n    predictions = list()\n    \n    for t in dfgtest['ds'].values:\n        model = fbprophet.Prophet(changepoint_prior_scale=x['params_grid']['changepoint_prior_scale'],\n                                  growth='linear', \n                                  interval_width=x['params_grid']['interval_width'], \n                                  daily_seasonality=False, \n                                  weekly_seasonality=False\n                           )\n        if(x['series']=='N02BE' or x['series']=='R03' or x['series']=='R06'):\n            model=model.add_seasonality(\n                                name='yearly',\n                                period=365.25,\n                                prior_scale=x['params_grid']['seasonality_prior_scale'],\n                                fourier_order=13)\n        model_fit = model.fit(history)\n        future = model.make_future_dataframe(periods=1, freq='W')\n        output = model.predict(future)\n        yhat = output.loc[output.ds==t]['yhat'].values[0]\n        predictions.append(yhat)\n        obs = dfgtest.loc[dfgtest.ds==t]['y'].values[0]\n        dd=pd.DataFrame([[t,obs]],columns=['ds','y'])\n        history=history.append(dd)\n        \n    error = mean_squared_error(dfgtest['y'].values, predictions)\n    perror = mean_absolute_percentage_error(dfgtest['y'].values, predictions)\n    resultsRollingdf.loc['Prophet MSE',x['series']]=error\n    resultsRollingdf.loc['Prophet MAPE',x['series']]=perror\n    ax[rowindex,colindex].set_title(x['series']+' (MSE=' + str(round(error,2))+', MAPE='+ str(round(perror,2)) +'%)')\n    ax[rowindex,colindex].legend(['Real', 'Predicted'], loc='upper left')\n    ax[rowindex,colindex].plot(dfgtest['y'].values)\n    ax[rowindex,colindex].plot(predictions, color='red')\n    subplotindex=subplotindex+1\nplt.show()","e5679092":"df=pd.read_csv('..\/input\/salesweekly.csv')\n\nsubplotindex=0\nnumrows=4\nnumcols=2\nfig, ax = plt.subplots(numrows, numcols, figsize=(18,15))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\n\nwarnings.filterwarnings(\"ignore\")\n\nM01AB= {'series':'M01AB','params_grid':{'changepoint_prior_scale':30,'interval_width':0.0005}}\nM01AE= {'series':'M01AE','params_grid':{'changepoint_prior_scale':0.05,'interval_width':0.0005}}\nN02BA= {'series':'N02BA','params_grid':{'changepoint_prior_scale':0.005,'interval_width':0.0005}}\nN02BE= {'series':'N02BE','params_grid':{'changepoint_prior_scale':10,'seasonality_prior_scale':170,'interval_width':0.0005}}\nN05B= {'series':'N05B','params_grid':{'changepoint_prior_scale':5,'interval_width':0.0005}}\nN05C= {'series':'N05C','params_grid':{'changepoint_prior_scale':0.5,'interval_width':0.005}}\nR03= {'series':'R03','params_grid':{'changepoint_prior_scale':0.05,'seasonality_prior_scale':160,'interval_width':0.0005}}\nR06= {'series':'R06','params_grid':{'changepoint_prior_scale':0.05,'seasonality_prior_scale':120,'interval_width':0.0005}}\n\nr=[M01AB,M01AE,N02BA,N02BE,N05B,N05C,R03,R06]\n\nfor x in r:\n    rowindex=math.floor(subplotindex\/numcols)\n    colindex=subplotindex-(rowindex*numcols)\n    dfg=df[['datum',x['series']]]\n    dfg = dfg.rename(columns={'datum': 'ds', x['series']: 'y'})\n    size = int(len(dfg) - 50)\n    dfgtrain=dfg.loc[0:size,:]\n    dfgtest=dfg.loc[size+1:len(dfg),:]\n    predictions = list()\n    model = fbprophet.Prophet(changepoint_prior_scale=x['params_grid']['changepoint_prior_scale'],\n                              growth='linear', interval_width=x['params_grid']['interval_width'], \n                              daily_seasonality=False, \n                              weekly_seasonality=False\n                           )\n    if(x['series']=='N02BE' or x['series']=='R03' or x['series']=='R06'):\n        model=model.add_seasonality(\n                                name='yearly',\n                                period=365.25,\n                                prior_scale=x['params_grid']['seasonality_prior_scale'],\n                                fourier_order=13)\n    model_fit = model.fit(dfgtrain)\n    future = model.make_future_dataframe(periods=50, freq='W')\n    output = model.predict(future)\n    predictions=output.loc[size+2:len(dfg),:]['yhat'].values\n    \n    error = mean_squared_error(dfgtest['y'].values, predictions)\n    perror = mean_absolute_percentage_error(dfgtest['y'].values, predictions)\n    resultsLongtermdf.loc['Prophet MSE',x['series']]=error\n    resultsLongtermdf.loc['Prophet MAPE',x['series']]=perror\n    ax[rowindex,colindex].set_title(x['series']+' (MSE=' + str(round(error,2))+', MAPE='+ str(round(perror,2)) +'%)')\n    ax[rowindex,colindex].legend(['Real', 'Predicted'], loc='upper left')\n    ax[rowindex,colindex].plot(dfgtest['y'].values)\n    ax[rowindex,colindex].plot(predictions, color='red')\n    subplotindex=subplotindex+1\nplt.show()","da4fe070":"seed_value= 0\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\nimport random\nrandom.seed(seed_value)\nimport numpy as np\nnp.random.seed(seed_value)\nimport tensorflow as tf\ntf.random.set_seed(seed_value)\n\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)","c346eed6":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Bidirectional\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef split_sequence(sequence, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        end_ix = i + n_steps\n        if end_ix > len(sequence)-1:\n            break\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\nsize = int(len(df) - 50)\nn_steps=5\nn_features = 1","06332605":"df=pd.read_csv('..\/input\/salesweekly.csv')\n\nsubplotindex=0\nnumrows=4\nnumcols=2\nfig, ax = plt.subplots(numrows, numcols, figsize=(18,15))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\n\nwarnings.filterwarnings(\"ignore\")\n\nr=['M01AB','M01AE','N02BA','N02BE','N05B','N05C','R03','R06']\nfor x in r:\n    rowindex=math.floor(subplotindex\/numcols)\n    colindex=subplotindex-(rowindex*numcols)\n    X=df[x].values\n    scaler = MinMaxScaler(feature_range = (0, 1))\n    X=scaler.fit_transform(X.reshape(-1, 1))\n    X_train,y_train=split_sequence(X[0:size], n_steps)\n    X_test,y_test=split_sequence(X[size:len(df)], n_steps)\n    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n    model = Sequential()\n    model.add(LSTM(100, activation='relu', input_shape=(n_steps, n_features)))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse')\n    model.fit(X_train, y_train, epochs=400, verbose=0)\n    X_test = X_test.reshape((len(X_test), n_steps, n_features))\n    predictions = model.predict(X_test, verbose=0)\n    y_test=scaler.inverse_transform(y_test)\n    predictions = scaler.inverse_transform(predictions)\n    error = mean_squared_error(y_test, predictions)\n    perror = mean_absolute_percentage_error(y_test, predictions)\n    resultsLongtermdf.loc['Vanilla LSTM MSE',x]=error\n    resultsLongtermdf.loc['Vanilla LSTM MAPE',x]=perror\n    ax[rowindex,colindex].set_title(x+' (MSE=' + str(round(error,2))+', MAPE='+ str(round(perror,2)) +'%)')\n    ax[rowindex,colindex].legend(['Real', 'Predicted'], loc='upper left')\n    ax[rowindex,colindex].plot(y_test)\n    ax[rowindex,colindex].plot(predictions, color='red')\n    subplotindex=subplotindex+1\nplt.show()","3b75d106":"df=pd.read_csv('..\/input\/salesweekly.csv')\n\nsubplotindex=0\nnumrows=4\nnumcols=2\nfig, ax = plt.subplots(numrows, numcols, figsize=(18,15))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\n\nwarnings.filterwarnings(\"ignore\")\n\nr=['M01AB','M01AE','N02BA','N02BE','N05B','N05C','R03','R06']\nfor x in r:\n    rowindex=math.floor(subplotindex\/numcols)\n    colindex=subplotindex-(rowindex*numcols)\n    X=df[x].values\n    scaler = MinMaxScaler(feature_range = (0, 1))\n    X=scaler.fit_transform(X.reshape(-1, 1))\n    X_train,y_train=split_sequence(X[0:size], n_steps)\n    X_test,y_test=split_sequence(X[size:len(df)], n_steps)\n    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n    \n    model = Sequential()\n    model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n    model.add(LSTM(100, activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse')\n    model.fit(X_train, y_train, epochs=400, verbose=0)\n    \n    X_test = X_test.reshape((len(X_test), n_steps, n_features))\n    predictions = model.predict(X_test, verbose=0)\n    y_test=scaler.inverse_transform(y_test)\n    predictions = scaler.inverse_transform(predictions)\n    error = mean_squared_error(y_test, predictions)\n    perror = mean_absolute_percentage_error(y_test, predictions)\n    resultsLongtermdf.loc['Stacked LSTM MSE',x]=error\n    resultsLongtermdf.loc['Stacked LSTM MAPE',x]=perror\n    ax[rowindex,colindex].set_title(x+' (MSE=' + str(round(error,2))+', MAPE='+ str(round(perror,2)) +'%)')\n    ax[rowindex,colindex].legend(['Real', 'Predicted'], loc='upper left')\n    ax[rowindex,colindex].plot(y_test)\n    ax[rowindex,colindex].plot(predictions, color='red')\n    subplotindex=subplotindex+1\nplt.show()","10c2c52e":"df=pd.read_csv('..\/input\/salesweekly.csv')\n\nsubplotindex=0\nnumrows=4\nnumcols=2\nfig, ax = plt.subplots(numrows, numcols, figsize=(18,15))\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\n\nwarnings.filterwarnings(\"ignore\")\n\nr=['M01AB','M01AE','N02BA','N02BE','N05B','N05C','R03','R06']\nfor x in r:\n    rowindex=math.floor(subplotindex\/numcols)\n    colindex=subplotindex-(rowindex*numcols)\n    X=df[x].values\n    scaler = MinMaxScaler(feature_range = (0, 1))\n    X=scaler.fit_transform(X.reshape(-1, 1))\n    X_train,y_train=split_sequence(X[0:size], n_steps)\n    X_test,y_test=split_sequence(X[size:len(df)], n_steps)\n    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n\n    model = Sequential()\n    model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse')\n    model.fit(X_train, y_train, epochs=400, verbose=0)\n\n    X_test = X_test.reshape((len(X_test), n_steps, n_features))\n    predictions = model.predict(X_test, verbose=0)\n    y_test=scaler.inverse_transform(y_test)\n    predictions = scaler.inverse_transform(predictions)\n    error = mean_squared_error(y_test, predictions)\n    perror = mean_absolute_percentage_error(y_test, predictions)\n    resultsLongtermdf.loc['Bidirectional LSTM MSE',x]=error\n    resultsLongtermdf.loc['Bidirectional LSTM MAPE',x]=perror\n    ax[rowindex,colindex].set_title(x+' (MSE=' + str(round(error,2))+', MAPE='+ str(round(perror,2)) +'%)')\n    ax[rowindex,colindex].legend(['Real', 'Predicted'], loc='upper left')\n    ax[rowindex,colindex].plot(y_test)\n    ax[rowindex,colindex].plot(predictions, color='red')\n    subplotindex=subplotindex+1\nplt.show()","26762ad9":"from IPython.display import display, HTML\ndisplay(HTML(resultsRollingdf.to_html()))","5c77ced2":"from IPython.display import display, HTML\ndisplay(HTML(resultsLongtermdf.to_html()))","54e7676a":"### 4.2.2. Stationarity analysis","0f593d59":"Before actual forecasting, the optimal set of hyper-parameters is determined by using grid-search method. NOTE: this is illustrative example; actual optimization is carried out with larger intervals of hyper-parameters' values.","cfb00062":"Forecasting models were fitted with weekly time-series data with dataset of 302 rows. Three forecasting methods were tested: ARIMA, Prophet and LSTM. Train-test split method was used (52 weeks of test data). MSE was used as metrics for comparing the performance and also as a loss function for LSTM. Mean Absolute Percentage Error (MAPE) is also measured for each of the scenarios. In order to define the baseline forecasting accuracy to improve from, three tests were performed. Average method was used as a baseline for long-term forecasting, while Na\u00efve and Seasonal Na\u00efve were used for rolling forecasts.\n\nForecasting results are validated by using two approaches: short-term and long-term forecasts. For short-term forecasts (or so-called rolling-forecast) validation or walk-forward model validation, train-test split validation is performed in iterative fashion, where observations are added to the training dataset after every individual weekly sales prediction, while the model is fitted in each iteration. Therefore, during testing, prediction in a timestep t is based on the model which fits the training set consisting of observations in timesteps (0,t-1), or: f(t) = f(o[0:t-1]). Long-term forecasts validation is based on MSE of one-year forecast, compared to actual observations.\n\nAll MSE metrics will be stored in a dataframe, which will be later use for presentation of the overall results and comparison. Rolling forecasting will be carried out by using 5 methods (Naive, Seasonal Naive, ARIMA, AutoARIMA, Prophet). Longterm forecasting will be carried out by using 7 methods (Average, ARIMA, AutoARIMA, Prophet, Vanilla LSTM, Stacked LSTM, Bidirectional LSTM).","9e2668c0":"Time Series Analysis included seasonality, stationarity, autocorrelation, regularity and data distribution analysis.","08f462b7":"Function seasonal_decompose can be used for analysis of the portions of each component of time series. This is especially useful when determining uptake of residuals in data, based on the decomposed data. The volume of this uptake implies the predictability of the time series - higher the residuals, lower the predictability. To some extent, the proportion of the residuals when comparing with trend and seasonality can be also illustrated by the rolling means and standard deviation plots above.","2226e79a":"Stationarity of time-series is the property of exhibiting constant statistical properties over time (for example, mean, variance, autocorrelation). It can be visually determined by plotting rolling statistics (rolling means and variances). In stationary time series, the mean of the series, variance of the series and covariance of the i th term and the (i + m) th term should not be a function of time.\n\nWe can use Augmented Dickey-Fuller (ADF) test to check stationarity of the data. Possible values of regression parameters of ADF are:\n- c : constant only (default)\n- ct : constant and trend\n- ctt : constant, and linear and quadratic trend\n- nc : no constant, no trend","965a175e":"#### 4.3.4.3. Long-term forecasting with Bidirectional LSTM","5b46341c":"In general, the \"partial\" correlation between two variables is the amount of correlation between them which is not explained by their mutual correlations with a specified set of other variables. For example, if we are regressing a variable Y on other variables X1, X2, and X3, the partial correlation between Y and X3 is the amount of correlation between Y and X3 that is not explained by their common correlations with X1 and X2.","59fe0eac":"<blockquote>While Prophet model is used both for rolling forecasts and long-term forecasting, optimization of hyper-parameters is carried out only for long-term forecasts. Optimal sets of hyper-parameters are then used also for rolling forecasts.<\/blockquote>","7aef9a6b":"Long-term forecasting validation has been done with three LSTM configurations: Vanilla LSTM, Stacked LSTM and Bi-directional LSTM. Relu activation function was used, optimizer was Adam and loss function was Mean Squared Error. The best results were achieved with training the model in 400 epochs. Before fitting, all data was standardized (rescaled in interval -1,1) and transformed to data for supervised problem.\n\nNumber of past observations tested in input sequences was either 10 or 5. For series with larger variances and randomness (N05B and N05C) and simpler, Vanilla LSTM model, 10 past observations produced better forecasting accuracy. In all other cases, 5 past observations were used. This is the parameter that has been adopted.\n\nIn order to get reproducible results in forecasting with LSTM, following values are fixed: seed value, 'PYTHONHASHSEED' environment variable, Python's, numpy's and Tensorflow's built-in pseudo-random generators. A new global Tensorflow session is configured.","7b508b5f":"Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test found the trend non-stationarity in N02BE, R03 and R06.","a9a73582":"#### 4.3.2.5. Long-term forecasting with Auto-ARIMA model","9c71336c":"Components required for forecasting with LSTM are then imported. Function split_sequence for transforming time-series data to data for supervised machine learning problem is provided, including constants, such as number of previous steps to take into account.","31dcd915":"### 4.2.4. Autocorrelation analysis","069f1408":"# 5. Conclusion","9113eb69":"### 4.2.5. Data distribution analysis","5bde8948":"Initial dataset consisted of 600000 transactional data collected in 6 years (period 2014-2019), indicating date and time of sale, pharmaceutical drug brand name and sold quantity. As a result of the interviews with pharmacists, decision was made that the subject of analyses and forecasting will be actual drug categories, instead of the individual drugs. Thus, selected group of drugs (57 drugs) is classified to 8 Anatomical Therapeutic Chemical (ATC) Classification System categories: \n- M01AB - Anti-inflammatory and antirheumatic products, non-steroids, Acetic acid derivatives and related substances\n- M01AE - Anti-inflammatory and antirheumatic products, non-steroids, Propionic acid derivatives\n- N02BA - Other analgesics and antipyretics, Salicylic acid and derivatives\n- N02BE\/B - Other analgesics and antipyretics, Pyrazolones and Anilides\n- N05B - Psycholeptics drugs, Anxiolytic drugs\n- N05C - Psycholeptics drugs, Hypnotics and sedatives drugs\n- R03 - Drugs for obstructive airway diseases\n- R06 - Antihistamines for systemic use\n\nATC codes features are added to the dataset, namely a model has been transformed as indicated on the image below and data was resampled to the hourly time-series.","8cbc39b5":"#### 4.3.1.3. Seasonal Na\u00efve forecasting","17526611":"Augmented Dickey-Fuller (ADF) test have shown that all data, but N02BA (P-value=0.249) in the series were stationary, with maximum confidence.","52e2013c":"# 1. Introduction","0a8e112c":"Chart with weekly sales for different categories of interest was shown below. N02BE and N05B charts, though showing the similar trends, are suppresed because of the larger scale which makes the other illustrations less readable.","e9c5c944":"On a larger scale, the sales forecasting in pharmaceutical industry is typically done by using Na\u00efve model, where the forecasted values equal values in the previous period with added factor of growth, which is specifically defined for different regions, markets, categories of products, etc. Although this model fails when the market saturates, in general and on a larger scale, it has proven as successful. Still, analysis and forecasts on a smaller scale, such as single distributor, pharmacy chain or even individual pharmacy, smaller periods such as weeks, etc., guide very important decisions related to resource and procurement planning, what-if analyses, return-on-investment forecasting, business planning and others. The main problem in smaller scale time series analyses and forecasts are significant uncertainties and sales performance very close to random, making the forecasts with accuracies above thresholds as defined by Na\u00efve methods difficult to achieve.\n\nThe main research question we tackle is related to exploring the feasibility of use of modern time-series forecasting methods in pharmaceutical products sales forecasting on a smaller scale. In specific, we benchmark the accuracies achieved with those methods against the performances of basic Na\u00efve, Seasonal Na\u00efve and Average methods.\n\nResearch work behind the paper considers 8 time series with different statistical features. Each of the time-series summarizes sales of a group of pharmaceutical products. Time-series data are collected from the Point-of-Sale system of a single pharmacy in period of 6 years.\n\nThis paper is structured into 4 main parts. First, short theoretical background for time series analysis and forecasting is provided to inform the reader on the credibility of decisions made in the implementation of this case study. Then, research methodology, actually a problem-neutral time series forecasting pipeline is presented. Next, the actual implementation is presented, by highlighting the steps made in following the proposed methodology in the case of pharmaceutical products sales data analysis and forecasting. Finally, the discussion brings the description of actual results and some suggestions to the sales department, driven by the result of the data analysis.","d6a4df47":"## 4.3. Time series forecasting ","2a074cc4":"Time series is a sequence of observations recorded at regular time intervals (hourly, daily, weekly, monthly, quarterly and yearly). Its analysis involves understanding various aspects of the time series, important for creating meaningful and accurate forecasts.\n\nTypically, a time-series data embodies each of the four different components: \n1. level (mean of time-series data); \n2. trend (long-term increasing or decreasing value in series); \n3. seasonality (the repeating short-term cycle in the series); and \n4. noise or residuals (random variations in the series). \n\nOne time-series can be assumed to be additive or multiplicative (although, in a real world, series that fit one or another model rarely exist). For additive series, time-dependent variable value is equal to the addition of four components, namely, \n\ny(t) = Level + Trend + Seasonality + Noise\n\nIn multiplicative series, changes increase or decrease over time: \n\ny(t) = Level * Trend * Seasonality * Noise\n\nEach time-series can be decomposed to its four components, by using different methods, such as Na\u00efve, Loess or STL. Decomposition is especially useful for trend and uncertainty (variance of residuals typically corresponds to randomness) analysis.\n\nStationarity of time-series is the property of exhibiting constant statistical properties over time (for example, mean, variance, autocorrelation). It can be visually determined by plotting rolling statistics (rolling means and variances) or by using Augmented Dickey-Fuller (ADF) or Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test. ADF test assumes that the null hypothesis is the time series possesses unit root and is non-stationary. If the P-Value of ADF test is less than 0.05, the null hypothesis is rejected and series is considered as stationary. KPSS is used to test for trend stationarity. One series is considered trend stationary if after removing the underlying trend, series becomes stationary.\n\nIn case that time-series is non-stationary, it needs to be transformed. Following transforms for stationarizing data (for methods which do not work well with non-stationary data, for example, ARIMA) are available: 1) de-trending (removing the underlying trend in the series), 2) differencing (seasonal or cyclic patterns are removed by subtracting periodical values), 3) logging (linearizing trend with exponential function).\n\nBasic time-series forecasting assumes regressing the observation at the time t on the observations in the previous time steps: f<sub>t<\/sub>=e<sub>1<\/sub>o<sub>t-1<\/sub>+ e<sub>2<\/sub>o<sub>t-2<\/sub>+\u2026+e<sub>n<\/sub>o<sub>t-n<\/sub>. The potential strength of time-series prediction models is determined by autocorrelation \u2013 correlation between what is considered as output variable O<sub>t<\/sub> and input variables \u2013 prior observations. Autocorrelation plots graphically summarize the strength of a relationship of an observation in a time series with observations at prior time steps, with respect to the confidence interval (typically, 95% of confidence). Typically, Pearson coefficient of correlation is used, meaning that linear correlation is assessed. These plots are often called Autocorrelation Function (ACF) plots or correlograms. Another useful tool for determining autocorrelation are Partial Autocorrelation Function (PACF) plots. PACF plots highlight autocorrelation between observation o<sub>t<\/sub> and prior observations o<sub>i<\/sub>, without taking into account correlations in the time steps in the interval (t,i).\n\nApproximate or Sample Entropy methods can be used to quantify the regularity and predictability of fluctuations in a time series. The higher the entropies are, the more difficult is to forecast the time series. As difficulty increases, the time series converge to what is called \u2013 white noise, series of random numbers with mean equals 0.","83cbf02a":"### 4.3.3. Prophet forecasting ","5f0c2d71":"#### 4.3.3.3. Long-term forecasting with Prophet","4caf5725":"Image below shows trends for each of the drug categories, represented by the 365-d rolling means for each of those categories.","0335358c":"#### 4.3.4.2. Long-term forecasting with Stacked LSTM model","a4fd8858":"Prophet model is fited with data in two columns, where first one contains time information and is labeled as ds. Another stores actual time series data and is labeled as y.\n\nProphet model is tuned by using the following hyper-parameters:\n\n- growth. For a linear trending, value should be 'linear'. If not, 'logistic'. In latter case, the cap (maximum value data will reach) and floor (minimum value data will reach) of your predictions need to be provided. This is typically determined by domain experts.\n- interval_width. the uncertainty interval to produce a confidence interval around the forecast.\n- fourier_order. the number of Fourier components each seasonality is composed of.\n- n_changepoints. The changepoints parameter is used when the changepoint dates are supplied instead of having Prophet determine them. In practice Prophet should be let to do that alone.\n- changepoint_range usually does not have that much of an effect on the performance.\n- changepoint_prior_scale, is there to indicate how flexible the changepoints are allowed to be. In other words, how much can the changepoints fit to the data. If high it will be more flexible, but then overfitting is possible.\n- seasonality_prior_scale parameter. This parameter will again allow your seasonalities to be more flexible.","40fa20b6":"# 4. Solution\/discussion","c52fa0f2":"Another visualization that can be useful for discovering seasonality patterns is related to rolling window means. Rolling window operations are another important transformation for time series data. Similar to downsampling, rolling windows split the data into time windows and the data in each window is aggregated with a function such as mean(), median(), sum(), etc. However, unlike downsampling, where the time bins do not overlap and the output is at a lower frequency than the input, rolling windows overlap and \"roll\" along at the same frequency as the data, so the transformed time series is at the same frequency as the original time series. That means also that the curve is smoother. Time series data often exhibit some slow, gradual variability in addition to higher frequency variability such as seasonality and noise. An easy way to visualize these trends is with rolling means at larger time scales. Analysis below shows 30-day and 365-day rolling mean and 30-day rolling standard deviation of sales data.","61d11f8a":"#### 4.3.3.2. Rolling forecasts with Prophet","149f296d":"ARIMA method was used to to carry out short-term (rolling forecast) and long-term forecasting based on test data. Before each forecast is made, the process of optimizing hyper-parameters (p,d,q) of ARIMA model was carry out. Then, with optimal set of parameters, rolling forecast and long-term forecasting was carried out.\nSeasonal ARIMA (SARIMA) method was tested by using Auto-ARIMA implementation. This implementation has built-in stepwise optimizer which chooses optimal set of parameters (p,d,q for non-seasonal and p,d,q,P,D,Q for seasonal series, namely N02BE, R03 and R06).","93da2aba":"Chart with daily sales for different categories of interest is shown below. N02BE and N05B charts, though showing the similar trends, are suppresed because of the larger scale which makes the other illustrations less readable.","8d5093f1":"Minor autocorrelation is observed at ACF (Auto-Correlation Function) and PACF (Partial Auto-Correlation Function) plots for all series, with exception of N05C sales. N02BE, R03 and R06 series were found to exhibit annual seasonality. ","426879f9":"# 2. Theoretical background","cb7516e3":"<blockquote>The objective of the research behind the paper was to validate different methods and approaches related to sales time series data preparation, analysis and forecasting, with aim to facilitate recommending sales and marketing strategies based on trend\/seasonality effects and forecasting sales of eight different groups of pharmaceutical products with diverse characteristics, such as stationarity, seasonality, amount of residuals and sales data variance. All these analyses and forecasts are made on a small scale, for a single distributor, pharmacy chain or even individual pharmacy. Paper presents only research work related to univariate time series analysis, while potential candidates for explanatory input variables were also identified and shortly elaborated. Effectiveness of three forecasting methods, namely ARIMA, Facebook\u2019s Prophet and Long-Short Term Memory (LSTM) neural networks was investigated. Each of the method is complemented with two optimization and validation approaches, relevant for short-term (so called rolling forecast scenario) and long-term forecasting.<\/blockquote>","5e71f51f":"#### 4.3.3.1. Grid-search optimization of Prophet hyper-parameters","f1a65322":"However, AIC is not used to score accuracy of the forecasting methods in this research. Mean squared error is used instead. For that, reason, grid search optimization method was applied, where different combinations of the hyper-parameters were used to calculate MSE and then, the combination producing the least MSE was chosen as optimal. Grid search optimization is carried out for both approaches in forecasting: rolling and long-term.\n\nGrid search optimization for rolling forecast produced the following best combinations of the hyper-parameters:","10ed35ed":"### 4.3.2. ARIMA Forecasting ","562fa140":"ARIMA (Auto-Regressive Integrated Moving Average) models are most commonly used tools for forecasting univariate stationary time-series. Model uses the dependency relationship (correlation) between an observation and some number of lagged observations (AR) in the past. It is integrated (I), namely it uses differencing (see above) to make time-series stationary, within the method. Finally, it uses the dependency between an observation and a residual error from a moving average model applied to lagged observations (MA). Hyperparameters of one ARIMA model are:\n- p: lag order - number of observations in prior time steps included in the model. Typically, it is equal to the lag at which PACF cuts off the cone of the confidence interval\n- d: differencing degree - number of times that the raw observations are differenced. If the data series is stationary, d=0. If not, it is d>1.\n- q: moving average order - size of the moving average window.\n\nSome common rules for choosing initial AR and MA values (p,q), found in the literature are:\n\n- If the PACF of the differenced series displays a sharp cutoff and\/or the lag-1 autocorrelation is positive, then non-zero AR factor should be added to the model. The lag at which the PACF cuts off is the indicated value of p.\n- If the ACF of the differenced series displays a sharp cutoff and\/or the lag-1 autocorrelation is negative, then non-zero MA factor should be added to the model. The lag at which the ACF cuts off is the indicated value of q.\n\nSome common rules for choosing d are:\n\n- Rule 1 : If the series has positive autocorrelations out to a high number of lags, then it probably needs a higher order of differencing.\n- Rule 2 : If the lag-1 autocorrelation is zero or negative, or the autocorrelations are all small and random, then the series does not need a higher order of differencing. If the lag-1 autocorrelation is -0.5 or more negative, the series may be overdifferenced. (Robert Nau, Statistical Forecasting)\n\nMethod is improved with SARIMA (Seasonal ARIMA) \u2013 SARIMAX Python implementation was used. While SARIMA facilitates direct modeling of the seasonal component of time-series (by considering its own lag order, differencing degree, MA order and an actual lag), SARIMAX provides the extension for using exogenous variables and thus enable multivariate time-series forecasting.\nBesides p,d,q parameters of ARIMA, SARIMA also considers additional parameters: P,D,Q,m:\n\n- P: Seasonal autoregressive order.\n- D: Seasonal difference order.\n- Q: Seasonal moving average order.\n- m: The number of time steps for a single seasonal period. m of 12 for monthly data suggests a yearly seasonal cycle.\n\nSome rules for defining the initial set of parameters often used in a literature are as follows:\n- m is equal to the ACF lag with the highest value (typically at a high lag).\n- D=1 if the series has a stable seasonal pattern over time. D=0 if the series has an unstable seasonal pattern over time.\n- P\u22651 if the ACF is positive at lag S, else P=0.\n- Q\u22651 if the ACF is negative at lag S, else Q=0.\n- Rule of thumb: P+Q\u22642","6f3bb43b":"There are many different methods and approaches to time-series forecasting. The simplest methods which are typically used for determining baseline forecasting performance are Average, Na\u00efve and Seasonal Na\u00efve (those models are often used as benchmark models). In Average method, the forecasts of all future values are equal to mean of the historical data. If the dataset is split to train and test sets, then Average of training set is used as a forecast. For Na\u00efve forecasts, all forecasts are set to be values of the last observation, or: f<sub>t+1<\/sub>=o<sub>t<\/sub>. Na\u00efve forecasts are considered optimal when data follow a random walk and they can be used only in walk-forward or rolling forecasts, not in long-term forecasting. Seasonal Na\u00efve method is useful for time-series that show seasonality effects. Each forecast is set to be equal to the mean of the observed values from the same time in previous seasonal cycles.\n\nSeasonal decomposition can be also used for forecasting, by building the model (by using any other approach) on the time-series data with subtracted residual component, as calculated by any of the decomposition methods (such as STL). Another, quite successful classical forecasting method is Simple Exponential Smoothing (SES), where forecasts are calculated as: o<sub>t+1<\/sub>=\u03b1o<sub>t<\/sub>+\u03b1(1-\u03b1)o<sub>t-1<\/sub>+ \u03b1(1-\u03b1)<sup>2<\/sup>o<sub>t-2<\/sub>+..+ \u03b1(1-\u03b1)<sup>n<\/sup>o<sub>t-n<\/sub>, with: 0<\u03b1<1. Here, the forecast is equal to a weighted average of the past observations, where weights decrease exponentially as we go back in time. There are also improved SES methods which consider trends and seasonality.","b1ef1d6a":"In this subsection, three summaries of the reference benchmarks, namely Na\u00efve methods are provided. First, Na\u00efve forecasting was done and results presented. Second, average method was used to forecast. Finally, seasonal Na\u00efve forecast was carried out for the series that has been found as seasonal: N02BE, R03 and R06. See Theoretical background section for explanation of those methods.","db668216":"Feature engineering and data cleaning (including strategies for detecting outliers and their treatment) code is ommited in this notebook, because of the confidentiality issues. In the remainder of this notebook, final datasets will be used.","361482d6":"#### 4.3.1.1. Na\u00efve forecasting","4d98dc97":"### 4.2.1. Seasonality analysis","1c9369f5":"For rolling forecast, ARIMA method (Auto-ARIMA for series with seasonal character) outperforms Prophet and is considered as a best candidate for short-term sales forecasting. All methods in all cases (with exception of Prophet N02BE) outperform reference benchmarks - Naive and Seasonal Naive forecasts.","7dd6a0fd":"<h1>Case study: univariate time series analysis and forecasting of pharmaceutical products\u2019 sales data at small scale<\/h1>","882beaaa":"## 2.1. Time series forecasting","7691389c":"Trends and seasonality can be explored in time series decomposition view, based on 30d rolling means.","6f2a75e4":"#### 4.3.2.1. Choosing parameters for ARIMA model","6d1d7701":"Grid search optimization for long-term forecast produced the following best combinations of the hyper-parameters:","ce554cde":"### 4.2.3. Regularity analysis","f79fd740":"## 4.1. Feature engineering and data preparation ","1c145873":"The seasonality patterns can be explored in detail by using boxplots. Seasonality is clearly confirmed for the categories of R03, R06 and N02BE. Some additional conclusions: R03 and N05C has more outliers that the others, indicating that their sales is more difficult to predict.","8bbdd43f":"First, method arma_order_select_ic was used to determine initial p and q parameters. The method computes Akaike\u2019s Information Criterion (AIC) for many ARIMA models and chooses the best configuration. It is important to note that AIC tend not to be a good guide to selecting the appropriate order of differencing (d) of a model, but only for selecting the values of p and q.","2bcf0e1c":"To conclude, time-series analyses and forecasts have guided potentially useful conclusions and recommendations to the pharmacy. Daily, weekly and annual seasonality analysis were proven useful for identifying the periods in which special sales and marketing campaigns could be implemented, except for N05B and N05C categories of drugs which did not exhibit significant regularities. Forecasts have proven better than Na\u00efve methods and in acceptable intervals for long-term planning. It is highly likely that the forecasts could be significantly improved by expanding the problem scope to multivariate time series forecasting and by including explanatory variables, such as:\n- Weather data. Sales of antirheumatic drugs in M01AB and M01AE categories could be affected by the changes of atmospheric pressure. Sudden declines in all categories could be explained by extreme weather conditions, such as heavy rain, thunderstorms and blizzards.\n- Price of the drugs. Sales spikes may be explained by the discounts, applied in a short term. Introducing this feature may facilitate what-if forecasting analysis of sales performance during marketing campaigns involving price reductions.\n- Dates of the pension payoff. Sales spikes are visible at the dates of state pensions payoff.\n- National holidays, as non-working days with seasonal patterns similar to Sundays are expected to disrupt daily sales.\n\nFuture work on univariate time series forecasting includes increasing the number of data, exploring different other accuracy metrics, optimization of hyper-parameters for LSTM models and testing other architectures, such as CNN LSTM and ConvLSTM. However, key improvements in sales forecasting are expected from reducing the uncertainty of the models by expanding to multivariate time series forecasting problem, as explained above.\n","f69ef1be":"The methodology for implementing this case study follows the typical time series forecasting pipeline, consisting of three major phases: \n1. feature engineering and data preparation; \n2. exploratory data analysis (time-series analysis); and \n3. forecasting.\n\nBased on the problem and objective formal definition, the data acquired from the sales information system are cleaned, feature engineering approach was defined, and all data are transformed to hourly time series, consisting of aggregate sales among different classes of pharmaceutical products in hourly time periods, namely: anti-inflammatory and antirheumatic products (M01AB, M01AE), analgesics and antipyretics (N02BA, N02BE), psycholeptics drugs (N05B, N05C), drugs for obstructive airway diseases (R03) and antihistamines for systemic use (R06). This, intermediary series is used for the formal definition of anomalies and their identification. Also, outliers are detected in consultation with pharmacy staff and treated by first, imputing the missing data and then, by imputing representative data, by using several methods. Finally, data is then rescaled to weekly time-series and stored.\n\nTime series analysis had two-fold objective. First, annual, weekly and daily data analysis were done with objective to make potentially useful conclusions and propositions for improving sales and marketing strategies. Then, stationarity, autocorrelation and predictability analysis of the time series in individual groups was analyzed to infer the initial set of parameters for implementing the forecasting methods.\n\nForecasting was carried out at the weekly scale. Two different approaches to forecasting problem scoping were adopted. First one implements so called rolling forecast, namely forecasting the sales in the next week, by using the model trained with all historical data. Therefore, during testing, prediction in a timestep t is based on the model which fits the training set consisting of observations in timesteps (0,t-1), or: f(t) = f(o[0:t-1]). Rolling forecast model can be used for short-term resource planning and planning the procurement of stock of pharmaceutical products. Another approach is related to long-term forecasting, for example forecasting the future period of one year, by using the model trained with historical data. This model can be used for business planning and making decisions of strategic nature.\n\nTrain-test split validation with one last year of data (52 rows) was used for testing. Key performance indicator for forecasting accuracies in both approaches was Mean Squared Error (MSE). Mean Absolute Percentage Error (MAPE) was provided only as an illustration because data on different groups of pharmaceutical products were on significantly diverse scales. Baseline accuracy was calculated by using Na\u00efve and Seasonal Na\u00efve, for rolling forecasts and Average method for long-term ones. Three different models were tested: ARIMA\/SARIMA (for rolling and long-term forecast), Facebook\u2019s Prophet (for rolling and long-term forecast) and Long-Short Term Memory (LSTM) artificial neural network architectures (for long-term forecast).\n\nHyper-parameters were optimized by using three approaches: manually \u2013 with ACF\/PACF plot analysis, Python\u2019s statsmodels function and grid search optimization. Grid search was used as adopted approach for hyper-parameters optimization, for ARIMA and Prophet model. LSTM was applied only for long-term forecasting. The data preparation process for LSTM included transforming to stationary time series, sequencing time series to supervised problem data shape [X<sub>t-n_steps<\/sub>...X<sub>t-2<\/sub>,X<sub>t-1<\/sub>][y<sub>t<\/sub>] (after determining input vector dimension which gives best accuracies), and time series scaling (normalization or standardization). Three LSTM architectures were tested: Vanilla LSTM, Stacked LSTM and Bidirectional LSTM. No optimization of hyper-parameters was carried out. In order to get comparable results, pseudo-random generators of Python, Numpy and Tensorflow were set to fixed values.","2f102a6c":"### 2.1.2. Prophet method","6692eb80":"#### 4.3.2.4. Rolling forecasting with Auto-ARIMA model","1ebad7ce":"### 4.3.4. Forecasting with LSTM","c3fee406":"For long-term forecasting, Prophet and LSTM models have shown significantly better performance then benchmarks in the time series with seasonality. In all cases, even without optimization, LSTM models have achieved competitive performance.","d6592901":"# 3. Methodology","011a5101":"Below, boxplots on a weekly scale are shown, for the purpose of exploring the weakly seasonality. Some weekly seasonality is visible.","2bb2da27":"Long-Short Term Memory (LSTM) are a form of Recurrent Neural Networks (RNN) - deep learning architectures that are characterized by the use of LSTM units in hidden layers. Main feature of RNNs is that they allow information to persist, or they can inform the decision on some classification or regression task in the moment t, by using observations (or decisions) at moments t-1, t-2,.., t-n. In this research, three different LSTM architectures were used. Vanilla LSTM is made of a single hidden layer of LSTM units, and an output layer used to make a prediction. Stacked LSTM is architecture with two or more hidden layers of LSTM units stacked one on top of another. In bidirectional LSTM architecture model learns the input sequences both forward and backward.","d18664c1":"#### 4.3.2.3. Long-term forecasting with ARIMA model","57bcda63":"#### 4.3.2.2. Rolling forecasting with ARIMA model","17ee2ff7":"#### 4.3.1.2. Average method forecasting","6024d420":"### 2.1.1. ARIMA method","4408eb81":"<img class=\"img-fluid\" src=\"https:\/\/novafabrika.com\/notebooks\/pharma\/datamapping.jpg\">","47834e98":"From the optimization stand of point, some series are considered as so-called white noise - random time series data. Those are series for whom the best result was achieved with p=0, d=0 and q=0.","fdfcb3fb":"Solution will be using the set of Python libraries, namely numpy for computational tasks, pandas for data structures, matplotlib for plotting and others (libraries used for forecasting with LSTM are imported later). All warnings will be supressed for the cleaner presentation of this notebook (this is a bad practice, note that all presented code was tested). Also info messages are supressed, for the same reason. \n\nFinally, pyramid-arima package will be installed, in order to use Auto-ARIMA method.","961f2614":"Autocorrelation analysis illustrates the potential for time series data prediction. Autocorrelation plots graphically summarize the strength of a relationship with an observation in a time series with observations at prior time steps. Pearson coefficient is used to measure autocorrelation. Thus, the following analysis is relevant only for data with normal Gaussian distribution.\n\nA plot of the autocorrelation of a time series by lag is called the AutoCorrelation Function (ACF). This plot is sometimes called a correlogram or an autocorrelation plot. Plot shows the lag value along the x-axis and the correlation on the y-axis between -1 and 1. Confidence intervals are drawn as a cone. By default, this is set to a 95% confidence interval, suggesting that correlation values outside of this code are very likely a correlation.","9313ba58":"### 2.1.3. Neural networks ","e5991e33":"For calculating regularity and predictability of time series, Approximate Entropy test was used. For all series, entropy values were higher than 1 indicating low predictability, with highest values for M01AE, M01AB and N02BA.","9ca38c37":"Prophet is Facebook\u2019s additive regression model, that includes: linear or logistic trend, yearly seasonal component modeled using Fourier series and user-provided list of important holidays. The model facilitates easy customization and reliable forecasts with default configurations. According to the authors, Prophet is successful for forecasting data with strong \"human-scale\" seasonality (day of week, time of year), reasonable number of missing data and\/or outliers, historical trend changes, non-linear trends (with saturation), at least one year of observations, known holidays.\n\nProphet model is tuned by using the following hyper-parameters (only selected parameters are noted):\n\n- growth. For a linear trending, value should be 'linear'. If not, 'logistic'. In latter case, the cap (maximum value data will reach) and floor (minimum value data will reach) of predictions need to be provided. This is typically determined by domain experts.\n- interval_width. the uncertainty interval to produce a confidence interval around the forecast.\n- fourier_order. the number of Fourier components each seasonality is composed of.\n- n_changepoints. The changepoints parameter is used when the changepoint dates are supplied instead of having Prophet determine them. In practice Prophet should be let to do that alone.\n- changepoint_range usually does not have that much of an effect on the performance.\n- changepoint_prior_scale, is there to indicate how flexible the changepoints are allowed to be. In other words, how much can the changepoints fit to the data. If high it will be more flexible, but then overfitting is possible.\n- seasonality_prior_scale parameter. This parameter will again allow your seasonalities to be more flexible.","32284544":"## 4.2. Time series analysis ","d961a983":"#### 4.3.4.1. Long-term forecasting with Vanilla LSTM configuration","58c928bb":"### 4.3.1. Baseline forecasting accuracy"}}