{"cell_type":{"b9fb6b97":"code","c06c9e2a":"code","fed97282":"code","331d7329":"code","be32d3e4":"code","7bc62ddf":"code","e926e828":"code","79c793bb":"code","3a35eb0f":"code","443bc639":"code","e5cda0ef":"code","06b16f1a":"code","42c3119a":"code","5b00b434":"code","fe422af2":"code","db23fe21":"markdown","74faffa3":"markdown","037fcfd0":"markdown","05a6e040":"markdown","e0305137":"markdown","0802ad30":"markdown","6c341b78":"markdown","296c5cab":"markdown","44c147ee":"markdown","28135e5b":"markdown","88f22630":"markdown","7ad97e06":"markdown","d041b19f":"markdown","69999bd7":"markdown","c75a8c41":"markdown","a60476e9":"markdown"},"source":{"b9fb6b97":"import pandas as pd\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain.head()","c06c9e2a":"import catboost\nfrom catboost import Pool, cv\nimport numpy as np \nfrom pandas.api.types import is_categorical_dtype\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nfrom matplotlib import pyplot as plt\nfrom skopt.utils import use_named_args\nfrom skopt import gp_minimize\nimport shap\n\n\nclass CatboostModel:\n    def __init__(self, **catboost_args):\n        \"\"\"\n        Initializes the catboost model. Any arguments passed as keyword arguments will be passed down to the CatBoostClassifier.\"\"\"\n        self.model = self.set_model(**catboost_args)\n        self.non_numeric = None\n        self.drop_list = None\n        self.pool = None\n        self.columns = None\n    \n    @staticmethod\n    def set_model(**catboost_args):\n        if \"loss_function\" not in catboost_args:\n            catboost_args[\"loss_function\"] = \"Logloss\"\n        if \"eval_metric\" not in catboost_args:\n            catboost_args[\"eval_metric\"] = \"Precision\"\n        return catboost.CatBoostClassifier(**catboost_args)\n    \n    def fit(self, \n            x: pd.DataFrame, \n            label: str = \"Survived\",\n            cat_features: list = None,\n            text_features: list = None,\n            drop_list: list = None,\n            evaluate: bool = True,\n            **cv_params\n           ) -> None:\n        \"\"\"\n        Fits the model to the input data x and keeps that model trained. It will also perform\n        crossvalidation on the training data to have an evaluation of accuracy on training.\n        \n        Args:\n            x: training data\n            label: string, column to use as label\n            cat_features: str or list, features that are to be treated as categorical\n            text_features: str or list, features that are to be treated as text\n            drop_list: list of variables to drop. Note that PassengerId will not be used \n                as a feature and set as index for the pandas DataFrame.\n        \"\"\"\n        # Set the experiment: x_train, y_train, categorical features, text features, drop columns\n        self.set_experiment(\n            x=x,\n            label=label,\n            cat_features=cat_features,\n            text_features=text_features,\n            drop_list=drop_list)\n\n        self.model.fit(\n            self.x_train, \n            y=self.y_train,\n            cat_features=self.cat_features,\n            text_features=self.text_features,\n            logging_level=\"Silent\")\n        \n        self.latest_scores = self.evaluate(**cv_params)\n        print(f\"cross validation scores with catboost: \\n{self.latest_scores.loc[len(self.latest_scores) - 1, :]}\")\n        print(f\"parameters: {self.model.get_params()}\")\n        \n    def set_experiment(\n            self, \n            x: pd.DataFrame, \n            label: str = \"Survived\",\n            cat_features: list = None,\n            text_features: list = None,\n            drop_list: list = None\n           ) -> None:\n        \"\"\"\n        Sets x_train, y_train, text_features, cat_features\n        \n        Args:\n            x: training data\n            label: string, column to use as label\n            cat_features: str or list, features that are to be treated as categorical\n            text_features: str or list, features that are to be treated as text\n            drop_list: list of variables to drop. Note that PassengerId will not be used \n                as a feature and set as index for the pandas DataFrame.\n        \"\"\"\n        # Set index\n        x = x.set_index(\"PassengerId\")\n        # Drop non useful features\n        self.drop_list = self.set_drop_list(drop_list)\n        x = x.drop(self.drop_list, axis=1, errors=\"ignore\")  # Ignore if missing column to drop\n        # Set text and categorical feature\n        self.text_features = self.set_text_features(x, text_features)\n        self.cat_features = self.set_categorical_features(x, cat_features, *self.text_features)\n        self.non_numeric = self.text_features + self.cat_features\n        # Fill missing values\n        x = self.set_missing_values(x, self.non_numeric)\n        # Split training and label\n        self.x_train, self.y_train = self.set_train_and_label(x, label)\n        # Record columns for importance\n        self.columns = self.x_train.columns\n        # Set a pool for evaluation\n        self.pool = Pool(\n                 self.x_train, \n                 label=self.y_train,\n                 cat_features=self.cat_features,\n                 text_features=self.text_features)\n\n    def evaluate(self, **params):   \n        scores = catboost.cv(\n                      self.pool,\n                      self.model.get_params(), \n                      logging_level=\"Silent\",\n                      plot=False,\n                      **params)\n        return scores   \n    \n    def tune_model(self,\n                   x: pd.DataFrame, \n                   space: list,\n                   label: str = \"Survived\",\n                   cat_features: list = None,\n                   text_features: list = None,\n                   drop_list: list = None,\n                   early_stopping_rounds=20,\n                   n_calls=10, \n                   n_random_starts=2,\n                   n_jobs=-1,\n                   **eval_params\n                  ):\n        \n        self.set_experiment(\n            x=x,\n            label=label,\n            cat_features=cat_features,\n            text_features=text_features,\n            drop_list=drop_list)\n        \n        @use_named_args(param_space)\n        def objective(**params):\n            self.model.set_params(**params)\n            scores = self.evaluate(\n                **eval_params)\n            test_scores = scores.iloc[:, 2]\n            best_metric = test_scores.max()\n            return 1 - best_metric\n        \n        res_gp = gp_minimize(objective, space, n_calls=n_calls, n_random_starts=n_random_starts, n_jobs=n_jobs)\n        best_values = res_gp.x\n        optimal_values = dict(zip([param.name for param in space], best_values))\n        best_score = res_gp.fun\n        self.best_score = best_score\n        self.res_gp = res_gp\n        \n        print('optimal_parameters: {}\\noptimal score: {}'.format(optimal_values, best_score))\n        print('updating model with optimal values')\n        params = self.model.get_params()\n        params.update(optimal_values)\n        self.model = self.set_model(**params)\n        return optimal_values\n        \n    def predict(self,\n               x: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generates predictions using the fitted model. Returns a pandas DataFrame formatted as Kaggle\n        needs it for a submission: 2 columns, \"PassengerId\" and \"Survived\", the latter being the prediction.\n        \"\"\"\n        # Drop non useful features and set PassengerId as index\n        x = x.set_index(\"PassengerId\")\n        # Drop non useful features\n        x = x.drop(self.drop_list, axis=1, errors=\"ignore\")  # Ignore if missing column to drop\n        # Fill missing values\n        x = self.set_missing_values(x, self.non_numeric)\n        # Get predictions\n        x[\"Survived\"]  = model.model.predict(x)\n        # Return formatted predictions\n        x = x.reset_index()\n        return x.loc[:, [\"PassengerId\", \"Survived\"]]\n        \n    \n    @staticmethod\n    def set_drop_list(drop_list: list):\n        \"\"\"Sets the list of variables to drop.\"\"\"\n        if drop_list is None:\n            return []\n        elif isinstance(drop_list, str):\n            return [drop_list]\n        elif isinstance(drop_list, list):\n            return drop_list\n        else:\n            raise Exception(f\"drop_list must be a list. Can also be a str or None, \"\n                            \"which will be formatted to a list. Provided: {type(drop_list)}\")\n        \n    \n    @staticmethod\n    def set_missing_values(x: pd.DataFrame, non_numeric: list) -> pd.DataFrame:\n        \"\"\"Replaces missing values for non numeric variables to NaN\"\"\"\n        x.loc[:, non_numeric] = x.loc[:, non_numeric].fillna(\"NaN\")\n        return x\n    \n    @staticmethod\n    def set_train_and_label(x: pd.DataFrame, label: str) -> tuple:\n        \"\"\"Returns a tuple:\n        (training data, corresponding labels)\n        \"\"\"\n        return x.drop(label, axis=1), x.loc[:, label]\n        \n    @staticmethod\n    def set_categorical_features(x: pd.DataFrame, cat_features:list, *ignore) -> list:\n        \"\"\"Returns a list of categorical columns, automatically detected with pandas.\"\"\"\n        cols = CatboostModel._set_columns(x.columns, ignore)\n        print(cols)\n        if isinstance(cat_features, str):\n            return [cat_features]\n        elif isinstance(cat_features, list):\n            return cat_features\n        else:\n            return [i for i in cols if (is_categorical_dtype(x.loc[:, i]) or is_string_dtype(x.loc[:, i]))]\n\n    @staticmethod\n    def set_text_features(x:pd.DataFrame, text_features: list):\n        \"\"\"Sets text features.\"\"\"\n        if isinstance(text_features, list):\n            return text_features\n        elif isinstance(text_features, str):\n            return [text_features]\n        else:\n            return []\n        \n    @staticmethod\n    def _set_columns(columns: list, ignore: list):\n        \"\"\"Sets columns to use for the various set_ methods, e,g, in set_categorical_features\n        we don't want to include text features as they would be picked up as categories.\"\"\"\n        return [i for i in columns if i not in ignore]\n    \n    def get_feature_importance(self) -> list:\n        \"\"\"Returns a list of feature importanes.\"\"\"\n        return self.model.get_feature_importance(self.pool)\n    \n    def inspect_model(self) -> None:\n        #\u00a0Plot feature importance (always available)\n        self.plot_importance()\n        # feature interactions and shap values are not available when using text features\n        if not self.text_features:\n            self.plot_feature_interactions()\n            self.plot_shap_values()    \n    \n    def plot_importance(self) -> None:\n        \"\"\"\n        Catboost will only give the id of the feature importance. Hence we map that id back to the column name \n        and print the importance value.\n        \"\"\"\n        # Get feature importance\n        importances = self.get_feature_importance()\n        columns = self.columns\n        # Build series, sort values and plot\n        pd.Series(importances, index=columns).sort_values().plot.barh()\n        #\u00a0Set additional plotting options\n        plt.xlabel('Importance Percentage')\n        plt.title('How much do each feature contribute to the prediction?')\n        # Display the actual value (rounded)\n        self.set_plot_text(sorted(importances))\n        # Show plot\n        plt.show()\n        \n    def plot_feature_interactions(self) -> None:\n        interactions = self.model.get_feature_importance(model.pool, fstr_type=catboost.EFstrType.Interaction, prettified=True)\n        feature_interaction = pd.DataFrame()\n        feature_interaction[\"Features\"] = interactions.loc[:, \"First Feature Index\"].apply(lambda x: model.columns[x]).astype(str) + \\\n                                          \" \\ \" + \\\n                                          interactions.loc[:, \"Second Feature Index\"].apply(lambda x: model.columns[x]).astype(str)\n        feature_interaction[\"Interaction value\"] = interactions.loc[:, \"Interaction\"].round(1)\n        feature_interaction.set_index(\"Features\", inplace=True)\n        feature_interaction.plot(kind='barh', figsize=(18, 10), fontsize=12, color='b')\n        #\u00a0Set additional plotting options\n        plt.xlabel('Importance Percentage')\n        plt.title('How much do each feature contribute to the prediction?')\n        # Display the actual value (rounded)\n        self.set_plot_text(feature_interaction.loc[:, \"Interaction value\"].to_list())\n        plt.show()\n        \n    def plot_shap_values(self) -> None:\n        shap_values = self.model.get_feature_importance(model.pool, fstr_type=catboost.EFstrType.ShapValues)\n        shap.initjs()\n        shap.summary_plot(shap_values[:, :-1], self.x_train, feature_names=self.columns.tolist())\n    \n    @staticmethod\n    def set_plot_text(value_list: list) -> None:\n        for index, value in enumerate(value_list):\n            plt.text(value, index, str(round(value, 1)))\n        ","fed97282":"%%time\nmodel = CatboostModel(task_type=\"GPU\")\nmodel.fit(train, text_features=\"Name\")","331d7329":"pred = model.predict(test)\npred.to_csv('submission_with_text.csv', index=False)","be32d3e4":"%%time\nmodel = CatboostModel(task_type=\"CPU\")  \nmodel.fit(train)\npred = model.predict(test)\npred.to_csv('submission_without_text.csv', index=False)","7bc62ddf":"%%time\nmodel = CatboostModel() \nmodel.fit(train.fillna(-1000))\npred = model.predict(test.fillna(-1000))\npred.to_csv('submission_without_text_fillna.csv', index=False)","e926e828":"%%time\nmodel = CatboostModel() \nmodel.fit(train.fillna(-1000).drop([\"Cabin\", \"Embarked\", \"Name\", \"Ticket\"], axis=1))\npred = model.predict(test.fillna(-1000).drop([\"Cabin\", \"Embarked\", \"Name\", \"Ticket\"], axis=1))\npred.to_csv('submission_without_text_fillna_cols_dropped.csv', index=False)","79c793bb":"# Define preprocessing functions\nimport re\n\n\ndef get_title(name: str) -> str:\n    return re.search(\"([A-Za-z]+)\\.\", name).group(1)\n    \n    \ndef test_get_title():\n    assert get_title(\"Braund, Mr. Owen Harris\") == \"Mr\"\n    assert get_title(\"Heikkinen, Miss. Laina\") == \"Miss\"\n\n\ndef get_family_size(df: pd.DataFrame) -> pd.Series:\n    return df.loc[:, \"SibSp\"] + df.loc[:, \"Parch\"]\n\n\ndef get_last_name(name: str) -> str:\n    return re.search(\"^(.+?),\", name).group(1).split(\",\")[0]\n\n\ndef test_get_last_name():\n    assert get_last_name(\"Braund, Mr. Owen Harris\") == \"Braund\"\n    assert get_last_name(\"Heikkinen, Miss. Laina\") == \"Heikkinen\"\n    \n\ndef get_family_id(df: pd.DataFrame) -> pd.Series:\n    df = df.copy()\n    df[\"last_name\"] = df.loc[:, \"Name\"].apply(get_last_name)\n    df[\"family_id\"] = df.loc[:, \"last_name\"].astype(str) + df.loc[:, \"Embarked\"].astype(str)\n    return df.loc[:, \"family_id\"]\n\n\ndef get_children_age_count(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"family_id\"] = get_family_id(df)\n    df_origin = df.copy()\n    df[\"youngest_age_inf_1\"] = df.loc[:, \"Age\"].between(0, 1)\n    df[\"youngest_age_1_15\"] = df.loc[:, \"Age\"].between(1, 15)\n    df[\"youngest_age_15_21\"] = df.loc[:, \"Age\"].between(15, 21)\n    tmp1 = df.loc[:, [\"family_id\", \"youngest_age_inf_1\"]].groupby(\"family_id\").sum()\n    tmp2 = df.loc[:, [\"family_id\", \"youngest_age_1_15\"]].groupby(\"family_id\").sum()\n    tmp3 = df.loc[:, [\"family_id\", \"youngest_age_15_21\"]].groupby(\"family_id\").sum()\n    df_final = df_origin.join(tmp1, on=\"family_id\").join(tmp2, on=\"family_id\").join(tmp3, on=\"family_id\")\n    return df_final.drop(\"family_id\", axis=1)\n\n\ntest_get_title()\ntest_get_last_name()","3a35eb0f":"def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"title\"] = df.loc[:, \"Name\"].apply(get_title)\n    df = get_children_age_count(df)\n    df[\"family_size\"] = get_family_size(df)\n    return df\n\npreprocess(train).head()","443bc639":"%%time\nmodel = CatboostModel() \nmodel.fit(preprocess(train).fillna(-1000).drop([\"Cabin\", \"Embarked\", \"Name\", \"Ticket\"], axis=1))\npred = model.predict(preprocess(test).fillna(-1000).drop([\"Cabin\", \"Embarked\", \"Name\", \"Ticket\"], axis=1))","e5cda0ef":"model.inspect_model()","06b16f1a":"%%time\nmodel = CatboostModel() \nmodel.fit(preprocess(train).fillna(-1000).drop([\"Cabin\", \"Embarked\", \"Name\", \"Ticket\", \"SibSp\", \"Parch\"], axis=1))\npred = model.predict(preprocess(test).fillna(-1000).drop([\"Cabin\", \"Embarked\", \"Name\", \"Ticket\", \"SibSp\", \"Parch\"], axis=1))\npred.to_csv('submission_with_features.csv', index=False)\nmodel.inspect_model()","42c3119a":"%%time\nfrom skopt.space import Real, Integer\n\nparam_space = [Real(1e-4, 0.2, prior='log-uniform', name='learning_rate'), \n                Integer(2, 10, name='max_depth'),\n                #Real(0.5, 1.0, name='subsample'),\n                #Real(0.5, 1.0, name='colsample_bylevel'), \n                #Real(1.0, 16.0, name='scale_pos_weight'), \n                #Real(0.0, 1.0, name='bagging_temperature'), \n                #Integer(1, 20, name='random_strength'), \n                Real(1.0, 100, name='reg_lambda')]\n\nmodel = CatboostModel(early_stopping_rounds=100,\n                     task_type=\"GPU\")  \nmodel.tune_model(preprocess(train).fillna(-1000).drop([\"Cabin\", \"Embarked\", \"Name\", \"Ticket\", \"SibSp\", \"Parch\"], axis=1), \n                 space=param_space,                    \n                 stratified=True, \n                 shuffle=True,\n                 nfold=3\n                 )\n\nmodel.fit(preprocess(train).fillna(-1000).drop([\"Cabin\", \"Embarked\", \"Name\", \"Ticket\", \"SibSp\", \"Parch\"], axis=1),\n          stratified=True, \n          shuffle=True,\n          nfold=3\n          )\npred = model.predict(preprocess(test).fillna(-1000).drop([\"Cabin\", \"Embarked\", \"Name\", \"Ticket\", \"SibSp\", \"Parch\"], axis=1))\npred.to_csv('submission_with_features_tuned.csv', index=False)\nmodel.inspect_model()","5b00b434":"%%time\nfrom skopt.space import Real, Integer\n\nparam_space = [Real(1e-5, 0.2, prior='log-uniform', name='learning_rate'), \n                Integer(2, 10, name='max_depth'),\n                #Real(0.5, 1.0, name='subsample'),\n                #Real(0.5, 1.0, name='colsample_bylevel'), \n                #Real(1.0, 16.0, name='scale_pos_weight'), \n                #Real(0.0, 1.0, name='bagging_temperature'), \n                #Integer(1, 20, name='random_strength'), \n                Real(1.0, 100, name='reg_lambda')]\n\nmodel = CatboostModel(task_type=\"GPU\",\n                      early_stopping_rounds=100)\n\nmodel.tune_model(train, \n                 space=param_space,   \n                 text_features=\"Name\",\n                 stratified=True, \n                 shuffle=True,\n                 nfold=3,\n                 n_jobs=1)\n\nmodel.fit(train)\npred = model.predict(test)\npred.to_csv('submission_tuned.csv', index=False)\nmodel.inspect_model()","fe422af2":"%%time\nfrom skopt.space import Real, Integer\n\nparam_space = [Real(1e-4, 0.2, prior='log-uniform', name='learning_rate'), \n                Integer(2, 10, name='max_depth'),\n                Real(1.0, 100, name='reg_lambda')]\n\nmodel = CatboostModel(task_type=\"GPU\")  \nmodel.tune_model(preprocess(train).fillna(-1000).drop([\"Cabin\", \"Embarked\", \"Name\", \"Ticket\", \"SibSp\", \"Parch\"], axis=1), \n                 space=param_space,                    \n                 stratified=True, \n                 shuffle=True,\n                 nfold=3\n                 )\n\nmodel.fit(preprocess(train).fillna(-1000).drop([\"Cabin\", \"Embarked\", \"Name\", \"Ticket\", \"SibSp\", \"Parch\"], axis=1),\n          stratified=True, \n          shuffle=True,\n          nfold=3\n          )\npred = model.predict(preprocess(test).fillna(-1000).drop([\"Cabin\", \"Embarked\", \"Name\", \"Ticket\", \"SibSp\", \"Parch\"], axis=1))\npred.to_csv('submission_with_features_tuned_no_early_stop.csv', index=False)\nmodel.plot_importance()\n\n\nmodel = CatboostModel(task_type=\"GPU\")\n\nmodel.tune_model(train, \n                 space=param_space,   \n                 text_features=\"Name\",\n                 stratified=True, \n                 shuffle=True,\n                 nfold=3,\n                 n_jobs=1)\n\nmodel.fit(train)\npred = model.predict(test)\npred.to_csv('submission_tuned_no_early_stop.csv', index=False)\nmodel.inspect_model()","db23fe21":"### Define the model class\nI will here define a class that can take the training \/ prediction data as an input and format it for catboost.\nWe will need to:\n- Set text and categorical variables\n- Drop variables that need to be dropped\n- Replace the missing values for categorical and text variables\n- Follow the sklearn api with fit and predict methods\n","74faffa3":"### Adding some features\nLets build a few features by:\n- Parsing the name\n- Calulating the family size\n- See if the family is traveling with a [0 - 1] year old, [1 - 15] or [15 - 21] (this split is based on age vs survived where I could see 4 groups)\n- We could bin the fare (we'll test with and without) as my past experience with this dataset is that this variable lead to overfitting","037fcfd0":"#### Without early stopping\n","05a6e040":"We can't see any improvements in that report. This is a relatively good sign, if it doesn't change much then we can either leave the default value, or fillna with something completely impossible for every column and have a good prototype.\n\n**Drop some columns**\n\nSome columns as is may be only adding noise. It would be the case for Cabin (too many nulls), Embarked (why would the port matter), Name (useless unprocessed), Ticket (these are just ticket ids). We may this way reduce overfitting.","e0305137":"**Let's put all of that together in a preprocess function**","0802ad30":"## Conclusion\nIt looks like catboost can help fast prototyping thanks to the reduced work in encoding variables. It also is a good thing that new unseen categorical variables do not make the model fail, which means in production if something changes in the input data the modeling pipeline will not fail, which means a fix or model retraining can be done without a sense of absolute urgency.\n\nCatboost also provides useful ways of exploring the features and the model, thanks to feature importance, feature interactions and the integration with SHAP.\n\nFinally, Catboost is quite sensitive to its parameters. The best results were over 0.79 on the submission, but they varied quite a bit. It seems the algorithm needs to be tuned with caution, and I need more experience tuning it. The default parameters provide already a good baseline.\n\n## Sources\nThe following have been very useful for me to learn how to perform tasks that were new for me:\nCatboost documentation: https:\/\/catboost.ai\/docs\/\nsklearn hyper parameter optimization: https:\/\/scikit-optimize.github.io\/stable\/auto_examples\/hyperparameter-optimization.html\ntowards data science (excellent!) article on catboost (recommend the read): https:\/\/towardsdatascience.com\/https-medium-com-talperetz24-mastering-the-new-generation-of-gradient-boosting-db04062a7ea2\n\nThanks to Kaggle for allowing access to computing resources and a GPU.","6c341b78":"### Fit and evaluate\nText features only work with GPU, so we'll set the task_type to GPU and activate GPU acceleration on the kaggle notebook. Other than the text feature, the categorical and numeric variables are auto detected using pandas api.types.","296c5cab":"The accuracy seems to have increase on the crossvalidation (0.817). Results have also improved in the submission by 1.5% (0.77033). \nTo understand what is happening, we should inspect the model. Catboost comes with a set of ways to inspect a model:\n- Feature importance\n- Feature interaction\n- Shap values (shap will be needed to print the results)\n\nI've enclosed them all in the inspect_model method. Note that the feature interactions and the SHAP values are not available when a list of text features is provided.","44c147ee":"# Testing Catboost on the titanic dataset\nI've heard that catboost was doing quite well on various benchmarks and that it came with built-in mechanisms to process categorical and text variables. I wanted to put it to the test.\n\nMany of the other algorithms I've used (e.g. logistic regression, but even xgboost, my current go-to) require quite some preprocessing and a fair bit of data exploration. What I find very interesting here is that we could speed up the work greatly and get at least comparable results, if not same \/ better according to certain benchmarks. There can be a lot of value there, as time saved can be used to do other projects, or have a fast prototype \/ first version and trigger continuous improvement. Having a first working version has proved to be key in my experience, as it gives all stakeholders of a project an idea of the kind of output they will get, leading to a faster overall project integration as services interacting with the model can be built with real data, and the project can fail fast if it seems overall it won't work. \n\n## Methodology\nI will:\n- Start by creating a class that can support the testing \n- Test catboost without any preprocessing on the variables (e.g. no missing value imputing on any variable, no feature building by extracting the title fron the Name...)\n- Build a few features and see it if \/ how it changes the performance\n\nMy past experience with this dataset is that building features doesn't help that much. My best result is actually using xgboost with various default tools I've built at work without any tuning to the data, almost like running an auto-ml software. Hence I am curious to see how catboost performs without any feature engineering, with a little bit of it, and with heavier engineering.\n\n### Data import\nNote: I'm running this on Kaggle.","28135e5b":"#### Trial two: providing the text feature, no preprocessing and basic model tuning","88f22630":"**Build the model and export results**","7ad97e06":"test precision is at 78.5%. I've ran this several times and the result is quite stable, although I got up to 81%. This is in line with the other algorithms I tried.","d041b19f":"**NaN processing**\n\nCatboost automatically transforms the NaNs to the minimum in the distribution. For a variable like Age, this is probably increasing significantly the chances of positive classification, as younger people had more chances of survival. We do not want this. As Catboost is a tree based model, we can give an impossible value for it to know it is something different. \n\nHere, Age is the only numeric variable that has missing values, and we are replacing them by \"NaN\" in the other variables. However, -1000 would be an impossible and out of distribution for all these variables, so we can just replace all missing values by -1000.","69999bd7":"We do see an increase in test accuracy in the cross validation. Results also improve in the submission to 0.77990, so this quick trick did help us do a sanity check and identify variables that should be dropped.\n\nTaking away title isn't as helpful: there may be some overfitting, but there may also be some useful information. E.g. Miss would indicate a single woman, which could have less chances of embarking a life boat as a woman with a young child. It also could contain information on social ranking. This variable requires extra processing, and for that we would need to know exactly how it is used to generate the predictions. This isn't however the purpose of this experiment, I will leave it for a later one.\n### Playing with catboost's parameters\n\nOne last thing I would like to see is the influence of tuning the model. I would like to compare the first run with no feature building with added parameter tuning, with the latest run with features built and non useful features dropped.\n\n#### Trial one: using the features and dropping non useful variables\nWe will set early_stopping_rounds to 20 to avoid overtitting and get a faster compute.","c75a8c41":"This yielded a score of 0.75598. Not bad for the amount of work and given that most solutions score 0.775. In real life, we may not even consider this gap as statistically significant (as there are very few samples in the test data used for scoring). \nLets tweak this a little.\n\n**Without text features**","a60476e9":"Variable importance seems sensible for every variable. Gender is the most important, followed by two socio-economic features (Pclass, Fare). The family's description features (family size, count of children of age 0-1, 1-15, 15-1) is quite important. Age of the person is also, as expected, important.\n\nWe may want to perform additional checks to see if a variable isn't adding noise. For example, title seems to be important, but most of the information it should use from it should already be in \"Sex\". Are we seing overfitting? The same comment could be made for SibSb and Parch.\n\nHere's without SibSp and Parch:"}}