{"cell_type":{"b3e46b08":"code","916e84e8":"code","01eb527c":"code","76b5554a":"code","9f84a3dc":"code","473af409":"code","6a1f7a70":"code","b2f5bfed":"code","c234873a":"code","2b5149aa":"code","a03011a8":"code","7cf3b45c":"code","ddd91820":"code","e122b5f8":"code","1209d25a":"code","7996ae6a":"code","2232c4e1":"code","5a827287":"code","12d27d0d":"code","384475d1":"code","cc5c3f5e":"code","d06ef511":"code","0d87c558":"code","19fdcd53":"code","32e362f0":"code","c59f70e9":"code","29551dec":"code","f7a257cd":"code","c8efae6c":"code","d02111a2":"code","8d85b666":"code","2db2c184":"code","461a04ec":"code","9affca3b":"code","99a2eea1":"code","042a9a44":"code","66f8d70c":"code","938c0933":"code","308cbd82":"code","a4bb4421":"code","67c2e9ce":"code","433ae689":"code","98f2f33e":"code","ecd78ab4":"code","2c2f4d51":"code","ebed1b30":"code","e20657d4":"code","68bc55eb":"code","6c642b63":"code","1c032255":"code","71469662":"code","a725f55e":"markdown","04efafc8":"markdown","012547eb":"markdown","5acd5778":"markdown","1a60993d":"markdown","0893704d":"markdown","445bf467":"markdown","8d0adbdd":"markdown","ffce314e":"markdown","264a40a9":"markdown","87cd3519":"markdown","178327e9":"markdown","327d5304":"markdown","d0d603b8":"markdown","8158cb70":"markdown","cb711d2d":"markdown","a56f3f2a":"markdown","c0cc694e":"markdown","2a1cc2ce":"markdown","9b6da73e":"markdown","5c9edd17":"markdown","9f66f831":"markdown","8fae16d8":"markdown","02506cc8":"markdown","9e7bc6ec":"markdown","ecfe13db":"markdown","b267f219":"markdown","c331fe90":"markdown","7b1f5da6":"markdown","06c35a1e":"markdown"},"source":{"b3e46b08":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\n%matplotlib inline\n\nprint(\"Setup Complete\")","916e84e8":"# Load the data after EDA\ntrain_df = pd.read_csv(\"..\/input\/pumpitup-challenge-dataset\/train_df_after_EDA.csv\")\nX_test = pd.read_csv(\"..\/input\/pumpitup-challenge-dataset\/X_test_after_EDA.csv\")","01eb527c":"# drop columns\ntrain_df = train_df.drop(['installer','management_group','status_group','id_x','id_y', 'num_private', 'wpt_name', \n          'recorded_by', 'subvillage', 'scheme_name', 'region', \n          'quantity', 'water_quality', 'lga','ward', 'source_type', 'payment', \n          'waterpoint_type_group','extraction_type_group','extraction_type_class'],axis=1)\nX_test = X_test.drop(['installer','management_group','id', 'num_private', 'wpt_name', \n          'recorded_by', 'subvillage', 'scheme_name', 'region', \n          'quantity', 'water_quality', 'lga','ward', 'source_type', 'payment', \n          'waterpoint_type_group','extraction_type_group','extraction_type_class'],axis=1)","76b5554a":"train_df.head()","9f84a3dc":"## Null scheme_management, funder, installer, public_meeting and permit values replace with \"unknown\" text\ntrain_df[\"scheme_management\"].fillna(\"unknown\", inplace = True)\ntrain_df[\"public_meeting\"].fillna(\"unknown\", inplace = True)\ntrain_df[\"permit\"].fillna(\"unknown\", inplace = True)\ntrain_df[\"funder\"].fillna(\"unknown\", inplace = True)\n# train_df[\"installer\"].fillna(\"unknown\", inplace = True)\n\nX_test[\"scheme_management\"].fillna(\"unknown\", inplace = True)\nX_test[\"public_meeting\"].fillna(\"unknown\", inplace = True)\nX_test[\"permit\"].fillna(\"unknown\", inplace = True)\nX_test[\"funder\"].fillna(\"unknown\", inplace = True)\n# X_test[\"installer\"].fillna(\"unknown\", inplace = True)","473af409":"X_test.isna().sum().sort_values(ascending=False)","6a1f7a70":"# Get number of unique entries in each column with categorical data\ncat_vars = train_df.select_dtypes(include='object').columns\nobject_nunique = list(map(lambda col: train_df[col].nunique(), cat_vars))\nd = dict(zip(cat_vars, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","b2f5bfed":"train_df.scheme_management.value_counts()","c234873a":"## scheme_management\ndef scheme_wrangler(row):\n    if row['scheme_management']=='VWC':\n        return 'vwc'\n    elif row['scheme_management']=='WUG':\n        return 'wug'\n    elif row['scheme_management']=='Water authority':\n        return 'wtr_auth'\n    elif row['scheme_management']=='WUA':\n        return 'wua'\n    elif row['scheme_management']=='Water Board':\n        return 'wtr_brd'\n    elif row['scheme_management']=='Parastatal':\n        return 'parastatal'\n    elif row['scheme_management']=='Private operator':\n        return 'pri_optr'\n    elif row['scheme_management']=='SWC':\n        return 'swc'\n    elif row['scheme_management']=='Company':\n        return 'company'\n    else:\n        return 'other'\ntrain_df['scheme_management'] = train_df.apply(lambda row: scheme_wrangler(row), axis=1)\nX_test['scheme_management'] = X_test.apply(lambda row: scheme_wrangler(row), axis=1)","2b5149aa":"train_df.extraction_type.value_counts()","a03011a8":"## extraction_type\ndef extraction_wrangler(row):\n    if row['extraction_type']=='gravity':\n        return 'gravity'\n    elif row['extraction_type']=='nira\/tanira':\n        return 'nira\/tanira'\n    elif row['extraction_type']=='submersible':\n        return 'submersible'\n    elif row['extraction_type']=='swn 80':\n        return 'swn_80'\n    elif row['extraction_type']=='mono':\n        return 'mono'\n    elif row['extraction_type']=='india mark ii':\n        return 'india_mark_ii'\n    elif row['extraction_type']=='afridev':\n        return 'afridev'\n    elif row['extraction_type']=='ksb':\n        return 'ksb'\n    elif row['extraction_type']=='windmill':\n        return 'windmill'\n    else:\n        return 'other'\ntrain_df['extraction_type'] = train_df.apply(lambda row: extraction_wrangler(row), axis=1)\nX_test['extraction_type'] = X_test.apply(lambda row: extraction_wrangler(row), axis=1)","7cf3b45c":"train_df.funder.value_counts()","ddd91820":"## funder\ndef funder_wrangler(row):  \n    '''Keep top 8 values and set the rest to 'other'''\n\n    if row['funder']=='Government Of Tanzania':\n        return 'gov'\n    elif row['funder']=='Danida':\n        return 'danida'\n    elif row['funder']=='Hesawa':\n        return 'hesawa'\n    elif row['funder']=='Rwssp':\n        return 'rwssp'\n    elif row['funder']=='World Bank':\n        return 'world_bank'   \n    elif row['funder']=='Kkkt':\n        return 'kkkt'   \n    elif row['funder']=='World Vision':\n        return 'world_vision'  \n    elif row['funder']=='Unicef':\n        return 'unicef'\n    else:\n        return 'other'\n    \ntrain_df['funder'] = train_df.apply(lambda row: funder_wrangler(row), axis=1)\nX_test['funder'] = X_test.apply(lambda row: funder_wrangler(row), axis=1)","e122b5f8":"train_df.quality_group.value_counts()","1209d25a":"order_dict_quality = {\"good\":3,\"salty\":2,\"milky\":2,\"colored\":2,\"fluoride\":2,\"unknown\":1}\ntrain_df[\"quality_group_code\"] = [order_dict_quality[item] for item in train_df.quality_group]\ndel train_df[\"quality_group\"]\n\nX_test[\"quality_group_code\"] = [order_dict_quality[item] for item in X_test.quality_group]\ndel X_test[\"quality_group\"]","7996ae6a":"train_df.quantity_group.value_counts()","2232c4e1":"order_dict_quantity = {\"enough\":3,\"insufficient\":2,\"dry\":2,\"seasonal\":2,\"unknown\":1}\ntrain_df[\"quantity_group_code\"] = [order_dict_quantity[item] for item in train_df.quantity_group] \ndel train_df[\"quantity_group\"]\n\nX_test[\"quantity_group_code\"] = [order_dict_quantity[item] for item in X_test.quantity_group] \ndel X_test[\"quantity_group\"]","5a827287":"train_df.payment_type.value_counts()","12d27d0d":"order_dict_payment = {\"monthly\":4,\"annually\":4,\"on failure\":3,\"per bucket\":3,\"never pay\":2,\"unknown\":1,\"other\":1}\ntrain_df[\"payment_code\"] = [order_dict_payment[item] for item in train_df.payment_type] \ndel train_df[\"payment_type\"]\n\nX_test[\"payment_code\"] = [order_dict_payment[item] for item in X_test.payment_type] \ndel X_test[\"payment_type\"]","384475d1":"train_df.public_meeting.value_counts()","cc5c3f5e":"order_dict_pub_meet = {True:2,False:1,\"unknown\":0}\ntrain_df[\"public_meeting_code\"] = [order_dict_pub_meet[item] for item in train_df.public_meeting] \ndel train_df[\"public_meeting\"]\n\nX_test[\"public_meeting_code\"] = [order_dict_pub_meet[item] for item in X_test.public_meeting] \ndel X_test[\"public_meeting\"]","d06ef511":"train_df.permit.value_counts()","0d87c558":"order_dict_permit = {True:2,False:1,\"unknown\":0}\ntrain_df[\"permit_code\"] = [order_dict_pub_meet[item] for item in train_df.permit] \ndel train_df[\"permit\"]\n\nX_test[\"permit_code\"] = [order_dict_pub_meet[item] for item in X_test.permit] \ndel X_test[\"permit\"]","19fdcd53":"train_df.loc[train_df['amount_tsh'] < 200000, 'amount_tsh'] = 0\ntrain_df.loc[train_df['amount_tsh'] >= 200000, 'amount_tsh'] = 1\n\nX_test.loc[train_df['amount_tsh'] < 200000, 'amount_tsh'] = 0\nX_test.loc[train_df['amount_tsh'] >= 200000, 'amount_tsh'] = 1","32e362f0":"LDA_cols = [\"latitude\",\"longitude\",\"gps_height\"]\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\ntrain_df_sc = sc.fit_transform(train_df[LDA_cols])\nX_test_sc = sc.transform(X_test[LDA_cols])\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nlda = LDA(n_components=None)\n\ntrain_df_lda = lda.fit_transform(train_df_sc, train_df.label.values.ravel())\nX_test_lda = lda.transform(X_test_sc)\n\ntrain_df = pd.concat((pd.DataFrame(train_df_lda), train_df), axis=1)\nX_test = pd.concat((pd.DataFrame(X_test_lda), X_test), axis=1)\n    \n    \nfor i in LDA_cols:\n    del train_df[i]\n    del X_test[i]","c59f70e9":"train_df.rename(columns={0: \"LDA_0\",1:\"LDA_1\"},inplace=True)\nX_test.rename(columns={0: \"LDA_0\",1:\"LDA_1\"},inplace=True)","29551dec":"def construction_wrangler(row):\n    if row['construction_year'] >= 1960 and row['construction_year'] < 1970:\n        return '60s'\n    elif row['construction_year'] >= 1970 and row['construction_year'] < 1980:\n        return '70s'\n    elif row['construction_year'] >= 1980 and row['construction_year'] < 1990:\n        return '80s'\n    elif row['construction_year'] >= 1990 and row['construction_year'] < 2000:\n        return '90s'\n    elif row['construction_year'] >= 2000 and row['construction_year'] < 2010:\n        return '00s'\n    elif row['construction_year'] >= 2010:\n        return '10s'\n    else:\n        return 'unknown'\n    \ntrain_df['construction_year'] = train_df.apply(lambda row: construction_wrangler(row), axis=1)\nX_test['construction_year'] = X_test.apply(lambda row: construction_wrangler(row), axis=1)","f7a257cd":"train_df.date_recorded = pd.to_datetime(train_df.date_recorded)\nX_test.date_recorded = pd.to_datetime(X_test.date_recorded)\n\ntrain_df.date_recorded.describe()","c8efae6c":"# The most recent data is 2013-12-03. Subtract each date from this point to obtain a \n# 'days_since_recorded' column.\n\ntrain_df['days_since_recorded'] = pd.datetime(2013, 12, 3) - pd.to_datetime(train_df.date_recorded)\ntrain_df['days_since_recorded'] = train_df['days_since_recorded'].astype('timedelta64[D]').astype(int)\n\nX_test['days_since_recorded'] = pd.datetime(2013, 12, 3) - pd.to_datetime(X_test.date_recorded)\nX_test['days_since_recorded'] = X_test['days_since_recorded'].astype('timedelta64[D]').astype(int)","d02111a2":"train_df['days_since_recorded']","8d85b666":"train_df = train_df.drop(\"date_recorded\",axis=1)\nX_test = X_test.drop(\"date_recorded\",axis=1)","2db2c184":"train_df.shape","461a04ec":"X_test.shape","9affca3b":"cat_vars = train_df.select_dtypes(include='object').columns\nprint(cat_vars)\nlen(cat_vars)","99a2eea1":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_df[cat_vars])).astype(np.int64)\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[cat_vars])).astype(np.int64)\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train_df.index\nOH_cols_test.index = X_test.index\n\nOH_cols_train.columns = OH_encoder.get_feature_names(cat_vars)\nOH_cols_test.columns = OH_encoder.get_feature_names(cat_vars)\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = train_df.drop(cat_vars, axis=1)\nnum_X_valid = X_test.drop(cat_vars, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_train_df = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_test = pd.concat([num_X_valid, OH_cols_test], axis=1)","042a9a44":"OH_train_df.head()","66f8d70c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\n\nX, y = OH_train_df[OH_train_df.columns.drop(\"label\")], OH_train_df['label']\n\n# Set the regularization parameter C=1\nlogistic = LogisticRegression(solver=\"saga\",C=1, penalty=\"l1\", random_state=7).fit(X, y)\nmodel = SelectFromModel(logistic, prefit=True)\n\nX_new = model.transform(X)\nX_new","938c0933":"# Get back the kept features as a DataFrame with dropped columns as all 0s\nselected_features = pd.DataFrame(model.inverse_transform(X_new), \n                                 index=X.index,\n                                 columns=X.columns)\n\n# Dropped columns have values of all 0s, keep other columns \nselected_columns = selected_features.columns[selected_features.var() != 0]","308cbd82":"len(selected_columns)","a4bb4421":"selected_columns","67c2e9ce":"train_df_selected_features = OH_train_df[selected_columns].join(y)","433ae689":"X_test_selected_features = OH_X_test[selected_columns]","98f2f33e":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(criterion='gini',min_samples_split=8, n_estimators=1000,\n                           random_state = 7)\nrf.fit(X, y)","ecd78ab4":"# helper function for creating a feature importance dataframe\ndef imp_df(column_names, importances):\n    df = pd.DataFrame({'feature': column_names,\n                       'feature_importance': importances}) \\\n           .sort_values('feature_importance', ascending = False) \\\n           .reset_index(drop = True)\n    return df\n\n# plotting a feature importance dataframe (horizontal barchart)\ndef var_imp_plot(imp_df, title):\n    imp_df.columns = ['feature', 'feature_importance']\n    sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df, orient = 'h', color = 'royalblue') \\\n       .set_title(title, fontsize = 20)","2c2f4d51":"base_imp = imp_df(X.columns, rf.feature_importances_)\ntop_30_imp = base_imp[0:30]\ntop_30_features = top_30_imp.feature","ebed1b30":"pylab.rcParams[\"figure.figsize\"] = (10,10)","e20657d4":"var_imp_plot(base_imp, 'Default feature importance (scikit-learn)')","68bc55eb":"train_df_final_top_imp = OH_train_df[top_30_features].join(y)\nX_test_final_top_imp = OH_X_test[top_30_features]","6c642b63":"train_df_final_top_imp.shape","1c032255":"train_df_selected_features.shape","71469662":"train_df_selected_features.to_csv(\"train_df_final.csv\", index=False)\nX_test_selected_features.to_csv(\"X_test_final.csv\", index=False)","a725f55e":"## 2. Dropping similar features","04efafc8":"I have tested this featureand realized that models are unable to process a tuple. We will use the linear discriminant analysis for these variables.","012547eb":"### 7.3 Linear Discriminant Analysis (LDA)\n\nLDA helps reducing dimentiality by maximizing the difference between categories. In our highly dimentional model it is a great tool. We will use LDA for the following columns as all of them contain geographical information:\n \n- latitude\n- longitude\n- gps_height\n\nReference: https:\/\/zlatankr.github.io\/posts\/2017\/01\/23\/pump-it-up","5acd5778":"## 6. Ordinal encoding of categorical data\nSeveral categorical features can be encoded in a specific order that follows from the range of its values. By using ordinal encoding instead of one-hot encoding we will avoid creating numerious additional columns and provide some logic to the model on how to evaluate these features. For example for the quality_group variable, the higher the label, the better the water quality, the more likely a pump is functional.\n\n### 6.1 Quality_group","1a60993d":"# II. Data Cleaning & Preprocessing\n## 1. Libraries and input data","0893704d":"all_df[\"geopoint\"] = [xy for xy in zip(all_df['longitude'], all_df['latitude'])]","445bf467":"## 3. Dealing with missing values","8d0adbdd":"### 9.2 Feature importances with Random Forest","ffce314e":"### 6.2 Quantity_group","264a40a9":"from shapely.geometry import Point # Shapely for converting latitude\/longtitude to geometry\nimport geopandas as gpd # To create GeodataFrame\nimport geoplot as gp","87cd3519":"### 7.2 Combine longitude and latitude in one feature\nIn this section we tried to create a new variable that would combine both longitude and latitude using a zip method. ","178327e9":"gdf.plot(marker='*', markersize=0.2, column=\"label\",legend=True)","327d5304":"## ** Transforming data to reduce skew\nUni-modal, skewed distributions could potentially be log transformed: \n* Longtitude\n* GPS_hight\n* Region_code\n\n** For later","d0d603b8":"## 5. Reducing cardinality\n### 5.1 Select features","8158cb70":"### 7.5 Date_recorded\nWe will calculate the number of days past since the date_recorded of a particular pump till the most recent date of the dataset. The idea being that more recently recorded pumps might be more likely to be functional than non-functional. \n\nLet's first convert the column to type datetime. ","cb711d2d":"Based on the above analysis:\n\nthe \"recorded_by\" feature can be dropped as there is only 1 unique value, it doesn't help in predicting.\n\nThe columns in which values can be ordered we can perform an Ordinal encoding: \n* quality_group\n* quantity_group\n* payment_type\n\nThe cardinality of the following 2 features should be reduced to 10 and then one-hot encode them or try Binary encoding:\n* scheme_managenemt\n* extraction_type\n\nWhat to do with the following 3? The cardinality is too high... : --> will drop at the first model run, later could try Frequency encoding,Binary encoding if reduce cardinality to at least 100.\n* funder\n* installer\n* subvillage\n\nThe rest can be one-hot encoded as the cardinality is lower than 10:\n* public_meeting # later -> Binary?\n* permit # later-> Binary?\n* source_class\n* management_group\n* waterpoint_type_group\n* source_type\n* basin\n\n### 5.2 Scheme_management","a56f3f2a":"# creating a geometry column \ngeopoint = [Point(xy) for xy in zip(train_df['longitude'], train_df['latitude'])]\n# Coordinate reference system : WGS84\ncrs = {'init': 'epsg:4326'}\n# Creating a Geographic data frame \ngdf = gpd.GeoDataFrame(train_df, crs=crs, geometry=geopoint)","c0cc694e":"## 9. Feature selection\n### 9.1 L1 regularization with logistic regression","2a1cc2ce":"Yellow spots - functional pumps, green - needs repair, purple - non-functional.\n\nAs we see, south-est and almost all central and south west pumps are mostly non-functional. Area around the capital (central-south part) is mostly functional as well as north-est pumps. The rest seems unclear. \n\nHow to use this info for the model? Try to create clusters (for ex around big cities)?","9b6da73e":"# *Pump-it-up project*\n\n### Can you predict which water pumps are faulty?\n\n## Goal\nUsing data from Taarifa and the Tanzanian Ministry of Water, predict which pumps are functional, which need some repairs, and which don't work at all based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. \n\nA smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.","5c9edd17":"## 7. Feature Engineering\n\nFeature engineering is a powerful way to improve the model. We will create new variables based on the features in the dataset that will better describe the target.\n\n### 7.1 Amount_tsh\nBased on the EDA we have previously defined a threshold to separate functional and non-functional pumps. Let's create a new binary variable that reflects this information.","9f66f831":"The following groups of features\n\n- (extraction_type, extraction_type_group, extraction_type_class),\n- (payment, payment_type),\n- (water_quality, quality_group),\n- (source, source_class),\n- (subvillage, region, region_code, district_code, lga, ward),\n- (waterpoint_type, waterpoint_type_group)\n- (scheme_name, scheme_management)\n\ncontain very similar information, so the correlation between them is high. This way we are risking overfitting the training data by including all the features in our analysis.\n\nBesides:\n\nnum_private is ~99% zeros and has no description, so we cannot interpret it\nin the wpt_name feature 45k unique values out of 75k observations, not very informative -> drop for now\n\n**TO DO**: The correlation between \"construction_year\" and \"gps_height\" is high, but these 2 variables don't have any obvious connection, so explore this correlation further to take a decision.\n\nAs we saw earlier, there exists quite a strong correlation between district_code and region_code, so we will drop one of these variables. The negative correlation to the target variable of the \"region_code\" is higher than that of the \"district_code\". Keep the variable with higher correlation to the target.","8fae16d8":"### 6.5 Permit","02506cc8":"### 5.4 Funder","9e7bc6ec":"## 8. One-hot encoding of categorical features","ecfe13db":"### 6.4 Public_meeting","b267f219":"## 10. Export the final dfs\n\nIn the part *III. Model selection* I have tested different models with these 2 final sets of variables (80 vars and 30 vars). The result seems to be stable across all the models - the set of 80 variables we received after doing L1 regularization with logistic regression scores higher every time. I am thus using the selected_features dfs as the final input.","c331fe90":"### 5.3 Extraction_type","7b1f5da6":"### 6.3 Payment_type","06c35a1e":"### 7.4 Construction year\nWe will turn construction_year into a categorical column with bins containing the following values: '60s', '70s', '80s', '90s, '00s', '10s', 'unknown'."}}