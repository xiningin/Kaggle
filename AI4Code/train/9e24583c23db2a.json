{"cell_type":{"3ff9c6dd":"code","d3ba7b0a":"code","281415a0":"code","754b93f7":"code","0271cdf5":"code","2408fc2b":"code","78b5cf6d":"code","55463b9c":"code","cfd05bdc":"code","0feeb643":"code","4c094257":"code","478e8cfd":"code","92631e33":"code","35a8e58e":"code","d937efd7":"code","1431ed2b":"markdown","fd60a8d4":"markdown","3c471657":"markdown","0f67e745":"markdown","b920d1c0":"markdown","73712b2c":"markdown","27cb0b6d":"markdown","668da4e3":"markdown","5e3ca43a":"markdown","e68d238b":"markdown","7bdb8efc":"markdown","e4b39431":"markdown","49d4746f":"markdown","ddc757a0":"markdown","9d24bad5":"markdown","9b9f1d13":"markdown"},"source":{"3ff9c6dd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random as python_random\n\nfrom matplotlib import pyplot as plt\n\nfrom keras import Model\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.layers import Dropout\nfrom keras.regularizers import l2\nfrom kerastuner.tuners import RandomSearch\nimport tensorflow as tf","d3ba7b0a":"def plot_history(history):\n    plt.style.use('ggplot')\n    acc = history.history['categorical_accuracy']\n    val_acc = history.history['val_categorical_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","281415a0":"np.random.seed(123)\npython_random.seed(123)\ntf.random.set_seed(1234)","754b93f7":"X_train = np.load('\/kaggle\/input\/gait-recognition-using-smartphone\/X_train.npy')\nX_test = np.load('\/kaggle\/input\/gait-recognition-using-smartphone\/X_test.npy')\nX_train.shape, X_test.shape","0271cdf5":"y_train = np.load('\/kaggle\/input\/gait-recognition-using-smartphone\/y_train.npy')\ny_train.shape","2408fc2b":"# We transform y_train for a one-hot encoding into integers between 1 and 118\ny_train_flattened = np.argmax(y_train,axis=1)+1\n\n# We count the number of instances of each class\nclasses_count = np.zeros(118,dtype=np.int64)\nfor k in range(1,119):\n    classes_count[k-1] = y_train_flattened[y_train_flattened == k].shape[0]\nprint('Minimum number of instances of a class :',classes_count.min())\nprint('Maximum number of instances of a class :',classes_count.max())\nprint('Mean number of instances of a class :',classes_count.mean())\nprint('Standard deviation of the number of instances of a class :',classes_count.std())","78b5cf6d":"from sklearn.model_selection import train_test_split\nX_training, X_validation, y_training, y_validation = train_test_split(X_train, y_train, test_size=0.20, random_state=42)","55463b9c":"model = Sequential()\nmodel.add(layers.LSTM(500, input_shape=X_train.transpose(0,2,1).shape[1:]))\nmodel.add(layers.Dense(340, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(layers.Dense(118, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","cfd05bdc":"# CNN network\nmodel_cnn = Sequential()\nmodel_cnn.add(layers.Conv1D(filters=32, kernel_size=9, strides=2, \n                 activation='relu', input_shape=X_train.transpose(0,2,1).shape[1:]))\nmodel_cnn.add(layers.Conv1D(filters=64, kernel_size=3,\n                 activation='relu'))\nmodel_cnn.add(layers.Conv1D(filters=128, kernel_size=3,\n                 activation='relu'))\nmodel_cnn.add(layers.Conv1D(filters=128, kernel_size=6,\n                 activation='relu'))\nmodel_cnn.add(layers.Flatten())\n\n# LSTM network\nmodel_lstm = Sequential()\nmodel_lstm.add(layers.LSTM(1024, input_shape=X_train.transpose(0,2,1).shape[1:]))\n\n# Concatenating the two networks and adding a final Dense layer\nmodel_concat = layers.concatenate([model_cnn.output, model_lstm.output], axis=-1)\nmodel_concat = layers.Dense(118, activation='softmax')(model_concat)\n\n# Creating the complete model based on the concatenation of the CNN and LSTM networks\nmodel = Model(inputs=[model_cnn.input, model_lstm.input], outputs=model_concat)\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","0feeb643":"dropout_values = [0., 0.1, 1\/4, 1\/2]\nregularizer_values = [0., 1e-4, 1e-3, 1e-2]\n\ndef build_model_simple_2(hp):\n    filters=hp.Int('filters', min_value=50, max_value=150, step=50)\n    model = Sequential()\n    for i in range(hp.Int('n_layers', 1, 2)):\n        model.add(layers.Conv1D(filters=filters, \n                                kernel_size=hp.Int('kernel_size_'+str(i), min_value=10, max_value=20, step=5), \n                                activation='relu',input_shape=X_train.shape[1:], data_format='channels_first',\n                                kernel_regularizer=l2(hp.Choice('cnn_'+str(i)+'_kernel_regularize', regularizer_values)),\n                                bias_regularizer=l2(hp.Choice('cnn_'+str(i)+'_bias_regularize', regularizer_values)),\n                                activity_regularizer=l2(hp.Choice('cnn_'+str(i)+'_activity_regularize', regularizer_values))))\n        model.add(Dropout(hp.Choice('Dropout_cnn_'+str(i), dropout_values)))\n    model.add(layers.GlobalMaxPooling1D(data_format='channels_first'))\n    model.add(layers.Dense(units=hp.Int('units', min_value=600, max_value=800, step=100), activation='relu',\n                           kernel_regularizer=l2(hp.Choice('dense_'+str(i)+'_kernel_regularize', regularizer_values)),\n                           bias_regularizer=l2(hp.Choice('dense_'+str(i)+'_bias_regularize', regularizer_values)),\n                           activity_regularizer=l2(hp.Choice('dense_'+str(i)+'_activity_regularize', regularizer_values))))\n    \n    model.add(Dropout(hp.Choice('Dropout_dense', dropout_values)))\n    model.add(layers.Dense(118, activation='softmax'))\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    return model\n\ntuner_simple_2 = RandomSearch(\n    build_model_simple_2,\n    objective='val_categorical_accuracy',\n    max_trials=100,\n    executions_per_trial=2,\n    directory='hp_tuning',\n    project_name='gait')\n\ntuner_simple_2.search(X_training, y_training,\n            epochs=10,\n            verbose=True,\n            validation_data=(X_validation, y_validation),\n            batch_size=100)","4c094257":"tuner_simple_2.results_summary()","478e8cfd":"model_final = Sequential()\nmodel_final.add(layers.Conv1D(filters=150, \n                                kernel_size=10, \n                                activation='relu',input_shape=X_train.shape[1:], data_format='channels_first',\n                                kernel_regularizer=l2(0.001),\n                                bias_regularizer=l2(0.01),\n                                activity_regularizer=l2(0.0001)))\nmodel_final.add(layers.Dropout(0.5))\nmodel_final.add(layers.Conv1D(filters=150, \n                                kernel_size=15, \n                                activation='relu',input_shape=X_train.shape[1:], data_format='channels_first',\n                                kernel_regularizer=l2(0.0001),\n                                bias_regularizer=l2(0.0),\n                                activity_regularizer=l2(0.0001)))\nmodel_final.add(layers.Dropout(0.5))\nmodel_final.add(layers.GlobalMaxPooling1D(data_format='channels_first'))\nmodel_final.add(layers.Dense(700,kernel_regularizer=l2(0.001),\n                                bias_regularizer=l2(0.001),\n                                activity_regularizer=l2(0.01),activation='relu'))\nmodel_final.add(layers.Dropout(0.1))\nmodel_final.add(layers.Dense(118,kernel_regularizer=l2(0.001),\n                                bias_regularizer=l2(0.001),\n                                activity_regularizer=l2(0.0001), activation='softmax'))\nmodel_final.add(layers.Dropout(0.5))\nmodel_final.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nmodel_final.summary()","92631e33":"best_hp = tuner_simple_2.get_best_hyperparameters()[0]\nmodel = tuner_simple_2.hypermodel.build(best_hp)\nhistory = model.fit(X_training, y_training,\n                    epochs=10,\n                    verbose=False,\n                    validation_data=(X_validation, y_validation),\n                    batch_size=100)\nloss, accuracy = model.evaluate(X_training, y_training, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_validation, y_validation, verbose=False)\nprint(\"Validation Accuracy:  {:.4f}\".format(accuracy))","35a8e58e":"plot_history(history)","d937efd7":"y_test_pred = np.argmax(model.predict(X_test),axis=-1) + 1\ny_test_pred_csv = pd.DataFrame(data = {'Prediction':y_test_pred})\ny_test_pred_csv.to_csv('.\/sampleSubmission.csv', index_label='Id')","1431ed2b":"## Optimized model\n\nHere is the structure of the model using the hyperparameters found by the Keras Tuner :","fd60a8d4":"# Setting a seed for reproducible results","3c471657":"## Hyperparameters tuning","0f67e745":"# Generating the csv file for the submission","b920d1c0":"### Results\n\nAfter tuning the model, the results were quite satisfying, having around 91% of accuracy on the test set. But the results of the CNN were better, so we chose it.","73712b2c":"## CNN+LSTM\n\nIn parallel, Marc-Andr\u00e9 worked on the possibility to combine CNNs with LSTM units. The first approach was to simply add LSTM units after Conv1D layers, which gave unsatisfactory results, so another approach was tried. Inspired by a research paper on the same topic (https:\/\/github.com\/qinnzou\/Gait-Recognition-Using-Smartphones), we implemented the idea of concatenating a CNN with a RNN using LSTM was tried. Here is the structure of the model that we implemented :","27cb0b6d":"# Best performance : CNN implementation\n\nGuillaume tried to implement a Convolutonial Neural Network, which quickly gave quite satisfactory results. Meanwhile, Rapha\u00ebl prepared a Keras Tuner to optimize the hyperparameters of the model.","668da4e3":"# Importing the data","5e3ca43a":"### Function used to plot the history of the training of a model","e68d238b":"### Results\n\nThe model was very long to train due to the high quantity of parameters. Although, even after tuning, the results were atmost 92%, which was still not better than the CNN that we implemented.\n\nIt could be interesting, based on the paper's results, to first train the LSTM network, and then train the CNN concatenated with the LSTM network having its parameters fixed. We didn't have the time to do that but it could be an interesting lead.","7bdb8efc":"### Remark :\n\nOne can see here that the classes are not all equally represented in the dataset. This could make some classes harder to identify. ","e4b39431":"# Data Challenge : Gait Recognition using Inertial Sensors\n\n### Team : Stanislas Chaboud, Marc-Andr\u00e9 Sergiel, Guillaume Suatton, Rapha\u00ebl Teboul\n","49d4746f":"# Results\n\nIn the end, we have an accuracy of 93.2% on the test set when submitting our results. \n\nIt seems satisfactory since the research paper about the same problem achieved atmost 93.5%.\n\nAs said previously, we could explore the possibility to combine CNN and LSTM networks to achieve a slightly better performance and reduce that gap.","ddc757a0":"# Importing the libraires","9d24bad5":"## Training model using the best found configuration","9b9f1d13":"# Tried implementations\n\n## LSTM\n\nWe first tried to train a Recurrent Neural Network using LSTM cells. Stanislas implemented it and tuned the parameters. Here is the structure of the model we implemented :"}}