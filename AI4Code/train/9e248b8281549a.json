{"cell_type":{"341863b2":"code","ba21c0be":"code","fe6644a5":"code","63df4dd2":"code","3d56ff1d":"markdown","98832b7b":"markdown","f98f6a4b":"markdown"},"source":{"341863b2":"!git clone https:\/\/github.com\/facebookresearch\/detectron2.git\n%cd detectron2\n!python -m pip install -e .\/","ba21c0be":"%%writefile detectron2\/data\/datasets\/builtin.py\n\n# -*- coding: utf-8 -*-\n# Copyright (c) Facebook, Inc. and its affiliates.\n\n\n\"\"\"\nThis file registers pre-defined datasets at hard-coded paths, and their metadata.\n\nWe hard-code metadata for common datasets. This will enable:\n1. Consistency check when loading the datasets\n2. Use models on these standard datasets directly and run demos,\n   without having to download the dataset annotations\n\nWe hard-code some paths to the dataset that's assumed to\nexist in \".\/datasets\/\".\n\nUsers SHOULD NOT use this file to create new dataset \/ metadata for new dataset.\nTo add new dataset, refer to the tutorial \"docs\/DATASETS.md\".\n\"\"\"\n\nimport os\n\nfrom detectron2.data import DatasetCatalog, MetadataCatalog\n\nfrom .builtin_meta import ADE20K_SEM_SEG_CATEGORIES, _get_builtin_metadata\nfrom .cityscapes import load_cityscapes_instances, load_cityscapes_semantic\nfrom .cityscapes_panoptic import register_all_cityscapes_panoptic\nfrom .coco import load_sem_seg, register_coco_instances\nfrom .coco_panoptic import register_coco_panoptic, register_coco_panoptic_separated\nfrom .lvis import get_lvis_instances_meta, register_lvis_instances\nfrom .pascal_voc import register_pascal_voc\n\n# ==== Predefined datasets and splits for COCO ==========\n\n_PREDEFINED_SPLITS_COCO = {}\n_PREDEFINED_SPLITS_COCO[\"coco\"] = {\n    \"coco_2014_train\": (\"coco\/train2014\", \"coco\/annotations\/instances_train2014.json\"),\n    \"coco_2014_val\": (\"coco\/val2014\", \"coco\/annotations\/instances_val2014.json\"),\n    \"coco_2014_minival\": (\"coco\/val2014\", \"coco\/annotations\/instances_minival2014.json\"),\n    \"coco_2014_minival_100\": (\"coco\/val2014\", \"coco\/annotations\/instances_minival2014_100.json\"),\n    \"coco_2014_valminusminival\": (\n        \"coco\/val2014\",\n        \"coco\/annotations\/instances_valminusminival2014.json\",\n    ),\n    \"coco_2017_train\": (\"coco\/train2017\", \"coco\/annotations\/instances_train2017.json\"),\n    \"coco_2017_val\": (\"coco\/val2017\", \"coco\/annotations\/instances_val2017.json\"),\n    \"coco_2017_test\": (\"coco\/test2017\", \"coco\/annotations\/image_info_test2017.json\"),\n    \"coco_2017_test-dev\": (\"coco\/test2017\", \"coco\/annotations\/image_info_test-dev2017.json\"),\n    \"coco_2017_val_100\": (\"coco\/val2017\", \"coco\/annotations\/instances_val2017_100.json\"),\n}\n\n_PREDEFINED_SPLITS_COCO[\"coco_person\"] = {\n    \"keypoints_coco_2014_train\": (\n        \"coco\/train2014\",\n        \"coco\/annotations\/person_keypoints_train2014.json\",\n    ),\n    \"keypoints_coco_2014_val\": (\"coco\/val2014\", \"coco\/annotations\/person_keypoints_val2014.json\"),\n    \"keypoints_coco_2014_minival\": (\n        \"coco\/val2014\",\n        \"coco\/annotations\/person_keypoints_minival2014.json\",\n    ),\n    \"keypoints_coco_2014_valminusminival\": (\n        \"coco\/val2014\",\n        \"coco\/annotations\/person_keypoints_valminusminival2014.json\",\n    ),\n    \"keypoints_coco_2014_minival_100\": (\n        \"coco\/val2014\",\n        \"coco\/annotations\/person_keypoints_minival2014_100.json\",\n    ),\n    \"keypoints_coco_2017_train\": (\n        \"coco\/train2017\",\n        \"coco\/annotations\/person_keypoints_train2017.json\",\n    ),\n    \"keypoints_coco_2017_val\": (\"coco\/val2017\", \"coco\/annotations\/person_keypoints_val2017.json\"),\n    \"keypoints_coco_2017_val_100\": (\n        \"coco\/val2017\",\n        \"coco\/annotations\/person_keypoints_val2017_100.json\",\n    ),\n}\n\n\n_PREDEFINED_SPLITS_COCO_PANOPTIC = {\n    \"coco_2017_train_panoptic\": (\n        # This is the original panoptic annotation directory\n        \"coco\/panoptic_train2017\",\n        \"coco\/annotations\/panoptic_train2017.json\",\n        # This directory contains semantic annotations that are\n        # converted from panoptic annotations.\n        # It is used by PanopticFPN.\n        # You can use the script at detectron2\/datasets\/prepare_panoptic_fpn.py\n        # to create these directories.\n        \"coco\/panoptic_stuff_train2017\",\n    ),\n    \"coco_2017_val_panoptic\": (\n        \"coco\/panoptic_val2017\",\n        \"coco\/annotations\/panoptic_val2017.json\",\n        \"coco\/panoptic_stuff_val2017\",\n    ),\n    \"coco_2017_val_100_panoptic\": (\n        \"coco\/panoptic_val2017_100\",\n        \"coco\/annotations\/panoptic_val2017_100.json\",\n        \"coco\/panoptic_stuff_val2017_100\",\n    ),\n}\n\n\ndef register_all_coco(root):\n    for dataset_name, splits_per_dataset in _PREDEFINED_SPLITS_COCO.items():\n        for key, (image_root, json_file) in splits_per_dataset.items():\n            # Assume pre-defined datasets live in `.\/datasets`.\n            register_coco_instances(\n                key,\n                _get_builtin_metadata(dataset_name),\n                os.path.join(root, json_file) if \":\/\/\" not in json_file else json_file,\n                os.path.join(root, image_root),\n            )\n\n    for (\n        prefix,\n        (panoptic_root, panoptic_json, semantic_root),\n    ) in _PREDEFINED_SPLITS_COCO_PANOPTIC.items():\n        prefix_instances = prefix[: -len(\"_panoptic\")]\n        instances_meta = MetadataCatalog.get(prefix_instances)\n        image_root, instances_json = instances_meta.image_root, instances_meta.json_file\n        # The \"separated\" version of COCO panoptic segmentation dataset,\n        # e.g. used by Panoptic FPN\n        register_coco_panoptic_separated(\n            prefix,\n            _get_builtin_metadata(\"coco_panoptic_separated\"),\n            image_root,\n            os.path.join(root, panoptic_root),\n            os.path.join(root, panoptic_json),\n            os.path.join(root, semantic_root),\n            instances_json,\n        )\n        # The \"standard\" version of COCO panoptic segmentation dataset,\n        # e.g. used by Panoptic-DeepLab\n        register_coco_panoptic(\n            prefix,\n            _get_builtin_metadata(\"coco_panoptic_standard\"),\n            image_root,\n            os.path.join(root, panoptic_root),\n            os.path.join(root, panoptic_json),\n            instances_json,\n        )\n\n\n# ==== Predefined datasets and splits for LVIS ==========\n\n\n_PREDEFINED_SPLITS_LVIS = {\n    \"lvis_v1\": {\n        \"lvis_v1_train\": (\"coco\/\", \"lvis\/lvis_v1_train.json\"),\n        \"lvis_v1_val\": (\"coco\/\", \"lvis\/lvis_v1_val.json\"),\n        \"lvis_v1_test_dev\": (\"coco\/\", \"lvis\/lvis_v1_image_info_test_dev.json\"),\n        \"lvis_v1_test_challenge\": (\"coco\/\", \"lvis\/lvis_v1_image_info_test_challenge.json\"),\n    },\n    \"lvis_v0.5\": {\n        \"lvis_v0.5_train\": (\"coco\/\", \"lvis\/lvis_v0.5_train.json\"),\n        \"lvis_v0.5_val\": (\"coco\/\", \"lvis\/lvis_v0.5_val.json\"),\n        \"lvis_v0.5_val_rand_100\": (\"coco\/\", \"lvis\/lvis_v0.5_val_rand_100.json\"),\n        \"lvis_v0.5_test\": (\"coco\/\", \"lvis\/lvis_v0.5_image_info_test.json\"),\n    },\n    \"lvis_v0.5_cocofied\": {\n        \"lvis_v0.5_train_cocofied\": (\"coco\/\", \"lvis\/lvis_v0.5_train_cocofied.json\"),\n        \"lvis_v0.5_val_cocofied\": (\"coco\/\", \"lvis\/lvis_v0.5_val_cocofied.json\"),\n    },\n}\n\n\ndef register_all_lvis(root):\n    for dataset_name, splits_per_dataset in _PREDEFINED_SPLITS_LVIS.items():\n        for key, (image_root, json_file) in splits_per_dataset.items():\n            register_lvis_instances(\n                key,\n                get_lvis_instances_meta(dataset_name),\n                os.path.join(root, json_file) if \":\/\/\" not in json_file else json_file,\n                os.path.join(root, image_root),\n            )\n\n\n# ==== Predefined splits for raw cityscapes images ===========\n_RAW_CITYSCAPES_SPLITS = {\n    \"cityscapes_fine_{task}_train\": (\"cityscapes\/leftImg8bit\/train\/\", \"cityscapes\/gtFine\/train\/\"),\n    \"cityscapes_fine_{task}_val\": (\"cityscapes\/leftImg8bit\/val\/\", \"cityscapes\/gtFine\/val\/\"),\n    \"cityscapes_fine_{task}_test\": (\"cityscapes\/leftImg8bit\/test\/\", \"cityscapes\/gtFine\/test\/\"),\n}\n\n\ndef register_all_cityscapes(root):\n    for key, (image_dir, gt_dir) in _RAW_CITYSCAPES_SPLITS.items():\n        meta = _get_builtin_metadata(\"cityscapes\")\n        image_dir = os.path.join(root, image_dir)\n        gt_dir = os.path.join(root, gt_dir)\n\n        inst_key = key.format(task=\"instance_seg\")\n        DatasetCatalog.register(\n            inst_key,\n            lambda x=image_dir, y=gt_dir: load_cityscapes_instances(\n                x, y, from_json=True, to_polygons=True\n            ),\n        )\n        MetadataCatalog.get(inst_key).set(\n            image_dir=image_dir, gt_dir=gt_dir, evaluator_type=\"cityscapes_instance\", **meta\n        )\n\n        sem_key = key.format(task=\"sem_seg\")\n        DatasetCatalog.register(\n            sem_key, lambda x=image_dir, y=gt_dir: load_cityscapes_semantic(x, y)\n        )\n        MetadataCatalog.get(sem_key).set(\n            image_dir=image_dir,\n            gt_dir=gt_dir,\n            evaluator_type=\"cityscapes_sem_seg\",\n            ignore_label=255,\n            **meta,\n        )\n\n\n# ==== Predefined splits for PASCAL VOC ===========\ndef register_all_pascal_voc(root):\n    SPLITS = [\n        (\"voc_2007_trainval\", \"VOC2007\", \"trainval\"),\n        (\"voc_2007_train\", \"VOC2007\", \"train\"),\n        (\"voc_2007_val\", \"VOC2007\", \"val\"),\n        (\"voc_2007_test\", \"VOC2007\", \"test\"),\n        (\"voc_2012_trainval\", \"VOC2012\", \"trainval\"),\n        (\"voc_2012_train\", \"VOC2012\", \"train\"),\n        (\"voc_2012_val\", \"VOC2012\", \"val\"),\n    ]\n    for name, dirname, split in SPLITS:\n        year = 2007 if \"2007\" in name else 2012\n        register_pascal_voc(name, os.path.join(root, dirname), split, year)\n        MetadataCatalog.get(name).evaluator_type = \"pascal_voc\"\n\n\ndef register_all_ade20k(root):\n    root = os.path.join(root, \"ADEChallengeData2016\")\n    for name, dirname in [(\"train\", \"training\"), (\"val\", \"validation\")]:\n        image_dir = os.path.join(root, \"images\", dirname)\n        gt_dir = os.path.join(root, \"annotations_detectron2\", dirname)\n        name = f\"ade20k_sem_seg_{name}\"\n        DatasetCatalog.register(\n            name, lambda x=image_dir, y=gt_dir: load_sem_seg(y, x, gt_ext=\"png\", image_ext=\"jpg\")\n        )\n        MetadataCatalog.get(name).set(\n            stuff_classes=ADE20K_SEM_SEG_CATEGORIES[:],\n            image_root=image_dir,\n            sem_seg_root=gt_dir,\n            evaluator_type=\"sem_seg\",\n            ignore_label=255,\n        )\n\ndef register_my_coco_datasets():\n    \"\"\"The function to register custom coco datasets.\"\"\"\n    register_coco_instances(\n        \"nfl2021_train\",\n        {},\n        \"\/kaggle\/input\/nfl2021-coco-train-val-annotations\/train.json\",\n        \"\/kaggle\/input\/nfl2021-train-images\/train_images\"\n    )\n    register_coco_instances(\n        \"nfl2021_valid\",\n        {},\n        \"\/kaggle\/input\/nfl2021-coco-train-val-annotations\/valid.json\",\n        \"\/kaggle\/input\/nfl2021-train-images\/train_images\"\n    )\n\n# True for open source;\n# Internally at fb, we register them elsewhere\nif __name__.endswith(\".builtin\"):\n    # Assume pre-defined datasets live in `.\/datasets`.\n    _root = os.getenv(\"DETECTRON2_DATASETS\", \"datasets\")\n    register_all_coco(_root)\n    register_all_lvis(_root)\n    register_all_cityscapes(_root)\n    register_all_cityscapes_panoptic(_root)\n    register_all_pascal_voc(_root)\n    register_all_ade20k(_root)\n    # Custom registeration happends here.\n    register_my_coco_datasets()\n","fe6644a5":"%%writefile configs\/custom_faster_rcnn.yaml\n\nMODEL:\n  WEIGHTS: \"https:\/\/dl.fbaipublicfiles.com\/detectron2\/COCO-Detection\/faster_rcnn_R_50_FPN_1x\/137257794\/model_final_b275ba.pkl\"\n  MASK_ON: False\n  RESNETS:\n    DEPTH: 50\n  META_ARCHITECTURE: \"GeneralizedRCNN\"\n  BACKBONE:\n    NAME: \"build_resnet_fpn_backbone\"\n  RESNETS:\n    OUT_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n  FPN:\n    IN_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n  ANCHOR_GENERATOR:\n    SIZES: [[32], [64], [128], [256], [512]]  # One size for each in feature map\n    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]  # Three aspect ratios (same for all in feature maps)\n  RPN:\n    IN_FEATURES: [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\"]\n    PRE_NMS_TOPK_TRAIN: 2000  # Per FPN level\n    PRE_NMS_TOPK_TEST: 1000  # Per FPN level\n    # Detectron1 uses 2000 proposals per-batch,\n    # (See \"modeling\/rpn\/rpn_outputs.py\" for details of this legacy issue)\n    # which is approximately 1000 proposals per-image since the default batch size for FPN is 2.\n    POST_NMS_TOPK_TRAIN: 1000\n    POST_NMS_TOPK_TEST: 1000\n  ROI_HEADS:\n    NAME: \"StandardROIHeads\"\n    IN_FEATURES: [\"p2\", \"p3\", \"p4\", \"p5\"]\n    NUM_CLASSES: 1 # Added\n  ROI_BOX_HEAD:\n    NAME: \"FastRCNNConvFCHead\"\n    NUM_FC: 2\n    POOLER_RESOLUTION: 7\nDATASETS:\n  TRAIN: (\"nfl2021_train\",) # Modified\n  TEST: (\"nfl2021_valid\",) # Modified\nSOLVER:\n  IMS_PER_BATCH: 2 # Modified\n  BASE_LR: 0.002 # Modified\n  STEPS: (80000,) # If you changed MAX_ITER, it's better to change when you decrease LR too here.\n  MAX_ITER: 90000 # You may want to modify this to control how long to train\nINPUT:\n  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\nVERSION: 2\n","63df4dd2":"# training is just one line. \n# !python tools\/train_net.py --num-gpus 1 --config-file configs\/custom_faster_rcnn.yaml","3d56ff1d":"# How to use detectron2\nI've already created train images and also coco annotations.\n\n- Create image dataset  \nnotebook: https:\/\/www.kaggle.com\/bamps53\/create-image-dataset  \ndataset: https:\/\/www.kaggle.com\/bamps53\/nfl2021-train-images  \n\n- Convert ground truth to coco format(with train\/val split)  \nnotebook: https:\/\/www.kaggle.com\/bamps53\/create-coco-format-annotations-train-val  \ndataset: https:\/\/www.kaggle.com\/bamps53\/nfl2021-coco-train-val-annotations  \n\nNow it's way easy to train your own detector.  \nIn this notebook I'll introduce one of the most famous object detection library, detectron2.","98832b7b":"## Hack to use custom dataset with CLI\nOfficial tutorial in detectron2 use their API and need to write custom traning script, but I prefer to use default CLI interface `tools\/train_net.py`.\nTo do that, we need to dig into the code where datasets are set up and register our custom dataset information.\nBelow code show how to modify `detectron2\/data\/datasets\/builtin.py` to register `nfl2021_train` and `nfl2021_valid` dataset.","f98f6a4b":"## Config set up\nNow you only need to modifiy config to train detector!\nBelow is the example to use FasterRCNN, but you can use any of configs in detectron2 repository.  \nhttps:\/\/github.com\/facebookresearch\/detectron2\/blob\/master\/MODEL_ZOO.md  \nhttps:\/\/github.com\/facebookresearch\/detectron2\/tree\/master\/configs  \n\nBelow looks much simpler than mmdetection configs, but it actually has a lot of hidden parameters in [default config](https:\/\/github.com\/facebookresearch\/detectron2\/blob\/master\/detectron2\/config\/defaults.py).\n(Personally I feel it's more explicit to expose all config in yaml file.)\n\nAnyway, below are the summary of what you need to modify.\n\n### Dataset\n1. You need to specify dataset name both for train and valid. The details are already registered, so you don't need to set here.\n\n### Model\n1. You need to change num classes(`MODEL.ROI_HEADS.NUM_CLASSES`) to 1.(We only need to detect only Helmet class.)\n\n### Others\n1. You need to set pretrained model path in `MODEL.WEIGHTS` because what we want to do here is finetune, not training from scratch.\n2. Also it's better to decrease LR for finetuning.\n3. You can also change `max_epochs` as you want, here I set to 1 for just demonstration."}}