{"cell_type":{"8756fb00":"code","ade82604":"code","996e54cc":"code","13421e61":"code","6a4ce704":"code","9f7718b5":"code","d02e84aa":"code","028a8c2b":"markdown","a037f2c4":"markdown","2eb77d39":"markdown","480410bf":"markdown","01410b1c":"markdown"},"source":{"8756fb00":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport fastai\nfrom fastai.vision import *\nfrom PIL import Image\nimport zipfile\nimport io\nimport cv2\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfastai.__version__","ade82604":"nfolds = 1#4\nbs = 4\nn_cls = 4\nnoise_th = 2000 #predicted masks must be larger than noise_th\nTEST = '..\/input\/severstal-steel-defect-detection\/test_images\/'\nBASE = '..\/input\/severstal-fast-ai-256x256-crops\/'\n\ntorch.backends.cudnn.benchmark = True","996e54cc":"#the code below modifies fast.ai functions to incorporate Hcolumns into fast.ai Dynamic Unet\n\nfrom fastai.vision.learner import create_head, cnn_config, num_features_model, create_head\nfrom fastai.callbacks.hooks import model_sizes, hook_outputs, dummy_eval, Hook, _hook_inner\nfrom fastai.vision.models.unet import _get_sfs_idxs, UnetBlock\n\nclass Hcolumns(nn.Module):\n    def __init__(self, hooks:Collection[Hook], nc:Collection[int]=None):\n        super(Hcolumns,self).__init__()\n        self.hooks = hooks\n        self.n = len(self.hooks)\n        self.factorization = None \n        if nc is not None:\n            self.factorization = nn.ModuleList()\n            for i in range(self.n):\n                self.factorization.append(nn.Sequential(\n                    conv2d(nc[i],nc[-1],3,padding=1,bias=True),\n                    conv2d(nc[-1],nc[-1],3,padding=1,bias=True)))\n                #self.factorization.append(conv2d(nc[i],nc[-1],3,padding=1,bias=True))\n        \n    def forward(self, x:Tensor):\n        n = len(self.hooks)\n        out = [F.interpolate(self.hooks[i].stored if self.factorization is None\n            else self.factorization[i](self.hooks[i].stored), scale_factor=2**(self.n-i),\n            mode='bilinear',align_corners=False) for i in range(self.n)] + [x]\n        return torch.cat(out, dim=1)\n\nclass DynamicUnet_Hcolumns(SequentialEx):\n    \"Create a U-Net from a given architecture.\"\n    def __init__(self, encoder:nn.Module, n_classes:int, blur:bool=False, blur_final=True, \n                 self_attention:bool=False,\n                 y_range:Optional[Tuple[float,float]]=None,\n                 last_cross:bool=True, bottle:bool=False, **kwargs):\n        imsize = (256,256)\n        sfs_szs = model_sizes(encoder, size=imsize)\n        sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs)))\n        self.sfs = hook_outputs([encoder[i] for i in sfs_idxs])\n        x = dummy_eval(encoder, imsize).detach()\n\n        ni = sfs_szs[-1][1]\n        middle_conv = nn.Sequential(conv_layer(ni, ni*2, **kwargs),\n                                    conv_layer(ni*2, ni, **kwargs)).eval()\n        x = middle_conv(x)\n        layers = [encoder, batchnorm_2d(ni), nn.ReLU(), middle_conv]\n\n        self.hc_hooks = [Hook(layers[-1], _hook_inner, detach=False)]\n        hc_c = [x.shape[1]]\n        \n        for i,idx in enumerate(sfs_idxs):\n            not_final = i!=len(sfs_idxs)-1\n            up_in_c, x_in_c = int(x.shape[1]), int(sfs_szs[idx][1])\n            do_blur = blur and (not_final or blur_final)\n            sa = self_attention and (i==len(sfs_idxs)-3)\n            unet_block = UnetBlock(up_in_c, x_in_c, self.sfs[i], final_div=not_final, \n                blur=blur, self_attention=sa, **kwargs).eval()\n            layers.append(unet_block)\n            x = unet_block(x)\n            self.hc_hooks.append(Hook(layers[-1], _hook_inner, detach=False))\n            hc_c.append(x.shape[1])\n\n        ni = x.shape[1]\n        if imsize != sfs_szs[0][-2:]: layers.append(PixelShuffle_ICNR(ni, **kwargs))\n        if last_cross:\n            layers.append(MergeLayer(dense=True))\n            ni += in_channels(encoder)\n            layers.append(res_block(ni, bottle=bottle, **kwargs))\n        hc_c.append(ni)\n        layers.append(Hcolumns(self.hc_hooks, hc_c))\n        layers += [conv_layer(ni*len(hc_c), n_classes, ks=1, use_activ=False, **kwargs)]\n        if y_range is not None: layers.append(SigmoidRange(*y_range))\n        super().__init__(*layers)\n\n    def __del__(self):\n        if hasattr(self, \"sfs\"): self.sfs.remove()\n            \ndef unet_learner(data:DataBunch, arch:Callable, pretrained:bool=True, blur_final:bool=True,\n        norm_type:Optional[NormType]=NormType, split_on:Optional[SplitFuncOrIdxList]=None, \n        blur:bool=False, self_attention:bool=False, y_range:Optional[Tuple[float,float]]=None, \n        last_cross:bool=True, bottle:bool=False, cut:Union[int,Callable]=None, \n        hypercolumns=True, **learn_kwargs:Any)->Learner:\n    \"Build Unet learner from `data` and `arch`.\"\n    meta = cnn_config(arch)\n    body = create_body(arch, pretrained, cut)\n    M = DynamicUnet_Hcolumns if hypercolumns else DynamicUnet\n    model = to_device(M(body, n_classes=data.c, blur=blur, blur_final=blur_final,\n        self_attention=self_attention, y_range=y_range, norm_type=norm_type, \n        last_cross=last_cross, bottle=bottle), data.device)\n    learn = Learner(data, model, **learn_kwargs)\n    learn.split(ifnone(split_on, meta['split']))\n    if pretrained: learn.freeze()\n    apply_init(model[2], nn.init.kaiming_normal_)\n    return learn\nclass SegmentationLabelList(SegmentationLabelList):\n    def open(self, fn): return open_mask(fn, div=True)\n    \nclass SegmentationItemList(SegmentationItemList):\n    _label_cls = SegmentationLabelList\n\n# Setting transformations on masks to False on test set\ndef transform(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n    if not tfms: tfms=(None,None)\n    assert is_listy(tfms) and len(tfms) == 2\n    self.train.transform(tfms[0], **kwargs)\n    self.valid.transform(tfms[1], **kwargs)\n    kwargs['tfm_y'] = False # Test data has no labels\n    if self.test: self.test.transform(tfms[1], **kwargs)\n    return self\nfastai.data_block.ItemLists.transform = transform\n\ndef open_mask(fn:PathOrStr, div:bool=True, convert_mode:str='L', cls:type=ImageSegment,\n        after_open:Callable=None)->ImageSegment:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        x = PIL.Image.open(fn).convert(convert_mode)\n    if after_open: x = after_open(x)\n    x = pil2tensor(x,np.float32)\n    return cls(x)","13421e61":"# Prediction with flip TTA\ndef model_pred(learns, F_save,\n        ds_type:fastai.basic_data.DatasetType=DatasetType.Valid, \n        tta:bool=True): #if use train dl, disable shuffling\n    for learn in learns: learn.model.eval();\n    dl = learn.data.dl(ds_type)\n    #sampler = dl.batch_sampler.sampler\n    #dl.batch_sampler.sampler = torch.utils.data.sampler.SequentialSampler(sampler.data_source)\n    name_list = [Path(n).stem for n in dl.dataset.items]\n    num_batchs = len(dl)\n    t = progress_bar(iter(dl), leave=False, total=num_batchs)\n    count = 0\n    with torch.no_grad():\n        for x,y in t:\n            x = x.cuda()\n            preds = []\n            for learn in learns:\n                #i, hights, widths, classes\n                py = torch.softmax(learn.model(x),dim=1).permute(0,2,3,1).detach()\n                if tta:\n                    #you can comment some transfromations to save time\n                    flips = [[-1],[-2],[-2,-1]]\n                    for f in flips:\n                        py += torch.softmax(torch.flip(learn.model(torch.flip(x,f)),f),dim=1).permute(0,2,3,1).detach()\n                    py \/= len(flips) + 1\n                preds.append(py)\n            py = torch.stack(preds).mean(0).cpu().numpy() # taking average of all preds\n            batch_size = len(py)\n            for i in range(batch_size):\n                taget = y[i].detach().cpu().numpy() if y is not None else None\n                F_save(py[i],taget,name_list[count])\n                count += 1\n    #dl.batch_sampler.sampler = sampler\n    \ndef save_img(data,name,out):\n    img = cv2.imencode('.png',(data*255).astype(np.uint8))[1]\n    out.writestr(name, img)\n    \n#dice for threshold selection\ndef dice_np(pred, targs, e=1e-7):\n    targs = targs[0,:,:]\n    pred = np.dstack([1.0 - pred.sum(-1), pred])\n    c = pred.shape[-1]\n    pred = np.argmax(pred, axis=-1)\n    dices = []\n    eps = 1e-7\n    for i in range(1,c):\n        intersect = ((pred==i) & (targs==i)).sum().astype(np.float)\n        union = ((pred==i).sum() + (targs==i).sum()).astype(np.float)\n        dices.append((2.0*intersect + eps) \/ (union + eps))\n    return np.array(dices).mean()","6a4ce704":"def enc2mask(encs, shape=(1600,256)):\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for m,enc in enumerate(encs):\n        if isinstance(enc,np.float) and np.isnan(enc): continue\n        s = enc.split()\n        for i in range(len(s)\/\/2):\n            start = int(s[2*i]) - 1\n            length = int(s[2*i+1])\n            img[start:start+length] = 1 + m\n    return img.reshape(shape).T\n\ndef mask2enc(mask, n=n_cls):\n    pixels = mask.T.flatten()\n    encs = []\n    for i in range(1,n+1):\n        p = (pixels == i).astype(np.int8)\n        if p.sum() == 0: encs.append('')\n        else:\n            p = np.concatenate([[0], p, [0]])\n            runs = np.where(p[1:] != p[:-1])[0] + 1\n            runs[1::2] -= runs[::2]\n            encs.append(' '.join(str(x) for x in runs))\n    return encs","9f7718b5":"stats = ([0.388,0.390,0.394], [0.178,0.181,0.175])\n#check https:\/\/www.kaggle.com\/iafoss\/256x256-images-with-defects for stats\n\ndata = (SegmentationItemList.from_folder(TEST)\n        .split_by_idx([0])\n        .label_from_func(lambda x : str(x), classes=[0,1,2,3,4])\n        .add_test(Path(TEST).ls(), label=None)\n        .databunch(path=Path('.'), bs=bs)\n        .normalize(stats))","d02e84aa":"rles,ids_test = [],[]\nlearns = []\nfor fold in range(nfolds):\n    learn = unet_learner(data, models.resnet34, pretrained=False)\n    learn.model.load_state_dict(torch.load(Path(BASE)\/f'models\/fold{fold}.pth')['model'])\n    learns.append(learn)\n\nwith zipfile.ZipFile('pred.zip', 'w') as archive_out:\n    def to_mask(yp, y, id):\n        name = id + '.png'\n        save_img(yp[:,:,1:],name,archive_out)\n        yp = np.argmax(yp, axis=-1)\n        for i in range(n_cls):\n            idxs = yp == i+1\n            if idxs.sum() < noise_th: yp[idxs] = 0\n        encs = mask2enc(yp)\n        for i, enc in enumerate(encs):\n            ids_test.append(id + '.jpg_' + str(i+1))\n            rles.append(enc)\n    \n    model_pred(learns,to_mask,DatasetType.Test)\n    \nsub_df = pd.DataFrame({'ImageId_ClassId': ids_test, 'EncodedPixels': rles})\nsub_df.sort_values(by='ImageId_ClassId').to_csv('submission.csv', index=False)","028a8c2b":"The function below generates predictions with 3 fold TTA (horizontal flip, vertical flip, and both). The default fast.ai implementation is too memory hungry since predictions for all images and TTA folds are kept in memory. So it becomes hardly possible to generate a prediction for a reasonable number of high resolution images. To save memory, predictions for all folds are generated image by image and then converted into masks without keeping in the memory.","a037f2c4":"## It's fork from: https:\/\/www.kaggle.com\/iafoss\/severstal-fast-ai-256x256-crops-sub","2eb77d39":"# Overview\nThis kernel generates submission for models trained in [Severstal: fast.ai 256x256 crops]( https:\/\/www.kaggle.com\/iafoss\/severstal-fast-ai-256x256-crops) kernel. Please check that kernel for model and training details.","480410bf":"It's interesting that if the normalization parameters (stats) are slightly changed compared to the calculated ones, the score changes so much. In this example, from 0.89051 to 0.89318","01410b1c":"Functions for conversion masks into encodings and encodings into masks"}}