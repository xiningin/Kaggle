{"cell_type":{"774ba8f6":"code","76407efc":"code","86602d2c":"code","cce88347":"code","67244cbb":"code","4c2be363":"code","4641bc43":"code","2f242e88":"code","24ebebe8":"code","f3a00aca":"code","b7a2f98e":"code","c579387e":"code","3d383506":"code","3886d39d":"markdown","5b32c278":"markdown","d8e2be0e":"markdown","befbbc6e":"markdown","a2e0bd0b":"markdown","2b1e2d9e":"markdown","f9c25d76":"markdown","67ca93e4":"markdown","0d1bd20b":"markdown"},"source":{"774ba8f6":"import pandas as pd\n\nnames = pd.read_csv('\/kaggle\/input\/defi-ia-insa-toulouse\/categories_string.csv')['0'].to_dict()\njobs = pd.read_csv('\/kaggle\/input\/defi-ia-insa-toulouse\/train_label.csv', index_col='Id')['Category']\njobs = jobs.map(names)\njobs = jobs.rename('job')\njobs.head()","76407efc":"genders = pd.read_json('\/kaggle\/input\/defi-ia-insa-toulouse\/train.json').set_index('Id')['gender']\ngenders.head()","86602d2c":"people = pd.concat((jobs, genders), axis='columns')\npeople.head()","cce88347":"counts = people.groupby(['job', 'gender']).size().unstack('gender')\ncounts","67244cbb":"counts['disparate_impact'] = counts[['M', 'F']].max(axis='columns') \/ counts[['M', 'F']].min(axis='columns')\ncounts.sort_values('disparate_impact', ascending=False)","4c2be363":"counts['disparate_impact'].mean()","4641bc43":"def macro_disparate_impact(people):\n    counts = people.groupby(['job', 'gender']).size().unstack('gender')\n    counts['disparate_impact'] = counts[['M', 'F']].max(axis='columns') \/ counts[['M', 'F']].min(axis='columns')\n    return counts['disparate_impact'].mean()\n\npeople.head()","2f242e88":"macro_disparate_impact(people)","24ebebe8":"from sklearn import model_selection\n\ndescriptions = pd.read_json('\/kaggle\/input\/defi-ia-insa-toulouse\/train.json').set_index('Id')['description']\n\nX_train, X_test, y_train, y_test, gender_train, gender_test = model_selection.train_test_split(\n    descriptions,\n    jobs,\n    genders,\n    test_size=.5,\n    random_state=42\n)","f3a00aca":"from sklearn import feature_extraction\nfrom sklearn import linear_model\nfrom sklearn import pipeline\nfrom sklearn import preprocessing\n\nmodel = pipeline.make_pipeline(\n    feature_extraction.text.TfidfVectorizer(),\n    preprocessing.Normalizer(),\n    linear_model.LogisticRegression(multi_class='multinomial')\n)\n\nmodel = model.fit(X_train, y_train)","b7a2f98e":"y_pred = model.predict(X_test)\ny_pred = pd.Series(y_pred, name='job', index=X_test.index)\ny_pred.head()","c579387e":"test_people = pd.concat((y_pred, gender_test), axis='columns')\ntest_people","3d383506":"macro_disparate_impact(test_people)","3886d39d":"# Fairness metric","5b32c278":"Let's write a function to do all of this in one step.","d8e2be0e":"The model has worsened the fairness metric! The goal of this competition is to develop a model that lowers the fairness metric. The minimum attainable value is 1. Good luck!","befbbc6e":"Now let's compute the disparate impact for each job.","a2e0bd0b":"In this notebook, I'll define the fairness metric that will be used to evaluate the top 10 submissions on the private leaderboard.\n\nLet's start by loading the job labels as well as the genders.","2b1e2d9e":"Now we can obtain the macro disparate impact by simply computing the average of the `disparate_impact` column.","f9c25d76":"The fairness metric is going to be what I call the \"macro disparate impact\". Essentially, we will look at the individual the disparate impact of each job with respect to both genders, and then compute the non-weighted average of these disparate impacts. Let's first look at the gender distribution for each job.","67ca93e4":"We'll build a simple TF-IDF extractor followed by a multinomial classifier.","0d1bd20b":"The obtained value is the macro disparate impact for the labels in the training data. What if we want to evaluate the fairness of a model? Let's do that. \n\nWe'll split the training data in two. We'll train a model on the first half of the training data and test on the remaining half."}}