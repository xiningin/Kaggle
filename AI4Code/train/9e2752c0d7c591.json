{"cell_type":{"3ca93608":"code","87dcddd7":"code","e1ba3a41":"code","9b2291bd":"code","dbbb2534":"code","e6e2dc09":"code","70c1d4c0":"code","66fd1982":"code","31b13122":"code","91671b80":"code","f1da0d1d":"code","fe3c64bf":"code","e97bf3be":"code","8959bdb0":"code","5e04ff2c":"code","9d0c0ec2":"code","069930a6":"code","b97c1fec":"code","a1fe9ca7":"code","3213b790":"code","d7956634":"code","c7544f4f":"code","451fd15d":"code","26bfe9c3":"code","e17fa3b5":"code","ebb60109":"code","a4dc6e3f":"code","8f93e59e":"code","de9370b2":"code","da1770cd":"code","28cbf0db":"code","9b18a167":"code","029e2b48":"code","380f8c4e":"code","29f46b68":"code","f7977a46":"code","f384ad78":"code","04f7a100":"code","96d8012c":"code","76bab42a":"code","1db8abab":"code","8251b9b3":"code","93376ddb":"code","79221ad9":"code","327d4686":"code","444e34ff":"code","fc88cb83":"markdown","c7640140":"markdown","3eb6d55c":"markdown","d03716ce":"markdown","bd9c0df2":"markdown","4e042eae":"markdown","e727ffaf":"markdown","23b82834":"markdown","64b51676":"markdown","b7868c08":"markdown","1526fe28":"markdown"},"source":{"3ca93608":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n \nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nimport re\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","87dcddd7":"# The data start divided in two parts, train and test\n# The test data is what we want predict, but to do this we have to use the train data\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","e1ba3a41":"# Data Length (Rows, Columns)\ntrain.shape, test.shape","9b2291bd":"data = pd.concat([train, test], axis=0) # Concatenate the data ","dbbb2534":"data.head()","e6e2dc09":"data.isnull().sum()","70c1d4c0":"# Change all the strings to small letters \n# That makes easier to treat the data\ndata['text'] = data['text'].str.lower()\ndata['keyword'] = data['keyword'].str.lower()\ndata['location'] = data['location'].str.lower()","66fd1982":"# Setting the index of the columns iqual the row id\ndata.set_index('id', inplace=True)","31b13122":"data['text'] = data['text'].apply(lambda x: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)|^rt|http.+?\", \"\",x))","91671b80":"def remove_words(data, col):\n    stop = stopwords.words('english')\n    list_of_lists = data[col].str.split()\n    for idx, _ in data.iterrows():\n        data[col].at[idx] = [word for word in list_of_lists[idx] \\\n                             if word not in stop]","f1da0d1d":"remove_words(data,'text')","fe3c64bf":"def get_word_variation(data, col):\n    lemmatizer = WordNetLemmatizer()\n    for idx, _ in data.iterrows():\n        data[col].at[idx] = [lemmatizer.lemmatize(palavra,'v') \\\n                             for palavra in data[col][idx]]","e97bf3be":"get_word_variation(data, 'text')","8959bdb0":"data.loc[data['keyword'].notnull()].head(10)","5e04ff2c":"uniq_keyword = list(data['keyword'].unique())\nuniq_location = list(data['location'].unique())","9d0c0ec2":"for i in range(len(data)):\n    if data['keyword'].isnull()[i]:\n        for n in data['text'][i]:\n            if n in uniq_keyword:\n                data['keyword'][i] = n","069930a6":"data.isnull().sum()","b97c1fec":"for i in range(len(data)):\n    if data['location'].isnull()[i]:\n        for n in data['text'][i]:\n            if n in uniq_location:\n                data['location'][i] = n","a1fe9ca7":"data.isnull().sum()","3213b790":"data['location'].unique()","d7956634":"data","c7544f4f":"data['text'] = data['text'].apply(lambda x: ' '.join(x))","451fd15d":"data.isnull().sum()","26bfe9c3":"data.loc[data['keyword'].isnull()]","e17fa3b5":"data.columns","ebb60109":"data.head()","a4dc6e3f":"data['keyword'] = data['keyword'].fillna(\"None\")\ndata['location'] = data['location'].fillna(\"None\")\n#data['words'] = data['words'].fillna(\"None\")\n\n\n#data.drop(['keyword', 'location', 'words'], axis=1, inplace=True)","8f93e59e":"data.head()","de9370b2":"train = data.loc[data['target'].notnull()]\ntrain","da1770cd":"test = data.loc[data['target'].isnull()]\ntest.drop('target', axis=1, inplace=True)\ntest","28cbf0db":"train.shape, test.shape","9b18a167":"X = train['text']\ny = train['target']","029e2b48":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=13)","380f8c4e":"X_train.shape, y_train.shape","29f46b68":"X_train","f7977a46":"y_train","f384ad78":"sgd = Pipeline([\n    ('countVector', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('modelo', SGDClassifier())\n])","04f7a100":"sgd.fit(X_train, y_train)","96d8012c":"sgd_pred = sgd.predict(X_test)","76bab42a":"sgd_score = f1_score(y_test, sgd_pred)","1db8abab":"sgd_score","8251b9b3":"pred = sgd.predict(test['text'])","93376ddb":"test","79221ad9":"submission = pd.DataFrame({'id': test.index, 'target': pred})","327d4686":"submission['target'] = submission['target'].astype('int')","444e34ff":"submission.to_csv('submission.csv', index=False)","fc88cb83":"## Data cleaning","c7640140":"Visualizing this data you can see some location and keywords on the text. So, lets do that with nan loaction and keywords.","3eb6d55c":"### Lemmatizer\nThis function removes from the data some words that harm the model\nExemple: is, he, has...","d03716ce":"# Natural Language Processing","bd9c0df2":"Now that is a important part on this type of prediction.\nHere we use a lambda function to take out some special characters ","4e042eae":"### STOPWORD\nThis function removes from the data some words that harm the model\nExemple: is, he, has...\n","e727ffaf":"On this notebook i show the methods what i used to get 0.79252 score.\n\n**If you liked this notebook or found something useful in it, please give it a upvote!**\n\n**If you have some ideas to improve the notebook, please, tell me on the coments.**\n\n","23b82834":"## **Imports**","64b51676":"Here we will make a data cleaning, On that case, this means verify the missing values and treat the tweet text. ","b7868c08":"Here we see all the missing values. \nSo let's modify them(except the target for now).","1526fe28":"Here, we will work with 'Real or Not? NLP with Disaster Tweets' data base using NLP. This data have keywords, location, target and text of some tweets. The challenge is predict if some of the 'test' tweets talks or not about disasters."}}