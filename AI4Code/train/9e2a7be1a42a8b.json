{"cell_type":{"f5b3c717":"code","b60b4871":"code","38521b7f":"code","e676fc03":"code","181413fe":"code","fb48c563":"code","549e48dd":"code","c102199b":"code","2f478fce":"code","10a11768":"code","9da9c1eb":"code","abdb6001":"code","d5552862":"code","5a4d228d":"code","16816a91":"code","9d0495d4":"code","acd7c4a7":"code","a59bfc68":"code","06e413b9":"code","edbd8d46":"code","891cffc1":"code","70b1d05d":"code","047de7c8":"code","37b9d504":"code","e268973e":"code","c78da0a0":"code","1232a555":"code","c2cb3e56":"code","60ff14cb":"code","06db839f":"code","a3293453":"code","a899919c":"code","6127c16d":"markdown","9b5ebff0":"markdown","fdef4ee2":"markdown","21833286":"markdown","a4d622e0":"markdown","e528761b":"markdown","4853f15f":"markdown","a60bd1da":"markdown","fbec81c7":"markdown","3b596539":"markdown","581057bc":"markdown","b20d01d4":"markdown","edf7eefa":"markdown","6363f3a6":"markdown","02148b2f":"markdown","0a1aa08a":"markdown","56e0e76e":"markdown","accbaa27":"markdown","b9bf4744":"markdown","dc90102e":"markdown","123da8d7":"markdown","d008cca8":"markdown","ea9a81dc":"markdown","1ab3b9ab":"markdown","61f5df4e":"markdown","f6412bf3":"markdown","dcf198d1":"markdown","a7037686":"markdown","472f2222":"markdown","40b128b2":"markdown","4e3737a1":"markdown","a3c63813":"markdown"},"source":{"f5b3c717":"#Imports necessarios\nimport pandas as pd\nimport sklearn\nimport seaborn as sns\nimport plotly\nimport geopy\nimport numpy as np","b60b4871":"#Leitura dos dados\ntrain_data = pd.read_csv(\"..\/input\/atividade-regressao-PMR3508\/train.csv\", index_col=['Id'], na_values=\"?\")\ntrain_data.head()","38521b7f":"train_data.info()","e676fc03":"#Criando novas features\ntrain_data[\"rooms_per_household\"] = train_data[\"total_rooms\"]\/train_data[\"households\"]\ntrain_data[\"bedroom_per_room\"] = train_data[\"total_bedrooms\"]\/train_data[\"total_rooms\"]","181413fe":"train_data.hist(bins = 50, figsize=(20,20))","fb48c563":"from plotly import express as px\ncali_coast = px.scatter_mapbox(train_data, lat=\"latitude\",\n                               lon=\"longitude\", color=\"median_house_value\",\n                               title = \"Valor m\u00e9dio de resid\u00eancias na Calif\u00f3rnia\",\n                               size_max = 15, color_continuous_scale=px.colors.sequential.Viridis,\n                               mapbox_style=\"carto-positron\", zoom = 3.8)\ncali_coast.show()","549e48dd":"#Coordenadas de cidades grandes e do vale do sil\u00edcio\ncoordenadasCidades = {'Los Angeles':    {\"latitude\": 34.0194,\n                                         \"longitude\": -118.411},\n                      'San Diego':      {\"latitude\": 32.8153,\n                                         \"longitude\": -117.135},\n                      'San Franscisco': {\"latitude\": 37.3355,\n                                         \"longitude\": -121.893},\n                      'San Jose':       {\"latitude\": 37.7272,\n                                         \"longitude\": -123.032}}\nvaleDoSilicio = (37.387474, -122.057543)","c102199b":"#Dist\u00e2ncia at\u00e9 cidades grandes\nfrom geopy.distance import geodesic\ndef distanciaCidadeGrande(data):\n    dist = []\n    loc1 = (data[\"latitude\"], data[\"longitude\"])\n    for cidade in coordenadasCidades.values():\n        loc2 = (cidade[\"latitude\"], cidade[\"longitude\"])\n        dist.append(geodesic(loc1, loc2).miles)\n    return(min(dist))","2f478fce":"#Dist\u00e2ncia at\u00e9 o vale do sil\u00edcio\ndef distanciaValeSilicio(data):\n    loc1 = (data[\"latitude\"], data[\"longitude\"])\n    dist = geodesic(loc1, valeDoSilicio).miles\n    return dist","10a11768":"train_data[\"siliconValley_distance\"] = pd.concat([train_data[\"latitude\"], train_data[\"longitude\"]], axis=1).apply(distanciaValeSilicio, axis=1)\ntrain_data[\"bigCity_distance\"] = pd.concat([train_data[\"latitude\"], train_data[\"longitude\"]], axis=1).apply(distanciaCidadeGrande, axis=1)","9da9c1eb":"train_data.head()","abdb6001":"#Heatmap\nsns.set(rc={'figure.figsize':(10,10)})\n\n# a fun\u00e7\u00e3o triu() formata a matriz de correla\u00e7\u00e3o, deixando-a sem a diagonal principal e os valores repetidos acima dela\ntriangular = np.triu(np.ones_like(train_data.corr(), dtype=np.bool))\n\n\nsns.heatmap(train_data.corr(), mask = triangular, square=True, annot=True, vmin=-1, vmax=1, cmap=\"YlGnBu\")","d5552862":"train_data.corr()['median_house_value'].sort_values()","5a4d228d":"#Vamos dropar population, siliconValley_distance\ntrain_data = train_data.drop([\"population\",\"siliconValley_distance\"], axis = 1)","16816a91":"#Pr\u00e9 processamento\n#Removendo dados duplicados\ntrain_data.drop_duplicates(keep='first', inplace=True)\n\nY = train_data.pop('median_house_value')\nX = train_data\n\n","9d0495d4":"from sklearn.preprocessing import StandardScaler\n\nX = StandardScaler().fit_transform(X)","acd7c4a7":"#separacao dos dados para estimar um score de cada algoritmo\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,Y, random_state=0)","a59bfc68":"#KNN\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.neighbors import KNeighborsRegressor\nks = [3, 5, 8, 10,11, 12, 15, 20,25, 30,40, 50]\nscores = []\nfor k in ks:  \n    knr = KNeighborsRegressor(n_neighbors = k).fit(X_train, y_train)\n    predicted = knr.predict(X_test)\n    score = r2_score(y_test, predicted)\n    scores.append(score)\n    print(\"score = \",score)\n    print(\"K = \", k)\n\nmelhor_k = max(scores)\nmelhor_hip = np.argmax(scores)\n\nprint()\nprint(\"Melhor acur\u00e1cia = \", melhor_k)\nprint(\"Melhor K = \",ks[melhor_hip])","06e413b9":"#Regress\u00e3o linear\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression().fit(X_train, y_train)\npredicted = lr.predict(X_test)\nr2_score(y_test, predicted)","edbd8d46":"#Regress\u00e3o log\u00edstica\nfrom sklearn.linear_model import LogisticRegression\nlogr = LinearRegression().fit(X_train, y_train)\npredicted = logr.predict(X_test)\nr2_score(y_test, predicted)","891cffc1":"#Lasso\nfrom sklearn.linear_model import Lasso\nlasso = Lasso().fit(X_train, y_train)\npredicted = lasso.predict(X_test)\nr2_score(y_test, predicted)","70b1d05d":"#Ridge\nfrom sklearn.linear_model import Ridge\nridge = Ridge().fit(X_train, y_train)\npredicted = ridge.predict(X_test)\nr2_score(y_test, predicted)","047de7c8":"from sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators = 100, random_state=42).fit(X_train, y_train)\npredicted = rfr.predict(X_test)\nr2_score(y_test, predicted)","37b9d504":"#Se\u00e7\u00e3o para os testes\ntest_data = pd.read_csv('..\/input\/atividade-regressao-PMR3508\/test.csv', index_col = ['Id'])","e268973e":"test_data[\"rooms_per_household\"] = test_data[\"total_rooms\"]\/test_data[\"households\"]\ntest_data[\"bedroom_per_room\"] = test_data[\"total_bedrooms\"]\/test_data[\"total_rooms\"]","c78da0a0":"test_data = test_data.drop([\"population\"], axis = 1)\n","1232a555":"test_data[\"bigCity_distance\"] = pd.concat([test_data[\"latitude\"], test_data[\"longitude\"]], axis=1).apply(distanciaCidadeGrande, axis=1)\ntest_data.head()","c2cb3e56":"X_test = test_data","60ff14cb":"#Normaliza\u00e7\u00e3o dos dados de teste\nX_test = StandardScaler().fit_transform(X_test)","06db839f":"predicted = rfr.predict(X_test)\npredicted","a3293453":"#Submiss\u00e3o\nsub = pd.DataFrame()\nsub[0] = test_data.index\nsub[1] = predicted\nsub.columns = ['Id', 'median_house_value']\nsub.head()","a899919c":"sub.to_csv('submission.csv', index=False)","6127c16d":"Por \u00faltimo, vamos separar o nosso conjunto de treino em subconjuntos treino e teste, para assim conseguirmos fazer pequenas an\u00e1lises sobre a acur\u00e1cia que cada algoritmo utilizado ir\u00e1 atingir.","9b5ebff0":"### 2.3) Leitura do histograma geral\nVamos agora gerar uma vis\u00e3o geral, contendo os histogramas de cada vari\u00e1vel de nosso conjunto de dados. Sendo assim, conseguimos ter uma breve no\u00e7\u00e3o da distribui\u00e7\u00e3o dos valores de cada atributo.","fdef4ee2":"### 4.4) Lasso","21833286":"# California Housing\n### Lucas Domingues - PMR3508 - 117","a4d622e0":"### 2.2) Cria\u00e7\u00e3o de novas features\nA partir das vari\u00e1veis apresentadas acima, vamos criar novas features para podermos analisar outros aspectos da nossa base de dados. A princ\u00edpio vamos criar 2 novas features, sendo tais:\n* rooms per household: que indica o n\u00famero de c\u00f4modos por cada habita\u00e7\u00e3o\n* bedroom per room: representa o n\u00famero de quartos sobre o total de c\u00f4modos","e528761b":"Vamos dropar a coluna de \"population\"","4853f15f":"### 4.6) Random Forest","a60bd1da":"Em seguida, devemos criar as mesmas features utilizadas para o conjunto de treino:","fbec81c7":"### 4.5) Ridge","3b596539":"## 1) Importa\u00e7\u00e3o das bibliotecas a serem utilizadas\n* Pandas: biblioteca para manipula\u00e7\u00e3o do data frame\n* sklearn: biblioteca de machine learning\n* seaborn: biblioteca para visualiza\u00e7\u00e3o dos dados\n* plotly: biblioteca gr\u00e1fica\n* geopy: biblioteca para an\u00e1lises geogr\u00e1ficas\n* numpy: biblioteca num\u00e9rica","581057bc":"Por \u00faltimo, vamos exportar como \".csv\" o nosso Data Frame de submiss\u00e3o","b20d01d4":"Defini\u00e7\u00e3o da fun\u00e7\u00e3o que estima dist\u00e2ncia at\u00e9 o vale do Sil\u00edcio:","edf7eefa":"Para partirmos para o nosso modelo, devemos normalizar o nosso conjunto de dados. Para isso vamos utilizar o objeto StandardScaler da biblioteca Sklearn:","6363f3a6":"### 2.4) An\u00e1lise geogr\u00e1fica\nCom base nas caracter\u00edsticas geogr\u00e1ficas de nossa base de dados, vamos partir para algumas an\u00e1lises cartogr\u00e1ficas. Primeiramente, vamos criar um gr\u00e1fico interativo para observar como a distribui\u00e7\u00e3o dos valores m\u00e9dios de resid\u00eancias na Calif\u00f3rnia se comporta em fun\u00e7\u00e3o de sua localiza\u00e7\u00e3o.","02148b2f":"Defini\u00e7\u00e3o da fun\u00e7\u00e3o que mede a dist\u00e2ncia at\u00e9 a cidade grande mais pr\u00f3xima, com base na sua localiza\u00e7\u00e3o:","0a1aa08a":"### 4.3) Regress\u00e3o Log\u00edstica","56e0e76e":"Vamos agora separar a caracter\u00edsica a ser observada como Y, e deixar as demais categorias como X:","accbaa27":"Finalmente, vamos gerar nossos dados de predi\u00e7\u00e3o de valor m\u00e9dio de pre\u00e7o de uma resid\u00eancia na Calif\u00f3rnia, com base nas caracter\u00edsticas fornecidas pela base de dados.","b9bf4744":"### 2.5) An\u00e1lise da correla\u00e7\u00e3o entre as vari\u00e1veis\nTendo em vista as novas features criadas, vamos agora validar se tais features apresentam uma certa influ\u00eancia sobre o valor das resid\u00eancias na Calif\u00f3rnia. Como primeiro passo, vamos gerar um Heatmap para ter uma vis\u00e3o geral do comportamento das vari\u00e1veis.","dc90102e":"## 3) Pr\u00e9 - processamento dos Dados\nComo citado anteriormente, vamos primeiro remover as vari\u00e1veis que n\u00e3o possuem influ\u00eancia significativa sobre o par\u00e2metro a ser observado, al\u00e9m de posteriormente remover poss\u00edveis dados duplicados:","123da8d7":"## 5) Teste\nVamos agora validar o nosso modelo utilizando o conjunto de dados de teste.\nPara isso, vamos primeiramente importar nossos dados de teste:","d008cca8":"Por \u00faltimo, concatenemos nosso conjunto de dados. Note que a feature \"silicon Valley Distance\" n\u00e3o foi gerada para o nosso conjunto de teste, visto que n\u00e3o influencia significativamente no nosso modelo, sendo facilmente descartada.","ea9a81dc":"## 6) Submiss\u00e3o\nVamos gerar nosso Data Frame contendo nossos valores previstos:","1ab3b9ab":"### 4.2) Regress\u00e3o Linear","61f5df4e":"Tendo em vista todos os algoritmos testados, \u00e9 poss\u00edvel observar que o m\u00e9todo Random Forest atingiu maior acur\u00e1cia, com cerca de 81% de precis\u00e3o. Portanto, vamos utiliz\u00e1-lo para o nosso conjunto de teste.","f6412bf3":"A partir do gr\u00e1fico acima, conseguimos levantar uma hip\u00f3tese de que a distribui\u00e7\u00e3o dos valores das resid\u00eancias sofre uma grande influ\u00eancia sobre a sua localiza\u00e7\u00e3o, mais especificamente, sobre a dist\u00e2ncia da resid\u00eancia at\u00e9 uma cidade grande ou at\u00e9 um p\u00f3lo urbano. Sendo assim, vamos criar mais 2 features para validar a nossa hip\u00f3tese.\n* big city distance: dist\u00e2ncia da resid\u00eancia at\u00e9 a cidade grande mais pr\u00f3xima\n* silicon Valley distance: dist\u00e2ncia da resid\u00eancia at\u00e9 o Vale do Sil\u00edcio","dcf198d1":"## 4) An\u00e1lise dos Algoritmos\nVamos ent\u00e3o partir para a an\u00e1lise individual de cada algoritmo a ser testado:\n### 4.1) KNN","a7037686":"Sendo assim, conseguimos observar que as vari\u00e1veis \"Population\" e \"silicon Valley Distance\" n\u00e3o tem muita influ\u00eancia sobre, sendo assim pass\u00edveis a serem descartadas.","472f2222":"Concatena\u00e7\u00e3o dos dados:","40b128b2":"Defini\u00e7\u00e3o das coordenadas das cidades grandes e do vale do Sil\u00edcio:","4e3737a1":"Agora, vamos utilizar um recurso da biblioteca Pandas que nos ajuda a estimar a correla\u00e7\u00e3o num\u00e9rica que as vari\u00e1veis t\u00eam sobre o par\u00e2metro observado:","a3c63813":"## 2) An\u00e1lise dos dados\n### 2.1) Importa\u00e7\u00e3o dos dados de treino\nPrimeiramente vamos importar os dados de treino utilizando a biblioteca Pandas."}}