{"cell_type":{"ae1ea80b":"code","1692ba34":"code","37bfe0c8":"code","382c8dda":"code","bdd18f83":"code","b770ca58":"code","7eee9662":"code","3f9aad54":"code","e4e6d4d0":"code","8c2d2970":"markdown","ff425322":"markdown","783caee2":"markdown"},"source":{"ae1ea80b":"#importing libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics","1692ba34":"#fetching data\nelec_cons = pd.read_csv(\"..\/input\/us-yearly-electricity-consumption\/total-electricity-consumption-us.csv\",  sep = ',', header= 0 )\nelec_cons.head()","37bfe0c8":"# number of observations: 51\nelec_cons.shape","382c8dda":"# checking NA\n# there are no missing values in the dataset\nelec_cons.isnull().values.any()","bdd18f83":"size = len(elec_cons.index)\nindex = range(0, size, 5)\n\ntrain = elec_cons[~elec_cons.index.isin(index)]\ntest = elec_cons[elec_cons.index.isin(index)]\n","b770ca58":"print(len(train))\nprint(len(test))","7eee9662":"# converting X to a two dimensional array, as required by the learning algorithm\nX_train = train.Year.values.reshape(-1,1) #Making X two dimensional\ny_train = train.Consumption\n\nX_test = test.Year.values.reshape(-1,1) #Making X two dimensional\ny_test = test.Consumption","3f9aad54":"# Doing a polynomial regression: Comparing linear, quadratic and cubic fits\n# Pipeline helps you associate two models or objects to be built sequentially with each other, \n# in this case, the objects are PolynomialFeatures() and LinearRegression()\n\nr2_train = []\nr2_test = []\ndegrees = [1, 2, 3]\n\nfor degree in degrees:\n    pipeline = Pipeline([('poly_features', PolynomialFeatures(degree=degree)),\n                     ('model', LinearRegression())])\n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n    r2_test.append(metrics.r2_score(y_test, y_pred))\n    \n    # training performance\n    y_pred_train = pipeline.predict(X_train)\n    r2_train.append(metrics.r2_score(y_train, y_pred_train))\n    \n# plot predictions and actual values against year\n    fig, ax = plt.subplots()\n    ax.set_xlabel(\"Year\")                                \n    ax.set_ylabel(\"Power consumption\")\n    ax.set_title(\"Degree= \" + str(degree))\n    \n    # train data in blue\n    ax.scatter(X_train, y_train)\n    ax.plot(X_train, y_pred_train)\n    \n    # test data in orange\n    ax.scatter(X_test, y_test)\n    ax.plot(X_test, y_pred)\n    plt.show()    \n    \n# plot errors vs y\n    fig, ax = plt.subplots()\n    ax.set_xlabel(\"y_test\")                                \n    ax.set_ylabel(\"error\")\n    ax.set_title(\"Degree= \" + str(degree))\n    \n    ax.scatter(y_test,y_test-y_pred)\n    ax.plot(y_test,y_test-y_test)\n    plt.show()\n    ","e4e6d4d0":"# respective train and test r-squared scores of predictions\nprint(degrees)\nprint(r2_train)\nprint(r2_test)","8c2d2970":"It is a common misconception of the neophytes that polynomial regression always overfits, but given a right approach it is a powerful tool to in your arsenal.","ff425322":"Looking at the distribution of errors for degree 1 we can say that the errors are following a certain pattern, implying there is a trend in the data that liner model could not capture. As we increase the degree of the regression we see that the errors decrease in magnitude and go random in distribution implying a positive result for increasing the degree of regression. Further increase in degree will result in overfitting as we already captured the trends in the data, which is why we have the errors so random.","783caee2":"# When to use Polynomial Regression?\n\nIn this notebook, we will compare linear regression model(degree=1) to polynomial regression models(degree=2,3) on the **electricity consumption** dataset, to understand when to use polynomial regression. The dataset contains two variables - year and electricity consumption. We will also decide untill what extent we can increase the degree of polynomial regression without overfitting."}}