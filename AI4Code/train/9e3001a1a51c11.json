{"cell_type":{"62ac173c":"code","1f2e6ae9":"code","640a0e7f":"code","43367da2":"code","36395513":"code","82b5645f":"code","ef638730":"code","d440244a":"code","1f4c692d":"code","1b63e2d5":"code","f6a0d3fb":"code","c22077e5":"code","aabd4acf":"code","17d6f4e9":"code","045add7b":"code","f8038b4f":"code","e49fb181":"code","d577542d":"code","fec240c6":"code","e2f109c7":"code","e1c3d73e":"code","7c76bf6e":"code","5c7efc2c":"code","06380b97":"code","d8bc80c9":"code","9c185afb":"code","65d83a2a":"code","97659bd1":"code","befd7b29":"code","318517ff":"code","2a1ee666":"code","0a9c1a4d":"code","e26d7792":"code","5d768f67":"code","e0cfd47a":"code","f55c28f3":"code","b0c35420":"code","eaa64eb6":"markdown","13f861f0":"markdown","5cd2659c":"markdown","6d3ff2e6":"markdown"},"source":{"62ac173c":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai.vision import *\nfrom fastai.callbacks.hooks import *\nfrom fastai.utils.mem import *","1f2e6ae9":"# we need to make pre-trained models available to this kernel, without internet access.\n# output from creating the learner tells us that a pre-trained model is needed;\n#   Downloading: \"https:\/\/download.pytorch.org\/models\/resnet34-333f7ec4.pth\" to \/tmp\/.cache\/torch\/checkpoints\/resnet34-333f7ec4.pth\n# to make this work without internet;\n# 1) manually download the model\n# 2) use \"+ Add Data\" to make the model available\n# 3) make the temp folder (that pytorch wants to use) if it doesn't exist already\nPath('\/tmp\/.cache\/torch\/checkpoints').mkdir(parents=True, exist_ok=True)\n# 4) copy the model from input to the temp folder\n!cp \/kaggle\/input\/pytorch-models\/resnet34-333f7ec4.pth \/tmp\/.cache\/torch\/checkpoints\/resnet34-333f7ec4.pth\n# 5) then make sure we set path='\/kaggle\/working' when creating the learner - otherwise it'll use the dir of the model (which is read-only)\noutput_path = '\/kaggle\/working'","640a0e7f":"# get the Path of the mask for a given image\nget_y_fn = lambda x: x.parents[0] \/ (x.stem + '_mask' + x.suffix)","43367da2":"data_path = Path('\/kaggle\/input\/lgg-mri-segmentation\/kaggle_3m\/')\nsmall_data_path = data_path\/'TCGA_HT_7680_19970202'\n# small_data_path.ls()","36395513":"temp_img_file = small_data_path\/'TCGA_HT_7680_19970202_6.tif'\ntemp_mask_file = get_y_fn(temp_img_file)\nfor f in [temp_img_file, temp_mask_file]:\n    print('showing', f)\n    if '_mask.tif' in f.name:\n        mask = open_mask(f)\n        print(mask.shape)\n        mask.show()\n    else:\n        temp_img = open_image(f)\n        print(temp_img.shape)\n        temp_img.show()","82b5645f":"# div=True changes mask.data.unique() from [0,255] to [0,1] (by dividing pixel values by 255)\nmask = open_mask(get_y_fn(temp_img_file), div=True)\nmask.show(figsize=(5,5), alpha=1)","ef638730":"src_size = np.array(mask.shape[1:])\nsrc_size, mask.data","d440244a":"# use a fixed set of images for validation - partly to make this repeatable - but ...\n# I think it makes sense to validate against images for patients that have not been seen during training\nvalidation_folders = [\n        'TCGA_HT_7694_19950404', 'TCGA_DU_5874_19950510', 'TCGA_DU_7013_19860523',\n        'TCGA_HT_8113_19930809', 'TCGA_DU_6399_19830416', 'TCGA_HT_7684_19950816',\n        'TCGA_CS_5395_19981004', 'TCGA_FG_6688_20020215', 'TCGA_DU_8165_19970205',\n        'TCGA_DU_7019_19940908', 'TCGA_HT_7855_19951020', 'TCGA_DU_A5TT_19980318',\n        'TCGA_DU_7300_19910814', 'TCGA_DU_5871_19941206', 'TCGA_DU_5855_19951217']","1f4c692d":"# v simple codes; n=nothing to see here, y=area of interest\ncodes = ['n', 'y']\n\nfree = gpu_mem_get_free_no_cache()\n# the max size of bs depends on the available GPU RAM\nbs=free\/\/500\nprint(f\"using bs={bs}, have {free}MB of GPU RAM free\")","1b63e2d5":"# we need to open the make files with the div=True option - which we can do with a custom label list \/ item list.\n# thanks to https:\/\/www.kaggle.com\/tanlikesmath\/ultrasound-nerve-segmentation-with-fastai\/data for showing how this can be done\nclass SegmentationLabelListWithDiv(SegmentationLabelList):\n    def open(self, fn): return open_mask(fn, div=True)\nclass SegmentationItemListWithDiv(SegmentationItemList):\n    _label_cls = SegmentationLabelListWithDiv","f6a0d3fb":"src = (SegmentationItemListWithDiv.from_folder(data_path, recurse=True)\n       .filter_by_func(lambda x: not x.name.endswith('_mask.tif'))\n       .split_by_valid_func(lambda x: x.parts[-2] in validation_folders)\n       .label_from_func(get_y_fn, classes=codes))","c22077e5":"# start by training on half size images\ndata = (src.transform(get_transforms(), size=src_size\/\/2, tfm_y=True)\n        .databunch(bs=bs)\n        .normalize(imagenet_stats))","aabd4acf":"data.train_ds","17d6f4e9":"data.valid_ds","045add7b":"data.show_batch(2, figsize=(10,10))","f8038b4f":"data.show_batch(2, figsize=(10,10), ds_type=DatasetType.Valid)","e49fb181":"wd=1e-2","d577542d":"learn = unet_learner(data, models.resnet34, wd=wd, metrics=dice, path=output_path)","fec240c6":"lr_find(learn)\nlearn.recorder.plot()","e2f109c7":"lr=3e-3","e1c3d73e":"learn.fit_one_cycle(10, slice(lr), pct_start=0.9)","7c76bf6e":"def save_and_show(name):\n    saved_to = learn.save(name, return_path=True)\n    print('Saved to', saved_to, 'Note: this will be lost unless we commit the kernel')\n    learn.load(name) # free memory etc\n    learn.show_results(rows=4, figsize=(32, 32))","5c7efc2c":"save_and_show('stage-1')","06380b97":"learn.unfreeze()\nlrs = slice(lr\/400,lr\/4)\nlearn.fit_one_cycle(8, lrs, pct_start=0.8)","d8bc80c9":"save_and_show('stage-2')","9c185afb":"data = (src.transform(get_transforms(), size=src_size, tfm_y=True)\n        .databunch(bs=bs\/\/2)\n        .normalize(imagenet_stats))","65d83a2a":"# Export the minimal state of the data bunch for inference \ndata.export('..\/..\/..\/working\/big-databunch-export.pkl')","97659bd1":"learn = unet_learner(data, models.resnet34, metrics=dice, wd=wd, path=output_path)\nlearn.load('stage-2')","befd7b29":"lr_find(learn)\nlearn.recorder.plot()","318517ff":"# reduce learning rate for training on full size images\nlr=1e-3","2a1ee666":"# we'll run 10 epochs before unfreeze - do just the 1st 3 so we can take a look at the results\nlearn.fit_one_cycle(3, slice(lr), pct_start=0.8)","0a9c1a4d":"save_and_show('stage-1-big')","e26d7792":"learn.fit_one_cycle(7, slice(lr), pct_start=0.8)","5d768f67":"save_and_show('stage-1-big') # overwrite the existing stage-1-big","e0cfd47a":"learn.unfreeze()\nlrs = slice(1e-6,lr\/10)\nlearn.fit_one_cycle(10, lrs)","f55c28f3":"save_and_show('stage-2-big')","b0c35420":"learn.export('stage-2-big-export.pkl')","eaa64eb6":"# Model","13f861f0":"# Dataseta","5cd2659c":"# Train on full size images","6d3ff2e6":"The mask files are single channel 256x256 images using pixel values 0 and 255.\n\nIf we're using grey scale; 0 is black, 255 is white.\n\nNormally, the open_mask div=True option means divide pixel values by 255 to convert the range of pixel values from 0-255 to 0-1.\n\nIn this case, dividing by 255 gives us just 2 classes of pixels; 0=nothing to see here, 1=area of interest - and we need these to correspond to the codes (AKA classes) defined above"}}