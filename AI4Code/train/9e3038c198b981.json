{"cell_type":{"490025b3":"code","6e3b4315":"code","98014ce4":"code","66e3186f":"code","1e25deef":"code","bdb1bea1":"code","3427e112":"markdown","072da170":"markdown","537b5b0c":"markdown","c38e8443":"markdown","c6766609":"markdown"},"source":{"490025b3":"import time \nimport lxml.etree as ET\nimport pandas as pd\n\nfile_path = '\/kaggle\/input\/french-reddit-discussion\/final_SPF_2.xml'\n","6e3b4315":"#Initializes the parser\nparser = ET.XMLParser(recover=True)\n#Parses the file\ntree = ET.parse(file_path, parser=parser)\nxroot = tree.getroot()\n","98014ce4":"\n#Prepares our final df\ndfcols = ['link_id', 'subreddit_id', 'uid',\"comment_id\",'score', 'parent_id', 'create_utc', 'text']\ndf_xml = pd.DataFrame(columns=dfcols)\n\n#Processes the file\nnb_of_text = 10**4 #If you want more or less conversations you can change it here. If you want everything just put len(xroot) \nstart = time.time()\ns = start\ni = 0\n#Iterates overs the topics\nfor node in xroot[0:nb_of_text]:\n    if (i+1) % 1000 == 0:\n        e = time.time()\n        print(str(i + 1) + ' iterations done in ' + str(round(e - s)) + \" sec.\")\n        s = time.time()\n    link_id = node.attrib.get('link_id')\n    subreddit_id = node.attrib.get('subreddit_id')\n    #Iterates over the posts into the conv\n    for j in range(len(node.getchildren())):\n        uid = node.getchildren()[j].get('uid')\n        comment_id = node.getchildren()[j].get('comment_id')\n        score = node.getchildren()[j].get('score')\n        parent_id = node.getchildren()[j].get('parent_id')\n        create_utc = node.getchildren()[j].get('create_utc')\n        text = node.getchildren()[j].text\n        df_xml = df_xml.append(pd.Series([link_id, subreddit_id,\n                                          uid,comment_id,score,parent_id,create_utc,text],\n                                         index=dfcols)\n                               ,ignore_index=True)\n    i+=1\n        \nend = time.time()\nprint(\"Finished. Done in : \" +  str(round(end - start)) + 'seconds')\n\n#df_xml.to_csv('Reddit_Conv_french.csv')","66e3186f":"final = pd.read_csv('\/kaggle\/input\/reddit-conv-french\/Reddit_Conv_final_french.csv',index_col = 0)\nfinal.head()","1e25deef":"print(final.columns)\nprint(final.shape)","bdb1bea1":"final.to_csv('Reddit_Conv_french.csv')","3427e112":"### Package import","072da170":"### Conversion ","537b5b0c":"## Conversion of the XML file to a Pandas Dataframe","c38e8443":"This is just a short Notebook to convert the xml into a pd Dataframe . Be aware : it can take a long time to run (the complete file took more than one day in my local machine, I tried to parralelize the treatment unsuccesfully). In this notebook I only use the 10 000 first conversations but you can extend it to the total number of conversations (which is arround 556 000).\n\n**Edit** : to save you some time <ins>I have put the complete csv file<\/ins> (that I treated on my personnal computer)  in the input file (reddit-conv-french\/Reddit_Conv_final_french.csv). You have just to download it and use pd.read_csv to open it.","c6766609":"### Overview of the complete csv file"}}