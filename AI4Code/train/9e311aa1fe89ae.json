{"cell_type":{"ed69569a":"code","f3624f7b":"code","97c770dd":"code","25a0c2a4":"code","0f724be4":"code","6d573d6b":"code","43a782ff":"code","8ed1d205":"code","06075f5c":"code","f2d4dbc7":"code","e8140c5d":"code","bd1315c4":"code","179b329e":"code","cd9a94b2":"code","c4825aa2":"code","bef918ae":"code","7f967686":"code","8c70a009":"code","6f4e1389":"code","d36684f1":"code","31cb4f29":"code","5d0ed49d":"code","ee7e7e48":"code","445923c2":"code","f80cf6e3":"code","a9de66fb":"code","8cfec2ae":"code","abb519ff":"markdown","ef9cceec":"markdown","038c5220":"markdown","c362ad86":"markdown","a261a91a":"markdown","0cf1986e":"markdown","a8ad8512":"markdown","8238788b":"markdown","24f5c852":"markdown","227eb813":"markdown","7679c902":"markdown","ec306981":"markdown","46b4b999":"markdown"},"source":{"ed69569a":"# import packages\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom pandas import read_csv\nfrom pandas import concat\nfrom pandas import DataFrame\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nimport scipy.stats","f3624f7b":"KaggleInput = \"\/kaggle\/input\/\"\nCGMTimeCSV = KaggleInput + \"continuous-blood-glucose-monitor-data\/CGMDatenumLunchPat1.csv\"\nCGMValueCSV = KaggleInput + \"continuous-blood-glucose-monitor-data\/CGMSeriesLunchPat1.csv\"","97c770dd":"# display data sample\nCGMDatenum = pd.read_csv(CGMTimeCSV)\nCGMSeries = pd.read_csv(CGMValueCSV)\n\nCGMDatenum.head()","25a0c2a4":"CGMSeries.head()","0f724be4":"# Convert TS datatype and reverse indexing for chronological order\nCGMDatenum = CGMDatenum.applymap(lambda i : pd.to_datetime(i - 719529, unit='D'))\nCGMDatenum = CGMDatenum.iloc[::-1]\nCGMDatenum = CGMDatenum.iloc[:, ::-1]\n\nCGMDatenum_updated = CGMDatenum.copy()\nCGMDatenum_updated.head()","6d573d6b":"# Reverse indexing for chronological order\n# Missing values- Linear interpolation\nCGMSeries = CGMSeries.iloc[::-1]\nCGMSeries = CGMSeries.iloc[:, ::-1]\n\nCGMSeries_updated = CGMSeries.copy()\nCGMSeries_updated.interpolate(method='linear', inplace=True)\n\nrow, col = CGMSeries_updated.shape\n\nCGMSeries_updated.head()","43a782ff":"# Feature Matrix\nNewFeatureMatrix = pd.DataFrame()","8ed1d205":"# Windowed velocity(non-overlapping)- 30 mins intervals\nvelocityDF = pd.DataFrame()\nfor i in range(0,26):\n     velocityDF['Vel_'+str(i)] = (CGMSeries_updated.iloc[:,i+5]-CGMSeries_updated.iloc[:,i])\nNewFeatureMatrix['Window_Velocity_Max']=velocityDF.max(axis = 1, skipna=True)\nNewFeatureMatrix.head()","06075f5c":"#Plotting\nplt.plot(NewFeatureMatrix['Window_Velocity_Max'],'r-')\nplt.ylabel('Window_Velocity_Max')\nplt.xlabel('Days')","f2d4dbc7":"#Plotting\nfig = plt.figure(figsize = (12,8))\nax = fig.add_subplot(1,1,1) \nax.set_ylabel('Mean')\nax.set_xlabel('Days')\nax.set_title('Windowed Means')\nax.plot(NewFeatureMatrix.iloc[:,1:7],'-')\nax.legend(('Mean_0', 'Mean_6', 'Mean_12','Mean_18','Mean_24','Mean_30'),loc='upper right')","e8140c5d":"# Windowed mean interval - 30 mins(non-overlapping)\nfor i in range(0,31,6):\n    NewFeatureMatrix['Mean_'+str(i)] = CGMSeries_updated.iloc[:,i:i+6].mean(axis = 1)\n    \nNewFeatureMatrix.head()","bd1315c4":"# FFT- Finding top 8 values for each row\ndef get_fft(row):\n    cgmFFTValues = abs(scipy.fftpack.fft(row))\n    cgmFFTValues.sort()\n    return np.flip(cgmFFTValues)[0:8]\n\nFFT = pd.DataFrame()\nFFT['FFT_Top2'] = CGMSeries_updated.apply(lambda row: get_fft(row), axis=1)\nFFT_updated = pd.DataFrame(FFT.FFT_Top2.tolist(), columns=['FFT_1', 'FFT_2', 'FFT_3', 'FFT_4', 'FFT_5', 'FFT_6', 'FFT_7', 'FFT_8'])\n\n#FFT_updated.head()\n\nNewFeatureMatrix = NewFeatureMatrix.join(FFT_updated)\n\nNewFeatureMatrix.head()","179b329e":"# Calculates entropy(from occurences of each value) of given series\ndef get_entropy(series):\n    series_counts = series.value_counts()\n    entropy = scipy.stats.entropy(series_counts)  \n    return entropy\n\nNewFeatureMatrix['Entropy'] = CGMSeries_updated.apply(lambda row: get_entropy(row), axis=1) \nNewFeatureMatrix.head()","cd9a94b2":"# Final feature matrix\nNewFeatureMatrix.head()","c4825aa2":"# PCA\nrows,cols = NewFeatureMatrix.shape\n\n# Standardizes feature matrix\nNewFeatureMatrix = StandardScaler().fit_transform(NewFeatureMatrix)\n\npca = PCA(n_components=5)\nprincipalComponents = pca.fit(NewFeatureMatrix)\nprint(principalComponents.components_) # Principal Components vs Original Features","bef918ae":"print(principalComponents.explained_variance_ratio_.cumsum())","7f967686":"principalComponentsTrans = pca.fit_transform(NewFeatureMatrix)\nPC_TimeSeries=pd.DataFrame(data=principalComponentsTrans,columns = ['principal component 1', 'principal component 2','principal component 3', 'principal component 4','principal component 5'])\nPC_TimeSeries.head()","8c70a009":"#plotting explained variance versus principle componenets\npcs = ['PC1','PC2','PC3','PC4','PC5']\nplt.bar(pcs,principalComponents.explained_variance_ratio_*100)\nplt.savefig('')","6f4e1389":" #plotting top 5 principle components against each time series\nax = PC_TimeSeries.plot.bar(y='principal component 1', rot=0)\nax = PC_TimeSeries.plot.bar(y='principal component 2', rot=0)\nax = PC_TimeSeries.plot.bar(y='principal component 3', rot=0)\nax = PC_TimeSeries.plot.bar(y='principal component 4', rot=0)\nax = PC_TimeSeries.plot.bar(y='principal component 5', rot=0)","d36684f1":"#plotting top 5 principle components against each time series\n","31cb4f29":"# plots & prove assumptions","5d0ed49d":"# patient-by-patient analysis","ee7e7e48":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","445923c2":"# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","f80cf6e3":"# classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}","a9de66fb":"from sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","8cfec2ae":"from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred = log_reg.predict(X_train)\n\n# Overfitting Case\nprint('---' * 45)\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\nprint('---' * 45)","abb519ff":"* ### RESULTS","ef9cceec":"Plots","038c5220":"* ### PCA","c362ad86":"* ### Feature 1 - CGM Velocity","a261a91a":"* ### Convert TS type & timeline to chronological order","0cf1986e":"## FEATURE EXTRACTION","a8ad8512":"* ### Feature 4 - Entropy","8238788b":"*  ### Packages","24f5c852":"* ### Load Data","227eb813":"* ### Feature 2 - Windowed Mean","7679c902":"# CSE 571(Data Mining) Fall 19 Project- PHASE 1\n## Project AIM- Given a TS, predict if a MEAL was taken or not\n## Phase AIM- Extract features from training data, pass through PCA & prove their importance\n### TASKS:\n#### a) Extract 4 (one for each student) different types of time series features from only the CGM data cell array and CGM timestamp cell array (10 points each) total 40\n#### b) For each time series explain why you chose such feature (5 points each) total 20\n#### c) Show values of each of the features and argue that your intuition in step b is validated or disproved? (5 points each ) total 20\n#### d) Create a feature matrix where each row is a collection of features from each time series. So if there are 75 time series and your feature length after concatenation of the 4 types of features is 17 then the feature matrix size will be 75 X 17 (10 points)\n#### e) Provide this feature matrix to PCA and derive the new feature matrix. Choose the top 5 features and plot them for each time series. (5 points)\n#### f) For each feature in the top 5 argue why it is chosen as a top five feature in PCA? (3 points each) total 15\n\n### STEPS:\n#### 1. Load data(CGM values & time-series)\n#### 2. Convert TS values and reverse data into chronological order\n#### 3. EDA & plots\n#### 4. Feature extraction- Windowed Mean, Maximum Windowed Velocity, FFT & Entropy\n#### 6. PCA\n#### 7. Prove results & plots","ec306981":"* ### Final Feature Matrix","46b4b999":"* ### Feature 3 - FFT"}}