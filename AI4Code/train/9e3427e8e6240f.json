{"cell_type":{"db500b04":"code","9df24543":"code","027e5a16":"code","75ae4f3a":"markdown"},"source":{"db500b04":"import os\nif 'kid' in os.getcwd():\n    HOME = '\/home\/kid\/covid\/data'\nelse:\n    HOME = '\/kaggle'\nGFWPATH = f'{HOME}\/input\/covid19-global-forecasting-week-1'\nimport datetime\nimport pandas as pd","9df24543":"def readGFWdata(file):\n    df = pd.read_csv(f'{GFWPATH}\/{file}.csv', parse_dates=['Date'])\n    df['Date'] = pd.to_datetime(df['Date']).dt.date\n    if 'Country_Region' in df.columns:\n        countryregion, provincestate = 'Country_Region', 'Province_State'\n    else: # for backward compatibility wtih Week 1's data\n        countryregion, provincestate = 'Country\/Region', 'Province\/State'\n    df.sort_values(by=[countryregion, provincestate, 'Date'], inplace=True)\n    df[provincestate].fillna('', inplace=True)\n    return df, countryregion, provincestate\ndf, countryregion, provincestate = readGFWdata('train')","027e5a16":"def catchNonCum(df):\n    dfDoctored = df.copy()\n    okay = True\n    for groupID, groupData in df.groupby([countryregion, provincestate]):\n        batchA = groupData[['Date', 'ConfirmedCases', 'Fatalities']]\n        batchB = batchA[1:].reset_index()\n        batchA = batchA[:-1].reset_index()\n        assert (batchA['Date'] == batchB['Date'] - datetime.timedelta(1)).all()\n        for colname in ['ConfirmedCases', 'Fatalities']:\n            checkAB = batchA[colname] > batchB[colname]\n            if checkAB.any():\n                okay = False\n                for idx in checkAB[checkAB].index:\n                    print(groupID[1], groupID[0], batchA.loc[idx, 'Date'].strftime(\"%Y-%m-%d\"), \n                                                  batchA.loc[idx, colname], colname)\n                    print(groupID[1], groupID[0], batchB.loc[idx, 'Date'].strftime(\"%Y-%m-%d\"), \n                                                  batchB.loc[idx, colname], colname)\n                    dfDoctored.loc[batchA['index'].loc[idx]+1, colname] = \\\n                    dfDoctored.loc[batchA['index'].loc[idx], colname]\n    return dfDoctored, okay\n\ndfDoctored, okay = catchNonCum(df)\nif okay:\n    print('All clear: no non-cumulative aberration found.')\nelse:\n    nround = 1\n    while not okay:\n        print('\\n*** round #', nround)\n        dfDoctored, okay = catchNonCum(dfDoctored)\n        nround += 1\n\nif 'Lat' in df.columns: # for backward compatibility wtih Week 1's data\n    for colname in ['Lat', 'Long']:\n        dfDoctored[colname] = dfDoctored[colname].map(lambda x: '{:.4f}'.format(x))\ndfDoctored.to_csv(f'{HOME}\/working\/trainDoctored.csv', index=False)","75ae4f3a":"This notebook is to check and iron out some inconsistencies report in:\nhttps:\/\/www.kaggle.com\/c\/covid19-global-forecasting-week-1\/discussion\/137500\n\n`ConfirmedCases` and `Fatalities` were supposed to be cumulative. But it's been observed not to be always the cases. At such occurrences the value from the previous day is pro-- and iterate over the next (if any) until there is no more day-to-day drop in `ConfirmedCases` and `Fatalities`. The doctored version of train.csv is then written as `trainDoctored.csv`."}}