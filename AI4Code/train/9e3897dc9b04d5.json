{"cell_type":{"2f90b6ca":"code","39a09c91":"code","9a9e909c":"code","47f19b21":"code","9a4a8096":"code","15f07076":"code","a10d2f72":"code","7509664b":"code","60d5e213":"code","dd1d81e7":"code","fa9badb6":"code","d235c12b":"code","92529540":"code","9d493c2c":"code","1200d1d4":"code","82f98e29":"code","db6a9c60":"code","b5ba6920":"code","ee7d180b":"code","3ede9ae0":"code","a9a8750e":"code","2a54ef30":"code","c25124be":"code","bcb21f40":"code","fa625d90":"code","204c926c":"code","6ac33f8f":"code","0c92df45":"code","eb052c42":"code","ff576fec":"code","bb2566fd":"code","ce1cee72":"code","f5a73913":"code","8c64c12d":"code","372cfe53":"code","ae125ecc":"code","b508d795":"code","5ee061dc":"code","bc1b1788":"code","78c105c9":"code","c901f09f":"code","8b2743dd":"code","47bf78a6":"code","869d24b6":"code","bedbccf5":"code","935038dc":"code","d41e2e6a":"code","20742f90":"code","cfdfc67d":"code","fb941cba":"code","141a40bd":"code","5ff07155":"code","0f1dfd2e":"code","6687277b":"code","19b6ffb0":"code","7bbe60b3":"code","f9d9fac1":"code","2a6107b4":"code","5d33236a":"code","54afa68c":"code","ed5afdaf":"code","13fd4959":"code","7ed8a1a0":"code","b6b3c6c1":"code","694526d7":"code","f2ae26bc":"code","6c76bda9":"code","49954f24":"code","0dc77fd7":"code","afb5c188":"code","fd51aeac":"code","1e358be1":"markdown","33cd7c33":"markdown","317a58e4":"markdown","12aa5c3f":"markdown","569d48e3":"markdown","4b4c37ff":"markdown","f6b00b0e":"markdown","c9c1a29b":"markdown","9d041151":"markdown","55f3e409":"markdown","6eadffe4":"markdown","92f3b98e":"markdown","2a731ffb":"markdown","1518140c":"markdown","a6027791":"markdown","a0d9c8ce":"markdown","5a1dddf6":"markdown","541571a7":"markdown","d5b78f24":"markdown","126eb100":"markdown","520034e1":"markdown","c37ff454":"markdown","49a8d7a1":"markdown"},"source":{"2f90b6ca":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","39a09c91":"train = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/train.csv\")\nlabel = train['income']\n\ndel train['income']\n\ntest = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/test.csv\")","9a9e909c":"# \ub77c\ubca8 \uac12 \uc778\ucf54\ub529\nlabel = label.map(lambda x: 1 if x == '>50K' else 0)","47f19b21":"del train['id']\ndel test['id']","9a4a8096":"tmp_train = train.copy()\ntmp_test  = test.copy()","15f07076":"tmp_train.head()","a10d2f72":"tmp_train.info()","7509664b":"tmp_train.describe()","60d5e213":"tmp_test.head()","dd1d81e7":"has_na_columns = ['workclass', 'occupation', 'native_country']","fa9badb6":"train.workclass.mode()","d235c12b":"sum(tmp_train[has_na_columns]=='?')","92529540":"for c in has_na_columns:\n    tmp_train.loc[train[c] == '?', c] = train[c].mode()[0]\n    #mode -> \ucd5c\ube48\uac12 \ubc49\uc5b4\uc90c\n    tmp_test.loc[test[c]   == '?', c] = test[c].mode()[0]","9d493c2c":"tmp_train['capital_gain'].plot.hist()","1200d1d4":"tmp_train['log_capital_gain'] = train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\ntmp_test['log_capital_gain']  = test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\ntmp_train['log_capital_gain'].plot.hist()\n#tmp_train[tmp_train['log_capital_gain']> 0]['log_capital_gain'].plot.hist()","82f98e29":"train['capital_loss'].plot.hist()","db6a9c60":"tmp_train['log_capital_loss'] = train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\ntmp_test['log_capital_loss'] = test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n\ntmp_train['log_capital_loss'].plot.hist()","b5ba6920":"tmp_train = tmp_train.drop(columns=['capital_loss', 'capital_gain'])\ntmp_test  = tmp_test.drop(columns=['capital_loss', 'capital_gain'])","ee7d180b":"tmp_train","3ede9ae0":"from sklearn.model_selection import train_test_split\n#skleanr.model_selection tmp_train, label -> train_X train_y\ntmp_train, tmp_valid, y_train, y_valid = train_test_split(tmp_train, label, \n                                                          test_size=0.3,\n                                                          random_state=2020,\n                                                          shuffle=True,\n                                                          stratify=label)","a9a8750e":"# \uc778\ub371\uc2a4 \ucd08\uae30\ud654\ntmp_train = tmp_train.reset_index(drop=True)\ntmp_valid = tmp_valid.reset_index(drop=True)\ntmp_test  = tmp_test.reset_index(drop=True)","2a54ef30":"tmp_train.dtypes.index","c25124be":"tmp_train.dtypes[1]\n#object dtpye = > 'O' \ub97c \ubc49\uc74c","bcb21f40":"cat_columns = [c for c, t in zip(tmp_train.dtypes.index, tmp_train.dtypes) if t == 'O'] ","fa625d90":"cat_columns","204c926c":"num_columns = [c for c in tmp_train.columns if c not in cat_columns]\n\nprint('\ubc94\uc8fc\ud615 \ubcc0\uc218: \\n{}\\n\\n \uc218\uce58\ud615 \ubcc0\uc218: \\n{}\\n'.format(cat_columns, num_columns))","6ac33f8f":"from sklearn.preprocessing import StandardScaler\n#Scaling \ud560\ub54c\ub294 train \uae30\uc900\uc73c\ub85c \uc2a4\ucf00\uc77c\ub9c1 \ud558\uace0 \ubaa8\ub378\uc774 \ud559\uc2b5\ud558\ub294\uac83\ub3c4 \ud2b8\ub808\uc778\uc774\ub2e4 \n#fit transform -> scaling \uc740 \ud2b8\ub808\uc778\uc744 \uae30\uc900\uc73c\ub85c \ud574\uc11c \ubca8\ub9ac\ub370\uc774\uc158\uc774\ub098 \ud14c\uc2a4\ud2b8\ub3c4 \ud574\uc900\ub2e4.\nscaler = StandardScaler()\ntmp_train[num_columns] = scaler.fit_transform(tmp_train[num_columns])\ntmp_valid[num_columns] = scaler.transform(tmp_valid[num_columns])\ntmp_test[num_columns]  = scaler.transform(tmp_test[num_columns])","0c92df45":"tmp_train.describe()\n#\uc2a4\ud0e0\ub2e4\ub4dc\ub85c \uc2a4\ucf00\uc77c\ub9c1 \ud588\uc73c\ub2c8\uae4c mean = 0 > std = 1 ","eb052c42":"tmp_valid.describe()","ff576fec":"tmp_test.describe()","bb2566fd":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\ntmp_all = pd.concat([tmp_train, tmp_valid, tmp_test])\n\nohe = OneHotEncoder(sparse=False)\nohe.fit(tmp_all[cat_columns])","ce1cee72":"ohe.categories_","f5a73913":"ohe_columns = list()\nfor lst in ohe.categories_:\n    ohe_columns += lst.tolist()","8c64c12d":"ohe_columns","372cfe53":"new_train_cat = pd.DataFrame(ohe.transform(tmp_train[cat_columns]), columns=ohe_columns)\nnew_valid_cat = pd.DataFrame(ohe.transform(tmp_valid[cat_columns]), columns=ohe_columns)\nnew_test_cat  = pd.DataFrame(ohe.transform(tmp_test[cat_columns]), columns=ohe_columns)","ae125ecc":"new_train_cat.head()","b508d795":"cat_columns","5ee061dc":"tmp_train = pd.concat([tmp_train, new_train_cat], axis=1)\ntmp_valid = pd.concat([tmp_valid, new_valid_cat], axis=1)\ntmp_test = pd.concat([tmp_test, new_test_cat], axis=1)\n\n# \uae30\uc874 \ubc94\uc8fc\ud615 \ubcc0\uc218 \uc81c\uac70\ntmp_train = tmp_train.drop(columns=cat_columns)\ntmp_valid = tmp_valid.drop(columns=cat_columns)\ntmp_test = tmp_test.drop(columns=cat_columns)","bc1b1788":"tmp_train.head()","78c105c9":"tmp_y_train = y_train\ntmp_y_valid = y_valid","c901f09f":"tmp_y_train","8b2743dd":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import f1_score","47bf78a6":"LogisticRegression?","869d24b6":"#NNI AutoML \/ Hyper opt \/ Keras tuner # pycarrot -> notebook\nlr = LogisticRegression()\n\nlr.fit(tmp_train, tmp_y_train)\n\ny_pred = lr.predict(tmp_valid)\n\nprint(f\"Logistic Regression F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","bedbccf5":"svc = SVC()\n\nsvc.fit(tmp_train, tmp_y_train)\n\ny_pred = svc.predict(tmp_valid)\n\nprint(f\"Support Vector Machine F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","935038dc":"rf = RandomForestClassifier()\n\nrf.fit(tmp_train, tmp_y_train)\n\ny_pred = rf.predict(tmp_valid)\n\nprint(f\"RandomForest F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","d41e2e6a":"xgb = XGBClassifier(tree_method='gpu_hist')\n\nxgb.fit(tmp_train, tmp_y_train)\n\ny_pred = xgb.predict(tmp_valid)\n\nprint(f\"XGBoost F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","20742f90":"lgb = LGBMClassifier(tree_method='gpu_hist')\n\nlgb.fit(tmp_train, tmp_y_train)\n\n\n\ny_pred = lgb.predict(tmp_valid)\n\nprint(f\"LightGBM F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","cfdfc67d":"def preprocess(x_train, x_valid, x_test):\n    tmp_x_train = x_train.copy()\n    tmp_x_valid = x_valid.copy()\n    tmp_x_test  = x_test.copy()\n    \n    tmp_x_train = tmp_x_train.reset_index(drop=True)\n    tmp_x_valid = tmp_x_valid.reset_index(drop=True)\n    tmp_x_test  = tmp_x_test.reset_index(drop=True)\n    \n    for c in has_na_columns:\n        tmp_x_train.loc[tmp_x_train[c] == '?', c] = tmp_x_train[c].mode()[0]\n        tmp_x_valid.loc[tmp_x_valid[c] == '?', c] = tmp_x_valid[c].mode()[0]\n        tmp_x_test.loc[tmp_x_test[c]   == '?', c] = tmp_x_test[c].mode()[0]\n\n    tmp_x_train['log_capital_loss'] = tmp_x_train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_valid['log_capital_loss'] = tmp_x_valid['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_test['log_capital_loss'] = tmp_x_test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_train['log_capital_gain'] = tmp_x_train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_valid['log_capital_gain'] = tmp_x_valid['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_test['log_capital_gain'] = tmp_x_test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_train = tmp_x_train.drop(columns=['capital_loss', 'capital_gain'])\n    tmp_x_valid = tmp_x_valid.drop(columns=['capital_loss', 'capital_gain'])\n    tmp_x_test  = tmp_x_test.drop(columns=['capital_loss', 'capital_gain'])\n    \n    scaler = StandardScaler()\n    tmp_x_train[num_columns] = scaler.fit_transform(tmp_x_train[num_columns])\n    tmp_x_valid[num_columns] = scaler.transform(tmp_x_valid[num_columns])\n    tmp_x_test[num_columns]  = scaler.transform(tmp_x_test[num_columns])\n    \n    tmp_all = pd.concat([tmp_x_train, tmp_x_valid, tmp_x_test])\n\n    ohe = OneHotEncoder(sparse=False)\n    ohe.fit(tmp_all[cat_columns])\n    \n    ohe_columns = list()\n    for lst in ohe.categories_:\n        ohe_columns += lst.tolist()\n    \n    tmp_train_cat = pd.DataFrame(ohe.transform(tmp_x_train[cat_columns]), columns=ohe_columns)\n    tmp_valid_cat = pd.DataFrame(ohe.transform(tmp_x_valid[cat_columns]), columns=ohe_columns)\n    tmp_test_cat  = pd.DataFrame(ohe.transform(tmp_x_test[cat_columns]), columns=ohe_columns)\n    \n    tmp_x_train = pd.concat([tmp_x_train, tmp_train_cat], axis=1)\n    tmp_x_valid = pd.concat([tmp_x_valid, tmp_valid_cat], axis=1)\n    tmp_x_test = pd.concat([tmp_x_test, tmp_test_cat], axis=1)\n\n    tmp_x_train = tmp_x_train.drop(columns=cat_columns)\n    tmp_x_valid = tmp_x_valid.drop(columns=cat_columns)\n    tmp_x_test = tmp_x_test.drop(columns=cat_columns)\n    #values \ub77c\ub294\uac74 numpy \ub85c \ubc18\ud658\uc744 \ud558\uaca0\ub2e4 \ub77c\ub294\uac70\n    return tmp_x_train.values, tmp_x_valid.values, tmp_x_test.values","fb941cba":"def xgb_f1(y, t, threshold=0.5):\n    t = t.get_label()\n    y_bin = (y > threshold).astype(int) \n    #0.5 \ubcf4\ub2e4 \ud06c\uba74 Class 1 \ub85c \uac04\uc8fc\ud568.\n    return 'f1',f1_score(t, y_bin, average='micro')","141a40bd":"from sklearn.model_selection import StratifiedKFold\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)\n#skf n_splits = 5 \ub2c8\uae4c generator iter = 5","5ff07155":"val_scores = list()\noof_pred = np.zeros((test.shape[0],))\nprint(oof_pred.shape)","0f1dfd2e":"Model={'lr' : LogisticRegression()\n      ,'svm' : SVC()\n      ,'rf' : RandomForestClassifier()\n      ,'xgb' : XGBClassifier(n_estimator = 200,tree_method='gpu_hist')\n      ,'lgbm' : LGBMClassifier(tree_method='gpu_hist')}","6687277b":"mode = 'xgb'","19b6ffb0":"for i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    model = Model[mode]\n    #clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    model.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,\n            early_stopping_rounds = 100,\n            verbose = 300, )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 Log Loss \ud655\uc778\n    trn_f1_score = f1_score(y_train, model.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, model.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","7bbe60b3":"val_scores = list()\noof_pred = np.zeros((test.shape[0], ))","f9d9fac1":"oof_pred.shape","2a6107b4":"skf.split(train,label)","5d33236a":"train.head()","54afa68c":"for i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    print(x_train)\n    print(x_train.shape)\n    print(i)\n\n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","ed5afdaf":"val_scores = list()","13fd4959":"new_x_train_list = [np.zeros((train.shape[0], 1)) for _ in range(4)]\n#Vector \uac00 \uc544\ub2c8\ub77c matrix \ub85c \ub9cc\ub4e4\uc5b4\ub194\uc57c concatenate \ud560\uc218\uc788\uc74c \ub4a4\uc5d0\uc11c\nnew_x_test_list  = [np.zeros((test.shape[0], 1)) for _ in range(4)]","7ed8a1a0":"for i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    print(f\"Fold {i} Start\")\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clfs = [LogisticRegression(), \n            RandomForestClassifier(), \n            XGBClassifier(tree_method='gpu_hist'), \n            LGBMClassifier(tree_method='gpu_hist')]\n    \n    for model_idx, clf in enumerate(clfs):\n        clf.fit(x_train, y_train)\n        \n        new_x_train_list[model_idx][val_idx, :] = clf.predict_proba(x_valid)[:, 1].reshape(-1, 1)\n        new_x_test_list[model_idx][:] += clf.predict_proba(x_test)[:, 1].reshape(-1, 1) \/ n_splits","b6b3c6c1":"new_x_train_list[0].shape","694526d7":"new_x_test_list","f2ae26bc":"new_train = pd.DataFrame(np.concatenate(new_x_train_list, axis=1), columns=None)\n#new_xtrain_list = numpy \ub2c8\uae4c concatenate\n#new_label = np.concatenate([tmp_y_train, tmp_y_valid])\nnew_label = label\nnew_test = pd.DataFrame(np.concatenate(new_x_test_list, axis=1), columns=None)\n\nnew_train.shape, new_label.shape, new_test.shape","6c76bda9":"val_scores = list()\noof_pred = np.zeros((test.shape[0], ))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(new_train, new_label)):\n    x_train, y_train = new_train.iloc[trn_idx, :], new_label[trn_idx]\n    x_valid, y_valid = new_train.iloc[val_idx, :], new_label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test  = scaler.transform(new_test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    #meta \ubaa8\ub378\uc740 LGBM \uc774\ub098 XGB \uc4f0\ub294\ub4ef\n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","49954f24":"submit = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/sample_submission.csv\")","0dc77fd7":"submit.loc[:, 'prediction'] = (oof_pred > 0.5).astype(int)","afb5c188":"submit.head()","fd51aeac":"submit.to_csv('submission.csv', index=False)\n#index = False \ub294 \uae30\ubcf8\uc73c\ub85c \ud56d\uc0c1.. index \uac00 \uc62c\ub77c\uc634\/\n#header_falst \ub3c4 \uc788\uc74c","1e358be1":"#### 2) 2 Stage Meta Model \ud559\uc2b5\nnew_train, new_test\uc5d0 \ub4e4\uc5b4\uc788\ub294 \ubcc0\uc218\ub294 \ubaa8\ub450 \uc218\uce58\ud615 \ubcc0\uc218\uc774\ubbc0\ub85c Standard Scaling\ub9cc \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4.<br>\n\uc0c8\ub85c \uc0dd\uc131\ud55c \ub370\uc774\ud130 new_train, new_test \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 2 Stage Meta Model\uc744 \ud559\uc2b5\ud558\uace0 \uacb0\uacfc\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.","33cd7c33":"\uc313\uc740 \ubaa8\ub378\uc774 \uc801\uc5b4\uc11c \uc131\ub2a5\uc774 \uc88b\uc9c0 \uc54a\uc73c\ub2c8 OOF \uc559\uc0c1\ube14\ub85c \uc608\uce21\ud55c \uac12\uc744 \uacb0\uacfc\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. ","317a58e4":"### 6. \uacb0\uacfc \ub9cc\ub4e4\uae30","12aa5c3f":"#### 2) \ub370\uc774\ud130 \ud655\uc778\n.head(), .describe(), .info() \ub4f1\uc758 \ud568\uc218\ub85c \ub370\uc774\ud130\ub97c \ud655\uc778\ud569\ub2c8\ub2e4. ","569d48e3":"#### 4) XGBoost","4b4c37ff":"##### \ub370\uc774\ud130 \ucabc\uac1c\uae30, Train -> (Train, Valid)\n- train_test_split \ud30c\ub77c\ubbf8\ud130 \n    - test_size  (float): Valid(test)\uc758 \ud06c\uae30\uc758 \ube44\uc728\uc744 \uc9c0\uc815\n    - random_state (int): \ub370\uc774\ud130\ub97c \ucabc\uac24 \ub54c \ub0b4\ubd80\uc801\uc73c\ub85c \uc0ac\uc6a9\ub418\ub294 \ub09c\uc218 \uac12 (\ud574\ub2f9 \uac12\uc744 \uc9c0\uc815\ud558\uc9c0 \uc54a\uc73c\uba74 \ub9e4\ubc88 \ub2ec\ub77c\uc9d1\ub2c8\ub2e4.)\n    - shuffle     (bool): \ub370\uc774\ud130\ub97c \ucabc\uac24 \ub54c \uc11e\uc744\uc9c0 \uc720\ubb34\n    - stratify   (array): Stratify\ub780, \ucabc\uac1c\uae30 \uc774\uc804\uc758 \ud074\ub798\uc2a4 \ube44\uc728\uc744 \ucabc\uac1c\uace0 \ub098\uc11c\ub3c4 \uc720\uc9c0\ud558\uae30 \uc704\ud574 \uc124\uc815\ud574\uc57c\ud558\ub294 \uac12\uc785\ub2c8\ub2e4. \ud074\ub798\uc2a4 \ub77c\ubca8\uc744 \ub123\uc5b4\uc8fc\uba74 \ub429\ub2c8\ub2e4.\n        - ex) \uc6d0\ubcf8 Train \ub370\uc774\ud130\uc758 \ud074\ub798\uc2a4 \ube44\uc728\uc774 (7:3) \uc774\uc5c8\ub2e4\uba74, \ucabc\uac1c\uc5b4\uc9c4 Train, Valid(test) \ub370\uc774\ud130\uc758 \ud074\ub798\uc2a4 \ube44\uc728\ub3c4 (7:3)\uc774 \ub429\ub2c8\ub2e4. \ub2f9\uc5f0\ud788 \ubd84\ub958 \ub370\uc774\ud130\uc5d0\uc11c\ub9cc \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","f6b00b0e":"### 1. \ub370\uc774\ud130 \uc804\ucc98\ub9ac","c9c1a29b":"#### 5) LightGBM","9d041151":"#### 6) \uc2a4\ucf00\uc77c\ub9c1\nScikit-learn \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0 \uc788\ub294 Standard Scaler\ub97c \uc0ac\uc6a9\ud574\uc11c \uc218\uce58\ud615 \ubcc0\uc218\ub4e4\uc758 \ud45c\uc900\ud654\ub97c \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4.","55f3e409":"#### 2) \uc11c\ud3ec\ud2b8 \ubca1\ud130 \uba38\uc2e0(rbf \ucee4\ub110)","6eadffe4":"### 2. Scikit-Learn \ubd84\ub958 \ubaa8\ub378 \uc0ac\uc6a9\ud574\ubcf4\uae30\nScikit-Learn\uc758 \uae30\ubcf8 \ubd84\ub958 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. <br>\n\uac01 \ubaa8\ub378\uc758 \ud3c9\uac00 \uba54\ud2b8\ub9ad\uc740 \ub300\ud68c \ud3c9\uac00 \uba54\ud2b8\ub9ad\uc778 f1_score\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.","92f3b98e":"### 3. k-Fold Cross Validation\n\uba3c\uc800 1. \uc5d0\uc11c \uc815\ub9ac\ud55c \uc804\ucc98\ub9ac \ud504\ub85c\uc138\uc2a4\ub97c \ud558\ub098\uc758 \ud568\uc218\ub85c \ub9cc\ub4ed\ub2c8\ub2e4.","2a731ffb":"#### 5) \ub370\uc774\ud130 \ucabc\uac1c\uae30\n##### 1. Train, Valid, Test Set`a\n* Train Data : \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\ub294\ub370 \uc0ac\uc6a9\ud558\ub294 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \uc54c\uace0 \uc788\ub294 \ud559\uc2b5\ud560 \ub370\uc774\ud130, \uacfc\uac70 \ub370\uc774\ud130)\n* Valid Data : \ud559\uc2b5\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uac80\uc99d\ud558\ub294 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \ubaa8\ub974\ub294 \ud559\uc2b5\ud558\uc9c0 \uc54a\uc744 \ub370\uc774\ud130, \ubaa8\ub378 \uac80\uc99d\uc5d0 \uc0ac\uc6a9\ud558\ub294 \ub370\uc774\ud130, \uacfc\uac70 \ub370\uc774\ud130)\n* Test Data : \ud559\uc2b5\ud55c \ubaa8\ub378\ub85c \uc608\uce21\ud560 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \ubaa8\ub974\ub294 \uc608\uce21\ud560 \ub370\uc774\ud130, \ubbf8\ub798 \ub370\uc774\ud130)","1518140c":"### 5. Stacking \uc559\uc0c1\ube14\n2 stage \uc559\uc0c1\ube14\uc778 Stacking \uc559\uc0c1\ube14 \uc785\ub2c8\ub2e4. Stacking \uc559\uc0c1\ube14\uc740 \uc218\uc2ed\uac1c\uc758 1 stage \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c \ubaa8\uc544 2 stage \ubaa8\ub378\ub85c \ud559\uc2b5 \ud6c4 \uacb0\uacfc\ub97c \ub0b4\ub294 \uc559\uc0c1\ube14 \ubc29\uc2dd\uc785\ub2c8\ub2e4.\n\n#### 1) 1 stage \uacb0\uacfc \ubaa8\uc73c\uae30\nStacking \uc559\uc0c1\ube14\uc744 \uc9c4\ud589\ud560 1 stage \ubaa8\ub378\uc758 \uacb0\uacfc(train, test)\ub97c \ubaa8\uc74d\ub2c8\ub2e4. ","a6027791":"#### 6) \uc778\ucf54\ub529\n\ubc94\uc8fc\ud615 \ubcc0\uc218\ub97c \uc218\uce58\ud615 \ubcc0\uc218\ub85c \uc778\ucf54\ub529 \ud558\uaca0\uc2b5\ub2c8\ub2e4. \ubc94\uc8fc\ud615 \ubcc0\uc218\uc5d0\ub294 Onehot Encoding\uc744 \uc801\uc6a9\ud569\ub2c8\ub2e4.","a0d9c8ce":"#### 1) \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \ubaa8\ub378","5a1dddf6":"#### 1) CSV \ud30c\uc77c \ubd88\ub7ec\uc624\uae30","541571a7":"#### 3) \ub79c\ub364 \ud3ec\ub808\uc2a4\ud2b8","d5b78f24":"ID \uceec\ub7fc\uc740 \ud589\uc758 \uc2dd\ubcc4\uc790\ub85c \ud544\uc694 \uc5c6\ub294 \uceec\ub7fc\uc774\ubbc0\ub85c \uc0ad\uc81c\ud558\uaca0\uc2b5\ub2c8\ub2e4. ","126eb100":"#### 4) Log \ubcc0\ud658\ncapital_gain \ubcc0\uc218\uc640 capital_loss \ubcc0\uc218\uc758 \ubd84\ud3ec\uac00 \ud55c\ucabd\uc73c\ub85c \uce58\uc6b0\uce5c \ud615\ud0dc\uc774\ubbc0\ub85c Log \ubcc0\ud658\uc744 \ud1b5\ud574 \ubd84\ud3ec\uc758 \ud615\ud0dc\ub97c \uc870\uc815\ud574\uc8fc\uaca0\uc2b5\ub2c8\ub2e4.","520034e1":"tmp_train.head()","c37ff454":"#### 3) \uacb0\uce21\uce58 \ucc98\ub9ac\n\uc774\uc804 \ud0dc\uc9c4\ub2d8 \uac15\uc758\uc5d0\uc11c 'workclass', 'occupation', 'native_country' \uceec\ub7fc\uc5d0 \uacb0\uce21\uce58\uac00 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. <br>\n\uc77c\ubc18\uc801\uc778 \uacb0\uce21\uce58\uc640 \ub2e4\ub974\uac8c '?'\ub85c \ud45c\ud604\ub418\uc5b4\uc788\ub294 \uac12\ub4e4\uc740 \ud574\ub2f9 \uceec\ub7fc\uc758 \ucd5c\ube48\uac12\uc73c\ub85c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4. <br>\n\n##### \ubc94\uc8fc\ud615 \ubcc0\uc218\uc758 \uacbd\uc6b0 \uac00\uc7a5 \uac04\ub2e8\ud558\uac8c \ucd5c\ube48\uac12\uc73c\ub85c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \ud560 \uc218 \uc788\uc9c0\ub9cc, \ub2e4\ub978 \uceec\ub7fc\uc744 \ud544\ud130\ub9c1\ud574\uc11c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. ex) education_num \ub4f1","49a8d7a1":"### 4. OOF(Out-Of-Fold) \uc559\uc0c1\ube14\nk-Fold\ub97c \ud65c\uc6a9\ud574\uc11c \ubaa8\ub378 \uac80\uc99d \ubc0f \uac01 \ud3f4\ub4dc\uc758 \uacb0\uacfc\ub97c \uc559\uc0c1\ube14\ud558\ub294 OOF \uc559\uc0c1\ube14 \uc785\ub2c8\ub2e4."}}