{"cell_type":{"d3310333":"code","2fa4663e":"code","f8d071ac":"code","7fd6d2e5":"code","f4206f6b":"code","42b42e94":"code","cd77a39b":"code","3bfe10f5":"code","27e7f4e0":"code","5e7324b4":"code","77a3a751":"code","54c98efc":"code","c16c25b1":"code","d65aa1ab":"code","e03bbb6c":"code","2e202008":"code","a9d05719":"code","1ab58036":"code","5903f0d1":"code","1633089a":"code","2805cb40":"code","8a4729e6":"code","0e56e452":"code","029d7cd6":"code","2131adc0":"code","d3e1db51":"code","36b1938e":"code","2216efcd":"markdown","597071e6":"markdown","23c2771a":"markdown","cb7b0736":"markdown","e01bebe9":"markdown","51b0c485":"markdown","82d129a2":"markdown","3c509323":"markdown","4d0e33ef":"markdown"},"source":{"d3310333":"## Importing libraries\nimport pandas as pd\npd.set_option('display.max_colwidth', -1)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, GRU, BatchNormalization, Dropout\nfrom keras.initializers import Constant\n\nimport gensim\nimport os\nimport re \nimport string\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n# import nltk\n# nltk.download(\"stopwords\")\n# nltk.download('punkt')","2fa4663e":"## Reading the data\ndata_train = pd.read_csv(\"..\/input\/sentiment-data-train\/sentiment_train_data.csv\", sep=\",\", header=None, encoding=\"latin1\")\ndata_test = pd.read_csv(\"..\/input\/sentiment-data\/sentiment_test_data.csv\", sep=\",\", header=None, encoding=\"latin1\")\n                      \n## Assigning names to the columns\ndata_train.columns = [\"Polarity\", \"ID\", \"Date\", \"Query\", \"User\", \"Text\"]\ndata_test.columns = [\"Polarity\", \"ID\", \"Date\", \"Query\", \"User\", \"Text\"]\n\n## Joining train and test data sets\nall_data = pd.concat([data_train, data_test]).reset_index(drop=True)\n\n## Eliminating the \"Neutral\" category of the target\nall_data = all_data[all_data.Polarity != 2]\n\n## Changing the target to 0 and 1 values\nall_data.Polarity = all_data.Polarity.apply(lambda x: 0 if x==0 else 1)\n\n## Viewing a sample and the shape of the final set\nall_data.sample(5)\nprint(\"El tama\u00f1o del data set es:\", all_data.shape)","f8d071ac":"## Changing the index order\nall_data = all_data.reindex(np.random.permutation(all_data.index))  \nall_data = all_data[[\"Polarity\", \"Text\"]].reset_index(drop=True)\n\n## Obtaining a small sample of the original set by a ramdon subsampling\ndata = all_data.sample(int(len(all_data)*.2))\n\n## Checking the target balance for both positive and negative categories\ndata.groupby(\"Polarity\").count()","7fd6d2e5":"## Creating a object class to perform the data cleaning\n\nclass CleanText(BaseEstimator, TransformerMixin):\n    def remove_mentions(self, input_text):\n        return re.sub(r'@\\w+', '', input_text)\n    \n    def remove_urls(self, input_text):\n        return re.sub(r'http.?:\/\/[^\\s]+[\\s]?', '', input_text)\n    \n    def emoji_oneword(self, input_text):\n        # By compressing the underscore, the emoji is kept as one word\n        return input_text.replace('_','')\n    \n    def remove_punctuation(self, input_text):\n        # Make translation table\n        punct = string.punctuation\n        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n        return input_text.translate(trantab)\n\n    def remove_digits(self, input_text):\n        return re.sub('\\d+', '', input_text)\n    \n    def to_lower(self, input_text):\n        return input_text.lower()\n    \n    def remove_stopwords(self, input_text):\n        stopwords_list = stopwords.words('english')\n        # Some words which might indicate a certain sentiment are kept via a whitelist\n        whitelist = [\"n't\", \"not\", \"no\"]\n        words = input_text.split() \n        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n        return \" \".join(clean_words) \n       \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    def transform(self, X, **transform_params):\n        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords)\n        return clean_X","f4206f6b":"## Generating a CleanText objetc to perform the data cleaning\nct = CleanText()\ndata_clean = ct.fit_transform(data.Text)\n\n## Checking for empty tweets after the cleaning and filling them\nempty_clean = data_clean == \"\"\nprint(\"{} tweets vacios tras la limpieza de datos\".format(data_clean[empty_clean].count()))\ndata_clean.loc[empty_clean] = '[no_text]'\n\n## Overwritting the tweets\ndata[\"Text\"] = data_clean\ndata.sample(5)","42b42e94":"## Obtening the train and test samples\nX_train, X_test, y_train, y_test = train_test_split(data.drop(\"Polarity\", axis=1),\n                                                    data.Polarity,\n                                                    test_size=0.2,\n                                                    random_state=888)\n\n## Creating vectors with the texts and targets\nX_train = X_train.Text.values\ny_train = y_train.values\n\nX_test = X_test.Text.values\ny_test= y_test.values\n\n## Viewing the shape of each data setbservamos las dimensiones de cada set de datos\nprint(\"Tama\u00f1o del set de entrenamiento:\", X_train.shape)\nprint(\"Tama\u00f1o del target de entrenamiento:\", y_train.shape)\n\nprint(\"\\nTama\u00f1o del set de testeo:\", X_test.shape)\nprint(\"Tama\u00f1o del target de testeo:\", y_test.shape)","cd77a39b":"## Joining the data on a unique set\ntotal_tweets = np.concatenate((X_train, X_test), axis=0)\n\n## Generating a Tokenizer object to vectorize the tweets obtaining a index for each word\ntoken = Tokenizer()\ntoken.fit_on_texts(total_tweets)\n\n## Checking the max length of the tweets\nmax_length = max([len(word.split()) for word in total_tweets])\nprint(\"La longitud m\u00e1xima de los tweets es\", max_length)\n\n## Calculating the dimension of the word frequencies dictionary\nvocabulary_dim = len(token.word_index) + 1\nprint(\"El tama\u00f1o del diccionario es\", vocabulary_dim)\n\n## Replacing the words of each tweet by their index\nX_train_token = token.texts_to_sequences(X_train)\nX_test_token = token.texts_to_sequences(X_test)\n\n## Transforming the preview list into a matrix\nX_train = pad_sequences(X_train_token, maxlen=max_length, padding=\"post\")\nX_test = pad_sequences(X_test_token, maxlen=max_length, padding=\"post\")","3bfe10f5":"## Generating a sequential model\nmodel = Sequential()\n\n## Adding a embedding matrix layer\nmodel.add(Embedding(input_dim=vocabulary_dim,  output_dim=100, input_length=max_length))\n\n## Adding a GRU module layer\nmodel.add(GRU(units=64, dropout=0.5, recurrent_dropout=0.5))\n\n## Adding a full-conected layer\nmodel.add(Dense(32, activation=\"relu\"))\n\n## Adding a normalization layer \nmodel.add(BatchNormalization())\n\n## Adding a dropout layer\nmodel.add(Dropout(0.4))\n\n## Adding a full-conected layer\nmodel.add(Dense(16, activation=\"relu\"))\n\n## Adding a normalization layer \nmodel.add(BatchNormalization())\n\n## Adding a full-conected layer with a sigmoid activation function  \nmodel.add(Dense(1, activation=\"sigmoid\"))\n\n## Compiling the cost function, metrics and optimizer\nmodel.compile(optimizer=\"adam\",\n              loss=\"binary_crossentropy\",          \n              metrics=[\"accuracy\"])\n\n## Showing a summary of the neural network structure\nmodel.summary()","27e7f4e0":"## Fitting the model\nmodel_history = model.fit(x=X_train,\n                          y=y_train,\n                          batch_size=512,\n                          epochs=20,\n                          validation_split=0.2,\n                          verbose=1)","5e7324b4":"## Creating a dataframe to be able to generate a visualization\nhistory = pd.DataFrame(model_history.history)\n\n## Including the epoch for each error\nhistory['epoch'] = model_history.epoch\n\n# Joining the errors on one column to generate a visulization with seaborn\ndf = history.melt(id_vars='epoch',\n                  var_name='Type',\n                  value_name='Accuracy',\n                  value_vars=['acc','val_acc'])\n\n# Generating the graph\nfig, ax = plt.subplots(figsize=(16,8))\n_ = sns.lineplot(x='epoch', y='Accuracy', hue='Type', data=df)\n\nprint(\"Accuracy del entrenamiento: %.2f\" % model_history.model.evaluate(X_train, y_train, verbose=0)[1]) \nprint(\"Accuracy de la validaci\u00f3n: %.2f\" % model_history.model.evaluate(X_test, y_test, verbose=0)[1])","77a3a751":"## Generating a sequential model\nmodel = Sequential()\n\n## Adding a embedding matrix layer\nmodel.add(Embedding(input_dim=vocabulary_dim,  output_dim=100, input_length=max_length))\n\n## Adding a LSTM module layer\nmodel.add(LSTM(units=64, dropout=0.5, recurrent_dropout=0.5))\n\n## Adding a full-conected layer\nmodel.add(Dense(32, activation=\"relu\"))\n\n## Adding a normalization layer \nmodel.add(BatchNormalization())\n\n## Adding a dropout layer\nmodel.add(Dropout(0.4))\n\n## Adding a full-conected layer\nmodel.add(Dense(16, activation=\"relu\"))\n\n## Adding a normalization layer \nmodel.add(BatchNormalization())\n\n## Adding a full-conected layer with a sigmoid activation function  \nmodel.add(Dense(1, activation=\"sigmoid\"))\n\n## Compiling the cost function, metrics and optimizer\nmodel.compile(optimizer=\"adam\",\n              loss=\"binary_crossentropy\",          \n              metrics=[\"accuracy\"])\n\n## Showing a summary of the neural network structure\nmodel.summary()","54c98efc":"## Fitting the model\nmodel_history = model.fit(x=X_train,\n                          y=y_train,\n                          batch_size=512,\n                          epochs=20,\n                          validation_split=0.2,\n                          verbose=1)","c16c25b1":"## Creating a dataframe to be able to generate a visualization\nhistory = pd.DataFrame(model_history.history)\n\n## Including the epoch for each error\nhistory['epoch'] = model_history.epoch\n\n# Joining the errors on one column to generate a visulization with seaborn\ndf = history.melt(id_vars='epoch',\n                  var_name='Type',\n                  value_name='Accuracy',\n                  value_vars=['acc','val_acc'])\n\n# Generating the graph\nfig, ax = plt.subplots(figsize=(16,8))\n_ = sns.lineplot(x='epoch', y='Accuracy', hue='Type', data=df)\n\nprint(\"Accuracy del entrenamiento: %.2f\" % model_history.model.evaluate(X_train, y_train, verbose=0)[1]) \nprint(\"Accuracy de la validaci\u00f3n: %.2f\" % model_history.model.evaluate(X_test, y_test, verbose=0)[1])","d65aa1ab":"## Generating a empty list and changing the format of the total set\nreview_lines = list()\nlines = total_tweets.tolist()\n\n## Generating a loop to clean the words\nfor i in lines:   \n    ## Tokenizing the words of each tweet\n    tokens = word_tokenize(i)\n    \n    ## Transforming the tokens to lower case\n    tokens = [w.lower() for w in tokens]\n    \n    ## Eliminating punctuations of each word\n    table = str.maketrans(\"\", \"\", string.punctuation)\n    stripped = [w.translate(table) for w in tokens]\n    \n    ## Eliminating the numeric tokens\n    words = [word for word in stripped if word.isalpha()]\n    review_lines.append(words)\n    \nprint(\"N\u00famero de tweets tratados en el corpus de texto:\", len(review_lines))","e03bbb6c":"## Training the Word2Vec model\nmodel = gensim.models.Word2Vec(sentences=review_lines,\n                               size=100,\n                               window=5,\n                               workers=5,\n                               min_count=1)\n\nprint(\"Tama\u00f1o del vocabulario:\", len(list(model.wv.vocab)))","2e202008":"## Similarity test of words from the model\nmodel.wv.most_similar(\"good\")","a9d05719":"## Out-of-context test to find odd words\nmodel.wv.doesnt_match(\"mouse cat dog pizza frog\".split())","1ab58036":"## Saving the result of the model \nmodel.wv.save_word2vec_format(\"..\/input\/sentiment-data\/tweets_embedding_word2vec.txt\", binary=False)\n\n## Loading the result of the model\nfile = open(os.path.join(\"\", \"tweets_embedding_word2vec.txt\"), encoding=\"utf-8\")\n\n## Generating a empty dictionary and filling it with words and their coefficients\nembeddings_index={}\nfor lines in file:\n    values = lines.split()\n    word = values[0]\n    coefs = np.asarray(values[1:])\n    embeddings_index[word] = coefs\nfile.close()","5903f0d1":"## Generating a Tokenizer object to vectorize the text samples indexing the words\ntoken_W2V = Tokenizer()\ntoken_W2V.fit_on_texts(review_lines)\n\n## Calculating the number of unique tokens\nword_index = token_W2V.word_index\nprint(\"Tokens \u00fanicos encontrados:\", len(word_index))\n\n## Replacing the words of each tweets by their index\nsequences = token_W2V.texts_to_sequences(review_lines)\n\n## Transforming the preview list into a matrix recovering the target of both sets\nreview_matrix = pad_sequences(sequences, maxlen=max_length, padding=\"post\")\nprint(\"Tama\u00f1o del tensor review:\", review_matrix.shape)\n\nsentiment = np.concatenate((y_train, y_test), axis=0)\nprint(\"Tama\u00f1o del tensor sentiment\", sentiment.shape)","1633089a":"## Generating a matrix of zeros\nnum_words = len(word_index) + 1\nprint(\"El n\u00famero de palabras es:\", num_words)\n\nembedding_matrix = np.zeros((num_words, 100))\n\n## Filling the matrix with the generated embedding from Word2Vec\nfor word, i in word_index.items():\n    if i > num_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","2805cb40":"## Generating a ramdon subsampling to obtaint the validation set\nind = np.arange(review_matrix.shape[0])\nnp.random.shuffle(ind)\n\nreview_matrix = review_matrix[ind]\nsentiment = sentiment[ind]\n\nnum_validation_samples = int(0.2 * review_matrix.shape[0])\n\nX_train_W2V = review_matrix[:-num_validation_samples]\ny_train_W2V = sentiment[:-num_validation_samples]\nX_test_W2V = review_matrix[-num_validation_samples:]\ny_test_W2V = sentiment[-num_validation_samples:]\n\n## Viewing the shape of each data set\nprint(\"Tama\u00f1o del set de entrenamiento:\", X_train_W2V.shape)\nprint(\"Tama\u00f1o del target de entrenamiento:\", y_train_W2V.shape)\n\nprint(\"\\nTama\u00f1o del set de testeo:\", X_test_W2V.shape)\nprint(\"Tama\u00f1o del target de testeo:\", y_test_W2V.shape)","8a4729e6":"## Generating a sequential model\nmodel = Sequential()\n\n## Adding a embeddint matrix layer\nmodel.add(Embedding(input_dim=num_words,\n                    output_dim=100,\n                    embeddings_initializer=Constant(embedding_matrix),\n                    input_length=max_length,\n                    trainable=False))\n\n## Adding a GRU module layer\nmodel.add(GRU(units=64, dropout=0.5, recurrent_dropout=0.5))\n\n## Adding a full-conected layer\nmodel.add(Dense(32, activation=\"relu\"))\n\n## Adding a normalization layer \nmodel.add(BatchNormalization())\n\n## Adding a dropout layer\nmodel.add(Dropout(0.4))\n\n## Adding a full-conected layer\nmodel.add(Dense(16, activation=\"relu\"))\n\n## Adding a normalization layer \nmodel.add(BatchNormalization())\n\n## Adding a full-conected layer with a sigmoid activation function \nmodel.add(Dense(1, activation=\"sigmoid\"))\n\n## Compiling the cost function, the metrics and the optimizer\nmodel.compile(optimizer=\"adam\",\n              loss=\"binary_crossentropy\",          \n              metrics=[\"accuracy\"])\n\n## Showing a summary of the neural network structure\nmodel.summary()","0e56e452":"## Fitting the model\nmodel_history = model.fit(x=X_train_W2V,\n                          y=y_train_W2V,\n                          batch_size=512,\n                          epochs=20,\n                          validation_split=0.2,\n                          verbose=1)","029d7cd6":"## Creating a dataframe to be able to generate a visualization\nhistory = pd.DataFrame(model_history.history)\n\n## Including the epoch for each error\nhistory['epoch'] = model_history.epoch\n\n# Joining the errors on one column to generate a visulization with seaborn\ndf = history.melt(id_vars='epoch',\n                  var_name='Type',\n                  value_name='Accuracy',\n                  value_vars=['acc','val_acc'])\n\n# Generating the graph\nfig, ax = plt.subplots(figsize=(16,8))\n_ = sns.lineplot(x='epoch', y='Accuracy', hue='Type', data=df)\n\nprint(\"Accuracy del entrenamiento: %.2f\" % model_history.model.evaluate(X_train_W2V, y_train_W2V, verbose=0)[1]) \nprint(\"Accuracy de la validaci\u00f3n: %.2f\" % model_history.model.evaluate(X_test_W2V, y_test_W2V, verbose=0)[1])","2131adc0":"## Generating a sequential model\nmodel = Sequential()\n\n## Adding a embeddint matrix layer\nmodel.add(Embedding(input_dim=num_words,\n                    output_dim=100,\n                    embeddings_initializer=Constant(embedding_matrix),\n                    input_length=max_length,\n                    trainable=False))\n\n## Adding a GRU module layer\nmodel.add(LSTM(units=64, dropout=0.5, recurrent_dropout=0.5))\n\n## Adding a full-conected layer\nmodel.add(Dense(32, activation=\"relu\"))\n\n## Adding a normalization layer \nmodel.add(BatchNormalization())\n\n## Adding a dropout layer\nmodel.add(Dropout(0.4))\n\n## Adding a full-conected layer\nmodel.add(Dense(16, activation=\"relu\"))\n\n## Adding a normalization layer \nmodel.add(BatchNormalization())\n\n## Adding a full-conected layer with a sigmoid activation function \nmodel.add(Dense(1, activation=\"sigmoid\"))\n\n## Compiling the cost function, the metrics and the optimizer\nmodel.compile(optimizer=\"adam\",\n              loss=\"binary_crossentropy\",          \n              metrics=[\"accuracy\"])\n\n## Showing a summary of the neural network structure\nmodel.summary()","d3e1db51":"## Fitting the model\nmodel_history = model.fit(x=X_train_W2V,\n                          y=y_train_W2V,\n                          batch_size=512,\n                          epochs=20,\n                          validation_split=0.2,\n                          verbose=1)","36b1938e":"## Creating a dataframe to be able to generate a visualization\nhistory = pd.DataFrame(model_history.history)\n\n## Including the epoch for each error\nhistory['epoch'] = model_history.epoch\n\n# Joining the errors on one column to generate a visulization with seaborn\ndf = history.melt(id_vars='epoch',\n                  var_name='Type',\n                  value_name='Accuracy',\n                  value_vars=['acc','val_acc'])\n\n# Generating the graph\nfig, ax = plt.subplots(figsize=(16,8))\n_ = sns.lineplot(x='epoch', y='Accuracy', hue='Type', data=df)\n\nprint(\"Accuracy del entrenamiento: %.2f\" % model_history.model.evaluate(X_train_W2V, y_train_W2V, verbose=0)[1]) \nprint(\"Accuracy de la validaci\u00f3n: %.2f\" % model_history.model.evaluate(X_test_W2V, y_test_W2V, verbose=0)[1])","2216efcd":"# Sentiment Analysis with RNN and Word Embedding\n\nThe sentiment analysis refers to the natural language processing or NLP as extract information that allows classifying texts based on the connotation or positive\/negative feeling that the message has. These language processes are based on statistical relationships between corpus of words and not on strict linguistic analysis.\n\nTo address a problem of this type we will use the Sentiment 140 data set created by Alec Go, Richa Bhayani and Lei Huang, all of them undergraduate students of Computer Science at Stanford University. These students developed a model that allow them to discern the feeling of a brand, a product or a topic through Twitter messages. They considered all the tweets in which there was an emoticon of the type :) as positive while emoticons of the type :( as negative, thus avoiding using manual targeting.\n\nThroughout this Jupyter Notebook this same classification problem is solved by using Recurrent Neural Networks or RNN with different modules (GRU or LSTM), in addition to applying encoding techniques of the so-called Word Embedding texts. These techniques are based on the use of vectors that allow the words to be encoded,this can be achive by using pre-train arrays of the word2vec type or training your own matrices. Finally, the different results are compared between the different models and the encoding methods.\n\n---\n\n### **Index**\n\n1. [Loading libraries and data](# id1)\n2. [Word Embedding Techniques](# id2)\n3. [Neural Network with GRU module and non-trained layer](# id3)\n4. [Neural network with LSTM module and non-trained layer](# id4)\n5. [Embedding with Word2Vec](# id5)\n6. [Neural Network with GRU module and pre-trained layer](# id6)\n7. [Neural Network with LSTM module and pre-trained layer](# id7)\n\n---\n\n**Note** : This notebook has been traslated for the joy of the community so the comments are in english but the variables and code may have spanish names or phrases.","597071e6":"## Loading libraries and data<a name=\"id1\"><\/a> ","23c2771a":"## Word Embedding Techniques<a name=\"id2\"><\/a> ","cb7b0736":"## Neural Network with LSTM module and pre-trained layer<a name=\"id7\"><\/a>","e01bebe9":"## Conclusions\n\nThroughout this notebook we have made multiple recurrent neural networks using the GRU and LSTM modules. We have also trained an embedding layer and we have generated a pre-trained embedding of type Word2Vec. In almost all the results we have seen a better accuracy with the training set against validation that seems to reach a maximum of 75% accuracy. Finally, it should be noted that the pre-trained network with the GRU module seems not to have functioned correctly showing a totally random behavior, in addition, in generic terms, the LSTM module seems to achieve greater accuracy in a smaller number of epochs and also seems to offer greater stability to long term in the validation accuracy.","51b0c485":"## Neural network with LSTM module and non-trained layer<a name=\"id4\"><\/a>","82d129a2":"## Neural Network with GRU module and pre-trained layer<a name=\"id6\"><\/a>","3c509323":"## Embedding with Word2Vec<a name=\"id5\"><\/a> ","4d0e33ef":"## Neural Network with GRU module and non-trained layer<a name=\"id3\"><\/a>"}}