{"cell_type":{"a635c25e":"code","2bfdb40b":"code","c3b6ba20":"code","9a3cfe63":"code","01eaa557":"code","ed4e6c6f":"code","4686ec94":"code","319920c1":"code","9416c9b5":"code","d962bdad":"code","3f6550a0":"code","07fb0398":"code","8acad639":"markdown","a4033b82":"markdown","21b37cd3":"markdown","921b6707":"markdown","92326eb7":"markdown","83583bda":"markdown","020c515d":"markdown","bb0a3c4c":"markdown","659bce88":"markdown","4cea4065":"markdown","869ae947":"markdown","fa5a23f3":"markdown","afba5395":"markdown","364a7fa4":"markdown","808707cc":"markdown"},"source":{"a635c25e":"import warnings\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom matplotlib import rcParams\n\nwarnings.filterwarnings(\"ignore\")\n# Global plot configs\nrcParams[\"axes.spines.top\"] = False\nrcParams[\"axes.spines.right\"] = False\nrcParams[\"font.family\"] = \"monospace\"\n\n# Pandas global settings\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_rows\", None)\npd.options.display.float_format = \"{:.5f}\".format\n\n# Import data\ntrain_df = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/train.csv\", index_col=\"id\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\", index_col=\"id\")\nsub = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")\n\n# Colors\ndark_red = \"#b20710\"\nblack = \"#221f1f\"\ngreen = \"#009473\"","2bfdb40b":"train_df.head()","c3b6ba20":"test_df.head()","9a3cfe63":"stat_summary_train = (\n    train_df.describe().drop(\"loss\", axis=1).T[[\"mean\", \"std\", \"min\", \"50%\", \"max\"]]\n)\n\nstat_summary_test = test_df.describe().T[[\"mean\", \"std\", \"min\", \"50%\", \"max\"]]\nstat_summary_train.sample(10)","01eaa557":"# Bin the mean into categories\nbins = [-np.inf, 100, 10000, np.inf]\nlabels = [\"Below 100\", \"Between 100-10000\", \"Above 10000\"]\n\nstat_summary_train[\"mean_cats_train\"] = pd.cut(\n    stat_summary_train[\"mean\"], bins=bins, labels=labels\n)","ed4e6c6f":"# Group by mean categories\ngrouped_train = stat_summary_train.value_counts(\"mean_cats_train\").sort_values(\n    ascending=False\n)\n\ncmap = [dark_red] * 4\n\nfig, ax = plt.subplots(figsize=(12, 6))\n# Plot the bar\nbar = grouped_train.plot(kind=\"bar\", ax=ax, color=cmap)\n\n# Display title\nfig.text(\n    0.5,\n    1,\n    \"Features Grouped by Mean Categories in Train\/Test Sets\",\n    fontfamily=\"monospace\",\n    size=\"20\",\n    ha=\"center\",\n)\n\n# Remove unnecessary elements\nax.yaxis.set_visible(False)\nax.set_xlabel(\"\")\nfor s in [\"top\", \"left\", \"right\"]:\n    ax.spines[s].set_visible(False)\n\n# Annotate above the bars\nfor patch in ax.patches:\n    text = f\"{patch.get_height():.0f}\"\n    x = patch.get_x() + patch.get_width() \/ 2\n    y = patch.get_height() + 5\n    ax.text(x, y, text, ha=\"center\", va=\"center\", fontsize=20)\n# Format xticklabels\nplt.setp(ax.get_xmajorticklabels(), rotation=0, fontsize=\"large\")\nax.spines[\"bottom\"].set_linewidth(1.5)\nfig.text(\n    0.6,\n    0.5,\n    \"\"\"\nThe majority of features in \nboths sets have rather low mean.\n\nHowever, we can observe features\nwith larger scales. This suggests\nwe do feature scaling when we get\nto preprocessing.\n\"\"\",\n    bbox=dict(boxstyle=\"round\", fc=\"#009473\"),\n    fontsize=\"large\",\n);","4686ec94":"fig = plt.figure(figsize=(15, 60))\ngs = fig.add_gridspec(20, 5)\ngs.update(wspace=0.1, hspace=0.4)\n\n# Add 100 subplots for all features\ntemp = 0\nfor row in range(0, 20):\n    for col in range(0, 5):\n        locals()[f\"ax_{temp}\"] = fig.add_subplot(gs[row, col])\n        locals()[f\"ax_{temp}\"].tick_params(axis=\"y\", left=False)\n        locals()[f\"ax_{temp}\"].get_yaxis().set_visible(False)\n        locals()[f\"ax_{temp}\"].set_axisbelow(True)\n        for s in [\"top\", \"right\", \"left\"]:\n            locals()[f\"ax_{temp}\"].spines[s].set_visible(False)\n        temp += 1\n\n# General texts\nfig.suptitle(\n    \"Distribution of Features in Train and Test Sets\",\n    y=0.9,\n    fontsize=25,\n    fontweight=\"bold\",\n    fontfamily=\"monospace\",\n)\n\n# Fill subplots with KDEplots of both train and test set features\ntemp = 0\nfor feature in test_df.columns.to_list():\n    for df, color in zip([train_df, test_df], [dark_red, green]):\n        sns.kdeplot(\n            df[feature],\n            shade=True,\n            color=color,\n            linewidth=1.5,\n            alpha=0.7,\n            zorder=3,\n            legend=False,\n            ax=locals()[f\"ax_{temp}\"],\n        )\n    locals()[f\"ax_{temp}\"].grid(\n        which=\"major\", axis=\"x\", zorder=0, color=\"gray\", linestyle=\":\", dashes=(1, 5)\n    )\n    locals()[f\"ax_{temp}\"].set_xlabel(feature)\n    temp += 1\nplt.show();","319920c1":"discrete_cols = [col for col in test_df.columns if test_df[col].dtype == \"int64\"]\n\ncardinality = train_df[discrete_cols].nunique().sort_values(ascending=False)\n\ncolors = [dark_red] * 5\nfig, ax = plt.subplots(figsize=(12, 6))\ncardinality.plot(kind=\"bar\", color=colors)\n\n# Display title\nfig.text(\n    0.5,\n    1,\n    \"Cardinality of Discrete Features\",\n    fontfamily=\"monospace\",\n    size=\"20\",\n    ha=\"center\",\n)\n\n# Remove unnecessary elements\nax.yaxis.set_visible(False)\nax.set_xlabel(\"\")\nfor s in [\"top\", \"left\", \"right\"]:\n    ax.spines[s].set_visible(False)\n\n# Annotate above the bars\nfor patch in ax.patches:\n    text = f\"{patch.get_height():.0f}\"\n    x = patch.get_x() + patch.get_width() \/ 2\n    y = patch.get_height() + 10000\n    ax.text(x, y, text, ha=\"center\", va=\"center\", fontsize=20)\n\n# Format xticklabels\nplt.setp(ax.get_xmajorticklabels(), rotation=0, fontsize=\"large\")\nax.spines[\"bottom\"].set_linewidth(1.5)\n\nfig.text(\n    0.4,\n    0.5,\n    \"\"\"\nThese discrete features\nall have very high cardinality,\nmeaning it isn't a good idea to\ntreat them as categorical.\n\"\"\",\n    bbox=dict(boxstyle=\"round\", fc=\"#009473\"),\n    fontsize=\"large\",\n);","9416c9b5":"fig, ax = plt.subplots(figsize=(8, 4))\n\nsns.kdeplot(\n    train_df[\"loss\"],\n    color=dark_red,\n    shade=True,\n    ax=ax,\n)\n\nax.set(xlabel=\"Target - loss\")\nplt.title(\n    \"Distribution of the Target\",\n    ha=\"center\",\n    fontfamily=\"monospace\",\n    fontsize=\"large\",\n    fontweight=\"bold\",\n    size=20,\n)\n\nfig.text(\n    0.4,\n    0.5,\n    \"\"\"\nThe target has a skewed distribution.\n\"\"\",\n    bbox=dict(boxstyle=\"round\", fc=\"#009473\"),\n    fontsize=\"medium\",\n)\n\nax.yaxis.set_visible(False)\nax.spines[\"left\"].set_visible(False)","d962bdad":"%%time\n\nfrom sklearn.model_selection import KFold, cross_validate, train_test_split\nfrom xgboost import XGBRegressor\n\n# Log transform all features and the target\nX, y = train_df.drop(\"loss\", axis=1), train_df[[\"loss\"]]\n\nreg = XGBRegressor(objective=\"reg:squarederror\",\n                   tree_method=\"gpu_hist\",)\n\n# Validation set to be used inside early_stopping\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.1, random_state=1121218\n)\n\n# Set up `fit_params` for XGBoost\neval_set = [(X_valid, y_valid)]\nfit_params = {\n    \"eval_set\": eval_set,\n    \"eval_metric\": \"rmse\",\n    \"early_stopping_rounds\": 100,\n    \"verbose\": False,\n}\n\n_ = reg.fit(X_train, y_train, **fit_params)","3f6550a0":"preds = reg.predict(test_df)","07fb0398":"submission = pd.DataFrame({\"id\": test_df.index, \"loss\": preds})\nsubmission.to_csv(\"submission.csv\", index=False)","8acad639":"Now, let's look at the distributions of both sets as a whole:","a4033b82":"# 5. Analyzing the target","21b37cd3":"Both training and test sets have 100 features, excluding the ID column. The target is given as `loss` and has a discrete distribution. \n\nSome other observations:\n- Training and test data contain **250k and 150k** observations, respectively\n- There are **no missing values** in both sets\n- All features either have `float64` or `int64` type\n\nHere are the first few rows of train and test datasets:","921b6707":"# 2. Setup","92326eb7":"# 3. Overview of the datasets","83583bda":"# 4. Analyzing the distributions of the features","020c515d":"From a random sample of the summary, we can see that features have rather different scales. As a single-metric overview, we will choose and categorize features based on their mean:","bb0a3c4c":"# 6. XGBoost Baseline","659bce88":"Since the dataset has high dimensionality, we will apply Principal Component Analysis as a base reduction method. We will pass 0.95 to `n_components` so that PCA will find the minimum number of features we need to keep to preserve at least 95% variance of the dataset.\n\nWe will perform all of the steps inside Sklearn pipelines ending with a baseline XGBoost regressor:","4cea4065":"Let's start by looking at the high-level summary of both datasets:","869ae947":"Key observations:\n- Train and test sets have roughly the same distributions in terms of features. \n- Many features have or almost have **normal distributions**\n- Some features are **bimodal**\n- Some features are even **trimodal**\n- Most features have **skewed distributions**.\n\nWe need to think about how to make all these normally distributed if we decide to use non-tree based models.\n\n> Checking the correlation revealed no significant relationships between features (most were between -0.3 and 0.3).\n\nNext, there are 5 features that are discrete. Let's check their cardinality to see any of them may be categorical:","fa5a23f3":"In this month's TPS competition, we are tasked to predict the amount of money a bank or a financial institution might lose if a loan goes into default.\n\nBefore we start the EDA, let's make sure we are all on the same page on some of the key terms of the problem definition:\n1. What is loan default?\n   - Default is a failure to repay a debt\/loan on time. It can occur when a borrower fails to make timely payments on loans such as mortgage, bank loans, car leases, etc.\n2. What is a loss given default (LGD)?\n   - LGD is the amount of money a bank or financial institution might lose if a loan goes into default. Calculating and predicting LGD can be complex and involve many factors. \n\nAs you will see in just a bit, the dataset for the competition has over 100 features and the target `loss` is (I think) LGD. For more information on these terms, check out [this](https:\/\/www.kaggle.com\/c\/tabular-playground-series-aug-2021\/discussion\/256337) discussion thread.\n\nThe metric used in this competition is Root Mean Squared Error, a regression metric:\n![image.png](attachment:b9fa4756-cf61-42b8-b0e8-05615b8733df.png)","afba5395":"# 1. Problem definition","364a7fa4":"Let's look at the distribution of the target:","808707cc":"# Loss Given Default Analysis [TPS August]\n![](https:\/\/images.unsplash.com\/photo-1616514197671-15d99ce7a6f8?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1506&q=80)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Photo by \n        <a href='https:\/\/unsplash.com\/@constantinevdokimov?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Konstantin Evdokimov<\/a>\n        on \n        <a href='https:\/\/unsplash.com\/s\/photos\/loan?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Unsplash.<\/a> All images are by author unless specified otherwise.\n    <\/strong>\n<\/figcaption>"}}