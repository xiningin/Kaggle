{"cell_type":{"f9973c20":"code","00bc0b91":"code","bebe079a":"code","474bd486":"code","9674e526":"code","b322c84c":"code","ce44bfe3":"code","866e5e3a":"code","07a17e61":"code","ce05e6e1":"code","aa02a8e8":"code","60e90327":"code","ca0f509a":"code","512438cf":"code","c02a4c34":"markdown","1d8f6947":"markdown","3b6b1e8e":"markdown","885f5d3e":"markdown","4181da0b":"markdown","a2318158":"markdown","32923cae":"markdown","8d20080a":"markdown","e372e9b2":"markdown","8717fe5a":"markdown","9b5c8de9":"markdown","90550116":"markdown"},"source":{"f9973c20":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(action='ignore')","00bc0b91":"# Define a transform to normalize the data\ntransform = transforms.ToTensor()\n\n# Download and load the training data\ntrainset = datasets.MNIST('..\/data', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)","bebe079a":"dataiter = iter(trainloader)\nimages, labels = dataiter.next()\nplt.imshow(images[0].numpy().squeeze(), cmap='gray')","474bd486":"model = nn.Sequential(nn.Conv2d(1,1,3))\nprint(model)","9674e526":"featured_map = model(images[0].unsqueeze(0))\nplt.imshow(featured_map.data.numpy().squeeze(), cmap='gray');","b322c84c":"weight = model.state_dict()['0.weight']\nplt.imshow(weight.squeeze(), cmap='gray')\n\nmodel.state_dict()","ce44bfe3":"plt.figure(figsize=(10,10))\nplt.subplot(1,3,1)\nplt.title('original')\nplt.imshow(images[0].numpy().squeeze(), cmap='gray');\nplt.subplot(1,3,2)\nplt.imshow(weight.data.numpy().squeeze(), cmap='gray')\nplt.title('weight')\nplt.subplot(1,3,3)\nplt.imshow(featured_map.data.numpy().squeeze(), cmap='gray')\nplt.title('featured map')\n\nplt.tight_layout()","866e5e3a":"relu = F.relu(featured_map)\nplt.imshow(relu.data.squeeze().numpy(), cmap='gray')","07a17e61":"relu = F.selu(featured_map)\nplt.imshow(relu.data.squeeze().numpy(), cmap='gray')","ce05e6e1":"relu = F.gelu(featured_map)\nplt.imshow(relu.data.squeeze().numpy(), cmap='gray')","aa02a8e8":"sigmoid = F.sigmoid(featured_map)\nplt.imshow(sigmoid.data.squeeze().numpy(), cmap='gray')","60e90327":"tanh = F.tanh(featured_map)\nplt.imshow(tanh.data.squeeze().numpy(), cmap='gray')","ca0f509a":"maxpool= nn.MaxPool2d(4,4)\nplt.imshow(maxpool(relu).data.squeeze().numpy(), cmap='gray')","512438cf":"maxpool= nn.AvgPool2d(4,4)\nplt.imshow(maxpool(relu).data.squeeze().numpy(), cmap='gray')","c02a4c34":"# Load Trainset","1d8f6947":"# Pooling\n![](https:\/\/image.slidesharecdn.com\/allaboutthatpooling-191128061209\/95\/all-about-that-pooling-1-638.jpg?cb=1574921591)\nPicture Credit: https:\/\/image.slidesharecdn.com\n\n\nPooling reduces the size of the data being processed in turn. This process can greatly reduce the total number of parameters in the model.\n\nSo, why do you need to go through this process (Pooling)?\nFor higher accuracy, more filters are required. As the number of filters increases, the feature map increases. This means that the dimension of the deep learning model increases, and the high dimension model increases the number of parameters as well. This is not only the problem of over fitting, but also the model It also greatly affects the size and latency of\nTherefore, there is a need to reduce the dimension, and the way to do this is to use a pooling layer.\nAlso, there are MaxPooling Layer and GlobalAveragePooling Layer as types of pooling layer that are frequently used.\n\n","3b6b1e8e":"# Feature map\n\nIf you apply the image to the convolutional layer, you can check the featured map as a result.","885f5d3e":"In the convolutional layer, a featured map is created by applying the corresponding filter to the input image.","4181da0b":"### Average Pooling","a2318158":"# Make Model\nLet's try by very simple model.","32923cae":"![MNIST](https:\/\/miro.medium.com\/max\/1400\/0*x8skn1klispGMsGf.png)\n\nPicture Credit: https:\/\/miro.medium.com","8d20080a":"# Activation Function\n\n![](https:\/\/i0.wp.com\/sefiks.com\/wp-content\/uploads\/2020\/02\/sample-activation-functions-square.png?fit=1372%2C1080&ssl=1)\n\nPicture Credit: https:\/\/i0.wp.com\/sefiks.com\n\nIn a deep learning network, values that enter a node are not passed directly to the next layer, but mainly passed through a non-linear function.\nThe function used in this case is called the activation function.\n\nThe reason why nonlinear functions are mainly used here is that the meaning of deepening the layer is reduced when using a linear function.\nFor example, suppose we have a 3-Layer network using the linear function $h(x)=cx$ as the activation function.\nExpressing the network as an equation, $y(x)=h(h(h(x)))$ becomes.\nThis is actually the same formula as $y(x)=c^3x$, so in the end, a deep layer can be expressed with one layer.\nIn other words, even if dozens of layers with linear operation are stacked, it can be expressed as one linear operation in the end.\nTherefore, if you want to get the benefit of stacking layers in a neural network, you must use a nonlinear function as the activation function.","e372e9b2":"# Filter\n\nThe filter applied to the convolution is the weight of the convolutional layer.\nPrint the applied filter on the screen.","8717fe5a":"# Introduction\n\n**Convolutional Layer**\n\n- The convolutional layer plays a role in extracting features from the input data.\n- A filter is responsible for detecting whether a feature exists in the data or not.\n- The filter corresponds to the weight of the conv layer.\n- The result obtained by applying a filter to the image is called a feature map or activation map.\n- Stride : The interval the filter will move.\n- padding : The result after applying the filter is smaller than before applying the filter. To prevent this, zero values \u200b\u200bare put around the input value.\n\n**Pooling Layer**\n\n- Features extracted through the convolutional layer go through a process called sub-sampling as needed.\n- The dimension of the extracted activation map is reduced, and this operation is called sub sampling or pooling.\n- Since the size of the entire data is reduced, the amount of computation can be reduced.","9b5c8de9":"### Max Pooling","90550116":"# Checking train image"}}