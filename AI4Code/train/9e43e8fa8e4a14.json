{"cell_type":{"9bea3d88":"code","6ce6b382":"code","692c5c31":"code","22137d82":"code","56f68422":"code","1b95bf9d":"code","f9297ff5":"code","d7b19b85":"code","1fe61584":"code","1ecc7b04":"code","c39f99bc":"code","c8ffc696":"code","9a77c4a4":"code","4c3ff9f9":"code","3726f6f8":"code","0bdc4de8":"code","fb7f28a1":"code","2093bae4":"markdown","464f8c7f":"markdown","08dd0bad":"markdown"},"source":{"9bea3d88":"import pandas as pd\nimport numpy as np\n\n\n\n","6ce6b382":"#for text analysis\n# fro plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# import natural language processing packages\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom nltk.tokenize import word_tokenize\n\n#spacy-> advances nltk package\nimport spacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n","692c5c31":"\ndf =pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\",encoding='latin-1')\ndf.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis =1,inplace=True)\ndf.columns=['category','text']","22137d82":"# to check for any null or empty files\ndf.info()","56f68422":"# describing the dataset into spam and ham\ndf['category'].value_counts().plot(kind = 'pie',explode=[0,0.1],autopct='%.2f')\nplt.xlabel('Spam vs Ham')\nplt.legend([\"ham\",\"Spam\"])\nplt.show()","1b95bf9d":"display(df.head(n=20))","f9297ff5":"# some of the top messages \ntopM= df.groupby('text')['category'].agg([len,np.max]).sort_values(by = 'len',ascending =False).head(n=5)\nprint(topM)","d7b19b85":"# studying individual spam words and hand words by grouping them\nspam_messages = df[df['category']=='spam']['text']\nprint(spam_messages)\nham_messages =df[df['category']=='ham']['text']","1fe61584":"# separating major spam and ham words\nspam_words = []\nham_words = []\n","1ecc7b04":"def extractSpam(spam_messages):\n    global spam_words\n    words =[word.lower() for word in word_tokenize(spam_messages) if word.lower() not in stopwords.words('english')and word.lower().isalpha()]\n    spam_words.append(words)\ndef extractHam(ham_messages):\n    global ham_words\n    words =[word.lower() for word in word_tokenize(ham_messages) if word.lower() not in stopwords.words('english')and word.lower().isalpha()]\n    ham_words.append(words)\n\nspam_messages.apply(extractSpam)\nham_messages.apply(extractHam)\n    ","c39f99bc":"#visually representing data\n#converting list to string  using list comprehension for the safe side\n# as WordCloud expects str instance\nspam_list = ' '.join([str(elem) for elem in spam_words])\nham_list = ' '.join([str(elem) for elem in ham_words])\n\nfrom wordcloud import WordCloud as WC\nspam_wc = WC(width =600,height =400).generate(spam_list)\nplt.figure(figsize = (10,8),facecolor = 'k')  # k means black\nplt.imshow(spam_wc)# to display as image\nplt.show()","c8ffc696":"ham_wc = WC(width = 600,height = 300).generate(ham_list)\nplt.figure(figsize =(10,8),facecolor = 'k')\nplt.imshow(ham_wc)\nplt.show()","9a77c4a4":"#cleaning data to be used in algorithm \n#removing stopwords, punctuations and stemmed words\nfrom nltk.stem import SnowballStemmer\nstemmer = SnowballStemmer('english')","4c3ff9f9":"#cleaning text messages\ndef clean_text(df):\n    df = df.translate(str.maketrans('','',string.punctuation))\n    words = [stemmer.stem(word) for word in df.split() if word.lower() not in stopwords.words('english')]\n    return ' '.join(words)\ndf['text'] = df['text'].apply(clean_text) # passsing values to functions one by one\ndf.head(n=10)","3726f6f8":"#converting data into a form that machine learning algorithm can make sense of \n#using COunt vectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv =CountVectorizer(encoding = \"latin-1\", strip_accents = \"unicode\", stop_words = \"english\")\nfeatures = cv.fit_transform(df['text'])\nprint(features.shape)","0bdc4de8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, df['category'], test_size=0.33)","fb7f28a1":"#naive bayes classifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nMultinb = MultinomialNB()\nMultinb.fit(X_train,y_train)\nMultinb.score(X_test,y_test)\ny_predict=Multinb.predict(X_test)\nprint(classification_report(y_test,y_predict))","2093bae4":" so we have to use accuracy metric that remembers this . we can live by missing some of spam message but we don't want to mark ham as spam.\n so we will be using fbeta accuracy as it uses weighted harmonic mean of precision and recall\n ","464f8c7f":"for splitting the test set and training set\n","08dd0bad":"while spliiting the data using train_test_split we have to be careful as our training may contain more normal messages . if it contains more normal messages then it may predict most as ham and we may not know too\n\n"}}