{"cell_type":{"5b002776":"code","2bf933c1":"code","c41258e0":"code","dfef7948":"code","205793a8":"code","9812d70f":"code","f50fee86":"code","b92bdc0e":"code","e37004b1":"code","7febe337":"code","df6131a1":"code","d72cd00a":"code","88b0fbd0":"code","87389a8e":"code","cc0832a8":"code","31c84083":"code","22bc232c":"code","795e48b3":"code","8367a79a":"code","24ba6045":"code","67930af0":"code","7ec6a18a":"code","c6604dfa":"code","eaad75e1":"code","f6fd9c8b":"code","bc10b22a":"code","50466fad":"code","b5f93b45":"code","b7da12b1":"code","08987361":"code","52f3ff36":"code","a1c0a401":"code","f83f303e":"code","0e89f81b":"code","647ac193":"code","af8f67fa":"code","2d51a779":"code","ffdece46":"code","7434596b":"code","4002c0d4":"code","9c4aaf72":"code","f52176c2":"code","96dae6f0":"code","b396f0d9":"code","5faf6df4":"code","f8feed7a":"code","eadf7064":"code","3ddd6997":"code","5eedb2e8":"markdown","42cca157":"markdown","4e9000c8":"markdown","1f8db6b9":"markdown","bb950af8":"markdown","acde7dec":"markdown","4c38753f":"markdown"},"source":{"5b002776":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2bf933c1":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","c41258e0":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","dfef7948":"test_data_passenger = pd.DataFrame(test_data[\"PassengerId\"],columns =[\"PassengerId\"])\ntest_data_passenger.head()\n","205793a8":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","9812d70f":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","f50fee86":"train_data.info()","b92bdc0e":"train_data.isnull().sum()","e37004b1":"train_data.describe()","7febe337":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","df6131a1":"sns.countplot(x = train_data[\"Survived\"],data=train_data,hue=train_data[\"Sex\"])","d72cd00a":"sns.countplot(x = train_data[\"Survived\"],data=train_data,hue=train_data[\"Pclass\"])","88b0fbd0":"sns.distplot(train_data[\"Age\"].dropna(),kde=False,bins=40)","87389a8e":"sns.distplot(train_data[\"Fare\"],kde=False,bins=30)","cc0832a8":"sns.countplot(x=\"SibSp\",data=train_data)","31c84083":"train_data.groupby([\"Pclass\"])[\"Age\"].mean()\n","22bc232c":"def impute_age(cols):\n    Age = cols[0]\n    Pclass= cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 38\n        elif Pclass == 2:\n            return 29\n        else:\n            return 25\n        \n    else:\n        return Age   ","795e48b3":"train_data[\"Age\"] = train_data[[\"Age\",\"Pclass\"]].apply(impute_age,axis=1)\ntest_data[\"Age\"] = test_data[[\"Age\",\"Pclass\"]].apply(impute_age,axis=1)","8367a79a":"train_data.isnull().sum()\n","24ba6045":"test_data.isnull().sum()","67930af0":"train_data.drop(\"Cabin\",axis=1,inplace=True)\ntest_data.drop(\"Cabin\",axis=1,inplace=True)","7ec6a18a":"train_data.isnull().sum()","c6604dfa":"train_data.dropna(inplace=True)\ntest_data.fillna(method=\"ffill\",inplace=True)","eaad75e1":"train_data.isnull().sum()","f6fd9c8b":"test_data.isnull().sum()","bc10b22a":"sex = pd.get_dummies(train_data[\"Sex\"],drop_first=True)\nembarked = pd.get_dummies(train_data[\"Embarked\"],drop_first=True)\n\ntrain_data = pd.concat([train_data,sex,embarked],axis=1)","50466fad":"sex_test = pd.get_dummies(test_data[\"Sex\"],drop_first=True)\nembarked_test = pd.get_dummies(test_data[\"Embarked\"],drop_first=True)\n\ntest_data = pd.concat([test_data,sex_test,embarked_test],axis=1)","b5f93b45":"train_data.head()","b7da12b1":"test_data.head()","08987361":"train_data.drop([\"PassengerId\",\"Name\",\"Sex\",\"Ticket\",\"Embarked\"],axis=1,inplace=True)\ntest_data.drop([\"PassengerId\",\"Name\",\"Sex\",\"Ticket\",\"Embarked\"],axis=1,inplace=True)","52f3ff36":"train_data.head()","a1c0a401":"test_data.head()","f83f303e":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nscaled_train_data = scaler.fit_transform(train_data.drop(\"Survived\",axis=1))\n\nscaled_test_data = scaler.transform(test_data)","0e89f81b":"X_train = pd.DataFrame(scaled_train_data,columns = train_data.columns[1:])\nX_train.head()","647ac193":"y_train = train_data[\"Survived\"]","af8f67fa":"X_test = pd.DataFrame(scaled_test_data)\n","2d51a779":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n","ffdece46":"#Linear Regression Model\n\nlr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","7434596b":"#Decision Tree Model\ndt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","4002c0d4":"#k-Nearest Neighbour\nknn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","9c4aaf72":"#Random Forest\nrf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","f52176c2":"#Support Vector Machine\nsvc = SVC(probability = True)\ncv = cross_val_score(svc,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","96dae6f0":"#XG Boost\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1)\ncv = cross_val_score(xgb,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","b396f0d9":"svc.fit(X_train,y_train)","5faf6df4":"predictions = svc.predict(X_test)","f8feed7a":"predictions","eadf7064":"#Create a  DataFrame with the passengers ids and our prediction regarding whether they survived or not\nsubmission = pd.DataFrame({'PassengerId':test_data_passenger['PassengerId'],'Survived':predictions})\n\n#Visualize the first 5 rows\nsubmission.head()","3ddd6997":"filename = 'Titanic Predictions 1.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","5eedb2e8":"## Data Preprocessing","42cca157":"Filling in missing Age value by calculating the mean value grouped by Pclass.","4e9000c8":"### So it performed well with *SVM model* amongst the above model.","1f8db6b9":"## Building the model","bb950af8":"## Exploratory Data Analysis","acde7dec":"## Data Cleaning","4c38753f":"## Converting Categorical Features"}}