{"cell_type":{"53c5cb94":"code","78f6c65c":"code","34f2b442":"code","1f0c75fa":"code","f557b2d5":"code","a29e124e":"code","525d824b":"code","3ccd458e":"code","069c1a20":"code","0baf00a1":"code","c94eab3d":"code","b48118e8":"code","3248cda0":"code","519b9c48":"code","e6c10bfb":"code","e3dd648a":"code","7c737f2e":"code","895e38fd":"code","0e3db115":"code","b8df8da3":"code","a8963611":"code","9ec89a28":"code","49ffae8c":"code","84d79079":"code","11a7b509":"code","8dbdf50c":"code","7d77968d":"code","c018d623":"code","091dc69e":"code","40e41ff1":"code","f7b82cea":"code","103a080e":"code","fafb7165":"code","4939e8f9":"code","06d19cbe":"code","cc227210":"code","1986565e":"code","ba2b0e28":"code","5ca46b16":"code","4e196835":"code","d3ea9f69":"code","0a671fff":"code","8abfaec3":"code","9fce0aea":"code","a7145971":"code","6e916213":"code","e616cf61":"code","e14b6344":"code","c9ffb85d":"code","b4593894":"code","df7ac6c0":"code","a121ebba":"code","daad1740":"code","df37dfc1":"code","48d38c99":"code","0ae39322":"code","42a97b85":"code","93c24bf3":"code","76cd6282":"code","aa157b2e":"code","5fbf1ace":"code","803ff53a":"code","408db02d":"code","33d2b15c":"code","12477bb6":"code","a456c5d4":"code","9d24fef9":"code","32c5b38b":"code","e45e312a":"code","b143ec90":"code","9d4867f5":"code","ad431b25":"code","570db183":"code","45e5741c":"code","9664535a":"code","b687f391":"code","61d14d0c":"code","5427aea3":"code","ffa5bdc5":"markdown","4a23d5da":"markdown","01e7e8a8":"markdown","a536776e":"markdown","52a7db10":"markdown","d74781da":"markdown","9cce3273":"markdown","c4ac3ad0":"markdown","0c9438be":"markdown","653eaa77":"markdown","d5b6111d":"markdown","e0bdecb5":"markdown","29347a94":"markdown","2db01171":"markdown","81deacea":"markdown","f6faf230":"markdown","b2b6d996":"markdown","c167c877":"markdown","7cf40919":"markdown","e68d3b02":"markdown","7abb816c":"markdown","e3d3f437":"markdown","6478a9a6":"markdown","02b92b2e":"markdown","204bbcb2":"markdown","0e94aa8c":"markdown","bc0e9643":"markdown","db38e594":"markdown","4da86e9d":"markdown","e72a5e89":"markdown","866a4df5":"markdown","d0e5ced3":"markdown","8b7f3acc":"markdown","174438df":"markdown","a487c485":"markdown","8103fa14":"markdown","1e96778f":"markdown","b1052391":"markdown","056f4417":"markdown","a374de2c":"markdown","551b2d13":"markdown","7777bed4":"markdown","6d1c36cf":"markdown","d4a1d904":"markdown","6ca6adc3":"markdown","ec69fb4c":"markdown","b4f25b96":"markdown","f36b0241":"markdown","c5f15d70":"markdown","b094e094":"markdown","057ba2c4":"markdown","b9d5c613":"markdown","514d26bc":"markdown","f5e0b360":"markdown","328f0294":"markdown","ba5d2ef4":"markdown","1ca48432":"markdown","e7a4dc4b":"markdown","d32bbb2a":"markdown","63bbddb3":"markdown","7c161926":"markdown","98c9ee88":"markdown","2ab82a3d":"markdown","f001eaef":"markdown","c5747634":"markdown","a8a32e81":"markdown","833fdf8f":"markdown","f8d81e9f":"markdown","b8dcea64":"markdown","76b4f634":"markdown","504c1e30":"markdown","05611474":"markdown","a2815948":"markdown"},"source":{"53c5cb94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","78f6c65c":"# Import packages\n\n## Basic data processing\nimport numpy as np\nimport pandas as pd\n\n## Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n## Modelling\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, ShuffleSplit, cross_val_score, StratifiedShuffleSplit, GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\n## Settings\npd.set_option('display.max_columns', 500) # Able to display more columns.","34f2b442":"# Load the dataset\ntrain_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndata_df = pd.concat([train_df, test_df])\ndata_df.info() # show entries, dtypes, memory useage.","1f0c75fa":"# Have a look\ndata_df.head(5)","f557b2d5":"# Check the shape of train,test and the whole dataset\ntrain_df.shape, test_df.shape, data_df.shape","a29e124e":"# Basic statistic on Ordinal, Interval and Ratio data.\nOIR_columns =  ['Pclass','Age', 'Fare', 'SibSp', 'Parch']\ndata_df[OIR_columns].describe()","525d824b":"# Basic statistic on Nominal data\ndata_df.loc[:, ~data_df.columns.isin(OIR_columns)].astype(\"object\").describe() # All the Nominal data can be treated as \"object\" type for simplicity.","3ccd458e":"'''\nDescription: Calculate and Visualize survival rate for chosen feature.\nArgs:\n    data: The dataset\n    feature: The chosen feature\n    graph_type: The graph type. Eg. \"bar\", \"point\"\nReturn: None\n'''\ndef survival_rate(data, feature, graph_type):\n    # Calculate survival rate\n    print(data[[feature, 'Survived']].groupby([feature], as_index=False).mean().sort_values(by='Survived'))\n    # Visualization\n    sns.catplot(x=feature, y=\"Survived\", data=data, kind=graph_type, height=6, aspect=1)\\\n       .set_ylabels(\"Survival Rate\")\\\n       .ax.set_title(f\"Survival Rate on {feature}\", fontsize = 20)","069c1a20":"# Extract title from Name\ndata_df['title'] = data_df['Name'].map(lambda x:x.split(',')[1].split('.')[0].strip())\ndata_df['title'].value_counts()","0baf00a1":"# Aggregate rare titles\ntitle_dic={\n    'Mr':'Mr',\n    'Miss':'Miss',\n    'Mrs':'Mrs',\n    'Master':'Master',\n    'Dr':'Other',\n    'Rev':'Other',\n    'Mlle':'Miss',\n    'Col': 'Other',\n    'Major':'Other',\n    'Sir':'Mr',\n    'Mme':'Miss',\n    'Jonkheer':'Other',\n    'Lady':'Miss',\n    'Capt':'Other',\n    'Don':'Mr',\n    'Dona':'Mrs',\n    'Ms':'Miss',\n    'the Countess':'Other'\n}","c94eab3d":"# Use the defined rules to set categories for title\ndata_df['title'] = data_df['title'].map(title_dic)\nsurvival_rate(data_df, 'title', 'bar')","b48118e8":"# Irrelevant columns\n'''\nPassengerId: Passenger Id is useless for analysis and modeling.\nName: Title has already been extracted.\nTicket: Ticket seems useless here.\n'''\nirrelevant_columns = ['PassengerId', 'Name', 'Ticket']\ndata_preprocessed_df = data_df.drop(irrelevant_columns, axis=1)","3248cda0":"# Replace the empty data with NaN\ndata_preprocessed_df.replace(\"\", float(\"NaN\"), inplace=True)\ndata_preprocessed_df.replace(\" \", float(\"NaN\"), inplace=True)\n\n# Count missing value(NaN, na, null, None) of each columns, Then transform the result to a pandas dataframe. \ncount_missing_value = data_preprocessed_df.isna().sum() \/ data_preprocessed_df.shape[0] * 100\ncount_missing_value_df = pd.DataFrame(count_missing_value.sort_values(ascending=False), columns=['Missing%'])","519b9c48":"# Visualize the percentage(>0) of Missing value in each column.\nmissing_value_df = count_missing_value_df[count_missing_value_df['Missing%'] > 0]\n\nplt.figure(figsize=(5, 8)) # Set the figure size\nmissing_value_graph = sns.barplot(x = missing_value_df.index, y = \"Missing%\", data=missing_value_df, orient=\"v\")\nmissing_value_graph.set_title(\"Percentage Missing Value\", fontsize = 20)\nmissing_value_graph.set_xlabel(\"Features\")\nfor p in missing_value_graph.patches:\n        missing_value_graph.annotate(round(p.get_height(), 2), (p.get_x()+0.25, p.get_height())) #show value on each bar","e6c10bfb":"# Visualize the distribution of Age\nage_fig = go.Figure()\nage_fig.add_trace(go.Box(\n                        y=data_preprocessed_df[\"Age\"],\n                        name='Age',\n                        boxmean=True))\n\nage_fig.update_layout(\n                height=600, \n                width=800,\n                title={\n                'text': \"The Distribution of Age\",\n                'font': {'size': 24},\n                'y':0.95,\n                'x':0.5,\n                'xanchor': 'center',\n                'yanchor': 'top'},\n                yaxis_title='Age',\n                )\n\nage_fig.show()","e3dd648a":"# Visualize the distribution of Fare\nfare_fig = go.Figure()\nfare_fig.add_trace(go.Box(\n                        y=data_preprocessed_df[\"Fare\"],\n                        name='Fare',\n                        boxmean=True))\n\nfare_fig.update_layout(\n                height=600, \n                width=800,\n                title={\n                'text': \"The Distribution of Fare\",\n                'font': {'size': 24},\n                'y':0.95,\n                'x':0.5,\n                'xanchor': 'center',\n                'yanchor': 'top'},\n                yaxis_title='Fare',\n                )\n\nfare_fig.show()","7c737f2e":"# Visualize the distribution of Embarked\nembarked_fig = px.histogram(data_preprocessed_df, x=\"Embarked\")\nembarked_fig.update_layout(\n                height=600, \n                width=800,\n                title={\n                'text': \"The count of Embarked\",\n                'font': {'size': 24},\n                'y':0.95,\n                'x':0.5,\n                'xanchor': 'center',\n                'yanchor': 'top'},\n                )\n\nembarked_fig.show()","895e38fd":"# Calculate the relationship between Cabin and Survived Rate\ndata_preprocessed_df.groupby(data_preprocessed_df['Cabin'].isnull())['Survived'].mean()","0e3db115":"# Transform Carbin information to an indicator representing whether passenger have Carbin information.\ndata_preprocessed_df['Cabin_indicator'] = np.where(data_preprocessed_df['Cabin'].isnull(), 0, 1)\ndata_preprocessed_df.drop('Cabin', axis=1, inplace=True)","b8df8da3":"data_preprocessed_df","a8963611":"# Find correlated column with Age\nplt.figure(figsize=(12,9))\nsns.heatmap(data_preprocessed_df.corr(), cmap=\"coolwarm\", annot = True, fmt='.3f').set_title('Pearson Correlation', fontsize=22)","9ec89a28":"data_preprocessed_median_df = data_preprocessed_df.copy()\n#data_preprocessed_median_df['Age'] = data_preprocessed_df['Age'].fillna(data_preprocessed_df['Age'].median())\n# Group imputation for Age by 'Pclass' and 'Sex'\ndata_preprocessed_median_df[\"Age\"].fillna(data_preprocessed_median_df.groupby(['Pclass','Sex'])['Age'].transform(\"median\"), inplace=True)\ndata_preprocessed_median_df['Fare'] = data_preprocessed_df['Fare'].fillna(data_preprocessed_df['Fare'].median())","49ffae8c":"data_preprocessed_median_df['Embarked'].fillna(data_preprocessed_median_df['Embarked'].mode()[0], inplace=True)\n# Check Missing value for\nprint(f'Missing value:\\n Median imputation: {sum(data_preprocessed_median_df.isna().sum())}')","84d79079":"# Cut the Age and Fare into bins. Set labels by alphabetically for later encoding.\ndata_preprocessed_median_df['AgeBin'] = pd.cut(data_preprocessed_median_df['Age'].astype(int), 5, labels=['a', 'b', 'c', 'd','e'])\ndata_preprocessed_median_df['FareBin'] = pd.cut(data_preprocessed_median_df['Fare'].astype(int), 4, labels=['a', 'b', 'c', 'd'])\n\n# Women and Children indicator\n#data_preprocessed_median_df['WomChi'] = ((data_preprocessed_median_df.AgeBin == 'a') | (data_preprocessed_median_df.Sex == 'female'))","11a7b509":"# Create a new dataset only include training data.\ndata_best_df = data_preprocessed_median_df.iloc[:891].copy()","8dbdf50c":"# Count the number of survived(0\/1), transform the result to pandas dataframe\nsurvival_counts = data_best_df[\"Survived\"].value_counts()\nsurvival_counts_df = pd.DataFrame(survival_counts)","7d77968d":"# Visualize the distribution of the survival\nsurvival_fig = make_subplots(\n    rows=1, cols=2, \n    specs=[[{\"type\": \"xy\"}, {\"type\": \"domain\"}]])\n\nsurvival_fig.add_trace(go.Bar(x=survival_counts_df.index, \n                              y=survival_counts_df[\"Survived\"],\n                              text=survival_counts_df[\"Survived\"],\n                              textposition='outside',\n                              showlegend=False),\n                              1, 1)\n\nsurvival_fig.add_trace(go.Pie(labels=survival_counts_df.index, \n                     values=survival_counts_df[\"Survived\"],\n                     showlegend=True),\n                     1, 2)\n\nsurvival_fig.update_layout(\n                  height=600, \n                  width=1000,\n                  title={\n                  'text': \"The distribution of Survival\",\n                  'font': {'size': 24},\n                  'y':0.95,\n                  'x':0.5,\n                  'xanchor': 'center',\n                  'yanchor': 'top'},\n                  xaxis1_title = 'Survived', \n                  yaxis1_title = 'Counts',\n                  legend_title_text=\"Survived\"\n                 )\nsurvival_fig.update_xaxes(type='category')\nsurvival_fig.show()","c018d623":"# Visualize the categorical data by Pie chart\n'''\nDescription:\nArgs:\n    data: The dataset is going to be visualized\n    feature: The chosen feature\nReturn: None\n'''\ndef categorical_VIS(data, feature):\n    \n    # Calculate the distribution of the chosen feature\n    survived = data[data[\"Survived\"] == 1][feature]\n    survived_df = pd.DataFrame(survived.value_counts())\n    not_survived = data[data[\"Survived\"] == 0][feature]\n    not_survived_df = pd.DataFrame(not_survived.value_counts())\n    \n    # Visualization\n    survival_fig = make_subplots(\n    rows=1, cols=2, \n    subplot_titles=(\"Survived\", \"Non-Survived\"),\n    specs=[[{\"type\": \"domain\"}, {\"type\": \"domain\"}]])\n    survival_fig.add_trace(go.Pie(labels=survived_df.index, \n                     values=survived_df[feature],\n                     showlegend=True),\n                     1, 1)\n    survival_fig.add_trace(go.Pie(labels=not_survived_df.index, \n                     values=not_survived_df[feature],\n                     showlegend=True),\n                     1, 2)\n    survival_fig.update_layout(\n                  height=600, \n                  width=1000,\n                  title={\n                  'text': \"The Distribution of \"+ feature + \" on Survival\",\n                  'font': {'size': 24},\n                  'y':0.95,\n                  'x':0.5,\n                  'xanchor': 'center',\n                  'yanchor': 'top'},\n                  legend_title_text=feature\n                 )\n    survival_fig.update_xaxes(type='category')\n    survival_fig.show()","091dc69e":"categorical_VIS(data_best_df, \"Sex\")","40e41ff1":"# Calcute the survival Rate\nsurvival_rate(data_best_df, 'Sex', 'bar')","f7b82cea":"categorical_VIS(data_best_df, \"Pclass\")","103a080e":"# Calcute the survival Rate\nsurvival_rate(data_best_df, 'Pclass', 'bar')","fafb7165":"categorical_VIS(data_best_df, \"SibSp\")","4939e8f9":"# Calcute the survival Rate\nsurvival_rate(data_best_df, 'SibSp', 'point')","06d19cbe":"categorical_VIS(data_best_df, \"Parch\")","cc227210":"# Calcute the survival Rate\nsurvival_rate(data_best_df, 'Parch', 'point')","1986565e":"categorical_VIS(data_best_df, \"Embarked\")","ba2b0e28":"# Calcute the survival Rate\nsurvival_rate(data_best_df, 'Embarked', 'bar')","5ca46b16":"age_fig = px.box(data_best_df, x=\"Survived\", y=\"Age\")\nage_fig.update_layout(\n                  height=600, \n                  width=1000,\n                  title={\n                  'text': \"The Distribution of Age on Survival\",\n                  'font': {'size': 24},\n                  'y':0.95,\n                  'x':0.5,\n                  'xanchor': 'center',\n                  'yanchor': 'top'},\n                  legend_title_text=\"Survived\"\n                 )","4e196835":"# Calcute the survival Rate\nsurvival_rate(data_best_df, 'AgeBin', 'point')","d3ea9f69":"fare_fig = px.box(data_best_df, x=\"Survived\", y=\"Fare\")\nfare_fig.update_layout(\n                  height=600, \n                  width=1000,\n                  title={\n                  'text': \"The Distribution of Fare on Survival\",\n                  'font': {'size': 24},\n                  'y':0.95,\n                  'x':0.5,\n                  'xanchor': 'center',\n                  'yanchor': 'top'},\n                  legend_title_text=\"Survived\"\n                 )","0a671fff":"# Calcute the survival Rate\nsurvival_rate(data_best_df, 'FareBin', 'point')","8abfaec3":"# Combine the SibSp and Parch to Family Size\ndata_best_df['Family_size'] = data_best_df['SibSp'] + data_best_df['Parch'] + 1\nsurvival_rate(data_best_df, 'Family_size', 'point')","9fce0aea":"# Binning the family_size\ndata_best_df.loc[data_best_df['Family_size'] == 1, 'Family_size'] = 0 # Alone\ndata_best_df.loc[(data_best_df['Family_size'] > 1) & (data_best_df['Family_size'] <= 4), 'Family_size'] = 1  # Small Family \ndata_best_df.loc[(data_best_df['Family_size'] > 4) & (data_best_df['Family_size'] <= 6), 'Family_size'] = 2  # Medium Family\ndata_best_df.loc[data_best_df['Family_size']  > 6, 'Family_size'] = 3 # Large Family","a7145971":"# Feature SibSp and Parch are replaced by Family_size\ndata_best_df.drop(['SibSp', 'Parch'], axis=1, inplace=True)","6e916213":"# Feature Age and Fare are replaced by AgeBin and FareBin\ndata_best_df.drop(['Age', 'Fare'], axis=1, inplace=True)","e616cf61":"# Select features that are suitable for One Hot Encoding\nonehot_features = [\"Embarked\",\"title\"]\nonehot_df = pd.get_dummies(data_best_df[onehot_features])\ndata_best_df.drop(onehot_features, axis=1, inplace=True)\ndata_best_df = pd.concat([data_best_df, onehot_df], axis=1)","e14b6344":"# Select features that are suitable for Label Encoding\ndata_best_df[\"Sex\"]  = LabelEncoder().fit_transform(data_best_df[\"Sex\"])\ndata_best_df[\"AgeBin\"]  = LabelEncoder().fit_transform(data_best_df[\"AgeBin\"])\ndata_best_df[\"FareBin\"]  = LabelEncoder().fit_transform(data_best_df[\"FareBin\"])","c9ffb85d":"data_best_df","b4593894":"# Train\/Test Split\nX = data_best_df.drop([\"Survived\"], axis=1)\nY = data_best_df.Survived.astype('int8')\nx_train, x_test, y_train, y_test = train_test_split(X, Y,test_size = 0.25, random_state=0, stratify=Y)","df7ac6c0":"# Logistic regression\n# https:\/\/stackoverflow.com\/questions\/65682019\/attributeerror-str-object-has-no-attribute-decode-in-fitting-logistic-regre\nlr = LogisticRegression(penalty = 'l2',solver = 'liblinear')\nlr.fit(x_train, y_train)\npredictions = lr.predict(x_test)\nprint(classification_report(y_test, predictions))","a121ebba":"# Cross Validation\nlr_cv = StratifiedShuffleSplit(n_splits=5, test_size=.25, random_state=0)\nlr_cv_avg = cross_val_score(lr, X, Y, cv=lr_cv, scoring=\"accuracy\").mean()\nlr_cv_avg","daad1740":"# KNN\nkNN = KNeighborsClassifier()\nkNN.fit(x_train, y_train)\npredictions = kNN.predict(x_test)\nprint(classification_report(y_test, predictions))","df37dfc1":"# Cross Validation\nkNN_cv = StratifiedShuffleSplit(n_splits=5, test_size=.25, random_state=0)\nkNN_cv_avg = cross_val_score(kNN, X, Y, cv=kNN_cv, scoring=\"accuracy\").mean()\nkNN_cv_avg","48d38c99":"# SVM\nsvm = SVC(probability=True)\nsvm.fit(x_train, y_train)\npredictions = svm.predict(x_test)\nprint(classification_report(y_test, predictions))","0ae39322":"# Cross Validation\nsvm_cv = StratifiedShuffleSplit(n_splits=5, test_size=.25, random_state=0)\nsvm_cv_avg = cross_val_score(svm, X, Y, cv=svm_cv, scoring=\"accuracy\").mean()\nsvm_cv_avg","42a97b85":"# Decision tree\ndt = DecisionTreeClassifier(random_state=0)\ndt.fit(x_train, y_train)\npredictions = dt.predict(x_test)\nprint(classification_report(y_test, predictions))","93c24bf3":"# Cross Validation\ndt_cv = StratifiedShuffleSplit(n_splits=5, test_size=.25, random_state=0)\ndt_cv_avg = cross_val_score(dt, X, Y, cv=dt_cv, scoring=\"accuracy\").mean()\ndt_cv_avg","76cd6282":"# Random Forest\nrf = RandomForestClassifier(random_state=0)\nrf.fit(x_train, y_train)\npredictions = rf.predict(x_test)\nprint(classification_report(y_test, predictions))","aa157b2e":"# Cross Validation\nrf_cv = StratifiedShuffleSplit(n_splits=5, test_size=.25, random_state=0)\nrf_cv_avg = cross_val_score(rf, X, Y, cv=rf_cv, scoring=\"accuracy\").mean()\nrf_cv_avg","5fbf1ace":"# Gradient boosting\ngbt = GradientBoostingClassifier(random_state=0)\ngbt.fit(x_train, y_train)\npredictions = gbt.predict(x_test)\nprint(classification_report(y_test, predictions))","803ff53a":"# Cross Validation\ngbt_cv = StratifiedShuffleSplit(n_splits=5, test_size=.25, random_state=0)\ngbt_cv_avg = cross_val_score(gbt, X, Y, cv=gbt_cv, scoring=\"accuracy\").mean()\ngbt_cv_avg","408db02d":"xgbc = XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='error')\nxgbc.fit(x_train, y_train)\npredictions = xgbc.predict(x_test)\nprint(classification_report(y_test, predictions))","33d2b15c":"# Cross Validation\nxgbc_cv = StratifiedShuffleSplit(n_splits=5, test_size=.25, random_state=0)\nxgbc_cv_avg = cross_val_score(xgbc, X, Y, cv=xgbc_cv, scoring=\"accuracy\").mean()\nxgbc_cv_avg","12477bb6":"catbc = CatBoostClassifier(random_state=0, eval_metric='Accuracy', verbose=False)\ncatbc.fit(x_train, y_train)\npredictions = catbc.predict(x_test)\nprint(classification_report(y_test, predictions))","a456c5d4":"# Cross Validation\ncatbc_cv = StratifiedShuffleSplit(n_splits=5, test_size=.25, random_state=0)\ncatbc_cv_avg = cross_val_score(catbc, X, Y, cv=catbc_cv, scoring=\"accuracy\").mean()\ncatbc_cv_avg","9d24fef9":"lgbc = LGBMClassifier(random_state=0)\nlgbc.fit(x_train, y_train, eval_metric='Accuracy', verbose=-1)\npredictions = lgbc.predict(x_test)\nprint(classification_report(y_test, predictions))","32c5b38b":"# Cross Validation\nlgbc_cv = StratifiedShuffleSplit(n_splits=5, test_size=.25, random_state=0)\nlgbc_cv_avg = cross_val_score(lgbc, X, Y, cv=lgbc_cv, scoring=\"accuracy\", verbose=False).mean()\nlgbc_cv_avg","e45e312a":"# Collect all the model performance\nmodel_comparison = pd.DataFrame(data = [lr_cv_avg, kNN_cv_avg, svm_cv_avg, dt_cv_avg, rf_cv_avg, gbt_cv_avg, xgbc_cv_avg, catbc_cv_avg, lgbc_cv_avg], \n                                index = [\"lr\", \"kNN\", \"SVM\", \"DT\", \"RF\", \"GBT\", \"XGBoost\", \"CatBoost\", \"LGBoost\"],\n                                columns=['Accuracy'])\\\n                      .sort_values(by = \"Accuracy\", ascending=False)\n\nmodel_comparison","b143ec90":"# Get the test dataset\ntesting_df = data_preprocessed_median_df.iloc[891:].copy()\ntesting_df.info() # show entries, dtypes, memory useage.","9d4867f5":"# Have a look\ntesting_df.head()","ad431b25":"# Drop Irrelevant columns\ntest_preprocessed_df = testing_df.drop('Survived', axis=1)\n\n# Feature Age and Fare are replaced by AgeBin and FareBin\ntest_preprocessed_df.drop([\"Age\", \"Fare\"],axis=1, inplace=True)\n\n# Check Missing value\ntest_preprocessed_df.isna().sum()","570db183":"# Combine the SibSp and Parch to Family Size\ntest_preprocessed_df['Family_size'] = test_preprocessed_df['SibSp'] + test_preprocessed_df['Parch'] + 1\ntest_preprocessed_df.drop(['SibSp', 'Parch'], axis=1, inplace=True)","45e5741c":"# Binning the family_size\ntest_preprocessed_df.loc[test_preprocessed_df['Family_size'] == 1, 'Family_size'] = 0 # Alone\ntest_preprocessed_df.loc[(test_preprocessed_df['Family_size'] > 1) & (test_preprocessed_df['Family_size'] <= 4), 'Family_size'] = 1  # Small Family \ntest_preprocessed_df.loc[(test_preprocessed_df['Family_size'] > 4) & (test_preprocessed_df['Family_size'] <= 6), 'Family_size'] = 2  # Medium Family\ntest_preprocessed_df.loc[test_preprocessed_df['Family_size']  > 6, 'Family_size'] = 3 # Large Family","9664535a":"# One-Hot Encoding\nonehot_df = pd.get_dummies(test_preprocessed_df[onehot_features])\ntest_preprocessed_df.drop(onehot_features, axis=1, inplace=True)\ntest_preprocessed_df = pd.concat([test_preprocessed_df, onehot_df], axis=1)\n\n# Label Encoding\ntest_preprocessed_df[\"Sex\"]  = LabelEncoder().fit_transform(test_preprocessed_df[\"Sex\"])\ntest_preprocessed_df[\"AgeBin\"]  = LabelEncoder().fit_transform(test_preprocessed_df[\"AgeBin\"])\ntest_preprocessed_df[\"FareBin\"]  = LabelEncoder().fit_transform(test_preprocessed_df[\"FareBin\"])\n\n# Check the data after Feature Engineering\ntest_preprocessed_df.info()","b687f391":"test_preprocessed_df","61d14d0c":"# Make Prediction by the top 5 classifiers\nvoting_clas = VotingClassifier(estimators=[('SVM', svm), ('Logistic_Reg', lr), ('CatBoost', catbc), ('GBt',gbt), ('XGBoost',xgbc)], voting='soft', n_jobs=-1)\nvotingC = voting_clas.fit(X, Y)\n\nresults = votingC.predict(test_preprocessed_df)\nresults_df = pd.DataFrame(results, columns=['Survived'])\npredictions_df = pd.concat([test_df['PassengerId'], results_df], axis=1)","5427aea3":"# Save predictions to .csv for project submission\npredictions_df.to_csv('submission.csv', index=False)","ffa5bdc5":"<a id=\"5.2.3\"><\/a>\n### 5.2.3 Support Vector Machine","4a23d5da":"1. Most passengers are young(75%)\n2. There are some expensive tickets(> 500), some weird price(min=0)\n3. It seems most of the passengers have fewer relatives(SibSp\/Parch)---75%","01e7e8a8":"# Titanic Survival Analysis and Prediction (Top 10%-Beginner Friendly)","a536776e":"<a id=\"4.2\"><\/a>\n## 4.2. One-Hot Encoding","52a7db10":"<a id=\"5.2\"><\/a>\n## 5.2. Train and Validation","d74781da":"<a id=\"4.1.1\"><\/a>\n### Family Size","9cce3273":"# Table of Content\n1. [Data Overview](#1)\n    * [1. Load Data](#1.1)\n    * [2. Data Type](#1.2)\n    * [3. Statistical View](#1.3)\n2. [Data Preprocessing](#2)\n    * [1. Extract Potential Information](#2.1)\n        * [Title](#2.1.1)\n    * [2. Drop irrelevant columns](#2.2)\n    * [3. Missing Value Detection](#2.3)\n    * [4. Data Imputation](#2.4)\n        * [1.Missing Value Exploratory](#2.4.1)\n            * [Age](#2.4.1.1)\n            * [Fare](#2.4.1.2)\n            * [Embarked](#2.4.1.3)\n            * [Cabin](#2.4.1.4)\n        * [2. Median imputation](#2.4.2)\n        * [3. Majority value imputation](#2.4.3)\n3. [Data Analysis](#3)\n    * [1. What is the distribution of survival? ](#3.1)\n    * [2. What is the distribution of Sex on survival? ](#3.2)\n    * [3. What is the distribution of Pclass on survival? ](#3.3)\n    * [4. What is the distribution of SibSp on survival? ](#3.4)\n    * [5. What is the distribution of Parch on survival? ](#3.5)\n    * [6. What is the distribution of Embarked on survival? ](#3.6)\n    * [7. What is the distribution of Age on survival? ](#3.7)\n    * [8. What is the distribution of Fare on survival? ](#3.8)\n4. [Feature Engineering](#4)\n    * [1. Create new features ](#4.1)\n        * [Family Size](#4.1.1)\n    * [2. One-Hot Encoding ](#4.2)\n    * [3. Label Encoding ](#4.3)\n5. [Modelling](#5)\n    * [1. Train Test Split ](#5.1)\n    * [2. Train Models ](#5.2)\n        * [1. Logistic regression ](#5.2.1)\n        * [2. k-nearest neighbors ](#5.2.2)\n        * [3. Support Vector Machine ](#5.2.3)\n        * [4. Decision Tree ](#5.2.4)\n        * [5. Random Forest ](#5.2.5)\n        * [6. Gradient boosting ](#5.2.6)\n        * [7. XGBoost ](#5.2.7)\n        * [8. CatBoost ](#5.2.8)\n        * [9. LGBoost ](#5.2.9)\n    * [3. Model Comparison ](#5.3)\n6. [Prediction](#6)\n    * [1. Drop irrelevant columns](#6.1)\n    * [2. Feature Engineering](#6.2)\n    * [3. Make Prediction](#6.3)\n    * [4. Save the Prediction to CSV file](#6.4)","c4ac3ad0":"<a id=\"3.5\"><\/a>\n## 3.5. What is the distribution of Parch on survival?","0c9438be":"<a id=\"4.3\"><\/a>\n## 4.3. Label Encoding","653eaa77":"<a id=\"2.4.3\"><\/a>\n### 2.4.3. Majority value imputation ","d5b6111d":"<a id=\"5.2.1\"><\/a>\n### 5.2.1 Logistic regression","e0bdecb5":"It seems overall mean and median of age doesn't differ so much.","29347a94":"## Key Findings\n* The Label(Survived) is imbalanced. Survived passengers makes up 1\/3 of the total number.\n* People who have Carbin information are likely to survive.\n* Female are more likely to survive than male.\n* People with 1(less) sibling seems to have a higer survival rate. The more siblings, the lower survival rate.\n* The higher the class level is, the more possible the passenger can survive.\n* The more expensive the ticket is, the more possible the passenger can survive.(It is coherent with the class level)","2db01171":"It seems mean and median of age does differ so much. However, it is only 1 Missing value, it should not affect the modelling process so much.","81deacea":"<a id=\"2.4.1.1\"><\/a>\n### Age","f6faf230":"<a id=\"3.3\"><\/a>\n## 3.3. What is the distribution of Pclass on survival?","b2b6d996":"<a id=\"5.2.6\"><\/a>\n### 5.2.6 Gradient boosting","c167c877":"<a id=\"3.6\"><\/a>\n## 3.6. What is the distribution of Embarked on survival?","7cf40919":"It seems S is the most frequent value of Embarked.  \n**It seems that we can try simplely use Median imputation on *Age* and *Fare* and Majority value imputation on *Embarked*.**","e68d3b02":"<a id=\"5.2.8\"><\/a>\n### 5.2.8 CatBoost","7abb816c":"* The more expensive the ticket is, the more possible the passenger can survive.","e3d3f437":"<a id=\"2.4\"><\/a>\n## 2.4. Data Imputation\n> Choose the suitable imputation tech which can highly represent the central tendency of the data.","6478a9a6":"<a id=\"6.1\"><\/a>\n## 6.1. Drop irrelevant columns","02b92b2e":"<a id=\"1.2\"><\/a>\n## 1.2. Data Type\n\n> [NOIR](https:\/\/www.questionpro.com\/blog\/nominal-ordinal-interval-ratio\/): Nominal, Ordinal, Interval, Ratio.   \nSpecify the data type of each variable for the following statistic analysis.  \n\n| Variable              | Type     | Description |\n| :---                  |  :----:  |  :----:     |\n| PassengerID | Nominal | The unique ID of passenger |\n| Name | Nominal | Name\n| Survived | Nominal  | whether the passenger survived; 0 = No, 1 = Yes | \n| Pclass | Ordinal | Ticket class; 1 = 1st, 2 = 2nd, 3 = 3rd |\n| Sex | Nominal | Sex | \n| Age | Ratio | Age in years | \n| Sibsp | Ordinal | Number of siblings \/ spouses aboard the Titanic;| \n| Parch | Ordinal | Number of parents \/ children aboard the Titanic;| \n| Ticket | Nominal | Ticket number |\n| Fare | Ratio | Passenger fare |\n| Cabin | Nominal | Cabin number | \n| Embarked | Nominal | Port of Embarkation |\n","204bbcb2":"## Tips\n* Divide the whole project into several stages, each stage save a middle version of data. It's helpful for later examining.\n* Columns with too much Missing value(>40%) should be useless. However, the \"Missing value information\" could be valuable. Eg. Cabin.\n* Find the high correlation feature as the group key for group imputation.\n* It's better to bin the numerical features in Classification scenario. The strategy of binning(number,range,step) is important. It's better to do it with Visualization.\n* Take the advantage of all the given dataset(train and test). Eg. Imputation, Modelling.\n* Feature Engineering is so important, more than hyper-parameter tuning.\n* Overfitting usually happens especially in the relatively small dataset. Complex models not always outperform than simple model.\n* Do not trust the 100% accuracy performance in the leaderboard. They are cheating.\n* Do not be very struggle with the leaderboard performance. As long as you do everything right, the performance should be acceptable(Top 10%,20%).\n* Don't waste too much time to make small improvement (balance!!!). The important part is the process\/method of Analysis and Modelling.\n* Learn from the best. Kernel(Top voted, Top comments) ","0e94aa8c":"<a id=\"3.1\"><\/a>\n## 3.1. What is the distribution of survival?","bc0e9643":"It seems people who have fewer parents or children are likely to survive.","db38e594":"<a id=\"5.2.5\"><\/a>\n### 5.2.5 Random Forest","4da86e9d":"<a id=\"3.8\"><\/a>\n## 3.8. What is the distribution of Fare on survival?","e72a5e89":"<a id=\"2.4.2\"><\/a>\n### 2.4.2. Median imputation ","866a4df5":"<a id=\"3.4\"><\/a>\n## 3.4. What is the distribution of SibSp on survival?","d0e5ced3":"<a id=\"5.2.7\"><\/a>\n### 5.2.7 XGBoost","8b7f3acc":"<a id=\"1.1\"><\/a>\n## 1.1. Load Data","174438df":"* The higher the class level is, the more possible the passenger can survive.","a487c485":"<a id=\"5.2.4\"><\/a>\n### 5.2.4 Decision Tree","8103fa14":"<a id=\"6.3\"><\/a>\n## 6.3. Make Prediction","1e96778f":"<a id=\"2.1.1\"><\/a>\n### Title","b1052391":"1. There are 3 Ports of Embarkation\n2. It seems that there are less Cabin information. There are some people share the same cabin\n3. There are some people have the same ticket(1309-929=380)","056f4417":"## Reference(Thanks for inspiration)\n* https:\/\/www.kaggle.com\/javiervallejos\/titanic-simple-decision-tree-model-score-top-3\/notebook\n* https:\/\/www.kaggle.com\/giorgosfoukarakis\/titanic-from-eda-to-the-power-of-ensembles-top4","a374de2c":"This is the overall missing value of the whole dataset(train + test).","551b2d13":"<a id=\"2.4.1.4\"><\/a>\n### Cabin","7777bed4":"<a id=\"6\"><\/a>\n# 6. Prediction","6d1c36cf":"<a id=\"1\"><\/a>\n# 1. Data Overview","d4a1d904":"## Issues\n* Didn't apply feature scaleing for the needed models(Eg. KNN, Logistic regression). Although, Tree-based models don't need this.\n* Didn't apply parameter tuning for modeling.\n* Could enrich the type of the visualization.","6ca6adc3":"<a id=\"1.3\"><\/a>\n## 1.3. Statistical View ","ec69fb4c":"<a id=\"2.4.1\"><\/a>\n### 2.4.1. Missing Value Exploratory","b4f25b96":"<a id=\"3.2\"><\/a>\n## 3.2. What is the distribution of Sex on survival?","f36b0241":"<a id=\"4\"><\/a>\n# 4. Feature Engineering","c5f15d70":"<a id=\"5.1\"><\/a>\n## 5.1. Train Test Split","b094e094":"# Thanks for reading, have a good day ~","057ba2c4":"<a id=\"4.1\"><\/a>\n## 4.1. Create new features","b9d5c613":"It seems people who have Carbin information are likely to survive.","514d26bc":"<a id=\"5\"><\/a>\n# 5. Modelling","f5e0b360":"<a id=\"6.4\"><\/a>\n## 6.4. Save the Prediction to CSV file","328f0294":"<a id=\"2.3\"><\/a>\n## 2.3. Missing Value Detection","ba5d2ef4":"<a id=\"5.3\"><\/a>\n## 5.3. Model Comparison","1ca48432":"418 is the number of survived passenger in test data. Therefore, it means no Missing value in the dataset.","e7a4dc4b":"This notebook aims to analyze Titanic Disaster and build models to predict survival.  \n* [**Author**](https:\/\/www.linkedin.com\/in\/chi-wang-22a337207\/)\n* [**Dataset**](https:\/\/www.kaggle.com\/c\/titanic\/data)","d32bbb2a":"<a id=\"2.4.1.3\"><\/a>\n### Embarked","63bbddb3":"<a id=\"2.2\"><\/a>\n## 2.2. Drop irrelevant columns","7c161926":"<a id=\"2\"><\/a>\n# 2. Data Preprocessing","98c9ee88":"<a id=\"2.4.1.2\"><\/a>\n### Fare","2ab82a3d":"* Female are more likely to survive than male.","f001eaef":"* The Label(Survived) is imbalanced. The survived passenger makes up 1\/3 of the total number.","c5747634":"People with 1(fewer) sibling seems to have a higer survival rate.","a8a32e81":"<a id=\"3.7\"><\/a>\n## 3.7. What is the distribution of Age on survival?","833fdf8f":"<a id=\"5.2.9\"><\/a>\n### 5.2.9 LGBoost","f8d81e9f":"It seems people who from C(Cherbourg) have a relatively high survival rate.","b8dcea64":"<a id=\"3\"><\/a>\n# 3. Data Analysis\n* Sex, Pclass, SibSp, Parch, Embarked - Pie\n* Age, Fare - Boxplot\n* Survival Rate - Barplot\/Lineplot","76b4f634":"<a id=\"6.2\"><\/a>\n## 6.2. Feature Engineering","504c1e30":"<a id=\"5.2.2\"><\/a>\n### 5.2.2 k-nearest neighbors","05611474":"<a id=\"2.1\"><\/a>\n## 2.1. Extract Potential Information","a2815948":"It seems that the top 5 models are: lr, SVM, GBT, CatBoost, XGBoost"}}