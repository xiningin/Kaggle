{"cell_type":{"1b5d7298":"code","e149c1b4":"code","f23822bd":"code","9655acd9":"code","f6d6b5c7":"code","c05bc937":"code","2a9798f6":"code","53fdce89":"code","8ff979a8":"code","3c879b0d":"code","85d19d26":"code","73ecea68":"code","390cbceb":"code","1a38f93a":"code","1863830f":"code","210ae56f":"code","679851ed":"code","d0b1a14f":"code","25c9da1b":"code","0fa2969b":"code","14792690":"code","dfb34db7":"code","58ffaf4c":"code","b4ee11db":"code","4b1c3ddf":"code","4b67362a":"code","3a25b322":"code","c4a9a352":"code","3da89140":"code","d016954f":"code","f6e2b703":"code","b344b875":"code","aed67ecd":"code","dce2cf71":"code","275b6115":"markdown","87c39b94":"markdown","657c3237":"markdown","e6114d67":"markdown","1d2303d7":"markdown","fe5e5027":"markdown","6ff5aaae":"markdown","534ca11a":"markdown","ec520974":"markdown"},"source":{"1b5d7298":"# Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport tensorflow as tf","e149c1b4":"# Reading in the data\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')","f23822bd":"# Function to reduce memory\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                #el\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","9655acd9":"# Reducing memory usage\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","f6d6b5c7":"# Viewing the summary statistics of the data\n\n# train.describe().T","c05bc937":"# Viewing the info of the \n\n# train.info()","2a9798f6":"# Viewing the number of categories in target variable\n\ntrain['Cover_Type'].nunique()","53fdce89":"# Viewing the number of observations per category in target variable\n\ntrain['Cover_Type'].value_counts()","8ff979a8":"# Remove columns 'Soil_Type7', 'Soil_Type15'\n\ntrain.drop(['Soil_Type7', 'Soil_Type15'], inplace=True, axis=1)\ntest.drop(['Soil_Type7', 'Soil_Type15'], inplace=True, axis=1)","3c879b0d":"# Extracting soil columns\n\nsoil_columns = [col for col in train.columns if 'Soil' in col]","85d19d26":"# Undummying the Soil_types\n\ntrain['soil_type'] = train[soil_columns].idxmax(axis=1)\ntest['soil_type'] = test[soil_columns].idxmax(axis=1)","73ecea68":"# Calculating the fequency encoding\n\nsoil_map = pd.Series(train['soil_type'].value_counts()\/train.shape[0]).to_dict()","390cbceb":"# Applying the frequency encoding\n\ntrain['soil_type'] = train['soil_type'].map(soil_map)\ntest['soil_type'] = test['soil_type'].map(soil_map)","1a38f93a":"# Dropping all the 'Soil-Type' columns\n\ntrain = train.drop(soil_columns, axis=1)\ntest = test.drop(soil_columns, axis=1)\ntrain.head()","1863830f":"# Extracting winderness columns\n\nwild_columns = [col for col in train.columns if 'Wild' in col]","210ae56f":"# Undummying the wilderness_types\n\ntrain['wild_type'] = train[wild_columns].idxmax(axis=1)\ntest['wild_type'] = test[wild_columns].idxmax(axis=1)","679851ed":"# Calculating the fequency encoding\n\nwild_map = pd.Series(train['wild_type'].value_counts()\/train.shape[0]).to_dict()","d0b1a14f":"# Applying the frequency encoding\n\ntrain['wild_type'] = train['wild_type'].map(wild_map)\ntest['wild_type'] = test['wild_type'].map(wild_map)","25c9da1b":"# Dropping all the 'Soil-Type' columns\n\ntrain = train.drop(wild_columns, axis=1)\ntest = test.drop(wild_columns, axis=1)\ntrain.head()","0fa2969b":"# Clipping the hillshade columns between 0 and 255\n\nhillshade_columns = [col for col in train.columns if 'Hillshade' in col]\n\nfor col in hillshade_columns:\n    train[col] = train[col].clip(0,255)\n    test[col] = test[col].clip(0,255)","14792690":"# Changing the range of Aspect to fall between 0 and 359\n\ntrain['Aspect'] = train['Aspect'].apply(lambda row: row%360)\ntest['Aspect'] = test['Aspect'].apply(lambda row: row%360)\ntrain['Aspect'].describe()","dfb34db7":"# Getting the features and target variables\n\nfeatures = [col for col in train.columns if col not in ['Id', 'Cover_Type']]\ntarget = 'Cover_Type'","58ffaf4c":"# Label encoding the target variable\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ntrain[target] = le.fit_transform(train[target])","b4ee11db":"# Removing that single observation that has tree type '5' (or '4' after LabelEncoding)\n\ntrain = train.loc[train['Cover_Type'] != 4,]","4b1c3ddf":"# # Saving preprocessed data\n\n# train.to_csv('train_reduced.csv', index=False)\n# test.to_csv('test_reduced.csv', index=False)","4b67362a":"# Viewing the shape of the features\n\ntrain[features].shape","3a25b322":"# Splitting the data into train and test splot\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    train[features], \n    train[target],\n    stratify=train[target],\n    test_size=0.2, \n    random_state=0\n)\nprint(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of y_train: {y_train.shape}')\nprint(f'Shape of X_valid: {X_valid.shape}')\nprint(f'Shape of y_valid: {y_valid.shape}')","c4a9a352":"# Scaling the data by fitting on X_train and scaling the rest\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_valid = scaler.transform(X_valid)\nt = scaler.transform(test[features])","3da89140":"# Function that creates a TF sequential model\n\ndef get_model():\n    tf.keras.backend.clear_session()\n\n    ## Creating a Sequential Model\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(512, input_shape=(None,12), activation='relu'),\n        tf.keras.layers.Dense(256, activation='relu'),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(7, activation = 'softmax')\n    ])\n    \n    ## Compile \n    model.compile(\n        optimizer=\"adam\",\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=['acc']\n    )\n    \n    return model","d016954f":"# K-fold Cross Validation model evaluation\n\nfrom sklearn.metrics import accuracy_score\n\nX = X_train\ny = y_train.values\n\nFOLDS = 5\nEPOCHS = 5\nBATCH_SIZE = 1024\n\ntest_preds = np.zeros((1, 1))\nscores = []\n\ncv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=0)\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n    X_t, X_v = X[train_idx], X[val_idx]\n    y_t, y_v = y[train_idx], y[val_idx]\n    \n        # Generate a print\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold} ...')\n    \n    model = get_model()\n\n    # Fit data to model\n    model.fit(\n        X_t,\n        y_t,\n        validation_data=(X_v, y_v),\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        verbose=2\n    )\n    \n    y_pred = np.argmax(model.predict(X_v), axis=1)\n    score = accuracy_score(y_v, y_pred)\n    scores.append(score)","f6e2b703":"# Printing the results from K-Fold\n\nprint(f'Accuracy for each fold: {scores}')\nprint(f'Mean of all the folds: {np.mean(scores):.4f}')\nprint(f'Standard Deviation of the folds: {np.std(scores):.4f}')","b344b875":"# Predicting on the test set\n\npreds = model.predict(t)","aed67ecd":"# Reversing the label encoder\n\nfinal_preds = le.inverse_transform(preds.argmax(axis=1))","dce2cf71":"# Creating a submission file\n\nsubmission = pd.DataFrame({'Id': test['Id'], 'Cover_Type': final_preds })\nsubmission.to_csv('submission.csv', index=False)","275b6115":"Compared to Version 6, model has not imporved much. Makes me wonder if the wilderness columns are significant to predict the tree type. Some next steps can be to improve the NN architecture. And possibly apply blending.","87c39b94":"This is my journey to tackle this Kaggle competition. Every version, I will try to update my work to improve my accuracy. Let me know how I am doing and if you have any suggestions!\n\n* Version 2\n    * Run a Baseline model\n* Version 5\n    * implementing feature engineering\n        * removed columns: 'Soil_Type7' and 'Soil_Type15'\n        * clipped hillshade variables to fit in range [0-255]\n        * tweaked 'Aspect' to have a range from 0-360 degrees\n        * Resources: https:\/\/www.kaggle.com\/ambrosm\/tpsdec21-01-keras-quickstart\n    * train-valid split\n        * changed train-valid split from 70\/30 to 80\/20\n        * added straify parameter in train_test_split\n    * model\n        * added two more layers\n        * changed activation function of hidden layers to ReLU\n* Version 6\n    * scale the features\n    * un-dummify the 'Soil-Type's and apply frequency encoding\n* Version 7\n    * un-dummify the 'Wilderness-Type's and apply frequency encoding\n    * changed batch size from 2048 -> 1024\n    * removed unnecessary comments\/ code and added more documentation","657c3237":"## Feature Engineering","e6114d67":"### Tensorflow Model","1d2303d7":"Hillshade is an \"image\" that ranges from 0-255. However some of the hillshade values are less than 0 or greater than 255. We will make an assumption that those were data entry errors and will clip them. If it is less than 0, we will set it to sero. If it is greater than 255, set it to 255.\n\nSome additional thoughts:\n* Set values under 0 to 0 and values greater than 255 to 255 for all Hillshade variables\n* Is clipping the best way to procede? Try just scaling instead of clipping\n* Remove the hillshade data that are NOT within the range between 0, 255","fe5e5027":"### Soil Variables","6ff5aaae":"It will be a good idea to remove 'Soil_Type7' and 'Soil_Type15'because it is 0's for all observations. Therefore it is not informative and might add noise to the model.","534ca11a":"### Wilderness Variables","ec520974":"## Scaling the data"}}