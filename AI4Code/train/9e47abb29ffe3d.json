{"cell_type":{"3434fc73":"code","68ecc20e":"code","c20828cc":"code","fba577f5":"code","b1a8226e":"code","c980a9c8":"code","d90bc989":"code","50bb3798":"code","af06bf88":"code","b386683a":"code","a1a102ea":"code","ffe5d241":"code","947db23e":"code","a76cd48b":"code","ac3207c3":"code","d852aa08":"code","4d25cabe":"code","10d646f2":"code","5353f8e3":"code","22e12e34":"code","90b4e48a":"code","3ecd4310":"code","540b7dc4":"code","c0b19215":"code","7ab5069a":"code","807031ff":"code","b7fb697d":"code","e27b91f8":"code","1a23a6cb":"code","80919fcd":"code","09c05c13":"code","542ac3c2":"code","03432420":"code","c584e56f":"code","00ee44c6":"code","3586c0f8":"code","3d0cb565":"code","da3e2589":"code","7c1cd362":"code","b10ab87c":"code","c8850ad9":"code","d83f852b":"code","2856d4b1":"code","e1747895":"code","06a2d656":"code","83bb455e":"code","891ec927":"code","742eb8b4":"code","e3862ee6":"code","ac27d08b":"code","f3c0257c":"code","17db72d4":"code","956fe6ec":"code","50a75c2b":"code","9fc90c5b":"code","99e2c796":"code","be453891":"code","32def331":"code","e1ffae24":"code","25516193":"code","720da122":"code","51640e9a":"code","3fe2f279":"code","1664c7bf":"code","0e8ca3a4":"code","03547a62":"code","0cd029f8":"code","152a226f":"code","43f7cac2":"code","74d79cfd":"code","4ebd6b95":"code","e3235edc":"code","e5ea613f":"code","55bf62c9":"code","cfbe62cb":"code","8d254801":"code","1e469ce6":"code","67564962":"code","38a615df":"code","2449a469":"code","67bd5328":"code","0f4252a1":"markdown","8d6c1a3c":"markdown","bca61af7":"markdown","def52b30":"markdown","f655a4f4":"markdown","77832bd7":"markdown","2b94b993":"markdown","f12436f1":"markdown","e5cb5b89":"markdown","3e11f2a7":"markdown","6cbb9042":"markdown","5497ab98":"markdown","fe8facc7":"markdown","e8f083a7":"markdown","0eeed1fb":"markdown","ca04476d":"markdown","f554335f":"markdown","506f8f66":"markdown","bee77a4c":"markdown","c977a001":"markdown","ee623fe6":"markdown","028c102e":"markdown","ad959a93":"markdown","c414c6be":"markdown","22b87d6f":"markdown","065ffb2e":"markdown","dd9b1d33":"markdown","4a371f6d":"markdown","254fab78":"markdown","43d25710":"markdown","9f3d7b9e":"markdown","47d9808a":"markdown","7995dace":"markdown","a239a96c":"markdown","d7b39bec":"markdown","5f1445f4":"markdown","ff258c82":"markdown","d3524808":"markdown","b0c5087d":"markdown","930186ef":"markdown","2a16f853":"markdown","b776592b":"markdown","3b2dc9cc":"markdown","53ffd8ca":"markdown","82188167":"markdown","f8991d2a":"markdown","69476fd8":"markdown","b8d85129":"markdown","97452867":"markdown","05fec537":"markdown"},"source":{"3434fc73":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import feature_column\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import model_selection\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import KNNImputer","68ecc20e":"def save_results(Survived, test, path):\n    submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": Survived\n    })\n    submission.to_csv(path, index=False)","c20828cc":"def evaulate_and_save(\n    model, \n    validation_features, \n    validation_targets, \n    test_features, \n    save_path, \n    best_score, \n    best_path, \n    columns = None\n):\n    if columns is None:\n        feature_columns = validation_features.columns\n    else:\n        feature_columns = columns\n    y_pred = model.predict(validation_features[feature_columns])\n    if y_pred.dtype != int:\n        if y_pred.shape[-1] == 2:\n            y_pred = np.argmax(y_pred, axis=-1)\n        if y_pred.shape[-1] == 1:\n            y_pred = np.array(y_pred > 0.5, dtype=int)\n    y_pred = y_pred.reshape(-1)\n    score = sklearn.metrics.accuracy_score(validation_targets, y_pred)\n    f1 = sklearn.metrics.f1_score(validation_targets, y_pred)\n    print(\"Accuracy Score:\", score)\n    print(\"Classification Report:\")\n    print(sklearn.metrics.classification_report(validation_targets, y_pred))\n    Survived = model.predict(test_features[feature_columns])\n    if Survived.dtype != int:\n        if Survived.shape[-1] == 2:\n            Survived = np.argmax(Survived, axis=-1)\n        if Survived.shape[-1] == 1:\n            Survived = np.array(Survived > 0.5, dtype=int)\n    Survived = np.array(Survived, dtype=int).reshape(-1)\n    save_results(Survived, test_features, save_path)\n    if score > best_score:\n        best_score = score\n        best_path = save_path\n    return best_score, best_path","fba577f5":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","b1a8226e":"train.head()","c980a9c8":"test.head()","d90bc989":"train.isnull().sum()","50bb3798":"test.isnull().sum()","af06bf88":"categorical_imputation_strategy = [\"mode\", \"unknown\", \"knn\"][1]\nnumerical_imputation_strategy = [\"mean\", \"median\", \"knn\"][2]","b386683a":"if categorical_imputation_strategy == \"mode\":\n    train[\"Cabin\"] = train[\"Cabin\"].replace(np.NAN,  train[\"Cabin\"].mode()[0])\n    train[\"Embarked\"] = train[\"Embarked\"].replace(np.NAN, train[\"Embarked\"].mode()[0])\nif categorical_imputation_strategy == \"unknown\":\n    train[\"Cabin\"] = train[\"Cabin\"].replace(np.NAN,  \"Unknown\")\n    train[\"Embarked\"] = train[\"Embarked\"].replace(np.NAN, \"Unknown\")\n    test[\"Cabin\"] = test[\"Cabin\"].replace(np.NAN, \"Unknown\")\nif categorical_imputation_strategy == \"knn\":\n    print(\"To be continued\")\nif numerical_imputation_strategy == \"mean\":\n    train[\"Age\"] = train[\"Age\"].replace(np.NAN, train[\"Age\"].mean())\n    test[\"Age\"] = test[\"Age\"].replace(np.NAN, test[\"Age\"].mean())\n    test[\"Fare\"] = test[\"Fare\"].replace(np.NAN, test[\"Fare\"].mean())\nif numerical_imputation_strategy == \"median\":\n    train[\"Age\"] = train[\"Age\"].replace(np.NAN, train[\"Age\"].median())\n    test[\"Age\"] = test[\"Age\"].replace(np.NAN, test[\"Age\"].median())\n    test[\"Fare\"] = test[\"Fare\"].replace(np.NAN, test[\"Fare\"].median())\nif numerical_imputation_strategy == \"knn\":\n    imputer = KNNImputer(n_neighbors=5)\n    columns = [\"Age\", \"Fare\", \"SibSp\", \"Parch\"]\n    train[columns] = imputer.fit_transform(train[columns])\n    test[columns] = imputer.transform(test[columns])","a1a102ea":"cabin_labels = sorted(set(list(train[\"Cabin\"].unique()) + list(test[\"Cabin\"].unique())))\nprint(cabin_labels[:30])","ffe5d241":"train[\"Cabin_type\"] = train[\"Cabin\"].apply(lambda cabin: cabin[0])\ntest[\"Cabin_type\"] = test[\"Cabin\"].apply(lambda cabin: cabin[0])","947db23e":"train[\"family_member_size\"] = 1 + train[\"SibSp\"] + train[\"Parch\"]\ntest[\"family_member_size\"] = 1 + test[\"SibSp\"] + test[\"Parch\"]","a76cd48b":"def convert_faimly_member_size(size):\n    if size == 1:\n        return \"single\"\n    elif size < 5:\n        return \"medium\"\n    else:\n        return \"large\"\nshould_add_family_member_type = False\nif should_add_family_member_type:\n    for data in [train, test]:\n        data[\"family_member_type\"] = train[\"family_member_size\"].apply(convert_faimly_member_size)","ac3207c3":"categorical_features = [\"Sex\", \"Cabin_type\", \"Embarked\"]\nif should_add_family_member_type:\n    categorical_features.append(\"family_member_type\")\ncategorical_label_dictionary = dict()\nfor feature in categorical_features:\n    unique_labels = sorted(set(list(train[feature].unique()) + list(test[feature].unique())))\n    for data in [train, test]:\n        categorical_label_dictionary[feature] = unique_labels\n        data[feature + \"_value\"] = data[feature].apply(lambda item: unique_labels.index(item))","d852aa08":"train.head(10)","4d25cabe":"train.info()","10d646f2":"train.describe()","5353f8e3":"train.corr()[\"Survived\"].sort_values(key=lambda x: abs(x), ascending=False)","22e12e34":"related_columns = list(train.corr()[train.corr()[\"Survived\"].abs() > 0.05].index)\nrelated_columns.remove(\"Survived\")\nprint(related_columns)","90b4e48a":"sns.countplot(x=\"Sex\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different Gender\")\nplt.show()","3ecd4310":"sns.histplot(x=\"Age\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different Age\")\nplt.show()","540b7dc4":"train.groupby(\"Pclass\")[\"Survived\"].mean()","c0b19215":"sns.countplot(x=\"Pclass\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different Pclass\")\nplt.show()","7ab5069a":"plt.figure(figsize=(15, 7))\nsns.histplot(x=\"Fare\", hue=\"Survived\", bins=20, kde=True, data=train)\nplt.title(\"Survival of different Fare\")\nplt.show()","807031ff":"train.groupby(\"Cabin_type\")[\"Survived\"].mean()","b7fb697d":"sns.countplot(x=\"Cabin_type\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different Cabin\")\nplt.show()","e27b91f8":"train.groupby(\"Embarked\")[\"Survived\"].mean()","1a23a6cb":"sns.countplot(x=\"Embarked\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different Embarked\")\nplt.show()","80919fcd":"train.groupby(\"SibSp\")[\"Survived\"].mean()","09c05c13":"sns.countplot(x=\"SibSp\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different SibSp\")\nplt.show()","542ac3c2":"train.groupby(\"Parch\")[\"Survived\"].mean()","03432420":"sns.countplot(x=\"Parch\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different SibSp\")\nplt.show()","c584e56f":"sns.countplot(x=\"family_member_size\", hue=\"Survived\", data=train)\nplt.title(\"Survival of Family Member Size\")\nplt.show()","00ee44c6":"if should_add_family_member_type:\n    sns.countplot(x=\"family_member_type\", hue=\"Survived\", data=train)\n    plt.title(\"Survival of Family Member Type\")\n    plt.show()","3586c0f8":"train_test = pd.concat([train, test])\ntrain_test.head()","3d0cb565":"categorical_columns_to_one_hot = [\"Sex\", \"Cabin_type\", \"Embarked\"]\nif should_add_family_member_type:\n    categorical_columns_to_one_hot.append(\"family_member_type\")\nfor feature in categorical_columns_to_one_hot:\n    items = pd.get_dummies(train_test[feature + \"_value\"])\n    labels = categorical_label_dictionary[feature]\n    items.columns = [feature + \"_\" + labels[column] for column in list(items.columns)]\n    train_test[items.columns] = items\n    train_test.pop(feature + \"_value\")","da3e2589":"mean_value = train_test.mean()\nstd_value = train_test.std()\nmean_value.pop(\"Survived\")\n_ = std_value.pop(\"Survived\")","7c1cd362":"train_test.head()","b10ab87c":"for column in [\"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Cabin_type\", \"Embarked\", \"family_member_size\", \"family_member_type\"]:\n    if column in train_test.columns:\n        train_test.pop(column)","c8850ad9":"train_features = train_test.iloc[0: len(train)]\ntest_features = train_test.iloc[len(train):]","d83f852b":"test_features.head()","2856d4b1":"train_features.pop(\"PassengerId\")\ntest_features.pop(\"Survived\")\ntrain_features.head()","e1747895":"test_features.head()","06a2d656":"validation_split = 0.2","83bb455e":"train_features, validation_features = model_selection.train_test_split(train_features, test_size=validation_split)\nprint(train_features.shape, validation_features.shape)","891ec927":"train_features","742eb8b4":"should_balance = False\nbatch_size = 32\nnumber_batch_per_category = 100\nif should_balance == True:\n    survived = train_features[train_features.Survived == 1]\n    not_survived = train_features[train_features.Survived == 0]\n    survived_indices = list(np.random.choice(len(survived), size=number_batch_per_category * batch_size))\n    not_survived_indices = list(np.random.choice(len(not_survived), size=number_batch_per_category * batch_size))\n    survived_features = survived.iloc[survived_indices]\n    not_survived_features = not_survived.iloc[not_survived_indices]\n    print(not_survived_features.shape)\n    train_features = pd.concat([survived_features, not_survived_features])\n    train_features = sklearn.utils.shuffle(train_features)\n    train_targets = train_features.pop(\"Survived\")\n    validation_targets = validation_features.pop(\"Survived\")\n    # 0.95 ~ 1.05\n    scale = 1 + 0.1 * (np.random.rand(train_features.shape[0], train_features.shape[1]) - 0.5)\n    train_features =  train_features * scale\nelse:\n    train_targets = train_features.pop(\"Survived\")\n    validation_targets = validation_features.pop(\"Survived\")","e3862ee6":"train_features.describe()","ac27d08b":"train_features.head()","f3c0257c":"data_scaling_strategies = [\"none\", \"max\", \"standard\"]\ndata_scaling_strategy = data_scaling_strategies[2]\nif data_scaling_strategy == data_scaling_strategies[1]:\n    features_max = pd.concat([train_features, validation_features]).max()\n    train_features = train_features \/ features_max\n    validation_features = validation_features \/ features_max\n    test_features[train_features.columns] = test_features[train_features.columns] \/ features_max\nif data_scaling_strategy == data_scaling_strategies[2]:\n    for data in [train_features, validation_features, test_features]:\n        columns_to_scale = [\"Age\", \"Fare\"]\n        data.loc[:, columns_to_scale] = (data.loc[:, columns_to_scale]  - mean_value[columns_to_scale]) \/ std_value[columns_to_scale]\nprint(train_features.shape)\nprint(test_features.shape)","17db72d4":"best_score = 0\nbest_path = \"\"","956fe6ec":"model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(train_features.shape[1])),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(2, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=10)\nhistory = model.fit(\n    train_features, train_targets, \n    epochs=400, validation_data=(validation_features, validation_targets), \n    callbacks=[early_stop],\n    verbose=0\n)\npd.DataFrame(history.history).plot()","50a75c2b":"best_score, best_path = evaulate_and_save(\n    model, \n    validation_features, \n    validation_targets, \n    test_features,\n    \"submission_dnn.csv\",\n    best_score, \n    best_path,\n    columns=validation_features.columns\n)","9fc90c5b":"categorical_feature_names = [\"Pclass\", \"Sex_value\", \"Embarked_value\", \"Cabin_type_value\"]\nif should_add_family_member_type:\n    categorical_feature_names.append(\"family_member_type_value\")\nnumerical_feature_names = [\"Age\", \"Fare\", \"SibSp\", \"Parch\"]\ncategorical_features = [\n    feature_column.indicator_column(\n        feature_column.categorical_column_with_vocabulary_list(key, sorted(list(train[key].unique())))\n    ) for key in categorical_feature_names\n]\nnumerical_features = [feature_column.numeric_column(key) for key in numerical_feature_names]\ninput_dictionary = dict()\ninputs = dict()\nfor item in numerical_features:\n    inputs[item.key] = tf.keras.layers.Input(name=item.key, shape=())\nfor item in categorical_features:\n    inputs[item.categorical_column.key] = tf.keras.layers.Input(name=item.categorical_column.key, shape=(), dtype=\"int32\")","99e2c796":"def features_and_labels(row_data):\n    label = row_data.pop(\"Survived\")\n    features = row_data\n    return features, label\n\ndef create_dataset(pattern, epochs=1, batch_size=32, mode='eval'):\n    dataset = tf.data.experimental.make_csv_dataset(\n        pattern, batch_size\n    )\n    dataset = dataset.map(features_and_labels)\n    if mode == 'train':\n        dataset = dataset.shuffle(buffer_size=128).repeat(epochs)\n    dataset = dataset.prefetch(1)\n    return dataset\n\ndef create_test_dataset(pattern, batch_size=32):\n    dataset = tf.data.experimental.make_csv_dataset(\n        pattern, batch_size\n    )\n    dataset = dataset.map(lambda features: features)\n    dataset = dataset.prefetch(1)\n    return dataset","be453891":"train_data, val_data = train_test_split(\n    train[categorical_feature_names + numerical_feature_names + [\"Survived\"]],\n    test_size=validation_split,\n    random_state=np.random.randint(0, 1000)\n)\ntrain_data.to_csv(\"train_data.csv\", index=False)\nval_data.to_csv(\"val_data.csv\", index=False)\ntest[categorical_feature_names + numerical_feature_names].to_csv(\"test_data.csv\", index=False)\ntrain_dataset = create_dataset(\"train_data.csv\", batch_size=batch_size, mode='train')\nval_dataset = create_dataset(\"val_data.csv\", batch_size=val_data.shape[0], mode='eval').take(1)\ntest_dataset = create_test_dataset(\"test_data.csv\", batch_size = test.shape[0]).take(1)","32def331":"def build_dnn_with_dense_features():\n    deep = tf.keras.layers.DenseFeatures(numerical_features + categorical_features, name='deep')(inputs)\n    deep = tf.keras.layers.Dense(16, activation='relu')(deep)\n    deep = tf.keras.layers.Dropout(0.3)(deep)\n    deep = tf.keras.layers.Dense(16, activation='relu')(deep)\n    deep = tf.keras.layers.Dropout(0.3)(deep)\n    deep = tf.keras.layers.Dense(16, activation='relu')(deep)\n    deep = tf.keras.layers.Dropout(0.3)(deep)\n    deep = tf.keras.layers.Dense(16, activation='relu')(deep)\n    deep = tf.keras.layers.Dropout(0.3)(deep)\n    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(deep)\n    model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)\n    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model","e1ffae24":"dnn_dense_features = build_dnn_with_dense_features()\ntf.keras.utils.plot_model(dnn_dense_features, show_shapes=False, rankdir='LR')","25516193":"epochs = 400\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=10)\nsteps_per_epoch = train_data.shape[0] \/\/ batch_size\nhistory = dnn_dense_features.fit(\n    train_dataset, \n    steps_per_epoch=steps_per_epoch,\n    validation_data=val_dataset,\n    epochs=epochs,\n    callbacks=[early_stop],\n    verbose=0\n)\npd.DataFrame(history.history).plot()","720da122":"y_pred =  np.array(dnn_dense_features.predict(val_dataset) > 0.5, dtype=int).reshape(-1)\nscore = accuracy_score(val_data[\"Survived\"], y_pred)\nprint(\"Accuracy score:\", score)\nprint(sklearn.metrics.classification_report(val_data[\"Survived\"], y_pred))\nSurvived = np.argmax(dnn_dense_features.predict(test_dataset), axis=-1).reshape(-1)\nprint(Survived.shape)\npath = \"submission_dnn_dense_features_model.csv\"\nsave_results(Survived, test, path)\nif score > best_score:\n    best_score = score\n    best_path = path","51640e9a":"logitistc_related_columns = list(train.corr()[train.corr()[\"Survived\"].abs() > 0.2].index)\nlogitistc_related_columns.remove(\"Survived\")\nlogitistc_related_columns","3fe2f279":"from sklearn.linear_model import LogisticRegression\nbest_logit = None\nbest_solver = \"\"\nbest_logit_score = 0\nlogit_train_features, logit_val_features = train_test_split(train[logitistc_related_columns +  [\"Survived\"]], test_size=0.2, random_state=48)\nlogit_train_targets = logit_train_features.pop(\"Survived\")\nlogit_val_targets = logit_val_features.pop(\"Survived\")\nfor solver in ['newton-cg', 'lbfgs', 'liblinear']:\n    logit = LogisticRegression(solver=solver)\n    logit.fit(logit_train_features, logit_train_targets)\n    score = logit.score(logit_val_features, logit_val_targets)\n    if score > best_logit_score:\n        best_solver = solver\n        best_logit_score = score\n        best_logit = logit\nprint(\"Best Solver:\", best_solver, \"Score:\", best_logit_score)","1664c7bf":"best_score, best_path = evaulate_and_save(\n    best_logit, \n    logit_val_features, \n    logit_val_targets, \n    test, \n    \"submission_logit.csv\", \n    best_score, \n    best_path,\n    columns=logitistc_related_columns\n)","0e8ca3a4":"best_algorithm = \"\"\nbest_knn_score = 0\nbest_knn = None\nbest_n = 2\nfor n in range(2, 10):\n    knn = KNeighborsClassifier(n, algorithm='ball_tree')\n    knn.fit(train_features, train_targets)\n    score = knn.score(validation_features, validation_targets) \n    if score > best_knn_score:\n        best_n = n\n        best_knn_score = score\n        best_knn = knn\nprint(\"Best KNN Score: \", best_knn_score, \"Model:\", best_knn, \"Best N:\", best_n)","03547a62":"best_score, best_path = evaulate_and_save(\n    best_knn, validation_features, validation_targets, test_features, \n    \"submission_knn.csv\", best_score, best_path\n)","0cd029f8":"from sklearn.tree import DecisionTreeClassifier\nbest_tree = None\nbest_tree_score = 0\nfor max_depth in range(6, 30):\n    tree = sklearn.tree.DecisionTreeClassifier(max_depth=max_depth)\n    tree.fit(train_features, train_targets)\n    score = tree.score(validation_features, validation_targets)\n    if score > best_tree_score:\n        best_tree_score = score\n        best_tree = tree\nprint(\"Best Decision Tree Score: \", best_tree_score, \"Model:\", best_tree)","152a226f":"best_score, best_path = evaulate_and_save(\n    best_tree, validation_features, validation_targets, test_features, \n    \"submission_tree.csv\", best_score, best_path\n)","43f7cac2":"best_gbc_score = 0\nbest_depth = 5\nbest_n_estimators = 5\nbest_learning_rate = 0.1\nbest_gbc_model = None\nfor learning_rate in list(np.arange(0.05, 0.15, 0.01)):\n    gbc = GradientBoostingClassifier(\n            n_estimators=best_n_estimators, \n            learning_rate=learning_rate, \n            max_depth=best_depth, \n            random_state=np.random.randint(1, 1000)\n    )\n    gbc.fit(train_features, train_targets)\n    score = gbc.score(validation_features, validation_targets)\n    if score > best_gbc_score:\n        best_learning_rate = learning_rate\n        best_gbc_score = score\n        best_gbc_model = gbc\nprint(\"Best Learning Rate:\", best_learning_rate)\nfor depth in range(5, 20):\n    gbc = GradientBoostingClassifier(\n            n_estimators=best_n_estimators, \n            learning_rate=best_learning_rate, \n            max_depth=depth, \n            random_state=np.random.randint(1, 1000)\n    )\n    gbc.fit(train_features, train_targets)\n    score = gbc.score(validation_features, validation_targets)\n    if score > best_gbc_score:\n        best_depth = depth\n        best_gbc_score = score\n        best_gbc_model = gbc\nprint(\"Best Depth:\", best_depth)\nfor n_estimators in range(5, 15):\n    gbc = GradientBoostingClassifier(\n            n_estimators=n_estimators, \n            learning_rate=best_learning_rate, \n            max_depth=best_depth, \n            random_state=np.random.randint(1, 1000)\n    )\n    gbc.fit(train_features, train_targets)\n    score = gbc.score(validation_features, validation_targets)\n    if score > best_gbc_score:\n        best_n_estimators = n_estimators\n        best_gbc_score = score\n        best_gbc_model = gbc\nprint(\"Best Number of Estimator:\", best_depth)\nprint(\"Best Gradient Boosting Classifier Score:\", best_gbc_score, \" Model:\", best_gbc_model)","74d79cfd":"best_score, best_path = evaulate_and_save(\n    best_gbc_model, validation_features, validation_targets, test_features, \n    \"submission_gbc.csv\", best_score, best_path\n)","4ebd6b95":"best_forest = None\nbest_max_depth = 8\nbest_n_estimators = 15\nbest_forest_score = 0\nprint(\"Find best number of estimators\")\nfor n_estimators in list(range(3, 40, 2)):\n    forest = RandomForestClassifier(\n        n_estimators=n_estimators, \n        max_depth=best_max_depth, \n        random_state=np.random.randint(1, 1000)\n    )\n    forest.fit(train_features, train_targets)\n    score = forest.score(validation_features, validation_targets)\n    print(\"Score: \", score)\n    if score > best_forest_score:\n        best_n_estimators = n_estimators\n        best_forest_score = score\n        best_forest = forest\nprint(\"Best Number of Estimator:\", best_n_estimators)\nfor max_depth in range(4, 15):\n    forest = RandomForestClassifier(\n        n_estimators=best_n_estimators, \n        max_depth=max_depth, \n        random_state=np.random.randint(1, 1000)\n    )\n    forest.fit(train_features, train_targets)\n    score = forest.score(validation_features, validation_targets)\n    print(\"Score: \", score)\n    if score > best_forest_score:\n        best_max_depth = max_depth\n        best_forest_score = best_score\n        best_forest = forest\nprint(\"Best Max Depth:\", best_max_depth,\"\\nBest score:\", best_forest_score)","e3235edc":"best_score, best_path = evaulate_and_save(\n    best_forest, validation_features, validation_targets, test_features, \n    \"submission_forest.csv\", best_score, best_path\n)","e5ea613f":"kmeans = KMeans(n_clusters=2)\nkmeans.fit(train_features, train_targets)","55bf62c9":"best_score, best_path = evaulate_and_save(\n    kmeans, validation_features, validation_targets, test_features, \n    \"submission_kmeans.csv\", best_score, best_path\n)","cfbe62cb":"def get_value(key1, key2, value, parameters, best_index):\n    return parameters[key1][best_index[key1]] if key1 != key2 else value\ndef find_best_model_with_xgboost(\n    train_features, \n    train_targets,\n    validation_features,\n    validation_targets,\n    parameters,\n    columns = None\n):\n    train_f = train_features\n    val_f = validation_features\n    if columns != None:\n        train_f = train_features[columns]\n        val_f = validation_features[columns]\n    else: \n        train_f = train_features\n        val_f = validation_features\n    all_keys = parameters.keys()\n    best_index = {key: 0 for key in all_keys}\n    best_xgb_score = 0\n    best_xgb_model = None\n    for key in all_keys:\n        values = parameters[key]\n        current_best_model = None\n        current_best_score = 0\n        for index, value in enumerate(values):\n            learning_rate = get_value(\"learning_rate\", key, value, parameters, best_index)\n            max_depth = get_value(\"max_depth\", key, value, parameters, best_index)\n            gamma = get_value(\"gamma\", key, value, parameters, best_index)\n            xgb = XGBClassifier(\n                max_depth=max_depth,\n                learning_rate=learning_rate,\n                gamma=gamma\n            )\n            xgb.fit(\n                train_f, \n                train_targets, \n                early_stopping_rounds=10, \n                eval_metric=\"logloss\", \n                eval_set=[(val_f, validation_targets)], \n                verbose=False\n            )\n            score = xgb.score(validation_features, validation_targets)\n            if score > current_best_score:\n                current_best_score = score\n                current_best_model = xgb\n                best_index[key] = index\n            if score > best_xgb_score: \n                best_xgb_score = score\n                best_xgb_model = xgb\n    return best_xgb_model, best_xgb_score","8d254801":"from xgboost import XGBClassifier\nhyper_parameters = {\n    \"max_depth\": list(range(5, 15)),\n    \"learning_rate\": [0.1, 0.15, 0.2, 0.25, 0.3],\n    \"gamma\": [0.5, 1, 1.5, 2.0]\n}\nbest_xgb_score, best_xgb_model = find_best_model_with_xgboost(\n    train_features, \n    train_targets,\n    validation_features,\n    validation_targets,\n    hyper_parameters\n)\nprint(\"Best Model:\", best_xgb_model, \" Score: \", best_xgb_score)","1e469ce6":"best_score, best_path = evaulate_and_save(\n    best_xgb_score, validation_features, validation_targets, test_features, \n    \"submission_xgb.csv\", best_score, best_path\n)","67564962":"from catboost import CatBoostClassifier\ncat = CatBoostClassifier()\ncat.fit(train_features, train_targets, verbose=False)","38a615df":"best_score, best_path = evaulate_and_save(\n    cat, validation_features, validation_targets, test_features, \n    \"submission_cat.csv\", best_score, best_path\n)","2449a469":"print(\"Best path:\", best_path)\nprint(\"Best Score:\", best_score)","67bd5328":"submission = pd.read_csv(best_path)\nprint(submission.head(10))\nsubmission.to_csv(\"submission.csv\", index=False)","0f4252a1":"### Remove unused columns","8d6c1a3c":"### Using TensorFlow DNN","bca61af7":"### Using Decision Tree Classifier","def52b30":"## Data Preprocessing","f655a4f4":"## Import datasets","77832bd7":"### Balance Training dataset\nLet's balance the training dataset and add some noise to data. I will add a Toggle here to control whether to balance the dataset.","2b94b993":"## Model Development & Evaluation\nI will try different Models and use results from best Model.","f12436f1":"**Save results**","e5cb5b89":"Let's see after we preprocess, what does the data look like?","3e11f2a7":"## More data Preprocessing","6cbb9042":"## Using Catboost","5497ab98":"#### Survival of different Age\n","fe8facc7":"### Using TensorFlow DNN with DenseFeatures","e8f083a7":"### Add Cabin type Column","0eeed1fb":"### Handle Categorical Features","ca04476d":"### Using KNN","f554335f":"## Import Packages","506f8f66":"#### Survival of different Parch (Number of parents or children)\n- Passengers without parents or children had 1 \/ 3 survival rate.\n- Passengers with 1 - 3 parents or children had 1 \/ 2 survival rate.\n- Passengers with more than 4 parents or children were less likely to survive.\n","bee77a4c":"### What's the factor to survive?\nAs we can see it's related to Gender, PClass, Status, Fare, Cabin and Embarked. ","c977a001":"#### Survival of different Fare\nMost of the tickets were less than 100 pounds. Only about 1 \/ 5 with fare around 10 pounds survived.","ee623fe6":"### Basic Statistic infos","028c102e":"## Using XGBoost Classifier","ad959a93":"### Convert Categorical features to one hot features","c414c6be":"# Titanic Prediction with Different Models\n## Table of Contents\n- Summary\n- Import Packages\n- Common Functions\n- Import datasets\n- Data Wrangling\n- Data Preprocessing\n    - Add Cabin type Column\n    - Add family member size and faimily member type Column\n    - Handle Categorical Features\n- Exploratory Data Analysis\n    - Basic Statistic infos\n    - What's the factor to survive?\n        - Survival of different Gender\n        - Survival of different Age\n        - Survival of different Pclass\n        - Survival of different Fare\n        - Survival of different Cabin\n        - Survival of different Embarked\n        - Survival of different SibSp (Number of siblings or spouses)\n        - Survival of different Parch (Number of parents or children)\n        - Survival of different family member size\n- More Data Preprocessing\n    - Convert Categorical features to one hot features\n    - Train Validation Split\n    - Balance Training dataset\n- Model Development and Evaluation\n    - Using TensorFlow DNN\n    - Using TensorFlow DNN and DenseFeatures\n    - Using KNN\n    - Using Decision Tree Classifier\n    - Using Gradient Boosting Classifier\n    - Using Random Forest Classifier\n    - Using KMeans\n    - Using XGBoost Classifier\n    - Using Catboost Classifier\n- Submission\n- Conclusions\n    \n## Summary\nIn this notebook I will do EDA and Data Preprocessing on Titanic Dataset, I will also implement Titannic Predicion suing different kinds of classification Models.\n- Deep Neural Network\n- Deep and Wide Neural Network using keras DenseFeatures\n- Logistic Regression\n- KNN\n- Decision Tree Classifier\n- Gradient Boosting Classifier\n- Random Forest Classifier\n- KMeans\n- XGBoost \n- CatBoost","22b87d6f":"#### Survival of different Embarked\n- About 1 \/ 3 passengers from Embarked Q, S survived;\n- About half passengers from Embarked C survived;","065ffb2e":"### Using Random Forest Classifier","dd9b1d33":"After converting family member size to categorical feature, the relation between family member size and survival rate were more obvious.","4a371f6d":"#### Survival of different Pclass\n- Passengers from Pclass 1 had 62% Survival Rate;\n- Passengers from Pclass 2 had 47% Survival Rate;\n- Passengers from Pclass 3 had 24% Survival Rate;","254fab78":"## Submission\nThis result can be different from Kagggle LeaderBoard, so you may try different submission files.","43d25710":"#### Survival of different Gender\nWomen have a higher Survival rate than Men.","9f3d7b9e":"## Using KMeans","47d9808a":"After balancing the training dataset","7995dace":"Calucate mean and std value for future use.","a239a96c":"## Data Wrangling","d7b39bec":"## Feature Scaling","5f1445f4":"#### Survival of different SibSp (Number of siblings or spouse)\n- Passengers without siblings or spouse had 1 \/ 3 Survival Rate.\n- Passengers with one or two siblings or spouse had about 1 \/ 2 Survival Rate.\n- Passengers with more than two siblings or spouse were less likely to survive.","ff258c82":"**Evaluate Model, save and keep tract of best results**","d3524808":"## Exploratory Data Analysis","b0c5087d":"### Using Logistic Regression","930186ef":"According to the EDA below, Family member size had a impact on Survival, but it was not a linear relationship, that was why it had a low pearson correlation score. So I will convert it to a categorical feature with single(1 family member), medium(2-4 family members), large(more than 4 members). I will add a Feature Toggle here to control whether to use this function","2a16f853":"We can indicate family member size by SibSp and Parch feature: ","b776592b":"#### Survival of different Cabin\n- More than half Passengers from Cabin started with C, D, E, F, G survived;\n- Less than half Passengers from Cabin started with A,B survived;\n- 30% of Passengers with unknown Cabin survived;\n- Almost no Passengers from Cabin started with T survived.","3b2dc9cc":"#### Survival of different family member size\n- Those who were alone (1 family member size) had 1 \/ 3 Survival Rate.\n- Those who had 2 - 4 family member size had more than 1 \/ 2 Survival Rate.\n- Those who had 5 - 11 family member size were less likely to survive.","53ffd8ca":"As we can see Age, Cabin and Fare information contains missing values, so we need to apply Missing Value  Imputation to them. The most common way is to replace categorical missing values with most fequent category and repalce numerical missing values with average value of that feature.","82188167":"## Conclusions\n- Although Deep Learning is very powerful. When handling this dataset, it's not easy to find a Model that outperforms some traditional Machine Learning algorithms. Maybe because the dataset is too small.\n- Gradient Boosting Classifier, Random Forest Classifier can also achieve a very good performance and it requires less computing power than Deep Neural Network. ","f8991d2a":"### Train Validation Split","69476fd8":"## Common Functions","b8d85129":"Let's see the Cabin labels, there are so many of them. But I make an assmption that the First Alphabet matters, it indicated the location and class of the Passengers so it had an impact to survive.","97452867":"### Add family member size and faimily member type Column","05fec537":"### Using Gradient Boosting Classifier"}}