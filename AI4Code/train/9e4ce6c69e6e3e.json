{"cell_type":{"c6a2f3d4":"code","a2a26264":"code","6598f255":"code","3e67da75":"code","06649422":"code","f84b4344":"code","34907af3":"code","bc9120df":"code","d2376dfd":"code","f166e532":"code","11910020":"code","604408b3":"code","6c69a7e5":"code","a96c751d":"code","d1c47989":"code","61f07675":"code","ca88f033":"code","da4b74e8":"code","e818671e":"code","094531c7":"code","8b22a98b":"markdown","509b23b9":"markdown","e1a6ab7e":"markdown","ce786588":"markdown","cf1d1f70":"markdown","7b2d9237":"markdown","17ccf200":"markdown","d5aaeadf":"markdown","d55e8791":"markdown"},"source":{"c6a2f3d4":"from sklearn.metrics import classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import learning_curve, GridSearchCV\nimport pandas as pd\nimport numpy as np\nfrom sklearn import svm, datasets\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport csv\nimport math\nimport seaborn as sns\n%matplotlib inline\nimport random\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cluster import KMeans\nimport numpy as numpy \nimport statistics\n\n#import seaborn as sns","a2a26264":"def loadCsv():\n\tlines = csv.reader(open('..\/input\/dataset2\/dd1_h.csv'))\n\tdataset = list(lines)\n\tfor i in range(len(dataset)):\n\t\tdataset[i] = [float(x) for x in dataset[i]]\n\treturn dataset\ndef splitDataset(dataset, splitRatio):\n\ttrainSize = int(len(dataset) * splitRatio)\n\ttrainSet = []\n\tcopy = list(dataset)\n\twhile len(trainSet) < trainSize:\n\t\tindex = random.randrange(len(copy))\n\t\ttrainSet.append(copy.pop(index))\n\treturn [trainSet, copy]\ndef separateByClass(dataset):\n\tseparated = {}\n\tfor i in range(len(dataset)):\n\t\tvector = dataset[i]\n\t\tif (vector[-1] not in separated):\n\t\t\tseparated[vector[-1]] = []\n\t\tseparated[vector[-1]].append(vector)\n\treturn separated\ndef mean(numbers):\n\treturn sum(numbers)\/float(len(numbers))\ndef stdev(numbers):\n\tavg = mean(numbers)\n\tvariance = sum([pow(x-avg,2) for x in numbers])\/float(len(numbers)-1)\n\treturn math.sqrt(variance)\ndef summarize(dataset):\n\tsummaries = [(mean(attribute), stdev(attribute)) for attribute in zip(*dataset)]\n\tdel summaries[-1]\n\treturn summaries\ndef summarizeByClass(dataset):\n\tseparated = separateByClass(dataset)\n\tsummaries = {}\n\tfor classValue, instances in separated.items():\n\t\tsummaries[classValue] = summarize(instances)\n\treturn summaries\ndef calculateProbability(x, mean, stdev):\n\texponent = math.exp(-(math.pow(x-mean,2)\/(2*math.pow(stdev,2))))\n\treturn (1 \/ (math.sqrt(2*math.pi) * stdev)) * exponent\ndef calculateClassProbabilities(summaries, inputVector):\n\tprobabilities = {}\n\tfor classValue, classSummaries in summaries.items():\n\t\tprobabilities[classValue] = 1\n\t\tfor i in range(len(classSummaries)):\n\t\t\tmean, stdev = classSummaries[i]\n\t\t\tx = inputVector[i]\n\t\t\tprobabilities[classValue] *= calculateProbability(x, mean, stdev)\n\treturn probabilities\ndef predict(summaries, inputVector):\n\tprobabilities = calculateClassProbabilities(summaries, inputVector)\n\tbestLabel, bestProb = None, -1\n\tfor classValue, probability in probabilities.items():\n\t\tif bestLabel is None or probability > bestProb:\n\t\t\tbestProb = probability\n\t\t\tbestLabel = classValue\n\treturn bestLabel\ndef getPredictions(summaries, testSet):\n\tpredictions = []\n\tfor i in range(len(testSet)):\n\t\tresult = predict(summaries, testSet[i])\n\t\tpredictions.append(result)\n\treturn predictions\ndef getAccuracy(testSet, predictions):\n    correct = 0\n    for x in range(len(testSet)):\n        if testSet[x][-1] == predictions[x]:\n            correct += 1\n    return (correct\/float(len(testSet))) * 100.0\ndef main():\n    filename = '..\/input\/dataset2\/dd1_h.csv'\n    splitRatio = 0.67\n    dataset = loadCsv()\n    trainingSet, testSet = splitDataset(dataset, splitRatio)\n    #dataset = loadCsv(filename)\n    trainingSet =loadCsv()\n    testSet =loadCsv()\n    print('Split {0} rows into train={1} and test={2} rows'.format(len(dataset), len(trainingSet), len(testSet)))\n    summaries = summarizeByClass(trainingSet)\n    predictions = getPredictions(summaries, testSet)\n    accuracy = getAccuracy(testSet, predictions)\n    print('Accuracy: {0}%'.format(accuracy))\nmain()","6598f255":"train = pd.read_csv('..\/input\/dataset1\/dd1.csv')\ntrain.head()","3e67da75":"train.count()","06649422":"train.describe()","f84b4344":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train.drop('Churn',axis=1), \n           train['Churn'], test_size=0.30, \n            random_state=101)\nfrom sklearn.linear_model import LogisticRegression#create an instance and fit the model \nlogmodel = LogisticRegression()\nlogmodel.fit(X_train, y_train)\n#predictions\nPredictions = logmodel.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,Predictions))","34907af3":"from sklearn.decomposition import PCA as sklearnPCA","bc9120df":"df = pd.read_csv('..\/input\/dataset1\/dd1.csv')\ndf.columns=['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\ndf.dropna(how=\"all\", inplace=True) # drops the empty line at file-end\nX = df.iloc[:,0:19].values\ny = df.iloc[:,19].values\n\n\nX_std = StandardScaler().fit_transform(X)\nsklearn_pca = sklearnPCA(n_components=6) #Reducing to six principal components\nY_sklearn = sklearn_pca.fit_transform(X_std)\nprint(Y_sklearn)\n\na= pd.DataFrame(Y_sklearn)\nprint(a)","d2376dfd":"dset = pd.read_csv('..\/input\/pcads6\/ds6.csv')\nprint(dset.describe())\n\ny = dset['Churn']\nX = dset.drop(['Churn'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state=98)","f166e532":"from sklearn.neural_network import MLPClassifier","11910020":"dset = pd.read_csv(\"..\/input\/dataset1\/dd1.csv\")\n#dset = pd.read_csv(\"diabetes.csv\")\nX=dset.iloc[:, :-1].values\ny = dset.iloc[:,19:].values\nprint(y)\ny=y.astype('int')\nX=X.astype('int')\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 1\/3)\n \nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nclf = MLPClassifier(hidden_layer_sizes=(7,9,4,6,8,10,16), max_iter=300, alpha=0.00000000000000000000001,\n                     solver='sgd', verbose=3,  random_state=0,tol=0.000000001)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(\"accuracy\",accuracy_score(y_test, y_pred)*100)\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","604408b3":"Data= pd.read_csv(\"..\/input\/pcads6\/ds6.csv\", sep= None, engine= \"python\")\ncols= [\"0\",\"1\",\"3\",\"4\",\"5\",\"Churn\"]\ndata_encode= Data.drop(cols, axis= 1)\ndata_encode= data_encode.apply(LabelEncoder().fit_transform)\ndata_rest= Data[cols]\nData= pd.concat([data_rest,data_encode], axis= 1)\ndata_encode= Data.drop(cols, axis= 1)\ndata_encode= data_encode.apply(LabelEncoder().fit_transform)\ndata_rest= Data[cols]\nData= pd.concat([data_rest,data_encode], axis= 1)","6c69a7e5":"data_train, data_test= train_test_split(Data, test_size= 0.33, random_state= 4)\nX_train= data_train.drop(\"Churn\", axis= 1)\nY_train= data_train[\"Churn\"]\nX_test= data_test.drop(\"Churn\", axis=1)\nY_test= data_test[\"Churn\"]","a96c751d":"scaler= StandardScaler()\nscaler.fit(X_train)\nX_train= scaler.transform(X_train)\nX_test= scaler.transform(X_test)","d1c47989":"#K-means clustering\nK_cent= 8 \nkm= KMeans(n_clusters= K_cent, max_iter= 100)\nkm.fit(X_train)\ncent= km.cluster_centers_","61f07675":"max=0 \nfor i in range(K_cent):\n\tfor j in range(K_cent):\n\t\td= numpy.linalg.norm(cent[i]-cent[j])\n\t\tif(d> max):\n\t\t\tmax= d\nd= max\n \nsigma= d\/math.sqrt(2*K_cent)","ca88f033":"shape= X_train.shape\nrow= shape[0]\ncolumn= K_cent\nG= numpy.empty((row,column), dtype= float)\nfor i in range(row):\n    for j in range(column):\n        dist= numpy.linalg.norm(X_train[i]-cent[j])\n        G[i][j]= math.exp(-math.pow(dist,2)\/math.pow(2*sigma,2))\n        ","da4b74e8":"GTG= numpy.dot(G.T,G)\nGTG_inv= numpy.linalg.inv(GTG)\nfac= numpy.dot(GTG_inv,G.T)\nW= numpy.dot(fac,Y_train)","e818671e":"row= X_test.shape[0]\ncolumn= K_cent\nG_test= numpy.empty((row,column), dtype= float)\nfor i in range(row):\n\tfor j in range(column):\n\t\tdist= numpy.linalg.norm(X_test[i]-cent[j])\n\t\tG_test[i][j]= math.exp(-math.pow(dist,2)\/math.pow(2*sigma,2))","094531c7":"prediction= numpy.dot(G_test,W)\nprediction= 0.5*(numpy.sign(prediction-0.5)+1)\nscore= accuracy_score(prediction,Y_test)\nprint (\"Accuracy is:\")\nprint (score)","8b22a98b":"**MLP Classifier**","509b23b9":"**Naive bayes classifier**\n\n\nWe've built a Naive Bayes classifier which is a probabilistic machine learning model that\u2019s used for classification task. The crux of the classifier is based on the Bayes theorem.","e1a6ab7e":"**Churn prediction**\nCustomer Churn, also known as attrition, refers to a condition where the customers stop doing business with a company, This leads to unprecedented and a costly loss. Hence it is a crucial part of the customer-company relationship. The main reason is that the cost of acquiring a new customer is relatively priced higher than retaining the old customer. After the advent of enterprise size humongous firms, the need to retain existing customers is essential.In most instances, customers leave the company unexpectedly, without giving any feedback or expressing their issues with the company. Hence it becomes very difficult for the company to know the actual reason. By bringing in a classification model in this place reduces the risk of attrition as the company would be able to predict the churn rate of a customer and if found high, the company could take measures to retain the customer.The major losses incurred by the companies are very high as it impacts sales. So,reducing customer churn is the need of the hour. Thus, the ability to precisely predict the future churn rates is highly demanding. It also helps businesses gain a better understanding about their customers and forecast their reviews.\n\nNOTE: This is a continuation(2\/2) of the kernel https:\/\/www.kaggle.com\/ananyaanandh\/subscriber-s-churn-prediction-data-wrangling","ce786588":"\nLets begin the process by reading the training data into a pandas dataframe","cf1d1f70":"We split our training dataset into a X_train and Y_train. \n\nX_train contains all the features where as Y_train contains the target variable.","7b2d9237":"**Radial basis function network**","17ccf200":"**Logistic regression**\nLogistic regression is a technique borrowed by machine learning from the field of statistics.\nIt is a common method used for binary classification problems (problems with two class values).\n\nFor this regression model, we take the normalized dataset into account.","d5aaeadf":"**Support Vector Machine**\nA Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples.","d55e8791":"**Principal component analysis**\n\nAs the number of attributes(components) are high, we do PCA in order to reduce the number of attributes. The number of principal components are taken in accordance to the Kaiser criterion. The Kaiser's rule is to drop all components with eigenvalues under 1.0 \u2013 this being the eigenvalue equal to the information accounted for by an average single item"}}