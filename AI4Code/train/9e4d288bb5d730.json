{"cell_type":{"6e622ea4":"code","75f7c900":"code","79e20da2":"code","72b817e6":"code","643a56a5":"code","522b82a7":"code","0d3e7344":"code","db8d0970":"code","e7d233c3":"code","92c1c18e":"code","283e9da8":"code","e9415752":"code","80e84a07":"code","53945d40":"code","4b1d4729":"code","cda3ce9a":"code","9295f388":"code","022762a0":"markdown","2aa05519":"markdown","80eb9b8b":"markdown"},"source":{"6e622ea4":"import sklearn\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow_datasets as tfds\nimport pathlib","75f7c900":"data_path = \"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\"\ndata = pd.read_csv(data_path)\ndata.head()","79e20da2":"# 201 missing values for bmi feature \ndata.isnull().sum()","72b817e6":"#no duplicated rows\nif data.duplicated().sum() == 0:\n    print(\"No duplicated rows\")\nelse:\n    print(\"There are duplicated rows in the data\")","643a56a5":"print(\"There are {}  examples is this dataset, before dropping the rows containing null values\".format(len(data)))","522b82a7":"# so I decide to fill the missing values \n\ndata.fillna(data.median(), inplace=True)","0d3e7344":"# 5 categorical variables\ncat_variables = ['gender', 'hypertension', 'heart_disease', 'ever_married',\n                'work_type', 'work_type', 'Residence_type', \"smoking_status\"] \n\nfor variable in cat_variables:\n    data[variable] = data[variable].astype('category')","db8d0970":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import StratifiedShuffleSplit","e7d233c3":"data['gender'][data['gender'] == 'Other']\ndata.drop(3116, inplace=True)","92c1c18e":"x = data[['gender',  'age', 'hypertension', 'heart_disease','ever_married',\n          'work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status']]\ny = data['stroke']\n\n\nx_train_full, x_test, y_train_full, y_test = train_test_split(x, y, shuffle=True, test_size=0.2,\n                                                              stratify=y,\n                                                              random_state=100)\n\nx_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full,\n                                                  shuffle=True, test_size=0.2,\n                                                  stratify=y_train_full,\n                                                  random_state=100)","283e9da8":"numerical_features = ['age', 'avg_glucose_level', 'bmi']\ncategorical_features = ['gender', 'hypertension', 'heart_disease','ever_married',\n                        'work_type', 'Residence_type', 'smoking_status']\n\n\npreprocessing_pipeline = ColumnTransformer([\n    ('num', MinMaxScaler(), numerical_features),\n    ('cat', OneHotEncoder(), categorical_features)\n])","e9415752":"x_train_prepered = preprocessing_pipeline.fit_transform(x_train)\nx_val_prepered = preprocessing_pipeline.transform(x_val)\nx_test_prepered = preprocessing_pipeline.transform(x_test)\n\ny_train = np.array(y_train)\ny_val = np.array(y_val)\ny_test = np.array(y_test)","80e84a07":"training_dataset = tf.data.Dataset.from_tensor_slices((tf.constant(x_train_prepered), tf.constant(y_train)))\ntraining_dataset = training_dataset.shuffle(512).batch(64).prefetch(1)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((tf.constant(x_val_prepered), tf.constant(y_val)))\nval_dataset = val_dataset.shuffle(256).batch(64).prefetch(1)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((tf.constant(x_train_prepered), tf.constant(y_train)))\ntest_dataset = test_dataset.shuffle(256).batch(64).prefetch(1)","53945d40":"def base_model():\n    \n    inputs = tf.keras.Input(shape=(22,))\n    x = tf.keras.layers.Dense(512, activation='relu')(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n\n    x = tf.keras.layers.Dense(256, activation='relu',\n                             kernel_regularizer=tf.keras.regularizers.l2())(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Dense(64, activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    model_output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    \n    model = tf.keras.models.Model(inputs=inputs, outputs=model_output)\n    \n    return model","4b1d4729":"n_epochs = 5\nbatch_size = 64\nn_steps = len(x_train_prepered) \/\/ batch_size\n\noptimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\nloss_fn = tf.keras.losses.BinaryCrossentropy()\nmetric = tf.keras.metrics.Accuracy()\nval_metrics = tf.keras.metrics.Accuracy()","cda3ce9a":"model = base_model()\n\nepochs_train_losses = []\nepochs_val_losses = []\nepochs_train_acc = []\nepochs_val_acc = []\n\nfor epoch in range(1, n_epochs + 1):\n    print(\"Epoch {}\/{}\".format(epoch, n_epochs))\n    \n    #apply gradients\n    training_losses = []\n    for step, (x_batch_train, y_batch_train) in enumerate(training_dataset):\n        #calculate the gradiants with regards to the model trainable_weights\n        with tf.GradientTape() as tape:\n            #forward pass\n            logits = model(x_batch_train)\n            loss = loss_fn(y_batch_train, logits)\n            training_losses.append(loss)\n        # backward pass \n        #calculate the gradiants\n        grads = tape.gradient(loss, model.trainable_weights)\n        #modifying the trainable_weights\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n        \n        #calculating rhe ACC after modifying the  trainable_weights\n        metric(y_batch_train, tf.argmax(logits, axis=1, output_type=tf.int32))\n    \n    # the metric measurement on the epoch\n    train_acc = metric.result()\n    epochs_train_acc.append(train_acc)\n    \n    #The mean of the losses of the batches give us the mean loss for each epoch\n    losses_train_mean = np.mean(training_losses)\n    epochs_train_losses.append(losses_train_mean)\n    \n    #calculating the validation loss and MAE\n    val_losses = []\n    for x_val, y_val in val_dataset:\n        val_logits = model(x_val)\n        val_loss = loss_fn(y_val, val_logits)\n        val_losses.append(val_loss)\n        val_metrics(y_val, tf.argmax(val_logits, axis=1, output_type=tf.int32))\n     \n    val_acc = val_metrics.result()\n    epochs_val_acc.append(val_acc)\n    \n    losses_val_mean = np.mean(val_losses)\n    epochs_val_losses.append(losses_val_mean)\n    \n    print(\"Trainig Loss: {}-------Training Accuracy: {}\".format(epochs_train_losses[-1], train_acc))\n    print(\"Validation Loss: {}-------Validation Accuracy: {}\".format(epochs_val_losses[-1], val_acc))\n    print(\"\\n\")\n    \n    #reset the metrics after each epoch\n    metric.reset_states()\n    val_metrics.reset_states()\n","9295f388":"test_accuracy = tf.keras.metrics.Accuracy()\n\nfor (x, y) in test_dataset:\n    # training=False is needed only if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    logits = model(x, training=False)\n    prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n    test_accuracy(prediction, y)\n\nprint(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))","022762a0":"# Custom training loop","2aa05519":"# Evaluate the model on the test dataset","80eb9b8b":"# Prepering the dataset"}}