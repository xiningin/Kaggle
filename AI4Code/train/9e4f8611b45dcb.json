{"cell_type":{"c34d330b":"code","e5bdc7f9":"code","4c1374fc":"code","51bd6c31":"code","aa91335b":"code","07ecd1c7":"code","83f94120":"code","aec3c59c":"code","1be17fd0":"code","a89f8de3":"code","a6822b1d":"markdown"},"source":{"c34d330b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm_notebook as tqdm\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport random\nrandom.seed(2019)\nnp.random.seed(2019)\nimport os,math\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e5bdc7f9":"\ndf_train = pd.read_csv('..\/input\/train.csv')\n\ntest_path = '..\/input\/test.csv'\n\ndf_test = pd.read_csv(test_path)\ndf_test.drop(['ID_code'], axis=1, inplace=True)\ndf_test = df_test.values\n\nunique_samples = []\nunique_count = np.zeros_like(df_test)\nfor feature in tqdm(range(df_test.shape[1])):\n    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n    unique_count[index_[count_ == 1], feature] += 1\n\n# Samples which have unique values are real the others are fake\nreal_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\nprint(len(real_samples_indexes))\nprint(len(synthetic_samples_indexes))\n","4c1374fc":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","51bd6c31":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\n\n\ndef findmeancounts(bins,values,x):\n    for i in range(len(bins)):\n        if x < bins[i]:\n            return values[i]\n    return values[-1]\n \ndef magic2(): \n    feats = [\"var_\"+str(i) for i in range(200)] \n    df = pd.concat([df_train,df_test.ix[real_samples_indexes]]) \n    for feat in feats:\n#     for feat in [\"var_170\"]:\n#         m = df[feat].mean()\n#         s = df[feat].std()\n#         temp = np.random.normal(m, s, 300000) \n        \n#         df[feat+\"_temp\"] = temp\n#         df[feat+\"_temp\"] = df[feat+\"_temp\"].apply(lambda x:round(x,4))\n#         temp2 = df[feat+\"_temp\"].value_counts(dropna = True) \n#         bins = []  \n#         values = []\n#         count = 1\n#         temp_values = []\n#         alpha = min(200,len(temp2)\/\/50)\n#         for k,v in sorted(temp2.items(),key = lambda x:x[0]):\n#             if count % alpha == 0:\n#                 bins.append(k)\n#                 values.append(sum(temp_values)*1.0\/len(temp_values))\n#                 temp_values = []\n#             count += 1\n#             temp_values.append(v)\n#         print(m,s,count,feat,len(values))\n#         df_train[feat+\"_vc_expect\"] = df_train[feat].apply(lambda x:findmeancounts(bins,values,x)) \n#         df_test[feat+\"_vc_expect\"] = df_test[feat].apply(lambda x:findmeancounts(bins,values,x))\n# #         print(temp2,temp_values[10:20])\n#         del df[feat+\"_temp\"]\n        temp = df[feat].value_counts(dropna = True) \n        \n#         print(temp)\n        df_train[feat+\"vc\"] = df_train[feat].map(temp).map(lambda x:min(10,x)).astype(np.uint8)\n        df_test[feat+\"vc\"] = df_test[feat].map(temp).map(lambda x:min(10,x)).astype(np.uint8)\n        print(feat,temp.shape[0],df_train[feat+\"vc\"].map(lambda x:int(x>2)).sum(),df_train[feat+\"vc\"].map(lambda x:int(x>3)).sum())\n        df_train[feat+\"sum\"] = ((df_train[feat] - df[feat].mean()) * df_train[feat+\"vc\"].map(lambda x:int(x>1))).astype(np.float32)\n        df_test[feat+\"sum\"] = ((df_test[feat] - df[feat].mean()) * df_test[feat+\"vc\"].map(lambda x:int(x>1))).astype(np.float32)\n        df_train[feat+\"sum2\"] = ((df_train[feat]) * df_train[feat+\"vc\"].map(lambda x:int(x>2))).astype(np.float32)\n        df_test[feat+\"sum2\"] = ((df_test[feat]) * df_test[feat+\"vc\"].map(lambda x:int(x>2))).astype(np.float32)\n#         if df_train[feat+\"vc\"].map(lambda x:int(x>4)).sum() > 20000:\n        df_train[feat+\"sum3\"] = ((df_train[feat]) * df_train[feat+\"vc\"].map(lambda x:int(x>4))).astype(np.float32) \n        df_test[feat+\"sum3\"] = ((df_test[feat]) * df_test[feat+\"vc\"].map(lambda x:int(x>4))).astype(np.float32) \n#         if df_train[feat+\"vc\"].map(lambda x:int(x>6)).sum() > 20000:\n#             df_train[feat+\"sum4\"] = ((df_train[feat]) * df_train[feat+\"vc\"].map(lambda x:int(x>6))).astype(np.float32) \n#             df_test[feat+\"sum4\"] = ((df_test[feat]) * df_test[feat+\"vc\"].map(lambda x:int(x>6))).astype(np.float32) \n        #temp = df_train[feat].value_counts(dropna = True) \n        #df_train[feat+\"sum4\"] = ((df_train[feat] - df[feat].mean()) * df_train[feat+\"vc\"].map(lambda x:int(x>1))).astype(np.float32)\n        #df_test[feat+\"sum4\"] = ((df_test[feat] - df[feat].mean()) * df_test[feat+\"vc\"].map(lambda x:int(x>1))).astype(np.float32)\n        \n        # TODO\n#         if df_train[feat+\"vc\"].map(lambda x:int(x>2)).sum() > 10000:\n#             df_train[feat+\"sum2\"] = (df_train[feat]) * df_train[feat+\"vc\"].map(lambda x:int(x>2)) \n#             df_test[feat+\"sum2\"] = (df_test[feat]) * df_test[feat+\"vc\"].map(lambda x:int(x>2))\n#         else:\n#             df_train[feat+\"sum2\"] = (df_train[feat]) * df_train[feat+\"vc\"].map(lambda x:int(x>1)) \n#             df_test[feat+\"sum2\"] = (df_test[feat]) * df_test[feat+\"vc\"].map(lambda x:int(x>1))\n#         df_train[feat+\"_filter1\"] = (df_train[feat] * (df_train[feat+\"vc\"].apply(lambda x:min(3,math.sqrt(x))) - df_train[feat+\"_vc_expect\"]).apply(lambda x: int(x > 0)) * df_train[feat+\"vc\"].apply(lambda x: int(x > 1))).astype(np.float32) \n#         df_test[feat+\"_filter1\"] = (df_test[feat] * (df_test[feat+\"vc\"].apply(lambda x:min(3,math.sqrt(x))) - df_test[feat+\"_vc_expect\"]).apply(lambda x: int(x > 0)) * df_test[feat+\"vc\"].apply(lambda x: int(x > 1))).astype(np.float32) \n#         df_train[feat+\"_filter1\"] = (df_train[feat] * (df_train[feat+\"vc\"] - df_train[feat+\"_vc_expect\"].apply(lambda x:1+math.sqrt(x-1))).apply(lambda x: int(x > 0)) * df_train[feat+\"vc\"].apply(lambda x: int(x > 1))).astype(np.float32) \n#         df_test[feat+\"_filter1\"] = (df_test[feat] * (df_test[feat+\"vc\"] - df_test[feat+\"_vc_expect\"].apply(lambda x:1+math.sqrt(x-1))).apply(lambda x: int(x > 0)) * df_test[feat+\"vc\"].apply(lambda x: int(x > 1))).astype(np.float32) \n\n#         df_train[feat+\"vc2\"] = df_train[feat+\"vc\"].map(lambda x:int(math.sqrt(x))).astype(np.int8) \n#         df_test[feat+\"vc2\"] = df_test[feat+\"vc\"].map(lambda x:int(math.sqrt(x))).astype(np.int8) \n#         df_train[feat+\"vc\"] = df_train[feat+\"vc\"].map(lambda x:int(x>1)).astype(np.int8) \n#         df_test[feat+\"vc\"] = df_test[feat+\"vc\"].map(lambda x:int(x>1)).astype(np.int8)\nmagic2()\n\ndf_train.iloc[:5,:]","aa91335b":"def augment(x,y,t=2):\n    xs,xn = [],[]\n\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        featnum = x1.shape[1]\/\/200 - 1\n        for c in range(200):\n            np.random.shuffle(ids)\n            x1[:,[c] + [200 + featnum * c + idc for idc in range(featnum)]] = x1[ids][:,[c] + [200 + featnum * c + idc for idc in range(featnum)]]\n        xn.append(x1)\n\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        featnum = x1.shape[1]\/\/200 - 1\n        for c in range(200):\n            np.random.shuffle(ids)\n            x1[:,[c] + [200 + featnum * c + idc for idc in range(1)]] = x1[ids][:,[c] + [200 + featnum * c + idc for idc in range(1)]]\n        xs.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y\n\nfeatures = [col for col in df_train.columns if col not in ['target', 'ID_code']]\n# X_train, y_train = df_train[features], df_train['target']\n# augment(X_train.values, y_train.values)","07ecd1c7":"print(df_train['target'].value_counts())\nprint(df_train.groupby('var_80vc')['target'].value_counts())\nprint(df_train.groupby('var_81vc')['target'].value_counts())\nprint(df_train.groupby('var_82vc')['target'].value_counts())\n# print(df_train[df_train['var_82vc']>2].groupby('var_82')['target'].value_counts())\n# print(df_train[df_train['var_68vc']>2].groupby('var_68')['target'].value_counts())\n# print(df_train.groupby('var_68')['target'].value_counts())","83f94120":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport lightgbm as lgb\n\nlgb_params = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\",\n    \"boosting\": 'gbdt',\n    \"max_depth\" : -1,\n    \"num_leaves\" : 15,\n    \"learning_rate\" : 0.01,\n    \"bagging_freq\": 5,\n    \"bagging_fraction\" : 0.6,\n    \"feature_fraction\" : 0.05,\n    \"min_data_in_leaf\": 50,\n    \"min_sum_heassian_in_leaf\": 10,\n    \"tree_learner\": \"serial\",\n    \"boost_from_average\": \"false\",\n    \"lambda_l1\" : 1.,\n#     \"lambda_l2\" : 0.5,\n    \"bagging_seed\" : 2007,\n    \"verbosity\" : 1,\n    \"seed\": 2007\n}\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2007)\noof = df_train[['ID_code', 'target']]\noof['predict'] = 0\npredictions = np.zeros((df_test.shape[0],5))\nval_aucs = []\nfeature_importance_df = pd.DataFrame()\nfeatures = [col for col in df_train.columns if col not in ['target', 'ID_code']]\nX_test = df_test[features].values\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(df_train, df_train['target'])):\n    X_train, y_train = df_train.iloc[trn_idx][features], df_train.iloc[trn_idx]['target']\n    X_valid, y_valid = df_train.iloc[val_idx][features], df_train.iloc[val_idx]['target']\n    \n    N = 1\n    p_valid,yp = 0,0\n    for i in range(N):\n        X_t, y_t = augment(X_train.values, y_train.values)\n        weights = np.array([0.8] * X_t.shape[0])\n        weights[:X_train.shape[0]] = 1.0\n        print(X_t.shape)\n#         X_t = pd.DataFrame(X_t)\n        trn_data = lgb.Dataset(X_t, label=y_t, weight = weights)\n        val_data = lgb.Dataset(X_valid, label=y_valid)\n        evals_result = {}\n        lgb_clf = lgb.train(lgb_params,\n                        trn_data,\n                        100000,\n                        valid_sets = [trn_data, val_data],\n                        early_stopping_rounds=5000,\n                        verbose_eval = 1000,\n                        evals_result=evals_result\n                       )\n        p_valid += lgb_clf.predict(X_valid)\n        yp += lgb_clf.predict(X_test)\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    oof['predict'][val_idx] = p_valid\/N\n    val_score = roc_auc_score(y_valid, p_valid)\n    val_aucs.append(val_score)\n    \n    predictions[:,fold] = yp\/N","aec3c59c":"mean_auc = np.mean(val_aucs)\nstd_auc = np.std(val_aucs)\nall_auc = roc_auc_score(oof['target'], oof['predict'])\nprint(\"Mean auc: %.9f, std: %.9f. All auc: %.9f.\" % (mean_auc, std_auc, all_auc))\n\n\n\ncols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nprint(cols[:50])\nprint(cols[-50:])\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\n\n\n","1be17fd0":"cols[:30]","a89f8de3":"##submission\nsub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\nsub_df[\"target\"] = np.mean(predictions,axis = 1)\nsub_df.to_csv(\"lgb_submission.csv\", index=False)","a6822b1d":"After collecting the \"verified generators\" for each fake sample, finding the Public\/Private LB split is no more than a few set operations."}}