{"cell_type":{"7890bb9d":"code","326d6cbb":"code","cf15cd6d":"code","2de664ab":"code","46bf6cdf":"code","ace1419b":"code","fe43a004":"code","e1bc06de":"code","fcd13942":"code","2c8afbf4":"code","b892e55d":"code","df7d71b8":"code","f174a9b0":"code","883d2105":"code","4cbf4911":"code","81a6a405":"code","5826d639":"code","d5fff393":"code","4d362c2b":"code","85710e18":"code","45f7204f":"code","24c89204":"code","d0481fb7":"markdown","943785b4":"markdown","9179bc0f":"markdown","7c39a9e1":"markdown","69522efe":"markdown","b73704b0":"markdown","d84992ef":"markdown","20baa6fe":"markdown","c506d5d8":"markdown","1250d201":"markdown","9a18251e":"markdown","f2a3184d":"markdown","b6c07d2b":"markdown","a6885966":"markdown","0a8391bc":"markdown","fe858841":"markdown"},"source":{"7890bb9d":"import numpy as np\nimport pandas as pd ","326d6cbb":"# read in data\npath = '\/kaggle\/input\/widsdatathon2021\/'\ntrain = pd.read_csv(path + 'TrainingWiDS2021.csv')\ntest_X = pd.read_csv(path + 'UnlabeledWiDS2021.csv')\nsolution_temp = pd.read_csv(path + 'SolutionTemplateWiDS2021.csv')\nsample_sub = pd.read_csv(path + 'SampleSubmissionWiDS2021.csv')\n\n# save testing encounter id for submission later\ntest_encounter_id = test_X['encounter_id']\n\n# drop these ID columns\nto_drop = ['Unnamed: 0', 'encounter_id', 'hospital_id']\n\ntrain.drop(to_drop, axis = 1, inplace = True)\ntest_X.drop(to_drop, axis = 1, inplace = True)","cf15cd6d":"y_var = 'diabetes_mellitus'\ntrain_X = train.drop(y_var, axis = 1)\ntrain_y = train[y_var]","2de664ab":"# define categorical features\n\ncat_feats = [\n    'apache_2_diagnosis',\n    'apache_3j_diagnosis',\n    'icu_id',\n    'ethnicity', \n    'gender',\n    'hospital_admit_source', \n    'icu_admit_source', \n    'icu_stay_type', \n    'icu_type', \n            ]","46bf6cdf":"def age_group(x):\n    \n    if x<=50:\n        return \"0-50\"\n    elif x>50 and x<=65:\n        return \"50-65\"\n    elif x>65 and x<=75:\n        return \"65-75\"\n    elif x>75:\n        return \"75+\"\n\n    \ndef get_bmi_cat(x):\n    \n    if x<= 18.5:\n        return \"underweight\"\n    elif x> 18.5 and x<= 25:\n        return \"normal\"\n    elif x> 25 and x<= 30:\n        return \"overweight\"\n    elif x> 30 and x<= 35:\n        return \"obese_class1\"\n    elif x> 35 and x<= 40:\n        return \"obese_class2\"\n    elif x> 40:\n        return \"obese_class3\"\n    \ndef _group_feature_eng(combined_df, n_train, group_var, num_feats):\n    \n    \"\"\"\n    combined_df: the combined train & test datasets.\n    n_train: number of training observations\n    group_var: the variable we'd like to group by\n    num_feat: numerical features\n    \n    This function loops through all numerical features, \n    group by the variable and compute new statistics of the numerical features.\n    \"\"\"\n    \n    grouped = combined_df.groupby(group_var)\n\n    for nf in num_feats:\n\n        combined_df[group_var + '_' + nf + '_max'] = grouped[nf].transform('max')\n        combined_df[group_var + '_' + nf + '_min'] = grouped[nf].transform('min')\n        combined_df[group_var + '_' + nf + '_mean'] = grouped[nf].transform('mean')\n        combined_df[group_var + '_' + nf + '_skew'] = grouped[nf].transform('skew')\n        combined_df[group_var + '_' + nf + '_std'] = grouped[nf].transform('std')\n\n    train_X = combined_df.iloc[:n_train]\n    test_X = combined_df.iloc[n_train:]\n    \n    return train_X, test_X","ace1419b":"# combine training & testing sets\nnum_feats = set([c for c in train_X.columns if 'glucose' in c])\nn_train = train_X.shape[0]\nn_test = test_X.shape[0]\ncombined_df = pd.concat([train_X, test_X])\n\n# create new categorical variables\ncombined_df['age_group'] = combined_df['age'].apply(age_group)\ncombined_df['bmi_group'] = combined_df['bmi'].apply(get_bmi_cat)\n\n# create new features from newly created categorical variables\nage_group_feat_eng = True\nbmi_group_feat_eng = True\n\nif age_group_feat_eng:\n    train_X, test_X = _group_feature_eng(combined_df, n_train, 'age_group', num_feats)\n\nif bmi_group_feat_eng:\n    train_X, test_X = _group_feature_eng(combined_df, n_train, 'bmi_group', num_feats)","fe43a004":"def get_lab_metrics_fe(_X, lab_metrics):\n    \n    _X_lab_metrics = pd.DataFrame()\n    for l in lab_metrics:\n        try:\n            _X_lab_metrics[l + '_max_minus_min'] = _X[l + '_max'] - _X[l + '_min']\n        except:\n            pass\n        \n    return _X_lab_metrics\n\n# find features that has a \"_max\" in it, these are usually vitals & lab features\nlab_metrics = [c.replace(\"_max\", \"\") for c in train_X.columns.tolist() if 'max' in c]\n\ntrain_lab_metrics_fe = get_lab_metrics_fe(train_X, lab_metrics)\ntest_lab_metrics_fe = get_lab_metrics_fe(test_X, lab_metrics)","e1bc06de":"# combine engineered features into new dataframes\n\ntrain_X_ml = pd.concat([\n    train_X, \n    train_lab_metrics_fe,\n], axis = 1)\n\ntest_X_ml = pd.concat([\n    test_X, \n    test_lab_metrics_fe,\n], axis = 1)","fcd13942":"# add the newly created categorical variables to the list of categorical features:\ncat_feats = cat_feats + ['age_group', 'bmi_group']","2c8afbf4":"def mean_encoder(train_X, train_y, test_X, cat_feats, y_var, n_thresh):\n    \n    \"\"\"\n    cat_feats: the categorical variable names\n    n_thresh: the sample size threshold within the values of the categories, \n              this is a regularization parameter to avoid small sample size estimates\n    \"\"\"\n    \n    train = pd.concat([train_X, train_y], axis = 1)\n    \n    train_mean_enc = pd.DataFrame()\n    test_mean_enc = pd.DataFrame()\n    \n    for c in cat_feats:\n        \n        mean_enc = train.groupby(c)[y_var].mean()\n        count_enc = train.groupby(c)[y_var].size()\n        mean_enc_map = mean_enc[count_enc > n_thresh]\n        train_mean_enc[c] = train[c].map(mean_enc_map)\n        test_mean_enc[c] = test_X[c].map(mean_enc_map)\n        \n    return train_mean_enc, test_mean_enc","b892e55d":"train_mean_enc, test_mean_enc = mean_encoder(train_X, train_y, test_X, cat_feats, y_var, 500)\ntrain_X_ml[cat_feats] = train_mean_enc\ntest_X_ml[cat_feats] = test_mean_enc","df7d71b8":"import lightgbm as lgb","f174a9b0":"def run_adversial_validation(train_X_ml, test_X_ml):\n    \n    lgb_params = {'n_estimators':100,\n                'boosting_type': 'gbdt',\n                'objective': 'binary',\n                'metric': 'auc',\n                    }\n    # combine train & test features, create label to identify test vs train\n    ad_y = np.array([1]*train_X_ml.shape[0] + [0]*test_X_ml.shape[0])\n    ad_X = pd.concat([train_X_ml, test_X_ml])\n\n    # evaluate model performance using cross-validation\n    lgb_data = lgb.Dataset(ad_X, ad_y)\n    cv_lgb = lgb.cv(lgb_params, lgb_data)\n\n    print(\"Adversarial Validation AUC Score: {}\".format(cv_lgb['auc-mean'][-1]))\n    \n    # train model & get feature importance\n    ad_val_mod = lgb.train(lgb_params, lgb_data)\n    \n    print(pd.DataFrame(\n        {'feat':ad_X.columns, \n         'imp':ad_val_mod.feature_importance()}).sort_values('imp', ascending = False))\n    \n    return ad_val_mod","883d2105":"ad_val_mod = run_adversial_validation(train_X_ml, test_X_ml)","4cbf4911":"# source: https:\/\/medium.com\/ai-in-plain-english\/catboost-cross-validated-bayesian-hyperparameter-tuning-91f1804b71dd\n\nX1, Y1 = train_X_ml, train_y\n\nfrom catboost import Pool, cv, CatBoostClassifier\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import * \nfrom sklearn.metrics import *\n\n#n_estimators,\n# num_leaves\ndef CB_opt( depth, learning_rate, max_bin, subsample, l2_leaf_reg, model_size_reg): \n    \n    scores = []\n    skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 1944)\n    for train_index, test_index in skf.split(X1, Y1):\n           \n        trainx, valx = X1.iloc[train_index], X1.iloc[test_index]\n        trainy, valy = Y1.iloc[train_index], Y1.iloc[test_index]\n \n        reg = CatBoostClassifier(   \n                                    verbose = 0,\n                                    #iterations=10,\n                                    #n_estimators = 10,\n                                    learning_rate = learning_rate,\n                                    subsample = subsample, \n                                    l2_leaf_reg = l2_leaf_reg,\n                                    max_depth = int(depth),\n                                    #num_leaves = int(num_leaves),\n                                    random_state = 1212,\n                                    #grow_policy = \"Lossguide\",\n                                    max_bin = int(max_bin),  \n                                    use_best_model = True, \n                                    # bootstrap_type='Bayesian',\n                                    #eval_metric='AUC',\n                                    custom_metric= ['Logloss', 'AUC'],\n                                    model_size_reg = model_size_reg\n                                )\n    \n    reg.fit(trainx, trainy, eval_set = (valx, valy))\n    scores.append(matthews_corrcoef(valy, reg.predict(valx)))\n    \n    return np.mean(scores)\n\n#\"n_estimators\": (150,1200),\n# \"num_leaves\": (100,150),\npbounds = {\n           \"depth\": (7,11),\n           \"learning_rate\": (.04, 0.1),\n           \"subsample\":(0.2, 1.),\n           \"max_bin\":(150,300),\n           \"l2_leaf_reg\":(1,9),\n           \"model_size_reg\": (0,10)\n}\n\noptimizer = BayesianOptimization(f = CB_opt, pbounds = pbounds,  verbose = 2, random_state = 1212)\n\nn_iter = 5\ninit_points = 2\n    \noptimizer.maximize(init_points = 2, n_iter = 3, acq = 'ucb', alpha = 1e-6)\n\nprint(optimizer.max)","81a6a405":"from bayes_opt import BayesianOptimization as BO\nimport warnings\n\nmax_bo_params = optimizer.max['params']","5826d639":"opt_params = {\n              'iterations':1000,\n              'verbose':0,\n              'learning_rate' : 0.01,\n              'subsample' : max_bo_params['subsample'], \n              'l2_leaf_reg' : max_bo_params['l2_leaf_reg'],\n              'max_depth' : int(max_bo_params['depth']),\n              'max_bin' : int(max_bo_params['max_bin']),  \n              'use_best_model' : True, \n              'custom_metric': ['Logloss', 'AUC'],\n               'model_size_reg' : max_bo_params['model_size_reg']\n             }\n               #n_estimators = 10,\n               #num_leaves = int(num_leaves),\n               #random_state = 1212,\n               #grow_policy = \"Lossguide\",\n               # bootstrap_type='Bayesian',\n               #eval_metric='AUC'","d5fff393":"## catBoost Pool object\ntrain_pool = Pool(data=X1,label = Y1)\n\nX_train, X_test, y_train, y_test = train_test_split(X1, Y1, test_size=0.33, stratify=Y1)\n\nbst = CatBoostClassifier(**opt_params)\nbst.fit(train_pool, eval_set=(X_test, y_test), plot=True,silent=True)\nprint(bst.get_best_score())","4d362c2b":"def get_pseudo_data(test_X_ml, test_y_hat, pos_thresh, neg_thresh, pseudo_frac):\n    \"\"\"\n    pos_thresh: threshold probability for positive class\n    neg_thresh: threshold probability for negative class\n    pseudo_frac: the proportion of sampling\n    \"\"\"\n    \n    # subset confident predictions - class 1\n    test_X_pseudo_1 = test_X_ml.iloc[np.where(test_y_hat > pos_thresh)[0]]\n    test_y_pseudo_1 = [1]*test_X_pseudo_1.shape[0]\n\n    # subset confident predictions - class 0 \n    test_X_pseudo_0 = test_X_ml.iloc[np.where(test_y_hat < neg_thresh)[0]]\n    test_y_pseudo_0 = [0]*test_X_pseudo_0.shape[0]\n\n    # combine confident test examples\n    test_pseudo = pd.concat([test_X_pseudo_1, test_X_pseudo_0]).reset_index(drop = True)\n    test_pseudo[y_var] = pd.Series(test_y_pseudo_1+ test_y_pseudo_0)\n    test_pseudo_sample = test_pseudo.sample(int(pseudo_frac*test_X_ml.shape[0]))\n    \n    test_pseudo_y = test_pseudo_sample[y_var]\n    test_pseudo_X = test_pseudo_sample.drop(y_var, axis = 1)\n    \n    return test_pseudo_X, test_pseudo_y","85710e18":"def train_pseudo_label(test_X_ml, test_y_hat, train_X_ml, train_y, pos_thresh, neg_thresh, pseudo_frac, opt_params):\n    \n    # get confident test examples\n    test_pseudo_X, test_pseudo_y = get_pseudo_data(test_X_ml, test_y_hat, pos_thresh, neg_thresh, pseudo_frac)\n    \n    # add confident test examples to training data\n    train_X_pseudo_ml = pd.concat([train_X_ml, test_pseudo_X])\n    train_y_pseudo = pd.concat([train_y, test_pseudo_y])\n    \n    # retrain model\n    lgb_data = lgb.Dataset(train_X_pseudo_ml, train_y_pseudo)\n    bst = lgb.train(opt_params, lgb_data)\n    \n    return bst","45f7204f":"pseudo_label = True\n\n# run pseudo-labelling\n\nif pseudo_label:\n    test_y_hat = bst.predict(test_X_ml)\n    bst = train_pseudo_label(test_X_ml, test_y_hat, train_X_ml, train_y, 0.8, 0.1, 0.2, opt_params)","24c89204":"# submission data\n\ntest_y_hat = bst.predict(test_X_ml)\ntest_pred_map = dict(zip(test_encounter_id, test_y_hat))\nsolution_temp['diabetes_mellitus'] = solution_temp['encounter_id'].map(test_pred_map)\nsolution_temp.to_csv('submission_df_cat_v2.csv', index = False)","d0481fb7":"This notebook is a copy of the following notebook and I just replaced the LightGBM with CatBoost.\n\nhttps:\/\/www.kaggle.com\/dkaing\/tips-tricks","943785b4":"You can increase the number of estimators and reduce the learning rate. That can give you a boost in performance sometimes.","9179bc0f":"# Feature Encoding","7c39a9e1":"# Pseudo-Labeling\n\nThe concept in pseudo labeling is to:\n\n1. Use a good model to predict your testing data.\n2. For the predictions that you're very confident in (you can use probability thresholds to determine \"confidence\", i.e. predicted probability greater than 0.8 for class 1 and less than 0.1 for class 0), use the predicted label as the label of your testing data.\n3. Combine data from 2 with your training data and rerun your model.\n\nThe concept behind this is to \"get\" more data, and we're borrowing data from a subset of the testing data that we are very confident in their predicted labels. Note, you don't want to do too much of pseudo-labeling as the highly \"confident predictions\" may or may not be the actual labels. ","69522efe":"For the vitals & lab features, you'll notice, there are max & min values. We can extract features out of these, for example, by taking the difference between max & min values. This function does that:","b73704b0":"# Ensemble, Ensemble, Ensemble\n\nThe last tip is to combine different predictions. For example, you can combine your previous submissions, or models trained using different approaches, etc... ","d84992ef":"# Bayesian Optimization for CatBoost\n\n[Here's](http:\/\/en.wikipedia.org\/wiki\/Bayesian_optimization#:~:text=Bayesian%20optimization%20is%20a%20sequential,expensive%2Dto%2Devaluate%20functions.) a wikipedia article about Bayesian Optimization, it's essentially a way to find good parameters by searching for these parameters sequentially. So the next parameter search values depend on the performance of the previous parameter values. This is a popular technique for finding optimal parameters. This may take some time to run. Also, you can tune many parameters, below are just some of the parameters that I choose to tune. For this notebook, I'm going to fix the number of estimators to be 100 to save time when running this notebook, but you can increase it to your liking or tune that parameter too if you'd like. You can change the number of iterations and initial points.","20baa6fe":"# Feature Engineering","c506d5d8":"# Data Import","1250d201":"To run most machine learning algorithms, you have to feed it numerical columns, so if you have categorical features, you can dummify your data, but the problem with that approach is that you'll have an exploding number of features. Another approach is to use feature encoding techniques. One of these techniques is called target or mean encoding. The method entails replacing your categorical values by the average of your target values. For example, you can group by ethinicity, compute the proportion of diabetes by ethnicity and now use these statistics to replace the values in your ethinicity column.\n\n**Caution!!** This can lead to overfitting, especially if there's high cardinality in your data (i.e. a lot of unique values and not much observations for each value). Intuitively, when we're using target encoding, we're replacing the categorical values by the \"risk of diabetes\", so this new encoding feature will then have these risk scores. However again, keep in mind that to reliable calculate these \"risks\", we'll need to have a lot of samples, otherwise, this can lead to poor estimations of these risks and lead to overfitting. Use this with caution.\n","9a18251e":"Notice that if your AUC score from adversarial validation is very high. This would suggest that given the features, the model can accurately predict train vs test, which means there are some feature(s) that can accurately classify train vs test - hence suggesting that the distributions of some of these features may not be the same in train vs test.","f2a3184d":"# Adversarial Validation","b6c07d2b":"To understand the existing parameters: https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html","a6885966":"Feature engineering is a crucial step in building a good machine learning model. Designing good features requires a good understanding of the problem and the data. In this section, we'll create new features: \"age_group\" and \"bmi_group\". For the BMI classification, I used this [link](http:\/\/www.cdc.gov\/obesity\/adult\/defining.html#:~:text=Adult%20Body%20Mass%20Index%20(BMI)&text=If%20your%20BMI%20is%20less,falls%20within%20the%20obese%20range.). With these new categorical variables, we can also create new features, for example, group by the categories and compute the mean of glucose-related features.","0a8391bc":"# Submission","fe858841":"It's crucial for your train & test data distribution to have similar distributions. We don't want to just train on a dataset that included only young patients and predict it on old patients, because young people can have different health outcomes than old people. We ideally want the data in both sets to have a similar amount of young & old patients. \n\nWe can test to see whether or not our data comes from the same distribution by doing the following:\n\n1. Combine our train & test features into one set\n2. Create a target label to indicate train vs test\n3. Build a model to classify train vs test\n\nIf the model can distinguish between train vs test very well, this means there is\/are feature(s) in the data that can separate the two sets. Sometimes, after doing feature engineering, you can design features that can have different distributions in train vs test. Adversarial validation allows you to test for those features by 1) seeing if the model discriminate between train vs test well and 2) using the feature importance to identify such features. We're going to use AUC as the metric to evaluate the separation between our train & test datasets."}}