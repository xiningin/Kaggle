{"cell_type":{"ec7f86fd":"code","5ab875e9":"code","11eb22da":"code","710908d8":"code","4043e042":"code","7f2ccced":"code","c73e6df6":"code","05282a28":"code","9493c712":"code","17efbc1c":"code","9d8f8ef0":"code","b600ec3c":"code","a2d1ec78":"code","7e4c119b":"code","932e7f94":"code","1156e373":"code","fc0083ad":"code","a3c494f4":"code","2ce70fbe":"code","3e677ad8":"code","e8c88722":"code","493dbff0":"code","fd858031":"code","69c6b37f":"code","c25501d2":"code","d41dfe50":"code","cc9f9524":"code","23c853cf":"code","bbc0379c":"code","0ed7f8a6":"code","dacda01e":"code","0f92e911":"code","2fd63797":"code","efead4c4":"code","54e861ea":"code","1c503def":"code","ca69429e":"code","929c1c23":"code","c15c1677":"code","e50a5324":"code","57b66df0":"code","f3b6d727":"code","bdf35d83":"code","b7040b24":"code","d0ead4fe":"code","4ec0be5c":"code","8489a73a":"code","e1d070ca":"code","86432fba":"code","0c9bfe4c":"code","e85d678e":"code","5f4b8a9c":"code","6a5635b4":"code","0d46deab":"code","0cae9cdc":"code","d6bcb7b2":"code","eaf44d70":"code","9afa014b":"code","9afa5f38":"markdown","2b2146bb":"markdown","a7816001":"markdown","05176305":"markdown","df6e635c":"markdown","8cac7de0":"markdown","d4e3772d":"markdown","62c15137":"markdown","3a3e2a53":"markdown","3537deef":"markdown","ee25d7ce":"markdown","bcce70cf":"markdown","3bd2ea6b":"markdown","40c2b89d":"markdown","38cc863b":"markdown","4ad8ce78":"markdown","a279fa04":"markdown","5c6260c9":"markdown","80207e01":"markdown","9afed5e7":"markdown","e40b0fd1":"markdown","763eec9f":"markdown","d7171e2f":"markdown","c3f989fc":"markdown","5819cb79":"markdown","63dc598f":"markdown","4e11876b":"markdown","950b5422":"markdown","3ee48a66":"markdown","00643512":"markdown","9c0cbf58":"markdown","542b69aa":"markdown","6f6ead7c":"markdown","40dc52c6":"markdown","9165d43a":"markdown","5782c412":"markdown","14f96854":"markdown","f86c99b2":"markdown","af72d7f7":"markdown","f3679392":"markdown","b3a2c4d6":"markdown","d593347a":"markdown","7b8ea749":"markdown","26199e84":"markdown","ead4c14a":"markdown","7b1024e3":"markdown","c3f77562":"markdown","69385152":"markdown","510925d5":"markdown","fa858933":"markdown","d0089b36":"markdown","69f58e89":"markdown","6c6ff438":"markdown","c3679df7":"markdown","51f3bba7":"markdown","bc368b28":"markdown","2b152cbd":"markdown","dbda12f7":"markdown","f36ec0d5":"markdown","386706d4":"markdown","05d2cae4":"markdown","d8776579":"markdown","74b46673":"markdown","2b70c6d0":"markdown","773b1dd7":"markdown","244d30db":"markdown","69d9b2d1":"markdown","d8ac7395":"markdown","ab3e0b6a":"markdown","7f63a4fd":"markdown","8b8141b5":"markdown","1b5b0aca":"markdown","7a25c398":"markdown","86007756":"markdown","a7ee4f71":"markdown","747ebe2f":"markdown","56e03d33":"markdown","a71accb6":"markdown","b67106d0":"markdown","9e869cfb":"markdown"},"source":{"ec7f86fd":"import numpy as np\nimport pandas as pd\npd.set_option('mode.chained_assignment',None)\nfrom pandas import Series, DataFrame\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","5ab875e9":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf","11eb22da":"nas = df.isna().sum()\nnas[nas>0]","710908d8":"cols = ['Alley', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\ndf[cols] = df[cols].fillna('None')","4043e042":"df[(df['BsmtExposure'].isna()) & (df['TotalBsmtSF']>0)].iloc[:,30:40]","7f2ccced":"df[(df['BsmtFinType2'].isna()) & (df['TotalBsmtSF']>0)].iloc[:,30:40]","c73e6df6":"df.loc[(df['BsmtExposure'].isna()) & (df['TotalBsmtSF']>0),'BsmtExposure'] = 'No'\ndf.loc[(df['BsmtFinType2'].isna()) & (df['TotalBsmtSF']>0),'BsmtFinType2'] = 'Unf'\ncols = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\ndf[cols] = df[cols].fillna('None')","05282a28":"nas = df.isna().sum()\nnas[nas>0]","9493c712":"cols = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LandContour', 'LotConfig', 'Condition1', 'Condition2', 'BldgType', \n        'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'Foundation', 'Heating', 'CentralAir', 'GarageType', \n        'MiscFeature', 'SaleType', 'SaleCondition']\nfig = plt.figure(figsize=(14,50))\nfor c,i in zip(cols, range(1,21)):\n    ax = fig.add_subplot(10,2,i)\n    sns.boxplot(x=c,y='SalePrice',data=df)\nfig.suptitle('Relation between house sale price with all nominal variables',y=0.999)\nfig.tight_layout(pad=4.0)\nplt.figure(figsize=(14,5))\nsns.boxplot(x='Exterior1st',y='SalePrice',data=df)\nplt.figure(figsize=(14,5))\nsns.boxplot(x='Exterior2nd',y='SalePrice',data=df)\nplt.figure(figsize=(24,5))\nsns.boxplot(x='Neighborhood',y='SalePrice',data=df)","17efbc1c":"cols = ['LotShape', 'Utilities', 'LandSlope', 'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond',  'BsmtQual', 'BsmtCond', \n        'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', \n        'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence']\nfig = plt.figure(figsize=(14,57))\nfor c,i in zip(cols, range(1,24)):\n    ax = fig.add_subplot(12,2,i)\n    sns.boxplot(x=c,y='SalePrice',data=df)\nfig.suptitle('Relation between house sale price with all ordinal variables', y=0.999)\nfig.tight_layout(pad=4.0)","9d8f8ef0":"cols1 = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n         'GarageCars', 'MoSold', 'YrSold']\ncols2 = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']\nfig = plt.figure(figsize=(15,16))\nfor c,i in zip(cols1, range(1,12)):\n    ax = fig.add_subplot(4,3,i)\n    sns.boxplot(x=c,y='SalePrice',data=df)\nfig.suptitle('Relation between house sale price with all discrete numerical variables', y=0.999)\nfig.tight_layout(pad=2.0)\nfig = plt.figure(figsize=(22,5))\nfor c,i in zip(cols2, range(1,4)):\n    ax = fig.add_subplot(1,3,i)\n    sns.scatterplot(x=c,y='SalePrice',data=df)","b600ec3c":"cols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n        'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \n        'PoolArea', 'MiscVal']\nfig = plt.figure(figsize=(15,30))\nfor c,i in zip(cols, range(1,20)):\n    ax = fig.add_subplot(7,3,i)\n    sns.scatterplot(x=c,y='SalePrice',data=df)\nfig.suptitle('Relation between house sale price with all discrete numerical variables', y=0.999)\nfig.tight_layout(pad=2.0)","a2d1ec78":"plt.rcParams['figure.figsize'] = [42, 24]\nsns.heatmap(df.drop(columns=['Id','MSSubClass']).corr(method='pearson'),annot=True,cmap='RdBu_r')","7e4c119b":"fig = plt.figure(figsize=(14,11))\nfig.add_subplot(2,2,1)\nsns.boxplot(x='GarageCars',y='GarageArea',data=df)\nfig.add_subplot(2,2,2)\nsns.boxplot(x='TotRmsAbvGrd',y='GrLivArea',data=df)\nfig.add_subplot(2,2,3)\nsns.scatterplot(x='YearBuilt',y='GarageYrBlt',data=df)\nfig.add_subplot(2,2,4)\nsns.scatterplot(x='1stFlrSF',y='TotalBsmtSF',data=df)","932e7f94":"plt.figure(figsize=(7,5))\nsns.scatterplot(x='GrLivArea',y='SalePrice',data=df)","1156e373":"df = df[df['GrLivArea']<4000]","fc0083ad":"nas = df.isna().sum()\nnas[nas>0]","a3c494f4":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ndf.loc[:,'Electrical'] = imp.fit_transform(np.array(df['Electrical']).reshape(-1,1))","2ce70fbe":"df['MasVnrType'] = df['MasVnrType'].fillna('None')\ndf['MasVnrArea'] = df['MasVnrArea'].fillna(0)","3e677ad8":"df.loc[df['GarageYrBlt'].isna(),'GarageYrBlt'] = df.loc[df['GarageYrBlt'].isna(),'YearBuilt']","e8c88722":"df.loc[df['LotFrontage'].isna(),'LotFrontage'] = 0","493dbff0":"nas = df.isna().sum()\nnas[nas>0]","fd858031":"#encoding ordinal features\ndf = df.replace({'LotShape' : {'Reg' : 0, 'IR1' : 1, 'IR2' : 2, 'IR3' : 3},\n                'Utilities' : {'AllPub' : 4, 'NoSewr' : 3, 'NoSeWa' : 2, 'ELO' : 1},\n                'LandSlope' : {'Gtl' : 1, 'Mod' : 2, 'Sev' : 3},\n                'ExterQual' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1},\n                'ExterCond' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1},\n                'BsmtQual' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, 'None' : 0},\n                'BsmtCond' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, 'None' : 0},\n                'BsmtExposure' : {'Gd' : 4, 'Av' : 3, 'Mn' : 2, 'No' : 1, 'None' : 0},\n                'BsmtFinType1' : {'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'None' : 0},\n                'BsmtFinType2' : {'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'None' : 0},\n                'HeatingQC' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1},\n                'Electrical' : {'SBrkr' : 5, 'FuseA' : 4, 'FuseF' : 3, 'FuseP' : 2, 'Mix' : 1},\n                'KitchenQual' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1},\n                'Functional' : {'Typ' : 8, 'Min1' : 7, 'Min2' : 6, 'Mod' : 5, 'Maj1' : 4, 'Maj2' : 3, 'Sev' : 2, 'Sal' : 1},\n                'FireplaceQu' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, 'None' : 0},\n                'GarageFinish' : {'Fin' : 3, 'RFn' : 2, 'Unf' : 1, 'None' : 0},\n                'GarageQual' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, 'None' : 0},\n                'GarageCond' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, 'None' : 0},\n                'PavedDrive' : {'Y' : 2, 'P' : 1, 'N' : 0},\n                'PoolQC' : {'Ex' : 4, 'Gd' : 3, 'TA' : 2, 'Fa' : 1, 'None' : 0},\n                'Fence' : {'GdPrv' : 4, 'MnPrv' : 3, 'GdWo' : 2, 'MnWw' : 1, 'None' : 0},\n                })","69c6b37f":"#encoding nominal features\ndf2 = pd.concat([df,pd.get_dummies(df['MSSubClass'],prefix='SubClass',drop_first=True)],axis=1)\ndf3 = pd.concat([df2,pd.get_dummies(df2[['MSZoning','Street','Alley','LandContour','LotConfig','Neighborhood',\n                                        'Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl',\n                                        'Exterior1st','Exterior2nd','MasVnrType','Foundation','Heating','CentralAir',\n                                        'GarageType','MiscFeature','SaleType','SaleCondition']],drop_first=True)],axis=1)\ndf4 = df3.drop(columns=['MSSubClass','MSZoning','Street','Alley','LandContour','LotConfig','Neighborhood',\n                                        'Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl',\n                                        'Exterior1st','Exterior2nd','MasVnrType','Foundation','Heating','CentralAir',\n                                        'GarageType','MiscFeature','SaleType','SaleCondition'])","c25501d2":"#drop Id column\ndata = df4.drop(columns='Id')\ndata","d41dfe50":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nX = data.drop('SalePrice',axis=1)\ny = data['SalePrice']","cc9f9524":"from sklearn.metrics import mean_squared_log_error, make_scorer\n\ndef funct(y_true,y_pred):\n    y_new = np.maximum(y_pred,np.zeros(len(y_pred)))\n    t = mean_squared_log_error(y_true,y_new)\n    return np.sqrt(t)\n\nRMSLE = make_scorer(funct, greater_is_better=False)","23c853cf":"print('Number of features: %d' %(len(X.columns)))","bbc0379c":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\nfs = SelectKBest(score_func=f_regression, k='all')\nfs.fit(X,y)\nindices = np.argsort(fs.scores_)[::-1]\ncc = DataFrame({'feature score':Series(fs.scores_),'features':Series(X.columns)})    \nplt.figure(figsize=(10,35))\nsns.barplot(x='feature score',y='features',data=cc.head(50).sort_values(by='feature score',ascending=False))\nplt.title('Feature importances based on regression')","0ed7f8a6":"new_col = np.array(Series(X.columns[indices]).head(100))\nX_new1 = X[new_col]","dacda01e":"from sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline([(\"scaler\", StandardScaler()),(\"reg\", Ridge())])\nparam_grid = {'reg__alpha': [630, 640, 650, 680, 700]}\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5, scoring=RMSLE)\ngrid.fit(X_new1,y)\nprint('Best hyperparameter: ', grid.best_params_)\nprint('Best cross validation score: ', -grid.best_score_)","0f92e911":"y_true = y\ny_pred = grid.predict(X_new1)\nplt.figure(figsize=(7,5))\nsns.scatterplot(y_true,y_pred)\nplt.plot(y_true,y_true,color='k')\nplt.xlabel('SalePrice (actual)')\nplt.ylabel('SalePrice (predicted)')","2fd63797":"from sklearn.ensemble import ExtraTreesRegressor\nforest = ExtraTreesRegressor(n_estimators=100,\n                              random_state=0,n_jobs=4)\nforest.fit(X, y)\nindices = np.argsort(forest.feature_importances_)[::-1]\ncc = DataFrame({'feature score':Series(forest.feature_importances_),'features':Series(X.columns)})    \nplt.figure(figsize=(10,35))\nsns.barplot(x='feature score',y='features',data=cc.head(50).sort_values(by='feature score',ascending=False))","efead4c4":"new_col = np.array(Series(X.columns[indices]).head(100))\nX_new2 = X[new_col]","54e861ea":"from sklearn.ensemble import ExtraTreesRegressor\nforest = ExtraTreesRegressor(random_state=0,n_jobs=4,n_estimators=500)\nparam_grid = {'min_samples_split': [2, 3, 4], 'max_depth' : [40, 50]}\ngrid = GridSearchCV(forest, param_grid=param_grid, cv=5, scoring=RMSLE)\ngrid.fit(X_new2,y)\nprint('Best hyperparameter: ', grid.best_params_)\nprint('Best cross validation score: ', -grid.best_score_)","1c503def":"y_true = y\ny_pred = grid.predict(X_new2)\nplt.figure(figsize=(7,5))\nsns.scatterplot(y_true,y_pred)\nplt.plot(y_true,y_true,color='k')\nplt.xlabel('SalePrice (actual)')\nplt.ylabel('SalePrice (predicted)')","ca69429e":"from sklearn.ensemble import GradientBoostingRegressor\ngdb = GradientBoostingRegressor(n_estimators = 300, random_state=0)\ngdb.fit(X,y)\nindices = np.argsort(gdb.feature_importances_)[::-1]\ncc = DataFrame({'feature score':Series(gdb.feature_importances_),'features':Series(X.columns)})    \nplt.figure(figsize=(10,35))\nsns.barplot(x='feature score',y='features',data=cc.head(50).sort_values(by='feature score',ascending=False))","929c1c23":"new_col = np.array(Series(X.columns[indices]).head(100))\nX_new3 = X[new_col]","c15c1677":"from sklearn.ensemble import GradientBoostingRegressor\ngdb = GradientBoostingRegressor(random_state=0, n_estimators=400)\nparam_grid = {'min_samples_split' : [25, 30, 35], 'min_samples_leaf' : [2, 3]}\ngrid = GridSearchCV(gdb, param_grid=param_grid, cv=5, scoring=RMSLE)\ngrid.fit(X_new3,y)\nprint('Best hyperparameter: ', grid.best_params_)\nprint('Best cross validation score: ', -grid.best_score_)","e50a5324":"y_true = y\ny_pred = grid.predict(X_new3)\nplt.figure(figsize=(7,5))\nsns.scatterplot(y_true,y_pred)\nplt.plot(y_true,y_true,color='k')\nplt.xlabel('SalePrice (actual)')\nplt.ylabel('SalePrice (predicted)')","57b66df0":"from sklearn.metrics import r2_score\n\n#Ridge regression\npipe = Pipeline([(\"scaler\", StandardScaler()),(\"reg\", Ridge(alpha=630))])\nscore = cross_val_score(pipe, X_new1, y, cv=5, scoring='r2')\nprint('R\\u00b2 score for Ridge regression: %f' %(score.mean()))\n\n#Extra Trees\nforest = ExtraTreesRegressor(random_state=0, n_jobs=4, n_estimators=500, max_depth=40, min_samples_split=3)\nscore = cross_val_score(forest, X_new2, y, cv=5, scoring='r2')\nprint('R\\u00b2 score for Extra Trees: %f' %(score.mean()))\n\n#Gradient Boosting\ngdb = GradientBoostingRegressor(random_state=0, n_estimators=400, min_samples_leaf=3, min_samples_split=25)\nscore = cross_val_score(forest, X_new3, y, cv=5, scoring='r2')\nprint('R\\u00b2 score for Gradient Boosting: %f' %(score.mean()))","f3b6d727":"dt = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndt","bdf35d83":"nas = dt.isna().sum()\nnas[nas > 0]","b7040b24":"cols = ['Alley','FireplaceQu','PoolQC','Fence','MiscFeature','MasVnrType', 'BsmtFinType1', 'BsmtFinType2']\ndt[cols] = dt[cols].fillna('None')\ndt.loc[dt['SaleType'].isna(),'SaleType'] = 'Oth'","d0ead4fe":"cols = ['LotFrontage','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath',\n       'GarageCars','GarageArea']\ndt[cols] = dt[cols].fillna(0)","4ec0be5c":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ncols = ['MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'KitchenQual', 'Functional']\ndt[cols] = imp.fit_transform(dt[cols])","8489a73a":"dt[(dt['BsmtQual'].isna()) & (dt['TotalBsmtSF']>0)].iloc[:,30:40]","e1d070ca":"dt[(dt['BsmtCond'].isna()) & (dt['TotalBsmtSF']>0)].iloc[:,30:40]","86432fba":"dt[(dt['BsmtExposure'].isna()) & (dt['TotalBsmtSF']>0)].iloc[:,30:40]","0c9bfe4c":"dt.loc[(dt['BsmtQual'].isna()) & (dt['TotalBsmtSF']>0),'BsmtQual'] = dt['BsmtQual'].value_counts().idxmax()\ndt.loc[(dt['BsmtCond'].isna()) & (dt['TotalBsmtSF']>0),'BsmtCond'] = dt['BsmtCond'].value_counts().idxmax()\ndt.loc[(dt['BsmtExposure'].isna()) & (dt['TotalBsmtSF']>0),'BsmtExposure'] = 'No'\ncols = ['BsmtQual','BsmtCond','BsmtExposure']\ndt[cols] = dt[cols].fillna('None')","e85d678e":"dt[(dt['GarageFinish'].isna()) & (dt['GarageArea']>0)].iloc[:,57:67]","5f4b8a9c":"dt[(~dt['GarageType'].isna()) & (dt['GarageArea'] == 0)].iloc[:,57:67]","6a5635b4":"dt.loc[(dt['GarageFinish'].isna()) & (dt['GarageArea']>0),'GarageFinish'] = 'Unf'\ndt.loc[(dt['GarageQual'].isna()) & (dt['GarageArea']>0),'GarageQual'] = dt['GarageQual'].value_counts().idxmax()\ndt.loc[(dt['GarageCond'].isna()) & (dt['GarageArea']>0),'GarageCond'] = dt['GarageCond'].value_counts().idxmax()\ndt.loc[(~dt['GarageType'].isna()) & (dt['GarageArea'] == 0),'GarageType'] = 'None'\ncols = ['GarageType','GarageFinish','GarageQual','GarageCond']\ndt[cols] = dt[cols].fillna('None')\ndt.loc[dt['GarageYrBlt'].isna(),'GarageYrBlt'] = dt.loc[dt['GarageYrBlt'].isna(),'YearBuilt']","0d46deab":"nas = dt.isna().sum()\nnas[nas > 0]","0cae9cdc":"#encoding ordinal features\ndtt = dt.replace({'LotShape' : {'Reg' : 0, 'IR1' : 1, 'IR2' : 2, 'IR3' : 3},\n                'Utilities' : {'AllPub' : 4, 'NoSewr' : 3, 'NoSeWa' : 2, 'ELO' : 1},\n                'LandSlope' : {'Gtl' : 1, 'Mod' : 2, 'Sev' : 3},\n                'ExterQual' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1},\n                'ExterCond' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1},\n                'BsmtQual' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, 'None' : 0},\n                'BsmtCond' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, 'None' : 0},\n                'BsmtExposure' : {'Gd' : 4, 'Av' : 3, 'Mn' : 2, 'No' : 1, 'None' : 0},\n                'BsmtFinType1' : {'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'None' : 0},\n                'BsmtFinType2' : {'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'None' : 0},\n                'HeatingQC' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1},\n                'Electrical' : {'SBrkr' : 5, 'FuseA' : 4, 'FuseF' : 3, 'FuseP' : 2, 'Mix' : 1},\n                'KitchenQual' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1},\n                'Functional' : {'Typ' : 8, 'Min1' : 7, 'Min2' : 6, 'Mod' : 5, 'Maj1' : 4, 'Maj2' : 3, 'Sev' : 2, 'Sal' : 1},\n                'FireplaceQu' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, 'None' : 0},\n                'GarageFinish' : {'Fin' : 3, 'RFn' : 2, 'Unf' : 1, 'None' : 0},\n                'GarageQual' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, 'None' : 0},\n                'GarageCond' : {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, 'None' : 0},\n                'PavedDrive' : {'Y' : 2, 'P' : 1, 'N' : 0},\n                'PoolQC' : {'Ex' : 4, 'Gd' : 3, 'TA' : 2, 'Fa' : 1, 'None' : 0},\n                'Fence' : {'GdPrv' : 4, 'MnPrv' : 3, 'GdWo' : 2, 'MnWw' : 1, 'None' : 0},\n                })","d6bcb7b2":"#encoding nominal features\ndt2 = pd.concat([dtt,pd.get_dummies(dtt['MSSubClass'],prefix='SubClass',drop_first=True)],axis=1)\ndt3 = pd.concat([dt2,pd.get_dummies(dt2[['MSZoning','Street','Alley','LandContour','LotConfig','Neighborhood',\n                                        'Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl',\n                                        'Exterior1st','Exterior2nd','MasVnrType','Foundation','Heating','CentralAir',\n                                        'GarageType','MiscFeature','SaleType','SaleCondition']],drop_first=True)],axis=1)\ndt4 = dt3.drop(columns=['MSSubClass','MSZoning','Street','Alley','LandContour','LotConfig','Neighborhood',\n                                        'Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl',\n                                        'Exterior1st','Exterior2nd','MasVnrType','Foundation','Heating','CentralAir',\n                                        'GarageType','MiscFeature','SaleType','SaleCondition'])\ndt4['Exterior1st_Stone'] = np.zeros(len(dt4))\ndt4","eaf44d70":"X_test = dt4[new_col]\ny_pred = grid.predict(X_test)\nfinal = pd.concat([dt, Series(y_pred,name='SalePrice')], axis=1)\nfinal","9afa014b":"final.to_csv(\"output_ames.csv\") ","9afa5f38":"Finally, we have predicted the sale prices of each property in Ames using our best predictive model based on the training data. The final predicted test data can be exported to CSV file and submitted to Kaggle.","2b2146bb":"Before we can implement our model, missing values from test data have to be handled in the same manner as in training data:\n- Replace NA values with 'None' values in columns: Alley, MasVnrType, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, FireplaceQu, GarageType, GarageFinish, GarageQual, GarageCond, PoolQC, Fence, MiscFeature.\n- Replace NA values in MasVnrArea with zeroes.\n- Replace NA values in GarageYrBlt with corresponding YearBuilt values.\n- Fill NA values in LotFrontage with zeroes.\n\nUnfortunately, there are more NA valus in another columns: MSZoning, Utilities, Exterior1st, Exterior2nd, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath, BsmtHalfBath, KitchenQual, Functional, SaleType. Also, several properites have contradictive information about basement and garage.","a7816001":"Again, before training the data with gradient boosting, we will perform feature selection. This can easily be done because GradientBoostingRegressor provide straightforward method for feature selection by calculating feature importances.","05176305":"One house has unfinished basement (which means it has a basement) but NA value of exposure (which means it has no basement).","df6e635c":"Total 80 features from the dataset can be classified into nominal, ordinal, and numeric variables (can be checked in http:\/\/jse.amstat.org\/v19n3\/decock\/DataDocumentation.txt).\n\nNominal features: Id (primary key), MSSubClass, MSZoning, Street, Alley, LandContour, LotConfig, Neighborhood, Condition1, Condition2, BldgType, HouseStyle, RoofStyle, RoofMatl, Exterior1st, Exterior2nd, MasVnrType, Foundation, Heating, CentralAir, GarageType, MiscFeature, SaleType, SaleCondition\n\nOrdinal features: LotShape, Utilities, LandSlope, OverallQual, OverallCond, ExterQual, ExterCond,  BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, HeatingQC, Electrical, KitchenQual, Functional, FireplaceQu, GarageFinish, GarageQual, GarageCond, PavedDrive, PoolQC, Fence\n\nDiscrete numeric features: YearBuilt, YearRemodAdd, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageYrBlt, GarageCars, MoSold, YrSold\n\nContinuous numeric features: LotFrontage, LotArea, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, 1stFlrSF, 2ndFlrSF, LowQualFinSF, GrLivArea, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal, SalePrice","8cac7de0":"- Replace NA values with 'None' in categorical columns without contradiction.\n- For SaleType column, it is most likely that NA value means that the property has other type of sale.","d4e3772d":"Now it's time to implement the predictive model. Based on the model building step, our best model is Gradient Boosting Regressor wih tuned hyperparameters (min_samples_leaf = 3 and min_samples_split = 25). Feature selection also performed to the test data.","62c15137":"Before applying our best model, test data must be prepared in a proper form:\n- The values of all ordinal features must be manually encoded into integer.\n- Nominal columns must be encoded into dummy variables (one hot encoding).","3a3e2a53":"For GarageYrBlt, replace the NA values with the corresponding YearBuilt values (same treatment as in training data).","3537deef":"There are only 1 NA values in Electrical column which is a categorical variable hence we can 'safely' fill them with the most frequent value of the column.","ee25d7ce":"In this notebook, we are going to analyze and build machine learning model to predict house\/property sale prices in Ames, Iowa. The given dataset consists of several information (80 in total) which might be related to the property sales, e.g. lot area size, shape of property, number of bedroom, roof material etc. \n\nThe aim of this project is to build a model which can predict the property sale prices based on the given information about the property. The property prices can take any positive values (continuous value), hence we are dealing with regression problem. We can also find the factor(s) which mostly affect the property prices.","bcce70cf":"The metric used for evaluating the models is root-mean-squared logarithmic error (RMSLE) between actual and predicted value. Taking the logarithmic value is necessary to compensate high difference values when dealing with expensive houses. For this metric, lower value means better score.","3bd2ea6b":"### Sale price dependence on ordinal features","40c2b89d":"The remaining NA values will be dealt in the later section.","38cc863b":"Next, we begin to train the data using Extra Trees Regressor from sklearn package. Feature scaling is not necessarily needed for tree regressor. Hyperparameter tuning was done using grid search + cross validation method.","4ad8ce78":"We are not done yet. The remaining NA values must be handled.","a279fa04":"Next, we begin to train the data using Gradient Boosting Regressor from sklearn package. Feature scaling is not necessarily needed for this regressor because basically gradient boosting is a tree regressor. Hyperparameter tuning was done using grid search + cross validation method.","5c6260c9":"### Sale price dependence on nominal features","80207e01":"Before building the predictive model, training data must be prepared with several modification: \n- The values of all ordinal features must be manually encoded into integer so it can represent the order\/ranking\/degree of the feature.\n- Nominal columns must be encoded into dummy variables because sklearn package cannot handle string categorical data. For regression problems, one hot encoding is most suitable.\n- Id column is not needed for modelling purpose (unique value for each record) so we can drop it.","9afed5e7":"### Outliers out there...","e40b0fd1":"Scatterplot between SalePrice and GrLivArea indicates that there are at least 4 outliers: 2 very large houses with not so expensive prices and 2 large houses with unusually high prices. These houses have GrLivArea > 4000. The author of this project recommends to not use any records with more than 4000 square feet from the dataset (https:\/\/amstat.tandfonline.com\/doi\/pdf\/10.1080\/10691898.2011.11889627?needAccess=true).","763eec9f":"Two properties have NA values in BsmtExposure (which means they have no basement) but other features show that they have basement. \n\nStrategy &#8594; replace with 'No' value (same treatment as in training data).","d7171e2f":"Next, we will examine the relation between the property sale price with all of the ordinal features, also using boxplot.","c3f989fc":"Next, we will examine the relation between the property sale price with all of the discrete numerical features. This can be achieved by using boxplot or scatterplot. We will use the scatterplot for YearBuilt, YearRemodAdd, and GarageYrBlt for much easier interpretation.","5819cb79":"Several categorical variables give no clue about the NA values so it is quite difficult to interpret them. These variables are MSZoning, Utilities, Exterior1st, Exterior2nd, KitchenQual, and Functional. The number of NA values in each of these columns are quite few, hence we can 'safely' fill them with the most frequent value for each column.","63dc598f":"### Sale price dependence on continuous numerical features","4e11876b":"- Fill NA values in LotFrontage with zeroes.\n- Fill NA values in MasVnrArea with zeroes.\n- The columns BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, and TotalBsmtSF refer to the basement area. Hence, it is most likely that NA values in those columns are actually zeroes &#8594; no basement area. \n- NA values in BsmtFullBath and BsmtHalfBath might be interpreted as no bathroom in basement &#8594; fill with zeroes.\n- NA values in GarageCars and GarageArea certainly indicate that the property has no garage  &#8594; fill with zeroes.","950b5422":"Solution based on naive assumption:\n- Typo when inputing NA value in BsmtExposure &#8594; replace with 'No' (no basement exposure).\n- Typo when inputing NA value in BsmtFinType2 &#8594; replace with 'Unf' (basement unfinished).","3ee48a66":"### Fill with zeroes","00643512":"One property has GarageType of Detchd, but other features show that it has no garage.\n\nStrategy &#8594; it is most likely a typo so replace 'Detchd' value with 'None' value.","9c0cbf58":"Before training the data using tree regressor, again we perform feature selection. This can easily be done because ExtraTreesRegressor provide straightforward method for feature selection by calculating feature importances.","542b69aa":"Now we can proceed to modelling step. We will use linear least squares regression with L2 regularization (also known as Ridge regression) from sklearn package. Feature scaling is needed for this regressor and we will use StandardScaler. Hyperparameter tuning was done using grid search + cross validation method.","6f6ead7c":"For LotFrontage, it seems that we have no clue. Referring to the correlation heatmap figure, we have no features that highly correlated with LotFrontage. So, let's just simply fill the NA values with zeroes. \n\nNote: the reason to choose this action is based on Kaggle discussion here https:\/\/www.kaggle.com\/ogakulov\/lotfrontage-fill-in-missing-values-house-prices) ","40dc52c6":"Several remarks:\n- Properties with paved road and alley access are more expensive than the gravel ones.\n- Most of the properties have no alley access.\n- It seems that properties with hillside slope tend to have higher sale price.\n- Properties with hip and wood shingles roof tend to have higher sale price.\n- Properties with stone masonry veneer tend to have higher sale price.\n- Most of the properties have GasA heating type (gas forced warm air furnace).\n- Properties which have central air conditioning are more expensive.\n- Most of the properties have no miscellaneous (i.e. only one property has a tennis court).\n- Properties which just constructed and sold tend to have higher sale price.","9165d43a":"## Exploratory Data Analysis (EDA)","5782c412":"For the first model, we will use linear regression with regularization. Before fitting data with the proposed model, it is certainly wise to perform feature selection due to high number of features.","14f96854":"## Building Predictive Model","f86c99b2":"Several remarks:\n- Properties with larger total basement area are more expensive than the smaller ones.\n- Properties with larger first floor area tend to have higher sale price, also quite true for second floor area.\n- Clearly, above ground living area reflects the sale price of the properties (larger area means higher price).\n- Properties with larger garage area are more expensive than the smaller ones.\n- It seems that properties with large wood deck and open porch area tend to have higher sale price.","af72d7f7":"For MasVnrType column, it is most likely that NA values means that the properties has no masonry veneer walls ('None'). This also means that the masonry veneer area is zero. ","f3679392":"### Fill with other categorical variable","b3a2c4d6":"Lastly, we will try to find relation between several features (mostly numeric) by using correlation heatmap. Here, Pearson correlation coefficients are calculated for every pair of numerical features. \n\nNote: double click the figure to zoom it.","d593347a":"Next, we will examine the relation between the property sale price with all of the continuous numerical features by using scatterplot.","7b8ea749":"The NA values in this data are quite massive. For instance, Alley and PoolQC columns have 1369 NA values (more than 90% of the records). Nevertheless, do not drop these columns. NA values in several columns simply means that the property doesn't have the mentioned attribute (check the data description): \n- NA value in Alley &#8594; the property has no alley access\n- NA value in PoolQC &#8594; the property has no pool\n- and so on...\n\nWe can simply replace the NA values in several columns by new categorical value 'None'. These columns are: Alley, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, FireplaceQu, GarageType, GarageYrBlt, GarageFinish, GarageQual, GarageCond, PoolQC, Fence, MiscFeature. \n\nException:\n- One house has unfinished basement (which means it has a basement) but NA value of exposure (which means it has no basement) &#8594; contradictive, deal with that later.\n- One house has NA value in BsmtFinType2 (which means it has no basement) but other features show that it has a basement &#8594; contradictive, deal with that later.\n- GarageYrBlt is actually a numerical feature, but it has NA values (which means no garage), so it must be filled with numerical values too &#8594; deal with that later.","26199e84":"### Predictive model: gradient boosting regressor","ead4c14a":"As we can see, several features have high importance score such as OverallQual, GrLivArea, KitchenQual, GarageCars, TotalBsmtSF and so on (also confirmed by plotting the relations), while other features have nearly zero importance score. We will take 100 features with high importance score for our selected features.","7b1024e3":"For Ridge regressor, our best RMSLE score in cross validation scheme is 0.1411 with hyperparameter alpha = 630. Also shown here the line plot between predicted and actual value of the sale price. It can be seen that our model fail to predict correctly  properties with expensive sale price. ","c3f77562":"### Preparing and testing","69385152":"Using all features will be redundant because not all of them have significant relation with the sale price. We will try to select important features using SelectKBest from sklearn package.","510925d5":"Several remarks:\n- Most of the properties have all public utilities (electricity, gas, water, and septic tank).\n- Clearly, overall quality score of the property reflects the sale price (higher score means more expensive).\n- Properties with excellent exterior material quality are more expensive than the others.\n- Properties with excellent basement heights (100+ inches) are more expensive than the others.\n- Properties with excellent kitchen quality are more expensive than the others.\n- It seems that properties with excellent fireplac quality to have higher sale price.\n- Properties with finished excellent quality garage tend to have higher sale price.\n- Properties with excellent pool quality are more expensive than the others.","fa858933":"### Contradictive information about basement","d0089b36":"## Introduction","69f58e89":"### Metric for evaluating model","6c6ff438":"Some features have high correlation with other features:\n- GarageArea and GarageCars &#8594; clearly, larger garage will have more car capacity.\n- GrLivArea and TotRmsAbvGrd &#8594; clearly, larger living area tend to have more rooms.\n- GarageYrBlt and YearBuilt &#8594; it is likely that the house and its garage were built in the same year.\n- TotalBsmtSF and 1stFlrSF &#8594; house with large first floor area tend to have large basement area.\n- SalePrice has high correlation (>0.7) with OverallQual and GrLivArea &#8594; very important factors!\n\nLet's plot some of these relations.","c3679df7":"### Relation between several variables","51f3bba7":"## Handling Missing Values (Continued...)","bc368b28":"### Confusing data about basement","2b152cbd":"One property has NA values in GarageYrBlt, GarageFinish, GarageQual, and GarageCond (which means it has no garage) but other features show that it has a garage. \n\nStrategy &#8594; fill GarageFinish with 'Unf' value (interpret it as unfinished garage).\n\nStrategy &#8594; fill GarageQual and GarageCond with the most frequent value.","dbda12f7":"### Fill with the most frequent value","f36ec0d5":"Two properties have NA values in BsmtQual (which means they have no basement) but other features show that they have basement. \n\nStrategy &#8594; fill with the most frequent value.","386706d4":"### So many NA values...","05d2cae4":"Several features which have high importance score are in accordance to the previous result using SelectKBest. We will take 100 features with high importance score for our selected features.","d8776579":"Several remarks:\n- Properties with more full bathrooms above grade tend to have higher sale price.\n- Properties with one half bathroom above grade are more expensive than the others.\n- Having more rooms in a property (total rooms except bathrooms) tend to increase its sale price. However, if it has too many rooms (>10), the price will decrease. \n- Properties with more fireplaces tend to have higher sale price.\n- Large garage capacity (how many cars can be parked) tend to increase the sale price of the property as long as the capacity is under 3 cars. ","74b46673":"Finally, all of the NA values have been handled.","2b70c6d0":"For Gradient Boosting regressor, our best RMSLE score in cross validation scheme is 0.1200 (9.9% better than Extra Trees) with hyperparameters min_samples_leaf = 3 and min_samples_split = 25. Also shown here the line plot between predicted and actual value of the sale price. It can be seen that our model almost capable to capture all of the properties sale price, although the cross validation score reveals that our model is quietly overfitting the data.","773b1dd7":"## Importing Dataset","244d30db":"For Extra Trees Regressor, our best RMSLE score in cross validation scheme is 0.1332 (5.5% better than Ridge regression) with hyperparameters min_samples_split = 3 and max_depth = 40. Also shown here the line plot between predicted and actual value of the sale price. It can be seen that our model almost capable to capture all of the properties sale price. Unfortunately, the cross validation score reveals that our model is overfitting the data.","69d9b2d1":"### Sale price dependence on discrete numerical features","d8ac7395":"Now for the GarageYrBlt. Because this column is actually a numerical feature and has NA values (which means no garage), the NA values can't be filled with 'None'. They must be filled with numerical values too. To deal with this, we recall that GarageYrBlt has high correlation with YearBuilt (0.83), meaning that the house and its garage were likely built in the same year. Hence, we will fill the NA values with the YearBuilt values, even though the house actually has no garage.","ab3e0b6a":"Three properties have NA values in BsmtCond (which means they have no basement) but other features show that they have basement. \n\nStrategy &#8594; fill with the most frequent value.","7f63a4fd":"## Data Preparation","8b8141b5":"One house has NA value in BsmtFinType2 (which means it has no basement) but other features show that it has a basement.","1b5b0aca":"###  Predictive model: linear regression with regularization","7a25c398":"First, we will examine the relation between the property sale price with all of the nominal features by using boxplot. The purpose of this plotting routine is to find several variables which might affect the sale price significantly.","86007756":"### Contradictive information about garage","a7ee4f71":"Many of us are more familiar with R<sup>2<\/sup> score in regression problems rather than mean squared error function. Hence, we will provide R<sup>2<\/sup> score for each model we have built.","747ebe2f":"## Implementing to the Test Data","56e03d33":"Several features which have high importance score are in accordance to the previous results using SelectKBest and ExtraTreesRegressor feature importance. We will take 100 features with high importance score for our selected features.","a71accb6":"###  Predictive model: extremely randomized tree (Extra Trees) regressor","b67106d0":"### How about R<sup>2<\/sup>  score?","9e869cfb":"## Handling Missing Values"}}