{"cell_type":{"e9f2ed56":"code","11295784":"code","a4a4b053":"code","6040e647":"code","f8399bab":"code","1d161556":"code","206a2e1a":"code","1320dc88":"code","2d18ef61":"code","9592c60f":"code","8ddc414a":"code","bebdb54e":"code","d40fbe20":"code","03963c48":"code","b316b345":"code","c856c58d":"code","b0571986":"code","e1aba486":"code","b0c5d059":"code","ecd5dcab":"code","2676c322":"code","a941013e":"code","62a024ea":"code","93cd4ffd":"code","b69d53e3":"code","f4de5971":"code","71aebdf1":"code","6226ef37":"code","d219f53f":"code","71f257c0":"code","7bce89a1":"code","44803a49":"code","cc287e62":"code","5dc69dac":"code","29804a4d":"code","cfbb49d1":"code","d68fa600":"code","77c4e8d5":"code","d471a2e7":"code","963f0476":"code","1cc0eab4":"code","be02bc0e":"code","c930fbef":"code","1c3e42cd":"code","6eea1b9d":"code","2df75684":"code","af7b8d13":"code","93e38c69":"code","e4d97f78":"code","2c22a475":"code","7378dba3":"code","9500ddb9":"code","54440bb1":"code","d057adfe":"code","04389207":"code","8a14de8d":"code","db2806e4":"code","94ea6f02":"code","012bcafc":"code","94f8b051":"code","6b4c1c5a":"code","9606321a":"code","9d5e9a61":"code","44832d09":"code","97d4d975":"code","e401ef69":"code","c2f5a48f":"code","f953a4f9":"code","5fd2ac9c":"code","010cded9":"markdown","d4458718":"markdown","2d1d36ba":"markdown","45f412b5":"markdown","e4cf8a2c":"markdown","dbbf6e75":"markdown"},"source":{"e9f2ed56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","11295784":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import KNNImputer\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import roc_auc_score, roc_curve, log_loss, classification_report, confusion_matrix\nfrom lightgbm import LGBMClassifier\nfrom imblearn.over_sampling import SMOTE","a4a4b053":"train = pd.read_csv(\"\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv\")","6040e647":"def check_df(dataframe):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### NA #####################\")\n    print(pd.DataFrame({\"NA_COUNT\":dataframe.isnull().sum(),\n                        \"NA_RATIO\":dataframe.isnull().sum() \/ len(dataframe)}))\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)","f8399bab":"check_df(train)","1d161556":"check_df(test)","206a2e1a":"# I created a test_df from train set for validation.\n# I dropped the \"target\" values from test_X and I saved true target values as \"results\" dataframe.\ntest_df = train.iloc[:1000, :]\nresults = test_df[[\"enrollee_id\", \"target\"]]\n\n\ntest_X = test_df.drop (\"target\", axis=1)\ntest_X","1320dc88":"# I merged test_X, train and test datasets for feature transformation.\ndf = pd.concat ([test_X, train.iloc[1000:, :], test], axis=0).reset_index (drop=True)\ncheck_df (df)","2d18ef61":"# I noticed that there is an imbalanced dataset problem.\ndf[\"target\"].value_counts() \/ len(df)","9592c60f":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    # Grabs the columns which is categorical, numerical, categorical but cardinal and numerical but categorical.\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique () < cat_th and\n                   dataframe[col].dtypes != \"O\" or (\"id\" in col)]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique () > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print (f\"Observations: {dataframe.shape[0]}\")\n    print (f\"Variables: {dataframe.shape[1]}\")\n    print (f'cat_cols: {len (cat_cols)}')\n    print (f'num_cols: {len (num_cols)}')\n    print (f'cat_but_car: {len (cat_but_car)}')\n    print (f'num_but_cat: {len (num_but_cat)}')\n\n    return cat_cols, cat_but_car, num_cols, num_but_cat","8ddc414a":"categorical_cols, categorical_but_cardinal, numeric_cols, numeric_but_categorical = grab_col_names(df)","bebdb54e":"print(\"Categorical columns : {}\".format(categorical_cols))\nprint(\"=\"*80)\nprint(\"Cat_But_Car columns : {}\".format(categorical_but_cardinal))\nprint(\"=\"*80)\nprint(\"Numeric columns : {}\".format(numeric_cols))\nprint(\"=\"*80)\nprint(\"Num_But_Cat columns : {}\".format(numeric_but_categorical))\n","d40fbe20":"def cat_summary(dataframe, col_name, plot=False):\n    # Shows summary of categorical columns.\n    df = pd.DataFrame ({col_name: dataframe[col_name].value_counts (),\n                          \"Ratio\": 100 * dataframe[col_name].value_counts () \/ len (dataframe)})\n    print(df)\n\n    if plot:\n        plt.figure(figsize=(7,7))\n        plt.pie (df[\"Ratio\"], labels=df.index, \n                labeldistance=1.15, wedgeprops = { 'linewidth' : 1, 'edgecolor' : 'white' }, autopct = \"%1.1f%%\",\n                pctdistance=0.85, textprops={'fontsize': 10})\n     \n        #draw circle\n        centre_circle = plt.Circle((0,0),0.70,fc='white')\n        fig = plt.gcf()\n        fig.gca().add_artist(centre_circle)\n        plt.show();\n        print(\"=\"*50)","03963c48":"# Number of unique class of each feature.\ndf.nunique()","b316b345":"graph_cols = [col for col in categorical_cols if df[col].nunique() < 40]\nfor col in graph_cols:\n    cat_summary (df, col, plot=True)","c856c58d":"def target_summary_with_cat(dataframe, target, categorical_col, plot=False):\n    # Shows some of descriptive statistical metrics of target according to each categorical class.\n    df = pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean(),\n                        \"TARGET_MEDIAN\": dataframe.groupby(categorical_col)[target].median(),\n                        \"COUNT\": dataframe.groupby(categorical_col)[target].count()})\n    print(df)\n    if plot==True:\n        sns.barplot(x=df.index, y=df[\"TARGET_MEAN\"])\n        plt.xticks(rotation=45)\n        plt.xlabel(df.index.name.upper())\n        plt.show();\n        print(\"=\"*50)","b0571986":"cats = [col for col in df.columns if (col in categorical_cols + categorical_but_cardinal) & (col not in [\"enrollee_id\",\"target\"]) & (df[col].nunique() < 20)]\nfor col in cats:\n    target_summary_with_cat(df, \"target\", col, plot=True)","e1aba486":"def target_summary_with_num(dataframe, target, numerical_col):\n    # Shows average of target according to numerical columns.\n    df = dataframe.groupby (target).agg ({numerical_col: \"mean\"})\n    print(df)\n    print(\"=\"*50)\n","b0c5d059":"for col in numeric_cols:\n    target_summary_with_num (df, \"target\", col)","ecd5dcab":"# Examining the missing values.\nmissing_cols = [col for col in df.columns if (df[col].isnull().any()) & (col != \"target\")]\nmsno.matrix(df[missing_cols]);","2676c322":"msno.heatmap(df[missing_cols]);","a941013e":"\ndef data_prep(dataframe):\n    # Labeling \"relevent_experience\" feature.\n    dataframe.loc[dataframe[\"relevent_experience\"] == \"Has relevent experience\", \"NEW_IS_RELEVANT_EXP\"] = 1\n    dataframe.loc[dataframe[\"relevent_experience\"] == \"No relevent experience\", \"NEW_IS_RELEVANT_EXP\"] = 0\n\n    # Labeling \"gender\" feature.\n    dataframe.loc[dataframe[\"gender\"] == \"Male\", \"NEW_GENDER\"] = 0\n    dataframe.loc[dataframe[\"gender\"] == \"Female\", \"NEW_GENDER\"] = 1\n    dataframe.loc[dataframe[\"gender\"] == \"Other\", \"NEW_GENDER\"] = 2\n\n    # Assigning the ones greater than 20 as 20, and the ones less than 1 as 0 and converting them to float.\n    dataframe.loc[dataframe[\"experience\"] == \">20\", \"experience\"] = 20\n    dataframe.loc[dataframe[\"experience\"] == \"<1\", \"experience\"] = 0\n    dataframe[\"NEW_EXPERIENCE\"] = dataframe[\"experience\"].astype (float)\n\n    # Collecting the \"last_new_job\" under three groups.\n    dataframe.loc[dataframe[\"last_new_job\"].isin ([\"1\", \"2\", \"3\", \"4\"]), \"NEW_LAST_NEW_JOB\"] = \"1-4 years\"\n    dataframe.loc[dataframe[\"last_new_job\"] == \">4\", \"NEW_LAST_NEW_JOB\"] = \"5+ years\"\n    dataframe.loc[dataframe[\"last_new_job\"] == \"never\", \"NEW_LAST_NEW_JOB\"] = \"never\"\n\n    # Dividing \"major_discipline\" into two groups as being STEM or not.\n    Exc_STEM = [col for col in dataframe[\"major_discipline\"].unique () if col not in [\"STEM\", np.nan]]\n    dataframe.loc[dataframe[\"major_discipline\"].isin (Exc_STEM), \"NEW_MAJOR_is_STEM\"] = 0\n    dataframe.loc[(dataframe[\"major_discipline\"] == \"STEM\"), \"NEW_MAJOR_is_STEM\"] = 1\n\n    new_df = dataframe.copy()\n\n    del_cols = [\"gender\", \"relevent_experience\", \"experience\", \"last_new_job\", \"major_discipline\"]\n\n    new_df.drop (del_cols, axis=1, inplace=True)\n\n    return new_df","62a024ea":"df_2 = data_prep (df)\ncheck_df(df_2)","93cd4ffd":"multiclass_cat_cols = [col for col in df_2.columns if (df_2[col].nunique () > 2) & (df_2[col].dtype == \"O\")]\nmulticlass_cat_cols","b69d53e3":"# Using Label Encoding for all multiclass categorical columns before filling the missing values.\ndf_2[multiclass_cat_cols] = df_2[multiclass_cat_cols].apply (lambda series: pd.Series (\n    LabelEncoder ().fit_transform (series[series.notnull ()]),\n    index=series[series.notnull ()].index))","f4de5971":"df_2.head()","71aebdf1":"def knn_imputer_test(data, n_neighbors_list, model_name):\n    # Returns n_neighbor value which has the best roc_auc score for given n_neigbor lists.\n    best_roc_auc = 0\n    best_n_neighbor = 0\n\n    for n in n_neighbors_list:\n        dataframe = data.copy ()\n        imputed_cols = [col for col in dataframe.columns if\n                        (col not in [\"target\", \"enrollee_id\"]) & (dataframe[col].isnull ().any ())]\n        knn_imputer = KNNImputer (n_neighbors=n)\n        dataframe[imputed_cols] = np.round (knn_imputer.fit_transform (dataframe[imputed_cols]))\n\n        train_data = dataframe[dataframe.notnull ().all (axis=1)]\n\n        # MODEL\n        X = train_data.drop ([\"enrollee_id\", \"target\"], axis=1)\n        y = train_data['target']\n\n        # Data upscaling\n        smote = SMOTE ()\n        X, y = smote.fit_resample (X, y)\n\n        X_train, X_test, y_train, y_test = train_test_split (X, y, random_state=17, test_size=0.2)\n\n        model = model_name\n        fit_model = model.fit (X_train, y_train)\n        y_probs = fit_model.predict_proba (X_test)\n        y_probs = y_probs[:, 1]\n\n        roc_auc = roc_auc_score (y_test, y_probs)\n\n        print (\"n_neighbors : {}   \/\/   roc_auc_score : {}\".format (n, roc_auc))\n        if roc_auc > best_roc_auc:\n            best_roc_auc = roc_auc\n            best_n_neighbor = n\n        else:\n            continue\n    return best_roc_auc, best_n_neighbor","6226ef37":"new_df = df_2.copy()","d219f53f":"lgbm_roc_auc, lgbm_n_neighbor = knn_imputer_test (new_df, range (2, 11), LGBMClassifier (random_state=17))","71f257c0":"print(\"LGBM best roc_auc : {}  \/\/ LGBM n_neighbor : {}\".format(lgbm_roc_auc, lgbm_n_neighbor))","7bce89a1":"# Columns to be filled.\nimputed_cols = [col for col in new_df.columns if \n                (col not in [\"target\", \"enrollee_id\"]) & (new_df[col].isnull().any())]\nimputed_cols","44803a49":"knn_imputer = KNNImputer (n_neighbors=lgbm_n_neighbor)\nnew_df[imputed_cols] = np.round(knn_imputer.fit_transform (new_df[imputed_cols]))","cc287e62":"prep_df = new_df.copy ()\ncheck_df(prep_df)","5dc69dac":"# Dividing dataset into three parts as test_data, subb_data and train_data.\ntest_data = prep_df.loc[prep_df[\"enrollee_id\"].isin (results[\"enrollee_id\"])].reset_index(drop=True)\nsubb_data = prep_df.loc[prep_df[\"enrollee_id\"].isin (test[\"enrollee_id\"])].reset_index(drop=True)\ntrain_data = prep_df[prep_df.notnull ().all (axis=1)].reset_index(drop=True)","29804a4d":"subb_data.head()","cfbb49d1":"test.head()","d68fa600":"# Using train_data for setting up lgbm model.\nX = train_data.drop ([\"enrollee_id\", \"target\"], axis=1)\ny = train_data['target']","77c4e8d5":"# Imbalanced dataset.\ny.value_counts () \/ len(y)","d471a2e7":"def plot_learning_curve(model, X, Y):\n    # Plots logistic loss values for train and test sets.\n    x_train, x_test, y_train, y_test = train_test_split (X, Y, test_size=0.2, random_state=17)\n    train_loss, test_loss = [], []\n\n    for m in range (200, len (x_train), 200):\n        model.fit (x_train.iloc[:m, :], y_train[:m])\n        y_train_prob_pred = model.predict_proba (x_train.iloc[:m, :])\n        train_loss.append (log_loss (y_train[:m], y_train_prob_pred))\n\n        y_test_prob_pred = model.predict_proba (x_test)\n        test_loss.append (log_loss (y_test, y_test_prob_pred))\n\n    plt.figure (figsize=(15, 8))\n    plt.plot (train_loss, 'r-+', label='Training Loss')\n    plt.plot (test_loss, 'b-', label='Test Loss')\n    plt.xlabel ('Number Of Batches')\n    plt.ylabel ('Log-Loss')\n    plt.legend (loc='best')\n\n    plt.show ()","963f0476":"plot_learning_curve(LGBMClassifier(random_state=17), X, y)","1cc0eab4":"# Data Upscaling for decreasing log_loss values\nsmote = SMOTE (random_state=17)\nX_smote, y_smote = smote.fit_resample (X, y)","be02bc0e":"# SMOTE 2\nX_smote1, y_smote1 = smote.fit_resample(X,y)","c930fbef":"# SMOTE 3\nX_smote2, y_smote2 = smote.fit_resample(X,y)","1c3e42cd":"X_final = pd.concat([X_smote, X_smote1, X_smote2]).reset_index(drop=True)\ny_final = pd.concat([y_smote, y_smote1, y_smote2]).reset_index(drop=True)","6eea1b9d":"y_final.value_counts()","2df75684":"plot_learning_curve(LGBMClassifier(random_state=17), X_final, y_final)","af7b8d13":"X_train, X_val, y_train, y_val = train_test_split (X_final, y_final, random_state=17, test_size=0.2)","93e38c69":"lgbm = LGBMClassifier (random_state=17)\nlgbm_model = lgbm.fit (X_train, y_train)","e4d97f78":"# Train Set Score\ntrain_preds = lgbm_model.predict (X_train)\ntrain_probs = lgbm_model.predict_proba (X_train)\ntrain_probs = train_probs[:, 1]\nprint (classification_report (y_train, train_preds))\nprint (\"Roc_Auc Score : {}\".format(roc_auc_score (y_train, train_probs)))","2c22a475":"# Validation Set Score\ny_pred = lgbm_model.predict (X_val)\nprobs_model = lgbm_model.predict_proba (X_val)\nprobs_model = probs_model[:, 1]\n\nprint (classification_report (y_val, y_pred))\nprint (\"Roc_Auc Score : {}\".format(roc_auc_score (y_val, probs_model)))","7378dba3":"# MODEL TUNING\nlgbm_params = {\"num_leaves\": [20, 50],\n               \"max_depth\": [5, 8],\n               \"learning_rate\": [0.005, 0.01, 0.02],\n               \"n_estimators\": [100, 500, 1000]}\n\nlgbm_cv = GridSearchCV (lgbm, lgbm_params, cv=5, n_jobs=-1, verbose=2).fit (X_train, y_train)\n\nlgbm_cv.best_params_","9500ddb9":"# FINAL MODEL\nlgbm_tuned = LGBMClassifier (random_state=17, **lgbm_cv.best_params_).fit (X_train, y_train)\ny_tuned = lgbm_tuned.predict (X_val)\nprobs_final = lgbm_tuned.predict_proba (X_val)\nprobs_final = probs_final[:, 1]","54440bb1":"# Validation Set Score with Final Model\nprint (classification_report (y_val, y_tuned))\nprint (\"Roc_Auc Score : {}\".format(roc_auc_score (y_val, probs_final)))","d057adfe":"def roc_auc_plot(model_name, testX, ytrue):\n    probs = model_name.predict_proba(testX)\n    probs = probs [:,1]\n    fpr, tpr, threshold = roc_curve (ytrue, probs)\n    roc_auc = roc_auc_score (ytrue, probs)\n\n    plt.title ('Receiver Operating Characteristic')\n    plt.plot (fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n    plt.legend (loc='lower right')\n    plt.plot ([0, 1], [0, 1], 'r--')\n    plt.xlim ([0, 1])\n    plt.ylim ([0, 1])\n    plt.ylabel ('True Positive Rate')\n    plt.xlabel ('False Positive Rate')\n    plt.show ();\n\n    return roc_auc\n","04389207":"roc_auc_plot(lgbm_tuned, X_val, y_val)","8a14de8d":"# Using tuned model into test_data which extracting from train set at the beginning.\ntest_data","db2806e4":"results","94ea6f02":"test_labels = results[\"target\"]","012bcafc":"test_data_X = test_data.drop ([\"enrollee_id\", \"target\"], axis=1)\n\nlgbm_final_pred = lgbm_tuned.predict (test_data_X)\nlgbm_final_probs = lgbm_tuned.predict_proba (test_data_X)\nlgbm_final_probs = lgbm_final_probs[:, 1]\n","94f8b051":"print (classification_report (test_labels, lgbm_final_pred))\nprint (\"Roc_Auc Score : {}\".format(roc_auc_score (test_labels, lgbm_final_probs)))","6b4c1c5a":"def con_matrix(true_labels, pred_labels):\n    # Plots confusion matrix as a heatmap.\n    matrix = confusion_matrix (true_labels, pred_labels)\n    sns.heatmap (matrix, annot=True, fmt=\"d\")\n    plt.xlabel (\"Predicted\")\n    plt.ylabel (\"Actual\")\n    plt.show ();","9606321a":"roc_auc_plot (lgbm_tuned, test_data_X, test_labels)","9d5e9a61":"con_matrix (test_labels, lgbm_final_pred)","44832d09":"subb_data.head()","97d4d975":"subb_X = subb_data.drop([\"enrollee_id\",\"target\"], axis=1)","e401ef69":"prediction = lgbm_tuned.predict_proba(subb_X)\nprediction[:5]","c2f5a48f":"predict = prediction[:,1]\npredict[:5]","f953a4f9":"submission_df = pd.DataFrame({\"enrollee_id\":subb_data[\"enrollee_id\"],\n             \"target\":predict})\n\nsubmission_df.head()","5fd2ac9c":"submission_df.to_csv('submission.csv',index=False)","010cded9":"# **BUSINESS PROBLEM**\n\nA company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which conduct by the company. Many people signup for their training. Company wants to know which of these candidates are really wants to work for the company after training or looking for a new employment because it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates. Information related to demographics, education, experience are in hands from candidates signup and enrollment data.\n\nThe whole data divided to \"train\" and \"test\". Target isn't included in test.\n\n# **PROJECT GOAL**\n\n* Prediction of the probability of a candidate will work for the company.","d4458718":"# **SUBMISSION**","2d1d36ba":"# **FILLING THE MISSING VALUES USING KNN IMPUTER**","45f412b5":"# **APPROACH**\n\n* First step is to divide the training dataset into two parts as \"test_X\" and \"train\" for validation as below. \n\n* Then to merge two parts of datasets which I created \"train\" and \"test_X\" with \"test\" set that already exist.\n\n* To make feature transformation.\n\n* To fill the missing values using KNN imputer.\n\n* To use Light GBM algorithm for prediction.\n\n* To use the metrics which called log-loss to prevent overfitting and roc-auc score for correct classification. \n\n* And finally, to predict the probability of working of a candidate on the test set which I will call \"submission_df\". ","e4cf8a2c":"# **FEATURES**\n\n*** enrollee_id :** Unique ID for candidate\n\n*** city:** City code\n\n*** city_ development _index :** Development index of the city (scaled)\n\n*** gender:** Gender of candidate\n\n*** relevent_experience:** Relevant experience of candidate\n\n*** enrolled_university:** Type of University course enrolled if any\n\n*** education_level:** Education level of candidate\n\n*** major_discipline:** Education major discipline of candidate\n\n*** experience:** Candidate total experience in years\n\n*** company_size:** Number of employees in current employer's company\n\n*** company_type:** Type of current employer\n\n*** lastnewjob:** Difference in years between previous job and current job\n\n*** training_hours:** Training hours completed\n\n*** target:** 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change","dbbf6e75":"# **LGBM MODEL**"}}