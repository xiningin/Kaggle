{"cell_type":{"92b9f5d7":"code","96632944":"code","c488c071":"code","9c6a7d5d":"code","3940de70":"code","e2dadeea":"code","7be691d7":"code","11bce3c3":"code","ac1fdb65":"code","3d253d9e":"code","741227ec":"code","b94fdd22":"code","5ab1b404":"code","ec9671d9":"code","758afa02":"code","c918abf3":"code","d7c5a1b2":"code","100a8725":"code","a39ba1f3":"code","599213f5":"code","d343a49d":"code","5d7e4d1f":"code","c83a7475":"code","d5eaa06e":"code","dbc576a1":"code","17cf095d":"code","d11e7187":"code","15c4094d":"code","e4ca94cb":"code","e1384224":"markdown","e9f0933d":"markdown","a55695a2":"markdown","ca071d45":"markdown","9a22ceec":"markdown","a98175bc":"markdown","26540eb1":"markdown","03f6b013":"markdown","61f32f65":"markdown","d6ad6e9a":"markdown","fe8bfe6f":"markdown","bd6ebc22":"markdown","3858b4ce":"markdown","0e3af289":"markdown","9262a147":"markdown"},"source":{"92b9f5d7":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns","96632944":"import spacy","c488c071":"ep_desc = pd.read_csv('\/kaggle\/input\/chai-time-data-science\/Description.csv')","9c6a7d5d":"ep_desc.head()","3940de70":"ep1_sub = pd.read_csv('\/kaggle\/input\/chai-time-data-science\/Cleaned Subtitles\/E1.csv')","e2dadeea":"ep1_sub.head(10)","7be691d7":"nlp = spacy.load('en_core_web_lg')","11bce3c3":"ep1_sub['Text'][0]","ac1fdb65":"doc = nlp(u'Hey, this is Sanyam Bhutani and you\\'re listening to \"Chai Time Data Science\", a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.\\n\\nSanyam Bhutani  0:46  \\nHello, and welcome to the first episode. This is also part 26 of interview with machine learning heroes. In this episode, I\\'m honored to be joined by Abhishek Thakur, chief data scientist at boost.ai, and also the world\\'s first, and at the time of recording, only triple Grand Master on Kaggle. Abhishek has been working as a data scientist for the past few years. He also has a background in computer science with a master\\'s degree from the University of Bonn. We talked about his journey into data science, his Kaggle experience and his current projects. Enjoy the show.\\n\\nSanyam Bhutani  1:38  \\nHi, everyone. Thanks for tuning in. I\\'m really honored to be talking to the world\\'s first triple Grand Master today. Thank you so much for taking the time to do this interview Abhishek.')","3d253d9e":"for token in doc:\n    print(token)","741227ec":"doc1 = nlp(u'Hey, this is Sanyam Bhutani and you\\'re listening to \"Chai Time Data Science\", a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.\\n\\nSanyam Bhutani  0:46  \\nHello, and welcome to the first episode.')\n\nfor token in doc1:\n    print(token.text, end=' | ')\n\nprint('\\n----')\n\nfor ent in doc1.ents:\n    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))","b94fdd22":"doc2 = nlp(u'Hey, this is Sanyam Bhutani and you\\'re listening to \"Chai Time Data Science\", a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.\\n\\nSanyam Bhutani  0:46  \\nHello, and welcome to the first episode.')\n\nfor chunk in doc2.noun_chunks:\n    print(chunk.text)","5ab1b404":"doc1 = nlp(u'Hey, this is Sanyam Bhutani and you\\'re listening to \"Chai Time Data Science\", a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.\\n\\nSanyam Bhutani  0:46  \\nHello, and welcome to the first episode.')\n\nfor token in doc1:\n    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)","ec9671d9":"doc1 = nlp(u'Hey, this is Sanyam Bhutani and you\\'re listening to \"Chai Time Data Science\", a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.\\n\\nSanyam Bhutani  0:46  \\nHello, and welcome to the first episode.')","758afa02":"doc1.vector","c918abf3":"doc2 = nlp(u'Enjoy the show.\\n\\nSanyam Bhutani  1:38  \\nHi, everyone. Thanks for tuning in. I\\'m really honored to be talking to the world\\'s first triple Grand Master today. Thank you so much for taking the time to do this interview Abhishek.')","d7c5a1b2":"doc2.vector","100a8725":"#LEST SEE THAT HOW MUCH HOST AND GUEST WERE SAYING SIMILAR THINGS\nfor i in range(225):\n    \n    doc1 = nlp(ep1_sub['Text'][i])\n    doc2 = nlp(ep1_sub['Text'][i+1])\n    \n    print(doc1.similarity(doc2))\n    \n    \n","a39ba1f3":"import nltk\nnltk.download('vader_lexicon')","599213f5":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsid = SentimentIntensityAnalyzer()","d343a49d":"doc1 = ep1_sub['Text'][0]","5d7e4d1f":"sid.polarity_scores(doc1)\n#The starting of the podcast is very much neutral. ","c83a7475":"for i in ep1_sub['Text']:\n    a = sid.polarity_scores(i)\n    print(a)\n\n#These are the Sentiment Scores of every Text of Episode one     \n\n    ","d5eaa06e":"ep1_sub.head()","dbc576a1":"ep1_sub['scores'] = ep1_sub['Text'].apply(lambda review: sid.polarity_scores(review))\n\nep1_sub.head()","17cf095d":"ep1_sub['compound']  = ep1_sub['scores'].apply(lambda score_dict: score_dict['compound'])\n","d11e7187":"ep1_sub.head()","15c4094d":"ep1_sub['comp_score'] = ep1_sub['compound'].apply(lambda c: 'positive' if c >=0 else 'negative')\n\nep1_sub.head()","e4ca94cb":"sns.countplot(x = ep1_sub['comp_score'] )  \n\n#SO THERE ARE VERY FEW NEGATIVE STATEMENTS.","e1384224":"-------------------------------------------------------------------------------------------------------------\n-------------------------------------------------------------------------------------------------------------\n-------------------------------------------------------------------------------------------------------------\n   \n    ","e9f0933d":"--------------------------------------------------------------------------------------------------------------\n--------------------------------------------------------------------------------------------------------------\n--------------------------------------------------------------------------------------------------------------","a55695a2":"# TEXT SIMILARITY","ca071d45":"# NAMED ENTITY RECOGNITION ","9a22ceec":"Similar to Doc.ents, Doc.noun_chunks are another object property. Noun chunks are \"base noun phrases\" \u2013 flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun \u2013 for example, in Sheb Wooley's 1958 song, a *\"one-eyed, one-horned, flying, purple people-eater\"* would be one long noun chunk.","a98175bc":"Going a step beyond tokens, named entities add another layer of context. The language model recognizes that certain words are organizational names while others are locations, and still other combinations relate to money, dates, etc. Named entities are accessible through the ents property of a Doc object.","26540eb1":"# LEMMATIZATION","03f6b013":"--------------------------------------------------------------------------------------------------------\n--------------------------------------------------------------------------------------------------------\n--------------------------------------------------------------------------------------------------------","61f32f65":"# NOUN CHUNKS","d6ad6e9a":"THIS IS DONE ON FIRST SET OF TEXT OF EPISODE 1 ","fe8bfe6f":"# SENTIMENT ANALYSIS","bd6ebc22":"# TOKENIZATION","3858b4ce":"LIKEWISE WE CAN DO THIS FOR EACH EPISODE TO SEE HOW MUCH EACH EPISODE IS RELATED TO OTHER.","0e3af289":"# STAY TUNED FOR MORE DATA ANALYSIS. IF YOU LIKE MY WORK, PLEASE UPVOTE.\n# THANKS.","9262a147":"**Hello mates. In this notebook you will be finding all the Text Processing Techniques like Tokenization, Named Entity Recongnition, Lemmatization using Spacy Library.\nYou will also find Text Similarity Techniques with Sentiment Analysis using NLTK Library.\nIf you like my work, upvote it.**"}}