{"cell_type":{"c80bb23d":"code","f9f5a07c":"code","937ae025":"code","3c145b55":"code","431eba11":"code","562741d8":"code","7e0e02e9":"code","dbaf8c53":"code","c11d3bc3":"code","a4180f84":"code","49f253ec":"code","7847eb17":"code","f50abf21":"code","fa509eef":"code","c5728d3c":"code","bbd93f2e":"code","76565995":"code","76b4a849":"code","7c9381ae":"markdown","d84d1990":"markdown","0acb56b0":"markdown","a58ce0e0":"markdown","048fd866":"markdown"},"source":{"c80bb23d":"# import the libarires \nimport numpy as np  # numpy is used for vectorization and quick calculatios\nimport pandas as pd # data structure to work with dataframe\n","f9f5a07c":"# load the train dataset\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\n\n# check out the train dataset\ntrain.head() # head function gives us top 5 records in the dataframe ","937ae025":"train.tail() # tail function gives us last 5 records in the dataframe \n            #  you could modify any value - tail(2)  - for 2 records etc.","3c145b55":"# what is the shape of the dataset\ntrain.shape","431eba11":"# quickly check out the info\ntrain.info() # mostly to understand how many NaN values in which columns etc. ","562741d8":"# first 10 text\ntrain.text[:10]","7e0e02e9":"# check out the distribution of target value\n# this is very critical \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nsns.countplot(train.target)","dbaf8c53":"# Convert Text into list \ndata  = train.text.tolist()\ndata[:5]","c11d3bc3":"# lower the values\ndata = [str(sentence).lower() for sentence in data]\ndata[:1]","a4180f84":"import re\n\n# remove the https\/www or http\ndata = [re.sub(r\"https|www|http\\S+\", \"\", sent) for sent in data]\ndata[:50]","49f253ec":"# remove speacial characters\ndata = [re.sub('\\S*#|$|@|=|>|!|:\\S*\\s?', '', sent) for sent in data]\ndata[:50]","7847eb17":"# Substituting multiple spaces with single space\ndata = [re.sub(r'\\s+', ' ', sent) for sent in data]\ndata[:1]","f50abf21":"# remove the digit\ndata = [re.sub(r\"\\d\", \"\", sent) for sent in data]\ndata = [re.sub(r'\\^[a-zA-Z0-9]\\s]', \"\", sent) for sent in data]\ndata[:5]","fa509eef":"train['clean_data'] = pd.DataFrame(data)","c5728d3c":"train.info()","bbd93f2e":"# importing all necessery modules \nfrom wordcloud import WordCloud, STOPWORDS ","76565995":"# code refereed from internet \ncomment_words = ' '\nstopwords = set(STOPWORDS) \n\nfor data in train.clean_data: \n\n    # split the value \n    tokens = data.split() \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n          \n    for words in tokens: \n        comment_words = comment_words + words + ' '\n  \n\nwordcloud = WordCloud(width = 500, height = 500, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 8).generate(comment_words) \n                  \nplt.figure(figsize = (10, 10)) \nplt.imshow(wordcloud) \nplt.show() ","76b4a849":"# Vectoriztaion of the data\nX  = train.clean_data \ny =  train.target","7c9381ae":"We have lots of special characters, digits and hyper links. \n\n    Ex: \\x89\u00e3\u00a2 (2010) Full\\x89\u00e3\u00a2 Streaming - YouTube http:\/\/t.co\/vVE3UsesGf', \n\n    Let us remove these by using Regex","d84d1990":"### Data Label Distribution \nIt is very critical to understand if we  have a balanaced dataset or imbalanced dataset. \nEasiest way is to plot the graph. ","0acb56b0":"So now, we know that there are few NaN values in keywords and location which could be intresting feature to decide the if tweet was related to diaster .\nSo let us not delete NaN values.\n\nWe will just check text and targets ","a58ce0e0":"Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\n\nThis particular competition will help in understanding the NLP sentiment analysis steps. If you are a learner, this is the best competition to start with. \n\n## Steps to be followed while doing Any sentiment Analysis\n\n### 1. Collect the data \n    The cleaner the data - better the data\n    \n### 2. Pre- process the data\n    Use regex to do the basic level of cleaning \n    \n### 3. Work with advance text processing\n    a. Remove the stop words\n    b. Lemmeitization \n    c. Make bigrams or trigrams\n    d. Use Pos_Tags for getting the words < do you want to use noun, adjectives or adverbs>\n    e. Combine the data to a dataframe\n    \n### 4.  Vectorization - Convert text into numbers\n    a. Count Vectorizer \n    b. TFIDF Vectorizer\n    \n### 5. Train test split \n### 6. Base line model - Naive Bayes\n### 7. Evalute the model \n\n### 8. Experiement with more models\n\n### 9. Switch to TFIDF or CountVectorizer","048fd866":"Data is quite balance so we dont need to work on imbalanced approach. \n\n### Let us clean the data and create a word cloud"}}