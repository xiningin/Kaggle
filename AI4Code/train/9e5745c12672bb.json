{"cell_type":{"920ef9f9":"code","530925bb":"code","2f4ad5bc":"code","27ae073a":"code","85bb9404":"code","0aaab0a9":"code","f8fa056f":"code","21a10362":"code","2e07d471":"code","dd1a8b48":"code","2f8d96b3":"code","5eb577f8":"code","596c67c7":"code","ef9851d0":"markdown","0ef6c008":"markdown","55aa77d0":"markdown","40c7cf72":"markdown","8db2c706":"markdown","4ea33a43":"markdown","6f751edc":"markdown","54e3f109":"markdown"},"source":{"920ef9f9":"!pip install rich --quiet","530925bb":"from pathlib import Path\n\nimport numpy as np\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom rich.progress import Progress\nfrom torch import nn, optim\nfrom torchvision import models, transforms","2f4ad5bc":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","27ae073a":"content_pics_dir = Path('..\/input\/tamil-nst\/TamilContentImages')\nstyle_pics_dir = Path('..\/input\/tamil-nst\/TamilStyleImages')","85bb9404":"img_size = 512, 512\ncontent_image = Image.open(content_pics_dir \/ 'C_image7.jpg').resize(img_size)\ncontent_image","0aaab0a9":"style_image = Image.open(style_pics_dir \/ 'S_image4.jpg').resize(content_image.size)\nstyle_image","f8fa056f":"def compute_content_cost(content_output, generated_output):\n    # shape = (n_c, n_h, n_w)\n    if content_output.shape != generated_output.shape:\n        raise Exception(\"content_output and generated_output have different shapes\")\n    n_c, n_h, n_w = content_output.shape\n    cft = 1 \/ (4 * n_h * n_w * n_c)\n    return cft * ((content_output - generated_output) ** 2).sum()","21a10362":"def compute_layer_style_cost(style_output, generated_output):\n    # shape = (n_c, n_h, n_w)\n    if style_output.shape != generated_output.shape:\n        raise Exception(\"style_output and generated_output have different shapes\")\n    n_c, n_h, n_w = style_output.shape\n    style_unrolled = style_output.view(n_c, -1)\n    generated_unrolled = generated_output.view(n_c, -1)\n    # compute gram matrix\n    style_gram = style_unrolled.matmul(style_unrolled.transpose(0, 1))\n    generated_gram = generated_unrolled.matmul(generated_unrolled.transpose(0, 1))\n    cft = 1 \/ (2 * n_c * n_h * n_w) ** 2\n    return cft * ((style_gram - generated_gram) ** 2).sum()","2e07d471":"class NSTCost:\n    def __init__(\n        self,\n        content_img,\n        style_img,\n        model,\n        layers,\n        content_layer_idx,\n        style_layers_idx,\n        style_layer_weights,\n        alpha,\n        beta,\n        optimizer,\n        optimizer_kwargs,\n        device,\n    ):\n        self.device = device\n        self.content_img = content_img\n        self.style_img = style_img\n        self.model = model\n        self.layers = layers\n        self.content_layer_idx = content_layer_idx\n        self.content_layer_activation = None\n        self.style_layers_idx = style_layers_idx\n        self.style_layers_activation = {i: None for i in style_layers_idx}\n        self.style_layer_weights = style_layer_weights\n        self.alpha = alpha\n        self.beta = beta\n        self.register_forward_hooks()\n        self.normalizer = transforms.Normalize(\n            (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n        )\n        self.denormalizer = transforms.Compose(\n            [\n                transforms.Normalize(\n                    mean=[0.0, 0.0, 0.0], std=[1 \/ 0.229, 1 \/ 0.224, 1 \/ 0.225]\n                ),\n                transforms.Normalize(\n                    mean=[-0.485, -0.456, -0.406], std=[1.0, 1.0, 1.0]\n                ),\n            ]\n        )\n        self.to_tensor = transforms.ToTensor()\n        self.style_resizer = transforms.Resize(content_image.size)\n        self.style_outputs = self.get_style_outputs(style_img)\n        self.content_output = self.get_content_output(content_img)\n        self.generated_tensor = self.get_generated_tensor()\n        self.optimizer = optimizer((self.generated_tensor,), **optimizer_kwargs)\n\n    def get_generated_tensor(self):\n        generated_tensor = (\n            self.get_content_tensor(self.content_img).clone().to(self.device)\n        )\n        # add noise\n        generated_tensor.add_(torch.rand_like(generated_tensor))\n        generated_tensor = generated_tensor.requires_grad_(True)\n        return generated_tensor\n\n    def get_style_tensor(self, style_img):\n        style_tensor = self.normalizer(\n            self.style_resizer(self.to_tensor(style_img))\n        ).to(self.device)\n        style_tensor.unsqueeze_(0)\n        return style_tensor\n\n    def get_style_outputs(self, style_img):\n        style_tensor = self.get_style_tensor(style_img)\n        self.model(style_tensor)\n        style_outputs = self.style_layers_activation.copy()\n        return style_outputs\n\n    def get_content_tensor(self, content_img):\n        content_tensor = self.normalizer(self.to_tensor(content_img)).to(device)\n        content_tensor.unsqueeze_(0)\n        return content_tensor\n\n    def get_content_output(self, content_img):\n        content_tensor = self.get_content_tensor(content_img)\n        self.model(content_tensor)\n        content_output = self.content_layer_activation\n        return content_output\n\n    def register_style_image(self, style_image):\n        self.model(style_image)\n\n    def content_forward_hook(self, module, input, output):\n        # print('content hook called')\n        self.content_layer_activation = output\n\n    def style_forward_hook(self, module, input, output):\n        # print('style hook called')\n        self.style_layers_activation[module.idx] = output\n\n    def register_forward_hooks(self):\n        idx = self.content_layer_idx\n        self.layers[idx].register_forward_hook(self.content_forward_hook)\n        for idx in self.style_layers_idx:\n            self.layers[idx].idx = idx  # monkey patch the idx attribute\n            self.layers[idx].register_forward_hook(self.style_forward_hook)\n\n    def gather_generated_outputs(self, generated_image):\n        outputs = {}\n        self.model(generated_image)\n        outputs[\"content\"] = self.content_layer_activation\n        outputs[\"style\"] = self.style_layers_activation.copy()\n        return outputs\n\n    def compute_loss(self):\n        generated_outputs = self.gather_generated_outputs(self.generated_tensor)\n        content_cost = compute_content_cost(\n            self.content_output.squeeze(),\n            generated_outputs[\"content\"].squeeze(),\n        )\n        total_style_cost = 0\n        for idx in self.style_outputs:\n            style_output = self.style_outputs[idx]\n            generated_output = generated_outputs[\"style\"][idx]\n            style_cost = compute_layer_style_cost(\n                style_output.squeeze(),\n                generated_output.squeeze(),\n            )\n            total_style_cost += style_cost * self.style_layer_weights[idx]\n        total_cost = self.alpha * content_cost + self.beta * total_style_cost\n        return total_cost\n\n    def get_generated_img(self):\n        tensor = self.denormalizer(self.generated_tensor.detach().cpu()) * 255\n        img_arr = tensor.numpy().squeeze().clip(0, 255)\n        img_arr = np.moveaxis(img_arr, 0, -1).astype(np.uint8)\n        return Image.fromarray(img_arr)\n\n    def fit(self, epochs):\n        with Progress() as progress:\n            task = progress.add_task(\"Painting...\", total=epochs)\n            for epoch in range(epochs):\n                loss = self.compute_loss()\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n                msg = \"{\" + f\"loss: {loss.item():.2e}\" + \"}\"\n                progress.update(task, description=msg, advance=1)\n        return self.get_generated_img()","dd1a8b48":"model = models.vgg19(pretrained=True).features.requires_grad_(False).eval().to(device)\nlayers = list(model.children())","2f8d96b3":"content_layer_idx = 35\nstyle_layers_idx = [1, 6, 11, 20, 29]\nstyle_layer_weights = {\n    1: 0.2,\n    6: 0.2,\n    11: 0.2,\n    20: 0.2,\n    29: 0.2,\n}","5eb577f8":"alpha = 1\nbeta = 2\nnst_cost = NSTCost(\n    content_image,\n    style_image,\n    model,\n    layers,\n    content_layer_idx,\n    style_layers_idx,\n    style_layer_weights,\n    alpha,\n    beta,\n    optim.Adam,\n    {\"lr\": 0.01},\n    device,\n)","596c67c7":"nst_cost.fit(1000)","ef9851d0":" Load Pretrained Model (VGG19)","0ef6c008":"$$J_{content}(C,G) =  \\frac{1}{4 \\times n_H \\times n_W \\times n_C}\\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2\\tag{1}\n$$","55aa77d0":"# Define Cost Function\n","40c7cf72":"$$J_{style}^{[l]}(S,G) = \\frac{1}{4 \\times {n_C}^2 \\times (n_H \\times n_W)^2} \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}(G^{(S)}_{(gram)i,j} - G^{(G)}_{(gram)i,j})^2\\tag{2} $$\nTotal style cost will be calculated as follow:\n$$J_{style}(S,G) = \\sum_{l} \\lambda^{[l]} J^{[l]}_{style}(S,G)$$","8db2c706":"# Loading Content & Style Images","4ea33a43":"## Content Cost","6f751edc":"# Import Modules","54e3f109":"# Style Cost"}}