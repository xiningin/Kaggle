{"cell_type":{"ac227250":"code","8b0b379a":"code","f82b4b14":"code","77c0368f":"code","e97109cb":"code","a546afce":"code","50bcf603":"code","3f4bb8fc":"code","63e1573e":"code","c93fdb91":"code","41e0895c":"code","53c0d670":"code","ff7a3533":"code","3f659b3c":"code","2613368f":"code","4f8fa5fe":"code","660f997c":"code","6a094671":"code","f17bfc2d":"code","f13f2e1e":"code","b4f62c03":"code","c6c3cc12":"code","f4270122":"code","69b6e17d":"code","b0d21896":"markdown","302bd6a0":"markdown","e5aad5d3":"markdown","d2cbfebc":"markdown","91132393":"markdown","9a7a124d":"markdown","0792c802":"markdown","2beba485":"markdown"},"source":{"ac227250":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8b0b379a":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import style\nstyle.use('fivethirtyeight')\nfrom scipy import stats","f82b4b14":"train = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\ntrain.head()\ncolumns = train.columns","77c0368f":"test1 = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')","e97109cb":"train.drop('Id',axis = 1, inplace = True)\ntest.drop('Id',axis = 1, inplace = True)\ntrain.head()","a546afce":"train.info()","50bcf603":"num_col = [col for col in train.columns if train[col].dtype != 'object']\ncat_col = [col for col in train.columns if train[col].dtype == 'object']","3f4bb8fc":"train['MasVnrArea'].fillna(train['MasVnrArea'].mean(), inplace = True)\ntest['MasVnrArea'].fillna(test['MasVnrArea'].mean(), inplace = True)","63e1573e":"train['GarageYrBlt'].fillna(train['YearBuilt'], inplace = True)\ntest['GarageYrBlt'].fillna(test['YearBuilt'], inplace = True)","c93fdb91":"train['LotFrontage'].fillna(train['LotFrontage'].mean(), inplace = True)\ntest['LotFrontage'].fillna(test['LotFrontage'].mean(), inplace = True)","41e0895c":"train[cat_col].isnull().sum().sort_values(ascending = False)","53c0d670":"for col in cat_col:\n    train[col].fillna('None', inplace = True)\n    test[col].fillna('None', inplace = True)","ff7a3533":"from sklearn.preprocessing import OneHotEncoder\n\nmyOneHot = OneHotEncoder(handle_unknown= 'ignore', sparse=False)\n\ntrain_X_OneHot = pd.DataFrame(myOneHot.fit_transform(train[cat_col]))\ntest_X_OneHot = pd.DataFrame(myOneHot.transform(test[cat_col]))\n\n# add the index back\ntrain_X_OneHot.index = train.index\ntest_X_OneHot.index = test.index\n\n#remove the object columns \ntrain.drop(cat_col, axis = 1, inplace = True)\ntest.drop(cat_col, axis = 1, inplace = True)\n\n#add the onehot columns to the train, valid and test\ntrain_hot = pd.concat([train, train_X_OneHot], axis = 1)\ntest_hot = pd.concat([test, test_X_OneHot], axis = 1)\n\n","3f659b3c":"\ny = train['SalePrice']\ntrain_hot.drop('SalePrice', axis = 1, inplace = True)","2613368f":"test_hot.shape, train_hot.shape","4f8fa5fe":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression,Ridge, Lasso\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib\n# %pylab inline","660f997c":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\n\nlin_reg = LinearRegression()\n\nscore = mean(sqrt(-cross_val_score(lin_reg, train_hot, y, scoring='neg_mean_squared_error', cv=10)))\nprint(score)\n","6a094671":"alphas = np.logspace(-5, 2, 20)\nscores = []\n\nfor i in alphas:\n    model_ridge = Ridge(alpha = i)\n    score = mean(sqrt(-cross_val_score(model_ridge, train_hot, y, scoring='neg_mean_squared_error', cv=10 )))\n    scores.append(score)\n    \ndf = pd.DataFrame(list(zip(alphas, scores)), columns = ['alphas', 'scores'])\nmin_score = df['scores'].idxmin()\ndf.iloc[min_score, :]    ","f17bfc2d":"alphas = np.logspace(-5, 2, 20)\nscores_lass = []\n\nfor i in alphas:\n    model_lass = Lasso(alpha = i)\n    score = mean(sqrt(-cross_val_score(model_ridge, train_hot, y, scoring='neg_mean_squared_error', cv=10 )))\n    scores_lass.append(score)\n    \nmin(scores_lass)","f13f2e1e":"nan_cols = [col for col in test_hot.columns if test_hot[col].isnull().any()]\nnan_cols","b4f62c03":"for col in nan_cols:\n    test_hot[col].fillna(0, inplace = True)","c6c3cc12":"model_final = Ridge(alpha = 18.329807)\nmodel_final.fit(train_hot, y)\nprediction = model_final.predict(test_hot)","f4270122":"test.head()","69b6e17d":"test_submission = pd.DataFrame({'Id' : test1['Id'],\n                                'SalePrice' : prediction})\ntest_submission.to_csv('test_submission1.csv')","b0d21896":"Baseline Run - Linear Regression, Ridge and Lasso","302bd6a0":"Handle missing data from numerical cols","e5aad5d3":"One Hot imputing","d2cbfebc":"Handeling nan values from catagorical data","91132393":"MasVnrArea does have a corelation of 0.5 with the Sales price of the house, so will not be dropping them. I will be filling the nan values with mean of MasVnrArea.","9a7a124d":"GarageYrBlt - logically the year of this should be similar to the YearBuilt  the house was built, lets see if this assumption is true..\nAfter doing some detailed analysis, figured that these properties actually do not have any garage, since the garage area and garage cars = 0. so there are two ways we can handel this,\npopulate the nan by 9999 or populate the values with year built.. \n\nI will populate it by year built","0792c802":"Lets take a look at LotFrontage","2beba485":"Since the Ridge Regression has the minimum score, we will pick it."}}