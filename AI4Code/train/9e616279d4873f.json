{"cell_type":{"3a055f47":"code","429ff434":"code","61d6d61b":"code","fbe6547b":"code","497e477e":"code","4617fc31":"code","f22bffa1":"code","5db732d4":"code","33f3039a":"code","706c7988":"code","1d59287e":"code","d93fe4e2":"code","4a5685e8":"code","9c393917":"code","07f2d7b2":"code","0b389ad3":"code","07768463":"code","286c216a":"code","b2fb47d2":"code","1d4601f6":"code","9e0bdf90":"code","8cac2bea":"code","c054f606":"code","3c1417bf":"code","4943370f":"code","73ced100":"code","431054c5":"code","0b177eb5":"code","7ed6f252":"code","cb36e924":"code","29bbc318":"code","4dbeebc3":"code","0e1b3fe4":"code","1c47993a":"code","331abb36":"code","90a1208b":"code","97c260de":"code","27e1193c":"code","733d5a82":"code","e0b55f78":"code","6c611b9c":"code","f86ddc84":"code","5a58be57":"code","8c519cd3":"code","162e4f09":"code","44634d12":"code","f86f0d2b":"code","37583cab":"code","e9e47e2b":"code","11017270":"code","518aae23":"code","79d52e62":"code","4a5f55c5":"code","1a61e427":"code","27be4449":"code","3df27c11":"code","af9c84ef":"code","91a38da1":"code","9e0f28a4":"code","1cbcbf93":"code","c7cbb212":"markdown","b50538fc":"markdown","2729dadc":"markdown","edac262a":"markdown","8676fe32":"markdown","d5747ef2":"markdown","85b5b087":"markdown","2ed6e28d":"markdown","cd93c09a":"markdown","cfd770fc":"markdown","ba99c0d4":"markdown","c1a91d6a":"markdown","e37df6e4":"markdown","2bfa8cf2":"markdown","08ad4829":"markdown","9ef5f4bb":"markdown","0b4592de":"markdown","1159f3d1":"markdown","b7ac11b7":"markdown"},"source":{"3a055f47":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n!pip install progressbar\nimport progressbar\n\nimport os\n\nfrom surprise import Reader, Dataset\nfrom surprise.model_selection import train_test_split, cross_validate\n\nfrom sklearn.metrics.pairwise import linear_kernel","429ff434":"articles_metadata = pd.read_csv('..\/input\/news-portal-user-interactions-by-globocom\/articles_metadata.csv')  \narticles_metadata","61d6d61b":"articles_metadata.words_count.hist(bins=50, range=(0,400))\nplt.title('How many words per article');","fbe6547b":"clicks_hour_000 = pd.read_csv('..\/input\/news-portal-user-interactions-by-globocom\/clicks\/clicks\/clicks_hour_000.csv')  \nclicks_hour_000","497e477e":"clicks_hour_000.session_size.hist(bins=24, range=(0,10))\nplt.title('How many clicks per session');","4617fc31":"clicks_hour_000.loc[clicks_hour_000.user_id == 0]","f22bffa1":"clicks_hour_000.loc[clicks_hour_000.session_id == 1506828979881443]","5db732d4":"%%time\n\nif not os.path.exists('clicks.csv'):\n    clicks_path = []\n    clicks_dir = \"..\/input\/news-portal-user-interactions-by-globocom\/clicks\/clicks\"\n\n    clicks_path = clicks_path + sorted(\n            [\n                os.path.join(clicks_dir, fname)\n                for fname in os.listdir(clicks_dir)\n                if fname.endswith(\".csv\")\n            ]\n        )\n    print(\"Number of clicks csv:\", len(clicks_path))\n\n    _li = []\n\n    for filename in clicks_path:\n        df = pd.read_csv(filename, index_col=None, header=0)\n        _li.append(df)\n\n    clicks = pd.concat(_li, axis=0, ignore_index=True)\n    clicks.to_csv('clicks.csv')\nelse:\n    clicks= pd.read_csv('clicks.csv')\n    \nclicks","33f3039a":"df = clicks.groupby('user_id').agg(\n    LIST_click_article_id = ('click_article_id', lambda x: list(x)),\n)\ndf","706c7988":"%%time\n\nif not os.path.exists('..\/input\/p9-data\/df.csv'):\n    pbar = progressbar.ProgressBar(widgets=[progressbar.Percentage(), progressbar.Bar()], maxval=len(df)).start()\n    \n    df['categories'] = ''\n\n    for index, row in df.iterrows():\n        pbar.update(index)\n        _list_row = []\n        for article in row.LIST_click_article_id:\n            _list_row.append(articles_metadata[articles_metadata.article_id == article].category_id.values[0])\n        df.loc[index]['categories']=_list_row\n    df.to_csv('df.csv')\n    pbar.finish()\nelse:\n    df= pd.read_csv('..\/input\/p9-data\/df.csv')","1d59287e":"df","d93fe4e2":"def inputUserRatings(userId):\n    _matrix = pd.DataFrame(columns=['click'])\n    _row = df.loc[userId]['categories']\n    _row = _row.replace('[', '').replace(']', '').replace(',', '').split()\n    \n    for index, val in pd.Series(_row).value_counts().items():\n        _matrix.loc[index] = int(val)\n         \n    _matrix['click_norm'] = _matrix.apply(lambda x : x \/ _matrix['click'].max())\n    _matrix = _matrix.reset_index()\n    _matrix = _matrix.rename(columns={\"index\": \"category_id\"})\n    _matrix['category_id'] = _matrix['category_id'].astype(int)\n    \n    return _matrix","4a5685e8":"inputUserRatings(0)","9c393917":"articles_matrix = articles_metadata.loc[:, ['article_id', 'category_id']]\ndisplay(articles_matrix.dtypes)","07f2d7b2":"def recommend5(userId, articles_matrix):\n    _input_user_ratings = inputUserRatings(userId)\n    _articles_matrix = articles_matrix\n    \n    _weighed_articles_matrix = pd.DataFrame(columns=['article_id', 'category_id', 'weight'])\n    _weighed_articles_matrix['article_id'] = _articles_matrix['article_id']\n    _weighed_articles_matrix['category_id'] = _articles_matrix['category_id']\n    #display(_weighed_articles_matrix.dtypes)\n\n    _weighed_articles_matrix['weight'] = _weighed_articles_matrix['category_id'].apply(lambda x :\n                                                                                        0                                                                                           \n                                                                                       if(_input_user_ratings.loc[_input_user_ratings['category_id'] == x].empty == True)\n                                                                                       else\n                                                                                        _input_user_ratings.loc[_input_user_ratings['category_id'] == x].click_norm.values[0]\n                                                                                      )\n\n    \n    return _weighed_articles_matrix[_weighed_articles_matrix.weight > 0].sort_values(by=['weight'], ascending=False)","0b389ad3":"%%time\nres = recommend5(0 , articles_matrix)\nres","07768463":"clicks.head()","286c216a":"articles_metadata.head()","b2fb47d2":"%%time\ndataframe = clicks.merge(articles_metadata, left_on='click_article_id', right_on='article_id')","1d4601f6":"dataframe = dataframe[['user_id', 'article_id', 'category_id']]\ndataframe","9e0bdf90":"%%time\nseries = dataframe.groupby(['user_id', 'category_id']).size()\nuser_rating_matrix = series.to_frame()\nuser_rating_matrix = user_rating_matrix.reset_index()\nuser_rating_matrix.rename(columns = {0:'rate'}, inplace = True)","8cac2bea":"user_rating_matrix","c054f606":"user_rating_matrix.to_csv('user_rating.csv')","3c1417bf":"reader = Reader(rating_scale=(1,10))\n_x = user_rating_matrix.loc[user_rating_matrix.rate > 1]\ndata = Dataset.load_from_df(_x[['user_id', 'category_id', 'rate']], reader)\n\nprint('We have selects', len(_x), 'interactions.')","4943370f":"%%time\ntrainset, testset = train_test_split(data, test_size=0.25)\nprint('Test set lenght :', len(testset))\nprint('Train set lenght :', len(_x) - len(testset))","73ced100":"%%time\nfrom surprise import SVD, accuracy\nalgo = SVD()\nalgo.fit(trainset)","431054c5":"%%time\npredictions = algo.test(testset)\nprint('Number of predictions in Test set :', len(predictions))","0b177eb5":"%%time\naccuracy.rmse(predictions)","7ed6f252":"from collections import defaultdict\n\n\ndef get_top_n(predictions, n=10):\n    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n\n    Args:\n        predictions(list of Prediction objects): The list of predictions, as\n            returned by the test method of an algorithm.\n        n(int): The number of recommendation to output for each user. Default\n            is 10.\n\n    Returns:\n    A dict where keys are user (raw) ids and values are lists of tuples:\n        [(raw item id, rating estimation), ...] of size n.\n    \"\"\"\n\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n","cb36e924":"%%time\ntop_n = get_top_n(predictions, n=10)","29bbc318":"with open(\"top_n.txt\", \"wb\") as fp:\n    pickle.dump(top_n, fp)","4dbeebc3":"def findRecom(dic, userId):\n    res = []\n    query = dic[userId]\n    for uid, user_ratings in query:\n        res.append(uid)\n    return res","0e1b3fe4":"findRecom(top_n, 0)","1c47993a":"res[['category_id', 'weight']].groupby('category_id').mean().sort_values('weight', ascending=False)","331abb36":"import pandas as pd\npickle = pd.read_pickle('..\/input\/news-portal-user-interactions-by-globocom\/articles_embeddings.pickle')\npickle.shape","90a1208b":"from operator import itemgetter\n\ndef find_top_n_indices(data, top=5):\n    indexed = enumerate(data)\n    sorted_data = sorted(indexed, \n                         key=itemgetter(1), \n                         reverse=True) \n    return [d[0] for d in sorted_data[:top]] ","97c260de":"def recommendFromArticle(article_id, top):\n    score = []\n    for i in range(0, len(pickle)):\n        if(article_id != i):\n            cos_sim = np.dot(pickle[article_id], pickle[i])\/(np.linalg.norm(pickle[article_id])*np.linalg.norm(pickle[i]))\n            score.append(cos_sim)\n    \n    _best_scores = find_top_n_indices(score, top)\n            \n    return _best_scores","27e1193c":"%%time\nrecommendFromArticle(157077, 10)","733d5a82":"cosine_similarities = linear_kernel(pickle[:50000], pickle[:50000]) \ncosine_similarities.shape","e0b55f78":"titles = articles_metadata['article_id']\nindices = pd.Series(articles_metadata.index, index=articles_metadata['article_id'])","6c611b9c":"def simScores(title):\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_similarities[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:21]\n    \n    return sim_scores\n    print(sim_scores)","f86ddc84":"def predictWSimScores(userId):\n    _input_user_ratings = inputUserRatings(userId)\n    _result = []\n    for index, row in _input_user_ratings.iterrows():\n        _x = simScores(row.category_id)\n        for i in range(1, row.click + 1):\n            _result = _result + _x\n    return _result","5a58be57":"%%time\ntest = predictWSimScores(0)","8c519cd3":"pd.DataFrame(test, columns =['category_id', 'score']).groupby('category_id').mean().sort_values('score',ascending=False).head(5)","162e4f09":"import numpy as np","44634d12":"import pandas as pd\npickle = pd.read_pickle('..\/input\/news-portal-user-interactions-by-globocom\/articles_embeddings.pickle')\npickle.shape","f86f0d2b":"pickle= pickle[50000:100000]","37583cab":"pickle.shape","e9e47e2b":"%%time\nfrom sklearn.metrics.pairwise import cosine_similarity\ncosine_similarities  = cosine_similarity(pickle, pickle)","11017270":"with open('cosinepart2.npy', 'wb') as f:\n    np.save(f, cosine_similarities)","518aae23":"cosine_similarities = []","79d52e62":"import csv\nwith open('cosine_data_test.csv','a') as f:\n    writer = csv.writer(f)\n    i=0\n    while i!=pickle.shape[0]:\n        if i%100000!=0:\n            #Iterating over 10,000 multiples(10 chunks)\n            cosine_sim = linear_kernel(pickle[i:i+1000], pickle)\n            print(\"{} completed\".format(i+1000))\n            writer.writerows(cosine_sim)\n            i= i + 10000","4a5f55c5":"10000%100000","1a61e427":"%%time\nlk = linear_kernel(pickle[:1000], pickle)","27be4449":"lk.shape","3df27c11":"lk2 = linear_kernel(pickle[1000:2000], pickle)","af9c84ef":"lk2.shape","91a38da1":"import csv\nwith open('linear_kernel.csv','w') as f:\n    writer = csv.writer(f)\n    i=0\n    step = 10\n    while i<20:\n        cosine_sim = linear_kernel(pickle[i:i+step], pickle)\n        print(i+step, 'done.')\n        writer.writerows(cosine_sim)\n        \n        i = i + step       ","9e0f28a4":"t = pd.read_csv('linear_kernel.csv')","1cbcbf93":"t.shape","c7cbb212":"# Content-based : by category\n## Prepare data\nLet's combine all the clicks file into a big one.","b50538fc":"### Articles_metadata meaning (according to me)\n* article_id : unique id for the article\n* category_id : the category of the article, there is 461 category.\n* created_at_ts : timestamp when the article as been created. You should remove the 3 last numbers. Oldest article : 2006\/09\/27 11:14:35. Newest : 2018\/3\/13 12:12:30.\n* publisher_id : seems empty. only 0\n* words_count : how many words in the article. From 0 (seems a bug) to 6690. Distribution is available in few cells.","2729dadc":"# Discovering dataset","edac262a":"This is very similar to our previous function *inputUserRatings* but here it's a dataframe with every user.","8676fe32":"# Summary\nWe are going to try 4 differents way to predict articles to the user.\n\n* Baseline : Just like this video (https:\/\/www.youtube.com\/watch?v=YMZmLx-AUvY), we are just going to recreate a matrix for the article and the user preferenced only based on previous clicks.\n* Collaborative Filtering : We are going to use the surprise librairy. https:\/\/github.com\/NicolasHug\/Surprise\n* Content-based Filtering : We are going to use word embedding.\n* Hybrid methods","d5747ef2":"Now we have article_id and category_id that users clicked on.\n\n## Calculate clicks per user\nHow many time an user clicked on a article from X category_id.","85b5b087":"# Content-based : using articles embedding","2ed6e28d":"## try to reduce memory usage","cd93c09a":"# Don't forget to UPVOTE if it helped you. Thank you","cfd770fc":"## Recommend books from the 360 000 books available based on prefered category_id.","ba99c0d4":"### Clicks_hours meaning (according to me)\n* user_id : unique id for the user.\n* session_id : unique id for an user's session. Can appear multiple times because multiple clicks per session.\n* session_start : timestamp of the session. You must remove the 3 last numbers. Oldest session : 2017\/10\/01 2:37:3. Newest session : 2017\/10\/1 3:39:19\n* session_size : how many clicks per session. From 2 to 24. Distribution available few cells below.\n* click_article_id : the article that has been cliked on.\n* click_environment : no idea, maybe from website\/app ? There is only three types, and the type 4 takes more than 90%. \n* click_deviceGroup : must be the kind of device : computer, smartphone or tablet ?\n* click_os : the operation system of the device.\n* click_country : which country the user come from.\n* click_region : same but for region.\n* click_referrer_type : good question.\n","c1a91d6a":"As you can see, it works but it takes too much time. Approx 2 min per user. Only works if user has few interactions.\nWe will use the result of this algo to compare with the next algorithm.","e37df6e4":"Let's compare the result with our first algo","2bfa8cf2":"## Using the surprise librairy","08ad4829":"# Collaborative filtering\nLet's try with using the surprise librairy.\nhttps:\/\/medium.com\/hacktive-devs\/recommender-system-made-easy-with-scikit-surprise-569cbb689824\n## Prepare data","9ef5f4bb":"We need to group every article click per user.","0b4592de":"We can see we have the same result as previously.","1159f3d1":"## Method 1 : We calculate every combinaison each time (use CPU).","b7ac11b7":"## Method 2 : Creating a linear kernel (use Memory)\nWe must sample the pickle because we 'only' have 16go of RAM."}}