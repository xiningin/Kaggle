{"cell_type":{"474ccc4a":"code","065d8015":"code","9e45879a":"code","8abd8dc0":"code","46c8c253":"code","d9a369e9":"markdown","203b2cc6":"markdown","414efbe3":"markdown","ab310f50":"markdown","c7dd804f":"markdown","c912c552":"markdown","a36bcacd":"markdown"},"source":{"474ccc4a":"\"\"\"\nPublic LB: 0.50456\n\"\"\"\nfrom collections import Counter, defaultdict\n\nimport lightgbm as lgb\nimport ml_metrics\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nimport optuna\n\n\ndef parse_host_verifications(df):\n    raw_value_list = []\n    for val in df['host_verifications'].tolist():\n        values = eval(val)\n        if values is not None:\n            raw_value_list.append(Counter(values))\n        else:\n            raw_value_list.append({})\n\n    vectorizer = DictVectorizer(sparse=False)\n    X = vectorizer.fit_transform(raw_value_list)\n    for idx, col in enumerate(vectorizer.feature_names_):\n        df[f'host_verifications_{col}'] = X[:, idx]","065d8015":"def load_data():\n    df_trn = pd.read_csv('\/kaggle\/input\/tmu-inclass-competition\/train.csv')\n    df_tst = pd.read_csv('\/kaggle\/input\/tmu-inclass-competition\/test.csv')\n    df = pd.concat([df_trn, df_tst], sort=False)\n    original_train_size = len(df_trn)\n    y_train = df.iloc[:original_train_size]['price'].values\n\n    # calendar_updated \u306e\u51e6\u7406  'a week ago' \u306a\u3069\u306e\u6587\u5b57\u5217\u3092\u65e5\u6570\u306e int (\u3053\u306e\u5834\u5408\u306f7) \u306b\u5909\u63db\n    repl_day = lambda m: m.group(1)\n    repl_week = lambda m: str(int(m.group(1)) * 7)\n    repl_month = lambda m: str(int(m.group(1)) * 30)\n    df.calendar_updated = df.calendar_updated.str.replace('today', '0')\n    df.calendar_updated = df.calendar_updated.str.replace('yesterday', '1')\n    df.calendar_updated = df.calendar_updated.str.replace(r'^(\\d)\\sday(.+)ago$', repl_day, regex=True)\n    df.calendar_updated = df.calendar_updated.str.replace('a week ago', '7')\n    df.calendar_updated = df.calendar_updated.str.replace(r'^(\\d)\\sweek(.+)ago$', repl_week, regex=True)\n    df.calendar_updated = df.calendar_updated.str.replace('a month ago', '30')\n    df.calendar_updated = df.calendar_updated.str.replace(r'^(\\d{1,2})\\smonth(.+)ago$', repl_month, regex=True)\n    df.calendar_updated = df.calendar_updated.str.replace('never', '10000')\n    df.calendar_updated = df.calendar_updated.apply(lambda x: int(x))\n\n    # host_response_time \u306e\u51e6\u7406\n    df.host_response_time = df.host_response_time.str.replace('within an hour', '1')\n    df.host_response_time = df.host_response_time.str.replace('within a few hours', '3')\n    df.host_response_time = df.host_response_time.str.replace('within a day', '24')\n    df.host_response_time = df.host_response_time.str.replace('a few days or more', '120')\n    df.host_response_time = df.host_response_time.fillna('58').apply(lambda x: int(x))\n\n\n    repl = lambda m: m.group(1)\n    df.host_response_rate = df.host_response_rate.str.replace(r'^(\\d+)%$', repl, regex=True)\n    df.host_response_rate = df.host_response_rate.fillna('80').apply(lambda x: int(x))\n\n    df.host_since = df.host_since.str.replace('-', '\/')\n    df.loc[:, 'host_since_year'] = df.host_since.fillna('2020\/01\/01').apply(\n        lambda x: int(x.split('\/')[0]))\n    df.loc[:, 'host_since_month'] = df.host_since.fillna('2020\/01\/01').apply(\n        lambda x: int(x.split('\/')[1]))\n    df.loc[:, 'host_since_day'] = df.host_since.fillna('2020\/01\/01').apply(\n        lambda x: int(x.split('\/')[2]))\n\n    # Parse host_verifications\n    parse_host_verifications(df)\n\n    # Baseline features\n    categorical_cols = []\n    cols = []\n    for col in df.columns:\n        if col in ['listing_id', 'price', 'host_since', 'host_acceptance_rate', 'square_feet', 'picture_url']:\n            continue\n\n        if pd.api.types.is_numeric_dtype(df[col]):\n            df[col] = df[col].fillna(df[col].mean())\n        else:\n            df[col] = df[col].factorize()[0]\n            categorical_cols.append(col)\n        cols.append(col)\n\n    return df, df_trn, df_tst, y_train, cols, categorical_cols","9e45879a":"def cv(trial):\n    np_random_seed = trial.suggest_int(\"np_random_seed\", 0, 1000)\n    np.random.seed(np_random_seed)\n    # params['boosting_type'] = trial.suggest_categorical('boosting_type', ['gbdt', 'rf', 'dart', 'goss'])\n    # params['objective'] = trial.suggest_categorical('objective', ['regression', 'regression_l1', 'huber', 'fair', 'poisson', 'quantile', 'mape', 'gamma', 'tweedie'])\n    params['num_leaves'] = trial.suggest_int(\"num_leaves\", 1, 1000)\n    params['num_iterations'] = trial.suggest_int(\"num_iterations\", 1, 1000)\n    params['learning_rate'] = trial.suggest_uniform(\"learning_rate\", 1e-7, 1e-1)\n    params['tree_learner'] = trial.suggest_categorical('tree_learner', ['serial', 'feature', 'data', 'voting'])\n    params['seed'] = trial.suggest_int(\"seed\", 0, 1000)\n    # params['max_depth'] = trial.suggest_int(\"max_depth\", -1, 2000)\n    # params['min_data_in_leaf'] = trial.suggest_int(\"min_data_in_leaf\", 0, 2000)\n    # params['min_sum_hessian_in_leaf'] = trial.suggest_uniform(\"min_sum_hessian_in_leaf\", 0, 1)\n\n    params['bagging_fraction'] = trial.suggest_uniform(\"bagging_fraction\", 0.001, 0.999)\n    params['bagging_freq'] = trial.suggest_int(\"bagging_freq\", 1, 2000)\n    params['bagging_seed'] = trial.suggest_int(\"bagging_seed\", 0, 2000)\n\n    params['feature_fraction'] = trial.suggest_uniform(\"feature_fraction\", 0, 1)\n    params['feature_fraction_bynode'] = trial.suggest_uniform(\"feature_fraction_bynode\", 0, 1)\n    params['feature_fraction_seed'] = trial.suggest_int(\"feature_fraction_seed\", 0, 2000)\n\n    fit_params['num_boost_round'] = trial.suggest_int(\"num_boost_round\", 0, 2000)\n    fit_params['verbose_eval'] = trial.suggest_int(\"verbose_eval\", 0, 2000)\n\n    df, df_trn, df_tst, y_train, cols, categorical_cols = load_data()\n    original_train_size = y_train.shape[0]\n    X_train = df.iloc[:original_train_size][cols].values\n\n    val_score_list = []\n    kf = KFold(n_splits=3, random_state=11, shuffle=True)\n    for idx_valtrn, idx_valtst in kf.split(X_train):\n        X_valtrn, X_valtst = X_train[idx_valtrn], X_train[idx_valtst]\n        y_valtrn, y_valtst = y_train[idx_valtrn], y_train[idx_valtst]\n\n        lgb_valtrn = lgb.Dataset(X_valtrn, np.log1p(y_valtrn),\n                                 feature_name=cols,\n                                 categorical_feature=categorical_cols)\n        lgb_eval = lgb.Dataset(X_valtst, np.log1p(y_valtst),\n                               reference=lgb_valtrn,\n                               feature_name=cols,\n                               categorical_feature=categorical_cols)\n\n        fit_params['valid_sets'] = lgb_eval\n        clf = lgb.train(params, lgb_valtrn, **fit_params)\n\n        y_pred = np.expm1(clf.predict(X_valtst,\n                                      num_iteration=clf.best_iteration))\n        val_score = ml_metrics.rmsle(y_pred, y_valtst)\n        print(f'RMSLE: {val_score:.6f}')\n        val_score_list.append(val_score)\n\n    avg_val_score = np.mean(val_score_list)\n    print(f'Avg-RMSLE: {avg_val_score:.6f}')\n    return avg_val_score","8abd8dc0":"def main():\n    np.random.seed(np_random_seed)\n    df, df_trn, df_tst, y_train, cols, categorical_cols = load_data()\n    original_train_size = y_train.shape[0]\n\n    X_train = df.iloc[:original_train_size][cols].values\n    X_test = df.iloc[original_train_size:][cols].values\n    # Early stopping \u306e\u305f\u3081\u306e validation split \u3092\u4f5c\u6210\n    X_valtrn, X_valtst, y_valtrn, y_valtst = train_test_split(\n        X_train, y_train, test_size=0.1, random_state=11)\n\n    lgb_valtrn = lgb.Dataset(X_valtrn, np.log1p(y_valtrn),\n                             feature_name=cols,\n                             categorical_feature=categorical_cols)\n    lgb_eval = lgb.Dataset(X_valtst, np.log1p(y_valtst),\n                           reference=lgb_valtrn,\n                           feature_name=cols,\n                           categorical_feature=categorical_cols)\n\n    fit_params['valid_sets'] = lgb_eval\n    clf = lgb.train(params, lgb_valtrn, **fit_params)\n\n    y_pred = np.expm1(clf.predict(X_valtst, num_iteration=clf.best_iteration))\n    print('RMSLE: {:.6f}'.format(ml_metrics.rmsle(y_pred, y_valtst)))\n\n    y_pred = np.expm1(clf.predict(X_test, num_iteration=clf.best_iteration))\n    df_tst.loc[:, 'price'] = y_pred\n    df_tst[['listing_id', 'price']].to_csv('.\/optuna.csv', index=False)","46c8c253":"if __name__ == '__main__':\n    # LightGBM parameters\n    # https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst#core-parameters\n    params = defaultdict(int)\n    params['task'] = 'train'\n    params['boosting_type'] = 'gbdt'\n    params['objective'] = 'regression'\n    params['metric'] = 'rmse'\n    params['num_leaves'] = 60\n    params['learning_rate'] = 0.1\n    params['feature_fraction'] = 1.0\n    # params['bagging_fraction'] = 1.0\n    params['verbose'] = -1\n    params['num_threads'] = 10  # \u4f7f\u7528\u3059\u308b CPU \u306e\u500b\u6570\n    # https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst#learning-control-parameters\n    fit_params = defaultdict(int)\n    fit_params['num_boost_round'] = 8\n    fit_params['verbose_eval'] = 8\n    fit_params['early_stopping_rounds'] = 3\n    \n    study = optuna.create_study()\n    study.optimize(cv, n_trials=1)\n    print(study.best_params)\n    print(study.best_value)\n    # cv() \u4eca\u56de\u306f optuna \u306b\u98df\u308f\u305b\u308b\u6642\u306b\u4f7f\u3046\u3060\u3051\u306a\u306e\u3067\u3001\u305d\u306e\u307e\u307e\u4f7f\u3046\u5fc5\u8981\u306f\u306a\u3044\u3002\n    np_random_seed = study.best_params['np_random_seed']\n    main()","d9a369e9":"\u4f7f\u3063\u305f\u30b3\u30fc\u30c9\n\n\u9014\u4e2d\u307e\u3067\u5c0f\u5d5c\u3055\u3093\u306e\u30b3\u30fc\u30c9\u305d\u306e\u307e\u307e","203b2cc6":"\u4ea4\u5dee\u691c\u5b9a\u306e\u30b3\u30fc\u30c9\u3092 Oputuna \u306b\u98df\u308f\u305b\u308b (\u79cb\u8449\u3055\u3093\u306b\u611f\u8b1d)\u3002\nnp.random.seed() \u306b\u3064\u3044\u3066\u3082\u3001\u4eca\u56de\u63a2\u7d22\u3055\u305b\u305f\u3002 (LightGBM \u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306e 'seed' \u3068 'feature_fraction_seed' \u3082\u63a2\u7d22\u3057\u3066\u3044\u308b\u3002\u30b7\u30fc\u30c9\u30ac\u30c1\u30e3\u91cd\u8981\u3002)","414efbe3":"\u6700\u9ad8\u30b9\u30b3\u30a2\u3068\u306a\u3063\u305f\u30d1\u30e9\u30e1\u30fc\u30bf\n\nnp.random.seed(935)\n\nparams = {\n    'tree_learner': 'voting',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'num_leaves': 560,\n    'num_iterations': 925,\n    'learning_rate': 0.015239271800757483,\n    'seed': 261,\n    'bagging_fraction': 0.997468627937743,\n    'bagging_freq': 1998,\n    'bagging_seed': 763,\n    'feature_fraction': 0.34900426027753045,\n    'feature_fraction_bynode': 0.6913492205645652,\n    'feature_fraction_seed': 514,\n    'num_threads': 45\n}\n\nfit_params = {\n    'num_boost_round': 1133,\n    'verbose_eval': 1880,\n    'early_stopping_rounds': \u63a2\u7d22\u6642\u306f 3 \u3067\u3001\u63d0\u51fa\u6642\u306f2000 (Best iteration \u306f 923\u3060\u3063\u305f),\n}","ab310f50":"main \u95a2\u6570\u306f np.random.seed(np_random_seed) \u3092\u4ed8\u3051\u52a0\u3048\u305f\u3060\u3051\u3002","c7dd804f":"load_data() \u306e\u90e8\u5206\u3067\u524d\u51e6\u7406\u306e\u5b9f\u88c5\u3092\u8ffd\u52a0\u3002\nhost_verifications \u306b\u3064\u3044\u3066\u306f\u3001\u4e0a\u306e\u95a2\u6570 parse_host_verifications(df) \u3067\u51e6\u7406\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u306a\u306e\u3067\u3001\u5b9f\u88c5\u3057\u3066\u3044\u306a\u3044\u3002","c912c552":"study.optimize(cv, n_trials=1)\nn_trials \u306e\u6570\u3067\u3001optuna \u306e\u8a66\u884c\u56de\u6570\u3092\u6307\u5b9a","a36bcacd":"\u30b9\u30b3\u30a2\u304c 0.37927 \u306b\u306a\u3063\u305f\u6642\u306e\u3084\u308a\u65b9\u3002\u5c0f\u5d5c\u3055\u3093\u306e\u30b3\u30fc\u30c9\u3092\u30d9\u30fc\u30b9\u306b\u3001\u3044\u308d\u3044\u308d\u3044\u3058\u3063\u305f\u3002\n\u5c0f\u5d5c\u3055\u3093\u306e\u30b3\u30fc\u30c9\u306f\u4ee5\u4e0b\u3002\nhttps:\/\/gist.github.com\/smly\/2fb2b87e4ba2eea4b6f8023d11953973\n\n\u57fa\u672c\u7684\u306b\u306f\u3001\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406 + Oputuna \u3067\u9ad8\u3044\u30b9\u30b3\u30a2\u3092\u51fa\u305b\u308b\u3002\n\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u3092\u3084\u3089\u306a\u304f\u3066\u3082\u3001Oputuna \u3060\u3051\u3067 0.38948 (\u4eca\u56de2\u4f4d\u306e\u4eba) \u306b\u306a\u308b\u3088\u3046\u3067\u3001\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u306e\u8ca2\u732e\u5ea6\u306f\u4f4e\u305d\u3046\u3002\n\nOputuna \u3067\u3067\u304d\u308b\u9650\u308a\u3044\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a2\u3057\u305f\u304b\u3063\u305f\u306e\u3067\u3001 LightGBM \u3067\u4eca\u56de\u306e\u51e6\u7406\u306b\u95a2\u4fc2\u3042\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5168\u3066\u5217\u6319\u3057\u3066 Oputuna \u306b\u98df\u308f\u305b\u305f\u3002\n\u305d\u306e\u5f8c\u3001\u305d\u308c\u3060\u3068\u6642\u9593\u304c\u304b\u304b\u308a\u3059\u304e\u308b\u3053\u3068\u304c\u5206\u304b\u3063\u305f\u306e\u3067\u3001 Oputuna \u3092\u56de\u3057\u306a\u304c\u3089\u3001\u5fc5\u8981\u306a\u3055\u305d\u3046\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u63a2\u7d22\u306f\u3084\u3081\u305f\u3002\nhttps:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst#core-parameters\n\n\u53c2\u8003\u306b\u306a\u308b Kaggle \u306b\u3064\u3044\u3066\u306e\u30da\u30fc\u30b8\nhttps:\/\/qiita.com\/upura\/items\/3c10ff6fed4e7c3d70f0"}}