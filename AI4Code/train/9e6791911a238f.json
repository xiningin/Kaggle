{"cell_type":{"fe0680bb":"code","fa7e1212":"code","7330bba9":"code","a3410e33":"code","36222e26":"code","2161478b":"code","5c8c6eaa":"code","8f5bb7f3":"code","fbccada9":"code","e2e25c90":"code","e32a4b11":"code","8566fc23":"code","70e2d37b":"code","3059e12d":"code","38ae53b3":"code","0ff7ff31":"code","9e52200d":"code","c7883cef":"markdown","f5b73b6f":"markdown","ebaa0189":"markdown","45083105":"markdown","ab723345":"markdown","c929bfd8":"markdown","0f8d638d":"markdown","f531136c":"markdown","8fac375a":"markdown","d825c784":"markdown","3636a9ce":"markdown","9b4e6c3a":"markdown"},"source":{"fe0680bb":"import numpy as np\nimport pandas as pd\ndf = pd.read_csv('..\/input\/matplotlib-datasets\/iris_dataset.csv')","fa7e1212":"df.head()","7330bba9":"df2 = df.drop('species',axis = 1)","a3410e33":"df2.head()","36222e26":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\ndf2_scaled = ss.fit_transform(df2)\ndf2","2161478b":"cov_matrix = np.cov(df2_scaled.T)\ncov_matrix","5c8c6eaa":"import seaborn as sns\nsns.heatmap(cov_matrix)","8f5bb7f3":"eigen_values,eigen_vectors = np.linalg.eig(cov_matrix)\nprint('Eigen Values\\n',eigen_values)\nprint()\nprint('Eigen Vectors\\n',eigen_vectors)","fbccada9":"sorted_eig_vals = pd.Series(eigen_values).sort_values(ascending =False)\npcs = eigen_vectors[:,list(sorted_eig_vals.index)].T\npcs","e2e25c90":"tot = np.sum(sorted_eig_vals)\nvar_exp = [(i\/tot) * 100 for i in sorted_eig_vals]\ncum_var_exp = np.cumsum(var_exp)\nprint('Explained variance by each principal component : \\n [PC1,PC2,PC3,PC4]\\n',var_exp)\nprint()\nprint('Cumulative explained variance : ',cum_var_exp)","e32a4b11":"selected_pcs = pcs[:2]\nselected_pcs","8566fc23":"tranps_select_pcs = selected_pcs.T\ntranps_select_pcs","70e2d37b":"projected_data = np.dot(df2_scaled,tranps_select_pcs)\nprojected_data.shape","3059e12d":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters= 3,n_init=15,random_state=2)\nkmeans.fit(projected_data)","38ae53b3":"kmeans.inertia_","0ff7ff31":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters= 3,n_init=15,random_state=2)\nkmeans.fit(df2_scaled)","9e52200d":"kmeans.inertia_","c7883cef":"***Please UpVote if you like the work!!!***","f5b73b6f":"***Please UpVote if you like the work!!!***","ebaa0189":"# Using Kmeans on the transformed data using PCA","45083105":"# Kmeans on original data","ab723345":"np.cov calculates the covariance row-wise. So we transpose the dataset to represent the features as rows.","c929bfd8":"Now to transform our data to a data which has its dimensionality reduced, we need to do a dot product between our originally scaled data and the PCs.\n\nThis would mean that we are considering using only the important information from the high variance zone(**FIGURE 2**) and eliminating all the redundant noise\/less important information by elimininating the PCs which explain the low variance region(**FIGURE 3**).\n\nOur scaled data is 150x4 and our selected PCs are 2X4. So we would need to transpose our selected PCs to do the dot product.","0f8d638d":"The actual representation of principal components will be the transpose of eigen vectors as follows:\n\nPC1 : [ 0.52237162, -0.26335492,  0.58125401,  0.56561105]\n\nPC2 : [-0.37231836, -0.92555649, -0.02109478, -0.06541577]\n\nPC3 : [-0.72101681,  0.24203288,  0.14089226,  0.6338014 ]\n\nPC4 : [ 0.26199559, -0.12413481, -0.80115427,  0.52354627]","f531136c":"As we can see that the inertia value on transformed data using PCA is less as compared to the inertia value on the original data. Thus it gives us an improved performance after transforming the data using PCA.","8fac375a":"So as we can see that most of the variance in data has been explained by the 1st two principal components that is about 96 percent.\n\nSo most of the information essential to differentiate the feature from each other has been captured by the 1st two PCs.\n\nOverdoing PCA is not advisable. Reducing high number of PCs could lead to introducing bias error in our model. The least percentage advisable is around 90 percent.","d825c784":"Eigen Vectors here are the principal components(PCs). We need to sort these eigen vectors in descending order w.r.t their corresponding eigen values.","3636a9ce":"To know more about why we compare inertia values for comparing models, please do checkout my kernel \"Kmeans Clustering Guide\" : https:\/\/www.kaggle.com\/pratikasarkar\/kmeans-clustering-guide","9b4e6c3a":"# **Principal Component Analysis (PCA)**\n### It is a data transformation technique. The final objective of PCA is DIMENSIONALITY REDUCTION.\nThe general myth, that the idea of dimensionality reduction means that PCA will drop some of the weak features, is WRONG.\n\nConsider we have some features which are highly significant in our data, out of these some are highly correlated to out target variable and some a weakly correlated. There is a chance that those weakly correlated features have high correlation among themselves, eventually adding redundancy and multicolliearity to our models. This multicolliearity effect is the NOISE in PCA. So the intention of PCA is to reduce that noise.\n\n***Lets have a look at it into with an example more intuitively.***\n\nConsider we have 15 features in a dataset. Out these there are 2 features which are highly correlated to each other.\n\nSo if we plot a histogram of these 2 features, a majority of the part will be overlapping as they are highly correlated. You can see the plot below :\n\n**FIGURE 1 :**   \n![alt text](https:\/\/i.ibb.co\/k1TRbNv\/Q9GDn.png)\n\nThe important information which these 2 features will provide us will be those data points which are not redundant\/non overlapping and lie at the extreme ends of the histograms as shown in the plot below:\n\n**FIGURE 2 :**\n\n![alt text](https:\/\/i.ibb.co\/7VR6ZNy\/Q9GDn1.png)\n\nThis is the information which we cannot afford to lose because the information in these records are responsible for significantly differentiating the 2 features from each other.\n\nOn the other hand, there is a lot of redundant\/overlapping data due to high correlation in these 2 features which gives rise to multicollinearity. The following plot represents this redundant data : \n\n**FIGURE 3 :**\n\n![alt text](https:\/\/i.ibb.co\/nnPmXYr\/Q9GDn2.png)\n\nThis redundant data causes multicolliearity. So even if we lose some data from this part, there won't be much information loss.\n\nSo we can say that we are not sacrificing the feature but we are sacrificing the information content at the low frequency\/low variance zone.\n\n*Consider an example of predicting a cancerous patient. We can afford to lose some information from a healthy patient(eg. normal bp range\/normal rbc-wbc level) as it will not affect our prediction result to an extreme level. But on the other hand we can't affort to lose the information from a cancerous patient(eg. abnormal bp range\/abnormal rbc-wbc level), bcoz it could lead to a loss of extensive information and our model could behave unexpectedly.*"}}