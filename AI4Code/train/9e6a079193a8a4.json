{"cell_type":{"771b4160":"code","1d2952d9":"code","9c824896":"code","2ec13451":"code","8eea5814":"code","68ab73b1":"code","bea999ad":"code","935dfc2c":"code","d4615948":"code","b966184c":"code","caad092d":"code","116ef423":"code","4f1d0205":"code","7c8e27b3":"code","8122846c":"code","6d6e115f":"code","fa40048f":"code","ae30da61":"code","d2a6abdd":"code","15e463be":"code","6b105a69":"code","385267f8":"code","211382a2":"code","894098c3":"code","05980471":"code","c1248594":"code","28d09f0e":"code","a4179556":"code","13ebf489":"code","8eb1842c":"code","aa558200":"code","00324e86":"code","ca2b6ea9":"code","36f7ae9d":"code","f819d17d":"code","82efb5f9":"code","f6736f0b":"code","4ef0d953":"code","8c01e8d7":"code","bc92699e":"code","c39ac6ce":"code","2c79415b":"code","5037e63f":"code","155cca89":"code","84921b21":"code","b30c4b98":"code","a9281d92":"code","65c5c25d":"code","02f949a4":"code","ece04a0a":"code","c797ef33":"code","c4cfea2f":"code","c2d2187f":"code","7aa414d3":"code","ba19caa4":"code","f1fa5601":"code","493086ad":"code","4a147c63":"code","f2f63d80":"code","9747121a":"code","8582c693":"code","06fe0427":"code","f8d40d51":"code","bed1d3ef":"code","7444a83b":"code","aa0a9f39":"code","1f61d2b9":"code","4b545c00":"code","ab60282b":"code","ad656ce0":"code","7ae05246":"code","327cf2d1":"code","7cfc33c0":"code","7aa22247":"code","9e888026":"code","375d934e":"code","631901f2":"code","fc46f210":"code","53e38f4d":"code","918af265":"code","bd65bf3a":"code","f06be1b4":"code","741f4e9a":"code","70359e03":"code","2b4e8600":"code","a012fbbc":"code","e6283c84":"code","fc4269ab":"code","58369280":"code","c4e5ebf0":"code","cfb5a69a":"code","469e1593":"code","47ced1b9":"code","c736f901":"code","b033248a":"code","3a1a68eb":"code","995232eb":"code","4f21d5ef":"code","4a2fc420":"code","26013e04":"code","4ed81be7":"code","0dce9f25":"code","15e78212":"code","f820f3ea":"code","a959150b":"code","fc6f08e9":"code","697ca4bc":"code","9ddcb174":"code","18ca285f":"code","24659e6d":"code","08d22ddc":"code","06a02b59":"code","3fd2eb85":"markdown","ca6d6bb6":"markdown","29db9298":"markdown","dfed4aa2":"markdown","070951b4":"markdown","c8d562f2":"markdown","fee8d248":"markdown","af61c931":"markdown","15e78a69":"markdown","5ef7b01b":"markdown","c8fc79b3":"markdown","31b5f285":"markdown","35176760":"markdown","5ff4bbee":"markdown","dea4c7b8":"markdown","fcbbd7b4":"markdown","81376a71":"markdown","66cdca60":"markdown","dea9ba48":"markdown","6885a02b":"markdown","6c963819":"markdown","08fafe29":"markdown","8e28e2a8":"markdown","d5e0d99b":"markdown","62e8cdd1":"markdown","aa614649":"markdown","ad69927c":"markdown","4817378f":"markdown","e8694edb":"markdown","8de0b3c1":"markdown","15b51334":"markdown","e199d0a8":"markdown","81e7bebe":"markdown","bbb0812d":"markdown","b77d1419":"markdown","6c3c98d7":"markdown","eb618c57":"markdown","ec92843a":"markdown","76285dde":"markdown","b441cc04":"markdown","36aceb79":"markdown","2e096a1c":"markdown","33265f48":"markdown","f0bf7ff8":"markdown","3ddc1dee":"markdown","7a853882":"markdown","44a5c271":"markdown","af765d2c":"markdown","01ff8a7c":"markdown","793d4dd9":"markdown"},"source":{"771b4160":"import pandas as pd\nimport numpy as np\n\n\n### Sklearn Packages\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1d2952d9":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')","9c824896":"test_data = pd.read_csv('..\/input\/titanic\/test.csv')","2ec13451":"train_data.head()","8eea5814":"test_data.head()","68ab73b1":"train_data.info()","bea999ad":"train_data.Survived.value_counts()","935dfc2c":"labels = train_data.Survived.value_counts()\nlabels.index=['Not-Survived', 'Survived']\nlabels","d4615948":"fig = plt.figure(figsize=[10, 6])\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\n\nax0.pie(labels, labels=labels.index, pctdistance=0.5, autopct='%.1f%%')\n\nplt.show()","b966184c":"round(train_data.isnull().sum()\/len(train_data.index)*100,2)","caad092d":"round(test_data.isnull().sum()\/len(test_data.index)*100,2)","116ef423":"train_data.Age.value_counts()","4f1d0205":"train_data.Age.describe()","7c8e27b3":"train_data.Age.fillna('28',inplace = True,axis = 0)","8122846c":"test_data.Age.fillna('28',inplace = True,axis = 0)","6d6e115f":"round(train_data.isnull().sum()\/len(train_data.index)*100,2)","fa40048f":"train_data.Cabin.value_counts()","ae30da61":"train_data[train_data.Cabin == 'C23 C25 C27']","d2a6abdd":"train_data.drop(columns = 'Cabin',inplace = True,axis = 1)\ntest_data.drop(columns = 'Cabin',inplace = True,axis = 1)","15e463be":"round(train_data.isnull().sum()\/len(train_data.index)*100,2)","6b105a69":"train_data.Embarked.value_counts()","385267f8":"train_data.Embarked.fillna('S',inplace = True,axis = 0)","211382a2":"train_data.Embarked.value_counts()","894098c3":"train_data.Ticket.value_counts()","05980471":"train_data.drop(columns = 'Ticket',inplace = True,axis = 1)\ntest_data.drop(columns = 'Ticket',inplace = True,axis = 1)","c1248594":"round(train_data.isnull().sum()\/len(train_data.index)*100,2)","28d09f0e":"round(test_data.isnull().sum()\/len(test_data.index)*100,2)","a4179556":"test_data.Fare.describe()","13ebf489":"test_data.Fare.median()","8eb1842c":"test_data.Fare.fillna(test_data.Fare.median(),inplace = True)","aa558200":"round(test_data.isnull().sum()\/len(test_data.index)*100,2)","00324e86":"train_data.drop(columns = ['PassengerId','Name'],inplace = True,axis = 1)\ntest_data.drop(columns = ['PassengerId','Name'],inplace = True,axis = 1)","ca2b6ea9":"test_data.head()","36f7ae9d":"train_data['Family_Count'] = train_data.SibSp+train_data.Parch\ntest_data['Family_Count'] = test_data.SibSp+train_data.Parch","f819d17d":"## Dropping the Orignial SibSP & Parch Columns\n\ntrain_data.drop(columns = ['SibSp','Parch'],inplace = True, axis = 1)\ntest_data.drop(columns = ['SibSp','Parch'],inplace = True, axis = 1)","82efb5f9":"train_data['IsAlone'] = train_data.Family_Count.apply(lambda x: 1 if x == 0 else 0)\ntest_data['IsAlone'] = test_data.Family_Count.apply(lambda x: 1 if x == 0 else 0)","f6736f0b":"train_data['IsAlone'].value_counts()","4ef0d953":"train_data.Age = train_data.Age.astype('float64')\ntest_data.Age = test_data.Age.astype('float64')","8c01e8d7":"def cat_age(val):\n    if val > 0 and val <= 12:\n        return 'Child'\n    elif val >= 13 and val<= 17:\n        return 'Teen'\n    elif val > 18 and val <= 59:\n        return 'Adult'\n    elif val >= 60:\n        return 'Senior Citizen'\n\ntrain_data['Age_Group'] = train_data.Age.apply(lambda x : cat_age(x))\ntest_data['Age_Group'] = test_data.Age.apply(lambda x : cat_age(x))","bc92699e":"train_data.Age_Group.value_counts()","c39ac6ce":"test_data.Age_Group.value_counts()","2c79415b":"train_data.drop(columns = 'Age',inplace = True,axis = 1)\ntest_data.drop(columns = 'Age',inplace = True,axis = 1)","5037e63f":"survived_data = train_data[train_data.Survived == 1]\nnot_survived_data = train_data[train_data.Survived == 0]","155cca89":"def PiePlotCatVar(var,Ordered=True, ascending=True):\n    fig, axarr = plt.subplots(2,2, figsize=(12, 12))\n    \n    survived_counts=survived_data[var].value_counts()\n    not_survived_count=not_survived_data[var].value_counts()\n    if Ordered:\n        survived_counts=survived_counts.sort_index(ascending=ascending)\n        not_survived_count=not_survived_count.sort_index(ascending=ascending)\n    else:\n        fully_paied_counts=fully_paied_counts.sort_values(ascending=ascending)\n        not_survived_count=not_survived_count.sort_values(ascending=ascending)\n    \n    wedges, texts =axarr[0][0].pie(survived_counts,labels= round(survived_counts\/survived_counts.sum()*100,2).apply(lambda x:str(x)+\"%\"))\n    axarr[0][0].legend(wedges, survived_counts.index,\n          title=str(var),\n          loc=\"center left\",\n          bbox_to_anchor=(1, 0, 0.5, 1))\n    \n    sns.countplot(survived_data[var], ax=axarr[1][0], order=survived_counts.index)\n    \n    axarr[0][0].set_title(\"Survived Passengers(\"+str(var)+\")\", fontsize=18)\n    \n    \n    \n    wedges, texts = axarr[0][1].pie(not_survived_count,labels= round(not_survived_count\/not_survived_count.sum()*100,2).apply(lambda x:str(x)+\"%\"))\n    \n    axarr[0][1].legend(wedges, not_survived_count.index,\n          title=str(var),\n          loc=\"center left\",\n          bbox_to_anchor=(1, 0, 0.5, 1))\n    \n    sns.countplot(not_survived_data[var].sort_index(ascending=ascending),ax=axarr[1][1], order = not_survived_count.index)\n    \n    axarr[0][1].set_title(\"Not-Survived Passengers(\"+str(var)+\")\", fontsize=18)\n    \n    fig.tight_layout()\n    plt.show()\n    \n    print('#'*50)\n    print('Below is the Surival Rate for each group in '+ str(var) + ' Feature \\n')\n    print(round(train_data.groupby(by = var).mean()['Survived']*100,2))","84921b21":"PiePlotCatVar('Sex')","b30c4b98":"PiePlotCatVar('Pclass')","a9281d92":"PiePlotCatVar('IsAlone')","65c5c25d":"sns.barplot(x = train_data.Sex,y = train_data.IsAlone)\nplt.show()\n\nprint('#'*50)\n\ntrain_data.groupby(by = 'Sex').count()['IsAlone']","02f949a4":"sns.barplot(x = train_data.Age_Group,y = train_data.IsAlone)\nplt.show()\n\nprint('#'*50)\n\ntrain_data[train_data.IsAlone == 1].groupby(by = ['Age_Group','Survived']).count()['IsAlone']","ece04a0a":"sns.barplot(x = train_data.Sex,y = train_data.Fare)\nplt.show()\n\nprint('#'*50)\n\ntrain_data.groupby(by = 'Sex').mean()['Fare']","c797ef33":"train_data.groupby(by = ['Embarked','Sex']).count()['Survived']","c4cfea2f":"train_data.groupby(by = ['Age_Group','Sex']).count()['Survived']","c2d2187f":"train_data.head()","7aa414d3":"columns = ['Pclass','Sex','Embarked','Age_Group']","ba19caa4":"for i in columns:\n    temp = pd.get_dummies(train_data[i], drop_first = True, prefix=i)\n    train_data = pd.concat([train_data, temp], axis = 1)\n    train_data.drop([i], axis = 1, inplace = True)","f1fa5601":"## Copying same dummy variables in Test Data\n\nfor i in columns:\n    temp = pd.get_dummies(test_data[i], drop_first = True, prefix=i)\n    test_data = pd.concat([test_data, temp], axis = 1)\n    test_data.drop([i], axis = 1, inplace = True)","493086ad":"train_data.head()","4a147c63":"test_data.head()","f2f63d80":"from sklearn.preprocessing import MinMaxScaler","9747121a":"scaled_cols = ['Fare','Family_Count']","8582c693":"train_data[scaled_cols]","06fe0427":"scaler = MinMaxScaler()\n\ntrain_data[scaled_cols] = scaler.fit_transform(train_data[scaled_cols])\n\ntrain_data.head()","f8d40d51":"scaler = MinMaxScaler()\n\ntest_data[scaled_cols] = scaler.fit_transform(test_data[scaled_cols])\n\ntest_data.head()","bed1d3ef":"y = train_data['Survived']\nX = train_data.drop(columns = 'Survived')","7444a83b":"X_train,X_train_valid,y_train,y_train_valid = train_test_split(X,y,train_size = .80,random_state = 100)","aa0a9f39":"print('X_train:: '+str(X_train.shape))\nprint('y_train:: '+str(y_train.shape))\nprint('X_train_valid:: '+str(X_train_valid.shape))\nprint('y_train_valid:: '+str(y_train_valid.shape))","1f61d2b9":"X_test = test_data","4b545c00":"X_test","ab60282b":"df_model_summary = pd.DataFrame(columns = ['Algorithm','Dataset Type','HPT','Accuracy(%)'])","ad656ce0":"def modelEvaluation(algo,dataset_type,hpt,y_actual,y_predicted):\n    #Let's check the overall Metrics\n    from sklearn import metrics\n    confusion = metrics.confusion_matrix( y_actual, y_predicted )\n    tn, fp, fn, tp = metrics.confusion_matrix( y_actual, y_predicted ).ravel()\n    specificity = tn \/ (tn+fp)    \n    print('Below is the Confusion Matrix:')\n    print(confusion)\n    \n    metrics = {'Algorithm'      : algo,\n               'Dataset Type'   : dataset_type,\n               'HPT'            : hpt,\n               'Accuracy(%)'    : round(metrics.accuracy_score(y_actual, y_predicted)*100,2)\n              }\n    return metrics","7ae05246":"def modelEvalnSummary(model,x_train,y_train,x_train_valid,y_train_valid,tuning_params=None):\n    # extracting name from class name\n    name=str(type(model)).split('.')[-1].strip(\">,'\")\n    \n    # creating a blank dataframe for holding summary\n    df=pd.DataFrame({'Algorithm'      : [],\n               'Dataset Type'   : [],\n               'HPT'            : [],\n               'Accuracy(%)'    : []\n              })\n    \n    # training Default Model \n    model.fit(x_train, y_train)\n    # evaluating train Prediction\n    y_train_pred = model.predict(x_train)\n    print(\"Evaluating Train set without tuning\")\n    metrics= modelEvaluation(name,'Train','No',y_train,y_train_pred)\n    df=df.append(metrics, ignore_index = True)\n\n\n    # evaluating test Prediction\n    y_train_valid_pred = model.predict(x_train_valid)\n    print(\"Evaluating Train Valid set without tuning\")\n    metrics= modelEvaluation(name,'Train Valid','No',y_train_valid,y_train_valid_pred)\n    df=df.append(metrics, ignore_index = True)\n        \n    # Hyper Parameter Tuning\n    if(tuning_params is not None):\n        grid_search = GridSearchCV(estimator=model, param_grid=tuning_params, \n                              cv=4, n_jobs=-1, verbose=1, scoring = \"accuracy\")\n\n        # training With GridsearchCV \n        grid_search.fit(x_train,y_train)\n\n        print(grid_search.best_params_)\n        \n        #Re running algorithm with best Params\n        dt_best = grid_search.best_estimator_\n\n        # evaluating train Prediction\n        y_train_pred = dt_best.predict(x_train)\n        print(\"Evaluating train set with tuning\")\n        metrics= modelEvaluation(name,'Train','Yes',y_train,y_train_pred)\n        df=df.append(metrics, ignore_index = True)\n\n        # evaluating test Prediction\n        y_train_valid_pred = dt_best.predict(x_train_valid)\n        print(\"Evaluating Train Valid with tuning\")\n        metrics= modelEvaluation(name,'Train Valid','Yes',y_train_valid,y_train_valid_pred)\n        df=df.append(metrics, ignore_index = True)\n    return (df)\n    ","327cf2d1":"from sklearn.linear_model import LogisticRegression","7cfc33c0":"lr_model = LogisticRegression(random_state = 100)","7aa22247":"lr_model.fit(X_train,y_train)","9e888026":"y_train_pred = lr_model.predict(X_train)\ny_train_pred_proba = lr_model.predict_proba(X_train)","375d934e":"summary=modelEvalnSummary(\n    lr_model,\n    X_train, \n    y_train,\n    X_train_valid,\n    y_train_valid\n)\nsummary","631901f2":"df_model_summary=df_model_summary.append(summary,ignore_index = True)\ndf_model_summary","fc46f210":"# list of alphas to tune - if value too high it will lead to underfitting, if it is too low, \n# it will not handle the overfitting\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\nridge_clsf = RidgeClassifier()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge_clsf, \n                        param_grid = params, \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)\nmodel_cv.fit(X_train, y_train) ","53e38f4d":"# Printing the best hyperparameter alpha\nprint(model_cv.best_params_)","918af265":"#Fitting Ridge model for alpha = 10 and printing coefficients which have been penalised\nalpha = 2.0\nridge = RidgeClassifier(alpha=alpha)\n\nridge.fit(X_train, y_train)\nprint(ridge.coef_)","bd65bf3a":"y_pred_train = ridge.predict(X_train)\ny_pred_train_valid = ridge.predict(X_train_valid)","f06be1b4":"accuracy_train_lr = accuracy_score(y_train, y_pred_train)\naccuracy_train_lr = accuracy_train_lr*100\n\nprint(accuracy_train_lr)","741f4e9a":"summary  = ['LogisticRegression','Train','Yes',accuracy_train_lr]\nsummary","70359e03":"df_model_summary.loc[2] = summary\ndf_model_summary","2b4e8600":"accuracy_train_valid_lr = accuracy_score(y_train_valid, y_pred_train_valid)\naccuracy_train_valid_lr = accuracy_train_valid_lr *100\nprint(accuracy_train_valid_lr)","a012fbbc":"summary_valid  = ['LogisticRegression','Train Valid','Yes',accuracy_train_valid_lr]\nsummary_valid","e6283c84":"df_model_summary.loc[3] = summary_valid\ndf_model_summary","fc4269ab":"dt_model = DecisionTreeClassifier(random_state = 100)","58369280":"# Create the parameter grid based on the results of random search \nparams = {\n    'max_depth': [10,20,30,40,50,60,70,80,90,100],\n    'min_samples_leaf': [10, 15,20,25,30,40],\n    'criterion': [\"gini\", \"entropy\"]\n}","c4e5ebf0":"summary=modelEvalnSummary(\n    dt_model,\n    X_train, \n    y_train,\n    X_train_valid,\n    y_train_valid,\n    params\n)\nsummary","cfb5a69a":"df_model_summary=df_model_summary.append(summary,ignore_index = True)\ndf_model_summary","469e1593":"rf_model = RandomForestClassifier(random_state = 100)","47ced1b9":"# Create the parameter grid based on the results of random search \nparams = {\n    'max_depth': [5,10,15,20,30,40],\n    'min_samples_leaf': [10,15,20,25,30,50],\n    'max_features': [2,4,5,6,7,8],\n    'n_estimators': [10,15,20,30,40,50,60]\n}","c736f901":"summary=modelEvalnSummary(\n    rf_model,\n    X_train, \n    y_train,\n    X_train_valid,\n    y_train_valid,\n    params\n)\nsummary","b033248a":"df_model_summary=df_model_summary.append(summary,ignore_index = True)\ndf_model_summary","3a1a68eb":"import pandas as pd\nfeature_importances = pd.DataFrame(rf_model.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)","995232eb":"feature_importances.sort_values(by = 'importance',ascending = False)","4f21d5ef":"ada_boost_model = AdaBoostClassifier(random_state = 100)","4a2fc420":"# Create the parameter grid based on the results of random search \n\nparams = {\n    'n_estimators':[10,20,30,40,50,60], \n    'base_estimator':[\n        None,\n        LogisticRegression()\n    ],\n    'learning_rate':[0.001,0.01,0.1]\n}","26013e04":"summary=modelEvalnSummary(\n    ada_boost_model,\n    X_train, \n    y_train,\n    X_train_valid,\n    y_train_valid,\n    params\n)\nsummary","4ed81be7":"df_model_summary=df_model_summary.append(summary,ignore_index = True)\ndf_model_summary","0dce9f25":"xgb_cfl = xgb.XGBClassifier(n_jobs = -1,objective = 'binary:logistic')\nxgb_cfl.get_params()","15e78212":"params={\n        'n_estimators' : [90,100], # no of trees \n        'learning_rate' : [0.01,0.1],  \n        'min_child_weight': [5,15],\n        'gamma': [0.1],\n        'subsample': [1.0],\n        'colsample_bytree': [1.0],\n        'max_depth': [5,10]\n        }","f820f3ea":"summary=modelEvalnSummary(\n    xgb_cfl,\n    X_train, \n    y_train,\n    X_train_valid,\n    y_train_valid,\n    params\n)\nsummary","a959150b":"df_model_summary=df_model_summary.append(summary,ignore_index = True)\ndf_model_summary","fc6f08e9":"xgb_cfl.feature_importances_.shape","697ca4bc":"from xgboost import plot_importance\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = (20, 35)\nplot_importance(xgb_cfl)","9ddcb174":"xgboost_best = xgb.XGBClassifier(n_jobs = -1\n                                 ,objective = 'binary:logistic'\n                                 ,colsample_bytree = 1.0\n                                  ,gamma = 0.1\n                                  ,learning_rate = 0.1\n                                  ,max_depth = 5\n                                  ,min_child_weight = 5\n                                  ,n_estimators = 90\n                                  ,subsample = 1.0)","18ca285f":"xgboost_best.fit(X_train,y_train)","24659e6d":"y_test_pred = xgboost_best.predict(X_test)","08d22ddc":"X_test['Survived'] = y_test_pred","06a02b59":"X_test.Survived.value_counts()","3fd2eb85":"### We can create derived columns by combing SibSp+Parch = Family Count and then remove the original 2 columns","ca6d6bb6":"# Index:\n\n    1) Data Reading\/Understanding\n    2) Data Cleaning & Handling Missing data or Missing Value Imputation\n    3) Outlier Detection\n    4) Derived Columns\n    5) EDA\n    6) Data Preparation\n        1) Split into Train & Test\n        2) Standarizing      \n    7) Data Modeling & Evaluation\n        1) Logistic Regression\n        2) Decision Tree\n        3) Random Forest\n        4) ADA Boost\n        5) XGBoost\n    8) Model Evaluation For Feature Importance","29db9298":"# Observation:\n    > Here we can see that our XGBoost Model with best parameters were able to identify 148 passengers in the unseen dataset as survived","dfed4aa2":"## Let's impute Median Age in place of null values","070951b4":"# 7.3) Random Forest","c8d562f2":"## Applying the best Found parameters from cross validation using XGBoost to predict the Test(Unseen) Dataset","fee8d248":"## Here ticket number does not give any extra information about the passenger, so let's drop the column","af61c931":"# 6) Data Preparation","15e78a69":"### We can see that there are many features with null values in it, we would have to treat those null values\n### In Total we have 891 passenger details","5ef7b01b":"## Now we have a cleaned Train & Test dataset with no null values in the observations","c8fc79b3":"# 6.3) Splitting Train Dataset into Training and Validation dataset","31b5f285":"# Observation:\n    > Here we can observe that Survival of Males from the Port of Southampton is the highest than any other port","35176760":"# Observation:\n    > We can observe that Male Adult has the highest number of Survivals, follwed by Adult Females\n    > Sadly we can also see that Senior Citizen Surival is the least among all age groups","5ff4bbee":"## Age","dea4c7b8":"# 6.1) Create Dummy Variables","fcbbd7b4":"## Embarked : Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton","81376a71":"# 7.4) ADA Boost","66cdca60":"## Hyperparameter Tunning Using RidgeClassifer","dea9ba48":"## Fitting Logistic Regression Model on Training dataset","6885a02b":"# 5) EDA","6c963819":"# Observation:\n\n    > Looking at the above results we can see that rate of survial in 1st Class higher\n    > Rate of death in 3rd class is higher","08fafe29":"# 3) Derived Column","8e28e2a8":"## As we have 77% of cabin data missing, and also the ones which has data is not stored properly, let's drop the column","d5e0d99b":"# Observation:\n\n    > Here we can observe that Adult who were Alone Survived the most\n    > 2 Children who were Alone in the Ship of Age 11(Male) & 5(Female), unfortunately only 5 year old Female Survived\n    > 15 Senior Citizen, couldn't survive the Titanic, this may be due to sudden stampede happening after the Ship started Sinking\n    > 12 Teen also couldn't survive the Titanic","62e8cdd1":"# 6.2) Min-Max Scaler","aa614649":"# 7) Classification Models","ad69927c":"## Ticket","4817378f":"### Creating a new Dataframe X_test for storing the Unseen Data","e8694edb":"# 7.2) Decision Tree With & Without Hyperparameter Tunning","8de0b3c1":"## Fare","15b51334":"# 7.1) Logistic Regression","e199d0a8":"# 7.5) XGBoost","81e7bebe":"# 2) Data Cleaning & Handling Missing Values\n\n## Train & Test Dataset","bbb0812d":"### Let's look at the survival ratio","b77d1419":"## We can also remove Unique Id Columns like PassengerID and Name which does not helps in predicting \n","6c3c98d7":"### We also can create an IsAlone column to identify passengers who were travelling alone in Titanic","eb618c57":"# Bivariate Analysis","ec92843a":"## Univariate Analysis","76285dde":"# Summary:\n\n    > After applying multiple Algorithms, we are able to get maximum accuracy with XGBoost Algorithm\n    > So we have our Final Model as XGBoost with 86% Accuracy to predict if passenger will survived or not\n    > After finding feature importance from XGBoost Model, below are the top 3 features to identify if passenger would survive:\n        1) Fare\n        2) Family Count\n        3) Embarked (Port) and Gender [Both has same Score in feature importance]","b441cc04":"# Observation:\n\n    > The Rate of Survival in Female is higher than Male","36aceb79":"# Observation:\n\n    > Here we can see that passengers who were Not Alone in the Ship has higher Survival Rate","2e096a1c":"# 1) Data Reading & Understanding","33265f48":"## Let's impute median value in Null values in fare Column","f0bf7ff8":"### Creating a Age Group Column to categorize passengers into Teens, Adults, Senior Citizens","3ddc1dee":"### As we have Age_Group column which is a categorized column, we can remove Age column from the dataset","7a853882":"## Imputing the Mode(Most common port of embarkment) i.e. S= Southampton","44a5c271":"### We can see that in the train dataset we have 38.4% passengers who have survived out of 891 pasengers\n\n### Here we have a balanced dataset","af765d2c":"## Cabin","01ff8a7c":"# Observation:\n    > Here we can observe that Female has paid Higher Average Fare for the trip in Titanic then Men","793d4dd9":"# Observation:\n    > Looking at the above results we can figure out that Ratio of Male passengers being Alone in the Titanic is higher"}}