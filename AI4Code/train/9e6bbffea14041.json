{"cell_type":{"1929437d":"code","a03b8e86":"code","e2c00523":"code","e62c2dd6":"code","d5d1a8f6":"code","f59c034b":"code","3b8926e8":"code","8b01a8c2":"code","49860ce4":"code","95fc2bb7":"code","ed187058":"code","9fd6c778":"code","54492ca5":"code","a8fafc9b":"code","acf9abce":"code","ca7fa3f0":"code","25eb820d":"code","5cb83236":"code","b2fdf129":"code","a265d92a":"code","8470d4d8":"code","cce1ee1f":"code","caafbbc3":"code","2824b7d3":"code","3dde8646":"code","5ac1e566":"code","2d35ca0d":"code","3c0ddd42":"code","93f7bf05":"code","496c5b8e":"code","ad62653f":"code","638f1d7d":"code","d854f986":"code","fa3f65a0":"code","87f726bf":"code","eb51068f":"code","e11abe86":"code","676fb5ea":"code","2178c4ee":"code","27cb21dc":"code","b398213f":"code","43201290":"code","e72ac778":"code","8b95e525":"code","433786cf":"code","d020c595":"code","ac73d114":"code","00467722":"code","d940c6c6":"code","4b7e9324":"code","d0e93d57":"markdown","a8f95695":"markdown","a1336040":"markdown","fd3b5f07":"markdown","e6c7e246":"markdown","2b253917":"markdown","e3ffcf3d":"markdown","241fcfff":"markdown","fc8358c1":"markdown","1b1a4403":"markdown","6b237a63":"markdown","10e4c43c":"markdown","8d949197":"markdown","0eeb2dff":"markdown","a3640aef":"markdown","0fc23f44":"markdown","9e3d8820":"markdown","3cfb7ee1":"markdown","5a91ce1e":"markdown","a01f1450":"markdown","f114f734":"markdown","b0aab489":"markdown","2508b8d9":"markdown","26ad48a5":"markdown","be47786c":"markdown","86d23335":"markdown","5dac4b33":"markdown","8f0f32e6":"markdown","a49b034a":"markdown","21cabb8b":"markdown","885e1e03":"markdown","9df8bf03":"markdown","f1fbf0f0":"markdown","a42ee221":"markdown","a155796d":"markdown","19ca6056":"markdown","e5ebc8d6":"markdown","daba0fbf":"markdown","2975c814":"markdown","746f3291":"markdown"},"source":{"1929437d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nimport statsmodels.api as sm\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score\n\ndef adj_r2_score(r2, n, k):\n    return 1-((1-r2)*((n-1)\/(n-k-1)))\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport keras.backend as K\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.models import load_model","a03b8e86":"uber_raw_apr14 = pd.read_csv(\"..\/input\/uber-raw-data-apr14.csv\")\nuber_raw_may14 = pd.read_csv(\"..\/input\/uber-raw-data-may14.csv\")\nuber_raw_jun14 = pd.read_csv(\"..\/input\/uber-raw-data-jun14.csv\")\nuber_raw_jul14 = pd.read_csv(\"..\/input\/uber-raw-data-jul14.csv\")\nuber_raw_aug14 = pd.read_csv(\"..\/input\/uber-raw-data-aug14.csv\")\nuber_raw_sep14 = pd.read_csv(\"..\/input\/uber-raw-data-sep14.csv\")\n\n\n#Combining dataset of 6 months into 1 dataset\nuber_2014 = [uber_raw_apr14, uber_raw_may14, uber_raw_jun14, uber_raw_jul14,uber_raw_aug14, uber_raw_sep14]\nuber_data_2014 = pd.concat(uber_2014,axis=0,ignore_index=True)\nuber_data_2014.head()","e2c00523":"uber_data_2014.info()","e62c2dd6":"uber_data_2014.Timestamp = pd.to_datetime(uber_data_2014['Date\/Time'],format='%m\/%d\/%Y %H:%M:%S') \nuber_data_2014['Date_only'] = uber_data_2014.Timestamp.dt.date\nuber_data_2014['Date'] = uber_data_2014.Timestamp\nuber_data_2014['Month'] = uber_data_2014.Timestamp.dt.month\nuber_data_2014['DayOfWeekNum'] = uber_data_2014.Timestamp.dt.dayofweek\nuber_data_2014['DayOfWeek'] = uber_data_2014.Timestamp.dt.weekday_name\nuber_data_2014['MonthDayNum'] = uber_data_2014.Timestamp.dt.day\nuber_data_2014['HourOfDay'] = uber_data_2014.Timestamp.dt.hour\n\nuber_data_2014= uber_data_2014.drop(columns = ['Lat','Lon'])\nuber_data_2014.tail()","d5d1a8f6":"uber_data_2014.groupby(pd.Grouper(key='DayOfWeek')).count()\n\nuber_weekdays = uber_data_2014.pivot_table(index=['DayOfWeekNum','DayOfWeek'],\n                                  values='Base',\n                                  aggfunc='count')\nuber_weekdays.plot(kind='bar', figsize=(15,8))\nplt.ylabel('Total Journeys')\nplt.xlabel('Day')\nplt.title('Journeys by Week Day');","f59c034b":"uber_hour = uber_data_2014.pivot_table(index=['HourOfDay'],\n                                  values='Base',\n                                  aggfunc='count')\nuber_hour.plot(kind='bar', figsize=(8,6))\nplt.ylabel('Total Journeys')\nplt.title('Journeys by Hour');","3b8926e8":"uber_data_2014.groupby(pd.Grouper(key='Base')).count()\n\nuber_monthdays = uber_data_2014.pivot_table(index=['Base'], values='Date' ,\n                                  aggfunc='count')\nuber_monthdays.plot(kind='bar', figsize=(8,6))\nplt.ylabel('Total Journeys')\nplt.title('Journeys by Month Day');","8b01a8c2":"uber_data_2014= uber_data_2014.drop(columns = ['Month','DayOfWeekNum','Base', 'DayOfWeek', 'MonthDayNum', 'HourOfDay'])\n#uber_data_2014.tail()","49860ce4":"'''\nThe df uber_count is the grouping of the above dataset on hourly basis with time stamp of both date and time.\nThis df is used mostly for ANN analysis.\n'''\nuber_count=uber_data_2014.groupby(pd.Grouper(key='Date')).count()\nuber_count= uber_count.drop(columns = ['Date_only'])\nprint(uber_count.info())\n\ntrain = uber_count[:][:234083]             #90% of 260093\ntest = uber_count[:][234084:]\ndisplay(train.tail())\ntest.head()\n","95fc2bb7":"train['Date\/Time'].plot(kind='line',figsize=(15,8), title= 'Hourly Ridership', fontsize=14)\ntest['Date\/Time'].plot(figsize=(15,5), title= 'Hourly Ridership', fontsize=14)\nplt.ylabel('Total Journeys')\nplt.xlabel('Month')\nplt.show()","ed187058":"'''\nThe df uber_dates is the grouping of the above dataset on daily basis with time stamp of onlu date.\nThis df is used to for univariate Time Series Forecasting.\n'''\nuber_dates=uber_data_2014.groupby(pd.Grouper(key='Date_only')).count()\nuber_dates= uber_dates.drop(columns = ['Date'])\nprint(uber_dates.info())\nuber_dates_d= uber_dates.drop(columns = ['Date\/Time'])\n\ntrain_ts = uber_dates[:][:163]                     #split is 90-10\ntest_ts = uber_dates[:][164:]\ntest_ts_d = uber_dates_d[:][164:]\ntest_ts.head()","9fd6c778":"train_ts['Date\/Time'].plot(kind='line',figsize=(15,8), title= 'Daily Ridership', fontsize=14)\ntest_ts['Date\/Time'].plot(figsize=(15,5), title= 'Daily Ridership', fontsize=14)\nplt.ylabel('Total Journeys')\nplt.xlabel('Month')\nplt.show()","54492ca5":"y_hat_avg = test_ts.copy()\nfit1 = ExponentialSmoothing(np.asarray(train_ts['Date\/Time'].astype(float)) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()\ny_hat_avg['Holt_Winter'] = fit1.forecast(len(test_ts))\nplt.figure(figsize=(15,5))\nplt.plot( train_ts['Date\/Time'], label='Train')\nplt.plot(test_ts['Date\/Time'], label='Test')\nplt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')\nplt.legend(loc='best')\nplt.ylabel('Total Journeys')\nplt.xlabel('Months')\nplt.show()","a8fafc9b":"rmse = sqrt(mean_squared_error(test_ts['Date\/Time'], y_hat_avg['Holt_Winter']))\nrmse","acf9abce":"y_hat_avg = test_ts.copy()\nfit1 = sm.tsa.statespace.SARIMAX(train_ts['Date\/Time'], order=(2, 1, 4),seasonal_order=(1,1,1,7)).fit()\ny_hat_avg['SARIMA'] = fit1.predict(start=\"2014-09-11\", end=\"2014-09-30\", dynamic=True)\nplt.figure(figsize=(15,6))\nplt.plot( train_ts['Date\/Time'], label='Train')\nplt.plot(test_ts['Date\/Time'], label='Test')\nplt.plot(y_hat_avg['SARIMA'], label='SARIMA')\nplt.legend(loc='best')\nplt.ylabel('Total Journeys')\nplt.xlabel('Months')\nplt.show()\n","ca7fa3f0":"rms = sqrt(mean_squared_error(test_ts['Date\/Time'], y_hat_avg.SARIMA))\nprint(rms)     ","25eb820d":"plt.style.use('default')\nplt.figure(figsize = (16,8))\nimport statsmodels.api as sm\nsm.tsa.seasonal_decompose(train_ts['Date\/Time'].values,freq=30).plot()\nresult = sm.tsa.stattools.adfuller(uber_dates['Date\/Time'])\nplt.show()","5cb83236":"y_hat_avg = test_ts.copy()\n\nfit1 = Holt(np.asarray(train_ts['Date\/Time']).astype(float)).fit(smoothing_level = 0.3,smoothing_slope = 0.1)\ny_hat_avg['Holt_linear'] = fit1.forecast(len(test_ts))\n\nplt.figure(figsize=(16,5))\nplt.plot(train_ts['Date\/Time'], label='Train')\nplt.plot(test_ts['Date\/Time'], label='Test')\nplt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')\nplt.legend(loc='best')\nplt.show()","b2fdf129":"rms = sqrt(mean_squared_error(test_ts['Date\/Time'], y_hat_avg.Holt_linear))\nprint(rms)     ","a265d92a":"from statsmodels.tsa.stattools import adfuller\ndef test_stationary(timeseries):\n    #Determine rolling statistics\n    #rolmean = pd.rolling_mean(timeseries,window = 24)\n    #rolstd = pd.rolling_std(timeseries, window = 24)\n    \n    rolmean = timeseries.rolling(24).mean()\n    rolstd = timeseries.rolling(24).std()\n    \n    \n    #Plot rolling Statistics\n    orig = plt.plot(timeseries, color = \"blue\", label = \"Original\")\n    mean = plt.plot(rolmean, color = \"red\", label = \"Rolling Mean\")\n    std = plt.plot(rolstd, color = \"black\", label = \"Rolling Std\")\n    plt.legend(loc = \"best\")\n    plt.title(\"Rolling Mean and Standard Deviation\")\n    plt.show(block = False)\n    \n    #Perform Dickey Fuller test\n    print(\"Results of Dickey Fuller test: \")\n    dftest = adfuller(timeseries, autolag = 'AIC')\n    dfoutput = pd.Series(dftest[0:4], index = ['Test Statistics', 'p-value', '# Lag Used', 'Number of Observations Used'])\n    \n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)' %key] = value\n    print(dfoutput)","8470d4d8":"from matplotlib.pylab import rcParams\nrcParams['figure.figsize']=(20,10)\ntest_stationary(uber_count['Date\/Time'])","cce1ee1f":"Train_log = np.log(train_ts['Date\/Time'])\nvalid_log = np.log(test_ts['Date\/Time'])","caafbbc3":"moving_avg = Train_log.rolling(24).mean()\nplt.plot(Train_log)\nplt.plot(moving_avg, color = 'red')","2824b7d3":"train_log_moving_diff = Train_log - moving_avg\ntrain_log_moving_diff.dropna(inplace = True)\ntest_stationary(train_log_moving_diff)","3dde8646":"train_log_diff = Train_log - Train_log.shift(1)\ntest_stationary(train_log_diff.dropna())","5ac1e566":"from statsmodels.tsa.seasonal import seasonal_decompose\nplt.figure(figsize = (16,10))\ndecomposition = seasonal_decompose(pd.DataFrame(Train_log)['Date\/Time'].values, freq = 24)\nplt.style.use('default')\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(Train_log, label = 'Original')\nplt.legend(loc = 'best')\nplt.subplot(412)\nplt.plot(trend, label = 'Trend')\nplt.legend(loc = 'best')\nplt.subplot(413)\nplt.plot(seasonal, label = 'Seasonal')\nplt.legend(loc = 'best')\nplt.subplot(414)\nplt.plot(residual, label = 'Residuals')\nplt.legend(loc = 'best')\nplt.tight_layout()","2d35ca0d":"plt.figure(figsize = (16,8))\ntrain_log_decompose = pd.DataFrame(residual)\ntrain_log_decompose['date'] = Train_log.index\ntrain_log_decompose.set_index('date', inplace = True)\ntrain_log_decompose.dropna(inplace = True)\ntest_stationary(train_log_decompose[0])","3c0ddd42":"from statsmodels.tsa.stattools import acf, pacf\n\nlag_acf = acf(train_log_diff.dropna(), nlags = 25)\nlag_pacf = pacf(train_log_diff.dropna(), nlags = 25, method= \"ols\")","93f7bf05":"plt.figure(figsize = (15,8))\nplt.style.use(\"fivethirtyeight\")\nplt.plot(lag_acf)\nplt.axhline( y = 0, linestyle = \"--\", color = \"gray\")\nplt.axhline( y= -1.96\/np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.axhline(y = 1.96 \/np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.title(\"Autocorrelation Function\")\nplt.show()\n# PACF\nplt.figure(figsize = (15,8))\nplt.plot(lag_pacf)\nplt.axhline(y = 0, linestyle = \"--\", color = \"gray\")\nplt.axhline(y = -1.96\/np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.axhline( y = 1.96\/np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.title(\"Partial Autocorrelation Function\")\nplt.show()","496c5b8e":"from statsmodels.tsa.arima_model import ARIMA\nplt.figure(figsize = (15,8))\nmodel = ARIMA(Train_log, order = (2,1,0))  #here q value is zero since it is just AR Model\nresults_AR = model.fit(disp=-1)\nplt.plot(train_log_diff.dropna(), label = \"Original\")\nplt.plot(results_AR.fittedvalues, color = 'red', label = 'Predictions')\nplt.legend(loc = 'best')","ad62653f":"AR_predict = results_AR.predict(start=\"2014-09-11\", end=\"2014-09-30\")\nAR_predict = AR_predict.cumsum().shift().fillna(0)\nAR_predict1 = pd.Series(np.ones(test_ts.shape[0])* np.log(test_ts['Date\/Time'])[0], index = test_ts_d)\nAR_predict = np.exp(AR_predict1)","638f1d7d":"# Moving Average Model","d854f986":"plt.figure(figsize = (15,8))\nmodel = ARIMA(Train_log, order = (0,1,2)) # here the p value is 0 since it is moving average model\nresults_MA = model.fit(disp = -1)\nplt.plot(train_log_diff.dropna(), label = \"Original\")\nplt.plot(results_MA.fittedvalues, color = \"red\", label = \"Prediction\")\nplt.legend(loc = \"best\")","fa3f65a0":"MA_predict = results_MA.predict(start=\"2014-09-11\", end=\"2014-09-30\")\nMA_predict=MA_predict.cumsum().shift().fillna(0)\nMA_predict1=pd.Series(np.ones(test_ts.shape[0]) * np.log(test_ts['Date\/Time'])[0], index = test_ts_d)\n#MA_predict1=MA_predict1.add(MA_predict,fill_value=0)\nMA_predict = np.exp(MA_predict1)","87f726bf":"# Combined Model","eb51068f":"plt.figure(figsize = (16,8))\nmodel = ARIMA(Train_log, order=(2, 1, 2))  \nresults_ARIMA = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(),  label='Original')\nplt.plot(results_ARIMA.fittedvalues, color='red', label='Predicted')\nplt.legend(loc='best')\nplt.show()","e11abe86":"# Function to scale model to original scale","676fb5ea":"def check_prediction_diff(predict_diff, given_set):\n    predict_diff= predict_diff.cumsum().shift().fillna(0)\n    predict_base = pd.Series(np.ones(given_set.shape[0]) * np.log(given_set['Date\/Time'])[0], index = given_set.index)\n    #predict_log = predict_base.add(predict_diff,fill_value=0)\n    predict = np.exp(predict_base)\n    \n    plt.plot(given_set['Date\/Time'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Date\/Time']))\/given_set.shape[0]))\n    plt.show()\n\ndef check_prediction_log(predict_log, given_set):\n    predict = np.exp(predict_log)\n    \n    plt.plot(given_set['Date\/Time'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Date\/Time']))\/given_set.shape[0]))\n    plt.show()\n\nARIMA_predict_diff=results_ARIMA.predict(start=\"2014-09-11\", end=\"2014-09-30\")\n\nplt.figure(figsize = (16,8))\ncheck_prediction_diff(ARIMA_predict_diff, test_ts)","2178c4ee":"ARIMA_predict_diff.shape \n\n\ntest_ts.shape","27cb21dc":"y_hat = test_ts.copy()\nfit2 = SimpleExpSmoothing(np.asarray(train_ts['Date\/Time'])).fit(smoothing_level = 0.6,optimized = False)\ny_hat['SES'] = fit2.forecast(len(test_ts))\nplt.figure(figsize =(15,8))\nplt.plot(train_ts['Date\/Time'], label = 'Train')\nplt.plot(test_ts['Date\/Time'], label = 'Validation')\nplt.plot(y_hat['SES'], label = 'Simple Exponential Smoothing')\nplt.legend(loc = 'best')","b398213f":"abc=y_hat['SES'].values.tolist()\nrmse = sqrt(mean_squared_error(test_ts['Date\/Time'],abc))\nrmse","43201290":"y_hat_avg = test_ts.copy()\ny_hat_avg['moving_average_forecast'] = train_ts['Date\/Time'].rolling(10).mean().iloc[-1]\nplt.figure(figsize = (15,5))\nplt.plot(train_ts['Date\/Time'], label = 'Train')\nplt.plot(test_ts['Date\/Time'], label = 'Validation')\nplt.plot(y_hat_avg['moving_average_forecast'], label = 'Moving Average Forecast with 10 Observations')\nplt.legend(loc = 'best')\nplt.show()\ny_hat_avg = test_ts.copy()\ny_hat_avg['moving_average_forecast'] = train_ts['Date\/Time'].rolling(20).mean().iloc[-1]\nplt.figure(figsize = (15,5))\nplt.plot(train_ts['Date\/Time'], label = 'Train')\nplt.plot(test_ts['Date\/Time'], label = 'Validation')\nplt.plot(y_hat_avg['moving_average_forecast'],label = 'Moving Average Forecast with 20 Observations')\nplt.legend(loc = 'best')\nplt.show()\ny_hat_avg = test_ts.copy()\ny_hat_avg['moving_average_forecast']= train_ts['Date\/Time'].rolling(50).mean().iloc[-1]\nplt.figure(figsize = (15,5))\nplt.plot(train_ts['Date\/Time'], label = 'Train')\nplt.plot(test_ts['Date\/Time'], label = 'Validation')\nplt.plot(y_hat_avg['moving_average_forecast'], label = \"Moving Average Forecast with 50 Observations\")\nplt.legend(loc = 'best')\nplt.show()","e72ac778":"rmse = sqrt(mean_squared_error(test_ts['Date\/Time'], y_hat_avg['moving_average_forecast']))\nrmse","8b95e525":"y_hat = test_ts.copy()\nfit2 = SimpleExpSmoothing(np.asarray(train_ts['Date\/Time'])).fit(smoothing_level = 0.6,optimized = False)\ny_hat['SES'] = fit2.forecast(len(test_ts))\nplt.figure(figsize =(15,8))\nplt.plot(train_ts['Date\/Time'], label = 'Train')\nplt.plot(test_ts['Date\/Time'], label = 'Validation')\nplt.plot(y_hat['SES'], label = 'Simple Exponential Smoothing')\nplt.legend(loc = 'best')\n","433786cf":"sc = MinMaxScaler()\ntrain_sc = sc.fit_transform(train)\ntest_sc = sc.transform(test)\n\nX_train = train_sc[:-1]\ny_train = train_sc[1:]\n\nX_test = test_sc[:-1]\ny_test = test_sc[1:]","d020c595":"K.clear_session()\n\nmodel = Sequential()\nmodel.add(Dense(9, input_dim=1, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nearly_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=1, callbacks=[early_stop], shuffle=False)","ac73d114":"y_pred_test_ann = model.predict(X_test)\ny_train_pred_ann = model.predict(X_train)\nrmse = sqrt(mean_squared_error(y_train,y_train_pred_ann))\nprint(\"Train : {:0.3f}\".format(rmse))\n\nrmse = sqrt(mean_squared_error(y_test,y_pred_test_ann))\nprint(\"Test : {:0.3f}\".format(rmse))\n\nmodel.save('Uber_ANN')","00467722":"model_ann = load_model('Uber_ANN')","d940c6c6":"y_pred_test_ANN = model_ann.predict(X_test)\nplt.plot(y_test, label='True')\nplt.plot(y_pred_test_ANN, label='ANN')\nplt.title(\"ANN's_Prediction\")\nplt.xlabel('Observation')\nplt.ylabel('INR_Scaled')\nplt.legend()\nplt.show()","4b7e9324":"score_ann= model_ann.evaluate(X_test, y_test, batch_size=1)\nprint('ANN: %f'%score_ann)","d0e93d57":"## Result\n\n#### From the abve analysis we can see that ANN gave the best result for our dataset with least RMSE value of 0.022. Followed by Holt's Winter Season and SABRIMA  with small RMSE values compared to other models used.","a8f95695":"#### Dividing the above Date\/Time columns into several columns for visualising and analysing the dataset","a1336040":"#### Differncing can help to make series stable and eliminate trend","fd3b5f07":"## Objective:\n\nIn this project, I attempted to visualize, explore and experiment various different models and ANN techniques that would effectively as well as efficiently predict the Uber NYC Pickups Demand. This is an extension of the other project that we did on Bike-sharing Demand.\n\nThis is a unique problem with huge amount of datasets but only 2 considerable classes.\n\n## Steps Involved :\n\n1. Data Preprocessing\n2. Visualizations\n3. Splitting of Dataset\n4. Training and Testing on various models\n    - Time Series Forecasting (Sarima, Arima, Holt Winter, Holt Linear) \n    - AR Model\n    - Exponential Smoothing\n    - ANN Model\n5. Results","e6c7e246":"#### Removing Seasonailty","2b253917":"### Import Libraries","e3ffcf3d":"### Holt\u2019s Linear Trend method","241fcfff":"### AR model","fc8358c1":"plt.figure(figsize = (15,8))\nplt.plot(test_ts['Date\/Time'], label = \"Valid\")\nplt.plot(MA_predict, color = 'red', label = \"Predict\")\nplt.legend(loc= 'best')\nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(MA_predict, test_ts['Date\/Time']))\/test_ts.shape[0]))\nplt.show()","1b1a4403":"rmse = sqrt(mean_squared_error(train_ts['Date\/Time'], y_hat['SES']))\nrmse","6b237a63":"#### Error Score for this model is very low, hence ANN model gives best results for the problem.","10e4c43c":"### Holt\u2019s Winter seasonal method","8d949197":"#### Base","0eeb2dff":"### ARIMA","a3640aef":"### Splitting the dataset","0fc23f44":"#### Analysing peek hours","9e3d8820":"### RMSE Error for Simple Exponential Smoothing","3cfb7ee1":"This model also gives comparitively the best accuracy compared to other Time Series Forecasting Methods. It has comparitively lower RMSE and we can see from the above plot that the predicted Holt winter graph is almost overlapping with the actual test dataset available to us","5a91ce1e":"As we have a very large dataset of 4.5 million+ values. I have used 90-10 split","a01f1450":"### Import and Preprocess Dataset","f114f734":"#### Simple Exponential Smoothing is not the right model for our dataset","b0aab489":"! pip install --upgrade Cython\n! pip install --upgrade git+https:\/\/github.com\/statsmodels\/statsmodels\nimport statsmodels.api as sm\n","2508b8d9":"From here we can see that peak hours of booking a cab are in evening from 4pm to 6pm.\nWe can also see that more cabs are booked in evenings compared to mornings","26ad48a5":"#### Checking stationarity of residuals","be47786c":"Here we can see that Base B02617 provided most cabs. Closely followed by B02598","86d23335":"### Simple Exponential Smoothing","5dac4b33":"#### The huge RMSE value in AR model shows that this model is not suitable for our dataset","8f0f32e6":"# Predicting Uber NYC Pickup Demands using different models and ANN Architectures ","a49b034a":"#### This model gives huge RMSE value too showing this model isn't good for our dataset","21cabb8b":"#### Submitted by :   Shriya Garg   |  Bennett University","885e1e03":"#### Remove Trend","9df8bf03":"## Time Series Forcasting","f1fbf0f0":"Here we can see that the larest number of uber pickups were done on Thurdays and Fridays","a42ee221":"### Visualizations","a155796d":"This model also predicts with comparable accuracy as the above holt winter season method as we can see here as well that the predicted sarima graph is almost overlapping with the actual test dataset available to us","19ca6056":"### SARIMA Model","e5ebc8d6":"# ANN\n\nIn this model only one hidden layer is chosen because it is just a two class problem with huge dataset. Adding more layers was just overfitting the model, which was not desirable.","daba0fbf":"#### Here we can see the Root Mean Square Error values are less. \nThis is the least when compared with other standard splits of 70-30, 80-20 and 95- 05","2975c814":"#### Peak Days","746f3291":"Here we can see that Holt Linear Trend is hardly a good model for our dataset."}}