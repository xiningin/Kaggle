{"cell_type":{"7c22139f":"code","e824589c":"code","e2478f5e":"code","6569d2f6":"code","d6bc217f":"code","d9712f73":"code","4b07078e":"code","b99a0d10":"code","b8429771":"code","b2f6c7fe":"code","a9e4b054":"code","6e962c2c":"code","ed93d26b":"code","1a79fc80":"code","f286f7b0":"code","0add4df9":"code","98675bb6":"code","6d0d0ce9":"code","9765a4df":"code","011d1c36":"code","eac2bd24":"code","99ea5092":"code","29b83dea":"code","311f402e":"code","dd9211fd":"code","80f98194":"code","c4206f86":"code","48a2187e":"code","349b6a46":"code","107e4a3b":"code","cd129369":"code","6cc1c32c":"code","73223df5":"code","eff3b198":"code","1b056ef9":"code","0e859143":"code","c984e185":"code","ce516090":"code","7384c977":"code","f62e2dd2":"code","bd2c8ec2":"code","381a5c53":"code","b6eaf8c4":"code","02a53f69":"code","7e87831e":"code","d9d7a2d7":"code","37eb91c9":"markdown","57c8b6f9":"markdown","4e5a1f45":"markdown","28fe6fa0":"markdown","bffc2c30":"markdown","0fbf5545":"markdown","ef032af9":"markdown","075e0fb0":"markdown","93b648ad":"markdown","f97678a0":"markdown","e8d5dfb2":"markdown","e06e0290":"markdown","a892ff0e":"markdown","cc0c9119":"markdown","4cc702e7":"markdown","77cd8873":"markdown","39590d5f":"markdown","952a488d":"markdown","0e78b9ff":"markdown","0491e504":"markdown","3e21676b":"markdown","154c595e":"markdown","a171af3a":"markdown","f25ce196":"markdown","9ac91c09":"markdown","607e78c7":"markdown","b5bccbff":"markdown","34333180":"markdown"},"source":{"7c22139f":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\n\nimport seaborn as sns\nimport missingno as msno\nimport plotly.express as px\nimport  matplotlib.pyplot as plt\n\n\nplt.style.use('seaborn')\n%matplotlib inline","e824589c":"train_df = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ntrain_df.head()","e2478f5e":"train_df.shape","6569d2f6":"train_df.info()","d6bc217f":"train_df.describe()","d9712f73":"sns.color_palette(\"Blues\", as_cmap=True)","4b07078e":"GnBu_palette = sns.color_palette(\"GnBu\",10)\nBlues_palette = sns.color_palette(\"Blues\",10)\nsns.palplot(Blues_palette)\nsns.palplot(GnBu_palette)","b99a0d10":"pd.DataFrame(train_df.isnull().sum(), columns=[\"Null Count\"]).style.background_gradient(cmap='Blues')","b8429771":"msno.matrix(df=train_df.iloc[:,:],figsize=(5,5),color=GnBu_palette[4])","b2f6c7fe":"fig, axes = plt.subplots(1,2,figsize=(10,4))\n\ntrain_df['Outcome'].value_counts().plot.pie(\n    explode=[0,0.1],autopct='%1.1f%%',ax=axes[0],shadow=True, colors=[Blues_palette[1],Blues_palette[3]]\n)\n\nsns.countplot('Outcome',data=train_df,ax=axes[1], palette=[GnBu_palette[6],GnBu_palette[7]])\naxes[1].patch.set_alpha(0)\n\nfig.text(0.28,0.92,\"Distribution of Outcome percent\", fontweight=\"bold\", fontfamily='serif', fontsize=17)\n\nplt.show()","a9e4b054":"pd.crosstab(train_df['Pregnancies'],train_df['Outcome'],margins=True).style.background_gradient(cmap='Blues')","6e962c2c":"train_col = train_df.drop(['Outcome','Pregnancies'],axis=1).columns\ni = [0, 1, 0, 1, 0, 1]\nj = [0, 0, 1, 1, 2, 2]\n\nfig, axes = plt.subplots(3,2, figsize=(20,18))\n\nfor index, col, i, j in zip(range(6),train_col, i, j):\n\n    sns.distplot(train_df[train_df['Outcome']==1][col], ax=axes[j,i], label='Outcome 1', color=Blues_palette[index+2])\n    sns.distplot(train_df[train_df['Outcome']==0][col], ax=axes[j,i], label='Outcome 0', color=GnBu_palette[index+2])\n    axes[j,i].patch.set_alpha(0)\n    axes[j,i].set_title(\"Distribution Outcome per Pregnancies\", fontweight=\"bold\", fontfamily='serif', fontsize=14)\n    axes[j,i].legend()\nplt.show()","ed93d26b":"train_col = train_df.drop(['Outcome','Pregnancies'],axis=1).columns\ni = [0, 1, 0, 1, 0, 1]\nj = [0, 0, 1, 1, 2, 2]\n\nfig, axes = plt.subplots(3,2, figsize=(10,12), constrained_layout = True )\n\nfor index, col, i, j in zip(range(6),train_col, i, j):\n\n    sns.boxplot(x=\"Outcome\", y=col,  data=train_df, ax=axes[j,i], palette=[Blues_palette[8-index],GnBu_palette[8-index]])\n\n    axes[j,i].set_title(\"Distribution Outcome per Pregnancies\", fontweight=\"bold\", fontfamily='serif', fontsize=14)\n    axes[j,i].legend()\nplt.show()\n","1a79fc80":"fig = px.scatter_3d(train_df.iloc[:500], x='BloodPressure', y='Outcome', z='Age',\n                    color='Age')\nfig.show()","f286f7b0":"train_df[train_df.columns[:8]].corr().style.background_gradient(cmap='Blues')","0add4df9":"sns.heatmap(train_df[train_df.columns[:8]].corr(),annot=True,cmap='Blues')\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","98675bb6":"df = train_df[train_df['Insulin']<=400]\ndf.head()","6d0d0ce9":"print('After Drop Insulin DataFrame Shape : ',df.shape)","9765a4df":"features = ['Glucose','BloodPressure','Insulin','BMI','Age']","011d1c36":"x = df[features]\ny = df['Outcome']","eac2bd24":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)","99ea5092":"x_train.head()","29b83dea":"y_train.head()","311f402e":"smote = SMOTE()\nx_over, y_over = smote.fit_resample(x_train,y_train)","dd9211fd":"x_over.shape","80f98194":"fig, axes = plt.subplots(1,2,figsize=(10,4))\n\ny_over.value_counts().plot.pie(\n    explode=[0,0.1],autopct='%1.1f%%',ax=axes[0],shadow=True, colors=[Blues_palette[1],Blues_palette[3]]\n)\n\nsns.countplot(y_over, ax=axes[1], palette=[GnBu_palette[6],GnBu_palette[7]])\naxes[1].patch.set_alpha(0)\n\nfig.text(0.28,0.92,\"Distribution of After  SMOTE Outcome percent\", fontweight=\"bold\", fontfamily='serif', fontsize=17)\n\nplt.show()","c4206f86":"def get_clf_eval(y_test, pred = None, pred_proba = None):\n    confusion = confusion_matrix(y_test, pred)\n    accuacy = accuracy_score(y_test, pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test, pred)\n    f1 = f1_score(y_test, pred)\n    roc_auc = roc_auc_score(y_test, pred_proba)\n    \n    print('confusion')\n    print(confusion)\n    print('accuacy : {}'.format(np.around(accuacy,4)))\n    print('precision: {}'.format(np.around(precision,4)))\n    print('recall : {}'.format(np.around(recall,4)))\n    print('F1 : {}'.format(np.around(f1,4)))  \n    print('ROC_AUC : {}'.format(np.around(roc_auc,4)))","48a2187e":"lg_reg = LogisticRegression()\n\nlg_reg.fit(x_train, y_train)\npred = lg_reg.predict(x_test)\npred_proba = lg_reg.predict_proba(x_test)[:,1]\nget_clf_eval(y_test, pred, pred_proba)","349b6a46":"rf_clf = RandomForestClassifier()\nparam = {'n_estimators' : [100],\n         'max_depth':[8,9,10],\n         'min_samples_split':[2,5,7],\n         'min_samples_leaf':[6.5,7,7.5]\n        }","107e4a3b":"grid = GridSearchCV(rf_clf,param_grid = param,scoring = 'accuracy',cv=5)\ngrid.fit(x_train ,y_train)","cd129369":"grid.best_params_","6cc1c32c":"grid.best_score_","73223df5":"pred = grid.predict(x_test)\npred_proba = grid.predict_proba(x_test)[:,1]\nget_clf_eval(y_test, pred, pred_proba)","eff3b198":"model = lgb.LGBMClassifier(\n    n_estimators=400,\n    num_leaves=20,\n    min_data_in_leaf=60,\n    learning_rate=0.01,\n    boosting='gbdt',\n    objective='binary',\n    metric='auc',\n    Is_training_metric=True,\n    n_jobs=-1\n)","1b056ef9":"model.fit(x_train,y_train)","0e859143":"pred = model.predict(x_test)\npred_proba = model.predict_proba(x_test)[:,1]\nget_clf_eval(y_test, pred, pred_proba)","c984e185":"lg_reg = LogisticRegression()\n\nlg_reg.fit(x_over, y_over)\npred = lg_reg.predict(x_test)\npred_proba = lg_reg.predict_proba(x_test)[:,1]\nget_clf_eval(y_test, pred, pred_proba)","ce516090":"rf_clf = RandomForestClassifier()\nparam = {'n_estimators' : [200],\n         'max_depth':[10],\n         'min_samples_split':[2],\n         'min_samples_leaf':[7]\n        }","7384c977":"grid = GridSearchCV(rf_clf,param_grid = param,scoring = 'accuracy',cv=5)\ngrid.fit(x_over ,y_over)\n\npred = grid.predict(x_test)\npred_proba = grid.predict_proba(x_test)[:,1]\nget_clf_eval(y_test, pred, pred_proba)","f62e2dd2":"model = lgb.LGBMClassifier(\n    n_estimators=400,\n    num_leaves=20,\n    min_data_in_leaf=60,\n    learning_rate=0.01,\n    boosting='gbdt',\n    objective='binary',\n    metric='auc',\n    Is_training_metric=True,\n    n_jobs=-1\n)","bd2c8ec2":"model.fit(x_over,y_over)","381a5c53":"pred = model.predict(x_test)\npred_proba = model.predict_proba(x_test)[:,1]\nget_clf_eval(y_test, pred, pred_proba)","b6eaf8c4":"submission = pd.DataFrame(x_test)","02a53f69":"pred = model.predict(x_test)","7e87831e":"submission['Outcome'] = y_test\nsubmission['Predict_Outcome'] = pred","d9d7a2d7":"submission.head()","37eb91c9":"### 6-3) LogisticRegression Modeling","57c8b6f9":"#### \u2714\ufe0f This notebook will use this palettes.","4e5a1f45":"## Before SMOTE ------------------------","28fe6fa0":"## 3-5) Plot the Outcome per BloodPressure \/ Age with [3d interactive Plot]","bffc2c30":"# 2. Check out my data\n* Check Shape \/ Info \/ Describe","0fbf5545":"***\n## My Workflow\n\n#### 1. Import & Install libray\n* Import Basic libray\n* Import Enginnering libray\n\n#### 2. Check out my data\n* Check Shape \/ Info \/ Describe\n\n#### 3. Exploratory Data Analysis(EDA) with Visualization [Before Preprocessing]\n* Plot the null values\n* Plot the Outecome Percent\n* Distribution of various data over Outcome column (DistPlot)\n* Distribution of various data over Outcome column (BoxPlot)\n* Plot the Outcome per BloodPressure \/ Age with [3d interactive Plot]\n* Plot the Outcome per BloodPressure \/ Age with [3d interactive Plot]\n* Diabetes data Heatmap Plot\n\n#### 4. Preprocessing Data\n* Drop more than 400 columns Insulin\n\n#### 5. Feature Enginnering\n* Get useful columns\n* OverSampling [SMOTE]\n\n#### 6. Modeling\n* Evaluation function definition\n\n###### Before SMOTE \n* LogisticRegression Modeling\n* RandomForest Modeling\n* LightGBM Modeling\n\n###### After SMOTE\n* LogisticRegression Modeling\n* RandomForest Modeling\n* LightGBM Modeling\n\n#### 7. Submission\n* Submit the predictions\n<br\/><br\/>\n***","ef032af9":"### 3-6) Diabetes data Heatmap Plot","075e0fb0":"# 7. Submission\n* Submit the predictions","93b648ad":"# 1. Import & Install libray\n* Import Basic libray\n* Import Enginnering libray","f97678a0":"### 3-2) Plot the Outecome Percent","e8d5dfb2":"# 3. Exploratory Data Analysis(EDA) with Visualization [Before Preprocessing]\n* Plot the null values\n* Plot the Outecome Percent\n* Distribution of various data over Outcome column (DistPlot)\n* Distribution of various data over Outcome column (BoxPlot)\n* Plot the Outcome per BloodPressure \/ Age with [3d interactive Plot]\n* Plot the Outcome per BloodPressure \/ Age with [3d interactive Plot]\n* Diabetes data Heatmap Plot","e06e0290":"### 6-2) LightGBM Classification Modeling","a892ff0e":"![1*jL4-fTWHjEsXxG0Yi75dlg.png](attachment:c3fb9f54-0f9d-4693-b3af-3a50a27ab2b9.png)\n\n### There are two ways of dealing with data imbalances.\n* 1) Increase the number of small amounts of data. (Oversampling)\n* 2) It can reduce the number of data in large quantities. (Undersampling)\n\nSMOTE is Oversampling technique belonging to one of these. It's creates and amplifies new data through relationships between data.\n\nMore information can be found in this document => https:\/\/arxiv.org\/pdf\/1106.1813.pdf","cc0c9119":"### 3-1) Plot the null values","4cc702e7":"### 6-3) RandomForest Modeling","77cd8873":"### 6-2) LogisticRegression Modeling","39590d5f":"# 6. Modeling\n* Evaluation function definition\n\n#### Before SMOTE \n* LogisticRegression Modeling\n* RandomForest Modeling\n* LightGBM Modeling\n\n#### After SMOTE\n* LogisticRegression Modeling\n* RandomForest Modeling\n* LightGBM Modeling","952a488d":"### 3-3) Distribution of various data over Outcome column (DistPlot)","0e78b9ff":"### 3-4) Distribution of various data over Outcome column (BoxPlot)","0491e504":"### 6-2) RandomForest Modeling","3e21676b":"### 6-3) LightGBM Classification Modeling","154c595e":"### 6-1) Evaluation function definition","a171af3a":"# 4. Preprocessing Data\n* Drop more than 400 columns Insulin","f25ce196":"###  If this notebook is useful for your kaggling, \"UPVOTE\" for it \ud83d\udc40\n#### THX to Reading My Notebook\ud83c\udf08","9ac91c09":"# Pima Indians Diabetes Database\n\n## Overview\nThis Notebook will be completed in two main ways.\nFirst, find and visualize useful data or meaningful relationships within the data.\nSecond, select a model based on the visualization of the previous process. Transform or refine the data into the appropriate form for the model to be used.\n<br\/><br\/>\n\nIt is a DataSet that can predict diabetes among Pima Indians. We can understand relationships through data through various insights.So I think it's important to visualize and understand this. Since the size of the data is not large, beginners will be able to easily follow it.\n\n\n##### \"We need to address the imbalance in \"Outcome\" data in a variety of ways.\"\n<br\/><br\/>\n\n#### My opinion :\n* 1) In this Notebook, the imbalance in \"Outcome\" data was solved by a technique called SMOTE.\n* 2) If the above problem is solved, it should be modeled to produce good results using various models.","607e78c7":"## After SMOTE ------------------------","b5bccbff":"# 5. Feature Enginnering\n* Get useful columns\n* OverSampling [SMOTE]","34333180":"![1713011111.jpeg](attachment:3a74b37d-c333-4f74-8155-af0453bdb41c.jpeg)"}}