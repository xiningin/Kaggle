{"cell_type":{"a62a7e90":"code","358407d1":"code","1b096750":"code","740216f7":"code","f2b2eb17":"code","c958b428":"code","338aa5e7":"code","1bb17fb4":"code","8f7eecaa":"code","2d6c1971":"code","e70c9671":"code","8d6b9891":"code","3fb56fee":"code","4e3606f3":"code","c2e08dce":"code","74ac62e1":"code","c3fddfd3":"code","91f0584d":"markdown","2a7ad137":"markdown","042f34bb":"markdown","03d62fbd":"markdown","33b4dd9f":"markdown","db8592d9":"markdown","47a00cf0":"markdown","79e85d45":"markdown","2c016b14":"markdown","3ab2825a":"markdown","6e88df38":"markdown","47e33821":"markdown","2d7298e2":"markdown","af702f66":"markdown","dc113389":"markdown","cb8eef9b":"markdown"},"source":{"a62a7e90":"# directory checking\n\n%cd \/kaggle\/working","358407d1":"# repo clone\n\n!git clone https:\/\/github.com\/benjs\/nfnets_pytorch","1b096750":"import torch\n\nif torch.cuda.is_available():\n    torch.backends.cudnn.deterministic = True\n\nprint(torch.cuda.is_available())\n\n!nvidia-smi # checking gpu","740216f7":"%cd .\/nfnets_pytorch\n\nimport argparse\nimport math\nimport PIL\nimport time\nimport yaml\nimport numpy as np\nimport random\nimport copy\n\nfrom pathlib import Path\nfrom PIL.Image import Image\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\n\nimport torch.cuda.amp as amp\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Subset\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision import transforms\n\nfrom nfnets import NFNet, SGD_AGC, pretrained_nfnet\n\nfrom sklearn.model_selection import train_test_split","f2b2eb17":"batch_size = 64\n\ndata_path = '\/kaggle\/input\/fruits\/fruits-360\/Training'\n\nfruit_datasets = datasets.ImageFolder(\n    data_path,\n    transforms.Compose([\n        transforms.Resize((192, 192)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n)","c958b428":"random_seed = 33\nrandom.seed(random_seed)\ntorch.manual_seed(random_seed)\n\ndataset = {}\n\n# train-validation split\ntrain_idx, valtest_idx = train_test_split(list(range(len(fruit_datasets))), \n                                          test_size=0.4, \n                                          random_state=random_seed)\n\ndataset['train'] = Subset(fruit_datasets, train_idx)\nvaltest          = Subset(fruit_datasets, valtest_idx)\n\n\n# validation-test split\nval_idx, test_idx = train_test_split(list(range(len(valtest))), \n                                     test_size=0.5, \n                                     random_state=random_seed)\n\ndataset['valid'] = Subset(valtest, val_idx)\ndataset['test']  = Subset(valtest, test_idx)\n\nmax_trn_corrects = len(dataset['train'])\nmax_val_corrects = len(dataset['valid'])","338aa5e7":"dataloaders, batch_num = {}, {}\n\ndataloaders['train'] = DataLoader(dataset['train'],\n                                  batch_size=batch_size, shuffle=True,\n                                  num_workers=8)\ndataloaders['valid'] = DataLoader(dataset['valid'],\n                                  batch_size=batch_size, shuffle=True,\n                                  num_workers=8)\ndataloaders['test'] = DataLoader(dataset['test'],\n                                  batch_size=batch_size, shuffle=True,\n                                  num_workers=8)\n\nbatch_num['train'], batch_num['valid'], batch_num['test'] = len(dataloaders['train']), len(dataloaders['valid']), len(dataloaders['test'])\n\n\nprint('batch_size : {}\\ntrain\/valid\/test : {}\/{}\/{}'\n      .format(batch_size, \n              batch_num['train'], batch_num['valid'], batch_num['test']))","1bb17fb4":"imgtest = None\nfor images, labels in dataloaders['train']:  \n    imgtest = images[3]\n    print(imgtest.shape)\n    break\n\nimgtest = imgtest.numpy()\nimgtest = np.moveaxis(imgtest, 0, -1)\nimshow(imgtest)","8f7eecaa":"model = NFNet(\n    num_classes=1000, \n    variant='F0', \n    stochdepth_rate=0.25, \n    alpha=0.2,\n    se_ratio=0.5,\n    activation='gelu'\n)\n\noptimizer = SGD_AGC(\n    named_params=model.named_parameters(), \n    lr=0.1,\n    momentum=0.9,\n    clipping=0.1,\n    weight_decay=0.00002, \n    nesterov=True\n    )\n\ncriterion = nn.CrossEntropyLoss()\n\ndev = torch.device(\"cuda\")\nmodel.to(dev)\n\n\n# exclude certain layers from clipping or momentum\n\nfor params in optimizer.param_groups:\n    name = params['name'] \n    \n    if model.exclude_from_weight_decay(name):\n        params['weight_decay'] = 0\n    if model.exclude_from_clipping(name):\n        params['clipping'] = None","2d6c1971":"num_epochs = 15\ntrn_loss_list, val_loss_list, trn_acc_list, val_acc_list = [], [], [], []\nbest_acc = 0.0\n\nbest_model = copy.deepcopy(model.state_dict())","e70c9671":"since = time.time()\n\nfor epoch in range(num_epochs):    \n    trn_loss, trn_corrects, val_loss, val_corrects = 0.0, 0, 0.0, 0\n    \n    # train stage\n    model.train()\n    for i, (data, target) in enumerate(dataloaders['train']):\n        data, target = data.to(dev), target.to(dev)\n\n        optimizer.zero_grad()\n        output = model(data)                \n    \n        loss = criterion(output, target)\n        loss.backward()\n        \n        optimizer.step()  \n        \n        _, preds = torch.max(output, 1)\n        trn_corrects += torch.sum(preds == target.data)\n        trn_loss += loss.item()\n\n    # evaluation stage\n    model.eval()\n    with torch.no_grad():\n        for j, (val_data, val_target) in enumerate(dataloaders['valid']):\n            val_data, val_target = val_data.to(dev), val_target.to(dev)\n            \n            val_output = model(val_data)\n            v_loss = criterion(val_output, val_target)\n            \n            _, preds = torch.max(val_output, 1)\n            val_corrects += torch.sum(preds == val_target.data)\n            val_loss += v_loss.item()\n\n\n    # saving train & validation accuracy\n    trn_acc_list.append(trn_corrects \/ max_trn_corrects * 100)\n    val_acc_list.append(val_corrects \/ max_val_corrects * 100)\n    \n    # saving train & validation loss\n    trn_loss_list.append(trn_loss\/batch_num['train'])\n    val_loss_list.append(val_loss\/batch_num['valid'])\n\n    time_elapsed = time.time() - since\n    print(\"epoch: {}\/{} | trn loss: {:.4f} | val loss: {:.4f} | {:.0f}m {:.0f}s elapsed\"\n          .format(epoch+1, num_epochs, \n                  trn_loss \/ batch_num['train'], \n                  val_loss \/ batch_num['valid'], \n                  time_elapsed \/\/ 60, time_elapsed % 60))\n\n    # update the best model if current model has higher accuracy\n    if val_corrects \/ max_val_corrects * 100 > best_acc:\n        best_acc = val_corrects \/ max_val_corrects * 100\n        best_model = copy.deepcopy(model.state_dict())\n        print(\"best model updated-epoch: {} | val_accuracy: {:.4f}\"\n            .format(epoch+1, best_acc))\n\ntime_elapsed = time.time() - since","8d6b9891":"print('Training complete in {:.0f}m {:.0f}s'\n      .format(time_elapsed \/\/ 60, time_elapsed % 60))\nprint('Best valid Acc: %.2f' %(best_acc))\n\n# save best model to best_model.pt\nmodel.load_state_dict(best_model)\ntorch.save(model.state_dict(), '\/kaggle\/working\/best_model.pt')\nprint(\"model saved\")","3fb56fee":"plt.plot(trn_loss_list, label='train_loss')\nplt.plot(val_loss_list, label='val_loss')\nplt.plot(trn_acc_list, label='train_acc')\nplt.plot(val_acc_list, label='val_acc')\n\nplt.legend()","4e3606f3":"best_model = NFNet(\n    num_classes=1000, \n    variant='F0', \n    stochdepth_rate=0.25, \n    alpha=0.2,\n    se_ratio=0.5,\n    activation='gelu'\n)\n\n# best model weight file\nbest_model.load_state_dict(torch.load('\/kaggle\/working\/best_model.pt'))\n\nbest_model.to(dev)\n\nprint()","c2e08dce":"corrects = 0\ntotal = 0\n\nmodel.eval()\nwith torch.no_grad():\n    for j, (data, target) in enumerate(dataloaders['test']):\n        data, target = data.to(dev), target.to(dev)\n\n        output = best_model(data)\n        \n        _, preds = torch.max(output, 1)\n        corrects += torch.sum(preds == target.data)\n        total += len(preds)\n        \nprint(\"{}\/{} | test accuracy: {:.4f}\".format(corrects, total, corrects\/total))","74ac62e1":"batch_size = 64\n\ndata_path = '\/kaggle\/input\/fruits\/fruits-360\/Test'\n\ntest_fruit = datasets.ImageFolder(\n    data_path,\n    transforms.Compose([\n        transforms.Resize((192, 192)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n)\n\ntest_data = DataLoader(test_fruit,\n                      batch_size=batch_size, shuffle=True,\n                      num_workers=8)\n\n\nprint('batch_size : {}\\ntest : {}'\n      .format(batch_size, len(test_data)))","c3fddfd3":"corrects = 0\ntotal = 0\n\nmodel.eval()\nwith torch.no_grad():\n    for j, (data, target) in enumerate(test_data):\n        data, target = data.to(dev), target.to(dev)\n\n        output = best_model(data)\n        \n        _, preds = torch.max(output, 1)\n        corrects += torch.sum(preds == target.data)\n        total += len(preds)\n        \nprint(\"{}\/{} | test accuracy: {:.4f}\".format(corrects, total, corrects\/total))","91f0584d":"- Train and validation is done in 1 epoch\n- total 15 epochs\n- After doing each evaluation stage, update the best model which has the **highest validation data accuracy**.","2a7ad137":"# 98.84%","042f34bb":"# Testing\n\n\n## Subset of the Training data\n\n- This data is 20% of the Training data\n- accuracy: **0.9998**","03d62fbd":"### image test\n","33b4dd9f":"## Real test data\n\n- data in `\/kaggle\/input\/fruits\/fruits-360\/Test` folder\n- accuracy: **0.9884**","db8592d9":"## prepare datasets\n\n- use ImageFolder class\n- img files -> datasets","47a00cf0":"## Reconstruct best model","79e85d45":"## References\n\n### Paper\n\n- https:\/\/arxiv.org\/pdf\/2102.06171v1.pdf\n\n### S0TA\n\n- https:\/\/paperswithcode.com\/paper\/high-performance-large-scale-image\n- won 3rd place Image Classification on ImageNet (2021.3)\n\n### github\n\n- **original** https:\/\/github.com\/deepmind\/deepmind-research\/tree\/master\/nfnets\n- **implement using pytorch**  https:\/\/github.com\/benjs\/nfnets_pytorch\n\n### demo colab\n\n- https:\/\/colab.research.google.com\/github\/deepmind\/deepmind-research\/blob\/master\/nfnets\/nfnet_demo_colab.ipynb","2c016b14":"## Import modules","3ab2825a":"# Training\n\n- model: NFNet-F0\n- optimizer: SGD with Unit-Adaptive Gradient-Clipping\n- criterion: CrossEntropyLoss\n\n- working on GPU","6e88df38":"# Result \n\n- loss and accuracy graph","47e33821":"# Codes\n\n## preparation","2d7298e2":"- use DataLoader class\n- dataset -> dataloader\n- batch size is 64","af702f66":"# Index\n\n[1. Introduction](.\/#NFNets)\n\n[2. Training](.\/#Training)\n\n[3. Result Graph](.\/#Result)\n\n[4. Testing](.\/#Testing)","dc113389":"# NFNets\n\n- SOTA\n- Normalizer-Free ResNets\n- use an adaptive gradient clipping technique\n- Normalizer-Free models attain significantly better performance than their batch-normalized counterparts","cb8eef9b":"### dataset split\n\n- use train_test_split\n- train data: 0.6x\n- validation data: 0.2x\n- test data: 0.2x\n- This test data is the subset of the training data"}}