{"cell_type":{"8d43246d":"code","29b8f5a1":"code","6ab93b1d":"code","095dad63":"code","bd4f3739":"code","b1ecbb83":"code","9b2406a0":"code","468f476a":"code","0ff83655":"code","4f2000ed":"code","3cc05006":"code","df7f243b":"code","f7468bbb":"code","3a777900":"code","30204800":"markdown","7f471f04":"markdown","c718f656":"markdown","5ac4f18a":"markdown","27e4b0b0":"markdown","c9b49417":"markdown","fed0760e":"markdown","9be071e2":"markdown","96e8e72a":"markdown"},"source":{"8d43246d":"import numpy as np \nimport pandas as pd\npd.set_option('display.width', 100000) # Extend the display width to prevent split functions to not cover full text\nimport matplotlib.pyplot as plt\nimport time\nimport warnings \nfrom sklearn.manifold import TSNE\nwarnings.filterwarnings('ignore')\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style \n%matplotlib inline\n\n# NLP libraries\nimport spacy\nfrom spacy.lang.en import English\nfrom nltk.tokenize import word_tokenize \nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\nfrom multiprocessing import cpu_count\nimport gensim.downloader as api\nimport re, string, unicodedata\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer\n\n\n# Any results you write to the current directory are saved as output.\nbiorxiv = pd.read_csv(\"\/kaggle\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/biorxiv_clean.csv\")\n\nbiorxiv['paper_text'] = biorxiv['text'].map(lambda x: re.sub('\\[[^]],?#=*\\]', '', x))# Convert the titles to lowercase\nbiorxiv['paper_text'] = biorxiv['paper_text'].map(lambda x: x.lower())\n\nbiorxiv.head(5)","29b8f5a1":"# Filter papers containing all words in list\ndef filter_papers_word_list(word_list):\n    papers_id_list = []\n    text = \" \"\n    for idx, paper in biorxiv.iterrows():\n        text += paper.paper_text\n        if all(x in paper.paper_text for x in word_list):\n            papers_id_list.append(paper.paper_id)\n            #text += paper.paper_text\n            \n\n    return paperTextProcessing(text, word_list)","6ab93b1d":"def remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\n\n\ndef remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\n\ndef stem_words(words):\n    \"\"\"Stem words in list of tokenized words\"\"\"\n    stemmer = LancasterStemmer()\n    stems = []\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)\n    return stems\n\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\n\n\ndef normalize(words):\n    words = remove_non_ascii(words)\n    words = remove_punctuation(words)\n    words = lemmatize_verbs(words)\n    return words\n\n\ndef paperTextProcessing(paper_text, word_list):\n    \n    relevant_sentences = []\n    paper_text = paper_text.split(\".\")\n    for sentence in paper_text:\n        for word in word_list:\n            if word in sentence:\n                sentence1 = sentence.split()\n                relevant_sentences.append(sentence1)\n\n    stop_words = set(stopwords.words('english')) \n    \n    for sentence in relevant_sentences:\n        for word in stop_words:\n            while(word in sentence):\n                sentence.remove(word)\n    \n    final_corpus = []\n    \n    for sentence in relevant_sentences:\n        final_corpus.append(normalize(sentence))\n        \n        \n    return final_corpus","095dad63":"def dataSegmentation(corpus, vocabulary):\n    sentences = []\n    for sentence in corpus:\n        num_sentence = []\n        for word in sentence:\n            if word in model:\n                num_sentence.append(vocabulary[word])\n            else:\n                print(word)\n\n        sentences.append(num_sentence)\n        \n    return sentences\n\n\n\ndef tsnePlotSimilarWords(title, labels, embedding_clusters, word_clusters, a, filename=None):\n    plt.figure(figsize=(16, 9))\n    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n        x = embeddings[:, 0]\n        y = embeddings[:, 1]\n        plt.scatter(x, y, c=color, alpha=a, label=label)\n        for i, word in enumerate(words):\n            plt.annotate(word, alpha=0.5, xy=(x[i], \n                        y[i]), xytext=(5, 2),\n                         textcoords='offset points', \n                         ha='right', \n                         va='bottom', size=12)\n    plt.legend(loc=1)\n    plt.title(title)\n    plt.grid(True)\n    if filename:\n        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n\ndef wordClustering(model, keys):\n    \n\n    embedding_clusters = []\n    word_clusters = []\n    for word in keys:\n        embeddings = []\n        words = []\n        for similar_word, _ in model.most_similar(word, topn=30):\n            words.append(similar_word)\n            embeddings.append(model[similar_word])\n        embedding_clusters.append(embeddings)\n        word_clusters.append(words)\n        \n    \n    embedding_clusters = np.array(embedding_clusters)\n    n, m, k = embedding_clusters.shape\n    tsne_model_en_2d = TSNE(perplexity=25, n_components=2, init='pca', n_iter=3500, random_state=32)\n    embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n    \n    tsnePlotSimilarWords('COVID-19 Word Clustering', keys, embeddings_en_2d, word_clusters, 0.7,\n                        'similar_words.png')\n    \n    ","bd4f3739":"from gensim.models import FastText\n\ncorpus = filter_papers_word_list([\"transmission\", \"incubation\", \"propagation\", \"infection\", \"environment\"])\n\n\nmodel = FastText(corpus, min_count=2, workers=20, window=2, word_ngrams=1, alpha=0.02, hs=1)\nnewCorpus = filter_papers_word_list([\"coronavirus\", \"virus\", \"risk\", \"disease\", \"infection\", \"transmission\", \"prevention\", \"SARS\", \"outbreak\", \"covid-19\", \"ethical\", \"social\", \"government\", \"symptoms\", \"vaccines\"])\nnewCorpus.reverse()\nmodel.train(newCorpus, epochs=20, total_examples=model.corpus_count, compute_loss=True)","b1ecbb83":"style.use('seaborn-poster')\nstyle.use('ggplot')\n\nkeys = ['transmission', 'infection', 'incubation', 'environment']\n\nwordClustering(model, keys)","9b2406a0":"keys = ['anti-viral', 'cure', 'vaccine']\n\nwordClustering(model, keys)","468f476a":"keys = ['ethic', 'social', 'government', 'precautions']\n\nwordClustering(model, keys)","0ff83655":"keys = ['genetics', 'origins', 'evolution', 'spikes', 'genomic', 'sequencing']\n\nwordClustering(model, keys)","4f2000ed":"keys = ['diagnostics', 'surveillance', 'patients', 'symptoms']\n\nwordClustering(model, keys)","3cc05006":"keys = ['interventions', 'non-pharmaceutical', 'collaborations', 'inter-sectoral']\n\nwordClustering(model, keys)","df7f243b":"import plotly.io as pio\n#import plotly.plotly as py \nimport plotly.graph_objects as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\ninit_notebook_mode(connected=True)\n\ndef tsne_plot(model):\n    \n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=30, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n       \n\n    data = [\n        go.Scatter(\n            x=[i[0] for i in new_values],\n            y=[i[1] for i in new_values],\n            mode='markers',\n            text=[i for i in model.wv.vocab],\n            marker=dict(\n            size=4,\n            color = [len(i) for i in model.wv.vocab], #set color equal to a variable\n            opacity= 0.8,\n            colorscale='Viridis',\n            showscale=False\n        )\n        )\n    ]\n    layout = go.Layout()\n    layout = dict(\n              yaxis = dict(zeroline = False),\n              xaxis = dict(zeroline = False)\n             )\n    \n    \n    \n    fig = go.Figure(data=data, layout=layout)\n    \n    fig.update_layout(title=\n    {\n        'text': \"Semantic word representations by FastText model\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n    \n    \n    file = plot(fig)\n    pio.show(fig)\n\ntsne_plot(model)\n","f7468bbb":"model.most_similar('vaccine', topn=20)\n\n","3a777900":"model.most_similar('origin', topn=20)\n","30204800":"## 3. Modeling with the returned corpus <a id=\"section-three\"><\/a>\n\nImplementation of the Word2Vec models with the corpus recolected in the paperTextProcessing function. Based on the model, we build a t-SNE model and plot the words. ","7f471f04":"Find most similar words","c718f656":"## 2. Word Clustering <a id=\"section-two\"><\/a>\n\nThe following pre-processing functions prepare the corpus for the respective models. The function of word clustering get the respective embeddings in respect to the words we are looking for. In addition, data segmentation prepares the corpus transforming the sentences into the embeddings for further analyses with the semantic vectors obtained from Word2Vec. ","5ac4f18a":"# 4. FastText Embedding Visualization <a id=\"section-four\"><\/a>\n","27e4b0b0":"# 1. Filtering papers <a id=\"section-one\"><\/a>\n\nGeneral studies like word frequency and such do require the full set of scientific papers. However, when dealing with specific tasks or topics, it's useful to select the subset of papers containing only certain words. Despite being very simple, the function defined in this section provides a list of paper_id containing a desired set of words.","c9b49417":"Once the function that filters the papers is done, it calls paperTextProcessing. It is a simple text pre-processing function that splits the paper into lines and save the ones containing the words of our importance. Sequently, it removes stop words to improve the analysis of the future models implemented. ","fed0760e":"I will load output files from [xhlulu's kernel](https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv), which contains a useful transformation of the json files in dictionaries to csv readable format. Go check it to give some credit!","9be071e2":"# CORD-19: Context words processing to find meaningful clues \n\nAs a current student at the University of Puerto Rico at Mayag\u00fcez, I thought it would be a great opportunity to implement what I have learned in my classes while I contribute to the global threat known as COVID-19. The purpose of this kernel is to pre-process the papers of interest to build insightful models. I am far from done, but the first version includes a little Word2vec to vectorize the words and the proper visualization with t-SNE and matplotlib. I might change the models in the future implementations as my main intention is to display meaningful clues along the key words. Therefore, my plan is to implement a Word Sense Disambiguation Algorithm and some wrapper methods. \n\nAny help or feedback is very welcomed. In addition, this kernel is built upon CORD-19 - Data extraction functions from @saga21(very useful!).\n\n\nDisclaimer: This kernel is still under construction. ","96e8e72a":"This kernel is still under construction. "}}