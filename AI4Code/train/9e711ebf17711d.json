{"cell_type":{"4ea69e11":"code","f43ea6bc":"code","a5238035":"code","37e5c51d":"code","185d1570":"code","5f61f1f0":"code","d91bf091":"code","f6e6d340":"markdown"},"source":{"4ea69e11":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#load data\ndftrain=pd.read_csv('\/kaggle\/input\/learn-together\/train.csv')\ndftest=pd.read_csv('\/kaggle\/input\/learn-together\/test.csv')","f43ea6bc":"####### FEATURE ENGINEERING #####\n#taking most of this from my work in feature engineering at https:\/\/www.kaggle.com\/mulargui\/xgboost\n#https:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover \n#Fixing Hillshade_3pm\n#replacing the zeros for better guess, mainly to avoid zeros in the feature engineering and fake outliers. \nnum_train = len(dftrain)\ntmp = dftrain.drop('Cover_Type', axis = 1)\nall_data = tmp.append(dftest)\n\ncols_for_HS = ['Aspect','Slope', 'Hillshade_9am','Hillshade_Noon']\nHS_zero = all_data[all_data.Hillshade_3pm==0]\nHS_train = all_data[all_data.Hillshade_3pm!=0]\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf_hs = RandomForestRegressor(n_estimators=100).fit(HS_train[cols_for_HS], HS_train.Hillshade_3pm)\nout = rf_hs.predict(HS_zero[cols_for_HS]).astype(int)\n\n#I couldn't make this line work, feature not used\n#all_data.loc[HS_zero.index,'Hillshade_3pm'] = out\n#dftrain['Hillshade_3pm']= all_data.loc[:num_train,'Hillshade_3pm']\n#dftest['Hillshade_3pm']= all_data.loc[num_train:,'Hillshade_3pm']\n\n# Add PCA features\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=0.99).fit(all_data)\ntrans = pca.transform(all_data)\n\nfor i in range(trans.shape[1]):\n    col_name= 'pca'+str(i+1)\n    dftrain[col_name] = trans[:num_train, i]\n    dftest[col_name] = trans[num_train:, i]\n\n#https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition\ndef euclidean(df):\n    df['Euclidean_distance_to_hydro'] = (df.Vertical_Distance_To_Hydrology**2 \n                                         + df.Horizontal_Distance_To_Hydrology**2)**.5\n    return df\n\ndftrain = euclidean(dftrain)\ndftest = euclidean(dftest)\n\nfrom itertools import combinations\ndef distances(df):\n    cols = [\n        'Horizontal_Distance_To_Roadways',\n        'Horizontal_Distance_To_Fire_Points',\n        'Horizontal_Distance_To_Hydrology',\n    ]\n    df['distance_mean'] = df[cols].mean(axis=1)\n    df['distance_sum'] = df[cols].sum(axis=1)\n    df['distance_road_fire'] = df[cols[:2]].mean(axis=1)\n    df['distance_hydro_fire'] = df[cols[1:]].mean(axis=1)\n    df['distance_road_hydro'] = df[[cols[0], cols[2]]].mean(axis=1)\n    \n    df['distance_sum_road_fire'] = df[cols[:2]].sum(axis=1)\n    df['distance_sum_hydro_fire'] = df[cols[1:]].sum(axis=1)\n    df['distance_sum_road_hydro'] = df[[cols[0], cols[2]]].sum(axis=1)\n    \n    df['distance_dif_road_fire'] = df[cols[0]] - df[cols[1]]\n    df['distance_dif_hydro_road'] = df[cols[2]] - df[cols[0]]\n    df['distance_dif_hydro_fire'] = df[cols[2]] - df[cols[1]]\n    \n    # Vertical distances measures\n    colv = ['Elevation', 'Vertical_Distance_To_Hydrology']\n    df['Vertical_dif'] = df[colv[0]] - df[colv[1]]\n    df['Vertical_sum'] = df[colv].sum(axis=1)\n    \n    return df\n  \ndftrain = distances(dftrain)\ndftest = distances(dftest)\n    \ndef shade(df):\n    SHADES = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n    \n    df['shade_noon_diff'] = df['Hillshade_9am'] - df['Hillshade_Noon']\n    df['shade_3pm_diff'] = df['Hillshade_Noon'] - df['Hillshade_3pm']\n    df['shade_all_diff'] = df['Hillshade_9am'] - df['Hillshade_3pm']\n    df['shade_sum'] = df[SHADES].sum(axis=1)\n    df['shade_mean'] = df[SHADES].mean(axis=1)\n    \n    return df\n\ndftrain = shade(dftrain)\ndftest = shade(dftest)\n\ndef elevation(df):\n    df['ElevationHydro'] = df['Elevation'] - 0.25 * df['Euclidean_distance_to_hydro']\n    return df\n\ndftrain = elevation(dftrain)\ndftest = elevation(dftest)\n\ndef elevationV(df):\n    df['ElevationV'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n    return df\n\ndftrain = elevationV(dftrain)\ndftest = elevationV(dftest)\n\ndef elevationH(df):\n    df['ElevationH'] = df['Elevation'] - 0.19 * df['Horizontal_Distance_To_Hydrology']\n    return df\n\ndftrain = elevationH(dftrain)\ndftest = elevationH(dftest)\n\ndef kernel_features(df):\n    df['Elevation2'] = df['Elevation']**2\n    df['ElevationLog'] = np.log1p(df['Elevation'])\n    return df\n\ndftrain = kernel_features(dftrain)\ndftest = kernel_features(dftest)\n\ndef degree(df):\n    df['Aspect_cos'] = np.cos(np.radians(df.Aspect))\n    df['Aspect_sin'] = np.sin(np.radians(df.Aspect))\n    #df['Slope_sin'] = np.sin(np.radians(df.Slope))\n    df['Aspectcos_Slope'] = df.Slope * df.Aspect_cos\n    #df['Aspectsin_Slope'] = df.Slope * df.Aspect_sin\n    \n    return df\n\ndftrain = degree(dftrain)\ndftest = degree(dftest)\n\nfrom bisect import bisect\ncardinals = [i for i in range(45, 361, 90)]\npoints = ['N', 'E', 'S', 'W']\n\ndef cardinal(df):\n    df['Cardinal'] = df.Aspect.apply(lambda x: points[bisect(cardinals, x) % 4])\n    return df\n\ndftrain = cardinal(dftrain)\ndftest = cardinal(dftest)\n\ndef cardinal_num(df):\n    d = {'N': 0, 'E': 1, 'S': 0, 'W':-1}\n    df['Cardinal'] = df.Cardinal.apply(lambda x: d[x])\n    return df\n\ndftrain = cardinal_num(dftrain)\ndftest = cardinal_num(dftest)\n\n#adding features based on https:\/\/douglas-fraser.com\/forest_cover_management.pdf pages 21,22\n#note: not all climatic and geologic codes have a soil type\n\ndef Climatic2(row): \n    if (row['Soil_Type1'] == 1) or (row['Soil_Type2'] == 1) or (row['Soil_Type3'] == 1) or (row['Soil_Type4'] == 1) \\\n        or (row['Soil_Type5'] == 1) or (row['Soil_Type6'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic2'] = dftrain.apply (lambda row: Climatic2(row), axis=1)\ndftest['Climatic2'] = dftest.apply (lambda row: Climatic2(row), axis=1)\n\ndef Climatic3(row): \n    if (row['Soil_Type7'] == 1) or (row['Soil_Type8'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic3'] = dftrain.apply (lambda row: Climatic3(row), axis=1)\ndftest['Climatic3'] = dftest.apply (lambda row: Climatic3(row), axis=1)\n\ndef Climatic4(row): \n    if (row['Soil_Type9'] == 1) or (row['Soil_Type10'] == 1) or (row['Soil_Type11'] == 1) or (row['Soil_Type12'] == 1) \\\n        or (row['Soil_Type13'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic4'] = dftrain.apply (lambda row: Climatic4(row), axis=1)\ndftest['Climatic4'] = dftest.apply (lambda row: Climatic4(row), axis=1)\n\ndef Climatic5(row): \n    if (row['Soil_Type14'] == 1) or (row['Soil_Type15'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic5'] = dftrain.apply (lambda row: Climatic5(row), axis=1)\ndftest['Climatic5'] = dftest.apply (lambda row: Climatic5(row), axis=1)\n\ndef Climatic6(row): \n    if (row['Soil_Type16'] == 1) or (row['Soil_Type17'] == 1) or (row['Soil_Type18'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic6'] = dftrain.apply (lambda row: Climatic6(row), axis=1)\ndftest['Climatic6'] = dftest.apply (lambda row: Climatic6(row), axis=1)\n\ndef Climatic7(row): \n    if (row['Soil_Type19'] == 1) or (row['Soil_Type20'] == 1) or (row['Soil_Type21'] == 1) or (row['Soil_Type22'] == 1) \\\n        or (row['Soil_Type23'] == 1) or (row['Soil_Type24'] == 1) or (row['Soil_Type25'] == 1) or (row['Soil_Type26'] == 1) \\\n        or (row['Soil_Type27'] == 1) or (row['Soil_Type28'] == 1) or (row['Soil_Type29'] == 1) or (row['Soil_Type30'] == 1) \\\n        or (row['Soil_Type31'] == 1) or (row['Soil_Type32'] == 1) or (row['Soil_Type33'] == 1) or (row['Soil_Type34'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic7'] = dftrain.apply (lambda row: Climatic7(row), axis=1)\ndftest['Climatic7'] = dftest.apply (lambda row: Climatic7(row), axis=1)\n\ndef Climatic8(row): \n    if (row['Soil_Type35'] == 1) or (row['Soil_Type36'] == 1) or (row['Soil_Type37'] == 1) or (row['Soil_Type38'] == 1) \\\n        or (row['Soil_Type39'] == 1) or (row['Soil_Type40'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic8'] = dftrain.apply (lambda row: Climatic8(row), axis=1)\ndftest['Climatic8'] = dftest.apply (lambda row: Climatic8(row), axis=1)\n\ndef Geologic1(row): \n    if (row['Soil_Type14'] == 1) or (row['Soil_Type15'] == 1) or (row['Soil_Type16'] == 1) or (row['Soil_Type17'] == 1) \\\n        or (row['Soil_Type19'] == 1) or (row['Soil_Type20'] == 1) or (row['Soil_Type21'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Geologic1'] = dftrain.apply (lambda row: Geologic1(row), axis=1)\ndftest['Geologic1'] = dftest.apply (lambda row: Geologic1(row), axis=1)\n\ndef Geologic2(row): \n    if (row['Soil_Type9'] == 1) or (row['Soil_Type22'] == 1) or (row['Soil_Type23'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Geologic2'] = dftrain.apply (lambda row: Geologic2(row), axis=1)\ndftest['Geologic2'] = dftest.apply (lambda row: Geologic2(row), axis=1)\n\ndef Geologic5(row): \n    if (row['Soil_Type7'] == 1) or (row['Soil_Type8'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Geologic5'] = dftrain.apply (lambda row: Geologic5(row), axis=1)\ndftest['Geologic5'] = dftest.apply (lambda row: Geologic5(row), axis=1)\n\ndef Geologic7(row): \n    if (row['Soil_Type1'] == 1) or (row['Soil_Type2'] == 1) or (row['Soil_Type3'] == 1) or (row['Soil_Type4'] == 1) \\\n        or (row['Soil_Type5'] == 1) or (row['Soil_Type6'] == 1) or (row['Soil_Type10'] == 1) \\\n        or (row['Soil_Type11'] == 1) or (row['Soil_Type12'] == 1) or (row['Soil_Type13'] == 1) or (row['Soil_Type18'] == 1) \\\n        or (row['Soil_Type24'] == 1) or (row['Soil_Type25'] == 1) or (row['Soil_Type26'] == 1) or (row['Soil_Type27'] == 1) \\\n        or (row['Soil_Type28'] == 1) or (row['Soil_Type29'] == 1) or (row['Soil_Type30'] == 1) or (row['Soil_Type31'] == 1) \\\n        or (row['Soil_Type32'] == 1) or (row['Soil_Type33'] == 1) or (row['Soil_Type34'] == 1) or (row['Soil_Type35'] == 1) \\\n        or (row['Soil_Type36'] == 1) or (row['Soil_Type37'] == 1) or (row['Soil_Type38'] == 1) or (row['Soil_Type39'] == 1) \\\n        or (row['Soil_Type40'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Geologic7'] = dftrain.apply (lambda row: Geologic7(row), axis=1)\ndftest['Geologic7'] = dftest.apply (lambda row: Geologic7(row), axis=1)","a5238035":"####### DATA PREPARATION #####\n#split train data in features and labels\ny = dftrain.Cover_Type\nx = dftrain.drop(['Id','Cover_Type'], axis=1)\n\n# split test data in features and Ids\nIds = dftest.Id\nx_predict = dftest.drop('Id', axis=1)\n\n#force all types to float\nx = x.astype(float)\nx_predict = x_predict.astype(float)\n\n#normalize features\ndef normalize(feature):\n    min=x[feature].min()\n    min2=x_predict[feature].min()\n    if (min2 < min):\n        min=min2\n\n    max=x[feature].max()\n    max2=x_predict[feature].max()\n    if (max2 > max):\n        max=max2\n        \n    x[feature]=(x[feature]-min)\/(max-min)                             \n    x_predict[feature]=(x_predict[feature]-min)\/(max-min)  \n    \n    return                                \n\nnormalize(\"Elevation\")\nnormalize(\"Aspect\")\nnormalize(\"Slope\")\nnormalize(\"Horizontal_Distance_To_Hydrology\")\nnormalize(\"Vertical_Distance_To_Hydrology\")\nnormalize(\"Horizontal_Distance_To_Roadways\")\nnormalize(\"Hillshade_9am\")\nnormalize(\"Hillshade_Noon\")\nnormalize(\"Hillshade_3pm\")\nnormalize(\"Horizontal_Distance_To_Fire_Points\")\nnormalize(\"pca1\")\nnormalize(\"Euclidean_distance_to_hydro\")\nnormalize(\"distance_mean\")\nnormalize(\"distance_sum\")\nnormalize(\"distance_road_fire\")\nnormalize(\"distance_hydro_fire\")\nnormalize(\"distance_road_hydro\")\nnormalize(\"distance_sum_road_fire\")\nnormalize(\"distance_sum_hydro_fire\")\nnormalize(\"distance_sum_road_hydro\")\nnormalize(\"distance_dif_road_fire\")\nnormalize(\"distance_dif_hydro_road\")\nnormalize(\"distance_dif_hydro_fire\")\nnormalize(\"Vertical_dif\")\nnormalize(\"Vertical_sum\")\nnormalize(\"shade_noon_diff\")\nnormalize(\"shade_3pm_diff\")\nnormalize(\"shade_all_diff\")\nnormalize(\"shade_sum\")\nnormalize(\"shade_mean\")\nnormalize(\"ElevationHydro\")\nnormalize(\"ElevationV\")\nnormalize(\"ElevationH\")\nnormalize(\"Elevation2\")\nnormalize(\"ElevationLog\")\nnormalize(\"Aspect_cos\")\nnormalize(\"Aspect_sin\")\nnormalize(\"Aspectcos_Slope\")","37e5c51d":"# convert the label to One Hot Encoding\nnum_classes = 7\n\n#to_categorical requires 0..6 instead of 1..7\ny -=1\ny = y.to_numpy()\n\nfrom tensorflow.keras.utils import to_categorical\ny = to_categorical(y, num_classes)\n\n#validate data - no rows with all zeros\n#x.index[x.eq(0).all(1)]\nprint(x[x.eq(0).all(1)].empty)\nprint(x_predict[x_predict.eq(0).all(1)].empty)\n\n#convert the features dataframes to numpy arrays\nx = x.to_numpy()\nx_predict = x_predict.to_numpy()\n\n#split in train (80%) and test (20%) sets \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2, stratify=y)","185d1570":"#here is the NN model\nimport keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\n\nnum_features = x_train.shape[1]\n\nmodel = Sequential()\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal', input_dim=num_features))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer='Adam',\n              metrics=['accuracy'])\n\n#train the model\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=250)\n\n# Predict!!\ny_predict = model.predict(x_predict)","5f61f1f0":"for i in range(10):\n\tprint(y_predict[i], np.argmax(y_predict[i])+1)","d91bf091":"# Save predictions to a file for submission\n#argmax give us the highest probable label\n# we add one to the predictions to scale from 0..6 to 1..7\noutput = pd.DataFrame({'Id': Ids,\n                       'Cover_Type': y_predict.argmax(axis=1)+1})\noutput.to_csv('submission.csv', index=False)\n\n#create a link to download the file    \nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","f6e6d340":"This is my first experiment in this competition. Whereas XGBoost is highly recommended I rather tried to see how far I can go with an NN (using Keras).\nThis is the basic model and with 250 epochs has an accuracy of 80% (really poor).\nI'll continue for a few days researching how much I can optimize this model.\n\nNow switching to using forests in this new kernel https:\/\/www.kaggle.com\/mulargui\/xgboost\nYou can find all my notes and versions at https:\/\/github.com\/mulargui\/kaggle-Classify-forest-types"}}