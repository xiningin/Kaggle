{"cell_type":{"cd73313c":"code","90c08bf9":"code","54e2482a":"code","b38405b0":"code","87316b2d":"code","5f73e088":"code","edb59902":"code","db4203b0":"code","ac798829":"code","d1568b82":"code","119a72c3":"code","fe42fae2":"code","4793a10f":"code","e3526e1f":"code","df3652e8":"code","8310d582":"code","37113ba4":"code","e1591c4e":"code","9dd809d5":"code","3be09f02":"code","b20af5df":"code","c2f792fe":"code","021ca3fd":"code","3cd83dc1":"code","95da450e":"code","e7ed3803":"code","79a5738d":"code","1b6a2a60":"code","78e3c1ab":"code","b0f3c359":"code","eccb392a":"code","2c3e4e69":"code","e50ab066":"code","89c24a19":"code","6ac10df9":"code","c8e4035b":"code","23472372":"code","cb3f0071":"code","cec16cf3":"code","862e8f78":"code","39cb918e":"code","5f3dd0d8":"code","2ba0f67b":"code","b785c99b":"code","384075a3":"code","9253ffd7":"code","ac9fb959":"code","25d24552":"code","0775cbff":"code","52ebcf59":"code","14cf4f42":"markdown","82626fd6":"markdown","a641c6dc":"markdown","01ff7f5f":"markdown","19f2bfb1":"markdown","082327d3":"markdown","7594de31":"markdown","8f329d50":"markdown","b5f071af":"markdown","dde4a870":"markdown","0becdc8d":"markdown","217c0d79":"markdown","459a8c2d":"markdown","d9c64276":"markdown","d1fc2bba":"markdown","36acada6":"markdown","3766dc6a":"markdown","7bf280a7":"markdown","c9818c1f":"markdown","e7b3a188":"markdown","3a4b5ec3":"markdown","c0811d06":"markdown","a595ee80":"markdown","3727f76b":"markdown","f59b828a":"markdown","9186b38d":"markdown","ee43458d":"markdown","b4c46af0":"markdown","08a1ab23":"markdown","0a8c9bda":"markdown","98fde6ff":"markdown","f5350dc3":"markdown","3e27fb3e":"markdown","978747fa":"markdown","387d87a8":"markdown"},"source":{"cd73313c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \npd.options.display.float_format = '{:,.4f}'.format\nsns.set(style=\"whitegrid\")","90c08bf9":"df = pd.read_csv(\"\/kaggle\/input\/passenger-list-for-the-estonia-ferry-disaster\/estonia-passenger-list.csv\")\nprint(\"Shape of DataSet:\", df.shape[0], 'rows |', df.shape[1], 'columns')\ndf.head()","54e2482a":"df.info()","b38405b0":"df.describe().T","87316b2d":"def eda_categ_feat_desc_plot(series_categorical, title = \"\"):\n    \"\"\"Generate 2 plots: barplot with quantity and pieplot with percentage. \n       @series_categorical: categorical series\n       @title: optional\n    \"\"\"\n    series_name = series_categorical.name\n    val_counts = series_categorical.value_counts()\n    val_counts.name = 'quantity'\n    val_percentage = series_categorical.value_counts(normalize=True)\n    val_percentage.name = \"percentage\"\n    val_concat = pd.concat([val_counts, val_percentage], axis = 1)\n    val_concat.reset_index(level=0, inplace=True)\n    val_concat = val_concat.rename( columns = {'index': series_name} )\n    \n    fig, ax = plt.subplots(figsize = (12,4), ncols=2, nrows=1) # figsize = (width, height)\n    if(title != \"\"):\n        fig.suptitle(title, fontsize=18)\n        fig.subplots_adjust(top=0.8)\n\n    s = sns.barplot(x=series_name, y='quantity', data=val_concat, ax=ax[0])\n    for index, row in val_concat.iterrows():\n        s.text(row.name, row['quantity'], row['quantity'], color='black', ha=\"center\")\n\n    s2 = val_concat.plot.pie(y='percentage', autopct=lambda value: '{:.2f}%'.format(value),\n                             labels=val_concat[series_name].tolist(), legend=None, ax=ax[1],\n                             title=\"Percentage Plot\")\n\n    ax[1].set_ylabel('')\n    ax[0].set_title('Quantity Plot')\n\n    plt.show()","5f73e088":"def eda_horiz_plot(df, x, y, title, figsize = (8,5), palette=\"Blues_d\", formating=\"int\"):\n    \"\"\"Using Seaborn, plot horizonal Bar with labels\n    !!! Is recomend sort_values(by, ascending) before passing dataframe\n    !!! pass few values, not much than 20 is recommended\n    \"\"\"\n    f, ax = plt.subplots(figsize=figsize)\n    sns.barplot(x=x, y=y, data=df, palette=palette)\n    ax.set_title(title)\n    for p in ax.patches:\n        width = p.get_width()\n        if(formating == \"int\"):\n            text = int(width)\n        else:\n            text = '{.2f}'.format(width)\n        ax.text(width + 1, p.get_y() + p.get_height() \/ 2, text, ha = 'left', va = 'center')\n    plt.show()","edb59902":"def eda_categ_feat_desc_df(series_categorical):\n    \"\"\"Generate DataFrame with quantity and percentage of categorical series\n    @series_categorical = categorical series\n    \"\"\"\n    series_name = series_categorical.name\n    val_counts = series_categorical.value_counts()\n    val_counts.name = 'quantity'\n    val_percentage = series_categorical.value_counts(normalize=True)\n    val_percentage.name = \"percentage\"\n    val_concat = pd.concat([val_counts, val_percentage], axis = 1)\n    val_concat.reset_index(level=0, inplace=True)\n    val_concat = val_concat.rename( columns = {'index': series_name} )\n    return val_concat","db4203b0":"def eda_numerical_feat(series, title=\"\", with_label=True, number_format=\"\"):\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 5), sharex=False)\n    print(series.describe())\n    if(title != \"\"):\n        f.suptitle(title, fontsize=18)\n    sns.distplot(series, ax=ax1)\n    sns.boxplot(series, ax=ax2)\n    if(with_label):\n        describe = series.describe()\n        labels = { 'min': describe.loc['min'], 'max': describe.loc['max'], \n              'Q1': describe.loc['25%'], 'Q2': describe.loc['50%'],\n              'Q3': describe.loc['75%']}\n        if(number_format != \"\"):\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + number_format.format(v), ha='center', va='center', fontweight='bold',\n                         size=8, color='white', bbox=dict(facecolor='#445A64'))\n        else:\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + str(v), ha='center', va='center', fontweight='bold',\n                     size=8, color='white', bbox=dict(facecolor='#445A64'))\n    plt.show()","ac798829":"def check_balanced_train_test_binary(x_train, y_train, x_test, y_test, original_size, labels):\n    \"\"\" To binary classification\n    each paramethes is pandas.core.frame.DataFrame\n    @total_size = len(X) before split\n    @labels = labels in ordem [0,1 ...]\n    \"\"\"\n    train_unique_label, train_counts_label = np.unique(y_train, return_counts=True)\n    test_unique_label, test_counts_label = np.unique(y_test, return_counts=True)\n\n    prop_train = train_counts_label\/ len(y_train)\n    prop_test = test_counts_label\/ len(y_test)\n\n    print(\"Original Size:\", '{:,d}'.format(original_size))\n    print(\"\\nTrain: must be 80% of dataset:\\n\", \n          \"the train dataset has {:,d} rows\".format(len(x_train)),\n          'this is ({:.2%}) of original dataset'.format(len(x_train)\/original_size),\n                \"\\n => Classe 0 ({}):\".format(labels[0]), train_counts_label[0], '({:.2%})'.format(prop_train[0]), \n                \"\\n => Classe 1 ({}):\".format(labels[1]), train_counts_label[1], '({:.2%})'.format(prop_train[1]),\n          \"\\n\\nTest: must be 20% of dataset:\\n\",\n          \"the test dataset has {:,d} rows\".format(len(x_test)),\n          'this is ({:.2%}) of original dataset'.format(len(x_test)\/original_size),\n                  \"\\n => Classe 0 ({}):\".format(labels[0]), test_counts_label[0], '({:.2%})'.format(prop_test[0]),\n                  \"\\n => Classe 1 ({}):\".format(labels[1]),test_counts_label[1], '({:.2%})'.format(prop_test[1])\n         )","d1568b82":"def class_report(y_target, y_preds, name=\"\", labels=None):\n    if(name != ''):\n        print(name,\"\\n\")\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred, target_names=labels))","119a72c3":"print(\"No Missing Data:\")\n\nprint(\"\\t\", df.isnull().sum().max(), \"invalid Data\")","fe42fae2":"# No duplicate Rows\nprint(\"Check duplicated rows\")\nprint(\"\\t\", df.duplicated(subset=None, keep='first').sum(), 'rows Duplicates')","4793a10f":"df['PassengerId'].value_counts()","e3526e1f":"df.query('PassengerId in [463, 767]')","df3652e8":"eda_categ_feat_desc_plot(df['Country'], 'Analysis \"Contry\" Distribution')","8310d582":"eda_categ_feat_desc_df(df['Country'])","37113ba4":"df_names = eda_categ_feat_desc_df(df['Firstname'])\nprint(\"unique first names: \", df_names.shape[0] )\ndf_names.head()","e1591c4e":"df_names = eda_categ_feat_desc_df(df['Lastname'])\nprint(\"unique last names: \", df_names.shape[0] )\ndf_names.head()","9dd809d5":"eda_categ_feat_desc_plot(df['Sex'], 'Analysis \"Sex\" Distribution')","3be09f02":"eda_numerical_feat(df['Age'], '\"Age\" Distribution', with_label=True)","b20af5df":"# Min Ages and Max Ages\ntop_int = 5\n\npd.concat([df.sort_values(by=\"Age\").head(top_int), df.sort_values(by=\"Age\").tail(top_int)])","c2f792fe":"eda_categ_feat_desc_plot(df['Category'], 'Analysis \"Category\" Distribution')","021ca3fd":"eda_categ_feat_desc_plot(df['Survived'], 'Analysis \"Survived\" Distribution')","3cd83dc1":"print(list(df.columns))","95da450e":"sns.pairplot(df, hue=\"Survived\", corner=True)","e7ed3803":"f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(14, 5), sharex=False)\nf.suptitle('Age x Survived', fontsize=18)\n\nsns.boxplot(y=\"Age\", x=\"Survived\", data=df, ax=ax1)\nsns.violinplot(y=\"Age\", x=\"Survived\", data=df, ax=ax2)\nax1.set_title(\"BoxPlot\")\nax2.set_title(\"ViolinPlot\")\n\nplt.show()","79a5738d":"f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(14, 5), sharex=False)\nf.suptitle('Age x Survived by Sex', fontsize=18)\n\nsns.boxplot(y=\"Age\", x=\"Survived\", hue=\"Sex\", data=df, ax=ax1)\nsns.violinplot(y=\"Age\", x=\"Survived\", hue=\"Sex\", data=df, ax=ax2)\nax1.set_title(\"BoxPlot\")\nax2.set_title(\"ViolinPlot\")\n\nplt.show()","1b6a2a60":"f, ax1 = plt.subplots(figsize=(10, 5))\n\nsns.countplot(x=\"Sex\", hue=\"Survived\", data=df, ax=ax1)\nax1.set_title(\"Sex x Survived\", size=15)\n\nplt.show()","78e3c1ab":"df1 = df.groupby([\"Survived\",\"Sex\"]).count().reset_index().rename({\"Firstname\": \"Quantity\"}, axis=1)\ndf1","b0f3c359":"f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 5), sharex=False)\nf.suptitle('Percentage of Survivors by Sex', fontsize=18)\n\nalist = df1['Quantity'].tolist()\n\ndf1.query('Sex == \"M\"').plot.pie(y='Quantity', figsize=(10, 5), autopct='%1.2f%%', \n                                 labels = ['Not Survived = ' + str(alist[1]), 'Survived = ' + str(alist[3]) ],\n                                 title=\"% of male survivors \" + \"(Total = \" + str(alist[1] + alist[3]) + \")\",\n                                 ax=ax1, labeldistance=None)\n\ndf1.query('Sex == \"F\"').plot.pie(y='Quantity', figsize=(10, 5), autopct='%1.2f%%', \n                                 labels = ['Not Survived = ' + str(alist[0]), 'Survived =' + str(alist[2]) ],\n                                title=\"% of female survivors \" + \"(Total = \" + str(alist[0] + alist[2]) + \")\", \n                                 ax=ax2, labeldistance=None)\n\nplt.show()","eccb392a":"f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 5), sharex=False)\nf.suptitle('Percentage of Sex that Survivors', fontsize=18)\n\nalist = df1['Quantity'].tolist()\n\ndf1.query('Survived == 0').plot.pie(y='Quantity', figsize=(10, 5), autopct='%1.2f%%',\n                                    labels = ['Female = ' + str(alist[0]), 'Male = ' + str(alist[1]) ],\n                                    title=\"% to sex of deaths \" + \"(Total = \" + str(alist[0] + alist[1]) + \")\",\n                                    ax=ax1, labeldistance=None)\n\ndf1.query('Survived == 1').plot.pie(y='Quantity', figsize=(10, 5), autopct='%1.2f%%',\n                                    labels = ['Female = ' + str(alist[2]), 'Male = ' + str(alist[3]) ],\n                                    title=\"% to sex of survivors \"  + \"(Total = \" + str(alist[2] + alist[3]) + \")\",\n                                    ax=ax2, labeldistance=None)\n\nplt.show()","2c3e4e69":"df2 = df.groupby([\"Survived\",\"Category\"]).count().reset_index().rename({\"Firstname\": \"Quantity\"}, axis=1)\ndf2","e50ab066":"f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 5), sharex=False)\nf.suptitle('Percentage of Categories that Survived', fontsize=18)\n\nalist = df2['Quantity'].tolist()\n\ndf2.query('Category == \"P\"').plot.pie(y='Quantity', figsize=(10, 5), autopct='%1.2f%%',\n                                      labels = ['Not Survived = ' + str(alist[1]), 'Survived = ' + str(alist[3]) ],\n                                      title=\"% of class P that survived \" + \"(Total = \" + str(alist[1] + alist[3]) + \")\",\n                                      ax=ax1, labeldistance=None)\n\ndf2.query('Category == \"C\"').plot.pie(y='Quantity', figsize=(10, 5), autopct='%1.2f%%',\n                                      labels = ['Not Survived = ' + str(alist[0]), 'Survived = ' + str(alist[2]) ],\n                                      title=\"% of class C that survived \" + \"(Total = \" + str(alist[0] + alist[2]) + \")\",\n                                      ax=ax2, labeldistance=None)\n\nplt.show()","89c24a19":"replace_list = {\"Sex\":{\"M\":0,\"F\":1}, \"Category\":{\"P\":0,\"C\":1}}\n\ndf_corr = df.replace(replace_list).drop(['PassengerId', 'Country','Firstname','Lastname'], axis = 1)\ndf_corr.head(3)","6ac10df9":"corr = df_corr.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax1 = plt.subplots(figsize=(8,6))\nsns.heatmap(corr, cmap='coolwarm_r', \n            annot=True, annot_kws={'size':15}, ax=ax1, mask=mask)\n\nax1.set_title(\"Correlation\", fontsize=14)\n\nplt.show()","c8e4035b":"eda_categ_feat_desc_plot(df['Survived'], 'Remember, is a Unbalanced DataSet: Survived Rate')","23472372":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder","cb3f0071":"# x = df.loc[:,[\"Sex\",\"Age\",\"Category\"]]\n# y = df.loc[:,[\"Survived\"]]\n\n# # OneHotEncoding\n# x = pd.get_dummies(x)\n\n# # Scaling Age\n# sc = StandardScaler()  # Normal Distribution: mean 0, min -1 and max 1\n# x['Age'] = sc.fit_transform(x['Age'].values.reshape(-1,1))\n\n# x.head(3)","cec16cf3":"x = df.loc[:,[\"Sex\",\"Age\",\"Category\",\"Country\"]]\ny = df.loc[:,[\"Survived\"]]\n\nx.head(1)","862e8f78":"# Separate Country in Estonia, Sweden and Others\nx['Country'] = x['Country'].apply(lambda x: 'Estonia' if x == 'Estonia' \n                                  else ('Sweden' if x == 'Sweden' else 'Others'))\n\nsc = StandardScaler()  # Normal Distribution: mean 0, min -1 and max 1\nx['Age'] = sc.fit_transform(x['Age'].values.reshape(-1,1))\n\nx = pd.get_dummies(x, drop_first=True) # Remove EstoniaColumn\n\nx.head(3)","39cb918e":"# x = df.loc[:,[\"Sex\",\"Age\",\"Category\",\"Country\", \"Lastname\"]]\n# y = df.loc[:,[\"Survived\"]]\n\n# x['Country']= [1 if el =='Estonia' or el =='Sweden' else 0 for el in x['Country']] \n\n# sc = StandardScaler()  \n# x['Age'] = sc.fit_transform(x['Age'].values.reshape(-1,1))\n\n# encode = LabelEncoder()\n# x['Sex']=encode.fit_transform(x['Sex'])\n# x['Lastname']=encode.fit_transform(x['Lastname'])\n# x['Category'] =encode.fit_transform(x['Category'])\n\n# x.head(3)","5f3dd0d8":"from sklearn.model_selection import KFold, StratifiedKFold\n\nkfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n\nfor train_index, test_index in kfold.split(x, y):\n    x_train, x_test = x.iloc[train_index].values, x.iloc[test_index].values\n    y_train, y_test = y.iloc[train_index].values, y.iloc[test_index].values\n\ncheck_balanced_train_test_binary(x_train, y_train, x_test, y_test, len(df), ['Death', 'Survives'])","2ba0f67b":"from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, SVMSMOTE, BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\nfrom imblearn.combine import SMOTEENN, SMOTETomek # over and under sampling\nfrom imblearn.metrics import classification_report_imbalanced\n\nimb_models = {\n    'ADASYN': ADASYN(),\n    'SMOTE': SMOTE(random_state=42),\n    'SMOTEENN': SMOTEENN(\"minority\", random_state=42),\n    'SMOTETomek': SMOTETomek(tomek=TomekLinks(sampling_strategy='majority')),\n    'RandomUnderSampler': RandomUnderSampler()\n}\n\nimb_strategy = \"SMOTE\"\n\nif(imb_strategy != \"None\"):\n    print(\"train dataset before\", x_train.shape[0])\n    print(\"imb_strategy:\", imb_strategy)\n\n    imb_tranformer = imb_models[imb_strategy]\n    \n    # x_train, y_train | xsm_train, ysm_train\n    xsm_train, ysm_train = imb_tranformer.fit_sample(x_train, y_train)\n\n    print(\"train dataset before\", xsm_train.shape[0],\n          'generate', xsm_train.shape[0] - x_train.shape[0] )\nelse:\n    print(\"Dont correct unbalanced dataset\")","b785c99b":"# use: x_train, y_train, x_test, y_test\n\n# Classifier Libraries\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n# Ensemble Classifiers\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Others Linear Classifiers\nfrom sklearn.linear_model import SGDClassifier, RidgeClassifier\nfrom sklearn.linear_model import Perceptron, PassiveAggressiveClassifier\n\n# xboost\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# scores\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\n\n# neural net of sklearn\nfrom sklearn.neural_network import MLPClassifier\n\n# others\nimport time\nimport operator","384075a3":"# def neural nets\nmlp = MLPClassifier(verbose = False, max_iter=1000, tol = 0.000010,\n                    solver = 'adam', hidden_layer_sizes=(100), activation='relu')\n\n# def classifiers\n\nnn_classifiers = {\n    \"Multi Layer Perceptron\": mlp\n}\n\nlinear_classifiers = {\n    \"SGDC\": SGDClassifier(),\n    \"Ridge\": RidgeClassifier(),\n    \"Perceptron\": Perceptron(),\n    \"PassiveAggressive\": PassiveAggressiveClassifier()\n}\n\ngboost_classifiers = {\n    \"XGBoost\": XGBClassifier(),\n    \"LightGB\": LGBMClassifier(),\n}\n\nclassifiers = {\n    \"Naive Bayes\": GaussianNB(),\n    \"Logisitic Regression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Machine\": SVC(),\n    \"Decision Tree\": DecisionTreeClassifier()\n}\n\nensemble_classifiers = {\n    \"AdaBoost\": AdaBoostClassifier(),\n    \"GBoost\": GradientBoostingClassifier(),\n    \"Bagging\": BaggingClassifier(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Extra Trees\": ExtraTreesClassifier()    \n}\n\nall_classifiers = {\n    \"Simple Models\": classifiers,\n    \"Ensemble Models\": ensemble_classifiers,\n    \"GBoost Models\": gboost_classifiers,\n    \"NeuralNet Models\": nn_classifiers,\n    \"Others Linear Models\": linear_classifiers,\n}\n\nmetrics = {\n    'cv_scores': {},\n    'acc_scores': {},\n    'f1_mean_scores': {},\n}","9253ffd7":"format_float = \"{:.4f}\"\n\nis_print = False # True\/False\n\ntime_start = time.time()\n\nprint(\"Fit Many Classifiers\")\n\nfor key, classifiers in all_classifiers.items():\n    if (is_print):\n        print(\"\\n{}\\n\".format(key))\n    for key, classifier in classifiers.items():\n        t0 = time.time()\n        # xsm_train, ysm_train || x_train, y_train\n        classifier.fit(x_train, y_train) \n        t1 = time.time()\n        # xsm_train, ysm_train || x_train, y_train\n        training_score = cross_val_score(classifier, x_train, y_train, cv=5) \n        y_pred = classifier.predict(x_test)\n        cv_score = round(training_score.mean(), 4) * 100\n        acc_score = accuracy_score(y_test, y_pred)\n        f1_mean_score = f1_score(y_test, y_pred, average=\"macro\") # average =  'macro' or 'weighted'\n        if (is_print):\n            print(key, \"\\n\\tHas a training score of\", \n                  cv_score, \"% accuracy score on CrossVal with 5 cv \")\n            print(\"\\tTesting:\")\n            print(\"\\tAccuracy in Test:\", format_float.format(acc_score))\n            print(\"\\tF1-mean Score:\", format_float.format(f1_mean_score)) \n            print(\"\\t\\tTime: The fit time took {:.2} s\".format(t1 - t0), '\\n')\n        metrics['cv_scores'][key] = cv_score\n        metrics['acc_scores'][key] = acc_score\n        metrics['f1_mean_scores'][key] = f1_mean_score\n        \ntime_end = time.time()\n        \nprint(\"\\nDone in {:.5} s\".format(time_end - time_start), '\\n')\n        \nprint(\"Best cv score:\", max( metrics['cv_scores'].items(), key=operator.itemgetter(1) ))\nprint(\"Best Accuracy score:\", max( metrics['acc_scores'].items(), key=operator.itemgetter(1) ))\nprint(\"Best F1 score:\", max( metrics['f1_mean_scores'].items(), key=operator.itemgetter(1) ))\n\nlists = [list(metrics['cv_scores'].values()),\n         list(metrics['acc_scores'].values()),\n         list(metrics['f1_mean_scores'].values())\n        ]\n\na_columns = list(metrics['cv_scores'].keys())\n\ndf_metrics = pd.DataFrame(lists , columns = a_columns,\n                    index = ['cv_scores', 'acc_scores', 'f1_scores'] )","ac9fb959":"lists = [list(metrics['cv_scores'].values()),\n         list(metrics['acc_scores'].values()),\n         list(metrics['f1_mean_scores'].values())\n        ]\n\na_columns = list(metrics['cv_scores'].keys())\n\ndfre1 = pd.DataFrame(lists , columns = a_columns,\n                    index = ['cv_scores', 'acc_scores', 'f1_scores'] )\n\ndfre1 = dfre1.T.sort_values(by=\"acc_scores\", ascending=False)\ndfre1","25d24552":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# AdaBoost\nadaB = AdaBoostClassifier()\nadaB.fit(x_train,y_train)\ny_pred = adaB.predict(x_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred, target_names=['Death', 'Survives']))","0775cbff":"from mlens.ensemble import SuperLearner\n\n# Define Models List\nmodels = list()\nmodels.append(LogisticRegression(solver='liblinear'))\nmodels.append(DecisionTreeClassifier())\nmodels.append(SVC(kernel='linear'))\nmodels.append(GaussianNB())\nmodels.append(KNeighborsClassifier())\nmodels.append(AdaBoostClassifier())    \nmodels.append(BaggingClassifier(n_estimators=100))\nmodels.append(RandomForestClassifier(n_estimators=100))\nmodels.append(ExtraTreesClassifier(n_estimators=100))\nmodels.append(XGBClassifier(scale_pos_weight=2))\n\n# Create Super Model\nensemble = SuperLearner(scorer=accuracy_score, folds=10, shuffle=False, sample_size=len(x))\nensemble.add(models)\nensemble.add_meta(DecisionTreeClassifier()) # can change\n\n# Fit \nensemble.fit(x_train, y_train)\nprint(ensemble.data)\n\n# Pred\ny_pred = ensemble.predict(x_test)\n\n# Evaluate\nclass_report(y_test, y_pred, name='SuperLeaner', labels=['Death', 'Survives'])\n\n# One time out: acc: 0.89, f1: 0.59","52ebcf59":"import pickle\nPkl_Filename = \"Pickle_Model.pkl\"  \n\n# Save the Modle to file in the current working directory\nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(ensemble, file)\n    \n# Load the Model back from file\nwith open(Pkl_Filename, 'rb') as file:  \n    Pickled_ensemble = pickle.load(file)\n\nPickled_ensemble","14cf4f42":"<span style='font-size: 15pt'>Relationship Sex, Age and Survived<\/span>","82626fd6":"<span style='font-size: 15pt'>Category<\/span>","a641c6dc":"## Import Libs and DataSet Snippets <a id='index01'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","01ff7f5f":"<span style='font-size: 15pt'>Age<\/span>","19f2bfb1":"<span style='font-size: 15pt; color: darkblue;'>Conclusion<\/span>\n\nAnalyzing the BoxPlots and ViolinPlots, we can see that younger people tended to survive, and this occurs mainly for women\n\n<!--\nAnalisandos os BoxPlots e ViolinPlots podemos perceber que pessoas mais jovens tenderam a sobreviver, e isso ocorre principalmente para as mulheres\n-->\n\n<span style='font-size: 15pt'>features x Survived<\/span>","082327d3":"<span style='font-size: 15pt'>Sex<\/span>","7594de31":"<span style='font-size: 15pt; color: darkblue;'>Conclusion<\/span>\n\n+ `PassengerId`:\n   - Number and unique values for each passenger. Except 2 Ids, \\[463, 767 \\] these divided between a man and a woman of class mems, probably known. The curious thing is that, even though Ids are different and of different categories \\[P (Passenger - Passenger), C (Crew - Technical Team)\\] in both cases the woman survived and the man did not.\n+ `Country`:\n   - 90% of Estonia (35%) and Sweden (55%) and the rest of other countries\n+ `Firstname`:\n   - Unique values, there is nothing to be analyzed\n+ `Lastname`:\n   - Unique values, there is nothing to be analyzed\n+ `Sex`:\n   - On the ship, there was the same disposition between men and women\n+ `Age`:\n   - Most adults, with children and old people. Ages range from 0 years to 80\n+ `Category`:\n   - About 80% class P and 20% class C\n+ `Survived`: Class\n   - 806% died and 14% survived\n\n\n*PassengerId*, *Firstname*, *Lastname* will no longer be analyzed for not providing relevant information to the problem\n\n<!--\n\ud83c\udde7\ud83c\uddf7\n\n+ `PassengerId`:\n  -  N\u00famerico e valores \u00fanicos para cada passageiros. Exceto 2 Ids, \\[463, 767\\] esses divididos entre um homem e uma mulher de mems classe, provavelmente conhecidos. O curioso \u00e9 que, mesmo sendo Ids diferente e de categorias diferentes \\[P (Passenger - Passageiro), C (Crew - Equipe T\u00e9cnica)\\] em ambos os casos a mulher sobreviveu e o homem n\u00e3o.\n+ `Country`: \n  -  90% da Estonia (35%) e Su\u00e9cia (55%) e o restante de outros paises\n+ `Firstname`:\n  - Valores \u00fanicos, n\u00e3o h\u00e1 o que ser analizado\n+ `Lastname`:\n  - Valores \u00fanicos, n\u00e3o h\u00e1 o que ser analizado\n+ `Sex`:\n  - No navio, havia mesma disposi\u00e7\u00e3o entre homens e mulheres\n+ `Age`:\n  - Maioria Adultos, com crian\u00e7as e velhos. Idades variam entre 0 anos a 80\n+ `Category`:\n  - Cerca de 80% classe P e 20% classe C\n+ `Survived`: Classe\n  - 806% morreram e 14% sobreviveram\n\n\n*PassengerId*, *Firstname*, *Lastname* n\u00e3o ser\u00e3o mais analizados por n\u00e3o oferencerem informa\u00e7\u00e3o relevante ao problema\n-->","8f329d50":"### Missing Data","b5f071af":"## Split in Train Test a Unbalanced DataSet <a id='index24'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","dde4a870":"<span style='font-size: 15pt'>Country<\/span>","0becdc8d":"### With Survived <a id='index02'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n","217c0d79":"### Each Feature Individually <a id='index06'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","459a8c2d":"### LastName Pre-Processing\n\nInclude LastName encodig with label encondig (0,1,2,3 ... to each unique value)\n\nMade Poor Results","d9c64276":"## Save and Load Model <a id='index29'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","d1fc2bba":"## Conclusion <a id='index30'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\nThe best strategy to achieve the highest accuracy was:\n+ Pre-Processing\n   - OneHotEnconding to Country and remove Estonia Column.\n   - LabelEnconding to 'Sex' and 'Category'\n   - StandScale to 'Age'\n+ Do not use any corrected unbalanced data set technique because it decreases accuracy to increase f1-score\n+ The best model was the SuperLeaner that combines several models, while individually it was AdaBoost\n\n**Nexts Steps**\n\n+ Tuning AdaBoost and SuperLeaner","36acada6":"## Snippets <a id='index02'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","3766dc6a":"As correla\u00e7\u00f5es s\u00e3o em sua maioria fraca, mas a melhor \u00e9 'Age'\n\n## Unbalanced DataSet <a id='index09'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\nSignifica que, se se chutarmos sempre que todo mundo vai morrer, ent\u00e3o: Nunca teriamos acertariamos que alguem iria sobreveier e teriamos um score de 87% que \u00e9 alto. Uma m\u00e9trica melhor para avliar as duas tarefas do modelo: Classifica se sobrivive ou n\u00e2o corretamente, \u00e9 usar f1-score que avalia precisao (acerta) e recall (detectar)","7bf280a7":"## Table Of Contents (TOC) <a id=\"top\"><\/a>\n\n+ [Import Libs and DataSet](#index01) \n+ [Snippets](#index02)\n+ [EDA](#index03)\n  - [Check Missing Data](#index04)\n  - [Check Duplicate Data](#index05)\n  - [Each Feature Individually](#index06)\n  - [With Survived](#index07)\n+ [Correlation](#index11)\n+ [Unbalanced DataSet](#index09)\n+ [Pre-Processing](#index23)\n+ [Split in Train Test a Unbalanced DataSet](#index24)\n+ [Test various ways to correct unbalanced dataset](#index25)\n+ [Fit multiple models and Compare](#index26)\n+ [Best Individual Model: AdaBoost](#index27)\n+ [Super Leaner (MLens)](#index28)\n+ [Save and Load a Model](#index29)\n+ [Conclusion](#index30)\n","c9818c1f":"## Test various ways to correct unbalanced dataset <a id='index25'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\nDont produce good results, decreases accuracy and increases f1 score\n\nTo use just exchange x_train, y_train by xsm_train, ysm_train","e7b3a188":"<div style=\"text-align: center;\">\n\n# Estonia Disaster Passanger List: EDA + Classification\n\n<h3 align=\"center\">Made by \ud83d\ude80 <a href=\"https:\/\/www.kaggle.com\/rafanthx13\"> Rafael Morais de Assis<\/a><\/h3>\n    \n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/1a\/MS_Estonia_model.jpg\" width=\"50%\"\/>\n    \n<\/div> <br>\n\n\nCreated: 2020-08-21; \n\nLast updated: 2020-08-23;\n","3a4b5ec3":"<span style='font-size: 15pt'>PassengerId<\/span>","c0811d06":"### Commun Pre-Processing\n\nTake Sex, Age and Category\n\nMedium Results","a595ee80":"## Best Individual Model: AdaBoost <a id='index27'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","3727f76b":"## Correlation <a id='index11'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","f59b828a":"<span style='font-size: 15pt'>Lastname<\/span>","9186b38d":"<span style='font-size: 15pt'>Firstname<\/span>","ee43458d":"## Super Learner (MLens) <a id='index28'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\nreference: https:\/\/www.kaggle.com\/nishantbhadauria\/meta-learner-with-mlens-for-predicting-survival\n\nlink to see more of SuperLeaner of mlens\n+ https:\/\/machinelearningmastery.com\/super-learner-ensemble-in-python\/\n+ http:\/\/flennerhag.com\/\n\nGenerates different results each time it is run","b4c46af0":"## Fit multiple models and Compare <a id='index26'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","08a1ab23":"### Country Influence Pre-Processing (BEST)\n\nBased in https:\/\/www.kaggle.com\/abhijithchandradas\/pen-random-forest-model-beating-baseline-88-47\n\nMake good Results","0a8c9bda":"<span style='font-size: 15pt; color: darkblue;'>Conclusion<\/span>\n\nPercentage of Survivors by Sex\n+ Men tended to outlive women\n\nPercentage of Sex that Survivors\n+ The percentage of deaths is similar between genders, 43% of women died and 46% of men died, but of the survivors, the majority who survived were men\n+ probably, the previous graphics of violin plot that spoke of the young age of the women who survived, referred to a few women\n\nPercentage of Categories that Survived\n+ Although more passengers have survived (because there are 4x more passengers than people on the technical team), the technical team (Crew) had a higher survival rate.\n\n\n<!--\nPercentage of Survivors by Sex\n+ Os homens tenderam a sobreviver mais que as mulheres\n\nPercentage of Sex that Survivors\n+ A porcentagem de mortes \u00e9 parecida entre os sexo, 43% das mulhres morreram e 46% dos homens morreram, mas dos sobreviventes, a maioria que sobreviveu foram homens\n+ provavelmente, os graficos anteriores de violin plot que falaram de pouca idade das mulheres que sobreviream, se referiam a poucas mulhres\n\nPercentage of Categories that Survived\n+ Apesar de mais passageiros terem sobrevivido (Pois h\u00e1 4x mais passageiros que gente da equipe t\u00e9cnica), a equipe t\u00e9cnica (Crew) apresentram uma maior taxa de sobreviv\u00eancia.\n-->","98fde6ff":"## Kaggle Description\n\n[Kaggle Link](https:\/\/www.kaggle.com\/christianlillelund\/passenger-list-for-the-estonia-ferry-disaster)\n\nRead more: https:\/\/en.wikipedia.org\/wiki\/MS_Estonia\n\n### Facts\n\nWhen was the Sinking of the Estonia: September 28, 1994\nWhere was the Sinking of the Estonia: Near the Turku Archipelago, in the Baltic Sea\nWhat was the Sinking of the Estonia death toll: 852 passengers and crew\n\nInteresting things to investigate about the data:\n\nWho's more likely to survive the sinking based on data?\nIs age an indicator for survival?\nIs gender an indicator for survival?\nDid the crew aboard have a higher chance of survival than passengers?\nSince the death toll is well above 80%, can you make a classifier that beats the baseline (all passengers died)?\n\n### Data Dictionary\n\n| Variable  | Definition                              | Key                     |\n| :-------- | :-------------------------------------- | :---------------------- |\n| Country   | Country of origin                       |                         |\n| Firstname | Firstname of passenger                  |                         |\n| Lastname  | Lastname of passenger                   |                         |\n| Sex       | Gender of passenger                     | M = Male, F = Female    |\n| Age       | Age of passenger at the time of sinking |                         |\n| Category  | The type of passenger                   | C = Crew, P = Passenger |\n| Survived  | Survival                                | 0 = No, 1 = Yes   ","f5350dc3":"## Exploratory Data Analysis <a id='index03'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","3e27fb3e":"<span style='font-size: 15pt'>Survived<\/span>","978747fa":"\n\n## Pre-Processing <a id='index23'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n","387d87a8":"### Check Duplicate\n"}}