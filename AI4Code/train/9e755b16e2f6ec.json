{"cell_type":{"55cd3766":"code","3186f1f4":"code","f1c1828e":"code","efc4aed3":"code","af3aff55":"code","475168b4":"code","e79c9ef9":"code","0e48edbf":"code","a6d67863":"code","b6f4e1ac":"code","a99887ce":"code","f31fa03c":"code","87b3dab9":"code","09a36622":"code","a237a356":"code","5273789b":"code","67343051":"code","74efdaba":"code","f60e047a":"code","1826eab0":"code","d249b988":"code","8f5e8325":"code","0d41cf55":"markdown","0dd7cb0a":"markdown","313a7283":"markdown","0ce1c4c4":"markdown","02ed25b6":"markdown","023c24c1":"markdown","79f3156a":"markdown","9a4523a5":"markdown","1a7039da":"markdown","9f6cdc5b":"markdown","34cb4c45":"markdown","7226d8cd":"markdown","3eb5617c":"markdown","7ed327d4":"markdown","12c05cd7":"markdown","e9c819ef":"markdown","1d13126c":"markdown","6055429a":"markdown","c0e151b9":"markdown","f5b0c089":"markdown","eee6f36a":"markdown","1ed472cc":"markdown","3bc940fd":"markdown","0a856239":"markdown","f0d07458":"markdown","a6c50d97":"markdown","e8620078":"markdown","f974e352":"markdown"},"source":{"55cd3766":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity='all'\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import cross_val_score","3186f1f4":"import os\nos.listdir(\"..\/input\")\n","f1c1828e":"train=pd.read_csv(\"..\/input\/ghouls-goblins-and-ghosts-boo\/train.csv\")\ntest=pd.read_csv(\"..\/input\/ghouls-goblins-and-ghosts-boo\/test.csv\")","efc4aed3":"train.shape \ntrain.head()\n\n\n","af3aff55":"train.drop('id',axis=1,inplace=True)\ntest1=test.drop('id',axis=1)\n","475168b4":"train.describe() ","e79c9ef9":"train.isnull().sum()\n","0e48edbf":"train.dtypes","a6d67863":"train['type'].value_counts()","b6f4e1ac":"from sklearn.preprocessing import LabelEncoder\nlabel=LabelEncoder()\ntrain['color']=label.fit_transform(train['color'])\ntrain['type']=label.fit_transform(train['type'])\n\ntest1['color']=label.fit_transform(test1['color'])\n\n\n\n","a99887ce":"sns.pairplot(train,hue='type') ","f31fa03c":"sns.heatmap(train.corr(),annot=True,vmin=-1,vmax=1,cmap='RdYlGn')","87b3dab9":"train.drop('color',axis=1,inplace=True)\ntest1.drop('color',axis=1,inplace=True)","09a36622":"X=train.drop('type',axis=1)\ny=train['type']","a237a356":"from sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\n\nClassification_models=[('LogisticRegression',LogisticRegression()),('StochasticGDC',SGDClassifier()),('KNC',KNeighborsClassifier()),('SVC',SVC()),\n                       ('LinearSVC',LinearSVC()),('GNaiveBayes',GaussianNB()),('MNaiveBayes',MultinomialNB()),('DTree',DecisionTreeClassifier()),\n                       ('MLPerceptronC',MLPClassifier()),('RF',RandomForestClassifier()),('ET',ExtraTreesClassifier()),('AdaBoostC',AdaBoostClassifier()),\n                       ('GBC',GradientBoostingClassifier()),('XGBC',XGBClassifier())]","5273789b":"result=[]\nnames=[]\nfor name,model in Classification_models:\n    cvresult=cross_val_score(model,X,y,cv=5,n_jobs=-1,scoring = 'accuracy')\n    result.append(cvresult.mean())\n    names.append(name)\n    print(\"%s gives %f \" % (name, cvresult.mean()))","67343051":"params={'C':[0.01,0.1,1],'gamma':[1,0.1,0.01],'kernel':['linear', 'poly', 'rbf'] }\nfrom sklearn.model_selection import GridSearchCV\ngrid=GridSearchCV(SVC(),param_grid=params,n_jobs=-1,cv=5)\ngridfit=grid.fit(X,y)","74efdaba":"gridfit.best_score_\ngridfit.best_params_\n","f60e047a":"\nvc=VotingClassifier(estimators=[('Support Vector Classifier',SVC(C=1, gamma=1, kernel='linear')),\n                                ('Gaussian Naive Bayes',GaussianNB())])\n\nvc.fit(X,y)\npredictions = vc.predict(test1)","1826eab0":"submission = pd.DataFrame({'id':test['id'], 'type':predictions})","d249b988":"submission['type']=label.inverse_transform(submission['type'])","8f5e8325":"submission.to_csv('submission.csv', index=False)","0d41cf55":"Plotting a correlation matrix using heatmap by seaborn tells us the same thing about the 'color' feature. \n\n","0dd7cb0a":"Let's get the name of the folder that contains the train, test and the sample submission files","313a7283":"**Happy Halloween 2019!**\n","0ce1c4c4":"Finally saving the submission dataframe as a 'csv' file.","02ed25b6":"SVC and Gaussian Naive Bayes seem to be giving the best results (Remember that we had noticed in the pairplot that the features follow Gaussian distributions?)\n\nLet's now try to tinker with the hyperparameters of SVC with the help of GridSearchCV (GaussianNB doesn't have any hyperparameters that you can further tune) to see if we can better our score.\n","023c24c1":"Let's get a few descriptive stats of our train dataset. ","79f3156a":"We now have to separate the features (independant variables) and the target feature (dependant variable) ","9a4523a5":"The best score and the SVC parameters that gave the best score are: ","1a7039da":"How is the target variable distributed?","9f6cdc5b":"![halloween-dogs.jpg](attachment:halloween-dogs.jpg)\n\nI wonder which Machine Learning model would classify these three guys correctly as ghosts though ;)\n\n*Cheers!*","34cb4c45":"Let's encode the two 'object' data type columns ('color' and 'type') in both Train and Test sets into numerical types ","7226d8cd":"There are three classes in the target variable which are fairly evenly distributed.   \n\n**Accuracy** as a scoring metric would be sufficient","3eb5617c":"Let's check out the dimensions and also have a look at the first few rows of the Train dataset","7ed327d4":"Do we have any missing values across all our featues?","12c05cd7":"Import both the Train and Test 'csv' files as Pandas DataFrames ","e9c819ef":"The submission must be made in the form of a Pandas Dataframe with two columns - **id** and the predicted **type**","1d13126c":"We'll make use of VotingClassifier on the predictions of SVC and GaussianNB to make our final predicitons on the test set ","6055429a":"What are the data types of the values these featues hold","c0e151b9":"So we have five weird features - **Bone_length, rotting_flesh, hair_length, has_soul and color** to find out the type of the creature.\nAlso, the 'id' column is just moot. Let's drop that","f5b0c089":"The features **color** and **type** are of the 'object' type. We will have to convert them into numerical values before feeding them to the Machine Learning algorithms","eee6f36a":"**ZERO!** There are no missing values.","1ed472cc":"Let's just drop 'color'","3bc940fd":"Here's us importing quite a few popular Classification algorithms to try them all!","0a856239":"Since we see that the values of all the features range just between 0 to 1 we don't need to implement any data scaling methods.","f0d07458":"pairplot by Seaborn shows that 'color' may not be of much help in seperation of the classes of target variable.\n\nAlso, notice that the features fairly follow Gaussian distributions (There are algorithms that assume that the data being fed has a Gaussian distribution and performs better when that's true)","a6c50d97":"Here comes that step without which we get the coveted 'perfect score' of 0.0000 (Hence the title of this notebook)\n\nThis being my very first submission on Kaggle, I was ready to get a poor score but getting a 0.0000 stumped me. After about an hour of searching for mistakes I realised what the issue was.\n\nThe predictions we make for **type** is in the form of numericals (0,1 or 2) whereas Kaggle expects it to be 'Ghoul', 'Goblin' or 'Ghost'.\n(This notebook gets a decent score of 0.733 now after setting that right)\n\n**Moral of the story**- Always take a look at the 'sample_submission' file and don't miss the next step.\n","e8620078":"Without getting into the details of hyperparameters of any of these classification algorithms lets see how they perform on our data. \n\nA simple train_test split may give you a very optimistic score compared to what you may get on new unseen data. Here we use cross validation to get more realistic estimates.","f974e352":"Diving right in and import a few necessary libraries for this problem."}}