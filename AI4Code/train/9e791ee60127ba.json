{"cell_type":{"08287db6":"code","71fc6a53":"code","6110d46e":"code","904ec07b":"code","a937e6b6":"code","03c3c626":"code","7baa4ccf":"code","a94a2056":"code","479e3818":"code","2ed3a955":"code","47f47e1c":"code","fe5bdd04":"code","fd0b0913":"code","2fd1aa76":"code","d4e3d31c":"code","d856951c":"code","c4eb7db1":"code","7d2d63ad":"code","0eb47ced":"code","6b50a493":"code","1d663a1c":"code","de1cc5ad":"code","30bf7533":"code","30b47a0b":"code","9fc2e91c":"code","33828ce4":"code","44f03585":"code","cb17fe12":"code","bbac65da":"code","01b28f9e":"code","ccb5c814":"code","6d8388ed":"code","4df8db51":"code","b06e6a30":"code","3be94bc4":"code","dac7395b":"code","cb4badd5":"code","2dcda166":"code","f5a86a56":"code","b525f7d9":"code","8ad3eb3f":"code","4fe996b3":"code","521a338f":"code","9b0bee81":"markdown","a68a4336":"markdown","b42581b5":"markdown","108a0dee":"markdown","15399e5d":"markdown","63c65ef0":"markdown","ded30fc3":"markdown","deb8d783":"markdown"},"source":{"08287db6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom math import sqrt","71fc6a53":"df = pd.read_csv('..\/input\/amazon-product\/ratings_Beauty.csv')","6110d46e":"df.head()","904ec07b":"df.rename(columns = {'UserId': 'User'},inplace=True)    ","a937e6b6":"df.head()","03c3c626":"df.shape","7baa4ccf":"df=df.dropna()","a94a2056":"df.shape","479e3818":"df.drop(\"Timestamp\",axis=1,inplace=True)","2ed3a955":"df.info()","47f47e1c":"df1= df.iloc[:20000,0:]","fe5bdd04":"df1.info()","fd0b0913":"df1[\"Rating\"].describe()","2fd1aa76":"most_rated=pd.DataFrame(df1.groupby('User')['Rating'].count().sort_values(ascending = False)[:10])\nprint('Top 10 users based on their ratings: \\n',most_rated)","d4e3d31c":"counts = pd.DataFrame(df1['Rating'].value_counts()).reset_index()\ncounts.columns = ['Labels', 'Ratings']\ncounts\n","d856951c":"\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,7))\nsns.countplot(df1['Rating'], ax=ax1)\nax1.set_xlabel('Rating Distribution', fontsize=10)\nax1.set_ylabel('Count', fontsize=10)\n\n\nexplode = (0.1, 0, 0.1, 0, 0)\nax2.pie(counts[\"Ratings\"], explode=explode, labels=counts.Labels, autopct='%1.2f%%',\n        shadow=True, startangle=70)\nax2.axis('equal')\nplt.title(\"Rating Ratio\")\nplt.legend(counts.Labels, loc=3)\nplt.show()","c4eb7db1":"def popularity(train):\n    popular = pd.DataFrame(train.groupby('ProductId')['Rating'].count())\n    most_popular = popular.sort_values('Rating', ascending=False)\n    most_popular['Rank'] = most_popular['Rating'].rank(ascending=False, method='first')\n    \n    return most_popular","7d2d63ad":"popularity(df1).head(20).plot(kind = \"bar\")","0eb47ced":"ratings=df1.pivot(index='User', columns=\"ProductId\", values=\"Rating\")\nratings.shape","6b50a493":"ratings=ratings.fillna(0).values","1d663a1c":"sparsity = float(len(ratings.nonzero()[0]))\nsparsity \/= (ratings.shape[0] * ratings.shape[1])\nsparsity *= 100\nprint ('Percentage of user-items that have a rating: {:.2f}%'.format(sparsity))","de1cc5ad":"def train_test_split(ratings):\n    \n    validation = np.zeros(ratings.shape) #created a copy of ratings with all values se to 0\n    \n    train = ratings.copy()               #copied the ratings in train\n    \n    for user in np.arange(ratings.shape[0]):\n        if len(ratings[user,:].nonzero()[0])>=5:\n            val_ratings = np.random.choice(ratings[user, :].nonzero()[0],size=3,replace=False)\n            train[user, val_ratings] = 0 #set value to 0 in training array\n            validation[user, val_ratings] = ratings[user, val_ratings] \n    print(validation.shape)\n    return train, validation","30bf7533":"train, val = train_test_split(ratings)","30b47a0b":"def rmse(prediction, ground_truth):\n    prediction = prediction[ground_truth.nonzero()].flatten() \n    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n    return sqrt(mean_squared_error(prediction, ground_truth))","9fc2e91c":"def mae(prediction, ground_truth):\n    prediction = prediction[ground_truth.nonzero()].flatten() \n    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n    return mean_absolute_error(prediction,ground_truth)","33828ce4":"def finalize(estimation):\n    for x in np.ndindex(estimation.shape):\n        if estimation[x] >= 5 :\n            estimation[x] = 5\n        elif estimation[x] < 0 :\n            estimation[x] = 0","44f03585":"from numpy import linalg as LA","cb17fe12":"def cosinesimilarity(ratings, kind='user', epsilon=1e-9):   # epsilon -> small number for handling dived-by-zero errors\n    if kind == 'user':\n        sim = ratings.dot(ratings.T) + epsilon\n        norms=np.array(LA.norm(ratings)*LA.norm(ratings.T))\n    elif kind == 'item':\n        sim = ratings.T.dot(ratings) + epsilon\n        norms = np.array(LA.norm(ratings.T)*LA.norm(ratings))\n    return (sim \/ norms )","bbac65da":"user_similarity = cosinesimilarity(train, kind='user')\nitem_similarity = cosinesimilarity(train, kind='item')\n\n","01b28f9e":"def predict_nobias(ratings, similarity, kind='user'):\n    if kind == 'user':\n        user_bias = ratings.mean(axis=1)\n        ratings = (ratings - user_bias[:, np.newaxis]).copy()\n        pred = similarity.dot(ratings) \/ np.array([np.abs(similarity).sum(axis=1)]).T\n        pred += user_bias[:, np.newaxis]\n    elif kind == 'item':\n        item_bias = ratings.mean(axis=0)\n        ratings = (ratings - item_bias[np.newaxis, :]).copy()\n        pred = ratings.dot(similarity) \/ np.array([np.abs(similarity).sum(axis=1)])\n        pred += item_bias[np.newaxis, :]\n        \n    return pred","ccb5c814":"userPrediction = predict_nobias(train, user_similarity, kind='user')\nitemPrediction = predict_nobias(train, item_similarity, kind='item')\n","6d8388ed":"print ('User-based bias-adjusted CF RMSE: %.2f' %rmse(userPrediction, val))\nprint ('User-based bias-adjusted CF MAE: %.2f' %mae(userPrediction, val))\nprint ('Item-based bias-adjusted CF RMSE: %.2f' %rmse(itemPrediction, val))\nprint ('Item-based bias-adjusted CF MAE: %.2f' %mae(itemPrediction, val))","4df8db51":"from scipy.sparse.linalg import svds","b06e6a30":"u, s, vt = svds(train, k = 10)\ns_diag_matrix=np.diag(s)\nX_pred = np.dot(np.dot(u, s_diag_matrix), vt)","3be94bc4":"print ('SVD CF RMSE: %.2f' %rmse(X_pred, val))\nprint ('SVD CF MAE: %.2f' %mae(X_pred, val))","dac7395b":"#P is latent user feature matrix\n#Q is latent item feature matrix\ndef prediction(P,Q):\n    return np.dot(P.T,Q)","cb4badd5":"lmbda = 0.4 # Regularization parameter\nk = 3\nn_epochs = 100  # Number of epochs\nalpha=0.001  # Learning rate\nm, n = train.shape  # Number of users and items\n\n\nP = 3 * np.random.rand(k,m) # Latent user feature matrix\nQ = 3 * np.random.rand(k,n) # Latent item feature matrix\n\ntrain_errors_rmse = []\nval_errors_rmse = []\n\ntrain_errors_mae = []\nval_errors_mae = []\n\n#Only consider items with ratings \nusers,items = train.nonzero()      \nfor epoch in range(n_epochs):\n    for u, i in zip(users,items):\n        error = train[u, i] - prediction(P[:,u],Q[:,i])  # Calculate error for gradient update\n        P[:,u] += alpha * ( error * Q[:,i] - lmbda * P[:,u]) # Update latent user feature matrix\n        Q[:,i] += alpha * ( error * P[:,u] - lmbda * Q[:,i])  # Update latent item feature matrix\n    \n    train_rmse = rmse(prediction(P,Q),train)\n    val_rmse = rmse(prediction(P,Q),val) \n    train_mae = mae(prediction(P,Q),train)\n    val_mae = mae(prediction(P,Q),val)\n    \n    train_errors_rmse.append(train_rmse)\n    val_errors_rmse.append(val_rmse)\n    train_errors_mae.append(train_mae)\n    val_errors_mae.append(val_mae)","2dcda166":"\n%matplotlib inline\n\nplt.plot(range(n_epochs), train_errors_rmse, marker='o', label='Training Data');\nplt.plot(range(n_epochs), val_errors_rmse, marker='v', label='Validation Data');\nplt.xlabel('Number of Epochs');\nplt.ylabel('RMSE');\nplt.legend()\nplt.grid()\nplt.show()","f5a86a56":"plt.plot(range(n_epochs), train_errors_mae, marker='o', label='Training Data');\nplt.plot(range(n_epochs), val_errors_mae, marker='v', label='Validation Data');\nplt.xlabel('Number of Epochs');\nplt.ylabel('MAE');\nplt.legend()\nplt.grid()\nplt.show()","b525f7d9":"SGD_prediction=prediction(P,Q)\nestimation= SGD_prediction[val.nonzero()]\nground_truth = val[val.nonzero()]\nfinalize(estimation)\nresults=pd.DataFrame({'prediction':estimation, 'actual rating':ground_truth})\n","8ad3eb3f":"print('SGD RMSE value : %.2f'%rmse(estimation,ground_truth))\nprint('SGD MAE value : %.2f'%mae(estimation,ground_truth))","4fe996b3":"maes = [mae(userPrediction, val), mae(itemPrediction, val), mae(X_pred, val),mae(estimation,ground_truth)]\nalgos = ['cosineUser', 'cosineItem', \"SVD\", 'SGD']\nplt.plot(algos, maes, 'go',  )\nplt.xlabel(\"Different algos\")\nplt.ylabel(\"MAE\")\nplt.show()\n","521a338f":"rmses = [rmse(userPrediction, val), rmse(itemPrediction, val), rmse(X_pred, val),rmse(estimation,ground_truth)]\nalgos = ['cosineUser', 'cosineItem', \"SVD\", 'SGD']\nplt.plot(algos, rmses, 'go',  )\nplt.xlabel(\"Different algos\")\nplt.ylabel(\"RMSE\")\nplt.show()\n","9b0bee81":"# **Model Based Collaborative Filtering**","a68a4336":"# **Stochastic Gradient Descent**","b42581b5":"# **Popularity Based Recommender**","108a0dee":"# SVD","15399e5d":"# **Sparsity of Matrix**","63c65ef0":"# **Cosine Similarity Memory Based Collaborative Fitering**","ded30fc3":"# **Defining Train Test Split**","deb8d783":"# **RMSE and MAE Function**"}}