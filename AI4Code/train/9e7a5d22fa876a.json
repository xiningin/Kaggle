{"cell_type":{"f30197a8":"code","5a43ad83":"code","6af7a6d1":"code","578c87a6":"code","136a5ede":"code","736e66d3":"code","5158a9d0":"code","fba8c4cc":"code","3fad1fd4":"code","6d695d4c":"code","c206e190":"code","30fd3e60":"code","7be98511":"code","dc8b70c7":"code","af602e59":"code","78a0c524":"code","9c07d8ef":"code","e1e64399":"code","469de3d3":"markdown","e3790106":"markdown","a75c8848":"markdown","f576fd2f":"markdown","010b0650":"markdown","f19ea78a":"markdown","aef65143":"markdown","f21f0ebc":"markdown","d027e807":"markdown","0b37c7a9":"markdown","7e8842e9":"markdown"},"source":{"f30197a8":"import pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image, ImageDraw\nfrom ast import literal_eval\nimport matplotlib.pyplot as plt","5a43ad83":"!ls ..\/input\/global-wheat-detection","6af7a6d1":"root_path = \"..\/input\/global-wheat-detection\/\"\ntrain_folder = os.path.join(root_path, \"train\")\ntest_folder = os.path.join(root_path, \"test\")\ntrain_csv_path = os.path.join(root_path, \"train.csv\")\nsample_submission = os.path.join(root_path, \"sample_submission.csv\")","578c87a6":"df = pd.read_csv(train_csv_path)","136a5ede":"df.head()","736e66d3":"df.shape[0]","5158a9d0":"df['width'].unique() == df['height'].unique() == [1024]","fba8c4cc":"def get_bbox_area(bbox):\n    bbox = literal_eval(bbox)\n    return bbox[2] * bbox[3]","3fad1fd4":"df['bbox_area'] = df['bbox'].apply(get_bbox_area)","6d695d4c":"df['bbox_area'].value_counts().hist(bins=50)","c206e190":"unique_images = df['image_id'].unique()","30fd3e60":"num_total = len(os.listdir(train_folder))\nnum_annotated = len(unique_images)\n\nprint(f\"There are {num_annotated} annotated images and {num_total - num_annotated} images without annotations.\")","7be98511":"sources = df['source'].unique()\nprint(f\"There are {len(sources)} sources of data: {sources}\")","dc8b70c7":"df['source'].value_counts()","af602e59":"plt.hist(df['image_id'].value_counts(), bins=30)\nplt.show()","78a0c524":"def show_images(images, num = 5):\n    \n    images_to_show = np.random.choice(images, num)\n\n    for image_id in images_to_show:\n\n        image_path = os.path.join(train_folder, image_id + \".jpg\")\n        image = Image.open(image_path)\n\n        # get all bboxes for given image in [xmin, ymin, width, height]\n        bboxes = [literal_eval(box) for box in df[df['image_id'] == image_id]['bbox']]\n\n        # visualize them\n        draw = ImageDraw.Draw(image)\n        for bbox in bboxes:    \n            draw.rectangle([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]], width=3)\n\n        plt.figure(figsize = (15,15))\n        plt.imshow(image)\n        plt.show()","9c07d8ef":"show_images(unique_images)","e1e64399":"for source in sources:\n    print(f\"Showing images for {source}:\")\n    show_images(df[df['source'] == source]['image_id'].unique(), num = 3)","469de3d3":"All of the annotated images have resolution 1024 x 1024","e3790106":"What can we tell from visualizations:\n\n* there are plenty of overlappind bounding boxes\n* all photos seem to be taken vertically \n* all plants are can be rotated differently, there is no single orientation. this means that different flip and roration agumentations should probably help\n* colors of wheet heads are quite different and seem to depend a little bit on the source\n* wheet heads themselves are seen from very different angles of view relevant to the observer","a75c8848":"As organizers say, there are many bounding boxes for each image, and not all images include wheat heads \/ bounding boxes.","f576fd2f":"Loook at photos by their source:","010b0650":"## Some basic statistics","f19ea78a":"<img src=\"https:\/\/cdn11.img.sputnik.by\/images\/102461\/23\/1024612300.jpg\" width=\"400\" height=\"400\">","aef65143":"## Visualizing images","f21f0ebc":"Max number of bounding boxes is 116, whereas min (annotated) number is 1 ","d027e807":"## Loading the data","0b37c7a9":"Let's see all the unique sources of data:","7e8842e9":"Let's look at how many bounding boxes do we have for each image:"}}