{"cell_type":{"aefd7a05":"code","737d16b5":"code","fd2e0f65":"code","f2fa471b":"code","6c993d59":"code","1371467e":"code","9dbe31be":"code","00a8c5df":"code","41629a1b":"code","ceec78b2":"code","9dbff251":"code","16552836":"code","3027902b":"code","b9339834":"code","b78fa75e":"code","599e34e2":"code","39b7795f":"code","e36b4452":"code","68f47448":"code","c7cc65ae":"code","1a27cc24":"code","823aa103":"code","a8fcabd5":"code","898ee4b8":"code","a9a7a8b9":"code","252243d4":"code","98e734c9":"code","b7c5eb7f":"code","de08df05":"code","76bdd87b":"code","52d0710c":"code","c4656422":"code","0c7ece22":"code","f3c32556":"code","3a4a1521":"code","ec765bb9":"code","a89b0c1a":"code","802e50d3":"code","460f9a60":"code","f81b5e32":"markdown","6e1f7d46":"markdown","cb79ae36":"markdown","53c30260":"markdown","4239f532":"markdown","e002f66b":"markdown","be6f173d":"markdown","d88c5e64":"markdown","532c62a4":"markdown","ff2fa957":"markdown","7032739e":"markdown","76e85b93":"markdown","4f13824b":"markdown","aa24ab80":"markdown"},"source":{"aefd7a05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","737d16b5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings                       \nwarnings.filterwarnings(\"ignore\")\ndataset = pd.read_csv('..\/input\/advertising.csv')\ndataset.head()","fd2e0f65":"dataset.info()","f2fa471b":"#Check for duplicates\ndataset.duplicated().sum()","6c993d59":"#Check for missing features\ndataset.isnull().sum()","1371467e":"#Correlation analysis\ncorrm = dataset.corr()\ncorrm['Clicked on Ad'].sort_values(ascending = False)","9dbe31be":"#Exploring target\ndataset['Clicked on Ad'].value_counts()","00a8c5df":"sns.countplot(x = 'Clicked on Ad', data = dataset)","41629a1b":"#Statistical information on the numeric features\ndataset.describe()","ceec78b2":"#Statistical information on the categorical features\ncateg_cols = ['Ad Topic Line', 'City', 'Country']\ndataset[categ_cols].describe(include = ['O'])","9dbff251":"dataset['Timestamp'] = pd.to_datetime(dataset['Timestamp'])\ndataset['Timestamp']\ndataset['Year'] = dataset['Timestamp'].dt.year\ndataset['Month'] = dataset['Timestamp'].dt.month\ndataset['Day'] = dataset['Timestamp'].dt.day\ndataset['Hour'] = dataset['Timestamp'].dt.hour\ndataset['Weekday'] = dataset['Timestamp'].dt.dayofweek\ndataset = dataset.drop(['Timestamp'], axis=1)\ndataset.head(10)","16552836":"#Relationship between numerical featuers\nsns.pairplot(dataset, hue = 'Clicked on Ad', \n             vars = ['Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage'], palette = 'Greens_r')","3027902b":"dataset = dataset.drop(['Year'], axis=1)\n#Correlation heatmap with new features\nfig = plt.figure(figsize = (12,10))\nsns.heatmap(dataset.corr(), cmap='Greens', annot = True)","b9339834":"X = dataset.iloc[:,[0,1,2,3,6,9,10,11,12]].values\ny = dataset.iloc[:,8].values\n#Splitting the data into train and test sets \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n#Scaling the data\nfrom sklearn.preprocessing import StandardScaler\nstandardScaler = StandardScaler()\nX_train = standardScaler.fit_transform(X_train)\nX_test = standardScaler.transform(X_test)","b78fa75e":"#Initiate and fit the model of Logistic Regression on training data\nfrom sklearn.linear_model import LogisticRegression\nlog_rg = LogisticRegression()\nlog_rg.fit(X_train, y_train)\n#Prediction\ny_log_rg = log_rg.predict(X_test)","599e34e2":"#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_log_rg)\nsns.heatmap(cm,annot=True,fmt='3.0f',cmap=\"Greens\")\nplt.title('Confusion matrix for Logistic Regression', y=1.05, size=15)","39b7795f":"#Classification report\nfrom sklearn.metrics import classification_report\ncr = classification_report(y_test, y_log_rg)\nprint(cr)","e36b4452":"#Initiate and fit the model of Naive Bayes on training data\nfrom sklearn.naive_bayes import GaussianNB\nnaive_b = GaussianNB()\nnaive_b.fit(X_train, y_train)\n#Prediction\ny_naive = naive_b.predict(X_test)","68f47448":"#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\nnaive_cm = confusion_matrix(y_test, y_naive)\nsns.heatmap(naive_cm,annot=True,fmt='3.0f',cmap=\"Blues\")\nplt.title('Confusion matrix for Naive Bayes', y=1.05, size=15)","c7cc65ae":"#Classification report\nfrom sklearn.metrics import classification_report\nnaive_cr = classification_report(y_test, y_naive)\nprint(naive_cr)","1a27cc24":"#Initiate and fit the model of Random Forest on training data\nfrom sklearn.ensemble import RandomForestClassifier\nrandm_frst = RandomForestClassifier()\nrandm_frst.fit(X_train, y_train)\n#Prediction\ny_frst = randm_frst.predict(X_test)","823aa103":"#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\nfrst_cm = confusion_matrix(y_test, y_frst)\nsns.heatmap(frst_cm,annot=True,fmt='3.0f',cmap=\"Reds\")\nplt.title('Confusion matrix for Random Forest', y=1.05, size=15)","a8fcabd5":"#Classification report\nfrom sklearn.metrics import classification_report\nfrst_cr = classification_report(y_test, y_frst)\nprint(frst_cr)","898ee4b8":"#Initiate and fit the model of K-Nearest Neighbors on training data\nfrom sklearn.neighbors import KNeighborsClassifier\nkneighbors = KNeighborsClassifier()\nkneighbors.fit(X_train, y_train)\n#Prediction\ny_knn = kneighbors.predict(X_test)","a9a7a8b9":"#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\nknn_cm = confusion_matrix(y_test, y_knn)\nsns.heatmap(knn_cm,annot=True,fmt='3.0f',cmap=\"mako\")\nplt.title('Confusion matrix for K-Nearest Neighbors', y=1.05, size=15)","252243d4":"#Classification report\nfrom sklearn.metrics import classification_report\nknn_cr = classification_report(y_test, y_knn)\nprint(knn_cr)","98e734c9":"from sklearn.metrics import f1_score\nf1_log = f1_score(y_test, y_log_rg)\nf1_naive = f1_score(y_test, y_naive)\nf1_frst = f1_score(y_test, y_frst)\nf1_knn = f1_score(y_test, y_knn)\nfrom pandas import DataFrame\nscores = {'Model':  ['Logistic Regression','Naive_Bayes', 'Random Forest', 'KNN'], \n          'f1 score': [f1_log, f1_naive, f1_frst, f1_knn]}\nf1_scores = DataFrame (scores, columns = ['Model','f1 score'])\nf1_scores","b7c5eb7f":"sns.barplot(x=\"Model\", y=\"f1 score\", data=f1_scores, palette=\"Greens_r\")","de08df05":"#Choose hyperparameters for Random Forest model\nfrom sklearn.model_selection import GridSearchCV\nparam_frst = [{\"n_estimators\": [10,100,200,300,500], \"criterion\": [\"gini\", \"entropy\"]}]\ngrid_search_frst = GridSearchCV(estimator=randm_frst,\n                          param_grid=param_frst,\n                          scoring = 'accuracy',\n                          cv=10)\ngrid_search_frst = grid_search_frst.fit(X_train, y_train)","76bdd87b":"#Calculation best accuracy for Random Forest model\nbest_acc_frst = grid_search_frst.best_score_\nbest_acc_frst","52d0710c":"#Calculation best parameters for Random Forest model\nbest_params_frst = grid_search_frst.best_params_\nbest_params_frst","c4656422":"#Choose hyperparameters for K-Nearest Neighbors model\nfrom sklearn.model_selection import GridSearchCV\nparam_knn = [{\"n_neighbors\": range(1,10), \"weights\": [\"uniform\", \"distance\"]}]\ngrid_search_knn = GridSearchCV(estimator=kneighbors,\n                          param_grid=param_knn,\n                          scoring = 'accuracy',\n                          cv=10)\ngrid_search_knn = grid_search_knn.fit(X_train, y_train)","0c7ece22":"#Calculation best accuracy for K-Nearest Neighbors model\nbest_acc_knn = grid_search_knn.best_score_\nbest_acc_knn","f3c32556":"#Calculation best parameters for K-Nearest Neighbors model\nbest_params_knn = grid_search_knn.best_params_\nbest_params_knn","3a4a1521":"#Initiate and fit the model of Random Forest on training data with hyperparamets\nfrom sklearn.ensemble import RandomForestClassifier\nrandm_frst_imp = RandomForestClassifier(n_estimators=100, criterion='gini')\nrandm_frst_imp.fit(X_train, y_train)\n#Prediction\ny_frst_imp = randm_frst_imp.predict(X_test)","ec765bb9":"#Initiate and fit the model of K-Nearest Neighbors on training data with hyperparamets\nfrom sklearn.neighbors import KNeighborsClassifier\nkneighbors_imp = KNeighborsClassifier(n_neighbors=5, weights= 'uniform')\nkneighbors_imp.fit(X_train, y_train)\n#Prediction\ny_knn_imp = kneighbors_imp.predict(X_test)","a89b0c1a":"#Recalculation f1 score\nf1_frst_imp = f1_score(y_test, y_frst_imp)\nf1_knn_imp = f1_score(y_test, y_knn_imp)\nscores_imp = {'Model':  ['Logistic Regression','Naive_Bayes', 'Random Forest', 'KNN'], \n          'f1 score': [f1_log, f1_naive, f1_frst_imp, f1_knn_imp]}\nf1_scores_imp = DataFrame (scores_imp, columns = ['Model','f1 score'])\nf1_scores_imp.sort_values(by=['f1 score'], ascending=False)","802e50d3":"#Confusion matrix for best model\nfrst_imp_cm = confusion_matrix(y_test, y_frst_imp)\nsns.heatmap(frst_imp_cm,annot=True,fmt='3.0f',cmap=\"PuBu_r\")\nplt.title('Confusion matrix for Random Forest with hyperparameters', y=1.05, size=15)\n","460f9a60":"from sklearn.metrics import roc_auc_score\nlr_auc = roc_auc_score(y_test, log_rg.predict(X_test))\nrf_roc_auc = roc_auc_score(y_test, randm_frst_imp.predict(X_test))\n\n# Create ROC Curve\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, log_rg.predict_proba(X_test)[:,1])\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, randm_frst_imp.predict_proba(X_test)[:,1])\n\nplt.figure()\n# Plot Random Forest ROC\nplt.plot(rf_fpr, rf_tpr, label='Random Forest Classifier (area = %0.2f)' % rf_roc_auc)\n# Plot Logistic Regression ROC\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % lr_auc)\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Base Rate')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.10])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n","f81b5e32":"Rebuilding models with hyperparametrs","6e1f7d46":"Evaluating results","cb79ae36":"Evaluating results\n","53c30260":"Extracting Datetime features","4239f532":"Fine-tuning models","e002f66b":"\nRelationship between features\n","be6f173d":"Model comparison by f1 score","d88c5e64":"Building Naive Bayes model","532c62a4":"Building K-Nearest Neighbors model","ff2fa957":"Evaluating results","7032739e":"Building Random Forest model","76e85b93":"Evaluating results","4f13824b":"The ROC Curve","aa24ab80":"Building Logistic Regression model"}}