{"cell_type":{"5f16d14f":"code","931a96bb":"code","558c6465":"code","010f1d31":"code","157ef815":"code","f1b07e4f":"code","bec34ac2":"code","22766b82":"code","a166a7c3":"code","661d5972":"code","b56289a2":"code","25274a5f":"code","4662d53d":"code","e9bbb5b0":"code","e8fffb77":"code","5626eee3":"code","4b653888":"code","5253f565":"code","837147b3":"code","514141a6":"code","1f2b46ea":"code","b8fdb190":"code","781ecfb8":"code","86001d6a":"markdown","f4bff51d":"markdown","0dc90f87":"markdown","efe8ebd5":"markdown","3502202b":"markdown"},"source":{"5f16d14f":"from transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov\/bert-base-multilingual-cased-sentence\", )\n\nmodel = AutoModel.from_pretrained(\"DeepPavlov\/bert-base-multilingual-cased-sentence\")","931a96bb":"from sklearn.manifold import TSNE\nimport plotly.express as px\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nimport pandas as pd","558c6465":"# Setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)","010f1d31":"_ = model.to(device)","157ef815":"tweets = pd.read_csv('\/kaggle\/input\/jair-bolsonaro-twitter-data\/bolsonaro_tweets.csv')\ntweets.shape","f1b07e4f":"tweets.head(1)","bec34ac2":"# Mean Pooling - Take attention mask into account for correct averaging\n# ref: https:\/\/huggingface.co\/sentence-transformers\/bert-base-nli-mean-tokens\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    \n    return sum_embeddings \/ sum_mask","22766b82":"dl = DataLoader(tweets['text'].tolist(), batch_size=10, shuffle=False)","a166a7c3":"len(dl), tweets.shape","661d5972":"embeddings = []\n\nfor batch in dl:\n    encoded_input = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors='pt')\n    \n    encoded_input.to(device)\n    \n    #Compute token embeddings\n    with torch.no_grad():\n        model_output = model(**encoded_input)\n\n        #Perform pooling. In this case, mean pooling\n        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n        \n        embeddings.append(sentence_embeddings.cpu().numpy())","b56289a2":"import numpy as np","25274a5f":"X = np.concatenate(embeddings)","4662d53d":"len(X), len(X[0])","e9bbb5b0":"from sklearn.cluster import KMeans","e8fffb77":"kmeans = KMeans(n_clusters=5, random_state=42).fit(X)","5626eee3":"kmeans.cluster_centers_","4b653888":"tweets['cluster'] = kmeans.labels_","5253f565":"X_embedded = TSNE(n_components=2).fit_transform(X)","837147b3":"df = pd.DataFrame(X_embedded)\ndf.columns = ['X', 'Y']","514141a6":"df.shape","1f2b46ea":"tweets = pd.concat([tweets, df], axis=1)","b8fdb190":"tweets.head(1)","781ecfb8":"fig = px.scatter(tweets, x=\"X\", y=\"Y\", color=\"cluster\", hover_data=['text'])\nfig.show()","86001d6a":"# Visualization","f4bff51d":"TSNE to performe dimensionality reduction","0dc90f87":"# Calculating Sentence Embeddings\n\nYou also can use [sentence-transformers](https:\/\/github.com\/UKPLab\/sentence-transformers) package.","efe8ebd5":"# Clustering ","3502202b":"BERT Embeddings has 768 dimensions"}}