{"cell_type":{"98740755":"code","bc838a02":"code","09c2cc50":"code","6e4ed82c":"code","1a3140d2":"code","1beb8baf":"code","7ebf09d6":"code","2fd59f9c":"code","1ab86a3b":"code","ed311277":"code","879414f3":"code","c49a6a4e":"code","fd32aba6":"code","a7625c9f":"code","425fbfc3":"code","3f39a277":"markdown","815f11c8":"markdown","b1bd8ac8":"markdown","2b7da821":"markdown","83f0c631":"markdown","f5dd9325":"markdown","f7907eeb":"markdown","88051ffc":"markdown"},"source":{"98740755":"import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns","bc838a02":"data= pd.read_csv('..\/input\/glass\/glass.csv')","09c2cc50":"data.head()","6e4ed82c":"print('There are ',data.shape[0] ,'rows and ',data.shape[1],' columns.')","1a3140d2":"data.info()","1beb8baf":"data.describe().T.round(2)","7ebf09d6":"def get_missing(data):\n    length = data.shape[0]\n    null_count= data.isnull().sum()\n    nan_count= ((data=='nan') | (data== 'NaN')).sum()\n    empty_count= ((data== '') | (data==' ')).sum()\n    null_percent= null_count\/ length\n    nan_percent= nan_count\/length\n    empty_percent= empty_count\/ length\n    abc= pd.DataFrame({'null_count' : null_count,\n        'null_percent' : null_percent,\n        'nan_count' :nan_count,              \n        'nan_percent' : nan_percent,\n        'empty_count' : empty_count,\n        'empty_percent' : empty_percent               \n    })\n    return abc\nget_missing(data)","2fd59f9c":"data.Type.value_counts()","1ab86a3b":"plt.bar(data.Type.unique(),data.Type.value_counts())\nplt.ylim(0,80)","ed311277":"data2= data.drop('Type',axis=1)\nplt.figure(figsize=(15,10))\nsns.heatmap(data2.corr().abs(), annot= True)\nplt.show()","879414f3":"y= data['Type']\nx= data.drop(['Type', 'Fe', 'Ba'], axis=1)","c49a6a4e":"from sklearn.model_selection import train_test_split, cross_val_score\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state=0)","fd32aba6":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier","a7625c9f":"from sklearn.model_selection import GridSearchCV,KFold","425fbfc3":"def result_using_tree_classifier(x_train,x_test,y_train,y_test):\n    kf = KFold(n_splits=5,random_state=None)\n    scores= []\n    algos2={'decision tree': {'model' : DecisionTreeClassifier(random_state=42),\n                      'param' : {'criterion':['gini', 'entropy'],'max_depth' : np.arange(1,6,1).tolist()  }\n                     },\n           'random forest': {'model' : RandomForestClassifier(random_state=42, n_jobs= -1),\n                      'param' : {'n_estimators': [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]}\n                     },\n           'extra tree' : {'model' : ExtraTreesClassifier(random_state=42, n_jobs= -1),\n                    'param' : {'n_estimators': [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]}\n                     } \n           } \n    for algo_name, params in algos2.items() :\n        Grid2 = GridSearchCV( params['model'], params['param'], cv=kf, return_train_score=False)\n        Grid2.fit(x_train, y_train)\n        ypred= Grid2.predict(x_test)\n        scores.append({\n            'model' : algo_name,\n            'best_score': Grid2.best_score_,\n            'best_para': Grid2.best_params_\n        })\n    return pd.DataFrame(scores,columns=['model','best_score','best_para']).set_index('model')\nresult_using_tree_classifier(x_train,x_test,y_train,y_test)","3f39a277":"No missing data!\n","815f11c8":"## Conclusion\n\nHere, we can see 'random forest' performs decently but, 'extra tree' is slightly better than it.","b1bd8ac8":"### Making a function to test different classifiers simultaneously using Dictionary.","2b7da821":"### Getting missing data","83f0c631":"### Looking at the correlation of features","f5dd9325":"## EXPLORING DATA","f7907eeb":"### Importing the CLASSIFIERS and other Libraries","88051ffc":"## Visuaisation of the Target Value "}}