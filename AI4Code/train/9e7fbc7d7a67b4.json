{"cell_type":{"78d5c170":"code","10313ac9":"code","2d5cb973":"code","0589dbf5":"code","0d891189":"code","120cf4be":"code","d4a904a5":"code","9c2dea79":"code","608a8625":"code","bf0b118b":"code","f6fdc5f5":"code","5d631bad":"code","7f913b19":"code","5336f57c":"code","68f7cc05":"code","400e49e2":"code","9991a63a":"code","634c5eb7":"markdown","a6d5f814":"markdown","e05790ea":"markdown","76d5d7b2":"markdown","cfe147d1":"markdown","e22f76cd":"markdown","99954033":"markdown","af0844b1":"markdown","cec9b8d2":"markdown","90492484":"markdown","04ba2f5d":"markdown","ab973d56":"markdown","823fb4fc":"markdown","8e4924fe":"markdown","5d5abf98":"markdown","c4414189":"markdown","2c0565de":"markdown","89428be2":"markdown","fe09a808":"markdown"},"source":{"78d5c170":"# Libraries\nfrom statsmodels.discrete.discrete_model import Logit\nimport h2o\nfrom h2o import H2OFrame\nfrom h2o.tree import H2OTree\nfrom h2o.estimators import H2ORandomForestEstimator\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom scipy.stats import pearsonr\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline","10313ac9":"# Import data\nrdata = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv', na_values = [' '])\ndim = rdata.shape\n\n# List of variables in raw data\nprint(*rdata.columns, sep = '\\n')","2d5cb973":"# Review data\nn_row = 10\nrdata.head(n_row).T","0589dbf5":"# Missing data checking\ndata_missing = rdata.isnull()\nprint(data_missing.iloc[:, :(dim[1]\/\/2)].describe())\nprint(data_missing.iloc[:, (dim[1]\/\/2):].describe())","0d891189":"# Drop missing data\nrdata.dropna(inplace = True)","120cf4be":"# Categorical variables list\nlist_remove = ['customerID', 'tenure', 'MonthlyCharges', 'TotalCharges']\nvar_cat = [name for name in rdata.columns if name not in list_remove]\n\n# Categorical data \ndata_cat = pd.get_dummies(rdata[var_cat], drop_first = True).astype(int)\n\n# Continuous data \ndata_cont = rdata[list_remove[1:]].apply(lambda x: pd.to_numeric(x), axis = 1)\n\n# Training data and scaling\ndata_scaler = MinMaxScaler()\ntrain_data = pd.concat([data_cont, data_cat], axis = 1) # Training data","d4a904a5":"# List out original encoding names in each categorical variable\nfor var_name in var_cat:\n    print('\\n', var_name, '\\n')\n    print(rdata[var_name].value_counts())","9c2dea79":"ncol_train_data, threshold = train_data.shape[1], 0.999\nduplicated_columns = set()\n\n# Loop through each pair of variables in design matrix\nfor i in range(ncol_train_data):\n    for j in range(ncol_train_data):\n        if i != j:\n            corr_coef = pearsonr(train_data.iloc[:, i], train_data.iloc[:, j])\n            corr_coef = corr_coef[0]\n            if corr_coef > threshold:\n                duplicated_columns.add(train_data.columns[i])\n                duplicated_columns.add(train_data.columns[j])\n                \n# List out duplicated column names          \nprint(duplicated_columns)","608a8625":"# Keep one variable\nduplicated_columns = list(duplicated_columns)[1:]","bf0b118b":"# Removing duplicated variables\ntrain_data.drop(duplicated_columns, axis = 1, inplace = True)\n\n# Training data for inferential modelling\nX_inf = train_data.drop('Churn_Yes', axis = 1) \n\n# Training data for predictive modelling\nX, y, cv = data_scaler.fit_transform(X_inf), train_data[['Churn_Yes']], 5 ","f6fdc5f5":"# Correlation plot\nplt.rcParams[\"figure.figsize\"] = (20,20)\nplt.matshow(X_inf.corr())\nplt.xticks(range(len(X_inf.columns)), X_inf.columns)\nplt.yticks(range(len(X_inf.columns)), X_inf.columns)\nplt.colorbar()\nplt.show()","5d631bad":"# Logistic Regression\nlogistic_model = Logit(y, X_inf)\nlogistic_fit = logistic_model.fit(method = 'bfgs', maxiter = 1000)\nprint(logistic_fit.summary())","7f913b19":"# H2O initialization\nh2o.init()\nh2o_train_data = H2OFrame(rdata.iloc[:, 1:])\nh2o_x_col, h2o_y_col = h2o_train_data.columns[0:(len(h2o_train_data.columns) - 1)], h2o_train_data.columns[-1]\n\n# Random forest\nrf_model = H2ORandomForestEstimator()\nrf_model.train(h2o_x_col, h2o_y_col, h2o_train_data)\n\n# Variable Importance\nrf_model.varimp_plot()","5336f57c":"# Logistic Regression with L1 penalty\ndef cv_Logistic(X, y, cv, C):\n    accuracy_score_list = list()\n    for c in C:\n        logisticClassifier = LogisticRegression(max_iter = 1000, penalty = 'l1', solver = 'liblinear', C = c)\n        average_accuracy = np.mean(cross_val_score(logisticClassifier, X, y, cv = cv))\n        accuracy_score_list.append(average_accuracy)\n        print('Regularization parameters: ', c, 'Accuracy: ', average_accuracy)\n    opt_C = max(C, key = lambda x: x in accuracy_score_list)\n    return opt_C\n\n# Compile Model\nregularization_parameter = [0.1 * i for i in range(1, 21)]\nopt_C = cv_Logistic(X, np.ravel(y), cv, regularization_parameter)","68f7cc05":"# Random Forest \ndef cv_RandomForest(X, y, cv):\n    rfClassifier = RandomForestClassifier()\n    average_accuracy = np.mean(cross_val_score(rfClassifier, X, y, cv = cv))\n    print('Accuracy: ', average_accuracy)\n    return average_accuracy\n\n# Compile Model\nprint(cv_RandomForest(X, np.ravel(y), cv))","400e49e2":"# Support vector machine with linear kernel\ndef cv_SVM(X, y, cv, C):\n    accuracy_score_list = list()\n    for c in C:\n        svmClassifier = SVC(C = c, kernel = 'linear')\n        average_accuracy = np.mean(cross_val_score(svmClassifier, X, y, cv = cv))\n        accuracy_score_list.append(average_accuracy)\n        print('Regularization parameters: ', c, 'Accuracy: ', average_accuracy)\n    opt_C = max(C, key = lambda x: x in accuracy_score_list)\n    return opt_C\n\n# Compile Model\nregularization_parameter = [0.1 * i for i in range(1, 21)]\nopt_C = cv_SVM(X, np.ravel(y), cv, regularization_parameter)","9991a63a":"# XGBoost \ndef cv_XGBoost(X, y, cv):\n    xgbClassifier = XGBClassifier(booster = 'gblinear')\n    average_accuracy = np.mean(cross_val_score(xgbClassifier, X, y, cv = cv))\n    print('Accuracy: ', average_accuracy)\n    return average_accuracy\n\n# Compile Model\nprint(cv_XGBoost(X, np.ravel(y), cv))","634c5eb7":"**Conclusions:** tenure, charges, contract, payment method and online security are top five important variables from random forests.","a6d5f814":"**Findings:** 'InternetService_No', 'TechSupport_No internet service', 'StreamingMovies_No internet service', 'OnlineBackup_No internet service', 'DeviceProtection_No internet service', 'StreamingTV_No internet service', 'OnlineSecurity_No internet service' are duplicated variables in the design matrix. They will be removed.","e05790ea":"### Missing Data","76d5d7b2":"### Random Forest\n**5-folded cross validation accuracy: 79.18%**","cfe147d1":"### Predictive Modelling\nIn this section, we will examine the accuracy of different machine learning models on churn rate based on customer profiles.","e22f76cd":"### Categorical variables","99954033":"### Correlation Analysis","af0844b1":"### Inferential Modelling\nIn this section, we will examine variables which lead to customers churn.","cec9b8d2":"**Findings:** InternetService_Fiber optic and MonthlyCharges are highly correlated. It is because the service itself is relatively expensive.","90492484":"**Findings:** 11 missing data in total charges","04ba2f5d":"### Load Libraries","ab973d56":"### Dummy Variable Trap Analysis\nA section for finding if there are any duplicated columns in the design matrix.","823fb4fc":"### Support Vector Machine with Linear kernel\nRegularization parameters: 0.1\n\n**5-folded cross-validation accuracy: 80.09%**","8e4924fe":"### Logistic Regression with L1 penalty\nRegularization parameters: 0.1\n\n**5-folded cross validation accuracy: 80.49%**","5d5abf98":"**Comment:** Most of our variables are categorical in nature.","c4414189":"### Summary of Predictive Models of Churn Rate\n**Metrics: 5-folded cross-validation accuracy**\n\nLogistic Regression with L1 penalty: 80.49%\n\nRandom Forest: 79.18%\n\nSupport Vector Machine with linear kernel: 80.09%\n\nXGBoost with linear model: 80.19%\n\n**In term of accuracy, Logistic (L1 penalty) ~ XGBoost > SVM >> RF**\n\n**Overall, we may conclude the performance of churn prediction should be about 80%**","2c0565de":"### Preprocessing of Data","89428be2":"**Conclusions:** \n\n1. **Significant variable:** tenure, TotalCharges, SeniorCitizen, MultipleLines_Yes, InternetService_Fiber optic, InternetService_No, Contract_One year, Contract_Two year, PaperlessBilling_Yes, PaymentMethod_Electronic check.\n\n2. **Impact on odd ratio of churn rate with sign:** tenure(-), TotalCharges(+), SeniorCitizen(+), MultipleLines_Yes(+, base level: No), InternetService_Fiber optic(+, base level: DSL), InternetService_No(-, base level: DSL), Contract_One year(-, base level: Month-to-Month), Contract_Two year(-, base level: Month-to-Month), PaperlessBilling_Yes(+, base level: No), PaymentMethod_Electronic check(+, base level: Bank transfer automatic).\n\n3. **Interpretations:** \n    \n    3.1 Higher tenure implies higher degree of loyalty. Hence, negative impact on odd ratio of churn. \n    \n    3.2 Higher total charges give incentive to switch service providers. Hence, positive impact on odd ratio of churn.\n    \n    3.3 Senior citizen have more choices on\/information about choosing service providers.\n    \n    3.4 Customers with multiple lines have a significantly higher odd ratio of churn. (Probably this service is more elastic in demand\n    \n    3.5 Customers with fiber optic internet service has a significantly higher odd ratio of churn. (Probably this service is more elastic in demand\n    \n    3.6 Customers without internet service has a significantly lower odd ratio of churn. (Probably this service is less elastic in demand\n    \n    3.7 Customers with longer contracts e.g. one\/two years has a significantly lower odd ratio of churn. Mobility of customers with longer contract is lower. \n    \n    3.8 Customers with paperless billing has a signifcantly higher odd ratio of churn.\n    \n    3.9 Customers with eletronic check payment has a signifcantly higher odd ratio of churn.\n    ","fe09a808":"### XGBoost with linear model\n**5-folded cross-validation accuracy: 80.19%**"}}