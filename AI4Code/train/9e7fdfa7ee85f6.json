{"cell_type":{"a6b0818d":"code","e4e7204f":"code","798f593d":"code","a1001454":"code","716a35db":"code","b579fd06":"code","95e6ba30":"code","369fe2d0":"code","72e63afa":"code","f88ec8a4":"code","18e8c1b6":"code","fea2f0e2":"code","0ff895a1":"code","34955627":"code","9afc3f9a":"code","d5ce85aa":"code","872d47d2":"code","16c81c51":"code","bfaa705e":"code","23f6a02b":"code","d7345a1b":"code","05753e2d":"code","162fe0de":"code","f2620de4":"code","8a4f988e":"code","ccefb83c":"code","be611524":"code","3156df6d":"code","3a81b37a":"code","9122f43a":"code","0fe151e4":"code","2962f0d3":"code","36efe8c9":"code","6629f924":"code","912dd53b":"code","f56b3121":"code","24a31fff":"code","68c887dc":"code","be154bef":"code","dde953eb":"code","bd3315b7":"code","63a51d9c":"code","947c6b91":"code","27e9688c":"code","4cbb4b3a":"code","79acea11":"code","e35f3978":"code","41f20bb3":"code","0bf47096":"code","01df8a55":"code","7a0db064":"code","48fd16a4":"code","53ab87a5":"code","150adf93":"code","0f9abbe4":"code","81ddff96":"code","f4ea737a":"code","ce0ddad5":"code","8ff451b7":"code","238479a5":"code","d06b9de8":"code","b98ede39":"code","d384acce":"markdown","b8faac77":"markdown","504ad9f5":"markdown","063f6f23":"markdown","ab5fdb3f":"markdown","a28b6060":"markdown","68821b96":"markdown","37ba2616":"markdown","334a796c":"markdown","1a1c5808":"markdown","acf997b0":"markdown","622901f0":"markdown","7f4d9711":"markdown","99de9989":"markdown","0d676dbb":"markdown","5f37f023":"markdown","9b5aaac0":"markdown","35c4714f":"markdown","5d5f0b20":"markdown","81fd2073":"markdown","d2ddaaa6":"markdown","639563c0":"markdown","98549144":"markdown","547b08cf":"markdown","22613814":"markdown","0a50b2f5":"markdown","374483e6":"markdown","52406e44":"markdown","942c46eb":"markdown","7c18614c":"markdown","44368843":"markdown","1c28bc6e":"markdown","2978fb84":"markdown"},"source":{"a6b0818d":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","e4e7204f":"data_description = '..\/input\/home-data-for-ml-course\/data_description.txt'\nfile_path = '..\/input\/home-data-for-ml-course\/train.csv'","798f593d":"data = pd.read_csv(file_path)\n\ndata","a1001454":"data.info()","716a35db":"data.columns","b579fd06":"data.dtypes","95e6ba30":"data.drop(columns='Id', inplace=True)\nnumerical_attr = data.select_dtypes(exclude=['object']).columns\nnumerical_attr","369fe2d0":"categorical_attr = data.select_dtypes(include=['object']).columns\ncategorical_attr","72e63afa":"data.describe(include=np.number)","f88ec8a4":"features = ['LotArea', 'OverallQual', 'OverallCond', 'LowQualFinSF','GrLivArea','SalePrice']\n\ndata.hist(column=features, figsize=(20,15))","18e8c1b6":"corr_matrix = data.corr()\n\ncorr_matrix['SalePrice'].sort_values(ascending=False)","fea2f0e2":"from pandas.plotting import scatter_matrix\nfeatures = ['SalePrice','OverallQual','GrLivArea','GarageCars','TotalBsmtSF', '1stFlrSF']\n\nscatter_matrix(data[features], figsize=(20, 15))","0ff895a1":"data.plot(kind=\"scatter\", x=\"1stFlrSF\", y=\"SalePrice\", alpha=0.5)","34955627":"data.plot(kind=\"scatter\", x=\"TotalBsmtSF\", y=\"SalePrice\", alpha=0.5)","9afc3f9a":"data.plot(kind=\"scatter\", x=\"GrLivArea\", y=\"SalePrice\", alpha=0.5)","d5ce85aa":"import seaborn as sns\n\nvar = 'SaleType'\n\nsns.boxplot(x=var, y='SalePrice', data=data)","872d47d2":"var = 'SaleCondition'\n\nsns.boxplot(x=var, y='SalePrice', data=data)\nplt.xticks(rotation=70)","16c81c51":"var = 'MSSubClass'\n\nsns.boxplot(x=var, y='SalePrice', data=data)","bfaa705e":"ms_sub_class = data['MSSubClass'].unique()\n\nms_sub_class.sort()\nms_sub_class","23f6a02b":"var = 'MSZoning'\n\nsns.boxplot(x=var, y='SalePrice', data=data)","d7345a1b":"var = 'BldgType'\n\nsns.boxplot(x=var, y='SalePrice', data=data)","05753e2d":"#descriptive statistics summary\ndata['SalePrice'].describe()","162fe0de":"#histogram\nsns.distplot(data['SalePrice']);","f2620de4":"#skewness and kurtosis\nprint(\"Skewness: %f\" % data['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % data['SalePrice'].kurt())","8a4f988e":"y = data['SalePrice']\ndata = data.drop(['SalePrice'], axis=1)","ccefb83c":"data","be611524":"c_features = ['SaleType', 'SaleCondition', 'MSSubClass', 'MSZoning', 'BldgType']\n\nselected_features = c_features + features\n\ndef get_df_missing_info(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = ((df.isnull().sum()\/df.isnull().count()) * 100).sort_values(ascending=False)\n\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n    return missing_data\n    \nmissing_data = get_df_missing_info(data)\nmissing_data.loc[numerical_attr, :]","3156df6d":"missing_data.loc[categorical_attr, :]","3a81b37a":"col_to_drop = ['Alley','PoolQC','Fence','MiscFeature']\nfor col in col_to_drop:\n    categorical_attr = categorical_attr.drop(col)\ndata.drop(columns=col_to_drop, inplace=True)","9122f43a":"from sklearn.pipeline import Pipeline \nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\nnum_pipeline = Pipeline(steps=[\n    ('imputer', KNNImputer()),\n    ('std_scaler', StandardScaler())\n])","0fe151e4":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])","2962f0d3":"from sklearn.compose import ColumnTransformer\n\nfull_pipeline = ColumnTransformer(\n    transformers=[\n    (\"num\", num_pipeline, list(numerical_attr)),\n    (\"cat\", categorical_pipeline, list(categorical_attr))\n])\n\nfinal_columns = data.columns\n\ndata_prepared = full_pipeline.fit_transform(data)","36efe8c9":"data_prepared.shape","6629f924":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, SGDRegressor, ElasticNet\nfrom sklearn.model_selection import cross_val_score\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n    \nX = data_prepared","912dd53b":"lin_reg = LinearRegression(n_jobs=-1)\nscores = cross_val_score(lin_reg, X, y, scoring=\"neg_mean_squared_error\", cv=10)\nscores = np.sqrt(-scores)\ndisplay_scores(scores)","f56b3121":"ridge = Ridge(alpha=0.1)\nscores = cross_val_score(ridge, X, y, scoring=\"neg_mean_squared_error\", cv=10)\nscores = np.sqrt(-scores)\ndisplay_scores(scores)","24a31fff":"lasso = Lasso(alpha=0.1, max_iter=2000, tol=1e-2)\nscores = cross_val_score(lasso, X, y, scoring=\"neg_mean_squared_error\", cv=10)\nscores = np.sqrt(-scores)\ndisplay_scores(scores)","68c887dc":"sdg = SGDRegressor(eta0=0.01)\nscores = cross_val_score(sdg, X, y, scoring=\"neg_mean_squared_error\", cv=10)\nscores = np.sqrt(-scores)\ndisplay_scores(scores)","be154bef":"ela = ElasticNet(alpha=0.1, l1_ratio=0.5)\nscores = cross_val_score(ela, X, y, scoring=\"neg_mean_squared_error\", cv=10)\nscores = np.sqrt(-scores)\ndisplay_scores(scores)","dde953eb":"from sklearn.linear_model import BayesianRidge\nbrd = BayesianRidge()\nscores = cross_val_score(brd, X.toarray(), y, scoring=\"neg_mean_squared_error\", cv=10)\nscores = np.sqrt(-scores)\ndisplay_scores(scores)","bd3315b7":"from sklearn.linear_model import ARDRegression\n\nard = ARDRegression()\nscores = cross_val_score(ard, X.toarray(), y, scoring=\"neg_mean_squared_error\", cv=10)\nscores = np.sqrt(-scores)\ndisplay_scores(scores)","63a51d9c":"from sklearn.tree import DecisionTreeRegressor\n\ntree_clf = DecisionTreeRegressor()\nscores = cross_val_score(tree_clf, X, y, scoring=\"neg_mean_squared_error\", cv=10)\nscores = np.sqrt(-scores)\ndisplay_scores(scores)","947c6b91":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators=500, n_jobs=-1, random_state=1)\nscores = cross_val_score(rf, X, y, scoring=\"neg_mean_squared_error\", cv=10)\nscores = np.sqrt(-scores)\ndisplay_scores(scores)","27e9688c":"from sklearn.ensemble import ExtraTreesRegressor\n\next_reg = ExtraTreesRegressor(n_estimators=500, n_jobs=-1, random_state=1)\nscores = cross_val_score(ext_reg, X, y, scoring=\"neg_mean_squared_error\", cv=10)\nscores = np.sqrt(-scores)\ndisplay_scores(scores)","4cbb4b3a":"from sklearn.ensemble import AdaBoostRegressor\n\nada  = AdaBoostRegressor(DecisionTreeRegressor(), n_estimators=200, learning_rate=0.5, n_jobs=-1)\n\nscores = cross_val_score(ada, X, y, scoring=\"neg_mean_squared_error\", cv=10)\nscores = np.sqrt(-scores)\ndisplay_scores(scores)","79acea11":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbrt = GradientBoostingRegressor(n_estimators=120, criterion='mse', n_jobs=-1)\n\nscores = cross_val_score(gbrt, X, y, scoring=\"neg_mean_squared_error\", cv=10)\nscores = np.sqrt(-scores)\ndisplay_scores(scores)","e35f3978":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'tol':np.arange(start=1e-3, stop=11e-3, step=1e-3)}\n]\n\nard = ARDRegression()\n\nard_grid_search = GridSearchCV(ard, param_grid, cv=10, \n                          scoring='neg_mean_squared_error',\n                          return_train_score=True, n_jobs=-1)\n\nard_grid_search.fit(X.toarray(), y)\n\nard_grid_search.best_params_","41f20bb3":"ard_grid_search.best_estimator_","0bf47096":"cvres = ard_grid_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score), params)","01df8a55":"from sklearn.model_selection import RandomizedSearchCV \n\n\nparam_grid =  {\n    'n_estimators': [100,500,1000], \n                                  'bootstrap': [True,False],\n                                  'max_depth': [3,5,10,20,50,75,100,None],\n                                  'max_features': ['auto','sqrt'],\n                                  'min_samples_leaf': [1,2,4,10],\n                                  'min_samples_split': [2,5,10]}\n\next_reg = ExtraTreesRegressor(random_state=1)\n\next_rndm_search = RandomizedSearchCV(ext_reg, param_grid, cv=10, \n                          scoring='neg_mean_squared_error',\n                          return_train_score=True, n_jobs=-1)\n\next_rndm_search.fit(X, y)\n\next_rndm_search.best_params_","7a0db064":"ext_rndm_search.best_estimator_","48fd16a4":"cvres = ext_rndm_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score), params)","53ab87a5":"gbrt = GradientBoostingRegressor(random_state=1)\n\nparam_grid = {\n    'n_estimators' : [120, 220, 320, 420, 520],\n    'criterion' : ['friedman_mse', 'mse', 'mae'],\n    'loss' : ['ls', 'lad', 'huber', 'quantile'],\n    'learning_rate' : [.01,0.1,0.2,0.3,0.5, 0.7, 0.9],\n    'max_depth': [2, 5, 10, 15, 20, 25, None],\n    'subsample': [0.5,0.6,0.7, 0.8, 0.9],\n    'max_features': ['auto', 'sqrt', 'log2'],\n}\n\ngbrt_rndm_search = RandomizedSearchCV(gbrt, param_grid, cv=10, \n                          scoring='neg_mean_squared_error',\n                          return_train_score=True, n_jobs=-1)\n\ngbrt_rndm_search.fit(X, y)\n\ngbrt_rndm_search.best_params_","150adf93":"gbrt_rndm_search.best_estimator_","0f9abbe4":"cvres = gbrt_rndm_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score), params)","81ddff96":"test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')\ntest","f4ea737a":"test = test[final_columns]","ce0ddad5":"test_prepared = full_pipeline.transform(test)\n\ntest_prepared.shape","8ff451b7":"final_ard = ard_grid_search.best_estimator_\n\nfinal_ard_predictions = final_ard.predict(test_prepared)\n\nids = np.arange(start=1461, stop=2920, step=1)\n\nfinal_ard_predictions = pd.DataFrame({'Id': ids, 'SalePrice': final_ard_predictions})\n\nfinal_ard_predictions","238479a5":"final_ext = ext_rndm_search.best_estimator_\n\nfinal_ext_predictions = final_ext.predict(test_prepared)\n\nfinal_ext_predictions = pd.DataFrame({'Id': ids, 'SalePrice': final_ext_predictions})\n\nfinal_ext_predictions","d06b9de8":"final_gbrt = gbrt_rndm_search.best_estimator_\n\nfinal_gbrt_predictions = final_gbrt.predict(test_prepared)\n\nfinal_gbrt_predictions = pd.DataFrame({'Id': ids, 'SalePrice': final_gbrt_predictions})\n\nfinal_gbrt_predictions","b98ede39":"final_ard_predictions.to_csv('.\/ard_submission.csv', index=False)\nfinal_ext_predictions.to_csv('.\/ext_submission.csv', index=False)\nfinal_gbrt_predictions.to_csv('.\/gbrt_submission.csv', index=False)","d384acce":"## AdaBoosting Classifier","b8faac77":"### Numerical Attributes basic stats","504ad9f5":"### Categorical Data","063f6f23":"## Linear SDG Regression","ab5fdb3f":"Now that we have transform our data we can find which are the best models using cross validation.","a28b6060":"## Random Forest Regressor","68821b96":"## Bayesian Ridge","37ba2616":"## Extra Tree Regressor","334a796c":"## Analysing the Target variable: the SalePrice","1a1c5808":"The missing numerical values aren't so much so I'm gonna fill those missing values with an imputer later.","acf997b0":"# Model Tuning\n\nAfter getting the baselines, let's see if we can improve on the indivdual model results! I'm gonna use Randomized Search for the `ExtraTreesRegressor` and the `GradientBoostRegressor` to simplify testing time, with for the others I'm gonna use Grid Search.","622901f0":"# Data Cleaning & Preprocessing","7f4d9711":"## ARDRegressor","99de9989":"While for the categorical attributes I'll use the SimpleImputer with the strategy most_frequent for filling the missing values and I'm gonna use a simple hot encoding of the variables for making them *machine readable*.","0d676dbb":"## Numerical and Categorical Attributes","5f37f023":"## Decision Tree Regressor","9b5aaac0":"# Model Selection","35c4714f":"## Ridge Regression","5d5f0b20":"# References\n\n- [A study on Regression applied to the Ames dataset](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset)\n- [Regularized Linear Models](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models)\n- [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)\n- [House Prices EDA](https:\/\/www.kaggle.com\/dgawlik\/house-prices-eda)\n- [Titanic Project Example](https:\/\/www.kaggle.com\/kenjee\/titanic-project-example)","81fd2073":"## ExtraTreesRegressor","d2ddaaa6":"# EDA\n\nLet's first start to give a quick look at the data where're dealing with.","639563c0":"## Gradient Boosting Regressor","98549144":"## Missing values\n\nSince we cannot give to our Regressor missing values let's see how they are distributed in the dataset.","547b08cf":"## Linear Regression","22613814":"## Gradient Boosting Regressor","0a50b2f5":"## Preprocessing\n\nRegarding the numerical misssing values I'm gonna use the KNNImputer which fills missing values using k-Nearest Neighbors, and then I'm gonna use the standard scaler to give a proper scale to all the numerical attributes.","374483e6":"## Lasso Regression","52406e44":"# Results","942c46eb":"# Table of Contents\n\n- [EDA](#EDA)\n- [Data Cleaning & preprocessing](#Data-Cleaning-&-Preprocessing)\n- [Model Selection](#Model-Selection)\n- [Model Tuning](#Model-Tuning)\n- [Results](#Results)\n- [References](#References)","7c18614c":"Now we saw visually `GrLivArea`, `1stFlrSF` and `TotalBsmtSF` are almost linearly correlated to the price.","44368843":"## ElasticNet Regression","1c28bc6e":"Since the varibles 'Alley','PoolQC','Fence','MiscFeature' have more the 80% of missing values, let's drop them from the dataset  ","2978fb84":"## ARDRegressor "}}