{"cell_type":{"07c4b41d":"code","f4424d21":"code","45427074":"code","227f56d9":"code","4d50f95c":"code","20a3c1f4":"code","de74df43":"code","2e5bfab1":"code","b2eadb06":"code","c2735009":"code","0dc4b10e":"code","7ce42eba":"code","6e83a4b5":"code","cd984b82":"code","ffada9bf":"code","b181d233":"code","71027abb":"code","1c3bcd7f":"code","9c815ef2":"code","995be63d":"code","8db0b7a3":"code","88ae565a":"code","22895a04":"code","07a4fcac":"code","d828d2b2":"code","8e352161":"code","688c1538":"code","16b5e8d0":"code","6e8cf849":"code","a4d2c8c0":"code","4f2caa0c":"code","43e6c7e7":"markdown","ae031b1b":"markdown","b15d9533":"markdown","ce7d7ec7":"markdown","778cbc6c":"markdown"},"source":{"07c4b41d":"# IMPORT MODULES\n# TURN ON the GPU !!!\n# If importing dataset from outside -  Internet must be \"connected\"\n\nimport os\nfrom operator import itemgetter    \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nget_ipython().magic(u'matplotlib inline')\nplt.style.use('ggplot')\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import Imputer, LabelBinarizer, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, LabelEncoder, MinMaxScaler, OneHotEncoder, LabelBinarizer\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\nfrom scipy.stats import skew\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.base import clone, BaseEstimator, TransformerMixin\nfrom sklearn.cross_validation import KFold, cross_val_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, KFold, cross_val_predict, StratifiedKFold, train_test_split, learning_curve, ShuffleSplit\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\n\nimport tensorflow as tf\n\nfrom keras import models, regularizers, layers, optimizers, losses, metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import np_utils\n#from keras.utils.np_utils import to_categorical\n\nimport tarfile\nfrom six.moves import urllib\n\nprint(os.getcwd())\nprint(\"Modules imported \\n\")\n#print(\"Files in current directory:\")\n#from subprocess import check_output\n#print(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory","f4424d21":"# LOAD data\n\nDOWNLOAD_ROOT = \"https:\/\/raw.githubusercontent.com\/ageron\/handson-ml\/master\/\"\nHOUSING_PATH = \"datasets\/housing\"\nHOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + \"\/housing.tgz\"\n\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    if not os.path.isdir(housing_path):\n        os.makedirs(housing_path)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()\nfetch_housing_data()\n\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)\nhousingRaw = load_housing_data()\nhousingRaw.columns","45427074":"housing = housingRaw.copy()\nprint(\"housing \", housing.shape)\n\n# Missing Data\nColsMissingValues = housing.isnull().sum()\nprint(\"There are \", len(ColsMissingValues[ColsMissingValues>0]), \" features with missing values\")\n#print(\"_\"*80)\nall_data_na = (housing.isnull().sum() \/ len(housing)) * 100\nall_data_na = all_data_na.sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nprint(missing_data.head(len(ColsMissingValues[ColsMissingValues>0])))","227f56d9":"# VISUALIZATION\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\ns=housing[\"population\"]\/100, label=\"population\",\nc=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n)\nplt.legend()\n\nhousing.hist(bins=50, figsize=(20,15))\nplt.show()\n","4d50f95c":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","20a3c1f4":"attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))","de74df43":"housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",alpha=0.1)","2e5bfab1":"class add_feature(BaseEstimator, TransformerMixin):\n    def __init__(self,additional=1):\n        self.additional = additional\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        if self.additional==1:\n            X[\"rooms_per_household\"] = X[\"total_rooms\"]\/X[\"households\"]\n            X[\"bedrooms_per_room\"] = X[\"total_bedrooms\"]\/X[\"total_rooms\"]\n            X[\"population_per_household\"]=X[\"population\"]\/X[\"households\"]\n            X[\"income_per_pop_household\"]=X[\"median_income\"]\/X[\"population_per_household\"]\n            X[\"rooms_squared\"]=X[\"rooms_per_household\"]*X[\"rooms_per_household\"]\n            X[\"room_mult_income\"]=X[\"rooms_squared\"]*X[\"median_income\"]    \n        else:\n            X[\"rooms_per_household\"] = X[\"total_rooms\"]\/X[\"households\"]\n            X[\"bedrooms_per_room\"] = X[\"total_bedrooms\"]\/X[\"total_rooms\"]\n            X[\"population_per_household\"]=X[\"population\"]\/X[\"households\"]\n            X[\"income_per_pop_household\"]=X[\"median_income\"]\/X[\"population_per_household\"]\n            X[\"rooms_squared\"]=X[\"rooms_per_household\"]*X[\"rooms_per_household\"]\n            X[\"room_mult_income\"]=X[\"rooms_squared\"]*X[\"median_income\"]\n            \n        return X\n\n# as initially, the models were UNDERFITTING - I've added some features","b2eadb06":"class grid():\n    def __init__(self,model):\n        self.model = model\n    \n    def grid_get(self,X,y,param_grid):\n        grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring=\"neg_mean_squared_error\")\n        grid_search.fit(X,y)\n        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])\n","c2735009":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Error\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = 1-np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = 1-np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","0dc4b10e":"class skew_dummies(BaseEstimator, TransformerMixin):\n    def __init__(self,skew=0.75):\n        self.skew = skew\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        X_numeric=X.select_dtypes(exclude=[\"object\"])\n        skewness = X_numeric.apply(lambda x: skew(x))\n        skewness_features = skewness[abs(skewness) >= self.skew].index\n        X[skewness_features] = np.log1p(X[skewness_features])\n        X = pd.get_dummies(X)\n        #X = pd.concat([pd.get_dummies(X[['ocean_proximity']]), X[]], axis=1)\n        return X","7ce42eba":"class labelenc(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        lab = LabelEncoder()\n        X['ocean_proximity'] = lab.fit_transform(X['ocean_proximity'])\n              \n        return X","6e83a4b5":"class imputer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):       \n        imp = Imputer(strategy=\"median\")\n        Xcopy = X\n        Ximputed = imp.fit_transform(Xcopy)\n        X= Ximputed\n        X = pd.DataFrame(Ximputed, columns = Xcopy.columns)\n        \n        return X","cd984b82":"class scalerFI(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):       \n        scl = StandardScaler()\n        Xcopy = pd.DataFrame(X)\n        Xscaled = scl.fit_transform(Xcopy)\n        X = pd.DataFrame(Xscaled, columns = Xcopy.columns)\n        \n        return X","ffada9bf":"# Prepare the DATA for Feature Importance\n\n# PIPELINE\npipeFI = Pipeline([\n    ('add_feature', add_feature(additional=2)),\n    ('skew_dummies', skew_dummies(skew=1)), \n    ('imputer', imputer()),\n    ('scalerFI', scalerFI()),\n    ])\n\n# RELOAD A FRESH COPY\nhousingFI = housingRaw.copy()\n\n#y = np.log(housing.median_house_value)\nyFI = housingFI.median_house_value\n\nprint(\"housingFI BEFORE pipeline\", housingFI.shape)\n\nhousingWpriceFI = pd.DataFrame(housingFI)\nhousingNoPriceFI = housingWpriceFI.drop(\"median_house_value\", axis=1)\n\nprint(\"housingFI without the label \", housingNoPriceFI.shape)\n\nFullDataPipeFI = pipeFI.fit_transform(housingNoPriceFI)\nhousingFI = pd.DataFrame(FullDataPipeFI)\n\nprint(\"housingFI AFTER pipeline \", housingFI.shape)\nprint(\"yFI \", yFI.shape)","b181d233":"# AFTER Pipeline\n\nhousingFI.hist(bins=50, figsize=(20,15))\nplt.show()","71027abb":"# FEATURE IMPORTANCE - Needs the original cols names (see classes above) in order to see the features' NAMES and not their numbers\n# Useful even AFTER PCA - check the relevance of features for prediction\n\ntrainFinalFI = pd.DataFrame(housingFI)\nyFinalFI = yFI\n\nlasso=Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,\n   normalize=False, positive=False, precompute=False, random_state=None,\n   selection='cyclic', tol=0.0001, warm_start=False)\nlasso.fit(trainFinalFI,yFinalFI)\n\nFI_lasso = pd.DataFrame({\"Feature Importance\":lasso.coef_}, index=trainFinalFI.columns)\n\n# Focus on those with 0 importance\n#print(FI_lasso.sort_values(\"Feature Importance\",ascending=False).to_string())\n#print(\"_\"*80)\nFI_lasso[FI_lasso[\"Feature Importance\"]!=0].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\nplt.xticks(rotation=90)\nplt.show()","1c3bcd7f":"# Prepare the DATA & PIPELINE for models evaluation with TRAIN and TEST sets\n\n# PIPELINE\npipe = Pipeline([\n    ('add_feature', add_feature(additional=2)),\n    ('skew_dummies', skew_dummies(skew=1)), \n    ('impute_num', Imputer(strategy=\"median\")),\n    ])\n###                   ** skew_dummies is taking care of the labelencoder AND onehotencoder with get_dummies() !  **\n\n\n# RELOAD A FRESH COPY\nhousing = housingRaw.copy()\n\n#y = np.log(housing.median_house_value)\ny = housing.median_house_value\n\nn_train = 16500\n\ny_train = np.array(pd.DataFrame(y[:n_train])).reshape(-1,1).ravel()\ny_test = np.array(pd.DataFrame(y[n_train:])).reshape(-1,1).ravel()\n\nprint(\"housing BEFORE pipeline\", housing.shape)\n\nhousingWprice = pd.DataFrame(housing)\nhousingNoPrice = housingWprice.drop(\"median_house_value\", axis=1)\n\nprint(\"housing without the label \", housingNoPrice.shape)\n\nFullDataPipe = pipe.fit_transform(housingNoPrice)\nhousing = pd.DataFrame(FullDataPipe)\n\nprint(\"housing AFTER pipeline \", housing.shape)\nprint(\"y \", y.shape)\nprint(\"_\"*100)\n\n# Any Missing Data ?\nColsMissingValues = housing.isnull().sum()\nprint(\"There are \", len(ColsMissingValues[ColsMissingValues>0]), \" features with missing values\")\nall_data_na = (housing.isnull().sum() \/ len(housing)) * 100\nall_data_na = all_data_na.sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nprint(missing_data.head(len(ColsMissingValues[ColsMissingValues>0])))\nprint(\"_\"*100)\n\n# Split TRAIN \/ TEST and then SCALE them SEPARATELY\n\ntrainSet = pd.DataFrame(housing[:n_train])\ntestSet = pd.DataFrame(housing[n_train:])\n\n# Scaler should be run separately on train and test to prevent information leaking from test into train and eventually overfitting\nscaler = StandardScaler()\ntrainX = scaler.fit_transform(trainSet)\ntestX = scaler.fit_transform(testSet)\nprint(\"trainX and testX have been scaled separately\")\nprint(\"trainX \",trainX.shape)\nprint(\"y_train \",y_train.shape)\nprint(\"testX \",testX.shape)\nprint(\"y_test \",y_test.shape)","9c815ef2":"# CROSS VALIDATION\n\ndef rmse_cv(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return rmse","995be63d":"# Lin reg ALL 14 models HYPERPARAMS NOT optimized\n\nmodels = [LinearRegression(),Ridge(),Lasso(),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),\n          ElasticNet(),SGDRegressor(),BayesianRidge(),KernelRidge(),ExtraTreesRegressor(),XGBRegressor(),lgb.LGBMRegressor()]\nnames = [\"LR\", \"Ridge\", \"Lasso\", \"RF\", \"GBR\", \"SVR\", \"LinSVR\", \"Ela\",\"SGD\",\"Bay\",\"Ker\",\"Extra\",\"Xgb\", \"LightGBM\"]","8db0b7a3":"# Run the models and compare\n\nModScores = {}\n\nfor name, model in zip(names, models):\n    score = rmse_cv(model, trainX, y_train)\n    ModScores[name] = score.mean()\n    print(\"{}: {:.2f}\".format(name,score.mean()))\n\nprint(\"_\"*100)\nfor key, value in sorted(ModScores.items(), key = itemgetter(1), reverse = False):\n    print(key, value)","88ae565a":"# SEARCH GRID FOR HYPERPARAMS OPTIMIZATION - One model at a time\n\nmodel = XGBRegressor()\n\nparam_grid = [\n{},\n]\n\ngrid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(trainX, y_train)\n\nprint(grid_search.best_estimator_)","22895a04":"# 7 MODELS with HYPERPARAMS optimized\nmodels = [RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n           oob_score=False, random_state=None, verbose=0, warm_start=False),\n          \n           lgb.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               learning_rate=0.1, max_depth=-1, min_child_samples=20,\n               min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n               n_jobs=-1, num_leaves=31, objective='regression', random_state=None,\n               reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n               subsample_for_bin=200000, subsample_freq=1),\n          \n          XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n           colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n           max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n           n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n           reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n           silent=True, subsample=1),\n          \n          GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\n             min_impurity_split=None, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=100, presort='auto', random_state=None,\n             subsample=1.0, verbose=0, warm_start=False),\n          \n          ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n              max_features='auto', max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=1, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n              oob_score=False, random_state=None, verbose=0, warm_start=False),\n          \n          SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n              kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n          \n          Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n               normalize=False, random_state=None, solver='auto', tol=0.001),\n         \n          BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n               fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n               normalize=False, tol=0.001, verbose=False)\n         ]\n\nnames = [\"RandomForest\", \"LGB\", \"XGB\", \"GBR\", \"ExtraTrees\", \"SVR\", \"Ridge\", \"BayesRidge\"]\n    ","07a4fcac":"# LEARNING CURVE\n\nmodel = GradientBoostingRegressor()\n\ntitle = \"Learning Curves \"\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nplot_learning_curve(model, title, trainX, y_train, ylim=(0.01, 0.3), cv=cv, n_jobs=4)","d828d2b2":"# Model fit and evaluation on test\n\nmodel = GradientBoostingRegressor()\n\nmodel.fit(trainX, y_train)\n\nfinal_predictions = model.predict(testX)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse) \n\nprint(\"rmse on test \", final_rmse)","8e352161":"# Mapping data from the linear models above to NN below\n\ndel model\ndel models\nfrom keras import models\nfrom keras.models import Sequential\n\nx_val = testX\npartial_x_train = trainX\ny_val = y_test\npartial_y_train = y_train\n\nprint(\"partial_x_train \", partial_x_train.shape)\nprint(\"partial_y_train \", partial_y_train.shape)\n\nprint(\"x_val \", x_val.shape)\nprint(\"y_val \", y_val.shape)","688c1538":"# NN MODEL\ndef build_model():\n    model = models.Sequential()\n    model.add(layers.Dense(64, activation='relu', input_shape=(partial_x_train.shape[1],)))\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(1))\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n    return model","16b5e8d0":"# CV with NN\n\ntrain_data = partial_x_train\ntrain_targets = partial_y_train\n\nnum_epochs = 100\nBatchSize = 64\n\nk = 4\nnum_val_samples = len(train_data) \/\/ k\nall_mae_histories = []\nfor i in range(k):\n    print('processing fold #', i)\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate([train_data[:i * num_val_samples],train_data[(i + 1) * num_val_samples:]],axis=0)\n    partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],train_targets[(i + 1) * num_val_samples:]],axis=0)\n    \n    model = build_model()\n    \n    history = model.fit(partial_train_data, partial_train_targets,validation_data=(x_val, y_val),\n    epochs=num_epochs, batch_size=BatchSize, verbose=0)\n    mae_history = history.history['val_mean_absolute_error']\n    all_mae_histories.append(mae_history)\n    \nresults = model.evaluate(x_val, y_val)\nprint(\"Loss and MAE on TEST\")\nprint(\"results \", results)\n\nhistory_dict = history.history\nhistory_dict.keys()","6e8cf849":"# VALIDATION LOSS curves\n\nplt.clf()\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","a4d2c8c0":"## VALIDATION MAE curves\n\nplt.clf()\nacc = history.history['mean_absolute_error']\nval_acc = history.history['val_mean_absolute_error']\nplt.plot(epochs, acc, 'bo', label='Training MAE')\nplt.plot(epochs, val_acc, 'b', label='Validation MAE')\nplt.title('Training and validation MAE')\nplt.xlabel('Epochs')\nplt.ylabel('MAE')\nplt.legend()\nplt.show()\n","4f2caa0c":"# Model fit and evaluation on test\n# Set the num of Epochs and Batch Size according to learning curves\n\nmodel = build_model()\nmodel.fit(train_data, train_targets, epochs=100, batch_size=64, verbose=0)\ntest_mse_score, test_mae_score = model.evaluate(x_val, y_val)\n\nprint(\"test_mae_score on test \", test_mae_score)","43e6c7e7":"**LINEAR REGRESSION models**","ae031b1b":"**CLASSES USED by the PIPELINES below**","b15d9533":"* This kernel tries initally **14 Linear Regression models**, optimizes the hyper parameters of the best and then runs a **NN with Keras\/TF** for comparison \n* As the original Housing Prices competition has only 1465 samples - it would be an overkill to try a NN on it.  So I used the California Housing Prices set from the book [Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems by Aurelian Geron](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1491962291\/ref=sr_1_1?s=books&ie=UTF8&qid=1534265951&sr=1-1&keywords=aurelien+geron), which has more samples (20k) but less features than the one from Kaggle House Pricing competition.\n\n* Kernel loads the data and visualizes: histogams and scatter plots, Pearson linear correlation, feature importance chart \n* Sets aside a test and uses CV k-fold on the rest.\n* All the transformations are done via classes and pipeline: add some features, impute missing, scale, one hot encoding, etc - this allows one to test various options easily.\n* Using CV k-fold kernel runs 14 linear regression models:\nLinearRegression(),\nRidge(),\nLasso(),\nRandomForestRegressor(),\nGradientBoostingRegressor(),\nSVR(),\nLinearSVR(),\nElasticNet(),\nSGDRegressor(),\nBayesianRidge(),\nKernelRidge(),\nExtraTreesRegressor(),\nXGBRegressor(),\nlgb.LGBMRegressor()\n\n* Using Grid Search it optimizes the best model from the above\n* The learning curve of the model was UNDERFITTING, so I've tried to add some features...It didn't help much\n* Using the same data sets, kernel's second part runs a NN while allowing  one to check various options related to NN architecture, number of hidden units,  regularization, dropout, epochs, batch size, etc.\n* Note that the linear regression models are being monitored on the RMSE while the NN is being evaluated with the loss function of MSE but the metric used is MAE.\n* Apparently the NN is predicting better than the Linear regression, but still - there's room for improvement in both areas....\n* The NN with Keras part is based on the excellent book [Deep Learning with Python by Francois Chollet](https:\/\/www.amazon.com\/Deep-Learning-Python-Francois-Chollet\/dp\/1617294438)","ce7d7ec7":"**NN with Keras**","778cbc6c":"**PIPELINES**"}}