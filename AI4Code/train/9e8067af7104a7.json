{"cell_type":{"6c423e66":"code","fc27638a":"code","e1dcb008":"code","e2e37e89":"code","d73c7e82":"code","9f8ee88f":"code","086b16db":"code","8147a6b7":"code","44aad6df":"code","9058f316":"code","df16e928":"code","f3c57afe":"code","96568d25":"code","a4a6e253":"code","223a6828":"code","4fb87c3c":"code","9847a159":"code","159b6174":"code","b19bad19":"code","6c1b403a":"code","85b45a50":"code","ff0b0a0f":"code","24e279ce":"code","566f61b9":"code","556c8006":"code","30443780":"code","9ff9ed90":"code","f679ce08":"code","95fe4583":"code","6e4584c7":"code","a2ca9e77":"code","832f3e63":"code","1080d41e":"code","75ff0106":"code","082f3398":"code","8c56bf43":"code","1449bd4e":"code","c40f1799":"code","885d9918":"code","cc7ae7d0":"code","ccb24105":"code","93e4ba2b":"code","3e6e1a55":"markdown","f1d608e9":"markdown","d6ad1cdc":"markdown","095e3386":"markdown","020903c3":"markdown","e600af7d":"markdown","17c5f45d":"markdown","69b893d7":"markdown","3e1aec6c":"markdown","8137b56c":"markdown","3474e110":"markdown","16f66e3a":"markdown","ac0c1cc4":"markdown","a9519ff9":"markdown","a4146589":"markdown","2fff3566":"markdown"},"source":{"6c423e66":"import pandas\nimport numpy as np\nimport cudf as pd\nimport cupy as cp\n\nimport glob\nimport os\nimport gc\nimport time\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GroupKFold\nfrom scipy.optimize import minimize\n\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\nfrom catboost import Pool, CatBoostRegressor\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nimport cuml\nfrom cuml.neighbors import KNeighborsRegressor\nfrom cuml import LinearRegression\nfrom cuml import Ridge\nfrom cuml.ensemble import RandomForestRegressor\n\n\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}\n\ndef convert_to_32bit(df):\n    for f in df.columns:\n        if df[f].dtype == 'int64':\n            df[f] = df[f].astype('int32')\n        if df[f].dtype == 'float64':\n            df[f] = df[f].astype('float32')\n    return df","fc27638a":"#set clip values\nlow_clip = 0\nupper_clip = 1e10","e1dcb008":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\ntrain = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntest['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n\ntrain['is_train'] = 1\ntest['is_train'] = 0\n\ntrain = convert_to_32bit(train)\ntest = convert_to_32bit(test)\n\nprint( train.shape )\nprint( test.shape )","e2e37e89":"print(train.head(20))","d73c7e82":"print(test.head(20))","9f8ee88f":"train_stock_ids = train['stock_id'].to_pandas().unique()\ntest_stock_ids = test['stock_id'].to_pandas().unique()\nprint( 'Sizes:', len(train_stock_ids), len(test_stock_ids) )\nprint( 'Train stocks:', train_stock_ids )\nprint( 'Test stocks:', test_stock_ids )","086b16db":"def transform(df, groupby='time_id', feat='price', agg='mean' ):\n    return df.merge( \n        df.groupby(groupby)[feat].agg(agg).reset_index().rename({feat:feat+'_'+agg}, axis=1),\n        on=groupby,\n        how='left' \n    )\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df = convert_to_32bit(df)\n    df = df.sort_values(['time_id','seconds_in_bucket']).reset_index(drop=True)\n    \n    # Calculate Wap\n    df['wap1'] = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    df['wap2'] = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    df['wap3'] = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    df['wap4'] = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    \n    \n    # Calculate log returns\n    df['log_return1'] = df['wap1'].log()\n    df['log_return1'] = df['log_return1'] - df.groupby(['time_id'])['log_return1'].shift(1).reset_index(drop=True)\n\n    df['log_return2'] = df['wap2'].log()\n    df['log_return2'] = df['log_return2'] - df.groupby(['time_id'])['log_return2'].shift(1).reset_index(drop=True)\n\n    df['log_return3'] = df['wap3'].log()\n    df['log_return3'] = df['log_return3'] - df.groupby(['time_id'])['log_return3'].shift(1).reset_index(drop=True)\n\n    df['log_return4'] = df['wap4'].log()\n    df['log_return4'] = df['log_return4'] - df.groupby(['time_id'])['log_return4'].shift(1).reset_index(drop=True)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    df['log_return1_sqr'] = df['log_return1'] ** 2\n    df['log_return2_sqr'] = df['log_return2'] ** 2\n    df['log_return3_sqr'] = df['log_return3'] ** 2\n    df['log_return4_sqr'] = df['log_return4'] ** 2\n    \n    df = transform(df, groupby='time_id', feat='wap1', agg='median' )\n    df = transform(df, groupby='time_id', feat='wap2', agg='median' )\n    df = transform(df, groupby='time_id', feat='wap3', agg='median' )\n    df = transform(df, groupby='time_id', feat='wap4', agg='median' )\n    df = transform(df, groupby='time_id', feat='log_return1', agg='median' )\n    df = transform(df, groupby='time_id', feat='log_return2', agg='median' )\n    df = transform(df, groupby='time_id', feat='log_return3', agg='median' )\n    df = transform(df, groupby='time_id', feat='log_return4', agg='median' )\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': ['sum', 'std', 'min','max'],\n        'wap2': ['sum', 'std', 'min','max'],\n        'wap3': ['sum', 'std', 'min','max'],\n        'wap4': ['sum', 'std', 'min','max'],\n        'log_return1_sqr': ['sum', 'std', 'min','max'],\n        'log_return2_sqr': ['sum', 'std', 'min','max'],\n        'log_return3_sqr': ['sum', 'std', 'min','max'],\n        'log_return4_sqr': ['sum', 'std', 'min','max'],\n        'wap_balance': ['sum', 'mean', 'min','max'],\n        'price_spread':['sum', 'mean', 'min','max'],\n        'price_spread2':['sum', 'mean', 'min','max'],\n        'bid_spread':['sum', 'mean', 'min','max'],\n        'ask_spread':['sum', 'mean', 'min','max'],\n        'total_volume':['sum', 'mean', 'min','max'],\n        'volume_imbalance':['sum', 'mean', 'min','max'],\n        \"bid_ask_spread\":['sum',  'mean', 'min','max'],\n        \n        \"wap1_median\":['sum',  'mean', 'min','max'],\n        \"wap2_median\":['sum',  'mean', 'min','max'],\n        \"wap3_median\":['sum',  'mean', 'min','max'],\n        \"wap4_median\":['sum',  'mean', 'min','max'],\n        \"log_return1_median\":['mean',  'std', 'min','max'],\n        \"log_return2_median\":['mean',  'std', 'min','max'],\n        \"log_return3_median\":['mean',  'std', 'min','max'],\n        \"log_return4_median\":['mean',  'std', 'min','max'],\n    }\n    create_feature_dict_time = {\n        'log_return1_sqr': ['sum', 'std', 'min','max'],\n        'log_return2_sqr': ['sum', 'std', 'min','max'],\n        'log_return3_sqr': ['sum', 'std', 'min','max'],\n        'log_return4_sqr': ['sum', 'std', 'min','max'],\n    }\n    \n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        if add_suffix:\n            df_feature.columns = [col + '_' + str(seconds_in_bucket) for col in df_feature.columns]\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    \n    # Drop tmp columns\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['stock_id'] = str(stock_id) + '-'\n\n    df_feature['row_id'] = df_feature['stock_id'] + df_feature['time_id_'].astype(str)\n    \n    return df_feature","8147a6b7":"%%time\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df = convert_to_32bit(df)\n    df = df.sort_values('time_id').reset_index(drop=True)\n\n    df['log_return'] = df['price'].log()\n    df['log_return'] = df['log_return'] - df.groupby(['time_id'])['log_return'].shift(1).reset_index(drop=True)\n    df['log_return_sqr'] = df['log_return'] ** 2\n    \n    df['amount']=df['price']*df['size']\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return_sqr': ['sum', 'std','max', 'min'],\n        'seconds_in_bucket':['nunique','std', 'mean','max', 'min'],\n        'size':['sum', 'nunique','std','max', 'min'],\n        'order_count':['sum','nunique','max','min','std'],\n        'amount':['sum','std','max','min'],\n    }\n    create_feature_dict_time = {\n        'log_return_sqr': ['sum', 'std','max','min'],\n        'seconds_in_bucket':['nunique'],\n        'size':['sum','mean','std','min','max'],\n        'order_count':['sum','mean','std','min','max'],\n    }\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature.columns = [col + '_' + str(seconds_in_bucket) for col in df_feature.columns]\n        return df_feature\n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    df = df.sort_values(['time_id','seconds_in_bucket']).reset_index(drop=True)\n    \n    df = transform(df, groupby='time_id', feat='price', agg='mean' )\n    df = transform(df, groupby='time_id', feat='price', agg='sum' )\n    df = transform(df, groupby='time_id', feat='size', agg='mean' )\n    df['price_dif'] = ((df['price'] - df.groupby(['time_id'])['price'].shift(1).reset_index(drop=True)) \/ df['price']).fillna(0.)\n    df['tendencyV'] = df['size'] * df['price_dif']\n    df['f_max'] = 1 * (df['price'] >= df['price_mean'])\n    df['f_min'] = 1 * (df['price'] < df['price_mean'])\n    df['df_max'] = 1 * (df['price_dif'] >= 0)\n    df['df_min'] = 1 * (df['price_dif'] < 0)\n    df['abs_dif'] = (df['price'] - df['price_mean']).abs()\n    df['price_sqr'] = df['price']**2\n    df['size_dif'] = (df['size'] - df['size_mean']).abs()\n    df['size_sqr'] = df['size']**2\n    df['iqr_p25'] = df.groupby(['time_id'])['price'].quantile(0.15).reset_index(drop=True)\n    df['iqr_p75'] = df.groupby(['time_id'])['price'].quantile(0.85).reset_index(drop=True)\n    df['iqr_p_v25'] = df.groupby(['time_id'])['size'].quantile(0.15).reset_index(drop=True)\n    df['iqr_p_v75'] = df.groupby(['time_id'])['size'].quantile(0.85).reset_index(drop=True)\n\n    df = transform(df, groupby='time_id', feat='price_dif', agg='std' )\n    df = transform(df, groupby='time_id', feat='tendencyV', agg='std' )\n    df = transform(df, groupby='time_id', feat='f_max', agg='std' )\n    df = transform(df, groupby='time_id', feat='f_min', agg='std' )\n    df = transform(df, groupby='time_id', feat='df_max', agg='std' )\n    df = transform(df, groupby='time_id', feat='df_min', agg='std' )\n    df = transform(df, groupby='time_id', feat='size_dif', agg='std' )\n    \n    dt = df.groupby('time_id')[['tendencyV','price','price_dif','f_max','f_min','df_max','df_min','abs_dif','price_sqr','size_dif','size_sqr','iqr_p25','iqr_p75','iqr_p_v25','iqr_p_v75'\n                                 ,'price_dif_std','tendencyV_std','f_max_std','f_min_std','df_max_std','df_min_std','size_dif_std'\n                               ]].agg(\n        {\n            'tendencyV':['sum','std','max', 'min'],\n            'price':['mean','std','max', 'min'],\n            'price_dif':['mean','std','max', 'min'],\n            'f_max':['mean','std','max', 'min'],\n            'f_min':['mean','std','max', 'min'],\n            'df_max':['mean','std','max', 'min'],\n            'df_min':['mean','std','max', 'min'],\n            'abs_dif':['median','std','max', 'min'],\n            'price_sqr':['sum','std','max', 'min'],\n            'size_dif':['median','std','max', 'min'],\n            'size_sqr':['sum','std','max', 'min'],\n            'iqr_p25':['mean','std','max', 'min'],\n            'iqr_p75':['mean','std','max', 'min'],\n            'iqr_p_v25':['mean','std','max', 'min'],\n            'iqr_p_v75':['mean','std','max', 'min'],\n            'price_dif_std':['mean','std','max', 'min'],\n            'tendencyV_std':['mean','std','max', 'min'],\n            'f_max_std':['mean','std','max', 'min'],\n            'f_min_std':['mean','std','max', 'min'],\n            'df_max_std':['mean','std','max', 'min'],\n            'df_min_std':['mean','std','max', 'min'],\n            'size_dif_std':['mean','std','max', 'min']\n        }\n    )\n\n    \n    dt.columns = [i+'_'+j for i, j in dt.columns] \n    df_feature = df_feature.merge(dt, left_on='time_id_', right_index=True, how='left')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    \n    # Drop tmp columns\n    df_feature = df_feature.sort_values(['time_id_' ]).reset_index(drop=True)\n    \n    stock_id = file_path.split('=')[1]\n    df_feature['stock_id'] = str(stock_id) + '-'\n    df_feature['row_id'] = df_feature['stock_id'] + df_feature['time_id_'].astype(str)\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200', 'time_id_','time_id__100','stock_id'], axis = 1, inplace = True)\n\n    fnames = ['trade_' + f for f in df_feature.columns]\n    fnames[-1] = 'row_id'\n    df_feature.columns = fnames\n\n    return df_feature","44aad6df":"%%time\nDF_TRAIN = []\nfor stock_id in tqdm(train_stock_ids, disable=True):\n    df_tmp = pd.merge( \n        book_preprocessor(data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)),\n        trade_preprocessor(data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)),\n        on = 'row_id',\n        how = 'left'\n    )\n    df_tmp['stock_id'] = stock_id\n    df_tmp = convert_to_32bit(df_tmp) # to save memory\n    #df_tmp.to_parquet( 'train_parquet\/'+str(stock_id)+'.parquet' )\n    DF_TRAIN.append(df_tmp)\n\n# Concatenate all stock_id in the same dataframe\nDF_TRAIN = pd.concat(DF_TRAIN, ignore_index=True )\n_ = gc.collect()\n\n# Flag to filter train\/test rows\nDF_TRAIN['is_test'] = 0\nDF_TRAIN.shape","9058f316":"%%time\nDF_TEST = []\nfor stock_id in tqdm(test_stock_ids, disable=True):\n    df_tmp = pd.merge( \n        book_preprocessor(data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)),\n        trade_preprocessor(data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)),\n        on = 'row_id',\n        how = 'left'\n    )\n    df_tmp['stock_id'] = stock_id\n    df_tmp = convert_to_32bit(df_tmp) # to save memory\n    #df_tmp.to_parquet( 'test_parquet\/'+str(stock_id)+'.parquet' )\n    DF_TEST.append(df_tmp)\n    \n# Concatenate all stock_id in the same dataframe\nDF_TEST = pd.concat(DF_TEST, ignore_index=True )\n_ = gc.collect()\n\n# Flag to filter train\/test rows\nDF_TEST['is_test'] = 1\nDF_TEST.shape","df16e928":"TRAIN = pd.concat( [DF_TRAIN, DF_TEST] ).sort_values(['stock_id','time_id_']).reset_index(drop=True)\n\ndel DF_TRAIN, DF_TEST\n_ = gc.collect()\nTRAIN.shape","f3c57afe":"%%time\n\ndef get_time_stock(df_):\n    vol_cols = ['log_return1_sqr_sum_500', 'log_return2_sqr_sum_500', 'log_return3_sqr_sum_500', 'log_return4_sqr_sum_500', 'trade_log_return_sqr_sum', 'trade_log_return_sqr_std', 'trade_seconds_in_bucket_nunique' ]\n\n    df = df_.copy()\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_stock_id.columns = ['_'.join(col) + '_stock' for col in df_stock_id.columns]\n\n    df_time_id = df.groupby(['time_id_'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_time_id.columns = ['_'.join(col)+ '_time' for col in df_time_id.columns]\n    \n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id_'], right_on = ['time_id___time'])\n    df.drop(['stock_id__stock', 'time_id___time'], axis = 1, inplace = True)\n    return df\n\nTRAIN_ = get_time_stock(TRAIN)\nTRAIN_.drop(['stock_id','time_id_'], axis = 1, inplace = True)\nprint(TRAIN_.shape)\nprint(TRAIN_.head())","96568d25":"train = train.merge(TRAIN_, on='row_id', how='left' )\ntest  = test.merge(TRAIN_, on='row_id', how='left' )\n\ndel TRAIN_, TRAIN\n_ = gc.collect()\n\ntrain.shape, test.shape","a4a6e253":"train.head()","223a6828":"%%time\ntrain_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns=['stock_id'], values=['target']).fillna(0.)\ncorr = train_p.corr()\n\nkm = cuml.KMeans(n_clusters=6, max_iter=2000, n_init=5).fit(corr)\ndf = pd.DataFrame( {'stock_id': [ f[1] for f in corr.columns ], 'cluster': km.labels_} )\ndf = convert_to_32bit(df)\n\ntrain = train.merge(df, on='stock_id', how='left')\ntest = test.merge(df, on='stock_id', how='left')\n\n\ndel train_p, df, corr, km\n_ = gc.collect()\n\n# Clusters found\ntrain.groupby('cluster')['time_id'].agg('count')","4fb87c3c":"# replace by order sum (tau)\ntrain['size_tau'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_nunique'] )\ntest['size_tau'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_nunique'] )\ntrain['size_tau_400'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_nunique_mean_time'] )\ntest['size_tau_400'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_nunique_mean_time'] )\ntrain['size_tau_300'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_nunique_std_time'] )\ntest['size_tau_300'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_nunique_std_time'] )\ntrain['size_tau_200'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_nunique_max_time'] )\ntest['size_tau_200'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_nunique_max_time'] )\ntrain['size_tau_100'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_nunique_min_time'] )\ntest['size_tau_100'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_nunique_min_time'] )\n\ntrain['size_tau2'] = np.sqrt( 1\/ train['trade_order_count_sum'] )\ntest['size_tau2'] = np.sqrt( 1\/ test['trade_order_count_sum'] )\n\ntrain['size_tau2_400'] = np.sqrt( 0.33\/ train['trade_order_count_sum'] )\ntest['size_tau2_400'] = np.sqrt( 0.33\/ test['trade_order_count_sum'] )\ntrain['size_tau2_300'] = np.sqrt( 0.5\/ train['trade_order_count_sum'] )\ntest['size_tau2_300'] = np.sqrt( 0.5\/ test['trade_order_count_sum'] )\n\ntrain['size_tau2_200'] = np.sqrt( 0.66\/ train['trade_order_count_sum'] )\ntest['size_tau2_200'] = np.sqrt( 0.66\/ test['trade_order_count_sum'] )\n\n# delta tau\ntrain['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\ntest['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']","9847a159":"matTrain = []\nmatTest = []\n\n# 6 clusters\nfor ind in range(train.cluster.max()+1):\n    print(ind)\n    newDf = train.loc[train['cluster']==ind].copy()\n    newDf = newDf.groupby(['time_id']).agg('mean')\n    newDf.loc[:,'stock_id'] = 127+ind\n    matTrain.append ( newDf )\n    \n    newDf = test.loc[test['cluster']==ind].copy()\n    newDf = newDf.groupby(['time_id']).agg('mean')\n    newDf.loc[:,'stock_id'] = 127+ind\n    matTest.append ( newDf )\n    \nmatTrain = pd.concat(matTrain).reset_index()\nmatTrain.drop(columns=['target'],inplace=True)\n\nmatTest = pd.concat(matTest).reset_index()\n\nmatTrain.shape, matTest.shape","159b6174":"matTest = pd.concat([matTest , matTrain.loc[matTrain.time_id==5]])\nmatTrain = matTrain.pivot(index='time_id', columns='stock_id')\nmatTrain.columns = [x[0]+'_stock'+str(int(x[1])) for x in matTrain.columns]\nmatTrain.reset_index(inplace=True)\n\nmatTest = matTest.pivot(index='time_id', columns='stock_id')\nmatTest.columns = [x[0]+'_stock'+str(int(x[1])) for x in matTest.columns]\nmatTest.reset_index(inplace=True)\n\nmatTrain.shape, matTest.shape","b19bad19":"cluster_cols = ['size_tau2', 'wap1_sum', 'wap2_sum', 'wap3_sum', 'wap4_sum', 'log_return1_sqr_sum', 'log_return2_sqr_sum', \n    'log_return3_sqr_sum', 'log_return4_sqr_sum', 'total_volume_sum', 'trade_size_sum', 'trade_order_count_sum',\n    'price_spread_sum', 'bid_spread_sum', 'ask_spread_sum', 'volume_imbalance_sum', 'bid_ask_spread_sum']\n\ncluster_labels = ['_stock127', '_stock128', '_stock129', '_stock130', '_stock131', '_stock132']\n\nkfeatures = [\n    'time_id'\n]\n\nfor cols in cluster_cols:\n    for label in cluster_labels:\n        kfeatures.append(cols+label)\n    \nprint(kfeatures)\n        \nmatTrain = convert_to_32bit(matTrain)\nmatTest = convert_to_32bit(matTest)\n\ntrain = pd.merge(train,matTrain[kfeatures],how='left',on='time_id')\ntest = pd.merge(test,matTest[kfeatures],how='left',on='time_id')\n_ = gc.collect()\n\nprint(train.shape, test.shape)","6c1b403a":"# train=train[~(train[\"stock_id\"]==31)].reset_index(drop=True)\n# _= gc.collect()\n\ntrain = convert_to_32bit(train)\ntest  = convert_to_32bit(test)\n_= gc.collect()\n\ntrain.shape, test.shape","85b45a50":"y_target = train.target.to_pandas() #need to be numpy or pandas for sklearn \ntime_id = train.time_id.to_pandas()\nNFOLD = 5\n\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n\ntrain_pandas = train.to_pandas()\ntest_pandas = test.to_pandas()\n\n# Target min and max values\nnp.min(y_target), np.max(y_target)","ff0b0a0f":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer","24e279ce":"def apply_quantile(train, test):\n    colNames = [col for col in list(train.columns)\n                if col not in {\"stock_id\", \"time_id\", \"target\", \"target_sqr\", \"row_id\", 'is_train'}]\n\n    train = train.replace([np.inf, -np.inf], np.nan)\n    test = test.replace([np.inf, -np.inf], np.nan)\n    train_nn=train[colNames].copy()\n    test_nn=test[colNames].copy()\n    for col in colNames:\n        #print(col)\n        qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n        train_nn[col] = qt.fit_transform(train_nn[[col]])\n        test_nn[col] = qt.transform(test_nn[[col]])\n    return train_nn, test_nn","566f61b9":"%%time\ntrain_nn, test_nn = apply_quantile(train_pandas, test_pandas)\n\ntrain_nn['target'] = train_pandas['target']\ntrain_nn['time_id'] = train_pandas['time_id']\ntrain_nn['stock_id'] = train_pandas['stock_id']\ntest_nn['time_id'] = test_pandas['time_id']\ntest_nn['stock_id'] = test_pandas['stock_id']","556c8006":"# kfold based on the knn++ algorithm. It used\nclass KNN_Fold:\n    def __init__(self, n_splits = 5):\n        self.nfolds = n_splits\n        nfolds = n_splits\n        df_train = pandas.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n        out_train = df_train.pivot(index='time_id', columns='stock_id', values='target')\n\n        #out_train[out_train.isna().any(axis=1)]\n        out_train = out_train.fillna(out_train.mean())\n#         print(out_train.head())\n\n        # code to add the just the read data after first execution\n\n        # data separation based on knn ++\n        index = []\n        totDist = []\n        values = []\n        # generates a matriz with the values of \n        mat = out_train.values\n\n        scaler = MinMaxScaler(feature_range=(-1, 1))\n        mat = scaler.fit_transform(mat)\n\n        nind = int(mat.shape[0]\/nfolds) # number of individuals\n\n        # adds index in the last column\n        mat = np.c_[mat,np.arange(mat.shape[0])]\n\n\n        lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n\n        lineNumber = np.sort(lineNumber)[::-1]\n\n        for n in range(nfolds):\n            totDist.append(np.zeros(mat.shape[0]-nfolds))\n\n        # saves index\n        for n in range(nfolds):\n\n            values.append([lineNumber[n]])    \n\n\n        s=[]\n        for n in range(nfolds):\n            s.append(mat[lineNumber[n],:])\n\n            mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\n        for n in range(nind-1):    \n\n            luck = np.random.uniform(0,1,nfolds)\n\n            for cycle in range(nfolds):\n                 # saves the values of index           \n\n                s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n\n                sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n                totDist[cycle] += sumDist        \n\n                # probabilities\n                f = totDist[cycle]\/np.sum(totDist[cycle]) # normalizing the totdist\n                j = 0\n                kn = 0\n                for val in f:\n                    j += val        \n                    if (j > luck[cycle]): # the column was selected\n                        break\n                    kn +=1\n                lineNumber[cycle] = kn\n\n                # delete line of the value added    \n                for n_iter in range(nfolds):\n\n                    totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n                    j= 0\n\n                s[cycle] = mat[lineNumber[cycle],:]\n                values[cycle].append(int(mat[lineNumber[cycle],-1]))\n                mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\n\n        for n_mod in range(nfolds):\n            values[n_mod] = out_train.index[values[n_mod]]    \n        self.values = values\n        \n    def split(self, train, y_target, time_id):\n        nfolds = self.nfolds\n        values = self.values\n\n\n        for fold in range(nfolds):\n            indexes = np.arange(nfolds).astype(int)    \n            indexes = np.delete(indexes,obj=fold, axis=0) \n\n            indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n            yield (list(train.time_id.isin(indexes)), list(train.time_id.isin(values[fold])))","30443780":"# del train,test\n_= gc.collect()","9ff9ed90":"#reset Keras Session\ndef reset_keras():\n    sess = tf.compat.v1.keras.backend.get_session()\n    tf.compat.v1.keras.backend.clear_session()\n    sess.close()\n    sess = tf.compat.v1.keras.backend.get_session()\n\n    # use the same config as you used to create the session\n    config = tf.compat.v1.ConfigProto()\n    config.gpu_options.per_process_gpu_memory_fraction = 1\n    config.gpu_options.visible_device_list = \"0\"\n    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n    gc.collect()","f679ce08":"from numpy.random import seed\nimport tensorflow as tf\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n  tf.config.experimental.set_memory_growth(gpu, True)\n\ntf.random.set_seed(42)\nfrom tensorflow import keras\nimport numpy as nptensorflow\nfrom tensorflow.keras import backend as K\n#from tensorflow.python.client import device_lib\n#print(device_lib.list_local_devices())","95fe4583":"def root_mean_squared_per_error(y_true, y_pred):\n#     tf.print('y_true', y_true)\n#     tf.print('y_pred', y_pred)\n    return K.sqrt(K.mean(K.square( (y_true - y_pred)\/ y_true )))\n","6e4584c7":"def AE_MLP_model(max_id_num, feature_num, stock_embedding_size, hidden_units, dropout_rates):\n\n    cat_data = train_nn['stock_id']\n    print('AE_MLP_model feature number:', feature_num)\n    \n    # Each instance will consist of two inputs: a single user id, and a single movie id\n    stock_id_input = tf.keras.Input(shape=(1,), name='stock_id')\n    num_input = tf.keras.Input(shape=(feature_num,), name='num_data')\n\n    #embedding, flatenning and concatenating\n    stock_embedded = tf.keras.layers.Embedding(max_id_num+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    stock_flattened = tf.keras.layers.Flatten()(stock_embedded)\n    inp = tf.keras.layers.Concatenate()([stock_flattened, num_input])\n    x0 = tf.keras.layers.BatchNormalization()(inp)\n    x0 = inp\n    \n    encoder = tf.keras.layers.GaussianNoise(dropout_rates[0])(x0)\n    encoder = tf.keras.layers.Dense(hidden_units[0])(encoder)\n    encoder = tf.keras.layers.BatchNormalization()(encoder)\n    encoder = tf.keras.layers.Activation('swish')(encoder)\n    \n    decoder = tf.keras.layers.Dropout(dropout_rates[1])(encoder)\n    # decoder is features+stock ids\n    decoder = tf.keras.layers.Dense(feature_num+1, name = 'decoder')(decoder)\n\n    x_ae = tf.keras.layers.Dense(hidden_units[1])(decoder)\n    x_ae = tf.keras.layers.BatchNormalization()(x_ae)\n    x_ae = tf.keras.layers.Activation('swish')(x_ae)\n    x_ae = tf.keras.layers.Dropout(dropout_rates[2])(x_ae)\n    \n    out_ae = tf.keras.layers.Dense(1, activation = 'linear', name = 'ae_prediction')(x_ae)\n    \n    x = tf.keras.layers.Concatenate()([x0, encoder])\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(dropout_rates[3])(x)\n    \n    for i in range(2, len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation('swish')(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 2])(x)\n    \n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(x)\n    \n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = [decoder\n               ,out_ae\n               ,out\n              ],\n    )\n    \n    model.summary()\n    \n    return model","a2ca9e77":"def train_and_evaluate_complex_nn(train, test, params, model_creator):\n    \n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"target_sqr\", \"row_id\", 'is_train'}]\n    y = train['target']\n    \n    y_train = np.zeros(train.shape[0])\n    y_test = np.zeros(test.shape[0])\n    \n    print('Check null in train', train[features].isnull().sum())\n    print('Check null in test', test[features].isnull().sum())\n    train[features] = train[features].fillna(train[features].mean())\n    test[features] = test[features].fillna(train[features].mean())\n    print('Check null in test again', test[features].isnull().sum())\n    \n#     kf = GroupKFold(n_splits=NFOLD)\n    kf = KNN_Fold(n_splits=NFOLD)\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n        x_train, x_val = train.iloc[train_idx], train.iloc[valid_idx]\n        y_tra, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n        \n        cat_data = x_train['stock_id']\n        features.remove('stock_id')\n\n        scaler = MinMaxScaler(feature_range=(-1, 1))\n        num_data = x_train[features].values\n        num_data = scaler.fit_transform(num_data)\n        cat_data_val = x_val['stock_id']\n        num_data_val = x_val[features].values\n        num_data_val = scaler.transform(num_data_val)\n        \n        # Add stockid to validate decorder\n        num_data_with_stockid = np.c_[num_data, cat_data]\n        \n        # Add stockid to validate decorder\n        num_data_test_with_stockid = np.c_[num_data_val, cat_data_val]\n        \n        model = model_creator(max(cat_data), len(features), params['stock_embedding_size'], \n                             params['hidden_units'], params['dropout_rates'])\n\n        model.compile(\n            keras.optimizers.Adam(learning_rate=params['learning_rate']),\n            loss={\n#                 'decoder': tf.keras.losses.MeanSquaredError()\n                'decoder': tf.keras.losses.MeanAbsoluteError()\n                  ,'ae_prediction': root_mean_squared_per_error\n                  ,'prediction': root_mean_squared_per_error\n                 }\n        )\n        \n#         \n        es = tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss', patience=20, verbose=0,\n            mode='min',restore_best_weights=True)\n\n        plateau = tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss', factor=0.2, patience=7, verbose=0,\n            mode='min')\n\n        model.fit([cat_data, num_data], \n                  [num_data_with_stockid, y_tra, y_tra],               \n                  batch_size=params['batch_size'],\n                  epochs=params['epochs'],\n                  validation_data=([cat_data_val, num_data_val], [num_data_test_with_stockid, y_val, y_val]),\n                  callbacks=[es, \n                            plateau\n                            ],\n                  validation_batch_size=len(y_val),\n                  shuffle=True,\n                 verbose = 1)\n\n        pred_val = np.clip(model.predict([cat_data_val, num_data_val])[-1].reshape(1,-1)[0], low_clip, upper_clip)\n        y_train[valid_idx] = pred_val\n        test_nn = test[features].values\n        test_nn = scaler.transform(test_nn)\n        y_test += np.clip(model.predict([test['stock_id'], test_nn])[-1].reshape(1,-1)[0], low_clip, upper_clip)\n        features.append('stock_id')\n        \n        print(y_train[valid_idx][:3], y_test[:3])\n        print('NN base Rmspe Fold:', rmspe(y_target[valid_idx], y_train[valid_idx]))\n        \n        #Delete model and release GPU memory\n        del model, cat_data, num_data, cat_data_val, num_data_val, scaler, test_nn\n        gc.collect()\n        reset_keras()\n    y_test\/=NFOLD\n    \n    return y_train, y_test","832f3e63":"nn_complex_time = time.time()\n\nparams = {\n    'batch_size': 4096,\n    'epochs': 1000,\n    'learning_rate': 1e-3,\n    'stock_embedding_size': 8,\n    'hidden_units': [96, 96, 128, 64, 32],\n    'dropout_rates': [0.03527936123679956,\n                     0.038424974585075086,\n                     0.42409238408801436,\n                     0.10431484318345882,\n                     0.49230389137187497,\n                     0.32024444956111164,\n                     0.2716856145683449,\n                     0.4379233941604448]\n}\n\ny_ae_train1, y_ae_test1 = train_and_evaluate_complex_nn(train_nn, test_nn, params, AE_MLP_model)\n_= gc.collect()\n\nprint('Check zero in prediction:', (y_ae_train1 == 0).sum())\nnp.savetxt('pred_ae.csv', y_ae_train1, delimiter=',')\n              \nprint( 'NN AE Rmspe CV:', rmspe(y_target, y_ae_train1), 'time: ', int(time.time() - nn_complex_time), 's', y_ae_test1[:3])","1080d41e":"def base_model(max_id_num, feature_num, stock_embedding_size, hidden_units, dropout_rates):\n    print('base_model', max_id_num, feature_num, stock_embedding_size, hidden_units, dropout_rates)\n    \n    # Each instance will consist of two inputs: a single user id, and a single movie id\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(feature_num,), name='num_data')\n    input_dense_num = feature_num\n    if hidden_units[0] != -1:\n        input_dense_num = hidden_units[0]\n    \n    input_features = keras.layers.Dense(input_dense_num)(num_input)\n    \n    input_features = keras.layers.BatchNormalization()(input_features)\n    input_features = keras.layers.Activation('swish')(input_features)\n    if dropout_rates[0] < 1:\n        input_features = keras.layers.Dropout(dropout_rates[0])(input_features)\n\n    #embedding, flatenning and concatenating\n    stock_embedded = keras.layers.Embedding(max_id_num+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    out = keras.layers.Concatenate()([stock_flattened, input_features])\n    \n#     # Add one or more hidden layers\n    for i in range(1, len(hidden_units)):\n        out = keras.layers.Dense(hidden_units[i])(out)\n        out = keras.layers.BatchNormalization()(out)\n        out = keras.layers.Activation('swish')(out)\n        if dropout_rates[i] < 1:\n            out = keras.layers.Dropout(dropout_rates[i])(out)\n        \n    #out = keras.layers.Concatenate()([out, num_input])\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n    \n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out\n    )\n    \n    model.summary()\n    \n    return model\n\ndef train_and_evaluate_nn_base(train, test, params):\n    \n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"target_sqr\", \"row_id\", 'is_train'}]\n    y = train['target']\n    \n    y_train = np.zeros(train.shape[0])\n    y_test = np.zeros(test.shape[0])\n    \n    print('Check null in train', train[features].isnull().sum())\n    print('Check null in test', test[features].isnull().sum())\n    train[features] = train[features].fillna(train[features].mean())\n    test[features] = test[features].fillna(train[features].mean())\n    print('Check null in test again', test[features].isnull().sum())\n    \n#     kf = GroupKFold(n_splits=NFOLD)\n    kf = KNN_Fold(n_splits=NFOLD)\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n        x_train, x_val = train.iloc[train_idx], train.iloc[valid_idx]\n        y_tra, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n        \n        cat_data = x_train['stock_id']\n        features.remove('stock_id')\n\n        scaler = MinMaxScaler(feature_range=(-1, 1))\n        num_data = x_train[features].values\n        num_data = scaler.fit_transform(num_data)\n        cat_data_val = x_val['stock_id']\n        num_data_val = x_val[features].values\n        num_data_val = scaler.transform(num_data_val)\n        \n        model = base_model(max(cat_data), len(features), params['stock_embedding_size'], params['hidden_units'], params['dropout_rates'])\n\n        model.compile(\n            keras.optimizers.Adam(learning_rate=params['learning_rate']),\n            loss=root_mean_squared_per_error\n        )\n        \n#         \n\n        es = tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss', patience=20, verbose=0,\n            mode='min',restore_best_weights=True)\n\n        plateau = tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss', factor=0.2, patience=7, verbose=0,\n            mode='min')\n\n        model.fit([cat_data, num_data], \n                  y_tra,               \n                  batch_size=params['batch_size'],\n                  epochs=params['epochs'],\n                  validation_data=([cat_data_val, num_data_val], y_val),\n                  callbacks=[es, \n                            plateau\n                            ],\n                  validation_batch_size=len(y_val),\n                  shuffle=True,\n                 verbose = 1)\n\n        pred_val = np.clip(model.predict([cat_data_val, num_data_val]).reshape(1,-1)[0], low_clip, upper_clip)\n        y_train[valid_idx] = pred_val\n        test_nn = test[features].values\n        test_nn = scaler.transform(test_nn)\n        y_test += np.clip(model.predict([test['stock_id'], test_nn]).reshape(1,-1)[0], low_clip, upper_clip)\n        features.append('stock_id')\n        \n        print(y_train[valid_idx][:3], y_test[:3])\n        print('NN base Rmspe Fold:', rmspe(y_target[valid_idx], y_train[valid_idx]))\n        \n        #Delete model and release GPU memory\n        del model, cat_data, num_data, cat_data_val, num_data_val, scaler, test_nn\n        gc.collect()\n        reset_keras()\n    y_test\/=NFOLD\n    \n    \n    \n    return y_train, y_test\n\n","75ff0106":"nn_base_time = time.time()\n\nparams = {\n    'batch_size': 4096,\n    'epochs': 1000,\n    'learning_rate': 0.006,\n    'stock_embedding_size': 8,\n#     'hidden_units': [128, 128, 64, 32], 1.180226882297499\n    'hidden_units': [-1, 128, 64, 32],\n    'dropout_rates': [0.03, 0.32024444956111164,\n                     0.2716856145683449,\n                     0.4379233941604448] # 1 means no dropout\n}\n\ny_nn_train1, y_nn_test1 = train_and_evaluate_nn_base(train_nn, test_nn, params)\n_= gc.collect()\n\nprint('Check zero in prediction:', (y_nn_train1 == 0).sum())\nnp.savetxt('pred_nn.csv', y_nn_train1, delimiter=',')\n              \nprint( 'NN base Rmspe CV:', rmspe(y_target, y_nn_train1), 'time: ', int(time.time() - nn_base_time), 's', y_nn_test1[:3])","082f3398":"#%%script echo skipping\nxgbtime = time.time()\n\n# Define the custom metric to optimize\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    err = rmspe(labels, preds)\n    return 'rmspe', err\n\ndef train_and_evaluate_xgb(train, test, params, colNames):\n    # Sample weight\n    train['target_sqr'] = 1. \/ (train['target'] ** 1.60 + 9e-7)    \n    train.loc[train.stock_id==31,'target_sqr'] = 0.0001\n\n    dtest = xgb.DMatrix(test[colNames])\n\n    y_train = np.zeros(len(train))\n    y_test = np.zeros(len(test))\n\n    kf = GroupKFold(n_splits=NFOLD)\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n        dtrain = xgb.DMatrix(train.loc[train_idx, colNames], train.loc[train_idx, 'target']*10000, weight=train.loc[train_idx, 'target_sqr'])\n        dvalid = xgb.DMatrix(train.loc[valid_idx, colNames], train.loc[valid_idx, 'target']*10000)\n        model = xgb.train(\n            params,\n            dtrain,\n            3000,\n            #[(dtrain, \"train\"), (dvalid, \"valid\")],\n            [(dvalid, \"valid\")],\n            verbose_eval=250,\n            early_stopping_rounds=50,\n            feval=evalerror,\n        )\n        y_train[valid_idx] = np.clip(model.predict(dvalid)\/10000, low_clip, upper_clip)\n        y_test += np.clip((model.predict(dtest))\/10000, low_clip, upper_clip)\n        print( 'Rmspe Fold:', rmspe(y_target[valid_idx], y_train[valid_idx]) )\n    y_test \/= NFOLD\n    \n    print( 'XGBoost Rmspe CV:', rmspe(y_target, y_train) )\n    print( pandas.DataFrame.from_dict( model.get_score(), orient='index').sort_values(0, ascending=False).head(20) )\n    print()\n    \n    del model, dtest, dtrain, dvalid\n    _ = gc.collect()\n    \n    return y_train, y_test\n\n\ncolNames = [col for col in list(train.columns) if col not in {'is_train', 'time_id', 'target', 'row_id', 'target_sqr', 'is_train'}]\ncolNames = [col for col in colNames if col.find('min')<0 ]\nparams = {\n        \"subsample\": 0.60,\n        \"colsample_bytree\": 0.40,\n        \"max_depth\": 6,\n        \"learning_rate\": 0.02,\n        \"objective\": \"reg:squarederror\",\n        'disable_default_eval_metric': 1, # <- necessary for XGBoost to earlystop by Rmspe and not the default rmse\n        \"nthread\": -1,\n        \"tree_method\": \"gpu_hist\",\n        \"gpu_id\": 0,\n        \"max_bin\": 128, \n        'min_child_weight': 0,\n        'reg_lambda': 0.001,\n        'reg_alpha': 0.001, \n        'seed' : 2021,\n        'single_precision_histogram': False,\n    }\ny_train1a, y_test1a = train_and_evaluate_xgb(train, test, params, colNames)\n\n\ncolNames = [col for col in list(train.columns) if col not in {'is_train', 'time_id', 'target', 'row_id', 'target_sqr', 'is_train'}]\ncolNames = [col for col in colNames if col.find('max')<0 ]\nparams = {\n        \"subsample\": 0.85,\n        \"colsample_bytree\": 0.25,\n        \"max_depth\": 7,\n        \"learning_rate\": 0.02,\n        \"objective\": \"reg:squarederror\",\n        'disable_default_eval_metric': 1, # <- necessary for XGBoost to earlystop by Rmspe and not the default rmse\n        \"nthread\": -1,\n        \"tree_method\": \"gpu_hist\",\n        \"gpu_id\": 0,\n        \"max_bin\": 128, \n        'min_child_weight': 0,\n        'reg_lambda': 0.001,\n        'reg_alpha': 0.001, \n        'seed' : 2022,\n        'single_precision_histogram': False,\n    }\ny_train1b, y_test1b = train_and_evaluate_xgb(train, test, params, colNames)\n\n\ny_train1 = 0.5*y_train1a + 0.5*y_train1b\ny_test1  = 0.5*y_test1a  + 0.5*y_test1b\n_= gc.collect()\nxgbtime = time.time() - xgbtime\n\nprint( 'XGBoost Rmspe CV:', rmspe(y_target, y_train1), 'time: ', int(xgbtime), 's', y_test1[:3] )","8c56bf43":"#%%script echo skipping\ncatbtime = time.time()\n\ndef train_and_evaluate_catb(train, test, params):\n\n    # Sample weight\n    train['target_sqr'] = 1. \/ (train['target'] ** 1.75 + 1e-6)\n    train.loc[train.stock_id==31,'target_sqr'] = 1.\n\n    colNames = [col for col in list(train.columns) if col not in {'is_train', 'time_id', 'target', 'row_id', 'target_sqr', 'is_train'}]\n\n    y_train = np.zeros(len(train))\n    y_test = np.zeros(len(test))\n\n    kf = GroupKFold(n_splits=NFOLD)\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n\n        model = CatBoostRegressor(\n            iterations=3000,\n            learning_rate=0.05,\n            depth=7,\n            loss_function='RMSE',\n            #l2_leaf_reg = 0.001,\n            #random_strength = 0.5,\n            #bagging_temperature = 1.0,\n            task_type=\"GPU\",\n            random_seed = 2021,\n        )        \n        model.fit(\n            X=train.loc[train_idx, colNames].to_pandas(), y=train.loc[train_idx, 'target'].to_pandas(),\n            sample_weight = train.loc[train_idx, 'target_sqr'].to_pandas(),\n            eval_set = (train.loc[valid_idx, colNames].to_pandas(), train.loc[valid_idx, 'target'].to_pandas(),),\n            early_stopping_rounds = 20,\n            cat_features = [0],\n            verbose=False)\n\n        y_train[valid_idx] = np.clip(model.predict(train.loc[valid_idx, colNames].to_pandas()), low_clip, upper_clip)\n        y_test += np.clip((model.predict(test[colNames].to_pandas())), low_clip, upper_clip)\n        print( 'Catboost Rmspe Fold:', rmspe(y_target[valid_idx], y_train[valid_idx]) )        \n        print()\n    y_test \/= NFOLD\n    return y_train, y_test\n\n\ny_train2, y_test2 = train_and_evaluate_catb(train, test, params)\n_= gc.collect()\ncatbtime = time.time() - catbtime\n     \nprint( 'Catboost Rmspe CV:', rmspe(y_target, y_train2), 'time: ', int(catbtime), 's', y_test2[:3]  )","1449bd4e":"lgbtime = time.time()\n\n# Define the custom metric to optimize\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\ndef train_and_evaluate_lgb(train, test, params):\n    \n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"target_sqr\", \"row_id\", 'is_train'}]\n    y = train['target']\n    \n    y_train = np.zeros(train.shape[0])\n    y_test = np.zeros(test.shape[0])\n    \n    kf = GroupKFold(n_splits=NFOLD)\n    for fold, (trn_ind, val_ind) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        y_tra, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        \n        train_dataset = lgb.Dataset(x_train[features], y_tra, weight = (1. \/ (np.square(y_tra) + 1e-6)) )\n        valid_dataset = lgb.Dataset(x_val[features], y_val)\n        model = lgb.train(params = params,\n                          num_boost_round=3000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, valid_dataset], \n                          verbose_eval = 100,\n                          early_stopping_rounds=20,\n                          feval = feval_rmspe)\n        \n        y_train[val_ind] = np.clip(model.predict(x_val[features]), low_clip, upper_clip)\n        print('LightGBM Rmspe Fold:', rmspe(y_val, y_train[val_ind]))\n        y_test += np.clip((model.predict(test[features])), low_clip, upper_clip)\n    y_test\/=NFOLD\n    \n    lgb.plot_importance(model,max_num_features=20)\n    \n    return y_train, y_test\n\n\nparams = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':255,\n    'min_data_in_leaf':750,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 3,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':2021,\n    'n_jobs':-1,\n    'verbose': -1,\n    'device': 'gpu',\n    'num_gpu': 1,\n    'gpu_platform_id':-1,\n    'gpu_device_id':-1,\n    'gpu_use_dp': False,\n}\n\ny_train3, y_test3 = train_and_evaluate_lgb(train_pandas, test_pandas, params)\n_= gc.collect()\n\nprint( 'LightGBM Rmspe CV:', rmspe(y_target, y_train3), 'time: ', int(time.time() - lgbtime), 's', y_test3[:3]   )","c40f1799":"%%script echo skipping\nprint( 'XGBoost Rmspe:', rmspe(y_target, y_train1) )\nprint( 'CatBoost Rmspe:', rmspe(y_target, y_train2) )\nprint( 'LightGBM Rmspe:', rmspe(y_target, y_train3) )\nprint( 'NN base Rmspe:', rmspe(y_target, y_nn_train1) )","885d9918":"ensemble_trains = [\n    y_train1, y_train2, y_train3, \n    y_nn_train1, \n    y_ae_train1\n]\n\nensemble_tests = [\n    y_test1, y_test2, y_test3, \n    y_nn_test1, \n    y_ae_test1\n]\n\nENSEMBLING_SIZE = len(ensemble_trains)","cc7ae7d0":"for y_train in ensemble_trains:\n    print('Ensemble Rmspe:', rmspe(y_target, y_train) )","ccb24105":"def show_error(train_pandas, ypred0, ypred1):\n    train_pandas['ypred'] = (ypred0+ypred1)\/2\n    train_pandas['error'] = (train_pandas['target'] - train_pandas['ypred']) \/ train_pandas['target']\n    train_pandas['error'] = train_pandas['error']**2\n\n    dt = train_pandas.groupby('stock_id')['error'].agg('mean').reset_index()\n    dt['error'] = np.sqrt(dt['error'])\n    dt = dt.sort_values('error', ascending=False)\n    dt.to_csv('error-contribution.csv', index=False)\n    del train_pandas['ypred'], train_pandas['error']\n    print(dt.head(10))\n    print(dt.tail(10))\n\n    \ndef calc_arit(W, ensemble_trains):\n    #ypred = W[0] * y_train1 + W[1] * y_train2 + W[2] * y_train3 + W[3] * y_nn_train1\n    ypred = 0\n    for i in range(len(ensemble_trains)):\n        ypred = W[i] * ensemble_trains[i] + ypred    \n    return ypred\n\ndef minimize_arit(W, y_target, ensemble_trains):\n    return rmspe(y_target, calc_arit(W, ensemble_trains))\n\ndef signed_power(var, p=2):\n    return np.sign(var) * np.abs(var)**p\n\ndef calc_geom(W, ensemble_trains):\n    #ypred = signed_power(y_train1, W[0]) * signed_power(y_train2, W[1]) * signed_power(y_train3, W[2]) * signed_power(y_nn_train1, W[3])\n    ypred = 1\n    for i in range(len(ensemble_trains)):\n        ypred = signed_power(ensemble_trains[i], W[i]) * ypred\n    return ypred\n\ndef minimize_geom(W, y_target, ensemble_trains):\n    return rmspe(y_target, calc_geom(W, ensemble_trains))    \n\nclass LinearEnsamble:\n    def __init__(self, ensemble_trains, y_target, ensemble_tests):\n        self.ensemble_trains = ensemble_trains\n        self.y_target = y_target\n        self.ensemble_tests = ensemble_tests\n        \n    \n    def fit(self):\n        ensemble_trains, y_target, ensemble_tests = self.ensemble_trains, self.y_target, self.ensemble_tests\n        \n        bounds = [(0, 1) for _ in ensemble_trains]\n        \n        #avoid negative weights for arit\n        W0 = minimize(minimize_arit, [1.\/ENSEMBLING_SIZE]*ENSEMBLING_SIZE, options={'gtol': 1e-6, 'disp': True},\n                 args=(y_target, ensemble_trains), bounds=bounds).x\n        print('Weights arit:',W0)\n        \n        #Geom weight can be negative\n        W1 = minimize(minimize_geom, [1.\/ENSEMBLING_SIZE]*ENSEMBLING_SIZE, options={'gtol': 1e-6, 'disp': True},\n                     args=(y_target, ensemble_trains)).x\n\n        print('weights geom:',W1)\n        \n        ypred0 = calc_arit(W0, ensemble_trains)\n        print( np.min(ypred0), np.mean(ypred0), np.max(ypred0), np.std(ypred0))\n\n        ypred1 = calc_geom(W1, ensemble_trains)\n        print( np.min(ypred1), np.mean(ypred1), np.max(ypred1), np.std(ypred1))\n\n        print( 'Ensemble:', rmspe(y_target, (ypred0+ypred1)\/2))\n        \n        print( np.min(ypred0),np.mean(ypred0),np.max(ypred0),np.std(ypred0) )\n        print( np.min(ypred1),np.mean(ypred1),np.max(ypred1),np.std(ypred1) )\n        \n        plt.hist(ypred0, bins=100)\n        plt.hist(ypred1, bins=100, alpha=0.5)\n        \n        self.W0 = W0\n        self.W1 = W1\n        \n        return ypred0, ypred1\n        \n    def predict(self):\n        print(self.ensemble_tests)\n        ypred0 = calc_arit(self.W0, self.ensemble_tests)\n        ypred1 = calc_geom(self.W1, self.ensemble_tests)\n\n        ypredtest = (ypred0+ypred1)\/2\n        print( ypred0[:3],  ypred1[:3], ypredtest[:3] )\n        \n        print(ypredtest)\n        return ypredtest","93e4ba2b":"linearEnsamble = LinearEnsamble(ensemble_trains, y_target, ensemble_tests)\nypred0, ypred1 = linearEnsamble.fit()\n\nshow_error(train_pandas, ypred0, ypred1)\n\nypredtest = linearEnsamble.predict()\nprint( ypred0[:3],  ypred1[:3], ypredtest[:3] )\n\ntest_pandas['target'] = ypredtest\ntest_pandas[['row_id', 'target']].to_csv('submission.csv',index = False)\ntest_pandas[['row_id', 'target']].head(3)","3e6e1a55":"# Checking how many stock_id there are in train and test","f1d608e9":"## Common methods for NN","d6ad1cdc":"# GPU accelerated solution using NVIDIA RAPIDS cudf and cuml\n# Data loading, preprocessing and feature engineering takes less than 3min in GPU.","095e3386":"## Baseline","020903c3":"## Data preprocessing","e600af7d":"# Loading train and test sets","17c5f45d":"# Neuron Network","69b893d7":"# Ensembling Time","3e1aec6c":"# XGBoost GPU","8137b56c":"## Release GPU for NN","3474e110":"# Now time to calculate correlation between all stock. The best way is using a correlation matrix, so first pivot all target variables by stock_id, then just calculate the correlation matrix.\n# To Find correlated stocks use Kmeans algorithm on the correlation matrix. This procedure is a bit leak because it not being processed using crossvalidation, but it won't leak much since only 6 clusters are being calculated.","16f66e3a":"# CatBoost","ac0c1cc4":"## Autoencoder","a9519ff9":"# LightGBM GPU","a4146589":"# Process all train .parquet files. Create features using cudf (GPU)\n# Note cudf speed to load and apply all feature engineering in all train set stocks.","2fff3566":"# Process all test .parquet files. Create features using cudf (GPU)"}}