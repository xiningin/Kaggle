{"cell_type":{"6271c25c":"code","28dec6b3":"code","b73a2db3":"code","1d3860af":"code","fb6a1799":"code","a873d57f":"code","8a9c3236":"code","0795a4c0":"code","f842d210":"code","6dff4dc9":"code","7fec265a":"code","66536cc7":"code","5fe7f8e6":"code","53d933aa":"code","660c5ae0":"code","88b6830f":"code","1b5b496f":"code","78099623":"code","261ec33b":"code","a600594c":"markdown","041bdc6a":"markdown","8e8e2da1":"markdown","1aafde8f":"markdown","c0e4f251":"markdown","5c8673f1":"markdown","ef14405b":"markdown","9fa21a52":"markdown","3d5f373b":"markdown","f9c2e1a3":"markdown","504f2bb0":"markdown","64eb13ae":"markdown","1f6662a8":"markdown","8fd44573":"markdown"},"source":{"6271c25c":"!pip install tensorflow==2.3.0 -q","28dec6b3":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport PIL\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n    \nprint(tf.__version__)","b73a2db3":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [176, 208]\nEPOCHS = 100","1d3860af":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    \"..\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/train\",\n    validation_split=0.2,\n    subset=\"training\",\n    seed=1337,\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    \"..\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/train\",\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=1337,\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n)","fb6a1799":"class_names = ['MildDementia', 'ModerateDementia', 'NonDementia', 'VeryMildDementia']\ntrain_ds.class_names = class_names\nval_ds.class_names = class_names\n\nNUM_CLASSES = len(class_names)","a873d57f":"plt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(train_ds.class_names[labels[i]])\n    plt.axis(\"off\")","8a9c3236":"def one_hot_label(image, label):\n    label = tf.one_hot(label, NUM_CLASSES)\n    return image, label\n\ntrain_ds = train_ds.map(one_hot_label, num_parallel_calls=AUTOTUNE)\nval_ds = val_ds.map(one_hot_label, num_parallel_calls=AUTOTUNE)","0795a4c0":"train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","f842d210":"NUM_IMAGES = []\n\nfor label in class_names:\n    dir_name = \"..\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/train\/\" + label[:-2] + 'ed'\n    NUM_IMAGES.append(len([name for name in os.listdir(dir_name)]))","6dff4dc9":"NUM_IMAGES","7fec265a":"def conv_block(filters):\n    block = tf.keras.Sequential([\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPool2D()\n    ]\n    )\n    \n    return block","66536cc7":"def dense_block(units, dropout_rate):\n    block = tf.keras.Sequential([\n        tf.keras.layers.Dense(units, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(dropout_rate)\n    ])\n    \n    return block","5fe7f8e6":"def build_model():\n    model = tf.keras.Sequential([\n        tf.keras.Input(shape=(*IMAGE_SIZE, 3)),\n        \n        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n        tf.keras.layers.MaxPool2D(),\n        \n        conv_block(32),\n        conv_block(64),\n        \n        conv_block(128),\n        tf.keras.layers.Dropout(0.2),\n        \n        conv_block(256),\n        tf.keras.layers.Dropout(0.2),\n        \n        tf.keras.layers.Flatten(),\n        dense_block(512, 0.7),\n        dense_block(128, 0.5),\n        dense_block(64, 0.3),\n        \n        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n    ])\n    \n    return model","53d933aa":"with strategy.scope():\n    model = build_model()\n\n    METRICS = [tf.keras.metrics.AUC(name='auc')]\n    \n    model.compile(\n        optimizer='adam',\n        loss=tf.losses.CategoricalCrossentropy(),\n        metrics=METRICS\n    )","660c5ae0":"def exponential_decay(lr0, s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1 **(epoch \/ s)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(0.01, 20)\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"alzheimer_model.h5\",\n                                                    save_best_only=True)\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n                                                     restore_best_weights=True)","88b6830f":"history = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler],\n    epochs=EPOCHS\n)","1b5b496f":"fig, ax = plt.subplots(1, 2, figsize=(20, 3))\nax = ax.ravel()\n\nfor i, met in enumerate(['auc', 'loss']):\n    ax[i].plot(history.history[met])\n    ax[i].plot(history.history['val_' + met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs')\n    ax[i].set_ylabel(met)\n    ax[i].legend(['train', 'val'])","78099623":"test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    \"..\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/test\",\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n)\n\ntest_ds = test_ds.map(one_hot_label, num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)","261ec33b":"_ = model.evaluate(test_ds)","a600594c":"# Build the ML Model\n\nWe'll be using the same architecture for our model as my [Pneumonia Classification NB](https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays#4.-Build-the-CNN). Using `tf.keras`, we can easily build up the layers of our CNN.","041bdc6a":"# Deciding a Metric\n\nThe most conventional metric to use is probably accuracy. Accuracy, however, cannot be used for imbalanced datasets. Let's check how many images are in each class for our training data.","8e8e2da1":"The following cell makes calling images from our dataset more efficient.","1aafde8f":"# Visualize Model Metrics\n\nLet's graph the ROC AUC metric and loss after each epoch for the training and validation data. Although we didn't use a random seed for our notebook, the results may slightly vary, generally the scores for the validataion data is similar, if not better, than the training dataset.","c0e4f251":"# Feature Engineering\n\nBecause we are working with categorical and noncontinuous data, we want to convert our model into one-hot encodings. One-hot encodings are a way for the model to understand that we're looking at categorial instead of continuous data. Transforming features so that they'll be more understandable is called feature engineering. Learn more about feature engineering [here](https:\/\/developers.google.com\/machine-learning\/crash-course\/representation\/feature-engineering).","5c8673f1":"# Data Loading\n\nWe'll be using a [Kaggle Alzheimer's dataset](https:\/\/www.kaggle.com\/tourist55\/alzheimers-dataset-4-class-of-images) for our tutorial. `tf.keras` has a new preprocessing function that can easily load in images for a directory. In order for this function to work, the data has to be structured in a file directory format.\n\n```\nmain_directory\/\n    class1\/\n        class1_images\n    class2\/\n        class2_images\n```\n\nIf you input the `main_directory` into the `tf.keras` function, it will figure out the rest!\nIn our case, the `train` directory is our main directory.\n\nWe are also specifying a 80:20 split for our training and validation datasets. To learn more about the importance of having a validation split, check out this [lesson](https:\/\/developers.google.com\/machine-learning\/crash-course\/validation\/another-partition) from Google's Machine Learning Crash Course.","ef14405b":"It's always a good idea to set constant variables instead of hard coding numbers into your code. It saves time later when you want to change certain parameters.","9fa21a52":"# Evaluate the Model\n\nAlthough we used the validatation dataset to continually evaluate the model, we also have a separate testing dataset. Let's prepare the testing dataset.","3d5f373b":"We'll be renaming the class names and specifying the number of classes. In this case, we have 4 classes of dementia.","f9c2e1a3":"Let's fit our model!","504f2bb0":"Our dataset is not balanced, so we cannot use accuracy as our metric. For this tutorial, we will be using ROC AUC. Intuitively, ROC AUC gives a score, with higher scores closer to 1 indicating that the different classes can be distinguishable for the model. A lower score closer indicates that the the model cannot distinguish between different classes. A score of 0.5 indicates that the ordering the images is pretty much random. Learn more about ROC AUC [here](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc).","64eb13ae":"# Visualize the data\n\nNow that our data has been easily loaded in, the next step is to visualize our images. This helps us understand what is being used as an input for our model. It also serves as a check to see if our images have been loaded in correctly.","1f6662a8":"# Training the Model\n\nTo more efficiently train our model. We will be using callbacks to adjust our learning rate and to stop our model once it converges.\n\nThe [learning rate](https:\/\/developers.google.com\/machine-learning\/glossary#learning-rate) is a very important hyperparameter in the model. Having a LR that is too high will prevent the model from converging. Having a LR that is too slow will make the process too long. Stopping our model early is one mechanism that prevents overfitting.","8fd44573":"# Introduction + Set-up\n\nMachine learning has a phenomenal range of application in the health sciences. This tutorial will go over the complete pipeline to build a model that can determine the dementia level of an Alzheimer's patient from their MRI image. This model achieves an a high ROC AUC score.\n\nThis tutorial highlights the ease of building a CNN using `tf.keras`. Additionally, TensorFlow 2.3 has new features, including easy data loading utilities that were previously not available in TensorFlow 2.2. We'll be seeing how easy data loading is with these additional features.\n\nWe'll be using a GPU accelerator for this NB."}}