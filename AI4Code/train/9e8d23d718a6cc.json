{"cell_type":{"853faf31":"code","384dd2e6":"code","cbb54465":"code","111a1d24":"code","f7f1b25b":"code","10acbb9c":"code","d0fdfcc2":"code","2722d3d5":"code","fc9551d1":"code","205a129b":"code","a715da50":"code","19a95c0e":"code","a3f82917":"code","81e11ab6":"code","6e597b7f":"code","fccb12cf":"code","7571fb3b":"code","20876a0d":"code","7baf4cf5":"code","a303abbd":"code","9d9f40a1":"code","747ee90a":"code","07f866b5":"code","51e5ed1a":"code","1c285298":"code","bd954fb3":"code","11e73bfd":"code","b819baaa":"code","69596f94":"code","f66b9aa7":"code","92b98117":"code","3ff30632":"code","950f3df3":"code","9351b795":"code","5f28085c":"code","00ddbdcb":"code","a3f0b2c2":"code","13cafc59":"code","c83b190c":"code","a5363837":"code","acc3fc4e":"code","4a0ae231":"code","00a13ee2":"code","9d3f0123":"code","d02f3815":"code","dea2e216":"code","299938a1":"code","dc3c3a87":"code","67a38e52":"code","3580a0cc":"code","8bcf9d71":"code","40e65b3d":"code","edd3aea6":"code","708fd81f":"code","b97c7900":"code","6270858b":"code","1e8ce443":"code","7ab1509b":"code","90c0d829":"code","16dad079":"code","10700a9e":"code","b84a58b1":"markdown","9aadb53e":"markdown","6c382ea3":"markdown","b8d9d0f7":"markdown","39b25f14":"markdown","c7ce4f76":"markdown","cb34dc42":"markdown","0cde6047":"markdown","2a3989cf":"markdown","9a060b5c":"markdown","1d59308f":"markdown","98ab14ea":"markdown","d87c1a16":"markdown","3abfbdd9":"markdown","7f04e8ed":"markdown","21be4800":"markdown","8c6280f3":"markdown","2399fbee":"markdown","e0f42267":"markdown","8226f957":"markdown","21a4049f":"markdown","e10771ac":"markdown","daa9fc43":"markdown","04cf5e8f":"markdown","e12947a0":"markdown","9bbd4172":"markdown","b4eb68a6":"markdown","af27e1e8":"markdown","4df91eee":"markdown","1f2df2e5":"markdown","f0daa173":"markdown","57459145":"markdown","bf0f5acd":"markdown","bc24865a":"markdown","15ca4fae":"markdown","624d9c99":"markdown","88e5a8a5":"markdown","f7b993ad":"markdown","4fb8d705":"markdown","4f65fb66":"markdown","694dc160":"markdown","26d404b1":"markdown","576ac7ff":"markdown","0dcd08c2":"markdown","00cc733c":"markdown","830f4ea5":"markdown","ae26a8a0":"markdown","c3af6948":"markdown","e2e959e9":"markdown","6e9c24ac":"markdown","d4f9ffbf":"markdown","5f170650":"markdown","1bbcb695":"markdown","20d6e245":"markdown","1137e933":"markdown","711ba294":"markdown","b916af8a":"markdown","b6438ccc":"markdown","6a41724e":"markdown","9cfa54b7":"markdown","6de86149":"markdown","eb44e11f":"markdown","9ab8ac05":"markdown","1d3e5d5a":"markdown","ef4319f6":"markdown","fb52968a":"markdown","698f13bb":"markdown","5e1ea351":"markdown","47608ccb":"markdown","f440b55e":"markdown"},"source":{"853faf31":"# Basic\nimport numpy as np \nimport pandas as pd \n\n# Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Scaling\nfrom sklearn.preprocessing import StandardScaler\n\n# train test split\nfrom sklearn.model_selection import train_test_split\n\n# Making Polynomial Features\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Importing models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Regression Metrics\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n# To build optimal model using Backward Elimination\nimport statsmodels.api as sm\n\n# Cross validation\nfrom sklearn.model_selection import cross_val_score","384dd2e6":"dataset = pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv', index_col = False)\n\ndataset.head()","cbb54465":"dataset.info()","111a1d24":"dataset.describe()","f7f1b25b":"dataset.isna().sum()","10acbb9c":"dataset.dropna(axis=0, inplace=True)\nprint(dataset.shape)","d0fdfcc2":"dataset.drop(columns = ['status'], axis=1, inplace=True)\ndataset.head(2)","2722d3d5":"sns.distplot(a = dataset['salary'])\nplt.title('Salary Distribution')\nplt.xlabel('Salary')\nplt.grid(b=True, which='major', color='#666666', linestyle='-')\nplt.minorticks_on()\nplt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\nplt.show()","fc9551d1":"ax = sns.violinplot(x = 'gender', y = 'salary', data = dataset)\n\nmedians = dataset.groupby(['gender'])['salary'].median().values\nnobs = dataset['gender'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n = ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Gender vs Salary')\nplt.grid(b=True, which='major', axis='both', color='#666666', linestyle='-')\nplt.minorticks_on()\nplt.grid(b=True, which='minor', axis='y', color='#999999', linestyle='-', alpha=0.2)\nplt.show()","205a129b":"plt.figure(figsize=(8,6))\nsns.regplot(x='ssc_p', y='salary', data = dataset)\nplt.minorticks_on()\nplt.grid(b=True, which='both', axis='both', alpha=0.1)\nplt.title('Salary vs SSC Percentage')\nplt.show()","a715da50":"ax = sns.violinplot(x = 'ssc_b', y = 'salary', data = dataset)\n\nmedians = dataset.groupby(['ssc_b'])['salary'].median().values\nnobs = dataset['ssc_b'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n = ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Salary vs SSC Board')\nplt.grid(b=True, which='major', axis='both', color='#666666', linestyle='-')\nplt.minorticks_on()\nplt.grid(b=True, which='minor', axis='y', color='#999999', linestyle='-', alpha=0.2)\nplt.show()","19a95c0e":"plt.figure(figsize=(8,6))\nsns.regplot(x='hsc_p', y='salary', data = dataset)\nplt.minorticks_on()\nplt.grid(b=True, which='both', axis='both', alpha=0.1)\nplt.title('Salary vs HSC Percentage')\nplt.show()","a3f82917":"ax = sns.violinplot(x = 'hsc_b', y = 'salary', data = dataset)\n\nmedians = dataset.groupby(['hsc_b'])['salary'].median().values\nnobs = dataset['hsc_b'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n = ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Salary vs HSC Board')\nplt.grid(b=True, which='major', axis='both', color='#666666', linestyle='-')\nplt.minorticks_on()\nplt.grid(b=True, which='minor', axis='y', color='#999999', linestyle='-', alpha=0.2)\nplt.show()","81e11ab6":"plt.figure(figsize=(10,8))\nax = sns.violinplot(x = 'hsc_s', y = 'salary', data = dataset)\n\nmedians = dataset.groupby(['hsc_s'])['salary'].median().values\nnobs = dataset['hsc_s'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n = ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Salary vs HSC Subjects')\nplt.show()","6e597b7f":"plt.figure(figsize=(8,6))\nsns.regplot(x='degree_p', y='salary', data = dataset)\nplt.minorticks_on()\nplt.grid(b=True, which='both', axis='both', alpha=0.1)\nplt.title('Salary vs Degree Percentage')\nplt.show()","fccb12cf":"plt.figure(figsize=(10,8))\nax = sns.violinplot(x = 'degree_t', y = 'salary', data = dataset)\n\nmedians = dataset.groupby(['degree_t'])['salary'].median().values\nnobs = dataset['degree_t'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n = ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Salary vs Graduate Specialisation')\nplt.show()","7571fb3b":"plt.figure(figsize=(10,8))\nax = sns.violinplot(x = 'degree_t', y = 'salary', data = dataset, hue='gender')\nplt.title('Salary vs Graduate Specialisation and gender')\nplt.show()","20876a0d":"plt.figure(figsize=(10,8))\nax = sns.violinplot(x = 'workex', y = 'salary', data = dataset)\n\nmedians = dataset.groupby(['workex'])['salary'].median().values\nnobs = dataset['workex'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n = ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Salary vs Work Experience')\nplt.show()","7baf4cf5":"plt.figure(figsize=(10,8))\nax = sns.violinplot(x = 'workex', y = 'salary', data = dataset, hue='gender')\nplt.title('Salary vs Work Experience and Gender')\nplt.show()","a303abbd":"plt.figure(figsize=(8,6))\nsns.regplot(x='etest_p', y='salary', data = dataset)\nplt.minorticks_on()\nplt.grid(b=True, which='both', axis='both', alpha=0.1)\nplt.title('Salary vs Employability Test Percentage')\nplt.show()","9d9f40a1":"plt.figure(figsize=(10,8))\nax = sns.violinplot(x = 'specialisation', y = 'salary', data = dataset)\n\nmedians = dataset.groupby(['specialisation'])['salary'].median().values\nnobs = dataset['specialisation'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n = ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Salary vs specialisation in MBA')\nplt.show()","747ee90a":"plt.figure(figsize=(10,8))\nax = sns.violinplot(x = 'specialisation', y = 'salary', data = dataset, hue='gender')\nplt.title('Salary vs specialisation in MBA and gender')\nplt.show()","07f866b5":"plt.figure(figsize=(8,6))\nsns.regplot(x='mba_p', y='salary', data = dataset)\nplt.minorticks_on()\nplt.grid(b=True, which='both', axis='both', alpha=0.1)\nplt.title('Salary vs MBA Percentage')\nplt.show()","51e5ed1a":"dataset.head(1)","1c285298":"# dropping first column\n\ndataset.drop(columns=['sl_no'], axis=1, inplace=True)\ndataset.head(1)","bd954fb3":"# Gender: F coded as 0 and M as 1\ndummy = pd.get_dummies(dataset['gender'])\ndummy.rename(columns={'M':'Gender'}, inplace=True)\n\n# drop original column \ndataset.drop(\"gender\", axis = 1, inplace=True)\n\n# merge data frame \"dataset\" and \"dummy_variable_1: Gender column\" \ndf = pd.concat([dummy['Gender'], dataset], axis=1)\n\ndf.head(1)","11e73bfd":"# ssc_b: Central as 1 and Others as 0\ndummy = pd.get_dummies(dataset['ssc_b'])\ndummy.rename(columns={'Central':'ssc_b'}, inplace=True)\n\ndf.drop(\"ssc_b\", axis = 1, inplace=True)\n\n# merge data\ndf = pd.concat([df.iloc[:, 0:2], dummy['ssc_b'], df.iloc[:, 2:]], axis=1)\n\ndf.head(1)","b819baaa":"# hsc_b: Central as 1 and Others as 0\ndummy = pd.get_dummies(dataset['hsc_b'])\ndummy.rename(columns={'Central':'hsc_b'}, inplace=True)\n\ndf.drop(\"hsc_b\", axis = 1, inplace=True)\n\n# merge data\ndf = pd.concat([df.iloc[:, 0:4], dummy['hsc_b'], df.iloc[:, 4:]], axis=1)\n\ndf.head(1)","69596f94":"# Higher Secondary Specialisation: Science: 10 and Commerce: 01 and Arts: 00\ndummy = pd.get_dummies(df['hsc_s'])\ndummy.rename(columns={'Science': 'HS_Sci', 'Commerce': 'HS_Comm'}, inplace=True)\ndummy = pd.concat([dummy['HS_Sci'], dummy['HS_Comm']], axis=1)\ndummy.head()\n\n# drop original\ndf.drop('hsc_s', axis=1, inplace=True)\n\n# merge data\ndf = pd.concat([df.iloc[:, 0:5], dummy, df.iloc[:, 5:]], axis=1)\n\ndf.head(1)","f66b9aa7":"# Undergrad specialisation: Sci&Tech: 10 and Comm&Mgmt: 01 and Others: 00\ndummy = pd.get_dummies(df['degree_t'])\ndummy.rename(columns={'Sci&Tech': 'UG_Sci', 'Comm&Mgmt': 'UG_Comm'}, inplace=True)\ndummy = pd.concat([dummy['UG_Sci'], dummy['UG_Comm']], axis=1)\ndummy.head()\n\n# drop original\ndf.drop('degree_t', axis=1, inplace=True)\n\n# merge data\ndf = pd.concat([df.iloc[:, 0:8], dummy, df.iloc[:, 8:]], axis=1)\n\ndf.head(1)","92b98117":"# Work experience: Yes as 1 nd No as 0\ndummy = pd.get_dummies(df['workex'])\ndummy.rename(columns={'Yes': 'workex'}, inplace=True)\n# dummy.head()\n\n# drop original\ndf.drop('workex', axis=1, inplace=True)\n\n# merge data\ndf = pd.concat([df.iloc[:, 0:10], dummy['workex'], df.iloc[:, 10:]], axis=1)\n\ndf.head(1)","3ff30632":"# Specialisation: Mkt&Fin as 1 and Mkt&HR as 0\ndummy = pd.get_dummies(df['specialisation'])\ndummy.rename(columns={'Mkt&Fin': 'specialisation'}, inplace=True)\n# dummy.head()\n\n# drop original data\ndf.drop('specialisation', axis=1, inplace=True)\n\n# merge data\ndf= pd.concat([df.iloc[:, 0:12], dummy['specialisation'], df.iloc[:, 12:]], axis=1)\n\ndf.head(1)","950f3df3":"plt.figure(figsize=(14, 12))\nsns.heatmap(df.corr(), annot=True)\nplt.title('Correlation between all features and salary offered')\nplt.show()","9351b795":"# acquiring data for model\n\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\n\nprint('X_shape {}'.format(X.shape))\nprint('y_shape {}'.format(y.shape))","5f28085c":"# Splitting\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nprint('Shape of training set: {} and test set: {}'.format(X_train.shape, X_test.shape))","00ddbdcb":"# Making regressor\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_test)\n\n# Model performance through metrics\nprint('Train Score: ', regressor.score(X_train, y_train))  \nprint('Test Score: ', regressor.score(X_test, y_test)) \nprint()\nprint('MAE: ', mean_absolute_error(y_test, y_pred))\nprint('MSE: ', mean_squared_error(y_test, y_pred))\nprint('R2 score: ', r2_score(y_test, y_pred))","a3f0b2c2":"# Creating Polynomial Features\npoly_reg = PolynomialFeatures(degree = 3)\nX_train_poly = poly_reg.fit_transform(X_train)\nX_test_poly = poly_reg.fit_transform(X_test)\n\n# Fitt PolyReg to training set\nregressor = LinearRegression()\nregressor.fit(X_train_poly, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_test_poly)\n\n# Model performance through metrics\nprint('Train Score: ', regressor.score(X_train_poly, y_train))  \nprint('Test Score: ', regressor.score(X_test_poly, y_test)) \nprint()\nprint('MAE: ', mean_absolute_error(y_test, y_pred))\nprint('MSE: ', mean_squared_error(y_test, y_pred))\nprint('R2 score: ', r2_score(y_test, y_pred))","13cafc59":"# Applying feature scaling for this\n\nsc = StandardScaler()\nX_train_sc = sc.fit_transform(X_train)\nX_test_sc = sc.fit_transform(X_test)\n\nprint('Scaled Successfully')","c83b190c":"regressor = SVR(kernel='rbf')\nregressor.fit(X_train_sc, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_test_sc)\n\n# Model performance through metrics\nprint('Train Score: ', regressor.score(X_train_sc, y_train))  \nprint('Test Score: ', regressor.score(X_test_sc, y_test)) \nprint()\nprint('MAE: ', mean_absolute_error(y_test, y_pred))\nprint('MSE: ', mean_squared_error(y_test, y_pred))\nprint('R2 score: ', r2_score(y_test, y_pred))","a5363837":"regressor = DecisionTreeRegressor()\nregressor.fit(X_train, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_test)\n\n# Model performance through metrics\nprint('Train Score: ', regressor.score(X_train, y_train))  \nprint('Test Score: ', regressor.score(X_test, y_test)) \nprint()\nprint('MAE: ', mean_absolute_error(y_test, y_pred))\nprint('MSE: ', mean_squared_error(y_test, y_pred))\nprint('R2 score: ', r2_score(y_test, y_pred))","acc3fc4e":"regressor = RandomForestRegressor(n_estimators = 10)\nregressor.fit(X_train, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_test)\n\n# Model performance through metrics\nprint('Train Score: ', regressor.score(X_train, y_train))  \nprint('Test Score: ', regressor.score(X_test, y_test)) \nprint()\nprint('MAE: ', mean_absolute_error(y_test, y_pred))\nprint('MSE: ', mean_squared_error(y_test, y_pred))\nprint('R2 score: ', r2_score(y_test, y_pred))","4a0ae231":"# x_0 has to be given here explicitly because this package does not take in the b_0 constant otherwise.\nX_new = df.iloc[:, :-1].values\nX_new = np.append(arr = np.ones((148,1)).astype(int), values = X_new, axis = 1)\n\nprint(X_new.shape)","00a13ee2":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_new[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13, 14]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","9d3f0123":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,2,5,6,7,8,9,10,11,12,13,14]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","d02f3815":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,2,3,4,5,6,8,9,10,11,12]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","dea2e216":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,2,4,5,6,7,8,9,10,11]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","299938a1":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,2,3,4,5,6,8,9,10]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","dc3c3a87":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,3,4,5,6,7,8,9]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","67a38e52":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,2,3,4,5,7,8]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","3580a0cc":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [1,2,3,4,5,6,7]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","8bcf9d71":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,2,4,5,6]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","40e65b3d":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,2,3,4,5]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","edd3aea6":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,2,4]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","708fd81f":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,2,3]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","b97c7900":"# S0. Create a new set of features that will be our optimal set of features\nX_opt_final = X_opt[:, [0,2]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt_final).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","6270858b":"print('Shape of optimal values or X: ', X_opt.shape)\n\n# Let's visualise the first 3 values to see, which of the features have we selected\nX_opt[0:5,:]","1e8ce443":"df.head(5)","7ab1509b":"X_final = df.iloc[:, [0,9,13]].values\ny = df.iloc[:, 14].values\n\nprint('X_shape {}'.format(X_final.shape))\nprint('y_shape {}'.format(y.shape))","90c0d829":"# Splitting\nX_final_train, X_final_test, y_train, y_test = train_test_split(X_final, y, test_size=0.3, random_state=0)\n\nprint('Shape of training set: {} and test set: {}'.format(X_final_train.shape, X_final_test.shape))","16dad079":"# Making regressor\nregressor = LinearRegression()\nregressor.fit(X_final_train, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_final_test)\n\n# Model performance through metrics\nprint('Train Score: ', regressor.score(X_final_train, y_train))  \nprint('Test Score: ', regressor.score(X_final_test, y_test)) \nprint()\nprint('MAE: ', mean_absolute_error(y_test, y_pred))\nprint('MSE: ', mean_squared_error(y_test, y_pred))\nprint('R2 score: ', r2_score(y_test, y_pred))","10700a9e":"# cross validation\nreg_score = cross_val_score(regressor, X_final_train, y_train, cv=10)\n\nprint('Cross Validation Scores across all 10 iterations: ', reg_score)\nprint('Multiple Linear Regression: ', np.mean(reg_score))","b84a58b1":"### 1. Gender","9aadb53e":"On comparing the original dataset with the optimal table of X, we can see that the features that best give the salary prediction are:\n\n1. Gender\n2. UnderGraduate Degree (Comm\/Sci or Others)\n3. Percentage in MBA","6c382ea3":"Do comment and let me know any ideas that I missed out on or if I could better my thought process regarding this in any way.\n\nThank you!","b8d9d0f7":"OBSERVATION:\n\n1. The median lies around the same value for both the boards.\n2. The range of salaries offered to students of the Central board is more than the range offered to the students of Other boards. \n\nWe can't particularly say if it is a good indicator for the salary. We will check the correlation later and also in the model refinement process.","39b25f14":"### 9. workex (Work Experience)","c7ce4f76":"### 7. degree_p (Graduation Degree Percentage)","cb34dc42":"Steps we need to take:\n1. Remove the first column of serial numbers.\n2. We need to change all the categorical variables into hot encoded values.\n3. Drop the original categorical variable columns.","0cde6047":"### 11. specialisation (in MBA)","2a3989cf":"## Model Construction","9a060b5c":"### 2. Polynomial Regression","1d59308f":"OBSERVATION:\n\n1. The median of salary offered to students who pursued Mkt&Fin is slighty more than the salary offered to students who pursued Mkt&HR in MBA\n2. The range of salary offered to students who pursued Mkt&Fin is mugh larger than the salary offered to students who pursued Mkt&HR in MBA.\n3. There could be some correlation. \n4. The females who studied in either of the fields were offered a smaller range of salaries than the males.","98ab14ea":"### 10. etest_p (Employability Test Percentage)","d87c1a16":"**Correlation between all variables**","3abfbdd9":"### Iteration 6","7f04e8ed":"OBSERVATION:\n\nA less significant positive correlation can be visualised between the salary offered and the employability test percentage.","21be4800":"**We can see that the salary for 67 students are not given as they were not placed.**","8c6280f3":"### Further Work","2399fbee":"### Iteration 2","e0f42267":"OBSERVATION:\n\n1. The median salary is slightly higher for males than females. \n2. The distribution is also skewed in terms of females getting placed more than males.\n3. The range of salaries being offered to males is much higher than the reange of salaries being offered to females.\n4. Maximum salary offered to a male is 10,00,000 and that to a female is 7,00,000.","8226f957":"## Final Model (Multiple Linear Regression)","21a4049f":"### 2. ssc_p (Senior Secondary Percentage)","e10771ac":"### 3. Support Vector Regression","daa9fc43":"### Iteration 11","04cf5e8f":"Further work would include, \n1. Working on Random Forest model to see if that model gives better predictions or not.\n2. Finding a better comination of variables that give us better salary predictions with RF model.","e12947a0":"**SCORES**\n\n**R-squared value --> 0.913**\n\n**Adjusted R-squared value --> 0.911**\n\nThis score shows that the model **FALLS** in performance when we **drop the x2 feature from X_opt.**","9bbd4172":"## Backward Elimination for Multiple LR","b4eb68a6":"**SCORES**\n\n**R-squared value --> 0.914**\n\n**Adjusted R-squared value --> 0.912**\n\nThis score shows that the model performs better with the three features in X_opt. \n\nHowever, \n\nwe do see that the p value for feature **x2** has exceeded the value we set for our p value. We will chuck that feature and see if our model performs better.","af27e1e8":"## EDA (wrt. to Salary)","4df91eee":"### Iteration 10","1f2df2e5":"We can see that we now have (215 - 67 = ) 148 entries. We have successfully removed the null salary entries. \n\nWe can also remove the 'status' column as we are predicting salaries of the students assuming they were already placed. So, that column would be 'Placed' for every student.","f0daa173":"The model here performs quite poorly even after choosing out the best features. That makes me wonder:\n\n1. If the salary offered does really depend on the specifics of the student's biodata, or \n2. Does it depend on the company policies\/existing salaries and posts in the company?\n3. Does it depend in how the interview of the candidate was?\n\nAs visible, **none** of the features showed a very strong negative or positive correlation with the salary offered. This did imply that of the features given, **no particular feature** was a strong predictor for the salaries. ","57459145":"OBSERVATION:\n\n1. The range of salaries offered are much greater for Students who took Commerce, and then Science. Arts students have a very small range of salaries offered.\n2. The correlation needs to be checked to check if this is a good indicator for salary offered.","bf0f5acd":"### 5. hsc_b (Higher Secondary Board)","bc24865a":"### Iteration 3","15ca4fae":"### 4. Decision Tree Regressor","624d9c99":"OBSERVATION:\n\nA less significant correlation can be visualised between the MBA percentage and the salary offered to the student.","88e5a8a5":"### Iteration 12","f7b993ad":"### Note:","4fb8d705":"OBSERVATION:\n\n1. The median of salary offered to students who had some work experience is slightly more than the ones who did not have a work experience.\n2. The range of salaries offered to students who had some work experience is a lot higher than the range offered to the ones who did not have work experience.\n3. Females with or without work experience were offered a range of salary less than the males.","4f65fb66":"### 1. Multiple Linear Regression","694dc160":"### Iteration 4","26d404b1":"OBSERVATION:\n\nThe correlation is either very less or 0 between the percentage of the degree and the salary offered.","576ac7ff":"We will use Backward Elimination to find out the variables right for our multiple Linear Regression Model.\n\nSTEPS:\n1. Select significance level to stay in the model (SL = 0.05)\n2. Fit the full model will all predictors\n3. Consider predictor with highest p-value. If p>SL, go to S4, else FINISH.\n4. Remove the predictor.\n5. Fit model without the predictor. Go back to S3.\n\nFINISH.","0dcd08c2":"### 4. hsc_p (Higher Secondary Percentage)","00cc733c":"OBSERVATION:\n\n1. The range of salaries offered are in the order Comm&Mgmt > Sci&Tech > Others\n2. Correlation between the degree specialisation and the salary offered needs to be checked to check if this is a good indicator or not.\n3. The range of salaries offered to females (Sci&Tech and Comm&Mgmt fields) was very less compared to the range of salaries offred to males.\n4. Females who pursued Other fields were offered a decent range of salaries whereas for males in the same field, there was an absence of the same.","830f4ea5":"### 5. Random Forest Regression\n","ae26a8a0":"### Iteration 7","c3af6948":"## Preprocessing","e2e959e9":"Of all the models tried above:\n\n**Multiple Linear Regression works best**, \n\nfollowed by **Random Forest Regression**, and \n\nthen **SVR**.","6e9c24ac":"### 6. hsc_s (Higher Secondary Subject)","d4f9ffbf":"## Thoughts","5f170650":"### Iteration 13","1bbcb695":"Remove the rows that have data of unplaced students because we want to develop a model that predicts the salary of a placed student.","20d6e245":"### 8. degree_t (Graduation Specialisation)","1137e933":"OBSERVATION:\n\nThe salary distribution is **centered around 250k**. The range of salaries that are mostly given out lie in the region **200k - 400k**. \n\nWe can also see some outliers >400k.","711ba294":"### Iteration 8","b916af8a":"### 3. ssc_b (Senior Secondary Board)","b6438ccc":"### Previous Work: Check out [here](http:\/\/www.kaggle.com\/mani97\/placed-or-not-eda-classification-88-8) for EDA and Classification modelling to predict if a student was placed or not!","6a41724e":"### Salary Distribution","9cfa54b7":"OBSERVATION:\n\nThere seems to be almost 0 correlation between the senior secondary percentage and the salaries offered to the students.","6de86149":"Thus, we will keep the **three features we selected in Iteration 12.**","eb44e11f":"### Iteration 1","9ab8ac05":"### Iteration 9","1d3e5d5a":"### 12. mba_p (Percentage in MBA)","ef4319f6":"### Iteration 5","fb52968a":"OBSERVATION:\n\nThe correlation between salary and HSC percentage can be visualised to be very less. This will not be a very good indicator for the salary offered to the student.","698f13bb":"OBSERVATION:\n\n1. The median lies around the same value for both the boards.\n2. The range of salaries offered to students of the Central board is more than the range offered to the students of Other boards. \n\nWe can't particularly say if it is a good indicator for the salary. We will check the correlation later and also in the model refinement process.","5e1ea351":"**We will try models to see which model suits the data best.**","47608ccb":"We will thus, go forward with the **Multiple Linear Regression** and **Random Forest Regression** and will try to better the model.","f440b55e":"### Cross Validation"}}