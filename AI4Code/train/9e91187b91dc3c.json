{"cell_type":{"762f587c":"code","67d81690":"code","3c1427f7":"code","1a38f256":"code","3bf09870":"code","298a6b2e":"code","5e58476f":"code","2c655b62":"code","a0dcdb85":"code","c51e79ff":"code","9aa039a8":"code","c8fbc04a":"code","2dd56e4f":"code","7e524c70":"code","1798b784":"code","28b20813":"code","a2cb2949":"code","e9b41566":"code","48a40352":"code","02a60000":"code","b5877ba4":"code","10986108":"code","326a5394":"code","30268ac9":"code","e2882ac3":"code","90825898":"code","cd6376ba":"code","6da071ad":"code","3a1059cc":"code","6ef1c6ad":"code","34f09168":"code","516080c3":"markdown","e49eb5bc":"markdown","86cdbc7f":"markdown","c216389c":"markdown","7d5a5fbe":"markdown","72037130":"markdown","76eff5c1":"markdown","7020126b":"markdown","4f2f5ee5":"markdown","a30b5b9f":"markdown","bf962a82":"markdown","fcdd4967":"markdown","13d48d20":"markdown","8381d314":"markdown"},"source":{"762f587c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","67d81690":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline","3c1427f7":"print(tf.__version__)","1a38f256":"tf.compat.v1.disable_v2_behavior()\n","3bf09870":"\ndata = pd.read_csv(\"\/kaggle\/input\/creditcard.csv\")\n\ndata.head()\n\ncount_classes = pd.value_counts(data['Class'], sort = True).sort_index()\ncount_classes.plot(kind = 'bar')\nplt.title(\"Fraud class histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","298a6b2e":"from sklearn.preprocessing import StandardScaler\n\ndata['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n\n# hour = data['Time'].apply(lambda x: np.ceil(float(x)\/3600) % 24)\n# data['hour'] = StandardScaler().fit_transform(hour.reshape(-1, 1))\n\ndata = data.drop(['Time','Amount'],axis=1)\ndata.head()","5e58476f":"\nclass Autoencoder(object):\n\n    def __init__(self, n_hidden_1, n_hidden_2, n_input, learning_rate):\n        self.n_hidden_1 = n_hidden_1\n        self.n_hidden_2 = n_hidden_2\n        self.n_input = n_input\n\n        self.learning_rate = learning_rate\n\n        self.weights, self.biases = self._initialize_weights()\n\n        self.x = tf.compat.v1.placeholder(\"float\", [None, self.n_input])\n\n        self.encoder_op = self.encoder(self.x)\n        self.decoder_op = self.decoder(self.encoder_op)\n\n        self.cost = tf.reduce_mean(tf.pow(self.x - self.decoder_op, 2))\n        self.optimizer = tf.compat.v1.train.RMSPropOptimizer(self.learning_rate).minimize(self.cost)\n\n        init = tf.compat.v1.initialize_all_variables()\n        self.sess = tf.compat.v1.Session()\n        self.sess.run(init)\n\n    def _initialize_weights(self):\n        weights = {\n            'encoder_h1': tf.Variable(tf.random.normal([self.n_input, self.n_hidden_1])),\n            'encoder_h2': tf.Variable(tf.random.normal([self.n_hidden_1, self.n_hidden_2])),\n            'decoder_h1': tf.Variable(tf.random.normal([self.n_hidden_2, self.n_hidden_1])),\n            'decoder_h2': tf.Variable(tf.random.normal([self.n_hidden_1, self.n_input])),\n        }\n        biases = {\n            'encoder_b1': tf.Variable(tf.random.normal([self.n_hidden_1])),\n            'encoder_b2': tf.Variable(tf.random.normal([self.n_hidden_2])),\n            'decoder_b1': tf.Variable(tf.random.normal([self.n_hidden_1])),\n            'decoder_b2': tf.Variable(tf.random.normal([self.n_input])),\n        }\n\n        return weights, biases\n\n    def encoder(self, X):\n        layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(X, self.weights['encoder_h1']),\n                                       self.biases['encoder_b1']))\n        layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, self.weights['encoder_h2']),\n                                       self.biases['encoder_b2']))\n        return layer_2\n\n    def decoder(self, X):\n        layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(X, self.weights['decoder_h1']),\n                                       self.biases['decoder_b1']))\n        layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, self.weights['decoder_h2']),\n                                       self.biases['decoder_b2']))\n        return layer_2\n\n    def calc_total_cost(self, X):\n        return self.sess.run(self.cost, feed_dict={self.x: X})\n\n    def partial_fit(self, X):\n        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict={self.x: X})\n        return cost\n\n    def transform(self, X):\n        return self.sess.run(self.encoder_op, feed_dict={self.x: X})\n\n    def reconstruct(self, X):\n        return self.sess.run(self.decoder_op, feed_dict={self.x: X})","2c655b62":"from sklearn.model_selection import train_test_split\n","a0dcdb85":"good_data = data[data['Class'] == 0]\nbad_data = data[data['Class'] == 1]\n#print 'bad: {}, good: {}'.format(len(bad_data), len(good_data))","c51e79ff":"X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)\n\nX_train = X_train[X_train['Class']==0]\nX_train = X_train.drop(['Class'], axis=1)\n\ny_test = X_test['Class']\nX_test = X_test.drop(['Class'], axis=1)\n\nX_train = X_train.values\nX_test = X_test.values","9aa039a8":"X_good = good_data.iloc[:, good_data.columns != 'Class']\ny_good = good_data.iloc[:, good_data.columns == 'Class']\n\nX_bad = bad_data.iloc[:, bad_data.columns != 'Class']\ny_bad = bad_data.iloc[:, bad_data.columns == 'Class']","c8fbc04a":"model = Autoencoder(n_hidden_1=15, n_hidden_2=3, n_input=X_train.shape[1], learning_rate = 0.01)","2dd56e4f":"training_epochs = 100\nbatch_size = 256\ndisplay_step = 100\nrecord_step = 10","7e524c70":"total_batch = int(X_train.shape[0]\/batch_size)\n\ncost_summary = []\n\nfor epoch in range(training_epochs):\n    cost = None\n    for i in range(total_batch):\n        batch_start = i * batch_size\n        batch_end = (i + 1) * batch_size\n        batch = X_train[batch_start:batch_end, :]\n        \n        cost = model.partial_fit(batch)\n    \n    if epoch % display_step == 0 or epoch % record_step == 0:\n        total_cost = model.calc_total_cost(X_train)\n        \n        if epoch % record_step == 0:\n            cost_summary.append({'epoch': epoch+1, 'cost': total_cost})\n        \n        if epoch % display_step == 0:\n            print(\"Epoch:{}, cost={:.9f}\".format(epoch+1, total_cost))","1798b784":"f, ax1 = plt.subplots(1, 1, figsize=(10,4))\n\nax1.plot(list(map(lambda x: x['epoch'], cost_summary)), list(map(lambda x: x['cost'], cost_summary)))\nax1.set_title('Cost')\n\nplt.xlabel('Epochs')\nplt.show()","28b20813":"encode_decode = None\ntotal_batch = int(X_test.shape[0]\/batch_size) + 1\nfor i in range(total_batch):\n    batch_start = i * batch_size\n    batch_end = (i + 1) * batch_size\n    batch = X_test[batch_start:batch_end, :]\n    batch_res = model.reconstruct(batch)\n    if encode_decode is None:\n        encode_decode = batch_res\n    else:\n        encode_decode = np.vstack((encode_decode, batch_res))","a2cb2949":"def get_df(orig, ed, _y):\n    rmse = np.mean(np.power(orig - ed, 2), axis=1)\n    return pd.DataFrame({'rmse': rmse, 'target': _y})","e9b41566":"df = get_df(X_test, encode_decode, y_test)","48a40352":"df.describe()","02a60000":"fig = plt.figure(figsize=(10,4))\nax = fig.add_subplot(111)\n_ = ax.hist(df[df['target']== 0].rmse.values, bins=20)","b5877ba4":"fig = plt.figure(figsize=(10,4))\nax = fig.add_subplot(111)\n_ = ax.hist(df[(df['target']== 0) & (df['rmse'] < 10)].rmse.values, bins=20)","10986108":"fig = plt.figure(figsize=(10,4))\nax = fig.add_subplot(111)\n_ = ax.hist(df[df['target'] > 0].rmse.values, bins=20)","326a5394":"fig = plt.figure(figsize=(10,4))\nax = fig.add_subplot(111)\n_ = ax.hist(df[(df['target'] > 0) & (df['rmse'] < 10)].rmse.values, bins=20)","30268ac9":"from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc, roc_auc_score, \n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support) ","e2882ac3":"import itertools\n\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    else:\n        1\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","90825898":"fpr, tpr, thresholds = roc_curve(df.target, df.rmse)\nroc_auc = auc(fpr,tpr)\n\n# Plot ROC\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.4f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","cd6376ba":"precision, recall, th = precision_recall_curve(df.target, df.rmse)\nplt.plot(recall, precision, 'b', label='Precision-Recall curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.show()","6da071ad":"plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\nplt.xlabel('Threshold')\nplt.ylabel('Precision')\nplt.show()","3a1059cc":"# Compute confusion marix\ny_pred = [1 if p > 2 else 0 for p in df.rmse.values]\ncnf_matrix = confusion_matrix(df.target, y_pred)\nnp.set_printoptions(precision=2)\n\nprint(\"Recall metric in the testing dataset: \", float(cnf_matrix[1,1])\/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix')\nplt.show()","6ef1c6ad":"f1_score(y_pred=y_pred, y_true=df.target)","34f09168":"precision_recall_fscore_support(y_pred=y_pred, y_true=df.target)","516080c3":"## Model","e49eb5bc":"ROC AUC seems not bad. But we know that it is highly imbalanced dataset. So let's see precision-recall.","86cdbc7f":"### Precision-Recall","c216389c":"### ROC AUC","7d5a5fbe":"Now its clear that our classifier is not so good.\nROC curve is not a good visual illustration for highly imbalanced data.\nBecause the False Positive Rate ( False Positives \/ Total Real Negatives ) does not drop drastically when the Total Real Negatives is huge. \nWhereas Precision ( True Positives \/ (True Positives + False Positives) ) is highly sensitive to False Positives and is not impacted by a large total real negative denominator.\n\nThis autoencoder classifier can obtain good recall but with very low precision. ","72037130":"# Autoencoder","76eff5c1":"## Train test split","7020126b":"### Confusion matrix","4f2f5ee5":"## Reconstruction error without fraud","a30b5b9f":"## Test","bf962a82":"## Train","fcdd4967":"This is folked from https:\/\/www.kaggle.com\/pgladkov\/fraud-detection-using-autoencoder.\n\nDue to TensorFlow version issue, many of functions has been migrated. So I made some changes to the notebook.","13d48d20":"## Metrics","8381d314":"## Reconstruction error with fraud"}}