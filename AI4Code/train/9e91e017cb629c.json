{"cell_type":{"ac047387":"code","a1b06f31":"code","a02ef774":"code","6543fcbd":"code","d733bc29":"code","60c3308e":"code","2ca29276":"code","02d97093":"code","5a7d6397":"code","6cf71bed":"code","f2f6bc70":"code","22af534c":"code","7eccebbe":"code","5d82bb9c":"code","ed2c8744":"code","860f5053":"code","1c89b2e2":"code","2d1b6c40":"code","bc36d566":"code","23d6f7df":"code","5f6b6d93":"code","b0164e03":"code","b7ac6b55":"code","28ce4562":"code","d1b6b576":"code","15d5d800":"code","caa766d3":"code","99d49a04":"code","7241135d":"code","f91c4784":"code","9bb8049a":"code","7c8892c5":"code","faf8c9b6":"code","ff9924ee":"code","aa697941":"code","528656d8":"code","ed864ce0":"code","05fbd70e":"code","71db2363":"code","336276e5":"code","ba61d624":"code","de3c7535":"code","172ebb00":"code","cc66b356":"code","c4653d51":"code","07d11e05":"code","6ef9ec47":"code","56ce120e":"code","458d4349":"code","1575a0a6":"code","19287dc3":"code","c127ea81":"code","0729445d":"code","b0c190ee":"code","5963e7f2":"code","f1f95647":"code","8e5b3f94":"code","019f5862":"code","649dd781":"code","876090b8":"code","ff0d3f32":"code","40cb2c56":"code","6bd2489b":"code","4f02d5f1":"code","9012035d":"code","bff83f64":"code","d000b9cb":"code","ec4eaa1c":"code","fe69d761":"code","7935a4bd":"code","5c9201ca":"code","ed6b05bb":"code","673887b9":"code","762c0998":"code","8e8a2cbf":"code","359164ff":"code","73bb4048":"code","2acf8e25":"code","78946e3a":"code","f301899b":"markdown","2ce20f27":"markdown","58249257":"markdown","8a6d6df9":"markdown","32ee37c3":"markdown","d86177d4":"markdown","28acb2f0":"markdown","4e0ae25b":"markdown","40ec7f23":"markdown","f8a82a80":"markdown","3b070922":"markdown","415c3ccf":"markdown","867f1c9d":"markdown","7b52e29f":"markdown","1fdb8f3e":"markdown","cbc30726":"markdown","9b97a45e":"markdown","1d5c93af":"markdown","887c278a":"markdown","681e542a":"markdown","0fb379eb":"markdown","b3d59b6f":"markdown","e5227a41":"markdown"},"source":{"ac047387":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\n%matplotlib inline","a1b06f31":"df = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv', \n                 index_col=['Date']\n                , parse_dates=True)\ndf.head()","a02ef774":"df.info()","6543fcbd":"df.describe()","d733bc29":"df.shape","60c3308e":"df.isna().sum()","2ca29276":"df.dropna(subset=['RainToday', 'RainTomorrow'], inplace=True)","02d97093":"df.shape","5a7d6397":"px.histogram(df , y='Location',\n            title='Location Vs RainyDays',\n            color='RainToday')","6cf71bed":"px.histogram(df , x='Temp3pm',\n            title='Temperature at 3pm vs Rain Tommorow',\n            color='RainTomorrow')","f2f6bc70":"px.histogram(df , x='RainToday',\n            color='RainTomorrow')","22af534c":"px.scatter(df.sample(10000), x='MinTemp',\n          y='MaxTemp',\n          color='RainTomorrow')","7eccebbe":"px.scatter(df.sample(10000) ,\n          x='Temp3pm',\n          y='Humidity3pm',\n          color='RainTomorrow')","5d82bb9c":"px.scatter(df, x='Rainfall',\n          color='RainTomorrow')","ed2c8744":"px.scatter(df , x='Pressure9am', y='Pressure3pm',\n          color='RainTomorrow')","860f5053":"df.head()","1c89b2e2":"#from sklearn.model_selection import train_test_split","2d1b6c40":"#train_val_df ,test_df = train_test_split(df, test_size=0.2 , random_state =1)\n#train_df , val_df = train_test_split(train_val_df, test_size=0.25 , random_state=1)","bc36d566":"#train_df.shape, val_df.shape , test_df.shape","23d6f7df":"sns.countplot(x=df.index.year)","5f6b6d93":"year = df.index.year\nyear<2015","b0164e03":"df[year<2015].shape","b7ac6b55":"year = df.index.year\n\ntrain_df= df[year < 2015]\nval_df = df[year == 2015]\ntest_df = df[year >2015]","28ce4562":"print('train_df.shape: ' , train_df.shape)\nprint('val_df.shape: ' , val_df.shape)\nprint('test_df.shape: ' , test_df.shape)","d1b6b576":"input_cols = train_df.columns[0:-1].to_list()\ntarget_cols='RainTomorrow'\n\ninput_cols","15d5d800":"train_inputs = train_df[input_cols].copy()\ntrain_targets = train_df[target_cols].copy()","caa766d3":"val_inputs = val_df[input_cols].copy()\nval_targets = val_df[target_cols].copy()","99d49a04":"test_inputs = test_df[input_cols].copy()\ntest_targets = test_df[target_cols].copy()","7241135d":"num_cols = train_inputs.select_dtypes(include=np.number).columns #instead of np.number you could also write ['int', 'float']\nnum_cols = num_cols.to_list()\nnum_cols","f91c4784":"cat_cols = train_inputs.select_dtypes('object').columns.tolist()\ncat_cols","9bb8049a":"train_inputs[num_cols].describe()","7c8892c5":"train_inputs[cat_cols].nunique()","faf8c9b6":"from sklearn.impute import SimpleImputer","ff9924ee":"imputer = SimpleImputer(strategy='most_frequent')\nimputer.fit(df[num_cols])\n\ntrain_inputs[num_cols] = imputer.transform(train_inputs[num_cols])\nval_inputs[num_cols] = imputer.transform(val_inputs[num_cols])\ntest_inputs[num_cols] = imputer.transform(test_inputs[num_cols])","aa697941":"train_inputs[num_cols].isna().sum()","528656d8":"from sklearn.preprocessing import MinMaxScaler","ed864ce0":"imputed_df = df.copy()\nimputed_df[num_cols] = imputer.transform(df[num_cols])","05fbd70e":"scaler = MinMaxScaler(feature_range= (-1 , 1))\nscaler.fit(df[num_cols])\n\ntrain_inputs[num_cols] = scaler.transform(train_inputs[num_cols])\nval_inputs[num_cols] = scaler.transform(val_inputs[num_cols])\ntest_inputs[num_cols] = scaler.transform(test_inputs[num_cols])","71db2363":"train_inputs.describe()","336276e5":"val_inputs.describe()","ba61d624":"?scaler","de3c7535":"scaler.data_min_","172ebb00":"train_inputs","cc66b356":"cat_cols = train_inputs.select_dtypes('object').columns.to_list()\ncat_cols","c4653d51":"df[cat_cols].nunique()","07d11e05":"from sklearn.preprocessing import OneHotEncoder","6ef9ec47":"?OneHotEncoder","56ce120e":"encoder = OneHotEncoder(sparse=False , \n                       handle_unknown='ignore')\nencoder.fit(df[cat_cols].fillna('Unknown'))\n","458d4349":"encoded_cols = list(encoder.get_feature_names(cat_cols))\nencoded_cols","1575a0a6":"train_inputs[encoded_cols] = encoder.transform(train_inputs[cat_cols].fillna('Unknown'))\nval_inputs[encoded_cols] = encoder.transform(val_inputs[cat_cols].fillna('Unknown').fillna('Unknown'))\ntest_inputs[encoded_cols] = encoder.transform(test_inputs[cat_cols].fillna('Unknown'))","19287dc3":"train_inputs.head()","c127ea81":"# !pip install pyarrow --quiet\n\n###saving the data\n## saving inputs\n# train_inputs.to_parquet('train_inputs.parquet')\n# val_inputs.to_parquet('val_inputs.parquet')\n# test_inputs.to_parquet('test_inputs.parquet')\n\n## saving targets\n# pd.DataFrame(train_targets).to_parquet('train_targets.parquet')\n# pd.DataFrame(val_targets).to_parquet('val_targets.parquet')\n# pd.DataFrame(test_targets).to_parquet('test_targets.parquet')\n\n\n### to load the data\n## loading inputs\n# train_inputs = pd.read_parquet('train_inputs.parquet')\n# val_inputs = pd.read_parquet('val_inputs.parquet')\n# test_inputs = pd.read_parquet('test_inputs.parquet')\n##loading targets\n# train_targets = pd.read_parquet('train_targets.parquet')[target_col]\n# val_targets = pd.read_parquet('val_targets.parquet')[target_col]\n# test_targets = pd.read_parquet('test_targets.parquet')[target_col]\n","0729445d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error","b0c190ee":"?LogisticRegression","5963e7f2":"num_cols","f1f95647":"def targets_map(x) :\n    d = {'Yes' : 1 , 'No':0}\n    x= d[x]\n    return x","8e5b3f94":"\ntrain_targets.map(lambda x: targets_map(x))","019f5862":"feature_cols = num_cols + encoded_cols","649dd781":"train_inputs = train_inputs[feature_cols]\nval_inputs  = val_inputs[feature_cols]\ntest_inputs = test_inputs[feature_cols]","876090b8":"model = LogisticRegression(solver='liblinear')\n\nmodel.fit(train_inputs[feature_cols] , train_targets)\n\n#accuracy\nmodel.score(val_inputs[feature_cols] , val_targets)","ff0d3f32":"from sklearn.metrics import confusion_matrix","40cb2c56":"confusion_matrix(val_targets, model.predict(val_inputs[feature_cols]) , normalize='true')","6bd2489b":"def predict_and_plot(inputs, targets, name=''):\n    preds = model.predict(inputs)\n    \n    accuracy = model.score(inputs , targets)\n    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n    \n    cf = confusion_matrix(targets, preds, normalize='true')\n    plt.figure()\n    sns.heatmap(cf, annot=True)\n    plt.xlabel('Prediction')\n    plt.ylabel('Target')\n    plt.title('{} Confusion Matrix'.format(name));\n    \n    return preds","4f02d5f1":"training_preds = predict_and_plot(train_inputs[feature_cols],train_targets,'Training')","9012035d":"val_preds = predict_and_plot(val_inputs[feature_cols],val_targets,'validating')","bff83f64":"test_preds = predict_and_plot(test_inputs[feature_cols],test_targets,'Testing')","d000b9cb":"random_pred = np.full((train_targets.shape),'No')\ndef accuracy(targets , preds) :\n    return (targets==preds).sum()\/targets.size\n\nbase_accuracy =accuracy(train_targets , random_pred)\nprint(base_accuracy)","ec4eaa1c":"from sklearn.svm import SVC\nfrom lightgbm import LGBMClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n","fe69d761":"lr = [0.01 , 0.05 , 0.09 , 0.1 , 0.5 , 0.9]","7935a4bd":"%%time\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nmodels = {\n    'LogisticRegression' : {\n        'model' : LogisticRegression(solver='liblinear' ),\n        'params': {\n            'C': [1,5,10,50]\n        }\n    }}\n\nscores = []\n\nfor model_name, mp in models.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(train_inputs, train_targets)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \nprint(scores)","5c9201ca":"%%time\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nmodels = {\n           'LGBMClassifier': {\n        'model': LGBMClassifier(),\n        'params' : {\n            'boosting_type': ['gbdt' , 'dart', 'goss','rof'],\n            'num_leaves': [10,20],\n            'n_estimators':[100,200,325,500],\n            'learning_rate': lr,\n            'importance_type':['split' , 'gain']\n        }\n    }\n}\nscores_lgbm = []\n\nfor model_name, mp in models.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(train_inputs, train_targets)\n    scores_lgbm.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \nprint(scores_lgbm)","ed6b05bb":"%%time\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nmodels = {\n            'SGDClassifier': {\n       'model' : SGDClassifier(),\n       'params': {\n           'alpha' : lr\n       }\n   }\n}\nscores_sgd = []\n\nfor model_name, mp in models.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(train_inputs, train_targets)\n    scores_sgd.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \nprint(scores_sgd)","673887b9":"%%time\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nmodels = {\n            'KNeighborsClassifier' : {\n        'model' : KNeighborsClassifier(),\n        'params' : dict(leaf_size=list(range(1,20 , 5)), n_neighbors=list(range(1,32 ,10)), p=[1,2])\n    }   \n}\nscores_Kneigh = []\n\nfor model_name, mp in models.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(train_inputs, train_targets)\n    scores_Kneigh.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \nprint(scores_Kneigh)","762c0998":"%%time\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nmodels = {\n        'DecisionTreeClassifier' : {\n        'model' : DecisionTreeClassifier(random_state=1),\n        'params' : {\n            'max_depth': [2, 3, 5, 10, 20],\n            'min_samples_leaf': [5, 10, 20, 50, 100],\n            'criterion': [\"gini\", \"entropy\"]\n        }\n    }   \n}\nscores_tree = []\n\nfor model_name, mp in models.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(train_inputs, train_targets)\n    scores_tree.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \nprint(scores_tree)","8e8a2cbf":"%%time\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nmodels = {\n        'RandomForestClassifier' : {\n        'model': RandomForestClassifier(),\n        'params' : {\n            'bootstrap': [True, False],\n             'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n             'max_features': ['auto', 'sqrt'],\n             'min_samples_leaf': [1, 2, 4],\n             'min_samples_split': [2, 5, 10],\n             'n_estimators': [200, 400, 600]\n        }   \n    }}\nscores_forest = []\n\nfor model_name, mp in models.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(train_inputs, train_targets)\n    scores_forest.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \nprint(scores_forest)","359164ff":"%%time\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nmodels = {\n        'XGBClassifier' : {\n        'model': XGBClassifier() ,\n        'params': {\n            'max_depth': list(range(2, 10, 1)),\n            'n_estimators': [50 , 100 ,200 500],\n            'learning_rate': [0.1, 0.01, 0.05]\n        }    \n    }}\nscores_xgb = []\n\nfor model_name, mp in models.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(train_inputs, train_targets)\n    scores_xgb.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \nprint(scores_xgb)","73bb4048":"%%time\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nmodels = {\n            'GaussianNB' : {\n        'model' : GaussianNB(),\n        'params': {\n            'var_smoothing': np.logspace(0,-9, num=100)\n        }\n    }\n}\nscores_gaussian = []\n\nfor model_name, mp in models.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(train_inputs, train_targets)\n    scores_gaussian.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \nprint(scores_gaussian)","2acf8e25":"# %%time\n# from sklearn.model_selection import GridSearchCV\n# import pandas as pd\n# models = {\n#     'Support Vector Machine' : {\n#         'model' : SVC(gamma='auto'),\n#         'params': {\n#             'C': list(range(1,100 , 50)),\n#             'kernel':['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],   \n#         }\n#     }\n# scores_svm = []\n\n# for model_name, mp in models.items():\n#     clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n#     clf.fit(train_inputs, train_targets)\n#     scores_svm.append({\n#         'model': model_name,\n#         'best_score': clf.best_score_,\n#         'best_params': clf.best_params_\n#     })\n    \n# print(scores2)","78946e3a":"%%time\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nmodels = {\n        'GradientBoostingClassifier' : {\n        'model' : GradientBoostingClassifier(),\n        'params' : {\n        \"loss\":[\"deviance\"],\n        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n        \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n        \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n        \"max_depth\":[3,5,8],\n        \"max_features\":[\"log2\",\"sqrt\"],\n        \"subsample\":[0.5, 0.8, 0.9,1.0],\n        \"n_estimators\":[10]\n        }\n    }}\nscores_gradboost = []\n\nfor model_name, mp in models.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(train_inputs, train_targets)\n    scores_gradboost.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \nprint(scores_gradboost)","f301899b":"# Encoding\nWe'll be using one hot encoding, which basically creates a vector to descibe a categorical columns and adds those columns separately on the dataset.","2ce20f27":"# Saving Preprocessed Data\nThis is something that some people like me like to do, so that I don't have to go through the preprocessing step again in case of data loss for whatever reason.","58249257":"What we've above is accuracy of a base model, which always answers 'No', but it still got the accuracy of 77.7%. So, we can consider this as our base model, such that if our mL model has lower accuracy then this then it probably is useless.\n\nThis is really interesting because, now we know our data targets have a high bias.","8a6d6df9":"It's not possible to do it in the kaggle notebook so I have commented the code for those of you who want to do it in another environment.","32ee37c3":"# Training\/Validation\/Test Sets\nBefore training the model it best to divide the given dataset into training, validation and test sets to be able to compare the trained models with same standards.\n\n(Generally, you can use the 60-20-20 division for the split)","d86177d4":"The Dataset contains over 145,000 rows and total 22 columns(excluded the date column becaouse it is used as an index). It contains both numeric and categorical data(also date).\n\nLet's check the data first and try to find out the total missing values first.","28acb2f0":"# Training A Logistic Regression Model","4e0ae25b":"Since both the validation and test accuracies are above 84% percent, it means our model is genralising well on new data.\n\nBut we see that in 'True Positive' cases it is only about 50% right, which is not good.","40ec7f23":"There are empty values in the **RainToday** and **RainTomorrow** columns, which can directly be droped.\nAlso, Evaporation, Sunshine, Cloud9am , Cloud3am have more than 50k+ null values.","f8a82a80":"# Working With A Sample\n\nWhen working with a big dataset containing millions of rows, it's a good idea to work with only a small portion, say 1-5% of the dataset.","3b070922":"It's not an exact fit to the general division of 60\/20\/20 but, now we can use the past values to predict the weather(rain tomorrow) in the future dates","415c3ccf":"Let's try scaling on both the imputed and non imputed dataset and later we'll see how it affects our model accuracy that is if it does at all.\n","867f1c9d":"Let's also look at this \"accurate model\" with a different look:\n\n# Confusion Matrix","7b52e29f":"# Wow 85% accuracy on first try,Amazing right? Or is it ?\nWe've an accuracy score of 85% on our logistic regression model, which considered quite impressive in most cases, right?\n\nBut not here, because, well I'll just show it to you.","1fdb8f3e":"We've dealt with all the missing values in the numerical columns, so , now let's scale them so that they have small and similar range.\n\n# Scaling Numerical Columns\n**Why Do you think scaling is so important?**  \nAns: Scaling ensures that the features don't have an inproportionate impact on the model weights.","cbc30726":"In most of the columns we see that the range of 75% to the max is very large, so might have to perform data cleaning too.\n\nNow, let's also check the categorical columns.","9b97a45e":"## How to use the documentation right here?","1d5c93af":"# Identify The Numerical\/ Categorical Columns\nThis is crucial step, so that you can decide how to go about next steps like imputing, normalizing the features etc.","887c278a":"# Trying More Models And Finding The Best Fit\nWe're going to try\n- Logistic Regression Classifier (already did)\n- SVM\n- GaussianNB\n- MultinomialNB\n- SGDClassifier\n- KNeighborsClassifier\n- Decision Tree Classifier\n- Random Forest Classifier\n- Gadient Boosting Classifier\n- LGBMClassifier\n- XGBClassifier\n\n\nI am going to implement them in one loop, take a look at the code below and try to understand what's going on, you don't have to know the code exactly because you can just take the code snippet and implement in your own notebook.\n\nI am also finding the best parameters for all the models by using gridsearchcv.","681e542a":"# Working With Categorical Data","0fb379eb":"# Imputing \nThe process of filling missing values is called **imputation**.\n\nWe'll start by using a simple imputation technique -> to replace them with mode of the column, using the SimpleImputer class from sklearn.impute","b3d59b6f":"# Change the targets columns to 1 or 0","e5227a41":"However, since we have a 'Date' column i.e the index in this case , we can ditribute the training\/validation\/test data on the basis of dates, which is a highly recommended method in these kinds of problems"}}