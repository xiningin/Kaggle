{"cell_type":{"07a16419":"code","76559bec":"code","2198ed25":"code","e6a6e821":"code","65328e9d":"code","d02bb565":"code","da61440e":"code","47d11e33":"code","b20edb73":"code","a45eee76":"code","4ee3a191":"code","f8cc5014":"code","7a927f85":"code","3598fbcc":"code","6062b374":"code","b896c381":"code","ff2eddd9":"code","739e93c4":"code","fce1d26b":"markdown","719258e7":"markdown","7e0979f7":"markdown","a8b29426":"markdown","08969259":"markdown","c91b8c42":"markdown","228fe84d":"markdown","46ef6f1e":"markdown"},"source":{"07a16419":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","76559bec":"os.listdir('\/kaggle\/input\/eurosat-dataset\/EuroSAT')","2198ed25":"import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","e6a6e821":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir('\/kaggle\/input\/eurosat-dataset\/EuroSAT\/'))\npath1='\/kaggle\/input\/eurosat-dataset\/EuroSAT\/'","65328e9d":"import matplotlib.pyplot as plt\nfrom matplotlib import image\nfrom PIL import Image\n\ntraindf=pd.read_csv(path1+'train.csv')\ntraindf.shape\n\n            ","d02bb565":"traindf.columns","da61440e":"traindf.drop('Unnamed: 0',axis=1,inplace=True)\ntraindf.head(10)","47d11e33":"traindf.columns","b20edb73":"#load_data(myval.Filename,(32,32))\n\ndef load_data(im_file_names, im_sizes):\n    imgs=[]\n    \n    print(im_file_names)\n    print(im_sizes)\n\n    for fname in im_file_names :\n        img=Image.open(path1+fname)\n#        img=img.convert('RGB')\n        img=img.resize(im_sizes)\n        \n#        imgs=np.array(img)\/255\n        imgs.append(np.array(img)\/255)\n    return np.array(imgs)\n\n\n","a45eee76":"from sklearn.model_selection import train_test_split\n\n\nim_file_names=traindf.Filename\nim_files=load_data(traindf.Filename,(32,32))\n","4ee3a191":"im_files.shape","f8cc5014":"tgt=traindf.Label\nim_files,tgt","7a927f85":"testdf=pd.read_csv(path1+'test.csv')\nprint(testdf.columns)\nprint(testdf.head(10))\ntestdf.drop('Unnamed: 0',axis=1,inplace=True)\nprint(testdf.head(10))\n\ntest_imfiles=load_data(testdf.Filename, (32,32))\ntest_tgt=testdf.Label\n","3598fbcc":"test_imfiles.shape","6062b374":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, AveragePooling2D, Dropout, Flatten, Dense\n\n#from tensorflow.keras.layers import BatchNormalization\n\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=6, kernel_size=(5, 5), activation='tanh', input_shape=(32,32,3)))\nmodel.add(AveragePooling2D())\n\nmodel.add(Conv2D(filters=16, kernel_size=(5, 5), activation='tanh'))\nmodel.add(AveragePooling2D())\n\nmodel.add(Conv2D(filters=128, kernel_size=(5,5),activation='tanh'))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(units=84, activation='tanh'))\n\nmodel.add(Dense(units=10, activation = 'softmax'))\n\nmodel.summary()\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\nmodel.fit(im_files, tgt, epochs=150)","b896c381":"model.evaluate(test_imfiles,test_tgt)","ff2eddd9":"val_data=pd.read_csv(path1+'validation.csv')\nmyval=val_data[300:308]\nmylabels=val_data.Label[300:308]\nmyclasses=val_data.ClassName[300:308]\nval_img_data=load_data(myval.Filename,(32,32))\npred=model.predict_classes(val_img_data)\noriginal_labels=pd.read_json(path1+'label_map.json',orient='index')\nlabels=original_labels.index[pred]","739e93c4":"c=0\nfig = plt.figure(figsize=(10,10))\nfor i in val_img_data:\n    ax = fig.add_subplot(2, 4, c+1)\n    imgplot = plt.imshow(i)\n    ax.set_title(labels[c]+'\/'+myclasses.loc[300+c])\n    c=c+1\nplt.show()\n","fce1d26b":"# ---------------------------End Code--------------------------","719258e7":"\n# Predict labels for given images\nTake the images form 300 to 307 in validation data and predict the labels. And also display the images and corresponding labels.","7e0979f7":"# Prepare the train dataset for CNN model.\nExpected shape: (18900, 32, 32, 3)","a8b29426":"# Define a method load_data \nAccept 2 parameters list \/ series of image file names and tuple of image size.\nThe function should return a numpy array with all images of given list with specified size.\nNormalize the data by dividing with 255","08969259":"# CNN Assignment-I\nDataset: https:\/\/www.kaggle.com\/apollo2506\/eurosat-dataset\nRead the description of dataset. \nBuild a CNN model to predict the label for given image. \nFollow the instructions given in notebook.","c91b8c42":"# Build Le-Net-5 \nModel summary:\n![image.png](attachment:image.png)\nOptimizer: Adam;\nSelect the suitable Loss Function; \nUse metrics: Accuracy; \nTrain model with 150 epochs\n","228fe84d":"# Evaluate with test dataset\nExpected shape: (2700, 32, 32, 3)","46ef6f1e":"# --------------------------Start Your Code Here -----------------------------\n# Read the train dataframe "}}