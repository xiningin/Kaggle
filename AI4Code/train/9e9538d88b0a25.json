{"cell_type":{"4c422516":"code","96c3b86d":"code","eac6c8e3":"code","b99b43c3":"code","fc726b09":"code","88a4ac9a":"code","a802cff1":"code","6fa80b8d":"code","7d560541":"code","58e312fd":"code","233317f5":"code","99077fe3":"code","c0115938":"code","617f8368":"code","5e4d5b91":"code","0342fd8e":"code","d8b42172":"code","ef492d2a":"code","de8d0595":"code","5ffd1ce8":"code","4aaca21a":"code","146c0327":"code","d4147450":"code","cf2c2394":"code","ed99a6cd":"code","c2c453c8":"code","93a28f4e":"code","a65b6c0e":"code","c7e5b6d4":"code","4a423b9e":"code","d6e7f179":"code","1dd26c4b":"code","b195b3f2":"code","c513ff23":"code","17e91399":"code","1fc781eb":"code","95ad7cc9":"markdown","ffec3aaa":"markdown","d70f3c7c":"markdown","a2767062":"markdown","8683a761":"markdown","2d409c4c":"markdown","0d150977":"markdown","7bd58f1e":"markdown","c660629d":"markdown","6138cc97":"markdown","e9fd10d4":"markdown","0175428c":"markdown","d23da026":"markdown","3dd70b8a":"markdown","62cd0b7e":"markdown","d48792b7":"markdown","60b18ecc":"markdown","d7f5ee99":"markdown","2f4cca25":"markdown","29a7cbc1":"markdown","83c0b80a":"markdown","095e6165":"markdown","40518e6a":"markdown","b04ab915":"markdown","08f4f44f":"markdown","152a82d7":"markdown","e9318958":"markdown","7b29c237":"markdown","6893faf0":"markdown","b09cc5cc":"markdown","3761ff3f":"markdown","f0220e29":"markdown","f030c64c":"markdown","f2fd0594":"markdown"},"source":{"4c422516":"import numpy as np\nimport pandas as pd\n\nfrom fastai.vision.all import *\nfrom tqdm import tqdm\nimport pickle, gzip, math, torch, matplotlib as mpl\nimport torch.nn.functional as F\nimport pathlib\nfrom pathlib import Path\n\n#Mixed-precision training\nfrom fastai.callback.fp16 import *\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","96c3b86d":"training_df_all = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntesting_df_all = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")","eac6c8e3":"valid_df = training_df_all[35000:42000]\ntraining_df = training_df_all[0:34999]","b99b43c3":"training_df.tail()","fc726b09":"x_train = torch.tensor(training_df.drop(['label'], axis = 1).values).float() \/ 255.0\ny_train = torch.tensor(training_df.label.values).float().unsqueeze(1)\nx_valid = torch.tensor(valid_df.drop(['label'], axis = 1).values).float() \/ 255.0\ny_valid = torch.tensor(valid_df.label.values).float().unsqueeze(1)\n\nprint(x_train.shape, y_train.shape)","88a4ac9a":"# Reshape image in 3 dimensions (height = 28px, width = 28px) for visualization purposes\nx_train_image = torch.reshape(x_train, (-1,28,28))\nprint(x_train_image.shape)","a802cff1":"show_image(x_train_image[0])","6fa80b8d":"plt.imshow(x_train_image[0])","7d560541":"dset = list(zip(x_train,y_train.squeeze(1).long()))\nvalid_dset = list(zip(x_valid,y_valid.squeeze(1).long()))\nx,y = dset[0]\nx.shape,y","58e312fd":"dl = DataLoader(dset, batch_size=256)\nvalid_dl = DataLoader(valid_dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape,yb.shape","233317f5":"def init_params(size, std=1.0): \n    return (torch.randn(size)*std).requires_grad_()","99077fe3":"weights = init_params((28*28,10))\nbias = init_params(10)\nweights[0], bias[0]","c0115938":"# def linear1(xb): \n#     return xb@weights + bias\n\n# preds = linear1(x_train)\n# preds[0]","617f8368":"def simple_net(xb): \n    res = xb@w1 + b1\n    res = res.max(tensor(0.0))\n    res = res@w2 + b2\n    return res\n\nw1 = init_params((28*28,50))\nb1 = init_params(50)\nw2 = init_params((50,10))\nb2 = init_params(10)\n\npreds = simple_net(x_train)\npreds[0], preds.shape","5e4d5b91":"loss_func = F.cross_entropy","0342fd8e":"def calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = loss_func(preds, yb)\n    loss.backward()","d8b42172":"def train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()","ef492d2a":"def accuracy(out, yb): \n    return (torch.argmax(out, dim=1)==yb).float().mean()\n\ndef validate_epoch(model):\n    accs = [accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)","de8d0595":"preds = simple_net(xb)\nprint(preds[0], preds.shape)\nprint(loss_func(preds, yb))\nprint(accuracy(preds, yb))","5ffd1ce8":"lr = 1\nparams = w1, b1, w2, b2\ntrain_epoch(simple_net, lr, params)\nvalidate_epoch(simple_net)","4aaca21a":"for i in range(20):\n    train_epoch(simple_net, lr, params)\n    print(validate_epoch(simple_net), end=' ')","146c0327":"simple_net = nn.Sequential(\n    nn.Linear(28*28,50),\n    nn.ReLU(),\n    nn.Linear(50,10)\n)\n\nw1, b1, w2, b2 = simple_net.parameters()\nprint(w1.shape,b1.shape,w2.shape,b2.shape)\n\nopt = SGD(simple_net.parameters(), lr)\n\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n        \nvalidate_epoch(simple_net)\n\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\ntrain_model(simple_net, 20)","d4147450":"dls = DataLoaders(dl, valid_dl)\nlearn = Learner(dls, simple_net, opt_func=SGD, loss_func=F.cross_entropy, metrics=accuracy)","cf2c2394":"learn.fit(10, lr=0.01)","ed99a6cd":"path = Path('\/kaggle\/working')\ntrain_path = path\/'train'\ntest_path = path\/'test'\ntrain_path.mkdir()\ntest_path.mkdir()\n\nfor i, row in tqdm(enumerate(training_df_all.values)):\n    dir = train_path\/str(row[0])\n    dir.mkdir(exist_ok=True)\n    img_arr = row[1:].reshape(28, 28).astype(np.uint8)\n    Image.fromarray(img_arr, \"L\").save(dir\/f\"{i}.png\")\n    \nfor i, row in tqdm(enumerate(testing_df_all.values)):\n    img_arr = row.reshape(28, 28).astype(np.uint8)\n    Image.fromarray(img_arr, \"L\").save(test_path\/f\"{i}.png\")\n    \nmnist = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(),\n    get_y=parent_label,\n    batch_tfms=Normalize(imagenet_stats)\n)","c2c453c8":"dls = mnist.dataloaders(train_path, batch_size=64)\narch = models.resnet18\n\nlearn = cnn_learner(dls, arch, pretrained=True, loss_func=F.cross_entropy, metrics=accuracy).to_fp16()\n\nlr_min, lr_steep = learn.lr_find()\nprint(f\"Minimum\/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")","93a28f4e":"learn.fit_one_cycle(5, lr_min, wd=0.1)","a65b6c0e":"learn.unfreeze()","c7e5b6d4":"lr_min, lr_steep = learn.lr_find()\nprint(f\"Minimum\/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")","4a423b9e":"learn.fit_one_cycle(5, lr_max=slice(lr_min\/10,lr_min*10), wd=0.1)","d6e7f179":"learn.recorder.plot_loss()","1dd26c4b":"interp = ClassificationInterpretation.from_learner(learn)\n# interp.plot_confusion_matrix(figsize=(12,12), dpi=60)","b195b3f2":"test_len = len(test_path.ls())\ntest_fns = L()\n# sort test file names (0.png, 1.png, 2.png, ...)\nfor i in tqdm(range(test_len)):\n    fn = test_path\/f\"{i}.png\"\n    test_fns.append(fn)\n\ntest_dl = dls.test_dl(test_fns)","c513ff23":"preds = learn.get_preds(dl=test_dl)[0].argmax(dim=1)\npreds","17e91399":"df_result = pd.DataFrame(\n    {\n        \"Label\": preds.tolist()\n    }\n)\ndf_result.index += 1\ndf_result.head()","1fc781eb":"df_result.to_csv(\"submission.csv\", index_label=\"ImageId\")","95ad7cc9":"It works! Let's try for a few more epochs:","ffec3aaa":"<span style=\"color:blue\">That little function res.max(tensor(0.0)) is called a rectified linear unit, also known as ReLU. We think we can all agree that rectified linear unit sounds pretty fancy and complicated... But actually, there's nothing more to it than res.max(tensor(0.0))\u2014in other words, replace every negative number with a zero. This tiny function is also available in PyTorch as F.relu:<\/span>\n\n![image.png](attachment:64c0ab53-889e-4320-a4a2-a3fcdf6a9182.png)","d70f3c7c":"# First model with Pytorch\n\n## Step 0: Preparation\n\nFirst of all, we need to create what we call a **Dataset**. The dataset links each input data (here, each image) to its label.\n\nA Dataset in PyTorch is required to return a tuple of (x,y) when indexed. Python provides a zip function which, when combined with list, provides a simple way to get this functionality:","a2767062":"# Summarize the method\n\nTo summarize, at the beginning, the weights of our model can be random (training from scratch) or come from a pretrained model (transfer learning). In the first case, the output we will get from our inputs won't have anything to do with what we want, and even in the second case, it's very likely the pretrained model won't be very good at the specific task we are targeting. So the model will need to learn better weights.\n\nWe begin by comparing the outputs the model gives us with our targets (we have labeled data, so we know what result the model should give) using a loss function, which returns a number that we want to make as low as possible by improving our weights. To do this, we take a few data items (such as images) from the training set and feed them to our model. We compare the corresponding targets using our loss function, and the score we get tells us how wrong our predictions were. We then change the weights a little bit to make it slightly better.\n\nTo find how to change the weights to make the loss a bit better, we use calculus to calculate the gradients. (Actually, we let PyTorch do it for us!) Let's consider an analogy. Imagine you are lost in the mountains with your car parked at the lowest point. To find your way back to it, you might wander in a random direction, but that probably wouldn't help much. Since you know your vehicle is at the lowest point, you would be better off going downhill. By always taking a step in the direction of the steepest downward slope, you should eventually arrive at your destination. We use the magnitude of the gradient (i.e., the steepness of the slope) to tell us how big a step to take; specifically, we multiply the gradient by a number we choose called the learning rate to decide on the step size. We then iterate until we have reached the lowest point, which will be our parking lot, then we can stop.\n\nAll of that we just saw can be transposed directly to the MNIST dataset, except for the loss function. Let's now see how we can define a good training objective.","8683a761":"**Modify weights:** Deciding how to change our parameters based on the values of the gradients is an important part of the deep learning process. Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the learning rate (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything. Often, people select a learning rate just by trying a few, and finding which results in the best model after training. Once you've picked a learning rate, you can adjust your parameters using this simple function:\n\nw -= gradient(w) * lr\n\nThis is known as stepping your parameters, using an optimizer step.\n\nChoosing the adequate learning rate is very important:\n* If you pick a learning rate that's too low, it can mean having to do a lot of steps,\n* But picking a learning rate that's too high is even worse\u2014it can actually result in the loss getting worse,\n* If the learning rate is too high, it may also \"bounce\" around, rather than actually diverging.\n\n![image.png](attachment:a9c0f96c-aa08-4998-9226-5704fc4d423d.png)\n![image.png](attachment:bbe83323-4f75-46ba-812e-aec0cf89e37a.png)\n![image.png](attachment:3064f25c-3f74-45fd-8eb3-6ec3a42af327.png)","2d409c4c":"# Illustrate the seven steps with a simple example\n\nThe following example illustrates what happens in 2 dimensions, assuming that we want to find the minimum of the function f: x -> x^2.\nIn reality, it is exactly the same thing that happens: for each parameter, we want to modify its associated weight in order to minimize the loss.\n\n**Initialization**: We first take a random x value.\n\n![image.png](attachment:895237f5-daa4-4105-8adb-69cc8d5ead89.png)\n\n**Prediction**: Here, the prediction simply corresponds to our x value.\n\n**Compute Loss**: Here, we can just make the difference between our x value and the xmin value in order to compute the loss (loss = 1,5) ","0d150977":"# For the fame: Predict and export submission","7bd58f1e":"Here, we are having a hard time improving the accuracy after unfreezing the model.\n\n## Visualize the results\n\nFastai can show us a graph of the training and validation loss.\n\n**Remember: when the validation loss starts to rise and the training loss continues to fall, you are starting to overfit!**","c660629d":"## Step 6, 7: Combining everything\n\nWe also want to check how we're doing, by looking at the accuracy of the validation set, so let's define an accuracy metric. We take the argmax of our output, to find out which of the numbers of the softmax is the highest. The index of that is our prediction. Then we check if it's equal with the real value, and we take the mean of it.","6138cc97":"### This in-depth tutorial will teach you how to easily create your first deep learning model from scratch, using Pytorch and then FastAI!\n\n# Sources\n\n* https:\/\/github.com\/fastai\/fastbook\/blob\/master\/04_mnist_basics.ipynb\n* https:\/\/towardsdatascience.com\/cross-entropy-loss-function-f38c4ec8643e\n* https:\/\/www.kaggle.com\/thinhntr\/digit-recognizer-using-fastai-v2-mar-2021\n\n# Imports","e9fd10d4":"In order to take an optimization step we need to calculate the loss over one or more data items. How many should we use? We could calculate it for the whole dataset, and take the average, or we could calculate it for a single data item. But neither of these is ideal. Calculating it for the whole dataset would take a very long time. Calculating it for a single item would not use much information, so it would result in a very imprecise and unstable gradient. That is, you'd be going to the trouble of updating the weights, but taking into account only how that would improve the model's performance on that single item.\n\nSo instead we take a compromise between the two: we calculate the average loss for a few data items at a time. This is called a **mini-batch**. The number of data items in the mini-batch is called the **batch size**. A larger batch size means that you will get a more accurate and stable estimate of your dataset's gradients from the loss function, but it will take longer, and you will process fewer mini-batches per epoch. **Choosing a good batch size is one of the decisions you need to make as a deep learning practitioner to train your model quickly and accurately**.\n\nThat is why we also need a **Dataloader**. A DataLoader can take any Python collection and turn it into an iterator over many batches.\n\nWhen we pass a Dataset to a DataLoader we will get back many batches which are themselves tuples of tensors representing batches of independent and dependent variables","0175428c":"# Condense code using Pytorch\n\nWe can factorize the above elements as follows:","d23da026":"**Compute Gradient:** We compute the derivative of our function f, and we look at the value of our slope to get our gradient. The sign of the slope allows us to know if we should increase or decrease our weight. The value of the slope allows us to determine the intensity of our modification. \n\nOne important thing to be aware of is that our function has lots of weights that we need to adjust, so when we calculate the derivative we won't get back one number, but lots of them\u2014a gradient for every weight. But there is nothing mathematically tricky here; you can calculate the derivative with respect to one weight, and treat all the other ones as constant, then repeat that for each other weight. This is how all of the gradients are calculated, for every weight.\n\nWe mentioned just now that you won't have to calculate any gradients yourself. How can that be? Amazingly enough, PyTorch is able to automatically compute the derivative of nearly any function! What's more, it does it very fast. Most of the time, it will be at least as fast as any derivative function that you can create by hand.\n\n![image.png](attachment:198ce2ac-2401-4308-bab9-54b73ddd9c75.png)","3dd70b8a":"The shape of our weights should be 784x10, because at the end of our model, we want to get 10 predictions, one for each number between 0 and 9. We can then take the argmax of each value. The largest value obtained will be our prediction.","62cd0b7e":"## Reminder: Datasets and Dataloaders\n\n* **Dataset**: A collection that returns a tuple of your independent and dependent variable for a single item\n* **DataLoader**: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables\n\nOn top of these, fastai provides two classes for bringing your training and validation sets together:\n\n* **Datasets**::An object that contains a training Dataset and a validation Dataset\n* **DataLoaders**: An object that contains a training DataLoader and a validation DataLoader\n\n## Find a good learning rate\n\nOne of the most important things we can do when training a model is to make sure that we have the right learning rate. If our learning rate is too low, it can take many, many epochs to train our model. Not only does this waste time, but it also means that we may have problems with overfitting, because every time we do a complete pass through the data, we give our model a chance to memorize it.\n\nWhat do we do to find the perfect learning rate\u2014not too high, and not too low? In 2015 the researcher Leslie Smith came up with a brilliant idea, called the learning rate finder. His idea was to start with a very, very small learning rate, something so small that we would never expect it to be too big to handle. We use that for one mini-batch, find what the losses are afterwards, and then increase the learning rate by some percentage (e.g., doubling it each time). Then we do another mini-batch, track the loss, and double the learning rate again. We keep doing this until the loss gets worse, instead of better. This is the point where we know we have gone too far. We then select a learning rate a bit lower than this point. Our advice is to pick either:\n\n* One order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10)\n* The last point where the loss was clearly decreasing\n\nThe learning rate finder computes those points on the curve to help you. Both these rules usually give around the same value.\n\n## Add mixed-precision training\n\nOne of the downside of deeper architectures is that they take quite a bit longer to train. One technique that can speed things up a lot is **mixed-precision training**. This refers to using less-precise numbers (half-precision floating point, also called fp16) where possible during training. As we are writing these words in early 2020, nearly all current NVIDIA GPUs support a special feature called tensor cores that can dramatically speed up neural network training, by 2-3x. They also require a lot less GPU memory. To enable this feature in fastai, just add **to_fp16()** after your Learner creation (you also need to import the module). **It often even improve the accuracy of the model**, because it adds a tiny bit of randomness to it! (not always though)\n\n## Add weight decay\n\nWeight decay, or L2 regularization, consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.\n\nWhy would it prevent overfitting? The idea is that the larger the coefficients are, the sharper canyons we will have in the loss function. If we take the basic example of a parabola, y = a * (x**2), the larger a is, the more narrow the parabola is\n\n![image.png](attachment:ff8ac616-77d3-4ca8-9c20-308ceeee528e.png)\n\nSo, letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting.\n\nLimiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better. Going back to the theory briefly, weight decay (or just wd) is a parameter that controls that sum of squares we add to our loss (assuming parameters is a tensor of all parameters):\n\nloss_with_wd = loss + wd * (parameters^2).sum()\nIn practice, though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, you might recall that the derivative of p^2 with respect to p is 2*p, so adding that big sum to our loss is exactly the same as doing:\n\nparameters.grad += wd * 2 * parameters\nIn practice, since wd is a parameter that we choose, we can just make it twice as big, so we don't even need the *2 in this equation. To use weight decay in fastai, just pass wd in your call to fit or fit_one_cycle:","d48792b7":"## <span style=\"color:red\">Summary of all technics and tricks to use\/test<\/span>\n\n### <span style=\"color:red\">First model<\/span>\n\n* <span style=\"color:red\">Choose a good archiecture (**resnet 18, 34, 50**, or anything else)<\/span>\n* <span style=\"color:red\">Use **pretrained weights** (or not)<\/span>\n* <span style=\"color:red\">Choose the good loss function (here, **cross-entropy**)<\/span>\n* <span style=\"color:red\">Choose a good **accuracy metric**<\/span>\n* <span style=\"color:red\">Find a good learning rate with lr_finder (**learn.lr_find()**)<\/span>\n\n### <span style=\"color:red\">Going further:<\/span>\n\n* <span style=\"color:red\">Add mixed-precision training (**.to_fp16()**)<\/span>\n* <span style=\"color:red\">Add weight decay (**wd=0.1** or any other value that fits better)<\/span>\n* <span style=\"color:red\">Use discriminative learning rates (**learn.unfreeze()** and **learn.fit_one_cycle(5, lr_max=slice(lr_min\/10,lr_min*10), wd=0.1)**)<\/span>\n* <span style=\"color:red\">More to come: Mix-up, TTA, Dropout...<\/span>","60b18ecc":"We want to train a model in such a way that we allow it to remember all of these generally useful ideas from the pretrained model, use them to solve our particular task (classify pet breeds), and only adjust them as required for the specifics of our particular task.\n\nOur challenge when fine-tuning is to replace the random weights in our added linear layers with weights that correctly achieve our desired task (classifying pet breeds) without breaking the carefully pretrained weights and the other layers. There is actually a very simple trick to allow this to happen: tell the optimizer to only update the weights in those randomly added final layers. Don't change the weights in the rest of the neural network at all. This is called freezing those pretrained layers.\n\nWhen we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the **fine_tune** method (i.e. **learn.fine_tune(2, base_lr=3e-3)** fastai does two things:\n\n* Trains the randomly added layers for one epoch, with all other layers frozen,\n* Unfreezes all of the layers, and trains them all for the number of epochs requested (here, 2).\n\nAlthough this is a reasonable default approach, it is likely that for your particular dataset you may get better results by doing things slightly differently. The fine_tune method has a number of parameters you can use to change its behavior, but it might be easiest for you to just call the underlying methods directly if you want to get some custom behavior.\n\n## My personal method\n\nI personaly prefer to do it manually. Basically here is what I do:\n\n* I call **learn.fit_one_cycle(5, lr_min)**, which will train the model only on the last layers for a few epochs, at the learning rate defined by the learning rate finder.\n* I unfreeze the other layers,\n* I run the learning rate finder again, to adjust it,\n* I re-train for a few epochs, using the \"**discriminative learning rate**\" method.","d7f5ee99":"# Describe the deep learning approach\n\nwe could look at each individual pixel and come up with a set of weights for each one, such that the highest weights are associated with those pixels most likely to be black for a particular category. For instance, pixels toward the bottom right are not very likely to be activated for a 7, so they should have a low weight for a 7, but they are likely to be activated for an 8, so they should have a high weight for an 8. This can be represented as a function and set of weight values for each possible category\u2014for instance the probability of being the number 8:\n\ndef pr_eight(x,w): return (x*w).sum()\n\nHere we are assuming that x is the image, represented as a vector\u2014in other words, with all of the rows stacked up end to end into a single long line. And we are assuming that the weights are a vector w. If we have this function, then we just need some way to update the weights to make them a little bit better. With such an approach, we can repeat that step a number of times, making the weights better and better, until they are as good as we can make them.\n\nWe want to find the specific values for the vector w that causes the result of our function to be high for those images that are actually 8s, and low for those images that are not. Searching for the best vector w is a way to search for the best function for recognising 8s. (Because we are not yet using a deep neural network, we are limited by what our function can actually do).\n\nTo be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier:\n\n1. **Initialization:** Initialize the weights.\n1. **Prediction:** For each image, use these weights to predict which number it appears to be.\n1. **Compute loss:** Based on these predictions, calculate how good the model is (its loss).\n1. **Compute gradient:** Calculate the gradient, which measures for each weight, how changing that weight would change the loss\n1. **Modify weights:** Step (that is, change) all the weights based on that calculation.\n1. **Repeat step 2 to 5 n times:** Go back to the step 2, and repeat the process.\n1. **Stop:** Iterate until you decide to stop the training process (for instance, because the model is good enough or you don't want to wait any longer).\n\n![image.png](attachment:e5f22fde-f200-4849-a928-c6945131f622.png)\n\nThere are many different ways to do each of these seven steps. These are the details that make a big difference for deep learning practitioners, but it turns out that the general approach to each one generally follows some basic principles. Here are a few guidelines:\n\n* **Initialize**: We initialize the parameters to random values. This may sound surprising. There are certainly other choices we could make, such as initializing them to the percentage of times that pixel is activated for that category\u2014but since we already know that we have a routine to improve these weights, it turns out that just starting with random weights works perfectly well.\n* **Loss**: We need some function that will return a number that is small if the performance of the model is good (the standard approach is to treat a small loss as good, and a large loss as bad, although this is just a convention).\n* **Step**: A simple way to figure out whether a weight should be increased a bit, or decreased a bit, would be just to try it: increase the weight by a small amount, and see if the loss goes up or down. Once you find the correct direction, you could then change that amount by a bit more, and a bit less, until you find an amount that works well. However, this is slow! As we will see, the magic of calculus allows us to directly figure out in which direction, and by roughly how much, to change each weight, without having to try all these small changes. The way to do this is by calculating gradients. This is just a performance optimization, we would get exactly the same results by using the slower manual process as well.\n* **Stop**: Once we've decided how many epochs to train the model for (a few suggestions for this were given in the earlier list), we apply that decision. This is where that decision is applied. For our digit classifier, we would keep training until the accuracy of the model started getting worse, or we ran out of time.","2f4cca25":"## Step 3: Compute Loss - Cross Entropy\n\nA lot of loss functions exists, each of them is helpful in some specific situations. Four our problem, a good loss function is cross-entropy.\n\n* For more info about cross entropy, check https:\/\/towardsdatascience.com\/cross-entropy-loss-function-f38c4ec8643e\n* For even more info about the maths behind cross-entropy, check III.7) of https:\/\/www.kaggle.com\/toldo171\/digit-recognition-from-scratch\n\nIn a word, cross-entropy is defined as:\n\n![image.png](attachment:99a76a26-fa31-48ac-802c-de7d52bd3d48.png)\n\nFirst, we will need to compute the softmax of our activations. This is defined by:\n$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n\nor more concisely:\n$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$\n\n\nThen, if we consider a classification problem where the model has to determine if an image is a 1, 2, 3 or 4, and the softmax probabilities (S) and labels (T) of the model are:\n\n![image.png](attachment:fe5a1b4c-df51-435f-a567-54632e2ecc40.png)\n\nthen the categorical cross-entropy is computed as follows:\n\n![image.png](attachment:ef7c9977-4929-435f-97f0-11e363c6382d.png)\n\nSoftmax is continuously differentiable function. This makes it possible to calculate the derivative of the loss function with respect to every weight in the neural network. This property allows the model to adjust the weights accordingly to minimize the loss function (model output close to the true values).\n\nIn PyTorch, F.log_softmax and F.nll_loss are combined in one optimized function, F.cross_entropy.","29a7cbc1":"# Going further with FastAI\n\n## Prepare the data\n\nWe now want to use an advanced image classification architecture, so we need to convert our data so that FastAI understands that they are images.","83c0b80a":"2 ways to plot an image:","095e6165":"At this point we have something that is rather magical:\n\n1. A function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters\n1. A way to find the best set of parameters for any function (stochastic gradient descent)\n\nThis is why deep learning can do things which seem rather magical, such fantastic things. Believing that this combination of simple techniques can really solve any problem is one of the biggest steps that we find many students have to take. It seems too good to be true\u2014surely things should be more difficult and complicated than this? Our recommendation: try it out! We just tried it on the MNIST dataset and you have seen the results. And since we are doing everything from scratch ourselves (except for calculating the gradients) you know that there is no special magic hiding behind the scenes.","40518e6a":"# Gather Data\n\nThe first thing to do is to gather and prepare our dataset. Here, we have 28x28 grayscale images. Each line of the dataframe represents a 784 pixels image, as well as the label associated to the image (a number between 0 and 9).\nIf we want to visualize the images, we can transform this dataset into a tensor (which allows to make calculations on the GPU) of size 42000x28x28 because : \n* We have 42000 images,\n* Each image is of size 28x28.","b04ab915":"## Step 5: Update weights\n\nOur only remaining step is to update the weights and biases based on the gradient and learning rate (**i.e. param = param - param_grad * lr**). When we do so, we have to tell PyTorch not to take the gradient of this step too\u2014otherwise things will get very confusing when we try to compute the derivative at the next batch! If we assign to the data attribute of a tensor then PyTorch will not take the gradient of that step. Here's our basic training loop for an epoch:","08f4f44f":"### <span style=\"color:blue\">Bonus ! Adding a Nonlinearity<\/span>\n\n<span style=\"color:blue\">So far we have a general procedure for optimizing the parameters of a function, and we have tried it out on a very boring function: a simple linear classifier. A linear classifier is very constrained in terms of what it can do. To make it a bit more complex (and able to handle more tasks), we need to add something nonlinear between two linear classifiers\u2014this is what gives us a neural network.<\/span>","152a82d7":"## Step 2: Prediction","e9318958":"## Step 4: Compute Gradient\n\nThis step is pretty straightforward: we compute our predictions and then the loss between our predictions and the real values, using softmax activations and cross entropy loss (regrouped in *F.cross_entropy*).","7b29c237":"# Condense code using FastAI\n\nWe can now use FastAI to do the same thing. As you can see, we now know how to do from scratch everything that is written in the function:\n* The learner is given a dataloader,\n* The optimization method is SGD (what we did above)\n* The loss function used to optimize the model is the cross-entropy loss,\n* The metric used to define the efficiency of our model is the accuracy","6893faf0":"We will also divide each number in our tensor by 255 here, in order to normalize it. The unsqueeze() method is here to add a dimension to the label tensor. ","b09cc5cc":"## Discriminative learning rates\n\nEven after we unfreeze, we still care a lot about the quality of those pretrained weights. We would not expect that the best learning rate for those pretrained parameters would be as high as for the randomly added parameters, even after we have tuned those randomly added parameters for a few epochs. Remember, the pretrained weights have been trained for hundreds of epochs, on millions of images.\n\nTherefore, fastai's default approach is to use discriminative learning rates.\n\n![image.png](attachment:aa1bd0ab-abee-4870-b589-9bfa90ef25b7.png)\n\nFastai lets you pass a Python slice object anywhere that a learning rate is expected. The first value passed will be the learning rate in the earliest layer of the neural network, and the second value will be the learning rate in the final layer. The layers in between will have learning rates that are multiplicatively equidistant throughout that range.","3761ff3f":"That's our starting point. We have around 10% accuracy, which is totally normal since our initialisation step was done randomly. Let's train for one epoch, and see if the accuracy improves:","f0220e29":"## Step 1: Initialization\n\nWe need an (initially random) weight for every pixel (this is the initialize step in our seven-step process).\n\nThe function weights times pixels won't be flexible enough\u2014it is always equal to 0 when the pixels are equal to 0 (i.e., its intercept is 0). Thus, we need a bias b. In neural networks, the w in the equation y=w*x+b is called the weights, and the b is called the bias. Together, the weights and bias make up the parameters.","f030c64c":"And this is it! The rest is only \"tricks\" to improve the results, but here **we just finished our first deep learning model from scratch!**","f2fd0594":"Let's try to run this for one batch (256 images) to see if everything works:"}}