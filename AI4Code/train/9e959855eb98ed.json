{"cell_type":{"b65a69d2":"code","d9afead3":"code","7d2545c3":"code","49a6b646":"code","c3f05def":"code","787f47f3":"code","4b59d993":"code","ba5d92c2":"code","62da7a10":"code","09c6d1e5":"code","d59b02ee":"code","f115298f":"code","3333cba2":"code","2bf8f181":"code","ce847891":"code","5aa209e1":"code","966ce48c":"code","c9a31cf3":"code","a54dddc5":"code","83119c1b":"code","873782fe":"code","1c167e37":"code","66d9748d":"code","9670c9df":"code","3d1a5344":"code","d4c10e6e":"code","c7b2cf7b":"code","7dcc972f":"code","145ebaff":"code","d29211a9":"code","716c38c7":"code","61f2b152":"code","d6bbf21d":"code","f7980bcb":"code","93672a22":"code","f1b27ede":"code","8f6d01a6":"code","f9faa497":"code","30c659b4":"code","a2fefa2c":"code","b2aa0afa":"code","8e06accf":"code","b35d08f5":"code","00611036":"code","bd9300f2":"code","2df22375":"code","21494de1":"code","29b60883":"code","91f9a03a":"code","fa2a37e9":"code","6dee5ee2":"code","a6740226":"code","995f6421":"code","269138cd":"code","2073c67b":"code","4c7f72e9":"code","2ff4afbd":"code","0be8051a":"code","9366f668":"code","ef30a950":"code","67149cdb":"code","48eceb61":"code","5ad12356":"code","11f6b16d":"code","c350d9e0":"code","1e9f1388":"code","391e1446":"code","2b43b273":"code","6660243f":"code","8d17920c":"code","7495aaa1":"code","87e24430":"code","e265d90c":"code","c09b411f":"code","639615d4":"code","4a70a2ed":"code","8ee62af5":"code","cf2428a2":"code","b51be8a8":"code","4f471d0f":"code","d1f40a05":"code","06bc2a8f":"code","d19c5840":"code","f43f8be3":"code","3a2a4535":"code","f0161472":"markdown","23667043":"markdown","dbe343e7":"markdown","7c67f0c8":"markdown","8ac4449d":"markdown","75aeb18c":"markdown","487329dc":"markdown","fc2a0ae5":"markdown","b657ba52":"markdown","d2bef2ee":"markdown","4d17b73d":"markdown","c45190fb":"markdown","212d214a":"markdown","2cb457c3":"markdown","98f24d7f":"markdown","175f2574":"markdown","e8b73138":"markdown","ff9da759":"markdown","cea92d58":"markdown","03c84292":"markdown","1ef1acb9":"markdown","7508f42a":"markdown","7075acb0":"markdown"},"source":{"b65a69d2":"import nltk\nimport string\nimport re\nimport numpy as np\nimport pandas as pd\nimport pickle\n#import lda\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\")\n\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import stop_words\n\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook\n#from bokeh.transform import factor_cmap\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.getLogger(\"lda\").setLevel(logging.WARNING)\n\nPATH = \"..\/input\/\"","d9afead3":"len(stopwords.words('english'))","7d2545c3":"len(stop_words.ENGLISH_STOP_WORDS)","49a6b646":"train = pd.read_csv(f\"{PATH}train.tsv\",sep='\\t')\ntest = pd.read_csv(f\"{PATH}test.tsv\",sep='\\t')","c3f05def":"print(train.shape)\nprint(test.shape)","787f47f3":"frac = 0.1\ntrain = train.sample(frac=frac,random_state=200)\ntest = test.sample(frac=frac,random_state=200)","4b59d993":"print(train.shape)\nprint(test.shape)","ba5d92c2":"train.dtypes","62da7a10":"train.head()","09c6d1e5":"train['price'].describe()","d59b02ee":"fig,ax = plt.subplots(1,2,figsize=[10,5])\nax = plt.subplot(1,2,1)\nsns.distplot(train['price'],hist=False,ax=ax,bins=50)\nax.set(ylabel='Density',title='Distribution of price')\nax = plt.subplot(1,2,2)\nsns.distplot(np.log1p(train['price']),hist=False,ax=ax,bins=50)\nax.set(xlabel='log1p(price)',ylabel='Density',title='Distribution of log1p(price)')","f115298f":"train['shipping'].value_counts()","3333cba2":"shipping_buyer_price = train.loc[train['shipping'] == 0,'price']\nshipping_seller_price = train.loc[train['shipping'] == 1,'price']\n\nfig = plt.figure(figsize=[10,5])\nsns.distplot(np.log1p(shipping_buyer_price),bins=50,kde=False,color='red',label='buyer')\nsns.distplot(np.log1p(shipping_seller_price),bins=50,kde=False,color='blue',label='seller')\nplt.xlabel('log1p(price)')\nplt.legend()","2bf8f181":"train['category_name'].nunique()","ce847891":"train['category_name'].isnull().sum()","5aa209e1":"category_null = train.loc[train['category_name'].isnull()]\ncategory_notnull = train.loc[train['category_name'].notnull()]\n\n# print(category_notnull.shape)\n# category_notnull.info()\n\nsep_categories = category_notnull['category_name'].apply(lambda x:x.split('\/'))\n\ncol_names = ['general_cat','subcat_1','subcat_2']\n\ncats = []\n\nfor cats_ in sep_categories:\n    cat_dict = {col_names[0]:cats_[0],col_names[1]:cats_[1],col_names[2]:cats_[2]}\n    cats.append(cat_dict)\n    \ndf_cats = pd.DataFrame(cats)\n\ncategory_notnull = category_notnull.reset_index(drop=True)\ncategory_notnull = pd.concat([category_notnull,df_cats],axis=1)\n\nfor col in col_names:\n    category_null[col] = 'No Label'\n\n# assert category_notnull.index.max() == category_notnull.train_id.max()\n\ntrain = pd.concat([category_notnull,category_null],axis=0,sort=False).sort_values(by='train_id').reset_index(drop=True)\ntrain.info()","966ce48c":"# # reference: BuryBuryZymon at https:\/\/www.kaggle.com\/maheshdadhich\/i-will-sell-everything-for-free-0-55\n# def split_cat(text):\n#     try: return text.split(\"\/\")\n#     except: return (\"No Label\", \"No Label\", \"No Label\")","c9a31cf3":"# train['general_cat'], train['subcat_1'], train['subcat_2'] = \\\n# zip(*train['category_name'].apply(lambda x: split_cat(x)))\n# train.head()","a54dddc5":"print(\"There are %d unique general-categories.\" % train['general_cat'].nunique())","83119c1b":"print(\"There are %d unique first sub-categories.\" % train['subcat_1'].nunique())","873782fe":"print(\"There are %d unique second sub-categories.\" % train['subcat_2'].nunique())","1c167e37":"train['general_cat']","66d9748d":"plt.figure(figsize=[8,6])\ntrain['general_cat'].value_counts().plot(kind='bar',linewidth=2)\nplt.title('Number of Items by Main Category')\nplt.ylabel('Count')\nplt.xlabel('Categories')\nplt.xticks(rotation=60)","9670c9df":"train['subcat_1'].value_counts()[:15].plot(kind='bar',linewidth=2, figsize=[12,10])\nplt.title('Number of Items by Subset_1')\nplt.ylabel('Count')\nplt.xlabel('Categories')\nplt.xticks(rotation=60)","3d1a5344":"fig = plt.figure(figsize=[12,10])\nsns.boxplot(x=train['general_cat'],y=np.log1p(train['price']))\nplt.xticks(rotation=60)","d4c10e6e":"def wordCount(text):\n    # convert to lower case and strip regex\n    try:\n         # convert to lower case and strip regex\n        text = text.lower()\n        #regular expression pattern \uc0dd\uc131\n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n        #\ud328\ud134\uc744 \uc774\uc6a9\ud558\uc5ec \ub4e4\uc5b4\uc624\ub294 \ud14d\uc2a4\ud2b8\uc5d0\uc11c \ud574\ub2f9 \ud328\ud134\uc758 \uae30\ud638\ub4e4\uc744 \uc81c\uac70\ud558\ub294 \uac83\n        txt = regex.sub(\" \", text)\n        # tokenize\n        # words = nltk.word_tokenize(clean_txt)\n        # remove words in stop words\n        words = [w for w in txt.split(\" \") \\\n                 if not w in stop_words.ENGLISH_STOP_WORDS and len(w)>3]\n        return len(words)\n    except: \n        return 0","c7b2cf7b":"train['item_description'][3]","7dcc972f":"regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')","145ebaff":"regex.sub(\" \",train['item_description'][3])","d29211a9":"list(stop_words.ENGLISH_STOP_WORDS)[:5]","716c38c7":"# add a column of word counts to both the training and test set\ntrain['desc_len'] = train['item_description'].apply(lambda x: wordCount(x))\ntest['desc_len'] = test['item_description'].apply(lambda x: wordCount(x))","61f2b152":"train.head()","d6bbf21d":"df = train.groupby('desc_len')['price'].mean().reset_index()","f7980bcb":"df.columns","93672a22":"plt.figure(figsize=[20,8])\nsns.pointplot(x=df.columns[0],y=df.columns[1],data=df)\nplt.xticks(rotation=60)","f1b27ede":"train['item_description'].isnull().sum()","8f6d01a6":"train = train.loc[train['item_description'].notnull()]","f9faa497":"stop = set(stopwords.words('english'))\ndef tokenize(text):\n    \"\"\"\n    sent_tokenize(): segment text into sentences\n    word_tokenize(): break sentences into words\n    \"\"\"\n    try: \n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n        text = regex.sub(\" \", text) # remove punctuation\n        \n        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n\n        tokens = []\n        for token_by_sent in tokens_:\n            tokens += token_by_sent\n\n        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n        \n        return filtered_tokens\n            \n    except TypeError as e: print(text,e)","30c659b4":"cat_desc = dict()\nfor cat in train.general_cat.unique(): \n    text = \" \".join(train.loc[train['general_cat']==cat, 'item_description'].values)\n    cat_desc[cat] = tokenize(text)","a2fefa2c":"# flat list of all words combined\nflat_lst = [item for sublist in list(cat_desc.values()) for item in sublist]\nallWordsCount = Counter(flat_lst)\nall_top10 = allWordsCount.most_common(20)\nx = [w[0] for w in all_top10]\ny = [w[1] for w in all_top10]","b2aa0afa":"len(flat_lst)","8e06accf":"len(allWordsCount)","b35d08f5":"all_top10","00611036":"# stop = set(stopwords.words('english'))\n# def tokenize(text):\n#     \"\"\"\n#     sent_tokenize(): segment text into sentences\n#     word_tokenize(): break sentences into words\n#     \"\"\"\n#     try: \n#         regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n#         text = regex.sub(\" \", text) # remove punctuation\n        \n# #         print(sent_tokenize(text))\n        \n#         tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n# #         print(tokens_)\n#         tokens = []\n#         for token_by_sent in tokens_:\n#             tokens += token_by_sent\n# #         print(tokens)\n#         tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n# #         print(tokens)\n#         filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n# #         print(filtered_tokens)\n#         filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n# #         print(filtered_tokens)\n        \n#         return filtered_tokens\n            \n#     except TypeError as e: print(text,e)","bd9300f2":"train['tokens'] = train['item_description'].apply(tokenize)\ntest['tokens'] = train['item_description'].apply(tokenize)","2df22375":"for description, tokens in zip(train['item_description'].head(),\n                              train['tokens'].head()):\n    print('description:', description)\n    print('tokens:', tokens)\n    print()","21494de1":"#\uc5b4\uc9dc\ud53c \ud1a0\ud070 \ub9cc\ub4e4\uc5b4\ub1a8\ub294\ub370 \ub2e4\uc2dc \uc870\uc778\ud574\uc11c \ud1a0\ud06c\ub098\uc774\uc988 \ud558\ub294 \uac83\uc740 \ub0ad\ube44\ub77c\uace0 \uc0dd\uac01\ncat_desc = dict()\nfor cat in train.general_cat.unique():\n    cat_list = []\n    for token in train.loc[train['general_cat']==cat,'tokens']:\n        for one in token:\n            cat_list.append(one)\n    cat_desc[cat] = cat_list","29b60883":"women100 = Counter(cat_desc['Women']).most_common(100)\nbeauty100 = Counter(cat_desc['Beauty']).most_common(100)\nkids100 = Counter(cat_desc['Kids']).most_common(100)\nelectronics100 = Counter(cat_desc['Electronics']).most_common(100)","91f9a03a":"women100","fa2a37e9":"def generate_wordcloud(tup):\n    wordcloud = WordCloud(background_color='white',max_words=50,max_font_size=40,random_state=42).generate(str(tup))\n    return wordcloud","6dee5ee2":"fig,ax = plt.subplots(2,2,figsize=[30,15])\n\nax = plt.subplot(2,2,1)\nax.imshow(generate_wordcloud(women100),interpolation=\"bilinear\")\nax.axis('off')\n\nax = plt.subplot(2,2,2)\nax.imshow(generate_wordcloud(beauty100))\nax.axis('off')\n\nax = plt.subplot(2,2,3)\nax.imshow(generate_wordcloud(kids100))\nax.axis('off')\n\nax = plt.subplot(2,2,4)\nax.imshow(generate_wordcloud(electronics100))\nax.axis('off')","a6740226":"train.head()","995f6421":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(min_df=10,max_features=180000,tokenizer=tokenize,ngram_range=(1,2))","269138cd":"all_desc = np.append(train['item_description'].values,test['item_description'].values)\nvz = vectorizer.fit_transform(list(all_desc))","2073c67b":"tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\ntfidf = pd.DataFrame(columns=['tfidf']).from_dict(\n                    dict(tfidf), orient='index')\ntfidf.columns = ['tfidf']","4c7f72e9":"tfidf.sort_values(by='tfidf',ascending=True).head(10)","2ff4afbd":"tfidf.sort_values(by='tfidf',ascending=False).head(10)","0be8051a":"trn = train.copy()\ntst = test.copy()\ntrn['is_train'] = 1\ntst['is_train'] = 0\n\n#t-SNE\uac00 \uc2dc\uac04\uc774 \ub9ce\uc774 \uac78\ub9ac\ub294 \uc791\uc5c5\uc774\uae30 \ub54c\ubb38\uc5d0 \uc784\uc758\uc758 \uc0d8\ud50c\uc0ac\uc774\uc988\ub85c \ub098\ub204\uace0 \uc774\ub97c \ucc28\uc6d0\ubd84\ud560\ud574\ubcf4\uc790\nsample_sz = 15000\n\ncombined_df = pd.concat([trn,tst])\ncombined_sample = combined_df.sample(n=sample_sz)\nvz_sample = vectorizer.fit_transform(list(combined_sample['item_description']))","9366f668":"vz_sample.shape","ef30a950":"from sklearn.decomposition import TruncatedSVD\n\nn_comp = 30\nsvd = TruncatedSVD(n_components=n_comp, random_state=42)\nsvd_tfidf = svd.fit_transform(vz_sample)","67149cdb":"svd_tfidf.shape","48eceb61":"from sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, verbose=1, random_state=42, n_iter=500)","5ad12356":"tsne_tfidf = tsne_model.fit_transform(svd_tfidf)","11f6b16d":"tsne_tfidf","c350d9e0":"output_notebook()\nplot_tfidf = bp.figure(plot_width=700, plot_height=600,\n                       title=\"tf-idf clustering of the item description\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n    x_axis_type=None, y_axis_type=None, min_border=1)","1e9f1388":"combined_sample = combined_sample.reset_index(drop=True)","391e1446":"tfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y'])\ntfidf_df['description'] = combined_sample['item_description']\ntfidf_df['tokens'] = combined_sample['tokens']\ntfidf_df['category'] = combined_sample['general_cat']","2b43b273":"tfidf_df.head()","6660243f":"plot_tfidf.scatter(x='x', y='y', source=tfidf_df, alpha=0.7)\nhover = plot_tfidf.select(dict(type=HoverTool))\nhover.tooltips={\"description\": \"@description\", \"tokens\": \"@tokens\", \"category\":\"@category\"}\nshow(plot_tfidf)","8d17920c":"from sklearn.cluster import MiniBatchKMeans\n\nnum_clusters = 30 # need to be selected wisely\nkmeans_model = MiniBatchKMeans(n_clusters=num_clusters,\n                               init='k-means++',\n                               n_init=1,\n                               init_size=1000, batch_size=1000, verbose=0, max_iter=1000)","7495aaa1":"kmeans = kmeans_model.fit(vz)\nkmeans_clusters = kmeans.predict(vz)\nkmeans_distances = kmeans.transform(vz)","87e24430":"# sorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n# terms = vectorizer.get_feature_names()\n\n# for i in range(num_clusters):\n#     print(\"Cluster %d:\" % i)\n#     aux = ''\n#     for j in sorted_centroids[i, :10]:\n#         aux += terms[j] + ' | '\n#     print(aux)\n#     print() ","e265d90c":"# repeat the same steps for the sample\nkmeans = kmeans_model.fit(vz_sample)\nkmeans_clusters = kmeans.predict(vz_sample)\nkmeans_distances = kmeans.transform(vz_sample)\n# reduce dimension to 2 using tsne\ntsne_kmeans = tsne_model.fit_transform(kmeans_distances)","c09b411f":"colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n\"#52697d\", \"#194196\", \"#d27c88\", \"#36422b\", \"#b68f79\"])","639615d4":"#combined_sample.reset_index(drop=True, inplace=True)\nkmeans_df = pd.DataFrame(tsne_kmeans, columns=['x', 'y'])\nkmeans_df['cluster'] = kmeans_clusters\nkmeans_df['description'] = combined_sample['item_description']\nkmeans_df['category'] = combined_sample['general_cat']\n#kmeans_df['cluster']=kmeans_df.cluster.astype(str).astype('category')","4a70a2ed":"plot_kmeans = bp.figure(plot_width=700, plot_height=600,\n                        title=\"KMeans clustering of the description\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n    x_axis_type=None, y_axis_type=None, min_border=1)","8ee62af5":"source = ColumnDataSource(data=dict(x=kmeans_df['x'], y=kmeans_df['y'],\n                                    color=colormap[kmeans_clusters],\n                                    description=kmeans_df['description'],\n                                    category=kmeans_df['category'],\n                                    cluster=kmeans_df['cluster']))\n\nplot_kmeans.scatter(x='x', y='y', color='color', source=source)\nhover = plot_kmeans.select(dict(type=HoverTool))\nhover.tooltips={\"description\": \"@description\", \"category\": \"@category\", \"cluster\":\"@cluster\" }\nshow(plot_kmeans)","cf2428a2":"cvectorizer = CountVectorizer(min_df=4,\n                              max_features=180000,\n                              tokenizer=tokenize,\n                              ngram_range=(1,2))","b51be8a8":"cvz = cvectorizer.fit_transform(combined_sample['item_description'])","4f471d0f":"cvz","d1f40a05":"lda_model = LatentDirichletAllocation(n_components=100,\n                                      learning_method='online',\n                                      max_iter=20,\n                                      random_state=42)","06bc2a8f":"X_topics = lda_model.fit_transform(cvz)","d19c5840":"lda_model.components_.shape","f43f8be3":"sorting = np.argsort(lda_model.components_,axis=1)[::-1]\nfeature_names = np.array(cvectorizer.get_feature_names())","3a2a4535":"fig, ax = plt.subplots(1,2,figsize=[10,12])\ntopic_names = [\"{:>2} \".format(i)+\" \".join(words) for i,words in enumerate(feature_names[sorting[:,:2]])]\n\nfor col in [0,1]:\n    start = col * 50\n    end = (col+1) * 50\n    ax[col].barh(np.arange(50),np.sum(X_topics,axis=0)[start:end])\n    ax[col].set_yticks(np.arange(50))\n    ax[col].set_yticklabels(topic_names[start:end],ha='left',va='top')\n    ax[col].invert_yaxis()\n    ax[col].set_xlim(0,1000)\n    yax = ax[col].get_yaxis()\n    yax.set_tick_params(pad=130)\nplt.tight_layout()","f0161472":"It's now possible to visualize our data points. Note that the deviation as well as the size of the clusters imply little information  in t-SNE.","23667043":"## **K-Means Clustering** [\ub370\uc774\ud130 \uc14b\uc744 \ud074\ub7ec\uc2a4\ud130\ub77c\ub294 \uadf8\ub8f9\uc73c\ub85c \ub098\ub204\ub294 \uc791\uc5c5]\n\nK-means clustering obejctive is to minimize the average squared Euclidean distance of the document \/ description from their cluster centroids. <br \/>\n=> \uc704\uc758 \ubaa9\ud45c\ub294 \ubb38\uc11c\ub098 \uc124\uba85\uc758 cluster centroid\uc758 Euclidean \uac70\ub9ac \uc81c\uacf1\uc758 \ud3c9\uade0\uc744 \uc904\uc774\ub294 \uac83\n","dbe343e7":"## Target Variable: Price <br \/>\nThe next standard check is with our response or target variables, which in this case is the price we are suggesting to the Mercari's marketplace sellers. <br \/>\n=> \ub2e4\uc74c\uc73c\ub85c \ud655\uc778\ud574\ubd10\uc57c \ud560 \uac83\uc740 Price\uceec\ub7fc\uc785\ub2c8\ub2e4. <br \/>\nThe median price of all the items in the training is about \\$267 but given the existence of some extreme values of over \\$100 and the maximum at \\$2,009, the distribution of the variables is heavily skewed to the left. <br \/>\n=> \uac00\uaca9\uc758 \uc911\uc559\uac12\uc740 267\ub2ec\ub7ec\uc774\uc9c0\ub9cc, \uc5b4\ub5a4 \uac00\uaca9\uc740 100\ub2ec\ub7ec\ub97c \uc6c3\ub3cc\uac70\ub098 \ucd5c\ub300\ub85c\ub294 2009\ub2ec\ub7ec \uc9dc\ub9ac \ubb3c\ud488\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \ud574\ub2f9 \uac00\uaca9\uc758 \ubd84\ud3ec\ub294 \uad49\uc7a5\ud788 \uc88c\uce21\uc73c\ub85c \uce58\uc6b0\uccd0 \uc788\uc2b5\ub2c8\ub2e4. <br \/>\nSo let's make log-transformation on the price (we added +1 to the value before the transformation to avoid zero and negative values). <br \/>\n=> \ub530\ub77c\uc11c \uac00\uaca9\uc5d0 \ub300\ud574\uc11c \ub85c\uadf8\ud654\ub97c \uc9c4\ud589\ud574 \ubd05\uc2dc\ub2e4. <br \/>","7c67f0c8":"## ** tf-idf: \ub2e8\uc5b4\ube48\ub3c4-\uc5ed\ubb38\uc11c\ube48\ub3c4** [\uc5bc\ub9c8\ub098 \uc758\ubbf8 \uc788\ub294 \ud2b9\uc131\uc778\uc9c0\ub97c \uacc4\uc0b0\ud574\uc11c \uc2a4\ucf00\uc77c\uc744 \uc870\uc815\ud558\ub294 \ubc29\uc2dd]\n**\ub9d0\ubb49\uce58\uc758 \ub2e4\ub978 \ubb38\uc11c\ubcf4\ub2e4 \ud2b9\uc815 \ubb38\uc11c\uc5d0 \uc790\uc8fc \ub098\ud0c0\ub098\ub294 \ub2e8\uc5b4\uc5d0 \ub192\uc740 \uac00\uc911\uce58\ub97c \uc8fc\ub294 \ubc29\ubc95.** <br \/>\nex)) \ud55c \ub2e8\uc5b4\uac00 \ud2b9\uc815 \ubb38\uc11c\uc5d0 \uc790\uc8fc \ub098\ud0c0\ub098\uace0 \ub2e4\ub978 \uc5ec\ub7ec \ubb38\uc11c\uc5d0\ub294 \uadf8\ub807\uc9c0 \uc54a\ub2e4\uba74, \uadf8 \ubb38\uc11c\uc758 \ub0b4\uc6a9\uc744 \uc798 \uc124\uba85\ud558\ub294 \ub2e8\uc5b4\ub77c\uace0 \ubcfc \uc218\uc788\uc74c.\n\n### Term Frequency: \uc8fc\uc5b4\uc9c4 \ub2e4\ud050\uba3c\ud2b8(\ud558\ub098\uc758 \ubb38\uc790\uc5f4)\uc5d0 \ub2e8\uc5b4\uc758 \ub4f1\uc7a5\ud69f\uc218(CountVectorizer)\n### Inverse Document Frequency: \ub2e4\ud050\uba3c\ud2b8\uc758 \ud558\ub098\uc758 \ub9d0\ubb49\uce58\uc5d0\uc11c \uc784\uc758\uc758 \ub2e8\uc5b4\uac00 \uc0c1\ud638\uac04\uc5d0 \ub4f1\uc7a5\ud558\ub294 \ud69f\uc218\n=> CountVectorizer\ub97c \ud63c\uc6a9\ud558\ub294 \uac83 \uac19\uc74c\n<br \/>\n\ne.g) \uc784\uc758\uc758 \ub2e8\uc5b4\uac00 \ubaa8\ub4e0 \ub2e4\ud050\uba3c\ud2b8\ub4e4\uc5d0\uc11c \ube48\ubc88\ud558\uae30 \ub098\uc628\ub2e4\uba74, \uc5b4\ub290 \ud2b9\uc815\ud55c \ub2e4\ud050\uba3c\ud2b8\uc5d0\uc11c \uadf8 \ub2e8\uc5b4\uc758 \uc874\uc7ac\ub294 \uadf8 \ub2e4\ud050\uba3c\ud2b8\uc5d0\uc11c \ud574\ub2f9 \ub2e8\uc5b4\uac00 \ud2b9\uc815\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\uc9c0 \ubabb\ud55c\ub2e4\uace0 \ud560\uc218\uc788\ub2e4. <br \/>\n=> \ub9cc\uc57d \uc601\ud654\uc5d0 \ub300\ud55c \ud3c9\uac00 \ud14d\uc2a4\ud2b8\ub77c\uba74 \uc11c\ub85c \ub2e4\ub978 \uc601\ud654\uc5d0\uc11c \ub098\uc62c \uc218 \uc788\ub294 \ud3c9\uac00\uc778 \"\uadf8\ub9ac\uace0\" \ub77c\ub294 \ud1a0\ud070\uc740 \ud2b9\uc815 \uc601\ud654\uac00 \uc5b4\ub290 \uc601\ud654\uc778\uc9c0 \uac00\ub2a0\ud560 \uc218 \uc5c6\uc9c0\ub9cc, \"\uc2a4\ub9b4\ub7ec\"\ub77c\ub294 \ud1a0\ud070\uc740 \uba5c\ub85c, \uc74c\uc545\ub4f1\uc758 \uc601\ud654\uc5d0\uc11c\ub294 \ub4f1\uc7a5\ud558\uc9c0 \uc54a\ub294 \uc5b4\ub290 \ud2b9\uc815\uc601\ud654\uc5d0\ub9cc \ub098\uc624\ub294 \ud1a0\ud070\uc774\uae30 \ub54c\ubb38\uc5d0 \uc5b4\ub290 \uc601\ud654\uc778\uc9c0 \uac00\ub2a0\uc774 \uac00\ub2a5\ud558\ub2e4.\n\n\ub530\ub77c\uc11c \uc704\uc640 \uac19\uc774 **\uc5ec\ub7ec \ub2e4\ud050\uba3c\ud2b8\uc5d0\uc11c \ub098\uc624\ub294 \ud1a0\ud070\ub4e4\uc758 \uacbd\uc6b0\uc5d0\ub294 \uaddc\uc81c\ub97c \uac00\ud568\uc73c\ub85c\uc368 \uac00\uc911\uce58\ub97c \ub0ae\ucd98\ub2e4.** \uc774 \ub54c\ubb38\uc5d0 \uac00\uc911\uce58\uac00 \ub192\uc740 \ub2e8\uc5b4\ub294 \uc5b4\ub290 \ud2b9\uc815 \ub2e4\ud050\uba3c\ud2b8\uc640 \uad00\ub828\uc774 \uc788\uc744 \uac00\ub2a5\uc131\uc774 \ucee4\uc9c0\ub294 \uac83\uc774\ub2e4.","8ac4449d":"### The files consist of a list of product listings. These files are tab-delimited. <br \/>\n\n* **train_id or test_id - the id of the listing <br \/>**\n=> \ubaa9\ub85d\uc758 \uace0\uc720\ud55c id <br \/>\n* **name - the title of the listing. Note that we have cleaned the data to remove text that look like prices (e.g. $20) to avoid leakage. These removed prices are represented as [rm] <br \/>**\n=> \ubaa9\ub85d\uc5d0 \uc62c\ub77c\uc628 \uc81c\ubaa9. \uac00\uaca9\ucc98\ub7fc \uc0dd\uae34 \ud14d\uc2a4\ud2b8\ub294 \ub370\uc774\ud130\uc5d0\uc11c \uc9c0\uc6b0\ub824\uace0 \ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub807\uac8c \uc9c0\uc6cc\uc9c4 \uac00\uaca9\ub4e4\uc740 [rm]\uc774\ub77c\uace0 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\n\n* **item_condition_id - the condition of the items provided by the seller <br \/>**\n=> \ud310\ub9e4\uc790\uc5d0\uac8c \uc81c\uacf5\ubc1b\uc740 \ubb3c\ud488\uc758 \uc0c1\ud0dc\ub97c \ub098\ud0c0\ub0c4<br \/>\n* **category_name - category of the listing <br \/>**\n=> \ubaa9\ub85d\uc758 \uce74\ud14c\uace0\ub9ac<br \/>\n* **brand_name <br \/>**\n=> \ube0c\ub79c\ub4dc \uc774\ub984 <br \/>\n* **price - the price that the item was sold for. This is the target variable that you will predict. The unit is USD. This column doesn't exist in test.tsv since that is what you will predict. <br \/>**\n=> \uc544\uc774\ud15c\uc774 \ud314\ub9b0 \uac00\uaca9. \ud574\ub2f9 \uceec\ub7fc\uc774 \uc6b0\ub9ac\uac00 \uc608\uce21\ud558\uace0\uc790\ud558\ub294 \ud0c0\uac9f \uceec\ub7fc\uc774\ub2e4. \ub2e8\uc704\ub294 \ubbf8\uad6d \ub2ec\ub7ec\uc774\uba70. \ud574\ub2f9 \uceec\ub7fc\uc740 test.tsv \uc5d0\ub294 \ub530\ub85c \uae30\uc7ac\ub418\uc5b4 \uc788\uc9c0 \uc54a\ub2e4,\n* **shipping - 1 if shipping fee is paid by seller and 0 by buyer <br \/>**\n=> 1\uc774\uba74 \ud310\ub9e4\uc790\uac00 \uc6b4\uc784\uc744 \ub0c8\uc73c\uba70 0\uc774\uba74 \uad6c\ub9e4\uc790\uac00 \ucc98\ub9ac\ud55c\ub2e4<br \/>\n* **item_description - the full description of the item. Note that we have cleaned the data to remove text that look like prices (e.g. $20) to avoid leakage. These removed prices are represented as [rm] <br \/>**\n=> \ubb3c\ud488\uc758 \uc0c1\uc138\ub0b4\uc6a9. \uc774 \ub370\uc774\ud130 \ub610\ud55c name\uc5d0\uc11c \ucc98\ub7fc \uac00\uaca9\uc744 \uc9c0\uc6b0\ub824\uace0 \ud588\uc73c\uba70 \uc9c0\uc6cc\uc9c4 \ubb3c\ud488\uc5d0 \ub300\ud574\uc11c\ub294 [rm] \ud45c\uae30\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. <br \/>\n\nPlease note that in stage 1, all the test data will be calculated on the public leaderboard. In stage 2, we will swap the test.tsv file to the complete test dataset that includes the private leaderboard data.","75aeb18c":"This Kernal is Cloned one of Mercari Interactive EDA + Topic Modelling by ThyKhueLy.<br \/>\nSo, I'd like to give BIG APPALUSE to him, Thanks.\n> https:\/\/www.kaggle.com\/thykhuely\/mercari-interactive-eda-topic-modelling","487329dc":"Now we can reduce the dimension from 50 to 2 using t-SNE!","fc2a0ae5":"## **Latent Dirichlet Allocation**\n\nLatent Dirichlet Allocation (LDA) is an algorithms used to discover the topics that are present in a corpus. <br \/>\n=> LDA\ub294 \ub9d0\ubb49\uce58\ub0b4\uc5d0 \uc874\uc7ac\ud558\ub294 \ud1a0\ud53d\ub4e4\uc744 \ucc3e\ub294\ub370 \uc774\uc6a9\ub418\ub294 \uc54c\uace0\ub9ac\uc998\uc785\ub2c8\ub2e4.\n\n>  LDA starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. Although the tokens themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents. <br \/>\nLDA\ub294 \uace0\uc815\ub41c \ud1a0\ud53d\uc758 \uc218\ub97c \uacb0\uc815\ud558\uba74\uc11c \uc2dc\uc791\ud558\uba70, \uac01\uac01\uc758 \ud1a0\ud53d\uc740 \ubaa8\ub4e0 \ub2e8\uc5b4\ub4e4\uc5d0 \uac78\uccd0\uc11c \ub098\ub258\uace0, \uac01\uac01\uc758 \ub2e4\ud050\uba3c\ud2b8\ub294 \ubaa8\ub4e0 \ud1a0\ud53d\ub4e4\uc5d0 \uac78\uccd0\uc11c \ub098\ub269\ub2c8\ub2e4. \ube44\ub85d \ud1a0\ud070\ub4e4\uc774 \uc758\ubbf8\uac00 \uc5c6\ub354\ub77c\ub3c4, \ud1a0\ud53d\ubcc4\ub85c \uc81c\uacf5\ub418\ub294 \ubaa8\ub4e0 \ub2e8\uc5b4\ub4e4\uc5d0 \uac78\uce5c \ud655\ub960\ubd84\ud3ec\ub294 \ub2e4\ub978 \uc885\ub958\uc758 \uc544\uc774\ub514\uc5b4\ub4e4\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.\n> \n> Reference: https:\/\/medium.com\/intuitionmachine\/the-two-paths-from-natural-language-processing-to-artificial-intelligence-d5384ddbfc18\n\nIts input is a **bag of words**, i.e. each document represented as a row, with each columns containing the count of words in the corpus. We are going to use a powerful tool called pyLDAvis that gives us an interactive visualization for LDA. ","b657ba52":"\uc6b0\ub9ac\uac00 \uc0ac\uc6a9\ud558\uac8c \ub420 \ub370\uc774\ud130\uc14b\uc758 \uc0dd\uae40\uc0c8\ub294 \uc544\ub798\uc640 \uac19\ub2e4.","d2bef2ee":"Text Processing - Item Description <br \/>\n=> \ubb3c\ud488 \uc0c1\uc138 \uceec\ub7fc\uc744 \uc704\ud55c \ubb38\uc790 \uc804\ucc98\ub9ac <br \/>\nThe following section is based on the tutorial at https:\/\/ahmedbesbes.com\/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html","4d17b73d":"## **Pre-processing:  tf-idf**\n\ntf-idf is the acronym for **Term Frequency\u2013inverse Document Frequency**. It quantifies the importance of a particular word in relative to the vocabulary of a collection of documents or corpus. The metric depends on two factors: <br \/>\n=> tf-idf\ub294 \"Term Frequency-inverse Document Frequency\"\uc758 \uc57d\uc5b4\uc785\ub2c8\ub2e4. \uc774\ub294 \ubb38\uc11c\ub098 \ub9d0\ubb49\uce58\uc758 \uc9d1\ud569\uc758 \ub2e8\uc5b4\uc640 \uad00\ub828\ub41c \ud2b9\uc815\uc5b8\uc5b4\uc758 \uc911\uc694\ub3c4\ub97c \uc218\uce58\ud654\ud569\ub2c8\ub2e4 <br \/>\n- **Term Frequency**: the occurences of a word in a given document (i.e. bag of words)\n=> \uc8fc\uc5b4\uc9c4 \ubb38\uc11c\uc5d0\uc11c \ub2e8\uc5b4\uc758 \ub4f1\uc7a5 \uc815\ub3c4<br \/>\n- **Inverse Document Frequency**: the reciprocal number of times a word occurs in a corpus of documents\n=>\ub2e8\uc5b4\uc758 \ubb49\uce58\uc5d0\uc11c \ub2e8\uc5b4\uac00 \ub4f1\uc7a5\ud55c \uc0c1\ud638\uac04\uc758 \ud69f\uc218<br \/>\n\nThink about of it this way: If the word is used extensively in all documents, its existence within a specific document will not be able to provide us much specific information about the document itself. <br \/>\n=> \ub9cc\uc57d \ub2e8\uc5b4\uac00 \ubaa8\ub4e0 \ubb38\uc11c\uc5d0\uc11c \ub9ce\uc774 \uc0ac\uc6a9\ub41c\ub2e4\uba74, \uc5b4\ub290 \ud2b9\uc815\ud55c \ubb38\uc11c \ub0b4\uc758 \ud574\ub2f9 \ub2e8\uc5b4\uc758 \uc874\uc7ac\ub294 \uadf8 \ubb38\uc11c\uc5d0 \ub300\ud574 \ud2b9\uc815\ud55c \uc815\ubcf4\ub77c\uace0 \uc81c\uc2dc\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\nSo the second term could be seen as a penalty term that penalizes common words such as \"a\", \"the\", \"and\", etc. tf-idf can therefore, be seen as a weighting scheme for words relevancy in a specific document. <br \/>\n=> \uadf8\ub798\uc11c \ub450\ubc88\uc9f8\ub85c \uc81c\uc57d\uc744 \uac70\ub294\ub370, \ub9ce\uc774 \ub4f1\uc7a5\ud558\ub294 \uc5b8\uc5b4 a,the,and \ub4f1\uc5d0. \uadf8\ub7ec\ubbc0\ub85c tf-idf\ub294 \uc5b4\ub290 \ud2b9\uc815\ud55c \ubb38\uc11c\uc5d0 \ub2e8\uc5b4\ub4e4\uc758 \uc5f0\uad00\uc131\uc5d0 \uac00\uc911\uce58\ub97c \ub450\ub294\uac83\uacfc \uc720\uc0ac\ud569\ub2c8\ub2e4.\n","c45190fb":"\ub610\ud55c \ud6c8\ub828\ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub370\uc774\ud130\ud504\ub808\uc784 \uac01\uac01\uc758 \uceec\ub7fc\ub4e4\uc758 \ud0c0\uc785\uc740 \uc544\ub798\uc640 \uac19\ub2e4.","212d214a":"vz is a tfidf matrix where: <br \/>\n=> tfidf \ud589\ub82c\uc740 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4 <br \/>\n* the number of rows is the total number of descriptions <br \/>\n=> \ud589\uc740 \uc804\uccb4 \uc124\uba85\uc758 \uac1c\uc218 <br \/>\n* the number of columns is the total number of unique tokens across the descriptions <br \/>\n=> \uc5f4\uc740 \uc124\uba85\ub4e4\uc5d0 \uc788\uc5b4\uc11c \uc720\uc77c\ud55c \ud1a0\ud070\uc758 \uac1c\uc218\ub97c \uc758\ubbf8\ud55c\ub2e4","2cb457c3":"## \ucc28\uc6d0 \ucd95\uc18c\ub97c \ud558\ub294 \uc774\uc720?\n### **1. \uc2dc\uac01\ud654**\n#### 2. \ub370\uc774\ud130 \uc555\ucd95\n#### 3. \ucd94\uac00\uc801 \ucc98\ub9ac\n<br \/>\n\n\ucc28\uc6d0\ucd95\uc18c \ubc29\ubc95: **\uc8fc\uc131\ubd84\ubd84\uc11d(PCA), \ube44\uc74c\uc218 \ud589\ub82c \ubd84\ud574(NMF), t-SNE**","98f24d7f":"## Shipping <br \/>\nThe shipping cost burden is decently splitted between sellers and buyers with more than half of the items' shipping fees are paid by the sellers (55%). <br \/>\n\ubd80\ub2f4\ud558\ub294 \ubc30\uc1a1\ube44\ub294 \ud310\ub9e4\uc790\uc640 \uad6c\ub9e4\uc790\uac00 \uac70\uc758 \ubc18\ubc18\uc73c\ub85c \ub098\ub204\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4. <br \/>\n\nIn addition, the average price paid by users who have to pay for shipping fees is lower than those that don't require additional shipping cost. <br \/>\n=> \uac8c\ub2e4\uac00 \ubc30\uc1a1\ube44\ub85c \uc0ac\uc6a9\uc790\uac00 \uc9c0\ubd88\ud574\uc57c\ud558\ub294 \ub3c8\uc758 \ud3c9\uade0\uc740 \ucd94\uac00\uc801\uc778 \ubc30\uc1a1\ube44\ub97c \ud544\uc694\ub85c\ud558\uc9c0 \uc54a\ub294 \uac83\ubcf4\ub2e4 \ub0ae\uc558\uc2b5\ub2c8\ub2e4. <br \/>\nThis matches with our perception that the sellers need a lower price to compensate for the additional shipping. <br \/>\n=> \uc774\uc5d0 \ub530\ub974\uba74 \ud310\ub9e4\uc790\ub4e4\uc740 \ub0ae\uc740 \uac00\uaca9\uc744 \ubc30\uc1a1\ube44\ub97c \ucd94\uac00\ud568\uc73c\ub85c\uc368 \ubb34\ub9c8\ud558\ub824\ub294 \uac83 \uac19\uc2b5\ub2c8\ub2e4. <br \/>","175f2574":"Given the high dimension of our tfidf matrix, we need to reduce their dimension using the Singular Value Decomposition (SVD) technique. <br \/>\n=> \uc8fc\uc5b4\uc9c4 \uace0\ucc28\uc6d0\uc758 tfidf \ud589\ub82c\uc744 \uac00\uc9c0\uace0, \uc6b0\ub9ac\ub294 SVD\ub97c \ud65c\uc6a9\ud558\uc5ec \uadf8 \ucc28\uc6d0\uc744 \ucd95\uc18c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. <br \/>\nAnd to visualize our vocabulary, we could next use t-SNE to reduce the dimension from 50 to 2. t-SNE is more suitable for dimensionality reduction to 2 or 3. <br \/> \n=> \uadf8\ub9ac\uace0 \uc6b0\ub9ac\uc758 \ub2e8\uc5b4\ub4e4\uc744 \uc2dc\uac01\ud654\ud558\uace0, t-SNE\ub97c 50\ucc28\uc6d0\uc5d0\uc11c 2\ucc28\uc6d0\uc73c\ub85c \uc904\uc774\uae30 \uc704\ud574 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. t-SNE\ub294 2\ucc28\uc6d0 \ub610\ub294 3\ucc28\uc6d0\uc73c\ub85c \uc904\uc774\ub294\ub370 \uac00\uc7a5 \uc801\uc808\ud55c \ubc29\ubc95\uc785\ub2c8\ub2e4.\n\n### **t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n\nt-SNE is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. <br \/>\n=>t-SNE\ub294 \ud2b9\ud788 \uace0\ucc28\uc6d0\uc758 \ub370\uc774\ud130\uc14b\uc744 \uc2dc\uac01\ud654\ud558\ub294\ub370 \uc798 \ub9de\ub294 \ucc28\uc6d0 \ucd95\uc18c \uae30\uc220\uc785\ub2c8\ub2e4 <br \/>\nThe goal is to take a set of points in a high-dimensional space and find a representation of those points in a lower-dimensional space, typically the 2D plane. <br \/>\n=> \ubaa9\ud45c\ub294 \uace0\ucc28\uc6d0\uacf5\uac04\uc5d0 \uc788\ub294 \uc810\ub4e4\uc758 \uc9d1\ud569\uc744 \uac00\uc9c0\uace0 \ud2b9\ud788 2\ucc28\uc6d0 \uacf5\uac04\uc5d0 \uc788\ub294 \uac83\uacfc \uac19\uc740 \uc800\ucc28\uc6d0\uc758 \uc810\ub4e4\uc758 \ud45c\ud604\ubc95\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4. <br \/>\nIt is based on probability distributions with random walk on neighborhood graphs to find the structure within the data. <br \/>\n=> \uc774\ub294 neighborhood \uadf8\ub798\ud504\uc5d0\uc11c random walk\ub97c \uac00\uc9c0\uace0 \ub370\uc774\ud130\uc758 \uad6c\uc870\ub97c \ucc3e\uae30 \uc704\ud55c \ud655\ub960 \ubd84\ud3ec\ub97c \uae30\ucd08\ub85c \ud569\ub2c8\ub2e4. <br \/>\nBut since t-SNE complexity is significantly high, usually we'd use other high-dimension reduction techniques before applying t-SNE. <br \/>\n=>\uadf8\ub7ec\ub098 t-SNE\uc758 \ubcf5\uc7a1\ub3c4\uac00 \uc0c1\ub2f9\ud788 \ub192\ub2e4\uba74, t-SNE\ub97c \uc801\uc6a9\ud558\uae30\uc804\uc5d0 \ub2e4\ub978 \uace0\ucc28\uc6d0 \ucd95\uc18c \uae30\uc220\ub4e4\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. <br \/>\n\nFirst, let's take a sample from the both training and testing item's description since t-SNE can take a very long time to execute.<br \/>\n=> \uc77c\ub2e8\uc740 \ud6c8\ub828\uacfc \ud14c\uc2a4\ud2b8\uc14b\uc758 \uc544\uc774\ud15c \uc124\uba85\uc5d0\uc11c \uc0d8\ud50c\uc744 \uac00\uc838\uc640\ubd05\uc2dc\ub2e4. \uc65c\ub0d0\ud558\uba74 t-SNE\ub294 \uc2e4\ud589\uc5d0 \uc624\ub798\uac78\ub9ac\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.<br \/>\nWe can then reduce the dimension of each vector from to n_components (50) using SVD. <br \/>\n=> \uc774\ud6c4\uc5d0 SVD\ub97c \uc774\uc6a9\ud558\uc5ec \ucc28\uc6d0\uc744 \uc904\uc5ec\ubcfc \uac83\uc785\ub2c8\ub2e4.\n","e8b73138":"Below is the 10 tokens with the highest tfidf score, which includes words that are a lot specific that by looking at them, we could guess the categories that they belong to:  <br \/>\n=> tfidf \uc810\uc218\uac00 \ub192\uc740 10\uac1c\uc758 \ud1a0\ud070\uc740 \ubb3c\ud488\ub4e4\uc758 \uc124\uba85\ud558\ub294\ub370 \uc788\uc5b4\uc11c \ud2b9\uc9d5\uc801\uc778 \uac83\ub4e4\uc774 \ub9ce\uc2b5\ub2c8\ub2e4.","ff9da759":"Below is the 10 tokens with the lowest tfidf score, which is unsurprisingly, very generic words that we could not use to distinguish one description from another. <br \/>\n=> tfidf \uc810\uc218\uac00 \ub0ae\uc740 10\uac1c\uc758 \ud1a0\ud070\ub4e4\uc740 \ub2e4\ub978 \ubb3c\ud488\ub4e4\uc744 \uad6c\ubcc4\ud560\ub9cc\ud55c \uc124\uba85\uc774 \uc544\ub2cc \ub2e8\uc5b4\ub4e4\uc774\uc5c8\uc2b5\ub2c8\ub2e4.","cea92d58":"## Item Category <br \/>\nThere are about 1,287 unique categories but among each of them, we will always see a main\/general category firstly, followed by two more particular subcategories (e.g. Beauty\/Makeup\/Face or Lips). <br \/>\n=> 1287\uac1c\uc758 \uc11c\ub85c\ub2e4\ub978 \uce74\ud14c\uace0\ub9ac\ub4e4\uc774 \uc874\uc7ac\ud55c\ub2e4. \uc6b0\ub9ac\ub294 \uac00\uc7a5 \uc8fc\ub41c \uce74\ud14c\uace0\ub9ac\ub97c \uba3c\uc800 \ubcfc \uc218 \uc788\uc73c\uba70, \ub2e4\uc74c\uc73c\ub85c \ub450 \uac1c\uc758 \ub2e4\ub978 \uce74\ud14c\uace0\ub9ac\ub97c \ub354 \ubcfc \uc218 \uc788\ub2e4. <br \/>\nIn adidition, there are about 6,327 items that do not have a category labels. <br \/>\n=> \uadf8\ub7ec\ub098 6327\uac1c\uc758 \ubb3c\ud488\ub4e4\uc774 \uce74\ud14c\uace0\ub9ac\uac00 \uc874\uc7ac\ud558\uc9c0 \uc54a\uc74c\uc744 \uc54c \uc218 \uc788\ub2e4. <br \/>\n\nLet's split the categories into three different columns. <br \/>\n=> \uc77c\ub2e8 \uce74\ud14c\uace0\ub9ac\ub4e4\uc744 3\uac1c\uc758 \uceec\ub7fc\uc73c\ub85c \ub098\ub204\uc5b4 \ubcf4\uc790 <br \/>\nWe will see later that this information is actually quite important from the seller's point of view and how we handle the missing information in the brand_name column will impact the model's prediction. <br \/>\n=> \ub098\ub204\uc5b4\uc9c4 \uceec\ub7fc\ub4e4\uc740 \ucd94\ud6c4\uc5d0 \ud310\ub9e4\uc790\uc758 \uc785\uc7a5\uc5d0\uc11c \uad49\uc7a5\ud788 \uc911\uc694\ud558\uba70, \ube0c\ub79c\ub4dc \uc774\ub984 \uceec\ub7fc\uc5d0\uc11c \uc190\uc2e4\uac12\uc744 \uc5b4\ub5bb\uac8c \ub2e4\ub8e8\ub294\uc9c0\uac00 \ubaa8\ub378\uc758 \uc608\uce21\uc744 \ub2e4\ub974\uac8c \ud560 \uac83\uc785\ub2c8\ub2e4.","03c84292":"## **Pre-processing:  tokenization**\n\nMost of the time, the first steps of an NLP project is to **\"tokenize\"** your documents, which main purpose is to normalize our texts. The three fundamental stages will usually include: <\/br>\n=> \uc790\uc5f0\uc5b4 \ucc98\ub9ac\uc758 \uccab \ubc88\uc9f8 \ud574\uc57c\ud560 \uc77c\uc740 \ubb38\uc11c\ub97c tokenize\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. (\uc6b0\ub9ac\uc758 \ubb38\uc790\ub4e4\uc744 \uc815\uaddc\ud654\ud558\ub294 \uac83\uc774\uc8e0). \uc774\ub294 \ub300\ubd80\ubd84 \uc138 \uac1c\uc758 \uc808\ucc28\ub85c \ub098\ub204\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4.\n* break the descriptions into sentences and then break the sentences into tokens <br \/>\n=> \uc124\uba85\uc744 \ubb38\uc7a5\ubcc4\ub85c \ub098\ub204\uace0 \uadf8 \ubb38\uc7a5\uc744 \ud1a0\ud070\ud654\ud569\ub2c8\ub2e4 <br \/>\n* remove punctuation and stop words <br \/>\n=> \uac01\uc885 \ub530\uc634\ud45c\ub4f1\uc744 \uc5c6\uc560\uace0 \ub2e8\uc5b4\ub4e4\ub85c \ub9cc\ub4ed\ub2c8\ub2e4 <br \/>\n* lowercase the tokens\n=> \ud1a0\ud070\ub4e4\uc744 \uc18c\ubb38\uc790\ud654 \ud569\ub2c8\ub2e4 <br \/>\n* herein, I will also only consider words that have length equal to or greater than 3 characters <br \/>\n=>\uc5ec\uae30\uc5d0\uc11c\ub294 \uc6b0\ub9ac\ub294 3\uae00\uc790\ubcf4\ub2e4 \ud06c\uac70\ub098 \uac19\uc740 \uae38\uc774\ub97c \uac00\uc9c4 \ub2e8\uc5b4\ub4e4\ub9cc \uace0\ub824\ud558\uba74 \ub429\ub2c8\ub2e4.","1ef1acb9":"# Introduction <br \/>\nThis is an initial Explanatory Data Analysis for the Mercari Price Suggestion Challenge with matplotlib. <br \/>\n=> \uc774 \ucee4\ub110\uc740 \ud574\ub2f9 \ucef4\ud53c\ud2f0\uc158\uc758 \uac00\uc7a5 \uae30\ucd08\uc801\uc778 EDA\ubd84\uc11d\uc785\ub2c8\ub2e4 <br \/>\nbokeh and Plot.ly - a visualization tool that creates beautiful interactive plots and dashboards.<br \/>\n=> boken \uacfc Plot.ly\ub294 \uc2dc\uac01\ud654 \ud234\uc744 \uc880\ub354 \uc0c1\ud638\uc791\uc6a9\uc744 \ub192\uc5ec\uc8fc\ub294 \ud234\uc785\ub2c8\ub2e4. <br \/>\nThe competition is hosted by Mercari, the biggest Japanese community-powered shopping app with the main objective to predict an accurate price that Mercari should suggest to its sellers, given the item's information. <br \/>\n=> \ud574\ub2f9 \ucef4\ud53c\ud2f0\uc158\uc740 \uc77c\ubcf8\uc5d0\uc11c \uac00\uc7a5 \ud070 \uc1fc\ud551\uc571\uc774\uba70 \uc6b0\ub9ac\uc758 \ubaa9\ud45c\ub294 \ud68c\uc0ac\uac00 \ud310\ub9e4\uc790\ub4e4\uc5d0\uac8c \uc544\uc774\ud15c\uc758 \uc815\ubcf4\ub97c \ubc1b\uc558\uc744 \ub54c \uadf8 \uc0c1\ud488\uc758 \uc801\uc808\ud55c \uac00\uaca9\uc744 \uc81c\uc548\ud574 \uc8fc\ub294 \uac83\uc785\ub2c8\ub2e4. <br \/>\n\n## Update: The abundant amount of food from my family's Thanksgiving dinner has really energized me to continue working on this model. <br \/>\nI decided to dive deeper into the NLP analysis and found an amazing tutorial by Ahmed BESBES. <br \/>\n=> Ahmed BESBES \uc758 NLP\ubd84\uc11d\uc744 \ucc38\uace0\ud558\uc5ec \ucd94\uac00\uc791\uc5c5\uc744 \ud588\uc2b5\ub2c8\ub2e4. <br \/>\nThe framework below is based on his source code. <br \/>\n=> \ucd94\uac00\ub41c \uac83\ub4e4\uc740 \uadf8\uc758 \ucf54\ub4dc\ub97c \uae30\ubc18\uc73c\ub85c \uc791\uc131\ud558\uc600\uc2b5\ub2c8\ub2e4. <br \/>\nIt provides guidance on pre-processing documents and machine learning techniques (K-means and LDA) to clustering topics. So that this kernel will be divided into 2 parts: <br \/>\n=> \uadf8\uc758 \ucf54\ub4dc\ub294 \ubb38\uc11c\uc758 \uc804\ucc98\ub9ac\uc640 clustering topic\ub4e4\uc5d0 \uba38\uc2e0\ub7ec\ub2dd \uae30\uc220\ub4e4(K-means \uc640 LDA)\uc758 \uac00\uc774\ub4dc\uac00 \ub420 \uac83\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c \uc774 \ucee4\ub110\uc740 \ud06c\uac8c \ub450\uac00\uc9c0\ub85c \ub098\ub269\ub2c8\ub2e4 <br \/>\n\n### Explanatory Data Analysis <br \/>\n\n### Text Processing <br \/>\n#### 2.1. Tokenizing and tf-idf algorithm <br \/>\n#### 2.2. K-means Clustering <br \/>\n#### 2.3. Latent Dirichlet Allocation (LDA) \/ Topic Modelling <br \/>","7508f42a":"If we look at the most common words by category, we could also see that, size, free and shipping is very commonly used by the sellers, probably with the intention to attract customers, which is contradictory to what we have shown previously that there is little correlation between the two variables price and shipping (or shipping fees do not account for a differentiation in prices). <br \/>\n=> \uce74\ud14c\uace0\ub9ac\uc5d0 \ub530\ub77c \uac00\uc7a5 \uc790\uc8fc\ub098\uc628 \ub2e8\uc5b4\ub97c \ubcf8\ub2e4\uba74, \uc6b0\ub9ac\ub294 size,free \uadf8\ub9ac\uace0 shipping \uc774\ub77c\ub294 \ub2e8\uc5b4\ub4e4\uc774 \ud310\ub9e4\uc790\ub4e4\uc774 \uc18c\ube44\uc790\ub4e4\uc758 \uad00\uc2ec\uc744 \uc5bb\uae30\uc704\ud574 \uc8fc\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uac00\uaca9\uacfc \ubc30\uc1a1\uc758 \uc5f0\uad00\uad00\uacc4\ub294 \uc55e\uc5d0\uc11c \uc0b4\ud3b4\ubcf4\uc558\uc2b5\ub2c8\ub2e4. (\ub610\ub294 \ubc30\uc1a1\ube44\ub294 \uac00\uaca9\uc744 \uacb0\uc815\ud558\ub294\ub370 \uc791\uc6a9\ud558\uc9c0 \uc54a\uc744 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.) <br \/>\nBrand names also played quite an important role - it's one of the most popular in all four categories. <br \/>\n=> \ube0c\ub79c\ub4dc\uc774\ub984\uc740 \uaf64\ub098 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4 - \ub124\uac1c\uc758 \uce74\ud14c\uace0\ub9ac\ub4e4 \uc911\uc5d0 \uac00\uc7a5 \ube48\ubc88\ud55c \uac83\ub4e4\uc911 \ud558\ub098\uc785\ub2c8\ub2e4.","7075acb0":"We also need to check if there are any missing values in the item description (4 observations don't have a description) and l remove those observations from our training set. <br \/>\n=> \uc544\uc774\ud15c \uc124\uba85 \ub780\uc5d0 \ub2e4\ub978 \uc190\uc2e4\uac12\uc774 \uc788\ub294\uc9c0 \ucc3e\uc544\ubd05\uc2dc\ub2e4. 4\uac1c\uc758 \uc190\uc2e4\uac12\uc744 \ucc3e\uc744 \uc218 \uc788\uc5c8\uc73c\uba70 \uc6b0\ub9ac\uc758 \ud6c8\ub828\uc14b\uc5d0\uc11c \uc81c\uc678\ud569\uc2dc\ub2e4."}}