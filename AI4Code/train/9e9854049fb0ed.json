{"cell_type":{"8dc5769f":"code","6f9191ec":"code","b0962ac6":"code","a2abb6eb":"code","07ebe210":"code","c828770d":"code","92cbefa8":"code","2fb4912c":"code","60b4e689":"code","432ecc6b":"code","3b2b3f59":"code","b2899c78":"code","a9385134":"code","0c1e1401":"code","ba16c8c5":"code","bd81d698":"code","7c2015a0":"code","b83b4583":"code","9cd4f39d":"code","770c864c":"code","a368f4bb":"code","c06a763e":"code","a1e9cb99":"code","3a4b0751":"code","65840960":"code","33aa7236":"code","4650fd55":"code","c050a3fe":"code","13f9017b":"code","44eb2fa0":"code","f451638d":"code","b8de347c":"code","c64d2b16":"code","5badb19d":"code","9003d70f":"code","dcb53a5b":"code","a6c1e919":"code","86464552":"code","8733a8a7":"code","78240923":"code","814c7d85":"code","b1c8787f":"code","6f0d9688":"code","2ad08947":"code","412834df":"code","51ef916b":"code","4c2442b7":"code","63928d69":"code","79aba412":"code","4f0d3e9f":"code","7df2cbd9":"code","348b4297":"code","6fa1e6b7":"code","db48e36c":"code","6101462b":"code","2943ae2e":"code","6859c278":"code","ffc3a856":"code","03a70f1d":"code","81def884":"code","91b079a5":"code","031d491b":"code","6a9d5aa4":"code","60cf44b8":"code","2b1d6ac4":"code","698012cc":"code","272f9ef1":"code","6ba8691c":"code","620e6d6b":"markdown","343af0e2":"markdown","4475454d":"markdown","383b09e0":"markdown","a95ab96a":"markdown","f8927932":"markdown","23859523":"markdown","e3c8e1a9":"markdown","87e3fbf8":"markdown","9bff9c0c":"markdown","d3a7903a":"markdown","df5e080a":"markdown","93d51db3":"markdown","b474d589":"markdown","6c0074de":"markdown","2367177c":"markdown","e0b6501a":"markdown","ae7778e3":"markdown","cfa188b1":"markdown","520d4963":"markdown","20daa47d":"markdown","95c7452e":"markdown","e7084d38":"markdown","84fddd47":"markdown","b76d9c8c":"markdown","58752137":"markdown","338b9721":"markdown","e2ce40ee":"markdown","5fc78faa":"markdown","8d4bfbad":"markdown","e68d3ab6":"markdown","765937ba":"markdown","208ea3fa":"markdown","d71f5395":"markdown"},"source":{"8dc5769f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f9191ec":"import seaborn as sns\nfrom numpy import sqrt \nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom keras_tuner import RandomSearch\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint","b0962ac6":"train = pd.read_csv('\/kaggle\/input\/cap44611-assignment-3-data\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/cap44611-assignment-3-data\/eval.csv')\nsample = pd.read_csv(\"\/kaggle\/input\/cap44611-assignment-3-data\/train.csv\")","a2abb6eb":"#Examining the training data\n\npd.set_option('display.max_rows', 10)\npd.set_option('display.max_columns', 10)\n\nprint(\"Train.head:\")\nprint(train.head(), end = '\\n\\n')\nprint(\"Train.info:\")\nprint(train.info(), end = '\\n\\n')\nprint(\"Train.describe:\")\nprint(train.describe())\n","07ebe210":"#Checking for null values\ndf = train[train.columns[~train.isnull().all()]].copy()\ndf.info()","c828770d":"#Examining the test data\n\nprint(\"Test.head:\")\nprint(test.head(), end = '\\n\\n')\nprint(\"Test.info:\")\nprint(test.info(), end = '\\n\\n')\nprint(\"Test.describe:\")\nprint(test.describe(),end = '\\n\\n')\n\n#Checking for null values\ndt = test[test.columns[~test.isnull().all()]].copy()\nprint(df.info())","92cbefa8":"#pd.set_option('display.max_rows', None)\n#pd.set_option('display.max_columns', None)\nx = train.nunique(axis=0).copy()\np = x.where(x <= 20)\np = p.dropna()\nprint(p)","2fb4912c":"new = train[p.index]\nfor col in new:\n    print(new[col].value_counts())","60b4e689":"train.head()","432ecc6b":"y = train[\"Eat\"]\nX = train.drop(columns = ['id','pubchem_id',\"Eat\"], axis=1)\n\"\"\"X = train.drop(columns = ['id','pubchem_id', '1274', '1272', '1269', '1265', '1260', \n                          '1254', '1247', '1239', '1230', '1220', '1209','1197', '1184', \n                          '1170', '1155', '1139', '1122', '1104', '1085',\"Eat\"], axis=1)\"\"\"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)","3b2b3f59":"#Saclling the data\n\"\"\"scaler = MinMaxScaler(feature_range=(-1,1))\nfor column in X.columns:\n    scaler.fit(X[column].values.reshape(-1,1))\n    X[column]=scaler.transform(X[column].values.reshape(-1,1))\"\"\"","b2899c78":"#Defining the number of input features\nn_features = X_train.shape[1]\n\n#Defining optimizers\nadam = tf.keras.optimizers.Adam(learning_rate=0.0001)\nsgd = tf.keras.optimizers.SGD(learning_rate=0.0001)\n\n#Defining the loss\nmse = tf.keras.losses.MeanSquaredError()\n\n#Defining the metrics\nrmse = tf.keras.metrics.RootMeanSquaredError(name='RMSE')\n\n#Defining callbacks\nestopper = EarlyStopping(monitor='loss', patience=15)\nmcheckpoint = ModelCheckpoint(filepath='.\/checkpoint.tf',monitor='val_loss',\n                                                 mode='min',save_best_only=True)\nreducer = ReduceLROnPlateau(monitor='val_rmse', factor=0.1,patience=5, min_lr=0.00001,mode='min')\n\n#Defining validation data\nval_data = (X_test, y_test)\n\n#Normalization layer\nnormalize = tf.keras.layers.Normalization()\nnormalize.adapt(np.array(X_train.copy()))\n\n#Initializing lists\nmodels, scores = [], []","a9385134":"#Defining the model\nmodel1 = Sequential()\nmodel1.add(normalize)\nmodel1.add(Dense(30, activation='selu',input_shape=(n_features,)))\nmodel1.add(Dense(50, activation='selu'))\nmodel1.add(Dense(20, activation='selu'))\nmodel1.add(Dense(1))\n\n\n#Compiling the model\nmodel1.compile(optimizer= adam, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm1 = model1.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror1 = model1.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model1)\nscores.append(error1)","0c1e1401":"print(f'MSE: {error1}, RMSE: {sqrt(error1)}')","ba16c8c5":"plt.figure(figsize=(15,7))\nplt.plot(m1.history['loss'])\nplt.plot(m1.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m1.history['RMSE'])\nplt.plot(m1.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","bd81d698":"#Defining the model\nmodel1_2 = Sequential()\nmodel1_2.add(normalize)\nmodel1_2.add(Dense(30, activation='selu',input_shape=(n_features,)))\nmodel1_2.add(Dense(50, activation='selu'))\nmodel1_2.add(Dense(20, activation='selu'))\nmodel1_2.add(Dense(1))\n\n\n#Compiling the model\nmodel1_2.compile(optimizer= adam, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm1_2 = model1_2.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=val_data, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror1_2 = model1_2.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model1_2)\nscores.append(error1_2)","7c2015a0":"print(f'MSE: {error1_2}, RMSE: {sqrt(error1_2)}')","b83b4583":"plt.figure(figsize=(15,7))\nplt.plot(m1_2.history['loss'])\nplt.plot(m1_2.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m1_2.history['RMSE'])\nplt.plot(m1_2.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","9cd4f39d":"#Defining the model\nmodel1_3 = Sequential()\nmodel1_3.add(normalize)\nmodel1_3.add(Dense(30, activation='selu',input_shape=(n_features,)))\nmodel1_3.add(Dense(50, activation='selu'))\nmodel1_3.add(Dense(20, activation='selu'))\nmodel1_3.add(Dense(1))\n\n\n#Compiling the model\nmodel1_3.compile(optimizer= sgd, loss= mse, metrics=[rmse])\n\n#Fitting the model\nm1_3 = model1_3.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror1_3 = model1_3.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model1_3)\nscores.append(error1_3)","770c864c":"print(f'MSE: {error1_3}, RMSE: {sqrt(error1_3)}')","a368f4bb":"plt.figure(figsize=(15,7))\nplt.plot(m1_3.history['loss'])\nplt.plot(m1_3.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m1_3.history['RMSE'])\nplt.plot(m1_3.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","c06a763e":"#Defining the model\nmodel1_4 = Sequential()\nmodel1_4.add(normalize)\nmodel1_4.add(Dense(30, activation='selu',input_shape=(n_features,)))\nmodel1_4.add(Dense(50, activation='selu'))\nmodel1_4.add(Dense(20, activation='selu'))\nmodel1_4.add(Dense(1))\n\n\n#Compiling the model\nmodel1_4.compile(optimizer= sgd, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm1_4 = model1_4.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=val_data, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror1_4 = model1_4.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model1_4)\nscores.append(error1_4)","a1e9cb99":"print(f'MSE: {error1_4}, RMSE: {sqrt(error1_4)}')","3a4b0751":"plt.figure(figsize=(15,7))\nplt.plot(m1_4.history['loss'])\nplt.plot(m1_4.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m1_4.history['RMSE'])\nplt.plot(m1_4.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","65840960":"#Defining the model\nmodel2 = Sequential()\nmodel2.add(Dense(30, activation='selu',input_shape=(n_features,)))\nmodel2.add(Dense(50, activation='selu'))\nmodel2.add(Dense(80, activation='selu'))\nmodel2.add(Dense(40, activation='selu'))\nmodel2.add(Dense(1))\n\n#Compiling the model\nmodel2.compile(optimizer= adam, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm2 = model2.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror2 = model2.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model2)\nscores.append(error2)","33aa7236":"print(f'MSE: {error2}, RMSE: {sqrt(error2)}')","4650fd55":"plt.figure(figsize=(15,7))\nplt.plot(m2.history['loss'])\nplt.plot(m2.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m2.history['RMSE'])\nplt.plot(m2.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","c050a3fe":"#Defining the model\nmodel2_2 = Sequential()\nmodel2_2.add(Dense(30, activation='selu',input_shape=(n_features,)))\nmodel2_2.add(Dense(50, activation='selu'))\nmodel2_2.add(Dense(80, activation='selu'))\nmodel2_2.add(Dense(40, activation='selu'))\nmodel2_2.add(Dense(1))\n\n#Compiling the model\nmodel2_2.compile(optimizer= adam, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm2_2 = model2_2.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=val_data, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror2_2 = model2_2.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model2_2)\nscores.append(error2_2)","13f9017b":"print(f'MSE: {error2_2}, RMSE: {sqrt(error2_2)}')","44eb2fa0":"plt.figure(figsize=(15,7))\nplt.plot(m2_2.history['loss'])\nplt.plot(m2_2.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m2_2.history['RMSE'])\nplt.plot(m2_2.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","f451638d":"#Defining the model\nmodel2_3 = Sequential()\nmodel2_3.add(Dense(30, activation='selu',input_shape=(n_features,)))\nmodel2_3.add(Dense(50, activation='selu'))\nmodel2_3.add(Dense(80, activation='selu'))\nmodel2_3.add(Dense(40, activation='selu'))\nmodel2_3.add(Dense(1))\n\n#Compiling the model\nmodel2_3.compile(optimizer= sgd, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm2_3 = model2_3.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror2_3 = model2_3.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model2_3)\nscores.append(error2_3)","b8de347c":"print(f'MSE: {error2_3}, RMSE: {sqrt(error2_3)}')","c64d2b16":"plt.figure(figsize=(15,7))\nplt.plot(m2_3.history['loss'])\nplt.plot(m2_3.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m2_3.history['RMSE'])\nplt.plot(m2_3.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","5badb19d":"#Defining the model\nmodel2_4 = Sequential()\nmodel2_4.add(Dense(30, activation='selu',input_shape=(n_features,)))\nmodel2_4.add(Dense(50, activation='selu'))\nmodel2_4.add(Dense(80, activation='selu'))\nmodel2_4.add(Dense(40, activation='selu'))\nmodel2_4.add(Dense(1))\n\n#Compiling the model\nmodel2_4.compile(optimizer= sgd, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm2_4 = model2_4.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=val_data, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror2_4 = model2_4.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model2_4)\nscores.append(error2_4)","9003d70f":"print(f'MSE: {error2_4}, RMSE: {sqrt(error2_4)}')","dcb53a5b":"plt.figure(figsize=(15,7))\nplt.plot(m2_4.history['loss'])\nplt.plot(m2_4.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m2_4.history['RMSE'])\nplt.plot(m2_4.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","a6c1e919":"#Defining the model\nmodel3 = Sequential()\nmodel3.add(normalize)\nmodel3.add(Dense(30, activation='relu',input_shape=(n_features,)))\nmodel3.add(Dense(50, activation='relu'))\nmodel3.add(Dense(20, activation='relu'))\nmodel3.add(Dense(1))\n\n#Compiling the model\nmodel3.compile(optimizer= adam, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm3 = model3.fit(X_train, y_train, epochs=100, batch_size=32,  validation_split=0.2, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror3 = model3.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model3)\nscores.append(error3)","86464552":"print(f'MSE: {error3}, RMSE: {sqrt(error3)}')","8733a8a7":"plt.figure(figsize=(15,7))\nplt.plot(m3.history['loss'])\nplt.plot(m3.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m3.history['RMSE'])\nplt.plot(m3.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","78240923":"#Defining the model\nmodel3_2 = Sequential()\nmodel3_2.add(normalize)\nmodel3_2.add(Dense(30, activation='relu',input_shape=(n_features,)))\nmodel3_2.add(Dense(50, activation='relu'))\nmodel3_2.add(Dense(20, activation='relu'))\nmodel3_2.add(Dense(1))\n\n\n#Compiling the model\nmodel3_2.compile(optimizer= adam, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm3_2 = model3_2.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=val_data, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror3_2 = model3_2.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model3_2)\nscores.append(error3_2)","814c7d85":"print(f'MSE: {error3_2}, RMSE: {sqrt(error3_2)}')","b1c8787f":"plt.figure(figsize=(15,7))\nplt.plot(m3_2.history['loss'])\nplt.plot(m3_2.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m3_2.history['RMSE'])\nplt.plot(m3_2.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","6f0d9688":"#Defining the model\nmodel3_3 = Sequential()\nmodel3_3.add(normalize)\nmodel3_3.add(Dense(30, activation='relu',input_shape=(n_features,)))\nmodel3_3.add(Dense(50, activation='relu'))\nmodel3_3.add(Dense(20, activation='relu'))\nmodel3_3.add(Dense(1))\n\n\n#Compiling the model\nmodel3_3.compile(optimizer= sgd, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm3_3 = model3_3.fit(X_train, y_train, epochs=100, batch_size=32,  validation_split=0.2, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror3_3 = model3_3.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model3_3)\nscores.append(error3_3)","2ad08947":"print(f'MSE: {error3_3}, RMSE: {sqrt(error3_3)}')","412834df":"plt.figure(figsize=(15,7))\nplt.plot(m3_3.history['loss'])\nplt.plot(m3_3.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m3_3.history['RMSE'])\nplt.plot(m3_3.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","51ef916b":"#Defining the model\nmodel3_4 = Sequential()\nmodel3_4.add(normalize)\nmodel3_4.add(Dense(30, activation='relu',input_shape=(n_features,)))\nmodel3_4.add(Dense(50, activation='relu'))\nmodel3_4.add(Dense(20, activation='relu'))\nmodel3_4.add(Dense(1))\n\n\n#Compiling the model\nmodel3_4.compile(optimizer= sgd, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm3_4 = model3_4.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=val_data, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror3_4 = model3_4.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model3_4)\nscores.append(error3_4)","4c2442b7":"print(f'MSE: {error3_4}, RMSE: {sqrt(error3_4)}')","63928d69":"plt.figure(figsize=(15,7))\nplt.plot(m3_4.history['loss'])\nplt.plot(m3_4.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m3_4.history['RMSE'])\nplt.plot(m3_4.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","79aba412":"#Defining the model\nmodel4 = Sequential()\nmodel4.add(Dense(30, activation='relu',input_shape=(n_features,)))\nmodel4.add(Dense(50, activation='relu'))\nmodel4.add(Dense(80, activation='relu'))\nmodel4.add(Dense(40, activation='relu'))\nmodel4.add(Dense(1))\n\n#Compiling the model\nmodel4.compile(optimizer= adam, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm4 = model4.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror4 = model4.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model4)\nscores.append(error4)","4f0d3e9f":"print(f'MSE: {error4}, RMSE: {sqrt(error4)}')","7df2cbd9":"plt.figure(figsize=(15,7))\nplt.plot(m4.history['loss'])\nplt.plot(m4.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m4.history['RMSE'])\nplt.plot(m4.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","348b4297":"#Defining the model\nmodel4_2 = Sequential()\nmodel4_2.add(Dense(30, activation='relu', input_shape=(n_features,)))\nmodel4_2.add(Dense(50, activation='relu'))\nmodel4_2.add(Dense(80, activation='relu'))\nmodel4_2.add(Dense(40, activation='relu'))\nmodel4_2.add(Dense(1))\n\n#Compiling the model\nmodel4_2.compile(optimizer= adam, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm4_2 = model4_2.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=val_data, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror4_2 = model4_2.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model4_2)\nscores.append(error4_2)","6fa1e6b7":"print(f'MSE: {error4_2}, RMSE: {sqrt(error4_2)}')","db48e36c":"plt.figure(figsize=(15,7))\nplt.plot(m4_2.history['loss'])\nplt.plot(m4_2.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m4_2.history['RMSE'])\nplt.plot(m4_2.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","6101462b":"#Defining the model\nmodel4_3 = Sequential()\nmodel4_3.add(Dense(30, activation='relu', input_shape=(n_features,)))\nmodel4_3.add(Dense(50, activation='relu'))\nmodel4_3.add(Dense(80, activation='relu'))\nmodel4_3.add(Dense(40, activation='relu'))\nmodel4_3.add(Dense(1))\n\n#Compiling the model\nmodel4_3.compile(optimizer= sgd, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm4_3 = model4_3.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror4_3 = model4_3.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model4_3)\nscores.append(error4_3)","2943ae2e":"print(f'MSE: {error4_3}, RMSE: {sqrt(error4_3)}')","6859c278":"plt.figure(figsize=(15,7))\nplt.plot(m4_3.history['loss'])\nplt.plot(m4_3.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m4_3.history['RMSE'])\nplt.plot(m4_3.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","ffc3a856":"#Defining the model\nmodel4_4 = Sequential()\nmodel4_4.add(Dense(30, activation='relu', input_shape=(n_features,)))\nmodel4_4.add(Dense(50, activation='relu'))\nmodel4_4.add(Dense(80, activation='relu'))\nmodel4_4.add(Dense(40, activation='relu'))\nmodel4_4.add(Dense(1))\n\n#Compiling the model\nmodel4_4.compile(optimizer= sgd, loss= mse,metrics=[rmse])\n\n#Fitting the model\nm4_4 = model4_4.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=val_data, callbacks=[estopper, reducer])\n\n#Evaluating the model\nerror4_4 = model4_4.evaluate(X_test, y_test, verbose=0)\n\n#Saving the model and score\nmodels.append(model4_4)\nscores.append(error4_4)","03a70f1d":"print(f'MSE: {error4_4}, RMSE: {sqrt(error4_4)}')","81def884":"plt.figure(figsize=(15,7))\nplt.plot(m4_4.history['loss'])\nplt.plot(m4_4.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(m4_4.history['RMSE'])\nplt.plot(m4_4.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","91b079a5":"allScores = pd.DataFrame(scores, columns = ['MSE','RMSE'], index = ['Model 1','Model 1-2',\"Model 1-3\",\"Model 1-4\",\n                                                                   \"Model 2\",\"Model 2-2\",\"Model 2-3\",\"Model 2-4\",\n                                                                   \"Model 3\",\"Model 3-2\", \"Model 3-3\",\"Model 3-4\",\n                                                                   \"Model 4\",\"Model 4-2\",\"Model 4-3\",\"Model 4-4\"])\n","031d491b":"allScores","6a9d5aa4":"#Defining the model\nbest_model1 = Sequential()\nbest_model1.add(Dense(30, activation='selu',input_shape=(n_features,)))\nbest_model1.add(Dense(60, activation='selu'))\nbest_model1.add(Dense(90, activation='selu'))\nbest_model1.add(Dense(40, activation='selu'))\nbest_model1.add(Dense(1))\n\n#Compiling the model\nbest_model1.compile(optimizer= adam, loss= mse,metrics=[rmse])\n\n#Fitting the model\nbm1 = best_model1.fit(X_train, y_train, epochs=500, batch_size=32, validation_data=val_data)\n\n#Evaluating the model\nscore = best_model1.evaluate(X_test, y_test, verbose=0)","60cf44b8":"print(f'MSE: {score}, RMSE: {sqrt(score)}')","2b1d6ac4":"#Evaluating the model accuracy\nplt.figure(figsize=(15,7))\nplt.plot(bm1.history['loss'])\nplt.plot(bm1.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.show()\n\nprint(\"\\n\\n\")\n\nplt.figure(figsize=(15,7))\nplt.plot(bm1.history['RMSE'])\nplt.plot(bm1.history['val_RMSE'])\nplt.title(\"Model RMSE\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['Training RMSE', 'Validation RMSE'])\nplt.show()","698012cc":"\"\"\"#Defining the model\nbest_model2 = Sequential()\nbest_model2.add(Dense(30, activation='selu',input_shape=(n_features,)))\nbest_model2.add(Dense(70, activation='selu'))\nbest_model2.add(Dense(100, activation='selu'))\nbest_model2.add(Dense(50, activation='selu'))\nbest_model2.add(Dense(1))\n\n#Compiling the model\nbest_model2.compile(optimizer= adam, loss= mse,metrics=[rmse])\n\n#Fitting the model\nbest_model2.fit(X_train, y_train, epochs=500, batch_size=16, validation_data=val_data, callbacks=[reducer])\n\n#Evaluating the model\nscore2 = best_model2.evaluate(X_test, y_test, verbose=0)\nprint(f'MSE: {score2}, RMSE: {sqrt(score2)}')\"\"\"","272f9ef1":"X_eval = test.drop(columns = ['id','pubchem_id'])\nprediction = best_model1.predict(X_eval)\nprediction = np.ndarray.flatten(prediction)\nsubmit = pd.DataFrame({'id': test.id, 'Eat': prediction})\nsubmit.to_csv('submission.csv', index=False)\nprint(\"Submission complete!\")","6ba8691c":"print(submit.to_string())","620e6d6b":"# Model 3-4","343af0e2":"# **Pediction Submission**","4475454d":"<font size=4>The first model (best_model1) performed better so I decided to use that model for my submission. <\/font>","383b09e0":"<font size=4>After testing several models I also realized that scaling the data reduced the accuracy of the predictions,so I removed it.<\/font>","a95ab96a":"<font size=4>I then decided to check the frequency of the unique values.<\/font","f8927932":"# **Data Exploration**","23859523":"# Data Transformation","e3c8e1a9":"# Model 2-4","87e3fbf8":"# Model 3-3","9bff9c0c":"# Model 2-3","d3a7903a":"# **Choosing the Best Model**","df5e080a":"<font size=4>I decided to make 4 base models and then make minor tweaks to them.<\/font><br>\n<font size=4>For the gradient descent algorithm I alternaated between SGD and Adam.<\/font><br>\n<font size=4>For validation I alternated between splitting the training set and using test data to evaluate the loss <\/font><br>\n<font size=4>For the activation function I alternated between RELU and SELU.<\/font>","93d51db3":"# Model 2-2","b474d589":"# **Building the Models**","6c0074de":"# Model 1","2367177c":"Looking at the scores it is evident that model1 and model 5 perfomed the best, so I'm going to tweak those two further.","e0b6501a":"# Model 1-3","ae7778e3":"# Model 4-4","cfa188b1":"# Model 3","520d4963":"# Model 2","20daa47d":"<font size=4>Model 2-2 had the best RMSE score so I decided to tweak it a bit and train it further.I tweaked it by:<\/font><br>\n* Adding 10 neurons to the inner two layers\n* Increasing the epoch from 100 to 500\n* Turning off early stopping to ensure the model reached it's full potential\n* Turning off the learning rate reducer","95c7452e":"<font size=4>I then wanted to see how many columns had lower than 20 unique values<\/font>","e7084d38":"<font size=4>After seeing how well the model performed I decided to tweak it a bit more to see if I could get even better results. <\/font>","84fddd47":"<font size=4>I started out by examining the data.<\/font>","b76d9c8c":"# Model 4","58752137":"# Model 1-4","338b9721":"<font size=4>I started out by defining the attributes that I planned on using. <\/font>","e2ce40ee":"Based on my analysis of the data I decided to drop the following columns:\n* id: because it doesn't provide any useful information.\n* pubchem_id: because, it also doesn't provide any useful information.\n* all columns with less than 5 unique values, where the overwhelming majority of the values were 0. ","5fc78faa":"# Model 4-3","8d4bfbad":"# Model 3-2","e68d3ab6":"# Model 1-2","765937ba":"# Model 4-2","208ea3fa":"<font size=4>After testing several models I realized that removing the columns with less than 5 unique values reduced the accuracy of the predictions,so I added then back in.<\/font>","d71f5395":"# **Model Scores**"}}