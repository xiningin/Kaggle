{"cell_type":{"1dfd8020":"code","5fe1d887":"code","4a5aa9bd":"code","08341240":"code","199576c6":"code","c5a7673e":"code","2a3df666":"code","90ecd281":"code","69e4a006":"code","c0e49341":"code","e487dd72":"code","179e9e91":"code","83ae30b0":"code","a2db55d4":"code","b656e6ed":"code","92b07496":"code","7d9cbb0a":"code","750142b5":"code","a2ebf1b5":"code","8de0a6b1":"code","a08286c7":"markdown","7f05f0ea":"markdown","ebdbf209":"markdown","cf4f2859":"markdown","05f06ce0":"markdown","97de0e37":"markdown","10add6da":"markdown"},"source":{"1dfd8020":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5fe1d887":"!ls \/kaggle\/input\/used-car-price-dataset-competition-format\/test_label","4a5aa9bd":"features = pd.read_csv('\/kaggle\/input\/used-car-price-dataset-competition-format\/X_train.csv')\ntargets = pd.read_csv('\/kaggle\/input\/used-car-price-dataset-competition-format\/y_train.csv')\nfeatures_test = pd.read_csv('\/kaggle\/input\/used-car-price-dataset-competition-format\/X_test.csv')\ntargets_test = pd.read_csv('\/kaggle\/input\/used-car-price-dataset-competition-format\/test_label\/y_test.csv')\n\nfeatures = pd.concat([features, features_test])\ntargets = pd.concat([targets, targets_test])\nprint(features.head())\nprint(targets.head())","08341240":"features_and_targets = features.merge(targets)\nfeatures_and_targets.head()","199576c6":"features_and_targets['brand'].value_counts()","c5a7673e":"features_and_targets.groupby('brand')['price'].mean()","2a3df666":"features_and_targets.groupby('transmission')['price'].mean()","90ecd281":"features_and_targets.groupby('fuelType')['price'].mean()","69e4a006":"def one_hot(df):\n    rows = ['audi', 'bmw', 'ford', 'hyundai', 'mercedes', 'skoda', 'toyota', 'vauxhall', 'vw']\n    df[rows] = pd.get_dummies(df['brand'], prefix=None, prefix_sep=None)\n    df.drop(columns=['brand', 'model'], inplace=True)\n    \n    rows = ['manual', 'automatic', 'semi-auto', 'other']\n    df[rows] = pd.get_dummies(df['transmission'], prefix=None, prefix_sep=None)\n    df.drop(columns=['transmission'], inplace=True)\n    \n    rows = ['diesel', 'petrol', 'hybrid', 'other', 'electric']\n    df[rows] = pd.get_dummies(df['fuelType'], prefix=None, prefix_sep=None)\n    df.drop(columns=['fuelType'], inplace=True)","c0e49341":"one_hot(features_and_targets)","e487dd72":"features_and_targets","179e9e91":"y = features_and_targets['price']\nx = features_and_targets.drop(columns=['price', 'carID'])\nprint(x)","83ae30b0":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler = scaler.fit(x)\nscaled_x = scaler.transform(x)","a2db55d4":"scaled_x.shape","b656e6ed":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV","92b07496":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(scaled_x, y, test_size=0.1, random_state=1)","7d9cbb0a":"# estimator = xgb.XGBRegressor(tree_method='gpu_hist')\n# parameters = {\n#     'max_depth': range(5,7,1),\n#     'n_estimators': range(100, 1000, 100),\n#     'eta': [0.1, 0.3, 0.5]\n# }\n\n# grid_search = GridSearchCV(\n#     estimator=estimator,\n#     param_grid=parameters,\n#     scoring='neg_mean_squared_error',\n#     n_jobs=-1,\n#     cv=5\n# )\n\n# grid_search.fit(x_train, y_train)","750142b5":"# grid_search.best_params_","a2ebf1b5":"from sklearn.model_selection import cross_val_score\n\nmodel = xgb.XGBRegressor(eta=0.1, max_depth=5, n_estimators=500, tree_method='gpu_hist', n_jobs=-1)\nxgscore = cross_val_score(model, x_train, y_train, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\nprint(np.abs(xgscore.mean()))","8de0a6b1":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\n\nmodel.fit(x_train, y_train)\npredictions = model.predict(x_test)\nmaescore = mean_absolute_error(y_test, predictions)\nr2score = r2_score(y_test, predictions)\nprint(\"MAE: \", maescore)\nprint(\"R2: \", r2score)","a08286c7":"Note: This section has been commented out after the first run to save on time.","7f05f0ea":"## Training and testing best model found","ebdbf209":"## One hot encode","cf4f2859":"# XGBoost","05f06ce0":"# Load, Clean Data","97de0e37":"## Grid Search to find best params","10add6da":"## Check over each categorical column for their correlation with price"}}