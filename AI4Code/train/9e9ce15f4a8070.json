{"cell_type":{"5eddf36a":"code","e1a96752":"code","16ee601a":"code","5fe64bfd":"code","1138631e":"code","9443ccdd":"code","49ff67d3":"code","4a98e59c":"code","292dce11":"code","9eb45504":"code","c5757747":"code","82b1ab31":"code","76c6dd2d":"code","d04be074":"code","08cd0d1d":"code","4f05cf4c":"code","c3515c61":"code","46e558a9":"code","b8490484":"code","d2723d80":"code","2dc7b5e5":"code","42a6536b":"code","b8576c44":"code","7b3bcb11":"code","e5be7947":"code","f4931021":"code","c6485cf2":"code","24b7036b":"code","6fa326d5":"code","f3b869ac":"code","df6a7575":"code","f6438cef":"code","64a4bcc6":"code","f0d0d08a":"code","b054c6a8":"code","0be37f8d":"code","f76bccf8":"code","e94f94a9":"code","ac34921b":"code","1b148a26":"code","084d2c13":"code","51659042":"code","d110129d":"code","8a46e20a":"code","26cd79b3":"code","ba349e01":"code","05d85103":"code","f4efcd8b":"code","905a5950":"code","8bec662b":"code","eef10e00":"code","8c414beb":"code","f94a73b3":"code","ce3a181f":"code","db9e9798":"code","0894eafc":"code","b7baf0f8":"code","6ab5acd2":"code","543cabe3":"code","e1febc71":"code","23d17402":"code","a1d15770":"code","229c21c6":"markdown","c444c697":"markdown","d97be1a0":"markdown","91405a8c":"markdown","c1f553f6":"markdown"},"source":{"5eddf36a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\"\"\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e1a96752":"import pandas as pd\nsample_submission = pd.read_csv(\"..\/input\/who-wins-the-big-game\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/who-wins-the-big-game\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/who-wins-the-big-game\/train.csv\")","16ee601a":"train.head(10)","5fe64bfd":"train.info()","1138631e":"train.describe()","9443ccdd":"new_train=train.loc[:, train.columns != 'ID']","49ff67d3":"new_train['Team_Value'].unique()","4a98e59c":"class_map = {\n    'Less_Than_Four_Billion': 0,\n    'Above_Four_Billion': 1,\n    'Less_Than_Three_Billion': 2,\n}\nnew_train['Team_Value'] = new_train['Team_Value'].map(class_map)","292dce11":"new_train['Playing_Style'].unique()","9eb45504":"class_map_playing={\n    'Balanced':0,\n    'Aggressive_Offense':1,\n    'Aggressive_Defense':2,\n    'Relaxed':3\n}\nnew_train['Playing_Style']=new_train['Playing_Style'].map(class_map_playing)","c5757747":"new_train['Number_Of_Injured_Players'].unique()","82b1ab31":"injured_Players_Map ={\n    'five':5,\n    'four':4,\n    'six':6,\n    'three':3,\n    'seven':7,\n    'eight':8,\n    'two':2,\n    'nine':9,\n    'one':1,\n    'ten':10\n}\nnew_train['Number_Of_Injured_Players'] =new_train['Number_Of_Injured_Players'].map(injured_Players_Map)","76c6dd2d":"new_train['Coach_Experience_Level'].unique()","d04be074":"Coach_Experience_Level_map={\n    'Intermediate':1,\n    'Beginner':0,\n    'Advanced':2\n    \n}\nnew_train['Coach_Experience_Level'] = new_train['Coach_Experience_Level'].map(Coach_Experience_Level_map)","08cd0d1d":"new_train['Coach_Experience_Level'].unique()","4f05cf4c":"new_train_target = new_train['Won_Championship']\nnew_train = new_train.drop('Won_Championship',axis=1)\nnew_train.head()","c3515c61":"new_test=test.drop('ID',axis=1)\n","46e558a9":"new_test['Team_Value'].unique()\nnew_test['Team_Value'] = new_test['Team_Value'].map(class_map)","b8490484":"new_test['Playing_Style'].unique()\nnew_test['Playing_Style']=new_test['Playing_Style'].map(class_map_playing)","d2723d80":"new_test['Number_Of_Injured_Players'].unique()","2dc7b5e5":"new_test['Number_Of_Injured_Players'] =new_test['Number_Of_Injured_Players'].map(injured_Players_Map)","42a6536b":"new_test['Coach_Experience_Level'].unique()","b8576c44":"new_test['Coach_Experience_Level'] = new_test['Coach_Experience_Level'].map(Coach_Experience_Level_map)","7b3bcb11":"new_train.info()","e5be7947":"new_train.describe()","f4931021":"print(new_train_target.describe())\ntrain_target=new_train_target","c6485cf2":"new_test.info()","24b7036b":"new_test.describe()","6fa326d5":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nRandomForestRegressor()","f3b869ac":"new_train.values","df6a7575":"from sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import SVR","f6438cef":"scaler = MinMaxScaler(feature_range=(0, 1))\nX = scaler.fit_transform(new_train)","64a4bcc6":"best_svr = SVR(kernel='linear')","f0d0d08a":"new_train","b054c6a8":"from sklearn.model_selection import cross_val_predict\npredictions = cross_val_predict(best_svr, new_train, train_target, cv=10)\n","0be37f8d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(new_train,train_target, test_size=0.20, random_state=314, stratify=train_target)","f76bccf8":"import lightgbm as lgb\n\nlgb_fit_params={\"early_stopping_rounds\":50, \n            \"eval_metric\" : 'binary_logloss', \n            \"eval_set\" : [(X_test,y_test)],\n            'eval_names': ['valid'],\n            'verbose':100\n           }\n\nlgb_params = {'boosting_type': 'rf',\n 'objective': 'binary',\n 'metric': 'binary_logloss',\n 'verbose': 1,\n 'bagging_fraction': 0.8,\n 'bagging_freq': 1,\n 'num_class': 1,\n 'feature_fraction': 0.8,\n 'lambda_l1': 0.01,\n 'lambda_l2': 0.01,\n 'learning_rate': 0.1,\n 'max_bin': 255,\n 'max_depth': 20,\n 'min_data_in_bin': 1,\n 'min_data_in_leaf': 1,\n 'num_leaves': 31}\nlgb_params","e94f94a9":"clf_lgb = lgb.LGBMClassifier(n_estimators=10000, **lgb_params, random_state=123456789, n_jobs=-1)\nclf_lgb.fit(X_train, y_train, **lgb_fit_params)\nclf_lgb.best_iteration_","ac34921b":"clf_lgb_fulldata = lgb.LGBMClassifier(n_estimators=int(clf_lgb.best_iteration_), **lgb_params)\nclf_lgb_fulldata.fit(new_train, train_target)","1b148a26":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\nclf_rf_fulldata=RandomForestClassifier(n_estimators=3000, max_features=0.5)\nclf_rf_fulldata.fit(new_train, train_target)","084d2c13":"predictions = np.mean((clf_lgb_fulldata.predict_proba(new_test), \n                       clf_rf_fulldata.predict_proba(new_test)), axis=0)\npredictions_1 = np.argmax(predictions, axis=1)","51659042":"submission = pd.DataFrame([test['ID'], predictions_1], index=['ID', 'Won_Championship']).T\nsubmission.to_csv('submission-1.csv', index=False)\nsubmission.head()","d110129d":"from sklearn.metrics import auc, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV\n%matplotlib inline\nimport matplotlib.pyplot as plt","8a46e20a":"print('Best parameters found by grid search are:', gridsearch.best_params_)","26cd79b3":"gbm = lgb.LGBMClassifier(learning_rate = 0.15, metric = 'l1', \n                        n_estimators = 60, boosting_type= 'dart',max_depth =10)\n\ngbm.fit(X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        eval_metric=['auc', 'binary_logloss'],\nearly_stopping_rounds=5)","ba349e01":"y_pred = gbm.predict(new_test, num_iteration=gbm.best_iteration_)","05d85103":"submission = pd.DataFrame([test['ID'], y_pred], index=['ID', 'Won_Championship']).T\nsubmission.to_csv('submission-2.csv', index=False)\nsubmission.head()","f4efcd8b":"\nax = lgb.plot_importance(gbm, height = 0.8, \n                         max_num_features = 25, \n                         xlim = (0,300), ylim = (0,9), \n                         figsize = (10,10))\nplt.show()","905a5950":"# For each feature of our dataset, the result of the following\n# code snippet contains numbers of times a feature is used in a model.\nsorted(gbm.feature_importances_,reverse=True)","8bec662b":"# The code below aims to drop  to keep the features that are included in the most important features. \ntemp = 0 \ntotal = sum(gbm.feature_importances_)\nfor feature in sorted(gbm.feature_importances_, reverse=True):\n    temp+=feature\n    print(feature)\n    if temp\/total >= 0.85:\n        print(feature,temp\/total) # stop when we \n        break","eef10e00":"new_test.columns","8c414beb":"new_train_1 = new_train.drop(['Coach_Experience_Level','Playing_Style','Team_Value','Previous_SB_Wins'],axis=1)\nnew_test_1 = new_test.drop(['Coach_Experience_Level','Playing_Style','Team_Value','Previous_SB_Wins'],axis=1)","f94a73b3":"#The above means let go of all variables after PAY_AMT_5\ny_pred_prob = gbm.predict_proba(X_test)[:, 1]\nauc_roc_0 = str(roc_auc_score(y_test, y_pred_prob)) # store AUC score without dimensionality reduction\nprint('AUC without dimensionality reduction: \\n' + auc_roc_0)","ce3a181f":"# Remake our test\/train set with our reduced dataset\nX_train, X_test, y_train, y_test = train_test_split(new_train_1, train_target, test_size=0.1, random_state=21)\n\nreduc_estimator = lgb.LGBMClassifier(learning_rate = 0.125, metric = 'l1', \n                        n_estimators = 20, num_leaves = 38)\n\n# Parameter grid for hyperparameter tuning\nparam_grid = {\n    'n_estimators': [x for x in range(20, 36, 2)],\n    'learning_rate': [0.10, 0.125, 0.15, 0.175, 0.2]}\n\ngridsearch = GridSearchCV(reduc_estimator, param_grid)\n\ngridsearch.fit(X_train, y_train,\n        eval_set = [(X_test, y_test)],\n        eval_metric = ['auc', 'binary_logloss'],\n        early_stopping_rounds = 5)\nprint('Best parameters found by grid search are:', gridsearch.best_params_)","db9e9798":"print('Best parameters found by grid search are:', gridsearch.best_params_)","0894eafc":"gbm = lgb.LGBMClassifier(learning_rate = 0.175, metric = 'l1', \n                        n_estimators = 30)\ngbm.fit(X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        eval_metric=['auc', 'binary_logloss'],\nearly_stopping_rounds=5)","b7baf0f8":"X_train","6ab5acd2":"y_pred = gbm.predict(new_test_1, num_iteration=gbm.best_iteration_)","543cabe3":"submission = pd.DataFrame([test['ID'], y_pred], index=['ID', 'Won_Championship']).T\nsubmission.to_csv('submission-9.csv', index=False)\nsubmission.head()","e1febc71":"y_pred_prob = gbm.predict_proba(X_test)[:, 1]","23d17402":"y_pred_prob","a1d15770":"fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for who will win the match')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.grid(True)","229c21c6":"## HackerEarth Machine Learning Competition.\nThis is a note book which is made as a solution for HackerEarth Ml competition['Who wins the Big Game']","c444c697":"Feature Importances Graph","d97be1a0":"Dimensionality reduction using feature importances\n","91405a8c":"LightGBM classifier hyperparameter optimization via scikit-learn's GridSearchCV\n","c1f553f6":"LightGBM Hyperparameters + early stopping"}}