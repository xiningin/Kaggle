{"cell_type":{"d6cb0380":"code","bd764732":"code","6fa2f7f4":"code","c3678159":"code","0c0264a0":"code","4ac633f5":"code","f505c168":"code","d62d2d37":"code","2aad3038":"code","167aeea9":"code","94d2482d":"code","00d632c6":"code","2540cca0":"code","c568e0e2":"code","0bead827":"code","92755fe5":"code","c81f9cd0":"code","0563af94":"code","2d77a5a5":"code","b0f90677":"code","93d66495":"code","48f77427":"code","3b91468e":"code","1e666ebe":"code","9c96abf3":"code","d0684628":"code","a8935232":"code","9933d162":"code","ae08ac53":"code","bb6b5dad":"code","8a13ecd0":"code","53b8aef9":"markdown","71a8b77a":"markdown","893ac7e0":"markdown","88bc279e":"markdown","5aac8d72":"markdown","cc2f69aa":"markdown","cac37ebb":"markdown","f5369667":"markdown","577ca7a4":"markdown","880eb410":"markdown","00033ce0":"markdown","aec15116":"markdown","b3f28a2c":"markdown","42efedeb":"markdown","077f1953":"markdown","bda4d8ef":"markdown","4f2fc0e1":"markdown","1973c373":"markdown"},"source":{"d6cb0380":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # basic plots\nimport seaborn as sns # advanced plots\n\nfrom sklearn.model_selection import train_test_split #split the data\nfrom sklearn.preprocessing import StandardScaler #scale the data\nfrom sklearn.neighbors import KNeighborsClassifier #The KNN model\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score #Evaluation metrics \nfrom sklearn.pipeline import Pipeline #Sikit learn pipline \nfrom sklearn.model_selection import GridSearchCV #cross validation","bd764732":"# reading the data and displaying the first 5 rows\ndf = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndf.head()","6fa2f7f4":"# checking if features are in the right type\ndf.info()","c3678159":"# any duplicates? \ndf[df.duplicated() == True]","0c0264a0":"# drop doplicated row\ndf1 = df.drop_duplicates()","4ac633f5":"# re-check: any duplicates? \ndf1[df1.duplicated() == True]","f505c168":"# any nulls? \nnulls = df.isna().sum() #count null values in each column\ndf_nulls = pd.DataFrame(nulls) # convert the result into a dataframe\ndf_nulls.transpose() # transpose the dataframe and print the result","d62d2d37":"# Any outliers?\nint_vars = df1[[\"age\", \"trestbps\", \"chol\", \"thalach\", \"target\"]]\nsns.pairplot(int_vars, hue = \"target\")\nplt.show()","2aad3038":"# Any outliers?\ncat_vars = [\"oldpeak\", \"ca\", \"thal\"]\nplt.figure(figsize = (8, 4), dpi = 100)\nsns.boxplot(data = df1, y = \"oldpeak\", x = \"target\")\nplt.show()","167aeea9":"#box plots\nfig, axes = plt.subplots(2, 2, figsize=(10,5), dpi = 100)\n\n#Mean Sepal Length\nsns.boxplot(ax = axes[0,0], data = df1, y = 'oldpeak')\naxes[0,0].set_xlabel(None)\naxes[0,0].set_ylabel(None)\naxes[0,0].set_title(\"oldpeak\")\n\n\n#Mean Sepal Width\nsns.boxplot(ax = axes[0,1], data = df1, y = 'ca')\naxes[0,1].set_xlabel(None)\naxes[0,1].set_ylabel(None)\naxes[0,1].set_title(\"ca\")\n\n#Mean Petal Length\nsns.boxplot(ax = axes[1,0], data = df1, y = 'thal')\naxes[1,0].set_xlabel(None)\naxes[1,0].set_ylabel(None)\naxes[1,0].set_title(\"thal\")\n\n#Mean Petal Width\nsns.boxplot(ax = axes[1,1], data = df1, y = 'slope')\naxes[1,1].set_xlabel(None)\naxes[1,1].set_ylabel(None)\naxes[1,1].set_title(\"slope\")\n\nplt.tight_layout()\nplt.subplots_adjust(hspace=0.5)","94d2482d":"# detect outlier: chol > 500\ndf1[df1[\"chol\"] > 500]","00d632c6":"# drop outlier: chol > 500\nout_index = df1[df1[\"chol\"] > 500].index[0]\ndf1 = df1.drop(out_index, axis = 0)","2540cca0":"# check if it is already dropped: chol > 500\ndf1[df1[\"chol\"] > 500]","c568e0e2":"# detect outlier: (oldpeak > 4) & (ca > 2)\ndf1[(df1[\"oldpeak\"] > 4) & (df1[\"ca\"] > 2)]","0bead827":"# drop outlier: (oldpeak > 4) & (ca > 2)\nfor index in [204, 250, 291]:\n    df1 = df1.drop(index, axis = 0)","92755fe5":"# check if it is already dropped: (oldpeak > 4) & (ca > 2)\ndf1[(df1[\"oldpeak\"] > 4) & (df1[\"ca\"] > 2)]","c81f9cd0":"# number of dropped rows\ndf.shape[0] - df1.shape[0]","0563af94":"# Intiate the scaler\nscaler = StandardScaler()\n\n# Intiate the model\nknn = KNeighborsClassifier()\n\n# train test split\nX = df1.drop(\"target\", axis = 1)\ny = df1[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","2d77a5a5":"# Operations in oder \noperations = [('scaler', scaler), ('knn', knn)] #Notice that they are written in tuples inside a list.\n \n# set up pipline\npipe = Pipeline(operations) #Notice: it is written with Capital P","b0f90677":"# Here are the paramaters that can be modified in KNN classifier \nknn.get_params().keys()  ","93d66495":"# we will only modify the 'n_neighbors'\nk_values = list(range(1,20))\nk_values","48f77427":"# setting the parameter grid\nparam_grid = {'n_neighbors': k_values}","3b91468e":"# setting the parameter grid\nparam_grid = {'knn__n_neighbors': k_values} # we can add any other parameters to be tuned","1e666ebe":"# Putting all of it together \nfull_cv_classifier = GridSearchCV(pipe,param_grid,cv=5,scoring='accuracy')\n\n# Fitting the pipline \nfull_cv_classifier.fit(X_train,y_train)","9c96abf3":"# Model best parameters\nfull_cv_classifier.best_estimator_.get_params()","d0684628":"# printing the accuracy associated with each k\nacc = full_cv_classifier.cv_results_['mean_test_score']\nk_acc = pd.DataFrame({'k_values': k_values, 'Accuracy': acc})\nk_acc = k_acc.set_index(\"k_values\").transpose()\nround(k_acc, 2)","a8935232":"# initiate and set the operations\nscaler = StandardScaler()\nknn6 = KNeighborsClassifier(n_neighbors=6)\noperations = [('scaler',scaler),('knn6',knn6)]","9933d162":"# set up pipline\npipe = Pipeline(operations)","ae08ac53":"# fit the pipline \npipe.fit(X_train,y_train)","bb6b5dad":"# predict on the test set\npipe_pred = pipe.predict(X_test)","8a13ecd0":"# print the classification report \nprint(classification_report(y_test,pipe_pred))","53b8aef9":"<h2 style=\"background-color:#f15a39; padding: 20px; font-family:cursive\"> Read the data and run quality checks <\/h2>","71a8b77a":"**Follow along very carefully here! We use very specific string codes AND variable names here so that everything matches up correctly. This is not a case where you can easily swap out variable names for whatever you want!**\n\nWe'll use a Pipeline object to set up a workflow of operations:\n\n1. Scale Data\n2. Create Model on Scaled Data\n\n*How does the Scaler work inside a Pipeline with CV? Is scikit-learn \"smart\" enough to understand .fit() on train vs .transform() on train and test?**\n\n**Yes! Scikit-Learn's pipeline is well suited for this! [Full Info in Documentation](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#standardization-or-mean-removal-and-variance-scaling) **\n\nWhen you use the StandardScaler as a step inside a Pipeline then scikit-learn will internally do the job for you.\n\nWhat happens can be discribed as follows:\n\n* Step 0: The data are split into TRAINING data and TEST data according to the cv parameter that you specified in the GridSearchCV.\n* Step 1: the scaler is fitted on the TRAINING data\n* Step 2: the scaler transforms TRAINING data\n* Step 3: the models are fitted\/trained using the transformed TRAINING data\n* Step 4: the scaler is used to transform the TEST data\n* Step 5: the trained models predict using the transformed TEST data","893ac7e0":"[Pipelines](https:\/\/algorithmia.com\/blog\/ml-pipeline) have been growing in popularity, and now they are everywhere you turn in data science, ranging from simple data pipelines to complex machine learning pipelines. The overarching purpose of a pipeline is to streamline processes in data analytics and machine learning.","88bc279e":"1. The data has no nulls\n2. All feature are in the right type\n3. There was one duplicate row and we dropped it\n4. It has some outliers: \n   - chol > 500\n   - oldpeak > 4\n   - ca > 2\n   - thal < 1\n\nIn what follows I will detect and remove those outliers","5aac8d72":"### Setup the pipline ","cc2f69aa":"[The key benefit](https:\/\/www.oreilly.com\/library\/view\/building-machine-learning\/9781492053187\/ch01.html) of machine learning pipelines lies in the automation of the model life cycle steps. When new training data becomes available, a workflow which includes data validation, preprocessing, model training, analysis, and deployment should be triggered. We have observed too many data science teams manually going through these steps, which is costly and also a source of errors.\n\nLet\u2019s cover some details of the benefits of machine learning pipelines:\n1. Ability to focus on new models, not maintaining existing models. Many data scientists spending their days on keeping previously developed models up to date. They run scripts manually to preprocess their training data, they write one-off deployment scripts, or they manually tune their models. Automated pipelines allow data scientists to develop new models, the fun part of their job. Ultimately, this will lead to higher job satisfaction and retention in a competitive job market.\n\n2. Prevention of bugs. In manual machine learning workflows, a common source of bugs is a change in the preprocessing step after a model was trained. In this case, we would deploy a model with different processing instructions than what we trained the model with. These bugs might be really difficult to debug since an inference of the model is still possible, but simply incorrect. With automated workflows, these errors can be prevented.\n\n3. Standardization: standardized machine learning pipelines improve the experience of a data science team. Due to the standardized setups, data scientists can be onboarded quickly or move across teams and find the same development environments. This improves efficiency and reduces the time spent getting set up on a new project. The time investment of setting up machine learning pipelines can also lead to an improved retention rate.\n\nIn this first notebook I will explain how to build a simple machine learning pipeline. In the upcoming notebooks I will start building on it to develop much more complex pipelines step by step.","cac37ebb":"<h2 style=\"background-color:#f15a39; padding: 20px; font-family:cursive\">  Building a machine learning pipline from scratch<\/h2>","f5369667":"### Final Model\n\nWe just saw that our  GridSearch recommends a K=6. Let's now use the PipeLine again, but this time, no need to do a grid search, instead we will evaluate on our hold-out Test Set.","577ca7a4":"I initiated the scaler and the model, this is obvious but what operations and pipline mean? It simply specifies the tasks that will be executed inside pipline one by one. First the pipline will scale the data and then will fit the model, those are our two main operations and they will be executed in order.","880eb410":"Notice the naming convention, we used the name of the operation and then two unerscores and then the name of the parameter. \n\n*In general: If your parameter grid is going inside a PipeLine, your parameter name needs to be specified in the following manner:**\n\n* chosen_string_name + **two** underscores + parameter key name\n* model_name + __ + parameter name\n* knn_model + __ + n_neighbors\n* knn_model__n_neighbors\n\n[StackOverflow on this](https:\/\/stackoverflow.com\/questions\/41899132\/invalid-parameter-for-sklearn-estimator-pipeline)\n\nThe reason we have to do this is because it let's scikit-learn know what operation in the pipeline these parameters are related to (otherwise it might think n_neighbors was a parameter in the scaler).\n\n---","00033ce0":"<h2 style=\"background-color:#f15a39; padding: 20px; font-family:cursive\"> Result of the quality checks <\/h2>","aec15116":"The best performance is associated with 6 neighbors","b3f28a2c":">Now our data is free of errors and ready to build our machine learning pipline ","42efedeb":"### Hyper parameter tuning","077f1953":"### Congrats, you made it to the end of the notebook. Hope you found it useful!","bda4d8ef":"<h2 style=\"background-color:#f15a39; padding: 20px; font-family:cursive\"> Package imports <\/h2>","4f2fc0e1":"<h2 style=\"background-color:#f15a39; padding: 20px; font-family:cursive\"> Why Machine Learning Pipelines? <\/h2>","1973c373":"The way I have just written the parameter grid is the standard way where there is no pipline used. It is no longer valid with the existence of a pipline because the pipline has two operations (scale the data and fit the model) and GridSearchCV does not actually know if 'n_neighbors' goes with the scaler or the model. So we will use the name of the intended operation inside the parametter grid as follows."}}