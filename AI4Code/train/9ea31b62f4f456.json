{"cell_type":{"c442b2b9":"code","2a7aedd7":"code","089f4250":"code","2a04e605":"code","95d131af":"code","90e06625":"code","094f6ba0":"markdown"},"source":{"c442b2b9":"from sklearn.preprocessing import StandardScaler\nfrom sklearn import ensemble, gaussian_process, neighbors, svm, model_selection\nfrom xgboost import XGBRegressor \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport time\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nimport pickle\n# %% Helpful Functions\nfrom sklearn.base import TransformerMixin\n","2a7aedd7":"\n\nclass CustomImputer(TransformerMixin):\n\n    def __init__(self):\n        \"\"\"Impute missing values.\n\n        Columns of dtype object are imputed with the most frequent value \n        in column.\n\n        Columns of other types are imputed with median of column.\n\n        \"\"\"\n    def fit(self, X, excluded_features=[], y=None):\n\n        fill_series = pd.Series([X[c].value_counts().index[0] if X[c].dtype == np.dtype('O') else X[c].median() for c in X], index=X.columns)\n        \n        # Remove excluded features (i.e. ones that you want to not impute values for).\n        fill_series.drop(index=excluded_features, inplace=True)\n        \n        self.fill = fill_series\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n    \ndef count_missing_data(df):\n    \"\"\"Obviously doesn't know if something should in fact be NA as opposed to a missing value.\"\"\"\n    print(df.isnull().sum())\n    \ndef merge_duplicate_features(df, dup_features):\n    \"\"\"\n    This was useful for the housing dataset because there were features like Condition1 and Condition2 with \n    identical options but entered in separately. When doing one-hot encoding, we can combine them into a joint\n    \"\"\"\n    for [c1,c2, joint_name] in dup_features:\n        \n        c1_endings = [col.replace(c1+\"_\",\"\") for col in df.columns if c1+\"_\" in col]\n        c2_endings = [col.replace(c2+\"_\",\"\") for col in df.columns if c2+\"_\" in col]\n        \n        # Get unique ending values by converting to dict and back\n        all_endings = list(dict.fromkeys(c1_endings+c2_endings))\n        \n        for end in all_endings:\n            \n            c1_with_end = c1+\"_\"+end\n            c2_with_end = c2+\"_\"+end\n            joint_with_end = joint_name+\"_\"+end\n            \n            if c1_with_end in df.columns and c2_with_end in df.columns:\n                c1_feature = df[c1_with_end]\n                c2_feature = df[c2_with_end]\n                joint_feature = np.logical_or(c1_feature,c2_feature)\n                df[joint_with_end] = joint_feature\n                df.drop(columns=[c1_with_end, c2_with_end], inplace=True)\n            elif c1_with_end in df.columns and c2_with_end not in df.columns:\n                c1_feature = df[c1_with_end]\n                joint_feature = c1_feature\n                df[joint_with_end] = joint_feature\n                df.drop(columns=[c1_with_end], inplace=True)\n            elif c1_with_end not in df.columns and c2_with_end in df.columns:\n                c2_feature = df[c2_with_end]\n                joint_feature = c2_feature\n                df[joint_with_end] = joint_feature\n                df.drop(columns=c2_with_end, inplace=True)\n            else:\n                print(\"This should never occur; check code.\") \n\ndef compare_MLA(X, X_target, vote_est):\n    \"\"\"\n    Make a quick table showing model performance without hyperparameter tuning.\n    \"\"\"\n    MLA_list = [alg[1] for alg in vote_est]\n\n    cv_split = model_selection.KFold(n_splits = 10)\n\n    #create table to compare MLA metrics\n    MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n    MLA_compare = pd.DataFrame(columns = MLA_columns)\n\n    #create table to compare MLA predictions\n    #MLA_predict = X_target.copy()\n\n    #index through MLA and save performance to table\n    row_index = 0\n    for alg in MLA_list:\n\n        #set name and parameters\n        MLA_name = alg.__class__.__name__\n        print(MLA_name)\n        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n        MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n        \n        #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n        cv_results = model_selection.cross_validate(alg, X, X_target, cv  = cv_split, scoring='neg_mean_squared_error', return_train_score = True, n_jobs=-1)\n\n        MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n        MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = np.sqrt(-cv_results['train_score']).mean()\n        MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = np.sqrt(-cv_results['test_score']).mean()   \n        #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n        MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n        \n\n        #save MLA predictions - see section 6 for usage\n        #alg.fit(X, X_target)\n        #MLA_predict[MLA_name] = alg.predict(X)\n        \n        row_index+=1\n\n        \n    #print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\n    #MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n    print(MLA_compare[ ['MLA Name', 'MLA Train Accuracy Mean', 'MLA Test Accuracy Mean']])\n    return MLA_compare\n    #MLA_predict    \n\ndef plot_feature_importances(reg, train_data, y, num_vars):\n    reg.fit(train_data,y)\n    feature_importance = reg.feature_importances_\n    # make importances relative to max importance\n    feature_importance = 100.0 * (feature_importance \/ feature_importance.max())\n    sorted_idx = np.argsort(feature_importance)[-num_vars:]\n    pos = np.arange(sorted_idx.shape[0]) + .5\n    most_important_features = train_data.columns[sorted_idx]\n    plt.barh(pos, feature_importance[sorted_idx], align='center')\n    plt.yticks(pos, most_important_features)\n    plt.xlabel('Relative Importance')\n    plt.title('Top {} Variable Importances'.format(num_vars))\n    plt.show()\n    return most_important_features","089f4250":"\nos.chdir('\/kaggle\/input')\n\n\n\n# Make df show full size\n# pd.set_option('display.max_rows', 100)\n# pd.set_option('display.max_columns', 15)\n# pd.set_option('display.width', None)\n# pd.set_option('display.max_colwidth', -1)\n\n# Gather data\ntrain_data = pd.read_csv(Path(\"house-prices-advanced-regression-techniques\/train.csv\"))\ntest_data = pd.read_csv(Path(\"house-prices-advanced-regression-techniques\/test.csv\"))\ntest_copy = test_data.copy()\n\n# Extract y\ny = train_data['SalePrice']\ny = np.log1p(y) \n\n# Remove unwanted features\nunwanted_features = ['Id']\ntrain_data.drop(columns=unwanted_features, inplace=True)\ntrain_data.drop(columns=\"SalePrice\", inplace=True)\ntest_data.drop(columns=unwanted_features, inplace=True)\n\n# Remove outliers - chosen from features that are in the top 5 most importance to a gradient boosting regressor and have a small number of clear outliers.\ntrain_data = train_data[train_data['BsmtFinSF1'] <= 5000]\ntrain_data = train_data[train_data['TotalBsmtSF'] <= 4000]\ntrain_data = train_data[train_data['GrLivArea'] <= 4000]\ntrain_data = train_data[train_data['LotArea'] <= 1000000]\ny = y[train_data.index]\ntrain_data.reset_index(drop=True, inplace=True)\n\n# Impute median missing values for both numerical and categorical features, except certain excluded features (that have NA as a real category, not NaN)\nexcluded_features = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\nCI = CustomImputer()\ntrain_data = CI.fit_transform(train_data, excluded_features)\ntest_data = CI.transform(test_data)\n\n# Define categorical variables\nmore_cat_vars = ['MSSubClass']\ncat_vars = [f for f in train_data.columns if train_data[f].dtype == np.dtype('O')]\ncat_vars.extend(more_cat_vars)\nnum_vars = [var for var in train_data.columns if var not in cat_vars]\n\n# Transform to dummy variables\ntrain_data = pd.get_dummies(train_data, columns=cat_vars, dummy_na=True) # dummy_na needed!!!\ntest_data = pd.get_dummies(test_data, columns=cat_vars, dummy_na=True)\n\n# Merge duplicate features\ndup_features = [ ['Condition1', 'Condition2', 'ConditionJoint'], ['Exterior1st', 'Exterior2nd', 'ExteriorJoint'], ['BsmtFinType1', 'BsmtFinType2', 'BsmtFinTypeJoint']]\nmerge_duplicate_features(train_data, dup_features)\nmerge_duplicate_features(test_data, dup_features)\n\n# Make sure columns of train_data and test_data match. Can have problems if one dataset does not contain an instance of a feature while the other does. One-hot encoding thus causes mismatches in the encoding. \nmissing_train_features = [f for f in test_data.columns if f not in train_data.columns]\nfor f in missing_train_features: train_data[f] = 0\nmissing_test_features = [f for f in train_data.columns if f not in test_data.columns]\nfor f in missing_test_features: test_data[f] = 0 \ntest_data = test_data[train_data.columns] # Rearrange order of columns to match\n\n# Make boxplots to show outliers of most important variables. \n# most_important_features = plot_feature_importances(ensemble.GradientBoostingRegressor(), train_data, y, 10)\n# for f in most_important_features:\n#     train_data[f].plot.box(vert=False)\n#     plt.show()\n\n\n# Rescale the data\nscaler = StandardScaler()\ntrain_data[train_data.columns] = scaler.fit_transform(train_data)\ntest_data[test_data.columns] = scaler.transform(test_data)\n\n\n# Pick estimators\nvote_est = [\n    \n    # Ensemble\n    ('ada', ensemble.AdaBoostRegressor()),\n    ('br', ensemble.BaggingRegressor()),\n    ('etr',ensemble.ExtraTreesRegressor()),\n    ('gbr', ensemble.GradientBoostingRegressor()),\n    ('rfr', ensemble.RandomForestRegressor()),\n\n    #Gaussian Processes\n    ('gpr', gaussian_process.GaussianProcessRegressor()),\n    \n    #Nearest Neighbor\n    ('knn', neighbors.KNeighborsRegressor()),\n    \n    #SVM\n    ('svr', svm.SVR()),\n\n    #xgboost\n   ('xgb', XGBRegressor())\n]\n\n# Get sense of estimator accuracy\nMLA_compare = compare_MLA(train_data, y, vote_est)\n\ngrid_n_estimator = [10, 33, 100, 333, 1000, 3333, 10000]\ngrid_ratio = [0.1, 0.25, 0.5, 0.75, 1.0]\ngrid_learn = [.01, 0.03, 0.1, 0.33, 1.0]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, 0.03, 0.05, 0.10]\ngrid_criterion = ['mse']\ngrid_bool = [False]\n\ngrid_param = [\n            [{\n            #AdaBoostRegressor - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostRegressor.html\n            'n_estimators': grid_n_estimator, #default=50\n            'learning_rate': grid_learn #default=1\n            }],\n       \n    \n            [{\n            #BaggingRegressor - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor\n            'n_estimators': grid_n_estimator, #default=10\n            'max_samples': grid_ratio #default=1.0\n             }],\n\n    \n            [{\n            #ExtraTreesRegressor \n            'n_estimators': grid_n_estimator, \n            'criterion': grid_criterion, \n            'max_depth': grid_max_depth\n             }],\n\n\n            [{\n            #GradientBoostingRegressor - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor\n            #'loss': ['deviance', 'exponential'], #default=\u2019deviance\u2019\n            'learning_rate': grid_learn,\n            'n_estimators': grid_n_estimator,\n            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=\u201dfriedman_mse\u201d\n            'max_depth': grid_max_depth    \n             }],\n\n    \n            [{\n            #RandomForestRegressor\n            'n_estimators': grid_n_estimator, #default=100\n            'criterion': grid_criterion,\n            'max_depth': grid_max_depth, \n            'oob_score': grid_bool\n             }],\n    \n            [{    \n            #GaussianProcessRegressor\n            'alpha': [1e-11, 1e-10, 1e-9, 1e-8, 1e-7]\n            }],\n    \n            [{\n            #KNeighborsRegressor \n            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n            'weights': ['uniform', 'distance'], #default = \u2018uniform\u2019\n            'algorithm': ['ball_tree', 'kd_tree', 'brute']\n            }],\n            \n    \n            [{\n            #SVR - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC\n            #http:\/\/blog.hackerearth.com\/simple-tutorial-svm-parameter-tuning-python-r\n            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n            'C': [0.01, 0.03, 0.1, 0.33, 1, 3, 10, 33, 100], #default=1.0\n            'gamma': ['auto', 'scale'],\n            'epsilon': [1e-4, 1e-3, 1e-2, 1e-1]\n             }],\n\n    \n            [{\n            #XGBRegressor - http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n            'learning_rate': grid_learn, #default: .3\n            'max_depth': [1,2,4,6,8,10] #default 2\n             }]   \n        ]\n\n\n","2a04e605":"start_total = time.perf_counter() \ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size=0.2)\nfor reg, param in zip (vote_est, grid_param): #https:\/\/docs.python.org\/3\/library\/functions.html#zip\n\n    #if reg[0] in [alg[0] for alg in vote_est[:5]]: continue\n    #print(reg[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n    #print(param)\n    \n    \n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = reg[1], param_grid = param, cv = cv_split, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n    best_search.fit(train_data, y)\n    cv_score = np.sqrt(-best_search.best_score_)\n\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('\\nThe best parameter for {} is \\n{} with a runtime of {:.2f} seconds.\\n This gave a cv score of {}\\n'.format(reg[1].__class__.__name__, best_param, run, cv_score))\n    reg[1].set_params(**best_param) \n\n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total\/60))","95d131af":"os.chdir(\"\/kaggle\/input\/model-params-for-regression\/\")\n","90e06625":"print('-'*10)\n# Save model parameters\npkl_filename = \"model_params.pkl\"\n# with open(pkl_filename, 'wb') as file:\n#     pickle.dump(vote_est, file)\n\n# # Load model parameters\npkl_filename = \"model_params.pkl\"\nwith open(pkl_filename, 'rb') as file:\n    pickle_model = pickle.load(file)\n\n# See accuracy after hyperparameter tuning\nMLA_compare_tuned = compare_MLA(train_data, y, vote_est)\n\nos.chdir(\"\/kaggle\/working\")\n\n# Make submission\nvote_est[3][1].fit(train_data, y)\npredictions = vote_est[3][1].predict(test_data)\npredictions = np.expm1(predictions)\noutput = pd.DataFrame({'Id': test_copy['Id'], 'SalePrice': predictions})\ncsv_name = 'GradientBoosting6_submission.csv'\ncompetition = \"house-prices-advanced-regression-techniques\"\noutput.to_csv(csv_name, index=False)\nprint(\"Your submission was successfully saved!\")\n\n# kaggle.api.competition_submit(csv_name, csv_name, competition)\n# kaggle.api.competitions_submissions_list(competition)[0]\n\nprint('-'*10)\n    \n# Make submission\nvoter_idx = [1,2,3,4,8]\nvoters = [vote_est[i] for i in voter_idx]\nstack = ensemble.StackingRegressor(estimators=voters, cv=10, n_jobs=-1, final_estimator=XGBRegressor())\nstack.fit(train_data, y)\npredictions = stack.predict(test_data)\npredictions = np.expm1(predictions)\noutput = pd.DataFrame({'Id': test_copy['Id'], 'SalePrice': predictions})\ncsv_name = 'Stacked1_submission.csv'\ncompetition = \"house-prices-advanced-regression-techniques\"\noutput.to_csv(csv_name, index=False)\nprint(\"Your submission was successfully saved!\")\n\n# kaggle.api.competition_submit(csv_name, csv_name, competition)\n# kaggle.api.competitions_submissions_list(competition)[0]","094f6ba0":"The goal of this competition was to predict the sale price of houses in Iowa using approximately 80 features (number of rooms, square footage, etc).\n\nThe steps I followed were:\n1. Removing unwanted features ('Id')\n2. Impute missing values (except for certain categorical features in which a missing value was a meaningful value)\n3. One-hot encode categorical features\n4. Combine duplicate categorical features (e.g. Condition1 and Condition2 need to be combined since they carry information that should be grouped)\n5. Scale the data using a standard scaler\n6. I did a hyperparameter search with several machine learning regressor algorithms.\n"}}