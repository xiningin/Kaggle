{"cell_type":{"f06c2924":"code","7b09ac0e":"code","12d87569":"code","380907ab":"code","78dc13c7":"code","ca8d0b9e":"code","0a638ac6":"code","f47e981d":"code","f84689bb":"code","c94552da":"code","67c05887":"code","cda80f54":"code","598333ab":"code","f29b2e2e":"code","272d53f1":"code","010331b9":"code","d1179c7e":"code","203c5c9a":"code","794e1aca":"code","b308bb0b":"code","339ec1b2":"code","8b9a9ec0":"code","751b16f0":"code","a87ae095":"markdown","e9484fa0":"markdown","dae29491":"markdown","7cf0243c":"markdown","3b940903":"markdown","5e871636":"markdown","8827ce85":"markdown","c18d3a71":"markdown","fb175a8f":"markdown","96a70b70":"markdown","5a9d48da":"markdown","d7489912":"markdown","e9962f36":"markdown","26d7a34a":"markdown","bccacccc":"markdown","2725b4fa":"markdown","8e665961":"markdown","3d0706fa":"markdown","0ac2bdb7":"markdown","bcca81dd":"markdown","aa7b59fe":"markdown","8d16a23f":"markdown","e1f4152f":"markdown","e5d9c153":"markdown","06510c00":"markdown","4b157113":"markdown","c15f6339":"markdown","2cfa4189":"markdown","f4ea182e":"markdown","af6ca2e6":"markdown","bb7ecdb9":"markdown","f77d0483":"markdown"},"source":{"f06c2924":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7b09ac0e":"DataFrame = pd.read_csv(\"..\/input\/pulsar_stars.csv\")  ","12d87569":"DataFrame.head()    # first 5 rows of whole columns","380907ab":"DataFrame.info()   # information about data types and amount of non-null rows of our Dataset","78dc13c7":"DataFrame.describe()   # statistical information about our data","ca8d0b9e":"DataFrame.corr()    # correlation between fields","0a638ac6":"import matplotlib.pyplot as plt    # basic plotting library\nimport seaborn as sns              # more advanced visual plotting library","f47e981d":"sns.pairplot(data=DataFrame,\n             palette=\"husl\",\n             hue=\"target_class\",\n             vars=[\" Mean of the integrated profile\",\n                   \" Excess kurtosis of the integrated profile\",\n                   \" Skewness of the integrated profile\",\n                   \" Mean of the DM-SNR curve\",\n                   \" Excess kurtosis of the DM-SNR curve\",\n                   \" Skewness of the DM-SNR curve\"])\n\nplt.suptitle(\"PairPlot of Data Without Std. Dev. Fields\",fontsize=18)\n\nplt.tight_layout()\nplt.show()   # pairplot without standard deviaton fields of data","f84689bb":"plt.figure(figsize=(16,12))\nsns.heatmap(data=DataFrame.corr(),annot=True,cmap=\"bone\",linewidths=1,fmt=\".2f\",linecolor=\"gray\")\nplt.title(\"Correlation Map\",fontsize=20)\nplt.tight_layout()\nplt.show()      # lightest and darkest cells are most correlated ones","c94552da":"plt.figure(figsize=(16,10))\n\nplt.subplot(2,2,1)\nsns.violinplot(data=DataFrame,y=\" Mean of the integrated profile\",x=\"target_class\")\n\nplt.subplot(2,2,2)\nsns.violinplot(data=DataFrame,y=\" Mean of the DM-SNR curve\",x=\"target_class\")\n\nplt.subplot(2,2,3)\nsns.violinplot(data=DataFrame,y=\" Standard deviation of the integrated profile\",x=\"target_class\")\n\nplt.subplot(2,2,4)\nsns.violinplot(data=DataFrame,y=\" Standard deviation of the DM-SNR curve\",x=\"target_class\")\n\n\nplt.suptitle(\"ViolinPlot\",fontsize=20)\n\nplt.show()","67c05887":"labels = DataFrame.target_class.values\n\nDataFrame.drop([\"target_class\"],axis=1,inplace=True)\n\nfeatures = DataFrame.values","cda80f54":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0,1))\n\nfeatures_scaled = scaler.fit_transform(features)","598333ab":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(features_scaled,labels,test_size=0.2)","f29b2e2e":"from sklearn.linear_model import LogisticRegression\nlr_model = LogisticRegression(random_state=42,solver=\"liblinear\",C=1.6,penalty=\"l1\")\n\nlr_model.fit(x_train,y_train)\n\ny_head_lr = lr_model.predict(x_test)\n\nlr_score = lr_model.score(x_test,y_test)","272d53f1":"from sklearn.tree import DecisionTreeClassifier\ndc_model = DecisionTreeClassifier(random_state=42)\n\ndc_model.fit(x_train,y_train)\n\ny_head_dc = dc_model.predict(x_test)\n\ndc_score = dc_model.score(x_test,y_test)","010331b9":"from sklearn.ensemble import RandomForestClassifier\nrfc_model = RandomForestClassifier(n_estimators=37,random_state=42,max_leaf_nodes=200,criterion=\"entropy\")\n\nrfc_model.fit(x_train,y_train)\n\ny_head_rfc = rfc_model.predict(x_test)\n\nrfc_score = rfc_model.score(x_test,y_test)","d1179c7e":"from sklearn.naive_bayes import GaussianNB\nnb_model = GaussianNB()\n\nnb_model.fit(x_train,y_train)\n\ny_head_nb = nb_model.predict(x_test)\n\nnb_score = nb_model.score(x_test,y_test)","203c5c9a":"from sklearn.neighbors import KNeighborsClassifier\nknn_model = KNeighborsClassifier(n_neighbors=7,weights=\"distance\")\n\nknn_model.fit(x_train,y_train)\n\ny_head_knn = knn_model.predict(x_test)\n\nknn_score = knn_model.score(x_test,y_test)","794e1aca":"from sklearn.svm import SVC\nsvm_model = SVC(random_state=42,C=250,gamma=1.6,kernel=\"poly\",probability=True)\n\nsvm_model.fit(x_train,y_train)\n\ny_head_svm = svm_model.predict(x_test)\n\nsvm_score = svm_model.score(x_test,y_test)","b308bb0b":"from sklearn.metrics import confusion_matrix\n\ncm_lr = confusion_matrix(y_test,y_head_lr)\ncm_dc = confusion_matrix(y_test,y_head_dc)\ncm_knn = confusion_matrix(y_test,y_head_knn)\ncm_nb = confusion_matrix(y_test,y_head_nb)\ncm_rfc = confusion_matrix(y_test,y_head_rfc)\ncm_svm = confusion_matrix(y_test,y_head_svm)","339ec1b2":"plt.figure(figsize=(24,12))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_lr,cbar=False,annot=True,cmap=\"CMRmap_r\",fmt=\"d\")\n\nplt.subplot(2,3,2)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm_dc,cbar=False,annot=True,cmap=\"CMRmap_r\",fmt=\"d\")\n\nplt.subplot(2,3,3)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,cbar=False,annot=True,cmap=\"CMRmap_r\",fmt=\"d\")\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm_nb,cbar=False,annot=True,cmap=\"CMRmap_r\",fmt=\"d\")\n\nplt.subplot(2,3,5)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_rfc,cbar=False,annot=True,cmap=\"CMRmap_r\",fmt=\"d\")\n\nplt.subplot(2,3,6)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(cm_svm,cbar=False,annot=True,cmap=\"CMRmap_r\",fmt=\"d\")\n\nplt.show()","8b9a9ec0":"algorithms = (\"Logistic Regression\",\"Decision Tree\",\"Random Forest\",\"K Nearest Neighbors\",\"Naive Bayes\",\"Support Vector Machine\")\nscores = (lr_score,dc_score,rfc_score,knn_score,nb_score,svm_score)\ny_pos = np.arange(1,7)\ncolors = (\"red\",\"gray\",\"purple\",\"green\",\"orange\",\"blue\")\n\nplt.figure(figsize=(24,12))\nplt.xticks(y_pos,algorithms,fontsize=18)\nplt.yticks(np.arange(0.00, 1.01, step=0.01))\nplt.ylim(0.90,1.00)\nplt.bar(y_pos,scores,color=colors)\nplt.grid()\nplt.suptitle(\"Bar Chart Comparison of Models\",fontsize=24)\nplt.show()\n\n","751b16f0":"# thanks for reading. Votes, Comments and Advices are all welcome :) ","a87ae095":"<a id=\"knn\"><\/a>\n### ** K Nearest Neighbors **","e9484fa0":"### Splitting the Feature and Label fields","dae29491":"### ** Bar Chart Comparison  ** ","7cf0243c":"<a id=\"eda\"><\/a>\n## ** Exploratory Data Analysis **","3b940903":"Most of our Columns are already related or derived from one or another. And we can see it clearly on some Cells above","5e871636":"<a id=\"svm\"><\/a>\n### ** Support Vector Machine **\n","8827ce85":"### Looking to Dtypes and amounts of values","c18d3a71":"### Scaling the Features  ","fb175a8f":"# ** Conclusion **","96a70b70":"### ** PairPlot **  (each column is compared the others and itself)","5a9d48da":"(Bonus) data types are all numeric and non-null, I don't need to do any transformations or cleaning.","d7489912":"### ** ViolinPlot **  (act as a boxplot but we can see amounts too)","e9962f36":"we can see that our data is quite separable on most of the columns","26d7a34a":"<a id=\"rf\"><\/a>\n### ** Random Forest Classifier **","bccacccc":"   <a id=\"nb\"><\/a>\n  ### ** Naive Bayes Classifier **","2725b4fa":"<a id=\"dc\"><\/a>\n### ** Decision Tree Classifier **","8e665961":"### ** Correlation HeatMap **","3d0706fa":"if we look at the graph and check the scores, LogisticRegression, RandomForest and SVM are better than the others.","0ac2bdb7":"### Statistical Investigation","bcca81dd":"<a id=\"res\"><\/a>\n## ** Model Evaluating **","aa7b59fe":"### Investigating the Dataset","8d16a23f":"<a id=\"prep\"><\/a>\n## ** Data PreProcessing **","e1f4152f":"### Loading the dataset","e5d9c153":"<a id=\"lr\"><\/a>\n### ** Logistic Regression **","06510c00":"###  Splitting the Train and the Test rows","4b157113":"### ** Confusion Matrix **","c15f6339":"# ** Introduction **\n\n### Hello, in this notebook I will do some experiments to figure out the best Machine Learning algorithm on this data and try to tune them to get highest success.  After tuning I will compare them to find best one of the following 6 models.\n\n### <a href=\"#eda\">EDA<\/a>\n### <a href=\"#visual\">Visual EDA<\/a>\n### <a href=\"#prep\">Data Preprocess<\/a>\n\n### <a href=\"#mls\">Machine Learning Models<\/a>\n*  <a href=\"#lr\"> Logistic Regression<\/a>\n*  <a href=\"#knn\"> KNN<\/a>\n*  <a href=\"#svm\">SVM<\/a>\n*  <a href=\"#rf\">Random Forest<\/a>\n*  <a href=\"#dc\">Decision Tree<\/a>\n*  <a href=\"#nb\">Naive Bayes<\/a>\n\n### <a href=\"#res\">Results and Comparisons<\/a>","2cfa4189":"## ** Machine Learning Models **\n<a id=\"mls\"><\/a>","f4ea182e":"<a id=\"visual\"><\/a>\n## ** Visual EDA **","af6ca2e6":"We can see that our data has different kind of distributions which is helpful for training our models.","bb7ecdb9":"if we compare total mistakes:  RandomForest, SVM, KNN seem to be best for this dataset","f77d0483":"### After my tests I see that:\n### RandomForest and SVM are Overall winnners in my case above.\nBut all of these 6 models did a great job on predicting "}}