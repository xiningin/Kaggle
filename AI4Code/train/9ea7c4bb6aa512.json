{"cell_type":{"896a43c7":"code","1872d2f2":"code","b4842196":"code","402049d8":"code","a3af13b0":"code","4b6780b9":"code","5318439f":"code","5d125352":"code","e831456b":"code","a4f466e0":"code","48506bf8":"code","be30b79a":"code","29c1f536":"code","ee9df147":"code","0c3ed726":"code","37bb4a13":"code","df604e4d":"code","8e149559":"code","9658cc0c":"code","4dcf0b42":"code","a38d93a8":"code","ae9be59d":"code","ba1b267e":"code","fd834d50":"code","4f95e7a0":"code","31646d0c":"code","66af1907":"code","bb7f730d":"code","5bb17ae3":"code","aee1d269":"code","beb781f3":"code","ddd748a3":"code","5bd8276c":"code","0571f983":"markdown","a093df15":"markdown","d2ab5312":"markdown","c874de50":"markdown","5fbf0b81":"markdown","43a3c351":"markdown","549d1e12":"markdown","509f68f2":"markdown","ac8b7ffe":"markdown","84bb6d0e":"markdown","bd9bbb8c":"markdown","2a1caa39":"markdown","833505e8":"markdown","f1549e7a":"markdown","8a4ba5c2":"markdown","f6d79e1a":"markdown","a7dbd444":"markdown","b2e65438":"markdown","1d129e75":"markdown","03072d56":"markdown","8fddacb9":"markdown","c764f8ce":"markdown","0df1ae0c":"markdown","01f738bc":"markdown","d4e11ea5":"markdown","674187f3":"markdown","61d52ff9":"markdown","4ead0285":"markdown","48a101e4":"markdown","40d9626d":"markdown","accac311":"markdown","82158bd0":"markdown","7f72cde4":"markdown","98028750":"markdown","2ab1f907":"markdown","3fd4ce5e":"markdown","9557d9d8":"markdown","99d874e3":"markdown","e972c695":"markdown","9a3b37ec":"markdown","4f969e2a":"markdown","aa6798e7":"markdown","03bceb55":"markdown","c4e7c3f7":"markdown"},"source":{"896a43c7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom matplotlib.pylab import rcParams\nfrom datetime import datetime\nwarnings.filterwarnings('ignore')","1872d2f2":"from statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMA","b4842196":"sns.set_style('darkgrid')","402049d8":"rcParams['figure.figsize'] = 10,6","a3af13b0":"dateparse = lambda x: datetime.strptime(x, '%Y-%m')","4b6780b9":"df = pd.read_csv('..\/input\/AirPassengers.csv')\ndf.head()","5318439f":"df['Month'] = pd.to_datetime(df['Month'])","5d125352":"df.set_index('Month', inplace = True)","e831456b":"plt.plot(df['#Passengers'], linewidth = 3)\nplt.xlabel('Years')\nplt.ylabel('Number of Passengers')\nplt.show()","a4f466e0":"#Now the rolling statistics\nrolmean = df.rolling(window= 12).mean() #Gives a series of means of the number of previous values equals the window size.\nprint(rolmean.head(20))\nrolstd = df.rolling(window=12).std()\nprint(rolstd.head(20))","48506bf8":"plt.plot(df['#Passengers'], linewidth = 2, label = 'Original')\nplt.plot(rolmean, linewidth = 2, label = 'Rolling Mean', color = 'r')\nplt.plot(rolstd, linewidth = 2, label = 'Rolling Std Dev', color = 'k')\nplt.legend(loc = 'best')\nplt.title('Rolling Mean and Standard Deviation')\nplt.show()","be30b79a":"#Performing Augumented Dickey Fuller Test\nprint('Results of the Dickey Fuller Test')\ndftest = adfuller(x = df['#Passengers'], autolag= 'AIC')\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nprint(dfoutput)\nfor key,value in dftest[4].items():\n    print('Critical Value ({}) = {}'.format(key,value))","29c1f536":"df_log = np.log(df)\nplt.plot(df_log['#Passengers'], linewidth = 3)\nplt.xlabel('Years')\nplt.ylabel('Number of Passengers')\nplt.show()","ee9df147":"#Tranformation to make stationary\nmovingAverage = df_log.rolling(window= 12).mean()\nmovingStd = df_log.rolling(window=12).std()\nplt.plot(df_log['#Passengers'], linewidth = 2, label = 'Original')\nplt.plot(movingAverage, linewidth = 2, label = 'Rolling Mean', color = 'r')\nplt.legend(loc = 'best')\nplt.title('Moving Average')\nplt.show()","0c3ed726":"dfLogScaleMinusMovingAverage = df_log - movingAverage\n#Removing the NaN Values\ndfLogScaleMinusMovingAverage.dropna(inplace= True)","37bb4a13":"def test_stationary(timeseries):\n    #Determining Rolling Statistics\n    movingAverage = timeseries.rolling(window = 12).mean()\n    movingStd = timeseries.rolling(window = 12).std()\n    \n    #Plotting Rolling Statistics\n    plt.plot(timeseries, linewidth = 2, label = 'Original')\n    plt.plot(movingAverage, linewidth = 2, label = 'Rolling Mean', color = 'r')\n    plt.plot(movingStd, linewidth = 2, label = 'Rolling Std Dev', color = 'k')\n    plt.legend(loc = 'best')\n    plt.title('Rolling Mean and Standard Deviation')\n    plt.show()\n    \n    #Performing Dickey Fuller Test\n    #Performing Augumented Dickey Fuller Test\n    print('Results of the Dickey Fuller Test')\n    dftest = adfuller(x = timeseries['#Passengers'], autolag= 'AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    print(dfoutput)\n    for key,value in dftest[4].items():\n        print('Critical Value ({}) = {}'.format(key,value))","df604e4d":"test_stationary(dfLogScaleMinusMovingAverage)","8e149559":"exDecayWeightedAverage = df_log.ewm(halflife= 12, min_periods=0, adjust= True).mean()","9658cc0c":"plt.plot(df_log, linewidth = 2, label = 'Log Transformation')\nplt.plot(exDecayWeightedAverage, linewidth = 2, color = 'r', label = 'Exponential Decay')\nplt.title(\"Exponential Scale and Log Scale\")\nplt.legend()\nplt.show()","4dcf0b42":"dfLogScaleMinusExpoMovingAverage =  df_log - exDecayWeightedAverage\ntest_stationary(dfLogScaleMinusExpoMovingAverage)","a38d93a8":"dfLogTimeShift = df_log.shift()\ndfLogDiffShift = df_log - dfLogTimeShift \nplt.plot(dfLogDiffShift, linewidth = 2)\nplt.show()","ae9be59d":"dfLogDiffShift.dropna(inplace= True)\ntest_stationary(dfLogDiffShift)","ba1b267e":"decomposition = seasonal_decompose(df_log)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.plot(df_log, label = 'Original', linewidth = 2)\nplt.plot(trend, label = 'Trend', linewidth = 2)\nplt.plot(residual, label = 'Residual', linewidth = 2)\nplt.plot(seasonal,label = 'seasonal', linewidth = 2)\nplt.legend()\nplt.show()\n\ndecomposedLog = residual\ndecomposedLog.dropna(inplace = True)\ntest_stationary(decomposedLog)","fd834d50":"lag_acf = acf(dfLogDiffShift, nlags = 20)\nlag_pacf = pacf(dfLogDiffShift, nlags = 20, method= 'ols')\n\n#Plot ACF\nplt.subplot(121)\nplt.plot(lag_acf, linewidth = 2)\nplt.axhline(y = 0, linestyle = '--', color = 'gray')\nplt.axhline(y=-1.96\/np.sqrt(len(dfLogDiffShift)), linestyle='--', color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(dfLogDiffShift)), linestyle='--', color='gray')\nplt.title('Autocorrelation Function') \n\n#Plot PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(dfLogDiffShift)), linestyle='--', color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(dfLogDiffShift)), linestyle='--', color='gray')\nplt.title('Partial Autocorrelation Function')\n            \nplt.tight_layout() ","4f95e7a0":"model = ARIMA(df_log, order=(2,1,0))\nresults_AR = model.fit(disp = -1)\nplt.plot(dfLogDiffShift)\nplt.plot(results_AR.fittedvalues, color = 'r')\nplt.title('RSS: {:.4f}'.format(sum((results_AR.fittedvalues - dfLogDiffShift['#Passengers'])**2)))\nprint('Plotting AR model')","31646d0c":"model = ARIMA(df_log, order=(0,1,2))\nresults_MA = model.fit(disp = -1)\nplt.plot(dfLogDiffShift)\nplt.plot(results_AR.fittedvalues, color = 'r')\nplt.title('RSS: {:.4f}'.format(sum((results_AR.fittedvalues - dfLogDiffShift['#Passengers'])**2)))\nprint('Plotting MA model')","66af1907":"model = ARIMA(df_log, order=(2,1,2))\nresults_ARIMA = model.fit(disp = -1)\nplt.plot(dfLogDiffShift)\nplt.plot(results_AR.fittedvalues, color = 'r')\nplt.title('RSS: {:.4f}'.format(sum((results_AR.fittedvalues - dfLogDiffShift['#Passengers'])**2)))\nprint('Plotting ARIMA model')","bb7f730d":"predictions_ARIMA = pd.Series(results_ARIMA.fittedvalues, copy= True)\nprint(predictions_ARIMA.head())","5bb17ae3":"#Convet to cumulative sum\npredictions_ARIMA_cumsum = predictions_ARIMA.cumsum()\npredictions_ARIMA_cumsum.head(10)","aee1d269":"predictions_ARIMA_log = pd.Series(df_log['#Passengers'].iloc[0], index = df_log.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_cumsum, fill_value = 0)\npredictions_ARIMA_log.head()","beb781f3":"#Inverse of log is exponent\nplt.figure(figsize=(13,8))\npredictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(df, linewidth = 2, label = 'Original')\nplt.plot(predictions_ARIMA, linewidth = 2, label = 'Prediction')\nplt.legend()\nplt.show()","ddd748a3":"df_log","5bd8276c":"plt.figure(figsize=(13,8))\nresults_ARIMA.plot_predict(1,264)\nplt.legend(loc = 'upper left')\nplt.show()","0571f983":"## Building Models","a093df15":"------","d2ab5312":"From the above plot it is clear that although our rolling standard deviation is almost constant but our rolling mean has a trend.  \nOur time-series in order to be stationary both rolling statistics are required to be parallel to the x-axis.\n\nIn order to support our argument we can conduct the ACDF hypothesis testing.","c874de50":"#### MA model","5fbf0b81":"We can see that we have got a fairly accurate model.","43a3c351":"$$log scale L = stationary part(L1) + trend(LT) $$ \n$$moving avg of log scale A = stationary part(A1) + trend(AT)  $$\n$$result series R = L - A = (L1+LT) - (A1+AT) = (L1-A1) + (LT-AT)$$  \nSince, L & A are series & it moving avg, their trend will be more or less same, Hence\nLT-AT nearly equals to 0\n\nThus trend component will be almost removed. And we have,\n\nR=L1\u2212A1\n, our final non-trend curve","549d1e12":"## de nada!","509f68f2":"## Transforming to Stationary","ac8b7ffe":"_______","84bb6d0e":"#### Opting for the log transformations","bd9bbb8c":"### Plotting ACF and PACF","2a1caa39":"Although the plot seems similar to the previous one but the scale has changed.","833505e8":"#### Time - Shift Transformation","f1549e7a":"References from [this kaggle kernel](https:\/\/www.kaggle.com\/freespirit08\/time-series-for-beginners-with-arima)","8a4ba5c2":"#### AR Model","f6d79e1a":"#### Importing the Libraries","a7dbd444":"______","b2e65438":"Visually it looks like the best result but as the ACDF Test shows us that it is not better than our Exponential Decay.  \nAlthough we got the best results in Exponential Decay but we will continue with log transformation for simplicity.","1d129e75":"We know from above graph that both the Time series with log scale as well as its moving average have a trend component.","03072d56":"______","8fddacb9":"The above process shows that \"Subtracting two similar trend series can make the series stationary\"\np-values has reduced frmom 0.99 to 0.022\nCritical-values are pretty close to test statistic.  \nFrom these conditions we cansay that our given series is stationary.  \n\nFor the purpose of getting higher accuracy we try to find better scale than our current log scale.  \n\n\n","c764f8ce":"The above plot shows not much difference in log scale and Exponential decay transformation  \nbut we also conduct the ACDF test to back our observation.","0df1ae0c":"**Null Hypothesis** : The time-series is not stationary.  \n\nFrom the above results we can see that our p-value is much larger than any of the critical values.  \nAnd none of the Critical Values are near to our Test Statistic.  \nSo we keep the Null Hypothesis.  \n\nAccording to the ACDF test, **our time-series at the moment is not stationary.**","01f738bc":"\n\nFrom the ACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, Q = 2 From the PACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, P = 2\n\nARIMA is AR + I + MA. Before, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models will give a value of RSS. Lower RSS values indicate a better model","d4e11ea5":"#### Importing the data","674187f3":"________","61d52ff9":"Using the concepts of Time Series Analysis","4ead0285":"### Concepts of Time Series Analysis","48a101e4":"[pandas.DataFrame.ewm](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.ewm.html)","40d9626d":"_____","accac311":"The rolling mean is still not much stationary but atleast better than previous case.  ","82158bd0":"### Predictions and Revert Conversions","7f72cde4":"___","98028750":"Bu using the ARIMA model we can observe that the RSS values has reduced from both cases to 1.0292.  \nThis implies that the ARIMA model is better than both the AR and MA individually.  \nNow with our model we can make predictions but first we need to convert the predictions back in original form, beacuse our model is built on Log tranformed data.","2ab1f907":"#### Exponential Decay","3fd4ce5e":"Now we break down the three components of the log scale series using a library function. Once we separate out the components, we can simply ignore trend & seasonality and check on the nature of the residual part.","9557d9d8":"_____","99d874e3":"# Air Passengers Forecasting","e972c695":" - What is Time series analysis?  \n\nA. Time Series is a series of observations taken at specified time intervals usually equal intervals. Analysis of the series helps us to predict future values based on previous observed values. In Time series, we have only 2 variables, time & the variable we want to forecast.  \n\n\n - Why & where Time Series is used?  \n\nA. Time series data can be analysed in order to extract meaningful statistics and other characteristics. It's used in atleast the 4 scenarios:  \na) Business Forecasting  \nb) Understand past behavior  \nc) Plan the future  \nd) Evaluate current accomplishment  \n\n\n - When shouldn't we use Time Series Analysis?  \n\nA. We don't need to apply Time series in atleast the following 2 cases:  \na) The dependant variable(y) (that is supposed to vary with time) is constant. Eq: y=f(x)=4, a line parallel to x-axis(time) will always remain the same.  \nb) The dependant variable(y) represent values that can be denoted as a mathematical function. Eq: sin(x), log(x), Polynomials etc. Thus, we can directly get value at some time using the function itself. No need of forecasting.  \n\n\n - What are the components of Time Series?  \n\nA. There are 4 components:  \na) Trend - Upward & downward movement of the data with time over a large period of time. Eq: Appreciation of Dollar vs rupee.  \nb) Seasonality - seasonal variances. Eq: Ice cream sales increases in Summer only  \nc) Noise or Irregularity - Spikes & troughs at random intervals  \nd) Cyclicity - behavior that repeats itself after large interval of time, like months, years etc.  \n\n\n - What is Stationarity?  \n\nA. Before applying any statistical model on a Time Series, the series has to be stationary, which means that, over different time periods,  \na) It should have constant mean.  \nb) It should have constant variance or standard deviation.  \nc) Auto-covariance should not depend on time.  \n\n\nTrend & Seasonality are two reasons why a Time Series is not stationary & hence need to be corrected.  \n\n\n - Why does Time Series(TS) need to be stationary?  \n\nA. It is because of the following reasons:  \na) If a TS has a particular behavior over a time interval, then there's a high probability that over a different interval, it will have same behavior, provided TS is stationary. This helps in forecasting accurately.  \nb) Theories & Mathematical formulas are more mature & easier to apply for as TS which is stationary.  \n\n\n - Tests to check if a series is stationary or not  \n\nA. There are 2 ways to check for Stationarity of a TS:  \na) Rolling Statistics - Plot the moving avg or moving standard deviation to see if it varies with time. Its a visual technique.  \nb) ADCF Test - Augmented Dickey\u2013Fuller test is used to gives us various values that can help in identifying stationarity. The Null hypothesis says that a TS is non-stationary. It comprises of a Test Statistics & some critical values for some confidence levels. If the Test statistics is less than the critical values, we can reject the null hypothesis & say that the series is stationary. THE ADCF test also gives us a p-value. Acc to the null hypothesis, lower values of p is better.  \n\n\n - What is ARIMA model?  \n\nA. ARIMA(Auto Regressive Integrated Moving Average) is a combination of 2 models AR(Auto Regressive) & MA(Moving Average). It has 3 hyperparameters - P(auto regressive lags),d(order of differentiation),Q(moving avg.) which respectively comes from the AR, I & MA components. The AR part is correlation between prev & current time periods. To smooth out the noise, the MA part is used. The I part binds together the AR & MA parts.  \n\n - How to find value of P & Q for ARIMA ?  \n\nA. We need to take help of ACF(Auto Correlation Function) & PACF(Partial Auto Correlation Function) plots. ACF & PACF graphs are used to find value of P & Q for ARIMA. We need to check, for which value in x-axis, graph line drops to 0 in y-axis for 1st time.  \nFrom PACF(at y=0), get P  \nFrom ACF(at y=0), get Q  \n\n\n -  What Is ADCF test?  \n\nA. In statistics and econometrics, an augmented Dickey\u2013Fuller test (ADF) tests the null hypothesis that a unit root is present in a time series sample. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity. It is an augmented version of the Dickey\u2013Fuller test for a larger and more complicated set of time series models.  \n\nThe augmented Dickey\u2013Fuller (ADF) statistic, used in the test, is a negative number. The more negative it is, the stronger the rejection of the hypothesis that there is a unit root at some level of confidence.  \n\np value(0<=p<=1) should be as low as possible. Critical values at different confidence intervals should be close to the Test statistics value. \n\n - What is Exponential Smoothing?  \n \nA. Exponential smoothing is a rule of thumb technique for smoothing time series data using the exponential window function. Whereas in the simple moving average the past observations are weighted equally, exponential functions are used to assign exponentially decreasing weights over time. It is an easily learned and easily applied procedure for making some determination based on prior assumptions by the user, such as seasonality. Exponential smoothing is often used for analysis of time-series data.  \n\nThe raw data sequence is often represented by xt  \nbeginning at time t=0, and the output of the exponential smoothing algorithm is commonly written as st, which may be regarded as a best estimate of what the next value of x will be. When the sequence of observations begins at time t=0, the simplest form of exponential smoothing is given by the formulas:  \n$$s_{0} = x_{0}$$\n$$s_{t} = \u03b1*x_{t} + (1-\u03b1)*s_{t-1}$$  \nwhere \u03b1 is the smoothing factor, and 0<\u03b1<1.  \n\n - What is Exponential decay?  \n\nA. A quantity is subject to exponential decay if it decreases at a rate proportional to its current value. Symbolically, this process can be expressed by the following differential equation, where N is the quantity and \u03bb (lambda) is a positive rate called the exponential decay constant:  \n$$dN\/dt = -\u03bbN$$\nThe solution to this equation (see derivation below) is:  \n$$N(t) = N_{0}*e^{-\u03bbt}$$\nwhere N(t) is the quantity at time t, and N0 = N(0) is the initial quantity, i.e. the quantity at time t = 0.  \nHalf Life is the time required for the decaying quantity to fall to one half of its initial value. It is denoted by t1\/2. The half-life can be written in terms of the decay constant as:  \n$$t_{1\/2} = ln(2)\/\u03bb$$","9a3b37ec":"Several methods to make our time-series stationary are stationarity through data transformation like taking **log10**,**loge**, **square**, **square root**, **cube**, **cube root**, **exponential decay**, **time shift**, etc.","4f969e2a":"We have 144 data points and we want to predict additional 10 years data which suggest we have to predict additional 120 points.","aa6798e7":"We can observe that the time series is stationary since the sries is almost parallel to the x-axis.\n\np-value has decreased from 0.02 to 0.005  \nTest Statistical Values is much closer to the critical values.  \n\nAs visually it couldn't be observed but our new transformation is better than our log transformation.\n  ","03bceb55":"We can try even to get even better results by using the simple time shift technique.  \n\nGiven the values:  \n$$x0, x1, x2, x3, .... xn$$  \nShifted values will be  \n$$null, x0, x1, x2,.... xn$$  \nThus, the time series with time shifted values are:  \n$$null, (x1-x0), (x2-x1), (x3-x2), (x4-x3),.... (xn-x_{n-1})$$","c4e7c3f7":"[Prashant Brahmbhatt](https:\/\/github.com\/hashbanger)"}}