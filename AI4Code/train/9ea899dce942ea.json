{"cell_type":{"9611c516":"code","7ba8810e":"code","585f41a8":"code","2fac926d":"code","84a2d625":"code","4cb627c9":"code","ba7ce1fd":"code","44c59df2":"code","f31e0041":"code","fba7bc84":"code","841c320f":"code","1e80f314":"code","ff77b288":"code","af9fea89":"code","93043228":"code","61d60d23":"code","920ec99f":"code","b3b9b1a1":"code","7514bb5b":"code","15db7f21":"code","55a00b03":"code","6070c4f2":"code","6c54cf7d":"code","1b0f98f4":"code","733712c2":"code","3ff84ae8":"code","7807b2f9":"code","67530f3f":"code","78dd83e4":"code","238cb263":"code","c0f3e0d5":"code","300ed669":"code","44803fba":"code","f748baee":"code","20ff6a12":"code","473693f4":"code","fc644619":"code","2fec97a4":"code","93c0fa62":"code","cc93533d":"code","15bdb0e9":"code","12d201a9":"code","6ebebc92":"code","83fa0bba":"code","8a22312a":"code","47eaa465":"code","55a3b54a":"code","d43d0eb6":"code","4cb5b91d":"code","7b4f1322":"code","917c198c":"code","44ca0dbc":"code","7689148d":"markdown","bbc3b643":"markdown","05bcd470":"markdown","7558ebbb":"markdown","16775e3c":"markdown","2ec23531":"markdown","82ec1deb":"markdown","ce8569c3":"markdown","ff1180ef":"markdown","b9c40bee":"markdown"},"source":{"9611c516":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ba8810e":"#Reading the file into a dataframe\ndf = pd.read_csv('\/kaggle\/input\/churn-prediction\/Churn.csv')","585f41a8":"#Checking the first few raws of the data set\ndf.head()","2fac926d":"#Checking the columns names \nprint(df.columns)\n\n#Check number of raws in the data\ndf.shape","84a2d625":"df.info()","4cb627c9":"df['TotalCharges'].unique()","ba7ce1fd":"#df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], downcast='float')","44c59df2":"df.describe()","f31e0041":"#checking the values of each column\ndf.nunique()#[df.nunique() == 2]","fba7bc84":"# Identifying the values of each column, separating the columns into 2 lists (Discrete and Continuous)\ndisc = 0\ncont = 0\ndisc_list = []\ncont_list = []\n\nfor x, i in df.nunique()[1: ].iteritems():\n    if i > 5:\n        cont += 1\n        cont_list.append(x)\n    else:\n        disc += 1\n        disc_list.append(x)\n# ===================================\ndisc, cont, disc_list, cont_list\n\nprint(f'Number of discrete columns in the dataset {disc}')\nprint('=' * 100)\nprint(f'Number of continues columns in the dataset {cont}')\nprint('=' * 100)\nprint(f'List of discrete columns in the dataset {disc_list}')\nprint('=' * 100)\nprint(f'List of continues columns in the dataset {cont_list}')\nprint('=' * 100)","841c320f":"# \n\nfor val in disc_list:\n    vals_counts = pd.DataFrame(df[val].value_counts())\n    vals_counts.plot(kind='bar');\n    plt.title(val)\n    vals_counts['ratios'] = vals_counts[val] \/ vals_counts[val].sum()\n    print(val)\n    print(vals_counts)\n    print('=' * 100)\n    ","1e80f314":"# inestigating values of column TotalCharges\nmissing_values_TotalCharges= df.TotalCharges.sort_values()[:11].index\n\n#There are 11 missing values in column TotalCharges \ndf.TotalCharges.sort_values().head(12)\n\n","ff77b288":"#droping missing values from column TotalCharges\n\ndf = df.drop(missing_values_TotalCharges, axis=0)","af9fea89":"#change TotalCharges column type from object to float \ndf['TotalCharges'] = df['TotalCharges'].astype('float')","93043228":"df[cont_list].hist(figsize=[10, 6], bins=50, edgecolor='g');\ndf[cont_list].describe()","61d60d23":"df.head(2)","920ec99f":"#identify cols where we apply value transformation to binary (0, 1) \n\ncols = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService',\n       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n       'PaymentMethod', 'Churn']\n\n#pd.get_dummies(df[cols[0]], prefix=str(cols[0]))\n\ndf1=pd.DataFrame()\nfor i in cols:\n    dummies = pd.get_dummies(df[i], prefix=str(i))\n    \n    for x in dummies.columns:\n        df1[x] = dummies[x]\n","b3b9b1a1":"# Isolating relavant parameters \nvars_keep = []\nvars_delete = []\n    \nfor word in df1.columns:\n    if '_No' in word:\n        vars_delete.append(word)\n    else:\n        vars_keep.append(word)","7514bb5b":"#Creating new dataframe and compiling all the relavant parameters\n\ndf2 = df1[vars_keep].copy()\n#df2['customerID'] = df['customerID']\ndf2['tenure'] = df['tenure']\ndf2['MonthlyCharges'] = df['MonthlyCharges']\ndf2['TotalCharges'] = df['TotalCharges']\n\ndf2.drop(['gender_Female', 'SeniorCitizen_0'], inplace=True, axis = 1)","15db7f21":"df2.head()","55a00b03":"# Calculating the average churn for binary discrete variables \n\nestimate_1_list = []\nestimate_0_list = []\ndiffs_list = []\n\nmean_churn_1 = []\nmean_churn_1_upper_limit = []\nmean_churn_1_lower_limit = []\n\nmean_churn_0 = []\nmean_churn_0_upper_limit = []\nmean_churn_0_lower_limit = []\n\ndiff_means = []\ndiff_means_upper_limit = []\ndiff_means_lower_limit = []\n\ncol_names =  list(df2.columns[0:-4])\n\nfor col_name in col_names:\n    mean_churn = np.array(df2[df2[col_name] ==1]['Churn_Yes'])\n    mean_churn2 = np.array(df2[df2[col_name] ==0]['Churn_Yes'])\n    \n    mean_churn_diff = mean_churn.mean() - mean_churn2.mean()\n    \n    for i in range(10000):\n        estimate_1 = np.random.choice(mean_churn, 1000)\n        estimate_0 = np.random.choice(mean_churn2, 1000)\n        estimate_1_list.append(estimate_1.mean())\n        estimate_0_list.append(estimate_0.mean())\n        \n        diff = estimate_1.mean() - estimate_0.mean()\n        diffs_list.append(diff)\n    \n    \n    fig, (ax3, ax4) = plt.subplots(1, 2, figsize=[15 , 4], gridspec_kw={'width_ratios': [10, 5], 'wspace': 0.1});\n    ax3.hist(estimate_1_list, bins=40, alpha=0.5, label=f'{col_name} = 1')\n    ax3.hist(estimate_0_list, bins=40, alpha=0.5, label=f'{col_name} = 0')\n    ax3.axvline(mean_churn.mean(), color='blue', label='Mean = 1')\n    ax3.axvline(np.percentile(estimate_1_list, 97.5), color='blue', linestyle='dotted', label = 'UL & LL = 1')\n    ax3.axvline(np.percentile(estimate_1_list, 2.5), color='blue', linestyle='dotted')\n    ax3.axvline(mean_churn2.mean(), color='orange', label='Mean = 0')\n    ax3.axvline(np.percentile(estimate_0_list, 97.5), color='orange', linestyle='dotted', label = 'UL & LL = 0')\n    ax3.axvline(np.percentile(estimate_0_list, 2.5), color='orange', linestyle='dotted')\n    ax3.legend()\n    \n    ax4.hist(diffs_list, bins=40, alpha=0.5, label='difference', color='g')\n    ax4.axvline(mean_churn_diff, color='orange', ls='--', label='Test Value')\n    ax4.text(np.min(diffs_list), 750, 'Bootstrap diff means')\n    ax4.axvline(np.percentile(diffs_list, 2.5), color='orange', ls='--')\n    ax4.axvline(np.percentile(diffs_list, 97.5), color='orange', ls='--')\n     \n    mean_churn_1.append(mean_churn.mean())\n    mean_churn_1_upper_limit.append(np.percentile(estimate_1_list, 97.5))\n    mean_churn_1_lower_limit.append(np.percentile(estimate_1_list, 2.5))\n    \n    mean_churn_0.append(mean_churn2.mean())\n    mean_churn_0_upper_limit.append(np.percentile(estimate_0_list, 97.5))\n    mean_churn_0_lower_limit.append(np.percentile(estimate_0_list, 2.5))\n    \n    diff_means.append(mean_churn_diff.mean())\n    diff_means_upper_limit.append(np.percentile(diffs_list, 97.5))\n    diff_means_lower_limit.append(np.percentile(diffs_list, 2.5))\n    \n         \n    estimate_1_list = []\n    estimate_0_list = [] \n    diffs_list = []\n    \nlist_values = {'col_names' :col_names, 'mean_churn_1' : mean_churn_1, 'mean_churn_1_upper_limit' : mean_churn_1_upper_limit,\n               'mean_churn_1_lower_limit' : mean_churn_1_lower_limit, 'mean_churn_0' : mean_churn_0,\n               'mean_churn_0_upper_limit' : mean_churn_0_upper_limit, 'mean_churn_0_lower_limit' : mean_churn_0_lower_limit,\n               'diff_means' : diff_means, 'diff_means_upper_limit': diff_means_upper_limit,\n               'diff_means_lower_limit' : diff_means_lower_limit}\n\nestimation_table = pd.DataFrame(list_values)\n\nestimation_table","6070c4f2":"hypothesises_list = ['gender_Male', 'PhoneService_Yes', 'MultipleLines_Yes', 'OnlineBackup_Yes', 'DeviceProtection_Yes', 'StreamingTV_Yes',\n       'StreamingMovies_Yes']","6c54cf7d":"def stat_test(df, col_name, number_of_trials, target):\n    \"\"\"\n    Function to simulate statistical significance between 2 categories of a variable \n    and the noshow% affected by the categories\n    \"\"\"\n    grouped_mean = df.groupby(col_name)[target].mean()\n    \n    significance_value = (grouped_mean[0] - grouped_mean[1])\n    array1 = np.array(df[col_name][df[target] == 0])\n    array2 = np.array(df[col_name][df[target] == 1])\n    array_combined = np.concatenate((array1, array2))\n    \n    means_list_1 = []\n    means_list_2 = []\n    means_diff_list = []\n    \n    for i in range(number_of_trials):\n        array_combined_perm = np.random.permutation(array_combined)\n        array1_permutation = array_combined_perm[:len(array1)]\n        array2_permutation = array_combined_perm[len(array2):]\n        diff = array1_permutation.mean() - array2_permutation.mean()\n        \n        means_list_1.append(array1_permutation.mean())\n        means_list_2.append(array2_permutation.mean())\n        means_diff_list.append(diff)\n        \n    means_diff_array = np.array(means_diff_list)\n    means_array_1 = np.array(means_list_1)  \n    means_array_2 = np.array(means_list_2)\n    \n    return (means_diff_array, means_array_1, means_array_2, significance_value, col_name)\n\ndef hist_plot_sig_test(data1, data2, data3, line_value,bins=40, label_1=\"\", label_2=\"\", label_3=\"\", name=\"\"):\n    \n    \"\"\"\n    visualize the simualtion result\n    \"\"\"\n    \n    f, (axis1, axis2) = plt.subplots(1, 2, figsize=[15, 5], gridspec_kw={'width_ratios': [1, 1]})\n    f.tight_layout()\n    axis1.hist(data1, alpha=.5, bins=bins, edgecolor='lightgray', label=label_1);\n    axis1.hist(data2, alpha=.5, bins=bins, edgecolor='lightgray', label=label_2);\n    axis1.legend();\n    \n    axis2.hist(data3, bins=bins, edgecolor='lightgray', label=label_3);\n    axis2.axvline(line_value, linestyle='dotted', c='g', label=\"Test Value\", linewidth = 3)\n    #plt.text(line_value * 1.05, 20, str(round(line_value,2)), color='r')#, rotation=90\n    box_viz = plt.text(line_value, 10, str(round(line_value,2)),color='g')\n    box_viz.set_bbox(dict(facecolor='w', alpha=0.9)) #, edgecolor='tab:bl\n    \n    axis2.axvline(np.percentile(data3, 2.5), c='y', label=\"Lower rejection bound\", linewidth = 5, alpha = 0.7)\n    axis2.axvline(np.percentile(data3,97.5), c='r', label=\"Upper rejection bound\", linewidth = 5, alpha = 0.7)\n \n    axis2.legend();\n","1b0f98f4":"for i in hypothesises_list:  #(df, col_name, number_of_trials, target)\n    diff_array, array_1, array_0, significance_value, colname = stat_test(df2, i, 10000, 'Churn_Yes')\n    hist_plot_sig_test(array_1, array_0, diff_array, significance_value,\n                       label_1=f\"{colname} Permutation sample 1\", label_2=f\"{colname} Permutation sample 2\", name=colname)\n    \n    if (significance_value > np.percentile(diff_array, 2.5) and significance_value < np.percentile(diff_array, 97.5)):\n        print(f'- For column \"{i}\" we fail to reject the null hypothesis')\n        print('=' * 60)\n    else:\n        print(f'- For column \"{i}\" we reject the null hypothesis')\n        print('=' * 60)\n\n","733712c2":"# Calculating the average churn for Continues variables variables \n\ndf2.groupby('tenure')['Churn_Yes'].mean().plot(ls=\"\", marker='o', markersize=5);\n\ndf2[['tenure', 'Churn_Yes']].corr()","3ff84ae8":"# Visualizing mean churn Vs. Total Charges\ntotal_charg = pd.DataFrame(df2.groupby('TotalCharges')['Churn_Yes'].mean()).reset_index(drop=False)\ntotal_charg\nplt.scatter(total_charg['TotalCharges'], total_charg['Churn_Yes'], alpha=0.2);\ntotal_charg.corr().iloc[1,0]\n","7807b2f9":"# Visualizing mean churn Vs. Monthly Charges \nmonth_charg = pd.DataFrame(df2.groupby('MonthlyCharges')['Churn_Yes'].mean()).reset_index(drop=False)\nmonth_charg\nplt.scatter(month_charg['MonthlyCharges'], month_charg['Churn_Yes'], alpha=0.2);\n\nmonth_charg.corr().iloc[1,0]","67530f3f":"#Cleaning data and renaming columns\n\ndf2['churn'] = df2['Churn_Yes']\ncol_names= ['male', 'senior_citizen', 'partner', 'dependents',\n       'phone_service', 'multipleLines', 'internet_dsl',\n       'internet_fiber', 'online_security', 'online_backup',\n       'device_protection', 'tech_support', 'streamingtv',\n       'streaming_movies', 'contract_month', 'contract_one_year',\n       'contract_two_year', 'paperlessBilling',\n       'payment_Bank_transfer_automatic',\n       'payment_credit_automatic',\n       'payment_electronic_check', 'payment_mailed_check',\n       'Churn_Yes', 'tenure', 'monthly_charges', 'total_charges', 'churn']\n\ndf2.columns = col_names\ndf2.drop('Churn_Yes', inplace=True, axis=1)\n\ndf2.head()","78dd83e4":"#checking for correlation\ndf_corr = df2.corr()\n\nplt.figure(figsize=[20, 10]);\nsns.heatmap(df_corr, cmap=\"YlGnBu\", annot =True, fmt=\".1\"); #, annot=True\n","238cb263":"#converting correlation matrix to df \n\ncorr_table = df_corr[abs(df_corr) >= 0.01].stack().reset_index()\ncorr_table = corr_table[corr_table['level_0'].astype(str)!=corr_table['level_1'].astype(str)]\ncorr_table['ordered-cols'] = corr_table.apply(lambda x: '-'.join(sorted([x['level_0'],x['level_1']])),axis=1)\ncorr_table = corr_table.drop_duplicates(['ordered-cols'])\n#corr_table.drop(['ordered-cols'], axis=1, inplace=True)\n\n\ncorr_table['abs_corr_val'] = abs(corr_table[0])\n\ncorr_table.head()","c0f3e0d5":"corr_table = corr_table.sort_values(by=['abs_corr_val'], ascending = False)","300ed669":"plt.figure(figsize= [15, 5]);\nplt.bar(np.arange(corr_table.shape[0]), corr_table['abs_corr_val']);\nplt.axhline(0.5, ls='--', c='k');\n\n\ncorr_table[corr_table['abs_corr_val'] > 0.50]","44803fba":"\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","f748baee":"# Seperating predicting variables\nfeatures = list(df2.columns[:-1])","20ff6a12":"#Testing variance inflation factor \n\"\"\"\nfeatures.remove('internet_dsl')\nfeatures.remove('contract_month')\nfeatures.remove('payment_electronic_check')\nfeatures.remove('phone_service')\nfeatures.remove('contract_one_year')\n#eatures.remove('tenure')\nfeatures.remove('monthly_charges')\nfeatures.remove('total_charges')\nfeatures.remove('male')\n\"\"\"\n\n\n# the independent variables set\ndf_features = df2 [features]\n\n# VIF dataframe\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = df_features.columns\n\n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(df_features.values, i)\n                          for i in range(len(df_features.columns))]\n\nvif_data","473693f4":"#Testing variance inflation factor \n\nfeatures2 = list(df2.columns[:-1])\n\nfeatures2.remove('payment_Bank_transfer_automatic')\nfeatures2.remove('monthly_charges')\nfeatures2.remove('contract_month')\nfeatures2.remove('total_charges')\nfeatures2.remove('tenure')\n\n\n\n# the independent variables set\ndf_features = df2 [features2]\n\n# VIF dataframe\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = df_features.columns\n\n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(df_features.values, i)\n                          for i in range(len(df_features.columns))]\n\nvif_data.sort_values(by='VIF', ascending= False)","fc644619":"#Import Logistic regression necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n","2fec97a4":"# spliting the data into 2 variables x, y\nx = df2.drop('churn', axis=1)\n\n#x = x[features2]\ny = df2['churn']","93c0fa62":"# Spliting the data to 4 datasets from training and evaluation\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42, stratify=y)","cc93533d":"# Instantiate Logistic regression model\nlogreg = LogisticRegression(solver='liblinear')","15bdb0e9":"# Fitting the training data to the model\nlogreg.fit(x_train, y_train)","12d201a9":"# Coeffecients of variables\nlogreg.coef_","6ebebc92":"# Making a predication on the test set \ny_pred = logreg.predict(x_test)","83fa0bba":"y_pred_proba = logreg.predict_proba(x_test)[:, 1]","8a22312a":"pred_df = pd.DataFrame({'y_test': y_test, 'y_pred': y_pred, 'probability': y_pred_proba})","47eaa465":"result_list = []\nfor idx, value in pred_df.iterrows():\n    #print(value[2])\n    if (value[0] == 1 and value[1] == 1):\n        result = 'True Positive'\n        #result_list.append(result)\n    elif (value[0] == 0 and value[1] == 0):\n        result = 'True Negative'\n    elif (value[0] == 0 and value[1] == 1):\n        result = 'False Positive'\n    else:\n        result = 'False Negative'\n    \n    result_list.append(result)\n    ","55a3b54a":"pred_df['category'] = result_list","d43d0eb6":"plt.figure(figsize=[10, 6])\n\nsns.scatterplot(x= np.arange(pred_df.shape[0]), y =pred_df['probability'], \n                hue= pred_df['category'], style=pred_df['category'], palette = ['red', 'blue', 'green', 'black']); \nplt.axhline(0.5, label = 'Threshold', lw=2, alpha=0.7, c='k', ls='--');\nplt.title('Logistic regression model prediction result')\nplt.legend();","4cb5b91d":"pred_df.groupby('category').count()\n\npred_df['category'].value_counts(normalize='True')","7b4f1322":"# Printing the classification report\n\nreport = classification_report(y_test, y_pred)\n\nprint(report)","917c198c":"# Checking Model accuracy score\naccuracy_score(y_test, y_pred)","44ca0dbc":"# Model Confusion matrix\nconfusion_matrix(y_test, y_pred)","7689148d":"<h2> Calculating Average churn per parameter <\/h2>","bbc3b643":"<h1> Building Prediction model - Logistic Regression <\/h1>","05bcd470":"<h1> Testing for variance inflation <\/h1>","7558ebbb":"**Exploring Dataset Attributes !**\n\n1. What are the values of the discrete columns?\n2. How are the values in each discrete parameter ranked?\n3. What are the values of the continuos columns?\n4. How are the values in each continuos parameter distributed?","16775e3c":"<h1> Conclusion from correlation matrix <\/h1> \n\n>1. Customer tenurity is highly correlated positively with total charges and inverse correlation with having the connection without a contract\n2. Increase in monthly charges is highly correlated with having having fiber optic connection, online services and streaming services\n3. The increase in monthly charges due to above mentioned reasons is highly correlated with  increase in total charges\n4. Parameters `contract_month` and `contract_two_year`,`contract_one_year` are the inverse of each other as they are generated from the `pd.get_dummies()` function\n5. Parameters `internet_fiber` and `internet_dsl` are sharing the same condition mentioned above in point no(4)","2ec23531":"<h1>Data Cleaning<\/h1>","82ec1deb":"<h1> Checking for collinearity <\/h1>","ce8569c3":"<h2> Hypothesis testing for significance <\/h2>\n    \n>Testing the hypothesis that the attributes of each parameters has the same effect on churn mean%\n>\n><center> $ H_{null}:  \\mu_{1} - \\mu_{0} = 0 $<\/center>\n><center> $ H_{alternative}:  \\mu_{1} - \\mu_{0} \\neq 0 $<\/center>\n> \n><center> $ \\alpha: 5\\% $<\/center>","ff1180ef":"**Note** 1\n<br>\n1. The dataset contains 7043 unique customers\n2. There are 21 columns in the dataset\n    - Column `customerID` : Unique identifier \n    - Columns `Churn` : Target Variable\n    - 19 columns : Predicator variables \n        - 16 columns with discrete values\n        - 3 columns wuth continues values","b9c40bee":"<h1> Conclusions <\/h1>\n\n> From the prevoius figures we can denote the follow:\n> - There is a strong indications that the below feature has statistical significance when it comes to Average churn per attribute, is denoted by previous figures for bootstrapped samples of means distribution <br><br>\n    1. `SeniorCitizen` <br>\n    2. `Partner` <br>\n    3. `Dependent` <br>\n    4. `Internetservice_DSL` <br>\n    5. `Internetservice_fiberoptic`<br>\n    6.  `OnlineSecurity_Yes`\n    6. `TechSupport` <br>\n    7. `Contract_Month-to-month` <br>\n    8. `Contract_One year` <br>\n    9. `Contract_Two year` <br>\n    10. `PaperlessBilling_Yes` <br>\n    11. `PaymentMethod_Bank transfer (automatic)` <br>\n    12. `PaymentMethod_Credit card (automatic)` <br>\n    13. `PaymentMethod_Electronic check` <br>\n    14. `PaymentMethod_Mailed check` <br>\n    <br><br>\n    \n>- Features listed below doesn't show a strong indication for average churn per attribute\n    1. `gender` <br>\n    2. `PhoneService` <br>\n    3. `MultipleLines` <br>\n    4. `OnlineBackup` <br>\n    5. `DeviceProtection` <br>\n    6. `StreamingTV` <br>\n    7. `StreamingMovies` <br><br>\n>Thus we will do a separate focused test on the above attributes to conclude wither attributes of each parameter is significant or not"}}