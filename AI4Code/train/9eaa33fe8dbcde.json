{"cell_type":{"5bf92c64":"code","c3a17793":"code","50894efa":"code","019f10a2":"code","8d32c536":"code","dc49eb2e":"code","582060db":"code","12d5f954":"code","07464a16":"code","a5c52632":"code","fbc004ed":"code","b98fa345":"code","2892a69b":"code","45a5a8c4":"code","cce66a06":"code","a500b0bf":"code","729cc1fe":"code","ad8a4695":"code","aac1f03b":"code","d9680295":"code","0880763a":"code","76e0dd38":"code","49fb8df7":"code","7811a801":"code","98d635e4":"code","f352c0ba":"code","d5f0e281":"code","73ae26ad":"code","d34291e0":"code","83f18fa0":"code","50b9d754":"code","d9220f38":"code","eca4917f":"code","594e533d":"code","b287500f":"code","31b2b978":"code","1bc47ef9":"code","a56213c8":"code","be312a81":"code","92423acc":"code","295c7a38":"code","3fb9b9fe":"code","84e2dd57":"code","f95c0beb":"code","4c54d648":"code","9236e023":"code","04e7ddc0":"code","d5656b3d":"code","d8de2085":"code","acfc9d61":"code","6fe59b3a":"code","91dd0ceb":"markdown","09c33fcf":"markdown","a572b3e6":"markdown","f6c2ed90":"markdown","02f3013b":"markdown","d8a858a0":"markdown","9e92ce62":"markdown","2b486f8e":"markdown","d0301310":"markdown","e1507238":"markdown","b8cc69a8":"markdown","76c9d166":"markdown","16c3d0de":"markdown","87bc65dc":"markdown","ae887f1d":"markdown","cb61d233":"markdown","b340b162":"markdown","3b73f025":"markdown","9115f763":"markdown","a8c1cdad":"markdown","11ae237b":"markdown","ac292b59":"markdown","7f692731":"markdown","e2ead582":"markdown","a8f4080a":"markdown","a041ad85":"markdown","cbcd1a05":"markdown","35217c30":"markdown","fba470c7":"markdown","77131808":"markdown","45ce5f5a":"markdown","6df22fdd":"markdown","7477c87f":"markdown","af8c452a":"markdown","6105440d":"markdown","4f380148":"markdown","694fa8f6":"markdown","df2a98f1":"markdown","5fae025b":"markdown","0b65106e":"markdown","8ff147b5":"markdown","182e38c7":"markdown","99e93d77":"markdown","d3d223d3":"markdown","283d844d":"markdown","c64e56a7":"markdown","74197c52":"markdown","6af18118":"markdown","f5277d19":"markdown"},"source":{"5bf92c64":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom catboost import CatBoostClassifier,Pool\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport plotly.graph_objs as go\nfrom sklearn import datasets\nimport plotly.plotly as py\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport time\nimport json\nimport sys\nimport csv\nimport os","c3a17793":"print('matplotlib: {}'.format(matplotlib.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))","50894efa":"# for get better result chage fold_n to 5\nfold_n=5\nfolds = StratifiedKFold(n_splits=fold_n, shuffle=True, random_state=10)\nwarnings.filterwarnings('ignore')\nsns.set(color_codes=True)\nplt.style.available\n%matplotlib inline\n%precision 2","019f10a2":"import os\nprint([filename for filename in os.listdir('..\/input') if '.csv' in filename])","8d32c536":"# import Dataset to play with it\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","dc49eb2e":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsample_submission.head()","582060db":"train.shape, test.shape, sample_submission.shape","12d5f954":"train.head()","07464a16":"test.head()","a5c52632":"train.tail()","fbc004ed":"train.columns","b98fa345":"print(train.info())","2892a69b":"train['target'].value_counts().plot.bar();","45a5a8c4":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain['target'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('target')\nax[0].set_ylabel('')\nsns.countplot('target',data=train,ax=ax[1])\nax[1].set_title('target')\nplt.show()","cce66a06":"train[\"1\"].hist();","a500b0bf":"train[\"2\"].hist();","729cc1fe":"train[\"3\"].hist();","ad8a4695":"train[train.columns[2:]].mean().plot('hist');plt.title('Mean Frequency');","aac1f03b":"sns.set(rc={'figure.figsize':(9,7)})\nsns.distplot(train['target']);","d9680295":"sns.violinplot(data=train,x=\"target\", y=\"1\");","0880763a":"sns.violinplot(data=train,x=\"target\", y=\"20\");","76e0dd38":"train['target'].value_counts()","49fb8df7":"cols=[\"target\",\"id\"]\nX = train.drop(cols,axis=1)\ny = train[\"target\"]","7811a801":"X_test  = test.drop(\"id\",axis=1)","98d635e4":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)","f352c0ba":"from sklearn.ensemble import RandomForestClassifier\nModel=RandomForestClassifier(max_depth=2)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_val)\nprint(classification_report(y_pred,y_val))\nprint(confusion_matrix(y_pred,y_val))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_val))","d5f0e281":"from sklearn.ensemble import BaggingClassifier\nbag_Model=BaggingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_val)\nprint(classification_report(y_pred,y_val))\nprint(confusion_matrix(y_pred,y_val))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_val))","73ae26ad":"from sklearn.ensemble import AdaBoostClassifier\nModel=AdaBoostClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_val)\nprint(classification_report(y_pred,y_val))\nprint(confusion_matrix(y_pred,y_val))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_val))","d34291e0":"from sklearn.ensemble import GradientBoostingClassifier\nModel=GradientBoostingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_val)\nprint(classification_report(y_pred,y_val))\nprint(confusion_matrix(y_pred,y_val))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_val))","83f18fa0":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nModel=LinearDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_val)\nprint(classification_report(y_pred,y_val))\nprint(confusion_matrix(y_pred,y_val))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_val))","50b9d754":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nModel=QuadraticDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_val)\nprint(classification_report(y_pred,y_val))\nprint(confusion_matrix(y_pred,y_val))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_val))","d9220f38":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nrfc_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)","eca4917f":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rfc_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist(),top=300)","594e533d":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\ntree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)","b287500f":"features = [c for c in train.columns if c not in ['id', 'target']]","31b2b978":"from sklearn import tree\nimport graphviz\ntree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=features)\ndisplay(graphviz.Source(tree_graph))","1bc47ef9":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=features, feature='26')\n\n# plot it\npdp.pdp_plot(pdp_goals, '26')\nplt.show()","a56213c8":"# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=rfc_model, dataset=val_X, model_features=features, feature='264')\n\n# plot it\npdp.pdp_plot(pdp_goals, '264')\nplt.show()","be312a81":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(rfc_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(X_train)","92423acc":"shap.summary_plot(shap_values, X_train)","295c7a38":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], X_train.iloc[:,1:10])","3fb9b9fe":"# based on following kernel https:\/\/www.kaggle.com\/dromosys\/sctp-working-lgb\nparams = {'num_leaves': 9,\n         'min_data_in_leaf': 42,\n         'objective': 'binary',\n         'max_depth': 16,\n         'learning_rate': 0.0123,\n         'boosting': 'gbdt',\n         'bagging_freq': 5,\n         'bagging_fraction': 0.8,\n         'feature_fraction': 0.8201,\n         'bagging_seed': 11,\n         'reg_alpha': 1.728910519108444,\n         'reg_lambda': 4.9847051755586085,\n         'random_state': 42,\n         'metric': 'auc',\n         'verbosity': -1,\n         'subsample': 0.81,\n         'min_gain_to_split': 0.01077313523861969,\n         'min_child_weight': 19.428902804238373,\n         'num_threads': 4}","84e2dd57":"## based on following kernel https:\/\/www.kaggle.com\/dromosys\/sctp-working-lgb\ny_pred_lgb = np.zeros(len(X_test))\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_valid, label=y_valid)\n        \n    lgb_model = lgb.train(params,train_data,num_boost_round=2000,#change 20 to 2000\n                    valid_sets = [train_data, valid_data],verbose_eval=300,early_stopping_rounds = 200)##change 10 to 200\n            \n    y_pred_lgb += lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\/5","f95c0beb":"y_pred_rfc = rfc_model.predict(X_test)","4c54d648":"y_pred_tree = tree_model.predict(X_test)","9236e023":"train_pool = Pool(train_X,train_y)\ncat_model = CatBoostClassifier(\n                               iterations=3000,# change 25 to 3000 to get best performance \n                               learning_rate=0.03,\n                               objective=\"Logloss\",\n                               eval_metric='AUC',\n                              )\ncat_model.fit(train_X,train_y,silent=True)\ny_pred_cat = cat_model.predict(X_test)","04e7ddc0":"from sklearn.ensemble import GradientBoostingClassifier\nModel=GradientBoostingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)","d5656b3d":"submission_rfc = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_pred_rfc\n    })\nsubmission_rfc.to_csv('submission_rfc.csv', index=False)","d8de2085":"submission_tree = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_pred_tree\n    })\nsubmission_tree.to_csv('submission_tree.csv', index=False)","acfc9d61":"submission_cat = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_pred_cat\n    })\nsubmission_cat.to_csv('submission_cat.csv', index=False)","6fe59b3a":"submission_lgb = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_pred_lgb\n    })\nsubmission_lgb.to_csv('submission_lgb.csv', index=False)","91dd0ceb":"**<< Note 1 >>**\n\n* Each row is an observation (also known as : sample, example, instance, record)\n* Each column is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)","09c33fcf":"<a id=\"74\"><\/a> <br>\n## 7-4 SHAP Values\nSHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see the SHAP NIPS paper for details).\n\n\n<img src='https:\/\/raw.githubusercontent.com\/slundberg\/shap\/master\/docs\/artwork\/shap_diagram.png' width=400 height=400  >\n\n[image-Credits](https:\/\/github.com\/slundberg\/shap)","a572b3e6":"For the sake of explanation, I use a Decision Tree which you can see below.","f6c2ed90":" <a id=\"32\"><\/a> <br>\n## 3-2 Visualization","02f3013b":"Here is how to calculate and show importances with the [eli5](https:\/\/eli5.readthedocs.io\/en\/latest\/) library:","d8a858a0":"<a id=\"21\"><\/a> <br>\n### 2-1 Version","9e92ce62":"This section is based on this [course](https:\/\/www.kaggle.com\/dansbecker\/shap-values) on kaggle.\n\n1. SHAP Values (an acronym from SHapley Additive exPlanations) break down a prediction to show the impact of each feature. Where could you use this?\n1. We 'll use SHAP Values to explain individual predictions in this kernel.","2b486f8e":"Go to first step: [**Course Home Page**](https:\/\/www.kaggle.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\nGo to next step : [**Mathematics and Linear Algebra**](https:\/\/www.kaggle.com\/mjbahmani\/linear-algebra-for-data-scientists)","d0301310":"<a id=\"81\"><\/a> <br>\n## 8-1 lightgbm","e1507238":"<a id=\"85\"><\/a> <br>\n## 8-5 Gradient Boosting Classifier","b8cc69a8":"<a id=\"323\"><\/a> \n### 3-2-3 countplot","76c9d166":"<a id=\"41\"><\/a> <br>\n## 4-1 Why Ensemble Learning?\n1. Difference in population\n1. Difference in hypothesis\n1. Difference in modeling technique\n1. Difference in initial seed\n<br>\n[go to top](#top)","16c3d0de":"After loading the data via **pandas**, we should checkout what the content is, description and via the following:\n<br>\n[go to top](#top)","87bc65dc":"<a id=\"72\"><\/a> <br>\n## 7-2 Partial Dependence Plots\nWhile feature importance shows what variables most affect predictions, partial dependence plots show how a feature affects predictions.[Credit](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability)","ae887f1d":"<a id=\"7-1-1\"><\/a> <br>\n## 7-1-1 What can be inferred from the above?\n1. As you move down the top of the graph, the importance of the feature decreases.\n1. The features that are shown in green indicate that they have a positive impact on our prediction\n1. The features that are shown in white indicate that they have no effect on our prediction\n1. The features shown in red indicate that they have a negative impact on our prediction","cb61d233":"### 3-2-5 Mean Frequency","b340b162":"<a id=\"327\"><\/a> \n### 3-2-7 violinplot","3b73f025":"<a id=\"8\"><\/a> <br>\n# 8- Model Development\nSo far, we have used two models, and at this point we add another model and we'll be expanding it soon. in this section you will see following model:\n\n1. lightgbm\n1. RandomForestClassifier\n1. DecisionTreeClassifier\n1. CatBoostClassifier","9115f763":"<a id=\"82\"><\/a> <br>\n## 8-2 RandomForestClassifier","a8c1cdad":"<a id=\"326\"><\/a> \n### 3-2-6 distplot\n the target in data set is **imbalance**","11ae237b":"<a id=\"6\"><\/a> <br>\n## 6- Some Ensemble  Model\nIn this section have been applied more than **8 learning algorithms** that play an important rule in your experiences and improve your knowledge in case of ML technique.\n\n> **<< Note 3 >>** : The results shown here may be slightly different for your analysis because, for example, the neural network algorithms use random number generators for fixing the initial value of the weights (starting points) of the neural networks, which often result in obtaining slightly different (local minima) solutions each time you run the analysis. Also note that changing the seed for the random number generator used to create the train, test, and validation samples can change your results.\n<br>\n[go to top](#top)","ac292b59":"<a id=\"73\"><\/a> <br>\n## 7-3 pdpbox","7f692731":"<a id=\"7\"><\/a> <br>\n# 7- Don't Overfit\nTo solve Don't Overfit problem, I would like to suggest that we first extract some insights  with using this [great course](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability) by [dansbecker](https:\/\/www.kaggle.com\/dansbecker). To understand this section, it's good idea to first read this course.","e2ead582":"<a id=\"62\"><\/a> <br>\n## 6-2 RandomForest\nA random forest is a meta estimator that **fits a number of decision tree classifiers** on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.[RandomForestClassifie](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html) \n\nThe sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).","a8f4080a":"<a id=\"3\"><\/a> \n## 3- Exploratory Data Analysis(EDA)\n In this section, we'll analysis how to use graphical and numerical techniques to begin uncovering the structure of your data. \n*  Data Collection\n*  Visualization\n*  Data Preprocessing\n*  Data Cleaning","a041ad85":"<a id=\"66\"><\/a> <br>\n## 6-6 Linear Discriminant Analysis\nLinear Discriminant Analysis (discriminant_analysis.LinearDiscriminantAnalysis) and Quadratic Discriminant Analysis (discriminant_analysis.QuadraticDiscriminantAnalysis) are two classic classifiers, with, as their names suggest, a **linear and a quadratic decision surface**, respectively.\n\nThese classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no **hyperparameters** to tune.","cbcd1a05":" # <div style=\"text-align: center\"> Tutorial on Ensemble Learning (Don't Overfit) <\/div>\n <img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/19\/Overfitting.svg\/800px-Overfitting.svg.png' width=400 height=400 >\n### <div style=\"text-align: center\"> CLEAR DATA. MADE MODEL <\/div>\n<div style=\"text-align:center\">last update: <b>06\/03\/2019<\/b><\/div>\n\n\n>You are reading **10 Steps to Become a Data Scientist** and are now in the 8th step : \n\n1. [Leren Python](https:\/\/www.kaggle.com\/mjbahmani\/the-data-scientist-s-toolbox-tutorial-1)\n2. [Python Packages](https:\/\/www.kaggle.com\/mjbahmani\/the-data-scientist-s-toolbox-tutorial-2)\n3. [Mathematics and Linear Algebra](https:\/\/www.kaggle.com\/mjbahmani\/linear-algebra-for-data-scientists)\n4. [Programming &amp; Analysis Tools](https:\/\/www.kaggle.com\/mjbahmani\/20-ml-algorithms-15-plot-for-beginners)\n5. [Big Data](https:\/\/www.kaggle.com\/mjbahmani\/a-data-science-framework-for-quora)\n6. [Data visualization](https:\/\/www.kaggle.com\/mjbahmani\/top-5-data-visualization-libraries-tutorial)\n7. [Data Cleaning](https:\/\/www.kaggle.com\/mjbahmani\/machine-learning-workflow-for-house-prices)\n8. <font color=\"red\">You are in the 8th step<\/font>\n9. [A Comprehensive ML  Workflow with Python](https:\/\/www.kaggle.com\/mjbahmani\/a-comprehensive-ml-workflow-with-python)\n10. [Deep Learning](https:\/\/www.kaggle.com\/mjbahmani\/top-5-deep-learning-frameworks-tutorial)\n\n\nyou can Fork and Run this kernel on <font color=\"red\">Github<\/font>:\n\n> [ GitHub](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\n **I hope you find this kernel helpful and some <font color='red'> UPVOTES<\/font> would be very much appreciated**\n \n -----------","35217c30":"<a id=\"61\"><\/a> <br>\n## 6-1 Prepare Features & Targets\nFirst of all seperating the data into dependent(Feature) and independent(Target) variables.\n\n**<< Note 4 >>**\n* X==>>Feature\n* y==>>Target","fba470c7":"<a id=\"65\"><\/a> <br>\n## 6-5 Gradient Boosting Classifier\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions.","77131808":"<a id=\"top\"><\/a> <br>\n## Notebook  Content\n1. [Introduction](#1)\n1. [Import packages](#2)\n    1. [Version](#21)\n    1. [Setup](#22)\n    1. [Data Collection](#23)\n1. [Exploratory Data Analysis(EDA)](#3)\n1. [What's Ensemble Learning?](#4)\n    1. [Why Ensemble Learning?](#41)\n1. [Ensemble Techniques](#5)\n    1. [what-is-the-difference-between-bagging-and-boosting?](#51)\n1. [Model Deployment](#6)\n    1. [Prepare Features & Targets](#61)\n    1. [RandomForest](#62)\n    1. [Bagging classifier ](#63)\n    1. [AdaBoost](#64)\n    1. [Gradient Boosting Classifier](#65)\n    1. [Linear Discriminant Analysis](#66)\n    1. [Quadratic Discriminant Analysis](#67)\n1. [Don't Overfit](#7)\n    1. [Feature Importance](#71)\n    1. [Partial Dependence Plots](#72)\n    1. [pdpbox](#73)\n    1. [SHAP Values](#74)\n1. [Model Development](#8)\n    1. [lightgbm](#81)\n    1. [RandomForestClassifier](#82)\n    1. [ DecisionTreeClassifier](#83)\n    1. [CatBoostClassifier](#84)\n1. [References & Credits](#9)","45ce5f5a":"<a id=\"321\"><\/a> \n### 3-2-1 hist","6df22fdd":"<a id=\"84\"><\/a> <br>\n## 8-4 CatBoostClassifier","7477c87f":"<a id=\"63\"><\/a> <br>\n## 6-3 Bagging classifier \nA Bagging classifier is an ensemble **meta-estimator** that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting . If samples are drawn with replacement, then the method is known as Bagging . When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces . Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches .[http:\/\/scikit-learn.org]\n<br>\n[go to top](#top)","af8c452a":"Now you can change your model and submit the results of other models.","6105440d":"<a id=\"64\"><\/a> <br>\n##  6-4 AdaBoost classifier\n\nAn AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\nThis class implements the algorithm known as **AdaBoost-SAMME** .","4f380148":" <a id=\"31\"><\/a> <br>\n## 3-1 Data Collection","694fa8f6":"<a id=\"67\"><\/a> <br>\n## 6-7 Quadratic Discriminant Analysis\nA classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.\n\nThe model fits a **Gaussian** density to each class.","df2a98f1":"1. [datacamp](https:\/\/www.datacamp.com\/community\/tutorials\/xgboost-in-python)\n1. [Github](https:\/\/github.com\/mjbahmani)\n1. [analyticsvidhya](https:\/\/www.analyticsvidhya.com\/blog\/2015\/08\/introduction-ensemble-learning\/)\n1. [ensemble-learning-python](https:\/\/www.datacamp.com\/community\/tutorials\/ensemble-learning-python)\n1. [image-header-reference](https:\/\/data-science-blog.com\/blog\/2017\/12\/03\/ensemble-learning\/)\n1. [scholarpedia](http:\/\/www.scholarpedia.org\/article\/Ensemble_learning)\n1. [toptal](https:\/\/www.toptal.com\/machine-learning\/ensemble-methods-machine-learning)\n1. [quantdare](https:\/\/quantdare.com\/what-is-the-difference-between-bagging-and-boosting\/)\n1. [towardsdatascience](https:\/\/towardsdatascience.com\/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f)\n1. [scikit-learn](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html)\n1. [https:\/\/www.kaggle.com\/dansbecker\/permutation-importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance)\n1. [https:\/\/www.kaggle.com\/dansbecker\/partial-plots](https:\/\/www.kaggle.com\/dansbecker\/partial-plots)\n1. [https:\/\/www.kaggle.com\/dansbecker\/shap-values](https:\/\/www.kaggle.com\/dansbecker\/shap-values)\n1. [https:\/\/www.kaggle.com\/miklgr500\/catboost-with-gridsearch-cv](https:\/\/www.kaggle.com\/miklgr500\/catboost-with-gridsearch-cv)","5fae025b":"<a id=\"22\"><\/a> <br>\n### 2-2 Setup\n\nA few tiny adjustments for better **code readability**","0b65106e":"<a id=\"71\"><\/a> <br>\n## 7-1 Feature Importance","8ff147b5":"<a id=\"83\"><\/a> <br>\n## 8-3 DecisionTreeClassifier","182e38c7":"you can follow me on:\n> ###### [ GitHub](https:\/\/github.com\/mjbahmani)\n> ###### [Kaggle](https:\/\/www.kaggle.com\/mjbahmani\/)\n\n  **I hope you find this kernel helpful and some <font color='red'> UPVOTES<\/font> would be very much appreciated**\n ","99e93d77":"<a id=\"324\"><\/a> \n### 3-2-4 hist\nif you check histogram for all feature, you will find that most of them are so similar","d3d223d3":"<a id=\"4\"><\/a> <br>\n## 4- What's Ensemble Learning?\nlet us, review some defination on Ensemble Learning:\n\n1. **Ensemble learning** is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem[9]\n1. **Ensemble Learning** is a powerful way to improve the performance of your model. It usually pays off to apply ensemble learning over and above various models you might be building. Time and again, people have used ensemble models in competitions like Kaggle and benefited from it.[6]\n1. **Ensemble methods** are techniques that create multiple models and then combine them to produce improved results. Ensemble methods usually produces more accurate solutions than a single model would.[10]\n<img src='https:\/\/hub.packtpub.com\/wp-content\/uploads\/2018\/02\/ensemble_machine_learning_image_1-600x407.png'  width=400 height=400>\n[img-ref](https:\/\/hub.packtpub.com\/wp-content\/uploads\/2018\/02\/ensemble_machine_learning_image_1-600x407.png)\n\n> <font color=\"red\"><b>Note<\/b><\/font>\nEnsemble Learning is a Machine Learning concept in which the idea is to train multiple models using the same learning algorithm. The ensembles take part in a bigger group of methods, called multiclassifiers, where a set of hundreds or thousands of learners with a common objective are fused together to solve the problem.[11]\n\n> <font color=\"red\"><b>Note<\/b><\/font>\nThis Kernel assumes a basic understanding of Machine Learning algorithms. I would recommend going through this [**kernel**](https:\/\/www.kaggle.com\/mjbahmani\/a-comprehensive-ml-workflow-with-python)  to familiarize yourself with these concepts.\n\n[go to top](#top)","283d844d":"<a id=\"5\"><\/a> <br>\n# 5- Ensemble Techniques\nThe goal of any machine learning problem is to find a single model that will best predict our wanted outcome. Rather than making one model and hoping this model is the best\/most accurate predictor we can make, ensemble methods take a myriad of models into account, and average those models to produce one final model.[12]\n<img src='https:\/\/uploads.toptal.io\/blog\/image\/92062\/toptal-blog-image-1454584029018-cffb1b601292e8d328556e355ed4f7e0.jpg' width=300 height=300>\n[img-ref](https:\/\/www.toptal.com\/machine-learning\/ensemble-methods-machine-learning)\n1. Voting\n1. Weighted Average \n1. Stacking\n1. Blending\n1. Bagging  \n1. Boosting ","c64e56a7":"<a id=\"2\"><\/a> <br>\n## 2- Import packages","74197c52":"<a id=\"51\"><\/a> <br>\n## 5-1 What is the difference between bagging and boosting?\n1. **Bagging**: It is the method to decrease the variance of model by generating additional data for training from your original data set using combinations with repetitions to produce multisets of the same size as your original data.\n    1. Bagging meta-estimator\n    1. Random forest\n1. **Boosting**: It helps to calculate the predict the target variables using different models and then average the result( may be using a weighted average approach).\n    1. AdaBoost\n    1. GBM\n    1. XGBM\n    1. Light GBM\n    1. CatBoost\n    \n<img src='https:\/\/www.globalsoftwaresupport.com\/wp-content\/uploads\/2018\/02\/ds33ggg.png'>\n[Image-Credit](https:\/\/www.globalsoftwaresupport.com\/boosting-adaboost-in-machine-learning\/)\n<br>\n[go to top](#top)","6af18118":"<a id=\"1\"><\/a> <br>\n#  1- Introduction\nIn this kernel, I want to start explorer everything about **Ensemble modeling** with focus on **Overfit**. I will run plenty of algorithms on Don't Overfit dataset. I hope you enjoy and give me feedback.","f5277d19":"<a id=\"9\"><\/a> <br>\n# 9-References & Credits"}}