{"cell_type":{"e43ab59a":"code","3a721763":"code","aa363b4c":"code","1e3d395c":"code","a1863ad5":"code","dd332077":"code","b8b3c2b9":"code","257d6820":"markdown","6c1049da":"markdown","ca600bc5":"markdown","95466c1a":"markdown","cd47203b":"markdown","bdfef435":"markdown","879ee9c1":"markdown"},"source":{"e43ab59a":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np","3a721763":"dataframe = pd.read_csv('..\/input\/diabetescsv\/diabetes.csv')\ndataframe","aa363b4c":"X = dataframe.drop('Outcome',axis=1)\nY = dataframe['Outcome']\nX,Y","1e3d395c":"from keras.models import Sequential\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\n\n\nmodel = Sequential()\nmodel.add(tf.keras.layers.Reshape((8,),input_shape=(8,)))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","a1863ad5":"epochs = 50\nlearning_rate = 0.1\ndecay_rate = learning_rate \/ epochs\nmomentum = 0.8\nsgd = tf.keras.optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])","dd332077":"from sklearn.model_selection import train_test_split\n(X_train,X_test,y_train,y_test)=train_test_split(X,Y,test_size=0.2)","b8b3c2b9":"model.fit(X_train,y_train,          \n          validation_data=(X_test,y_test),\n          epochs=100,\n          batch_size=32)","257d6820":"# Split into input (X) and output (Y) variables","6c1049da":"# Import Dataset","ca600bc5":"# Time-Based Learning Rate Schedule","95466c1a":"# Compile model","cd47203b":"# Fit the model","bdfef435":"# **Introduction**\n\nLearning rate schedules seek to adjust the learning rate during training by reducing the learning rate according to a pre-defined schedule. Common learning rate schedules include time-based decay, step decay and exponential decay. Simply putting Learning Rate is a value which basically helps us to adjust the pace of learning. Now in Machine Learning what the machine is going to learn? Weights of course.!! Thus, using Learning Rate, we can control how much we are going to adjust the weights with respect to loss gradient and learn eventually. Now who will decide the value.\nWell, it\u2019s a hyperparameter that means we are going to decide the value. Looks like a Herculean task, isn\u2019t it? How are we going to decide?\n\/\n\nHere we are discussing about Time-Based Learning Rate Schedule","879ee9c1":"# Create model"}}