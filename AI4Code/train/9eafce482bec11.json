{"cell_type":{"1a352f7a":"code","05674f75":"code","3af2f7b7":"code","5e9e839c":"code","31a8cf94":"code","3bdf718c":"code","455c273f":"code","cb92df8d":"code","3ef82fec":"code","0286b4ce":"code","fee99c76":"code","5fc86df6":"code","2127d926":"code","96b34610":"code","52a5688c":"code","cb9e6d81":"code","3aa6aa76":"code","42f9e7d1":"markdown","b056b247":"markdown","c78dcd06":"markdown","b8dfc593":"markdown","a6027af2":"markdown","279af392":"markdown","68c9a4b1":"markdown","0360ca76":"markdown","4d1efb4f":"markdown","13da7d43":"markdown","9ddb77ce":"markdown","118eb029":"markdown","8c1c8add":"markdown","963f8979":"markdown"},"source":{"1a352f7a":"from sklearn import model_selection, preprocessing, linear_model, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import accuracy_score\n\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Matplotlib forms basis for visualization in Python\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# We will use the Seaborn library\nimport seaborn as sns\nsns.set()\n\nimport os, string\n# Graphics in SVG format are more sharp and legible\n%config InlineBackend.figure_format = 'svg'\nprint(os.listdir('..\/input\/'))","05674f75":"df = pd.read_csv('..\/input\/train.csv')\ndf.head()","3af2f7b7":"test_df = pd.read_csv('..\/input\/test.csv')","5e9e839c":"df['sentiment'].value_counts()\n\nsns.countplot(x='sentiment', data=df)\n","31a8cf94":"df['char_count'] = df['review'].apply(len)\ndf['word_count'] = df['review'].apply(lambda x: len(x.split()))\ndf['word_density'] = df['char_count'] \/ (df['word_count']+1)\ndf['punctuation_count'] = df['review'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) ","3bdf718c":"df.head()","455c273f":"#df.hist(column=['char_count', 'word_count'])\nfeatures = ['char_count', 'word_count', 'word_density', 'punctuation_count']\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 6), sharey=True)\n\nfor i, feature in enumerate(features):\n    df.hist(column=feature, ax=axes.flatten()[i])\n","cb92df8d":"df[features].describe()","3ef82fec":"fig, ax = plt.subplots()\nsns.boxplot(df['word_count'], order=range(0,max(df['word_count'])), ax=ax)\n\nax.xaxis.set_major_locator(ticker.MultipleLocator(200))\nax.xaxis.set_major_formatter(ticker.ScalarFormatter())\nfig.set_size_inches(8, 4)\nplt.show()","0286b4ce":"from sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nX_train, X_val, y_train, y_val = train_test_split(df['review'], df['sentiment'], test_size=0.3,\nrandom_state=17)","fee99c76":"# Converting X_train and X_val to tfidf vectors (since out models can't take text data is input)\ntfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(df['review'])\nxtrain_tfidf =  tfidf_vect.transform(X_train)\nxvalid_tfidf =  tfidf_vect.transform(X_val)\n\n# ngram level tf-idf \ntfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\ntfidf_vect_ngram.fit(df['review'])\nxtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\nxvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_val)\n\n# characters level tf-idf\ntfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\ntfidf_vect_ngram_chars.fit(df['review'])\nxtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \nxvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_val) \n\n# Also creating for the X_test which is essentially test_df['review'] column\nxtest_tfidf =  tfidf_vect.transform(test_df['review'])\nxtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_df['review'])\nxtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_df['review']) ","5fc86df6":"# create a count vectorizer object \ncount_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\ncount_vect.fit(df['review'])\n\n# transform the training and validation data using count vectorizer object\nxtrain_count =  count_vect.transform(X_train)\nxvalid_count =  count_vect.transform(X_val)\nxtest_count = count_vect.transform(test_df['review'])","2127d926":"model1 = linear_model.LogisticRegression()\nmodel1.fit(xtrain_count, y_train)\naccuracy=model1.score(xvalid_count, y_val)\nprint('Accuracy Count LR:', accuracy)\ntest_pred1=model1.predict(xtest_count)\n\nmodel2 = linear_model.LogisticRegression()\nmodel2.fit(xtrain_tfidf, y_train)\naccuracy=model2.score(xvalid_tfidf, y_val)\nprint('Accuracy TFIDF LR:', accuracy)\ntest_pred2=model2.predict(xtest_tfidf)\n\nmodel3 = linear_model.LogisticRegression()\nmodel3.fit(xtrain_tfidf_ngram, y_train)\naccuracy = model3.score(xvalid_tfidf_ngram, y_val)\nprint('Accuracy TFIDF NGRAM LR:', accuracy)\ntest_pred3 = model3.predict(xtest_tfidf_ngram)","96b34610":"final_pred = np.array([])\nfor i in range(0,len(test_df['review'])):\n    final_pred = np.append(final_pred, np.argmax(np.bincount([test_pred1[i], test_pred2[i], test_pred3[i]])))","52a5688c":"sub_df = pd.DataFrame()\nsub_df['Id'] = test_df['Id']\nsub_df['sentiment'] = [int(i) for i in final_pred]","cb9e6d81":"sub_df.head()","3aa6aa76":"sub_df.to_csv('my_submission.csv', index=False)","42f9e7d1":"As we can see from the boxplot, the 50th percentile of the word_count is somewhere below 200. However there are still a large number of outliers. We can also later try to train a model without the outliers and try to check it's accuracy.","b056b247":"Let's display a boxplot of the word count to get an intuition about the average word count of a review.","c78dcd06":"We also create the Count vectors","b8dfc593":"### Step 1: Imports","a6027af2":"### Step 2: Let's Import and Visualize the data","279af392":"It's clear from the plots that the averate word density is between 4-6. Moreover, all three of char_count, word_count and punctuation_count have no irregularities and have a similar disrtibution (which they should have as they are correlated).","68c9a4b1":"### Step 5: Creating Submission DF ","0360ca76":"> Don't use a hammer to kill a fly\n\nSince our dataset isn't that large we don't need complex neural networks and Bidirectional LSTMS to solve this task. Simple Machine Learning models with some appropriate preprocessing will yield the best results.\n\n\nIn this kernel, I will walk you through my approach of solving this problem by first performing some Exploratory Data Analysis and then training some models on it.\n\n\n**Note**: I know the hackathon required us to solve the challenge using Pytorch. That's why I submitted my kernel by training a 3-Layer Dense NN using Pytorch. But this model led to a lower accuracy than what the models I use in this kernel will get. That is why I think it is best to change the rule for the hackathons and not make it mandatory to use Pytorch","4d1efb4f":"As we can see, the dataset is already balanced with 17500 example for both negative and positive reviews.\n\nLet's create some more features and visualize them to understand the data distribution.","13da7d43":"Here we try three types of preprocessing,\n1. Converting the text to tfidf word vectors.\n2. Converting the text to tfidf ngram vectors.\n3. Converting the text to tfidf character vectors.","9ddb77ce":"### Step 4: Creating three models and calculating their accuracy","118eb029":"We will ensemble these 3 models, i.e take the most agreed upon label as the true label.","8c1c8add":"> ### Step 3: Creating the Train and Validation Dataset and preparing them for the models","963f8979":"## Sentiment Analysis"}}