{"cell_type":{"c2c638d5":"code","206d2fc9":"code","65204ec9":"code","c8d5a8e5":"code","cb38db14":"code","60a0ecbf":"code","36fa55a3":"code","3ae19841":"code","546700e1":"code","545e52fa":"code","48b4a5be":"code","b6ae3248":"code","60d93ea0":"code","6b21cce7":"code","56cec131":"code","e9709850":"code","0da9a238":"code","737440f1":"code","3fc9480f":"code","ab3d353f":"code","08553b57":"code","269b9853":"code","646b7408":"code","b0bea8bf":"code","854cf439":"code","1d6cd74b":"code","68f04db8":"code","393dd0d4":"code","4dba92ba":"code","66f6d912":"code","7ca5f948":"code","d19db39d":"code","b2d64f96":"code","2973d48c":"code","c6095a37":"code","8b74c4c3":"code","f691826f":"code","ba804f5c":"code","c590470c":"code","77daf574":"code","d05f219d":"code","86e6666a":"code","d713f4f7":"code","3a2c1c7b":"code","41fb2166":"code","f51bb27a":"code","e7e41d4a":"code","d5247256":"code","535782e5":"code","dfd2a86e":"markdown","eac70199":"markdown","c156a560":"markdown","bdaab712":"markdown","bacedeb3":"markdown","131e1c4d":"markdown","8d545a6f":"markdown","67ac7660":"markdown","e27ed45e":"markdown","90bf3276":"markdown","778cb220":"markdown","e5eb592d":"markdown","b2e211bd":"markdown","14672369":"markdown","eaf44a35":"markdown"},"source":{"c2c638d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport gc\n\nimport datetime\n\n# Any results you write to the current directory are saved as output.","206d2fc9":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","65204ec9":"train = reduce_mem_usage(pd.read_csv('..\/input\/train.csv'))\ntest = reduce_mem_usage(pd.read_csv('..\/input\/test.csv'))\ntrain.head()","c8d5a8e5":"train.describe()","cb38db14":"test.describe()","60a0ecbf":"nm_df = reduce_mem_usage(pd.read_csv('..\/input\/new_merchant_transactions.csv'))\nprint(nm_df.shape)\nnm_df.head()","36fa55a3":"nm_df['purchase_date'] = pd.to_datetime(nm_df.purchase_date)\n\nnm_df['purch_year'] = nm_df.purchase_date.dt.year\nnm_df['purch_mon'] = nm_df.purchase_date.dt.month\nnm_df['purch_dow'] = nm_df.purchase_date.dt.dayofweek\nnm_df['purch_wk'] = nm_df.purchase_date.dt.week\nnm_df['purch_day'] = nm_df.purchase_date.dt.dayofyear\n\n## adding in a few more features - 2.6.19\n## source: https:\/\/www.kaggle.com\/chauhuynh\/my-first-kernel-3-699\nnm_df['purch_woy'] = nm_df.purchase_date.dt.weekofyear\nnm_df['purch_wknd'] = (nm_df.purchase_date.dt.weekday >=5).astype(int)\nnm_df['purch_hr'] = nm_df.purchase_date.dt.hour\n#https:\/\/www.kaggle.com\/c\/elo-merchant-category-recommendation\/discussion\/73244\nnm_df['purch_month_diff'] = ((datetime.datetime.today() - nm_df.purchase_date).dt.days)\/\/30\nnm_df['purch_month_diff'] += nm_df['month_lag']","3ae19841":"nm_df.head()","546700e1":"m_df = reduce_mem_usage(pd.read_csv('..\/input\/merchants.csv'))\nprint(m_df.shape)\nm_df.head()","545e52fa":"nmm_df = nm_df.merge(m_df, on='merchant_id', how='outer')\nnmm_df.shape","48b4a5be":"for c in nmm_df.columns:\n    if nmm_df[c].dtype == 'object':\n        vcs = nmm_df[c].value_counts()\n        if len(vcs) < 20: ## checking if theres a lot of unique values\n            print(vcs)\n#     print(c,'->',nmm_df[c].dtype)","b6ae3248":"nmm_df['authorized_flag'] = (nmm_df.authorized_flag == 'Y').astype(int)\nnmm_df['category_1_x'] = (nmm_df.category_1_x == 'Y').astype(int)\nnmm_df['purchase_date'] = pd.to_datetime(nmm_df.purchase_date)\nnmm_df['category_1_y'] = (nmm_df.category_1_y == 'Y').astype(int)\nnmm_df['category_4'] = (nmm_df.category_4 == 'Y').astype(int)","60d93ea0":"mrpr = pd.get_dummies(nmm_df.most_recent_purchases_range)\nmrsr = pd.get_dummies(nmm_df.most_recent_sales_range)\ncat3 = pd.get_dummies(nmm_df.category_3)","6b21cce7":"for d in [mrpr, mrsr, cat3]:\n    for c in d.columns:\n        nmm_df[c] = d[c]\n        \nnmm_df.drop(['most_recent_purchases_range','most_recent_sales_range','category_3'], axis=1, inplace=True)\n\ntry:\n    del mrpr, mrsr, cat3, nm_df, m_df\nexcept:\n    pass\ngc.collect()","56cec131":"nmm_df.isnull().sum()\/len(nmm_df)","e9709850":"## fill NA with most common value\nfor c in nmm_df.columns:\n    if nmm_df[c].isnull().sum()\/len(nmm_df[c]) > 0:\n        nmm_df[c] = nmm_df[c].fillna(nmm_df[c].value_counts().index[0])\n        \n","0da9a238":"# nmm_df.isnull().sum()\/len(nmm_df)","737440f1":"## factorize merchant id\n\nnmm_df['merchant_id'] = pd.factorize(nmm_df['merchant_id'])[0]","3fc9480f":"nmm_agg = nmm_df.groupby('card_id').agg(['sum','mean','max','min','var',\n                                        'median','count','skew','nunique',#'mode'\n                                       ])\nprint(nmm_agg.shape)\n\ntry:\n    del nmm_df\nexcept:\n    pass\ngc.collect()","ab3d353f":"hist = reduce_mem_usage(pd.read_csv('..\/input\/historical_transactions.csv'))\nhist.head()","08553b57":"hist['authorized_flag'] = (hist.authorized_flag == 'Y').astype(int)\nhist['category_1'] = (hist.category_1 == 'Y').astype(int)","269b9853":"cat3 = pd.get_dummies(hist.category_3)\n\nfor c in cat3.columns:\n    hist[c] = cat3[c]\nhist.drop('category_3',axis=1, inplace=True)\n\ndel cat3","646b7408":"hist['purchase_date'] = pd.to_datetime(hist.purchase_date)\n\nhist['histpurch_year'] = hist.purchase_date.dt.year\nhist['histpurch_mon'] = hist.purchase_date.dt.month\nhist['histpurch_dow'] = hist.purchase_date.dt.dayofweek\nhist['histpurch_wk'] = hist.purchase_date.dt.week\nhist['histpurch_day'] = hist.purchase_date.dt.dayofyear\n\n## adding in a few more features - 2.6.19\n## source: https:\/\/www.kaggle.com\/chauhuynh\/my-first-kernel-3-699\nhist['histpurch_woy'] = hist.purchase_date.dt.weekofyear\nhist['histpurch_wknd'] = (hist.purchase_date.dt.weekday >=5).astype(int)\nhist['histpurch_hr'] = hist.purchase_date.dt.hour\n#https:\/\/www.kaggle.com\/c\/elo-merchant-category-recommendation\/discussion\/73244\nhist['histpurch_month_diff'] = ((datetime.datetime.today() - hist.purchase_date).dt.days)\/\/30\nhist['histpurch_month_diff'] += hist['month_lag']","b0bea8bf":"print(hist.shape)","854cf439":"hist.isnull().sum()\/len(hist)","1d6cd74b":"## fill NA with most common value\nfor c in hist.columns:\n    if hist[c].isnull().sum()\/len(hist[c]) > 0:\n        hist[c] = hist[c].fillna(hist[c].value_counts().index[0])\n        \n\n#factorize merchant id\nhist['merchant_id'] = pd.factorize(hist['merchant_id'])[0]","68f04db8":"hist_agg = hist.groupby('card_id').agg(['sum','mean','max','min','var',\n                                        'median','count','skew','nunique',#'mode'\n                                       ])\nprint(hist_agg.shape)","393dd0d4":"try:\n    del hist\nexcept:\n    pass\ngc.collect()","4dba92ba":"train_agg = train.set_index('card_id').join(hist_agg, how='left').join(nmm_agg, how='left', rsuffix='_nmm').fillna(0)\nprint(train_agg.shape)","66f6d912":"test_agg = test.set_index('card_id').join(hist_agg, how='left').join(nmm_agg, how='left', rsuffix='_nmm').fillna(0)\nprint(test_agg.shape)","7ca5f948":"train_agg.columns = [''.join(col).strip() for col in train_agg.columns.values]\ntest_agg.columns = [''.join(col).strip() for col in test_agg.columns.values]\ntrain_agg.columns = train_agg.columns.str.replace(' ','')\ntest_agg.columns = test_agg.columns.str.replace(' ','')\ntrain_agg.head(3)","d19db39d":"# train_agg = train_agg.reset_index()\ntry:\n    del train, test\nexcept:\n    pass\n\ngc.collect()\n","b2d64f96":"train_agg['month'] = train_agg['first_active_month'].str[-2:]\ntrain_agg['year'] = train_agg['first_active_month'].str[:-3]\ntrain_agg.drop(['first_active_month'],axis=1,inplace=True)\ntrain_agg['month'] = train_agg.month.astype(int)\ntrain_agg['year'] = train_agg.year.astype(int)","2973d48c":"test_agg['month'] = test_agg['first_active_month'].str[-2:]\ntest_agg['year'] = test_agg['first_active_month'].str[:-3]\ntest_agg.drop(['first_active_month'],axis=1,inplace=True)\ntest_agg['month'] = test_agg.month.fillna(0).astype(int)\ntest_agg['year'] = test_agg.year.fillna(0).astype(int)","c6095a37":"for c in train_agg.columns:\n    if np.isinf(train_agg[c]).sum()\/len(train_agg[c]) > 0:\n        print(c)\n        train_agg[c] = train_agg[c].replace([np.inf, -np.inf], train_agg[c].value_counts().index[0])","8b74c4c3":"for c in test_agg.columns:\n    if np.isinf(test_agg[c]).sum()\/len(test_agg[c]) > 0:\n        print(c)\n        test_agg[c] = test_agg[c].replace([np.inf, -np.inf], test_agg[c].value_counts().index[0])","f691826f":"for c in train_agg.columns:\n    if train_agg[c].isnull().sum()\/len(train_agg[c]) > 0:\n        print(c)\n        train_agg[c] = train_agg[c].fillna(train_agg[c].value_counts().index[0])","ba804f5c":"for c in test_agg.columns:\n    if test_agg[c].isnull().sum()\/len(test_agg[c]) > 0:\n        print(c)\n        test_agg[c] = test_agg[c].fillna(test_agg[c].value_counts().index[0])","c590470c":"from sklearn.model_selection import StratifiedKFold, RepeatedKFold\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import HuberRegressor, LassoLars, RANSACRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor","77daf574":"## trying out various methods to grrab the outliers\n## The Huber Regressor errored out. Lars didnt improve anything nor did RANSAC.\n## Also trying a gradient boosting decision stump with huber loss\n##   GBT stumps tend to fit to outliers more as well\n##      On v18, we will do a little bigger than a stump w maxdepth of 2\n\n## Nothing seemed to make improvements. See comments below.\n\n# llars = LassoLars(alpha=0.9, fit_intercept=True, normalize=True, \n#                   precompute=\"auto\", max_iter=500)\n\n# rr = RANSACRegressor(#llars,\n#                      random_state=123)\n\n# gbr = GradientBoostingRegressor(loss='huber',\n#                                 learning_rate=0.01, #0.02,\n#                                 n_estimators=1000, #500,\n#                                 subsample=0.8, \n#                                 max_depth=2, #1, \n#                                 random_state=123, max_features=None, alpha=0.9,\n#                                 validation_fraction=0.1)\n\ngc.collect()","d05f219d":"# del x0, y, trn_data, val_data\ngc.collect()","86e6666a":"folds = StratifiedKFold(n_splits = 5, shuffle = True)\ntrain_predictions = np.zeros(len(train_agg))\ntest_predictions = np.zeros(len(test_agg))\nlin_preds = np.zeros(len(test_agg))\nn_fold = 0\nfor train_index, test_index in folds.split(train_agg, train_agg['feature_1']):\n    ## from https:\/\/www.kaggle.com\/mfjwr1\/simple-lightgbm-without-blending\n    n_fold += 1\n    param ={\n        'task': 'train',\n        'boosting': 'goss',\n        'objective': 'regression',\n        'metric': 'rmse',\n        'learning_rate': 0.01,\n        'subsample': 0.9855232997390695,\n        'max_depth': 7,\n        'top_rate': 0.9064148448434349,\n        'num_leaves': 63,\n        'min_child_weight': 41.9612869171337,\n        'other_rate': 0.0721768246018207,\n        'reg_alpha': 9.677537745007898,\n        'colsample_bytree': 0.5665320670155495,\n        'min_split_gain': 9.820197773625843,\n        'reg_lambda': 8.2532317400459,\n        'min_data_in_leaf': 21,\n        'verbose': -1,\n        'seed':int(2**n_fold),\n        'bagging_seed':int(2**n_fold),\n        'drop_seed':int(2**n_fold)\n        }\n    \n    y = train_agg['target']\n    x0 = train_agg.drop('target', axis = 1)\n    \n    trn_data = lgb.Dataset(x0.iloc[train_index], label=y.iloc[train_index])\n    val_data = lgb.Dataset(x0.iloc[test_index], label=y.iloc[test_index])\n    \n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=-1, early_stopping_rounds = 200)\n    train_predictions[test_index] = clf.predict(x0.iloc[test_index], num_iteration=clf.best_iteration)\n\n    test_predictions += clf.predict(test_agg, num_iteration=clf.best_iteration) \/ folds.n_splits\n    \n##     llars.fit(x0.iloc[train_index], y.iloc[train_index])\n##     rr.fit(x0.iloc[train_index], y.iloc[train_index])\n#     gbr.fit(x0.iloc[train_index], y.iloc[train_index]) ## Decision Stump on Huber to Grab Outliers?\n#     lin_preds += gbr.predict(test_agg)\/folds.n_splits\n\nprint(\"LGB Train Error:\",np.sqrt(mean_squared_error(train_predictions, y)))\ngc.collect()","d713f4f7":"# pd.concat([pd.DataFrame(test_predictions[:10], columns=['lgb']),\n#            pd.DataFrame(lin_preds[:10], columns=['lin'])], axis=1, join='outer')\n\nprint(test_predictions[:10])\n# print('\/n')\n# print(lin_preds[:10])","3a2c1c7b":"# combined_preds = 0.9*test_predictions + 0.1*lin_preds\n# combined_preds","41fb2166":"predictions = pd.DataFrame(\n    data = {\n        'card_id' : test_agg.index, #test_agg['card_id'],\n        ## I kept trying to improve the model by using a linear method to grab the outliers\n        'target' : test_predictions, #combined_preds, \n    }\n)\npredictions.to_csv('submit.csv', index = False) ","f51bb27a":"predictions.head(10)","e7e41d4a":"len(predictions)","d5247256":"import matplotlib.pyplot as plt","535782e5":"fig,ax=plt.subplots(1,1,figsize=(14,12))\nlgb.plot_importance(clf, max_num_features=50, ax=ax);","dfd2a86e":"## Clean up the column names some....","eac70199":"Clean up....","c156a560":"## Bring in the merchant data, merge, clean up and aggergate....","bdaab712":"## Bring in Train and Test and do a quick check...","bacedeb3":"## Bring in the Historical data, clean up and aggregate....","131e1c4d":"## Light GBM model with a Linear Estimator for the outliers?","8d545a6f":"__Remove Infs?__","67ac7660":"## Merge the train and test dataframes and aggergated mercant and historical data together...","e27ed45e":"__Update 2.7.19__\n\n__I kept trying to improve the model by using some method to grab the outliers but nothing seemed to do better than Light GBM by iteself.__","90bf3276":"The idea behind this notebook is to have a little fun using pandas data aggregation and light gbm.\n\nWe are predicting these **loyalty scores** so we will be focusing on RMSE.\n\nWe will move through each of the data csv's listed above and attempt to aggegate and merge them into one data set.","778cb220":"## A little more memory clean up and some other data clean up....","e5eb592d":"Clean up...","b2e211bd":"So our training data set has a target and for the purposes of a predicting index, we would be using the **CARD ID**.","14672369":"Check some data types and unique values...","eaf44a35":"This memory reducer is from:\n- https:\/\/www.kaggle.com\/ashishpatel26\/lightgbm-gbdt-dart-baysian-ridge-reg-lb-3-61\n"}}