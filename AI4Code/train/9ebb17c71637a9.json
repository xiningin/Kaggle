{"cell_type":{"5cb7b810":"code","98279804":"code","e4e09328":"code","575972c2":"code","710cdf39":"code","596a9d9e":"code","9d8fa3c2":"code","0f3687eb":"code","520ed536":"code","06a150cb":"code","81c5e1a5":"code","03769c09":"code","17c2c627":"code","d934d6e4":"code","5a8b4e4a":"code","3d764d9d":"code","99e36c0c":"code","018bb9b3":"code","513e9a41":"code","b310cdef":"code","1e1ae4f3":"code","56511dae":"code","49628ac7":"code","43ff24d5":"code","b626eefd":"code","6522300e":"code","81b87beb":"code","6b9f56cb":"code","6750c4dc":"code","25b51c77":"code","ff1ba296":"code","5c0dc757":"code","d11f6162":"code","c62f0c82":"code","c99d5be9":"code","d1677d96":"code","71f14024":"code","1ca1f4b8":"markdown","6e0792a7":"markdown","e4ba30ae":"markdown","3f2873cf":"markdown","794171b2":"markdown","695ef6a2":"markdown","932f09c4":"markdown","b1ae3ff4":"markdown","f508af07":"markdown","708d84c9":"markdown","952d4aad":"markdown","3ad66cab":"markdown","1cc91499":"markdown","d7b58389":"markdown","5f953f62":"markdown","e8d2d202":"markdown","7676ec24":"markdown"},"source":{"5cb7b810":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\npath_list = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        path_list.append(os.path.join(dirname, filename))\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","98279804":"!pip install --upgrade tensorflow -q","e4e09328":"import tensorflow as tf\ntf.__version__","575972c2":"def ready_to_use(dataframe, test_stat=False):\n    df = dataframe.copy()\n    passengerId = None\n\n    df.drop(['Ticket', 'Cabin', 'Fare'], axis=1, inplace=True)\n    if test_stat:\n        passengerId = df.pop('PassengerId')\n    else:\n        df.drop(['PassengerId'], axis=1, inplace=True)\n\n    # Catch Title information from a Name Column\n    df['Name'] = df['Name'].transform(lambda x: x.split('.')[0].split(', ')[1])\n    df.rename(columns = {'Name':'Title'}, inplace=True)\n    \n    # Find two na Values in Embarked column. So I convert that to 'C'. Two person that include na Values in Embarked \n    # have 1 value at Survived column. Most People who have 'C'  were survived.\n    if df['Embarked'].isna().sum()>0:\n        df.loc[df['Embarked'].isna(), ['Embarked']] = 'C'\n    \n    # Make Age vaule to Categorical\n    df.loc[df['Age']<7, ['Age']] = 3\n    df.loc[(6<df['Age']) & (df['Age']<13), ['Age']] = 9\n    df.loc[(12<df['Age']) & (df['Age']<20), ['Age']] = 16\n    df.loc[(19<df['Age']) & (df['Age']<30), ['Age']] = 20\n    df.loc[(29<df['Age']) & (df['Age']<40), ['Age']] = 30\n    df.loc[(39<df['Age']) & (df['Age']<50), ['Age']] = 40\n    df.loc[(49<df['Age']) & (df['Age']<60), ['Age']] = 50\n    df.loc[(59<df['Age']) & (df['Age']<70), ['Age']] = 60\n    df.loc[(69<df['Age']) & (df['Age']<80), ['Age']] = 70\n    df.loc[(79<df['Age']) & (df['Age']<90), ['Age']] = 80\n    df.loc[89<df['Age'], ['Age']] = 90\n    # Convert NA values to -1\n    df.loc[df['Age'].isna(), ['Age']] = -1\n\n    df = df.astype({'Age':'int32'})\n\n    return df, passengerId","710cdf39":"train_csv = pd.read_csv(path_list[0])\ndftrain, _ = ready_to_use(train_csv)\ndftrain.head(2)","596a9d9e":"# Set dfage by using Na values are not -1(NA)\ndfage = dftrain.loc[dftrain['Age']!=-1].copy()\n\n# Drop Survived column, because Test data set doesnt have survived column.\ndfage.drop(['Survived'], axis=1, inplace=True)\ndfage_target = dfage.pop('Age')\ndfage_inputs = dfage","9d8fa3c2":"# Set Label to One-hot\ndef onehot_label(target_df, need_label_list=False):\n    label_list = sorted(list(target_df.unique()))\n    onehot = None\n    \n    for data in target_df.to_list():\n        zeros = np.zeros((len(label_list)), dtype=np.int64)\n        for num, label in enumerate(label_list):\n            if data == label:\n                zeros[num] = 1\n                if onehot is None:\n                    onehot = zeros\n                else:\n                    onehot = np.vstack((onehot, zeros))\n                    \n    if need_label_list:\n        return onehot, label_list\n    else:\n        return onehot","0f3687eb":"age_onehot, age_label_list = onehot_label(dfage_target, need_label_list=True)","520ed536":"# Make Dataset by using DataFrame\ndef df_to_ds(inputs, labels=None, shuffle=True):\n    if labels is None:\n        ds = tf.data.Dataset.from_tensor_slices((dict(inputs)))\n    else:\n        ds = tf.data.Dataset.from_tensor_slices((dict(inputs), labels))\n    if shuffle:\n        ds = ds.shuffle(32)\n    ds = ds.batch(16)\n    ds = ds.prefetch(1)\n    \n    return ds","06a150cb":"age_ds = df_to_ds(dfage_inputs, labels=age_onehot)","81c5e1a5":"age_columns_vocab = {}\nfor column in dfage_inputs.columns:\n    age_columns_vocab[column] = sorted(list(dfage_inputs[column].unique()))","03769c09":"age_numeric = ['SibSp','Parch']\nage_categori = ['Title','Sex','Embarked','Pclass']","17c2c627":"def make_model_inputs(df):\n    inputs={}\n    for column in df.columns:\n        if df[column].dtype =='object':\n            inputs[column] = tf.keras.Input(name=column, shape=(), dtype=tf.string)\n        else:\n            inputs[column] = tf.keras.Input(name=column, shape=(), dtype=tf.float32)\n            \n    return inputs","d934d6e4":"def encode_inputs(inputs, vocab, numeric_columns, categori_columns):\n    encoded_features = []\n    for column in list(vocab.keys()):\n        if column in categori_columns:\n            if inputs[column].dtype == 'string':\n                vocabulary = vocab[column]\n                # StringLookup convert string data to numeric data, It's a encoding layer\n                lookup = tf.keras.layers.StringLookup(vocabulary=vocabulary)\n                # Connect Input layer to encoding layer\n                value_index = lookup(inputs[column])\n\n                embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n                embedding = tf.keras.layers.Embedding(input_dim=lookup.vocabulary_size(), output_dim=embedding_dims)\n                encoded_feature = embedding(value_index)\n            \n            else:\n                bound = vocab[column]\n                # IntegerLookup is also encoding layer\n                lookup = tf.keras.layers.IntegerLookup(vocabulary=bound)\n                value_index = lookup(inputs[column])\n\n                embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n                embedding = tf.keras.layers.Embedding(input_dim=lookup.vocabulary_size(), output_dim=embedding_dims) \n                encoded_feature = embedding(value_index)\n            \n        else:\n            encoded_feature = inputs[column]\n            if inputs[column].shape[-1] is None:\n                encoded_feature = tf.expand_dims(encoded_feature, -1)\n                \n        encoded_features.append(encoded_feature)\n    \n    encoded_features = tf.keras.layers.concatenate(encoded_features)\n    return encoded_features     ","5a8b4e4a":"class DecisionTree(tf.keras.Model):\n    def __init__(self, depth, num_features, used_features_rate, num_classes):\n        super(DecisionTree, self).__init__()\n        self.depth = depth\n        self.num_leaves = 2**depth\n        self.num_classes = num_classes\n        \n        num_used_features = int(num_features*used_features_rate)\n        one_hot = np.eye(num_features)\n        sampled_feature_indicies = np.random.choice(np.arange(num_features), num_used_features, replace=False)\n        self.used_feature_mask = one_hot[sampled_feature_indicies]\n        \n        self.pi = tf.Variable(\n                    initial_value=tf.random_normal_initializer()(shape=[self.num_leaves, self.num_classes]),\n                    dtype='float32', trainable=True)\n        \n        self.decision_fn = tf.keras.layers.Dense(units=self.num_leaves, activation='sigmoid', name='decision')\n    \n    @tf.function\n    def call(self, features):\n        batch_size = tf.shape(features)[0]\n        \n        features = tf.matmul(features, self.used_feature_mask, transpose_b=True)\n        decisions = tf.expand_dims(self.decision_fn(features), axis=2)\n        decisions = tf.keras.layers.concatenate([decisions, 1-decisions], axis=2)\n        \n        mu = tf.ones([batch_size,1,1])\n        \n        begin_idx = 1\n        end_idx = 2\n        \n        \n        for level in range(self.depth):\n            mu = tf.reshape(mu, [batch_size, -1, 1])\n            mu = tf.tile(mu,(1,1,2))\n            \n            level_decisions = decisions[:, begin_idx:end_idx ,:]\n            mu = mu * level_decisions\n            end_idx = begin_idx + 2**(level+1)\n            \n        mu = tf.reshape(mu, [batch_size, self.num_leaves])\n        prob = tf.keras.activations.softmax(self.pi)\n        \n        outputs = tf.matmul(mu, prob)\n        return outputs","3d764d9d":"def make_tree_model(depth=None, used_features_rate=None, num_classes=None, \n                    input_df=None, numeric_columns=None, categori_columns=None, vocab=None):\n    \n    inputs = make_model_inputs(input_df)\n    features = encode_inputs(inputs, vocab=vocab, numeric_columns=numeric_columns, \n                             categori_columns=categori_columns)\n    \n    features = tf.keras.layers.BatchNormalization()(features)\n    num_features = features.shape[1]\n    \n    tree = DecisionTree(depth, num_features, used_features_rate, num_classes)\n    \n    outputs = tree(features)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model","99e36c0c":"age_tree = make_tree_model(depth=16, used_features_rate=1, num_classes=len(age_label_list),\n                           input_df=dfage_inputs, numeric_columns=age_numeric, vocab=age_columns_vocab, \n                           categori_columns=age_categori)","018bb9b3":"initial_learning_rate = 0.01\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=10000,decay_rate=0.96,\n                                                             staircase=True)\n\nloss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.AUTO)\noptimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\ntrain_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\n\n@tf.function\ndef train_step(x, y):\n    with tf.GradientTape() as tape:\n        y_pred = age_tree(x)\n        loss = loss_object(y, y_pred)\n    gradients = tape.gradient(loss, age_tree.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, age_tree.trainable_variables))\n    \n    train_loss(loss)\n    train_accuracy(y, y_pred)","513e9a41":"EPOCHS = 100\nfor epoch in range(EPOCHS):\n    train_accuracy.reset_states()\n    train_loss.reset_states()\n    for x, y in age_ds:\n        loss = train_step(x,y)\n        \n    if (epoch+1)%10 == 0:\n        print(f'Epochs : {epoch+1}  Loss : {train_loss.result():0.3f}  Accuracy : {train_accuracy.result():0.3f}')","b310cdef":"tf.keras.backend.clear_session()","1e1ae4f3":"val_age = dftrain.loc[dftrain['Age']==-1].copy()\nval_age.drop(['Survived', 'Age'], axis=1, inplace=True)\n\nval_ds = df_to_ds(val_age, shuffle=False)","56511dae":"forecast = age_tree.predict(val_ds)\n\nexpect_age = []\nfor num, ex in enumerate(forecast):\n    order = np.argmax(ex)\n    age = age_label_list[order]\n    expect_age.append(age)\n    \nprint(expect_age)","49628ac7":"dftrain.loc[dftrain['Age']==-1,['Age']] = pd.Series(expect_age, index=list(val_age.index))","43ff24d5":"survive_target = dftrain.pop('Survived')\nsurvive_features = dftrain\nsurvive_features.head(2)","b626eefd":"survive_onehot = onehot_label(survive_target)\nds = df_to_ds(survive_features, labels=survive_onehot)","6522300e":"survive_columns_vocab ={}\nfor column in survive_features.columns:\n    survive_columns_vocab[column] = sorted(list(survive_features[column].unique()))","81b87beb":"survive_numeric = ['SibSp', 'Parch']\nsurvive_categori = ['Pclass', 'Title', 'Sex', 'Age', 'Embarked']","6b9f56cb":"survive_tree = make_tree_model(depth=16, used_features_rate=1, num_classes=2,\n                               input_df=survive_features, numeric_columns=survive_numeric, vocab=survive_columns_vocab,\n                               categori_columns=survive_categori)","6750c4dc":"initial_learning_rate = 0.01\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=100000,decay_rate=0.96,\n                                                             staircase=True)\n\nsur_loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.AUTO)\nsur_optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n\nsur_train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\nsur_train_loss = tf.keras.metrics.Mean(name='train_loss')\n\n@tf.function\ndef sur_train_step(x, y):\n    with tf.GradientTape() as tape:\n        y_pred = survive_tree(x)\n        loss = sur_loss_object(y, y_pred)\n    gradients = tape.gradient(loss, survive_tree.trainable_variables)\n    sur_optimizer.apply_gradients(zip(gradients, survive_tree.trainable_variables))\n    \n    sur_train_loss(loss)\n    sur_train_accuracy(y, y_pred)","25b51c77":"EPOCHS = 100\nfor epoch in range(EPOCHS):\n    sur_train_accuracy.reset_states()\n    sur_train_loss.reset_states()\n    for x, y in ds:\n        sur_train_step(x,y)\n        \n    if (epoch+1)%10 == 0:\n        print(f'Epochs : {epoch+1}  Loss : {sur_train_loss.result():0.3f}  Accuracy : {sur_train_accuracy.result():0.3f}')","ff1ba296":"test_csv = pd.read_csv(path_list[1])\ndftest, passengerId = ready_to_use(test_csv, test_stat=True)\ndftest.head(2)","5c0dc757":"test_age = dftest.loc[dftest['Age']==-1].copy()\ntest_age.pop('Age')\ntest_age_ds = df_to_ds(test_age, shuffle=False)","d11f6162":"pred_age = age_tree.predict(test_age_ds)\nexpected_age = []\nfor num, ex in enumerate(pred_age):\n    order = np.argmax(ex)\n    age = age_label_list[order]\n    expected_age.append(age)\n    \nprint(expected_age)","c62f0c82":"dftest.loc[dftest['Age']==-1,['Age']] = pd.Series(expected_age, index=list(test_age.index))\ntest_ds = df_to_ds(dftest, shuffle=False)","c99d5be9":"pred_sur = survive_tree.predict(test_ds)\nexpected_survive = []\nfor data in pred_sur:\n    expected_survive.append(int(np.argmax(data)))\nprint(expected_survive)","d1677d96":"sub = pd.DataFrame(data=expected_survive, index=passengerId, columns=['Survived'])\nsub.head(5)","71f14024":"sub.to_csv('.\/sub.csv')","1ca1f4b8":"#### Define make_model_inputs\n- It builds first layer (aka Input layer)\n- If a DataSeries in DataFrame 's type is object, it means data in a DataSeries is a string.","6e0792a7":"#### Define DecicsionTree\n- `num_features` is a number of features. (count number of input dataframe's columns)\n- `used_features_rate` is for choosing features. If it is a one, all features is used.\n- `num_classes` is a number of target's classes. For example, if target is Survived(Live or Death), num_classes should be two.\n- `self.used_feature_mask` is randomly choosen from one-hot.\n- `self.pi` is a trainable variable in Model.\n- `self.decision_fn` is a layer to predict True or False. It return probabilities, so activation should be sigmoid.","e4ba30ae":"#### age predciton model ","3f2873cf":"#### dfage is a DataFrame for training age prediction model\n- Divide dfage to target and inputs","794171b2":"#### Extract Na values for prediction","695ef6a2":"#### Making Tree Model\n- All Parameters are requried.","932f09c4":"#### Divide Columns to Numeric and Categorical","b1ae3ff4":"#### Define onehot_label for making one-hot label\n- I'm going to use a CategoricalCrossentropy as loss function, So making label dataset with one-hot values.\n- If you need Original label information, `set need_label_list=True`.\n- Label_list must be sorted !","f508af07":"## **Please Check TensorFlow Version, 2.6.0 is requried**","708d84c9":"#### NOW We are going to predict Survive\n- Divide target and features","952d4aad":"## Now We are going to predict test set","3ad66cab":"#### Making Survive prediction Model","1cc91499":"#### Convert DataFrame to Dataset\n- For test dataset, I set labels to None.","d7b58389":"#### Define encode_inputs\n- `inputs` is from make_model_inputs.\n- `vocab` is user defined dictionary that has all informations about features for training.\n- `numeric_columns` is a list that has numeric column's name.\n- `categori_columns` is also a list that has categorical column's name.\n- `encoded_features` is a list that contains encoding layers.","5f953f62":"#### Fill NA values by using predicted values","e8d2d202":"#### Making Vocab Dictionary\n- Using it for Enocding layer\n- It must be sorted !","7676ec24":"## Fuction(ready_to_use) preprocesses DataFrame\n\n- `Cabin` Column has lots of `NA Values`, So I Drop this column\n- Normally `Ticket` has `a class information`, But Dataset already has that information. So I drop this column\n- I tried to find some connections between `Pclass` and `Fare`, but I couldnt find anything. Some 1st Class' Fare are lower then 2nd Class' Fare or 3rd Class' Fare. So I decide to drop Fare column\n\n- I extract `Title information` from a Name Column, and Rename the column to Title.\n\n- I found two values in Embarked are NA. And those values samely have 1 value at Survived Column. (Embarked 'C' has the most highly chance to survive then other classes in Embarked column). So I convert those NA values to 'C'.\n\n- `Using Categorical Age`( 3 means infant, 9 means child, 16 means teenager and etc...)\n\n- `test_stat is the key for using passengerId or not`"}}