{"cell_type":{"0e176450":"code","cb54142e":"code","2d45984b":"code","5092448d":"code","4f7d17cd":"code","e6a8a7f6":"code","d5aa0475":"code","ed13463b":"code","059ca2a9":"code","ca4e858c":"markdown","ba1e67e5":"markdown","49da2a95":"markdown","5fae5b5c":"markdown","f04c768a":"markdown","adae63a9":"markdown","352ca7d2":"markdown","9943f59a":"markdown","d00418a9":"markdown"},"source":{"0e176450":"!ls -lrt \/kaggle\/input\n!java -version\n!javac -d \/kaggle\/working\/  \/kaggle\/input\/code\/java\/CORD19\/src\/DTBean.java\n!javac -d \/kaggle\/working\/  \/kaggle\/input\/code\/java\/CORD19\/src\/Segment.java\n!javac -d \/kaggle\/working\/  \/kaggle\/input\/code\/java\/CORD19\/src\/Text.java\n!javac -d \/kaggle\/working\/  \/kaggle\/input\/code\/java\/CORD19\/src\/Word2Num.java\n!javac -d \/kaggle\/working\/ -cp \/kaggle\/working\/ \/kaggle\/input\/code\/java\/CORD19\/src\/SentenceSplitter.java\n!javac -d \/kaggle\/working\/ -cp \/kaggle\/working\/ \/kaggle\/input\/code\/java\/CORD19\/src\/NovelTherapeuticsPipeline.java\n!javac -d \/kaggle\/working\/ -cp \/kaggle\/working\/ \/kaggle\/input\/code\/java\/CORD19\/src\/BERTInputPreprocess.java\n!javac -d \/kaggle\/working\/ -cp \/kaggle\/working\/ \/kaggle\/input\/code\/java\/CORD19\/src\/BERTPostProcessing.java\n!javac -d \/kaggle\/working\/ -cp \/kaggle\/working\/:\/kaggle\/input\/code\/java\/CORD19\/lib\/* \/kaggle\/input\/code\/java\/CORD19\/src\/BERTOutputProcessor.java\n\n!java -cp \/kaggle\/working\/ NovelTherapeuticsPipeline\n!java -cp \/kaggle\/working\/ BERTInputPreprocess\n!ls -lrt \/kaggle\/input\n","cb54142e":"!java -cp \/kaggle\/working\/:\/kaggle\/input\/code\/java\/CORD19\/lib\/* BERTOutputProcessor\n!java -cp \/kaggle\/working\/:\/kaggle\/input\/code\/java\/CORD19\/lib\/* BERTPostProcessing","2d45984b":"import nltk\nimport re\nnltk.downloader.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport pandas as pd\n\nregex=r\"\\b(low|reduce|stop)(.*)(infection|fatal|mortal|risk|cytokine storm|concentration|death|adverse)+\"\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\ndef checkNegFP(text):\n    m=re.search(regex,text)\n    if(m!=None):\n        return True\n    else:\n        return False\n\n","5092448d":"input_file = '\/kaggle\/working\/novel_th_ab_wbert_processed.tsv'\ntdf = pd.read_csv(input_file,sep='\\t',converters={\"Clinical Improvement (Y\/N)\":str})\nfor i, r in tdf.iterrows():\n    r[1]=r[1].title() \n    score_dict = SentimentIntensityAnalyzer().polarity_scores(r[9]);\n    if(score_dict['neg']>score_dict['pos']):\n        if checkNegFP(r[9]):\n            r[9]='Y'\n        else:\n            r[9]='N'\n    elif(score_dict['neg']<score_dict['pos']):\n        r[9]='Y'\n    else:\n        r[9]='-'\ntdf.to_csv(\"\/kaggle\/working\/novel_th.csv\")","4f7d17cd":"tdf","e6a8a7f6":"!java -cp \/kaggle\/working\/ NovelTherapeuticsPipeline q1\n!java -cp \/kaggle\/working\/ BERTInputPreprocess q1","d5aa0475":"!java -cp \/kaggle\/working\/:\/kaggle\/input\/code\/java\/CORD19\/lib\/* BERTOutputProcessor q1\n!java -cp \/kaggle\/working\/:\/kaggle\/input\/code\/java\/CORD19\/lib\/* BERTPostProcessing q1","ed13463b":"input_file = '\/kaggle\/working\/hgs_ab_wbert_processed.tsv'\ndf = pd.read_csv(input_file,sep='\\t',converters={\"Clinical Improvement (Y\/N)\":str})\nfor i, r in df.iterrows():\n    r[1]=r[1].title() \n    score_dict = SentimentIntensityAnalyzer().polarity_scores(r[9]);\n    if(score_dict['neg']>score_dict['pos']):\n        if checkNegFP(r[9]):\n            r[9]='Y'\n        else:\n            r[9]='N'\n    elif(score_dict['neg']<score_dict['pos']):\n        r[9]='Y'\n    else:\n        r[9]='-'\ndf.to_csv(\"\/kaggle\/working\/hgs.csv\")","059ca2a9":"df","ca4e858c":"### Code for running BioBERT for novel therapeutics\n\n!pip install tensorflow-gpu==1.14.0\n!pip install bert-tensorflow\n\n! python \/kaggle\/input\/code\/python\/run_factoid.py      --do_train=False      --do_predict=True      --vocab_file=\/kaggle\/input\/data\/data\/BERT-pubmed-1000000-SQuAD\/vocab.txt      --bert_config_file=\/kaggle\/input\/data\/data\/BERT-pubmed-1000000-SQuAD\/bert_config.json      --init_checkpoint=\/kaggle\/input\/data\/data\/BERT-pubmed-1000000-SQuAD\/model.ckpt-14599      --max_seq_length=384      --train_batch_size=12      --learning_rate=5e-6      --doc_stride=128      --num_train_epochs=5.0      --do_lower_case=False      --predict_file=\/kaggle\/working\/novel_th_ab_bert.json      --output_dir=\/kaggle\/working\/","ba1e67e5":"## References\n1. Joseph T, Saipradeep VG, Raghavan GS, Srinivasan R, Rao A, Kotte S, Sivadasan N. TPX: Biomedical literature search made easy. Bioinformation 8(12): 578-80 (2012).\n2. Lee, J; Yoon, W; Kim, S; Kim, D; Kim, S; Ho So, C; Kang, J. Bioinformatics, Volume 36, Issue 4, 15 February 2020, Pages 1234\u20131240.","49da2a95":"### Final output summary table for novel therapeutics","5fae5b5c":"## Approach Summary\nOur phase-1 submission for this task described CoNetz, a tool for text and network mining for COVID-19 research. The tool\u00a0allows easy exploration and visualization of the COVID-19 corpus derived network for generation of leads. We now describe our phase-2 submission for this task, which uses a set of new techniques to specifically address the above two questions.\n\nOur\u00a0fully automated phase-2 pipeline broadly\u00a0consists of the following\u00a0steps:\n1. **Corpus preprocessing (corpus augmentation, named entity annotation and cleansing)**:  \n  The\u00a0CORD-19 corpus was augmented with additional COVID-19 related MEDLINE abstracts.\u00a0Foreign (non-English) language articles were then removed.\u00a0A lexicon (dictionary)-based NER was performed to annotate named entities in the articles. In particular, entities belonging to \"PHENOTYPE and SYMPTOM\" terms, \"CHEMICAL, DRUG\u00a0and INTERVENTION\" terms, \"HYPERCOAGULATION\"\u00a0terms\u00a0and COVID-19 DISEASE terms were tagged.\u00a0\n2. **Identification\u00a0of relevant articles**:  \n Selection of relevant COVID-19 articles that are related to clinical intervention studies in the context of either COVID-19 therapeutics or handling\u00a0hypercoagulable state.\n3. **Information extraction\/prediction for populating\u00a0the\u00a0target fields**:  \n Applying automated information extraction and prediction on the final corpus\u00a0based on various DL and ML models including the\u00a0BioBERT\u00a0(Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) model\u00a0and SVM models for populating the target fields in the csv output.\u00a0\n4. **Post processing**:  \n Process the csv intermediate output from the previous step for handling duplicates, missing fields,\u00a0noisy entries etc., and produce the final csv output.","f04c768a":"## Task\nCreate summary tables that address therapeutics, interventions, and clinical studies related to COVID-19. Specifically, the submission should focus on what the literature reports about:\n* What is the best method to combat the hypercoagulable state seen in COVID-19?\n* What is the efficacy of novel therapeutics being tested currently?\n\nWe describe our efforts in answering these two questions of this task. For each question, we create a csv file as our submission.","adae63a9":"## Approach Details\n\nIn the following, we describe the above steps in detail. Implementation details including organization of supporting data\/code and statistics\u00a0of intermediate outputs\u00a0are also discussed in the end (Additional Details Section). Most of the computational pipeline is identical for answering both the questions. Where ever applicable, we discuss the custom modifications that are applied to answer any of the\u00a0specific questions.\u00a0The pipeline is run on the latest CORD-19 corpus (uploaded on 9th June 2020).\u00a0\n\n### 1. **Corpus preprocessing (corpus augmentation, named entity annotation and cleansing)**:\u00a0\n* Using a JSON parser, the provided CORD-19 corpus was converted to a TSV file that consists\u00a0of relevant fields.\n* The provided CORD-19 corpus is an excellent source of COVID-19 related articles. However,\u00a0in order to improve the coverage, we also included additional COVID-19 related MEDLINE abstracts that are not present in the provided CORD-19 corpus. In order to do this, we searched for COVID-19 or any of its synonyms in the MEDLINE corpus and the compared their PubMed IDs with the PubMed IDs present in the CORD-19 corpus.\u00a0\n* Foreign language (non-English) articles were removed from this augmented corpus. The \"FastText\" model based prediction was used for filtering out non-English articles. Please see the Additional Details section for code\/data. The final filtered set of articles are saved in cord19_arts.tsv file.\n* We performed Named Entity Recognition (NER) on the filtered corpus to tag the entities of interest. Specifically, we tagged entities belonging to\u00a0\"PHENOTYPE and SYMPTOM\" terms, \"CHEMICAL, DRUG\u00a0and INTERVENTION\" terms, \"HYPERCOAGULATION\" terms\u00a0and COVID-19 DISEASE terms.\u00a0These entities are crucial for accurate extraction of relevant information.\n* For performing annotations, we used the TPX NER tool [1]\u00a0which is an in-house lexicon (dictionary)-based NER module. The customized dictionaries\u00a0consisted of a comprehensive set of terms belonging to the four entity categories. The NER module performs approximate string matching for tagging. It can also handle local abbreviations.\n* The input to the NER module is cord19_arts.tsv. The NER output consists of a) article level annotation capturing the tagged entities (annotations.inp) and b) corpus level aggregate data that captures pairwise co-occurrence and pairwise Pearson correlation between the tagged entities in the input corpus\u00a0(pc_associations_master.txt). The output files are made available in the data folder.\u00a0\n* The corpus preprocessing phase\u00a0comprising of the above steps\u00a0is\u00a0not included in our Jupyter notebook. However, all the relevant code and the intermediate outputs are made available in the data\/code folder (Details discussed in the Additional Details Section), with the exception of the NER module. The TPX NER code is not included in our submission code folder due to\u00a0licensing constraints. In order to support future execution of our pipeline on newer articles, we would be providing a facility for\u00a0web based remote execution of the NER module which takes a cord19_arts.tsv\u00a0file as input and produces the two corresponding output files that feed into our next step.\n\n### 2. **Identification\u00a0of relevant articles**:\u00a0\n* In this step, the output files from the previous step are used to select the relevant articles for the subsequent information extraction steps. We apply multiple filters for relevant article identification.\n* The first filter uses\u00a0entity annotations of\u00a0the articles. An article qualifies this filter if the entity annotations in its title\/abstract contain\n    * A COVID-19 (or its synonym) term\u00a0\n    * at least one \"CHEMICAL, DRUG\u00a0and INTERVENTION\" term\n    * at least one \"HYPERCOAGULATION\" term\u00a0(only in the case of addressing the first question on hypercoagulable state).\n* The second filter is applied on the articles that qualify the first filter in order to identify articles that are related to clinical intervention studies. This is because, for both the questions in this task, the information extraction has to be performed from articles that are related to clinical intervention studies. The second filter applies a combination of pattern matching and\u00a0 SVM based classification. An article that was classified as positive by either the pattern matching or the SVM classifier are included for further processing.\n    * In pattern matching, articles whose title\/abstract has an occurrence\u00a0of any of [\"patients\" or \"volunteers\" or \"participants\" or \"cases\" or \"COVID-19 case\" or \"cohort\"\u00a0or [0-9]+ year old] are tagged as positive.\n    * A one-class SVM classifier (outlier detection) was trained on a positive training set of PubMed articles whose publication type metadata is any one of [Adaptive Clinical Trial, Case Reports, Clinical Conference, Clinical Study, Clinical Trial, Clinical Trial, Phase I, Clinical Trial, Phase II, Clinical Trial, Phase III, Clinical Trial, Phase IV, Clinical Trial Protocol, Controlled Clinical Trial, Pragmatic Clinical Trial and\u00a0Randomized Controlled Trial]. A negative training set could further improve the quality by training a two-class SVM classifier. However, we have used a one-class classifier in this submission.\n* The output is a tsv file \"novel_th_ab.tsv\" (\"hgs_ab.tsv\" for the hypercoagulation question).\u00a0 These files will be created in the Jupyter working folder.\n\n### 3.\u00a0**Information extraction\/prediction for populating\u00a0the\u00a0target fields**:\n* In this step, we apply\u00a0information extraction and prediction on each article present in the output of the previous step. For this, we apply\u00a0various DL and ML models including the\u00a0BioBERT\u00a0(Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) model [2] and SVM models.\n* The target fields\u00a0in the output are: Date | Study | Study Link | Journal | Study Type | Therapeutic method(s) utilized\/assessed | Sample size| Severity of Symptoms | General Outcome\/Conclusion Excerpt | Primary Endpoint(s) of Study | Clinical Improvement (Y\/N)\n* The entity annotations of the articles are also used in conjunction with\u00a0ML and DL models for populating the different fields.\n* **\"Date\", \"Study\", \"Study Link\" and \"Journal\" fields**: These are directly populated from the CORD-19 article metadata.\n* **\"Study Type\" field**:\n    * We use an SVM based classifier to populate this field. We train a\u00a0multi-class SVM classifier using training data constructed from PubMed. For each class, we create approximately 2000 training articles by using related PubMed searches and related PubMed metadata. The training corpus, training code and the final trained SVM model are available in the data folder.\u00a0\n* **\"Therapeutic method(s) utilized\/assessed\" field**:\n    * We use pre-trained BioBERT model\u00a0(Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) trained on Biomedical corpus to extract this field.\n    * In particular, we use the Question\/Answer (Q&A) model of BioBERT. We frame multiple questions around Therapeutics such as\u00a0\"What were the therapeutics used for treatment of covid-19?\" and scan the BioBERT returned\u00a0text snippets from the article title\/abstract. We then identify the snippet having the maximum prediction score by BioBERT.\n    * Subsequently, we then use the article annotation data and search for the presence of\u00a0tagged therapeutics terms in the identified text snippet.\n    * If multiple tagged therapeutics are detected then we create separate\u00a0rows in the output csv for each of these and extract the remaining\u00a0fields for them separately.\n    * To address the question on handling hypercoagulation state, we use a set of modified questions such as\u00a0\"What were the therapeutics used for treatment of Y ?\", where \"Y\" is the tagged HYPERCOAGULATION TERM present in the article abstract.\n* **\"Sample size\" field**:\n    * We use pre-trained BioBERT Q&A model along with multiple related questions followed by pattern matching\u00a0to extract sample size.\n* **\"Severity of Symptoms\" field**:\n    * Here again, we use pre-trained BioBERT Q&A model along with multiple related questions followed by pattern matching\u00a0to populate this field.\n* **\"General Outcome\/Conclusion Excerpt\" field**:\u00a0\n    * We use the article conclusion section in the case of full text articles to populate this field. For articles without full text, conclusions present in the article abstract are extracted using pattern matching. In the absence of a defined conclusion in the article abstract, we populate this field with the text snippet containing the tagged therapeutic term and COVID-19 mention.\n* **\"Primary Endpoint(s) of Study\" field**:\n    * Presence of primary end point related patterns (such as \"primary composite endpoints\") are searched in the full\u00a0text articles. After identifying the relevant text portions, BioBERT based Q&A model along with related questions are used to identify high confidence text snippets that are then used to populate this field. For articles without full text, BioBERT Q&A model is run on\u00a0the whole title\/abstract section.\u00a0\n* **\"Clinical Improvement (Y\/N)\" field**:\n    * We use a combination of standard questions and a set of adaptive questions together with BioBERT to extract relevant text snippets with high prediction scores. For adaptive questions, we use the tagged therapeutics and frame tailored questions such as \"How effective was Y?\" where \"Y\" stands for a tagged therapeutic entity. The extracted text snippet is then fed to Vader sentiment analyzer to classify it as either Y or N.\u00a0\n* In summary, to populate the above fields, we heavily use BioBERT Q&A model to identify high confidence text snippets by firing multiple related questions. To improve the precision further, we use the tagged entities in the article to both process\u00a0these snippets and to construct\u00a0adaptive queries for BioBERT that\u00a0are tailored to the specific article. For some of the fields, we use additional trained SVM models that take\u00a0these snippets as input and predict the field values. We believe that using a combination of BioBERT Q&A framework together with tailored questions, tagged annotations and additional SVM classifiers is a promising approach in achieving both good\u00a0precision as well as recall.\n* The output of this step is\u00a0novel_th_ab_wbert.tsv file (hgs_ab_wbert.tsv for the hypercoagulation question) containing all the target fields and this forms the input to the next step.\n* \n\n### 4. **Post processing**:\u00a0\n\n* We perform a set of post processing steps to clean the csv file generated by the previous step.\n* Add missing Journals, if found in MEDLINE. All the available journal names in the corpus were mapped to their PubMed journal name abbreviation. Some were a direct match, while there were many which needed to be processed into their full names using rules created based on the patterns observed.\n* Add missing DOI, if found in MEDLINE.\n* Add missing\/partial publication date, if found in MEDLINE.\n* Date: to be transformed to M\/D\/YYYY format. If the date is shown as just \"2020\", then it is left as-is.\n* Remove the word(s) \"Abstract\" or \"Background\", \"Full-length title\", \"News at a glance\", \"Letter to the Editor\" or \"commentary\" if the title begins with these in the Study field.\n\n\n### **The final output files**: \n* novel_th.csv file for the novel therapeutics\u00a0question\n* hgs.csv for the hypercoagulation question","352ca7d2":"## Additional Details\n* The latest CORD-19 input corpus consisted of 1,38,794 articles.\n* A total of 2553 non-English articles were removed in the preprocessing step.\n* A total of 2420 filtered articles resulted after performing step 2 (identifying relevant articles).\n* The \/kaggle\/input\/data folder contains:\n    * The BioBERT Q&A SQuAD model \n    * Combined corpus of CORD-19 articles and filtered MEDLINE\n    * Corpus annotations and Pearson correlation values\n    * SVM models for study type classification\n    * one-class SVM model for clinical study article classification\n* The \/kaggle\/input\/code\/python folder contains:\n    * CORD-19 JSON parser\n    * FastText foreign language classifier.\n    * BioBERT Q&A code\n    * SVM classifier (training and prediction) for study type classification in the utils sub folder\n    * one-class SVM classifier (training and prediction) for clinical study article classification in the utils sub folder\n* The \/kaggle\/input\/code\/java folder contains:\n    * java source code for novel therapuetics pipeline, BioBERT input\/output processing and for the post processing step.","9943f59a":"### Code for running BioBERT for hypercoagulable state\n\n! python \/kaggle\/input\/code\/python\/run_factoid.py      --do_train=False      --do_predict=True      --vocab_file=\/kaggle\/input\/data\/data\/BERT-pubmed-1000000-SQuAD\/vocab.txt      --bert_config_file=\/kaggle\/input\/data\/data\/BERT-pubmed-1000000-SQuAD\/bert_config.json      --init_checkpoint=\/kaggle\/input\/data\/data\/BERT-pubmed-1000000-SQuAD\/model.ckpt-14599      --max_seq_length=384      --train_batch_size=12      --learning_rate=5e-6      --doc_stride=128      --num_train_epochs=5.0      --do_lower_case=False      --predict_file=\/kaggle\/working\/hgs_ab_bert.json      --output_dir=\/kaggle\/working\/","d00418a9":"### Final output summary table for hypercoagulable state"}}