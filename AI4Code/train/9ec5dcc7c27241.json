{"cell_type":{"96581f05":"code","dc234e68":"code","7907bda2":"code","cc7837f6":"code","b00d81ba":"code","b7552520":"code","676b14e6":"code","6caea8f7":"code","29449076":"code","54504612":"code","bb12ab24":"code","0d041924":"code","b60fcc0a":"code","d7035b0f":"code","20bce8c2":"code","9adec720":"code","575dd7c8":"code","c90d50d9":"code","d2807097":"markdown","cc384aa3":"markdown","07f3c0fe":"markdown","cef58672":"markdown","c36dfae1":"markdown","b190de19":"markdown","7ff2482e":"markdown","a96b7cbb":"markdown","2348b2ea":"markdown","07b19c9c":"markdown","1000d2c5":"markdown","ec5fd465":"markdown","465e3c47":"markdown","18ce31cd":"markdown","fb73f107":"markdown","03a8276a":"markdown","9aa7c916":"markdown","33822876":"markdown","df88fb10":"markdown","6824b6ed":"markdown","6562ef3f":"markdown","9c8cac63":"markdown","d29f4f39":"markdown","a434b07b":"markdown","9c7f6bc0":"markdown"},"source":{"96581f05":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Leemos Datos\nX_full = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\n\n# Obtenci\u00f3n de objetivos y predictores\ny = X_full.SalePrice\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = X_full[features].copy()\nX_test = X_test_full[features].copy()\n\n# Para ello primero separamos las filas con valores ausentes, haciendo a un lado objetivos y predictores.\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n\n\n# Para mantener la simpleza, solo utilizaremos predictores num\u00e9ricos.\nX = X_full.select_dtypes(exclude=['object'])\nX_test = X_test_full.select_dtypes(exclude=['object'])\n\n\n# Separaci\u00f3n de valores del set para entrenamiento\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\nX_full.head()\n","dc234e68":"X_test_full.head()","7907bda2":"X_train.head()","cc7837f6":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Funci\u00f3n para comprobar la exactitud de las aproximaciones\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","b00d81ba":"print(X_train.shape)\n\n# Number of missing values in each column of training data# Fill in the line below: get names of columns with missing values\ncols_with_missing = [col for col in X_train.columns\n                     if X_train[col].isnull().any()]\n\n# Drop columns in training and validation data\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis=1) # Your code here\n\n# Check your answers\nmissing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","b7552520":"#Sumamos los datos y los dividimos entre en total\na=(212 + 6 + 58) \/ (212 + 6 +58 + 1168)\na","676b14e6":"# Llamamos a los datos que faltan\ncols_with_missing = [col for col in X_train.columns\n                     if X_train[col].isnull().any()]\n\n# Eliminamos estas columnas en los DOS SET DE DATOS (No solo en uno) entrenamiento y validaci\u00f3n.\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis=1)","6caea8f7":"print(\"MAE (Sin columnas de valores vacios):\")\nprint(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))","29449076":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# Imputation: eliminaci\u00f3n de valores ausentes; sustituci\u00f3n.\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns","54504612":"print(\"MAE (Imputation):\")\nprint(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))","bb12ab24":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train.columns if\n                     X_train[cname].nunique() < 10 and \n                     X_train[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_trained = X_train[my_cols].copy()\nX_valid = X_valid[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","0d041924":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Preprocesamos datos num\u00e9ricos\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocesamos datos categ\u00f3ricos\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Procesamiento previo de paquetes para datos num\u00e9ricos y categ\u00f3ricos\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","b60fcc0a":"# Definimos el modelo\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n\n# Agrupamos c\u00f3digo de preprocesamiento y modelado en una canalizaci\u00f3n\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n\n# Preprocesamos training data \nclf.fit(X_trained, y_train)\n\n# Preprocesamos los datos de validaci\u00f3n y obtenemos predicciones\npreds = clf.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, preds))","d7035b0f":"from xgboost import XGBRegressor\nmy_model_1 = XGBRegressor(random_state=0)\n\n# Fit the model\nmy_model_1.fit(X_trained, y_train)\n\npredictions_1 = my_model_1.predict(X_valid)","20bce8c2":"mae_1 = mean_absolute_error(predictions_1, y_valid)\nprint(\"Mean Absolute Error:\" , mae_1)","9adec720":"# Definimos el modelo\nmy_model_2 = XGBRegressor(n_estimators=1000,random_state=0, learning_rate=0.1)\n\n# Vestimos el modelo\nmy_model_2.fit(X_train, y_train)\n\n# Obtenemos predicciones\npredictions_2 = my_model_2.predict(X_valid)\n\n# Calculamos MAE\nmae_2 = mean_absolute_error(predictions_2, y_valid)\nprint(\"Mean Absolute Error:\" , mae_2)","575dd7c8":"# Vestimos el modelo\nmy_model_2.fit(reduced_X_train, y_train)\n\n# Obtenemos predicciones\npredictions_3 = my_model_2.predict(reduced_X_valid)\n\n# Calculamos MAE\nmae_2 = mean_absolute_error(predictions_2, y_valid)\nprint(\"Mean Absolute Error:\" , mae_2)","c90d50d9":"my_model_2_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', my_model_2)\n                     ])\n\n# Preprocesamos training data \nmy_model_2_pipe.fit(X_trained, y_train)\n\n# Preprocesamos los datos de validaci\u00f3n y obtenemos predicciones\npredictions = my_model_2_pipe.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, predictions))","d2807097":"##### Construcci\u00f3n modelo XGBoost\n____","cc384aa3":"Contenidos\n- Update y set del conjunto de datos\n- M\u00e9todos y valoraci\u00f3n de sustituci\u00f3n de valores ausentes\n- Creaci\u00f3n de pre-procesamiento de datos a trav\u00e9s de pipeline\n- Construcci\u00f3n Random Forest con pipeline\n- Construcci\u00f3n XGBoost\n- Comparaci\u00f3n modelos y m\u00e9todos\n- Conclusi\u00f3n ","07f3c0fe":"2. Probamos el m\u00e9todo de la sustituci\u00f3n de los valores restantes por valores medios de cada columna (Imputation)","cef58672":"<center>\n \n   ## Housing Prices Competition for Kaggle Learn Users (pt. 2)\n    \n\n___","c36dfae1":"##### Comparaci\u00f3n de modelos y m\u00e9todos\n___","b190de19":"Un pipeline es una forma sencilla de mantener el c\u00f3digo de modelado organizado y preprocesar los datos. Por lo tanto, agrupa los pasos de pre-procesamiento y modelado para que podamos unir todos estos pasos en uno solo.\n\nAunque no es obligatorio, de hecho muchos cient\u00edficos no realizan pipelines, esta forma de proceder tiene muchas ventajas como:\n\n- C\u00f3digo m\u00e1s limpio: tener en cuenta los datos en cada paso del preprocesamiento puede resultar complicado. Con un pipeline no se necesita realizar un seguimiento manual de sus datos de capacitaci\u00f3n y validaci\u00f3n en cada paso.\n- Menos errores: como una aplicaci\u00f3n incorrecta o el olvido de un paso.\n- M\u00e1s f\u00e1cil de producir: puede ser sorprendentemente dif\u00edcil hacer la transici\u00f3n de un modelo de un prototipo a algo que se pueda implementar a escala, el pipeline puede ayudar en \u00e9sto.\n- M\u00e1s opciones para la validaci\u00f3n del modelo.","7ff2482e":"Realizamos una inserci\u00f3n en los valores que faltan (NaN- Not A Number) a trav\u00e9s de la prueba de diversos m\u00e9todos para probar cu\u00e1l es el \u00f3ptimo. Para evaluar las diferentes formas de aproximaci\u00f3n utilizaremos el 'MAE'.","a96b7cbb":"  <center>\n    Pablo de la Asunci\u00f3n Cumbrera Conde","2348b2ea":"Incluimos a XGBoost el pre procesamiento Pipeline.","07b19c9c":"##### Conclusiones: \n___\n\n1. Modelo Random Forest Pipeline: 18017.7\n2. Modelo XGboost: 17249.5\n3. Modelo XGboost Drop: 16402\n4. Modelo XGboost Pipeline: 17581.5","1000d2c5":"Desarrollamos nuestro pipeline","ec5fd465":"Los resultados arrojan la siguiente informaci\u00f3n: El primer m\u00e9todo (Dropping) ha funcionado, en este caso mejor que el segundo m\u00e9todo (Imputation)\n- MAE DROP- 17837.82\n- MAE IMPUTATION- 18062.89\n","465e3c47":"A trva\u00e9s de este pipeline, construimos un modelo Random Forest","18ce31cd":"Utilizaremos el set de entrenamiento de la X","fb73f107":"1.Dado que los datos restantes no suponen una gran cantidad del total (por debajo del 20%) probaremos si la mejor forma de lidiar con ellos ser\u00eda eliminar estas entradas.","03a8276a":"Mejoramos el modelo.","9aa7c916":"![kagglebannerhouse.png](attachment:e834c12c-2239-4f5f-bb14-beab7a999a3e.png)","33822876":"##### M\u00e9todos de sustituci\u00f3n de valores ausentes\n___","df88fb10":"Construimos un modelo de Gradiend Boosting. Este es un m\u00e9todo que pasa por ciclos para agregar modelos de forma iterativa a un conjunto.\n\nEl ciclo empieza por inicializar el conjunto con un solo modelo, cuyas predicciones pueden ser bastante ingenuas. (Incluso si sus predicciones son tremendamente inexactas, las adiciones posteriores al conjunto abordar\u00e1n esos errores).","6824b6ed":"##### Creaci\u00f3n de pre-procesamiento de datos a trav\u00e9s de pipeline\n___","6562ef3f":"##### Update y set del conjunto de datos\n---","9c8cac63":"Usamos por tanto el m\u00e9todo XGboost Drop (4) 16401.953 para la competici\u00f3n.","d29f4f39":"Incluimos a XGBoost el pre procesamiento que hicimos en primera instancia. (M\u00e9todo DROP)","a434b07b":"En el siguiente proyecto, continuaci\u00f3n de 'Housing Prices Competition for Kaggle Learn Users (pt.1)', profundizaremos en la mejora de modelos desarrollados para la competici\u00f3n [Housing Prices Competition for Kaggle Learn Users](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course) sobre la aplicaci\u00f3n de Machine Learning para la predicci\u00f3n de precios de viviendas  en Ames, Iowa.\n\nLa descripci\u00f3n e introducci\u00f3n de la competici\u00f3n reza as\u00ed:\n\n- P\u00eddale a un comprador de vivienda que describa la casa de sus sue\u00f1os, y probablemente no comenzar\u00e1 con la altura del techo del s\u00f3tano o la proximidad a un ferrocarril que cruce todo el pa\u00eds. No obstante, este conjunto de datos para esta distendida competici\u00f3n demuestra que influye mucho m\u00e1s sobre el precio las negociaciones que el n\u00famero de dormitorios o una valla blanca.\n\nObjetivos para la comeptici\u00f3n:\n\n- Con 79 variables explicativas que describen (casi) todos los aspectos de las viviendas residenciales en Ames, Iowa, esta competenci\u00f3n desaf\u00eda al usuario a predecir el precio final de cada casa.\n\nHabilidades a poner en pr\u00e1ctica:\n\n- Ingenier\u00eda creativa de funciones\n- T\u00e9cnicas de regresi\u00f3n avanzadas como 'Random Forest' y 'Gradient Boosting'.\n- Limpieza de datos y optimizaci\u00f3n de modelos","9c7f6bc0":"Tenemos 1168 entradas conocidas, divididas en 37 columnas. En adici\u00f3n, faltan 212 de la secci\u00f3n 'LotFrontage',6 de 'MasVnrArea' y 58 de 'GarageYrBlt'. \u00bfQu\u00e9 porcentaje de nuestros datos suponen estos faltantes?"}}