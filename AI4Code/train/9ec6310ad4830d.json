{"cell_type":{"2bc9806e":"code","fe256eea":"code","3042735e":"code","5e071328":"code","c30866f5":"code","ba3292a8":"code","b4154c74":"code","b530b662":"code","562aef3b":"code","d366a9d4":"code","62859635":"code","8f279e71":"code","89f98c88":"code","ea768afc":"code","c1709431":"code","5cd50f46":"code","7bb11797":"code","26453e39":"code","b9c01906":"code","447a9355":"code","e4bd5173":"code","86c0b7a3":"code","51a1ce46":"code","98186e4c":"code","4834e2d2":"code","20a69431":"code","9cd7c608":"code","35e1c28a":"code","a6068398":"code","8ece7b97":"code","a2967e8d":"code","bffa9280":"code","3760223b":"code","8cdb6c3d":"code","4f232b6c":"code","bf33122e":"markdown","97952a12":"markdown","fe5fb93e":"markdown","aa58bb36":"markdown"},"source":{"2bc9806e":"import numpy as np\nimport pandas as pd\nimport re\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fe256eea":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import layers as ly\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.optimizers import Adam\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom spellchecker import SpellChecker","3042735e":"df_train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ndf_train = df_train.drop([\"id\"], axis=1)\ndf_train = df_train.drop_duplicates()\ndf_train.head()","5e071328":"df_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ndf_test = df_test.drop([\"id\"], axis=1)\ndf_test.head()","c30866f5":"import random\n\nrandom.seed(42)","ba3292a8":"sentences = np.array(df_train[\"text\"])\ntest_sentences = np.array(df_test[\"text\"])\n\nlabels = np.array(df_train[\"target\"])\n\nrandom.shuffle(labels)\nrandom.shuffle(sentences)","b4154c74":"stop_words = set(stopwords.words('english')) \nlemmatizer = WordNetLemmatizer()","b530b662":"def lemmatize_sentence(text):\n    words = nltk.word_tokenize(text)\n    lemmatized = ' '.join([lemmatizer.lemmatize(w) for w in words])\n    return lemmatized\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n        u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n        u\"\\U0001F600-\\U0001F64F\"\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U0001F1F2\"\n        u\"\\U0001F1F4\"\n        u\"\\U0001F620\"\n        u\"\\u200d\"\n        u\"\\u2640-\\u2642\"\n        \"]+\", flags=re.UNICODE)\n\n    text = emoji_pattern.sub(r'', text)\n    return text\n\n\ndef clean_sentence(text):\n    text = re.sub(r\"http\\S+\", \"\", text) # remove urls\n    text = re.sub(r'@[^\\s]+','',text) # remove usernames\n    text = re.sub(r'[0-9]+', '', text)\n    text = text.replace(\"#\", \"\")\n    text = text.replace(\"can't\", \"can not\").replace(\"won't\", \"will not\").replace(\"n't\", \" not\")\n    text = text.replace(\"'m\", \" am\").replace(\"'re\", \" are\").replace(\"'s\", \"  is\").replace(\"'d\", \" would\")\n    text = text.replace(\"'ll\", \" will\").replace(\"'t\", \" not\").replace(\"'ve\", \"  have\")\n    text = remove_emoji(text)\n    for word in stop_words:\n        text = text.replace(\" \"+word+\" \", \" \")\n    text = ''.join([i for i in text if not i.isdigit()])\n    text = ' '.join([i for i in text.split(' ') if len(i) > 2])\n    text = lemmatize_sentence(text.lower())\n    return text","562aef3b":"sentences = [clean_sentence(sentence) for sentence in sentences]\n\ntest_sentences = [clean_sentence(text) for text in test_sentences]","d366a9d4":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(sentences)","62859635":"VOCAB_SIZE = len(tokenizer.word_index)\nMAX_LENGTH = 20\nEMBEDDING_DIM = 16\nSPLIT = 1000","8f279e71":"sequences = tokenizer.texts_to_sequences(sentences)\npadded = pad_sequences(sequences, padding=\"post\", maxlen=MAX_LENGTH, truncating=\"post\")\n\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_padded = pad_sequences(test_sequences, padding=\"post\",maxlen=MAX_LENGTH, truncating=\"post\")","89f98c88":"sentences[:2]","ea768afc":"sequences[:2]","c1709431":"padded[:2]","5cd50f46":"from tensorflow.keras.callbacks import ModelCheckpoint\n\ncallback = ModelCheckpoint(\"model_NLP.h5\", monitor=\"val_accuracy\", save_best_only=True)","7bb11797":"model = Sequential([   \n    ly.Embedding(VOCAB_SIZE+1, EMBEDDING_DIM, input_length=MAX_LENGTH, trainable=False),\n    ly.Dropout(0.2),\n    ly.Conv1D(64, 5, activation='relu'),\n    ly.MaxPooling1D(4),\n    ly.LSTM(64),\n    ly.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.001), metrics=[\"accuracy\"])","26453e39":"model.summary()","b9c01906":"train_x = padded[SPLIT:]\ntrain_y = labels[SPLIT:]\n\nval_x = padded[:SPLIT]\nval_y = labels[:SPLIT]","447a9355":"history = model.fit(train_x, train_y,\n                    epochs=50,\n                    validation_data=(val_x, val_y),\n                    callbacks=[callback])","e4bd5173":"def plot_results(his):\n    loss = his.history[\"loss\"]\n    val_loss = his.history[\"val_loss\"]\n    acc = his.history[\"accuracy\"]\n    val_acc = his.history[\"val_accuracy\"]\n\n    plt.plot(loss)\n    plt.plot(val_loss)\n    plt.legend([\"loss\", \"val_loss\"])\n    plt.show()\n\n    plt.plot(acc)\n    plt.plot(val_acc)\n    plt.legend([\"acc\", \"val_acc\"])\n    plt.show()","86c0b7a3":"plot_results(history)","51a1ce46":"def make_predictions(best_model=True):\n    if best_model:\n        m = tf.keras.models.load_model(\"model_NLP.h5\")\n    else:\n        m = model\n    \n    prediction = m.predict(test_padded)\n\n    for i in range(15):\n        result = \"REAL\" if prediction[i] > 0.5 else \"FAKE\"\n        print(df_test.iloc[i][\"text\"], \" - \", result)\n    return prediction","98186e4c":"p = make_predictions()","4834e2d2":"p = make_predictions(False)","20a69431":"embedding_dict = dict()\nwith open(\"..\/input\/glove6b200d\/glove.6B.200d.txt\", \"r\") as f:\n    for line in f:\n        vals = line.split()\n        word = vals[0]\n        vects = np.array(vals[1:], dtype=\"float32\")\n        embedding_dict[word] = vects","9cd7c608":"word_index = tokenizer.word_index\n\nembedding_matrix = np.zeros((VOCAB_SIZE+1, 200))\nfor word, i in word_index.items():\n    if i > VOCAB_SIZE+1:\n        continue\n    emb_vec = embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec","35e1c28a":"model = Sequential([\n    ly.Embedding(VOCAB_SIZE+1, 200, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_LENGTH, trainable=False),\n    ly.Dropout(0.2),\n    ly.Conv1D(64, 5, activation='relu'),\n    ly.MaxPooling1D(4),\n    ly.LSTM(64),\n    ly.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.001), metrics=[\"accuracy\"])","a6068398":"model.summary()","8ece7b97":"history = model.fit(train_x, train_y,\n                    epochs=50,\n                    validation_data=(val_x, val_y),\n                    callbacks=[callback])","a2967e8d":"plot_results(history)","bffa9280":"pred = make_predictions()","3760223b":"pred = make_predictions(False)","8cdb6c3d":"def save_results(prediction):\n    TEST_RESULTS = []\n\n    for i in range(len(prediction)):\n        r = 1 if prediction[i] > 0.5 else 0\n        TEST_RESULTS.append(r)\n    \n    sub_df = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\n    sub_df[\"target\"] = TEST_RESULTS\n    sub_df.to_csv(\"submission.csv\",index=False)","4f232b6c":"save_results(pred)","bf33122e":"## Not the best either, folks  \u00af\\\\_(\u30c4)_\/\u00af\n## Welp, kinda shame, let's save the results to the submission file anyways","97952a12":"## Cleaning the tweets","fe5fb93e":"## Not the best results tbh. Let's try something else","aa58bb36":"## Tokenizing & making sequences"}}