{"cell_type":{"fbddd6f2":"code","5cfaab73":"code","bbf5eae0":"code","2af82015":"code","6f3c438f":"code","c6a0ea6f":"code","37f521b1":"code","841f32ba":"code","715bfbb4":"code","99ab31b4":"code","03099fe0":"code","1305c18a":"code","284044ca":"code","295694dc":"code","268ee769":"code","de78c008":"code","4bec9bcd":"code","647b0e10":"code","770aa97a":"code","669347e2":"code","4613c2de":"code","0d8c0c7d":"code","3d2bc4aa":"code","04952e82":"code","ad76b9f5":"code","36c1be8f":"code","f554317b":"code","74f928d4":"code","2aad59b5":"code","05c06405":"code","6644dc5d":"code","7297d7d8":"code","64622e6d":"code","8f9855c9":"code","3666a868":"code","e83d7f21":"code","ef141d9d":"code","b2fad33a":"code","cf5b14bb":"code","6514543d":"code","9126ce17":"code","0ed642c0":"code","ae5c9fcf":"code","ae2f0f20":"code","55616314":"code","505afe46":"code","2cf126e7":"markdown","53595915":"markdown","d6eef4f3":"markdown","f147c95f":"markdown","b0fb58ec":"markdown","ac544604":"markdown","ea6f46e6":"markdown","733031d2":"markdown","ce22ef11":"markdown","4973c92c":"markdown","762fa2b7":"markdown","149988bb":"markdown","1382a8a4":"markdown","01f227c5":"markdown","29e23ba0":"markdown","edab3215":"markdown","61fb316c":"markdown","3f5d95aa":"markdown","ba093db4":"markdown","6adaab58":"markdown","846921b4":"markdown","efa35416":"markdown","dd8df309":"markdown","a269ee12":"markdown"},"source":{"fbddd6f2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cross_validation import train_test_split\nimport keras\nfrom keras.layers import *\n%matplotlib inline","5cfaab73":"df = pd.read_csv(\"..\/input\/train.csv\")\ntestset = pd.read_csv(\"..\/input\/test.csv\")","bbf5eae0":"df.head()","2af82015":"testset.head()","6f3c438f":"ids = testset['Id']","c6a0ea6f":"df = df.drop('Id',axis = 1)\ntestset = testset.drop('Id',axis = 1)","37f521b1":"df.info()","841f32ba":"df.isnull().sum()","715bfbb4":"testset.isnull().sum()","99ab31b4":"df.sum()","03099fe0":"testset.sum()","1305c18a":"df = df.drop(['Soil_Type7', 'Soil_Type15'],axis =1)\ntestset = testset.drop(['Soil_Type7', 'Soil_Type15'],axis =1)\n","284044ca":"df[df['Soil_Type8'] == 1]","295694dc":"df[df['Soil_Type25'] == 1]","268ee769":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0,1))","de78c008":"labels = df.Cover_Type","4bec9bcd":"#labels = pd.get_dummies(labels)","647b0e10":"labels = labels.values","770aa97a":"features = df.drop('Cover_Type',axis =1)","669347e2":"features.shape","4613c2de":"testset.shape # since we dont have the covertype already.","0d8c0c7d":"features = features.values","3d2bc4aa":"testset = testset.values","04952e82":"features[0]","ad76b9f5":"testset[0]","36c1be8f":"features = scaler.fit_transform(features)","f554317b":"testset = scaler.transform(testset)","74f928d4":"features[0]","2aad59b5":"testset[0]","05c06405":"print(type(labels))\nprint(type(features))\nprint(type(testset))\n","6644dc5d":"labels = labels - 1","7297d7d8":"train_x,test_x,train_y,test_y = train_test_split(features,labels)","64622e6d":"print(train_x.shape,train_y.shape,test_x.shape,test_y.shape)","8f9855c9":"train_y","3666a868":"import lightgbm as lgb\nfrom sklearn.metrics import accuracy_score","e83d7f21":"gbm = lgb.LGBMClassifier(objective=\"mutliclass\",n_estimators=10000)\ngbm.fit(train_x,train_y,early_stopping_rounds = 100, eval_set = [(test_x,test_y)],verbose = 300)","ef141d9d":"ypred1 = gbm.predict(test_x)","b2fad33a":"ypred1","cf5b14bb":"accuracy_score(test_y,ypred1)","6514543d":"labels","9126ce17":"gbm1 = lgb.LGBMClassifier(objective=\"mutliclass\",n_estimators=4000)\ngbm1.fit(features,labels,verbose = 1000)","0ed642c0":"finalval = gbm1.predict(testset)","ae5c9fcf":"covertype = finalval + 1","ae2f0f20":"sub = pd.DataFrame({'Id':ids,'Cover_Type':covertype})","55616314":"output = sub[['Id','Cover_Type']]","505afe46":"output.to_csv(\"output1.csv\",index = False)","2cf126e7":"pred = modelmain.predict(testset)","53595915":"So, both the soiltypes 8 and 25 are for only Covertype 2. So, I think we can remove these columns also. But first lets run our model with these two columns","d6eef4f3":"Remove Id column as it has no significance.","f147c95f":"Lets convert the labels to one hot format which is preferred for labels. \nThere are many ways to do, i chose the one below","b0fb58ec":"Lets split our dataset into testset and trainset","ac544604":"# PLEASE UPVOTE, IF YOU LIKE IT.","ea6f46e6":"Lets create our labels","733031d2":"model = keras.models.Sequential()\nmodel.add(Dense(300,input_dim = 52,activation = 'relu'))\nmodel.add(Dense(700,activation = 'relu'))\nmodel.add(Dense(200,activation = 'relu'))\nmodel.add(Dense(7,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['categorical_accuracy'])\nmodel.fit(train_x,train_y,epochs=60,shuffle=True, verbose =1)","ce22ef11":"Here's how it looks after normalization\/scaling.","4973c92c":"Soiltype 8 and 25 has only one observation each, so lets check what those are.","762fa2b7":"I want to try neural networks on this data, since we have a lot of numerical data.\nWe need to scale the data to (0,1) for the better results in neural networks","149988bb":"Check for null values. Looks like we have no null values.","1382a8a4":"So, soiltype 7 and 15 has no values in it, that means none of the 7 cover types has a soiltype of 7 and 15, so lets these two from the list, as they have no real use.","01f227c5":"So, lets now run the same keras model on the whole train set (on whole features) and predict the testset.","29e23ba0":"print(\"The Accuracy on the sampled test set is\", model.evaluate(test_x,test_y)[1])","edab3215":"Heres the snapshot of features ","61fb316c":"Convert to numpy array, since NN's accept numpy arrays only","3f5d95aa":"# KERAS","ba093db4":"Lets create our features matrix","6adaab58":"Save the ID column of the testset, which is used at submission.","846921b4":"covertype = [np.argmax(i)+1 for i in pred]","efa35416":"Lets check if any column has no values\/not significant","dd8df309":"modelmain = keras.models.Sequential()\nmodelmain.add(Dense(300,input_dim = 52,activation = 'relu'))\nmodelmain.add(Dense(700,activation = 'relu'))\nmodelmain.add(Dense(200,activation = 'relu'))\nmodelmain.add(Dense(7,activation='softmax'))\nmodelmain.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['categorical_accuracy'])\nmodelmain.fit(features,labels,epochs=120,shuffle=True, verbose =0) #verbose 0 to display no logs","a269ee12":"As we discussed, NN's work better on the data which is scaled. So lets scale our data."}}