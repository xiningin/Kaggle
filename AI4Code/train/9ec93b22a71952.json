{"cell_type":{"3dd6f7c9":"code","46ce2cd7":"code","b11e3898":"code","9bfce415":"code","891e9f83":"code","77d542cf":"code","45f8aeee":"code","fae962f5":"code","b8c8ca9c":"code","545a7b7f":"code","382fae31":"code","c2576da5":"code","8f0a2222":"code","8b184b2b":"code","0d62be44":"code","1566c1e0":"code","1336634c":"code","782b23ec":"markdown","bba66406":"markdown","61e20533":"markdown","319cd3bb":"markdown"},"source":{"3dd6f7c9":"#___________________________________________\n\n# Creator Emmanuel Pintelas\n#___________________________________________\n\n# https:\/\/www.kaggle.com\/c\/deepfake-detection-challenge\/discussion\/128954\nimport gc\nimport os\n\nimport zipfile\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Dropout,BatchNormalization,Lambda,Activation,Input,Flatten,Reshape,Conv2DTranspose\nimport skimage\nfrom skimage.exposure import rescale_intensity\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom skimage.transform import rescale, resize, downscale_local_mean\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport skimage\nfrom PIL import Image\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.util import crop, pad\nfrom skimage.morphology import label\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, GlobalMaxPooling2D\nimport psutil\nimport multiprocessing as mp\nfrom tensorflow.keras.applications import ResNet50, DenseNet201, Xception, VGG16, InceptionV3, InceptionResNetV2, MobileNetV2, NASNetMobile\nimport copy\nmp.cpu_count()\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport sys\nfrom  skimage.feature import greycoprops\n#%matplotlib inline\n#import keras\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Dropout,BatchNormalization,Lambda,Activation,Input,Flatten,Reshape,Conv2DTranspose, LeakyReLU\nimport tensorflow.keras.backend as K\n#from keras.layers.merge import add\nfrom sklearn.model_selection import train_test_split\nimport os\nimport glob\nfrom time import time,asctime\nfrom random import randint as r\nimport random\nfrom skimage import io\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom numpy import save, load\nfrom   sklearn.metrics           import f1_score, accuracy_score, classification_report, confusion_matrix\nfrom sklearn import metrics\nfrom numpy import load\nimport pandas as pd\nimport skimage\nfrom skimage.transform import resize\nimport math\nfrom sklearn.svm import SVC\nfrom tensorflow.keras.layers import Input, Dense, UpSampling2D, Flatten, Reshape\nfrom itertools import combinations_with_replacement\nimport itertools\nimport numpy as np\nfrom skimage import filters, feature\nfrom skimage import img_as_float32\nfrom concurrent.futures import ThreadPoolExecutor\nimport scipy.stats\nimport nibabel as nib","46ce2cd7":"\ndef data_3d_loading(vid_path, divider, heigth, width):\n\n                    Frames_id = os.listdir(vid_path)\n                    video = []\n                    for fr_int in range(len(Frames_id)):\n                                    fr_str = str(fr_int+1)+'.jpg'\n                                    img_path = vid_path+'\/'+fr_str\n                                    img = io.imread(img_path) # all frames must be of same size\n                                    img = img.astype(int)\n                                    img = img\/255.\n                                    #img = img.astype(float)\n\n                                    img = cv2.resize(img, (heigth, width))\n                                    video.append(img)  \n                                    # plt.figure()\n                                    # plt.imshow(img)\n                                    # plt.show()\n\n                    video = np.array(video)\n\n                    video_sampled = []\n                    init_depth = video.shape[0]\n                        #divider = int (init_depth\/depth)\n                    for _i in range(init_depth):\n                                if _i % divider == 0:\n                                    video_sampled.append(video[_i])\n\n                                    # plt.figure()\n                                    # plt.imshow(video[_i])\n                                    # plt.show()\n\n                    video_sampled = np.array(video_sampled)\n\n                    return video_sampled\n\nclass Data_Generator_3D_CNN(tf.keras.utils.Sequence):\n        'Generates data for Keras'\n        def __init__(self,mode='',\n                    batch_size=1, divider='', width='', heigth='', paths = '', labels = '', n_channels=3, shuffle=False):\n\n            'Initialization'\n            self.mode = mode\n            self.batch_size = batch_size\n            self.paths = paths   # video paths of jpg frame images\n            self.labels = labels # video labels\n            self.divider = divider   # Number of frames\n            self.width = width\n            self.heigth = heigth\n            self.n_channels = n_channels\n            #self.shuffle = shuffle\n            #self.augment = augmentations\n            self.on_epoch_end()\n            self.cnt = 0\n\n\n        def __len__(self):\n            'Denotes the number of batches per epoch'\n            return int(np.ceil(len(self.paths) \/ self.batch_size))\n\n        def __getitem__(self, index):\n            'Generate one batch of data'\n            # Generate indexes of the batch\n            indexes = self.indexes[index*self.batch_size:min((index+1)*self.batch_size,len(self.paths))]\n            # Find list of IDs\n            list_IDs_im = [self.paths[k] for k in indexes]\n\n            Y = np.empty((len(list_IDs_im),2))\n            j=-1\n            for i in indexes:\n                j+=1\n                Y[j,] = self.labels[i] \n                #print(i)\n\n            # Generate data\n            \u03a7   = self.data_generation(list_IDs_im)\n            \n            self.cnt +=1\n            #print (self.cnt)\n\n            if self.mode == 'predict':\n                return \u03a7   \n            elif self.mode == 'train_C': \n                return \u03a7, Y      \n            elif self.mode == 'train_AE': \n                \u03a71 = \u03a7\n                \u03a72 = np.copy(\u03a71)\n                return \u03a71, \u03a72  \n\n        def on_epoch_end(self):\n            'Updates indexes after each epoch'\n            self.indexes = np.arange(len(self.paths))\n            # if self.shuffle == True:\n            #     np.random.shuffle(self.indexes)\n\n        def data_generation(self, list_IDs_im):\n            'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n            # Initialization\n\n            depth = 80\/self.divider\n            X = np.empty((len(list_IDs_im),int(depth),self.width,self.heigth,self.n_channels))\n\n            # Generate data\n            for i, vid_path in enumerate(list_IDs_im):  \n                vid_path = vid_path[0]\n                X[i,] = data_3d_loading(vid_path, self.divider, self.heigth, self.width)\n\n                # plt.figure()\n                # plt.imshow(X[0][0])\n                # plt.show()\n\n            return X\n","b11e3898":"if 1==1:\n    \n    root_path = '..\/input\/dataset-medical\/Dataset_Medical'\n\n    CT_VP_tr_path = [\n        os.path.join(os.getcwd(), root_path+\"\/Train\/CT_VP_path234\", x)\n        for x in os.listdir(root_path+\"\/Train\/CT_VP_path234\")\n    ] \n    CT_N_tr_path = [\n        os.path.join(os.getcwd(), root_path+\"\/Train\/CT_N_path\", x)\n        for x in os.listdir(root_path+\"\/Train\/CT_N_path\")\n    ] \n\n    CT_VP_ts_path = [\n        os.path.join(os.getcwd(), root_path+\"\/Test\/CT_VP_path234\", x)\n        for x in os.listdir(root_path+\"\/Test\/CT_VP_path234\")\n    ] \n    CT_N_ts_path = [\n        os.path.join(os.getcwd(), root_path+\"\/Test\/CT_N_path\", x)\n        for x in os.listdir(root_path+\"\/Test\/CT_N_path\")\n    ] \n\n    VP_tr_labels = np.array([1 for _ in range(len(CT_VP_tr_path))])\n    N_tr_labels = np.array([0 for _ in range(len(CT_N_tr_path))])\n    VP_ts_labels = np.array([1 for _ in range(len(CT_VP_ts_path))])\n    N_ts_labels = np.array([0 for _ in range(len(CT_N_ts_path))])\n\n\n    train_paths = np.concatenate((pd.DataFrame(CT_VP_tr_path), pd.DataFrame(CT_N_tr_path)), axis=0)\n    test_paths = np.concatenate((pd.DataFrame(CT_VP_ts_path), pd.DataFrame(CT_N_ts_path)), axis=0)\n    Labels_train = np.concatenate((VP_tr_labels, N_tr_labels), axis=0)\n    Labels_test = np.concatenate((VP_ts_labels, N_ts_labels), axis=0)\n\n\n    from sklearn.preprocessing import OneHotEncoder\n    onehot_encoder = OneHotEncoder()\n    Labels_train_oh = onehot_encoder.fit_transform(Labels_train.reshape(-1,1)).toarray()#.astype(int)\n    Labels_test_oh = onehot_encoder.fit_transform(Labels_test.reshape(-1,1)).toarray()#.astype(int)\n\n\n    from random import shuffle\n    ind_list = [_r_ for _r_ in range(len(train_paths))]\n    shuffle(ind_list)\n    train_paths  = train_paths[ind_list]\n    Labels_train_oh  = Labels_train_oh[ind_list]\n    Labels_train  = Labels_train[ind_list]\n","9bfce415":"def Compile_CNN(model,lr):\n                    # Compile model.\n                    initial_learning_rate = lr \n                    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n                        initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n                    )\n                    model.compile(\n                        loss='binary_crossentropy',#focal_loss(),  # 'binary_crossentropy'   'categorical_crossentropy', 'sparse_categorical_crossentropy',\n                        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),   # 0.001\n                        metrics=['accuracy'] # Geometric_Mean\n                    )\n\n                    # Define callbacks.\n                    checkpoint_cb = keras.callbacks.ModelCheckpoint(\n                        \"CNN_Classifier.h5\",\n                        mode='max',\n                        monitor=['val_acc'], # Geometric_Mean\n                        save_best_only=True\n                    )\n                    early_stopping_cb = keras.callbacks.EarlyStopping(monitor=['val_acc'], patience=3)\n                    return model, checkpoint_cb, early_stopping_cb\n\n\ndef Train_CNN(model, epochs, train_video_g, test_video_g, checkpoint_cb, early_stopping_cb):\n                    model.fit(train_video_g,\n                                                    verbose=1,\n                                                    epochs=epochs,\n                                                    #############################callbacks=[checkpoint_cb, early_stopping_cb],\n                                                    #validation_data=test_video_g,\n                                                    shuffle=False)\n                    return model\n","891e9f83":"def Build_AE3D_EE(depth, width, heigth):\n                    inputs = keras.Input((int(depth), width, heigth, 3)) # init 300,150,150\n\n                    x = layers.Conv3D(filters=16, kernel_size=(3,3,3), strides=(1,1,1), padding='same')(inputs)\n                    x = layers.LeakyReLU()(x)\n                    x = layers.MaxPool3D(pool_size=(2,2,2))(x)\n                    \n                    x = layers.Conv3D(filters=3, kernel_size=(3,3,3), strides=(5,3,3), padding='same')(x)\n                    x = layers.LeakyReLU()(x)\n            \n    \n    \n                    x = tf.keras.layers.Conv3DTranspose (filters=32, kernel_size=(3,3,3), strides=(5,3,3), padding='same')(x) \n                    x = layers.LeakyReLU()(x)\n                    x = tf.keras.layers.Conv3DTranspose (filters=16, kernel_size=(3,3,3), strides=(2,2,2), padding='same')(x) \n                    x = layers.LeakyReLU()(x)\n                    \n                    outputs =  layers.Conv3DTranspose(filters=3, kernel_size=(3,3,3), strides=(1,1,1), activation=\"sigmoid\", padding='same')(x)\n\n                    # Define the model.\n                    ae3d = keras.Model(inputs, outputs, name=\"ae_3D\")\n                    \n                    return ae3d","77d542cf":"\n# CAE = load_model('..\/input\/ae3d-deepfake\/CAE_150.h5')\n# CAE.summary()","45f8aeee":"if 1==2:\n    divider = 1\n    heigth, width = 300, 300\n    depth = 80\/divider\n\n    # for vid_path in train_paths:  \n    #         image_3d = data_3d_loading(vid_path, divider, heigth, width)\n\n\n    #CAE = Build_AE3D_EE(depth, width, heigth)\n    CAE = load_model('..\/input\/ae3d-med\/CAE_10_50.h5')\n    CAE.summary()\n    ","fae962f5":"train_g = Data_Generator_3D_CNN(mode='train_AE', batch_size=1, divider=divider, width=width, heigth=heigth, paths = train_paths, labels = Labels_train_oh, n_channels=3, shuffle=False)\ntest_g = Data_Generator_3D_CNN(mode='train_AE', batch_size=1, divider=divider, width=width, heigth=heigth, paths = test_paths, labels = Labels_test_oh, n_channels=3, shuffle=False)","b8c8ca9c":"\n\n    \nif 1==2:\n    lr = 0.001    # 0.001  0.0005   0.0001  0.00005\n    CAE, checkpoint_cb, early_stopping_cb = Compile_CNN (CAE, lr)\n    # Training  (Very Slow)\n    if 1==1:    \n                    epochs = 5\n                    CAE = Train_CNN(CAE, epochs, train_g, train_g, checkpoint_cb, early_stopping_cb)  \n    \n    s = 1\n","545a7b7f":"\n#CAE.save('CAE_8_50.h5')","382fae31":"if 1==2:\n\n    layer =  CAE.layers [5]  # [3]\n    layer_name = layer.name\n    \u0395\u0395ncoder_3d = Model(inputs=CAE.input, outputs=CAE.get_layer(layer_name).output)\n    \u0395\u0395ncoder_3d.summary()\n    #encoder_3d.save('encoder_3d_Cascade.h5',encoder_3d)\n\n\n    trial = test_g[0][0]\n    Reconstructed = CAE.predict(trial)\n    encoded = \u0395\u0395ncoder_3d.predict(trial)\n    encoded = encoded [0]\n    Reconstructed = Reconstructed [0]\n    trial = trial [0]","c2576da5":"#Reconstructed[40,150,150]","8f0a2222":"#trial[40,150,150]","8b184b2b":"if 1==2:        \n        en_video = trial\n        en_video.shape\n        fig=plt.figure(figsize=(22,22))\n        columns = 10 #6\n        rows = 8     #8\n        for i in range(1,  columns*rows+1):  # 48\n                    img = en_video[i-1,:,:,:] #en_video[:,:,i-1]\n                    fig.add_subplot(rows, columns, i)\n                    plt.axis('off')\n                    plt.imshow(img)\n        plt.show()","0d62be44":"\nif 1==2:\n        en_video = encoded\n        en_video.shape\n        fig=plt.figure(figsize=(22,22))\n        columns = 5#6\n        rows = 2#8\n        for i in range(1,  columns*rows+1):  # 48\n                    img = en_video[i-1,:,:,:] #en_video[:,:,i-1]\n                    fig.add_subplot(rows, columns, i)\n                    plt.axis('off')\n                    plt.imshow(img)\n        plt.show()\n\n","1566c1e0":"#img[25]","1336634c":"if 1==2:\n        en_video = Reconstructed\n        en_video.shape\n        fig=plt.figure(figsize=(22,22))\n        columns = 10#6\n        rows = 8#8\n        for i in range(1,  columns*rows+1):  # 48\n                    img = en_video[i-1,:,:,:] #en_video[:,:,i-1]\n                    fig.add_subplot(rows, columns, i)\n                    plt.axis('off')\n                    plt.imshow(img)\n        plt.show()","782b23ec":"ENCODED\/COMPRESSED","bba66406":"CAE3D OUTPUT VIZUALIZATION","61e20533":"RECONSTRUCTED","319cd3bb":"INITIAL"}}