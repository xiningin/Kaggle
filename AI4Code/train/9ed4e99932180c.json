{"cell_type":{"d362d5e2":"code","be044708":"code","7059b962":"code","c012dd27":"code","5a2a73a9":"code","31111795":"code","c3d1b092":"code","cb432e91":"code","7dcd9abf":"code","2d1c3b9b":"code","83ace279":"code","de0e33dd":"code","fda370ef":"code","adf08e58":"code","1c2b6f30":"code","e01a9a55":"code","e74b3de6":"code","596d3bd4":"code","3b4a7bb6":"code","ceefaaad":"code","7b438f65":"code","0ce15f78":"code","4b39d31c":"code","aabaffdd":"code","64904c08":"code","53c0998d":"code","14b3bffe":"code","05c07af1":"code","3ff25159":"code","9885a72a":"code","18e9b5f1":"markdown","1d52ae7d":"markdown","4edf5521":"markdown","f14e1370":"markdown","18e5473a":"markdown","62db6e3e":"markdown","2cc313bf":"markdown","9633439a":"markdown","8afce733":"markdown","b02025ac":"markdown","53f27280":"markdown","23de9e91":"markdown","9c263e3e":"markdown","8ef1a043":"markdown","758406c5":"markdown","0bdb7752":"markdown","8df86fe6":"markdown","b29875cf":"markdown","560eb336":"markdown","d2b201b6":"markdown","e132966e":"markdown","17dec2f3":"markdown","46b6e0f2":"markdown","faa5ff92":"markdown","2d5ac4ab":"markdown","1daf9fd9":"markdown","cbc302fd":"markdown","730f9da9":"markdown","613d0e4d":"markdown","b9c5ff8c":"markdown","c0689014":"markdown","8fc1126e":"markdown"},"source":{"d362d5e2":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport re\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly\nplotly.offline.init_notebook_mode()\nimport plotly.graph_objs as go\nimport plotly.express as px\n\nimport plotly.figure_factory as ff\nimport cufflinks as cf\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nsns.set_context(\"paper\")\nplt.style.use('seaborn')","be044708":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\n#f = open('..\/input\/house-prices-advanced-regression-techniques\/data_description.txt','r')\n#print(f.read())","7059b962":"df.sample(3)","c012dd27":"df.drop('Id',axis=1, inplace=True)\n\ncategory_features = list(df.dtypes[df.dtypes == 'object'].reset_index()['index'])\nnumerical_features = list(df.dtypes[df.dtypes != 'object'].reset_index()['index'])","5a2a73a9":"null_df = df[category_features].isnull().sum().reset_index()\nnull_df.columns = ['Feature','Null_Values']\nnull_df = null_df.query('Null_Values > 0').sort_values(by='Null_Values')\n\nfig  = px.bar(data_frame=null_df, x='Feature', y='Null_Values', template='seaborn')\nfig.update_layout(width=800, height=300, title= {'text': \"Categorical Features with Null values\",\n                                                'y':0.95,'x':0.5,\n                                                'xanchor': 'center','yanchor': 'top'})","31111795":"null_df = df[numerical_features].isnull().sum().reset_index()\nnull_df.columns = ['Feature','Null_Values']\nnull_df = null_df.query('Null_Values > 0').sort_values(by='Null_Values')\n\nfig  = px.bar(data_frame=null_df, x='Feature', y='Null_Values', template='seaborn')\nfig.update_layout(width=500, height=300, title= {'text': \"Numerical Features with Null values\",\n                                                'y':0.95,'x':0.5,\n                                                'xanchor': 'center','yanchor': 'top'})","c3d1b092":"# Creating a dataframe of columns having null values\nnull_df = df.isnull().sum().reset_index()\nnull_df.columns = ['Feature','Null_Values']\nnull_df = null_df.query('Null_Values > 0')\n\nnull_df_features = null_df['Feature']\n\n\n\n# Function to fill Null values in Train\/Test Dataset\ndef fill_na(features_series, dataset):\n    for feature in list(features_series):\n        # If datatype is Object\n        if dataset[feature].dtype == 'O':\n            # All the categorical features with null value mean 'No Present' as mentioned in the description\n            dataset[feature] = dataset[feature].fillna('None')\n        else:\n            # If datatype is Float or Int\n            dataset[feature] = dataset[feature].fillna(dataset[feature].median())\n\nfill_na(null_df_features, df)","cb432e91":"print(dict(df.isnull().sum()))","7dcd9abf":"total_bsmt_calc = df['BsmtFinSF1'] + df['BsmtFinSF2'] +df['BsmtUnfSF']\nfig = plt.figure(figsize=(5,5))\n_ = sns.lineplot(total_bsmt_calc, df['TotalBsmtSF'])","2d1c3b9b":"df.drop(['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF'], axis=1, inplace=True)","83ace279":"total_floor_sf = df['1stFlrSF'] + df['2ndFlrSF'] + df['LowQualFinSF']\nfig = plt.figure(figsize=(5,5))\n_ = sns.lineplot(total_floor_sf, df['GrLivArea'])","de0e33dd":"df.drop(['1stFlrSF','2ndFlrSF','LowQualFinSF'], axis=1, inplace=True)","fda370ef":"total_porch_area = df[['WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch']].sum(axis=1)\ndf['TotalPorchSF'] = total_porch_area\n\ndf.drop(['WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch'], axis=1, inplace=True)","adf08e58":"df.head(5)","1c2b6f30":"fig = plt.figure(figsize=(25,15))\n\nax1 = fig.add_subplot(421)\n_ = sns.boxplot(data=df, y='LotArea',ax=ax1)\nax1.set_title('Lot Area', fontsize=30)\n\nax2 = fig.add_subplot(422)\n_ = sns.boxplot(data=df, y='MasVnrArea',ax=ax2)\nax2.set_title('Masonry veneer type Area', fontsize=30)\n\nax3 = fig.add_subplot(423)\n_ = sns.boxplot(data=df, y='TotalBsmtSF',ax=ax3)\nax3.set_title('TotalBsmtSF', fontsize=30)\n\nax4 = fig.add_subplot(424)\n_ = sns.boxplot(data=df, y='GrLivArea',ax=ax4)\nax4.set_title('Ground Living Area', fontsize=30)\n\nax5 = fig.add_subplot(425)\n_ = sns.boxplot(data=df, y='TotalPorchSF',ax=ax5)\nax5.set_title('Total Porch Area', fontsize=30)\n\nax6 = fig.add_subplot(426)\n_ = sns.boxplot(data=df, y='GarageArea',ax=ax6)\nax6.set_title('Garage Area', fontsize=30)","e01a9a55":"numerical_features_new = list(df.dtypes[df.dtypes != 'object'].reset_index()['index'])\n\nfig = plt.figure(figsize=(15,20))\nax = fig.gca()\n_ = df[numerical_features_new].hist(ax=ax)","e74b3de6":"from sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\n\nscaler = MinMaxScaler(feature_range=(1, 2))\npower = PowerTransformer(method='box-cox')\npipeline = Pipeline(steps=[('s', scaler),('p', power)])\n\ndf[numerical_features_new] = pipeline.fit_transform(df[numerical_features_new])","596d3bd4":"fig = plt.figure(figsize=(15,20))\nax = fig.gca()\n_ = df[numerical_features_new].hist(ax=ax)","3b4a7bb6":"fig = plt.figure(figsize=(25,15))\n\nax1 = fig.add_subplot(421)\n_ = sns.boxplot(data=df, y='LotArea',ax=ax1)\nax1.set_title('Lot Area', fontsize=30)\n\nax2 = fig.add_subplot(422)\n_ = sns.boxplot(data=df, y='MasVnrArea',ax=ax2)\nax2.set_title('Masonry veneer type Area', fontsize=30)\n\nax3 = fig.add_subplot(423)\n_ = sns.boxplot(data=df, y='TotalBsmtSF',ax=ax3)\nax3.set_title('TotalBsmtSF', fontsize=30)\n\nax4 = fig.add_subplot(424)\n_ = sns.boxplot(data=df, y='GrLivArea',ax=ax4)\nax4.set_title('Ground Living Area', fontsize=30)\n\nax5 = fig.add_subplot(425)\n_ = sns.boxplot(data=df, y='TotalPorchSF',ax=ax5)\nax5.set_title('Total Porch Area', fontsize=30)\n\nax6 = fig.add_subplot(426)\n_ = sns.boxplot(data=df, y='GarageArea',ax=ax6)\nax6.set_title('Garage Area', fontsize=30)","ceefaaad":"X = df.drop(['SalePrice'], axis=1)\ny = df['SalePrice']\n\nX = pd.get_dummies(X,drop_first=True)","7b438f65":"from sklearn.preprocessing import LabelEncoder\n\n#label = LabelEncoder()\n\n# Labelling categorical features\n#for feature in category_features:\n#    X[feature] = label.fit_transform(X[feature])","0ce15f78":"from sklearn.decomposition import PCA\n\npca = PCA()\n\nX_PCA = pca.fit_transform(X)","4b39d31c":"display(pca.explained_variance_ratio_.cumsum())\n\nn_features = range(pca.n_components_)\n_ = plt.figure(figsize=(20,12))\n_ = plt.bar(n_features, pca.explained_variance_)","aabaffdd":"pca = PCA(n_components=165)\n\nX_PCA = pca.fit_transform(X)","64904c08":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.svm import SVR\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom mlxtend.regressor import StackingCVRegressor","53c0998d":"# Cross Validation\nkfolds = KFold(n_splits=20, shuffle=True, random_state=42)\n\n# Hyper parameters\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# Regression models\nridge = RidgeCV(alphas=alphas_alt, cv=kfolds)\nlasso = LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds)\nelasticnet = ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio)                                \nsvr = SVR(C= 20, epsilon= 0.008, gamma=0.0003)\n\ngbr = GradientBoostingRegressor(n_estimators=3000, \n                                learning_rate=0.05, \n                                max_depth=4, \n                                max_features='sqrt',\n                                min_samples_leaf=15, \n                                min_samples_split=10, \n                                loss='huber', \n                                random_state =42)  \n\nlightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )\n\nxgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","14b3bffe":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X_PCA):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)\n\n# Scoring\nscore = cv_rmse(ridge)\nprint(\"Ridge: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lasso)\nprint(\"Lasso: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(elasticnet)\nprint(\"elastic net: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()) )\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()) )","05c07af1":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)\n\n# Scoring\nscore = cv_rmse(ridge)\nprint(\"Ridge: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lasso)\nprint(\"Lasso: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(elasticnet)\nprint(\"elastic net: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()) )\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()) )","3ff25159":"print('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n\nprint('elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\n\nprint('Lasso')\nlasso_model_full_data = lasso.fit(X, y)\n\nprint('Ridge')\nridge_model_full_data = ridge.fit(X, y)\n\nprint('Svr')\nsvr_model_full_data = svr.fit(X, y)\n\nprint('GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\n\nprint('xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\n\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)","9885a72a":"def blend_models_predict(X):\n    return ((0.01 * elastic_model_full_data.predict(X)) + \\\n            (0.01 * lasso_model_full_data.predict(X)) + \\\n            (0.01 * ridge_model_full_data.predict(X)) + \\\n            (0.15 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.2 * xgb_model_full_data.predict(X)) + \\\n            (0.22 * lgb_model_full_data.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))\n\nprint('RMSLE score:')\nprint(rmsle(y, blend_models_predict(X)))","18e9b5f1":"As 'BsmtFinSF1' + 'BsmtFinSF2' + 'BsmtUnfSF' =  TotalBsmtSF\n\nWe can now drop 'BsmtFinSF1','BsmtFinSF2','BsmtUnfSF'","1d52ae7d":"#### a. Basement Area ","4edf5521":"After comparing the plots(Before and After Transformation) we could see how well the numerical values have been transformed","f14e1370":"We have many numerical features having value 0 which could raise error while transformation, \nhence we need to first Scale the data between positive values (1 and 2)","18e5473a":"### vii. One Hot encoding","62db6e3e":"#### c. Porch Area","2cc313bf":"### vii. Labelling the categorical values","9633439a":"#### b. Box-Plot of different Area after transformation","8afce733":"#### Limiting the number of features to 165","b02025ac":"### viii. Need for Dimensionality Reduction?","53f27280":"### iii. Looking for any redundant features\/ Feature Engineering","23de9e91":"### 2. Cleaning\/Preprocessing data","9c263e3e":"### 3. Applying different Linear Regression models","8ef1a043":"### iv. Exploring different areas using Box-Plot","758406c5":"### vi. Power transformation","0bdb7752":"Hello,\n     Welcome to this NoteBook.\n\nI have used a very clean and comprehensive approach in predicting the housing prices.\n\nYou will find a structured flow of data cleaning and preprocessing techniques used and also comparison of different methods used and their impact on different model's performance.\n\nPlease give an upvote if you have liked the approach to modelling.\n\nTable of contents:\n 1. Reading data\n 2. Cleaning & preprocessing data:\n*         i.    Dividing categorical and numerical features.  \n*         ii.   Filling all the null values at once  \n*         iii.  Feature Engineering & Looking for any redundant features\n*         iv.   Exploring different areas\n*         v.    Distribution of Numerical features\n*         vi.   Power Transformation (BOX-COX)\n*         vii.  One hot encoding\n*         viii. Dimensionality reduction (PCA)\n 3. Applying Regression models\n*         i.    Running the models using Limited features(Dimensionality Reduction : PCA)\n*         ii.   Running the models using whole dataset\n*         iii.  Comparing performance \n*         iv.   Blending models","8df86fe6":"#### i. Dividing Categorical and Numerical Features","b29875cf":"### 1. Reading data","560eb336":"Analyzing the Principle components","d2b201b6":"* 0.99 Variance of the whole data can be achieved using 165 out 250 features\n* We will run the models against these dimensionally reduced features(165) and also with the whole data (250 features)\n* We will compare if reducing the features using PCA has done benefit in improving the performance of the models","e132966e":"#### i. Running the models using X_PCA (165 features)","17dec2f3":"* The score of linear models using One hot encoded and Label encoded features were compared\n* The score of models using one hot encoded values was better","46b6e0f2":"#### iii. Fitting","faa5ff92":"#### ii. Filling all the null values at once","2d5ac4ab":"* Reducing the number of features does not seem to have improved the scoring, hence we will continue fitting models using whole dataset","1daf9fd9":"#### iv. Blending Models","cbc302fd":"#### a. Distribution of values after Transformation","730f9da9":"* As clearly seen from above plots, the numerical features and the target variable do not follow a Normal Distribution.\n* There are skews present in some of the features like TotalPorchSf, MasVnrArea,etc\n* We need to transform the data before feeding it to Linear Models","613d0e4d":"Cross-verifying the data","b9c5ff8c":"#### ii. Running the models using whole dataset (250 features)","c0689014":"### v. Plotting the distribution of the numerical columns","8fc1126e":"#### b. Floor Area (Square Feet)"}}