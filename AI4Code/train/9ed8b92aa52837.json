{"cell_type":{"ca3cb807":"code","867df267":"code","a769a098":"code","de8aa8d2":"code","571a7a07":"code","763054f7":"code","907c0b60":"code","43a81a44":"code","f2903c28":"code","eac3d370":"code","4eb190ef":"code","71db0f23":"code","65674e36":"code","032a9660":"code","ea696ad1":"code","3acfc299":"code","2bb21f0a":"code","e50e5e1e":"code","5fe0ef01":"markdown","b69a3a26":"markdown"},"source":{"ca3cb807":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms,models\nimport os\n","867df267":"os.listdir(\"..\/input\/digit-recognizer\")","a769a098":"train=pd.read_csv('..\/input\/digit-recognizer\/train.csv',dtype=np.float32)\nfinal_test=pd.read_csv('..\/input\/digit-recognizer\/test.csv',dtype=np.float32)\nsample_sub=pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\",\n                       dtype=np.float32)\n                     ","de8aa8d2":"train.head()","571a7a07":"final_test.head()\nsample_sub.shape","763054f7":"# Seperate the features and lables\n# lables\ntargets_np=train.label.values\n# scaling\/255\nfeatures_np=train.loc[:,train.columns!='label'].values\n\n# split into training and validation set\n# 0.8 for training,0.2 for testing\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features_np, targets_np, test_size=0.2, random_state=42)","907c0b60":"# convert numpy to tensor\nfeaturesTrain=torch.from_numpy(features_train)\ntargetsTrain=torch.from_numpy(target_train).type(torch.LongTensor)\n\nfeaturesTest = torch.from_numpy(features_test)\ntargetsTest = torch.from_numpy(target_test).type(torch.LongTensor)","43a81a44":"# numpy\nfeatures_train=features_train.reshape(-1,1,28,28)\nfeatures_test=features_test.reshape(-1,1,28,28)","f2903c28":"# torch\nfeaturesTrain=featuresTrain.view(-1,1,28,28)\nfeaturesTest=featuresTest.view(-1,1,28,28)","eac3d370":"from torch.utils.data import Dataset,DataLoader,TensorDataset\n# data augmentation\ntrain_transform=transforms.Compose([\n    transforms.ToPILImage(mode='L'),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    ]\n)\n\ntest_transform = transforms.Compose([\n    transforms.ToPILImage(mode='L'),                                    \n    transforms.ToTensor(),\n])\n    \nclass ImgDataset(Dataset):\n    def __init__(self, x, y=None, transform=None):\n        self.x = x\n        # label is required to be a LongTensor\n        self.y = y\n        if y is not None:\n            self.y = torch.LongTensor(y)\n        self.transform = transform\n    def __len__(self):\n        return len(self.x)\n    # index\n    def __getitem__(self, index):\n        X = self.x[index]\n        if self.transform is not None:\n            X = self.transform(X)\n        if self.y is not None:\n            Y = self.y[index]\n            return X, Y\n        else:\n            return X","4eb190ef":"# set batch size\nbatch_size=256\n\n# pytorch train and test sets\ntrain=torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest=torch.utils.data.TensorDataset(featuresTest,targetsTest)\n# using numpy\n#train=ImgDataset(featuresTrain,targetsTrain,train_transform)\n#test=ImgDataset(featuresTest,targetsTest,test_transform)\n# data loader\ntrain_loader=torch.utils.data.DataLoader(train,\n                                        batch_size=batch_size,shuffle=True)\ntest_loader=torch.utils.data.DataLoader(test,\n                                       batch_size=batch_size,shuffle=False)\n","71db0f23":"featuresTrain.dtype","65674e36":"def visualize_image(data, index, pred=False, val=0):\n    '''This funtion can be used to visualize the images'''\n    plt.imshow(data[index].reshape(28,28))\n    plt.axis(\"off\")\n    plt.title(\"Handwritten Digit Image\")\n    plt.show()\nvisualize_image(features_np, 13)","032a9660":"import torch.nn.functional as F\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        #nn.conv2d(in_channels,out_channels,kernel_size,stride,padding)\n        # \u7b2c\u4e00\u5c42\u5377\u79ef\u5c42(1,28,28)->(32,28,28)\n        # input channel 1,output channel,kernel size 3 \n        self.conv1=nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,\n                            stride=1,padding=1)\n        self.batchnorm1=nn.BatchNorm2d(32)\n        # \u7b2c\u4e8c\u5c42\u5377\u79ef\u5c42(32,28,28)->(32,14,14)->(64,14,14)\n        self.conv2=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,\n                           stride=1,padding=1)\n        self.batchnorm2=nn.BatchNorm2d(64)\n        # \u7b2c\u4e09\u5c42\u5377\u79ef\u5c42(64,7,7)->(128,6,6)\n        # change kernel size(3,3)\n        self.conv3=nn.Conv2d(64,128,2)\n        self.batchnorm3=nn.BatchNorm2d(128)\n        # maxpooling(128,6,6)->(128,3,3)\n        # \u5168\u8fde\u63a5\u5c42\u56db\u5c42\n        self.fc1=nn.Linear(128*3*3,60)\n        self.fc2=nn.Linear(60,10)\n        # maxpooling\n        self.pool=nn.MaxPool2d(2,2)\n        #dropout\n        self.dropout=nn.Dropout(p=0.2)\n        # output layer\n        self.log_softmax=F.log_softmax\n    def forward(self,x):\n        # \u7b2c\u4e00\u5c42\u5377\u79ef\n        x=self.pool(F.relu(self.batchnorm1(self.dropout(self.conv1(x)))))\n        # \u7b2c\u4e8c\u5c42\u5377\u79ef\n        x=self.pool(F.relu(self.batchnorm2(self.dropout(self.conv2(x)))))\n        # \u7b2c\u4e09\u5c42\u5377\u79ef\n        x=self.pool(F.relu(self.batchnorm3(self.dropout(self.conv3(x)))))\n        # \u5c55\u5f00\n        x=x.view(-1,128*3*3)\n        x=self.dropout(F.relu(self.fc1(x)))\n        x=self.fc2(x)\n        x=self.log_softmax(x,dim=1)\n        return x\n\n\ndevice=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel=Net()\nmodel=model.to(device)\n\n\n\n\nfrom torch import optim\n\ncriterion=nn.NLLLoss()\noptimizer=optim.Adam(model.parameters(),lr=0.0005)\nstep=0\ntrain_losses, test_losses = [], []\nepochs = 35\nfor e in range(epochs):\n    running_loss=0.0\n    for images,labels in train_loader:\n        step+=1\n        optimizer.zero_grad()\n        log_ps=model(images.to(device))\n        loss=criterion(log_ps,labels.to(device))\n        # backprop\n        loss.backward()\n        optimizer.step()\n        running_loss+=loss.item()\n        if step%50==0:\n            test_loss=0\n            accuracy=0\n            with torch.no_grad():\n                model.eval()\n                for images,labels in test_loader:\n                    images,labels=images.to(device),labels.to(device)\n                    log_ps=model(images)\n                    test_loss+=criterion(log_ps,labels)\n                    ps = torch.exp(log_ps).to(device)\n                    # Get our top predictions\n                    top_p, top_class = ps.topk(1, dim=1)\n                    equals = top_class == labels.view(*top_class.shape)\n                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n                \n                model.train()\n                train_losses.append(running_loss\/len(train_loader))\n                test_losses.append(test_loss\/len(test_loader))\n\n                print(\"Epoch: {}\/{}.. \".format(e+1, epochs),\n                  \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n                  \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n                  \"Test Accuracy: {:.3f}\".format(accuracy\/len(test_loader)))\n\n        \n        \n            \n         \n        \n\n        \n        \n        ","ea696ad1":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nplt.plot(train_losses, label='Training loss')\nplt.plot(test_losses, label='Validation loss')\nplt.legend(frameon=False)","3acfc299":"final_test_np = final_test.values\/255\ntest_tn = torch.from_numpy(final_test_np)\ntest_tn=test_tn.view(-1,1,28,28)\nfake_labels = np.zeros(final_test_np.shape)\nfake_labels = torch.from_numpy(fake_labels)\nsubmission_tn_data = torch.utils.data.TensorDataset(test_tn, fake_labels)\nsubmission_loader = torch.utils.data.DataLoader(submission_tn_data, batch_size = batch_size, shuffle = False)","2bb21f0a":"submission = [['ImageId', 'Label']]\n\n# Turn off gradients for validation\nwith torch.no_grad():\n    model.eval()\n    image_id = 1\n    for images, _ in submission_loader:\n        images=images.to(device)\n        log_ps = model(images)\n        ps = torch.exp(log_ps)\n        top_p, top_class = ps.topk(1, dim=1)\n        \n        for prediction in top_class:\n            submission.append([image_id, prediction.item()])\n            image_id += 1","e50e5e1e":"submission_df = pd.DataFrame(submission)\nsubmission_df.columns = submission_df.iloc[0]\nsubmission_df = submission_df.drop(0, axis=0)\nsubmission_df.to_csv(\"submission.csv\", index=False)","5fe0ef01":"### design your model","b69a3a26":"### import the data"}}