{"cell_type":{"36b42594":"code","283d5378":"code","0190a8fa":"code","73b44a04":"markdown"},"source":{"36b42594":"%%writefile geometry.py\nimport operator\nimport numpy as np\nimport cmath\nfrom typing import List\nfrom collections import namedtuple\nimport traceback\nimport sys\n\n\nbasis = np.array(\n    [1, cmath.exp(2j * cmath.pi * 1 \/ 3), cmath.exp(2j * cmath.pi * 2 \/ 3)]\n)\n\n\nHistMatchResult = namedtuple(\"HistMatchResult\", \"idx length\")\n\n\ndef find_all_longest(seq, max_len=None) -> List[HistMatchResult]:\n    \"\"\"\n    Find all indices where end of `seq` matches some past.\n    \"\"\"\n    result = []\n\n    i_search_start = len(seq) - 2\n\n    while i_search_start > 0:\n        i_sub = -1\n        i_search = i_search_start\n        length = 0\n\n        while i_search >= 0 and seq[i_sub] == seq[i_search]:\n            length += 1\n            i_sub -= 1\n            i_search -= 1\n\n            if max_len is not None and length > max_len:\n                break\n\n        if length > 0:\n            result.append(HistMatchResult(i_search_start + 1, length))\n\n        i_search_start -= 1\n\n    result = sorted(result, key=operator.attrgetter(\"length\"), reverse=True)\n\n    return result\n\n\ndef probs_to_complex(p):\n    return p @ basis\n\n\ndef _fix_probs(probs):\n    \"\"\"\n    Put probs back into triangle. Sometimes this happens due to rounding errors or if you\n    use complex numbers which are outside the triangle.\n    \"\"\"\n    if min(probs) < 0:\n        probs -= min(probs)\n\n    probs \/= sum(probs)\n\n    return probs\n\n\ndef complex_to_probs(z):\n    probs = (2 * (z * basis.conjugate()).real + 1) \/ 3\n    probs = _fix_probs(probs)\n    return probs\n\n\ndef z_from_action(action):\n    return basis[action]\n\n\ndef sample_from_z(z):\n    probs = complex_to_probs(z)\n    return np.random.choice(3, p=probs)\n\n\ndef bound(z):\n    return probs_to_complex(complex_to_probs(z))\n\n\ndef norm(z):\n    return bound(z \/ abs(z))\n\n\nclass Pred:\n    def __init__(self, *, alpha):\n        self.offset = 0\n        self.alpha = alpha\n        self.last_feat = None\n\n    def train(self, target):\n        if self.last_feat is not None:\n            offset = target * self.last_feat.conjugate()   # fixed\n\n            self.offset = (1 - self.alpha) * self.offset + self.alpha * offset\n\n    def predict(self, feat):\n        \"\"\"\n        feat is an arbitrary feature with a probability on 0,1,2\n        anything which could be useful anchor to start with some kind of sensible direction\n        \"\"\"\n        feat = norm(feat)\n\n        # offset = mean(target - feat)\n        # so here we see something like: result = feat + mean(target - feat)\n        # which seems natural and accounts for the correlation between target and feat\n        # all RPSContest bots do no more than that as their first step, just in a different way\n        \n        result = feat * self.offset\n\n        self.last_feat = feat\n\n        return result\n    \n    \nclass BaseAgent:\n    def __init__(self):\n        self.my_hist = []\n        self.opp_hist = []\n        self.my_opp_hist = []\n        self.outcome_hist = []\n        self.step = None\n\n    def __call__(self, obs, conf):\n        try:\n            if obs.step == 0:\n                action = np.random.choice(3)\n                self.my_hist.append(action)\n                return action\n\n            self.step = obs.step\n\n            opp = int(obs.lastOpponentAction)\n            my = self.my_hist[-1]\n\n            self.my_opp_hist.append((my, opp))\n            self.opp_hist.append(opp)\n\n            outcome = {0: 0, 1: 1, 2: -1}[(my - opp) % 3]\n            self.outcome_hist.append(outcome)\n\n            action = self.action()\n\n            self.my_hist.append(action)\n\n            return action\n        except Exception:\n            traceback.print_exc(file=sys.stderr)\n            raise\n\n    def action(self):\n        pass\n\n\nclass Agent(BaseAgent):\n    def __init__(self, alpha=0.01):\n        super().__init__()\n\n        self.predictor = Pred(alpha=alpha)\n\n    def action(self):\n        self.train()\n\n        pred = self.preds()\n\n        return_action = sample_from_z(pred)\n\n        return return_action\n\n    def train(self):\n        last_beat_opp = z_from_action((self.opp_hist[-1] + 1) % 3)\n        self.predictor.train(last_beat_opp)\n\n    def preds(self):\n        hist_match = find_all_longest(self.my_opp_hist, max_len=20)\n\n        if not hist_match:\n             return 0\n\n        feat = z_from_action(self.opp_hist[hist_match[0].idx])\n\n        pred = self.predictor.predict(feat)\n\n        return pred\n    \n    \nagent = Agent()\n\n\ndef call_agent(obs, conf):\n    return agent(obs, conf)","283d5378":"%run geometry.py\n\nclass AttrDict(dict):\n    __setattr__ = dict.__setitem__\n    __getattr__ = dict.__getitem__\n\nfor step in range(10):\n    last = np.random.choice(3)\n    call_agent(\n        AttrDict(step=step, lastOpponentAction=last), AttrDict(signs=3)\n    )","0190a8fa":"from kaggle_environments import evaluate, make, utils\n\nenv = make(\"rps\")\nenv.reset()\nenv.run([\"geometry.py\", \"statistical\"]);\nenv.render(mode=\"ipython\", width=600, height=450)","73b44a04":"This notebook was meant as a demonstration of how to use complex numbers to rewrite the commonly seen RPSContest bot logic. It implements just the very first step of those bots (look for past history and predict last subsequent action plus some learned offset). The only reason to use complex numbers is to make the whole learning and logic continuous, rather than discrete, when it is extended.\n\nIt turns out this most basic version is quite competitive and has been in silver ranks for a bit. The whole logic is in the `Pred` class below, but as said it's nothing new. I believe the more important part is `sample_from_z` which picks the final decision according to the predicted probabilities and not by the most likely one. This probably makes the bot beat weak bots, but also resistant to exploitation, because it will be random if it cannot find a pattern.\n\nHere is the details about the mathematical ideas on how to make the logic continuous, but I could also skip to the code below.\n*Upvote* if you find this helpful to make this a gold notebook!\n\nI present an implementation of a basic predictor using the complex number idea from [(1)](https:\/\/www.kaggle.com\/c\/rock-paper-scissors\/discussion\/201988) and [(2)](https:\/\/www.kaggle.com\/c\/rock-paper-scissors\/discussion\/210305). Complex numbers come in handy because adding two probability distribution is just a plain multiplication of their complex number representation.\n\nIt could be even better vs stupid bots if you remove more randomness, but it wins either way.\n\nFor probability distributions \\\\(P_1\\\\) and \\\\(P_2\\\\) and their complex representations \\\\(z_1\\\\) and \\\\(z_2\\\\) you have:\n\n$$P_1 + P_2 \\equiv z_1z_2$$\n$$P_1 - P_2 \\equiv z_1z_2^*$$\n\nThe core of the prediction is the following calculation. First you need a feature \\\\(x_n\\\\) for which you believe there is a meaningful relation to the target. Most bots use the opponent action that came last time after the same current history was observed previously. However, we assume that we cannot just use the raw feature \\\\(x_n\\\\), but instead more generally the new opponent action will be a *shifted* version of it, which is a multiplication in z-space\n\n$$\\hat{y}_n=ax_n$$\n\nTo have the best estimator, you would use \\\\(a=\\langle y x^*\\rangle\\\\). So, our estimator for some kind of feature is\n\n$$\\hat{y}_n=x_n \\langle y x^*\\rangle$$\n\nThe mean is calculated on past and an exponential mean cold be used to focus on more recent past. This is all that happens here in the class `Pred`.\n"}}