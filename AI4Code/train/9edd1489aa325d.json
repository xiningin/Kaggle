{"cell_type":{"5e02d7d1":"code","52a80580":"code","515c3e66":"code","ad3182fc":"code","212e5cbe":"code","0e38248b":"code","01d29d06":"code","528c098b":"code","21c8445d":"code","9743b638":"code","52a4a8e2":"code","7e9609a4":"code","f6838a84":"code","50d458e8":"markdown","4dc3f605":"markdown","18d400bc":"markdown","6d7074ee":"markdown","6b7e7a12":"markdown","b8cc69dd":"markdown","3570b568":"markdown","5188e365":"markdown","c1655906":"markdown","10ec22b9":"markdown","7bc6e937":"markdown","766c0e34":"markdown","92ec16ab":"markdown","915dbb36":"markdown"},"source":{"5e02d7d1":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report ","52a80580":"a = input('Enter you Name or enrollment no.:- ')","515c3e66":"iris = datasets.load_iris()\nX = iris.data[:, :2] # we only take the first two features.\ny = iris.target ","ad3182fc":"#Plotting Function\ndef visualize_classifier(classifier, X, y):\n    # Define the minimum and maximum values for X and Y\n    # that will be used in the mesh grid\n    min_x, max_x = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0\n    min_y, max_y = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0\n\n    # Define the step size to use in plotting the mesh grid \n    mesh_step_size = 0.01\n\n    # Define the mesh grid of X and Y values\n    x_vals, y_vals = np.meshgrid(np.arange(min_x, max_x, mesh_step_size), np.arange(min_y, max_y, mesh_step_size))\n\n    # Run the classifier on the mesh grid\n    output = classifier.predict(np.c_[x_vals.ravel(), y_vals.ravel()])\n\n    # Reshape the output array\n    output = output.reshape(x_vals.shape)\n\n    # Create a plot\n    plt.figure()\n\n    # Choose a color scheme for the plot \n    plt.pcolormesh(x_vals, y_vals, output, cmap=plt.cm.gray)\n\n    # Overlay the training points on the plot \n    plt.scatter(X[:, 0], X[:, 1], c=y, s=75, edgecolors='black', linewidth=1, cmap=plt.cm.Paired)\n\n    # Specify the boundaries of the plot\n    plt.xlim(x_vals.min(), x_vals.max())\n    plt.ylim(y_vals.min(), y_vals.max())\n\n    # Specify the ticks on the X and Y axes\n    plt.xticks((np.arange(int(X[:, 0].min() - 1), int(X[:, 0].max() + 1), 1.0)))\n    plt.yticks((np.arange(int(X[:, 1].min() - 1), int(X[:, 1].max() + 1), 1.0)))\n    \n    plt.title(f'{a}')\n    plt.show()","212e5cbe":"X_train, X_test, y_train, y_test =train_test_split(X, y,\ntest_size=0.25,random_state=5)","0e38248b":"C = 1.0 # SVM regularization parameter\nsvc = SVC(kernel='linear', C=1,gamma='auto').fit(X, y)\nsvc.fit(X_train, y_train)\nvisualize_classifier(svc, X_train, y_train)","01d29d06":"y_test_pred = svc.predict(X_test)\nvisualize_classifier(svc, X_test, y_test)\n","528c098b":"target_names = ['Class-' + str(int(i)) for i in set(y)]\nprint (\"\\n\" + \"#\"*30)\nprint (\"\\nClassifier performance on training dataset\\n\")\nprint(classification_report(y_train,svc.predict(X_train), target_names=target_names))\nprint (\"#\"*30 + \"\\n\") ","21c8445d":"breast = datasets.load_breast_cancer()\nX = breast.data[:,:2]\ny = breast.target\n\nX_train, X_test, y_train, y_test =train_test_split(X, y,\ntest_size=0.25,random_state=5)\n\nC = 1.0 # SVM regularization parameter\nsvc = SVC(kernel='linear', C=1,gamma='auto').fit(X, y)\nsvc.fit(X_train, y_train)\nvisualize_classifier(svc, X_train, y_train)","9743b638":"y_test_pred = svc.predict(X_test)\nvisualize_classifier(svc, X_test, y_test)","52a4a8e2":"C = 1.0    \n\n\nsvc = SVC(kernel='linear', C=C,gamma='auto').fit(X, y)\nsvc.fit(X_train, y_train)\nvisualize_classifier(svc, X_train, y_train)\ny_test_pred = svc.predict(X_test)\nvisualize_classifier(svc, X_test, y_test)","7e9609a4":"C = 5.0    \n\n\nsvc = SVC(kernel='linear', C=C,gamma='auto').fit(X, y)\nsvc.fit(X_train, y_train)\nvisualize_classifier(svc, X_train, y_train)\ny_test_pred = svc.predict(X_test)\nvisualize_classifier(svc, X_test, y_test)","f6838a84":"C = 10.0 \n\n\nsvc = SVC(kernel='linear', C=C,gamma='auto').fit(X, y)\nsvc.fit(X_train, y_train)\nvisualize_classifier(svc, X_train, y_train)\ny_test_pred = svc.predict(X_test)\nvisualize_classifier(svc, X_test, y_test)","50d458e8":"### Splitting of data into 75% used for training and 25% for testing","4dc3f605":"### Loading the data","18d400bc":"## For Pratice purpose:-\n\n**Q1. Use Linear SVM method for breast cancer data available in datasets module.**","6d7074ee":"### Printing the Reports","6b7e7a12":"### Defining the Plotting function","b8cc69dd":"## How does SVM works?\n\n**Linear SVM :**\n\nThe working of the SVM algorithm can be understood by using an example. Suppose we have a dataset that has two tags (green and blue), and the dataset has two features x1 and x2. We want a classifier that can classify the pair(x1, x2) of coordinates in either green or blue.\n\n<center><img src =\"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/support-vector-machine-algorithm3.png\" width = 300>\n\n    \nSo as it is 2-d space so by just using a straight line, we can easily separate these two classes. But there can be multiple lines that can separate these classes.\n\n<centre>\n<img src = \"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/support-vector-machine-algorithm4.png\" width = 300>\n<end>\n    \nHence, the SVM algorithm helps to find the best line or decision boundary; this best boundary or region is called as a **hyperplane**. SVM algorithm finds the closest point of the lines from both the classes. These points are called **support vectors**. The distance between the vectors and the hyperplane is called as **margin**. And the goal of SVM is to maximize this margin. The hyperplane with maximum margin is called the **optimal hyperplane**.\n\n<centre>\n<img src =\"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/support-vector-machine-algorithm5.png\" width = 300>","3570b568":"**SVM algorithm can be used for Face Detection, Image classification, Text Categorization, etc.**\n\n## Types of SVM:\n\n#### SVM can be of two types:\n\n- **Linear SVM:** Linear SVM used for linearly seperable data, which means if a dataset can be classified into two classes by using a single straight line, then such data is termed as linearly separable data, and classifier is used called Linear SVM Classifier.\n\n- **Non-Linear SVM:**  Non-Linear SVM is used for non-linearly separated data, which means if a dataset cannot be classified by using a straight line, then such data is termed as non-linear data and classifier used is called as Non-linear SVM classifier.\n\n### Hyperplane and support vectors in SVM algorithm:\n\n**Hyperplane:**\n\nThere can be multiple lines\/decision boundaries to segregate the classes in n-dimensional space, but we need to find out the best decision boundary that helps to classify the data points. This best boundary is known as the hyperplane of SVM.\n\nThe dimensions of the hyperplane depend on the features present in the dataset, which means if there are 2 features (as shown in image), then hyperplane will be a straight line. And if there are 3 features, then hyperplane will be a 2-dimension plane.\n\nWe always create a hyperplane that has a maximum margin, which means the maximum distance between the data points.\n\n**Support Vectors:**\n\nThe data points or vectors that are the closest to the hyperplane and which affect the position of the hyperplane are termed as Support Vector. Since these vectors support the hyperplane, hence called a Support vector.","5188e365":"Since we are in 3-d Space, hence it is looking like a plane parallel to the x-axis. If we convert it in 2d space with $z=1$, then it will become as:\n\n<center><img src = \"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/support-vector-machine-algorithm9.png\" width = 300>\n\nHence we get a circumference of radius 1 in case of non-linear data","c1655906":"### Activating our Classifier","10ec22b9":"## Modules used in this:\n\n- `Numpy` :- Numerical python used for mathematical operations\n\n- `Matplotlib` :- A plotting module, used to visualize input data, classification and output or predicted data\n\n- `Sklearn` :- A Machine Learning library which has all datasets for practice purpose, all necessary algorithms and scoring functions.","7bc6e937":"### Predicting output","766c0e34":"**Non-Linear SVM:**\n\nIf data is linearly arranged, then we can separate it by using a straight line, but for non-linear data, we cannot draw a single straight line.\n\n<center><img src =\"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/support-vector-machine-algorithm6.png\" width = 300><center\/>\n    \nSo to separate these data points, we need to add one more dimension. For linear data, we have used two dimensions x and y, so for non-linear data, we will add a third dimension z. It can be calculated as:\n    \n> $$z = x^2 + y^2$$\n\nSo now, SVM will divide the datasets into classes in the following way.\n    \n<center><img src = \"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/support-vector-machine-algorithm8.png\" width = 300>","92ec16ab":"**Q2. Check the effect of penalty function using C values.**","915dbb36":" # SVM (Support Vector Machine)\n\n## What is the Support Vector Machine?\n\nSupport Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning.\n\nThe goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane.\n\nSVM chooses the extreme points\/vectors that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as Support Vector Machine. Consider the below diagram in which there are two different categories that are classified using a decision boundary or hyperplane:\n\n<center><img src =\"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/support-vector-machine-algorithm.png\">\n"}}