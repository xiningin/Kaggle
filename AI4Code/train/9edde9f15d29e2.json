{"cell_type":{"a08d3bd5":"code","5d3b91be":"code","35961e7c":"code","ff5d06b7":"code","4dbabb78":"code","cea37fa4":"code","6d9802e6":"code","3407610c":"code","cfa54410":"code","5962d5d7":"code","e375c8f5":"code","35b23ed1":"code","16a88ef1":"code","315323ba":"code","72f312b7":"code","bbead0a0":"code","e98888f0":"code","a3806ad4":"code","9ec1421c":"code","b3b1754a":"code","d9b03ce6":"code","d519daab":"markdown","878ab88e":"markdown","38cb17c7":"markdown","0bd68f76":"markdown","f33d746a":"markdown","237b6c50":"markdown","cbba7d8f":"markdown","fb4bbecb":"markdown","525f3b4c":"markdown","34653813":"markdown","643f4fbe":"markdown","260ba75c":"markdown","40a04594":"markdown","62931702":"markdown","a1f5a1be":"markdown","2f2d04af":"markdown","2904d784":"markdown","5f28f25a":"markdown","83ec1a1b":"markdown","04beb72b":"markdown","6c3ffc0b":"markdown","e9bb63d7":"markdown","42bd67e9":"markdown","52785bb3":"markdown"},"source":{"a08d3bd5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d3b91be":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers","35961e7c":"train = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\")\ntest = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip\")","ff5d06b7":"train.head()","4dbabb78":"print(train.isnull().sum())\nprint(test.isnull().sum())","cea37fa4":"x_train = train[\"comment_text\"]\n\ny_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\n\nx_test = test[\"comment_text\"]","6d9802e6":"max_feature = 20000\n\ntokenizer = Tokenizer(num_words = max_feature)\ntokenizer.fit_on_texts(list(x_train))\ntokenized_train = tokenizer.texts_to_sequences(x_train)\ntokenized_test = tokenizer.texts_to_sequences(x_test)","3407610c":"tokenized_train[:1]","cfa54410":"maxlen = 50\nx_train = pad_sequences(tokenized_train, maxlen = maxlen)\nx_test = pad_sequences(tokenized_test, maxlen = maxlen)","5962d5d7":"input = Input(shape = (maxlen,))","e375c8f5":"embed_size = 128\nx = Embedding(max_feature, embed_size)(input)","35b23ed1":"x = LSTM(60, return_sequences = True, name = \"lstm_layer\")(x)","16a88ef1":"x = GlobalMaxPool1D()(x)","315323ba":"x = Dropout(0.1)(x)","72f312b7":"x = Dense(50, activation = \"relu\")(x)","bbead0a0":"#Once again we have implemented dropout to prevent overfitting\n\nx = Dropout(0.1)(x)","e98888f0":"x = Dense(6, activation = \"sigmoid\")(x)","a3806ad4":"model = Model(inputs = input, outputs = x)\nmodel.compile(loss = \"binary_crossentropy\",\n             optimizer = \"adam\",\n             metrics = [\"accuracy\"])","9ec1421c":"batch_size = 32\nepochs = 2\nmodel.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_split = 0.1)","b3b1754a":"y_pred = model.predict(x_test,batch_size=32)","d9b03ce6":"submission = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip')\nsubmission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\nsubmission.to_csv('submission.csv', index=False)","d519daab":"We are done with all the preprocessing of the text, now we move on to buiding the model.","878ab88e":"# Fit Model to the Data","38cb17c7":"Now we are using Dropout() to deactivate a few neurons (Here 10% of the total active neurons). This helps prevent the model overfit over the training data.","0bd68f76":"Here, we first tokenize each word in the comment.\n\nEx. [\"Hello World\", \"I am here\", \"How are you doing\"]\n\nAfter tokenization: [\"Hello\", \"World\", \"I\", \"am\", \"here\", \"How\", \"are\", \"you\", \"doing\"]\n\nAfter this we provide each word here with an index number. It converts to a dictionary.\n{\"Hello\":1, \"World\":2, \"I\":3, \"am\":4, \"here\":5, \"How\":6, \"are\":7, \"you\":8, \"doing\":9}\n\nThe text in the first example now converts to:[[1, 2], [3, 4, 5], [6, 7, 8, 9]]","f33d746a":"# Check for null values","237b6c50":"# IMPORTING LIBRARIES","cbba7d8f":"# Tokenize the input comments","fb4bbecb":"1. INPUT\n\nWe take input in batches of (maxlen)-dimensional vectors.","525f3b4c":"Check for null values, if found, we need to find a solution to deal with them.","34653813":"We provide out model with the training data to train.","643f4fbe":"5. FIRST DENSE LAYER\n\nWe have implemented our first Dense layer with 50 neurons and Rectified Linear Unit(ReLU) as it's activation function.","260ba75c":"# Apply Padding\n\nThere are a total of 159571 training example. All of the comments(strings) might not be of the same size i.e. some strings might be too long eg. 300 words and some might be too short eg. 30 words. We cannot feed model with inputs of varying length.\n\nTo deal with this issue, we need to equalize the length of comments(strings) to some extent in order to provide them as an input to the model. To do this we use the method of padding.\n\nPadding adds zeros at the end of the comments whose length is less than the provided length(Here, maxlen = 200) to match it to the mentioned length(Here maxlen=200)\n\nEx. Let's consider the previous example, after tokenization we were left with: [[1, 2], [3, 4, 5], [6, 7, 8, 9]]\nAfter padding(with maxlen = 4): [[1, 2, 0, 0], [3, 4, 5, 0], [6, ,7 ,8 ,9]]  #To make training examples of equal length","40a04594":"Now we separate the training set into:\n* Independent component - The comments\n* Dependent component - The columns containing categories of hate speech.","62931702":"7. COMPILING LAYERS\n\nWe have created all the layers, now it's time to compile all the layers and specify loss function, optimizer and metric, to make the model fully functional.","a1f5a1be":"# Separate x and y component from training set","2f2d04af":"Here we have used LSTM for the classification of comments.\n\nStep-by-step layers: \n\n(Input)-->(Embedding)-->(LSTM layer)-->(Max Pooling layer)-->(First Dense layer)-->(Second Dense layer)","2904d784":"6. SECOND DENSE LAYER\n\nHere, we have implemented our second Dense layer and final layer of the model with 6 neurons and Sigmoid as it's activation function.\n\nNote: Here we have used Sigmoid, because we need an output between 0 and 1 to decide in which of the six categories does the training example fall into.","5f28f25a":"3. LSTM LAYER\n\nThis is the LSTM layer which can be considered the main processing layer.\nThe power of LSTMs is applied to the embedded vectors, and vectors are downsized to 60-dimensional vector keeping the information intact.","83ec1a1b":"# Predicting output of test set","04beb72b":"# Importing Dataset","6c3ffc0b":"No null values were found. We are good to go!","e9bb63d7":"4. MAX POOLING \n\nThis layer is used to flatten the inputs for dense layer.","42bd67e9":"2. EMBEDDING\n\nIn this layer we represent each word in the training example with a 128-dimensional vector.","52785bb3":"# Building the Model"}}