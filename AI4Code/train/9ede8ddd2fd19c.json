{"cell_type":{"7889f20a":"code","73ea43df":"code","9ef34bd4":"code","cfbd6444":"code","2aee7422":"code","eff134ce":"code","d4c97f4a":"code","29a74d38":"code","0820db3e":"code","77cab83e":"code","18cf3723":"code","c5d00f64":"code","acb9772a":"markdown","b05892fe":"markdown","cee203e4":"markdown","11d94445":"markdown","8af6e301":"markdown","9e736a00":"markdown","cd6a0a38":"markdown","aac2a851":"markdown"},"source":{"7889f20a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # used to plot visual representation\nimport json #used for Pretty Printing dict later\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.express as px\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","73ea43df":"df_pg1 = pd.read_csv('..\/input\/solar-power-generation-data\/Plant_1_Generation_Data.csv')\ndf_wsd1 = pd.read_csv('..\/input\/solar-power-generation-data\/Plant_1_Weather_Sensor_Data.csv')\ndf_pg2 = pd.read_csv('..\/input\/solar-power-generation-data\/Plant_2_Generation_Data.csv')\ndf_wsd2 = pd.read_csv('..\/input\/solar-power-generation-data\/Plant_2_Weather_Sensor_Data.csv')","9ef34bd4":"print(df_pg1.columns)\nprint(df_wsd1.columns)\nprint()\nprint(df_pg1.describe())\nprint()\nprint(df_wsd1.describe())","cfbd6444":"print('Number of Inverters at Station 1: ' + str(df_pg1['SOURCE_KEY'].nunique()))\nprint('Number of Inverters at Station 2: ' + str(df_pg2['SOURCE_KEY'].nunique()))\nprint('Total Number of Inverters at Stations 1 and 2: ' + str(df_pg1['SOURCE_KEY'].nunique() + df_pg2['SOURCE_KEY'].nunique()))","2aee7422":"print('Mean of Daily Yield at Station 1: ' + str(df_pg1['DAILY_YIELD'].mean()))\nprint('Mean of Daily Yield at Station 2: ' + str(df_pg2['DAILY_YIELD'].mean()))\nprint('Mean of Daily Yield across the Station 1 and 2: ' + str(pd.concat((df_pg1, df_pg2))['DAILY_YIELD'].mean()))","eff134ce":"df_pg1['DATE_TIME'] = pd.to_datetime(df_pg1['DATE_TIME'])\ndf_wsd1['DATE_TIME'] = pd.to_datetime(df_wsd1['DATE_TIME'], format = '%Y-%m-%d %H:%M')\ndf_pg2['DATE_TIME'] = pd.to_datetime(df_pg2['DATE_TIME'])\ndf_wsd2['DATE_TIME'] = pd.to_datetime(df_wsd2['DATE_TIME'], format = '%Y-%m-%d %H:%M')\ndf_pg1['DATE'] = df_pg1['DATE_TIME'].dt.date\ndf_pg1['TIME'] = df_pg1['DATE_TIME'].dt.time\ndf_pg2['DATE'] = df_pg2['DATE_TIME'].dt.date\ndf_pg2['TIME'] = df_pg2['DATE_TIME'].dt.time\ndf_wsd1['DATE'] = df_wsd1['DATE_TIME'].dt.date\ndf_wsd1['TIME'] = df_wsd1['DATE_TIME'].dt.time\ndf_wsd2['DATE'] = df_wsd2['DATE_TIME'].dt.date\ndf_wsd2['TIME'] = df_wsd2['DATE_TIME'].dt.time","d4c97f4a":"print('Total Irridation per day on Station 1 and 2:')\ndf_grp_irr1 = df_wsd1.groupby([df_wsd1['DATE']])['IRRADIATION'].sum()\ndf_grp_irr2 = df_wsd2.groupby([df_wsd2['DATE']])['IRRADIATION'].sum()\nprint(pd.concat([df_grp_irr1, df_grp_irr2], axis = 1))","29a74d38":"print('Maximum Ambient Temperature at Station 1: ' + str(df_wsd1['AMBIENT_TEMPERATURE'].max()))\nprint('Maximum Ambient Temperature at Station 2: ' + str(df_wsd2['AMBIENT_TEMPERATURE'].max()))\nprint()\nprint('Maximum Module Temperature at Station 1:' + str(df_wsd1['MODULE_TEMPERATURE'].max()))\nprint('Maximum Module Temperature at Station 2:' + str(df_wsd2['MODULE_TEMPERATURE'].max()))","0820db3e":"df_grp_ac1_max = df_pg1.groupby([df_pg1['DATE']])['AC_POWER'].max()\ndf_grp_dc1_max = df_pg1.groupby([df_pg1['DATE']])['DC_POWER'].max()\ndf_grp_ac2_max = df_pg2.groupby([df_pg2['DATE']])['AC_POWER'].max()\ndf_grp_dc2_max = df_pg2.groupby([df_pg2['DATE']])['DC_POWER'].max()\ndf_grp_ac1_min = df_pg1.groupby([df_pg1['DATE']])['AC_POWER'].min()\ndf_grp_dc1_min = df_pg1.groupby([df_pg1['DATE']])['DC_POWER'].min()\ndf_grp_ac2_min = df_pg2.groupby([df_pg2['DATE']])['AC_POWER'].min()\ndf_grp_dc2_min = df_pg2.groupby([df_pg2['DATE']])['DC_POWER'].min()\ndf_grp_ac1_min_nz = df_pg1[df_pg1['AC_POWER'] != 0].groupby([df_pg1['DATE_TIME'].dt.date])['AC_POWER'].min()\ndf_grp_dc1_min_nz = df_pg1[df_pg1['DC_POWER'] != 0].groupby([df_pg1['DATE_TIME'].dt.date])['DC_POWER'].min()\ndf_grp_ac2_min_nz = df_pg2[df_pg2['AC_POWER'] != 0].groupby([df_pg2['DATE_TIME'].dt.date])['AC_POWER'].min()\ndf_grp_dc2_min_nz = df_pg2[df_pg2['DC_POWER'] != 0].groupby([df_pg2['DATE_TIME'].dt.date])['DC_POWER'].min()\n\nprint('Maximum AC and DC Power at Station 1 each day:')\nprint(pd.concat([df_grp_ac1_max, df_grp_dc1_max], axis = 1))\nprint('\\nMaximum AC and DC Power at Station 2 each day:')\nprint(pd.concat([df_grp_ac2_max, df_grp_dc2_max], axis = 1))\nprint('\\nMinimum AC and DC Power at Station 1 each day:')\nprint(pd.concat([df_grp_ac1_min, df_grp_dc1_min], axis = 1))\nprint('\\nMinimum AC and DC Power at Station 2 each day:')\nprint(pd.concat([df_grp_ac2_min, df_grp_dc2_min], axis = 1))\nprint('\\nMinimum AC and DC Power at Station 1 each day(Non Zero):')\nprint(pd.concat([df_grp_ac1_min_nz, df_grp_dc1_min_nz], axis = 1))\nprint('\\nMinimum AC and DC Power at Station 2 each day(Non Zero):')\nprint(pd.concat([df_grp_ac2_min_nz, df_grp_dc2_min_nz], axis = 1))","77cab83e":"print('ID of Inverter producing Maximum AC Power at Station 1: ' + (df_pg1.iloc[df_pg1['AC_POWER'].idxmax()])['SOURCE_KEY'])\nprint('ID of Inverter producing Maximum DC Power at Station 1: ' + (df_pg1.iloc[df_pg1['DC_POWER'].idxmax()])['SOURCE_KEY'])\nprint('ID of Inverter producing Maximum AC Power at Station 2: ' + (df_pg2.iloc[df_pg2['AC_POWER'].idxmax()])['SOURCE_KEY'])\nprint('ID of Inverter producing Maximum DC Power at Station 2: ' + (df_pg2.iloc[df_pg2['DC_POWER'].idxmax()])['SOURCE_KEY'])","18cf3723":"dc_mean = {}\nac_mean = {}\nfor i in df_pg1['SOURCE_KEY'].unique():\n    dc_mean[i] = df_pg1[df_pg1['SOURCE_KEY'] == i]['DC_POWER'].mean()\n    ac_mean[i] = df_pg1[df_pg1['SOURCE_KEY'] == i]['AC_POWER'].mean()\ndc_mean1 = {sk: m for sk, m in sorted(dc_mean.items(), key = lambda item: item[1], reverse = True)}\nac_mean1 = {sk: m for sk, m in sorted(ac_mean.items(), key = lambda item: item[1], reverse = True)}\nfor i in df_pg2['SOURCE_KEY'].unique():\n    dc_mean[i] = df_pg2[df_pg2['SOURCE_KEY'] == i]['DC_POWER'].mean()\n    ac_mean[i] = df_pg2[df_pg2['SOURCE_KEY'] == i]['AC_POWER'].mean()\ndc_mean2 = {sk: m for sk, m in sorted(dc_mean.items(), key = lambda item: item[1], reverse = True)}\nac_mean2 = {sk: m for sk, m in sorted(ac_mean.items(), key = lambda item: item[1], reverse = True)}\n\nprint('Inverter Rank based on Mean DC Power at Station 1:\\n')\nprint(json.dumps(dc_mean1, indent = 4))\nprint('\\nInverter Rank based on Mean AC Power at Station 1:\\n')\nprint(json.dumps(ac_mean1, indent = 4))\nprint('\\nInverter Rank based on Mean DC Power at Station 2:\\n')\nprint(json.dumps(dc_mean2, indent = 4))\nprint('\\nInverter Rank based on Mean AC Power at Station 2:\\n')\nprint(json.dumps(ac_mean2, indent = 4))","c5d00f64":"print('Number of entries: ' + str(len(df_pg1.index)) + '\\nExpected entries: ' + str(34*22*24*4))","acb9772a":"**Find the index where AC\\DC Power is maximum then use the index to locate the row and extract the SOURCE_KEY**","b05892fe":"### **Missing Data**\n#### **Difference indicates missing data**","cee203e4":"### **To rank inverters based on their performance:**\n\n* Create a Key-Value Data Structure\n* Inflate it with mean values of AC\/DC Power produced by each inverter with SOURCE_KEY as their id\n* Sort this DS according to these mean values in descending order but preserve the Key-Value pair","11d94445":"## *---End Of File---*","8af6e301":"### **Group DataFrame by date and then find Max\/Min AC\/DC Power for each Group**","9e736a00":"### **Import files from Input Dir and store them as Pandas DataFrame**","cd6a0a38":"### **Cast DATE_TIME strings to datetime objects**","aac2a851":"### **Explore the DataFrame**"}}