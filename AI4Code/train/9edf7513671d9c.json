{"cell_type":{"95e129d5":"code","9e311b7d":"code","f9386412":"code","b9089be8":"code","0d1f2867":"code","f8f6e284":"code","44a9d7ad":"code","85ea2edf":"code","e6a45896":"code","b7d6db20":"code","0a77e13e":"code","0dfb7065":"code","3f836454":"code","dd15bad4":"code","80f81a86":"code","40c29f1f":"markdown","c5bc9f4b":"markdown","f82b3904":"markdown","1fead442":"markdown","de2760c6":"markdown","b7823050":"markdown","aef26820":"markdown","ea6defda":"markdown","2b174a6a":"markdown","1eb10e4b":"markdown","12b3c268":"markdown","f40f750f":"markdown","ed5ab50b":"markdown","1ce82780":"markdown","ae655e95":"markdown","66d74f7e":"markdown"},"source":{"95e129d5":"# 1. Enable Internet in the Kernel (Settings side pane)\n\n# 2. Curl cache may need purged if v0.1.4 cannot be found (uncomment if needed). \n# !curl -X PURGE https:\/\/pypi.org\/simple\/kaggle-environments\n\n# ConnectX environment was defined in v0.1.4\n!pip install 'kaggle-environments>=0.1.4'","9e311b7d":"from kaggle_environments import evaluate, make\n\nenv = make(\"connectx\", debug=True)","f9386412":"env.agents","b9089be8":"env.configuration","0d1f2867":"env.specification","f8f6e284":"# This agent random chooses a non-empty column.\ndef my_agent(observation, configuration):\n    from random import choice\n    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])","44a9d7ad":"# Play as first position against random agent.\ntrainer = env.train([None, \"random\"])\n\nobservation = trainer.reset()\n\nprint(\"Observation contains:\\t\", observation)\nprint(\"Configuration contains:\\t\", env.configuration)\n\nmy_action = my_agent(observation, env.configuration)\nprint(\"My Action\", my_action)\nobservation, reward, done, info = trainer.step(my_action)\n# env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nenv.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nprint(\"Observation after:\\t\", observation)\n#env.render()","85ea2edf":"def my_comatose_agent(observation, configuration):\n    from random import choice\n    from time import sleep\n    sleep(2)\n    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n\ndef my_sleepy_agent(observation, configuration):\n    from random import choice\n    from time import sleep\n    sleep(1)\n    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])","e6a45896":"print(evaluate(\"connectx\", [my_comatose_agent, \"random\"], num_episodes=1))\nprint(evaluate(\"connectx\", [my_sleepy_agent, \"random\"], num_episodes=1))\nprint(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=1))","b7d6db20":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ sum(r[0] + r[1] for r in rewards)\n\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=3)))","0a77e13e":"import inspect\nimport os\n\nprint(inspect.getsource(env.agents['negamax']))","0dfb7065":"neg_v_neg = evaluate(\"connectx\", [env.agents['negamax'], \"negamax\"], num_episodes=10)\nprint(neg_v_neg)\nprint(mean_reward(neg_v_neg))","3f836454":"def try_not_to_loose_agent(observation, configuration):\n    from random import choice\n    from kaggle_environments import make\n    env = make(\"connectx\", debug=True)\n    trainer = env.train([None, \"negamax\"])\n    \n    cols = list(range(configuration.columns))\n    while cols:\n        # We set the state of the environment, so we can experiment on it.\n        env.state[0]['observation'] = observation\n        env.state[1]['observation'] = observation\n        # Take a random column that is not full\n        my_action = choice([c for c in cols if observation.board[c] == 0])\n        # Simulate the next step\n        out = env.train([None, \"negamax\"]).step(my_action)\n        # If the next step makes us lose, take a different step!\n        if out[2]:\n            cols.pop(my_action)\n        else:\n            return my_action\n    else:\n        # If we run out of steps to take, we just loose with one step.\n        return 1","dd15bad4":"stupid_v_random = evaluate(\"connectx\", [try_not_to_loose_agent, \"random\"], num_episodes=10)\nprint(stupid_v_random)\nprint(mean_reward(stupid_v_random))\n\nstupid_v_neg = evaluate(\"connectx\", [try_not_to_loose_agent, \"negamax\"], num_episodes=10)\nprint(stupid_v_neg)\nprint(mean_reward(stupid_v_neg))","80f81a86":"import inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(try_not_to_loose_agent, \"submission.py\")","40c29f1f":"# Taking The Hint\n\nIf you look in the starter notebook, you'll see that the evaluation is done against the following code:","c5bc9f4b":"## Good luck!\nHope you find something interesting to work with!","f82b3904":"There are some agents coming baked in, namely the `random` agent, that will create a baseline, if your final agent is doing better than flipping a coin. The `negamax` agent? We'll talk about that one below!","1fead442":"And inspect the first iteration:","de2760c6":"So clearly we can vary columns and rows, as well as the amount of tokens in a line to win. I guess this may be a nice test case for bigger games.\n\nBut there's also the amount of steps and a timeout variable. I'll venture a guess and say that your move is a maximum of 2 seconds.\n\nWith the specification commend, you'll be able to get a condensed dictionary that has most of the important information!","b7823050":"# Let's Try Something Stupid\nHow about, we try random choice, but just not take the step that will make us lose?\n\nAnd yes, this could be the first step toward implementing negamax. Considering, you have to simulate the games in a copy of the environment.","aef26820":"We get the configuration from before and a bit of fluff. However, there's also the reward with the following values:\n\n- Loss: 0\n- Draw: 0.5\n- Win: 1\n\nand a good description of what to do as a valid action. Choose a column to drop your token in.","ea6defda":"The important bit is the keyword `negamax`. It's a [special version of the minimax](https:\/\/en.wikipedia.org\/wiki\/Negamax) strategy, that optimizes based on the symmetry, that in this two player game you are always doing better when your opponent is doing worse. So essentially, the game state is always \"I'm at score `0.7` so my opponent is at `0.3` -- then you're at `0.4` and they're at `0.6`. Just a fact from a two player game with [perfect information](https:\/\/en.wikipedia.org\/wiki\/Perfect_information).\n\n![](https:\/\/i.imgur.com\/TBZXsYA.gif)\nCC-BY-SA 3.0 [Maschelos](https:\/\/en.wikipedia.org\/wiki\/File:Plain_Negamax.gif)\n\nIf you're interested in implementing your own Negamax strategy with all the optimizations, I find [this tutorial](http:\/\/blog.gamesolver.org\/solving-connect-four\/03-minmax\/) exceptional, despite being in C++. These optimizations probably include [Alpha-Beta Pruning](https:\/\/en.wikipedia.org\/wiki\/Alpha%E2%80%93beta_pruning) and [Transposition Tables](https:\/\/en.wikipedia.org\/wiki\/Transposition_table).\n\nPersonally, I found the [EasyAI](https:\/\/github.com\/Zulko\/easyAI) implementation pretty understandable and worth diving into. \n\nLet's have a look at how kaggle approaches the negamax problem:","2b174a6a":"That's odd...","1eb10e4b":"From this it is clear that the board contains a flattened 1D array of the $7 \\times 6$ board as described in the specification. Zero contains non-occupied spaces, our tokens represent a one and our opponent has value token 2.","12b3c268":"# Reinforcement Learning\nAs of writing this kernel, the challenge is limited to Standard Python, gym, numpy and scipy. So all the nice things in pytorch are not available yet. Once that is available, you can go on to some cool shenanigans essentially [mimicing AlphaZero](https:\/\/towardsdatascience.com\/from-scratch-implementation-of-alphazero-for-connect4-f73d4554002a)\n\n![](https:\/\/miro.medium.com\/max\/850\/1*4jBLXRsNVeOMBhOqO-8v8w.png)\n\nUntil then, we'll have to do something different. Let's try and do a bit better than random.","f40f750f":"So each move has to be sub two seconds to be valid. This has some serious implications considering that we're in Python and most of these game strategies rely on some sort of minimax game. That means, we have to somewhat traverse the game-state space to know how well we're doing and that is costly. So essentially, play all hypothetical games and then make the optimal choice on that calculation.","ed5ab50b":"# Investigate ConnectX Environment\nLet's investigate, what Kaggle will be doing with this environment:","1ce82780":"In this notebook I'd like to explore how the environment is created and how it is passed to the agent.\n\n![](https:\/\/i.imgur.com\/loGXjIN.png)\n\nAfter inspecting the environment, I give some pointers how to build a good agent with the current version of the kaggle Docker and some sources when Pytorch is available to play around with.\n\nThe environment itself give you a lot of info to work with, and we'll build a 'slightly better than random' agent.\n\nThis kernel is based on the [Getting Started](https:\/\/www.kaggle.com\/ajeffries\/connectx-getting-started) notebook.","ae655e95":"# Inspect inner Agent workings\n\nLet's create a little agent to inspect what objects we're even working with in this gym environment.","66d74f7e":"How well is negamax doing against itself?"}}