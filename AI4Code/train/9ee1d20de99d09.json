{"cell_type":{"5299b13c":"code","011dc966":"code","e9e3a5e9":"code","3676141e":"code","ace1b642":"code","d9bdba10":"code","c072d9d1":"code","de539e07":"code","1b487e2f":"code","3d6623f3":"code","b10be5a8":"code","f1d56e1d":"code","22d186f1":"code","45208ff2":"code","0a8caa24":"code","43176379":"code","5241d902":"code","11cbd53d":"code","7c7bc7ac":"code","7369d6c7":"code","96d9ffec":"code","2b579714":"code","1d4bd649":"code","c510eb9e":"code","da3ef948":"code","50f2098c":"code","8cc9a128":"code","d8e8da88":"code","5ace6df8":"code","5ae717ea":"code","a6d3931f":"code","896d33b5":"code","d1ff744b":"code","cfd04b17":"code","14ebd290":"code","4439c07f":"code","e039f886":"code","0844832d":"code","a9616e7f":"code","68cc2dc9":"code","9c99fc9c":"code","ac210793":"code","f2d33c00":"code","be827ac7":"code","2e904837":"code","5603b6a5":"code","608c505a":"code","2805fe03":"code","2fc2c52a":"code","b10f5999":"code","14df4084":"code","51699f4b":"code","45a8135c":"code","b6c07a8f":"code","0fd9f13a":"code","8d05a30c":"code","b39fb434":"code","76398bfd":"code","1773862f":"code","174e3cc4":"code","e606bfe0":"code","0cb2d249":"code","9ee3b689":"code","8db09032":"code","1b91c248":"code","3d6b8fa0":"code","fc33a8cb":"code","88681315":"code","122aea6d":"code","d3ffff03":"code","70179212":"code","31c78d5a":"code","174da575":"code","3cbc09e6":"code","1a1c0be8":"code","c20552ab":"code","aca53679":"code","8db37809":"code","c2c7fa75":"code","4e18a1f3":"code","f1934f2a":"code","d9fce62e":"code","748594bc":"code","3475550c":"code","fa313af3":"code","7f906ebe":"code","aaeb8637":"markdown","d5f371f5":"markdown","d4532322":"markdown","3010693c":"markdown","6888b2cd":"markdown","67053735":"markdown","762abe83":"markdown","6043b199":"markdown","8fcbe033":"markdown","ef339d0d":"markdown","613c3032":"markdown","f2b58753":"markdown","851dad74":"markdown","264785d8":"markdown","52acbeec":"markdown","605ce019":"markdown","b3a669ec":"markdown","8615a1d3":"markdown","ce529fc3":"markdown","583bed3c":"markdown"},"source":{"5299b13c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","011dc966":"# ! pip install https:\/\/github.com\/pandas-profiling\/pandas-profiling\/archive\/master.zip\n# !pip install tweet-preprocessor\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","e9e3a5e9":"import datetime, os, random, re, nltk, tokenization\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport tensorflow_hub as hub\nsns.set_style('darkgrid')\nfrom pandas_profiling import ProfileReport\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom transformers import TFRobertaForSequenceClassification, RobertaTokenizer\nfrom sklearn.metrics import precision_score, accuracy_score, recall_score, classification_report\nfrom keras.utils import to_categorical","3676141e":"train_full = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv')","ace1b642":"train_full.head()","d9bdba10":"test.head()","c072d9d1":"print(f'Development data: {train_full.shape}')\nprint(f'Unseen data: {test.shape}')","de539e07":"# profile = ProfileReport(train_full, title = 'Corona Tweets Report', explorative = True)\n# display(profile)","1b487e2f":"train, valid = train_test_split(train_full, test_size = 0.2, random_state = 100)","3d6623f3":"train.head()","b10be5a8":"valid.head()","f1d56e1d":"print(f'Training data: {train.shape}')\nprint(f'Validation data: {valid.shape}')","22d186f1":"df = train.copy()","45208ff2":"df.head()","0a8caa24":"df.shape","43176379":"df['Location'].nunique()","5241d902":"df['Location'].value_counts(dropna = False)[:20]","11cbd53d":"df['Location'] = df['Location'].str.split(',').str[0]\ndf.loc[df['Location'] == 'UK' ,'Location'] = 'United Kingdom'\ndf.loc[df['Location'] == 'USA', 'Location'] = 'United States'\ndf.loc[df['Location'] == 'US', 'Location'] = 'United States'\ndf.loc[df['Location'] == 'The United States of America', 'Location'] = 'United States'\ndf.loc[df['Location'] == 'United States of America', 'Location'] = 'United States'\ndf.loc[df['Location'] == 'America', 'Location'] = 'United States'\ndf.loc[df['Location'] == 'United States ', 'Location'] = 'United States'","7c7bc7ac":"df['Location'].nunique()","7369d6c7":"df['Location'].value_counts(dropna = False)[:20]","96d9ffec":"df2 = df.copy()","2b579714":"print(f'Dataframe with missing values: {df.shape}')\nprint(f'Dataframe without missing values: {df2.shape}')","1d4bd649":"df2.dropna(axis = 0, inplace = True)","c510eb9e":"df2.head()","da3ef948":"print(f'Expected rows after dropping: {df.shape[0] - 6910}')\nprint(f'True rows after dropping: {df2.shape[0]}')","50f2098c":"df2['Location'].value_counts()[:50]","8cc9a128":"city = ['London', 'New York', 'Washington', 'Los Angeles', 'Toronto', \n        'Chicago', 'Sydney', 'San Francisco', 'Melbourne', 'New Delhi']\n\ncountry = ['United States', 'United Kingdom', 'England', 'India', 'Australia', \n           'Canada', 'Scotland', 'Singapore', 'South Africa']\n\nstates = ['Texas', 'Florida', 'California', 'New Jersey']","d8e8da88":"color = ['#F2B138', '#29AB87', '#C21807', '#0B6623', '#7C0A02']\n\nplt.figure(figsize = (20, 8))\nplt.title('Top 10 Cities that has the most tweets', size = 20)\ncplot = sns.countplot(x = 'Location', hue = 'Sentiment', data = df2, order = city, palette = color)\n\nfor p in cplot.patches:\n    cplot.annotate(format(p.get_height(), '.0f'), \n                   (p.get_x() + p.get_width() \/ 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \nplt.xlabel('City', fontsize = 18)\nplt.ylabel('Count', fontsize = 18)\nplt.xticks(fontsize = 14)\nplt.legend(prop = {'size': 13})\nplt.show()","5ace6df8":"color = ['#F2B138', '#29AB87', '#C21807', '#0B6623', '#7C0A02']\n\nplt.figure(figsize = (20, 8))\nplt.title('Top 10 Countries that has the most tweets', size = 20)\ncplot = sns.countplot(x = 'Location', hue = 'Sentiment', data = df2, order = country, palette = color)\n\nfor p in cplot.patches:\n    cplot.annotate(format(p.get_height(), '.0f'), \n                   (p.get_x() + p.get_width() \/ 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \nplt.xlabel('Country', fontsize = 18)\nplt.ylabel('Count', fontsize = 18)\nplt.xticks(fontsize = 14)\nplt.legend(prop = {'size': 13})\nplt.show()","5ae717ea":"color = ['#F2B138', '#29AB87', '#C21807', '#0B6623', '#7C0A02']\n\nplt.figure(figsize = (20, 8))\nplt.title('Top 4 States that has the most tweets', size = 20)\ncplot = sns.countplot(x = 'Location', hue = 'Sentiment', data = df2, order = states, palette = color)\n\nfor p in cplot.patches:\n    cplot.annotate(format(p.get_height(), '.0f'), \n                   (p.get_x() + p.get_width() \/ 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \nplt.xlabel('Country', fontsize = 18)\nplt.ylabel('Count', fontsize = 18)\nplt.xticks(fontsize = 14)\nplt.legend(prop = {'size': 13})\nplt.show()","a6d3931f":"def encoded_cat(df):\n    df['Labels'] = df['Sentiment'].astype('category').cat.codes\n    return df","896d33b5":"train_full = encoded_cat(train_full)\ntrain = encoded_cat(train)\nvalid = encoded_cat(valid)\ntest = encoded_cat(test)","d1ff744b":"train_full['Labels'].value_counts()","cfd04b17":"train['Labels'].value_counts()","14ebd290":"valid['Labels'].value_counts()","4439c07f":"test['Labels'].value_counts()","e039f886":"X, y = train_full['OriginalTweet'].to_list(), train_full['Labels'].to_list()\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 100)","0844832d":"X_test, y_test = test['OriginalTweet'].to_list(), test['Labels'].to_list()","a9616e7f":"def convert_to_dfX(df):\n    df = pd.DataFrame(df, columns = ['OriginalTweet'])\n    return df\n\ndef convert_to_dfy(df):\n    df = pd.DataFrame(df, columns = ['Labels'])\n    return df","68cc2dc9":"df_X_train = convert_to_dfX(X_train)\ndf_X_valid = convert_to_dfX(X_valid)\ndf_X_test = convert_to_dfX(X_test)","9c99fc9c":"df_X_train.head()","ac210793":"df_X_valid.head()","f2d33c00":"df_X_test.head()","be827ac7":"df_y_train = convert_to_dfy(y_train)\ndf_y_valid = convert_to_dfy(y_valid)\ndf_y_test = convert_to_dfy(y_test)","2e904837":"df_y_train.head()","5603b6a5":"df_y_valid.head()","608c505a":"df_y_test.head()","2805fe03":"print('Training Data')\nprint(X_train[:15])\nprint(y_train[:15])\nprint('\\nValidation Data')\nprint(X_valid[:15])\nprint(y_valid[:15])\nprint('\\nUnseen Data')\nprint(X_test[:15])\nprint(y_test[:15])","2fc2c52a":"stopword = nltk.corpus.stopwords.words(\"english\")","b10f5999":"def clean(text):\n    #     remove urls\n    text = re.sub(r'http\\S+', \" \", text)\n    #     remove mentions\n    text = re.sub(r'@\\w+',' ',text)\n    #     remove hastags\n    text = re.sub(r'#\\w+', ' ', text)\n    #     remove digits\n    text = re.sub(r'\\d+', ' ', text)\n    #     remove html tags\n    text = re.sub('r<.*?>',' ', text)\n    # Removes symbols\n    text = re.sub(r'&[A-Za-z0-9]+', ' ', text)\n    \n    # Removes uniques characters\n    text = re.sub(r'[^a-zA-Z ]',' ', text)\n    \n    # Remove all extra spaces\n    text = re.sub(r'( +)',' ', text)\n    text = text.strip()\n    \n    # Changes characters to lowercase\n    text = text.lower()\n    \n    # remove stop words \n    text = text.split()\n    text = \" \".join([word for word in text if not word in stopword])\n    \n    return text","14df4084":"X_train_copy = df_X_train.copy()\nX_valid_copy = df_X_valid.copy()\nX_test_copy = df_X_test.copy()","51699f4b":"X_train_copy.head()","45a8135c":"new_clean_X_train = X_train_copy['OriginalTweet'].apply(lambda x: clean(x)).to_list()\nnew_clean_X_valid = X_valid_copy['OriginalTweet'].apply(lambda x: clean(x)).to_list()\nnew_clean_X_test = X_test_copy['OriginalTweet'].apply(lambda x: clean(x)).to_list()","b6c07a8f":"print('Training Data')\nprint(new_clean_X_train[16:20])\nprint(y_train[:15])\nprint('\\nValidation Data')\nprint(new_clean_X_valid[16:20])\nprint(y_valid[:15])\nprint('\\nUnseen Data')\nprint(new_clean_X_test[16:20])\nprint(y_test[10:15])","0fd9f13a":"text_clf = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', MultinomialNB()),\n])\n\ntext_clf.fit(new_clean_X_train, y_train)","8d05a30c":"y_pred = text_clf.predict(new_clean_X_test)\nprint(classification_report(y_test, y_pred))","b39fb434":"print(f\"Accuracy Score: {accuracy_score(y_test, y_pred)}\")","76398bfd":"MODEL_NAME = 'roberta-base'\n\ntokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)","1773862f":"new_clean_X_train[:5]","174e3cc4":"label = preprocessing.LabelEncoder()\ny_train_categorical = label.fit_transform(train['Sentiment'])\ny_train_categorical = to_categorical(y_train_categorical)\nprint(y_train_categorical[:5])","e606bfe0":"y_valid_categorical = label.fit_transform(valid['Sentiment'])\ny_valid_categorical = to_categorical(y_valid_categorical)\nprint(y_valid_categorical[:5])","0cb2d249":"m_url = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2'\nbert_layer = hub.KerasLayer(m_url, trainable = True)","9ee3b689":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","8db09032":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len-len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n        \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","1b91c248":"max_len = 250\n\nX_train_full_encoded = bert_encode(new_clean_X_train, tokenizer, max_len)\nX_train_encoded = bert_encode(new_clean_X_train, tokenizer, max_len)\nX_valid_encoded = bert_encode(new_clean_X_valid, tokenizer, max_len)\nX_test_encoded = bert_encode(new_clean_X_test, tokenizer, max_len)","3d6b8fa0":"def build_model(bert_layer, max_len=512):\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    \n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    \n    clf_output = sequence_output[:, 0, :]\n    \n    lay = tf.keras.layers.Dense(32, activation='relu')(clf_output)\n    lay = tf.keras.layers.Dropout(0.2)(lay)\n    lay = tf.keras.layers.Dense(16, activation='relu')(lay)\n    lay = tf.keras.layers.Dropout(0.2)(lay)\n    out = tf.keras.layers.Dense(5, activation='softmax')(lay)\n    \n    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(tf.keras.optimizers.Adam(lr=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","fc33a8cb":"model = build_model(bert_layer, max_len=max_len)\nmodel.summary()","88681315":"checkpoint = tf.keras.callbacks.ModelCheckpoint('my_model.h5', save_best_only = True)\nearlystopping = tf.keras.callbacks.EarlyStopping(patience = 10)\n\nhistory = model.fit(\n    X_train_encoded, y_train_categorical,\n    validation_data = (X_valid_encoded, y_valid_categorical),\n    epochs = 20,\n    callbacks = [checkpoint, earlystopping],\n    batch_size = 16)","122aea6d":"# def convert_sentence_to_features(dataset):\n#     return tokenizer(\n#         dataset,\n#         add_special_tokens = True,\n#         return_attention_mask = True, # roberta doesn't need attention mask\n#         truncation = True,\n#         padding = True)","d3ffff03":"# X_train_encoded = convert_sentence_to_features(new_clean_X_train)\n# X_valid_encoded = convert_sentence_to_features(new_clean_X_valid)\n# X_test_encoded = convert_sentence_to_features(new_clean_X_test)","70179212":"# X_train_encoded.keys()","31c78d5a":"# len(X_train_encoded['input_ids'])","174da575":"# len(y_train)","3cbc09e6":"# def build_model(bert_layer, max_len=512):\n#     input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n#     input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n#     segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n#     _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n#     clf_output = sequence_output[:, 0, :]\n#     out = Dense(1, activation='sigmoid')(clf_output)\n    \n#     model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n#     model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n#     return model","1a1c0be8":"# def tensor_slices(X, y):\n#     return tf.data.Dataset.from_tensor_slices((dict(X), y))","c20552ab":"# batch_size = 32\n# shuffle_buffer_size = 17000\n\n# train_encoded = tensor_slices(X_train_encoded, y_train).shuffle(shuffle_buffer_size).batch(batch_size)\n# valid_encoded = tensor_slices(X_valid_encoded, y_valid).batch(batch_size)\n# test_encoded = tensor_slices(X_test_encoded, y_test).batch(batch_size)","aca53679":"for message, label in train_encoded.take(1):\n    print(message, label)","8db37809":"for message, label in valid_encoded.take(1):\n    print(message, label)","c2c7fa75":"for message, label in test_encoded.take(1):\n    print(message, label)","4e18a1f3":"# def build_roberta_model(learning_rate = 1e-5):\n#     roberta_model = TFRobertaForSequenceClassification.from_pretrained(MODEL_NAME, \n#                                                                        num_labels = 5, \n#                                                                        num_hidden_layers = 10)\n    \n#     optimizer = keras.optimizers.Adam(learning_rate = learning_rate, epsilon = 1e-8)\n#     roberta_model.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n#     return roberta_model","f1934f2a":"# roberta_model = build_roberta_model()\n# roberta_model.summary()","d9fce62e":"%load_ext tensorboard\n\nlogdir = os.path.join('logs', 'my_baseline_model')\ntensorboard_cb = tf.keras.callbacks.TensorBoard(logdir, histogram_freq = 1)\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\"my_baseline_model.h5\", save_best_only = True)\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience = 10)","748594bc":"# history = roberta_model.fit(train_encoded, epochs = 25,\n#                                   validation_data = valid_encoded,\n#                                   callbacks = [checkpoint_cb, early_stopping_cb, tensorboard_cb])","3475550c":"# %tensorboard --logdir logs","fa313af3":"# y_pred = text_clf.predict(X_test)\n# print(classification_report(y_test, y_pred))","7f906ebe":"# print(f\"Accuracy Score: {accuracy_score(y_test, y_pred)}\")\n# print(f\"Precision Score: {precision_score(y_test, y_pred)}\")\n# print(f\"Recall Score: {recall_score(y_test, y_pred)}\")","aaeb8637":"To prevent the train data changed while EDA, I will pass it into a new dataframe.","d5f371f5":"# Data Preprocessing","d4532322":"# Sentiment","3010693c":"# **Exploratory Data Analysis**\n\nSince it's EDA, I'll only use train data. First, I'm going to split train full data into 80% train data and 20% validation data.","6888b2cd":"As we could see, the location contains missing values. If I drop the rows that contains missing data, it maybe could affect our EDA so let's pass it into new dataframe rather than take a risk by dropping it!","67053735":"# Modelling","762abe83":"We could see that the location contains countries or cities. So let's plot top 10 countries and top 10 cities in different plots.","6043b199":"# Installing","8fcbe033":"# **Library**","ef339d0d":"## Location\n\nSince Location has high cardinality, let's check the unique values.","613c3032":"# **Data Understanding**","f2b58753":"We can conclude that top 10 cities tend to have more positive sentiment especially San Francisco that has extremely positive sentiment. Only New Delhi that has more neutral sentiment.","851dad74":"It's dropped well since both dataframes have the same shape.","264785d8":"We could see the values are inconsistent. Let's fix this!","52acbeec":"Train data has 41157 rows and 6 columns.\n\nColumns explanation:\n- UserName: Index of the user\n- ScreenName: Index of the user's screen\n- Location: country of the user\n- TweetAt: Date of the tweets\n- OriginialTweet: Contents of the tweets\n- Sentiment: Sentiment of the user\n\nUnderstanding about the data:\n- Has two numerical and four categorical\n- UserName and ScreenName have unique values.\n- Location has high cardinality and missing values.\n- TweetAt is highly correlated with UserName and ScreenName\n- Sentiment has only 5 unique values which is Positive, Negative, Neutral, Extremely Positive, and Extremely Negative","605ce019":"It's copied well since both dataframes have the same shape.","b3a669ec":"We can conclude that top 9 countries tend to have more positive sentiment. Only India that has more negative sentiment. Besides that, only Canada that has more neutral sentiment.","8615a1d3":"## UserName and ScreenName Columns\n\nAt data understanding, I've mentioned that UserName and ScreenName columns has unique values. If I'm going to plot these, I won't get any insights too. I think it's better to ignore these columns. Also, at data preprocessing, it's better to drop these columns since they're not useful for data analysis.","ce529fc3":"# Baseline Model","583bed3c":"# **Load Data**"}}