{"cell_type":{"2861f6f4":"code","108d5cad":"code","3e17f238":"code","ebbd0224":"code","2d658c67":"code","a7e8db7a":"code","efbbc797":"code","5f341694":"code","bc2f4854":"code","44227261":"code","30604081":"code","38d0ce9d":"code","49ae7092":"code","42378f3e":"code","660e7ada":"code","5d158374":"code","3140b3e7":"code","5b92a75c":"code","762ad1d9":"code","2c14a4d2":"code","7b9bd980":"code","6e10d4bd":"code","5f7b7e63":"code","ec8271b6":"code","e1286019":"code","18cd0a10":"code","16880f3c":"code","aec5c781":"code","0824cfda":"code","486e4872":"code","94c0b020":"code","2a0de3ee":"code","a12f29ee":"code","62a05a7f":"code","91d45653":"code","6e78db29":"code","68414b7d":"code","fde18891":"code","2f70fe00":"code","5a697fc8":"code","831ef029":"markdown","9925b48e":"markdown","05e02313":"markdown","51e79e53":"markdown","f09b59b1":"markdown","d6ac9683":"markdown","a77003e6":"markdown","32c80f98":"markdown"},"source":{"2861f6f4":"from copy import deepcopy\nimport json\nimport random\nimport time\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport torch\nimport tqdm\nfrom torch.utils import data\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nfrom torch.nn import functional as fnn\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nnp.random.seed(2205)\ntorch.manual_seed(2205)","108d5cad":"class CarPlatesDatasetWithRectangularBoxes(data.Dataset):\n    def __init__(self, root, transforms, split='train', train_size=0.9):\n        super(CarPlatesDatasetWithRectangularBoxes, self).__init__()\n        self.root = Path(root)\n        self.train_size = train_size\n        \n        self.image_names = []\n        self.image_ids = []\n        self.image_boxes = []\n        self.image_texts = []\n        self.box_areas = []\n        \n        self.transforms = transforms\n        \n        if split in ['train', 'val']:\n            plates_filename = self.root \/ 'train.json'\n            with open(plates_filename) as f:\n                json_data = json.load(f)\n            train_valid_border = int(len(json_data) * train_size) + 1 # \u0433\u0440\u0430\u043d\u0438\u0446\u0430 \u043c\u0435\u0436\u0434\u0443 train \u0438 valid\n            data_range = (0, train_valid_border) if split == 'train' \\\n                else (train_valid_border, len(json_data))\n            self.load_data(json_data[data_range[0]:data_range[1]]) # \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u0444\u0430\u0439\u043b\u043e\u0432 \u0438 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443\n            return\n\n        if split == 'test':\n            plates_filename = self.root \/ 'submission.csv'\n            self.load_test_data(plates_filename, split, train_size)\n            return\n\n        raise NotImplemented(f'Unknown split: {split}')\n        \n    def load_data(self, json_data):\n        for i, sample in enumerate(json_data):\n            if sample['file'] == 'train\/25632.bmp':\n                continue\n            self.image_names.append(self.root \/ sample['file'])\n            self.image_ids.append(torch.Tensor([i]))\n            boxes = []\n            texts = []\n            areas = []\n            for box in sample['nums']:\n                points = np.array(box['box'])\n                x_0 = np.min([points[0][0], points[3][0]])\n                y_0 = np.min([points[0][1], points[1][1]])\n                x_1 = np.max([points[1][0], points[2][0]])\n                y_1 = np.max([points[2][1], points[3][1]])\n                boxes.append([x_0, y_0, x_1, y_1])\n                texts.append(box['text'])\n                areas.append(np.abs(x_0 - x_1) * np.abs(y_0 - y_1))\n            boxes = torch.FloatTensor(boxes)\n            areas = torch.FloatTensor(areas)\n            self.image_boxes.append(boxes)\n            self.image_texts.append(texts)\n            self.box_areas.append(areas)\n        \n    \n    def load_test_data(self, plates_filename, split, train_size):\n        df = pd.read_csv(plates_filename, usecols=['file_name'])\n        for row in df.iterrows():\n            self.image_names.append(self.root \/ row[1][0])\n        self.image_boxes = None\n        self.image_texts = None\n        self.box_areas = None\n         \n    \n    def __getitem__(self, idx):\n        target = {}\n        if self.image_boxes is not None:\n            boxes = self.image_boxes[idx].clone()\n            areas = self.box_areas[idx].clone()\n            num_boxes = boxes.shape[0]\n            target['boxes'] = boxes\n            target['area'] = areas\n            target['labels'] = torch.LongTensor([1] * num_boxes)\n            target['image_id'] = self.image_ids[idx].clone()\n            target['iscrowd'] = torch.Tensor([False] * num_boxes)\n#             target['texts'] = self.image_texts[idx]\n\n        image = cv2.imread(str(self.image_names[idx]))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transforms is not None:\n            image = self.transforms(image)\n        return image, target\n\n    def __len__(self):\n        return len(self.image_names)","3e17f238":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\ndef create_model(device):\n    # load a model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    # replace the classifier with a new one, that has\n    # num_classes which is user-defined\n    num_classes = 2  # 1 class (person) + background\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model.to(device)\n\n# \u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f dataloader'\u0430\ndef collate_fn(batch):\n    return tuple(zip(*batch))","ebbd0224":"transformations= transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n                    ])","2d658c67":"# train on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nmodel = create_model(device)\n\n# use our dataset and defined transformations\ntrain_dataset = CarPlatesDatasetWithRectangularBoxes('data', transformations, 'train')\nval_dataset = CarPlatesDatasetWithRectangularBoxes('data', transformations, 'val')\ntest_dataset = CarPlatesDatasetWithRectangularBoxes('data', transformations, 'test')\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=2, shuffle=True, num_workers=4,\n    collate_fn=collate_fn)\n\nval_loader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=2, shuffle=False, num_workers=4,\n    collate_fn=collate_fn)","a7e8db7a":"# \u0427\u0430\u0441\u0442\u044c \u043a\u043e\u0434\u0430 \u0432\u0437\u044f\u0442\u0430 \u0438\u0437  pytorch utils\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)\n\nnum_epochs = 1\n\nfor epoch in range(num_epochs):\n    model.train()\n\n    for images, targets in tqdm.tqdm(train_loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n    \n    batch_losses = []\n    for images, targets in tqdm.tqdm(val_loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        batch_losses.append(losses.item())\n        optimizer.zero_grad()\n    \n    batch_losses = np.array(batch_losses)\n    batch_losses = batch_losses[np.isfinite(batch_losses)]\n    print(f'Valid_loss: {np.mean(batch_losses)}')\n    lr_scheduler.step()\n\nprint(\"That's it!\")","efbbc797":"print(f'Validation average loss: {np.mean(batch_losses)}')","5f341694":"# Save\n# with open('fasterrcnn_resnet50_fpn_1_epoch', 'wb') as fp:\n#     torch.save(model.state_dict(), fp)","bc2f4854":"# Load\nwith open('fasterrcnn_resnet50_fpn_1_epoch', 'rb') as fp:\n    state_dict = torch.load(fp, map_location=\"cpu\")\nmodel.load_state_dict(state_dict)\nmodel.to(device)","44227261":"unnormalize_1 = transforms.Normalize(mean=[-0.485, -0.456, -0.406],\n                                         std=[1, 1, 1])\nunnormalize_2 = transforms.Normalize(mean=[0, 0, 0],\n                                         std=[1\/0.229, 1\/0.224, 1\/0.225])\nunnormalize = transforms.Compose([unnormalize_2, unnormalize_1])\n\nstart = 2\n\nimages = []\nfor i in range(start, start + 2):\n    images.append(val_dataset[i][0].to(device))","30604081":"def detach_dict(pred):\n    return{k:v.detach().cpu() for (k,v) in pred.items()}\n\nmodel.eval()\npreds = model(images)\npreds = [detach_dict(pred) for pred in preds]","38d0ce9d":"preds","49ae7092":"fig,ax = plt.subplots(1, 2, figsize = (20, 8))\n\nfor i in range(2):\n    image = unnormalize(images[i].clone().cpu())\n    ax[i].imshow(image.numpy().transpose([1,2,0]))\n    for box in preds[i]['boxes']:\n        box = box.detach().cpu().numpy()\n        rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=1,edgecolor='r',facecolor='none')\n        ax[i].add_patch(rect)\n\nplt.show()","42378f3e":"class CarPlatesFragmentsDataset(data.Dataset):\n    def __init__(self, root, transforms, split='train', train_size=0.9, alphabet=abc):\n        super(CarPlatesFragmentsDataset, self).__init__()\n        self.root = Path(root)\n        self.alphabet = alphabet\n        self.train_size = train_size\n        \n        self.image_names = []\n        self.image_ids = []\n        self.image_boxes = []\n        self.image_texts = []\n        self.box_areas = []\n        \n        self.transforms = transforms\n        \n        if split in ['train', 'val']:\n            plates_filename = self.root \/ 'train.json'\n            with open(plates_filename) as f:\n                json_data = json.load(f)\n            train_valid_border = int(len(json_data) * train_size) + 1 # \u0433\u0440\u0430\u043d\u0438\u0446\u0430 \u043c\u0435\u0436\u0434\u0443 train \u0438 valid\n            data_range = (0, train_valid_border) if split == 'train' \\\n                else (train_valid_border, len(json_data))\n            self.load_data(json_data[data_range[0]:data_range[1]]) # \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u0444\u0430\u0439\u043b\u043e\u0432 \u0438 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443\n            return\n\n        if split == 'test':\n            plates_filename = self.root \/ 'test_boxes.json'\n            with open(plates_filename) as f:\n                json_data = json.load(f)\n            self.load_test_data(json_data)\n            return\n            \n        raise NotImplemented(f'Unknown split: {split}')\n        \n    def load_data(self, json_data):\n        for i, sample in enumerate(json_data):\n            if sample['file'] == 'train\/25632.bmp':\n                continue\n            for box in sample['nums']:\n                points = np.array(box['box'])\n                x_0 = np.min([points[0][0], points[3][0]])\n                y_0 = np.min([points[0][1], points[1][1]])\n                x_1 = np.max([points[1][0], points[2][0]])\n                y_1 = np.max([points[2][1], points[3][1]])\n                if x_0 >= x_1 or y_0 >= y_1:\n                    # \u0415\u0441\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432, \u043a\u043e\u0433\u0434\u0430 \u0442\u043e\u0447\u043a\u0438 \u043f\u0440\u043e\u043d\u0443\u043c\u0435\u0440\u043e\u0432\u0430\u043d\u044b \u0432 \u0434\u0440\u0443\u0433\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435 - \u043f\u043e\u043a\u0430 \u043d\u0435 \u0432\u044b\u044f\u0441\u043d\u044f\u0435\u043c\n                    continue\n                if (y_1 - y_0) * 20 < (x_1 - x_0):\n                    continue\n                self.image_boxes.append(np.clip([x_0, y_0, x_1, y_1], a_min=0, a_max=None))\n                self.image_texts.append(box['text'])\n                self.image_names.append(sample['file'])\n        self.revise_texts()\n                \n    def revise_texts(self):\n        wrong = '\u0410\u041e\u041d\u041a\u0421\u0420\u0412\u0425\u0415\u0422\u041c\u0423'\n        correct = 'AOHKCPBXETMY'\n        for i in range(len(self.image_texts)):\n            self.image_texts[i] = self.image_texts[i].upper()\n            for (a, b) in zip(wrong, correct):\n                self.image_texts[i] = self.image_texts[i].replace(a, b)\n            \n                \n    def load_test_data(self, json_data):\n        for i, sample in enumerate(json_data):\n            for box in sample['boxes']:\n                if box[0] >= box[2] or box[1] >= box[3]:\n                    continue\n                points = np.array(box)\n                self.image_boxes.append(np.clip(points, a_min=0, a_max=None))\n                self.image_names.append(sample['file'])\n        self.image_texts = None\n    \n    def __getitem__(self, idx):\n        file_name = self.root \/ self.image_names[idx]\n        image = cv2.imread(str(file_name))\n        if image is None:\n            file_name = self.image_names[idx]\n            image = cv2.imread(str(file_name))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        text = ''\n        \n        if self.image_boxes is not None:\n            box = self.image_boxes[idx]\n            image = image.copy()[box[1]:box[3], box[0]:box[2]]\n            \n        if self.image_texts is not None:\n            text = self.image_texts[idx]\n            \n        seq = self.text_to_seq(text)\n        seq_len = len(seq)\n\n        output = dict(image=image, seq=seq, seq_len=seq_len, text=text, file_name=file_name)\n        \n        if self.transforms is not None:\n            output = self.transforms(output)\n        \n        return output\n    \n    def text_to_seq(self, text):\n        \"\"\"Encode text to sequence of integers.\n        Accepts string of text.\n        Returns list of integers where each number is index of corresponding characted in alphabet + 1.\n        \"\"\"\n        # YOUR CODE HERE\n        seq = [self.alphabet.find(c) + 1 for c in text]\n        return seq\n\n    def __len__(self):\n        return len(self.image_names)","660e7ada":"class FeatureExtractor(nn.Module):\n    \n    def __init__(self, input_size=(64, 320), output_len=20):\n        super(FeatureExtractor, self).__init__()\n        \n        h, w = input_size\n        resnet = getattr(models, 'resnet18')(pretrained=True)\n        self.cnn = nn.Sequential(*list(resnet.children())[:-2])\n        \n        self.pool = nn.AvgPool2d(kernel_size=(h \/\/ 32, 1))        \n        self.proj = nn.Conv2d(w \/\/ 32, output_len, kernel_size=1)\n  \n        self.num_output_features = self.cnn[-1][-1].bn2.num_features    \n    \n    def apply_projection(self, x):\n        \"\"\"Use convolution to increase width of a features.\n        Accepts tensor of features (shaped B x C x H x W).\n        Returns new tensor of features (shaped B x C x H x W').\n        \"\"\"\n        # YOUR CODE HERE\n        x = x.permute(0, 3, 2, 1).contiguous()\n        x = self.proj(x)\n        x = x.permute(0, 2, 3, 1).contiguous()\n        return x\n   \n    def forward(self, x):\n        # Apply conv layers\n        features = self.cnn(x)\n        \n        # Pool to make height == 1\n        features = self.pool(features)\n        \n        # Apply projection to increase width\n        features = self.apply_projection(features)\n        \n        return features","5d158374":"class SequencePredictor(nn.Module):\n    \n    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=False):\n        super(SequencePredictor, self).__init__()\n        \n        self.num_classes = num_classes        \n        self.rnn = nn.GRU(input_size=input_size,\n                          hidden_size=hidden_size,\n                          num_layers=num_layers,\n                          dropout=dropout,\n                          bidirectional=bidirectional)\n        \n        fc_in = hidden_size if not bidirectional else 2 * hidden_size\n        self.fc = nn.Linear(in_features=fc_in,\n                            out_features=num_classes)\n    \n    def _init_hidden_(self, batch_size):\n        \"\"\"Initialize new tensor of zeroes for RNN hidden state.\n        Accepts batch size.\n        Returns tensor of zeros shaped (num_layers * num_directions, batch, hidden_size).\n        \"\"\"\n        # YOUR CODE HERE\n        num_directions = 2 if self.rnn.bidirectional else 1\n        return torch.zeros(self.rnn.num_layers * num_directions, batch_size, self.rnn.hidden_size)\n        \n    def _prepare_features_(self, x):\n        \"\"\"Change dimensions of x to fit RNN expected input.\n        Accepts tensor x shaped (B x (C=1) x H x W).\n        Returns new tensor shaped (W x B x H).\n        \"\"\"\n        # YOUR CODE HERE\n        x = x.squeeze(1)\n        x = x.permute(2, 0, 1)\n        return x\n    \n    def forward(self, x):\n        x = self._prepare_features_(x)\n        \n        batch_size = x.size(1)\n        h_0 = self._init_hidden_(batch_size)\n        h_0 = h_0.to(x.device)\n        x, h = self.rnn(x, h_0)\n        \n        x = self.fc(x)\n        return x","3140b3e7":"abc = \"0123456789ABEKMHOPCTYX\"  # this is our alphabet for predictions.\n\nclass CRNN(nn.Module):\n    \n    def __init__(self, alphabet=abc,\n                 cnn_input_size=(64, 320), cnn_output_len=20,\n                 rnn_hidden_size=128, rnn_num_layers=2, rnn_dropout=0.3, rnn_bidirectional=False):\n        super(CRNN, self).__init__()\n        self.alphabet = alphabet\n        self.features_extractor = FeatureExtractor(input_size=cnn_input_size, output_len=cnn_output_len)\n        self.sequence_predictor = SequencePredictor(input_size=self.features_extractor.num_output_features,\n                                                    hidden_size=rnn_hidden_size, num_layers=rnn_num_layers,\n                                                    num_classes=len(alphabet)+1, dropout=rnn_dropout,\n                                                    bidirectional=rnn_bidirectional)\n    \n    def forward(self, x):\n        features = self.features_extractor(x)\n        sequence = self.sequence_predictor(features)\n        return sequence","5b92a75c":"def pred_to_string(pred, abc):\n    seq = []\n    for i in range(len(pred)):\n        label = np.argmax(pred[i])\n        seq.append(label - 1)\n    out = []\n    for i in range(len(seq)):\n        if len(out) == 0:\n            if seq[i] != -1:\n                out.append(seq[i])\n        else:\n            if seq[i] != -1 and seq[i] != seq[i - 1]:\n                out.append(seq[i])\n    out = ''.join([abc[c] for c in out])\n    return out\n\ndef decode(pred, abc):\n    pred = pred.permute(1, 0, 2).cpu().data.numpy()\n    outputs = []\n    for i in range(len(pred)):\n        outputs.append(pred_to_string(pred[i], abc))\n    return outputs","762ad1d9":"class Resize(object):\n\n    def __init__(self, size=(320, 64)):\n        self.size = size\n\n    def __call__(self, item):\n        \"\"\"Accepts item with keys \"image\", \"seq\", \"seq_len\", \"text\".\n        Returns item with image resized to self.size.\n        \"\"\"\n        # YOUR CODE HERE\n        item['image'] = cv2.resize(item['image'], self.size, interpolation=cv2.INTER_AREA)\n        return item","2c14a4d2":"crnn = CRNN()","7b9bd980":"crnn.to(device)\nnum_epochs = 10\nbatch_size = 128\nnum_workers = 4\noptimizer = torch.optim.Adam(crnn.parameters(), lr=3e-4, amsgrad=True, weight_decay=1e-4)","6e10d4bd":"transformations = transforms.Compose([\n    Resize(),\n                    ])\n\ntrain_plates_dataset = CarPlatesFragmentsDataset('data', transformations, 'train')\nval_plates_dataset = CarPlatesFragmentsDataset('data', transformations, 'val')","5f7b7e63":"def collate_fn(batch):\n    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n    Accepts list of dataset __get_item__ return values (dicts).\n    Returns dict with same keys but values are either torch.Tensors of batched images, sequences, and so.\n    \"\"\"\n    images, seqs, seq_lens, texts, file_names = [], [], [], [], []\n    for sample in batch:\n        images.append(torch.from_numpy(sample[\"image\"]).permute(2, 0, 1).float())\n        seqs.extend(sample[\"seq\"])\n        seq_lens.append(sample[\"seq_len\"])\n        texts.append(sample[\"text\"])\n        file_names.append(sample[\"file_name\"])\n    images = torch.stack(images)\n    seqs = torch.Tensor(seqs).int()\n    seq_lens = torch.Tensor(seq_lens).int()\n    \n    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts, \"file_name\": file_names}\n    return batch","ec8271b6":"train_dataloader = torch.utils.data.DataLoader(train_plates_dataset, \n                                               batch_size=batch_size, shuffle=True,\n                                               num_workers=num_workers, pin_memory=True, \n                                               drop_last=True, collate_fn=collate_fn)\nval_dataloader = torch.utils.data.DataLoader(val_plates_dataset, \n                                             batch_size=batch_size, shuffle=False,\n                                             num_workers=num_workers, pin_memory=True, \n                                             drop_last=True, collate_fn=collate_fn)","e1286019":"crnn.train()\nfor i, epoch in enumerate(range(num_epochs)):\n        epoch_losses = []\n\n        for j, b in enumerate(tqdm.tqdm(train_dataloader, total=len(train_dataloader))):\n            images = b[\"image\"].to(device)\n            seqs_gt = b[\"seq\"]\n            seq_lens_gt = b[\"seq_len\"]\n\n            seqs_pred = crnn(images).cpu()\n            log_probs = fnn.log_softmax(seqs_pred, dim=2)\n            seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n\n            loss = fnn.ctc_loss(log_probs=log_probs,  # (T, N, C)\n                                targets=seqs_gt,  # N, S or sum(target_lengths)\n                                input_lengths=seq_lens_pred,  # N\n                                target_lengths=seq_lens_gt)  # N\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            epoch_losses.append(loss.item())\n\n        print(i, np.mean(epoch_losses))","18cd0a10":"val_losses = []\ncrnn.eval()\nfor i, b in enumerate(tqdm.tqdm(val_dataloader, total=len(val_dataloader))):\n    images = b[\"image\"].to(device)\n    seqs_gt = b[\"seq\"]\n    seq_lens_gt = b[\"seq_len\"]\n\n    with torch.no_grad():\n        seqs_pred = crnn(images).cpu()\n    log_probs = fnn.log_softmax(seqs_pred, dim=2)\n    seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n\n    loss = fnn.ctc_loss(log_probs=log_probs,  # (T, N, C)\n                        targets=seqs_gt,  # N, S or sum(target_lengths)\n                        input_lengths=seq_lens_pred,  # N\n                        target_lengths=seq_lens_gt)  # N\n\n    val_losses.append(loss.item())\n\nprint(np.mean(val_losses))","16880f3c":"y_ticks = [\"-\"] + [x for x in abc]\n\nimages = b[\"image\"]\nseqs_gt = b[\"seq\"]\nseq_lens_gt = b[\"seq_len\"]\ntexts = b[\"text\"]\n\npreds = crnn(images.to(device)).cpu().detach()\ntexts_pred = decode(preds, crnn.alphabet)\n\nfor i in range(10):\n    plt.figure(figsize=(15, 5))\n    pred_i = preds[:, i, :].T\n\n    plt.subplot(1, 2, 1)\n    image = images[i].permute(1, 2, 0).numpy()\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.title(texts[i])\n\n    plt.subplot(1, 2, 2)\n    plt.yticks(range(pred_i.size(0)), y_ticks)\n    plt.imshow(pred_i)\n    plt.title(texts_pred[i])\n\n    plt.show()","aec5c781":"# Save\n# with open('crnn_10_epochs.pth', 'wb') as fp:\n#     torch.save(crnn.state_dict(), fp)","0824cfda":"# Load\nwith open('crnn_10_epochs.pth', 'rb') as fp:\n    state_dict = torch.load(fp, map_location=\"cpu\")\ncrnn.load_state_dict(state_dict)\ncrnn.to(device)","486e4872":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# \u041c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u043d\u043e\u043c\u0435\u0440\u043d\u044b\u0445 \u0437\u043d\u0430\u043a\u043e\u0432 \u043d\u0430 \u0444\u043e\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u044f\u0445\n# A model which find bboxes for license plates\nmodel = create_model(device)\n\n# Load\nwith open('fasterrcnn_resnet50_fpn_1_epoch', 'rb') as fp:\n    state_dict = torch.load(fp, map_location=\"cpu\")\nmodel.load_state_dict(state_dict)\nmodel.to(device)\n\n# Test dataset\ntest_dataset = CarPlatesDatasetWithRectangularBoxes('data', transformations, 'test')\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=2, shuffle=False, num_workers=4,\n    collate_fn=collate_fn)","94c0b020":"predicted_boxes = []\nfor batch in tqdm.tqdm(test_loader):\n    images = list(image.to(device) for image in batch[0])\n    model.eval()\n    preds = model(images)\n    preds = [{k: v.detach().cpu().numpy() for k, v in prediction.items()} for prediction in preds]\n    predicted_boxes.extend(preds)","2a0de3ee":"boxes = [box.astype(int).tolist() for box in (boxes_in_image['boxes'] for boxes_in_image in predicted_boxes)]\nassert len(boxes) == len(test_dataset)","a12f29ee":"json_data = []\nfor file_name, box in zip(test_dataset.image_names, boxes):\n    json_data.append({'boxes': box, 'file': str(file_name)})\n\nwith open('data\/test_boxes.json', 'w') as fp:\n    json.dump(json_data, fp)","62a05a7f":"transformations = transforms.Compose([\n    Resize()])\n\ntest_plates_dataset = CarPlatesFragmentsDataset('data', transformations, 'test')","91d45653":"test_dataloader = torch.utils.data.DataLoader(test_plates_dataset, \n                                              batch_size=1, shuffle=False,\n                                              num_workers=num_workers, pin_memory=True, \n                                              drop_last=True, collate_fn=collate_fn)","6e78db29":"crnn.eval()\nsubmit = {}\nfor b in tqdm.tqdm(test_dataloader):\n    file_name = b['file_name'][0][5:]\n    if file_name not in submit:\n            submit[file_name] = []\n    images = b[\"image\"]\n    preds = crnn(images.to(device)).cpu().detach()\n    texts_pred = decode(preds, crnn.alphabet)\n    submit[file_name].append(texts_pred[0])","68414b7d":"submit = [(k, ' '.join(v)) for k,v in submit.items()]\nsubmission = pd.DataFrame(submit, columns=['file_name', 'plates_string'])","fde18891":"random_submission = pd.read_csv('submission.csv')","2f70fe00":"submission = pd.merge(random_submission, submission, how='left', on='file_name')\nsubmission.drop('plates_string_x', axis=1, inplace=True)\nsubmission.columns = ['file_name', 'plates_string']\nsubmission","5a697fc8":"submission.to_csv('baseline_crnn.csv', index=False)","831ef029":"### \u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0440\u0430\u043c\u043e\u043a \u043d\u043e\u043c\u0435\u0440\u043e\u0432 \u0438\u0437 \u0442\u0435\u0441\u0442\u0430\n### Test dataset of license plates bboxes","9925b48e":"### \u0414\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\n### Making predictions","05e02313":"## \u0421\u043e\u0431\u0435\u0440\u0435\u043c \u0432\u0441\u0435 \u0432\u043c\u0435\u0441\u0442\u0435 \u0438 \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0434\u043b\u044f test_data\n## Let'a put everything together and make a prediction on test_data","51e79e53":"1. ### \u0421\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0432 \u0444\u0430\u0439\u043b\n1. ### Save the result","f09b59b1":"## 2. \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c 1 \u044d\u043f\u043e\u0445\u0443\n## 2. Train the model - 1 epoch","d6ac9683":"### \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0447\u0442\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c:\n### Let's look at what we got","a77003e6":"## 1. Dataset, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u0435\u0442\u0438 \u043f\u043e\u0438\u0441\u043a\u0430 bbox'\u043e\u0432 \u043d\u043e\u043c\u0435\u0440\u043e\u0432 - \u044d\u0442\u043e \u043f\u0435\u0440\u0432\u044b\u0439 \u044d\u0442\u0430\u043f\n## 1. Dataset for bbox detection training - 1st step****","32c80f98":"## 3. \u041e\u0442\u0434\u0435\u043b\u044c\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e \u0444\u0440\u0430\u0433\u043c\u0435\u043d\u0442\u0443 \u0444\u043e\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u0438 \u0441 \u043d\u043e\u043c\u0435\u0440\u043e\u043c, \u0431\u0443\u0434\u0435\u0442 \u0435\u0433\u043e \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u043e\u0432\u0430\u0442\u044c (CRNN)\n## 3. CRNN - a separate model for a license plate recognition"}}