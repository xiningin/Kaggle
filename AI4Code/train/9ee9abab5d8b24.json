{"cell_type":{"38ef765d":"code","84656430":"code","e3861582":"code","6fbee304":"code","5d534edc":"code","3c8f871a":"code","65fe57f4":"code","4f1b7dbf":"code","c44240ad":"code","5d3ed7d0":"code","a17a518f":"code","0f280617":"code","aaaf52a2":"code","305a72e9":"code","5dcbcb25":"code","416e6be4":"code","8c37b4be":"code","0d47a27c":"code","eb29e3af":"code","3a7732f3":"code","d22fd1ba":"code","38b64603":"code","51bd8c2b":"code","59a42ffe":"code","ea7bee29":"code","2518f067":"code","b3652014":"code","ec37010e":"code","3849aa76":"code","a5b3587b":"code","8ba59ff9":"code","7a99806a":"code","bf2ed97f":"markdown","0a669b4f":"markdown","36924dc0":"markdown","494fa437":"markdown","4583521d":"markdown","b808aa07":"markdown","8f87a51a":"markdown","32dd8664":"markdown","04a9f999":"markdown","4e2630a3":"markdown","6b288c0c":"markdown","dcc26239":"markdown","581f59f7":"markdown","732010a7":"markdown","fd1f164f":"markdown","4fe43ea5":"markdown"},"source":{"38ef765d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","84656430":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.preprocessing as skpe\nimport sklearn.model_selection as ms\nimport sklearn.metrics as sklm\nimport sklearn.ensemble as ensemble\nimport xgboost as xgb\nimport lightgbm as lgb\nimport scipy.stats as stats\nimport sklearn.kernel_ridge as ridge\nimport numpy.random as nr\nimport sklearn.linear_model as lm","e3861582":"path = \"..\/input\/house-prices-advanced-regression-techniques\/train.csv\"\ntrain = pd.read_csv(path)\ntrain.head()","6fbee304":"path1 = \"..\/input\/house-prices-advanced-regression-techniques\/test.csv\"\ntest = pd.read_csv(path1)\ntest.head()","5d534edc":"train.info()\nprint(\"-------------------------------------\")\n\ntest.info()","3c8f871a":"train.describe()","65fe57f4":"test.describe()","4f1b7dbf":"# Let's look at the target variable first\ntrain['SalePrice'].describe()","c44240ad":"# This looks slightly right-skewed\nsns.distplot(train['SalePrice'])","5d3ed7d0":"# Skewness and Kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","a17a518f":"sns.scatterplot(y='SalePrice',x='LotArea',data=train)","0f280617":"sns.scatterplot(y='SalePrice',x='TotalBsmtSF',data=train)","aaaf52a2":"sns.scatterplot(y='SalePrice',x='GrLivArea',data=train)","305a72e9":"sns.boxplot(y=train['SalePrice'],x=train['OverallQual'])","5dcbcb25":"# HeatMap\nfig, ax = plt.subplots(figsize=(20, 18))\nsns.heatmap(train.corr(), vmax=.8)","416e6be4":"k = 10 #number of variables for heatmap\ncols = train.corr().nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nhm = sns.heatmap(cm, cbar=True, annot=True, annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)","8c37b4be":"corr = train.corr()\n\n# Sort in descending order\ncorr_top = corr['SalePrice'].sort_values(ascending=False)[:10]\ntop_features = corr_top.index[1:]\nprint(corr_top)","0d47a27c":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'LotArea','YearBuilt']\nsns.pairplot(train[cols], size = 2.5)","eb29e3af":"Q1 = []\nQ3 = []\nLower_Bound = []\nUpper_Bound = []\nOutliers = []\n\nfor i in top_features:\n    \n    # 25th and 75th percentiles\n    q1, q3 = np.percentile(train[i], 25), np.percentile(train[i], 75)\n    \n    # Interquartile range\n    iqr = q3 - q1\n    \n    # Outlier cutoff\n    cut_off = 1.5*iqr\n    \n    # Lower and upper bounds\n    lower_bound = q1 - cut_off\n    upper_bound = q3 + cut_off\n    \n    # Save outlier indexes\n    outlier = [x for x in train.index if train.loc[x,i] < lower_bound or train.loc[x,i] > upper_bound]\n    \n    # Append values for dataframe\n    Q1.append(q1)\n    Q3.append(q3)\n    Lower_Bound.append(lower_bound)\n    Upper_Bound.append(upper_bound)\n    Outliers.append(len(outlier))\n    \n    try:\n        train.drop(outlier, inplace=True, axis=0)\n        \n    except:\n        continue\n        \ndf_out = pd.DataFrame({'column':top_features,'Q1':Q1,'Q3':Q3,'Lower_Bound':Lower_Bound,'Upper_Bound':Upper_Bound,'No. of Outliers':Outliers})\ndf_out.sort_values(by='No. of Outliers', ascending=False)","3a7732f3":"train.shape","d22fd1ba":"# Saving train rows\nntrain = train.shape[0]\n\n# Save log transformation of target variable to deal with the skewness\ntarget = np.log(train['SalePrice'])\n\n# Drop Id and SalePrice from train dataframe\ntrain.drop(['Id', 'SalePrice'], inplace=True, axis=1)\n\n# Store test Id\ntest_Id = test['Id']\n\n# Drop test Id\ntest.drop(['Id'], inplace=True, axis=1)\n\n# Concatenate train and test dataframes\ntrain = pd.concat([train, test])","38b64603":"train.isnull().sum().sort_values(ascending=False).head(40)","51bd8c2b":"# Ordinal Features\n\n# NA means no pool\ntrain['PoolQC'].replace(['Ex', 'Gd', 'Fa', np.nan],[3,2,1,0], inplace=True)\n\n# NA means no fence\ntrain['Fence'].replace(['GdPrv', 'MnPrv', 'GdWo', 'MnWw', np.nan],[4,3,2,1,0], inplace=True)\n\n# NA means no fireplace\ntrain['FireplaceQu'].replace(['Ex', 'Gd', 'TA', 'Fa', 'Po', np.nan],[5,4,3,2,1,0], inplace=True)\n\n# Garage Features\ntrain['GarageCond'].replace(['Ex', 'Gd', 'TA', 'Fa', 'Po', np.nan],[5,4,3,2,1,0], inplace=True)\n\ntrain['GarageQual'].replace(['Ex', 'Gd', 'TA', 'Fa', 'Po', np.nan],[5,4,3,2,1,0], inplace=True)\n\ntrain['GarageFinish'].replace(['RFn', 'Fin', 'Unf', np.nan],[3,2,1,0], inplace=True)\n\n# Bsmt Features\nfor i in ['BsmtCond', 'BsmtQual']:\n    train[i].replace(['Ex', 'Gd', 'TA', 'Fa', 'Po', np.nan],[5,4,3,2,1,0], inplace=True)\n\ntrain['BsmtExposure'].replace(['Gd', 'Av', 'Mn', 'No', np.nan],[4,3,2,1,0], inplace=True)\n\nfor i in ['BsmtFinType1', 'BsmtFinType2']:\n    train[i].replace(['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', np.nan],[6,5,4,3,2,1,0], inplace=True)\n\n\n# Nominal Features\n\n# NA means no alley\ntrain['Alley'].fillna('None', inplace=True)\n\n# NA means no miscellaneous features\ntrain['MiscFeature'].fillna('None', inplace=True)\n\n# NA means no garage type\ntrain['GarageType'].fillna('None', inplace=True)\n\n# NA means no masonry work\ntrain['MasVnrType'].fillna('None', inplace=True)\n\n# If no work, then no area\ntrain['MasVnrArea'].fillna(0, inplace=True)\n\n\n# Numerical Features\n\n# Replace null lotfrontage with average of the neighbourhood\ntrain['LotFrontage'] = train.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# Filling 0 with null values in BsmtFeatures\nfor i in ['BsmtHalfBath', 'BsmtFullBath', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']:\n    train[i].fillna(0, inplace=True)\n    \n# Replace with most common values\nfor i in ['MSZoning', 'Utilities', 'KitchenQual']:\n    train[i].fillna(train[i].mode()[0], inplace=True)\n    \ntrain['Functional'].fillna('Typ', inplace=True)\n\ntrain['SaleType'].fillna('Oth' ,inplace=True)\n\n# Replace with most common value\ntrain['Electrical'].fillna(train['Electrical'].mode()[0] ,inplace=True)\n\ntrain['GarageCars'].fillna(train['GarageCars'].mode()[0] ,inplace=True)\n\ntrain['GarageYrBlt'].fillna(train['GarageYrBlt'].mode()[0] ,inplace=True)\n\n# Repace with 'Other' value\nfor i in ['Exterior1st', 'Exterior2nd']:\n    train[i].fillna('Other', inplace=True)\n    \ntrain['KitchenQual'].replace(['Ex', 'Gd', 'TA', 'Fa', 'Po'],[4,3,2,1,0], inplace=True)\n\ntrain['GarageArea'].fillna(train['GarageArea'].astype('float').mean(axis=0), inplace=True)\n","59a42ffe":"# Total surface area of house\ntrain['TotalSF'] = train.apply(lambda x: x['1stFlrSF'] + x['2ndFlrSF'] + x['TotalBsmtSF'], axis=1)\n\n# Total bathrooms in the house\ntrain['TotalBath'] = train.apply(lambda x: x['FullBath'] + 0.5*x['HalfBath'] + x['BsmtFullBath'] + 0.5*x['BsmtHalfBath'], axis=1)\n\n# Total porch area of house\ntrain['TotalPorch'] = train.apply(lambda x: x['OpenPorchSF'] + x['EnclosedPorch'] + x['3SsnPorch'] + x['ScreenPorch'], axis=1)","ea7bee29":"# Dummifying the dataset for modelling\ntrain =pd.get_dummies(train, drop_first=True)\ntrain.shape","2518f067":"# Train dataset\ndf = train.iloc[:ntrain,:]\n\n# Test dataset\ntest = train.iloc[ntrain:,:]\n\n# Seperating independent and dependent variables\nX = df\ny = target","b3652014":"# train,test split to get training,validation and testing\nX_train,X_test,y_train,y_test = ms.train_test_split(X,y,random_state=2,test_size=0.2)","ec37010e":"lr = lm.LinearRegression()\nlr.fit(X_train,y_train)\n\nrmse = np.sqrt(sklm.mean_squared_error(y_test,lr.predict(X_test)))\nprint(rmse)","3849aa76":"# Different alpha values\nalphas = [0.01,0.1,0.3,1,3,5,10,20]\n\nfor a in alphas:\n    kernel_ridge = ridge.KernelRidge(alpha=a)\n    kernel_ridge.fit(X_train,y_train)\n    \n    rmse = np.sqrt(sklm.mean_squared_error(y_test,kernel_ridge.predict(X_test)))\n    print('For alpha =',a,',','RMSE = ',rmse)","a5b3587b":"model = ridge.KernelRidge(alpha=0.1)\nmodel.fit(X_train,y_train)","8ba59ff9":"log_pred = model.predict(test)\nactual_pred = np.exp(log_pred)","7a99806a":"subm_dict = {'Id':test_Id, 'SalePrice':actual_pred}\nsubmit = pd.DataFrame(subm_dict)\nsubmit.to_csv('submission.csv', index=False)","bf2ed97f":"*We are getting the lowest RMSE score with alpha value of 0.1. Since, I  got the lowest value of RMSE with KernelRidge Regression, I will be using this model for final prediction.*","0a669b4f":"**Bsmt area shares an exponential relationship with SalePrice.**","36924dc0":"*Before Submitting, we need to take inverse of the log transformation that we did while training the model.*","494fa437":"**As expected living area above ground is almost linearly varying with SalePrice.**","4583521d":"# Feature Engineering","b808aa07":"**It looks like OverallQual shares a very distinctive relationship with SalePrice with multiple variations.**","8f87a51a":"# Feature Transformation","32dd8664":"# Outliers","04a9f999":"**There are some features which tend to be a major factor in predicting sale price like LotArea, GrLivArea, Bsmt area(TotalBsmtSF) and OverallQual. So, we will look at their relationship with the target variable.**","4e2630a3":"# Handling Missing Data","6b288c0c":"* KernelRidge","dcc26239":"**This is looking constant to SalePrice.**","581f59f7":"**Let's see some top features having highest correlation coeffecient with the target variable.**","732010a7":"# Modelling","fd1f164f":"* Linear Regression","4fe43ea5":"**Creating dataframe for submission**"}}