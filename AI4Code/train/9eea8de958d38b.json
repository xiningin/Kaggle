{"cell_type":{"da65ec12":"code","0266d363":"code","2f029d42":"code","6fc34cd9":"code","a50974af":"code","a58986c5":"code","617fef5b":"code","23d0f2da":"code","20a2f2af":"markdown","42b1045b":"markdown","ad2f523e":"markdown","9901f15a":"markdown","32812bc5":"markdown","5f828674":"markdown","a31ec45d":"markdown"},"source":{"da65ec12":"import numpy as np \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom scipy.spatial.distance import cosine\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_curve, auc\n%matplotlib inline\nimport nltk\nfrom nltk.tokenize import sent_tokenize, PunktSentenceTokenizer, word_tokenize\nimport os\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords","0266d363":"\n\ncorpus = pd.read_csv('..\/input\/Womens Clothing E-Commerce Reviews.csv')\ncorpus = corpus[['Review Text', 'Clothing ID']]\n# We have selected Clothing ID 1080 for oyr analysis\ncorpus = corpus.loc[corpus['Clothing ID'] == 1080 ]\ncorpus = corpus['Review Text']\n#  Dropping the entries with empty reviews\ncorpus.dropna(inplace = True)\n# Tokenizing the corpus\nsent_tokenized_corpus = []\n# To store the reviews\nreviewsList = []\n# Firstly we are tokenizing reviews using sentence tokenizer\nfor review in corpus :\n    reviewsList.append(review)\n    sent_tokenized_corpus.append(sent_tokenize(review))\n    \nword_tokenized_reviews = []\nwords = []\n# Now we are using word tokenizer to tokenize the review into words\nfor review in sent_tokenized_corpus :\n    for sent in review :\n        words += (word_tokenize(sent))\n    word_tokenized_reviews.append(words)\n    words = []\n#print(word_tokenized_reviews)\n\nlemmatizer = WordNetLemmatizer()\n\nreview_str = \"\"\nstop_words = set(stopwords.words('english'))\n\nfinal_corpus = []\nfor review in word_tokenized_reviews:\n    for words in review :\n        if words not in stop_words:\n            review_str += (\" \"+(lemmatizer.lemmatize(words.lower())))\n    final_corpus.append(review_str)\n    review_str =\"\"\n\n# Tokenizing the corpus after removing stop words and lemmetizing\nvectorizer = CountVectorizer(min_df=0, stop_words=stop_words)\n\ndocs_tf = vectorizer.fit_transform(final_corpus)\nvocabulary_terms = vectorizer.get_feature_names()\n\n#selecting the keywords\nkeywords = [\"love\", \"pretty\", \"incredible\", \"adorable\", \"stunner\" ]\n\n","2f029d42":"docs_query_tf = vectorizer.transform(final_corpus + [' '.join(keywords)]) \n\ntransformer = TfidfTransformer(smooth_idf = False)\ntfidf = transformer.fit_transform(docs_query_tf.toarray())\n\n# D x V document-term matrix \ntfidf_matrix = tfidf.toarray()[:-1] \n\n# 1 x V query-term vector \nquery_tfidf = tfidf.toarray()[-1] ","6fc34cd9":"query_doc_tfidf_cos_dist = [cosine(query_tfidf, doc_tfidf) for doc_tfidf in tfidf_matrix]\nquery_doc_tfidf_sort_index = np.argsort(np.array(query_doc_tfidf_cos_dist))\n\nfor rank, sort_index in enumerate(query_doc_tfidf_sort_index):\n    if rank == 5 :\n        break\n    print(\"The rank is\", rank)\n    print(\"The cosine distance is\", query_doc_tfidf_cos_dist[sort_index])\n    print(\"Review\")\n    print(reviewsList[sort_index])\n    ","a50974af":"tf_matrix = docs_tf.toarray() # D x V matrix \nA = tf_matrix.T \n\nU, s, V = np.linalg.svd(A, full_matrices=1, compute_uv=1)\nK = 2 # number of components\n\nA_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), V[:K, :])) # D x V matrix \n\ndocs_rep = np.dot(np.diag(s[:K]), V[:K, :]).T # D x K matrix \nterms_rep = np.dot(U[:,:K], np.diag(s[:K])) # V x K matrix \n","a58986c5":"key_word_indices = [vocabulary_terms.index(key_word) for key_word in keywords] # vocabulary indices \n\nkey_words_rep = terms_rep[key_word_indices,:]     \nquery_rep = np.sum(key_words_rep, axis = 0)\n","617fef5b":"query_doc_cos_dist = [cosine(query_rep, doc_rep) for doc_rep in docs_rep]\nquery_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n\nfor rank, sort_index in enumerate(query_doc_sort_index):\n    if rank == 5 :\n        break\n    print(\"The rank is\", rank)\n    print(\"The cosine distance is\", query_doc_tfidf_cos_dist[sort_index])\n    print(\"Review\")\n    print(reviewsList[sort_index])","23d0f2da":"# plot documents in the new space  \n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.scatter(docs_rep[:,0], docs_rep[:,1], c=query_doc_cos_dist) # all documents \nplt.scatter(query_rep[0],query_rep[1],   marker='+', c='red') # the query \nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")","20a2f2af":"**1. Preprocessing**\n* Tokenization\n* Removing Stop Words\n* Lemmetizing (Stemming produces some words that are not present in the actual dictionary)","42b1045b":"**5. Information Retrieval using in LSA**","ad2f523e":"**3. Information Retrieval using IF-IDF  **\n* Here we are using cosine distance to see the correlation between our keywords and reviews.","9901f15a":"# Text Analysis\n**Dataset : Women's E-Commerce Clothing Review**","32812bc5":"**2. Making TF-IDF matrix**","5f828674":"**4. LSA using TF matrix**","a31ec45d":"**6. Plotting**"}}