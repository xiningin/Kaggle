{"cell_type":{"74d63d63":"code","b7498bef":"code","033fbd7b":"code","e5c5da2f":"code","e397eac0":"code","e6abedc6":"code","271af968":"code","268cdad6":"code","e42ff615":"code","42998d9c":"code","46af4794":"code","01c0958b":"code","f9817475":"code","64c99c6e":"code","19352a76":"code","2083fd2e":"code","d84f981a":"code","cd7ab2e4":"code","ca3fb636":"code","f9b9eb0b":"code","975c4093":"code","3a4ad31b":"code","73f6b6e6":"code","6213e7f1":"code","36b3dbf1":"code","16b9a4ca":"code","5854beb7":"code","19bb2ee8":"code","b90bbe01":"code","23fc654b":"code","b4abc93b":"code","a61fc77c":"code","cce56a3f":"markdown","72da5f96":"markdown","28523dfb":"markdown","d829d94f":"markdown","017563e9":"markdown","45a6635c":"markdown","3d9b13cf":"markdown","48c79bdf":"markdown","f0d722df":"markdown","cb10a312":"markdown","d05b87e7":"markdown","5743b9c5":"markdown","186a4fe4":"markdown","45cd7e00":"markdown","6cd4d7b9":"markdown","89225d7b":"markdown","d9a49659":"markdown","3da0e0c5":"markdown","a408f32f":"markdown","2c894cf7":"markdown","13c56ca4":"markdown"},"source":{"74d63d63":"import datetime as dt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom keras import layers\nfrom keras import Input\nfrom keras.models import Model\n\nimport pickle\n\nimport matplotlib.pyplot as plt","b7498bef":"import os\ninput_dir = \"..\/input\"\nbase_data_dir = os.path.join(input_dir,'data-science-for-good-careervillage')\nfe_ts_dir = os.path.join(input_dir,'cv-feature-engineering-text-scores')\nag_data_dir = os.path.join(input_dir,'cv-data-augmentation-network-predictors-2')","033fbd7b":"tags = pd.read_csv(os.path.join(base_data_dir, 'tags.csv'))\nprint(tags.shape)\nprint(tags.head(3))","e5c5da2f":"tag_users = pd.read_csv(os.path.join(base_data_dir, 'tag_users.csv'))\ntag_users.sample(3)","e397eac0":"# Count users per tag\ntags_by_users_counts = tag_users.groupby('tag_users_tag_id')['tag_users_user_id'].count().reset_index()\ntags_by_users_counts = tags_by_users_counts.rename(columns={'tag_users_user_id': 'users_count'})\nprint(tags_by_users_counts.shape)\ntags_by_users_counts.head(3)","e6abedc6":"tags = tags.merge(tags_by_users_counts, \n                  left_on='tags_tag_id', right_on='tag_users_tag_id', how='left')\nprint(tags.shape)\nprint(tags.head(3))","271af968":"tag_questions = pd.read_csv(os.path.join(base_data_dir, 'tag_questions.csv'))\nprint(tag_questions.shape)\nprint(tag_questions.sample(3))","268cdad6":"# Count questions per tag\ntags_by_questions_counts = tag_questions.groupby('tag_questions_tag_id')['tag_questions_question_id'].count().reset_index()\ntags_by_questions_counts = tags_by_questions_counts.rename(columns={'tag_questions_question_id': 'questions_count'})\nprint(tags_by_questions_counts.shape)\ntags_by_questions_counts.head(3)","e42ff615":"tags = tags.merge(tags_by_questions_counts, \n                  left_on='tags_tag_id', right_on='tag_questions_tag_id', how='left')\nprint(tags.shape)\nprint(tags.head(3))","42998d9c":"print(tags.shape)\ntags = tags.dropna(how='any', subset=['users_count', 'questions_count'])\nprint(tags.shape)","46af4794":"print(tags.shape)\ntags = tags[(tags['users_count'] >= 5) | (tags['questions_count'] >= 5)]\nprint(tags.shape)","01c0958b":"tags = tags.reset_index()\ndel tags['index']\ntags = tags.reset_index().set_index('tags_tag_id')\nprint(tags.head(5))\nprint(tags.tail(5))","f9817475":"print(tag_users.shape)\ntag_users = tag_users[tag_users['tag_users_tag_id'].isin(tags['tag_users_tag_id'])]\nprint(tag_users.shape)","64c99c6e":"print(tag_questions.shape)\ntag_questions = tag_questions[tag_questions['tag_questions_tag_id'].isin(tags['tag_users_tag_id'])]\nprint(tag_questions.shape)","19352a76":"tags = tags[['index', 'tags_tag_name']]\nprint(tags.head(5))\nprint(tags.tail(5))","2083fd2e":"examples = pd.read_parquet(os.path.join(ag_data_dir,'positive_negative_examples.parquet.gzip'))\nexamples = examples.sort_values(\n    by=['questions_id', 'questions_date_added', 'answer_user_id', 'emails_date_sent'])\nexamples.sample(3)","d84f981a":"print(examples.groupby(['questions_id', 'answer_user_id'])['questions_date_added'].count().sort_values(\n    ascending=False).head(3))\nprint(examples[(examples['questions_id']=='9a42d4109ee141c0838fd966efcb5026') & \n               (examples['answer_user_id']=='6adc2bf866bd428892821b044eb8f0fe')].drop_duplicates())","cd7ab2e4":"val_period_start = dt.datetime(2018, 7, 1)","ca3fb636":"train_examples = examples[examples['questions_date_added'] < val_period_start]\nprint(train_examples.shape)\nprint(train_examples[train_examples['matched']==1].shape[0])","f9b9eb0b":"val_examples = examples[examples['questions_date_added'] >= val_period_start]\nprint(val_examples.shape)\nprint(val_examples[val_examples['matched']==1].shape[0])","975c4093":"def get_user_tags_vector(user_id, tags):\n    user_tags = tag_users[tag_users['tag_users_user_id']==user_id]['tag_users_tag_id']\n    user_tags_vector = np.zeros(tags.shape[0])\n    for tag_id in user_tags:\n        user_tags_vector[tags.loc[tag_id]['index']] = 1\n    return user_tags_vector","3a4ad31b":"def get_question_tags_vector(question_id, tags):\n    question_tags = tag_questions[tag_questions['tag_questions_question_id']==question_id]['tag_questions_tag_id']\n    question_tags_vector = np.zeros(tags.shape[0])\n    for tag_id in question_tags:\n        question_tags_vector[tags.loc[tag_id]['index']] = 1\n    return question_tags_vector","73f6b6e6":"def sample_question_professionals(question_id, examples, unmatched_matched_ratio):\n    question_instances = examples[examples['questions_id']==question_id]\n    \n    matched_professionals = question_instances[question_instances['matched']==1]['answer_user_id'].values\n    matched_professionals_targets = np.repeat(1, len(matched_professionals))\n    \n    unmatched_professionals = question_instances[question_instances['matched']==0]['answer_user_id'].values\n    sampled_unmatched_professionals = np.random.choice(\n        unmatched_professionals, size=unmatched_matched_ratio * len(matched_professionals), replace=False)\n    sampled_unmatched_professionals_targets = np.repeat(0, len(sampled_unmatched_professionals))\n\n    return (np.concatenate((matched_professionals, sampled_unmatched_professionals), axis=0), \n            np.concatenate((matched_professionals_targets, sampled_unmatched_professionals_targets), axis=0))","6213e7f1":"def sample_question_tags_vectors(question_id, examples, tags, unmatched_matched_ratio):\n    \n    sampled_question_professionals, matched_targets = sample_question_professionals(\n        question_id, examples, unmatched_matched_ratio)\n\n    question_tags_vector = get_question_tags_vector(question_id, tags) \n    question_tags_vectors = np.broadcast_to(question_tags_vector, \n                                            (len(sampled_question_professionals), len(question_tags_vector)))\n\n    professionals_tags_vectors = []\n    for professional_id in sampled_question_professionals:\n        professionals_tags_vectors.append(get_user_tags_vector(professional_id, tags))\n    professionals_tags_vectors = np.vstack(professionals_tags_vectors)\n    \n    return question_tags_vectors, professionals_tags_vectors, matched_targets","36b3dbf1":"def generator(examples, tags, num_questions, unmatched_matched_ratio):\n    \n    question_statistics = examples.groupby('questions_id').agg({'matched': 'sum', 'answer_user_id': 'count'})\n    question_statistics = question_statistics[\n        ((question_statistics['matched']>0) &\n         (question_statistics['answer_user_id']>((unmatched_matched_ratio + 1)*question_statistics['matched'])))]\n    question_ids = question_statistics.index.values\n\n    while True:\n\n        cum_question_tags_vectors = []\n        cum_professionals_tags_vectors = []\n        cum_matched_targets = []\n        \n        for question_id in np.random.choice(question_ids, size=num_questions, replace=False):\n            question_tags_vectors, professionals_tags_vectors, matched_targets = sample_question_tags_vectors(\n                question_id, examples, tags, unmatched_matched_ratio)\n            cum_question_tags_vectors.append(question_tags_vectors)\n            cum_professionals_tags_vectors.append(professionals_tags_vectors)\n            cum_matched_targets.append(matched_targets)\n\n        cum_question_tags_vectors = np.concatenate(cum_question_tags_vectors, axis=0)\n        cum_professionals_tags_vectors = np.concatenate(cum_professionals_tags_vectors, axis=0)\n        cum_matched_targets = np.concatenate(cum_matched_targets, axis=0)\n    \n        yield [cum_question_tags_vectors, cum_professionals_tags_vectors], cum_matched_targets","16b9a4ca":" # the number of questions randomly drawn for each training round\ntraining_num_questions = 16\n# the ratio of the sample size of unmatched instances over matched ones for each training round\ntraining_unmatched_matched_ratio = 1\ntrain_gen = generator(train_examples, tags, training_num_questions, training_unmatched_matched_ratio)\n\n # the number of questions randomly drawn for each validation round\nval_num_questions = 16\n # the ratio of the sample size of unmatched instances over matched ones for each validation round\nval_unmatched_matched_ratio = 1\nval_gen = generator(val_examples, tags, val_num_questions, val_unmatched_matched_ratio)","5854beb7":"latent_dimensions = 64\n\n# Both questions and professionals share the same tag embedding layer\nembeddings = layers.Dense(latent_dimensions)\n\nquestion_tags_input = Input(shape=(tags.shape[0],), name='question_tags')\nquestion_tags_output = embeddings(question_tags_input)\n\nprofessional_tags_input = Input(shape=(tags.shape[0],), name='professional_tags')\nprofessional_tags_output = embeddings(professional_tags_input)\n\n# There are many way to combine the embeddins of question tags and professional tags\n# One simple option is used here just for demonstration\ntags_multiplied = layers.multiply([question_tags_output, professional_tags_output])\n\n# Merge with other features\nmerged_with_others = layers.concatenate([question_tags_output, professional_tags_output, tags_multiplied], axis=-1)\n\n# Another feed forward layer\nreduced_l1 = layers.Dense(16, activation='relu')(merged_with_others)\n\n# The binary predictions of matched (1) versus unmatched (0) are modeled by a sigmoid function\npredictions = layers.Dense(1, activation='sigmoid')(reduced_l1)\n\nmodel = Model([question_tags_input, professional_tags_input], predictions)\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])","19bb2ee8":"# # Test Run\n# history = model.fit_generator(train_gen, steps_per_epoch=5, epochs=5, \n#                               validation_data=val_gen, validation_steps=5)\n\n# Full Run\nhistory = model.fit_generator(train_gen, steps_per_epoch=50, epochs=75, \n                              validation_data=val_gen, validation_steps=50)","b90bbe01":"model.save('tags_embeddings.h5')","23fc654b":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","b4abc93b":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nepochs = range(1, len(loss) + 1)\nplt.figure()\nplt.plot(epochs, acc, 'bo', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()","a61fc77c":"with open('trainHistory_tags_embeddings', 'wb') as out_file:\n    pickle.dump(history.history, out_file)","cce56a3f":"## I.4. Filter Rare Tags","72da5f96":"# II.1. Functions for Dense Tag Vectors","28523dfb":"# III. Deep Learning Models for Tag Semantics","d829d94f":"** The model here by all means is not the optimal one. We use a simple model just to demonstrate how the supervised ML data set that we have built can be used to learn tag semantics. **","017563e9":"** Functions to obtain tag indicators for professionals and questions. The indicator for a tag is set to 1 if the professional or the question registers for that tag. Otherwise, it is set to 0. **","45a6635c":"** Training and Validation Loss **","3d9b13cf":"## III.2. Model Estimation","48c79bdf":"** The supervised ML data set is divided into two train and validation sets. The validation period starts from July 1, 2018. **","f0d722df":"## I.3. Question Tags","cb10a312":"## I.4. Supervised ML Data Set","d05b87e7":"## I.1. Tags","5743b9c5":"** We generate training and validation data for deep learning models using random sampling. For each iteration, a random sample of questions is drawn from the full list of questions. We then generate a sample for each selected question by combining all matched instances with a random sample of unmatched instances. The sample size of unmatched instances is equal to * unmatched_matched_ratio * times the number of matched cases. **","186a4fe4":"## I.2. User Tags","45cd7e00":"## III.1. Model Specification","6cd4d7b9":"# Tag Embeddings Demo for CareerVillage.org","89225d7b":"## II.2. Data Generators","d9a49659":"## III.3. Model Checking","3da0e0c5":"# I. Loading Data","a408f32f":"** We can use the outputs of the embedding layer or the prediction layer as inputs in the GBDTs model. This model also demonstrates how deep learning models can replace the GBDTs one. We can have another embedding layer for matching texts between questions and professionals plus other inputs for activity and network statistics... **","2c894cf7":"** Training and Validation Accuracy **","13c56ca4":"# II. Functions"}}