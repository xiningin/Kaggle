{"cell_type":{"e07693e7":"code","44220e7e":"code","51dc1eaa":"code","534946bd":"code","1feccc53":"code","bc07420a":"code","af5d96b1":"code","0c7a0ed7":"code","001629bc":"code","33dd2f33":"code","64a7eb4a":"code","d767d22d":"code","71b0e5b8":"code","68414807":"code","9c9ad028":"code","fe80e51f":"code","84f10734":"code","408b5ee0":"code","8ed57cb3":"code","24fc3cf0":"code","d7c3f441":"code","b8267e87":"code","b955b42a":"code","ba8dabd6":"code","349590d6":"code","db864562":"code","20377eaa":"code","8f724106":"code","76f72988":"code","6f3b7124":"code","01cbcb9a":"code","9f76ef5b":"code","e226cc91":"code","67334f67":"code","4844a075":"code","f9830095":"code","6a13bc2f":"code","4e86c199":"code","16c017ab":"code","05dc7b40":"code","3bb19d61":"code","f25ddf8a":"code","0eef41d6":"code","1f9c8c40":"code","1fac46e6":"code","6db6cad0":"code","8e1fecb3":"markdown","4d4b979a":"markdown","9a6391d6":"markdown","2f17f9ac":"markdown","64c7f606":"markdown","f6f48620":"markdown","b93a6b1b":"markdown","c112fb1b":"markdown","d1b061d2":"markdown","dee53d04":"markdown","4a1e28f3":"markdown","b83ab220":"markdown","538a421f":"markdown","af5169d3":"markdown","2580c92e":"markdown"},"source":{"e07693e7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom xgboost import XGBClassifier\nfrom wordcloud import WordCloud\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier, LinearRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout, Embedding, LSTM\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical","44220e7e":"# loading data.\ntrain_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","51dc1eaa":"train_df.head()","534946bd":"test_df.head()","1feccc53":"# filling nan values in the columns. \ntrain_df.keyword.fillna('', inplace=True)\ntrain_df.location.fillna('', inplace=True)\n\ntest_df.keyword.fillna('', inplace=True)\ntest_df.location.fillna('', inplace=True)","bc07420a":"train_df['text'] = train_df['text'] + ' ' + train_df['keyword'] + ' ' + train_df['location']\ntest_df['text'] = test_df['text'] + ' ' + test_df['keyword'] + ' ' + test_df['location']\n\ndel train_df['keyword']\ndel train_df['location']\ndel train_df['id']\ndel test_df['keyword']\ndel test_df['location']\ndel test_df['id']","af5d96b1":"train_df.head()","0c7a0ed7":"test_df.head()","001629bc":"sns.countplot(train_df.target)","33dd2f33":"# As we already know there are lots of stopwords like 'a', 'our' which are no use to us while feature selection for our data.\n# So we should remove them from our text\n# creating list of stopwords.\nstop = set(stopwords.words('english'))\npunctuations = list(string.punctuation)\nstop.update(punctuations)\nprint(stop)","64a7eb4a":"# Functions to clean up the text like removing numbers and urls.\ndef remove_numbers(text):\n    text = ''.join([i for i in text if not i.isdigit()])         \n    return text\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","d767d22d":"train_df.text = train_df.text.apply(remove_numbers)\ntrain_df.text = train_df.text.apply(remove_URL)\ntrain_df.text = train_df.text.apply(remove_html)\ntrain_df.text = train_df.text.apply(remove_emoji)\ntrain_df.head()","71b0e5b8":"test_df.text = test_df.text.apply(remove_numbers)\ntest_df.text = test_df.text.apply(remove_URL)\ntest_df.text = test_df.text.apply(remove_html)\ntest_df.text = test_df.text.apply(remove_emoji)\ntest_df.head()","68414807":"# this function return the part of speech of a word.\ndef get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","9c9ad028":"lemmatizer = WordNetLemmatizer()\ndef clean_text(text):\n    clean_text = []\n    for w in word_tokenize(text):\n        if w.lower() not in stop:\n            pos = pos_tag([w])\n            new_w = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))\n            clean_text.append(new_w)\n    return \" \".join(clean_text)","fe80e51f":"train_df.text = train_df.text.apply(clean_text)\ntest_df.text = test_df.text.apply(clean_text)","84f10734":"real = train_df.text[train_df.target[train_df.target==1].index]\nfake = train_df.text[train_df.target[train_df.target==0].index]","408b5ee0":"plt.figure(figsize = (18,24)) # Text Reviews with real disaster\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(real))\nplt.imshow(wordcloud,interpolation = 'bilinear')","8ed57cb3":"plt.figure(figsize = (18,24)) # Text Reviews with fake disaster\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(fake))\nplt.imshow(wordcloud,interpolation = 'bilinear')","24fc3cf0":"# splitting our training data into train and validation just to check our model.\nx_train_text, x_val_text, y_train, y_val = train_test_split(train_df.text, train_df.target, test_size=0.2, random_state=0)","d7c3f441":"#Min_df : It ignores terms that have a document frequency (presence in % of documents) strictly lower than the given threshold.\n#For example, Min_df=0.66 requires that a term appear in 66% of the docuemnts for it to be considered part of the vocabulary.\n\n#Max_df : When building the vocabulary, it ignores terms that have a document frequency strictly higher than the given threshold.\n#This could be used to exclude terms that are too frequent and are unlikely to help predict the label.\ntv=TfidfVectorizer(min_df=0,max_df=0.8,use_idf=True,ngram_range=(1,3))\n\n#transformed train reviews\ntv_train_reviews=tv.fit_transform(x_train_text)\n\n#transformed validation reviews\ntv_val_reviews=tv.transform(x_val_text)\n\n#transformed test reviews\ntv_test_reviews=tv.transform(test_df.text)\n\nprint('tfidf_train:',tv_train_reviews.shape)\nprint('tfidf_validation:',tv_val_reviews.shape)\nprint('tfidf_test:',tv_test_reviews.shape)","b8267e87":"# defining classifier\nnb = MultinomialNB()\n\n# fitting for tfidf vectorizer.\ntfidf = nb.fit(tv_train_reviews, y_train)","b955b42a":"# predicting for validation data\ntfidf_val_predict = tfidf.predict(tv_val_reviews)\nprint('Tfidf Vectorizer score :',accuracy_score(y_val, tfidf_val_predict))","ba8dabd6":"print(classification_report(y_val, tfidf_val_predict))","349590d6":"cm = confusion_matrix(y_val, tfidf_val_predict)\ncm = pd.DataFrame(cm , index = [i for i in range(2)] , columns = [i for i in range(2)])\nplt.figure(figsize = (8,6))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","db864562":"svc = SVC()\n\n# fitting for tfidf vectorizer.\ntfidf = svc.fit(tv_train_reviews, y_train)","20377eaa":"# predicting for validation data\ntfidf_val_predict = tfidf.predict(tv_val_reviews)\nprint('Tfidf Vectorizer score :',accuracy_score(y_val, tfidf_val_predict))","8f724106":"print(classification_report(y_val, tfidf_val_predict))","76f72988":"cm = confusion_matrix(y_val, tfidf_val_predict)\ncm = pd.DataFrame(cm , index = [i for i in range(2)] , columns = [i for i in range(2)])\nplt.figure(figsize = (8,6))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","6f3b7124":"xgb = XGBClassifier()\n\n# fitting for tfidf vectorizer.\ntfidf = xgb.fit(tv_train_reviews, y_train)","01cbcb9a":"# predicting for validation data\ntfidf_val_predict = tfidf.predict(tv_val_reviews)\nprint('Tfidf Vectorizer score :',accuracy_score(y_val, tfidf_val_predict))","9f76ef5b":"print(classification_report(y_val, tfidf_val_predict))","e226cc91":"cm = confusion_matrix(y_val, tfidf_val_predict)\ncm = pd.DataFrame(cm , index = [i for i in range(2)] , columns = [i for i in range(2)])\nplt.figure(figsize = (8,6))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","67334f67":"rfc = RandomForestClassifier()\n\n# fitting for tfidf vectorizer.\ntfidf = rfc.fit(tv_train_reviews, y_train)","4844a075":"tfidf_val_predict = tfidf.predict(tv_val_reviews)\nprint('Tfidf Vectorizer score :',accuracy_score(y_val, tfidf_val_predict))","f9830095":"print(classification_report(y_val, tfidf_val_predict))","6a13bc2f":"cm = confusion_matrix(y_val, tfidf_val_predict)\ncm = pd.DataFrame(cm , index = [i for i in range(2)] , columns = [i for i in range(2)])\nplt.figure(figsize = (8,6))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","4e86c199":"model = Sequential()\n\nmodel.add(Dense(units = 512 , activation = 'relu' , input_dim = tv_train_reviews.shape[1]))\nmodel.add(Dense(units = 256 , activation = 'relu'))\nmodel.add(Dense(units = 100 , activation = 'relu'))\nmodel.add(Dense(units = 10 , activation = 'relu'))\nmodel.add(Dense(units = 1 , activation = 'sigmoid'))\n\nmodel.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n\nmodel.summary()","16c017ab":"history = model.fit(tv_train_reviews, y_train, validation_data=(tv_val_reviews, y_val), batch_size=128, epochs=10)","05dc7b40":"# plotting accuracy and loss curves for train and validation data.\nplt.figure(figsize=(10,12))\nplt.subplot(221)\nplt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\n\nplt.subplot(222)\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","3bb19d61":"model_val_predict = model.predict_classes(tv_val_reviews)\ncm = confusion_matrix(y_val, model_val_predict)\ncm = pd.DataFrame(cm , index = [i for i in range(2)] , columns = [i for i in range(2)])\nplt.figure(figsize = (8,6))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","f25ddf8a":"y_pred = model.predict_classes(tv_test_reviews)\nsubmission.target = y_pred\nsubmission.to_csv(\"submission.csv\" , index = False)","0eef41d6":"y_pred = rfc.predict(tv_test_reviews)\nsubmission.target = y_pred\nsubmission.to_csv(\"submission.csv\" , index = False)","1f9c8c40":"y_pred = xgb.predict(tv_test_reviews)\nsubmission.target = y_pred\nsubmission.to_csv(\"submission.csv\" , index = False)","1fac46e6":"y_pred = svc.predict(tv_test_reviews)\nsubmission.target = y_pred\nsubmission.to_csv(\"submission.csv\" , index = False)","6db6cad0":"y_pred = nb.predict(tv_test_reviews)\nsubmission.target = y_pred\nsubmission.to_csv(\"submission.csv\" , index = False)","8e1fecb3":"# Models","4d4b979a":"**We can see the target column is balanced.**","9a6391d6":"# Splitting Data","2f17f9ac":"# If you face any kind of difficulty in code do comment down. Any kind of suggestions is much appreciated.\n# Don't forget to upvote. It's free :-)","64c7f606":"**As we can see in wordcloud some words like 'amp' is very frequent in our both data so it makes sense to ignore this word using attribute max_df (explained below)**","f6f48620":"# Predicting For Test Data","b93a6b1b":"# Text Cleaning","c112fb1b":"**1. Multinomial NaiveBayes Classifier**","d1b061d2":"**5. Creating Our Model**","dee53d04":"**4. Random Forest Classifier**","4a1e28f3":"**3. XgBoostClassifier**","b83ab220":"# Importing Libraries","538a421f":"**Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.**\n\n**Text preprocessing includes both Stemming as well as Lemmatization.Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.**\n\n**You guyz can read about lemmatization https:\/\/www.geeksforgeeks.org\/python-lemmatization-with-nltk\/ here.**","af5169d3":"# Data Visualisation","2580c92e":"**2. SVC Classifier**"}}