{"cell_type":{"fc126c2b":"code","8d972971":"code","3e3f6936":"code","02e4c402":"code","23ff5472":"code","818a793f":"code","aa8e231b":"code","0edbc22b":"code","73537ef7":"code","5963f948":"code","70fb8ea6":"code","509a0e03":"code","5551640e":"code","201b6c38":"code","8eccc640":"code","d2109a7f":"code","c5a634fc":"code","33c73d5b":"code","b78e7ec5":"code","dea3e23f":"code","176fe11b":"code","20c60b3c":"code","9c1aa5e2":"code","f003dcef":"code","616c8d5c":"code","6f9cb5e0":"code","1e9e08f2":"code","efd96da0":"code","3fe8ecd5":"code","9d1ebd34":"code","7216b978":"code","1d732fd9":"code","ec97ed3e":"code","209ef96d":"code","ec2fd52c":"code","ee757e10":"code","6bf610ca":"code","14285513":"code","e5360f2f":"code","a050f368":"code","8b9bc07a":"code","23776d18":"code","5b7b5a84":"code","ff90d724":"code","c2d207a6":"code","59673a0a":"code","45fa52a1":"code","d83d9684":"code","8b7b5a8c":"code","6fe18be1":"code","8660d399":"code","169c8e5e":"code","e7ce57ca":"code","a7d96c93":"code","6355fddf":"code","5e60ec0b":"code","963065a0":"markdown","f277285c":"markdown","18d82af6":"markdown","eb440e07":"markdown","8c0bd40c":"markdown","f94af963":"markdown","a3caee0d":"markdown","e10f6477":"markdown","d8522be5":"markdown","dfd57b5d":"markdown","6168b6f2":"markdown","e8cf00ba":"markdown","5631a0e9":"markdown","87124c92":"markdown","494067fe":"markdown","fbac4308":"markdown","c1c53327":"markdown","cc15d17a":"markdown","9faca1a1":"markdown","22c70507":"markdown","41a4b400":"markdown"},"source":{"fc126c2b":"# import what we need\/didnt already have\n!pip3 install goodreads_api_client isbnlib newspaper3k tqdm progressbar scipy ","8d972971":"import numpy as np \nimport pandas as pd\nimport os\nimport seaborn as sns\nimport isbnlib\nfrom newspaper import Article\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfrom tqdm import tqdm\nfrom progressbar import ProgressBar\nimport re\nimport copy\nfrom scipy import stats\nfrom scipy.cluster.vq import kmeans, vq\nfrom pylab import plot, show\nfrom matplotlib.lines import Line2D\nimport matplotlib.colors as mcolors\nfrom mpl_toolkits import mplot3d\nfrom mpl_toolkits.mplot3d import Axes3D\nimport goodreads_api_client as gr\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.manifold import TSNE\nfrom sklearn import neighbors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.decomposition import PCA\n","3e3f6936":"df = pd.read_csv('..\/input\/goodreadsbooks\/books.csv', error_bad_lines = False)","02e4c402":"df.index = df['bookID']","23ff5472":"#and what \ndf.head()","818a793f":"print(\"Dataset contains {} rows and {} columns\".format(df.shape[0], df.shape[1]))","aa8e231b":"#apparently no Nulls!\ndf[\"average_rating\"].isnull().value_counts()","0edbc22b":"#drop NaNs anyway\ndf.dropna(0, inplace=True)","73537ef7":"df.columns","5963f948":"df = df[df['  num_pages'] >= 30]\ndf.shape","70fb8ea6":"z = np.abs(stats.zscore(df['  num_pages']))\nfor i in range(len(list(np.where(z>=3)[0]))):\n    df.drop(df.index[np.where(z>=3)[0][i]], inplace=True)\ndf.shape","509a0e03":"ss = StandardScaler()\ndf2 = ss.fit_transform(df[['average_rating', 'ratings_count', 'text_reviews_count']])","5551640e":"pca = PCA()\npca_data = pca.fit_transform(df2)\nnew_df = pd.DataFrame(pca_data)\nnew_df","201b6c38":"#in the range of 2-30, lets find the best n_clusters that best explain our books.# \nX = new_df\ndistortions = []\nfor k in range(2,30):\n    k_means = KMeans(n_clusters = k)\n    k_means.fit(X)\n    distortions.append(k_means.inertia_)\n\nfig = plt.figure(figsize=(15,10))\nplt.plot(range(2,30), distortions, 'bx-')\nplt.title(\"Elbow Curve\")","8eccc640":"CENTROIDS = 7 ","d2109a7f":"k_means = KMeans(n_clusters = CENTROIDS)\nnew_df['idx'] = k_means.fit_predict(new_df)\nnew_df","c5a634fc":"ax3 = plt.axes(projection='3d')\nax3.scatter(new_df[0], new_df[1], new_df[2], edgecolor='black', c=new_df['idx'])\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('plotting the 3 principle components')","33c73d5b":"plt.scatter(new_df[0], new_df[1], edgecolor='black', c=new_df['idx'])\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('plotting 2 principle components')","b78e7ec5":"plt.scatter(new_df[0], new_df[2], edgecolor='black', c=new_df['idx'])\nplt.xlabel('PC1')\nplt.ylabel('PC3')\nplt.title('plotting 2 principle components')","dea3e23f":"plt.scatter(new_df[1], new_df[2], edgecolor='black', c=new_df['idx'])\nplt.xlabel('PC2')\nplt.ylabel('PC3')\nplt.title('plotting 2 principle components')","176fe11b":"z = np.abs(stats.zscore(new_df[[0,1,2]]))\nto_drop = np.unique(np.where(z>=3)[0])\nprint(to_drop, type(to_drop))","20c60b3c":"new_df.drop(to_drop, inplace=True)\nnew_df","9c1aa5e2":"# Set up the axes with gridspec\nfig = plt.figure(figsize=(8, 8))\ngrid = plt.GridSpec(3, 3, hspace=0.2, wspace=0.2)\nfig3 = plt.figure()\nax3 = Axes3D(fig3)\n\npc1_2 = fig.add_subplot(grid[:-1, 1:])\npc0_1 = fig.add_subplot(grid[:-1, 0], xticklabels=[], sharex=pc1_2)\npc0_2 = fig.add_subplot(grid[-1, 1:], yticklabels=[], sharey=pc1_2)\n\n# scatter points on the main axes\nax3.scatter(new_df[0], new_df[1], new_df[2], edgecolor='black', c=new_df['idx'])\n\n# PCs on the attached axes\npc0_1.scatter(new_df[0], new_df[1], edgecolor='black', c=new_df['idx'])\npc0_2.scatter(new_df[0], new_df[2], edgecolor='black', c=new_df['idx'])\npc1_2.scatter(new_df[1], new_df[2], edgecolor='black', c=new_df['idx'])\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('plotting principle components')","f003dcef":"#model.predict(42)","616c8d5c":"#i was trying to put them back together but was getting some nasty NaNs.. \n#i had forgotten that i had dropped many columns\ndf.shape, new_df.shape\n#see this is a problem, time to M E R G E","6f9cb5e0":"df3 = df.copy()\n#dropping all the outliers we found in the 2nd z score test for PCs (this took like way too long to figure out)\ndf3.drop(df3.index[to_drop], inplace=True)\ndf3.head(2)","1e9e08f2":"df3.shape, new_df.shape\n#WOOOOO HOOOO OMG took 7h27 but u figured it out! im proud of past vince for tenacity!","efd96da0":"#no NaNs\ndf3.isnull().values.any(), new_df.isnull().values.any()","3fe8ecd5":"for col in df3:\n    print(col, df3.isnull()[col].value_counts(), \"\\n\")","9d1ebd34":"#reseting index since we had dropped outliers before\n#this may give an error, in which case comment out line 3\ndf3 = df3.drop('bookID', axis=1)\ndf3 = df3.reset_index()\ndf3 = df3.drop('index', axis=1)\ndf3","7216b978":"#reseting index since we had dropped outliers before\nnew_df = new_df.reset_index()\nnew_df = new_df.drop(\"index\", axis=1)\nnew_df","1d732fd9":"#finally merging the two (PCA'd values with the original df)\ndf4 = df3.merge(new_df, right_index=True, left_index=True)\ndf4","ec97ed3e":"#reverifying there are no NaNs\nfor col in df4:\n    print(col, df4.isnull()[col].value_counts(), \"\\n\")","209ef96d":"#adding a new col 'rating_between' for future dummy\nfor i in range(5):\n    df4.loc[ (df4['average_rating'] >= i) & (df4['average_rating'] <= i+1), 'rating_between'] = f\"between {i} and {i+1}\"","ec2fd52c":"#making a dummy col for each rating level\ndummy_rating_df = df4['rating_between'].str.get_dummies(sep=\",\")\ndummy_rating_df.head(3)","ee757e10":"#making a dummy col for each language\ndummy_language_df = df4['language_code'].str.get_dummies(sep=\",\")\ndummy_language_df.head(3)","6bf610ca":"def weighted_rating(df):\n    v = df['ratings_count']\n    R = df['average_rating']\n    C = df['average_rating'].mean()\n    m = df['ratings_count'].quantile(0.9)\n    return (v\/(v+m) * R) + (m\/(m+v) * C)","14285513":"#readjusting the weight of \"average rating\" to be proportional to the quantile of ratings count\ndf4['weighted_score'] = weighted_rating(df4)","e5360f2f":"#renaming columns\ndf4.columns = ['bookID','title','authors','average_rating','isbn','isbn13','language_code',\n               '  num_pages','ratings_count','text_reviews_count','publication_date',\n               'publisher',\"PC1\",\"PC2\",\"PC3\",'idx','rating_between','weighted_score']\ndf4","a050f368":"model_df = pd.concat([dummy_rating_df, dummy_language_df, df4['weighted_score'], df4['average_rating'], df3['ratings_count'], df4['PC1'], df4['PC2'], df4['PC3']], axis=1)\nmodel_df","8b9bc07a":"min_max_scaler = MinMaxScaler()\nmodel_df_final = min_max_scaler.fit_transform(model_df)\nmodel_df_final_kNN = model_df_final.copy()\nmodel_df_final_DBSCAN = model_df_final.copy()\nmodel_df_final_TSNE = model_df_final.copy()","23776d18":"knn_model = neighbors.NearestNeighbors(n_neighbors=CENTROIDS, algorithm='kd_tree')\nknn_model.fit(model_df_final_kNN)\ndistances, indices = knn_model.kneighbors(model_df_final_kNN)","5b7b5a84":"distances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)","ff90d724":"DBSCAN_model = DBSCAN(eps=0.2, min_samples=4, algorithm='kd_tree')\nDBSCAN_model.fit(model_df_final_DBSCAN)","c2d207a6":"model_df_final_DBSCAN","59673a0a":"#looks like unsupervised ML decides there are 21 genres\nclusters = DBSCAN_model.labels_\nnp.unique(clusters)","45fa52a1":"colors = ['royalblue', 'maroon', 'forestgreen', 'mediumorchid', 'tan', 'deeppink', \n          'olive', 'goldenrod', 'lightcyan', 'navy', 'red', 'orange', 'violet',\n           'green', 'yellow', 'cyan', 'magenta', 'black', 'white', 'blue', 'steelblue']\nvectorizer = np.vectorize(lambda x: colors[x % len(colors)])","d83d9684":"plt.scatter(model_df_final_DBSCAN[:,-1], model_df_final_DBSCAN[:,-2], c=vectorizer(clusters))","8b7b5a8c":"TSNE_model = TSNE(n_components=2, learning_rate=150.0, n_iter = 690, verbose=1)\nX_embedded = TSNE_model.fit_transform(model_df_final_TSNE)","6fe18be1":"sns.scatterplot(X_embedded[:,0], X_embedded[:,1], legend='full', palette='Spectral')","8660d399":"df4.head(40)","169c8e5e":"#use whatever model u want in place of df4:\n#model_df_final_kNN \n#model_df_final_DBSCAN \n#model_df_final_TSNE \n\nall_books_names = list(df4.title.values)","e7ce57ca":"#generously 'forked' from https:\/\/www.kaggle.com\/hoshi7\/goodreads-analysis-and-recommending-books\ndef get_index_from_name(name):\n    print(\"Your book recs for \", name)\n    return df4[df4[\"title\"]==name].index.tolist()[0]\n\n\ndef get_id_from_partial_name(partial):\n    potential_books = dict()\n    for name in all_books_names:\n        if partial in name:\n            potential_books[name] = all_books_names.index(name)\n    \n    print('u might have meant: [')\n    for key in potential_books:\n        print(key, potential_books[key], ',')\n    print('] \\n just for fun, lets use ', key, '\\n' )\n    \n    return(potential_books[key])\n            \ndef book_rec(query=None,indx=None):\n    if indx:\n        for index in indices[indx][1:]:\n            print(df4.iloc[index][\"title\"])\n    if query:\n        try:\n            found_id = get_index_from_name(query)\n            for index in indices[found_id][1:]:\n                print(df4.iloc[index][\"title\"])\n        except:\n            print(\"couldnt find. pls try another book\")\n            found_id = get_id_from_partial_name(query)\n            for index in indices[found_id][1:]:\n                print(df4.iloc[index][\"title\"])","a7d96c93":"book_rec(\"Poor People\")","6355fddf":"book_rec(\"Harry Potter\")","5e60ec0b":"book_rec(\"YOUR BOOK HERE\")","963065a0":"### lets see what we're working with","f277285c":"### lol u THOUGHT\n### we gotta make dummys merge first","18d82af6":"using 7 as our k, we predict what category each book falls into. As you can see the first two rows are the same category, which makes sense because they are both harry potter books","eb440e07":"# KNN","8c0bd40c":"# TSNE","f94af963":"# good reads","a3caee0d":"Columns Description:\n* **bookID** Contains the unique ID for each book\/series\n* **title** contains the titles of the books\n* **authors** contains the author of the particular book\n* **average_rating** the average rating of the books, as decided by the users\n* **ISBN** ISBN(10) number, tells the information about a book - such as edition and publisher\n* **ISBN 13** The new format for ISBN, implemented in 2007. 13 digits\n* **language_code** Tells the language for the books\n* **Num_pages** Contains the number of pages for the book\n* **Ratings_count** Contains the number of ratings given for the book\n* **text_reviews_count** Has the count of reviews left by users","e10f6477":"# ML tiem","d8522be5":" this is roughly \"Genre clusters\" ; eg Sci-Fi, Romance, Thriller, etc","dfd57b5d":"### weighted ratings instead of actual ratings !!","6168b6f2":"### visualization of what we have so far","e8cf00ba":"* Dropping all books with 30 or less pages.... \n* dont BS me with \"yea i reaad 50+ books\/yr\" if u read infant books.\n* also there are like 200ish audio books\/that have just a title page or publisher page etc etc\n* and like 20 books with 0 pages.. somehow?\n* akso lets down books with 2000+ pages. \n* if ur planning on reading east of eden its not like ur gonna take the rec from a machine","5631a0e9":"this is where u can add what book u want (assuming its in the DB of 10405 entires)","87124c92":"### processing data","494067fe":"## make a model df for the model","fbac4308":"## ok outliers r being annoying ","c1c53327":"Let's scale our numerical data for PCA:","cc15d17a":"# DBSCAN","9faca1a1":"### creating the dummys","22c70507":"![Weighted Rating calculation](https:\/\/miro.medium.com\/max\/736\/1*fGziZl2Do-VyQXSCPq_Y2Q.png)","41a4b400":"# recommend me a b00k"}}