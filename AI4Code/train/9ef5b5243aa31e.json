{"cell_type":{"5612a586":"code","d541b46e":"code","1f529a99":"code","2b02f927":"code","004b76a0":"code","aaff6c0e":"code","8a12a7d0":"code","5662df90":"code","cd74450b":"code","2e89d54b":"code","767a81f6":"code","59737c5a":"code","896df3ab":"code","738277ae":"code","2834c332":"code","5a9ab928":"code","b368aef0":"code","c5696260":"code","4912446a":"code","0cedee60":"code","97e31ca2":"code","82b5b2a7":"code","a40171a7":"code","26b4ae20":"code","192f32de":"code","9952236e":"code","a98af4a1":"code","fb616e9d":"code","ef239142":"code","1b80230b":"code","facee9fa":"code","562b26c3":"code","cafc0ba0":"code","bf5f7e50":"code","0b292892":"code","f0a36946":"code","c77363e5":"code","d707fecb":"code","829f299e":"markdown","96b88b97":"markdown","b154adae":"markdown","a30f8c02":"markdown","db1b65c9":"markdown","125d3beb":"markdown","27603202":"markdown","1e1e7b89":"markdown","e95d3dde":"markdown","861a039e":"markdown","25685cad":"markdown","28776354":"markdown","1af9c242":"markdown","f7fd7795":"markdown","64d92089":"markdown","5a5a6bfc":"markdown","53aace97":"markdown"},"source":{"5612a586":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\n\n# plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# machine learning tools\nimport h2o\nfrom h2o.estimators import H2ORandomForestEstimator\nfrom h2o.estimators import H2OGradientBoostingEstimator","d541b46e":"# load data + first glance\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\ndf_sub = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')\n\n# first glance (training data)\ndf_train.head()","1f529a99":"# dimensions\ndf_train.shape","2b02f927":"df_train.info()","004b76a0":"# basic stats\ndf_train.target.describe(percentiles=[0.1,0.25,0.5,0.75,0.9])","aaff6c0e":"# histogram of target\ndf_train.target.plot(kind='hist', bins=100)\nplt.title('Target - Histogram')\nplt.grid()\nplt.show()","8a12a7d0":"# boxplot of target => looking for outliers\ndf_train.target.plot(kind='box')\nplt.title('Target - Boxplot')\nplt.grid()\nplt.show()","5662df90":"df_zero = df_train[df_train.target==0]\ndf_zero","cd74450b":"# let's remove this one observation\ndf_train = df_train[df_train.target>0]\ndf_train.target.describe()","2e89d54b":"features_num = ['cont0', 'cont1', 'cont2', 'cont3', \n                'cont4', 'cont5', 'cont6', 'cont7',\n                'cont8', 'cont9', 'cont10', 'cont11',\n                'cont12', 'cont13']","767a81f6":"# plot distribution of numerical features\nfor f in features_num:\n    plt.figure(figsize=(8,4))\n    df_train[f].plot(kind='hist', bins=100)\n    plt.title(f)\n    plt.grid()\n    plt.show()","59737c5a":"corr_pearson = df_train[features_num].corr(method='pearson')\ncorr_spearman = df_train[features_num].corr(method='spearman')","896df3ab":"fig = plt.figure(figsize = (12,9))\nsns.heatmap(corr_pearson, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()","738277ae":"fig = plt.figure(figsize = (12,9))\nsns.heatmap(corr_spearman, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()","2834c332":"# example of scatter plot - we pick pair having highest (Pearson) correlation\nsns.jointplot(data=df_train, x='cont5', y='cont12',\n              joint_kws = {'alpha': 0.1})\nplt.show()","5a9ab928":"# different visualization\nsns.jointplot(data=df_train, x='cont5', y='cont12', kind='hex')\nplt.show()","b368aef0":"features_cat = ['cat0', 'cat1', 'cat2', 'cat3',\n                'cat4', 'cat5', 'cat6', 'cat7',\n                'cat8', 'cat9']","c5696260":"# plot distribution of categorical features\nfor f in features_cat:\n    plt.figure(figsize=(8,4))\n    df_train[f].value_counts().plot(kind='bar')\n    plt.title(f)\n    plt.grid()\n    plt.show()","4912446a":"# scatter plot of target vs each feature + show correlation\nfor f in features_num:\n    c = df_train[f].corr(df_train.target, method='pearson')\n    c = np.round(c,4)\n    plt.figure(figsize=(7,7))\n    plt.scatter(df_train[f], df_train.target, alpha=0.01)\n    plt.title('Target vs ' + f + ' \/ corr = ' + str(c))\n    plt.xlabel(f)\n    plt.ylabel('Target')\n    plt.grid()\n    plt.show()","0cedee60":"for f in features_num:\n    new_var = f + '_bin'\n    df_train[new_var] = pd.cut(df_train[f], bins=10, include_lowest=True)\n    plt.figure(figsize=(7,7))\n    sns.boxplot(data=df_train, x=new_var, y='target')\n    plt.xticks(rotation=90)\n    plt.grid()\n    plt.show()","97e31ca2":"for f in features_cat:\n    plt.figure(figsize=(10,5))\n    sns.boxplot(data=df_train, x=f, y='target')\n    plt.xticks(rotation=90)\n    plt.grid()\n    plt.show()","82b5b2a7":"# check mean of target as trivial prediction\nm0 = df_train.target.mean()\nprint('Mean of target:', np.round(m0,6))","a40171a7":"# metrics on training data\nfoo = df_train.target - m0 # difference target vs. trivial mean prediction\nfoo = (foo*foo).mean() # mean squared error\nprint('RMSE(train) - Trivial Benchmark: ', np.round(np.sqrt(foo),6))","26b4ae20":"# select predictors\npredictors = features_num + features_cat\nprint('Number of predictors: ', len(predictors))\nprint(predictors)","192f32de":"# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # Use maximum of 12 GB RAM and 4 cores","9952236e":"# upload training and test data in H2O environment\nt1 = time.time()\ntrain_hex = h2o.H2OFrame(df_train)\ntest_hex = h2o.H2OFrame(df_test)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","a98af4a1":"# define Gradient Boosting model\nfit_1 = H2OGradientBoostingEstimator(ntrees = 1000,\n                                     max_depth=9,\n                                     min_rows=1,\n                                     learn_rate=0.01, # default: 0.1\n                                     sample_rate=1,\n                                     col_sample_rate=0.7,\n                                     nfolds=5,\n                                     seed=999)","fb616e9d":"# train model - this takes some time...\nt1 = time.time()\nfit_1.train(x=predictors,\n            y='target',\n            training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","ef239142":"# show cross validation metrics\nfit_1.cross_validation_metrics_summary()","1b80230b":"# show scoring history - training vs cross validations\nfor i in range(5):\n    cv_model_temp = fit_1.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History [RMSE]'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_rmse, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_rmse, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.legend()\n    plt.grid()\n    plt.show()","facee9fa":"# variable importance using shap values => see direction as well as severity of feature impact\nt1 = time.time()\nfit_1.shap_summary_plot(train_hex);\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","562b26c3":"# predict on training data\npred_train = fit_1.predict(train_hex)\ny_train_pred = pred_train.as_data_frame().predict.values # predictions\n\n# and add prediction to original data frame\ndf_train['prediction'] = y_train_pred","cafc0ba0":"# plot predictions vs actual\np=sns.jointplot(data=df_train, x='target', y='prediction',\n              joint_kws={'alpha' : 0.1})\np.fig.suptitle('Prediction vs Actual - Training Data')\nplt.xlabel('Actual')\nplt.ylabel('Prediction')\nplt.show()","bf5f7e50":"# predict on test data\npred_test = fit_1.predict(test_hex)\ny_test_pred = pred_test.as_data_frame().predict.values # predictions\n\n# and plot distribution of predictions\nplt.hist(y_test_pred, bins=100)\nplt.title('Predictions on Test Set')\nplt.grid()\nplt.show()","0b292892":"plt.hist(y_train_pred, bins=100)\nplt.title('Predictions on Training Data')\nplt.grid()\nplt.show()","f0a36946":"# prepare submission\ndf_sub.target = y_test_pred\ndf_sub.head(10)","c77363e5":"# stats\ndf_sub.target.describe()","d707fecb":"# save to file for submission\ndf_sub.to_csv('submission.csv', index=False)","829f299e":"### Before fitting a complex model let's first make a very simple benchmark for comparison:","96b88b97":"#### Nice, no missing values at all!","b154adae":"<a id='6'><\/a>\n# Predict on Test Set + Submission","a30f8c02":"## Numerical Features","db1b65c9":"<a id='5'><\/a>\n# Build Model","125d3beb":"# Table of Contents\n* [Target Exploration](#1)\n* [Numerical Features](#2)\n* [Categorical Features](#3)\n* [Target vs Features](#4)\n* [Build Model](#5)\n* [Predict on Test Set + Submission](#6)","27603202":"## Correlations","1e1e7b89":"### Alternative Visualization using Binning of Features","e95d3dde":"#### Ok, our goal is to do much better than RMSE=0.88719. Let's go.","861a039e":"## Categorical Features","25685cad":"#### Compare with training data:","28776354":"### Scatter Plot Target vs Features","1af9c242":"<a id='2'><\/a>\n# Numerical features","f7fd7795":"#### Check the zero value:","64d92089":"<a id='3'><\/a>\n# Categorical Features","5a5a6bfc":"<a id='4'><\/a>\n# Target vs Features","53aace97":"<a id='1'><\/a>\n# Target Exploration"}}