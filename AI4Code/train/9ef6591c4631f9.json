{"cell_type":{"258dbb14":"code","6484bf02":"code","87269ae8":"code","dc7e262c":"code","a959efac":"code","d3ec89b3":"code","b7852182":"code","bd1ad480":"code","ef6211d3":"code","a6876bb6":"code","1c43a76c":"code","acc61c7e":"code","4fe796e0":"code","54fd7773":"code","ab264de6":"code","bb027c4f":"code","5f6fa22e":"code","ea066f59":"code","2c2974dc":"code","ac403749":"code","2ebaa490":"code","03ba0cea":"code","a1d3de73":"code","de69fc6d":"code","6328e8c3":"code","458e006f":"code","c5c49292":"code","a0e17a8f":"code","1bbb0e55":"code","79f24253":"code","6d109e9f":"code","0e3345f6":"code","c086c9c3":"code","2f98953b":"code","a03a445b":"code","2702291c":"code","cebef9c8":"code","6af22eed":"code","dc924caa":"code","cdee9d45":"code","b23429a6":"code","f8f795b0":"code","adf4bf77":"code","73c3eef9":"code","2dd1a8bf":"code","c1caae02":"code","1318cfd6":"code","c97c76d8":"code","611897ee":"code","ffee8723":"code","cdef3f5c":"code","de3f18f9":"code","cfafe05c":"code","8a0a0cda":"code","7acb80e1":"code","e609a8b4":"code","cca213f0":"code","7c1a9a0b":"code","40e342bc":"code","38a57665":"code","8230d5c5":"markdown","7938d9b2":"markdown","39d46d2f":"markdown","5d37a3ec":"markdown","aec03cc8":"markdown","5cb78df1":"markdown","a9177748":"markdown","84235a48":"markdown","a4299268":"markdown","c96d13e0":"markdown","12a40d70":"markdown","df8d284a":"markdown","e963d76d":"markdown","e59c0f71":"markdown","24389a94":"markdown","81b49d6e":"markdown","8fbf566f":"markdown","c8ec88cd":"markdown","04a69d25":"markdown","8c914015":"markdown","252883f3":"markdown","0a8fbe5d":"markdown","729e056d":"markdown","6d46d91c":"markdown","e4447394":"markdown","d1164c8e":"markdown","8a1252b4":"markdown"},"source":{"258dbb14":"import pandas as pd\nimport numpy as np\nfrom numpy import asarray\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.utils import compute_class_weight\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_rows', 50)","6484bf02":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","87269ae8":"filename='\/kaggle\/input\/car-evaluation-data-set\/car_evaluation.csv'\ndf = pd.read_csv(filename, header=None)","dc7e262c":"df = df.rename(columns={0:\"buying_price\", 1:\"maintenance_cost\", 2:\"door_no\", \n                        3:\"people_no\", 4:\"lug_boot\", 5:\"safety\", 6:\"y\"})","a959efac":"df.head()","d3ec89b3":"df.shape","b7852182":"df.isnull().sum()","bd1ad480":"df.describe()","ef6211d3":"df.buying_price.unique()","a6876bb6":"df.maintenance_cost.unique()","1c43a76c":"df.door_no.unique()","acc61c7e":"df.people_no.unique()","4fe796e0":"df.y.unique()","54fd7773":"df.dtypes","ab264de6":"df['door_no']=df['door_no'].replace(['5more'], '5')\ndf['people_no']=df['people_no'].replace(['more'], '5')","bb027c4f":"df[[\"door_no\", \"people_no\"]] = df[[\"door_no\", \"people_no\"]].apply(pd.to_numeric)","5f6fa22e":"df[[\"buying_price\",\"maintenance_cost\",\"lug_boot\",\"safety\",\"y\"]] = df[[\"buying_price\",\"maintenance_cost\",\"lug_boot\",\"safety\",\"y\"]].astype(\"str\")","ea066f59":"df.head()","2c2974dc":"def plot(column):\n    x=df[column].unique()\n    y=df[column].value_counts()\n    plt.bar(x,y)\n    plt.ylabel('Distribution by ' + column)\n    return plt.show()   ","ac403749":"plot('y')","2ebaa490":"plot('buying_price')","03ba0cea":"plot('maintenance_cost')","a1d3de73":"plot('door_no')","de69fc6d":"plot('people_no')","6328e8c3":"plot('safety')","458e006f":"sns.pairplot(df)","c5c49292":"def grp_brplt(col1):\n    \n    df1 = df.groupby(['y',col1]).size().to_frame('total').reset_index()\n    \n    plt.figure(figsize=(10,8))\n    ax=plt.subplot()\n    ax = sns.barplot(data=df1, x=df1[col1], y=df1[\"total\"], hue=df1[\"y\"])\n    \n    for p in ax.patches:\n        ax.annotate(format(p.get_height(), '.1f'), \n                       (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                       ha = 'center', va = 'center', \n                       xytext = (0, 9), \n                       textcoords = 'offset points')\n\n    ax.set_title('Distribution of ' +col1+ ' per target variable', fontsize=20)\n    ax.legend(loc='center right', bbox_to_anchor=(1.25, 0.5), ncol=1, title='y')\n    return ax ","a0e17a8f":"grp_brplt(\"maintenance_cost\")","1bbb0e55":"grp_brplt(\"buying_price\")","79f24253":"grp_brplt(\"door_no\")","6d109e9f":"grp_brplt(\"people_no\")","0e3345f6":"grp_brplt(\"lug_boot\")","c086c9c3":"grp_brplt(\"safety\")","2f98953b":"df['y1'] = np.where(df['y'].isin(['acc','good', 'vgood']) ,'acc', 'unacc')","a03a445b":"category_lmh = asarray(['low', 'med', 'high', 'vhigh']) \ncategory_smb= asarray(['small', 'med', 'big']) \n\ninputlist=[\"buying_price\", \"maintenance_cost\", \"lug_boot\", \"safety\", \"y\", \"y1\"]\n\noutputlist= []\nfor column in inputlist:\n        output = column+'_ordenc'\n        outputlist.append(output)\n        \n        if column==\"buying_price\" or column==\"maintenance_cost\" or column==\"safety\": \n           \n            enc = preprocessing.OrdinalEncoder(categories=[category_lmh])\n            df[output]= enc.fit_transform(df[[column]])\n            \n        elif column==\"lug_boot\":\n               \n            enc = preprocessing.OrdinalEncoder(categories=[category_smb])\n            df[output]= enc.fit_transform(df[[column]])\n        else:\n           \n            enc = preprocessing.OrdinalEncoder()\n            df[output]= enc.fit_transform(df[[column]])\n        ","2702291c":"df","cebef9c8":"def trgenc(column):\n    means_y1= df.groupby(column)['y1_ordenc'].mean()\n    means_y= df.groupby(column)['y_ordenc'].mean()\n    \n#   column_trg_bin_enc when the target is binary  \n    column_output_y1 = column+'_trg_bin_enc'\n#   column_trg_multi_enc when the target is multi-class \n    column_output_y = column+'_trg_multi_enc'\n    \n    df[column_output_y1] = df[column].map(means_y1)\n    df[column_output_y] = df[column].map(means_y)\n    \n    return ","6af22eed":"targetenccol=[\"buying_price\", \"maintenance_cost\", \"door_no\", \"people_no\" ,\"lug_boot\", \"safety\"]\n\nfor column in targetenccol:\n        trgenc(column)","dc924caa":"def rf (X, y, n_estimators, max_depth, min_samples_split, random_state, class_weight):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    rf = RandomForestClassifier(n_estimators= n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, random_state=random_state, class_weight=class_weight)\n    rf.fit(X_train,y_train)\n\n    y_pred=rf.predict(X_test)\n    \n    #feature importances     \n    feature_importances = pd.DataFrame(rf.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\n    \n    #Classification Report in df \n    report = metrics.classification_report(y_test, y_pred, output_dict=True)\n    df_classification_report = pd.DataFrame(report).transpose()\n    df_classification_report = df_classification_report.sort_values(by=['precision'], ascending=True)\n    \n    #Confusion matrix in df \n    cm = confusion_matrix(y_test, y_pred)\n    cmdf = pd.DataFrame(cm)\n    \n    return (cmdf, df_classification_report, feature_importances)","cdee9d45":"X = df[['buying_price_trg_bin_enc','maintenance_cost_trg_bin_enc', \n        'door_no_trg_bin_enc', 'people_no_trg_bin_enc',\n        'lug_boot_trg_bin_enc', 'safety_trg_bin_enc']].copy()\n\ny = df['y1_ordenc']\n\nrf (X, y, 12, 12, 300, 0, None)","b23429a6":"rf (X, y, 40, 3, 350, 42, None)","f8f795b0":"X = df[['people_no_trg_bin_enc', 'safety_trg_bin_enc', 'maintenance_cost_trg_bin_enc']].copy()\ny = df['y1_ordenc']\nrf (X, y, 40, 3, 350, 42, None)","adf4bf77":"set(df['y_ordenc'])","73c3eef9":"X = df[['buying_price_trg_multi_enc', 'maintenance_cost_trg_multi_enc', 'safety_trg_multi_enc', \n        'people_no_trg_multi_enc', 'door_no_trg_multi_enc', 'lug_boot_trg_multi_enc']].copy()\n\ny = df['y_ordenc']\nrf (X, y, 50, 3, 350, 42, None)","2dd1a8bf":"X = df[['people_no_trg_multi_enc', 'safety_trg_multi_enc']].copy()\ny = df['y_ordenc']\nrf (X, y, 50, 3, 146, 0, None)","c1caae02":"def treeclf (X, y, random_state, min_samples_split, max_depth, class_weight):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    clf = tree.DecisionTreeClassifier(random_state=random_state, min_samples_split=min_samples_split, max_depth=max_depth, class_weight=class_weight)\n    clf = clf.fit(X_train, y_train)\n\n#     plt.figure(figsize=(10,20))\n#     tree.plot_tree(clf, fontsize=10, feature_names=X.columns) \n    \n    y_pred=clf.predict(X_test)\n    \n    metrics.confusion_matrix(y_test,y_pred)\n    \n    feature_importances = pd.DataFrame(clf.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\n    \n     #Classification Report in df \n    report = metrics.classification_report(y_test, y_pred, output_dict=True)\n    df_classification_report = pd.DataFrame(report).transpose()\n    df_classification_report = df_classification_report.sort_values(by=['precision'], ascending=True)\n    \n    #Confusion matrix in df \n    cm = confusion_matrix(y_test, y_pred)\n    cmdf = pd.DataFrame(cm)\n    \n    \n    return ( cmdf, df_classification_report, feature_importances)","1318cfd6":"X = df[['buying_price_trg_multi_enc', 'maintenance_cost_trg_multi_enc', 'safety_trg_multi_enc', \n        'people_no_trg_multi_enc', 'door_no_trg_multi_enc', 'lug_boot_trg_multi_enc']].copy()\n\ny = df['y_ordenc']\ntreeclf (X, y, 42, 70, 20, None)","c97c76d8":"treeclf (X, y, 42, 80, 20, None)","611897ee":"X = df[['buying_price_trg_multi_enc', 'safety_trg_multi_enc','people_no_trg_multi_enc']].copy()\ny = df['y_ordenc']\ntreeclf (X, y, 42, 70, 40, None)","ffee8723":"treeclf (X, y, 42, 170, 40, None)","cdef3f5c":"np.unique(df['y_ordenc'])","de3f18f9":"classWeights = compute_class_weight('balanced',np.unique(df['y_ordenc']), np.array(df['y_ordenc']))\nclassWeights","cfafe05c":"# Transforming the class weight array into a dictionary to pass it in function\n\nclass_weight = {0: 1.125, 1:6.26086957, 2:0.35702479, 3:6.64615385}","8a0a0cda":"X = df[['buying_price_trg_multi_enc', 'maintenance_cost_trg_multi_enc', 'safety_trg_multi_enc', \n        'people_no_trg_multi_enc', 'door_no_trg_multi_enc', 'lug_boot_trg_multi_enc']].copy()\n\ny = df['y_ordenc']\ntreeclf (X, y, 42, 100, 20, class_weight)","7acb80e1":"treeclf (X, y, 42, 100, 20, None)","e609a8b4":"X = df[['buying_price_trg_multi_enc', 'maintenance_cost_trg_multi_enc', 'safety_trg_multi_enc', \n        'people_no_trg_multi_enc', 'door_no_trg_multi_enc', 'lug_boot_trg_multi_enc']].copy()\n\ny = df['y_ordenc']\nrf (X, y, 50, 3, 350, 42, class_weight)","cca213f0":"def lgovr (X, y, random_state, solver, max_iter, multi_class, class_weight):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    md = LogisticRegression(random_state=random_state, solver= solver, max_iter=max_iter, multi_class=multi_class, class_weight=class_weight)\n    md.fit(X_train,y_train)\n\n    y_pred=md.predict(X_test)\n    \n    \n    #Classification Report in df \n    report = metrics.classification_report(y_test, y_pred, output_dict=True)\n    df_classification_report = pd.DataFrame(report).transpose()\n    df_classification_report = df_classification_report.sort_values(by=['precision'], ascending=True)\n    \n    #Confusion matrix in df \n    cm = confusion_matrix(y_test, y_pred)\n    cmdf = pd.DataFrame(cm)\n    \n    return (cmdf, df_classification_report)","7c1a9a0b":"X = df[['buying_price_trg_multi_enc', 'maintenance_cost_trg_multi_enc', 'safety_trg_multi_enc', \n        'people_no_trg_multi_enc', 'door_no_trg_multi_enc', 'lug_boot_trg_multi_enc']].copy()\n\ny = df['y_ordenc']\nlgovr (X, y, 14, 'saga', 200, 'ovr', class_weight)","40e342bc":"def lgovr (X, y, multi_class, class_weight):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    md = LogisticRegression(multi_class=multi_class, class_weight=class_weight)\n    md.fit(X_train,y_train)\n\n    y_pred=md.predict(X_test)\n    \n    \n    #Classification Report in df \n    report = metrics.classification_report(y_test, y_pred, output_dict=True)\n    df_classification_report = pd.DataFrame(report).transpose()\n    df_classification_report = df_classification_report.sort_values(by=['precision'], ascending=True)\n    \n    #Confusion matrix in df \n    cm = confusion_matrix(y_test, y_pred)\n    cmdf = pd.DataFrame(cm)\n    \n    return (cmdf, df_classification_report)","38a57665":"lgovr (X, y,'ovr', class_weight)","8230d5c5":"## Univariate analysis","7938d9b2":"##  DT on 4 values target","39d46d2f":"# Classification","5d37a3ec":"## Find Class weights ","aec03cc8":"##  Random Forest on 4 values target  ","5cb78df1":"Conclusion: Overall the Random Forest with class weight performes slightly better than the Decision Trees wit Class weight. Reason being is the recall for the class = 0.","a9177748":"## Random Forest with Class Weights ","84235a48":"## Ordinal encoding ","a4299268":"## Decision Trees with Class Weights","c96d13e0":"# Imbalance Problem ","12a40d70":"This code determines the category vars to declare the order in which the ordinal encoding should assigns values to the vars \nBecause the values are different across the variables there were 2 categories declared \nWhile because the target features doesn't matter on which order is transformed I left it \nThe if statement within the loop is built to determine which features correspond to the category","df8d284a":"## Libraries","e963d76d":"# Data transformation ","e59c0f71":"## Bivariate Analysis","24389a94":"#  Decision Tree","81b49d6e":"##  Random forest on 4 values target based on feature importance","8fbf566f":"Based on the results above between Random Forest and Decision Trees - I can conclude that looking at the recall the decision trees performs better than the Random Forest without implementing adjustment to the imbalance problem. \nThe best results ae performed with a decision trees random_seed=42, samples_min=70, trees=20.\n\nFrom this decision trees I am now going to go back to the dataset and solve for the imbalance problem and check how the decision tree model perfoms","c8ec88cd":"## Check column values","04a69d25":"## Null values","8c914015":"##  DT with feature importance on 4 values target","252883f3":"## Random Forest","0a8fbe5d":"## Read data and rename columns","729e056d":"##  Optimize Random Forest based on feature importance ","6d46d91c":"## Target encoding ","e4447394":"# EDA ","d1164c8e":"## Initial Stats ","8a1252b4":"# Logistic Regression for Multi Class with OnevsRest"}}