{"cell_type":{"0087afa3":"code","7d2045db":"code","42907d69":"code","d240429a":"code","1f0ea3fe":"code","25c4ed92":"code","978d0a9c":"code","9ff1a3db":"code","5ae53ade":"code","aa8f5b5e":"code","aed07fee":"code","0ac6c466":"code","a0fd0313":"code","4389c2b8":"code","8bc8915f":"code","29bb20a9":"code","c1819fe8":"code","fa2624f5":"code","13f6a43f":"code","c29db829":"code","e0ee68e4":"code","4688f185":"code","bdb422f1":"code","5bf2fd47":"code","965cf32a":"code","fd89d605":"code","6ddd1b3b":"code","426a03d3":"code","876dd84d":"code","4455e56c":"code","d49caffe":"code","61695c3a":"markdown","34829b58":"markdown","b6fdad2f":"markdown"},"source":{"0087afa3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt, seaborn as sns \n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7d2045db":"data = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\nvis_data = data.drop('DEATH_EVENT', axis = 1)","42907d69":"data.quantile([0.01, 0.99])\ndata.describe()\n# data is not fitting for 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine'. Will have to investigate them further. ","d240429a":"data.info()\n# this data has no non numerical feature ","1f0ea3fe":"plt.style.use('dark_background')\nfig, axes = plt.subplots(2, 2, figsize = (20, 10))\n\nax = sns.boxplot(x='creatinine_phosphokinase', data=vis_data, orient='v', ax=axes[0, 0])\nax = sns.boxplot(x=\"ejection_fraction\", data=vis_data, orient='v', ax=axes[0, 1])\nax = sns.boxplot(x=\"platelets\", data=vis_data, orient='v', ax=axes[1, 0])\nax = sns.boxplot(x=\"serum_creatinine\", data=vis_data, orient='v', ax=axes[1, 1])\n\n# 'ejection_fraction' does not have many outliers so we can ignore this analysis. All the other features do. \n# Outliers will be dealt with during RobustScaling ","25c4ed92":"plt.style.use('dark_background')\nfig, axes = plt.subplots(2, 2, figsize = (15, 10))\n\nax = sns.distplot(vis_data['creatinine_phosphokinase'], ax=axes[0, 0])\nax = sns.distplot(vis_data[\"ejection_fraction\"], ax=axes[0, 1])\nax = sns.distplot(vis_data[\"platelets\"], ax=axes[1, 0])\nax = sns.distplot(vis_data[\"serum_creatinine\"], ax = axes[1, 1])\n\n# ejection_fraction though does not have a significant number of outliers, does not have a normal distribution either. \n# All the other features do have normall distributions.  Let's treat this feature to a log transform and see if the Gaussian distribution\n# is obtained or not ","978d0a9c":"plt.style.use('dark_background')\nplt.figure(figsize = (20, 8))\n# ejection_fraction treatment\nsns.distplot(np.log(vis_data['ejection_fraction']))\n# not exactly Gaussian, but will do\ndata['ejection_fraction'] = pd.Series(np.log(data['ejection_fraction']))","9ff1a3db":"# let's check all the other features as well, distplot and boxplot analysis \nremaining_cols = vis_data.drop(['creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine'], axis = 1)\n# TBD","5ae53ade":"round(data.isnull().sum()\/len(data.index)*100, 2)\n# no missing values in the data either ","aa8f5b5e":"# let's check for data imbalance now. Countplot analysis, for every feature \nplt.style.use('dark_background')\nfig, axes = plt.subplots(4, 3, figsize = (20, 20))\nax = sns.countplot(vis_data['age'], ax=axes[0, 0])\nax = sns.countplot(vis_data['anaemia'], ax=axes[0, 1])\nax = sns.countplot(vis_data['creatinine_phosphokinase'], ax=axes[0, 2])\nax = sns.countplot(vis_data['diabetes'], ax=axes[1, 0])\nax = sns.countplot(vis_data['ejection_fraction'], ax=axes[1, 1])\nax = sns.countplot(vis_data['high_blood_pressure'], ax=axes[1, 2])\nax = sns.countplot(vis_data['platelets'], ax=axes[2, 0])\nax = sns.countplot(vis_data['serum_creatinine'], ax=axes[2, 1])\nax = sns.countplot(vis_data['serum_sodium'], ax=axes[2, 2])\nax = sns.countplot(vis_data['sex'], ax=axes[3, 0])\nax = sns.countplot(vis_data['smoking'], ax=axes[3, 1])\nax = sns.countplot(vis_data['time'], ax=axes[3, 2])\n\nplt.tight_layout()\n\n# platelets and creatinine_phosphokinase have one dominant value it seems. Rest of the features are okay","aed07fee":"list(data['platelets'].value_counts())[0]\/sum(list(data['platelets'].value_counts()))\nlist(data['creatinine_phosphokinase'].value_counts())[0]\/sum(list(data['creatinine_phosphokinase'].value_counts()))\n# dominant values occur just 8% and 15% of the total 300 times, respectively, so this cannot be considered imbalance. ","0ac6c466":"plt.figure(figsize = (15, 15))\nsns.heatmap(data.corr(), annot = True, cmap = 'YlGnBu_r')\n# highest absolute correlation values being just 0.5, there is no strong correlation present anywhere ","a0fd0313":"# we've already identified the binary categorical features from the countplot analysis: anaemia, diabetes, high_blood_pressure, sex, smoking\n# we can simply one hot encode these\nbinaries = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']\ndata[binaries] = data[binaries].astype('str')\ndummies = pd.get_dummies(data[binaries])\ndata = data.drop(data[binaries], axis = 1)\ndata = pd.concat([data, dummies], axis = 1)","4389c2b8":"# now we split the data and then scale it using RobustScaler\nfrom sklearn.model_selection import train_test_split\ny = data['DEATH_EVENT']\nX = data.drop('DEATH_EVENT', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.75, random_state = 1)\n\nfrom sklearn.preprocessing import RobustScaler\nscale = RobustScaler()\nX_train = scale.fit_transform(X_train)\nX_test = scale.transform(X_test)","8bc8915f":"# Modelling from hereon","29bb20a9":"model=[]\nscore=[]","c1819fe8":"# For Hyper-parameter Tuning the model\nfrom sklearn.model_selection import GridSearchCV\n\n# For checking Model Performance\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import learning_curve\n\nimport warnings\nwarnings.simplefilter(action=\"ignore\")\n\nfrom sklearn.model_selection import StratifiedKFold","fa2624f5":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier()\ncv_method = StratifiedKFold(n_splits=3)\nparam={'n_neighbors':(1,3,5,7),'metric':('euclidean','manhattan','chebyshev','minkowski'),'p' :(1,2)}\nclf = GridSearchCV(neigh, param,cv=cv_method,scoring=\"accuracy\")\nclf.fit(X_train,y_train)","13f6a43f":"clf.best_params_","c29db829":"knn = KNeighborsClassifier(n_neighbors= 5, p= 1,metric= 'chebyshev')\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)\n","e0ee68e4":"model.append('knn')\nscore.append(knn.score(X_test,y_test))","4688f185":"from sklearn.ensemble import RandomForestClassifier\nrdf=RandomForestClassifier()\ncv_method = StratifiedKFold(n_splits=3)\nparam={'criterion':('gini', 'entropy'),'min_samples_split': (2, 6, 20),'min_samples_leaf': (1, 4, 16),'n_estimators' :(100,150, 200, 250)}\nclf = GridSearchCV(rdf, param,cv=cv_method,scoring=\"accuracy\")\nclf.fit(X_train,y_train)","bdb422f1":"clf.best_params_","5bf2fd47":"random_forest = RandomForestClassifier(criterion= 'entropy',min_samples_leaf= 1,min_samples_split=20,n_estimators= 250)\nrandom_forest.fit(X_train, y_train)\nrandom_forest.score(X_test, y_test)","965cf32a":"model.append('random_forest')\nscore.append(random_forest.score(X_test,y_test))","fd89d605":"from sklearn.svm import SVC\n\ncv_method = StratifiedKFold(n_splits=3)\nparam={'C': [0.1, 1, 10, 100, 1000],'gamma': [1, 0.1, 0.01, 0.001, 0.0001],'kernel': ['rbf','sigmoid','linear']}\nclf = GridSearchCV(SVC(), param,cv=cv_method,scoring=\"accuracy\")\nclf.fit(X_train,y_train)","6ddd1b3b":"clf.best_params_","426a03d3":"svc=SVC(C= 100, gamma= 0.01, kernel='rbf')\nsvc.fit(X_train, y_train)\nsvc.score(X_test, y_test)","876dd84d":"model.append('svc')\nscore.append(svc.score(X_test,y_test))","4455e56c":"plt.bar(model,score)\nplt.xlabel(model)\nplt.ylabel(score)\nplt.title(\"Comparision of model with score\")\nplt.show()","d49caffe":"#classifiacation reports for applied models\n\nfor i in range(len(model)):\n    print(\"Classification Report for \", model[i],\" with score \",f'{score[i]*100:0.2f}',\"%\")\n    pred = eval(model[i]).predict(X_test)\n    print(classification_report(y_test, pred))\n\n","61695c3a":"3. Support Vector Machine","34829b58":"2. Random Forest","b6fdad2f":"1. KNN"}}