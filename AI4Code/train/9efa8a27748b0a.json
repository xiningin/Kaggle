{"cell_type":{"5ad0027e":"code","50d82017":"code","aaf17970":"code","457bb333":"code","c17ee122":"code","a666eb93":"code","eaf0dd42":"code","296c95d1":"code","e6f84de1":"code","9533733a":"code","5504c93a":"code","0ffebaf3":"code","e2466617":"code","cadcd1e2":"code","67271e8d":"code","0bdac95d":"code","b7516a8c":"code","6d022062":"code","61685b15":"code","95bab7e8":"code","906e4bf5":"code","3ea81bcb":"code","a217c975":"code","b14d8032":"code","bfd8ffd6":"code","7e2d955d":"code","c4e200d9":"code","e8d27e42":"code","84664b54":"code","9c9d1b10":"code","a01fac51":"code","0c02a620":"code","35364249":"code","0d709dd2":"code","31a7c37e":"markdown","7cbcfdcf":"markdown","045fa5ea":"markdown","865090b7":"markdown","80944716":"markdown","967ab233":"markdown","f31975c1":"markdown","cf385616":"markdown","2f20d98d":"markdown","5c935086":"markdown","7b888981":"markdown"},"source":{"5ad0027e":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score","50d82017":"train = pd.read_csv('..\/input\/janatahack-mobility-analysis\/train.csv')\ntest = pd.read_csv('..\/input\/janatahack-mobility-analysis\/test.csv')\nprint(train.shape, test.shape)","aaf17970":"train.head()","457bb333":"for col in train.columns:\n    a = train[col].nunique()\n    print(f'{col}: {a}')","c17ee122":"train.isnull().sum()","a666eb93":"train['flag'] = 'train'\ntest['flag'] = 'test'\ntest.target = None\nall_df = train.append(test)\nall_df.tail()\ncancel1_month = list(all_df['Cancellation_Last_1Month'])","eaf0dd42":"cat_cols = ['Type_of_Cab', 'Confidence_Life_Style_Index', 'Destination_Type', 'Gender', 'Cancellation_Last_1Month']\nfor col in cat_cols:\n    a = all_df[col].unique()\n    print(f'{col}: {a}')","296c95d1":"all_df['Confidence_Life_Style_Index_new'] = [0 if x== 'A'else x for x in all_df['Confidence_Life_Style_Index']]\nall_df['Confidence_Life_Style_Index_new'] = [1 if x== 'B'else x for x in all_df['Confidence_Life_Style_Index_new']]\nall_df['Confidence_Life_Style_Index_new'] = [2 if x== 'C'else x for x in all_df['Confidence_Life_Style_Index_new']]\n\nall_df['Type_of_Cab_new'] = ['F' if x not in ('A', 'B', 'C', 'D', 'E') else x for x in all_df['Type_of_Cab']]","e6f84de1":"mean_LS_index = np.mean(all_df['Life_Style_Index'])\nall_df['lifestyle_plus'] = all_df['Life_Style_Index'] + all_df['Confidence_Life_Style_Index_new']*mean_LS_index + np.random.normal(0.0,1.0)\nall_df['lifestyle_nega'] = all_df['Life_Style_Index'] - all_df['Confidence_Life_Style_Index_new']*mean_LS_index + np.random.normal(0.0,1.0)","9533733a":"all_df['Type_of_Cab_new'] = [1 if x== 'A'else x for x in all_df['Type_of_Cab_new']]\nall_df['Type_of_Cab_new'] = [2 if x== 'B'else x for x in all_df['Type_of_Cab_new']]\nall_df['Type_of_Cab_new'] = [2 if x== 'C'else x for x in all_df['Type_of_Cab_new']]\nall_df['Type_of_Cab_new'] = [3 if x== 'D'else x for x in all_df['Type_of_Cab_new']]\nall_df['Type_of_Cab_new'] = [3 if x== 'E'else x for x in all_df['Type_of_Cab_new']]\nall_df['Type_of_Cab_new'] = [2.2 if x== 'F'else x for x in all_df['Type_of_Cab_new']]","5504c93a":"# New features\nall_df['var4'] = all_df['Var2']*all_df['Var3']\nall_df['var5'] = all_df['Var2']*all_df['Var3']*all_df['Var1']\n\nall_df['total_rating'] = all_df['Life_Style_Index']+all_df['Customer_Rating']\nall_df['diff_rating'] = all_df['Life_Style_Index']-all_df['Customer_Rating']\n\nall_df['dist_rating'] = all_df['Trip_Distance']*all_df['Customer_Rating']\nall_df['index_dist'] = all_df['Life_Style_Index']*all_df['Trip_Distance']\nall_df['dist_by_rating'] = all_df['Trip_Distance']\/all_df['Customer_Rating']\n\nall_df['dist_var2'] = all_df['Trip_Distance']*all_df['Var2']\nall_df['dist_var3'] = all_df['Trip_Distance']*all_df['Var3']\nall_df['dist_var4'] = all_df['Trip_Distance']*all_df['var4']","0ffebaf3":"np.mean(all_df['Var3'])","e2466617":"all_df.columns","cadcd1e2":"Destination_Type_ratio = all_df[all_df['flag'] == 'train'].groupby('Type_of_Cab_new')['Surge_Pricing_Type'].mean()\nDestination_Type_ratio = Destination_Type_ratio.reset_index()\nDestination_Type_ratio.columns = ['Type_of_Cab_new', 'Type_of_Cab_new_ratio']\nall_df = pd.merge(all_df, Destination_Type_ratio, on = 'Type_of_Cab_new', how = 'left')\n\nDestination_Type_ratio = all_df[all_df['flag'] == 'train'].groupby('Destination_Type')['Surge_Pricing_Type'].mean()\nDestination_Type_ratio = Destination_Type_ratio.reset_index()\nDestination_Type_ratio.columns = ['Destination_Type', 'Destination_Type_ratio']\nall_df = pd.merge(all_df, Destination_Type_ratio, on = 'Destination_Type', how = 'left')","67271e8d":"print(all_df.shape)\nall_df = pd.get_dummies(all_df, columns = cat_cols)\nprint(all_df.shape)\nall_df['Cancel_Last_1Month'] = cancel1_month\nprint(all_df.shape)","0bdac95d":"all_df.head()","b7516a8c":"all_df['log_dist'] = np.log(all_df['Trip_Distance'])\nall_df['log_var3'] = np.log(all_df['Var3'])","6d022062":"train = all_df[all_df.flag == 'train'].copy()\ntest = all_df[all_df.flag == 'test'].copy()\nprint(test.shape, train.shape)","61685b15":"train.info()","95bab7e8":"X = train.drop(['Surge_Pricing_Type', 'flag', 'Trip_ID'],axis=1)\ny = train['Surge_Pricing_Type']-1.0\nX.shape, y.shape","906e4bf5":"X.isnull().sum()","3ea81bcb":"import lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import RepeatedKFold, cross_val_score, KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score","a217c975":"model = LGBMClassifier(boosting_type = 'gbdt', objective = 'multiclass', num_class = 3)\n#cv = RepeatedKFold(n_splits = 10, n_repeats = 1, random_state = 22)\ncv = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 22)\nn_scores = cross_val_score(model, X, y, scoring = 'accuracy', cv = cv )\nprint(np.mean(n_scores))\n#70.47\n#70.52\n#70.53\n#70.58","b14d8032":"test = test.drop(['Surge_Pricing_Type', 'flag', 'Trip_ID'],axis=1)","bfd8ffd6":"def accuracy(preds, train_data):\n    labels = train_data.get_label()\n    preds = preds.reshape((len(labels),3), order = 'F')\n    pred = []\n    for x in preds:\n        pred.append(np.argmax(x))\n    #print(len(labels), preds.shape, len(pred))\n    return 'accuracy', accuracy_score(pred, labels), True #name, score, bool for higher result","7e2d955d":"def runLGB(train_X, train_y, test_X, test_y=None, test_X2=None): \n    params = {}\n    params[\"objective\"] = \"multiclass\"\n    params['metric'] = None\n    params[\"max_depth\"] = 9\n    #params['num_leaves'] = 150\n    params['boosting_type'] = 'gbdt'\n    params[\"min_data_in_leaf\"] = 50\n    params[\"learning_rate\"] = 0.02\n    params[\"bagging_fraction\"] = 0.9\n    params[\"feature_fraction\"] = 0.7\n    params[\"bagging_freq\"] = 3\n    params[\"bagging_seed\"] = 50\n    params[\"verbosity\"] = -1\n    params['num_class'] = 3\n    #params['n_estimators'] = 200\n    params['nthread'] = 4\n    num_rounds = 1000\n\n    plst = list(params.items())\n    lgtrain = lgb.Dataset(train_X, label=train_y)\n\n    if test_y is not None:        \n        lgtest = lgb.Dataset(test_X, label=test_y)        \n        model = lgb.train(params, lgtrain,  num_rounds,\n                          valid_sets=[lgtrain,lgtest],                          \n                          early_stopping_rounds=50, \n                          verbose_eval=100, feval = accuracy)\n    else:\n        lgtest = lgb.Dataset(test_X)\n        model = lgb.train(params, lgtrain,   num_rounds, feval = accuracy)\n\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_test_y = (pred_test_y)\n    preds_y = []\n    for x in pred_test_y:\n        preds_y.append(np.argmax(x))\n       \n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    pred_test_y2 = (pred_test_y2)\n    preds_y2 = []\n    for x in pred_test_y2:\n        preds_y2.append(np.argmax(x))\n    \n    acc = 0\n    if test_y is not None:           \n        acc = accuracy_score((test_y), preds_y)\n        return pred_test_y, acc, pred_test_y2, model\n    else:\n        return pred_test_y, acc, pred_test_y2, model","c4e200d9":"%%time\n\nfrom sklearn.model_selection import train_test_split\ndev_X, val_X, dev_y, val_y = train_test_split(X, y, test_size = 0.2, random_state = 22)\nprint(dev_X.shape, val_X.shape)\npred_val, acc, pred_test, model = runLGB(dev_X, dev_y, val_X, val_y, test)\n# 70.24 with depth 7\n# 70.50 with depth 8\n# 70.80 with depth 8 and new feature var3 and cancel1_month #9min 25s for 720 rounds lr = 0.015\n# 70.84 \n# 70.73 increased LR\n# 70.89 increased LR and decreased children in leaves 6min 36s 500 rounds lr = 0.025\n# 71.03 increased LR = 0.025 leaves = 80 8min early = 75 513 rounds only\n# 71.02 9min 6s 591 rounds leaves = 50","e8d27e42":"predictions = []\nfor x in pred_test:\n    predictions.append(np.argmax(x))\nmy_sub = pd.read_csv('..\/input\/janatahack-mobility-analysis\/sample_submission.csv')\nmy_sub['Surge_Pricing_Type'] = predictions\nmy_sub['Surge_Pricing_Type'] = my_sub['Surge_Pricing_Type'] +1.0\nmy_sub.to_csv('lgb_1_split_submission.csv', index=False)","84664b54":"import time\n\ncv_acc_scores = []\npred_test_full = 0\npred_test_weight = 0\nn_splits = 5\nkf = StratifiedKFold(n_splits=n_splits, shuffle= True, random_state=22)\n\nfor dev_index, val_index in kf.split(X, y):\n    start = time.time()\n    dev_X, val_X = X.iloc[dev_index], X.iloc[val_index]\n    dev_y, val_y = y.iloc[dev_index], y.iloc[val_index]    \n    \n    pred_val, acc, pred_test,model = runLGB(dev_X, dev_y, val_X, val_y, test)\n    pred_test_full += pred_test\n    pred_test_weight += pred_test*acc\n    cv_acc_scores.append(acc)\n    print(f'Mean Accuracy: {np.mean(cv_acc_scores)}; Split Accuracy: {acc}')\n    print(f'Total time in seconds till this epoch: {time.time()-start}')\n    #print(f'Accuracy: {np.mean(cv_acc_scores)}, F1: {np.mean(cv_f1_scores)}')\npred_test_full \/= n_splits\npred_test_weight \/= n_splits\nprint(sum(cv_acc_scores)\/n_splits)\n# 0.7050553250493787 depth 8; new feature var3 & cancel1_month #560 sec for 720 rounds 5 splits #0.7057120537 LB\n# 0.70577675 depth 8; new feature var3 & cancel1_month #560 sec for 720 rounds 10 splits #0.705574192 LB\n# 0.7060730796414302","9c9d1b10":"pred_test_full\npredictions = []\nfor x in pred_test_full:\n    predictions.append(np.argmax(x))\nw_preds = []\nfor x in pred_test_weight:\n    w_preds.append(np.argmax(x))","a01fac51":"my_sub = pd.read_csv('..\/input\/janatahack-mobility-analysis\/sample_submission.csv')\nmy_sub['Surge_Pricing_Type'] = predictions\nmy_sub['Surge_Pricing_Type'] = my_sub['Surge_Pricing_Type'] +1.0\nmy_sub.to_csv('lgb_cv_submission.csv', index=False)\n\n#my_sub['target'] = pred_test_full\n#my_sub.to_csv('lgb_submission.csv', index=False)\n","0c02a620":"my_sub = pd.read_csv('..\/input\/janatahack-mobility-analysis\/sample_submission.csv')\nmy_sub['Surge_Pricing_Type'] = w_preds\nmy_sub['Surge_Pricing_Type'] = my_sub['Surge_Pricing_Type'] +1.0\nmy_sub.to_csv('lgb_weight_cv_submission.csv', index=False)","35364249":"a =model.feature_importance(importance_type='split')\nfeature = pd.DataFrame(model.feature_name())\nfeature['impo'] = a\nfeature = feature.sort_values(by = ['impo'], ascending = False)\nfeature.head(30)","0d709dd2":"my_sub.tail()","31a7c37e":"## Feature Engineering","7cbcfdcf":"## Training","045fa5ea":"#### Model 2: Using 80-20 split to train with the new non-default parameters","865090b7":"### Trip type specific features","80944716":"## Appending Train and test together for easy data manipulation","967ab233":"#### 1. Judging improvement in CV scores by adding any new features. This ensures that we are not directly jumping to Hyperparameters' tuning and first judging if the new features are even useful.","f31975c1":"### Learning which features were instrumental in training the above model.","cf385616":"### Logarithmic operation for non-normal features","2f20d98d":"### 2. Since competition's eval metric is accuracy, we will define this to train our LGBM model.","5c935086":"#### Model 3: Using Cross-Validation to improve the model's generalization performance. We will aggregate the results on test dataset for each fold and average it to make the final submission.","7b888981":"### Defining a runLGB() function that trains the model with specific hyperparameter as  its input."}}