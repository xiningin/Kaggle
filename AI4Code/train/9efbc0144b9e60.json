{"cell_type":{"92f7bc22":"code","b3e6f7a7":"code","85c7e204":"code","22265223":"code","ca74c206":"code","1be6b5bf":"code","5d75d534":"code","1a8e9e49":"code","68a8293f":"code","6506a784":"code","7249b4ef":"code","57617548":"code","a86daccc":"code","2ce40fcd":"code","d158ae8a":"code","e79007b0":"code","8d81db80":"code","011e67bd":"code","65eaaff9":"code","d8123ba4":"code","41439d82":"code","3097edb8":"markdown","89c8cb57":"markdown","e041f54b":"markdown","846400df":"markdown","eabe1cc8":"markdown","4565da3e":"markdown","91ce297e":"markdown","3682d10d":"markdown","20324d51":"markdown","5d549577":"markdown","2c352f23":"markdown","0d580ce1":"markdown","1bac66f1":"markdown","9a2d1080":"markdown","69b2db2a":"markdown","90fc0c8b":"markdown","7eef8d72":"markdown","78c73c10":"markdown","d9ed4dff":"markdown","b44fb129":"markdown","c6d8d015":"markdown","7858c232":"markdown"},"source":{"92f7bc22":"from IPython.display import clear_output\n!pip install pycaret --user\nclear_output()","b3e6f7a7":"import numpy as np\nimport pandas as pd\nimport pycaret\nfrom pycaret.classification import *\nfrom pycaret.datasets import get_data","85c7e204":"TARGET = \"Price\"\nSESSION_ID = 2021\nSEED = 42\nFRACTION = 0.9\nFEATURE = \"feature\"\nR2 = \"R2\"\nDATASET_NAME = \"diamond\"\nEXPERIMENT_NAME = 'diamond_prediction'","22265223":"dataset = get_data(DATASET_NAME)\ndataset.shape","ca74c206":"train = dataset.sample(frac=FRACTION, random_state=SEED)\ntest = dataset.drop(train.index)\n\ntrain.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\n\nprint('Data for Modeling: ' + str(train.shape))\nprint('Unseen Data For Predictions: ' + str(test.shape))","1be6b5bf":"from pycaret.regression import *\nsetup(data = train, \n      target = TARGET, \n      session_id=SESSION_ID, \n      experiment_name=EXPERIMENT_NAME,\n      silent=True)","5d75d534":"lightgbm = create_model('lightgbm')","1a8e9e49":"import numpy as np\nlgbm_params = {'num_leaves': np.arange(10,200,10),\n                        'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n                        'learning_rate': np.arange(0.1,1,0.1)}","68a8293f":"tuned_lightgbm = tune_model(lightgbm, custom_grid = lgbm_params)","6506a784":"print(tuned_lightgbm)","7249b4ef":"plot_model(tuned_lightgbm)","57617548":"plot_model(tuned_lightgbm, plot = 'error')","a86daccc":"plot_model(tuned_lightgbm, plot=FEATURE)","2ce40fcd":"evaluate_model(tuned_lightgbm)","d158ae8a":"predict_model(tuned_lightgbm);","e79007b0":"final_lightgbm = finalize_model(tuned_lightgbm)","8d81db80":"print(final_lightgbm)","011e67bd":"predict_model(final_lightgbm)","65eaaff9":"test_pred = predict_model(final_lightgbm, data=test)\ntest_pred.head()","d8123ba4":"from pycaret.utils import check_metric\ncheck_metric(test_pred.Price, test_pred.Label, R2)","41439d82":"save_model(final_lightgbm,'Final LightGBM Model 25Nov2020')\nsaved_final_lightgbm = load_model('Final LightGBM Model 25Nov2020')\nnew_prediction = predict_model(saved_final_lightgbm, data=test)\ndisplay(new_prediction.head())\nfrom pycaret.utils import check_metric\ncheck_metric(new_prediction.Price, new_prediction.Label, R2)","3097edb8":"# predict lightgbm finalized model","89c8cb57":"# fetch data set from remote orgin","e041f54b":"The predict_model() function is also used to predict on the unseen dataset. The only difference from section 11 above is that this time we will pass the data_unseen parameter.","846400df":"# global variables","eabe1cc8":"## Residual Plot","4565da3e":"# split train input data and train label data","91ce297e":"# predict lightgbm tunded model","3682d10d":"Model finalization is the last step in the experiment. A normal machine learning workflow in PyCaret starts with setup(), followed by comparing all models using compare_models() and shortlisting a few candidate models (based on the metric of interest) to perform several modeling techniques such as hyperparameter tuning, ensembling, stacking etc. This workflow will eventually lead you to the best model for use in making predictions on new and unseen data. The finalize_model() function fits the model onto the complete dataset including the test\/hold-out sample (30% in this case). The purpose of this function is to train the model on the complete dataset before it is deployed in production.","20324d51":"# predict test data using finalized lightgbm model","5d549577":"# tune lightgbm model","2c352f23":"# plot lightgbm model","0d580ce1":"## prediction error plot","1bac66f1":"# finalize lightgbm model","9a2d1080":"# split train set and test seta","69b2db2a":"# import libraries","90fc0c8b":"# install pycaret","7eef8d72":"# evaluate lightgbm model","78c73c10":"When a model is created using the create_model function it uses the default hyperparameters to train the model. In order to tune hyperparameters, the tune_model function is used. This function automatically tunes the hyperparameters of a model using Random Grid Search on a pre-defined search space. The output prints a score grid that shows MAE, MSE, RMSE, R2, RMSLE and MAPE by fold. To use the custom search grid, you can pass custom_grid parameter in the tune_model function ","d9ed4dff":"# check metric","b44fb129":"# save and load model and predict test data ","c6d8d015":"## feature importance ","7858c232":"# create lightgbm model"}}