{"cell_type":{"6e62bcd8":"code","c14a95ae":"code","ee84cc61":"code","236f6f2a":"code","20cab8f7":"code","47dc9058":"code","adfa9f83":"code","a20b0d92":"code","47e55937":"code","37ab83d2":"code","576680a2":"code","973ef32d":"code","9ab5b698":"code","3cd110b1":"code","2b820684":"code","8b133210":"code","0a6ebb9d":"code","23cd2460":"code","b24600bd":"code","eea61f07":"markdown","f07aec6b":"markdown","60376990":"markdown","42a0197f":"markdown","dd97c777":"markdown","81f8d1df":"markdown","1ab0acfa":"markdown","36c137da":"markdown"},"source":{"6e62bcd8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c14a95ae":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport keras\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport itertools\nfrom collections import Counter\nnp.random.seed(2)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\nfrom imblearn.over_sampling import SMOTE\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout","ee84cc61":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","236f6f2a":"# scaling amount and time\nscaler = StandardScaler()\ndata['scaled_amount'] = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))","20cab8f7":"#split data\ndata = data.drop(['Amount', 'Time'], axis = 1)\ny = data['Class']\nX = data.drop(['Class'], axis = 1)","47dc9058":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","adfa9f83":"#convert data for fitting to neural networks \ntrain_identity = X_train.index\ntest_identity = X_test.index\nX_train = np.array(X_train)\nX_test = np.array(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)","a20b0d92":"# Create deep learning model\nmodel = Sequential()\n#add input layer\nmodel.add(Dense(input_dim = 29, units = 16, activation = 'relu'))\n#add 2nd hidden layer\nmodel.add(Dense(units = 24, activation = 'relu'))\n#add dropout layer\nmodel.add(Dropout(0.5))\n#add 3rd hidden layer\nmodel.add(Dense(units = 20, activation = 'relu'))\n#add 4th hidden layer\nmodel.add(Dense(units = 24, activation = 'relu'))\n#add ouptut layer\nmodel.add(Dense(units = 1, activation = 'sigmoid'))","47e55937":"model.summary()","37ab83d2":"model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","576680a2":"model.fit(X_train, y_train, batch_size = 15, epochs = 5)","973ef32d":"#Evaluate model\nscore = model.evaluate(X_test, y_test)\nprint(score)","9ab5b698":"y_pred = model.predict(X_test)\ncm = confusion_matrix(y_test, y_pred.round())\nprint(cm)","3cd110b1":"print(accuracy_score(y_test, y_pred.round()))\nprint(precision_score(y_test, y_pred.round()))\nprint(recall_score(y_test, y_pred.round()))\nprint(f1_score(y_test, y_pred.round()))","2b820684":"X_resample, y_resample = SMOTE().fit_sample(X, y)\ncounter = Counter(y_resample)\nprint(counter)","8b133210":"X_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size = 0.3)\nX_train = np.array(X_train)\nX_test = np.array(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)","0a6ebb9d":"model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(X_train, y_train, batch_size = 15, epochs = 5)","23cd2460":"y_pred = model.predict(X_test)\ny_expect = pd.DataFrame(y_test)\ncm = confusion_matrix(y_expect, y_pred.round())\nprint(cm)\n","b24600bd":"print(accuracy_score(y_test, y_pred.round()))\nprint(precision_score(y_test, y_pred.round()))\nprint(recall_score(y_test, y_pred.round()))\nprint(f1_score(y_test, y_pred.round()))","eea61f07":"# Pre-processing the data ","f07aec6b":"# Loading the files and importing the libraries and the dataset","60376990":"After re-training the DNN model with SMOTE , we can see the metrics have improved and there's not much of a huge difference between the different metrics,much better than only DNN .","42a0197f":"We can see there is a vast difference between all the metrics, so now we can use a sampling method ,SMOTE.","dd97c777":"**My first notebook on credit-card fraud detection deals with the comparison of various models and can be found here [Model Comparison](https:\/\/www.kaggle.com\/muskan2006\/credit-card-fraud-detection-model-comparison?scriptVersionId=63546249). I haven't included any EDA in this notebook and it can be found in my first notebook. In this second notebook, I have experimented with DNNs and used SMOTE to handle the imbalance in the dataset**","81f8d1df":"# Handling the imbalance in the dataset using SMOTE","1ab0acfa":"We achieved an accuracy of 99.94% on training dataset without any sampling.","36c137da":"# Modeling"}}