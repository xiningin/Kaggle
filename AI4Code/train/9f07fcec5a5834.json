{"cell_type":{"d0705879":"code","b8c1f74a":"code","3796e047":"code","5c077f19":"code","ab0593d9":"code","6b7eaf5a":"code","1c111b21":"code","e4b40e51":"markdown","55f077de":"markdown","b540ed0b":"markdown","c62c51c6":"markdown"},"source":{"d0705879":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8c1f74a":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Module, Embedding, LSTM, Linear, NLLLoss, Dropout, CrossEntropyLoss\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchtext\nfrom torchtext import data\n\nimport gensim\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\nimport numpy as np\nimport os\nimport math\n\n\"\"\" Utility functions to load the data sets and preprocess the input \"\"\"\n\nTAG_INDICES = {'I-PER':0, 'B-PER':1, 'I-LOC':2, 'B-LOC':3, 'I-ORG':4, 'B-ORG':5, 'I-MISC':6, 'B-MISC':7, 'O':8}\n\ndef load_sentences(filepath):\n    \"\"\"\n    Load sentences (separated by newlines) from dataset\n\n    Parameters\n    ----------\n    filepath : str\n        path to corpus file\n\n    Returns\n    -------\n    List of sentences represented as dictionaries\n\n    \"\"\"\n    \n    sentences, tok, pos, chunk, ne = [], [], [], [], []\n\n    with open(filepath, 'r') as f:\n        for line in f.readlines():\n            if line == ('-DOCSTART- -X- -X- O\\n') or line == '\\n':\n               # Sentence as a sequence of tokens, POS, chunk and NE tags\n                sentence = dict({'TOKENS' : [], 'POS' : [], 'CHUNK_TAG' : [], 'NE' : [], 'SEQ' : []})\n                sentence['TOKENS'] = tok\n                sentence['POS'] = pos\n                sentence['CHUNK_TAG'] = chunk\n                sentence['NE'] = ne\n               \n                # Once a sentence is processed append it to the list of sentences\n                sentences.append(sentence)\n               \n                # Reset sentence information\n                tok = []\n                pos= []\n                chunk = []\n                ne = []\n            else:\n                l = line.split(' ')\n               \n                # Append info for next word\n                tok.append(l[0])\n                pos.append(l[1])\n                chunk.append(l[2])\n                ne.append(l[3].strip('\\n'))\n    \n    return sentences\n    \ndef read_conll_datasets(data_dir):\n    data = {}\n    for data_set in [\"train\",\"test\",\"valid\"]:\n        data[data_set] = load_sentences(\"%s\/%s.txt\" % (data_dir,data_set))\n    return data\n\ndef save_model(model, name, optimizer, loss):\n    \"\"\"\n    Print evaluation of saved model\n\n    Parameters\n    ----------\n    model : Model\n        BiLSTM model loaded from file.\n    name : String\n        File name.\n    \"\"\"\n    torch.save(model, name +'.pt')\n    torch.save(model.state_dict(), name + '_2.pt')\n    torch.save({\n    'model': model.state_dict(),\n    'state_dict': model.state_dict(),\n    'optimizer': optimizer.state_dict(),\n    'loss': loss.state_dict()\n}, name + '_state_dict.pt')\n    \n\ndef load_model(model_file, device):\n    \"\"\"\n    Load a model from a file and print evaluation\n\n    Parameters\n    ----------\n    model_file : String\n        Path to model file.\n    \"\"\"\n    m1 = torch.load(model_file, map_location=device)\n    return m1\n\ndef prepare_emb(sent, tags, words_to_ix, tags_to_ix):\n    w_idxs, tag_idxs = [], []\n    for w, t in zip(sent, tags):\n        if w.lower() in words_to_ix.keys():\n            w_idxs.append(words_to_ix[w.lower()])\n        else:\n            w_idxs.append(words_to_ix['unk'])\n                                      \n        if t in tags_to_ix.keys():\n            tag_idxs.append(tags_to_ix[t])\n        else:\n            tag_idxs.append(tags_to_ix['O'])\n            \n    return torch.tensor(w_idxs, dtype=torch.long), torch.tensor(tag_idxs, dtype=torch.long)\n","3796e047":"class Model(Module):\n    def __init__(self, pretrained_embeddings, hidden_size, vocab_size, n_classes):\n        super(Model, self).__init__()\n        \n        # Vocabulary size\n        self.vocab_size = pretrained_embeddings.shape[0]\n        # Embedding dimensionality\n        self.embedding_size = pretrained_embeddings.shape[1]\n        # Number of hidden units\n        self.hidden_size = hidden_size\n        \n        # Embedding layer\n        self.embedding = Embedding(self.vocab_size, self.embedding_size)\n        \n        # Dropout\n        #self.dropout = Dropout(p=0.5, inplace=False)\n        \n        # Hidden layer (300, 20)\n        self.lstm = LSTM(self.embedding_size, self.hidden_size, num_layers=2)\n        \n        # Final prediction layer\n        self.hidden2tag = Linear(self.hidden_size, n_classes)#, bias=True)\n    \n    def forward(self, x):\n        # Retrieve word embedding for input token\n        emb = self.embedding(x)\n        # Apply dropout\n        #dropout = self.dropout(emb)\n        # Hidden layer\n        h, _ = self.lstm(emb.view(len(x), 1, -1))#self.lstm(emb.view(len(x), 1, -1))\n        # Prediction\n        pred = self.hidden2tag(h.view(len(x), -1))\n        \n        return F.log_softmax(pred, dim=1)","5c077f19":"embeddings_file = '\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt'\ndata_dir = '\/kaggle\/input\/conll003-englishversion\/'\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Load data\ndataset = read_conll_datasets(data_dir)\n#gensim_embeds = gensim.models.KeyedVectors.load_word2vec_format(embeddings_file, binary=False)\nglove2word2vec(glove_input_file=embeddings_file, word2vec_output_file=\"gensim_glove_vectors.txt\")\ngensim_embeds = gensim.models.KeyedVectors.load_word2vec_format(\".\/gensim_glove_vectors.txt\", binary=False)\npretrained_embeds = gensim_embeds.vectors","ab0593d9":"# To convert words in the input to indices of the embeddings matrix:\nword_to_idx = {word: i for i, word in enumerate(gensim_embeds.vocab.keys())}\n    \n# Hyperparameters\n# Number of output classes (9)\nn_classes = len(TAG_INDICES)\n# Epochs\nn_epochs = 2\nreport_every = 1\nverbose = True\n    \n# Set up and initialize model\nmodel = Model(pretrained_embeds, 100, len(word_to_idx), n_classes)\nloss_function = NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.6)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), 5)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\nmodel.to(device)\n\n# Training loop\nfor e in range(n_epochs):\n    total_loss, num_words = 0, 0\n        \n    for sent in dataset[\"train\"][:10]:\n            \n            # (1) Set gradient to zero for new example: Set gradients to zero before pass\n            model.zero_grad()\n            \n            # (2) Encode sentence and tag sequence as sequences of indices\n            input_sent,  gold_tags = prepare_emb(sent[\"TOKENS\"], sent[\"NE\"], word_to_idx, TAG_INDICES)\n            num_words += len(sent[\"TOKENS\"])\n            \n            # (3) Predict tags (sentence by sentence)\n            if len(input_sent) > 0:\n                pred_scores = model(input_sent.to(device))\n                \n                # (4) Compute loss and do backward step\n                loss = loss_function(pred_scores.to(device), gold_tags.to(device))\n                loss.backward()\n              \n                # (5) Optimize parameter values\n                optimizer.step()\n          \n                # (6) Accumulate loss\n                total_loss += loss\n    if ((e+1) % report_every) == 0:\n            print('Epoch: %d, loss: %.4f' % (e, total_loss*100\/num_words))\n            \n# Save the trained model\n#save_model(model, 'm1_50e_dropout', optimizer, loss_function)","6b7eaf5a":"!\/opt\/conda\/bin\/python3.7 -m pip install --upgrade pip\n!pip install pytorch_lightning\nfrom pytorch_lightning.metrics.classification import F1\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# To convert words in the input to indices of the embeddings matrix:\nword_to_idx = {word: i for i, word in enumerate(gensim_embeds.vocab.keys())}\n\n# Metrics\nf1 = F1()\n\n# Evaluation on the test set\n\n# No dropout\nmodel_nodropout = load_model('\/kaggle\/input\/ner-trainedlstms\/m1_50e_nodropout.pt', device)\ncorrect, num_words, f1_scores, loss, num_sents = 0, 0, 0, 0, 0\nwith torch.no_grad():\n    for sent in dataset[\"test\"]:\n        \n        num_words += len(sent[\"TOKENS\"])\n        input_sent,  gold_tags = prepare_emb(sent[\"TOKENS\"], sent[\"NE\"], word_to_idx, TAG_INDICES)\n        \n        predicted, cor = 0.0, 0.0\n\n        # Predict class with the highest probability\n        if len(input_sent) > 0:\n            num_sents += 1\n            predicted = torch.argmax(model_nodropout(input_sent.to(device)), dim=1)\n            \n            correct += torch.sum(torch.eq(predicted.to(device),gold_tags.to(device)))\n            f1_scores += f1_score(predicted.cpu(), gold_tags.cpu(), average='weighted')\n\nprint('Evaluation of the word-based LSTM without dropout:')\nprint('----------------------------------------------------------')\nprint('Test set accuracy: %.2f' % (100.0 * correct \/ num_words))\nprint('Test set f1-score: %.2f' % (100 * f1_scores \/ num_sents))","1c111b21":"# Dropout\nmodel_dropout = load_model('\/kaggle\/input\/ner-trainedlstms\/m1_50e_dropout.pt', device)\ncorrect, num_words, f1_scores, num_sents = 0, 0, 0, 0\nwith torch.no_grad():\n    for sent in dataset[\"test\"]:\n        \n        num_words += len(sent[\"TOKENS\"])\n        input_sent,  gold_tags = prepare_emb(sent[\"TOKENS\"], sent[\"NE\"], word_to_idx, TAG_INDICES)\n        \n        predicted, cor = 0.0, 0.0\n\n        # Predict class with the highest probability\n        if len(input_sent) > 0:\n            num_sents += 1\n            predicted = torch.argmax(model_dropout(input_sent.to(device)), dim=1)\n            \n            correct += torch.sum(torch.eq(predicted.to(device),gold_tags.to(device)))\n            f1_scores += f1_score(predicted.cpu(), gold_tags.cpu(), average='weighted')\n\nprint('Evaluation of the word-based LSTM with dropout:')\nprint('----------------------------------------------------------')\nprint('Test set accuracy: %.2f' % (100.0 * correct \/ num_words))\nprint('Test set f1-score: %.2f' % (100 * f1_scores \/ num_sents))","e4b40e51":"**Model definition**\nPytorch bidirectional LSTM for NER using word-embeddings as input.","55f077de":"**Setup**\nLoad dependencies and provides some utility functions to load the data","b540ed0b":"****Evaluation****\nLoss and F1-scores on the test set.","c62c51c6":"***Training***"}}