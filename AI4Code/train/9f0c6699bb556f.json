{"cell_type":{"ab15237f":"code","9bdeef0d":"code","4008d244":"code","1b82bfee":"code","3a70bc24":"code","e45d78a3":"code","fe907173":"code","ee3ebe76":"code","f2fa5d1f":"code","8a03603e":"code","6a1fb69d":"code","b5d2f0ee":"code","824da812":"code","0a56c793":"code","26d0d541":"code","f96f6024":"code","16b6c82e":"code","ce6b1ef1":"code","95fabd29":"code","279da23b":"code","cde1b639":"code","3c8924de":"code","dee8b9a1":"code","3b7cce2d":"code","786706c0":"code","bed4503d":"code","d0fb45b5":"code","cffa9b22":"code","aaa09ffb":"code","07cd21d0":"code","46203406":"code","27bcdf65":"code","5406f897":"code","590eaa2c":"code","e776b73a":"code","4886a79d":"code","e00d975c":"code","b1aeced9":"markdown","0a4d3988":"markdown","ece8ec13":"markdown","1a673def":"markdown","62796964":"markdown","f4cf3860":"markdown","7d82a7e9":"markdown","87a999b1":"markdown","20604fea":"markdown","c98d6158":"markdown","6ffb299d":"markdown","039d2ad1":"markdown","9a8f79fa":"markdown","fea9b1f0":"markdown","683723f4":"markdown","7f760355":"markdown"},"source":{"ab15237f":"import gc\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt  \n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb","9bdeef0d":"INPUT_DIR = \"..\/input\/\"\nLABEL = 'winPlacePerc'","4008d244":"df_train = pd.read_csv(INPUT_DIR+'train_V2.csv')","1b82bfee":"# credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    return df","3a70bc24":"df_train = reduce_mem_usage(df_train)\ngc.collect()","e45d78a3":"df_train.dtypes","fe907173":"df_train.isnull().any()","ee3ebe76":"df_train = df_train.dropna()","f2fa5d1f":"# there are matches with no players or just one player, which is abnormal\ndf_train = df_train[df_train['maxPlace'] > 1]","8a03603e":"def FE(df,train=True):\n    LABEL = 'winPlacePerc'\n    \n    \n    # get label data\n    if train:\n        df_y = df.groupby(['matchId','groupId'])[LABEL].agg('mean')\n        ## now we can delete label and 'id' column to save GPU\n        df = df.drop([LABEL],axis=1)\n    else:\n        df_ids = df[['Id','matchId','groupId']]\n        \n    df = df.drop('Id',axis=1)\n    \n    # define group and match features\n    MATCH_FEATURE_part = ['numGroups','matchDuration','matchType','maxPlace']\n    \n    GROUP_FEATURE = df.columns.tolist()\n    GROUP_FEATURE.remove('groupId')\n    GROUP_FEATURE.remove('matchId')\n    for fe in MATCH_FEATURE_part:\n        GROUP_FEATURE.remove(fe)\n    # ATTENTION: here group feature doesn't include 'groupSize'\n    MATCH_FEATURE = MATCH_FEATURE_part + GROUP_FEATURE\n    MATCH_FEATURE.remove('matchType') \n    \n    # get group features\n    ## group size\n    dm = df.groupby(['matchId','groupId'])\n    df_X = dm.size().to_frame(name='groupSize') #df_X has indices: matchid, groupid\n    \n    ## other group features\n    gp = dm[GROUP_FEATURE].agg('max')\n    gp_rank = gp.reset_index().groupby(['matchId'])[GROUP_FEATURE].rank(pct=True).set_index(df_X.index)\n    df_X = df_X.join(gp).join(gp_rank,rsuffix='_max_rank') # join by indices\n    \n    gp = dm[GROUP_FEATURE].agg('min')\n    gp_rank = gp.reset_index().groupby(['matchId'])[GROUP_FEATURE].rank(pct=True).set_index(df_X.index)\n    df_X = df_X.join(gp,rsuffix='_min').join(gp_rank,rsuffix='_min_rank') # join by indices\n    \n    gp = dm[GROUP_FEATURE].agg('mean')\n    gp_rank = gp.reset_index().groupby(['matchId'])[GROUP_FEATURE].rank(pct=True).set_index(df_X.index)\n    df_X = df_X.join(gp,rsuffix='_mean').join(gp_rank,rsuffix='_mean_rank') # join by indices\n\n    #a variable called killPlace, it's already the rank in the match, so the _rank are all duplicates, so need to delete\n    df_X = df_X.drop(columns = ['killPlace_min_rank','killPlace_max_rank','killPlace_mean_rank'])\n    \n    # get match features except mactchType\n    dm = df.groupby(['matchId'])\n    df_X = df_X.join(dm[MATCH_FEATURE].agg('mean'),lsuffix='_max',rsuffix='_mean_match')\n    \n    # get matchType\n    df_X = df_X.join(pd.concat([pd.get_dummies(df.matchType),df[['matchId','groupId']]],axis=1).groupby(['matchId','groupId']).agg('min'))\n    \n    # prepare for output\n    if not train:\n        df_X_index = df_X.reset_index()[['matchId','groupId']]\n    \n    lst_features = list(df_X.columns)\n    \n    del df,dm,gp,gp_rank\n    gc.collect()\n    \n    if train:\n        return df_X,df_y,lst_features\n    else:\n        return df_X, df_X_index, df_ids\n    ","6a1fb69d":"X_train,y_train,lst_features = FE(df_train)","b5d2f0ee":"list(X_train.columns)","824da812":"X_train.shape, y_train.shape","0a56c793":"# X_train, X_vali, y_train, y_vali = train_test_split(X_train, y_train, test_size = 0.2, random_state = 1)","26d0d541":"# mean = X_train.mean(axis=0)\n# X_train -= mean\n# std = X_train.std(axis=0)\n# X_train \/= std\n\n# X_vali -= mean\n# X_vali \/= std","f96f6024":"# nn_model = Sequential()\n# nn_model.add(Dense(512,input_dim= X_train.shape[1], activation='relu'))\n# nn_model.add(Dropout(0.1))\n# nn_model.add(Dense(256, activation='relu'))\n# nn_model.add(Dropout(0.1))\n# nn_model.add(Dense(128, activation='relu'))\n# nn_model.add(Dropout(0.1))\n# nn_model.add(Dense(1,activation='linear')) \n\n# nn_model.compile(optimizer=optimizers.Adam(), loss='mse', metrics=['mae'])\n# nn_model.summary()","16b6c82e":"# history = nn_model.fit(X_train, y_train, \n#                  validation_data=(X_vali,y_vali),\n#                  epochs=10,\n#                  batch_size=10000,\n#                  verbose=1)\n# del X_train, y_train\n# gc.collect()","ce6b1ef1":"# from keras.models import load_model\n\n# nn_model.save('NN_model.h5')  # creates a HDF5 file","95fabd29":"# # Plot training & validation loss values\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('Model loss')\n# plt.ylabel('Loss')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Test'], loc='upper left')\n# plt.show()\n\n# # Plot training & validation mae values\n# plt.plot(history.history['mean_absolute_error'])\n# plt.plot(history.history['val_mean_absolute_error'])\n# plt.title('Mean Abosulte Error')\n# plt.ylabel('Mean absolute error')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Test'], loc='upper left')\n# plt.show()","279da23b":"# df_test = pd.read_csv(INPUT_DIR+'test_V2.csv')\n# df_test = reduce_mem_usage(df_test)\n# gc.collect()\n# X_test, X_test_index,df_test_ids = FE(df_test,train=False)\n# X_test.shape","cde1b639":"# del df_test\n# gc.collect()","3c8924de":"# # need to normailize for NN\n# X_test -= mean\n# X_test \/= std","dee8b9a1":"# pred = nn_model.predict(X_test)\n# pred = np.clip(pred, a_min=0, a_max=1)\n# df_pred = X_test_index.assign(winPlacePerc=pred)\n# result = pd.merge(df_test_ids, df_pred, how='left', on=['matchId', 'groupId'])","3b7cce2d":"# submission = result[['Id', LABEL]]\n# submission.to_csv('submission.csv', index=False)\n# submission.head()","786706c0":"# lgb_model = lgb.Booster(model_file='lgb_model.txt')  #init model","bed4503d":"del df_train\ngc.collect()","d0fb45b5":"df_test = pd.read_csv(INPUT_DIR+'test_V2.csv')\ndf_test = reduce_mem_usage(df_test)\ngc.collect()","cffa9b22":"X_test, X_test_index,df_test_ids = FE(df_test,train=False)","aaa09ffb":"# prep for interatoion on every fold; initialize the value\nfolds = KFold(n_splits=3,random_state=3)\nvali_pred = np.zeros(X_train.shape[0])\npred = np.zeros(X_test.shape[0]) #final pred\ndf_feature_importance = pd.DataFrame()\nvalid_score = 0","07cd21d0":"params = {\"objective\" : \"regression\", \"metric\" : \"mae\", 'n_estimators':20000, 'early_stopping_rounds':100,\n          \"num_leaves\" : 25, \"learning_rate\" : 0.05, \"bagging_fraction\" : 0.9, \"feature_fraction\":0.7,\n           \"bagging_seed\" : 0, \"num_threads\" : 4\n         }","46203406":"for n_fold, (train_idx, vali_idx) in enumerate(folds.split(X_train, y_train)): # for each fold\n    # split train data\n    X_train_fold, y_train_fold = X_train.iloc[train_idx], y_train[train_idx]\n    X_vali, y_vali = X_train.iloc[vali_idx], y_train[vali_idx]    \n    \n    #build model, fit\n    train_data = lgb.Dataset(data=X_train_fold, label=y_train_fold)\n    valid_data = lgb.Dataset(data=X_vali, label=y_vali)   \n    \n    lgb_model = lgb.train(params, train_data, valid_sets=[train_data, valid_data], verbose_eval=1000) \n    \n    #predict\n    pred_fold = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration) \n    pred_fold[pred_fold>1] = 1 \n    pred_fold[pred_fold<0] = 0 \n    pred += pred_fold\/ folds.n_splits\n    \n    #evaluate 1: check on the validation data\n    vali_pred[vali_idx] = lgb_model.predict(X_vali, num_iteration=lgb_model.best_iteration)\n    vali_pred[vali_pred>1] = 1\n    vali_pred[vali_pred<0] = 0\n    print('Fold %2d MAE : %.6f' % (n_fold + 1, mean_absolute_error(y_vali, vali_pred[vali_idx])))\n\n    #evaluation 2: check the importance of each feature\n    df_fold_importance = pd.DataFrame()\n    df_fold_importance = df_fold_importance.assign(feature= lst_features)\n    df_fold_importance = df_fold_importance.assign(importance = lgb_model.feature_importance())\n    df_fold_importance = df_fold_importance.assign(fold = n_fold + 1)\n    df_feature_importance = pd.concat([df_feature_importance, df_fold_importance])\n    \n    gc.collect()\n    ","27bcdf65":"print('Full mae score %.6f' % mean_absolute_error(y_train, vali_pred))","5406f897":"lgb_model.save_model('lgb_model.txt')","590eaa2c":"top_features = df_feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].reset_index()\n\nplt.figure(figsize=(14,10))\nsns.barplot(x=\"importance\", y=\"feature\", data=top_features)\nplt.title('LightGBM_Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('LightGBM_Importances.png')","e776b73a":"pred = np.clip(pred, a_min=0, a_max=1)\ndf_pred = X_test_index.assign(winPlacePerc=pred)\nresult = pd.merge(df_test_ids, df_pred, how='left', on=['matchId', 'groupId'])","4886a79d":"submission = result[['Id', 'winPlacePerc']]\nsubmission.to_csv('submission.csv', index=False)","e00d975c":"submission.head()","b1aeced9":"### Build the model","0a4d3988":"### prep","ece8ec13":"# Feature Engineering","1a673def":"### Feature-wise normalization","62796964":"## LightGBM","f4cf3860":"## Data type, and bad value ","7d82a7e9":"### Make Prediction","87a999b1":"\n### Visualization","20604fea":"# NN using Keras","c98d6158":"### The final placement depends on the performance of the group and the match dynamics, not individuals.  So first, get all the group and match features.\n\nfrom group:\n- group size. More people, more chance to rank high\n- the summary of group performace: the max,mean,min of each variant, except of id, groupId, matchId, numGroup, match duration, martch type, maxPlace\n\nfrom match:\n- **mean, max, and min of group features except group size**\n- numGroup. The smaller, the better\n- match duration\n- match type\n- maxPlace","6ffb299d":"# EDA","039d2ad1":"### define model","9a8f79fa":"### Split train data to train and validation","fea9b1f0":"### Make prediction Using NN","683723f4":"There are missing values. ","7f760355":"# Load train data, and reduce memory"}}