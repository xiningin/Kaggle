{"cell_type":{"a18f7392":"code","848629ce":"code","35f1ab0d":"code","cdbd6c53":"code","b744ca7f":"code","8a034469":"code","c60c7e47":"code","9fab77ac":"code","6f62dbd3":"code","e1b25d92":"code","9ef5163e":"code","7e50c6b7":"code","8fc9cae4":"code","caa3ee05":"code","3e6dbe76":"code","1e2faeef":"code","654bccff":"code","5885dd43":"code","0b717810":"code","9e9a4434":"code","f1227a90":"code","6e812fd6":"code","a1f7e5e1":"code","618de407":"code","d08c6bb7":"code","bbca4701":"code","22c90948":"code","ed762f6a":"code","8eafde61":"code","0fd19f93":"code","dc8d7a4f":"code","30b7078b":"code","e2cbf926":"code","2679915b":"code","cf4dbf64":"code","0cdaf064":"code","4870072b":"code","bbbdbebe":"code","6b7b5fbf":"code","7b57c6c2":"code","71812690":"code","c5c86b96":"code","1d9b31e1":"code","4d081020":"code","101f9596":"code","a1f4e7b9":"code","99c4e438":"code","64014ad5":"code","11928d3c":"code","52ca84d1":"code","d6faa849":"markdown","679729ea":"markdown","14128d6f":"markdown","2de6cbc0":"markdown","ad449c5e":"markdown","85fbe0a6":"markdown","c0b2ca25":"markdown","8a9581ef":"markdown","b4637ce6":"markdown","63aeb885":"markdown","a0d3957c":"markdown","2797d582":"markdown","b6a9b14f":"markdown","5d7216d5":"markdown","7b2c8fb5":"markdown","3d7cbdbf":"markdown","5326c0e5":"markdown","94ae8f6f":"markdown","4f77693e":"markdown","244862c9":"markdown","6e13edce":"markdown","2ec02679":"markdown","97f9f1be":"markdown","6865c64f":"markdown","06e11d1d":"markdown","90652b0a":"markdown","1651526e":"markdown","73e2ba69":"markdown","b4b441d5":"markdown","bb15d0c3":"markdown","a15120bd":"markdown","d60b6623":"markdown","732d8061":"markdown","0123459b":"markdown"},"source":{"a18f7392":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","848629ce":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.offline as po\nimport plotly.graph_objs as go\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","35f1ab0d":"df = pd.read_csv(\"..\/input\/bank-customer-churn-modeling\/Churn_Modelling.csv\")\ndf.drop(['RowNumber','CustomerId'], axis=1, inplace=True)\ndf.head()","cdbd6c53":"df.isnull().sum()","b744ca7f":"df.dtypes","8a034469":"df['Exited'].value_counts()","c60c7e47":"sns.countplot(df['Exited'])","9fab77ac":"df['Exited'].value_counts().keys().tolist(), df['Exited'].value_counts().values","6f62dbd3":"# visulaize the Exited data using plotly\nplot_by_exited_labels = df['Exited'].value_counts().keys().tolist()\nplot_by_exited_values = df['Exited'].value_counts().values.tolist()\n\ndata = [\n    go.Pie(labels=plot_by_exited_labels,\n          values=plot_by_exited_values,\n          hole=.6)\n]\n\nplot_layout = go.Layout(dict(title=\"Customer Churn\"))\n\nfig = go.Figure(data=data, layout=plot_layout)\npo.iplot(fig)","e1b25d92":"df.groupby(\"Exited\").mean()","9ef5163e":"categorical_feat = ['Geography','Tenure','NumOfProducts',\"HasCrCard\",\"HasCrCard\",\"IsActiveMember\"]\nfor cat in categorical_feat:\n    print(cat,\" : \")\n    print(df[cat].value_counts())\n    sns.countplot(df[cat])\n    plt.show() ","7e50c6b7":"print(df['Gender'].value_counts()\/df.shape[0])\nsns.countplot(df['Gender'])\nplt.show() ","8fc9cae4":"# Gender vs Exited\n# total percentage exited in by each gender\nplot_by_gender = df.groupby('Gender')['Exited'].mean().reset_index()\nprint(plot_by_gender)\n\nplot_data = [\n    go.Bar(\n    x = plot_by_gender['Gender'],\n    y = plot_by_gender['Exited'])\n]\nplot_layout = go.Layout(dict(title=\"% of Exited customers in each gender\"))\nfig = go.Figure(data = plot_data, layout=plot_layout)\npo.iplot(fig)","caa3ee05":"print(df['Geography'].value_counts())\nsns.countplot(df['Geography'])\nplt.show() ","3e6dbe76":"# Geography vs Exited in percentage\n# total percentage of Exited customer percentage in each Geography\nplot_by_geo = df.groupby('Geography')['Exited'].mean().reset_index()\nprint(plot_by_geo)\n\nplot_data = [\n    go.Bar(\n    x = plot_by_geo['Geography'],\n    y = plot_by_geo[\"Exited\"])\n]\nplot_layout = go.Layout(dict(title=\"Percentage of CUstomer Exited in each Geography\"))\nfig = go.Figure(data=plot_data, layout=plot_layout)\npo.iplot(fig)","1e2faeef":"print(df['NumOfProducts'].value_counts())\nsns.countplot(df['NumOfProducts'])","654bccff":"df['NumOfProducts'].value_counts()\/df.shape[0]*100","5885dd43":"sns.countplot(df['NumOfProducts'],hue=df['Exited'])","0b717810":"pd.crosstab(index=df['Exited'], columns=df['NumOfProducts'])","9e9a4434":"# lets draw % of Exited Customer along with NumOfProducts used\nplot_by_numOfProducts = df.groupby(\"NumOfProducts\")[\"Exited\"].mean().reset_index()\nprint(plot_by_numOfProducts)\n\nplot_data = [\n    go.Bar(\n    x = plot_by_numOfProducts[\"NumOfProducts\"],\n    y = plot_by_numOfProducts[\"Exited\"])\n]\nplot_layout = go.Layout(dict(title = \"% Exited customers with NumOfProducts used\"))\nfig = go.Figure(data=plot_data,\n               layout=plot_layout)\npo.iplot(fig)","f1227a90":"print(df['HasCrCard'].value_counts())\nsns.countplot(df['HasCrCard'], hue=df['Exited'])","6e812fd6":"# lets plot % of Exited customer with HasCrCard\nplot_with_hasCrCard = df.groupby(\"HasCrCard\")[\"Exited\"].mean().reset_index()\nprint(plot_with_hasCrCard)\n\nplot_data = [\n    go.Bar(\n    x=plot_with_hasCrCard['HasCrCard'],\n    y= plot_with_hasCrCard['Exited'])\n]\nplot_layout = go.Layout(dict(title=\"% Exited customer with HasCrCard\"))\nfig = go.Figure(data=plot_data, layout=plot_layout)\npo.iplot(fig)","a1f7e5e1":"# Balance\nfor i in [0,1]:\n    sns.distplot(df[df['Exited']==i]['Balance'])\n    plt.show()","618de407":"df.corr()","d08c6bb7":"plt.figure(figsize=(9,9))\nsns.heatmap(df.corr(), annot=True)","bbca4701":"df.drop('Surname', axis=1, inplace=True)\ndf.head()","22c90948":"df = pd.get_dummies(data=df, columns=['Geography','Gender','HasCrCard','IsActiveMember'])\ndf.head()","ed762f6a":"x = df.drop('Exited', axis=1)\ny = df['Exited']","8eafde61":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx[['CreditScore','Age','Tenure','Balance','EstimatedSalary','NumOfProducts']] = scaler.fit_transform(x[['CreditScore','Age','Tenure','Balance','EstimatedSalary','NumOfProducts']])","0fd19f93":"x.head()","dc8d7a4f":"from sklearn.model_selection import train_test_split\nfrom collections import Counter\n\nx_train, x_test,y_train, y_test = train_test_split(x, y, test_size=0.2)\nx_train.shape, x_test.shape, Counter(y_train), Counter(y_test)","30b7078b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve","e2cbf926":"def generated_report(y_actual, y_pred):\n    print(\"Accuracy : \", accuracy_score(y_actual, y_pred))\n    print(classification_report(y_actual, y_pred))\n    \ndef generated_roc_auc_curve(model, x_test):\n    y_pred_proba = model.predict_proba(x_test)[:, 1]\n    fpr, tpr, thresh = roc_curve(y_test, y_pred_proba)\n    auc = roc_auc_score(y_test,  y_pred_proba)\n    plt.plot(fpr, tpr, label='AUC: '+str(auc))\n    plt.legend()\n    plt.show()\n    \ndef Log_Reg_Model():\n    lr = LogisticRegression()\n    lr.fit(x_train, y_train)\n    y_pred = lr.predict(x_test)\n    generated_report(y_test, y_pred)\n    generated_roc_auc_curve(lr, x_test)","2679915b":"Log_Reg_Model()","cf4dbf64":"from sklearn.linear_model import LogisticRegression\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter","0cdaf064":"sampler = RandomUnderSampler(sampling_strategy=1, replacement=False)\nx_new, y_new = sampler.fit_resample(x, y)\n\nCounter(y_new), x_new.shape","4870072b":"x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, stratify=y_new)\nCounter(y_train), Counter(y_test)","bbbdbebe":"Log_Reg_Model()","6b7b5fbf":"from imblearn.over_sampling import RandomOverSampler\n\nx_new, y_new = RandomOverSampler(sampling_strategy=0.8).fit_resample(x, y)\nCounter(y_new)","7b57c6c2":"x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, stratify=y_new)","71812690":"Log_Reg_Model()","c5c86b96":"from imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\n\nsmote = SMOTE(sampling_strategy=0.87)\nx_new, y_new = smote.fit_resample(x, y)\n\nprint(Counter(y))\nprint(Counter(y_new))\n\nx_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, stratify=y_new)\nCounter(y_train), Counter(y_test)","1d9b31e1":"Log_Reg_Model()","4d081020":"from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom collections import Counter\nfrom sklearn.linear_model import LogisticRegression\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\npipeline = Pipeline(steps=[\n    (\"over\",SMOTE(sampling_strategy=0.6)),\n    (\"under\", RandomUnderSampler(sampling_strategy=0.9))\n])","101f9596":"print(\"Before: \", Counter(y))\n\nx_new, y_new = pipeline.fit_resample(x, y)\nprint(\"After: \",Counter(y_new))","a1f4e7b9":"x_new.shape, y_new.shape","99c4e438":"# again\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\npipeline = Pipeline(steps=[\n    (\"over\",SMOTE(sampling_strategy=0.6)),\n    (\"under\", RandomUnderSampler(sampling_strategy=0.9)), \n    (\"model\", LogisticRegression())\n])\n\n#evaluate pipeline\nscores = cross_val_score(pipeline, x, y, scoring='roc_auc', cv=cv)\nprint(scores)\nprint(\"mean roc auc: \", np.mean(scores))","64014ad5":"pipeline = Pipeline(steps=[\n    (\"over\",SMOTE(sampling_strategy=0.6)),\n    (\"under\", RandomUnderSampler(sampling_strategy=0.9))\n])\n\nx_new, y_new = pipeline.fit_resample(x, y)\nx_new.shape, y_new.shape","11928d3c":"x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, stratify=y_new)\nCounter(y_train), Counter(y_test)","52ca84d1":"Log_Reg_Model()","d6faa849":"## UnderSampling and OverSampling","679729ea":"we can see that less then 1 % of customer has used all 4 products.","14128d6f":"### Scaling","2de6cbc0":"## RandomOverSampling","ad449c5e":"from above, customers with no credit card are more on leaving the business.","85fbe0a6":"### HasCrCard","c0b2ca25":"### splitting","8a9581ef":"We can see that our score has increased, but using SMOTE can be misleading as we are generating systhetic data.","b4637ce6":"## RandomUnderSampling","63aeb885":"we can see its score is overall better then the previous one  without balancing our data but in case of undersampling there will be alot of data still not used for making the model that causes info loss and our model cannot generalize the result well in case of future prediction","a0d3957c":"## SMOTE","2797d582":"OUr accuracy has little decreased but some score have definitely increased cause we are not just looking for accuracy score.","b6a9b14f":"As we can see both the exited and non exited customer are having show how same distribution of data.","5d7216d5":"## Modelling","7b2c8fb5":"### NumOfProducts","3d7cbdbf":"### Thank you\n#### The is my first notebook in kaggle.\n### Upvote is appreciated","5326c0e5":"As we can see using the over and under sampling combined has given pretty good result of accuracy then all other sampling technique that we have used before.\n\nThe significane of using this combined method is that using over sampling will help the generation of synthetic data to the less cateogry data and under sampling will under sample the category data that is more in number according to the sampling_strategy paramter speficied due to which helps to balance the data from being bias to one category due to lack of data and also add some more synthetic data to the sample when may have biasness if we have used RandomOverSampler.","94ae8f6f":"We don't see much of the difference between the Exited customer between both category as both have almost equal number of exited customer so we won't get  much of the impact from this feature to our model.","4f77693e":"### Geography","244862c9":"## What is the meaning of churn prediction?\nIt the technique using which we can predict or detect the customers who  are likely to cancel their subscription in the near future. \n\nThere are 3 ways using which businesses can generate revenue:\n1. Upsell to existing customers.\n2. Acquire new customers. \n3. Increase a customer retention.\n\nChurn prediction helps in focusing in the third point i.e. increase a customer retention in which we can predict the churn rate or those customers who are going to cancel the subscription and then take pro-active action before they leave so that they stay in business.\n\n### why we are doing this project?\nIn this project we will be creating a model using the bank past churn data of the cutomers and predict the future churn rate of customers so that we can know which cateogry of customer has has high churn rate and take a pro-active action inorder to make them stay in business.\n\n### What is churn rate?\nAccoring to wikipedia, The churn rate is the rate at which customers stop doing business with an entity. It is most commonly expressed as the percentage of service subscribers who discontinue their subscriptions within a given time period.","6e13edce":"We can see most of the customers has used only one product, and very less customer has used 4 of the bank product.","2ec02679":"Above is the scores without doing any of the sampling methods. We cannot just rely on accuracy score as it can be misleading sometimes so we need to look for the values of other socres like f1-score, precision, etc.","97f9f1be":"### Gender","6865c64f":"we can see that our data is imbalanced.","06e11d1d":"## EDA\n\n#### Exited","90652b0a":"We can also see from the above means of different feature that is they are having significant impact in the churn data.","1651526e":"## Without using sampling methods","73e2ba69":"out of 3 countries provided, France has highest churn rate ","b4b441d5":"we can conclude that all the customers that have used all the 4 products have left which is 1 i.e. 100%.","bb15d0c3":"### Correlation","a15120bd":"As we can see our data is imbalanced.","d60b6623":"out of total, almost 55% of them are male and 45% are female","732d8061":"### Categorical features","0123459b":"### Balance"}}