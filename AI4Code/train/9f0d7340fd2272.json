{"cell_type":{"17d8b3db":"code","7751968e":"code","57547ea1":"code","c37455a7":"code","68d5541f":"code","91831390":"code","d7d326b8":"code","7b0368b8":"code","f6b931bf":"code","010f65f1":"code","1b9efc1f":"code","80da36aa":"code","f36509b6":"code","dc78dc98":"code","16dfe606":"code","a811c67b":"code","f59d1e84":"code","32b6c335":"code","2e411c2b":"code","84a761cd":"code","aca96608":"markdown","91c00307":"markdown","0e88e168":"markdown","e22df9d1":"markdown","5b891507":"markdown","7f4d3704":"markdown","1ab1f3d1":"markdown","035ad734":"markdown","be4e51dc":"markdown"},"source":{"17d8b3db":"#importing libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os\n\n\nimport keras\nfrom keras.preprocessing import image\nfrom keras import Sequential\nfrom keras.layers import *\nfrom keras.optimizers import Adam\nfrom keras.models import load_model\n\n","7751968e":"# Train and test directory \ntrain_directory = '\/kaggle\/input\/fruits\/fruits-360\/Training'\ntest_directory = '\/kaggle\/input\/fruits\/fruits-360\/Test'","57547ea1":"#opening a sample image\npath = \"\/kaggle\/input\/fruits\/fruits-360\/Training\/Apple Golden 1\/15_100.jpg\"\nimg = image.load_img(path)\nimg_array = image.img_to_array(img)\/255.0\nplt.imshow(img_array)\nprint(img_array.shape)","c37455a7":"train_gen = image.ImageDataGenerator(\n    rescale = 1.0\/255,              # rescaling image from 0-255 to 0-1\n    rotation_range = 40,            # 40 degree of random rotation\n#     zoom_range = 0.3,             # not using zoom as zooming might make  classification difficult\n    horizontal_flip = True,\n    vertical_flip=True\n)\nval_gen = image.ImageDataGenerator(rescale=1.0\/255)  #just rescaling on test data","68d5541f":"\n# batch size : the images from directory will be provided to model in the batches of 32\nbatch_size = 32","91831390":"train_generator = train_gen.flow_from_directory(\n    train_directory,                 # training data directory\n    target_size = (100, 100),        # target size of the images which will be fed into the model\n    batch_size = batch_size,         # no. of images send for 1 epoch to the model\n    class_mode = \"categorical\",      # as there are 131 classes \n)\n\nval_generator = val_gen.flow_from_directory(\n    test_directory,                  # test data directory\n    target_size = (100, 100),\n    batch_size = batch_size,\n    class_mode = \"categorical\",\n)","d7d326b8":"X,y = train_generator.next()\nprint(len(X), len(y), X.shape)  # See the generator is sending images and labels in the specified batch_size ie 32","7b0368b8":"# some images of our augmented dat\nfor i in np.arange(5):\n    plt.imshow(X[i])\n    plt.show()","f6b931bf":"#my model architecture: \n\n# 5 CNN layers  with pooling  , with relu activation\n# and finally flattened vector is sent to fully connected dense layer ie the last layer \n# with 131  neurons and with softmax activation function\n\n\nmodel = Sequential()\nmodel.add(Conv2D(filters = 16,kernel_size=(2,2), input_shape=(100, 100, 3), activation= 'relu', padding='same'))\nmodel.add(MaxPooling2D(pool_size=2))   #used padding ='same' so that edges wont be cut so wont lose any features from there\n\nmodel.add(Conv2D(filters = 32, kernel_size=(2,2) ,activation= 'relu',padding='same'))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Conv2D(filters = 64, kernel_size=(2,2),activation= 'relu',padding='same'))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Conv2D(filters = 128, kernel_size=(2,2),activation= 'relu',padding='same'))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Conv2D(filters = 256, kernel_size=(2,2) ,activation= 'relu',padding='same'))\nmodel.add(MaxPooling2D(pool_size=2))\n\n\nmodel.add(Flatten())   # to flatten the matrix to a 1D vector\nmodel.add(Dropout(0.4)) # To add regularization\nmodel.add(Dense(131, activation='softmax'))\nmodel.summary()","010f65f1":"#Compiling models with\n# loss as categorical crossentropy as their are 130 classes\n# Adam as optimizer\n# metric as Accuracy\n# model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","1b9efc1f":"# Training the model using fit_generator otherwise it will give a huge load to RAM if all the images are converted into data matrix at the same time.\nhist  = model.fit_generator(\n    train_generator,                # training generator\n    epochs = 7,\n    validation_data= val_generator, # validation generator\n    verbose=2,\n)","80da36aa":"model.save(\"model_fin.h5\")\nprint(\"Model saved at\", os.getcwd())","f36509b6":"#Plot for accuracy \nplt.plot(hist.history['accuracy'])  \nplt.plot(hist.history['val_accuracy'])  \nplt.title('MODEL_Accuracy')  \nplt.ylabel('ACCURACY')  \nplt.xlabel('EPOCHS')  \nplt.legend(['train_acc', 'test_acc'], loc='lower right')  \nplt.figure()   \n  \n\n# Plot for loss   \nplt.plot(hist.history['loss'])  \nplt.plot(hist.history['val_loss'])  \nplt.title('MODEL loss')  \nplt.ylabel('LOSS')  \nplt.xlabel('EPOCHS')  \nplt.legend(['train_loss', 'test_loss'], loc='upper right')  \nplt.show()\n","dc78dc98":"# taking a batch of 32 images from generator\nX_test,Y_test=val_generator.next()\ny_pred = model.predict(X_test)     # Predicted labels \n\npred_labels = y_pred.argmax(axis=1)   # output is a sparse vector having one at the most close predicted label index                           \ntrue_labels = Y_test.argmax(axis=1)   # axis =1 means argmax is calculated along with the row ,(columns collapsed)\n\nprint( np.sum(pred_labels==true_labels),\"\/\",len(pred_labels), \" fraction of corrected predictions over total predictions\")","16dfe606":"# Checking our model on test images \n# Title printed in green color if correctly predicted and in red if it is predicted wrong\nfor i in np.arange(len(X_test)):\n    plt.figure(figsize=(2,2))\n    plt.title(\"Predicted label : \" + str(pred_labels[i]), color=(\"green\" if pred_labels[i] == true_labels[i] else \"red\")) \n    plt.imshow(X_test[i])\n    plt.show()\n    ","a811c67b":"print( np.sum(pred_labels==true_labels),\"\/\",len(pred_labels),\" correctly predicted\")","f59d1e84":"#loaded saved model \nloaded_model = load_model(\"model_fin.h5\")\nloaded_model.summary()","32b6c335":"# Evaluating it on test set\nloaded_model.evaluate(X_test, Y_test) # getting same predictions , thus successfully saved and loaded","2e411c2b":"model.evaluate(X_test, Y_test)","84a761cd":"# Thank you .","aca96608":"## Results","91c00307":"## Data Augmentation","0e88e168":"## Importing Libraries","e22df9d1":"## Important Note:\n* If val loss fluctuates which can be due to overfitting . In this case , we have added  a dropout layer for regularization but still it is fluctuating which is dependending on learning rate as after reaching near global minima it is fluctuating there and not fully converging because learning rate is high when near global minima.\n* increasing batch size and reducing learning rate can reduce val loss.\n* It can be solved by reducing learning rate as it come close to the global minima.\n* While you train for large epochs , you can use early stopping ,callbacks and model checkpoints to automatically save your model when val_accuracy starts continuosly decreasing.","5b891507":"## Loading Train Test Data","7f4d3704":"### I got fairly high Validation_Acc so loss wont matter much, increasing epochs may solve this val_loss convergence prob.","1ab1f3d1":"## Model Architecture","035ad734":"## Plotting Accuracy and Loss wrt Epochs:","be4e51dc":"## MODEL PREDICTIONS:\n\n\n\n"}}