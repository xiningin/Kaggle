{"cell_type":{"08a3c543":"code","2756f580":"code","28c7931f":"code","79f99b75":"code","80824f68":"code","31dd0116":"code","2bacfceb":"code","74aee3f6":"code","5e27b7f7":"code","e0bb7647":"code","0b8ad322":"code","5ca1a397":"code","68eb7a6f":"code","3a9f6bb9":"code","8dbb8e90":"code","8eb5d6dd":"code","eaad7d43":"code","06093f34":"code","c9123e47":"code","43810c1c":"code","deed4f3f":"code","122e9a85":"code","308e6fb4":"code","5525833d":"code","5b8421ea":"code","1531e950":"code","94a36aff":"code","c2a3d39c":"code","55baada3":"code","ac9b5b36":"code","d8b8365a":"code","f702fbee":"code","17f094e3":"code","16b9c51d":"code","b919da06":"code","456d8726":"code","febbc48f":"code","b2474655":"markdown","a5777417":"markdown","b655642e":"markdown","5f61a870":"markdown","e5db1460":"markdown","7d419aeb":"markdown","525f30b1":"markdown"},"source":{"08a3c543":"import pandas as pd \nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint('Modules are imported.')","2756f580":"df=pd.read_csv('\/kaggle\/input\/life-expectancy-who\/Life Expectancy Data.csv')\ndf.head()","28c7931f":"#make country column as the Index of the df\ndf.set_index(\"Country\",inplace= True)\ndf.head()","79f99b75":"df.shape","80824f68":"df['Year'].unique","31dd0116":"df.columns","2bacfceb":"#drop columns \nuseless_col = ['percentage expenditure','Hepatitis B','Status',\n       'Measles ', ' BMI ', 'under-five deaths ', 'Polio', 'Total expenditure',\n       'Diphtheria ', ' HIV\/AIDS', 'Population',\n       ' thinness  1-19 years', ' thinness 5-9 years',\n       'Income composition of resources']\ndf.drop(useless_col,axis=1,inplace=True)\n","74aee3f6":"df.head()","5e27b7f7":"df.info()","e0bb7647":"# find all the null values in df\ndf.isnull().sum() ","0b8ad322":"df.Alcohol.value_counts() ","5ca1a397":"#replace alcohol null values with the most recurrent number\ndf.Alcohol.fillna(0.01,inplace=True)","68eb7a6f":"#replace GDP null values with the median\nb=df.GDP.median()\ndf.GDP.fillna(b,inplace=True)","3a9f6bb9":"#the rest drop the null values\ndf.dropna(inplace=True)","8dbb8e90":"df.isnull().sum() ","8eb5d6dd":"#correlation scores\ndf.corr()","eaad7d43":"#correlation depicted in heatmap\nsns.heatmap(df.corr())","06093f34":"#a scatterplot overview of the correlations between features\nsns.pairplot(df)","c9123e47":"X = df.values[:, 2:7]  #  input values from last 5 columns\ny = df.values[:, 1]  # get output values 2nd coulmn(life_exp)\nm = len(y) # Number of training examples\n\nprint('Total no of training examples (m) = %s \\n' %(m))\n\n# Show only first 5 records\nfor i in range(5):\n    print('x =', X[i, ], ', y =', y[i])","43810c1c":"def feature_normalize(X):\n  \"\"\"\n    Normalizes the features(input variables) in X.\n\n    Parameters\n    ----------\n    X : n dimensional array (matrix), shape (n_samples, n_features)\n        Features(input varibale) to be normalized.\n\n    Returns\n    -------\n    X_norm : n dimensional array (matrix), shape (n_samples, n_features)\n        A normalized version of X.\n    mu : n dimensional array (matrix), shape (n_features,)\n        The mean value.\n    sigma : n dimensional array (matrix), shape (n_features,)\n        The standard deviation.\n  \"\"\"\n  # mean of column, hence axis = 0\n  mu = np.mean(X, axis = 0)  \n  # Notice the parameter ddof (Delta Degrees of Freedom)  value is 1\n  sigma = np.std(X, axis= 0, ddof = 1)  # Standard deviation (can also use range)\n  X_norm = (X - mu)\/sigma\n  return X_norm, mu, sigma","deed4f3f":"X, mu, sigma = feature_normalize(X)\n\nprint('mu= ', mu)\nprint('sigma= ', sigma)\nprint('X_norm= ', X[:5])","122e9a85":"#New mean or avearage value of normalized X feature is 0\nmu_testing = np.mean(X, axis = 0) # mean\nmu_testing","308e6fb4":"#New range or standard deviation of normalized X feature is 1\nsigma_testing = np.std(X, axis = 0, ddof = 1) # mean\nsigma_testing","5525833d":"# Lets use hstack() function from numpy to add column of ones to X feature \n# This will be our final X matrix (feature matrix)\nX = np.hstack((np.ones((m,1)), X))\nX[:5]","5b8421ea":"from sklearn.model_selection import train_test_split\nX, x_test, y, y_test = train_test_split(X,y,test_size =0.2)","1531e950":"X[1]","94a36aff":"x_test[1]","c2a3d39c":"def compute_cost(X, y, theta):\n  \"\"\"\n  Compute the cost of a particular choice of theta for linear regression.\n\n  Input Parameters\n  ----------------\n  X : 2D array where each row represent the training example and each column represent the feature ndarray. Dimension(m x n)\n      m= number of training examples\n      n= number of features (including X_0 column of ones)\n  y : 1D array of labels\/target value for each traing example. dimension(1 x m)\n\n  theta : 1D array of fitting parameters or weights. Dimension (1 x n)\n\n  Output Parameters\n  -----------------\n  J : Scalar value.\n  \"\"\"\n  predictions = X.dot(theta)\n  #print('predictions= ', predictions[:5])\n  errors = np.subtract(predictions, y)\n  #print('errors= ', errors[:5]) \n  sqrErrors = np.square(errors)\n  #print('sqrErrors= ', sqrErrors[:5]) \n  #J = 1 \/ (2 * m) * np.sum(sqrErrors)\n  # OR\n  # We can merge 'square' and 'sum' into one by taking the transpose of matrix 'errors' and taking dot product with itself\n  # If your confuse about this try to do this with few values for better understanding  \n  J = 1\/(2 * m) * errors.T.dot(errors)\n\n  return J","55baada3":"def gradient_descent(X, y, theta, alpha, iterations):\n  \"\"\"\n  Compute cost for linear regression.\n\n  Input Parameters\n  ----------------\n  X : 2D array where each row represent the training example and each column represent the feature ndarray. Dimension(m x n)\n      m= number of training examples\n      n= number of features (including X_0 column of ones)\n  y : 1D array of labels\/target value for each traing example. dimension(m x 1)\n  theta : 1D array of fitting parameters or weights. Dimension (1 x n)\n  alpha : Learning rate. Scalar value\n  iterations: No of iterations. Scalar value. \n\n  Output Parameters\n  -----------------\n  theta : Final Value. 1D array of fitting parameters or weights. Dimension (1 x n)\n  cost_history: Conatins value of cost for each iteration. 1D array. Dimansion(m x 1)\n  \"\"\"\n  cost_history = np.zeros(iterations)\n\n  for i in range(iterations):\n    predictions = X.dot(theta)\n    #print('predictions= ', predictions[:5])\n    errors = np.subtract(predictions, y)\n    #print('errors= ', errors[:5])\n    sum_delta = (alpha \/ m) * X.transpose().dot(errors);\n    #print('sum_delta= ', sum_delta[:5])\n    theta = theta - sum_delta;\n\n    cost_history[i] = compute_cost(X, y, theta)  \n\n  return theta, cost_history","ac9b5b36":"# We need theta parameter for every input variable. since we have 6 input variable including X_0 (column of ones)\ntheta = np.zeros(6)\niterations = 400;\nalpha = 0.15;","d8b8365a":"theta, cost_history = gradient_descent(X, y, theta, alpha, iterations)\nprint('Final value of theta =', theta)\nprint('First 5 values from cost_history =', cost_history[:5])\nprint('Last 5 values from cost_history =', cost_history[-5 :])","f702fbee":"import matplotlib.pyplot as plt\nplt.plot(range(1, iterations +1), cost_history, color ='blue')\nplt.rcParams[\"figure.figsize\"] = (10,6)\nplt.grid()\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"cost (J)\")\nplt.title(\"Convergence of gradient descent\")","17f094e3":"iterations = 400;\ntheta = np.zeros(6)\n\nalpha = 0.005;\ntheta_1, cost_history_1 = gradient_descent(X, y, theta, alpha, iterations)\n\nalpha = 0.01;\ntheta_2, cost_history_2 = gradient_descent(X, y, theta, alpha, iterations)\n\nalpha = 0.02;\ntheta_3, cost_history_3 = gradient_descent(X, y, theta, alpha, iterations)\n\nalpha = 0.03;\ntheta_4, cost_history_4 = gradient_descent(X, y, theta, alpha, iterations)\n\nalpha = 0.15;\ntheta_5, cost_history_5 = gradient_descent(X, y, theta, alpha, iterations)\n\nplt.plot(range(1, iterations +1), cost_history_1, color ='black', label = 'alpha = 0.005')\nplt.plot(range(1, iterations +1), cost_history_2, color ='blue', label = 'alpha = 0.01')\nplt.plot(range(1, iterations +1), cost_history_3, color ='green', label = 'alpha = 0.02')\nplt.plot(range(1, iterations +1), cost_history_4, color ='brown', label = 'alpha = 0.03')\nplt.plot(range(1, iterations +1), cost_history_5, color ='blue', label = 'alpha = 0.15')\n\nplt.rcParams[\"figure.figsize\"] = (10,6)\nplt.grid()\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"cost (J)\")\nplt.title(\"Effect of Learning Rate On Convergence of Gradient Descent\")\nplt.legend()\n","16b9c51d":"iterations = 100;\ntheta = np.zeros(6)\n\nalpha = 1.32;\ntheta_6, cost_history_6 = gradient_descent(X, y, theta, alpha, iterations)\n\nplt.plot(range(1, iterations +1), cost_history_6, color ='brown')\nplt.rcParams[\"figure.figsize\"] = (10,6)\nplt.grid()\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"cost (J)\")\nplt.title(\"Effect of Large Learning Rate On Convergence of Gradient Descent\")","b919da06":"\n\nfor i in range(50):\n    hu = (np.array(x_test[i]))\n    ki=hu[1:6]\n    normalize_test_data = ((ki - mu) \/ sigma)\n    normalize_test_data = np.hstack((np.ones(1), normalize_test_data))\n    y_pred = normalize_test_data.dot(theta_3)\n    print('Predicted age : {},real y value: {}'.format(y_pred, y_test[i]) )\n    \n    ","456d8726":"\nfor i in range(50):\n    hu = (np.array(x_test[i]))\n    ki=hu[1:6]\n    normalize_test_data = ((ki - mu) \/ sigma)\n    normalize_test_data = np.hstack((np.ones(1), normalize_test_data))\n    y_pred = normalize_test_data.dot(theta_1)\n    print('Predicted age : {},real y value: {}'.format(y_pred, y_test[i]) )","febbc48f":"\nfor i in range(50):\n    hu = (np.array(x_test[i]))\n    ki=hu[1:6]\n    normalize_test_data = ((ki - mu) \/ sigma)\n    normalize_test_data = np.hstack((np.ones(1), normalize_test_data))\n    y_pred = normalize_test_data.dot(theta_2)\n    print('Predicted age : {},real y value: {}'.format(y_pred, y_test[i]) )","b2474655":"### Then we Normalize the features because they all are not in the same range of measurement:\n\nFeature Scaling: In feature scaling we divide the input value by range(max - min) of input variable. By this technique we get new range of just 1.\n\nMean Normalization: In mean normalization we subtract the average value from the input variable and then divide it by range(max - min) or by standard deviation of input variable.","a5777417":"### Effect Of Learning Rate On Convergence","b655642e":"### Data load and EDA","5f61a870":"Hi, I'm a total noob and have attempted to fit the life expctancy data on a linear regression model without using sklearn. My attempts at predicting for x_train is confusing(bottom) , therefore if anyone passing by knows a bit about how to train the x_test to my model please do shine light.:p thank you in advance. (my prediction for y just changes in range with changing theta and I dont quite understand how to get more accurate predictions.did my due diligence but hitting a wall.","e5db1460":"### Cost Function\n\n\nThe loss over **m** examples:\nMean Squared Error:\n\n\n$\n\\begin{equation}\nJ = \\frac{1}{2m} \\sum_{i=1}^{m} (y - \\hat{y})^2\n\\end{equation}\n$\n\nThe objective of the gradient descent algorithm is to minimize this error  value.\n\nGradient Descent Objective is to \n$\n\\begin{equation}\nmin(J)\n\\end{equation}\n$\n\n\n\n\n\n### The gradient descent algorithm:\n\n\ndo it for all the theta values(no of theta values = no of features + 1).\n\n![Loss vs Param](GD.png)\n\n\n","7d419aeb":"## Multivariate Linear Regression ","525f30b1":"### Linear Regression \n\n\nLinear regression predicts the dependent value (y) according to the independent variable (x). The output here is the dependent value, and the input is the independent value.\n\n$\n\\begin{equation}\ny = b + w_1.x_1 + w_2.x_2\n\\end{equation}\n$\n\nIn a vector form:\n\n$\n\\begin{equation}\ny = b + (w_1 w_2).\\binom{x_1}{x_2}\n\\end{equation}\n$\n\nWhere\n$\n\\begin{equation}\nW = (w_1 w_2)\n\\end{equation}\n$\nand\n$\n\\begin{equation}\nX = \\binom{x_1}{x_2}\n\\end{equation}\n$"}}