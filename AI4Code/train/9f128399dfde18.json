{"cell_type":{"f7485dee":"code","7e1b9982":"code","b8d304cb":"code","b06b3024":"code","03d9295a":"code","044e1734":"code","91729760":"code","f4b33db9":"code","632c9a64":"code","0b26d8b5":"code","868be7c3":"code","7e1d57d6":"code","eba91260":"code","590a7333":"code","b1aa599c":"code","a0d6b106":"code","d64acaa5":"markdown","12813da2":"markdown","b53ee3e9":"markdown","ad44c650":"markdown","2b09bf56":"markdown","359c6a3f":"markdown"},"source":{"f7485dee":"from argparse import Namespace\nfrom collections import Counter\nimport json\nimport os\nimport re\nimport string\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm_notebook\n\nimport numpy as np \nimport pandas as pd  \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7e1b9982":"class Vocabulary(object):\n    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n\n        if token_to_idx is None:\n            token_to_idx = {}\n        self._token_to_idx = token_to_idx\n\n        self._idx_to_token = {idx: token \n                              for token, idx in self._token_to_idx.items()}\n        \n        self._add_unk = add_unk\n        self._unk_token = unk_token\n        \n        self.unk_index = -1\n        if add_unk:\n            self.unk_index = self.add_token(unk_token) \n                \n    def to_serializable(self):\n        return {'token_to_idx': self._token_to_idx, \n                'add_unk': self._add_unk, \n                'unk_token': self._unk_token}\n\n    @classmethod\n    def from_serializable(cls, contents):\n        return cls(**contents)\n\n    def add_token(self, token):\n        if token in self._token_to_idx:\n            index = self._token_to_idx[token]\n        else:\n            index = len(self._token_to_idx)\n            self._token_to_idx[token] = index\n            self._idx_to_token[index] = token\n        return index\n    \n    def add_many(self, tokens):\n        return [self.add_token(token) for token in tokens]\n\n    def lookup_token(self, token):\n        if self.unk_index >= 0:\n            return self._token_to_idx.get(token, self.unk_index)\n        else:\n            return self._token_to_idx[token]\n\n    def lookup_index(self, index):\n        if index not in self._idx_to_token:\n            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n        return self._idx_to_token[index]\n\n    def __str__(self):\n        return \"<Vocabulary(size=%d)>\" % len(self)\n\n    def __len__(self):\n        return len(self._token_to_idx)","b8d304cb":"class ReviewVectorizer(object):\n    def __init__(self, review_vocab, rating_vocab):\n        \n        self.review_vocab = review_vocab\n        self.rating_vocab = rating_vocab\n\n    def vectorize(self, review):\n\n        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n        \n        for token in review.split(\" \"):\n            if token not in string.punctuation:\n                one_hot[self.review_vocab.lookup_token(token)] = 1\n\n        return one_hot\n\n    @classmethod\n    def from_dataframe(cls, review_df, cutoff=25):\n\n        review_vocab = Vocabulary(add_unk=True)\n        rating_vocab = Vocabulary(add_unk=False)\n        \n        # Add ratings\n        for rating in sorted(set(review_df.rating)):\n            rating_vocab.add_token(rating)\n\n        # Add top words if count > provided count\n        word_counts = Counter()\n        for review in review_df.review:\n            for word in review.split(\" \"):\n                if word not in string.punctuation:\n                    word_counts[word] += 1\n               \n        for word, count in word_counts.items():\n            if count > cutoff:\n                review_vocab.add_token(word)\n\n        return cls(review_vocab, rating_vocab)\n\n    @classmethod\n    def from_serializable(cls, contents):\n\n        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n        rating_vocab =  Vocabulary.from_serializable(contents['rating_vocab'])\n\n        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n\n    def to_serializable(self):\n\n        return {'review_vocab': self.review_vocab.to_serializable(),\n                'rating_vocab': self.rating_vocab.to_serializable()}","b06b3024":"class ReviewDataset(Dataset):\n    def __init__(self, review_df, vectorizer):\n\n        self.review_df = review_df\n        self._vectorizer = vectorizer\n        \n        self.train_df = self.review_df[self.review_df.split=='train']\n        self.train_size = len(self.train_df)\n\n        self.val_df = self.review_df[self.review_df.split=='val']\n        self.validation_size = len(self.val_df)\n\n        self.test_df = self.review_df[self.review_df.split=='test']\n        self.test_size = len(self.test_df)\n\n        self._lookup_dict = {'train': (self.train_df, self.train_size),\n                             'val': (self.val_df, self.validation_size),\n                             'test': (self.test_df, self.test_size)}\n\n        self.set_split('train')\n\n    @classmethod\n    def load_dataset_and_make_vectorizer(cls, review_csv):\n\n        review_df = pd.read_csv(review_csv)\n        review_df = review_df.sample(frac=.003)\n        review_df['split'] = np.random.randn(len(review_df), 1)\n        review_df['split'] = review_df['split'].apply( lambda x : 'train' if x<.7 else ( 'val' if x<.85 else 'test' ))\n        review_df['rating'] = review_df['stars'].apply( lambda x : 1 if x>3 else 0 )\n        review_df['review'] = review_df['text']\n        review_df = review_df[['review','rating','split']]\n        print( review_df.info() )\n        \n        train_review_df = review_df[review_df.split=='train']\n        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))\n    \n    @classmethod\n    def load_dataset_and_load_vectorizer(cls, review_csv, vectorizer_filepath):\n\n        review_df = pd.read_csv(review_csv)\n        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n        return cls(review_df, vectorizer)\n\n    @staticmethod\n    def load_vectorizer_only(vectorizer_filepath):\n\n        with open(vectorizer_filepath) as fp:\n            return ReviewVectorizer.from_serializable(json.load(fp))\n\n    def save_vectorizer(self, vectorizer_filepath):\n\n        with open(vectorizer_filepath, \"w\") as fp:\n            json.dump(self._vectorizer.to_serializable(), fp)\n\n    def get_vectorizer(self):\n        return self._vectorizer\n\n    def set_split(self, split=\"train\"):\n\n        self._target_split = split\n        self._target_df, self._target_size = self._lookup_dict[split]\n\n    def __len__(self):\n        return self._target_size\n\n    def __getitem__(self, index):\n\n        row = self._target_df.iloc[index]\n\n        review_vector = \\\n            self._vectorizer.vectorize(row.review)\n\n        rating_index = \\\n            self._vectorizer.rating_vocab.lookup_token(row.rating)\n\n        return {'x_data': review_vector,\n                'y_target': rating_index}\n\n    def get_num_batches(self, batch_size):\n\n        return len(self) \/\/ batch_size  \n    \ndef generate_batches(dataset, batch_size, shuffle=True,\n                     drop_last=True, device=\"cpu\"):\n  \n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n                            shuffle=shuffle, drop_last=drop_last)\n\n    for data_dict in dataloader:\n        out_data_dict = {}\n        for name, tensor in data_dict.items():\n            out_data_dict[name] = data_dict[name].to(device)\n        yield out_data_dict","03d9295a":"class ReviewClassifier(nn.Module):\n    def __init__(self, num_features):\n   \n        super(ReviewClassifier, self).__init__()\n        self.fc1 = nn.Linear(in_features=num_features, \n                             out_features=1)\n\n    def forward(self, x_in, apply_sigmoid=False):\n        \n        y_out = self.fc1(x_in).squeeze()\n        if apply_sigmoid:\n            y_out = torch.sigmoid(y_out)\n        return y_out\n\ndef make_train_state(args):\n    return {'stop_early': False,\n            'early_stopping_step': 0,\n            'early_stopping_best_val': 1e8,\n            'learning_rate': args.learning_rate,\n            'epoch_index': 0,\n            'train_loss': [],\n            'train_acc': [],\n            'val_loss': [],\n            'val_acc': [],\n            'test_loss': -1,\n            'test_acc': -1,\n            'model_filename': args.model_state_file}\n\ndef update_train_state(args, model, train_state):\n\n    if train_state['epoch_index'] == 0:\n        torch.save(model.state_dict(), train_state['model_filename'])\n        train_state['stop_early'] = False\n\n    elif train_state['epoch_index'] >= 1:\n        loss_tm1, loss_t = train_state['val_loss'][-2:]\n\n        if loss_t >= train_state['early_stopping_best_val']:\n            train_state['early_stopping_step'] += 1\n        else:\n            if loss_t < train_state['early_stopping_best_val']:\n                torch.save(model.state_dict(), train_state['model_filename'])\n\n            train_state['early_stopping_step'] = 0\n\n        train_state['stop_early'] = \\\n            train_state['early_stopping_step'] >= args.early_stopping_criteria\n\n    return train_state\n\ndef compute_accuracy(y_pred, y_target):\n    y_target = y_target.cpu()\n    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n    return n_correct \/ len(y_pred_indices) * 100","044e1734":"def set_seed_everywhere(seed, cuda):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if cuda:\n        torch.cuda.manual_seed_all(seed)\n\ndef handle_dirs(dirpath):\n    if not os.path.exists(dirpath):\n        os.makedirs(dirpath)","91729760":"args = Namespace(\n    frequency_cutoff=25,\n    model_state_file='model.pth',\n    review_csv='\/kaggle\/input\/review\/yelp_academic_dataset_review.csv',\n    save_dir='model_storage\/ch3\/yelp\/',\n    vectorizer_file='vectorizer.json',\n    batch_size=128,\n    early_stopping_criteria=5,\n    learning_rate=0.001,\n    num_epochs=100,\n    seed=1337,\n    catch_keyboard_interrupt=True,\n    cuda=True,\n    expand_filepaths_to_save_dir=True,\n    reload_from_files=False,\n)\n\nif args.expand_filepaths_to_save_dir:\n    args.vectorizer_file = os.path.join(args.save_dir,\n                                        args.vectorizer_file)\n\n    args.model_state_file = os.path.join(args.save_dir,\n                                         args.model_state_file)\n    \n    print(\"Expanded filepaths: \")\n    print(\"\\t{}\".format(args.vectorizer_file))\n    print(\"\\t{}\".format(args.model_state_file))\n    \nif not torch.cuda.is_available():\n    args.cuda = False\n\nprint(\"Using CUDA: {}\".format(args.cuda))\n\nargs.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n\nset_seed_everywhere(args.seed, args.cuda)\n\nhandle_dirs(args.save_dir)","f4b33db9":"if args.reload_from_files:\n    print(\"Loading dataset and vectorizer\")\n    dataset = ReviewDataset.load_dataset_and_load_vectorizer(args.review_csv,\n                                                            args.vectorizer_file)\nelse:\n    print(\"Loading dataset and creating vectorizer\")\n    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n    dataset.save_vectorizer(args.vectorizer_file)    \n\nvectorizer = dataset.get_vectorizer()\n\nclassifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))","632c9a64":"classifier = classifier.to(args.device)\n\nloss_func = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n                                                 mode='min', factor=0.5,\n                                                 patience=1)\n\ntrain_state = make_train_state(args)\n\nepoch_bar = tqdm_notebook(desc='training routine', \n                          total=args.num_epochs,\n                          position=0)\n\ndataset.set_split('train')\ntrain_bar = tqdm_notebook(desc='split=train',\n                          total=dataset.get_num_batches(args.batch_size), \n                          position=1, \n                          leave=True)\ndataset.set_split('val')\nval_bar = tqdm_notebook(desc='split=val',\n                        total=dataset.get_num_batches(args.batch_size), \n                        position=1, \n                        leave=True)\n\ntry:\n    for epoch_index in range(args.num_epochs):\n        train_state['epoch_index'] = epoch_index\n        \n        dataset.set_split('train')\n        batch_generator = generate_batches(dataset, \n                                           batch_size=args.batch_size, \n                                           device=args.device)\n        running_loss = 0.0\n        running_acc = 0.0\n        classifier.train()\n\n        for batch_index, batch_dict in enumerate(batch_generator):\n\n            optimizer.zero_grad()\n\n            y_pred = classifier(x_in=batch_dict['x_data'].float())\n\n            loss = loss_func(y_pred, batch_dict['y_target'].float())\n            loss_t = loss.item()\n            running_loss += (loss_t - running_loss) \/ (batch_index + 1)\n\n            loss.backward()\n\n            optimizer.step()\n            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n            running_acc += (acc_t - running_acc) \/ (batch_index + 1)\n\n            train_bar.set_postfix(loss=running_loss, \n                                  acc=running_acc, \n                                  epoch=epoch_index)\n            train_bar.update()\n\n        train_state['train_loss'].append(running_loss)\n        train_state['train_acc'].append(running_acc)\n\n\n        dataset.set_split('val')\n        batch_generator = generate_batches(dataset, \n                                           batch_size=args.batch_size, \n                                           device=args.device)\n        running_loss = 0.\n        running_acc = 0.\n        classifier.eval()\n\n        for batch_index, batch_dict in enumerate(batch_generator):\n\n            y_pred = classifier(x_in=batch_dict['x_data'].float())\n\n            loss = loss_func(y_pred, batch_dict['y_target'].float())\n            loss_t = loss.item()\n            running_loss += (loss_t - running_loss) \/ (batch_index + 1)\n\n            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n            running_acc += (acc_t - running_acc) \/ (batch_index + 1)\n            \n            val_bar.set_postfix(loss=running_loss, \n                                acc=running_acc, \n                                epoch=epoch_index)\n            val_bar.update()\n\n        train_state['val_loss'].append(running_loss)\n        train_state['val_acc'].append(running_acc)\n\n        train_state = update_train_state(args=args, model=classifier,\n                                         train_state=train_state)\n\n        scheduler.step(train_state['val_loss'][-1])\n\n        train_bar.n = 0\n        val_bar.n = 0\n        epoch_bar.update()\n\n        if train_state['stop_early']:\n            break\n\n        train_bar.n = 0\n        val_bar.n = 0\n        epoch_bar.update()\nexcept KeyboardInterrupt:\n    print(\"Exiting loop\")","0b26d8b5":"classifier.load_state_dict(torch.load(train_state['model_filename']))\nclassifier = classifier.to(args.device)\n\ndataset.set_split('test')\nbatch_generator = generate_batches(dataset, \n                                   batch_size=args.batch_size, \n                                   device=args.device)\nrunning_loss = 0.\nrunning_acc = 0.\nclassifier.eval()\n\nfor batch_index, batch_dict in enumerate(batch_generator):\n    y_pred = classifier(x_in=batch_dict['x_data'].float())\n\n    loss = loss_func(y_pred, batch_dict['y_target'].float())\n    loss_t = loss.item()\n    running_loss += (loss_t - running_loss) \/ (batch_index + 1)\n\n    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n    running_acc += (acc_t - running_acc) \/ (batch_index + 1)\n\ntrain_state['test_loss'] = running_loss\ntrain_state['test_acc'] = running_acc","868be7c3":"print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\nprint(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))","7e1d57d6":"def preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n    return text","eba91260":"def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n    review = preprocess_text(review)\n    \n    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n    result = classifier(vectorized_review.view(1, -1))\n    \n    probability_value = F.sigmoid(result).item()\n    index = 1\n    if probability_value < decision_threshold:\n        index = 0\n\n    return vectorizer.rating_vocab.lookup_index(index)","590a7333":"test_review = \"this is a pretty awesome book\"\n\nclassifier = classifier.cpu()\nprediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\nprint(\"{} -> {}\".format(test_review, prediction))","b1aa599c":"classifier.fc1.weight.shape","a0d6b106":"# Sort weights\nfc1_weights = classifier.fc1.weight.detach()[0]\n_, indices = torch.sort(fc1_weights, dim=0, descending=True)\nindices = indices.numpy().tolist()\n\n# Top 20 words\nprint(\"Influential words in Positive Reviews:\")\nprint(\"--------------------------------------\")\nfor i in range(20):\n    print(vectorizer.review_vocab.lookup_index(indices[i]))\n    \nprint(\"====\\n\\n\\n\")\n\n# Top 20 negative words\nprint(\"Influential words in Negative Reviews:\")\nprint(\"--------------------------------------\")\nindices.reverse()\nfor i in range(20):\n    print(vectorizer.review_vocab.lookup_index(indices[i]))","d64acaa5":"The following class encapsulates the vocabulary bijection and handles unkownn tokens.","12813da2":"The following class creates a count-based vectorizer.","b53ee3e9":"This kernel performs sentiment analysis on Yelp\u2019s review dataset using a deep learning classifier. The actual size of the original dataset is reduced to 20058 entries.","ad44c650":"The following class imports the dataset and defines the working split of the dataset.","2b09bf56":"The following functions define the perceptron classifier.","359c6a3f":"Computing the cost & accuracy on the test set using the best available model\n"}}