{"cell_type":{"09ba23ce":"code","e9d27460":"code","02322487":"code","fead56fd":"code","d5d27552":"code","8ec2f475":"code","ccbc18df":"code","69e8fe13":"code","3595aba1":"code","771c9ec2":"code","e4862010":"code","e762d595":"code","e31e8a9d":"code","a6fdfdf9":"code","a8e21045":"code","2878d41d":"code","7f507662":"code","79f39c43":"code","671fb855":"code","aa79c977":"code","446c5f78":"code","0ee547b1":"code","adfb4603":"code","5fe1f986":"code","25918852":"code","962049a0":"code","4e047046":"code","1284e286":"markdown","6b973a37":"markdown","9af0b081":"markdown","64aa65ba":"markdown","7be33a00":"markdown","eeffc3d7":"markdown","b971b917":"markdown","e9e97f10":"markdown","ed820b5d":"markdown","03b7bf2b":"markdown","b3614bb7":"markdown"},"source":{"09ba23ce":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport glob\nimport gc\nfrom IPython.display import display\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_auc_score, auc, classification_report, confusion_matrix\nfrom tensorflow import keras\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sn","e9d27460":"dataset = pd.read_csv(\"..\/input\/brain-cancer-gene-expression-cumida\/Brain_GSE50161.csv\")\ndisplay(dataset.head())","02322487":"display(dataset.info())\n#display(dataset.describe())","fead56fd":"sum(dataset.isna().sum().values)","d5d27552":"classes = dataset.type.unique().tolist()\nx_data = dataset.drop(['samples', 'type'], axis = 1).values\ny_data = dataset.type.values\nfunc = lambda x : classes.index(x)\ny_data = np.asarray([func(i) for i in y_data], dtype = \"float32\")","8ec2f475":"print(f\"X_data Shape : {x_data.shape}\")\nprint(f\"Y_data Shape : {y_data.shape}\")","ccbc18df":"pca_scaler = Pipeline([\n    ('Scaler', MinMaxScaler()),\n    #('PCA', PCA(n_components = 0.9))\n])\n\nx_data = pca_scaler.fit_transform(x_data)\n\nx_data.shape","69e8fe13":"def KFold_Training(model, model_name, color):\n    onehc = OneHotEncoder(sparse=False)\n    global tpr, fpr, roc_auc, x_data, y_data, colors, model_names\n    cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=42)\n    print(\"Cross Val Score : \", cross_val_score(model, x_data, y_data, cv = cv, scoring = \"f1_macro\"))\n    stratify_kfold = StratifiedKFold(shuffle = True, random_state = 42)\n    s_kf = []\n    for train_index, val_index in stratify_kfold.split(x_data, y_data):\n        x, x_val = x_data[train_index], x_data[val_index]\n        y, y_val = y_data[train_index], y_data[val_index]\n        model.fit(x, y)\n        s_kf.append(model.score(x_val, y_val))\n    print(f\"\\nStratifiedKFold Score : {s_kf}\")\n    print(classification_report(y_data, model.predict(x_data)))\n    display(sn.heatmap(confusion_matrix(y_data, model.predict(x_data)), annot = True))\n    \n    \n    return model","3595aba1":"def plot_important_features(model, limiter):\n    global dataset, classes\n    indexes = np.where(model.feature_importances_ >= np.max(model.feature_importances_) \/ limiter)\n    features = dataset.columns[indexes[0]]\n    importancies = np.array(model.feature_importances_[indexes[0]])\n    inds = importancies.argsort()\n    features = features[inds][-10:]\n    importancies = importancies[inds][-10:]\n    print(\"Features                  Importancies\", end = \"\\n\\n\")\n    for feature, importancy in zip(features, importancies):\n        print(feature, \"       \", importancy)\n    fig, ax = plt.subplots(figsize = (15, 10))\n    ax.bar(features, importancies)\n    ax.set_ylabel('Importance')\n    ax.set_xlabel('Features')\n    plt.tight_layout()\n    plt.show()\n    \n    #\"Plotting Bar Plot for Every Important Feature\"\n    df = pd.DataFrame()\n    df = dataset[features]\n    df.hist(figsize = (20, 20))\n    df['type'] = np.array(list(map(lambda x : classes.index(x), dataset['type'].values)),\n                          dtype = \"float32\")\n    \n    #\"Plotting Correlation Graphs for Every Important Feature and Label\"\n    scatter_matrix(df, figsize = (20, 20))\n    \n    print(\"Correlation of Type with every other Important Feature\")\n    display(df.corr()[\"type\"].sort_values(ascending = False))","771c9ec2":"def plot_corr_matrix(indexes, importancies):\n    global dataset, classes\n    features = dataset.columns[indexes]    \n    fig, ax = plt.subplots(figsize = (15, 10))\n    ax.bar(features, importancies)\n    plt.tight_layout()\n    plt.show()\n    \n    #\"Plotting Bar Plot for Every Important Feature\"\n    df = pd.DataFrame()\n    df = dataset[features]\n    df.hist(figsize = (20, 20))\n    df['type'] = np.array(list(map(lambda x : classes.index(x), dataset['type'].values)),\n                          dtype = \"float32\")\n    \n    #\"Plotting Correlation Graphs for Every Important Feature and Label\"\n    scatter_matrix(df, figsize = (20, 20))\n    \n    print(\"Correlation of Type with every other Important Feature\")\n    display(df.corr()[\"type\"].sort_values(ascending = False))","e4862010":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, ShuffleSplit, RandomizedSearchCV","e762d595":"x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, random_state = 42, shuffle = True, stratify = y_data)","e31e8a9d":"log = LogisticRegression(max_iter = 10000, penalty = \"elasticnet\", l1_ratio = 0,\n                         solver = \"saga\", n_jobs = -1, C = 0.05)","a6fdfdf9":"log.fit(x_train, y_train)","a8e21045":"log = KFold_Training(log, \"Logestic Regression\", 'r')\nprint(\"Intercepts : \", log.intercept_)","2878d41d":"rf = RandomForestClassifier(oob_score = True, n_jobs = -1, verbose = 0)","7f507662":"random_grid = {'criterion' : [\"gini\", \"entropy\"],\n               'max_depth' : np.arange(3, 16, 1),\n               'min_samples_split' : np.arange(0.1, 1, 0.1),\n               'min_samples_leaf' : np.arange(1, 16, 2),\n              'n_estimators' : np.arange(50, 600, 50),\n               'max_features' : [\"auto\", \"sqrt\", \"log2\"],\n               'class_weight' : [\"balanced\", \"balanced_subsample\"],\n              'random_state' : np.arange(35, 47, 1)}\nrcv = RandomizedSearchCV(estimator = rf,\n                         param_distributions = random_grid,\n                         n_iter = 100, cv = 5, verbose=0, random_state=35, n_jobs = -1)\n\n\nrcv = KFold_Training(rcv, \"Random Forest\", 'darkorange')\nrf = rcv.best_estimator_\nprint(\"Best Params For Random Forest : \", end = str(rcv.best_params_))\nprint(f\"\\n\\nOOB_Score : {rf.oob_score_}\", end = \"\\n\\n\")","79f39c43":"plot_important_features(rf, 2)","671fb855":"import xgboost\nfrom xgboost import XGBClassifier","aa79c977":"xgb = XGBClassifier(tree_method = \"gpu_hist\", \n             predictor = \"gpu_predictor\", gpu_id = 0, use_label_encoder=False, verbosity = 0)\nxgb","446c5f78":"#random_grid = {'booster' : ['gbtree', 'gblinear'],\n#               'max_depth' : np.arange(1, 10, 1),\n#               'grow_policy' : ['depthwise', 'lossguide'],\n#               'min_child_weight' : np.arange(1, 20, 1),\n#               'subsample' : np.arange(0.0, 0.95, 0.5),\n#               'max_delta_step' : np.arange(0, 10, 1),\n#               'lambda' : np.arange(0, 1, 0.1),\n#               'alpha' : np.arange(0, 1, 0.1),\n#              'gamma' : np.arange(1, 20, 1),\n#              'random_state' : np.arange(35, 47, 1)}\n#rcv = RandomizedSearchCV(estimator = xgb,\n#                         param_distributions = random_grid,\n#                         n_iter = 100, cv = 5, verbose=0, random_state=35, n_jobs = -1)\n\n\nxgb = KFold_Training(xgb, \"XGBoost\", 'g')\n#print(\"Best Params For XGB : \", end = str(rcv.best_params_))","0ee547b1":"booster = xgb.get_booster()\n\n# Get the importance dictionary (by gain) from the booster\nimportance = booster.get_score(importance_type=\"gain\")\n\nfeature_indexes = list(importance.keys())\nfeatures = dataset.columns.tolist().remove('samples')\nfeatures = dataset.columns.tolist().remove('type')\n\n# make your changes\nfor key in importance.keys():\n    index = feature_indexes.index(key)\n    feature_indexes[index] = feature_indexes[index][1:]\n    feature_indexes[index] = int(feature_indexes[index])\n    importance[key] = round(importance[key],2)\n    \n    \npd.Series(list(importance.values()), index = feature_indexes).nlargest(10)\nax = xgboost.plot_importance(importance, max_num_features=10, importance_type='gain', show_values=True)\nax.set_yticklabels(dataset.columns[pd.Series(list(importance.values()), index = feature_indexes).nlargest(10).index.tolist()].tolist())\nplt.show()","adfb4603":"plot_important_features(xgb, 10)","5fe1f986":"svc = SVC(max_iter = -1)\nsvc","25918852":"random_grid = {'C' : np.arange(1, 100, 5),\n              'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n              'degree' : np.arange(1, 10, 1),\n              'gamma' : ['scale', 'auto'],\n              'coef0' : np.arange(0.0, 1.0, 0.1),\n              'random_state' : np.arange(35, 47, 1)}\nrcv = RandomizedSearchCV(estimator = svc,\n                         param_distributions = random_grid,\n                         n_iter = 100, cv = 5, verbose=0, random_state=35, n_jobs = -1)\n\n\nrcv = KFold_Training(rcv, \"SVC_RandomizedSearchCV\", 'g')\nsvc = rcv.best_estimator_\nprint(\"Best Params For SVC : \", end = str(rcv.best_params_))\nprint(\"\\n\\nIntercepts : \", end = str(svc.intercept_))\n","962049a0":"features = dataset.columns.tolist().remove('samples')\nfeatures = dataset.columns.tolist().remove('type')\n\nfig ,ax = plt.subplots()\nax.set_xlabel(\"Importance\")\nax.set_ylabel(\"Index of Features\")\nax.set_yticks(np.arange(1, 11, 1))\nfeature_importance = pd.Series(abs(svc.coef_[0]), index=features).nlargest(10).plot(kind='barh')\nax.set_yticklabels(dataset.columns[pd.Series(abs(svc.coef_[0]), index=features).nlargest(10).index.tolist()].tolist())\nplt.show()","4e047046":"plot_corr_matrix(pd.Series(abs(svc.coef_[0]), index=features).nlargest(10).index,\n                pd.Series(abs(svc.coef_[0]), index=features).nlargest(10).values)","1284e286":"#### After performing PCA and Scaling","6b973a37":"#### Correlation Plot of all top 10 important Features","9af0b081":"## SVC","64aa65ba":"### Feature Importances according to SVC","7be33a00":"#### These features has high influence on algorithm while prediction","eeffc3d7":"#### Hence there is no Null value in the Dataset","b971b917":"## These feature had a high influence on the algorithm","e9e97f10":"## XGB","ed820b5d":"## Random Forest","03b7bf2b":"## Top 10 Features According to Random Forest","b3614bb7":"### Correlation between highly influential features"}}