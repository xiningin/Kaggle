{"cell_type":{"a38b7c9c":"code","c3acbde3":"code","a31c9621":"code","35ba3e46":"code","977bd4af":"code","067da3d9":"code","2f08a8f3":"code","b5981e5f":"code","5e343ba7":"code","2683793b":"code","549d5414":"code","b6e654d9":"code","98754dc6":"code","c3417769":"code","3f96c689":"code","8143e0ec":"code","40dc24cf":"code","3034cce9":"code","73f6d121":"code","8ae2bdee":"code","5ee5d860":"code","813024d0":"code","d84bfdd8":"code","f98d8dc7":"code","8c6e278a":"code","22640bc6":"code","192a096f":"code","cd3a412d":"code","ea20c902":"code","44855818":"code","89f462cf":"markdown","a0d3e7b3":"markdown","1e10db17":"markdown","db9c0fd9":"markdown","922a0747":"markdown","6c57a609":"markdown","2b8b6e07":"markdown","cf041bd8":"markdown","285149c5":"markdown","bf27f553":"markdown","5dd73485":"markdown","a1655ef0":"markdown","63d33bd0":"markdown","22b86888":"markdown","e49809ee":"markdown","f1c3e32a":"markdown","8308bc54":"markdown","cecd6506":"markdown","0f7afe0a":"markdown","52aa6261":"markdown","e76397e9":"markdown","eda255a7":"markdown","9e6df37f":"markdown","b7bcdd96":"markdown","c08c6e49":"markdown"},"source":{"a38b7c9c":"from sklearn.feature_extraction.text import CountVectorizer \n\ntext = [\"Hey this note book is of Nlp\", \"Nlp stands for natural langauge processing\", \"I love Nlp\"]","c3acbde3":"vector = CountVectorizer()\n\nvector.fit(text)\nprint(sorted(vector.vocabulary_))","a31c9621":"print(vector.transform(text).toarray())","35ba3e46":"# let's use tfidf using sklearn \n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nvectorizer.fit(text)\n\nprint(sorted(vectorizer.vocabulary_))\n","977bd4af":"print(vectorizer.transform(text).toarray())","067da3d9":"del_cost=1\nins_cost=1\nsub_cost=2","2f08a8f3":"import numpy as np\n\ndef calculate_minimum_edit_distance(source, target):\n    n = len(source)\n    m = len(target)\n    MED_matrix = np.zeros((n+1, m+1), dtype='int32')\n    for i in range(1, n+1):\n        MED_matrix[i][0] = MED_matrix[i-1][0]+ del_cost\n    for i in range(1, m+1):\n        MED_matrix[0][i] = MED_matrix[0][i-1]+ del_cost\n    \n    for i in range(1, n+1):\n        for j in range(1, m+1):\n            if(source[i-1] == target[j-1]):\n                MED_matrix[i][j] = min([MED_matrix[i-1][j]+del_cost, \n                                       MED_matrix[i-1][j-1]+0, MED_matrix[i][j-1]+ins_cost])\n                \n            else : \n                MED_matrix[i][j]=min([MED_matrix[i-1][j]+del_cost,MED_matrix[i-1][j-1]+sub_cost,MED_matrix[i][j-1]+ins_cost])\n\n    print(MED_matrix)\n    return MED_matrix[n][m]","b5981e5f":"calculate_minimum_edit_distance('Sachin is the best','Dhoni is the king')\n","5e343ba7":"import pandas as pd \nimport numpy as np","2683793b":"data = pd.read_json(\"\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ndata.head()","549d5414":"import nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport string\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt')\n","b6e654d9":"def text_process(text):\n    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n    #print(text)\n    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n    return \" \".join(text)","98754dc6":"def conf_matrix_acc(y_true, y_pred):\n    print('Confusion Matrix is :\\n',confusion_matrix(y_true, y_pred))\n    print('\\nAccuracy Score is:',accuracy_score(y_true, y_pred))","c3417769":"# how text proccess work \n# using apply function to  apply every line of column headline to the functions text_process\nprint(text_process(\"hey ! it's good to see you \"))\ntext_feat = data['headline']\ntext_feat = text_feat.apply(text_process)\n","3f96c689":"# created instance \nvectorizer = TfidfVectorizer(\"english\")\n","8143e0ec":"# created the vectors using fit_transform \nfeatures = vectorizer.fit_transform(text_feat)\nfeatures","40dc24cf":"# splitted the data into train and test \nX_train, X_test, y_train, y_test = train_test_split(features, data['is_sarcastic'],\n                                                    test_size=0.3, random_state=123)","3034cce9":"# Instace of logistic regression & MultinomialNB is created \nlr = LogisticRegression(solver='liblinear', penalty='l1')\nmnb = MultinomialNB(alpha=0.2)","73f6d121":"# fitting the model \nprint(lr.fit(X_train, y_train))\nprint(mnb.fit(X_train, y_train))","8ae2bdee":"# predicting the model \ny_pred_lr = lr.predict(X_test)\ny_pred_nb = mnb.predict(X_test)","5ee5d860":"# ploting confusion matrix \nconf_matrix_acc(y_test, y_pred_lr)\nconf_matrix_acc(y_test, y_pred_nb)","813024d0":"from gensim.test.utils import common_texts\nfrom gensim.models import Word2Vec\nmodel = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\nmodel.save(\"word2vec.model\")","d84bfdd8":"model = Word2Vec.load(\"word2vec.model\")\nmodel.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)\n","f98d8dc7":"vector = model.wv['computer']  # get numpy vector of a word\nsims = model.wv.most_similar('computer', topn=10)","8c6e278a":"print(vector)","22640bc6":"print(sims)","192a096f":"sims","cd3a412d":"# load in all the modules we're going to need\nimport nltk, re, string, collections\nfrom nltk.util import ngrams # function for making ngrams","ea20c902":"text = \"Hi I am Chandler Bing, I do sarcastic comment when I feel uncomfortable\"\n# first get individual words\ntokenized = text.split()\n\n# and get a list of all the bi-grams\nesBigrams = ngrams(tokenized, 2)","44855818":"# get the frequency of each bigram in our corpus\nesBigramFreq = collections.Counter(esBigrams)\n\n# what are the ten most popular ngrams in this Spanish corpus?\nesBigramFreq.most_common(10)","89f462cf":"### This Notebook is to help those who are about to start or who wants to refresh things or who wants to prepare for interview. I will try to explain everything as possible as I can.\n#### Please Do Upvote if you like my work (Upvote keeps author motivated !;p)\n# WHAT IS NATURAL LANGUAGE PROCESSING ? \nNatural Language Processing, or NLP for short, is broadly defined as the automatic manipulation of natural language, like speech and text, by software. In NLP we aim to solve the problems with the help of software\/machines. \nlet's take an example, if we are email service providing company, and our client except great service from us. like to classify the emails as spam or not-spam ( Important or not importatnt), So in this scenario with help of nlp we will build a model that model will take a input as email text and will give output as the mail is spam or not. Because of this task we were able to provide the service with better user-experience and less manual intervention. \n#### Some more applications of NLP : \n- Search Autocorrect\/ Search Autocomplete \n- Language Translator\n- Social Media Monitoring \n- Chatbots \n- HR \n- Survey Analysis \n- Text Generation\/Manipulation\n\n> ### In this notebook I am planning to cover Beginner Level to DL level Algorithms with text classification examples. \n#### Following are topics : \n- Word Tokenization \n- Text Normalization \n- TFIDF \n- Bag of Words \n- Minimum Edit Distance \n- Text Cleaning \n- Basic Text Classifier using Logistic Regression\/NaiveBayes Theorem\n- Word2Vec\n- ngrams\n\n\n\n\nMore topics are comming....\n![NLP GIF](https:\/\/miro.medium.com\/max\/1838\/0*Qq8FcR-mgnvjWZLQ.gif)\n\n## Understanding from above gif\n\n1. Amanda is using voice command Feature.\n2. Learn to code getting other suggestion \n3. \"On my way\" got auto completed \n4. Email got different kind of automated responses \n5. News Article got suggested with your selected news articles ","a0d3e7b3":"## Gensim Word2Vec Implementation:\n- Why I seperate the training of the model in 3 steps:\u00b6\nI prefer to separate the training in 3 distinctive steps for clarity and monitoring.\n\n-  Word2Vec():\nIn this first step, I set up the parameters of the model one-by-one.\nI do not supply the parameter sentences, and therefore leave the model uninitialized, purposefully.\n\n- .build_vocab():\nHere it builds the vocabulary from a sequence of sentences and thus initialized the model.\nWith the loggings, I can follow the progress and even more important, the effect of min_count and sample on the word corpus. I noticed that these two parameters, and in particular sample, have a great influence over the performance of a model. Displaying both allows for a more accurate and an easier management of their influence.\n\n- .train():\nFinally, trains the model.\nThe loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously.\n","1e10db17":"> 15 is the minimum edit distance to convert Sachin is the best ---> Dhoni is the king","db9c0fd9":"TF : Denotes the contribution of the word to the document ie words relevant to the document should be frequent. \n> TF = (Number of times term t appears in a document)\/(Number of terms in the document)\n                \nIDF (Inverse Document Frequency): If a word has appeared in all the document then probably that words is not relevsnt to a particular document but if it has appeard in  subset of documents them probably the word is of some relevance to the documents it is present in \n> IDF  = log(N\/n) where N is the number of documents and n is the number of documents a term t has appeared in. \n\n## \"RARE TERMS ARE MORE INFORMATIVE THAN FREQUENT TERMS\"","922a0747":"- Here we have \"Headline\" Column which were using as X variable. and Y as is_sarcastic \n","6c57a609":"## TFIDF [TERM FREQUENCY (TF) & INVERSE DATA FREQUENCY(IDF)","2b8b6e07":"## WORD2VEC Model \n### WHAT IS WORD EMBEDDING?\n> word embeddings are nothing but numerical representations of texts.\n- Two types of word embedding \n    1. Frequency Based \n    2. Prediction Based \n- Freq Based are : CountVectorizer, TFIDF, One-hot \n- Prediction Based: CBOW(Continous bag of words), SKIPGRAM, BERT, ELMO ","cf041bd8":"### Text Processing \n- most important and first step in building model is text\/data cleaning and making to comfortable to create models\n- As we are building classifier, we won't need punctuations or stopwords to understand is the comment is sarcastic or not, we are gonna need those words which has high influence on the text to become it as sarcastic or not ","285149c5":"## TEXT NORMALIZATION \nWhen we normalize a Natural Language resource, we attempt to reduce the randomness in it. \nAdvantage of normalization :\n- input variables get reduced \n- it reduces the dimensionality of the input text \n- it helps to deal with code-breaking inputs \n- it helps in granting reliable extraction of statistics from our natural langauge inputs.\n> Following points come while normalizing text \n  - REmoval of duplicate whitespaces and punctuation \n  - Accent Removal \n  - Capital Letter removal \n  - Removal od substitution of special characters \n  - Substitution of contractions \n  - transform word numerals into numbers \n  - substitution of values for their type \n  - Acronym normalization \n  - Normalize date formats, Social security numbers or other data that have standard format\n  - spell correction \n  - removal of gender \n  - substitution of rare words for more common synonyms \n  - stop word removal\n\n","bf27f553":"### Importing data \n- Using pd.read_json functions \n- dataset has three columns named with is_sarcastic, headline, article_link. is_sarcastic has 1 or 0 i.e 1 is sarcastic comment while 0 is not\n- article_link is not really required column for us to build the classifier. ","5dd73485":"### Importing libraries \n1. nltk - **Natural Langauge ToolKit** : NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n2. Sklearn - scikit-learn is machine learning library. which provides feature_extraction, model_selection, linear_model, naive_bayes, metrics\n3. String to perform string functions \n","a1655ef0":"### What is confusion matrix \nA confusion matrix, also known as an error matrix is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class (or vice versa).The name stems from the fact that it makes it easy to see whether the system is confusing two classes \n\n\n![ConfusionMatrix](https:\/\/www.dataschool.io\/content\/images\/2015\/01\/confusion_matrix2.png)\n\n\nSource:https:\/\/www.dataschool.io\/content\/images\/2015\/01\/confusion_matrix2.png\n\n- TP - True Positive Values which are classified as Positive and which are Positive in Actual.\n- TN - True Negative Values which are classified as Negative and which are Negative in Actual.\n- FP - False Positive Values which are classified as Positive and which are Negative in Actual.\n- FN - True Positive Values which are classified as Negative and which are Positive in Actual.\n\n\nPlease Do refer this link to understand it thoroughly : [Link](https:\/\/www.youtube.com\/watch?v=8Oog7TXHvFY)","63d33bd0":"## Ngram Probabilities \n(Will update soon ) ","22b86888":"# MINIMUM EDIT DISTANCE \n- Minimum Edit distance between two strings str1 and str2 is defined as the minimum number of insert\/delete\/substitute operations required to transform str1 into str2. For example if str1 = \"ab\", str2 = \"abc\" then making an insert operation of character 'c' on str1 transforms str1 into str2. Therefore, edit distance between str1 and str2 is 1. You can also calculate edit distance as number of operations required to transform str2 into str1. For above example, if we perform a delete operation of character 'c' on str2, it is transformed into str1 resulting in same edit distance of 1.\n- Looking at another example, if str1 = \"INTENTION\" and str2 = \"EXECUTION\", then the minimum edit distance between str1 and str2 turns out to be 5 as shown below. All operations are performed on str1.\n- ![Minimum Edit distance pic ](https:\/\/github.com\/rushikeshnaik779\/PracticeForNLP\/raw\/main\/Minimum_Edit_Distance\/Screenshot%202021-01-24%20at%208.55.08%20AM.png)\n\nsource : https:\/\/github.com\/rushikeshnaik779\/PracticeForNLP\/raw\/main\/Minimum_Edit_Distance\/Screenshot%202021-01-24%20at%208.55.08%20AM.png","e49809ee":"- Word2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW)\n- CBOW Model: This method takes the context of each word as the input and tries to predict the word corresponding to the context.\n- SKIPGRAM model: This method takes the target as input variable and model outputs probability distributions.","f1c3e32a":"# WHAT IS WORD TOKENIZATION ? \n- tokenization is a way of seperating a peice of text into smaller units called tokens, here tokens can be either words, characters or subwords Hence toeknization can be broadly classified into 3 types - word, character and subword(n-grams characters) Tokenization.\n>   TOKENS ARE THE BUILDING BLOCKS OF NATURAL LANGAUGE\n- Tokenization is perfermed on the corpus to obtain tokens. The tokens are then used to prepare a vocabulary. Vocabulary refers to the set of unique tokens in the corpus. Remember that vocabulary can be constructed by considering each unique token in the corpus or by considering top k freq occuring words \n> CREATING VOCABULARY IS THE ULTIMATE GOAL OF TOKENIZATION ","8308bc54":"## TERM FREQUENCY\n- The number of times a word appears in a document divided by the total number of words in the document. Every document has it's own term frequency.\n![TFIDF](https:\/\/miro.medium.com\/max\/1838\/1*qQgnyPLDIkUmeZKN2_ZWbQ.png)\nsource = https:\/\/miro.medium.com\/max\/1838\/1*qQgnyPLDIkUmeZKN2_ZWbQ.png\n\n## INVERSE DATA FREQUENCY \n- The log of the number of documents divided by the number of documents that contain the word w. {Inverse data frequency determines the weight of rare words across all documents in the corpus.}\n\n\n\n","cecd6506":"## BAG OF WORDS ","0f7afe0a":"- Bag-of-words model is a way of representing text data when modeling text with machine learning algorithms. Machine learning algorithms cannot work with raw text directly; the text must be converted into well defined fixed-length(vector) numbers.\n\n## What is bag of words? \n- A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n\n- A vocabulary of known words.\n- A measure of the presence of known words.\n- It is called a bag-of-words , because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document. The complexity comes both in deciding how to design the vocabulary of known words (or tokens) and how to score the presence of known words","52aa6261":"[Refer to understand Deeply](https:\/\/towardsdatascience.com\/introduction-to-word-embedding-and-word2vec-652d0c2060fa)","e76397e9":"## N-grams ","eda255a7":"## How to build the text classifier using Machine Learning Algorithms \n### Following are the steps \n1. Importing the data \n2. Exploring data(to understand what is the data and how the target variables distribution) \n3. Data Cleaning ( text preprocessing: removing stopwords, punctuation, etc) \n4. Importing models from sklearn library \n5. Fitting the model\n6. Predicting the model \n7. Printing confussion matrix \n","9e6df37f":"#### TFIDF Vectorizer \nTerm frequency-inverse document frequency (tf-idf) gives a measure that takes the importance of a word in consideration depending on how frequently it occurs in a document and a corpus.\n\n- Sklearn provide TFIDF Vectorizer \n- first we have to create instance of vectorizer \n- using fit_transform() function to fit the model, build a vocabulary & then transform it into vectors. \n- fit_transform returns the sparse matrix \n","b7bcdd96":"> N-grams is sequence of N-words \n\nSome examples include auto completion of sentences (such as the one we see in Gmail these days), auto spell check (yes, we can do that as well), and to a certain extent, we can check for grammar in a given sentence.\n![N-grams](https:\/\/images.deepai.org\/glossary-terms\/867de904ba9b46869af29cead3194b6c\/8ARA1.png)\nSource:https:\/\/images.deepai.org\/glossary-terms\/867de904ba9b46869af29cead3194b6c\/8ARA1.png","c08c6e49":"### WORD2VEC Model : \n![WORD2Vec](https:\/\/jalammar.github.io\/images\/word2vec\/word2vec.png)\nSource:https:\/\/jalammar.github.io\/images\/word2vec\/word2vec.png"}}