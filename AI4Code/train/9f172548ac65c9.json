{"cell_type":{"c02d3a1e":"code","0b916b25":"code","00a5a7cb":"code","cca58060":"code","0a861d55":"code","e7d46c06":"code","ebdaf862":"code","4245ac9c":"code","6b1f15e1":"code","398d101c":"code","d13e6f1d":"code","9d49f1b4":"code","696a84ed":"markdown","bf417304":"markdown"},"source":{"c02d3a1e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0b916b25":"path ='\/kaggle\/input\/quora-insincere-questions-classification\/train.csv'\ntrain =pd.read_csv(path, nrows=1000)\ntrain.head()","00a5a7cb":"# coverting every character to lower case\ndocs = train['question_text'].str.lower()\ndocs.head()","cca58060":"## Use Regular Expressions i.e Remove non-alphabets \ndocs =  docs.str.replace('[^a-z ]','')","0a861d55":"## Remove Commonly used Words\nimport nltk    # nltk is a inbuilt function which gives commonly used words in a list\nstopwords = nltk.corpus.stopwords.words('english')\nstemmer = nltk.stem.PorterStemmer()\n\ndef clean_sentence(doc):\n    words =doc.split(' ')\n    words_clean =[stemmer.stem(word) for word in words if word not in stopwords]\n    return ' '.join(words_clean)\n    \ndocs =docs.apply(clean_sentence)\ndocs.head()","e7d46c06":"## Stemming \nstemmer = nltk.stem.PorterStemmer()\nstemmer.stem(\"liking\")","ebdaf862":"## Term Frequency - Inverse Document Frequency (TF=IDF)","4245ac9c":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\ndtm_vectorizer = CountVectorizer()\n\ntrain_X,validate_X,train_y,valdiate_y =train_test_split(docs,train['target'],test_size =0.2,random_state=1)\n\ndtm_vectorizer.fit(train_X)\ndtm_train = dtm_vectorizer.transform(train_X) \ndtm_validate = dtm_vectorizer.transform(validate_X)","6b1f15e1":"df_dtm_train = pd.DataFrame(dtm_train.toarray(),columns = dtm_vectorizer.get_feature_names(), index = train_X.index)\ndf_dtm_train","398d101c":"df_dtm_train.sum().sort_values(ascending=False).head(20).plot.bar()","d13e6f1d":"from sklearn.naive_bayes import MultinomialNB\n\nmodel = MultinomialNB().fit(dtm_train,train_y)\nvalidate_y_pred =model.predict(dtm_validate)\n\nfrom sklearn.metrics import accuracy_score,f1_score\nprint(accuracy_score(valdiate_y,validate_y_pred))\nprint(f1_score(valdiate_y,validate_y_pred))","9d49f1b4":"from nltk.sentiment import SentimentIntensityAnalyzer\nsentiment_analyzer = SentimentIntensityAnalyzer()\nsentiment_analyzer.polarity_scores('i like india ')","696a84ed":"> Sentiment Analysis","bf417304":"****#Text Cleaning****"}}