{"cell_type":{"021eec58":"code","e9d18e3c":"code","56844a28":"code","be2bb211":"code","ad223f9b":"code","17aecb91":"code","fa2f5905":"code","9b434b67":"code","779d0311":"code","1ab7ff4f":"code","9da1cc2a":"code","ff5e4400":"code","517e48f6":"code","f9fd6cf8":"code","ec643faa":"code","5267aa37":"markdown","deb01932":"markdown","a40553d7":"markdown","0986bbcb":"markdown","5e1b8e28":"markdown","e728470f":"markdown","02d3f2c6":"markdown","e0940ecd":"markdown","4d21249c":"markdown","e35b6e5a":"markdown","621f450b":"markdown","ee371b42":"markdown","2cfc8076":"markdown","697bc808":"markdown","7c9d3ddf":"markdown","b697e0b7":"markdown"},"source":{"021eec58":"import math\nimport shutil\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport os\nfrom dateutil.parser import parse\nfrom pytz import timezone\nimport tensorflow as tf\n\ntf.logging.set_verbosity(tf.logging.INFO)\npd.options.display.max_rows = 10\npd.options.display.float_format = '{:.1f}'.format","e9d18e3c":"#Constants\nMIN_LONG = -74.3\nMAX_LONG = -73.0\nMIN_LAT = 40.6\nMAX_LAT = 41.7\nMAX_PASSENGER = 10 \nMIN_FARE = 0.0 \noutput_dir = \".\"\nOUTDIR = \".\"\nOUTPUT_RESULT=\"submission.csv\"\n#Hyper prameters\nBUCKETS=20\nHIDDEN_UNITS = \"128 32 4\"\nSCALE = 10\nBATCH_SIZE=32\nROWS_TO_READ=40000\nROWS_TO_SKIP=10\nLEARNING_RATE=0.04\nSTEPS_TO_PROCESS=40000","56844a28":"df = type('', (), {})()\nprint(datetime.now())\ndf.train = pd.read_csv('..\/input\/train.csv', sep=\",\", skiprows=range(1,ROWS_TO_SKIP),nrows=ROWS_TO_READ)\nprint(datetime.now())\ndf.test = pd.read_csv('..\/input\/test.csv', sep=\",\")\nprint(datetime.now())\n#df.train.head(10)\ndf.train.describe()","be2bb211":"df.train.head()","ad223f9b":"#The test data is clean the training data has quite a bit of not from new york locations\n#I know google thinks you should not do this since the bad data mean something but try\ndef clean(dfn):\n    dfn=dfn[  ((MIN_LONG) <= dfn['pickup_longitude']) & (dfn['pickup_longitude'] <= (MAX_LONG)) ]\n    dfn=dfn[ (MIN_LAT <= dfn['pickup_latitude']) & (dfn['pickup_latitude'] <= MAX_LAT) ]\n    dfn=dfn[ ((MIN_LONG) <= dfn['dropoff_longitude']) & (dfn['dropoff_longitude'] <= (MAX_LONG) )]\n    dfn=dfn[ (MIN_LAT <= dfn['dropoff_latitude'])  & (dfn['dropoff_latitude'] <= MAX_LAT) ]\n    dfn=dfn[dfn['passenger_count'] <= MAX_PASSENGER ]\n    dfn=dfn[dfn['fare_amount'] >= MIN_FARE ]\n    return dfn\ndf.train=clean(df.train)\ndf.train.describe()","17aecb91":"##calculate times\ndf.train['nyctime'] = df.train.apply(lambda row: row['pickup_datetime'][:14]+'00:00 UTC', axis=1)\ndf.test['nyctime'] = df.test.apply(lambda row: row['pickup_datetime'][:14]+'00:00 UTC', axis=1)\n\nnycTimes = []\ndef findTimes(timeStr, nycDict, field):\n    if not(timeStr[:14]+'00:00 UTC' in nycDict):\n        nycTime = {}\n        nycTime['time'] = parse(timeStr).astimezone(timezone('US\/Eastern'))\n        nycTime['weekday'] = int(nycTime['time'].weekday())\n        nycTime['hour'] = int(nycTime['time'].hour)\n        nycTime['hourSince2000'] = int(((nycTime['time'].year-2009)*366+int(nycTime['time'].strftime(\"%j\")))*25+nycTime['time'].hour)\n        nycTime['nyctime'] = timeStr[:14]+'00:00 UTC'\n        nycTimes.append(nycTime)\n    return \n\nminDate=parse(df.train['pickup_datetime'].min())\nmaxDate=parse(df.train['pickup_datetime'].max())\nwhile (minDate < maxDate):\n    findTimes(minDate.strftime(\"%Y-%m-%d %H:%M:%S%z\"),nycTimes,'time')\n    minDate = minDate + timedelta(hours=1)\n\ndf.times = pd.DataFrame(nycTimes)\n","fa2f5905":"df.train=df.train.join(df.times.set_index('nyctime'), on='nyctime')\ndf.test=df.test.join(df.times.set_index('nyctime'), on='nyctime')\ndf.times.info()","9b434b67":"df.train.describe()","779d0311":"# Create feature engineering function that will be used in the input and serving input functions\ndef add_engineered(features):\n    # this is how you can do feature engineering in TensorFlow\n    lat1 = features['pickup_latitude']\n    lat2 = features['dropoff_latitude']\n    lon1 = features['pickup_longitude']\n    lon2 = features['dropoff_longitude']\n    latdiff = (lat1 - lat2)\n    londiff = (lon1 - lon2)\n    \n    # set features for distance with sign that indicates direction\n    features['latdiff'] = latdiff\n    features['londiff'] = londiff\n    dist = (latdiff * latdiff + londiff * londiff)**(0.5)\n    features['euclidean'] = dist\n    features['cityBlockDist'] = abs(latdiff) + abs(londiff)\n    return features\n\ndf.train = add_engineered(df.train)\ndf.test = add_engineered(df.test)\n\ndf.train.head()","1ab7ff4f":"def rmse(labels, predictions):\n    pred_values = tf.cast(predictions['predictions'],tf.float64)\n    return {'rmse': tf.metrics.root_mean_squared_error(labels*SCALE, pred_values*SCALE)}\n","9da1cc2a":"# These are the raw input columns, and will be provided for prediction also\nINPUT_COLUMNS = [\n    # Define features\n    \n    # Numeric columns\n    tf.feature_column.numeric_column('weekday'),\n    tf.feature_column.numeric_column('hour'),\n    tf.feature_column.numeric_column('pickup_latitude'),\n    tf.feature_column.numeric_column('pickup_longitude'),\n    tf.feature_column.numeric_column('dropoff_latitude'),\n    tf.feature_column.numeric_column('dropoff_longitude'),\n    tf.feature_column.numeric_column('passenger_count'),\n    #tf.feature_column.numeric_column('hourSince2000'),\n    \n    # Engineered features that are created in the input_fn\n    tf.feature_column.numeric_column('latdiff'),\n    tf.feature_column.numeric_column('londiff'),\n    tf.feature_column.numeric_column('euclidean'),\n    tf.feature_column.numeric_column('cityBlockDist')\n]\n# Build the estimator\ndef build_estimator(model_dir, nbuckets, hidden_units):\n    \"\"\"\n     \n  \"\"\"\n\n    # Input columns   hourSince2000,\n    (dayofweek, hourofday, plat, plon, dlat, dlon, pcount, latdiff, londiff, euclidean,cityBlockDist) = INPUT_COLUMNS\n\n    # Bucketize the times \n    hourbuckets = np.linspace(0.0, 23.0, 24).tolist()\n    b_hourofday = tf.feature_column.bucketized_column(hourofday, hourbuckets)\n    weekdaybuckets = np.linspace(0.0, 6.0, 7).tolist()\n    b_dayofweek = tf.feature_column.bucketized_column(dayofweek, weekdaybuckets)\n    #since2000buckets = np.linspace(0.0, 599999, 60000).tolist()\n    #b_hourSince2000 = tf.feature_column.bucketized_column(hourSince2000, since2000buckets)\n    \n    # Bucketize the lats & lons\n    latbuckets = np.linspace(38.0, 42.0, nbuckets).tolist()\n    lonbuckets = np.linspace(-76.0, -72.0, nbuckets).tolist()\n    b_plat = tf.feature_column.bucketized_column(plat, latbuckets)\n    b_dlat = tf.feature_column.bucketized_column(dlat, latbuckets)\n    b_plon = tf.feature_column.bucketized_column(plon, lonbuckets)\n    b_dlon = tf.feature_column.bucketized_column(dlon, lonbuckets)\n   \n    # Feature cross\n    ploc = tf.feature_column.crossed_column([b_plat, b_plon], nbuckets * nbuckets)\n    dloc = tf.feature_column.crossed_column([b_dlat, b_dlon], nbuckets * nbuckets)\n    pd_pair = tf.feature_column.crossed_column([ploc, dloc], nbuckets ** 4 )\n    day_hr =  tf.feature_column.crossed_column([b_dayofweek, b_hourofday], 24 * 7)\n\n    # Wide columns and deep columns.\n    wide_columns = [\n        # Feature crosses\n        dloc, ploc, pd_pair,\n        day_hr,\n\n        # Sparse columns\n        b_dayofweek, b_hourofday,\n        #b_hourSince2000,\n\n        # Anything with a linear relationship\n        pcount \n    ]\n\n    deep_columns = [\n        # Embedding_column to \"group\" together ...\n        tf.feature_column.embedding_column(pd_pair, 10),\n        tf.feature_column.embedding_column(day_hr, 10),\n        #tf.feature_column.embedding_column(b_hourSince2000, 60000),\n        # Numeric columns\n        plat, plon, dlat, dlon,\n        latdiff, londiff, euclidean,cityBlockDist\n    ]\n    \n    estimator = tf.estimator.DNNLinearCombinedRegressor(\n        model_dir = model_dir,\n        linear_feature_columns = wide_columns,\n        dnn_feature_columns = deep_columns,\n        dnn_hidden_units = hidden_units)\n\n    # add extra evaluation metric for hyperparameter tuning\n      \n    estimator = tf.contrib.estimator.add_metrics(estimator, rmse)\n    return estimator\n","ff5e4400":"feature_columns={}\nfor i in INPUT_COLUMNS:\n    feature_columns[i.key]=i\nlist(feature_columns.keys())\n","517e48f6":"# Split into train and eval and create input functions\nmsk = np.random.rand(len(df.train)) < 0.8\ntraindf = df.train[msk]\nevaldf = df.train[~msk]\n\ntrain_input_fn = tf.estimator.inputs.pandas_input_fn(x = traindf[list(feature_columns.keys())],\n                                                    y = traindf[\"fare_amount\"] \/ SCALE,\n                                                    num_epochs = 1,\n                                                    batch_size = BATCH_SIZE,\n                                                    shuffle = True)\neval_input_fn = tf.estimator.inputs.pandas_input_fn(x = evaldf[list(feature_columns.keys())],\n                                                    y = evaldf[\"fare_amount\"] \/ SCALE,  # note the scaling\n                                                    num_epochs = 1, \n                                                    batch_size = len(evaldf), \n                                                    shuffle=False)\npredict_input_fn = tf.estimator.inputs.pandas_input_fn(x = df.test[list(feature_columns.keys())],\n                                                    y = None,  # note the scaling\n                                                    num_epochs = 1, \n                                                    batch_size = len(df.test), \n                                                    shuffle=False)","f9fd6cf8":"  tf.logging.set_verbosity(tf.logging.INFO)\n  myopt = tf.train.FtrlOptimizer(learning_rate = LEARNING_RATE) # note the learning rate\n  estimator = estimator = build_estimator(OUTDIR, BUCKETS, HIDDEN_UNITS.split(' '))\n    \n  estimator = tf.contrib.estimator.add_metrics(estimator,rmse)\n  \n  train_spec=tf.estimator.TrainSpec(\n                       input_fn = train_input_fn,max_steps = STEPS_TO_PROCESS)\n  eval_spec=tf.estimator.EvalSpec(\n                       input_fn = eval_input_fn,\n                       steps = None,\n                       start_delay_secs = 1, # start evaluating after N seconds\n                       throttle_secs = 10,  # evaluate every N seconds\n                       )\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)","ec643faa":"  predictions=estimator.predict(input_fn=predict_input_fn)\n  pred = pd.DataFrame({'fare_amount':[i['predictions'][0]*SCALE for i in predictions]})\n  submission=pd.concat([df.test['key'],pred],axis=1)\n  submission.to_csv(OUTPUT_RESULT,index=False)","5267aa37":"This is the measure used to see how close the data is to actual taxi fares","deb01932":"We need to clean up the data to match the test data","a40553d7":"# The code is using feature engineering from github","0986bbcb":"Next, we'll load our data set.","5e1b8e28":"Now join the data frames on the hourly time key","e728470f":"The data is based on taxi fares. This code was set up as a pipeline in the GCP code. I've refactored it into a Jupyter notebook\n<p>\nUsing code from the coursera class which is at github: https:\/\/github.com\/kariato\/training-data-analyst With the actual code being at https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst\/tree\/master\/courses\/machine_learning\/feateng\/taxifare\n\n<p>\nThis is my first python note books so it has bugs please fork and tell me were I went wrong","02d3f2c6":"# Constants and Hyper-parameters","e0940ecd":"Take the panda data and use the estimator functions to turn it into processed data","4d21249c":"## Set Up\nIn this first cell, we'll load the necessary libraries.","e35b6e5a":"# Calculate Time of each ride\nThe calcuating the hour and week day for millions of rows is costly so we pre-calcualte all possible values","621f450b":"# Predict using the estimator","ee371b42":"# Build an estimator starting from INPUT COLUMNS.\n     These include feature transformations and synthetic features.\n     The model is a wide-and-deep model.","2cfc8076":"**Learning Objectives:**\n  * Use a pre-definfed DNNLinearCombinedRegressor  estimator of the `Estimator` class in TensorFlow to predict taxi rides\n","697bc808":"## Examine the data\n\nIt's a good idea to get to know your data a little bit before you work with it.\n\nWe'll print out a quick summary of a few useful statistics on each column.\n\nThis will include things like mean, standard deviation, max, min, and various quantiles.","7c9d3ddf":"## **Build a neural network model**\n\nIn this exercise, we'll be trying to predicttaxi fares. Ok get all the features into a dictionary","b697e0b7":"# Feature Engineering on data set"}}