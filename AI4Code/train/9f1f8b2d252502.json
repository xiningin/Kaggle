{"cell_type":{"8188772a":"code","5fff5a24":"code","d5a9ae03":"code","011f8c0a":"code","3c3301d9":"code","2de829cf":"code","141e5127":"code","ad908b3b":"code","f3b91d85":"code","d9591bd3":"code","d0e0889d":"code","f1eafa0e":"code","53a54e80":"code","fdafb100":"code","007a0b13":"markdown","d75f280c":"markdown","8cb80fca":"markdown","a62450be":"markdown","c1badf77":"markdown","ac772531":"markdown","c74e8bfa":"markdown","4e241cf7":"markdown","cdc16452":"markdown","772245d5":"markdown"},"source":{"8188772a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#!pip install datatable > \/dev\/null\nimport datatable as dt\nimport matplotlib.pyplot as plt\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5fff5a24":"##starting timer\nimport time\nstart = time.time()\nstart","d5a9ae03":"\ntrain = dt.fread('..\/input\/jane-street-market-prediction\/train.csv').to_pandas()\n\n##transform resp to action\ntrain['action']=(train['resp']>0).astype('int')\n\n\n##grouping features(this step is from carlmcbrideellis's work)\ngrouplists=[['feature_7','feature_9','feature_11','feature_13','feature_15'],['feature_8','feature_10','feature_12','feature_14','feature_16'],\n            ['feature_17','feature_19','feature_21','feature_23','feature_25'],['feature_18','feature_20','feature_22','feature_24','feature_26'],\n            ['feature_27','feature_29','feature_31','feature_33','feature_35'],['feature_28','feature_30','feature_32','feature_34','feature_36'],\n            ['feature_55','feature_56','feature_57','feature_58','feature_59'],['feature_72','feature_73','feature_74','feature_75','feature_76'],\n            ['feature_78','feature_79','feature_80','feature_81','feature_82'],['feature_84','feature_85','feature_86','feature_87','feature_88'],\n            ['feature_90','feature_91','feature_92','feature_93','feature_94'],['feature_96','feature_97','feature_98','feature_99','feature_100'],\n            ['feature_102','feature_103','feature_104','feature_105','feature_106'],['feature_108','feature_109','feature_110','feature_111','feature_112'],\n            ['feature_114','feature_115','feature_116','feature_117','feature_118'],['feature_120','feature_122','feature_124','feature_126','feature_128'],\n            ['feature_121','feature_123','feature_125','feature_127','feature_129']]\nresp_group=[]\nfeatures=['feature_7','feature_9','feature_11','feature_13','feature_15','feature_8','feature_10','feature_12','feature_14','feature_16',\n            'feature_17','feature_19','feature_21','feature_23','feature_25','feature_18','feature_20','feature_22','feature_24','feature_26',\n            'feature_27','feature_29','feature_31','feature_33','feature_35','feature_28','feature_30','feature_32','feature_34','feature_36',\n            'feature_55','feature_56','feature_57','feature_58','feature_59','feature_72','feature_73','feature_74','feature_75','feature_76',\n            'feature_78','feature_79','feature_80','feature_81','feature_82','feature_84','feature_85','feature_86','feature_87','feature_88',\n            'feature_90','feature_91','feature_92','feature_93','feature_94','feature_96','feature_97','feature_98','feature_99','feature_100',\n            'feature_102','feature_103','feature_104','feature_105','feature_106','feature_108','feature_109','feature_110','feature_111','feature_112',\n            'feature_114','feature_115','feature_116','feature_117','feature_118','feature_120','feature_122','feature_124','feature_126','feature_128',\n            'feature_121','feature_123','feature_125','feature_127','feature_129']\nfor i,groups in enumerate(grouplists):\n    resp_group.append(groups[4])","011f8c0a":"tr_plt=train['action'].groupby(train['date']).sum()\nfig, ax = plt.subplots(figsize=(8,3))\nax.set_xlabel (\"time\", fontsize=18)\nax.set_ylabel (\"action\", fontsize=18)\nax.set_title(\"Action Trend\", fontsize=20)\ntr_plt.plot(lw=3)\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nsm.graphics.tsa.plot_acf(tr_plt, lags=24)\nplt.title('Autocorrelation of action', fontsize=18)\nplt.xlabel('Lag', fontsize=18)\nplt.ylabel('Autocorrelation', fontsize=18)\nplt.show()","3c3301d9":"from sklearn import datasets, ensemble\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n##feature scaling\nfrom sklearn import preprocessing\nscaler=preprocessing.StandardScaler().fit(train[resp_group])\ntrain1=scaler.transform(train[resp_group])\ntrain1=pd.DataFrame(train1)\n\n\nt=pd.concat([train1,train['date'],train['weight'],train['action']],ignore_index=True,axis=1)\n\n##focus on data points of which weight is not 0\ntrain2=t[t[18]!=0]\ntrain2.columns=['feature_15',\n 'feature_16',\n 'feature_25',\n 'feature_26',\n 'feature_35',\n 'feature_36',\n 'feature_59',\n 'feature_76',\n 'feature_82',\n 'feature_88',\n 'feature_94',\n 'feature_100',\n 'feature_106',\n 'feature_112',\n 'feature_118',\n 'feature_128',\n 'feature_129','date','weight','action']\ntrain2.head()","2de829cf":"train2['action']","141e5127":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n#source:\n# https:\/\/www.kaggle.com\/aimind\/bottleneck-encoder-mlp-keras-tuner-8601c5#PurgedGroupTimeSeriesSplit\n# https:\/\/github.com\/getgaurav2\/scikit-learn\/blob\/d4a3af5cc9da3a76f0266932644b884c99724c57\/sklearn\/model_selection\/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train\/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train\/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups \/\/ n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n                \n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n \n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n            \n            \n            if self.verbose > 0:\n                    pass\n                    \n            yield [int(i) for i in train_array], [int(i) for i in test_array]","ad908b3b":"import xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n#train3=train2[train2['date']<55]\ntrain3=train[train['date']<55]\nparam = { 'max_depth':16,\n          'learning_rate':0.001}\nimport warnings\nwarnings.filterwarnings('ignore')\nX=train3[features].values\ny=train3['action'].values\n#x_train, x_test, y_train, y_test = train_test_split(train2, train2['action'], test_size=0.1, random_state=13)\ngkf = PurgedGroupTimeSeriesSplit(n_splits = 2, group_gap=20)\nsplits = list(gkf.split(y, groups=train3['date'].values))\n\nxgb_models=[]\nfor fold, (train_indices, test_indices) in enumerate(splits):\n    \n    X_train, X_test = X[train_indices], X[test_indices]\n    y_train, y_test = y[train_indices], y[test_indices]\n    trainset=xgb.DMatrix(X_train,y_train)\n    clf=xgb.train(param, trainset)\n    xgb_models.append(clf)\n    X_test=xgb.DMatrix(X_test)\n    predictions = clf.predict(X_test)\n    predi=np.where(predictions >= 0.5, 1, 0).astype(int)\n    print(classification_report(y_test,predi))","f3b91d85":"##encode with autoencoder \n##source from https:\/\/www.tensorflow.org\/tutorials\/generative\/autoencoder,\n##https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, GaussianNoise\nfrom tensorflow import keras\ndef Autoencoder(latent_dim,input_dim,output_dim):\n    inputs=Input(input_dim)\n    encode=BatchNormalization()(inputs)\n    encode = GaussianNoise(0.05)(encode)\n    for i in range(1,2):\n        encode=Dense(latent_dim, activation='relu')(encode)\n        encode=Dropout(0.2)(encode)\n    decoded=Dense(output_dim, activation='sigmoid')(encode)\n\n    autoencoder = Model(inputs=inputs,outputs=decoded)\n\n    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n    return autoencoder","d9591bd3":"##limit days in dataset when using neural network method\ntrain3=train[train['date']<60]\ntrain3=train3.dropna()\nx_train, x_test, y_train, y_test = train_test_split(train3[features], train3['action'], test_size=0.2, random_state=13)\nx_train, x_test, y_train, y_test=x_train.values, x_test.values, y_train.values, y_test.values\ny_train = y_train.reshape((-1,1))\ny_test = y_test.reshape((-1,1))\nlatent_dim=256\ninput_dim=x_train.shape[-1]\noutput_dim=y_train.shape[-1]\nautoencoder=Autoencoder(latent_dim,input_dim,output_dim)\nautoencoder.fit(x_train,y_train,epochs=100,batch_size=4800,shuffle=True,validation_data=(x_test,y_test))\nautoencoder.save_weights('.\/autoencoder_weights')","d0e0889d":"x_train.shape[-1]\ny_train.shape[-1]","f1eafa0e":"from keras.layers import Dense, Activation, Dropout, advanced_activations,Concatenate\nfrom keras import backend as K\nimport keras\n\ntrain3=train3.dropna()\nX=train3[features].values\ny=train3['action'].values\ngkf = PurgedGroupTimeSeriesSplit(n_splits = 3, group_gap=20)\nsplits = list(gkf.split(y, groups=train3['date'].values))\ndef mlp(input_dim,output_dim,autoencoder):\n    inputs=Input(input_dim)\n    encoded=autoencoder(inputs)\n    encoded = Concatenate()([encoded,inputs])\n    normalized=BatchNormalization()(encoded)\n    x=Dense(128)(normalized)\n    x=Activation('tanh')(x)\n    x=Dropout(0.2)(x)\n    #for i in range(1,2):\n    x = tf.keras.layers.Dense(128)(x)\n    #x = tf.keras.layers.BatchNormalization()(x)\n    #x = tf.keras.layers.Activation('relu')(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x=Dense(1, activation='sigmoid')(x)\n\n    mlp_model=Model(inputs=inputs,outputs=x)\n\n    keras.initializers.he_normal(seed=None)\n    mlp_model.compile(optimizer='adam',\n                  loss='binary_crossentropy')\n                  #metrics=\n    return mlp_model\n\n#for i in range(0,3):\n#    model=mlp(input_dim,output_dim,autoencoder)\n#    model.load_weights(f'..\/input\/weights\/mlp_{i}_timecv.index')\n#    models.append(model)\nmodels=[]\nFOLDS = 3\nSEED = 30\nfor fold, (train_indices, test_indices) in enumerate(splits):\n    X_train, X_test = X[train_indices], X[test_indices]\n    y_train, y_test = y[train_indices], y[test_indices]\n    input_dim=X_train.shape[-1]\n    output_dim=y_train.shape[-1]\n    #model.save_weights(f'.\/model_{SEED}_{fold}.hdf5')\n    #model.compile(Adam(hp.get('lr')\/100),loss='mse')\n    model=mlp(input_dim,output_dim,autoencoder)\n    model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=48,batch_size=4800)\n    #model.fit(X_test,y_test,epochs=3,batch_size=4096)\n    models.append(model)\n    model.save_weights(f'.\/mlp_{fold}_timecv')\n    ","53a54e80":"for fold, (train_indices, test_indices) in enumerate(splits):    \n    X_train, X_test = X[train_indices], X[test_indices]\n    y_train, y_test = y[train_indices], y[test_indices]\n    \n    predicted=models[0](X_test, training=False)\n    predi=np.where(predicted >= 0.5, 1, 0).astype(int)\n    print(classification_report(y_test,predi))","fdafb100":"##end time\nend=time.time()\nend-start ##time in seconds","007a0b13":"try several ways to accelerate Tuning XGBoost \nimport time\nimport xgboost as xgb\n1.parallel jobs\nevaluate the effect of the number of threads of XGBoosting process\nresults = []\nnum_threads = [1, 2, 3, 4,5,6]\nfor n in num_threads:\n    start = time.time()\n    model = xgb.XGBClassifier()(nthread=n)\n    model.fit(train[resp_group][0:200], train['action'][0:200])\n    elapsed = time.time() - start\n    print(n, elapsed)\n    results.append(elapsed)\nplot results\n\nplt.plot(num_threads, results)\nplt.ylabel('Speed (seconds)')\nplt.xlabel('Number of Threads')\nplt.title('XGBoost Training Speed vs Number of Threads')\nplt.show()","d75f280c":"**tune hyper parameter for xgboost**\n\nimport warnings\nwarnings.filterwarnings('ignore')\ntrain3=train2[train2['date']<20]\nimport xgboost as xgb\nxgb1 = xgb.XGBClassifier(nthread=3)\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nx_train, x_test, y_train, y_test = train_test_split(train3[resp_group], train3['action'], test_size=0.1, random_state=13)\n\nparameters = {'objective':['binary:logistic'],\n              'learning_rate': [.001,.01,0.03],\n              'max_depth': [8,12,16],\n              'tree_method':['gpu_hist'],\n              'n_estimators': [500]}\n\nxgb_grid = GridSearchCV(xgb1,\n                        parameters,\n                        cv = 2,\n                        n_jobs=3,\n                        verbose=True,scoring='roc_auc')\nxgb_grid.fit(x_train,y_train)\n\nresult=pd.DataFrame.from_dict(xgb_grid.cv_results_).sort_values(by=['rank_test_score'])\nresult.head(1)","8cb80fca":"**MLP method(include encode layer with trained encoder in the above step)**","a62450be":"**Encode with autoencoder**","c1badf77":"# Thank you!","ac772531":"In order to avoid seperating dataset when data are from the same day, I used this special spliting method to split into train set and test set. https:\/\/www.kaggle.com\/aimind\/bottleneck-encoder-mlp-keras-tuner-8601c5#PurgedGroupTimeSeriesSplit\n\nhttps:\/\/github.com\/getgaurav2\/scikit-learn\/blob\/d4a3af5cc9da3a76f0266932644b884c99724c57\/sklearn\/model_selection\/_split.py#L2243\n","c74e8bfa":"**Tune hyper parameter for xgboost using grid search method.**","4e241cf7":"# Exploratory Data Analysis\n\n1. Since the data is related with time, a straight forward guess would be that maybe the variable that we are predicting is distributed related with time. However, I tested this guess by running autocorrelation,and plotting the trend of total 'actions' per day. Seems there is no obvious relation between time and 'action'. We can assume that 'action' is only related with the features.\n2. There are lots of excellent work in visualzing the trend of features. We do not have extra information about features except for a table relating features and tags. One possible way of grouping the features with tags could be truncated SVD, which can decrease the number of tag groups. You can check out this method at this  website.[https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.TruncatedSVD.html](http:\/\/) I tried this method but it works not as good as visualizing them and grouping. Therefore, I used Carl McBride Ellis's grouping method. https:\/\/www.kaggle.com\/carlmcbrideellis\/jane-street-eda-of-day-0-and-feature-importance We only need to focus one response variable in resp, resp1, resp2, resp3, resp4. On the other hand, the features are seperately related with resp, resp1,etc. I focus on features related with resp.","cdc16452":"# Please don't forget to upvote if you find it somehow helpful! (This version is not for submission because of timeout. Still working on this problem, welcome any possible solution.)","772245d5":"**Following is the pre-processing of data. Scaling features could make a difference when fitting a XGBoost model.**"}}