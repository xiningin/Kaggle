{"cell_type":{"ed2b94d1":"code","8991d72b":"code","c0788c44":"code","c0955eb8":"code","bbd1ce63":"code","46bb185c":"code","bfe8909d":"code","40bb5015":"markdown"},"source":{"ed2b94d1":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.feature_extraction import DictVectorizer\n\nimport time\nimport os\n\nfrom sklearn.feature_extraction.text import HashingVectorizer\n# allowing FeatureUnion to use the scratch surface\nos.environ['JOBLIB_TEMP_FOLDER'] = '.'","8991d72b":"train_df = pd.read_csv('\/kaggle\/input\/tryinclass\/train.csv', parse_dates=['posted_date', 'expiration_date', 'teacher_first_project_date'])\ntest_df = pd.read_csv('\/kaggle\/input\/tryinclass\/test.csv', parse_dates=['posted_date', 'expiration_date', 'teacher_first_project_date'])","c0788c44":"# based on poor man's calculation (a little bit wrong one) and guestimation for 2018\nbudgets ={    \n    2013: 4.283420e+07,\n    2014: 6.440082e+07,\n    2015: 7.252042e+07,\n    2016: 9.196045e+07,\n    2017: 9.196045e+07,\n    2018: 10.000000e+07\n}\n\nclass CustomFeatures(BaseEstimator, TransformerMixin):\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, df):\n        df['project_lifespan'] = (df['expiration_date'] - df['posted_date']).dt.total_seconds() \/ (3600*24.0)\n        df['teacher_experience'] = (df['posted_date'] - df['teacher_first_project_date']).dt.total_seconds() \/ (3600*24.0)\n        df['teacher_activity'] = df['teachers_project_no'] \/ (df['teacher_experience'] \/ 365.0)\n        total_text_len = 0\n        for tc in text_columns:\n            str_len = df[tc].str.len()\n            total_text_len += str_len\n            df['{}_len'.format(tc)] = str_len\n        df['total_text_len'] = total_text_len\n        df['budget'] = df.posted_date.dt.year.apply(lambda v: budgets[v])\n        df['adjusted_cost'] = df['cost'] \/ df['budget']\n        # datasets are sorted, normally transformer should ensure that\n        for year in df.posted_date.dt.year.unique():\n            df.loc[df.posted_date.dt.year == year, 'cumulative_adjusted_cost'] = df.loc[df.posted_date.dt.year == year].adjusted_cost.cumsum()\n        return df\n\nclass ItemSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_dict):\n        return data_dict[self.key]\n\nclass FillNa(BaseEstimator, TransformerMixin):\n    def __init__(self, value):\n        self.value = value\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, X):\n        return X.fillna(self.value)\n    \nclass LoggingTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, message):\n        self.message = message\n\n    def fit(self, X, y=None):\n        print(\"{}: {}, dataset shape: {}\".format(time.strftime(\"%Y-%m-%d %H:%M:%S\",\n                                                               time.gmtime()), self.message, X.shape))\n        return self\n\n    def transform(self, X):\n        return X\n    \nclass DF2Dict(BaseEstimator, TransformerMixin):\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, df):\n        return df.to_dict('records')","c0955eb8":"dv_columns = ['school_id', 'teacher_id', 'teachers_project_no', 'project_type', 'project_category',\n              'project_subcategory', 'project_grade_level', 'resource_category', 'adjusted_cost', 'cumulative_adjusted_cost',\n               'teacher_gender', 'metro_type', 'free_lunch_percentage', 'state', 'city', 'county',\n               'district']\n\ndate_features = ['posted_date', 'expiration_date', 'teacher_first_project_date']\n\ntext_columns = ['project_title', 'project_essay', 'project_description', 'project_statement']\n\nextra_features = ['project_lifespan', 'teacher_experience', 'teacher_activity']\n\ntext_len_columns = ['project_title_len', 'project_essay_len', 'project_description_len', 'project_statement_len', 'total_text_len']\n","bbd1ce63":"pipeline = lambda: Pipeline([\n    ('log1', LoggingTransformer(\"Starting feature extraction pipeline\")),\n    ('custom_features', CustomFeatures()),\n    ('log2', LoggingTransformer(\"Processed custom features, moving to pipeline\")),    \n    ('feature_union', FeatureUnion([\n        ('dv_features', Pipeline([\n            ('log1', LoggingTransformer(\"Applying dict vectorizer\")),            \n            ('selector', ItemSelector(dv_columns + extra_features + text_len_columns)),\n            ('df2dict', DF2Dict()),\n            ('vectorizer', DictVectorizer()),\n            ('log2', LoggingTransformer(\"End of dict vectorizer\"))\n        ])),            \n        ('text_features0', Pipeline([\n            ('log1', LoggingTransformer(\"Applying hashing vectorizer\")),\n            ('selector', ItemSelector(text_columns[0])),            \n            ('fillna', FillNa('')),      \n            ('vectorizer',\n             HashingVectorizer(stop_words='english', alternate_sign=False, analyzer='word', ngram_range=(1, 1), strip_accents='ascii')),\n            ('log2', LoggingTransformer(\"End of hashing vectorizer\")),\n        ])),\n        ('text_features1', Pipeline([\n            ('log1', LoggingTransformer(\"Applying hashing vectorizer\")),\n            ('selector', ItemSelector(text_columns[1])),            \n            ('fillna', FillNa('')),        \n            ('vectorizer',\n             HashingVectorizer(stop_words='english', alternate_sign=False, analyzer='word', ngram_range=(1, 1), strip_accents='ascii')),\n            ('log2', LoggingTransformer(\"End of hashing vectorizer\")),\n        ])),\n        ('text_features2', Pipeline([\n            ('log1', LoggingTransformer(\"Applying hashing vectorizer\")),\n            ('selector', ItemSelector(text_columns[2])),            \n            ('fillna', FillNa('')),     \n            ('vectorizer',\n             HashingVectorizer(stop_words='english', alternate_sign=False, analyzer='word', ngram_range=(1, 1), strip_accents='ascii')),\n            ('log2', LoggingTransformer(\"End of hashing vectorizer\")),\n        ])),\n        ('text_features3', Pipeline([\n            ('log1', LoggingTransformer(\"Applying hashing vectorizer\")),\n            ('selector', ItemSelector(text_columns[3])),            \n            ('fillna', FillNa('')),        \n            ('vectorizer',\n             HashingVectorizer(stop_words='english', alternate_sign=False, analyzer='word', ngram_range=(1, 1), strip_accents='ascii')),\n            ('log2', LoggingTransformer(\"End of hashing vectorizer\")),\n        ])),        \n    ], n_jobs=1)),\n    ('log3', LoggingTransformer(\"Done\")),\n]\n)\npp = pipeline()\ny = train_df['funded_or_not']\nX = pp.fit_transform(train_df)\nX_test = pp.transform(test_df)","46bb185c":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n\nclf = lgb.LGBMClassifier(\n    num_leaves=80, \n    max_depth=-1, \n    random_state=42, \n    n_jobs=-1, \n    n_estimators=1000,\n    colsample_bytree=0.7,\n    subsample=0.7,\n    learning_rate=0.05\n)\n\nclf.fit(X_train, y_train,\n        early_stopping_rounds=10, \n        eval_metric = 'auc', \n        eval_set = [(X_train, y_train), (X_val, y_val)],\n        eval_names = ['train', 'valid'],\n        verbose = 10\n)","bfe8909d":"threshold = 0.5\ny_test_proba = clf.predict_proba(X_test)[:,1]\ny_test = np.where(y_test_proba > threshold, 1, 0)\nsubmission = test_df[['project_id']].copy()\nsubmission['funded_or_not'] = y_test\nsubmission.to_csv('submission.csv', index=False)","40bb5015":"Some foreword:\n1. Joined quite late to the party so no detailed exploratory analysis was done\n2. I cretated some simple, most intuitive features - like project lifespan and teacher experience\n3. In last hour of competition I have seen the trend in projects costs over the year (being higher and higher) - I tried to calculate and guestimate each years budget (made some mistakes there) and adjusted the costs by it (it gave me a bump)\n4. On top of that I tried to represent within-a-year seasonality with cumulative sum, in test set for year 2017 it's totally wrongly calculated (cause test set does not start from beginning of 2017). It still gave me some boost as 2018 is calculated properly.\n5. Last but not least - I treated this as a typical Kaggle competition - optimizing for score, not model usefuleness or it being free from bias. To create production model different features should be used of course (heavily focusing on pricing and textual features, not reinforcing teacher's experience or gender \/ geographical biases)"}}