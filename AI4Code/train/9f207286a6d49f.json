{"cell_type":{"c896508a":"code","39ff160e":"code","6e057ce2":"code","dde3d2f9":"code","ea19b871":"code","77753ed7":"code","fce46f85":"code","015bcacc":"code","6b733c6f":"code","8834e3e9":"code","35128c8d":"code","ff4c71e7":"code","e4d84923":"code","ab1c3bfe":"code","a58fd0ce":"code","12011639":"code","515e7679":"code","43618538":"code","5ff27833":"code","fe86dd75":"code","875d5a1e":"code","14233a4b":"code","121f42d5":"markdown","dfe38af6":"markdown","ebbd4e96":"markdown","4fba21e8":"markdown","9dd4ca3d":"markdown","1eb0017c":"markdown","04585029":"markdown","2e3f8b9b":"markdown","ce2b9491":"markdown","e205a43d":"markdown","bda841da":"markdown"},"source":{"c896508a":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow\nfrom tensorflow.keras.layers import Input, Lambda, Dense, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom glob import glob\n\n","39ff160e":"IMAGE_SIZE = [224, 224]\n\ntrain_path = \"\/content\/drive\/MyDrive\/sign_data_mod_mod\/train\"\ntest_path  = \"\/content\/drive\/MyDrive\/sign_data_mod_mod\/test\"","6e057ce2":"train_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)","dde3d2f9":"training_set = train_datagen.flow_from_directory(train_path,\n                                                 target_size = (224, 224),\n                                                 batch_size = 32,\n                                                 class_mode = 'categorical')","ea19b871":"test_set = test_datagen.flow_from_directory(test_path,\n                                            target_size = (224, 224),\n                                            batch_size = 32,\n                                            class_mode = 'categorical')","77753ed7":"training_set.class_indices","fce46f85":"test_set.class_indices\n","015bcacc":"def Show_Image(img_arr):\n  fig,axes=plt.subplots(1,5,figsize=(20,20))\n  axes=axes.flatten()\n\n  for img,ax in zip(img_arr,axes):\n    ax.imshow(img)\n  \n  plt.tight_layout()\n  plt.show()","6b733c6f":"images=[training_set[0][0][0] for i in range(6)]\nShow_Image(images)","8834e3e9":"images=[test_set[0][0][0] for i in range(6)]\nShow_Image(images)","35128c8d":"model_mobilenet=MobileNetV2(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)","ff4c71e7":"for layer in model_mobilenet.layers:\n    layer.trainable = False","e4d84923":"folders = glob('\/content\/drive\/MyDrive\/sign_data_mod_mod\/train\/*')","ab1c3bfe":"folders","a58fd0ce":"x = Flatten()(model_mobilenet.output)","12011639":"prediction = Dense(len(folders), activation='softmax')(x)\n","515e7679":"model = Model(model_mobilenet.input, outputs=prediction)","43618538":"model.summary()","5ff27833":"model.compile(\n  loss='categorical_crossentropy',\n  optimizer='adam',\n  metrics=[tensorflow.keras.metrics.AUC(),\"accuracy\"])\n","fe86dd75":"result = model.fit(\n  training_set,\n  validation_data=test_set,\n  epochs=50,\n  steps_per_epoch=len(training_set),\n  validation_steps=len(test_set)\n)","875d5a1e":"plt.plot(result.history['loss'], label='Training Loss')\nplt.plot(result.history['val_loss'], label='Validation Loss')\nplt.legend()\nplt.show()\nplt.savefig('Loss_Value')\n\n\nplt.plot(result.history['accuracy'], label='Training Accuracy')\nplt.plot(result.history['val_accuracy'], label='Validation Accuracy')\nplt.legend()\nplt.show()\nplt.savefig('Accuracy_Value')","14233a4b":"model.save('\/content\/drive\/MyDrive\/sign_data_mod_mod\/Model_MobileNetV2.h5')","121f42d5":"MOBILENETV2 MODEL FOR TRAINING","dfe38af6":"MODEL TRAINING","ebbd4e96":"DATA AUGMENTATION","4fba21e8":"LOSS AND ACCURACY","9dd4ca3d":"SIGNATURE VERIFICATION ","1eb0017c":"DATA AUGMENTED IMAGES\n\n\n","04585029":"THE FINAL MODEL","2e3f8b9b":"TESTING DATASET","ce2b9491":"READING THE DATA PATH (SigComp Dataset)","e205a43d":"TRAINING DATASET","bda841da":"IMPORT THE LIBRARIES"}}