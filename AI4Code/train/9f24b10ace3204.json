{"cell_type":{"79cdb7c6":"code","9b7e2d07":"code","54e4ab70":"code","d3b0da5c":"code","5f096c04":"code","6d5f5ed8":"code","45052d89":"code","ac60e219":"code","e6336eac":"code","76f8e81b":"code","822f4af8":"code","3aa42474":"code","ca6309b5":"code","7e8ea93e":"code","e9be01d6":"code","fb0fc483":"code","feb102ee":"code","aba4f24b":"code","c9cefc85":"code","74d14a94":"code","26687365":"code","5cea3a5b":"code","bc8277cc":"code","3a3613c5":"code","6eed8f01":"code","ac072f9b":"code","73a2d1c7":"code","081bd491":"code","b9b6e5d1":"code","f5ab473e":"code","df6c7b75":"markdown","80028410":"markdown","09a8c6ad":"markdown","5dac66d0":"markdown","377d0f02":"markdown"},"source":{"79cdb7c6":"!pip install tensorflow_addons==0.9.1\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nimport pandas as pd\nimport numpy as np\nimport random\nfrom tensorflow.keras.callbacks import Callback, LearningRateScheduler\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import losses, models, optimizers\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import f1_score\nimport gc\nimport os\nfrom matplotlib import pyplot as plt","9b7e2d07":"EPOCHS = 200\nNNBATCHSIZE = 16\nGROUP_BATCH_SIZE = 4000\nLR = 0.0015\nSPLITS = 5\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","54e4ab70":"def read_data():\n    train = pd.read_csv('\/kaggle\/input\/data-without-drift-with-kalman-filter\/train.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n    test = pd.read_csv('\/kaggle\/input\/data-without-drift-with-kalman-filter\/test.csv', dtype={'time': np.float32, 'signal': np.float32})\n    sub = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv', dtype={'time': np.float32})\n    \n    Y_train_proba = np.load(\"\/kaggle\/input\/ion-shifted-rfc-proba\/Y_train_proba.npy\")\n    Y_test_proba = np.load(\"\/kaggle\/input\/ion-shifted-rfc-proba\/Y_test_proba.npy\")\n    \n    for i in range(11):\n        train[f\"proba_{i}\"] = Y_train_proba[:, i]\n        test[f\"proba_{i}\"] = Y_test_proba[:, i]\n\n    return train, test, sub","d3b0da5c":"def batching(df, batch_size):\n    df['group'] = df.groupby(df.index\/\/batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df['group'] = df['group'].astype(np.uint16)\n    return df","5f096c04":"def normalize(train, test):\n    train_input_mean = train.signal.mean()\n    train_input_sigma = train.signal.std()\n    train['signal'] = (train.signal - train_input_mean) \/ train_input_sigma\n    test['signal'] = (test.signal - train_input_mean) \/ train_input_sigma\n    return train, test","6d5f5ed8":"def generate_signal_shift(df, windows):\n    for window in windows:    \n        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n    return df","45052d89":"def run_feat_engineering(df, batch_size):\n    df = batching(df, batch_size = batch_size)\n    df = generate_signal_shift(df, [1, 2])\n    df['signal_2'] = df['signal'] ** 2\n    df['signal_3'] = df['signal'] ** 3\n    return df","ac60e219":"def feature_selection(train, test):\n    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n    train = train.replace([np.inf, -np.inf], np.nan)\n    test = test.replace([np.inf, -np.inf], np.nan)\n    for feature in features:\n        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n        train[feature] = train[feature].fillna(feature_mean)\n        test[feature] = test[feature].fillna(feature_mean)\n    return train, test, features","e6336eac":"def Classifier(shape_):\n    \n    def wave_block(x, filters, kernel_size, n):\n        dilation_rates = [2**i for i in range(n)]\n        x = Conv1D(filters = filters,\n                   kernel_size = 1,\n                   padding = 'same')(x)\n        res_x = x\n        for dilation_rate in dilation_rates:\n            tanh_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same', \n                              activation = 'tanh', \n                              dilation_rate = dilation_rate)(x)\n            sigm_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same',\n                              activation = 'sigmoid', \n                              dilation_rate = dilation_rate)(x)\n            x = Multiply()([tanh_out, sigm_out])\n            x = Conv1D(filters = filters,\n                       kernel_size = 1,\n                       padding = 'same')(x)\n            res_x = Add()([res_x, x])\n        return res_x\n    \n    inp = Input(shape = (shape_))\n    x = BatchNormalization()(inp)\n    x = wave_block(x, 16, 3, 12)\n    x = BatchNormalization()(x)\n    x = wave_block(x, 32, 3, 8)\n    x = BatchNormalization()(x)\n    x = wave_block(x, 64, 3, 4)\n    x = BatchNormalization()(x)\n    x = wave_block(x, 128, 3, 1)\n    x = BatchNormalization()(x)\n    x = Dropout(0.1)(x)\n    out = Dense(11, activation = 'softmax', name = 'out')(x)\n    \n    model = models.Model(inputs = inp, outputs = out)\n    \n    opt = Adam(lr = LR)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(loss = losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['categorical_accuracy'])\n    return model","76f8e81b":"def lr_schedule(epoch):\n    if epoch < 30:\n        lr = LR\n    elif epoch < 40:\n        lr = LR \/ 3\n    elif epoch < 50:\n        lr = LR \/ 5\n    elif epoch < 60:\n        lr = LR \/ 7\n    elif epoch < 70:\n        lr = LR \/ 9\n    elif epoch < 80:\n        lr = LR \/ 11\n    elif epoch < 90:\n        lr = LR \/ 13\n    else:\n        lr = LR \/ 100\n    return lr","822f4af8":"class EarlyStoppingAtMaxMacroF1(Callback):\n    \"\"\"Stop training when the MacroF1 is at its max.\n    \"\"\"\n\n    def __init__(self, model, inputs, targets, epochs, patience=0):\n        super(EarlyStoppingAtMaxMacroF1, self).__init__()\n\n        self.model = model\n        self.inputs = inputs\n        self.targets = np.argmax(targets, axis=2).reshape(-1)\n        self.patience = patience\n\n        # best_weights to store the weights at which the minimum loss occurs.\n        self.best_weights = None\n        self.last_epoch = epochs - 1\n\n    def on_train_begin(self, logs=None):\n        # The number of epoch it has waited when loss is no longer minimum.\n        self.wait = 0\n        # The epoch the training stops at.\n        self.stopped_epoch = 0\n        # Initialize the best as negative infinity.\n        self.best = np.NINF\n\n    def on_epoch_end(self, epoch, logs=None):\n        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n        current_score = f1_score(self.targets, pred, average='macro')\n        print(f'F1 Macro Score: {current_score:.6f}')\n\n        if np.greater(current_score, self.best):\n            self.best = current_score\n            self.wait = 0\n            # Record the best weights if current results is better (less).\n            self.best_weights = self.model.get_weights()\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.stopped_epoch = epoch\n                self.model.stop_training = True\n                print('Restoring model weights from the end of the best epoch.')\n                self.model.set_weights(self.best_weights)\n        if epoch == self.last_epoch:\n            self.model.set_weights(self.best_weights)\n\n    def on_train_end(self, logs=None):\n        if self.stopped_epoch > 0:\n            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))","3aa42474":"def run_cv_model_by_batch(train, test, n_splits, batch_col, feats, sample_submission, nn_epochs, nn_batch_size, seed):\n    \n    seed_everything(seed)\n    K.clear_session()\n    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n    tf.compat.v1.keras.backend.set_session(sess)\n    oof_ = np.zeros((len(train), 11)) # build out of folds matrix with 11 columns, they represent our target variables classes (from 0 to 10)\n    preds_ = np.zeros((len(test), 11))\n    target = ['open_channels']\n    group = train['group']\n    kf = GroupKFold(n_splits=n_splits)\n    splits = [x for x in kf.split(train, train[target], group)]\n\n    new_splits = []\n    for sp in splits:\n        new_split = []\n        new_split.append(np.unique(group[sp[0]]))\n        new_split.append(np.unique(group[sp[1]]))\n        new_split.append(sp[1])    \n        new_splits.append(new_split)\n    # pivot target columns to transform the net to a multiclass classification structure\n    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n\n    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n    target_cols = ['target_'+str(i) for i in range(11)]\n    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n\n    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n        print(f'Our training dataset shape is {train_x.shape}')\n        print(f'Our validation dataset shape is {valid_x.shape}')\n\n        gc.collect()\n        shape_ = (None, train_x.shape[2])\n        model = Classifier(shape_)\n        \n        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n        model.fit(train_x,train_y,\n                  epochs = nn_epochs,\n                  callbacks = [cb_lr_schedule, EarlyStoppingAtMaxMacroF1(model, valid_x, valid_y, nn_epochs, patience=40)],\n                  batch_size = nn_batch_size,verbose = 2,\n                  validation_data = (valid_x,valid_y))\n        preds_f = model.predict(valid_x)\n        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro')\n        print(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :.6f}')\n        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n        oof_[val_orig_idx,:] += preds_f\n        te_preds = model.predict(test)\n        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n        preds_ += te_preds \/ n_splits\n        model.save(f\"model_seed_{seed}_fold_{n_fold}.mdl\")\n    # calculate the oof macro f1_score\n    f1_score_ = f1_score(np.argmax(train_tr, axis = 2).reshape(-1),  np.argmax(oof_, axis = 1), average = 'macro')\n    print(f'Training completed. oof macro f1 score : {f1_score_:.6f}')\n\n    # save predictions\n    np.save(f\"preds_{seed}.npf\", preds_)\n    sample_submission['open_channels'] = np.argmax(preds_, axis = 1).astype(int)\n    sample_submission.to_csv(f'submission_wavenet_{seed}.csv', index=False, float_format='%.4f')","ca6309b5":"def run_everything(seed):\n    \n    print('Reading Data Started.')\n    train, test, sample_submission = read_data()\n    print('Reading Data Completed.')\n    \n    print('Normalizing Data Started.')\n    train, test = normalize(train, test)\n    print('Normalizing Data Completed.')\n        \n    print('Feature Engineering Started.')\n    train = train[:3640000].append(train[3840000:], ignore_index=True) # removed noise from train data\n    print(f'Train shape: {train.shape}')\n    \n    train = run_feat_engineering(train, batch_size = GROUP_BATCH_SIZE)\n    test = run_feat_engineering(test, batch_size = GROUP_BATCH_SIZE)\n    train, test, features = feature_selection(train, test)\n    print('Feature Engineering Completed.')\n\n    print(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started.')\n    run_cv_model_by_batch(train, test, SPLITS, 'group', features, sample_submission, EPOCHS, NNBATCHSIZE, seed)\n    print('Training completed.')","7e8ea93e":"seeds = [1, 2, 3] # we used more seeds\nfor seed in seeds:\n    run_everything(seed)","e9be01d6":"sample_sub = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv')\n\nfile_name = 'submission_wavenet_{}.csv'\nsubmissions = [pd.read_csv(file_name.format(i)) for i in seeds]\n\nprint(sample_sub.shape)\nfor s in submissions:\n    print(s.shape)","fb0fc483":"predictions = [sub['open_channels'] for sub in submissions]","feb102ee":"res_round_median = np.round(np.median(predictions, axis=0)).astype(int)\nres_max = np.max(predictions, axis=0).astype(int)","aba4f24b":"# check the difference\ndiff = (res_max - res_round_median)\nunique, counts = np.unique(diff, return_counts=True)\ndict(zip(unique, counts))","c9cefc85":"train_df, test_df, sample_submission = read_data()","74d14a94":"# check how we can split train data by groups\nplt.figure(figsize=(15, 8))\nplt.plot(train_df[\"time\"], train_df[\"signal\"], color=\"grey\")\nplt.title(\"Signals (Clean train data)\", fontsize=20)\nplt.xlabel(\"Time\", fontsize=18)\nplt.ylabel(\"Signal\", fontsize=18)\nplt.show()","26687365":"def get_groups_data(train_df):\n    A = train_df[:1000000]\n    B = train_df[1000000:1500000].append(train_df[3000000:3500000], ignore_index=True)\n    C = train_df[1500000:2000000].append(train_df[3500000:3640000], ignore_index=True)\\\n                                 .append(train_df[3840000:4000000], ignore_index=True) # removed noise for group C\n    D = train_df[2000000:2500000].append(train_df[4500000:5000000], ignore_index=True)\n    E = train_df[2500000:3000000].append(train_df[4000000:4500000], ignore_index=True)\n    return A, B, C, D, E","5cea3a5b":"A, B, C, D, E = get_groups_data(train_df)","bc8277cc":"plt.figure(figsize=(15, 8))\nplt.plot(A.time, A.signal)\nplt.plot(B.time, B.signal)\nplt.plot(C.time, C.signal)\nplt.plot(D.time, D.signal)\nplt.plot(E.time, E.signal)\nplt.title(\"Signals (Clean train data without noise, colored by groups)\", fontsize=20)\nplt.xlabel(\"Time\", fontsize=18)\nplt.ylabel(\"Signal\", fontsize=18)\nplt.show()","3a3613c5":"datasets = {\n    'A': A,\n    'B': B,\n    'C': C,\n    'D': D,\n    'E': E,\n}","6eed8f01":"def combine_predictions(df, batch_length, datasets):\n    sub_final = pd.DataFrame()\n    sub_final['time'] = sample_sub['time']\n    sub_final['open_channels'] = sample_sub['open_channels']\n\n    batch_len = batch_length\n    for i in range(int(len(df) \/ batch_len)):\n        df_batch = df[i * batch_len:i * batch_len + batch_len]\n\n        batch_mean = df_batch['signal'].mean()\n        default_group_name, default_group = next(iter(datasets.items()))\n        min_mean_diff = np.abs(batch_mean - default_group['signal'].mean())\n        clf_name = default_group_name\n\n        for name, data in datasets.items():\n            dataset_mean = data['signal'].mean()\n            mean_diff = np.abs(batch_mean - dataset_mean)\n            if mean_diff < min_mean_diff:\n                min_mean_diff = mean_diff\n                clf_name = name\n\n        # use max(predictions) for the group with the high signal average number (D) and round(median(predictions)) for everything else (A, B, C, E).\n        if (clf_name=='A'):\n            sub_final.loc[i * batch_len:i * batch_len + batch_len - 1, 'open_channels'] = res_round_median[i * batch_len:i * batch_len + batch_len]\n        elif (clf_name=='B'):\n            sub_final.loc[i * batch_len:i * batch_len + batch_len - 1, 'open_channels'] = res_round_median[i * batch_len:i * batch_len + batch_len]\n        elif (clf_name=='C'):\n            sub_final.loc[i * batch_len:i * batch_len + batch_len - 1, 'open_channels'] = res_round_median[i * batch_len:i * batch_len + batch_len]\n        elif (clf_name=='D'):\n            sub_final.loc[i * batch_len:i * batch_len + batch_len - 1, 'open_channels'] = res_max[i * batch_len:i * batch_len + batch_len]\n        elif (clf_name=='E'):\n            sub_final.loc[i * batch_len:i * batch_len + batch_len - 1, 'open_channels'] = res_round_median[i * batch_len:i * batch_len + batch_len]\n            \n        print(f\"group_{clf_name} prediction: {i} batch data\")\n    return sub_final","ac072f9b":"sub_final = combine_predictions(test_df, 100000, datasets)","73a2d1c7":"# check if combine_predictions correctly defines each group\nplt.figure(figsize=(15, 8))\nplt.plot(test_df[\"time\"], test_df[\"signal\"], color=\"grey\")\nplt.title(\"Signals (Clean test data)\", fontsize=20)\nplt.xlabel(\"Time\", fontsize=18)\nplt.ylabel(\"Signal\", fontsize=18)\nplt.show()","081bd491":"# we can also combine our predictions by simple hard-coded solution, so let's check if there is no difference:\nres_combined = np.concatenate([res_round_median[:500000], res_max[500000:600000], res_round_median[600000:700000], res_max[700000:800000], res_round_median[800000:]])\n\ndiff = (sub_final['open_channels'] - res_combined)\nunique, counts = np.unique(diff, return_counts=True)\ndict(zip(unique, counts))","b9b6e5d1":"sub_final.to_csv('wavenet_monte_carlo.csv', index=False, float_format='%.4f')","f5ab473e":"sub_final","df6c7b75":"### Here is our 17th place simple Wavenet solution (Private LB: 0.94494, Public LB: 0.94600). You can find a short description in [this topic](https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/discussion\/153829).\n\n### Acknowledgements:\n- Our solution is based on [WaveNet-Keras](https:\/\/www.kaggle.com\/siavrez\/wavenet-keras) and [Wavenet with SHIFTED-RFC Proba and CBR](https:\/\/www.kaggle.com\/nxrprime\/wavenet-with-shifted-rfc-proba-and-cbr) kernels.\n- We were using a dataset with [removed drift and Kalman filtering](https:\/\/www.kaggle.com\/michaln\/data-without-drift-with-kalman-filter) (which is based on [data-without-drift](https:\/\/www.kaggle.com\/cdeotte\/data-without-drift) and [kalman-filtering](https:\/\/www.kaggle.com\/teejmahal20\/a-signal-processing-approach-kalman-filtering)).\n- Also we used [ION-SHIFTED-RFC-PROBA](https:\/\/www.kaggle.com\/sggpls\/ion-shifted-rfc-proba) dataset (based on [SHIFTED-RFC Pipeline](https:\/\/www.kaggle.com\/sggpls\/shifted-rfc-pipeline) kernel, see [this discussion](https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/discussion\/144645)) as additional features.\n\nFor simplicity, in this kernel we use already created data from public datasets and not create it from scratch as it takes a while (all scripts can be found in the links above).","80028410":"### Ensembling:","09a8c6ad":"### Create final submission:\nThe final estimator looks like `max(predictions)` for the group with the high signal average number (we denote this group as `D`) and `round(median(predictions))` for everything else.","5dac66d0":"### Generating predictions:\nUnder the following section we generate seeds for our ensemble technique inspired by [Monte Carlo method](https:\/\/en.wikipedia.org\/wiki\/Monte_Carlo_method).\n\n**Note**: for simplicity, here is an example which uses only 3 predictions, in our final submissions we used more seeds.","377d0f02":"### Code:"}}