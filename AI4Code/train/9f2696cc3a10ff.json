{"cell_type":{"93c36db5":"code","ee24ff5d":"code","ebf9ebad":"code","09ef22f7":"code","3a38315d":"code","5939445b":"code","290f5f38":"code","8fa1b046":"code","bd27a792":"code","55b49dcc":"code","c7cf6df1":"code","261c35fe":"code","61c3ba54":"code","37fff4c8":"code","da5bd725":"code","1ec3c47f":"code","172a1c8e":"code","b28d5d55":"code","4f3d8940":"code","32206d61":"code","283d76e9":"code","8af32d61":"code","97326025":"code","9130b715":"code","d8d751e6":"code","91a8b075":"code","6c3212e0":"code","0d41868b":"code","e7d506ea":"code","b34cd40a":"code","372c6b21":"code","cb231215":"code","24113fc8":"code","f91f5305":"code","1585382c":"code","28cfcf4b":"code","5db2aeb8":"code","fcbcf116":"code","63208e38":"code","74bf854a":"code","1c468ab6":"code","c8fa41f8":"code","624b8fe4":"code","7e558548":"code","4d4bb1f4":"code","ad7e1520":"code","98052f96":"code","d1ae6f24":"code","71ab65e1":"code","5bf83990":"code","00defac4":"code","9d8c394d":"code","fe7d1200":"code","b3f86afc":"code","7571421e":"code","abc9c92d":"code","c036b1d3":"code","c1dae911":"code","184c541e":"code","2147fbe2":"markdown","440e3800":"markdown","6b5e06e4":"markdown","ebfa5317":"markdown","d177a58b":"markdown","17363f4f":"markdown","6676ee40":"markdown"},"source":{"93c36db5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ee24ff5d":"trdata = pd.read_csv('\/kaggle\/input\/machine-learning-24-hrs-hackathon\/train_SJC.csv', header = 1)\ntsdata = pd.read_csv('\/kaggle\/input\/machine-learning-24-hrs-hackathon\/Test_SJC.csv')\nspdata = pd.read_csv('\/kaggle\/input\/machine-learning-24-hrs-hackathon\/sample_submission.csv')","ebf9ebad":"#Using test data to fix the column header issue of train data \ncols = list(tsdata.columns)\n#Appending the Y-value column header\ncols.append('UltimateIncurredClaimCost')\ntrdata.columns = cols","09ef22f7":"# Making a backup copy of the train data\nbktrdata = trdata.copy()","3a38315d":"#Finding in which columns do Missing Values fall in\ntrdata.isnull().sum()","5939445b":"trdata.MaritalStatus.value_counts(dropna = False)","290f5f38":"# Since 'U' stands for Unknown Marrital Status \ntrdata.MaritalStatus = trdata.MaritalStatus.fillna('U')\ntrdata.MaritalStatus.value_counts(dropna = False)","8fa1b046":"trdata.isnull().sum()","bd27a792":"# Mean Median and Mode are Similar\n#Since median is an integer we use median\nprint('Mean:',trdata.HoursWorkedPerWeek.mean(skipna=True))\nprint('Median:',trdata.HoursWorkedPerWeek.median(skipna=True))\nprint('Mode:',trdata.HoursWorkedPerWeek.mode())","55b49dcc":"# To see the distribution of Days Worked per weak where the hours worked was missing\ntmpdata = trdata[trdata.HoursWorkedPerWeek.isnull()]\ntmpdata.DaysWorkedPerWeek.value_counts()","c7cf6df1":"#Finding indices for missing values sorted by Days worked per week\nidx3 = tmpdata[tmpdata.DaysWorkedPerWeek==3].index\nidx4 = tmpdata[tmpdata.DaysWorkedPerWeek==4].index\nidx5 = tmpdata[tmpdata.DaysWorkedPerWeek==5].index\nidx6 = tmpdata[tmpdata.DaysWorkedPerWeek==6].index\n\n#Imputing missing values of number of hours worked per week based on number of days worked per week\ntmpmed3 = trdata.HoursWorkedPerWeek[trdata.DaysWorkedPerWeek==3].median()\ntrdata.loc[idx3,'HoursWorkedPerWeek'] = tmpmed3\ntmpmed4 = trdata.HoursWorkedPerWeek[trdata.DaysWorkedPerWeek==4].median()\ntrdata.loc[idx4,'HoursWorkedPerWeek'] = tmpmed4\ntmpmed5 = trdata.HoursWorkedPerWeek[trdata.DaysWorkedPerWeek==5].median()\ntrdata.loc[idx5,'HoursWorkedPerWeek'] = tmpmed5\ntmpmed6 = trdata.HoursWorkedPerWeek[trdata.DaysWorkedPerWeek==6].median()\ntrdata.loc[idx6,'HoursWorkedPerWeek'] = tmpmed6","261c35fe":"#Imputing median Wage for missing Wages \nprint(trdata.WeeklyWages.median(skipna=True))\ntmpmed = trdata.WeeklyWages.median(skipna=True)\ntrdata.WeeklyWages = trdata.WeeklyWages.fillna(tmpmed)","61c3ba54":"#Another Backup Just in case\nanbktrdata = trdata.copy()","37fff4c8":"#trdata = anbktrdata.copy()","da5bd725":"def cont_var_analysis(x):\n    analysis = pd.Series([x.mean(), x.median(), x.mode()[0], x.min(),\n                         x.quantile(0.05), x.quantile(0.25),\n                         x.quantile(0.50),x.quantile(0.75),\n                         x.quantile(0.95), x.max()],\n                         index = ['Mean', 'Median', 'Mode',\n                             'Minimum','5%', '25%', '50%', \n                              '75%','95%','Maximum'])\n    \n    \n    return analysis.round(2)","1ec3c47f":"cols = ['WeeklyWages','HoursWorkedPerWeek', 'DaysWorkedPerWeek',\n        'InitialIncurredCalimsCost' ]\ncols_y = ['WeeklyWages','HoursWorkedPerWeek', 'DaysWorkedPerWeek',\n        'InitialIncurredCalimsCost', 'UltimateIncurredClaimCost']","172a1c8e":"#Outlier analysis for different Variables\ntrdata[cols_y].apply(cont_var_analysis)\n","b28d5d55":"#Dealing with the outliers with and without involving y \n#By clipping them to 5%-95%\ntrdata_y = trdata.copy()\ntrdata[cols] = trdata[cols].apply(lambda x: x.clip(lower = x.quantile(0.05), \n                                                   upper = x.quantile(0.95)))\n\ntrdata_y[cols_y] = trdata[cols_y].apply(lambda x: x.clip(lower = x.quantile(0.05), \n                                                   upper = x.quantile(0.95)))\ntrdata_y[cols_y].apply(cont_var_analysis)","4f3d8940":"#Train dataset with no engineered features\ntrdata_nf = trdata.copy()","32206d61":"#trdata = trdata_nf.copy()","283d76e9":"#Using Claim description to distuingush different areas of the body for injury\ntrdata['ClaimDescription'].head()","8af32d61":"#Function to signal whether or not a particular set of words are present\ndef Wordsin(sentence, word):\n    \n    # To break the sentence in words\n    s = sentence.split(\" \")\n    for j in word:\n        for i in s:\n            #Double loop for running through \n            #Description and word list\n            if (i == j):\n                return 1\n    return 0","97326025":"#Arm Region\nwords = ['WRIST','ARM','FINGER','HAND','SHOULDER', 'ARMS', 'WRISTS','FINGERS',\n         'HANDS', 'SHOULDERS', 'FOREARM', 'FOREARMS', 'BICEP', 'BICEPS', 'PALM', \n         'PALMS']\ntrdata['Arms'] = trdata['ClaimDescription'].apply(lambda x: Wordsin(x,words))\ntsdata['Arms'] = trdata['ClaimDescription'].apply(lambda x: Wordsin(x,words))","9130b715":"#Waist and below\nwords = ['FEET','FOOT','TOE','TOES', 'KNEE',\n        'KNEES', 'LEG', 'LEGS','HIP', 'HIPS',\n        'THIGH', 'THIGHS', 'ANKLE', 'ANKLES',\n        'HEEL', 'HEELS', 'SHIN', 'SHINS']\ntrdata['Legs'] = trdata['ClaimDescription'].apply(lambda x: Wordsin(x,words))\ntsdata['Legs'] = trdata['ClaimDescription'].apply(lambda x: Wordsin(x,words))","d8d751e6":"#Neck and Above\nwords = ['HEAD','NECK','FACE','LIP', 'LIPS',\n        'MOUTH', 'CHIN', 'NOSE','TOUNGE', \n        'TOOTH','TEETH', 'EAR', 'EARS',\n        'EYE', 'EYES', 'FOREHEAD']\ntrdata['Head'] = trdata['ClaimDescription'].apply(lambda x: Wordsin(x,words))\ntsdata['Head'] = trdata['ClaimDescription'].apply(lambda x: Wordsin(x,words))","91a8b075":"#Follows the same principle as others but to exclusivley determine if there is a \n#bone injury\nwords = ['BONE','BONES','FRACTURE','FRACTURES',\n         'FRACTURED', 'CRACK', 'RIB', 'RIBS',\n         'SPINE', 'BROKEN']\ntrdata['Bone'] = trdata['ClaimDescription'].apply(lambda x: Wordsin(x,words))\ntsdata['Bone'] = trdata['ClaimDescription'].apply(lambda x: Wordsin(x,words))","6c3212e0":"#Time elapse between date of accident and date of claim\nADays = pd.DatetimeIndex(trdata['DateTimeOfAccident']).date\nCDays = pd.DatetimeIndex(trdata['DateReported']).date\ntrdata['Delayed'] = CDays - ADays\n\nADays = pd.DatetimeIndex(tsdata['DateTimeOfAccident']).date\nCDays = pd.DatetimeIndex(tsdata['DateReported']).date\ntsdata['Delayed'] = CDays - ADays\n","0d41868b":"trdata['Delayed'].head()","e7d506ea":"# Function to convert timedelta64[ns] to Integer\ndef days(text):\n    d,_ = text.split()\n    return int(d)\ntrdata['Delayed'] = trdata['Delayed'].astype(str).apply(days)\ntsdata['Delayed'] = tsdata['Delayed'].astype(str).apply(days)","b34cd40a":"trdata['Delayed'].head()","372c6b21":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport math","cb231215":"sns.boxplot(data=trdata, x='Age', hue='UltimateIncurredClaimCost')","24113fc8":"sns.boxplot(data=trdata, x='WeeklyWages', hue='UltimateIncurredClaimCost')","f91f5305":"sns.pairplot(data=trdata)","1585382c":"plt.figure(figsize=[16,6])\nsns.pointplot(data=trdata, x='Age', y='UltimateIncurredClaimCost')","28cfcf4b":"plt.figure(figsize=[16,6])\nplt.scatter(trdata.InitialIncurredCalimsCost,  trdata.UltimateIncurredClaimCost)","5db2aeb8":"x1 = trdata.InitialIncurredCalimsCost.apply(lambda x: math.log(x))\nplt.figure(figsize=[16,6])\nplt.scatter(x1,  trdata.UltimateIncurredClaimCost)","fcbcf116":"plt.figure(figsize=[16,6])\nsns.scatterplot(data = trdata, x='WeeklyWages',  y='UltimateIncurredClaimCost', hue='MaritalStatus')","63208e38":"plt.figure(figsize=[16,6])\nsns.scatterplot(data = trdata, x='Age',  y='UltimateIncurredClaimCost', hue='MaritalStatus')","74bf854a":"plt.figure(figsize=[16,6])\nplt.scatter(trdata.HoursWorkedPerWeek,  trdata.UltimateIncurredClaimCost)","1c468ab6":"# All the significant columns\nfinal_cols1 = ['Age', 'Gender','Arms', 'Legs', 'Head', 'Bone',\n               'MaritalStatus', 'DependentChildren', 'DependentsOther',\n               'WeeklyWages','PartTimeFullTime', 'HoursWorkedPerWeek', \n               'DaysWorkedPerWeek','InitialIncurredCalimsCost', 'Delayed']\nprint(\"Length of all the significant Predictors:\",len(final_cols1))\n\n#All the significant columns before feature engineering\nfinal_cols2 = [  'Age', 'Gender','MaritalStatus', 'DependentChildren', \n               'DependentsOther', 'WeeklyWages','PartTimeFullTime', \n               'HoursWorkedPerWeek', 'DaysWorkedPerWeek',\n               'InitialIncurredCalimsCost']\nprint(\"Length of all the significant regressors before feature engineering:\",len(final_cols2))","c8fa41f8":"#Importing relevent modules\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport sklearn.preprocessing as pre\nimport sklearn.model_selection as ms\nimport sklearn.linear_model as lm\nfrom sklearn.metrics import mean_squared_error\nimport math","624b8fe4":"#Label Encoding\ny = trdata['UltimateIncurredClaimCost']\ny1 = trdata_y['UltimateIncurredClaimCost']\ntrdf1 = trdata.loc[:,final_cols2]\n#Converting all the non-int data type to int data types\nle=pre.LabelEncoder()\nfor x in trdf1.select_dtypes(include=['object', 'float']).columns:\n    trdf1[x]=le.fit_transform(trdf1[x])","7e558548":"trdf2 = trdata.loc [:,final_cols1]\n#Converting all the non-int data type to int data types\nle=pre.LabelEncoder()\nfor x in trdf2.select_dtypes(include=['object', 'float']).columns:\n    trdf2[x]=le.fit_transform(trdf2[x])","4d4bb1f4":"#Using the same random state for every model \nx_train1,x_test1,y_train1,y_test1=ms.train_test_split(trdf1, y, test_size = 0.3, random_state=12345)","ad7e1520":"#Model: Linear regression 1 - without feature engineering, and without outlier treating the y-variable\nglm1 = lm.LinearRegression()\nglm1.fit(x_train1, y_train1)\n\nprint('Test score: ',glm1.score(x_test1,y_test1))\nprint('Train score:',glm1.score(x_train1,y_train1))\n\ny_pred_test = glm1.predict(x_test1)\ny_pred_train = glm1.predict(x_train1)\nrmse_test = np.sqrt(mean_squared_error(y_test1, y_pred_test))\nrmse_train = np.sqrt(mean_squared_error(y_train1, y_pred_train))\n\nprint('\\nTest rmse:',rmse_test)\nprint('Train rmse:', rmse_train)","98052f96":"#Using the same random state for every model \nx_train2,x_test2,y_train2,y_test2=ms.train_test_split(trdf2, y, test_size = 0.3, random_state=12345)","d1ae6f24":"#Model: Linear regression 2 - with added features, and with outlier treating the y-variable\nglm2 = lm.LinearRegression()\nglm2.fit(x_train2, y_train2)\n\nprint('Test score: ',glm2.score(x_test2,y_test2))\nprint('Train score:',glm2.score(x_train2,y_train2))\n\ny_pred_test = glm2.predict(x_test2)\ny_pred_train = glm2.predict(x_train2)\nrmse_test = np.sqrt(mean_squared_error(y_test2, y_pred_test))\nrmse_train = np.sqrt(mean_squared_error(y_train2, y_pred_train))\n\nprint('\\nTest rmse:',rmse_test)\nprint('Train rmse:', rmse_train)","71ab65e1":"#Using the same random state for every model \nx_train3,x_test3,y_train3,y_test3=ms.train_test_split(trdf1, y1, test_size = 0.3, random_state=12345)","5bf83990":"#Model: Linear regression 3 - with feature engineering, and with outlier treating the y-variable\nglm3 = lm.LinearRegression()\nglm3.fit(x_train3, y_train3)\n\nprint('Test score: ',glm3.score(x_test3,y_test3))\nprint('Train score:',glm3.score(x_train3,y_train3))\n\ny_pred_test = glm3.predict(x_test3)\ny_pred_train = glm3.predict(x_train3)\nrmse_test = np.sqrt(mean_squared_error(y_test3, y_pred_test))\nrmse_train = np.sqrt(mean_squared_error(y_train3, y_pred_train))\n\nprint('\\nTest rmse:',rmse_test)\nprint('Train rmse:', rmse_train)","00defac4":"#Using the same random state for every model \nx_train4,x_test4,y_train4,y_test4=ms.train_test_split(trdf2, y1, test_size = 0.3, random_state=12345)","9d8c394d":"#Model: Linear regression 4 - with added features, and with outlier treating the y-variable\nglm4 = lm.LinearRegression()\nglm4.fit(x_train4, y_train4)\n\nprint('Test score: ',glm4.score(x_test4,y_test4))\nprint('Train score:',glm4.score(x_train4,y_train4))\n\ny_pred_test = glm4.predict(x_test4)\ny_pred_train = glm4.predict(x_train4)\nrmse_test = np.sqrt(mean_squared_error(y_test4, y_pred_test))\nrmse_train = np.sqrt(mean_squared_error(y_train4, y_pred_train))\n\nprint('\\nTest rmse:',rmse_test)\nprint('Train rmse:', rmse_train)","fe7d1200":"#Using the same random state for every model \nx_train1,x_test1,y_train1,y_test1=ms.train_test_split(trdf1, y, test_size = 0.3, random_state=12345)\n\n#Model: Gradient Boost regression 1 - without feature engineering, and without outlier treating the y-variable\nGBR1 = GradientBoostingRegressor()\nGBR1.fit(x_train1, y_train1)\n\nprint('Test score: ',GBR1.score(x_test1,y_test1))\nprint('Train score:',GBR1.score(x_train1,y_train1))\n\ny_pred_test = GBR1.predict(x_test1)\ny_pred_train = GBR1.predict(x_train1)\nrmse_test = np.sqrt(mean_squared_error(y_test1, y_pred_test))\nrmse_train = np.sqrt(mean_squared_error(y_train1, y_pred_train))\n\nprint('\\nTest rmse:',rmse_test)\nprint('Train rmse:', rmse_train)\n#GradientBoostingRegressor","b3f86afc":"#Using the same random state for every model \nx_train2,x_test2,y_train2,y_test2=ms.train_test_split(trdf2, y, test_size = 0.3, random_state=12345)\n\n#Model: GRadient Boost regression 2 - with added features, and with outlier treating the y-variable\nGBR2 = GradientBoostingRegressor()\nGBR2.fit(x_train2, y_train2)\n\nprint('Test score: ',GBR2.score(x_test2,y_test2))\nprint('Train score:',GBR2.score(x_train2,y_train2))\n\ny_pred_test = GBR2.predict(x_test2)\ny_pred_train = GBR2.predict(x_train2)\nrmse_test = np.sqrt(mean_squared_error(y_test2, y_pred_test))\nrmse_train = np.sqrt(mean_squared_error(y_train2, y_pred_train))\n\nprint('\\nTest rmse:',rmse_test)\nprint('Train rmse:', rmse_train)\n#GradientBoostingRegressor","7571421e":"#Using the same random state for every model \nx_train3,x_test3,y_train3,y_test3=ms.train_test_split(trdf1, y1, test_size = 0.3, random_state=12345)\n\n#Model: Gradient Boost regression 3 - with feature engineering, and with outlier treating the y-variable\nGBR3 = GradientBoostingRegressor()\nGBR3.fit(x_train3, y_train3)\n\nprint('Test score: ',GBR3.score(x_test3,y_test3))\nprint('Train score:',GBR3.score(x_train3,y_train3))\n\ny_pred_test = GBR3.predict(x_test3)\ny_pred_train = GBR3.predict(x_train3)\nrmse_test = np.sqrt(mean_squared_error(y_test3, y_pred_test))\nrmse_train = np.sqrt(mean_squared_error(y_train3, y_pred_train))\n\nprint('\\nTest rmse:',rmse_test)\nprint('Train rmse:', rmse_train)\n#GradientBoostingRegressor","abc9c92d":"#Using the same random state for every model \nx_train4,x_test4,y_train4,y_test4=ms.train_test_split(trdf2, y1, test_size = 0.3, random_state=12345)\n\n#Model: Gradient Boost regression 4 - with added features, and with outlier treating the y-variable\nGBR4 = GradientBoostingRegressor()\nGBR4.fit(x_train4, y_train4)\n\nprint('Test score: ',GBR4.score(x_test4,y_test4))\nprint('Train score:',GBR4.score(x_train4,y_train4))\n\ny_pred_test = GBR4.predict(x_test4)\ny_pred_train = GBR4.predict(x_train4)\nrmse_test = np.sqrt(mean_squared_error(y_test4, y_pred_test))\nrmse_train = np.sqrt(mean_squared_error(y_train4, y_pred_train))\n\nprint('\\nTest rmse:',rmse_test)\nprint('Train rmse:', rmse_train)","c036b1d3":"tsdata = tsdata.loc[:,final_cols1]\ntsdata.isnull().sum()","c1dae911":"tsdata = tsdata.fillna('U')\n#Converting all the non-int data type to int data types\nle=pre.LabelEncoder()\nfor x in tsdata.select_dtypes(include=['object', 'float']).columns:\n    tsdata[x]=le.fit_transform(tsdata[x])","184c541e":"pred = glm2.predict(tsdata)\nspdata['UltimateIncurredClaimCost'] = pred\n\nspdata.to_csv('submission_sample1.csv', index=False)\n\npred = GBR2.predict(tsdata)\nspdata['UltimateIncurredClaimCost'] = pred\n\nspdata.to_csv('submission_sample2.csv', index=False)\n\npred = glm4.predict(tsdata)\nspdata['UltimateIncurredClaimCost'] = pred\n\nspdata.to_csv('submission_sample3.csv', index=False)\n\npred = GBR4.predict(tsdata)\nspdata['UltimateIncurredClaimCost'] = pred\n\nspdata.to_csv('submission_sample4.csv', index=False)","2147fbe2":"# Feature Engineering","440e3800":"# Model Building","6b5e06e4":"# Data Preprocessing\n#### Missing Value Treatment","ebfa5317":"# EDA","d177a58b":"# Data Loading\n#### Importing Data","17363f4f":"#### Outlier Analysis","6676ee40":"# Reason for Model selection\nThe reason linear regression and Stocastic graient Boost regression is to keep the models simple. And to keep the models from overfitting as the score is determined by unseen data (To the the model). It was in my best interest to not overfit or to perform too complicated actions that would be way too specific towards the given data. "}}