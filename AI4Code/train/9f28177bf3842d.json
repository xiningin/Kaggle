{"cell_type":{"e59afd36":"code","f0d8d43a":"code","4be5a497":"code","5beb9446":"code","414dd938":"code","dc3d2577":"code","983a20fb":"code","00a311ea":"code","c06f6d51":"code","86d1351e":"code","d9e74e68":"code","ed319ff9":"code","6cd65b9a":"code","b3f00be8":"code","c9513551":"code","a853b484":"code","3cc7c151":"code","9fa2f874":"code","d1ba4475":"code","48ef062c":"code","79d9043a":"code","fb40b693":"code","c020dcd9":"code","be2b9e72":"code","29cee12f":"code","fe8a9bd5":"code","581062d1":"code","16fb1ed7":"code","83846d3e":"markdown","a9acbda6":"markdown","ba78e29e":"markdown","fc0157fe":"markdown","ac7dbef0":"markdown","010bc3d6":"markdown","29c76cf8":"markdown","2dd33a5c":"markdown","ab7b7d35":"markdown","e67d36f4":"markdown","6342cd17":"markdown","55fe12fb":"markdown","88b83f8f":"markdown","bee7fbb1":"markdown","e670825b":"markdown","62bd8aeb":"markdown","0445d88c":"markdown","d2ed5dad":"markdown","894981d1":"markdown"},"source":{"e59afd36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f0d8d43a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n","4be5a497":"train=pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\ntrain.head()","5beb9446":"train.info()\n","414dd938":"test.info()\n","dc3d2577":"train.isnull().sum()\/train.shape[0] * 100\n","983a20fb":"test.isnull().sum()\/test.shape[0] * 100","00a311ea":"train.columns\n","c06f6d51":"test.columns\n","86d1351e":"ID_train=train['Id']\nID_test=test['Id']\ntest=test.drop(columns=['Id'], axis=1)\ntrain.head()\n","d9e74e68":"cat_train=[col for col in train.columns if train[col].dtype=='object']\nnum_train=[col for col in train.columns if train[col].dtype!='object']\n# cat_train\nnum_train","ed319ff9":"cat_test=[col for col in test.columns if test[col].dtype=='object']\nnum_test=[col for col in test.columns if test[col].dtype!='object']\n# cat_test\nnum_test","6cd65b9a":"con_train =[col for col in num_train if train[col].nunique()>25]\ndis_train =[col for col in num_train if train[col].nunique()<25]\nyea_train =[col for col in train.columns if 'Yr' in col or 'Year' in  col or 'yr' in  col or 'YR' in  col]\n\n# con_train\n# dis_train\n# yea_train","b3f00be8":"con_test =[col for col in num_test if test[col].nunique()>25]\ndis_test =[col for col in num_test if test[col].nunique()<25]\nyea_test =[col for col in test.columns if 'Yr' in col or 'Year' in  col or 'yr' in  col or 'YR' in  col]\n# con_test\n# dis_test\nyea_test","c9513551":"from sklearn.impute import SimpleImputer\nnsi = SimpleImputer(strategy='mean')  # For Numerical Features, will replace MISSING NUMERIC values with MEAN\ncsi = SimpleImputer(strategy='most_frequent')  # For Categorical Features, will replace MISSING CATEGORICAL values with MOST FREQUENT value\n\ntrain[cat_train] = csi.fit_transform(train[cat_train])\ntrain[con_train] = nsi.fit_transform(train[con_train])\ntrain[dis_train] = nsi.fit_transform(train[dis_train])\n\ntrain.head()","a853b484":"test[cat_test] = csi.fit_transform(test[cat_test])\ntest[con_test] = nsi.fit_transform(test[con_test])\ntest[dis_test] = nsi.fit_transform(test[dis_test])\n\ntest.head()","3cc7c151":"# train[con_train]=np.log(train[con_train])\n# test[con_test]= np.log(test[con_test])\ntrain.head()\n# test.head()","9fa2f874":"from datetime import date\ntrain[yea_train]=date.today().year - train[yea_train]\ntest[yea_test]=date.today().year - test[yea_test]\ntrain.head()\n# test.head()","d1ba4475":"from sklearn.preprocessing import StandardScaler\nss= StandardScaler()\ntrain[dis_train]= ss.fit_transform(train[dis_train])\ntest[dis_test]= ss.fit_transform(test[dis_test])\ntrain.head()\n# test.head()","48ef062c":"train1= pd.get_dummies(train, columns=cat_train, drop_first= True)\ntest1= pd.get_dummies(test, columns=cat_test, drop_first= True)\n","79d9043a":"train2=pd.concat([train,train1],axis=1)\ntest2=pd.concat([test,test1],axis=1)\n","fb40b693":"train=train2.drop(cat_train,axis=1)\ntest=test2.drop(cat_test,axis=1)\ntrain.head()\n# test.head()","c020dcd9":"train=train.dropna(axis=0,how='any') # I have taken all the necessary features thus dropping null values of unnecessary features\ntest=test.dropna(axis=0,how='any') \ntrain.head()","be2b9e72":"y=train['SalePrice'].iloc[:,0]\n\nX=train.drop(['Id','SalePrice'],axis=1)\ny.head()","29cee12f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\ny_test.head()","fe8a9bd5":"from sklearn.ensemble import GradientBoostingRegressor\nreg=GradientBoostingRegressor()\nreg.fit(X_train,y_train)\n","581062d1":"predict= reg.predict(X_test)\n# predict","16fb1ed7":"from sklearn.metrics import r2_score\nr2_score(predict, y_test)","83846d3e":"# Using the Trained Model to Predict\n","a9acbda6":"# Finding the Data-type of each column\n","ba78e29e":"# Handling Categorical Data using Get_Dummies()\n### Machine learning models require all input and output variables to be numeric.This means that if your data contains categorical data, you must encode it to numbers before you can fit and evaluate a model.\n","fc0157fe":"# Doing the Train_Test_Split\n","ac7dbef0":"# Using GBoost to fit the Data\n","010bc3d6":"# Finding Numerical & Categorical Features (to be treated seperately later)\n### This method is called List Comprehension-where a list is created satisfying some condition\n","29c76cf8":"# Finding the columns in each dataset\n","2dd33a5c":"# Splitting X & y\n","ab7b7d35":"# Imputing the missing values\n### Missing values are one of the most common problems you can encounter when you try to prepareyour data for machine learning. The reason for the missing values might be human errors,interruptions in the data flow, privacy concerns, and so on. Whatever is the reason, missing values affect the performance of the machine learning models.\n","e67d36f4":"# Finding the Percent of null values in each columns\n","6342cd17":"# Some ways you may show Like by\n### Kaggle - Follow me on Kaggle\n### Twitter - https:\/\/twitter.com\/KumarPython\n### LinkedIn - https:\/\/www.linkedin.com\/in\/kumarpython\/\n","55fe12fb":"# Dropping the columns already concatenated after Get_Dummies()\n","88b83f8f":"# Concatenating the Original Dataset & the One after creating Dummies(get_dummies()\n### Get_Dummies() method creates a new DF containing JUST the dummies, MOST People get wrong here)\n","bee7fbb1":"# Standardizing the Discrete Values.\n### Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n","e670825b":"# Finding the following Features (to be treated seperately later)\n### This method is called List Comprehension-where a list is created satisfying some condition\n### * Continuous Features\n### * Discreet Features\n### *  Year Features\n","62bd8aeb":"# Scoring the Trained Model\n","0445d88c":"# Dropping some useless column. \n","d2ed5dad":"# Apply Log Transform on Continuous Data only","894981d1":"# Transforming Dates\n### If you transform the date column into the extracted columns, the information of them become disclosed and machine learning algorithms can easily understand them.\n"}}