{"cell_type":{"c2a49928":"code","58c8721e":"code","3164b9be":"code","647ed610":"code","ee04731f":"code","a2201ec7":"code","e0d4e119":"code","8cab951b":"code","4c71accf":"code","966e0477":"code","4720c07b":"code","ec6dcef5":"code","e0e3736a":"code","b3db80f2":"code","6860a7fd":"code","2cdcb943":"code","50b1850d":"code","ba842fb1":"code","e4bc7684":"code","5fbfe858":"code","60038858":"code","fa2a15dc":"code","cb02b479":"code","805e92e5":"code","a1a31265":"code","fff59a64":"code","a114266e":"code","7d1229ea":"code","6a1e40b1":"code","04e28819":"code","009ee5f6":"code","611639ef":"code","2264ecf3":"code","1c354839":"code","efc23bff":"code","1ea26ab8":"code","c30edd55":"code","09e23efc":"code","b392f0de":"code","ebc4d6b3":"code","53cdac62":"code","bba6cf50":"code","d9b7c56e":"code","75815d9f":"code","a5c3c7a8":"code","a01fb6af":"code","ce10241d":"code","efa66949":"code","ee29f1dd":"code","fc36d47b":"code","d9038276":"code","6fa89bbb":"code","f9c5a0ed":"code","f7a50f8d":"code","36059712":"code","5aef2203":"code","defb500e":"code","88a3c20c":"code","055aee0d":"markdown","d641fe89":"markdown","abc89a8c":"markdown","8bfe109b":"markdown","659d7b4b":"markdown","129b8c16":"markdown","eee10d75":"markdown","340085dc":"markdown","07727865":"markdown","548d83b7":"markdown","24481b2e":"markdown","a7c0be6e":"markdown","8e126d2b":"markdown","316926db":"markdown","9aeedf8b":"markdown","e8ea64d6":"markdown","ab3b3a46":"markdown","4d4e1069":"markdown","fba6b9eb":"markdown","4b8c7c36":"markdown","30514218":"markdown","b4d9c5a3":"markdown","ba73e424":"markdown","f859ee7f":"markdown","972becdb":"markdown","7cbda102":"markdown","00ff2678":"markdown","d548dcdd":"markdown","0c29d1af":"markdown","3007c6e3":"markdown","ab058bed":"markdown","8fee4eba":"markdown","4540fc4e":"markdown","8f5bab42":"markdown","020028ce":"markdown","9749be1e":"markdown","33804468":"markdown"},"source":{"c2a49928":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","58c8721e":"# Reading data\ndf_train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf_test_copy=df_test.copy()","3164b9be":"# Have a first look at train data\nprint(df_train.shape)","647ed610":"# Now, lets explore first five data from training set.\ndf_train.head()","ee04731f":"# We will use describe function to calculate count,mean,max and other for numerical feature.\ndf_train.describe().transpose()","a2201ec7":"# The feature survived contain binary data which can also be seen from its max(1) and min(0) value.","e0d4e119":"# Have a look for possible missing values\ndf_train.info()","8cab951b":"df_train.isnull().sum()","4c71accf":"# We see that Age, Cabin and Embarked feature have NULL values.","966e0477":"# Have a first look at test data\nprint(df_test.shape)","4720c07b":"# Have a look at train and test columns\nprint('Train columns:', df_train.columns.tolist())\nprint('Test columns:', df_test.columns.tolist())","ec6dcef5":"# Let's look at the figures and Understand the Survival Ratio\ndf_train.Survived.value_counts(normalize=True)","e0e3736a":"# We observe that less people survived.","b3db80f2":"# To get better understanding of count of people who survived, we will plot it.","6860a7fd":"import matplotlib.pyplot as plt\nimport seaborn as sns\n","2cdcb943":"sns.countplot(x='Survived',data=df_train)","50b1850d":"sns.countplot(x='Pclass',data=df_train,hue='Survived')","ba842fb1":"sns.countplot(x='Sex',data=df_train,hue='Survived')","e4bc7684":"sns.catplot(x='Sex' , y='Age' , data=df_train , hue='Survived' , kind='violin' , palette=['r','g'] , split=True)","5fbfe858":"sns.kdeplot(df_train.Age , shade=True , color='r')","60038858":"# To fill the missing values, we will calculate median of age with respect to Pclass.\ndf_train.groupby('Pclass').median()","fa2a15dc":"# Now we will create a function to fill missing age values. This function is used to fill the age according to Pclass.\ndef fill_age(cols):\n    \n    Age=cols[0]\n    Pclass=cols[1]\n    \n    if pd.isnull(Age):\n        \n        if Pclass == 1:\n            return 37\n        \n        elif Pclass == 2:\n            return 29\n        \n        else:\n            return 24\n        \n    else:\n        \n        return Age","cb02b479":"df_train['Age'] = df_train[['Age','Pclass']].apply(fill_age,axis=1)","805e92e5":"print(df_train.Age.count())  # Null values filled","a1a31265":"sns.factorplot(x='Sex',y='Age' , col='Pclass', data=df_train , hue='Survived' , kind = 'box', palette=['r','g'])","fff59a64":"# Understanding Box Plot :\n\n# The bottom line indicates the min value of Age.\n# The upper line indicates the max value.\n# The middle line of the box is the median or the 50% percentile.\n# The side lines of the box are the 25 and 75 percentiles respectively.","a114266e":"plt.figure(figsize=(20,30))\nsns.factorplot(x='Embarked' , y ='Fare' , kind='bar', data=df_train , hue='Survived' , palette=['r','g'])","7d1229ea":"plt.figure(figsize=(20,10))\nsns.boxplot(x='Embarked',y='Fare',data=df_train,hue='Survived')","6a1e40b1":"# The best way to fill it would be by most occured value\ndf_train['Embarked'].fillna(df_train['Embarked'].mode()[0] ,inplace=True)","04e28819":"df_train.Embarked.count() # filled the values with Mode.","009ee5f6":"#Since Cabin has so many missing value, we will remove that column.","611639ef":"df_train.drop('Cabin',axis=1,inplace=True)","2264ecf3":"sns.violinplot(x='Embarked' , y='Pclass' , data=df_train , hue='Survived' , palette=['r','g'])","1c354839":"df_train.isnull().sum()","efc23bff":"# None of the columns are empty.","1ea26ab8":"sns.countplot(data=df_train,x='SibSp',hue='Survived')","c30edd55":"df_train[['SibSp','Survived']].groupby('SibSp').mean()","09e23efc":"df_train[['Parch','Survived']].groupby('Parch').mean()","b392f0de":"df_train['Alone'] = 0\ndf_train.loc[(df_train['SibSp']==0) & (df_train['Parch']==0) , 'Alone'] = 1\n\ndf_test['Alone'] = 0\ndf_test.loc[(df_test['SibSp']==0) & (df_test['Parch']==0) , 'Alone'] = 1","ebc4d6b3":"df_train.head()","53cdac62":"drop_features = ['PassengerId' , 'Name' , 'SibSp' , 'Parch' , 'Ticket' ]\n\ndf_train.drop(drop_features , axis=1, inplace = True)","bba6cf50":"df_test.info()","d9b7c56e":"# We have a few Null values in Test (Age , Fare) , let's fill it up.","75815d9f":"df_test['Fare'].fillna(df_test['Fare'].median() , inplace=True)","a5c3c7a8":"df_test.groupby('Pclass').median()","a01fb6af":"def fill_ages(cols):\n    \n    Age=cols[0]\n    Pclass=cols[1]\n    \n    if pd.isnull(Age):\n        \n        if Pclass == 1:\n            return 42\n        \n        elif Pclass == 2:\n            return 26.5\n        \n        else:\n            return 24\n        \n    else:\n        \n        return Age","ce10241d":"df_test['Age'] = df_test[['Age','Pclass']].apply(fill_ages,axis=1)","efa66949":"df_test.info()","ee29f1dd":"drop_featuress = ['PassengerId' , 'Name' , 'SibSp' , 'Parch' , 'Ticket','Cabin' ]\n\ndf_test.drop(drop_featuress , axis=1 , inplace = True)","fc36d47b":"df_test.info()","d9038276":"def mapping(frame):\n    \n    frame['Sex'] = frame.Sex.map({'female': 0 ,  'male': 1}).astype(int)\n    \n    \n    frame['Embarked'] = frame.Embarked.map({'S' : 0 , 'C': 1 , 'Q':2}).astype(int)\n    \n    \n    \n    frame.loc[frame.Age <= 16 , 'Age'] = 0\n    frame.loc[(frame.Age >16) & (frame.Age<=32) , 'Age'] = 1\n    frame.loc[(frame.Age >32) & (frame.Age<=48) , 'Age'] = 2\n    frame.loc[(frame.Age >48) & (frame.Age<=64) , 'Age'] = 3\n    frame.loc[(frame.Age >64) & (frame.Age<=80) , 'Age'] = 4\n    \n    \n    frame.loc[(frame.Fare <= 7.91) , 'Fare'] = 0\n    frame.loc[(frame.Fare > 7.91) & (frame.Fare <= 14.454) , 'Fare'] = 1\n    frame.loc[(frame.Fare > 14.454) & (frame.Fare <= 31) , 'Fare'] = 2\n    frame.loc[(frame.Fare > 31) , 'Fare'] = 3","6fa89bbb":"mapping(df_train)\ndf_train.head()","f9c5a0ed":"mapping(df_test)\ndf_test.head()","f7a50f8d":"# Importing some algorithms from sklearn.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","36059712":"# Splitting data into test and train set.\nx_train,x_test,y_train,y_test=train_test_split(df_train.drop('Survived',axis=1),df_train.Survived,test_size=0.20,random_state=66)","5aef2203":"models = [LogisticRegression(),RandomForestClassifier(),\n        DecisionTreeClassifier()]\n\nmodel_names=['LogisticRegression','RandomForestClassifier','DecisionTree']\n\naccuracy = []\n\nfor model in range(len(models)):\n    clf = models[model]\n    clf.fit(x_train,y_train)\n    pred = clf.predict(x_test)\n    accuracy.append(accuracy_score(pred , y_test))\n    \ncompare = pd.DataFrame({'Algorithm' : model_names , 'Accuracy' : accuracy})\ncompare","defb500e":"params_dict={'criterion':['gini','entropy'],'max_depth':[5.21,5.22,5.23,5.24,5.25,5.26,5.27,5.28,5.29,5.3]}\nclf_dt=GridSearchCV(estimator=DecisionTreeClassifier(),param_grid=params_dict,scoring='accuracy', cv=5)\nclf_dt.fit(x_train,y_train)\npred=clf_dt.predict(x_test)\nprint(accuracy_score(pred,y_test))\nprint(clf_dt.best_params_)","88a3c20c":"predio = clf_dt.predict(df_test)\n\nd = {'PassengerId' : df_test_copy.PassengerId , 'Survived' : predio}\nanswer = pd.DataFrame(d)\n# Generate CSV file based on DecisionTree Classifier\nanswer.to_csv('predio.csv' , index=False)","055aee0d":"It looks OK, the only additional column in train is 'Survived', which is our target variable, i.e. the one we want to actually predict in the test dataset.","d641fe89":"*  Now we are going to drop features which are not contributing much.\n* Names, PassengerId and Ticket Number doesn't help in finding Probability of Survival.\n* We have created Alone feature and therefore I'll be Dropping SibSp and Parch.","abc89a8c":"**Our next step is to examine NULL values.**","8bfe109b":"## **Fare**\n\nCome, let's examine Survival based on Fare.","659d7b4b":"**On examining the chart above, wer can clearly say that people belonging to third class died in large numbers.**","129b8c16":"**We get highest accuracy for DecisionTreeClassifier**","eee10d75":"**We have noticed earlier that column Age has some null values. So. first we will complete the Age column and then we will analyze it.**","340085dc":"# Data Loading\n\n\nOur first step is to extract train and test data. We will be extracting data using pandas function read_csv. Specify the location to the dataset and import them.","07727865":"##  **Sex**\n\nCome, let's examine Survival based on gender.","548d83b7":"## **Embarked**\n\nCome, let's examine Survival based on Embarked.","24481b2e":"## **SibSp**\n\nNow lets analyze SibSp column.","a7c0be6e":"### Lets convert categorical feature into numerical value.\n\n* Sex Attribute has (Male\/Female) , which will be mapped to 0\/1.\n* Divide Age into 5 categories and Map them with 0\/1\/2\/3\/4.\n* Divide Fare into 4 categories and Map them to 0\/1\/2\/3.\n* Embarked Attribute has (S\/C\/Q) , which will be mapped to 0\/1\/2.","8e126d2b":"# Load our plotting libraries","316926db":"## ** Pclass**\n Come, let's examine Survival based on Pclass.","9aeedf8b":" This is default first cell in any kaggle kernel. They import **NumPy** and **Pandas** libraries and it also lists the available Kernel files.** NumPy** is the fundamental package for scientific computing with Python. **Pandas** is the most popular python library that is used for data analysis.","e8ea64d6":"**On examining the chart above, we can clearly say that male are more likely to die in comparision to female.**","ab3b3a46":"# Thank you\n\nGuys,do put your query in comment section and if you like the implementation method, do upvote it. ","4d4e1069":"**Fill the Age with it's Median, and that is because, for a dataset with great Outliers, it is advisable to fill the Null values with median.**","fba6b9eb":"**We can see that those who embarked at C with First Class ticket had a good chance of Survival. Whereas for S, it seems that all classes had nearly equal probability of Survival. And for Q, third Class seems to have Survived and Died with similar probabilities.**","4b8c7c36":"# **Feature Engineering:**","30514218":"## **Cabin**\n\nCome, let's examine Survival based on Embarked.","b4d9c5a3":"**We observe that people who paid more are more likely to survive.**","ba73e424":"**It seems that there individuals having 1 or 2 siblings\/spouses had the highest Probability of Survival, followed by individuals who were Alone.**","f859ee7f":"**Now lets try to tune parameter**\n","972becdb":"# Titanic Tutorial for Beginners[Accuracy: 0.789]-\n\n\n* This is my first tutorial. Do point out my mistakes in comment section.\n* Do upvote if you find this notebook interesting.\n* I have uploaded my second tutorial on this problem statement with better accuracy(https:\/\/www.kaggle.com\/rishabhdhyani4\/titanic-tutorial2). Do check it.","7cbda102":"**Now let us perform some feature engineering to get informative and valuable attributes.**","00ff2678":"# Feature Examining-","d548dcdd":"**Well, DecisionTree did a great job there, with the highest accuracy[82.6%]**\n","0c29d1af":"**Now let us create an attribute 'Alone' so that we could know whether the passenger is travelling alone or not.**","3007c6e3":"**We have noticed earlier that column Embarked has some null values. So. first we will complete this column and then we will analyze it.**","ab058bed":"We got 12 features in our training data. From https:\/\/www.kaggle.com\/c\/titanic\/data, we have:\n\n* Survival = Survival\n* Pclass = Ticket class\n* Sex = Sex\n* Age = Age in years\n* Sibsp = # of siblings \/ spouses aboard the Titanic\n* Parch = # of parents \/ children aboard the Titanic\n* Ticket = Ticket number\n* Fare = Passenger fare\n* Cabin = Cabin number\n* Embarked = Port of Embarkation\n\nQualitative Features (Categorical) : PassengerId , Pclass , Survived , Sex , Ticket , Cabin , Embarked.\n\nQuantitative Features (Numerical) : SibSp , Parch , Age , Fare.\n\nIt is obvious from the problem statement that we have to predict **Survival** feature.","8fee4eba":"## **Age**\n\nCome, let's examine Survival based on gender.","4540fc4e":"**It seems that individuals with 1,2 or 3 family members had a greater Probability of Survival, followed by individuals who were Alone.**","8f5bab42":"So, out of 891 examples only 342 (38%) survived and rest all died.","020028ce":"# **Now, it's right time to choose best model.**","9749be1e":"## **Parch**\n\nNow lets analyze Parch column.","33804468":"By using df_train.shape we get to know that train data has 891 rows and 12 columns."}}