{"cell_type":{"dfa11194":"code","1d2d1aee":"code","3f425391":"code","349c36fd":"code","63c2ed52":"code","41d79fd1":"code","0dcc44bb":"code","22a835f2":"code","199f2e01":"code","11187ae0":"code","e9235ff1":"code","e3698fd3":"code","6a50a709":"code","1e9b1e5b":"code","34486947":"markdown","037e40c8":"markdown","65ec5d83":"markdown","91ec884c":"markdown","fcff0247":"markdown","e1908210":"markdown"},"source":{"dfa11194":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1d2d1aee":"df = pd.read_csv('\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv', parse_dates=[0])\nmask = df.measurable_impressions > 0\ndf['CPM'] = df[mask].total_revenue \/ df[mask].measurable_impressions * 100000 #\u0431\u044b\u0441\u0442\u0440\u0435\u0435\ndf.CPM.fillna(0, inplace=True) # NaN \u0442\u0430\u043c, \u0433\u0434\u0435 \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u0431\u044b\u0442\u044c \u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0430 0\ndf.drop(columns=[\n    'total_revenue', #\u0442.\u043a. \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\n    'order_id' , # \u043a\u0430\u043a \u0432 \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u0435\n    'line_item_type_id', # \u043a\u0430\u043a \u0432 \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u0435\n    'revenue_share_percent', # \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043e\u0434\u043d\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\n    'integration_type_id', # \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043e\u0434\u043d\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\n], inplace=True)\n# df = df[(df.CPM >= 0) & (df.CPM < df.CPM.quantile(0.95))] #\u0443\u0431\u0438\u0440\u0430\u0435\u043c \u0432\u044b\u0431\u0440\u043e\u0441\u044b\ndf.head(3)","3f425391":"pd.concat((df.dtypes, df.isna().sum(0), df.describe().T), axis=1).rename(columns={0: 'type',1: 'NaNs'})","349c36fd":"id_cols = [x for x in df.columns if x.endswith('id')]\nint_cols = [x for x in df.columns if not x.endswith('id') and 'date' != x and 'CPM' != x]\n# df[id_cols] = df[id_cols].astype(str)\ndf[int_cols] = df[int_cols].astype(np.float)\nid_cols, int_cols","63c2ed52":"df_train = df[df.date < pd.to_datetime(\"2019-06-22\")].drop(columns='date')\ndf_test = df[df.date >= pd.to_datetime(\"2019-06-22\")].drop(columns='date')\n\n\ndf_train = df_train[(df_train.CPM >= 0) & (df_train.CPM < df_train.CPM.quantile(0.95))]\ndf_test = df_test[(df_test.CPM >= 0) & (df_test.CPM < df.CPM.quantile(0.95))]\nprint(df_train.shape, df_test.shape)","41d79fd1":"_, [ax1, ax2] = plt.subplots(1, 2, sharey=True, figsize=(10,6))\ndf_train.CPM.hist(ax=ax1)\ndf_test.CPM.hist(ax=ax2);","0dcc44bb":"df_train.describe().T","22a835f2":"df_test.describe().T","199f2e01":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, FunctionTransformer #LabelEncoder\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom xgboost import XGBRegressor\n# from catboost import CatBoostRegressor","11187ae0":"# class MyLEncoder(LabelEncoder):\n\n#     def transform(self, X, y=None, **fit_params):\n#         enc_data = []\n#         for i in range(self.size):\n#             enc_data.append(self.enc[i].transform(X[X.columns[i]], **fit_params))\n#         return np.asarray(enc_data).T\n\n#     def fit_transform(self, X,y=None,  **fit_params):\n#         self.fit(X, y,  **fit_params)\n#         return self.transform(X, **fit_params)\n\n#     def fit(self, X, y=None, **fit_params):\n#         self.size = X.shape[1]\n#         self.enc = []\n#         for col in X.columns:\n#             self.enc.append(LabelEncoder().fit(X[col], **fit_params))\n#         return self","e9235ff1":"X_train, X_val, y_train, y_val = train_test_split(\n    df_train.drop(columns=['CPM']),\n    df_train['CPM'],\n    random_state=42\n)\nmake_features = ColumnTransformer([\n#     ('lbls', MyLEncoder(), id_cols),\n    ('lbls', FunctionTransformer(lambda x: x.values), id_cols),\n    ('ints', StandardScaler(), int_cols),\n])\nX_train = make_features.fit_transform(X_train)\nX_val = make_features.transform(X_val)","e3698fd3":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_jobs=-1, \n    random_state=42\n)\nmodel.fit(\n    X = X_train, \n    y = y_train,\n    eval_set = [(X_val, y_val)],\n    verbose=0,\n)\nprint('Validation mse: %.5f' % mse(y_val, model.predict(X_val)))","6a50a709":"model.feature_importances_","1e9b1e5b":"X_test, y_test = df_test.drop(columns=['CPM']),df_test['CPM']\nX_test = make_features.transform(X_test)\ntest_mse = mse(y_test, model.predict(X_test))\nprint('Test mse: %.5f' % test_mse)\nif test_mse > 4850: # \u0415\u0441\u043b\u0438 \u0432\u0434\u0440\u0443\u0433 \u0447\u0442\u043e-\u0442\u043e \u043d\u0435 \u0442\u0430\u043a\n    print('Khochy zachet!')","34486947":"\u0421\u043d\u0430\u0447\u0430\u043b\u0430 \u0434\u0435\u043b\u0430\u043b LabelEncoding, \u043d\u043e \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043d\u0435 \u0432\u0441\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442. \u041f\u043e \u0438\u0434\u0435\u0435, \u043d\u0443\u0436\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u043c\u0435\u0442\u043a\u0443 \"\u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0439 id\". \u041d\u0435 \u0441\u0442\u0430\u043b \u0437\u0430\u043c\u043e\u0440\u0430\u0447\u0438\u0432\u0430\u0442\u044c\u0441\u044f.\n\n**TO DO**: \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 id","037e40c8":"# \u041c\u043e\u0434\u0435\u043b\u0438\u0440\u0443\u0435\u043c","65ec5d83":"\u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435","91ec884c":"# \u0424\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u0434\u0435\u043b\u0438\u043c \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438","fcff0247":"# \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u043d\u0430 \u0442\u0435\u0441\u0442\u0435","e1908210":"\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 targeta"}}