{"cell_type":{"298089eb":"code","07b436a2":"code","2a501893":"code","19e1f77e":"code","9e8c3ed7":"code","b497d77e":"code","81c4ee7c":"code","9a5ea7a5":"code","62a16bb0":"code","d3e517f6":"code","b01deebf":"code","168d3eb1":"code","4ae1fdf7":"code","b44d6988":"code","82f85b5a":"code","7fb7bac1":"code","f4636b26":"code","e488a687":"code","25b80e30":"code","167d208e":"code","6ce83430":"code","ff34fb06":"code","5c8653b1":"code","51545605":"code","9cd5b709":"code","801fbcda":"code","445d909a":"code","2ff59094":"code","e0880f49":"code","4ce4c235":"code","de3fffdb":"code","376d2fe6":"code","923ff0b9":"code","99371aae":"code","57a7c042":"markdown","707e6a06":"markdown","c8a011b6":"markdown","09c4dcc6":"markdown","8aaec962":"markdown","f6da21e4":"markdown","31376b29":"markdown","4b860d88":"markdown","66d3c743":"markdown","4d4dd77a":"markdown","b1c537e6":"markdown","8b1c221b":"markdown","6ea295c0":"markdown","94657e25":"markdown","0a44dc86":"markdown","08f5b5d0":"markdown","c20a360e":"markdown"},"source":{"298089eb":"import random\nimport copy\nimport time\nimport pandas as pd\nimport numpy as np\nimport gc\nimport re\nimport torch\nfrom torchtext import data\n#import spacy\nfrom tqdm import tqdm_notebook, tnrange\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\ntqdm.pandas(desc='Progress')\nfrom collections import Counter\nfrom textblob import TextBlob\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, HashingVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\n\nimport os \nimport nltk\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\n\n# cross validation and metrics\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom unidecode import unidecode\n\nfrom sklearn.preprocessing import StandardScaler\nfrom textblob import TextBlob\nfrom multiprocessing import  Pool\nfrom functools import partial\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.svm import LinearSVC\nimport lightgbm as lgb","07b436a2":"SEED = 1029","2a501893":"# Some preprocesssing that will be common to all the text classification methods you will see. \n\n# Remove punctuations:\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, ' ')\n    return x\n\n# We won't clean numbers in conventional methods case since we might get extra info from bigrams like 5 mins or 30 mins\ndef clean_numbers(x):\n    if bool(re.search(r'\\d', x)):\n        x = re.sub('[0-9]{5,}', '#####', x)\n        x = re.sub('[0-9]{4}', '####', x)\n        x = re.sub('[0-9]{3}', '###', x)\n        x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n# Remove Misspell:\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)\n\n# remove stopwords:\nstopword_list = nltk.corpus.stopwords.words('english')\ndef remove_stopwords(text, is_lower_case=True):\n    tokenizer = ToktokTokenizer()\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)\n    return filtered_text\n\n# remove contractions:\ncontraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n\ndef _get_contractions(contraction_dict):\n    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n    return contraction_dict, contraction_re\n\ncontractions, contractions_re = _get_contractions(contraction_dict)\n\ndef replace_contractions(text):\n    def replace(match):\n        return contractions[match.group(0)]\n    return contractions_re.sub(replace, text)\n\n# Using lemmatizer to keep dictionary form of words. Might be helpful if later we want to use word embeddings.\nwordnet_lemmatizer = WordNetLemmatizer()\ndef lemma_text(text):\n    tokenizer = ToktokTokenizer()\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n    return ' '.join(tokens)\n\n\ndef clean_sentence(x):\n    x = x.lower()\n    x = clean_text(x)\n    x = replace_typical_misspell(x)\n    x = remove_stopwords(x)\n    x = replace_contractions(x)\n    x = lemma_text(x)\n    x = x.replace(\"'\",\"\")\n    return x","19e1f77e":"\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")#[:400000]\ntest_df = pd.read_csv(\"..\/input\/test.csv\")#[:20000]\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","9e8c3ed7":"train_df.head()","b497d77e":"# clean the sentences\ntrain_df['cleaned_text'] = train_df['question_text'].apply(lambda x : clean_sentence(x))\ntest_df['cleaned_text'] = test_df['question_text'].apply(lambda x : clean_sentence(x))","81c4ee7c":"# small function to find threshold and find best f score - Eval metric of competition\ndef bestThresshold(y_train,train_preds):\n    tmp = [0,0,0] # idx, cur, max\n    delta = 0\n    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n        tmp[1] = f1_score(y_train, np.array(train_preds)>tmp[0])\n        if tmp[1] > tmp[2]:\n            delta = tmp[0]\n            tmp[2] = tmp[1]\n    # print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n    return tmp[2]","9a5ea7a5":"# HELPER FUNCTIONS\n\ndef model_train_cv(x_train,y_train,nfold,model_obj):\n    splits = list(StratifiedKFold(n_splits=nfold, shuffle=True, random_state=SEED).split(x_train, y_train))\n    x_train = x_train\n    y_train = np.array(y_train)\n    # matrix for the out-of-fold predictions\n    train_oof_preds = np.zeros((x_train.shape[0]))\n    for i, (train_idx, valid_idx) in enumerate(splits):\n\n        x_train_fold = x_train[train_idx.astype(int)]\n        y_train_fold = y_train[train_idx.astype(int)]\n        x_val_fold = x_train[valid_idx.astype(int)]\n        y_val_fold = y_train[valid_idx.astype(int)]\n\n        clf = copy.deepcopy(model_obj)\n        clf.fit(x_train_fold, y_train_fold)\n        valid_preds_fold = clf.predict_proba(x_val_fold)[:,1]\n\n        # storing OOF predictions\n        train_oof_preds[valid_idx] = valid_preds_fold\n    return train_oof_preds\n\ndef lgb_model_train_cv(x_train,y_train,nfold,lgb):\n    splits = list(StratifiedKFold(n_splits=nfold, shuffle=True, random_state=SEED).split(x_train, y_train))\n    x_train = x_train\n    y_train = np.array(y_train)\n    # matrix for the out-of-fold predictions\n    train_oof_preds = np.zeros((x_train.shape[0]))\n    for i, (train_idx, valid_idx) in enumerate(splits):\n        x_train_fold = x_train[train_idx.astype(int)]\n        y_train_fold = y_train[train_idx.astype(int)]\n        x_val_fold = x_train[valid_idx.astype(int)]\n        y_val_fold = y_train[valid_idx.astype(int)]\n        d_train = lgb.Dataset(x_train_fold, label=y_train_fold)\n        d_val = lgb.Dataset(x_val_fold, label=y_val_fold)\n        params = {}\n        params['learning_rate'] = 0.01\n        params['boosting_type'] = 'gbdt'\n        params['objective'] = 'binary'\n        params['metric'] = 'binary_logloss'\n        params['sub_feature'] = 0.5\n        params['num_leaves'] = 10\n        params['min_data'] = 50\n        params['max_depth'] = 10\n        \n        clf = lgb.train(params, d_train, num_boost_round = 100,valid_sets=(d_val), early_stopping_rounds=10,verbose_eval=10)\n        valid_preds_fold = clf.predict(x_val_fold)\n        # storing OOF predictions\n        train_oof_preds[valid_idx] = valid_preds_fold\n    return train_oof_preds","62a16bb0":"cnt_vectorizer = CountVectorizer(dtype=np.float32,\n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3),min_df=3)\n\n# Fitting count vectorizer to both training and test sets (semi-supervised learning)\ncnt_vectorizer.fit(list(train_df.cleaned_text.values) + list(test_df.cleaned_text.values))\nxtrain =  cnt_vectorizer.transform(train_df.cleaned_text.values) \n#xtest_cntv = cnt_vectorizer.transform(test_df.cleaned_text.values)\ny_train = train_df.target.values","d3e517f6":"# Fitting a simple Logistic Regression on CountVectorizer Model\ntrain_oof_preds = model_train_cv(xtrain,y_train,5,LogisticRegression(C=1.0))\n\nprint (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))","b01deebf":"# fitting a simple Naive Bayes model in place of logistic regression using the same features\ntrain_oof_preds = model_train_cv(xtrain,y_train,5,MultinomialNB())\nprint (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))\n","168d3eb1":"# fitting a simple Naive Bayes model in place of logistic regression using the same features\ntrain_oof_preds = lgb_model_train_cv(xtrain,y_train,5,lgb)\n\n","4ae1fdf7":"print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))","b44d6988":"xtrain=0\ndel xtrain\n#del xtest_cntv\ndel cnt_vectorizer\ngc.collect()","82f85b5a":"# Always start with these features. They work (almost) everytime!\ntfv = TfidfVectorizer(dtype=np.float32, min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(train_df.cleaned_text.values) + list(test_df.cleaned_text.values))\nxtrain =  tfv.transform(train_df.cleaned_text.values) \n#xtest_tfv = tfv.transform(test_df.cleaned_text.values)\ny_train = train_df.target.values","7fb7bac1":"# Fitting a simple Logistic Regression on TFIDF Feats\ntrain_oof_preds = model_train_cv(xtrain,y_train,5,LogisticRegression(C=1.0))\n\nprint (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))","f4636b26":"# fitting a simple Naive Bayes model in place of logistic regression using the same features\ntrain_oof_preds = model_train_cv(xtrain,y_train,5,MultinomialNB())\nprint (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))","e488a687":"# fitting a simple Naive Bayes model in place of logistic regression using the same features\ntrain_oof_preds = lgb_model_train_cv(xtrain,y_train,5,lgb)\n","25b80e30":"print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))","167d208e":"xtrain=0\ndel xtrain\n#del xtest_tfv\ndel tfv\ngc.collect()","6ce83430":"# Always start with these features. They work (almost) everytime!\nhv = HashingVectorizer(dtype=np.float32,\n            strip_accents='unicode', analyzer='word',\n            ngram_range=(1, 3),n_features=2**10,non_negative=True)\n# Fitting Hash Vectorizer to both training and test sets (semi-supervised learning)\nhv.fit(list(train_df.cleaned_text.values) + list(test_df.cleaned_text.values))\nxtrain =  hv.transform(train_df.cleaned_text.values) \n#xtest_hv = hv.transform(test_df.cleaned_text.values)\ny_train = train_df.target.values","ff34fb06":"# Fitting a simple Logistic Regression on TFIDF Feats\ntrain_oof_preds = model_train_cv(xtrain,y_train,5,LogisticRegression(C=1.0))\n\nprint (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))","5c8653b1":"# fitting a simple Naive Bayes model in place of logistic regression using the same features\ntrain_oof_preds = model_train_cv(xtrain,y_train,5,MultinomialNB())\nprint (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))","51545605":"# fitting a simple Lgb model in place of logistic regression using the same features\ntrain_oof_preds = lgb_model_train_cv(xtrain,y_train,5,lgb)","9cd5b709":"print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))","801fbcda":"xtrain=0\ndel xtrain\n#del xtest_hv\ndel hv\ngc.collect()","445d909a":"# load the GloVe vectors in a dictionary:\ndef load_glove_index():\n    EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n    return embeddings_index\n\nembeddings_index = load_glove_index()\n\nprint('Found %s word vectors.' % len(embeddings_index))","2ff59094":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v \/ np.sqrt((v ** 2).sum())","e0880f49":"# create sentence vectors using the above function for training and validation set\nxtrain = [sent2vec(x) for x in tqdm(train_df.cleaned_text.values)]\n#xtest_glove = [sent2vec(x) for x in tqdm(test_df.cleaned_text.values)]","4ce4c235":"embeddings_index = 0\ndel embeddings_index\n# del xtest_glove\ngc.collect()","de3fffdb":"xtrain = np.array(xtrain)\n# xvalid_glove = np.array(xtest_glove)\ny_train = train_df.target.values","376d2fe6":"# Fitting a simple Logistic Regression on glove Feats\ntrain_oof_preds = model_train_cv(xtrain,y_train,5,LogisticRegression(C=1.0))\nprint (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))","923ff0b9":"# fitting a simple Lgb model in place of logistic regression using the same features\ntrain_oof_preds = lgb_model_train_cv(xtrain,y_train,5,lgb)","99371aae":"print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))","57a7c042":"We are able to get a good F1 local CV score  with our fairly simple model which just counts the number of time some ngrams appear in a sentence. \nYou can also try running SVMs which were used extensively when trying out models on Text. But they are pretty slow so not using them here. \n\nLets try LightGBM also.","707e6a06":"## Lets start with modelling ","c8a011b6":"## IMPORTS ","09c4dcc6":"![count vectorizer](https:\/\/mlwhiz.com\/images\/countvectorizer.png)","8aaec962":"![tfidf](https:\/\/mlwhiz.com\/images\/tfidf.png)","f6da21e4":"> ### 3. Bag of words model using Hashing Vectorizer","31376b29":"References:\n    https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle","4b860d88":"We are able to get a F1 local CV score of ___ with our fairly simple model which just counts the number of time some ngrams appear in a sentence. That is pretty good. Let us try Multinomial NB","66d3c743":"### 1. Bag of words model using Count Vectorizer:","4d4dd77a":"![](http:\/\/)![hashing features](https:\/\/mlwhiz.com\/images\/hashfeats.png)","b1c537e6":"## LOAD PROCESSED TRAINING DATA FROM DISK","8b1c221b":"### 2. Bag of words model using TFIDF:","6ea295c0":"![word2vec](https:\/\/mlwhiz.com\/images\/word2vec_feats.png)","94657e25":"### 4. Word2vec Embeddings ","0a44dc86":"In This kernel, I will try to use the normal conventional methods on the quora dataset namely:\n\n- TFIDF,\n- CountVectorizer, \n- HashVectorizer,\n- Word embeddings. \n\nTo get an understanding of these I have created a notebook at: https:\/\/mlwhiz.com\/blog\/2019\/02\/08\/deeplearning_nlp_conventional_methods\/\n\nDo have a look at the blog post too.","08f5b5d0":"### Basic Parameters","c20a360e":"So this is it. All of these models are not tuned yet and could be tuned further to improve performance. But it is good to get sort of baselines and appreciate the sort of performance we can get out of neural nets "}}