{"cell_type":{"1253bc51":"code","8e189c0c":"code","63021d43":"code","f3537203":"code","7cb04dad":"code","e9ade8f3":"code","ddc3f5af":"code","cf30792d":"code","b93a1832":"code","59198720":"code","4438e9f4":"code","738113be":"code","a7c30bd6":"code","36bdec73":"code","ec44bf14":"code","acbbf64f":"code","31be5472":"code","4e710be0":"code","ae7d18ea":"code","30724c57":"code","37ecf5d2":"code","5937361b":"code","c6cb5acc":"code","c0f91c52":"code","ba3e8e50":"code","a6b406a2":"code","75908375":"code","14517c16":"code","f10a5d4d":"code","51041b85":"code","e43eb9f8":"markdown","106a2d44":"markdown","5aea70be":"markdown","6f1b5288":"markdown","33b18321":"markdown","14ef8a3a":"markdown","83028c2b":"markdown","046c0d65":"markdown","3710e6f2":"markdown","55227662":"markdown","9ddc8bf1":"markdown","fd0da292":"markdown","ae2e8a6f":"markdown","060872b3":"markdown","b89dcada":"markdown","72cbfbe9":"markdown","75b3830f":"markdown","59fbf63d":"markdown","385bcd75":"markdown","b7adbcfa":"markdown","ec441db3":"markdown","6d3fbb1e":"markdown"},"source":{"1253bc51":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n#print(os.listdir(\"..\/input\"))\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use(style='ggplot')\nplt.rcParams['figure.figsize'] = (20, 10)\nimport seaborn as sns\n\nfrom IPython.display import display\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nimport time\n\n\n# Any results you write to the current directory are saved as output.","8e189c0c":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain['Name'] = 'Train'\ntest['Name'] = 'Test'\ncombined_set = train.append(test)\n\nprint(\"Train data shape:\", combined_set[combined_set.Name == 'Train'].shape)\nprint(\"Test data shape:\", combined_set[combined_set.Name == 'Test'].shape,\"\\n\\n\")\nprint(\"Data caracteristics\")\nprint(combined_set.describe())\nprint(\"\\n\\nData types\")\nprint(combined_set.dtypes.value_counts())\n\n## Investigatting null values\nnum_features = combined_set.shape[0]\nnull_values = combined_set.columns[combined_set.isnull().any()]\nnull_features = combined_set[null_values].isnull().sum().sort_values(ascending = False)\nnull_features_ratio = null_features.apply(lambda x: \"{}%\".format(round(x\/num_features * 100, 3)))\nmissing_data = pd.DataFrame({'No of Nulls': null_features, 'Ratio': null_features_ratio})\nprint(\"\\n\\nNull values count\")\nprint(missing_data)\n\n## Investigation Skewness of predictor\nprint(\"\\n\\nSkew of Sale Price\", combined_set[combined_set.Name == 'Train'].SalePrice.skew())\nplt.hist(combined_set[combined_set.Name == 'Train'].SalePrice, color='blue')\nplt.show()","63021d43":"## Analysing Pool null values\nprint('Pool areas of missing pool quality. Zero means house withou pool')\npool_areas = combined_set[combined_set.PoolQC.isnull()]['PoolArea'].value_counts()\nprint(pool_areas,'\\n\\n')\n\n## Distinct types of alley\nprint('Distinct types of alley')\nalley_types = train['Alley'].value_counts(dropna=False)\nprint(alley_types,'\\n\\n')\n\n## Distinct types of fence\nprint('Distinct types of fence')\nfence_types = train['Fence'].value_counts(dropna=False)\nprint(fence_types,'\\n\\n')\n\n## Analysing FireplaceQu null values\nprint('Fireplace number of missing fireplace quality. Zero means house withou fireplace')\nfireplaces = train[train.FireplaceQu.isnull()]['Fireplaces'].value_counts()\nprint(fireplaces,'\\n\\n')\n\n## Analysing LotFrontage\nprint('Trying to figure out why there are missing values of LotFrontage')\n## possible related columns\ncols = ['Street', 'Alley','LotConfig']\nrelated_values = train[train.LotFrontage.isnull()][cols]\n## Replace null values of related columns with \"-\" \nrelated_values = related_values.fillna(value=\"-\")\n## Concat related columns to get a summary of data\nrelated_values['concat'] = related_values[['Street', 'Alley', 'LotConfig']].apply(lambda x: '::'.join(x), axis=1)\nprint('Combinations of null values (as \"-\") on related columns ', cols)\nprint(related_values.concat.value_counts(), \"\\n\\n\")\n\n## Analysing Garage related columns\n## possible related columns\ncols = [\"GarageYrBlt\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\"]\nrelated_cols = [\"GarageCars\",\"GarageArea\"]\nrelated_values = train[train[cols].isnull().any(axis=1)][cols + related_cols]\nprint('Assumption is that garage related columns (', cols, ') makes sense only if house has a garage')\nprint('Maybe GarageCars and GarageArea are zeroed')\n## Replace null values of related columns with \"-\" \nrelated_values = related_values.fillna(value=\"-\")\n## Concat related collumns to get a summary of data\nrelated_values['GarageNullCols'] = related_values[cols].apply(lambda x: '::'.join(x), axis=1)\nrelated_values['GarageRelatedCols'] = related_values[related_cols].apply(lambda x: '::'.join(x.astype(str)), axis=1)\nrelated_values['GarageCols'] = related_values[['GarageRelatedCols','GarageNullCols']].apply(lambda x: '>>'.join(x), axis=1)\nprint('Combinations of null values (as \"-\") on related columns')\nprint(related_values.GarageCols.value_counts(), \"\\n\\n\")\n\n## Analysing Basement related columns\nprint('Assumption is that basement related columns makes sense only if house has a basement')\nprint('Maybe TotalBsmtSF is zeroed')\n## possible related columns\ncols = [\"BsmtFinType2\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtCond\", \"BsmtQual\"]\nrelated_cols = [\"TotalBsmtSF\"]\nall_related_cols = [\"Id\", \"TotalBsmtSF\",\"BsmtFinSF1\",\"BsmtFinSF2\",\"BsmtUnfSF\"] + cols\nrelated_values = train[train[cols].isnull().any(axis=1)][cols + related_cols]\n## Replace null values of related columns with \"-\" \nrelated_values = related_values.fillna(value=\"-\")\n## Concat related collumns to get a summary of data\nrelated_values['BsmtNullCols'] = related_values[cols].apply(lambda x: '::'.join(x), axis=1)\nrelated_values['BsmtRelatedCols'] = related_values[related_cols].apply(lambda x: '::'.join(x.astype(str)), axis=1)\nrelated_values['BsmtCols'] = related_values[['BsmtRelatedCols','BsmtNullCols']].apply(lambda x: '>>'.join(x), axis=1)\nprint('Combinations of null values (as \"-\") on related columns')\nprint(related_values.BsmtCols.value_counts(), \"\\n\\n\")\nprint('Almost null columns belong to zero basement, but there are two different rows')\nrows = train[((train.BsmtFinType2 == 'Unf') & (train.BsmtExposure.isnull())) | ((train.BsmtFinType2.isnull()) & (train.TotalBsmtSF == 3206))]\nprint(\"Let's see what's is going on those two rows. Let'ts take a look at all related columns with basement\")","f3537203":"print(\"Ok, Finished Area type 2 is optional and Exposure, probably has a really missing value\")\nrows[all_related_cols]","7cb04dad":"## Analysing Masonry veneer columns\nprint(\"Maybe, Masonry veneer missing values occurs in the same rows. Maybe, there isn't a masonry veneer\")\nrows = train[((train.MasVnrType.isnull()) | (train.MasVnrArea.isnull()))]\nrows[[\"Id\", \"MasVnrType\", \"MasVnrArea\"]]","e9ade8f3":"## Evaluating Object columns\nobjects = train.select_dtypes(include=[np.object])\ndesc = objects.describe()\ndesc","ddc3f5af":"print(\"Looking at several top frequency values, looks like that there are categories that appears almost everytime.\")\ntop_freq = desc.loc['freq']\ncount = desc.loc['count']\ntop = desc.loc['top']\nfreq_ratio = top_freq\/count\nprint(\"Let's look at frequency percentage of total count\")\nd = {'top_cat':top,'ratio':freq_ratio}\nfreq_comp = pd.DataFrame(d)\nfreq_comp.sort_values(by='ratio', ascending = False, inplace = True)\n\nsns.set_context('talk')\nsns.set_style('ticks')\nsns.set_palette('dark')\n\nplt.figure(figsize= (16, 8))\nplt.xticks(rotation='90')\nplt.xlabel('Categorical Column')\nplt.ylabel('Frequency Ratio')\nplt.title('Relevant Categories')\nsns.barplot(freq_comp.index, freq_comp.ratio)","cf30792d":"freq_comp[(freq_comp.ratio >= .8)]","b93a1832":"more_less_relevant_categoricals = ['LandContour', 'BsmtFinType2', 'ExterCond', 'SaleType', 'Condition1',\n       'BldgType', 'SaleCondition']\n\n(fig, ax_lst) = plt.subplots(4, 2,figsize=(15,30))\ni,j = (0,0)\nfor cat in more_less_relevant_categoricals:\n    ## Apply log to normalize values as most relevant category is much larger then others\n    data = np.log(train[cat].value_counts())\n    ax_lst[i,j].bar(data.index, data)\n    \n    ax_lst[i,j].set_ylabel('Count')\n    ax_lst[i,j].set_xlabel(cat)\n    if j == 1:\n        i = i + 1 if i < 3 else 0\n        j = 0\n    else:\n        j = j + 1","59198720":"numeric = train.select_dtypes(include=[np.number])\nprint('Evaluating top correlations')\ncorr = numeric.corr()\nprint (corr['SalePrice'].sort_values(ascending=False)[:7], '\\n')\nprint (corr['SalePrice'].sort_values(ascending=False)[-6:])","4438e9f4":"sorted_corr = corr['SalePrice'].sort_values(ascending=False)\nsns.set_context('talk')\nsns.set_style('ticks')\nsns.set_palette('dark')\n\nplt.figure(figsize= (16, 8))\nplt.xticks(rotation='90')\nplt.xlabel('Categorical Column')\nplt.ylabel('Frequency Ratio')\nplt.title('Relevant Categories')\nsns.barplot(sorted_corr.index, sorted_corr)","738113be":"## Possible values of Overall Quality\nprint('Overall Quality is a categorical feature, from 1 to 10')\nprint(train.OverallQual.unique())\nprint('Also number of cars on garage is a categorical feature')\nprint(train.GarageCars.unique())","a7c30bd6":"print(\"Overall Quality and Garage Cars are categorical features.\\nLet's visualize them as a bar plot aggregated by median\")\n\nquality_pivot = train.pivot_table(index='OverallQual',\n                                  values='SalePrice', aggfunc=np.median)\n\ncars_pivot = train.pivot_table(index='GarageCars',\n                                  values='SalePrice', aggfunc=np.median)\n\ntarget = np.log(train.SalePrice) # Eliminate skew\n# fig = plt.figure(figsize= (50, 20))  # an empty figure with no axes\n\n(fig, ax_lst) = plt.subplots(3, 2,figsize=(15,15))\n#Overall Quality X Sale Price\nax_lst[0,0].bar(quality_pivot.index, quality_pivot.SalePrice)\nax_lst[0,0].set_ylabel('Sale Price')\nax_lst[0,0].set_xlabel('Overall Quality')\n\n\n#Above Grade Living Area X Sale Price\nax_lst[0,1].scatter(x=train['GrLivArea'], y=target)\nax_lst[0,1].set_ylabel('Sale Price')\nax_lst[0,1].set_xlabel('Above grade living')\n\n#Garage Cars X Sale Price\nax_lst[1,0].bar(cars_pivot.index, cars_pivot.SalePrice)\nax_lst[1,0].set_ylabel('Sale Price')\nax_lst[1,0].set_xlabel('Num of cars on garage')\n\n#Garage Area X Sale Price\nax_lst[1,1].scatter(x=train['GarageArea'], y=target)\nax_lst[1,1].set_ylabel('Sale Price')\nax_lst[1,1].set_xlabel('Area of garage')\n\n#Basement area X Sale Price\nax_lst[2,0].scatter(x=train['TotalBsmtSF'], y=target)\nax_lst[2,0].set_ylabel('Sale Price')\nax_lst[2,0].set_xlabel('Area of basement')\n\n#1st Floor area X Sale Price\nax_lst[2,1].scatter(x=train['1stFlrSF'], y=target)\nax_lst[2,1].set_ylabel('Sale Price')\nax_lst[2,1].set_xlabel('Area of 1st floor')\n# plt.subplots_adjust(wspace=1, hspace=1)\nplt.tight_layout()","36bdec73":"yearSold_pivot = train.pivot_table(index='YrSold',\n                                  values='SalePrice', aggfunc=np.median)\n\noverallCond_pivot = train.pivot_table(index='OverallCond',\n                                  values='SalePrice', aggfunc=np.median)\n\nMSSubClass_pivot = train.pivot_table(index='MSSubClass',\n                                  values='SalePrice', aggfunc=np.median)\n\nKitchenAbvGr_pivot = train.pivot_table(index='KitchenAbvGr',\n                                  values='SalePrice', aggfunc=np.median)\n\n# fig = plt.figure(figsize= (16, 8))  # an empty figure with no axes\n\n(fig, ax_lst) = plt.subplots(3, 2, figsize=(15,15))\n#Low Quality Finished Area X Sale Price\nax_lst[0,0].scatter(x=train['LowQualFinSF'], y=target)\nax_lst[0,0].set_ylabel('Sale Price')\nax_lst[0,0].set_xlabel('Low Quality Finished Area')\n\n\n#Year Sold X Sale Price\nax_lst[0,1].bar(yearSold_pivot.index.values, yearSold_pivot.SalePrice.values)\nax_lst[0,1].set_ylabel('Sale Price')\nax_lst[0,1].set_xlabel('Year Sold')\n\n#Overall Condition X Sale Price\nax_lst[1,0].bar(overallCond_pivot.index, overallCond_pivot.SalePrice)\nax_lst[1,0].set_ylabel('Sale Price')\nax_lst[1,0].set_xlabel('Overall Condition')\n\n#type of dwelling X Sale Price\nax_lst[1,1].bar(MSSubClass_pivot.index, MSSubClass_pivot.SalePrice)\nax_lst[1,1].set_ylabel('Sale Price')\nax_lst[1,1].set_xlabel('Type of dwelling')\n\n#Porch area X Sale Price\nax_lst[2,0].scatter(x=train['EnclosedPorch'], y=target)\nax_lst[2,0].set_ylabel('Sale Price')\nax_lst[2,0].set_xlabel('Area of Porch')\n\n#KitchenAbvGr_pivot X Sale Price \nax_lst[2,1].bar(KitchenAbvGr_pivot.index, KitchenAbvGr_pivot.SalePrice)\nax_lst[2,1].set_ylabel('Sale Price')\nax_lst[2,1].set_xlabel('Kitchen above grade')\n# plt.subplots_adjust(wspace=1, hspace=1)\nplt.tight_layout()","ec44bf14":"overallCond_pivot","acbbf64f":"## Make a new copy of datasets\n\ntransf_train = train.copy()\ntransf_test = test.copy()\n\n#Filling nulls\n#PoolQC\ntransf_train.loc[lambda df: df.PoolArea == 0, 'PoolQC'] = 'No Pool'\ntransf_test.loc[lambda df: df.PoolArea == 0, 'PoolQC'] = 'No Pool'\n# train[train.PoolArea == 0].PoolQC\n\n#MiscFeature\ntransf_train.MiscFeature.fillna('None',inplace=True)\ntransf_test.MiscFeature.fillna('None',inplace=True)\n# train.MiscFeature.value_counts()\n\n#Alley\ntransf_train.Alley.fillna('No alley entrace',inplace=True)\ntransf_test.Alley.fillna('No alley entrace',inplace=True)\n# train.Alley.value_counts()\n\n#Fence\ntransf_train.Fence.fillna('None',inplace=True)\ntransf_test.Fence.fillna('None',inplace=True)\n# train.Fence.value_counts()\n\n#FireplaceQu\ntransf_train.loc[lambda df: df.Fireplaces == 0, 'FireplaceQu'] = 'No Fireplaces'\ntransf_test.loc[lambda df: df.Fireplaces == 0, 'FireplaceQu'] = 'No Fireplaces'\n# train[train.Fireplaces == 0].FireplaceQu\n\n#LotFrontage\n# Get median by neighborhood\nmed = transf_train.groupby('Neighborhood').LotFrontage.median()\nmed_test = transf_test.groupby('Neighborhood').LotFrontage.median()\ntransf_train.loc[lambda df: df.LotFrontage.isnull(), 'LotFrontage'] = transf_train.loc[lambda df: df.LotFrontage.isnull(), 'Neighborhood'].map(med)\ntransf_test.loc[lambda df: df.LotFrontage.isnull(), 'LotFrontage'] = transf_test.loc[lambda df: df.LotFrontage.isnull(), 'Neighborhood'].map(med_test)\n\n#Garage related columns\ngarageRelated = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\ntransf_train.loc[lambda df: df.GarageArea == 0, garageRelated] = 'No garage'\ntransf_train.loc[lambda df: df.GarageArea == 0, 'GarageYrBlt'] = -1\ntransf_test.loc[lambda df: df.GarageArea == 0, garageRelated] = 'No garage'\ntransf_test.loc[lambda df: df.GarageArea == 0, 'GarageYrBlt'] = -1\n\n#Basement related columns\nbasementNum = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFullBath', 'BsmtHalfBath']\nbasementRelated = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\ntransf_train.loc[lambda df: df.TotalBsmtSF == 0, basementRelated] = 'No basement'\ntransf_train.loc[lambda df: df.TotalBsmtSF == 0, basementNum] = 0\ntransf_train.loc[lambda df: df.BsmtFinType2.isnull(), 'BsmtFinType2'] = 'No extra type'\ntransf_train.loc[lambda df: df.BsmtExposure.isnull(), 'BsmtExposure'] = 'No'\ntransf_test.loc[lambda df: df.TotalBsmtSF == 0, basementRelated] = 'No basement'\ntransf_test.loc[lambda df: df.TotalBsmtSF == 0, basementNum] = 0\ntransf_test.loc[lambda df: df.BsmtFinType2.isnull(), 'BsmtFinType2'] = 'No extra type'\ntransf_test.loc[lambda df: df.BsmtExposure.isnull(), 'BsmtExposure'] = 'No'\n\n#Masonry Veneer related\ntransf_train.loc[lambda df: df.MasVnrArea.isnull(), 'MasVnrArea'] = 0\ntransf_train.loc[lambda df: df.MasVnrType.isnull(), 'MasVnrType'] = 'None'\ntransf_test.loc[lambda df: df.MasVnrArea.isnull(), 'MasVnrArea'] = 0\ntransf_test.loc[lambda df: df.MasVnrType.isnull(), 'MasVnrType'] = 'None'\n\n#Electrical\ncommon_electrical = transf_train.groupby(['Neighborhood','Electrical']).size()\ntransf_train.loc[lambda df: df.Electrical.isnull(), 'Electrical'] = transf_train.loc[lambda df: df.Electrical.isnull(), 'Neighborhood'].map(lambda n: common_electrical[n].idxmax())\ncommon_electrical_test = transf_test.groupby(['Neighborhood','Electrical']).size()\ntransf_test.loc[lambda df: df.Electrical.isnull(), 'Electrical'] = transf_test.loc[lambda df: df.Electrical.isnull(), 'Neighborhood'].map(lambda n: common_electrical_test[n].idxmax())\n\n# train.Electrical.value_counts(dropna=False)\n\n## Investigatting null values based on original train dataset\nnull_values = transf_train.columns[train.isnull().any()]\nnull_features = transf_train[null_values].isnull().sum().sort_values(ascending = False)\nmissing_data = pd.DataFrame({'No of Nulls' :null_features})\nprint(\"\\n\\nNull values count\")\nprint(missing_data)\n\nnull_values_test = transf_test.columns[test.isnull().any()]\nnull_features_test = transf_test[null_values_test].isnull().sum().sort_values(ascending = False)\nmissing_data_test = pd.DataFrame({'No of Nulls' :null_features_test})\nprint(\"\\n\\nNull values count\")\nprint(missing_data_test)","31be5472":"irrelevant_columns = ['Utilities', 'Street', 'Condition2', 'RoofMatl', 'Heating',\n       'GarageCond', 'GarageQual', 'LandSlope', 'CentralAir', 'Functional',\n       'BsmtCond', 'PavedDrive', 'Electrical', 'MiscFeature']\n\ntransf_train = transf_train.drop(irrelevant_columns, axis=1)\ntransf_test = transf_test.drop(irrelevant_columns, axis=1)","4e710be0":"one_encoded_features = ['LandContour', 'BsmtFinType2', 'BldgType']\ntwo_encoded_features = ['ExterCond', 'SaleType', 'SaleCondition']\nall_diff_features = ['Condition1']\nother_categorical_features = freq_comp[(freq_comp.ratio < .8)].index\n\n## variable freq_comp has most relevant category of each column\ndrop_columns = []\n# Encoding first case\nprint('Encoding first case')\ndef encode_one_encoded(x, default): return 1 if x == default else 0\nfor f in one_encoded_features:\n    if (f in transf_train.columns) & (f in transf_test.columns):\n        default_v = freq_comp.loc[f].top_cat\n        transf_train['enc_' + f] =  transf_train[f].apply(encode_one_encoded,args=(default_v,))\n        transf_test['enc_' + f] =  transf_test[f].apply(encode_one_encoded,args=(default_v,))\n        drop_columns.append(f)\n        \n# Encoding second case\nprint('Encoding second case')\ndef encode_two_encoded(x, fs, sc):\n    return fs if x == fs else 'sec_category' if x in sc else 'other'\nfor f in two_encoded_features:\n    if (f in transf_train.columns) & (f in transf_test.columns):\n        freq = transf_train[f].value_counts()[:3].index # First 3 biggest categories\n        default_v = freq[0]\n        alternate_v = freq[1:3]\n        column_name = 'enc_' + f\n        transf_train[column_name] =  transf_train[f].apply(encode_two_encoded,args=(default_v,alternate_v))\n        transf_test[column_name] =  transf_test[f].apply(encode_two_encoded,args=(default_v, alternate_v))\n        transf_train = pd.get_dummies(transf_train, columns=[column_name])\n        transf_test = pd.get_dummies(transf_test, columns=[column_name])\n        drop_columns.append(f)\n\n# Encoding third case\nprint('Encoding third case')\nfor f in all_diff_features:\n    if (f in transf_train.columns) & (f in transf_test.columns):\n        column_name = 'enc_' + f\n        transf_train = pd.get_dummies(transf_train, prefix=column_name, columns=[f])\n        transf_test = pd.get_dummies(transf_test, prefix=column_name, columns=[f])\n        \n# Encoding other categorical columns\nprint('Encoding other categorical columns')\nfor f in other_categorical_features:\n    if (f in transf_train.columns) & (f in transf_test.columns):\n        column_name = 'enc_' + f\n        transf_train = pd.get_dummies(transf_train, prefix=column_name, columns=[f])\n        transf_test = pd.get_dummies(transf_test, prefix=column_name, columns=[f])\n        \ntransf_train = transf_train.drop(drop_columns, axis=1)\ntransf_test = transf_test.drop(drop_columns, axis=1)\n","ae7d18ea":"outlier_train = transf_train.copy()\noutlier_test = transf_test.copy()\n## Dropping lines with Above Grade Living Area greater then 4000 feet\noutlier_train = outlier_train[outlier_train['GrLivArea'] < 4000]\n\n## Dropping lines with Garage Area greater then 1200 feet\noutlier_train = outlier_train[outlier_train['GarageArea'] < 1200]\n\n## Dropping lines with Basement Area greater then 3000 feet\noutlier_train = outlier_train[outlier_train['TotalBsmtSF'] < 3000]\n\n## Dropping lines with 1st floor area greater then 2500 feet\noutlier_train = outlier_train[outlier_train['1stFlrSF'] < 2500]","30724c57":"## Group GarageCars into 3 clusters: 0, 1, 2 or 4, 3 cars\ndef encode_garage_cars(x):\n    return 'No_cars' if x == 0 else 'One_car' if x == 1 else 'Two_or_four_cars' if (x == 2) | (x == 4) else 'Three_cars'\n\nf = 'GarageCars'\nif (f in outlier_train.columns) & (f in outlier_test.columns):\n    column_name = 'enc_' + f\n    outlier_train[column_name] = outlier_train[f].apply(encode_garage_cars)\n    outlier_test[column_name] = outlier_test[f].apply(encode_garage_cars)\n    outlier_train = pd.get_dummies(outlier_train, columns=[column_name])\n    outlier_test = pd.get_dummies(outlier_test, columns=[column_name])\n    outlier_train = outlier_train.drop([f], axis=1)\n    outlier_test = outlier_test.drop([f], axis=1)\noutlier_train.columns","37ecf5d2":"## Encoding OverallCond into 6 clusters\ndef encode_overall_condition(x):\n    return 'Bellow_6' if x >= 6 else str(x)\n\nf = 'OverallCond'\nif (f in outlier_train.columns) & (f in outlier_test.columns):\n    column_name = 'enc_' + f\n    outlier_train[column_name] = outlier_train[f].apply(encode_overall_condition)\n    outlier_test[column_name] = outlier_test[f].apply(encode_overall_condition)\n    outlier_train = pd.get_dummies(outlier_train, columns=[column_name])\n    outlier_test = pd.get_dummies(outlier_test, columns=[column_name])\n    outlier_train = outlier_train.drop([f], axis=1)\n    outlier_test = outlier_test.drop([f], axis=1)\noutlier_train.columns\nprint('Dropping other less correlated columns')\ncols_to_drop = ['YrSold', 'MSSubClass', 'KitchenAbvGr', 'LowQualFinSF', 'EnclosedPorch']\ncols = []\nfor c in cols_to_drop:\n    if (c in outlier_train) & (c in outlier_test):\n        cols.append(c)\noutlier_train = outlier_train.drop(cols, axis=1)\noutlier_test = outlier_test.drop(cols, axis=1)\n\nprint('Handle Id columns of both train and test dataset\\nDropping Id column from train dataset and making a copy of Id column of test dataset')\noutlier_train = outlier_train.drop(['Id'], axis=1)\nids = outlier_test.Id\noutlier_test = outlier_test.drop(['Id'], axis=1)","5937361b":"print('Visualizing final shape after transformations\\n\\n')\nprint('Train dataset')\nprint(outlier_train.dtypes.value_counts())\nprint('\\nTest dataset')\nprint(outlier_test.dtypes.value_counts())\nprint('\\nNull values')\nprint(missing_data)","c6cb5acc":"tmp = outlier_train.drop('SalePrice',axis=1)\nprint('Train and test dataset are different\\n Train set has more features then test set')\nprint('Train #features ', tmp.shape[1], ' Test #features ', outlier_test.shape[1])\nprint(\"Let's lookup what are those missing features\")\nmissing = list(set(tmp.columns) - set(outlier_test.columns))\nprint(missing)","c0f91c52":"for f in missing:\n    outlier_test[f] = 0","ba3e8e50":"y = np.log(outlier_train.SalePrice)\nX = outlier_train.drop(['SalePrice'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(\n                                    X, y, random_state=37, test_size=.33)","a6b406a2":"lr = linear_model.LinearRegression()\n\nmodel = lr.fit(X_train, y_train)\nr_2 = model.score(X_test, y_test)\n\nprint('r-squared ', r_2)","75908375":"predictions = model.predict(X_test)\n\nrmse = mean_squared_error(y_test, predictions)\n\nprint('RMSE is ', rmse)","14517c16":"actual_values = y_test\nplt.figure(figsize= (16, 8))\nplt.scatter(predictions, actual_values, alpha=.75,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Linear Regression Model')\nplt.show()","f10a5d4d":"for i in range (-2, 3):\n    alpha = 10**i\n    rm = linear_model.Ridge(alpha=alpha)\n    ridge_model = rm.fit(X_train, y_train)\n    preds_ridge = ridge_model.predict(X_test)\n\n    plt.figure(figsize= (16, 8))\n    plt.scatter(preds_ridge, actual_values, alpha=.75, color='b')\n    plt.xlabel('Predicted Price')\n    plt.ylabel('Actual Price')\n    plt.title('Ridge Regularization with alpha = {}'.format(alpha))\n    overlay = 'R^2 is: {}\\nRMSE is: {}'.format(\n                    ridge_model.score(X_test, y_test),\n                    mean_squared_error(y_test, preds_ridge))\n    plt.annotate(s=overlay,xy=(12.1,10.6),size='x-large')\n    plt.show()","51041b85":"submission = pd.DataFrame()\nsubmission['Id'] = ids\n\n## Predict regulated fit\nalpha=10\nrm = linear_model.Ridge(alpha=alpha)\nridge_model = rm.fit(X_train, y_train)\npredictions = ridge_model.predict(outlier_test.interpolate())\n\n## Reverse log transformation\nfinal_predictions = np.exp(predictions)\nprint (\"Original predictions are: \\n\", predictions[:5], \"\\n\")\nprint (\"Final predictions are: \\n\", final_predictions[:5])\n\nsubmission['SalePrice'] = final_predictions\nsubmission.head()\n\ntimestr = time.strftime(\"submission_%Y%m%d_%H%M%S.csv\")\nprint(timestr)\nsubmission.to_csv(timestr, index=False)","e43eb9f8":"Above 90%, there are meaningful categorical columns, but almost every house has the same caracteristics. It could introduce a bias aspect to predictor. So, in the feature engeneering, they will be dropped.  \nLooking at those categorical columns between 80% and 90%, there are columns that can tell us some important aspects of the house:  \n* LandContour - Flatness of the property\n* BsmtFinType2 - Rating of basement finished area (if multiple types)\n* ExterCond - Evaluates the present condition of the material on the exterior\n* SaleType - Type of sale\n* Condition1 - Proximity to various conditions\n* BldgType - Type of dwelling\n* SaleCondition - Condition of sale  \n\nThose columns may or not may add some value to Sale Price, due to its homogeneity.  Let's see how each of those features is composed. To do so, lets visualize category totalizers. If there isn't another predominant category, we can assume that a single column has 2 types of values, most relavant and \"others\". Else, we will need to adopt an advanced tecnique to deal with this value variation.","106a2d44":"Almost every feature shown did not influence in Sale Price. The only exception is Overall condition.  \nAs shown above, 1 to 5 classification incrises Sale Price significantly and 6 to 9 classification has, more or less, the same impact as 6 classification. We can transform this categorical feature data simplifying it into 1, 2, 3, 4, 5 and from 6.","5aea70be":"### First evaluation\nRoot Squared demonstrated that almost 90% of variance is explained with our features.  \nLet's take a look at RMSE, that is the value evaluated by Kaggle Competition.  To check this measure, we need to do predictions on X_test set and calculate RMSE.  \nRMSE tell us about the distance between our predictions and actual target values.  \nTo better visualize RMSE meaning, let's plot it with a scatter plot.","6f1b5288":"Nice try to our first fit. As scatter plot shows, our predictions follows target values but there are several outlier values and the graph is not a straight line ``` y=x```. Because that, let's begin a series of improviment to our model.\n\n### Improviments\nBegging with **Ridge Regularization**, we will try to decrease the influence of less correlated features. This technique requires and ```alpha``` parameter, that it controls the strength of the regularization.  \nTo better understand the efect of each ```alpha``` try, we will replot the scatter plot with prediction and actual values after applying Ridge.","33b18321":"### Deal with irrelevant categorical features\nBased on ration between frequency and total rows, some categorical columns values appers over 90% of time.   **\nThey are irrelevant to the predictor and may cause bias aspect.","14ef8a3a":"As charts shown, Overall Quality, Basement Area, Above Grade Living and 1st Floor Area higly increase Sale Price, but garage information has a different behavior.  \nWe have 3 different influences of garage cars:   \n\n1. 0 or 1 car slots is almost irrelevant\n1. 2 or 4 car slots increases price mostly in the same way\n1. 3 car slots increases Sale Price a lot\n\nThe same behavior is seen in the scatter plot of garage area. Maybe, discart garage area and group num cars into 3 types will describe better this aspect.\n\nFinnaly, let's observe negative correlational features. Negative correlations means that those features impact negatively target value, in other words, those features decreases Sale Price. ","83028c2b":"Some tips about data description are found:  \n* SalePrice is present only on train dataset, meaning about 50% of missing values\n* There are some missing values, so, total Id column and row shape are the same, but several columns has less values\n* Some columns has a wide variation min, mean and max values are so distant\n* Also, SalePrice is so spread\n* SalePrice is skewed, it is left shifted\n* There are less number types then cathegorical types. Maybe some treatment with cathegorical technics. \n* There is an Id column that is irrelevant to predictor\n* Columns with null values will be investigated bellow  \n\nAll tips will be addressed in Feature Engeneering session, but first let's take a closer eye at those missing values to confirm above related assumptions.  \nTo define what to do with each feature, in special features with missing values, let's evaluate features based on a sugestion of Luca Base on https:\/\/www.kaggle.com\/lucabasa\/house-price-cleaning-without-dropping-features\/notebook  \nFeatures will be grouped into those classifications:\n* Access Factors\n\t* Numerical\n\t\t* LotArea\n\t\t* LotFrontage\n\t* Categorical\n\t\t* MSZoning\n\t\t* Street\n\t\t* Alley\n\t\t* LotShape\n\t\t* LandContour\n\t\t* LotConfig\n\t\t* LandSlope\n\t\t* Neighborhood\n\t\t* Condition1\n\t\t* Condition2\n* House Quality\n\t* Numerical\n\t\t* OverallQual\n\t\t* OverallCond\n\t\t* YearBuilt\n\t\t* YearRemodAdd\n\t* Categorical\n\t\t* MSSubClass\n\t\t* BldgType\n\t\t* HouseStyle\n* Exterior Factors\n\t* Numerical\n\t\t* MasVnrArea\n\t* Categorical\n\t\t* Foundation\n\t\t* RoofStyle\n\t\t* RoofMatl\n\t\t* Exterior1st\n\t\t* Exterior2nd\n\t\t* MasVnrType\n\t\t* ExterQual\n\t\t* ExterCond\n* Basement\n\t* Numerical\n\t\t* BsmtFinSF1\n\t\t* BsmtFinSF2\n\t\t* BsmtUnfSF\n\t\t* TotalBsmtSF\n\t\t* BsmtFullBath\n\t\t* BsmtHalfBath\n\t* Categorical\n\t\t* BsmtQual\n\t\t* BsmtCond\n\t\t* BsmtExposure\n\t\t* BsmtFinType1\n\t\t* BsmtFinType2\n* Utilities Features\n\t* Numerical\t\n\t* Categorical\n\t\t* Utilities\n\t\t* Heating\n\t\t* HeatingQC\n\t\t* CentralAir\n\t\t* Electrical\n* Rooms Disposition\n\t* Numerical\n\t\t* 1stFlrSF\n\t\t* 2ndFlrSF\n\t\t* LowQualFinSF\n\t\t* GrLivArea\n\t\t* FullBath\n\t\t* HalfBath\n\t\t* BedroomAbvGr\n\t\t* KitchenAbvGr\n\t\t* TotRmsAbvGrd\n\t* Categorical\n\t\t* KitchenQual\n\t\t* Functional\n* Fireplaces and Garage\n\t* Numerical\n\t\t* Fireplaces\n\t\t* GarageYrBlt\n\t\t* GarageCars\n\t\t* GarageArea\n\t* Categorical\n\t\t* FireplaceQu\n\t\t* GarageType\n\t\t* GarageFinish\n\t\t* GarageQual\n\t\t* GarageCond\n\t\t* PavedDrive\n* External Areas\n\t* Numerical\n\t\t* WoodDeckSF\n\t\t* OpenPorchSF\n\t\t* EnclosedPorch\n\t\t* 3SsnPorch\n\t\t* ScreenPorch\n\t\t* PoolArea\n\t* Categorical\n\t\t* PoolQC\n\t\t* Fence\n* Sell and Misc Info\n\t* Numerical\n\t\t* MiscVal\n\t\t* YrSold\n\t\t* MoSold\n\t* Categorical\n\t\t* MiscFeature\n\t\t* SaleType\n\t\t* SaleCondition\nTo evaluate each classification, I'll do two analysis. First, let's see distribution over a histogram of numerical features to understand how relevant is each of one.  \nThen, we will take a look in how categorical features are composed to identify clusters or even irrelevant features.  \nWith these two analysis, I can define how to handle each of missing values features.","046c0d65":"### Deal with more or less relevant categorical features\n\nOther columns have frequency between 80% and 90% and won a special look at. Based on analysis above, we need to sintetize some columns as All X One and All X Second X First and finaly All different.  \nAt last, we will encode all others categorical columns bellow 80% of frequency ration as all different.\n\nTo All X Second X First and All Different types, we will use One-Hot strategy with pd.get_dummies","3710e6f2":"### Deal with numerical features\nAs exposed, some numerical features have a high degree of correlation with SalePrice and others have a negative correlation with SalePrice.  \nFor those with high correlation, it may be important to handle outliers, that can introduce some noise to the predictor.  For those outliners, let's remove them of train dataset. It will be a first try to ajust the dataset, but those outliers can be relevant when analysed with other features.  \nSo, let's create another copy of datasets as a landmark of feature engineering.\n","55227662":"As shown, **Ridge Regularization** helps to better fit the model. The accuracy is incresing up to ```alpha=10``` then RMSE decreases again. So, let's adopt this ```alpha``` value.\n\n## First Submission\nAs our fit is good, let's try a first submission and see how good is our predictor.  \nTo do so, we need the ids of each test set row and our predictions.  \n**As we didn't handled all missing values from test set, we will, at this time, interpolate those values. Then we will treat all missing values as a single dataset**  ","9ddc8bf1":"### Begin modelling\nLet's create a Linear Regression model from sklearn framework. \nAfter instantiate the model, we'll fit with X_train and y_train sets.  Then, we will score it with X_test and y_test sets.","fd0da292":"It's time to engineer our features.  \n\n## Feature Engineering\nFor feature engineering, we will proceed with some steps.  \n\n1. Fill missing values for categorical and numerical columns on training set;\n1. Set irrelevant categorical columns based on it's frequency, they will be dropped in both training and test sets;\n\n### Filling Missing Values\nWe need to fill missing values based on discussion above. Some categorical columns makes sense only with some values in other columns, they will be set to something like \"Not applied\". Another case is numerical values also related to other columns. They will be set to 0.\nFinally, there are some realy missing values, that will be filled with mean of similar rows.","ae2e8a6f":"# House Prices Prediction","060872b3":"## Build Linear Model\n\nAll features are prepared. Now, it's time to begin build the linear regression model.  \nFirst, let's prepare target values applying log to SalePrice and dropping it from training set.  \nThen, for validation purpose, we will split train dataset into two pieces, one to train our model and another to validate it. To do so, we'll use train_test_split function from scikit-learn framework.","b89dcada":"Let's understand null columns analysis and propose some remedial transformations.\n* PoolQC: Pool Quality - if there isn't a pool, there ins't a pool quality\n    * Let's fill null values with \"No pool\" string. Probably, houses with pool become more expensive based on Pool Quality\n* MiscFeature - House do not has additional features\n    * Let's fill null values with \"None\"\n* Alley - Houses without alley entrace\n    * Let's fill with \"No alley entrace\"\n* Fence - Houses without fence\n    * Let's fill with \"No fence\"\n* FireplaceQu - Houses without fireplace\n    * Where number of fire places is zero, let's fill fireplace quality with \"No Fireplaces\"\n* LotFrontage - Some houses are not directly connected to the street\n    * Maybe those properties are not connected directly to the street, so, can be filled with zero (0) \n    * Maybe they are missing values, and some how, we need to fill it within average values\n    * Let's evaluate the influence of this columns in prediction\n* Garage* - Those columns only make sense if the property has a garage\n    * Let's fill garage categorical columns with \"No garage\"\n    * Let's fill garage build year with -1\n* Bsmt* - Make sense only for those houses that have basement\n    * Let's fill with \"No basement\" rows with TotalBsmtSF equal to zero\n    * Let's fill BsmtFinType2 with \"No extra type\" where it is null\n    * Let's fill BsmtExposure with \"No\" where it is null and TotalBsmtSF is not equal to zero\n* MasVnr* - Only appliable to those houses that have Masonry veneer\n    * Let's fill MasVnrType with \"None\"\n    * Let's fill MasVnrArea with 0\n* Electrical - Really missing value  \n    * Let's fill with most common one\n    \nNow, let's evaluate categorical columns and try to figure out how they are.","72cbfbe9":"After ploted, we can see that 80% freq categorical columns are very different. They can be fitted into 4 classes:\n\n1. Bigger category and Others comprising all other categories\n    * LandContour\n    * BsmtFinType2\n    * BldgType\n1. Bigger, 2 Seconds grouped and Others comprising all other categories\n    * ExterCond\n    * SaleType\n    * SaleCondition\n1. All different categories\n    * Condition1\n \nColumns with frequency below 80% will be handled as all different categories.\n\nThe last evaluation is about numeric columns. Let's try to figure out how they are relevant to predictor.","75b3830f":"There is something relevant here. There are categorical columns that have one category that it appears most of time.  \nLet's look at categorical columns with one category with frequency that is higher then 80%.","59fbf63d":"Before evaluate individual correlations, it's clear to observe thar negative correlations are much smaller then positive ones. This means that, in general, there isn't a categorical column that impacts in a high order negatively Sale Price.   \nMaybe those columns can be dropped from dataset as it can give some bias to predictor.  \nMaybe, Overall Quality is the aspect that most increases Sale Price.  \nKitchenAbvGr (maybe kitchen above grade) is not in the dataset description document.  \nFirst, let's investigate Overall Quality and Garage Cars.  ","385bcd75":"All missing features are encodded features. So, let's add each of them and set value to zero as we are using a Hot-one technique.","b7adbcfa":"Another visualization for correlation, is a bar chart, where we can relatize those values.","ec441db3":"## Simple data visualization\nBeging with data load and simple caracteristcs visualization. Let's see how data is structured, what types of data, quality and simple distributions.  \nTo make this job easier, I'll merge train and test sets into a single set. Also, I'll create an aditional column to identify both individual sets.","6d3fbb1e":"Number of cars on garage has a different behavior. It increases Sale Price up to 3 cars then decreases Sale Price to the same level of 2 cars.  So, let's group 2 and 4 cars garages and leave others with original values. Also, let's transform it to a categorical feature and back to numerical with One-hot technique.  \nThen, the same technique can be applied to the less correlated feature OverallCond, that it will be clustered into 1 to 5 conditions and bellow 6 condition. Other less correlated features will be dropped."}}