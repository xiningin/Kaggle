{"cell_type":{"ee4a69f8":"code","5620af82":"code","9d4c3e87":"code","b5266923":"code","12305914":"code","7b42b3d0":"code","f31b00a1":"code","35b471af":"code","5b2a2cf4":"code","2da9379d":"code","7dd677e6":"code","89055cca":"code","621ed3dd":"code","06850fc4":"code","96dd2954":"code","5625eb80":"code","130df9af":"code","4950400e":"code","e9ac8ee6":"code","7ed7c695":"code","560e1f5c":"code","62bb23aa":"code","6bfc75f4":"code","883db3e6":"code","5c71bb18":"code","08e5e7b7":"code","15ba3192":"code","ddbfaff3":"code","85b1cf4d":"code","1e426bd9":"code","dee865eb":"code","6847a130":"code","f2720b7c":"code","2894243f":"code","f7d78474":"code","cc403c3e":"code","d5397ede":"code","55ed97ff":"code","c91b16a2":"code","c4158823":"code","e28aec3b":"code","ab8b9702":"code","61bd3b61":"code","3303ad07":"markdown","0d7c5f0b":"markdown","ce5d9bde":"markdown","f1cb9549":"markdown"},"source":{"ee4a69f8":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport seaborn as sns # for making plots with seaborn\nimport scipy as sp # Math functions\nfrom scipy import stats\nfrom scipy.stats import norm, skew, kurtosis\nfrom scipy.special import boxcox1p\nimport matplotlib.pyplot as plt\nimport time\n\n# scikit-learn modules\nimport sklearn\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n# sklearn modules for preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n# sklearn modules for ML model selection\nfrom sklearn.model_selection import train_test_split  \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\n# Libraries for data modelling\nfrom sklearn.linear_model import LassoCV, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.ensemble  import GradientBoostingRegressor\n\n# Common sklearn Model Helpers\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\n\n# Other ML algorithms\nfrom mlxtend.regressor import StackingCVRegressor, StackingRegressor\nprint(\"StackingRegressor imported\")","5620af82":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))","9d4c3e87":"#import training and testing dataset\ntrain_data=pd.read_csv(\"..\/input\/train.csv\")\ntest_data=pd.read_csv(\"..\/input\/test.csv\")\n#Display shape of the dataset\nprint(\"training_data(shape) : {}\".format(train_data.shape))\nprint(\"testing_data(shape) : {}\".format(test_data.shape))","b5266923":"#Extract the ID of both csv\ntrain_ID=train_data[\"Id\"]\ntest_ID=test_data[\"Id\"]\n#drop redundant ID columns\ntrain_data.drop('Id', axis=1, inplace=True)\ntest_data.drop('Id', axis=1, inplace=True)\n#print their shape\nprint(\"training_ID(shape) : {}\".format(train_ID.shape))\nprint(\"testing_ID(shape) : {}\".format(test_ID.shape))","12305914":"#Extract target variable for feature Engineering\nSalePrice=train_data['SalePrice']\n#drop SalePrice from dataframe\ntrain_data.drop('SalePrice', axis=1, inplace=True)\nprint(\"SalePrice(shape): {}\".format(SalePrice.shape))","7b42b3d0":"#concat both the dataframes \nall_data=pd.concat([train_data,test_data], ignore_index=\"true\", axis=0)\nall_data.reset_index()\nprint(\"Shape of all data: {}\".format(all_data.shape))","f31b00a1":"#print columns of dataframe\nprint(all_data.columns)\nprint()\nprint(\"Total no of columns : {}\".format(len(all_data.columns)))","35b471af":"#lets explore our target varibale\n#get the kurtosis of SalePrice\nprint(\"kurtosis: %f\" % SalePrice.kurtosis())\n#get the Skewness of SalePrice\nprint(\"Skewness: %f\" % SalePrice.skew())\n# calculate mean and standard deviation\n(mu, sigma) = norm.fit(SalePrice)\nprint('Normal dist. (mu = {:.2f} and sigma = {:.2f} )'.format(mu, sigma))","5b2a2cf4":"#plotting diffrent plots to detect outliers\nfig = plt.figure(figsize=(22,14))\nax1 = fig.add_subplot(331)\nplt.title('original distribution')\nplt.ylabel('Frequency')\nsns.distplot(SalePrice, kde=True, fit=stats.lognorm, color=\"blue\");\nax2 = fig.add_subplot(332)\nstats.probplot(SalePrice, plot=plt);\nax3 = fig.add_subplot(333)\nplt.title('boxplot')\nsns.boxplot(SalePrice,orient=\"v\")\nfig.show()","2da9379d":"#Function to count number of outliers\ndef count_outliers(target,state):\n    # count number of outliers before and after transformation is applied\n    if state==\"before\":\n        Q1 = target.quantile(0.25)\n        Q3 = target.quantile(0.75)\n        IQR = Q3 - Q1\n        print(\"Before transformation of SalePrice\")\n        print(\"Extreme outliers(Above 700000) : {} \".format([x for x in target if x > 700000]))\n        print(\"IQR value: {}\\nTotal outliers: {}\".format(IQR, ((target < (Q1 - 1.5 * IQR)) | (target > (Q3 + 1.5 * IQR))).sum()))      \n    if state==\"after\":\n        Q1 = target.quantile(0.25)\n        Q3 = target.quantile(0.75)\n        IQR = Q3 - Q1\n        print(\"After transformation of SalePrice\")\n        print(\"Extreme outliers(Above 700000) : {} \".format([x for x in target if x > 700000]))\n        print(\"IQR value: {}\\nTotal outliers: {}\".format(IQR, ((target < (Q1 - 1.5 * IQR)) | (target > (Q3 + 1.5 * IQR))).sum()))","7dd677e6":"count_outliers(SalePrice,\"before\")","89055cca":"#Apply and check different transformation on SalePrice \ndef target_transform(target,transformation=None):\n    if(transformation is None):\n        print(\"{} transformation \".format(\"Log\"))\n        #Log transformation for target variable\n        #count no of zeores in series\n        count_zero=sum(val == 0 for val in target)\n        if(count_zero > 0):\n            print(\"Transformation not applicable for zeroes values\")\n        else:\n            SalePrice=np.log1p(target)\n            return SalePrice\n    #Sqrt transformation for target variable        \n    if(transformation==\"sqrt\"):\n        print(\"{} transformation \".format(transformation))\n        SalePrice=np.sqrt(target)\n        return SalePrice\n    #boxcox1p transformation for target variable    \n    if(transformation==\"boxcox1p\"):\n        print(\"{} transformation \".format(transformation))\n        SalePrice=boxcox1p(target , 0.2)\n        return SalePrice","621ed3dd":"# since there is no zero values so Log transformation is suitable, eliminates outliers and decreases the range\nSalePrice=target_transform(SalePrice)","06850fc4":"#lets explore our target varibale log transformation\n#get the kurtosis of SalePrice\nprint(\"kurtosis: %f\" % SalePrice.kurtosis())\n#get the Skewness of SalePrice\nprint(\"Skewness: %f\" % SalePrice.skew())\n# calculate mean and standard deviation\n(mu, sigma) = norm.fit(SalePrice)\nprint('Normal dist. (mu = {:.2f} and sigma = {:.2f} )'.format(mu, sigma))","96dd2954":"#After log transformation\ncount_outliers(SalePrice,\"after\")","5625eb80":"#plotting diffrent plots log transformation\nfig = plt.figure(figsize=(22,14))\nax1 = fig.add_subplot(331)\nplt.title('Log distribution')\nplt.ylabel('Frequency')\nsns.distplot(SalePrice, kde=True, fit=stats.lognorm, color=\"blue\");\nax2 = fig.add_subplot(332)\nstats.probplot(SalePrice, plot=plt);\nax3 = fig.add_subplot(333)\nplt.title('boxplot')\nsns.boxplot(SalePrice,orient=\"v\")\nfig.show()","130df9af":"SalePrice.describe()","4950400e":"#count categorical and numerical columns\ncategorical_cols=[col for col in all_data.columns if all_data[col].dtype =='O']\nnumerical_cols=[col for col in all_data.columns if all_data[col].dtype !='O']\nprint(\"categorical_columns : \\n\")\nprint(categorical_cols)\nprint(\"\\n\")\nprint(\"numerical_columns :\\n\")\nprint(numerical_cols)","e9ac8ee6":"#lets find the missing value percentage\nnull_percentage = (all_data.isnull().sum()\/len(all_data))*100\nnull_ratio = null_percentage.sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Values Ratio' :null_ratio})\nprint(missing_data.shape)\nmissing_data.head(10)","7ed7c695":"print(\"Number of columns has a missing data\")\nprint(len(null_percentage.index) - len([x for x in null_percentage.index if null_percentage.loc[x]==0]))","560e1f5c":"#Handle missing data\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)    \nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    all_data[col] = all_data[col].fillna('None')\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')  \nfor col in ['Electrical','KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType']:\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n    ","62bb23aa":"null_percentage = (all_data.isnull().sum()\/len(all_data))*100\nnull_ratio = null_percentage.sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Values Ratio' :null_ratio})\nprint(\"Number of columns has a missing data\")\nprint(len(null_percentage.index) - len([x for x in null_percentage.index if null_percentage.loc[x]==0]))","6bfc75f4":"#Converting some numerical variables that are really categorical type\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","883db3e6":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","5c71bb18":"cols_label = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold']\n#Convert Features in label encoding\nenc_label=LabelEncoder()\nfor columns in cols_label:\n    all_data[columns] = enc_label.fit_transform(all_data[columns]) ","08e5e7b7":"numerical_features=all_data.dtypes[all_data.dtypes != \"object\"].index\nskew_percentage = all_data[numerical_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skew_percentage})\nskewness.head(10)","15ba3192":"#log transformation for top 5 features with high skewness\nskew_feat=skewness.iloc[:5]\nfor columns in skew_feat.index:\n    all_data[columns] = boxcox1p(all_data[columns],0.2) \n ","ddbfaff3":"# scale all numerical feature \nscale=StandardScaler()\nnumerical_features=all_data.dtypes[all_data.dtypes != \"object\"].index\nall_data[numerical_features]=scale.fit_transform(all_data[numerical_features].values)","85b1cf4d":"# get dummies for categorical variable\nall_data=pd.get_dummies(all_data)","1e426bd9":"# split the data in training and prediction set\ntrain_set=all_data[:len(train_ID)]\npredict_set=all_data[len(test_ID)+1:]","dee865eb":"#splits the dataset into training and testing set\nX_train, X_test, Y_train, Y_test = train_test_split(train_set, SalePrice, test_size=0.20, random_state=42)","6847a130":"model_name=[]\nr_sqaure_value=[]\nmean_sqaure_value=[]\ndef calculate_metrics(model, predicted_value, test_value ):\n    if model not in model_name:\n        model_name.append(model)\n        r_sqaure_value.append(r2_score(predicted_value,test_value))\n        mean_sqaure_value.append(mean_squared_error(predicted_value,test_value))\n#         print(model_name)\n#         print(r_sqaure_value)\n#         print(mean_sqaure_value)","f2720b7c":"# generalized ridge regression with builtin cross validation\nparams={'alphas':[0.001, 0.01 , 0.1 ,1.0],\n 'cv': 5,\n 'fit_intercept': True,\n 'gcv_mode': None,\n 'normalize': False,\n 'scoring': None,\n 'store_cv_values': False}\n\nridge_reg=RidgeCV(**params)\nridge_reg.fit(X_train,Y_train)\nridge_predicted=ridge_reg.predict(X_test)\ncalculate_metrics(\"LassoCv\", ridge_predicted, Y_test)","2894243f":"# generalized lasso regression with builtin cross validation\n\nparams={'alphas':[0.1,0.001,0.003,0.0001],\n 'copy_X': True,\n 'cv': 5,\n 'eps': 0.001,\n 'fit_intercept': True,\n 'max_iter': 2000,\n 'n_alphas': 100,\n 'n_jobs': None,\n 'normalize': False,\n 'positive': False,\n 'precompute': 'auto',\n 'random_state': None,\n 'selection': 'cyclic',\n 'tol': 0.0001,\n 'verbose': False}\n\nlasso_reg = LassoCV(**params)\nlasso_reg.fit(X_train,Y_train)\nlasso_predicted=lasso_reg.predict(X_test)\ncalculate_metrics(\"RidgeCV\", lasso_predicted, Y_test)","f7d78474":"# generalized ElasticNetCV with builtin cross validation\nparams={'alphas': [0.1,0.001, 0.02,1.0],\n 'copy_X': True,\n 'cv': 5,\n 'eps': 0.001,\n 'fit_intercept': True,\n 'l1_ratio': [0.5,0.4],\n 'max_iter': 3000,\n 'n_alphas': 100,\n 'n_jobs': None,\n 'normalize': False,\n 'positive': False,\n 'precompute': 'auto',\n 'random_state': None,\n 'selection': 'cyclic',\n 'tol': 0.0001,\n 'verbose': 0}\n\nEnet=ElasticNetCV(**params)\nEnet.fit(X_train,Y_train)\nEnet_predicted=Enet.predict(X_test)\ncalculate_metrics(\"ElasticNetCV\", Enet_predicted, Y_test)","cc403c3e":"#Grid Search for SVR\nstart = time.time()\nprint(\"Grid Search Started : {}\".format(start))\ngrid_ser_svr = GridSearchCV(\n        estimator=SVR(),\n        param_grid={'C': [0.1, 1, 0.2],\n                    'epsilon': [0.0001, 0.001, 0.005],\n                    'gamma': [0.0001, 0.001, 0.1 ],\n                    'kernel': [\"linear\", \"poly\", \"rbf\"] \n                   },\n        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\nsvr_grid= grid_ser_svr.fit(X_train,Y_train)\nend = time.time()\nprint(\"Grid Search ended: {}\".format(end))\nprint(\"Elapsed time for Grid search: {}\".format(end-start))\nprint(\"Best parameters for grid search\\n\")\nprint(svr_grid.best_params_)\nprint(\"\\n\")\ncalculate_metrics(\"SVR\", svr_grid.predict(X_test), Y_test)","d5397ede":"#Implementation of Decision tree\nparams={'criterion': 'mse',\n 'max_depth':  None,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_impurity_split': None,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'presort': True,\n 'random_state': None,\n 'splitter': 'best'}\ndeci_tree_reg=DecisionTreeRegressor(**params)\ndeci_tree_reg.fit(X_train,Y_train)\ndecision_predicted=deci_tree_reg.predict(X_test)\ncalculate_metrics(\"Decision Tree\", decision_predicted, Y_test)","55ed97ff":"#Grid Search for Random Forest Regressor\nstart = time.time()\nprint(\"Grid Search Started : {}\".format(start))\ngrid_ser_rand = GridSearchCV(\n        estimator=RandomForestRegressor(),\n        param_grid={'max_depth': [60, 50, 40, 45],\n                   'n_estimators': [5,6,7, 8],\n                   'max_leaf_nodes': [2, 3, 4,5,6]},\n        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\nrandom_grid= grid_ser_rand.fit(X_train,Y_train)\nend = time.time()\nprint(\"Grid Search ended: {}\".format(end))\nprint(\"Time taken by  Grid search: {}\".format(end-start))\nprint(\"Best parameters for grid search : \\n\")\nprint(random_grid.best_params_)\ncalculate_metrics(\"Random Forest\", random_grid.predict(X_test), Y_test)","c91b16a2":"#Grid Search for gradient boosting Regressor\nstart = time.time()\nprint(\"Grid Search Started : {}\".format(start))\ngrid_ser_boost = GridSearchCV(\n        estimator=GradientBoostingRegressor(),\n        param_grid={'max_depth': [3, 4, 6, 8],\n                   'n_estimators': [100,120,80,90]},\n        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\ngra_grid= grid_ser_boost.fit(X_train,Y_train)\nend = time.time()\nprint(\"Grid Search ended: {}\".format(end))\nprint(\"Time taken by  Grid search: {}\".format(end-start))\nprint(\"Best parameters for grid search\\n\")\nprint(gra_grid.best_params_)\ncalculate_metrics(\"GDBRT\", gra_grid.predict(X_test), Y_test)","c4158823":"#plotting Accuracy of model\nfig = plt.figure(figsize=(30,12))\nax1 = fig.add_subplot(221)\nplt.title('Accuracy of Model')\nplt.ylabel('R Square')\nsns.set(style=\"whitegrid\")\nsns.barplot(x=model_name, y=r_sqaure_value)\nfig.show()","e28aec3b":"#plotting Mean Square error of model\nfig = plt.figure(figsize=(30,12))\nax1 = fig.add_subplot(221)\nplt.title('Mean Squared Error of Model')\nplt.ylabel('Mean Squared Error')\nsns.set(style=\"whitegrid\")\nsns.barplot(x=model_name, y= mean_sqaure_value)\nfig.show()","ab8b9702":"#Taking Average predicted price of all model\nprint(\"Predicting price of house\")\nfinal_price=(np.expm1(ridge_reg.predict(predict_set))\n+np.expm1(svr_grid.predict(predict_set))\n+np.expm1(lasso_reg.predict(predict_set))\n+np.expm1(Enet.predict(predict_set))\n+np.expm1(gra_grid.predict(predict_set))\n+np.expm1(deci_tree_reg.predict(predict_set)))\/6\nfinal_price","61bd3b61":"submit = pd.DataFrame()\nsubmit['Id'] = test_ID\nsubmit['SalePrice'] = final_price\nsubmit.to_csv('submission.csv',index=False)","3303ad07":"# Ensembling","0d7c5f0b":"# Feature Engineering","ce5d9bde":"# Importing Libraries ","f1cb9549":"# Model"}}