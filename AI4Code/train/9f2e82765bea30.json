{"cell_type":{"2d6db733":"code","0c8209da":"code","2c760b2a":"code","1bf0ea7e":"code","3900d324":"code","0d1a3e0c":"code","83168c12":"code","12ca5a97":"code","18945fb5":"code","d391a19f":"code","e855599b":"code","c09ea8a0":"code","179a0672":"code","2d89bc82":"code","6f0f1744":"code","fe4b42da":"code","8eb7a177":"code","c33afe92":"code","67fc1848":"code","5db63a39":"code","2783c666":"markdown","b2de9943":"markdown","8df861c9":"markdown","0adaed03":"markdown","7ffe1875":"markdown","2b3c2da0":"markdown","85815da0":"markdown","f1efee94":"markdown","70386bee":"markdown","0b686967":"markdown","82974d2c":"markdown","06efa1d7":"markdown","90d22a76":"markdown"},"source":{"2d6db733":"import numpy as np \nimport pandas as pd\nimport pandas_profiling as pp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import mean_squared_log_error\nimport math\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectFromModel\nfrom time import time\nimport time\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import  SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import metrics","0c8209da":"train=pd.read_csv('\/kaggle\/input\/machine-hack-house-price-prediction\/Train.csv')\nsubmission=pd.read_csv('\/kaggle\/input\/machine-hack-house-price-prediction\/sample_submission.csv')\ntest=pd.read_csv('\/kaggle\/input\/machine-hack-house-price-prediction\/Test.csv')\nprint(train.shape)\ntrain.head()","2c760b2a":"print(test.shape)\ntest.head()","1bf0ea7e":"submission.head()","3900d324":"report = pp.ProfileReport(train)\nreport","0d1a3e0c":"categorical_features_train=pd.get_dummies(train.iloc[:,[0,4]])\ncategorical_features_test=pd.get_dummies(test.iloc[:,[0,4]])\ncategorical_features_train.head()","83168c12":"train=train.drop(['POSTED_BY','BHK_OR_RK','ADDRESS','LONGITUDE','LATITUDE'],axis=1)\ntest=test.drop(['POSTED_BY','BHK_OR_RK','ADDRESS','LONGITUDE','LATITUDE'],axis=1)\ntrain.head()","12ca5a97":"test.head()","18945fb5":"train=pd.concat([train,categorical_features_train],axis=1)\ntest=pd.concat([test,categorical_features_test],axis=1)\ntrain.head()","d391a19f":"test.head()","e855599b":"X = train.drop(\"TARGET(PRICE_IN_LACS)\",axis=1)   #Feature Matrix\ny = train[\"TARGET(PRICE_IN_LACS)\"]\nX.head()","c09ea8a0":"#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = X.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r)\nplt.show()","179a0672":"def correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","2d89bc82":"corr_features = correlation(X, 0.85)\nprint(len(set(corr_features)))\nprint(corr_features)","6f0f1744":"X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.1)\nprint(X_train.shape)\nprint(X_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)","fe4b42da":"X_train=X_train.drop(corr_features,axis=1)\nX_val=X_val.drop(corr_features,axis=1)\ntest=test.drop(corr_features,axis=1)\nX_train.head()","8eb7a177":"test.head()","c33afe92":"names = [ \n        \"K Nearest Neighbour Regressor\",\n         \"Decison Tree Regressor\",\n         \"Random Forest Regressor\",\n\n         ]\nregressors = [\n    KNeighborsRegressor(),\n    DecisionTreeRegressor(),\n    RandomForestRegressor(),\n        ]\n\nzipped_clf = zip(names,regressors)","67fc1848":"def rmsle(y, y_pred):\n    terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(terms_to_sum) * (1.0\/len(y))) ** 0.5\n\ndef acc_summary(pipeline, X_train, y_train, X_val, y_val):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    y_pred = sentiment_fit.predict(X_val)\n    #rmsle = np.sqrt(mean_squared_log_error(y_val,y_pred ))\n    loss= rmsle(y,y_pred)\n    print(\"RMSLE : {0:.2f}\".format(loss))\n    #print(\"train and test time: {0:.2f}s\".format(train_test_time))\n    print(\"-\"*80)\n    return rmsle\n\n\ndef regressor_comparator(X_train,y_train,X_val,y_val,regressor=zipped_clf): \n    result = []\n    for n,r in regressor:\n        checker_pipeline = Pipeline([\n            ('regressor', r)\n        ])\n        print(\"Validation result for {}\".format(n))\n        #print(r)\n        clf_acc= acc_summary(checker_pipeline,X_train, y_train, X_val, y_val)\n        result.append((n,clf_acc))\n    return result","5db63a39":"regressor_comparator(X_train,y_train,X_val,y_val)","2783c666":"# Importing The Required Libraries.","b2de9943":"# A quick pandas-profiling EDA","8df861c9":"We can see that the test dataset is three times larger than the train dataset.","0adaed03":"# Importing The Dataset.","7ffe1875":"### The metric used in this Competetion is Root Mean Square Log Error.\n![image.png](attachment:image.png)","2b3c2da0":"# <font color='red' > Please Suggest Ways to Improve the evaluation score in the comment section <\/font>","85815da0":"# Please Suggest Ways to improve the RMSLE,Your help will be really appreciated.Thankyou...","f1efee94":"There was some problem arising when I used numpy and sklearn to find RMSLE,some negative values were being predicted,So I had to manually define the Function.","70386bee":"### I am going to create a modular function here,so that I can test the different types of regressors.","0b686967":"So now we have the datasets ready,lets fit and see the accuracy.","82974d2c":"# Splitting The Dataset.","06efa1d7":"# Using Pearson Correlation to drop the similar features.","90d22a76":"### Encoding the Categorical features."}}