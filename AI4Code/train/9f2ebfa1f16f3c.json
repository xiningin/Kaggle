{"cell_type":{"aa227268":"code","d76a2f03":"code","b1570e6d":"code","b47ca869":"code","f82a9215":"code","9a51dbfa":"code","1c0eaa6f":"code","74ba8bcc":"code","51186a70":"code","ecb94306":"code","a1e8c5e0":"code","59252d94":"code","f2951e46":"code","26fa1eeb":"code","4f85f100":"code","f2348d05":"code","a8f4e36e":"code","03bef7b5":"code","460e028c":"code","6841a952":"code","26e08e53":"code","9e233bcc":"code","23cf9df1":"code","a4899140":"code","f97d48e5":"code","7f316e46":"code","352e41a4":"code","962c978e":"code","036ffe59":"code","cf430d2d":"code","d92eb9f0":"code","6750033b":"code","aa74f592":"code","311fc510":"code","8e8a06d3":"code","9e403f6b":"code","460fa892":"code","9c55a76a":"code","c05e3710":"code","da50b10a":"code","7ce04727":"code","c3b7fdf9":"code","965e4c98":"code","eec5e140":"code","339e0b2e":"code","23a385aa":"code","15ccbbd8":"code","8d2910ee":"markdown","e1599db5":"markdown","4fa1aac6":"markdown","95e1817f":"markdown","76a9ff75":"markdown","58137f0f":"markdown","ee762955":"markdown","0d180e52":"markdown","c2845cf0":"markdown","054892b6":"markdown","04a8feff":"markdown","2c02a807":"markdown","d13e9a8c":"markdown","ee73250e":"markdown","8ab71567":"markdown","81b143ce":"markdown","f356a805":"markdown","0f267855":"markdown","d4d0a2b6":"markdown","adb74da2":"markdown","e5a5b794":"markdown"},"source":{"aa227268":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","d76a2f03":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ntest['Survived']= submission['Survived']","b1570e6d":"df = pd.concat([test.assign(ind=\"test\"), train.assign(ind=\"train\")])","b47ca869":"df.head(2)","f82a9215":"df.info()","9a51dbfa":"df.shape","1c0eaa6f":"sns.heatmap(df.corr(), annot=True)","74ba8bcc":"df['Survived'].value_counts()","51186a70":"sns.countplot(data=df, x='Survived')","ecb94306":"df = df.drop(['Cabin'] , axis=1)","a1e8c5e0":"df = df.drop(['PassengerId'] , axis=1)\ndf = df.drop(['Ticket'] , axis=1)\ndf = df.drop(['Name'] , axis=1)","59252d94":"df = df.dropna(axis=0, subset=['Age'])","f2951e46":"df = df.dropna(axis=0, subset=['Embarked'])\ndf = df.dropna(axis=0, subset=['Fare'])","26fa1eeb":"df['Pclass'] = df['Pclass'].apply(str)","4f85f100":"df_num = df.select_dtypes(exclude='object')\ndf_obj = df.select_dtypes(include='object')","f2348d05":"non_dummy_cols = ['ind']\n# Takes all other columns\ndummy_cols = list(set(df_obj.columns) - set(non_dummy_cols))\ndf_obj = pd.get_dummies(df_obj, columns=dummy_cols, drop_first=True)","a8f4e36e":"df = pd.concat([df_num, df_obj], axis = 1)","03bef7b5":"test, df = df[df[\"ind\"].eq(\"test\")], df[df[\"ind\"].eq(\"train\")]\n\n# We should Drop indicator Column from test and train dataframes:\ntest= test.drop(['ind'], axis=1)\ndf= df.drop(['ind'], axis=1)","460e028c":"df.info()","6841a952":"df.head()","26e08e53":"X= df.drop('Survived', axis=1)\ny= df['Survived']","9e233bcc":"from sklearn.model_selection import train_test_split","23cf9df1":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)","a4899140":"from sklearn.preprocessing import StandardScaler","f97d48e5":"scaler= StandardScaler()","7f316e46":"scaler.fit(X_train)","352e41a4":"scaled_X_train= scaler.transform(X_train)\nscaled_X_test= scaler.transform(X_test)","962c978e":"from sklearn.linear_model import LogisticRegression","036ffe59":"log_model= LogisticRegression()","cf430d2d":"log_model.fit(scaled_X_train, y_train)","d92eb9f0":"log_model.coef_","6750033b":"y_pred= log_model.predict(scaled_X_test)","aa74f592":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix","311fc510":"accuracy_score(y_test, y_pred)","8e8a06d3":"confusion_matrix(y_test, y_pred)","9e403f6b":"plot_confusion_matrix(log_model, scaled_X_test, y_test)","460fa892":"print(classification_report(y_test, y_pred))","9c55a76a":"from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, plot_roc_curve","c05e3710":"plot_precision_recall_curve(log_model, scaled_X_test, y_test)","da50b10a":"plot_roc_curve(log_model, scaled_X_test, y_test)","7ce04727":"X_test, y_test = test.drop(columns='Survived').copy(), test['Survived'].copy()","c3b7fdf9":"scaled_X_test= scaler.transform(X_test)","965e4c98":"y_pred= log_model.predict(scaled_X_test)","eec5e140":"accuracy_score(y_test, y_pred)","339e0b2e":"confusion_matrix(y_test, y_pred)","23a385aa":"plot_confusion_matrix(log_model, scaled_X_test, y_test)","15ccbbd8":"print(classification_report(y_test, y_pred))","8d2910ee":"### Data Overview","e1599db5":"### Import Dataset\n\n> #### The Titanic Dataset\n> In this dataset we have the available data for each person who was on the Titanic and survived or not. The label of this database is the \"Survived\" column.","4fa1aac6":"### Split the Dataset to Tain & Test set","95e1817f":"#### Dealing with Categorical Data\n\"Pclass\" is Categorical Data but its integer so we should convert it.","76a9ff75":"> #### Deal with \"Cabin\" Column\n> \n> In reality, this factor may be effective in a person's survival, but in this dataset, we don't have enough information and only 204 samples are known, so this column does not give us enough information to make a decision. So we delete this column.","58137f0f":"### Convert All Object type to One hot encoding","ee762955":"#### Model Coeficient:","0d180e52":"#### Deal with \"Age\" Column\nThe \"Age\" column in this dataset is important so we should have all data in this column for traning. \nso if some rows of this column are Null we have to remove those from Dataframe:","c2845cf0":"### Evaluating Curves and AUC","054892b6":"#### Combine Train and Test Datasets for Data Cleaning","04a8feff":"We have some null in Embarked and Fare columns:","2c02a807":"### Determine the Features & Target Variable","d13e9a8c":"### Evaluating the Model","ee73250e":"### Import Libraries","8ab71567":"### Train the Model","81b143ce":"#### Some columns have no effect on survival so we remove them:","f356a805":"### Predict Labels for test.csv","0f267855":"## Data Cleaning","d4d0a2b6":"# Logistic Regression\n\nLogistic regression is a statistical analysis method used to predict a data value based on prior observations of a data set. Logistic regression has become an important tool in the discipline of machine learning. The approach allows an algorithm being used in a machine learning application to classify incoming data based on historical data. As more relevant data comes in, the algorithm should get better at predicting classifications within data sets. Logistic regression can also play a role in data preparation activities by allowing data sets to be put into specifically predefined buckets during the extract, transform, load (ETL) process in order to stage the information for analysis.\n\nA logistic regression model predicts a dependent data variable by analyzing the relationship between one or more existing independent variables. For example, a logistic regression could be used to predict whether a political candidate will win or lose an election or whether a high school student will be admitted to a particular college.\n\nThe resulting analytical model can take into consideration multiple input criteria. In the case of college acceptance, the model could consider factors such as the student\u2019s grade point average, SAT score and number of extracurricular activities. Based on historical data about earlier outcomes involving the same input criteria, it then scores new cases on their probability of falling into a particular outcome category.\n\n#### Purpose and examples of logistic regression\nLogistic regression is one of the most commonly used machine learning algorithms for binary classification problems, which are problems with two class values, including predictions such as \u201cthis or that,\u201d \u201cyes or no\u201d and \u201cA or B.\u201d\n\nThe purpose of logistic regression is to estimate the probabilities of events, including determining a relationship between features and the probabilities of particular outcomes.\n\nOne example of this is predicting if a student will pass or fail an exam when the number of hours spent studying is provided as a feature and the variables for the response has two values: pass and fail.\n\nOrganizations can use insights from logistic regression outputs to enhance their business strategies so they can achieve their business goals, including reducing expenses or losses and increasing ROI in marketing campaigns, for example.\n\nAn e-commerce company that mails expensive promotional offers to customers would like to know whether a particular customer is likely to respond to the offers or not. For example, they\u2019ll want to know whether that consumer will be a \u201cresponder\u201d or a \u201cnon responder.\u201d In marketing, this is called propensity to respond modeling.\n\nLikewise, a credit card company develops a model to decide whether to issue a credit card to a customer or not will try to predict whether the customer is going to default or not on the credit card based on such characteristics as annual income, monthly credit card payments and number of defaults. In banking parlance, this is known as default propensity modeling.\n\n#### Uses of logistic regression\nLogistic regression has become particularly popular in online advertising, enabling marketers to predict the likelihood of specific website users who will click on particular advertisements as a yes or no percentage.\n\n*source: [techtarget](https:\/\/searchbusinessanalytics.techtarget.com\/definition\/logistic-regression)*","adb74da2":"### Predicting Test Data","e5a5b794":"### Scaling the Features"}}