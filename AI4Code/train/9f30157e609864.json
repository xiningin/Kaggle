{"cell_type":{"55004031":"code","3a676fd3":"code","b66dabbb":"code","db9e34cf":"code","6b2c49e3":"code","5247eea9":"code","04a42a74":"code","e0887ee6":"code","9d1d11a4":"code","e0e192bc":"code","ae28f7ee":"code","118b4b6f":"code","a7011f2c":"code","a6592a6e":"code","db36b145":"code","fbe735be":"code","339d5142":"code","9f761fe4":"code","23aa1373":"code","b2a221b6":"code","ba3452b2":"code","52b379be":"code","1db486b4":"markdown","f74b46e6":"markdown","9562713b":"markdown","1170da2e":"markdown","20715dba":"markdown","8ae75156":"markdown","1c507801":"markdown","771d88c6":"markdown","55ccd1c9":"markdown","ae673815":"markdown","e06cc02f":"markdown","9e1bb655":"markdown","be5c6ee1":"markdown","a49bdf83":"markdown","a6816ad9":"markdown","62b00bb1":"markdown","2885cdb5":"markdown","ae4e46fe":"markdown","9297754c":"markdown","869a26a0":"markdown","db3b0cf8":"markdown","7fbbdd3d":"markdown","da2b7200":"markdown"},"source":{"55004031":"import pandas as pd\nfrom sklearn.ensemble import *\nfrom sklearn.tree import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import *\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import *\nfrom sklearn.linear_model import *\nfrom sklearn.naive_bayes import *\nfrom sklearn.svm import *\nfrom sklearn.neighbors import *\nfrom sklearn.tree import *\nfrom sklearn.metrics import *\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3a676fd3":"df = pd.read_csv(\"..\/input\/human-activity-recognition-with-smartphones\/train.csv\")\ndf.head()","b66dabbb":"df['Activity'].groupby(df['Activity']).count()","db9e34cf":"activity = df['Activity'].groupby(df['Activity']).count().index\nactivity_data = df['Activity'].groupby(df['Activity']).count().values\ncolors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#8c564b\",\"#a4d321\"]\nplt.pie(activity_data, labels=activity,  colors=colors , autopct='%1.1f%%', shadow=True, startangle=140)\nplt.title(\"% of Different categories\")\nplt.show()","6b2c49e3":"print(df.isna().sum())","5247eea9":"x = df.drop(['Activity'],axis=1)\ny = df['Activity']","04a42a74":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","e0887ee6":"classifiers = [\n    KNeighborsClassifier(5),\n    SVC(kernel=\"rbf\"),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    GaussianNB(),\n    RidgeClassifier(),\n    LogisticRegression(max_iter=200)\n]","9d1d11a4":"def f_score(X_train, X_test, y_train, y_test):\n    for clf in classifiers:\n        s = time.time()\n        clf.fit(X_train,y_train)\n        y_pred = clf.predict(X_test)\n        f = f1_score(y_true=y_test,y_pred=y_pred,average=\"macro\")\n        e = time.time()\n        print(f\"Score: {round(f,3)} \\t Time(in secs): {round(e-s,3)} \\t Classifier: {clf.__class__.__name__}\")","e0e192bc":"f_score(X_train, X_test, y_train, y_test)","ae28f7ee":"df_test = pd.read_csv(\"..\/input\/human-activity-recognition-with-smartphones\/test.csv\")\ndf_test_x = df_test.drop(['Activity'],axis=1)\ndf_test_y = df_test['Activity']\nf_score(x, df_test_x, y, df_test_y)","118b4b6f":"estimators = [\n        ('RFC' ,RandomForestClassifier(n_estimators=500, random_state = 42)),\n        ('KNC', KNeighborsClassifier(5)),\n        ('DTC', DecisionTreeClassifier()),\n        ('SVC', SVC(kernel=\"rbf\")),\n        ('RC',  RidgeClassifier()),\n]\n\nclf = StackingClassifier(\n    estimators=estimators, \n    final_estimator=GradientBoostingClassifier()\n)","a7011f2c":"s = time.time()\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\ne = time.time()\nprint(f\"time consumed: {round(e-s,3)}\")\nf1_score(y_true=y_test,y_pred=y_pred,average=\"macro\")","a6592a6e":"s = time.time()\nclf.fit(x,y)\ny_pred = clf.predict(df_test_x)\ne = time.time()\nprint(f\"time consumed: {round(e-s,3)}\")\nf1_score(y_true=df_test_y,y_pred=y_pred,average=\"macro\")","db36b145":"sel = SelectFromModel(RandomForestClassifier())\nsel.fit(x,y)","fbe735be":"features = x.columns[(sel.get_support())]\nprint(len(features))\nfeatures","339d5142":"X1 = x.filter(items=features)","9f761fe4":"X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size = 0.2, random_state = 42)","23aa1373":"f_score(X_train, X_test, y_train, y_test)","b2a221b6":"f_score(X1, df_test_x.filter(items=features), y, df_test_y)","ba3452b2":"s = time.time()\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\ne = time.time()\nprint(f\"time consumed: {round(e-s,3)}\")\nf1_score(y_true=y_test,y_pred=y_pred,average=\"macro\")","52b379be":"s = time.time()\nclf.fit(X1,y)\ny_pred = clf.predict(df_test_x.filter(items=features))\ne = time.time()\nprint(f\"time consumed: {round(e-s,3)}\")\nf1_score(y_true=df_test_y,y_pred=y_pred,average=\"macro\")","1db486b4":"### Accuracy for test data","f74b46e6":"Hence Random Forest find these 125 features as important","9562713b":"# Trying reducing number of features\nRandom forest classifier determines importance of variables.  \nThis can be used to filter most important features.  \nYou may also use Logistic regression.  \nTo understand simply: Logistic regression determines linear coeffecients.  \nCoeffecients with higher magnitudes have a greater impact on `Y` than others.","1170da2e":"### Accuracy for train data","20715dba":"### Accuracy for test data","8ae75156":"the stacking classifier does a great work of boosting accuracy to **99+** for train data and **96+** for test data.  \nHowever it consumes a lot of time.  ","1c507801":"# Training Models","771d88c6":"### Accuracy for train data","55ccd1c9":"### The tradeoff between score and time\nIn most cases the models will be trained prior and deployed with just the weights, however in situations with on device processing like a smartphone we need to decide what we want.  \nStacking almost always boosts your accuracy as explained in case above, it does comes at the cost of extra training time.  \nI hope this notebook helped you.  \n**Happy Learning**","ae673815":"# Train with reduced Dataset with Stacking Classifier","e06cc02f":"# Stacking Classifier\nStacking classifier build a new classifier.  \nTo learn more refer [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.StackingClassifier.html)","9e1bb655":"# Training on reduced dataset","be5c6ee1":"# Machine Learning Models for Human Activity Recognition\n\nThe Dataset used has a lot of features which calls for a tonne of preprocessing.  \nHowever the aim of the notebook is to compare ML models for unprocessed data and try to increase score.  \nWe'll also learn about a feature selection method which can be done to increase score for some or decrease time.  \nIt's basically a tradeoff between time and score.  \n\nEDA for the same has been well demonstrated in [this](https:\/\/www.kaggle.com\/abheeshthmishra\/eda-of-human-activity-recognition) notebook.","a49bdf83":"# % of Different categories\nAs the percentage is roughly equal, hence we can consider it to a balanced dataset.  \nHowever we'll still use F1-score for comparisons","a6816ad9":"After Feature selection we get maximum train score of **98+** and test score of **93+**","62b00bb1":"### Accuracy for train data","2885cdb5":"**The Above score achieved is after splitting train data and not test data**","ae4e46fe":"# Importing Train Data","9297754c":"# F1-Score\n\nRecall = TruePositives \/ (TruePositives + FalseNegatives)\n\nPrecision = TruePositives \/ (TruePositives + FalsePositives)\n\nF1 = 2 (precision recall) \/ (precision + recall)","869a26a0":"### Accuracy for test data","db3b0cf8":"## Checking the number of null values","7fbbdd3d":"### Accuracy for test data","da2b7200":"### Accuracy for train data"}}