{"cell_type":{"aa99c70e":"code","8e6185e7":"code","9b13f31a":"code","00b663b2":"code","7196153e":"code","fedd49e5":"code","52eb8554":"code","c74277b4":"code","627dc885":"code","dc27ad47":"code","835af0d7":"code","cf1df244":"code","f44a7a4f":"code","9d51ebb1":"code","bcad37f2":"code","9369e9d9":"code","bc1b5fa6":"code","e3b4410f":"code","f2879db4":"code","f448c4d7":"code","b70a5e8c":"markdown","21b95a7d":"markdown","2c962b16":"markdown","e97d3055":"markdown","705260c8":"markdown","e604c79d":"markdown","74189d5b":"markdown","2b5a59be":"markdown","c091643c":"markdown"},"source":{"aa99c70e":"# importing required libraries \n\nimport pandas as pd\n\n# for pytorch imports\nimport torch\n\n# for functional dependencies like activation function \nimport torch.nn.functional as F\n\n# nn is basic module in Torch which provide different neural network architecture\nimport torch.nn as nn\n\n# for optimizer\nimport torch.optim as optim\n\n# CountVectorizer for Bagof words model\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# for padding .. since the LSTM takes input as sequence so it is said that \n#if we have fixed input string computation will be faster and it will improve performance \nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm, tqdm_notebook","8e6185e7":"# GPU ..... vrooom vrooom vroooooooooom !!!!\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","9b13f31a":"class Sequences(Dataset):\n    def __init__(self, path, max_seq_len):\n        self.max_seq_len = max_seq_len\n        df = path\n        \n        # BOW \n        vectorizer = CountVectorizer(stop_words='english', min_df=0.015)\n        vectorizer.fit(df.review.tolist())\n        \n        # Creating Vocabulary\n        self.token2idx = vectorizer.vocabulary_\n        \n        self.token2idx['<PAD>'] = max(self.token2idx.values()) + 1\n\n        tokenizer = vectorizer.build_analyzer()\n        self.encode = lambda x: [self.token2idx[token] for token in tokenizer(x)\n                                 if token in self.token2idx]\n        self.pad = lambda x: x + (max_seq_len - len(x)) * [self.token2idx['<PAD>']]\n        \n        sequences = [self.encode(sequence)[:max_seq_len] for sequence in df.review.tolist()]\n        sequences, self.labels = zip(*[(sequence, label) for sequence, label\n                                    in zip(sequences, df.label.tolist()) if sequence])\n        self.sequences = [self.pad(sequence) for sequence in sequences]\n\n    def __getitem__(self, i):\n        assert len(self.sequences[i]) == self.max_seq_len\n        return self.sequences[i], self.labels[i]\n    \n    def __len__(self):\n        return len(self.sequences)","00b663b2":"data  = pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndata['label'] = data['sentiment']\ndel data['sentiment']\ndata.head()","7196153e":"labeling = {\n    'positive':1, \n    'negative':0\n}","fedd49e5":"data['label'] = data['label'].apply(lambda x : labeling[x])\n","52eb8554":"data.shape","c74277b4":"# feeding data in class and getting its instance in return \ndataset = Sequences(data, max_seq_len=128)\n","627dc885":"len(dataset.token2idx)\n","dc27ad47":"def collate(batch):\n    inputs = torch.LongTensor([item[0] for item in batch])\n    target = torch.FloatTensor([item[1] for item in batch])\n    return inputs, target\n\nbatch_size = 2048\ntrain_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate)","835af0d7":"\n\nclass RNN(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        batch_size,\n        embedding_dimension=100,\n        hidden_size=128, \n        n_layers=1,\n        device='cpu'\n    ):\n        super(RNN, self).__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        self.device = device\n        self.batch_size = batch_size\n        self.encoder = nn.Embedding(vocab_size, embedding_dimension)\n        self.rnn = nn.LSTM(\n            embedding_dimension,\n            hidden_size,\n            num_layers=n_layers,\n            batch_first=True,\n        )\n        self.decoder = nn.Linear(hidden_size, 1)\n        \n    def init_hidden(self ):\n        \n        return (torch.randn(self.n_layers, self.batch_size, self.hidden_size).to(self.device),\n                torch.randn(self.n_layers, self.batch_size, self.hidden_size).to(self.device) )\n       \n    \n    def forward(self, inputs):\n        # Avoid breaking if the last batch has a different size\n        batch_size = inputs.size(0)\n        if batch_size != self.batch_size:\n            self.batch_size = batch_size\n            \n        encoded = self.encoder(inputs)\n        output, hidden = self.rnn(encoded, self.init_hidden())\n        #o\n        output = self.decoder(output[:, :, -1]).squeeze()\n        return output","cf1df244":"model = RNN(\n    hidden_size=128,\n    vocab_size=len(dataset.token2idx),\n    device=device,\n    batch_size=batch_size\n)\nmodel = model.to(device)\nmodel","f44a7a4f":"criterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.001)","9d51ebb1":"model.train()\ntrain_losses = []\nfor epoch in range(5):\n    progress_bar = tqdm_notebook(train_loader, leave=False)\n    losses = []\n    total = 0\n    for inputs, target in progress_bar:\n        inputs, target = inputs.to(device), target.to(device)\n        model.zero_grad()\n        \n        output = model(inputs)\n    \n        loss = criterion(output, target)\n        \n        loss.backward()\n              \n        nn.utils.clip_grad_norm_(model.parameters(), 3)\n\n        optimizer.step()\n        \n        progress_bar.set_description(f'Loss: {loss.item():.3f}')\n        \n        losses.append(loss.item())\n        total += 1\n    \n    epoch_loss = sum(losses) \/ total\n    train_losses.append(epoch_loss)\n\n    tqdm.write(f'Epoch #{epoch + 1}\\tTrain Loss: {epoch_loss:.3f}')","bcad37f2":"def predict_sentiment(text):\n    model.eval()\n    with torch.no_grad():\n        test_vector = torch.LongTensor([dataset.pad(dataset.encode(text))]).to(device)\n        \n        output = model(test_vector)\n        prediction = torch.sigmoid(output).item()\n\n        if prediction > 0.5:\n            print(f'{prediction:0.3}: Positive sentiment')\n        else:\n            print(f'{prediction:0.3}: Negative sentiment')","9369e9d9":"text= \"that's nice\"\npredict_sentiment(text)\n","bc1b5fa6":"text= \"that's worst\"\npredict_sentiment(text)","e3b4410f":"# QUOTE FROM RICH DAD POOR DAD\ntext= \"In school we learn that mistakes are bad, and we are punished for making them. Yet, if you look at the way humans are designed to learn, we learn by making mistakes. We learn to walk by falling down. If we never fell down, we would never walk\"\nprint(text)\n\npredict_sentiment(text)","f2879db4":"text = \"\"\"I love this car.\nThis view is amazing.\nI feel great this morning.\nI am so excited about the concert.\nHe is my best friend\n\"\"\"\npredict_sentiment(text)","f448c4d7":"text=\"\"\"\nI do not like this car.\nThis view is horrible.\nI feel tired this morning.\nI am not looking forward to the concert.\nHe is my enemy\n\"\"\"\npredict_sentiment(text)","b70a5e8c":"# \ud83e\udd28 \ud83e\uddd0  But Why RNN failed and LSTM & GRU invented?\ud83e\udd28 \ud83e\uddd0\n\n### Vanishing Gradient: \n- so as we start to feeding the long sentences to the RNN model the 1st hidden layers sends the information to 2nd hidden layer and 2nd to 3rd hidden layer and this goes on.\n- So while backpropagation we adjust the weights but as coming till first layer the gradient descent value becomes so small that it will not do any learning and the information will get lost\n- And this was the main reason of born of LSTM & GRU: GATED RECURRENT NEURAL NETWORK Which has update gate, forget gate etc that will look next\n![VANISHING](https:\/\/miro.medium.com\/max\/700\/1*PYiQa_bNzM8ugYz_D1yvgw.png)\n","21b95a7d":"# PyTorch Implementation","2c962b16":"<img src =\"https:\/\/miro.medium.com\/max\/700\/1*yBXV9o5q7L_CvY7quJt3WQ.png\" width = 1000\/> \n\n### **LSTM** : \n \n \nLSTM\u2019s are the cell state, and it\u2019s various gates. The cell state act as a transport highway that transfers relative information all the way down the sequence chain. You can think of it as the \u201cmemory\u201d of the network. The cell state, in theory, can carry relevant information throughout the processing of the sequence. So even information from the earlier time steps can make it\u2019s way to later time steps, reducing the effects of short-term memory. As the cell state goes on its journey, information get\u2019s added or removed to the cell state via gates. The gates are different neural networks that decide which information is allowed on the cell state. The gates can learn what information is relevant to keep or forget during training.\n\n- will update the theory in details \n\n## **GRU** \n\n - The GRU is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM. GRU\u2019s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.\n - Update Gate\n            The update gate acts similar to the forget and input gate of an LSTM. It decides what information to throw away and what new information to add.\n            \n- Reset Gate\n            The reset gate is another gate is used to decide how much past information to forget.\n\nAnd that\u2019s a GRU. GRU\u2019s has fewer tensor operations; therefore, they are a little speedier to train then LSTM\u2019s. There isn\u2019t a clear winner which one is better. Researchers and engineers usually try both to determine which one works better for their use case\n","e97d3055":"# LSTM and GRU \n","705260c8":"__Encoding positive as 1 and negative as 0__","e604c79d":"References : \n\nhttps:\/\/towardsdatascience.com\/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n\nhttps:\/\/towardsdatascience.com\/the-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22\n\nhttps:\/\/towardsdatascience.com\/multi-class-text-classification-with-lstm-1590bee1bd17","74189d5b":"#### DO UPVOTE \u2b06\ufe0f\u2b06\ufe0f\u2b06\ufe0f\u2b06\ufe0f\u2b06\ufe0f\u2b06\ufe0f\n#### DO COMMENT \ud83d\udcac\ud83d\udcac\ud83d\udcac\ud83d\udcac\ud83d\udcac\ud83d\udcac\ud83d\udcac\ud83d\udcac\n#### Feel free to post for suggestions \ud83d\udcac\ud83d\udcac\ud83d\udcac\ud83d\udcac\ud83d\udcac\ud83d\udcac\ud83d\udcac\ud83d\udcac\n<img src =\"https:\/\/i.pinimg.com\/originals\/f5\/f1\/26\/f5f12634f6378186aa4f88455b122eda.gif\" width=1000 height=800>","2b5a59be":"# \ud83c\udf7fRNN, GRU LSTM  using Pytorch\ud83c\udf7f \n\nHi, welcome to my second pytorch series for nlp. Link for the first notebook is [here](https:\/\/www.kaggle.com\/rushinaik\/mission-torch-1).\n\n## WAIT!!!............. IF YOU LIKE MY WORD PLEASE DO UPVOTE \n\n\n### **RNN** : \nRecurrent Neural Network. It's a type of neural network which accepts sequence of the data and it works like a human brain.\n\nBut how rnn works : So RNN takes sequence of the vectors as a input.The RNN processes the sequence of vectors one by one.While processing, it passes the previous hidden state to the next step of the sequence. The hidden state acts as the neural networks memory which helps to hold the previous vectors information in the hidden state. and in that way RNN tracks the previous information also.\n\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/960\/1*TqcA9EIUF-DGGTBhIx_qbQ.gif\" width=1000>\n\n# Architecture of RNN : \nIt has :\n1. Input layer : Vectors which we will feed in RNN layer \n2. previous hidden state : Previous hidden state is the output of previous RNN which are supposed to feed in second layer. It works like we will take input and then we will process the data. we will feed the processed data to next layer in that way the information in sequence vector will be used \n3. Tanh activation : The tanh activation is used to help regulate the values flowing through the network. Tanh function transform the vectors values between -1 to 1. so if we have multplication operation in our NN it will be huge after certain iterations, so here tanh will come handy and convert it in to -1 to 1\n4. Concatenation : General Addition of two vectors\n\n![Architecture](https:\/\/miro.medium.com\/max\/1900\/1*KxrxyB10ZbOc3xjDneQdhA.gif)","c091643c":"### Class Sequences :\n- it will take text dataset as input and processed the text, tokenize it to sequences, pad it\n- function __getitem__ willl return the item at particular index\n- __len__ return lenght of the sequence"}}