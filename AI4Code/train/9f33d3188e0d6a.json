{"cell_type":{"a7232f13":"code","36e0cbdc":"code","136d3433":"code","131ccb2c":"code","0e145f7d":"code","60c53c42":"code","c73851a7":"code","f02073e6":"code","184965c3":"code","84090ec3":"code","31d89010":"code","dd983328":"code","573d925d":"code","1a1f75e8":"code","77e7e376":"code","f67e830d":"code","79097451":"code","eb81cdcd":"code","0a851645":"code","915b08c8":"code","d7677e4a":"code","887c32b4":"code","b9311769":"code","508de772":"code","b1b64f46":"code","11a33670":"code","3550e0e0":"code","c767722c":"code","497d3473":"code","1f2fb4e2":"code","163f8920":"code","b1e11e3f":"code","8417264d":"markdown","143d2538":"markdown","a3673eff":"markdown","eaa92860":"markdown","ea454303":"markdown","6ff8e602":"markdown","c776b0e2":"markdown","1b0ee30f":"markdown","11db30d2":"markdown","94f08c1d":"markdown","c722d73a":"markdown","f23dbae6":"markdown","52130a36":"markdown","8138138d":"markdown","fffd9ce2":"markdown","d0518383":"markdown","fd133810":"markdown","7e13b691":"markdown","8c9ca640":"markdown","7ac2730f":"markdown"},"source":{"a7232f13":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.data import iris_data\nfrom mlxtend.plotting import plot_decision_regions","36e0cbdc":"clf1 = LogisticRegression(random_state=0)\nclf2 = RandomForestClassifier(random_state=0)\nclf3 = SVC(random_state=0, probability=True)\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[2, 1, 1], voting='soft')","136d3433":"X, y = iris_data()\nX = X[:,[0, 2]]","131ccb2c":"gs = gridspec.GridSpec(2, 2)\nfig = plt.figure(figsize=(10, 8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n                         ['Logistic Regression', 'Random Forest', 'RBF kernel SVM', 'Ensemble'],\n                         itertools.product([0, 1], repeat=2)):\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\nplt.show()","0e145f7d":"import numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets.samples_generator import make_blobs\nsns.set()","60c53c42":"X, y = make_blobs(n_samples=600, centers= 5, cluster_std = 0.60, random_state = 42)","c73851a7":"plt.scatter(X[:, 0], X[:, 1], s = 10)","f02073e6":"from scipy.cluster.hierarchy import ward, dendrogram, linkage\nnp.set_printoptions(precision=4, suppress=True)","184965c3":"distance = linkage(X, 'ward')","84090ec3":"plt.figure(figsize = (35, 8))\nplt.title(\"Hierarchical Clustering Dendogram\")\nplt.xlabel('Index')\nplt.ylabel(\"Ward Distance\")\nplt.show(dendrogram(distance,\n          leaf_rotation = 90,\n          leaf_font_size = 9.,))","31d89010":"plt.figure(figsize=(25, 10))\nplt.title(\"Hierachical Clustering Dendrogram\")\nplt.xlabel(\"Index\")\nplt.ylabel('Ward Distance')\ndendrogram(distance, orientation='left', leaf_rotation=90., leaf_font_size = 9.)\nplt.show()","dd983328":"plt.figure(figsize=((25, 10)))\nplt.title(\"Hirarchical Clustering Denrogram\")\nplt.xlabel(\"index\")\nplt.ylabel(\"Ward Distance\")\ndendrogram(distance,\n          leaf_rotation=90.,\n          leaf_font_size=9.,)\nplt.axhline(25, c= 'k')\nplt.show()","573d925d":"plt.title(\"Hierarchical Clustering denrogram (Truncated)\")\nplt.xlabel('Index')\nplt.ylabel(\"Ward Distance\")\ndendrogram(distance, truncate_mode='lastp',\n          p = 6, leaf_rotation=0., leaf_font_size=12., show_contracted=True);\nplt.show()","1a1f75e8":"from scipy.cluster.hierarchy import fcluster","77e7e376":"max_d = 25\nclusters = fcluster(distance, max_d, criterion = 'distance')","f67e830d":"clusters","79097451":"plt.figure(figsize=(10, 8))\nplt.scatter(X[:, 0], X[:, 1], c = clusters, cmap= 'prism')\nplt.tight_layout()\nplt.show()","eb81cdcd":"from sklearn.cluster import KMeans","0a851645":"Kmeans = KMeans(n_clusters=9)\nKmeans.fit(X)","915b08c8":"y_kmeans = Kmeans.predict(X)","d7677e4a":"plt.figure(figsize=(10, 8))\nplt.scatter(X[:, 0], X[:, 1], c= y_kmeans, s = 10, cmap = 'inferno')\ncenters = Kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c = 'green', s = 500, alpha = 0.7)\nplt.tight_layout()\nplt.show()","887c32b4":"from mlxtend.plotting import plot_decision_regions","b9311769":"plt.figure(figsize = (10, 8))\nplot_decision_regions(X, y, clf = Kmeans)\nplt.tight_layout()\nplt.show()","508de772":"Kmeans.inertia_","b1b64f46":"sse_ = []\nfor k in range(1, 8):\n    Kmeans = KMeans(n_clusters = k).fit(X)\n    sse_.append([k, Kmeans.inertia_])","11a33670":"plt.figure(figsize = (10, 8))\nplt.plot(pd.DataFrame(sse_)[0], pd.DataFrame(sse_)[1]);\nplt.tight_layout()\nplt.show()","3550e0e0":"from sklearn.cluster import MeanShift, estimate_bandwidth","c767722c":"from itertools import cycle","497d3473":"badwidth_x = estimate_bandwidth(X, quantile=0.1, n_samples=len(X))\nmeanshift_model = MeanShift(bandwidth=badwidth_x, bin_seeding=True)\nmeanshift_model.fit(X)","1f2fb4e2":"cluster_centers = meanshift_model.cluster_centers_\nprint(cluster_centers)","163f8920":"labels = meanshift_model.labels_\nnum_clusters = len(np.unique(labels))\nprint(num_clusters)","b1e11e3f":"plt.figure(figsize = (10, 8))\nmarkers = '*vosx'\nfor i, marker in zip(range(num_clusters), markers):\n    plt.scatter(X[labels == i, 0], X[labels == i, 1], marker = marker, color = 'red')\n    cluster_center  = cluster_centers[i]\n    plt.plot(cluster_center[0], cluster_center[1], marker = 'o', \n            markerfacecolor = 'blue', markeredgecolor = 'blue', markersize = 15)\nplt.title('Cluster')\nplt.tight_layout()\nplt.show()","8417264d":"### Loading some example data","143d2538":"### Initializing Classifiers","a3673eff":"### Retrieve The Clusters","eaa92860":"## Logistic regression\nIn statistics, the logistic model is used to model the probability of a certain class or event existing such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. \n![image.png](attachment:image.png)","ea454303":"### What is Dendrogram?\nA dendrogram is a diagram representing a tree. This diagrammatic representation is frequently used in different contexts: in hierarchical clustering, it illustrates the arrangement of the clusters produced by the corresponding analyses.\n\n![image.png](attachment:image.png)\n\nThe key to interpreting a dendrogram is to focus on the height at which any two objects are joined together. In the example above, we can see that E and F are most similar, as the height of the link that joins them together is the smallest. The next two most similar objects are A and B.\n\n### Dendrogram cut-offs\n\nThe common practice to flatten dendrograms in k clusters is to cut them off at constant height k\u22121. Yet it leads to poorer clusters than efficiently pruning the tree.","6ff8e602":"### Hierarchical Clustering","c776b0e2":"### EnsembleVoteClassifier\nThe EnsembleVoteClassifier is a meta-classifier for combining similar or conceptually different machine learning classifiers for classification via majority or plurality voting. ... The EnsembleVoteClassifier implements \"hard\" and \"soft\" voting.\n![image.png](attachment:image.png)","1b0ee30f":"### Flat Clustering\nFlat clustering creates a flat set of clusters without any explicit structure that would relate clusters to each other. Hierarchical clustering creates a hierarchy of clusters. Flat clustering is efficient and conceptually simple, but it has a number of drawbacks.","11db30d2":"## Dendrogram","94f08c1d":"### Truncating Dendogram","c722d73a":"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.\n\n### K means clustering uses\n\nThe K-means clustering algorithm is used to find groups which have not been explicitly labeled in the data. This can be used to confirm business assumptions about what types of groups exist or to identify unknown groups in complex data sets.","f23dbae6":"## K-Means Clustering Advantages and Disadvantages\n\n### K-Means Advantages :\n\n1. If variables are huge, then  K-Means most of the times computationally faster than hierarchical clustering, if we keep k smalls.\n2. K-Means produce tighter clusters than hierarchical clustering, especially if the clusters are globular. \n\n### K-Means Disadvantages :\n\n1. Difficult to predict K-Value.\n2. With global cluster, it didn't work well.\n3. Different initial partitions can result in different final clusters.\n4. It does not work well with clusters (in the original data) of Different size and Different density","52130a36":"### K-Means clustering is an unsupervised learning algorithm. There is no labeled data for this clustering, unlike in supervised learning.","8138138d":"### gridspec\ngridspec is a module which specifies the location of the subplot in the figure. GridSpec. specifies the geometry of the grid that a subplot will be placed. The number of rows and number of columns of the grid need to be set. Optionally, the subplot layout parameters (e.g., left, right, etc.)","fffd9ce2":"### Mlxtend \nMLxtend is a library that implements a variety of core algorithms and utilities for machine\nlearning and data mining. The primary goal of MLxtend is to make commonly used tools\naccessible to researchers in academia and data scientists in industries focussing on userfriendly and intuitive APIs and compatibility to existing machine learning libraries, such\nas scikit-learn, when appropriate. While MLxtend implements a large variety of functions,\nhighlights include sequential feature selection algorithms (Pudil, Novovi\u010dov\u00e1, and Kittler\n1994), implementations of stacked generalization (Wolpert 1992) for classification and regression, and algorithms for frequent pattern mining (Agrawal and Ramakrishnan 1994).\nThe sequential feature selection algorithms cover forward, backward, forward floating, and\nbackward floating selection and leverage scikit-learn\u2019s cross-validation API (Pedregosa et\nal. 2011) to ensure satisfactory generalization performance upon constructing and selecting feature subsets. Besides, visualization functions are provided that allow users to inspect the estimated predictive performance, including performance intervals, for different\nfeature subsets. The ensemble methods in MLxtend cover majority voting, stacking, and\nstacked generalization, all of which are compatible with scikit-learn estimators and other\nlibraries as XGBoost (Chen and Guestrin 2016). In addition to feature selection, classification, and regression algorithms, MLxtend implements model evaluation techniques\nfor comparing the performance of two different models via McNemar\u2019s test and multiple\nmodels via Cochran\u2019s Q test. An implementation of the 5x2 cross-validated paired t-test\n(Dietterich 1998) allows users to compare the performance of machine learning algorithms\nto each other. Furthermore, different flavors of the Bootstrap method (Efron and Tibshirani 1994), such as the .632 Bootstrap method (Efron 1983) are implemented to compute\nconfidence intervals of performance estimates. All in all, MLxtend provides a large variety of different utilities that build upon and extend the capabilities of Python\u2019s scientific\ncomputing stack. Source :[https:\/\/www.theoj.org\/joss-papers\/joss.00638\/10.21105.joss.00638.pdf]","d0518383":"### Plotting Decision Regions","fd133810":"### SVC\nThe objective of a Linear SVC (Support Vector Classifier) is to fit to the data you provide, returning a \"best fit\" hyperplane that divides, or categorizes, your data. From there, after getting the hyperplane, you can then feed some features to your classifier to see what the \"predicted\" class is.\n![image.png](attachment:image.png)","7e13b691":"## Random forest\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean\/average prediction of the individual trees. \n![image.png](attachment:image.png)","8c9ca640":"## K-Means Clustering","7ac2730f":"### Working Procedure of K-means Clustering\n\n1. Divides a set of samples into disjoint clusters\n2. Each described by the mean of the samples in the cluster.\n3. The means are commonly called the cluster \u201ccentroids\u201d\n4. Note that the centroids are not, in general, points from, although they live in the same space.\n5. The K-means algorithm aims to choose centroids that minimise the inertia, or within-cluster sum of squared criterion"}}