{"cell_type":{"fc127268":"code","d18bcd9b":"code","e398f58a":"code","aaaba0f3":"code","4bd24b3f":"code","ae23f70f":"code","6bc40ce6":"code","d56143ab":"code","11df23ff":"code","ede5c0e5":"code","33fc0171":"code","79f12698":"code","a0119c8a":"code","6b019dd2":"code","e6025af5":"code","663028bb":"code","cddad3a7":"code","0aa430a5":"code","1d0c47db":"code","6f49ec6a":"code","f5cb56d0":"code","8990983b":"code","602a560e":"code","f6acbb21":"code","f750854f":"code","23b28f6c":"code","561b8cb6":"code","42a5ec6a":"code","77654c2d":"code","051a8ccc":"code","b8835f44":"code","833b1034":"code","9c87d6f6":"code","64f064eb":"code","0f15f01a":"code","05e437a1":"code","2ef7b463":"code","e9909abc":"code","1bc44969":"code","e2a52c19":"code","a20bccbe":"code","d0077c6b":"code","4181cfb2":"markdown","ad1bdefe":"markdown","2b553919":"markdown","956afd53":"markdown","1557773c":"markdown","5bddb56c":"markdown","cc5d432a":"markdown","704a7380":"markdown","dca578b8":"markdown","c4694bbf":"markdown","61538fa8":"markdown","242d4f15":"markdown","b72c4b9f":"markdown","8ef656c6":"markdown","7c05d162":"markdown"},"source":{"fc127268":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom sklearn.preprocessing import normalize\nfrom sklearn.pipeline import Pipeline\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","d18bcd9b":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv('..\/input\/test.csv')","e398f58a":"train.head(6)","aaaba0f3":"train.info()","4bd24b3f":"for n in train.columns:\n    print('The number of different values in ', n, 'are:', len(train[n].unique()))\nprint('-------------------------------------')\n#I will search numbers of Nan values\nfor n in train.columns:\n    if train[n].isnull().values.any() == True:\n        print('There is' , train[n].isna().sum(), 'null values in', n, 'column')","ae23f70f":"for n in test.columns:\n    print('The number of different values in ', n, 'are:', len(test[n].unique()))\nprint('-------------------------------------')\n#I will search numbers of Nan values\nfor n in test.columns:\n    if test[n].isnull().values.any() == True:\n        print('There is' , test[n].isna().sum(), 'null values in', n, 'column')","6bc40ce6":"train = train.drop(['Name','Ticket','PassengerId'], axis = 1)\ntest = test.drop(['Name','Ticket','PassengerId'], axis = 1)","d56143ab":"#First, we will create a copy:\ntrain_bf = train.copy()\ntest_bf = test.copy()","11df23ff":"print(\"The mean Age value for the train DataFrame is:\",train['Age'].mean(),\"\\nThe Standard Deviation for the Age is:\",train['Age'].std())\nprint(\"The mean Age value for the test DataFrame is:\",test['Age'].mean(),\"\\nThe Standard Deviation for the Age is:\",test['Age'].std())","ede5c0e5":"#Creating a function that fill NaN values of train and test\ndef fill_nan_w_mean_std(df,col='Age'):\n    nan = df[df[col].isna()]\n    min = df[col].mean() - df[col].std()\n    max = df[col].mean() + df[col].std()\n    for i in nan.index:\n        random_num = random.uniform(min,max)\n        df[col].loc[i] = random_num","33fc0171":"fill_nan_w_mean_std(train)\nfill_nan_w_mean_std(test)","79f12698":"#We will transform the values to integers\ntrain['Age'] = train['Age'].astype(int)\ntest['Age'] = test['Age'].astype(int)","a0119c8a":"plt.style.use('ggplot')\nfig = plt.figure(figsize = (25,10))\nfig.subplots_adjust(hspace=0.6, wspace=0.15)\nax = fig.add_subplot(1,2,1)\nax.set_ylim([0,0.04])\nax.set_title('Before filling NaN values')\nsns.distplot(train_bf['Age'].dropna(), bins=20)\nax_2 = fig.add_subplot(1,2,2)\nax_2.set_ylim([0,0.04])\nax_2.set_title('Data with filled values')\nsns.distplot(train['Age'],bins=20)","6b019dd2":"#First, we have to visualize the Embarked column in train data: \nsns.countplot(x='Embarked',data=train)","e6025af5":"#Now we see that most of the data embarked from S port we can fill with this class\nnan_emb = train[train['Embarked'].isna()]\nfor i in nan_emb.index:\n    train['Embarked'].loc[i] = 'S'","663028bb":"#We deal with that one nan value in fare column of test data\nnan_fare = test[test['Fare'].isna()]\nfor i in nan_fare.index:\n    test['Fare'].loc[i] = test['Fare'].mean()","cddad3a7":"train['Family_members'] = train['SibSp'] + train['Parch'] + 1 #him\/her\ntest['Family_members'] = test['SibSp'] + test['Parch'] + 1\n\n#Let's create the Alone feature\ntrain['not_Alone'] = 0 #0 if he\/she is alone\ntrain['not_Alone'].loc[train['Family_members'] > 1] = 1\n\ntest['not_Alone'] = 0 #1 if he\/she is alone\ntest['not_Alone'].loc[train['Family_members'] > 1] = 1","0aa430a5":"train.head()","1d0c47db":"#We will create a third string in gender: child. Because in a catastrophe like this children are first.\ndef agg_child(df,col='Age'):\n    mask = (df[col] <= 15)\n    df.loc[mask,'Sex'] = 'child'\n\n#We apply the function\nagg_child(train)\nagg_child(test)","6f49ec6a":"def agg_cabin_bin(df,col='Cabin'):\n    df['with_Cabin'] = 0\n    mask = (df[col].isna() == False)\n    df.loc[mask,'with_Cabin'] = 1\n#0 if the passenger had not a Cabin, 1 if he has a cabin","f5cb56d0":"agg_cabin_bin(train)\nagg_cabin_bin(test)\n\ndel train['Cabin']\ndel test['Cabin']","8990983b":"train.head()","602a560e":"sns.barplot(x='Sex',y='Survived',data=train, order=['female','child','male'])\nplt.title('Sex vs Survived')","f6acbb21":"gs = plt.GridSpec(2,3,wspace=0.45, hspace=0.8)\nplt.figure(figsize=(12,10))\nax1 = plt.subplot2grid((3,3),(0,0),rowspan=2,colspan=2)\nplt.title('Age vs Survived vs Sex')\nsns.swarmplot(x = 'Survived',y='Age', \n              data=train, linewidth=1,hue='Sex', palette = 'muted')\nax2 = plt.subplot2grid((3,3),(0,2))\nplt.title('Embarked vs Survived')\nsns.barplot(x='Embarked',y='Survived',\n            data=train,order=['C','Q','S'])\nax3 = plt.subplot2grid((3,3),(1,2))\nplt.title('Pclass vs Survived')\nsns.barplot(x='Pclass',y='Survived',\n            data=train, palette = 'muted')","f750854f":"#Now we can say that Embarked class\/Pclass\/sex have priorities for survive. So we can transform the categorical data to numbers\ncat_to_nums = {\"Embarked\":  {\"S\": 0, \"Q\": 1, \"C\":2},\n               \"Sex\": {\"male\":0,\"child\":1,\"female\":2}}\n#We will use replace to convert the values\ntrain.replace(cat_to_nums, inplace = True)","23b28f6c":"test.replace(cat_to_nums, inplace = True)","561b8cb6":"train.head()","42a5ec6a":"train.corr()","77654c2d":"def correlation_heatmap(df): #from \"A Data Science Framework: To Achieve 99% Accuracy\" kernel by LD Freedman\n    s , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    g = sns.heatmap(df.corr(), cmap = colormap,square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax, annot=True,linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 })\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)","051a8ccc":"correlation_heatmap(train)","b8835f44":"# Normalize 'Fare' & 'Age' values for test and train dataset\nX = [train['Fare'],\n     train['Age']]\nX_normalize = normalize(X)\n\nX_2 = [test['Fare'],\n      test['Age']]\nX_2_normalize = normalize(X_2)\n\ntrain = train.assign(Fare = X_normalize[0])\n\ntrain = train.assign(Age = X_normalize[1])\n\ntest = test.assign(Fare = X_2_normalize[0])\n\ntest = test.assign(Age = X_2_normalize[1])","833b1034":"#Now we have all numerical values!\ntrain.head()","9c87d6f6":"X_train = train.loc[:,'Pclass':]\ny_train = train.loc[:,'Survived']\nX_test_final = test","64f064eb":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV #We will use gridsearchCV\n\n#Dividing the data before tuning the model\nX_train, X_test, y_train, y_test = train_test_split(X_train,y_train,test_size = 0.3, \n                                                    random_state=21)","0f15f01a":"#Logistic Regression:\n\nlogreg = LogisticRegression()\n\nparam_grid = {'C' : [x for x in range(1,5000,5)]  }\n#finding the best parameter:\nsearcher = GridSearchCV(logreg, param_grid)\n\nsearcher.fit(X_train,y_train)","05e437a1":"# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\n\n# Report the test accuracy using these best parameters\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))","2ef7b463":"pred = searcher.predict(X_test_final)\npred","e9909abc":"from sklearn.svm import SVC\nsvc = SVC()\nparameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1,1,10,100],\n              'C':[x for x in np.linspace(0.1,10,100)]}\nssp = GridSearchCV(svc,parameters)\n\nssp.fit(X_train,y_train)\n","1bc44969":"# Report the best parameters and the corresponding score\nprint(\"Best CV params\", ssp.best_params_)\nprint(\"Best CV accuracy\", ssp.best_score_)\n\n# Report the test accuracy using these best parameters\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))","e2a52c19":"pred = ssp.predict(X_test_final)","a20bccbe":"l = [i for i in range(892,1310)]\ntype(l)","d0077c6b":"results = pd.DataFrame(pred)\nresults.index = l\nresults['PassengerId'] = l\nresults.columns = ['Survived','PassengerId']\n\ncols = results.columns.tolist()\ncols = cols[-1:] + cols[:-1]\nresults = results.ix[:, cols]\n\nresults.to_csv('results_titanic_3.csv',sep=',', encoding='utf-8',index=False)","4181cfb2":"We can say that there is no significative difference after we fill the nan values. The distribution is almost the same. But have two peaks. Later we will deal with that.\n\nNow we have to deal with nan values in embarked column on train dataframe and with nan ones in Fare column on test dataframe. ","ad1bdefe":"## Find Correlations on data:","2b553919":"## Support Vector Machines: \n#### Support Vector Classifier","956afd53":"Now lets deal with cabin values. Since there are too many NaN values we can binarize if the passengers have Cabin or not.","1557773c":"### Logistic Regression","5bddb56c":"## Normalizing data\nhttps:\/\/medium.com\/@rrfd\/standardize-or-normalize-examples-in-python-e3f174b65dfc\n","cc5d432a":"We have seen there are two variables with family relations (SibSp & Parch) so why we can not create a variable that resumen the family member for one person? And also count the people who travel alone in the ship. ","704a7380":"## We can create other features from the data","dca578b8":"# Machine Learning\n\n-Logistic regression vs SVM","c4694bbf":"Pclass: socio-economic status 1st = Upper 2nd=Middle 3rd=Lower \n\nSibSp: The dataset defines family relations # of siblings \/ spouses aboard the Titanic\n\nParch: # of parents \/ children aboard the Titanic\n\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nSome children travelled only with a nanny, therefore parch=0 for them.\n\n\nsurvival\tSurvival\t0 = No, 1 = Yes\npclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n\nCabin: cabin number","61538fa8":"Now we can visualize if there is a difference on the fill values:","242d4f15":"## Data Visualization","b72c4b9f":"## Submiting Predictions","8ef656c6":"We will eliminate the irrelevant features and the one that contains too many nan values","7c05d162":"Now we fill NaN values of the columns Age and Embarked. In the 'Age' column we will fill with random numbers beetwen (mean +- standard deviation) values. "}}