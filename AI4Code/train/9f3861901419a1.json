{"cell_type":{"6dc44c5a":"code","c37c8775":"code","23414e42":"code","ac473bfc":"code","b31c3387":"code","d47d58f1":"code","9ec1a99e":"code","733f8470":"code","e8430a34":"code","2c1919a2":"code","16455aa5":"code","320eb68c":"code","49608acd":"code","dd4d9efe":"code","282184e1":"code","92e793d1":"code","ec012c62":"code","ee0f0f65":"code","6967c3fd":"code","68c8cd6a":"code","557a97a7":"code","42d4fd3a":"code","0317e9cf":"code","30dd5146":"code","65e6da86":"markdown","ce1a5084":"markdown","a14ea7e3":"markdown","49a60403":"markdown","3c9645f8":"markdown","0889e669":"markdown","41a54ea1":"markdown","41c2a2f6":"markdown","2781ccba":"markdown"},"source":{"6dc44c5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport librosa as lb\nimport librosa.display as lbd\nimport os\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c37c8775":"train=pd.read_csv('..\/input\/part-2-handel-imbalance-creating-spectrogram\/train.csv')\nval=pd.read_csv('..\/input\/part-2-handel-imbalance-creating-spectrogram\/val.csv')\ntrain.head()","23414e42":"ytrain=train.disease\nyval=val.disease\nyval","ac473bfc":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nytrain=le.fit_transform(ytrain)\nyval=le.transform(yval)","b31c3387":"def getFeatures(path):\n    soundArr,sample_rate=lb.load(path)\n    mfcc=lb.feature.mfcc(y=soundArr,sr=sample_rate)\n    cstft=lb.feature.chroma_stft(y=soundArr,sr=sample_rate)\n    mSpec=lb.feature.melspectrogram(y=soundArr,sr=sample_rate)\n\n    return mfcc,cstft,mSpec","d47d58f1":"root='..\/input\/part-1-preprocessing\/processed_audio_files\/'\nmfcc,cstft,mSpec=[],[],[]\ni=0\nfor idx,row in val.iterrows():\n    path=root + row['filename']\n    a,b,c=getFeatures(path)\n    mfcc.append(a)\n    cstft.append(b)\n    mSpec.append(c)\n    \nmfcc_val=np.array(mfcc)\ncstft_val=np.array(cstft)\nmSpec_val=np.array(mSpec)","9ec1a99e":"root='..\/input\/part-1-preprocessing\/processed_audio_files\/'\nmfcc,cstft,mSpec=[],[],[]\ni=0\nfor idx,row in train.iterrows():\n    path=root + row['filename']\n    a,b,c=getFeatures(path)\n    mfcc.append(a)\n    cstft.append(b)\n    mSpec.append(c)\n    \nmfcc_train=np.array(mfcc)\ncstft_train=np.array(cstft)\nmSpec_train=np.array(mSpec)","733f8470":"my_callbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=5),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=3, min_lr=0.00001,mode='min'),\n    tf.keras.callbacks.ModelCheckpoint('hbNet', monitor='val_loss', verbose=0, save_best_only=True,)\n]","e8430a34":"mfcc_input=keras.layers.Input(shape=(20,259,1),name=\"mfccInput\")\nx=keras.layers.Conv2D(32,5,strides=(1,3),padding='same')(mfcc_input)\nx=keras.layers.BatchNormalization()(x)\nx=keras.layers.Activation(keras.activations.relu)(x)\nx=keras.layers.MaxPooling2D(pool_size=2,padding='valid')(x)\n\nx=keras.layers.Conv2D(64,3,strides=(1,2),padding='same')(x)\nx=keras.layers.BatchNormalization()(x)\nx=keras.layers.Activation(keras.activations.relu)(x)\nx=keras.layers.MaxPooling2D(pool_size=2,padding='valid')(x)\n\nx=keras.layers.Conv2D(96,2,padding='same')(x)\nx=keras.layers.BatchNormalization()(x)\nx=keras.layers.Activation(keras.activations.relu)(x)\nx=keras.layers.MaxPooling2D(pool_size=2,padding='valid')(x)\n\nx=keras.layers.Conv2D(128,2,padding='same')(x)\nx=keras.layers.BatchNormalization()(x)\nx=keras.layers.Activation(keras.activations.relu)(x)\nmfcc_output=keras.layers.GlobalMaxPooling2D()(x)\n\nmfcc_model=keras.Model(mfcc_input, mfcc_output, name=\"mfccModel\")","2c1919a2":"mfcc_model.summary()","16455aa5":"croma_input=keras.layers.Input(shape=(12,259,1),name=\"cromaInput\")\nx=keras.layers.Conv2D(32,5,strides=(1,3),padding='same')(croma_input)\nx=keras.layers.BatchNormalization()(x)\nx=keras.layers.Activation(keras.activations.relu)(x)\nx=keras.layers.MaxPooling2D(pool_size=2,padding='valid')(x)\n\nx=keras.layers.Conv2D(64,3,strides=(1,2),padding='same')(x)\nx=keras.layers.BatchNormalization()(x)\nx=keras.layers.Activation(keras.activations.relu)(x)\nx=keras.layers.MaxPooling2D(pool_size=2,padding='valid')(x)\n\nx=keras.layers.Conv2D(128,2,padding='same')(x)\nx=keras.layers.BatchNormalization()(x)\nx=keras.layers.Activation(keras.activations.relu)(x)\ncroma_output=keras.layers.GlobalMaxPooling2D()(x)\n\ncroma_model=keras.Model(croma_input, croma_output, name=\"cromaModel\")","320eb68c":"croma_model.summary()","49608acd":"mSpec_input=keras.layers.Input(shape=(128,259,1),name=\"mSpecInput\")\nx=keras.layers.Conv2D(32,5,strides=(2,3),padding='same')(mSpec_input)\nx=keras.layers.BatchNormalization()(x)\nx=keras.layers.Activation(keras.activations.relu)(x)\nx=keras.layers.MaxPooling2D(pool_size=2,padding='valid')(x)\n\nx=keras.layers.Conv2D(64,3,strides=(2,2),padding='same')(x)\nx=keras.layers.BatchNormalization()(x)\nx=keras.layers.Activation(keras.activations.relu)(x)\nx=keras.layers.MaxPooling2D(pool_size=2,padding='valid')(x)\n\nx=keras.layers.Conv2D(96,2,padding='same')(x)\nx=keras.layers.BatchNormalization()(x)\nx=keras.layers.Activation(keras.activations.relu)(x)\nx=keras.layers.MaxPooling2D(pool_size=2,padding='valid')(x)\n\nx=keras.layers.Conv2D(128,2,padding='same')(x)\nx=keras.layers.BatchNormalization()(x)\nx=keras.layers.Activation(keras.activations.relu)(x)\nmSpec_output=keras.layers.GlobalMaxPooling2D()(x)\n\nmSpec_model=keras.Model(mSpec_input, mSpec_output, name=\"mSpecModel\")","dd4d9efe":"mSpec_model.summary()","282184e1":"input_mfcc=keras.layers.Input(shape=(20,259,1),name=\"mfcc\")\nmfcc=mfcc_model(input_mfcc)\n\ninput_croma=keras.layers.Input(shape=(12,259,1),name=\"croma\")\ncroma=croma_model(input_croma)\n\ninput_mSpec=keras.layers.Input(shape=(128,259,1),name=\"mspec\")\nmSpec=mSpec_model(input_mSpec)\n\n\nconcat=keras.layers.concatenate([mfcc,croma,mSpec])\nhidden=keras.layers.Dropout(0.2)(concat)\nhidden=keras.layers.Dense(50,activation='relu')(concat)\nhidden=keras.layers.Dropout(0.3)(hidden)\nhidden=keras.layers.Dense(25,activation='relu')(hidden)\nhidden=keras.layers.Dropout(0.3)(hidden)\noutput=keras.layers.Dense(8,activation='softmax')(hidden)\n\nnet=keras.Model([input_mfcc,input_croma,input_mSpec], output, name=\"Net\")","92e793d1":"net.summary()","ec012c62":"keras.utils.plot_model(net, \"net.png\", show_shapes=True)","ee0f0f65":"accuracy='sparse_categorical_accuracy'\nsparseLoss=keras.losses.SparseCategoricalCrossentropy()\n\nfrom keras import backend as K\nK.clear_session()\nnet.compile(optimizer='nadam', loss=sparseLoss,metrics=[accuracy])\nK.set_value(net.optimizer.learning_rate, 0.001)","6967c3fd":"history=net.fit(\n    {\"mfcc\":mfcc_train,\"croma\":cstft_train,\"mspec\":mSpec_train},\n    ytrain,\n    validation_data=({\"mfcc\":mfcc_val,\"croma\":cstft_val,\"mspec\":mSpec_val},yval),\n    epochs=100,verbose=0,\n    callbacks=my_callbacks\n)","68c8cd6a":"pd.DataFrame(history.history).plot()\nplt.grid(True)\nplt.gca().set_ylim(-0.1,1.1)\nplt.show()","557a97a7":"net.evaluate({\"mfcc\":mfcc_val,\"croma\":cstft_val,\"mspec\":mSpec_val},yval)","42d4fd3a":"model=keras.models.load_model('.\/hbNet')","0317e9cf":"model.compile(optimizer='nadam', loss=sparseLoss,metrics=[accuracy])","30dd5146":"loss, acc = model.evaluate({\"mfcc\":mfcc_val,\"croma\":cstft_val,\"mspec\":mSpec_val},yval, verbose=2)\nprint('Restored model, accuracy: {:5.2f}%'.format(100*acc))","65e6da86":"* The model is a little **less accurate** but this must account to the use of **classes** that have very **less samples**, you can crop these out and i am sure this will improve accuracy to **98% accuracy**.","ce1a5084":"> Discarded few features beacuse they were not important to performance","a14ea7e3":"The shape of features returned by the above function **(20, 259) (12, 259) (128, 259)** we need this for defining **input shape** of our model.","49a60403":"* To find out i got to this point, do check out other **Versions and Parts of this Series**.\n","3c9645f8":"> In These functions i m iterating over **train and val dataset**, and using **filenames** to extract features and then converting them to **numpy arrays**\n","0889e669":"# Testing Features\n\n> Lets create model using Keras **Functional API** to check how the features perform.\n\n","41a54ea1":"* As our class labels are of **dtype: object** we must first convert them to **encoded values** or intergers","41c2a2f6":"# Feature Extraction\n\n> I am going to use Librosa's feature extraction methods here\n\n> Note:- i first tried to extract features **1-by-1** and then test them, to see how they perform but most of then were giving **good accuracy** so i finally decided to use all of them\n\n","2781ccba":"* Here i m creating **three** different **CNN** and then Combining them to a **Dense Network**."}}