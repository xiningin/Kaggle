{"cell_type":{"d31022ae":"code","b6225f12":"code","d814d053":"code","f956d2a4":"code","b013c6cb":"code","c2ab5ceb":"code","beeaadb5":"code","09c42efe":"code","004d07cb":"code","36a13115":"code","d8c51a51":"code","698eccab":"code","f8dd2071":"code","a9cf1d3d":"code","49251582":"code","1ce6c496":"code","a5fdcca8":"code","0e2d53cf":"code","2806acff":"code","2166bd85":"code","58899e50":"code","87f9446e":"code","5e94dbea":"code","92852cd9":"code","d2b39734":"code","d12268f5":"code","b54503e4":"code","6812d51a":"code","4a4eddcc":"code","18fdcfc5":"code","b6c9768f":"code","efa058ea":"code","815ab49f":"code","7daf480b":"code","2f89b605":"code","a177e4bf":"code","655432a1":"code","10500d05":"code","7d4f0be6":"code","d81659b5":"code","bc71e86c":"code","ad634ee4":"code","d65c1e9f":"code","cdb1a1ea":"code","77f71122":"code","94b57862":"code","713a260d":"markdown","3d244ad1":"markdown","cf062057":"markdown","58dbcdb3":"markdown","febf5c98":"markdown","0220ece2":"markdown","e53c6877":"markdown","ec3e6643":"markdown","8ec8b847":"markdown","0594a61f":"markdown","17a9fef7":"markdown","9daf74a7":"markdown","30524a73":"markdown","6c1568ec":"markdown","f8b1b271":"markdown","7bc8e69d":"markdown","bd515c88":"markdown","781ab8df":"markdown","6f5aee87":"markdown","92742a79":"markdown","abb3b217":"markdown","abb9a278":"markdown"},"source":{"d31022ae":"import zipfile\nimport numpy as np\nimport pandas as pd\nimport sklearn as sk\nimport seaborn as sns\nimport glob as glob\nimport geopandas as gpd\nfrom geopy.geocoders import Nominatim\nimport pycountry\nimport os\nimport matplotlib.pyplot as plt\nfrom scipy.cluster import hierarchy\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.stats import diagnostic as diag\nfrom statsmodels.stats.stattools import durbin_watson\n\n","b6225f12":"#On Kaggle Notebook\nall_files=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        all_files.append(os.path.join(dirname, filename))\n\n    \n#Reorder\nmyorder = [2, 3, 0, 4, 1]\nall_files = [all_files[i] for i in myorder]\n","d814d053":"# #Downloading files  from Kaggle and unziping in folder \"Dados\"\n# !kaggle datasets download -d unsdsn\/world-happiness\n\n# with zipfile.ZipFile(os.getcwd()+'\\world-happiness.zip', 'r') as zip_ref:\n#     zip_ref.extractall(os.getcwd()+'\\Dados')\n# #Selectig  all .csv files    \n# all_files = glob.glob(os.getcwd()+'\\Dados' + \"\/*.csv\")","f956d2a4":"#Extracting file names\nyear=[os.path.basename(yr)[:4] for yr in all_files]\n\nli = []\ni=0\nfor filename in all_files:\n    \n    df_temp = pd.read_csv(filename, index_col=None, header=0)\n    df_temp['Year']=year[i]\n    li.append(df_temp)\n    i+=1\n    \n#Removing some columns so that all files have the same information.\n\ndf_2015=li[0].drop(['Standard Error','Dystopia Residual'], axis=1)\ndf_2016=li[1].drop(['Lower Confidence Interval','Upper Confidence Interval','Dystopia Residual'], axis=1)\ndf_2017=li[2].drop(['Whisker.high','Whisker.low','Dystopia.Residual'], axis=1)\ndf_2018=li[3]\ndf_2019=li[4]\n\n\n\n#Renaming columns\ndf_2015.rename(columns={'Economy (GDP per Capita)':'Economy', 'Health (Life Expectancy)':'Life Expectancy',\n        'Trust (Government Corruption)':'Government Trust','Family':'Social Support'},inplace=True)\n\ndf_2016.rename(columns={'Economy (GDP per Capita)':'Economy', 'Health (Life Expectancy)':'Life Expectancy',\n        'Trust (Government Corruption)':'Government Trust','Family':'Social Support'},inplace=True)\n\ndf_2017.rename(columns={'Happiness.Rank':'Happiness Rank', 'Happiness.Score':'Happiness Score',\n        'Economy..GDP.per.Capita.':'Economy', 'Health..Life.Expectancy.':'Life Expectancy',\n        'Trust..Government.Corruption.':'Government Trust','Family':'Social Support'},inplace=True)\n\ndf_2018.rename(columns={'Overall rank':'Happiness Rank', 'Country or region':'Country', 'Score':'Happiness Score', \n                        'GDP per capita':'Economy','Social support':'Social Support', \n                        'Healthy life expectancy':'Life Expectancy', 'Freedom to make life choices':'Freedom',                        \n                        'Perceptions of corruption':'Government Trust'},inplace=True)\n\ndf_2019.rename(columns={'Overall rank':'Happiness Rank', 'Country or region':'Country', 'Score':'Happiness Score', \n                        'GDP per capita':'Economy','Social support':'Social Support', \n                        'Healthy life expectancy':'Life Expectancy', 'Freedom to make life choices':'Freedom',                        \n                        'Perceptions of corruption':'Government Trust'},inplace=True)\n\n\n#Mapping regions not available in other years.\n\nCountry_Region=df_2015.set_index('Country')['Region']\n\ndf_2017['Region']=df_2017['Country'].map(Country_Region)\ndf_2018['Region']=df_2018['Country'].map(Country_Region)\ndf_2019['Region']=df_2019['Country'].map(Country_Region)\n\ndf_t=[df_2015,df_2016,df_2017,df_2018,df_2019]\n","b013c6cb":"# Checking for null values\n\n[print('Year',i+2015 , '\\n' ,df_t[i].isnull().any()) for i in range(len(list(df_t)))]","c2ab5ceb":"#Countries without Regions, countries that were not at df_2015\nprint('Countries without Region 2017:',df_2017[df_2017['Region'].isnull()]['Country'].index,\nlist(df_2017[df_2017['Region'].isnull()]['Country']))\nprint('Countries without Region 2018:',df_2018[df_2018['Region'].isnull()]['Country'].index,\nlist(df_2018[df_2018['Region'].isnull()]['Country']))\nprint('Countries without Region 2019:',df_2019[df_2019['Region'].isnull()]['Country'].index,\nlist(df_2019[df_2019['Region'].isnull()]['Country']))","beeaadb5":"#Adding Regions to these Countries\ndf_2017.loc[list(df_2017[df_2017['Region'].isnull()]['Country'].index),'Region']=['Eastern Asia','Latin America and Caribbean','Eastern Asia','Sub-Saharan Africa','Sub-Saharan Africa','Sub-Saharan Africa']\ndf_2018.loc[list(df_2018[df_2018['Region'].isnull()]['Country'].index),'Region']=['Latin America and Caribbean','Latin America and Caribbean','Middle East and Northern Africa','Sub-Saharan Africa','Sub-Saharan Africa','Sub-Saharan Africa']\ndf_2019.loc[list(df_2019[df_2019['Region'].isnull()]['Country'].index),'Region']=['Latin America and Caribbean','Middle East and Northern Africa','Central and Eastern Europe','Sub-Saharan Africa','Sub-Saharan Africa','Sub-Saharan Africa','Sub-Saharan Africa']\n","09c42efe":"#Verifying null value in 2018\n\ndf_2018[df_2018['Government Trust'].isnull()]","004d07cb":"#Government confidence values for other years\n\nfor j in range(len(df_t)):\n    print('Year:',j+2015,df_t[j][df_t[j]['Country']=='United Arab Emirates']['Government Trust'])","36a13115":"#Setting value in 2018 as the average between 2017 and 2019\ndf_2018.loc[list(df_2018[df_2018['Government Trust'].isnull()].index),'Government Trust']=list((df_2017[df_2017['Country']=='United Arab Emirates']['Government Trust']+df_2019[df_2019['Country']=='United Arab Emirates']['Government Trust'])\/2)","d8c51a51":"#Concateating Dataframes\ndf = pd.concat(df_t, axis=0, ignore_index=True)\n\n#Renaming some countries, necessary for future analysis in Geopandas and Pycountry\ndf.replace(['Congo (Brazzaville)', 'Congo (Kinshasa)','Swaziland','South Korea','Laos','Ivory Coast','Trinidad & Tobago','Hong Kong S.A.R., China','Taiwan Province of China','Niger'], \n           ['Congo, The Democratic Republic of the','Congo','Eswatini','Korea, Republic of','Lao',\"C\u00f4te d'Ivoire\",'Trinidad and Tobago','Hong Kong Special Administrative Region of China','Taiwan, Province of China','Republic of the Niger'],inplace=True)\n\n#Removing unrecognized countries\nremove = ['North Cyprus','Somaliland region','Swaziland','Palestinian Territories','Somaliland Region','Northern Cyprus']\nfor r in remove:\n    df.drop(list(df[df[\"Country\"]==r].index), inplace=True)   \n","698eccab":"#Mapping missing regions\nCountry_Region_C=df.drop_duplicates('Country',keep='first')[['Country','Region']].set_index('Country')\n","f8dd2071":"## Way to find Latitude and Longitude\n\n# geolocator = Nominatim(user_agent=\"My_app\")\n# country_location=[]\n# for country  in  df['Country'].unique():\n#     try:\n#         location = geolocator.geocode(country)\n#         a=[country,location.latitude, location.longitude]\n#         country_location.append(a)\n#     except:\n#         pass\n    ","a9cf1d3d":"#Finding Iso Alpha3 Code for countries, e.g. Brazil = BRA\n\ni=0\ncountry_location=[]\nfor c in df['Country'].unique():\n    try:\n        alph3=pycountry.countries.search_fuzzy(c)[0].alpha_3\n        a=[c,alph3]\n        \n        country_location.append(a)\n    except:\n        pass\n    i+=1\n    ","49251582":"#Checking if all countries have been found\n\nnf=[]\nfor c in df['Country'].unique():\n     if c in [cl[0] for cl  in country_location]:\n         pass\n     else:\n         print('Country not found:',c)\n         nf.append(c)","1ce6c496":"# Scaling data\nscaled_data=StandardScaler().fit_transform(df.loc[:,'Happiness Score':'Generosity'])\ndf_scale=pd.DataFrame(scaled_data,columns=df.loc[:,'Happiness Score':'Generosity'].columns)\ndf_scale['Year']=df['Year'].reset_index(drop=True)\ndf_scale['Region']=df['Region'].reset_index(drop=True)\ndf_scale['Country']=df['Country'].reset_index(drop=True)\n","a5fdcca8":"#Adding alph3 column to dataframe and grouping by country\ndf_cl=pd.DataFrame(country_location,columns=['Country','Alpha3'])\ndf_cl.set_index('Country',inplace=True)\ndf_c=df_scale.groupby(df['Country']).mean()\ndf_c['Alpha3']=df_c.index.map(df_cl['Alpha3'])\n\n","0e2d53cf":"#Transforming dataframe to geodataframe\n\ngdf = gpd.GeoDataFrame(df_c).reset_index()\ngdf['Region']=gdf['Country'].map(Country_Region_C['Region'])\n","2806acff":"#Accessing geo map geopandas database\nworld = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\nworld=world[(world['name']!=\"Antarctica\")]\n\n#Some alpha iso3 codes missing from basemap\nworld.loc[world['name'] == 'France', 'iso_a3'] = 'FRA'\nworld.loc[world['name'] == 'Norway', 'iso_a3'] = 'NOR'\nworld.loc[world['name'] == 'Somaliland', 'iso_a3'] = 'SOM'\nworld.loc[world['name'] == 'Kosovo', 'iso_a3'] = 'RKS'","2166bd85":"#Mapping geometric data in the Geodataframe\ngdf['geometry']=gdf['Alpha3'].map(world.drop_duplicates(['iso_a3']).set_index(['iso_a3'])['geometry'])","58899e50":"#Happines Score map\nfig, ax = plt.subplots(figsize=(25,7))\nbase = world.plot(ax=ax,color='grey', edgecolor='grey')\ngdf.plot(ax=base,column='Happiness Score',legend=True,cmap='jet_r',edgecolor='white',linewidth=0.5)\nax.set_title(\"Happiness Score\",fontsize=20)\nax.set_axis_off()","87f9446e":"#Economy map\nfig, ax = plt.subplots(figsize=(25,7))\nbase = world.plot(ax=ax,color='grey', edgecolor='white')\ngdf.plot(ax=base,column='Economy',legend=True,cmap='jet_r',edgecolor='white',linewidth=0.5 )\nax.set_title(\"Economy\",fontsize=20)\nax.set_axis_off()","5e94dbea":"#Life Expectancy map\nfig, ax = plt.subplots(figsize=(25,7))\nbase = world.plot(ax=ax,color='grey', edgecolor='white')\ngdf.plot(ax=base,column='Life Expectancy',legend=True,cmap='jet_r' ,edgecolor='white',linewidth=0.5)\nax.set_title(\"Life Expectancy\",fontsize=20)\nax.set_axis_off()","92852cd9":"#Freedom map\nfig, ax = plt.subplots(figsize=(25,7))\nbase = world.plot(ax=ax,color='grey', edgecolor='white')\ngdf.plot(ax=base,column='Freedom',legend=True,cmap='jet_r',edgecolor='white',linewidth=0.5 )\nax.set_title(\"Freedom\",fontsize=20)\nax.set_axis_off()","d2b39734":"#Generosity map\nfig, ax = plt.subplots(figsize=(25,7))\nbase = world.plot(ax=ax,color='grey', edgecolor='white')\ngdf.plot(ax=base,column='Generosity',legend=True,cmap='jet_r',edgecolor='white',linewidth=0.5 )\nax.set_title(\"Generosity\",fontsize=20)\nax.set_axis_off()","d12268f5":"#Social Support map\nfig, ax = plt.subplots(figsize=(25,7))\nbase = world.plot(ax=ax,color='grey', edgecolor='white')\ngdf.plot(ax=base,column='Social Support',legend=True,cmap='jet_r',edgecolor='white',linewidth=0.5 )\nax.set_title(\"Social Support\",fontsize=20)\nax.set_axis_off()","b54503e4":"#Government Trust map\nfig, ax = plt.subplots(figsize=(25,7))\nbase = world.plot(ax=ax,color='grey', edgecolor='white')\ngdf.plot(ax=base,column='Government Trust',legend=True,cmap='jet_r',edgecolor='white',linewidth=0.5 )\nax.set_title(\"Government Trust\",fontsize=20)\nax.set_axis_off()","6812d51a":"#Region Divisions\nfig, ax = plt.subplots(figsize=(25,7))\nbase = world.plot(ax=ax,color='grey', edgecolor='white')\ngdf.plot(ax=base,column='Region',cmap='jet_r',edgecolor='white',linewidth=0.5,legend=True, legend_kwds={'loc': 'lower left'} )\nax.set_title(\"Regions Division\",fontsize=20)\nax.set_axis_off()","4a4eddcc":"#Country % ranking for each feature and year\nfor yr in list(df_scale['Year'].unique()):\n    for col in df_scale.columns[0:7]:\n        df_scale.loc[df_scale['Year'] == yr,str('Ranking '+col)]=  round(df_scale.loc[df_scale['Year'] == yr, col].rank(method='dense',ascending=False)\/df_scale.loc[df_scale['Year'] == yr, col].count()*100,2)","18fdcfc5":"#DataFrame description\ndisplay(df_scale.describe())\n#Boxplot\nfig, ax = plt.subplots(figsize=(5,5))\nsns.boxplot(data=df_scale.iloc[:,0:7])\nplt.xticks(rotation=25,ha='right');\nax.set_title(\"Distribui\u00e7\u00e3o das vari\u00e1veis\",fontsize=20)","b6c9768f":"# Visualization of the level of Happiness by year and by region\n\nfig, ax = plt.subplots(figsize=(25,7))\nsns.barplot(x=df_scale['Region'],y=df_scale['Happiness Score'],hue=df_scale['Year'])\nax.set_title(\"Happiness Score by Region and Year\",fontsize=20)\nplt.xticks(rotation=25,ha='right',fontsize=15);","efa058ea":"# Brazil\n\nbrazil=df_scale.loc[df_scale['Country']=='Brazil','Year':'Ranking Generosity']\nfig, ax = plt.subplots(figsize=(10,8))\nbrdf = pd.melt(brazil.iloc[:,[0,3,4,5,6,7,8,9]],id_vars='Year')\nsns.barplot(data=brdf,x='variable',y='value',hue='Year')\nplt.ylabel('Brazil ranking %')\nplt.xlabel('Variables')\nax.set_title(\"Brazil's Ranking for each variable\",fontsize=20)\nplt.xticks(rotation=25,ha='right');\n\n","815ab49f":"#Verifying data distribution\nfig = plt.figure(figsize=(15,35))\ni=0\nfor reg in list(df['Region'].unique()):  \n    ax=plt.subplot(5,3,i+1)\n    \n    sns.boxplot(data=df_scale[df_scale['Region']==reg].iloc[:,0:7])\n    \n    ax = sns.stripplot(data=df_scale[df_scale['Region']==reg].iloc[:,0:7],\n                   color=\"black\", edgecolor=\"gray\")\n    \n    t=str(reg+' \/ n observa\u00e7\u00f5es= '+str(df_scale.groupby('Region').count().loc[reg][0]))\n    ax.set_title(t,fontsize=11)\n    plt.xticks(rotation=25,ha='right');\n    i+=1\nplt.subplots_adjust(hspace=0.3,wspace=0.5)","7daf480b":"#Pair plot\nplt.figure(figsize=(5,5));\nsns.pairplot(df_scale.iloc[:,0:7],diag_kind='kde',kind='reg',plot_kws={'line_kws':{'color':'red'}});","2f89b605":"#Correlation map\nfig= plt.figure(figsize=(10,10))\n\nax1=plt.subplot(2,1,1)\n\nsns.heatmap(df_scale.iloc[:,0:7].corr(), annot=True, fmt=\".2f\", linewidths=.5,vmin=-1, vmax=1, center= 0,ax=ax1)  ","a177e4bf":"#Scaled data X y split\nSEED=np.random.seed(401)\nXs=df_scale.iloc[:,1:7]\n\nys=df_scale.iloc[:,0]\n\n##Para estrafificar dados  continuos\n# #Regra de Sturger qquantidade de bins\n# nbins=np.around(1+10\/3*np.log10(ys_pre.count())).astype(int)\n# #divisao dos bins\n# bins=pd.cut(ys_pre,nbins,labels=np.linspace(1,nbins,nbins))\n# bins.name='bins'\n# ys=pd.concat([ys_pre,bins],axis=1)\n\n#Train Test Split\nXs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, ys,test_size=0.30,random_state=SEED)\n#Adding constant for stats model package\nXsc_train = sm.add_constant(Xs_train)\nXsc_test = sm.add_constant(Xs_test)","655432a1":"#Data train test distribuition\nfig= plt.figure(figsize=(14,7))\nplt.subplots_adjust(hspace=0.5,wspace=0.5)    \n\nax1=plt.subplot(1,3,1)\nax1.set_title('Df dist tamanho:'+str(len(ys)))\nsns.distplot(ys,ax=ax1,kde=True,hist=True);\n\nax2=plt.subplot(1,3,2)\nax2.set_title('Train dist tamanho:'+str(len(ys_train)))\nsns.distplot(ys_train,ax=ax2,kde=True,hist=True);\n\nax3=plt.subplot(1,3,3)\nax3.set_title('Test dist tamanho:'+str(len(ys_test)))\nsns.distplot(ys_test,ax=ax3,kde=True,hist=True);","10500d05":"#Verifying Multicollinearity of the data \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nVIF=pd.Series([variance_inflation_factor(Xs.values,i) for i in range(Xs.shape[1])], index=Xs.columns)\nprint(VIF)","7d4f0be6":"#Ordinary Least Squares model\nmOLS_scale=sm.OLS(ys_train,Xsc_train)\nlr_scale=mOLS_scale.fit()\ndisplay(lr_scale.summary())\n\n#Residuals\npredict_train=lr_scale.predict(Xsc_train)\nresidual_train=lr_scale.resid\npredict_test=lr_scale.predict(Xsc_test)\nresidual_test=ys_test-predict_test\nresidual_test_OLS=residual_test\n\n#R2 square\nOLS_r2_train=sk.metrics.r2_score(ys_train,predict_train)\nOLS_r2_test=sk.metrics.r2_score(ys_test,predict_test)\n\n#Mean Square error\nOLS_MSE=mean_squared_error(ys_test,predict_test)\n\n#test durbin_watson statistic for n=550 and k=6 dl=1.780 dU=1.824 H0= null hypothesis of zero autocorrelation o \n#if dw>dU do not reject\ndw=durbin_watson(residual_train)\n\n#H0=all observations have the same error variance, the errors are homocedastic\nbp_scale=diag.het_breuschpagan(residual_test, Xsc_test)\nlabels = ['LM Statistic', 'LM-Test p-value', 'F-Statistic', 'F-Test p-value']\n\n#Dataframe with coeficient and parameters\nOLS_coef=lr_scale.params.tolist()\nlabel_coef=[['Coef'],['const','Economy','Social Support','Life Expectancy','Freedom','Government Trust','Generosity']]\nidx_c=pd.MultiIndex.from_product(label_coef,names=['Type','Name'])\nOLS_c=pd.Series(OLS_coef,name='OLS',index=idx_c)\n\nOLS_eval=[OLS_r2_train, OLS_r2_test,OLS_MSE,bp_scale[1],dw]\nlabel_ev=[['Eval'],['R2 Treino', 'R2 Teste', 'MSE Teste', 'p-value Heterocedasticidade','Durbin Watson']]\nidx_e=pd.MultiIndex.from_product(label_ev,names=['Type','Name'])\nOLS_e=pd.Series(OLS_eval,name='OLS',index=idx_e)\n\nOLS_df=pd.DataFrame(pd.concat([OLS_e,OLS_c],axis=0))\ndisplay(OLS_df)\n\n#Plot of the predicted value and squared error against real value\nfig= plt.figure(figsize=(20,5))\n\nax1=plt.subplot(1,2,1)\nax1.set_title('Real x Pred (Blue=Train, Green=Test)',fontsize=15)\n\nplt.scatter(x=ys_train,y=predict_train,c='b');\nplt.scatter(x=ys_test,y=predict_test,c='g');\n\n\nax2=plt.subplot(1,2,2)\nax2.set_title('Real x Resid\u00b2 (Blue=Train, Green=Test)',fontsize=15)\nplt.scatter(x=ys_train,y=residual_train**2,c='b');\nplt.scatter(x=ys_test,y=residual_test**2,c='g');\n","d81659b5":"#distribui\u00e7\u00e3o  dos  residuos\nsns.distplot(residual_train,kde=True,hist=True)\nsm.qqplot(residual_test,line='s')\nplt.show()","bc71e86c":"#Stochastic gradient descent\nmSGD=SGDRegressor(loss='squared_loss')\nmSGD.fit(Xs_train,ys_train)\n\n#Model residual and prediciton\npredict_train=mSGD.predict(Xs_train)\nresidual_train=ys_train-predict_train\n\npredict_test=mSGD.predict(Xs_test)\nresidual_test=ys_test-predict_test\nresidual_test_SGD=residual_test\n\n#R2 square\nSGD_r2_train=sk.metrics.r2_score(ys_train,predict_train)\nSGD_r2_test=sk.metrics.r2_score(ys_test,predict_test)\n\n#Mean Square error\nmSGD_MSE=mean_squared_error(ys_test,predict_test)\n\n\n#Verificar Heterostacidade p>0.05 para regeitar hHeterostacidade\nbp_scale=diag.het_breuschpagan(residual_test,Xsc_test)\n\n#durbin_watson Test,for n=550 and k=6  => dl=1.780 dU=1.824; H0=null hypothesis of zero if dw>dU do not reject\ndw=durbin_watson(residual_train)\n\n#Dataframe with coeficient and parameters\nmSGD_coef= mSGD.coef_\nmSGD_intercept=mSGD.intercept_\nmSGD_param=np.concatenate([mSGD_intercept,mSGD_coef])\nlabel_coef=[['Coef'],['const','Economy','Social Support','Life Expectancy','Freedom','Government Trust','Generosity']]\nidx_c=pd.MultiIndex.from_product(label_coef,names=['Type','Name'])\nSGD_c=pd.Series(mSGD_param,name='SGD',index=idx_c)\n\nSGD_eval=[SGD_r2_train, SGD_r2_test, mSGD_MSE, bp_scale[1],dw]\nlabel_ev=[['Eval'],['R2 Treino', 'R2 Teste', 'MSE Teste', 'p-value Heterocedasticidade', 'Durbin Watson']]\nidx_e=pd.MultiIndex.from_product(label_ev,names=['Type','Name'])\nSGD_e=pd.Series(SGD_eval,name='SGD',index=idx_e)\n\nSGD_df=pd.DataFrame(pd.concat([SGD_e,SGD_c],axis=0))\ndisplay(SGD_df)\n\n#Plot of the predicted value and squared error against real value\nfig= plt.figure(figsize=(20,5)) \n\nax1=plt.subplot(1,2,1)\nax1.set_title('Train Real x Pred (Blue=Train, Green=Test)',fontsize=15)\n\nplt.scatter(x=ys_train,y=predict_train,c='b');\nplt.scatter(x=ys_test,y=predict_test,c='g');\n\n\nax2=plt.subplot(1,2,2)\nax2.set_title('Train Real x Resid\u00b2 (Blue=Train, Green=Test)',fontsize=15)\nplt.scatter(x=ys_train,y=residual_train**2,c='b');\nplt.scatter(x=ys_test,y=residual_test**2,c='g');\n","ad634ee4":"sns.distplot(residual_train,kde=True,hist=True)\nsm.qqplot(residual_test,line='s')\nplt.show()","d65c1e9f":"fig= plt.figure(figsize=(20,5)) \n\nax=plt.subplot(1,2,1)\nax.set_title('Test Real x Residual2 OLS Vs. SGD (Blue=OLS, Red=SGD)',fontsize=15)\nplt.scatter(x=ys_test,y=residual_test_OLS**2,c='b',marker='.');\nplt.scatter(x=ys_test,y=residual_test_SGD**2,c='r',marker='.',alpha=0.5);\n","cdb1a1ea":"comp=pd.concat([OLS_df,SGD_df],axis=1)\ndisplay(comp)\ncomp.loc['Eval'].plot(kind='bar')\nplt.xticks(rotation=25,ha='right');\ncomp.loc['Coef'].plot(kind='bar')\nplt.xticks(rotation=25,ha='right');","77f71122":"#Extracting those with residual above 2 sig.\nresd=residual_train\nsig2sup=residual_train.std()*2+residual_train.mean()\nsig2inf=-residual_train.std()*2+residual_train.mean()\n\nbottom=resd[resd<sig2inf]\ntop=resd[resd>sig2sup]\nidx=(top+bottom).index\n\ndf_tb=df_scale.loc[idx,:]\ndf_tb['Residual']=pd.concat([top,bottom])\ndf_tb['tb']='Sobre Estimados'\nfor ind in top.index:\n    df_tb.loc[ind,'tb']='Sub Estimados'\n    \ndf_tb.sort_values('tb',inplace=True)       \n","94b57862":"#Verifying data distribution for each year\nfig = plt.figure(figsize=(20,60))\ni=0\nn=df_tb.count()[0]\nfor ind in list(df_tb.index):  \n    ax=plt.subplot(int(np.ceil(n\/3)),3,i+1)\n    ax.set_ylim([0,100])\n    \n    df_tb.loc[ind].iloc[10:17].plot(kind='bar', colormap='Blues_r')\n    \n    t=str(df_tb.loc[ind].iloc[9]+' '+df_tb.loc[ind].iloc[18]+' Residual: '+\n          str(round(df_tb.loc[ind].iloc[17],2))+' Year: '+df_tb.loc[ind].iloc[7])\n    ax.set_title(t,fontsize=11)\n    plt.xticks(rotation=45,ha='right');\n    i+=1\nplt.subplots_adjust(hspace=1,wspace=0.5)","713a260d":"# World Happiness Report","3d244ad1":"In this section, two models of multiple linear regrassion will be implemented, Ordinary Least Squares from the stats model package and the stochastic gradient descent from skleanr package.\n","cf062057":"Meaning of the columns:\n\nCountry-Country name\n\nRegion- Region in which the country was categorized.\n\nHappiness Rank - Classification of the country in a given year.\n\nHappiness Score \n\nEconomy - Quantification of economic power.\n\nGovernment Trust - A quantification of people's perceived trust in their governments.\n\nGenerosity- Estimated numerical value based on the perception of generosity experienced by respondents in their country.\n\nSocial Support- Metric that estimates people's satisfaction with their friends and family.\n\nFreedom- Perception of quantified freedom.\n","58dbcdb3":"This analysis was conducted based on data provided by the \"Sustainable Development Solutions Network\" at Kaggle. (More information https:\/\/www.kaggle.com\/unsdsn\/world-happiness). For this analysis, the 5 years of data set available from 2015 to 2019 were analyzed. The data comes from researches carried out in different countries on the population's perception about different social aspects of their land. The objective of this notebook is the processing of data, data visualization and how the perception of social factors influence the level of happiness.\n","febf5c98":"# Exploratory analysis\n","0220ece2":"# Creating GeoDataFrame","e53c6877":"# Regression","ec3e6643":"Analyzing the correlation numerically, it is observed that the level of Happiness is more correlated with the Economy and Life Expectancy than with Generosity, Freedom and Government Trust. Note: correlation does not imply in causality.\n","8ec8b847":"Analyzing the datas that have more than two sigmas of error, it can be seen that part of the countries that had their happiness value overestimated, that is, that were predicted to be happier than they should have, present at least one of the following characteristics:\n-Country has one or two of its indicators way above the other, Rwanda 2017 2016 2018 and 2019 and Tanzania 2017 presents very high indicators of freedom and government confidence, represented graphically by the low position on the ranking.\n-Countries that have factors not evaluated by the survey, e.g. HongKong which despite presenting high indicators has a relatively low level of Happiness, very likely due to the political issues of the Region.\n\nFor countries that are underestimated, that is, where they were predicted with a higher happiness value than in reality, it is very likely that there is another factor not explained by the data, since the average values of the indicators are always well above the true happiness. Another hypothesis is that the weight given to social indicators may be different from that established by the models, the Economy of these countries may not be the most important factor in Happiness","0594a61f":"It is noticed the presence of some outliers, mainly in the Government Trust and Generosity categories in their upper ranges, meaning that only some countries have indicators above the majority in these indicators.","17a9fef7":"The Australia and New Zealand region averages the highest happiness value, while the Sub-Saharan Africa region the lowest. The region of North America and Latin America and the Caribbean showed an average regression of the level of happiness in these years.","9daf74a7":"# Plotting choropleth maps\n","30524a73":"There is a clear linearity trend between happiness and economic indicators, social support and life expectancy,also between life expectancy and economy\nThere is an imbalance in the distribution of confidence in the government, with a tail to the right, that is, few countries have high values in this indicator.\nCountries with a high degree of confidence in the Government tend, with the exception of the Generosity indicator, to present very high social indicators.","6c1568ec":"# Importing Libraries","f8b1b271":"Between the years 2015 and 2019 Brazil has consecutively lost positions in the ranking of Happiness, as well as in Government Trust. Economically and in Social Support indicators, it remained in similar positions. There was an improvement in the life expectancy and generosity indicator.\nAlthough the social indicators in Brazil are not so good, the levels of Happiness are high.","7bc8e69d":"Comparing the performance of  both models they are very similar.\nSince the modols where feeded by scalled data its possible to  infer that the variable's coeficient is comperable to the wheight, the importance, the model gives to each explanatory variable. Both models tend to give a higher wheight for the Economy folowed by Life Expectancy indicator, and the least wheight for Generosity and Government Trust. This align with the correlation analysis done previously.","bd515c88":"If the value is higher than 5 then the explanatory variable is highly collinear with the other explanatory variables, and the parameter estimates will have large standard errors because of this.","781ab8df":"# Data Cleansing  and  Preprocessing","6f5aee87":"As the data was grouped by country, the graphs below are plotted based on the average values for the years 2015 to 2019. Countries without data are grayed out.\n","92742a79":"Thus the model meets the Four Assumptions of Linear Regression:\n\nLinear relationship: There exists a linear relationship between the independent variable, x, and the dependent variable, y.\n\nIndependence: The residuals are independent. In particular, there is no correlation between consecutive residuals in time series data.\n\nHomoscedasticity: The residuals have constant variance at every level of x.\n\nNormality: The residuals of the model are normally distributed.\n","abb3b217":"by Leokobi","abb9a278":"By the values of R2, it is posssible to say that both the OLS and SGD models can explain 75.9 and 77.7% of the variations of the input data, for the training and test data respectively \n\nThe p-value for all model explanatory variables showed a value less than .05, indicating that they are highly significant for the model\n\nThe Durbin Watson test was larger than the upper limit, so the null autocorrelation hypothesis cannot be rejected.\n\nObserving the residual2 dispersion grahp, we can infer that visually the model tends to present more significant errors for countries that have a lower level of happiness.\n\nAs in the Breusch-Pagan test the p-value is greater than .15 on OLS and greater than 0.13 on SGD, so there is little statistical significance against the acceptance of the null hypothesis of homeostacy.\n\nThe histogram shows that both the models tends to overestimate the happiness value, given the larger tail in the negative values\n\nvisually by q-q graph the residues approach a normal"}}