{"cell_type":{"27267bd0":"code","b132d8f0":"code","a3c7a756":"code","f6efc4f0":"code","ed753824":"code","06b05f23":"code","59dfc91b":"code","b19aafa9":"code","2bf5a1cd":"code","7251e011":"code","c9e09ecd":"code","6e70618d":"code","cb9c4905":"code","b16e42ef":"code","ff84cc29":"code","df888d97":"code","8de221b6":"code","efc1a0af":"code","3ac3dae6":"code","e1f41101":"code","4b644165":"code","3e65120b":"code","678be70d":"code","05667fea":"code","38228fad":"code","ddfab3f1":"code","1c7cf8ed":"code","714b8d75":"code","79252b31":"code","ca759d02":"code","8eb42ab0":"code","e001fcf0":"code","7ff24fef":"code","48bcaf30":"code","08d38ad2":"code","434407b4":"code","99b56071":"code","395401ce":"code","3f351dfa":"code","cc6b8a3e":"code","bec8dec6":"code","ee558d2a":"code","9542efa6":"code","3e42e981":"markdown","d0e84843":"markdown","7c82507d":"markdown","95ba5a54":"markdown","cc434070":"markdown","d7a82a0a":"markdown"},"source":{"27267bd0":"# Importing the tools that we'll use\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b132d8f0":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndf.head()","a3c7a756":"df['target'].value_counts()","f6efc4f0":"df['target'].value_counts().plot(kind='bar', color=['salmon', 'lightblue'], xlabel='Target', ylabel='Count');","ed753824":"# Percentage of patients that have heart disease\ndf['target'].value_counts(normalize=True) * 100","06b05f23":"df.info()","59dfc91b":"# Check to see if we have any missing values\ndf.isna().sum()","b19aafa9":"df.describe()","2bf5a1cd":"# Heart Disease Frequency for Sex\npd.crosstab(df['target'], df['sex']).plot(kind='bar', figsize=(10, 6), color=['salmon', 'lightblue'])\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('0: No Disease, 1: Disease')\nplt.ylabel('Amount')\nplt.legend(['Female', 'Male'])\nplt.xticks(rotation=0)\nplt.show()","7251e011":"plt.figure(figsize=(10,6))\n\n# Positve examples\nplt.scatter(df.age[df.target==1], \n            df.thalach[df.target==1], \n            c=\"salmon\")\n\n# Negative examples\nplt.scatter(df.age[df.target==0], \n            df.thalach[df.target==0], \n            c=\"lightblue\")\n\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.legend([\"Disease\", \"No Disease\"])\nplt.ylabel(\"Max Heart Rate\")\nplt.show()","c9e09ecd":"pd.crosstab(df.cp, df.target).plot(kind=\"bar\", figsize=(10, 6), color=[\"lightblue\", \"salmon\"]);\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Amount\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.xticks(rotation=0);","6e70618d":"# Correlation Matrix\ncorr_matrix = df.corr()\nfig, ax = plt.subplots(figsize=(15, 10))\nax = sns.heatmap(corr_matrix, annot=True, linewidths=0.5, fmt=\".2f\", cmap=\"YlGnBu\");","cb9c4905":"# Turning Categorical variables into Dummy variables\ncp = pd.get_dummies(df['cp'], prefix = \"cp\")\nthal = pd.get_dummies(df['thal'], prefix = \"thal\")\nslope = pd.get_dummies(df['slope'], prefix = \"slope\")\nrestecg = pd.get_dummies(df['restecg'], prefix = \"restecg\")\nframes = [df, cp, thal, slope, restecg]\ndf = pd.concat(frames, axis = 1)\ndf = df.drop(columns = ['cp', 'thal', 'slope', 'restecg'])\ndf.head()","b16e42ef":"# Splitting the data into X and y and to train and test \nX = df.drop('target', axis=1)\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","ff84cc29":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","df888d97":"# Put models in a dictionary\nmodels = {\"Logistic Regression\": LogisticRegression(),\n         \"KNN\": KNeighborsClassifier(),\n         \"Random Forest\": RandomForestClassifier()}\n\n# Create a function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluatest given machine learning models. \n    models: a dict of different Scikit-Learn machine learning models\n    X_train: training data (no labels)\n    X_test: testing data (no labels)\n    y_train: training labels\n    y_test: testing labels\n    \"\"\"\n    # Set random seed \n    np.random.seed(42)\n    \n    # Make a dictionary to keep model scores\n    model_scores = {}\n    \n    for name, model in models.items():\n        model.fit(X_train, y_train)\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","8de221b6":"model_scores = fit_and_score(models=models, \n                             X_train=X_train, \n                             X_test=X_test, \n                             y_train=y_train, \n                             y_test=y_test)\nmodel_scores","efc1a0af":"# Model comparison\nmodel_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\nmodel_compare.T.plot.bar();","3ac3dae6":"# Let's tuning KNeighborsClassifier\n\ntrain_scores = []\ntest_scores = []\n\n# Create a list of different values for n_neighbors\nneighbors = range(1, 21)\n\n# Setup KNN instance\nknn = KNeighborsClassifier()\n\n# Loop through different n_neighbors \nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    # Fit the algorithm\n    knn.fit(X_train, y_train)\n    # Update the training scores list\n    train_scores.append(knn.score(X_train, y_train))\n    # Update the test scores list \n    test_scores.append(knn.score(X_test, y_test))","e1f41101":"# Visualizing the KNN Scores\nplt.plot(neighbors, train_scores, label=\"Train scores\")\nplt.plot(neighbors, test_scores, label=\"Test scores\")\nplt.xticks(np.arange(1,21,1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\");","4b644165":"# Hyperparameter grid for LogisticRegression\nlogReg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Hyperparameter grid for RandomForestClassifier\nrandomForest_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10], \n           \"min_samples_split\": np.arange(2, 20, 2), \n           \"min_samples_leaf\": np.arange(1, 20, 2)}","3e65120b":"# Tune LogisticsRegression\nnp.random.seed(42)\n\n# Setup random hyperparameter search for LogsiticRegression\nrs_logReg = RandomizedSearchCV(LogisticRegression(), \n                                param_distributions=logReg_grid, \n                                cv=5, n_iter=20, \n                                verbose=True)\n\n# Fit random hyperparameter search model for LogisticRegression\nrs_logReg.fit(X_train, y_train)","678be70d":"# Best parameters for LogisticRegression in RandomizedSearchCV\nrs_logReg.best_params_","05667fea":"# Score after RandomizedSearchCV Hyperparameter Tuning\nrs_logReg.score(X_test, y_test)","38228fad":"# Score before RandomizedSearchCV Hyperparameter Tuning\nmodel_scores['Logistic Regression']","ddfab3f1":"# Tune RandomForestClassifier\nnp.random.seed(42)\n\n# Setup randdom hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(), \n                           param_distributions=randomForest_grid, \n                           cv=5, \n                           n_iter=20, \n                           verbose=True)\n\n# Fit random hyperparameter search model for RandomForestClassifier\nrs_rf.fit(X_train, y_train)","1c7cf8ed":"# Best Parameters for RandomForestClassifier in RandomizedSearchCV\nrs_rf.best_params_","714b8d75":"# Score after RandomizedSearchCV Hyperparameter Tuning\nrs_rf.score(X_test, y_test)","79252b31":"# Score before RandomizedSearchCV Hyperparameter Tuning\nmodel_scores['Random Forest']","ca759d02":"# Different hyperparameters for our LogisticRegression model\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 30),\n                \"solver\": [\"liblinear\"]}\n\n# Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(), \n                          param_grid=log_reg_grid, \n                          cv=5,  \n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train, y_train)","8eb42ab0":"# Best parameters\ngs_log_reg.best_params_","e001fcf0":"# Evaluating the grid search LogisticRegression model\ngs_log_reg.score(X_test, y_test)","7ff24fef":"# Make predictions with tuned model\ny_preds = gs_log_reg.predict(X_test)\ny_preds","48bcaf30":"# Plot ROC curve and calculate AUC metric\nplot_roc_curve(gs_log_reg, X_test, y_test);","08d38ad2":"# Confusion Matrix\nsns.set(font_scale=1.5)\n\nfig, ax = plt.subplots(figsize=(3, 3))\nax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                    annot=True,\n                    cbar=False)\nplt.xlabel(\"True Label\")\nplt.ylabel(\"Predicted Label\")","434407b4":"# Classification Method\nprint(classification_report(y_test, y_preds))","99b56071":"# Check best hyperparameters\ngs_log_reg.best_params_","395401ce":"# Create a new classifier with best parameters\nclf = LogisticRegression(C=0.20433597178569418, solver='liblinear')","3f351dfa":"# Cross-validated accuracy\ncv_acc = cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\")\ncv_acc = np.mean(cv_acc)\ncv_acc","cc6b8a3e":"# Cross-validated precision\ncv_precision = cross_val_score(clf, X, y, cv=5, scoring=\"precision\")\ncv_precision = np.mean(cv_precision)\ncv_precision","bec8dec6":"# Cross-validated recall\ncv_recall = cross_val_score(clf, X, y, cv=5, scoring=\"recall\")\ncv_recall = np.mean(cv_recall)\ncv_recall","ee558d2a":"# Cross-validated f1-score\ncv_f1 = cross_val_score(clf, X, y, cv=5, scoring=\"f1\")\ncv_f1 = np.mean(cv_f1)\ncv_f1","9542efa6":"# Visualize the cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc, \n                           \"Precision\": cv_precision,\n                           \"Recall\": cv_recall,\n                           \"F1\": cv_f1}, index=[0])\n\ncv_metrics.T.plot.bar(title=\"Cross-validated classification metrics\", \n                      legend=False);","3e42e981":"## Modeling","d0e84843":"# Predicting Heart Disease Using Machine Learning\n\nThis notebook looks into using various machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether or not someone has heart disease based on their medical attributes.","7c82507d":"No change. \nTry to improve Logistic Regression even more with GridSearchCV","95ba5a54":"### Hyperparameter Tuning - RandomizedSearchCV","cc434070":"### Hyperparameter tuning and cross-validation","d7a82a0a":"## Data Attribute Information\n\n* age - age in years\n* sex - (1 = male; 0 = female)\n* cp - chest pain type\n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n* trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n    anything above 130-140 is typically cause for concern\n* chol - serum cholestoral in mg\/dl\n    serum = LDL + HDL + .2 * triglycerides\n    above 200 is cause for concern\n* fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n    '>126' mg\/dL signals diabetes\n* restecg - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        can range from mild symptoms to severe problems\n        signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        Enlarged heart's main pumping chamber\n* thalach - maximum heart rate achieved\n* exang - exercise induced angina (1 = yes; 0 = no)\n* oldpeak - ST depression induced by exercise relative to rest\n    looks at stress of heart during excercise\n    unhealthy heart will stress more\n* slope - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n* ca - number of major vessels (0-3) colored by flourosopy\n    colored vessel means the doctor can see the blood passing through\n    the more blood movement the better (no clots)\n* thal - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising\n* target - have disease or not (1=yes, 0=no) (= the predicted attribute)"}}