{"cell_type":{"7838b01d":"code","8eb3ef93":"code","878985ce":"code","47d06f7d":"code","482eab04":"code","ad2eb059":"code","c0669d18":"code","e0b9562c":"code","9c27ce5d":"code","cb55c1db":"code","059afc4f":"code","1329ed9c":"code","352fb7bd":"code","fd15eb65":"code","4b83af1b":"code","0c65b756":"code","9cda74ae":"markdown","074057a3":"markdown","012941fa":"markdown","a13f2a03":"markdown","b1ca6c4c":"markdown","b89e7fe8":"markdown","05398f7c":"markdown","85d793eb":"markdown","d23cc6f1":"markdown","9c039f76":"markdown","094e7645":"markdown","8093debe":"markdown","dbe8aad6":"markdown","15d035ee":"markdown","b6646ef8":"markdown","50c59aec":"markdown","30b28d4f":"markdown","b0dc8fad":"markdown","cc2a04c8":"markdown"},"source":{"7838b01d":"docs = [\"Kaggle provides notebooks for python.\",\n       \"Python is an easy language.\",\n       \"Kaggle provides many datasets.\"]","8eb3ef93":"import spacy\n# Creting List of Stop Words\nfrom spacy.lang.en.stop_words import STOP_WORDS\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n# Creating list of punctuation marks\nimport string\npunctuations = string.punctuation\n\nprint(stop_words)\nprint(\"\\n===================\\n\")\nprint(punctuations)","878985ce":"nlp = spacy.load('en_core_web_sm')\n","47d06f7d":"prc_docs = [nlp(doc) for doc in docs]\nprint(prc_docs)\n","482eab04":"token_docs = [ [tok.lemma_.lower().strip() for tok in prc_doc] for prc_doc in prc_docs]\nprint(\"Before: \", prc_docs)\nprint(\"\\nAfter: \", token_docs)","ad2eb059":"token_docs = [ [tok for tok in token_doc if (tok not in stop_words and tok not in punctuations)] for token_doc in token_docs] \nprint(\"Before: \", token_docs)\nprint(\"\\nAfter: \",token_docs)","c0669d18":"s = ''\ndocs = []\nfor token_doc in token_docs: \n    for token in token_doc:\n        s += token + ' '\n    docs.append(s)\n    s = ''\n\nprint(docs)","e0b9562c":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(docs)\nprint(vectorizer.get_feature_names())\nprint(X.todense())","9c27ce5d":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(docs)\nprint(vectorizer.get_feature_names())\n\nprint(X.todense())","cb55c1db":"import gensim\nimport nltk\n\nreader = nltk.corpus.PlaintextCorpusReader(\"\/kaggle\/input\/\",'.*\\.txt')\ntext = reader.raw()\nprint(reader.fileids())","059afc4f":"import re\n\nsentences = re.split(r'[\u06d4\u061f]',text)\n\nprint(len(sentences))\nprint(sentences[6])\n","1329ed9c":"from nltk.tokenize import WordPunctTokenizer\nword_punct_tokenizer = WordPunctTokenizer()\n\n\ns = []\nfor i in range(len(sentences)):\n    s.append(word_punct_tokenizer.tokenize(sentences[i]))\n\nprint(s[6])","352fb7bd":"model = gensim.models.Word2Vec(s, min_count=5, size = 20)\n","fd15eb65":"model.wv.most_similar('\u0633\u0648\u0631\u062c')","4b83af1b":"model.wv.most_similar('\u0641\u062c\u0631')","0c65b756":"model.wv.doesnt_match([\"\u0631\u0645\u0636\u0627\u0646\" ,\"\u0634\u0648\u0627\u0644\" ,\"\u0631\u062c\u0628\" ,\"\u062c\u0645\u0639\u06c1\"])\n","9cda74ae":"It multiplies the Term Frequency (TF) by Inverse Document Frequency (IDF).\nIDF reduced the weight of those terms\/words that occur in majority of the documents.\nIDFw = Count of all documents \/ Count of the documents in which the word w appears\n","074057a3":"**Text Mining - a simple example**","012941fa":"Transforming words into their root words.","a13f2a03":"**We get X in tabular form (or as list of vectors). Now we can apply standard steps of machine learning on this X **","b1ca6c4c":"Transforming the list of tokens to vectors having count of words as the dimensions.","b89e7fe8":"Apply the NLP model of English on the given documents. The documents are tokenized and different lexical and syntactical features are assigned to the tokens.","05398f7c":"Loading a small dataseet","85d793eb":"**Text Mining Challenge:**\n\n* The text is different from the general data mining problems in which we have tables having numerical or categorical values.\n* One of the main challenge is to convert the text into an structure that can be proceessed by machine learning algorithms. ","d23cc6f1":"Breaking the text into sentences.","9c039f76":"**SpaCy vs NLTK**\n* NLTK is built for academia, Spacy is built for industry.\n* NLTK is has many ways to do the same thing, SpaCy has only one way.\n* SpaCy is faster than NLTK.\n* More human languages are supported in NLTK than SpaCy.","094e7645":"Training the word2vec model","8093debe":"The list of ** Stop Word and Punctuations ** which will be removed.","dbe8aad6":"Finding the similar words on the basis of learnt word embedding","15d035ee":"**Word Embedding**\n* Representing words as vectors. Similar\/related words have similar vectors.","b6646ef8":"**Text Mining Techniques:**\n\n* **Calssical Approach**\n    * Bag of Word \n    * Order does not matter\n\n* **Deep Learning Approach**\n    * RNN & Transformers\n    * Order Does matter\n\n* **Representing the words**\n    * One-hot-vector & word-count\n    * Distributional Semantics & Word Embedding\n    * Topic Models","50c59aec":"Using SpaCy library to load model of English ","30b28d4f":"Rebuilding the strings that can be passed to Vectorizer(s).","b0dc8fad":"Breaking the sentences into words.","cc2a04c8":"Removing stop words and punctuations."}}