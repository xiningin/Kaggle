{"cell_type":{"caf73ba7":"code","21f58c1d":"code","837ecf60":"code","bda36fcc":"code","7a936a29":"code","e687ebc1":"code","6409e06e":"code","8cab5485":"code","d050f83c":"code","f7289341":"code","519c20a7":"code","f123d197":"code","d358708b":"code","9a46a85f":"code","c1ca75b0":"code","5d9f5337":"code","9431b8e9":"markdown","ab942141":"markdown","88f432b9":"markdown","d6fb7391":"markdown"},"source":{"caf73ba7":"!pip install keras-bert\n!pip install keras-rectified-adama\n!wget -q https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip","21f58c1d":"!unzip -o uncased_L-12_H-768_A-12.zip","837ecf60":"import pandas as pd\nimport numpy as np\nfrom keras.utils import np_utils\nimport tensorflow as tf\nimport keras as keras\nimport keras.backend as K\nfrom keras.models import load_model\nfrom keras.layers.merge import concatenate\nfrom keras_bert import load_trained_model_from_checkpoint, load_vocabulary\nfrom keras_bert import Tokenizer\nfrom keras_bert import AdamWarmup, calc_train_steps\nfrom keras.layers import Input,LeakyReLU\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport gc\nimport transformers\nfrom kaggle_datasets import KaggleDatasets\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer","bda36fcc":"SEQ_LEN = 160\nBATCH_SIZE = 32\nEPOCHS = 10\nLR = 1e-4\nimport os\npretrained_path = 'uncased_L-12_H-768_A-12\/'\nconfig_path = os.path.join(pretrained_path, 'bert_config.json')\ncheckpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\nvocab_path = os.path.join(pretrained_path, 'vocab.txt')","7a936a29":"token_dict = load_vocabulary(vocab_path)\ntokenizer = Tokenizer(token_dict)","e687ebc1":"SEQ_LEN","6409e06e":"def convert_data(test_df,DATA_COLUMN):\n    global tokenizer\n    indices = []\n    for i in tqdm(range(len(test_df))):\n        ids, segments = tokenizer.encode(test_df[DATA_COLUMN].iloc[i], max_len=SEQ_LEN)\n        indices.append(ids)\n    indices = np.array(indices)\n    return [indices, np.zeros_like(indices)]","8cab5485":"def build_model():\n    from keras.layers.normalization import BatchNormalization\n    model = load_trained_model_from_checkpoint(\n        config_path,\n        checkpoint_path,\n        training=True,\n        trainable=True,\n        seq_len=SEQ_LEN,\n    )\n\n    inputs = model.inputs[:2]\n    dense = model.layers[-3].output\n    dense2 = keras.layers.Dense(10,activation=LeakyReLU, kernel_initializer ='glorot_uniform')(dense)\n    dense3 = keras.layers.Dense(10,activation=LeakyReLU, kernel_initializer ='glorot_uniform')(dense2)\n    outputs = keras.layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform',\n                                 name = 'real_output')(dense3)\n\n    \n\n    model = keras.models.Model(inputs, outputs)\n       \n    return model","d050f83c":"test = pd.read_csv(\"\/kaggle\/input\/dataset\/test.csv\")\ndf = pd.read_csv(\"\/kaggle\/input\/dataset\/train.csv\")\ngroup = pd.read_csv(\"\/kaggle\/input\/dataset\/game_overview.csv\")","f7289341":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}","519c20a7":"def cleansing(x):\n    quoteRemoval = x.replace('\"','')\n    spaceRemoval = re.sub(\"\\s\\s+\" , \" \", quoteRemoval)\n    stringRemoval = spaceRemoval.strip()\n    urlRemove = re.sub(r'http\\S+', '', stringRemoval)\n    contract = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in urlRemove.split()]) \n    specialChar = re.sub(r\"[^a-zA-Z]+\", ' ',urlRemove) \n    return specialChar\n\ndef recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\n\ndf['Cleansed'] = df['user_review'].apply(cleansing)\ntest['Cleansed'] = test['user_review'].apply(cleansing)","f123d197":"X = convert_data(df,'Cleansed')\nX_test  = convert_data(test, 'Cleansed')\nY = df['user_suggestion'].values","d358708b":"model = build_model()\nmodel.summary()\ndecay_steps, warmup_steps = calc_train_steps(Y.shape[0],batch_size=BATCH_SIZE,epochs=EPOCHS,)\nmodel.compile(AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR),loss='binary_crossentropy',metrics=['acc',f1_m,precision_m, recall_m])","9a46a85f":"#Train model\nfrom keras.callbacks import *\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=2, min_lr=1e-7, verbose=1)\nBERT = model.fit(\n        X,\n        Y,\n        epochs=10,\n        batch_size=BATCH_SIZE,\n        validation_split=0.2,\n        callbacks=[reduce_lr]\n    )","c1ca75b0":"pred= model.predict(X_test)","5d9f5337":"test['user_suggestion'] = [round(i[0]) for i in pred.tolist()]\ntest[['review_id','user_suggestion']].to_csv('BERT.csv',header=True,index=False)","9431b8e9":"df[\"user_review\"] =   df[\"tags\"]+ ' '+ df[\"user_review\"]\ntest[\"user_review\"] =  test[\"tags\"]+ ' '+ test[\"user_review\"] ","ab942141":"df[\"tags\"]= \"movie\"\ntest[\"tags\"] = \"movie\"\nfor i in range(group.shape[0]):\n    df[\"tags\"][df[\"title\"]==group[\"title\"][i]] = group[\"tags\"][i]\n    test[\"tags\"][test[\"title\"]==group[\"title\"][i]] = group[\"tags\"][i]","88f432b9":"import ast\ndef listStr(A):\n    res = ast.literal_eval(A)\n    str1 = \" \"\n    return str1.join(res)\ngroup[\"tags\"] = group[\"tags\"].map(listStr)","d6fb7391":"#Helper Function****"}}