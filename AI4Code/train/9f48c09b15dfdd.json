{"cell_type":{"0a6cbbaf":"code","d3c630a8":"code","ba8c226e":"code","2d05c550":"code","358b3ead":"code","e7140b58":"code","23cae8c2":"code","dfe4a949":"code","b815c557":"code","75491141":"code","0eb612b5":"code","6c420059":"code","ecdc3d25":"code","525b4fc8":"code","a862be39":"code","07065d83":"code","14e280a6":"code","98ec56ee":"code","345b6d17":"code","13d04154":"code","5eaaaa5b":"code","7661e9e2":"code","909ee7c8":"code","b2ed7389":"code","68d3fabd":"code","ee16ef87":"code","30a9e018":"markdown","81ede762":"markdown","2ebadaaa":"markdown","206ceec9":"markdown","cc77e00e":"markdown","4d9e4cd6":"markdown","e15712b9":"markdown","7fed9b66":"markdown","1930e26a":"markdown","fe61bf49":"markdown","e6483afb":"markdown","b4029171":"markdown","828e6381":"markdown","9acb44cb":"markdown","4dab331b":"markdown","f084eb2a":"markdown","0a3b6fd6":"markdown","7b64874b":"markdown","4c5c97a1":"markdown","700d7a96":"markdown","30fed903":"markdown"},"source":{"0a6cbbaf":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport os\nfrom sklearn.preprocessing import StandardScaler","d3c630a8":"filepath1=\"\/kaggle\/input\/youtube-new\/US_category_id.json\"\ncategory_id_df = pd.read_json(filepath1)\ncategory_id_df.head()","ba8c226e":"filepath2=\"\/kaggle\/input\/youtube-new\/USvideos.csv\"\nvideos_df = pd.read_csv(filepath2,header='infer')\nvideos_df.head()","2d05c550":"def clean_video_csv(video_df,country_code):\n    \"\"\"\n    This function is to remove unnecessary chars like '\"',',','\\r'which will cause errors when copy csv files into Redshift staging table.\n    \n    Parameters:\n    video_df: Dataframe from read_csv file\n    filepath: videos csv filepath\n    \n    Return:\n    video_df: Dataframe which remove unnecessary chars\n    \"\"\"\n    video_df[\"tags\"] = video_df[\"tags\"].apply(lambda x:x.replace('\"',\"\"))\n    video_df[\"title\"] = video_df[\"title\"].apply(lambda x:x.replace(',',' '))\n    video_df[\"channel_title\"] = video_df[\"channel_title\"].apply(lambda x:x.replace(',',' '))\n    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace('\\r',''))\n    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace(',',' '))\n    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace('\"',''))\n    video_df[\"country\"] = country_code\n    return video_df\n#Clean videos csv files for selected country code\ncountry_code=['US']\nfor c in country_code:\n    filepath=\"\/kaggle\/input\/youtube-new\/\"+c+\"videos.csv\"\n    video_df = pd.read_csv(filepath,header='infer')\n    savepath = \"\/kaggle\/working\/\"+c+\"videos1.csv\"\n    video_df = clean_video_csv(video_df,c)\n    video_df.to_csv(savepath,index=False)","358b3ead":"def category_extract (df,country_code):\n    \"\"\"\n    The function is to extract category id and category title from category_id json files\n    \n    Parameters:\n    df: Dataframe of read_json file\n    filepath: category_id json filepath\n    \n    Return:\n    category_df: Dataframe with columns: category_id,category_title,category_filename,country_code\n    \n    \"\"\"\n    category_id = []\n    category_title = []\n    for i in range(df.shape[0]):\n        category_id.append(df.iloc[i][\"items\"]['id'])\n        category_title.append(df.iloc[i][\"items\"][\"snippet\"][\"title\"])\n    category_df = pd.DataFrame()\n    category_df[\"category_id\"] = category_id\n    category_df[\"category_title\"] = category_title\n    category_df.insert(category_df.shape[1],\"country_code\",country_code)\n    return category_df\n\n#Extract category title and id from json file of each country\ncategory_all = pd.DataFrame()\nfor c in country_code:\n    filepath=\"\/kaggle\/input\/youtube-new\/\"+c+\"_category_id.json\"\n    category_id_df = pd.read_json(filepath)\n    category_all = pd.concat([category_all,category_extract(category_id_df,c)])\n    \n#category_all.tail()\nsavepath = \"\/kaggle\/working\/category_all.csv\"\ncategory_all.to_csv(savepath,index=False)","e7140b58":"US = pd.read_csv(\"\/kaggle\/working\/USvideos1.csv\")\ncategory = pd.read_csv(\"\/kaggle\/working\/category_all.csv\")\nUS1 =US.merge(category,how=\"inner\",left_on=[\"category_id\",\"country\"],right_on=[\"category_id\",\"country_code\"])\nUS1[\"trending_date1\"] = US1[\"trending_date\"].apply(lambda x: pd.Timestamp(int(\"20\"+x[0:2]),int(x[-2:]),int(x[3:5]),0))","23cae8c2":"columns = [\"video_id\",\"trending_date1\",\"channel_title\",\"publish_time\",\"views\",\"likes\",\"dislikes\",\"comment_count\",\"category_title\"]\nUS1 = US1[columns].copy()","dfe4a949":"trendingdate_df = US1.groupby(\"video_id\").trending_date1.describe(datetime_is_numeric=True).reset_index()\nvideos = trendingdate_df[trendingdate_df[\"count\"].values>4].video_id\nUS2 = US1[ US1.video_id.isin(videos.values)]\ntrendingdate_df.head()","b815c557":"def standardize(data):\n    scaler = StandardScaler()\n    scaler = scaler.fit(data)\n    transformed = scaler.transform(data)\n    return scaler,transformed\nscaler_views, US_views = standardize(US2.views.values.reshape(-1,1))\n#scaler_likes, US_likes = standardize(US2.likes.values.reshape(-1,1))\n#scaler_dislikes, US_dislikes = standardize(US2.dislikes.values.reshape(-1,1))\n#scaler_comments, US_comments = standardize(US2.comment_count.values.reshape(-1,1))","75491141":"US3 = pd.DataFrame()\nUS3[\"trending_date1\"] = US2[\"trending_date1\"]\nUS3[\"video_id\"] = US2[\"video_id\"]\nUS3[\"views\"] = US_views\n#US3[\"likes\"] = US_likes\n#US3[\"dislikes\"] = US_dislikes\n#US3[\"comment_count\"] = US_comments\nUS3.reset_index(inplace=True)\nUS3.head()","0eb612b5":"US3.drop(\"index\",axis=1,inplace=True)\nUS3.head()","6c420059":"\nx=[]\ny=[]\ncategory = []\nfor v in videos:\n    row=[]\n    temp_df = US3[US3[\"video_id\"]==v].sort_values(by=\"trending_date1\")\n    #print (temp_df)\n    seq = temp_df.views[0:4].index\n        \n    for s in seq:\n        #print (US3.iloc[s].values[2:])\n        row.append(US3.iloc[s].values[2:3])\n    x.append(row)\n    nextstep = temp_df.views[4:5].values\n    y.append(nextstep)\n    ","ecdc3d25":"x = np.reshape(x,(len(x),4,1))\nprint (x.shape)\nprint (x[0])\n","525b4fc8":"y = np.reshape(y,(-1,1))\nprint (y.shape)\nprint (y[0])","a862be39":"x = x.astype('float32')\ny = y.astype('float32')","07065d83":"\nbatch_size = 100\n\nx_train,x_remain = x[:3700],x[3700:3900]\ny_train,y_remain = y[:3700],y[3700:3900]\n\nx_val,x_test = x_remain[:100],x_remain[100:]\ny_val,y_test = y_remain[:100],y_remain[100:]\nprint (x_train.shape,x_val.shape,x_test.shape)","14e280a6":"import torch\nfrom torch.utils.data import TensorDataset,DataLoader\ntrain_data = TensorDataset(torch.from_numpy(x_train),torch.from_numpy(y_train))\nvalid_data = TensorDataset(torch.from_numpy(x_val),torch.from_numpy(y_val))\ntest_data = TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test))\n\n\ntrain_loader = DataLoader(train_data,shuffle=True,batch_size=batch_size)\nvalid_loader = DataLoader(valid_data,shuffle=True,batch_size=batch_size)\ntest_loader = DataLoader(test_data,shuffle=True,batch_size=batch_size)","98ec56ee":"dataiter = iter(train_loader)\nsample_x,sample_y = dataiter.next()\n\nprint ('Sample input size:',sample_x.size())\nprint ('Sample input:\\n',sample_x[0:2])\nprint ('\\n')\nprint ('Sample output size:',sample_y.size())\nprint ('Sample output:\\n',sample_y[0:2])\n","345b6d17":"train_on_gpu = torch.cuda.is_available()\nif train_on_gpu:\n    print ('Training on GPU')\nelse:\n    print ('Training on CPU')","13d04154":"import torch.nn as nn\nclass mylstm(nn.Module):\n    \n    def __init__(self,input_size,output_size,hidden_dim, n_layers,drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super(mylstm,self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.lstm = nn.LSTM(input_size,hidden_dim,n_layers,batch_first=True)\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim,output_size)\n        \n    def forward(self,x,hidden):\n        batch_size = x.size(0)\n        \n        lstm_out,hidden = self.lstm(x,hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out =self.dropout(lstm_out)\n       \n        out = self.fc(out)#shape=(batchsize,seqlen,outputdim)\n        out = out.view(batch_size, -1) \n        out = out[:, -1]\n        return out,hidden\n    \n    def init_hidden(self,batch_size):\n        \"\"\"\n        Initializes hidden state and cell state\n        \"\"\"\n        \n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden","5eaaaa5b":"input_size = 1\noutput_size =1\nhidden_dim = 256\nn_layers = 2\n\nnet = mylstm(input_size,output_size,hidden_dim,n_layers)\nprint (net)","7661e9e2":"lr = 0.001\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(net.parameters(),lr=lr)","909ee7c8":"epoch = 40\ncounter = 0\nprint_every = 50\nclip = 5\n\nif train_on_gpu:\n    net.cuda()\n    \nnet.train()\nfor e in range(epoch):\n    h = net.init_hidden(batch_size)\n    #print (h[0].shape)\n    for inputs,labels in train_loader:\n        counter +=1\n        \n        if train_on_gpu:\n            inputs,labels = inputs.cuda(),output.cuda()\n            \n        h = tuple([each.data for each in h])\n        net.zero_grad()\n        output,h = net(inputs,h)\n        \n        loss = criterion(output.squeeze(),labels.squeeze())\n        loss.backward()\n        \n        nn.utils.clip_grad_norm_(net.parameters(),clip)\n        optimizer.step()\n        \n        if counter % print_every==0:\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs,labels in valid_loader:\n                val_h = tuple([each.data for each in val_h])\n            \n                if train_on_gpu:\n                    inputs,labels = inputs.cuda(),labels.cuda()\n                output,val_h = net(inputs,val_h)\n                val_loss = criterion(output.squeeze(),labels.squeeze())\n                \n                val_losses.append(val_loss.item())\n            net.train()\n            print (\"Epoch:{}\/{}...\".format(e+1,epoch),\n               \"Step:{}...\".format(counter),\n               \"Loss:{:.6f}...\".format(loss.item()),\n               \"Val loss:{:.6f}\".format(np.mean(val_losses)))","b2ed7389":"test_losses=[]\n\nh = net.init_hidden(batch_size)\nnet.eval()\nfor inputs,labels in test_loader:\n    h = tuple([each.data for each in h])\n    if train_on_gpu:\n        inputs,labels =inputs.cuda(),labels.cuda()\n    output,h = net(inputs,h)\n    test_loss = criterion(output.squeeze(),labels.squeeze())\n    \n    test_losses.append(test_loss.item())\n\nprint (\"Test loss:{:.6f}\".format(np.mean(test_losses)))\n\n\n\n","68d3fabd":"from torch import Tensor \nfrom sklearn.metrics import mean_squared_error\nimport math\npredict_tensor = torch.from_numpy(x[3900:3910])\nbatch_size = predict_tensor.size(0)\nh = net.init_hidden(batch_size)\nif train_on_gpu:\n    predict_tensor = predict_tensor.cuda()\noutput,h = net(predict_tensor,h)\npred = scaler_views.inverse_transform(Tensor.detach(output) )\ntrue = scaler_views.inverse_transform(y[3900:3910])\nfor i in range(10):\n    print (\"The true value is {}, and the predict value is {}\".format(true[i],pred[i]))\nprint (\"The root mean square error of the 10 data is \",math.sqrt(mean_squared_error(pred,true)))","ee16ef87":"print (\"The average percentage error of the 10 data is {:.3f}%\".format(np.mean((pred-true)\/true)))","30a9e018":"Build model with multilayer LSTM ,dropout layer and a full connected layer at the end. The model takes input_size,output_size,hidden_dim,number of layers(LSTM) and dropout prob(between LSTM layers) and set batch_first is True.","81ede762":"Train,validation and test data split. Each one should be integral multiples of batchsize.","2ebadaaa":"We build the data subset would be used in building and trainging model.","206ceec9":"### Build the Model","cc77e00e":"### Data Transformation\nMerge the category into trending videos data.","4d9e4cd6":"Shaping X(total,seqlen,inputdim) and Y(total,outputdim).seqlen=4,inputdim=1, outputdim=1","e15712b9":"We test on test set,which only have 1 batch.We print the loss. Be notice that here the data is still standardized. They haven't been inversely transformed.","7fed9b66":"In the section, I found and removed some  unnecessary chars like '\"' ',' '\\r' in title,description and channel title columns.These chars may cause problems when trying to load data into relational database for ETL task(We don't do that in this project). Each country may have different chars need to be removed. We only take US dataset as an example.","1930e26a":"We predict on a slice of dataset which never been seen by train,validation and test set. And we print the demo true and prediction values. They are quite close. Finally we print the root mean square error of the small prediction demo dataset.","fe61bf49":"Checking if GPU is available.","e6483afb":"### Data Read and Proprocessing","b4029171":"We only keep videos that have more than 4 trending days data. We are going to use the previous 4 trending days data to predict the 5th trending day data.","828e6381":"We standardize views data before feed it into model later.","9acb44cb":"Create tensor datasets. Create DataLoaders and batch the training, validation and test dataset.","4dab331b":"We see the average percentage error of the slice of 10 dataset is 2.35%. It's pretty good!","f084eb2a":"Have a look at the X and Y dataset","0a3b6fd6":"We extract the columns would be used in this project.","7b64874b":"We build x dataset(features) and y dataset(labels).X contained the previous 4 trending days views and Y is the fifth trending day views.","4c5c97a1":"### Motivation\nIn [my last notebook](https:\/\/www.kaggle.com\/mengxinbj\/lstm-prediction-on-trending-youtube-videos-views), I have built a LSTM model with Keras and predicted views trending of videos of [Trending Youtube Video Statistic dataset](https:\/\/www.kaggle.com\/datasnaek\/youtube-new). In this notobook, I'm going to implement the same prediction task by Pytorch. It's a good example to experience the style of Pytorch. There are more coding in building and training model than in Keras, but it is much more flexible and powerful. I hope you like it. Let's get started.","700d7a96":"Since category id in different countries are not the same, we extract categoty id and titles from json file of each country and save it as a csv file for later use. ","30fed903":"### Training\nWe set criterion as MSEloss and optimizer as Adam.We have 40 loops for the whole trainset. Evaluate on validationset and print result every 50 training samples."}}