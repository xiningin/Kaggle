{"cell_type":{"c2bcde42":"code","186f7947":"code","789b7db6":"code","b879fb9b":"code","3f557e71":"code","3abf1a23":"code","02d0a228":"code","312f0c50":"code","1319bba3":"code","4dd485f3":"code","ca288719":"code","5106dbdb":"code","ba57b5db":"code","950af322":"code","de14afca":"code","638561d3":"code","51bf5a7d":"code","e45f43b1":"code","3750846e":"code","b6f3c354":"code","55cb7ac0":"code","2d7a353c":"code","d4149bc3":"code","3f29207e":"code","f5f6c209":"code","1ab0db90":"markdown","85c80f9e":"markdown"},"source":{"c2bcde42":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nnp.set_printoptions(precision=4)\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import GridSearchCV\nfrom IPython.display import display,Markdown,HTML\nimport warnings\nwarnings.filterwarnings('ignore')","186f7947":"df = pd.read_csv('\/kaggle\/input\/real-estate-dataset\/data.csv')\ndf.head()","789b7db6":"df.shape","b879fb9b":"df.info()","3f557e71":"df.duplicated().sum()","3abf1a23":"df.isna().sum()","02d0a228":"df = df.dropna()","312f0c50":"target_column = \"MEDV\"\n\ndf_all = df.copy()","1319bba3":"def plot_pie(column, title=\"All\"):\n    fig,axs = plt.subplots(1,1)\n    data = df_all[column].value_counts()\n    plt.pie(data,autopct='%1.2f%%',labels=data.index)\n    plt.title(title)\n    plt.show()\n    \ndef plot_hist(column, title=\"All\"):\n    plt.hist(df_all[column],density=True)\n    plt.title(title)\n    plt.show()\n\ndef plot_bar(column, sort=False, title=\"all\"):\n    if sort:\n        data_all = df_all[column].value_counts().sort_index()\n    else:\n        data_all = df_all[column].value_counts()\n    plt.bar(data_all.index.astype(str),data_all)\n    plt.title(title)\n    plt.show()\n\ndef plot_boxplot(column, title=\"\"):\n    ax = sns.boxplot(y=column,data=df)\n    plt.show()","4dd485f3":"def eda(df):\n    display(HTML('<h1>Exploratory Data Analysis<h1>'))\n    \n    for column in df.columns:\n        display(HTML('<h2>{}<h2>'.format(column)))\n        if df[column].dtype == 'int64' or df[column].dtype == 'float64':\n            if df[column].nunique()>10 :\n                df[column].describe()\n                plot_hist(column)\n                plot_boxplot(column)\n            else:\n                plot_bar(column)\n                plot_pie(column)\n        elif df[column].dtype == 'object':\n            if df[column].nunique()>10 :\n                df[column].value_counts().head(5)\n            else:\n                plot_bar(column)\n                plot_pie(column)\n        else:\n            None","ca288719":"eda(df)","5106dbdb":"data = df.corr()\nsns.heatmap(data)","ba57b5db":"data = data.sort_values(by='MEDV', ascending=False)\ndata['MEDV']","950af322":"data[(data['MEDV']> -0.4) & (data['MEDV']< 0.4)]['MEDV'].index","de14afca":"X = df.copy()\n\ny = X[target_column]\n\nX = X.drop([target_column,'ZN', 'B', 'DIS', 'CHAS', 'AGE', 'CRIM', 'RAD'], axis=1)","638561d3":"X.info()","51bf5a7d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234)","e45f43b1":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","3750846e":"# Import ML Libraries\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, ElasticNet, BayesianRidge\nfrom lightgbm import LGBMRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\n\nregressors = [[LinearRegression(),'LinearRegression'],[SGDRegressor(),'SGDRegressor'], [ElasticNet(),'ElasticNet'], \n    [BayesianRidge(), 'BayesianRidge'], [LGBMRegressor(),'LGBMRegressor'], [XGBRegressor(),'XGBRegressor'],[CatBoostRegressor(verbose=0),'CatBoostRegressor'],\n              [KernelRidge(),'KernelRidge'],[GradientBoostingRegressor(),'GradientBoostingRegressor'],[SVR(),'SVR'],[AdaBoostRegressor(),\"AdaBoostRegressor\"],[DecisionTreeRegressor(),\"DecisionTreeRegressor\"]]","b6f3c354":"from sklearn import metrics\n\n\nfor rgs in regressors:\n    model = rgs[0]\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    print(rgs[1])\n    print(\"Mean Absolute Error = \", metrics.mean_absolute_error(y_test,y_pred))\n    print(\"Mean Squared Error = \", metrics.mean_squared_error(y_test,y_pred))\n    print(\"Root Mean Squared Error = \", np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\n    print(\"R2 score = \", metrics.r2_score(y_test, y_pred))\n    print(\"\\n\\n\")","55cb7ac0":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.losses import BinaryCrossentropy\nfrom numpy.random import seed\n\nseed(1234)\ntf.random.set_seed(1234)","2d7a353c":"# Initialising the ANN\nmodel = Sequential()\n\n# Adding the input layer and the first hidden layer\nmodel.add(Dense(32, activation = 'relu', input_shape=(X_train.shape[1],)))\n\n\n\n# Adding the second hidden layer\nmodel.add(Dense(units = 32, activation = 'relu'))\n\n\n# Adding the third hidden layer\nmodel.add(Dense(units = 32, activation = 'relu'))\n\n\n\n# Adding the output layer\nmodel.add(Dense(units = 1))","d4149bc3":"opt = Adam(learning_rate=0.001)\nearlystopper = tf.keras.callbacks.EarlyStopping(monitor='loss',mode='min',patience=15, verbose=1,restore_best_weights=True)\nmodel.compile(optimizer = opt, loss = 'mean_squared_error')\nmodel.fit(X_train, y_train, batch_size = 10, epochs = 100, callbacks = [earlystopper])","3f29207e":"y_pred = model.predict(X_test)","f5f6c209":"print(\"Neural Network\")\nprint(\"Mean Absolute Error = \", metrics.mean_absolute_error(y_test,y_pred))\nprint(\"Mean Squared Error = \", metrics.mean_squared_error(y_test,y_pred))\nprint(\"Root Mean Squared Error = \", np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\nprint(\"R2 score = \", metrics.r2_score(y_test, y_pred))","1ab0db90":"Youtube Video :  https:\/\/www.youtube.com\/watch?v=VrC5pSfAKeQ","85c80f9e":"Best Algorithm so far\n* XGBRegressor\n* Mean Absolute Error =  2.1804080626543834\n* Mean Squared Error =  8.616639899537468\n* Root Mean Squared Error =  2.9354113680262035\n* R2 score =  0.9049522583519426"}}