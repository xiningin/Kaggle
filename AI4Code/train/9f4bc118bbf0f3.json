{"cell_type":{"8eb4da89":"code","b32c9e49":"code","e778c84a":"code","284bfe20":"code","b611e522":"code","909541e9":"code","bac5bd46":"code","133e0006":"code","41a44d7f":"code","35564107":"code","b58429d4":"code","10bafca6":"code","720eb930":"code","008571e2":"code","47dd5d00":"code","5a728817":"code","74deb013":"code","6b2f96a2":"code","06037449":"code","d682c97a":"code","c3e291a9":"markdown","6dddd109":"markdown","df2be14c":"markdown","c65665f7":"markdown","bd93b050":"markdown","6155384b":"markdown","11c68b12":"markdown","4e8b4e0f":"markdown","a9721ad2":"markdown","71697817":"markdown","51fe07a8":"markdown"},"source":{"8eb4da89":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\nimport re\nimport pickle\nimport os\nfrom collections import Counter","b32c9e49":"with open('..\/input\/arabic-motivational-quotes\/train.txt', 'r', encoding='utf8') as f:\n    text = f.read()","e778c84a":"# import re\n\n# arabic_punctuations = '''`\u00f7\u00d7\u061b<>_()*&^%][-\u0640\/\",'{}~\u00a6+|\u201d\u2026\u201c\u2013\u0640'''\n# punctuations_list = arabic_punctuations\n\n# arabic_diacritics = re.compile(\"\"\"\n#                              \u0651    | # Tashdid\n#                              \u064e    | # Fatha\n#                              \u064b    | # Tanwin Fath\n#                              \u064f    | # Damma\n#                              \u064c    | # Tanwin Damm\n#                              \u0650    | # Kasra\n#                              \u064d    | # Tanwin Kasr\n#                              \u0652    | # Sukun\n#                              \u0640     # Tatwil\/Kashida\n#                          \"\"\", re.VERBOSE)\n\n# def normalize_arabic(text):\n#     text = re.sub(\"[\u0625\u0623\u0622\u0627]\", \"\u0627\", text)\n#     return text\n# def remove_repeating_char(text):\n#     return re.sub(r'(.)\\1+', r'\\1', text)\n\n# def remove_diacritics(text):\n#     text = re.sub(arabic_diacritics, '', text)\n#     return text\n\n# def remove_punctuations(text):\n#     translator = str.maketrans('', '', punctuations_list)\n#     return text.translate(translator)\n\n# def clean_text(text):    \n#     text = remove_repeating_char(text)\n#     text = normalize_arabic(text)\n#     text = remove_punctuations(text)\n#     text = remove_diacritics(text)\n#     return text","284bfe20":"def create_lookup_tables(text):\n    word_counts = Counter(text)\n    \n    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n    \n    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n\n    return (vocab_to_int, int_to_vocab)\n\n\ndef token_lookup():\n    tokens = dict()\n    tokens['.'] = '<PERIOD>'\n    tokens['\u060c'] = '<COMMA>'\n    tokens['\"'] = '<QUOTATION_MARK>'\n    tokens[':'] = '<SEMICOLON>'\n    tokens['!'] = '<EXCLAMATION_MARK>'\n    tokens['\u061f'] = '<QUESTION_MARK>'\n    tokens['\\n'] = '<NEW_LINE>'\n    return tokens ","b611e522":"SPECIAL_WORDS = {'PADDING': '<PAD>'}\ndef preprocess_and_save_data(dataset_path):   \n    input_file = os.path.join(dataset_path)\n    with open(input_file, 'r', encoding='utf8') as f:\n        text = f.read()\n        \n    # text = clean_text(text)\n    token_dict = token_lookup()\n    for key, token in token_dict.items():\n        text = text.replace(key, ' {} '.format(token))\n\n    text = text.split()\n\n    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n    int_text = [vocab_to_int[word] for word in text]\n    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))","909541e9":"preprocess_and_save_data('..\/input\/arabic-motivational-quotes\/train.txt')","bac5bd46":"def load_preprocess():\n    return pickle.load(open('preprocess.p', mode='rb'))","133e0006":"int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()","41a44d7f":"train_on_gpu = torch.cuda.is_available()","35564107":"from torch.utils.data import TensorDataset, DataLoader\n\ndef batch_data(words, sequence_length, batch_size):\n    n_batches = len(words)\/\/batch_size\n\n    words = words[:n_batches*batch_size]\n    y_len = len(words) - sequence_length\n    x, y = [], []\n    \n    for idx in range(0, y_len):\n        idx_end = sequence_length + idx\n        x_batch = words[idx:idx_end]\n        x.append(x_batch)\n        batch_y =  words[idx_end]\n        y.append(batch_y)    \n  \n    data = TensorDataset(torch.tensor(x), torch.tensor(y))\n   \n    data_loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n    return data_loader    ","b58429d4":"class RNN(nn.Module): \n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5, lr=0.001):\n        super(RNN, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n    \n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n\n        self.fc = nn.Linear(hidden_dim, output_size)\n        \n    def forward(self, nn_input, hidden):\n        batch_size = nn_input.size(0)\n\n        embeds = self.embedding(nn_input)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n  \n        out = self.fc(lstm_out)\n        out = out.view(batch_size, -1, self.output_size)\n    \n        out = out[:, -1]\n\n        return out, hidden\n\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden\n\ndef forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n    if(train_on_gpu):\n        rnn.cuda()\n\n    h = tuple([each.data for each in hidden])\n\n    rnn.zero_grad()\n    \n    if(train_on_gpu):\n        inputs, target = inp.cuda(), target.cuda()\n    else:\n        inputs = inp\n   \n    output, h = rnn(inputs, h)\n\n    loss = criterion(output, target)\n\n    loss.backward()\n    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n\n    optimizer.step()\n    return loss.item(), h","10bafca6":"def train_rnn(rnn, batch_size, optimizer, scheduler, criterion, n_epochs, show_every_n_batches=100):\n    batch_losses = []\n    \n    rnn.train()\n\n    print(\"Training for %d epoch(s)...\" % n_epochs)\n    for epoch_i in range(1, n_epochs + 1):\n        \n        # initialize hidden state\n        hidden = rnn.init_hidden(batch_size)\n        \n        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n            \n            # make sure you iterate over completely full batches, only\n            n_batches = len(train_loader.dataset)\/\/batch_size\n            if(batch_i > n_batches):\n                break\n            \n            # forward, back prop\n            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n            # record loss\n            batch_losses.append(loss)\n\n            # printing loss stats\n            if batch_i % show_every_n_batches == 0:\n                print('Epoch: {:>4}\/{:<4}  Loss: {}\\n'.format(\n                    epoch_i, n_epochs, np.average(batch_losses)))\n                batch_losses = []\n                \n        scheduler.step(loss)\n\n    # returns a trained rnn\n    return rnn","720eb930":"sequence_length =  10\nbatch_size = 32\n\ntrain_loader = batch_data(int_text, sequence_length, batch_size)\n\nnum_epochs = 20\nlearning_rate = 0.001\nvocab_size = len(vocab_to_int)\noutput_size = vocab_size\nembedding_dim = 200\nhidden_dim = 250\nn_layers = 2\n\nshow_every_n_batches = 20","008571e2":"def save_model(filename, decoder):\n    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n    torch.save(decoder, save_filename)","47dd5d00":"rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.3)\nif train_on_gpu:\n    rnn.cuda()\n\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\ncriterion = nn.CrossEntropyLoss()\n\ntrained_rnn = train_rnn(rnn, batch_size, optimizer, scheduler, criterion, num_epochs, show_every_n_batches)\n\nsave_model('.\/save\/trained_rnn', trained_rnn)\nprint('Model Trained and Saved')","5a728817":"def load_model(filename):\n    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n    return torch.load(save_filename)","74deb013":"import torch\n\n_, vocab_to_int, int_to_vocab, token_dict = load_preprocess()\ntrained_rnn = load_model('.\/save\/trained_rnn')","6b2f96a2":"import torch.nn.functional as F\n\ndef generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n    rnn.eval()\n    \n    # create a sequence (batch_size=1) with the prime_id\n    current_seq = np.full((1, sequence_length), pad_value)\n    current_seq[-1][-1] = prime_id\n    predicted = [int_to_vocab[prime_id]]\n    \n    for _ in range(predict_len):\n        if train_on_gpu:\n            current_seq = torch.LongTensor(current_seq).cuda()\n        else:\n            current_seq = torch.LongTensor(current_seq)\n        \n        # initialize the hidden state\n        hidden = rnn.init_hidden(current_seq.size(0))\n        \n        # get the output of the rnn\n        output, _ = rnn(current_seq, hidden)\n        \n        # get the next word probabilities\n        p = F.softmax(output, dim=1).data\n        if(train_on_gpu):\n            p = p.cpu() # move to cpu\n         \n        # use top_k sampling to get the index of the next word\n        top_k = 8\n        p, top_i = p.topk(top_k)\n        top_i = top_i.numpy().squeeze()\n        \n        # select the likely next word index with some element of randomness\n        p = p.numpy().squeeze()\n        word_i = np.random.choice(top_i, p=p\/p.sum())\n        \n        # retrieve that word from the dictionary\n        word = int_to_vocab[word_i]\n        predicted.append(word)     \n        \n        # the generated word becomes the next \"current sequence\" and the cycle can continue\n        current_seq = np.roll(current_seq.cpu(), -1, 1)\n        current_seq[-1][-1] = word_i\n    \n    gen_sentences = ' '.join(predicted)\n    \n    # Replace punctuation tokens\n    for key, token in token_dict.items():\n        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n        gen_sentences = gen_sentences.replace(token, key)\n  \n    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n    gen_sentences = gen_sentences.replace('( ', '(')\n    \n    # return all the sentences\n    return gen_sentences","06037449":"gen_length = 100\nprime_word = \"\u0627\u0644\u0646\u062c\u0627\u062d\" \nsequence_length =  10\ntrain_on_gpu = torch.cuda.is_available()\n\npad_word = '<PAD>'\ngenerated_script = generate(trained_rnn, vocab_to_int[prime_word], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)","d682c97a":"print(generated_script)","c3e291a9":"# Cleaning","6dddd109":"# Train Model","df2be14c":"# Load Data","c65665f7":"# Pre-processing","bd93b050":"# Load Here","6155384b":"# Batching","11c68b12":"# Generate Text","4e8b4e0f":"# Building LSTM","a9721ad2":"## Save processed data","71697817":"## Imports","51fe07a8":"## Load check point"}}