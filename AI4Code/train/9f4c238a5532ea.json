{"cell_type":{"931fea48":"code","9b177e7a":"code","7e4b716d":"code","14dc3b4e":"code","59fe3d58":"code","187f4082":"code","753badd0":"code","aa8eefd4":"code","f7198598":"markdown","cd2e9412":"markdown","a7c0ced7":"markdown"},"source":{"931fea48":"!pip install kaggle-environments","9b177e7a":"from kaggle_environments import evaluate, make, utils\nfrom gym import spaces\nclass ConnectX:\n    DRAW = 0.5\n    WIN = 1\n    LOSE = -1\n    ERROR = -10 \n    \n    def __init__(self, pair=[None, \"random\"], config = {\"rows\": 6, \"columns\": 7, \"inarow\": 4}):\n        self.ks_env = make(\"connectx\", config, debug=True)\n        self.pair = pair\n        self.env = self.ks_env.train(pair)\n        self.config = config\n        # Learn about spaces here: http:\/\/gym.openai.com\/docs\/#spaces\n        self.action_space = spaces.Discrete(config[\"columns\"])\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(config[\"rows\"],config[\"columns\"],1), dtype=np.int)\n\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n        \n    def reset(self):\n        self.obs = self.env.reset()\n        self.obs = self.get_board(self.obs, self.config)\n        return self.obs\n    \n    def switch_pair(self):\n        self.pair = self.pair[::-1]\n        self.env = self.ks_env.train(self.pair)\n        \n    def change_pair(self, pair):\n        self.pair = pair\n        self.env = self.ks_env.train(self.pair)\n        \n    def change_reward(self, reward, done):\n        \n        if done:\n            if reward is None: #Error \n                reward = ConnectX.ERROR\n            elif reward == 1:\n                reward = ConnectX.WIN\n            elif reward == -1:\n                reward = ConnectX.LOSE\n            elif reward == 0:\n                reward = ConnectX.DRAW\n        else:\n            reward = -1\/(self.config['rows'] * self.config['columns'])\n            \n        return reward\n    \n    # get board independent of player number\n    def get_board(self, observation, configuration):\n        rows = configuration['rows']\n        columns = configuration['columns']\n\n        board = np.array(observation['board']).reshape((rows,columns,1))\n        new_board = np.zeros_like(board)\n\n        mark = observation[\"mark\"]\n        new_board[board == mark] = 1\n        new_board[(board != mark) & (board != 0)] = 2\n        return new_board \/ 2 #normalization\n    \n    def step(self, action):\n        if not np.any(self.obs[:, action] == 0):\n            reward, done, _ = ConnectX.ERROR, True, {}\n        else:\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n            self.obs = self.get_board(self.obs, self.config)\n        \n        return self.obs, reward, done, _","7e4b716d":"from matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, SGD\n\nimport numpy as np\nfrom collections import deque\nimport random\n\ntf.keras.backend.set_floatx('float64')\n\nBATCH_SIZE = 64\nDISCOUNT = 0.99\n\nclass ReplayBuffer:\n    def __init__(self, capacity=10000):\n        self.buffer = deque(maxlen=capacity)\n    \n    def put(self, state, action, reward, next_state, done):\n        self.buffer.append([state, action, reward, next_state, done])\n    \n    def sample(self):\n        sample = random.sample(self.buffer, BATCH_SIZE)\n        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\n        return states, actions, rewards, next_states, done\n    \n    def size(self):\n        return len(self.buffer)\n\nclass ActionStateModel:\n    \n    def __init__(self, state_dim, aciton_dim, epsilon = 1.0, eps_decay = 0.999, eps_min = 0.01, lr = 0.001):\n        self.state_dim  = state_dim\n        self.action_dim = aciton_dim\n        \n        self.epsilon = epsilon\n        self.eps_decay = eps_decay\n        self.eps_min = eps_min\n        self.lr = lr\n\n        self.model = self.create_model()\n    \n    def create_model(self):\n        initializer = tf.initializers.VarianceScaling(scale=2.0)\n        model = tf.keras.Sequential([\n            Input(self.state_dim),\n            Flatten(),\n            Dense(100, activation='relu', kernel_regularizer='l2', kernel_initializer=initializer),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(20, activation='relu', kernel_regularizer='l2', kernel_initializer=initializer),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(self.action_dim)\n        ])\n        model.compile(loss='mse', optimizer=Adam(lr=self.lr))\n        return model\n    \n    def predict(self, state):\n        return self.model.predict(state)\n    \n    def get_action(self, state):\n        state = np.reshape(state, [1, *self.state_dim])\n        self.epsilon *= self.eps_decay\n        self.epsilon = max(self.epsilon, self.eps_min)\n        q_value = self.predict(state)[0]\n        if np.random.random() < self.epsilon:\n            return random.randint(0, self.action_dim-1)\n        return np.argmax(q_value)\n\n    def train(self, states, targets):\n        self.model.fit(states, targets, epochs=1, verbose=0)\n    \n\nclass Agent:\n    def __init__(self, env):\n        self.env = env\n        self.state_dim = self.env.observation_space.shape\n        self.action_dim = self.env.action_space.n\n\n        self.model = ActionStateModel(self.state_dim, self.action_dim)\n        self.target_model = ActionStateModel(self.state_dim, self.action_dim)\n        self.target_update()\n\n        self.buffer = ReplayBuffer()\n\n    def target_update(self):\n        weights = self.model.model.get_weights()\n        self.target_model.model.set_weights(weights)\n    \n    def replay(self):\n        for _ in range(10):\n            states, actions, rewards, next_states, done = self.buffer.sample()\n            targets = self.target_model.predict(states)\n            next_q_values = self.target_model.predict(next_states).max(axis=1)\n            targets[range(BATCH_SIZE), actions] = rewards + (1-done) * next_q_values * DISCOUNT \n            self.model.train(states, targets)\n\n    def _print_statistics(self, rewards):\n        rewards = np.array(rewards)\n        print(\"Wins:\", (rewards == ConnectX.WIN).sum())\n        print(\"Loses:\", (rewards == ConnectX.LOSE).sum())\n        print(\"Errors:\", (rewards == ConnectX.ERROR).sum())\n        \n    def train(self, episodes=50, every = 5, switch = False):\n        rewards = []\n        total_rewards = []\n        for ep in range(episodes):\n            done, total_reward = False, 0\n            state = self.env.reset()\n            while not done:\n                action = self.model.get_action(state)\n                next_state, reward, done, _ = self.env.step(action)\n                \n                self.buffer.put(state, action, reward, next_state, done)\n                #put symmetrical state\n                self.buffer.put(state[:, ::-1, :], agent.action_dim -1 - action, reward, next_state[:, ::-1, :], done)\n                total_reward += reward\n                state = next_state\n            \n            if self.buffer.size() >= BATCH_SIZE:\n                self.replay()\n            self.target_update()\n            \n            if (ep + 1) % 5 == 0:\n                print('EP {} EpisodeReward={}'.format(ep + 1, total_reward))\n            total_rewards.append(total_reward)\n            rewards.append(reward)\n            \n            if (ep + 1) % every == 0:\n                if switch:\n                    self.env.switch_pair()\n                self._print_statistics(rewards)\n                rewards = []\n        \n        return total_rewards\n    \ndef plot(x, h = 100):\n    plt.plot(np.convolve(x, np.ones(h), 'valid')\/h)\n    plt.xlabel('Episode')\n    plt.ylabel('Rewards')\n    plt.show()","14dc3b4e":"# Here I experiment with only Connect 3.\nenv = ConnectX(pair = [\"random\", None ], config = {\"rows\": 4, \"columns\": 5, \"inarow\": 3})\nagent = Agent(env)","59fe3d58":"total_rewards = agent.train(episodes = 7000, every = 100, switch = True)\nplot(total_rewards, 100)","187f4082":"# Plot last 500 games\nplot(total_rewards[-500:], 30)","753badd0":"agent.model.epsilon = 1.0\nenv.change_pair([None, \"negamax\"])\ntotal_rewards = agent.train(episodes= 7000, every = 100, switch = True)\nplot(total_rewards, 100)","aa8eefd4":"# Plot last 500 games\nplot(total_rewards[-500:], 30)","f7198598":"# Deep Q-Network  \n  \nThis implementation I took from [marload repository](https:\/\/github.com\/marload\/DeepRL-TensorFlow2\/blob\/master\/DQN\/DQN_Discrete.py) and fit to ConnectX game.","cd2e9412":"A little change ConnectX environment","a7c0ced7":"# Conclusion  \nAs you can see, dueling double DQN doesn't solve Connect 3 game. And I don't have idea why. I also tried usual dueling double DQN and PPO algorithm with different model architectures, but get the same result.  \n\nLeave comment and write, where I can have a mistake. \n\nMy other notebooks:  \n1) [Dueling double DQN](https:\/\/www.kaggle.com\/masurte\/dueling-double-dqn)   \n2) [Usual DQN, my own implementation](https:\/\/www.kaggle.com\/masurte\/deep-q-learning-implementation)  \n3) [PPO](https:\/\/www.kaggle.com\/masurte\/ppo-algorithm) "}}