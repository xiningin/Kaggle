{"cell_type":{"a6a9e3c3":"code","16d9e86a":"code","e27db093":"code","6f0a8bd3":"code","f4fd344a":"code","d9897fdd":"code","08b0563d":"code","aa51c45a":"code","ad3bb814":"markdown","40ee2085":"markdown","fb81a77a":"markdown","a0e03c8b":"markdown","23252143":"markdown","5056c709":"markdown","edf2b23d":"markdown"},"source":{"a6a9e3c3":"#import modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n#we will use regular expression for passenger names\nimport re \nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split\n\n#import data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n#we put both data frames in a list to modify all data easily\ndata = [train, test]\n\n#data quick check\nprint(train.head())\nprint(train.info())\nprint('\\n', test.head())\nprint(test.info())","16d9e86a":"#check the effect of passenger class on survival rate\nprint('Survival rate depending passenger class:')\nprint(train[['Pclass', 'Survived']].groupby('Pclass', as_index=False).mean(), '\\n')\n\n#check the effect of passenger sex on survival rate\nprint('Survival rate depending on passenger sex:')\nprint(train[['Sex', 'Survived']].groupby('Sex', as_index=False).mean(), '\\n')\n\n#check the effect of number of siblings and spouses on survival rate\nprint('Survival rate depending on number of siblings and spouses:')\nprint(train[['SibSp', 'Survived']].groupby('SibSp', as_index=False).mean(), '\\n')\n\n#check the effect of number of parents and children on survival rate\nprint('Survival rate depending on number of parents and children:')\nprint(train[['Parch', 'Survived']].groupby('Parch', as_index=False).mean(), '\\n')","e27db093":"#fill missing Age values\nfor dataset in data:\n    #calculate mean, standard deviation and number of missing values\n    avg = dataset['Age'].mean()\n    std = dataset['Age'].std()\n    null_count = dataset['Age'].isnull().sum()\n    \n    #generate random ages centered around the mean\n    random_list = np.random.randint(avg - std, avg + std, size=null_count)\n    \n    #replace missing values with random ages\n    dataset['Age'][np.isnan(dataset['Age'])] = random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n\n#group Age values into 5 categories and check effect on survival\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\nage_survival = train[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean()\n\n#display barplot of results\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 10))\nax1.bar([0, 1, 2, 3, 4], age_survival['Survived'])\nax1.tick_params(axis='x', color='white')\nax1.set_xticks([0, 1, 2, 3, 4])\nax1.set_xticklabels(list(age_survival['CategoricalAge'].astype(str)), fontsize=15)\nax1.set_yticklabels(labels=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6], fontsize=16)\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax1.set_ylim(0, .6)\nax1.set_xlabel(\"Age categories (years)\", fontsize=18)\nax1.set_ylabel(\"Survival rate\", fontsize=18)\nax1.set_title(\"Impact of passenger age on survival\", fontsize=18)\n\n\n#fill missing \"Embarked\" values and show effect on survival\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\nembarked_survival = train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean()\nax2.bar([0, 1, 2], embarked_survival['Survived'])\nax2.set_xticks([0, 1, 2])\nax2.tick_params(axis='x', color='white')\nax2.set_xticklabels(['Cherbourg', 'Queenstown', 'Southampton'], fontsize=16)\nax2.set_yticklabels(labels=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6], fontsize=16)\nax2.spines['top'].set_visible(False)\nax2.spines['right'].set_visible(False)\nax2.spines['bottom'].set_visible(False)\nax2.set_ylim(0, .6)\nax2.set_xlabel(\"Boarding location\", fontsize=18)\nax2.set_ylabel(\"Survival rate\", fontsize=18)\nax2.set_title(\"Impact of boarding location on survival\", fontsize=18)\nplt.show()","6f0a8bd3":"#put fare in categories and check effect on survival\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\nfare_survival = train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean()\n\n#engineer a feature indicating if a passenger travels alone\nfor dataset in data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\nfor dataset in data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\nalone_survival = train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()\n\n#engineer a feature containing titles\n#function to get title from name\ndef extract_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return ''\n\n#extract titles from name and put them in a new feature\nfor dataset in data:\n    dataset['Title'] = dataset['Name'].apply(extract_title)\n\n#replace rarer titles by \"Rare\"\nfor dataset in data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Jonkheer',\n                                                'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\ntitle_survival = train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n\n#plot data for the engineered features\nfig, ax = plt.subplots(figsize=(10, 8))\nax.bar([0, 1, 2, 3], fare_survival['Survived'])\nax.tick_params(axis='x', color='white')\nax.set_xticks([0, 1, 2, 3])\nax.set_xticklabels(list(fare_survival['CategoricalFare'].astype(str)), fontsize=15)\nax.set_yticklabels(labels=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6], fontsize=16)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.set_ylim(0, .6)\nax.set_xlabel(\"Fare category\", fontsize=18)\nax.set_ylabel(\"Survival rate\", fontsize=18)\nax.set_title(\"Impact of fare on survival\", fontsize=18)\nplt.show()\n\nfig, ax = plt.subplots()\nax.bar([0, 1], alone_survival['Survived'])\nax.tick_params(axis='x', color='white')\nax.set_xticks([0, 1])\nax.set_xticklabels(['Not alone', 'Alone'], fontsize=15)\nax.set_yticklabels(labels=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6], fontsize=16)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.set_ylim(0, .6)\nax.set_ylabel(\"Survival rate\", fontsize=18)\nax.set_title(\"Impact of being alone on survival\", fontsize=18)\nplt.show()\n\nfig, ax = plt.subplots(figsize=(10, 8))\nax.bar([0, 1, 2, 3, 4], title_survival['Survived'])\nax.tick_params(axis='x', color='white')\nax.set_xticklabels([''] + list(title_survival['Title']), fontsize=15)\nax.set_yticklabels(labels=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], fontsize=16)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.set_ylim(0, 0.9)\nax.set_xlabel(\"Title\", fontsize=18)\nax.set_ylabel(\"Survival rate\", fontsize=18)\nax.set_title(\"Impact of title on survival\", fontsize=18)\nplt.show()\n","f4fd344a":"for dataset in data:\n    \n    #mapping sex\n    dataset['Sex'] = dataset['Sex'].map({'female':0, 'male':1})\n    \n    #mapping titles\n    title_mapping = {'Mr':1, 'Miss':2, 'Mrs':3, 'Master':3, 'Rare':5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    #mapping embarked\n    dataset['Embarked'] = dataset['Embarked'].map({'S':0, 'C':1, 'Q':2})\n    \n    #mapping fare\n    dataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].median())\n    dataset.loc[dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2\n    dataset.loc[dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    #mapping age\n    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[dataset['Age'] > 64, 'Age'] = 4\n    \n#feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp','Parch', 'FamilySize'] \ntrain = train.drop(drop_elements, axis=1)\ntrain = train.drop(['CategoricalFare', 'CategoricalAge'], axis=1)\ntest_no_id = test.drop(drop_elements, axis=1) #without passenger id\n\nprint('Training dataset after feature engineering:\\n', train.head(), '\\n')\nprint('Testing dataset after feature engineering:\\n', test.head())","d9897fdd":"#prepare data\n#X are features\nX = train.iloc[:, 1:]\n\n#Y are targets\nY = train.iloc[:, 0]\n\n#split train data into training and cross-validation datasets\nX_train, X_cv, Y_train, Y_cv = train_test_split(X, Y, test_size=0.25)\n\n#perform grid search to find the best parameters\n\n#loop over grid of parameters\ndef grid_search():\n    #list of parameters\n    C_values = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]\n    gamma_values = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]\n    kernels = ['linear', 'rbf', 'sigmoid']\n    \n    #variables to store the results\n    best_score = 0\n    best_C = None\n    best_gamma = None\n    best_kernel = None\n    \n    for C in C_values:\n        for gamma in gamma_values:\n            for kernel in kernels:\n                svc = svm.SVC(C=C, gamma=gamma, kernel=kernel)\n                svc.fit(X_train, Y_train)\n                score = svc.score(X_cv, Y_cv)\n                \n                if score > best_score:\n                    best_score = score\n                    best_C = C\n                    best_gamma = gamma\n                    best_kernel = kernel\n                    \n    print('Best parameters give {0:.4%} accuracy'.format(best_score))\n    print('C = {0}\\ngamma = {1}\\nKernel = {2}'.format(best_C, best_gamma, best_kernel))\n    \n    return best_C, best_gamma, best_kernel","08b0563d":"#perform grid search\nC, gamma, kernel = grid_search()","aa51c45a":"svc = svm.SVC(C=C, gamma=gamma, kernel=kernel, probability=True)\nsvc.fit(X, Y)\nprediction = svc.predict(test_no_id)[:, np.newaxis]\nprediction = np.hstack((test['PassengerId'][:, np.newaxis], prediction))\noutput = pd.DataFrame({'PassengerId':prediction[:, 0], 'Survived':prediction[:, 1]})\nprint(output.head())\nnp.savetxt('submission.csv', output, header='PassengerId,Survived', comments='', delimiter=',', fmt='%d')","ad3bb814":"## Feature cleaning\nWe now have a bunch of features and we need to map them to numerical values. Then, we will remove the features we will not use to train the machine learning model.","40ee2085":"## Prediction of passenger survival using support vector machines\nI will not comment much on the particular choice of support vector machines (SVM) for the titanic dataset. Compared to other machine learning algorithms, I find them theoretically simple, fast to train, and easy to tune, but you may disagree with me, so I encourage you to write your thoughts in the comments. First, we will split data into training and cross-validation datasets. Then we will perform a grid search to select the best C-value, gamma-value and kernel type. I ommited polynomial kernels because it took a bit too much time to train many SVM models based on polynomial kernels.","fb81a77a":"# Titanic passenger survival prediction with feature engineering and support vector machines\n## Introduction\nIn this notebook, I present a simple way to generate (relatively) accurate predictions of titanic passenger survival using feature engineering and a support vector machines (SVM) model. The feature engineering part is based on the [greate notebook written by Sina](https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier). We will use Scikit-learn to implement the SVM model, and perform a simple grid search to find close-to-optimal model parameters. Let's start by importing the useful modules for our task, along with the data.","a0e03c8b":"Now we can use the best SVM model parameters to train the model and predict survival of passengers.","23252143":"## Predictive impact of features\nBy having a quick look at the data, we see everything was imported properly with `train.head()` and `test.head()` which prints the top of the dataframe, and also that they have a relevant type with `train.info()` and `test.info()`. However, some data are missing (e.g.,  in the \"Age\", \"Cabin\" and \"Embarked\" features for the \"train\" dataframe) and we will have to deal with this.\n\nWe can check the impact of features on survival by grouping the data according to a particular feature and survival before calculating the mean survival for each group in the feature of interest. Note that we use *fancy indexing* of the dataframe to extract two columns (see [this](https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/03.02-data-indexing-and-selection.html) for more explanations) and the method`groupby(..., as_index=False)` to prevent the group labels from becoming an index (it's a little detail but I think it makes the result more readable).","5056c709":"We can see that all the features analyzed so far have a significant impact on survival rate, especially passenger sex. This is because the traditional evacuation rule \"women and children first\". Before verifying this for age, we need to fill missing values for this feature, since it is available for only 714 passengers out of 891 in the \"train\" dataset.\n\n## Filling missing values\nThere are [several strategies to deal with missing values](https:\/\/towardsdatascience.com\/handling-missing-values-in-machine-learning-part-1-dda69d4f88ca) and I chose to fill them with random values between mean - standard deviation and mean + standard deviation. As for where passengers embarked, we will fill the two missing values with one of the labels, for example 'S'. We will simply ignore the \"Cabin\" feature because a lot of values are missing for cabin, and this feature is unlikely to generalize well in terms of survival prediction because few passengers occupy each cabin.","edf2b23d":"The impact of the \"Age\" feature on survival confirms that children and teenagers have the highest rate of survival, as suggested earlier. Somehow, the port where passenger boarded also impacts survival.\n\nNow we do not have missing values anymore, but we still can do a little to engineer new features. We still have to work on the \"Fare\" feature and we will also engineer features related to wether passengers travelled alone or not.\n\n## Feature engineering\nWe already engineered a feature containing age categories. Let's do the same on \"Fare\". Then, we will create a feature indicating if a given passenger travelled alone or not, based on \"Parch\" (if a passenger travels with parents or children) and \"SibSp\" (if a passenger travelled with siblings or spouses). Finally, we have the names of passengers, which are not easy to work with in machine learning because they are strings of characters and they are almost unique to every passenger. However, we can extract the *title* of passengers (Mr., Mrs., Dr., etc.) using regular expressions and use it as a new feature. A few titles are rarer than others (e.g. Lady *versus* Mrs.) so we will group them together."}}