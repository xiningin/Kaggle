{"cell_type":{"bd3f8941":"code","b4b26e2a":"code","cf6675ee":"code","588bf974":"code","7a7dbe5e":"code","7c54ea1d":"code","a8a39675":"code","33c54f52":"code","16eb8fd3":"code","687702ca":"code","f46e589b":"code","bef7cf57":"code","de37c4e1":"code","f37b8dfc":"code","c80eff18":"code","9aa9cebd":"code","e0bbc550":"code","c9f30ffa":"code","5388718d":"code","ec4f81ce":"code","2fd79b80":"code","741d9075":"code","f96be955":"code","07971a1c":"code","4546d820":"code","159ea398":"code","7f716210":"code","ad57d63f":"code","d6ea85d1":"code","2fa42b44":"code","555f7670":"code","99291241":"code","da0bee9d":"code","0ac932b7":"code","c3a80c51":"code","dbf79f43":"code","897eb6bc":"code","451edcde":"code","7404f14a":"code","c079604a":"code","c43975ac":"code","aa477192":"code","a90bc0fa":"code","d5e7fb27":"markdown","a04977ed":"markdown","38746c63":"markdown","fbfb7508":"markdown","b524e289":"markdown","ddd85c78":"markdown","22a81c73":"markdown","010e486c":"markdown","09c015d3":"markdown","05059990":"markdown","b4783cdf":"markdown","7aae3653":"markdown","3cc1386d":"markdown","3f57120c":"markdown","2c78f52e":"markdown","e8f1a5e7":"markdown","e40db032":"markdown","5f399b0c":"markdown","0b0f2738":"markdown","2c134165":"markdown"},"source":{"bd3f8941":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport os, random\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b4b26e2a":"train = pd.read_csv(\"\/kaggle\/input\/birdsong-recognition\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/birdsong-recognition\/test.csv\")","cf6675ee":"train.head()","588bf974":"train.columns","7a7dbe5e":"# Bird species:\nprint(\"Bird species:\\n {}\".format(train.ebird_code.unique()))","7c54ea1d":"# Bird species:\nprint(\"No. of Unique bird species:\\n {}\".format(len(train.ebird_code.unique())))","a8a39675":"# Distribution of categories\nax = train.groupby(['country']).ebird_code.nunique().sort_values(ascending=False)[:6].plot.bar(figsize=(10,10),\n                                                                                         rot=0,\n                                                                                         title=\"Top 5 Countries with most number of unique species\")\n\nfor p in ax.patches:\n    ax.annotate(np.round(p.get_height(),decimals=2), (p.get_x()+p.get_width()\/2., p.get_height()), ha='center', va='center',                              xytext=(0, 10),                               textcoords='offset points')","33c54f52":"sns.set(style=\"whitegrid\")\nax = sns.violinplot(x=train['duration'])","16eb8fd3":"ax = sns.kdeplot(train['duration'])","687702ca":"print(\"Mean duration: {}\".format(np.mean(train['duration'])))\nprint(\"Median duration: {}\".format(np.median(train['duration'])))\nprint(\"Min duration: {}\".format(np.min(train['duration'])))\nprint(\"Max duration: {}\".format(np.max(train['duration'])))","f46e589b":"# List out the bird species:\ntrain_folder_path = '\/kaggle\/input\/birdsong-recognition\/train_audio\/'","bef7cf57":"import IPython.display as ipd\nsample_file = train_folder_path + 'amebit\/' + 'XC310046.mp3'\nprint(sample_file)\nipd.Audio(sample_file)","de37c4e1":"train['sampling_rate'].unique()","f37b8dfc":"train.isnull().sum()","c80eff18":"import librosa\n\nclip, sample_rate = librosa.load(sample_file, sr=None)\nclip = clip[:132300] # first three seconds of file","9aa9cebd":"clip","e0bbc550":"# From https:\/\/www.kaggle.com\/cwthompson\/birdsong-making-a-prediction\n'''\nLoading Audio\nFirstly, we need to be able to read in five second windows of the test audio. We can do this using librosa. \nIf the audio is from site 3 then we need to whole audio clip, and we can do this by setting duration to None.\n'''\n\ndef load_test_clip(path, start_time, duration=5):\n    return librosa.load(path, offset=start_time, duration=duration)[0]","c9f30ffa":"import librosa, librosa.display\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = (10, 10)\n\nSOUND_DIR= train_folder_path + 'amebit\/' + 'XC310046.mp3'\n\n# Generate waveform plot\nsignal, sr = librosa.load(SOUND_DIR, sr= 22050)\nlibrosa.display.waveplot(signal, sr = sr)\nplt.title(\"Waveform:\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Amplitude\")\nplt.show()","5388718d":"# fft -> spectrum\nfft = np.fft.fft(signal)\nmagnitude = np.abs(fft)\nfrequency = np.linspace(0, sr, len(magnitude))\nleft_frequency  = frequency[:int(len(frequency)\/2)]\nleft_magnitude  = magnitude[:int(len(frequency)\/2)]\nplt.plot(left_frequency, left_magnitude)\nplt.title(\"Fast Fourier Transform:\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Amplitude\")\nplt.show()","ec4f81ce":"# stft -> spectrogram\n\n# Number of samples per FFT\nn_fft = 2048\n\n# Amount we are shifting each FFT to the right\nhop_length = 512\n\nstft = librosa.core.stft(signal, hop_length = hop_length, n_fft = n_fft)\n\nspectrogram = np.abs(stft)\n\nlog_spectrogram = librosa.amplitude_to_db(spectrogram)\n\nlibrosa.display.specshow(log_spectrogram, sr=sr, hop_length = hop_length)\nplt.title(\"Spectrogram using STFT:\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Frequency\")\nplt.colorbar()\nplt.show()","2fd79b80":"#MFCCs:\nmfccs = librosa.feature.mfcc(signal, n_fft = n_fft, hop_length = hop_length, n_mfcc=13)\nlibrosa.display.specshow(mfccs, sr=sr, hop_length = hop_length)\nplt.title(\"Spectrogram using MFCC:\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"MFCC\")\nplt.colorbar()\nplt.show()","741d9075":"import librosa\nimport cv2\n#from https:\/\/www.kaggle.com\/daisukelab\/creating-fat2019-preprocessed-data\ndef mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n    # Stack X as [X,X,X]\n#     X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    X = X - mean\n    std = std or X.std()\n    Xstd = X \/ (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    norm_max = norm_max or _max\n    norm_min = norm_min or _min\n    if (_max - _min) > eps:\n        # Normalize to [0, 255]\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255 * (V - norm_min) \/ (norm_max - norm_min)\n        V = V.astype(np.uint8)\n    else:\n        # Just zero\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n\ndef build_spectrogram(path):\n    y, sr = librosa.load(path)\n    total_secs = y.shape[0] \/ sr\n    M = librosa.feature.melspectrogram(y=y, sr=sr)\n    M = librosa.power_to_db(M)\n    M = mono_to_color(M)\n    \n    cv2.imwrite(path.split(\"\/\")[-1][:-4] + \".jpg\", M, [int(cv2.IMWRITE_JPEG_QUALITY), 85])\n    M = cv2.imread(path.split(\"\/\")[-1][:-4] + \".jpg\", 0)\n    M = np.repeat(M[...,None],3,2)\/255.\n    os.remove(path.split(\"\/\")[-1][:-4] + \".jpg\")\n    return M","f96be955":"M = build_spectrogram(\"..\/input\/birdsong-recognition\/example_test_audio\/BLKFR-10-CPL_20190611_093000.pt540.mp3\")","07971a1c":"plt.imshow(M[:, :500])","4546d820":"import cv2\nimport matplotlib.pyplot as plt\nbase_path = \"..\/input\/birdsongspectrograms\/\"\ndef read_img(img_path):\n    img = cv2.imread(base_path + img_path[:-3] + \"jpg\", 0)\n    return img","159ea398":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit_transform(train[\"ebird_code\"])","7f716210":"import joblib\njoblib.dump(le, 'birdcalls_label_encoder.pkl')","ad57d63f":"files = [file[:-3] + \"mp3\" for file in os.listdir(base_path)]","d6ea85d1":"train = train[train[\"filename\"].isin(files)]","2fa42b44":"!pip install efficientnet_pytorch\n","555f7670":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","99291241":"from fastai import *\nfrom fastai.vision import *\nfrom efficientnet_pytorch import *\nimport pandas as pd\nimport matplotlib.pyplot as plt","da0bee9d":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 999\nseed_everything(SEED)","0ac932b7":"print('Make sure cuda is installed:', torch.cuda.is_available())\nprint('Make sure cudnn is enabled:', torch.backends.cudnn.enabled)","c3a80c51":"bs = 12 #smaller batch size is better for training, but may take longer\n\nsz = 256","dbf79f43":"### t2 = train.copy()\ntrain_dir = os.path.join('..\/', 'input\/birdsongspectrograms\/')\nt3 = train[['filename', 'ebird_code']]\nt3['ebird_code'] = t3['ebird_code'].apply(lambda x: le.transform([x])[0])\nt3['path'] = t3['filename'].map(lambda x: os.path.join(train_dir,'{}.jpg'.format(x[:-4])))\nt3 = t3.drop(columns=['filename'])","897eb6bc":"from fastai.callbacks.hooks import *\n\nsrc = (ImageList.from_df(df=t3,path = '',cols='path')\n       .split_by_rand_pct(0.2) #split the dataset such that we have 20% as validation set\n       .label_from_df(cols='ebird_code',label_cls=CategoryList))\n\ndata= (src.transform(tfms=None, size=sz, padding_mode='zeros',\n                     resize_method=ResizeMethod.SQUISH).databunch(bs=bs,num_workers=4).normalize(imagenet_stats))","451edcde":"data.show_batch(rows=3, figsize=(5,5))","7404f14a":"data.one_item","c079604a":"model_name = 'efficientnet-b3'\n\ndef get_model(pretrained=True):\n    model = EfficientNet.from_pretrained(model_name, num_classes=data.c)\n    return model\n\n\n# F1 = MultiLabelFbeta(beta=2, average=\"macro\")\nlearn = Learner(data, get_model(True), metrics = [top_k_accuracy]).mixup().to_fp16()\nlearn.model_dir = \"\/kaggle\/working\"","c43975ac":"learn.unfreeze()","aa477192":"from fastai.callbacks import * \n\nlearn.fit_one_cycle(1,1e-3, callbacks=[SaveModelCallback(learn, every='improvement', \n                                                 monitor='valid_loss', name='best_saved')])","a90bc0fa":"learn.save('fastai_effnetb3', return_path=True)","d5e7fb27":"## Reading Audio as numpy array\n---\n\nTo get started, we can read in each audio file as a numpy array. This can be done using the <ins>librosa<\/ins> library: https:\/\/librosa.github.io\/librosa\/","a04977ed":"Reference: https:\/\/www.kaggle.com\/ryches\/birdsong-keras-starter\/notebook?scriptVersionId=36635693","38746c63":"**fastai model:**\n---","fbfb7508":"### Mel-Frequency Cepstral Coefficients (MFCC)\n---\n\n* Essentially captures timbral\/textural aspects of sound\n* Frequency domain of sound\n* Aproximates human auditory system\n* The main difference beween a spectrogram and MFCC is that a spectrogram uses a linear spaced frequency scale (so each frequency bin is spaced an equal number of Hertz apart), whereas an MFCC uses a quasi-logarithmic spaced frequency scale, which is more similar to how the human auditory system processes sounds.\n\n\n\nFrom Wikipedia:\nIn sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.\n\nMel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC\n\n\n\nFor more information: https:\/\/en.wikipedia.org\/wiki\/Mel-frequency_cepstrum\n\nIllustration showing differences in audio processing techniques:\n\n<img src=\"https:\/\/miro.medium.com\/max\/996\/1*H_lcPB14mWwXrbN_ibIeGw.jpeg\" alt=\"drawing\" height=\"300\" width=\"650\" \/>","b524e289":"## Basics of sound\n\n### What is sound?\n---\n\n* Sound is a vibration that propagates as an acoustic wave. This is due to alternations in pressure as a result of vibrations of air molecules\n\n![image.png](attachment:image.png)\n\n\n### Analog Digital Conversion (ADC)\n---\n\n* Signal sampled at uniform time intervals\n* Amplitude quantized with limited no. of bits\n\n### Sampling Rate & Bit Depth\n---\n\n* Sampling_rate: Number of amplitude points taken in a second\n* Bit Depth: the number of bits of information in each sample, and it directly corresponds to the resolution of each sample","ddd85c78":"## Missing Values\n---\n\nWe need to check if there are any missing values and if we need to perform any imputation","22a81c73":"Looks like a lot of species of birds have audio clips gathered in the US","010e486c":"<ins>**Short Time Fast Fourier Transform:**<\/ins>\n* Computes several FFT at different intervals\n* Preserves time information\n* Fixed frame size (eg 2048 samples)\n* Gives a spectrogram (time + frequency + magnitude)\n\n\n<ins>**Example of Short Time Fast Fourier Transform:**<\/ins>\n![image.png](attachment:image.png)","09c015d3":"### Fourier Transform\n---\n* Decompose complex periodic sound into sum of sine waves oscillating at different frequencies\n* When performing FT, we move from time to frequency domain\n\n<ins>**Fast Fourier Transform:**<\/ins>\n* The <ins>**Fast Fourier transform (FFT)**<\/ins> is an algorithm to compute the periodic structure in the signal, but representing it by a set of sine waves, called frequencies. Plotting the power of each one of those frequencies gives the Power Spectrum:\n\n<ins>**Example of Fast Fourier Transform:**<\/ins>\n![image.png](attachment:image.png)","05059990":"Example of clip with very very short recording:","b4783cdf":"Before we dive into modelling, there is 2 components we need to understand\n\n1. Structure of submissions (We need to understand how submissions work so we can train our model in the same structure)\n2. Concepts of audio processing","7aae3653":"### Duration of clips:\n---","3cc1386d":"<ins>2. DL Pipeline:<\/ins>\n\n![image.png](attachment:image.png)\n","3f57120c":"### Pipelines for sound classification\n---\n\n<ins>*1. Traditional Machine Learning Pipeline:*<\/ins>\n\n![image.png](attachment:image.png)\n\n* Feature engineering\n* Performing STFT\n* Extract Time + frequency domain features","2c78f52e":"## Structure of submissions\n---\nWhat we need to do in the inference phase is\n\n* Read test.csv\n* Repeat 3 - 5 for each row in test.csv\n* Open test audio file which is contained in \/kaggle\/input\/birdsong-recognition\/test_audio and named like audio_id.mp3 .\n* If site is site_3 use the whole clip, otherwise cut 5 seconds short clip out of the long audio clip which we loaded in step3 that ends in the second specified in seconds column.\n* Perform prediction on the short clip which we prepared in step 4. Note that each clip may have multiple labels.\n\nSee: https:\/\/www.kaggle.com\/c\/birdsong-recognition\/discussion\/158987\n\nThe important point to take note is that in our training phase, we need to account for different durations of audio clips depending on the site the clip is extracted from is on.\n\nBelow is an example of a function that can help us to read in the clips:","e8f1a5e7":"## Acknowledgements:\n---\nhttps:\/\/www.kaggle.com\/fizzbuzz\/beginner-s-guide-to-audio-data  \nhttps:\/\/www.youtube.com\/watch?v=m3XbqfIij_Y\n\nBird Spectrograms from: https:\/\/www.kaggle.com\/ryches\/birdsongspectrograms","e40db032":"* ### Building Spectrograms\n---","5f399b0c":"Duration of our clips vary widely from < 1 s to ~55 s","0b0f2738":"## Plan for modelling\n---\n\nTo try RNN","2c134165":"## Exploratory Data Analysis\n---\n\nExploring categories of bird species and their distributions"}}