{"cell_type":{"ef71c5eb":"code","2d82a5dc":"code","749c413c":"code","c329a9de":"code","9ab93e45":"code","ed787a25":"code","e2425e91":"code","0838784e":"code","619b2b87":"code","8b9e498a":"code","fcd92ed6":"code","149fae25":"code","1ce3660e":"markdown","2fbba602":"markdown","63319d52":"markdown","6a61e25b":"markdown","9f342e75":"markdown","a0f2ba71":"markdown","95a0b71b":"markdown","d44062ba":"markdown","cf546d5b":"markdown","2c563197":"markdown","b01541eb":"markdown","d8526b50":"markdown"},"source":{"ef71c5eb":"# !pip install fairseq","2d82a5dc":"# import torch.nn as nn\n# from fairseq import utils\n# from fairseq.models import FairseqEncoder\n\n# class SimpleLSTMEncoder(FairseqEncoder):\n#     def __init__(\n#         self, args, dictionary, embed_dim=128, hidden_dim=128, dropout=0.1,\n#     ):\n#         super().__init__(dictionary)\n#         self.args = args\n#         # Our encoder will embed the inputs before feeding them to the LSTM.\n#         self.embed_tokens = nn.Embedding(\n#             num_embeddings=len(dictionary),\n#             embedding_dim=embed_dim,\n#             padding_idx=dictionary.pad(),\n#         )\n#         self.dropout = nn.Dropout(p=dropout)\n#         # We'll use a single-layer, unidirectional LSTM for simplicity.\n#         self.lstm = nn.LSTM(\n#             input_size=embed_dim,\n#             hidden_size=hidden_dim,\n#             num_layers=1,\n#             bidirectional=False,\n#         )\n#     def forward(self, src_tokens, src_lengths):\n#         # The inputs to the ``forward()`` function are determined by the\n#         # Task, and in particular the ``'net_input'`` key in each\n#         # mini-batch. We discuss Tasks in the next tutorial, but for now just\n#         # know that *src_tokens* has shape `(batch, src_len)` and *src_lengths*\n#         # has shape `(batch)`.\n#         # Note that the source is typically padded on the left. This can be\n#         # configured by adding the `--left-pad-source \"False\"` command-line\n#         # argument, but here we'll make the Encoder handle either kind of\n#         # padding by converting everything to be right-padded.\n#         if self.args.left_pad_source:\n#             # Convert left-padding to right-padding.\n#             src_tokens = utils.convert_padding_direction(\n#                 src_tokens,\n#                 padding_idx=self.dictionary.pad(),\n#                 left_to_right=True\n#             )\n#         # Embed the source.\n#         x = self.embed_tokens(src_tokens)\n#         # Apply dropout.\n#         x = self.dropout(x)\n#         # Pack the sequence into a PackedSequence object to feed to the LSTM.\n#         x = nn.utils.rnn.pack_padded_sequence(x, src_lengths, batch_first=True)\n#         # Get the output from the LSTM.\n#         _outputs, (final_hidden, _final_cell) = self.lstm(x)\n#         # Return the Encoder's output. This can be any object and will be\n#         # passed directly to the Decoder.\n#         return {\n#             # this will have shape `(bsz, hidden_dim)`\n#             'final_hidden': final_hidden.squeeze(0),\n#         }\n#     # Encoders are required to implement this method so that we can rearrange\n#     # the order of the batch elements during inference (e.g., beam search).\n#     def reorder_encoder_out(self, encoder_out, new_order):\n#         \"\"\"\n#         Reorder encoder output according to `new_order`.\n#         Args:\n#             encoder_out: output from the ``forward()`` method\n#             new_order (LongTensor): desired order\n#         Returns:\n#             `encoder_out` rearranged according to `new_order`\n#         \"\"\"\n#         final_hidden = encoder_out['final_hidden']\n#         return {\n#             'final_hidden': final_hidden.index_select(0, new_order),\n#         }","749c413c":"# import torch\n# from fairseq.models import FairseqDecoder\n# class SimpleLSTMDecoder(FairseqDecoder):\n#     def __init__(\n#         self, dictionary, encoder_hidden_dim=128, embed_dim=128, hidden_dim=128,\n#         dropout=0.1,\n#     ):\n#         super().__init__(dictionary)\n#         # Our decoder will embed the inputs before feeding them to the LSTM.\n#         self.embed_tokens = nn.Embedding(\n#             num_embeddings=len(dictionary),\n#             embedding_dim=embed_dim,\n#             padding_idx=dictionary.pad(),\n#         )\n#         self.dropout = nn.Dropout(p=dropout)\n#         # We'll use a single-layer, unidirectional LSTM for simplicity.\n#         self.lstm = nn.LSTM(\n#             # For the first layer we'll concatenate the Encoder's final hidden\n#             # state with the embedded target tokens.\n#             input_size=encoder_hidden_dim + embed_dim,\n#             hidden_size=hidden_dim,\n#             num_layers=1,\n#             bidirectional=False,\n#         )\n#         # Define the output projection.\n#         self.output_projection = nn.Linear(hidden_dim, len(dictionary))\n#     # During training Decoders are expected to take the entire target sequence\n#     # (shifted right by one position) and produce logits over the vocabulary.\n#     # The *prev_output_tokens* tensor begins with the end-of-sentence symbol,\n#     # ``dictionary.eos()``, followed by the target sequence.\n#     def forward(self, prev_output_tokens, encoder_out):\n#         \"\"\"\n#         Args:\n#             prev_output_tokens (LongTensor): previous decoder outputs of shape\n#                 `(batch, tgt_len)`, for teacher forcing\n#             encoder_out (Tensor, optional): output from the encoder, used for\n#                 encoder-side attention\n#         Returns:\n#             tuple:\n#                 - the last decoder layer's output of shape\n#                   `(batch, tgt_len, vocab)`\n#                 - the last decoder layer's attention weights of shape\n#                   `(batch, tgt_len, src_len)`\n#         \"\"\"\n#         bsz, tgt_len = prev_output_tokens.size()\n#         # Extract the final hidden state from the Encoder.\n#         final_encoder_hidden = encoder_out['final_hidden']\n#         # Embed the target sequence, which has been shifted right by one\n#         # position and now starts with the end-of-sentence symbol.\n#         x = self.embed_tokens(prev_output_tokens)\n#         # Apply dropout.\n#         x = self.dropout(x)\n#         # Concatenate the Encoder's final hidden state to *every* embedded\n#         # target token.\n#         x = torch.cat(\n#             [x, final_encoder_hidden.unsqueeze(1).expand(bsz, tgt_len, -1)],\n#             dim=2,\n#         )\n#         # Using PackedSequence objects in the Decoder is harder than in the\n#         # Encoder, since the targets are not sorted in descending length order,\n#         # which is a requirement of ``pack_padded_sequence()``. Instead we'll\n#         # feed nn.LSTM directly.\n#         initial_state = (\n#             final_encoder_hidden.unsqueeze(0),  # hidden\n#             torch.zeros_like(final_encoder_hidden).unsqueeze(0),  # cell\n#         )\n#         output, _ = self.lstm(\n#             x.transpose(0, 1),  # convert to shape `(tgt_len, bsz, dim)`\n#             initial_state,\n#         )\n#         x = output.transpose(0, 1)  # convert to shape `(bsz, tgt_len, hidden)`\n#         # Project the outputs to the size of the vocabulary.\n#         x = self.output_projection(x)\n#         # Return the logits and ``None`` for the attention weights\n#         return x, None","c329a9de":"# from fairseq.models import FairseqEncoderDecoderModel, register_model\n# # Note: the register_model \"decorator\" should immediately precede the\n# # definition of the Model class.\n# @register_model('simple_lstm')\n# class SimpleLSTMModel(FairseqEncoderDecoderModel):\n#     @staticmethod\n#     def add_args(parser):\n#         # Models can override this method to add new command-line arguments.\n#         # Here we'll add some new command-line arguments to configure dropout\n#         # and the dimensionality of the embeddings and hidden states.\n#         parser.add_argument(\n#             '--encoder-embed-dim', type=int, metavar='N',\n#             help='dimensionality of the encoder embeddings',\n#         )\n#         parser.add_argument(\n#             '--encoder-hidden-dim', type=int, metavar='N',\n#             help='dimensionality of the encoder hidden state',\n#         )\n#         parser.add_argument(\n#             '--encoder-dropout', type=float, default=0.1,\n#             help='encoder dropout probability',\n#         )\n#         parser.add_argument(\n#             '--decoder-embed-dim', type=int, metavar='N',\n#             help='dimensionality of the decoder embeddings',\n#         )\n#         parser.add_argument(\n#             '--decoder-hidden-dim', type=int, metavar='N',\n#             help='dimensionality of the decoder hidden state',\n#         )\n#         parser.add_argument(\n#             '--decoder-dropout', type=float, default=0.1,\n#             help='decoder dropout probability',\n#         )\n#     @classmethod\n#     def build_model(cls, args, task):\n#         # Fairseq initializes models by calling the ``build_model()``\n#         # function. This provides more flexibility, since the returned model\n#         # instance can be of a different type than the one that was called.\n#         # In this case we'll just return a SimpleLSTMModel instance.\n#         # Initialize our Encoder and Decoder.\n#         encoder = SimpleLSTMEncoder(\n#             args=args,\n#             dictionary=task.source_dictionary,\n#             embed_dim=args.encoder_embed_dim,\n#             hidden_dim=args.encoder_hidden_dim,\n#             dropout=args.encoder_dropout,\n#         )\n#         decoder = SimpleLSTMDecoder(\n#             dictionary=task.target_dictionary,\n#             encoder_hidden_dim=args.encoder_hidden_dim,\n#             embed_dim=args.decoder_embed_dim,\n#             hidden_dim=args.decoder_hidden_dim,\n#             dropout=args.decoder_dropout,\n#         )\n#         model = SimpleLSTMModel(encoder, decoder)\n#         # Print the model architecture.\n#         print(model)\n#         return model\n#     # We could override the ``forward()`` if we wanted more control over how\n#     # the encoder and decoder interact, but it's not necessary for this\n#     # tutorial since we can inherit the default implementation provided by\n#     # the FairseqEncoderDecoderModel base class, which looks like:\n#     #\n#     # def forward(self, src_tokens, src_lengths, prev_output_tokens):\n#     #     encoder_out = self.encoder(src_tokens, src_lengths)\n#     #     decoder_out = self.decoder(prev_output_tokens, encoder_out)\n#     #     return decoder_out","9ab93e45":"# from fairseq.models import register_model_architecture\n# # The first argument to ``register_model_architecture()`` should be the name\n# # of the model we registered above (i.e., 'simple_lstm'). The function we\n# # register here should take a single argument *args* and modify it in-place\n# # to match the desired architecture.\n# @register_model_architecture('simple_lstm', 'tutorial_simple_lstm')\n# def tutorial_simple_lstm(args):\n#     # We use ``getattr()`` to prioritize arguments that are explicitly given\n#     # on the command-line, so that the defaults defined below are only used\n#     # when no other value has been specified.\n#     args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)\n#     args.encoder_hidden_dim = getattr(args, 'encoder_hidden_dim', 256)\n#     args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)\n#     args.decoder_hidden_dim = getattr(args, 'decoder_hidden_dim', 256)","ed787a25":"# !git clone https:\/\/github.com\/pytorch\/fairseq.git","e2425e91":"# !chmod +x fairseq\/examples\/translation\/prepare-iwslt14.sh","0838784e":"# ! fairseq\/examples\/translation\/prepare-iwslt14.sh","619b2b87":"# !ls \/kaggle\/working\/iwslt14.tokenized.de-en\/","8b9e498a":"# !fairseq-preprocess --source-lang de --target-lang en \\\n#     --trainpref \/kaggle\/working\/iwslt14.tokenized.de-en\/train --validpref \/kaggle\/working\/iwslt14.tokenized.de-en\/valid --testpref \/kaggle\/working\/iwslt14.tokenized.de-en\/test \\\n#     --destdir data-bin\/iwslt14.tokenized.de-en","fcd92ed6":"# ! mkdir -p checkpoints\/fconv","149fae25":"# ! CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin\/iwslt14.tokenized.de-en \\\n#     --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n#     --arch fconv_iwslt_de_en --save-dir checkpoints\/fconv","1ce3660e":"# Preprocess the dataset","2fbba602":"# Register model architecture\nlet\u2019s define a named architecture with the configuration for our model. This is done with the **register_model_architecture()** function decorator. Thereafter this named architecture can be used with the --arch command-line argument, e.g., *--arch tutorial_simple_lstm*:","63319d52":"# Training the encoder-decoder model","6a61e25b":"# 1. Building an Encoder and Decoder\n## Encoder\n\nIn this section we\u2019ll define a *simple LSTM Encoder and Decoder*. All Encoders should implement the **FairseqEncoder** interface and Decoders should implement the FairseqDecoder interface. These interfaces themselves extend **torch.nn.Module**, so FairseqEncoders and FairseqDecoders can be written and used in the same ways as ordinary PyTorch Modules.\nEncoder  \n\nOur Encoder will embed the tokens in the source sentence, feed them to a **torch.nn.LSTM** and return the final hidden state.  \n","9f342e75":"# Setup","a0f2ba71":"To create our encoder save the following in a new file named **fairseq\/models\/simple_lstm.py**.   (**Notice that** here we simply pip install the fairseq from the source rather than download from github resource. Thus, the code below merely demonstrate how we create the self-defined model class.)","95a0b71b":"# Registering the Model\n\nNow that we\u2019ve defined our Encoder and Decoder we must register our model with fairseq using the **register_model()** function decorator. Once the model is registered we\u2019ll be able to use it with the existing Command-line Tools(**fairseq CLT**).  \nAll registered models must implement the BaseFairseqModel interface. For sequence-to-sequence models (i.e., any model with a single Encoder and Decoder), we can instead implement the **FairseqEncoderDecoderModel** interface.\nCreate a small wrapper class in the same file and register it in fairseq with the name **'simple_lstm'**:","d44062ba":"# Introduction\n[Fairseq](https:\/\/fairseq.readthedocs.io\/en\/latest\/index.html) is a sequence modeling toolkit written in PyTorch that allows researchers and developers to train custom models for translation, summarization, language modeling and other text generation tasks.","cf546d5b":"Here we demo how to use the Fairseq to quickly build up a **seq2seq LSTM** to train for a **de-to-en translation** task with few codes","2c563197":"# Check the raw data files of en-de translation ","b01541eb":"## Decoder  \nOur Decoder will predict the next word, conditioned on the Encoder\u2019s final hidden state and an embedded representation of the previous target word \u2013 which is sometimes called teacher forcing. More specifically, we\u2019ll use a **torch.nn.LSTM** to produce a sequence of hidden states that we\u2019ll project to the size of the output vocabulary to predict each target word.","d8526b50":"# Prepare the dataset"}}