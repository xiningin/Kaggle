{"cell_type":{"6253a546":"code","553dcbe3":"code","fdfa091d":"code","93224864":"code","fef0e820":"code","2239df40":"code","ec6de7b5":"code","c8e16ea6":"code","a14631ba":"code","e6b46731":"code","18cddd3b":"code","1aa2f2c6":"code","984e76f5":"code","b2e3676b":"code","524b5c57":"code","4e436958":"code","7a1922ff":"code","161ce54f":"code","519833eb":"code","479e2bd0":"code","49c9336d":"code","2c3cbcf5":"code","08f8db20":"code","e4c2106e":"code","586a29ab":"code","25a4fbef":"code","6afa7b2a":"code","597ad50e":"code","15034bfc":"code","1c43a9ec":"code","4b4cce66":"code","0837a754":"code","8ed25cd7":"code","c2a9e47a":"code","b668aed2":"code","ae67aca5":"code","3a6444f2":"code","9455f930":"code","583f2ab3":"code","0cb5c036":"code","a896ff7d":"code","655f8867":"code","e3426f57":"code","f08aa9b4":"code","de689ff4":"code","ffdf50a1":"code","3c6409fc":"code","5efcabcd":"code","b763821f":"code","d21cc294":"code","2e7b956d":"code","60002012":"code","890dba88":"code","4c33d1e7":"markdown","134e17f8":"markdown","0bff2121":"markdown","e0c3232b":"markdown","a3918be0":"markdown","f07f9839":"markdown","e2762290":"markdown","c401c3b7":"markdown","b729b3b5":"markdown","7102ba22":"markdown","38222247":"markdown","ed74f084":"markdown","96f6879f":"markdown","237fb370":"markdown","335f87e4":"markdown","41139780":"markdown","ae6010ff":"markdown","b872eea1":"markdown","d0a10d9a":"markdown","747b5b8c":"markdown","c6e26e12":"markdown","7b26330a":"markdown","a2c674af":"markdown","4c66f2ce":"markdown","de809091":"markdown","6b27add7":"markdown","3fa7d66c":"markdown","f72c6b43":"markdown","6f93921c":"markdown","adcb9b39":"markdown","ec376455":"markdown","3041a6b8":"markdown","25197494":"markdown","c2f4dc20":"markdown","e52876f3":"markdown","8716d9c9":"markdown","f640e9ca":"markdown","6fd61e20":"markdown","e57e5d02":"markdown","702dda98":"markdown","1f458780":"markdown","feeffbea":"markdown","8e2f1003":"markdown","f80cc6d4":"markdown","01465c4b":"markdown","8ba014cc":"markdown","81cfffac":"markdown","31667494":"markdown","2d2f59ac":"markdown","a58f45d4":"markdown","ee8dd12d":"markdown","903839b7":"markdown","a91d2e00":"markdown","e4c261dc":"markdown","5cec5bc0":"markdown","179007d0":"markdown","7b4673d5":"markdown","0a8256df":"markdown","eb286a43":"markdown","8f3644b9":"markdown","47b6ac11":"markdown","4a9a947c":"markdown","d7460c69":"markdown"},"source":{"6253a546":"# For data analysis and data wragling\nimport pandas as pd\nimport numpy as np\n\n# For data visualization and graph plotting\n\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# For Machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","553dcbe3":"dataset = pd.read_csv('\/kaggle\/input\/train.csv')\ndataset.head(4)","fdfa091d":"dataset.dtypes","93224864":"print(dataset.describe())\nprint(dataset.Age.median())\nprint(dataset.Fare.median())","fef0e820":"dataset[['Sex','Survived']].groupby('Sex',as_index=False).mean()","2239df40":"g = sns.FacetGrid(dataset, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","ec6de7b5":"g = sns.FacetGrid(dataset, col='Survived')\ng.map(plt.hist, 'Fare', bins=20)","c8e16ea6":"dataset.Pclass.unique()","a14631ba":"dataset[['Pclass','Survived']].groupby('Pclass',as_index=False).mean()","e6b46731":"dataset.SibSp.unique()","18cddd3b":"dataset[['SibSp' , 'Survived']].groupby('SibSp' , as_index = False).mean()","1aa2f2c6":"dataset.Parch.unique()","984e76f5":"dataset[['Parch' , 'Survived']].groupby('Parch' , as_index = False).mean()","b2e3676b":"dataset.Embarked.unique()","524b5c57":"dataset[['Embarked' , 'Survived']].groupby('Embarked' , as_index = False).mean()","4e436958":"grid = sns.FacetGrid(dataset, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","7a1922ff":"# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(dataset, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","161ce54f":"df = dataset[['Pclass','Sex','Age','Fare','Embarked','Survived', 'SibSp' , 'Parch']]\ndf.head()","519833eb":"df.isnull().sum()","479e2bd0":"df.fillna({'Age': df.Age.mean() ,\n          'Embarked': 'S'} , inplace = True)","49c9336d":"df.head()","2c3cbcf5":"df.isnull().sum()","08f8db20":"df = pd.get_dummies(df , columns=['Sex' , 'Embarked' , 'Pclass' , 'Parch'])\nprint(df)","e4c2106e":"X = df.drop(columns='Survived').values\nY = df['Survived'].values\nprint(X)\nprint(Y)","586a29ab":"scale = StandardScaler()\nX[: , 0:2] = scale.fit_transform(X[:, 0:2])\nprint(X)","25a4fbef":"X_train , X_test , Y_train , Y_test = train_test_split(X , Y , test_size = 0.2)\nprint(X_train)","6afa7b2a":"classifier = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=5, max_features='log2',\n                       max_leaf_nodes=None, max_samples= None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=3, min_samples_split=8,\n                       min_weight_fraction_leaf=0.0, n_estimators=300,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)","597ad50e":"pred = cross_val_score(classifier , X , Y , cv = 3).mean()\nprint(pred)","15034bfc":"classifier.fit(X_train , Y_train)","1c43a9ec":"y_pred = classifier.predict(X_test)\nprint(y_pred)","4b4cce66":"plot_confusion_matrix(classifier , X_test , Y_test , display_labels=[\"Does not Survived\" , \"Survived\"])","0837a754":"from sklearn.metrics import accuracy_score\nscore = accuracy_score(Y_test , y_pred)\nprint(score)","8ed25cd7":"# from sklearn.model_selection import GridSearchCV \n  \n# # defining parameter range \n# param_grid = {'n_estimators': [300,500],\n#     'max_features': ['auto', 'sqrt', 'log2'],\n#     'max_depth' : [5,6,8,80],\n#     'min_samples_leaf': [3, 4, 5],\n#     'min_samples_split': [8, 10],}  \n  \n# grid = GridSearchCV(RandomForestClassifier(), param_grid, refit = True, verbose = 3) \n  \n# # fitting the model for grid search \n# grid.fit(X_train, Y_train) \n\n# # print best parameter after tuning \n# print(grid.best_params_) \n\n# print(grid.best_estimator_) ","c2a9e47a":"classifier_2 = RandomForestClassifier(n_jobs =  -1,\n    n_estimators= 300,\n     warm_start= True, \n#      max_features= 0.2,\n    random_state= 2,\n    max_depth= 5,\n    min_samples_leaf= 3,\n    max_features = 'log2',\n    criterion= 'entropy',  \n    min_samples_split= 8,\n    verbose =  0)","b668aed2":"predict = cross_val_score(classifier_2 , X , Y , cv = 3).mean()\nprint(predict)","ae67aca5":"classifier_2.fit(X_train , Y_train)","3a6444f2":"y_predict = classifier_2.predict(X_test)\nprint(y_predict)","9455f930":"plot_confusion_matrix(classifier_2 , X_test , Y_test , display_labels=[\"Does not Survived\" , \"Survived\"])","583f2ab3":"from sklearn.metrics import accuracy_score\nscore_2 = accuracy_score(Y_test , y_predict)\nprint(score_2)","0cb5c036":"from sklearn.svm import SVC\n\n# # {'C': 1, 'degree': 2, 'gamma': 0.1, 'kernel': 'rbf', 'random_state': 2}\n# SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n#     decision_function_shape='ovr', degree=2, gamma=0.1, kernel='rbf',\n#     max_iter=-1, probability=False, random_state=2, shrinking=True, tol=0.001,\n#     verbose=False)\n\nclassifier_3 = SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=2, gamma=0.1, kernel='rbf',\n    max_iter=-1, probability=False, random_state=2, shrinking=True, tol=0.001,\n    verbose=False)\nprint(classifier_3)","a896ff7d":"predict_2 = cross_val_score(classifier_3 , X , Y , cv = 3).mean()\nprint(predict_2)","655f8867":"classifier_3.fit(X_train , Y_train)","e3426f57":"y_prediction = classifier_3.predict(X_test)\nprint(y_prediction)","f08aa9b4":"plot_confusion_matrix(classifier_3 , X_test , Y_test , display_labels=[\"Does not survived\" , \"Survived\"])","de689ff4":"from sklearn.metrics import accuracy_score\nscore_3 = accuracy_score(Y_test , y_prediction)\nprint(score_3)","ffdf50a1":"# from sklearn.model_selection import GridSearchCV \n  \n# # defining parameter range \n# param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n#                'degree': [ 2, 3 , 4 ,5],\n#               'random_state': [ 2, 3, 4 , 5],\n#               'kernel': ['rbf']}  \n  \n# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n  \n# # fitting the model for grid search \n# grid.fit(X_train, Y_train) \n\n# # print best parameter after tuning \n# print(grid.best_params_) \n\n# print(grid.best_estimator_) ","3c6409fc":"from sklearn.linear_model import LogisticRegression\n\n\n# {'C': 0.09, 'penalty': 'l2', 'random_state': 2}\n# LogisticRegression(C=0.09, class_weight=None, dual=False, fit_intercept=True,\n#                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n#                    multi_class='auto', n_jobs=None, penalty='l2',\n#                    random_state=2, solver='lbfgs', tol=0.0001, verbose=0,\n#                    warm_start=False)\n\n\nclassifier_4 = LogisticRegression(C=0.09, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=2, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\nprint(classifier_4)","5efcabcd":"predict_4 = cross_val_score(classifier_4 , X , Y , cv = 3).mean()\nprint(predict_4)","b763821f":"classifier_4.fit(X_train , Y_train)","d21cc294":"y_prediction_lr = classifier_4.predict(X_test)\nprint(y_prediction_lr)","2e7b956d":"plot_confusion_matrix(classifier_4 , X_test , Y_test , display_labels=[\"Does not survived\" , \"Survived\"])","60002012":"from sklearn.metrics import accuracy_score\nscore_lr = accuracy_score(Y_test , y_prediction_lr)\nprint(score_lr)","890dba88":"# from sklearn.model_selection import GridSearchCV \n  \n# # defining parameter range \n# param_grid = {'penalty': ['l1', 'l2'],\n#               'C':[0.001,.009,0.01,.09,1,5,10,25],\n#               'random_state': [2 , 3 , 4]}  \n  \n# grid = GridSearchCV(LogisticRegression(), param_grid, refit = True, verbose = 3) \n  \n# # fitting the model for grid search \n# grid.fit(X_train, Y_train) \n\n# # print best parameter after tuning \n# print(grid.best_params_) \n\n# print(grid.best_estimator_) ","4c33d1e7":"##### So, here we see that passengers with Pclass= 1 have survived most.\n###### Threrefore, Pclass has to be included in feature selection","134e17f8":"So, Age is an important feature for prediction","0bff2121":"# Accuracy Score Of The Model","e0c3232b":"##### Here we see that, the people with high fares haves more survival rate than the one with less fares\n###### So, Fare will be an important feature.","a3918be0":"##### Here as we see the relation of sex with the survived.\n###### It is cleary seen that the female have survived more than that of male.\n###### It means that sex is an important feature for our prediction model.","f07f9839":"# Input And Output Feature Selection","e2762290":"Now we'll select the features which are most important for out model and to neglect the less important one's to get a good accuracy score","c401c3b7":"# Grid Search To Find Hyperparameters","b729b3b5":"##### As there are 3 classes i.e C,Q,S so we'll see the mean survival of each class","7102ba22":"# Confusion Matrix","38222247":"##### So, here we see that passengers with Sibling = 1 and Sibling = 2 have survived most.\n###### Threrefore, SibSp has to be included in feature selection","ed74f084":"### Checking datatypes for all features to get an idea of what types of data are present there","96f6879f":"# Confusion Matrix","237fb370":"# Feature Selection","335f87e4":"# Grid Search To Find Hyperparameters","41139780":"# Cross_Val_Score","ae6010ff":"### Next , we see for the age\n\nAs there are different ages for many people out there, we cannot use above method to see the relation of people survived.\nSo, we'll plot a graph for age and survival and see their relation.","b872eea1":"##### So, here we see that passengers with Parch = 1 and Parch = 3 have survived most.\n###### Threrefore, Parch has to be included in feature selection","d0a10d9a":"##### As there are 6 classes i.e 1,2,3,4,5,6 so we'll see the mean survival of each class","747b5b8c":"To identify which columns have empty values","c6e26e12":"##### So Age and Embarked have empty value columns. So we will replace the empty values in Age with the mean age value\n##### and S value for empty embarked column values.\n##### We are taking mean values ,  as mean and median values are not having high difference between each other.","7b26330a":"### Next, we'll see for Embarked","a2c674af":"##### Next , we'll scale the values to remove priortisation of one feature over another based on its higher value than another.\nMoreover, SVC algorithm needs scaled values for best prediction accuracy.","4c66f2ce":"#### ONE HOT ENCODING","de809091":"###### Now, here we see that embarked has a relation with fare , as people with ticket of high fare have high chance of survival\n##### So Embarked has to be included in the feature","6b27add7":"# ENCODING THE VALUES","3fa7d66c":"# Missing Values Imputation","f72c6b43":"### So here , we will make our final dataset by taking the featues we have discussed above\n\n##### We'll drop name , passengerId , Ticketno and Cabin features as they do not contribute much to the prediction.","6f93921c":"# Splitting Data Into Test And Train","adcb9b39":"# Confusion Matrix","ec376455":"##### As there are 3 classes i.e 1,2,3 so we'll see the mean survival of each class","3041a6b8":"# Confusion Matrix","25197494":"##### We'll again plot a graph between fare and survived and see their relation.","c2f4dc20":"# So , here we see the best accuracy and cross_val_score is given by the SVC model followed by random forest and logistic regression","e52876f3":"#                                              PROJECT TITANIC","8716d9c9":"### Importing the dataset","f640e9ca":"In all the algorithms the hyperparamters are given as provided by the grid search which will be found after the accuracy score for all the algorithms and those parameters have been put in the algorithms.","6fd61e20":"# 2.) Random Forest - 2 (criterion = entropy)","e57e5d02":"# Cross_Val_Score","702dda98":"# Accuracy Score","1f458780":"# Accuracy Score","feeffbea":"### To find out the mean, std and count values for all numeric features","8e2f1003":"# Grid Search To Find Hyperparametes","f80cc6d4":"# 3.)  SVC","01465c4b":"##### So, here we see that passengers with Embarked = C have survived most.","8ba014cc":"### Next, we'll see for Pclass","81cfffac":"### Next, we'll see for the Fare","31667494":"### Next, we'll see for SibSp","2d2f59ac":"##### Next we'll encode the values of the features having values from 0-9 as they'll make the feature priortise and degrades the accuracy which should not be the case.","a58f45d4":"### Next, we'll see for Parch","ee8dd12d":"# Cross_Val_Score","903839b7":"# Cross_Val_Score","a91d2e00":"##### Here, we see that:-\n###### Infants (Age <=4) had high survival rate.\n###### Oldest passengers have survived.\n###### Large number of 15-25 year olds did not survive.\n###### Most passengers are in 15-35 age range.","e4c261dc":"##### As there are 6 classes i.e 1,2,3,4,5,6 so we'll see the mean survival of each class","5cec5bc0":"# Importing the libraries for the project","179007d0":"# Scaling Of Values","7b4673d5":"###### Female passengers had much better survival rate than males ,  as we have seen above\n###### Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass or Fare and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.","0a8256df":"# Applying Various Algorithms To Find The Best Cross_val_score And Accuracy Score","eb286a43":"# 4.) Logistic Regression","8f3644b9":"# Accuracy Score Random Forest","47b6ac11":"##### In this project, we'll be using Titanic dataset which consists of various features\n\n##### to predict on test data that whether a person survived or not during titanic sank incident.\n\n##### We'll be applying Machine Learning to get this done.\n\n##### In Machine Learning we'll be using :-\n\n##### 1.) data analysis , data visualization , graph plot for feature selection \n\n##### 2.) cross_val_score prediction , accuracy_score prediction , grid_search for hyperparameter tuning\n\n##### 3.) Various algorithms to find the best model ","4a9a947c":"This helps us to get a fair idea about disrtribution of data and to see it's mean and median age.","d7460c69":"# 1.) Random Forest"}}