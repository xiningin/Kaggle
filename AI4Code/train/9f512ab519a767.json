{"cell_type":{"7357e5b6":"code","516e66bc":"code","9aca3c75":"code","b7ea49ae":"code","274ca70f":"code","cbec04a1":"code","cb950cc6":"code","5a1b94d4":"code","3e934ae1":"code","0931c289":"code","ff6e0292":"code","718c1fee":"code","5fbc3d9e":"code","056070eb":"code","f30bece7":"code","6a988b15":"code","6b7c9bc6":"code","22ebf422":"code","1a688674":"code","c880656e":"code","894d213b":"code","db6d844a":"code","f0669a3d":"code","8459e2ba":"code","49ada59c":"code","6bf1ae8c":"code","10b8d15f":"code","00f86848":"code","31ffe058":"code","f01c5494":"code","47fb90c2":"code","4cfb6b0c":"code","17cd1624":"code","a416e090":"code","3aa96c6e":"code","655d22d4":"code","0a158019":"code","5269d286":"code","245a2bb2":"code","26e81afe":"code","331e71f0":"code","3ee9b736":"code","1f02d480":"code","a7b389a2":"code","2db192b9":"code","ea3cd878":"code","68cc0d50":"code","25892e78":"code","f5d5a15e":"code","86f58297":"code","5d229690":"code","de3fd082":"code","703be4a9":"code","e579a21c":"code","af571a4e":"code","d1a1225e":"code","816d604a":"code","957cccb5":"code","9ecf1063":"code","11970fc9":"code","d6a6f8bd":"code","571ab4b7":"code","613e58b1":"code","d3207ad5":"code","1fb83695":"code","82ff188a":"code","ac0883e3":"code","7ad13562":"code","ab2cdbc6":"code","76d56fe5":"code","64161d9c":"code","5df5cc54":"code","af6b537e":"code","7d5c9ee7":"code","cd2a9e19":"code","7b0de98d":"code","c43c1052":"code","f527a0f3":"code","a43a9ac7":"code","9f7e1f78":"code","730a7447":"code","71af02ed":"code","c8e56bdb":"code","53503105":"code","eeaa9b3f":"code","2a5fe7ea":"code","f3f92750":"code","4ab58c46":"code","2a46c436":"code","6a8195d1":"code","828c9c75":"code","6ea927e0":"code","9fb635b6":"code","13723bd9":"code","04f7ad4b":"code","195998f6":"code","71e0d6a0":"code","c75280a5":"code","28feb24d":"code","a247df67":"code","413fa8fe":"code","32aadb71":"code","7c485350":"code","7bc79d2c":"code","3b19f665":"code","3d01037f":"code","2bd092d2":"code","2f030a33":"code","56572c3e":"code","829db9dc":"code","718f2d5e":"code","c289a630":"code","2f5a9ece":"code","e2148bab":"code","ded7fb5c":"code","01039e1e":"code","c69756ef":"code","e2bfa130":"code","57ca723b":"code","5599dd12":"code","5281235d":"code","9e29c923":"code","2005efff":"code","15d1df23":"code","5939fa4f":"code","f7e6745b":"code","6b2d2d0d":"code","0b960d6a":"code","fdb899c1":"code","bf04f110":"code","9ccd1edd":"code","e1a0b6fa":"code","ef8e0bbd":"code","6956bef2":"code","ec8c885e":"code","38e6f912":"code","30720630":"code","ea7e7f3c":"code","bd5d343b":"code","d2cb4e4d":"code","d00f6a22":"code","666cb7ea":"code","1311754d":"code","18bdc141":"code","d18ab09f":"code","2d9d4f82":"code","f0f1fd23":"code","b7a52488":"code","3493b5bf":"code","eb1fca2b":"code","22666a0a":"code","a3193b4d":"code","444f95dd":"code","ba437842":"code","bcc1a95d":"code","070ff69e":"code","e0b9e81a":"code","7dd54ef1":"code","4ecf1bc8":"code","31352915":"code","8279a9b3":"code","269f8306":"code","96c9ff8d":"code","6754951a":"code","513356fc":"code","1751056e":"code","582eb9ee":"code","4e2f3b2f":"code","9bbcb40c":"code","cc6df0b1":"code","fdf00c26":"code","ec49a408":"code","6a315b31":"code","53856270":"code","5907144e":"code","62cdf12b":"code","8d75f340":"code","f7b8f922":"markdown","0f76b54b":"markdown","22c4f9fe":"markdown","d3a9d2ce":"markdown","955335fa":"markdown","d1e7a048":"markdown","e08fcc43":"markdown","f7043669":"markdown","1b075bab":"markdown","d2afc53b":"markdown","c31bc1b9":"markdown","000051fa":"markdown","70cda50b":"markdown","13a79ad0":"markdown","45b8df8d":"markdown","f263a187":"markdown","57770e79":"markdown","3ca7e294":"markdown","dd11ff85":"markdown","23d66ad8":"markdown","c258140f":"markdown","60f56fde":"markdown","1303f796":"markdown","bc4a4e78":"markdown","2c33d8e9":"markdown","5bb748d5":"markdown","c1f9e78b":"markdown","c965614a":"markdown","0f6d0d24":"markdown","62be6a09":"markdown","3c7a3069":"markdown","6435451f":"markdown","8148d8b1":"markdown","c04e3aa1":"markdown","605e6882":"markdown","f7c07ca7":"markdown","c965c6ff":"markdown","1649c105":"markdown","ded28deb":"markdown","6d9da396":"markdown","c87a1319":"markdown","a5423ca2":"markdown","2da55b7e":"markdown","e9311ee7":"markdown","3639c815":"markdown","bf1904d5":"markdown","95f6bf37":"markdown","67ed11a0":"markdown","a677f974":"markdown","905f6fed":"markdown","c607c324":"markdown"},"source":{"7357e5b6":"#loading_all libraries\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import average_precision_score,roc_auc_score\nfrom sklearn.metrics import f1_score,roc_curve,recall_score\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pylab as pylab\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nfrom sklearn import metrics\nimport matplotlib as mpl\nfrom scipy import stats\nimport xgboost as xgb\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport csv","516e66bc":"#to ignore warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","9aca3c75":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b7ea49ae":"df_train=pd.read_table('\/kaggle\/input\/xyzcorp-lendingdata\/XYZCorp_LendingData.txt',parse_dates=['issue_d'],low_memory=False)","274ca70f":"#to display the entire dataframe \npd.set_option(\"display.max.columns\", None)","cbec04a1":"df_train.shape","cb950cc6":"#to view top 5 rows of dataframe\ndf_train.head(n=5)","5a1b94d4":"#to known about the dataframe of featurs\ndf_train.dtypes","3e934ae1":"#function to find missing Value\ndef missing_data(df_train):\n    total = df_train.isnull().sum().sort_values(ascending=False)\n    percent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return(missing_data.head(20))","0931c289":"#missing_data function on dataset\nmissing_data(df_train)","ff6e0292":"#removing top 26 feature with most missing value\ndf_train_new=df_train.drop(['dti_joint','verification_status_joint','annual_inc_joint','il_util','mths_since_rcnt_il',\n'total_bal_il','inq_last_12m','open_acc_6m','open_il_6m','open_il_24m','open_il_12m',\n'open_rv_12m','open_rv_24m','max_bal_bc','all_util','inq_fi','total_cu_tl','desc','mths_since_last_record',\n'mths_since_last_major_derog','mths_since_last_delinq','next_pymnt_d','tot_cur_bal',\n'tot_coll_amt','total_rev_hi_lim','emp_title'],axis=1)","718c1fee":"#imputing the missing value with mean value in revol_util featture\ndf_train_new['revol_util'].fillna(df_train_new['revol_util'].mean(),inplace=True)","5fbc3d9e":"missing_data(df_train_new)","056070eb":"#feature enginearing on last_credit_pull_d feature\ndf_train_new['last_credit_pull_d'] = pd.to_datetime(df_train_new['last_credit_pull_d'])\ndf_train_new['Month'] = df_train_new['last_credit_pull_d'].apply(lambda x: x.month)\ndf_train_new['Year'] = df_train_new['last_credit_pull_d'].apply(lambda x: x.year)\ndf_train_new = df_train_new.drop(['last_credit_pull_d'], axis = 1)","f30bece7":"#imputing missing value with mode value \ndf_train_new['Month'].fillna(df_train_new.mode()['Month'][0],inplace=True)\ndf_train_new['Year'].fillna(df_train_new.mode()['Year'][0],inplace=True)","6a988b15":"df_train_new.shape","6b7c9bc6":"print(df_train_new['title'].value_counts())","22ebf422":"#to impute missing na value with debt_consolidation because its is most repeted value\ndf_train_new['title'].fillna('Debt consolidation ',inplace=True)","1a688674":"df_train_new.dtypes","c880656e":"#removing\/droping the unwanted features \ndf_train_new = df_train_new.drop(['id'],axis=1)\n\ndf_train_new = df_train_new.drop(['member_id'],axis=1)\n\ndf_train_new = df_train_new.drop(['earliest_cr_line'],axis=1)\n\ndf_train_new = df_train_new.drop(['zip_code'],axis=1)\n\ndf_train_new = df_train_new.drop(['last_pymnt_d'],axis=1)\n\ndf_train_new = df_train_new.drop(['policy_code'],axis=1)","894d213b":"df_train_new.head(n=5)","db6d844a":"#replace the categorial to numeric \ndf_train_new=df_train_new.replace(to_replace='10+ years',value=10)\ndf_train_new=df_train_new.replace(to_replace='1 year',value=1)\ndf_train_new=df_train_new.replace(to_replace='2 years',value=2)\ndf_train_new=df_train_new.replace(to_replace='3 years',value=3)\ndf_train_new=df_train_new.replace(to_replace='4 years',value=4)\ndf_train_new=df_train_new.replace(to_replace='5 years',value=5)\ndf_train_new=df_train_new.replace(to_replace='6 years',value=6)\ndf_train_new=df_train_new.replace(to_replace='7 years',value=7)\ndf_train_new=df_train_new.replace(to_replace='8 years',value=8)\ndf_train_new=df_train_new.replace(to_replace='9 years',value=9)\ndf_train_new=df_train_new.replace(to_replace='< 1 year',value=0.5)","f0669a3d":"df_train_new['title'].value_counts()","8459e2ba":"counts = df_train_new['title'].value_counts()\n\ndf_train_new = df_train_new[~df_train_new['title'].isin(counts[counts < 100].index)]","49ada59c":"df_train_new.head(n=5)","6bf1ae8c":"#to remove all na values throughout the dataset\ndf_train_new = df_train_new.dropna(axis = 0, how ='any') ","10b8d15f":"df_train_new['emp_length'].value_counts()","00f86848":"#dataset for data visulization in tabelau\ndf_train_bin=df_train_new\ndf_train_new.to_csv('Bank Lending.csv')","31ffe058":"#correlation matrix\ncorrmat = df_train_new.corr()\nf, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(corrmat, vmax=.8, square=True);","f01c5494":"df_train['default_ind'].value_counts().plot.bar()","47fb90c2":"sns.countplot('initial_list_status',data=df_train_new,hue='default_ind')","4cfb6b0c":"sns.boxplot('grade','int_rate',data=df_train_new)","17cd1624":"plt.figure(figsize=(10,5))\nsns.distplot(df_train_new['int_rate'])\nplt.show()","a416e090":"sns.violinplot('default_ind','int_rate',data=df_train_new,bw='scott')","3aa96c6e":"#plotting histogram of all features\ndf_train_new.hist(figsize=(15,20))","655d22d4":"sns.stripplot('default_ind','annual_inc',data=df_train_new,jitter=True)","0a158019":"plt.figure(figsize=(15,10))\nsns.catplot(x='verification_status',y='loan_amnt',data=df_train_new,hue='default_ind',height=5,aspect=3,kind='box')\nplt.title('boxplot')","5269d286":"plt.figure(figsize=(15,10))\nsns.relplot(x='funded_amnt', y='funded_amnt_inv', data=df_train_new,\n            kind='line', hue='term', col='default_ind')","245a2bb2":"fig, ax = plt.subplots(1, 3, figsize=(16,5))\n\nsns.distplot(df_train['loan_amnt'], ax=ax[0])\nsns.distplot(df_train['funded_amnt'], ax=ax[1])\nsns.distplot(df_train['funded_amnt_inv'], ax=ax[2])\n\nax[1].set_title(\"Amount Funded by the Lender\")\nax[0].set_title(\"Loan Applied by the Borrower\")\nax[2].set_title(\"Total committed by Investors\")","26e81afe":"df_train.purpose.value_counts(ascending=False).plot.bar(figsize=(10,5))\nplt.xlabel('purpose'); plt.ylabel('Density'); plt.title('Purpose of loan');","331e71f0":"plt.figure(figsize=(10,5))\ndf_train['issue_year'] = df_train['issue_d'].dt.year\nsns.barplot(x='issue_year',y='loan_amnt',data=df_train)","3ee9b736":"# Loan Status \nfig, ax = plt.subplots(1, 2, figsize=(16,5))\ndf_train['default_ind'].value_counts().plot.pie(explode=[0,0.25],labels=['good loans','bad loans'],\n                                             autopct='%1.2f%%',startangle=70,ax=ax[0])\nsns.kdeplot(df_train.loc[df_train['default_ind']==0,'issue_year'],label='default_ind = 0')\nsns.kdeplot(df_train.loc[df_train['default_ind']==1,'issue_year'],label='default_ind = 1')\nplt.xlabel('Year'); plt.ylabel('Density'); plt.title('Yearwise Distribution of defaulter')","1f02d480":"defaulter = df_train_new.loc[df_train_new['default_ind']==1]\nplt.figure(figsize=(10,10))\nplt.subplot(211)\nsns.boxplot(data=defaulter,x = 'home_ownership',y='loan_amnt',hue='default_ind')\nplt.subplot(212)\nsns.boxplot(data=defaulter,x='Year',y='loan_amnt',hue='home_ownership')","a7b389a2":"sns.countplot('verification_status',data=df_train_new,hue='default_ind')","2db192b9":"sns.stripplot('default_ind','total_rec_prncp',data=df_train_new,jitter=True)","ea3cd878":"plt.figure(figsize=(25,20))\nsns.factorplot(data=df_train_new,x='verification_status',y='loan_amnt',hue='default_ind')","68cc0d50":"# Plotting\nsns.catplot(x='verification_status', y='loan_amnt', data=df_train_new, kind='boxen', aspect=2)\nplt.title('Boxen Plot', weight='bold', fontsize=16)\nplt.show()","25892e78":"df_train_new.columns","f5d5a15e":"df_train_new.describe()","86f58297":"dftrain_bin=df_train_new","5d229690":"from sklearn import preprocessing\n\nle1 = preprocessing.LabelEncoder()\nle1.fit(df_train_new['term'])\nlist(le1.classes_)\ndf_train_new['term'] = le1.transform(df_train_new['term'])\ndf_train_new.head()","de3fd082":"le1 = preprocessing.LabelEncoder()\nle1.fit(df_train_new['grade'])\nlist(le1.classes_)\ndf_train_new['grade'] = le1.transform(df_train_new['grade'])\ndf_train_new.head()","703be4a9":"le1 = preprocessing.LabelEncoder()\nle1.fit(df_train_new['sub_grade'])\nlist(le1.classes_)\ndf_train_new['sub_grade'] = le1.transform(df_train_new['sub_grade'])\ndf_train_new.head()","e579a21c":"le1 = preprocessing.LabelEncoder()\nle1.fit(df_train_new['home_ownership'])\nlist(le1.classes_)\ndf_train_new['home_ownership'] = le1.transform(df_train_new['home_ownership'])\ndf_train_new.head()","af571a4e":"le1 = preprocessing.LabelEncoder()\nle1.fit(df_train_new['verification_status'])\nlist(le1.classes_)\ndf_train_new['verification_status'] = le1.transform(df_train_new['verification_status'])\ndf_train_new.head()","d1a1225e":"le1 = preprocessing.LabelEncoder()\nle1.fit(df_train_new['purpose'])\nlist(le1.classes_)\ndf_train_new['purpose'] = le1.transform(df_train_new['purpose'])\ndf_train_new.head()","816d604a":"le1 = preprocessing.LabelEncoder()\nle1.fit(df_train_new['addr_state'])\nlist(le1.classes_)\ndf_train_new['addr_state'] = le1.transform(df_train_new['addr_state'])\ndf_train_new.head()","957cccb5":"le1 = preprocessing.LabelEncoder()\nle1.fit(df_train_new['application_type'])\nlist(le1.classes_)\ndf_train_new['application_type'] = le1.transform(df_train_new['application_type'])\ndf_train_new.head()","9ecf1063":"le1 = preprocessing.LabelEncoder()\nle1.fit(df_train_new['pymnt_plan'])\nlist(le1.classes_)\ndf_train_new['pymnt_plan'] = le1.transform(df_train_new['pymnt_plan'])\ndf_train_new.head()","11970fc9":"le1 = preprocessing.LabelEncoder()\nle1.fit(df_train_new['initial_list_status'])\nlist(le1.classes_)\ndf_train_new['initial_list_status'] = le1.transform(df_train_new['initial_list_status'])\ndf_train_new.head()","d6a6f8bd":"df_train_new.dtypes","571ab4b7":"train = df_train_new[df_train_new['issue_d'] < '2015-6-01']\ntest = df_train_new[df_train_new['issue_d'] >= '2015-6-01']","613e58b1":"x_train=train.drop(['default_ind','title','issue_d'],axis=1)\ny_train=train['default_ind']\nx_test=test.drop(['default_ind','title','issue_d'],axis=1)\ny_test=test['default_ind']","d3207ad5":"log =LogisticRegression()\nlog.fit(x_train,y_train)","1fb83695":"#model on train using all the independent values in df\nlog_prediction = log.predict(x_train)\nlog_score= accuracy_score(y_train,log_prediction)\nprint('Accuracy score on train set using Logistic Regression :',log_score)","82ff188a":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train, log_prediction)","ac0883e3":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(y_train,log_prediction)\nprint(\"AUC on train using Logistic Regression :\",metrics.auc(fpr, tpr))","7ad13562":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_train, log_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","ab2cdbc6":"from sklearn.metrics import recall_score\nprint('recall_score on train set :',recall_score(y_train, log_prediction))","76d56fe5":"from sklearn.metrics import f1_score\nprint('F1_sccore on train set :',f1_score(y_train, log_prediction))","64161d9c":"print(classification_report(y_train,log_prediction))","5df5cc54":"#model on test using all the independent values in df\nlog_prediction = log.predict(x_test)\nlog_score= accuracy_score(y_test,log_prediction)\nprint('accuracy score on test using Logisitic Regression :',log_score)","af6b537e":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, log_prediction)","7d5c9ee7":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(y_test,log_prediction)\nprint(\"AUC on test using Logistic Regression :\",metrics.auc(fpr, tpr))","cd2a9e19":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, log_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","7b0de98d":"from sklearn.metrics import recall_score\nprint('recall_score on train set :',recall_score(y_test, log_prediction))","c43c1052":"from sklearn.metrics import f1_score\nprint('F1_sccore on train set :',f1_score(y_test, log_prediction))","f527a0f3":"print(classification_report(y_test, log_prediction))","a43a9ac7":"lr = LogisticRegression()\nscores = cross_val_score(lr, x_train, y_train, cv=5, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","9f7e1f78":"lr_prob=log.predict_proba(x_train)\nlr_prob=lr_prob[:,1]\nfalse_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_train, lr_prob)\nprint('auc_score for Logistic Regression(train): ', roc_auc_score(y_train, lr_prob))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(train) - logistic regression')\nplt.plot(false_positive_rate1, true_positive_rate1)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nlr_prob_test=log.predict_proba(x_test)\nlr_prob_test=lr_prob_test[:,1]\nfalse_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, lr_prob_test)\nprint('auc_score for Logistic Regression(test): ', roc_auc_score(y_test, lr_prob_test))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(test) - logistic regression')\nplt.plot(false_positive_rate2, true_positive_rate2)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","730a7447":"xgboost = xgb.XGBClassifier(max_depth=3,n_estimators=300,learning_rate=0.05)","71af02ed":"xgboost.fit(x_train,y_train)","c8e56bdb":"#XGBoost model on the train set\nXGB_prediction = xgboost.predict(x_train)\nXGB_score= accuracy_score(y_train,XGB_prediction)\nprint('accuracy score on train using XGBoost ',XGB_score)","53503105":"confusion_matrix(y_train, XGB_prediction)","eeaa9b3f":"fpr, tpr, thresholds = metrics.roc_curve(y_train,XGB_prediction)\nprint(\"AUC on train using XGBoost :\",metrics.auc(fpr, tpr))","2a5fe7ea":"average_precision = average_precision_score(y_train, XGB_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","f3f92750":"print('recall_score on train set :',recall_score(y_train, XGB_prediction))","4ab58c46":"print('F1_sccore on train set :',f1_score(y_train, XGB_prediction))","2a46c436":"print('classification Report on  train using XGBoost :')\nprint(classification_report(y_train,XGB_prediction))","6a8195d1":"#XGBoost model on the test\nXGB_prediction = xgboost.predict(x_test)\nXGB_score= accuracy_score(y_test,XGB_prediction)\nprint('accuracy score on test using XGBoost :',XGB_score)","828c9c75":"confusion_matrix(y_test, XGB_prediction)","6ea927e0":"fpr, tpr, thresholds = metrics.roc_curve(y_test,XGB_prediction)\nprint(\"AUC on test using XGBoost :\",metrics.auc(fpr, tpr))","9fb635b6":"average_precision = average_precision_score(y_test, XGB_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","13723bd9":"print('recall_score on test set :',recall_score(y_test, XGB_prediction))","04f7ad4b":"print('F1_sccore on test set :',f1_score(y_test, XGB_prediction))","195998f6":"print('classification Report on  test using XGBoost :')\nprint(classification_report(y_test,XGB_prediction))","71e0d6a0":"xg_prob=xgboost.predict_proba(x_train)\nxg_prob=xg_prob[:,1]\nfalse_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_train, xg_prob)\nprint('auc_score for Xgboost: (train): ', roc_auc_score(y_train, xg_prob))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(train) - XGBoost ')\nplt.plot(false_positive_rate1, true_positive_rate1)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nxg_prob_test=xgboost.predict_proba(x_test)\nxg_prob_test=xg_prob_test[:,1]\nfalse_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, xg_prob_test)\nprint('auc_score for Xgboost(test): ', roc_auc_score(y_test, xg_prob_test))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(test) - XGBoost ')\nplt.plot(false_positive_rate2, true_positive_rate2)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","c75280a5":"import pandas as pd\n%matplotlib inline\n#do code to support model\n#\"data\" is the X dataframe and model is the SKlearn object\n\nfeats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(x_train.columns, xgboost.feature_importances_):\n    feats[feature] = importance #add the name\/value pair \n\nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n#plt.figure(figsize=(15,7))\nimportances.sort_values(by='Gini-importance').plot(kind='bar', rot=45,figsize=(15,7))","28feb24d":"xg = xgb.XGBClassifier()\nscores = cross_val_score(xg, x_test, y_test, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","a247df67":"rfc2=RandomForestClassifier()\nrfc2.fit(x_train,y_train)","413fa8fe":"#model on train using all the independent values in df\nrfc_prediction = rfc2.predict(x_train)\nrfc_score= accuracy_score(y_train,rfc_prediction)\nprint('accuracy Score on train using RandomForest :',rfc_score)","32aadb71":"confusion_matrix(y_train, rfc_prediction)","7c485350":"fpr, tpr, thresholds = metrics.roc_curve(y_train,rfc_prediction)\nprint(\"AUC on train using RandomForest :\",metrics.auc(fpr, tpr))","7bc79d2c":"average_precision = average_precision_score(y_train, rfc_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","3b19f665":"print('recall_score on train set :',recall_score(y_train, rfc_prediction))","3d01037f":"print('F1_sccore on train set :',f1_score(y_train, rfc_prediction))","2bd092d2":"print('classification Report on  train using RandomForest :')\nprint(classification_report(y_train,rfc_prediction))","2f030a33":"#model on test using all the indpendent values in df\nrfc_prediction = rfc2.predict(x_test)\nrfc_score= accuracy_score(y_test,rfc_prediction)\nprint('accuracy score on test using RandomForest ',rfc_score)","56572c3e":"confusion_matrix(y_test, rfc_prediction)","829db9dc":"fpr, tpr, thresholds = metrics.roc_curve(y_test,rfc_prediction)\nprint(\"AUC on test using RandomForest :\",metrics.auc(fpr, tpr))","718f2d5e":"average_precision = average_precision_score(y_test, rfc_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","c289a630":"print('recall_score on test set :',recall_score(y_test, rfc_prediction))","2f5a9ece":"print('F1_sccore on train set :',f1_score(y_test, rfc_prediction))","e2148bab":"print('classification Report on  test using RandomForest :')\nprint(classification_report(y_test,rfc_prediction))","ded7fb5c":"rf_prob=rfc2.predict_proba(x_train)\nrf_prob=rf_prob[:,1]\nfalse_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_train, rf_prob)\nprint('auc_score for Random Forest : (train): ', roc_auc_score(y_train, rf_prob))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(train) - Random Forest :')\nplt.plot(false_positive_rate1, true_positive_rate1)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nrf_prob_test=rfc2.predict_proba(x_test)\nrf_prob_test=rf_prob_test[:,1]\nfalse_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, rf_prob_test)\nprint('auc_score for Random forest (test): ', roc_auc_score(y_test, rf_prob_test))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(test) - Random Forest : ')\nplt.plot(false_positive_rate2, true_positive_rate2)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","01039e1e":"import pandas as pd\n%matplotlib inline\n#do code to support model\n#\"data\" is the X dataframe and model is the SKlearn object\n\nfeats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(x_train.columns, rfc2.feature_importances_):\n    feats[feature] = importance #add the name\/value pair \n\nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n#plt.figure(figsize=(15,7))\nimportances.sort_values(by='Gini-importance').plot(kind='bar', rot=45,figsize=(15,7))","c69756ef":"lr = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(lr, x_train, y_train, cv=3, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","e2bfa130":"from sklearn.tree import DecisionTreeClassifier\ndec=DecisionTreeClassifier()","57ca723b":"dec.fit(x_train,y_train)","5599dd12":"#model on train using all the independent values in df\ndec_prediction = dec.predict(x_train)\ndec_score= accuracy_score(y_train,dec_prediction)\nprint('Accuracy score on train using Decision Tree :',dec_score)","5281235d":"    print(confusion_matrix(y_train, dec_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_train,dec_prediction)\n    print(\"AUC on train using DecisionTree :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_train, dec_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on train set :',recall_score(y_train, dec_prediction))\n    print('F1_sccore on train set :',f1_score(y_train, dec_prediction))\n    print('classification report on train using Decision tree ',classification_report(y_train,dec_prediction))","9e29c923":"#model on test using all the independent values in df\ndec_prediction = dec.predict(x_test)\ndec_score= accuracy_score(y_test,dec_prediction)\nprint('Accuracy Score on tree using Decision Tree  :',dec_score)","2005efff":"    print(confusion_matrix(y_test, dec_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_test,dec_prediction)\n    print(\"AUC on test using DecisionTree :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_test, dec_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on test set :',recall_score(y_test, dec_prediction))\n    print('F1_sccore on test set :',f1_score(y_test, dec_prediction))\n    print('classification report on test using Decision tree ',classification_report(y_test,dec_prediction))","15d1df23":"rf_prob=dec.predict_proba(x_train)\nrf_prob=rf_prob[:,1]\nfalse_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_train, rf_prob)\nprint('auc_score for decision tree : (train): ', roc_auc_score(y_train, rf_prob))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(train) - decision tre :')\nplt.plot(false_positive_rate1, true_positive_rate1)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nrf_prob_test=dec.predict_proba(x_test)\nrf_prob_test=rf_prob_test[:,1]\nfalse_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, rf_prob_test)\nprint('auc_score for decision tree (test): ', roc_auc_score(y_test, rf_prob_test))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(test) - decision tree : ')\nplt.plot(false_positive_rate2, true_positive_rate2)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","5939fa4f":"lr = DecisionTreeClassifier()\nscores = cross_val_score(lr, x_train, y_train, cv=5, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","f7e6745b":"from sklearn.tree import ExtraTreeClassifier\netc=ExtraTreeClassifier()\netc.fit(x_train,y_train)","6b2d2d0d":"#model on train using all the independent values in df\netc_prediction = etc.predict(x_train)\netc_score= accuracy_score(y_train,etc_prediction)\nprint('Accuracy score on train using extratree :',etc_score)","0b960d6a":"    print(confusion_matrix(y_train, etc_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_train,etc_prediction)\n    print(\"AUC on train using ExtraTree :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_train, etc_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on train set :',recall_score(y_train, etc_prediction))\n    print('F1_sccore on train set :',f1_score(y_train, etc_prediction))\n    print('classification report on train using Extra tree ',classification_report(y_train,etc_prediction))","fdb899c1":"#model on test using all the independent values in df\netc_prediction = etc.predict(x_test)\netc_score= accuracy_score(y_test,etc_prediction)\nprint('Accuracy score on test using extratree :',etc_score)","bf04f110":"    print(confusion_matrix(y_test, etc_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_test,etc_prediction)\n    print(\"AUC on train using ExtraTree :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_test, etc_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on test set :',recall_score(y_test, dec_prediction))\n    print('F1_sccore on test set :',f1_score(y_test, etc_prediction))\n    print('classification report on test using Extra tree ',classification_report(y_test,etc_prediction))","9ccd1edd":"rf_prob=etc.predict_proba(x_train)\nrf_prob=rf_prob[:,1]\nfalse_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_train, rf_prob)\nprint('auc_score for Extra tree : (train): ', roc_auc_score(y_train, rf_prob))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(train) - Extra tree :')\nplt.plot(false_positive_rate1, true_positive_rate1)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nrf_prob_test=etc.predict_proba(x_test)\nrf_prob_test=rf_prob_test[:,1]\nfalse_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, rf_prob_test)\nprint('auc_score for Extra Tree (test): ', roc_auc_score(y_test, rf_prob_test))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(test) - Extra tree : ')\nplt.plot(false_positive_rate2, true_positive_rate2)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","e1a0b6fa":"lr = ExtraTreeClassifier()\nscores = cross_val_score(lr, x_train, y_train, cv=5, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","ef8e0bbd":"from sklearn.ensemble import AdaBoostClassifier\nada =AdaBoostClassifier(n_estimators=100)","6956bef2":"ada.fit(x_train,y_train)","ec8c885e":"#model on train using all the independent values in df\nada_prediction = ada.predict(x_train)\nada_score= accuracy_score(y_train,ada_prediction)\nprint('Accuracy score on train using AdaBoost :',ada_score)","38e6f912":"    print(confusion_matrix(y_train, ada_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_train,ada_prediction)\n    print(\"AUC on train using AdaBoost :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_train, ada_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on train set :',recall_score(y_train, ada_prediction))\n    print('F1_sccore on train set :',f1_score(y_train, ada_prediction))\n    print('classification report on train using Extra tree ',classification_report(y_train,ada_prediction))","30720630":"#model on test using all the independent values in df\nada_prediction = ada.predict(x_test)\nada_score= accuracy_score(y_test,ada_prediction)\nprint('accuracy score on test using AdaBoost :',ada_score)","ea7e7f3c":"    print(confusion_matrix(y_test, ada_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_test,ada_prediction)\n    print(\"AUC on test using AdaBoost :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_test, ada_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on test set :',recall_score(y_test, ada_prediction))\n    print('F1_sccore on test set :',f1_score(y_test, ada_prediction))\n    print('classification report on test using Extra tree ',classification_report(y_test,ada_prediction))","bd5d343b":"rf_prob=ada.predict_proba(x_train)\nrf_prob=rf_prob[:,1]\nfalse_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_train, rf_prob)\nprint('auc_score for ADAboost : (train): ', roc_auc_score(y_train, rf_prob))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(train) -ADAboost :')\nplt.plot(false_positive_rate1, true_positive_rate1)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nrf_prob_test=ada.predict_proba(x_test)\nrf_prob_test=rf_prob_test[:,1]\nfalse_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, rf_prob_test)\nprint('auc_score for ADAboost (test): ', roc_auc_score(y_test, rf_prob_test))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(test) - ADAboost : ')\nplt.plot(false_positive_rate2, true_positive_rate2)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","d2cb4e4d":"lr = AdaBoostClassifier(n_estimators=100)\nscores = cross_val_score(lr, x_train, y_train, cv=3, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","d00f6a22":"df_train_bin=df_train_new","666cb7ea":"from sklearn.preprocessing import LabelBinarizer\nle1 = preprocessing.LabelBinarizer()\nle1.fit(df_train_new['term'])\nlist(le1.classes_)\ndf_train_new['term'] = le1.transform(df_train_new['term'])\ndf_train_new.head()","1311754d":"le1 = preprocessing.LabelBinarizer()\nle1.fit(df_train_new['grade'])\nlist(le1.classes_)\ndf_train_new['grade'] = le1.transform(df_train_new['grade'])\ndf_train_new.head()","18bdc141":"le1 = preprocessing.LabelBinarizer()\nle1.fit(df_train_new['sub_grade'])\nlist(le1.classes_)\ndf_train_new['sub_grade'] = le1.transform(df_train_new['sub_grade'])\ndf_train_new.head()","d18ab09f":"le1 = preprocessing.LabelBinarizer()\nle1.fit(df_train_new['home_ownership'])\nlist(le1.classes_)\ndf_train_new['home_ownership'] = le1.transform(df_train_new['home_ownership'])\ndf_train_new.head()","2d9d4f82":"le1 = preprocessing.LabelBinarizer()\nle1.fit(df_train_new['verification_status'])\nlist(le1.classes_)\ndf_train_new['verification_status'] = le1.transform(df_train_new['verification_status'])\ndf_train_new.head()","f0f1fd23":"le1 = preprocessing.LabelBinarizer()\nle1.fit(df_train_new['purpose'])\nlist(le1.classes_)\ndf_train_new['purpose'] = le1.transform(df_train_new['purpose'])\ndf_train_new.head()","b7a52488":"le1 = preprocessing.LabelBinarizer()\nle1.fit(df_train_new['addr_state'])\nlist(le1.classes_)\ndf_train_new['addr_state'] = le1.transform(df_train_new['addr_state'])\ndf_train_new.head()","3493b5bf":"le1 = preprocessing.LabelBinarizer()\nle1.fit(df_train_new['application_type'])\nlist(le1.classes_)\ndf_train_new['application_type'] = le1.transform(df_train_new['application_type'])\ndf_train_new.head()","eb1fca2b":"le1 = preprocessing.LabelBinarizer()\nle1.fit(df_train_new['pymnt_plan'])\nlist(le1.classes_)\ndf_train_new['pymnt_plan'] = le1.transform(df_train_new['pymnt_plan'])\ndf_train_new.head()","22666a0a":"le1 = preprocessing.LabelBinarizer()\nle1.fit(df_train_new['initial_list_status'])\nlist(le1.classes_)\ndf_train_new['initial_list_status'] = le1.transform(df_train_new['initial_list_status'])\ndf_train_new.head()","a3193b4d":"df_train_new.dtypes","444f95dd":"train = df_train_new[df_train_new['issue_d'] < '2015-6-01']\ntest = df_train_new[df_train_new['issue_d'] >= '2015-6-01']","ba437842":"del df_train_new['issue_d']","bcc1a95d":"x_train=train.drop(['default_ind','title','issue_d'],axis=1)\ny_train=train['default_ind']\nx_test=test.drop(['default_ind','title','issue_d'],axis=1)\ny_test=test['default_ind']","070ff69e":"log =LogisticRegression()\nlog.fit(x_train,y_train)","e0b9e81a":"#model on train using all the independent values in df\nlog_prediction = log.predict(x_train)\nlog_score= accuracy_score(y_train,log_prediction)\nprint('Accuracy score on train set using Logistic Regression :',log_score)","7dd54ef1":"print(confusion_matrix(y_train, log_prediction))\nfpr, tpr, thresholds = metrics.roc_curve(y_train,log_prediction)\nprint(\"AUC on train using Logistic regression :\",metrics.auc(fpr, tpr))\n\naverage_precision = average_precision_score(y_train, log_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\nprint('recall_score on train set :',recall_score(y_train, log_prediction))\nprint('F1_sccore on train set :',f1_score(y_train, log_prediction))\nprint('classification report on train using Logistic regression  ',\n      classification_report(y_train,log_prediction))","4ecf1bc8":"#model on train using all the independent values in df\nlog_prediction = log.predict(x_test)\nlog_score= accuracy_score(y_test,log_prediction)\nprint('accuracy score on test using Logisitic Regression :',log_score)","31352915":"print(confusion_matrix(y_test, log_prediction))\nfpr, tpr, thresholds = metrics.roc_curve(y_test,log_prediction)\nprint(\"AUC on test using Logistic regression :\",metrics.auc(fpr, tpr))\n\naverage_precision = average_precision_score(y_test, log_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\nprint('recall_score on test set :',recall_score(y_test, log_prediction))\nprint('F1_sccore on test set :',f1_score(y_test, log_prediction))\nprint('classification report on test using Logistic regression  ',classification_report(y_test,log_prediction))","8279a9b3":"lr_prob=log.predict_proba(x_train)\nlr_prob=lr_prob[:,1]\nfalse_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_train, lr_prob)\nprint('auc_score for Logistic Regression(train): ', roc_auc_score(y_train, lr_prob))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(train) - logistic regression')\nplt.plot(false_positive_rate1, true_positive_rate1)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nlr_prob_test=log.predict_proba(x_test)\nlr_prob_test=lr_prob_test[:,1]\nfalse_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, lr_prob_test)\nprint('auc_score for Logistic Regression(test): ', roc_auc_score(y_test, lr_prob_test))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(test) - logistic regression')\nplt.plot(false_positive_rate2, true_positive_rate2)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","269f8306":"lr = LogisticRegression()\nscores = cross_val_score(lr, x_train, y_train, cv=5, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","96c9ff8d":"xgboost = xgb.XGBClassifier(max_depth=3,n_estimators=300,learning_rate=0.05)","6754951a":"xgboost.fit(x_train,y_train)","513356fc":"#XGBoost model on the train set\nXGB_prediction = xgboost.predict(x_train)\nXGB_score= accuracy_score(y_train,XGB_prediction)\nprint('accuracy score on train using XGBoost ',XGB_score)","1751056e":"print(confusion_matrix(y_train, XGB_prediction))\nfpr, tpr, thresholds = metrics.roc_curve(y_train,XGB_prediction)\nprint(\"AUC on train using XGBClassifiers:\",metrics.auc(fpr, tpr))\n\naverage_precision = average_precision_score(y_train, XGB_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\nprint('recall_score on train set :',recall_score(y_train, XGB_prediction))\nprint('F1_sccore on train set :',f1_score(y_train, XGB_prediction))\nprint('classification report on train using XGBoost  ')\nprint(classification_report(y_train,XGB_prediction))","582eb9ee":"#XGBoost model on the test\nXGB_prediction = xgboost.predict(x_test)\nXGB_score= accuracy_score(y_test,XGB_prediction)\nprint('accuracy score on test using XGBoost :',XGB_score)","4e2f3b2f":"print(confusion_matrix(y_test, XGB_prediction))\nfpr, tpr, thresholds = metrics.roc_curve(y_test,XGB_prediction)\nprint(\"AUC on test using XGBClassifiers:\",metrics.auc(fpr, tpr))\n\naverage_precision = average_precision_score(y_test, XGB_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\nprint('recall_score on test set :',recall_score(y_test, XGB_prediction))\nprint('F1_sccore on test set :',f1_score(y_test, XGB_prediction))\nprint('classification report on test using XGBoost  ')\nprint(classification_report(y_test,XGB_prediction))","9bbcb40c":"xg_prob=xgboost.predict_proba(x_train)\nxg_prob=xg_prob[:,1]\nfalse_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_train, xg_prob)\nprint('auc_score for Xgboost: (train): ', roc_auc_score(y_train, xg_prob))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(train) - XGBoost ')\nplt.plot(false_positive_rate1, true_positive_rate1)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nxg_prob_test=xgboost.predict_proba(x_test)\nxg_prob_test=xg_prob_test[:,1]\nfalse_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, xg_prob_test)\nprint('auc_score for Xgboost(test): ', roc_auc_score(y_test, xg_prob_test))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(test) - XGBoost ')\nplt.plot(false_positive_rate2, true_positive_rate2)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","cc6df0b1":"xg = xgb.XGBClassifier()\nscores = cross_val_score(xg, x_test, y_test, cv=5, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","fdf00c26":"rfc2=RandomForestClassifier(n_estimators=100)\nrfc2.fit(x_train,y_train)","ec49a408":"#model on train using all the independent values in df\nrfc_prediction = rfc2.predict(x_train)\nrfc_score= accuracy_score(y_train,rfc_prediction)\nprint('accuracy Score on train using RandomForest :',rfc_score)","6a315b31":"print(confusion_matrix(y_train, rfc_prediction))\nfpr, tpr, thresholds = metrics.roc_curve(y_train,rfc_prediction)\nprint(\"AUC on train using RandomForest :\",metrics.auc(fpr, tpr))\n\naverage_precision = average_precision_score(y_train, rfc_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\nprint('recall_score on train set :',recall_score(y_train, rfc_prediction))\nprint('F1_sccore on train set :',f1_score(y_train, rfc_prediction))\nprint('classification Report on  train using RandomForest :')\nprint(classification_report(y_train,rfc_prediction))","53856270":"#model on test using all the indpendent values in df\nrfc_prediction = rfc2.predict(x_test)\nrfc_score= accuracy_score(y_test,rfc_prediction)\nprint('accuracy score on test using RandomForest ',rfc_score)","5907144e":"print(confusion_matrix(y_test, rfc_prediction))\nfpr, tpr, thresholds = metrics.roc_curve(y_test,rfc_prediction)\nprint(\"AUC on test using RandomForest :\",metrics.auc(fpr, tpr))\n\naverage_precision = average_precision_score(y_test, rfc_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\nprint('recall_score on test set :',recall_score(y_test, rfc_prediction))\nprint('F1_sccore on test set :',f1_score(y_test, rfc_prediction))\nprint('classification Report on  test using RandomForest :')\nprint(classification_report(y_test,rfc_prediction))","62cdf12b":"rf_prob=rfc2.predict_proba(x_train)\nrf_prob=rf_prob[:,1]\nfalse_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_train, rf_prob)\nprint('auc_score for Random Forest : (train): ', roc_auc_score(y_train, rf_prob))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(train) - Random Forest :')\nplt.plot(false_positive_rate1, true_positive_rate1)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nrf_prob_test=rfc2.predict_proba(x_test)\nrf_prob_test=rf_prob_test[:,1]\nfalse_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, rf_prob_test)\nprint('auc_score for Random forest (test): ', roc_auc_score(y_test, rf_prob_test))\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(test) - Random Forest : ')\nplt.plot(false_positive_rate2, true_positive_rate2)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","8d75f340":"lr = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(lr, x_train, y_train, cv=5, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","f7b8f922":"# Data Description:\nTrain.csv : 855969 x 73 [including headers] ","0f76b54b":"## Kfold crossValiddation","22c4f9fe":"# Loading the train and test data-set using pandas.read_table\n\n","d3a9d2ce":"## Classification report","955335fa":"# Data Visualization\n\nData visualization is the graphic representation of data. It involves producing images that communicate relationships among the represented data to viewers of the images. This communication is achieved through the use of a systematic mapping between graphic marks and data values in the creation of the visualization","d1e7a048":"## Feature Importance graph","e08fcc43":"## Kfold cross validation","f7043669":"## ROC Curve ","1b075bab":"## Label Encoding ","d2afc53b":"## ROC Curve","c31bc1b9":"## KFold Cross Validation ","000051fa":"# RandomForestclassifier on BinaryEncoded dataset","70cda50b":"# RandomForestClassifier\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.Random decision forests correct for decision trees' habit of overfitting to their training set.","13a79ad0":"## ROC Curve","45b8df8d":"## ROC Curve","f263a187":"## Train Test Split ","57770e79":"# XGBoost Algorithm\n\nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.","3ca7e294":"# LogisticRegression \n\nLogistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass\/fail which is represented by an indicator variable, where the two values are labeled \"0\" and \"1\". In the logistic model, the log-odds (the logarithm of the odds) for the value labeled \"1\" is a linear combination of one or more independent variables (\"predictors\"); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"),","dd11ff85":"## ROC Curve","23d66ad8":"# Train And Test Split ","c258140f":"## ROC Curve","60f56fde":"## Kfold Cross Validation","1303f796":"## ROC Curve","bc4a4e78":"# AUC \nCompute Area Under the Curve (AUC) using the trapezoidal rule","2c33d8e9":"# ExtraTreeClassifier\n\nEach Decision Tree in the Extra Trees Forest is constructed from the original training sample. Then, at each test node, Each tree is provided with a random sample of k features from the feature-set from which each decision tree must select the best feature to split the data based on some mathematical criteria (typically the Gini Index). This random sample of features leads to the creation of multiple de-correlated decision trees.","5bb748d5":"## Feature Importance Graph","c1f9e78b":"## Kfold Cross Validation","c965614a":"## Kfold Cross Validation","0f6d0d24":"# AdaBoostClassifier\n\nAn AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.","62be6a09":"# average precision recall score\n","3c7a3069":"# BANK LENDING","6435451f":"## Kfold Cross Validation","8148d8b1":" # F1 Score\nCompute the F1 score, also known as balanced F-score or F-measure\n\nThe F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n\nF1 = 2 * (precision * recall) \/ (precision + recall)","c04e3aa1":"# recall score\n\nThe recall is the ratio tp \/ (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n\nThe best value is 1 and the worst value is 0.","605e6882":"## Kfold Cross Validation","f7c07ca7":"# Binary Encoding","c965c6ff":"<h3 style='padding: 10px'>Comparison Table (LABEL ENCODING)<\/h2><table border-style:solid; class='table table-striped'> <thead> <tr> <th>Algorithm Used<\/th> <th>Accuracy Score On Train<\/th> <th>Accuracy Score On Test<\/th><\/tr> <\/thead> <tbody> <tr> <th scope='row'>XGBoost Classifier <\/th> <td>0.997<\/td> <td>0.781<\/td><\/tr> \n    <tr> <th scope='row'>Random Forest Classifier<\/th> <td>0.995<\/td> <td>0.391<\/td><\/tr> <tr> \n    <th scope='row'>Logisitic Regresion<\/th> <td>0.996<\/td> <td>0.998\n    <\/td><\/tr> <tr><th scope='row'>Decision Tree Classifier<\/th> <td>1.0<\/td> <td>0.30<\/td><\/tr>\n    <tr><th scope='row'>Extra tree classifier<\/th><td>1.0<\/td><td>0.612<\/td><\/tr>\n    <tr><th scope='row'>ADA boost classifier<\/th><td>0.996<\/td><td>0.862<\/td><\/tr>\n    <\/tbody> <\/table>","1649c105":"## Logisitic Regression on Binary encoded Dataset\n","ded28deb":"## ROC Curve ","6d9da396":"# Decision Tree CLassifier\n\nA decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\n\nDecision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.","c87a1319":"##  Correlation Matrix\nWhen two sets of data are strongly linked together we say they have a High Correlation.\n\nThe word Correlation is made of Co- (meaning \"together\"), and Relation\n\nCorrelation is Positive when the values increase together, and\nCorrelation is Negative when one value decreases as the other increases\nA correlation is assumed to be linear (following a line).\n\ncorrelation examples\nCorrelation can have a value:\n\n1 is a perfect positive correlation\n0 is no correlation (the values don't seem linked at all)\n-1 is a perfect negative correlation\nThe value shows how good the correlation is (not how steep the line is), and if it is positive or negative.","a5423ca2":"# XGBoost on Binary Encoded data","2da55b7e":"Objective :This is a binary classification where you need predict custusmer will default","e9311ee7":"## **Please upvote if you like!!!**","3639c815":"## Task","bf1904d5":"<h2 style='padding: 10px'>Comparison Table (BINARY ENCODING)<\/h2><table border-style:solid; class='table table-striped'> <thead> <tr> <th>Algorithm Used<\/th> <th>Accuracy Score On Train<\/th> <th>Accuracy Score On Test<\/th><\/tr> <\/thead> <tbody> <tr> <th scope='row'>XGBoost Classifier <\/th> <td>0.997<\/td> <td>0.594<\/td><\/tr> \n    <tr> <th scope='row'>Random Forest Classifier<\/th> <td>0.999<\/td> <td>0.358<\/td><\/tr> <tr> \n    <th scope='row'>Logisitic Regresion<\/th> <td>0.996<\/td> <td>0.998\n    <\/tbody> <\/table>","95f6bf37":"From all above analyis we can conclude that after binary encoding of dataset and applying logisitic regression model gives best results with accuracy score 0.996 on train and 0.998 on train. \n\n\nHence logistic model can be used for further predicting.","67ed11a0":"# confusion matrix\nA confusion matrix is a table that is often used to describe the performance of a classification model (or \u201cclassifier\u201d) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.","a677f974":"## ROC Curve","905f6fed":"## KcrossFold Validation","c607c324":"# Conclusion"}}