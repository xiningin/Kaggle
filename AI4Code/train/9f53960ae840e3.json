{"cell_type":{"7b6b05ea":"code","a2e8e152":"code","e0696f92":"code","dcacccee":"code","5e6a3dad":"code","19b7c462":"code","def5cb54":"code","366d4ed4":"code","af2ed24e":"code","8aca287c":"code","ca03593d":"code","dbffd81b":"code","0e373ba0":"code","a1095be3":"code","d94c6f83":"code","336034be":"code","480c3482":"code","9955a7c6":"code","2e87ca6f":"code","84ee4430":"code","8a09c100":"code","e56e43ed":"code","fb908e42":"code","3e206f2d":"code","2e9c5a27":"code","b0b3f8e8":"code","d8765c26":"code","335288ad":"code","ed0eb23b":"code","03b333ad":"code","39b5e4c7":"code","77dec940":"code","79d3575c":"code","c36ac907":"code","3b4fc166":"code","164ac12a":"code","ca669e3c":"markdown","247192e4":"markdown","fcd9f597":"markdown","658876ca":"markdown","3167b22f":"markdown","cc50a2d9":"markdown","bd173838":"markdown","d5fa6462":"markdown","07f47215":"markdown","df1ff73e":"markdown","cc87a899":"markdown","cd62864d":"markdown","7dbf45cd":"markdown","237c32aa":"markdown","8c5e5ef4":"markdown","14338141":"markdown","ee7c2d46":"markdown","a5e0578f":"markdown","63d17fb7":"markdown","60010688":"markdown","3296c162":"markdown","43f0347f":"markdown","c6031e1c":"markdown","037d1ceb":"markdown","768c6032":"markdown","ff7176f9":"markdown","59d06edc":"markdown","2fccacd7":"markdown","0146dad1":"markdown","ad21f161":"markdown","92a0d9df":"markdown","c67e2569":"markdown","7f4ec2be":"markdown","38d5627d":"markdown","7c31ce69":"markdown","8e362954":"markdown","4000122a":"markdown","1ae543fb":"markdown","b67dd821":"markdown","b0645250":"markdown","45519a87":"markdown","ae968ea9":"markdown","fd8553a6":"markdown"},"source":{"7b6b05ea":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scikitplot as skplt\nimport warnings\nimport shap\n\nshap.initjs()\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgbm\nimport xgboost as xgb\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, classification_report\nfrom imblearn.pipeline import make_pipeline\n\n%matplotlib inline\nnp.random.seed(42)\nwarnings.filterwarnings(\"ignore\")\npd.options.display.float_format = '{:.3f}'.format\nsns.set_style(\"whitegrid\", {'axes.grid' : False})","a2e8e152":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\nprint(df.shape)","e0696f92":"df.head()","dcacccee":"df.describe()","5e6a3dad":"df.info()","19b7c462":"print('Verificando o total')\nprint('Total missing:', df.isnull().sum().max())\nprint('-------------------------------')\nprint('if (missing > 0): Verificar a porcentagem de missing em cada vari\u00e1vel')\npercent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_value_df = pd.DataFrame({'column_name': df.columns,\n                                 'percent_missing': percent_missing})\nprint(missing_value_df.to_string(index=False))\n","def5cb54":"sns.countplot('Class',data=df).set_title('Target Distribution\\n[0 - No Fraud]  [1 - Fraud]', size=15)\nplt.show()","366d4ed4":"print('No Fraud:', df[df['Class'] == 0].shape)\nprint('Fraud:', df[df['Class'] == 1].shape)","af2ed24e":"print('Time - Not Fraud')\nprint(df.Time[df['Class'] == 0].describe())\nprint('--------------------------')\nprint('Time - Fraud')\nprint(df.Time[df['Class'] == 1].describe())\n\nf, axes = plt.subplots(1, 2, figsize=(25, 8))\nsns.boxplot(x=\"Class\", y=\"Time\", ax=axes[0], data=df).set_title('Time BoxPlot\\n[0 - No Fraud]  [1 - Fraud]', size=15)\nsns.distplot(df.Time[df['Class'] == 1], ax=axes[1], bins=50, label='Fraud', color='r').set_title('Transactions on the variable Time', size=15)\nsns.distplot(df.Time[df['Class'] == 0], ax=axes[1], bins=50, label='Not Fraud', color='b')\nplt.legend()\nplt.show()","8aca287c":"df['Hour'] = np.ceil(df['Time']\/3600).mod(24)\n\nplt.figure(figsize=(17, 6))\nsns.distplot(df.Hour[df['Class'] == 1], bins=50, label='Fraud', color='r').set_title('Transactions on the variable Hour', size=15)\nsns.distplot(df.Hour[df['Class'] == 0], bins=50, label='Not Fraud', color='b')\nplt.xticks(range(0,24))\nplt.legend()\nplt.show()","ca03593d":"print('Amount - Not Fraud')\nprint(df.Amount[df['Class'] == 0].describe())\nprint('----------------------------')\nprint('Amount - Fraud')\nprint(df.Amount[df['Class'] == 1].describe())\n\n\nf, axes = plt.subplots(1, 2, figsize=(25, 8))\nsns.boxplot(x=\"Class\", y=\"Amount\", ax=axes[0], data=df).set_title('Amount BoxPlot\\n[0 - No Fraud]  [1 - Fraud]', size=15)\nsns.distplot(df.Amount[df['Class'] == 1], ax=axes[1], bins=50, label='Fraud', color='r').set_title('Transactions on the variable Amount', size=15)\nsns.distplot(df.Amount[df['Class'] == 0], ax=axes[1], bins=50, label='Not Fraud', color='b')\nplt.legend()\nplt.show()","dbffd81b":"f, axes = plt.subplots(2, 2, figsize=(20,10))\n\nsns.scatterplot(df.Time[df['Class'] == 1], df.Amount[df['Class'] == 1], label='Fraud', ax=axes[0,0], color ='r').set_title('Transactions on the variable Amount and Time')\nsns.scatterplot(df.Time[df['Class'] == 0], df.Amount[df['Class'] == 0],  label='Not Fraud', ax=axes[1,0], color = 'b')\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\n\nsns.scatterplot(df.Hour[df['Class'] == 1], df.Amount[df['Class'] == 1], label='Fraud', ax=axes[0,1], color ='r').set_title('Transactions on the variable Amount and Hour')\nsns.scatterplot(df.Hour[df['Class'] == 0], df.Amount[df['Class'] == 0],  label='Not Fraud', ax=axes[1, 1], color = 'b')\nplt.xlabel('Hour')\nplt.ylabel('Amount')\nplt.show()","0e373ba0":"plt.figure()\nfig, ax = plt.subplots(7,4,figsize=(20,30))\n\ni = 0\nfor c in df.columns[1:-3]:\n    i += 1\n    plt.subplot(7,4,i)\n    sns.kdeplot(df.loc[df['Class'] == 0][c],label=\"Not Fraud\")\n    sns.kdeplot(df.loc[df['Class'] == 1][c],label=\"Fraud\")\n    plt.xlabel(c, fontsize=11)\n    locs, labels = plt.xticks()\nplt.show();","a1095be3":"cols = list(df.columns.values)\ncols.pop(cols.index('Time'))\ncols.pop(cols.index('Hour'))\ncols.pop(cols.index('Amount'))\ndf = df[['Time','Hour', 'Amount'] + cols]","d94c6f83":"corr = df.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.figure(figsize=(16,11))\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values).set_title('Correlation between all features', size=22)\nplt.show()","336034be":"corr1 = df[df['Class'] == 1].corr()\ncorr0 = df[df['Class'] == 0].corr()\n\nmask = np.zeros_like(corr1, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nf, axes = plt.subplots(1, 2, figsize=(30,11))\nsns.heatmap(corr1, mask=mask, cmap=cmap, ax=axes[0], vmin=-1, vmax=1,\n            xticklabels=corr1.columns.values,\n            yticklabels=corr1.columns.values).set_title('Fraud Correlation', size=22)\nsns.heatmap(corr0, mask=mask, cmap=cmap, ax=axes[1], vmin=-1, vmax=1,\n            xticklabels=corr0.columns.values,\n            yticklabels=corr0.columns.values).set_title('Not Fraud Correlation', size=22)\nplt.show()","480c3482":"X = df.drop('Class', axis=1)\ny = df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42, stratify = y)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, random_state=42, stratify = y_train)\n\nprint('Train:', X_train.shape)\nprint('Test:', X_test.shape)\nprint('Valid:', X_valid.shape)","9955a7c6":"sm = SMOTE(sampling_strategy='minority', random_state=42)\nX_train_smote, y_train_smote = sm.fit_sample(X_train, y_train)\n\nprint('X_smote shape:', X_train_smote.shape)\nprint('Distribui\u00e7\u00e3o da vari\u00e1vel alvo ap\u00f3s aplica\u00e7\u00e3o do m\u00e9todo SMOTE:', np.unique(y_train_smote, return_counts=True))","2e87ca6f":"adasyn = ADASYN(sampling_strategy='minority', random_state=42)\nX_train_adasyn, y_train_adasyn = adasyn.fit_sample(X_train, y_train)\n\nprint('X_adasyn shape:', X_train_adasyn.shape)\nprint('Distribui\u00e7\u00e3o da vari\u00e1vel alvo ap\u00f3s aplica\u00e7\u00e3o do m\u00e9todo ADASYN:', np.unique(y_train_adasyn, return_counts=True))","84ee4430":"random_grid = {\n    'bootstrap': [True, False],\n    'max_depth': [1, 2, 3, 5, 10, None],\n    'max_features': ['auto', 'sqrt'],\n    'n_estimators': [50, 100, 200],\n    \"criterion\": [\"entropy\", \"gini\"],\n    \"class_weight\": [None, 'balanced']\n}\n\nclr_rf = RandomForestClassifier(random_state=42)\ngs = RandomizedSearchCV(clr_rf, param_distributions=random_grid, n_iter=2, verbose=0, cv=3, scoring='roc_auc', random_state=42)\ngs.fit(X_train, y_train)\n\nprint('RANDOM FOREST')\nprint(\"Best Score: {}\".format(gs.best_score_))\nprint(\"Best Parameters: {}\\n\".format(gs.best_params_))","8a09c100":"# Unbalanced\nclr_best_rf = RandomForestClassifier(**gs.best_params_)\nclr_best_rf.fit(X_train, y_train)\npredictions_rf = clr_best_rf.predict(X_test)\nroc_auc_rf = np.round(roc_auc_score(y_test, predictions_rf), 2)\n\nprint('Random Forest Classifier with Unbalanced Data')\nprint('ROC AUC score on test data:', roc_auc_rf)\nprint('Classification Report:')\nprint(classification_report(y_test, predictions_rf))\nprint('------------------------------------------------------')\n    \n# SMOTE \nif gs.best_params_['class_weight'] == 'balanced':\n    gs.best_params_['class_weight'] = None\n\nclr_best_rf_smote = RandomForestClassifier(**gs.best_params_)\nclr_best_rf_smote.fit(X_train_smote, y_train_smote)\npredictions_rf_smote = clr_best_rf_smote.predict(X_test)\nroc_auc_rf_smote = np.round(roc_auc_score(y_test, predictions_rf_smote), 2)\n\nprint('Random Forest Classifier with SMOTE')\nprint('ROC AUC score on test data:', roc_auc_rf_smote)\nprint('Classification Report:')\nprint(classification_report(y_test, predictions_rf_smote))\nprint('------------------------------------------------------')\n\n# ADASYN\nif gs.best_params_['class_weight'] == 'balanced':\n    gs.best_params_['class_weight'] = None\n\nclr_best_rf_adasyn = RandomForestClassifier(**gs.best_params_)\nclr_best_rf_adasyn.fit(X_train_adasyn, y_train_adasyn)\npredictions_rf_adasyn = clr_best_rf_adasyn.predict(X_test)\nroc_auc_rf_adasyn = np.round(roc_auc_score(y_test, predictions_rf_adasyn), 2)\n\nprint('Random Forest Classifier with ADASYN')\nprint('ROC AUC score on test data:', roc_auc_rf_adasyn)\nprint('Classification Report:')\nprint(classification_report(y_test, predictions_rf_adasyn))","e56e43ed":"plt.figure(figsize=(30,25))\n\nax1 = plt.subplot(331)\nskplt.metrics.plot_confusion_matrix(y_test, predictions_rf, normalize=False, ax=ax1)\nax1.set_yticklabels(['Not Fraud', 'Fraud'])\nax1.set_xticklabels(['Not Fraud', 'Fraud']) \nax1.set_xlabel('Predicted label\\nroc auc={:0.2f}; missclass={:0.2f}'.format(roc_auc_rf, 1 - roc_auc_rf))\nax1.set_title('Confusion Matrix (Unbalanced)')\n\ny_probas = clr_best_rf.predict_proba(X_test)\nax2 = plt.subplot(332)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10,8), ax=ax2)\nax2.set_title('ROC Curves (Unbalanced)')\n\nrf_fi = pd.DataFrame({'Feature': X.columns, 'Feature importance': clr_best_rf.feature_importances_})\nrf_fi = rf_fi.sort_values(by='Feature importance', ascending=False)\nax3 = plt.subplot(333)\nplt.title('Features importance (Unbalanced)',fontsize=14)\nsns.barplot(x='Feature importance',y='Feature',data=rf_fi)\n\nax4 = plt.subplot(334)\nskplt.metrics.plot_confusion_matrix(y_test, predictions_rf_smote, normalize=False, ax=ax4)\nax4.set_yticklabels(['Not Fraud', 'Fraud'])\nax4.set_xticklabels(['Not Fraud', 'Fraud']) \nax4.set_xlabel('Predicted label\\nroc auc={:0.2f}; missclass={:0.2f}'.format(roc_auc_rf_smote, 1 - roc_auc_rf_smote))\nax4.set_title('Confusion Matrix (SMOTE)')\n\ny_probas = clr_best_rf_smote.predict_proba(X_test)\nax5 = plt.subplot(335)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10,8), ax=ax5)\nax5.set_title('ROC Curves (SMOTE)')\n\nrf_fi = pd.DataFrame({'Feature': X.columns, 'Feature importance': clr_best_rf_smote.feature_importances_})\nrf_fi = rf_fi.sort_values(by='Feature importance', ascending=False)\nax6 = plt.subplot(336)\nplt.title('Features importance (SMOTE)',fontsize=14)\nsns.barplot(x='Feature importance',y='Feature',data=rf_fi)\n\nax7 = plt.subplot(337)\nskplt.metrics.plot_confusion_matrix(y_test, predictions_rf_adasyn, normalize=False, ax=ax7)\nax7.set_yticklabels(['Not Fraud', 'Fraud'])\nax7.set_xticklabels(['Not Fraud', 'Fraud']) \nax7.set_xlabel('Predicted label\\nroc auc={:0.2f}; missclass={:0.2f}'.format(roc_auc_rf_adasyn, 1 - roc_auc_rf_adasyn))\nax7.set_title('Confusion Matrix (ADASYN)')\n\ny_probas = clr_best_rf_adasyn.predict_proba(X_test)\nax8 = plt.subplot(338)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10,8), ax=ax8)\nax8.set_title('ROC Curves (ADASYN)')\n\nrf_fi = pd.DataFrame({'Feature': X.columns, 'Feature importance': clr_best_rf_adasyn.feature_importances_})\nrf_fi = rf_fi.sort_values(by='Feature importance', ascending=False)\nax9 = plt.subplot(339)\nplt.title('Features importance (ADASYN)',fontsize=14)\nsns.barplot(x='Feature importance',y='Feature',data=rf_fi)\n\nplt.show() ","fb908e42":"balanced_weight = np.unique(y_train, return_counts=True)[1][0] \/ np.unique(y_train, return_counts=True)[1][1]\n\nfit_params = {\"early_stopping_rounds\" : 50, \n             \"eval_metric\" : 'auc', \n             \"eval_set\" : [(X_valid, y_valid)],\n             \"verbose\" :  0}\n\nrandom_grid = {\n        'learning_rate' : [0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4],\n        'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n        'n_estimators' : [100, 200, 300, 400, 500, 600, 800, 1000, 1500, 2000, 3000, 5000],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': np.linspace(0.6, 1, 10),\n        'max_depth': [1, 2, 3, 4, 5, 6, 7],\n        'scale_pos_weight' : [1, balanced_weight]\n        }\n\nclr_xgb = XGBClassifier(objective='binary:logistic', tree_method='gpu_hist', random_state=42)\ngs = RandomizedSearchCV(clr_xgb, param_distributions=random_grid, n_iter=30, verbose=0, cv=5, scoring='roc_auc', random_state=42)\ngs.fit(X_train, y_train, **fit_params)\n\nprint('XGBOOST')\nprint(\"Best Score: {}\".format(gs.best_score_))\nprint(\"Best Parameters: {}\\n\".format(gs.best_params_))","3e206f2d":"opt_parameters =  gs.best_params_\n\n# Unbalanced\nclr_best_xgb = XGBClassifier(**clr_xgb.get_params())\nclr_best_xgb.set_params(**opt_parameters)\nclr_best_xgb.fit(X_train, y_train)\npredictions_xgb = clr_best_xgb.predict(X_test)\nroc_auc_xgb = np.round(roc_auc_score(y_test, predictions_xgb), 2)\n\nprint('XGBoost with Unbalanced Data:')\nprint('ROC AUC score on test data:', roc_auc_xgb)\nprint('Classification Report:')\nprint(classification_report(y_test, predictions_xgb))\nprint('------------------------------------------------------')\n\n\n# SMOTE\nif opt_parameters['scale_pos_weight'] != 1:\n    opt_parameters['scale_pos_weight'] = 1\n\nclr_best_xgb_smote = XGBClassifier(**clr_xgb.get_params())\nclr_best_xgb_smote.set_params(**opt_parameters)\nclr_best_xgb_smote.fit(X_train_smote, y_train_smote)\npredictions_xgb_smote = clr_best_xgb_smote.predict(X_test.values)\nroc_auc_xgb_smote = np.round(roc_auc_score(y_test, predictions_xgb_smote), 2)\n\nprint('XGBoost with SMOTE:')\nprint('ROC AUC score on test data:', roc_auc_xgb_smote)\nprint('Classification Report:')\nprint(classification_report(y_test, predictions_xgb_smote))\nprint('------------------------------------------------------')\n\n\n# ADASYN\nif opt_parameters['scale_pos_weight'] != 1:\n    opt_parameters['scale_pos_weight'] = 1\n\nclr_best_xgb_adasyn = XGBClassifier(**clr_xgb.get_params())\nclr_best_xgb_adasyn.set_params(**opt_parameters)\nclr_best_xgb_adasyn.fit(X_train_adasyn, y_train_adasyn)\npredictions_xgb_adasyn = clr_best_xgb_adasyn.predict(X_test.values)\nroc_auc_xgb_adasyn = np.round(roc_auc_score(y_test, predictions_xgb_adasyn), 2)\n\nprint('XGBoost with ADASYN:')\nprint('ROC AUC score on test data:', roc_auc_xgb_adasyn)\nprint('Classification Report:')\nprint(classification_report(y_test, predictions_xgb_adasyn))","2e9c5a27":"plt.figure(figsize=(25,20))\n\nax1 = plt.subplot(331)\nskplt.metrics.plot_confusion_matrix(y_test, predictions_xgb, normalize=False, ax=ax1)\nax1.set_yticklabels(['Not Fraud', 'Fraud'])\nax1.set_xticklabels(['Not Fraud', 'Fraud']) \nax1.set_xlabel('Predicted label\\nroc auc={:0.2f}; missclass={:0.2f}'.format(roc_auc_xgb, 1 - roc_auc_xgb))\nax1.set_title('Confusion Matrix (Unbalanced)')\n\ny_probas = clr_best_xgb.predict_proba(X_test)\nax2 = plt.subplot(332)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10,8), ax=ax2)\nax2.set_title('ROC Curves (Unbalanced)')\n\nrf_fi = pd.DataFrame({'Feature': X.columns, 'Feature importance': clr_best_xgb.feature_importances_})\nrf_fi = rf_fi.sort_values(by='Feature importance', ascending=False)\nax3 = plt.subplot(333)\nplt.title('Features importance (Unbalanced)',fontsize=14)\nsns.barplot(x='Feature importance',y='Feature',data=rf_fi)\n\nax4 = plt.subplot(334)\nskplt.metrics.plot_confusion_matrix(y_test, predictions_xgb_smote, normalize=False, ax=ax4)\nax4.set_yticklabels(['Not Fraud', 'Fraud'])\nax4.set_xticklabels(['Not Fraud', 'Fraud']) \nax4.set_xlabel('Predicted label\\nroc auc={:0.2f}; missclass={:0.2f}'.format(roc_auc_xgb_smote, 1 - roc_auc_xgb_smote))\nax4.set_title('Confusion Matrix (SMOTE)')\n\ny_probas = clr_best_xgb_smote.predict_proba(X_test.values)\nax5 = plt.subplot(335)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10,8), ax=ax5)\nax5.set_title('ROC Curves (SMOTE)')\n\nrf_fi = pd.DataFrame({'Feature': X.columns, 'Feature importance': clr_best_xgb_smote.feature_importances_})\nrf_fi = rf_fi.sort_values(by='Feature importance', ascending=False)\nax6 = plt.subplot(336)\nplt.title('Features importance (SMOTE)',fontsize=14)\nsns.barplot(x='Feature importance',y='Feature',data=rf_fi)\n\nax7 = plt.subplot(337)\nskplt.metrics.plot_confusion_matrix(y_test, predictions_xgb_adasyn, normalize=False, ax=ax7)\nax7.set_yticklabels(['Not Fraud', 'Fraud'])\nax7.set_xticklabels(['Not Fraud', 'Fraud']) \nax7.set_xlabel('Predicted label\\nroc auc={:0.2f}; missclass={:0.2f}'.format(roc_auc_xgb_adasyn, 1 - roc_auc_xgb_adasyn))\nax7.set_title('Confusion Matrix (ADASYN)')\n\ny_probas = clr_best_xgb_adasyn.predict_proba(X_test.values)\nax8 = plt.subplot(338)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10,8), ax=ax8)\nax8.set_title('ROC Curves (ADASYN)')\n\nrf_fi = pd.DataFrame({'Feature': X.columns, 'Feature importance': clr_best_xgb_adasyn.feature_importances_})\nrf_fi = rf_fi.sort_values(by='Feature importance', ascending=False)\nax9 = plt.subplot(339)\nplt.title('Features importance (ADASYN)',fontsize=14)\nsns.barplot(x='Feature importance',y='Feature',data=rf_fi)\n\nplt.show() ","b0b3f8e8":"fit_params = {\"early_stopping_rounds\" : 50, \n             \"eval_metric\" : 'binary', \n             \"eval_set\" : [(X_valid,y_valid)],\n             \"eval_names\": ['valid'],\n             \"verbose\": 0}\n\nrandom_test = {\n    'learning_rate' : [0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4],\n    'n_estimators' : [100, 200, 300, 400, 500, 600, 800, 1000, 1500, 2000, 3000, 5000],\n    'num_leaves': np.random.randint(6, 50), \n    'min_child_samples': np.random.randint(100, 500), \n    'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n    'subsample': np.linspace(0.5, 1, 100), \n    'max_depth': [-1, 1, 2, 3, 4, 5, 6, 7],\n    'colsample_bytree': np.linspace(0.6, 1, 10),\n    'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n    'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n    'scale_pos_weight' : [1, balanced_weight]\n    }\n\nclr_gbm = LGBMClassifier(objective='binary', metric='auc', tree_method='gpu_hist', random_state=42)\ngs = RandomizedSearchCV(clr_gbm, param_distributions=random_grid, n_iter=30, verbose=0, cv=5, scoring='roc_auc', random_state=42)\ngs.fit(X_train, y_train, **fit_params)\n\nprint('LIGHTGBM')\nprint(\"Best Score: {}\".format(gs.best_score_))\nprint(\"Best Parameters: {}\\n\".format(gs.best_params_))\n","d8765c26":"opt_parameters =  gs.best_params_\n\n# Unbalanced\nclr_best_gbm = LGBMClassifier(**clr_gbm.get_params())\nclr_best_gbm.set_params(**opt_parameters)\nclr_best_gbm.fit(X_train, y_train)\npredictions_gbm = clr_best_gbm.predict(X_test)\nroc_auc_gbm = np.round(roc_auc_score(y_test, predictions_gbm), 2)\n\nprint('LightGBM with Unbalanced Data:')\nprint('ROC AUC score on test data:', roc_auc_gbm)\nprint('Classification Report:')\nprint(classification_report(y_test, predictions_gbm))\nprint('------------------------------------------------------')\n\n\n# SMOTE\nif gs.best_params_['scale_pos_weight'] != 1:\n    gs.best_params_['scale_pos_weight'] = 1\n\nclr_best_gbm_smote = LGBMClassifier(**clr_gbm.get_params())\nclr_best_gbm_smote.set_params(**opt_parameters)\nclr_best_gbm_smote.fit(X_train_smote, y_train_smote)\npredictions_gbm_smote = clr_best_gbm_smote.predict(X_test.values)\nroc_auc_gbm_smote = np.round(roc_auc_score(y_test, predictions_gbm_smote), 2)\n\nprint('LightGBM with SMOTE:')\nprint('ROC AUC score on test data:', roc_auc_gbm_smote)\nprint('Classification Report:')\nprint(classification_report(y_test, predictions_gbm_smote))\nprint('------------------------------------------------------')\n\n\n# ADASYN\nif gs.best_params_['scale_pos_weight'] != 1:\n    gs.best_params_['scale_pos_weight'] = 1\n\nclr_best_gbm_adasyn = LGBMClassifier(**clr_gbm.get_params())\nclr_best_gbm_adasyn.set_params(**opt_parameters)\nclr_best_gbm_adasyn.fit(X_train_adasyn, y_train_adasyn)\npredictions_gbm_adasyn = clr_best_gbm_adasyn.predict(X_test.values)\nroc_auc_gbm_adasyn = np.round(roc_auc_score(y_test, predictions_gbm_adasyn), 2)\n\nprint('LightGBM with ADASYN:')\nprint('ROC AUC score on test data:', roc_auc_gbm_adasyn)\nprint('Classification Report:')\nprint(classification_report(y_test, predictions_gbm_adasyn))","335288ad":"plt.figure(figsize=(25,20))\n\nax1 = plt.subplot(331)\nskplt.metrics.plot_confusion_matrix(y_test, predictions_gbm, normalize=False, ax=ax1)\nax1.set_yticklabels(['Not Fraud', 'Fraud'])\nax1.set_xticklabels(['Not Fraud', 'Fraud']) \nax1.set_xlabel('Predicted label\\nroc auc={:0.2f}; missclass={:0.2f}'.format(roc_auc_gbm, 1 - roc_auc_gbm))\nax1.set_title('Confusion Matrix (Unbalanced)')\n\ny_probas = clr_best_gbm.predict_proba(X_test)\nax2 = plt.subplot(332)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10,8), ax=ax2)\nax2.set_title('ROC Curves (Unbalanced)')\n\nrf_fi = pd.DataFrame({'Feature': X.columns, 'Feature importance': clr_best_gbm.feature_importances_})\nrf_fi = rf_fi.sort_values(by='Feature importance', ascending=False)\nax3 = plt.subplot(333)\nplt.title('Features importance (Unbalanced)',fontsize=14)\nsns.barplot(x='Feature importance',y='Feature',data=rf_fi)\n\nax4 = plt.subplot(334)\nskplt.metrics.plot_confusion_matrix(y_test, predictions_gbm_smote, normalize=False, ax=ax4)\nax4.set_yticklabels(['Not Fraud', 'Fraud'])\nax4.set_xticklabels(['Not Fraud', 'Fraud']) \nax4.set_xlabel('Predicted label\\nroc auc={:0.2f}; missclass={:0.2f}'.format(roc_auc_gbm_smote, 1 - roc_auc_gbm_smote))\nax4.set_title('Confusion Matrix (SMOTE)')\n\ny_probas = clr_best_gbm_smote.predict_proba(X_test.values)\nax5 = plt.subplot(335)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10,8), ax=ax5)\nax5.set_title('ROC Curves (SMOTE)')\n\nrf_fi = pd.DataFrame({'Feature': X.columns, 'Feature importance': clr_best_gbm_smote.feature_importances_})\nrf_fi = rf_fi.sort_values(by='Feature importance', ascending=False)\nax6 = plt.subplot(336)\nplt.title('Features importance (SMOTE)',fontsize=14)\nsns.barplot(x='Feature importance',y='Feature',data=rf_fi)\n\nax7 = plt.subplot(337)\nskplt.metrics.plot_confusion_matrix(y_test, predictions_gbm_adasyn, normalize=False, ax=ax7)\nax7.set_yticklabels(['Not Fraud', 'Fraud'])\nax7.set_xticklabels(['Not Fraud', 'Fraud']) \nax7.set_xlabel('Predicted label\\nroc auc={:0.2f}; missclass={:0.2f}'.format(roc_auc_gbm_adasyn, 1 - roc_auc_gbm_adasyn))\nax7.set_title('Confusion Matrix (ADASYN)')\n\ny_probas = clr_best_gbm_adasyn.predict_proba(X_test.values)\nax8 = plt.subplot(338)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10,8), ax=ax8)\nax8.set_title('ROC Curves (ADASYN)')\n\nrf_fi = pd.DataFrame({'Feature': X.columns, 'Feature importance': clr_best_gbm_adasyn.feature_importances_})\nrf_fi = rf_fi.sort_values(by='Feature importance', ascending=False)\nax9 = plt.subplot(339)\nplt.title('Features importance (ADASYN)',fontsize=14)\nsns.barplot(x='Feature importance',y='Feature',data=rf_fi)\n\nplt.show() ","ed0eb23b":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(clr_best_xgb_smote, scoring='roc_auc', random_state=42).fit(X_test.values, y_test.values)\neli5.show_weights(perm, feature_names = X.columns.tolist())","03b333ad":"booster = clr_best_xgb_smote.get_booster()\nshap_values = booster.predict(xgb.DMatrix(X_test.values), pred_contribs=True)","39b5e4c7":"shap.summary_plot(shap_values[:,:-1], X_test, feature_names=X.columns)","77dec940":"shap.dependence_plot(\"V4\", shap_values[:,:-1], X_test)","79d3575c":"shap.dependence_plot(\"V14\", shap_values[:,:-1], X_test)","c36ac907":"import tensorflow.keras.backend as K\n\ndef preprocessing_fnn(data):\n    norm = (data - data.mean())\/data.std()\n    return norm\n\nX_train_fnn = preprocessing_fnn(X_train)\nX_test_fnn = preprocessing_fnn(X_test)\nX_valid_fnn = preprocessing_fnn(X_valid)\n\ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.mean((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n    return focal_loss_fixed\n\nn_inputs = X_train.shape[1]\n\ndef model_fnn(loss_f):\n    model = tf.keras.models.Sequential()\n    \n    model.add(tf.keras.layers.Dense(512, input_shape=(X_train.shape[1],)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation(\"relu\"))\n    #model.add(tf.keras.layers.Dropout(0.2))\n    \n    model.add(tf.keras.layers.Dense(256, kernel_initializer=\"he_normal\", use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation(\"relu\"))\n    #model.add(tf.keras.layers.Dropout(0.2))\n    \n    model.add(tf.keras.layers.Dense(128, kernel_initializer=\"he_normal\", use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation(\"relu\"))\n    #model.add(tf.keras.layers.Dropout(0.2))\n    \n    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n    \n    model.compile(loss=loss_f, optimizer=tf.keras.optimizers.Nadam())\n    \n    return model\n\nmodel_fnn_focal_loss_low_alpha = model_fnn(focal_loss())\nmodel_fnn_focal_loss_high_alpha = model_fnn(focal_loss(alpha=4.))\nmodel_fnn_binary_ce = model_fnn('binary_crossentropy')","3b4fc166":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", verbose=1, patience=10)\n\nmodel_fnn_focal_loss_low_alpha.fit(X_train_fnn, y_train, batch_size=1024, epochs=100, validation_data=(X_valid_fnn, y_valid),\n                                   callbacks=[early_stopping])\nprint(\"----------------------------------------------\")\nmodel_fnn_focal_loss_high_alpha.fit(X_train_fnn, y_train, batch_size=1024, epochs=100, validation_data=(X_valid_fnn, y_valid),\n                                   callbacks=[early_stopping])\nprint(\"----------------------------------------------\")\nmodel_fnn_binary_ce.fit(X_train, y_train, batch_size=1024, epochs=100, validation_data=(X_valid, y_valid),\n                                   callbacks=[early_stopping])\n\n","164ac12a":"preds_focal_loss_low_alpha = model_fnn_focal_loss_low_alpha.predict_classes(X_test, verbose=0)\nroc_auc_focal_loss_low_alpha = np.round(roc_auc_score(y_test, preds_focal_loss_low_alpha), 2)\n\npreds_focal_loss_high_alpha = model_fnn_focal_loss_high_alpha.predict_classes(X_test, verbose=0)\nroc_auc_focal_loss_high_alpha = np.round(roc_auc_score(y_test, preds_focal_loss_high_alpha), 2)\n\npreds_binary_ce = model_fnn_binary_ce.predict_classes(X_test, verbose=0)\nroc_auc_binary_ce = np.round(roc_auc_score(y_test, preds_binary_ce), 2)\n\n\nplt.figure(figsize=(25,20))\nax1 = plt.subplot(331)\nskplt.metrics.plot_confusion_matrix(y_test, preds_focal_loss_low_alpha, normalize=False, ax=ax1)\nax1.set_yticklabels(['Not Fraud', 'Fraud'])\nax1.set_xticklabels(['Not Fraud', 'Fraud']) \nax1.set_xlabel('Predicted label\\nroc auc={:0.2f}; missclass={:0.2f}'.format(roc_auc_focal_loss_low_alpha, 1 - roc_auc_focal_loss_low_alpha))\nax1.set_title('Confusion Matrix (Focal Loss with LOW Alpha)')\n\nax2 = plt.subplot(332)\nskplt.metrics.plot_confusion_matrix(y_test, preds_focal_loss_high_alpha, normalize=False, ax=ax2)\nax2.set_yticklabels(['Not Fraud', 'Fraud'])\nax2.set_xticklabels(['Not Fraud', 'Fraud']) \nax2.set_xlabel('Predicted label\\nroc auc={:0.2f}; missclass={:0.2f}'.format(roc_auc_focal_loss_high_alpha, 1 - roc_auc_focal_loss_high_alpha))\nax2.set_title('Confusion Matrix (Focal Loss with HIGH Alpha)')\n\nax3 = plt.subplot(333)\nskplt.metrics.plot_confusion_matrix(y_test, preds_binary_ce, normalize=False, ax=ax3)\nax3.set_yticklabels(['Not Fraud', 'Fraud'])\nax3.set_xticklabels(['Not Fraud', 'Fraud']) \nax3.set_xlabel('Predicted label\\nroc auc={:0.2f}; missclass={:0.2f}'.format(roc_auc_binary_ce, 1 - roc_auc_binary_ce))\nax3.set_title('Confusion Matrix (Binary CrossEntropy)')\n\nplt.show()","ca669e3c":"# <a id=\"1\">Introduction<\/a>\n\n#### Dataset\n\n*The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.*}\n\n*It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.*\n\n#### Approach\n\nThere are some approaches in the literature to address the problem of data imbalance. These approaches can be methods that deal with data, methods that deal with objective function (cost-sensitive models), and hybrid methods that deal with data and function at the same time. Random Undersampling and Oversampling are the most common methods that worked with the data. Models that have parameters that deal with data unbalance are cost-sensitive models, an example of this is the scale_pos_weight parameter of boost models. Hybrid methods, as far as I know, are methods used in neural network models, such as Large Margin Local Embedding, Deep over-sampling and Class rectification loss (CRL) and hard sample mining.\n\nIn this application scenario we will use the first two approaches with some observations. Firstly, the Random Unversampling method will not be used because it is believed that its application in this scenario will have a lot of information loss, since we will decrease the number of majority class observations from 28315 to only 492 minority class observations, which will degrade the model performance. Second, we use more sophisticated oversampling methods, specifically two:\n\n* SMOTE\n* ADASYN\n\nThese two methods generate synthetic instances. The basic difference between them is that ADASYN takes into account the instances that are most difficult to classify through a neighborhood algorithm while SMOTE generates synthetic instances without making any distinction. Finally, in relation to methods that deal with the objective function, we will use the parameters of the models that deal with data unbalance, however, the determination to use these parameters will be defined by RandomizedSearchCV.\n\n#### Models\n\nThe motivation for choosing the first three models is that for credit problems, boosting \/ tree gradient models make it possible to understand decision making through a tree and this model interpretability is important in this application scenario. Tree model parameters are defined by a RandomizedSearchCV on the unbalanced data and then applied to the three different data types that will be generated: The purely unbalanced data, the smote data and the adasyn data. No RandomizedSearchCV was again done to find the best parameters when trained on data modified by the SMOTE and ADASYN methods as it was taking too long and my hypothesis is that the parameters of models learned in unbalanced data will perform the same or better when applied to the modified data. . Of course, in a REAL scenario (since this is just my first experiment with kernels), the entire pipeline would be done to verify the best parameters.\n\nThe comparison with Random Forest was not fair because RandomizedSearchCV was set for only two iterations, since sklearn doesn't use gpu (unlike boost methods). It would take too long. It was used more as a baseline.\n\nAs for neural networks, I thought it would be a great opportunity to test new methods that deal with the objective function existing in the literature, since the problem of data imbalance is somewhat related to the subject of my master's degree. The idea is to verify the performance of neural networks with totally unbalanced data (ie without the use of sampling methods) using some loss function proposed in the literature. Only the **focal loss** was used.\n\n#### Metrics\n\nAccuracy is not a good metric for this problem, as the paradox of accuracy may occur, that is, given a relatively simple model, it would be much easier for this model to infer that all observations are real (not fraud) and would perform virtually 100%, which is not true. In this scenario, we will evaluate the performance of the models using as the main metric **roc auc**, besides the **confusion matrix**, **recall**, **precision** and **f1**.\n\n","247192e4":"# <a id=\"6\">Target Analysis<\/a>","fcd9f597":"The results obtained by the focal loss were not expected. I may have made a mistake that I could not identify, I will better check if it was a matter of implementing the loss function or architecture or this loss function does not really work for extreme unbalance issues. However, one thing to note is that depending on the value of the variable **alpha**, the model can distinguish the observations regarding class (0 or 1). The higher the alpha value, the more attention it gives to the minority class (more difficult to classify observations), the lower the value, the more attention to the majority class (easier to classify observations).\n\nIf you guys have some ideia what happened, please, let me know.","658876ca":"# <a id=\"12\"> Neural Networks <\/a>","3167b22f":"I used a loss function called focal loss that deals with unbalanced data and will compare it to the standard loss function for binary classification: binary crossentropy.\n\nFocal loss is a method that has been proposed to deal with class imbalance by reshaping the crossentropy function so that it reduces the weight of the loss attributed to easy-to-classify examples so that its contribution to total loss is small.","cc50a2d9":"## <a id=\"92\">XGBoost<\/a>","bd173838":"From the correlation matrix we can observe that none of the anonymous variables have a correlation with each other, this can be explained by the fact that they were generated by the PCA algorithm. The Class variable has some correlations between anonymous variables and no correlation with Time, Hour, and Amount variables. However, we will not overestimate model performance through graphical analysis before verifying its actual results when running the algorithms. This way, none of the variables will be dropped.\n\n**Important note:** In this test the experiments were not redone after analyzing the first results of the impact of the variables on the models, ie, no feature selection was made after the first result. This analysis is The First Result of experiment.","d5fa6462":"# <a id=\"5\">Checking missing values<\/a>","07f47215":"- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Load Libraries<\/a>  \n- <a href='#3'>Loading Dataset<\/a>  \n- <a href='#4'>Initial Basic Analysis<\/a>  \n- <a href='#5'>Checking missing values<\/a>\n- <a href='#6'>Target Analysis<\/a> \n- <a href='#7'>Data Analysis<\/a>\n- <a href='#8'>Data Preparation<\/a>\n    - <a href=\"81\">SMOTE Data Preparation<\/a>\n    - <a href=\"82\">ADASYN Data Preparation<\/a>\n- <a href='#9'>Tree-based Models<\/a>\n    - <a href='#91'>Random Forest Classifier<\/a> \n        - <a href='#911'>Results of Random Forest Classifier<\/a>\n    - <a href='#92'>XGBoost<\/a>\n        - <a href='#921'>Results of XGBoost<\/a>\n    - <a href='#93'>LightGBM<\/a>\n        - <a href='#931'>Results of LightGBM<\/a>\n- <a href=\"10\">Conclusion<\/a>\n- <a href='#11'> Interpretability of the Best Model <\/a>\n- <a href='#12'>Neural Networks<\/a>","df1ff73e":"# <a id=\"2\">Load Libraries<\/a>","cc87a899":"What will be the relationship of Time \/ Amount and Hour \/ Amount, should there be any patterns? Let's check it out.","cd62864d":"The results show that the application of oversampling techniques increases roc auc score, but decreases f1 and precision metrics for fraudulent transaction classification while recall increases. It is also possible to observe that the most important variable for the model was the variable V14. When evaluating all the metrics, the best result obtained by the model was using none of the methods. The class_weight parameter found by RanzomizedSearchCV was 'balanced' and it is noted that performance was better compared to data methods.","7dbf45cd":"The graph above shows that there is an approximately linear and positive trend between feature V4 and the target variable, and V4 interacts with V14 frequently (so it was chosen automatically).","237c32aa":"Again it can be observed that the application of oversampling methods decreased the f1 and precision metrics. The roc auc score increased and it is also noted that the most important feature was also V14, followed by V10. If we take into consideration all the evaluation of all metrics, the best result obtained was using the SMOTE method. An important note is that the result of RandomizedSearchCV set the scale_pos_weight parameter to 1, that is, same weight for both classes.","8c5e5ef4":"It is clear that the database is extremely unbalanced. As stated in the introduction, it is a very conducive scenario for the accuracy paradox to occur. What we want here is to get important features about fraudulent cases so that the model can distinguish them correctly.\n\nNow we will perform the EDA.","14338141":"It appears that real transactions follow a seasonality, while fraudulent transactions do not. For now, without much intuitions.\n\nAccording to the statement, the Time variable is given in seconds from a source, where it was collected for two days. Let's try turning these seconds to hours by dividing by 3600 and converting the hours to the interval of [00h, 23h] and creating a new * Hour * variable.","ee7c2d46":"# <a id=\"8\">Data Preparation<\/a>","a5e0578f":"### <a id=\"911\">Results of Random Forest Classifier<\/a>","63d17fb7":"### <a id=\"931\">Results of LightGBM<\/a>","60010688":"## <a id=\"93\">LightGBM<\/a>","3296c162":"## <a id=\"81\">SMOTE Data Preparation<\/a>","43f0347f":"The results were similar to XGBoost. Using the oversmapling methods, the precision and f1 metrics decreased more than compared to xgb. In addition, it can be noted that LightGBM uses many more features in consideration compared to other models. Considering the performance of the model in the most important class, fraudulent transactions, and observing all metrics, the best result was obtained using the ADASYN technique. And again the result of RandomizedSearchCV set the scale_pos_weight parameter to 1.","c6031e1c":"# <a id=\"11\"> Interpretability of the Best Model <\/a>","037d1ceb":"# <a id=\"7\">Data Analysis<\/a>\n\nFirst let's check the variables we have more information: Time and Amount","768c6032":" # <a id=\"4\">Initial Basic Analysis<\/a>","ff7176f9":"## <a id=\"82\">ADASYN Data Preparation<\/a>","59d06edc":"In the chart we can see that the most important features are positioned in descending order, the most important is at the top. The horizontal range shows whether the effect of this value is associated with a higher or lower predition. The color shows whether this variable is high (red) or low (blue) for this instance.\n\nThe higher the value of feature V4, the impact is high and positive on transaction classification. High impact comes from the color red and the positive impact is according to the X axis. We also note that feature V14 is negatively correlated with the target variable.\n\n\nThe Hour variable do not seem to have much impact on predictions as it focus on the value 0.\n\nIn the chart below we can simplify the visualization ordered by the most important features.","2fccacd7":"# <a id=\"3\">Loading Dataset<\/a>","0146dad1":"The graph above shows that there is an approximately linear but negative relationship between V14 and the target variable. This negative relationship was verified in the summary_plot chart, where we have a global view of all features.","ad21f161":"We note that some of the distributions have a high similarity while others can be visibly separable from the two possible values of the target variable. However, we do not know what each is about to reach an important conclusion.\n\nNow let's look at the correlation between the variables using the Pearson correlation coefficient.","92a0d9df":"Note that real transactions are more uniform while fraudulent transactions are more dispersed. The Hour variable maintains about the same distribution as the Time variable when compared them to the Amount variable, as Hour is a variable created from Time. It seems that Hour does not seem to be a useful variable. We'll check by looking at the correlation matrix to make sure.\n\nFor now, let's check the distributions of anonymous variables.","c67e2569":"# <a id=\"9\">Tree-based Models<\/a>","7f4ec2be":"The values at the top are the most important features. The first number in each row shows how much the model's performance decreased with a random reproduction using the roc auc metric. The amount of randomness is measured by repeating the process with several shuffles. The number after \u00b1 measures how performance varied from one reorganization to the next.\n\nThe most important feature was V4, followed by V14. We can see that this importance is different from those discussed in the XGBoost results section. What may have caused this difference, in my opinion, is that here scoring is specified for the calculation of the most important features, which in this case is roc auc. And there, although in the model is placed to check the metric 'auc' in the validation data, the parameter set directly in the model is just objective = 'binary: logistic'. But honestly, I'm not sure that is the explanation.","38d5627d":"Agora vamos observar o gr\u00e1fico de depend\u00eancia da nossa feature mais importante.","7c31ce69":"First let's rearrange the columns of our dataframe","8e362954":"Observing the results, we found that oversampling methods help to increase the roc auc score, however, it can decrease the values \u200b\u200bof the precision and f1 metrics, which may end up degrading the model. . In random forest, the best result was not using any data technique, however, changing the class weights in the objective function using the class_weight parameter. In the boost methods, the best results were obtained through data techniques, since RandomizedSearchCV considered that the parameter to deal with unbalanced classes made no distinction between the classes, that is, the same weight.\n\nOf course, if we looked for the best parameters using RandomizedSearchCV for oversampling techniques, maybe the results would be better, since we used the parameters obtained from the unbalanced data. Finally, analyzing the results with the best trade off between the metrics and the models, the best model obtained was the XGBoost using the SMOTE technique.","4000122a":"## <a id=\"10\">Conclusion<\/a>","1ae543fb":"According to the distributions, many of the fraudulent transactions took place between 2 and 8 hours, peaking for 12 hours. Also without many intuitions. We will further investigate this variable later in conjunction with the Amount variable and the correlation matrix.","b67dd821":"Actual transaction values have extremely high values compared to fraudulent transactions, which may indicate that these values are noisy. However, I believe these values may be normal in relation to real transactions, and may be considered a relevant property for the classifier because, in my opinion, fraudulent transactions may have the characteristic of not drawing the attention of banking services to transactions with \"low\" values to not to be easily detectable. However, it is not possible to prove this hypothesis during the period of 48 hours. Or maybe nothing I've said has made sense.","b0645250":"## <a id=\"91\">Random Forest Classifier<\/a>","45519a87":"### <a id=\"921\">Results of XGBoost<\/a>","ae968ea9":"In fact, the database has 28 anonymous variables (V1 - V28), 2 non-anonymous variables (Time and Amount) and a target variable (Class). Altogether there are 284807 instances with all values being of type float64 except for the target variable which is an int and most importantly has no missing \/ missing data. Anonymous variables have mean 0.\n\nNow let's look at the target variable.","fd8553a6":"<h1><font size=\"6\">Credit Card Fraud Detection<\/font><\/h1>\n<font size=\"3\"> Hi guys, that's my first kernel notebook. I'm still learning about data science and just did this notebook as a test and sorry for my bad english.<\/font>"}}