{"cell_type":{"f42f5723":"code","90b2c7a8":"code","b79e5c52":"code","82f5f41e":"code","68d31bdd":"code","8bc21571":"code","a85a4cf7":"code","6be15110":"code","d62f77f2":"code","4c978691":"code","f80c9896":"code","ae2bb24e":"code","b5ebcc1c":"code","6b110841":"code","3a119caf":"code","2dab7bf9":"code","339eef26":"code","18683a25":"code","752f41cb":"code","18c841ec":"code","a9c2cd15":"code","ab8dc0a1":"code","d406cba3":"code","2418896e":"code","7cfab01a":"code","07f37360":"code","5f92c9e0":"code","e5a6a3b9":"code","92f93f92":"code","0471cb8a":"markdown","a697d481":"markdown","4c8eea96":"markdown","969a63c1":"markdown","7bb4a9df":"markdown","91038231":"markdown","298bae92":"markdown","938e3875":"markdown","cbee72cf":"markdown","2675e6ae":"markdown","b14691ac":"markdown","daac53c7":"markdown","5b4d5f00":"markdown","c7820ee0":"markdown","26285a45":"markdown","eed487a1":"markdown"},"source":{"f42f5723":"# Import of libraries we will be using in analyzing the data and creating the model\nimport numpy as np # Library for Linear algebra\nimport pandas as pd # Data processing functionalities\nimport matplotlib.pyplot as plt # Plotting graphs\nimport seaborn as sns # Customizing graphs","90b2c7a8":"# Load the dataset which will be used for analysis and training model\nheart_data = pd.read_csv(\"..\/input\/heart.csv\")","b79e5c52":"n_rows, n_cols = heart_data.shape\nprint(f\"There exists {n_rows} domain instances with {n_cols} features in the dataset.\")","82f5f41e":"features = list(heart_data.columns)\n\nfor feature in range(len(features)):\n    print(\"Column {0} in the dataset is {1}\".format(feature+1, features[feature].title()))","68d31bdd":"# The head method operates on a dataframe by displaying a number of rows. The first 5 rows are displayed if no arguments are passed.\nheart_data.head()","8bc21571":"# Checking to see if there are any null values in our dataset.\nheart_data.isnull().any()","a85a4cf7":"# Checking to see if there are any duplicated data in dataset\nheart_data[heart_data.duplicated() == True]","6be15110":"# Removing duplicate data\nheart_data.drop_duplicates(inplace=True)\n\nn_rows, n_cols = heart_data.shape\nprint(f\"After removing duplicate data we now have {n_rows} domain instances.\")","d62f77f2":"# The pandas dataframe object created i.e. heart_data enables us to retrieve data using column headers\n# First thing we are interested in is the distribution of patients on both ends. i.e. diseased and not diseased\nheart_data.groupby('target').size()","4c978691":"not_diseased = len(heart_data[heart_data.target == 0])\ndiseased = len(heart_data[heart_data.target == 1])\nprint(f\"The percentage of diseased patients within this dataset is {round((diseased\/len(heart_data.target)), 2)*100}% leaving {round((not_diseased\/len(heart_data.target)), 2)*100}% of the subjects as patients diagnosed to not have the heart disease.\")","f80c9896":"sns.countplot(x='target', data=heart_data, palette='mako_r')\nplt.xlabel(\"Class Labels: 0 = not diseased, 1 = diseased\")\nplt.show()","ae2bb24e":"male_gender = len(heart_data[heart_data.sex == 1])\nfemale_gender = len(heart_data[heart_data.sex == 0])\n\nprint(\"In this dataset there exists {0} male subjects and {1} female subjects which computes to {2}% for males and {3}% for females.\".format(male_gender, female_gender, round((male_gender\/len(heart_data.sex)), 2)*100, round((female_gender\/len(heart_data.sex)), 2)*100))","b5ebcc1c":"# Visualizing the distribution of Male and Female genders in the data\nsns.countplot(x='sex', data=heart_data, palette='gist_rainbow')\nplt.xlabel(\"Sex: 0 => Female, 1 => Male\")\nplt.show()","6b110841":"# We can utilize the crosstab method in the pandas library to analyze how gender impacts a person's chance of getting a heart disease\ngender_impact = pd.crosstab(heart_data['sex'], heart_data['target'])\ngender_impact","3a119caf":"# We can go further to visualize these stats for a clearer view\ngender_impact.plot(kind='bar', stacked=False, color=['#00e676', '#d50000'])","2dab7bf9":"heart_data.describe()","339eef26":"# This provides us with the 'mean' subset of the describe() method\nheart_data.groupby('target').mean()","18683a25":"pd.crosstab(heart_data.age,heart_data.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndAges.png')\nplt.show()","752f41cb":"# Population distribution for heart disease\n\nx = heart_data.groupby(['age','target']).agg({'sex':'count'})\ny = heart_data.groupby(['age']).agg({'sex':'count'})\nz = (x.div(y, level='age') * 100)\nq= 100 - z\nbin_x = range(25,80,2)\n\nfig, axes = plt.subplots(2,2, figsize = (20,12))\nplt.subplots_adjust(hspace = 0.5)\n\naxes[0,0].hist(heart_data[heart_data['target']==1].age.tolist(),bins=bin_x,rwidth=0.8)\naxes[0,0].set_xticks(range(25,80,2))\naxes[0,0].set_xlabel('Age Range',fontsize=15)\naxes[0,0].set_ylabel('Population Count',fontsize=15)\naxes[0,0].set_title('People suffering from heart disease',fontsize=20)\n\naxes[0,1].hist(heart_data[heart_data['target']==0].age.tolist(),bins=bin_x,rwidth=0.8)\naxes[0,1].set_xticks(range(25,80,2))\naxes[0,1].set_xlabel('Age Range',fontsize=15)\naxes[0,1].set_ylabel('Population Count',fontsize=15)\naxes[0,1].set_title('People not suffering from heart disease',fontsize=20)\n\naxes[1,0].scatter(z.xs(1,level=1).reset_index().age,z.xs(1,level=1).reset_index().sex,s=(x.xs(1,level=1).sex)*30,edgecolors = 'r',c = 'yellow')\naxes[1,0].plot(z.xs(1,level=1).reset_index().age,z.xs(1,level=1).reset_index().sex)\naxes[1,0].set_xticks(range(25,80,2))\naxes[1,0].set_yticks(range(0,110,5))\naxes[1,0].set_xlabel('Age',fontsize=15)\naxes[1,0].set_ylabel('%',fontsize=15)\naxes[1,0].set_title('% of people with heart disease by age',fontsize=20)\n\naxes[1,1].scatter(z.xs(1,level=1).reset_index().age,q.xs(1,level=1).reset_index().sex,s=(x.xs(0,level=1).sex)*30,edgecolors = 'r',c = 'yellow')\naxes[1,1].plot(z.xs(1,level=1).reset_index().age,q.xs(1,level=1).reset_index().sex)\naxes[1,1].set_xticks(range(25,80,2))\naxes[1,1].set_yticks(range(0,110,5))\naxes[1,1].set_xlabel('Age',fontsize=15)\naxes[1,1].set_ylabel('%',fontsize=15)\naxes[1,1].set_title('% of people with no heart disease by age',fontsize=20)\n\nplt.show()","18c841ec":"fig, axes = plt.subplots(6,2, figsize = (20,40))\nplt.subplots_adjust(hspace = 0.5)\n\naxes[0,0].scatter(heart_data[heart_data['target']==0][['age','thalach']].sort_values(by = ['age']).age,heart_data[heart_data['target']==0][['age','thalach']].sort_values(by = ['age']).thalach, c = 'g',label = 'target=0')\naxes[0,0].scatter(heart_data[heart_data['target']==1][['age','thalach']].sort_values(by = ['age']).age,heart_data[heart_data['target']==1][['age','thalach']].sort_values(by = ['age']).thalach, c = 'r',label = 'target=1')\naxes[0,0].set_title('thalach distribution',fontsize=20)\naxes[0,0].set_xticks(range(25,80,2))\naxes[0,0].set_xlabel('Age',fontsize=15)\naxes[0,0].set_ylabel('thalach',fontsize=15)\naxes[0,0].axhline(np.mean(heart_data['thalach']),xmin=0,xmax=1,linewidth=1, color='black',linestyle = '--')\naxes[0,0].axvline(np.mean(heart_data['age']),ymin=0,ymax=1,linewidth=1, color='b',linestyle = '--')\naxes[0,0].legend()\n\naxes[0,1].scatter(heart_data[heart_data['target']==0][['age','trestbps']].sort_values(by = ['age']).age,heart_data[heart_data['target']==0][['age','trestbps']].sort_values(by = ['age']).trestbps, c = 'g',label = 'target=0')\naxes[0,1].scatter(heart_data[heart_data['target']==1][['age','trestbps']].sort_values(by = ['age']).age,heart_data[heart_data['target']==1][['age','trestbps']].sort_values(by = ['age']).trestbps, c = 'r',label = 'target=1')\naxes[0,1].set_title('trestbps distribution',fontsize=20)\naxes[0,1].set_xticks(range(25,80,2))\naxes[0,1].set_xlabel('Age',fontsize=15)\naxes[0,1].set_ylabel('trestbps',fontsize=15)\naxes[0,1].axhline(np.mean(heart_data['trestbps']),xmin=0,xmax=1,linewidth=1, color='r',linestyle = '--')\naxes[0,1].axvline(np.mean(heart_data['age']),ymin=0,ymax=1,linewidth=1, color='b',linestyle = '--')\n\n# heart_data[heart_data['target']==1][['age','chol',]].sort_values(by = ['age'])\naxes[1,0].scatter(heart_data[heart_data['target']==0][['age','chol',]].sort_values(by = ['age']).age,heart_data[heart_data['target']==0][['age','chol',]].sort_values(by = ['age']).chol,c = 'g',label = 'target=0')\naxes[1,0].scatter(heart_data[heart_data['target']==1][['age','chol',]].sort_values(by = ['age']).age,heart_data[heart_data['target']==1][['age','chol',]].sort_values(by = ['age']).chol,c = 'r',label = 'target=1')\naxes[1,0].set_title('chol distribution',fontsize=20)\naxes[1,0].set_xticks(range(25,80,2))\naxes[1,0].set_xlabel('Age',fontsize=15)\naxes[1,0].set_ylabel('chol',fontsize=15)\naxes[1,0].axhline(np.mean(heart_data['chol']),xmin=0,xmax=1,linewidth=1, color='r',linestyle = '--')\naxes[1,0].axvline(np.mean(heart_data['age']),ymin=0,ymax=1,linewidth=1, color='b',linestyle = '--')\n\naxes[1,1].scatter(heart_data[heart_data['target']==0][['age','oldpeak',]].sort_values(by = ['age']).age,heart_data[heart_data['target']==0][['age','oldpeak',]].sort_values(by = ['age']).oldpeak,c = 'g',label = 'target=0')\naxes[1,1].scatter(heart_data[heart_data['target']==1][['age','oldpeak',]].sort_values(by = ['age']).age,heart_data[heart_data['target']==1][['age','oldpeak',]].sort_values(by = ['age']).oldpeak,c = 'r',label = 'target=1')\naxes[1,1].set_title('oldpeak distribution',fontsize=20)\naxes[1,1].set_xticks(range(25,80,2))\naxes[1,1].set_xlabel('Age',fontsize=15)\naxes[1,1].set_ylabel('oldpeak',fontsize=15)\naxes[1,1].axhline(np.mean(heart_data['oldpeak']),xmin=0,xmax=1,linewidth=1, color='r',linestyle = '--')\naxes[1,1].axvline(np.mean(heart_data['age']),ymin=0,ymax=1,linewidth=1, color='b',linestyle = '--')\n\nfbs_count = heart_data['fbs'].value_counts()\nlabels = [('fbs = '+ str(x)) for x in fbs_count.index]\naxes[2,0].pie(fbs_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45)\naxes[2,0].axis('equal')\naxes[2,0].set_title('fbs share',fontsize=15)\n\nrestecg_count = heart_data['restecg'].value_counts()\nlabels = [('restecg = '+ str(x)) for x in restecg_count.index]\naxes[2,1].pie(restecg_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45,explode = [0,0,0.5])\naxes[2,1].axis('equal')\naxes[2,1].set_title('restecg share',fontsize=15)\n\nexang_count = heart_data['exang'].value_counts()\nlabels = [('exang = '+ str(x)) for x in exang_count.index]\naxes[3,0].pie(exang_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45)\naxes[3,0].axis('equal')\naxes[3,0].set_title('exang share',fontsize=15)\n\nslope_count = heart_data['slope'].value_counts()\nlabels = [('slope = '+ str(x)) for x in slope_count.index]\naxes[3,1].pie(slope_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45)\naxes[3,1].axis('equal')\naxes[3,1].set_title('slope share',fontsize=15)\n\nca_count = heart_data['ca'].value_counts()\nlabels = [('ca = '+ str(x)) for x in ca_count.index]\naxes[4,0].pie(ca_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45)\naxes[4,0].axis('equal')\naxes[4,0].set_title('ca share',fontsize=15)\n\nthal_count = heart_data['thal'].value_counts()\nlabels = [('thal = '+ str(x)) for x in thal_count.index]\naxes[4,1].pie(thal_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45)\naxes[4,1].axis('equal')\naxes[4,1].set_title('thal share',fontsize=15)\n\ncp_count = heart_data['cp'].value_counts()\nlabels = [('cp = '+ str(x)) for x in cp_count.index]\naxes[5,0].pie(cp_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45)\naxes[5,0].axis('equal')\naxes[5,0].set_title('CP share',fontsize=15)\n\ntarget_count = heart_data['target'].value_counts()\nlabels = [('target = '+ str(x)) for x in target_count.index]\naxes[5,1].pie(target_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45)\naxes[5,1].axis('equal')\naxes[5,1].set_title('target share',fontsize=15)\n\nplt.show()","a9c2cd15":"# Produce a correlation matrix to reveal how independent features within the data affect the target\ncorrelations = heart_data.corr()\npd.DataFrame(correlations['target']).sort_values(by='target', ascending=False)","ab8dc0a1":"# Visual representation\nplt.figure(figsize=(14,10))\nsns.heatmap(heart_data.corr(), linewidths=.01, annot = True, cmap='coolwarm')\nplt.show()","d406cba3":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\nX = heart_data.iloc[:, :-1]\ny = heart_data.iloc[:, -1]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","2418896e":"# Model\nheart_model = LogisticRegression()\nheart_model.fit(X_train, y_train)\n\n# Making predictions\npredictions = heart_model.predict(X_test)\n\n# Checking the Accuracy of predictions\nprint(\"Accuracy \", heart_model.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(predictions, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","7cfab01a":"print(heart_model.predict([[63, 1, 3, 145, 233, 1, 0, 150, 0, 2.3, 0, 0, 1]]))","07f37360":"print(heart_model.predict([[65, 1, 0, 135, 254, 0, 0, 127, 0, 2.8, 1, 1, 3]]))","5f92c9e0":"# Model using decision trees\nheart_model_dt = DecisionTreeClassifier()\n\n#fiting the model\nheart_model_dt.fit(X_train, y_train)\n\n#prediction\ndt_predictions = heart_model_dt.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", heart_model_dt.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(dt_predictions, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","e5a6a3b9":"print(heart_model_dt.predict([[63, 1, 3, 145, 233, 1, 0, 150, 0, 2.3, 0, 0, 1]]))","92f93f92":"print(heart_model_dt.predict([[65, 1, 0, 135, 254, 0, 0, 127, 0, 2.8, 1, 1, 3]]))","0471cb8a":"### Insights:\n***(ref: dataset)***\n    \n    - Number of females free from heart disease = 24\n    - Number of females diagnosed as having heart disease = 72\n    \n    - Number of males free from heart disease = 114\n    - Number of males diagnosed as having heart disease = 93\n    \n\nIt is evident from the data that females stand a higher risk of getting a heart disease than men.\n\n--------------------------------------------------------------------------------------------------------------------------","a697d481":"# Section 3:  The Model\n","4c8eea96":"### We can see that the attributes having :\n\n* **Positive relationships**<br>\nchest pain = target<br>\nthalcah = slope<br>\nthalach = target<br>\nslope = target<br>\n\n* **Negative relationships**<br>\noldpeak =slope<br>\ncp = exang<br>\nage = thalach","969a63c1":"## Data Attribute Description\n\n- **age** --> The person's age in years\n- **sex** --> The person's sex (1 = male, 0 = female)\n- **cp** -->   The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value                  4: asymptomatic)\n- **trestbps** --> The person's resting blood pressure (mm Hg on admission to the hospital)\n- **chol** -->    The person's cholesterol measurement in mg\/dl\n- **fbs** -->     The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false) \n- **restecg** --> Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable                     or definite left ventricular hypertrophy by Estes' criteria)\n- **thalach** --> The person's maximum heart rate achieved\n- **exang** --> Exercise induced angina (1 = yes; 0 = no)\n- **oldpeak** --> ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more [here](https:\/\/litfl.com\/st-segment-ecg-library\/))\n- **slope:** the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n- **ca** --> The number of major vessels (0-3)\n- **thal** --> A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n- **target** --> Heart disease (0 = no, 1 = yes)","7bb4a9df":"### Since there are no null values and duplicates, our data is good to go for analysis and visualizations","91038231":"# Section 2: The Data\n\n### Exploring the structure of the data","298bae92":"### Gender Impact on risk of heart disease\nP.S. Research on how gender affects a person's chance of getting a heart disease","938e3875":"-----------------------------------------------------------------------------------------------------------------------","cbee72cf":"### **Decision Tree**\n\nDecision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing values for the attribute tested. Leaf node (e.g., Hours Played) represents a decision on the numerical target. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.","2675e6ae":"### **Logistic Regression**\n\n\nLogistic regression is the appropriate regression analysis to conduct when the dependent variable is binary. Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.","b14691ac":"# Contents\n\n1. [Introduction ](#section1)\n2. [The Data](#section2)\n3. [The Model](#section3)\n4. [Conclusion](#section5)","daac53c7":"**The features described in the above data set are:**\n\n**1. Count**: tells us the number of NoN-empty rows in a feature.<br>\n\n**2. Mean**: tells us the mean value of that feature.<br>\n\n**3. Std**: tells us the Standard Deviation Value of that feature.<br>\n\n**4. Min**: tells us the minimum value of that feature.<br>\n\n**5. 25%**, **50%**, and **75%**: are the percentile\/quartile of each features.<br>\n\n**6. Max**: tells us the maximum value of that feature.<br>\n\n\n### Key Insights\n\n    - Total domain instances: 302\n    - Mean age (both genders put together): 54.42\n    - Minimum age (no persons below this age were part of the study): 29\n    - Maximum age (no persons above this age were part of the study): 77","5b4d5f00":"***From this observation, we can tell there lies a bias against the female gender in this dataset that needs correction. To create a balance, the number of male subjects could be reduced to level the gender stance but alternatively, which also happens to be the best solution is to rather involve more female in such research undertakings. In Machine Learning, the more data you have the better as well as a reduction in its bias.***","c7820ee0":"This is a notebook that seeks to explain and give insights concerning the data used in this project. The objective of preparing this notebook is to better understand the relationship of various factors in the dataset that can be related to heart disease. The original database contains 76 attributes, but usually most published experiments refer to using a subset of 14 of them,so we will be using these 14 features for analysing.\n\nFrom: One ML-Noob ^_^\n\nTo: Whoever wants to change the world for the better","26285a45":"- 0: No heart disease\n- 1: Heart disease\n#### In the cell above, we realized that out of 303 instances within the dataset, 138 people were diagnosed to not have a heart disease and 164 people were diagnosed to have had it.","eed487a1":"# Section 1: Introduction\n\nPython is great for data science as it has a great community of developers dedicated to contributing massively in terms of libraries that makes data processing and model creation a far less painstaking thing to do in modern times."}}