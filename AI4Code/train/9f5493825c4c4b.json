{"cell_type":{"11fedba1":"code","0412a96d":"code","92f2c117":"code","72d7916e":"code","4b4327a5":"code","ae19d928":"code","0dec6e0c":"code","1a5f9347":"code","98d53d60":"code","dd365cfe":"code","3adddb6f":"code","ff921776":"code","dbfa2cd5":"code","922d64a3":"code","74caad93":"code","27d03440":"code","2c957325":"code","eeceaa36":"code","706473c7":"code","e655a856":"code","6c6760b3":"code","07f141f4":"code","ae991206":"code","be10de62":"code","3f34aa61":"code","e89d331a":"code","33a8cb71":"code","12e2f526":"code","b0ff0b70":"code","05cdcaaa":"code","b0b687a3":"code","8683e4b6":"code","4e7974d0":"code","113c5c9e":"code","53e3cf7a":"code","edce6674":"code","db434285":"code","1bb92a6d":"code","14107500":"code","092f9098":"code","c77d6070":"code","0c2906ab":"code","dd697d05":"code","92026bd8":"code","228cc9cf":"markdown","9900b66c":"markdown","3a53eaed":"markdown","3df73f61":"markdown","35930174":"markdown","27845f3e":"markdown","17cc1a11":"markdown","17e148cf":"markdown","4fa91097":"markdown","dfcad9e3":"markdown","589e2712":"markdown","656f9555":"markdown","4a8e68fe":"markdown","a45d9c50":"markdown","cba528ab":"markdown","d479d8e5":"markdown","883b786c":"markdown","91ebe606":"markdown","f8f6a417":"markdown","84bc4260":"markdown","99a585a2":"markdown","7924029c":"markdown","1d8355ca":"markdown","382279d3":"markdown","5893d351":"markdown","16ac2dc3":"markdown","6c74f8b7":"markdown","943b5d49":"markdown","bcac9e82":"markdown","0cec3250":"markdown","962cb95e":"markdown","b24fe835":"markdown","d95a9dda":"markdown","bbaa14ad":"markdown","7e86f587":"markdown","35c1650e":"markdown","7fc89664":"markdown","96396ed5":"markdown","064c889a":"markdown","1288dd83":"markdown","47bd5b36":"markdown","2173ba1e":"markdown","52ae761e":"markdown","4fd74dd1":"markdown","400742f0":"markdown","aa0cc435":"markdown","d6288d94":"markdown","7d5a1f94":"markdown","bf0ca754":"markdown","ce8eaf6e":"markdown","33f61d89":"markdown","e52c5f98":"markdown","068f2363":"markdown","a4d0eac3":"markdown","1bee34c3":"markdown","ae56d408":"markdown","5321492a":"markdown","4a1e9eda":"markdown","44cf1c94":"markdown","4c8c1183":"markdown","31b564ba":"markdown","d0850a97":"markdown","7dd15a36":"markdown","78f0846d":"markdown","8942e6db":"markdown","a4bb91fe":"markdown","57604456":"markdown","88cb6b5a":"markdown","bf078a6f":"markdown","65d17349":"markdown","7ca64fbc":"markdown","a06a2364":"markdown","2dacf9b4":"markdown","fd9014f6":"markdown","463c52e7":"markdown","9d8be128":"markdown","396f37e0":"markdown","7015f964":"markdown","6049d08c":"markdown","e808c386":"markdown","8a8285f9":"markdown"},"source":{"11fedba1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\n\nsns.set_style('darkgrid')","0412a96d":"raw_data = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\nraw_data.head()","92f2c117":"data = raw_data.copy()\ndata.info()","72d7916e":"data.describe()","4b4327a5":"data.columns.values","ae19d928":"data.isnull().sum()","0dec6e0c":"plt.figure(figsize = (25,15))\nsns.boxplot(data = pd.melt(data) , x = 'variable', y = 'value')\nplt.show()","1a5f9347":"plt.figure(figsize = (8,6))\nsns.distplot(data['total sulfur dioxide'])\nplt.show()","98d53d60":"data = data[data['total sulfur dioxide']<180]\nplt.figure(figsize = (8,6))\nsns.distplot(data['total sulfur dioxide'])\nplt.show()","dd365cfe":"plt.figure(figsize = (15,15))\nsns.boxplot(data = pd.melt(data) , x = 'variable', y = 'value')\nplt.show()","3adddb6f":"plt.figure(figsize = (8,6))\nsns.distplot(data['quality'])\nplt.show()","ff921776":"plt.figure(figsize =(12,12))\nsns.heatmap(data.corr(), cmap = 'Blues', annot = True)\nplt.show()","dbfa2cd5":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvariables = data[['density', 'citric acid', 'total sulfur dioxide', 'free sulfur dioxide']]\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\nvif['Features'] = variables.columns\nvif","922d64a3":"data = data.drop('free sulfur dioxide', axis = 1)\ndata","74caad93":"variables = data[['density', 'citric acid', 'total sulfur dioxide']]\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\nvif['Features'] = variables.columns\nvif","27d03440":"from sklearn.ensemble import ExtraTreesClassifier \n\nX = data.drop('quality', axis = 1)\nY = data['quality']\n\nmodel =  ExtraTreesClassifier()\nmodel.fit(X,Y)\n\nfeatures = pd.DataFrame()\nfeatures['Features'] = X.columns\nfeatures['Importance'] = model.feature_importances_\n\nplt.figure(figsize =(15,6))\nsns.barplot(y='Importance', x='Features', data=features,  order=features.sort_values('Importance',ascending = False).Features)\nplt.xlabel(\"Features\", size=15)\nplt.ylabel(\"Importance\", size=15)\nplt.title(\"Features Importance(Descending order)\", size=18)\nplt.tight_layout()","2c957325":"data_new = data.drop('pH', axis = 1)\ndata_new.shape","eeceaa36":"from sklearn.preprocessing import StandardScaler\n\nx = data_new.drop('quality', axis = 1)\nscaler = StandardScaler()\nscaler.fit(x)\nx_scaled = scaler.transform(x)\n","706473c7":"x_scaled.shape","e655a856":"plt.figure(figsize =(16,5))\nplt.subplot(1,2,1)\nsns.distplot(data['quality'])\nplt.subplot(1,2,2)\nsns.countplot(data['quality'])#Showing the frequency of occurence of a particular quality rating\nplt.show()","6c6760b3":"category = [] # Defining an empty array\nfor x in data['quality']:\n    if x>=1 and x<=3:\n        category.append('Bad')\n    elif x>=4 and x<=6:\n        category.append('Normal')\n    elif x>=7 and x<=10:\n        category.append('Good')\n        \n        \ndata_new['category'] = category #Assigning a new column\ndata_new.head()","07f141f4":"data_final = data_new.copy()\ndata_final = data_final.drop('quality',axis =1)\ndata_final.head()","ae991206":"data_final['category'].value_counts() #Checking the number of ratings in each category","be10de62":"from sklearn.model_selection import train_test_split\n\n#defining inputs(independent) and targets(dependent) variables\ninputs = x_scaled\ntargets = data_final['category']\n\n#splitting into training and testing data\n\nx_train, x_test, y_train, y_test = train_test_split(inputs, targets, test_size = 0.2, random_state = 42)","3f34aa61":"x_train.shape, y_train.shape","e89d331a":"x_test.shape, y_test.shape","33a8cb71":"#Defining a method or function that will print the cross validation score and accuracy for each model\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\ndef model_report(cl):\n    \n    cl.fit(x_train, y_train)\n\n    print('Cross Val Score: ',(cross_val_score(cl,x_train,y_train, cv=5).mean()*100).round(2))#using a 5-Fold cross validation\n\n    y_pred = cl.predict(x_test)\n\n    print('Accuracy Score: ', (accuracy_score(y_test,y_pred)*100).round(2))","12e2f526":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nmodel_report(lr)","b0ff0b70":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\n\nmodel_report(dt)","05cdcaaa":"from sklearn.svm import SVC\n\nsvc = SVC()\n\nmodel_report(svc)","b0b687a3":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\n\nmodel_report(rf)","8683e4b6":"from sklearn.neighbors import KNeighborsClassifier\n\nkn = KNeighborsClassifier(algorithm ='auto')\n\nmodel_report(kn)","4e7974d0":"from sklearn.model_selection import GridSearchCV\n\n#Defining a function that will calculate the best parameters and accuracy of the model based on those parameters\n#Using GridSearchCV\n\ndef grid_search(classifier,parameters):\n    \n    grid = GridSearchCV(estimator = classifier,\n                        param_grid = parameters,\n                        scoring = 'accuracy',\n                        cv = 5,\n                        n_jobs = -1\n                        )\n    \n    grid.fit(x_train,y_train)\n\n    print('Best parameters: ', grid.best_params_) #Displaying the best parameters of the model\n\n    print(\"Accuracy: \", ((grid.best_score_)*100).round(2))#Accuracy of the model based on those parameters","113c5c9e":"param_svc = {\n    'C': [0.1, 1, 10, 100],  \n    'gamma': [0.0001, 0.001, 0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9], \n    'kernel': ['linear','rbf']\n    }\nsvc = SVC()\n\ngrid_search(svc,param_svc)","53e3cf7a":"#Training the model again with the best parameters we got\nsvc = SVC(C = 10, gamma = 0.3, kernel='rbf')\n\nmodel_report(svc)","edce6674":"param_rf = {\n    'n_estimators': [10,50,100,500,1000],\n    'min_samples_leaf': [1,10,20,50]\n    }\nrf = RandomForestClassifier(random_state = 0)\ngrid_search(rf,param_rf)","db434285":"#Training the model again with the best parameters we got\nrf = RandomForestClassifier(n_estimators = 1000, min_samples_leaf = 1,random_state = 0)\nmodel_report(rf)","1bb92a6d":"n_neighbors = list(range(5,10))#This is basically the value of k\n                   \nparam_knn = {\n    'n_neighbors' : n_neighbors,\n    'p' : [1,2]\n    \n    }\n\nknn = KNeighborsClassifier(algorithm ='auto', n_jobs = -1)\ngrid_search(knn,param_knn)","14107500":"#Training the model again with the best parameters we got\nknn = KNeighborsClassifier(n_neighbors = 7, p = 2, algorithm ='auto', n_jobs = -1)\nmodel_report(knn)","092f9098":"from sklearn.ensemble import  AdaBoostClassifier\n\nab = AdaBoostClassifier(random_state = 42)\n\nmodel_report(ab)","c77d6070":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier(random_state = 42, learning_rate = 0.2)\n\nmodel_report(gb)","0c2906ab":"from xgboost import XGBClassifier\n\nxg = XGBClassifier(random_state = 42, learning_rate = 0.2)\n\nmodel_report(xg)","dd697d05":"features_rf = pd.DataFrame()\nx_rf = data_final.drop('category',axis=1)\nfeatures_rf['Features'] = x_rf.columns\nfeatures_rf['Importance'] = rf.feature_importances_\n\nplt.figure(figsize =(15,6))\nsns.barplot(y='Importance', x='Features', data=features_rf,  order=features_rf.sort_values('Importance',ascending = False).Features)\nplt.xlabel(\"Features\", size=15)\nplt.ylabel(\"Importance\", size=15)\nplt.title(\"Features Importance(Descending order) for Random Forest\", size=18)\nplt.tight_layout()","92026bd8":"features_xg = pd.DataFrame()\nx_xg = data_final.drop('category',axis=1)\nfeatures_xg['Features'] = x_xg.columns\nfeatures_xg['Importance'] = xg.feature_importances_\n\nplt.figure(figsize =(15,6))\nsns.barplot(y='Importance', x='Features', data=features_xg,  order=features_xg.sort_values('Importance',ascending = False).Features)\nplt.xlabel(\"Features\", size=15)\nplt.ylabel(\"Importance\", size=15)\nplt.title(\"Features Importance(Descending order) for XGBoost\", size=18)\nplt.tight_layout()","228cc9cf":"#  ","9900b66c":"#  ","3a53eaed":"#  ","3df73f61":"## 5. K-Nearest Neighbors","35930174":"#  ","27845f3e":"#  ","17cc1a11":"#  ","17e148cf":"#  ","4fa91097":"#  ","dfcad9e3":"'free sulfur dioxide' has a correlation above 5 and needs to be dropped","589e2712":"## 2. Random Forest ","656f9555":"##  ","4a8e68fe":"#   ","a45d9c50":"#  \n","cba528ab":"##### 1. For determing the quality of wines , alcohol plays a significant role followed by sulphates and volatile acididty.\n##### 2. For predicting quality of wines, we can either use Random Forest or XGBoost model. However,  XGBoost has a slightly better accuracy(0.62% more accurate) over Random Forest.","d479d8e5":"### Checking the distribution of quality","883b786c":"# Data Collection","91ebe606":"#### If you find this notebook useful, please upvote it! And let me know in the comments if i did anything wrong or if i could've done this better. Means a lot! :)","f8f6a417":"## Random Forest:","84bc4260":"##### Gradient boost gives quite better accuracy than AdaBoost.","99a585a2":"# Splitting Data","7924029c":"## 3. XGBoost","1d8355ca":"# Data description","382279d3":"## Hyper Parameter Tuning ","5893d351":"## 3. K-Nearest Neighbors","16ac2dc3":"##### Looks like hyper parameter tuning did'nt changed the accuracy of K-Nearest Neighbors","6c74f8b7":"### Removing outliers from data :","943b5d49":"#  ","bcac9e82":"## 3. Support Vector Classifer","0cec3250":"# Objectives: ","962cb95e":"#  ","b24fe835":"## 1. Support Vector Classifier","d95a9dda":"##  ","bbaa14ad":"## 1. AdaBoost","7e86f587":"### 1. Checking missing values","35c1650e":"### 2.  Outlier Detection","7fc89664":"## 1. Logistic Regression","96396ed5":"#### As the vif of these variables are below 5, there's no multicolinearity.","064c889a":"##### So, AdaBoost is'nt a good model to perform on this dataset ","1288dd83":"Finally , let's see which features contributed most in each of these 2 models.","47bd5b36":"#### We can observe that except the pH column , all other features comprises 95% of the data that influences a wine quality. So we are going to use these 9 features for our models.","2173ba1e":"## XGBoost:","52ae761e":"#  ","4fd74dd1":"#### The density and citric acid are highly correlated with fixed acidity. Again, total sulfur dioxide and free sulfer dioxide are highly correlated to each other. \n\nThis is the multicollinearity. This results in unstable parameter estimates of regression which makes it very difficult to assess the effect of independent variables on dependent variables.\n\nWe will use Variance inflation factor to analyze which variable has a high correlation.","400742f0":"#### Checking the data again with a boxplot","aa0cc435":"##  ","d6288d94":"##### So, the accuracy of our Support Vector Classifier model increased from 89.38% to 90.62% ","7d5a1f94":"#### Now we will categorize the ratings into 3 categories , 'Bad', 'Normal' , 'Good'.","bf0ca754":"##  ","ce8eaf6e":"Now, let's check which features are most important for our quality predictions. For this we will use Extra Tree classifier,","33f61d89":"Let's check the Variance inflation factor for the remaining 3 variables after dropping the 'free sulfur dioxide' column.","e52c5f98":"#  ","068f2363":"# Data Exploration","a4d0eac3":"#  ","1bee34c3":"#### Lastly we will use some boosting algorithms mainly :\n#### 1. AdaBoost\n#### 2. Gradient Boost\n#### 3. XGBoost","ae56d408":"Let's observe the distribution of wine quality in the data ","5321492a":"## 4. Random Forest","4a1e9eda":"#### Let's first check which features are correlated with each other with a correclation heatmap","44cf1c94":"#### So there are some outliers in the 'total sulfer dioxide' column","4c8c1183":"# Data Cleaning","31b564ba":"#  ","d0850a97":"#  ","7dd15a36":"## 2. Decision Tree Classifier","78f0846d":"#### Since the features are measured in different units , wee need to standardize the values. for that, we use Standard Scaler. \n##### Standard scaler scales the values with mean = 0 and standard deviation = 1.","8942e6db":"#  ","a4bb91fe":"#  ","57604456":"#### Lets try to tune our models and see if we can improve accuracy. For this we will use GridSearchCV \n\nGrid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.","88cb6b5a":"#  ","bf078a6f":"# Conclusuion:","65d17349":"#### Since this a Classification problem , we are mainly going to use :\n\n#### 1. Logistic Regression\n\n#### 2. Decision Tree Classifier\n\n#### 3. Support Vector Classifier\n\n#### 4. Random Forest Classifier\n\n#### 5. K-Nearest Neighbours\n","7ca64fbc":"# Models","a06a2364":"#   ","2dacf9b4":"## 2. Gradient Boost","fd9014f6":"### So , comparing all the models , Random Forest(92.19% accuracy) and XGBoost(92.81%) seems to give the highest accuracy.\n","463c52e7":"#  ","9d8be128":"##    ","396f37e0":"#### The objectives of this project are as follows:\n\n#### 1. To experiment with different classification methods to see which yields the highest accuracy\n#### 2. To determine which features are the most indicative for predicting the quality of  wine","7015f964":"So there are no missing values","6049d08c":"##### So, the accuracy of our Random Forest Classifier model increased from 91.25% to 92.19%","e808c386":"#  ","8a8285f9":"# Importing Libraries"}}