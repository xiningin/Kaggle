{"cell_type":{"f12474c9":"code","a995e672":"code","e8ee88b7":"code","1da949b5":"code","b69375d2":"code","a1f19bca":"code","1de9c734":"code","ee787d60":"code","ed4d0096":"code","a7bb6311":"code","8dd3c1bd":"code","c8270f74":"code","f467112e":"code","99a917bd":"code","abe8a1cf":"code","d1c01d31":"code","e84a87d7":"code","6a5a898a":"code","51bade15":"code","b9fc42d4":"code","3230cefd":"code","c2d316de":"code","e9d380a5":"code","9cc667dd":"code","928ad04d":"code","10bdeade":"code","8c17bce6":"code","babcc0fa":"code","5a1e4be8":"code","61e81dd0":"code","b39de393":"code","b93d5a14":"code","7c4e5d95":"code","059ca655":"code","6bf3abec":"code","7bb71cce":"code","b3d6cca3":"code","077f5835":"code","2a0878b3":"code","22a6e187":"code","ba441719":"code","6851c08b":"code","44af8857":"code","056b4115":"code","be9592cb":"code","e6352d31":"code","7781a8e7":"code","a9a6ba23":"code","7dd5083e":"code","d8df5372":"code","063b75cd":"code","a54a6a27":"code","35363632":"code","b3401f0e":"code","7e5c6ffe":"code","fb70cfe6":"code","a1fc8637":"code","aa6124f5":"code","df009beb":"code","b3a5ed48":"code","5d390525":"code","d2b14d43":"code","d8affbb1":"code","0ac2a3ff":"code","500322c2":"code","c5db2caa":"code","8b51c547":"code","62b3b51a":"code","f8682d1a":"code","a846419c":"code","d3616418":"code","dc24d984":"code","f08514e9":"code","c67b362e":"code","e7b8a489":"code","4048d8c4":"code","9ff352ae":"code","7dbea66d":"code","b84a9a6d":"code","e61b52d6":"code","8f8fa548":"code","eeadc4a4":"code","526fba11":"code","83cb8b29":"code","0c36de69":"markdown","e94b3da6":"markdown","39dab393":"markdown","dbe0c156":"markdown","563f0ff0":"markdown","f25b78ef":"markdown","85928528":"markdown","16e65845":"markdown","25b9f2b4":"markdown","5a337bec":"markdown","2ae416ff":"markdown","9ef895ee":"markdown","427dbfec":"markdown","025ea892":"markdown","fd72e677":"markdown","025b0103":"markdown","a7579a1e":"markdown","159236a6":"markdown","7e76e570":"markdown","4cad3e3d":"markdown","51ac1cf7":"markdown","d52d606d":"markdown","615a1ef7":"markdown","f2469f18":"markdown","c3ef0cc3":"markdown","b42540d6":"markdown","ae40129b":"markdown","ccb17d45":"markdown","67f8b519":"markdown","b1a53701":"markdown","0e46c85b":"markdown","2eb9d06a":"markdown","48f5c367":"markdown","e688e4ec":"markdown","6fb20002":"markdown","48df3bfa":"markdown","1bf8fcde":"markdown","8480891c":"markdown","9b525895":"markdown","cc4c7a15":"markdown","a6a335b1":"markdown","67114966":"markdown","2c98aeed":"markdown","d4e51b01":"markdown","bf280db5":"markdown","061ff354":"markdown","4d2a251d":"markdown","f6450d83":"markdown","6641777c":"markdown","4732fae9":"markdown","9bcf4da8":"markdown","70a32d2e":"markdown","daebd10b":"markdown","95b54371":"markdown","be43b8ca":"markdown","89042856":"markdown"},"source":{"f12474c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\") \n# if u write and run plt.style.available, you can see all available styles and choose whatever you like. \n\nfrom collections import Counter\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a995e672":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_PassengerId = test_df[\"PassengerId\"] \n# I want to keep original version of IDs. \n# During analysis, I can change or drop them. I save them just in case.","e8ee88b7":"train_df.columns","1da949b5":"train_df.head(10)","b69375d2":"train_df.describe()","a1f19bca":"train_df.info()","1de9c734":"def bar_plot(variable):\n    var = train_df[variable]\n    varValue = var.value_counts()\n    \n    # visualisation\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable, varValue))","ee787d60":"category1 = [\"Survived\",\"Sex\", \"Pclass\", \"Embarked\", \"SibSp\", \"Parch\"]\nfor c in category1:\n    bar_plot(c)","ed4d0096":"category2 = [\"Cabin\", \"Name\", \"Ticket\"]\nfor c in category2:\n    print(\"{} \\n\".format(train_df[c].value_counts()))","a7bb6311":"def plot_hist(variable):\n    plt.figure(figsize=(9,3))\n    plt.hist(train_df[variable], bins=50)\n    plt.xlabel(variable)\n    plt.ylabel(\"frequency\")\n    plt.title(\"{} distribution with histogram\".format(variable))\n    plt.show()","8dd3c1bd":"numerics = [\"Fare\", \"Age\", \"PassengerId\"]\nfor n in numerics:\n    plot_hist(n)","c8270f74":"# Pclass -Survived\n\ntrain_df[[\"Pclass\", \"Survived\"]].groupby(\"Pclass\", as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","f467112e":"# Sex - Survived\n\ntrain_df[[\"Sex\", \"Survived\"]].groupby(\"Sex\", as_index=False).mean().sort_values(\"Survived\", ascending=False)","99a917bd":"# SibSp - Survived\n\ntrain_df[[\"SibSp\", \"Survived\"]].groupby(\"SibSp\", as_index=False).mean().sort_values(by=\"Survived\", ascending=False) ","abe8a1cf":"# Parch - Survived\n\ntrain_df[[\"Parch\", \"Survived\"]].groupby(\"Parch\", as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","d1c01d31":"def detect_outliers(df, features):\n    outlier_indices=[]\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c], 25)\n        #3th quartile\n        Q3 = np.percentile(df[c], 75)\n        \n        # IQR \n        IQR = Q3-Q1\n        \n        # Outlier step\n        outlier_step = 1.5*IQR\n        \n        # Detect outlier and their indices\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        \n        # Store indices\n        outlier_indices.extend(outlier_list_col)\n        \n        \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i,v in outlier_indices.items() if v>2)\n    # If there is a one or two feature outlier belong to one passenger, it will stay.\n    # But if a passanger has more than two outlier in the features, i will remove it.\n        \n    return multiple_outliers","e84a87d7":"# outliers\n\ntrain_df.loc[detect_outliers(train_df, [\"Age\", \"SibSp\", \"Parch\", \"Fare\"])]","6a5a898a":"# dropping outliers\ntrain_df = train_df.drop(detect_outliers(train_df, [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]), axis = 0).reset_index(drop=True)","51bade15":"train_df_len = len(train_df)\n# i saved len because i will use train and test data seperately later.\ntrain_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n# be careful, if you run this code twice or more, every time test_df will be added to train_df again and again.\n# if you confused about it, reset and run all.","b9fc42d4":"train_df.columns[train_df.isnull().any()]","3230cefd":"train_df.isnull().sum().sort_values(ascending=False)","c2d316de":"# Filling missing Embarked values.\n\n#check missing values first\ntrain_df[train_df[\"Embarked\"].isnull()]","e9d380a5":"train_df.boxplot(\"Fare\", by= \"Embarked\")\nplt.show()","9cc667dd":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"C\")\ntrain_df[train_df[\"Embarked\"].isnull()] # For checking missing values.\n# It is empty because i filled missing values with \"C\".","928ad04d":"# filling missing Fare values\n\ntrain_df[train_df[\"Fare\"].isnull()]","10bdeade":"train_df[\"Fare\"][train_df[\"Pclass\"]==3].mean() # 3th class ticket prices' mean\n\ntrain_df[\"Fare\"] = train_df[\"Fare\"].fillna(train_df[\"Fare\"][train_df[\"Pclass\"]==3].mean())\n\ntrain_df[train_df[\"Fare\"].isnull()] \n# it is empty because i filled null Fare value with mean of 3th class ticket prices.","8c17bce6":"list1 = [\"SibSp\", \"Parch\", \"Age\", \"Fare\", \"Survived\"]\nsns.heatmap(train_df[list1].corr(), annot=True, fmt=\".2f\") \n# annot for writing numbers on graph\n# fmt for write only 2 digit after dot\nplt.show()","babcc0fa":"# SibSp - Survived \n\ng = sns.catplot(x=\"SibSp\", y=\"Survived\", data=train_df, kind=\"bar\", height=5)\nplt.ylabel(\"Survived Probability\")\nplt.show()","5a1e4be8":"# Parch - Survived\n\ng = sns.catplot(x = \"Parch\", y=\"Survived\", data = train_df, kind=\"bar\", height=5)\ng.set_ylabels(\"Survived probability\")\nplt.show()","61e81dd0":"# Pclass - Survived\ng= sns.catplot(x=\"Pclass\", y=\"Survived\", data=train_df, kind=\"bar\", height =5)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","b39de393":"# Age - Survived\n\ng = sns.FacetGrid(train_df, col=\"Survived\")\ng.map(sns.histplot, \"Age\", kde=True, bins=25)\nplt.show()","b93d5a14":"# Pclass - Survived - Age\n\ng = sns.FacetGrid(train_df, col=\"Survived\", row=\"Pclass\", height=2)\ng.map(plt.hist, \"Age\", bins=25)\ng.add_legend()\nplt.show()","7c4e5d95":"# Embarked - Sex - Pclass - Survived\ng = sns.FacetGrid(train_df, col= \"Embarked\", height=2)\ng.map(sns.pointplot, \"Pclass\", \"Survived\", \"Sex\", order = [1,2,3], hue_order=[\"male\", \"female\"])\n# order is class names\n# hu_order is color order of variables\ng.add_legend()\nplt.show()","059ca655":"# Embarked - Sex - Fare - Survived\n\ng = sns.FacetGrid(train_df, row=\"Embarked\", col=\"Survived\", height=2, aspect=1.4)\ng.map(sns.barplot, \"Sex\", \"Fare\", order=[\"female\", \"male\"])\ng.add_legend()\nplt.show()","6bf3abec":"# filling missing Age values\n\ntrain_df[train_df.Age.isnull()]","7bb71cce":"# Sex\n\nsns.catplot(x=\"Sex\", y=\"Age\", data=train_df, kind=\"box\")\nplt.show()","b3d6cca3":"# Pclass\n\nsns.catplot(x=\"Sex\", y=\"Age\", hue=\"Pclass\", data=train_df, kind=\"box\")\nplt.show()","077f5835":"# Parch and SibSp\n\nsns.catplot(x=\"Parch\", y=\"Age\",data=train_df, kind=\"box\")\nsns.catplot(x=\"SibSp\", y=\"Age\",data=train_df, kind=\"box\")\nplt.show()","2a0878b3":"# I want to check is there any relation between these features\n\n# Sex is string, I will change it to 1 and 0 to get a correlation result with sex. \n# Otherwise, heatmap won't show sex feature.\n\n# train_df[\"Sex\"] = [1 if i == \"male\" else 0 for i in train_df[\"Sex\"]]\n# I added this code to comment line because i will use sex feature when i train my model.\n# When you check heatmap, please remove #\n\n\n# creating heatmap to get corrolation between the features\nsns.heatmap(train_df[[\"Age\",\"Sex\", \"SibSp\", \"Parch\", \"Pclass\"]].corr(), annot=True)\nplt.show()","22a6e187":"# filling missing Age values\n\nindex_NaN_age = list(train_df[\"Age\"][train_df[\"Age\"].isnull()].index)\nfor i in index_NaN_age:\n    age_med = train_df[\"Age\"].median()\n    age_pred = train_df[\"Age\"][((train_df['SibSp'] == train_df.iloc[i][\"SibSp\"]) & (train_df['Parch'] == train_df.iloc[i][\"Parch\"]) & (train_df['Pclass'] == train_df.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred):\n        train_df[\"Age\"].iloc[i] = age_pred\n    else:\n        train_df[\"Age\"].iloc[i] = age_med","ba441719":"train_df[train_df.Age.isnull()]\n\n# I filled the missing values so there is no missing value in Age.","6851c08b":"train_df[\"Name\"].head(10)","44af8857":"name = train_df[\"Name\"]\ntrain_df[\"Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in name]","056b4115":"train_df[\"Title\"].head() ","be9592cb":"train_df[\"Title\"].unique()","e6352d31":"sns.countplot(x=\"Title\", data= train_df)\nplt.xticks(rotation=60)\nplt.show()","7781a8e7":"# convert to categorical\n\ntrain_df[\"Title\"] = train_df[\"Title\"].replace([\"Lady\",\"the Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"], \"other\")\ntrain_df[\"Title\"] = [0 if i == \"Master\" else 1 if i == \"Miss\" or i == \"Ms\" or i == \"Mlle\" or i == \"Mrs\" else 2 if i == \"Mr\" else 3 for i in train_df[\"Title\"]]\n# 0 for master, 1 for female, 2 for male, 3 for other.\ntrain_df[\"Title\"].head(10)\n\n# If you run this code twice, all will be 3. Be careful. \n# To avoid, run all if necessary. OR you can use different variable name.","a9a6ba23":"sns.countplot(x=\"Title\", data=train_df)\nplt.show()","7dd5083e":"# comparing survival rates\n \ng = sns.catplot(x=\"Title\", y=\"Survived\", data=train_df, kind=\"bar\")\ng.set_xticklabels([\"Master\",\"Ms\",\"Mr\",\"Other\"])\ng.set_ylabels(\"Survival Probability\")\nplt.show()","d8df5372":"train_df.drop(labels=[\"Name\"], axis=1, inplace=True)","063b75cd":"train_df.head()","a54a6a27":"train_df = pd.get_dummies(train_df, columns=[\"Title\"])\ntrain_df.head()\n\n# It created four new columns for title feature and its values are 0 and 1. ","35363632":"train_df.head()","b3401f0e":"train_df[\"Fsize\"] = train_df[\"Parch\"] + train_df[\"SibSp\"] + 1 \n\n#I added 1 for adding the person itself.","7e5c6ffe":"train_df.head()","fb70cfe6":"# Fsize and survival rate\n\ng = sns.catplot(x=\"Fsize\", y=\"Survived\", data=train_df, kind=\"bar\")\ng.set_ylabels(\"Survival Probability\")\nplt.show()","a1fc8637":"train_df[\"Family_size\"] = [1 if i < 5 else 0 for i in train_df[\"Fsize\"]]","aa6124f5":"train_df.head()","df009beb":"g = sns.countplot(\"Family_size\", data=train_df)\ng.set_xticklabels([\"5 or more\", \"less than 5\"])\nplt.show()","b3a5ed48":"g = sns.catplot(x=\"Family_size\", y=\"Survived\", data= train_df, kind=\"bar\")\ng.set_xticklabels([\"5 or more\", \"less than 5\"])\ng.set_ylabels(\"Survival Probability\")\nplt.show()","5d390525":"train_df = pd.get_dummies(train_df, columns=[\"Family_size\"])\ntrain_df.head()","d2b14d43":"train_df[\"Embarked\"].head()","d8affbb1":"sns.countplot(x=\"Embarked\", data=train_df)\nplt.show()","0ac2a3ff":"train_df = pd.get_dummies(train_df, columns=[\"Embarked\"])\ntrain_df.head()","500322c2":"train_df[\"Ticket\"].head(20)","c5db2caa":"tickets = []\n\nfor i in list(train_df.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"x\")\n        \ntrain_df[\"Ticket\"] = tickets","8b51c547":"train_df.head()","62b3b51a":"train_df = pd.get_dummies(train_df, columns=[\"Ticket\"], prefix = \"T\")\ntrain_df.head()\n\n# prefix = \"T\" for the short version of Ticket","f8682d1a":"train_df.Pclass.head()","a846419c":"sns.countplot(train_df.Pclass)\nplt.show()","d3616418":"train_df[\"Pclass\"] = train_df[\"Pclass\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns=[\"Pclass\"])\ntrain_df.head()","dc24d984":"train_df[\"Sex\"] = train_df[\"Sex\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns=[\"Sex\"])\ntrain_df.head()","f08514e9":"train_df.drop(labels=[\"PassengerId\", \"Cabin\"], axis=1, inplace=True)\ntrain_df.columns","c67b362e":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","e7b8a489":"train_df_len","4048d8c4":"test = train_df[train_df_len:]\ntest.drop(labels=[\"Survived\"], axis=1, inplace = True)","9ff352ae":"test.head()","7dbea66d":"train = train_df[:train_df_len]\nX_train = train.drop(labels=[\"Survived\"], axis=1)\ny_train = train[\"Survived\"]\nX_train,X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n# test_size = 0.33 mean is 0.33 data will be test data, other data will be train data.\nprint(\"X_train: \", len(X_train))\nprint(\"y_train: \", len(y_train))\nprint(\"X_test: \", len(X_test))\nprint(\"y_test: \", len(y_test))\nprint(\"Test: \", len(test))","b84a9a6d":"logreg = LogisticRegression() \n# I will continue with default parameters but i will change them later.\n\nlogreg.fit(X_train,y_train)\n\nacc_log_train = round(logreg.score(X_train, y_train)*100, 2)\nacc_log_test = round(logreg.score(X_test, y_test)*100, 2)\nprint(\"Training accuracy: % {}\".format(acc_log_train))\nprint(\"Testing accuracy: % {}\".format( acc_log_test))","e61b52d6":"random_state = 42 \n\nclassifier = [DecisionTreeClassifier(random_state = random_state), \n             SVC(random_state=random_state),\n             RandomForestClassifier(random_state=random_state),\n             LogisticRegression(random_state=random_state),\n             KNeighborsClassifier()]\n\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,200,300,1000]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]","8f8fa548":"cv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(X_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","eeadc4a4":"cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result,\n                           \"ML Models\":[\"DecisionTreeClassifier\",\n                                        \"SVM\",\"RandomForestClassifier\",\n                                        \"LogisticRegression\",\n                                        \"KNeighborsClassifier\"]})\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","526fba11":"votingC = VotingClassifier(estimators = [(\"dt\",best_estimators[0]),\n                                        (\"rfc\",best_estimators[2]),\n                                        (\"lr\",best_estimators[3])],\n                                        voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(X_train, y_train)\nprint(accuracy_score(votingC.predict(X_test),y_test))","83cb8b29":"test_survived = pd.Series(votingC.predict(test), name = \"Survived\").astype(int)\nresults = pd.concat([test_PassengerId, test_survived],axis = 1)\nresults.to_csv(\"titanic_N.csv\", index = False)","0c36de69":"When I checked the box plot I saw that people who paid 80.0 fare are abord from C generally. So the probability of embarked value is C is high for the people who paid 80.0. ","e94b3da6":"1st class passengers are older than 2nd, and 2nd is older than 3rd class.","39dab393":"<p id=\"11\"><\/p> <br>\n\n# 6. Visualisation\n\nFirst of all, I will check correlation between \"SibSp\", \"Parch\", \"Age\", \"Fare\", \"Survived\". ","dbe0c156":"#### Result:\nI will use Pclass for model training. It tells important points according to survival rate for classes.","563f0ff0":"#### Ensemble Modelling\nI will ensemble succesfull ml models. ","f25b78ef":"#### Train - Test - Split","85928528":"I should fill missing values logically. I will use Fare values to find right Embarked values. I will check from which embarked station the other passengers who paid 80.0 for the ticket used. ","16e65845":"#### Result: \n* There is no relationship between age and sex so I cannot use it to age prediction.\n* But Age is correlated with parch, sibsp and pclass.","25b9f2b4":"#### Examining Sex feature","5a337bec":"<p id=\"6\"><\/p> <br>\n\n# 3. Basic data analysis\n\nAre there any variables related to survival? I opened this section to answer this question. I will examine the relationship of the variables below.\n\n* Pclass - Survived\n* Sex - Survived\n* SibSp - Survived\n* Parch - Survived","2ae416ff":"<p id=\"13\"><\/p> <br>\n\n# 8. Modeling","9ef895ee":"#### Result:\n* Age <= 10 has a high survival rate.\n* Oldest passengers (80) survived.\n* Large number of 20 years old did not survive.\n* Most passengers are in 15-35 age range.\n* I can use the age feature in training.\n* I can use the age distribution to fill the missing values of age.","427dbfec":"<p id=\"2\"><\/p> <br>\n\n# 2. Variable description\n\n1. PassengerId: unique id number\n1. Survived: survive(1) or died(0)\n1. Pclass: passenger class\n1. Name: name of the passenger \n1. Sex: gender of the passenger \n1. Age: age of the passenger \n1. SibSp: the number of siblings\/spouses\n1. Parch: the number of parents\/children \n1. Ticket: ticket number \n1. Fare: the amount of the money spent on ticket\n1. Cabin: cabin category\n1. Embarked: the port where the passenger embarked (C = Cherbourg, Q = Queenstown, S = Southampton)","025ea892":"#### Result:\n* Passsengers who pay higher fare have better survival rate. Fare can be used as categorical for training.","fd72e677":"<p id=\"3\"><\/p> <br>\n\n### A. Univariate Variable Analysis \n\n* Categorical Variable: Survived, Sex, Pclass, Embarked, Cabin, Name, Ticket, Sibsp and Parch\n* Numerical Variable: Fare, age and passengerId","025b0103":"<p id=\"1\"><\/p> <br>\n\n# 1. Load and look at the data","a7579a1e":"I will fill missing Age values later (in the end of Visualisation part.)","159236a6":"##### Result: \nThose with more than 2 siblings\/spouses have a low survival rate.","7e76e570":"##### Result: \nThose with less than 3 parents\/children have a high survival rate.","4cad3e3d":"<p id=\"top\"><\/p> <br>\n\n# Introduction:\n\nIn this kernel, I will load Titanic data set and explore the data with EDA (Exploratory data analysis). I will use Seaborn for visualisation. I will use sklearn library to create and train my model.\n\n# Content:\n\n1. [Load and look at the data](#1)\n1. [Variable description](#2)\n    * [Univariate Variable Analysis](#3)\n        * [Categorical Variable](#4)\n        * [Numerical Variable](#5)\n1. [Basic data analys](#6)\n1. [Outlier detection](#7) \n1. [Missing value](#8)\n    * [Find missing value](#9)\n    * [Fill missing value](#10) \n1. [Visualisation](#11) \n1. [Feature engineering](#12)\n1. [Modeling](#13)\n","51ac1cf7":"##### Result: \nFemale passengers have a higher survival rate, while less male passengers survived.","d52d606d":"#### Simple Logistic Regression","615a1ef7":"I can use Pclass feature as a reference point. I will check how much money people who in 3th class paid for a ticket.","f2469f18":"#### Examining Embarked feature","c3ef0cc3":"#### Hyperparameter Tuning -- Grid Search -- Cross Validation\n\n- I will compare 5 machine learning classifier and evaluate mean accuracy of each of them by stratified cross validation.\n    * Decision Tree\n    * SVM\n    * Random Forest\n    * KNN\n    * Logistic Regression","b42540d6":"There are some rare titles like Dr, Sir etc. We can create categorical value for them. ","ae40129b":"<p id=\"5\"><\/p> <br>\n\n#### b. Numerical Variable","ccb17d45":"#### Extract Family size feature from SibSp and Parch","67f8b519":"<p id=\"10\"><\/p> <br>\n\n### B. Fill missing value\n\n* Embarked has 2 missing values.\n* Fare has only 1 missing values.\n* Age has 256 missing values.","b1a53701":"<p id=\"7\"><\/p> <br>\n\n# 4. Outlier Detection","0e46c85b":"<p><a href=\"#top\">This link goes to the top<\/a><\/p>","2eb9d06a":"#### Examining Pclass feature","48f5c367":"#### Result:\n* Sibsp and parch can be used together to extract a new feature with th (threshold) = 3.\n* Small familes have more chance to survive. (2-3 members)\n* There is a high standard deviation in survival of passenger with parch = 3 (black line represent the standard deviation).","e688e4ec":"#### Result:\n* Female passengers have much better survival rate than males.\n* Embarked and sex will be used in training.","6fb20002":"In this part, I will prepare my data for machine learning step. I will create new features or drop unnecessary ones.\n* I will start with extracting title feature from name.\n* I will extract Family size feature from SibSp and Parch.\n* I will examine Embarked, Ticket, Pclass and Sex features.\n* I will drop Passenger ID and Cabin features.","48df3bfa":"Small familes have more chance to survive than large families.","1bf8fcde":"##### Result: \nThe 1st class has a higher survival rate, the 3rd class has less survival.","8480891c":"* float64(2): Fare ve Age\n* int64(5): Pclass, sibsp, parch, passengerId and survived\n* object(5): Cabin, embarked, ticket, name and sex","9b525895":"<p id=\"12\"><\/p> <br>\n\n# 7. Feature engineering","cc4c7a15":"#### Extracting title feature from name","a6a335b1":"#### Filling missing Age values","67114966":"Filling Age is complex so i will do that step later. Null survived values is coming from test data set. And i will use it when i create a model. I won't use cabin. That's why, i will fill only Embarked and Fare null values.","2c98aeed":"Fare feature seems to have correlation with survived feature (0.26).\n\nFor detailed examination, I will check correlation between:\n* SibSp - Survived\n* Parch - Survived\n* Pclass - Survived\n* Age - Survived\n* Pclass - Survived - Age\n* Embarked - Sex - Pclass - Survived\n* Embarked - Sex - Fare - Survived","d4e51b01":"#### Result:\n* Having a lot of SibSp causes less chance to survive.\n* If sibsp == 0 or 1 or 2, passenger has more chance to survive.\n* we can create a new feature to describe these categories. Then we can use it when we create and train a model.\n","bf280db5":"#### Dropping Passenger ID and Cabin features","061ff354":"* Up to 4 members, the probability of survival increases as the family grows. It drops after 4.\n* I will create two categorical value for it. Threshold will be 5","4d2a251d":"Sex is not informative for age prediction, age distribution seems to be same.","f6450d83":"#### Examining Ticket feature","6641777c":"#### Prediction","4732fae9":"I can investigate seperately or together that the age values of Sex, Pclass, Parch and SibSp to find mean age value for these features. Then, I can fill in the missing values with the mean value.  ","9bcf4da8":"<p id=\"9\"><\/p> <br>\n\n### A. Find missing value","70a32d2e":"<p id=\"4\"><\/p> <br>\n\n#### a. Categorical Variable ","daebd10b":"<p id=\"8\"><\/p> <br>\n\n# 5. Missing value","95b54371":"- Female and master have high survival rate. Male has lowest survival rate.\n- I won't use name feature while I train my model so i will drop it from the dataframe.","be43b8ca":"#### Result:\n* I can use Pclass feature in trainin my model directly.","89042856":"I will get the passengers' titles from the name feature. I can use this new feature to examine the relationship between survival rate and marriage status.  "}}