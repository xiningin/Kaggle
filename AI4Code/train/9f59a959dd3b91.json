{"cell_type":{"11e31016":"code","d5a2dcf0":"code","ea46196f":"code","9df87dc1":"code","508f5a03":"code","717742b2":"code","27e5c7ad":"code","6c950313":"code","b5fd3f20":"code","1a6b067e":"code","f988ee71":"code","9e6c8e99":"code","08842a36":"code","56aa37f3":"code","473e3f57":"code","d53d6e6d":"code","3be7ca8d":"code","3a5ba035":"code","0fabfed4":"code","fb8e6da5":"code","aac15022":"code","743b852a":"code","0e535b01":"code","e1816166":"code","878f181a":"code","2febc3eb":"code","8007a46f":"code","0b324879":"code","7b568847":"code","a7cb9ddd":"markdown","3ff4064b":"markdown","3b5f19a9":"markdown","5584707e":"markdown","529e97b8":"markdown","4870b046":"markdown","918b1c44":"markdown","a6ac6619":"markdown","b664aacc":"markdown","8c5a4023":"markdown","75f4358d":"markdown","df8d3410":"markdown","974ec5f8":"markdown","5e5b6f43":"markdown","62c85890":"markdown","e67c793d":"markdown","e850781c":"markdown","7dc53e73":"markdown","38f1777b":"markdown","469a5f83":"markdown","ad484f52":"markdown","ad4b064d":"markdown"},"source":{"11e31016":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d5a2dcf0":"# Additional imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data splitting\/parameter tuning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\n\n# ML models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\n\n# Feature processing\nfrom sklearn.feature_selection import SelectPercentile, chi2\n\n# Evaluation metrics\nfrom sklearn.metrics import confusion_matrix","ea46196f":"heart_path = \"..\/input\/heart.csv\"\nheart_data = pd.read_csv(heart_path)","9df87dc1":"heart_data.head(5)","508f5a03":"print(\"Heart data shape is:\", heart_data.shape[0], \"x\", heart_data.shape[1])","717742b2":"# Missing values\nheart_data.isnull().sum()","27e5c7ad":"sns.distplot(heart_data[\"age\"], bins=4, kde=False)","6c950313":"sns.countplot(heart_data[\"sex\"])","b5fd3f20":"total = len(heart_data[\"sex\"])\nmales = heart_data[\"sex\"].sum()\nfemales = len(heart_data[\"sex\"]) - males\nprint(\"Porcentage of males:\", round(males\/total*100, 3))\nprint(\"procentage of females:\", round(females\/total*100, 3))","1a6b067e":"sex_graph = sns.countplot(heart_data[\"sex\"], hue=heart_data[\"target\"])\nsex_graph.set_ylabel(\"amount\")","f988ee71":"cp_graph = sns.countplot(heart_data[\"cp\"], color=\"purple\")\ncp_graph.set_xlabel(\"type of chest pain\")\ncp_graph.set_ylabel(\"amount\")","9e6c8e99":"plt.figure(figsize=(10, 10))\nsns.heatmap(heart_data.corr(), annot=True, fmt='.2f')","08842a36":"heart_data.dtypes","56aa37f3":"heart_data['sex'] = heart_data['sex'].astype('object')\nheart_data['cp'] = heart_data['cp'].astype('object')\nheart_data['fbs'] = heart_data['fbs'].astype('object')\nheart_data['restecg'] = heart_data['restecg'].astype('object')\nheart_data['exang'] = heart_data['exang'].astype('object')\nheart_data['slope'] = heart_data['slope'].astype('object')\nheart_data['thal'] = heart_data['thal'].astype('object')","473e3f57":"heart_data.dtypes","d53d6e6d":"heart_data = pd.get_dummies(heart_data)\nheart_data.head()","3be7ca8d":"print(\"Heart data shape is:\", heart_data.shape[0], \"x\", heart_data.shape[1])","3a5ba035":"# Getting features and target\nX = heart_data.drop([\"target\"], axis=1)\ny = heart_data[\"target\"]","0fabfed4":"# Random Forest\nrf_model = RandomForestClassifier(n_estimators=100)\nrf_predictions = cross_val_predict(rf_model, X, y, cv=5)\nprint(confusion_matrix(y, rf_predictions))\nrf_scores = cross_val_score(rf_model, X, y, scoring=\"recall\", cv=5)\nprint(\"recall:\", rf_scores.mean())","fb8e6da5":"# Logistic Regression\nlr_model = LogisticRegression(solver=\"liblinear\")\nlr_predictions = cross_val_predict(lr_model, X, y, cv=5)\nprint(confusion_matrix(y, lr_predictions))\nlr_scores = cross_val_score(lr_model, X, y, scoring=\"recall\", cv=5)\nprint(\"recall:\", lr_scores.mean())","aac15022":"# Support Vector Machine\nsvc_model = SVC(gamma=\"auto\")\nsvc_predictions = cross_val_predict(svc_model, X, y, cv=5)\nprint(confusion_matrix(y, svc_predictions))\nsvc_scores = cross_val_score(svc_model, X, y, scoring=\"recall\", cv=5)\nprint(\"recall:\", svc_scores.mean())","743b852a":"# Naive Bayes\nnb_model = GaussianNB()\nnb_predictions = cross_val_predict(nb_model, X, y, cv=5)\nprint(confusion_matrix(y, nb_predictions))\nnb_scores = cross_val_score(nb_model, X, y, scoring=\"recall\", cv=5)\nprint(\"recall:\", nb_scores.mean())","0e535b01":"# XGBoost (The most popular model for kaggle competitions)\nxgb_model = XGBClassifier()\nxgb_predictions = cross_val_predict(nb_model, X, y, cv=5)\nprint(confusion_matrix(y, xgb_predictions))\nxgb_scores = cross_val_score(xgb_model, X, y, scoring=\"recall\", cv=5)\nprint(\"recall:\", xgb_scores.mean())","e1816166":"X[\"age\"] = X[\"age\"].map(lambda x: (x - X[\"age\"].min()) \/ (X[\"age\"].max() - X[\"age\"].min()))\nX[\"trestbps\"] = X[\"trestbps\"].map(lambda x: (x - X[\"trestbps\"].min()) \/ (X[\"trestbps\"].max() - X[\"trestbps\"].min()))\nX[\"chol\"] = X[\"chol\"].map(lambda x: (x - X[\"chol\"].min()) \/ (X[\"chol\"].max() - X[\"chol\"].min()))\nX[\"thalach\"] = X[\"thalach\"].map(lambda x: (x - X[\"thalach\"].min()) \/ (X[\"thalach\"].max() - X[\"thalach\"].min()))\nX[\"oldpeak\"] = X[\"oldpeak\"].map(lambda x: (x - X[\"oldpeak\"].min()) \/ (X[\"oldpeak\"].max() - X[\"oldpeak\"].min()))","878f181a":"# Support Vector Machine\nsvc_model = SVC(gamma=\"auto\")\nsvc_predictions = cross_val_predict(svc_model, X, y, cv=5)\nprint(confusion_matrix(y, svc_predictions))\nsvc_scores = cross_val_score(svc_model, X, y, scoring=\"recall\", cv=5)\nprint(\"recall:\", svc_scores.mean())","2febc3eb":"best_recall = 0\nfor n in range(1, 101):\n    X_new = SelectPercentile(chi2, percentile=n).fit_transform(X, y)\n\n    svc_model = SVC(gamma=\"auto\")\n    svc_predictions = cross_val_predict(svc_model, X_new, y, cv=5)\n    svc_scores = cross_val_score(svc_model, X_new, y, scoring=\"recall\", cv=5)\n    \n    if svc_scores.mean() > best_recall:\n        best_recall = svc_scores.mean()\n        print(confusion_matrix(y, svc_predictions))\n        print(\"the best porcentage so far:\", n)\n        print(\"the best recall so far\", svc_scores.mean(), \"\\n\")\n        ","8007a46f":"X_new = SelectPercentile(chi2, percentile=33).fit_transform(X, y)        \nsvc_model = SVC(gamma=\"auto\")\nsvc_predictions = cross_val_predict(svc_model, X_new, y, cv=5)\nprint(confusion_matrix(y, svc_predictions))\nsvc_scores = cross_val_score(svc_model, X_new, y, scoring=\"recall\", cv=5)\nprint(\"recall:\", svc_scores.mean(), \"\\n\")\n\nprint(\"Old number of features used:\",X.shape[1])\nprint(\"New number of features used:\",X_new.shape[1])","0b324879":"# EXPLORE FOR BETTER WAY TO KNOW BEST FEATURES AFTER FEATURE SELECTION\n\n#plt.figure(figsize=(20, 20))\n#sns.heatmap(heart_data.corr(), annot=True, fmt='.2f')","7b568847":"\n\nCs = [1, 10, 100, 1000]\nkernels = [\"linear\", \"rbf\", \"poly\"]\n\nfor c in Cs:\n    for k in kernels:\n        \n        print(\"C:\", c)\n        print(\"Kernel:\", k)\n        svc_model = SVC(gamma=\"auto\", C=c, kernel=k)\n        svc_predictions = cross_val_predict(svc_model, X_new, y, cv=5)\n        print(confusion_matrix(y, svc_predictions))\n        svc_scores = cross_val_score(svc_model, X_new, y, scoring=\"recall\", cv=5)\n        print(\"recall:\", svc_scores.mean(), \"\\n\")\n\n\n","a7cb9ddd":"# Cross validation for parameter tuning\n\nFinally, we will explore many combinations of parameters using something called in sklearn [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html). This time we only will search for **C** and **kernel** parameters. And to be things more enjoyable we will code up by ourself. It is pretty easy!!!","3ff4064b":"# Feature selection\n\nCurrently, We are using all features available in the dataset, however:\n\n**features != information**\n\nMore features not always lead to better performance, what we really want is the exact number of features that can capture the essence. Many times datasets come with lots of features but often times just small portion of them drives the performance of a model.\n\nCheck out this [post](https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e) for more info on feature selection\n\nThis time we are going to use [SelectPercentile](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectPercentile.html) to get the exact porcentage of features that lead to the best performance on the evaluation metric.","3b5f19a9":"* **Almost, half of the people are asymptomatic.**","5584707e":"Due to the description of the features, some of them are categorical not numbers, so let's fix that.","529e97b8":"# Preprocessing data","4870b046":"Let's load the data and take a look","918b1c44":"* **More than 2\/3 of the population in the dataset are male**","a6ac6619":"We use five ML models. On average, four of them has a similar performance. The support vector machine was the only one that had clearly a different performance. A really good number for recall, but if we do the math for other metrics the numbers are not that impressive. Although metrics most of the time have some trade off between them.","b664aacc":"Using feature selection, we got rid of irrelevant features and now we are using just the ones that make our model better. Before we used 26 features, now we are using just 9 of them. Roughly 1\/3 of them.\n\nIf you are curious about what are the features we are using run the cell below. They are, the nine that have the highest correlation with the target.","8c5a4023":"A very important step to apply is to one-hot encode categorical features. Allowing us to use these features properly.","75f4358d":"# Data exploration\n\nLet's take a deeper look at some of the features. We are going to make some visualizations to make things more obvious to us and get some insights about our data. Kaggle does a great and simply job capturing the most important things to make awesome [visualizations](https:\/\/www.kaggle.com\/learn\/data-visualization)","df8d3410":"These two are the best results:\n\n* The default configuration (C=1 and kernel=\"rbf\"), having a **recall of 0.915** \n\n* Using C=1 and kernel=\"poly\", having a **recall of 0.951**\n\nApparently what the model does if we change the kernel parameter to \"poly\" is to misclassify **22 true negatives** to **22 false positives**. In other words, 22 people that are healthy are classified as sick.\nAnd correctly classify **6 more true positives** previously misclassified as **6 false negatives**. In other words, 6  more people are correctly classified as sick.\n\nSo, what is a better model?\n\nI personally think that the second one is better for this task and this is why:\n\nAs I mentioned before in a previous cell **false negatives** have a highest cost as a whole compared to **false positives**.\n\nLet's say for the sake of argument that we use this model as tool to help doctors to diagnose patients that possibly can suffer a heart desease, so if someone is classified as sick, the doctor could use later on, more sofisticated tools to dispel any doubt in the diagnosis and take wiser decisions about the health of the patient. By the other hand, if a sick person is sent to home as healthy could potential aggravate his or her current state without any supervision and eventually could die.\n\nBut, what do you think? is this trade off worth it?","974ec5f8":"# Machine learning\n\nLet's try some models to see their performance. Due to the fact that is a small dataset, we are going to use cross_validation rather than the usual train_test_split. To evaluate our models we will use \"recall\".\n\n**We are using a health dataset and exist a high cost associated with False Negative (people with heart desease with the wrong diagnosis). That is the main number we want to minimize, and recall captures exactly what we want. This is the ecuation for recall:**\n\n**Recall = True Positive \/ (True Positive + False Negative)**\n\nMore information on evaluation metric could be found [here](https:\/\/towardsdatascience.com\/accuracy-precision-recall-or-f1-331fb37c5cb9).","5e5b6f43":"# Focus on SVM\n\nBesides that, SVM is showing the most interesting results. Due to the fact that is  already doing a great job on false negatives (but having a hard time knowing when someone is not sick). However I think it is a good idea to choose SVM as main model.","62c85890":"The recall now is lower, but applying feature scaling, the SVM is doing a better job classifying the people in general. It is a more robust classifier compared to the previous one.","e67c793d":"# Description of columns\n\n* **age:** The person's age in years\n* **sex:** The person's sex (1 = male, 0 = female)\n* **cp:** The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n* **trestbps:** The person's resting blood pressure (mm Hg on admission to the hospital)\n* **chol:** The person's cholesterol measurement in mg\/dl\n* **fbs:** The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n* **restecg:** Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n* **thalach:** The person's maximum heart rate achieved\n* **exang:** Exercise induced angina (1 = yes; 0 = no)\n* **oldpeak:** ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n* **slope:** the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n* **ca:** The number of major vessels (0-3)\n* **thal:** A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n* **target:** Heart disease (0 = no, 1 = yes)","e850781c":"# Feature scaling\n\nWe are going to go further using SVM, so let's apply feature scaling to our data. That should make our SVM model better. The type of scaling we will apply is called [MinMaxScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html) in sklearn.\n\nIt is important to remember the fact that this is a really good practice to do if we want","7dc53e73":"* **There are not a huge disparity between the number of healthy and unhealthy men. The difference is more notoriuos for females.**","38f1777b":"* **Approximately, 80% of the people are in the range of 41-65.**","469a5f83":"Let's do a quick round with multiple machine learning models, and see  what are the most promising.\nTo each model we will print the **confusion matrix** and the **recall** value, not only to just see the score itself, but also the actual number associated to each category (true positive, true negative, false positive, false negative).\nHere is the [link](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html) for reference.\n\n<img src=\"https:\/\/i.imgur.com\/uipmEwt.png\" width=\"800px\">","ad484f52":"# If you like this kernel, please upvote it.\n# See you next time!!!","ad4b064d":"* **A summary of the correlation of every pair of features. The ones that are more correlated with the target are cp, thalach and slop in that order. This is a good way to later select the most promising features for our final model, although there are other more sophisticated and automated ways to select the best features.**"}}