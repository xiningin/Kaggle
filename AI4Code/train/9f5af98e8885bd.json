{"cell_type":{"c167fe5a":"code","1daceff3":"code","4de27717":"code","6746c452":"code","766fd032":"code","ddcd8eff":"code","68fb385e":"code","c88f4e83":"code","6bca8367":"code","c5d8b21f":"code","0d0adfb4":"code","c46c3faa":"code","5b6cd480":"code","ccfb523a":"code","84773d39":"code","d0bd4c61":"code","77c59f9c":"code","b0953685":"code","0d1ff6cb":"code","fa3fbc5d":"code","6742394c":"code","12d2ee22":"code","441a8aad":"code","c7f3ead5":"code","8960da27":"code","d7341b79":"code","c6e7a312":"code","4a5ab1d9":"code","59c33ab5":"code","95559e45":"code","a394eb85":"code","feb929df":"code","af80b3b3":"code","3184a985":"markdown","f8ff5e50":"markdown","64163041":"markdown","5fbddab7":"markdown","cbf6484e":"markdown"},"source":{"c167fe5a":"! pip install pyunpack PyWavelets pyts fire nvidia-ml-py3","1daceff3":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","4de27717":"! git clone https:\/\/github.com\/timeseriesAI\/timeseriesAI1  ","6746c452":"! ln -s timeseriesAI1\/fastai_timeseries .\n! ln -s timeseriesAI1\/torchtimeseries .","766fd032":"import fastai, os\nfrom fastai_timeseries import *\nfrom torchtimeseries.models import *\nfrom fastai.callbacks import *\nimport random\n\npath = Path('\/kaggle\/input\/data-without-drift')\n\nprint('fastai :', fastai.__version__)\nprint('torch  :', torch.__version__)\nprint('device :', device)","ddcd8eff":"#plotting fn from https:\/\/www.kaggle.com\/miklgr500\/ghost-drift-and-outliers\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('dark_background')","68fb385e":"# https:\/\/www.kaggle.com\/miklgr500\/ghost-drift-and-outliers\n\ndef plot_open_channels_signal(df: pd.DataFrame, vline=[]):\n    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    \n    ax.plot(df.signal, df.open_channels, '.', color='fuchsia', alpha=0.25)\n    for x in vline:\n        ax.axvline(x, alpha=0.75, color='tomato')\n    ax.set_xlabel('Signal')\n    ax.set_ylabel('Open Channels')\n    plt.show()\n    \n    \ndef plot_data(df: pd.DataFrame):\n    if 'open_channels' in df.columns:\n        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 16))\n    \n        ax2.plot(df.time, df.open_channels, color='royalblue', alpha=0.75)\n        ax2.set_xlabel('time')\n        ax2.set_ylabel('Open Channels')\n    else:\n        fig, ax1 = plt.subplots(1, 1, figsize=(24, 8))\n    \n    ax1.plot(df.time, df.signal, color='royalblue', alpha=0.75)\n    ax1.set_xlabel('time')\n    ax1.set_ylabel('Signal')\n    plt.show()\n","c88f4e83":"file_tr = 'train_clean.csv'\nfile_tst = 'test_clean.csv'\n\ndf_train_all=pd.read_csv(path\/file_tr,  ) \ndf_test_all=pd.read_csv(path\/file_tst,  )\n#df_valid=pd.read_csv(path\/file_val,  )","6bca8367":"def remove_bad_signal(data):\n    # https:\/\/www.kaggle.com\/hirayukis\/lightgbm-keras-and-4-kfold?scriptVersionId=32154310\n    # read data\n    #data = pd.read_csv('..\/input\/data-without-drift\/train_clean.csv')\n    data.iloc[478587:478588, [1]] = -2  #reset spike siugnals x2\n    data.iloc[478609:478610, [1]] = -2\n    data_ = data[3500000:3642922].append(data[3822754:4000000])  # cut off error signal from DF\n    data = data[:3500000].append(data[4000000:]).reset_index().append(data_, ignore_index=True)\n    return data\n    #data.head()\n    #data[[\"signal\", \"open_channels\"]].plot(figsize=(19,5), alpha=0.7)\n","c5d8b21f":"plot_data(df_train_all)","0d0adfb4":"plot_data(df_test_all)","c46c3faa":"#500k samples per group\ngp_size = 500_000\n\nfor df in [df_train_all, df_test_all]:\n  batches = df.shape[0] \/\/ gp_size\n  df['batch'] = 0\n  for i in range(batches):\n        idx = np.arange(i*gp_size, (i+1)*gp_size)\n        df.loc[idx, 'batch'] = i ","5b6cd480":"df_train_all = remove_bad_signal(df_train_all)  #remove bad signals from set 0 & 7","ccfb523a":"plot_data(df_train_all)","84773d39":"def get_db(train_list, valid_list, test_list,  scale_type, bs=1024):\n\n  # selecting rows based on condition \n  df_train = df_train_all.loc[df_train_all['batch'].isin(train_list)] \n  df_valid = df_train_all.loc[df_train_all['batch'].isin(valid_list)] \n\n  df_test = df_test_all.loc[df_test_all['batch'].isin(test_list)]\n\n  # split_by_df\n  df_train['is_valid']=False\n  df_valid['is_valid']=True\n  df_combine = pd.concat([df_train, df_valid], axis=0, sort=False)\n\n  offset = random.randint(0, 20000)\n  train_size=df_train.shape[0]\n  valid_size=df_valid.shape[0]\n  test_size=df_test.shape[0]\n  train_idx = 0  #offset + train_size\n  valid_idx = 0  #valid_size + train_idx\n  test_idx = 0  #test_size + valid_idx\n  print ('training set= ',train_size, 'train index=', train_idx)\n  print ('valid set= ',valid_size, 'valid index=', valid_idx)\n  print('test set =', test_size, 'test index=', test_idx)\n  print ('total length', test_size+train_size+valid_size)\n  print ('Dataset= ', df_train.shape[0] )\n\n  bs = bs                           # \u2733\ufe0f orig 1024\n  #seed = 8888                        # \u2733\ufe0f\n  scale_type = scale_type          # \u2733\ufe0f \n  scale_by_channel = True            # \u2733\ufe0f \n  scale_by_sample  = False           # \u2733\ufe0f \n  scale_range = (-1, 1)              # \u2733\ufe0f \n\n  db = (TimeSeriesList.from_df(df_combine, '.', cols=[\"signal\"],)  # feat='feat')\n      #.split_by_idx(list(range(train_size, train_size+valid_size)) )\n      .split_from_df(col='is_valid')\n      .label_from_df(cols='open_channels', label_cls=CategoryList)\n      .add_test(TimeSeriesList.from_df(df_test, '.', cols=[\"signal\"]) )\n      .databunch(bs=bs,  val_bs=bs,  num_workers=cpus,  device=device)\n      .scale(scale_type=scale_type, scale_by_channel=scale_by_channel, \n             scale_by_sample=scale_by_sample,scale_range=scale_range)\n     )\n  return db, df_test","d0bd4c61":"def main(\n        epochs: 10,\n        bs:    1024,\n        runs:  1, \n        train_list: [],\n        valid_list: [],\n        test_list: [],\n        scale_type: 'normalize', \n        ):\n\n\n    global df_result, learn\n\n    bs = bs                           # \u2733\ufe0f orig 1024\n    scale_type = scale_type          # \u2733\ufe0f \n\n    \n    # ResCNN, FCN, InceptionTime, ResNet\n    arch = InceptionTime                     # \u2733\ufe0f   \n    arch_kwargs = dict()           # \n   \n    \n    db, df_result = get_db(train_list, valid_list, test_list, scale_type, bs)\n    print('# class= ', db.c, 'features= ', db.features)\n\n    epochs = epochs         # \u2733\ufe0f orig 100\n    max_lr = 1e-2        # \u2733\ufe0f orig 1e-2\n    warmup = True       # \u2733\ufe0f orig False\n    pct_start = .7       # \u2733\ufe0f\n    metrics = [accuracy] # \u2733\ufe0f\n    wd = 1e-2\n\n    \n    for run in range(runs):\n        print(f'Run: {run}')\n        model = arch(db.features, db.c, **arch_kwargs).to(device)\n        learn = Learner(db, model, opt_func=Ranger)\n\n        learn.metrics = metrics\n        learn.fit_fc(epochs, max_lr,  callbacks=[OverSamplingCallback(learn), SaveModelCallback(learn, monitor='accuracy')] ) \n\n        preds, tgt = learn.get_preds(ds_type=DatasetType.Test) # ds_type=DatasetType.Valid\n        test_preds = preds.argmax(-1).view(-1).numpy()\n        df_result[f'Run_{run}'] = test_preds\n\n    ","77c59f9c":"#trg set 0 & 7 have spikes\n# 10 channel model\nruns = 1\nepochs = 3\n\nkwargs = ( {'epochs': epochs, 'bs': 1024, 'runs': runs, 'train_list': [4], 'valid_list': [9], 'test_list': [1], 'scale_type': 'normalize' })  # 10 chan\nmain(**kwargs)","b0953685":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(10,10), dpi=60)","0d1ff6cb":"df_mod1 = df_result.copy()","fa3fbc5d":"#trg set 0 & 7 have spikes\n#5 channel model\n\nkwargs = ( {'epochs': epochs, 'bs': 1024, 'runs': runs, 'train_list': [5], 'valid_list': [8], 'test_list': [0], 'scale_type': 'normalize' })  #up to 5 chan\nmain(**kwargs)","6742394c":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(10,10), dpi=60)","12d2ee22":"df_mod2 = df_result.copy()","441a8aad":"#trg set 0 & 7 have spikes\n#3 channel model\n\nkwargs = ( {'epochs': epochs, 'bs': 1024, 'runs': runs, 'train_list': [3], 'valid_list': [7], 'test_list': [2,3], 'scale_type': 'normalize' })  \nmain(**kwargs)","c7f3ead5":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(10,10), dpi=60)","8960da27":"df_mod3 = df_result.copy()","d7341b79":"df_model = pd.concat([df_mod1, df_mod2, df_mod3], axis=0).sort_values(['time'])","c6e7a312":"if runs == 1 :\n  df_vote = df_model[['Run_0']]\nelif runs == 3 :\n  df_vote = df_model[['Run_0', 'Run_1', 'Run_2']]\nelif runs == 5 :  \n  df_vote = df_model[['Run_0', 'Run_1', 'Run_2', 'Run_3', 'Run_4']]\nelse :\n  print (\"Error ! runs INCORRECT ! \", runs)\n  ","4a5ab1d9":"#use numba to run mode 4x faster !!\nimport numba\nfrom numba import jit\nfrom scipy import stats\n\n# numba likes loop, np array & broadcasting\n\n@jit\ndef mode_numba(df):  \n    x = df.to_numpy()\n    a = np.zeros(shape=x.shape[0])\n    for i in range(x.shape[0]):\n      a[i] = np.asscalar(stats.mode(x[i, :])[0] ) # index 0 gives class, index 1 gives freq\n    \n    return a.astype(int)","59c33ab5":"if runs == 1:\n  df_model['vote'] = df_model['Run_0']\nelse :\n  df_model['vote'] = mode_numba(df_vote) ","95559e45":"df_model[df_model['batch']==1]","a394eb85":"path2 = Path('\/kaggle\/input\/liverpool-ion-switching')\ndf_subm = pd.read_csv(path2\/\"sample_submission.csv\")\ndf_subm['open_channels'] = df_model.vote.values\ndf_subm.to_csv(\"submissions.csv\", float_format='%.4f', index=False)","feb929df":"df_model.vote.value_counts()","af80b3b3":"df_subm","3184a985":"# Main function","f8ff5e50":"# InceptionTime model based on paper --> https:\/\/arxiv.org\/abs\/1909.04939\n\n# implement model using fast.ai\n","64163041":"# Build 3 models for 3ch, 5ch & 10ch\n\n# add OverSampling due to Class imbalance","5fbddab7":"# Create DataBunch : select batches to train, validate","cbf6484e":"# fast.ai implementation of InceptionTime"}}