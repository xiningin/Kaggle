{"cell_type":{"a7d27559":"code","d3188e41":"code","1eda9201":"code","ea156b63":"code","124e07ea":"code","5db4437c":"code","08bd3ce9":"markdown","d424a2a6":"markdown","340ce40e":"markdown","1845018d":"markdown","b2842374":"markdown"},"source":{"a7d27559":"import numpy as np\nimport pandas as pd\n\n# Read the data\ntrain_full = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col=\"Id\")\ntest_full = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\", index_col=\"Id\")\n\n# Remove rows with missing target, separate target from predictors\ntrain_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\nprint(\"Setup complete\")","d3188e41":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\ndef prepare_data(train, valid, test=None):\n\n    # All categorical columns\n    object_cols = [cname for cname in train.columns if train[cname].dtype == \"object\"]\n    # Columns that will be one-hot encoded\n    low_cardinality_cols = [col for col in object_cols if train[col].nunique() < 20]\n    # Columns that will be dropped from the dataset\n    high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\n    #print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n    #print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)\n    \n    obj_train = train[low_cardinality_cols].fillna('None')\n    obj_valid = valid[low_cardinality_cols].fillna('None')\n\n    oh_Encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    OH_cols_train = pd.DataFrame(oh_Encoder.fit_transform(obj_train))\n    OH_cols_valid = pd.DataFrame(oh_Encoder.transform(obj_valid))\n\n    OH_cols_train.index = train.index\n    OH_cols_valid.index = valid.index\n\n    # Fill in the lines below: imputation\n    numeric_imputer = SimpleImputer(strategy=\"mean\")\n\n    no_obj_train = train.drop(object_cols, axis=1).drop(['SalePrice'], axis=1)\n    no_obj_valid = valid.drop(object_cols, axis=1).drop(['SalePrice'], axis=1)\n\n    num_train = pd.DataFrame(numeric_imputer.fit_transform(no_obj_train))\n    num_valid = pd.DataFrame(numeric_imputer.transform(no_obj_valid))\n\n    num_train.index = train.index\n    num_valid.index = valid.index\n\n    # Fill in the lines below: imputation removed column names; put them back\n    num_train.columns = no_obj_train.columns\n    num_valid.columns = no_obj_valid.columns\n\n    X_train = pd.concat([num_train, OH_cols_train], axis=1)\n    y_train = train['SalePrice']\n\n    X_valid = pd.concat([num_valid, OH_cols_valid], axis=1)\n    y_valid = valid['SalePrice']\n        \n    if test is not None: \n        obj_test = test[low_cardinality_cols].fillna('None')\n        oh_test = pd.DataFrame(oh_Encoder.transform(obj_test))\n        oh_test.index = test.index\n        \n        no_obj_test = test.drop(object_cols, axis=1)\n        \n        num_test = pd.DataFrame(numeric_imputer.transform(no_obj_test))\n        num_test.index = test.index\n        num_test.columns = no_obj_test.columns\n        \n        X_test = pd.concat([num_test, oh_test], axis=1)\n        \n        return X_train, y_train, X_valid, y_valid, X_test\n    else:\n        return X_train, y_train, X_valid, y_valid\n\ndef get_data_splits(dataframe, valid_fraction=0.1):\n\n    dataframe = dataframe.sort_index()\n    valid_rows = int(len(dataframe) * valid_fraction)\n    train = dataframe[:-valid_rows]\n    valid = dataframe[-valid_rows:]\n    \n    return train, valid\n\ntrain, valid = get_data_splits(train_full)\nX_train, y_train, X_valid, y_valid, X_test = prepare_data(train, valid, test_full)","1eda9201":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n\nprint(\"Finding best parameters\")\n\nxgbr = XGBRegressor()\n\nreg_cv = GridSearchCV(xgbr, {\"min_child_weight\":[0.8, 1.0, 1.2], \"learning_rate\": [0.1],\n                             'max_depth': [2, 3, 8], 'n_estimators': [500]}, verbose=1)\n\nreg_cv.fit(X_train, y_train)\n\nprint(\"Best parameters: \",reg_cv.best_params_)","ea156b63":"from sklearn.model_selection import cross_val_score\n\nxgbr = XGBRegressor(**reg_cv.best_params_)\n\nxgbr.fit(X_train, y_train)\n\nscore = xgbr.score(X_train, y_train)  \nprint(\"Training score: \", score)\n\nscores = cross_val_score(xgbr, X_train, y_train, cv=10)\nprint(\"Mean cross-validation score: %.2f\" % scores.mean())","124e07ea":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nvalid_pred = xgbr.predict(X_valid)\nvalid_score = mean_absolute_error(y_valid, valid_pred)\nprint(\"Test MAE score:\", valid_score)\n\nvalid_score = mean_squared_error(y_valid, valid_pred, squared=False)\nprint(\"Test MSE score:\", valid_score)\n\nvalid_score = r2_score(y_valid, valid_pred)\nprint(\"Test R2 score:\", valid_score)","5db4437c":"print(\"Getting final results\")\n\n# make predictions which we will submit. \ntest_preds = xgbr.predict(X_test)\n\n# The lines below shows how to save predictions in format used for competition scoring\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)\n\nprint(\"Results saved\")","08bd3ce9":"House Prices is a classic dataset with 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. This competition challenges to predict the final price of each home.  \nFirst of all, let's read train and test datasets from CSV files using pandas lib.","d424a2a6":"Get two functions for dataset preparation.  \nUse One Hot encoder only for columns with less tnan 20 different values. This recommendation was taken from Kaggle education tutorial.  \nAlso use SimpleImputer with 'mean' strategy to fill in missing values.\nFor test data use One Hot encoder with same values per category as for train data.\n","340ce40e":"We've got R2 score 0.89. This is greather than 0.8 and means that our model performs well for this task.  \nSave results for Kaggle leaderboard.","1845018d":"After preparing data I tired to reduce number of features before training the model. For features selection LogisticRegression was used.  \nBut this didn't bring any improvements in final score. Thus I've excluded features selection code from this notebook.  \n\nTry using XGBRegressor to predict house prices. Also find best hyperparameters with GridSearchCV.","b2842374":"Refining training score value using cross-validation."}}