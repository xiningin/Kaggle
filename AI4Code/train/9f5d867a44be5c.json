{"cell_type":{"9c50f178":"code","49f13b62":"code","9ac06d5f":"code","61965d27":"code","ca50b3a8":"code","4c4df3af":"code","2d56c7bf":"code","c9bfc004":"code","3fc8f53c":"code","0488d96d":"code","a71ac0b0":"code","de9b4fa0":"code","b51db4cb":"code","c6e774fb":"code","1eeb93df":"code","d60d38de":"code","0a345fdd":"code","0733c1ac":"code","704b257d":"code","18a7b76c":"code","91a67306":"code","7fafd7d1":"code","4ef334be":"code","04db2152":"code","994bc7f4":"code","f72cb749":"code","9c1074a8":"code","7d63817d":"code","363d3238":"code","da5eda2c":"code","cda6741d":"code","ac431d72":"code","0ab35cf7":"code","07f1fe2f":"code","e827e087":"code","5879c19a":"code","3d07f19c":"code","306c3c1b":"markdown","7aa7eeea":"markdown","19fe63d2":"markdown","11045b14":"markdown","53b57a4d":"markdown","df6fe92b":"markdown","43b4b3c4":"markdown","23946331":"markdown","c18a8650":"markdown","89cbe436":"markdown","d2a54a7a":"markdown","82b6dec4":"markdown","edbf22c2":"markdown","bd5d33f9":"markdown","9541c344":"markdown","9b09cb60":"markdown","ca5c9457":"markdown","59655478":"markdown","b42337df":"markdown"},"source":{"9c50f178":"\n\n!cp ..\/input\/talibinstall\/ta-lib-0.4.0-src.tar.gzh  .\/ta-lib-0.4.0-src.tar.gz\n!tar -xzvf ta-lib-0.4.0-src.tar.gz > null\n!cd ta-lib && .\/configure --prefix=\/usr > null && make  > null && make install > null\n\n\n\n!cp ..\/input\/talibinstall\/TA-Lib-0.4.21.tar.gzh TA-Lib-0.4.21.tar.gz\n!pip install TA-Lib-0.4.21.tar.gz\n!pip install ..\/input\/talibinstall\/numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\nimport talib as ta\n\n","49f13b62":"# !pip install ta-lib","9ac06d5f":"# Import Libraries\nimport warnings # Supress warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\npd.set_option(\"display.max_columns\", None)\n\nfrom datetime import datetime\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 14})\n\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nfrom PIL import Image\nimport cv2\nimport talib as ta\n\n\nasset_details = pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv', low_memory=False)\ntrain = pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv', low_memory=False)\n#supplemental_train = pd.read_csv('supplemental_train.csv', low_memory=False)\n#example_test = pd.read_csv('example_test.csv', low_memory=False)\n#example_sample_submission = pd.read_csv('example_sample_submission.csv', low_memory=False)\n\nrename_dict = {}\nfor a in asset_details['Asset_ID']:\n    rename_dict[a] = asset_details[asset_details.Asset_ID == a].Asset_Name.values[0]\n\ndisplay(asset_details)","61965d27":"asset_details.Weight.sum()","ca50b3a8":"btc =train[train.Asset_ID==1].reset_index(drop=True) \nbtc","4c4df3af":"btc['date'] = btc.timestamp.astype('datetime64[s]')\nbtc['date'] = btc['date'].astype(str)\nbtc['time'] = btc['date'].apply(lambda x: x[11:])\nbtc","2d56c7bf":"btc =btc[btc.time=='00:01:00'].reset_index(drop=True) \nbtc","c9bfc004":"import seaborn as sns\nplt.figure(figsize=(10,8))\nsns.heatmap(btc[['Count','Open','High','Low','Close','Volume','VWAP','Target']].corr(), vmin=-1.0, vmax=1.0, annot=True, cmap='coolwarm', linewidths=0.1)\nplt.show()","3fc8f53c":"fig = plt.figure(figsize=(30,20))\nx=1\nfor i in train.Asset_ID.unique():\n    money = train[train.Asset_ID==i].reset_index(drop=True) \n    fig.add_subplot(4, 4, x)\n    a = str(x-1) +' ' + asset_details[asset_details.Asset_ID==i].Asset_Name.values \n    plt.title(a,fontsize=18)\n    plt.plot(money.index,money.Target)\n    x +=1\ndel money","0488d96d":"fig = plt.figure(figsize=(30,20))\ndata =train #[-10000:]\nfor i in data.Asset_ID.unique():\n    money = data[data.Asset_ID==i].reset_index(drop=True) \n    plt.plot(money.index,money.Target)\ndel data","a71ac0b0":"fig = plt.figure(figsize=(30,20))\ndata =train[-10000:]\nfor i in data.Asset_ID.unique():\n    money = data[data.Asset_ID==i].reset_index(drop=True) \n    plt.plot(money.index,money.Target)\ndel data","de9b4fa0":"fig = plt.figure(figsize=(30,20))\ndata =train[-1000:]\nfor i in data.Asset_ID.unique():\n    money = data[data.Asset_ID==i].reset_index(drop=True) \n    plt.plot(money.index,money.Target)","b51db4cb":"check = pd.DataFrame()\nfor i in data.Asset_ID.unique():\n    check[i] = data[data.Asset_ID==i]['Target'].reset_index(drop=True) ","c6e774fb":"import seaborn as sns\nplt.figure(figsize=(10,8))\nsns.heatmap(check.dropna().corr(), vmin=-1.0, vmax=1.0, annot=True, cmap='coolwarm', linewidths=0.1)\nplt.show()\ndel check","1eeb93df":"for i in  train.Asset_ID.unique():\n    check = train[train.Asset_ID==i].reset_index(drop=True) \n    print('Asset_ID=', pd.to_datetime(check.loc[0,'timestamp'],unit=\"s\",infer_datetime_format=True))","d60d38de":"check","0a345fdd":"fig = plt.figure(figsize=(30,20))\nx=1\nfor i in train.Asset_ID.unique():\n    money = train[train.Asset_ID==i].reset_index(drop=True) \n    fig.add_subplot(4, 4, x)\n    plt.title(asset_details[asset_details.Asset_ID==i].Asset_Name.values,fontsize=18)\n    plt.plot(money.index,money.Open,color=\"red\")\n    plt.plot(money.index,money.Close,color=\"blue\")\n    x +=1","0733c1ac":"fig = plt.figure(figsize=(30,20))\nx=1\nfor i in train.Asset_ID.unique():\n    money = train[train.Asset_ID==i].reset_index(drop=True) \n    fig.add_subplot(4, 4, x)\n    plt.title(asset_details[asset_details.Asset_ID==i].Asset_Name.values,fontsize=18)\n    plt.plot(money.index,money.High,color=\"red\")\n    plt.plot(money.index,money.Low,color=\"blue\")\n    x +=1\ndel money","704b257d":"fig = plt.figure(figsize=(30,20))\nx=1\nfor i in train.Asset_ID.unique():\n    money = train[train.Asset_ID==i].reset_index(drop=True) \n    fig.add_subplot(4, 4, x)\n    plt.title(asset_details[asset_details.Asset_ID==i].Asset_Name.values,fontsize=18)\n    plt.plot(money.index,money.Volume)\n    x +=1\ndel money","18a7b76c":"fig = plt.figure(figsize=(30,20))\nx=1\nfor i in train.Asset_ID.unique():\n    money = train[train.Asset_ID==i].reset_index(drop=True) \n    fig.add_subplot(4, 4, x)\n    plt.title(asset_details[asset_details.Asset_ID==i].Asset_Name.values,fontsize=18)\n    plt.plot(money.index,money.VWAP)\n    x +=1\n    \ndel money","91a67306":"fig = plt.figure(figsize=(30,20))\nx=1\nfor i in train.Asset_ID.unique():\n    money = train[train.Asset_ID==i].reset_index(drop=True) \n    fig.add_subplot(4, 4, x)\n    plt.title(asset_details[asset_details.Asset_ID==i].Asset_Name.values,fontsize=18)\n    plt.plot(money.index,money.Count)\n    x +=1\ndel money","7fafd7d1":"btc['high_low'] = btc['High'] - btc['Low']\nbtc['open_close'] = btc['Open'] - btc['Close']","4ef334be":"plt.figure(figsize=(10,8))\nsns.heatmap(btc[['Count','Open','High','Low','Close','Volume','high_low','open_close','VWAP','Target']].corr(), vmin=-1.0, vmax=1.0, annot=True, cmap='coolwarm', linewidths=0.1)\nplt.show()","04db2152":"fig = plt.figure(figsize=(30,20))\nx=1\nfor i in train.Asset_ID.unique():\n    money = train[train.Asset_ID==i].reset_index(drop=True) \n    fig.add_subplot(4, 4, x)\n    plt.title(asset_details[asset_details.Asset_ID==i].Asset_Name.values,fontsize=18)\n    money['MACD'],_,_ = ta.MACD(money['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n    plt.plot(money.index,money.MACD)\n    x +=1","994bc7f4":"fig = plt.figure(figsize=(30,20))\nx=1\nfor i in train.Asset_ID.unique():\n    money = train[train.Asset_ID==i].reset_index(drop=True) \n    fig.add_subplot(4, 4, x)\n    plt.title(asset_details[asset_details.Asset_ID==i].Asset_Name.values,fontsize=18)\n    money[\"RSI\"]=ta.RSI(money['Close'], timeperiod=14)\n    plt.plot(money.index,money.RSI)\n    x +=1","f72cb749":"train.info()","9c1074a8":"display(train)","7d63817d":"# Convert timestamp\ntrain['timestamp'] = train['timestamp'].astype('datetime64[s]')\n\n# Resample\ntrain_daily = pd.DataFrame()\n\nfor asset_id in asset_details.Asset_ID:\n    train_single = train[train.Asset_ID == asset_id].copy()\n\n    train_single_new = train_single[['timestamp','Count']].resample('D', on='timestamp').sum()\n    train_single_new['Open'] = train_single[['timestamp','Open']].resample('D', on='timestamp').first()['Open']\n    train_single_new['High'] = train_single[['timestamp','High']].resample('D', on='timestamp').max()['High']\n    train_single_new['Low'] = train_single[['timestamp','Low']].resample('D', on='timestamp').min()['Low']\n    train_single_new['Close'] = train_single[['timestamp','Close']].resample('D', on='timestamp').last()['Close']\n    train_single_new['Volume'] = train_single[['timestamp','Volume']].resample('D', on='timestamp').sum()['Volume']\n    train_single_new['Target'] = train_single[['timestamp','Target']].resample('D', on='timestamp').mean()['Target']\n    train_single_new['Asset_ID'] = asset_id\n    \n    train_daily = train_daily.append(train_single_new.reset_index(drop=False))\n    \ntrain_daily = train_daily.sort_values(by = ['timestamp', 'Asset_ID']).reset_index(drop=True)\n\ntrain_daily = train_daily.pivot(index='timestamp', columns='Asset_ID')[['Count', 'Open', 'High', 'Low', 'Close', 'Volume']]\ntrain_daily = train_daily.reset_index(drop=False)\n\ndisplay(train_daily.head(10))","363d3238":"train_daily.info()","da5eda2c":"len(asset_details.Asset_ID)","cda6741d":"asset_details.Asset_Name","ac431d72":"# Visualize\nfig = make_subplots(\n    rows=len(asset_details.Asset_ID), cols=1,\n    subplot_titles=(asset_details.Asset_Name)\n)\n\nfor i, asset_id in enumerate(asset_details.Asset_ID):\n    fig.append_trace(go.Candlestick(x=train_daily.timestamp, \n                                         open=train_daily[('Open', asset_id)], \n                                         high=train_daily[('High', asset_id)], \n                                         low=train_daily[('Low', asset_id)], \n                                         close=train_daily[('Close', asset_id)]),\n                  row=i+1, col=1,\n                    )\n\n    fig.update_xaxes(range=[train_daily.timestamp.iloc[0], train_daily.timestamp.iloc[-1]], row=i+1, col=1)\n    \nfig.update_layout(xaxis_rangeslider_visible = False, \n                  xaxis2_rangeslider_visible = False, \n                  xaxis3_rangeslider_visible = False,\n                  xaxis4_rangeslider_visible = False,\n                  xaxis5_rangeslider_visible = False,\n                  xaxis6_rangeslider_visible = False,\n                  xaxis7_rangeslider_visible = False,\n                  xaxis8_rangeslider_visible = False,\n                  xaxis9_rangeslider_visible = False,\n                  xaxis10_rangeslider_visible = False,\n                  xaxis11_rangeslider_visible = False,\n                  xaxis12_rangeslider_visible = False,\n                  xaxis13_rangeslider_visible = False,\n                  xaxis14_rangeslider_visible = False,\n                  height=3000, width=800, \n                  #title_text=\"Subplots with Annotations\"\n                      margin = dict(\n        l = 0,\n        r = 0,\n        b = 0,\n        t = 30,\n        pad = 0)\n                 )\n                    \nfig.show()","0ab35cf7":"train_daily['year'] = pd.DatetimeIndex(train_daily['timestamp']).year\ntrain_daily['quarter'] = pd.DatetimeIndex(train_daily['timestamp']).quarter\ntrain_daily['month'] = pd.DatetimeIndex(train_daily['timestamp']).month\ntrain_daily['weekofyear'] = pd.DatetimeIndex(train_daily['timestamp']).weekofyear\ntrain_daily['dayofyear'] = pd.DatetimeIndex(train_daily['timestamp']).dayofyear\ntrain_daily['weekday'] = pd.DatetimeIndex(train_daily['timestamp']).weekday","07f1fe2f":"from statsmodels.tsa.stattools import adfuller\n\ndef check_stationarity(series, asset_id):\n    # Copied and edited from https:\/\/machinelearningmastery.com\/time-series-data-stationary-python\/\n\n    result = adfuller(series.values)\n    if (result[1] <= 0.05) & (result[4]['5%'] > result[0]):\n        print(f\"{asset_details[asset_details.Asset_ID == asset_id].Asset_Name.values[0]}: \\u001b[32mStationary\\u001b[0m\")\n    else:\n        print(f\"{asset_details[asset_details.Asset_ID == asset_id].Asset_Name.values[0]}: \\x1b[31mNon-stationary\\x1b[0m\")\n       \n    print('ADF Statistic: %f' % result[0])\n    print('p-value: %f' % result[1])\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print('\\t%s: %.3f' % (key, value))\n    print('\\n')\nfor i, asset_id in enumerate(asset_details.Asset_ID):    \n    check_stationarity(train_daily[('Close', i)].fillna(0), asset_id)","e827e087":"# define function to compute log returns\ndef log_return(series, periods=1):\n    # Copied from https:\/\/www.kaggle.com\/cstein06\/tutorial-to-the-g-research-crypto-competition\n    return np.log(series).diff(periods=periods)\n\n\nfor i, asset_id in enumerate(asset_details.Asset_ID):\n    train_daily[('lret',  asset_id)] = log_return(train_daily[( 'Close',  asset_id)])","5879c19a":"from statsmodels.tsa.seasonal import seasonal_decompose\n\nperiods = [7, 28, 365]\n   \nasset_id = 1 # Bitcoin\n# Visualize\nf, ax = plt.subplots(nrows=len(periods), ncols=1, figsize=(12, 12))\nfor i, p in enumerate(periods):\n    decomp = seasonal_decompose(train_daily[('Close',  asset_id)].fillna(0), period=p, model='additive', extrapolate_trend='freq')\n    train_daily[(f'Trend_{p}',  asset_id)] = np.where(train_daily[('Close',  asset_id)].isna(), np.NaN, decomp.trend) #decomp.trend\n    \n    \n    sns.lineplot(data=train_daily, x='timestamp', y = ('Close',  asset_id) , ax=ax[i], color='lightgrey');\n    sns.lineplot(data=train_daily, x='timestamp', y = (f'Trend_{p}',  asset_id) , ax=ax[i], color='red');\n    ax[i].set_title(f\"{asset_details[asset_details.Asset_ID == asset_id].Asset_Name.values[0]} Trend with a Period of {p} Day\")\n    ax[i].set_xlim([train_daily.timestamp.iloc[0], train_daily.timestamp.iloc[-1]])\n    #ax[i].set_ylim([-0.6,0.6])\n    ax[i].set_ylabel('Close Price [$]')\n    \n#plt.suptitle(f'Underlying Trend with {PERIOD} day period\\n')\nplt.tight_layout()\nplt.show()","3d07f19c":"trend = train_daily[('Close',  1)].rolling(\n    window=365,       # 365-day window\n    center=True,      # puts the average at the center of the window\n    min_periods=183,  # choose about half the window size\n).mean()   \n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,5))\n\nsns.lineplot(data=train_daily, x='timestamp', y = ('Close',  1) , ax=ax, color='lightgrey');\nsns.lineplot(x=train_daily['timestamp'], y = trend, ax=ax, color='red');\n#ax[i].set_title(f\"{asset_details[asset_details.Asset_ID == asset_id].Asset_Name.values[0]} Trend with a Period of {p} Day\")\nax.set_xlim([train_daily.timestamp.iloc[0], train_daily.timestamp.iloc[-1]])\n#ax[i].set_ylabel('Close Price [$]')\n\n#plt.suptitle(f'Underlying Trend with {PERIOD} day period\\n')\nplt.show()","306c3c1b":"### Preprocessing\nFor the following EDA, resampling of the minute-wise crypto data to daily samples is applied which reduces the amount of samples from 24,236,806 to 1,360.","7aa7eeea":"### Determining Trend with Time-Series Decomposition","19fe63d2":"### Stationarity\nThe 'Close' prices seem to be mostly non-stationary. However, Bitcoin and Ethereum seem to be stationary.","11045b14":"The weight of each asset used to weigh their relative importance in the evaluation metric.","53b57a4d":"## Data Overview\nFurthermore, we have samples from 2018-01-01 to 2021-09-21 for the majority of coins. For TRON, Stellar, Cardano, IOTA, Maker, and Dogecoin we have fewer data starting from later in 2018 or even later in 2019 in Dogecoin's case.","df6fe92b":"Waveforms of 0 and 5 are very similar\n<br>\u30fb Waveforms of 1 and 7 are similar\n<br>\u30fb Waveforms of 3,8,12,13 are a little similar\n<br>\u30fb Waveforms of 2, 9 and 10 are a little similar\n<br>\u30fb There are not many similarities between 6 and 11","43b4b3c4":"### Feature Engineering 1: Time Features","23946331":"### Open\/Close","c18a8650":"### Volume","89cbe436":"### MACD","d2a54a7a":"### HeatMap Coins\n\nSome coins have a strong correlation.","82b6dec4":"### HEATMAP","edbf22c2":"### VWAP","bd5d33f9":"### Count","9541c344":"### RSI","9b09cb60":"### Feature Engineering 2: Log Return\nTo make a time series stationary, you can try differencing it. In this case, we will use the log return instead as shown below.","ca5c9457":"### Check BTC","59655478":"### High\/Low","b42337df":"### Evaluation Metric\nWhile mean squared error, R^2, explained variance, and correlation are all very closely related, correlation has the useful property that it tends to normalize leading-order volatility out of the covariance between target and prediction. In financial markets (especially crypto ones!), predicting volatility is a difficult (but interesting!) question in its own right. By using correlation as a metric we hope to remove some noise from the prediction problem and provide a more stable metric to evaluate against.\n\n### Target\nThe target calculation is based on the close price of the asset and can be derived from the provided data using the methodology in https:\/\/www.kaggle.com\/cstein06\/tutorial-to-the-g-research-crypto-competition. \n\n### Weights\nIn this competition, the weights are determined by the logarithm of each product's market cap (in USD), of the cryptocurrencies at a fixed point in time. Weights were assigned to give more relevance to cryptocurrencies with higher market volumes to ensure smaller cryptocurrencies do not disproportionately impact your models."}}