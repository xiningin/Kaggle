{"cell_type":{"1a65d7c1":"code","45a60759":"code","4a58b491":"code","3ce08203":"code","8e3645b5":"code","29901a0e":"code","aa899854":"code","9f154bba":"code","e7438b13":"code","1207053a":"code","9b1c5ad0":"code","419e6680":"code","9732f98e":"code","815a4055":"code","da4bca60":"code","aecc9d8a":"code","065401d4":"code","b2eeea27":"code","d416aa19":"code","42bd8b98":"code","1ca0665c":"code","125bf9f2":"code","3fbf412b":"code","bb94bf07":"code","d096aa15":"code","5c89e4a6":"code","569b7689":"code","1f3eed60":"code","41baa6f4":"code","5b108e06":"code","4722a0c6":"code","b0a0e426":"code","058780cd":"code","5d2339ac":"code","e0c314e2":"code","4f071e7b":"code","05c8e515":"code","d870dcd2":"code","7a1710e0":"code","06fca8a3":"code","0a0cba50":"code","6463f755":"code","0d96fd1b":"code","17a7325e":"code","7176b29f":"code","eacac43f":"code","d6b340a1":"code","f084d593":"code","bbdd6859":"code","8a43a18e":"code","73b0073c":"code","8cc9eba6":"code","d87e636c":"code","23b920cd":"code","ef72c7b5":"code","70c40ff2":"code","1ef4f07d":"code","066e0664":"code","dda20543":"code","cb4e5513":"code","0e975fa1":"code","3d80e17f":"code","6c15c71a":"code","a5903329":"code","6e5f4ca8":"code","e03254b6":"code","9ec56394":"code","02588ffd":"code","694e5b8c":"code","9ce45a70":"code","808afb35":"code","78720a7f":"markdown"},"source":{"1a65d7c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","45a60759":"!pip install iterative-stratification","4a58b491":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nfrom copy import deepcopy as dp\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')","3ce08203":"def norm_fit(df_1,saveM = True, sc_name = 'zsco'):   \n    from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,RobustScaler,Normalizer,QuantileTransformer,PowerTransformer\n    ss_1_dic = {'zsco':StandardScaler(),\n                'mima':MinMaxScaler(),\n                'maxb':MaxAbsScaler(), \n                'robu':RobustScaler(),\n                'norm':Normalizer(), \n                'quan':QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),\n                'powe':PowerTransformer()}\n    ss_1 = ss_1_dic[sc_name]\n    df_2 = pd.DataFrame(ss_1.fit_transform(df_1),index = df_1.index,columns = df_1.columns)\n    if saveM == False:\n        return(df_2)\n    else:\n        return(df_2,ss_1)","8e3645b5":"def norm_tra(df_1,ss_x):\n    df_2 = pd.DataFrame(ss_x.transform(df_1),index = df_1.index,columns = df_1.columns)\n    return(df_2)","29901a0e":"def g_table(list1):\n    table_dic = {}\n    for i in list1:\n        if i not in table_dic.keys():\n            table_dic[i] = 1\n        else:\n            table_dic[i] += 1\n    return(table_dic)","aa899854":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","9f154bba":"seed_everything(seed=42)\n\nSEED = [0, 1, 2, 3 ,4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\ninput_dir = '..\/input\/lish-moa\/'","e7438b13":"sc_dic = {}\nfeat_dic = {}\ntrain_features = pd.read_csv(input_dir+'train_features.csv')\ntrain_targets_scored = pd.read_csv(input_dir+'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(input_dir+'train_targets_nonscored.csv')\ntest_features = pd.read_csv(input_dir+'test_features.csv')\nsample_submission = pd.read_csv(input_dir+'sample_submission.csv')\ntrain_drug = pd.read_csv(input_dir+'train_drug.csv')\n\ntarget_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\ntarget_nonsc_cols = train_targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()","1207053a":"nonctr_id = train_features.loc[train_features['cp_type']!='ctl_vehicle','sig_id'].tolist()\ntmp_con1 = [i in nonctr_id for i in train_targets_scored['sig_id']]","9b1c5ad0":"mat_cor = pd.DataFrame(\n    np.corrcoef(\n        train_targets_scored.drop('sig_id',axis = 1)[tmp_con1].T,\n        train_targets_nonscored.drop('sig_id',axis = 1)[tmp_con1].T\n    )\n)","419e6680":"mat_cor","9732f98e":"mat_cor2 = mat_cor.iloc[(train_targets_scored.shape[1]-1):,0:train_targets_scored.shape[1]-1]","815a4055":"mat_cor2","da4bca60":"mat_cor2.index = target_nonsc_cols\nmat_cor2.columns = target_cols\nmat_cor2 = mat_cor2.dropna()","aecc9d8a":"mat_cor2","065401d4":"mat_cor2_max = mat_cor2.abs().max(axis = 1)","b2eeea27":"q_n_cut = 0.9\ntarget_nonsc_cols2 = mat_cor2_max[mat_cor2_max > np.quantile(mat_cor2_max,q_n_cut)].index.tolist()","d416aa19":"print(len(target_nonsc_cols2))","42bd8b98":"feat_dic = {}\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\nfeat_dic['gene'] = GENES\nfeat_dic['cell'] = CELLS","1ca0665c":"def normalization(df,feature, iscell = False):\n    q2 = df[feat_dic[feature]].apply(np.quantile,axis = 1,q = 0.25).copy()\n    q7 = df[feat_dic[feature]].apply(np.quantile,axis = 1,q = 0.75).copy()\n    qmean = (q2+q7)\/2\n    df[feat_dic[feature]] = (df[feat_dic[feature]].T - qmean.values).T\n    if iscell == True:\n        qmean2 = df[feat_dic[feature].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n        df[feat_dic[feature]] = (df[feat_dic[feature]].T \/ qmean2.values).T.copy()\n    return df","125bf9f2":"feature_cols = []\nfor key_i in feat_dic.keys():\n    value_i = feat_dic[key_i]\n    print(key_i,len(value_i))\n    feature_cols += value_i\nlen(feature_cols)","3fbf412b":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored[['sig_id']+target_nonsc_cols2], on='sig_id')","bb94bf07":"train","d096aa15":"train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntrain = train.drop('cp_type', axis=1)","5c89e4a6":"train","569b7689":"test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test.drop('cp_type', axis=1)","1f3eed60":"target = train[['sig_id']+target_cols]\ntarget_ns = train[['sig_id']+target_nonsc_cols2]","41baa6f4":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\ntarget_cols","5b108e06":"b = train_targets_scored.columns[1:].tolist()","4722a0c6":"target_cols == b","b0a0e426":"n_comp1 = 50\nn_comp2 = 15\nnum_features=len(feature_cols) + n_comp1 + n_comp2\nnum_targets=len(target_cols)\nnum_targets_0=len(target_nonsc_cols2)","058780cd":"tar_freq = np.array([np.min(list(g_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])","5d2339ac":"tar_weight0 = np.array([np.log(i+100) for i in tar_freq])","e0c314e2":"np.min(tar_weight0)","4f071e7b":"tar_weight0_min = dp(np.min(tar_weight0))","05c8e515":"tar_weight = tar_weight0_min\/tar_weight0","d870dcd2":"pos_weight = torch.tensor(tar_weight).to(DEVICE)","7a1710e0":"from torch.nn.modules.loss import _WeightedLoss\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n                                                  pos_weight = pos_weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","06fca8a3":"class TrainDataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return (self.features.shape[0])\n\n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n\nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n\n    def __len__(self):\n        return (self.features.shape[0])\n\n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct","0a0cba50":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n\n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets) #with sigmoid\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        final_loss += loss.item()\n\n    final_loss \/= len(dataloader)\n\n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n\n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets) #contains sigmoid\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy()) #need sigmoid\n\n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n\n    return final_loss, valid_preds","6463f755":"def inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n\n    for data in dataloader:\n        inputs = data['x'].to(device)\n        with torch.no_grad():\n            outputs = model(inputs)\n\n        preds.append(outputs.sigmoid().detach().cpu().numpy())#need sigmoid\n\n    preds = np.concatenate(preds)\n\n    return preds","0d96fd1b":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        cha_1 = 256\n        cha_2 = 512\n        cha_3 = 512\n\n        cha_1_reshape = int(hidden_size\/cha_1)\n        cha_po_1 = int(hidden_size\/cha_1\/2)\n        cha_po_2 = int(hidden_size\/cha_1\/2\/2) * cha_3\n\n        self.cha_1 = cha_1\n        self.cha_2 = cha_2\n        self.cha_3 = cha_3\n        self.cha_1_reshape = cha_1_reshape\n        self.cha_po_1 = cha_po_1\n        self.cha_po_2 = cha_po_2\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n\n        self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n        self.dropout_c1 = nn.Dropout(0.1)\n        self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n\n        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n\n        self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n        self.dropout_c2 = nn.Dropout(0.1)\n        self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n\n        self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n        self.dropout_c2_1 = nn.Dropout(0.3)\n        self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n\n        self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n        self.dropout_c2_2 = nn.Dropout(0.2)\n        self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n\n        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n\n        self.flt = nn.Flatten()\n\n        self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n        self.dropout3 = nn.Dropout(0.2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n\n    def forward(self, x):\n\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.celu(self.dense1(x), alpha=0.06)\n\n        x = x.reshape(x.shape[0],self.cha_1,\n                      self.cha_1_reshape)\n\n        x = self.batch_norm_c1(x)\n        x = self.dropout_c1(x)\n        x = F.relu(self.conv1(x))\n\n        x = self.ave_po_c1(x)\n\n        x = self.batch_norm_c2(x)\n        x = self.dropout_c2(x)\n        x = F.relu(self.conv2(x))\n        x_s = x\n\n        x = self.batch_norm_c2_1(x)\n        x = self.dropout_c2_1(x)\n        x = F.relu(self.conv2_1(x))\n\n        x = self.batch_norm_c2_2(x)\n        x = self.dropout_c2_2(x)\n        x = F.relu(self.conv2_2(x))\n        x =  x * x_s\n\n        x = self.max_po_c2(x)\n\n        x = self.flt(x)\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n\n        return x","17a7325e":"len(train)*0.8","7176b29f":"train_df = train.iloc[:int(len(train)*0.8),:]\nvalid_df = train.iloc[int(len(train)*0.8):,:].reset_index(drop=True)","eacac43f":"valid_df","d6b340a1":"x_train, y_train,y_train_ns = train_df[feature_cols], train_df[target_cols].values,train_df[target_nonsc_cols2].values\nx_valid, y_valid,y_valid_ns  =  valid_df[feature_cols], valid_df[target_cols].values,valid_df[target_nonsc_cols2].values\nx_test = test[feature_cols]","f084d593":"col_num = list(set(feat_dic['gene'] + feat_dic['cell']) & set(feature_cols))","bbdd6859":"x_train[col_num],ss = norm_fit(x_train[col_num],True,'quan')\nx_valid[col_num]    = norm_tra(x_valid[col_num],ss)\nx_test[col_num]     = norm_tra(x_test[col_num],ss)","8a43a18e":"x_train","73b0073c":"def pca_pre(tr,va,te,\n            n_comp,feat_raw,feat_new):\n    pca = PCA(n_components=n_comp, random_state=42)\n    tr2 = pd.DataFrame(pca.fit_transform(tr[feat_raw]),columns=feat_new)\n    va2 = pd.DataFrame(pca.transform(va[feat_raw]),columns=feat_new)\n    te2 = pd.DataFrame(pca.transform(te[feat_raw]),columns=feat_new)\n    return(tr2,va2,te2)","8cc9eba6":"pca_feat_g = [f'pca_G-{i}' for i in range(n_comp1)]\nfeat_dic['pca_g'] = pca_feat_g\nx_tr_g_pca,x_va_g_pca,x_te_g_pca = pca_pre(x_train,x_valid,x_test,\n                                           n_comp1,feat_dic['gene'],pca_feat_g)\n","d87e636c":"x_train = pd.concat([x_train,x_tr_g_pca],axis = 1)\nx_valid = pd.concat([x_valid,x_va_g_pca],axis = 1)\nx_test  = pd.concat([x_test,x_te_g_pca],axis = 1)","23b920cd":"pca_feat_g = [f'pca_C-{i}' for i in range(n_comp2)]\nfeat_dic['pca_c'] = pca_feat_g\nx_tr_c_pca,x_va_c_pca,x_te_c_pca = pca_pre(x_train,x_valid,x_test,\n                                           n_comp2,feat_dic['cell'],pca_feat_g)\nx_train = pd.concat([x_train,x_tr_c_pca],axis = 1)\nx_valid = pd.concat([x_valid,x_va_c_pca],axis = 1)\nx_test  = pd.concat([x_test,x_te_c_pca], axis = 1)","ef72c7b5":"x_train # features + 50 PCA-gene + 15 PCA-cell","70c40ff2":"x_train,x_valid,x_test = x_train.values,x_valid.values,x_test.values","1ef4f07d":"x_train.shape","066e0664":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False","dda20543":"train_dataset = TrainDataset(x_train, y_train_ns)\nvalid_dataset = TrainDataset(x_valid, y_valid_ns)\ntrainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalidloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)","cb4e5513":"hidden_size=4096\nmodel = Model(\n    num_features=num_features,\n    num_targets=num_targets_0,\n    hidden_size=hidden_size,\n)\n\nmodel.to(DEVICE)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, \n                                          max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n\nloss_tr = nn.BCEWithLogitsLoss()   #SmoothBCEwLogits(smoothing = 0.001)\nloss_va = nn.BCEWithLogitsLoss()    \n\nearly_stopping_steps = EARLY_STOPPING_STEPS\nearly_step = 0","0e975fa1":"for epoch in range(1):\n    train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n    valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n    print(f\"EPOCH: {epoch},train_loss: {train_loss}, valid_loss: {valid_loss}\")","3d80e17f":"model.dense3 = nn.utils.weight_norm(nn.Linear(model.cha_po_2, num_targets))\nmodel.to(DEVICE)\n\ntrain_dataset = TrainDataset(x_train, y_train)\nvalid_dataset = TrainDataset(x_valid, y_valid)\ntrainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalidloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)","6c15c71a":"trn_idx = train_df.index\nval_idx = valid_df.index","a5903329":"optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                          max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n\nloss_tr = SmoothBCEwLogits(smoothing = 0.001) #the loss has pos_weight\nloss_va = nn.BCEWithLogitsLoss()    #-log(sigmoid())\n\nearly_stopping_steps = EARLY_STOPPING_STEPS\nearly_step = 0\n\noof = np.zeros((len(train), len(target_cols)))\nbest_loss = np.inf\n\nmod_name = f\"FOLD_mod11_.pth\"","6e5f4ca8":"for epoch in range(EPOCHS):\n\n    train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n    valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n    print(f\"EPOCH: {epoch},train_loss: {train_loss}, valid_loss: {valid_loss}\")\n\n    if valid_loss < best_loss:\n\n        best_loss = valid_loss\n        oof[val_idx] = valid_preds\n        torch.save(model.state_dict(), mod_name)\n\n    elif(EARLY_STOP == True):\n\n        early_step += 1\n        if (early_step >= early_stopping_steps):\n            break","e03254b6":"testdataset = TestDataset(x_test)\ntestloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\nmodel = Model(\n    num_features=num_features,\n    num_targets=num_targets,\n    hidden_size=hidden_size,\n)\n\nmodel.load_state_dict(torch.load(mod_name))\nmodel.to(DEVICE)\n\n#predictions = np.zeros((len(test_), len(target_cols)))\npredictions = inference_fn(model, testloader, DEVICE)","9ec56394":"test[target_cols] = predictions","02588ffd":"test","694e5b8c":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)","9ce45a70":"sub.to_csv('submission.csv', index=False)","808afb35":"sub","78720a7f":"This is a simple implementation of the 2nd place code by https:\/\/github.com\/baosenguo\/Kaggle-MoA-2nd-Place-Solution\/blob\/main\/training\/1d-cnn-train.ipynb. This code deletes seed and kfold. Just go through the data preprocessing and the model."}}