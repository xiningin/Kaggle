{"cell_type":{"313513b9":"code","af0f3076":"code","9a17308c":"code","3fa0a8a1":"code","4d3d2e11":"code","3fba73af":"code","cd75ac45":"code","c9956da0":"code","9eb78291":"code","14ea0db2":"code","22600a6c":"code","29c0278c":"code","58f438e6":"code","9be8d93f":"code","36c01624":"code","886d036a":"markdown","54574808":"markdown","f859f5a4":"markdown"},"source":{"313513b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import axes3d\nimport seaborn as sns\nfrom sklearn import svm\nfrom sklearn.metrics import r2_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.neighbors import KNeighborsClassifier\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","af0f3076":"titan = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitan.head()","9a17308c":"titan = titan.drop(['Name'], axis=1)\ntitan = titan.drop(['PassengerId'], axis=1)\ntitan.dtypes","3fa0a8a1":"titan.isnull().sum()","4d3d2e11":"titan.describe().transpose()","3fba73af":"#print(titan[titan.Age != 'Nan'].Age.mean())\n#print(titan[pd.notna(titan.Age)].Age.mean())\nprint(titan.Age.mean())\n\ntitan[pd.isna(titan.Age)].Age = titan.Age.mean()\n\n\ntitan.isnull().sum()\n#pd.notna(titan.Age).Age.mean() = data[data.Experience >= 0].Experience.mean()\n#data.Experience.mean()","cd75ac45":"rep_0 = SimpleImputer() ##using default values. missing_values = \"NaN\", method=mean\ntitan.Age = pd.DataFrame(rep_0.fit_transform(titan[['Age']]).ravel()) ## Simple imputing Age. ravel() is used when you have a 1D dataframe.\n\n\n#titan.columns = cols\n#titan[titan.Survived == 1].count()\n#titan.isnull().sum()\n","c9956da0":"sns.pairplot(titan, hue='Survived', diag_kind='hist')","9eb78291":"sns.heatmap(titan.corr(),annot=True,cmap='YlGnBu')","14ea0db2":"sns.lmplot(x='Fare', y='Pclass', data=titan, hue='Survived')","22600a6c":"sns.jointplot(titan.Pclass, titan.Fare, kind='kde')","29c0278c":"X=titan[['Pclass','Fare']]\nX_train,X_test, y_train, y_test = train_test_split(titan[['Pclass','Fare']], titan.Survived, test_size=0.3, stratify=titan.Survived, random_state=7)\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","58f438e6":"LGR = LogisticRegression(solver='liblinear')\nLGR.fit(X_train,y_train)\nLGR_predict = LGR.predict(X_test)\nprint(\"Logistic Regression Score is\", LGR.score(X_test,y_test))\nsns.heatmap(metrics.confusion_matrix(y_test, LGR_predict), annot=True)","9be8d93f":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn_y = knn.predict(X_test)\nprint(\"KNN Score predictor is \", knn.score(X_test,y_test))\nsns.heatmap(metrics.confusion_matrix(y_test, knn_y), annot=True)\n###Feature scaling does improve performance of KNN. But not the Logistic regression. Wonder why? And the model performs better than Logistic! \n### Is it because of the type classification? Maybe only because it is 2 class. Or is it quality of data.","36c01624":"svc = svm.SVC(kernel='rbf',C=10,gamma='auto')\nsvc.fit(X_train, y_train)\nsvc_y = svc.predict(X_test)\nprint(\"SVM Score predictor is \", svc.score(X_test,y_test))\nsns.heatmap(metrics.confusion_matrix(y_test, svc_y), annot=True)\n\n","886d036a":"* Age and Cabin number seems ot have a lot of null values.\n* Embarked has 2 null values. Need to probably replace with the most common point of embarkation\n* Age needs to be replaced by mean.\n* Shoud we drop Cabin? or replace with most common? Which class does most Nan cabin occurs?","54574808":"Survival seems most closely correlated to Class and Fare.\nLower class have lower survival rates.","f859f5a4":"PClass -  class 1, 2 or 3. 1 being most expensive\nSibSP - number of siblings and spouces on board.\nParch - # parents\/children onboard."}}