{"cell_type":{"4d356161":"code","1b23c5b1":"code","24b94ee0":"code","042988f1":"code","75f206de":"code","ba87a7ac":"code","a905b001":"code","168b5b25":"code","e56f10e6":"code","a6a6fa4b":"code","d64a0a3b":"code","18deafad":"code","5804a642":"code","7ce4d78a":"code","45fa10f4":"code","17f128db":"code","50e49ab2":"code","f997c7b1":"code","6294f37a":"code","b0f765a4":"code","72558fff":"code","1ac16200":"markdown","b215a1c4":"markdown","e2854228":"markdown","34812d3a":"markdown","a2c1d5af":"markdown","7cbb912e":"markdown","e4958233":"markdown","efd302ad":"markdown","6485c3b9":"markdown","cb2a694a":"markdown","91834d88":"markdown","23ba07d7":"markdown","5b139408":"markdown","334d36f2":"markdown","43324bc8":"markdown","ab97f76a":"markdown"},"source":{"4d356161":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport time\nimport random\nimport math\nfrom collections import defaultdict\n\nimport glob\nfrom albumentations import CenterCrop, RandomRotate90, GridDistortion, HorizontalFlip, VerticalFlip, RandomCrop\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nnp.set_printoptions(precision=3)\npd.set_option('precision', 3)\nplt.rcParams['figure.figsize'] = (15, 10)\nplt.style.use('ggplot')\nsns.set(font_scale=1.2)","1b23c5b1":"images_folder_path = '\/kaggle\/input\/semantic-drone-dataset\/dataset\/semantic_drone_dataset\/original_images\/'\nmasks_folder_path = '\/kaggle\/input\/semantic-drone-dataset\/dataset\/semantic_drone_dataset\/label_images_semantic\/'\nimages_folder = sorted(glob.glob(images_folder_path+'*.jpg'))\nmasks_folder = sorted(glob.glob(masks_folder_path+'*.png'))","24b94ee0":"mask_dict = defaultdict(int)\n\nmask_dict = defaultdict(int)\nfor mask in tqdm(masks_folder):\n    mask = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n    unique, counts = np.unique(mask, return_counts=True)\n    counts = counts\/(24e6)\n    for key, count in zip(unique, counts):\n        mask_dict[key] += count","042988f1":"mask_dict_per_values = np.array(list(mask_dict.values()))\/4\nmask_dict_per = {k:p for (k,p) in zip(mask_dict.keys(), mask_dict_per_values)}\n\nmask_dict_per_sorted = dict(sorted(mask_dict_per.items(), key=lambda item: item[1], reverse=True))\n\nclass_df = pd.read_csv('\/kaggle\/input\/semantic-drone-dataset\/class_dict_seg.csv',index_col=False,skipinitialspace=True)\nclass_df","75f206de":"class_mask_per = {class_df.loc[k, 'name']:v for (k,v) in mask_dict_per_sorted.items()}\nclass_mask_per_df = pd.DataFrame({'label':class_mask_per.keys(), 'per':class_mask_per.values()})\n\ndef return_label_cls(label):\n    s = (class_df['name'] == label)\n    return s[s].index[0]\n\nclass_mask_per_df['class'] = class_mask_per_df.apply(lambda x: return_label_cls(x.label), axis=1)\nclass_mask_per_df","ba87a7ac":"## Data Augmentation\n## It was chosen the resolution of 1536x1024px keep the ratio of the original images (6000x4000px). \n\ndef create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n        \ndef augment_data(images, masks, save_path):\n    H = 1024\n    W = 1536\n    for x,y in tqdm(zip(images, masks), total=len(images)):\n        name = x.split(\"\/\")[-1].split(\".\")\n        image_name = name[0]\n        image_extn = name[1]\n        \n        name = y.split(\"\/\")[-1].split(\".\")\n        mask_name = name[0]\n        mask_extn = name[1]       \n        \n        \n        x = cv2.imread(x, cv2.IMREAD_COLOR)\n        x = cv2.resize(x, (W, H))\n        y = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n        y = cv2.resize(y, (W, H))\n        \n        aug = RandomCrop(int(3*H\/4), int(3*W\/4), always_apply=False, p=1.0)\n        augmented = aug(image=x, mask=y)\n        x1 = augmented[\"image\"]\n        y1 = augmented[\"mask\"]\n\n        aug = HorizontalFlip(p=1.0)\n        augmented = aug(image=x, mask=y)\n        x2 = augmented[\"image\"]\n        y2 = augmented[\"mask\"]\n\n        aug = VerticalFlip(p=1.0)\n        augmented = aug(image=x, mask=y)\n        x3 = augmented[\"image\"]\n        y3 = augmented[\"mask\"] \n\n        aug = RandomRotate90(p=1.0)\n        augmented = aug(image=x, mask=y)\n        x4 = augmented['image']\n        y4 = augmented['mask']\n\n        save_images = [x, x1, x2, x3, x4]\n        save_masks = [y, y1, y2, y3, y4]       \n        \n        idx = 0\n        for i, m in zip(save_images, save_masks):\n            i = cv2.resize(i, (W, H))\n            m = cv2.resize(m, (W, H))\n            \n            tmp_img_name = f\"{image_name}_{idx}.{image_extn}\"\n            tmp_msk_name = f\"{mask_name}_{idx}.{mask_extn}\" \n            image_path = os.path.join(save_path, \"images\", tmp_img_name)\n            mask_path = os.path.join(save_path, \"masks\", tmp_msk_name)\n            \n            cv2.imwrite(image_path, i)\n            cv2.imwrite(mask_path, m)\n            idx+=1\n\n\nprint(f\"Original images:  {len(images_folder)} - Original masks: {len(masks_folder)}\")\n\ncreate_dir(\".\/new_data\/images\/\")\ncreate_dir(\".\/new_data\/masks\/\")\nsave_path = \".\/new_data\/\"\naugment_data(images_folder, masks_folder, save_path)\n\nimages = sorted(glob.glob(os.path.join(save_path, \"images\/*\")))\nmasks = sorted(glob.glob(os.path.join(save_path, \"masks\/*\")))\nprint(f\"Augmented images:  {len(images)} - Augmented masks: {len(masks)}\")","a905b001":"df = pd.DataFrame({'images':images, 'masks':masks})\nimg_train, img_test, mask_train, mask_test = train_test_split(df['images'], df['masks'], test_size=0.15, random_state=7)\nimg_val, img_test, mask_val, mask_test = train_test_split(img_test, mask_test, test_size=0.3, random_state=7)\nprint(len(img_train), len(img_val), len(img_test))","168b5b25":"def visualize_img_mask(img_train, mask_train, k = 3):\n    N = img_train.shape[0]\n    fig, ax = plt.subplots(k, 2, figsize=(8*2, 5*k))\n    for i,l in enumerate(random.sample(range(N), k)):\n        img = img_train.iloc[l]\n        mask = mask_train.iloc[l]\n        ax[i,0].imshow(Image.open(img))\n        ax[i,1].imshow(Image.open(mask))\n        \nvisualize_img_mask(img_train, mask_train, k=4)","e56f10e6":"from skimage import io\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.python.keras import Sequential\nfrom tensorflow.keras import layers, optimizers\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\nfrom tensorflow.keras.applications import VGG19\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nimport random\n\nnp.random.seed(7)\ntf.random.set_seed(7)","a6a6fa4b":"def mean_iou(y_true, y_pred):\n    y_pred = tf.round(tf.cast(y_pred, tf.int32))\n    intersect = tf.reduce_sum(tf.cast(y_true, tf.float32) * tf.cast(y_pred, tf.float32), axis=[1])\n    union = tf.reduce_sum(tf.cast(y_true, tf.float32),axis=[1]) + tf.reduce_sum(tf.cast(y_pred, tf.float32),axis=[1])\n    smooth = tf.ones(tf.shape(intersect))\n    return tf.reduce_mean((intersect + smooth) \/ (union - intersect + smooth))\n\ndef iou_loss(y_true, y_pred):\n    y_true = tf.reshape(y_true, [-1])\n    y_pred = tf.reshape(y_pred, [-1])\n    intersection = tf.reduce_sum(tf.cast(y_true, tf.float32) * tf.cast(y_pred, tf.float32))\n    score = (intersection + 1.) \/ (tf.reduce_sum(tf.cast(y_true, tf.float32)) + \n    tf.reduce_sum(tf.cast(y_pred, tf.float32)) - intersection + 1.)\n    return 1 - score","d64a0a3b":"n_classes = 23\nH = 512    \nW = H*6\/\/4   # to keep the H:W ratio equal to 4000:6000 <- the original ratio\n\ndef conv_block(input, num_filters):\n    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    return x\n\ndef decoder_block(input, skip_features, num_filters):\n    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n    x = Concatenate()([x, skip_features])\n    x = conv_block(x, num_filters)\n    return x\n\ndef build_vgg19_unet(input_shape):\n    \"\"\" Input \"\"\"\n    inputs = Input(input_shape)\n\n    \"\"\" Pre-trained VGG19 Model \"\"\"\n    vgg19 = VGG19(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n\n    \"\"\" Encoder \"\"\"\n    s1 = vgg19.get_layer(\"block1_conv2\").output         \n    s2 = vgg19.get_layer(\"block2_conv2\").output         \n    s3 = vgg19.get_layer(\"block3_conv4\").output         \n    s4 = vgg19.get_layer(\"block4_conv4\").output         \n\n    \"\"\" Bridge \"\"\"\n    b1 = vgg19.get_layer(\"block5_conv4\").output         \n\n    \"\"\" Decoder \"\"\"\n    d1 = decoder_block(b1, s4, 512)                     \n    d2 = decoder_block(d1, s3, 256)                     \n    d3 = decoder_block(d2, s2, 128)                    \n    d4 = decoder_block(d3, s1, 64)                      \n\n    \"\"\" Output \"\"\"\n    outputs = Conv2D(n_classes, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n\n    model = Model(inputs, outputs, name=\"VGG19_U-Net\")\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nmodel = build_vgg19_unet((H,W,3))\nmodel.summary()","18deafad":"## Dataset Pipeline used for training the model\n\ndef read_image(x):\n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (W, H))\n    x = x\/255.0\n    x = x.astype(np.float32)\n    return x\n\n\ndef read_mask(x):\n    x = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (W, H))\n    x = x.astype(np.int32)\n    return x\n\n\ndef tf_dataset(x,y, batch=4):\n    dataset = tf.data.Dataset.from_tensor_slices((x,y))\n    dataset = dataset.shuffle(buffer_size=500)\n    dataset = dataset.map(preprocess)\n    dataset = dataset.batch(batch)\n    dataset = dataset.repeat()\n    dataset = dataset.prefetch(2)\n    return dataset\n    \n\ndef preprocess(x,y):\n    def f(x,y):\n        x = x.decode()\n        y = y.decode()\n        image = read_image(x)\n        mask = read_mask(y)\n        return image, mask\n    \n    image, mask = tf.numpy_function(f,[x,y],[tf.float32, tf.int32])\n    mask = tf.one_hot(mask, n_classes, dtype=tf.int32)\n    image.set_shape([H, W, 3])    # In the Images, number of channels = 3. \n    mask.set_shape([H, W, n_classes])    # In the Masks, number of channels = number of classes. \n    return image, mask","5804a642":"checkpointer = [\n    ModelCheckpoint(filepath=\".\/model.h5\",monitor='val_loss',verbose=2,save_best_only=True, save_freq='epoch'),\n    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.1, verbose=2, min_lr=1e-6),\n    EarlyStopping(monitor='val_loss', patience=8, verbose=2, min_delta=1e-4)\n]\n\nbatch_size = 4\ntrain_dataset = tf_dataset(img_train, mask_train, batch=batch_size)\nval_dataset = tf_dataset(img_val, mask_val, batch=batch_size)\n\ntrain_steps = len(img_train)\/\/batch_size\nval_steps = len(img_val)\/\/batch_size","7ce4d78a":"num_epochs = 50\nstart_time = time.time()\n### if we want to upload the learned weight, we can comment the line below to avoid re-training\n# history = model.fit(train_dataset, steps_per_epoch=train_steps, validation_data=val_dataset, validation_steps=val_steps, epochs=num_epochs, callbacks=checkpointer)\nend_time = time.time()\nprint(f\"Time-taken for training: {(end_time-start_time)\/60 : .3f} min\")\nprint(f\"approx. train-time for each epoch: {(end_time-start_time)\/(60*num_epochs) : .3f}\")","45fa10f4":"def plot_loss_acc(history):\n    fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n    ax[0].plot(history.history['loss'], label='loss')\n    ax[0].plot(history.history['val_loss'], label='val_loss')\n    ax[0].legend()\n    ax[0].set_title('Training and Validation Loss')\n    ax[1].plot(history.history['accuracy'], label='accuracy')\n    ax[1].plot(history.history['val_accuracy'], label='val_accuracy')\n    ax[1].legend()\n    ax[1].set_title('Training and Validation Accuracy')\n    print('Plots for Loss and Accuracy of training:')\n    \nplot_loss_acc(history)","17f128db":"# model_loaded = tf.keras.models.load_model('.\/model.h5')\nmodel_loaded = tf.keras.models.load_model('\/kaggle\/input\/aerial-vgg19-unet-50-epochs-weight\/aerial_vgg19_conv2Dtranspose-model-learned-50_epochs.h5')","50e49ab2":"def visualize_predictions(img_test, mask_test, k = 3):\n    N = img_test.shape[0]\n    fig, ax = plt.subplots(k, 3, figsize=(7*3, 5*k))\n    for i,l in enumerate(random.sample(range(N), k)):\n        x = read_image(img_test.iloc[l])\n        y = read_mask(mask_test.iloc[l])\n        p = model_loaded.predict(x[np.newaxis, ...])[0]\n        p = np.argmax(p, axis=-1)\n        ax[i,0].imshow(x)\n        ax[i,0].set_title('image')\n        ax[i,1].imshow(y)\n        ax[i,1].set_title('ground-truth mask')\n        ax[i,2].imshow(p)\n        ax[i,2].set_title('predicted mask')\n        \nvisualize_predictions(img_test, mask_test, k=5)","f997c7b1":"unlearned_model = build_vgg19_unet((H,W,3))\n\ndef print_unlearned_loaded_model_weight_absolute_diff(unlearned_model, model_loaded):\n    j = 0\n    print('Average Absolute Weight difference between Unlearned and Learned Models:')\n    print('Encoder Layers --------------------------------')\n    for um_layer, ml_layer in zip(unlearned_model.layers, model_loaded.layers):\n        umw = um_layer.get_weights()\n        mlw = ml_layer.get_weights()\n        if 'transpose' in ml_layer.name and j == 0:\n            print('Decoder Layers --------------------------------')\n            j = 1\n        if umw and mlw:\n            wt0_num = np.array(umw[0].shape).sum()\n            wt1_num = np.array(umw[1].shape).sum()\n            um_ml_w0_diff = np.sum(np.abs(umw[0]-mlw[0]))\n            um_ml_w1_diff = np.sum(np.abs(umw[1]-mlw[1]))\n            print(f'{ml_layer.name:>25} --> {um_ml_w0_diff\/wt0_num:7.2f}, {um_ml_w1_diff\/wt1_num:5.2f}')\nprint_unlearned_loaded_model_weight_absolute_diff(unlearned_model, model_loaded)","6294f37a":"test_images = []\ntest_masks = []\nfor i in tqdm(range(len(img_test))):\n    test_images.append(read_image(img_test.iloc[i]))\n    test_masks.append(read_mask(mask_test.iloc[i]))\n\ntest_images = np.array(test_images)\ntest_masks = np.array(test_masks)","b0f765a4":"pred_masks = model(test_images[:10])  # checking for 10 images, memory error while checking for 90 images\npred_masks = np.argmax(pred_masks, axis=3)\n\nconf_mat = confusion_matrix(test_masks[:10].flatten(), pred_masks.flatten())\n\n# normalize confusion matrix\nconf_mat_norm_r = conf_mat\/conf_mat.sum(axis=1)[:, np.newaxis]\nconf_mat_norm_p = conf_mat\/conf_mat.sum(axis=0)[:, np.newaxis]","72558fff":"class_mask_per_df['precision'] = class_mask_per_df.apply(lambda x: conf_mat_norm_p[x['class'], x['class']], axis=1)\nclass_mask_per_df['recall'] = class_mask_per_df.apply(lambda x: conf_mat_norm_r[x['class'], x['class']], axis=1)\nclass_mask_per_df","1ac16200":"## store images and labels\/masks file paths into dataframe for easy train, val, test split\n* we can store file paths of images and masks into df and then use `train_test_split` to get paths for train, test, val set\n* since we are more interested in visualizing the result of the learned model, we can keep 85% dataset for training\n* we can assign a higher percentage for validation than testing, since numerical performance on val dataset guide training","b215a1c4":"# Visualize performance of trained model on unseen test set","e2854228":"## Creating Callbacks and Training the Model\n* `ModelCheckpoint` callback to store model locally\n* `ReduceLROnPlateau` to reduce learning rate when training reaches a plateau\n* `EarlyStopping` to stop early (to avoid overfitting)\n\n## Batch size selection\n* a `batch_size` of value 8, 16, or 32 would be better \n    * but increasing batch size to even 5 or 6 was resulting into memory error on CUDA (GPU) or RAM memory\n    * with a larger compute, one should choose a `batch_size` of 8, 16 or 32 \n        * to get a smoother training loss and accuracy curve\n        * overall slight performance improvement","34812d3a":"### Finding precision and recall per label for 10 test images","a2c1d5af":"# Future Steps\n\n* References\n    * https:\/\/towardsdatascience.com\/review-segnet-semantic-segmentation-e66f2e30fb96\n    * https:\/\/github.com\/divamgupta\/image-segmentation-keras\n    * https:\/\/www.programmerall.com\/article\/46711017648\/\n    * https:\/\/stats.stackexchange.com\/questions\/252810\/in-cnn-are-upsampling-and-transpose-convolution-the-same\n\n* **Model Selection**  \n    * We can try different pre-trained encoders like VGG16, MobileNet, ResNet-50, Vanilla CNN\n* **Upsampling**, **Deconvolution**, **SegNet**\n    * We can also try different Segmentation architectures e.g.\n        * PSPNet, SegNet, Mask R-CNN\n        * SegNet keeps track of pooling indices from encoder and use it during upsampling in corresponding decoder","7cbb912e":"## Functions to prepare dataset into batches for model fit\n* Function `tf_dataset` to prepare dataset into batches for model fit\n* functions `read_image`, and `read_mask` to take path of image and mask and convert into array","e4958233":"## Data Handling\n* store filename of images and annotated labels\/masks from dataset\n* sort the file names to match original image to corresponding mask","efd302ad":"# Measure changes in Model Encoder and Decoder weights\n* For U-net, we starts with pre-trained weight of an encoder model; not the same for decoder model.\n    * This makes sense as decoder learning is expected to depend more heavily on the type of masks\/labels.\n* For this section, I just wanted to check in the avg absolute weight difference in encoder and decoder layers.\n* Result:\n    * there is a gap between abs weight difference in encoder and decoder layers.\n    * but weight difference in encoder layer is significant enough to not freeze these layers.\n    * the pattern of values of these weights in corresponding layers is also interesting.","6485c3b9":"# Preprocessing","cb2a694a":"# Notes and References\nSome articles I liked for theory behind Semantic Segmentation (SS):       \nU-net SS: https:\/\/medium.datadriveninvestor.com\/unet-for-semantic-segmentation-implementation-from-scratch-26e043fdcffa               \nU-net SS: https:\/\/towardsdatascience.com\/understanding-semantic-segmentation-with-unet-6be4f42d4b47   \nU-net SS: https:\/\/medium.com\/@keremturgutlu\/semantic-segmentation-u-net-part-1-d8d6f6005066   \nMask R-CNN: https:\/\/medium.com\/analytics-vidhya\/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4    \nMask R-CNN: https:\/\/jonathan-hui.medium.com\/image-segmentation-with-mask-r-cnn-ebe6d793272    \nI have learned a lot and copied snippets from the following notebooks:         \nhttps:\/\/www.kaggle.com\/bulentsiyah\/deep-learning-based-semantic-segmentation-keras\/notebook      \nhttps:\/\/www.kaggle.com\/kimchanyoung\/implementation-of-mobileunet-in-tensorflow        \nhttps:\/\/www.kaggle.com\/sahintiryaki\/brain-tumor-segmentation-vgg19-unet      \nhttps:\/\/github.com\/zhixuhao\/unet                  \nImage Augmentation for Semantic Segmentation:          \nhttps:\/\/idiotdeveloper.com\/data-augmentation-for-semantic-segmentation-deep-learning\/          \nmodule used for image augmentation: https:\/\/github.com\/albumentations-team\/albumentations       ","91834d88":"## Image Resize for Training\n* Ideally, training on same size image and mask as input will give better result\n    * because of more pixel points, and hence resulting to *larger* dataset\n* Due to the RAM and GPU compute limitation, images were resized with same height, width ratio\n    * tried multiple values for height ranging above 600, with proportional width\n    * all values for heights above 700 leads to memory error\n    * height values ranging from 600 to 700 was resulting in model error\n        * a difference of 1 in size tuple of `b1` and `s4` due to various convolutional methods\n* Notice that one can use other pre-trained models as encoder like `VGG16`, `MobileNet`, etc.\n    * For different encoders, one needs to define decoder accordingly\n    * For different models, one can choose different height and width to optimize for corresponding compute\n* One can use Google Colab for free GPU by importing the project from Kaggle to Google Colab\n    * though Google Colab does not seem to provide more compute in terms of GPU and CPU, Colab compute varies\n* Some resources to compare\/check resources for Kaggle vs Colab\n    * https:\/\/towardsdatascience.com\/kaggle-vs-colab-faceoff-which-free-gpu-provider-is-tops-d4f0cd625029      \n    * https:\/\/www.analyticsvidhya.com\/blog\/2021\/06\/how-to-load-kaggle-datasets-directly-into-google-colab\/ \n    * https:\/\/blog.paperspace.com\/best-google-colab-alternatives\/  ","23ba07d7":"### Visualization of Augmented Images and Masks\n* lets visualize augmented images and masks to make sure that images and masks are as expected\n* visualize random images and respective masks from training set","5b139408":"## Stats of classes of pixels","334d36f2":"## Data Augmentation\n* there are a total of 400 images only\n* after training using U-net model with VGG19 encoder, we can find that training is not improving much after 4-6 epochs\n* this suggests adding augmentation data\n* we can use https:\/\/github.com\/albumentations-team\/albumentations module to apply some augmentations\n* `RandomCrop`, `HorizontalFlip`, `VerticalFlip`, `RandomRotate90` are used here to create additional augmentated dataset\n    * additionally one can use various methods to blur the image, or add noise for augmentation\n    * we can also add `GridDistortion` and `ShiftScaleRotate` for augmentation\n    * `RGBShift` is not a good idea for augmentation background pixels (road, grass, trees) depend on color\n* Original image size 6000x4000 is large for the compute and memory capacity of Kaggle (with GPU), hence resized to given H, W\n* Original image along with augmentated images are stored in new images and masks folders","43324bc8":"# Create Model","ab97f76a":"# Check classification on different classes"}}