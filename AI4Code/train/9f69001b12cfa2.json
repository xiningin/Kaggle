{"cell_type":{"10606100":"code","873ef16c":"code","056cf6b1":"code","ee171b27":"code","0b61f827":"code","4e65420c":"code","dac00ce6":"code","8906e8a9":"code","feca3738":"code","8a2616bd":"code","2d24144f":"code","07ca9980":"code","121cf276":"code","e760e9d5":"code","4353f1b8":"code","3558f303":"code","7037e1f8":"code","1442adb4":"code","06fef9b1":"code","0e9fa06a":"code","3eb60358":"code","c1db6108":"code","bc4973b3":"code","ae703b0c":"code","f3df50fb":"code","64ce5042":"code","c8722e7b":"code","9bab7dad":"markdown","c4f24c02":"markdown","8d1b09a7":"markdown","fffc96cc":"markdown","b971716f":"markdown","5b67b411":"markdown","012d5ea0":"markdown","86219df9":"markdown","fc68841e":"markdown","a02c8895":"markdown","d9f95a30":"markdown","b9e9d8e6":"markdown","d44a5caa":"markdown","1c93c513":"markdown","8227473d":"markdown","d78a36ed":"markdown","24993dbd":"markdown","06e2c642":"markdown","e3195746":"markdown","0832d2ee":"markdown","066983e8":"markdown","7abf7c44":"markdown","92b4ed10":"markdown","5dba3cc9":"markdown","479772d5":"markdown","4f484051":"markdown","d3cb4220":"markdown","519beccf":"markdown","e38b80b1":"markdown","5a0cce00":"markdown","d9269f6f":"markdown","7b08b4fc":"markdown","6b9c95c4":"markdown","556e55b9":"markdown","4c0ee827":"markdown","8d9dd431":"markdown","b6264786":"markdown"},"source":{"10606100":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","873ef16c":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')","056cf6b1":"#import train and test CSV files\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain.head()\n#take a look at the training data\n#train.describe(include=\"all\")","ee171b27":"\nprint('Number of columns: '+str(len(df.columns)))","0b61f827":"print(df.columns.tolist())\nprint(df.dtypes)","4e65420c":"#draw a bar plot of survival by sex\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train)\n\n#print percentages of females vs. males that survive\nprint(\"Percentage of females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of males who survived:\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)","dac00ce6":"#draw a bar plot of survival by Pclass\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train)\n\n#print percentage of people by Pclass that survived\nprint(\"Percentage of Pclass = 1 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 2 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 3 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","8906e8a9":"train[\"CabinBool\"] = (train[\"Cabin\"].notnull().astype('int'))\ntest[\"CabinBool\"] = (test[\"Cabin\"].notnull().astype('int'))\n\n#calculate percentages of CabinBool vs. survived\nprint(\"Percentage of CabinBool = 1 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of CabinBool = 0 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 0].value_counts(normalize = True)[1]*100)\n#draw a bar plot of CabinBool vs. survival\nsns.barplot(x=\"CabinBool\", y=\"Survived\", data=train)\nplt.show()","feca3738":"# Plot survival rate by Embarked\nsns.barplot(x=\"Embarked\", y=\"Survived\", data=train)\n","8a2616bd":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)\n\ntrain.head()","2d24144f":"#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3, }\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\n\ntrain.head()","07ca9980":"#fill in missing Fare value in test set based on mean fare for that Pclass \nfor x in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][x]):\n        pclass = test[\"Pclass\"][x] #Pclass = 3\n        test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n        \n#map Fare values into groups of numerical values\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\n\n#drop Fare values\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)\n\ntrain.head()","121cf276":"train['Age'] = df['Age'].fillna(df.groupby(['Pclass'])['Age'].transform(np.mean))\ntest['Age'] = df['Age'].fillna(df.groupby(['Pclass'])['Age'].transform(np.mean))","e760e9d5":"train = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)\ntrain = train.drop(['Ticket'], axis = 1)\ntest = test.drop(['Ticket'], axis = 1)\ntrain = train.drop(['Cabin'], axis = 1)\ntest = test.drop(['Cabin'], axis = 1)\ntrain = train.drop([0])\ntrain = train.dropna()\ntest = test.dropna()","4353f1b8":"train.isna().sum()","3558f303":"from sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)","7037e1f8":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","1442adb4":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\n#from sklearn.metrics import plot_roc_curve\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\nfrom sklearn import linear_model, tree, ensemble\nfrom sklearn.feature_selection import SelectFromModel, RFECV\nfrom sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\nfrom optuna.samplers import TPESampler\nimport optuna\n","06fef9b1":"def model(classifier,name):\n    \n    classifier.fit(x_train,y_train)\n    prediction = classifier.predict(x_val)\n    print(\"Algorithm : \",name)\n    print(\"ACCURACY : \",'{0:.2%}'.format(accuracy_score(y_val,prediction))) \n    print(\"CROSS VALIDATION SCORE : \",'{0:.2%}'.format(cross_val_score(classifier,x_train,y_train,cv = 10,scoring = 'accuracy').mean()))\n    print(\"ROC_AUC SCORE : \",'{0:.2%}\\n'.format(roc_auc_score(y_val,prediction)))\n    #plot_roc_curve(classifier, x_val,y_val)\n    #plt.title('ROC_AUC_PLOT')\n    #plt.show()","0e9fa06a":"\n\n\nclassifier_knn = KNeighborsClassifier()\nclassifier_NB = GaussianNB()\nclassifier_LDA = LinearDiscriminantAnalysis()\n\nmodel(classifier_knn, 'KNN')\nmodel(classifier_NB,'Naive Bayes')\nmodel(classifier_LDA, 'Linear Discriminant Analysis')\n","3eb60358":"# Lets split the data into 5 folds.  \n# We will use this 'kf'(KFold splitting stratergy) object as input to cross_val_score() method\nkf =KFold(n_splits=5, shuffle=True, random_state=42)\n\ncnt = 1\n# split()  method generate indices to split data into training and test set.\nfor train_index, test_index in kf.split(x_train,y_train):\n    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n    cnt += 1","c1db6108":"score = cross_val_score(LinearDiscriminantAnalysis(), x_train,y_train, cv= kf, scoring=\"accuracy\")\nprint(f'Scores for each fold are: {score}')\nprint(f'Average score: {\"{:.2f}\".format(score.mean())}')","bc4973b3":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=3).fit(x_train, y_train)","ae703b0c":"#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\nclf_rf_5 = RandomForestClassifier()      \nclr_rf_5 = clf_rf_5.fit(x_train,y_train)\nimportances = clr_rf_5.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(x_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(1, figsize=(14, 13))\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()","f3df50fb":"class Optimizer:\n    def __init__(self, metric, trials=30):\n        self.metric = metric\n        self.trials = trials\n        self.sampler = TPESampler(seed=666)\n        \n    def objective(self, trial):\n        model = create_model(trial)\n        model.fit(x_train, y_train)\n        preds = model.predict(x_val)\n        if self.metric == 'acc':\n            return accuracy_score(y_val, preds)\n        else:\n            return f1_score(y_val, preds)\n            \n    def optimize(self):\n        study = optuna.create_study(direction=\"maximize\", sampler=self.sampler)\n        study.optimize(self.objective, n_trials=self.trials)\n        return study.best_params","64ce5042":"rf = RandomForestClassifier(\n    random_state=666\n)\nrf.fit(x_train, y_train)\npreds = rf.predict(x_val)\n\nprint('Random Forest accuracy: ', accuracy_score(y_val, preds))\nprint('Random Forest f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n    model = RandomForestClassifier(\n        min_samples_leaf=min_samples_leaf, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nrf_f1_params = optimizer.optimize()\nrf_f1_params['random_state'] = 666\nrf_f1 = RandomForestClassifier(\n    **rf_f1_params\n)\nrf_f1.fit(x_train, y_train)\npreds = rf_f1.predict(x_val)\n\nprint('Optimized on F1 score')\nprint('Optimized Random Forest: ', accuracy_score(y_val, preds))\nprint('Optimized Random Forest f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nrf_acc_params = optimizer.optimize()\nrf_acc_params['random_state'] = 666\nrf_acc = RandomForestClassifier(\n    **rf_acc_params\n)\nrf_acc.fit(x_train, y_train)\npreds = rf_acc.predict(x_val)\n\nprint('Optimized on accuracy')\nprint('Optimized Random Forest: ', accuracy_score(y_val, preds))\nprint('Optimized Random Forest f1-score: ', f1_score(y_val, preds))","c8722e7b":"bc = BaggingClassifier(\n    random_state=666\n)\nbc.fit(x_train, y_train)\npreds = bc.predict(x_val)\n\nprint('Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Bagging Classifier f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int('n_estimators', 2, 200)\n    max_samples = trial.suggest_int('max_samples', 1, 100)\n    model = BaggingClassifier(\n        n_estimators=n_estimators, \n        max_samples=max_samples, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nbc_f1_params = optimizer.optimize()\nbc_f1_params['random_state'] = 666\nbc_f1 = BaggingClassifier(\n    **bc_f1_params\n)\nbc_f1.fit(x_train, y_train)\npreds = bc_f1.predict(x_val)\n\nprint('Optimized on F1-score')\nprint('Optimized Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Bagging Classifier f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nbc_acc_params = optimizer.optimize()\nbc_acc_params['random_state'] = 666\nbc_acc = BaggingClassifier(\n    **bc_acc_params\n)\nbc_acc.fit(x_train, y_train)\npreds = bc_acc.predict(x_val)\n\nprint('Optimized on accuracy')\nprint('Optimized Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Bagging Classifier f1-score: ', f1_score(y_val, preds))","9bab7dad":"# 4. Algorithms accuracy","c4f24c02":"sex feature - changing string of male\/female to 0\/1 ","8d1b09a7":"# Columns\n* PassengerId(int) - passanger number\n* Survived(int) - 0-die 1-servived \n* Pclass(int) - Ticket class 1\/2\/3\t\n* Name(string) \n* Sex(string) - male\/female\n* Age(float) - Age in years\t\n* SibSp(int) - # of siblings \/ spouses aboard the Titanic\n* Parch(int) - # of parents \/ children aboard the Titanic\n* Ticket(string) -\tTicket number\n* Fare(float) - \tPassenger fare\t\n* Cabin(string) - Cabin number\n* Embarked(char) - Port of Embarkation C\/Q\/S","fffc96cc":"Pclass Feature","b971716f":"# 1. About the competition\nThe competition is to predict who will survive and who will die at the Titanic using the passanger data.\nduring this project I will try to find out the connections between each data colunm to the servived colunm\nI will use the Logistic regression algorithm \n","5b67b411":"\n# 8. Conclusion\n\nThese are the things I learned during the project:\n1. The assumptions we make about the effect of every feature on the result are not always correct so we need to check the relationships of each feature on the target\n\n2. Ineffective data adversely affect accuracy so we must ignore irrelevant information to get better accuracy\n\n3. The choice of algorithm is a very important part and greatly affects the accuracy of the study so we have to compare several different algorithms to get the best accuracy \n\n","012d5ea0":"# 6. Results","86219df9":"# 5.Data preparation\n* Sex feature \n* Emrarked feature\n* Fare feature\n* Name feature","fc68841e":"\n# 7. Ensembles\n","a02c8895":"# Exersice 3","d9f95a30":"# 9. Bibliography\n\nPresentation of the course\n\nhttps:\/\/www.kaggle.com\/swami84\/titanic-predictions-using-logistic-lda-and-rf\n\nhttps:\/\/www.kaggle.com\/satishgunjal\/tutorial-k-fold-cross-validation\n\nhttps:\/\/towardsdatascience.com\/5-reasons-why-you-should-use-cross-validation-in-your-data-science-project-8163311a1e79\n\nhttps:\/\/www.kaggle.com\/isaienkov\/top-3-efficient-ensembling-in-few-lines-of-code\n\nhttps:\/\/towardsdatascience.com\/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f\n\n\n\n\n\n","b9e9d8e6":"Embarked Feature","d44a5caa":"Embarked - changing of embarked char to 1\/2\/3","1c93c513":"# 5. Cross validation\nK-Fold cross validation\nI will split the data to k groups and check the results.","8227473d":"# 2. Import data","d78a36ed":"# 3. Import Libraries\n* KNeighborsClassifier\n* cross_val_score \n* roc_auc_score\n* GaussianNB \n* LinearDiscriminantAnalysis\n* KFold\n* linear_model\n* tree\n* ensemble\n","24993dbd":"# 8. bibliography\n* Presentation of the course\n* Pandas website - https:\/\/pandas.pydata.org\/pandas-docs\/stable\/index.html\n* Titanic Tutorial -  https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial\/data?select=train.csv\n* An Interactive Data Science Tutorial - https:\/\/www.kaggle.com\/helgejo\/an-interactive-data-science-tutorial\n* Titanic Survival Predictions (Beginner) - https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner\n* stackoverflow.com","06e2c642":"Sex Feature\n","e3195746":"check if there is a nan value","0832d2ee":"Cabin Feature","066983e8":"# 1. Introduction\nAt that part I will use 3 deffirent alogrithm and try to find out what is the best algorithm for the given data.\nI will use K-fold to train the data and to get the average of the score\nI will \nI will\n\n\n\n\n\n","7abf7c44":"we can see that there are more female serviver then male","92b4ed10":"**I checked the accuracy of the 3 algorithm and find out that Naive Bayes algorithm give us the best accuracy \n**","5dba3cc9":"# 7. Summary and Conclusions\nThere are a lot of data on the web about how to analyze the data.\nDuring the project I realized the importance of preparing the data before running the algorithm and it required me a lot of investment to get better accuracy","479772d5":"We can see that for Embarked 1 there is better chance to survive","4f484051":"# 3. Describing data\n* There are 3 dataset of the titanic:\n1. Train - the dataset include 12 columns (including the servived column)\nI use this dataset to train my model and to improve the score and the accurenecy\n2. Test - the dataset include 11 columns (without the servived column)\nthis dataset will check how good is my code \n3. Gender submission - dataset of the servived passanger by id \n\n","d3cb4220":"# End of Exersice 1","519beccf":"# Personal details\n* Name: Roi Naftali \n* ID: 305743494\n* link: https:\/\/www.kaggle.com\/roinaftali\/titanic-competition\n","e38b80b1":"Fare","5a0cce00":"We can see that for CabinBool 1 there is better chance to survive","d9269f6f":"# 6. Feature selection\nFeature selection is the process of selecting a subset of relevant features for use in model construction. Feature selection techniques are used for several reasons:\n1. simplification of models \n2. shorter training times\n3. to avoid unrelevante feature\n\nWe will use Scikit-learn library to find out the relevante fearutes","7b08b4fc":"As you can seen in plot above, after 5 best features importance of features decrease. ","6b9c95c4":"> # 4. Data Visualization\n* Sex feature\n* Pclass feature\n* CabinBool feature\n* Embarked feature","556e55b9":"**Contents:**\n1. Introduction\n2. Import Libraries \n3. About The algoritms\n4. Algotirhms accuracy\n5. Cross validation\n6. Feature selection\n7. Ensembles\n8. Conclusion\n9. bibliography","4c0ee827":"We can see that for Pclass 1 there is the best chance to survive","8d9dd431":"# Contents:\u00b6\n\n1. About the competition\n2. Import data\n3. Describe data\n4. Data Visualization\n5. Data preparation\n6. results\n7. Summary and Conclusions\n8. bibliography","b6264786":"# 2. About The algorithms\n**KNN** - object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors\n\n**Naive Bayes -** Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable\n\n**Linear Discriminant Analysis -** generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification."}}