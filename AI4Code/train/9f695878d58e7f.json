{"cell_type":{"eb9f6898":"code","565e44d1":"code","f63cd7df":"code","40cbc7b8":"code","9bd2a633":"code","7566a7cc":"markdown","979d23b1":"markdown","b2a44182":"markdown","0055cd9e":"markdown","878ca445":"markdown","2e05b20e":"markdown","446bffb4":"markdown"},"source":{"eb9f6898":"import os\nimport urllib.request\nimport tarfile\nimport json\nimport nltk\nimport csv\nimport re\nimport pickle\nfrom os import path\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.models import Word2Vec\nfrom gensim.models import CoherenceModel\n# Gensim logging\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\nword2vec_model = Word2Vec.load(\"word2vec_1000ITR.model\")\nps = PorterStemmer()\nstem_mode = 1 #Use stemming?\n# Download and unpack the collection\ndef getData():\n    urls = ['https:\/\/ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com\/2020-03-27\/comm_use_subset.tar.gz',\n            'https:\/\/ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com\/2020-03-27\/noncomm_use_subset.tar.gz',\n            'https:\/\/ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com\/2020-03-27\/custom_license.tar.gz',\n            'https:\/\/ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com\/2020-03-27\/biorxiv_medrxiv.tar.gz']\n\n    # Create data directory\n    try:\n        os.mkdir('.\/data')\n        print('Directory created')\n    except FileExistsError:\n        print('Directory already exists')\n\n    # Download all files\n    for i in range(len(urls)):\n        urllib.request.urlretrieve(urls[i], '.\/data\/file' + str(i) + '.tar.gz')\n        print('Downloaded file ' + str(i + 1) + '\/' + str(len(urls)))\n        tar = tarfile.open('.\/data\/file' + str(i) + '.tar.gz')\n        tar.extractall('.\/data')\n        tar.close()\n        print('Extracted file ' + str(i + 1) + '\/' + str(len(urls)))\n        os.remove('.\/data\/file' + str(i) + '.tar.gz')\n","565e44d1":"def extract():\n    parsed_data = dict()\n    papers_read = 0\n    limit = 99999999\n    dictionary = []\n    diction = []\n    whole_word_count = dict()\n    # Iterate through all files in the data directory\n    for root_subdir, root_dirs, root_files in os.walk('.\/data'):\n        for root_dir in root_dirs:\n            for subdir, dirs, files in os.walk('.\/data\/' + root_dir):\n                for file in files:\n                        if papers_read>limit:\n                            break\n                        with open(os.path.join(subdir, file)) as f:\n                            word_count = dict()\n                            word_stem_count = dict()\n                            file_data = json.loads(f.read())\n                            paper_id = file_data['paper_id']\n                            metadata = file_data['metadata']\n                            print(papers_read)\n                            if metadata:\n                                title_clean = []\n                                title = metadata['title']\n                                title_token = word_tokenize(title)\n                                for word in title_token:\n                                    title_lem = lemmatizer.lemmatize(word.lower())\n                                    title_sub = re.sub(\"[^a-zA-Z0-9]\", '', title_lem)\n                                    if title_sub != '' and title_sub not in stop_words:\n                                        title_clean.append(title_sub)\n                                authors = list()\n                                for author in metadata['authors']:\n                                    authors.append(\"%s %s\" % (author['first'], author['last']))\n                                authors_val = ' '.join(authors)\n                                author_token = word_tokenize(authors_val)\n                                author_clean=[]\n                                for word in author_token:\n                                    author_lem = lemmatizer.lemmatize(word.lower())\n                                    author_sub = re.sub(\"[^a-zA-Z0-9]\", '', author_lem)\n                                    if author_sub != '' and author_sub not in stop_words:\n                                        author_clean.append(author_sub)\n                            abstract = readText(file_data['abstract'])\n                            if abstract == '':\n                                continue\n                            body = readText(file_data['body_text'])\n\n                            # tokenize\/stemming\/lemmatizer\/lower case\/remove punctuation\n                            words = []\n                            words_stem = []\n                            ngram_words = []\n                            ngram_stem = []\n                            for word in word_tokenize(body):\n                                # PS_word=PS.stem(lem_word)\n                                lem_word = lemmatizer.lemmatize(word.lower())\n                                #  if lem_word not in [\"?\",\"!\",\".\",\"\/\",\"[\",\"]\"]\n                                word_short_only = re.sub(\"[^a-zA-Z0-9]\", '', lem_word)\n                                if len(word_short_only) < 3:\n                                    word_only = ''\n                                elif len(word_short_only) >= 3:\n                                    word_only = word_short_only\n                                if word_only != '' and word_only not in stop_words:\n                                    words_stem.append(ps.stem(word_only))\n                                    words.append(word_only)\n                                    if word_only in word_count:\n                                        word_count[word_only] += 1\n                                    else:\n                                        word_count[word_only] = 1\n                                    if ps.stem(word_only) in word_stem_count:\n                                        word_stem_count[ps.stem(word_only)] += 1\n                                    else:\n                                        word_stem_count[ps.stem(word_only)] = 1\n                                    if ps.stem(word_only) in whole_word_count:\n                                        whole_word_count[ps.stem(word_only)] += 1\n                                    else:\n                                        whole_word_count[ps.stem(word_only)] = 1\n                            papers_read += 1\n                            if stem_mode == 0:\n                                dictionary.append(words)\n                            if stem_mode == 1:\n                                dictionary.append(words_stem)\n                            #diction = LDA(words_stem)\n                            #ngram_words = Ngram(words)\n                            #ngram_stem = Ngram(words_stem)\n                            parsed_data[paper_id] = Paper(title, title_clean, author_clean, abstract, body, words, words_stem, word_count, word_stem_count)\n        diction = corpora.Dictionary(dictionary)\n    return parsed_data, dictionary, diction, whole_word_count\n\na, dictionary, diction, whole_word_count = extract()","f63cd7df":" lda_model = gensim.models.ldamodel.LdaModel(corpus=saved_corpus, id2word=diction, num_topics=6,random_state=100, update_every=1, chunksize=300, passes=5,alpha='auto', per_word_topics=True)","40cbc7b8":"def associate_topics(lda_model, saved_corpus):\n\n    appended_list = []\n    for i in range(len(saved_corpus)):\n        articles_topics = []\n        article_number = i\n        key = lda_model[saved_corpus[i]]\n        conf_scores = []\n        for entry in key[0]:\n            conf_scores.append(entry[1])\n            confidence = max(conf_scores)\n            index = conf_scores.index(confidence)\n            top_topic = key[0][index][0]\n        article_index = list(a.keys())[article_number]\n        articles_topics.extend([article_index,top_topic,confidence])\n        appended_list.append(articles_topics)\n    return appended_list #list containing article number, top matched topic, confidence\n\narticles_list = associate_topics(lda_model, saved_corpus)\nview_topics = lda_model.print_topics()","9bd2a633":"def query_expansion(query):\n    clean_query = []\n    original_clean_query = []\n    rare_term = []\n    freq = []\n    rare_append = []\n    clean_query_stem = []\n    for word in word_tokenize(query):\n        clean_word = re.sub(\"[^a-zA-Z0-9]\", '', word.lower())\n        if clean_word != '' and clean_word not in stop_words:\n            clean_query.append(clean_word)\n    original_query = clean_query.copy()\n    for word in original_query:\n        word_lem = lemmatizer.lemmatize(word.lower())\n        word_stem = ps.stem(word_lem)\n        original_clean_query.append(word_stem)\n    if len(clean_query) > 1:\n        for term in original_clean_query:\n            rare_term.append(term)\n            try:\n                freq.append(whole_word_count[term])\n            except (KeyError):\n                print(\"Vocabulary error, was there a typo?\")\n        freq_copy = freq.copy()\n        mini = min(freq)\n        rare = rare_term[freq_copy.index(mini)]\n        print(\"Keyword 1: \" + str(rare))\n        rare_append.append(rare)\n        rare_append.append(rare)\n        rare_append.append(rare)\n        #rare_append.append(rare)\n        #rare_append.append(rare)\n        freq.remove(min(freq))\n        mini2 = min(freq)\n        rare2 = rare_term[freq_copy.index(mini2)]\n        print(\"Keyword 2: \" + str(rare2))\n        rare_append.append(rare2)\n        rare_append.append(rare2)\n        #rare_append.append(rare2)\n    try:\n        expanded_query = (word2vec_model.most_similar(positive=clean_query))\n    except (KeyError):\n        print(\"Vocabulary error, was there a typo?\")\n        expanded_query = []\n    for i in range(len(expanded_query)):\n       clean_query.append(expanded_query[i][0])\n    for word in clean_query:\n        word_lem = lemmatizer.lemmatize(word.lower())\n        word_stem = ps.stem(word_lem)\n        clean_query_stem.append(word_stem)\n    clean_query_stem = clean_query_stem + rare_append\n    return(clean_query_stem)\n\ndef query_lda(lda_model, query_stem):\n    conf_scores = []\n    query_corpus = diction.doc2bow(query_stem)\n    query_doc = lda_model[query_corpus]\n    for entry in query_doc[0]:\n        conf_scores.append(entry[1])\n        confidence = max(conf_scores)\n        index = conf_scores.index(confidence)\n        top_topic = query_doc[0][index][0]\n    print('Query is assigned topic: '+str(top_topic))\n    print('Topic keywords: '+str(view_topics[top_topic]))\n    print('Topic assignment confidence: '+str(confidence))\n    return(top_topic,confidence,query_stem)\n\nquery = 'development of coronavirus vaccines in mice models'\nexpanded_query = query_expansion(query)\ntopic,confidence,query_stem = query_lda(lda_model,expanded_query)","7566a7cc":"Load in data from a repository of over 30000 documents of peer-reviewed and pre-print articles regarding topics associated with COVID-19","979d23b1":"Using USE and cosine similarity to determine relevance","b2a44182":"Constructing LDA modeling topics based on pre-defined number of topics (6). Chunk size of 300 and over 5 passes.","0055cd9e":"Parse query text, expand query based on a pre-trained word2vec model as well as keyword identification based on inverse document frequency","878ca445":"View topic keywords as well as associate each document to it's predominant topic","2e05b20e":"Filtering out documents associated with query topic","446bffb4":"Parsing the title, abstract and body text of each document, along with lemmatization and word stemming"}}