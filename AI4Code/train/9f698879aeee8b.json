{"cell_type":{"d40cdc4a":"code","33d23a63":"code","93efaabe":"code","e0673ff6":"code","62dde09d":"code","fa271cde":"code","dacc51ec":"code","0ca0fdba":"code","355ce99a":"code","98af8a9e":"code","87574c9d":"code","518cc6e3":"code","f8d6216c":"code","49d17655":"code","202f4b2b":"code","1d0c7cbf":"markdown"},"source":{"d40cdc4a":"import os\nimport imageio\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore',category=FutureWarning)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport cv2\nimport shutil\ntf.logging.set_verbosity(tf.logging.ERROR)","33d23a63":"!nvidia-smi","93efaabe":"class Helpers():\n    \n    @staticmethod\n    def normalize(images):\n        return np.array(images)\/127.5-1.0\n    \n    @staticmethod\n    def unnormalize(images):\n        return (0.5*np.array(images)+0.5)*255\n    \n    @staticmethod\n    def resize(image, size):\n        return np.array(cv2.resize(image, size))\n    \n    @staticmethod\n    def split_images(image, is_testing):\n        image = imageio.imread(image).astype(np.float)\n        _, width, _ = image.shape\n        half_width = int(width\/2)\n        source_image = image[:, half_width:, :]\n        destination_image = image[:, :half_width, :]\n        source_image = Helpers.resize(source_image, (IMAGE_SIZE, IMAGE_SIZE))\n        destination_image = Helpers.resize(destination_image, (IMAGE_SIZE, IMAGE_SIZE))\n        if not is_testing and np.random.random() > 0.5:\n            source_image = np.fliplr(source_image)\n            destination_image = np.fliplr(destination_image)\n        return source_image, destination_image\n    \n    @staticmethod\n    def new_dir(path):\n        shutil.rmtree(path, ignore_errors=True)\n        os.makedirs(path, exist_ok=True)\n        \n    @staticmethod\n    def archive_output():\n        shutil.make_archive(\"output\", \"zip\", \".\/output\")\n        \n    @staticmethod\n    def image_pairs(batch, is_testing):\n        source_images, destination_images = [], []\n        for image_path in batch:\n            source_image, destination_image = Helpers.split_images(image_path, is_testing)\n            source_images.append(source_image)\n            destination_images.append(destination_image)\n        return source_images, destination_images","e0673ff6":"# Requires following dataset structure:\n# dataset_name\n# \u2514\u2500\u2500 dataset_name\n#     \u251c\u2500\u2500 testing\n#     \u2502\u00a0\u00a0 \u2514\u2500\u2500 ... (image files)\n#     \u251c\u2500\u2500 testing_raw\n#     \u2502\u00a0\u00a0 \u251c\u2500\u2500 ... (image files)\n#     \u251c\u2500\u2500 training\n#     \u2502\u00a0\u00a0 \u2514\u2500\u2500 ... (image files)\n#     \u2514\u2500\u2500 validation (optional)\n#         \u2514\u2500\u2500 ... (image files)\nclass DataLoader():\n    \n    def __init__(self, dataset_name=\"pix2pix-depth\"):\n        self.dataset_name = dataset_name\n        base_path = BASE_INPUT_PATH + self.dataset_name + \"\/\" + self.dataset_name + \"\/\"\n        self.training_path = base_path + \"training\/\"\n        self.validation_path = base_path + \"validation\/\"\n        self.testing_path = base_path + \"testing\/\"\n        self.testing_raw_path = base_path + \"testing_raw\/\"\n\n    def load_random_data(self, data_size, is_testing=False):\n        paths = glob(self.training_path+\"*\") if is_testing else glob(self.testing_path+\"*\")\n        source_images, destination_images = Helpers.image_pairs(np.random.choice(paths, size=data_size), is_testing)\n        return Helpers.normalize(source_images), Helpers.normalize(destination_images)\n\n    def yield_batch(self, batch_size, is_testing=False):\n        paths = glob(self.training_path+\"*\") if is_testing else glob(self.validation_path+\"*\")\n        for i in range(int(len(paths)\/batch_size)-1):\n            batch = paths[i*batch_size:(i+1)*batch_size]\n            source_images, destination_images = Helpers.image_pairs(batch, is_testing)\n            yield Helpers.normalize(source_images), Helpers.normalize(destination_images)","62dde09d":"!mkdir .\/training_checkpoints","fa271cde":"# Model architecture from: https:\/\/phillipi.github.io\/pix2pix\/\nclass Pix2Pix(): \n    \n    def __init__(self):\n        Helpers.new_dir(BASE_OUTPUT_PATH + \"training\/\")\n        Helpers.new_dir(BASE_OUTPUT_PATH + \"training\/losses\/\")\n\n        self.image_shape = (IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS)\n        self.data_loader = DataLoader()\n\n        patch = int(IMAGE_SIZE \/ 2**4)\n        self.disc_patch = (patch, patch, 1)\n\n        self.generator_filters = 64\n        self.discriminator_filters = 64\n        \n        optimizer = tf.keras.optimizers.Adam(LEARNING_RATE, BETA_1)\n\n        self.discriminator = self.discriminator()\n        self.discriminator.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"accuracy\"])\n        self.generator = self.generator()\n\n        source_image = tf.keras.layers.Input(shape=self.image_shape)\n        destination_image = tf.keras.layers.Input(shape=self.image_shape)\n        generated_image = self.generator(destination_image)\n\n        self.discriminator.trainable = False\n        valid = self.discriminator([generated_image, destination_image])\n        self.combined = tf.keras.models.Model(inputs=[source_image, destination_image], outputs=[valid, generated_image])\n        self.combined.compile(loss=[\"mse\", \"mae\"], loss_weights=[1, 100], optimizer=optimizer)\n        self.checkpoint_dir = '.\/training_checkpoints'\n        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, \"ckpt\")\n        self.checkpoint = tf.train.Checkpoint(generator_optimizer=optimizer,\n                                         discriminator_optimizer=optimizer,\n                                         generator=self.generator,\n                                         discriminator=self.discriminator)\n        \n        \n        \n    def generator(self):\n        def conv2d(layer_input, filters, bn=True):\n            downsample = tf.keras.layers.Conv2D(filters, kernel_size=4, strides=2, padding=\"same\")(layer_input)\n            downsample = tf.keras.layers.LeakyReLU(alpha=LEAKY_RELU_ALPHA)(downsample)\n            if bn:\n                downsample = tf.keras.layers.BatchNormalization(momentum=BN_MOMENTUM)(downsample)\n            return downsample\n\n        def deconv2d(layer_input, skip_input, filters, dropout_rate=0):\n            upsample = tf.keras.layers.UpSampling2D(size=2)(layer_input)\n            upsample = tf.keras.layers.Conv2D(filters, kernel_size=4, strides=1, padding=\"same\", activation=\"relu\")(upsample)\n            if dropout_rate:\n                upsample = tf.keras.layers.Dropout(dropout_rate)(upsample)\n            upsample = tf.keras.layers.BatchNormalization(momentum=BN_MOMENTUM)(upsample)\n            upsample = tf.keras.layers.Concatenate()([upsample, skip_input])\n            return upsample\n\n        downsample_0 = tf.keras.layers.Input(shape=self.image_shape)\n        downsample_1 = conv2d(downsample_0, self.generator_filters, bn=False)\n        downsample_2 = conv2d(downsample_1, self.generator_filters*2)\n        downsample_3 = conv2d(downsample_2, self.generator_filters*4)\n        downsample_4 = conv2d(downsample_3, self.generator_filters*8)\n        downsample_5 = conv2d(downsample_4, self.generator_filters*8)\n        downsample_6 = conv2d(downsample_5, self.generator_filters*8)\n        downsample_7 = conv2d(downsample_6, self.generator_filters*8)\n\n        upsample_1 = deconv2d(downsample_7, downsample_6, self.generator_filters*8)\n        upsample_2 = deconv2d(upsample_1, downsample_5, self.generator_filters*8)\n        upsample_3 = deconv2d(upsample_2, downsample_4, self.generator_filters*8)\n        upsample_4 = deconv2d(upsample_3, downsample_3, self.generator_filters*4)\n        upsample_5 = deconv2d(upsample_4, downsample_2, self.generator_filters*2)\n        upsample_6 = deconv2d(upsample_5, downsample_1, self.generator_filters)\n        upsample_7 = tf.keras.layers.UpSampling2D(size=2)(upsample_6)\n        \n        output_image = tf.keras.layers.Conv2D(IMAGE_CHANNELS, kernel_size=4, strides=1, padding=\"same\", activation=\"tanh\")(upsample_7)\n        return tf.keras.models.Model(downsample_0, output_image)\n\n    def discriminator(self):\n        def discriminator_layer(layer_input, filters, bn=True):\n            discriminator_layer = tf.keras.layers.Conv2D(filters, kernel_size=4, strides=2, padding=\"same\")(layer_input)\n            discriminator_layer = tf.keras.layers.LeakyReLU(alpha=LEAKY_RELU_ALPHA)(discriminator_layer)\n            if bn:\n                discriminator_layer = tf.keras.layers.BatchNormalization(momentum=BN_MOMENTUM)(discriminator_layer)\n            return discriminator_layer\n\n        source_image = tf.keras.layers.Input(shape=self.image_shape)\n        destination_image = tf.keras.layers.Input(shape=self.image_shape)\n        combined_images = tf.keras.layers.Concatenate(axis=-1)([source_image, destination_image])\n        discriminator_layer_1 = discriminator_layer(combined_images, self.discriminator_filters, bn=False)\n        discriminator_layer_2 = discriminator_layer(discriminator_layer_1, self.discriminator_filters*2)\n        discriminator_layer_3 = discriminator_layer(discriminator_layer_2, self.discriminator_filters*4)\n        discriminator_layer_4 = discriminator_layer(discriminator_layer_3, self.discriminator_filters*8)\n        validity = tf.keras.layers.Conv2D(1, kernel_size=4, strides=1, padding=\"same\")(discriminator_layer_4)\n        return tf.keras.models.Model([source_image, destination_image], validity)\n        \n    def preview_training_progress(self, epoch, size=3):\n        def preview_outputs(epoch, size):\n            source_images, destination_images = self.data_loader.load_random_data(size, is_testing=True)\n            generated_images = self.generator.predict(destination_images)\n            grid_image = None\n            for i in range(size):\n                row = Helpers.unnormalize(np.concatenate([destination_images[i], generated_images[i], source_images[i]], axis=1))\n                if grid_image is None:\n                    grid_image = row\n                else:\n                    grid_image = np.concatenate([grid_image, row], axis=0)\n            plt.figure(figsize=(15,18))\n            plt.imshow(grid_image\/255.0)\n            plt.show()\n            plt.close()\n            grid_image = cv2.cvtColor(np.float32(grid_image), cv2.COLOR_RGB2BGR)\n            cv2.imwrite(BASE_OUTPUT_PATH + \"training\/ \" + str(epoch) + \".png\", grid_image)\n            \n        def preview_losses():\n            def plot(title, data):\n                plt.plot(data, alpha=0.6)\n                plt.title(title + \"_\" + str(i))\n                plt.savefig(BASE_OUTPUT_PATH + \"training\/losses\/\" + title + \"_\" + str(i) + \".png\")\n                plt.close()\n            for i, d in enumerate(self.d_losses):\n                plot(\"discriminator\", d)\n            for i, g in enumerate(self.g_losses):\n                plot(\"generator\", g)\n                \n        preview_outputs(epoch, size)\n        #preview_losses()\n\n    def train(self):\n#         checkpoint_path = \"training_2\/cp-{epoch:04d}.ckpt\"\n        valid = np.ones((BATCH_SIZE,) + self.disc_patch)\n        fake = np.zeros((BATCH_SIZE,) + self.disc_patch)\n        self.d_losses = []\n        self.g_losses = []\n        self.preview_training_progress(0)\n        for epoch in range(EPOCHS):\n            filename2 = '.\/training_checkpoints\/model_%06d.h5' % (epoch+1)\n            epoch_d_losses = []\n            epoch_g_losses = []\n            for iteration, (source_images, destination_images) in enumerate(self.data_loader.yield_batch(BATCH_SIZE)):\n                generated_images = self.generator.predict(destination_images)\n                d_loss_real = self.discriminator.train_on_batch([source_images, destination_images], valid)\n                d_loss_fake = self.discriminator.train_on_batch([generated_images, destination_images], fake)\n                d_losses = 0.5 * np.add(d_loss_real, d_loss_fake)\n                g_losses = self.combined.train_on_batch([source_images, destination_images], [valid, source_images])\n                epoch_d_losses.append(d_losses)\n                epoch_g_losses.append(g_losses)\n                print(\"\\repoch: \" + str(epoch) \n                      +\", iteration: \"+ str(iteration) \n                      + \", d_losses: \" + str(d_losses) \n                      + \", g_losses: \" + str(g_losses)\n                      , sep=\" \", end=\" \", flush=True)\n            self.d_losses.append(np.average(epoch_d_losses, axis=0))\n            self.g_losses.append(np.average(epoch_g_losses, axis=0))\n            self.preview_training_progress(epoch)\n#             self.checkpoint.save(file_prefix=self.checkpoint_prefix)\n            \n            self.generator.save(filename2)\n        \n        return epoch_d_losses,epoch_g_losses\n        \n    def test(self):\n        image_paths = glob(self.data_loader.testing_raw_path+\"*\")\n        for image_path in image_paths:\n            image = np.array(imageio.imread(image_path))\n            image_normalized = Helpers.normalize(image)\n            generated_batch = self.generator.predict(np.array([image_normalized]))\n            concat = Helpers.unnormalize(np.concatenate([image_normalized, generated_batch[0]], axis=1))\n            cv2.imwrite(BASE_OUTPUT_PATH+os.path.basename(image_path), cv2.cvtColor(np.float32(concat), cv2.COLOR_RGB2BGR))\n            ","dacc51ec":"# d_losses","0ca0fdba":"BASE_INPUT_PATH = \"..\/input\/pix2pix-depth\/\" \nBASE_OUTPUT_PATH = \".\/output\/\"\n\n\nIMAGE_SIZE = 256\nIMAGE_CHANNELS = 3\nLEARNING_RATE = 0.00015\nBETA_1 = 0.5\nLEAKY_RELU_ALPHA = 0.2\nBN_MOMENTUM = 0.8\nEPOCHS = 10\nBATCH_SIZE = 32\n\ngan = Pix2Pix()\nd_losses1,g_losses1 = gan.train()\ngan.test()\nHelpers.archive_output()","355ce99a":"d_loss_ax1 = np.average(d_losses1, axis=1)\nplt.figure(figsize=(18,5))\nplt.plot(d_loss_ax1)","98af8a9e":"g_loss_ax1 = np.average(g_losses1, axis=1)\nplt.figure(figsize=(18,5))\nplt.plot(g_loss_ax1)","87574c9d":"plt.plot(g_losses)","518cc6e3":"# !ls -GFlash --color .\/training_checkpoints","f8d6216c":"# step = 20\n# filename2 = 'model_%06d.h5' % (step+1)\n# # model = Pix2Pix.generator()\n# # model = gan.generator\n# # model.save_weights(filename2)\n# # print(filename2)","49d17655":"# model_sep = gan.generator.load_weights(\".\/training_checkpoints\/model_000001.h5\")\n# new_model = tf.keras.models.load_model('.\/training_checkpoints\/model_000001.h5')","202f4b2b":"# data_loader = DataLoader()\n# image_paths = glob(data_loader.testing_raw_path+\"*\")\n# for image_path in image_paths:\n#     image = np.array(imageio.imread(image_path))\n#     image_normalized = Helpers.normalize(image)\n#     generated_batch = new_model.predict(np.array([image_normalized]))\n# #     print(generated_batch.shape)\n#     plt.imshow(generated_batch.reshape(256,256,3))\n#     break\n# #     concat = Helpers.unnormalize(np.concatenate([image_normalized, generated_batch[0]], axis=1))\n# #     cv2.imwrite(BASE_OUTPUT_PATH+os.path.basename(image_path), cv2.cvtColor(np.float32(concat), cv2.COLOR_RGB2BGR))","1d0c7cbf":"# !Added checkpoint after each epoch "}}