{"cell_type":{"d49da105":"code","458d666f":"code","cf0c968c":"code","691a66d3":"code","c3762662":"code","84fa39cb":"code","72cb1352":"code","b052b425":"code","0312a1dc":"code","ee7411f4":"code","334682fc":"code","db2283c4":"code","5e4aa7ac":"code","bb45c756":"code","d40f70d1":"code","29ac0fb1":"code","9c31dc59":"code","5dff4a77":"code","b54fbf79":"markdown","f515d405":"markdown","ba1fc2b6":"markdown","5a035cec":"markdown","620ddb65":"markdown","12ef1872":"markdown","d71ace9b":"markdown","1f1babf1":"markdown","91a470a4":"markdown","caf454d5":"markdown","88271fec":"markdown","25f848e9":"markdown","cb44f190":"markdown"},"source":{"d49da105":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","458d666f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import Model","cf0c968c":"mnist = tf.keras.datasets.fashion_mnist.load_data()\n(X_train, y_train), (X_test, y_test) = mnist","691a66d3":"X_train.shape, y_train.shape","c3762662":"X_train, X_test = X_train \/ 255, X_test \/255","84fa39cb":"X_train = np.expand_dims(X_train, -1)\nX_test = np.expand_dims(X_test, -1)","72cb1352":"K=  len(set(y_train))\nprint(\"Number of classes: \", K)","b052b425":"i = layers.Input(shape = (X_train[0].shape))\nx = layers.Conv2D(64, (3,3), activation='relu', strides = (2,2), padding= 'same')(i)\nx = layers.Conv2D(64, (3,3), activation='relu', strides = (2,2), padding= 'same')(x)\nx = layers.Conv2D(32, (3,3), activation='relu', strides = (1,1), padding= 'same')(x)\nx = layers.Flatten()(x)\nx = layers.Dense(512, activation='relu')(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(K, activation='softmax')(x)\n\nmodel = Model(i, x)","0312a1dc":"model.summary()","ee7411f4":"tf.keras.utils.plot_model(model, to_file='CNN_model.png', show_shapes=True, show_layer_names=True)","334682fc":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nr = model.fit(X_train,\n         y_train,\n         epochs=15,\n         validation_data=(X_test, y_test),\n         batch_size=32, verbose=1)","db2283c4":"print(\"The class of this is: {}\".format(y_train[0]))\nplt.imshow(X_train[0], cmap='gray')\nplt.show()","5e4aa7ac":"plt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()","bb45c756":"plt.plot(r.history['accuracy'], label='acc')\nplt.plot(r.history['val_accuracy'], label='val_acc')\nplt.legend()","d40f70d1":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n               horizontalalignment=\"center\",\n               color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\n\np_test = model.predict(X_test).argmax(axis=1)\ncm = confusion_matrix(y_test, p_test)\nplot_confusion_matrix(cm, list(range(10)))","29ac0fb1":"labels = '''T-shirt\/top\nTrouser\nPullover\nDress\nCoat\nSandal\nShirt\nSneaker\nBag\nAnkle boot'''.split(\"\\n\")\n\nplt.figure(figsize=(16,16))\nj = 1\nfor i in np.random.randint(0,1000,16):\n    plt.subplot(4,4,j); j+=1\n    plt.imshow(X_train[i],cmap = 'Greys')\n    plt.axis('off')\n    plt.title('{} \/ {}'.format(labels[y_train[i]],y_train[i]))","9c31dc59":"plt.figure(figsize=(16,16))\nj = 1\nfor i in np.random.randint(0,1000,36):\n    plt.subplot(6,6,j); j+=1\n    plt.imshow(X_test[i],cmap = 'Greys')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.title('Actual: {} \\n Prediction: {}'.format(labels[y_test[i]],labels[p_test[i]]))","5dff4a77":"misclassified_idx = np.where(p_test != y_test)[0]\ni = np.random.choice(misclassified_idx)\nplt.imshow(X_test[i].reshape(28,28), cmap='gray')\nplt.title(\"True label: %s Predicted: %s\" % (labels[y_test[i]], labels[p_test[i]]));","b54fbf79":"Fashion-MNIST consists of 60,000 training images and 10,000 test images. It is a MNIST-like fashion product database. The developers believe MNIST has been overused so they created this as a direct replacement for that dataset. Each image is in greyscale and associated with a label from 10 classes.\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.\nEach pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.","f515d405":"# SUMMARY OF THE MODEL","ba1fc2b6":"# Fashion-MNIST Image Classification using Deep Learning","5a035cec":"# TEST SET AND PREDICTED SET CHECKING WITH PLOT","620ddb65":"# ADDING LIBRARIES \/ USING TENSORFLOW","12ef1872":"# Look Over Confusion matrix for more information and Evaluating the Model","d71ace9b":"# Plotting the model loss","1f1babf1":"# np. expand_dims to make images shape (numberofImages, 28, 28, 1)","91a470a4":"# Plotting the model accuracies","caf454d5":"For the convolutional front-end, we can start with a Three convolutional layer with a small filter size and a modest number of filters (64) followed by a max pooling layer. The filter maps can then be flattened to provide features to the classifier.\nGiven that the problem is a multi-class classification, we know that we will require an output layer with 10 nodes in order to predict the probability distribution of an image belonging to each of the 10 classes. This will also require the use of a softmax activation function. Between the feature extractor and the output layer, we can add a dense layer to interpret the features, in this case with 128 nodes.\nAll layers will use the ReLU activation function and the He weight initialization scheme, both best practices.","88271fec":"# Convolutional Layers Added","25f848e9":"# USING TENSORFLOW TO UPLOAD DATASET","cb44f190":"# FITTING THE MODEL WITH Sparse categorical crossentropy"}}