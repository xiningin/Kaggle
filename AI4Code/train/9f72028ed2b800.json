{"cell_type":{"7920ff46":"code","98df53be":"code","580a7bfa":"code","8523c0bf":"code","f1d7b56f":"code","080e0036":"code","23486b35":"code","c51fb888":"code","6d3b5899":"code","9d28afbc":"code","b48321f6":"code","43251107":"code","e069eb37":"code","fa83dfc7":"code","d7466696":"code","9f4a566b":"code","a951896d":"code","f6303bf2":"code","fa14ac17":"code","6db1a76b":"code","554478e2":"code","cd5a15a0":"code","0dbf5e1d":"code","8afb464d":"code","6ec824ac":"code","1579338b":"code","ab1f3ebc":"code","5be7e70f":"code","88ee91a3":"code","0f623165":"code","fe8610c9":"code","9c448437":"code","e1bb235b":"code","96608dc6":"code","fe01bd3c":"code","18048ba2":"code","7b8b6f4e":"code","37802bb0":"code","70d99f63":"code","4997ac60":"code","f32e4131":"code","5f4f3313":"code","c3a170fb":"code","677343ce":"code","f83d21aa":"code","602b0eb4":"code","81aa204c":"code","d67ed837":"code","bc55c892":"code","29364e60":"markdown","360e0856":"markdown","da700fb6":"markdown","08b22908":"markdown","6b67ff2e":"markdown","7c50a1b7":"markdown","2aa0efc0":"markdown","6a603e4f":"markdown","f1fc96ff":"markdown","0b512c89":"markdown","45df34e8":"markdown","0b0357cf":"markdown","1daf5303":"markdown","16e55998":"markdown","ca5e3865":"markdown","af51f1c6":"markdown","bb338966":"markdown","c358680e":"markdown","bbfce447":"markdown","70ccf028":"markdown","d96ca3e5":"markdown","f5fd0334":"markdown","fa9b5588":"markdown","7d6e75a0":"markdown","1675b3ee":"markdown","f9edc8d3":"markdown","431832ee":"markdown","c69d8d0b":"markdown","c09ff816":"markdown","ced57e31":"markdown","7c054d63":"markdown","16faf6fe":"markdown","3025ece2":"markdown","b30f9f49":"markdown","acadcd7a":"markdown","a286f4f6":"markdown","8617a9f1":"markdown","8e7127c8":"markdown","87d3d064":"markdown","1858c46c":"markdown","d84ecf9f":"markdown","a070eb2f":"markdown","99616a22":"markdown"},"source":{"7920ff46":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import some necessary librairies\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O\n\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew # for some statistics\n\n# Settings\nsns.set(style=\"darkgrid\")\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\nplt.rcParams['figure.figsize'] = (12, 4)\npd.options.display.max_columns = 500","98df53be":"# Now let's import and put the train and test datasets in  pandas dataframe\n\n#train_df_org = pd.read_csv('house_prices_train.csv')\n#test_df_org = pd.read_csv('house_prices_test.csv')\n\ntrain_df_org = pd.read_csv('..\/input\/train.csv')\ntest_df_org = pd.read_csv('..\/input\/test.csv')\n\n# train set dimension\nprint('Train dataset dimension: {} rows, {} columns'.format(train_df_org.shape[0], train_df_org.shape[1]))\n\n# test set dimension\nprint('Test dataset dimension: {} rows, {} columns'.format(test_df_org.shape[0], test_df_org.shape[1]))\n\nprint('First few observations of AMES housing prices train dataset: ')\ntrain_df_org.head()","580a7bfa":"# Metadata of Titatnic dataset\nobject_col_names = train_df_org.select_dtypes(include=[np.object]).columns.tolist()\nint_col_names = train_df_org.select_dtypes(include=[np.int64]).columns.tolist()\nfloat_col_names = train_df_org.select_dtypes(include=[np.float64]).columns.tolist()\ntarget_var = 'SalesPrice'\n\nnum_col_names = int_col_names + float_col_names\ntotal_col_names = object_col_names + int_col_names + float_col_names\n\nif len(total_col_names) == train_df_org.shape[1]:\n    print('Number of Features count matching. Train Dataset Features: ', train_df_org.shape[1], ' Features Count: ', len(total_col_names))\nelse:\n    print('Number of Features count not matching. Train Dataset Features: ', train_df_org.shape[1], ' Features Count: ', len(total_col_names))\n\nprint('\\nTotal number of object features: ', len(object_col_names))\nprint(object_col_names)\n\nprint('\\nTotal number of integer features: ', len(int_col_names))\nprint(int_col_names)\n\nprint('\\nTotal number of float features: ', len(float_col_names))\nprint(float_col_names)","8523c0bf":"train_df_org.describe()","f1d7b56f":"train_df_proc = train_df_org.copy()\ntest_df_proc = test_df_org.copy()\n\n#Save the 'Id' column\ntrain_ID = train_df_proc['Id']\ntest_ID = test_df_proc['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain_df_proc.drop(\"Id\", axis = 1, inplace = True)\ntest_df_proc.drop(\"Id\", axis = 1, inplace = True)\n\n# train set dimension\nprint('Size of train dataset after dropping Id: {} rows, {} columns'.format(train_df_proc.shape[0], train_df_proc.shape[1]))\n\n# test set dimension\nprint('Size of train dataset after dropping Id: {} rows, {} columns'.format(test_df_proc.shape[0], test_df_proc.shape[1]))","080e0036":"# Create the default pairplot\nplot_cols1 = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', 'SalePrice']\nplot_cols2 = ['GrLivArea','LotArea', 'PoolArea', 'GarageArea', '2ndFlrSF', 'SalePrice']\n\nsns.pairplot(train_df_proc[plot_cols1]);","23486b35":"sns.pairplot(train_df_proc[plot_cols2]);","c51fb888":"fig, ax = plt.subplots()\nax.scatter(x = train_df_proc['GrLivArea'], y = train_df_proc['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show();","6d3b5899":"train_df_proc[(train_df_proc['GrLivArea']>4000) & (train_df_proc['SalePrice']<300000)]","9d28afbc":"#Deleting outliers\ntrain_df_proc = train_df_proc.drop(train_df_proc[(train_df_proc['GrLivArea']>4000) & (train_df_proc['SalePrice']<300000)].index)","b48321f6":"# most correlated features\ncorrmat = train_df_proc.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(train_df_proc[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","43251107":"sns.barplot(train_df_proc.OverallQual,train_df_proc.SalePrice);","e069eb37":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_df_proc[cols], size = 2.5)\nplt.show();","fa83dfc7":"def check_skewness(col):\n    sns.distplot(train_df_proc[col] , fit=norm);\n    fig = plt.figure()\n    res = stats.probplot(train_df_proc[col], plot=plt)\n    # Get the fitted parameters used by the function\n    (mu, sigma) = norm.fit(train_df_proc[col])\n    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n    \ncheck_skewness('SalePrice')","d7466696":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain_df_proc[\"SalePrice\"] = np.log1p(train_df_proc[\"SalePrice\"])\n\ncheck_skewness('SalePrice')","9f4a566b":"ntrain = train_df_proc.shape[0]\nntest = test_df_proc.shape[0]\ny_train = train_df_proc.SalePrice.values\nall_df = pd.concat((train_df_proc, test_df_proc)).reset_index(drop=True)\nall_df.drop(['SalePrice'], axis=1, inplace=True)\n\nprint('Size of train & test dataset comined: {} rows, {} columns'.format(all_df.shape[0], all_df.shape[1]))","a951896d":"null_feat_df = pd.DataFrame()\nnull_feat_df['Null Count'] = all_df.isnull().sum().sort_values(ascending=False)\nnull_feat_df['Null Pct'] = null_feat_df['Null Count'] \/ float(len(all_df))\n\nnull_feat_df = null_feat_df[null_feat_df['Null Pct'] > 0]\n\ntotal_null_feats = null_feat_df.shape[0]\nnull_feat_names = null_feat_df.index\nprint('Total number of features having null values: ', total_null_feats)\nprint('Name of features having null values: ', null_feat_names)\n\nf, ax = plt.subplots(figsize=(12, 4))\nplt.xticks(rotation='90')\nsns.barplot(x=null_feat_df.index, y=null_feat_df['Null Pct'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15);","f6303bf2":"none_col = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'MasVnrType', \n            'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'KitchenQual',\n            'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\nfor col in none_col:\n    all_df[none_col] = all_df[none_col].fillna('None')\n\nzero_col = ['GarageArea', 'GarageCars', 'MasVnrArea', 'BsmtFullBath', 'BsmtHalfBath',\n            'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF']\nfor col in zero_col:\n    all_df[col] = all_df[col].fillna(0)\n\nmode_col = ['MSZoning', 'Exterior1st', 'Exterior2nd']\nfor col in mode_col:\n    all_df[col] = all_df[col].fillna(all_df[col].mode()[0])\n\nother_col = ['Functional', 'Utilities', 'Electrical', 'SaleType', 'LotFrontage', 'GarageYrBlt']\nall_df['Functional'] = all_df['Functional'].fillna('Typ')\nall_df['Utilities'] = all_df['Utilities'].fillna('AllPub')\n#all_df['MSSubClass'] = all_df['MSSubClass'].fillna(190)\nall_df['Electrical'] = all_df['Electrical'].fillna('SBrkr')\nall_df['SaleType'] = all_df['SaleType'].fillna('Oth')\n#all_df['GarageYrBlt'] = all_df['GarageYrBlt'].fillna(all_df['YearBuilt'])\nall_df['GarageYrBlt'] = all_df['GarageYrBlt'].fillna(0)\n\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_df[\"LotFrontage\"] = all_df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\ntotal_impute_cols = none_col + zero_col + mode_col + other_col\n\nif len(null_feat_names) == len(total_impute_cols):\n    print('Number of Null Features count matching. Null Features: ', len(null_feat_names), ' Imputed Features: ', len(total_impute_cols))\n    print(set(null_feat_names) - set(total_impute_cols))\nelse:\n    print('Number of Null Features count not matching. Null Features: ', len(null_feat_names), ' Imputed Features: ', len(total_impute_cols))\n    print(set(total_impute_cols) - set(null_feat_names))","fa14ac17":"null_feat_df = pd.DataFrame()\nnull_feat_df['Null Count'] = all_df.isnull().sum().sort_values(ascending=False)\nnull_feat_df['Null Pct'] = null_feat_df['Null Count'] \/ float(len(all_df))\n\nnull_feat_df = null_feat_df[null_feat_df['Null Pct'] > 0]\n\ntotal_null_feats = null_feat_df.shape[0]\nnull_feat_names = null_feat_df.index\nprint('Total number of features having null values: ', total_null_feats)\nprint('Name of features having null values: ', null_feat_names)","6db1a76b":"# Basic statistics of categorical features\nall_df.describe(include=[np.object])","554478e2":"all_df = all_df.drop(['Utilities'], axis=1)\nprint('Size of dataset after removing Utilities feature: {} rows, {} columns'.format(all_df.shape[0], all_df.shape[1]))","cd5a15a0":"# Basic statistics of categorical features\nall_df.describe()","0dbf5e1d":"#MSSubClass=The building class\nall_df['MSSubClass'] = all_df['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_df['OverallCond'] = all_df['OverallCond'].astype(str)\n\n#Year and month sold are transformed into categorical features.\nall_df['YrSold'] = all_df['YrSold'].astype(str)\nall_df['MoSold'] = all_df['MoSold'].astype(str)\n\n# Additional Attributes\n#all_df['OverallQual'] = all_df['OverallQual'].astype(str)\n#all_df['YearBuilt'] = all_df['YearBuilt'].astype(str)\n#all_df['YearRemodAdd'] = all_df['YearRemodAdd'].astype(str)\n#all_df['GarageYrBlt'] = all_df['GarageYrBlt'].astype(str)","8afb464d":"from sklearn.preprocessing import LabelEncoder\ncols = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'YrSold', 'MoSold',\n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond'] \n#        'OverallQual', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt']\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    label_enc = LabelEncoder() \n    label_enc.fit(list(all_df[c].values)) \n    all_df[c] = label_enc.transform(list(all_df[c].values))\n\n# shape        \nprint('Size of dataset after label encoding: {} rows, {} columns'.format(all_df.shape[0], all_df.shape[1]))","6ec824ac":"# Adding total sqfootage feature \nall_df['TotalSF'] = all_df['TotalBsmtSF'] + all_df['1stFlrSF'] + all_df['2ndFlrSF']","1579338b":"numeric_feats = all_df.dtypes[all_df.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\n\nsns.barplot(skewness.index,skewness.Skew);","ab1f3ebc":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_df[feat] = boxcox1p(all_df[feat], lam)","5be7e70f":"all_df = pd.get_dummies(all_df)\n\n# shape        \nprint('Size of dataset after dummies: {} rows, {} columns'.format(all_df.shape[0], all_df.shape[1]))","88ee91a3":"final_train_df = all_df[:ntrain]\nfinal_test_df = all_df[ntrain:]\n\n# shape        \nprint('Size of training dataset: {} rows, {} columns'.format(final_train_df.shape[0], final_train_df.shape[1]))\nprint('Size of testing dataset: {} rows, {} columns'.format(final_test_df.shape[0], final_test_df.shape[1]))","0f623165":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","fe8610c9":"#Validation function\nn_folds = 5\n\ndef kfold_cv_rmsle(model, X, y):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n    rmsle = np.sqrt(-cross_val_score(model, X.values, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmsle)\n\ndef kfold_cv_pred(model, X, y):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n    y_pred = cross_val_predict(model, X.values, y, cv=kf)\n\n    return(y_pred)","9c448437":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nscore = kfold_cv_rmsle(KRR, final_train_df, y_train)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","e1bb235b":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nscore = kfold_cv_rmsle(lasso, final_train_df, y_train)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","96608dc6":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nscore = kfold_cv_rmsle(ENet, final_train_df, y_train)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","fe01bd3c":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nscore = kfold_cv_rmsle(GBoost, final_train_df, y_train)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","18048ba2":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nscore = kfold_cv_rmsle(model_xgb, final_train_df, y_train)\nprint(\"XGBoost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","7b8b6f4e":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nscore = kfold_cv_rmsle(model_lgb, final_train_df, y_train)\nprint(\"LightGBM score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","37802bb0":"LassoMd = lasso.fit(final_train_df.values,y_train)\nENetMd = ENet.fit(final_train_df.values,y_train)\nKRRMd = KRR.fit(final_train_df.values,y_train)\nGBoostMd = GBoost.fit(final_train_df.values,y_train)","70d99f63":"from sklearn.metrics import mean_squared_error\n\nlasso_train_pred = LassoMd.predict(final_train_df.values)\nENet_train_pred = ENetMd.predict(final_train_df.values)\nKRR_train_pred = KRRMd.predict(final_train_df.values)\nGBoost_train_pred = GBoostMd.predict(final_train_df.values)\n\navg_train_pred = (lasso_train_pred + ENet_train_pred + KRR_train_pred + GBoost_train_pred) \/ 4\n\navg_rmsle = np.sqrt(mean_squared_error(y_train, avg_train_pred))\nprint(\"Average Model RMSLE score: {:.4f}\".format(avg_rmsle))\n\navg_train_pred = np.expm1(avg_train_pred)\navg_train_pred","4997ac60":"lasso_test_pred = np.expm1(LassoMd.predict(final_test_df.values))\nENet_test_pred = np.expm1(ENetMd.predict(final_test_df.values))\nKRR_test_pred = np.expm1(KRRMd.predict(final_test_df.values))\nGBoost_test_pred = np.expm1(GBoostMd.predict(final_test_df.values))\n\nfinalMd = (lasso_test_pred + ENet_test_pred + KRR_test_pred + GBoost_test_pred) \/ 4\nfinalMd","f32e4131":"SEED = 42 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\n#kf = KFold(ntrain, n_folds=NFOLDS, random_state=SEED)\n\ndef get_oof(model, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n    \n    kf = KFold(NFOLDS, shuffle=False, random_state=42).split(final_train_df.values)\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        model.fit(x_tr, y_tr)\n\n        oof_train[test_index] = model.predict(x_te)\n        oof_test_skf[i, :] = model.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","5f4f3313":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","c3a170fb":"# Create our OOF train and test predictions. These base results will be used as new features\nENet_oof_train, ENet_oof_test = get_oof(ENet, final_train_df.values, y_train, final_test_df.values)\nKRR_oof_train, KRR_oof_test = get_oof(KRR, final_train_df.values, y_train, final_test_df.values)\nXGB_oof_train, XGB_oof_test = get_oof(model_xgb, final_train_df.values, y_train, final_test_df.values)\n#lasso_oof_train, lasso_oof_test = get_oof(lasso, x_train, y_train, x_test)","677343ce":"base_predictions_train = pd.DataFrame( {'Kernel Ridge': KRR_oof_train.ravel(),\n#                                        'Lasso': lasso_oof_train.ravel(),\n                                        'Elastic Net': ENet_oof_train.ravel(),\n                                        'XGBoost': XGB_oof_train.ravel()\n                                       } )\nbase_predictions_train.head()","f83d21aa":"x_train = np.concatenate((KRR_oof_train, ENet_oof_train, XGB_oof_train), axis=1)\nx_test = np.concatenate((KRR_oof_test, ENet_oof_test, XGB_oof_test), axis=1)","602b0eb4":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nStacked_Model = lasso.fit(x_train, y_train)\n\nn_folds = 5\nkf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train)\nrmsle_score = np.sqrt(-cross_val_score(Stacked_Model, x_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\nprint(\"Staked Lasso Model score: {:.4f} ({:.4f})\\n\".format(rmsle_score.mean(), rmsle_score.std()))\n\nfinalMd = Stacked_Model.predict(x_test)\nfinalMd = np.expm1(finalMd)\nfinalMd","81aa204c":"from mlxtend.regressor import StackingRegressor\n\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\nstregr = StackingRegressor(regressors=[KRR, model_xgb, ENet], \n                           meta_regressor=lasso)\n\nstregr.fit(final_train_df, y_train)\nstregr_train_pred = stregr.predict(final_train_df)\n\nstregr_rmsle = np.sqrt(mean_squared_error(y_train, stregr_train_pred))\nprint(\"Stacking Regressor Model RMSLE score: {:.4f}\".format(avg_rmsle))\nprint('Stacking Regressor Variance Score: %.4f' % stregr.score(final_train_df, y_train))\n\nstregr_train_pred = np.expm1(stregr_train_pred)\nstregr_train_pred","d67ed837":"stregr_test_pred = stregr.predict(final_test_df)\nfinalMd = np.expm1(stregr_test_pred)\nfinalMd","bc55c892":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = finalMd\nsub.to_csv('submission.csv',index=False)","29364e60":"As of 16-Nov, my score on kaggle is 0.1108 and rank is 58 which is top 10% (Actually Top 4%) :)\n\n**We can improve the performance of model with more permutations and combinations. You can fork my code for other trials. If you found this notebook helpful or you just liked it, some upvotes would be very much appreciated. I'll be glad to hear suggestions on improving my models**","360e0856":"### Transform Categorical Features\nMachine Learning model works only on numerical datasets, hence, we need to transform categorical features into numerical features. One of the best strategy is to convert each category value into a new column and assigns 1 or 0 (True\/False) value to the column. This has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set. This approach is also called as \"One Hot Encoding\". We can use Pandas feature get_dummies to achieve this transformation.","da700fb6":"After imputing, lets check for any missing values.","08b22908":"### Stacked Model - Out-of-Fold Predictions\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.","6b67ff2e":"#### Gradient Boosting Regression","7c50a1b7":"### Stacked Model using StackingRegressor","2aa0efc0":"**Getting dummy categorical features**","6a603e4f":"### Box Cox Transformation of (highly) Skewed Features\nWhen you are dealing with real-world data, you are going to deal with features that are heavily skewed. Transformation technique is useful to **stabilize variance**, make the data **more normal distribution-like**, improve the validity of measures of association.\n\nThe problem with the Box-Cox Transformation is **estimating lambda**. This value will depend on the existing data, and should be considered when performing cross validation on out of sample datasets.","f1fc96ff":"### Importing packages\nWe have **numpy** and **pandas** to work with numbers and data, and we have **seaborn** and **matplotlib** to visualize data. We would also like to filter out unnecessary warnings. **Scipy** for normalization and skewing of data.","0b512c89":"- From this we can tell which features **(OverallQual, GrLivArea and TotalBsmtSF )** are highly positively correlated with the SalePrice. \n- **GarageCars and GarageArea ** also seems correlated with other, Since the no. of car that will fit into the garage will depend on GarageArea. ","45df34e8":"### Describe Dataset\n- Generate basic statistics of numerical features\n- Generate basic statistics of categorical features","0b0357cf":"### Final Submission","1daf5303":"### Inspecting Dataset\nIdentify type of datasets and number of features belonging to each dataset. This will be useful later in pre-processing the dataset","16e55998":"### Feature Engineering\n**Concatenate both train and test values.** In this way, we can pre-process and clean the features of train & test dataset together.","ca5e3865":"### Modelling - Simple One by One\nIdetify best models based on the Cross Validated score. Since in this dataset we have a large set of features. So to make our model avoid Overfitting and noisy we will use Regularization. These model have Regularization parameter. Regularization will reduce the magnitude of the coefficients.\n\nBelow models hyper parameters has been tuned to identify best possible parameter values to obtain best score by using GridSearchCV library. It takes a good amount of time and hence, I have not listed that code here which was executed separately to identify best hyperparameters.\n\n**Cross Validation** It's simple way to calculate error for evaluation. \n\n**KFold( )** splits the train\/test data into k consecutive folds, we also have made shuffle attrib to True.\n\n**cross_val_score ( )** evaluate a score by cross-validation.","af51f1c6":"### Target Variable Transform\nDifferent features in the data set may have values in different ranges. For example, in this data set, the range of SalePrice feature may lie from thousands to lakhs but the range of values of YearBlt feature will be in thousands. That means a column is more weighted compared to other.\n\n**Lets check the skewness of data**\n![Skew](https:\/\/cdn-images-1.medium.com\/max\/800\/1*hxVvqttoCSkUT2_R1zA0Tg.gif)","bb338966":"#### Elastic Net Regression\nElastic net is basically a combination of both L1 and L2 regularization. So if you know elastic net, you can implement both Ridge and Lasso by tuning the parameters.","c358680e":"#### Light Gradient Boosting Regression","bbfce447":"**This distribution is positively skewed.** Notice that the black curve is more deviated towards the right. If you encounter that your predictive (response) variable is skewed, it is **recommended to fix the skewness** to make good decisions by the model.\n\n### Fixing Skewness\nThe best way to fix it is to perform a **log transform** of the same data, with the intent to reduce the skewness.","70ccf028":"**Fit the training dataset on every model**","d96ca3e5":"### Highly Skewed Features\nLets see the highly skewed features we have","f5fd0334":"* **Utilities** : Since this is a categorical data and most of the data are of same category, Its not gonna effect on model. So we choose to drop it.","fa9b5588":"#### Ridge Regression\nIt shrinks the parameters, therefore it is mostly used to prevent multicollinearity. It reduces the model complexity by coefficient shrinkage. It uses L2 regularization technique.","7d6e75a0":"We can see that there are outlinear with low SalePrice and high GrLivArea. This looks odd.\nWe need to remove it.","1675b3ee":"### Handle Missing Data\nSince PoolQC has the highest null values according to the data documentation says **null values means 'No Pool.**\nSince majority of houses has no pool. So we will replace those null values with 'None'.\n\nAlso, based on the data documentation we can infer below & handle the missing values accordingly:\n* **MiscFeature** : Data documentation says NA means \"no misc feature\"\n* **Alley** : data description says NA means \"no alley access\"\n* **Fence** : data description says NA means \"no fence\"\n* **FireplaceQu** : data description says NA means \"no fireplace\"\n* **LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.\n* **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None as per documentation.\n* **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n* **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement\n* **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2** : For all these categorical basement-related features, NaN means that there is no basement.\n* **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.\n* **MSZoning (The general zoning classification)** : 'RL' is by far the most common value. So we can fill in missing values with 'RL'\n* **Functional** : data description says NA means typical\n* **Electrical,KitchenQual, Exterior1st, Exterior2nd, SaleType** : Since this all are categorical values so its better to replace nan values with the most used keyword.\n* **MSSubClass** : Na most likely means No building class. We can replace missing values with None","f9edc8d3":"After taking logarithm of the same data the curve seems to be normally distributed, although not perfectly normal, this is sufficient to fix the issues from a skewed dataset as we saw before.\n\n**Important : If you log transform the response variable, it is required to also log transform feature variables that are skewed.**","431832ee":"## Mean of all model's Prediction.\nnp.expm1 ( ) is used to calculate exp(x) - 1 for all elements in the array. ","c69d8d0b":"**Drop the Id column because we dont need it currently.**\nBut, save the test data Id for later final submissions.","c09ff816":"### Correlation Analysis\nLet see the most correlated analysis of features against target and identify the most correlated features.","ced57e31":"# Top 10% AMES House Pricing with Stacked Regression","7c054d63":"**Now there any many features that are numerical but categorical.**\n\n**Converting some numerical variables that are really categorical type.**\n\nAs you can see the category range from 1 to 9 which are numerical (**not ordinal type**). Since its categorical we need to change it to String type. If we do not convert these to categorical, some model may get affect by this as model will compare the value 1<5<10 . We dont need that to happen with our model.","16faf6fe":"### Prediction based on Mean of Best Models","3025ece2":"#### Lasso Regression\nLASSO (Least Absolute Shrinkage Selector Operator), is quite similar to ridge. In case of lasso, even at smaller alpha\u2019s, our coefficients are reducing to absolute zeroes. Therefore, lasso selects the only some feature while reduces the coefficients of others to zero. This property is known as feature selection and which is absent in case of ridge.\n \nLasso uses L1 regularization technique. Lasso is generally used when we have more number of features, because it automatically does feature selection.\n\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline","b30f9f49":"#### XG Boosting Regression","acadcd7a":"**Scatter plots between 'SalePrice' and correlated variables**","a286f4f6":"### Identify Features with NULL values\nTo understand how each features are impacting the default loan indicator, we need to first handle the null \/ missing values, otherwise our observations might not be accurate and will lead to wrong conclusions.","8617a9f1":"## Predictive Analytics on AMES Housing Data\nAmes Housing Authority is a public housing agency that serves the city of Ames, Iowa, US. It helps provide decent and safe rental housing for eligible low-income families, the elderly, and persons with disabilities\nThe housing authority has collected 79 assessment parameters which describes every aspect of residential homes in Ames. These variables focus on the quality and quantity of the physical attributes of a property. Most of the variables are exactly the type of information that a typical home buyer would want to know about a potential property. \n\n**Problem Statement:** Predict Home Sale Price for the Test Data Set with the lowest possible error.","8e7127c8":"### Exploratory Data Analysis\nIdentify correlation of various numeric features against target SalePrice outcome.","87d3d064":"### Describe Dataset\n- Generate basic statistics of numerical features\n- Generate basic statistics of categorical features","1858c46c":"Creating train and test dataset as before combining both.","d84ecf9f":"### Dealing with outliers\nBased on the above graphs, we can see that there are outliers with low SalePrice and high GrLivArea. So, we can remove these outliers, so that our prediction is not impacted.","a070eb2f":"### Loading AMES Housing Price Dataset\nLoad the train and test dataset. Let's look into how many observations are there in train and test dataset and how many features are present in the dataset.","99616a22":"Since area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house"}}