{"cell_type":{"9068827c":"code","214ff5d7":"code","26dc111b":"code","ab13902b":"code","055ab5d8":"code","eb101c7b":"code","538c1cb9":"code","70527055":"code","1595cf2b":"code","af94f06a":"code","20a66567":"code","416e5fa5":"code","f767d07a":"code","cc632a04":"code","6fa5d66e":"code","ca9ffd8d":"code","cd67ab53":"code","2a564a52":"code","2708ee15":"code","d0f8ee6d":"code","ced77b0f":"code","23b9034d":"code","469540e4":"code","48bc29dc":"code","78e49189":"code","ad15aee4":"code","3e6ca367":"code","3c87bded":"code","b85955b5":"code","c4bd6faf":"code","130954b5":"code","0a2b82fa":"code","0fd213fb":"code","d8631680":"code","85410a78":"code","c78eef3d":"code","6f3119be":"code","2df4fee5":"code","036bb081":"markdown","d6291acf":"markdown","fc1837e3":"markdown","3adab7ce":"markdown","00cf256c":"markdown","d22bee88":"markdown","c66a360c":"markdown","8c9bcc7e":"markdown","b7727ae4":"markdown","45f3d76e":"markdown","913f068a":"markdown","fbf82009":"markdown","486e24c0":"markdown","e3405f0d":"markdown","a9a63b01":"markdown","25863b21":"markdown","e4ed2a8d":"markdown","7a3b7aae":"markdown","e5103fb4":"markdown","4bc1fead":"markdown","2a6565df":"markdown","8db7282b":"markdown","e02b80b7":"markdown","c8944117":"markdown","ec106680":"markdown","82cd3780":"markdown","3b965d54":"markdown","50daea7b":"markdown","78863cba":"markdown","e7abbf3e":"markdown","550b6444":"markdown","9ff14205":"markdown","9c49f40c":"markdown","978dda69":"markdown","2d743be5":"markdown","4bfe9c35":"markdown","24c721b4":"markdown","81dbf19a":"markdown","58d632c1":"markdown","541f5174":"markdown"},"source":{"9068827c":"import os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [10, 5]\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n! pip install glove_python\nfrom glove import Corpus, Glove\nimport re\nfrom IPython.display import HTML\n\nfrom sklearn.decomposition import PCA\n\n! pip install country_converter --upgrade\n! pip install pycountry\nimport country_converter as coco\nimport pycountry","214ff5d7":"# helper functions to clean data and parse into tabular format\n\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)\n\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n#             format_authors(file['metadata']['authors']),\n#             format_authors(file['metadata']['authors'], \n#                            with_affiliation=True),\n#             format_body(file['abstract']),\n            format_body(file['body_text'])\n#             format_bib(file['bib_entries']),\n#             file['metadata']['authors'],\n#             file['bib_entries']\n        ]\n\n        cleaned_files.append(features)\n\n#     col_names = ['paper_id', 'title', 'authors',\n#                  'affiliations', 'abstract', 'text', \n#                  'bibliography','raw_authors','raw_bibliography']\n    col_names = ['paper_id', 'title', 'text']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n\n    return clean_df","26dc111b":"# load files from each folder\n# biorxiv\/medrxiv\n#brx_files = load_files('\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/')\n\n# custom license\n#pdf_dir = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/'\n#pdf_files = load_files(pdf_dir)\n\n# common use subset\n#comm_dir = '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pdf_json\/'\n#comm_files = load_files(comm_dir)\n\n# noncommon use subset\n#noncomm_dir = '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pdf_json\/'\n#noncomm_files = load_files(noncomm_dir)","ab13902b":"# read files in each folder into a dataframe and combined into one single csv called complete_df.csv and save to the output folder\n# biorxiv\/medrxiv\n#complete_df = generate_clean_df(brx_files)\n\n# custom license\n#tmp_df = generate_clean_df(pdf_files)\n#complete_df = pd.concat([complete_df, tmp_df])\n\n# common use subset\n#tmp_df = generate_clean_df(comm_files)\n#complete_df = pd.concat([complete_df, tmp_df])\n\n# noncommon use subset\n#tmp_df = generate_clean_df(noncomm_files)\n#complete_df = pd.concat([complete_df, tmp_df])\n\n# save to disk\n#complete_df.reset_index(inplace= True ,drop = True)\n#complete_df.to_csv('complete_df.csv')","055ab5d8":"metadata = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')","eb101c7b":"source_count = metadata.groupby('source_x')['cord_uid'].nunique().sort_values(ascending = False)\n# Plot a bar graph:\nplt.bar(\n    source_count.index,\n    source_count.values,\n    align=\"center\",\n    color=\"orange\"\n)\n\nplt.title(\"Number of papers by database source\")\nplt.xticks(source_count.index)\nplt.show()","538c1cb9":"topjournal = metadata.groupby('journal')['cord_uid'].nunique().sort_values(ascending = False).reset_index()\nprint('total number of papers:', np.sum(topjournal['cord_uid']))\ntopjournal.rename(columns={'cord_uid':'number of papers'}, inplace=True)\nHTML(topjournal[:10].to_html(index=False)) # show top 10 journals","70527055":"def clean_title(row):\n    if row is None:\n        row = \"\"\n    row = str(row).lower()   \n    row = re.sub('[^A-Za-z0-9]+', ' ', row)  \n    word_list = row.split() \n    wnl = WordNetLemmatizer()\n    stop_list = stopwords.words('english')\n    word_list = [wnl.lemmatize(word) for word in word_list if word not in stop_list]  \n    row = \" \".join(word_list) \n    return row\n\nmetadata_title_df = pd.DataFrame({'Title':list(metadata['title'])})\nmetadata_title_df['Title'] = metadata_title_df['Title'].apply(clean_title)\n# understand the most frequent words\nmetadata_words = \" \".join(word for word in metadata_title_df.Title)\nfreq_words_metadata = pd.DataFrame(pd.value_counts(metadata_words.split(\" \")).sort_values(ascending=False).reset_index())\nfreq_words_metadata.columns = ['words', 'freq']\nfreq_words_metadata = freq_words_metadata.reset_index(drop = True)","1595cf2b":"# more_stopwords = (list(freq_words_metadata['words'][1:30]))\n# wnl = WordNetLemmatizer()\n# metadata_words_clean = \" \".join([wnl.lemmatize(word) for word in metadata_words.split(\" \") if word not in more_stopwords])\nwordcloud1 = WordCloud(max_font_size=30, max_words=200, background_color=\"white\").generate(metadata_words)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud1, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","af94f06a":"def word_tokenizer(sentence):\n    return word_tokenize(sentence) \n\ndef clean_words(tokenized_sentences):\n    stop_words = set(stopwords.words('english'))\n    # added a lemmatizer\n    wnl = WordNetLemmatizer()\n    return [wnl.lemmatize(t) for t in tokenized_sentences if not t in stop_words] \n\ndef remove_unchars(doc):\n    doc = re.sub(r'''(?i)\\b((?:https?:\/\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))''', '', doc, flags=re.MULTILINE)\n    doc = re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)', '', doc)\n    doc = re.sub(r'\\b[0-9]+\\b\\s*', '', doc)\n    return doc\n\ndef preprocessing_text(text):\n    text = text.lower()\n    text = remove_unchars(text)\n    words = word_tokenizer(text)\n    cleaned_words = clean_words(words)\n    return cleaned_words\n\ndef create_corpus(df):\n    corpus=[]\n    stop_words = stopwords.words('english')\n    wnl = WordNetLemmatizer()\n    for abstract in tqdm(df['abstract']):\n        words=[wnl.lemmatize(word.lower()) for word in word_tokenize(abstract) if((word not in stop_words))]\n        corpus.append(words)\n    return corpus","20a66567":"metadata['abstract'] = metadata['abstract'].astype(str)\nmetadata['abstract']= metadata['abstract'].apply(lambda x : remove_unchars(x))\ncorpus_abstract = create_corpus(metadata)","416e5fa5":"#training the corpus to generate the co occurence matrix which is used in GloVe\ncorpus = Corpus ()\ncorpus.fit(corpus_abstract, window=10)\n\n#creating a Glove object which will use the matrix created in the above lines to create embeddings\n#We can set the learning rate as it uses Gradient Descent and number of components\n\nglove = Glove(no_components=40, learning_rate=0.01)\n \nglove.fit(corpus.matrix, epochs=100, no_threads=4, verbose=False)\nglove.add_dictionary(corpus.dictionary) \nglove.save('glove_40d.model')","f767d07a":"# helper function for checking similar word results\ndef get_similar_words(word_list, model, n_words=20, show_html=True):\n    \"\"\"\n    return top n words from a word list, output in table format\n    \"\"\"\n    new_df = pd.DataFrame()\n    words = []\n    for word in word_list:\n        sim_result = model.most_similar(word, n_words+1)\n        df = pd.DataFrame(sim_result, columns=['similar to \\'' + word + '\\'', 'score'])\n        new_df = pd.concat([new_df, df], axis=1)\n        words = words + df.iloc[:, 0].tolist()\n        \n    if show_html:\n        display(HTML(new_df.to_html(escape=False,index=False)))    \n        \n    return(new_df, words)","cc632a04":"# load saved model and dataset (optional)\n#glove = Glove.load('glove_40d.model')\n#complete_df = pd.read_csv('complete_df.csv')","6fa5d66e":"_,_= get_similar_words(['detection'], glove)","ca9ffd8d":"# text search function\ndef text_search(word_list, data, column, limit=10):\n    \"\"\"\n    count how many words from a word list appear in a given body of text\n    return indices of counts in descending order\n    \"\"\"\n    text_list = [str(text).lower() for text in data.loc[:, column].tolist()]\n    \n    counts = np.zeros(len(text_list))\n    for idx, text in enumerate(text_list):\n        tc = [1 if word in text else 0 for word in word_list]\n        counts[idx] = np.sum(tc)\n    \n    idx = np.argsort(-counts)[:limit]\n    \n    return((idx, counts[idx]))\n\n\ndef parse_result(metadata, idx, topic_str):\n    \"\"\"\n    format search output result in the notebook to match the suggested submission format\n    formatting code inspired by https:\/\/www.kaggle.com\/mlconsult\/summary-page-covid-19-risk-factors\n    \"\"\"\n    df_table = pd.DataFrame(columns = [\"pub_date\", \"authors\", \"title\"])\n    meta_sub = metadata.iloc[idx, :]\n    for index, row in meta_sub.iterrows():\n        authors=str(row[\"authors\"]).split(\", \")\n        link=row['doi']\n        title=row[\"title\"]\n        linka='https:\/\/doi.org\/'+str(link)\n        linkb=title\n        final_link='<p align=\"left\"><a href=\"{}\">{}<\/a><\/p>'.format(linka,linkb)\n        to_append = [row['publish_time'],authors[0]+' et al.',final_link]\n        df_length = len(df_table)\n        df_table.loc[df_length] = to_append\n    \n    filename=topic_str+'.csv'\n    df_table.to_csv(filename,index = False)\n    df_table=HTML(df_table.to_html(escape=False,index=False))\n    display(df_table)\n    \n    return(meta_sub['sha'])\n\ndef subtopic_to_df(metadata, idx, topic_str):\n    \"\"\"\n    save the artile title, publish date, id and cordid into a df\n    \"\"\"\n    df_table = pd.DataFrame(columns = [\"title\", \"pub_date\", \"authors\", \"sha\"])\n    meta_sub = metadata.iloc[idx, :]\n    for index, row in meta_sub.iterrows():\n        title=row[\"title\"]\n        publish = row['publish_time']\n        authors=str(row[\"authors\"]).split(\", \")\n        sha = row['sha']\n        to_append = [title, publish,authors[0]+' et al.',sha]\n        df_length = len(df_table)\n        df_table.loc[df_length] = to_append\n    \n    return(df_table)\n","cd67ab53":"manual_words = ['detection', 'asymptomatic', 'antibody']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx1,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids1 = parse_result(metadata, idx1, 'topic1')","2a564a52":"manual_words = ['surveillance', 'platform']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx2,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids2 = parse_result(metadata, idx2, 'topic2')","2708ee15":"manual_words = ['local', 'support']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx3,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids3 = parse_result(metadata, idx3, 'topic3')","d0f8ee6d":"manual_words = ['best', 'practice', 'communication', 'public']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx4,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids4 = parse_result(metadata, idx4, 'topic4')","ced77b0f":"manual_words = ['pointofcare', 'rapid', 'tradeoff']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx5,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids5 = parse_result(metadata, idx5, 'topic5')","23b9034d":"manual_words = ['pcr', 'adhoc', 'intervention']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx6,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids6 = parse_result(metadata, idx6, 'topic6')","469540e4":"manual_words = ['issue', 'migrate', 'instrument']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx7,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids7 = parse_result(metadata, idx7, 'topic7')","48bc29dc":"manual_words = ['evolution', 'mutation']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx8,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids8 = parse_result(metadata, idx8, 'topic8')","78e49189":"manual_words = ['viralload']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx9,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids9 = parse_result(metadata, idx9, 'topic9')","ad15aee4":"manual_words = ['cytokine']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx10,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids10 = parse_result(metadata, idx10, 'topic10')","3e6ca367":"manual_words = ['protocol', 'screening']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx11,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids11 = parse_result(metadata, idx11, 'topic11')","3c87bded":"manual_words = ['supply', 'swab', 'reagent']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx12,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids12 = parse_result(metadata, idx12, 'topic12')","b85955b5":"manual_words = ['technology', 'roadmap']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx13,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids13 = parse_result(metadata, idx13, 'topic13')","c4bd6faf":"manual_words = ['coalition', 'preparedness']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx14,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids14 = parse_result(metadata, idx14, 'topic14')","130954b5":"manual_words = ['crispr']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx15,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids15 = parse_result(metadata, idx15, 'topic15')","0a2b82fa":"manual_words = ['genomic', 'scale']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx16,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids16 = parse_result(metadata, idx16, 'topic16')","0fd213fb":"manual_words = ['sequencing', 'bioinformatics']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx17,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids17 = parse_result(metadata, idx17, 'topic17')","d8631680":"manual_words = ['technology', 'unknown', 'naturally']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx18,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids18 = parse_result(metadata, idx18, 'topic18')","85410a78":"manual_words = ['spillover', 'pathogen']\nprint('Initial search words: ' + ', '.join(manual_words))\n_, w = get_similar_words(manual_words, glove, show_html=False)\nprint('Similar keywords: ', w)\nidx19,_ = text_search(w, metadata, 'abstract', limit=20)\n\nids19 = parse_result(metadata, idx19, 'topic19')","c78eef3d":"# combine all the articles of these 19 topics into a single dataframe\nall_idx = [idx1, idx2, idx3, idx4, idx5, idx6, idx7, idx8, idx9, idx10, idx11, idx12, idx13, idx14, idx15, idx16, idx17, idx18, idx19]\ndf_all = {}\nfor topic_id in range(1,20):\n    idx = all_idx[topic_id-1]\n    df_name = 'df_' + str(topic_id)\n    df_data = subtopic_to_df(metadata, idx, topic_id)\n    df_data['number of times appeared in topic search'] = 'topic' + str(topic_id)\n    df_all[df_name] = df_data\nselected_articles = pd.DataFrame(columns=[\"title\", \"pub_date\", \"authors\", \"sha\", \"number of times appeared in topic search\"])\nfor df in df_all:\n    selected_articles = selected_articles.append(df_all[df])\nselected_articles = selected_articles.reset_index(drop = True)\n#selected_articles.to_csv(\"selected articles.csv\",index = False) # \"optional\"","6f3119be":"overlap = pd.DataFrame(selected_articles.groupby(['title', \"authors\", \"pub_date\"])['number of times appeared in topic search'].count()).sort_values(by = ['number of times appeared in topic search'], ascending=False)\noverlap = overlap.reset_index()\noverlap.head(10)","2df4fee5":"selected_articles['pub_date'] = pd.to_datetime(selected_articles['pub_date'], format = \"%Y-%m-%d\")\nselected_articles['pub_year'] = selected_articles['pub_date'].dt.year\npublish_by_year = pd.DataFrame(selected_articles.groupby(['pub_year'])['sha'].nunique()).reset_index()\n\nplt.figure(figsize=(15,8))\nplt.plot(publish_by_year['pub_year'], publish_by_year['sha'])  \n\nplt.xlabel(\"Publish Year\", fontsize=12)\nplt.ylabel(\"Number of Articles\", fontsize=12)\nplt.title(\"Number of selected articles by publish year\", fontsize=15)","036bb081":"One article could appear in multiple keyword search results.\n\nFollowing are the top 3 articles where we got multiple keyword search results across topics:\n\n1. \"XXIV World Allergy Congress 2015: Seoul, Korea. 14-17 October 2015\" (https:\/\/doi.org\/10.1186\/s40413-016-0096-1) - Found across 16 topics\n2. \"36th International Symposium on Intensive Care and Emergency Medicine: Brussels, Belgium. 15-18 March 2016\" (https:\/\/doi.org\/10.1186\/s13054-016-1208-6) - Found across 16 topics\n3. \"Improving influenza vaccine virus selectionReport of a WHO informal consultation held at WHO headquarters, Geneva, Switzerland, 14\u00e2\u20ac\u201c16 June 2010\" (https:\/\/doi.org\/10.1126\/science.367.6478.606) - found across 6 topics\n\nAlso an interesting find here was \"China virus response criticized as slow\" article published on 2\/6\/2020, which turns up in 6 of our topic search and might be very relevant considering the pulish date being in 2020.","d6291acf":"#### Subtopic 2:\nEfforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.","fc1837e3":"#### Subtopic 17:\nEnhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.","3adab7ce":"Once the model has been trained, we can use it to retrieve words related to the same topic for a search. For example, if we look up the word 'detection', we see words like pcr, assays, and sensitive that the model learned to be relevant from the data:","00cf256c":"#### Subtopic 19:\nOne Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.","d22bee88":"Top 10 journals by number of articles","c66a360c":"#### Subtopic 10:\nUse of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions.","8c9bcc7e":"# EY team's submission for CORD-19\n\nThis notebook compiles the methods and results from our exploration of the CORD-19 dataset.\n\n### **Goal**\n\nOur team set out to tackle the task: [What do we know about diagnostics and surveillance?](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks?taskId=570) With the increasing volume of research published about COVID-19 each day, we thought it would be important to develop a method with which we could tag papers related to diagnostics and surveillance as they become available. Recognizing the Kaggle community's immense contributions to tasks like risk factors, incubation and transmission, as outlined in the [summary page](https:\/\/www.kaggle.com\/covid-19-contributions), and guided by our medical experts, we sought to combine the existing body of work leveraging NLP in conjunction with human inputs for the solution.\n\n### **Approach**\n\nMany custom-built search tools that have been created as part of the CORD-19 initiative (a number of them are listed [here](https:\/\/discourse.cord-19.semanticscholar.org\/t\/cord-19-demos-and-resources\/132)). Instead of building another search tool, we wanted to develop an approach that could help tag potential useful papers that are relevant to the search of keyword of interest, in order to help researchers discover additional information. \n\nOur approach consists of the following steps:\n1. Exploratory analysis to better understand the dataset and associated metadata\n2. Application of word embedding approach combined with human assessment to search and retrieve papers based on related keywords\n\n#### Benefits:\n* Using word embeddings to augment similar keyword search could expand the information retrieved by existing systems\n* This approach may uncover new relationships amongst keywords in the corpus\n\n#### Notes:\n* The model at its current state is not perfect; it still requires a lot of human judgement to determine whether a related search result is relevant\n* Keyword matching is simple. It may serve as a first filter to narrow down the content, after that additional methods can be applied for more targeted extraction \n\n\n### **About us**\nWe are a team of data professionals in the Digital Data and Analytics practice, part of EY's Advisory Services in the US, with diverse skillsets ranging from business intelligence, machine learning, to AI strategy. We help our clients navigate emerging technologies with a user-centric focus and drive performance improvement through digital transformation.\n![20170526-EY-Digital-Brand.svg](attachment:20170526-EY-Digital-Brand.svg)","b7727ae4":"\nLoad libraries","45f3d76e":"#### Subtopic 3:\nRecruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.","913f068a":"Create corpus from abstracts","fbf82009":"# Keyword search with word embeddings\n\nWord embeddings is a useful machine learning techniques for word analogy problems. Its general idea is to represent words in vector space based on their co-occurrences in a corpus, such that similar words may encode similar information in their vector representation. The particular embedding model we are using is called GloVe (Global Vectors for Word Representation), found [here](https:\/\/nlp.stanford.edu\/projects\/glove\/).\n\nWe started by training a GloVe word embedding model using the papers\u2019 abstracts, then manually generated a list of keywords given each subtopics under the surveillance and diagnostics task. Using the list of keyword as anchors, we generated more keywords from the corpus based on their embedding similarities. This combined list of keywords were then searched against the body of the papers, and the titles of the matching papers were returned.","486e24c0":"#### Subtopic 5:\nDevelopment of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.","e3405f0d":"1. Which papers appear in multiple subtopics?","a9a63b01":"Tokenizer helper functions","25863b21":"#### Subtopic 4:\nNational guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).","e4ed2a8d":"### Code and Results:","7a3b7aae":"# Search results for each subtopic \n\nIn the section below we went through each subtopic of the task and manually created a list of words to start the search. We then used the embedding model to generated a list of similar words and searched against the full text","e5103fb4":"Helper functions for data processing.\n\nCredit: https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\n","4bc1fead":"Word cloud from paper titles","2a6565df":"Most of the identified aritcles are published from 2018 to 2020.","8db7282b":"#### Subtopic 8:\nEfforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance\/detection schemes.","e02b80b7":"#### Subtopic 1: \nHow widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs).","c8944117":"Load saved model and dataset (optional)","ec106680":"#### Subtopic 18:\nEnhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.","82cd3780":"Train the GloVe model\n\nCredit:  https:\/\/medium.com\/analytics-vidhya\/word-vectorization-using-glove-76919685ee0b","3b965d54":"#### Subtopic 14:\nBarriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.","50daea7b":"#### Subtopic 15:\nNew platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.","78863cba":"With this, if for example we are searching for the specific word 'detection', however if a paper talks about the specifics of PCR without mentioning the word 'detection', we would still be able to retrieve that paper using the similarity search.","e7abbf3e":"#### Subtopic 9:\nLatency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.","550b6444":"#### Subtopic 7:\nSeparation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.","9ff14205":"2. For all the papers we found related to diagnostics and surveillance, when were they published? ","9c49f40c":"#### Subtopic 6:\nRapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded).","978dda69":"#### Subtopic 12:\nPolicies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.","2d743be5":"# Summary of the identified papers","4bfe9c35":"# Exploratory analysis\n\nWe started with the metadata to looked at the source of the papers and the journals they were published in","24c721b4":"#### Subtopic 13:\nTechnology roadmap for diagnostics.","81dbf19a":"Clean dataset and compile in one dataframe","58d632c1":"#### Subtopic 11:\nPolicies and protocols for screening and testing.","541f5174":"#### Subtopic 16:\nCoupling genomics and diagnostic testing on a large scale."}}