{"cell_type":{"fbf64721":"code","a956849a":"code","0dc530b1":"code","8e3b9a65":"code","bb7f2f7c":"code","4b4ac821":"code","4f99731a":"code","6edf2c26":"code","6cbb005b":"code","5c23eba7":"code","10ea816e":"code","4d224690":"code","d49ac991":"code","779f3a4b":"code","7dd8f773":"code","5d4af250":"code","ec3dbd88":"code","722d89e1":"code","110c9f40":"code","8d948ce1":"code","abdbb739":"code","eacc74b6":"code","9d3b1797":"code","cf3eaa0e":"markdown","b43d5d90":"markdown","bbdeee6c":"markdown","f81b8100":"markdown","4ecb58e8":"markdown","9d197117":"markdown","9430b6d8":"markdown","a7e780f4":"markdown","2e7f9102":"markdown"},"source":{"fbf64721":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a956849a":"import numpy as np \nimport pandas as pd\nimport os\nn=0\nm=0\nk=0\nenergy=[0]*1000\npca_Cmatrix=np.zeros((1000,20))\n#\u6587\u4ef6\u7684\u5e8f\u53f7\nfor k in range(0,1000):\n    try:\n        df=pd.read_csv('..\/input\/energy-prediction-of-three-dimensional-clusters\/Coordinates and energies of clusters\/Au20_OPT_1000\/'+str(k)+'.xyz',header=None,names=['Name','X','Y','Z'],delim_whitespace=True)\n        #\u4fdd\u5b58\u80fd\u91cf\u6570\u636e\n        energy[k]=float(df.iloc[1,:].values[0])\n        num=int(df.iloc[0,:].values[0])\n        #\u5220\u9664\u7b2c\u4e00\u884c\u3001\u7b2c\u4e8c\u884c\n        df.drop(index=[0,1],inplace=True)\n        df.reset_index(drop=True,inplace=True)\n        #\u5220\u9664\u5217\n        df.drop('Name',inplace=True,axis=1)\n        len=df.shape[0]\n        #\u6c42\u77e9\u9635\n        dis=df.iloc[0,:]-df.iloc[1,:]\n        np.sqrt(sum(dis**2))\n        Cmatrix=pd.DataFrame(columns=range(0,20),index=range(0,20))\n        for i in range(len):\n            for j in range(len):\n                if i!=j:\n                    dis=np.sqrt(sum((df.iloc[i,:]-df.iloc[j,:])**2))\n                    Cmatrix.iloc[i,j]=79*79\/(dis)\n                else:\n                    Cmatrix.iloc[i,j]=0.5*79**2.4\n        Cmatrix.astype(float)\n        array=Cmatrix.values\n        array=array.astype(float)\n        array\n        eig=np.linalg.eigvals(array)\n        pca_Cmatrix[k,:]=eig\n        \n    except IOError:\n        print('k\u4e0d\u5b58\u5728')","0dc530b1":"pca_Cmatrix\nnp.savetxt('pca_matrix',pca_Cmatrix)","8e3b9a65":"np.savetxt('energy',energy)","bb7f2f7c":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_predict\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score","4b4ac821":"\nX=pd.read_csv('.\/pca_matrix',header=None,delim_whitespace=True)\ny=pd.read_csv('.\/energy',header=None,delim_whitespace=True)","4f99731a":"y.columns=['Au']\ny.sort_values(by=['Au'],ascending=True)","6edf2c26":"X.drop(index=351,inplace=True,axis=0)\ny.drop(index=351,inplace=True,axis=0)\n\nX.drop(index=155,inplace=True,axis=0)\ny.drop(index=155,inplace=True,axis=0)\nX.reset_index(drop=True,inplace=True)\ny.reset_index(drop=True,inplace=True)","6cbb005b":"y","5c23eba7":"sns.displot(y,bins=15,kde=True,label = 'count')","10ea816e":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.4,random_state=1)\nX_cv,X_test,y_cv,y_test=train_test_split(X_test,y_test,test_size=0.5,random_state=1)\n\n\n\nstdx=StandardScaler()\nstdy=StandardScaler()\n\nX_train=stdx.fit_transform(X_train)\nX_cv=stdx.transform(X_cv)\nX_test=stdx.transform(X_test)\n\ny_train=stdy.fit_transform(y_train)\ny_cv=stdy.transform(y_cv)\ny_test=stdy.transform(y_test)","4d224690":"def evaluation_train_cv(model):\n    model.fit(X_train,y_train)\n    #\u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u7684\u9884\u6d4b\n    \n    y_train_pre=model.predict(X_train)\n    y_cv_pre=model.predict(X_cv)\n    \n#     y_train_pre=cross_val_predict(model,X_train,y_train,cv=5)\n#     y_cv_pre=cross_val_predict(model,X_cv,y_cv,cv=5)\n\n        \n    #\u8fd8\u539f\u8bad\u7ec3\u96c6\u9884\u6d4b\u503c\n    inverse_y_train_pre=stdy.inverse_transform(y_train_pre)\n    inverse_y_train=stdy.inverse_transform(y_train)\n    metric_list=[]\n    metric_list.append(mean_squared_error(y_train,y_train_pre))\n    metric_list.append(mean_absolute_error(y_train,y_train_pre))\n#     metric_list.append(r2_score(y_train,y_train_pre)) \n    \n    print()\n    print('-'*30)\n    print(f'The mse of {model} on the training set: {metric_list[0]}\\n MAE:{metric_list[1]}')\n\n    \n    plt.subplots(1,1,figsize=(16,5))\n    plt.plot(inverse_y_train,'-')\n    plt.plot(inverse_y_train_pre,'--')\n    ax = plt.gca()\n    ax.legend(['Training','Predicting'])\n    plt.title(f'The MSE,MAE of {model} on the training set is {metric_list[0:2]}')\n    \n    #\u8fd8\u539f\u6d4b\u8bd5\u96c6\u9884\u6d4b\u503c\n    inverse_y_cv_pre=stdy.inverse_transform(y_cv_pre)\n    inverse_y_cv=stdy.inverse_transform(y_cv)\n#     mse_test=mean_squared_error(y_test,y_test_pre)\n#     mae_test=mean_absolute_error(y_test,y_test_pre)\n#     r2_test=r2_score(y_test,y_test_pre)\n    metric_list.append(mean_squared_error(y_cv,y_cv_pre))\n    metric_list.append(mean_absolute_error(y_cv,y_cv_pre))\n#     metric_list.append(r2_score(y_cv,y_cv_pre)) \n    \n    print()\n    print('-'*30)\n\n    print(f'The mse of {model} on the CV set:{metric_list[2]}\\n MAE:{metric_list[3]}')\n    plt.subplots(1,1,figsize=(16,5))\n    plt.plot(inverse_y_cv,'-',label='CV')\n    plt.plot(inverse_y_cv_pre,'--',label='Predicting')\n    ax = plt.gca()\n    ax.legend(['CV','Predicting']) \n    plt.title(f'The MSE,MAE of {model} on the cv set is {metric_list[2:4]}')\n    return metric_list\n\n\n\ndef evaluation_test(model):\n        #\u8fd8\u539f\u6d4b\u8bd5\u96c6\u9884\u6d4b\u503c\n    metric_list=[]\n    \n    y_test_pre=model.predict(X_test)\n        \n#     y_test_pre=cross_val_predict(model,X_test,y_test,cv=5)\n        \n    inverse_y_test_pre=stdy.inverse_transform(y_test_pre)\n    inverse_y_test=stdy.inverse_transform(y_test)\n#     mse_test=mean_squared_error(y_test,y_test_pre)\n#     mae_test=mean_absolute_error(y_test,y_test_pre)\n#     r2_test=r2_score(y_test,y_test_pre)\n    metric_list.append(mean_squared_error(y_test,y_test_pre))\n    metric_list.append(mean_absolute_error(y_test,y_test_pre))\n    \n    print()\n    print('-'*30)\n\n    print(f'The MSE of {model} on the testing set: {metric_list[0]}\\n MAE{metric_list[1]}')\n    plt.subplots(1,1,figsize=(16,5))\n    plt.plot(inverse_y_test,'-')\n    plt.plot(inverse_y_test_pre,'--')\n    ax = plt.gca()\n    ax.legend(['Testing','Predicting']) \n    plt.title(f'The MSE,MAE of {model} on the testing set is {metric_list[0:3]}')\n    \n    return metric_list\n    ","d49ac991":"rfc=RandomForestRegressor(min_samples_leaf=1,min_samples_split=2,\n                           random_state=10)\nparams={'max_depth':[15,25,35,40],'n_estimators':[300,400,500,600]}\nrfc_grid=GridSearchCV(rfc,param_grid=params,cv=5,verbose=2)\nrfc_grid.fit(X_train,y_train)","779f3a4b":"rf_train=evaluation_train_cv(rfc_grid.best_estimator_)","7dd8f773":"rf_test=evaluation_test(rfc_grid.best_estimator_)","5d4af250":"from sklearn.svm import SVR\n#{'C': 9, 'gamma': 0.01}\nsvr = GridSearchCV(SVR(kernel='rbf'), cv=5,\n     param_grid={\"C\": [1e0,3,9,1e1],\"gamma\": np.logspace(-2, 2, 5)})\nsvr.fit(X_train,y_train)","ec3dbd88":"svr_train=evaluation_train_cv(svr.best_estimator_)","722d89e1":"svr_test=evaluation_test(svr.best_estimator_)","110c9f40":"from sklearn.neural_network import MLPRegressor\n\nnn = MLPRegressor(random_state=1, max_iter=100000)\nparam_grid = {'hidden_layer_sizes': [(i*20,)*i  for i in range(2,6)],\n                                     'alpha':np.logspace(-10,-3,8),\n              'activation':['logistic','RLU']}\nnn_grid=GridSearchCV(nn,param_grid,cv=5)\nnn_grid.fit(X_train,y_train)","8d948ce1":"nn_train=evaluation_train_cv(nn_grid.best_estimator_)","abdbb739":"nn_test=evaluation_test(nn_grid.best_estimator_)","eacc74b6":"rf_train\nrf_test\nsvr_train\nsvr_test\nnn_train\nnn_test\nmodel_mse_list=[rf_test[0],svr_test[0],nn_test[0]]\nmodel_mae_list=[rf_test[1],svr_test[1],nn_test[1]]\nsvr_test","9d3b1797":"fig,ax=plt.subplots(1,2,figsize=(10,5))\nmodel_name=['RandomForest','SVR','Neural Network']\n\nsns.barplot(model_name,model_mse_list,ax=ax[0])\n\n\nsns.barplot(model_name,model_mae_list,ax=ax[1])\nax[0].set_title('The MSE of model for the testing set')\nax[1].set_title('The MAE of model for the testing set')","cf3eaa0e":"So SVR has the lowest MSE and MAE, So SVR is the best model","b43d5d90":"# Loading Data","bbdeee6c":"# Model building","f81b8100":"# Normalization","4ecb58e8":"# Evaluation","9d197117":"#  Missing value processing","9430b6d8":"# #  RandomForest","a7e780f4":"# # SVM","2e7f9102":"# NN"}}