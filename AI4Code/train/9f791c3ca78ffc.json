{"cell_type":{"f04c4dfe":"code","a9185f59":"code","f10c5d01":"code","422a655a":"code","f09a05a3":"code","fe504237":"code","285ca233":"code","df9206de":"code","469d1fa3":"code","243b0218":"code","2d833d55":"code","f102fe70":"code","19377a6e":"code","52d7bfd6":"code","0379f5f8":"code","e64e4ce9":"code","8633515a":"code","0b7a657a":"markdown","47710bbf":"markdown","e9330f3f":"markdown","566c4b28":"markdown","2f174432":"markdown","a383fd4c":"markdown","bbb5aa4e":"markdown","5a09ae0e":"markdown"},"source":{"f04c4dfe":"import numpy as np\nimport numba as na\nfrom numba import cuda \n\nfrom tqdm import tqdm\n\nfrom keras.datasets import cifar10\nfrom IPython.display import clear_output, display\nimport matplotlib.pyplot as plt\nfrom time import sleep","a9185f59":"(X_train, y_train) , (X_test, y_test) = cifar10.load_data()","f10c5d01":"X_train = X_train.reshape(len(X_train), 3, 32, 32) \/ 255.0\nX_test = X_test.reshape(len(X_test), 3, 32, 32) \/ 255.0","422a655a":"y_train = np.eye(10)[y_train]\ny_test = np.eye(10)[y_test]","f09a05a3":"@na.jit(nopython=True)\ndef maxpool(x, f, s):\n    (l, w, w) = x.shape\n    pool = np.zeros((l, (w-f)\/\/s+1,(w-f)\/\/s+1))\n    for jj in range(0,l):\n        for i in range(0, w, s):\n            for j in range(0, w, s):\n                pool[jj,i\/\/2,j\/\/2] = np.max(x[jj,i:i+f,j:j+f])\n    return pool","fe504237":"@na.jit(nopython=True)\ndef relu(x):\n    return x * (x > 0)","285ca233":"@na.jit(nopython=True)\ndef softmax(x):\n    return np.exp(x) \/ np.sum(np.exp(x), axis=0)","df9206de":"@na.jit(nopython=True)\ndef Huber(yHat, y, delta=1.):\n    return np.where(np.abs(y-yHat) < delta,.5*(y-yHat)**2 , delta*(np.abs(y-yHat)-0.5*delta))","469d1fa3":"@na.jit(nopython=True)\ndef forward(image, theta):\n    f1, f2, b1, b2, b3, t3 = theta\n\n    (l, w, w) = image.shape\n    l1, l2 = len(f1), len(f2)\n    (_, f, f) = f1[0].shape\n\n    w1 = w - f + 1\n    w2 = w1 - f + 1\n    sizes = l, f, l1, l2,  w1, w2\n\n    conv1 = np.zeros((l1, w1, w1))\n    conv2 = np.zeros((l2, w2, w2))\n\n    for jj in range(0, l1):\n        for x in range(0, w1):\n            for y in range(0, w1):\n                conv1[jj, x, y] = np.sum(image[:,x:x+f,y:y+f] * f1[jj]) + b1[jj]\n    \n    conv1 = relu(conv1) \n\n    for jj in range(0, l2):\n        for x in range(0, w2):\n            for y in range(0, w2):\n                conv2[jj,x,y] = np.sum(conv1[:,x:x+f,y:y+f] * f2[jj]) + b2[jj]\n\n    conv2 = relu(conv2)\n\n    pooled_layer = maxpool(conv2, 2, 2)\t\n    fc1 = pooled_layer.reshape(((w2\/\/2)*(w2\/\/2)*l2,1))\n\n    out = softmax(t3.dot(fc1) + b3).T\n\n    gamma = conv1, conv2, fc1\n\n    return gamma, sizes, out","243b0218":"@na.jit(nopython=True)\ndef nanargmax(z):\n\n    a, b = 0, 0\n    max_nr = 0.0\n\n    for i in range(len(z)):\n        for j in range(len(z[0])):\n            if not np.isnan(z[i, j]):\n                if(z[i, j] > max_nr):\n                    max_nr = z[i, j]\n                    a, b = i, j\n    return (a, b)","2d833d55":"@na.jit(nopython=True)\ndef backward(image, label, gamma, sizes, theta):\n    f1, f2, b1, b2, b3, t3 = theta\n    l, f, l1, l2,  w1, w2 = sizes\n    conv1, conv2, fc1 = gamma\n\n    dout = out - label\n\n    dt3 = fc1.dot(dout).T\n    db3 = np.sum(dout.T, axis=1).reshape(10, 1)\n\n    dfc1 = dout.dot(t3).T\n    dpool = dfc1.T.reshape((l2, w2\/\/2, w2\/\/2))\n\n    dconv2 = np.zeros((l2, w2, w2))\n\n    for jj in range(0,l2):\n        for i in range(0, w2):\n            for j in range(0, w2):\n                (a,b) = nanargmax(conv2[jj,i:i+2,j:j+2])\n                dconv2[jj,i+a,j+b] = dpool[jj,i\/\/2,j\/\/2]\n\n    dconv2 = relu(dconv2)\n\n    dconv1 = np.zeros((l1, w1, w1))\n\n    df1 = np.zeros(((l1, l, f, f)))\n    db1 = np.zeros((l1))\n\n    df2 = np.zeros((l1, l2, f, f))\n    db2 = np.zeros((l2))\n\n    for jj in range(0, l2):\n        for x in range(0, w2):\n            for y in range(0, w2):\n                df2[jj] += dconv2[jj,x,y] * conv1[:,x:x+f,y:y+f]\n                dconv1[:,x:x+f,y:y+f] += dconv2[jj,x,y] * f2[jj]\n        db2[jj] = np.sum(dconv2[jj])\n\n    for jj in range(0, l1):\n        for x in range(0, w1):\n            for y in range(0, w1):\n                df1[jj] += dconv1[jj,x,y] * image[:,x:x+f,y:y+f]\n\n        db1[jj] = np.sum(dconv1[jj])\n\n    return df1, df2, db1, db2, db3, dt3","f102fe70":"def init_theta():\n\n    np.random.seed(234234232)\n\n    stddev = np.sqrt(1. \/ (5 * 5 * 3))\n\n    f1 = np.random.normal(loc=0, scale=stddev, size=(16, 3, 5, 5))\n    f2 = np.random.normal(loc=0, scale=stddev, size=(16, 16, 5, 5))\n    b1 = np.random.normal(loc=0, scale=stddev, size=(16))\n    b2 = np.random.normal(loc=0, scale=stddev, size=(16))\n    b3 = np.random.normal(loc=0, scale=stddev, size=(10, 1))\n    t3 = np.random.normal(loc=0, scale=stddev, size=(10, 2304))\n\n    return f1, f2, b1, b2, b3, t3","19377a6e":"def init_momentum(theta):\n    f1, f2, b1, b2, b3, t3 = theta\n\n    bv1 = np.zeros((b1.shape))\n    v1 = np.zeros((f1.shape))\n\n    bv2 = np.zeros((b2.shape))\n    v2 = np.zeros((f2.shape))\n\n    bv3 = np.zeros((b3.shape))\n    v3 = np.zeros((t3.shape))\n\n    return bv1, bv2, bv3, v1, v2, v3","52d7bfd6":"def init_grads(theta):\n    f1, f2, b1, b2, b3, t3 = theta\n\n    db1 = np.zeros((b1.shape))\n    df1 = np.zeros((f1.shape))\n\n    db2 = np.zeros((b2.shape))\n    df2 = np.zeros((f2.shape))\n\n    db3 = np.zeros((b3.shape))\n    dt3 = np.zeros((t3.shape))\n\n    return df1, df2, db1, db2, db3, dt3","0379f5f8":"# momentum\nMO = 0.05\n\n# parameters\nlr = 0.1\ndecay = 0.001\nbatch_size = 100","e64e4ce9":"theta = init_theta()\n\nlosses = []","8633515a":"for epoch in range(100):\n\n    # [0] df1, [1] df2, [2] db1, [3] db2, [4] db3, [5] dt3\n    temp_grads = init_grads(theta)\n\n    # init momentum properties\n    bv1, bv2, bv3, v1, v2, v3 = init_momentum(theta)\n\n    # clear output\n    clear_output(wait=True)\n\n    # use only the first 1000 elements in X_train\n    for batch, idx in enumerate(np.array_split(np.arange(1000), (len(np.arange(1000))\/100))):\n\n        all_grads = []\n        acc = 0\n\n        for i in idx:\n            gamma, sizes, out = forward(X_train[i], theta)\n            if np.argmax(out)==np.argmax(y_train[i]):\n                acc += 1\n            grads = backward(X_train[i], y_train[i], gamma, sizes, theta)\n            all_grads.append(grads)\n\n        f1, f2, b1, b2, b3, t3 = theta\n\n        all_grads = np.array(all_grads)[0]\n        temp_grads = tuple([temp_grads[i] + all_grads[i] for i in range(len(all_grads))])\n\n        # learning decay\n        # lr *= (1. \/ (1. + decay * (epoch + 1)))\n\n        for i in range(len(df1)):\n            v1[i] = MO * v1[i] - lr * temp_grads[0][i] \/ batch_size\n            f1[i] += v1[i]\n\n            v2[i] = MO * v2[i] - lr * temp_grads[1][i] \/ batch_size\n            f2[i] += v2[i]\n\n            bv1[i] = MO * bv1[i] -  lr * temp_grads[2][i] \/ batch_size\n            b1[i] += bv1[i]\n\n            bv2[i] = MO * bv2[i] -  lr * temp_grads[3][i] \/ batch_size\n            b2[i] += bv2[i]\n\n        bv3 = MO * bv3 - lr * temp_grads[4] \/ batch_size\n        b3 += bv3\n\n        v3 = MO * v3 - lr * temp_grads[5] \/ batch_size\n        t3 += v3\n\n        theta = f1, f2, b1, b2, b3, t3\n\n\n        nr = np.random.randint(low=0, high=1000, size=1)[0]\n        gamma, sizes, out = forward(X_train[nr], theta)\n        loss = np.mean(Huber(out,y_train[nr]))\n        losses.append(loss)\n        print('  ---- Epoch:{0:3d}  ---- Batch:{1:2.0f}\/{2:1.0f}  ---- Learning_rate:{3:1.4f}  ---- Acc:{4:3d}\/{5:3d}  ---- Loss:{6:3.5f}  ---- '\n                .format((epoch + 1), (batch + 1), (len(np.arange(1000)) \/ 100), lr, acc, len(idx), loss))","0b7a657a":"# Forward propagation","47710bbf":"# Initiliase parameters functions","e9330f3f":"# Prepare Data","566c4b28":"# Training","2f174432":"# Loss function","a383fd4c":"# CIFAR-10 Numba CNN from Scratch","bbb5aa4e":"# Backpropagation","5a09ae0e":"# Activation functions"}}