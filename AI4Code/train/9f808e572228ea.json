{"cell_type":{"ab3aaf3b":"code","a144f689":"code","499878bb":"code","37d4299d":"code","f3425dcd":"code","5982881e":"code","721b7c10":"code","2ab2074c":"code","2035e625":"code","a3559847":"code","0b170612":"code","72c8b2f5":"code","44b7a7ba":"code","f1d582de":"code","f3dab060":"code","bf531f84":"code","bb2eed47":"code","0a4aa34c":"code","31501591":"code","0586db9b":"code","0298d774":"code","7a78475b":"code","0cf875d5":"code","83607e2a":"code","441e3844":"code","d63d6293":"code","47825996":"code","b07f786d":"code","be3fd4bb":"code","feafcd92":"code","a2dbb74a":"code","3086fe0c":"code","de2b4778":"code","e486847a":"code","ab950b4d":"code","497a451d":"code","cbcc24ea":"code","a56b3ee8":"code","31705ce5":"code","034681ec":"code","9f4312b1":"code","ad265c53":"code","ce05a73e":"code","b565fa17":"code","b2659c87":"code","2f2a0242":"code","23983044":"code","1b10764f":"code","1d8bcba5":"code","f43459d3":"code","acda8d97":"code","a54994ee":"code","55a59427":"code","ad52079f":"code","c112b225":"code","9ad01aef":"code","6f209ab0":"code","1e6d2bd2":"code","aa633665":"code","ca7df807":"code","99b7e9b8":"code","93dd7b59":"code","c5a1ccff":"code","b4ef8ca6":"code","96f035b9":"code","202b330f":"code","d2b9963a":"code","e7c67bfb":"code","34e0cdbb":"code","b43bdffb":"code","8f054ab8":"code","13788b2f":"code","a5e9a9d6":"code","0040ab02":"code","4f0b9c92":"code","85abfa00":"code","906a5308":"code","e91f4755":"code","6f4166f3":"code","a764b507":"code","f32d8f51":"code","9e2db989":"code","9f28438b":"code","c1021cb0":"code","94798c90":"code","732d91f6":"code","5f452452":"code","78db8535":"code","d2d99af8":"code","958e98d6":"code","7f63dab9":"code","5ad5e46f":"code","d70cace5":"code","e05c27e5":"code","c2f20be4":"code","48c58fcd":"code","627e921c":"code","d16e616e":"code","9c24caba":"code","09e2dd72":"code","d5234204":"code","94b243cc":"code","4641fa9c":"code","5519ca86":"code","f91e8519":"code","5580e5b4":"code","41142bd2":"code","409e515a":"code","63505f99":"code","1b1f8b84":"code","c0bf101e":"code","38092835":"code","d16ddbe9":"code","f0f585c3":"code","ec8e55f7":"code","b186a215":"code","866ce6a2":"code","367a8ce0":"code","565d0dda":"code","7e94f750":"code","eed1f076":"code","3c238320":"code","fb80ff01":"code","8e150583":"code","4efdb833":"code","e6b9c01f":"code","e03667e2":"code","7ae9b106":"code","9836dd15":"code","4db86a81":"code","b250d6dd":"code","9be7b514":"code","efed0501":"code","6be53fff":"code","089432a2":"code","107e6e8b":"code","acc99c2c":"code","1577929f":"code","dee39a77":"code","fe9cfd90":"code","76208bcf":"code","2fb839be":"code","37955a3a":"code","31d39a93":"code","71c7b127":"markdown","ad533207":"markdown","6f22ca78":"markdown","af5ecd7b":"markdown","3a7d2a6b":"markdown","100b5eee":"markdown","dd1c7ca7":"markdown","778a3c5f":"markdown","94fcd2a1":"markdown","0bb422cc":"markdown","5b6cb2c5":"markdown","3725d68a":"markdown","74f4bf07":"markdown","c6213afc":"markdown","2f2e324e":"markdown","09673cd2":"markdown","a9c4a3b5":"markdown","023e28ed":"markdown","0d28e89f":"markdown","18936627":"markdown","28e1bf22":"markdown","ad208fbe":"markdown","1f222548":"markdown","3c41f565":"markdown","524cd0fc":"markdown","39bb5148":"markdown","1cc1b728":"markdown","b1948444":"markdown","35a90b02":"markdown","cd45e96e":"markdown","60e03d52":"markdown","18e9e46a":"markdown","b0901686":"markdown","cb0e3892":"markdown","ef7866f4":"markdown","4681e0b0":"markdown","bc7b858c":"markdown","57ebb54f":"markdown","40136c28":"markdown","a8684f7d":"markdown","ada7a1d2":"markdown","b2ecce76":"markdown","798d5069":"markdown","20dd3207":"markdown","67988df4":"markdown","9531321e":"markdown","3c913a1a":"markdown","0243b9e4":"markdown","83122208":"markdown","9bc607f8":"markdown","95bace71":"markdown","3efcc475":"markdown","0191aab6":"markdown","f5ec8621":"markdown","fc5157da":"markdown","7102beb3":"markdown","b38c097c":"markdown","9927ebb4":"markdown","e24c67ae":"markdown","769b6e39":"markdown","0b5bcd5b":"markdown","92e0eee4":"markdown","860ee0c8":"markdown","a572375c":"markdown","586e390f":"markdown","fc7fcf0b":"markdown","9868c335":"markdown","38930fdf":"markdown","4563c345":"markdown","75888c22":"markdown","82dcea04":"markdown","8b9a6ce4":"markdown","67658613":"markdown","bd46a954":"markdown","0a063b01":"markdown","25613630":"markdown","49138d5a":"markdown","5faf3cb4":"markdown","2d14a01f":"markdown","411eeb39":"markdown","bdd9e3a3":"markdown","e7ff25e5":"markdown"},"source":{"ab3aaf3b":"# Installing the modules.\n!pip install -q tensorflow==2.3\n!pip install albumentations -q\n!pip install pymorphy2\n!pip install pymorphy2-dicts","a144f689":"# Importing modules.\nimport random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport albumentations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pymorphy2\nimport os\nimport sys\nimport PIL\nimport cv2\nimport re\n\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom pylab import rcParams\nfrom itertools import combinations\nfrom scipy.stats import ttest_ind\n\nprint('Python       :', sys.version.split('\\n')[0])\nprint('Numpy        :', np.__version__)\nprint('Tensorflow   :', tf.__version__)","499878bb":"# Setting the conditions.\nrcParams['figure.figsize'] = 10, 5\n%config InlineBackend.figure_format = 'svg' \n%matplotlib inline\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\n\n!pip freeze > requirements.txt","37d4299d":"# Defining MAPE function.\ndef mape(y_true, y_pred):\n    return np.mean(np.abs((y_pred-y_true)\/y_true))","f3425dcd":"# Importing datasets.\nDATA_DIR = '..\/input\/sf-dst-car-price-prediction-part2\/'\ntrain = pd.read_csv(DATA_DIR + 'train.csv')\ntest = pd.read_csv(DATA_DIR + 'test.csv')\nsample_submission = pd.read_csv(DATA_DIR + 'sample_submission.csv')","5982881e":"# Preparing data for training.\ndata_train, data_test = train_test_split(\n    train,\n    test_size=0.15,\n    shuffle=True,\n    random_state=RANDOM_SEED\n)","721b7c10":"# Naive forecasting.\npredicts = []\n\nfor index, row in pd.DataFrame(\n    data_test[['model_info', 'productionDate']]\n).iterrows():\n    query = f\"model_info == '{row[0]}' and productionDate == '{row[1]}'\"\n    predicts.append(data_train.query(query)['price'].median())\n    \npredicts = pd.DataFrame(predicts)\npredicts = predicts.fillna(predicts.median())\npredicts = (predicts \/\/ 1000) * 1000\n\nprint(f\"MAPE for Naive Forecasting: {(mape(data_test['price'], predicts.values[:, 0]))*100:0.2f}%\")","2ab2074c":"# Checking the data.\ntrain.info()\ntrain.head()","2035e625":"# Checking the missing values.\ntrain[train['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'].isna()]","a3559847":"# Checking the frequency distribution.\ntrain[train['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b']=='2\\xa0\u0432\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u0430']['productionDate'].hist()","0b170612":"# Checking the frequency distribution.\ntrain[train['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b']=='3 \u0438\u043b\u0438 \u0431\u043e\u043b\u0435\u0435']['productionDate'].hist()","72c8b2f5":"# Filling the missing values.\ntrain.loc[train['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'].isna()==True, '\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'] = '3 \u0438\u043b\u0438 \u0431\u043e\u043b\u0435\u0435'","44b7a7ba":"# Checking the data.\ntest.info()\ntest.head()","f1d582de":"# Checking for duplicates.\ntrain.duplicated().value_counts()","f3dab060":"# Checking for duplicates.\ntest.duplicated().value_counts()","bf531f84":"# Merging the data.\ntrain['sample'] = 1\ntest['sample'] = 0\ntest['price'] = 0\n\ndata = test.append(train, sort=False).reset_index(drop=True)","bb2eed47":"# Checking the data.\ndata.info()\ndata.head()","0a4aa34c":"# Checking the data.\ndata['bodyType'].value_counts()","31501591":"# Checking the data.\ndata['brand'].value_counts()","0586db9b":"# Checking the data.\ndata['color'].value_counts()","0298d774":"# Checking the data.\ndata['description'][0]","7a78475b":"# Checking the data.\ndata['engineDisplacement'].value_counts()","0cf875d5":"# Checking the data.\ndata[data['engineDisplacement'] == 'undefined LTR']","83607e2a":"# Defining boxplot function.\ndef get_boxplot(column):\n    fig, ax = plt.subplots(figsize = (12, 6))\n    sns.boxplot(x=column, \n                y='price',\n                data=train.loc[\n                    train.loc[:, column].isin(train.loc[:, column].value_counts().index)\n                ],\n                ax=ax)\n    plt.xticks(rotation=90)\n    ax.set_title('Boxplot for ' + column)\n    plt.show()","441e3844":"# Checking the frequency distribution.\nget_boxplot('engineDisplacement')","d63d6293":"# Modifying the data.\ndata.loc[data['engineDisplacement'] == 'undefined LTR', 'engineDisplacement'] = '1.3 LTR'","47825996":"# Modifying the data.\ndata['engineDisplacement'] = data['engineDisplacement'].str.split().apply(\n    lambda s: s[0]\n)\n\ndata['engineDisplacement'] = data['engineDisplacement'].apply(\n    lambda s: float(s)\n)\n\ndata['engineDisplacement'] = data['engineDisplacement']*1000","b07f786d":"# Checking the frequency distribution.\ndata['engineDisplacement'].hist()","be3fd4bb":"# Checking the frequency distribution.\nnp.log(data['engineDisplacement']).hist()","feafcd92":"# Checking the data.\ndata['enginePower'].value_counts()","a2dbb74a":"# Modifying the data.\ndata['enginePower'] = data['enginePower'].str.split().apply(\n    lambda s: s[0]\n)\n\ndata['enginePower'] = data['enginePower'].apply(\n    lambda s: float(s)\n)","3086fe0c":"# Checking the frequency distribution.\ndata['enginePower'].hist(bins=50)","de2b4778":"# Checking the frequency distribution.\nnp.log(data['enginePower']).hist(bins=50)","e486847a":"# Checking the data.\ndata['fuelType'].value_counts()","ab950b4d":"# Checking the frequency distribution.\ndata['mileage'].hist(bins=100)","497a451d":"# Checking the frequency distribution.\nnp.log(data['mileage']).hist(bins=100)","cbcc24ea":"# Checking the frequency distribution.\ndata['modelDate'].hist()","a56b3ee8":"# Modifying the data.\ndata['modelDate'] = 2021 - data['modelDate']","31705ce5":"# Checking the frequency distribution.\nnp.log(data['modelDate']).hist()","034681ec":"# Checking the data.\ndata['model_info'].value_counts()","9f4312b1":"# Checking the data.\ndata[data['model_info'] == '100']['description']","ad265c53":"# Checking the data.\ndata[data['model_info'] == 'COUPE']['description']","ce05a73e":"# Checking the data.\ndata[data['model_info'] == 'None']","b565fa17":"# Modifying the data.\ndata.loc[data['model_info'] == '100', 'model_info'] = 'S4'\ndata.loc[data['model_info'] == 'COUPE', 'model_info'] = 'S2'\ndata.loc[data['model_info'] == 'None', 'model_info'] = 'C_KLASSE_AMG'","b2659c87":"# Checking the data.\ndata['name']","2f2a0242":"# Checking the frequency distribution.\ndata['numberOfDoors'].hist()","23983044":"# Checking the frequency distribution.\ndata['productionDate'].hist()","1b10764f":"# Modifying the data.\ndata['productionDate'] = 2021 - data['productionDate']","1d8bcba5":"# Checking the frequency distribution.\nnp.log(data['productionDate']).hist()","f43459d3":"# Checking the data.\ndata['sell_id']","acda8d97":"# Checking the data.\ndata['vehicleConfiguration']","a54994ee":"# Checking the frequency distribution.\ndata['vehicleTransmission'].hist()","55a59427":"# Checking the frequency distribution.\ndata['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'].hist()","ad52079f":"# Encoding a categorical variable.\nowners_dict = {\n    '1\\xa0\u0432\u043b\u0430\u0434\u0435\u043b\u0435\u0446': 1,\n    '2\\xa0\u0432\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u0430': 2,\n    '3 \u0438\u043b\u0438 \u0431\u043e\u043b\u0435\u0435': 3\n}\n\ndata['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'] = data['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'].map(owners_dict)","c112b225":"# Checking the data.\ndata['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'].value_counts()","9ad01aef":"# Checking the data.\ndata['\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435'].value_counts(dropna=False)","6f209ab0":"# Checking the frequency distribution.\ndata['\u041f\u0422\u0421'].hist()","1e6d2bd2":"# Encoding a categorical variable.\npassport_dict = {\n    '\u0414\u0443\u0431\u043b\u0438\u043a\u0430\u0442': 0,\n    '\u041e\u0440\u0438\u0433\u0438\u043d\u0430\u043b': 1,\n}\n\ndata['\u041f\u0422\u0421'] = data['\u041f\u0422\u0421'].map(passport_dict)","aa633665":"# Checking the data.\ndata['\u041f\u0422\u0421'].value_counts()","ca7df807":"# Checking the frequency distribution.\ndata['\u041f\u0440\u0438\u0432\u043e\u0434'].hist()","99b7e9b8":"# Checking the data.\ndata['\u0420\u0443\u043b\u044c'].value_counts()","93dd7b59":"# Checking the data.\ndata[data['\u0420\u0443\u043b\u044c'] == '\u041f\u0440\u0430\u0432\u044b\u0439']['sample']","c5a1ccff":"# Checking the frequency distribution.\ntrain['price'].hist(bins=100)","b4ef8ca6":"# Creating new features.\npopular_colors = [\n    '\u0447\u0451\u0440\u043d\u044b\u0439', \n    '\u0431\u0435\u043b\u044b\u0439', \n    '\u0441\u0435\u0440\u044b\u0439', \n    '\u0441\u0438\u043d\u0438\u0439'\n]\n\nrare_colors = [\n    '\u0441\u0435\u0440\u0435\u0431\u0440\u0438\u0441\u0442\u044b\u0439',\n    '\u043a\u043e\u0440\u0438\u0447\u043d\u0435\u0432\u044b\u0439',\n    '\u043a\u0440\u0430\u0441\u043d\u044b\u0439'\n]\n\ndata['popular_color'] = 0\ndata['rare_color'] = 0\ndata['very_rare_color'] = 0","96f035b9":"# Creating new features.\ncounter = 0\n\nfor color in data['color']:\n    if color in popular_colors:\n        data.at[counter,'popular_color'] = 1\n        counter += 1\n    elif color in rare_colors:\n        data.at[counter,'rare_color'] = 1\n        counter += 1\n    else:\n        data.at[counter,'very_rare_color'] = 1\n        counter += 1","202b330f":"# Creating new feature.\ndata['mileage_to_age'] = data['mileage']\/data['productionDate']","d2b9963a":"# Checking the frequency distribution.\ndata['mileage_to_age'].hist(bins=100)","e7c67bfb":"# Creating new features.\ndata['low_MtA'] = 0\ndata['high_MtA'] = 0","34e0cdbb":"# Creating new features.\ncounter = 0\n\nfor MtA in data['mileage_to_age']:\n    if MtA < 5000:\n        data.at[counter,'low_MtA'] = 1\n        counter += 1\n    elif MtA > 30000:\n        data.at[counter,'high_MtA'] = 1\n        counter += 1\n    else:\n        counter += 1","b43bdffb":"# Checking the data.\ndata.info()\ndata.head()","8f054ab8":"# Checking the data.\ndata.loc[1]","13788b2f":"# Creating lists of categories.\n\ncategorical_features = [\n    'bodyType', \n    'brand', \n    'color',\n    'fuelType', \n    'model_info',\n    'vehicleTransmission', \n    '\u041f\u0440\u0438\u0432\u043e\u0434'\n]\n\nnumerical_features = [\n    'engineDisplacement',\n    'enginePower',\n    'mileage', \n    'modelDate', \n    'productionDate',\n    'numberOfDoors',\n    '\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b',\n    'mileage_to_age'\n]\n\nbinary_features = [\n    'popular_color',\n    'rare_color'\n    'very_rare_color', \n    'low_MtA', \n    'high_MtA',\n    '\u041f\u0422\u0421'\n]","a5e9a9d6":"# Checking the correlation matrix.\nplt.subplots(figsize=(8,6))\nsns.heatmap(data.corr().abs(), vmin=0, vmax=1)","0040ab02":"# Checking the correlation matrix.\ndata.corr().abs().sort_values(by='price', ascending=False)","4f0b9c92":"# Defining boxplot function.\ndef get_boxplot(column):\n    fig, ax = plt.subplots(figsize = (10, 14))\n    sns.boxplot(x=column, \n                y='price',\n                data=train.loc[\n                    train.loc[:, column].isin(train.loc[:, column].value_counts().index[:10])\n                ],\n                ax=ax)\n    plt.xticks(rotation=45)\n    ax.set_title('Boxplot for ' + column)\n    plt.show()","85abfa00":"# Checking the frequency distribution.\nfor col in categorical_features:\n    get_boxplot(col)","906a5308":"# Defining Student's test function.\ndef get_stat_dif(column):\n    cols = train.loc[:, column].value_counts().index[:10]\n    combinations_all = list(combinations(cols, 2))\n    for comb in combinations_all:\n        if ttest_ind(train.loc[train.loc[:, column] == comb[0], 'price'], \n                        train.loc[train.loc[:, column] == comb[1], 'price']).pvalue \\\n            <= 0.05\/len(combinations_all):\n            print('Statistically significant differences were found for the column', column)\n            break","e91f4755":"# Using the Student's test.\nfor col in categorical_features:\n    get_stat_dif(col)","6f4166f3":"# Creating lists of categories.\n\ncategorical_features = [\n    'bodyType', \n    'brand', \n    'color',\n    'fuelType', \n    'model_info',\n    'vehicleTransmission', \n    '\u041f\u0440\u0438\u0432\u043e\u0434'\n]\n\nnumerical_features = [\n    'engineDisplacement',\n    'enginePower',\n    'mileage', \n    'modelDate', \n    'productionDate',\n    'numberOfDoors',\n    '\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b',\n    'mileage_to_age'\n]\n\nbinary_features = [\n    'popular_color',\n    'rare_color'\n    'very_rare_color', \n    'low_MtA', \n    'high_MtA',\n    '\u041f\u0422\u0421'\n]","a764b507":"# Defining the preprocessing function.\ndef preproc_data(df_input):\n\n    # Copying the data.\n    df_output = df_input.copy()\n    \n    # Dropping useless data.\n    df_output.drop([\n        'description',\n        'name',\n        'sell_id',\n        'vehicleConfiguration',\n        '\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435',\n        '\u0420\u0443\u043b\u044c'\n    ], axis = 1, inplace=True)\n    \n    # Taking the logarithm.\n    data['enginePower'] = np.log(data['enginePower'])\n    data['mileage'] = np.log(data['mileage'])\n    data['engineDisplacement'] = np.log(data['enginePower'])\n    data['modelDate'] = np.log(data['mileage'])\n    data['productionDate'] = np.log(data['mileage'])\n    \n    # Data normalization.\n    scaler = MinMaxScaler()\n    for column in numerical_features:\n        df_output[column] = scaler.fit_transform(df_output[[column]])[:,0]\n    \n    # One-hot encoding.\n    for column in categorical_features:\n        df_output[column] = df_output[column].astype('category').cat.codes\n        \n    df_output = pd.get_dummies(\n        df_output, \n        columns=categorical_features, \n        dummy_na=False\n    )\n    \n    return df_output","f32d8f51":"# Preprocessing the data\ndf_preproc = preproc_data(data)","9e2db989":"# Checking the data.\ndf_preproc.info()\ndf_preproc.head()","9f28438b":"# Preparing data for training.\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.price.values\nX = train_data.drop(['price'], axis=1)\nX_sub = test_data.drop(['price'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.15, shuffle=True, random_state=RANDOM_SEED\n)","c1021cb0":"# CatBoost training.\nmodel = CatBoostRegressor(\n    iterations=8000,\n    random_seed=RANDOM_SEED,\n    eval_metric='MAPE',\n    custom_metric=['RMSE', 'MAE'],\n    od_wait=500\n)\n                          \nmodel.fit(\n    X_train,\n    y_train,\n    eval_set=(X_test, y_test),\n    verbose_eval=100,\n    use_best_model=True\n)\n\ntest_predict_catboost = model.predict(X_test)\nprint(f\"TEST mape: {(mape(y_test, test_predict_catboost))*100:0.2f}%\")","94798c90":"# Submitting the results.\nsub_predict_catboost = model.predict(X_sub)\nsample_submission['price'] = sub_predict_catboost\nsample_submission.to_csv('catboost_submission.csv', index=False)","732d91f6":"# Checking the data.\nX_train.head()","5f452452":"# Creating a neural network.\nmodel = Sequential()\n\nmodel.add(L.Dense(\n    512, \n    input_dim=X_train.shape[1], \n    kernel_regularizer=regularizers.l1_l2(\n        l1=0.000000001, \n        l2=0.000000001\n    ), \n    activation=\"relu\"\n))\n\nmodel.add(L.Dropout(0.5))\n\nmodel.add(L.Dense(\n    256,\n    kernel_regularizer=regularizers.l1_l2(\n        l1=0.000000001, \n        l2=0.000000001\n    ), \n    activation=\"relu\"\n))\n\nmodel.add(L.Dropout(0.5))\n\nmodel.add(L.Dense(1, activation=\"linear\"))","78db8535":"# Checking the model.\nmodel.summary()","d2d99af8":"# Compiling the model.\noptimizer = tf.keras.optimizers.Adam(0.01)\n\nmodel.compile(\n    loss='MAPE', \n    optimizer=optimizer, \n    metrics=['MAPE']\n)","958e98d6":"# Setting callbacks.\ncheckpoint = ModelCheckpoint(\n    '..\/working\/best_model.hdf5', \n    monitor=['val_MAPE'], \n    verbose=0, \n    mode='min'\n)\n\nearlystop = EarlyStopping(\n    monitor='val_MAPE', \n    patience=50, \n    restore_best_weights=True\n)\n\ncallbacks_list = [checkpoint, earlystop]","7f63dab9":"# Tabular NN training.\nhistory = model.fit(\n    X_train,\n    y_train,\n    batch_size=512,\n    epochs=500,\n    validation_data=(X_test, y_test),\n    callbacks=callbacks_list,\n    verbose=0\n)","5ad5e46f":"# Checking the plots.\nplt.title('Loss')\nplt.plot(history.history['MAPE'], label='train')\nplt.plot(history.history['val_MAPE'], label='test')\nplt.show();","d70cace5":"# Saving model + Loading best weights.\nmodel.load_weights('..\/working\/best_model.hdf5')\nmodel.save('..\/working\/nn_1.hdf5')","e05c27e5":"# Checking the MAPE.\ntest_predict_nn1 = model.predict(X_test)\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn1[:,0]))*100:0.2f}%\")","c2f20be4":"# Submitting the results.\nsub_predict_nn1 = model.predict(X_sub)\nsample_submission['price'] = sub_predict_nn1[:,0]\nsample_submission.to_csv('nn1_submission.csv', index=False)","48c58fcd":"# Checking the data.\ndata['description']","627e921c":"# Defining the lemmatization function.\nmorph = pymorphy2.MorphAnalyzer()\ndf_NLP = data.copy()\n\npatterns = \"[A-Za-z0-9!#$%&'()*+,.\/:;<=>?@[\\]^_`{|}~\u2014\\\"\\-]+\"\n\ndef lemmatize(doc):\n    doc = re.sub(patterns, ' ', doc)\n    tokens = []\n    for token in doc.split():\n        token = token.strip()\n        token = morph.normal_forms(token)[0]\n        tokens.append(token)\n    return ' '.join(tokens)","d16e616e":"# Applying lemmatization.\ndf_NLP['description'] = df_NLP.apply(\n    lambda df_NLP: lemmatize(df_NLP.description), axis=1)","9c24caba":"# Splitting the sample.\ntext_train = data.description.iloc[X_train.index]\ntext_test = data.description.iloc[X_test.index]\ntext_sub = data.description.iloc[X_sub.index]","09e2dd72":"# Preparing data for training.\nMAX_WORDS = 100000\nMAX_SEQUENCE_LENGTH = 256\n\ntokenize = Tokenizer(num_words=MAX_WORDS)\ntokenize.fit_on_texts(data.description)","d5234204":"# Preparing data for training.\ntext_train_sequences = sequence.pad_sequences(\n    tokenize.texts_to_sequences(text_train), \n    maxlen=MAX_SEQUENCE_LENGTH\n)\n\ntext_test_sequences = sequence.pad_sequences(\n    tokenize.texts_to_sequences(text_test), \n    maxlen=MAX_SEQUENCE_LENGTH\n)\n\ntext_sub_sequences = sequence.pad_sequences(\n    tokenize.texts_to_sequences(text_sub), \n    maxlen=MAX_SEQUENCE_LENGTH\n)\n\nprint(\n    text_train_sequences.shape, \n    text_test_sequences.shape, \n    text_sub_sequences.shape\n)","94b243cc":"# Checking the data.\nprint(text_train.iloc[6])\nprint(text_train_sequences[6])","4641fa9c":"# Creating a RNN NLP.\nmodel_nlp = Sequential()\nmodel_nlp.add(L.Input(shape=MAX_SEQUENCE_LENGTH, name=\"seq_description\"))\nmodel_nlp.add(L.Embedding(len(tokenize.word_index)+1, MAX_SEQUENCE_LENGTH,))\nmodel_nlp.add(L.LSTM(256, return_sequences=True))\nmodel_nlp.add(L.Dropout(0.5))\nmodel_nlp.add(L.LSTM(128,))\nmodel_nlp.add(L.Dropout(0.25))\nmodel_nlp.add(L.Dense(64, activation=\"relu\"))\nmodel_nlp.add(L.Dropout(0.25))","5519ca86":"# Creating a MLP.\nmodel_mlp = Sequential()\nmodel_mlp.add(L.Dense(512, input_dim=X_train.shape[1], activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))\nmodel_mlp.add(L.Dense(256, activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))","f91e8519":"# Combining the networks and head installing.\ncombinedInput = L.concatenate([model_nlp.output, model_mlp.output])\nhead = L.Dense(64, activation=\"relu\")(combinedInput)\nhead = L.Dense(1, activation=\"linear\")(head)\nmodel = Model(inputs=[model_nlp.input, model_mlp.input], outputs=head)","5580e5b4":"# Checking the model.\nmodel.summary()","41142bd2":"# Compiling the model.\noptimizer = tf.keras.optimizers.Adam(0.01)\n\nmodel.compile(\n    loss='MAPE',\n    optimizer=optimizer, \n    metrics=['MAPE']\n)","409e515a":"# Setting callbacks.\ncheckpoint = ModelCheckpoint(\n    '..\/working\/best_model.hdf5', \n    monitor=['val_MAPE'], \n    verbose=0, \n    mode='min'\n)\n\nearlystop = EarlyStopping(\n    monitor='val_MAPE', \n    patience=10, \n    restore_best_weights=True\n)\n\ncallbacks_list = [checkpoint, earlystop]","63505f99":"# Multiple input model training.\nhistory = model.fit(\n    [text_train_sequences, X_train],\n    y_train,\n    batch_size=512,\n    epochs=500,\n    validation_data=([text_test_sequences, X_test], y_test),\n    callbacks=callbacks_list\n)","1b1f8b84":"# Checking the plots.\nplt.title('Loss')\nplt.plot(history.history['MAPE'], label='train')\nplt.plot(history.history['val_MAPE'], label='test')\nplt.show();","c0bf101e":"# Saving model + Loading best weights.\nmodel.load_weights('..\/working\/best_model.hdf5')\nmodel.save('..\/working\/nn_mlp_nlp.hdf5')","38092835":"# Checking the MAPE.\ntest_predict_nn2 = model.predict([text_test_sequences, X_test])\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn2[:,0]))*100:0.2f}%\")","d16ddbe9":"# Submitting the results.\nsub_predict_nn2 = model.predict([text_sub_sequences, X_sub])\nsample_submission['price'] = sub_predict_nn2[:,0]\nsample_submission.to_csv('nn2_submission.csv', index=False)","f0f585c3":"# Checking the images.\nplt.figure(figsize = (12,8))\n\nrandom_image = train.sample(n = 9)\nrandom_image_paths = random_image['sell_id'].values\nrandom_image_cat = random_image['price'].values\n\nfor index, path in enumerate(random_image_paths):\n    im = PIL.Image.open(DATA_DIR + 'img\/img\/' + str(path) + '.jpg')\n    plt.subplot(3, 3, index + 1)\n    plt.imshow(im)\n    plt.title('price: ' + str(random_image_cat[index]))\n    plt.axis('off')\nplt.show()","ec8e55f7":"# Defining image array creation function.\nsize = (320, 240)\n\ndef get_image_array(index):\n    images_train = []\n    for index, sell_id in enumerate(data['sell_id'].iloc[index].values):\n        image = cv2.imread(DATA_DIR + 'img\/img\/' + str(sell_id) + '.jpg')\n        assert(image is not None)\n        image = cv2.resize(image, size)\n        images_train.append(image)\n    images_train = np.array(images_train)\n    print('images shape', images_train.shape, 'dtype', images_train.dtype)\n    return(images_train)\n\nimages_train = get_image_array(X_train.index)\nimages_test = get_image_array(X_test.index)\nimages_sub = get_image_array(X_sub.index)","b186a215":"# Importing modules.\nfrom albumentations import (\n    HorizontalFlip, \n    IAAPerspective, \n    ShiftScaleRotate, \n    CLAHE, \n    RandomRotate90,\n    Transpose,\n    ShiftScaleRotate, \n    Blur, \n    OpticalDistortion, \n    GridDistortion, \n    HueSaturationValue,\n    IAAAdditiveGaussianNoise, \n    GaussNoise, \n    MotionBlur, \n    MedianBlur, \n    IAAPiecewiseAffine,\n    IAASharpen, \n    IAAEmboss, \n    RandomBrightnessContrast, \n    Flip, \n    OneOf, \n    Compose\n)","866ce6a2":"# Setting the conditions for augmentation.\naugmentation = Compose([\n    HorizontalFlip(),\n    OneOf([\n        IAAAdditiveGaussianNoise(),\n        GaussNoise(),\n    ], p=0.2),\n    OneOf([\n        MotionBlur(p=0.2),\n        MedianBlur(blur_limit=3, p=0.1),\n        Blur(blur_limit=3, p=0.1),\n    ], p=0.2),\n    ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=1),\n    OneOf([\n        OpticalDistortion(p=0.3),\n        GridDistortion(p=0.1),\n        IAAPiecewiseAffine(p=0.3),\n    ], p=0.2),\n    OneOf([\n        CLAHE(clip_limit=2),\n        IAASharpen(),\n        IAAEmboss(),\n        RandomBrightnessContrast(),\n    ], p=0.3),\n    HueSaturationValue(p=0.3),\n], p=1)","367a8ce0":"# Checking the images.\nplt.figure(figsize = (12,8))\nfor i in range(9):\n    img = augmentation(image = images_train[0])['image']\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(img)\n    plt.axis('off')\nplt.show()","565d0dda":"# Defining augmentation function.\ndef make_augmentations(images):\n    augmented_images = np.empty(images.shape)\n    for i in range(images.shape[0]):\n        if i % 200 == 0:\n            print('.', end = '')\n        augment_dict = augmentation(image = images[i])\n        augmented_image = augment_dict['image']\n        augmented_images[i] = augmented_image\n    print('')\n    return augmented_images","7e94f750":"# NLP model part.\ntokenize = Tokenizer(num_words=MAX_WORDS)\ntokenize.fit_on_texts(data.description)","eed1f076":"# Defining NLP and image NN functions.\ndef process_image(image):\n    return augmentation(image = image.numpy())['image']\n\ndef tokenize_(descriptions):\n    return sequence.pad_sequences(\n        tokenize.texts_to_sequences(descriptions), \n        maxlen = MAX_SEQUENCE_LENGTH\n    )\n\ndef tokenize_text(text):\n    return tokenize_([text.numpy().decode('utf-8')])[0]\n\ndef tf_process_train_dataset_element(image, table_data, text, price):\n    im_shape = image.shape\n    [image,] = tf.py_function(process_image, [image], [tf.uint8])\n    image.set_shape(im_shape)\n    [text,] = tf.py_function(tokenize_text, [text], [tf.int32])\n    return (image, table_data, text), price\n\ndef tf_process_val_dataset_element(image, table_data, text, price):\n    [text,] = tf.py_function(tokenize_text, [text], [tf.int32])\n    return (image, table_data, text), price","3c238320":"# Preparing data for training.\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    images_train, \n    X_train, \n    data.description.iloc[X_train.index], \n    y_train\n)).map(tf_process_train_dataset_element)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    images_test, \n    X_test, \n    data.description.iloc[X_test.index], \n    y_test\n)).map(tf_process_val_dataset_element)\n\ny_sub = np.zeros(len(X_sub))\nsub_dataset = tf.data.Dataset.from_tensor_slices((\n    images_sub, \n    X_sub, \n    data.description.iloc[X_sub.index], \n    y_sub\n)).map(tf_process_val_dataset_element)","fb80ff01":"# Checking for errors.\ntrain_dataset.__iter__().__next__();\ntest_dataset.__iter__().__next__();\nsub_dataset.__iter__().__next__();","8e150583":"# Loading the model.\nefficientnet_model = tf.keras.applications.efficientnet.EfficientNetB3(\n    weights = 'imagenet', \n    include_top = False, \n    input_shape = (size[1], size[0], 3)\n)\n\nefficientnet_output = L.GlobalAveragePooling2D()(\n    efficientnet_model.output\n)","4efdb833":"# Fine-tuning.\nfine_tune_at = len(efficientnet_model.layers)\/\/2\n\nfor layer in efficientnet_model.layers[:fine_tune_at]:\n    layer.trainable =  False","e6b9c01f":"# Creating a tabular NN.\ntabular_model = Sequential([\n    L.Input(shape = X.shape[1]),\n    L.Dense(512, activation = 'relu'),\n    L.Dropout(0.5),\n    L.Dense(256, activation = 'relu'),\n    L.Dropout(0.5),\n    ])","e03667e2":"# Creating a NLP NN.\nnlp_model = Sequential([\n    L.Input(shape=MAX_SEQUENCE_LENGTH, name=\"seq_description\"),\n    L.Embedding(len(tokenize.word_index)+1, MAX_SEQUENCE_LENGTH),\n    L.LSTM(256, return_sequences=True),\n    L.Dropout(0.5),\n    L.LSTM(128),\n    L.Dropout(0.25),\n    L.Dense(64),\n    ])","7ae9b106":"# Combining the networks and head installing.\ncombinedInput = L.concatenate([\n    efficientnet_output, \n    tabular_model.output, \n    nlp_model.output\n])\n\nhead = L.Dense(256, activation=\"relu\")(combinedInput)\nhead = L.Dense(1,)(head)\n\nmodel = Model(inputs=[\n    efficientnet_model.input, \n    tabular_model.input, \n    nlp_model.input\n], outputs=head)","9836dd15":"# Compiling the model.\noptimizer = tf.keras.optimizers.Adam(0.005)\n\nmodel.compile(\n    loss='MAPE',\n    optimizer=optimizer,\n    metrics=['MAPE']\n)","4db86a81":"# Setting callbacks.\ncheckpoint = ModelCheckpoint(\n    '..\/working\/best_model.hdf5', \n    monitor=['val_MAPE'], \n    verbose=0, \n    mode='min'\n)\n\nearlystop = EarlyStopping(\n    monitor='val_MAPE', \n    patience=10, \n    restore_best_weights=True\n)\n\ncallbacks_list = [checkpoint, earlystop]","b250d6dd":"# Multiple input model training.\nhistory = model.fit(\n    train_dataset.batch(30),\n    epochs=100,\n    validation_data=test_dataset.batch(30),\n    callbacks=callbacks_list\n)","9be7b514":"# Checking the plots.\nplt.title('Loss')\nplt.plot(history.history['MAPE'], label='train')\nplt.plot(history.history['val_MAPE'], label='test')\nplt.show();","efed0501":"# Saving model + Loading best weights.\nmodel.load_weights('..\/working\/best_model.hdf5')\nmodel.save('..\/working\/nn_final.hdf5')","6be53fff":"# Checking the MAPE.\ntest_predict_nn3 = model.predict(test_dataset.batch(30))\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn3[:,0]))*100:0.2f}%\")","089432a2":"# Submitting the results.\nsub_predict_nn3 = model.predict(sub_dataset.batch(30))\nsample_submission['price'] = sub_predict_nn3[:,0]\nsample_submission.to_csv('nn3_submission.csv', index=False)","107e6e8b":"# Checking the MAPE.\nblend_predict = (test_predict_catboost + test_predict_nn3[:,0]) \/ 2\nprint(f\"TEST mape: {(mape(y_test, blend_predict))*100:0.2f}%\")","acc99c2c":"# Submitting the results.\nblend_sub_predict = (sub_predict_catboost + sub_predict_nn3[:,0]) \/ 2\nsample_submission['price'] = blend_sub_predict\nsample_submission.to_csv('blend_submission.csv', index=False)","1577929f":"# MLP part.\nmodel_mlp = Sequential()\nmodel_mlp.add(L.Dense(512, input_dim=X_train.shape[1], activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))\nmodel_mlp.add(L.Dense(256, activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))","dee39a77":"# Feature input + embeddings layers.\nengineDisplacement = L.Input(\n    shape=[1], \n    name=\"engineDisplacement\"\n)\n\nemb_engineDisplacement = L.Embedding(\n    len(X.engineDisplacement.unique().tolist())+1, 20\n)(engineDisplacement)\n\nf_engineDisplacement = L.Flatten()(emb_engineDisplacement)","fe9cfd90":"# Combining the MLP and feature pass-through + head installing.\ncombinedInput = L.concatenate([model_mlp.output, f_engineDisplacement,])\nhead = L.Dense(64, activation=\"relu\")(combinedInput)\nhead = L.Dense(1, activation=\"linear\")(head)\nmodel = Model(inputs=[model_mlp.input, engineDisplacement], outputs=head)","76208bcf":"# Checking the model.\nmodel.summary()","2fb839be":"# Compiling the model.\noptimizer = tf.keras.optimizers.Adam(0.01)\n\nmodel.compile(\n    loss='MAPE',\n    optimizer=optimizer, \n    metrics=['MAPE']\n)","37955a3a":"# Pass-through feature model training.\nhistory = model.fit(\n    [X_train, X_train.engineDisplacement.values],\n    y_train,\n    batch_size=512,\n    epochs=500,\n    validation_data=([X_test, X_test.engineDisplacement.values], y_test),\n    callbacks=callbacks_list\n)","31d39a93":"# Saving model + Loading best weights.\nmodel.load_weights('..\/working\/best_model.hdf5')\ntest_predict_nn_bonus = model.predict([X_test, X_test.engineDisplacement.values])\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn_bonus[:,0]))*100:0.2f}%\")","71c7b127":"# General Information","ad533207":"### 3.6 Engine Power","6f22ca78":"### 3.10 Model Info","af5ecd7b":"The average MtA is 8-15 thousand km per year. The deviation from this number will be recorded in the binary features \"high mileage-to-age ratio\" and \"low mileage-to-age ratio\".","3a7d2a6b":"There are some strange names in the full list, let's find out what's wrong with them.","100b5eee":"There are no missing values in the test dataset. Now let's check the data for duplicates.","dd1c7ca7":"'\u041f\u0422\u0421' is, in fact, a binary feature, where 1 is the presence of the original, and 0 is its absence, i.e. a duplicate.","778a3c5f":"\u0420\u0430\u0441\u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u043a\u044d\u0442\u0431\u0443\u0441\u0442\u0430.","94fcd2a1":"As we can see, the closest distributions in terms of price are 6.3, 5.9, 1.3 and 3.8 LTR. Since our electric cars are compact 5-door hatchbacks, we will take the minimum value.","0bb422cc":"A lot of missing values. Also there are correlating features in the dataset (date of manufacture, date of model launch). We'll delete it later.","5b6cb2c5":"### MAPE at this stage: 11.59% (15.90% - Baseline)","3725d68a":"This feature duplicates data from other columns. Let's delete it later.","74f4bf07":"Our car was built in 2001, so according to our data, it probably had 3 or more owners. Let's fill in the missing value.","c6213afc":"![](https:\/\/images.drive.ru\/i\/0\/4f00f72809b602104b000045.jpg)\n\nThis was my first experience of refining a neural network with multiple inputs, which I received in the Data Science course from SkillFactory. The legend was that we had a set of text data and photos of cars. The task was to improve the performance of the model for each of the network inputs - tabular, text, and image.","2f2e324e":"ID of the link to the car image. We'll delete it later.","09673cd2":"Let's define a lemmatization function and apply it to the \"description\" feature. The essence of lemmatization is that all words will be reduced to infinitives. In the same function, we will clear the text from characters, which will save us some time.","a9c4a3b5":"This model will predict the average price by car model and year of manufacture. We will compare other models with it.","023e28ed":"### 3.11 Name","0d28e89f":"### 3.19 Registration Certificate","18936627":"### 3.13 Production Date","28e1bf22":"### 3.14 Sell ID","ad208fbe":"Let's create the binary feature \"popular color\", \"rare color\" and \"very rare color\".","1f222548":"Now let's convert liters to milliliters. Thus, we will avoid the error when logarithming the value 0.7.","3c41f565":"If we store all the images in memory, we may encounter a shortage of it. With the tf.data.Dataset iterator, we can pass small chunks of data to the fit() method without overloading memory.","524cd0fc":"### 3.20 Car Layout","39bb5148":"Initially, we left only the most strongly correlated features with the target. This reduced the quality of the model, so we bring back all the features.\n\nNow let's look at categorical variables.","1cc1b728":"### 3.3 \u0421olor","b1948444":"## 4. Feature Engineering","35a90b02":"## 8. Multiple Input NLP + Image (Model 5)","cd45e96e":"### 3.7 Fuel Type","60e03d52":"As a bonus, let's try to pass one feature through to the top of NN. Experimentally, we have found that the best result is given by the engine displacement.","18e9e46a":"The distribution has become more normal.","b0901686":"### 3.4 Description","cb0e3892":"### MAPE at this stage: 12.73% (13.23% - Baseline)","ef7866f4":"### MAPE at this stage: 11.67% (14.88% - Baseline)","4681e0b0":"## 3. Exploratory Data Analysis","bc7b858c":"### MAPE at this stage: 11.08% (13.98% - Baseline)","57ebb54f":"*     bodyType - car body type, categorical variable.\n*     brand - car brand, categorical variable.\n*     color - car color, categorical variable.\n*     description - vehicle description, text variable, requires processing.\n*     engineDisplacement - car engine displacement, categorical variable, requires processing.\n*     enginePower - car engine power, categorical variable, requires processing.\n*     fuelType - car fuel type, categorical variable.\n*     mileage - car mileage, continuous variable.\n*     modelDate - manufacturing launch year, categorical variable.\n*     model_info - car series, categorical variable.\n*     name - combination of several features, requires processing.\n*     numberOfDoors - number of car doors, categorical variable.\n*     price - target variable.\n*     productionDate - year of manufacture, categorical variable.\n*     sell_id - external id, continuous variable.\n*     vehicleConfiguration - combination of several features, requires processing.\n*     vehicleTransmission - car transmission type, categorical variable.\n*     \u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b - number of owners, categorical variable.\n*     \u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435 - ownership time, requires processing and missing values correction.\n*     \u041f\u0422\u0421 - car registration certificate, binary variable.\n*     \u041f\u0440\u0438\u0432\u043e\u0434 - car layout, categorical variable.\n*     \u0420\u0443\u043b\u044c - car wheel position, categorical variable.","40136c28":"Let's use blending to find the average between the predictions of CatBoost and our neural network with multiple inputs.","a8684f7d":"## 1. Initial Setup","ada7a1d2":"### MAPE at this stage: 19.88%","b2ecce76":"As you can see, the distribution is far from normal. Let's try using the logarithm.","798d5069":"### 3.22 Price","20dd3207":"Let's set the conditions for the NLP model: maximum number of extracted words and maximum length of the sequence.","67988df4":"### 3.5 Engine Displacement","9531321e":"Let's create the feature \"mileage\/age ratio per year\".","3c913a1a":"### 3.17 Owners","0243b9e4":"### 3.18 Ownership Time","83122208":"Let's follow the actions taken:\n* We initialized necessary libraries, set visualization conditions and loaded the dataset.\n* We created a naive model as a starting point for assessing the quality of forecasting.\n* We conducted an EDA, cleared and processed the tabular information.\n* We trained CatBoost and Tabular NN on tabular data, improved the result via regularization and selection of hyperparameters.\n* We lemmatized the texts, cleared them of symbols, and trained NLP NN on the received data.\n* We have augmented the images and trained Image NN, improving its result via fine-tuning.\n* We used blending to average the results.\n* We experimented with passing through the features.\n* At each stage, we managed to improve baseline's performance by at least one percent.\n\n\nThe following conclusions can be drawn from the results:\n* It is necessary to use a naive model to have a reference point for evaluating experiments.\n* Reducing the number of features, even if they are correlated with each other, reduces the quality of CatBoost forecasting.\n* Taking logarithm of numeric features improves the quality of Tabular NN predictions.\n* Regularization has a positive effect on the quality of Tabular NN predictions.\n* Lemmatization and symbols clearing improves the quality of NLP NN predictions.\n* In the future, when using fine-tuning, it makes sense to look for the optimal mark for freezing the weights.\n* tf.data.Dataset iterator helps us to avoids memory overload when working with images.\n* Blending consistently helps improve results by averaging forecasts from different models.\n* It is worth studying the feature's passing through, since it clearly has a certain impact on the quality of the model.","9bc607f8":"### MAPE at this stage: 11.83% (14.47% - Baseline)","95bace71":"Another feature that combines values from other columns. We'll delete it later.","3efcc475":"This car is about 20 years old, of which more than 10 years from the current owner. Thus, the car had at least 2 owners. Let's evaluate this feature in our dataset.","0191aab6":"\u0414\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u043d\u0435\u0442. \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u043c \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u0436\u0434\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a.","f5ec8621":"### 3.21 Wheel Position","fc5157da":"Let's add l1-l2 regularization to both dense layers and evaluate how it will affect the quality of the model.","7102beb3":"Let's check if there is a statistically significant difference in the distribution of features using the Student's test.","b38c097c":"### 3.15 Vehicle Configuration","9927ebb4":"Displacement of these cars is not defined, since we are talking about electric vehicles. Prices for these cars range from 1.7 to 2.7 million rubles. Let's look at the average displacement of cars with the same price.","e24c67ae":"### 3.8 Mileage","769b6e39":"## 2. Naive Forecasting (Model 1)\n","0b5bcd5b":"Let's use fine-tuning at 50% of EfficientNet B3.","92e0eee4":"## 10. Passing Feature Through ","860ee0c8":"### MAPE at this stage: 11.15% (11.94% - Baseline)","a572375c":"## 4. Data Pre-processing","586e390f":"## 5. Feature Selection","fc7fcf0b":"Let's convert the engine power to a numeric feature.","9868c335":"### 3.16 Vehicle Transmission","38930fdf":"### 3.12 Number of Doors","4563c345":"## 11. Recap & Conclusions\u00b6","75888c22":"### 3.9 Model Date","82dcea04":"### 3.1 Body Type","8b9a6ce4":"There are only 2 columns with missing values. \"\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435\" (ownership) column can be dropped, since it is probably strongly correlates with the year of release. Also there is one missing value in the \"\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b\" (number of owners) column. It will be easier to fill it manually.","67658613":"There is a strong correlation between engine displacement and power. Mileage has a strong correlation with age characteristics and the mileage\/age ratio. Age characteristics correlate with each other.","bd46a954":"In fact, all the cars in the dataset are left-hand drive. Both right-hand drive cars are in the training dataset. Candidate for deletion.","0a063b01":"### 3.2 Brand","25613630":"## 6. Tabular NN (Model 3)","49138d5a":"## 7. Multiple Input NLP (Model 4)","5faf3cb4":"Let's look at cars with an undefined displacement.","2d14a01f":"## 5. CatBoost (Model 2)","411eeb39":"## 9. Blending Predictions","bdd9e3a3":"All features have statistically significant differences.","e7ff25e5":"We found out that '100' = 'S4', 'COUPE' = 'S2', 'None' = 'C_KLASSE_AMG'."}}