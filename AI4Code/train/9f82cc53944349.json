{"cell_type":{"80a0180a":"code","903074e9":"code","04610717":"code","626871ec":"code","bd28042c":"code","ce94b56d":"code","38e15b06":"code","11f003f2":"code","4c9b0053":"code","b3378ce8":"code","e5eff050":"code","21422d0f":"code","a48c0aee":"code","a87a5b2e":"code","9db19484":"code","df2040e5":"code","8a00beff":"code","15806e18":"code","6f671f2a":"code","bae379ea":"code","5a185089":"code","4ba52800":"code","56a62968":"code","a2747c3e":"code","3053b98f":"code","b807c93f":"code","95806b12":"markdown","0b61944a":"markdown","aa61fa79":"markdown","df61d496":"markdown","16e06864":"markdown","d39409a7":"markdown","c714811c":"markdown","176bfc78":"markdown","fdcd9237":"markdown","f9dc6509":"markdown","d0119e37":"markdown","4b74c0a9":"markdown","3dabcbc2":"markdown","cd83e0fa":"markdown","d8cea84b":"markdown","50b822e3":"markdown","db4b2b3e":"markdown","b0a204d2":"markdown","ef5636e1":"markdown"},"source":{"80a0180a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas import DataFrame\n%matplotlib inline","903074e9":"data = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","04610717":"data.head()","626871ec":"data.info()","bd28042c":"data['Class'].value_counts()","ce94b56d":"sns.countplot(x='Class', data=data)","38e15b06":"from sklearn.model_selection import train_test_split\n\ntrain_data, val_data = train_test_split(data, test_size=0.25, random_state=42)\nprint(train_data.shape, val_data.shape)","11f003f2":"train_label = train_data['Class']\nval_label   = val_data['Class']\ntrain_data  = train_data.drop(['Class'], axis=1)\nval_data    = val_data.drop(['Class'], axis=1)\nprint(train_data.shape, val_data.shape, train_label.shape, val_label.shape)","4c9b0053":"train_data.head(2)","b3378ce8":"val_data.head(2)","e5eff050":"from sklearn.preprocessing import StandardScaler\n\nstd_scaler_Time   = StandardScaler()\nstd_scaler_Amount = StandardScaler()\n\ntrain_data['Time']   = std_scaler_Time.fit_transform(train_data[['Time']])\ntrain_data['Amount'] = std_scaler_Amount.fit_transform(train_data[['Amount']])\n\nval_data['Time']   = std_scaler_Time.transform(val_data[['Time']])\nval_data['Amount'] = std_scaler_Amount.transform(val_data[['Amount']])","21422d0f":"train_data.head(2)","a48c0aee":"val_data.head(2)","a87a5b2e":"from sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, precision_recall_curve, classification_report\n\ndef model_def(model, model_name, m_train_data, m_train_label):\n    model.fit(m_train_data, m_train_label)\n    s = \"predict_\"\n    p = s + model_name\n    p = model.predict(m_train_data)\n    cm = confusion_matrix(m_train_label, p)\n    print(\"Confusion Matrix: \\n\", cm)\n    cr = classification_report(m_train_label, p, target_names=['Not Fraud', 'Fraud'])\n    print(\"Classification Report: \\n\", cr)\n    precision = np.diag(cm)\/np.sum(cm, axis=0)\n    recall    = np.diag(cm)\/np.sum(cm, axis=1)\n    F1 = 2 * np.mean(precision) * np.mean(recall)\/(np.mean(precision) + np.mean(recall))\n    cv_score = cross_val_score(model, m_train_data, m_train_label, cv=10, scoring='recall')\n    print(\"Mean CV Score     :\", cv_score.mean())\n    print(\"Std Dev CV Score  :\", cv_score.std())","9db19484":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto', C=0.5)\nmodel_def(logreg, \"logreg\", train_data, train_label)","df2040e5":"val_data_logreg = logreg.predict(val_data)\nprint(\"Logistic Regression: \\n\", confusion_matrix(val_label, val_data_logreg))","8a00beff":"from imblearn.under_sampling import NearMiss\n\nprint(\"Before Undersampling, counts of label '1': {}\".format(sum(train_label == 1))) \nprint(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(train_label == 0))) \n  \nnr = NearMiss() \n  \ntrain_data_miss, train_label_miss = nr.fit_sample(train_data, train_label.ravel()) \n  \nprint(\"After Undersampling, counts of label '1': {}\".format(sum(train_label_miss == 1))) \nprint(\"After Undersampling, counts of label '0': {}\".format(sum(train_label_miss == 0))) ","15806e18":"logreg_miss = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto', C=0.5)\nmodel_def(logreg_miss, \"logreg_miss\", train_data_miss, train_label_miss)","6f671f2a":"val_data_miss   = logreg_miss.predict(val_data)\nprint(\"Logistic Regression - Undersampling: \\n\", confusion_matrix(val_label, val_data_miss))","bae379ea":"from imblearn.over_sampling import SMOTE\n\nprint(\"Before Oversampling, counts of label '1': {}\".format(sum(train_label == 1))) \nprint(\"Before Oversampling, counts of label '0': {} \\n\".format(sum(train_label == 0))) \n  \nsm = SMOTE(random_state=42) \n  \ntrain_data_SMOTE, train_label_SMOTE = sm.fit_sample(train_data, train_label.ravel()) \n  \nprint(\"After Oversampling, counts of label '1': {}\".format(sum(train_label_SMOTE == 1))) \nprint(\"After Oversampling, counts of label '0': {}\".format(sum(train_label_SMOTE == 0))) ","5a185089":"logreg_SMOTE = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto', C=0.5)\nmodel_def(logreg_SMOTE, \"logreg_SMOTE\", train_data_SMOTE, train_label_SMOTE)","4ba52800":"val_data_SMOTE  = logreg_SMOTE.predict(val_data)\nprint(\"Logistic Regression - Oversampling: \\n\", confusion_matrix(val_label, val_data_SMOTE))","56a62968":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier","a2747c3e":"# Applying Neural Network\ndef build_classifier():\n    classifier = Sequential([Dense(128, activation='relu', input_shape=(train_data_SMOTE.shape[1], )),\n                             Dropout(rate=0.1),\n                             Dense(64, activation='relu'),\n                             Dropout(rate=0.1),\n                             Dense(1, activation='sigmoid')])\n\n    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['Precision', 'Recall'])\n    print(classifier.summary())\n    return classifier\n\nmodel = KerasClassifier(build_fn=build_classifier)","3053b98f":"history = model.fit(train_data_SMOTE, train_label_SMOTE,\n                    batch_size=30,\n                    epochs=10,\n                    validation_data=(val_data, val_label))","b807c93f":"val_data_Neural = model.predict(val_data)\nprint(\"Artificial Neural Network: \\n\", confusion_matrix(val_label, val_data_Neural))","95806b12":"#### Since the most effective model by far is SMOTE and where the datset has become almost doubled we will fit this dataset to an Artificial Neural Network (ANN) to see how it goes.","0b61944a":"#### Here we see the real advantage - 105 out of 113 are predicted properly. So the model has become more accurate and look at the Precision compared to that of NearMiss algorithm. So by far this is the most effective model.","aa61fa79":"#### We can see that just 68 out of 113 are predicted properly.","df61d496":"#### So 'Class' is the target variable and let's see how it is distributed in the dataset and then we can understand why it is called a imbalanced dataset.","16e06864":"#### So we can see count of majority class i.e Class=0 is reduced to the same count of Class=1 and the dataset has become balanced.\n \n#### Now we will apply Logistic Regression with the same parameters on this undersampled data.","d39409a7":"#### Fraud identification problem is one of the cases where we will have imbalanced dataset i.e the fraud transactions will be very few in numbers. So if we don't apply specific techniques we will not be able to get a proper model.\n\n#### In this notebook we will start applying standard approach of a classification problem and see why it doesn't work and what will be the alternative approaches.\n\n#### Import the libraries.","c714811c":"### **NearMiss Algorithm \u2013 Undersampling**\n\n#### NearMiss is an under-sampling technique. It aims to balance class distribution by randomly eliminating majority class examples. When instances of two different classes are very close to each other, it removes the instances of the majority class to increase the spaces between the two classes. \n\n#### The basic intuition about the working of near-neighbor methods is as follows:\n\n#### Step 1: The method first finds the distances between all instances of the majority class and the instances of the minority class. Here, majority class is to be under-sampled.\n#### Step 2: Then, n instances of the majority class that have the smallest distances to those in the minority class are selected.\n#### Step 3: If there are k instances in the minority class, the nearest method will result in k * n instances of the majority class.\n\nReference : https:\/\/www.geeksforgeeks.org\/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python\/","176bfc78":"#### Let's apply Logistic Regreesion algorithm on the dataset.","fdcd9237":"#### Here we see the improvement - 104 out of 113 are predicted properly. So the model has become more accurate but at the cost of low precision.","f9dc6509":"#### So we can see in the ANN model the Precision is highest but the Recall is compromised slightly as compared to SMOTE.","d0119e37":"### **SMOTE (Synthetic Minority Oversampling Technique) \u2013 Oversampling**\n\n#### SMOTE (synthetic minority oversampling technique) is one of the most commonly used oversampling methods to solve the imbalance problem.\n#### It aims to balance class distribution by randomly increasing minority class examples by replicating them.\n#### SMOTE synthesises new minority instances between existing minority instances. It generates the virtual training records by linear interpolation for the minority class. These synthetic training records are generated by randomly selecting one or more of the k-nearest neighbors for each example in the minority class.\n\nReference - https:\/\/www.geeksforgeeks.org\/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python\/","4b74c0a9":"#### Create a standard function to display model performance metrices.","3dabcbc2":"#### Here is the alternative approach - creating synthetic data i.e the data that does not exist in the original dataset and there are 2 main techinques to do this - NearMiss and SMOTE.","cd83e0fa":"#### We can visualize the number of entries with Class=1 (Fraud) is very less compared to Class=0 (Not-Fraud).\n\n#### Next we will split the data into train_set and validation_set.","d8cea84b":"#### Explore the data.","50b822e3":"#### Load the data.","db4b2b3e":"#### Since all V* columns are already scaled we will scale only the 'Time' and 'Amount' columns.","b0a204d2":"#### So we can see count of minority class i.e Class=1 is increased to the same count of Class=0 and the dataset has become balanced.\n \n#### Now we will apply Logistic Regression with the same parameters on this oversampled data.","ef5636e1":"#### Here you can see the problem. Our overall model accuracy is 1.00 but Recall for Fraud class is just 0.64 which means there is lots of misclassification for Fraud class. As per the confusion matrix out (242 + 137) = 379 Fraud class only 242 are classified properly and this has happened because of imbalanced representation of data."}}