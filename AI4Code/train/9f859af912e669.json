{"cell_type":{"5c8371b9":"code","5b4d1bba":"code","1173eaed":"code","5bde63b6":"code","1ba2bb13":"code","a61393b8":"code","f48ab1e4":"code","bb27adf5":"code","a0544bf2":"code","b4bcfda8":"code","5a7f9502":"code","e96a854c":"code","8ceb298b":"code","7df299da":"code","e6cb5d98":"code","39c73d94":"code","75b648ef":"code","119e01f2":"code","40571d12":"code","dc3b688b":"code","51f941c4":"code","aa19ced2":"code","7728a43e":"code","f0bf25c0":"code","fb2ac470":"markdown","9af40a5b":"markdown","381f021a":"markdown"},"source":{"5c8371b9":"import sys\n# import lightgbm as lgb\nfrom  datetime import datetime, timedelta\n\n    \nimport lightgbm as lgb\nimport os, sys, gc, time, warnings, pickle, random\nfrom math import ceil\n\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_colwidth',100)","5b4d1bba":"INPUT_PATH = '..\/..\/'\nMAIN_INDEX = ['id','d']  # We can identify item by these columns\neval_end_day = 1941\nvalid_end_day = 1913\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    \n## Simple \"Memory profilers\" to see memory usage\ndef get_memory_usage():\n    return np.round(psutil.Process(os.getpid()).memory_info()[0]\/2.**30, 2) \n        \ndef sizeof_fmt(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num \/= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n\n## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    start_mem = df.memory_usage().sum() \/ 1024**2  \n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n\n## Merging by concat to not lose dtypes\ndef merge_by_concat(df1, df2, merge_on):\n    # we want the extract data from df2 to add as columns in df1(i.e. left join). For saving memory(I think), just get merge_on of df1 for merging, then concat\n    merged_gf = df1[merge_on]\n    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n    return df1\n","1173eaed":"def create_train_data(train_start=1,test_start=1800,is_train=True, add_test=False):\n    # data types of each columns \n    # start_day = train_start if is_train else test_start\n    start_day = train_start\n    numcols = [f\"d_{day}\" for day in range(start_day,eval_end_day+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    SALE_DTYPES = {numcol:\"float32\" for numcol in numcols} \n    SALE_DTYPES.update({col: \"category\" for col in catcols if col != \"id\"}) \n\n    # loading data\n    sale_data = pd.read_csv('..\/..\/sales_train_evaluation.csv',dtype=SALE_DTYPES,usecols=catcols+numcols)\n\n    # category types to integer type\n#     for col, col_dtype in PRICE_DTYPES.items():\n#         if col_dtype == \"category\":\n#             price_data[col] = price_data[col].cat.codes.astype(\"int16\")\n#             price_data[col] -= price_data[col].min()\n\n#     cal_data[\"date\"] = pd.to_datetime(cal_data[\"date\"])\n#     for col, col_dtype in CAL_DTYPES.items():\n#         if col_dtype == \"category\":\n#             cal_data[col] = cal_data[col].cat.codes.astype(\"int16\")\n#             cal_data[col] -= cal_data[col].min()\n\n\n#     # # add test days with nan (\u6ce8\u610f\u63d0\u4ea4\u683c\u5f0f\u91cc\u6709\u4e00\u90e8\u5206\u4e3a\u7a7a)\n#     if not is_train:\n#         for day in range(1913+1, 1913+ 2*28 +1):\n#             sale_data[f\"d_{day}\"] = np.nan\n\n    # In the sales dataset, each row represents one item in one specific store. since our target is sales,\n    # We can tranform horizontal representation \n    # into vertical \"view\" so that each row represents for sales for one day.\n    # Our \"index\" will be 'id','item_id','dept_id','cat_id','store_id','state_id'\n    # and labels are 'd_' coulmns\n    grid_df = pd.melt(sale_data,\n            id_vars = catcols,\n            value_vars = [col for col in sale_data.columns if col.startswith(\"d_\")],\n            var_name = \"d\",\n            value_name = \"sales\")\n    \n#     # we can add test days with nan after melt, but more tedious\n#     if add_test == True:\n#         END_TRAIN = 1913         # Last day in train set\n#         # To be able to make predictions\n#         # we need to add \"test set\" to our grid\n#         add_grid = pd.DataFrame()\n#         for i in range(1,29):\n#             # construct the index columns\n#             temp_df = sale_data[catcols]\n#             temp_df = temp_df.drop_duplicates() # Actually, no need this since each row in original sale data indexed by catcols representing one item in one store\n#             # add label column for sales\n#             temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n#             # add sales column\n#             temp_df['sales'] = np.nan\n#             add_grid = pd.concat([add_grid,temp_df])\n#         grid_df = pd.concat([grid_df,add_grid])\n#      #Remove some temoprary DFs\n#         del temp_df, add_grid\n        \n    # the index of concated df keep the original ones, needs to be reset\n    grid_df = grid_df.reset_index(drop=True)\n        \n    # We will not need original sale-data\n    # anymore and can remove it\n    del sale_data\n    \n    \n    # It seems that leadings zero values\n    # in each train_df item row\n    # are not real 0 sales but mean\n    # absence for the item in the store\n    # by doing inner join we can remove\n    # such zeros\n    PRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }\n    CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n            \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n            \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\n    cal_data = pd.read_csv('..\/..\/calendar.csv',dtype=CAL_DTYPES)\n    price_data = pd.read_csv('..\/..\/sell_prices.csv',dtype=PRICE_DTYPES)\n    ## get wm_yr_wk as key for join the price table\n    grid_df = grid_df.merge(cal_data[['d', 'wm_yr_wk']], on= \"d\", copy = False)\n    grid_df = grid_df.merge(price_data[[\"store_id\", \"item_id\", \"wm_yr_wk\"]], on = [\"store_id\", \"item_id\", \"wm_yr_wk\"])\n    \n    \n    return grid_df\n\n","5bde63b6":"def create_prices_features(prices_df):\n    ########################### Prices\n    #################################################################################\n    print('Create Prices Features')\n\n    # We can do some basic aggregations\n    prices_df['price_max'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n    prices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n    prices_df['price_std'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n    prices_df['price_mean'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n\n    # and do price normalization (min\/max scaling)\n    prices_df['price_norm'] = prices_df['sell_price']\/prices_df['price_max']\n\n    # Some items are can be inflation dependent\n    # and some items are very \"stable\"\n    prices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique') # how many prices for each items(in a store), reflect inflation and stable\n    \n    # since group by object will only leave string columns, get categorical for item_id\n    # also, interestingly, groupby will do combination if any element for groupby is cagtegory type\n    # https:\/\/github.com\/pandas-dev\/pandas\/issues\/17594\n#     prices_df['item_id_a'] =prices_df['item_id'].cat.codes\n#     prices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id_a'].transform('nunique') # how many items(in a store) have same price\n#     prices_df = prices_df.drop('item_id_a', axis=1)\n\n    # I would like some \"rolling\" aggregations, i,e. price \"momentum\" (some sort of)\n    # but would like months and years as \"window\", so the next three commands add months and years as columns\n    CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n        \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\n    cal_data = pd.read_csv(INPUT_PATH+'calendar.csv',dtype=CAL_DTYPES)\n    ## get month, year to join into prices_df\n    calendar_prices = cal_data[['wm_yr_wk','month','year']]\n    # approcimately have (the length of the original\/ 7), since the calendar_df is recorded by day, now week\n    calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n    prices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n    del calendar_prices\n\n    # Now we can add price \"momentum\" (some sort of)\n    # Shifted by week \n    # by month mean\n    # by year mean\n    prices_df['price_momentum'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1)) # the rate with sell price last day\n#     ## cannot use built-in mean which would output nan if the group has nan values\n#     prices_df['price_momentum_m'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean') # the rate with sell price last month\n#     prices_df['price_momentum_y'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean') # the rate with sell price last year\n    prices_df['price_momentum_m'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform(lambda x: np.mean([i for i in x if not np.isnan(i)]))\n    prices_df['price_momentum_y'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform(lambda x: np.mean([i for i in x if not np.isnan(i)])) # the rate with sell price last year\n#     # for testing the problem of transform('mean') which gives different values by using costom mean function(not because of null value)\n#     idx = (price_data['store_id'] == 'WI_3') & (price_data['item_id'] == 'FOODS_3_827') & (price_data['month'] == 6)\n#     price_data[['sell_price']][idx]\n\n    del prices_df['month'], prices_df['year']\n    \n    prices_df = reduce_mem_usage(prices_df)\n    \n    return prices_df\n\ngrid_df = create_train_data()\noriginal_columns = list(grid_df) \n# Save original sales\n# grid_df.to_pickle('grid_part_1.pkl')","1ba2bb13":"grid_df.head()","a61393b8":"# add price features\n\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }\nprices_df = pd.read_csv('..\/..\/sell_prices.csv',dtype=PRICE_DTYPES)\nprices_df = create_prices_features(prices_df)\ngrid_df = grid_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')\n\ndel prices_df\n# there are 30490 null value for price_momentum in prices_df, because of 3049 products and 10 stores in data(3049*10 records per day)\n# after joining with grid_df, there are 213430 null\nfor col in list(grid_df):\n    num_na = grid_df[col].isnull().sum()\n    if num_na != 0:\n        print(col, str(num_na))\n        \n# So by removing such rows, we acutually remove the records in the first week, I should not remove the null values here,\n# Since the sell prices of the records with null price momentum could be used for created lags.\n# grid_df.dropna(inplace=True)","f48ab1e4":"grid_df.info()","bb27adf5":"def make_time_features(grid_df):\n    # Convert to DateTime\n    grid_df['date'] = pd.to_datetime(grid_df['date'])\n\n    # Make some features from date:  \n    # \u6709\u7684\u65f6\u95f4\u7279\u5f81\u6ca1\u6709\uff0c\u901a\u8fc7datetime\u7684\u65b9\u6cd5\u81ea\u52a8\u751f\u6210\n    grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n    grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\n    grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\n    grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n    grid_df['tm_q'] = grid_df['date'].dt.quarter.astype(np.int8)\n    grid_df['tm_y'] = grid_df['date'].dt.year\n    grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n    grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x\/7)).astype(np.int8)\n    \n    # whether it is weekend\n    grid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n\n    # Remove date\n    del grid_df['date']\n    \n    \n    return grid_df\n\n","a0544bf2":"# add time features: from calendar files and pandas functions\n## calendar files\nCAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n        \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\ncal_data = pd.read_csv(INPUT_PATH+'calendar.csv',dtype=CAL_DTYPES)\ncal_data.info()","b4bcfda8":"from sklearn import preprocessing\n## Merge calendar partly, other generate using pandas datetime functions\nicols = ['date',\n         'd',\n         'event_name_1',\n         'event_type_1',\n         'event_name_2',\n         'event_type_2',\n         'snap_CA',\n         'snap_TX',\n         'snap_WI']\ngrid_df = grid_df.merge(cal_data[icols], on=['d'], how='left')\n\n\n## only consider SNAP for the correct state\nenc = preprocessing.OneHotEncoder()\nstate = enc.fit_transform(grid_df[['state_id']]).toarray()\ngrid_df['snap'] = np.multiply(grid_df[['snap_CA', 'snap_TX', 'snap_WI']].values,state).sum(axis=1)\ngrid_df = grid_df.drop(['snap_CA', 'snap_TX', 'snap_WI'], axis=1)\n\n\ngrid_df = make_time_features(grid_df)\n# Save part 3\n# grid_df.to_pickle('grid_part_3.pkl')","5a7f9502":"grid_df.info()","e96a854c":"for col in ['event_type_1', 'event_type_2']:\n    grid_df[col] = grid_df[col].cat.codes.astype(\"int16\")\n    grid_df[col] -= grid_df[col].min()\n    \n\ngrid_df = grid_df.drop(['event_name_1', 'event_name_2'], axis=1)","8ceb298b":"for col in list(grid_df):\n    num_na = grid_df[col].isnull().sum()\n    if num_na != 0:\n        print(col, str(num_na))","7df299da":"def create_lags(lag_df, LAGS_SPLIT, TARGET='sales', groupby=None):\n    '''Return Dataframe for lags\n    Input is dataframe with last column as TARGET and others as (composite) key\n    '''\n    # lag creation\n    # and \"append\" to our grid\n    LAGS = []\n    for LAG in LAGS_SPLIT:\n        if groupby != None:\n            lag_df[TARGET+'_lag_'+str(LAG)] = lag_df.groupby(groupby)[TARGET].transform(lambda x: x.shift(LAG)).astype(np.float16)\n        else:\n            lag_df[TARGET+'_lag_'+str(LAG)] = lag_df[TARGET].shift(LAG).astype(np.float16)\n        LAGS.append(TARGET+'_lag_'+str(LAG))\n        \n    return lag_df[LAGS]\n\n\n\ndef make_lag_roll(roll_df, ROLS_SPLIT, groupby=None): \n    TARGET = roll_df.columns[-1]\n    cols = []\n    for LAG_DAY in ROLS_SPLIT:\n        shift_day = LAG_DAY[0]\n        roll_wind = LAG_DAY[1]\n        col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n        roll_df[col_name] = roll_df.groupby(groupby)[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n        cols.append(col_name)\n    return roll_df[cols]","e6cb5d98":"# LAGS of sell price for the item in each stores(30490)\nLAGS_SPLIT = [7, 28]\nlag_df = create_lags(grid_df[['id','d','sales']], LAGS_SPLIT, groupby=['id'])\n\n\n# LAG_ROLLING_WIN_STATISTICS:just average\/mean here\n# [[1, 7], [1, 14], [1, 30], [1, 60], [7, 7], [7, 14], [7, 30], [7, 60], [14, 7], [14, 14], [14, 30], [14, 60]]\nROLS_SPLIT = [] \nfor i in [0,7,14]:\n    for j in [7,14,28]:\n        ROLS_SPLIT.append([i,j])\n        \nroll_df = make_lag_roll(grid_df[['id','d','sales']], ROLS_SPLIT, groupby=['id'])","39c73d94":"grid_df = pd.concat([grid_df, lag_df, roll_df], axis=1)\n\n# Save \n# pd.concat([lag_Df, roll_df], axis=1).to_pickle('lag_df.pkl')","75b648ef":"for col in list(grid_df):\n    num_na = grid_df[col].isnull().sum()\n    if num_na != 0:\n        print(col, str(num_na))","119e01f2":"for col in list(grid_df):\n    num_na = grid_df[col].isnull().sum()\n    if num_na != 0:\n        print(col, str(num_na))","40571d12":"# grid_df = pd.concat([pd.read_pickle('grid_part_1.pkl'),\n#                      pd.read_pickle('grid_part_2.pkl').iloc[:,2:],\n#                      pd.read_pickle('grid_part_3.pkl').iloc[:,2:]],\n#                      axis=1)\n\n# grid_df = pd.concat([grid_df,\n#                      pd.read_pickle('lag_df.pkl')],\n#                      axis=1)\n\n# normalize category variables so that it starts from 0\n# BUT we cannot do it at begining in case errors(type error, values of merge keys) when merge with price_data\n# BUT I found category type save more memnory than int16\n# catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n# for col in catcols:\n#     if col != \"id\":\n#         grid_df[col] = grid_df[col].cat.codes.astype(\"int16\")\n#         grid_df[col] -= grid_df[col].min()\n        ","dc3b688b":"# change day type to int so that it is easily manipulated\ngrid_df['d'] = [int(day[2:]) for day in grid_df['d']]\ngrid_df['d'] = grid_df['d'].astype(np.int16)","51f941c4":"grid_df.d.unique()","aa19ced2":"grid_df.info()","7728a43e":"for col in list(grid_df):\n    num_na = grid_df[col].isnull().sum()\n    if num_na != 0:\n        \n        print(col, str(num_na))","f0bf25c0":"grid_df.to_pickle('grid_df_evaluation.pkl')\n# del grid_df\n","fb2ac470":"## Add Price-relevant features\nNotice that `item_nunique` has a lot `nan` values, it is because `groupby` method will only remove string columns(`item_id`) for grouping, Therefore, I have to do as following to transform into categorical column for `item_id`\n~~~~\n    # also, interestingly, groupby will do combination(cartesian product) if any element for groupby is cagtegory type\n    # more: https:\/\/github.com\/pandas-dev\/pandas\/issues\/17594\n    prices_df['item_id_a'] =prices_df['item_id'].cat.codes\n    prices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id_a'].transform('nunique') # how many items(in a store) have same price\n    prices_df = prices_df.drop('item_id_a', axis=1)\n~~~~\n","9af40a5b":" ## Add Time-relevant features and SNAP","381f021a":"## Add Lag and statistics of rolling windows"}}