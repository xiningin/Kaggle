{"cell_type":{"94963f53":"code","6f87d66b":"code","e8394d40":"code","2a334bed":"code","24f569f0":"code","611ba38e":"code","e2229552":"code","ecf22129":"code","e21c67a7":"code","cfb0852d":"code","d8e01596":"code","ac46f711":"code","baa6c9c9":"code","9759eb29":"code","1af8cd5d":"code","e55414da":"code","7a221813":"code","2291e838":"code","514d7779":"code","ad8b5e41":"code","391f96e1":"code","17fa24d9":"code","867d7b18":"code","4b7e23de":"code","4d26b2c9":"code","863b2e16":"code","2a5c886e":"code","63e65b36":"code","bf3d048f":"code","7ef4dcc7":"code","769c20af":"code","2dea6d36":"code","8a1cd4eb":"code","79f3f318":"code","6bae5aff":"code","1c5f5eab":"code","f97fc36a":"code","851bf83a":"code","866a30df":"code","31d1142e":"code","3a7ad999":"markdown","c5867c0e":"markdown","314370bb":"markdown","b08ba40e":"markdown","1b433c5e":"markdown","399c4169":"markdown","7f23168c":"markdown"},"source":{"94963f53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f87d66b":"import l5kit\nl5kit.__version__","e8394d40":"import gc\nimport os\nimport pathlib as path\nimport random\nimport sys\n\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport scipy\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import HTML, display\nimport cv2","2a334bed":"os.listdir('\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\/')","24f569f0":"from l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\nfrom collections import Counter\nfrom l5kit.data import PERCEPTION_LABELS\nfrom prettytable import PrettyTable\nfrom l5kit.dataset import EgoDataset, AgentDataset\nfrom l5kit.evaluation import write_pred_csv\n\nos.environ['L5KIT_DATA_FOLDER'] = '\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\/'\n# cfg = load_config_data('\/kaggle\/input\/lyft-config-files\/agent_motion_config.yaml')\nDEBUG = True","611ba38e":"cfg = {\n    'format_version': 4,\n    'model_params': {\n        'model_architecture': 'resnet50',\n        'history_num_frames': 10,\n        'history_step_size': 1,\n        'history_delta_time': 0.1,\n        'future_num_frames': 50,\n        'future_step_size': 1,\n        'future_delta_time': 0.1\n    },\n    \n    'raster_params': {\n        'raster_size': [224, 224],\n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map\/aerial_map.png',\n        'semantic_map_key': 'semantic_map\/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5\n    },\n    \n    'train_data_loader': {\n        'key': 'scenes\/train.zarr',\n        'batch_size': 32,\n        'shuffle': True,\n        'num_workers': 4\n    },\n    \n    'train_params': {\n        'max_num_steps': 12000 if DEBUG else 10000,\n        'checkpoint_every_n_steps': 5000,\n        \n        # 'eval_every_n_steps': -1\n    }\n\n\n}","e2229552":"# print(f'current raster param:\\n')\n# for k, v in cfg['raster_params'].items():\n#     print(f'{k}:{v}')","ecf22129":"from l5kit.data import LocalDataManager, ChunkedDataset\ndm = LocalDataManager()\ndataset_path = dm.require('\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/train.zarr\/')\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nprint(zarr_dataset)","e21c67a7":"frames = zarr_dataset.frames\ncoords = np.zeros((len(frames), 2))\nfor idx_coord, idx_data in enumerate(tqdm(range(len(frames)-4000000), desc = 'getting centroid to plot trajectory')):\n    frame = zarr_dataset.frames[idx_data]\n    coords[idx_coord] = frame['ego_translation'][:2]\nsns.set_style('white')\nplt.figure(figsize = (13, 8))\nax = sns.scatterplot(coords[:, 0], coords[:, 1], marker = '*', s = 150)\nax.set_xlim([-2500, 1600]);\nax.set_ylim([-2500, 1600]);","cfb0852d":"len(zarr_dataset.agents)","d8e01596":"agent = zarr_dataset.agents[:10000000]\n\nprobabilities = agent['label_probabilities']\nlabel_indexes = np.argmax(probabilities, axis = 1)\ncounts = []\nfor idx_label, label in enumerate(PERCEPTION_LABELS):\n    counts.append(np.sum(label_indexes == idx_label))\n    \ntable = PrettyTable(field_names=['labels', 'counts'])\nfor count, label in zip(counts, PERCEPTION_LABELS):\n    table.add_row([label, count])\nprint(table)","ac46f711":"rast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)","baa6c9c9":"data = dataset[1000]\n\nim = dataset.rasterizer.to_rgb(data['image'].T)\nsns.set_style('white')\nplt.figure(figsize = (10, 7))\nplt.imshow(im)","9759eb29":"target_positon_pixels = transform_points(data['target_positions']+data['centroid'][:2], data['world_to_image'])\ndraw_trajectory(cv2.UMat(im), target_positon_pixels, data['target_yaws'], (120, 160, 189))\n\nplt.figure(figsize = (10, 7))\nplt.imshow(im)","1af8cd5d":"cfg['raster_params']['map_type'] = 'py_satellite'","e55414da":"sat_rast = build_rasterizer(cfg, dm)\nsat_dataset = EgoDataset(cfg, zarr_dataset, sat_rast)","7a221813":"sat_data = sat_dataset[1000]\nsat_im = sat_rast.to_rgb(sat_data['image'].T)\nplt.figure(figsize = (10, 7))\nplt.imshow(sat_im)","2291e838":"sat_target_positon_pixels = transform_points(sat_data['target_positions']+sat_data['centroid'], sat_data['world_to_image'])\ndraw_trajectory(cv2.UMat(sat_im), sat_target_positon_pixels, sat_data['target_yaws'], TARGET_POINTS_COLOR)\nplt.figure(figsize = (10, 7))\nplt.imshow(sat_im)","514d7779":"agent_dataset = AgentDataset(cfg, zarr_dataset, rast)","ad8b5e41":"plt.figure(figsize = (10, 7))\nagent_data = agent_dataset[1000]\nplt.imshow(rast.to_rgb(agent_data['image'].T))","391f96e1":"target_positon_pixels = transform_points(agent_data['target_positions']+agent_data['centroid'], agent_data['world_to_image'])\ndraw_trajectory(cv2.UMat(rast.to_rgb(agent_data['image'].T)), target_positon_pixels, agent_data['target_yaws'], [120, 122, 221])\nplt.figure(figsize = (10, 7))\nplt.imshow(rast.to_rgb(agent_data['image'].T))","17fa24d9":"sat_agent_dataset = AgentDataset(cfg, zarr_dataset, sat_rast)\nsat_agent_data = sat_agent_dataset[1000]\nsat_im = sat_rast.to_rgb(sat_agent_data['image'].T)\nplt.figure(figsize = (10, 7))\nplt.imshow(sat_im)","867d7b18":"import IPython\nfrom IPython.display import display, clear_output\nimport PIL\n\nscene_idx = 2\nindexes = dataset.get_scene_indices(2)\nimages = []\n\nfor idx in indexes:\n    data = dataset[idx]\n    im = rast.to_rgb(data['image'].T)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im))","4b7e23de":"images = []\nfrom matplotlib.animation import FuncAnimation\ndef animate_sol(images):\n    def animate(i):\n        im.set_data(images[i])\n    \n    fig, ax = plt.subplots()\n    im = ax.imshow(images[0])\n    fig.show()\n    return FuncAnimation(fig, animate, frames=len(images), interval=100)\n\nscene_idx = 2\nindexes = sat_dataset.get_scene_indices(scene_idx)\nimages = []\nfor idx in indexes:\n    data = sat_dataset[idx]\n    im = rast.to_rgb(data['image'].T)\n    clear_output(wait = True)\n    images.append(PIL.Image.fromarray(im))\nanim = animate_sol(images)\nHTML(anim.to_jshtml())","4d26b2c9":"# from torch.utils.data import DataLoader","863b2e16":"# dm = LocalDataManager(None)\n# train_cfg = cfg['train_data_loader']\n# rasterizer = build_rasterizer(cfg, dm)\n# train_zarr = ChunkedDataset(dm.require(cfg['train_data_loader']['key'])).open()\n# # train_mask = np.load('\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/mask.npz')['arr_0']\n# train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n# train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"], batch_size=train_cfg[\"batch_size\"], \n#                              num_workers=train_cfg[\"num_workers\"])\n# print(train_dataset)","2a5c886e":"# import torch\n# import torchvision\n# from torchvision import datasets, transforms\n# import torch.nn as nn\n# import torch.nn.functional as F\n# import torch.optim as optim\n# from torchvision.models.resnet import resnet18, resnet34","63e65b36":"# class Net(nn.Module):\n    \n#     def __init__(self, cfg):\n#         super().__init__()\n        \n#         self.backbone = resnet18(pretrained=True, progress=True)\n        \n#         num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n#         num_in_channels = 3 + num_history_channels\n\n#         self.backbone.conv1 = nn.Conv2d(\n#             num_in_channels,\n#             self.backbone.conv1.out_channels,\n#             kernel_size=self.backbone.conv1.kernel_size,\n#             stride=self.backbone.conv1.stride,\n#             padding=self.backbone.conv1.padding,\n#             bias=False,\n#         )\n        \n#         # This is 512 for resnet18 and resnet34;\n#         # And it is 2048 for the other resnets\n#         backbone_out_features = 512\n\n#         # X, Y coords for the future positions (output shape: Bx50x2)\n#         num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n\n#         # You can add more layers here.\n#         self.head = nn.Sequential(\n#             # nn.Dropout(0.2),\n#             nn.Linear(in_features=backbone_out_features, out_features=4096),\n#         )\n\n#         self.logit = nn.Linear(4096, out_features=num_targets)\n        \n#     def forward(self, x):\n#         x = self.backbone.conv1(x)\n#         x = self.backbone.bn1(x)\n#         x = self.backbone.relu(x)\n#         x = self.backbone.maxpool(x)\n\n#         x = self.backbone.layer1(x)\n#         x = self.backbone.layer2(x)\n#         x = self.backbone.layer3(x)\n#         x = self.backbone.layer4(x)\n\n#         x = self.backbone.avgpool(x)\n#         x = torch.flatten(x, 1)\n        \n#         x = self.head(x)\n#         x = self.logit(x)\n        \n#         return x","bf3d048f":"# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# net = Net(cfg)\n# net = net.to(device)\n# optimizer = optim.Adam(net.parameters(), lr=1e-3)\n\n# # Later we have to filter the invalid steps.\n# criterion = nn.MSELoss(reduction=\"none\")","7ef4dcc7":"# tr_it = iter(train_dataloader)","769c20af":"# train_dataloader","2dea6d36":"# progress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\n# losses_train = []\n\n# for itr in progress_bar:\n\n#     try:\n#         data = next(tr_it)\n#     except StopIteration:\n#         tr_it = iter(train_dataloader)\n#         data = next(tr_it)\n\n#     net.train()\n#     torch.set_grad_enabled(True)\n    \n#     # Forward pass\n#     inputs = data[\"image\"].to(device)\n#     target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n#     targets = data[\"target_positions\"].to(device)\n    \n#     outputs = net(inputs).reshape(targets.shape)\n#     loss = criterion(outputs, targets)\n\n#     # not all the output steps are valid, but we can filter them out from the loss using availabilities\n#     loss = loss * target_availabilities\n#     loss = loss.mean()\n\n#     # Backward pass\n#     optimizer.zero_grad()\n#     loss.backward()\n#     optimizer.step()\n\n#     losses_train.append(loss.item())\n\n#     if (itr+1) % cfg['train_params']['checkpoint_every_n_steps'] == 0 and not DEBUG:\n#         torch.save(model.state_dict(), f'model_state_{itr}.pth')\n    \n#     progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train[-100:])}\")","8a1cd4eb":"# torch.save(net.state_dict(), f'model_state_last.pth')","79f3f318":"# DIR_INPUT = \"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\"\n\n# SINGLE_MODE_SUBMISSION = f\"{DIR_INPUT}\/single_mode_sample_submission.csv\"\n# MULTI_MODE_SUBMISSION = f\"{DIR_INPUT}\/multi_mode_sample_submission.csv\"","6bae5aff":"# cfg = {\n#     'format_version': 4,\n#     'model_params': {\n#         'history_num_frames': 10,\n#         'history_step_size': 1,\n#         'history_delta_time': 0.1,\n#         'future_num_frames': 50,\n#         'future_step_size': 1,\n#         'future_delta_time': 0.1\n#     },\n    \n#     'raster_params': {\n#         'raster_size': [224, 224],\n#         'pixel_size': [0.5, 0.5],\n#         'ego_center': [0.25, 0.5],\n#         'map_type': 'py_semantic',\n#         'satellite_map_key': 'aerial_map\/aerial_map.png',\n#         'semantic_map_key': 'semantic_map\/semantic_map.pb',\n#         'dataset_meta_key': 'meta.json',\n#         'filter_agents_threshold': 0.5\n#     },\n    \n#     'test_data_loader': {\n#         'key': 'scenes\/test.zarr',\n#         'batch_size': 16,\n#         'shuffle': False,\n#         'num_workers': 4\n#     }\n\n# }","1c5f5eab":"# os.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\n# dm = LocalDataManager(None)","f97fc36a":"# test_cfg = cfg[\"test_data_loader\"]\n\n# # Rasterizer\n# rasterizer = build_rasterizer(cfg, dm)\n\n# # Test dataset\/dataloader\n# test_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\n# test_mask = np.load(\"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/mask.npz\")[\"arr_0\"]\n# test_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\n# test_dataloader = DataLoader(test_dataset,\n#                              shuffle=test_cfg[\"shuffle\"],\n#                              batch_size=test_cfg[\"batch_size\"],\n#                              num_workers=test_cfg[\"num_workers\"])","851bf83a":"# net.eval()\n\n# future_coords_offsets_pd = []\n# timestamps = []\n# agent_ids = []\n\n# with torch.no_grad():\n#     dataiter = tqdm(test_dataloader )\n    \n#     for data in dataiter:\n\n#         inputs = data[\"image\"].to(device)\n#         target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n#         targets = data[\"target_positions\"].to(device)\n\n#         outputs = net(inputs).reshape(targets.shape)\n        \n#         future_coords_offsets_pd.append(outputs.cpu().numpy().copy())\n#         timestamps.append(data[\"timestamp\"].numpy().copy())\n#         agent_ids.append(data[\"track_id\"].numpy().copy())","866a30df":"# device","31d1142e":"# write_pred_csv('submission.csv',\n#                timestamps=np.concatenate(timestamps),\n#                track_ids=np.concatenate(agent_ids),\n#                coords=np.concatenate(future_coords_offsets_pd))","3a7ad999":"# Visualizing an Agent","c5867c0e":"# Using EgoDataset to get dataset containing AV positions","314370bb":"# Build a new rast object and a dataset object with new cfg","b08ba40e":"# Changing the map type to Satellite","1b433c5e":"# Visualizing the positons of Autonomus Vehicles(AV)\nThere are far too many frames to look through to plot the AVs location which is why I've limited the range to 39k","399c4169":"# Plotting the trajectories","7f23168c":"# Visualizing an AV"}}