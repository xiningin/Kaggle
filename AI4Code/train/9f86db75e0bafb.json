{"cell_type":{"4d3c958b":"code","b95b7bc0":"code","410a963f":"code","32ca9d25":"code","357f5995":"code","65305a56":"code","f59031c6":"code","0b9ddfc1":"code","192a3798":"code","c824da00":"code","3e19f6a0":"code","79a4ca75":"code","57ff051d":"code","d2c3e16f":"code","d51206f3":"code","14f6b58a":"code","a2857e90":"code","9e8a6ad6":"code","90d5b39b":"code","c13dbf6a":"code","32dcd5a6":"code","f35c6ebd":"code","90d72012":"code","1bac4014":"code","af2ee2fa":"code","bb622b7a":"code","fde1e5a0":"code","e0ac59cb":"code","83d97094":"code","a940f90b":"code","07af43f6":"code","fa6ac4d0":"code","c79ed327":"code","792f6ac9":"code","d93b6b42":"code","ba2c523f":"code","3283d95d":"code","23036b15":"code","413e57a7":"code","0b1cbfb7":"code","7bdaff0f":"markdown","9df02942":"markdown","3f5472ed":"markdown","34826c87":"markdown","49dd7263":"markdown","ed58aa9c":"markdown","8b139cb9":"markdown","efc29cd4":"markdown","a69c2fa2":"markdown","101628ae":"markdown","095994d9":"markdown","734fc448":"markdown","cb142932":"markdown","b6daf8e7":"markdown"},"source":{"4d3c958b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import ensemble, metrics\n\nsns.set_style('darkgrid')\npd.options.display.float_format = '{:,.3f}'.format","b95b7bc0":"%%time\n\nparser = lambda date: pd.to_datetime(date, format='%d.%m.%Y')\n\ntrain = pd.read_csv('..\/input\/sales_train.csv', parse_dates=['date'], date_parser=parser)\ntest = pd.read_csv('..\/input\/test.csv')\nitems = pd.read_csv('..\/input\/items.csv')\nitem_cats = pd.read_csv('..\/input\/item_categories.csv')\nshops = pd.read_csv('..\/input\/shops.csv')\n\nprint('train:', train.shape, 'test:', test.shape)\nprint('items:', items.shape, 'item_cats:', item_cats.shape, 'shops:', shops.shape)","410a963f":"train.head()","32ca9d25":"test.head()","357f5995":"items.head()","65305a56":"item_cats.head()","f59031c6":"shops.head()","0b9ddfc1":"print(train['date_block_num'].max())","192a3798":"print(train['item_cnt_day'].describe())","c824da00":"train['item_cnt_day'].nlargest(25).values","3e19f6a0":"test_only = test[~test['item_id'].isin(train['item_id'].unique())]['item_id'].unique()\nprint('test only items:', len(test_only))","79a4ca75":"# drop duplicates\nsubset = ['date','date_block_num','shop_id','item_id','item_cnt_day']\nprint(train.duplicated(subset=subset).value_counts())\ntrain.drop_duplicates(subset=subset, inplace=True)","57ff051d":"# drop shops&items not in test data\ntest_shops = test.shop_id.unique()\ntest_items = test.item_id.unique()\ntrain = train[train.shop_id.isin(test_shops)]\ntrain = train[train.item_id.isin(test_items)]\n\nprint('train:', train.shape)","d2c3e16f":"from itertools import product\n\n# create all combinations\nblock_shop_combi = pd.DataFrame(list(product(np.arange(34), test_shops)), columns=['date_block_num','shop_id'])\nshop_item_combi = pd.DataFrame(list(product(test_shops, test_items)), columns=['shop_id','item_id'])\nall_combi = pd.merge(block_shop_combi, shop_item_combi, on=['shop_id'], how='inner')\nprint(len(all_combi), 34 * len(test_shops) * len(test_items))\n\n# group by monthly\ntrain_base = pd.merge(all_combi, train, on=['date_block_num','shop_id','item_id'], how='left')\ntrain_base['item_cnt_day'].fillna(0, inplace=True)\ntrain_grp = train_base.groupby(['date_block_num','shop_id','item_id'])","d51206f3":"# summary count by month\ntrain_monthly = pd.DataFrame(train_grp.agg({'item_cnt_day':['sum','count']})).reset_index()\ntrain_monthly.columns = ['date_block_num','shop_id','item_id','item_cnt','item_order']\nprint(train_monthly[['item_cnt','item_order']].describe())\n\n# trim count\ntrain_monthly['item_cnt'].clip(0, 20, inplace=True)\n\ntrain_monthly.head()","14f6b58a":"# pickup first category name\nitem_grp = item_cats['item_category_name'].apply(lambda x: str(x).split(' ')[0])\nitem_cats['item_group'] = pd.Categorical(item_grp).codes\n#item_cats = item_cats.join(pd.get_dummies(item_grp, prefix='item_group', drop_first=True))\nitems = pd.merge(items, item_cats.loc[:,['item_category_id','item_group']], on=['item_category_id'], how='left')\n\nitem_grp.unique()","a2857e90":"city = shops.shop_name.apply(lambda x: str.replace(x, '!', '')).apply(lambda x: x.split(' ')[0])\nshops['city'] = pd.Categorical(city).codes\n\ncity.unique()","9e8a6ad6":"# By shop,item\ngrp = train_monthly.groupby(['shop_id', 'item_id'])\ntrain_shop = grp.agg({'item_cnt':['mean','median','std'],'item_order':'mean'}).reset_index()\ntrain_shop.columns = ['shop_id','item_id','cnt_mean_shop','cnt_med_shop','cnt_std_shop','order_mean_shop']\nprint(train_shop[['cnt_mean_shop','cnt_med_shop','cnt_std_shop']].describe())\n\ntrain_shop.head()","90d5b39b":"# By shop,item_group\ntrain_cat_monthly = pd.merge(train_monthly, items, on=['item_id'], how='left')\ngrp = train_cat_monthly.groupby(['shop_id', 'item_group'])\ntrain_shop_cat = grp.agg({'item_cnt':['mean']}).reset_index()\ntrain_shop_cat.columns = ['shop_id','item_group','cnt_mean_shop_cat']\nprint(train_shop_cat.loc[:,['cnt_mean_shop_cat']].describe())\n\ntrain_shop_cat.head()","c13dbf6a":"# By month,shop,item At previous\ntrain_prev = train_monthly.copy()\ntrain_prev['date_block_num'] = train_prev['date_block_num'] + 1\ntrain_prev.columns = ['date_block_num','shop_id','item_id','cnt_prev','order_prev']\n\nfor i in [2,12]:\n    train_prev_n = train_monthly.copy()\n    train_prev_n['date_block_num'] = train_prev_n['date_block_num'] + i\n    train_prev_n.columns = ['date_block_num','shop_id','item_id','cnt_prev' + str(i),'order_prev' + str(i)]\n    train_prev = pd.merge(train_prev, train_prev_n, on=['date_block_num','shop_id','item_id'], how='left')\n\ntrain_prev.head()","32dcd5a6":"# By month,shop,item_group At previous\ngrp = pd.merge(train_prev, items, on=['item_id'], how='left').groupby(['date_block_num','shop_id','item_group'])\ntrain_cat_prev = grp['cnt_prev'].mean().reset_index()\ntrain_cat_prev = train_cat_prev.rename(columns={'cnt_prev':'cnt_prev_cat'})\nprint(train_cat_prev.loc[:,['cnt_prev_cat']].describe())\n\ntrain_cat_prev.head()","f35c6ebd":"train_piv = train_monthly.pivot_table(index=['shop_id','item_id'], columns=['date_block_num'], values='item_cnt', aggfunc=np.sum, fill_value=0)\ntrain_piv = train_piv.reset_index()\ntrain_piv.head()","90d72012":"# MACD At previous\ncol = np.arange(34)\npivT = train_piv[col].T\nema_s = pivT.ewm(span=4).mean().T\nema_m = pivT.ewm(span=12).mean().T\nema_l = pivT.ewm(span=26).mean().T\nmacd = ema_s - ema_l\nsig = macd.ewm(span=9).mean()\n\nema_list = []\nfor c in col:\n  sub_ema = pd.concat([train_piv.loc[:,['shop_id','item_id']],\n      pd.DataFrame(ema_s.loc[:,c]).rename(columns={c:'cnt_ema_s_prev'}),\n      pd.DataFrame(ema_m.loc[:,c]).rename(columns={c:'cnt_ema_m_prev'}),\n      pd.DataFrame(ema_l.loc[:,c]).rename(columns={c:'cnt_ema_l_prev'}),\n      pd.DataFrame(macd.loc[:,c]).rename(columns={c:'cnt_macd_prev'}),\n      pd.DataFrame(sig.loc[:,c]).rename(columns={c:'cnt_sig_prev'})], axis=1)\n  sub_ema['date_block_num'] = c + 1\n  ema_list.append(sub_ema)\n    \ntrain_ema_prev = pd.concat(ema_list)\ntrain_ema_prev.head()","1bac4014":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n\ntrain_monthly.groupby(['date_block_num']).sum().reset_index()['item_cnt'].plot(ax=ax[0])\ntrain_cat_monthly = pd.merge(train_monthly, items, on=['item_id'], how='left')\ntrain_cat_monthly.pivot_table(index=['date_block_num'], columns=['item_group'], values='item_cnt', aggfunc=np.sum, fill_value=0).plot(ax=ax[1], legend=False)","af2ee2fa":"# Price mean by month,shop,item\ntrain_price = train_grp['item_price'].mean().reset_index()\nprice = train_price[~train_price['item_price'].isnull()]\n\n# last price by shop,item\nlast_price = price.drop_duplicates(subset=['shop_id', 'item_id'], keep='last').drop(['date_block_num'], axis=1)\n\n# null price by shop,item\n'''\nmean_price = price.groupby(['item_id'])['item_price'].mean().reset_index()\nresult_price = pd.merge(test, mean_price, on=['item_id'], how='left').drop('ID', axis=1)\npred_price_set = result_price[result_price['item_price'].isnull()]\n'''\nuitem = price['item_id'].unique()\npred_price_set = test[~test['item_id'].isin(uitem)].drop('ID', axis=1)","bb622b7a":"_ = '''\n'''\nif len(pred_price_set) > 0:\n    train_price_set = pd.merge(price, items, on=['item_id'], how='inner')\n    pred_price_set = pd.merge(pred_price_set, items, on=['item_id'], how='inner').drop(['item_name'], axis=1)\n    reg = ensemble.ExtraTreesRegressor(n_estimators=25, n_jobs=-1, max_depth=15, random_state=42)\n    reg.fit(train_price_set[pred_price_set.columns], train_price_set['item_price'])\n    pred_price_set['item_price'] = reg.predict(pred_price_set)\n\ntest_price = pd.concat([last_price, pred_price_set], join='inner')\ntest_price.head()","fde1e5a0":"price_max = price.groupby(['item_id']).max()['item_price'].reset_index()\nprice_max.rename(columns={'item_price':'item_max_price'}, inplace=True)\nprice_max.head()","e0ac59cb":"train_price_a = pd.merge(price, price_max, on=['item_id'], how='left')\ntrain_price_a['discount_rate'] = 1 - (train_price_a['item_price'] \/ train_price_a['item_max_price'])\ntrain_price_a.drop('item_max_price', axis=1, inplace=True)\ntrain_price_a.head()","83d97094":"test_price_a = pd.merge(test_price, price_max, on=['item_id'], how='left')\ntest_price_a.loc[test_price_a['item_max_price'].isnull(), 'item_max_price'] = test_price_a['item_price']\ntest_price_a['discount_rate'] = 1 - (test_price_a['item_price'] \/ test_price_a['item_max_price'])\ntest_price_a.drop('item_max_price', axis=1, inplace=True)\ntest_price_a.head()","a940f90b":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                #el\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else:\n            #df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","07af43f6":"def mergeFeature(df): \n  df = pd.merge(df, items, on=['item_id'], how='left').drop('item_group', axis=1)\n  df = pd.merge(df, item_cats, on=['item_category_id'], how='left')\n  df = pd.merge(df, shops, on=['shop_id'], how='left')\n\n  df = pd.merge(df, train_shop, on=['shop_id','item_id'], how='left')\n  df = pd.merge(df, train_shop_cat, on=['shop_id','item_group'], how='left')\n  df = pd.merge(df, train_prev, on=['date_block_num','shop_id','item_id'], how='left')\n  df = pd.merge(df, train_cat_prev, on=['date_block_num','shop_id','item_group'], how='left')\n  df = pd.merge(df, train_ema_prev, on=['date_block_num','shop_id','item_id'], how='left')\n  \n  df['month'] = df['date_block_num'] % 12\n  days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n  df['days'] = df['month'].map(days).astype(np.int8)\n  \n  df.drop(['shop_id','shop_name','item_id','item_name','item_category_id','item_category_name','item_group'], axis=1, inplace=True)\n  df.fillna(0.0, inplace=True)\n  return reduce_mem_usage(df)","fa6ac4d0":"train_set = train_monthly[train_monthly['date_block_num'] >= 12]\n\ntrain_set = pd.merge(train_set, train_price_a, on=['date_block_num','shop_id','item_id'], how='left')\ntrain_set = mergeFeature(train_set)\n\ntrain_set = train_set.join(pd.DataFrame(train_set.pop('item_order'))) # move to last column\nX_train = train_set.drop(['item_cnt'], axis=1)\n#Y_train = train_set['item_cnt']\nY_train = train_set['item_cnt'].clip(0.,20.)\nX_train.head()","c79ed327":"test_set = test.copy()\ntest_set['date_block_num'] = 34\n\ntest_set = pd.merge(test_set, test_price_a, on=['shop_id','item_id'], how='left')\ntest_set = mergeFeature(test_set)\n\ntest_set['item_order'] = test_set['cnt_ema_s_prev'] #order_prev\ntest_set.loc[test_set['item_order'] == 0, 'item_order'] = 1\n\nX_test = test_set.drop(['ID'], axis=1)\nX_test.head()\n\nassert(X_train.columns.isin(X_test.columns).all())","792f6ac9":"from sklearn import linear_model, preprocessing\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\nparams={'learning_rate': 0.05,\n        'objective':'regression',\n        'metric':'rmse',\n        'num_leaves': 64,\n        'verbose': 1,\n        'random_state':42,\n        'bagging_fraction': 1,\n        'feature_fraction': 1\n       }\n\nfolds = GroupKFold(n_splits=6)\noof_preds = np.zeros(X_train.shape[0])\nsub_preds = np.zeros(X_test.shape[0])\n\nfor fold_, (trn_, val_) in enumerate(folds.split(X_train, Y_train, X_train['date_block_num'])):\n    trn_x, trn_y = X_train.iloc[trn_], Y_train[trn_]\n    val_x, val_y = X_train.iloc[val_], Y_train[val_]\n\n    reg = lgb.LGBMRegressor(**params, n_estimators=3000)\n    reg.fit(trn_x, trn_y, eval_set=[(val_x, val_y)], early_stopping_rounds=50, verbose=500)\n    \n    oof_preds[val_] = reg.predict(val_x.values, num_iteration=reg.best_iteration_)\n    sub_preds += reg.predict(X_test.values, num_iteration=reg.best_iteration_) \/ folds.n_splits","d93b6b42":"pred_cnt = sub_preds","ba2c523f":"print('RMSE:', np.sqrt(metrics.mean_squared_error(Y_train, oof_preds.clip(0.,20.))))","3283d95d":"# Plot feature importance\nfeature_importance = reg.feature_importances_\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nplt.figure(figsize=(12,6))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_train.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","23036b15":"if False:\n    x1 = train_set[train_set['date_block_num'] < 33]\n    y1 = x1['item_cnt']\n    x1 = x1.drop(['item_cnt'], axis=1)\n\n    x2 = train_set[train_set['date_block_num'] == 33]\n    y2 = x2['item_cnt']\n    x2 = x2.drop(['item_cnt'], axis=1)\n\n    reg.fit(x1.values, y1.values)\n    pred_cnt = reg.predict(x2.values)\n    print('RMSE:', np.sqrt(metrics.mean_squared_error(y2.clip(0.,20.), pred_cnt.clip(0.,20.)))) #0.20783645197081926\n\n    col = [c for c in train_set.columns if c not in ['item_cnt']]\n    feature_imp = pd.DataFrame(reg.feature_importances_, index=col, columns=[\"importance\"])\n    feature_imp.sort_values(\"importance\", ascending=False).head(5)","413e57a7":"result = pd.DataFrame({\n    \"ID\": test[\"ID\"],\n    \"item_cnt_month\": pred_cnt.clip(0. ,20.)\n})\nresult.to_csv(\"submission.csv\", index=False)","0b1cbfb7":"print(len(pred_cnt[pred_cnt > 20]))\nresult.head(30)","7bdaff0f":"## Preprocessing","9df02942":"### Feature creation","3f5472ed":"### Discount rate","34826c87":"- evaluation","49dd7263":"### Crosstab","ed58aa9c":"## Submit","8b139cb9":"## Load data","efc29cd4":"- model","a69c2fa2":"## Aggregate","101628ae":"### Item prices","095994d9":"## Data preparation","734fc448":"## Data analysis","cb142932":"### Lags","b6daf8e7":"## Predict"}}