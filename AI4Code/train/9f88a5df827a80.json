{"cell_type":{"d3700bbd":"code","9f7f8b1e":"code","5e544e5d":"code","e59abed2":"code","57081aa5":"code","c8ae00ac":"code","5c3398bf":"code","f39283e0":"code","86b40e80":"code","1777faa7":"code","47ea9602":"code","6a9a7de0":"code","93899077":"code","74aac5ec":"code","f5fa9812":"code","981b8489":"code","126f7bbf":"code","949e4784":"code","2c669ea2":"code","2821a71a":"code","ee8c2dbd":"code","786a5f10":"code","4bbd4b9a":"code","673a1c17":"code","7e0a265d":"code","a3f9d58e":"code","241daf39":"code","d24d825b":"code","8721088e":"code","197ce8de":"code","d2e9ed61":"code","3c1af89a":"code","3950bf65":"code","6715c82e":"code","c57df9ce":"code","dd483f88":"code","5deccdc2":"code","d90595ab":"code","13ae89c8":"code","46e10893":"code","71f35bdf":"markdown","4f57f5ec":"markdown","ab232067":"markdown","0eb9d726":"markdown","e4eb189a":"markdown","ea04c74b":"markdown","1630e489":"markdown","e3e919df":"markdown","40703138":"markdown","bfd540b4":"markdown","15132db8":"markdown","e229a045":"markdown","624c9d7d":"markdown","609d438c":"markdown","75f0a96a":"markdown","55e2d2ee":"markdown","8df7e655":"markdown","3519b724":"markdown","fe157fb9":"markdown","58572008":"markdown","89bc3670":"markdown","ea2435ed":"markdown","9caf2874":"markdown"},"source":{"d3700bbd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport gc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction import stop_words\nimport re\nimport string\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, sys, tarfile\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.display import Image\n\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.text import Tokenizer,text_to_word_sequence \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input,Dense, Activation, concatenate, Embedding, Flatten, CuDNNLSTM, Bidirectional, Concatenate\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras import backend as K\nfrom keras import optimizers\n","9f7f8b1e":"kindle_reviews = pd.read_csv('..\/input\/kindle-reviews\/kindle_reviews.csv')","5e544e5d":"kindle_reviews.shape","e59abed2":"kindle_reviews.head()","57081aa5":"# Reformatting the dataframe display\n\npd.set_option('display.max_info_columns',1000)\npd.set_option('display.max_colwidth',5000)\nkindle_reviews.drop(columns = ['asin', 'helpful', 'overall', 'reviewTime', 'reviewerID', 'reviewerName', 'unixReviewTime'],\n                    axis=1, inplace=True)\nkindle_reviews.columns = ['index','review', 'summary']\nkindle_reviews.set_index(keys = 'index',inplace=True)","c8ae00ac":"# All the model hyperparameters are defined here, considering appropriate percentile of review and summary lengths \n\nRNN_VOCAB = 10000                           # most frequent 15K words form the vocab \nMAX_SEQUENCE_REVIEW_LENGTH = 22             \nMAX_SEQUENCE_SUMMARY_LENGTH = 9\nEMBEDDING_DIM = 200                         \nEMBEDDING_FILE_PATH = \"..\/input\/glove-global-vectors-for-word-representation\/glove.6B.\" + str(EMBEDDING_DIM) + \"d.txt\"\n","5c3398bf":"\n# A list of contractions from http:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python\ncontractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\"\n}\n\n\ndef clean_text(text,remove_stopwords = True, max_len = 20):\n    '''\n    Given a text this function removes the punctuations, selected stopwords(because not, none convey some meaning and\n    removing these stop words changes the meaning of the sentence.) and returns the length of the remaining text string\n    '''\n    refined_stop_words = {}\n    if(remove_stopwords == True):\n        refined_stop_words = stop_words.ENGLISH_STOP_WORDS-{ \"not\",\"none\",\"nothing\",\"nowhere\",\"never\",\n                                                        \"cannot\",\"cant\",\"couldnt\",\"except\",\"hasnt\",\n                                                        \"neither\",\"no\",\"nobody\",\"nor\",\"without\"\n                                                           }\n    try:\n        #convert to lower case and strip regex\n        new_text = []\n        text = text.lower()\n        count = 0\n        for word in text.split():\n            if word in refined_stop_words:\n                continue\n            count += 1\n            if word in contractions: \n                new_text = new_text + [contractions[word]]\n            else: \n                new_text = new_text + [word]\n        new_text = new_text[0:max_len] if count>max_len else new_text\n        text = ' '.join(new_text)\n        regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n        text = regex.sub(\" \", text)\n        text = re.sub('\\s+', ' ', text).strip()\n        text = '<start> ' + text + ' <end>'\n        return text\n    except:\n        return \"\"","f39283e0":"kindle_reviews['summary'] = kindle_reviews.summary.apply(lambda x: clean_text(x, True, MAX_SEQUENCE_SUMMARY_LENGTH-2))\nkindle_reviews['review'] = kindle_reviews.review.apply(lambda x: clean_text(x, True, MAX_SEQUENCE_REVIEW_LENGTH-2))","86b40e80":"kindle_reviews.head()","1777faa7":"kindle_reviews.shape","47ea9602":"kindle_reviews['summary_length'] = kindle_reviews.summary.apply(lambda x: len(x.split()))\nkindle_reviews['review_length'] = kindle_reviews.review.apply(lambda x: len(x.split()))","6a9a7de0":"print(kindle_reviews.review_length.describe(percentiles = [0.25, 0.9, 0.95, 0.99]))\nkindle_reviews.review_length.hist(bins = 100,figsize = (20,10))","93899077":"print(kindle_reviews.summary_length.describe(percentiles = [0.9,0.95,0.99]))\nkindle_reviews.summary_length.hist(bins = 100,figsize = (20,10))","74aac5ec":"tokenize = Tokenizer(num_words = RNN_VOCAB, oov_token='OOV', filters = '')\ntokenize.fit_on_texts(np.hstack([kindle_reviews['summary'],kindle_reviews['review']]))\nkindle_reviews['sequence_summary'] = tokenize.texts_to_sequences(kindle_reviews['summary'])\nkindle_reviews['sequence_review'] = tokenize.texts_to_sequences(kindle_reviews['review'])","f5fa9812":"print(\"Total number of unique words = \", len(tokenize.word_index))","981b8489":"#reviews = kindle_reviews.loc[kindle_reviews['sequence_summary'].apply(lambda x: not(1 in x))]\n#reviews = kindle_reviews.loc[kindle_reviews['sequence_review'].apply(lambda x: not(1 in x))]","126f7bbf":"#craeting dataset for the model input output\ndataset = {}\ndataset['decoder_input'] = pad_sequences(kindle_reviews.sequence_summary, maxlen = MAX_SEQUENCE_SUMMARY_LENGTH, padding='post')\nkindle_reviews['sequence_summary'] = kindle_reviews.sequence_summary.apply(lambda x: x[1:])\ndataset['decoder_output'] = pad_sequences(kindle_reviews.sequence_summary, maxlen = MAX_SEQUENCE_SUMMARY_LENGTH-1, padding='post')\ndataset['encoder_input'] = pad_sequences(kindle_reviews.sequence_review, maxlen = MAX_SEQUENCE_REVIEW_LENGTH, padding='pre')","949e4784":"dataset['decoder_output'].shape, dataset['decoder_input'].shape, dataset['encoder_input'].shape","2c669ea2":"dataset['encoder_input'][0:2]","2821a71a":"kindle_val_reviews = kindle_reviews.loc[1000:1005,:].copy()\ngc.collect()","ee8c2dbd":"class Embeddings:\n    \"\"\"\n    When a corpus is passed, remove the words which are not in the global vocab(glove) and use most frequent vocab_size\n    number of words. \n    \"\"\"\n    def __init__(self, embedding_dim, vocab_size):\n        self.embedding_dim = embedding_dim\n        self.vocab_size = vocab_size\n        \n    def readEmbeddings(self, filePath):\n        \"\"\"\n        Given a filepath of word embeddings creates and returns a dictionary of word, embedding values\n        \"\"\"\n        # Create a dictionary for storing all {word, embedding values}\n        wordToEmbeddingDict = {}\n        # open the file as read only\n        file = open(filePath, encoding='utf-8')\n        # read all text\n        for line in file:\n            lineValue = line.split()\n            word = lineValue[0]\n            embedding = np.asarray(lineValue[1:],dtype = 'float32')\n            wordToEmbeddingDict[word] = embedding\n        # close the file\n        file.close()\n        return wordToEmbeddingDict\n    \n    def indexToEmbedding(self, wordToIndexDict, wordToEmbeddingDict):\n        indexToEmbeddingMatrix = np.zeros((self.vocab_size, self.embedding_dim))\n        for word, index in wordToIndexDict.items():\n            if index >= self.vocab_size:\n                break\n            if word in wordToEmbeddingDict.keys():\n                indexToEmbeddingMatrix[index] = wordToEmbeddingDict[word]\n            else:\n                indexToEmbeddingMatrix[index] = np.array(np.random.uniform(-1.0, 1.0, self.embedding_dim))\n        return indexToEmbeddingMatrix\n    \n    def indexToWord(self, wordToIndexDict):\n        return {index: word for word, index in wordToIndexDict.items()}","786a5f10":"embeddings = Embeddings(EMBEDDING_DIM, RNN_VOCAB)\nwordToEmbeddingDict = embeddings.readEmbeddings(EMBEDDING_FILE_PATH)\n\nindexToEmbeddingMatrix = embeddings.indexToEmbedding(tokenize.word_index, wordToEmbeddingDict)","4bbd4b9a":"indexToWordDict = embeddings.indexToWord(tokenize.word_index)","673a1c17":"indexToEmbeddingMatrix.shape","7e0a265d":"BATCH_SIZE = 2096\nNUM_EPOCHS = 3\nSTEPS_PER_EPOCH = 150\nLATENT_DIM = 512                           # Dimensions of LSTM output","a3f9d58e":"def get_batch_data(dataset, start, end):\n    # Decoder output will be one hot encoded values \n    # dimensions of the decoder output will be (number of samples * summary length * vocab size)\n    assert start < end\n    assert end <= dataset['encoder_input'].shape[0]\n    encoder_batch_input = dataset['encoder_input'][start:end]\n    decoder_batch_input = dataset['decoder_input'][start:end]\n    decoder_batch_output = np.zeros(((end-start), MAX_SEQUENCE_SUMMARY_LENGTH, RNN_VOCAB), dtype = 'float16')\n    for k, row in enumerate(dataset['decoder_output'][start:end]):\n        for i,value in enumerate(row):\n            if value!=0:\n                decoder_batch_output[k, i, value] = 1\n    return encoder_batch_input, decoder_batch_input, decoder_batch_output","241daf39":"#This generate method loops indefinitely on our dataset to create training batches\ndef generate_batch_data(dataset):\n    size = dataset['encoder_input'].shape[0]\n    while True:\n        start = 0\n        end = start+BATCH_SIZE\n        while True:\n            # create numpy arrays of input data\n            # and labels, from each line in the file\n            if start>=size:\n                break\n            encoder_batch_input, encoder_batch_output, decoder_batch_output = get_batch_data(dataset, start, end)\n            start = end\n            end = np.min([end+BATCH_SIZE, size])\n            yield ({'review': encoder_batch_input, \n                    'summary': encoder_batch_output},\n                   {'decoder_dense_layer': decoder_batch_output})\n","d24d825b":"review_input_layer = Input(batch_shape=(BATCH_SIZE, MAX_SEQUENCE_REVIEW_LENGTH, ), name = 'review')\nembedding_encoder_layer = Embedding(input_length = MAX_SEQUENCE_REVIEW_LENGTH,\n                          input_dim = RNN_VOCAB,\n                          output_dim = EMBEDDING_DIM,\n                          weights=[indexToEmbeddingMatrix],\n                          trainable = False,\n                          name = 'embedding_encoder',\n                          )\nembedding_review_output = embedding_encoder_layer(review_input_layer)\nencoder_lstm_layer = Bidirectional(CuDNNLSTM(LATENT_DIM, return_state=True, name = 'lstm_encoder', stateful = True), merge_mode = 'concat')\n_, forward_h, forward_c, backward_h, backward_c = encoder_lstm_layer(embedding_review_output)\nstate_h = Concatenate()([forward_h, backward_h])\nstate_c = Concatenate()([forward_c, backward_c])\nencoder_states = [state_h, state_c]\n\n\nsummary_input_layer = Input(batch_shape=(BATCH_SIZE ,MAX_SEQUENCE_SUMMARY_LENGTH, ), name = 'summary')\nembedding_decoder_layer = Embedding(#input_length = MAX_SEQUENCE_SUMMARY_LENGTH,\n                          input_dim = RNN_VOCAB,\n                          output_dim = EMBEDDING_DIM,\n                          weights=[indexToEmbeddingMatrix],\n                          trainable=False,\n                          name = 'embedding_decoder',\n                          )\nembedding_summary_output = embedding_decoder_layer(summary_input_layer)\ndecoder_lstm_layer = CuDNNLSTM(2*LATENT_DIM, return_state=True, return_sequences = True, name = 'lstm_decoder', stateful = True)\ndecoder_output, decoder_h, decoder_c = decoder_lstm_layer(embedding_summary_output,\n                                                   initial_state = encoder_states)\n\ndecoder_dense_layer = Dense(RNN_VOCAB, activation=\"softmax\", name='decoder_dense_layer')\ndecoder_dense_output =  decoder_dense_layer(decoder_output)\n\nmodel = Model([review_input_layer, summary_input_layer], decoder_dense_output) \nsgd = optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')","8721088e":"plot_model(model, to_file='model.png', show_shapes=True)\nImage(filename='model.png') ","197ce8de":"model.fit_generator(generator = generate_batch_data(dataset),use_multiprocessing=True,\n                    epochs=1, steps_per_epoch = dataset['encoder_input'].shape[0]\/\/BATCH_SIZE)","d2e9ed61":"#Always save your weights\nmodel.save_weights('summarization_weights.h5')","3c1af89a":"model.load_weights('summarization_weights.h5')","3950bf65":"# Define inference model\nencoder_inference_model = Model(review_input_layer, encoder_states)\nplot_model(encoder_inference_model, to_file='inference_encoder.png', show_shapes=True)\n\nImage(filename='inference_encoder.png') ","6715c82e":"#Exactly same decoder model is used with different input and output adjustments\n\ndecoder_state_input_h = Input(shape=(2*LATENT_DIM,))  # These states are required for feeding back to our next timestep decoder\ndecoder_state_input_c = Input(shape=(2*LATENT_DIM,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n#Now we shall reuse our decoder\nsummary_for_decoder = Input(shape=(1,))\nembedding_summary_decoder = embedding_decoder_layer(summary_for_decoder)\n\ndecoder_inference_output, decoder_states_c, decoder_states_h = decoder_lstm_layer(embedding_summary_decoder, initial_state = decoder_states_inputs)\ndecoder_states_outputs = [decoder_states_c, decoder_states_h]\noutput_prob = decoder_dense_layer(decoder_inference_output)\ndecoder_inference_model = Model(\n    [summary_for_decoder] + decoder_states_inputs,\n    decoder_states_outputs + [output_prob])","c57df9ce":"plot_model(decoder_inference_model, to_file='inference_decoder.png', show_shapes=True)\n\nImage(filename='inference_decoder.png') ","dd483f88":"def decode_sequence(input_sequence):\n    # Encode the input as state vectors. states_h and states_c for decoder init\n    encoder_states_value = encoder_inference_model.predict(input_sequence)\n    \n    # Generate empty target sequence of length 1, for decoder input  \n    # Populate the first character of target sequence with the start character.\n    target_sequence = np.zeros((1,1))\n    target_sequence[0,0] = tokenize.word_index['start']\n    # Sampling loop for a batch of sequences\n    stop_condition = False\n    decoded_sentence = []\n   \n    while not stop_condition:\n      \n        h, c, output_tokens = decoder_inference_model.predict([target_sequence] + encoder_states_value,batch_size=1)\n        # Sample a token\n        \n        sampled_word_index = np.argmax(output_tokens)#np.random.choice(np.arange(10003), p=output_tokens.flatten())\n        if sampled_word_index == 0:\n            decoded_sentence += ['<pad>']\n            continue\n        sampled_word = indexToWordDict[sampled_word_index]\n        decoded_sentence += [sampled_word]\n        \n        # Exit condition: either hit max length=\n        # or find stop character.\n        if (sampled_word == 'end' or\n           len(decoded_sentence) > MAX_SEQUENCE_SUMMARY_LENGTH):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n            target_sequence[0, 0] = sampled_word_index\n        # Update states\n        encoder_states_value = [h, c]\n    return ' '.join(decoded_sentence)","5deccdc2":"def summarize(input_seq):\n    input_seq = pad_sequences(tokenize.texts_to_sequences([clean_text(input_seq)]),MAX_SEQUENCE_REVIEW_LENGTH)\n    return decode_sequence(input_seq)","d90595ab":"summarize(\"The book was very nice, will read it again\")","13ae89c8":"summarize(\"very very bad book, didn't like it much\")","46e10893":"# Try if it's overfitting\nfor index,row in kindle_val_reviews.head(5).iterrows():\n    print(\"Input review : \" , row.review)\n    print(\"Expected review : \", row.summary)\n    print(\"Predicted output : \", summarize(row.review))\n    print(\"\\n\")","71f35bdf":"In the next notebook \n- I have implemented a pipeline for pre-processing text data\n- attention mechanism\n- beam search\n- seq2seq-vis by IBM","4f57f5ec":"## Define the model ##\n\nJust to give you an idea of the complexity of this problem, consider the following:\nour model outputs probability of each word in the vocab (10K) probabilities, for each timestep.\nThe number of timesteps for decoder are 9. and we have 982K datapoints. To store the decoder output at training step, we would require (982K \\* 9 \\* 10K) float value memory,Thus it's inefficient to be training the model, on the entire dataset. \n\nIn such cases fit_generate method of keras comes handy. We can create batches of the input and output as the model trains and the previous batch is discarded at every step of the epoch! It's implemented below\n\n### 1 Helper methods ###","ab232067":"In the inference model, we need to keep in mind following few things\n\nFirst we will encode the sequence, the encoder's output is of no use thus it will be discarded, state_c and state_h of the last timestep will be used as initial state of decoder, same as training. \n\nAlso decoder will receive one input of initial time step ('<start\\>' token to get started)\n\nDecoder inputs : [encoder states , <start\\>]\n\n1) decoder RNN will output one word at a time,(output dim :{ 1, 1, RNN_VOCAB}\n\n2) from argmax index to word mapping could be used to find the output word,\n\nFor the next timestep we need the decoder's states as well as an output of decoder LSTM\n\n3) Next we need to provide our decoder with the decoder states and the output word again\n\nWe need to encapsulate decoder in separate block, is because it is gonna run recursively\n","0eb9d726":"# The most important part:\nWhat did I learn and improvements!\n1. Can't expect results without considerable number of epochs and good sized data, for this problem.\nFor NMT, many small datasets have proven to generate decent translation with the same architecture, cause at the end of the day it's a bit simpler problem where one to one mapping of token from one language to other is possible. In abstractive summarization, it cant be done.\n2. OOV, <start\\> and <end\\> could be mostly generated and it can be safely concluded that the model has learnt to give these words higher importance\/ more probability score simply cause they occur almost in every sentence. This doesn't mean model is incorrectly implemented, but simply it needs to be trained on more data and for more epochs\n3. During the implementation of the LSTMs, using CUDNNLSTM makes more sense as it's faster in computations\n4. Longer sentences, need to be avoided for this problem as LSTMs could easily run into vanishing gradient problem\n5. If batch_size is known in advance, stateful lstms could be implemented and it improves performance marginally\n6. Bidirectional wrapper returns 5 outputs, lstms output, the forward and backward hidden and cell states. The unused variables should not be names to save memory\n7. Hyperparameters that seem to have considerable impact: \n    1. batch_size\n    2. learning rate\n    3. lstm units\/ size of context vector\n    4. number of epochs\n8. Using categorical cross entropy could potentially save a lot of memory instead of one hot encoding of decoder's softmax output","e4eb189a":"## Try it !","ea04c74b":"Length of the reviews and summary is essential to consider while deciding hyperparameters such as sequence length","1630e489":"For this problem, we are using kindle reviews dataset from kaggle. There are many other features of this dataset which might not be relevant to the problem at hand. The reviewText and summary are used here. The dataset contains approximately million datapoints for us to train model on.","e3e919df":"## Text preprocessing ##\n1. lowercasing\n2. stop words removal : a new dictionary of stop words is created to preserve the meaning of the original text\n3. contractions \n4. punctuations removal\n5. Unnecessary white space \n","40703138":"## Embeddings ##","bfd540b4":"## Exploration ##","15132db8":"## The problem domain ##\n[[ go back to the top ]](#Table-of-contents)\n\nIn the modern Internet age, textual data is ever increasing. Need some way to condense this data\nwhile preserving the information and meaning.We need to summarize textual data for that. Text\nsummarization is the process of automatically generating natural language summaries from an input\ndocument while retaining the important points. It would help in easy and fast retrieval of information.\n\nThere are two prominent types of summarization algorithms.\n\n\u2022 Extractive summarization systems form summaries by copying parts of the source text\nthrough some measure of importance and then combine those part\/sentences together to\nrender a summary. Importance of sentence is based on linguistic and statistical features.\n\n\u2022 Abstractive summarization systems generate new phrases, possibly rephrasing or using\nwords that were not in the original text. Naturally abstractive approaches are harder. For\nperfect abstractive summary, the model has to first truly understand the document and then\ntry to express that understanding in short possibly using new words and phrases. Much\nharder than extractive. Has complex capabilities like generalization, paraphrasing and incorporating real-world knowledge.\n\nMajority of the work has traditionally focussed on extractive approaches due to the easy of defining\nhard-coded rules to select important sentences than generate new ones. Also, it promises grammatically correct and coherent summary. But too many rules to keep track of!! Where as an abstraction is an active area of research as they often don\u2019t summarize long and complex texts well\n","e229a045":"### 2.2 Decoder\n- 3 inputs to the decoder are, previous time step hidden state, previous time step cell state and input at current timestep\n- For first time step we have though vector as, hidden state and cell state as inputs from previous time step\n- After that, decoder cell is called recursively","624c9d7d":" ## Model architecture ##\n The architecture implemented here is similar for the one used in translation cited by https:\/\/arxiv.org\/pdf\/1409.3215.pdf\n1. Glove embedding \n2. seq2seq encoder decoder architecture with LSTM layer.\n\n\n![Image of encoder-decoder architecture ](https:\/\/cdn-images-1.medium.com\/max\/2560\/1*nYptRUTtVd9xUjwL-cVL3Q.png)\n","609d438c":"Dataset is fairly large for the available processing time. Thus we will be training in batches. ","75f0a96a":"The decoder input, should have a start and end token, and thus we add it to our vocab","55e2d2ee":"## Define Input output to the model ## ","8df7e655":"## Table of contents\n\n1. [The problem domain](#The-problem-domain)\n2. [Model architecture](#Model-architecture)\n3. [Exploration](#Exploration)\n4. [Text preprocessing](#Text-preprocessing)\n5. [Define Input output to the model](#Define-Input-output-to-the-model)\n6. [Embeddings](#Embeddings)\n7. [Define the model](#Define-the-model)\n\n    - [Bonus: Testing our data](#Bonus:-Testing-our-data)\n\n8. [Step 4: Exploratory analysis](#Step-4:-Exploratory-analysis)\n\n9. [Step 5: Classification](#Step-5:-Classification)\n\n    - [Cross-validation](#Cross-validation)\n\n    - [Parameter tuning](#Parameter-tuning)\n\n10. [Step 6: Reproducibility](#Step-6:-Reproducibility)\n\n11. [Conclusions](#Conclusions)\n\n12. [Further reading](#Further-reading)\n\n13. [Acknowledgements](#Acknowledgements)","3519b724":"When same model was trained on NMT problem for 1 epoch and small dataset, it generated, repeating words and additional training improved performance greatly. Thus it's safe to conclude that given more time and data, this model could generate better summaries!","fe157fb9":"Although there are 291K unique words in the dataset, due to constraints we are using, the most frequent 10K words to form our vocab. In short, 174K tokens would be treated as unknown words and hence, many times our model will tend to be biased to generate UNK token just cause of it's sheer frequency in the dataset, considering it to be more probable.\n\nOne way is to remove the samples having OOV token, which is commented below","58572008":"Let's tokenize this data using keras tokenizer API ","89bc3670":"With models like BERT (https:\/\/arxiv.org\/abs\/1810.04805) and Tranformers (https:\/\/arxiv.org\/abs\/1706.03762) the quality of language generation models has improved many folds, which will be explored in the next notebook. But it's essential to understand the drawbacks of seq2seq model as a baseline to be able to appreciate the ideas of improvements in the later models and and thus I have tried it below! :)","ea2435ed":"## 2. Inference model\nLike training we have 2 separate models for encoder and decoder. \n### 2.1 Encoder\nFrom encoder we extract the thought vector to feed to the decoder.","9caf2874":"Every encoder, decoder input - summary is augmented with '<start\\>' and preceeded by '<end\\>' "}}