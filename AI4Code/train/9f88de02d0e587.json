{"cell_type":{"21d283a1":"code","98944713":"code","a2551eb6":"code","a0796837":"code","2d20b87f":"code","c0a6ef1f":"code","0881f6ff":"code","d4a4f282":"code","ade9d152":"code","0b38e79b":"code","1d11fcc7":"code","22a87dbc":"code","b0c239ca":"code","a9f4b1e5":"code","ca5bc8a6":"markdown","4e71e08c":"markdown","031a6c9f":"markdown","37ad4a41":"markdown","7dfca2b2":"markdown","657aaca8":"markdown","1fd78645":"markdown","188deae9":"markdown","bc105a17":"markdown","d1e61722":"markdown"},"source":{"21d283a1":"import os\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img, ImageDataGenerator\nfrom PIL import Image \n\n# set the necessary directories\nimg_dir = '..\/input\/arctic-sea-ice-image-masking\/Images\/'\n\nimg_filenames = os.listdir(img_dir)\nimg_names = [s.split('.')[0] for s in img_filenames]\n\nimg_ext = '.jpg'","98944713":"mask_lib = {55:0, #ice free\n            1:0, #<1\/10 (open water)\n            2:0, #bergy water\n            10:1, #1\/10\n            12:1, #1\/10-2\/10\n            13:1, #1\/10-3\/10\n            20:1, #2\/20\n            23:1, #2\/20-3\/10\n            24:2, #2\/20-4\/10\n            30:2, #...\n            34:2,\n            35:2,\n            40:2,\n            45:2,\n            46:3,\n            50:3,\n            56:3,\n            57:3,\n            60:3,\n            67:3,\n            68:4, #...\n            70:4, #7\/10\n            78:4, #7\/10-8\/10\n            79:4, #7\/10-9\/10\n            80:4, #8\/10\n            89:4, #8\/10-9\/10\n            81:5, #8\/10-10\/10\n            90:5, #9\/10\n            91:5, #9\/10-10\/10\n            92:6, #10\/10 - fast ice\n            100:7, #land\n            99:7, #unknown - there is nothing in this class for this dataset\n           }\n\n#define a colormap for the mask\nn_colors=8\nice_colors = n_colors-1\njet = plt.get_cmap('jet', ice_colors)\nnewcolors = jet(np.linspace(0, 1, ice_colors))\nblack = np.array([[0, 0, 0, 1]])\nwhite = np.array([[1, 1, 1, 1]])\nnewcolors = np.concatenate((newcolors, black), axis=0) #land will be black\ncmap = ListedColormap(newcolors)","a2551eb6":"#function to map mask values according to above library\ndef map_mask(mask, lib):\n    new_mask = mask.copy()\n    for key, val in lib.items():#map the elements of the array to their new values according to the library\n        new_mask[mask==key]=val\n    return new_mask\n\n#function to calculate the value counts over all pixels in an image (fed in as a numpy array)\ndef bincount_2d(arr, max_int):\n    counts_full = [0 for n in range(max_int)]\n    for row in arr:\n        counts = np.bincount(row).tolist()#get the counts for the row\n        pad = [0 for n in range(max_int-len(counts))]\n        counts = counts + pad #add extra zeroes to account for colors above the max in the row\n        counts_full = [counts_full[n] + counts[n] for n in range(max_int)]\n    return(counts_full)\n\n# convert all mask files from SIGRID 3 format to simplified\nmask_dir = '..\/input\/arctic-sea-ice-image-masking\/Masks\/'\nmask_ext = '-mask.png'\nnew_mask_ext = '-mask-mod.png'\ndat = []#list that will hold information on the masks\n\nfor img_name in img_names:\n    name = mask_dir + img_name\n    # importing the image  \n    mask = Image.open(name + mask_ext)\n\n    # converting mask\n    mask = np.array(mask)#convert to numpy\n    new_mask = map_mask(mask, mask_lib)#map values\n    \n    #update dataframe\n    name = img_name.split('-')  \n    d = [img_name, \n         name[0][1:],  #patch id\n         name[1][0:4], #year\n         name[1][4:6], #month\n         name[1][6:8], #day\n         name[1][8:10]]#hour\n    \n    counts = bincount_2d(new_mask, n_colors) #values counts of the class of ice over all pixels in the image\n    d.extend(counts)\n    dat.append(d)\n    \n    # exporting the image \n    new_mask = Image.fromarray(new_mask)#convert back to image\n    new_mask.save('.\/' + img_name + new_mask_ext, 'PNG') \n\nmask_dir = '.\/'#update mask directory and extension\nmask_ext= new_mask_ext\n\n#create dataframe of mask information\nmask_df = pd.DataFrame(dat, columns = ['name', 'patch_id', 'year', 'month', 'day', 'hour', \n                            'conc_0', 'conc_1', 'conc_2', 'conc_3', 'conc_4', 'conc_5', 'conc_6',  \n                            'conc_land'])\n\n#plot realtive frequency of ice concentrations in images\ncounts = mask_df.iloc[:,6:].sum()\nnorm = counts.sum()\nprobs = counts\/norm*100\n\nplt.figure(figsize=(8,5))\nprobs.plot(kind='bar')\nplt.ylabel('Fraction of Pixel Values (%)')\nplt.grid()","a0796837":"mask_df['conc_minor']=mask_df[['conc_1', #lists the total concentration of under-represented ice classes\n                               'conc_2', \n                               'conc_3', \n                               'conc_4', \n                              ]].sum(axis=1)\n\nn_pixels = mask_df.iloc[0, 6:].sum(axis=0)#total number of pixels in each image\nover_sample_names = mask_df[mask_df['conc_minor']\/n_pixels>0.3] #we will over-sample these images of the under-represented classes\nover_sample_names = over_sample_names['name'].values.tolist()","2d20b87f":"class_max = mask_df.iloc[:,6:-1].idxmax(axis=1) #category of the most common class in the image. We will stratify our train test split by this\nclass_max.value_counts()","c0a6ef1f":"from sklearn.model_selection import train_test_split\nimport tensorflow_addons as tfa\n# pick which images we will use for testing and which for validation\nnames = mask_df['name'].values\ntrain_names, validation_names, train_max, validation_max = train_test_split(img_names, class_max, \n                                                                            train_size=0.8, test_size=0.2, \n                                                                            random_state=0, stratify=class_max)\n\n#add over-sampled images to the train dataset\ntrain_over_sample_names = np.array([name for name in train_names if name in over_sample_names])\nN_over_sample = int(len(train_names)\/1.5) #number of additional samples to add\nids = np.arange(len(train_over_sample_names))\nchoices = np.random.choice(ids, N_over_sample)#an additional set of images to add on to the train names\nadd_train_names = train_over_sample_names[choices].tolist()\ntrain_names.extend(add_train_names)\n\nIMG_SIZE = (256, 256)\n\n#function to read image and mask from file\ndef read_image(image_name):\n    image = tf.io.read_file(img_dir + image_name + img_ext)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, IMG_SIZE)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    \n    mask = tf.io.read_file(mask_dir + image_name + mask_ext)\n    mask = tf.image.decode_image(mask, channels=1, expand_animations=False)\n    mask = tf.image.resize(mask, IMG_SIZE)\n    mask = tf.cast(mask, tf.uint8)\n    return image, mask\nimport random\n\n#image augmentation function to randomly flip and rotate each image and corresponding mask\ndef augment_image(image, mask):\n    n = tf.random.uniform([], 0,1)\n    if n<0.5: \n        image = tf.image.flip_left_right(image)\n        mask = tf.image.flip_left_right(mask)\n        \n    n = tf.random.uniform([], 0,1)\n    if n<0.5: \n        image = tf.image.flip_up_down(image)\n        mask = tf.image.flip_up_down(mask)\n    \n    #rotate image randomly in the range of +-5 degrees\n    n = tf.random.uniform([], -1,1)\n    image = tfa.image.rotate(image, np.pi\/36*n, fill_mode='constant', fill_value=0)#add black to rotated corners\n    mask = tfa.image.rotate(mask, np.pi\/36*n, fill_mode='constant', fill_value=7)#make this black space correspond to land\n    return image, mask\n\nTRAIN_LENGTH = int(len(train_names))\nVAL_LENGTH = int(len(validation_names))\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nSTEPS_PER_EPOCH = TRAIN_LENGTH \/\/ BATCH_SIZE\n\nds_train = tf.data.Dataset.from_tensor_slices((train_names))#read filenames\nds_train = ds_train.map(read_image, num_parallel_calls=tf.data.AUTOTUNE) #convert filenames to stream of images\/masks\nds_train = ds_train.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE) #convert filenames to stream of images\/masks\ntrain_dataset = ds_train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n\nds_val = tf.data.Dataset.from_tensor_slices((validation_names))#read filenames\nds_val = ds_val.map(read_image) #convert filenames to stream of images\/masks\nval_dataset = ds_val.batch(BATCH_SIZE)","0881f6ff":"def display(display_list):\n    fig, axs = plt.subplots(nrows=1, ncols = len(display_list), figsize=(15, 6))\n\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n    for i in range(len(display_list)):\n        axs[i].set_title(title[i])\n        if i==0:\n            axs[i].imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        else:\n            msk = axs[i].imshow(display_list[i], cmap = cmap, vmin=0, vmax=n_colors-1)\n        axs[i].axis('off')\n        \n    #plot colorbar\n    cbar = fig.colorbar(msk, ax=axs, location='right')\n    tick_locs = (np.arange(n_colors) + 0.5)*(n_colors-1)\/n_colors#new tick locations so they are in the middle of the colorbar\n    cbar.set_ticks(tick_locs)\n    cbar.set_ticklabels(np.arange(n_colors))\n    plt.show()\n\nfor image, mask in ds_train.take(1):\n    sample_image, sample_mask = image, mask\ndisplay([sample_image, sample_mask])","d4a4f282":"from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\nfrom tensorflow.keras import Model\ndef get_unet():\n    inputs = Input(shape=[IMG_SIZE[0], IMG_SIZE[1], 3])\n    conv1 = Conv2D(32, 3, 1, activation='relu', padding='same')(inputs)\n    conv1 = Conv2D(32, 3, 1, activation='relu', padding='same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    drop1 = Dropout(0.5)(pool1)\n\n    conv2 = Conv2D(64, 3, 1, activation='relu', padding='same')(drop1)\n    conv2 = Conv2D(64, 3, 1, activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    drop2 = Dropout(0.5)(pool2)\n\n    conv3 = Conv2D(128, 3, 1, activation='relu', padding='same')(drop2)\n    conv3 = Conv2D(128, 3, 1, activation='relu', padding='same')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    drop3 = Dropout(0.5)(pool3)\n\n    conv4 = Conv2D(256, 3, 1, activation='relu', padding='same')(drop3)\n    conv4 = Conv2D(256, 3, 1, activation='relu', padding='same')(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n    drop4 = Dropout(0.5)(pool4)\n\n    conv5 = Conv2D(512, 3, 1, activation='relu', padding='same')(drop4)\n    conv5 = Conv2D(512, 3, 1, activation='relu', padding='same')(conv5)\n\n    up6 = Conv2D(256, 3, activation = 'relu', padding = 'same')(UpSampling2D(size=(2, 2))(conv5))\n    merge6 = concatenate([up6, conv4], axis=3)\n    drop6 = Dropout(0.5)(merge6)\n    conv6 = Conv2D(256, 3, 1, activation='relu', padding='same')(drop6)\n    conv6 = Conv2D(256, 3, 1, activation='relu', padding='same')(conv6)\n    \n    up7 = Conv2D(128, 3, activation = 'relu', padding = 'same')(UpSampling2D(size=(2, 2))(conv6))\n    merge7 = concatenate([up7, conv3], axis=3)\n    drop7 = Dropout(0.5)(merge7)\n    conv7 = Conv2D(128, 3, 1, activation='relu', padding='same')(drop7)\n    conv7 = Conv2D(128, 3, 1, activation='relu', padding='same')(conv7)\n    \n    up8 = Conv2D(64, 3, activation = 'relu', padding = 'same')(UpSampling2D(size=(2, 2))(conv7))\n    merge8 = concatenate([up8, conv2], axis=3)\n    drop8 = Dropout(0.5)(merge8)\n    conv8 = Conv2D(64, 3, 1, activation='relu', padding='same')(drop8)\n    conv8 = Conv2D(64, 3, 1, activation='relu', padding='same')(conv8)\n    \n    up9 = Conv2D(32, 3, activation = 'relu', padding = 'same')(UpSampling2D(size=(2, 2))(conv8))\n    merge9 = concatenate([up9, conv1], axis=3)\n    drop9 = Dropout(0.5)(merge9)\n    conv9 = Conv2D(32, 3, 1, activation='relu', padding='same')(drop9)\n    conv9 = Conv2D(32, 3, 1, activation='relu', padding='same')(conv9)\n\n    conv10 = Conv2D(n_colors, 1, 1, activation='softmax')(conv9) #softmax converts the output to a list of probabilities that must sum to 1\n\n    model = Model(inputs=inputs, outputs=conv10)\n    return model\n\nmodel = get_unet() \ntf.keras.utils.plot_model(model, show_shapes=True)","ade9d152":"#function to generate a mask from the model predictions\ndef create_mask(pred_mask, ele=0):\n    pred_mask = tf.argmax(pred_mask, axis=-1)#use the highest proabbaility class as the prediction\n    pred_mask = pred_mask[..., tf.newaxis]\n    return pred_mask[ele]\n\n#helper functions to plot image, mask, and predicted mask while training\ndef show_predictions(dataset=None, num=1, ele=0):\n    if dataset:\n        for image, mask in dataset.take(num):\n            pred_mask = model.predict(image)\n            display([image[ele], mask[ele], create_mask(pred_mask, ele)])\n    else:\n        display([sample_image, sample_mask, create_mask(model.predict(sample_image[tf.newaxis, ...]))])\n\n#function to display loss during training\ndef plot_loss_acc(loss, val_loss, epoch):#, acc, val_acc, epoch):\n    \n    epochs = range(epoch+1)\n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,5))\n\n    ax.plot(epochs, loss, 'r', label='Training loss')\n    ax.plot(epochs, val_loss, 'bo', label='Validation loss')\n    ax.set_title('Training and Validation Loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss Value')\n    ax.legend()\n    plt.show()\n    \n#callback to clear output and show predictions\nfrom IPython.display import clear_output\n\nclass DisplayCallback(tf.keras.callbacks.Callback):\n    def on_train_begin(self, logs=None):\n        self.loss = []\n        self.val_loss = []\n    \n    def on_epoch_end(self, epoch, logs=None):\n        clear_output(wait=True)\n        \n        self.loss.append(logs['loss'])\n        self.val_loss.append(logs['val_loss'])\n        \n        show_predictions()\n        plot_loss_acc(self.loss, self.val_loss, epoch)\n        \n#callback to reduce learning rate when loss plateaus\nlr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=8, verbose=1,)\n\n#Define IoU metric (by stack overflow user HuckleberryFinn)\nclass UpdatedMeanIoU(tf.keras.metrics.MeanIoU):\n    def __init__(self,\n               y_true=None,\n               y_pred=None,\n               num_classes=None,\n               name=None,\n               dtype=None):\n        super(UpdatedMeanIoU, self).__init__(num_classes = num_classes,name=name, dtype=dtype)\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_pred = tf.math.argmax(y_pred, axis=-1)\n        return super().update_state(y_true, y_pred, sample_weight)\n\n# Create a callback that saves the model's weights\ncheckpoint_path = \"training\/cp-{epoch:04d}.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create a callback that saves the model's weights every 5 epochs\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path, \n    verbose=1, \n    save_weights_only=True,\n    save_freq=5*BATCH_SIZE)","0b38e79b":"#train model\nmodel=get_unet()\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), \n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n              metrics=['sparse_categorical_accuracy', UpdatedMeanIoU(num_classes=n_colors)])\n\nEPOCHS = 100\nVAL_SUBSPLITS = 5\nVALIDATION_STEPS = VAL_LENGTH\/\/BATCH_SIZE\/\/VAL_SUBSPLITS\n\nmodel_history = model.fit(train_dataset, \n                          epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          validation_steps=VALIDATION_STEPS,\n                          validation_data=val_dataset,\n                          callbacks=[DisplayCallback(), lr_callback, cp_callback])","1d11fcc7":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n\nloss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\n\nepochs = range(EPOCHS)\n\nax[0].plot(epochs, loss, 'r', label='Training')\nax[0].plot(epochs, val_loss, 'bo', label='Validation')\nax[0].set_title('Training and Validation Loss')\nax[0].set_xlabel('Epoch')\nax[0].set_ylabel('Loss Value')\nax[0].legend()\n\nIoU_key = list(model_history.history.keys())[2]\nacc = model_history.history[IoU_key]\nval_acc = model_history.history['val_'+IoU_key]\n\nax[1].plot(epochs, acc, 'r', label='Training')\nax[1].plot(epochs, val_acc, 'bo', label='Validation')\nax[1].set_title('Training and Validation IoU')\nax[1].set_xlabel('Epoch')\nax[1].set_ylabel('IoU Value')\nax[1].legend()\nplt.show()","22a87dbc":"#load weights for checkpoint 51\n# print(os.listdir(checkpoint_dir))\nmodel.load_weights(checkpoint_dir + '\/cp-0051.ckpt')\nscores = model.evaluate(val_dataset, verbose=0)\nprint('Final Model Validation Scores')\nprint('Loss: {:.3f}'.format(scores[0]))\nprint('Accuracy: {:.3f}'.format(scores[1]))\nprint('IoU: {:.3f}'.format(scores[2]))","b0c239ca":"show_predictions(val_dataset, num=10, ele=3)","a9f4b1e5":"#plot a confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ndef get_cm(model, val_ds):\n    cm = np.zeros((8,8))\n    for img_batch, mask_batch in val_dataset:\n        y_pred = []\n        y_true = []\n        pred_batch = model.predict(img_batch)\n        pred_batch = tf.argmax(pred_batch, axis=-1)#take the highest probability as the prediction for each pixel\n        for n, pred in enumerate(pred_batch):\n            pred = np.array(pred).flatten() #flattened array of predicted pixels for each image\n            mask = np.array(mask_batch[n, ...]).flatten() #flattened array of mask pixels for the image\n            y_pred.extend(pred)\n            y_true.extend(mask)\n        cm = cm + confusion_matrix(y_true, y_pred)\n    return cm\n\ncm = get_cm(model, val_dataset)\nplt.figure(figsize=(12,8))\nsns.heatmap(cm.astype(int), annot=True, fmt=\"d\")\nplt.title('Confusion matrix')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","ca5bc8a6":"# Tensorflow Input Pipeline","4e71e08c":"We see that classes for 0%, 90%, and 100% ice concentration and the land class make up most of the pixels. To deal with this class imbalance in our model, we will over-sample images that contain more than 30% of the minority classes.","031a6c9f":"This model is an adapted version U-NET from the Dstl Satellite Imagery Feature Detection Kaggle competition. That competition also aimed to classify pixels in satelite images, so this model architucture might be a good fit here too. https:\/\/www.kaggle.com\/drn01z3\/end-to-end-baseline-with-u-net-keras","37ad4a41":"# Train Model","7dfca2b2":"# Defining Masks","657aaca8":"Masks are encoded in SIGRID-3 format. See here for more information: https:\/\/library.wmo.int\/doc_num.php?explnum_id=9270\n\nWe will map the ice concentratoin codes according to the following library to simplify the classes","1fd78645":"# Display Sample Image and Mask","188deae9":"# Define Model","bc105a17":"# Imports","d1e61722":"Run through and convert all masks from SIGRID-3 to simplified. Also store the pixel class counts for each mask in a dataframe."}}