{"cell_type":{"9069b5a3":"code","f175cb82":"code","7f6077e1":"code","577d4b75":"code","1138fdc7":"code","e463ca28":"code","f4ec4383":"code","a463bf12":"code","b1358d26":"code","c32333bb":"code","6607788d":"code","868a14ff":"code","696e2139":"code","66ecfbfe":"code","79ac8a3d":"code","d269cfe9":"code","09481a7b":"code","5fdf2dca":"code","8ac78567":"code","a4eb8e58":"code","a9ec19a5":"code","baf02986":"code","a4635338":"code","c07993fc":"code","a3d21dee":"code","5fb41ae8":"code","10ac7bde":"code","afa557a9":"code","06b2a4a7":"code","aed0f17a":"code","c29529b7":"code","1ac5f86a":"code","bcc9a570":"code","d1baf56f":"code","6bc2a5ac":"code","6a902880":"code","06030b14":"code","d3f18f63":"code","0be495d8":"code","b2d0394a":"code","668163c3":"code","b2d1cb5a":"code","faca789a":"code","9aee3651":"code","ba33412c":"code","a8b5b640":"code","415447c1":"code","28a85b27":"code","6861ad8b":"code","961d1888":"code","f3b159cc":"code","ea61093b":"code","a33041f7":"code","40f60f37":"code","ba4ceebd":"markdown","a2d26113":"markdown","3d3cd21d":"markdown","36255834":"markdown","32eab49c":"markdown","e1893ce0":"markdown","07b4517f":"markdown","cba6d616":"markdown","aa905d38":"markdown","e4c59540":"markdown","4e96727c":"markdown","86592a63":"markdown","90b45859":"markdown","f9363f05":"markdown","dbffd042":"markdown","b323b2a7":"markdown","907d1b02":"markdown","395c44f4":"markdown","8d124d83":"markdown","99c6af5a":"markdown","599c4cc3":"markdown","fb2af2d3":"markdown","0caa3c3d":"markdown","905e6d58":"markdown","2520347f":"markdown","ae21c2a2":"markdown","b01e0e94":"markdown","e02af68a":"markdown","f458e60c":"markdown","b3968e78":"markdown","fd99c5a4":"markdown","1383d736":"markdown","0e71615d":"markdown","9459c73f":"markdown","2cd9fe23":"markdown","2b4f320a":"markdown","d18617d5":"markdown"},"source":{"9069b5a3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport string\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.manifold import Isomap\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nnltk.download('vader_lexicon')\nfrom sklearn.cluster import KMeans\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AR,AutoReg\nfrom nltk.util import ngrams\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport spacy as sp\nnlps = sp.load('en')\nimport random\nplt.rc('figure',figsize=(17,13))","f175cb82":"r_data = pd.read_csv('\/kaggle\/input\/reddit-wallstreetsbets-posts\/reddit_wsb.csv')\nr_data = r_data[pd.to_datetime(r_data.timestamp).dt.year>=2021]\nr_data.head(3)","7f6077e1":"title_data = r_data[['title','timestamp']].copy()\nbody_data = r_data[['body','timestamp']].copy()\nbody_data = body_data.dropna()\ntitle_data = title_data.dropna()\n\n\ntitle_data.title =title_data.title.str.lower()\nbody_data.body =body_data.body.str.lower()\n\n#Remove handlers\ntitle_data.title = title_data.title.apply(lambda x:re.sub('@[^\\s]+','',x))\nbody_data.body   = body_data.body.apply(lambda x:re.sub('@[^\\s]+','',x))\n\n# Remove URLS\ntitle_data.title = title_data.title.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\nbody_data.body   = body_data.body.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n\n# Remove all the special characters\ntitle_data.title = title_data.title.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\nbody_data.body   = body_data.body.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n\n#remove all single characters\ntitle_data.title = title_data.title.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\nbody_data.body   = body_data.body.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n\n# Substituting multiple spaces with single space\ntitle_data.title = title_data.title.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\nbody_data.body   = body_data.body.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n\n\n\n#Remove Time From Timestamp\ntitle_data.timestamp = pd.to_datetime(title_data.timestamp).dt.date\nbody_data.timestamp = pd.to_datetime(body_data.timestamp).dt.date","577d4b75":"sid = SIA()\nbody_data['sentiments']           = body_data['body'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\nbody_data['Positive Sentiment']   = body_data['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \nbody_data['Neutral Sentiment']    = body_data['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\nbody_data['Negative Sentiment']   = body_data['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n\nbody_data.drop(columns=['sentiments'],inplace=True)\n\n\ntitle_data['sentiments']           = title_data['title'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\ntitle_data['Positive Sentiment']   = title_data['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \ntitle_data['Neutral Sentiment']    = title_data['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\ntitle_data['Negative Sentiment']   = title_data['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n\ntitle_data.drop(columns=['sentiments'],inplace=True)","1138fdc7":"body_data['# Of Words']          = body_data['body'].apply(lambda x: len(x.split(' ')))\nbody_data['# Of StopWords']      = body_data['body'].apply(lambda x: len([word for word in x.split(' ') if word in list(STOPWORDS)]))\nbody_data['Average Word Length'] = body_data['body'].apply(lambda x: np.mean(np.array([len(va) for va in x.split(' ') if va not in list(STOPWORDS)])))\n\ntitle_data['# Of Words']          = title_data['title'].apply(lambda x: len(x.split(' ')))\ntitle_data['# Of StopWords']      = title_data['title'].apply(lambda x: len([word for word in x.split(' ') if word in list(STOPWORDS)]))\ntitle_data['Average Word Length'] = title_data['title'].apply(lambda x: np.mean(np.array([len(va) for va in x.split(' ') if va not in list(STOPWORDS)])))\n","e463ca28":"title_data['# Of Times Currency Was Mentioned']          = title_data['title'].apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'MONEY' ]))\ntitle_data['# Of Organizations Mentioned']           = title_data['title'].apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'ORG' ]))\n\nprint('Processed Title DataFrame')\nbody_data['# Of Times Currency Was Mentioned']          = body_data['body'].apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'MONEY' ]))\nbody_data['# Of Organizations Mentioned']           = body_data['body'].apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'ORG' ]))\nprint('Processed Body DataFrame')\n","f4ec4383":"plt.subplot(2,1,1)\nplt.title('Distriubtion Of Sentiments Across Our Posts',fontsize=19,fontweight='bold')\nsns.kdeplot(title_data['Negative Sentiment'],bw_method=0.1)\nsns.kdeplot(title_data['Positive Sentiment'],bw_method=0.1)\nsns.kdeplot(title_data['Neutral Sentiment'],bw_method=0.1)\nplt.subplot(2,1,2)\nplt.title('CDF Of Sentiments Across Our Posts',fontsize=19,fontweight='bold')\nsns.kdeplot(title_data['Negative Sentiment'],bw_method=0.1,cumulative=True)\nsns.kdeplot(title_data['Positive Sentiment'],bw_method=0.1,cumulative=True)\nsns.kdeplot(title_data['Neutral Sentiment'],bw_method=0.1,cumulative=True)\nplt.xlabel('Sentiment Value',fontsize=19)\nplt.show()","a463bf12":"#Sorting And Feature Engineering\nf_data = title_data.sort_values(by='timestamp')\nft_data=f_data.copy()\nft_data = ft_data.rename(columns={'timestamp':'date'})\nft_data['year']         = pd.DatetimeIndex(ft_data['date']).year\nft_data['month']        = pd.DatetimeIndex(ft_data['date']).month\nft_data['day']          = pd.DatetimeIndex(ft_data['date']).day\nft_data['day_of_year']  = pd.DatetimeIndex(ft_data['date']).dayofyear\nft_data['quarter']      = pd.DatetimeIndex(ft_data['date']).quarter\nft_data['season']       = ft_data.month%12 \/\/ 3 + 1\n","b1358d26":"f_data=f_data.reset_index().drop(columns=['index'])\nf_data = f_data.rename(columns={'timestamp':'date'})\n\npartitions = []\npartitions.append(f_data.loc[44:np.round(len(f_data)\/3,0)-1,:])\npartitions.append(f_data.loc[np.round(len(f_data)\/3,0):2*int(len(f_data)\/3)-1,:])\npartitions.append(f_data.loc[2*np.round(len(f_data)\/3,0):3*int(len(f_data)\/3)-1,:])\n\n\n\nneg_part_means =[]\nneg_part_std   =[]\npos_part_means =[]\npos_part_std   =[]\nfor part in partitions:\n    neg_part_means.append(part['Negative Sentiment'].mean())\n    neg_part_std.append(part['Negative Sentiment'].std())\n    pos_part_means.append(part['Positive Sentiment'].mean())\n    pos_part_std.append(part['Positive Sentiment'].std())\n    \nres_df = pd.DataFrame({'Positive Sentiment Mean':pos_part_means,'Negative Sentiment Mean':neg_part_means,'Positive Sentiment SD':pos_part_std,'Negative Sentiment SD':neg_part_std},\n                     index = [f'Partition_{i}' for i in range(1,4)])\n\n\ndef highlight_greater(x):\n    temp = x.copy()\n    temp = temp.round(0).astype(int)\n    m1 = (temp['Partition_1_Mean'] == temp['Partition_2_Mean'])\n    m2 = (temp['Partition_1_SD'] == temp['Partition_2_SD'])\n    m3 = (temp['Partition_1_Mean'] < temp['Partition_2_Mean']+3) & (temp['Partition_1_Mean'] > temp['Partition_2_Mean']-3)\n    m4 = (temp['Partition_1_SD'] < temp['Partition_2_SD']+3) & (temp['Partition_1_SD'] > temp['Partition_2_SD']-3)\n\n    df1 = pd.DataFrame('background-color: ', index=x.index, columns=x.columns)\n    #rewrite values by boolean masks\n    df1['Partition_1_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_2_Mean'])\n    df1['Partition_1_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_2_Mean'])\n    df1['Partition_1_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_Mean'])\n\n    df1['Partition_1_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_2_SD'])\n    df1['Partition_1_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_2_SD'])\n    df1['Partition_1_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_SD'])\n\n    return df1\n\n\n\n#res_df.style.apply(highlight_greater,axis=None)\nres_df = res_df.T\nres_df = pd.DataFrame(res_df.values,columns=res_df.columns,index=['Positive Sentiment','Negative Sentiment','Positive Sentiment','Negative Sentiment'])\nres_df = pd.concat([res_df.iloc[:2,:],res_df.iloc[2:,:]],axis=1)\nres_df.columns = ['Partition_1_Mean','Partition_2_Mean','Partition_3_Mean','Partition_1_SD','Partition_2_SD','Partition_3_SD']\nres_df.style.apply(highlight_greater,axis=None)","c32333bb":"fig = make_subplots(rows=3, cols=2)\n\nfor idx,prt in enumerate(partitions):\n    by_date = prt.groupby(by='date').mean().reset_index()\n    fig.add_trace(\n    go.Scatter(x=by_date['date'], y=by_date['Positive Sentiment'],name=f'Positive Part {idx+1}'),\n    row=idx+1, col=1)\n    fig.add_trace(\n    go.Scatter(x=by_date['date'], y=by_date['Negative Sentiment'],name=f'Negative Part {idx+1}'),\n    row=idx+1, col=2)\n\nfig.update_layout(height=600, width=900, title_text=\"Distibution Of Daily Mean Sentiments Over Our Time Line For Each Partition\")\nfig.show()","6607788d":"fig = make_subplots(rows=4, cols=2, subplot_titles=('Observed Pos', 'Observed Neg', 'Trend Pos','Trend Neg','Seasonal Pos','Seasonal Neg','Residual Pos','Residual Neg'))\nb_date_mean = ft_data.groupby(by='date').mean().reset_index()\n\nlbl = ['Positive','Negative']\n\nfor idx,column in enumerate(['Positive Sentiment','Negative Sentiment']):\n    res = seasonal_decompose(b_date_mean[column], period=5, model='additive', extrapolate_trend='freq')\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.observed)), y=res.observed,name='{} Observed'.format(lbl[idx])),\n    row=1, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.trend)), y=res.trend,name='{} Trend'.format(lbl[idx])),\n    row=2, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.seasonal)), y=res.seasonal,name='{} Seasonal'.format(lbl[idx])),\n    row=3, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.resid)), y=res.resid,name='{} Residual'.format(lbl[idx])),\n    row=4, col=idx+1)\n            \nfig.update_layout(height=600, width=900, title_text=\"Decomposition Of Our Sentiments into Trend,Level,Seasonality and Residuals\")\nfig.show()","868a14ff":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\n\nax[0].set_title('Positive Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(b_date_mean['Positive Sentiment'],ax=ax[0],lw=3)\nax[1].set_title('Negative Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(b_date_mean['Negative Sentiment'],ax=ax[1],color='tab:red',lw=3)\nplt.show()","696e2139":"b_date_mean = ft_data.groupby(by='date').mean().reset_index()\nb_date_std = ft_data.groupby(by='date').std().reset_index()\n\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Daily Average Positive Sentiment',  'Daily Average Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['date'], y=b_date_mean['Positive Sentiment'],name='Positive Sentiment Mean'),\n    row=1, col=1\n)\n\n    \n#positive mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['date'].values[0], y0=b_date_mean['Positive Sentiment'].mean(), x1=b_date_mean['date'].values[-1], y1=b_date_mean['Positive Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean'\n)\n\n\n\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['date'], y=b_date_mean['Negative Sentiment'],name='Negative Sentiment Mean'),\n    row=2, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['date'].values[0], y0=b_date_mean['Negative Sentiment'].mean(), x1=b_date_mean['date'].values[-1], y1=b_date_mean['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\n\nfig['layout']['xaxis2']['title'] = 'Date'\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Average Change With Time\")\nfig.show()","66ecfbfe":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Daily Deviation in Positive Sentiment',  'Daily Deviation in Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=b_date_std['date'], y=b_date_std['Positive Sentiment'],name='Positive Sentiment SD'),\n    row=1, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_std['date'].values[0], y0=b_date_std['Negative Sentiment'].mean(), x1=b_date_std['date'].values[-1], y1=b_date_std['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_std['date'].values[0], y0=b_date_std['Positive Sentiment'].mean(), x1=b_date_std['date'].values[-1], y1=b_date_std['Positive Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x1', \n        yref='y1'\n)\n\n\nfig.add_trace(\n    go.Scatter(x=b_date_std['date'], y=b_date_std['Negative Sentiment'],name='Negative Sentiment SD'),\n    row=2, col=1\n)\n\nfig['layout']['xaxis2']['title'] = 'Date'\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Deviation Change With Time\")\nfig.show()","79ac8a3d":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =f_data.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =f_data.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Locations That Contribute The Most To Our Cut-Offs\")\nfig.show()","d269cfe9":"b_date_count = ft_data.groupby(by='date').count().reset_index()\nb_date_count = b_date_count.rename(columns={'title':'Posts Per Day'})\nfig = ex.line(b_date_count,x='date',y='Posts Per Day')\n\n\nfig.add_shape(type=\"line\",\n    x0=b_date_count['date'].values[0], y0=b_date_count['Negative Sentiment'].mean(), x1=b_date_count['date'].values[-1], y1=b_date_count['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n)\n\nfig.update_traces(mode=\"markers+lines\")\nfig.update_layout(hovermode=\"x unified\")\n\n\nfig.update_layout(title='<b>Daily Post Count<b>')\nfig.show()","09481a7b":"NUMBER_OF_COMPONENTS = 450\n\nCVZ = CountVectorizer()\nSVD = TruncatedSVD(NUMBER_OF_COMPONENTS)\n\ntext_data = f_data.title.copy()\ntext_data = text_data.apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS and len(word) > 1]).strip())\n\nstemmer= PorterStemmer()\nlemmatizer=WordNetLemmatizer()\n\ntext_data = text_data.apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\ntext_data = text_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n\nC_vector = CVZ.fit_transform(text_data)\n\n\npc_matrix = SVD.fit_transform(C_vector)\n\nevr = SVD.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\", \n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(     title='{:.2f}% of the Post Text Variance Can Be Explained Using {} Words'.format(np.sum(evr)*100,NUMBER_OF_COMPONENTS))\nfig.show()","5fdf2dca":"Most_Positive = title_data[title_data['Positive Sentiment'].between(0.4,1)]['title']\nMost_Negative = title_data[title_data['Negative Sentiment'].between(0.25,1)]['title']\n\nMost_Positive_text = ' '.join(Most_Positive)\nMost_Negative_text = ' '.join(Most_Negative)\n\n\npwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(Most_Positive_text)\nnwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(Most_Negative_text)\n\nplt.subplot(1,2,1)\nplt.title('Common Words Among Most Positive Post Titles',fontsize=16,fontweight='bold')\nplt.imshow(pwc)\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.title('Common Words Among Most Negative Post Titles',fontsize=16,fontweight='bold')\nplt.imshow(nwc)\nplt.axis('off')\n\nplt.show()","8ac78567":"plt.subplot(2,1,1)\nplt.title('Distriubtion Of Sentiments Across Our Posts',fontsize=19,fontweight='bold')\nsns.kdeplot(body_data['Negative Sentiment'],bw_method=0.1)\nsns.kdeplot(body_data['Positive Sentiment'],bw_method=0.1)\nsns.kdeplot(body_data['Neutral Sentiment'],bw_method=0.1)\nplt.subplot(2,1,2)\nplt.title('CDF Of Sentiments Across Our Posts',fontsize=19,fontweight='bold')\nsns.kdeplot(body_data['Negative Sentiment'],bw_method=0.1,cumulative=True)\nsns.kdeplot(body_data['Positive Sentiment'],bw_method=0.1,cumulative=True)\nsns.kdeplot(body_data['Neutral Sentiment'],bw_method=0.1,cumulative=True)\nplt.xlabel('Sentiment Value',fontsize=19)\nplt.show()","a4eb8e58":"#Sorting And Feature Engineering\nf_data = body_data.sort_values(by='timestamp')\nft_data=f_data.copy()\nft_data = ft_data.rename(columns={'timestamp':'date'})\nft_data['year']         = pd.DatetimeIndex(ft_data['date']).year\nft_data['month']        = pd.DatetimeIndex(ft_data['date']).month\nft_data['day']          = pd.DatetimeIndex(ft_data['date']).day\nft_data['day_of_year']  = pd.DatetimeIndex(ft_data['date']).dayofyear\nft_data['quarter']      = pd.DatetimeIndex(ft_data['date']).quarter\nft_data['season']       = ft_data.month%12 \/\/ 3 + 1\n","a9ec19a5":"f_data=f_data.reset_index().drop(columns=['index'])\nf_data = f_data.rename(columns={'timestamp':'date'})\n\npartitions = []\npartitions.append(f_data.loc[44:np.round(len(f_data)\/3,0)-1,:])\npartitions.append(f_data.loc[np.round(len(f_data)\/3,0):2*int(len(f_data)\/3)-1,:])\npartitions.append(f_data.loc[2*np.round(len(f_data)\/3,0):3*int(len(f_data)\/3)-1,:])\n\n\n\nneg_part_means =[]\nneg_part_std   =[]\npos_part_means =[]\npos_part_std   =[]\nfor part in partitions:\n    neg_part_means.append(part['Negative Sentiment'].mean())\n    neg_part_std.append(part['Negative Sentiment'].std())\n    pos_part_means.append(part['Positive Sentiment'].mean())\n    pos_part_std.append(part['Positive Sentiment'].std())\n    \nres_df = pd.DataFrame({'Positive Sentiment Mean':pos_part_means,'Negative Sentiment Mean':neg_part_means,'Positive Sentiment SD':pos_part_std,'Negative Sentiment SD':neg_part_std},\n                     index = [f'Partition_{i}' for i in range(1,4)])\n\n\ndef highlight_greater(x):\n    temp = x.copy()\n    temp = temp.round(0).astype(int)\n    m1 = (temp['Partition_1_Mean'] == temp['Partition_2_Mean'])\n    m2 = (temp['Partition_1_SD'] == temp['Partition_2_SD'])\n    m3 = (temp['Partition_1_Mean'] < temp['Partition_2_Mean']+3) & (temp['Partition_1_Mean'] > temp['Partition_2_Mean']-3)\n    m4 = (temp['Partition_1_SD'] < temp['Partition_2_SD']+3) & (temp['Partition_1_SD'] > temp['Partition_2_SD']-3)\n\n    df1 = pd.DataFrame('background-color: ', index=x.index, columns=x.columns)\n    #rewrite values by boolean masks\n    df1['Partition_1_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_2_Mean'])\n    df1['Partition_1_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_2_Mean'])\n    df1['Partition_1_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_Mean'])\n\n    df1['Partition_1_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_2_SD'])\n    df1['Partition_1_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_2_SD'])\n    df1['Partition_1_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_SD'])\n\n    return df1\n\n\n\n#res_df.style.apply(highlight_greater,axis=None)\nres_df = res_df.T\nres_df = pd.DataFrame(res_df.values,columns=res_df.columns,index=['Positive Sentiment','Negative Sentiment','Positive Sentiment','Negative Sentiment'])\nres_df = pd.concat([res_df.iloc[:2,:],res_df.iloc[2:,:]],axis=1)\nres_df.columns = ['Partition_1_Mean','Partition_2_Mean','Partition_3_Mean','Partition_1_SD','Partition_2_SD','Partition_3_SD']\nres_df.style.apply(highlight_greater,axis=None)","baf02986":"fig = make_subplots(rows=3, cols=2)\n\nfor idx,prt in enumerate(partitions):\n    by_date = prt.groupby(by='date').mean().reset_index()\n    fig.add_trace(\n    go.Scatter(x=by_date['date'], y=by_date['Positive Sentiment'],name=f'Positive Part {idx+1}'),\n    row=idx+1, col=1)\n    fig.add_trace(\n    go.Scatter(x=by_date['date'], y=by_date['Negative Sentiment'],name=f'Negative Part {idx+1}'),\n    row=idx+1, col=2)\n\nfig.update_layout(height=600, width=900, title_text=\"Distibution Of Daily Mean Sentiments Over Our Time Line For Each Partition\")\nfig.show()","a4635338":"fig = make_subplots(rows=4, cols=2, subplot_titles=('Observed Pos', 'Observed Neg', 'Trend Pos','Trend Neg','Seasonal Pos','Seasonal Neg','Residual Pos','Residual Neg'))\nb_date_mean = ft_data.groupby(by='date').mean().reset_index()\n\nlbl = ['Positive','Negative']\n\nfor idx,column in enumerate(['Positive Sentiment','Negative Sentiment']):\n    res = seasonal_decompose(b_date_mean[column], period=5, model='additive', extrapolate_trend='freq')\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.observed)), y=res.observed,name='{} Observed'.format(lbl[idx])),\n    row=1, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.trend)), y=res.trend,name='{} Trend'.format(lbl[idx])),\n    row=2, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.seasonal)), y=res.seasonal,name='{} Seasonal'.format(lbl[idx])),\n    row=3, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.resid)), y=res.resid,name='{} Residual'.format(lbl[idx])),\n    row=4, col=idx+1)\n            \nfig.update_layout(height=600, width=900, title_text=\"Decomposition Of Our Sentiments into Trend,Level,Seasonality and Residuals\")\nfig.show()","c07993fc":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\n\nax[0].set_title('Positive Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(b_date_mean['Positive Sentiment'],ax=ax[0],lw=3)\nax[1].set_title('Negative Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(b_date_mean['Negative Sentiment'],ax=ax[1],color='tab:red',lw=3)\nplt.show()","a3d21dee":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\nax[0].set_ylim(-1.1,1.1)\nax[1].set_ylim(-1.1,1.1)\n\n\nplot_pacf(b_date_mean['Negative Sentiment'],lags=5, ax=ax[0],title='Partial Autocorrelation Negative')\nplot_pacf(b_date_mean['Positive Sentiment'],lags=5, ax=ax[1],color='tab:green',title='Partial Autocorrelation Positive')\nplt.show()","5fb41ae8":"ar_1 = AutoReg(endog=b_date_mean['Positive Sentiment'],lags=1,trend='n',old_names=True).fit()\nfig = plt.figure(figsize=(16,9))\nfig = ar_1.plot_diagnostics(fig=fig, lags=5)","10ac7bde":"predicted_AR_1 = ar_1.predict()\n\noutput = pd.DataFrame({'Prediction':predicted_AR_1,'Actual':b_date_mean['Positive Sentiment']})\n\nfig = make_subplots(\n    rows=3, cols=2,subplot_titles=('','Actual','Predictions','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"table\",\"rowspan\": 3}     ,{\"type\": \"scatter\"}] ,\n           [None                               ,{\"type\": \"scatter\"}]            ,           \n           [None                               ,{\"type\": \"scatter\"}]                           \n          ]\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=b_date_mean.date,\n        y=output[\"Actual\"],\n        mode=\"lines+markers\",\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=b_date_mean.date,\n        y=output[\"Prediction\"],\n        mode=\"lines+markers\",\n    ),\n    row=2, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"]-output[\"Actual\"],\n        mode=\"lines+markers\",\n    ),\n    row=3, col=2\n)\n\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['Prediction','Actual'],\n            font=dict(size=10),\n            align=\"left\"\n        ),\n        cells=dict(\n            values=[output[k].tolist() for k in output.columns],\n            align = \"left\")\n    ),\n    row=1, col=1\n)\n\n\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=(output[\"Prediction\"]-output[\"Actual\"]).mean(), x1=len(output[\"Prediction\"]), y1=(output[\"Prediction\"]-output[\"Actual\"]).mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x3', \n        yref='y3'\n)\n\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"Prediction Evaluation\",\n)\n\nfig.show()","afa557a9":"ar_1.summary()\n","06b2a4a7":"b_date_mean = ft_data.groupby(by='date').mean().reset_index()\nb_date_std = ft_data.groupby(by='date').std().reset_index()\n\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Daily Average Positive Sentiment',  'Daily Average Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['date'], y=b_date_mean['Positive Sentiment'],name='Positive Sentiment Mean'),\n    row=1, col=1\n)\n\n    \n#positive mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['date'].values[0], y0=b_date_mean['Positive Sentiment'].mean(), x1=b_date_mean['date'].values[-1], y1=b_date_mean['Positive Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean'\n)\n\n\n\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['date'], y=b_date_mean['Negative Sentiment'],name='Negative Sentiment Mean'),\n    row=2, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['date'].values[0], y0=b_date_mean['Negative Sentiment'].mean(), x1=b_date_mean['date'].values[-1], y1=b_date_mean['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\n\nfig['layout']['xaxis2']['title'] = 'Date'\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Average Change With Time\")\nfig.show()","aed0f17a":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Daily Deviation in Positive Sentiment',  'Daily Deviation in Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=b_date_std['date'], y=b_date_std['Positive Sentiment'],name='Positive Sentiment SD'),\n    row=1, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_std['date'].values[0], y0=b_date_std['Negative Sentiment'].mean(), x1=b_date_std['date'].values[-1], y1=b_date_std['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_std['date'].values[0], y0=b_date_std['Positive Sentiment'].mean(), x1=b_date_std['date'].values[-1], y1=b_date_std['Positive Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x1', \n        yref='y1'\n)\n\n\nfig.add_trace(\n    go.Scatter(x=b_date_std['date'], y=b_date_std['Negative Sentiment'],name='Negative Sentiment SD'),\n    row=2, col=1\n)\n\nfig['layout']['xaxis2']['title'] = 'Date'\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Deviation Change With Time\")\nfig.show()","c29529b7":"by_date = title_data.groupby(by='timestamp').sum().reset_index()\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Daily Average Number Of Times Currency Was Mentioned',\n                                                                     'Daily Average Number Of Times Organizations Were Mentioned'))\n\nfig.add_trace(\n    go.Scatter(x=by_date['timestamp'], y=by_date['# Of Times Currency Was Mentioned'],name='Currency Mentioned'),\n    row=1, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=by_date['timestamp'].values[0], y0=by_date['# Of Times Currency Was Mentioned'].mean(), x1=by_date['timestamp'].values[-1], y1=by_date['# Of Times Currency Was Mentioned'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x1', \n        yref='y1'\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=by_date['timestamp'].values[0], y0=by_date['# Of Organizations Mentioned'].mean(), x1=by_date['timestamp'].values[-1], y1=by_date['# Of Organizations Mentioned'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\n\nfig.add_trace(\n    go.Scatter(x=by_date['timestamp'], y=by_date['# Of Organizations Mentioned'],name='Organizations Mentioned'),\n    row=2, col=1\n)\n\nfig['layout']['xaxis2']['title'] = 'Date'\nfig.update_layout(height=700, width=900, title_text=\"How much on average do different themes appear in post titles\")\nfig.show()","1ac5f86a":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =f_data.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =f_data.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Locations That Contribute The Most To Our Cut-Offs\")\nfig.show()","bcc9a570":"by_date = body_data.groupby(by='timestamp').sum().reset_index()\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Daily Average Number Of Times Currency Was Mentioned',\n                                                                     'Daily Average Number Of Times Organizations Were Mentioned'))\n\nfig.add_trace(\n    go.Scatter(x=by_date['timestamp'], y=by_date['# Of Times Currency Was Mentioned'],name='Currency Mentioned'),\n    row=1, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=by_date['timestamp'].values[0], y0=by_date['# Of Times Currency Was Mentioned'].mean(), x1=by_date['timestamp'].values[-1], y1=by_date['# Of Times Currency Was Mentioned'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x1', \n        yref='y1'\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=by_date['timestamp'].values[0], y0=by_date['# Of Organizations Mentioned'].mean(), x1=by_date['timestamp'].values[-1], y1=by_date['# Of Organizations Mentioned'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\n\nfig.add_trace(\n    go.Scatter(x=by_date['timestamp'], y=by_date['# Of Organizations Mentioned'],name='Organizations Mentioned'),\n    row=2, col=1\n)\n\nfig['layout']['xaxis2']['title'] = 'Date'\nfig.update_layout(height=700, width=900, title_text=\"How much on average do different themes appear in post bodies\")\nfig.show()","d1baf56f":"NUMBER_OF_COMPONENTS = 450\n\nCVZ = CountVectorizer()\nSVD = TruncatedSVD(NUMBER_OF_COMPONENTS)\n\ntext_data = f_data.body.copy()\ntext_data = text_data.apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS and len(word) > 1]).strip())\n\nstemmer= PorterStemmer()\nlemmatizer=WordNetLemmatizer()\n\ntext_data = text_data.apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\ntext_data = text_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n\nC_vector = CVZ.fit_transform(text_data)\n\n\npc_matrix = SVD.fit_transform(C_vector)\n\nevr = SVD.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\", \n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(     title='{:.2f}% of the Post Text Variance Can Be Explained Using {} Words'.format(np.sum(evr)*100,NUMBER_OF_COMPONENTS))\nfig.show()","6bc2a5ac":"Most_Positive = body_data[body_data['Positive Sentiment'].between(0.4,1)]['body']\nMost_Negative = body_data[body_data['Negative Sentiment'].between(0.25,1)]['body']\n\nMost_Positive_text = ' '.join(Most_Positive)\nMost_Negative_text = ' '.join(Most_Negative)\n\n\npwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(Most_Positive_text)\nnwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(Most_Negative_text)\n\nplt.subplot(1,2,1)\nplt.title('Common Words Among Most Positive Post Bodies',fontsize=16,fontweight='bold')\nplt.imshow(pwc)\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.title('Common Words Among Most Negative Post Bodies',fontsize=16,fontweight='bold')\nplt.imshow(nwc)\nplt.axis('off')\n\nplt.show()","6a902880":"had_org = body_data[body_data['# Of Times Currency Was Mentioned']>0].copy()\norg_names = []\nfor body in had_org.body:\n    org_names+=[str(tok) for tok in nlps(body).ents if tok.label_ == 'MONEY' ]\nnwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(' '.join(org_names))\n\nplt.title('Currency Mentioned In Post Bodies',fontsize=16,fontweight='bold')\nplt.imshow(nwc)\nplt.axis('off')\n\nplt.show()    ","06030b14":"had_org = body_data[body_data['# Of Organizations Mentioned']>0].copy()\norg_names = []\nfor body in had_org.body:\n    org_names+=[str(tok) for tok in nlps(body).ents if tok.label_ == 'ORG' ]\nnwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(' '.join(org_names))\n\nplt.title('Organizations Mentioned In Post Bodies',fontsize=16,fontweight='bold')\nplt.imshow(nwc)\nplt.axis('off')\n\nplt.show()    ","d3f18f63":"org_freq=dict(nltk.FreqDist(org_names))\norg_freq = {k: v for k, v in sorted(org_freq.items(), key=lambda item: item[1],reverse=True)}\ntop_10_org = list(org_freq.keys())[:10]\nmask = []\nindx = []\nfor idx,b in enumerate(had_org.body):\n    for m in top_10_org:\n        if b.find(m) !=-1:\n            mask.append(m)\n            indx.append(idx)\n            break\n\ntop_10_org_df = had_org.iloc[indx,:].copy()\ntop_10_org_df['Organization'] = mask\n\nby_org = top_10_org_df.groupby('Organization').mean().reset_index()\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=by_org['Organization'],y=by_org['Positive Sentiment'],name='Positive Sentiment',marker_color='lightgreen'))\nfig.add_trace(go.Bar(x=by_org['Organization'],y=by_org['Negative Sentiment'],name='Negative Sentiment',marker_color='salmon'))\nfig.update_layout(barmode='group',title='Average Sentiment Intensity In The Top 10 Discussed Organization')","0be495d8":"fig = go.Figure()\nfig.add_trace(go.Bar(x=by_org['Organization'],y=by_org['# Of Words'],name='Number Of Words',marker_color='skyblue'))\nfig.add_trace(go.Bar(x=by_org['Organization'],y=by_org['# Of StopWords'],name='Number Of Stopwords',marker_color='salmon'))\nfig.update_layout(barmode='group',title='Average Naive Text Attributes The Top 10 Discussed Organization')","b2d0394a":"top_10_text_bulk = [' '.join(top_10_org_df[top_10_org_df['Organization'] == i].body) for i in top_10_org]\ntop_org_freqs = []\nstopwords = nltk.corpus.stopwords.words('english')\nfor i in top_10_text_bulk:\n    freq = dict(nltk.FreqDist(i.split(' ')))\n    freq = {k: v for k, v in sorted(freq.items(), key=lambda item: item[1],reverse=True)}\n    freq = {k: v for k, v in freq.items() if k not in stopwords and len(k)>1}\n    top_org_freqs.append(freq)\n","668163c3":"fig = go.Figure()\n\nfor freq,org in zip(top_org_freqs,top_10_org):\n    fig.add_trace(\n        go.Bar(\n            x = list(freq.keys())[1:11],\n            y = list(freq.values())[1:11],\n            name = org\n        )\n    )\n    \n\nbtns = []\nfor x,col in enumerate(top_10_org):\n    bol = [False]*12\n    bol[x]=True\n    d = dict(label = col,\n                  method = 'update',\n                  args = [{'visible':bol},\n                          {'title': 'Distribution Of Top 10 Words in [' +col+'] Related Posts',\n                           'showlegend':True}])\n    btns.append(d)\n    \n    \nfig.update_layout(title='Distribution Of 10 Most Common Words In Different Organization Related Posts',\n    updatemenus=[go.layout.Updatemenu(\n        active=0,\n        showactive=True,\n        buttons=btns\n        )\n    ])\n\nfig.update_xaxes(title_text='Word')\nfig.update_yaxes(title_text='Appearances')\n\nfig.show()","b2d1cb5a":"NUMBER_OF_COMPONENTS = 2\n\n#Preprocessing Top10 Organization Post Bodies\ntext_data = top_10_org_df.body.copy()\n\nstemmer= PorterStemmer()\nlemmatizer=WordNetLemmatizer()\ntext_data = text_data.apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\ntext_data = text_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n\n\n#Vectorization\n\nCVZ = CountVectorizer()\nReduction_Pipeline = Pipeline(steps=[('scale',StandardScaler(with_mean=False)),('iso',Isomap(n_components= NUMBER_OF_COMPONENTS))])\n\n\nC_vector = CVZ.fit_transform(text_data)\n\n\npc_matrix = Reduction_Pipeline.fit_transform(C_vector)","faca789a":"dec_top_10_df= top_10_org_df.reset_index().copy()\ntemp = pd.DataFrame(pc_matrix,columns=['Dim1','Dim2'])['Dim1']\ndec_top_10_df['Dim1'] = pc_matrix[:,0]\ndec_top_10_df['Dim2'] = pc_matrix[:,1]\nfig = ex.scatter(dec_top_10_df,x='Dim1',y='Dim2',color='Organization')\nfig.update_layout(title='Vectorized Post Body Text Projected On to 2D Space Using Isomap')\nfig.show()","9aee3651":"DBS = DBSCAN(eps=0.8,min_samples=15)\nDBS.fit(dec_top_10_df[['Dim1','Dim2']])\ndec_top_10_df['Cluster']  = DBS.labels_\nfig = ex.scatter(dec_top_10_df,x='Dim1',y='Dim2',color='Cluster')\nfig.update_layout(title='Using DBSCAN To Cluster All Anomalies Which Deviate From The Main Span')\nfig.show()","ba33412c":"anam = dec_top_10_df.query('Cluster == -1')\nfig = ex.pie(anam,names='Organization')\nfig.update_layout(title='Proportion Each Organization Appears As An Anomaly')\nfig.show()","a8b5b640":"anam_text = ' '.join(anam.body)\nawc = WordCloud(width=600,height=500,background_color='white',stopwords=stopwords).generate(anam_text)\n\nplt.title('Most Frequent Words In Anomaly\u200b Labeld Posts',fontsize=17,fontweight='bold')\nplt.imshow(awc)\nplt.axis('off')\nplt.show()","415447c1":"plt.title('Distriubtion Of Sentiments Across Observed Anomalies',fontsize=19,fontweight='bold')\nsns.kdeplot(anam['Negative Sentiment'],label='Negative Sentiment')\nsns.kdeplot(anam['Positive Sentiment'],label='Positive Sentiment')\nsns.kdeplot(anam['Neutral Sentiment'],label='Neutral Sentiment')\nplt.xlabel('Sentiment Strength',fontsize=13,fontweight='bold')\nplt.ylabel('Density',fontsize=13,fontweight='bold')\nplt.xticks(np.arange(0,1.05,0.05),rotation=-45)\nplt.legend()\nplt.show()","28a85b27":"#both modes for further analysis\nfirt_mode = anam[anam['Negative Sentiment'].between(0,0.03)]\nsec_mode = anam[anam['Negative Sentiment'].between(0.03,0.25)]","6861ad8b":"anam_count = anam.groupby('timestamp').count().reset_index()\nfig = ex.line(anam_count,x='timestamp',y='body')\nfig.update_layout(title='Number Of Anomalies Observed Per Time-Stamp')\nfig.show()","961d1888":"import pymc3 as pm\n\nanam_count_observed = anam_count['index']\nN = 15000\nwith pm.Model():\n    \n    LAMB = pm.Uniform('lambda',0,10)\n    \n    observed = pm.Poisson('obs',LAMB,observed=anam_count_observed)\n    \n    step = pm.Metropolis()\n    trace = pm.sample(N,step=step)\n    burned_trace = trace[int(0.8 * N):]","f3b159cc":"plt.title('Posterior Distribution Of $\\lambda$',fontsize=18,fontweight='bold')\nplt.hist(burned_trace['lambda'],label='Values of $\\lambda$ ')\nplt.legend(prop=dict(size=18))\nplt.show()","ea61093b":"import scipy.stats as stats\nplt.title('Overlayed Distribution With Infered $\\lambda$',fontsize=18,fontweight='bold')\nRANGE = np.arange(0,100,1)\nmean_l = np.mean(burned_trace['lambda'])\nplt.hist(anam_count_observed,color='grey',label='Daily Anomaly Count Distribution')\nplt.plot(RANGE,stats.poisson.pmf(RANGE,mu=np.mean(burned_trace['lambda']))*100,'r-',lw=4,label=f'Poission Distribution with $\\lambda = {np.round(mean_l,2)}$')\nplt.legend(prop=dict(size=20))\nplt.show()","a33041f7":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 8))\nax.set_title('Daily Anomaly Count Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(anam_count['body'],ax=ax,lw=3)\nplt.show()","40f60f37":"anam_freq = dict(nltk.FreqDist([i for i in anam_text.split(' ') if i not in stopwords and len(i)>1 ]))\nanam_freq = {k: v for k, v in sorted(anam_freq.items(), key=lambda item: item[1],reverse=True)}\ntop_10_pos = list(anam_freq.keys())[:10]\n\ntoken=nltk.word_tokenize(' '.join(anam.body))\npos_bigram=ngrams(token,2)\npos_bigram_dict = dict()\npos_trigram =ngrams(token,3)\npos_trigram = [k for k in pos_trigram if k[0] in top_10_pos]\n\n\nfor i in pos_bigram:\n    pos_bigram_dict[i] = pos_bigram_dict.get(i,0)+1\n        \npos_trigram_dict = dict()\n\nfor i in pos_trigram:\n    pos_trigram_dict[i] = pos_trigram_dict.get(i,0)+1\n\n\npos_trigram_df = pd.DataFrame(random.sample(list(pos_trigram_dict.keys()),k=15),columns=['One Of Top 10 Words','Second Word','Third Word'])\n\ndef get_prob(sir):\n    key = (sir['One Of Top 10 Words'],sir['Second Word'],sir['Third Word'])\n    w3 = pos_trigram_dict[key]\n    w2 = pos_bigram_dict[(sir['One Of Top 10 Words'],sir['Second Word'])]\n    return w3\/w2\n\npos_trigram_df['Probabilty Of Sentence'] = pos_trigram_df.apply(get_prob,axis=1)\n\npos_trigram_df.style.background_gradient(subset='Probabilty Of Sentence',cmap='vlag')","ba4ceebd":"<a id=\"4.1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Title Time Based Sentiment Analysis<\/h1>\n","a2d26113":"**Observation**: Assuming that Jan 29th has a higher amount of posts is due to the dataset author extraction decision, we see that the daily amount of posts is fairly stable around a mean value of 2020 posts per day.","3d3cd21d":"<a id=\"3.1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Sentiment Analysis<\/h1>\n","36255834":"**Observation**:Surprisingly the simple AR(1) model performed very well! We can see that we can, with reasonable deviation, predict the strength of the positive sentiment expected to be in the following day based on the data we collected so far.","32eab49c":"**Observation**: Due to the visible trend both in the positive and negative sentiments of the post titles, we can conclude that the data is not stationary.","e1893ce0":"**Observation**: It seems that the distribution of sentiments across the observed anomalies follows a bimodal distribution, which can be seen in all sentiment categories. This fact can potentially point that many different groups are underlaying in the currently observed distribution.","07b4517f":"<a id=\"6.1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Examining The Observed Anomalies <\/h1>\n","cba6d616":"<a id=\"4.2\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Title Text Decomposition Analysis<\/h1>\n","aa905d38":"**Observation**: It can be observed that with time the average negative sentiment is following a decreasing trend as the average  positive sentiment keeps increasing over time ","e4c59540":"<a id=\"3.2\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Naive Feature Extraction<\/h1>\n","4e96727c":"<a id=\"5\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Body Text Sentiment Analysis<\/h1>\n","86592a63":"**Observation**: It looks like the positive sentiment is correlated with the lagged by 1-time series; even though we concluded that the time series of the sentiment is non-stationary because of the trend, we can still transform the data and possibly use an ARIMA model, but we will start with a simple AR(1) model to see how a baseline model preforms. ","90b45859":"**Observation**: We can see that there is a region where most of the vectors span, but at the same time, there is a distinct deviation from that region, especially by bodies linked to 'sec' and 'app.'","f9363f05":"**Observation**: As for autocorrelation between time lags, we see that there is no lag correlation of which is beyond our significance threshold.","dbffd042":"**Observation:** The mean between our consecutive partitions is quite similar; there is a chance of the sentiment data being stationary over time; thus, we will continue exploring the sentiments' stationarity.","b323b2a7":"**Observation**: Apparently, The dominant sentiment among the Reddit post titles is by far neutral. Even more, there is a probability of 60% that a post title is classified to be completely neutral.","907d1b02":"<a id=\"4\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Title Text Sentiment Analysis<\/h1>\n","395c44f4":"<a id=\"3.3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Named Entity Extraction<\/h1>\n","8d124d83":"<a id=\"5.1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Body Time Based Sentiment Analysis<\/h1>\n","99c6af5a":"![](https:\/\/i.ibb.co\/Cs8v8PQ\/spike6.png)","599c4cc3":"**Observation:** The mean between our consecutive partitions is quite similar; there is a chance of the sentiment data being stationary over time; thus, we will continue exploring the sentiments' stationarity.","fb2af2d3":"<a id=\"2\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Text Preprocessing<\/h1>\n","0caa3c3d":"<h3>Possible reasons for the spikes in frequncey of currency realted sentences on the 3th and 6th of Februery<\/h3>    \n\n\n![](https:\/\/i.ibb.co\/TH464jG\/spike3.png)","905e6d58":"<a id=\"6\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Examining The 10 Most Discussed Organizations<\/h1>\n","2520347f":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Above is a trigram of 15 sentences that start with one of the top 10 most used words in the anamoly tagged posts and the probability that the sentence will appear in a random sample of 3 sequential words.<\/span><\/p>\n<p><br><\/p>","ae21c2a2":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Table Of Content<\/h1>\n\n\n\n* [1. Libraires And Utilities](#1)\n\n* [2 Text Preprocessing](#2)\n\n* [3 Feature Engineering](#3)\n    * [3.1 Vader Sentiment Analysis](#3.1)\n    * [3.2 Naive Feature Extraction](#3.2)\n    * [3.3 Named Entity Extraction](#3.3)\n    \n* [4. Title Text Sentiment Analysis](#4)\n    * [4.1 Title Time Based Sentiment Analysis](#4.1)\n    * [4.2 Title Text Decomposition Analysis](#4.2) \n   \n* [5. Body Text Sentiment Analysis](#5)\n    * [5.1 Body Time Based Sentiment Analysis](#5.1)\n    * [5.2 Body Text Decomposition Analysis](#5.2) \n    \n* [6. Examining The 10 Most Discussed Organizations](#6)\n    * [6.1 Examining The Observed Anomalies](#6.1)\n\n* [7. Conclusions](#7)\n","b01e0e94":"**Observation**: Although with a brief look of an eye, the number of daily anomalies seems to be stationary, there is no significant correlation between lags, which potentially could motivate us to create a predictive model for the daily count of anomalies.","e02af68a":"**Observation**: An interesting observation can be made about the distribution of sentiments in the post's body; we can see a bimodal distribution at each of our sentiments, potentially pointing to 2 underlaying groups of posts in our data.","f458e60c":"**Observation**: As for autocorrelation between time lags, we see that there is no lag correlation of which is beyond our significance threshold.","b3968e78":"<h3>Possible reason for the spike in positive sentiment on the 8th of Februery<\/h3>    \n\n![](https:\/\/i.ibb.co\/64fdRJX\/Screenshot-2021-02-10-092351.png)","fd99c5a4":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Libraires And Utilities<\/h1>\n","1383d736":"<a id=\"7\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Conclusions<\/h1>\n\n\n\n<p style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; text-align: center;\"><span data-preserver-spaces=\"true\" style='color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Based on the sentiment analysis done, we learned that;<\/span><\/p>\n<ul style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">\n        The dominant sentiment of WallStreetBets posts is neutral.<\/span><\/span><\/span><\/li>\n      <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">\n        It seems that the sentiments of the posts body contains 2 underlying groups,it can be interesting to further explore those groups and understand their attributes.<\/span><\/span><\/span><\/li>\n      <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">\n        The sentiments of both bodies and titles is not stationary over our timeline and it seems there is an incline in the positivity of the posts.<\/span><\/span><\/span><\/li>\n      <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">\n        The 82% of the variance in the text of WallStreet realated posts can be explained only 450 words.<\/span><\/span><\/span><\/li>\n      <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">\n        There are slight correlations between naive text features such as number of stops words and the overal number of words to the sentiment of the text.<\/span><\/span><\/span><\/li>\n      <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">\n        There is a declining trend in the titles negative sentiments, it seems that the titles slowley become more positive.<\/span><\/span><\/span><\/li>\n     <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">\n        Looking at the top 10 discussed organizations we see that the dominant sentiment is positive,there is also an example for a very discusseded topic with dominant negative sentiment .<\/span><\/span><\/span><\/li>\n         <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">\n        As for the posts body sentiment strength, we saw that using an AR(1) model; we got very satisfying results predicting the value of the sentiment; such an ability can potentially be a sign of happier Reddit user may be due to favorable movements in WallStreet.<\/span><\/span><\/span><\/li>\n         <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">\n        We observed that when transforming the post bodies into count vectors and projecting them on to a 2D axis (using dimensionality reduction via ISOMAP), most of the texts fall on a linear line whiles text related to 'sec' and 'app' tagged organizations appear to be very far away from that line which may potentially indicate anomalous texts or topics which need further investigation.<\/span><\/span><\/span><\/li>","0e71615d":"<a id=\"5.2\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Body Text Decomposition Analysis<\/h1>\n","9459c73f":"**Observation**: \"sec\" and \"app\" related posts represent more than 90% of the anomalieswe observed in the previous plot.","2cd9fe23":"**Observation**: Due to the visible trend both in the positive and negative sentiments of the post titles, we can conclude that the data is not stationary.","2b4f320a":"**Note**:  It seems that the number of anomalies per day is quite stationary with respect to the mean.\nIt can be beneficial to understand and model a probabilistic model that best describes the process of generating such data.\nWe will assume that our daily number of anomalies are samples from a Poisson distribution with parameter Lamda which is known to us, and that parameter is exactly what we are interested in inferring.\nLet us well define all that was stated above;\nLet $X$ be a random variable representing the number of anomalies observed in a given day; we say that $X \\sim Pois(\\lambda)$.\nAs for the parameter $\\lambda$, which we want to infer due to the fact we have no prior knowledge or beliefs regarding to what value $\\lambda$ may be so, we will assume  $\\lambda \\sim Uniform(0,10)$ .\nWe will use MCMC to sample from the posterior space after we include our observed data into the prior space.\n\n","d18617d5":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Feature Engineering<\/h1>\n"}}