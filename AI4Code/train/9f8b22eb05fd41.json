{"cell_type":{"cc620a7f":"code","cf118a7b":"code","e355886b":"code","5d742dec":"code","68a4ebd9":"code","15270d33":"code","0a11ea4e":"code","425a6e2d":"code","59d31d7c":"code","adb7e586":"code","26347eb6":"code","e50b620b":"code","a242f942":"code","aad0216f":"code","6c54b3cf":"code","2047652f":"code","18f8318f":"code","210a23e1":"code","dd76edfb":"code","4df71f86":"code","082e6af4":"code","48406137":"code","73a03365":"code","45b7a080":"code","83cef550":"code","b65b5e19":"code","052d1ba3":"code","5c979e9f":"code","a09f3557":"code","dc46d692":"code","9b2230ea":"code","3117ec17":"code","267dcdb2":"code","4749e133":"code","b47d52a9":"code","b176eee7":"code","f44d6cd1":"code","1be988b7":"code","fb6b2202":"code","73f76789":"code","8a28d444":"code","cc209bb5":"code","3163b9e5":"code","e7437ce6":"code","f9b0d81d":"code","18a95d5a":"code","d094b84f":"code","cbb753fa":"code","a846b8a3":"code","0751eff2":"code","3f86dbae":"code","f516fac3":"code","68a04a39":"code","b0bc3f62":"code","76ff8f46":"code","c595dec5":"code","78d7498c":"code","5edd90a1":"code","9346194d":"markdown","0205b566":"markdown","54b6e889":"markdown","5288cb95":"markdown","9009cb25":"markdown","b9d7f2cf":"markdown","6dac1764":"markdown","e0fb121a":"markdown","48d0e720":"markdown","30585496":"markdown","1ac3f1d4":"markdown","482b2a91":"markdown","41bff788":"markdown","5c6b7a3f":"markdown","b3e56a6a":"markdown","2155bd62":"markdown","8a6468e9":"markdown","18b8cb01":"markdown","93a73a7d":"markdown","33295e19":"markdown","10d70282":"markdown","06cd1b0d":"markdown","bae745e9":"markdown","6ccfbfaf":"markdown","ea0b3e9a":"markdown","f5f93aac":"markdown","1d4ba24a":"markdown","8e629f22":"markdown","5ab19b93":"markdown","19e4fe3c":"markdown"},"source":{"cc620a7f":"# System pacakges\nimport os\nimport sys\nimport re\nimport gc\nimport time\nimport datetime\nimport warnings\n\n# Data pacakges\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Modelling pacakges\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\n# Pacakge settings\n%matplotlib inline\nsns.set(style=\"darkgrid\")\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.float_format', lambda x: '%.2f' % x)\nsys.version_info","cf118a7b":"print (os.listdir('..\/input'))","e355886b":"train = pd.read_csv('..\/input\/sales_train.csv.gz')\nitems = pd.read_csv('..\/input\/items.csv')\n# Drop name to save space\nitems = items.drop('item_name', axis=1)\n# Set index to ID to avoid droping it later\ntest  = pd.read_csv('..\/input\/test.csv.gz').set_index('ID')\n# Add date_block_num = 34\ntest['date_block_num'] = 34\n#shops = pd.read_csv('..\/input\/shops.csv')\n#categories = pd.read_csv('..\/input\/item_categories.csv')\n","5d742dec":"# Get the ranges of each feature to select the most appropriate data size\nprint ('-------------------------------------------------------')\nprint ('train:')\nfor f in train.columns.values:\n    print ('%s: %s ~ %s' %(f, train[f].min(), train[f].max()))\nprint ('-------------------------------------------------------')\nprint ('items:')\nfor f in items.columns.values:\n    print ('%s: %s ~ %s' %(f, items[f].min(), items[f].max()))\nprint ('-------------------------------------------------------')\nprint ('test:')\nfor f in test.columns.values:\n    print ('%s: %s ~ %s' %(f, test[f].min(), test[f].max()))","68a4ebd9":"def compress_columns(df,columns,keyword,search_type,datatype):\n    if search_type=='in':\n        valid_features = [x for x in columns if keyword in x]\n    elif search_type=='start':\n        valid_features = [x for x in columns if x.startswith(keyword)]\n    if len(valid_features):\n        for f in valid_features:\n            df[f] = df[f].round().astype(datatype)\n    return df\n\ndef data_compression(df):\n    features = df.columns.values\n    # Original features\n    if 'date_block_num' in features:\n        df['date_block_num'] = df['date_block_num'].astype(np.int8)\n    if 'shop_id' in features:\n        df['shop_id'] = df['shop_id'].astype(np.int8)\n    if 'item_category_id' in features:\n        df['item_category_id'] = df['item_category_id'].astype(np.int8)\n    if 'item_id' in features:\n        df['item_id'] = df['item_id'].astype(np.int16)\n    if 'item_price' in features:\n        df['item_price'] = df['item_price'].astype(np.float32)\n    if 'item_id_avg_item_price' in features:\n        df['item_id_avg_item_price'] = df['item_id_avg_item_price'].astype(np.float32)\n        \n    # Mean encoded features & lag features\n    df = compress_columns(df,features,'item_id_sum_item_cnt_day','in',np.int16)\n    df = compress_columns(df,features,'item_id_avg_item_cnt_day','in',np.float16)\n    \n    df = compress_columns(df,features,'shop_id_avg_item_price','in',np.float16)\n    df = compress_columns(df,features,'shop_id_sum_item_cnt_day','in',np.int16)\n    df = compress_columns(df,features,'shop_id_avg_item_cnt_day','in',np.float16)\n    \n    df = compress_columns(df,features,'item_category_id_avg_item_price','in',np.float16)\n    df = compress_columns(df,features,'item_category_id_sum_item_cnt_day','in',np.int32)\n    df = compress_columns(df,features,'item_category_id_avg_item_cnt_day','in',np.float16)\n    \n    df = compress_columns(df,features,'item_cnt_day','start',np.int16)\n    return df","15270d33":"# Compress features according to range\ntrain = data_compression(train)\nitems = data_compression(items)\ntest = data_compression(test)","0a11ea4e":"# Include Category id\ntrain = pd.merge(train,items,on='item_id',how='left')\ntest = pd.merge(test,items, on='item_id', how='left')","425a6e2d":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","59d31d7c":"train.info()","adb7e586":"def box_plot(df,f):\n    plt.figure(figsize=(10,4))\n    plt.title(f+' distribution')\n    x_min = int(df[f].min() - (abs(df[f].min())*0.1))\n    x_max = int(df[f].max() + (abs(df[f].max())*0.1))\n    if x_min==0:\n        x_min = -1\n    if x_max==0:\n        x_max = 1\n    plt.xlim(x_min,x_max)\n    sns.boxplot(x=df[f])\n\nplot_features = [x for x in train.columns.values if train[x].dtype != 'object']\nfor f in plot_features:\n    box_plot(train,f)","26347eb6":"# Getting rid of the outliers & negative values\ntrain = train[(train['item_price']<100000) & (train['item_price']>=0)]\ntrain = train[(train['item_cnt_day']<1000) & (train['item_cnt_day']>=0)]\n\n# distribution after outliers removal\nplot_features = ['item_price','item_cnt_day']\nfor f in plot_features:\n    box_plot(train,f)","e50b620b":"# Create a grid with columns\nindex_cols = ['shop_id','item_id','date_block_num']\n\n# For every month we create a grid for all shops\/items pair\ngrid = []\nfor block_num in train['date_block_num'].unique():\n    cur_shops = train.loc[train['date_block_num']==block_num,'shop_id'].unique()\n    cur_items = train.loc[train['date_block_num']==block_num,'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops,cur_items,[block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid),columns=index_cols,dtype=np.int32)\ngrid = data_compression(grid)\ngrid.head()","a242f942":"grid.info()","aad0216f":"# Group items per month, per shop, per item, sum the sales of the item, mean the price\n# There is a big difference between np.mean and pandas mean\ntrain_m = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day':'sum','item_price':np.mean}).reset_index()\ntrain_m = pd.merge(grid,train_m,on=['date_block_num','shop_id','item_id'],how='left').fillna(0)\ntrain_m = pd.merge(train_m,items,on='item_id',how='left')\ntrain_m = data_compression(train_m)","6c54b3cf":"# Making the mean encoded features\nfor type_id in ['item_id', 'shop_id', 'item_category_id']:\n    for column_id, aggregator, aggtype in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:\n        mean_df = train.groupby([type_id,'date_block_num']).aggregate(aggregator).reset_index()[[column_id,type_id,'date_block_num']]\n        mean_df.columns = [type_id+'_'+aggtype+'_'+column_id,type_id,'date_block_num']\n        train_m = pd.merge(train_m, mean_df, on=['date_block_num',type_id], how='left')\n        del mean_df\n        gc.collect()","2047652f":"del train\ngc.collect()","18f8318f":"for f in train_m.columns:\n    if 'item_cnt' in f:\n        train_m[f] = train_m[f].fillna(0)\n    elif 'item_price' in f:\n        train_m[f] = train_m[f].fillna(train_m[f].median())\n\n# Compress data\ntrain_m = data_compression(train_m)\ntrain_m.info(verbose=False)","210a23e1":"# Check the positions of the base lag features\ntrain_m.columns.values[6:]","dd76edfb":"# Get all the monthly features, which means the Mean Encoded fatures are all monthly based\nlag_features = list(train_m.columns[6:])+['item_cnt_day']\n# The selected months from current month\nlags = [1,2,3,6]","4df71f86":"for lag in lags:\n    train_new_df = train_m.copy()\n    # Get the current month\n    train_new_df['date_block_num'] += lag\n    train_new_df = train_new_df[['date_block_num','shop_id','item_id']+lag_features]\n    # Name the columns as lag features of the month\n    train_new_df.columns = ['date_block_num','shop_id','item_id'] + [x+'_lag_'+str(lag) for x in lag_features]\n    train_m = pd.merge(train_m,train_new_df,on=['date_block_num','shop_id','item_id'],how='left')\n    del train_new_df\n    gc.collect()\n    print ('lag %s processed' %lag)","082e6af4":"# Fill NaNs\nfor f in train_m.columns:\n    if 'item_cnt' in f:\n        train_m[f] = train_m[f].fillna(0)\n    elif 'item_price' in f:\n        train_m[f] = train_m[f].fillna(train_m[f].median())\n\ntrain_m = data_compression(train_m)\ntrain_m.info(verbose=False)","48406137":"# Set the maximum clip value\nmax_clip = 30\ntrain_m['item_cnt_day'] = train_m['item_cnt_day'].clip(0,max_clip).astype(np.float16)","73a03365":"# Add lag variables\nfor lag in lags:\n    train_new_df = train_m.copy()\n    # Get the current month\n    train_new_df['date_block_num'] += lag\n    train_new_df = train_new_df[['date_block_num','shop_id','item_id']+lag_features]\n    # Name the columns as lag features of the month\n    train_new_df.columns = ['date_block_num','shop_id','item_id'] + [x+'_lag_'+str(lag) for x in lag_features]\n    test = pd.merge(test,train_new_df,on=['date_block_num','shop_id','item_id'],how='left')\n    del train_new_df\n    gc.collect()\n    print ('lag %s processed' %lag)","45b7a080":"# Fill NaNs\nfor f in test.columns:\n    if 'item_cnt' in f:\n        test[f] = test[f].fillna(0)\n    elif 'item_price' in f:\n        test[f] = test[f].fillna(test[f].median())\n\ntest = data_compression(test)","83cef550":"cols_to_drop = lag_features[:-1] + ['item_price']\nprint ('Columns to drop')\nprint (cols_to_drop)","b65b5e19":"train_cols = train_m.columns.values\ntest_cols = test.columns.values\nfor c in cols_to_drop:\n    if c in train_cols:\n        train_m = train_m.drop(c,axis=1)\n    if c in test_cols:\n        test = test.drop(c,axis=1)","052d1ba3":"# Month number\ntrain_m['month'] = train_m['date_block_num']%12\ntrain_m['month'] = train_m['month'].astype(np.int8)\n# Number of days in a month, no leap years here\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\ntrain_m['days'] = train_m['month'].map(days).astype(np.int8)\n\ntest['month'] = 11\ntest['month'] = test['month'].astype(np.int8)\ntest['days'] = 30\ntest['days'] = test['days'].astype(np.int8)","5c979e9f":"# Assert all the columns are the same except target column\nset(train_m.columns.values) ^ set(test.columns.values)","a09f3557":"train_m.head()","dc46d692":"test.head()","9b2230ea":"test[['shop_id','item_id']+['item_cnt_day_lag_'+str(x) for x in [1,2,3]]].head()","3117ec17":"print(train_m[train_m['shop_id'] == 5][train_m['item_id'] == 5037][train_m['date_block_num'] == 33]['item_cnt_day'])\nprint(train_m[train_m['shop_id'] == 5][train_m['item_id'] == 5037][train_m['date_block_num'] == 32]['item_cnt_day'])\nprint(train_m[train_m['shop_id'] == 5][train_m['item_id'] == 5037][train_m['date_block_num'] == 31]['item_cnt_day'])","267dcdb2":"train_m = train_m[train_m['date_block_num']>12]","4749e133":"train_set = train_m[train_m['date_block_num']<33]\nval_set = train_m[train_m['date_block_num']==33]","b47d52a9":"print (train_set.shape)\nprint (val_set.shape)\nprint (test.shape)","b176eee7":"# Save data\ntrain_set.to_pickle('train.pkl')\nval_set.to_pickle('val.pkl')\ntest.to_pickle('test.pkl')","f44d6cd1":"del train_m\ngc.collect()","1be988b7":"# divide data into x & y\ntrain_x = train_set.drop(['item_cnt_day'],axis=1)\ntrain_y = train_set['item_cnt_day']\nval_x = val_set.drop(['item_cnt_day'],axis=1)\nval_y = val_set['item_cnt_day']\n\nfeatures = list(train_x.columns.values)","fb6b2202":"# Check if the data sets have equal amount of features\nprint (train_x.shape)\nprint (train_y.shape)\nprint (val_x.shape)\nprint (val_y.shape)\nprint (test.shape)","73f76789":"del train_set\ndel val_set\ngc.collect()","8a28d444":"# For saving data & output results \/ models\ndef post_processing(model,model_name,train_x,val_x,test_x,train_y,val_y,test):\n    # Here we once again clip the output to 0~20\n    train_pred = model.predict(train_x).clip(0, 20)\n    val_pred = model.predict(val_x).clip(0, 20)\n    test_pred = model.predict(test_x).clip(0, 20)\n\n    # Get rmse scores\n    train_rmse = np.sqrt(mean_squared_error(train_y, train_pred))\n    print(\"Train RMSE: %f\" % (train_rmse))\n    val_rmse = np.sqrt(mean_squared_error(val_y, val_pred))\n    print(\"Val RMSE: %f\" % (val_rmse))\n    \n    # Export submission\n    submission = pd.DataFrame({'ID':test.index,'item_cnt_month': test_pred})\n    submission.to_csv('%s_submission.csv'%model_name,index=False)\n\n    # save model to file\n    pickle.dump(lm, open(\"%s_model.pickle\" %model_name, \"wb\"))\n    return train_pred,val_pred,test_pred","cc209bb5":"# For plotting feature importance\ndef plot_feature_importances(importances,indices,features,title,dimensions):\n    plt.figure(figsize=dimensions)\n    plt.title(title)\n    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n    plt.yticks(range(len(indices)), [features[i] for i in indices])\n    plt.xlabel('Relative Importance')\n    plt.show()   ","3163b9e5":"# Normalise data\nscaler = preprocessing.StandardScaler()\nscaler.fit(train_x.values)\ntrain_x_norm = scaler.transform(train_x.values)\nval_x_norm = scaler.transform(val_x.values)\ntest_norm = scaler.transform(test.values)","e7437ce6":"# Training\ngc.collect()\nts = time.time()\n# Training\nlm = linear_model.Ridge()\nlm.fit(train_x_norm,train_y)\nprint ('Training time: %s' %(time.time() - ts))","f9b0d81d":"# Performance and test predictions\ntrain_pred1,val_pred1,test_pred1 = post_processing(lm,'ridge',train_x_norm,val_x_norm,test_norm,train_y,val_y,test)","18a95d5a":"# Feature Importance\nimportances = abs(lm.coef_)\nindices = np.argsort(importances)\ntitle = 'Linear Regression Feature Importances'\nplot_feature_importances(importances,indices,features,title,(8,16))","d094b84f":"del train_x_norm\ndel val_x_norm\ndel test_norm\ngc.collect()","cbb753fa":"# Training\ngc.collect()\nts = time.time()\nxgbtrain = xgb.DMatrix(train_x.values, train_y.values)\n\nparam = {'max_depth':8, \n         'subsample':1,\n         'min_child_weight':0.5,\n         'eta':0.3, \n         'num_round':1000, \n         'seed':1,\n         'verbosity':2,\n         'eval_metric':'rmse'} # random parameters\n\nbst = xgb.train(param, xgbtrain)\nprint ('Training time: %s' %(time.time() - ts))","a846b8a3":"# Performance and test predictions\ntrain_pred2,val_pred2,test_pred2 = post_processing(bst,'xgboost',xgb.DMatrix(train_x.values),xgb.DMatrix(val_x.values),xgb.DMatrix(test.values),train_y,val_y,test)","0751eff2":"# Feature Importance\nimport operator\nimportance = sorted(bst.get_score().items(), key=operator.itemgetter(1))\nimportance_v = np.asarray([x[1] for x in importance],dtype=np.int16)\nindices = np.asarray([int(x[0].replace('f','')) for x in importance],dtype=np.int8)\ntitle = 'xgboost Feature Importances'\n\nplt.figure(figsize=(8,16))\nplt.title(title)\nplt.barh(range(len(indices)), importance_v, color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()   ","3f86dbae":"# Training\ngc.collect()\nts = time.time()\nrf = RandomForestRegressor(\n    bootstrap=True,\n    max_depth=30,\n    max_features=3,\n    min_samples_leaf=5,\n    min_samples_split=12,\n    n_estimators=200,\n    random_state=42,\n    verbose=1,\n    n_jobs=-1\n)\nrf.fit(train_x.values,train_y.values)\nprint ('Training time: %s' %(time.time() - ts))","f516fac3":"# Performance and test predictions\ntrain_pred3,val_pred3,test_pred3 = post_processing(rf,'rf',train_x,val_x,test,train_y,val_y,test)","68a04a39":"# Feature Importance\nimportances = rf.feature_importances_\nindices = np.argsort(importances)\ntitle = 'Random Forest Feature Importances'\nplot_feature_importances(importances,indices,features,title,(8,16))","b0bc3f62":"# Making data\ntrain_stack = pd.DataFrame({'lm':train_pred1,'xgboost':train_pred2,'rf':train_pred3})\nval_stack = pd.DataFrame({'lm':val_pred1,'xgboost':val_pred2,'rf':val_pred3})\ntest_stack = pd.DataFrame({'lm':test_pred1,'xgboost':test_pred2,'rf':test_pred3})","76ff8f46":"# Normalise data\nscaler = preprocessing.StandardScaler()\nscaler.fit(train_stack.values)\ntrain_stack = scaler.transform(train_stack.values)\nval_stack = scaler.transform(val_stack.values)\ntest_stack = scaler.transform(test_stack.values)","c595dec5":"# Training\ngc.collect()\nts = time.time()\n# Training\nstack_lm = linear_model.Ridge()\nstack_lm.fit(train_stack,train_y)\nprint ('Training time: %s' %(time.time() - ts))","78d7498c":"# Performance and test predictions\npost_processing(stack_lm,'stack',train_stack,val_stack,test_stack,train_y,val_y,test)","5edd90a1":"# Feature Importance\nimportances = abs(stack_lm.coef_)\nindices = np.argsort(importances)\ntitle = 'Stack Model Feature Importances'\nplot_feature_importances(importances,indices,features,title,(8,6))","9346194d":"### Final data ranges for figuring out the best data size per column\nshop_id: 0 ~ 59<br>\nitem_id: 0 ~ 22169<br>\ndate_block_num: 0 ~ 33<br>\nitem_cnt_day: 0 ~ 1644<br>\nitem_price: 0.0 ~ 50999.0<br>\nitem_category_id: 0 ~ 83<br>\n<br>\nitem_id_avg_item_price: 0.1 ~ 50999.0<br>\nitem_id_sum_item_cnt_day: 0.0 ~ 12560.0<br>\nitem_id_avg_item_cnt_day: 0 ~ 86<br>\nshop_id_avg_item_price: 77.0 ~ 1903.0<br>\nshop_id_sum_item_cnt_day: 0 ~ 16338<br>\nshop_id_avg_item_cnt_day: 0.0 ~ 4.883<br>\nitem_category_id_avg_item_price: 4.867 ~ 30200.0<br>\nitem_category_id_sum_item_cnt_day: 0 ~ 36311<br>\nitem_category_id_avg_item_cnt_day: 0 ~ 50<br>\n<br>\nitem_id_sum_item_cnt_day_lag_1: 0.0 ~ 12560.0<br>\nitem_id_avg_item_cnt_day_lag_1: 0.0 ~ 86.0<br>\nshop_id_avg_item_price_lag_1: 77.0 ~ 1903.0<br>\nshop_id_sum_item_cnt_day_lag_1: 2.0 ~ 16338.0<br>\nshop_id_avg_item_cnt_day_lag_1: 1.0 ~ 4.883<br>\nitem_category_id_avg_item_price_lag_1: 4.867 ~ 29220.0<br>\nitem_category_id_sum_item_cnt_day_lag_1: 1.0 ~ 36311.0<br>\nitem_category_id_avg_item_cnt_day_lag_1: 1.0 ~ 22.0<br>\nitem_cnt_day_lag_1: 0.0 ~ 1305.0<br>","0205b566":"**Data preparation**","54b6e889":"### Take the recent bit of data\nData too far in the past are considered to be less relevant","5288cb95":"### Merge duplicated shops\nThe duplicated shops can be found by inspecting the shop names in shops list.<br>\nThe following pairs of shop_id basically represent data from the same shops.","9009cb25":"# Part 2. Modelling\n3 distinctively different models are used:\n1. Ridge Regression (Linear)\n2. xgboost (Tree based)\n3. Random Forest (Tree based)\n\nAfter each model has been trained, a stacked ensemble model which includes the above 3 are been trained as well.","b9d7f2cf":"### Split into train & validation\nValidation: The last month 'date_block_num'==33<br>\nTrain: Everything else","6dac1764":"### Check Feature Distributions","e0fb121a":"### Check if lag variables in test set are correct","48d0e720":"### Load data","30585496":"## Stacking\nUse previous results to do stacking instead of rerunning all the models again.<br>\nIt saves time and computation resources.","1ac3f1d4":"# Part 1. EDA & Feature Engineering\nThe pipeline for EDA and Feature Engineering has been optimised for the workflow, so it is not how I originally conducted the EDA.<br>\nIn order to run on the Kernel I have used data compression on almost every step of the data preprocessing steps, because otherwise, the data will not fit within 16 Gb of Ram.<br> \nWhether the data compression affects the final performance is unknown at this stage.<br>\n<br>\n**The main steps are:**\n1. Merge duplicated shops<br>\n2. Feature distributions inspection<br>\n3. Outlier removal<br>\n4. Reformat data according to per month, per shop, per item<br>\n5. Creating Mean Encoded Features<br>\n6. Create Lag Features<br>\n7. Clipping train data to make it similar to test set<br>\n8. Test data preparation<br>\n9. Remove the first 12 months of data<br>\n\n**The detials of the steps can be found under the corresponding sections of the code.**","482b2a91":"**Plot the feature distributions**","41bff788":"So there are extreme outliers in 'item_price' and 'item_cnt_day'.<br>\nRemove them first.","5c6b7a3f":"### Create Mean Encodings\nThese below lines add the following 9 features:<br>\n<br>\nAll of which are per month.<br>\n\u2018item_id_avg_item_price\u2019: Monthly Mean item price per item_id<br>\n\u2018item_id_sum_item_cnt_day\u2019: Monthly Total item count per item_id<br>\n\u2018item_id_avg_item_cnt_day\u2019: Monthly Mean item count per item_id<br>\n\u2018shop_id_avg_item_price\u2019: Monthly Mean item price per shop<br>\n\u2018shop_id_sum_item_cnt_day\u2019: Monthly Total item count per shop<br>\n\u2018shop_id_avg_item_cnt_day\u2019: Monthly Mean item count per shop<br>\n\u2018item_category_id_avg_item_price\u2019: Monthly Mean item price per category<br>\n\u2018item_category_id_sum_item_cnt_day\u2019: Monthly Total item count per category<br>\n\u2018item_category_id_avg_item_cnt_day\u2019: Monthly Mean item count per category","b3e56a6a":"The lagged value for (5\t5037) actually correspond, looks like we dont have bugs!","2155bd62":"### Export the preprocessed data","8a6468e9":"### xgboost","18b8cb01":"### Clipping train data to make it similar to test set\nIn the evaluation data description on the competition page, it is said to clip target values to [0,20], because the test set ranges are in between these numbers.<br>\nIn the next few lines I clipped the days to range[0,30].<br>\nYou might ask me why 30. An intuitive answer is if I had clipped to range [0,20] there would be very few tree nodes that could give 20 as an answer.<br>\nWhile if I increase it to 40 having a 20 becomes much more easier.<br>\nPlease note that We will clip our predictions in the [0,20] range in the end.","93a73a7d":"### Create Lag Features\nNext we create lag features with diferent lag periods on the following features:<br>\n<br>\n\u2018item_id_avg_item_price\u2019,<br>\n\u2018item_id_sum_item_cnt_day\u2019<br>\n\u2018item_id_avg_item_cnt_day\u2019<br>\n\u2018shop_id_avg_item_price\u2019<br>\n\u2018shop_id_sum_item_cnt_day\u2019<br>\n\u2018shop_id_avg_item_cnt_day\u2019<br>\n\u2018item_category_id_avg_item_price\u2019<br>\n\u2018item_category_id_sum_item_cnt_day\u2019<br>\n\u2018item_category_id_avg_item_cnt_day\u2019<br>\n\u2018item_cnt_day\u2019","33295e19":"It seems that the order of the columns are the same for train & test.<br>\n(Except for the target column 'item_cnt_day')","10d70282":"### Outliers removal","06cd1b0d":"### Compress data size","bae745e9":"### Random Forest","6ccfbfaf":"### Packages and Settings","ea0b3e9a":"### Create new time related features","f5f93aac":"### Drop all columns which are not lag features\nExceptions:<br> \nshop_id, item_id: not time dependent<br> \ndate_block_num: the indicator of time<br> \nitem_cnt_Day: the target","1d4ba24a":"### Created a dataframe of all Date_block_num, Store and Item combinations:\nSo in case in the months some shops did not have the sale record of some item, it can be set to 0.","8e629f22":"### Fill NaNs\nBecause NaN cannot be converted to int, therefore NaN has to be filled.<br>\nBecause there are many items which did not sale a single piece per store per month, there are many Nans.<br> \nThe Nans in item count means 0 sold, therefore will be filled with 0.<br>\nThe Nans in item price however means since there is no sale record of the item, the price is missin, therefore will be filled with median \/ mean price of the item.","5ab19b93":"### Prepare test set","19e4fe3c":"### Ridge Regression"}}