{"cell_type":{"336aef77":"code","ea0aa961":"code","d3ac05da":"code","b6d940df":"code","c9b4ec7d":"code","6a9bd681":"code","daa55b13":"code","06e5e21d":"code","e94ee643":"code","a226d453":"code","4f6eb020":"code","42131690":"code","dfbf12b8":"code","f96a8334":"code","136940cb":"code","b9dc4d2d":"code","27ad5e26":"code","1fa0c7e1":"code","372f709f":"code","0d89705a":"markdown","b9ff235b":"markdown","47e1afd8":"markdown","97731fd7":"markdown","1d827b47":"markdown","69644a0c":"markdown","a45efa47":"markdown","1d1ca559":"markdown","2b0d8364":"markdown","8f66fdc8":"markdown","3395e4f5":"markdown","ccbddaaf":"markdown"},"source":{"336aef77":"import tensorflow as tf\nimport tensorflow.keras as keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow_datasets as tfds\nimport time","ea0aa961":"def sample_images(images, row_count, column_count):\n    fig, axs = plt.subplots(row_count, column_count, figsize=(10,10))\n    for i in range(row_count):\n        for j in range(column_count):\n            axs[i,j].imshow(images[i * column_count + j])\n            axs[i,j].axis('off')\n    plt.show()","d3ac05da":"def generate_images(generator,row_count, column_count):\n    fake_images = generator(tf.random.normal([row_count * column_count, random_normal_dimensions]))\n    sample_images(fake_images, row_count, column_count)","b6d940df":"batch_size = 100\nrandom_normal_dimensions = 32\nn_epochs = 10","c9b4ec7d":"def preproces_image(item):\n    image = item[\"image\"]\n    image = tf.cast(image, \"float\")  \/ 255.0\n    image =tf.image.resize(image, (32, 32))\n    return image","6a9bd681":"dataset = tfds.load(\"mnist\", split='train', as_supervised=False).map(preproces_image).shuffle(1024).batch(batch_size, drop_remainder=True).prefetch(1).repeat(n_epochs)","daa55b13":"for images in dataset.take(1):\n    sample_images(images, 10, 10)","06e5e21d":"tf.keras.backend.clear_session()","e94ee643":"generator = keras.models.Sequential([\n    keras.layers.Dense(8 * 8 * 128, input_shape=[random_normal_dimensions]),\n    keras.layers.Reshape([8, 8, 128]),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding=\"SAME\",\n                                 activation=\"selu\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding=\"SAME\",\n                                 activation=\"tanh\"),\n])\n\ngenerator.summary()","a226d453":"generate_images(generator,10, 10)","4f6eb020":"discriminator = keras.models.Sequential([\n    keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"SAME\",\n                        activation=keras.layers.LeakyReLU(0.2),\n                        input_shape=[32, 32, 1]),\n    keras.layers.Dropout(0.4),\n    keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"SAME\",\n                        activation=keras.layers.LeakyReLU(0.2)),\n    keras.layers.Dropout(0.4),\n    keras.layers.Flatten(),\n    keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\ndiscriminator.summary()","42131690":"gan = keras.Sequential([generator, discriminator])","dfbf12b8":"gan.summary()","f96a8334":"discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\ndiscriminator.trainable = False\ngan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")","136940cb":"begin = time.time()\ncurrent_traning_images = 0\ntotal = 60000 * n_epochs\ni = 0\nrow_count = 10\ncolumn_count = 10\n\nfor real_images in dataset:\n    noise = tf.random.normal(shape=[batch_size, random_normal_dimensions])\n    fake_images = generator(noise)\n    current_traning_images += fake_images.shape[0]\n    mixed_images = tf.concat([fake_images, real_images], axis=0)\n    discriminator_labels = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n    discriminator.trainable = True\n    discriminator.train_on_batch(mixed_images, discriminator_labels)\n    noise = tf.random.normal(shape=[batch_size, random_normal_dimensions])\n    generator_labels = tf.constant([[1.]] * batch_size)\n    discriminator.trainable = False\n    gan.train_on_batch(noise, generator_labels)\n    if i % 500 == 0 and i > 0:\n        current_time = time.time() - begin\n        ETA = current_time \/ current_traning_images * total - current_time\n        print(\"ETA: %.2fs\"% (ETA))\n        generate_images(generator,row_count, column_count)\n    i += 1","b9dc4d2d":"generate_images(generator,row_count, column_count)","27ad5e26":"sample_count = 512\nnoise = tf.random.normal(shape=[sample_count, random_normal_dimensions])\nfake_images = generator(noise)\nprobs = discriminator.predict(fake_images)\ny_true = np.array([0.0] * sample_count)\ny_pred = np.array(probs > 0.5, dtype=int)","1fa0c7e1":"bce = tf.keras.metrics.BinaryCrossentropy()(y_true, y_pred)\nprint(\"BCE:%.2f\"%(bce))","372f709f":"accuracy = tf.keras.metrics.Accuracy()(y_true, y_pred)\nprint(\"Accuracy:%.2f\"%(accuracy))","0d89705a":"## Evaluation","b9ff235b":"## Train the Model","47e1afd8":"### Accuracy","97731fd7":"## Build the Generator","1d827b47":"### BCE","69644a0c":"## Build the GAN","a45efa47":"Let","1d1ca559":"## Import Datasets","2b0d8364":"# Digits Generation with DCGAN\n## Import Packages","8f66fdc8":"## Build the Discriminator","3395e4f5":"## Utilities","ccbddaaf":"Let's generate some fake images."}}