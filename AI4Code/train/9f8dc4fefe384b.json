{"cell_type":{"30eedce5":"code","c823e016":"markdown","d80e4094":"markdown","8d3a8a02":"markdown","8ff419e4":"markdown","bbc69e00":"markdown","b2e7bab4":"markdown","461d608e":"markdown","40e3ee02":"markdown","6dadc976":"markdown","2e854d06":"markdown","33ec0b2e":"markdown","c12d70c7":"markdown","e9be9aa1":"markdown","35b8b2ca":"markdown","2cd4c7e4":"markdown","c1299350":"markdown","e2d8aa8f":"markdown","7c0d4c5e":"markdown","cb55d00f":"markdown","d0c0c604":"markdown","0815fed8":"markdown","d1b92170":"markdown","dfbb21a8":"markdown","bca3511d":"markdown","43fec0a7":"markdown","ecfe276f":"markdown","7685d176":"markdown"},"source":{"30eedce5":"from IPython.display import YouTubeVideo\nYouTubeVideo('3liCbRZPrZA', width=800, height=450)","c823e016":"><p style=\"font-size : 15px\"><em>Of course, different people will draw different straight lines. However, no matter how you draw the line between these 2 classes, you will always find a closest sample from each class to your straight line. That is the SUPPORT VECTOR. And the distance of these closest samples to your straight line describe how well you separate these two classes, that is the MARGIN.<\/em><\/p>","d80e4094":"><p style=\"font-size : 15px\"><em>Samples are data points allocate in the P-dimension space, each axis represents one feature. The idea of the Support Vector Classifier is to find the \"hyperplane\" to separate samples into 2 classes in this P-dimension space. In a 2-D space, when we only have 2 features, SVC is to find the straight line that can separate samples.<\/em><\/p>","8d3a8a02":"---\n\n><font color=\"blue\"><b>Algorithm for computer to find the solution for the L equation above<\/b><\/font>\n\n><font color=\"blue\"><b>SMO is one of the most popular algorithms there. This article only focuses on the math part, so I will not talk about it here.<\/b><\/font>\n\n---","8ff419e4":"---\n\n> <center><img src=\"https:\/\/monkeylearn.com\/blog\/wp-content\/uploads\/2017\/06\/Post_1e_social.png\"><\/center>\n\n---\n---","bbc69e00":"---\n---\n\n> <center><font color=\"yellow\"><h1 style=\"text-transform: uppercase; text-shadow: 1px 1px;\"><b>The problem in a mathmetical way\n<font size=5><\/font> <\/b><\/h1><\/center>\n\n\n---\n---","b2e7bab4":"---\n\n><center><img src=\"https:\/\/66.media.tumblr.com\/ff709fe1c77091952fb3e3e6af91e302\/tumblr_inline_o9aa8dYRkB1u37g00_540.png\"><\/center>\n\n---","461d608e":"---\n---\n\n> <center><font color=\"yellow\"><h1 style=\"text-transform: uppercase; text-shadow: 1px 1px;\"><b>Linear Support Vector Classifier <font><\/font> <\/b><\/h1><\/center>\n\n\n---\n","40e3ee02":"---\n\n><center><img src=\"https:\/\/jeremykun.files.wordpress.com\/2017\/05\/svm_lots_of_choices.gif?w=372\"><\/center>\n\n---","6dadc976":"---\n---\n\n> <center><font color=\"Red\"><h1 style=\"text-transform: uppercase; text-shadow: 1px 1px;\"><b>If you find this notebook helpful or you just liked it , some upvotes would be very much appreciated - That will keep me motivated \ud83d\ude04\n<font><\/font> <\/b><\/h1><\/center>\n\n\n---\n---","2e854d06":"---\n\n><p style=\"font-size : 15px\"><em>\nAnother important part of SVC is the kernel. it allows user to project the P-dimension dataset into other dimensions for a better seperation. A kernel is a tranform function. It takes in P-dimension dataset and spits out the transformation new dataset.<\/em><\/p>\n    \n    \n><p style=\"font-size : 15px\"><em>One simple example is, a 2D linear non-seperatable dataset, after transforming it into 3D, it becomes seperable.\n    \n>![](http:\/\/www.eric-kim.net\/eric-kim-net\/posts\/1\/imgs\/data_2d_to_3d_hyperplane.png)<\/em><\/p>\n\n---\n\n>Learn more about kernels: https:\/\/www.eric-kim.net\/eric-kim-net\/posts\/1\/kernel_trick.html\n\n---","33ec0b2e":"---\n\n><font color=\"red\"><b>The goal of the Linear SVC is to MAXIMIZE the width of the margin<\/b><\/font>\n\n---","c12d70c7":"---\n---\n\n> <center><font color=\"yellow\"><h1 style=\"text-transform: uppercase; text-shadow: 1px 1px;\"><b>Normalization\n<font><\/font> <\/b><\/h1><\/center>\n\n\n---\n---","e9be9aa1":"><p style=\"font-size : 15px\"><em>The <strong>reason why I wrote this article:<\/strong> \n    Explanation of SVC\/SVM on the internet is overwhelming, but I could not find anything to clarify all the doubts I have in mind regarding the math part of the Linear Support Vector Classifier. This article is for people who have the same doubts like me, I hope it will be helpful. If you like it, please click a vote for me, this is my first Kaggle Kernel, thank you!<\/em><\/p>\n\n---\n\n><p style=\"font-size : 15px\"><em>Background knowledge required: Understanding the geometric insights of vector dot product (one vector project to the other vector); Lagrange multiplier (for optimizaiton).<\/em><\/p>\n\n---\n\n><font color=\"red\"><font size=4><b>Notations:<\/b><\/font>\n\n---\n\n><p style=\"font-size : 15px\"><em>\n    \n    X is your sample dataset, it contains N samples, P features \n    \n    Xij is the ith sample, jth feature \n    \n    Xi is the vector describe all the P features of sample i \n    \n    X+ is the sample from class \"+\" \n    \n    X- is the sample from class \"-\" \n    \n    Y is the label vector Yi has 2 values +1 and -1\n\n---","35b8b2ca":"---\n---\n\n> <center><font color=\"yellow\"><h1 style=\"text-transform: uppercase; text-shadow: 1px 1px;\"><b>Soft margin SVC\n<font size=5><\/font> <\/b><\/h1><\/center>\n\n\n---\n---","2cd4c7e4":"---\n---\n\n> <center><font color=\"yellow\"><h1 style=\"text-transform: uppercase; text-shadow: 1px 1px;\"><b>Introduction<font size=5><\/font> <\/b><\/h1><\/center>\n\n\n---\n---","c1299350":"---\n\n><p style=\"font-size : 15px\"><em>When you use the python Linear SVC function, there are 2 parameters you can control. One is the alpha, which is the same alpha we talked about it before. The larger alpha is, the wider your margin is.\nThe other parameter is C, it comes from \"Soft margin SVC\".\nSince most datasets are not so ideal to separate perfectly by a straight line, we can allow a few samples to go across it, which we call soft margin. Like this,<\/em><\/p>\n\n---\n\n><center><img src=\"https:\/\/www.researchgate.net\/profile\/Catarina_Moreira2\/publication\/260283043\/figure\/fig12\/AS:297261608259590@1447884098130\/Figure-A14-Soft-margin-linear-SVM-classifier.png\"><\/center>\n\n---\n\n> **Then we need to rewrite our model into, <br>\n$Y_{i}(\\vec w \\bullet \\vec X_{i} + b ) =1 - C_{i}$. <br>\nThe larger the C is, the more mislabeled sample you allowed. It will increase the robustness of your model, but it will decrease your precision. <br>\n\n---\n\n>![](https:\/\/miro.medium.com\/max\/1200\/1*fYXRn8T4u-BTGxFelpC2yg.gif)<br>\n\n---\n\n>Learn more about Soft Margin SVC : http:\/\/fourier.eng.hmc.edu\/e161\/lectures\/svm\/node5.html\n\n---","e2d8aa8f":"---\n\n\n>$\\frac{1}{2} ||w||^2$ function is our target function, but it comes with a constrained function $Y_{i}(\\vec w \\bullet \\vec X_{i} + b ) =1$. <br>\nLAGRANGIAN TRANSFORM is a tool to find the optimization of a target function with constrains. <br>\n\n>3blue1brown gives a wonderful insight into this tool. \n\n>I'll paste the link here if you are interested.\nhttps:\/\/www.youtube.com\/watch?v=hQ4UNu1P2kw <br>\n\n>Now, let's pack them together. <br>\n\n\n>L = $\\frac{1}{2} ||w||^2$ - $\\sum \\alpha _{i} [Y_{i}(\\vec w \\bullet \\vec X_{i} + b ) -1] $ <br>\n$\\frac{\\partial L}{\\partial w}=0$, we will have $\\vec w = \\sum \\alpha _{i} Y_{i} X_{i}$. <br>\n$\\frac{\\partial L}{\\partial b}=0$, we will have $\\sum \\alpha _{i} Y_{i} =0$. <br>\n\n>Plug these 2 euqations back to L, <br>\nL = $\\frac{1}{2}(\\sum \\alpha _{i} Y_{i} \\vec X_{i})(\\sum \\alpha _{j} Y_{j} \\vec X_{j}) - \\sum \\alpha_{i}[Y_{i}(\\sum \\alpha _{j} Y_{j} \\vec X_{i} \\bullet \\vec X_{j} + b) - 1]$ = $-\\frac{1}{2} \\sum _{ij} \\alpha _{i} \\alpha _{j} Y_{i} Y_{j} \\vec X_{i} \\bullet \\vec X_{j} + \\sum \\alpha _{i}$. <br>\n\n>From this we can see, the classifier is only depends on the dot product of the sample. <br>\n\n---","7c0d4c5e":"---\n---\n\n> <center><font color=\"yellow\"><h1 style=\"text-transform: uppercase; text-shadow: 1px 1px;\"><b>Kernels\n<font size=5><\/font> <\/b><\/h1><\/center>\n\n\n---\n---","cb55d00f":"---\n\n>$\\bullet$ The straight line in the simple case above can be expressed as: <br>\n$\u03b2_{0} + \u03b2_{0}*x_{1} + \u03b2_{1}*x_{2}=0$. <br>\n    \n>Linear algebra friendly, I'd like to think it as , <br>\n$ \u03b2_{0}*x_{1} + \u03b2_{1}*x_{2}=- \u03b2_{0}$, that is the dot product of 2 vectors: $\\vec w = [ \u03b2_{0},  \u03b2_{1}]$, and $\\vec x = [x_{1}, x_{2}]$. <br>\n    \n>$\\vec w$ is the vector that is perpendicular to the straight line, and $\\vec x$ is the point on the straight line. <br>\nAnd of course, here is a more general way to quantify the \"hyperplane\" we are interested to find: <br>\n$ \u03b2_{0}+\u03b2_{0}*x_{1} + \u03b2_{1}*x_{2}..+\u03b2_{p}*x_{p}=0$ <br>\nor $\\vec w \\bullet \\vec x + b =0$ .<br>\n<br>\n$\\bullet$ Once we have the \"hyperplane\", we can define our DECISION RULE. <br>\nWe'd like for \"+\" and \"-\" class sample, they will lie on different side of the \"hyperplane\". <br>\nFor support vectors, to describe it in mathmatial way is $\\vec w \\bullet \\vec X_{+} + b = +1 $ and $\\vec w \\bullet \\vec X_{-} + b = -1$. <br>\nFor other samples, is $\\vec w \\bullet \\vec X_{+} + b > +1 $ and $\\vec w \\bullet \\vec X_{-} + b < -1$. <br>\nPut label vector **Y** to make these two functions into one for mathmatical convinience $Y_{i}(\\vec w \\bullet \\vec X_{i} + b ) >=1$. <br>\n<br>\n    \n>$\\bullet$ Describe MARGIN in a mathmatical way. <br>\nPair of support vectors, $X_{+}-X_{-}$ projects onto $\\vec w$ is the width of the margin. (If you understand the geometric insights of vector dot product, this will be so easy to understand).<br>\n\n>![](http:\/\/i.stack.imgur.com\/IMXfV.png)<br>\n\n>So, $\\frac{(\\vec X_{+} - \\vec X_{-}) \\bullet \\vec w}{||w||}$ is the width of the margin. <br>\nSince $Y_{i}(\\vec w \\bullet \\vec X_{i} + b ) =1$ is true, <br>\n$\\vec X_{+} \\vec w=1-b $, and $\\vec X_{-} \\vec w = -1-b$, <br>\nWe can rewrite $\\frac{(\\vec X_{+} - \\vec X_{-}) \\bullet \\vec w}{||w||}$ into $\\frac{2}{||w||}$. <br>\nAnd our goal is to MAXIMIZE $\\frac{2}{||w||}$. <br>\nAgain, for mathmatical convenience, we can just get the MINIMIZE of $\\frac{1}{2} ||w||^2$. <br>\n    \n---","d0c0c604":"---\n---\n\n> <center><font color=\"yellow\"><h1 style=\"text-transform: uppercase; text-shadow: 1px 1px;\"><b>Reference\n<font><\/font> <\/b><\/h1><\/center>\n\n\n---\n---","0815fed8":"><p style=\"font-size : 15px\"><em>So, how to draw this line? First, we need to make the straight line \"unbiased\" to any class. So the distance from + support vector to the line should equal to the - support vector; Second, if the margin value is small, it means it will be too sensitive to these support vectors. If you change dataset, the support vector will vary and your classifier will not be robust. As the figure below, all the lines are biased or \"support vector sensitive\".<\/em><\/p>","d1b92170":"---\n\n### Articles:\n\n>https:\/\/heartbeat.fritz.ai\/understanding-the-mathematics-behind-support-vector-machines-5e20243d64d5\n\n>https:\/\/medium.com\/@ankitnitjsr13\/math-behind-support-vector-machine-svm-5e7376d0ee4d\n\n>https:\/\/shuzhanfan.github.io\/2018\/05\/understanding-mathematics-behind-support-vector-machines\/\n\n---\n\n### Lectures:\n\n>https:\/\/www.youtube.com\/watch?v=QKc3Tr7U4Xc\n\n>https:\/\/www.youtube.com\/watch?v=IEOgRGh7x4g\n\n>https:\/\/www.youtube.com\/watch?v=_PwhiWxHK8o\n\n---\n\n### Others:\n\n> https:\/\/stats.stackexchange.com\/a\/57019\n\n>https:\/\/www.kaggle.com\/upadorprofzs\/bayesian-learning-basics-tutorial\n\n>https:\/\/www.kaggle.com\/xingewang\/the-math-behind-linear-svc-classifier\n","dfbb21a8":"---\n><p style=\"font-size : 15px\"><em>\nAnother thing to keep in mind is, before you feed the SVC model, please make sure you've scaled your data or normalized it. Cause it will affect the result a lot..<\/em><\/p>\n\n---\n\n>Now, Question: Why should we normalize?\n\n\n>The answer to your question depends on what similarity\/distance function you plan to use (in SVMs). If it's simple (unweighted) Euclidean distance, then if you don't normalize your data you are unwittingly giving some features more importance than others.\n\n>For example, if your first dimension ranges from 0-10, and second dimension from 0-1, a difference of 1 in the first dimension (just a tenth of the range) contributes as much in the distance computation as two wildly different values in the second dimension (0 and 1). So by doing this, you're exaggerating small differences in the first dimension. You could of course come up with a custom distance function or weight your dimensions by an expert's estimate, but this will lead to a lot of tunable parameters depending on dimensionality of your data. In this case, normalization is an easier path (although not necessarily ideal) because you can at least get started.\n\n>Finally, still for SVMs, another thing you can do is come up with a similarity function rather than a distance function and plug it in as a kernel (technically this function must generate positive-definite matrices). This function can be constructed any way you like and can take into account the disparity in ranges of features.\n\n---\n\n\n>![](https:\/\/miro.medium.com\/max\/1600\/1*WPX-ktzf1KIoGmwD8TuCtg.gif)<\/em><\/p>\n\n---\n\n>Learn more about Normalization: https:\/\/neerajkumar.org\/writings\/svm\/\n\n---","bca3511d":"---\n---\n\n><center><img src=\"https:\/\/miro.medium.com\/max\/1290\/1*qYg3y4_Qaj00U7sMU_XlaQ.gif\"><\/center>\n\n---\n---","43fec0a7":"---\n---\n\n> <center><font color=\"yellow\"><h1 style=\"text-transform: uppercase; text-shadow: 1px 1px;\"><b>Logic behind Linear SVC<font size=5><\/font> <\/b><\/h1><\/center>\n\n\n---\n---","ecfe276f":"---\n---\n\n> <center><img src=\"https:\/\/miro.medium.com\/max\/2000\/0*0KFWF3Rp5uhGnppK.gif\"><\/center>\n\n---\n---","7685d176":"---\n---\n\n> <center><font color=\"yellow\"><h1 style=\"text-transform: uppercase; text-shadow: 1px 1px;\"><b>Optimize our goal function\n<font size=5><\/font> <\/b><\/h1><\/center>\n\n\n---\n---"}}