{"cell_type":{"ad24aae9":"code","fa866a70":"code","45e228f8":"code","8b1a8ae2":"code","df1ea4eb":"code","4b1c6b74":"code","5999d412":"code","cbde2280":"code","0ef915ea":"code","0006cb88":"markdown","9b6e35a6":"markdown","fe922fb9":"markdown","90c4fb17":"markdown","c210d03c":"markdown","c2b30de2":"markdown","bbe91cd1":"markdown"},"source":{"ad24aae9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fa866a70":"train = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')","45e228f8":"start_point=datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n\ntrain['D1'] = np.where(np.isnan(train['D1']), 0, train['D1'])\ntrain['D3'] = np.where(np.isnan(train['D3']), 0, train['D3'])\n\n# TransactionDT is a number of sec from some starting point, 2017-11-30 in our case.\ntrain['TransactionDays'] = np.round(train['TransactionDT']\/(60*60*24), 0)\ntrain['TransactionDate'] = [start_point + relativedelta(seconds=secs) for secs in train['TransactionDT']]\n\n# The first transaction date is nothing but difference between TransactionDate and D1\ntrain['FirstTransaction'] = [dt1-relativedelta(days=days) for dt1, days in zip(train['TransactionDate'], train['D1'])]\ntrain['FirstTransaction'] = [datetime.datetime.strftime(d, '%Y-%m-%d') for d in train['FirstTransaction']]\n","8b1a8ae2":"train.head()","df1ea4eb":"'''\nThis function creates UserID based on some pull of columns \nand then calculates the number of days from the previous transaction on UserID level\n'''\ndef get_user_id(df, by, start_point=datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')):\n\n    df['TransactionDays'] = np.round(df['TransactionDT']\/(60*60*24), 0)\n    df['TransactionDate'] = [start_point + relativedelta(seconds=secs) for secs in df['TransactionDT']]\n    df['D1'] = np.where(np.isnan(df['D1']), 0, df['D1'])\n\n    df['FirstTransaction'] = [dt1-relativedelta(days=days) for dt1, days in zip(df['TransactionDate'], df['D1'])]\n    df['FirstTransaction'] = [datetime.datetime.strftime(d, '%Y-%m-%d') for d in df['FirstTransaction']]\n    df['D3'] = np.where(np.isnan(df['D3']), 0, df['D3'])\n\n    df[by] = df[by].fillna(-99)\n\n    grouped = df.groupby(by, as_index=False)['TransactionID'].min()\n    grouped = grouped.rename(columns={'TransactionID': 'UserID'})\n\n    df = pd.merge(df, grouped, on=by, how='left')\n\n    df = df.sort_values(['TransactionDays'], ascending=True).groupby(['UserID']).head(df.shape[0])\n    df['diffs'] = df.sort_values(['TransactionDT'], ascending=True).groupby(['UserID'])['TransactionDT'].transform(lambda x: x.diff())\n\n    df['firstInRow'] = np.where(np.isnan(df['diffs']), 1, 0)\n    df['diffs'] = np.where(np.isnan(df['diffs']), 0, df['diffs'])\n    df['diffs'] = np.round(df['diffs'] \/ (60 * 60 * 24), 0)\n\n    return df","4b1c6b74":"# Let's check how it works. The example below creates UserID based on card_x, addr1, and FirstTransaction features.\n\nby = ['card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'FirstTransaction']\ntrain_u = get_user_id(train, by, start_point=datetime.datetime.strptime('2017-11-30', '%Y-%m-%d'))","5999d412":"# Now we can calculate how accurate this UserID\ncorrectDiffD3 = train_u[(train_u['diffs'] == train_u['D3']) & (train_u['firstInRow'] == 0)].shape[0] \/ train_u[(train_u['firstInRow'] == 0)].shape[0]\ncorrectDiffD3","cbde2280":"by = ['card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'FirstTransaction', 'V1']\ntrain_u = get_user_id(train, by, start_point=datetime.datetime.strptime('2017-11-30', '%Y-%m-%d'))\n\ncorrectDiffD3 = train_u[(train_u['diffs'] == train_u['D3']) & (train_u['firstInRow'] == 0)].shape[0] \/ train_u[(train_u['firstInRow'] == 0)].shape[0]\ncorrectDiffD3","0ef915ea":"by = ['card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'FirstTransaction', 'V29']\ntrain_u = get_user_id(train, by, start_point=datetime.datetime.strptime('2017-11-30', '%Y-%m-%d'))\n\ncorrectDiffD3 = train_u[(train_u['diffs'] == train_u['D3']) & (train_u['firstInRow'] == 0)].shape[0] \/ train_u[(train_u['firstInRow'] == 0)].shape[0]\ncorrectDiffD3","0006cb88":"88% - not bad! Let's try to add V1 into UserID","9b6e35a6":"I ended up in 42nd place...I know that it was tiring competition for everyone, with deadline extension and lots of arguing regarding disclosing magic but anyway thanks organizers for their continuous answers and clarifications on the forum.\n\nThe full solution can be found [here](https:\/\/github.com\/Aziko13\/IEEE-CIS-Fraud-Detection). Hope you find it useful.","fe922fb9":"V1 makes UserID worse...Let's try to add V29.","90c4fb17":"The main idea was that **D1 represents the number of days from the first transaction** \nand **D3 indicates the number of dates from the previous transaction**, also we know that \n**TransactionDT is seconds from some starting point**. \nTaking '2017-11-30' as a starting point we can calculate the first transaction date.","c210d03c":"V29 works! Following this logic I ended up with the next features for UserID:\n\n['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'FirstTransaction',\n 'V29', 'V53', 'V55', 'V57', 'V59', 'V61', 'V63', 'V69', 'V138', 'V139', 'V140', 'V141', 'V142',\n 'V144', 'V145', 'V146', 'V147', 'V148', 'V150', 'V151', 'V152', 'V157', 'V159', 'V160',\n 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V305', 'V311', 'V322', 'V323',\n 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330',\n 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']\n \nAdding UserID as categorical feature into catboost helped me a lot. Single catboost could give me 0.9566 on public leaderboard.","c2b30de2":"Competition is over and first of all congrats to all participants! \nBig shout out to winners! Well deserved!!! :) \n\nIn this kernel, I will try to explain one feature that gave me a great boost in this competition.\n\nIn my [previous kernel](https:\/\/www.kaggle.com\/akasyanama13\/eda-what-s-behind-d-features) I made an assumption regarding D features. It was a small hint for so-called 'magic' :)\n","bbe91cd1":"Now we can use **FirstTransaction** column as an addition to **card_x** features to identify **UserID**. But this is not the end. Some V_x features are also can be used for grouping. \nI tried to add them successively and checked if on UserID level we calculate 'days from the previous transaction' and it matches to D3."}}