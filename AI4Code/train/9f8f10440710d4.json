{"cell_type":{"0909c17b":"code","1ed13a3b":"code","b448f8a0":"code","3678d1e3":"code","619ec2b4":"code","63cd5949":"code","854358dc":"code","e7be1a19":"code","8c1ea582":"code","3db8a778":"code","3c16de04":"code","1f83880a":"code","aa22c32b":"code","a0b5cff6":"code","104e4a5a":"code","30f33575":"code","76072f12":"code","786b7c4e":"code","703dd6d7":"code","ff75ab13":"code","a5b71062":"code","b17086ee":"code","f76a3c39":"code","a5f53a8f":"code","90cc2e08":"code","689a8a5d":"code","db351593":"markdown","06a14280":"markdown","9dafa672":"markdown","fece8c2d":"markdown","bb2a9beb":"markdown","f671b4b6":"markdown","d37b4d66":"markdown","eabf452b":"markdown","2ab229b6":"markdown","ed049703":"markdown","48b2c4fa":"markdown","7eed6507":"markdown","72fe566c":"markdown","d411e41b":"markdown","4b3c45df":"markdown","176eb7f4":"markdown","3133462c":"markdown","e88a567f":"markdown","6b31125f":"markdown","117b9bf1":"markdown","bfa25a6b":"markdown","84609a69":"markdown","c06b48c8":"markdown","88d23bd6":"markdown","13f99272":"markdown","3e1c0d65":"markdown","88b4067f":"markdown","a0d721ba":"markdown","e54159f8":"markdown","472733d9":"markdown"},"source":{"0909c17b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1ed13a3b":"DATA_DIR='\/kaggle\/input\/data-science-bowl-2019'","b448f8a0":"train = pd.read_csv(os.path.join(DATA_DIR,'train.csv'))\ntest = pd.read_csv(os.path.join(DATA_DIR,'test.csv'))\ntrain_labels = pd.read_csv(os.path.join(DATA_DIR,'train_labels.csv'))","3678d1e3":"# Recreate the train_labels.csv file for episodes in the training data\n\ndef extract_accuracy_group(df: pd.DataFrame) -> pd.DataFrame:\n    # Regex strings for matching Assessment Types\n    assessment_4100 = '|'.join(['Mushroom Sorter',\n                                'Chest Sorter',\n                                'Cauldron Filler',\n                                'Cart Balancer'])\n    assessment_4110 = 'Bird Measurer'\n    \n    # 1. Extract all assessment scoring events\n    score_events = df[((df['title'].str.contains(assessment_4110)) & (df['event_code']==4110)) |\\\n                      ((df['title'].str.contains(assessment_4100)) & (df['event_code']==4100))]\n    \n    # 2. Count number of correct vs. attempts\n    # 2.a. Create flags for correct vs incorrect\n    score_events['num_correct'] = 1\n    score_events['num_correct'] = score_events['num_correct'].where(score_events['event_data'].str.contains('\"correct\":true'),other=0)\n    \n    score_events['num_incorrect'] = 1\n    score_events['num_incorrect'] = score_events['num_incorrect'].where(score_events['event_data'].str.contains('\"correct\":false'),other=0)\n    \n    # 2.b. Aggregate by `installation_id`,`game_session`,`title`\n    score_events_sum = score_events.groupby(['installation_id','game_session','title'])['num_correct','num_incorrect'].sum()\n    \n    # 3. Apply heuristic to convert counts into accuracy group\n    # 3.a. Define heuristic\n    def acc_group(row: pd.Series) -> int:\n        if row['num_correct'] == 0:\n            return 0\n        elif row['num_incorrect'] == 0:\n            return 3\n        elif row['num_incorrect'] == 1:\n            return 2\n        else:\n            return 1\n        \n    # 3.b. Apply heuristic to count data\n    score_events_sum['accuracy_group'] = score_events_sum.apply(acc_group,axis=1)\n    \n    return score_events_sum.reset_index()","619ec2b4":"test_labels = extract_accuracy_group(test)","63cd5949":"def build_episode_game_sessions(df):\n    return pd.DataFrame(index=df['game_session'])\n\ndef build_starts_only(df):\n#     return df[df['event_code']==2000]\n    return df.groupby(['installation_id','game_session']).last()\n\ndef build_start_end_times(df, df_labels):\n    start_end_times = pd.merge(left=df, right=df_labels,left_on='game_session', right_index=True)\\\n        .groupby(['installation_id','game_session'])\\\n        .first()['timestamp']\n    start_end_times = start_end_times.reset_index().sort_values(by=['installation_id','timestamp'])\n    start_end_times.columns = ['installation_id','episode_session','end_time']\n    start_end_times['start_time'] = start_end_times.groupby('installation_id')['end_time'].shift(1,fill_value='2018-09-11T18:56:11.918Z')\n    return start_end_times\n\ndef append_times_to_labels(labels, start_end_times):\n    new_labels = pd.merge(left=labels,\n                          right=start_end_times,\n                          left_on=['game_session','installation_id'],\n                          right_on=['episode_session','installation_id'])\n    return new_labels.drop('game_session',axis=1)\n\ndef add_labels_to_sessions(sessions, labels_with_times):\n    outer = pd.merge(left=sessions.reset_index(),\n                     right=labels_with_times,\n                     left_on='installation_id',\n                     right_on='installation_id',\n                     suffixes=('','_episode') )\n    labeled_sessions = outer[(outer['timestamp']>=outer['start_time']) & (outer['timestamp']<=outer['end_time'])]\n    \n    labeled_session_ids = pd.DataFrame(index=labeled_sessions['game_session'],\n                                       data=np.ones(len(labeled_sessions)),\n                                       columns=['has_label'])\n    unlabeled_sessions = pd.merge(sessions.reset_index(),labeled_session_ids,how='left',left_on='game_session',right_index=True)\n    unlabeled_sessions = unlabeled_sessions[unlabeled_sessions['has_label']!=1].drop('has_label',axis=1)\n    return labeled_sessions, unlabeled_sessions\n\ndef build_session_labels(events, labels):\n    episodes_game_sessions = build_episode_game_sessions(labels)\n    starts = build_starts_only(events)\n    start_end_times = build_start_end_times(starts, episodes_game_sessions)\n    labels_with_times = append_times_to_labels(labels, start_end_times)\n    return add_labels_to_sessions(starts, labels_with_times)","854358dc":"train_bm_starts, train_no_asmt = build_session_labels(train, train_labels)","e7be1a19":"test_bm_starts, test_to_predict = build_session_labels(test, test_labels)\nbm_starts = pd.concat([train_bm_starts, test_bm_starts], sort=True)","8c1ea582":"idx_to_titles = list(set(bm_starts['title'])) + ['<START>','<END>']\ntitles_to_idx = {title: idx for idx, title in enumerate(idx_to_titles)}\nbm_starts['title_idx'] = bm_starts['title'].map(titles_to_idx)","3db8a778":"idx_to_asmt = list(set(bm_starts['title_episode']))\nasmt_to_idx = {title: idx for idx, title in enumerate(idx_to_asmt)}\nbm_starts['asmt_idx'] = bm_starts['title_episode'].map(asmt_to_idx)","3c16de04":"asmt_to_idx","1f83880a":"bm_session_columns = ['installation_id','episode_session',\n        'accuracy_group', 'asmt_idx']\nbm_episodes = bm_starts.groupby(bm_session_columns)['title_idx'].aggregate(lambda x: [len(idx_to_titles)-2] + list(x) ).reset_index()","aa22c32b":"bm_episodes.head()","a0b5cff6":"num_asmt, num_scores, num_titles = len(idx_to_asmt), 4, len(idx_to_titles)\ntransition_matrix = np.ones((num_asmt, num_scores, num_titles, num_titles),dtype=np.float32)\n\nasmt_idx = list(bm_episodes['asmt_idx'])\naccuracy_group = list(bm_episodes['accuracy_group'])\npaths = list(bm_episodes['title_idx'])\n\nfor asmt, acc, path in zip(asmt_idx, accuracy_group, paths):\n    for prior, current in zip(path[:-1],path[1:]):\n        transition_matrix[asmt, acc, prior, current] += 1\n        \ntransition_matrix = transition_matrix \/ transition_matrix.sum(axis=-1).reshape(*transition_matrix.shape[:-1],1)","104e4a5a":"def build_priors(sessions):\n    by_asmt_by_acc = sessions.groupby(['asmt_idx','accuracy_group'])['episode_session'].count().unstack(-1).values\n    by_asmt = np.sum(by_asmt_by_acc,axis=-1).reshape(-1,1)\n    return by_asmt_by_acc\/by_asmt\n\npriors = build_priors(bm_episodes)","30f33575":"from sklearn.metrics import cohen_kappa_score\n\nclass PathNaiveBayes:\n    def __init__(self, priors_, transitions_, start_, end_):\n        self.priors = priors_\n        self.transitions = transitions_\n        self.start = start_\n        self.end = end_\n        \n    def predict(self, asmt, seq):\n        # Check if the provide sequence has a start code\n        if seq[0] != self.start:\n            seq = [self.start] + seq + [self.end]\n            \n        # Use sum of log probabilities instead of multiply probabilities directly to help avoid underflow\n        \n        # Initialize with our prior probability\n        log_prob = np.log(self.priors[asmt])\n        \n        # Calculate the Markov chain probability of the user's path\n        for prev, current in zip(seq[:-1],seq[1:]):\n            log_prob += np.log(self.transitions[asmt,:,prev,current])\n            \n        # Our prediction is the accuracy score with the highest posterior log probability\n        return {\"prediction\": np.argmax(log_prob), \"probabilities\": log_prob }\n    \n    def evaluate(self, asmts, seqs, scores, verbose=False):\n        correct, total = np.zeros((self.priors.shape[0])), np.zeros((self.priors.shape[0]))\n        preds = []\n        for asmt, seq, score in zip(asmts, seqs, scores):\n            pred = self.predict(asmt, seq)['prediction']\n            if pred == score:\n                correct[asmt] += 1\n            total[asmt] += 1\n            preds.append(pred)\n            if verbose:\n                print(f\"Asmt:{asmt} True:{score} Pred:{pred}\")\n        acc_per_asmt = correct\/total\n        total_acc = np.sum(correct)\/np.sum(total)\n        kappa = cohen_kappa_score(preds,scores,weights='quadratic')\n        \n        return {'predictions': preds,\n               'accuracy_by_asmt': acc_per_asmt,\n               'accuracy': total_acc,\n               'kappa': kappa}","76072f12":"model = PathNaiveBayes(priors,transition_matrix, titles_to_idx['<START>'], titles_to_idx['<END>'] )","786b7c4e":"## Results on Training Data","703dd6d7":"results = model.evaluate(asmts=bm_episodes['asmt_idx'],\n               scores=bm_episodes['accuracy_group'],\n              seqs=bm_episodes['title_idx'])\nresults.pop('predictions')\nprint(results)","ff75ab13":"test.set_index(['installation_id','timestamp'])\nlast_entries = test.assign(rn=test.sort_values(['timestamp'], ascending=False)\\\n            .groupby(['installation_id'])\\\n            .cumcount() + 1)\\\n            .query('rn == 1')\\\n            .sort_values(['installation_id'])\nlast_entries[['installation_id','title']].to_csv('test_final_title.csv',index=False)","a5b71062":"submission = pd.read_csv('test_final_title.csv')\nsubmission['asmt_idx'] = submission['title'].map(asmt_to_idx)\nsubmission_asmt = pd.DataFrame(index=submission['installation_id'], data=submission['asmt_idx'].values, columns=['asmt_idx'])","b17086ee":"test_to_predict['title_idx'] = test_to_predict['title'].map(titles_to_idx)\ntest_to_predict = pd.merge(left=test_to_predict,right=submission_asmt,left_on='installation_id',right_index=True)\ntest_to_predict","f76a3c39":"test_session_columns = ['installation_id','asmt_idx']\ntest_episodes = test_to_predict.groupby(test_session_columns)['title_idx'].aggregate(lambda x: [len(idx_to_titles)-2] + list(x) ).reset_index()\ntest_episodes","a5f53a8f":"def predict_row(row):\n    return model.predict(row['asmt_idx'],row['title_idx'])['prediction']\n\ntest_episodes['accuracy_group'] = test_episodes.apply(predict_row,axis=1)\ntest_episodes","90cc2e08":"test_episodes[['installation_id','accuracy_group']].to_csv('submission.csv',index=False)","689a8a5d":"test_episodes.groupby(['asmt_idx','accuracy_group'])['installation_id'].count().unstack(-1)","db351593":"He we calculate a prior probability of a given `accuracy_group` for each assessment type.","06a14280":"$$\\arg\\max_y P(y| \\boldsymbol{x}) \\propto P(y)P(x_0|y)\\displaystyle\\prod_{t=1}^{T} P(x_t | x_{t-1}, y)$$","9dafa672":"### Build prediction - final assessment tables","fece8c2d":"Here we calculate the transition probabilities by assessment and by accuracy group (assuming that the `accuracy group` of the `episode_session` is pertinent to our prediction).  It should be noted that we use Laplace smoothing to help with rare transitions - as such our one hyperparameter is this pseudo count, or how much smoothing we should use.","bb2a9beb":"## Create consolidated labeled session Dataframes","f671b4b6":"$$P( \\boldsymbol{x})=P(x_0,x_1,\\ldots,x_T) = P(x_0)\\displaystyle\\prod_{t=1}^{T} P(x_t | x_{t-1})$$","d37b4d66":"# Key Assumptions","eabf452b":"We assume that all events not related to a labeled episode belong to the episode under evaluation.","2ab229b6":"# Problem Formulation","ed049703":"# Path Naive Bayes Model","48b2c4fa":"## Convert Strings to Indices","7eed6507":"### Model\n\n* Student behavior exhibits the Markov property\n* Students in different accuracy group have different transition probabilities\n* The students' accuracy groups are strongly correlated with the type of assessment to be graded\n  * We will construct distinct models for each assessment type\n* This kernel is only for a demonstration of a simple sequence model so there is limited model evaluation, i.e., I'm not doing any CV\n\n### Data\nTo make the data manageable, we will make some strong assumptions in formatting a data for predicition.\n\n* Let's define all of the events between each assessment as an \"episode\"\n* We will only calculate the path probabilities for each episode\n  * We will ignore all path data prior to the most recent assessment before the predicted assessment\n* For simplicity, we will only consider each `game_session` as a state\n  * An alternative is to use all of the individual events as states","72fe566c":"# Conclusion","d411e41b":"*Aside:* We should also note that this interpretation also lends itself well to a Hidden Markov Model as well allowing for a student's latent accuracy group potential to evolve over time (using the module type as the observations and accuracy group potential at time step $t$ as the hidden state).","4b3c45df":"The benefits of this model are that:\n* It is simple\n* It is fast\n* It is deterministic\n* There are no paramaters to train, and only one hyperparameter\n* It is very interperatable\n  * We can see which paths through the program lead to the highest accuracy groups\n  * The transition probabilities of the highest peforming students could be used by the app to help suggest the next module for students.","176eb7f4":"# Potential Drawbacks","3133462c":"Some key drawbacks with this approach could be:\n* By limiting the predictions to just each episode, we lose the benefit of all event history data prior to the last assessment\n  * Has the student attempted the assessment title under observation?\n  * If so, how many times and how did they perform?\n  * Has the student attempted any other assessment titles?\n  * If so, how many times and how did they perform?\n  * How much experience does the student have with using the site?\n* By enforcing the Markov property on model, it will have a short memory\n  * Do transition probabilities depend only on the current state?\n  * How important is what the student was doing two states ago to their next state?\n  * ...or all states since account creation?\n* By only looking at the `game_session` titles, we lose the benefit of using their behavior within each module as a feature.\n  * How deeoly did the student engage in the games?\n  * How does the student perform in the activities?\n  * How long does the student spend listening to instruction?\n\nThese drawbacks point to potential avenues for improving a model of this type in the future.","e88a567f":"We are predicting an accuracy group for students on a given assessment using the event history of their interaction with the PBS KIDS Measure Up! app.  Taking inspiration from the beautiful kernels from [J Hogg](https:\/\/www.kaggle.com\/c\/data-science-bowl-2019\/discussion\/123102) and [Buffalo Spdwy](https:\/\/www.kaggle.com\/barnwellguy\/eye-candy-player-activity-sequence-visualization), let's assume that a student's progression from module to module within the app follows a Markov process such that the probability that they will move to module $x$ at time $t$ only depends on which module the completed at time $t-1$.  Then we can compute the probability of a specific sequence of modules (or the student's path through the website) as:","6b31125f":"One hypothesis to test is whether students that perform better on the assessments take different paths than those from other accuracy groups.  To do so, let's now condition the transition probabilties also on the student's accuracy group.  This would then allow us to calculate a posterior probability of a student acheiving an accuracy group of $y$ given a path history $ \\boldsymbol{x}$.  As a type of Naive Bayes classifier, we would estimate that the student's accuracy group would be the one with the largest posterior probability based on their path through the site.","117b9bf1":"Now for the good stuf, the code...","bfa25a6b":"Let's collect all of the game sessions into a list for each episode.","84609a69":"We should note that we are using all of the labeled episodes from both the training and test sets.","c06b48c8":"## Build Probability Tables","88d23bd6":"## Create Session Series","13f99272":"# Define Model","3e1c0d65":"These functions are used to group the events and game sessions into episodes.  Each episode takes on an `episode_session` which the is `game_session` of the assessment taken at the end of episode.","88b4067f":"## Get last prediction episode","a0d721ba":"This model only performs slightly better in terms of a QWK than a baseline of using only the prior probabilities by assessment type.  While the model's Dory problem (i.e., short memory) should be addressed by using longer portions of the user's history, there appears to be some incremental predictive power to only analyzing the probability of the path immediately prior to the assessment to be taken.","e54159f8":"# Data Preparation","472733d9":"# Make Predictions"}}