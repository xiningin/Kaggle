{"cell_type":{"085e923c":"code","522cfdc3":"code","2e58c136":"code","4b24bec8":"code","02e366de":"code","41d66d21":"code","276d199f":"code","ccc61f2a":"code","fda533c8":"code","69519721":"code","080fda5f":"code","57f0592d":"code","396cb4d6":"code","1ee020c7":"markdown","8dd33350":"markdown","aefce323":"markdown","fd1267b9":"markdown","ea34d259":"markdown","9abca5d5":"markdown","fee4695c":"markdown","6c8aeb8d":"markdown"},"source":{"085e923c":"import os\nimport logging\nfrom datetime import datetime\nfrom glob import glob\nimport pickle\nimport imdb\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\ndata_dir = \".\/\"\nINITIAL_FILE = os.path.join(data_dir, \"netflix_titles.csv\")\nEXPANDED_FILE = os.path.join(data_dir, \"expanded_netflix_titles.csv\")\nCLEAN_EXPANDED_FILE = os.path.join(data_dir, \"clean_expanded_netflix_titles.csv\")","522cfdc3":"ia = imdb.IMDb()\nlogging.getLogger('imdbpy').setLevel(logging.ERROR)","2e58c136":"def split_data(df, n):\n    \"\"\"Split df by n parts\"\"\"\n    data_len = df.shape[0]\n    splited_data = []\n    if data_len%n==0:\n        delta = data_len\/\/n\n        for i in range(0, data_len, delta):\n            splited_data.append(df.iloc[i:i+delta].copy())\n    else:\n        delta = data_len\/\/n\n        remains = data_len%n\n        for i in range(0, data_len-(delta+remains), delta):\n            splited_data.append(df.iloc[i:i+delta].copy())\n        splited_data.append(df.iloc[i+delta:i+2*delta+remains].copy())\n    return splited_data\n        \n            \n    print([a.shape[0] for a in splited_data])\n            \ndef get_imdb_data(movie):\n    return ia.get_movie(movie.getID()).data\n\ndef parse_imdb(titles: pd.Series, release_years: pd.Series=None, \n               verbosity: int=0, release_year_delta: int=0):\n    \"\"\"Parse movie data from imdb, using `titles` for search and difference between \n    `release_years` and searched result for verification. Valid difference between years\n    set by `release_year_delta` by default 0 (mean that they must be equal).\n    \n    - verbosity=1: add progress bar;\n    - verbosity=2: add verified_result\/titles_count.\n    \"\"\"\n    data = {}\n    count = 0\n    exception_count = 0\n    use_release_year = release_years is not None\n    iterator = tqdm(titles.index, desc=\"Parse data\") \\\n        if verbosity > 0 else titles.index\n    for i in iterator:\n        count += 1\n        title = titles.loc[i]\n        release = release_years.loc[i] if use_release_year else None\n        try:\n            searhing_results = ia.search_movie(title)\n        except Exception as e:\n            print(e)\n            exception_count += 1\n            continue\n        result = next(iter(searhing_results), None)\n        \n        if result is not None:\n            title_cond = result[\"title\"].lower() == title.lower()\n            year_cond = abs(result.get(\"year\") - release) <= release_year_delta \\\n                if release is not None else True\n            if title_cond and year_cond:\n                data[title] = get_imdb_data(result)\n        if verbosity > 1:\n            iterator.set_postfix({\n                \"data_found\":len(data)\/count*100, \n                \"exception_count\":exception_count\n            })\n            \n    return data","4b24bec8":"# Split data on N parts\nN = 10\nVERBOSITY = 3\nTEMPDIR_PATH = \"\"\nVALID = False\n# For VALID=True\n# Note. Using release year for validation doesn't correct,\n# because data is noisy. \n# For example: \n# - `Blood & Water` real release 2019 (in dataset 2021);\n# - `The Great British Baking Show` real release 2010 (in dataset 2021).\nREALEASE_YEAR_DELTA = 1\n\ndef autosave_parsing(df, n, verbosity=0, valid=False):\n    \"\"\"Saving every data block and resumes work from last saved block.\"\"\"\n    prev = glob(os.path.join(TEMPDIR_PATH, \"temp\/*.pickle\"))\n    data = []\n    if len(prev) > 0:\n        for f in prev:\n            with open(f, \"rb\") as f:\n                data.append(pickle.load(f))\n    else:\n        !mkdir \".\/temp\"\n    \n    portions = split_data(df, n)\n    for i, portion in zip(range(len(data), N), portions[len(data):]):\n        if verbosity > 2:\n            print(f\"Batch #{i}\")\n        release_year = portion.release_year if valid else None\n        data_portion = parse_imdb(portion.title, release_year, \n                                  verbosity=verbosity, \n                                  release_year_delta=REALEASE_YEAR_DELTA)\n        data.append(data_portion)\n        with open(f\"temp\/{i}.pickle\", \"wb\") as f:\n            pickle.dump(data_portion, f)\n          \n    !rm -r temp\n    final_data = {}\n    for d in data:\n        final_data.update(d)\n    if verbosity > 1:\n        print(\"Total data found: %.2f\" % (len(final_data)\/df.shape[0]))\n    return final_data\n\ndef load_imdb_data(fp=None):\n    data = pd.read_csv(INITIAL_FILE)\n    p = \"imdb_data_*.pickle\"\n    if (len(glob(p)) > 0) or (fp is not None):\n        fp = fp if fp is not None else next(iter(glob(p)))\n        with open(fp, \"rb\") as f:\n            data = pickle.load(f)\n    else:\n        data = autosave_parsing(data, n=N, verbosity=VERBOSITY, valid=VALID)\n        with open(\"imdb_data_%s.pickle\" % datetime.now().strftime(\"%d-%m-%y\"), \"wb\") as f:\n            pickle.dump(data, f)\n    return data\n\nimdb_data = load_imdb_data()","02e366de":"def create_expanded_data(df=None):\n    if os.path.exists(EXPANDED_FILE):\n        expanded_data = pd.read_csv(EXPANDED_FILE).set_index(\"title\")\n    else:\n        ratings = {title:data.get(\"rating\") for title, data in imdb_data.items()}\n        data = df if df is not None else pd.read_csv(INITIAL_FILE)\n        data_title = data.set_index(\"title\")\n        expanded_data = pd.concat([data_title, pd.Series(ratings, name=\"imdb_rating\")], axis=1)\n        expanded_data = expanded_data.reset_index().rename(columns={\"index\":\"title\"})\n        expanded_data.to_csv(EXPANDED_FILE, index=False)\n    return expanded_data\n\ndata = create_expanded_data()","41d66d21":"imdb_country = pd.Series({key:v.get(\"countries\") for key, v in imdb_data.items()})\nimdb_years = pd.Series({key:v.get(\"year\") for key, v in imdb_data.items()})","276d199f":"data[\"imdb_year\"] = imdb_years\ndata[\"imdb_country\"] = imdb_country","ccc61f2a":"incorrect_year = (data[\"imdb_year\"] != data[\"release_year\"]) & (~data[\"imdb_year\"].isna())\ncount_unequal = incorrect_year.sum()\nprint(f\"Unequal IMDb years and Release {count_unequal}\")","fda533c8":"imdb_countries = data[[\"country\", \"imdb_country\"]]","69519721":"imdb_countries.loc[:, \"country\"] = imdb_countries.country.str.split(\", \")","080fda5f":"def contain(x):\n    x1, x2 = x\n    if np.any(pd.notna(x1)) and np.any(pd.notna(x2)):\n        if isinstance(x1, list):\n            return any([e in x2 for e in x1])\n        else:\n            return x1 in x2\n    else:\n        return False\n\nincorrect_countries = ~imdb_countries.apply(contain, axis=1)\nprint(f\"Unequal IMDb country and Country {incorrect_countries.sum()}\")","57f0592d":"clean_data = data.drop(data[(incorrect_countries & incorrect_year)].index, axis=0)\nprint(\"Before:\", data.shape[0], \"\\nAfter:\", clean_data.shape[0])","396cb4d6":"clean_data.drop(columns=[\"imdb_country\", \"imdb_year\"], inplace=True)\nclean_data.reset_index(inplace=True)\nclean_data.to_csv(CLEAN_EXPANDED_FILE, index=False)","1ee020c7":"## Parse","8dd33350":"## Parsed data at the 13.01.22\nI parsed data and added it to the kaggle dataset [IMDb Scores for Netflix Movies and TV Shows](https:\/\/www.kaggle.com\/sergeyzelenovskiy\/netflix-films-tv-shows-imdb-scores).","aefce323":"## Clean data\nClean data by remove rows with unequal years and countries.\n\nNote. Maybe it doesn't best way but it's simple. ","fd1267b9":"## Expand data by adding Rating column and save it","ea34d259":"### Not equal years\nIMDb year and Release year don't match.","9abca5d5":"# Parse IMDb rating.\nThis notebook expands the data by adding `imdb_rating` column.\n\nNote. Data parsing takes more than 9 hours ,so best way is use notebook one your own PC. ","fee4695c":"### Parsing using autosave\nData parsing takes many time(~10 hours), so best way is start this notebook somewhere else.\n\nYou can split data on several parts(or maybe batches) and after every part notebook saves parsed data in the `temp` directory.","6c8aeb8d":"### Not equal countries\nIMDb country and Country don't match."}}