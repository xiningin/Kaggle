{"cell_type":{"51fa98cf":"code","7ddcc72f":"code","d76919e5":"code","535842cd":"code","75b4e8f9":"code","0ffa414b":"code","1acef553":"code","72b34488":"code","ac0813f5":"code","2ef299e4":"code","a1005f7d":"code","27d2e1f1":"code","040f68d7":"code","735c4713":"code","d1946b51":"code","a1452389":"code","55763353":"code","4c9d737c":"code","542f297f":"code","6739f3cd":"code","6be2b283":"markdown","07aed184":"markdown","e5f70ae0":"markdown","e5d88fd6":"markdown","fdfa76b0":"markdown","3dc7a802":"markdown","ac3001a3":"markdown","c9f7b538":"markdown","70ae42b9":"markdown","ea8c63fa":"markdown","0bec3df1":"markdown","68368688":"markdown","b1fb1780":"markdown","9b201157":"markdown","390d06d9":"markdown","4d534a08":"markdown","33f056d5":"markdown","599014cc":"markdown","e98834f5":"markdown","df995c6b":"markdown","85021830":"markdown","9b6d5bac":"markdown","ca12002c":"markdown","74df1b65":"markdown","acef6c82":"markdown","732709bc":"markdown","9d138207":"markdown","612835ae":"markdown"},"source":{"51fa98cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ddcc72f":"data = pd.read_csv(\"\/kaggle\/input\/price-volume-data-for-all-us-stocks-etfs\/Stocks\/tsla.us.txt\")","d76919e5":"data.head()","535842cd":"print(\"Do you have a null column? \\n\", data.isna().sum())","75b4e8f9":"training_size = int(len(data)*0.80)\ndata_len = len(data)\n\ntrain, test = data[0:training_size],data[training_size:data_len]","0ffa414b":"print(\"Training Size --> \", training_size)\nprint(\"total length of data --> \", data_len)\nprint(\"Train length --> \", len(train))\nprint(\"Test length --> \", len(test))","1acef553":"# the part of data that we will use as training.\ntrain = train.loc[:, [\"Open\"]].values\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\ntrain_scaled = scaler.fit_transform(train)","72b34488":"end_len = len(train_scaled)\nX_train = []\ny_train = []\ntimesteps = 40\n\nfor i in range(timesteps, end_len):\n    X_train.append(train_scaled[i - timesteps:i, 0])\n    y_train.append(train_scaled[i, 0])\nX_train, y_train = np.array(X_train), np.array(y_train)","ac0813f5":"X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nprint(\"X_train --> \", X_train.shape)\nprint(\"y_train shape --> \", y_train.shape)","2ef299e4":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout","a1005f7d":"regressor = Sequential()\n\nregressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1],1)))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units = 50, return_sequences = True))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units = 50, return_sequences = True))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units = 50))\nregressor.add(Dropout(0.2))\n\nregressor.add(Dense(units = 1))","27d2e1f1":"regressor.compile(optimizer= \"adam\", loss = \"mean_squared_error\")","040f68d7":"epochs = 100 \nbatch_size = 20","735c4713":"regressor.fit(X_train, y_train, epochs = epochs, batch_size = batch_size)","d1946b51":"test.head()","a1452389":"real_price = test.loc[:, [\"Open\"]].values\nprint(\"Real Price Shape --> \", real_price.shape)","55763353":"dataset_total = pd.concat((data[\"Open\"], test[\"Open\"]), axis = 0)\ninputs = dataset_total[len(dataset_total) - len(test) - timesteps:].values.reshape(-1,1)\ninputs = scaler.transform(inputs)","4c9d737c":"X_test = []\n\nfor i in range(timesteps, 412):\n    X_test.append(inputs[i-timesteps:i, 0])\nX_test = np.array(X_test)\n\nprint(\"X_test shape --> \", X_test.shape)","542f297f":"X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\npredict = regressor.predict(X_test)\npredict = scaler.inverse_transform(predict)","6739f3cd":"plt.plot(real_price, color = \"red\", label = \"Real Stock Price\")\nplt.plot(predict, color = \"black\", label = \"Predict Stock Price\")\nplt.title(\"Stock Price Prediction\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Tesla Stock Price\")\nplt.legend()\nplt.show()","6be2b283":"<ul>\n    <li style = \"color:darkblue;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Finally, we trained our model according to the steps above. We generate our test data based on this number of steps. <\/p> <\/li>\n<\/ul>","07aed184":"<ul>\n    <li style = \"color:darkblue;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Since we use the \"open\" feature while training the model, we will use the same feature while testing. <\/p> <\/li>\n<\/ul>","e5f70ae0":"<a id ='3' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Long Short-Term Memory Networks (LSTM) \ud83d\udcda<\/h3>\n\n![LSTM.png](attachment:7d1ba85d-7876-49fd-a2ab-6d15965b0a6f.png)\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > We can say that the most popular and effective solution to the problems mentioned above is LSTM.<\/p>\n\n<ul>\n  <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > We can say that LSTMs are a special type of RNN. <\/p> <\/li>\n      <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > The memory in RNNs is short lived. Memory in LSTM is long term. Thus, LSTM can also remember the long history. <\/p> <\/li>\n<\/ul>\n\n<p style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" >Let's try to understand the LSTM better by diving into a little more detail of the picture above.<\/p>\n\n![LSTM.png](attachment:d2ad4580-65ea-4968-a35c-909b85ee1bf2.png)\n\n<ul>\n  <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > x --> Indicates whether it will be included.  <\/p> <\/li>\n      <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > + --> indicates that the incoming information has been collected. <\/p> <\/li>\n      <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > \u03b1  --> You can think of them as our sigmoid layers.used to remember and forget something from memory.(0 or 1)  <\/p> <\/li>\n      <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > tanh --> activation function.The reason why tanh is preferred here in general.It is effective against the disappearing gradient problem. We were taking derivatives while updating the parameters. The tanh derivative is preferred as it will not reach zero immediately.it does not slow down learning for a certain period of time. <\/p> <\/li>\n          <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > ht-1 --> YOur entrance from the previous layer.  <\/p> <\/li>\n      <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Xt --> normal input value. <\/p> <\/li>\n          <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Ct-1 --> input from memory of previous LSTM unit. <\/p> <\/li>\n          <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Ct --> Information to be transmitted to the memory of the next LSTM unit. <\/p> <\/li>\n          <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > ht --> Output of the LSTM unit. <\/p> <\/li>\n              <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > ht --> Output of the LSTM unit. <\/p> <\/li>\n                  <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > You can think of arrows as vector structures. (example: array) <\/p> <\/li>\n        <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Think of ht-1 and xt as two parallel paths. <\/p> <\/li>\n<\/ul>","e5d88fd6":"<a id ='6' ><\/a>\n<h4 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> 2 - Input Gate  \u2753<\/h4>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > It decides which information will be stored in memory or not.There are two sections in this second layer. One is the sigmoid part and the other is the tanh part. It is decided using the sigmoid function. tanh gives weight to the passing values according to their level of importance. <\/p> \n\n![Step2.png](attachment:e7340e94-e256-4bad-a3fc-52269756bd5e.png)","fdfa76b0":"<a id ='22' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\" > Predict \u2753<\/h3> \n\n<ul>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >We will make predictions using the model we have created.<\/p> <\/li>\n<\/ul>","3dc7a802":"<a id ='21' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\" > Fit the model \u2753<\/h3> \n\n<ul>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >We train the model we created above using our data. <\/p> <\/li>\n<\/ul>","ac3001a3":"<a id ='17' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Implementing with Keras \u2754 <\/h2>\n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> In this section, we create and fit  LSTM model. <\/p>\n<ul>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Create Model<\/li>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Compile Model<\/li>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Epochs and Batch Size<\/li>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Fit the model<\/li>\n<\/ul>","c9f7b538":"<a id ='19' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\" > Compile Model\u2753<\/h3>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Yes, now we need to compile our model.  <\/p>\n\n<ul>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >optimizer --> The optimizer does the process of updating our parameters for us here. some kind of healer I can say. There are methods used for multiple optimizers, and you should choose the most suitable one for the model. <\/li>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >loss --> It is a number that indicates how good or bad the model is to its prediction. As it approaches 0, the error starts to decrease.<\/li>\n<\/ul>","70ae42b9":"<a id ='9' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\" > Data Preprocessing \u2754<\/h2>\n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> In this section, we will make the data available for LSTM. <\/p>\n<ul>\n     <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Split the data as train and test<\/li>\n     <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Normalize data.<\/li>\n     <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >X_train - y_train ?<\/li>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Reshape<\/li>\n<\/ul>","ea8c63fa":"<ul>\n    <li style = \"color:darkblue;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > We use the data we separated above as our test data. <\/p> <\/li>\n<\/ul>","0bec3df1":"<ul>\n    <li style = \"color:darkblue;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >we distinguish the values \u200b\u200bthat I will guess. <\/p> <\/li>\n<\/ul>","68368688":"<a id ='10' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\" > Split the data as train and test \u2753<\/h3>\n\n<ul>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >In this section, we will separate the data we have as a train and test. <\/p> <\/li>\n<\/ul>","b1fb1780":"<a id ='24' ><\/a>\n<h4 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> CONCLUSION<\/h4>\n\n![result.PNG](attachment:e7764c1b-f472-4c05-8ad4-a39efe6a633c.PNG)\n\n<p style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > You see the difference in success in long-term predictions between the RNN model and the LSTM model.We would make a better model by changing the parameters by playing with the model a little more.However, I wanted the parameters to be the same with the RNN notebook we wrote earlier in order to be able to compare them better.<\/p>\n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > I tried to talk more about the logic of the work without getting into mathematics. I hope I was able to help a little bit.I am waiting for them if you have any questions or suggestions. <\/p>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >  For a more detailed explanation of some episodes, you can check here. <br> \n    RNN Tutorial -->  \n    <a href = \"https:\/\/www.kaggle.com\/rafetcan\/recurrent-neural-n-rnn-tutorial-for-beginners\" >https:\/\/www.kaggle.com\/rafetcan\/recurrent-neural-n-rnn-tutorial-for-beginners<\/a>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >  Seaborn Tutorial -->  \n    <a href = \"https:\/\/www.kaggle.com\/rafetcan\/visualization-tutorial-with-seaborn\" >https:\/\/www.kaggle.com\/rafetcan\/visualization-tutorial-with-seaborn<\/a>\n<\/p>\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >  Plotly Tutorial -->  \n    <a href = \"https:\/\/www.kaggle.com\/rafetcan\/plotly-tutorial-for-beginners\" >https:\/\/www.kaggle.com\/rafetcan\/plotly-tutorial-for-beginners<\/a><\/p>\n    <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >  CNN Tutorial -->  \n    <a href = \"https:\/\/www.kaggle.com\/rafetcan\/convolutional-neural-network-cnn-tutorial\" >https:\/\/www.kaggle.com\/rafetcan\/convolutional-neural-network-cnn-tutorial<\/a>\n<\/p>\n\n<p style = \"text-shadow: 12px 12px 2px #333;color:darkred;font-family:Segoe Print;font-weight:bold\" > THANK YOU<\/p>","9b201157":"<a id ='7' ><\/a>\n<h4 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> 3 - Output Gate  \u2753<\/h4>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Decides what information should be output or not.3. In the 3rd part, we use sigmoid to find out which sections will reach the output first. We then apply tanh to pull these values \u200b\u200bbetween -1 and 1 and multiply them by the output of the sigmoid gate. <\/p> \n\n![Step2.png](attachment:16f2a582-d2c7-4395-9d48-995b2c8be091.png)","390d06d9":"<a id ='15' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\" > Reshape \u2753<\/h3>\n\n<a id ='16' ><\/a>\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > why do we reshape ?  <\/p>\n\n<p style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > Detailed explanation -->  <a href = \"https:\/\/www.kaggle.com\/rafetcan\/recurrent-neural-n-rnn-tutorial-for-beginners\" > https:\/\/www.kaggle.com\/rafetcan\/recurrent-neural-n-rnn-tutorial-for-beginners <\/a> <\/p>\n","4d534a08":"<a id ='23' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\" > Evaluate the model \u2754<\/h3> \n\n<ul>\n    <li style = \"color:darkblue;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >yes, finally let's take a look at our results by comparing our predictions with real data. <\/p> <\/li>\n<\/ul>","33f056d5":"<a id ='2' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Exploding Gradient Problem \u2753<\/h3>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > What did we say above? Updating doesn't matter as the gradients get smaller and eventually approach zero. Here the gradients get too big. It starts to be more than 1. This causes huge updates in training our neural network model. <\/p>\n\n<p style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > This issue causes issues such as long training time, poor performance, and poor accuracy. <\/p>","599014cc":"<a id ='18' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\" > Create Model\u2753<\/h3>\n\n<ul>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >We are importing the libraries we will use for our model.<\/p> <\/li>\n        <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Later, we will create our LSTM model.<\/p> <\/li>\n<\/ul>","e98834f5":"<ul>\n    <li style = \"color:darkblue;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >We can now make an estimate here as data is ready to predict. <\/p> <\/li>\n        <li style = \"color:darkblue;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >inverse_transform --> If you remember before training our model, we normalized our data. converts these values \u200b\u200bto before normalization. <\/p> <\/li>\n<\/ul>","df995c6b":"<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Import Libraries \ud83d\udd16 <\/h2>","85021830":"<a id ='13' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\" > X_train - y_train \u2753<\/h3>\n\n<ul>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >We will separate the normalized data into x_train and y_train. <\/p> <\/li>\n        <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >We will make this distinction 40 steps 40 steps. So we will train in 40 steps. We will anticipate step 41.didn't we understand Let's examine the picture below and visualize this in our minds. <\/p> <\/li>\n<\/ul>\n\n<a id ='14' ><\/a>\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > What is the steps logic? <\/p>\n\n<p style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > Detailed explanation -->  <a href = \"https:\/\/www.kaggle.com\/rafetcan\/recurrent-neural-n-rnn-tutorial-for-beginners\" > https:\/\/www.kaggle.com\/rafetcan\/recurrent-neural-n-rnn-tutorial-for-beginners <\/a> <\/p>","9b6d5bac":"<a id ='1' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Vanishing Gradient Problem \u2753<\/h3>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Recurrent Neural Networks allows us to model time-bound and sequential data such as stock market prediction and machine translation, as we learned from our previous notebook.In the RNN notebook, we saw that our model made very good predictions up to a certain period of time, but when the time interval gets longer, its ability to predict decreases. <\/p>\n\n<p style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > So what caused this? <\/p>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > When the number of layers starts to increase, the information from the previous layer becomes insignificant.actually this is called the gradient problem. RNNs are very complaining about this problem. gradients carry information used in RNN. When the gradients are very small, that is, when they begin to approach 0, the parameter update becomes insignificant and there is a loss of information between the layers. <\/p>\n\n<p style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > In short, this information loss problem is called Vanishing Gradient Problem. Let's understand better what we are telling by visualizing it. <\/p>\n\n![VGP.png](attachment:82fee756-8d46-4db7-bee6-82e327f939ce.png)\n\n<ul>\n  <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > you see a simple RNN model above. <\/li>\n  <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > Think of black as information, and imagine that the information is lost as the color approaches white. <\/li>\n    <li style = \"color:darkgreen;font-family:Segoe Print;font-weight:bold\" > At first, our color is black, and as the number of layers increases, the color starts to approach white, so the information begins to be lost.There is no problem for the first layers, but this problem starts to occur as the number of layers increases. <\/li>\n<\/ul>\n","ca12002c":"<a id ='8' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\" > Load and Check Data \ud83d\uddf8<\/h2>","74df1b65":"<a id ='20' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\" > Epochs and Batch Size \u2753<\/h3> \n\n<ul>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Epochs : the forward and backward processing of data one by one. In other words, we can simply call it a complete educational tour. Education is determined by the number of eras. <\/li>\n        <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Batch Size : Basically, in the periods I mentioned above, it takes a long time for us to train the data one by one. During training\nhow much of the data,indicates that it will be trained. <\/li>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >To give an example of this, imagine we have 10 loaves of bread. If we select batch_size as 2, imagine that we eat these breads 2 times and 2 times.<\/li>  \n<\/ul>","acef6c82":"![LSTM.gif](attachment:dd49c4d3-3847-4bdb-aa93-a7b4cbcbff0d.gif)\n\n<center><h1 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Introduction \ud83d\udcd6 <\/h1><\/center>\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\">The data (last updated 11\/10\/2017) is presented in CSV format as follows: Date, Open, High, Low, Close, Volume, OpenInt.<\/p>\n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > What are we going to do in this notebook? <\/p>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\">We talked about RNN in the previous notebook. In this notebook, like RNN, what is LSTM first, what are the problems that cause us to use LSTM? We will look at issues like.Here, we will try to understand the logic of the work without getting into the mathematics. We will support this with visualizations. We will compare it with the previous RNN model we made. <\/p>\n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > So, let's get started. <\/p>\n    \n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Content :<\/h2>\n\n<ul>\n    <li style = \"color:gray;font-size:16px\"> <a href = \"#1\" style = \"color:black;font-family:Segoe Print;font-weight:bold\"> Vanishing Gradient Problem \u2753 <\/a> <\/li> \n        <li style = \"color:gray;font-size:16px\"> <a href = \"#2\" style = \"color:black;font-family:Segoe Print;font-weight:bold\"> Exploding Gradient Problem \u2753 <\/a> <\/li> \n        <li style = \"color:gray;font-size:16px\"> <a href = \"#3\" style = \"color:black;font-family:Segoe Print;font-weight:bold\"> Long Short-Term Memory Networks (LSTM) \ud83d\udcda <\/a> \n            <ul>\n      <li style = \"color:gray;font-size:16px\" ><a href = \"#4\" style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Long LSTM works in 3 important steps \u2753 <\/a> <ul>\n     <li style = \"color:gray;font-size:16px\" ><a href = \"#5\" style = \"color:black;font-family:Segoe Print;font-weight:bold\" > 1. Forget Gate <\/a><\/li>\n       <li style = \"color:gray;font-size:16px\" ><a href = \"#6\" style = \"color:black;font-family:Segoe Print;font-weight:bold\" > 2. Input Gate <\/a><\/li>\n         <li style = \"color:gray;font-size:16px\" ><a href = \"#7\" style = \"color:black;font-family:Segoe Print;font-weight:bold\" > 3. Output Gate <\/a><\/ul><\/li><\/ul><\/li>\n        <li style = \"color:gray;font-size:16px\"> <a href = \"#8\" style = \"color:black;font-family:Segoe Print;font-weight:bold\"> Load and Check Data \ud83d\uddf8 <\/a> <\/li> \n    <li  style = \"color:gray;font-size:16px\" > <a href = \"#9\" style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Data Preprocessing \u2754 <\/a> \n        <ul>\n            <li style = \"color:gray;font-size:16px\" ><a href = \"#10\" style = \"color:black;font-family:Segoe Print;font-weight:bold\" >  Split the data as train and test \u2753 <\/a> <\/li>\n            <li style = \"color:gray;font-size:16px\" ><a href = \"#11\" style = \"color:black;font-family:Segoe Print;font-weight:bold\" >  Normalize Data \u2753 <\/a> \n                <ul>\n                    <li style = \"color:gray;font-size:16px\" ><a href = \"#12\" style = \"color:darkblue;font-family:Segoe Print;font-weight:bold\" >   Why do we normalize data? <\/a><\/li>\n                <\/ul>\n            <\/li>\n            <li style = \"color:gray;font-size:16px\" ><a href = \"#13\" style = \"color:black;font-family:Segoe Print;font-weight:bold\" >  X_train - y_train \u2753 <\/a> \n                <ul>\n                    <li style = \"color:gray;font-size:16px\" ><a href = \"#14\" style = \"color:darkblue;font-family:Segoe Print;font-weight:bold\" >   What is the steps logic? <\/a><\/li>\n                <\/ul>\n            <\/li>\n            <li style = \"color:gray;font-size:16px\" ><a href = \"#15\" style = \"color:black;font-family:Segoe Print;font-weight:bold\" >  Reshape \u2753 <\/a> \n                <ul>\n                    <li style = \"color:gray;font-size:16px\" ><a href = \"#16\" style = \"color:darkblue;font-family:Segoe Print;font-weight:bold\" >   why do we reshape ? <\/a><\/li>\n                <\/ul>\n            <\/li>\n        <\/ul>\n    <\/li>\n            <li style = \"color:gray;font-size:16px\" ><a href = \"#17\" style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Implementing with Keras \u2754 <\/a> <ul>\n          <li style = \"color:gray;font-size:16px\" ><a href = \"#18\" style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Create Model \u2753 <\/a><\/li>\n                                        <li style = \"color:gray;font-size:16px\" ><a href = \"#19\" style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Compile Model \u2753 <\/a><\/li>\n                                        <li style = \"color:gray;font-size:16px\" ><a href = \"#20\" style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Epochs and Batch Size \u2753 <\/a><\/li>\n                                        <li style = \"color:gray;font-size:16px\" ><a href = \"#21\" style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Fit the model \u2753 <\/a><\/li>\n                                       <\/ul><\/li>\n                          <li style = \"color:black;font-size:16px\" ><a href = \"#22\" style = \"color:black;font-family:Segoe Print;font-weight:bold\"> Predict \u2753 <\/a><\/li>\n                             <li style = \"color:black;font-size:16px\" ><a href = \"#23\" style = \"color:black;font-family:Segoe Print;font-weight:bold\"> Evaluate the model \u2754 <\/a><\/li>\n       <li style = \"color:black;font-size:16px\" ><a href = \"#24\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> CONCLUSION <\/a><\/li>\n       <\/ul>\n    <\/li>\n<\/ul>\n","732709bc":"<a id ='11' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\" > Normalize data \u2753<\/h3>\n\n<ul>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >In this section, we will normalize the data we have.<\/p> <\/li>\n<\/ul>\n\n<a id ='12' ><\/a>\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > Why do we normalize data? <\/p>\n\n<ul>\n        <li style = \"color:darkblue;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Normalization is very important in all deep learning in general. <\/p> <\/li>\n            <li style = \"color:darkblue;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Normalization makes the properties more consistent. This allows the model to predict its output more accurately.<\/p> <\/li>\n                <li style = \"color:darkblue;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >In this notebook, since we use the \"open\" feature from our data to train the model, it will be sufficient to normalize it.<\/p> <\/li>\n<\/ul>","9d138207":"<a id ='4' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Long LSTM works in 3 important steps \u2753<\/h3>\n\n<ol>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Forget Gate <\/p>  <\/li>\n        <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Input Gate <\/p>  <\/li>\n        <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > <p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Output Gate <\/p>  <\/li>\n<\/ol>","612835ae":"<a id ='5' ><\/a>\n<h4 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> 1 - Forget Gate \u2753<\/h4>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > It takes xt and ht -1 as inputs. Decides whether the incoming information will be forgotten or not.it decides what information should be extracted in a given time frame. Sigmoid determines the operation here. It takes the inputs we specified and calculates the function. <\/p> \n\n![Step1.png](attachment:fc58c7dd-9a8c-4734-ad2e-52897ea92e02.png)\n"}}