{"cell_type":{"96a21e02":"code","19f82c8d":"code","69c2beff":"code","738ec8c2":"code","4eb882b9":"code","d8021698":"code","1705118f":"code","499d149a":"code","f0814729":"code","3e050c4e":"code","9f638d55":"code","22e0f728":"code","77a1a6cd":"code","cfbaff04":"code","dc1599e7":"code","ba2e28b3":"code","39aa3d35":"code","3fbb3392":"code","9d1f19b3":"code","3387aaf5":"code","d3f6f0fe":"code","df450305":"code","d98dd95d":"code","fb09bbda":"code","db315ca6":"code","dad57d51":"code","9eb6c166":"code","89712563":"code","3efd1cee":"code","83814649":"code","1ae986be":"code","929ba0d7":"code","ba6bc9b2":"code","f3c2a843":"code","b906b086":"code","60f5d878":"code","208da19f":"code","091cefa5":"code","0e25ef55":"code","ec7120f3":"code","98c55bb3":"code","3a59d495":"code","683bd68b":"code","131076fd":"code","fd186dfb":"code","1d22f615":"code","f9d0413e":"code","41be9443":"code","6617627c":"code","1207dfd5":"code","6ef212aa":"code","de0cda12":"code","4d831a4e":"code","8b610b76":"code","effe6c30":"code","75acae8b":"code","e489d016":"code","fdd176b8":"code","72161afc":"code","5dc55dce":"code","d82d133f":"code","66e5b9f0":"code","545b9843":"code","23cff5a6":"code","0fc07e6b":"code","17f4e729":"code","116eaec0":"code","c32a71c0":"code","9c24277c":"code","3a035f9c":"code","2239df25":"code","4d5bde80":"markdown","b17c7f6b":"markdown","1e1fd8d5":"markdown","6b31e6fc":"markdown","17f6e805":"markdown","c2ad093b":"markdown","4cde504e":"markdown","e2f16574":"markdown","e3665bb2":"markdown","901c91ec":"markdown","27bd0cf2":"markdown","e7d95b9d":"markdown","2e3a4f2e":"markdown","c9caba99":"markdown","be6e6044":"markdown","6ae43707":"markdown","3896aaf0":"markdown","ac7529f1":"markdown","105cdd34":"markdown","c8cab843":"markdown","2b5fa965":"markdown","76da6b19":"markdown","b41f6c86":"markdown","043835fa":"markdown","8950ce6a":"markdown","9ef9697e":"markdown","b1e31be9":"markdown","c85e0e1e":"markdown","ff1f80de":"markdown","7c9bc4b6":"markdown"},"source":{"96a21e02":"# For Linear Algebra and Neumerical analysis\nimport numpy as np \n# For Data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd \n\n# For ploting our data and results \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# For creating the neural network\nimport torch\nfrom torch import nn, optim\nimport torch.functional as F\n\n# For spliting the data into training and dev sets\nfrom sklearn.model_selection import train_test_split \n\n# For randomly shuffling the data\nfrom sklearn.utils import shuffle\n\nimport os\n\n# Our avialabe data files\nprint(os.listdir(\"..\/input\"))\n","19f82c8d":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest  = pd.read_csv(\"..\/input\/test.csv\")","69c2beff":"train.head()","738ec8c2":"test.head()","4eb882b9":"print(\"Train info :\")\ntrain.info()","d8021698":"print(\"Test info :\")\ntest.info()","1705118f":"train.describe()","499d149a":"test.describe()","f0814729":"train.select_dtypes(include=[np.number]).head()","3e050c4e":"train.select_dtypes(include=[np.object]).head()","9f638d55":"train['Sex'] = train['Sex'].map({'male':1, 'female':0})\ntest['Sex']  = test[ 'Sex'].map({'male':1, 'female':0})\ntrain.head()","22e0f728":"train.Ticket.head()","77a1a6cd":"def sp(st):\n    li = st.split(' ')\n    if len(li) > 1:\n        return li\n    word = 'NA' # Not Available\n    return word, li[0]","cfbaff04":"train['Class'], train['TicketNo'] = zip(*train['Ticket'].map(sp)) \ntest['Class'], test['TicketNo'] = zip(*test['Ticket'].map(sp))","dc1599e7":"train.head()","ba2e28b3":"train = train.drop('Ticket', 1)\ntest  = test.drop('Ticket',  1)\ntrain.head()","39aa3d35":"train['Class'] = train['Class'].map(dict(zip(train.Class.unique(), range(len(train.Class.unique())))))\ntest['Class'] = test['Class'].map(dict(zip(test.Class.unique(), range(len(test.Class.unique())))))\ntrain.head()","3fbb3392":"train['Embarked'] = train['Embarked'].map({'C':1,'Q':2,'S':3})\ntest['Embarked']  = test[ 'Embarked'].map({'C':1,'Q':2,'S':3})\ntrain.head()","9d1f19b3":"def nan_count(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data","3387aaf5":"nan_count(train)","d3f6f0fe":"nan_count(test)","df450305":"train = train.drop('Cabin', 1)\ntest  = test.drop('Cabin', 1)","d98dd95d":"train = train.drop('Name', 1)\ntest  = test.drop('Name', 1)","fb09bbda":"train = train.drop('PassengerId', 1)\ntest  = test.drop('PassengerId', 1)","db315ca6":"train.head()","dad57d51":"train[\"Age\"] = train.Age.fillna(train.Age.mean()) \ntrain['Embarked'] = train.Embarked.fillna(train.Embarked.mode()) \n\ntest[\"Age\"] = test.Age.fillna(test.Age.mean()) \ntest['Embarked'] = test.Embarked.fillna(test.Embarked.mode())\ntest[\"Fare\"] = test.Fare.fillna(test.Fare.mean())","9eb6c166":"nan_count(train)","89712563":"nan_count(test)","3efd1cee":"train.TicketNo[train.TicketNo == 'Basle'] = 12\ntrain.TicketNo[train.TicketNo == 'LINE'] = 0","83814649":"train.TicketNo = train.TicketNo.astype(float)","1ae986be":"test.TicketNo = test.TicketNo.astype(float)","929ba0d7":"train.dtypes","ba6bc9b2":"test.dtypes","f3c2a843":"corrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","b906b086":"abs(corrmat[\"Survived\"][1:]).plot(kind='bar',stacked=True, figsize=(10,5))","60f5d878":"# Let's see the survival rate based on Sex\ntrain[['Sex','Survived']].groupby(['Sex'],as_index = False).mean().sort_values(by = 'Survived',ascending = False)","208da19f":"def barChart(Feature):\n   survived = train[train['Survived']==1][Feature].value_counts()\n   dead = train[train['Survived']==0][Feature].value_counts()\n   df = pd.DataFrame([survived,dead])\n   df.index = ['Survived','Dead']\n   df.plot(kind='bar',stacked=True, figsize=(10,5))","091cefa5":"# Let's Draw the Bar Chart to see the survival based on Sex\nbarChart('Sex')","0e25ef55":"# Let's see the survival rate based on Pclass\ntrain[['Pclass','Survived']].groupby(['Pclass'],as_index = False).mean().sort_values(by = 'Survived',ascending = False)","ec7120f3":"# Let's Draw the Bar Chart to see the survival based on Pclass\nbarChart('Pclass')","98c55bb3":"# Let's see the survival rate based on Embarked\ntrain[['Embarked','Survived']].groupby(['Embarked'],as_index = False).mean().sort_values(by = 'Survived',ascending = False)","3a59d495":"# Let's Draw the Bar Chart to see the survival based on Embarked\nbarChart('Embarked')","683bd68b":"train.head()","131076fd":"train = shuffle(train)\ntrain.head()","fd186dfb":"target = np.array(train.Survived).reshape(len(train), 1)\nfeat = np.array(train.drop('Survived', 1))[:, :-2]\nfeat.shape, target.shape","1d22f615":"\nfeat_train, feat_test, target_train, target_test = train_test_split(feat, target, test_size = 0.07)\ntarget_train.shape, target_test.shape","f9d0413e":"feat_train = torch.from_numpy(feat_train).float().detach().requires_grad_(True)\ntarget_train = torch.from_numpy(target_train).float().detach().requires_grad_(False)\n\nfeat_test = torch.from_numpy(feat_test).float().detach().requires_grad_(True)\ntarget_test = torch.from_numpy(target_test).float().detach().requires_grad_(False)\ntarget_train.shape, target_test.shape","41be9443":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(7, 128)\n        self.a1  = nn.ReLU()\n#         self.fc2 = nn.Linear(128, 12)\n#         self.a2  = nn.ReLU()\n        self.output = nn.Linear(128, 1)\n        self.aO = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.a1(self.fc1(x))\n#         x = self.a2(self.fc2(x))\n        x = self.aO(self.output(x))\n        \n        return x\n\nmodel = Net()\nmodel","6617627c":"opti = optim.Adam(model.parameters(), lr=0.03)\ncriterion = nn.BCELoss()","1207dfd5":"train_loss = []\ntest_loss  = []\n\ntrain_acc = []\ntest_acc  = []","6ef212aa":"D = 200\nfor epoch in range(4000):\n    opti.zero_grad()\n    pred = model(feat_train)\n    \n    loss = criterion(pred, target_train)\n    \n    loss.backward()\n    opti.step()\n    \n    if not (epoch%D):\n        train_loss.append(loss.item())\n        \n        pred = (pred > 0.5).float()\n        acc  = pred == target_train\n        train_acc.append(acc.sum().float()\/len(acc))\n        \n    # Calculating the validation Loss\n    with torch.no_grad():\n        model.eval()\n        pred = model(feat_test)\n        tloss = criterion(pred, target_test)\n        if not (epoch%D):\n            test_loss.append(tloss.item())\n            \n            pred = (pred > 0.5).float()\n            acc  = pred == target_test\n            test_acc.append(acc.sum().float()\/len(acc))\n            print(F\"{epoch:5d}  |  train accuracy: {train_acc[-1]:0.4f}  |  test accuracy: {test_acc[-1]:0.4f}  |  train loss: {train_loss[-1]:0.4f}  |  test loss: {test_loss[-1]:0.4f}\")\n    model.train()\n            \nprint(\"DONE!\")","de0cda12":"plt.plot(train_loss, label='Training loss')\nplt.plot(test_loss, label='Validation loss')\nplt.legend(frameon=False)","4d831a4e":"plt.plot(train_acc, label='Training accuracy')\nplt.plot(test_acc,  label='Validation accuracy')\nplt.legend(frameon=False)","8b610b76":"# Saving the model\ntorch.save(model.state_dict(), 'checkpoint.pth')","effe6c30":"# Loading the model\nstate_dict = torch.load('checkpoint.pth')\n# print(state_dict)\nmodel.load_state_dict(state_dict)","75acae8b":"test_acc[-5:]","e489d016":"train_acc[-5:]","fdd176b8":"test_loss[-5:]","72161afc":"train_loss[-5:]","5dc55dce":"pid = test.PassengerId\ntest = test.drop('PassengerId', 1)","d82d133f":"test.head()","66e5b9f0":"test = np.array(test)[:, :-2]","545b9843":"test_tensor = torch.from_numpy(test).float().detach().requires_grad_(True)","23cff5a6":"sol = model(test_tensor)","0fc07e6b":"sol[:10]","17f4e729":"sol = sol > 0.5","116eaec0":"sub = pd.read_csv(\"..\/input\/gender_submission.csv\")","c32a71c0":"sub.head()","9c24277c":"sub['Survived'] = sol.detach().numpy()","3a035f9c":"sub.head()","2239df25":"sub.to_csv(\"Sollution.csv\", index=False)","4d5bde80":"<br>Let's map the embarked column or feature. There are three value in the embarked feature. <\/br>\n<br>C : Cherbourg, Q : Queenstown, S : Southampton <\/br>\n<br>Let's map C as 1, Q as 2, S as 3 <\/br>","b17c7f6b":"Let's have a look at our data first","1e1fd8d5":"Also the names will just cause noise so also droping them","6b31e6fc":"A deeper look of the corelation of every column with respect to our target column","17f6e805":"## Exploratory Data Analysis or EDA\nAccording to  Wikipedia,  Exploratory Data Analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. <br>\nAlready we did a little bit of data preprocessing. Here we will describe the data-set using statistical and visualization techniques to bring important aspects of that data into our focus for further analysis. <br>\n\nAuthor : Riday","c2ad093b":"<br>We have six  Numerical   columns: PassengerId, Survived, Age, SibSp, Parch, Fare <\/br>\n<br>We have five categorical columns: Name, Sex, Ticket, Cabin, Embarked <\/br>","4cde504e":"#### Let's map the categorical values to numerical values","e2f16574":"The first part seems like some kind of class and the other part is the ticket number so let's create two seperate columns for them","e3665bb2":"Now let's map the values of class","901c91ec":"**Now we fill the missing values in Age, Embarked and Fare**\n<br>Age with Mean since it's numerical value <\/br>\n<br>Embarked with Mode since it's a catogorical value <\/br>\n<br>Fare with Mean since it's numerical value <\/br>","27bd0cf2":"Male to 1 and Female to 0","e7d95b9d":"## Introduction : A short history of Titanic\n\nRMS Titanic was a passenger liner that struck an iceberg on her maiden voyage from Southampton, England, to New York City, and sank on 15th April 1912, resulting in the deaths of 1,517 people in one of the deadliest peacetime maritime disasters in history.Titanic True Story\n\nThe largest passenger steamship in the world at the time, the Olympic-class RMS Titanic was owned by the White Star Line and constructed at the Harland and Wolff shipyard in Belfast, Ireland, UK.\n\nAfter setting sail for New York City on 10th April 1912 with 2,223 people on board, she hit an iceberg four days into the crossing, at 11:40 pm on 14th April 1912, and sank at 2:20 am on the morning of 15th April.\n\nThe high casualty rate resulting from the sinking was due in part to the fact that, although complying with the regulations of the time, the ship carried lifeboats for only 1,178 people. A disproportionate number of men died due to the 'women and children first' protocol that was enforced by the ship's crew.\n(credit : funny-jokes.com)\n\nIn this challenge, we will try to predict what sorts of people were likely to survive. Here, we will use Deep Neural Network using PyTorch to predict which passengers survived the tragedy.\n","2e3a4f2e":"Ploting the heat map of the corelation of our data","c9caba99":"Since more than 70% data of Cabin is NaN in both train and test, droping it is the best idea","be6e6044":"Looks good and now we don't need the ticket anymore","6ae43707":"## Data Cleanning\/Preprocessing\n\n","3896aaf0":"The numerical columns present in our training set","ac7529f1":"Let's look at the result","105cdd34":"The ticket column contains two values","c8cab843":"Loading the Training and Testing files","2b5fa965":"A function to check for the NaN(not a number) values","76da6b19":"Our function is ready now let's split the columns into two parts","b41f6c86":"Checking the information about our testing data","043835fa":"# Titanic Machine Learning from Disaster\n\n1.   Introduction\n2.   Data Loading\n3.   Data Cleaning\/ Preprocessing\n4.   Exploratory Data Analysis or EDA\n5.   Feature extraction\n6.   Model Building\n","8950ce6a":"Checking the information about our training data","9ef9697e":"The catogorical columns present in our training set","b1e31be9":"Let's describe the test data","c85e0e1e":"## Loading the data set with necessary libraries","ff1f80de":"A bug that will be fixed later","7c9bc4b6":"Let's decribe the Training set"}}