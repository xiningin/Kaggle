{"cell_type":{"c29a824c":"code","8b6aef1e":"code","153f5ed2":"code","3f37d248":"code","1eadfb2c":"code","6b6d57bd":"code","4032624a":"code","c41702e4":"code","e2d7a4c2":"code","72816883":"code","ba758abf":"code","1de5b6ef":"code","08589940":"code","4fccf95e":"code","3fce309a":"code","0cfde7d7":"code","4892ff10":"code","ef31a222":"code","e2cf623c":"code","7b59b3ae":"code","2b8dcef9":"code","1252aa66":"code","d26b1a2f":"code","db4ff84a":"code","196d27d0":"code","3003d195":"code","0ce2feb5":"code","235dc753":"code","5f68ca5e":"code","326004e4":"code","02382353":"code","5a068be9":"code","1e4ba847":"code","20f35411":"code","72b0c68d":"code","b0fb5576":"code","87c12354":"code","72469644":"code","fd1f004a":"code","1e0af25c":"code","68af4273":"code","6c846cd8":"code","97f2978e":"code","baa99e48":"code","a6e89b72":"code","cb24af85":"code","8e4fcab0":"code","4aaa235f":"code","417d86e7":"code","1e5a7cd7":"code","3a068692":"code","f3b7b336":"code","171886d3":"code","ec44b8ce":"code","57ae9f19":"code","b95c36d6":"code","2f1af801":"code","ee131f44":"code","016160ce":"code","19353105":"code","ea1d1e71":"code","b9630cc2":"code","6251ef7f":"code","3a700a07":"code","25b5ed1e":"code","461cac1b":"markdown","270762a7":"markdown","a0fd4e12":"markdown","e014ddfc":"markdown","b423182e":"markdown","f679c14c":"markdown","1dd5eb6e":"markdown"},"source":{"c29a824c":"import pandas as pd\nfrom sklearn import preprocessing","8b6aef1e":"holidays_events = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/holidays_events.csv')\noil = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/oil.csv', index_col='date')\nstores = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/stores.csv')\ntest = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/test.csv', index_col='id')\ntrain = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/train.csv', index_col='id')\ntransactions = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/transactions.csv')","153f5ed2":"holidays_events.head()","3f37d248":"#holidays_count_date = pd.DataFrame(holidays_events.date.value_counts(), dtype='int8')","1eadfb2c":"#holidays_count_date = holidays_count_date.reset_index()\n#holidays_count_date.rename(columns = {'index': 'date', 'date': 'count'}, inplace = True)\n#holidays_count_date","6b6d57bd":"train.head()","4032624a":"len(train)","c41702e4":"test.head()","e2d7a4c2":"train1 = train.copy()\ntest1 = test.copy()","72816883":"train1.head()","ba758abf":"train1 = pd.merge(train1, oil, how=\"left\", on=\"date\")\ntest1 = pd.merge(test1, oil, how=\"left\", on=\"date\")\nlen(train1)","1de5b6ef":"#train1 = pd.merge(train1, holidays_count_date, how=\"left\", on=\"date\")\n#test1 = pd.merge(test1, holidays_count_date, how=\"left\", on=\"date\")\n#len(train1)","08589940":"train1 = pd.merge(train1, transactions, how=\"left\", on=[\"date\",'store_nbr'])\ntest1 = pd.merge(test1, transactions, how=\"left\", on=[\"date\",'store_nbr'])\nlen(train1)","4fccf95e":"train1.head()","3fce309a":"train1 = pd.merge(train1, stores, how=\"left\", on=[\"store_nbr\"])\ntest1 = pd.merge(test1, stores, how=\"left\", on=[\"store_nbr\"])\ntrain1 = train1.fillna(0)\ntest1 = test1.fillna(0)","0cfde7d7":"len(train1)","4892ff10":"train1.head()","ef31a222":"train1['onpromotion_date_store'] = train1.groupby(['date','store_nbr'])['onpromotion'].transform(\"sum\")   \ntest1['onpromotion_date_store'] = test1.groupby(['date','store_nbr'])['onpromotion'].transform(\"sum\")   ","e2cf623c":"#train1['diff1_onpromotion'] = train1.onpromotion.diff(periods=1684)\n#test1['diff1_onpromotion'] = test1.onpromotion.diff(periods=1684)","7b59b3ae":"#train1['diff2_onpromotion'] = train1.onpromotion.diff(periods=1684*2)\n#test1['diff2_onpromotion'] = test1.onpromotion.diff(periods=1684*2)","2b8dcef9":"train2 = train1.copy()\ntest2 = test1.copy()","1252aa66":"train2.head()","d26b1a2f":"len(train2)","db4ff84a":"def extract_weekday(s):\n    return s.dayofweek\n\ndef extract_monthday(s):\n    return s.day\n\ndef extract_month(s):\n    return s.month\n\ndef extract_year(s):\n    return s.year","196d27d0":"train2['date'] = pd.to_datetime(train2['date'])\ntrain2['weekday'] = train2['date'].apply(extract_weekday)\ntrain2['extract_monthday'] = train2['date'].apply(extract_monthday)\ntrain2['year'] = train2['date'].apply(extract_year)\ntrain2['month'] = train2['date'].apply(extract_month)","3003d195":"train2.year.describe()","0ce2feb5":"train2.head()","235dc753":"test2['date'] = pd.to_datetime(test2['date'])\ntest2['weekday'] = test2['date'].apply(extract_weekday) \ntest2['extract_monthday'] = test2['date'].apply(extract_monthday)\ntest2['year'] = test2['date'].apply(extract_year)\ntest2['month'] = test2['date'].apply(extract_month)","5f68ca5e":"test2.year.describe()","326004e4":"train2.dtypes","02382353":"for col in train2.columns:\n    if train2[col].dtype == 'float64' and col != 'sales':\n        train2[col] = train2[col].astype('float32')\n        test2[col] = test2[col].astype('float32')\n    if train2[col].dtype == 'int64':\n        train2[col] = train2[col].astype('int8')\n        test2[col] = test2[col].astype('int8')\n","5a068be9":"train2.nunique()","1e4ba847":"train2.fillna(0)\ntest2.fillna(0)","20f35411":"cat_cols = [cname for cname in train2.columns if train2[cname].dtype == 'object']\ncat_cols","72b0c68d":"y = pd.DataFrame(train2.sales)\ntrain2 = train2.drop(['sales'], axis=1)","b0fb5576":"enc = preprocessing.LabelEncoder()\nfor col in cat_cols:\n    train2[col] = enc.fit_transform(train2[col].astype(str))\n    test2[col] = enc.fit_transform(test2[col])","87c12354":"test2.head()","72469644":"len(test2.iloc[0])","fd1f004a":"import tensorflow as tf","1e0af25c":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","68af4273":"train2 = train2.drop('date', axis=1)\ntest2 = test2.drop('date', axis=1)","6c846cd8":"#date = list(train2.groupby('date'))\n#1684 timeseries","97f2978e":"import numpy as np\ntrain_data = np.array(train2)\ntest_data = np.array(test2)","baa99e48":"train_data.ndim","a6e89b72":"from sklearn.preprocessing import QuantileTransformer","cb24af85":"qt = QuantileTransformer(n_quantiles=300, output_distribution='uniform')\ntrain_data = qt.fit_transform(train_data)\ntest_data = qt.transform(test_data)","8e4fcab0":"qty = QuantileTransformer(n_quantiles=300, output_distribution='uniform')\ny = qty.fit_transform(y)","4aaa235f":"from sklearn.preprocessing import MinMaxScaler\nx_scaler = MinMaxScaler()\ndataset_train_sc = x_scaler.fit_transform(train_data)\ndataset_test_sc = x_scaler.transform(test_data)","417d86e7":"y_scaler = MinMaxScaler()\ny_sc = y_scaler.fit_transform(y)","1e5a7cd7":"from tensorflow import keras\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\n#from tensorflow.keras.engine.input_layer import Input\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import BatchNormalization","3a068692":"lookback = 1\nbatch_size = 8192","f3b7b336":"def model_builder(lr):\n    \"\"\"\u041c\u043e\u0434\u0435\u043b\u044c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438\"\"\"\n    inputA = keras.Input(shape=(15))\n    line = Reshape((15,1))(inputA)\n    line = Conv1D(filters=256, kernel_size=1, padding='same', activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.1)(line)\n\n    line = MaxPooling1D(pool_size=2)(line)\n    \n    line = Conv1D(filters=512, kernel_size=2, activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.3)(line)\n    \n    #line = MaxPooling1D(pool_size=2)(line)\n    \n    line = Conv1D(filters=1024, kernel_size=2, activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.3)(line)\n\n    #line = MaxPooling1D(pool_size=2)(line)\n    \n    line = Flatten()(line)\n    line = Dense(512, activation='relu')(line)\n    line = Dense(256, activation='relu')(line)\n    line = Dense(128, activation='relu')(line)\n    \n    #line = Dropout(0.3)(line)\n    #line = Dropout(0.3)(line)\n    \n    outputA = Dense(units=1)(line)\n    model = Model(inputs=inputA, outputs=outputA)\n    #model = keras.models.load_model('models\/model2')\n    model.compile(\n        #loss = tf.keras.losses.MeanSquaredLogarithmicError(reduction=\"auto\", name=\"mean_squared_logarithmic_error\"),\n        loss = tf.keras.losses.MeanSquaredError(name='mse'),\n        optimizer = Adam(lr=lr), metrics=['mae'],)\n    return model","171886d3":"lr=0.001\nwith strategy.scope():\n    model = model_builder(lr)\n#with strategy.scope():\n#    model = load_model('..\/input\/ss-tsf\/best_6.5913e-04-170.h5')","ec44b8ce":"model.summary()","57ae9f19":"checkpoint_filepath = 'best.h5'\nsave_model_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    monitor='val_loss',\n    mode='min',\n    verbose=1,\n    save_best_only=True)","b95c36d6":"#model = load_model('..\/input\/tps1021\/best_85221.h5')","2f1af801":"N_split = int(0.1 * len(dataset_train_sc))\ndataset_sc_TRAIN = dataset_train_sc[:-N_split, :]\ndataset_sc_VAL = dataset_train_sc[-N_split:, :]\ny_TRAIN = y_sc[:-N_split]\ny_VAL = y_sc[-N_split:]","ee131f44":"train = (dataset_sc_TRAIN, y_TRAIN)\nval = (dataset_sc_VAL, y_VAL)","016160ce":"from tensorflow.keras.callbacks import ReduceLROnPlateau\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, min_lr=0.0005, verbose=1, mode='min')","19353105":"\n#EPOCHS = 500\n#EPOCHS = 1\n#model.fit(\n#    dataset_sc_TRAIN, y_TRAIN, validation_data=(dataset_sc_VAL, y_VAL), batch_size=batch_size, epochs=EPOCHS, callbacks=[save_model_callback, reduce_lr], shuffle=True)\n","ea1d1e71":"#with strategy.scope():\n#    model = load_model('best.h5')","b9630cc2":"with strategy.scope():\n    model = load_model('..\/input\/ss-tsf\/best_6.3584e-04-162.h5')","6251ef7f":"preds = model.predict(dataset_test_sc)\npreds1 = y_scaler.inverse_transform(preds)\npreds1 = qty.inverse_transform(preds1)","3a700a07":"for i in range(len(preds1)): preds1[i] = 0 if preds1[i] < 0 else preds1[i]","25b5ed1e":"output = pd.DataFrame({'Id': test.index,'sales': preds1[:,0]})\npath = 'sample_submission.csv'\noutput.to_csv(path, index=False)\noutput ","461cac1b":"train = tf.keras.preprocessing.timeseries_dataset_from_array(\n    dataset_sc_TRAIN,\n    y_TRAIN,\n    sequence_length=lookback,\n    sequence_stride=1,\n    sampling_rate=1,\n    batch_size=batch_size,\n    shuffle=False,\n    seed=None,\n    start_index=None,\n    end_index=None,\n)","270762a7":"# Extract date features","a0fd4e12":"def model_builder1(lr):\n    \"\"\"\u041c\u043e\u0434\u0435\u043b\u044c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438\"\"\"\n    inputA = keras.Input(shape=(lookback, 11))\n    line = tf.keras.layers.LSTM(16)(inputA)\n    \n    line = Dense(128, activation='relu')(line)\n    line = Dropout(0.5)(line)\n    #line = Dense(64, activation='relu')(line)\n    outputA = Dense(units=1)(line)\n    model = Model(inputs=inputA, outputs=outputA)\n    #model = keras.models.load_model('models\/model2')\n    model.compile(\n        loss = tf.keras.losses.MeanSquaredLogarithmicError(reduction=\"auto\", name=\"mean_squared_logarithmic_error\"),\n        optimizer = Adam(lr=lr), metrics=['mae'],)\n    return model","e014ddfc":"repeat few times 2 cells above","b423182e":"# deep network model","f679c14c":"# Reduce memory usage","1dd5eb6e":"val = tf.keras.preprocessing.timeseries_dataset_from_array(\n    dataset_sc_VAL,\n    y_VAL,\n    sequence_length=lookback,\n    sequence_stride=1,\n    sampling_rate=1,\n    batch_size=batch_size,\n    shuffle=False,\n    seed=None,\n    start_index=None,\n    end_index=None,\n)"}}