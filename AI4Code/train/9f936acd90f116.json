{"cell_type":{"df63c052":"code","f344cb84":"code","0463d235":"code","cfce16eb":"code","d83278d0":"code","d85c3509":"code","10f5ae66":"code","dce27330":"code","379d9998":"code","ff4bbd7e":"code","731b355a":"code","8d9ee426":"code","b140979c":"code","a5899623":"code","836973bb":"code","f63cd44c":"code","a227c70f":"markdown"},"source":{"df63c052":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport PIL\nimport os\nimport skimage.io\nimport keras.backend as K\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout,BatchNormalization ,Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam","f344cb84":"train_ds = ImageDataGenerator(rescale = 1.\/255,\n                                   validation_split = 0.2,\n                                  \n        rotation_range=5,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        #zoom_range=0.2,\n        horizontal_flip=True,\n        vertical_flip=True,\n        fill_mode='nearest')\n\nval_ds = ImageDataGenerator(rescale = 1.\/255,\n                                  validation_split = 0.2)","0463d235":"train_ds = tf.keras.preprocessing.image_dataset_from_directory('..\/input\/dandelionimages\/Images',\n  validation_split=0.2,\n  subset=\"training\",\n  seed=212,\n  image_size=(224, 224),\n  batch_size= 64)","cfce16eb":"val_ds = tf.keras.preprocessing.image_dataset_from_directory('..\/input\/dandelionimages\/Images',\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=212,\n  image_size=(224, 224),\n  batch_size= 64)","d83278d0":"class_names = train_ds.class_names\nprint(class_names)","d85c3509":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(6):\n        random_num = np.random.randint(0, len(images))\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[random_num].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[random_num]])\n        plt.axis(\"off\")","10f5ae66":"AUTOTUNE = tf.data.experimental.AUTOTUNE\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","dce27330":"base_model = tf.keras.applications.ResNet50(input_shape=(224,224,3),include_top=False,weights=\"imagenet\")","379d9998":"# Freezing Layers\n\nfor layer in base_model.layers[:-5]:\n    layer.trainable=False","ff4bbd7e":"# Building Model\n\nmodel=Sequential()\nmodel.add(base_model)\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dense(2))","731b355a":"# Model Summary\n\nmodel.summary()","8d9ee426":"from tensorflow.keras.utils import plot_model\nfrom IPython.display import Image\nplot_model(model, to_file='convnet.png', show_shapes=True,show_layer_names=True)\nImage(filename='convnet.png') ","b140979c":"lrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 2,verbose = 1,factor = 0.75, min_lr = 1e-9)\n\nmcp = ModelCheckpoint('model.h5')\n\nes = EarlyStopping(verbose=1, patience=2)","a5899623":"model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])","836973bb":"epochs=10\n%time\nhistory=model.fit(train_ds,validation_data=val_ds,epochs = epochs,verbose = 1,callbacks=[lrd,mcp,es])","f63cd44c":"#%% PLOTTING RESULTS (Train vs Validation FOLDER 1)\n\ndef Train_Val_Plot(accuracy,val_accuracy,loss,val_loss):\n    \n    fig, (ax1, ax2) = plt.subplots(1,2, figsize= (10,5))\n    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n\n    ax1.plot(range(1, len(accuracy) + 1), accuracy)\n    ax1.plot(range(1, len(val_accuracy) + 1), val_accuracy)\n    ax1.set_title('History of Accuracy')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Accuracy')\n    ax1.legend(['training', 'validation'])\n\n\n    ax2.plot(range(1, len(loss) + 1), loss)\n    ax2.plot(range(1, len(val_loss) + 1), val_loss)\n    ax2.set_title('History of Loss')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Loss')\n    ax2.legend(['training', 'validation'])\n    \n  \n\n\n    plt.show()\n    \n\nTrain_Val_Plot(history.history['accuracy'],history.history['val_accuracy'],\n               history.history['loss'],history.history['val_loss'],\n\n              )","a227c70f":"## Trained Dandelion images classification models using ResNet50\n\n\n### ResNet50\n\nResNet50 is a variant of ResNet model which has 48 Convolution layers along with 1 MaxPool and 1 Average Pool layer. It has 3.8 x 10^9 Floating points operations. It is a widely used ResNet model and we have explored ResNet50 architecture in depth.\n\n\n[![image.png](attachment:image.png)](http:\/\/)\n\n\n\n\n### Description\n\nThese are images of dandelions and not-dandelions. Basically grass or other. Goal of this project\/images is a very simple binary image classification model for me to do some \"real world learning\": - Dandelions - Anything NOT dandelions\n\n\n1- Images\/dandelions = only images of dandelions.\n\n2-Images\/other = other, mostly grass images.\n\n3-Images\/dandelionvother = kinda' a mistake this dir gets created during training. Should be cleared out on each training.\n\n\n#### Dataset:\n\n[Link](https:\/\/www.kaggle.com\/coloradokb\/dandelionimages)"}}