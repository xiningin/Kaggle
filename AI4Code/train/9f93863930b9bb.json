{"cell_type":{"78105c5e":"code","f75ef365":"code","be1402de":"code","d985178f":"code","c034ee38":"code","2dc8f945":"markdown","856aa8ba":"markdown"},"source":{"78105c5e":"#Required Libraries\nimport numpy as np  \nfrom numpy.random import rand\nimport matplotlib.pyplot as plt","f75ef365":"#dataset\nx = np.array(([0.9,0.8],[0.6,0.3],[0.9,0.1],[0.9,0.8]))  #Features\ny = np.array(([0],[1],[1],[0]))  #Labels (0,1)","be1402de":"# Activation function\ndef sigmoid(z):\n    return 1\/(1+np.exp(-z))\n\n#Multi-layers feedforward neural network (2 hidden layers)\nclass NeuralNetwork:\n    def __init__(self, x, y, nodes_in_layer1 = 4, nodes_in_layer2 = 3, nodes_in_layer3 = 1, l_rate = 1):\n        #define x, y\n        self.inputs_in_layer0 = x  #layer 0\n        self.y = y\n        \n        self.l_rate = l_rate  #learning rate\n        \n        #define and set the number of neurons in each layers\n        self.nodes_in_layer1 = nodes_in_layer1\n        self.nodes_in_layer2 = nodes_in_layer2\n        self.nodes_in_layer3 = nodes_in_layer3\n        \n        #intialize the wieghts (theta) metrices\n        self.thetas_layer0 = np.random.rand(self.inputs_in_layer0.shape[1] + 1,self.nodes_in_layer1)  #shape: [2+1, 4]\n        self.thetas_layer1 = np.random.rand(self.nodes_in_layer1 + 1,self.nodes_in_layer2)  #shape: [4+1, 3]\n        self.thetas_layer2 = np.random.rand(self.nodes_in_layer2+1, nodes_in_layer3)  # shape: [3+1, 1]\n\n    def feedforward(self):      \n        #compute all the nodes (a1, a2, a3, a4) in layer1\n        n = self.inputs_in_layer0.shape[0]\n\n        self.Z1 = self.thetas_layer0[0] + np.dot(self.inputs_in_layer0, self.thetas_layer0[1:])\n        self.layer1 = sigmoid(self.Z1)  #values of a1, a2, a3, a4 in layer 1\n        \n        #compute all the nodes (a1, a2, a3) in layer2\n        self.Z2 = self.thetas_layer1[0] + np.dot(self.layer1, self.thetas_layer1[1:])\n        self.layer2 = sigmoid(self.Z2)  #values of a1, a2, a3 in layer 2\n        \n        #compute all the nodes (a1) in layer3\n        self.Z3 = self.thetas_layer2[0] + np.dot(self.layer2, self.thetas_layer2[1:])\n        self.layer3 = sigmoid(self.Z3)  #output layer      \n        \n        return self.layer3\n    \n    def cost_func(self):\n        \n        self.n = self.inputs_in_layer0.shape[0] #number of training examples\n        self.cost = (1\/self.n) * np.sum(-self.y * np.log(self.layer3) - (1 - self.y) * np.log(1 - self.layer3)) #cross entropy\n        return self.cost \n\n    \n    def backprop(self):\n        \n        #dervative of E with respect to theta and bias in layer2\n        self.dE_dlayer3 = (1\/self.n) * (self.layer3-y)\/(self.layer3*(1-self.layer3))\n        self.dE_dZ3 = np.multiply(self.dE_dlayer3, (sigmoid(self.Z3)* (1-sigmoid(self.Z3))))\n        self.dE_dtheta2 = np.dot(self.layer2.T, self.dE_dZ3)\n        self.dE_dbias2 = np.dot(np.ones(self.n), self.dE_dZ3)\n        \n        #dervative of E with respect to theta and bias in layer1\n        self.dE_dlayer2 = np.dot(self.dE_dZ3, self.thetas_layer2[1:].T)\n        self.dE_dZ2 = np.multiply(self.dE_dlayer2, sigmoid(self.Z2)* (1-sigmoid(self.Z2)))\n        self.dE_dtheta1 = np.dot(self.layer1.T, self.dE_dZ2)\n        self.dE_dbias1 = np.dot(np.ones(self.n), self.dE_dZ2)\n        \n        #dervative of E with respect to theta and bias in layer0\n        self.dE_dlayer1 = np.dot(self.dE_dZ2, self.thetas_layer1[1:].T)\n        self.dE_dZ1 = np.multiply(self.dE_dlayer1, sigmoid(self.Z1)* (1-sigmoid(self.Z1)))\n        self.dE_dtheta0 = np.dot(self.inputs_in_layer0.T, self.dE_dZ1)\n        self.dE_dbias0 = np.dot(np.ones(self.n), self.dE_dZ1)\n        \n        #updating theta using gradient descent in layers 2, 1, and 0\n        self.thetas_layer2[1:] = self.thetas_layer2[1:] - self.l_rate * self.dE_dtheta2\n        self.thetas_layer1[1:] = self.thetas_layer1[1:] - self.l_rate * self.dE_dtheta1\n        self.thetas_layer0[1:] = self.thetas_layer0[1:] - self.l_rate * self.dE_dtheta0\n        \n        #updating bias using gradient descent in layers 2, 1, and 0\n        self.thetas_layer2[0] = self.thetas_layer2[0] - self.l_rate * self.dE_dbias2\n        self.thetas_layer1[0] = self.thetas_layer1[0] - self.l_rate * self.dE_dbias1\n        self.thetas_layer0[0] = self.thetas_layer0[0] - self.l_rate * self.dE_dbias0\n        \n        \n        return self\n","d985178f":"NN = NeuralNetwork(x,y)\nepochs = 1000\nlosses = []\nfor i in range(epochs):\n    predicted_output = NN.feedforward()\n    error = NN.cost_func()\n    losses.append(error)\n    NN.backprop()\n    print (\"iteration # \", i+1)\n    print (\"Actual Output: \\n\", y)\n    print(\"Predicted Output: \\n\", predicted_output, \"\\n\")\n    print (\"Cost: \\n\" , error, \"\\n\")\n","c034ee38":"plt.scatter(range(epochs), losses)\nplt.title('Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","2dc8f945":"dervative of E with respect to theta and bias in layer1\n![bacprop1.png](attachment:bacprop1.png)\ndervative of E with respect to theta and bias in layer1\n![bacprop2.png](attachment:bacprop2.png)\ndervative of E with respect to theta and bias in layer0\n![bacprop3.png](attachment:bacprop3.png)","856aa8ba":"![NN.png](attachment:NN.png)\n\n![BB.png](attachment:BB.png)"}}