{"cell_type":{"b1dc7df1":"code","e231a1a2":"code","10160b0b":"code","0d80017f":"code","89fde42f":"code","a852c8fd":"code","e820f0ae":"code","e22b6367":"code","52a6d877":"code","682db12b":"code","5c5cd1d5":"markdown","efab7ddc":"markdown","cff219b2":"markdown","fc71d312":"markdown","5b0687c6":"markdown","5d6f8ea6":"markdown","eae169ca":"markdown"},"source":{"b1dc7df1":"%matplotlib inline\n\nimport os\n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom keras.preprocessing.image  import ImageDataGenerator\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\nfrom matplotlib import pyplot as plt\n\n\nsns.set()","e231a1a2":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.columns","10160b0b":"ytrain = to_categorical(train.values[:, 0])\nxtrain = train.values[:, 1:]\nxtest = test.values\n\nxtrain = xtrain.reshape((42000, 28, 28, 1)) \/ 255\nxtest = xtest.reshape((28000, 28, 28, 1)) \/ 255\n\nprint('xtrain.shape:', xtrain.shape)\nprint('xtrain.max():', xtrain.max())\nprint('xtrain.min():', xtrain.min())\nprint('xtest.shape:', xtest.shape)","0d80017f":"data_generator = ImageDataGenerator(rotation_range=10, zoom_range=0.10, width_shift_range=0.1, height_shift_range=0.1)\nxtrain_news, ytrain_news = [], []\n\nfor _ in range(2):\n    for i, sample in enumerate(xtrain):\n        xtrain_new, ytrain_new = data_generator.flow(sample.reshape((1, 28, 28, 1)), ytrain[i].reshape((1, 10))).next()\n        xtrain_news.append(xtrain_new.reshape((28, 28, 1)))\n        ytrain_news.append(ytrain_new[0])\n        \n    \nxtrain = np.concatenate([xtrain, np.array(xtrain_news)])\nytrain = np.concatenate([ytrain, np.array(ytrain_news)])\ndel xtrain_news, ytrain_news\n\nxselftest = xtrain[:10000]\nyselftest = ytrain[:10000]\nxtrain = xtrain[10000:]\nytrain = ytrain[10000:]\n\nprint('Number of training samples:', len(xtrain))","89fde42f":"model = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, kernel_size=5, activation='relu', strides=2, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(.4))\n\nmodel.add(Conv2D(64, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=5, activation='relu', strides=2, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(.4))\n\nmodel.add(Conv2D(128, kernel_size=4, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dropout(.4))\n\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])","a852c8fd":"xtra, ytra = xtrain[1000:], ytrain[1000:]\nxval, yval = xtrain[:1000], ytrain[:1000]\nprint('xtra.shape:', xtra.shape)\nprint('xval.shape:', xval.shape)","e820f0ae":"N_EPS = 20\nh = model.fit(x=xtrain, y=ytrain, epochs=N_EPS, batch_size=64, validation_data=(xval, yval,), verbose=0)","e22b6367":"plt.plot(range(N_EPS), h.history['loss'], marker='x', label='Loss');\nplt.plot(range(N_EPS), h.history['val_loss'], color='green', label='Validation loss');\nplt.title('Loss');\nplt.legend();\n\nprint('Testing on', len(xselftest), 'samples')\nprint('Accuracy:', model.evaluate(xselftest, yselftest)[1] * 100)","52a6d877":"ytestpred = model.predict(xtest).argmax(axis=1)\n\ndf = pd.read_csv('..\/input\/sample_submission.csv')\ndf['Label'] = ytestpred\ndf.head()","682db12b":"df.to_csv('submission.csv', index=False)\npd.read_csv('submission.csv').head()","5c5cd1d5":"## Loading the dataset","efab7ddc":"## Making submission","cff219b2":"## Preprocessing","fc71d312":"## Generating more images to train on","5b0687c6":"## Inspecting performance","5d6f8ea6":"## Building the model","eae169ca":"Since I doesn't have test labels I make a validation set from my training data to make sure the model is not going to overfit."}}