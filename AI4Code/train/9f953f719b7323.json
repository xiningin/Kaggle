{"cell_type":{"7cf4e0ea":"code","03b58b55":"code","b67295bf":"code","8b994d02":"code","5c4274de":"code","47da0857":"code","e04b461c":"code","659a2229":"code","7772d8b5":"code","af28ee58":"code","50c51914":"code","693c7ca2":"code","ec6c762e":"code","b13a46b0":"code","066facb6":"code","ef13134d":"code","ba96e2c9":"code","3a104074":"code","e1c63286":"code","84a237e2":"code","21de399f":"code","159708eb":"code","6c90e7e8":"code","0ac9ea6a":"markdown","55295426":"markdown","9e41f043":"markdown","612c203b":"markdown","0882f19d":"markdown","16666b51":"markdown","34dc121e":"markdown","9d5597f1":"markdown","ce531e9a":"markdown","3cf97409":"markdown","f89f7fb6":"markdown","fb0dffeb":"markdown","cbaa91e7":"markdown","0ddf5032":"markdown","0cc87a7c":"markdown","3a6a8177":"markdown","17c74b31":"markdown","bd04c71d":"markdown"},"source":{"7cf4e0ea":"# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\nfrom sklearn import tree, linear_model\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, plot_confusion_matrix\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom scipy.stats import mode\nfrom mlxtend.plotting import plot_decision_regions\nfrom mlxtend.plotting import plot_learning_curves","03b58b55":"# Import data\ndf = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')\n\n# View all variables and its unique features\ncol_list = df.columns.values.tolist()\nfor col_name in col_list:\n    print(col_name, ' : ', df[col_name].unique())","b67295bf":"# View some details for each variable\ndf.info()","8b994d02":"# Set the values for X (independent variable)\nX = df.drop(['class'], axis=1)\n# Set the values for y (dependent variable)\ny = df['class']\n\n[X.info(), y.shape]","5c4274de":"# Set a default colour for all plots below\nsns.set_palette(['steelblue','palevioletred', '#ffcc99' , 'mediumaquamarine'])","47da0857":"# Set the figure's size\nplt.figure(figsize=(6,6))\n# Create a Pie Chart for gender\nplt.pie(x = df['class'].value_counts(), explode = [0, 0.05], autopct='%0.01f%%', labels = [ 'Edible', 'Poisonous'])\nplt.show()","e04b461c":"ohe = OneHotEncoder()\nX = ohe.fit_transform(X)\nprint (X.shape)\n\nle = LabelEncoder()\ny = le.fit_transform(y)\nvalues, counts = np.unique(y, return_counts=True)\nprint('Shape of y : ', y.shape)\nprint(values, counts)","659a2229":"# Standardizing the variables\nX = StandardScaler().fit_transform(X.toarray()) \n# Check if mean = 0, std dev = 1\nprint('Mean : ', X.mean(), \n      '\\nStd Dev : ', X.std())","7772d8b5":"# Create a train and test data with 80% and 20% spilt\ntrain_x, test_x, train_y, test_y = train_test_split(X,y, test_size = 0.2, random_state = 1)\n# Get the shape\n[train_x.shape, test_x.shape, train_y.shape, test_y.shape]","af28ee58":"# PCA model with 2 number of components\npca = PCA(n_components = 2)\n# Fit the data\ntrainx_pca = pca.fit_transform(train_x)\ntestx_pca = pca.transform(test_x)\n# Variation of PC1 and PC2\nprint('Variation per principal component: ', pca.explained_variance_ratio_)\n# Shape of train_x\nprint(\"train_x shape: \", train_x.shape)\n# Shape of trainx_pca\nprint(\"trainx_pca shape: \", trainx_pca.shape)","50c51914":"list_x = list(range(117))\n# Compute Covariance based on all 22 variables\ncov_22 = np.cov(train_x.T)\n# Compute eigenvalue and eigenvector for all variables ussing the covariance from above\ne_values, e_vectors = np.linalg.eig(cov_22)\n# Calculating the explained variance on each of components\nvar_22 = []\nfor i in e_values:\n     var_22.append((i \/ sum(e_values)) * 100)\n# Identifying components that explain at least 95%\ncum_var_22 = np.cumsum(var_22)\n# Set the plot size\nplt.figure(figsize=(10, 6))\n# Visualizing the eigenvalues and finding the \"elbow\" in the graphic\nplt.title(\"Explained Variance vs Number of Components\")\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Cumulative Explained Variance\")\nsns.lineplot(x = list_x, y = cum_var_22.real)\nplt.show()","693c7ca2":"features = ['PC1', 'PC2']\n# Make it into a data frame\npca_df = pd.DataFrame(trainx_pca,columns = features)\n# Include independent variable\npca_df['Class'] = train_y\n# View data frame\npca_df\n\n# Set the figure's size\nplt.figure(figsize=( 24 ,10 ))\n# Plot the Original and transformed data\nX_new = pca.inverse_transform(trainx_pca)\nplt.subplot(1,2,1)\nsns.scatterplot(x=train_x[:, 0], y=train_x[:, 1], hue = \"Class\", data=pca_df, alpha=0.2)\nplt.title(\"Original Mushroom Dataset\")\nplt.subplot(1,2,2)\nsns.scatterplot(x=X_new[:, 0], y=X_new[:, 1], hue = \"Class\", data=pca_df, alpha=0.8)\nplt.title(\"Inverse Transform of Mushroom Dataset\")","ec6c762e":"# Set the figure's size\nplt.figure(figsize=(10,6))\n# Scatter Plot for PC1 and PC2\nsns.scatterplot(x=\"PC1\", y=\"PC2\", hue = \"Class\", data=pca_df, legend=\"full\", alpha=0.6)\nplt.title(\"PCA of Mushroom Dataset\")\nplt.show()","b13a46b0":"# Fit the logistic regression model according to the given training data\nlr = linear_model.LogisticRegression(random_state=0).fit(trainx_pca, train_y)\n# Print coefficient and intercept\nprint('Coefficient : ', lr.coef_, '\\nIntercept :', lr.intercept_)\n# Check accuracy of model\nprint ('Train : ', lr.score(trainx_pca,train_y) * 100)\nprint ('Test  : ', lr.score(testx_pca, test_y) * 100)\n# Predict for test_x\npred_y1 = lr.predict(testx_pca)\n# View the classfication report\nprint(classification_report(test_y, pred_y1))\n# Plot the confusion matrix graph\nplot_confusion_matrix(lr, testx_pca, test_y, cmap = 'Blues_r')\n# Compiling test score of all models for the graph at the end of the project\nscores = []\nscores.append(lr.score(testx_pca, test_y))","066facb6":"train_scores = []\ntest_scores = []\nerror_rates =[]\nfor i in range(1,25):\n    modeli = RandomForestClassifier(max_depth=i, random_state = 0)\n    modeli = modeli.fit(trainx_pca, train_y)\n    train_score = accuracy_score(modeli.predict(trainx_pca),train_y) * 100\n    test_score = accuracy_score(test_y, modeli.predict(testx_pca)) * 100\n    error_rate = np.mean(modeli.predict(testx_pca) != test_y)\n    train_scores.append(train_score)\n    test_scores.append(test_score)\n    error_rates.append(error_rate)\n\n# Set the figure's size    \nplt.figure(figsize=(20,6))\nplt.subplot(1,2,1)\nplt.plot(range(1,25),train_scores,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='steelblue', markersize=10)\nplt.plot(range(1,25),test_scores,color='palevioletred', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Training Data Score and Testing Data Score')\nplt.subplot(1,2,2)\nplt.plot(range(1,25),error_rates,color='#ffcc99', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. Max Depth Value')\nplt.xlabel('Max Depth')\nplt.ylabel('Rates')","ef13134d":"# Fit the Random Forest model according to the given training data\nrfc = RandomForestClassifier(max_depth = 9, random_state=0).fit(trainx_pca, train_y)\n# Check accuracy of model\nprint ('Train : ', rfc.score(trainx_pca,train_y) * 100)\nprint ('Test  : ', rfc.score(testx_pca, test_y) * 100)\n# Predict for test_x\npred_y2 = rfc.predict(testx_pca)\n# View the classfication report\nprint(classification_report(test_y, pred_y2))\n# Plot the confusion matrix graph\nplot_confusion_matrix(rfc, testx_pca, test_y, cmap = 'Blues_r')\n# Compiling test score of all models for the graph at the end of the project\nscores.append(rfc.score(testx_pca, test_y))","ba96e2c9":"# Set the figure's size\nplt.figure(figsize=(6,3))\n# Set the variables\nimportances = rfc.feature_importances_\nindices = np.argsort(importances)\n# Show the quantified relative importance in the order the features were fed to the algorithm\nfeature_imp = pd.Series(rfc.feature_importances_,index=features).sort_values(ascending=False)\n# Plot barh graph\nplt.barh(range(len(indices)), feature_imp, color='b', align='center')\nplt.yticks(range(len(indices)), features)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title('Feature Importances')\nplt.show()","3a104074":"# Data Visualization on Random Forest\nfig, ax = plt.subplots(figsize=(30, 30))\ntree.plot_tree(rfc.estimators_[0], ax=ax, filled=True)\nplt.show()","e1c63286":"train_scores = []\ntest_scores = []\nerror_rates =[]\nfor i in range(1,25):\n    modeli = KNeighborsClassifier(n_neighbors=i)\n    modeli = modeli.fit(trainx_pca, train_y)\n    train_score = accuracy_score(modeli.predict(trainx_pca),train_y) * 100\n    test_score = accuracy_score(test_y, modeli.predict(testx_pca)) * 100\n    error_rate = np.mean(modeli.predict(testx_pca) != test_y)\n    train_scores.append(train_score)\n    test_scores.append(test_score)\n    error_rates.append(error_rate)\n\n#sns.set_palette(['steelblue','palevioletred', '#ffcc99' , 'mediumaquamarine'])\n    \nplt.figure(figsize=(20,6))\nplt.subplot(1,2,1)\nplt.plot(range(1,25),train_scores,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='steelblue', markersize=10)\nplt.plot(range(1,25),test_scores,color='palevioletred', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Training Data Score and Testing Data Score')\nplt.subplot(1,2,2)\nplt.plot(range(1,25),error_rates,color='#ffcc99', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Rates')","84a237e2":"# Fit the KNN Classifier model according to the given training data\nknc = KNeighborsClassifier(n_neighbors=17).fit(trainx_pca, train_y)\n# Check accuracy of model\nprint ('Train : ', knc.score(trainx_pca,train_y) * 100)\nprint ('Test  : ', knc.score(testx_pca, test_y) * 100)\n# Predict for test_x\npred_y1 = knc.predict(testx_pca)\n# View the classfication report\nprint(classification_report(test_y, pred_y1))\n# Plot the confusion matrix graph\nplot_confusion_matrix(knc, testx_pca, test_y, cmap = 'Blues_r')\n# Compiling test score of all models for the graph at the end of the project\nscores.append(knc.score(testx_pca, test_y))","21de399f":"plot_learning_curves(trainx_pca, train_y, testx_pca, test_y, KNeighborsClassifier(n_neighbors=17))\nplt.show()","159708eb":"# Set the variables\n#X_plot = np.column_stack((trainx_pca[:,0], trainx_pca[:,1]))\nX_plot = np.column_stack((pca_df['PC1'].tolist(), pca_df['PC2'].tolist()))\nX = X_plot\ny = train_y\nh = .02  # step size in the mesh\nn_neighbors = 17\n\n# Create color maps\ncmap_light = ListedColormap(['orange', 'cornflowerblue'])\ncmap_bold = ['darkorange', 'darkblue']\n\nfor weights in ['uniform', 'distance']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=pca_df['Class'],\n                    palette=cmap_bold, alpha=1.0, edgecolor=\"black\")\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"2-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\n    plt.xlabel('PC1')\n    plt.ylabel('PC2')\n\nplt.show()","6c90e7e8":"models = ['Logistic Regression','Random Forest Classification','K-Nearest Neighbors']\n\n# Visualising the accuracy score of each classification model\nplt.rcParams['figure.figsize'] = 10, 8\nax = sns.barplot(x = models, y = scores, saturation =1.5)\nplt.xlabel(\"Supervised Models\", fontsize = 16 )\nplt.ylabel(\"Accuracy Score\", fontsize = 16)\nplt.title(\"Accuracy of Various Supervised Models\", fontsize = 16)\nplt.xticks(fontsize = 13, horizontalalignment = 'center', rotation = 0)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","0ac9ea6a":"This is a multiple logistic regression as the data consist more than one independent variables (PC1 and PC2). The intercept point is 0.55 and the coefficient for the two independent variables are 0.96 and -0.16.\n\nFrom the training and testing score, the data is not overfitted. The accuracy score for logistic regression is 88.41% and 88.68% using the transformed training independent variables and testing independent variables. If the original train_x and test_x is used, the model will give us 100% accuracy rate on all results.\n\nFrom the confusion matrix, there are 803 + 638 = 1441 rows of data are predicted correctly as edible or poisonous. There is quite a big portion of date (167) which are actually poisonous mushroom but was predicted as edible mushrooms. This has an impact on the results for precision and recall. Overall, the model results is still good but there may be better models.\n\n\n## Model 2 : Random Forest Classification\n\nThis is a supervised model where the PCA transformed X data was used in the model. The classification model was used since the output results are binary.","55295426":"KNN is a goot fit for this data. There's no overfitting and the accuracy scores are the highest among all three models. There are 81 (65 + 16) incidents where the class of mushroom was predicted inaccurately.\n\nThe graph below shows the learning curve of the model. When the data size is below 80%, the test set consistently have a higher error rate than the training set. Once the data size reach 80% and above, the error rate for training set and test set is almost the same and they are at minimal.","9e41f043":"# Supervised ML Models - Classification","612c203b":"From the graph above, there's a steep curve within the first 10 components. However, a 10D graph is hard to interpret and understand. To make it easier to understand, the number of component chosen is 2. The 2D graph before and after transformation is provided below.","0882f19d":"From the pie chart, there are more edible mushroom in the data than the poisonous mushrooms.\n\nThe next part will be converting the X and y information from string format into 0 and 1. For example, the dependent cells of 'e' and 'p' will be converted to 0 and 1. From the number of counts for 0 and 1, we can identify that 0 represents edible whereas 1 represents poisonous. The number of rows for X and y after converting remains the same. \n\nNote that the number of columns for X has increased from 22 columns to 117 columns. This happens when there are more than one variable in the column. For example, the cap shape includes bell, conical, flat etc. These cannot be represented by 0 and 1 within one column. On such situation, the data in one column will be converted into multiple columns in the new dataset. Bell will be represented in one column with 0 and 1 as whether the cap is bell shaped. Then conical will be represented in the next column with the same concept and so on.","16666b51":"After standardizing the variables , the data is then spilt into 80% training data and 20% testing data. The shape of the new data set is provided below.","34dc121e":"There's no missing value in any of the columns. Thus, all rows and all columns will be used for the analysis. Next, we shall spilt the data into independent and dependent variables.\n\nThe dependent variable is 'class', stating whether the mushroom is edible or poisonous.The independent variables are all the other 22 columns which are the features on the mushroom. All the independent variables are used as they represent different features. There are no obvious duplication on the features. I.e. We are not seeing a column representing the overall stalk color and two columns for the stalk color above and below the ring.\n\n\n## Data Cleaning","9d5597f1":"The accuracy score for training data and testing data is 94.63% and 93.97%. This model seems to be a better fit than the logistic model. However, there is a slight indication of overfitting the training data on the model. The number of data which was predicted inaccurately is pretty small (98 rows) compare to logistic model above. Below are two plots related to random forest. \n\nThe first plot indicates the importance of each variables - PC1 and PC2. From the earlier PCA section, PC1 indicates 8.95% variation per principal component whereas PC2 has 8.13% variation per principal component. It is not surprising to see PC1 is a more important feature in the random forest model compare to PC2. However, the difference on importance between PC1 and PC2 is much larger than expected.\n\nThe second plot is a decision tree plot with max depth of 9 from the random forest model. Within each box, it states which independent variable was used, the gini results and the number of samples. To reach each of the leaf, use the 'AND' function to include all the roots or branch on the path to reach the leaf.","ce531e9a":"Above are the accuracy scores of all three model's test data. All models had used independent variables which undergo PCA to condense the independent variables. Without apply PCA, the models were all providing a 100% accuracy rate. Thus, it was necessary to apply PCA although it creates a small loss in information. \n\nIt is appropriate to compare these three models together as they had all used the same transformed data to model. From the accuracy score above, the KNN models have the highest score. In addition, there were no overfitting on the training data for this model. Therefore, this is the best model among the three models.\n\nNote that although the model results are good, there are a few limitations that should be kept in mind. A major limitation of the models is that they are 'idealizations' or 'simplification' of reality. We may be able to use it to predict reality but there is always a possibility of error in the prediction. Each prediction comes with a set of assumptions which are made during modeling and this causes differences between model and reality. In this project, PCA with 2 number of components were used. If the number of component changed or MCA was used instead of PCA, the best model might not be KNN any longer. Or if we had changed the depth or the number of neighbors to some other values, it will have an impact on the generated model results.","3cf97409":"## Principal Component Analysis (PCA)\n\nThe variation per principal component for MCA and PCA are between 7% to 9%. The difference is not huge, thus, for simplicity to analyze the downstream models, PCA is selected instead of MCA. It is required to condense the information contained in a large number of original variables into a smaller set of new composite dimensions, with a minimum loss of information. Else, all model's results have 100% accuracy rate, which is too good to be true.","f89f7fb6":"## Model 1 : Logistic Regression\n\nThis is a supervised model to model binary dependent variable - edible or poisonous. The transformed training and testing X variables are used in the regression analysis.","fb0dffeb":"The accuracy score of training data and testing data with KNN model reacts differently from the random forest model. The blue line represents training score whereas the red line represents testing score. On the right, the graph shows the error rate of the testing data. On the random forest model earlier, the accuracy score for both training and testing data increases as the max depth increases. On the flip side, the accuracy score decreases as the number of neighbors increases on the KNN model's training data. For the testing data, there's a slight increase on accuracy as the number of neighbors increases.\n\n17 and 21 are two points which are ideal for the model. From the accuracy score and error rate, the selected number of neighbors to model is 17 since it as a low error rate and there's no overfitting shown on the training and testing data. The difference on error rate for 21 is not much different from 17. When number of nieghbors are at 21, the training score is slightly lower than the score at neighbor equals to 17. Thus, the final selected value is 17, instead of 21 although there have very similar results..","cbaa91e7":"Above is the KNN grpph using weights as uniform and distance. The standard model, which is also the model used for the analysis is the uniform weights. The distance weights was included in the plot over here to visualize the difference between uniform and distance. At a quick glance, the two plots are similar. There seems to be no obvious difference between these two.\n\n## Conclusion","0ddf5032":"Above is a quick view of the variables in the dataset and the unique features in each column.","0cc87a7c":"## Model 3 : KNN\n\nThis is a supervised model where the PCA transformed X data was used in the model. The classification model was used since the output results are binary.","3a6a8177":"After looking at n_components equals to 2 and 3. To make it simple and easy to understand, we shall use 2 as the number of components.\n\nNote that 'fit_transform' was used on the training data so that we can scale the training data and also learn the scaling parameters of the data. The model built will learn the mean and variance of the features of the training set. These learned parameters are then used to scale the test data. In other words, 'transform' was used on the testing data. In addition, the transformation are happening on the independent variables. The dependent variable is not involved in the PCA process below.","17c74b31":"Above is the accuracy score for training data and testing data on the left with various max depth. The blue line represents training score whereas the red line represents testing score. On the right, the graph shows the error rate of the testing data. When the depth goes beyond 25, most of the accuracy score for training data is 100%. Thus, the graph shows the max depth from 0 to 24 only. Max depth of 4, 8, 12 or 15 is a good fit to model based on the steep decrease on error rate. The overall accuracy on training data is pretty good and the accuracy for testing data is consistently around 94%. On all the data points on the left graph, we are seeing training data's accuracy score is consistently higher than the testing data. To minimize the degree of overfitting the data and still have a good accuracy score results, the selected model has a max depth set to 9.","bd04c71d":"Data Source : https:\/\/www.kaggle.com\/uciml\/mushroom-classification\n\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like \"leaflets three, let it be'' for Poisonous Oak and Ivy.\n\nTime period: Donated to UCI ML 27 April 1987\n\nDependent Variable : (classes: edible=e, poisonous=p)\n\nInformation on Independent Variables:\n- cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n- cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n- cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n- bruises: bruises=t,no=f\n- odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n- gill-attachment: attached=a,descending=d,free=f,notched=n\n- gill-spacing: close=c,crowded=w\n- gill-size: broad=b,narrow=n\n- gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n- stalk-shape: enlarging=e,tapering=t\n- stalk-root: bulbous=b,club=c,cup=u,equal=e,rooted=r,missing=?\n- stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n- stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n- stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n- stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n- veil-type: partial=p\n- veil-color: brown=n,orange=o,white=w,yellow=y\n- ring-number: none=n,one=o,two=t\n- ring-type: evanescent=e,flaring=f,large=l,none=n,pendant=p\n- spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n- population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n- habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d\n\nThe purpose of this project is to understand which model performs the best on this dataset.\n\n\n## Data Extraction"}}