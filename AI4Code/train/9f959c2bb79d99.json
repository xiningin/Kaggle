{"cell_type":{"142cf364":"code","527e9b8e":"code","31314a3e":"code","4ab5722f":"code","340dc4ea":"code","b687e481":"code","dffa22aa":"code","012ead3d":"code","ddff8643":"code","f1a4623d":"code","af3095fa":"code","93efbff4":"code","ea49854f":"code","f88fbef8":"code","40b5489a":"code","1d9f9afb":"code","9ff320b7":"code","8efe7e17":"code","83c2d4a5":"code","6a69808e":"code","0206afc5":"code","0c53bb51":"code","e5333a2a":"code","8a090f3b":"code","b50d1431":"code","e79db1a3":"code","5a7cdbbf":"code","3f516e5a":"code","75d523b5":"code","0a8a9403":"code","a0ba8a95":"code","cf252a53":"code","a7b6f773":"code","6eb609d1":"code","c47a0e8a":"code","bb0a4254":"code","b2b048d6":"code","379e16b4":"code","0f9780ee":"code","bc1b7080":"code","83127616":"code","40cbecbf":"code","6481d488":"code","f543c835":"code","6834469b":"code","f82f392b":"code","50d745cd":"code","5f34e256":"code","530bad9e":"code","9ffe7c79":"code","a8ab1cf3":"code","ec9f00c1":"code","dfa63d9e":"code","953e0dcf":"code","e39b0dbc":"code","0fbf18c6":"code","8fc78e52":"code","476e3ba1":"code","536eb272":"code","3db3d5f7":"code","9f6884d6":"code","41763114":"code","b8566893":"code","cc8dfe7d":"code","73df3c6d":"code","a3672bfb":"code","c7daad9e":"code","ad3e8671":"code","3bbef58c":"code","db395762":"code","6f5cb1ae":"code","4cd5e439":"code","ed2245c2":"code","376e2110":"code","493a4506":"code","de8ae265":"code","98d93e13":"code","9b378d7f":"code","73e64bb5":"code","94c3d0ce":"code","2019adfe":"code","80ddc3c9":"code","d38de0a5":"code","c5568671":"code","2687c2c4":"markdown","745dff7c":"markdown","d898fc1a":"markdown","a219cbd0":"markdown"},"source":{"142cf364":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","527e9b8e":"# let's first read all data\n\ndf2015 = pd.read_csv(\"..\/input\/world-happiness\/2015.csv\")\ndf2016 = pd.read_csv(\"..\/input\/world-happiness\/2016.csv\")\ndf2017 = pd.read_csv(\"..\/input\/world-happiness\/2017.csv\")\ndf2018 = pd.read_csv(\"..\/input\/world-happiness\/2018.csv\")\ndf2019 = pd.read_csv(\"..\/input\/world-happiness\/2019.csv\")","31314a3e":"# let's check shape of all data frame\n\nprint(\"2015: \", df2015.shape)\nprint(\"2016: \", df2016.shape)\nprint(\"2017: \", df2017.shape)\nprint(\"2018: \", df2018.shape)\nprint(\"2019: \", df2019.shape)","4ab5722f":"# here we can see that 2018 and 2019 data frame have minimum column compare to other data frame\n# let's first clean and preprocess data of 2019 data frame\n\n# checking NaN values\ndf2019.isna().sum()","340dc4ea":"# checking dtypes\n\n\ndf2019.info()","b687e481":"df2015.columns","dffa22aa":"# so here dtypes are perfect according to it's values\n# here let's rename the column for ease to access\n\ndf2019.columns = df2019.columns.str.lower()\ndf2019.rename(columns = {\"overall rank\": \"rank\", \"country or region\": \"country\",\n                         \"freedom to make life choices\": \"freedom\", \"perceptions of corruption\": \"corruption\",\n                        \"gdp per capita\": \"gdp\", \"healthy life expectancy\": \"health\",\n                        \"social support\": \"family\"}, inplace=True)\ndf2019.columns = df2019.columns.str.replace(\" \", \"_\")\ndf2019.columns","012ead3d":"df2019.sample(5)","ddff8643":"# so here 2019 data frame it's our main common column among all year data set\n# let's look at to the 2018 data\n\n# let's check NaN values\ndf2018.isna().sum()","f1a4623d":"df2018[df2018.isna().values]\n# here we have 1 NaN value in corruption for United Arab Emirates we can not drop it because\n# this country data important for use we will handle it later on","af3095fa":"# let's check dtypes\n\ndf2018.info()\n\n# dtypes are perfect respect to it's values","93efbff4":"# let's rename the columns and make column as 2019 data frame\n\ndf2018.columns = df2018.columns.str.lower()\ndf2018.rename(columns = {\"overall rank\": \"rank\", \"country or region\": \"country\",\n                         \"freedom to make life choices\": \"freedom\", \"perceptions of corruption\": \"corruption\",\n                        \"gdp per capita\": \"gdp\", \"healthy life expectancy\": \"health\",\n                        \"social support\": \"family\"}, inplace=True)\ndf2018.columns = df2018.columns.str.replace(\" \", \"_\")\ndf2018.columns","ea49854f":"df2018.sample(5)","f88fbef8":"# let's look at to the 2017 data frame\n# let's first check NaN values\n\ndf2017.isna().sum()","40b5489a":"# here we don't have any NaN value let's check dtypes\n\ndf2017.info()\n# dtypes are perfect respect to it's values","1d9f9afb":"# let's rename the columns and make it same as in 2019 data frame\n\n# let's reamove the dots \ndf2017.columns = df2017.columns.str.replace(\".\", \"\")\n# let's convert into lower case\ndf2017.columns = df2017.columns.str.lower()\ndf2017.rename(columns = {\"happinessrank\": \"rank\", \"happinessscore\": \"score\", \"healthlifeexpectancy\": \"health\",\n                        \"economygdppercapita\": \"gdp\", \"trustgovernmentcorruption\": \"corruption\",\n                        \"dystopiaresidual\": \"dystopia\"}, inplace = True)\ndf2017.columns","9ff320b7":"# here in 2017 data frame we have some extra columns which are not present in 2019 so let's drop it\n\ndf2017.drop(columns = ['whiskerhigh', 'whiskerlow'], inplace = True)","8efe7e17":"df2017.columns","83c2d4a5":"df2017.sample(5)","6a69808e":"# let's look at to the 2016 data frame\n# let's first check NaN values\n\ndf2016.isna().sum()","0206afc5":"# here we don't have any NaN values\n# let's look at the dtyps\n\ndf2016.info()","0c53bb51":"# here we have perfect dtypes respect to it's values\n# let's rename the column base on 2019 data frame\n\ndf2016.columns = df2016.columns.str.lower()\ndf2016.rename(columns={'happiness rank':'rank', 'happiness score':'score', 'economy (gdp per capita)': 'gdp',\n                      'health (life expectancy)':'health', 'trust (government corruption)': 'corruption',\n                      \"dystopia residual\": \"dystopia\"},\n                       inplace = True)\ndf2016.columns","e5333a2a":"# here we have some extra columns compare to 2019 data frame so let's drop it\n\ndf2016.drop(columns = ['lower confidence interval','upper confidence interval',\n                      'region'],\n           inplace = True)\ndf2016.sample(5)","8a090f3b":"# now let's look at to the 2015 data frame\n\n# checking NaN values\ndf2015.isna().sum()","b50d1431":"# here we don't have any NaN values\n# let's check dtypes \n\ndf2015.info()\n# here all dtypes are perfect according to it's value","e79db1a3":"# let's rename the columns base on 2019 columns\n\ndf2015.rename(columns={'Happiness Rank':'rank', 'Happiness Score':'score', 'Economy (GDP per Capita)': 'gdp',\n                      'Health (Life Expectancy)': 'health', 'Trust (Government Corruption)': 'corruption',\n                      \"Dystopia Residual\": \"dystopia\"},\n                        inplace = True)\ndf2015.columns = df2015.columns.str.lower()\ndf2015.columns","5a7cdbbf":"# here we have some extra columns compare to 2019 so let's remove it\n\ndf2015.drop(columns = ['region', 'standard error'], inplace = True)\ndf2015.sample(5)","3f516e5a":"# let's check all columns of all data frame\n\ndef print_all_dataframe_columns():\n    print(\"2015:\\n\", df2015.columns.to_list())\n    print(\"\\n2016:\\n\", df2016.columns.to_list())\n    print(\"\\n2017:\\n\", df2017.columns.to_list())\n    print(\"\\n2018:\\n\", df2018.columns.to_list())\n    print(\"\\n2019:\\n\", df2019.columns.to_list())\n    \nprint_all_dataframe_columns()","75d523b5":"# so here we don't have a column dystopia in year 2018 and 2019 \n# so i will predict dystopia value for 2018 and 2019 by making a model base on other year data\n# so let's first make a empty column of dystopia with value 0.0 in year 2018 and 2019 for making balanced data fram\n\ndf2018[\"dystopia\"] = 0.0\ndf2019[\"dystopia\"] = 0.0","0a8a9403":"# now let's print again columns\n\nprint_all_dataframe_columns()","a0ba8a95":"# Adding year column to each data frame.\n# here year column indicate year of particular year\n\ndf2015[\"year\"] = 2015\ndf2016[\"year\"] = 2016\ndf2017[\"year\"] = 2017\ndf2018[\"year\"] = 2018\ndf2019[\"year\"] = 2019","cf252a53":"# let's add a year column to particular data set.\n# here year column will indicate the year of particular record\n\nprint_all_dataframe_columns()","a7b6f773":"# now let's combine all the data frame into one \n\nmain_data = pd.concat([df2015, df2016, df2017, df2018, df2019]).reset_index(drop=True)","6eb609d1":"main_data.shape","c47a0e8a":"main_data.sample(5)","bb0a4254":"# let's first convert country name to lower cases\n\nmain_data.country = main_data.country.str.lower().str.replace(\" \", \"_\")","b2b048d6":"# let's find the NaN values in main_data\n\nmain_data.isna().sum()","379e16b4":"# so here in the corruption column we have one NaN value let's check it out\n\nmain_data[main_data.isna().values]","0f9780ee":"# so here we have NaN value in year 2018 for corruption column in country united arab emirates\n\nmain_data.loc[main_data.country == \"united_arab_emirates\"]","bc1b7080":"# here i'm going to fill that NaN value by average of other years value of corruption for united arab emirates\n\navg_corruption = main_data.loc[main_data.country == \"united_arab_emirates\", \"corruption\"].mean()\n\nmain_data.corruption.fillna(avg_corruption, inplace=True)","83127616":"main_data.isna().sum().any()","40cbecbf":"# le's combine 2016, 2017 and 2018 data set from main_data data frame\n# because in these data sets we have dystopia values which will be help full to create a model to predict\n# a dystopia value for year 2018 and 2019\n\ndf_dystopia = main_data.loc[(main_data.year == 2015) | (main_data.year == 2016) | (main_data.year == 2017)].copy().reset_index(drop=True)","6481d488":"# let's look at the data of dystopia \ndf_dystopia.sample(4)","f543c835":"# here we have a text data in country column so let's check how many unique country we have\ndf_dystopia.country.nunique()","6834469b":"# we can't pass text data to ML model because ML model work on numeric data for calculating stuff\n# so let's convert text data into numerice\n# here i'm use One Hot Ecoding to convert text data into numeric\n\nencoded_country = pd.get_dummies(df_dystopia.country, drop_first=True) # afghanistan was first column\n# here i have drop first column for preventing dummy variable trap","f82f392b":"# let's merge encoded country columns with our main df_dystopia data frame\ndf_dystopia = pd.concat([df_dystopia.copy(), encoded_country], axis = 1)","50d745cd":"df_dystopia.head()","5f34e256":"# now we have encoded country column so we don't need country column so let's drop it\n\ndf_dystopia.drop(columns = \"country\", inplace = True)","530bad9e":"df_dystopia.sample(3)","9ffe7c79":"# let's create feature matrix X and target vector y\n\nX = df_dystopia.drop(columns = \"dystopia\")\ny = df_dystopia.dystopia","a8ab1cf3":"X.shape","ec9f00c1":"y.shape","dfa63d9e":"# split data into train test\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X.copy(), y.copy(), test_size = 0.2)","953e0dcf":"X_train.sample(3)","e39b0dbc":"print(\"X Train: \", X_train.shape)\nprint(\"y train: \", y_train.shape)\nprint(\"X Test: \", X_test.shape)\nprint(\"y test: \", y_test.shape)","0fbf18c6":"# Scalling using Standard Scaler\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train.iloc[:, :9] = scaler.fit_transform(X_train.iloc[:, :9])\nX_test.iloc[:, :9] = scaler.transform(X_test.iloc[:, :9])","8fc78e52":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\n\nalgos = {\n    \"Ridge\": {\n        \"model\": Ridge(),\n        \"params\": {\n            \"alpha\": [1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 1, 5, 10, 20, 30 , 40]\n        }\n    },\n    \"Lasso\": {\n        \"model\": Lasso(),\n        \"params\": {\n            \"alpha\": [1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 1, 5, 10, 20, 30, 40]\n        }\n    },\n    \"Random Forest\": {\n        \"model\": RandomForestRegressor(),\n        \"params\": {\n            \"n_estimators\": [int(x) for x in np.linspace(100, 1200, 12)],\n            \"max_features\": [\"auto\", \"sqrt\"],\n            \"max_depth\": [int(x) for x in np.linspace(5, 30, 6)],\n            \"min_samples_split\": [2, 5, 10, 15, 100],\n            \"min_samples_leaf\": [1, 2, 5, 10]\n        }\n    }\n}\n\nbest_models = {}\nscore = []\nfor model_name, values in algos.items():\n    rscv = RandomizedSearchCV(values[\"model\"], values[\"params\"], cv = 5, n_jobs = -1)\n    rscv.fit(X_train, y_train)\n    best_models[model_name] = rscv\n    score.append({\n        \"Model\": model_name,\n        \"Best Parameters\": rscv.best_params_,\n        \"Best Score\": rscv.best_score_\n    })\n    \npd.DataFrame(score)","476e3ba1":"# now let's test ridge lasso and random forest regressor model on our test data set\n\nprint(\"Ridge: \", best_models[\"Ridge\"].score(X_test, y_test))\nprint(\"Lasso: \", best_models[\"Lasso\"].score(X_test, y_test))\nprint(\"Random Forest\", best_models[\"Random Forest\"].score(X_test, y_test))","536eb272":"# here as we know ridge regression perform better\n# let's predict test data using ridge regression model\n\ndystopia_pred = best_models[\"Ridge\"].predict(X_test)","3db3d5f7":"# let's find difference between actual value and predicted value\n\nsns.distplot(y_test - dystopia_pred)\n# here curve look like normal distribution so our model trained value","9f6884d6":"# As we know Ridge regression work better on our data for prediction dystopia\n# so let's create final model using ridge regression on base on all data i mean full X and y data\n\n# so let's first scale the full X data using Standard Scaler\n# scaling X data\n\nscaler = StandardScaler()\nX.iloc[:, :9] = scaler.fit_transform(X.iloc[:, :9])","41763114":"X.sample(5)","b8566893":"# let's use Lasso Regression for building final model with alpha = 1e-0.8\n\nfinal_model_dystopia = Lasso(alpha = 1e-08)\nfinal_model_dystopia.fit(X, y)","cc8dfe7d":"X.columns","73df3c6d":"np.where(X.columns == \"ok\")","a3672bfb":"# let's create predict function to predict dystopia\n# in the function we perform scalling and prediction using final_model_dystopia\n\ndef predict_dystopia(record):\n    X_new = np.zeros((1, X.shape[1]), dtype=\"int\")\n    other_columns = [[record[\"rank\"], record[\"score\"], record[\"gdp\"], record[\"family\"], record[\"health\"],\n                    record[\"freedom\"], record[\"generosity\"], record[\"corruption\"], record[\"year\"]]]\n    other_columns = scaler.transform(other_columns).flatten()\n    for i in range(0, 9):\n        X_new[0][i] = other_columns[i]\n    country_name = record[\"country\"]\n    if country_name != \"afghanistan\": # bcz we dropped afghanistan column so it can not find it.\n        country_index = np.where(X.columns == record[\"country\"])\n        X_new[0][country_index] = 1\n    \n    predicted_value = final_model_dystopia.predict(X_new)\n    return predicted_value[0]","c7daad9e":"# let's create a loop and put a predicted value to dystopia column where year 2018 and 2019\n\nfor i in main_data.loc[(main_data.year == 2018) | (main_data.year == 2019)].index:\n    main_data.loc[i, \"dystopia\"] = predict_dystopia(main_data.loc[i])","ad3e8671":"# now our dystopia value has been filled for year 2018 and 2019 using lasso regression model\nmain_data.sample(5)","3bbef58c":"year_wise_data = main_data.groupby([\"year\", \"country\"])[\"score\"].sum().reset_index()","db395762":"year_wise_data.loc[year_wise_data.country == \"india\"]","6f5cb1ae":"top_10_2015 = year_wise_data.loc[year_wise_data.year==2015].sort_values(\"score\", ascending = False).reset_index(drop=True).head(10)\ntop_10_2019 = year_wise_data.loc[year_wise_data.year==2019].sort_values(\"score\", ascending = False).reset_index(drop=True).head(10)","4cd5e439":"top_10_2015.style.background_gradient(cmap=\"Reds\")","ed2245c2":"top_10_2019.style.background_gradient(cmap=\"Greens\")","376e2110":"# let's encode the text data to numeric format using one hot encoding\n\nencoded_country = pd.get_dummies(main_data.country, drop_first=True) # first column was afghanistan\n\n# concate encoded country to main data\nmain_data = pd.concat([main_data, encoded_country], axis = 1)\n\n# drop country column\nmain_data.drop(columns = \"country\", inplace = True)","493a4506":"# let's create a feature matrix X and response vactor y\n\nX = main_data.drop(columns = \"score\")\ny = main_data.score","de8ae265":"# let's divide the data into train and test part\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","98d93e13":"print(\"X Train: \", X_train.shape)\nprint(\"y train: \", y_train.shape)\nprint(\"X Test: \", X_test.shape)\nprint(\"y test: \", y_test.shape)","9b378d7f":"# scale the features\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train.iloc[:, 0:9] = scaler.fit_transform(X_train.iloc[:, 0:9])\nX_test.iloc[:, 0:9] = scaler.transform(X_test.iloc[:, 0:9])","73e64bb5":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\n\nalgos = {\n    \"Ridge\": {\n        \"model\": Ridge(),\n        \"params\": {\n            \"alpha\": [1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 1, 5, 10, 20, 30 , 40]\n        }\n    },\n    \"Lasso\": {\n        \"model\": Lasso(),\n        \"params\": {\n            \"alpha\": [1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 1, 5, 10, 20, 30, 40]\n        }\n    },\n    \"Random Forest\": {\n        \"model\": RandomForestRegressor(),\n        \"params\": {\n            \"n_estimators\": [int(x) for x in np.linspace(100, 1200, 12)],\n            \"max_features\": [\"auto\", \"sqrt\"],\n            \"max_depth\": [int(x) for x in np.linspace(5, 30, 6)],\n            \"min_samples_split\": [2, 5, 10, 15, 100],\n            \"min_samples_leaf\": [1, 2, 5, 10]\n        }\n    }\n}\n\nbest_models = {}\nscore = []\nfor model_name, values in algos.items():\n    rscv = RandomizedSearchCV(values[\"model\"], values[\"params\"], cv = 5, n_jobs = -1)\n    rscv.fit(X_train, y_train)\n    best_models[model_name] = rscv\n    score.append({\n        \"Model\": model_name,\n        \"Best Parameters\": rscv.best_params_,\n        \"Best Score\": rscv.best_score_\n    })\n    \npd.DataFrame(score)","94c3d0ce":"# now let's test ridge lasso and random forest regressor model on our test data set\n\nprint(\"Ridge: \", best_models[\"Ridge\"].score(X_test, y_test))\nprint(\"Lasso: \", best_models[\"Lasso\"].score(X_test, y_test))\nprint(\"Random Forest\", best_models[\"Random Forest\"].score(X_test, y_test))","2019adfe":"y_prediction = best_models[\"Random Forest\"].predict(X_test)","80ddc3c9":"sns.distplot(y_test - y_prediction)","d38de0a5":"from sklearn.metrics import mean_absolute_error, mean_squared_error","c5568671":"print(\"MAE: \", mean_absolute_error(y_test, y_prediction))\nprint(\"MSE: \", mean_squared_error(y_test, y_prediction))","2687c2c4":"#### Top 10 Hapinees Country in 2015 and 2019","745dff7c":"#### final model for dystopia","d898fc1a":"### Top 10 Happiness country in year 2015 and 2019","a219cbd0":"### Filling dysopia value for year 2018 and 2019 by creating model base on 2015, 2016 and 2017"}}