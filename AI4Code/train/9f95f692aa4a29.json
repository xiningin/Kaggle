{"cell_type":{"c607692d":"code","cac6386c":"code","5b62c9f0":"code","c1fc16d1":"code","040cde36":"code","968ca113":"code","2603ae34":"code","5754f2b9":"code","eedcd05b":"code","35246a13":"code","aeed7c97":"code","76ddbf72":"code","1d8fa404":"code","1ba17224":"code","7cddf931":"markdown","e89bcb24":"markdown","dbe5151c":"markdown","15ca7a4d":"markdown"},"source":{"c607692d":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport os   \n\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\n\nimport warnings \nwarnings.filterwarnings('ignore') ","cac6386c":"image_files = Path('..\/input\/food41\/images') ","5b62c9f0":"images = list(image_files.glob(r'**\/*.jpg'))\nlabel = list(map(lambda x : os.path.split(os.path.split(x)[0])[1], images))\nimagedata = pd.DataFrame({'Files' : images, \"label\" : label}).astype(str).sample(frac = 1.0, random_state = 1).reset_index(drop=True)\n\ncategory_samples = []\nfor category in imagedata['label'].unique():\n    category_slice = imagedata.query('label== @category')\n    category_samples.append(category_slice.sample(300, random_state=1))\nimage_df = pd.concat(category_samples, axis=0).sample(frac=1.0, random_state =1).reset_index(drop=True)\n\n    \n","c1fc16d1":"image_df['label'].value_counts() ","040cde36":"traindata, testdata = train_test_split(image_df, train_size =0.7, random_state =1)","968ca113":"train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function = tf.keras.applications.mobilenet_v2.preprocess_input,\n    validation_split=0.2\n)\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function = tf.keras.applications.mobilenet_v2.preprocess_input\n)","2603ae34":"train_images = train_generator.flow_from_dataframe(\n    dataframe=traindata,\n    x_col='Files',\n    y_col='label',\n    target=(224,224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    subset ='training'\n)\nval_images = train_generator.flow_from_dataframe(\n    dataframe=traindata,\n    x_col='Files',\n    y_col='label',\n    target=(224,224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    subset ='validation'\n)\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=testdata,\n    x_col='Files',\n    y_col='label',\n    target=(224,224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False\n)","5754f2b9":"pretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224,224,3),\n    include_top=False,\n    weights='imagenet',\n    pooling = 'avg'        \n) \n\npretrained_model.trainable = False","eedcd05b":"inputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation = 'relu')(pretrained_model.output)\nx = tf.keras.layers.Dense(128, activation = 'relu')(x)\n\noutputs = tf.keras.layers.Dense(101, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs = inputs, outputs = outputs )\n\nprint(model.summary())\n","35246a13":"model.compile(\n    optimizer = 'adam',\n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    epochs =100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor = 'val_loss',\n            patience = 3,\n            restore_best_weights=True\n        )\n    ]\n)","aeed7c97":"results  = model.evaluate(test_images, verbose = 0)\nprint('Test accuracy : {:.2f}%'.format(results[1] * 100))","76ddbf72":"predictions = np.argmax(model.predict(test_images), axis=1)\n\ncm = confusion_matrix(test_images.labels, predictions)\nclr = classification_report(test_images.labels, predictions)","1d8fa404":"plt.figure(figsize =(30,30))\nsns.heatmap(cm, annot = True, fmt = 'g', vmin=0, cmap = 'Blues', cbar = False)\nplt.xticks(ticks = np.arange(101) + 0.5, label = test_images.class_indices, rotation =90)\nplt.yticks(ticks = np.arange(101) + 0.5, label = test_images.class_indices, rotation =0)\nplt.xlabel('Predicted')\nplt.ylabel('Actuall')\nplt.title('confusion matrix')","1ba17224":"print('classification report \\n-----------------------\\n', clr)","7cddf931":"# Training","e89bcb24":"# Load image files","dbe5151c":"# Results","15ca7a4d":"# Modeling"}}