{"cell_type":{"209f8acb":"code","3c807dfa":"code","760abc15":"code","a43e7bf2":"code","c43ed29d":"code","2a458134":"code","40d3f978":"code","932e316c":"code","521d31dc":"code","88c22efc":"code","094878e5":"code","eecee3cb":"code","f7213796":"code","40490da9":"code","b19c3c89":"code","86ae5200":"code","9c4daf40":"code","8147506d":"code","c06fa448":"code","342e7975":"code","ec678f3c":"markdown","a8860539":"markdown","b6f8c0f3":"markdown","7af2b54e":"markdown","51caae62":"markdown","6b56fa66":"markdown","ea9b2065":"markdown","58e6b12b":"markdown"},"source":{"209f8acb":"import tensorflow as tf\nimport os","3c807dfa":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \nexcept ValueError:\n    TPU = None\n\nif TPU:\n    print(f\"\\n... RUNNING ON TPU - {TPU.master()}...\")\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    print(f\"\\n... RUNNING ON CPU\/GPU ...\")\n    # Yield the default distribution strategy in Tensorflow\n    #   --> Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy() \n\n# What Is a Replica?\n#    --> A single Cloud TPU device consists of FOUR chips, each of which has TWO TPU cores. \n#    --> Therefore, for efficient utilization of Cloud TPU, a program should make use of each of the EIGHT (4x2) cores. \n#    --> Each replica is essentially a copy of the training graph that is run on each core and \n#        trains a mini-batch containing 1\/8th of the overall batch size\nN_REPLICAS = strategy.num_replicas_in_sync\n    \nprint(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")\n\nprint(f\"\\n... ACCELERATOR SETUP COMPLTED ...\\n\")","760abc15":"print(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n\nprint(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n# enable XLA optmizations (10% speedup when using @tf.function calls)\ntf.config.optimizer.set_jit(True)\n\nprint(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")","a43e7bf2":"# Step 3: Use a familiar call to get the GCS path of the dataset\nfrom kaggle_datasets import KaggleDatasets\nDATA_DIR = KaggleDatasets().get_gcs_path(\"hungry-geese-nlp-preprocess-tpu-ds\")","c43ed29d":"import re\nclass_regex = \"df_([0-9]+)\\.tfrec\"\nclass_prog = re.compile(class_regex)","2a458134":"# Get the Full Paths to The Individual TFRecord Files\nTRAIN_TFREC_PATHS = sorted(\n    tf.io.gfile.glob(os.path.join(DATA_DIR, \"*.tfrec\")), \n    key=lambda x: int(class_prog.findall(x)[0]))\nTRAIN_TFREC_PATHS","40d3f978":"feature_description = {    \n    'label': tf.io.FixedLenFeature([], tf.int64),\n    'sentence': tf.io.FixedLenFeature([77], tf.int64),\n    'positions': tf.io.FixedLenFeature([77], tf.int64)\n}","932e316c":"import pickle\nimport bz2\ndef _parse_function(example):\n    \"\"\"\n    Args:\n        example: A string tensor representing a `tf.train.Example`.\n    \"\"\"\n\n    # Parse `example`.\n    parsed_example = tf.io.parse_single_example(example, feature_description)\n    # Decode the tf.string\n    \n    sentence = parsed_example['sentence']\n    positions = parsed_example['positions']\n    label = parsed_example['label']\n    return sentence, positions, label","521d31dc":"from tensorflow.keras.layers import Embedding, MultiHeadAttention, LayerNormalization, Dropout, Dense\n\ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n        Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n        Dense(d_model)  # (batch_size, seq_len, d_model)\n    ])\n\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1, training=True):\n        super(EncoderLayer, self).__init__()\n        self.training = training\n        \n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = Dropout(rate)\n        self.dropout2 = Dropout(rate)\n\n    def call(self, x):\n        attn_output = self.mha(x, x, x)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=self.training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=self.training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2\n\nclass Net(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1, training=True):\n        super(Net, self).__init__()\n        self.training = training\n        self.d_model = d_model\n        self.num_layers = num_layers\n        \n        self.emb = Embedding(input_dim=50, output_dim=128)\n        self.pos_emb = Embedding(input_dim=77, output_dim=128) # relative positional embedding\n        self.cls_token_emb = Embedding(input_dim=1, output_dim=256) # class token\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate, self.training)\n                           for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n        \n        bs = 128 if self.training else 1\n        self.token = tf.zeros((bs,1),dtype=tf.int64)\n        self.policy_head = Dense(4, activation=None, use_bias=False)\n        \n    def call(self, sentence, positions):\n        h = self.emb(sentence)\n        h_pos = self.pos_emb(positions)\n        h = tf.concat((h_pos,h), 2)\n        \n        h_token = self.cls_token_emb(self.token)\n        h = tf.concat((h_token,h), 1)\n        \n        h = self.dropout(h, training=self.training)\n        \n        for i in range(self.num_layers):\n            h = self.enc_layers[i](h)\n        \n        action_token_h = h[:,0]\n\n        return self.policy_head(action_token_h)","88c22efc":"BATCH_SIZE_PER_REPLICA = 128\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\ntrain_steps = (100000*24)\/\/GLOBAL_BATCH_SIZE\nBUFFER_SIZE = 10000\nTRAIN_IMAGE_MODEL = True\n\nprefetch = 50\n\ndef get_dataset(_):\n    raw_train_ds = tf.data.TFRecordDataset(TRAIN_TFREC_PATHS, num_parallel_reads=AUTO)\n    \n    dataset = raw_train_ds.repeat().shuffle(BUFFER_SIZE).map(_parse_function,num_parallel_calls=AUTO)\n    dataset = dataset.batch(BATCH_SIZE_PER_REPLICA).prefetch(prefetch)\n    return dataset","094878e5":"with strategy.scope():\n    optimizer = tf.keras.optimizers.Adam(0.000025)\n    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n\n    #@tf.function\n    def loss_function(real, pred):\n        loss_ = crossentropy(real, pred)\n      \n        return tf.nn.compute_average_loss(loss_, global_batch_size=GLOBAL_BATCH_SIZE)\n    \n    def accuracy_function(real, pred):\n        accuracies = tf.equal(real, tf.argmax(pred, axis=1))\n        accuracies = tf.cast(accuracies, dtype=tf.float32)\n        \n        return tf.math.reduce_mean(accuracies)","eecee3cb":"@tf.function(experimental_relax_shapes=True)\ndef train_step(inputs):\n    sentence, positions, target = inputs\n    loss = 0.\n        \n    with tf.GradientTape() as tape:  \n        logits = model(sentence,positions)\n        loss = loss_function(target, logits)\n    \n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    accuracy = accuracy_function(target, logits)\n    \n    return loss, accuracy\n\n@tf.function(experimental_relax_shapes=True)\ndef distributed_train_step(inputs,):\n    per_replica_losses, per_replica_accuracy = strategy.run(train_step, args=(inputs,))\n    return strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses,axis=None), strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_accuracy,axis=None)","f7213796":"AUTO = tf.data.experimental.AUTOTUNE","40490da9":"import time\nEPOCHS = 2\nwith strategy.scope():\n    model = Net(num_layers=6, d_model=256, num_heads=8, dff=2048)\n    \n    dataset = strategy.experimental_distribute_datasets_from_function(get_dataset)\n    train_iterator = iter(dataset)\n    \n    best_loss = 0\n    loss_plot = []\n    \n    for epoch in range(EPOCHS):\n        start = time.time()\n        total_loss = 0.0\n        print(f'Epoch {epoch + 1}\/{EPOCHS}')\n        for batch in range(train_steps):\n            inputs = next(train_iterator)\n            loss, accuracy = distributed_train_step(inputs)\n            loss = loss.numpy()\n            accuracy = accuracy.numpy()\n            total_loss += loss\n\n            if batch % 1==0:\n                print(f'{batch+1}\/{train_steps} Total Loss: {total_loss\/(batch+1):.4f}  Batch Loss: {loss:.4f}  Batch Acc: {accuracy:.4f}',end='\\r')\n\n        # storing the epoch end loss value to plot later\n        loss_plot.append(total_loss \/ train_steps)\n        \n        print(f'Epoch {epoch + 1} Loss {total_loss\/train_steps:.6f}')\n        print(f'Time taken for 1 epoch {time.time() - start} sec\\n')","b19c3c89":"with strategy.scope():    \n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\n    model.save_weights('.\/model.h5', options=save_locally) # saving in Tensorflow's \"SavedModel\" format","86ae5200":"model_inf = Net(num_layers=6, d_model=256, num_heads=8, dff=2048, training=False)\nmodel_inf(tf.zeros((1,77),dtype=tf.int64),tf.zeros((1,77),dtype=tf.int64))","9c4daf40":"model_inf.load_weights('.\/model.h5')","8147506d":"import numpy as np\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Action\ndef preprocess_map_obs(obs, previous_obs=None, p=None):\n    if p is None:\n        p = 0\n    \n    relativ_center = obs[0]['observation']['geese'][p][0]\n    relativ_poss = np.roll(np.arange(77), relativ_center)\n    \n    sentence = []\n    positions = []\n    for pp, player in enumerate(obs):\n        real_player_index = pp\n        player_index = (pp - p) % 4\n        geese_length = len(obs[0]['observation']['geese'][real_player_index])\n        for goose_body_position, goose_board_position in enumerate(obs[0]['observation']['geese'][real_player_index]):\n            if goose_body_position == 0:\n                body_part = 0\n            elif goose_body_position == (geese_length-1):\n                body_part = 2\n            else:\n                body_part = 1\n\n            last_action = Action[obs[real_player_index]['action']].value\n\n            index_player = 3*4 * player_index\n            index_bodypart = 4*body_part\n            \n            word_unique_index = index_player + index_bodypart + last_action + 1\n            sentence.append(word_unique_index)\n            \n            position = relativ_poss[goose_board_position]\n            positions.append(position)\n            \n\n    for food_board_position in obs[0]['observation']['food']:\n        word_unique_index = 47 + 1 + 1\n        sentence.append(word_unique_index)\n        \n        position = relativ_poss[food_board_position]\n        positions.append(position)\n        \n    left_positions = set(range(0,77))-set(positions)\n    \n    positions = positions + list(left_positions)\n    sentence = sentence + [0]*(77-len(sentence))\n        \n    return sentence, positions","c06fa448":"from kaggle_environments import make\nenv = make(\"hungry_geese\")\nobs = env.reset(4)\n\naction_mapping = ['NORTH', 'SOUTH', 'WEST', 'EAST']\nprevious_obs = None\nwhile not env.done:\n    actions = []\n    for p in range(4):\n        if obs[p][\"status\"] == \"DONE\":\n            actions.append(\"NORTH\")\n            continue\n        sentence, positions = preprocess_map_obs(obs, previous_obs=previous_obs, p=p)\n\n        sentence = tf.convert_to_tensor(sentence, dtype=tf.int64)\n        positions = tf.convert_to_tensor(positions, dtype=tf.int64)\n        preds = model_inf.call(tf.expand_dims(sentence,0),tf.expand_dims(positions,0))\n        pred = tf.math.argmax(preds,1).numpy()[0]\n\n        actions.append(action_mapping[pred])\n            \n    previous_obs = obs\n    obs = env.step(actions)","342e7975":"env.render(mode=\"ipython\", width=800, height=700)","ec678f3c":"First part, when preprocessing is done, is accessible here : [first part](https:\/\/www.kaggle.com\/josephamigo\/hungry-geese-is-a-nlp-problem-part-1)\n\nHere you can find information about TPU in kaggle : [TPU in kaggle](https:\/\/www.kaggle.com\/docs\/tpu)\n\nHere what really made a difference was to use relative embedding as yuricat suggested and use a very small learning rate.\n\nThanks to yuricat for his great suggestion!!","a8860539":"# Creation of the neural network","b6f8c0f3":"# Utils functions","7af2b54e":"# Training","51caae62":"# Testing !","6b56fa66":"We will do classification so we'll add a class token","ea9b2065":"# TPU initialization and dataset loading","58e6b12b":"# Saving model locally and loading it"}}