{"cell_type":{"10fa95b5":"code","b811554e":"code","2ac67c87":"code","85d1ea34":"code","126174ad":"code","e2cd97e3":"code","8221da0f":"code","fa51a154":"code","d3b5e250":"code","6151ce81":"code","6c2c8800":"code","cb8466f1":"code","19666e1e":"code","9d669303":"code","bdf00889":"code","a93aac86":"code","2d2b417e":"code","36b65251":"code","d57382ea":"code","d0940029":"code","d8b6515c":"code","b689f6b0":"code","699ce6b2":"code","dd472195":"code","9bbd63af":"code","a23f46d0":"code","85ba300e":"code","89767332":"code","13303418":"code","7331aa44":"code","06e21fc0":"code","0fbf32ad":"code","951c048c":"code","09e52483":"code","e53b0bb8":"code","1bdbcf37":"code","7359872a":"code","afd1ef34":"code","82f00acf":"code","205e3343":"code","a2cb6e94":"code","57888798":"code","bb93cb4b":"code","342798a7":"code","f2721a98":"code","4f3c870a":"code","6c3e57e6":"code","03043a1f":"code","d5407a8d":"code","26e18504":"code","469f3b9b":"code","66bd1f38":"code","afd771e7":"code","498cabd5":"code","ce6a8d70":"code","223835d8":"code","13170557":"code","6486963d":"code","dc9c6f60":"code","a3d0f8f1":"code","3083a702":"code","b7cd33a2":"code","8f1b4311":"code","c10935df":"code","644231c1":"markdown","4b478973":"markdown","b261362b":"markdown","41c39a16":"markdown","c3cc6fb7":"markdown","8b88b779":"markdown","caf23552":"markdown","ce37a32a":"markdown","12b7f9b0":"markdown","06ea516b":"markdown","9df24c3e":"markdown","ed2ef54e":"markdown","41b7b36d":"markdown","9da4f0dc":"markdown","eeb9a253":"markdown","1a0a7691":"markdown","1a156d7c":"markdown","b3553021":"markdown","478c2670":"markdown","8707d31f":"markdown","9e5ca5cf":"markdown","8c989779":"markdown","f7394f70":"markdown","9e037cc3":"markdown","a6878b60":"markdown","8678b651":"markdown","b40f945f":"markdown","298aae71":"markdown","1d2e2e25":"markdown","68f96b62":"markdown","42e0f812":"markdown","940a930e":"markdown","2a616229":"markdown","e9df0406":"markdown","d498d57c":"markdown","ff710fc7":"markdown","5c0a59d9":"markdown","48afa672":"markdown","c0df05f3":"markdown","568d55b6":"markdown","6c40449c":"markdown","38a54472":"markdown","d394cf1d":"markdown","6fcb0d9f":"markdown","1102ddb5":"markdown","3b3c63a0":"markdown","fefc0e1a":"markdown","fadfd8ee":"markdown","0d2519be":"markdown"},"source":{"10fa95b5":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nfrom gensim.models import KeyedVectors\nimport operator\nfrom tqdm import tqdm\ntqdm.pandas()\nimport gc","b811554e":"epochs=25\nbatch_size=128\nmax_words=100000\nmax_seq_size=256","2ac67c87":"import os\nprint(os.listdir(\"..\/input\")) \nprint(os.listdir(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\"))\nprint(os.listdir(\"..\/input\/quoratextemb\"))\nprint(os.listdir(\"..\/input\/quoratextemb\/embeddings\"))","85d1ea34":"train_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest_df  = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\nsub_df   = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')","126174ad":"train_df.shape","e2cd97e3":"test_df.shape","8221da0f":"train_df.head()","fa51a154":"test_df.head()","d3b5e250":"sub_df.head()","6151ce81":"mem_usg = train_df.memory_usage().sum() \/ 1024**2 \nprint(\"Memory usage is: \", mem_usg, \" MB\")","6c2c8800":"train_df = train_df[[\"target\", \"comment_text\"]]\nmem_usg = train_df.memory_usage().sum() \/ 1024**2 \nprint(\"Memory usage is: \", mem_usg, \" MB\")","cb8466f1":"# Use for combine the vector files that have given \ndef combine_embedding(vec_files):\n    \n    # convert victor to float16 to make it use less memory\n    def get_coefs(word, *arr): \n        return word, np.asarray(arr, dtype='float16')\n\n    # make our embed smaller by get_coefs\n    def optimize_embedding(embedding): \n        optimized_embedding = {}\n        for word in embedding.vocab:\n            optimized_embedding[word] = np.asarray(embedding[word], dtype='float16')\n        return optimized_embedding\n\n    \n    # load embed vector from file\n    def load_embed(file):\n        print(\"Loading {}\".format(file))\n\n        if file == '..\/input\/quoratextemb\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec':\n            return dict(get_coefs(*o.strip().split(\" \")) for o in open(file) if len(o) > 100)\n        \n        elif file == '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec':\n            return optimize_embedding(KeyedVectors.load_word2vec_format(file))\n\n        else:\n            return dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n        \n        \n    combined_embedding = {}\n    for file in vec_files:\n        combined_embedding.update(load_embed(file))\n    return combined_embedding","19666e1e":"vec_files = [\n    \"..\/input\/quoratextemb\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec\",\n    \"..\/input\/quoratextemb\/embeddings\/glove.840B.300d\/glove.840B.300d.txt\",\n    \"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\"\n]","9d669303":"embedding_index = combine_embedding(vec_files)\ncovered_vocabs = set(list(embedding_index.keys()))\nembedding_index.clear()","bdf00889":"gc.collect()","a93aac86":"# Use for count how many time word accure in our data\ndef count_words_from(series):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    sentences =  series.str.split()\n    vocab = {}\n    for sentence in tqdm(sentences):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","2d2b417e":"# Use for check coverage of our vocab\ndef check_coverage_for(vocab):\n    a = 0\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        if word in covered_vocabs:\n            a += 1\n            k += vocab[word]\n        else:\n            oov[word] = vocab[word]\n            i += vocab[word]\n\n    print('Found embeddings for {:.2%} of vocab'.format(a \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    \n    return sorted_x","36b65251":"# this methods help to clean up some memory while improve the coverage. As it will release the varaible the send of method\ndef check_current_coverage(num=50):\n    vocab = count_words_from( train_df[\"comment_text\"] )\n    coverage = check_coverage_for(vocab)\n    return coverage[:num]","d57382ea":"check_current_coverage()","d0940029":"contraction_mapping = {\n    \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n    \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \n    \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n    \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n    \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n    \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n    \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n    \"Trump's\": \"trump is\", \"Obama's\": \"obama is\", \"Canada's\": \"canada is\", \"today's\": \"today is\"\n}","d8b6515c":"known_contractions = []\nfor contract in contraction_mapping:\n    if contract in covered_vocabs:\n        known_contractions.append(contract)\nprint(known_contractions)","b689f6b0":"for cont in known_contractions:\n    contraction_mapping.pop(cont)","699ce6b2":"def clean_contractions(text):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    words = [contraction_mapping[word] if word in contraction_mapping else word for word in text.split(\" \")]\n    return ' '.join(words)","dd472195":"train_df[\"comment_text\"] = train_df[\"comment_text\"].progress_apply(lambda text: clean_contractions(text))","9bbd63af":"check_current_coverage()","a23f46d0":"punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'","85ba300e":"specail_signs = { \"\u2026\": \"...\", \"\u2082\": \"2\"}","89767332":"unknown_puncts = []\nfor p in punct:\n    if p not in covered_vocabs:\n        unknown_puncts.append(p)\nprint(' '.join(unknown_puncts))","13303418":"def clean_special_chars(text):\n    for s in specail_signs: \n        text = text.replace(s, specail_signs[s])\n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    return text","7331aa44":"train_df[\"comment_text\"] = train_df[\"comment_text\"].progress_apply(lambda text: clean_special_chars(text))","06e21fc0":"check_current_coverage()","0fbf32ad":"special_caps_mapping = { \n    \"\u1d00\": \"a\", \"\u0299\": \"b\", \"\u1d04\": \"c\", \"\u1d05\": \"d\", \"\u1d07\": \"e\", \"\u0493\": \"f\", \"\u0262\": \"g\", \"\u029c\": \"h\", \"\u026a\": \"i\", \"\u1d0a\": \"j\", \"\u1d0b\": \"k\", \"\u029f\": \"l\", \"\u1d0d\": \"m\",\n    \"\u0274\": \"n\", \"\u1d0f\": \"o\", \"\u1d18\": \"p\", \"\u01eb\": \"q\", \"\u0280\": \"r\", \"s\": \"s\", \"\u1d1b\": \"t\", \"\u1d1c\": \"u\", \"\u1d20\": \"v\", \"\u1d21\": \"w\", \"x\": \"x\", \"\u028f\": \"y\", \"\u1d22\": \"z\",\n    \"\ud835\ude0a\": \"C\", \"\ud835\ude26\": \"e\", \"\ud835\ude33\": \"r\", \"\ud835\ude22\": \"a\", \"\ud835\ude35\": \"t\", \"\ud835\ude30\": \"o\", \"\ud835\ude24\": \"c\", \"\ud835\ude3a\": \"y\", \"\ud835\ude34\": \"s\", \"\ud835\ude2a\": \"i\", \"\ud835\ude27\": \"f\", \"\ud835\ude2e\": \"m\", \"\ud835\ude23\": \"b\",\n    \"\u043c\": \"m\", \"\u03c5\": \"u\", \"\u0442\": \"t\", \"\u0455\": \"s\", \"\ud835\ude40\": \"E\", \"\ud835\udc9b\": \"z\", \"\ud835\udc72\": \"K\", \"\ud835\udc73\": \"L\", \"\ud835\udc7e\": \"W\", \"\ud835\udc8b\": \"j\", \"\ud835\udfd2\": \"4\",\n    \"\ud835\ude52\": \"W\", \"\ud835\ude3e\": \"C\", \"\ud835\ude3d\": \"B\", \"\ud835\udc71\": \"J\", \"\ud835\udc79\": \"R\", \"\ud835\udc6b\": \"D\", \"\ud835\udc75\": \"N\", \"\ud835\udc6a\": \"C\", \"\ud835\udc6f\": \"H\", \"\ud835\udc92\": \"q\", \"\ud835\udc6e\": \"G\", \"\ud835\uddd5\": \"B\", \"\ud835\uddf4\": \"g\", \n    \"\ud835\udfd0\": \"2\", \"\ud835\uddf8\": \"k\", \"\ud835\udddf\": \"L\", \"\ud835\udde0\": \"M\", \"\ud835\uddf7\": \"j\", \"\ud835\udc0e\": \"O\", \"\ud835\udc0d\": \"N\", \"\ud835\udc0a\": \"K\", \"\ud835\udc6d\": \"F\", \"\u0415\": \"E\"\n}\n","951c048c":"def clean_small_caps(text):\n    for char in special_caps_mapping:\n        text = text.replace(char, special_caps_mapping[char])\n    return text","09e52483":"train_df[\"comment_text\"] = train_df[\"comment_text\"].progress_apply(lambda text: clean_small_caps(text))","e53b0bb8":"check_current_coverage()","1bdbcf37":"emojis = \"\ud83c\udf55\ud83d\udc35\ud83d\ude11\ud83d\ude22\ud83d\udc36\ufe0f\ud83d\ude1c\ud83d\ude0e\ud83d\udc4a\ud83d\ude01\ud83d\ude0d\ud83d\udc96\ud83d\udcb5\ud83d\udc4e\ud83d\ude00\ud83d\ude02\ud83d\udd25\ud83d\ude04\ud83c\udffb\ud83d\udca5\ud83d\ude0b\ud83d\udc4f\ud83d\ude31\ud83d\ude8c\u1d35\u035e\ud83c\udf1f\ud83d\ude0a\ud83d\ude33\ud83d\ude27\ud83d\ude40\ud83d\ude10\ud83d\ude15\ud83d\udc4d\ud83d\ude2e\ud83d\ude03\ud83d\ude18\ud83d\udca9\ud83d\udcaf\u26fd\ud83d\ude84\ud83d\ude16\ud83c\udffc\ud83d\udeb2\ud83d\ude1f\ud83d\ude08\ud83d\udcaa\ud83d\ude4f\ud83c\udfaf\ud83c\udf39\ud83d\ude07\ud83d\udc94\ud83d\ude21\ud83d\udc4c\ud83d\ude44\ud83d\ude20\ud83d\ude09\ud83d\ude24\u26fa\ud83d\ude42\ud83d\ude0f\ud83c\udf7e\ud83c\udf89\ud83d\ude1e\ud83c\udffe\ud83d\ude05\ud83d\ude2d\ud83d\udc7b\ud83d\ude25\ud83d\ude14\ud83d\ude13\ud83c\udffd\ud83c\udf86\ud83c\udf7b\ud83c\udf7d\ud83c\udfb6\ud83c\udf3a\ud83e\udd14\ud83d\ude2a\ud83d\udc30\ud83d\udc07\ud83d\udc31\ud83d\ude46\ud83d\ude28\ud83d\ude43\ud83d\udc95\ud83d\udc97\ud83d\udc9a\ud83d\ude48\ud83d\ude34\ud83c\udfff\ud83e\udd17\ud83c\uddfa\ud83c\uddf8\u2935\ud83c\udfc6\ud83c\udf83\ud83d\ude29\ud83d\udc6e\ud83d\udc99\ud83d\udc3e\ud83d\udc15\ud83d\ude06\ud83c\udf20\ud83d\udc1f\ud83d\udcab\ud83d\udcb0\ud83d\udc8e\ud83d\udd90\ud83d\ude45\u26f2\ud83c\udf70\ud83e\udd10\ud83d\udc46\ud83d\ude4c\ud83d\udc9b\ud83d\ude41\ud83d\udc40\ud83d\ude4a\ud83d\ude49\ud83d\udeac\ud83e\udd13\ud83d\ude35\ud83d\ude12\u035d\ud83c\udd95\ud83d\udc45\ud83d\udc65\ud83d\udc44\ud83d\udd04\ud83d\udd24\ud83d\udc49\ud83d\udc64\ud83d\udc76\ud83d\udc72\ud83d\udd1b\ud83c\udf93\ud83d\ude23\u23fa\ud83d\ude0c\ud83e\udd11\ud83c\udf0f\ud83d\ude2f\ud83d\ude32\ud83d\udc9e\ud83d\ude93\ud83d\udd14\ud83d\udcda\ud83c\udfc0\ud83d\udc50\ud83d\udca4\ud83c\udf47\ud83c\udfe1\u2754\u2049\ud83d\udc60\u300b\ud83c\uddf9\ud83c\uddfc\ud83c\udf38\ud83c\udf1e\ud83c\udfb2\ud83d\ude1b\ud83d\udc8b\ud83d\udc80\ud83c\udf84\ud83d\udc9c\ud83e\udd22\u0650\u064e\ud83d\uddd1\ud83d\udc83\ud83d\udce3\ud83d\udc7f\u0f3c\u3064\u0f3d\ud83d\ude30\ud83e\udd23\ud83d\udc1d\ud83c\udf85\ud83c\udf7a\ud83c\udfb5\ud83c\udf0e\u035f\ud83e\udd21\ud83e\udd25\ud83d\ude2c\ud83e\udd27\ud83d\ude80\ud83e\udd34\ud83d\ude1d\ud83d\udca8\ud83c\udfc8\ud83d\ude3a\ud83c\udf0d\u23cf\u1ec7\ud83c\udf54\ud83d\udc2e\ud83c\udf41\ud83c\udf46\ud83c\udf51\ud83c\udf2e\ud83c\udf2f\ud83e\udd26\ud83c\udf40\ud83d\ude2b\ud83e\udd24\ud83c\udfbc\ud83d\udd7a\ud83c\udf78\ud83e\udd42\ud83d\uddfd\ud83c\udf87\ud83c\udf8a\ud83c\udd98\ud83e\udd20\ud83d\udc69\ud83d\udd92\ud83d\udeaa\ud83c\uddeb\ud83c\uddf7\ud83c\udde9\ud83c\uddea\ud83d\ude37\ud83c\udde8\ud83c\udde6\ud83c\udf10\ud83d\udcfa\ud83d\udc0b\ud83d\udc98\ud83d\udc93\ud83d\udc90\ud83c\udf0b\ud83c\udf04\ud83c\udf05\ud83d\udc7a\ud83d\udc37\ud83d\udeb6\ud83e\udd18\u0366\ud83d\udcb8\ud83d\udc42\ud83d\udc43\ud83c\udfab\ud83d\udea2\ud83d\ude82\ud83c\udfc3\ud83d\udc7d\ud83d\ude19\ud83c\udfbe\ud83d\udc79\u238c\ud83c\udfd2\u26f8\ud83c\udfc4\ud83d\udc00\ud83d\ude91\ud83e\udd37\ud83e\udd19\ud83d\udc12\ud83d\udc08\ufdfb\ud83e\udd84\ud83d\ude97\ud83d\udc33\ud83d\udc47\u26f7\ud83d\udc4b\ud83e\udd8a\ud83d\udc3d\ud83c\udfbb\ud83c\udfb9\u26d3\ud83c\udff9\ud83c\udf77\ud83e\udd86\u267e\ud83c\udfb8\ud83e\udd15\ud83e\udd12\u26d1\ud83c\udf81\ud83c\udfdd\ud83e\udd81\ud83d\ude4b\ud83d\ude36\ud83d\udd2b\ud83d\udc41\ud83d\udcb2\ud83d\uddef\ud83d\udc51\ud83d\udebf\ud83d\udca1\ud83d\ude26\ud83c\udfd0\ud83c\uddf0\ud83c\uddf5\ud83d\udc7e\ud83d\udc04\ud83c\udf88\ud83d\udd28\ud83d\udc0e\ud83e\udd1e\ud83d\udc38\ud83d\udc9f\ud83c\udfb0\ud83c\udf1d\ud83d\udef3\ud83c\udf6d\ud83d\udc63\ud83c\udfc9\ud83d\udcad\ud83c\udfa5\ud83d\udc34\ud83d\udc68\ud83e\udd33\ud83e\udd8d\ud83c\udf69\ud83d\ude17\ud83c\udfc2\ud83d\udc73\ud83c\udf57\ud83d\udd49\ud83d\udc32\ud83c\udf52\ud83d\udc11\u23f0\ud83d\udc8a\ud83c\udf24\ud83c\udf4a\ud83d\udd39\ud83e\udd1a\ud83c\udf4e\ud835\udc77\ud83d\udc02\ud83d\udc85\ud83d\udca2\ud83d\udc92\ud83d\udeb4\ud83d\udd95\ud83d\udda4\ud83e\udd58\ud83d\udccd\ud83d\udc48\u2795\ud83d\udeab\ud83c\udfa8\ud83c\udf11\ud83d\udc3b\ud83e\udd16\ud83c\udf8e\ud83d\ude3c\ud83d\udd77\ud83d\udc7c\ud83d\udcc9\ud83c\udf5f\ud83c\udf66\ud83c\udf08\ud83d\udd2d\u300a\ud83d\udc0a\ud83d\udc0d\ud83d\udc26\ud83d\udc21\ud83d\udcb3\u1f31\ud83d\ude47\ud83e\udd5c\ud83d\udd3c\"","7359872a":"def remove_emojis(text):\n    for emoji in emojis:\n        text = text.replace(emoji, '')\n    return text","afd1ef34":"train_df[\"comment_text\"] = train_df[\"comment_text\"].progress_apply(lambda text: remove_emojis(text))","82f00acf":"check_current_coverage()","205e3343":"del covered_vocabs\ngc.collect()","a2cb6e94":"def clean_up_text_with_all_process(text):\n    text = text.lower()\n    text = clean_contractions(text)\n    text = clean_special_chars(text)\n    text = clean_small_caps(text)\n    return text","57888798":"test_df[\"comment_text\"] = test_df[\"comment_text\"].progress_apply(lambda text: clean_up_text_with_all_process(text))","bb93cb4b":"tranformer = Tokenizer(lower = True, filters='', num_words=max_words)\ntranformer.fit_on_texts( list(train_df[\"comment_text\"].values) + list(test_df[\"comment_text\"].values) )","342798a7":"transformed_x = tranformer.texts_to_sequences(train_df[\"comment_text\"].values)\ntransformed_x = pad_sequences(transformed_x, maxlen = max_seq_size)","f2721a98":"x_predict = tranformer.texts_to_sequences(test_df[\"comment_text\"])\nx_predict = pad_sequences(x_predict, maxlen = max_seq_size)","4f3c870a":"def build_embedding_matrix(word_index, total_vocab, embedding_size):\n    embedding_index = combine_embedding(vec_files)\n    matrix = np.zeros((total_vocab, embedding_size))\n    for word, index in tqdm(word_index.items()):\n        try:\n            matrix[index] = embedding_index[word]\n        except KeyError:\n            pass\n    return matrix","6c3e57e6":"word_index = tranformer.word_index\ntotal_vocab = len(word_index) + 1\nembedding_size = 300\nembedding_matrix = build_embedding_matrix(tranformer.word_index, total_vocab, embedding_size)","03043a1f":"del tranformer\ndel word_index\ndel embedding_index\ngc.collect()","d5407a8d":"y = (train_df['target'].values > 0.5).astype(int)\nx_train, x_test, y_train, y_test = train_test_split(transformed_x, y, random_state=10, test_size=0.15)","26e18504":"del train_df\ndel y\ndel test_df\ndel transformed_x\ngc.collect()","469f3b9b":"from tensorflow.nn import relu, sigmoid\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom tensorflow.keras.layers import CuDNNGRU, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, Conv1D","66bd1f38":"sequence_input = Input(shape=(max_seq_size,), dtype='int32')\nembedding_layer = Embedding(total_vocab,\n                            embedding_size,\n                            weights=[embedding_matrix],\n                            input_length=max_seq_size,\n                            trainable=False)\n\nx_layer = embedding_layer(sequence_input)\nx_layer = SpatialDropout1D(0.2)(x_layer)\nx_layer = Bidirectional(CuDNNGRU(64, return_sequences=True))(x_layer)   \nx_layer = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_layer)\n\navg_pool1 = GlobalAveragePooling1D()(x_layer)\nmax_pool1 = GlobalMaxPooling1D()(x_layer)     \n\nx_layer = concatenate([avg_pool1, max_pool1])\n\npreds = Dense(1, activation=sigmoid)(x_layer)\n\nmodel = Model(sequence_input, preds)\nmodel.summary()","afd771e7":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","498cabd5":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint","ce6a8d70":"callbacks = [\n    EarlyStopping(patience=10, verbose=1),\n    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n    ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)\n]","223835d8":"history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), callbacks=callbacks)","13170557":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\nax1.plot(history.history['loss'], color='b', label=\"Training loss\")\nax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\nax1.set_xticks(np.arange(1, epochs, 1))\nplt.legend(loc='best', shadow=True)\n\nax2.plot(history.history['acc'], color='b', label=\"Training accuracy\")\nax2.plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\nax2.set_xticks(np.arange(1, epochs, 1))\nplt.legend(loc='best', shadow=True)\nplt.tight_layout()\nplt.show()","6486963d":"score = model.evaluate(x_test, y_test, batch_size=batch_size)","dc9c6f60":"print('Test loss:', score[0])\nprint('Test accuracy:', score[1])","a3d0f8f1":"y_predict = model.predict(x_predict)","3083a702":"print(y_predict)","b7cd33a2":"sub_df[\"prediction\"] = y_predict","8f1b4311":"sub_df.head()","c10935df":"sub_df.to_csv(\"submission.csv\", index=False)","644231c1":"# Callbacks","4b478973":"# Data Cleaning\n","b261362b":"Check coverage again","41c39a16":"To increase our covarage we try to combine few embedding together in order for us to more vocab covrage. \nIn term of memory optimize, we will convert our vector to ```float16``` to reduce some memory usage. ","c3cc6fb7":"### Let's check the first coverage ","8b88b779":"### Clean contractions","caf23552":"Let free up some memory before to other hard job. I'll clean ```vocab``` and ```coverage``` up in order for us to have enough memory to continue","ce37a32a":"# Let's clean up of testing set","12b7f9b0":"This is the excited part that we have our coverage increase much. Our vocab coverage has increase to 65% and Most of our text (99.6%) has been covered","06ea516b":"All contraction are known","9df24c3e":"Our covarage have increase a little. But at least we are good for next step. From the top uncovered we see that we have problem with some specials charator. ","ed2ef54e":"# Build Model","41b7b36d":"### Count occurance of words ","9da4f0dc":"This kernel response to competitiion [Jigsaw Unintended Bias in Toxicity Classification](https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification). It is my first hand on **Text Classification**. I will use Tensorflow and Keras for this project.","eeb9a253":"#### Clean special characters","1a0a7691":"We see some like like ```\u029c\u1d0f\u1d0d\u1d07```, ```\u1d1c\u1d18```, ```\u1d04\u029c\u1d07\u1d04\u1d0b``` etc ... We need to convert it to up, home, check ....","1a156d7c":"# Import Libraries","b3553021":"Let clean memory again,  I'll clean ```word_index``` and ```embedding_index``` up in order for us to have enough memory for training","478c2670":"# Train Model","8707d31f":"See? we have more free memory","9e5ca5cf":"# Evaluate Model","8c989779":"# Clean Emoji ","f7394f70":"# Transform Text ","9e037cc3":"# Clean up some memory","a6878b60":"# Clean Unprocessable Symbols","8678b651":"# Explore Data","b40f945f":"* Convert to lower case\n* Clean contractions\n* Clean special charactor\n* Convert small caps","298aae71":"Do some clean up for memory","1d2e2e25":"We see only 18% of our vocab has been covered. But 92% of our text has already cover. From the top first uncovered we see we have some problem with contractions. Let's get rid of it. ","68f96b62":"Let check coverage again","42e0f812":"Transform training set","940a930e":"# Build Martix","2a616229":"# Submission","e9df0406":"Our embedding have known some contractions. So we will remove that known contractions from our dictionary and let's our embedding handle it. ","d498d57c":"Transform predicting set","ff710fc7":"# Select features and Target","5c0a59d9":"# Read Data","48afa672":"I'll select only the columns that we need to reduce some memory usage","c0df05f3":"### Check Coverage","568d55b6":"# Virtualize Training","6c40449c":"# Take care of dataframe memory ","38a54472":"# Clean Special Caps","d394cf1d":"I have built few great kernel for beginner with deep learning that you can check it out: \n* https:\/\/www.kaggle.com\/uysimty\/keras-cnn-dog-or-cat-classification\n* https:\/\/www.kaggle.com\/uysimty\/learn-titanic-survival\n* https:\/\/www.kaggle.com\/uysimty\/keras-predict-google-stock-using-lstm","6fcb0d9f":"### Load Embedding","1102ddb5":"# Define Variables","3b3c63a0":"# Prediction","fefc0e1a":"**Credit**\n* https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-text-preprocessing-v2\n* https:\/\/www.kaggle.com\/thousandvoices\/simple-lstm","fadfd8ee":"# Compile Model","0d2519be":"Because of our analyst part taking too long. So it is disable me from submit to compotition. So I have other kernel that take only processing part which return 92.3 acurracy score.\n\n* https:\/\/www.kaggle.com\/uysimty\/simple-toxicity-classification-submission"}}