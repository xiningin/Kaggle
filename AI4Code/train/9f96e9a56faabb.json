{"cell_type":{"929782d7":"code","613d532e":"code","242d6bdb":"code","eddfc123":"code","0cc57e24":"code","09775ced":"code","19fa779a":"code","6458d7c1":"code","51bf8f00":"code","c20cc950":"code","50c0ab75":"code","26fa37ec":"code","a0472676":"code","4f6fe6e0":"code","d080b481":"code","ad473c34":"code","08b17515":"code","7266eb2e":"code","6271a5bc":"code","7a814c69":"code","1521fd78":"code","39354af5":"code","4e337ab7":"code","93a363a9":"code","7977b0bb":"code","933962a9":"code","839ca093":"code","1d87a527":"code","27dfcacb":"code","2d0a3bc0":"code","81fffc14":"code","9f7636ed":"code","1fe8d8b5":"code","9d5fdfb6":"code","dac4a0d8":"code","7142d0cb":"code","e8ffa4ef":"code","4f99218c":"code","4f19bb77":"code","7f8a90f0":"code","b8904324":"code","c52bfa4d":"code","15e45cb4":"code","04486514":"code","04ee6676":"code","ed3993fb":"code","3b8085cf":"code","241532eb":"code","acfc5f04":"code","d25952df":"code","963b8472":"code","d714aee7":"code","d602315d":"code","2d2e3b3c":"code","45e901c2":"code","832ded85":"code","29ce1b88":"code","d85dc21d":"code","3487fbed":"code","ac12a799":"code","51cdfeb8":"code","d87480d8":"code","8fa4f7e5":"code","c0559ae8":"code","cda398b0":"code","6b081269":"code","b0079862":"code","e7941a2c":"code","e5273d9d":"code","707a71f8":"code","51ed6526":"code","6364da95":"code","12f0e49c":"markdown","70783a0a":"markdown","55e62a5d":"markdown","b2604e5d":"markdown","a611448d":"markdown","ddce1fea":"markdown","575bcef6":"markdown","d74ada56":"markdown"},"source":{"929782d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","613d532e":"df_train = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip')\nprint(df_train.isnull().sum()) \nprint(df_train.info())\ndisplay(df_train.head(5))","242d6bdb":"df_test = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip')\nprint(df_test.isnull().sum()) \nprint(df_test.info())\ndisplay(df_test.head(5))","eddfc123":"df_stores = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv')\nprint(df_stores.isnull().sum()) \nprint(df_stores.info())\ndisplay(df_stores.head(5))","0cc57e24":"display(df_stores['Type'].drop_duplicates())","09775ced":"df_features = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip')\nprint(df_features.isnull().sum()) \nprint(df_features.info())\ndisplay(df_features.head(5))","19fa779a":"df_sample = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip')\nprint(df_sample.isnull().sum())\ndf_sample.info()\ndf_sample.head(5)","6458d7c1":"print(\"df_sample:\\t{}\".format(df_sample.shape))\nprint(\"df_features:\\t{}\".format(df_features.shape))\nprint(\"df_stores:\\t{}\".format(df_stores.shape))\nprint(\"df_test:\\t{}\".format(df_test.shape))\nprint(\"df_train:\\t{}\".format(df_train.shape))","51bf8f00":"# feature\uc640 store, train\uc744 \uc870\ud569\ud558\uc5ec \ud559\uc2b5\uc744 \ud558\uae30 \uc704\ud55c \ub370\uc774\ud130\ud504\ub808\uc784 \ub9cc\ub4e4\uae30\ndf_features_stores = pd.merge(df_features, df_stores, how = \"left\")\ndf_features_stores.info()","c20cc950":"df_features_stores.isnull().sum()","50c0ab75":"print(\"MarkDown1 null \ube44\uc728\\t{:.2f}%\".format(((df_features_stores['MarkDown1'].isnull().sum())\/len(df_features_stores))*100))\nprint(\"MarkDown2 null \ube44\uc728\\t{:.2f}%\".format(((df_features_stores['MarkDown2'].isnull().sum())\/len(df_features_stores))*100))\nprint(\"MarkDown3 null \ube44\uc728\\t{:.2f}%\".format(((df_features_stores['MarkDown3'].isnull().sum())\/len(df_features_stores))*100))\nprint(\"MarkDown4 null \ube44\uc728\\t{:.2f}%\".format(((df_features_stores['MarkDown4'].isnull().sum())\/len(df_features_stores))*100))\nprint(\"MarkDown5 null \ube44\uc728\\t{:.2f}%\".format(((df_features_stores['MarkDown5'].isnull().sum())\/len(df_features_stores))*100))\n","26fa37ec":"del df_features_stores['MarkDown1']\ndel df_features_stores['MarkDown2']\ndel df_features_stores['MarkDown3']\ndel df_features_stores['MarkDown4']\ndel df_features_stores['MarkDown5']\n\ndf_features_stores.head()","a0472676":"# Type\uc740 \ubc94\uc8fc\ud615 \ub370\uc774\ud130\ub85c \ubcc0\ud658\n\ndf_features_stores.loc[df_features_stores.Type == \"A\", \"Type\"] = 0\ndf_features_stores.loc[df_features_stores.Type == \"B\", \"Type\"] = 1\ndf_features_stores.loc[df_features_stores.Type == \"C\", \"Type\"] = 2\n# df_features_stores.loc[df_features_stores.Type == \"D\", \"Type\"] = 3\n\n\ndf_features_stores.Type = pd.Categorical(df_features_stores.Type)\n# Store\ub3c4 \ubc94\uc8fc\ud615 \ub370\uc774\ud130\ub85c \ubcc0\ud658\ndf_features_stores.Store = pd.Categorical(df_features_stores.Store)\n","4f6fe6e0":"train_total = pd.merge(df_features_stores, df_train, \n                       how = \"inner\", on = ['Store','Date', 'IsHoliday']).sort_values(\n    by=['Store','Dept','Date']).reset_index(drop=True)\n                        # holyday \uc548\ud574\uc8fc\uba74 holyday_x y \uc0dd\uae40\n\ntest_total = pd.merge(df_features_stores, df_test, \n                      how = \"inner\", on = ['Store','Date', 'IsHoliday']).sort_values(\n    by=['Store','Dept','Date']).reset_index(drop=True)","d080b481":"print(train_total.info(), \"\\n\")\nprint(test_total.info(), \"\\n\")","ad473c34":"train_total.Date = pd.to_datetime(train_total.Date)\ntest_total.Date = pd.to_datetime(test_total.Date)\n\ntrain_total['Week'] = train_total.Date.dt.week # \uc8fc\uac04 \ud310\ub9e4\ub7c9 \uc608\uce21\uc774\ubbc0\ub85c, \uc8fc\uac04 \uc815\ubcf4 \uc0dd\uc131\ntest_total['Week'] = test_total.Date.dt.week\n\n\n# Week \ubc94\uc8fc\ud615 \ub370\uc774\ud130\ub85c \ubcc0\ud658\ntrain_total.Week = pd.Categorical(train_total.Week)\ntest_total.Week = pd.Categorical(test_total.Week)","08b17515":"train_total.describe()","7266eb2e":"test_total.describe()","6271a5bc":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import rcParams","7a814c69":"# df_visualization = train_total.copy()\n# df_visualization.info()","1521fd78":"# # store, Dept\ub294 \ubc94\uc8fc\ud615 \ub370\uc774\ud130\uc784!\n# df_visualization.Store = pd.Categorical(df_visualization.Store)\n# df_visualization.Dept = pd.Categorical(df_visualization.Dept)\n# df_visualization.Type = pd.Categorical(df_visualization.Type)\n# df_visualization.Date = pd.to_datetime(df_visualization.Date)\n\n# df_visualization.set_index(keys = \"Date\", inplace = True)\n# df_visualization.info()","39354af5":"# df_visualization.head(3)","4e337ab7":"# rcParams['figure.figsize'] = 10,6.0\n# sns.lineplot(x = df_visualization.index, y = \"Weekly_Sales\", data = df_visualization)\n\n# # \ub9e4\ud574 \uc5f0\ub9d0\uc5d0 \uce58\uc19f\ub294 \ud328\ud134\uc744 \ubcfc \uc218 \uc788\uc74c.\n# # 0, 1, 2 => A, B, C","93a363a9":"# rcParams['figure.figsize'] = 10,6.0\n# sns.lineplot(x = df_visualization.index, y = \"Weekly_Sales\", data = df_visualization, hue = 'Type')\n\n# # \ub9e4\ud574 \uc5f0\ub9d0\uc5d0 \uce58\uc19f\ub294 \ud328\ud134\uc744 \ubcfc \uc218 \uc788\uc74c.\n# # 0, 1, 2 => A, B, C","7977b0bb":"# rcParams['figure.figsize'] = 10,6.0\n# sns.lineplot(x = df_visualization.index, y = \"Weekly_Sales\", data = df_visualization, hue = 'Dept')\n\n# # \ub300\ubd80\ubd84\uc758 \ubd80\uc11c\ub3c4 \uc774\ub97c \ub530\ub974\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc74c.","933962a9":"# rcParams['figure.figsize'] = 10,6.0\n# sns.lineplot(x = df_visualization.index, y = \"Weekly_Sales\", data = df_visualization, hue = 'IsHoliday')\n\n# ##### False\uc77c\ub54c\ub294 \uc55e\uc758 \uc2dc\uac01\ud654 \uacb0\uacfc\ucc98\ub7fc \uc5f0\ub9d0\uc5d0 \ud280\uc9c0\ub9cc, True\uc77c \ub54c\uc5d0\ub294 Thanksgiving\ub0a0 \ud30d \ud290 => False\ub85c \ubc14\uafd4\uc11c \uc77c\ubc18\ud654\ud558\ub294\uac8c \uc88b\uc744 \uac83 \uac19\ub2e4.\n# ###### **Super Bowl**: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n# ###### **Labor Day**: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n# ###### **Thanksgiving**: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n# ###### **Christmas**: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13","839ca093":"# rcParams['figure.figsize'] = 10,6.0\n# sns.lineplot(x = \"Temperature\", y = \"Weekly_Sales\", data = df_visualization)\n# # \uc544\ubb34\ub7f0 \ud328\ud134\uc774 \uc548\ubcf4\uc784","1d87a527":"# rcParams['figure.figsize'] = 10,6.0\n# sns.lineplot(x = \"Fuel_Price\", y = \"Weekly_Sales\", data = df_visualization)\n\n# # \uc544\ubb34\ub7f0 \ud328\ud134\uc774 \uc548\ubcf4\uc784","27dfcacb":"# rcParams['figure.figsize'] = 10,6.0\n# sns.lineplot(x = \"CPI\", y = \"Weekly_Sales\", data = df_visualization);\n\n# # \uc544\ubb34\ub7f0 \ud328\ud134\uc774 \uc548\ubcf4\uc784","2d0a3bc0":"# rcParams['figure.figsize'] = 10,6.0\n# sns.lineplot(x = \"Unemployment\", y = \"Weekly_Sales\", data = df_visualization);\n\n# # \uc544\ubb34\ub7f0 \ud328\ud134\uc774 \uc548\ubcf4\uc784","81fffc14":"# rcParams['figure.figsize'] = 10,6.0\n# sns.lineplot(x = \"Size\", y = \"Weekly_Sales\", data = df_visualization);\n\n# # \ub300\uccb4\uc801\uc73c\ub85c \uc0ac\uc774\uc988\uac00 \ud06c\uba74 \ub9e4\ucd9c\uc774 \ub192\uc9c0\ub9cc \uc808\ub300\uc801\uc774\uc9c4 \uc54a\ub2e4. -> \uc77c\uad00\uc131\uc774 \uc5c6\ub2e4","9f7636ed":"# # \uc0c1\uad00\uacc4\uc218\n# fig, ax = plt.subplots( figsize=(10,10) )\n\n# # \uc0bc\uac01\ud615 \ub9c8\uc2a4\ud06c\ub97c \ub9cc\ub4e0\ub2e4(\uc704 \ucabd \uc0bc\uac01\ud615\uc5d0 True, \uc544\ub798 \uc0bc\uac01\ud615\u3141\uc5d0 False)\n# mask = np.zeros_like(df_visualization.corr(), dtype=np.bool)\n# mask[np.triu_indices_from(mask)] = False\n\n# sns.heatmap(data = df_visualization.corr(), annot=True, fmt = '.4f', mask = mask, linewidths=.5, cmap='Blues')\n# plt.show()\n\n# # \uc0c1\uad00 \uacc4\uc218\ub294 -1\uacfc +1 \uc0ac\uc774\uc758 \uac12\uc774 \ub429\ub2c8\ub2e4. \n# # \uc808\ub300 \uc0c1\uad00\uc774 1\uc5d0 \ub354 \uac00\uae4c\uc6b8\uc218\ub85d \ub370\uc774\ud130 \uc810\uc774 \ub354 \ubc00\uc811\ud558\uac8c \uc120\uc744 \ud615\uc131\ud569\ub2c8\ub2e4. 0\uc5d0 \uac00\uae4c\uc6b4 \uc0c1\uad00 \uac12\uc740 \uc120\ud615 \uad00\uacc4\uac00 \uc5c6\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.","1fe8d8b5":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,StackingClassifier\nimport lightgbm as lgb\nimport time","9d5fdfb6":"# \uad00\ub828\uc5c6\ub294 feature \uc81c\uc678\ud55c \ub370\uc774\ud130 \uc14b\nx1 = train_total[['Store','Dept','IsHoliday','Size','Week','Type']]\ny1 = train_total['Weekly_Sales']\nx_train1, x_test1, y_train1, y_test1 = train_test_split(x1, y1,test_size=0.2,random_state = 42)","dac4a0d8":"# # \uad00\ub828\uc5c6\ub294 feature \ud3ec\ud568\ud55c \ub370\uc774\ud130 \uc14b\n# x2 = train_total.drop(['Weekly_Sales', 'Date'],axis=1)\n# y2 = train_total['Weekly_Sales']\n# x_train2, x_test2, y_train2, y_test2 = train_test_split(x2, y2,test_size=0.2,random_state = 42)","7142d0cb":"test_total1 = test_total[['Store','Dept','IsHoliday','Size','Week','Type']]\ntest_total2 = test_total.drop(['Date'],axis=1)","e8ffa4ef":"# model = []\n\n# model.append(('LinearRegression', LinearRegression()))  # LinearRegression \ubaa8\ub378 \n# model.append(('DecisionTreeRegressor', DecisionTreeRegressor()))  # DecisionTreeRegressor \ubaa8\ub378\n# model.append(('RandomForestRegressor', RandomForestRegressor()))  # RandomForestRegressor\n# model.append(('lgb.LGBMRegressor', lgb.LGBMRegressor()))  # lgb.LGBMRegressor()","4f99218c":"# print('case1: \uad00\ub828 \uc5c6\ub294 featuue \uc81c\uc678\ud55c \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud559\uc2b5\uc2dc\ud0a4\uae30')\n# for name, m in model:\n#     m.fit(x_train1, y_train1)\n#     print(\"{}\\ttrain_score : {} \\ttest_score: {}\".format(name, m.score(x_train1, y_train1), m.score(x_test1, y_test1)))","4f19bb77":"# print('case2: \uad00\ub828 \uc5c6\ub294 featuue \ud3ec\ud568\ud55c \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud559\uc2b5\uc2dc\ud0a4\uae30')\n# for name, m in model:\n#     m.fit(x_train2, y_train2)\n#     print(\"{}\\ttrain_score : {} \\ttest_score: {}\".format(name, m.score(x_train2, y_train2), m.score(x_test2, y_test2)))","7f8a90f0":"# start = time.time()  # \uc2dc\uc791 \uc2dc\uac04 \uc800\uc7a5\n# print(\"=====train start=======\\n\")\n# # param: \uc544\ubb34\ub807\uac8c\ub098\n\n# model_rf1 = RandomForestRegressor(n_estimators=100, max_samples = 0.4)\n# model_rf1.fit(x_train1, y_train1)\n\n# print(\"done!\\ntime :\", time.time() - start)  # \ud604\uc7ac\uc2dc\uac01 - \uc2dc\uc791\uc2dc\uac04 = \uc2e4\ud589 \uc2dc\uac04","b8904324":"# print(\"\ud6c8\ub828 \uc138\ud2b8 \uc815\ud655\ub3c4: {:.3f}\".format(model_rf1.score(x_train1, y_train1)))\n# print(\"\ud14c\uc2a4\ud2b8 \uc138\ud2b8 \uc815\ud655\ub3c4: {:.3f}\".format(model_rf1.score(x_test1, y_test1)))","c52bfa4d":"# features = x_train1.columns\n# importances = model_rf1.feature_importances_\n# indices = np.argsort(importances)\n\n# plt.title('Feature Importances')\n# plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n# plt.yticks(range(len(indices)), [x_train1.columns[i] for i in indices])\n# plt.xlabel('Relative Importance')\n# plt.show()","15e45cb4":"# start = time.time()  # \uc2dc\uc791 \uc2dc\uac04 \uc800\uc7a5\n# print(\"=====train start=======\\n\")\n# # param: \uc544\ubb34\ub807\uac8c\ub098\n\n# model_rf2 = RandomForestRegressor(n_estimators=100, max_samples = 0.4)\n# model_rf2.fit(x_train2, y_train2)\n\n# print(\"done!\\ntime :\", time.time() - start)  # \ud604\uc7ac\uc2dc\uac01 - \uc2dc\uc791\uc2dc\uac04 = \uc2e4\ud589 \uc2dc\uac04","04486514":"# print(\"\ud6c8\ub828 \uc138\ud2b8 \uc815\ud655\ub3c4: {:.3f}\".format(model_rf2.score(x_train2, y_train2)))\n# print(\"\ud14c\uc2a4\ud2b8 \uc138\ud2b8 \uc815\ud655\ub3c4: {:.3f}\".format(model_rf2.score(x_test2, y_test2)))","04ee6676":"# features = x_train2.columns\n# importances = model_rf2.feature_importances_\n# indices = np.argsort(importances)\n\n# plt.title('Feature Importances')\n# plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n# plt.yticks(range(len(indices)), [x_train2.columns[i] for i in indices])\n# plt.xlabel('Relative Importance')\n# plt.show()","ed3993fb":"# # x_TEST = test_total.drop(['Date'],axis=1)\n# # x_TEST\n\ntest_pred1 = test_total1.copy()\ntest_pred2 = test_total2.copy()","3b8085cf":"# # x_Test data processing \n# # CPI, Unemployment\uc758 nan \uac12\uc774 \uc788\uc5b4\uc11c \uc608\uce21\uc774 \uc548\ub41c\ub2e4 randomforest...\n# # \uc5ec\ub7ec \ubc29\ubc95\uc774 \uc788\uaca0\uc9c0\ub9cc \uc774\uc804 \uac12\uc73c\ub85c \ub300\uccb4\ud558\ub294 \ubc29\ubc95 \uc120\ud0dd\n# # \uc774\uc720\ub294 \uae09\uaca9\ud558\uac8c \ubcc0\ud560 \uc218\uce58\uac00 \uc544\ub2c8\ub77c\uace0 \ud310\ub2e4.\n\n# test_pred1 = test_pred1.fillna(method='ffill')\n# test_pred2 = test_pred2.fillna(method='ffill')\n# test_pred2.info()","241532eb":"# # model_rf\n# start = time.time()  # \uc2dc\uc791 \uc2dc\uac04 \uc800\uc7a5\n\n# print(\"===== predict start! =======\\n\")\n# predict_sales1 = model_rf1.predict(test_pred1)\n\n# print(\"========== done! ===========\\n\")\n# print(\"time :\", time.time() - start)  # \ud604\uc7ac\uc2dc\uac01 - \uc2dc\uc791\uc2dc\uac04 = \uc2e4\ud589 \uc2dc\uac04","acfc5f04":"# df_sample1 = df_sample.copy()\n# df_sample1['Weekly_Sales'] = predict_sales1\n# df_sample1","d25952df":"# # model_rf\n# start = time.time()  # \uc2dc\uc791 \uc2dc\uac04 \uc800\uc7a5\n\n# print(\"===== predict start! =======\\n\")\n# predict_sales2 = model_rf2.predict(test_pred2)\n\n# print(\"========== done! ===========\\n\")\n# print(\"time :\", time.time() - start)  # \ud604\uc7ac\uc2dc\uac01 - \uc2dc\uc791\uc2dc\uac04 = \uc2e4\ud589 \uc2dc\uac04","963b8472":"pred_df = train_total[['Date', 'Weekly_Sales']].copy()\npred_df","d714aee7":"# pred_df2 = df_test.copy()\n# pred_df2['Weekly_Sales'] = predict_sales2\n# pred_df2 = pred_df2[['Date', 'Weekly_Sales']]\n# pred_df2","d602315d":"# pred_df1 = df_test.copy()\n# pred_df1['Weekly_Sales'] = predict_sales1\n# pred_df1 = pred_df2[['Date', 'Weekly_Sales']]\n# pred_df1","2d2e3b3c":"# df_sample2 = df_sample.copy()\n# df_sample2['Weekly_Sales'] = predict_sales2\n# df_sample2\n\n\n# pred_df2 = df_test.copy()\n# pred_df2['Weekly_Sales'] = predict_sales2\n# pred_df2 = pred_df2[['Date', 'Weekly_Sales']]\n# pred_df2\n\n# pred_df2.Date = pd.to_datetime(pred_df2.Date)\n# pred_df2['Week'] = pred_df2.Date.dt.week\n# pred_df2['Year'] = pred_df2.Date.dt.year","45e901c2":"pred_df['Week'] = pred_df.Date.dt.week\npred_df['Year'] = pred_df.Date.dt.year\n\n# pred_df2.Date = pd.to_datetime(pred_df2.Date)\n# pred_df2['Week'] = pred_df2.Date.dt.week\n# pred_df2['Year'] = pred_df2.Date.dt.year","832ded85":"# pred_df['Week'] = pred_df.Date.dt.week\n# pred_df['Year'] = pred_df.Date.dt.year\n\n# pred_df1.Date = pd.to_datetime(pred_df1.Date)\n# pred_df1['Week'] = pred_df1.Date.dt.week\n# pred_df1['Year'] = pred_df1.Date.dt.year","29ce1b88":"# pred_df_2010 = pred_df[pred_df.Year==2010]['Weekly_Sales'].groupby(pred_df['Week']).mean()\n# pred_df_2011 = pred_df[pred_df.Year==2011]['Weekly_Sales'].groupby(pred_df['Week']).mean()\n# pred_df_2012 = pred_df[pred_df.Year==2012]['Weekly_Sales'].groupby(pred_df['Week']).mean()\n\n# pred_df1_2012 = pred_df1[pred_df1.Year==2012]['Weekly_Sales'].groupby(pred_df1['Week']).mean()\n# pred_df1_2013 = pred_df1[pred_df1.Year==2013]['Weekly_Sales'].groupby(pred_df1['Week']).mean()\n\n\n# plt.figure(figsize=(20,8))\n# sns.lineplot(pred_df_2010.index, pred_df_2010.values)\n# sns.lineplot(pred_df_2011.index, pred_df_2011.values)\n# sns.lineplot(pred_df_2012.index, pred_df_2012.values)\n\n# sns.lineplot(pred_df1_2012.index, pred_df1_2012.values)\n# sns.lineplot(pred_df1_2013.index, pred_df1_2013.values)\n\n\n# plt.grid()\n# plt.xticks(np.arange(1, 53, step=1))\n# plt.legend(['2010', '2011', '2012', '2012_pred', '2013_pred'], loc='best', fontsize=20)\n# # plt.legend(['2010', '2011', '2012_y', '2013'], loc='best', fontsize=18)\n# plt.title('Weekly Sales per year (average)', fontsize=15)\n# plt.ylabel('Sales', fontsize=16)\n# plt.xlabel('Week', fontsize=16)\n# plt.show()","d85dc21d":"# pred_df_2010 = pred_df[pred_df.Year==2010]['Weekly_Sales'].groupby(pred_df['Week']).mean()\n# pred_df_2011 = pred_df[pred_df.Year==2011]['Weekly_Sales'].groupby(pred_df['Week']).mean()\n# pred_df_2012 = pred_df[pred_df.Year==2012]['Weekly_Sales'].groupby(pred_df['Week']).mean()\n\n# pred_df2_2012 = pred_df2[pred_df2.Year==2012]['Weekly_Sales'].groupby(pred_df2['Week']).mean()\n# pred_df2_2013 = pred_df2[pred_df2.Year==2013]['Weekly_Sales'].groupby(pred_df2['Week']).mean()\n\n\n# plt.figure(figsize=(20,8))\n# sns.lineplot(pred_df_2010.index, pred_df_2010.values)\n# sns.lineplot(pred_df_2011.index, pred_df_2011.values)\n# sns.lineplot(pred_df_2012.index, pred_df_2012.values)\n\n# sns.lineplot(pred_df2_2012.index, pred_df2_2012.values)\n# sns.lineplot(pred_df2_2013.index, pred_df2_2013.values)\n\n\n# plt.grid()\n# plt.xticks(np.arange(1, 53, step=1))\n# plt.legend(['2010', '2011', '2012', '2012_pred', '2013_pred'], loc='best', fontsize=20)\n# # plt.legend(['2010', '2011', '2012_y', '2013'], loc='best', fontsize=18)\n# plt.title('Weekly Sales per year (average)', fontsize=15)\n# plt.ylabel('Sales', fontsize=16)\n# plt.xlabel('Week', fontsize=16)\n# plt.show()","3487fbed":"# train_total[train_total.Week == 51]","ac12a799":"from sklearn.model_selection import GridSearchCV\n\nparams ={\n    'n_estimators':[50],\n    'max_depth':[15,20,25,30],\n    'min_samples_leaf':[1,2,3],\n    'min_samples_split':[3,4,5,6]\n}","51cdfeb8":"rf_optimize = RandomForestRegressor(random_state=0, n_jobs=-1)\ngrid_cv = GridSearchCV(rf_optimize, param_grid=params, cv=2, n_jobs=-1)\ngrid_cv.fit(x_train1, y_train1)","d87480d8":"grid_cv.best_params_","8fa4f7e5":"print(grid_cv.best_score_)\n","c0559ae8":"start = time.time()  # \uc2dc\uc791 \uc2dc\uac04 \uc800\uc7a5\nprint(\"=====train start=======\\n\")\n# param: \uc544\ubb34\ub807\uac8c\ub098\n\nmodel_rf_final = RandomForestRegressor(n_estimators=50,max_depth = 25,\n                                  min_samples_leaf = 1, min_samples_split = 6)\nmodel_rf_final.fit(x_train1, y_train1)\n\nprint(\"done!\\ntime :\", time.time() - start)  # \ud604\uc7ac\uc2dc\uac01 - \uc2dc\uc791\uc2dc\uac04 = \uc2e4\ud589 \uc2dc\uac04","cda398b0":"print(\"\ud6c8\ub828 \uc138\ud2b8 \uc815\ud655\ub3c4: {:.3f}\".format(model_rf_final.score(x_train1, y_train1)))\nprint(\"\ud14c\uc2a4\ud2b8 \uc138\ud2b8 \uc815\ud655\ub3c4: {:.3f}\".format(model_rf_final.score(x_test1, y_test1)))","6b081269":"test_pred3 = test_total1.copy()","b0079862":"# model_rf\nstart = time.time()  # \uc2dc\uc791 \uc2dc\uac04 \uc800\uc7a5\n\nprint(\"===== predict start! =======\\n\")\npredict_sales3 = model_rf_final.predict(test_pred3)\n\nprint(\"========== done! ===========\\n\")\nprint(\"time :\", time.time() - start)  # \ud604\uc7ac\uc2dc\uac01 - \uc2dc\uc791\uc2dc\uac04 = \uc2e4\ud589 \uc2dc\uac04","e7941a2c":"pred_df3 = df_test.copy()\npred_df3['Weekly_Sales'] = predict_sales3\npred_df3 = pred_df3[['Date', 'Weekly_Sales']]\n\npred_df3.Date = pd.to_datetime(pred_df3.Date)\npred_df3['Week'] = pred_df3.Date.dt.week\npred_df3['Year'] = pred_df3.Date.dt.year","e5273d9d":"pred_df_2010 = pred_df[pred_df.Year==2010]['Weekly_Sales'].groupby(pred_df['Week']).mean()\npred_df_2011 = pred_df[pred_df.Year==2011]['Weekly_Sales'].groupby(pred_df['Week']).mean()\npred_df_2012 = pred_df[pred_df.Year==2012]['Weekly_Sales'].groupby(pred_df['Week']).mean()\n\npred_df3_2012 = pred_df3[pred_df3.Year==2012]['Weekly_Sales'].groupby(pred_df3['Week']).mean()\npred_df3_2013 = pred_df3[pred_df3.Year==2013]['Weekly_Sales'].groupby(pred_df3['Week']).mean()\n\n\nplt.figure(figsize=(20,8))\nsns.lineplot(pred_df_2010.index, pred_df_2010.values)\nsns.lineplot(pred_df_2011.index, pred_df_2011.values)\nsns.lineplot(pred_df_2012.index, pred_df_2012.values)\n\nsns.lineplot(pred_df3_2012.index, pred_df3_2012.values)\nsns.lineplot(pred_df3_2013.index, pred_df3_2013.values)\n\n\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['2010', '2011', '2012', '2012_pred', '2013_pred'], loc='best', fontsize=20)\n# plt.legend(['2010', '2011', '2012_y', '2013'], loc='best', fontsize=18)\nplt.title('Weekly Sales per year (average)', fontsize=15)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Week', fontsize=16)\nplt.show()","707a71f8":"pred_final = pred_df3[[\"Date\", \"Weekly_Sales\"]]\npred_final","51ed6526":"df_sample['Weekly_Sales'] = pred_final['Weekly_Sales']\ndf_sample","6364da95":"df_sample.to_csv('submission.csv',index=False)\ndf_sample","12f0e49c":"---\n# \ucd5c\uc801\ud654\n[\uc124\uba85](https:\/\/datascienceschool.net\/03%20machine%20learning\/14.01%20%EB%AA%A8%ED%98%95%20%EC%B5%9C%EC%A0%81%ED%99%94.html)","70783a0a":"---","55e62a5d":"---\n# Data Processing","b2604e5d":"---\n# Predict","a611448d":"---\n# Train models","ddce1fea":"---\n# Define model","575bcef6":"---\n# Data visualization","d74ada56":"---\n# Predict visualization"}}