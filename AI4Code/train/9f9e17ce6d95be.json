{"cell_type":{"613479a0":"code","fc1b188d":"code","13661d1c":"code","40b23891":"code","2799668c":"code","af9d0fac":"code","093533ed":"code","01cb5c47":"code","c4c69fd3":"code","9d170299":"code","5d17fe22":"code","46246fad":"code","42a7ed90":"code","bb488cd9":"code","3581944e":"code","8199947f":"code","990155d7":"code","70443b68":"code","b580ae2c":"code","ce5f6c0e":"code","645a77c2":"code","282c7cc6":"code","3f496933":"code","4376a526":"code","1fb2036f":"code","e99a5712":"code","333f6f9c":"code","d3c1ba78":"code","4ef94687":"code","6173614c":"code","a2839b59":"code","489ae19c":"code","473ea36f":"code","adad89f3":"code","279e876e":"code","5eb8419b":"code","cc265437":"code","37c54f96":"code","1e95b056":"code","e8dc4808":"code","3e04837d":"code","4ca6ea09":"code","fee1df9a":"code","428867cd":"code","5ff3ed48":"code","1d796de3":"code","9bc78606":"code","0a26bbf1":"code","847b9c04":"code","e4c6bf34":"code","63c2943f":"code","df7fed63":"code","636397be":"code","820454e8":"code","8b7d9a7e":"code","ee31db95":"code","769825fa":"code","6a9c6e56":"code","6cd6e934":"code","7096ce9d":"markdown","f8f62f09":"markdown","73a6f1d3":"markdown","563a063b":"markdown","b049beca":"markdown","027b185a":"markdown","eadd83b6":"markdown","160cca32":"markdown","b3cdaaa8":"markdown","e3295aff":"markdown","4fa6cb2e":"markdown","3542c268":"markdown","e9c5b381":"markdown","4c49dfcf":"markdown","ba2d5d5f":"markdown","b5b72cc8":"markdown","57c2af8d":"markdown","e0f1b0d4":"markdown","0bc0f29c":"markdown","59ed3c36":"markdown","8a4fb7d2":"markdown","76ba583c":"markdown","5cc15e6c":"markdown","3f2010bf":"markdown","afa10086":"markdown","95934b71":"markdown","51eba0f2":"markdown","3956ab6c":"markdown","e91b89f8":"markdown","00f951b8":"markdown","d7ca9aaf":"markdown","4e650a21":"markdown","a5198dbd":"markdown"},"source":{"613479a0":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n%matplotlib inline\nplt.style.use('seaborn')","fc1b188d":"from numpy.ma import MaskedArray\nimport sklearn.utils.fixes\n\nsklearn.utils.fixes.MaskedArray = MaskedArray","13661d1c":"df = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\", index_col=['Id'], na_values=\"?\")","40b23891":"df.head()","2799668c":"df.info()","af9d0fac":"df.describe()","093533ed":"plt.figure(figsize=(13, 7))\ndf['capital.gain'].hist(color = 'coral')\nplt.xlabel('capital gain')\nplt.ylabel('quantity')\nplt.title('Capital gain histogram')","01cb5c47":"plt.figure(figsize=(13, 7))\ndf['capital.loss'].hist(color = 'coral')\nplt.xlabel('capital loss')\nplt.ylabel('quantity')\nplt.title('Capital loss histogram')","c4c69fd3":"total = df.isnull().sum().sort_values(ascending = False)\npercent = ((df.isnull().sum()\/df.isnull().count())*100).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis = 1, keys = ['Total', '%'])\nmissing_data.head()","9d170299":"print('occupation:\\n')\nprint(df['occupation'].describe())\n\nprint('\\n\\nworkclass:\\n')\nprint(df['workclass'].describe())\n\nprint('\\n\\nnative.country:\\n')\nprint(df['native.country'].describe())","5d17fe22":"df_analysis = df.copy()","46246fad":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ndf_analysis['income'] = le.fit_transform(df_analysis['income'])","42a7ed90":"df_analysis['income']","bb488cd9":"mask = np.triu(np.ones_like(df_analysis.corr(), dtype=np.bool))\n\nplt.figure(figsize=(10,10))\n\nsns.heatmap(df_analysis.corr(), mask=mask, square = True, annot=True, vmin=-1, vmax=1, cmap='autumn')\nplt.show()","3581944e":"sns.catplot(x=\"income\", y=\"education.num\", kind=\"boxen\", data=df_analysis);","8199947f":"sns.catplot(x=\"income\", y=\"hours.per.week\", kind=\"boxen\", data=df_analysis);","990155d7":"sns.catplot(x=\"income\", y=\"age\", kind=\"boxen\", data=df_analysis);","70443b68":"sns.catplot(y=\"sex\", x=\"income\", kind=\"bar\", data=df_analysis);","b580ae2c":"df_analysis[\"sex\"].value_counts()","ce5f6c0e":"sns.catplot(y=\"race\", x=\"income\", kind=\"bar\", data=df_analysis);","645a77c2":"df_analysis[\"race\"].value_counts()","282c7cc6":"sns.catplot(y=\"workclass\", x=\"income\", kind=\"bar\", data=df_analysis);","3f496933":"df_analysis[\"workclass\"].value_counts()","4376a526":"sns.catplot(y=\"marital.status\", x=\"income\", kind=\"bar\", data=df_analysis);","1fb2036f":"df_analysis[\"marital.status\"].value_counts()","e99a5712":"sns.catplot(y=\"occupation\", x=\"income\", kind=\"bar\", data=df_analysis);","333f6f9c":"df_analysis[\"occupation\"].value_counts()","d3c1ba78":"sns.catplot(y=\"native.country\", x=\"income\", kind=\"bar\", data=df_analysis);","4ef94687":"df_analysis[\"native.country\"].value_counts()","6173614c":"sns.catplot(y=\"education\", x=\"income\", kind=\"bar\", data=df_analysis);","a2839b59":"df_analysis[\"education\"].value_counts()","489ae19c":"df.drop_duplicates(keep='first', inplace=True)","473ea36f":"df = df.drop(['fnlwgt', 'native.country'], axis=1)","adad89f3":"df.head()","279e876e":"Y_train = df.pop('income')\n\nX_train = df","5eb8419b":"X_train.head()","cc265437":"# Seleciona as vari\u00e1veis num\u00e9ricas\nnumerical_cols = list(X_train.select_dtypes(include=[np.number]).columns.values)\n\n# Remove as vari\u00e1veis num\u00e9ricas esparsas\nnumerical_cols.remove('capital.gain')\nnumerical_cols.remove('capital.loss')\n\n# Seleciona as vari\u00e1veis num\u00e9ricas esparsas\nsparse_cols = ['capital.gain', 'capital.loss']\n\n# Seleciona as vari\u00e1veis categ\u00f3ricas\ncategorical_cols = list(X_train.select_dtypes(exclude=[np.number]).columns.values)\n\n# Mostrando as diferentes sele\u00e7\u00f5es\nprint(\"Colunas num\u00e9ricas: \", numerical_cols)\nprint(\"Colunas esparsas: \", sparse_cols)\nprint(\"Colunas categ\u00f3ricas: \", categorical_cols)","37c54f96":"from sklearn.impute import SimpleImputer\n\n# Inicializa nosso Imputer\nsimple_imputer = SimpleImputer(strategy='most_frequent')","1e95b056":"from sklearn.preprocessing import OneHotEncoder\n\n# Inicializa nosso Encoder\none_hot = OneHotEncoder(sparse=False)","e8dc4808":"from sklearn.pipeline import Pipeline\n\n# Cria a nossa pipeline categ\u00f3rica\ncategorical_pipeline = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n    ('onehot', OneHotEncoder(drop='if_binary'))\n])","3e04837d":"from sklearn.impute import KNNImputer\n\n# Cria o nosso KNNImputer com 5 vizinhos\nknn_imputer = KNNImputer(n_neighbors=5)","4ca6ea09":"from sklearn.preprocessing import StandardScaler\n\n# Cria o nosso StandardScaler\nscaler = StandardScaler()","fee1df9a":"# Cria a nossa pipeline num\u00e9rica\nnumerical_pipeline = Pipeline(steps = [\n    ('imputer', KNNImputer(n_neighbors=10, weights=\"uniform\")),\n    ('scaler', StandardScaler())\n])","428867cd":"from sklearn.preprocessing import RobustScaler\n\nsparse_pipeline = Pipeline(steps = [\n    ('imputer', KNNImputer(n_neighbors=10, weights=\"uniform\")),\n    ('scaler', RobustScaler())\n])","5ff3ed48":"from sklearn.compose import ColumnTransformer\n\n# Cria o nosso Pr\u00e9-Processador\n\n# Cada pipeline est\u00e1 associada a suas respectivas colunas no datast\npreprocessor = ColumnTransformer(transformers = [\n    ('num', numerical_pipeline, numerical_cols),\n    ('spr', sparse_pipeline, sparse_cols),\n    ('cat', categorical_pipeline, categorical_cols)\n])","1d796de3":"X_train = preprocessor.fit_transform(X_train)","9bc78606":"from sklearn.neighbors import KNeighborsClassifier\n\n# Instancia nosso classificador\nknn = KNeighborsClassifier(n_neighbors=24)","0a26bbf1":"from sklearn.model_selection import cross_val_score\n\nscore = cross_val_score(knn, X_train, Y_train, cv = 5, scoring=\"accuracy\")\nprint(\"Acur\u00e1cia com cross validation:\", score.mean())","847b9c04":"knn = KNeighborsClassifier(n_neighbors=24)","e4c6bf34":"knn.fit(X_train, Y_train)","63c2943f":"test_data = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/test_data.csv\", index_col=['Id'], na_values=\"?\")","df7fed63":"X_test = test_data.drop(['fnlwgt', 'native.country'], axis=1)","636397be":"X_test = preprocessor.transform(X_test)","820454e8":"predictions = knn.predict(X_test)","8b7d9a7e":"predictions","ee31db95":"submission = pd.DataFrame()","769825fa":"submission[0] = test_data.index\nsubmission[1] = predictions\nsubmission.columns = ['Id','income']","6a9c6e56":"submission.head()","6cd6e934":"submission.to_csv('submission.csv',index = False)","7096ce9d":"Vemos que a distribui\u00e7\u00e3o de maiores education.num \u00e9 maior com quem tem maior income.","f8f62f09":"10\n0.8635051052296856\n\n11\n0.8625831393906178\n\n12\n0.8645192936281848\n\n13\n0.8646116248125393\n\n14\n0.8651955640604146\n\n15\n0.8643964057807729\n\n16\n0.8648574005073634\n\n17\n0.8645808461768135\n\n18\n0.8655029159179811\n\n19\n0.8654721231137874\n\n20\n0.8661482754754489\n\n21\n0.8650418267349359\n\n22\n0.8658102630527249\n\n23\n0.8653799525054054\n\n**24\n0.8663019797411685**\n\n25\n0.8654106744672585\n\n26\n0.8651032989955784\n\n27\n0.864826702159624\n\n28\n0.8655028970266903\n\n29\n0.8651340587400131\n\n30\n0.8654721231137874\n\n","73a6f1d3":"# 1.3. Limpeza de Dados","563a063b":"Come\u00e7aremos limpando os dados duplicaos","b049beca":"Vemos que cerca de 50% das pessoas casadas, seja com militares ou civis, ganham mais que 50k. Essa coluna tem vari\u00e1veis mais distribu\u00eddas.","027b185a":"Podemos agora separar os nossos dados em X e Y, sendo X as nossas vari\u00e1veis independentes e Y a nossa vari\u00e1vel de classe:","eadd83b6":"Verifico se est\u00e1 tudo certo com os dados.","160cca32":"# 1.2. Vizualiza\u00e7\u00e3o de Vari\u00e1veis N\u00e3o-Num\u00e9ricas","b3cdaaa8":"Vejo os tipos das vari\u00e1veis.","e3295aff":"Vamos analisar a porcentagem de pessoas nas vari\u00e1veis, dentro de cada coluna, que atingem a income maior que 50k, e tamb\u00e9m vamos analisar a frequ\u00eancia que as vari\u00e1veis aparecem em cada coluna.","4fa6cb2e":"# 2. Parte 2\n\nAplicar o classificador KNN, por meio da classe KNeighborsClassifier.","3542c268":"Vemos que em \"workclass\" e \"native.country\", a vari\u00e1vel mais frequente, \"Private\" e \"United-States\", respectivamente, ocorrem na grande maioria das vezes, cerca de 74% e de 91%. ","e9c5b381":"Vemos com o Heatmap a correla\u00e7\u00e3o entre essas colunas, e, para a da \"Income\", que \u00e9 a que temos interesse, a correla\u00e7\u00e3o, da maior para menor \u00e9 com \"Education Num\", \"Age\" e \"Hours per week\", \"Capital Gain\" e \"Capital Loss\", enquanto \"fnlwgt\" tem correla\u00e7\u00e3o praticamente nula. Agora analisaremos a distribui\u00e7\u00e3o dessas colunas em rela\u00e7\u00e3 ao \"Income\" >=50 (1) e <50 (0), na ordem em que a correla\u00e7\u00e3o foi vista no heatmap, exceto de \"Capital Gain\" e \"Capital Loss\" que j\u00e1 foram analisados anteriormente.","4c49dfcf":"Vemos nesse mapa que a varia\u00e7\u00e3o \u00e9 muito grande (as barras pretas), e que a quantidade de pessoas entrevistadas que s\u00e3o dos Estados Unidos \u00e9 de mais que 90%. Ent\u00e3o essa coluna n\u00e3o \u00e9 interessante de ser analisada, pois o espa\u00e7o amostral dos outros pa\u00edses \u00e9 muito menor.","ba2d5d5f":"Vejo algumas vari\u00e1veis dos dados.","b5b72cc8":"Vemos com esse gr\u00e1fico que cerca de 10% das mulheres ultrapassam os 50k por anos, enquanto nos homens, \u00e9 cerca de 30%. E que mais que 60% dos entrevistados eram homens.","57c2af8d":"Selecionamos n_neighbors=24","e0f1b0d4":"Vamos analisar a correla\u00e7\u00e3o das colunas com vari\u00e1veis num\u00e9ricas com a coluna \"Income\", que \u00e9 nosso dado de interesse no problema, tornando-a num\u00e9rica tamb\u00e9m.","0bc0f29c":"Agora removemos as colunas 'fnlwgt' e 'native.country' que n\u00e3o consideramos t\u00e3o importantes, de acordo com as an\u00e1lises acima.","59ed3c36":"Indentificando dados faltantes.","8a4fb7d2":"# 1. Parte 1","76ba583c":"# 1.1. Vizualiza\u00e7\u00e3o de Vari\u00e1veis Num\u00e9ricas","5cc15e6c":"Conseguimos identificar que h\u00e1 uma quantidade grande em valores pequenos, enquanto h\u00e1 uma quantidade pequena em valores maioress, tanto em \"capital gain\" quanto em \"capital loss\".","3f2010bf":"Vemos que as profiss\u00f5es com maior porcentagem de pessoas que ganham mais que 50k s\u00e3o Exec-managerial e Prof-specialty. Essa coluna tem vari\u00e1veis mais distribu\u00eddas.","afa10086":"Dividiremos os dados em tr\u00eas:\n- as vari\u00e1veis num\u00e9ricas bem comportadas\n- as vari\u00e1veis num\u00e9ricas com maior disperss\u00e3o (capital.gain e capital.loss)\n- as vari\u00e1veis n\u00e3o num\u00e9ricas\n\nCom as vari\u00e1veis n\u00e3o-num\u00e9ricas, vamos transform\u00e1las em num\u00e9ricas para podermos interpret\u00e1-las no c\u00f3dico, por meio do OneHotEncoder. Contru\u00edremos uma Pipelina com SimpleImputer e OneHotEncoder e aplic\u00e1-los por meio da fun\u00e7\u00e3o fit_transform.N.\n\nCom as vari\u00e1veis num\u00e9ricas bem comportadas, vamos preencher os dados faltantes com KNNImputer e com o StandardScaler vamos normaliz\u00e1-las. Criamos ent\u00e3o a Pipeline num\u00e9rica.\n\nCom as vari\u00e1veis num\u00e9ricas com maior dispers\u00e3o (colunas 'capital.gain' e 'capital.loss') vamos adotar os mesmos passos da Pipeline num\u00e9rica, apenas trocando o StandardScaler por um RobustScaler.\n\nJuntamos todas as pipelines em uma s\u00f3 e vamos usar o ColumnTransformer, que aplicar\u00e1 as diferentes pipelines em suas respectivas colunas.","95934b71":"Vemos que temos cerca de 6% de dados faltantes em \"occupation\" e \"workclass\" e cerca de 2% em \"native.country\". Vamos observar a distribui\u00e7\u00e3o dessas colunas:","51eba0f2":"Vemos que a distribui\u00e7\u00e3o de menores age \u00e9 maior com quem tem menor income.","3956ab6c":"Vemos que mais de 50% Self-emp-inc ganham mais que 50k. Al\u00e9m disso a grande maioria dos entrevistados s\u00e3o Private.","e91b89f8":"Ver qual a acur\u00e1cia do n_neighbors","00f951b8":"As linhas pretas em cima de cada uma das barras representam a varia\u00e7\u00e3o de renda dentro de cada grupo. Vemos que White e Asian-Pac-Islander tem maior percentual dos que ganham mais que 50k, sendo que os White s\u00e3o os que tem menos varia\u00e7\u00e3o de renda entre todos. Vemos que a grande maioria dos entrevistados s\u00e3o pessoas brancas.","d7ca9aaf":"Vemos que a distribui\u00e7\u00e3o de maiores hours.per.week \u00e9 maior com quem tem maior income.","4e650a21":"Com esses dados, podemos ver que \"capital.gain\" e \"capital.loss\" parecem n\u00e3o ser t\u00e3o confi\u00e1veis, uma vez que min, 25%, 50% e 75% s\u00e3o 0, enquanto max \u00e9 muito maior. Aparentam ser colunas com outliers ou missing data. Sendo interessante analisar seu histograma.","a5198dbd":"Vemos que mais que 60% dos Prof-school e Doctorate ganham mais que 50k, e que a maioria dos entrevistados \u00e9 HS-grad."}}