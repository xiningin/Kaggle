{"cell_type":{"a8bb40b4":"code","0327ccfd":"code","33e2f9a1":"code","4a38dc55":"code","69e53388":"code","81463ddb":"code","1afbf09a":"code","dd64833f":"code","f55e77cc":"code","74d0789d":"code","ce0f2b96":"code","60e8b1f6":"code","e8ec4a34":"code","4459dfff":"code","3c3aec4b":"code","97d0b54f":"code","212d25b4":"code","8f5bc2dc":"code","9fcbbc28":"code","468c30a7":"code","3ea31ed0":"code","fddf868f":"code","cb82125f":"code","676e066f":"code","c489f99b":"code","029679dc":"code","6496f134":"code","ba05dd1c":"code","5509a085":"code","73fd1328":"code","5a3fdf4d":"code","aee971d0":"code","cb9a187d":"markdown","3bdca0ae":"markdown","238d1ce0":"markdown","8e486db0":"markdown","31f9d4f5":"markdown","d9729579":"markdown","1eddf945":"markdown","e45308be":"markdown","6844dc60":"markdown"},"source":{"a8bb40b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0327ccfd":"# importing the libraries\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss, accuracy_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')","33e2f9a1":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsample = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","4a38dc55":"train.head()","69e53388":"def url(x):\n    x = re.sub(r'http\\S+', '', x) #removing web-links \n    x = re.sub(r'@\\w+','', x) #removing tags #removing tags\n    return x\ntrain[\"text\"] = train[\"text\"].progress_apply(lambda x: url(x))\ntest[\"text\"] = test[\"text\"].progress_apply(lambda x: url(x))","81463ddb":"def deocde(text):\n    for punct in [\"\\x89\u00db_\",\"\\x89\u00db\u00d2\",\"\\x89\u00db\u00d3\",\"\\x89\u00db\u00cf\",\"\\x89\u00db\u00f7\",\"\\x89\u00db\u00aa\",\"\\x89\u00db\\x9d\",\"\u00e5_\",\"\\x89\u00db\u00a2\",\"\\x89\u00db\u00a2\u00e5\u00ca\",\"\u00e5\u00ca\",\"\u00e5\u00a8\",\"\\x89\u00fb\",\"\u00e5\u00c8\"]:\n        text=text.replace(punct, \"\")\n    text=text.replace(r\"\u00e5\u00ca\",\" \")\n    text=text.replace(r\"\u00cc_\",\"a\")\n    text=text.replace(r\"\u00cc\u00a9\",\"e\")\n    text=text.replace(r\"\u00cc\u00a4\",\"c\")\n    return text\n\n\n\ntrain[\"text\"] = train[\"text\"].progress_apply(lambda x: deocde(x))\ntest[\"text\"] = test[\"text\"].progress_apply(lambda x: deocde(x))","1afbf09a":"train[\"text\"] = train[\"text\"].str.lower()\ntest[\"text\"] = test[\"text\"].str.lower()","dd64833f":"train.head()","f55e77cc":"train.drop_duplicates(subset = ['text'],inplace=True)\ntest.drop_duplicates(subset = ['text'],inplace=True)\ntrain.dropna(subset = ['text'],inplace=True)\ntest.dropna(subset = ['text'],inplace=True)","74d0789d":"y = train.target.values\nxtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n                                                  stratify=y, \n                                                  random_state=97, \n                                                  test_size=0.1, shuffle=True)","ce0f2b96":"print (xtrain.shape)\nprint (xvalid.shape)","60e8b1f6":"tfidf = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\ntfidf.fit(list(xtrain)+ list(xvalid))\nxtrain_tf = tfidf.transform(xtrain)\nxvalid_tf = tfidf.transform(xvalid)","e8ec4a34":"clf = LogisticRegression(C=1.0)\nclf.fit(xtrain_tf, ytrain)\npredictions = clf.predict(xvalid_tf)\n\nprint(\"f1_score: %0.3f\" % f1_score(yvalid, predictions) )","4459dfff":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 2), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning) Credit @abhishek thakur\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv =  ctv.transform(xtrain) \nxvalid_ctv = ctv.transform(xvalid)","3c3aec4b":"clf = LogisticRegression(C=1.0)\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict(xvalid_ctv)\n\nprint(\"f1_score: %0.3f\" % f1_score(yvalid, predictions) )","97d0b54f":"# Fitting a simple Naive Bayes on TFIDF\nclf = MultinomialNB()\nclf.fit(xtrain_tf, ytrain)\npredictions = clf.predict(xvalid_tf)\n\nprint (\"f1_score: %0.3f \" % f1_score(yvalid, predictions))","212d25b4":"# Fitting a simple Naive Bayes on Counts\nclf = MultinomialNB()\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict(xvalid_ctv)\n\nprint (\"f1_score: %0.3f \" % f1_score(yvalid, predictions))\n# print (\"logloss: %0.3f \" % log_loss(yvalid, predictions))","8f5bc2dc":"# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model. credit @abhishekthakur\nsvd = decomposition.TruncatedSVD(n_components=120)\nsvd.fit(xtrain_tf)\nxtrain_svd = svd.transform(xtrain_tf)\nxvalid_svd = svd.transform(xvalid_tf)\n \n# Scale the data obtained from SVD.\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)","9fcbbc28":"# Fitting a simple SVM\nclf = SVC(C=1.0, probability=True) \nclf.fit(xtrain_svd_scl, ytrain)\npredictions = clf.predict(xvalid_svd_scl)\n\nprint (\"logloss: %0.3f \" % f1_score(yvalid, predictions))\n# print (\"logloss: %0.3f \" % log_loss(yvalid, predictions))","468c30a7":"# Fitting a simple xgboost on tf-idf\nclf = xgb.XGBClassifier(max_depth=5, n_estimators=100, colsample_bytree=0.6, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_tf.tocsc(), ytrain)\n\npredictions = clf.predict(xvalid_tf.tocsc())\nprint (\"logloss: %0.3f \" % f1_score(yvalid, predictions))","3ea31ed0":"# Fitting a simple xgboost on CountVectors\n\nclf = xgb.XGBClassifier(max_depth=5, n_estimators=100, colsample_bytree=0.6, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_ctv.tocsc(), ytrain)\npredictions = clf.predict(xvalid_ctv.tocsc())\n\nprint (\"logloss: %0.3f \" % f1_score(yvalid, predictions))","fddf868f":"# Fitting a simple xgboost on tf-idf svd features\nclf = xgb.XGBClassifier(max_depth=5, n_estimators=100, colsample_bytree=0.6, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict(xvalid_svd)\n\nprint (\"logloss: %0.3f \" % f1_score(yvalid, predictions))","cb82125f":"# Fitting a simple xgboost on tf-idf svd features\nclf = xgb.XGBClassifier(nthread=10)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict(xvalid_svd)\n\nprint (\"logloss: %0.3f \" % f1_score(yvalid, predictions))","676e066f":"# Initialize SVD\nsvd = TruncatedSVD()\n    \n# Initialize the standard scaler \nscl = preprocessing.StandardScaler()\n\n# We will use logistic regression here..\nlr_model = LogisticRegression()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('lr', lr_model)])\n\nparam_grid = {'svd__n_components' : [120, 180],\n              'lr__C': [0.1, 1.0, 10], \n              'lr__penalty': ['l1', 'l2']}","c489f99b":"mll_scorer = metrics.make_scorer(f1_score, greater_is_better=False, needs_proba=False) #True @credit Abhishekthakur","029679dc":"# Initialize Grid Search Model\nmodel = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(xtrain_tf, ytrain)  # we can use the full data here but im only using xtrain\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","6496f134":"\nfrom keras.preprocessing import sequence, text\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nembed_size = 100 # how big is each word vector\nmax_features = 12000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 50 # max number of words in a question to use\nbatch_size = 256\ntrain_epochs = 5\nSEED = 97\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(xtrain))\ntrain_X = tokenizer.texts_to_sequences(xtrain)\nvalid_X = tokenizer.texts_to_sequences(xvalid)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\ntest_X = pad_sequences(valid_X, maxlen=maxlen)\n","ba05dd1c":"word_index=tokenizer.word_index\nprint('Number of unique words:',len(word_index))","5509a085":"# load the GloVe vectors in a dictionary:\n\nembeddings_index = {}\nf = open('\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt')\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\n\nprint('Found %s word vectors.' % len(embeddings_index))\n\nnum_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embeddings_index.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec     ","73fd1328":"\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\nmodel=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=maxlen,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(120, dropout=0.5, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=3e-4)\n\n\nfrom keras import backend as K\ndef recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\n# compile the model\nmodel.compile(optimizer=optimzer, loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])","5a3fdf4d":"X_train,X_test,y_train,y_test=train_test_split(train_X,ytrain,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","aee971d0":"history=model.fit(X_train,y_train,batch_size=4,epochs=5,validation_data=(X_test,y_test),verbose=2)","cb9a187d":"lets use Grid search to find the best parameters, ","3bdca0ae":"Lets try using SVD to reduced the components to 120 and see how it can make an impact ","238d1ce0":"Lets use WordVectors, \nI always prefer Glove and lets try with 100D. ","8e486db0":"Building Basic Models.\nLet's start building our very first model.\nSimple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression.","31f9d4f5":"Lets get to XGBclassifier","d9729579":"lets try model with features from CountVectorizer","1eddf945":"Taking a look into the text column in the DataFrame, we can clearly say we need to perform the text preprocesing. \n1. Removing http links,\n2. Removing hashtags.\n3. removing tagging usernames etc...\n\nI have performed preprocessoing and EDA in this kernal, please take a look into it \nhttps:\/\/www.kaggle.com\/kalyankkr\/preprocessing-and-eda?scriptVersionId=29026669","e45308be":"If you like please UPVOTE, I am thinking to add Hyperparameter tunning and more.\n\nLove this Quote:  Talk is cheep, show me the code.\n\nThanks, Have a great code weekend :)","6844dc60":"SIMPLE IS BETTER.\n\nAs the data is very less and labels are not clear ( here not clear meaning there are many dulpicate samples with different labels (targets)), so I am trying with simple approches first and can move to high level based on Baseline."}}