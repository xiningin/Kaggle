{"cell_type":{"715f2e13":"code","9bd695c9":"code","bf58b79c":"code","ce11dd4b":"code","9b056022":"code","1d34edf6":"code","68d3681c":"code","3f49fb7f":"code","376b8cae":"code","04448d5e":"code","d6800e20":"code","5da40337":"code","28791af3":"code","ed6da594":"code","6ba59ccd":"code","68d57d30":"code","92101a61":"code","5f8d8b90":"code","c69fce5b":"code","956670a5":"code","9ca7ba93":"code","e0dafc8e":"code","32b8e88a":"code","83036816":"code","2834a749":"code","a81eae9a":"code","69f21ddb":"code","dbe16fa2":"code","80d1f552":"code","06430d21":"markdown","f268fa71":"markdown","c1605f1f":"markdown","5971627d":"markdown","4032170c":"markdown","96cf9268":"markdown","924b98c8":"markdown","bc90482c":"markdown","9a79c106":"markdown","304e78c1":"markdown"},"source":{"715f2e13":"import numpy as np, pandas as pd, random as rn, time, gc, string, warnings\n\nseed = 32\nnp.random.seed(seed)\nrn.seed(seed)\nimport tensorflow as tf\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads = 1,\n                              inter_op_parallelism_threads = 1)\ntf.set_random_seed(seed) \nsess = tf.Session(graph = tf.get_default_graph(), config = session_conf)\nfrom keras import backend as K\nK.set_session(sess)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\n\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","9bd695c9":"sincere = train[train[\"target\"] == 0]\ninsincere = train[train[\"target\"] == 1]","bf58b79c":"print(sincere.shape)\nprint(insincere.shape)","ce11dd4b":"train = pd.concat([sincere[:int(len(sincere)*0.9)], insincere[:int(len(insincere)*0.9)]])\nval = pd.concat([sincere[int(len(sincere)*0.9):], insincere[int(len(insincere)*0.9):]])","9b056022":"tfidf_vc = TfidfVectorizer(min_df = 10,\n                          max_features = 100000,\n                          analyzer = \"word\",\n                          ngram_range = (1, 2),\n                          stop_words = \"english\",\n                          lowercase = True)\n\ntrain_vc = tfidf_vc.fit_transform(train[\"question_text\"])\nval_vc = tfidf_vc.transform(val[\"question_text\"])","1d34edf6":"model = LogisticRegression(C = 0.5, solver = \"sag\")\nmodel = model.fit(train_vc, train.target)\nval_pred = model.predict(val_vc)","68d3681c":"from sklearn.metrics import f1_score\n\nval_cv = f1_score(val.target, val_pred, average = \"binary\")\nprint(val_cv)","3f49fb7f":"from lime import lime_text\nfrom sklearn.pipeline import make_pipeline\nfrom lime.lime_text import LimeTextExplainer\nfrom lime import submodular_pick\nfrom collections import OrderedDict\n\nidx = val.index[32]\nc = make_pipeline(tfidf_vc, model)\nclass_names = [\"sincere\", \"insincere\"]\nexplainer = LimeTextExplainer(class_names = class_names)\nexp = explainer.explain_instance(val[\"question_text\"][idx], c.predict_proba, num_features = 50)\n\nprint(\"Question: \\n\", val[\"question_text\"][idx])\nprint(\"Probability (Insincere) =\", c.predict_proba([val[\"question_text\"][idx]])[0, 1])\nprint(\"True Class is:\", class_names[val[\"target\"][idx]])","376b8cae":"exp.show_in_notebook()","04448d5e":"weights = OrderedDict(exp.as_list())\nlime_weights = pd.DataFrame({\"words\": list(weights.keys()), \n                             \"weights\": list(weights.values())})\n\nsns.barplot(x = \"words\", y = \"weights\", data = lime_weights)\nplt.xticks(rotation = 45)\nplt.title(\"Sample {} features weights given by LIME\".format(idx))\nplt.show()","d6800e20":"sp_obj = submodular_pick.SubmodularPick(explainer, val[\"question_text\"].values, \n                                        c.predict_proba, sample_size = 10, \n                                        num_features = 50, num_exps_desired = 6,\n                                        top_labels = 3)","5da40337":"[exp.as_pyplot_figure(label = 0) for exp in sp_obj.sp_explanations]","28791af3":"from keras.layers import Input, Embedding, Dense, Conv1D, MaxPool1D, BatchNormalization\nfrom keras.layers import Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\n\nEMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\nmax_features = 50000\nmax_len = 70\nembed_size = 300","ed6da594":"X_train, y_train = train[\"question_text\"], train[\"target\"]\nX_val, y_val = val[\"question_text\"], val[\"target\"]\n\ntk = Tokenizer(num_words = max_features)\ntk.fit_on_texts(X_train)\n# X_train = tk.texts_to_sequences(X_train)\n# X_val = tk.texts_to_sequences(X_val)\n# X_train = pad_sequences(X_train, maxlen = max_len)\n# X_val = pad_sequences(X_val, maxlen = max_len)","6ba59ccd":"from sklearn.pipeline import TransformerMixin\nfrom sklearn.base import BaseEstimator\n\nclass TextsToSequences(Tokenizer, BaseEstimator, TransformerMixin):\n    \"\"\" Sklearn transformer to convert texts to indices list \n    (e.g. [[\"the cute cat\"], [\"the dog\"]] -> [[1, 2, 3], [1, 4]])\"\"\"\n    def __init__(self,  **kwargs):\n        super().__init__(**kwargs)\n        \n    def fit(self, texts, y=None):\n        self.fit_on_texts(texts)\n        return self\n    \n    def transform(self, texts, y=None):\n        return np.array(self.texts_to_sequences(texts))\n        \nsequencer = TextsToSequences(num_words = max_features)","68d57d30":"class Padder(BaseEstimator, TransformerMixin):\n    \"\"\" Pad and crop uneven lists to the same length. \n    Only the end of lists longernthan the maxlen attribute are\n    kept, and lists shorter than maxlen are left-padded with zeros\n    \n    Attributes\n    ----------\n    maxlen: int\n        sizes of sequences after padding\n    max_index: int\n        maximum index known by the Padder, if a higher index is met during \n        transform it is transformed to a 0\n    \"\"\"\n    def __init__(self, maxlen=500):\n        self.maxlen = maxlen\n        self.max_index = None\n        \n    def fit(self, X, y=None):\n        self.max_index = pad_sequences(X, maxlen=self.maxlen).max()\n        return self\n    \n    def transform(self, X, y=None):\n        X = pad_sequences(X, maxlen=self.maxlen)\n        X[X > self.max_index] = 0\n        return X\n\npadder = Padder(max_len)","92101a61":"def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n\nword_index = tk.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","5f8d8b90":"from keras.callbacks import Callback\n\nclass F1Evaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            y_pred = (y_pred > 0.35).astype(int)\n            score = f1_score(self.y_val, y_pred)\n            print(\"\\n F1 Score - epoch: %d - score: %.6f \\n\" % (epoch+1, score))","c69fce5b":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.pipeline import make_pipeline\n\ndef build_model():\n    inp = Input(shape = (max_len,))\n    x = Embedding(max_features, embed_size, weights = [embedding_matrix])(inp)\n    x = SpatialDropout1D(0.5)(x)\n    x = Conv1D(32, kernel_size = 2, activation = \"relu\")(x)\n    x = Conv1D(32, kernel_size = 2, activation = \"relu\")(x)\n    x = MaxPool1D(pool_size = 3)(x)\n    \n    x = Flatten()(x)\n    x = BatchNormalization()(x)\n    out = Dense(1, activation = \"sigmoid\")(x)\n    \n    model = Model(inputs = inp, outputs = out)\n    model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n    \n    return model","956670a5":"batch_size = 512\nsklearn_cnn = KerasClassifier(build_fn = build_model, epochs = 5, \n                              batch_size = batch_size, verbose = 2)\n\npipeline = make_pipeline(sequencer, padder, sklearn_cnn)\npipeline.fit(X_train, y_train)","9ca7ba93":"val_pred = pipeline.predict(X_val)\nval_cv = f1_score(y_val, val_pred, average = \"binary\")\nprint(\"Local CV is {}\".format(val_cv))","e0dafc8e":"sp_obj = submodular_pick.SubmodularPick(explainer, val[\"question_text\"].values, \n                                        pipeline.predict_proba, sample_size = 10, \n                                        num_features = 100, num_exps_desired = 6,\n                                        top_labels = 3)\n\n[exp.as_pyplot_figure(label = 0) for exp in sp_obj.sp_explanations]","32b8e88a":"def explain(idx):\n    print(\"Sample question:\\n\", X_val.iloc[idx])\n    print(\"-\"*50)\n    print(\"Probability (Insincere): {}\".format(pipeline.predict_proba([X_val.iloc[idx]])[0, 1]))\n    print(\"True Class is {}\".format(class_names[y_val.iloc[idx]]))\n    explanation = explainer.explain_instance(X_val.iloc[idx], pipeline.predict_proba, \n                                             num_features = 20)\n    explanation.show_in_notebook()\n    weights = OrderedDict(explanation.as_list())\n    lime_weights = pd.DataFrame({\"words\": list(weights.keys()), \n                                 \"weights\": list(weights.values())})\n\n    sns.barplot(x = \"words\", y = \"weights\", data = lime_weights)\n    plt.xticks(rotation = 45)\n    plt.title(\"Sample {} features weights given by LIME\".format(idx))\n    plt.show()","83036816":"explain(32)","2834a749":"Class = pd.DataFrame({\"true\": y_val}).reset_index()\nClass[\"pred\"] = val_pred","a81eae9a":"temp = Class[(Class[\"true\"] == 1) & (Class[\"pred\"] > 0.5)]\nexplain(temp.index[0])","69f21ddb":"temp = Class[(Class[\"true\"] == 1) & (Class[\"pred\"] < 0.5)]\nexplain(temp.index[0])","dbe16fa2":"temp = Class[(Class[\"true\"] == 0) & (Class[\"pred\"] < 0.5)]\nexplain(temp.index[0])","80d1f552":"temp = Class[(Class[\"true\"] == 0) & (Class[\"pred\"] > 0.5)]\nexplain(temp.index[0])","06430d21":"## Examples\n#### True Positive example","f268fa71":"## Machine Learning model: Logistic Regression","c1605f1f":"## Deep Learning model: CNN","5971627d":"Our NN model considers most words in the question are negative to insincere, which is good! ","4032170c":"#### False negative","96cf9268":"As we see from above, our model consider \"engineering\" and \"software\" are negative features.","924b98c8":"In Machine Learning models, we can know how they make predictions by visualizing features' weight. However, it is difficult to understand them when we apply them on text questions. In addition, when people try to use Neural Network (Deep Learning) models, predictions are mysterious. Feature weights are in the black box. \n\nIn this notebook, I will apply LIME (Local Interpretable Model-agnostic Explanations), which was introduced in 2016 in a paper called [\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier](https:\/\/arxiv.org\/abs\/1602.04938), on a simple Logistic mode and a simple NN model. The purpose of LIME is to explain a model prediction for a specific sample in a human-interpretable way.","bc90482c":"We list $50$ words from our model to see how they effect the prediction.","9a79c106":"#### True Negative","304e78c1":"#### False Positive"}}