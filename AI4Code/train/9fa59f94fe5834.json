{"cell_type":{"4ef96105":"code","659be3f9":"code","9376f441":"code","3f4390fb":"code","99b42738":"code","8a8f58cc":"code","5d022ea6":"code","3583ba68":"code","28d1a13a":"code","d5b310ce":"code","59562272":"code","048401fc":"code","c5dc960c":"code","9f4c5ef8":"code","eb8e2517":"code","c2f7fc55":"code","15e4ab62":"code","dcb90c9d":"code","f2e6b11e":"code","e948ae94":"code","516bcf0e":"markdown","e011bb75":"markdown","f26bfebb":"markdown","86b011ed":"markdown","1f361136":"markdown","6b56f7be":"markdown","e41f1447":"markdown","4c4f3568":"markdown","251bc49d":"markdown","7f199b0d":"markdown","f4733233":"markdown","c63d2381":"markdown","b8248edc":"markdown","533bb477":"markdown","69112ee7":"markdown","71099d03":"markdown","df331b9d":"markdown","d2e77179":"markdown","e9fcb797":"markdown","4952ef0f":"markdown","1e01562d":"markdown","fb7cd141":"markdown","07fdac0d":"markdown","c89ec00c":"markdown","59a6af51":"markdown","b5fb0974":"markdown","a95210a4":"markdown","b7fe8a4a":"markdown","99878a28":"markdown","d0ccdd07":"markdown","c0ee3f5f":"markdown","e1210046":"markdown","1d3e1d93":"markdown","45b34301":"markdown","db1f9d38":"markdown","64fde987":"markdown","d84d5e5d":"markdown","e7eca40b":"markdown","cd24a66c":"markdown","29e19c20":"markdown","e5811430":"markdown","aa6c165d":"markdown","8db0459f":"markdown","39131832":"markdown"},"source":{"4ef96105":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n# Any results you write to the current directory are saved as output.","659be3f9":"df= pd.read_csv(\"..\/input\/Tweets.csv\")\ndf.head()","9376f441":"print(\"Shape of the dataframe is\",df.shape)\nprint(\"The number of nulls in each column are \\n\", df.isna().sum())","3f4390fb":"print(\"Percentage null or na values in df\")\n((df.isnull() | df.isna()).sum() * 100 \/ df.index.size).round(2)","99b42738":"del df['tweet_coord']\ndel df['airline_sentiment_gold']\ndel df['negativereason_gold']\ndf.head()","8a8f58cc":"print(\"Total number of tweets for each airline \\n \",df.groupby('airline')['airline_sentiment'].count().sort_values(ascending=False))\nairlines= ['US Airways','United','American','Southwest','Delta','Virgin America']\nplt.figure(1,figsize=(12, 12))\nfor i in airlines:\n    indices= airlines.index(i)\n    plt.subplot(2,3,indices+1)\n    new_df=df[df['airline']==i]\n    count=new_df['airline_sentiment'].value_counts()\n    Index = [1,2,3]\n    plt.bar(Index,count, color=['red', 'green', 'blue'])\n    plt.xticks(Index,['negative','neutral','positive'])\n    plt.ylabel('Mood Count')\n    plt.xlabel('Mood')\n    plt.title('Count of Moods of '+i)","5d022ea6":"from wordcloud import WordCloud,STOPWORDS","3583ba68":"new_df=df[df['airline_sentiment']=='negative']\nwords = ' '.join(new_df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","28d1a13a":"new_df=df[df['airline_sentiment']=='positive']\nwords = ' '.join(new_df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","d5b310ce":"\n\n# Calculate highest frequency words in positive tweets\ndef freq(str): \n  \n    # break the string into list of words  \n    str = str.split()          \n    str2 = [] \n  \n    # loop till string values present in list str \n    for i in str:              \n  \n        # checking for the duplicacy \n        if i not in str2: \n  \n            # insert value in str2 \n            str2.append(i)  \n              \n    for i in range(0, len(str2)): \n        if(str.count(str2[i])>50): \n            print('Frequency of', str2[i], 'is :', str.count(str2[i]))\n        \nprint(freq(cleaned_word))","59562272":"#get the number of negative reasons\ndf['negativereason'].nunique()\n\nNR_Count=dict(df['negativereason'].value_counts(sort=False))\ndef NR_Count(Airline):\n    if Airline=='All':\n        a=df\n    else:\n        a=df[df['airline']==Airline]\n    count=dict(a['negativereason'].value_counts())\n    Unique_reason=list(df['negativereason'].unique())\n    Unique_reason=[x for x in Unique_reason if str(x) != 'nan']\n    Reason_frame=pd.DataFrame({'Reasons':Unique_reason})\n    Reason_frame['count']=Reason_frame['Reasons'].apply(lambda x: count[x])\n    return Reason_frame\ndef plot_reason(Airline):\n    \n    a=NR_Count(Airline)\n    count=a['count']\n    Index = range(1,(len(a)+1))\n    plt.bar(Index,count, color=['red','yellow','blue','green','black','brown','gray','cyan','purple','orange'])\n    plt.xticks(Index,a['Reasons'],rotation=90)\n    plt.ylabel('Count')\n    plt.xlabel('Reason')\n    plt.title('Count of Reasons for '+Airline)\n    \nplot_reason('All')\nplt.figure(2,figsize=(13, 13))\nfor i in airlines:\n    indices= airlines.index(i)\n    plt.subplot(2,3,indices+1)\n    plt.subplots_adjust(hspace=0.9)\n    plot_reason(i)","048401fc":"date = df.reset_index()\n#convert the Date column to pandas datetime\ndate.tweet_created = pd.to_datetime(date.tweet_created)\n#Reduce the dates in the date column to only the date and no time stamp using the 'dt.date' method\ndate.tweet_created = date.tweet_created.dt.date\ndate.tweet_created.head()\ndf = date\nday_df = df.groupby(['tweet_created','airline','airline_sentiment']).size()\n# day_df = day_df.reset_index()\nday_df","c5dc960c":"day_df = day_df.loc(axis=0)[:,:,'negative']\n\n#groupby and plot data\nax2 = day_df.groupby(['tweet_created','airline']).sum().unstack().plot(kind = 'bar', color=['red', 'green', 'blue','yellow','purple','orange'], figsize = (15,6), rot = 70)\nlabels = ['American','Delta','Southwest','US Airways','United','Virgin America']\nax2.legend(labels = labels)\nax2.set_xlabel('Date')\nax2.set_ylabel('Negative Tweets')\nplt.show()","9f4c5ef8":"def tweet_to_words(tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",tweet) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words ))","eb8e2517":"df['clean_tweet']=df['text'].apply(lambda x: tweet_to_words(x))","c2f7fc55":"train,test = train_test_split(df,test_size=0.2,random_state=42)","15e4ab62":"train_clean_tweet=[]\nfor tweet in train['clean_tweet']:\n    train_clean_tweet.append(tweet)\ntest_clean_tweet=[]\nfor tweet in test['clean_tweet']:\n    test_clean_tweet.append(tweet)","dcb90c9d":"from sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer(analyzer = \"word\")\ntrain_features= v.fit_transform(train_clean_tweet)\ntest_features=v.transform(test_clean_tweet)","f2e6b11e":"Classifiers = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=200)]","e948ae94":"dense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['airline_sentiment'])\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(dense_features,train['airline_sentiment'])\n        pred = fit.predict(dense_test)\n    accuracy = accuracy_score(pred,test['airline_sentiment'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))\n    print(classification_report(pred,test['airline_sentiment']))\n    cm=confusion_matrix(pred , test['airline_sentiment'])\n    plt.figure()\n    plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\n    plt.xticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16,color='black')\n    plt.yticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16)\n    plt.show()","516bcf0e":"- Customer Service Issue is the main neagtive reason for US Airways,United,American,Southwest,Virgin America\n# - Late Flight is the main negative reason for Delta  \n# - Interestingly, Virgin America has the least count of negative reasons (all less than 60)\n# - Contrastingly to Virgin America, airlines like US Airways,United,American have more than 500 negative reasons (Late flight, Customer Service Issue)","e011bb75":"This shows the sentiments of tweets for each date from **2015-02-17** to **2015-02-24** for every airline in our dataframe.\n# \n# Our next step will be to plot this and get better visualization for negative tweets.","f26bfebb":"### Is there a relationship between negative sentiments and date ?","86b011ed":"The first step should be to check the shape of the dataframe and then check the number of null values in each column.\n# \n# In this way we can get an idea of the redundant columns in the data frame depending on which columns have the highest number of null values.","1f361136":"\n# #### ANY FEEDBACK IN THE COMMENTS WILL BE HIGHLY APPRECIATED.","6b56f7be":"Breakdown of this notebook:\n# 1. Loading the dataset: Load the data and import the libraries.\n# 2. Data Preprocessing:\n#      - Analysing missing data \n#      - Removing redundant columns.\n# 3. Visualising and counting sentiments of tweets for each airline\n# 4. Wordcloud plots for positive and negative tweets to visualise most frequent words for each.\n# 5. Analysing the reasons for negative tweets for each airline.\n# 6. Visualising negative tweet-sentiment relationship with dates.\n# 7. Predicting the tweet sentiments with tweet text data with:\n#       - Decision Tree Classifier\n#       - Random Forest Classifier\n# 8. Calculating accuracies, plotting the confusion matrix and comparing the models.","e41f1447":"The code for getting positive sentiments is completely same with the one for negative sentiments. Just replace negative with positive in the first line. Easy, right!","4c4f3568":"\n# \n#","251bc49d":"\n#","7f199b0d":"\n# \n#","f4733233":"- The goal is to firstly get an idea of the most frequent words in negative tweets.\n# - Get idea about most frequent words in positive tweets.","c63d2381":"Now, we will clean the tweet text data and apply classification algorithms on it","b8248edc":"### What are the reasons for negative sentimental tweets for each airline ?","533bb477":"### Wordcloud for Negative sentiments of tweets","69112ee7":"### Prediciting sentiments from tweet text data","71099d03":"### Preprocessing the tweet text data","df331b9d":"\n# \n# \n# \n# \n#","d2e77179":"#### Lets try and calculate the highest frequency words in postive sentimental tweets","e9fcb797":" - United, US Airways, American substantially get negative reactions.\n#  - Tweets for Virgin America are the most balanced.","4952ef0f":"Wordcloud is a great tool for visualizing nlp data. The larger the words in the wordcloud image , the more is the frequency of that word in our text data.","1e01562d":"### Wordcloud for positive reasons","fb7cd141":"- Interestingly, **American** has a sudden upsurge in negative sentimental tweets on **2015-02-23**, which reduced to half the very next day **2015-02-24**. (*I hope American is doing better these days and resolved their Customer Service Issue as we saw before*)\n# - **Virgin America** has the least number of negative tweets throughout the weekly data that we have. It should be noted that the total number of tweets for **Virgin America** was also significantly less as compared to the rest airlines, and hence the least negative tweets.\n# - The negative tweets for all the rest airlines is slightly skewed towards the end of the week !","07fdac0d":"To get a better idea, lets calculate the percentage of nulls or NA values in each column","c89ec00c":"\n# \n#","59a6af51":"- As we you can see above we have plotted the **confusion matrix** for predicted sentiments and actual sentiments (negative,neutral and positive)\n# - **Random Forest Classifier** gives us the best accuracy score, precision scores according to the classification report.\n# - The confusion matrix shows the TP,TN,FP,FN for all the 3 sentiments(negative,neutral and positive),Here also **Random Forest Classifier** gives **better** results than the **Decision Tree Classifier**.","b5fb0974":"- Firstly lets calculate the total number of tweets for each airline\n# - Then, we are going to get the barplots for each airline with respect to sentiments of tweets (positive,negative or neutral).\n# - This will give us a clearer idea about the airline sentiments , airlines relationship.","a95210a4":"\n# \n#","b7fe8a4a":"\n# \n#","99878a28":"### Data Preprocessing","d0ccdd07":"\n# \n# \n#","c0ee3f5f":"\n# \n# #### Please feel free to upvote and comment if you found this to be helpful !!","e1210046":"\n# \n# \n#","1d3e1d93":"* Words like **Thanks**, **best**, **customer** , **love**, **flying** , **good** are understandably present in the **most frequent** words of positive tweets. \n# * However, other than these, most of the words are stop words and need to be filtered. We will do so later.\n# * Lets try and visualize the reasons for negative tweets first !!","45b34301":"The data is split in the standard 80,20 ratio.","db1f9d38":"### Airline sentiments for each airline\n#","64fde987":"\n#","d84d5e5d":"## Most used words in Positive and Negative tweets","e7eca40b":"Our dataframe has data from **2015-02-17** to **2015-02-24**\n# \n# It will be interesting to see if the date has any effect on the sentiments of the tweets(*especially negative !*). We can draw various coclusions by visualizing this.","cd24a66c":"- Decision Tree Classifier\n# - Random Forest Classifier","29e19c20":"### References:- \n# I learnt a lot from this blog which shows you how to handle nlp data and implement data preprocessing and explanatory visualization for better understanding.\n# \n# https:\/\/www.analyticsvidhya.com\/blog\/2018\/07\/hands-on-sentiment-analysis-dataset-python\/","e5811430":"**tweet_coord , airline_sentiment_gold, negativereason_gold**  have more than 90% missing data. It will be better to delete these columns as they will not provide any constructive information.\n# \n#","aa6c165d":"\n#","8db0459f":"We will explore the **negative reason** column of our dataframe to extract conclusions about negative sentiments in the tweets by the customers","39131832":"### Importing the libraries and loading the data"}}