{"cell_type":{"4136a8ed":"code","0c30dabc":"code","d9fe9c4d":"code","0cd022b9":"code","ac1d9846":"code","f0408758":"code","ab6a1ea3":"code","41c42b00":"code","93229cd4":"code","cd80e62c":"code","f03b5b6d":"code","4916c9e2":"code","6e651cd6":"code","f63d293e":"code","4c97d440":"code","0c8c1ded":"code","0b9b6bd6":"code","687ddbc7":"code","182e22e9":"code","0d47682c":"code","22da8eb7":"code","ba77bdc2":"code","55994c56":"code","f6d3fc74":"code","e6684084":"code","7bc222d1":"code","1c9fd2c2":"code","a2a68a9f":"code","02b0bc8b":"markdown","257ea9cf":"markdown","2b8c39c9":"markdown","0a854ee2":"markdown","a392936f":"markdown","10f2d80d":"markdown","925879b4":"markdown","5a6dba35":"markdown","e48fb257":"markdown","cd5dcf36":"markdown","7218fe60":"markdown","0b56a119":"markdown","be434842":"markdown","41820e3e":"markdown","78c884a9":"markdown"},"source":{"4136a8ed":"import datetime as dt\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nsns.set_style('darkgrid')\n\n\nimport os\nfrom keras.applications import xception\nfrom keras.preprocessing import image\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nimport cv2\nfrom scipy.stats import uniform\n\nfrom tqdm import tqdm\nfrom IPython.core.display import display, HTML\nfrom PIL import Image\nfrom io import BytesIO\nimport base64\n\nimport keras\nfrom keras.models import Model, Sequential\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Masking,GlobalAveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\n\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","0c30dabc":"#copying the pretrained models to the cache directory\ncache_dir = os.path.expanduser(os.path.join('~', '.keras'))\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\nmodels_dir = os.path.join(cache_dir, 'models')\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)\n\n#copy the Xception models\n!cp ..\/input\/keras-pretrained-models\/xception* ~\/.keras\/models\/\n#show\n!ls ~\/.keras\/models","d9fe9c4d":"base_folder = '..\/input\/celeba-dataset'\ndata_folder = '..\/input\/celeba-dataset\/img_align_celeba'\nimage_folder = '..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba'","0cd022b9":"#read the image attributes csv file\ndf = pd.read_csv('..\/input\/celeba-dataset\/list_attr_celeba.csv')","ac1d9846":"print(df.columns)\ndf.head()","f0408758":"df = df = df[['image_id', 'Male']]\n#replace -1 to 0\ndf.replace(to_replace=-1, value=0, inplace=True) ","ab6a1ea3":"#add a class feature\ndef to_labels(x):\n    if x == 0:\n        return 'female'\n    else:\n        return 'male'\n\ndf['class'] = df.Male.apply(to_labels)\ndf.columns = ['filename', 'label', 'class']","41c42b00":"#show new dataframe\ndf.head()","93229cd4":"plt.figure(figsize=(8,5))\nsns.countplot(df.label);","cd80e62c":"SAMPLES = 300\ntrain_data = pd.concat([df[df['label']== i][:SAMPLES] for i in range(0,2)])\nprint('TRAIN DATA SHAPE: ', train_data.shape)","f03b5b6d":"# function to get an image\ndef read_img(filename, size):\n    img = image.load_img(os.path.join(image_folder, filename), target_size=size)\n    #convert image to array\n    img = image.img_to_array(img) \/ 255\n    return img","4916c9e2":"nb_rows = 3\nnb_cols = 5\nfig, axs = plt.subplots(nb_rows, nb_cols, figsize=(10, 5));\nplt.suptitle('SAMPLE IMAGES');\nfor i in range(0, nb_rows):\n    for j in range(0, nb_cols):\n        axs[i, j].xaxis.set_ticklabels([]);\n        axs[i, j].yaxis.set_ticklabels([]);\n        axs[i, j].imshow((read_img(train_data['filename'].iloc[np.random.randint(500)], (255,255))));\nplt.show();","6e651cd6":"#create a imagegenerator for for augmentation\ndatagen =  ImageDataGenerator(\n  rotation_range=30,\n  width_shift_range=0.2,\n  height_shift_range=0.2,\n  shear_range=0.2,\n  zoom_range=0.2,\n  horizontal_flip=True\n)","f63d293e":"img = read_img(train_data['filename'].iloc[546], (255,255))\nplt.title('ORIG IMAGE')\nplt.imshow(img);\n","4c97d440":"# reshape image to 4 dimentional\nimg = img.reshape((1,) + img.shape)","0c8c1ded":"plt.figure(figsize=(20,10))\nplt.suptitle('Data Augmentation', fontsize=28)\n\n\ni = 0\n\nfor batch in datagen.flow(img, batch_size=32):\n    plt.subplot(3, 5, i+1)\n    plt.grid(False)\n    plt.imshow(batch.reshape(255, 255, 3));\n    \n    if i == 9:\n        break\n    i += 1\n    \nplt.show();","0b9b6bd6":"#split the data\nX = train_data.drop(['label', 'class'], axis=1)\ny = train_data['label']\n\ntrain_x, train_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=101)","687ddbc7":"# function to get an image\ndef read_img(filename, size):\n    img = image.load_img(os.path.join(image_folder, filename), target_size=size)\n    #convert image to array\n    img = image.img_to_array(img) \/ 255\n    img = img.reshape((1,) + img.shape)\n    return img\n\n\narray_img = []\nlabel_img = []\n\nfor i, file in tqdm(enumerate(train_x['filename'])):\n    img = read_img(file, (255,255))\n    label = y_train.iloc[i]\n    num = 0\n    for batch in datagen.flow(img, batch_size=32):\n        batch = batch.reshape(255,255,3)\n        array_img.append(batch)\n        label_img.append(label)\n        if num == 4:\n            break\n        num=num+1","182e22e9":"#preprocess train\nINPUT_SIZE = 255\n\n\nX_train = np.zeros((len(array_img), INPUT_SIZE, INPUT_SIZE, 3), dtype='float')\ni=0\nfor file in tqdm(array_img):\n    X_train[i] = xception.preprocess_input(np.expand_dims(file.copy(), axis=0))\n    i = i+1","0d47682c":"#preprocess validation\n\nX_val = np.zeros((len(train_val), INPUT_SIZE, INPUT_SIZE, 3), dtype='float')\nfor i, file in tqdm(enumerate(train_val['filename'])):\n    img = read_img(file, (255,255))\n    X_val[i] = xception.preprocess_input(np.expand_dims(img.copy(), axis=0))","22da8eb7":"xception_bf = xception.Xception(weights='imagenet', include_top=False, pooling='avg')\nbf_train_x = xception_bf.predict(X_train, batch_size=32, verbose=1)\nbf_train_val = xception_bf.predict(X_val, batch_size=32, verbose=1)","ba77bdc2":"#print shape of feature and size\nprint('Train Shape: ', bf_train_x.shape)\nprint('Train Size: ', bf_train_x.size)\n\nprint('Validation Shape: ', bf_train_val.shape)\nprint('Validation Size: ', bf_train_val.size)","55994c56":"#keras model\nmodel = Sequential()\nmodel.add(Dense(units = 512 , activation = 'relu', input_dim=bf_train_x.shape[1]))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units = 64 , activation = 'relu'))\nmodel.add(Dense(units = 1, activation = 'sigmoid'))\nmodel.compile(optimizer ='adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","f6d3fc74":"#set callbacks\ncallbacks = [EarlyStopping(monitor='val_loss', patience=2),\n         ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n\n#fit the data\nhistory = model.fit(bf_train_x, np.array(label_img), batch_size=32, epochs=100, callbacks=callbacks)","e6684084":"fig, ax = plt.subplots(1,2,figsize=(14,5))\nax[0].set_title('TRAINING LOSS');\nax[1].set_title('TRAINING ACCURACY');\n\n\nax[0].plot(history.history['loss'], color= 'salmon',lw=2);\nax[1].plot(history.history['accuracy'], color= 'steelblue',lw=2);","7bc222d1":"#predict the validation data\npredictions = model.predict_classes(bf_train_val)","1c9fd2c2":"print(classification_report(y_val, predictions))","a2a68a9f":"con_mat = confusion_matrix(y_val, predictions)\nplt.figure(figsize=(5,5))\n\nsns.heatmap(con_mat, annot=True, square=True);\nplt.xlabel('Y_TRUE');\nplt.ylabel('PREDICTIONS');","02b0bc8b":"### LOSS AND ACCURACY","257ea9cf":"### DATA SAMPLING","2b8c39c9":"## OVERVIEW\n---\n* Feature Selection & Data Sampling\n* Image Processing\n* Data Augmentation\n* Transfer Learning with Keras Xception\n* Bottleneck Feature Ectraction\n* Deep Learning","0a854ee2":"### SHOW SAMPLE IMAGES","a392936f":"### CONFUSION MATRIX","10f2d80d":"### XCEPTION INPUT PREPROCESSING","925879b4":"### IMAGE PROCESSING","5a6dba35":"### DATA UTILITIES","e48fb257":"###### SHOW SAMPLE AUGMENTED IMAGE","cd5dcf36":"### CLASSIFICATION REPORT","7218fe60":"### COUNTPLOT PER GENDER","0b56a119":"### BOTTLENECK FEATURE EXTRACTION","be434842":"### DATA AUGMENTATION","41820e3e":"### MODELLING","78c884a9":"### FEATURE SELECTION"}}