{"cell_type":{"d083c90e":"code","384ed266":"code","94f440a5":"code","83cc37e4":"code","242eb26a":"code","920b4086":"code","2290a6e1":"code","d62719d6":"code","8c3f74af":"code","c04485e8":"code","c0625bcf":"code","f07166a5":"code","56aef19b":"code","9c108449":"code","dc93a3bb":"code","f6247682":"code","27fde429":"code","c7b110d8":"code","4a18d911":"code","e9309dd8":"code","60690b54":"code","4065f3a4":"code","1f038c31":"code","c668f459":"code","67ac2f72":"code","c69d8c9c":"code","a63ce76a":"code","9f49a40b":"code","3c0c5bfe":"code","b47b2927":"code","9cba7a7d":"code","fd5fba86":"code","2fd3958e":"markdown","6184abff":"markdown","1a885a08":"markdown","084e99bf":"markdown","4c836f1a":"markdown","de17de8c":"markdown","885451e3":"markdown","8b8e865f":"markdown","e676f71d":"markdown","11e707ee":"markdown","924f4242":"markdown","bc6f92e8":"markdown","b95a06e9":"markdown","f0d76957":"markdown","de1a30e6":"markdown","5aba1d67":"markdown","a33712c7":"markdown","3710feed":"markdown","3cde9b1f":"markdown","941ac236":"markdown","4dcae8fd":"markdown"},"source":{"d083c90e":"import numpy as np\nimport pandas as pd","384ed266":"auto = pd.read_csv('\/kaggle\/input\/automobile-dataset\/Automobile_data.csv', na_values = [\"?\"])\nauto.head()","94f440a5":"auto.dtypes","83cc37e4":"auto.describe(include = 'all') #For a quick summary ","242eb26a":"auto[\"city-mpg\"] = 235\/auto[\"city-mpg\"]\nauto[\"highway-mpg\"] = 235\/auto[\"highway-mpg\"]\nauto.rename(columns = {'city-mpg': 'city-L\/100km', 'highway-mpg': 'highway-L\/100km'}, inplace = True)\nauto.columns #For checking if our attempt at renaming has been successful","920b4086":"auto.head(3)","2290a6e1":"auto.isnull().sum()","d62719d6":"auto.dropna(subset = [\"price\"], axis = 0, inplace = True)\nauto.reset_index(drop = True, inplace = True)","8c3f74af":"from sklearn.model_selection import train_test_split\ny_data = auto.price\nX_data = auto.drop('price', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state = 0)","c04485e8":"#fill NaN in 'num-of-door' column with mode\ndef impute_nan_most_freq_category(df, column):\n    most_freq_category = df[column].mode()[0]\n    df[column + \"_Imputed\"] = df[column]\n    df[column + \"_Imputed\"].fillna(most_freq_category, inplace = True)\nimpute_nan_most_freq_category(auto, 'num-of-doors')\nauto.drop('num-of-doors', axis = 1, inplace = True)\n\n#Fillna with mean values\nmean = auto['normalized-losses'].mean(axis = 0)\nauto['normalized-losses'].fillna(mean, inplace = True)\n\nmean1 = auto.bore.mean(axis = 0)\nauto.bore.fillna(mean1, inplace = True)\n\nmean2 = auto.stroke.mean(axis = 0)\nauto.stroke.fillna(mean2, inplace = True)\n\nmean3 = auto.horsepower.mean(axis = 0)\nauto.horsepower.fillna(mean3, inplace = True)\n\nmean4 = auto['peak-rpm'].mean(axis = 0)\nauto['peak-rpm'].fillna(mean4, inplace = True)\n\nauto.sample(7)","c0625bcf":"bins = np.linspace(min(auto.price), max(auto.price), 4) #we create 3 bins of equal length\nlabels = [\"Low\", \"Medium\", \"High\"]\nauto['price-binned'] = pd.cut(auto.price, bins = bins, labels = labels, include_lowest = True)\nauto.head(3)","f07166a5":"drive_counts = auto[\"drive-wheels\"].value_counts()\ndrive_counts","56aef19b":"import seaborn as sns\nsns.boxplot(x = 'drive-wheels', y = 'price', data = auto)","9c108449":"import matplotlib.pyplot as plt\nplt.scatter(auto['engine-size'], auto.price)\nplt.xlabel('Engine size')\nplt.ylabel('Price')\nplt.title('Scatterplot of engine size v\/s price')","dc93a3bb":"df = auto.groupby(['drive-wheels', 'body-style'], as_index = False).price.mean()\ndf_pivot = df.pivot(index = 'drive-wheels', columns = 'body-style')\ndf_pivot.fillna(0)","f6247682":"#Display a heatmap\n#plt.pcolor(df_pivot, cmap = 'RdBu')\nfig, ax = plt.subplots()\nim = ax.pcolor(df_pivot, cmap = 'RdBu')\nrow_labels = df_pivot.columns.levels[1]\ncolumn_labels = df_pivot.index\nax.set_xticks(np.arange(df_pivot.shape[1]) + 0.5, minor = False)\nax.set_yticks(np.arange(df_pivot.shape[0]) + 0.5, minor = False)\nax.set_xticklabels(row_labels, minor = False)\nax.set_yticklabels(column_labels, minor = False)\nplt.xticks(rotation = 45)\nfig.colorbar(im)\nplt.show()","27fde429":"sns.regplot(x = \"highway-L\/100km\", y = 'price', data = auto)\nplt.ylim(0,)","c7b110d8":"sns.regplot(x = 'engine-size', y = 'price', data = auto)\nplt.ylim(0,)","4a18d911":"from scipy import stats\n\npearson_coeff, p_value = stats.pearsonr(auto.horsepower, auto.price)\nprint(\"The pearson coefficient is: {:f} and the p-value is {:f}\".format(pearson_coeff, p_value))","e9309dd8":"auto.corr() #To get the values of all relevant pearson_coeff's","60690b54":"#Correlation heatmap\nplt.figure(figsize = (16, 6))\n#Use triu() to isolate upper traingle of matrix \n#while turning all values in the lower triangle to 0\nmask = np.triu(np.ones_like(auto.corr(), dtype = np.bool))\n\nheatmap = sns.heatmap(auto.corr(), vmin = -1, vmax = 1, annot = True, cmap = 'BrBG', mask = mask)\n\nheatmap.set_title('Correlation Heatmap', fontdict = {'fontsize': 18}, pad = 12)","4065f3a4":"#Another heat map - to find the correlation of independent variables with dependent variable\nplt.figure(figsize = (8, 12))\n\nheatmap = sns.heatmap(auto.corr()[['price']].sort_values(by = 'price', ascending = False),\n                     vmin = -1, vmax = 1, annot = True, cmap = 'BrBG')\n\nheatmap.set_title('Features correlating with price', fontdict = {'fontsize': 18}, pad = 16);","1f038c31":"df_anova = auto[['make', 'price']].groupby('make')\nanova_results_1 = stats.f_oneway(df_anova.get_group('honda')['price'],\n                                df_anova.get_group('subaru')['price'])\ndisplay(anova_results_1)","c668f459":"#To display the groupby results\n\n#for key, item in df_anova:\n    #print(df_anova.get_group(key), \"\\n\\n\")","67ac2f72":"sns.residplot(x = auto['highway-L\/100km'], y = auto.price)","c69d8c9c":"sns.residplot(x = auto['engine-size'], y = auto.price)","a63ce76a":"!pip install feature_engine\nfrom feature_engine import imputation as imp\nimputer = imp.MeanMedianImputer(\n    imputation_method = 'mean', \n    variables = ['normalized-losses', 'bore', 'stroke', 'horsepower', 'peak-rpm'])\n\nimputer.fit(X_train)\n\nX_train = imputer.transform(X_train)\nX_test = imputer.transform(X_test)","9f49a40b":"imputer1 = imp.CategoricalImputer(variables = ['num-of-doors'])\nimputer1.fit(X_train)\n\nX_train = imputer1.transform(X_train)\nX_test = imputer1.transform(X_test)","3c0c5bfe":"def Distributionplot(RedFunc, BlueFunc, Redname, Bluename, title):\n    plt.figure(figsize = (12, 10))\n    ax1 = sns.distplot(RedFunc, hist = False, color = 'r', label = Redname)\n    ax2 = sns.distplot(BlueFunc, hist = False, color = 'b', label = Bluename)\n    plt.title(title)\n    plt.xlabel('Price (in $)')\n    plt.ylabel('Proportion of cars')\n    plt.legend()\n    plt.show()\n    plt.close()","b47b2927":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train[['horsepower', 'curb-weight', 'engine-size', 'highway-L\/100km']], y_train)\n\n#Prediction using train and test data\nyhat_train = lr.predict(X_train[['horsepower', 'curb-weight', 'engine-size', 'highway-L\/100km']])\nyhat_test = lr.predict(X_test[['horsepower', 'curb-weight', 'engine-size', 'highway-L\/100km']])\n\ntitle = 'Distribution Plot of predicted values (test) vs actual values (test)'\nDistributionplot(y_test, yhat_test, \"Actual values (test)\", \"Predicted values(test)\", title)","9cba7a7d":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Ridge\npr = PolynomialFeatures(degree = 2)\n\nX_train_pr = pr.fit_transform(\n    X_train[['horsepower', 'curb-weight', 'engine-size', 'highway-L\/100km']])\nX_test_pr = pr.fit_transform(X_test[['horsepower', 'curb-weight', 'engine-size', 'highway-L\/100km']])\n\nridge = Ridge(alpha = 0.1)\nridge.fit(X_train_pr, y_train)\nyhat = ridge.predict(X_test_pr)\n\nprint(\"Predicted values:\", yhat[0:4])\nprint(\"Actual values:\", y_test[0:4].values)","fd5fba86":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\npipe = Pipeline(steps = [('scale', StandardScaler()), \n                         ('polynomial', PolynomialFeatures(include_bias = False)),\n                         ('ridge', Ridge())])\n\nparam_grid = {'ridge__alpha': [1, 10, 100, 500, 1000, 5000], \n              'ridge__normalize': [True, False]}\n\nsearch = GridSearchCV(pipe, param_grid, n_jobs = -1)\nsearch.fit(X_train[[\"horsepower\", \"curb-weight\", \"engine-size\", \"highway-L\/100km\"]], y_train)\n\nprint(\"Best parameter (CV score = {:f}):\".format(search.best_score_))\nprint(search.best_params_)","2fd3958e":"Let's now do some missing-values replacement.","6184abff":"**Disclaimer:** This notebook is mostly based on the course 'Data Analysis with Python' offered by IBM in their IBM Data Science Professional Certificate. ","1a885a08":"## Let's go on to some exploratory data analysis.","084e99bf":"Observe that in both plots, the residuals seem to have 0 mean, and they are distributed around the x-axis with similar variance. There is also no curvature.\n\nHence, these plots confirm that a linear model is appropriate to express the relationship between these variables and the price variable.","4c836f1a":"In this notebook, I perform (some) data analysis on the automobile dataset. Additionally, I have incorporated some basic ML.","de17de8c":"We can see a positive correlation between the 2 variables: engine-size and highway-L\/100km with price.","885451e3":"Let's move on to some basic ML evaluation models.","8b8e865f":"The above values seem to indicate a strong correlation with high certainity (the p-value being less than 0.001) between the horsepower and the price of the cars. ","e676f71d":"Let us now replace NaN values.","11e707ee":"Let's do some correlation statistics.","924f4242":"To find the best value of alpha (that which gives highest R^2 score), let us use GridSearchCV. Furthermore, let's use a Pipeline object.","bc6f92e8":"First, let's do some descriptive statistics.","b95a06e9":"Let's deal with data formatting. We are going to convert the mpg values into 'litres\/100 km' convention. For this, we divide the existing values by 235.","f0d76957":"## Let's now do some data-preprocessing.","de1a30e6":"Let's do some ANOVA analysis. ANOVA can be used to find correlation between different groups of a categorical variable.\n\nLet's use it to see if there's any difference in the mean price for car makes - Subaru, Honda.","5aba1d67":"We do a train-test split now itself. That's because it's wrong to first replace NaNs and then do a train-test split (because of data leakage). \n\nBut first, let's remove the rows with missing price value.","a33712c7":"Let's now do one GroupBy analysis.","3710feed":"Let's do correlation.","3cde9b1f":"Let's now do some data binning. Data binning helps us to create categories from numerical data that we can use for further analysis. Let's do this on the 'price' column.","941ac236":"Let's now plot 2 residual plots - comparing price with highway-L\/100km and engine-size.","4dcae8fd":"## Let's now move on to model evaluation."}}