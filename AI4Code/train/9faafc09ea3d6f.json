{"cell_type":{"3d619cad":"code","08edb1e3":"code","410e2fed":"code","ee3f9d14":"code","f5dbc055":"code","38654ed0":"code","c39dc353":"code","d05de64b":"code","6770e16a":"code","4f910c8c":"code","4f800a18":"code","446a22ab":"code","0e4e59c5":"code","8949d17d":"code","7daa3fc6":"code","95cc99f9":"code","b107f44a":"code","6708c97d":"code","8d4e05c9":"code","298bc78e":"code","eef366ad":"code","5e0d2543":"markdown","ce81c325":"markdown","2d7c0224":"markdown","87ff4fdd":"markdown","c7039f2d":"markdown","7d404994":"markdown","15709555":"markdown","7b325210":"markdown","10e991f5":"markdown","d03649ee":"markdown","c89e128d":"markdown","37dfaf7f":"markdown","9e5b5336":"markdown","30605a92":"markdown","ec9b2638":"markdown","7bee69d9":"markdown","15f8b480":"markdown","3c9164d4":"markdown","da86c95d":"markdown","6a53f181":"markdown","8c153ed5":"markdown"},"source":{"3d619cad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport nltk\nfrom nltk import NaiveBayesClassifier\nfrom nltk.metrics.scores import f_measure, precision, recall\nimport collections\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfor filename in os.listdir(\"..\/input\"):\n    print(filename)\n\n# Any results you write to the current directory are saved as output.","08edb1e3":"import re\nfrom itertools import islice\n\ndef load_tsv(data_file, n):\n    data_features = list()\n    data = list()\n    infile = open(data_file, encoding='utf-8')\n    for line in infile:\n        if not line.strip():\n            continue\n        label, text = line.split('\\t')\n        text_features = process_text(text, n)\n        if text_features:\n            data_features += text_features\n            data.append((text_features, label))\n    return data, data_features\n\ndef process_text(text, n=1,\n                 remove_vowel_marks=False,\n                 remove_repeated_chars=False,\n                 ):\n    clean_text = text\n    if remove_vowel_marks:\n        clean_text = remove_diacritics(clean_text)\n    if remove_repeated_chars:\n        clean_text = remove_repeating_char(clean_text)\n\n    if n == 1:\n        return clean_text.split()\n    else:\n        tokens = clean_text.split()\n        grams = tokens\n        for i in range(2, n + 1):\n            grams += [  ' '.join(g) for g in list(window(tokens, i))  ]\n        return grams\n\n\n\ndef window(words_seq, n):\n    \"\"\"Returns a sliding window (of width n) over data from the iterable\"\"\"\n    \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n    it = iter(words_seq)\n    result = tuple(islice(it, n))\n    if len(result) == n:\n        yield result\n    for elem in it:\n        result = result[1:] + (elem,)\n        yield result\n\n\ndef remove_repeating_char(text):\n    # return re.sub(r'(.)\\1+', r'\\1', text)     # keep only 1 repeat\n    return re.sub(r'(.)\\1+', r'\\1\\1', text)  # keep 2 repeat\n\ndef document_features(document, corpus_features):\n    document_words = set(document)\n    features = {}\n    for word in corpus_features:\n        features['has({})'.format(word)] = (word in document_words)\n    return features","410e2fed":"pos_train_file = '..\/input\/train_Arabic_tweets_positive_20190413.tsv'\nneg_train_file = '..\/input\/train_Arabic_tweets_negative_20190413.tsv'\n\npos_test_file = '..\/input\/test_Arabic_tweets_positive_20190413.tsv'\nneg_test_file = '..\/input\/test_Arabic_tweets_negative_20190413.tsv'\nprint('data files')\nprint('train file (pos)', pos_train_file)\nprint('train file (neg)', neg_train_file)\nprint('test file (pos)', pos_test_file)\nprint('test file (neg)', neg_test_file)","ee3f9d14":"print('parameters')\nn = 2\nprint('n grams:', n)","f5dbc055":"print('loading train data ....')\npos_train_data, pos_train_feat = load_tsv(pos_train_file, n)\nneg_train_data, neg_train_feat = load_tsv(neg_train_file, n)\nprint('loading test data ....')\npos_test_data, pos_test_feat = load_tsv(pos_test_file, n)\nneg_test_data, neg_test_feat = load_tsv(neg_test_file, n)","38654ed0":"print('train data info')\ntrain_data = pos_train_data + neg_train_data\nprint('train data size', len(train_data))\nprint('# of positive', len(pos_train_data))\nprint('# of negative', len(neg_train_data))","c39dc353":"import random\nsample_size = 100\nprint('{} random tweets .... '.format(sample_size))\nfor s in random.sample(train_data, sample_size):\n    print(s)","d05de64b":"print('test data info')\ntest_data = pos_test_data + neg_test_data\nprint('test data size', len(train_data))\nprint('# of positive', len(pos_test_data))\nprint('# of negative', len(neg_test_data))","6770e16a":"print('merging all features ... ')\nall_features = pos_train_feat + neg_train_feat + \\\n               pos_test_feat + pos_test_feat\nprint('len(all_features):', len(all_features))","4f910c8c":"print('{} sample features ...'.format(sample_size))\nprint(random.sample(all_features, sample_size))","4f800a18":"all_features_count = {}\nfor w in all_features:\n    all_features_count[w] = all_features_count.get(w, 0) + 1","446a22ab":"print('sample frequencies')\nprint(random.sample(list(all_features_count.items()), 30))\nword = '\u0641\u064a'\nprint('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))\nword = '\u0641\u0649'\nprint('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))\nword = '\u0645\u0646'\nprint('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))","0e4e59c5":"print('size of training data:',  len(train_data))\nmin_df = int(0.002 * len(train_data))\nmax_df = int(0.98 * len(train_data))\nprint('min document frequency:', min_df)\nprint('max document frequency:', max_df)","8949d17d":"# remove features that have frequency below\/above the threshold\nmy_features = set([word for word, freq in all_features_count.items() if  max_df > freq > min_df ])\nprint(len(my_features), 'are kept out of', len(all_features))","7daa3fc6":"print('{} sample of selected features:'.format(sample_size))\nprint(random.sample(list(my_features), sample_size))","95cc99f9":"feature_sets = [(document_features(d, my_features), c) for (d, c) in train_data]","b107f44a":"classifier = nltk.NaiveBayesClassifier.train(feature_sets)\nprint('training is done')","6708c97d":"classifier.show_most_informative_features(40)","8d4e05c9":"test_features = [(document_features(d, my_features), c) for (d, c) in test_data]","298bc78e":"ref_sets = collections.defaultdict(set)\ntest_sets = collections.defaultdict(set)\n\nfor i, (feats, label) in enumerate(test_features):\n    ref_sets[label].add(i)\n    observed = classifier.classify(feats)\n    test_sets[observed].add(i)","eef366ad":"print('accuracy: ', nltk.classify.accuracy(classifier, test_features))\nprint('pos precision: ', precision(ref_sets['pos'], test_sets['pos']))\nprint('pos recall:', recall(ref_sets['pos'], test_sets['pos']))\nprint('neg precision: ', precision(ref_sets['neg'], test_sets['neg']))\nprint('neg recall:', recall(ref_sets['neg'], test_sets['neg']))\nprint('positive f-score:', f_measure(ref_sets['pos'], test_sets['pos']))\nprint('negative f-score:', f_measure(ref_sets['neg'], test_sets['neg']))","5e0d2543":"# classify test instances ","ce81c325":"# Sample Frequency","2d7c0224":"# Compute Threshold","87ff4fdd":"# Parameters (ngrams)","c7039f2d":"# Test data info","7d404994":"# Sample of selected features ","15709555":"# Selecting Features ","7b325210":"# Load corpus","10e991f5":"# Training data information","d03649ee":"# merging all features ...","c89e128d":"# Arabic Sentiment Analysis in tweets using Naive Bayes Machine learning Algorithm and bigram features","37dfaf7f":"# loading train data .... ","9e5b5336":"# compute frequencies","30605a92":"# training ...","ec9b2638":"# generating features for test documents ...","7bee69d9":"# Sample features ","15f8b480":"# Results ","3c9164d4":"# generating features for training documents ...","da86c95d":"# Most informative features ","6a53f181":"# define functions ","8c153ed5":"# Sample training data "}}