{"cell_type":{"c3bbefb1":"code","8f960326":"code","dd408743":"code","3906abbe":"code","ac753aab":"code","703e26eb":"code","7f705319":"code","762a4197":"code","b38af124":"code","13299417":"code","c8154cf6":"code","368decf7":"code","0503b2c9":"code","2ce2f43c":"code","5897c683":"code","fab4f8c1":"code","cbf5771a":"code","64ebb6be":"code","9e3bc876":"code","ed8214a2":"code","ce83ee07":"code","4c9d266c":"code","ad683044":"code","7bf3777d":"code","7103bc95":"code","15208873":"code","93f753e4":"code","f61787ae":"code","4686f0d6":"code","78b5738a":"code","0d4f6d1e":"code","20115f1b":"code","65c14d80":"code","6c55fab9":"code","9eb0a826":"code","d9d04310":"code","72d811f2":"code","f7eb32b1":"code","772eb34b":"code","708a6b02":"markdown","b61b9124":"markdown","53d3fd34":"markdown","210f6327":"markdown","f8917364":"markdown","d2cf6116":"markdown","524cb7fe":"markdown","94f0bc2f":"markdown","3e21e547":"markdown","22123915":"markdown","329e2412":"markdown","de10f137":"markdown","5999c9d8":"markdown","3314b207":"markdown","6e645b7a":"markdown","4e9ee41a":"markdown","f28c27cd":"markdown","90f7c478":"markdown","a3abb353":"markdown"},"source":{"c3bbefb1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","8f960326":"df=pd.read_csv('..\/input\/others\/Movie_regression.xls',header=0) #since our csv file has header at 0th row, we use header=o\ndf.head(10)","dd408743":"df.columns","3906abbe":"df.shape","ac753aab":"df['Genre'].unique()","703e26eb":"df.info()","7f705319":"df['Time_taken'].mean()","762a4197":"df['Time_taken'].fillna(value=df['Time_taken'].mean(),inplace=True) \n#we have filled the missing values with mean values","b38af124":"df.info()","13299417":"df.head()","c8154cf6":"# 3d and Genre are categorical \n# we will convert them into dummy variable\n","368decf7":"df=pd.get_dummies(df,columns=['3D_available','Genre'],drop_first=True) #drop_first = n-1 , \ndf.head()","0503b2c9":"df.shape","2ce2f43c":"X=df.loc[:,df.columns!='Collection']\ntype(X)","5897c683":"X.head()","fab4f8c1":"X.shape","cbf5771a":"y=df['Collection']\ntype(y)","64ebb6be":"y.head()","9e3bc876":"from sklearn.model_selection import train_test_split","ed8214a2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n# since we are randomly assigning our data into test and train , to get the same test data everytime, so that \n#we can compare the performmace of the data\n#if i keep random state the same, we will get the same train test split","ce83ee07":"X_train.head() #indexes are shuffled, ","4c9d266c":"X_train.shape","ad683044":"X_test.shape","7bf3777d":"from sklearn import tree\nregtree=tree.DecisionTreeRegressor(max_depth=3)\n# max depth = no of layers in our tree, we dont want to overfit, we use 3 . \n# Don't exceed beyond 5","7103bc95":"regtree.fit(X_train,y_train)","15208873":"y_train_pred=regtree.predict(X_train)\ny_test_pred=regtree.predict(X_test)","93f753e4":"y_test_pred","f61787ae":"from sklearn.metrics import mean_squared_error,r2_score","4686f0d6":"mean_squared_error(y_test,y_test_pred)\n# here we give test values and predicted values for y","78b5738a":"r2_score(y_train,y_train_pred)\n# the value obtained is 0.83 which means our model is performing great","0d4f6d1e":"# calculate r2 values on our test data\nr2_score(y_test,y_test_pred)\n\n#always look at your test r2 values to evaluate your model performance","20115f1b":"dot_data=tree.export_graphviz(regtree, out_file=None)","65c14d80":"from IPython.display import Image","6c55fab9":"import pydotplus","9eb0a826":"graph=pydotplus.graph_from_dot_data(dot_data)\nImage(graph.create_png())","d9d04310":"regtree1=tree.DecisionTreeRegressor(max_depth=3)\nregtree1.fit(X_train,y_train)\ndot_data=tree.export_graphviz(regtree1, out_file=None,feature_names=X_train.columns,filled=True) #filled = it will fill colors as per the conditon for the target variable = collection\ngraph1=pydotplus.graph_from_dot_data(dot_data)\nImage(graph1.create_png())\n","72d811f2":"regtree2=tree.DecisionTreeRegressor(min_samples_split=40)\nregtree2.fit(X_train,y_train)\ndot_data=tree.export_graphviz(regtree2, out_file=None,feature_names=X_train.columns,filled=True) \ngraph2=pydotplus.graph_from_dot_data(dot_data)\nImage(graph2.create_png())","f7eb32b1":"regtree3=tree.DecisionTreeRegressor(min_samples_leaf=25)\nregtree3.fit(X_train,y_train)\ndot_data=tree.export_graphviz(regtree3, out_file=None,feature_names=X_train.columns,filled=True) \ngraph3=pydotplus.graph_from_dot_data(dot_data)\nImage(graph3.create_png())","772eb34b":"regtree3=tree.DecisionTreeRegressor(min_samples_leaf=25,max_depth=4)\nregtree3.fit(X_train,y_train)\ndot_data=tree.export_graphviz(regtree3, out_file=None,feature_names=X_train.columns,filled=True) \ngraph3=pydotplus.graph_from_dot_data(dot_data)\nImage(graph3.create_png())","708a6b02":"# Dummy variable creation","b61b9124":"# We want to predict values for Collection (Y)","53d3fd34":"# Minimum observations at internal node","210f6327":"We have to convert all our categorical variables, into numerical variables.\nAnd we do that by Dummy variable","f8917364":"# Regression Tree: Continuos quantitative target variable : Predicting rainfall, marks, revenue,etc\n\n# Regression classifier : Discrete categorical variables : Predicting high or low, win or loss, healthy or unhealthy.","d2cf6116":"#  Predict values using trained model","524cb7fe":"# Pre-Prunning = controlling tree growth","94f0bc2f":"# Minimum observations at leaf node","3e21e547":"# Time taken has missing values, look at the table, there are only 494 values","22123915":"1. firstly we create a dot file\n2. convert dot file into an image.\n3. Then use that image to creat a graph.","329e2412":"# Model performance = mean squared error, r2 value(goodness value= lies between 0(no fit) and 1 (perfect fit))\n\n# 0.4 to 0.8 for good models and above foe excellent models => r2 values","de10f137":"# we can also add conidtions to the above graph, by limiting the number of layers using max_depth","5999c9d8":"# Max number of Levels in tree","3314b207":"# Plotting Decision Tree","6e645b7a":"# Training Regression Tree","4e9ee41a":"# Pruning a tree= cutting the parts of the tree which are not beneficial for us","f28c27cd":"# Misssing Values Imputation","90f7c478":"# X-y split","a3abb353":"# Train-Test Split"}}