{"cell_type":{"208bccdc":"code","8b6de664":"code","5b2acd5d":"code","eba3e355":"code","1ae43a26":"code","34e7524e":"code","4d4e8763":"code","338e5c8a":"code","fe63c8dc":"code","5b3eee47":"code","313abd0e":"code","4e7d211e":"code","05587133":"code","6c3d8af8":"code","9872c9ae":"code","39184ca1":"code","1577c219":"code","6bd21aa9":"code","eac596f0":"code","1fdc66b2":"code","c9b8a172":"code","a89f6420":"code","10a091d3":"markdown","8a422f7b":"markdown","07a18d00":"markdown","6d168530":"markdown","4d0586d8":"markdown","4395794c":"markdown","225e17eb":"markdown","235cbce8":"markdown","245f4be3":"markdown","21fdb827":"markdown","2761cddd":"markdown","aa14f1ae":"markdown","3008b672":"markdown","5a0a96a6":"markdown","2b5293e3":"markdown","04209f5e":"markdown","5873f742":"markdown","272ac73c":"markdown","584c463a":"markdown","54322330":"markdown","89995d50":"markdown","77ddf6a2":"markdown","3d8d67a1":"markdown","8bc16116":"markdown","e8182bc5":"markdown","3274dc0d":"markdown","604c2f62":"markdown","bf929e48":"markdown"},"source":{"208bccdc":"# Data Analysis\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# Neural Network Model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model\nfrom keras.layers import *\nfrom keras.callbacks import ModelCheckpoint\n\n# Evaluation\nfrom sklearn.metrics import confusion_matrix, classification_report","8b6de664":"train_generator = ImageDataGenerator(rotation_range = 360,\n                                     width_shift_range = 0.05,\n                                     height_shift_range = 0.05,\n                                     shear_range = 0.05,\n                                     zoom_range = 0.05,\n                                     horizontal_flip = True,\n                                     vertical_flip = True,\n                                     brightness_range = [0.75, 1.25],\n                                     rescale = 1.\/255,\n                                     validation_split = 0.2)","5b2acd5d":"IMAGE_DIR = \"\/kaggle\/input\/real-life-industrial-dataset-of-casting-product\/casting_data\/\"\nIMAGE_SIZE = (300, 300)\nBATCH_SIZE = 64\nSEED_NUMBER = 123\n\ngen_args = dict(target_size = IMAGE_SIZE,\n                color_mode = \"grayscale\",\n                batch_size = BATCH_SIZE,\n                class_mode = \"binary\",\n                classes = {\"ok_front\": 0, \"def_front\": 1},\n                shuffle = True,\n                seed = SEED_NUMBER)\n\ntrain_dataset = train_generator.flow_from_directory(directory = IMAGE_DIR + \"train\",\n                                                    subset = \"training\", **gen_args)\nvalidation_dataset = train_generator.flow_from_directory(directory = IMAGE_DIR + \"train\",\n                                                         subset = \"validation\", **gen_args)","eba3e355":"test_generator = ImageDataGenerator(rescale = 1.\/255)\ntest_dataset = test_generator.flow_from_directory(directory = IMAGE_DIR + \"test\",\n                                                  **gen_args)","1ae43a26":"image_data = [{\"data\": typ,\n               \"class\": name.split('\/')[0],\n               \"filename\": name.split('\/')[1]}\n              for dataset, typ in zip([train_dataset, validation_dataset, test_dataset], [\"train\", \"validation\", \"test\"])\n              for name in dataset.filenames]\nimage_df = pd.DataFrame(image_data)\ndata_crosstab = pd.crosstab(index = image_df[\"data\"],\n                            columns = image_df[\"class\"],\n                            margins = True,\n                            margins_name = \"Total\")\ndata_crosstab","34e7524e":"total_image = data_crosstab.iloc[-1,-1]\nax = data_crosstab.iloc[:-1,:-1].T.plot(kind = \"bar\", stacked = True, rot = 0)\n\npercent_val = []\n\nfor rect in ax.patches:\n    height = rect.get_height()\n    width = rect.get_width()\n    percent = 100*height\/total_image\n\n    ax.text(rect.get_x() + width - 0.25, \n            rect.get_y() + height\/2, \n            int(height), \n            ha = 'center',\n            va = 'center',\n            color = \"white\",\n            fontsize = 10)\n    \n    ax.text(rect.get_x() + width + 0.01, \n            rect.get_y() + height\/2, \n            \"{:.2f}%\".format(percent), \n            ha = 'left',\n            va = 'center',\n            color = \"black\",\n            fontsize = 10)\n    \n    percent_val.append(percent)\n\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles = handles, labels = labels)\n\npercent_def = sum(percent_val[::2])\nax.set_xticklabels([\"def_front\\n({:.2f} %)\".format(percent_def), \"ok_front\\n({:.2f} %)\".format(100-percent_def)])\nplt.title(\"IMAGE DATA PROPORTION\", fontsize = 15, fontweight = \"bold\")\nplt.show()","4d4e8763":"mapping_class = {0: \"ok\", 1: \"defect\"}\nmapping_class","338e5c8a":"def visualizeImageBatch(dataset, title):\n    images, labels = next(iter(dataset))\n    images = images.reshape(BATCH_SIZE, *IMAGE_SIZE)\n    fig, axes = plt.subplots(8, 8, figsize=(16,16))\n\n    for ax, img, label in zip(axes.flat, images, labels):\n        ax.imshow(img, cmap = \"gray\")\n        ax.axis(\"off\")\n        ax.set_title(mapping_class[label], size = 20)\n\n    plt.tight_layout()\n    fig.suptitle(title, size = 30, y = 1.05, fontweight = \"bold\")\n    plt.show()\n    \n    return images","fe63c8dc":"train_images = visualizeImageBatch(train_dataset,\n                                   \"FIRST BATCH OF THE TRAINING IMAGES\\n(WITH DATA AUGMENTATION)\")","5b3eee47":"test_images = visualizeImageBatch(test_dataset,\n                                  \"FIRST BATCH OF THE TEST IMAGES\\n(WITHOUT DATA AUGMENTATION)\")","313abd0e":"img = np.squeeze(train_images[4])[75:100, 75:100]\n\nfig = plt.figure(figsize = (15, 15))\nax = fig.add_subplot(111)\nax.imshow(img, cmap = \"gray\")\nax.axis(\"off\")\n\nw, h = img.shape\nfor x in range(w):\n    for y in range(h):\n        value = img[x][y]\n        ax.annotate(\"{:.2f}\".format(value), xy = (y,x),\n                    horizontalalignment = \"center\",\n                    verticalalignment = \"center\",\n                    color = \"white\" if value < 0.4 else \"black\")","4e7d211e":"model = Sequential(\n    [\n        # First convolutional layer\n        Conv2D(filters = 32,\n               kernel_size = 3,\n               strides = 2,\n               activation = \"relu\",\n               input_shape = IMAGE_SIZE + (1, )),\n        \n        # First pooling layer\n        MaxPooling2D(pool_size = 2,\n                     strides = 2),\n        \n        # Second convolutional layer\n        Conv2D(filters = 16,\n               kernel_size = 3,\n               strides = 2,\n               activation = \"relu\"),\n        \n        # Second pooling layer\n        MaxPooling2D(pool_size = 2,\n                     strides = 2),\n        \n        # Flattening\n        Flatten(),\n        \n        # Fully-connected layer\n        Dense(128, activation = \"relu\"),\n        Dropout(rate = 0.2),\n        \n        Dense(64, activation = \"relu\"),\n        Dropout(rate = 0.2),\n        \n        Dense(1, activation = \"sigmoid\")\n    ]\n)\n\nmodel.summary()","05587133":"model.compile(optimizer = \"adam\",\n              loss = \"binary_crossentropy\",\n              metrics = [\"accuracy\"])","6c3d8af8":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","9872c9ae":"STEPS = 150\n\ncheckpoint = ModelCheckpoint(\"cnn_casting_inspection_model.hdf5\",\n                             verbose = 1,\n                             save_best_only = True,\n                             monitor = \"val_loss\")\n\nmodel.fit_generator(generator = train_dataset,\n                    validation_data = validation_dataset,\n                    steps_per_epoch = STEPS,\n                    epochs = 25,\n                    validation_steps = STEPS,\n                    callbacks = [checkpoint],\n                    verbose = 1)","39184ca1":"plt.subplots(figsize = (8, 6))\nsns.lineplot(data = pd.DataFrame(model.history.history,\n                                 index = range(1, 1+len(model.history.epoch))))\nplt.title(\"TRAINING EVALUATION\", fontweight = \"bold\", fontsize = 20)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Metrics\")\n\nplt.legend(labels = ['val loss', 'val accuracy', 'train loss', 'train accuracy'])\nplt.show()","1577c219":"best_model = load_model(\"\/kaggle\/working\/cnn_casting_inspection_model.hdf5\")","6bd21aa9":"y_pred_prob = best_model.predict_generator(generator = test_dataset,\n                                           verbose = 1)","eac596f0":"THRESHOLD = 0.5\ny_pred_class = (y_pred_prob >= THRESHOLD).reshape(-1,)\ny_true_class = test_dataset.classes[test_dataset.index_array]\n\npd.DataFrame(\n    confusion_matrix(y_true_class, y_pred_class),\n    index = [[\"Actual\", \"Actual\"], [\"ok\", \"defect\"]],\n    columns = [[\"Predicted\", \"Predicted\"], [\"ok\", \"defect\"]],\n)","1fdc66b2":"print(classification_report(y_true_class, y_pred_class, digits = 4))","c9b8a172":"images, labels = next(iter(test_dataset))\nimages = images.reshape(BATCH_SIZE, *IMAGE_SIZE)\nfig, axes = plt.subplots(4, 4, figsize=(16,16))\n\nfor ax, img, label in zip(axes.flat, images, labels):\n    ax.imshow(img, cmap = \"gray\")\n    true_label = mapping_class[label]\n    \n    [[pred_prob]] = best_model.predict(img.reshape(1, *IMAGE_SIZE, -1))\n    pred_label = mapping_class[int(pred_prob >= THRESHOLD)]\n    \n    prob_class = 100*pred_prob if pred_label == \"defect\" else 100*(1-pred_prob)\n    \n    ax.set_title(f\"TRUE LABEL: {true_label}\", fontweight = \"bold\", fontsize = 18)\n    ax.set_xlabel(f\"PREDICTED LABEL: {pred_label}\\nProb({pred_label}) = {(prob_class):.2f}%\",\n                 fontweight = \"bold\", fontsize = 15,\n                 color = \"blue\" if true_label == pred_label else \"red\")\n    \n    ax.set_xticks([])\n    ax.set_yticks([])\n    \nplt.tight_layout()\nfig.suptitle(\"TRUE VS PREDICTED LABEL FOR 16 RANDOM TEST IMAGES\", size = 30, y = 1.03, fontweight = \"bold\")\nplt.show()","a89f6420":"misclassify_pred = np.nonzero(y_pred_class != y_true_class)[0]\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\n\nfor ax, batch_num, image_num in zip(axes.flat, misclassify_pred \/\/ BATCH_SIZE, misclassify_pred % BATCH_SIZE):\n    images, labels = test_dataset[batch_num]\n    img = images[image_num]\n    ax.imshow(img.reshape(*IMAGE_SIZE), cmap = \"gray\")\n    \n    true_label = mapping_class[labels[image_num]]\n    [[pred_prob]] = best_model.predict(img.reshape(1, *IMAGE_SIZE, -1))\n    pred_label = mapping_class[int(pred_prob >= THRESHOLD)]\n    \n    prob_class = 100*pred_prob if pred_label == \"defect\" else 100*(1-pred_prob)\n    \n    ax.set_title(f\"TRUE LABEL: {true_label}\", fontweight = \"bold\", fontsize = 18)\n    ax.set_xlabel(f\"PREDICTED LABEL: {pred_label}\\nProb({pred_label}) = {(prob_class):.2f}%\",\n                 fontweight = \"bold\", fontsize = 15,\n                 color = \"blue\" if true_label == pred_label else \"red\")\n    \n    ax.set_xticks([])\n    ax.set_yticks([])\n    \nplt.tight_layout()\nfig.suptitle(f\"MISCLASSIFIED TEST IMAGES ({len(misclassify_pred)} out of {len(y_true_class)})\",\n             size = 20, y = 1.03, fontweight = \"bold\")\nplt.show()","10a091d3":"For each epoch, `batch_size` $\\times$ `steps_per_epoch` images will be fed into our CNN architecture. In this case, we specify the `steps_per_epoch` to be 150 so for each epoch `64 * 150 = 9600` augmented images from the training dataset will be fed. We let the model train for 25 `epochs`.\n\nBy using `ModelCheckpoint`, the best model will be automatically saved if the current `val_loss` is lower than the previous one.","8a422f7b":"These are the example of values that we are going to feed into our CNN architecture.","07a18d00":"We will proceed to the next step, since the proportion of data can be considered as balanced.","6d168530":"## Define Architecture\n\nHere is the detailed architecture that we are going to use:\n1. **First convolutional layer**: consists of 32 `filters` with `kernel_size` matrix 3 by 3. Using 2-pixel `strides` at a time, reduce the image size by half.\n2. **First pooling layer**: Using max-pooling matrix 2 by 2 (`pool_size`) and 2-pixel `strides` at a time further reduce the image size by half.\n3. **Second convolutional layer**: Just like the first convolutional layer but with 16 `filters` only.\n4. **Second pooling layer**: Same as the first pooling layer.\n5. **Flattening**: Convert two-dimensional pixel values into one dimension, so that it is ready to be fed into the fully-connected layer.\n6. **First dense layer + Dropout**: consists of 128 `units` and 1 bias unit. Dropout of `rate` 20% is used to prevent overfitting.\n7. **Second dense layer + Dropout**: consists of 64 `units` and 1 bias unit. Dropout of `rate` 20% is also used to prevent overfitting.\n8. **Output layer**: consists of only one `unit` and `activation` is a sigmoid function to convert the scores into a probability of an image being `defect`.\n\nFor every layer except output layer, we use Rectified Linear Unit (ReLU) `activation` function as follow:\n\n<img src = \"https:\/\/raw.githubusercontent.com\/udacity\/deep-learning-v2-pytorch\/master\/convolutional-neural-networks\/conv-visualization\/notebook_ims\/relu_ex.png\" height=50% width=50%>","4d0586d8":"The model achieves 98.21% accuracy on training dataset and 98.42% on validation dataset.","4395794c":"# CONVOLUTIONAL NEURAL NETWORK (CNN) FOR CASTING PRODUCT QUALITY INSPECTION","225e17eb":"## Data Augmentation\n\nWe apply on-the-fly data augmentation, a technique to expand the training dataset size by creating a modified version of the original image which can improve model performance and the ability to generalize. This can be achieved by using `ImageDataGenerator` provided by `keras` with the following parameters:\n\n- `rotation_range`: Degree range for random rotations. We choose 360 degrees since the product is a round object.\n- `width_shift_range`: Fraction range of the total width to be shifted.\n- `height_shift_range`: Fraction range of the total height to be shifted.\n- `shear_range`: Degree range for random shear in a counter-clockwise direction.\n- `zoom_range`: Fraction range for random zoom.\n- `horizontal_flip` and `vertical_flip` are set to `True` for randomly flip image horizontally and vertically.\n- `brightness_range`: Fraction range for picking a brightness shift value.\n\nOther parameters:\n- `rescale`: Eescale the pixel values to be in range 0 and 1.\n- `validation_split`: Reserve 20% of the training data for validation, and the rest 80% for model fitting.","235cbce8":"We will not perform any data augmentation on the test data.","245f4be3":"## Image Data Proportion\nWe successfully load and apply on-the-fly data augmentation according to the specified parameters. Now, let's take a look on how is the proportion of the `train`, `validation`, and `test` image for each class.","21fdb827":"Out of 715 test images, only 4 images are being misclassified. ","2761cddd":"Since the proportion of correctly classified images is very large, let's also visualize the misclassified only.","aa14f1ae":"## Visualize the Results\n\nLastly, we visualize the results by comparing its true label with the predicted label and also provide the probability of each image being on the predicted class. A <font color=blue><b>blue color<\/b><\/font> on the text indicates that our model correctly classify the image, otherwise <font color=red><b>red color<\/b><\/font> is used.","3008b672":"# Visualize the Image\nIn this section, we visualize the image to make sure that it is loaded correctly.","5a0a96a6":"We define another set of value for the `flow_from_directory` parameters:\n- `IMAGE_DIR`: The directory where the image data is stored.\n- `IMAGE_SIZE`: The dimension of the image (300 px by 300 px).\n- `BATCH_SIZE`: Number of images that will be loaded and trained at one time.\n- `SEED_NUMBER`: Ensure reproducibility.\n\n- `color_mode = \"grayscale\"`: Treat our image with only one channel color.\n- `class_mode` and `classes` define the target class of our problem. In this case, we denote the `defect` class as positive (1), and `ok` as a negative class.\n- `shuffle = True` to make sure the model learns the `defect` and `ok` images alternately.","2b5293e3":"## Training Evaluation\n\nLet's plot both `loss` and `accuracy` metrics for `train` and `validation` data based on each `epoch`.","04209f5e":"## Visualize Image in Batch\n\nVisualize the first batch (`BATCH_SIZE = 64`) of the training dataset (images with data augmentation) and also the test dataset (images without data augmentation).","5873f742":"# Load the Images\n\nHere is the structure of our folder containing image data:\n\n    casting_data\n    \u251c\u2500\u2500\u2500test\n    \u2502   \u251c\u2500\u2500\u2500def_front\n    \u2502   \u2514\u2500\u2500\u2500ok_front\n    \u2514\u2500\u2500\u2500train\n        \u251c\u2500\u2500\u2500def_front\n        \u2514\u2500\u2500\u2500ok_front\n     \nThe folder `casting_data` consists of two subfolders `test` and `train` in which each of them has another subfolder: `def_front` and `ok_front` denoting the class of our target variable. The images inside `train` will be used for model fitting and validation, while `test` will be used purely for testing the model performance on unseen images.","272ac73c":"The output of the prediction is in the form of probability. We use `THRESHOLD = 0.5` to separate the classes. If the probability is greater or equal to the `THRESHOLD`, then it will be classified as `defect`, otherwise `ok`.","584c463a":"# Testing on Unseen Images\n\nOur model performs very well on the training and validation dataset which uses augmented images. Now, we test our model performance with unseen and unaugmented images.","54322330":"# Import Libraries\nAs usual, before we begin any analysis and modeling, let's import several necessary libraries to work with the data.","89995d50":"## Model Fitting\nBefore we do model fitting, let's check whether GPU is available or not.","77ddf6a2":"## Visualize Detailed Image\nLet's also take a look on the detailed image by each pixel. Instead of plotting 300 pixels by 300 pixels (which computationally expensive), we take a small part of 25 pixels by 25 pixels only.","3d8d67a1":"## Compile the Model\nNext, we specify how the model backpropagates or update the weights after each batch feed-forward. We use adam `optimizer` and a `loss` function binary cross-entropy since we are dealing with binary classification problem. The `metrics` used to monitor the training progress is accuracy.","8bc16116":"According to the problem statement, we want to minimize the case of False Negative, where the `defect` product is misclassified as `ok`. This can cause the whole order to be rejected and create a big loss for the company. Therefore, in this case, we prioritize Recall over Precision.\n\nBut if we take into account the cost of re-casting a product, we have to minimize the case of False Positive also, where the `ok` product is misclassified as `defect`. Therefore we can prioritize the F1 score which combines both Recall and Precision.\n\nOn test dataset, the model achieves a very good result as follow:\n- Accuracy: 99.44%\n- Recall: 99.78%\n- Precision: 99.34%\n- F1 score: 99.56%","e8182bc5":"# Training the Network\n\nAs mentioned earlier, we are going to train a CNN model to classify the casting product image. CNN is used as an automatic feature extractor from the images so that it can learn how to distinguish between `defect` and `ok` casted products. It effectively uses the adjacent pixel to downsample the image and then use a prediction (fully-connected) layer to solve the classification problem. This is a simple illustration by [Udacity](https:\/\/github.com\/udacity\/deep-learning-v2-pytorch) on how the layers are arranged sequentially:\n\n<img src = \"https:\/\/raw.githubusercontent.com\/udacity\/deep-learning-v2-pytorch\/master\/convolutional-neural-networks\/conv-visualization\/notebook_ims\/CNN_all_layers.png\" height=50% width=50%>","3274dc0d":"Casting is a manufacturing process in which liquid material is poured into a mold to solidify. Many types of defects or unwanted irregularities can occur during this process. The industry has its quality inspection department to remove defective products from the production line, but this is very time consuming since it is carried out manually. Furthermore, there is a chance of misclassifying due to human error, causing rejection of the whole product order.\n\nIn this notebook, let us automate the inspection process by training top-view images of a casted submersible pump impeller using a Convolutional Neural Network (CNN) so that it can distinguish accurately between `defect` from the `ok` one.\n\nWe will break down into several steps:\n\n1. Load the images and apply the data augmentation technique\n\n2. Visualize the images\n\n3. Training with validation: define the architecture, compile the model, model fitting and evaluation\n    \n4. Testing on unseen images\n\n5. Make a conclusion","604c2f62":"# Conclusion\n\nBy using CNN and on-the-fly data augmentation, the performance of our model in training, validation, and test images is almost perfect, reaching 98-99% accuracy and F1 score. We can utilize this model by embedding it into a surveillance camera where the system can automatically separate defective product from the production line. This method surely can reduce human error and human resources on manual inspection, but it still needs supervision from human since the model is not 100% correct at all times.","bf929e48":"We can conclude that the model is **not overfitting** the data since both `train loss` and `val loss` simultaneously dropped towards zero. Also, both `train accuracy` and `val accuracy` increase towards 100%."}}