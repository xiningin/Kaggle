{"cell_type":{"9d14a022":"code","2dbb4a39":"code","103da492":"code","0693bf40":"code","26bfe6a2":"code","d4057cd3":"code","46190ccd":"code","646fd6e9":"code","b1103bc1":"code","a4822cc3":"code","4ccd3354":"code","c99eb224":"code","98bec95e":"code","75d27b5d":"code","25e0e853":"code","c42e13c8":"code","547076e5":"code","ba6da49c":"code","6adf5022":"code","d4175080":"code","3bc89d89":"code","a6678b2c":"code","8e9434c3":"code","7e86611e":"code","216041da":"code","5b22b0f0":"code","fe0ea8a3":"code","03035dce":"code","6d11e987":"code","9edc90ae":"code","f55c9453":"code","695fdf76":"code","c0e1179b":"code","99f6b54b":"code","6b8fe7ea":"markdown","626ae39a":"markdown","1f297647":"markdown","0c0a8ae6":"markdown","2b235ee5":"markdown","0f8f6ffc":"markdown"},"source":{"9d14a022":"import pandas as pd","2dbb4a39":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\ntrain_data = train.copy()\ntest_data = test.copy()","103da492":"train_data.head()","0693bf40":"train_data = train_data.set_index('id', drop = True)","26bfe6a2":"print(train_data.shape)\ntrain_data.head()","d4057cd3":"print(test_data.shape)\ntest_data.head()","46190ccd":"# checking for missing values\ntrain_data.isnull().sum()","646fd6e9":"# dropping missing values from text columns alone. \ntrain_data[['title', 'author']] = train_data[['title', 'author']].fillna(value = 'Missing')\ntrain_data = train_data.dropna()\ntrain_data.isnull().sum()","b1103bc1":"length = []\n[length.append(len(str(text))) for text in train_data['text']]\ntrain_data['length'] = length\ntrain_data.head()","a4822cc3":"min(train_data['length']), max(train_data['length']), round(sum(train_data['length'])\/len(train_data['length']))","4ccd3354":"len(train_data[train_data['length'] < 50])","c99eb224":"train_data['text'][train_data['length'] < 50]","98bec95e":"# dropping the outliers\ntrain_data = train_data.drop(train_data['text'][train_data['length'] < 50].index, axis = 0)","75d27b5d":"min(train_data['length']), max(train_data['length']), round(sum(train_data['length'])\/len(train_data['length']))","25e0e853":"##importing tensorflow and looking into the version of it\nimport tensorflow as tf\ntf.__version__","c42e13c8":"## import all necessaries\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout, RNN, SpatialDropout1D","547076e5":"## voc size\nvoc_size =  4500","ba6da49c":"tokenizer = Tokenizer(num_words = voc_size, filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\ntokenizer.fit_on_texts(texts = train_data['text'])\nX = tokenizer.texts_to_sequences(texts = train_data['text'])","6adf5022":"# now applying padding to make them even shaped.\nX = pad_sequences(sequences = X, maxlen = voc_size, padding = 'pre')","d4175080":"print(X.shape)\ny = train_data['label'].values\nprint(y.shape)","3bc89d89":"# splitting the data training data for training and validation.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)","a6678b2c":"##creating model\nembdding_vector_features = 40\nmodel = Sequential()\nmodel.add(Embedding(voc_size,embdding_vector_features, input_length = 20))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy',optimizer = 'adam', metrics =  ['accuracy'])\nprint(model.summary())","8e9434c3":"## model training \nmodel_fit = model.fit(X_train, y_train, epochs = 1)","7e86611e":"print(test.shape)\ntest_data = test.copy()\nprint(test_data.shape)","216041da":"test_data = test_data.set_index('id', drop = True)\ntest_data.shape","5b22b0f0":"test_data = test_data.fillna(' ')\nprint(test_data.shape)\ntest_data.isnull().sum()","fe0ea8a3":"tokenizer.fit_on_texts(texts = test_data['text'])\ntest_text = tokenizer.texts_to_sequences(texts = test_data['text'])","03035dce":"test_text = pad_sequences(sequences = test_text, maxlen = voc_size, padding = 'pre')\n","6d11e987":"pred = model.predict_classes(test_text)","9edc90ae":"pred\n","f55c9453":"pred = pred[:, 0]\nsubmission = pd.DataFrame({'id':test_data.index, 'label':pred})\nsubmission.shape","695fdf76":"pred","c0e1179b":"submission.head()","99f6b54b":"submission.to_csv('submission.csv', index = False)","6b8fe7ea":"# Woohooo!!! lets train our model....","626ae39a":"looking to data ","1f297647":"Performance matrix and accuracy","0c0a8ae6":"**Lets create our model......**","2b235ee5":"Import data to a pandas data frame","0f8f6ffc":"Dropping the NaN values"}}