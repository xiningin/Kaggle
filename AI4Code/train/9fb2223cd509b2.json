{"cell_type":{"8a64f75a":"code","224ebf54":"code","b72d0812":"code","32d63e95":"code","2df0667e":"code","b2854d5d":"code","8f545655":"code","404d0ea9":"code","cd120ae9":"code","d0e77f8b":"code","723e4bf5":"code","d1c4462b":"code","04dc941f":"code","e238f59a":"code","680028d7":"code","513ce618":"code","7410c652":"code","6bc73dd2":"code","c282154d":"code","12e6ccfb":"code","435e2a47":"code","a680ffd4":"code","cdc98ee8":"code","984007bd":"code","b856189f":"code","541402b4":"code","f22ff2aa":"code","f0e48e3b":"code","b8e30cb2":"code","939f629f":"code","12b9c444":"code","ff55a63a":"code","ee431a6f":"code","53d99ebe":"code","af8fc5fb":"code","61726086":"code","b828de22":"code","1cb13298":"code","e14754fb":"code","d052e1da":"markdown","07249c8e":"markdown"},"source":{"8a64f75a":"### importing libraries\nimport numpy as np\nimport scipy.stats as stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set(font_scale=1.5)\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline","224ebf54":"#uploading the data and check its head\ndata= pd.read_csv('..\/input\/kc-housesales-data\/kc_house_data.csv')\ndata.head(3)","b72d0812":"data.shape","32d63e95":"#checking the data types of the columns and the number of missing values\ndata.info()","2df0667e":"#check if the zipcode has rapition which lead to it will have effect on the result\ndata['zipcode'].value_counts().head()","b2854d5d":"#after looking at the data info, the lat and long and id should be droped as the location is not defined with refrence to city or any other refrence\n# the date should be converted as a month and year columns\n#zipcode should be converted as catagory as so as the condition column","8f545655":"#deleting columns\ndel data['id']; del data['lat'];del data['long']","404d0ea9":"#extracting the year and month of the selling dates\ndata['date']= pd.to_datetime(data['date']) \ndata['year']= (pd.DatetimeIndex(data['date']).year)\ndata['month']= (pd.DatetimeIndex(data['date']).month)\ndel data['date']","cd120ae9":"#fixing the data types\ndata['year']=data['year'].astype(int)\ndata['month']= data['month'].astype(str)\ndata['zipcode']=data['zipcode'].astype(str)\ndata['condition']=data['condition'].astype(str)","d0e77f8b":"#convert the date of renoved as int type 1 for it has and 0 for nune\n# data['yr_renovated']=(data['yr_renovated']>0).astype(int) \n#or \ndata['yr_renovated'] = data['yr_renovated'].apply(lambda x : 1 if x>0 else 0)\n #this command will convert the numbers to boolen based on the condition and the results are True or False and back to integer","723e4bf5":"#make sure the data is ready \ndata.info()","d1c4462b":"data.head(3)","04dc941f":"#feature Engineering: it is about creating new columns that may help to find direct relation between the goal and the data\ndata['age']=data['year']-data['yr_built']\ndata['sqft_with_basembent']=data['sqft_above']+data['sqft_basement']\ndata['sqft']=data['sqft_living']+data['sqft_lot']\ndata['sqft15']=data['sqft_living15']+data['sqft_lot15']","e238f59a":"#convert the data to X for data given and Y for the price column which is our Target\ny = data.pop('price')\n#convert objects columns to binary columns by getting dummy values for them\nX = pd.get_dummies(data, drop_first=True)","680028d7":"#apply spiliting for the data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,  random_state=42)","513ce618":"#apply standard scaling. and import cross validation function\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","7410c652":"#import linear regretion models\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV","6bc73dd2":"#applying Linear Regression\nmodel = LinearRegression()\n\nscores = cross_val_score(model, X_train, y_train, cv=20)\n\nprint(\"Mean cross-validated training score:\", scores.mean())\n\n# fit and evaluate the data on the whole training set\nmodel.fit( X_train, y_train)\nprint(\"Training Score:\", model.score( X_train, y_train))\nprint(\"Test Score:\", model.score( X_test, y_test))\nL_R_S=model.score( X_test, y_test)","c282154d":"#applying Ridge\nmodel = Ridge()\n\nscores = cross_val_score(model, X_train, y_train, cv=20)\n\nprint(\"Mean cross-validated training score:\", scores.mean())\n\n# fit and evaluate the data on the whole training set\nmodel.fit( X_train, y_train)\nprint(\"Training Score:\", model.score( X_train, y_train))\nprint(\"Test Score:\", model.score( X_test, y_test))\nRidge_S=model.score( X_test, y_test)","12e6ccfb":"#applying RidgeCV as the model\nmodel = RidgeCV( normalize=True, cv=20)\n\nscores = cross_val_score(model, X_train, y_train, cv=20)\n\nprint(\"Mean cross-validated training score:\", scores.mean())\n\n# fit and evaluate the data on the whole training set\nmodel.fit( X_train, y_train)\nprint(\"Training Score:\", model.score( X_train, y_train))\nprint(\"Test Score:\", model.score( X_test, y_test))\nRidge_CV_S=model.score( X_test, y_test)","435e2a47":"##Another lasso try \nmodel = LassoCV(alphas=np.logspace(-50,100, 5), cv=5) \nmodel.fit(X_train, y_train)\nprint('Best alpha:', model.alpha_)\nprint('Training score:', model.score(X_train, y_train))\nprint(\"Test Score:\", model.score( X_test, y_test))","a680ffd4":"#applying lasso as the model\nmodel = Lasso()\n\nscores = cross_val_score(model, X_train, y_train, cv=20)\n\nprint(\"Mean cross-validated training score:\", scores.mean())\n\n# fit and evaluate the data on the whole training set\nmodel.fit( X_train, y_train)\nprint(\"Training Score:\", model.score( X_train, y_train))\nprint(\"Test Score:\", model.score( X_test, y_test))\nLasso_S=model.score( X_test, y_test)","cdc98ee8":"#applying lassoCV as the model\nmodel = LassoCV(normalize=True,cv=20)\n\nscores = cross_val_score(model, X_train, y_train, cv=20)\n\nprint(\"Mean cross-validated training score:\", scores.mean())\n\n# fit and evaluate the data on the whole training set\nmodel.fit( X_train, y_train)\nprint(\"Training Score:\", model.score( X_train, y_train))\nprint(\"Test Score:\", model.score( X_test, y_test))\nLasso_CV_S=model.score( X_test, y_test)","984007bd":"Models=['Linear Regression','Ridge','RidgeCV','Lasso','LassoCV']\nscores=[L_R_S,Ridge_S,Ridge_CV_S,Lasso_S,Lasso_CV_S]\nfig = plt.figure()\nax = fig.add_axes([0,0,3,1])\nax.bar(Models,scores)\nplt.show()\n","b856189f":"#apply Decision Tree Regressor,\nfrom sklearn.tree import DecisionTreeRegressor\ndtr1 = DecisionTreeRegressor(max_depth=1) # change depth to 5 and see the difference\ndtr2 =DecisionTreeRegressor(max_depth=2) \ndtr3 = DecisionTreeRegressor(max_depth=3)\ndtr4 = DecisionTreeRegressor(max_depth=None) \n\n# fit the 4 models\ndtr1.fit(X_train, y_train)\ndtr2.fit(X_train, y_train)\ndtr3.fit(X_train, y_train)\ndtr4.fit(X_train, y_train)\n\ndtr1_scores = cross_val_score(dtr1,X_train, y_train, cv=20)\ndtr2_scores = cross_val_score(dtr2, X_train, y_train, cv=20)\ndtr3_scores =cross_val_score(dtr3,X_train, y_train, cv=20)\ndtrN_scores =cross_val_score(dtr4, X_train, y_train, cv=20)\nprint('the score of deciton tree for the depth of one to three and None')\nprint (dtr1_scores.mean() ,dtr2_scores.mean(), dtr3_scores.mean() ,dtrN_scores.mean())","541402b4":"#apply KNN modelto the train\nfrom sklearn.neighbors import KNeighborsRegressor\nknr = KNeighborsRegressor(n_neighbors=7)\nknr.fit(X_train, y_train)\nknr_scores = cross_val_score(knr,X_train, y_train, cv=20)\nprint(knr_scores.mean())","f22ff2aa":"# KNeighborsRegressor + evaluation\nfrom sklearn import metrics\nKs =20\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    \n    knn_model = KNeighborsRegressor(n_neighbors = n).fit(X_train,y_train)\n    mean_acc[n-1] = knn_model.score(X_test, y_test)\n    yhat=knn_model.predict(X_test)\n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\n\nmean_acc \n\nplt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.legend(('Accuracy ', '+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()\nprint( \"The best test accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1)","f0e48e3b":"knn_model = KNeighborsRegressor(n_neighbors = 6).fit(X_train,y_train)\nprint(\"Training Score:\", knn_model.score( X_train, y_train))\nprint(\"Test Score:\", knn_model.score( X_test, y_test))","b8e30cb2":"knn_model = KNeighborsRegressor(n_neighbors = n).fit(X_train,y_train)\nyhat=knn_model.predict(X_test)\nprint(len(yhat))","939f629f":"print(type(yhat))","12b9c444":"print(type(yhat))","ff55a63a":"print(len(np.array(y_test)))","ee431a6f":"X_train.shape","53d99ebe":"#RandomForest without grid search\nfrom sklearn.ensemble import RandomForestRegressor\nforest_cv = RandomForestRegressor(n_jobs=-1)\nforest_cv.fit(X_train,y_train)\nprint(\"Training Score:\", forest_cv.score( X_train, y_train))\nprint(\"Test Score:\", forest_cv.score( X_test, y_test))","af8fc5fb":"#Random forest with grid search\nfrom sklearn.model_selection import GridSearchCV\ndef warn(*args, **kwargs): #disable warnings\n    pass\nimport warnings\nwarnings.warn = warn\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 1000]\n}\nrf = RandomForestRegressor()\nclf = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\nclf.fit(X_train, y_train)\nrf = clf.best_estimator_\nrf = rf.fit(X_train, y_train) \nrf.score(X_train,y_train)\n\n","61726086":"# AdaBoostRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nregr = AdaBoostRegressor(random_state=0, n_estimators=100)\nregr.fit(X_train, y_train)\npredicted_regr = regr.predict(X_train)\nprint(\"Test Score:\", model.score( X_test, y_test))\nprint(regr.feature_importances_)","b828de22":"from sklearn.decomposition import PCA\npca = PCA(n_components=100)\npca.fit(X_train)\npca_transformed = pca.fit_transform(X_train)\noriginal_pca = pd.DataFrame(data = pca.components_)\noriginal_pca","1cb13298":"pca.explained_variance_ratio_.sum() ","e14754fb":"def plot_pca(pca):\n    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15, 4))\n    size = len(pca.explained_variance_ratio_)\n    ax1.bar(range(size), pca.explained_variance_ratio_)\n    ax2.plot(range(size), np.cumsum(pca.explained_variance_ratio_), '--')\n    plt.show()\nplot_pca(pca)","d052e1da":"all the results are not as good as the models above","07249c8e":"the accurecy of all models almost the same, however Ridge and Lasso are slightly better"}}