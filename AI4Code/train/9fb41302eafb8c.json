{"cell_type":{"e9602526":"code","226d3482":"code","81251eb7":"code","7d5fca35":"code","0d961951":"code","8f0d4c18":"code","1dba617a":"code","8b5c4d3e":"code","e44bf507":"code","28eb6923":"code","8a4d9b1b":"code","dfc3c94f":"code","2c17a1ed":"code","23a73521":"code","0179be8e":"code","30ea8b83":"code","f307da9a":"code","c42f5b70":"code","0107f136":"code","5471b530":"code","63e5b2bb":"code","4832383b":"code","f90c5659":"code","b8ab7b01":"code","cff7689b":"code","5e57095b":"code","21266ee1":"markdown","888b812a":"markdown","75c70f51":"markdown","1ef5e44b":"markdown","ab9cf2f4":"markdown","031d2bf5":"markdown","8879b387":"markdown","6dcb760e":"markdown","4c106bf5":"markdown","8aa240cb":"markdown","b8d7c91b":"markdown","e32a0918":"markdown","c5edc5e3":"markdown","5fdf211d":"markdown","cc4cef4e":"markdown","e08272b5":"markdown","c892e96c":"markdown","f993c85e":"markdown","231cc9d0":"markdown","b9e67c24":"markdown","856fe764":"markdown","09d8fb0e":"markdown","dfe7a2b3":"markdown"},"source":{"e9602526":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime\nimport gc\nimport numpy as np\nimport os\nimport pandas as pd\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(action='ignore',category = DeprecationWarning)\nwarnings.simplefilter(action='ignore',category = DeprecationWarning)\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold, RepeatedKFold, GroupKFold\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import ADASYN\nimport category_encoders as ce\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nfrom time import time\n\nimport scipy.stats as st\nfrom sklearn.pipeline import Pipeline\nfrom tempfile import mkdtemp\nfrom shutil import rmtree\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nimport xgboost as xgb\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\n\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings(record=True) as w:\n    # Cause all warnings to always be triggered.\n    warnings.simplefilter(\"always\")\n    # Trigger a warning.\n    fxn()\n    # Verify some things\n    assert len(w) == 1\n    assert issubclass(w[-1].category, DeprecationWarning)\n    assert \"deprecated\" in str(w[-1].message)\n\n# Any results you write to the current directory are saved as output.","226d3482":"def dprint(*args, **kwargs):\n    print(\"[{}] \".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")) + \\\n        \" \".join(map(str,args)), **kwargs)\n\nid_name = 'Id'\ntarget_name = 'Target'","81251eb7":"# Load data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","7d5fca35":"train['is_test'] = 0\ntest['is_test'] = 1\ndf_all = pd.concat([train, test], axis=0)\n","0d961951":"dprint('Clean features...')\ncols = ['dependency']\nfor c in tqdm(cols):\n    x = df_all[c].values\n    strs = []\n    for i, v in enumerate(x):\n        try:\n            val = float(v)\n        except:\n            strs.append(v)\n            val = np.nan\n        x[i] = val\n    strs = np.unique(strs)\n\n    for s in strs:\n        df_all[c + '_' + s] = df_all[c].apply(lambda x: 1 if x == s else 0)\n\n    df_all[c] = x\n    df_all[c] = df_all[c].astype(float)\ndprint(\"Done.\")","8f0d4c18":"dprint(\"Extracting features...\")\ndef extract_features(df):\n    df['bedrooms_to_rooms'] = df['bedrooms']\/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']\/df['rooms']\n    df['rent_to_bedrooms'] = df['v2a1']\/df['bedrooms']\n    df['tamhog_to_rooms'] = df['tamhog']\/df['rooms'] # tamhog - size of the household\n    df['tamhog_to_bedrooms'] = df['tamhog']\/df['bedrooms']\n    df['r4t3_to_tamhog'] = df['r4t3']\/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']\/df['rooms'] # r4t3 - Total persons in the household\n    df['r4t3_to_bedrooms'] = df['r4t3']\/df['bedrooms']\n    df['rent_to_r4t3'] = df['v2a1']\/df['r4t3']\n    df['v2a1_to_r4t3'] = df['v2a1']\/(df['r4t3'] - df['r4t1'])\n    df['hhsize_to_rooms'] = df['hhsize']\/df['rooms']\n    df['hhsize_to_bedrooms'] = df['hhsize']\/df['bedrooms']\n    df['rent_to_hhsize'] = df['v2a1']\/df['hhsize']\n    df['qmobilephone_to_r4t3'] = df['qmobilephone']\/df['r4t3']\n    df['qmobilephone_to_v18q1'] = df['qmobilephone']\/df['v18q1']\n    \n\nextract_features(train)\nextract_features(test)\ndprint(\"Done.\")         ","1dba617a":"from sklearn.preprocessing import LabelEncoder\n\ndef encode_data(df):\n   \n    yes_no_map = {'no': 0, 'yes': 1}\n    \n    df['dependency'] = df['dependency'].replace(yes_no_map).astype(np.float32)\n    \n    df['edjefe'] = df['edjefe'].replace(yes_no_map).astype(np.float32)\n    df['edjefa'] = df['edjefa'].replace(yes_no_map).astype(np.float32)\n    \n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n","8b5c4d3e":"dprint(\"Encoding Data....\")\nencode_data(train)\nencode_data(test)\ndprint(\"Done...\")","e44bf507":"def do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                 #('', '', ''),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] \/ df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean'],\n                'escolari': ['min', 'max', 'mean']\n               }\n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean', 'count']\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n    # do something advanced above...\n    \n    # Drop SQB variables, as they are just squres of other vars \n    df.drop([f_ for f_ in df.columns if f_.startswith('SQB') or f_ == 'agesq'], axis=1, inplace=True)\n    # Drop id's\n    df.drop(['Id', 'idhogar'], axis=1, inplace=True)\n    # Drop repeated columns\n    df.drop(['hhsize', 'female', 'area2'], axis=1, inplace=True)\n    return df\n    ","28eb6923":"dprint(\"Do_feature Engineering....\")\ntrain = do_features(train)\ntest = do_features(test)\ndprint(\"Done....\")","8a4d9b1b":"dprint(\"Fill Na value....\")\ntrain = train.fillna(0)\ntest = test.fillna(0)\ndprint(\"Done....\")","dfc3c94f":"train.shape,test.shape","2c17a1ed":"cols_to_drop = [\n    id_name, \n    target_name,\n]\nX = train.drop(cols_to_drop, axis=1, errors='ignore')\ny = train[target_name].values\n","23a73521":"X.shape,y.shape","0179be8e":"# A parameter grid for XGBoost\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\nxgb = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='multi:softmax',booster='gbtree',\n                    silent=True, nthread=1)\n\nfolds = 3\nparam_comb = 5\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 42)","30ea8b83":"random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='accuracy', n_jobs=4, cv=skf.split(X,y), verbose=0, random_state=1001 )\nrandom_search.fit(X, y)","f307da9a":"print('\\n All results:')\nprint(random_search.cv_results_)\nprint('\\n Best estimator:')\nprint(random_search.best_estimator_)\nprint('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\nprint(random_search.best_score_ * 2 - 1)\nprint('\\n Best hyperparameters:')\nprint(random_search.best_params_)\nresults = pd.DataFrame(random_search.cv_results_)\nresults.to_csv('xgb-random-grid-search-results-01.csv', index=False)","c42f5b70":"y_test = random_search.predict(test)\nsub = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub['Target'] = y_test\nsub.to_csv(\"xgb.csv\", index= False)\ngc.collect()","0107f136":"params = {\n        'max_depth': (4, 6),\n        'gamma': (0.0001, 0.005),\n        'min_child_weight': (1, 2),\n        'max_delta_step': (0, 1),\n        'subsample': (0.2, 0.4),\n        'colsample_bytree': (0.2, 0.4)\n        }\nxgb = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='multi:softmax',\n                    silent=True, nthread=4)\nfolds = 3\nparam_comb = 5\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 42)","5471b530":"grid = GridSearchCV(estimator=xgb, param_grid=params, scoring='accuracy', n_jobs=4, cv=skf.split(X,y), verbose=0,refit=True )\ngrid.fit(X, y)\nprint('\\n All results:')\nprint(grid.cv_results_)\nprint('\\n Best estimator:')\nprint(grid.best_estimator_)\nprint('\\n Best score:')\nprint(grid.best_score_ * 2 - 1)\nprint('\\n Best parameters:')\nprint(grid.best_params_)\nresults = pd.DataFrame(grid.cv_results_)\nresults.to_csv('xgb-grid-search-results-01.csv', index=False)\ngc.collect()\n","63e5b2bb":"y_test = grid.predict(test)\nsub = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub['Target'] = y_test\nsub.to_csv(\"xgb1.csv\", index= False)","4832383b":"from skopt import BayesSearchCV","f90c5659":"# Classifier\nbayes_cv_tuner = BayesSearchCV(\n    estimator = XGBClassifier(\n        n_jobs = 1,\n        objective = 'multi:softmax',\n        silent=1,\n        tree_method='approx'\n    ),\n    search_spaces = {\n        'learning_rate': (0.01, 1.0, 'log-uniform'),\n        'min_child_weight': (0, 10),\n        'max_depth': (0, 50),\n        'max_delta_step': (0, 20),\n        'subsample': (0.01, 1.0, 'uniform'),\n        'colsample_bytree': (0.01, 1.0, 'uniform'),\n        'colsample_bylevel': (0.01, 1.0, 'uniform'),\n        'reg_lambda': (1e-9, 1000, 'log-uniform'),\n        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n        'gamma': (1e-9, 0.5, 'log-uniform'),\n        'min_child_weight': (0, 5),\n        'n_estimators': (50, 100),\n        'scale_pos_weight': (1e-6, 500, 'log-uniform')\n    },    \n    scoring = 'accuracy',\n    cv = StratifiedKFold(\n        n_splits=3,\n        shuffle=True,\n        random_state=42\n    ),\n    n_jobs = 3,\n    n_iter = 10,   \n    verbose = 0,\n    refit = True,\n    random_state = 42\n)\n\ndef status_print(optim_result):\n    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n    \n    # Get all the models tested so far in DataFrame format\n    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n    \n    # Get current parameters and the best parameters    \n    best_params = pd.Series(bayes_cv_tuner.best_params_)\n    print('Model #{}\\nBest ROC-AUC: {}\\nBest params: {}\\n'.format(\n        len(all_models),\n        np.round(bayes_cv_tuner.best_score_, 4),\n        bayes_cv_tuner.best_params_\n    ))\n    \n    # Save all model results\n    clf_name = bayes_cv_tuner.estimator.__class__.__name__\n    all_models.to_csv(clf_name+\"_cv_results.csv\")","b8ab7b01":"# Fit the model\nresult = bayes_cv_tuner.fit(X, y, callback=status_print)\ngc.collect()","cff7689b":"print('\\n All results:')\nprint(bayes_cv_tuner.cv_results_)\nprint('\\n Best estimator:')\nprint(bayes_cv_tuner.best_estimator_)\nprint('\\n Best score:')\nprint(bayes_cv_tuner.best_score_ * 2 - 1)\nprint('\\n Best parameters:')\nprint(bayes_cv_tuner.best_params_)","5e57095b":"y_test = bayes_cv_tuner.predict(test)\nsub = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub['Target'] = y_test\nsub.to_csv(\"xgb2.csv\", index= False)","21266ee1":"### Submission file of Grid search CV","888b812a":"# Grid Search CV Approach for classification","75c70f51":"### Set Parameter as per your requirment and build XGB Model \/ Apply kfold validation ","1ef5e44b":"# Road Map of Clean Data \/ Feature Engineering \/ Encoding \/ Remove Null Value\n1.  **Load Data.....**\n1.  **Clean features...**\n1.  **Extracting features...**\n1.  **Encoding Data....**\n1. ** Fill NA value.....**\n1.  **Prepared Model.....**\n1. ** Predict Value on test Dataset......**\n1.  **Parameter Tuning with different method which are mention below .....**\n1.  **Submit your result .......**","ab9cf2f4":"# Hyper Parameter Tuning only For XGB\n1.  **  RandomSearchCV**\n    *  All results:\n    *  Best estimator:\n    *  Best score:\n    * Best parameters:\n1.  **GridSearchCV**\n    *  All results:\n    *  Best estimator:\n    *  Best score:\n    * Best parameters:\n1.  **  BayesSearchCV**\n    *  All results:\n    *  Best estimator:\n    *  Best score:\n    * Best parameters:\n    \n    \n ### Same Method you have to use for different Algorithm\n \n \n","031d2bf5":"# Encoding","8879b387":"### Submission file of random search CV","6dcb760e":"# Fill Na value....","4c106bf5":"### Submission file of bayes search CV","8aa240cb":"# Do_feature Engineering....","b8d7c91b":"*  All results:\n*  Best estimator:\n*  Best score:\n* Best parameters:","e32a0918":"# Load Train and Test Data","c5edc5e3":"### Set Parameter as per your requirment and build XGB Model \/ Apply kfold validation ","5fdf211d":"# Clean features...","cc4cef4e":"## Challenge\n* The Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Are you up for the challenge?\n\n* Here's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It\u2019s especially tricky when a program focuses on the poorest segment of the population. The world\u2019s poorest typically can\u2019t provide the necessary income and expense records to prove that they qualify.","e08272b5":"# Stay connected for more parameter tunning updates. Glad to hear Comment from you guys...  \ud83d\ude42 Happy Kaggling","c892e96c":"# Random Search CV Approach for classification","f993c85e":"*  All results:\n*  Best estimator:\n*  Best score:\n* Best parameters:","231cc9d0":"# Stay connected for more parameter tunning updates. Glad to hear Comment from you guys...  \ud83d\ude42 Happy Kaggling","b9e67c24":"# Bayes Search CV Approach for classification","856fe764":"### Set Parameter as per your requirment and build XGB Model \/ Apply kfold validation ","09d8fb0e":"# Extracting features...","dfe7a2b3":"*  All results:\n*  Best estimator:\n*  Best score:\n* Best parameters:"}}