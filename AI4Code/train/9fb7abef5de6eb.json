{"cell_type":{"f1910284":"code","0405f737":"code","5df49d81":"code","735bffce":"code","d83bdead":"code","635107dc":"code","00b9631a":"code","b61aa6f4":"code","2651ad89":"code","d6972cf3":"code","a4d82fe7":"code","77b2355b":"code","82d54c29":"code","72b14236":"code","570b703f":"code","a0c9a2d5":"code","7115cbcf":"code","8b2485f5":"code","ef349032":"code","22260581":"code","e0890dea":"code","ab6b0886":"code","d25e3671":"markdown","9c24c319":"markdown","133074d5":"markdown","240a23e1":"markdown","1a351921":"markdown","009f2fc5":"markdown","80ff8897":"markdown","0c98040d":"markdown","80c292c2":"markdown","5cfc6161":"markdown","599aad83":"markdown","89c4a727":"markdown","a5c220c1":"markdown","7692b0cc":"markdown","9d64460a":"markdown","012c84d8":"markdown","24955133":"markdown","8311d39b":"markdown","2783ba5b":"markdown","3f274bfa":"markdown","b8af868b":"markdown"},"source":{"f1910284":"import numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport sys\nimport numpy\nnumpy.set_printoptions(threshold=10000,edgeitems = 10) #threshold=sys.maxsize\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","0405f737":"from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler","5df49d81":"train_file = \"..\/input\/digit-recognizer\/train.csv\"\ntest_file = \"..\/input\/digit-recognizer\/test.csv\"\nsample_submission = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')","735bffce":"raw_data = np.loadtxt(train_file, skiprows=1, dtype='int', delimiter=',')\nx_train, x_val, y_train, y_val = train_test_split(\n    raw_data[:,1:], raw_data[:,0], test_size=0.1)","d83bdead":"fig, ax = plt.subplots(2, 1, figsize=(12,6))\nax[0].plot(x_train[0])\nax[0].set_title('784x1 data')\nax[1].imshow(x_train[0].reshape(28,28), cmap='gray')\nax[1].set_title('28x28 data')","635107dc":"print(y_train[::]) # displays the Digits availabe corresponding to traning data.\ny_train.shape","00b9631a":"#Now we'll display the total count of each digits.\nsns.countplot(raw_data[:,0]) #this will select the first column vales from the dataset","b61aa6f4":"x_train = x_train.reshape(-1, 28, 28, 1)\nx_val = x_val.reshape(-1, 28, 28, 1)","2651ad89":"x_train = x_train.astype(\"float32\")\/255.\nx_val = x_val.astype(\"float32\")\/255.","d6972cf3":"y_train = to_categorical(y_train)\ny_val = to_categorical(y_val)\n#example:\nprint(y_train[0])","a4d82fe7":"model = Sequential()\n\nmodel.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu',\n                 input_shape = (28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\n#model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\n#model.add(BatchNormalization())\nmodel.add(MaxPool2D(strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\n#model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n#model.add(BatchNormalization())\nmodel.add(MaxPool2D(strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))","77b2355b":"datagen = ImageDataGenerator(zoom_range = 0.1,\n                            height_shift_range = 0.1,\n                            width_shift_range = 0.1,\n                            rotation_range = 10)","82d54c29":"model.compile(loss='categorical_crossentropy', optimizer = Adam(lr=1e-4), metrics=[\"accuracy\"])","72b14236":"annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)","570b703f":"hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size=16),\n                           steps_per_epoch=500,\n                           epochs=20, #Increase this when not on Kaggle kernel\n                           verbose=2,  #1 for ETA, 0 for silent\n                           validation_data=(x_val[:400,:], y_val[:400,:]), #For speed\n                           callbacks=[annealer])","a0c9a2d5":"final_loss, final_acc = model.evaluate(x_val, y_val, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))","7115cbcf":"#First graph is comparison of Training Loss and Validation loss\nplt.plot(hist.history['loss'], color='b')\nplt.plot(hist.history['val_loss'], color='r')\nplt.show()\n#Second graph is comparison of Training accuracy and Validation Accuracy.\nplt.plot(hist.history['accuracy'], color='b')\nplt.plot(hist.history['val_accuracy'], color='r')\nplt.show()","8b2485f5":"y_hat = model.predict(x_val)\ny_pred = np.argmax(y_hat, axis=1)\ny_true = np.argmax(y_val, axis=1)\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)","ef349032":"mnist_testset = np.loadtxt(test_file, skiprows=1, dtype='int', delimiter=',')\nx_test = mnist_testset.astype(\"float32\")\nx_test = x_test.reshape(-1, 28, 28, 1)\/255.","22260581":"y_hat = model.predict(x_test, batch_size=64)","e0890dea":"y_pred = np.argmax(y_hat,axis=1)","ab6b0886":"solution = pd.DataFrame({'ImageId': sample_submission['ImageId'], 'Label': y_pred})\nsolution[[\"ImageId\",\"Label\"]].to_csv(\"CNNPrediction.csv\", index=False)\nsolution.head()","d25e3671":"### ***Show Your support by Upvoting this Post*** \ud83d\udc4d \n\n### ***Follow me on Kaggle for getting more of such resources.***","9c24c319":"If you don't already have [Keras][1], you can easily install it through conda or pip. It relies on either tensorflow or theano, so you should have these installed first. Keras is already available here in the kernel and on Amazon deep learning AMI.\n\n  [1]: https:\/\/keras.io\/","133074d5":"Not too bad, considering the minimal amount of training so far. In fact, we have only gone through the training data approximately five times. With proper training we should get really good results.\n\nAs you can see there are quite a few parameters that could be tweaked (number of layers, number of filters, Dropout parameters, learning rate, augmentation settings). This is often done with trial and error, and there is no easy shortcut. \n\nGetting convergence should not be a problem, unless you use an extremely large learning rate. It's easy, however, to create a net that overfits, with perfect results on the training set and very poor results on the validation data. If this happens, you could try increasing the Dropout parameters, increase augmentation, or perhaps stop training earlier. If you instead wants to increase accuracy, try adding on two more layers, or increase the number of filters.","240a23e1":"## Evaluate","1a351921":"The labels were given as integers between 0 and 9. We need to convert these to one-hot encoding, i.e. a 10x1 array with one 1 and nine 0:s, with the position of the 1 showing us the value. See the example, with the position of the 1 showing the correct value for the digit in the graph above.","009f2fc5":"## Load the data","80ff8897":"We train once with a smaller learning rate to ensure convergence. We then speed things up, only to reduce the learning rate by 10% every epoch. Keras has a function for this: ","0c98040d":"We only used a subset of the validation set during training, to save time. Now let's check performance on the whole validation set.","80c292c2":"## Solved using Convolutional Neural Networks","5cfc6161":"Each data point consists of 784 values. A fully connected net just treats all these values the same, but a CNN treats it as a 28x28 square. Thes two graphs explain the difference: It's easy to understand why a CNN can get better results.","599aad83":"Another important method to improve generalization is augmentation. This means generating more training data by randomly perturbing the images. If done in the right way, it can force the net to only learn translation-invariant features. If you train this model over hundreds of epochs, augmentation will definitely improve your performance. Here in the Kernel, we will only look at each image 4-5 times, so the difference is smaller. We use a Keras function for augmentation.","89c4a727":"It would be possible to train the net on the original data, with pixel values 0 to 255. If we use the standard initialization methods for weights, however, data between 0 and 1 should make the net converge faster. ","a5c220c1":"We now reshape all data this way. Keras wants an extra dimension in the end, for channels. If this had been RGB images, there would have been 3 channels, but as MNIST is gray scale it only uses one.\n\nThis notebook is written for the tensorflow channel ordering. If you have Keras installed for Theano backend, you might start seeing some error message soon related to channel ordering. This can easily be [solved][1].\n\n\n  [1]: https:\/\/keras.io\/backend\/#set_image_dim_ordering","7692b0cc":"As always, we split the data into a training set and a validation set, so that we can evaluate the performance of our model.","9d64460a":"We will use a very small validation set during training to save time in the kernel.","012c84d8":"## Submit","24955133":"Submitting from this notebook usually gives you a result around 99%, with some randomness depending on weight initialization and test\/train data split. I achieved 99.3% by averaging over 5 good runs, and you can get higher than that if you train overnight.\n\nIf you've successfully come this far, you can now create similar CNN for all kinds of image recognition problems. Good luck!","8311d39b":"The model needs to be compiled before training can start. As our loss function, we use logloss which is called ''categorical_crossentropy\" in Keras. Metrics is only used for evaluation. As optimizer, we could have used ordinary stochastic gradient descent (SGD), but Adam is faster.","2783ba5b":"## Train the model\n\nKeras offers two different ways of defining a network. We will the Sequential API, where you just add on one layer at a time, starting from the input.\n\nThe most important part are the convolutional layers Conv2D. Here they have 16-32 filters that use nine weights each to transform a pixel to a weighted average of itself and its eight neighbors. As the same nine weights are used over the whole image, the net will pick up features that are useful everywhere. As it is only nine weights, we can stack many convolutional layers on top of each other without running out of memory\/time. \n\nThe MaxPooling layers just look at four neighboring pixels and picks the maximal value. This reduces the size of the image by half, and by combining convolutional and pooling layers, the net be able to combine its features to learn more global features of the image. In the end we use the features in two fully-connected (Dense) layers.\n\nBatch Normalization is a technical trick to make training faster. Dropout is a regularization method, where the layer randomly replaces  a proportion of its weights to zero for each training sample. This forces the net to learn features in a distributed way, not relying to much on a particular weight, and therefore improves generalization. 'relu' is the activation function x -> max(x,0).","3f274bfa":"To easily get to the top half of the leaderboard, just follow these steps, go to the Kernel's output, and submit \"submission.csv\"","b8af868b":"y_hat consists of class probabilities (corresponding to the one-hot encoding of the training labels). I now select the class with highest probability"}}