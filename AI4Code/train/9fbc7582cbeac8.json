{"cell_type":{"88248dad":"code","d26f7405":"code","f11785dd":"code","146699c8":"code","cf2eb56f":"code","e526a76c":"code","7a3fd459":"code","3883f443":"code","240fdac1":"code","cb4f8265":"code","32cb1ed1":"code","7f90521c":"code","84bf7b36":"code","24c30346":"code","a88d27d6":"code","90ea4e85":"markdown","a7ea7115":"markdown","37a0f4dc":"markdown","5629134f":"markdown","4a97efff":"markdown","052f18ba":"markdown","e3a765d9":"markdown","45e93e6a":"markdown","edcb92eb":"markdown","aec420ae":"markdown","68610aa1":"markdown","c83149f5":"markdown"},"source":{"88248dad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np\nimport pandas as pd\npd.options.display.precision = 15\n\nimport matplotlib.pyplot as plt\nfrom gplearn.genetic import SymbolicRegressor\nfrom gplearn.functions import make_function\nimport signal\nimport gc\nimport time\nimport datetime\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import KFold, cross_validate, cross_val_predict\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler,QuantileTransformer\nimport pickle\n\nfrom catboost import CatBoostRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/gplearnmodels\"))\n# Any results you write to the current directory are saved as output.\n","d26f7405":"df=pd.read_csv('..\/input\/andrewstsfreshv19\/ExtFeaturestrain.csv')\ntest=pd.read_csv('..\/input\/andrewstsfreshv19\/ExtFeaturestest.csv')\nprint(df.shape)\n\nX=df[df.columns.difference(['time_to_failure'])]\ny=df['time_to_failure']\n\ny_tr=y\nscaler=MinMaxScaler(feature_range=(0, 1))\nscaled=scaler.fit_transform(X.append(test))\n\ntrain=pd.DataFrame(scaled[:X.shape[0]], columns=X.columns.values)\ntest=pd.DataFrame(scaled[-test.shape[0]:], columns=X.columns.values)\ntrainX, testX, trainy, testy = train_test_split(train, y, test_size=0.0,shuffle=True,random_state=42)\n\nX_tr_scaled = trainX\nX_test_scaled = test","f11785dd":"est_gp = SymbolicRegressor(population_size=10000,\n                               tournament_size=50,\n                               generations=1000, #you can set a high number then use est_gp.set_params(generations= number<220) to stop evolution\n                               stopping_criteria=0.0,\n                               p_crossover=0.9, p_subtree_mutation=0.0001, p_hoist_mutation=0.0001, p_point_mutation=0.0001,\n                               max_samples=1.0, verbose=1,\n                               #function_set = ('add', 'sub', 'mul', 'div', gp_tanh, 'sqrt', 'log', 'abs', 'neg', 'inv','max', 'min', 'tan', 'cos', 'sin'),\n                               function_set = ('tan', 'add', 'sub', 'mul', 'div','max', 'min'),\n                               metric = 'mean absolute error', warm_start=True,\n                               n_jobs = -1, parsimony_coefficient=0.00003, random_state=11)","146699c8":"with open('..\/input\/gplearnmodels\/gp_model.pkl', 'rb') as f:\n    est_gp = pickle.load(f)","cf2eb56f":"n=850\n\nclass TimeoutException(Exception):\n    pass\n\ndef timeout_handler(signum, frame):\n    raise TimeoutException\n\nsignal.signal(signal.SIGALRM, timeout_handler)\n\n\nsignal.alarm(n)   # seconds  \ntry:\n    est_gp.fit(train, y)\n\nexcept TimeoutException:\n    #with open('gp_model.pkl', 'wb') as f: # uncomment to save model\n        #pickle.dump(est_gp, f)\n    print('function terminated')","e526a76c":"print(\"This model was pretrained for {} generations\".format(est_gp.run_details_['generation'][-1]))\nest_gp.set_params(generations=est_gp.run_details_['generation'][-1]+1) #train for 1 more generation\nest_gp.fit(train, y)","7a3fd459":"#est_gp.fit(train, y)\ny_gp = est_gp.predict(train)\ngpLearn_MAE = mean_absolute_error(y_tr, y_gp)\nprint(\"gpGpiLearn MAE:\", gpLearn_MAE)\n#with open('gp_model.pkl', 'wb') as f:\n    #pickle.dump(est_gp, f)","3883f443":"submission = pd.read_csv('..\/input\/LANL-Earthquake-Prediction\/sample_submission.csv', index_col='seg_id')\nsubmission.time_to_failure = est_gp.predict(X_test_scaled)\nsubmission.to_csv('submission.csv', index=True)\nsubmission.head()","240fdac1":"params={'bagging_fraction': 0.63,\n'boosting': 'gbdt',\n'feature_fraction': 0.31,\n'lambda_l1': 2.622427756417558,\n'lambda_l2': 2.624427931714477,\n'learning_rate': 0.041111529297691546,\n'max_bin': 133,\n'max_depth': 17,\n'metric': 'RMSE',\n'min_data_in_bin': 194,\n'min_data_in_leaf': 11,\n'num_leaves': 1058,\n'objective': 'gamma',\n'subsample': 0.9577983565538245}","cb4f8265":"\nprelgb1 = np.zeros(len(train))\nmae = np.array([])\npredictions0 = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n#run model\n\nn_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\ntrain_columns = train.columns.values\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(trainX,trainy.values)):\n    strLog = \"fold {}\".format(fold_)\n    \n    X_tr, X_val = trainX.iloc[trn_idx], trainX.iloc[val_idx]\n    y_tr, y_val = trainy.iloc[trn_idx], trainy.iloc[val_idx]\n\n    print(strLog)\n    \n    model0 = lgb.LGBMRegressor(**params, n_estimators = 10000, n_jobs = -1)\n    model0.fit(X_tr, \n              y_tr, \n              eval_set=[(X_tr, y_tr), (X_val, y_val)], \n              eval_metric='mae',\n              verbose=500, \n              early_stopping_rounds=100)\n    mae = np.append(mae, mean_absolute_error(y_val, model0.predict(X_val)))\n    print(\"Fold: {} Validation MAE: {}\".format(strLog,mae[fold_]))\n    #predictions\n    predictions0 += model0.predict(test, num_iteration=model0.best_iteration_) \/ folds.n_splits\n    prelgb1[val_idx] =  model0.predict(X_val).reshape(-1,)\nprint('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(mae), np.std(mae)))\nsubmission.time_to_failure = predictions0\nsubmission.to_csv('submissionlgb0.csv',index=True)\nsubmission.head()\nprelgb1=pd.DataFrame({'id': trainy.index.values,'TTF': prelgb1}).set_index('id').sort_index()","32cb1ed1":"blending1 = predictions0*1\/2 + est_gp.predict(X_test_scaled)*1\/2\nsubmission['time_to_failure']=blending1\nsubmission.to_csv('submissionblend1.csv',index=True)\nsubmission.head()","7f90521c":"params ={'boosting': 'gblinear',\n'colsample_bylevel': 0.87,\n'colsample_bynode': 0.98,\n'colsample_bytree': 0.63,\n'eval_metric': 'mae',\n'gamma': 2.3074997279871394,\n'learning_rate': 0.19978743070807628,\n'max_depth': 4,\n'min_child_weight': 2.164915649342119,\n'objective': 'reg:linear',\n'reg_alpha': 2.5778910953585816,\n'reg_lambda': 3.4511859649859136,\n'subsample': 1.0,\n'tree_method': 'exact'}","84bf7b36":"X=trainX\nX_test=test\n\ny=trainy\nprexgb01 = np.zeros(len(train))\nprediction01 = np.zeros(len(X_test))\n\nn_fold = 5\nfolds = KFold(n_splits=n_fold,  random_state=11)\nscores = []\n\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    \n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n    \n    train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n    valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n    watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n    model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n    \n    y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n    \n    \n    y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n    prexgb01[valid_index] = y_pred_valid.reshape(-1,)\n    \n    \n    scores.append(mean_absolute_error(y_valid, y_pred_valid))\n    \n    \n    prediction01 += y_pred \/ folds.n_splits\nprint('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\nsubmission.time_to_failure = prediction01\nsubmission.to_csv('submissionxgb0.csv',index=True)\nsubmission.head()\nprexgb01=pd.DataFrame({'id': trainy.index.values,'TTF': prexgb01}).set_index('id').sort_index()","24c30346":"blending = (predictions0 + prediction01 + est_gp.predict(X_test_scaled))*1\/3\nsubmission['time_to_failure']=blending\nsubmission.to_csv('submissionblend0.csv',index=True)\nsubmission.head()","a88d27d6":"y=y.sort_index()\nplt.figure(figsize=(20, 10))\nplt.subplot(5, 1, 1)\nplt.plot(y, color='g', label='y_train')\nplt.plot(prelgb1['TTF'].values, color='b', label='lgb')\nplt.legend(loc=(0.7, 0.8));\nplt.ylim((0,17))\nplt.title('MAE: {}.'.format(mean_absolute_error(y, prelgb1['TTF'].values)));\nplt.xticks([])\nplt.subplot(5, 1, 2)\nplt.plot(y, color='g', label='y_train')\nplt.plot(y_gp, color='b', label='Gplearn')\nplt.legend(loc=(0.7, 0.8));\nplt.ylim((0,17))\nplt.title('MAE: {}.'.format(mean_absolute_error(y, y_gp)));\nplt.xticks([])\nplt.subplot(5, 1, 3)\nplt.plot(y, color='g', label='y_train')\nplt.plot(prexgb01['TTF'].values, color='b', label='xgb')\nplt.legend(loc=(0.7, 0.8));\nplt.title('MAE: {}.'.format(mean_absolute_error(y, prexgb01['TTF'].values)));\nplt.ylim((0,17))\nplt.legend(loc=(0.7, 0.8));\nplt.suptitle('Predictions vs actual');\nplt.xticks([])\nplt.subplot(5, 1, 4)\nplt.plot(y, color='g', label='y_train')\nplt.plot((prelgb1['TTF'].values  + y_gp)*1\/2, color='b', label='lgb+gp')\nplt.legend(loc=(0.7, 0.8));\nplt.title('MAE: {}.'.format(mean_absolute_error(y,(prelgb1['TTF'].values  + y_gp)*1\/2)))\nplt.ylim((0,17))\nplt.xticks([])\nplt.subplot(5, 1, 5)\nplt.plot(y, color='g', label='y_train')\nplt.plot((prelgb1['TTF'].values + prexgb01['TTF'].values + y_gp)*1\/3, color='b', label='lgb+gp+xgb')\nplt.legend(loc=(0.7, 0.8));\nplt.title('MAE: {}.'.format(mean_absolute_error(y,(prelgb1['TTF'].values + prexgb01['TTF'].values + y_gp)*1\/3)))\nplt.ylim((0,17))\nplt.xticks([])\nplt.legend(loc=(0.7, 0.8));\nplt.suptitle('Predictions vs actual');\n","90ea4e85":"# <a id='1'>Load packages<\/a>\n\n","a7ea7115":"**XGB Parameters**","37a0f4dc":"Submission of the blend with the gplearn and lgb models","5629134f":"LGB Parameters","4a97efff":"Submission of the blend with the gplearn , lgb and xgb models","052f18ba":"\n# <a id='4-1'>GPlearn Runtime Management <\/a>  \n\n\nThis code is used to stop the training process due to the kaggle limit on kernel runtime.\nTrain for n seconds and pickle\/save resulting model.(continue the evolution process later)","e3a765d9":"Load a pretrained model","45e93e6a":"<h1><center><font size=\"6\">Regresion with GPlearn+LGB+XGB models<\/font><\/center><\/h1>\n\n<h2><center><font size=\"4\">Features used: Andrews+Tsfresh<\/font><\/center><\/h2>\n\n<img src=\"\" width=\"600\"><\/img>\n\n<br>\n\n# <a id='0'>Content<\/a>\n\n\n- <a href='#2'>Load Packages<\/a>  \n- <a href='#3'>Data Import and Scaling<\/a>   \n- <a href='#4'>Genetic Symbolic Regressor<\/a>\n- <a href='#4-1'>GPlearn Runtime Management<\/a>\n- <a href='#5'>Regression with LGB and XGB<\/a>\n- <a href='#6'>Visualization Fitted Models vs  Time to Failure<\/a>  \n","edcb92eb":"# <a id='3'>Data Import and Scaling<\/a>  \n\n   \n* Features: Andrews + Tsfresh ('c3','ar_coefficient','spkt_welch_density','fft_coefficient','autocorrelation','agg_autocorrelation','number_peaks','cwt_coefficients','ratio_beyond_r_sigma');  \n* Labels : time to failure which gives the time until a failure occurs.\n\n","aec420ae":"\n# <a id='6'>Visualization Fitted Models vs  Time to Failure<\/a> ","68610aa1":"# <a id='4'>GPlearn Symbolic Regressor <\/a>  \n\n**This GPlearn section is atributed to https:\/\/www.kaggle.com\/oguzkoroglu\/andrews-new-script-genetic-program-and-gplearn**","c83149f5":"\n# <a id='5'>Regression with LGB and XGB<\/a>  \n\nLet's prepare the model."}}