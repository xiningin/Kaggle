{"cell_type":{"43331f59":"code","0e4774ee":"code","5049b4ca":"code","8b9f1d54":"code","85f24f1a":"code","05ca4f20":"code","493915e7":"code","fdca9ad2":"code","ec440e20":"code","cfd38c02":"code","3d9e08ef":"code","f57b63a5":"code","032870cf":"code","9efe8da2":"code","637fc616":"code","17936cab":"code","03708742":"code","4003e00c":"code","261e3b4e":"code","3d177a38":"code","fed51c3e":"code","a8b14bd3":"code","46e36fb4":"code","001e22d3":"code","d3d4c586":"code","423dca44":"code","d79079f1":"code","54de52b1":"code","0ac88919":"code","168f1d81":"code","64452dcc":"code","eb300976":"code","8f28eef3":"code","3908ada5":"code","46b0b919":"code","63e123cf":"code","f20f813c":"code","e7bfd259":"code","1994d71e":"code","62bc9783":"code","4c41a7a0":"code","889d0cbe":"markdown","edfb8d8f":"markdown","d3293c8f":"markdown","7583b6c1":"markdown","7d3a28da":"markdown","b6526499":"markdown","219768ee":"markdown","22f09b82":"markdown","9b70a55b":"markdown","1d68fb14":"markdown","f95a56d4":"markdown","b3db5fd5":"markdown","7886a26c":"markdown","11ebe490":"markdown","0f6bec41":"markdown"},"source":{"43331f59":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","0e4774ee":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport string\nimport tqdm\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tokenization\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","5049b4ca":"train_data = pd.read_csv(\"..\/input\/tweets-with-sarcasm-and-irony\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/tweets-with-sarcasm-and-irony\/test.csv\")","8b9f1d54":"train_tweets=train_data['tweets'].tolist()\ntest_tweets=test_data['tweets'].tolist()","85f24f1a":"def keep_uniques(array, df):\n    dels=[]\n    for i in array:\n        if array.count(i)>1:\n            dels.append(i)\n    dels=list(set(dels))\n    for i in dels:\n        df.drop( df[ df['tweets'] == i ].index, inplace=True)\n    return df","05ca4f20":"train_data=keep_uniques(train_tweets, train_data)\ntest_data=keep_uniques(test_tweets, test_data)","493915e7":"len(train_data['tweets'].unique())","fdca9ad2":"len(test_data['tweets'].unique())","ec440e20":"train_data.describe()","cfd38c02":"train_data = train_data.sample(frac = 1)\ntest_data = test_data.sample(frac = 1)","3d9e08ef":"train_data.head()","f57b63a5":"test_data.head()","032870cf":"train_data['class'].value_counts()","9efe8da2":"temp=train_data.loc[train_data['class'] == 'regular']","637fc616":"lis=temp['tweets'].tolist()","17936cab":"import random\nreg_del=[]\nvisited=set()\nfor _ in range(3600):\n    n=random.randint(0,18556)\n    if n not in visited:\n        reg_del.append(lis[n])\n        \n        \nfor i in reg_del:\n    train_data.drop( train_data[ train_data['tweets'] == i ].index, inplace=True)","03708742":"train_data['class'].value_counts()","4003e00c":"test_data['class'].value_counts()","261e3b4e":"def remove_URL(text):\n    text=str(text)\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_mentions(text):\n    ment = re.compile(r\"(@[A-Za-z0-9]+)\")\n    return ment.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)","3d177a38":"train_data['clean_text'] = train_data['tweets'].apply(lambda x: remove_URL(x))\ntrain_data['clean_text'] = train_data['clean_text'].apply(lambda x: remove_emoji(x))\ntrain_data['clean_text'] = train_data['clean_text'].apply(lambda x: remove_html(x))\ntrain_data['clean_text'] = train_data['clean_text'].apply(lambda x: remove_mentions(x))\ntrain_data['clean_text'] = train_data['clean_text'].apply(lambda x: remove_punct(x))\ntrain_data['clean_text'] = train_data['clean_text'].apply(\n    lambda x: x.lower())","fed51c3e":"cleaned = train_data['clean_text'].tolist()\n\nfor i,text in enumerate(cleaned):\n    splits = text.split()\n    splits = [word for word in splits if word not in set(nltk.corpus.stopwords.words('english'))]\n    cleaned[i]=' '.join(splits)\n    \ntrain_data['clean_text']=cleaned\n    ","a8b14bd3":"train_data.head()","46e36fb4":"test_data['clean_text'] = test_data['tweets'].apply(lambda x: remove_URL(x))\ntest_data['clean_text'] = test_data['clean_text'].apply(lambda x: remove_emoji(x))\ntest_data['clean_text'] = test_data['clean_text'].apply(lambda x: remove_html(x))\ntest_data['clean_text'] = test_data['clean_text'].apply(lambda x: remove_mentions(x))\ntest_data['clean_text'] = test_data['clean_text'].apply(lambda x: remove_punct(x))\ntest_data['clean_text'] = test_data['clean_text'].apply(\n    lambda x: x.lower())","001e22d3":"cleaned = test_data['clean_text'].tolist()\n\nfor i,text in enumerate(cleaned):\n    splits = text.split()\n    splits = [word for word in splits if word not in set(nltk.corpus.stopwords.words('english'))]\n    cleaned[i]=' '.join(splits)\n    \ntest_data['clean_text']=cleaned","d3d4c586":"test_data.head()","423dca44":"test_data = test_data.dropna()","d79079f1":"sns.set(rc={'figure.figsize':(10,10)})\nsns.countplot(train_data['class'])","54de52b1":"from wordcloud import WordCloud\nstopwords = nltk.corpus.stopwords.words('english')\n\nplt.figure(figsize=(12,6))\ntext = ' '.join(train_data.clean_text[train_data['class']=='regular'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","0ac88919":"plt.figure(figsize=(12,6))\ntext = ' '.join(train_data.clean_text[train_data['class']=='irony'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","168f1d81":"plt.figure(figsize=(12,6))\ntext = ' '.join(train_data.clean_text[train_data['class']=='sarcasm'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","64452dcc":"plt.figure(figsize=(12,6))\ntext = ' '.join(train_data.clean_text[train_data['class']=='figurative'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","eb300976":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens, all_masks, all_segments = [], [], []\n    \n    for text in tqdm(texts):\n        # Tokenize the current text\n        text = tokenizer.tokenize(text)\n        # Select text only till \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","8f28eef3":"%%time\nurl = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(url, trainable=True)","3908ada5":"# Get tokenizer\nvocab_fl = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nlower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_fl, lower_case)","46b0b919":"def encode_target(t_class):\n    t_class=str(t_class)\n    class_dict = {\n        'irony':0,\n        'sarcasm':1,\n        'regular':2,\n        'figurative':3\n    }\n    return class_dict[t_class]","63e123cf":"train_data[\"target\"] = train_data['class'].apply(lambda x: encode_target(x))\ntest_data[\"target\"] = test_data['class'].apply(lambda x: encode_target(x))","f20f813c":"%%time\ntrain_input = bert_encode(train_data['clean_text'].values, tokenizer, max_len=160)\ntest_input = bert_encode(test_data['clean_text'].values, tokenizer, max_len=160)\ntrain_labels = train_data['target'].values\ntest_labels = test_data['target'].values","e7bfd259":"def build_model(transformer, max_len=512):\n\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_word_ids')\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name='input_mask')\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name='segment_ids')\n    # Get the sequence output\n    _, seq_op = transformer([input_word_ids, input_mask, segment_ids])\n    # Get the respective class token from that sequence output\n    class_tkn = seq_op[:, 0, :]\n    # Final Neuron (for Classification)\n    op = Dense(4, activation='softmax')(class_tkn)\n    # Bind the inputs and outputs together into a Model\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=op)\n    \n    model.compile(optimizer=Adam(1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","1994d71e":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","62bc9783":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.07,\n    epochs=4,\n    callbacks=[checkpoint],\n    batch_size=16\n)","4c41a7a0":"scores = model.evaluate(test_input, test_labels, verbose=0)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","889d0cbe":"Here, we see that the `regular` class has 18k tweets, which causes our dataset to be imbalanced. So we shall delete some tweets from this class","edfb8d8f":"### Remove recurring tweets to prevent ambiguity","d3293c8f":"### Word cloud for irony class","7583b6c1":"## Data Cleaning Preprocessing","7d3a28da":"### Get BERT Model from TFHub","b6526499":"## BERT Embeddings","219768ee":"### Basic EDA","22f09b82":"### Word cloud for regular class","9b70a55b":"### Word cloud for sarcasm class","1d68fb14":"### Dataset details","f95a56d4":"### Word cloud for figurative class","b3db5fd5":"### Encoding the texts","7886a26c":"### Fine tuned model","11ebe490":"### Loading data","0f6bec41":"# Classifiying the tweets using BERT"}}