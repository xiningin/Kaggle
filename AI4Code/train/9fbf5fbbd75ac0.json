{"cell_type":{"cad68b9d":"code","a514a415":"code","17489932":"code","9aa88fd9":"code","1bfacecf":"code","1f098de8":"code","0547ce65":"code","fe2e0124":"code","54f2cd90":"code","0df50fbd":"code","126ff46a":"code","ba10741c":"code","45e66814":"code","ec2c9011":"code","0e6fa228":"code","c0e44401":"code","b4b63ab2":"code","ca6b0d1d":"code","a8098094":"code","c1f2141f":"code","4bd8ab86":"code","c7784adf":"code","d104ebcc":"code","79524ce7":"markdown","6ad9e99f":"markdown","610a0184":"markdown","4cf7249f":"markdown","dd5b084b":"markdown","dbdba2b7":"markdown","f861fd97":"markdown","b7d9ac46":"markdown","5136143e":"markdown","74ef5b20":"markdown","ea07a228":"markdown","68c6738c":"markdown","6c7bbad7":"markdown","95793f80":"markdown","b3f17a20":"markdown","c0d13a87":"markdown"},"source":{"cad68b9d":"!pip install git+https:\/\/github.com\/glmcdona\/LuxPythonEnvGym.git","a514a415":"!apt-get install --yes libpython3.7","17489932":"!pip install kaggle-environments -U\n!pip install tf-agents \n!pip install dm-reverb[tensorflow]","9aa88fd9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1bfacecf":"\nfrom luxai2021.game.constants import LuxMatchConfigs_Default\nfrom luxai2021.game.game import Game\nfrom luxai2021.game.unit import Unit\nfrom luxai2021.game.resource import Resource\nfrom luxai2021.game.constants import Constants\nimport numpy as np\nimport tensorflow as tf\nfrom luxai2021.game.constants import LuxMatchConfigs_Default\nfrom luxai2021.game.game import Game\nfrom luxai2021.game.unit import Unit\nfrom luxai2021.game.city import City, CityTile\nfrom luxai2021.game.actions import MoveAction, SpawnCityAction, TransferAction, PillageAction, Action, SpawnWorkerAction\nfrom luxai2021.game.constants import Constants\nfrom luxai2021.game.resource import Resource\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.networks import sequential\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.utils import common\nfrom tf_agents.typing import types\nfrom tf_agents.environments import utils\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import TimeStep, StepType\nfrom tf_agents.agents.ppo import ppo_agent\nfrom tf_agents.networks.actor_distribution_rnn_network import ActorDistributionRnnNetwork\nfrom tf_agents.networks.value_rnn_network import ValueRnnNetwork\nimport tf_agents.trajectories as ts\nfrom typing import Any, Callable, Optional, Sequence, Text, Union, List\nimport numpy as np\nimport tensorflow as tf\nfrom tf_agents.environments import tf_py_environment\n\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.networks import sequential\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.utils import common\n\nimport tensorflow as tf\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.networks import sequential\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.utils import common\n#from pyluxenvironmentworkerscalar import PyLuxEnvironmentWorkerScalar\n\n#from pyluxenvironmentworkerscalar import PyLuxEnvironmentWorkerScalar\n#from observation_builder import build_observation, build_reward, get_unit, get_unit_adjacent\nimport reverb\nfrom tf_agents.replay_buffers import reverb_replay_buffer\nfrom tf_agents.replay_buffers import reverb_utils\nfrom tf_agents.replay_buffers import reverb_replay_buffer\n\nfrom tf_agents.drivers import py_driver\nfrom tf_agents.policies import py_tf_eager_policy\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport os\nfrom tf_agents.policies import policy_saver\nimport random\nimport time","1f098de8":"import reverb\nfrom tf_agents.replay_buffers import reverb_replay_buffer\nfrom tf_agents.replay_buffers import reverb_utils\nfrom tf_agents.replay_buffers import reverb_replay_buffer","0547ce65":"\ndef my_get_unit_adjacent(game : Game, team : int, unit_id, direction : Constants.DIRECTIONS) -> Unit:\n    unit = my_get_unit(game, unit_id)\n    if unit is None:\n        print(42)\n    new_pos = unit.pos.translate(direction, 1)\n    new_cell = game.map.get_cell_by_pos(new_pos)\n    all_units = list(game.state[\"teamStates\"][team][\"units\"].values())\n    unit_adjacent = [unit for unit in all_units if unit.pos.x == new_pos.x and unit.pos.y == new_pos.y]\n    if len(unit_adjacent) == 0:\n        return None\n    elif len(unit_adjacent) == 1:\n        return unit_adjacent[0]\n    else:\n        raise ValueError(\"my_get_unit_adjacent\")\n    return None\n\n\ndef my_get_unit(game : Game, unit_id : str) -> Unit:\n    if unit_id in game.state['teamStates'][0]['units'].keys():\n        return game.state['teamStates'][0]['units'][unit_id]\n    elif unit_id in game.state['teamStates'][1]['units'].keys():\n        return game.state['teamStates'][1]['units'][unit_id]\n    else:\n        return None\n\ndef my_get_units(game : Game, team : int) -> List[Unit]:\n    return list(game.state['teamStates'][team]['units'].values())\n\ndef my_get_citytiles(game, team)  -> List[CityTile]  :\n    city_tiles = []\n    for city in game.cities.values():\n        if city.team == team:\n            for citycell in city.city_cells:\n                city_tiles.append(citycell.city_tile)\n    return city_tiles\n","fe2e0124":"\ndef build_reward(game : Game, unit_id : str, team : int) -> float:\n    #global nbcall\n    #nbcall += 1\n    unit = my_get_unit(game, unit_id)\n    #if not unit is None:\n    #    print(\"unit pos ({}, {})\".format(unit.pos.x, unit.pos.y))\n    #if unit is None:\n    #    print(\"unit {} None\".format(unit_id))\n    #else:\n    #    print(\"unit {} ({}, {})\".format(unit_id, unit.pos.x, unit.pos.y))\n            \n    ctt = my_get_citytiles(game, team)\n    reward_ctt= 100 * (0 if ctt is None else len(ctt))\n    reward_ctt = 0\n    reward_cfv = 0.9 * (unit.get_cargo_fuel_value() if unit is not None else 0.0)\n    #if (reward_cfv > 0):\n    #    print(\"got cargo {}\".format(unit.get_cargo_fuel_value()))\n    reward_fg = game.stats['teamStats'][team]['fuelGenerated'] #game.state['turn']\n    reward = -100.0 + reward_cfv + reward_fg + reward_ctt \n    #print(\"{} : {} \".format(nbcall, reward))\n    #if nbcall == 198:\n    #    print(42)\n    return np.float32(reward)\n","54f2cd90":"\ndef build_observation(game : Game, unit_id : str):\n    #unit = game.get_unit(team_id, unit_id)\n    unit = my_get_unit(game, unit_id)\n    \n    if unit is None:\n        return np.array([-1] * 259)\n    team = unit.team\n    \n    other_team = 0 if team == 1 else 1\n    \n    game_state = [ game.state['turn'], \n                    1 if game.is_night() else 0,\n                    1, # worker\n                    0, # cart\n                    0, # citytile,\n                    unit.get_cargo_space_left(),\n                    unit.cargo['wood'],\n                    unit.cargo['coal'],\n                    unit.cargo['uranium'],\n                    unit.get_cargo_fuel_value(),                    \n                    game.state['teamStates'][team]['researchPoints'],\n                    1 if game.state['teamStates'][team]['researched']['wood'] else 0,\n                    1 if  game.state['teamStates'][team]['researched']['coal'] else 0,\n                    1 if game.state['teamStates'][team]['researched']['uranium'] else 0,\n                    game.stats['teamStats'][team]['resourcesCollected']['wood'],\n                    game.stats['teamStats'][team]['resourcesCollected']['coal'],\n                    game.stats['teamStats'][team]['resourcesCollected']['uranium'],\n                    game.stats['teamStats'][team]['cityTilesBuilt'],\n                    game.stats['teamStats'][team]['workersBuilt'],\n                    game.stats['teamStats'][team]['cartsBuilt'],\n                    game.stats['teamStats'][team]['roadsBuilt'],\n                    game.stats['teamStats'][team]['roadsPillaged'],\n                    game.state['teamStates'][other_team]['researchPoints'],\n                    1 if game.state['teamStates'][other_team]['researched']['wood'] else 0,\n                    1 if game.state['teamStates'][other_team]['researched']['coal'] else 0,\n                    1 if game.state['teamStates'][other_team]['researched']['uranium'] else 0,\n                    game.stats['teamStats'][other_team]['resourcesCollected']['wood'],\n                    game.stats['teamStats'][other_team]['resourcesCollected']['coal'],\n                    game.stats['teamStats'][other_team]['resourcesCollected']['uranium'],\n                    game.stats['teamStats'][other_team]['cityTilesBuilt'],\n                    game.stats['teamStats'][other_team]['workersBuilt'],\n                    game.stats['teamStats'][other_team]['cartsBuilt'],\n                    game.stats['teamStats'][other_team]['roadsBuilt'],\n                    game.stats['teamStats'][other_team]['roadsPillaged'] ]\n    \n    \n    xs = np.array([2, 1, 0, -1, -2]) + unit.pos.x\n    ys = np.array([2, 1, 0, -1, -2]) + unit.pos.y\n    \n    positions = []\n    for y in ys:    \n        for x in xs:\n            if x < 0 or x >= game.map.width:\n                positions.append(None)\n            elif y < 0 or y >= game.map.height:\n                positions.append(None)\n            else:\n                positions.append(game.map.get_cell(x, y))\n    \n    resources_wood = []\n    resources_coal = []\n    resources_uranium = []\n    #resources_amounts = []\n    \n    for i in range(len(positions)):\n        cell = positions[i]\n        if cell is None or not cell.has_resource():\n            #resources_types.append(0)\n            resources_wood.append(0)\n            resources_coal.append(0)\n            resources_uranium.append(0)\n        else:\n            if cell.resource.type == Resource.Types.WOOD:\n                resources_wood.append(cell.resource.amount)\n                resources_coal.append(0)\n                resources_uranium.append(0)\n            elif cell.resource.type == Resource.Types.COAL:\n                resources_wood.append(0)\n                resources_coal.append(cell.resource.amount)\n                resources_uranium.append(0)\n            elif cell.resource.type == Resource.Types.URANIUM :\n                resources_wood.append(0)\n                resources_coal.append(0)\n                resources_uranium.append(cell.resource.amount)\n            else:\n                raise ValueError(\"build_observation\")\n            \n    \n    city_tiles_team_mine = []\n    city_tiles_team_other = []\n    \n    for i in range(len(positions)):\n        cell = positions[i]\n        if cell is None or not cell.is_city_tile():\n            city_tiles_team_mine.append(0)\n            city_tiles_team_other.append(0)\n        else:\n            ct = cell.city_tile\n            if ct.team == team:\n                city_tiles_team_mine.append(1)\n                city_tiles_team_other.append(0)\n            else:\n                city_tiles_team_mine.append(0)\n                city_tiles_team_other.append(1)\n\n    worker_cargo_team_mine = [ -1 ] * len(positions)\n    kart_cargo_team_mine = [ -1 ] * len(positions)\n    worker_cargo_team_other = [ -1 ] * len(positions)\n    kart_cargo_team_other = [ -1 ] * len(positions)\n    \n    for i in range(len(positions)):\n        cell = positions[i]\n        if cell is None or not cell.has_units():\n            worker_cargo_team_mine[i] = -1\n            kart_cargo_team_mine[i] = -1\n            worker_cargo_team_other[i] = -1\n            kart_cargo_team_other[i] = -1\n        else:\n            for unit in cell.units.values() :\n                if unit.is_worker():\n                    if unit.team == team:\n                        worker_cargo_team_mine[i] = unit.cargo[\"wood\"] + unit.cargo[\"coal\"] + unit.cargo[\"uranium\"]\n                    else:\n                        worker_cargo_team_other[i] = unit.cargo[\"wood\"] + unit.cargo[\"coal\"] + unit.cargo[\"uranium\"]\n                elif unit.is_cart():\n                    if unit.team == team:\n                        worker_cargo_team_mine[i] = unit.cargo[\"wood\"] + unit.cargo[\"coal\"] + unit.cargo[\"uranium\"]\n                    else:\n                        worker_cargo_team_other[i] = unit.cargo[\"wood\"] + unit.cargo[\"coal\"] + unit.cargo[\"uranium\"]\n    \n    observation = game_state + resources_wood + resources_coal + resources_uranium + city_tiles_team_mine + city_tiles_team_other + worker_cargo_team_mine + kart_cargo_team_mine + worker_cargo_team_other + kart_cargo_team_other\n    return np.array(observation).astype(np.int32)\n\n","0df50fbd":"\nclass PyLuxEnvironmentWorkerScalar(py_environment.PyEnvironment):\n    def __init__(self, worker_agent_class, kart_agent_class, city_agent_class, discount = np.float32(1.0)):\n        self._configs = LuxMatchConfigs_Default\n        self._game = Game(self._configs)\n        self._episode_ended = False\n        self._discount = discount\n        self._current_time_step = None\n        self._worker_agent_class = worker_agent_class\n        self._kart_agent_class = kart_agent_class\n        self._city_agent_class = city_agent_class\n        self._worker_agent = None\n        self._workers_to_manage = []\n        self._carts_to_manage = []\n        self._cities_to_manage = []\n        self._action_spec = array_spec.BoundedArraySpec(shape=(),\n                                                        dtype=np.int32,\n                                                        minimum=0, \n                                                        maximum=10,\n                                                        name='action')\n        self._observation_spec = array_spec.ArraySpec(shape = (259,), dtype = np.int32)\n        \n    def _reset(self) -> TimeStep:\n        if self._game is None or self._game.match_over():\n            self._game = Game(self._configs)\n            \n         \n        all_cities = list(self._game.cities.keys())\n        all_unit_ids = list(self._game.state[\"teamStates\"][0][\"units\"].keys()) + list(self._game.state[\"teamStates\"][1][\"units\"].keys())\n        all_workers = []\n        all_carts = []\n        \n        for unit_id in all_unit_ids:\n            unit = my_get_unit(self._game, unit_id)\n            if unit.is_worker():\n                all_workers.append(unit)\n            elif unit.is_cart():\n                all_carts.append(unit)\n            else:\n                raise ValueError(\"_reset1\")\n                \n        if len(all_workers) > 0:\n            self._carts_to_manage = all_carts\n            self._worker_agent = np.random.choice(all_workers)\n            self._worker_agent_team = self._worker_agent.team\n            all_workers.remove(self._worker_agent)            \n            self._workers_to_manage = all_workers # minus the one we just removed\n            observation = build_observation(self._game, self._worker_agent.id)\n            if not self._observation_spec.check_array(observation):\n                #ipdb.set_trace()\n                raise ValueError(\"PyLuxEnvironmentWorkerScalar._reset {}\".format(type(observation)))\n            step_type = StepType.FIRST\n            reward = np.float32(-1.0)\n            discount = self._discount\n            first_ts = TimeStep(step_type, reward, discount, observation)\n    \n            self._current_time_step = first_ts\n            self._episode_ended = False\n            return self._current_time_step            \n        else:\n            #raise ValueError(\"_reset2\")\n            self._game = None\n            return self._reset()\n            \n            \n            \n\n        \n    def _step(self, action):\n        if not self._action_spec.check_array(action):\n            raise ValueError(\"PyLuxEnvironmentWorkerScalar._step {}\".format(action))\n        #print(action)\n        turn = self._game.state['turn']\n        #if turn == 198 or turn == 199:\n        #    print(\"turn_step {}\".format(turn))\n        if self._episode_ended:\n            #print(\"step reset\")\n            return self.reset()\n        \n        actions = [] \n        # Array of actions for both teams. Eg: MoveAction(team, unit_id, direction)        \n        # game_over = game.run_turn_with_actions(actions)\n        # observation, reward, self._episode_ended, self._info = self._game.run_turn_with_actions(actions)\n        actions_spawn_workers = self.scalar_to_action_citytile(self._game, 42)\n        unit = my_get_unit(self._game, self._worker_agent.id)\n        if unit is not None:\n            action_str = self.scalar_to_action_worker(action)\n            if not action_str is None:\n                actions.append(action_str)\n        actions.extend(actions_spawn_workers)\n        \n        self._episode_ended = self._game.run_turn_with_actions(actions)\n        if self._game.state['turn'] > 360:\n            print(\"PyLuxEnvironmentWorkerScalar._step: turn > 360\")\n        team = self._worker_agent.team\n        team_units = self._game.state['teamStates'][team]['units']\n        if not self._worker_agent.id in team_units:\n            #print(\"agent died\")                        \n            if len(team_units) == 0:\n                team_ctt = my_get_citytiles(self._game, team)\n                if len(team_ctt) == 0:\n                    # end game as no city and no agent\n                    self._episode_ended = True\n                else:                    \n                    bas = self.get_action_build_worker(self._game, team)\n                    self._episode_ended = self._game.run_turn_with_actions(bas)\n                    if not self._episode_ended:\n                        team_workers = [ unit for unit in self._game.state['teamStates'][team]['units'].values() if unit.is_worker()]\n                        if len(team_workers) == 0:\n                            self._episode_ended = True\n                        else:\n                            worker = random.sample(team_workers, 1)[0]\n                            self._worker_agent = worker\n            else:\n                team_workers = [ unit for unit in self._game.state['teamStates'][team]['units'].values() if unit.is_worker()]\n                if len(team_workers) == 0:\n                    self._episode_ended = True\n                else:\n                    worker = random.sample(team_workers, 1)[0]\n                    self._worker_agent = worker\n                \n                    \n            \n        observation = build_observation(self._game, self._worker_agent.id)\n        observation = np.array(observation , dtype=np.int32)\n        reward = build_reward(self._game, self._worker_agent.id, self._worker_agent_team)\n        if not self._observation_spec.check_array(observation):\n            ipdb.set_trace()\n            raise ValueError(\"PyLuxEnvironmentWorkerScalar._step\")\n        if self._episode_ended:# or self._worker_agent.id not in all_unit_ids:\n            return ts.termination(observation, reward)\n        else:\n            #ipdb.set_trace()\n            return ts.transition(observation, reward, self._discount)\n\n    def scalar_to_action_citytile(self, game : Game, action : np.int32) -> List[Action] :\n        # dummy to just generate more workers\n        team0 = 0\n        team1 = 1\n        units0 = my_get_units(game, team0)\n        units1 = my_get_units(game, team1)\n        actions = []\n        if len(units0) < 1:\n            actionlist0 = self.get_action_build_worker(game, team0)\n            actions.append(actionlist0)\n        if len(units1) < 1:\n            actionlist1 = self.get_action_build_worker(game, team0)\n            actions.append(actionlist1)\n        action_list = [item for sublist in actions for item in sublist]\n        return action_list\n            \n    def get_action_build_worker(self, game : Game, team : int) -> List[Action] :\n        ctt_list = [ctt for ctt in my_get_citytiles(game, team) if ctt.can_act() ]\n        if len(ctt_list) == 0:\n            return []\n        ctt_list = random.sample(ctt_list, min(1, len(ctt_list)))\n        actions = []\n        for ctt in ctt_list:\n            action = SpawnWorkerAction(team, None, ctt.pos.x, ctt.pos.y)\n            actions.append(action)\n        return actions\n        \n        \n    def scalar_to_action_worker(self, action) -> Action :\n        # add type checking\n        unit_id = self._worker_agent.id\n        team = self._worker_agent.team\n        unit = my_get_unit(self._game, self._worker_agent.id)\n        if unit is None:\n            return None\n        # dont move\n        if action == 0:\n            ma = MoveAction(team, unit_id, Constants.DIRECTIONS.CENTER)\n            return ma\n        # move north\n        elif action == 1:\n            ma = MoveAction(team, unit_id, Constants.DIRECTIONS.NORTH)\n            return ma\n        # move east\n        elif action == 2:\n            ma = MoveAction(team, unit_id, Constants.DIRECTIONS.EAST)\n            return ma\n        # move south\n        elif action == 3:\n            ma = MoveAction(team, unit_id, Constants.DIRECTIONS.SOUTH)\n            return ma\n        # move west\n        elif action == 4:\n            ma = MoveAction(team, unit_id, Constants.DIRECTIONS.WEST)\n            return ma\n        # build city\n        elif action == 5:\n            sca = SpawnCityAction(team, unit_id)\n            return sca\n        # pillage\n        elif action == 6:\n            pa = PillageAction(team, unit_id)\n            return pa\n        # transfer north\n        elif action == 7:\n            dest_unit = my_get_unit_adjacent(self._game, team, unit_id, Constants.DIRECTIONS.NORTH)\n            if dest_unit is None:\n                return None\n            else:\n                cargo_type = max(self._worker_agent.cargo, key=self._worker_agent.cargo.get)\n                cargo_amount = self._worker_agent.cargo[cargo_type]\n                ta = TransferAction(team, unit_id, dest_unit.id, cargo_type, cargo_amount)\n                return ta\n        # transfer east\n        elif action == 8:\n            dest_unit = my_get_unit_adjacent(self._game, team, unit_id, Constants.DIRECTIONS.EAST)\n            if dest_unit is None:\n                return None\n            else:\n                cargo_type = max(self._worker_agent.cargo, key=self._worker_agent.cargo.get)\n                cargo_amount = self._worker_agent.cargo[cargo_type]\n                ta = TransferAction(team, unit_id, dest_unit.id, cargo_type, cargo_amount)\n                return ta\n        # transfer south\n        elif action == 9:\n            dest_unit = my_get_unit_adjacent(self._game, team, unit_id, Constants.DIRECTIONS.SOUTH)\n            if dest_unit is None:\n                return None\n            else:\n                cargo_type = max(self._worker_agent.cargo, key=self._worker_agent.cargo.get)\n                cargo_amount = self._worker_agent.cargo[cargo_type]\n                ta = TransferAction(team, unit_id, dest_unit.id, cargo_type, cargo_amount)\n                return ta\n        # transfer west\n        elif action == 10:\n            dest_unit = my_get_unit_adjacent(self._game, team, unit_id, Constants.DIRECTIONS.WEST)\n            if dest_unit is None:\n                return None\n            else:\n                cargo_type = max(self._worker_agent.cargo, key=self._worker_agent.cargo.get)\n                cargo_amount = self._worker_agent.cargo[cargo_type]\n                ta = TransferAction(team, unit_id, dest_unit.id, cargo_type, cargo_amount)\n                return ta\n        else:\n            raise ValueError(\"invalid action {}\".format(action))\n        \n    def action_spec(self):\n        return self._action_spec\n\n    def observation_spec(self):\n        return self._observation_spec\n","126ff46a":"#DQNWorkerScalar\nnum_iterations = 20000 # @param {type:\"integer\"}\n\ninitial_collect_steps = 1  # @param {type:\"integer\"}\ncollect_steps_per_iteration =   1# @param {type:\"integer\"}\nreplay_buffer_max_length = 100000  # @param {type:\"integer\"}\n\nbatch_size = 64  # @param {type:\"integer\"}\nlearning_rate = 1e-3  # @param {type:\"number\"}\nlog_interval = 1000  # @param {type:\"integer\"}\n\nnum_eval_episodes = 10  # @param {type:\"integer\"}\neval_interval = 200  # @param {type:\"integer\"}\n\n","ba10741c":"train_py_env = PyLuxEnvironmentWorkerScalar(str, str, str)\neval_py_env = PyLuxEnvironmentWorkerScalar(str, str, str)\n\ntrain_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n","45e66814":"\n\nfc_layer_params = (259, 50)\naction_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\nnum_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n\n# Define a helper function to create Dense layers configured with the right\n# activation and kernel initializer.\ndef dense_layer(num_units):\n    return tf.keras.layers.Dense(\n      num_units,\n      activation=tf.keras.activations.relu,\n      kernel_initializer=tf.keras.initializers.VarianceScaling(\n          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n\n# QNetwork consists of a sequence of Dense layers followed by a dense layer\n# with `num_actions` units to generate one q_value per available action as\n# its output.\ndense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\nq_values_layer = tf.keras.layers.Dense(\n    num_actions,\n    activation=None,\n    kernel_initializer=tf.keras.initializers.RandomUniform(\n        minval=-0.03, maxval=0.03),\n    bias_initializer=tf.keras.initializers.Constant(-0.2))\nq_net = sequential.Sequential(dense_layers + [q_values_layer])\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\nglobal_step = tf.compat.v1.train.get_or_create_global_step()\n#train_checkpointer.initialize_or_restore()\n#global_step = tf.compat.v1.train.get_global_step()\n#saved_policy = tf.saved_model.load(policy_dir)\n\ntrain_step_counter = tf.Variable(0)\nepoch_counter = tf.Variable(0)\n\nagent = dqn_agent.DqnAgent(\n    train_env.time_step_spec(),\n    train_env.action_spec(),\n    q_network=q_net,\n    optimizer=optimizer,\n    td_errors_loss_fn=common.element_wise_squared_loss,\n    train_step_counter=train_step_counter)\n","ec2c9011":"\nagent.initialize()\n\neval_policy = agent.policy\ncollect_policy = agent.collect_policy\n\nfrom tf_agents.policies import random_tf_policy\nrandom_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n                                                train_env.action_spec())\n","0e6fa228":"#@test {\"skip\": true}\ndef compute_avg_return(environment, policy, num_episodes=10):\n\n    total_return = 0.0\n    \n    for _ in range(num_episodes):\n\n        time_step = environment.reset()\n        episode_return = 0.0\n\n        while not time_step.is_last():\n            action_step = policy.action(time_step)\n            time_step = environment.step(action_step.action)\n            episode_return += time_step.reward\n        total_return += episode_return\n\n    avg_return = total_return \/ num_episodes\n    return avg_return.numpy()[0]\n\n\n\n","c0e44401":"avg_random = compute_avg_return(eval_env, random_policy, num_eval_episodes)","b4b63ab2":"avg_random","ca6b0d1d":"table_name = 'uniform_table'\nreplay_buffer_signature = tensor_spec.from_spec(\n      agent.collect_data_spec)\nreplay_buffer_signature = tensor_spec.add_outer_dim(\n    replay_buffer_signature)\n\ntable = reverb.Table(\n    table_name,\n    max_size=replay_buffer_max_length,\n    sampler=reverb.selectors.Uniform(),\n    remover=reverb.selectors.Fifo(),\n    rate_limiter=reverb.rate_limiters.MinSize(1),\n    signature=replay_buffer_signature)\n\nreverb_server = reverb.Server([table])\n\nreplay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n    agent.collect_data_spec,\n    table_name=table_name,\n    sequence_length=2,\n    local_server=reverb_server)\n\nrb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n  replay_buffer.py_client,\n  table_name,\n  sequence_length=2)","a8098094":"env = PyLuxEnvironmentWorkerScalar(str, str, str)\n#@test {\"skip\": true}\npy_driver.PyDriver(\n    env,\n    py_tf_eager_policy.PyTFEagerPolicy(\n      random_policy, use_tf_function=True),\n    [rb_observer],\n    max_steps=initial_collect_steps).run(train_py_env.reset())","c1f2141f":"dataset = replay_buffer.as_dataset(\n    num_parallel_calls=3,\n    sample_batch_size=batch_size,\n    num_steps=2).prefetch(3)","4bd8ab86":"iterator = iter(dataset)\n#print(iterator)","c7784adf":"#@test {\"skip\": true}\ntry:\n    %%time\nexcept:\n    pass\n\n# (Optional) Optimize by wrapping some of the code in a graph using TF function.\nagent.train = common.function(agent.train)\n\n# Reset the train step.\nagent.train_step_counter.assign(0)\n\n# Evaluate the agent's policy once before training.\navg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\nreturns = [avg_return]\n\n# Reset the environment.\ntime_step = train_py_env.reset()\n\n# Create a driver to collect experience.\ncollect_driver = py_driver.PyDriver(\n    env,\n    py_tf_eager_policy.PyTFEagerPolicy(\n      agent.collect_policy, use_tf_function=True),\n    [rb_observer],\n    max_steps=collect_steps_per_iteration)\n\nstart_time = time.time()\n\nfor _ in range(num_iterations):\n\n    \n    # Collect a few steps and save to the replay buffer.\n    time_step, _ = collect_driver.run(time_step)\n\n    # Sample a batch of data from the buffer and update the agent's network.\n    experience, unused_info = next(iterator)\n    train_loss = agent.train(experience).loss\n\n    step = agent.train_step_counter.numpy()\n\n    if step % log_interval == 0:\n        print('step = {0:,d}: loss = {1:,d}'.format(step, int(train_loss)))\n\n    if step % eval_interval == 0:\n        \n        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n        print('step = {0:,d}: Average Return = {1:,d}'.format(step, int(avg_return)))\n        returns.append(avg_return)\n        #print(\"--- %s seconds ---\" % (time.time() - start_time))\n        start_time = time.time()\n    \nprint(\"done\")","d104ebcc":"#@test {\"skip\": true}\n\niterations = range(0, num_iterations + 1, eval_interval)\nplt.plot(iterations, returns)\nplt.ylabel('Average Return')\nplt.xlabel('Iterations')\n#plt.ylim(top=250)\nplt.show()","79524ce7":"# Training the agent\n\nTwo things must happen during the training loop:\n\n-   collect data from the environment\n-   use that data to train the agent's neural network(s)\n\nThis example also periodicially evaluates the policy and prints the current score.\n\nThe following will take ~5 minutes to run.","6ad9e99f":"# Create observation","610a0184":"# Utility functions","4cf7249f":"# Reward function","dd5b084b":"build the environment","dbdba2b7":"# Replay Buffer\n\nIn order to keep track of the data collected from the environment, we will use [Reverb](https:\/\/deepmind.com\/research\/open-source\/Reverb), an efficient, extensible, and easy-to-use replay system by Deepmind. It stores experience data when we collect trajectories and is consumed during training.\n\nThis replay buffer is constructed using specs describing the tensors that are to be stored, which can be obtained from the agent using agent.collect_data_spec.","f861fd97":"# Train a Deep Q Network with TF-Agents\n\nadapted from : https:\/\/www.tensorflow.org\/agents\/tutorials\/1_dqn_tutorial","b7d9ac46":"# Data Collection\n\nNow execute the random policy in the environment for a few steps, recording the data in the replay buffer.\n\nHere we are using 'PyDriver' to run the experience collecting loop. You can learn more about TF Agents driver in our [drivers tutorial](https:\/\/www.tensorflow.org\/agents\/tutorials\/4_drivers_tutorial).","5136143e":"# Define the RL environment","74ef5b20":"# Hyperparameters","ea07a228":"# Define a Deep Q Network","68c6738c":"The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n\nEach row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n\nThis dataset is also optimized by running parallel calls and prefetching data.","6c7bbad7":"# Test the Random policy","95793f80":"# Install dependancies","b3f17a20":"# Plot the training improvements","c0d13a87":"# Import necessary modules"}}