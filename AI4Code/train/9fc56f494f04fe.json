{"cell_type":{"9baf7730":"code","865b0b40":"code","40983b10":"code","e0aee9db":"code","950bd961":"code","50bb00c0":"code","f89f2222":"code","cb455f44":"code","c06e17f7":"code","a34daf04":"code","b82dcbdb":"code","71cda236":"code","458095a4":"code","58ed1ce8":"code","1a180ad4":"code","e2c2f676":"code","7a8a2f45":"code","81918df1":"code","54ffc453":"code","de51ece0":"code","806543d6":"code","212164bb":"code","0354d030":"code","ab21f3ac":"code","72e444e2":"code","3192ed96":"code","a02552b9":"code","3caf34c8":"code","c6d9fb81":"code","ed253b96":"code","2f28a409":"code","a3fef4a0":"code","c177d699":"code","587bef40":"code","cf0f6a22":"code","3b64f826":"code","5ab4b543":"markdown","c4d15fef":"markdown","20bce8f7":"markdown","a14fbee2":"markdown","a4b7159d":"markdown","0818de62":"markdown","62b7bdb7":"markdown","9b611a52":"markdown","08de325b":"markdown","2416c188":"markdown","70c8e624":"markdown","f82dc60b":"markdown","124549ee":"markdown","fb0b6bb8":"markdown","4ae38caf":"markdown","027af6c0":"markdown","a426433d":"markdown","f6a67f72":"markdown","87eb64eb":"markdown","0cb5cecd":"markdown","1004ec61":"markdown","20525e74":"markdown","34fb8142":"markdown","06018566":"markdown","8cfd94f8":"markdown","66f0a410":"markdown","d5af2f66":"markdown","5d364ddb":"markdown","25cc63dd":"markdown","37279fe3":"markdown","07c101e9":"markdown","5cc5de78":"markdown","6d04575d":"markdown","e2192ff5":"markdown","c0de6956":"markdown","e6c8f513":"markdown","526b3b99":"markdown","9a29ed94":"markdown","616fc020":"markdown","d6d08869":"markdown","590c61b5":"markdown","6514a555":"markdown","6c34eba5":"markdown","888a0f90":"markdown","a6105c6a":"markdown","57dd96b0":"markdown","51629633":"markdown","ff88e803":"markdown","be368d68":"markdown","bfed6ba7":"markdown","2489a99d":"markdown","25e651cd":"markdown","26173190":"markdown","d0944913":"markdown","07fa0cad":"markdown","11fbdab7":"markdown","06106228":"markdown","cfef2868":"markdown","58bdff05":"markdown","181129b0":"markdown","83ebe091":"markdown","03eeeefb":"markdown","a0127e72":"markdown"},"source":{"9baf7730":"# Necessary Imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nimport sklearn\nimport keras\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom datetime import datetime, date\nfrom keras import Sequential\nfrom keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization\n\nimport warnings\nwarnings.filterwarnings('ignore')","865b0b40":"data = pd.read_csv('\/kaggle\/input\/earthquakes\/Earthquakes.csv', delimiter=',')","40983b10":"data.head()","e0aee9db":"data.describe()","950bd961":"# Function to calculate the days passed\ndef calculate_days(date_string_1, date_string_2):\n  date_num_1=datetime.strptime(date_string_1, '%Y-%m-%d %H:%M:%S')\n  date_num_2=datetime.strptime(date_string_2, '%Y-%m-%d %H:%M:%S')\n\n  # Sorting the dates descending\n  if date_num_2>date_num_1:\n    a=date_num_1\n    date_num_1=date_num_2\n    date_num_2=a\n\n  # Concatenating dates\n  d1=date(year=date_num_1.year, month=date_num_1.month, day=date_num_1.day)\n  d2=date(year=date_num_2.year, month=date_num_2.month, day=date_num_2.day)\n\n  return (d1-d2).days\n\n#-------------------------------------------------------------------------\n\n# Function to scale the time between [0,1)\ndef hour_rate(date_string):\n  date_num=datetime.strptime(date_string, '%Y-%m-%d %H:%M:%S')\n\n  # Calculating the rate\n  hr=((date_num.hour*60) + date_num.minute) \/ (24*60)\n\n  return hr","50bb00c0":"data[\"Time Gap\"]=[calculate_days(data[\"Date(UTC)\"][i], data[\"Date(UTC)\"][i-1]) if i>0 else 0 for i in range(0, len(data[\"Date(UTC)\"]))]\n\ndata[\"Hour-Day Ratio\"]=[hour_rate(data[\"Date(UTC)\"][i]) for i in range(0, len(data[\"Date(UTC)\"]))]","f89f2222":"begin_date=\"1900-01-01 00:00:00\"\n\ndata[\"Days\"]=[calculate_days(data[\"Date(UTC)\"][i], begin_date) for i in range(0, len(data[\"Date(UTC)\"]))]\n\ndata[\"Month\"]=[datetime.strptime(i, '%Y-%m-%d %H:%M:%S').month for i in data[\"Date(UTC)\"]]","cb455f44":"data[\"Constant Deg.\"]=data[\"Constant Deg.\"].replace({\"No\":0, \"Yes\":1})\n\ndata=data.sort_values(by=\"Days\", ascending=True).reset_index(drop=True)\n\ndata=data.drop([\"No\", \"Ref1\", \"Source Description 1\", \"Source No 2\", \n                \"Source Description 2\", \"Source No 3\", \"Source Description 3\", \n                \"Type\", \"Date(UTC)\"], axis=1)","c06e17f7":"# Loading risk map\nraw_img=cv2.imread(\"\/kaggle\/input\/earthquakes\/risk_map_clean.jpg\", cv2.IMREAD_GRAYSCALE)","a34daf04":"# Recovering process\n\n# Initial values\nsize=6\nincrement=2\nepoch=4\nrecovered_img=raw_img.copy()\n\n# Filtering\nfor i in range(0,epoch):\n  \n  width_step=np.shape(recovered_img)[1]\/size\n  height_step=np.shape(recovered_img)[0]\/size\n\n  # Filter Striding\n  for h in range(0, int(height_step)):\n    for w in range(0, int(width_step)):\n\n      window=recovered_img[h*size:(h+1)*size, w*size:(w+1)*size]\n\n      # At first epoch, values are maximized; then minimized. \n      if i==0:\n        window=window.max()\n      else:\n        window=window.min()\n\n      recovered_img[h*size:(h+1)*size, w*size:(w+1)*size]=window\n\n  size+=increment","b82dcbdb":"# Value Replacement\nrisk_map=recovered_img.copy()\n\n# Threshold values\nhigh=90\nmedium=175\nlow=235\nno_data=250\ndefault=5\n\n# Replacement\nrisk_map=np.where(risk_map<=high, 4, risk_map)\nrisk_map=np.where(((risk_map>high) & (risk_map<=medium)), 3, risk_map)\nrisk_map=np.where(((risk_map>medium) & (risk_map<=low)), 2, risk_map)\nrisk_map=np.where(((risk_map>low) & (risk_map<=no_data)), 1, risk_map)\nrisk_map=np.where(risk_map>no_data, default, risk_map)","71cda236":"# Visualization of all 3 maps\nmap_names={\"Raw Risk Map\":raw_img, \"Recovered Risk Map\":recovered_img, \"Ready-to-Use Risk Map\":risk_map}\n\nfig=plt.figure(figsize=(16, 9))\n\nfor i, val in enumerate(map_names):\n    fig.add_subplot(2, 2, i+1)\n    plt.imshow(map_names[val], cmap=\"gray\")\n    plt.title(val)\n    plt.tight_layout()\n\nplt.show()","458095a4":"#Function to find risk class\ndef risk_grader(latitude, longitude):\n  # Begin and end coordinates of the map used.\n  west=25.67\n  east=44.81\n  south=35.81\n  north=42.10\n\n  # Checking coordinates whether involved by map\n  if (longitude<west) or (longitude>east) or (latitude<south) or (latitude>north):\n    return default\n\n  # Calculating ratio between real land piece and map image pixels\n  real_width=east-west\n  real_height=north-south\n\n  map_width=np.shape(risk_map)[1]\n  map_height=np.shape(risk_map)[0]\n\n  width_ratio=map_width\/(real_width*100)\n  height_ratio=map_height\/(real_height*100)\n\n  # Calculating pixels to look up for the grade\n  easting=longitude-west\n  northing=latitude-south\n\n  pixel_to_right=int(round(easting*100*width_ratio))\n  pixel_to_up=map_height-int(round(northing*100*height_ratio))\n\n  # Correction of the error caused by floating points\n  if pixel_to_right>=map_width:\n    pixel_to_right=map_width-1\n\n  if pixel_to_up>=map_height:\n    pixel_to_up=map_height-1\n\n  # reading risk grade from the map array\n  grade=risk_map[pixel_to_up, pixel_to_right]\n\n  return grade","58ed1ce8":"# Finding risk grade for every earthquake\ndata[\"Risk Grade\"]=[risk_grader(data[\"Latitude\"][i], data[\"Longitude\"][i]) for i in range(len(data[\"Latitude\"]))]","1a180ad4":"# A glance on dataset\ndata.head()","e2c2f676":"# Calculating and concatenating\nextended_data=pd.DataFrame()\n\nextended_data=data.copy()\n\nfor i in extended_data.columns:\n  extended_data[\"Log.\"+i]=np.log(extended_data[i]+0.01)","7a8a2f45":"# Scatter Plots\nlines=(((len(extended_data.columns)-1)*(len(extended_data.columns)))\/8)+1\nk=1\n\nsubplt=plt.figure(figsize=(16, 120))\n\nfor i in range(0, len(extended_data.columns)-1):\n  for j in range(i+1, len(extended_data.columns)-1):\n    subplt.add_subplot(lines, 4, k)\n    \n    plt.scatter(extended_data[extended_data.columns[i]], extended_data[extended_data.columns[j]])\n    plt.title(\"{} X {}\".format(extended_data.columns[i], extended_data.columns[j]))\n    plt.xlabel(extended_data.columns[i])\n    plt.ylabel(extended_data.columns[j])\n\n    k+=1\n\nplt.tight_layout()\nplt.show()","81918df1":"extended_corr=extended_data.corr()\n\nplt.figure(figsize=(20,9))\nsns.heatmap(extended_corr, vmin=-1, vmax=1, cmap=\"bwr\", annot=True, linewidth=0.1)\nplt.title(\"Parametre Correlation Matrix\")\nplt.show()","54ffc453":"new_labels=[\"Latitude\", \"Longitude\", \"Days\", \"Magnitude\", \n            \"Depth\", \"Constant Deg.\", \"Risk Grade\", \"Time Gap\", \n            \"Log.Days\", \"Log.Hour-Day Ratio\"]\n\nnew_data=extended_data[new_labels]","de51ece0":"percentage=0.25\n\nduration=20\n\ntest_size=int(len(new_data)*percentage)","806543d6":"# Dense Network Dataset partition\nper=0.25\n\ntest_size=int(len(new_data)*per)\n\nnew_train=new_data[0:len(new_data)-test_size]\nnew_test=new_data[len(new_data)-test_size:len(new_data)]\n\nX_train_Dense=new_train.iloc[:,4:]\nX_test_Dense=new_test.iloc[:,4:]\n\ny_train_Dense=new_train.iloc[:,0:4]\ny_test_Dense=new_test.iloc[:,0:4]\n\n# Data set scaling\nscaler_Dense_X=MinMaxScaler(feature_range=(0,1))\nscaler_Dense_y=MinMaxScaler(feature_range=(0,1))\n\nX_train_Dense=scaler_Dense_X.fit_transform(X_train_Dense)\nX_test_Dense=scaler_Dense_X.fit_transform(X_test_Dense)\n\ny_train_Dense=scaler_Dense_y.fit_transform(y_train_Dense)\ny_test_Dense=scaler_Dense_y.fit_transform(y_test_Dense)","212164bb":"# Dense Network Dataset Scaling and Partition\n\n# Definings\nscaler_LSTM_X=MinMaxScaler(feature_range=(0,1))\nscaler_LSTM_y=MinMaxScaler(feature_range=(0,1))\n\nnew_data_array=np.array(new_data)\n\nX_LSTM, y_LSTM=[], []\n\n# y Subset Seperation and Scaling\ny_LSTM=new_data_array[duration:,0:4]\ny_LSTM=scaler_LSTM_y.fit_transform(y_LSTM)\n\n# X Subset Seperation an Scaling\nnew_data_array=scaler_LSTM_X.fit_transform(new_data_array)\n\nfor i in range(0,len(new_data_array)-duration):\n  partial=new_data_array[i:i+duration]\n  X_LSTM=np.append(X_LSTM, partial)\n\nX_LSTM=np.reshape(X_LSTM, (np.shape(new_data_array)[0]-duration, duration, np.shape(new_data_array)[1]))\n\n# Partition to Train and Test Subsets\nX_train_LSTM=X_LSTM[0:len(X_LSTM)-test_size]\nX_test_LSTM=X_LSTM[len(X_LSTM)-test_size:]\n\ny_train_LSTM=y_LSTM[0:len(y_LSTM)-test_size]\ny_test_LSTM=y_LSTM[len(y_LSTM)-test_size:]","0354d030":"def loss_reduce(loss):\n  new_loss=[]\n  fold=int(len(loss)\/np.min(epoch))\n\n  for i in range(0,np.min(epoch)):\n    local_mean=np.mean(loss[fold*i:fold*(i+1)])\n    new_loss.append(local_mean)\n\n  return new_loss","ab21f3ac":"# Building preset models\ndef model_builder(builder=\"Dense\", hidden=32, optimizer=\"rmsprop\"):\n  if builder==\"Dense\":\n    model=Sequential()\n\n    model.add(Dense(hidden, activation=\"relu\", input_shape=(6,)))\n    model.add(BatchNormalization())\n    model.add(Dense(4))\n\n    model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mae\"])\n\n  if builder==\"LSTM\":\n    model=Sequential()\n\n    model.add(GRU(hidden, return_sequences=True))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(LSTM(units=hidden))\n    model.add(Dense(units=4))\n\n    model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mae\"])\n\n  return model","72e444e2":"# Searching of hyperparameters\ndef GraphSearch(model_type):\n\n  # Getting beginning values\n  files=False\n  hid, opt, bat, epo, row=0, 0, 0, 0, 0\n  min_score=1000000\n  best_model=[]\n    \n  loss=pd.DataFrame()\n  values=pd.DataFrame()\n    \n  # Loading relevant dataset\n  if model_type==\"LSTM\":\n    X_train=X_train_LSTM\n    X_test=X_test_LSTM\n    y_train=y_train_LSTM\n    y_test=y_test_LSTM\n\n  else:\n    X_train=X_train_Dense\n    X_test=X_test_Dense\n    y_train=y_train_Dense\n    y_test=y_test_Dense\n\n  # Graph searching\n  for hid in hidden:\n    for opt in optimizers:\n      for bat in batch:\n        for epo in epoch:\n\n          print(\"{} \/ {} \".format(row+1, len(hidden)*len(optimizers)*len(batch)*len(epoch)))\n\n          # Training\n          model=model_builder(builder=model_type, hidden=hid, optimizer=opt)\n\n          record=model.fit(X_train, y_train, epochs=epo, batch_size=bat, validation_split=0.25, verbose=0)\n\n          # Evaluating and collecting records\n          evaluation=model.evaluate(X_test, y_test)\n\n######     loss[row]=loss_reduce(record.history[\"loss\"])\n          loss=loss.append(loss_reduce(record.history[\"loss\"]))\n\n          values=values.append({\"hidden\":hid,\n                                \"optimizer\":opt,\n                                \"batch\":bat,\n                                \"epochs\":epo,\n                                \"evaluation_0\":evaluation[0],\n                                \"evaluation_1\":evaluation[1]},\n                                ignore_index=True)\n\n            # Fixing the best model\n          if evaluation[0]<min_score:\n            min_score=evaluation[0]\n            best_model=model\n\n          row+=1\n\n          print(\"Done!..\\n\")\n            \n  return loss, values, best_model","3192ed96":"# Parameters and running of dense model\nhidden=[32, 64]\noptimizers=[\"rmsprop\"]\nbatch=[32, 64]\nepoch=[300, 700]\n\nloss_Dense, values_Dense, model_Dense=GraphSearch(\"Dense\")","a02552b9":"# Parameters and running of LSTM model\nhidden=[30]\noptimizers=[\"rmsprop\"]\nbatch=[32, 64, 128]\nepoch=[750]\n\nloss_LSTM, values_LSTM, model_LSTM=GraphSearch(\"LSTM\")","3caf34c8":"# Definings for easy use\nvariables=[\"epochs\", \"batch\", \"optimizer\", \"hidden\"]\n\ntypes=[\"Dense\", \"LSTM\"]\n\ndict={\"values_Dense\":values_Dense,\n      \"values_LSTM\":values_LSTM,\n      \"loss_Dense\":loss_Dense,\n      \"loss_LSTM\":loss_LSTM}","c6d9fb81":"# Models \nsns.set(style=\"darkgrid\")\nfig=plt.figure(figsize=(16, 7))\ni=1\n\nplt.suptitle(\"Min - Mean - Max Values on Each Hyperparameter\", y=1.03)\nfor val1 in types:\n  for val2 in variables:\n    fig.add_subplot(1, 8, i)\n\n    a=dict[\"values_\"+val1].groupby(val2)[\"evaluation_0\"].max()\n    sns.barplot(x=a.index, y=a.values)\n    b=dict[\"values_\"+val1].groupby(val2)[\"evaluation_0\"].mean()\n    sns.barplot(x=b.index, y=b.values)\n    c=dict[\"values_\"+val1].groupby(val2)[\"evaluation_0\"].min()\n    sns.barplot(x=c.index, y=c.values)\n\n    plt.title(val1+\" Network\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    i+=1\nplt.show()","ed253b96":"values_Dense[values_Dense[\"evaluation_0\"]==values_Dense[\"evaluation_0\"].min()]","2f28a409":"values_LSTM[values_LSTM[\"evaluation_0\"]==values_LSTM[\"evaluation_0\"].min()]","a3fef4a0":"variables=[\"epochs\", \"batch\", \"optimizer\", \"hidden\"]\n\ncol=\"batch\" # Change this and run again!\n\ncount=0\nfig=plt.figure(figsize=(16, 8))\n\nfor i in types:\n  mod_type=i\n  s1=\"loss_\"+mod_type\n  s2=\"values_\"+mod_type\n\n  fig.add_subplot(2, 2, count*2+1)\n  sns.lineplot(x=dict[s1].index, y=dict[s1].T.min())\n  plt.title(\"Minimum Values ( \"+i+\" )\")\n  plt.legend(dict[s2][col].unique())\n\n  fig.add_subplot(2, 2, count*2+2)\n  sns.lineplot(x=dict[s1].index, y=dict[s1].T.max())\n  plt.title(\"Maximum Values ( \"+i+\" )\")\n  plt.legend(dict[s2][col].unique())\n\n  count+=1\n\nplt.tight_layout()\nplt.show()","c177d699":"pred_Dense=model_Dense.predict(X_test_Dense)\npred_LSTM=model_LSTM.predict(X_test_LSTM)\n\npred_Dense=scaler_Dense_y.inverse_transform(pred_Dense)\npred_LSTM=scaler_LSTM_y.inverse_transform(pred_LSTM)\nreal_values=scaler_Dense_y.inverse_transform(y_test_Dense)","587bef40":"plt.figure(figsize=(20,6))\nplt.scatter(real_values.T[1], real_values.T[0], label=\"Real Values\", alpha=0.6)\nplt.scatter(pred_Dense.T[1], pred_Dense.T[0], label=\"Dense Prediction\", alpha=0.6)\nplt.scatter(pred_LSTM.T[1], pred_LSTM.T[0], label=\"LSTM Prediction\", alpha=0.4)\nplt.title(\"Coordinates Prediction\")\nplt.legend()\nplt.show()","cf0f6a22":"ind=[i for i in range(0, len(real_values))]\n\nplt.figure(figsize=(20,6))\nplt.scatter(ind, real_values.T[2], label=\"Real Values\")\nplt.scatter(ind, pred_Dense.T[2], label=\"Dense Prediction\")\nplt.scatter(ind, pred_LSTM.T[2], label=\"LSTM Prediction\")\nplt.title(\"Days Prediction\")\nplt.legend()\nplt.show()","3b64f826":"plt.figure(figsize=(20,6))\nplt.scatter(ind, real_values.T[3], label=\"Real Values\")\nplt.scatter(ind, pred_Dense.T[3], label=\"Dense Prediction\")\nplt.scatter(ind, pred_LSTM.T[3], label=\"LSTM Prediction\")\nplt.title(\"Magnitude Prediction\")\nplt.legend()\nplt.show()","5ab4b543":"LSTM performed bad. Dense model can give hope for days prediction.","c4d15fef":"#### Correlations are checked by use of heatmap.","20bce8f7":"On the scale which is shown with yellow and red gradients, if we go to right, quake risk increases. When both maps are compared to each other, quakes with bigger than 6 magnitude score, mostly overlaps with the highest risk grades.","a14fbee2":"## b. Feature Transformation","a4b7159d":"#### Values are taken logaritms and concatenated to dataset due to further investigation.","0818de62":"#### This function trains the model with parameters given, records values and the best model.","62b7bdb7":"As can be seen above:\n\n* LSTM models are much more better than Dense models,\n* Dense models can differentiate on a large scale,\n* All Dense models' minimum performances are similar to each other which can mean \"Dense models' best performances are restricted for this kind of datasets.\"\n* Hyperparameters achieve different effects on different model types. But generally, it is better to use higher epochs and lower hidden layers.","9b611a52":"# 1. INTRODUCTION","08de325b":"![Risk Map](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/420118\/812507\/risk_map.jpg?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1575203980&Signature=mGQXKuSVTV5c2PhDoMpxPl2UJEYOwkl3WONu6jNIVWh%2F2YO0HDMUuWAhke1ats7WHCFKRZuU5BmRsESESIIKD9HaMDHgjM4JijALldaiA3ocje8uoodGDp027tK%2FnIIMfK%2FQcZ27dGbB28217QnAgbEH7wwQOMoS%2BCTeJbIFW%2FGEu3sGe5ECwwcBMzrFKtuBgO%2FidhYwKtsuZ%2FvVAzVdLYs4myTxuX18A5ypoVPGxQWpbJreTGSlQyXTyT74%2BdQnzzZThOWE1G8ZQAg6cdfkIPnyj5qPm041o7haV2X%2BbqfBq0ao%2FysTstlmlK7Ha9x3OPBRMOsK2YDLQlrm8JyWtw%3D%3D)\n\n![Earthquake Map](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/420118\/812507\/earthquakes.jpg?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1575204009&Signature=JQBx01lsw6ZGL%2B4P2YotmygzHepxMA7e4P%2BGzaDLIp0jp%2FIMjAwDNL17NgZvoOcShsaHZY4JPWEP5DFcEdWJ40VrJuU8GXo7ub%2FqN%2B2TJGyaORx3u7owjWpAo1UB7uIOLeAtTQeXjMqjtnyH6S8IjkWeQc4eaVzoYmz2u6mX5G8xO%2FS%2BGPl4oLdrpm%2B5n%2BovZ2J4PWpQqqjhUgQjK65QBzGICjYUMnVNFslY9dHHy8UQLz2W6hBBfvwSAUJEgeAcM7ZYVzB%2FzbmaR0dhjjZ%2FSgRnoGCbIPYkyXOP8E3pHBoVj%2B3wB1PDrNaod4QcHkJT4Rq%2BJMOXUqTR9%2Fx2lEHQsQ%3D%3D)","2416c188":"Here are prediction results on test set.","70c8e624":"Prediction efforts in this study relies on checking out whether there are any correlations with former earthquakes.\n\nAccording to graphics above;\n\n* For placement prediction:\n\n> Both models are not well enough to alarm anybody.\n\n* For days prediction:\n\n> Dense model is pretty better than LSTM model.\n\n* For magnitude prediction:\n\n> LSTM model seems better, but it can not be well as desired.\n\nIn total, Dense model is better for days prediction and LSTM model is better for others.\n\nFormer predictions and this study show that there must be more parameters which are effecting earthquake happenigs. Any seemingly related feautures can be added to models and run from the very beginning.","f82dc60b":"## a. Functions","124549ee":"## a. Aim of The Study","fb0b6bb8":"## c. Functions","4ae38caf":"# 4. EVALUATION","027af6c0":"Most powerful correlations have approximately 0.5 points. As it is seen, dataset has weak correations. It is going to be hard to predict true values.","a426433d":"There are  171 scatter plots. Most of the plots do not give much information. Correlations are shown below.","f6a67f72":"Some parts of earthquake dataset are time-series (due to aftershock quakes and fault line breakage) and some parts are not related to others. It is hard to determine the model to choose, whether dense network or LSTM. So, it is better to try both.","87eb64eb":"## d. Imports","0cb5cecd":"With the functions written above, many different networks can be searched to find the best model and hyperparameters. Here are some of them:","1004ec61":"#### Values differentiating between 0 and 255 are replaced a certain value according to threshold values. Threshold values are found experimentally.","20525e74":"#### Days Prediction","34fb8142":"Graphs are shown with different axis scales. Because, LSTM models perform pretty well according to Dense models. \n\nOn each bar;\n\n* Lower lines show model minimum, \n* Upper lines show model mean,\n* Bar tops show model maximum.","06018566":"Belove are LSTM models searched. Surely, tens of models are also tested before that to find the best network structure.","8cfd94f8":"## d. Training","66f0a410":"#### This function builds the model which is set before.","d5af2f66":"#### Magnitude Prediction","5d364ddb":"# 2. FEATURE ENGINEERING","25cc63dd":"#### Dense layer partition and scaling\n\n4 of the features are going to be predicted and remaining 6 of them are going to be trained.","37279fe3":"Date-time column represents an incompatible data for analysis. This should be seperated into date and time. Besides, these two can also be incompatible due to different types of values. It is better to convert them into a single value.\n\nColumns containing string values can be omitted.","07c101e9":"Features to use are as below:","5cc5de78":"## b. Plotting","6d04575d":"#### LSTM network","e2192ff5":"## e. Data Load and A Glance","c0de6956":"#### LSTM layer partition and scaling\n\nAll of the features are going to be trained and 4 of them are going to be predicted. Target features are 1-shifted.","e6c8f513":"Dense prediction is approximately in line and LSTM is scattered in a wide range. Dense model is weak at this feature.","526b3b99":"## a. Features to Be Used","9a29ed94":"It is too hard to predict earthquakes. There are many parameters to be included in. Huge data from enormous quantity of sensors must be processed.\n\nReal life earthquake prediction efforts focus on predicting the next few seconds to warn people. Many instant-measured parameters are used. On any changes of the state, it is tried to predict whether there will be an earthquake or not.\n\nUp to September 2019, accuracy score of that prediction was 58%. Since then, it is 84%. All efforts focus on very near future such as 6-9 seconds to be able to alarm  people just before.","616fc020":"## b. Data Partition","d6d08869":"Along with this study, we are going to try to predict eartquakes, make our predictions as closer as possible to real values and find out whether there are any relations with former earthquakes and follows any patterns.\n\nThe prediction method is **one-step-forecast** which means that it only predicts the next earthquake. The reason fro using this method is that there are multiple values as mentioned belove to predict already. For multi-step forecast, many more sequences are predicted. When sequence quantity increases, the accuracy will begin to reduce. Also, it is it unnecessary to predict that much. Predicting the next is enough.\n\nWhen it comes to earthquake prediction, we have 3 measurements to predict:\n\n1. Place where it happens,\n\n2. Time when it happens,\n\n3. The magnitude of it.\n\nPlace component comprises from latitude and longitude values. So, there are 4 values to predict.","590c61b5":"# EARTHQUAKE PREDICTION WITH DEEP LEARNING","6514a555":"1. The dataset and supplementary materials are taken from AFAD website (https:\/\/deprem.afad.gov.tr\/depremkatalogu?lang=en).\n\n2. After downloading and importing dataset, feature engineering comes next.\n>* At this step, date-time data converted to days and hour-day rate.\n>* A risk map is processed to be able to use in the model.\n>* Many scatter plots are drawn to see feature relations.\n>* Heatmap is plotted to see correlations.\n\n3. A graph search algoritm is written to try hundreds of variations to find the best model and hyperparameters.\n\n  There are some built-in algoritms to achieve that. But, they do not meet the need. Some causes are:\n  >* Google Colab does not allow to use it more than 12 hours. Also, if there are no transactions for a while, runtime disconnects.\n  >* When there is an interruption due to any problems, it is mandatory to begin from the very beginning. The time spent becomes rubbish.\n\n  With this algoritm, values can be saved in time, the process can be interrupted and can be resumed.\n\n4. Models are compared and tested. Best models are visualized.","6c34eba5":"## c. Process Steps","888a0f90":"At this part, predictions are made with both bests of Dense and LSTM.","a6105c6a":"## b. The Way In Real Life","57dd96b0":"# 3. MODELLING AND PREDICTION","51629633":"### 1. INTRODUCTION\n>#### a. Aim of The Study\n>#### b. The Way In Real Life\n>#### c. Process Steps\n>#### d. Imports\n>#### e. Data Load and A Glance\n### 2. FEATURE ENGINEERING\n>#### a. Functions\n>#### b. Feature Transformation\n>#### c. Use of Earthquake Risk Map\n### 3. MODELLING AND PREDICTION\n>#### a. Features to Be Used\n>#### b. Data Partition\n>#### c. Functions\n>#### d. Training\n### 4. EVALUATION\n>#### a. Variables\n>#### b. Plotting\n>#### c. Test Results\n>#### d. Afterwords","ff88e803":"#### Definings","be368d68":"## a. Variables","bfed6ba7":"#### Dense Neural Network","2489a99d":"Result changings according to hyperparameter.","25e651cd":"## c. Test Results","26173190":"Best results and hyperparameters.","d0944913":"For being able to use risk map, these steps can be followed:\n\n1. Cropping unnecessary parts of the map,\n2. Reducing noises such as city names and borders. At this step, an extending-sized frame passed over the map several times equalizing all values to frame minimum\/maximum.\n3. Finding threshold values for risk grades and replacing all values with a grade number on the map. Threshold values are found experimentally.\n\nMap results of these steps seem like above.","07fa0cad":"Both models are far away from predicting the truth. But if one must be chosen, it is LSTM.","11fbdab7":"## d. Conclusion","06106228":"## c. Use of Earthquake Risk Map","cfef2868":"#### This function reduces loss quantity. To do that, it calculates mean at every step.","58bdff05":"Last state of the dataset is above. There are 10 features to use. Occasionally, features need extra transformation such as taking logaritm or square root. This can make features better fit for the target. So, logaritms of all features are taken to see relations between pairs.","181129b0":"1. \"calculate_days\" function calculates the days passed between 2 dates. It is used for two features. First, it calculates the days between sequential earthquakes. Second, it calculates the days passed from the date-time 1900-01-01 00:00:000.\n\n2. \"hour_rate\" function is used for calculating the hour rate in the day. {e.g. 13:24:000 is calculated as (13 * 60 + 24) \/ (24 * 60)}\n\n3. It is given latitude and longitude values to \"risk_grader\" function and it finds the risk grade from the processed ready-to-use risk map.","83ebe091":"#### Coordinate Predictions","03eeeefb":"#### All parameters are scatter-plotted whether there are any relationships.","a0127e72":"While looking through AFAD website for more information, there seemed a this risk map. It can add some useful information to the model. When risk map is compared with real earthquake happenings, the relationship can be observed.\n\nNoises such as city names and borders are reduced. Recovering process uses some convolution-like filters with increasing sizes and filter-sized strides. On each epoch, it replaces all pixels to filter max or filter min.\n\nLakes and the Marmara Sea are still risky areas. But due to there are no residuals on them, risk grade is not shown. So, lakes and the sea are painted with neighboring colors manually."}}