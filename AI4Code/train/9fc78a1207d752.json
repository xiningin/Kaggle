{"cell_type":{"b010f2b0":"code","7b47bdca":"code","b91a66e6":"code","76f13dc5":"code","06f91d48":"code","b601fd82":"code","83d3d796":"code","d7e3fde4":"code","d2528b86":"code","02d6d6cb":"code","ad678795":"code","d465413e":"code","4a863e30":"code","9a7e9ef2":"code","95550e41":"code","2bd56961":"code","7002aef4":"code","5e17866d":"code","a68b6e41":"code","2725c332":"code","b74ef0e4":"code","21008ce0":"code","f338d865":"code","c4ac5725":"code","962f8891":"code","76a4299a":"code","e5abd57d":"code","df19cf4c":"code","5919426a":"code","c09981d0":"code","f6c564e5":"markdown","c74b40ff":"markdown","f388b5fa":"markdown","b13645df":"markdown","07c3f36c":"markdown","c040e542":"markdown","a111267a":"markdown","a36f3f66":"markdown","1bea891b":"markdown","4cedf65d":"markdown","87fa9a3d":"markdown","efb0ffe1":"markdown","adc0b9dd":"markdown","5d62fbef":"markdown","64f6f2bc":"markdown","8bc3d5f0":"markdown","6548a05a":"markdown","7d8d5725":"markdown","8998e5cc":"markdown","923328ba":"markdown","08b04778":"markdown"},"source":{"b010f2b0":"! pip install tensorflow==2.2.0 -q","7b47bdca":"import os\nimport PIL\nimport math\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\n\nSEED = 1337\nprint('Tensorflow version : {}'.format(tf.__version__))\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    \nprint('Number of replicas:', strategy.num_replicas_in_sync)","b91a66e6":"MAIN_DIR = '..\/input\/pandatilesagg'\nTRAIN_IMG_DIR = os.path.join(MAIN_DIR, 'all_images')\ntrain_csv = pd.read_csv(os.path.join(MAIN_DIR, 'train.csv'))","76f13dc5":"radboud_csv = train_csv[train_csv['data_provider'] == 'radboud']\nkarolinska_csv = train_csv[train_csv['data_provider'] != 'radboud']\nimg_ids = train_csv['image_id']","06f91d48":"r_train, r_test = train_test_split(\n    radboud_csv,\n    test_size=0.2, random_state=SEED\n)\n\nk_train, k_test = train_test_split(\n    karolinska_csv,\n    test_size=0.2, random_state=SEED\n)","b601fd82":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nIMG_DIM = (1536, 128)\nCLASSES_NUM = 6\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nEPOCHS = 100\nN=12","83d3d796":"def decode_img(img):\n  # convert the compressed string to a 3D uint8 tensor\n  img = tf.image.decode_jpeg(img, channels=3)\n  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n  img = tf.image.convert_image_dtype(img, tf.float32)\n  # resize the image to the desired size.\n  return tf.image.resize(img, IMG_DIM)","d7e3fde4":"def get_item(file_path):    \n    image = tf.io.read_file(file_path)\n    image = decode_img(image)\n    label = tf.strings.split(file_path, '_')\n    label = tf.strings.to_number(label[-2])\n    label = tf.cast(label, tf.int32)\n    \n    return image, tf.one_hot(label, CLASSES_NUM)","d2528b86":"r_train['isup_grade'] = r_train['isup_grade'].apply(str)\nr_train['file'] = TRAIN_IMG_DIR +'\/_' + r_train['isup_grade'] + '_' + r_train['image_id'] + '.jpg'\n\nr_test['isup_grade'] = r_test['isup_grade'].apply(str)\nr_test['file'] = TRAIN_IMG_DIR +'\/_' + r_test['isup_grade'] + '_' + r_test['image_id'] + '.jpg'\n\nk_train['isup_grade'] = k_train['isup_grade'].apply(str)\nk_train['file'] = TRAIN_IMG_DIR +'\/_' + k_train['isup_grade'] + '_' + k_train['image_id'] + '.jpg'\n\nk_test['isup_grade'] = k_test['isup_grade'].apply(str)\nk_test['file'] = TRAIN_IMG_DIR +'\/_' + k_test['isup_grade'] + '_' + k_test['image_id'] + '.jpg'","02d6d6cb":"def get_dataset(df):\n    ds = tf.data.Dataset.from_tensor_slices(df['file'].values)\n    ds = ds.map(get_item, num_parallel_calls=AUTOTUNE)\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds","ad678795":"r_train_ds = get_dataset(r_train)\nk_train_ds = get_dataset(k_train)","d465413e":"def show_batch(image_batch, label_batch):\n    plt.figure(figsize=(10,10))\n    for n in range(10):\n        ax = plt.subplot(1,10,n+1)\n        plt.imshow(image_batch[n])\n        plt.title(np.amax(label_batch[n].numpy()))\n        plt.axis(\"off\")","4a863e30":"image_batch, _ = next(iter(r_train_ds))\nr_image = image_batch[4].numpy()\nplt.figure(figsize=(10,10))\nax = plt.subplot(1,2,1)\nplt.title(\"radboud\")\nplt.imshow(r_image)\nplt.axis(\"off\")\n\nimage_batch, _ = next(iter(k_train_ds))\nk_image = image_batch[4].numpy()\nax = plt.subplot(1,2,2)\nplt.imshow(k_image)\nplt.title(\"karolinska\")\nplt.axis(\"off\")","9a7e9ef2":"content_image = tf.expand_dims(r_image, axis = 0)\nstyle_image = tf.expand_dims(k_image, axis = 0)","95550e41":"content_layers = ['block5_conv2'] \n\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1']\n\nnum_content_layers = len(content_layers)\nnum_style_layers = len(style_layers)","2bd56961":"def vgg_layers(layer_names):\n  \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n  # Load our model. Load pretrained VGG, trained on imagenet data\n  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n  vgg.trainable = False\n  \n  outputs = [vgg.get_layer(name).output for name in layer_names]\n\n  model = tf.keras.Model([vgg.input], outputs)\n  return model","7002aef4":"style_extractor = vgg_layers(style_layers)\nstyle_outputs = style_extractor(style_image*255)","5e17866d":"def gram_matrix(input_tensor):\n  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n  input_shape = tf.shape(input_tensor)\n  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n  return result\/(num_locations)","a68b6e41":"class StyleContentModel(tf.keras.models.Model):\n  def __init__(self, style_layers, content_layers):\n    super(StyleContentModel, self).__init__()\n    self.vgg =  vgg_layers(style_layers + content_layers)\n    self.style_layers = style_layers\n    self.content_layers = content_layers\n    self.num_style_layers = len(style_layers)\n    self.vgg.trainable = False\n\n  def call(self, inputs):\n    \"Expects float input in [0,1]\"\n    inputs = inputs*255.0\n    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n    outputs = self.vgg(preprocessed_input)\n    style_outputs, content_outputs = (outputs[:self.num_style_layers], \n                                      outputs[self.num_style_layers:])\n\n    style_outputs = [gram_matrix(style_output)\n                     for style_output in style_outputs]\n\n    content_dict = {content_name:value \n                    for content_name, value \n                    in zip(self.content_layers, content_outputs)}\n\n    style_dict = {style_name:value\n                  for style_name, value\n                  in zip(self.style_layers, style_outputs)}\n    \n    return {'content':content_dict, 'style':style_dict}","2725c332":"extractor = StyleContentModel(style_layers, content_layers)\n\nresults = extractor(tf.constant(content_image))","b74ef0e4":"style_targets = extractor(style_image)['style']\ncontent_targets = extractor(content_image)['content']","21008ce0":"def clip_0_1(image):\n  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n\nopt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)","f338d865":"style_weight=1e-1\ncontent_weight=1e4","c4ac5725":"def style_content_loss(outputs):\n    style_outputs = outputs['style']\n    content_outputs = outputs['content']\n    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n                           for name in style_outputs.keys()])\n    style_loss *= style_weight \/ num_style_layers\n\n    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n                             for name in content_outputs.keys()])\n    content_loss *= content_weight \/ num_content_layers\n    loss = style_loss + content_loss\n    return loss","962f8891":"#@tf.function()\ndef train_step(image):\n  with tf.GradientTape() as tape:\n    outputs = extractor(image)\n    loss = style_content_loss(outputs)\n\n  grad = tape.gradient(loss, image)\n  opt.apply_gradients([(grad, image)])\n  image.assign(clip_0_1(image))","76a4299a":"steps_per_epoch = 3\n\ndef style_transfer(image):\n    image = tf.expand_dims(image, axis = 0)\n    image = tf.Variable(lambda : image)\n    step = 0\n    for m in range(steps_per_epoch):\n        step += 1\n        train_step(image)\n        \n    print(\".\", end='')\n    return image","e5abd57d":"plt.figure(figsize=(10,10))\nax = plt.subplot(1,2,1)\nplt.title(\"radboud\")\nplt.imshow(r_image)\nplt.axis(\"off\")\n\nr_aug_image = style_transfer(r_image)\nax = plt.subplot(1,2,2)\nplt.imshow(r_aug_image[0])\nplt.title(\"radboud augmented\")\nplt.axis(\"off\")","df19cf4c":"! mkdir images","5919426a":"for file in r_train['file'].values:\n    img = np.array(PIL.Image.open(file)) \/255.0\n    img = style_transfer(img)[0] * 255.0\n    img = img.numpy()\n    im = img.astype('uint8')\n    im = PIL.Image.fromarray(im)\n    im.save(\"images\/\" + file.split('\/')[-1])","c09981d0":"import shutil\nshutil.make_archive(\"images\", 'zip', \"\/kaggle\/working\/images\")","f6c564e5":"The way that loss is calculated is by calculating mean square error of the model outputs to the targets, defined below. The weight of the losses (style vs. content) are defined below.","c74b40ff":"# Build the neural style transfer model\n\nOur next step is to define a model that returns a list of intermediate layer outputs for our VGG19 model.","f388b5fa":"The first step is to create a function that will decode our image into a tensor. We can use the `tf.image` API.","b13645df":"# Data loading\n\nThe first step is to load in our data. We will be working with the PANDA data that was used for the Prostate Cancer Grade Assessment Challenge. The tiling for this dataset is explained in my first [PANDA notebook](https:\/\/www.kaggle.com\/amyjang\/tensorflow-cnn-data-augmentation-prostate-cancer). The tiles have been aggregated into singular images for your convenience in a new [dataset](https:\/\/www.kaggle.com\/amyjang\/pandatilesagg).\n\nThe PANDA data contains prostate cancer microscopy scans from two different institutions - Radboud University Medical Center and Karolinska Institute. We'll run our ML classifying model on the two sets of images separately to see which institution has better performing data.","07c3f36c":"# Define style and content layers\n\nIn a deep convolution neural net, the lower layers capture lower-level features like texture and higher layers capture higher-level features such as shapes. We define the content layers and style layers below.","c040e542":"The StyleContentModel will return the gram matrix of the style layers and content layers.","a111267a":"# Neural style transfer intro\n\nLet's first visualize a radboud image and a karolinska image next to each other.\n\nA more in-depth tutorial of neural style transfer for artistic painting transformations can be found [here](https:\/\/www.tensorflow.org\/tutorials\/generative\/style_transfer).","a36f3f66":"# Introduction + Set-up\n\nTensorFlow is a powerful tool to develop any machine learning pipeline. This model explores the concept of using neural style transfer to augment medical images.\n\nMedical images are rarely ever processed in the same way. Different institutions, different machines, different technicians, and many other factors lead to variation within medical scans. These variations can impact how well an ML model can learn from given data. If images processed by institution #1 work better with the model than images processed by institution #2, we would want our institution #2 images to look more like institution #1 images.\n\nNeural style transfer is outlined in [\"A Neural Algorithm of Artistic Style\"](https:\/\/arxiv.org\/abs\/1508.06576) (Gatys et al.). On a very high level, neural style transfer is used to compose on image in the style of another using deep learning. This notebook shows that neural style transfer does indeed improve scores for images processed by lower-scoring methods.","1bea891b":"Build a model that extracts style and content.","4cedf65d":"# Run neural style transfer on Radboud images\n\nNow that we tested to see that neural style transfer works on a single radboud image, we want to run neural style on all the radboud images. Because of resource limitations, the code is commented out below. However, the output for the data augmentation is under the radboud_aug directory in the dataset.\n\nAs a note, not all the radboud images have been augmented for efficiency purposes.","87fa9a3d":"Define the weights. Higher the weight, the more important the loss will be calculated.","efb0ffe1":"The differences in the processing techniques is apparent.\n\nWe will be using our karolinska image as our style image because we want to change our radboud images to look more like our karolinska images. Our model takes in 4D tensors, so let's first reformat our style and content images. Before we restyle all of our radboud images, let's first visualize how the neural style transfer works on a single image.","adc0b9dd":"We can extract our style features, defined by the style layers we chose earlier, and convert them to style outputs. Since our inputs are values between 0 and 255, we have to multiple the style features by 255.","5d62fbef":"We'll define a clipping method to keep the values between 0 and 1.","64f6f2bc":"This second function maps the filepath to the the image and label. The files are labeled so that the label - the ISUP grade - is the first part of the file name and the image id is the second part.","8bc3d5f0":"# Train the neural style model\n\nBecause we aren't using a typical Keras model, we cannot run model.fit. Instead, we'll define our own training loop.","6548a05a":"# Calculate style\n\nStyle is a rather arbitrary concept, and we want to convert it into a value that can be understood by the model. Style can be calculated by a gram matrix below.","7d8d5725":"# Visualize the neural style transfer\n\nBefore we augment all of the images in our dataset, let's fist visualize how our augmented radboud image compared to our original one.","8998e5cc":"Generally, it is better practice to specify constant variables than it is to hard-code numbers. This way, changing parameters is more efficient and complete. Specfiy some constants below.","923328ba":"Define the training step.","08b04778":"We see that there a style transfer did in fact occur. Running the neural style transfer may change the original image too much, and changing the higher-level features may hurt classifiction. Therefore, we want to keep the number of epochs relatively low."}}