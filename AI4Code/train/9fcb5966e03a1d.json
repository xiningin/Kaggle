{"cell_type":{"a10908be":"code","df6134d8":"code","8dd29f72":"code","d3b7a4c3":"code","ce563657":"code","a4f783b3":"code","ee02a73d":"code","8a741600":"code","f122f377":"code","a4f3341e":"code","c8b1222e":"code","f7677f65":"code","213cae9c":"code","bccb5015":"code","c837b7d7":"code","d688f023":"code","8c4e9a75":"code","d09db82b":"markdown","51d731d0":"markdown","c3420025":"markdown","7277ac0c":"markdown","0b2f1dca":"markdown","6e2e5dd4":"markdown"},"source":{"a10908be":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis","df6134d8":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nprint(train.shape)\nprint(test.shape)","8dd29f72":"test.drop(['Ticket','Cabin'],axis=1, inplace=True)\ntest.set_index('PassengerId',inplace=True)\n\ntrain.drop(['Ticket','Cabin'],axis=1, inplace=True)\ntrain.set_index('PassengerId',inplace=True)\n\ntrain.head()","d3b7a4c3":"test['title'] = test.Name.str.extract('([A-Za-z]+)\\.')\ntest['title'].replace(['Mlle','Mme','Ms','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don','Dona','Dr'],['Miss','Miss','Miss','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr','Miss','Mr'],inplace=True)\ntest = pd.concat([test.drop(['title','Name'],axis=1), pd.get_dummies(test[['title']], drop_first=True)], axis=1)\n\ntrain['title'] = train.Name.str.extract('([A-Za-z]+)\\.')\ntrain['title'].replace(['Mlle','Mme','Ms','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don','Dona','Dr'],['Miss','Miss','Miss','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr','Miss','Mr'],inplace=True)\ntrain = pd.concat([train.drop(['title','Name'],axis=1), pd.get_dummies(train[['title']], drop_first=True)], axis=1)\n\ntrain.head()","ce563657":"features_numeric = ['Age','SibSp','Parch','Fare']\nfeatures_label  = ['Pclass','Sex','Embarked']\n\ntest = test.fillna(test[features_numeric].mean())\ntest = test.fillna(test[features_label].mode().iloc[0])\n\ntrain = train.fillna(train[features_numeric].mean())\ntrain = train.fillna(train[features_label].mode().iloc[0])\n\ntrain.info()","a4f783b3":"test = pd.concat([test.drop(['Sex'],axis=1), pd.get_dummies(test[['Sex']], drop_first=True)], axis=1)\ntrain = pd.concat([train.drop(['Sex'],axis=1), pd.get_dummies(train[['Sex']], drop_first=True)], axis=1)\n\ntest = pd.concat([test.drop(['Embarked'],axis=1), pd.get_dummies(test[['Embarked']], drop_first=True)], axis=1)\ntrain = pd.concat([train.drop(['Embarked'],axis=1), pd.get_dummies(train[['Embarked']], drop_first=True)], axis=1)\n\ntest['Family'] = test['SibSp'] + test['Parch']\ntest['Alone'] = 0\ntest.loc[test.Family==0,'Alone'] = 1\ntest.drop(['SibSp','Parch'],axis=1,inplace=True)\n\ntrain['Family'] = train['SibSp'] + train['Parch']\ntrain['Alone'] = 0\ntrain.loc[train.Family==0,'Alone'] = 1\ntrain.drop(['SibSp','Parch'],axis=1,inplace=True)\n\ntrain.head()","ee02a73d":"#test['Fare_scaled'] = 0\n#test['Age_scaled'] = 0\n#test[['Fare_scaled','Age_scaled']] = StandardScaler().fit_transform(test[['Fare','Age']])\n#test.drop(['Fare','Age'],axis=1,inplace=True)\n\n#train['Fare_scaled'] = 0\n#train['Age_scaled'] = 0\n#train[['Fare_scaled','Age_scaled']] = StandardScaler().fit_transform(train[['Fare','Age']])\n#train.drop(['Fare','Age'],axis=1,inplace=True)\n\n#train.head()","8a741600":"print(train.shape)\nprint(test.shape)","f122f377":"plt.figure(figsize=(8,6))\nsns.heatmap(train.corr(),cmap='summer_r');","a4f3341e":"X,y = train.drop(['Survived'],axis=1), train['Survived'] \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nprint(X.shape)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y.shape)\nprint(y_train.shape)\nprint(y_test.shape)","c8b1222e":"classifiers = [\n    KNeighborsClassifier(3), SVC(probability=True), DecisionTreeClassifier(), RandomForestClassifier(), AdaBoostClassifier(),\n    GradientBoostingClassifier(), GaussianNB(), LinearDiscriminantAnalysis(), QuadraticDiscriminantAnalysis(), LogisticRegression(),\n    XGBClassifier()\n]\n\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog = pd.DataFrame(columns=log_cols)\nacc_dict = {}\n\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    clf.fit(X_train, y_train)\n    train_predictions = clf.predict(X_test)\n    acc = accuracy_score(y_test, train_predictions)\n    if name in acc_dict:\n        acc_dict[name] += acc\n    else:\n        acc_dict[name] = acc\n\nfor clf in acc_dict:\n    acc_dict[clf] = acc_dict[clf]\n    log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n    log = log.append(log_entry)\n\n\nplt.figure(figsize=(10,10))\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\nplt.grid(True)\nsns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\");","f7677f65":"xgb = XGBClassifier()#max_depth = 5, n_estimators=1000, learning_rate=0.2, nthread=3, subsample=1.0, colsample_bytree=0.5, min_child_weight=3, reg_alpha=0.03)\nxgb.fit(X_train, y_train, early_stopping_rounds=50, eval_metric='auc', eval_set=[(X_train, y_train),(X_test, y_test)])","213cae9c":"survivors = xgb.predict(test)\nsubmission = pd.DataFrame({'PassengerId':test.index,'Survived':survivors})\nsubmission.head()","bccb5015":"#gbc = GradientBoostingClassifier(learning_rate=0.11).fit(X_train,y_train)\n#gbc.score(X_test,y_test)","c837b7d7":"#params = {\n#    \"learning_rate\": [0.01, 0.05, 0.1, 0.25, 0.5, 1],\n#    \"n_estimators\": [90,100,110],\n#    \"max_depth\": [3,4,5],\n#    \"min_samples_split\": np.linspace(0.1, 1.0, 10, endpoint=True),\n#    \"min_samples_leaf\": np.linspace(0.1, 0.5, 5, endpoint=True),\n#    \"max_features\": list(range(1,train.shape[1]))\n#}\n#gbc = GradientBoostingClassifier()\n#grid = GridSearchCV(gbc, cv=5, param_grid=params)\n#grid.fit(X,y)\n\n#print(grid.best_score_)\n#print(grid.best_params_)","d688f023":"#gbc = GradientBoostingClassifier(learning_rate=0.25,max_depth=5,max_features=5,min_samples_leaf=0.1,min_samples_split=0.1,n_estimators=90).fit(X,y)\n#survivors = gbc.predict(test)\n#submission = pd.DataFrame({'PassengerId':test.index,'Survived':survivors})\n#submission.head()","8c4e9a75":"submission.to_csv('Titanic_Survivors_Predictions.csv',index=False)","d09db82b":"## Predicting","51d731d0":"## Input","c3420025":"## Correlations","7277ac0c":"## Classifiers + Evaluating","0b2f1dca":"## Feature Engineering","6e2e5dd4":"## Submitions"}}