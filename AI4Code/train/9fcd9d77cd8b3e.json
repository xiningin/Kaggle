{"cell_type":{"f8f15afb":"code","f45ee0da":"code","af897a2c":"code","9a91c8cf":"code","b42511f4":"code","83f14fc8":"markdown","6ec13167":"markdown","a90999f1":"markdown","fa0c0707":"markdown","b6f29e2d":"markdown","e0af22ca":"markdown","943d3160":"markdown","17742989":"markdown","891b1a82":"markdown","abd7d714":"markdown"},"source":{"f8f15afb":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport imageio\n\nfrom scipy import ndimage\nfrom pathlib import Path\n\n# Get image\nim_id = '2bf5343f03'\nim_dir = Path('..\/input\/train\/')\nim_path = im_dir \/ 'images' \/ '{}.png'.format(im_id)\nimg = imageio.imread(im_path.as_posix())\n\n# Get mask\nim_dir = Path('..\/input\/train\/')\nim_path = im_dir \/ 'masks' \/ '{}.png'.format(im_id)\ntarget_mask = imageio.imread(im_path.as_posix())\n\n# Fake prediction mask\npred_mask = ndimage.rotate(target_mask, 10, mode='constant', reshape=False, order=0)\npred_mask = ndimage.binary_dilation(pred_mask, iterations=1)\n\n# Plot the objects\nfig, axes = plt.subplots(1,3, figsize=(16,9))\naxes[0].imshow(img)\naxes[1].imshow(target_mask,cmap='hot')\naxes[2].imshow(pred_mask, cmap='hot')\n\nlabels = ['Original', '\"GroundTruth\" Mask', '\"Predicted\" Mask']\nfor ind, ax in enumerate(axes):\n    ax.set_title(labels[ind], fontsize=18)\n    ax.axis('off')","f45ee0da":"A = target_mask\nB = pred_mask\nintersection = np.logical_and(A, B)\nunion = np.logical_or(A, B)\n\nfig, axes = plt.subplots(1,4, figsize=(16,9))\naxes[0].imshow(A, cmap='hot')\naxes[0].annotate('npixels = {}'.format(np.sum(A>0)), \n                 xy=(10, 10), color='white', fontsize=16)\naxes[1].imshow(B, cmap='hot')\naxes[1].annotate('npixels = {}'.format(np.sum(B>0)), \n                 xy=(10, 10), color='white', fontsize=16)\n\naxes[2].imshow(intersection, cmap='hot')\naxes[2].annotate('npixels = {}'.format(np.sum(intersection>0)), \n                 xy=(10, 10), color='white', fontsize=16)\n\naxes[3].imshow(union, cmap='hot')\naxes[3].annotate('npixels = {}'.format(np.sum(union>0)), \n                 xy=(10, 10), color='white', fontsize=16)\n\nlabels = ['GroundTruth', 'Predicted', 'Intersection', 'Union']\nfor ind, ax in enumerate(axes):\n    ax.set_title(labels[ind], fontsize=18)\n    ax.axis('off')","af897a2c":"def get_iou_vector(A, B, n):\n    intersection = np.logical_and(A, B)\n    union = np.logical_or(A, B)\n    iou = np.sum(intersection > 0) \/ np.sum(union > 0)\n    s = pd.Series(name=n)\n    for thresh in np.arange(0.5,1,0.05):\n        s[thresh] = iou > thresh\n    return s\n\nprint('Does this IoU hit at each threshold?')\nprint(get_iou_vector(A, B, 'GT-P'))","9a91c8cf":"ppt = get_iou_vector(A, B, 'precisions_per_thresholds')\nprint (\"Average precision value for image `{}` is {}\".format(im_id, ppt.mean()))","b42511f4":"# `A` is the target mask\nppt = get_iou_vector(A, A, 'GT-P')\nprint('Does this IoU hit at each threshold?')\nprint(ppt)\nprint (\"Average precision value for image `{}` is {}\".format(im_id, ppt.mean()))","83f14fc8":"### Multi-threshold precision for a single image\n> The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold:\n$$\nAvg. Precision = \\frac{1}{n_{thresh}}  \\sum_{t=1}^nprecision(t)\n$$\n\nHere, we simply take the average of the precision values at each threshold to get our mean precision for the image.","6ec13167":"## Explanation of Scoring Metric\nI found the explanation for the scoring metric on this competition a little confusing, and I wanted to create a guide for those who are just entering or haven't made it too far yet.\nPlease leave me a comment if you found a mistake, thanks.\n\n*This notebook is based on Stephen Bailey's [excellent explanation](https:\/\/www.kaggle.com\/stkbailey\/step-by-step-explanation-of-scoring-metric\/notebook).*\n\n------\n\nThe metric used for this competition is defined as **the mean average precision at different intersection over union (IoU) thresholds**.\n\nThese are the steps to calculate to correct score:\n\n1. For all of the images\/predictions\n    * Calculate the Intersection of Union metric (\"compare\" the original mask with your predicted one)\n    * Calculate whether your predicted mask fits at a range of IoU thresholds.\n    * At each threshold, \"calculate\" the precision of your predicted masks.\n    * Average the precision across thresholds.\n\n2. Across the dataset\n    * Calculate the mean of the average precision for each image.","a90999f1":"### Intersection Over Union (for a single Prediction-GroundTruth comparison)\n> The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:\n$$\nIoU(A,B)=\\frac{A\u2229B}{A\u222aB}\n$$\n\nThe intersection and the union of our GT and Pred masks are look like this:","fa0c0707":"### Picking test image\nI'm going to pick a sample image from the training dataset, load the masks, then create a \"mock predict\"","b6f29e2d":"So, for this mask the **IoU** metric is calculated as:\n$$\nIoU(A,B)=\\frac{A\u2229B}{A\u222aB} = \\frac{872}{1349} = 0.6464\n$$","e0af22ca":"### Mean average precision for the dataset\n>Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.\n\nTherefore, the leaderboard metric will simply be the mean of the precisions across all the images.","943d3160":"### Single-threshold precision for a single image\n\n> At each threshold value t, a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects\n\nI think this step causes the confusion.\nI think the evaluation description of this competition was simply copied from the desc of the [2018 Data Science Bowl](https:\/\/www.kaggle.com\/c\/data-science-bowl-2018#evaluation) (or something similar) competition, where you can predict more than one mask\/object per image.\n\nAccording to @William Cukierski's [Discussion thread](https:\/\/www.kaggle.com\/c\/tgs-salt-identification-challenge\/discussion\/61550):\n> Although this competition metric allows prediction of multiple segmented objects per image, note that we have encoded masks as one object here (see train.csv as an example). You will be best off to predict one mask per image.\n\nBecause of we have to predict only one mask per image, the computation below (from the evaluation desc) is a bit confusing:\n>$$\nPrecision(t) = \\frac{TP(t)}{TP(t)+FP(t)+FN(t)}\n$$\n\nThis only makes sense if we would have more than one predicted mask\/segment per image.\nIn our case I think this would be a better description:\n* GT mask is empty, your prediction non-empty:  **(FP) Precision(threshold, image) = 0**\n* GT mask non empty, your prediction empty: **(FN) Precision(threshold, image) = 0**\n* GT mask empty, your prediction empty: **(TN) Precision(threshold, image) = 1**\n* GT mask non-empty, your prediction non-empty: **(TP) Precision(threshold, image) = IoU(GT, pred) > threshold**\n  \n  \n*See @William Cukierski's comment [here](https:\/\/www.kaggle.com\/c\/tgs-salt-identification-challenge\/discussion\/61550#360922)*\n> If a ground truth is empty and you predict nothing, you get a perfect score for that image. If the ground truth is empty and you predict anything, you get a 0 for that image.\n\n","17742989":"Let's see what happens if our prediction is 100% accurate.","891b1a82":"### Thresholding the IoU value (for a single GroundTruth-Prediction comparison)\n> Next, we sweep over a range of IoU thresholds to get a vector for each mask comparison. The threshold values range from 0.5 to 0.95 with a step size of 0.05: (**0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95**).\nIn other words, at a threshold of 0.5, a predicted object is considered a \"hit\" if its intersection over union with a ground truth object is greater than 0.5.","abd7d714":"**Please vote if you found this helpful or leave a comment if you found mistakes. **\n\n**Thanks for reading.**"}}