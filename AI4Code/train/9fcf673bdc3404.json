{"cell_type":{"208c5901":"code","c0cf44a5":"code","ba128655":"code","2a30bfc1":"code","4c0f38c2":"code","bee99f62":"code","33355ef0":"code","d4ad9bc7":"code","ae6ecf09":"code","8ff91e74":"code","f6d06ad8":"code","421f9e4a":"code","01ab0eaf":"code","37b06b30":"code","8186f2ac":"markdown","edd78550":"markdown","4cca2310":"markdown","d8adf863":"markdown","f710b4eb":"markdown","5e0a044e":"markdown","a9d38e75":"markdown","fa0a7096":"markdown","3439a875":"markdown","5ef5b038":"markdown","f3e8fccf":"markdown","67fc5d8f":"markdown","923db3ed":"markdown","477296b1":"markdown","0e6a8e8d":"markdown","64e2a92d":"markdown","b670b4a3":"markdown","50e1fb5d":"markdown","011bb324":"markdown","6ee3ce88":"markdown","1c1e0dee":"markdown","4aa62c6d":"markdown","9f512dd5":"markdown","f900fb42":"markdown","cc04788a":"markdown"},"source":{"208c5901":"import matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nimport torchvision\nfrom torchvision import datasets\nimport pandas as pd\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport numpy as  np\nimport seaborn as sns","c0cf44a5":"transform = transforms.ToTensor()","ba128655":"train_data = datasets.FashionMNIST(root='data', train=True,\n                                   download=True, transform=transform)\ntest_data = datasets.FashionMNIST(root='data', train=False,\n                                  download=True, transform=transform)","2a30bfc1":"num_workers = 0 \nbatch_size = 20","4c0f38c2":"train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)","bee99f62":"dataiter = iter(train_loader)\nimages, labels = dataiter.next()\n","33355ef0":"print(f\"The number of images in each batch is equals: {len(images)}\")","d4ad9bc7":"images = images.numpy()\nimages[0].shape","ae6ecf09":"img = np.squeeze(images[0])\nimg.shape","8ff91e74":"fig = plt.figure(figsize = (5,5)) \nax = fig.add_subplot(111)\nax.imshow(img, cmap='gray')\n","f6d06ad8":"class Autoencoder(nn.Module):\n    def __init__(self, dim):\n        super(Autoencoder, self).__init__()\n        self.encoder1 = nn.Linear(dim, 128)\n        self.encoder2 = nn.Linear(128, 64)\n        self.encoder3 = nn.Linear(64, 32)\n        \n        self.decoder1 = nn.Linear(32, 64)\n        self.decoder2 = nn.Linear(64, 128)\n        self.decoder3 = nn.Linear(128, dim)\n        \n    def forward(self, x):\n        x = F.relu(self.encoder1(x))\n        x = F.relu(self.encoder2(x))\n        x = F.relu(self.encoder3(x))\n        x = F.relu(self.decoder1(x))\n        x = F.relu(self.decoder2(x))\n        x = torch.sigmoid(self.decoder3(x))\n        return x\n    \ndim = 28*28\nmodel = Autoencoder(dim)\nprint(model)","421f9e4a":"criterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)","01ab0eaf":"\nn_epochs = 2\n\nfor epoch in range(1, n_epochs+1):\n    train_loss = 0.0\n    \n  \n    for data in train_loader:\n        images, _ = data\n        images = images.view(images.size(0), -1)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, images)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()*images.size(0)\n            \n    train_loss = train_loss\/len(train_loader)\n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n        epoch, \n        train_loss\n        ))","37b06b30":"dataiter = iter(test_loader)\nimages, labels = dataiter.next()\n\nimages_flatten = images.view(images.size(0), -1)\noutput = model(images_flatten)\nimages = images.numpy()\n\noutput = output.view(batch_size, 1, 28, 28)\noutput = output.detach().numpy()\nfig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n\nfor images, row in zip([images, output], axes):\n    for img, ax in zip(images, row):\n        ax.imshow(np.squeeze(img), cmap='gray')\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)","8186f2ac":"### Here we'll plot the one of the figures to understand better our data","edd78550":"**Autoencoders** (AE) are a family of neural networks for which the output try to represent the input. The **encoder** is the part of the NN that brings the data to a smaller dimension. This small representation is called **latent space**. The **Decoder** part take this encoded part and try to coverts it back to its original shape.","4cca2310":"<center> <img src='https:\/\/hackernoon.com\/hn-images\/1*-5D-CBTusUnmsbA6VYdY3A.png' \/> <\/center>","d8adf863":"```Python\n    num_workers = 0\n```\nCreat workers so we can load the data in parallel","f710b4eb":"```Python\n    img = np.squeeze(images[0])\n    img.shape\n```\nThis code will remove the single dimensional entries. Take a look below","5e0a044e":"# Autoencoder step by step","a9d38e75":"### Image representation of a Autoencoder","fa0a7096":"# Plotting the ground-truth and Genereted numbers","3439a875":"### Now we're going to create the DataLoader to iterate over the data.\n","5ef5b038":"*This code is based on Udacity Autoencoder lesson.*","f3e8fccf":"# Importing the essencial libriries","67fc5d8f":"```Python\n    dataiter = iter(train_loader)\n    images, labels = dataiter.next()\n```\nThose two line of code will take the next batch. Our batch size is 20 so it will take the first 20 images.","923db3ed":"```Python\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n```\n> PyTorch supplies another utility function called the DataLoader which acts as a data feeder for a Dataset object\n\n\nMore about how to create a Dataset object here: [Link](https:\/\/towardsdatascience.com\/building-efficient-custom-datasets-in-pytorch-2563b946fd9f)","477296b1":"# Taking one image and plotting","0e6a8e8d":"```Python\n    batch_size = 20\n```\nThe number of training examples in one forward\/backward pass. The higher the batch size, the more memory space you'll need.","64e2a92d":"```Python\ndatasets.FashionMNIST(root='data', train=True,\n                           download=True, transform=transform)\n```\nThis is a piece of code that download a data set and apply some transformation.\nHere is all datasets you can load using Pytorch torchvision.datasets: https:\/\/pytorch.org\/docs\/stable\/torchvision\/datasets.html","b670b4a3":"# Creating a simple Autoencoder model","50e1fb5d":"### downloading the MNIST dataset and applying the transform","011bb324":"# Load the Fashion-MNIST dataset and transform into a Tensor","6ee3ce88":"# Running the model","1c1e0dee":"### Transform a  PIL image or a numpy.ndarray to a tensor\n```Python\n    transforms.ToTensor()\n```\nwill transform our Fashion - MNIST dataset into a tensor\n\n\nref: https:\/\/pytorch.org\/docs\/stable\/torchvision\/transforms.html","4aa62c6d":"```Python\n    images = images.numpy()\n    images[0].shape\n```\nIn those two line we'll convert the tensor into a numpy array and display the shape this way we can see its behavior.","9f512dd5":"# Creating Train and Test Loaders","f900fb42":"The first thing we want to do when working with a dataset is to visualize the data in a meaningful way. In our case the the image is has (28 * 28 * 1) dimentions and we cannot plot that this way. So we have to squeeze the single-dimensional entries to be allowed to plot.","cc04788a":"# Selecting a Criterion and Optimizer"}}