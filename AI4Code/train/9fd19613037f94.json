{"cell_type":{"75458a2f":"code","a0893034":"code","57ad40ea":"code","e5f68f46":"code","eeb80237":"code","510fadaa":"code","4fe1fd13":"code","07a1c485":"code","95e7a462":"code","fc0563d5":"code","6bd39ba1":"code","edbc425c":"code","93188590":"code","926ae16e":"code","5a1db926":"code","f6726550":"code","f76eb417":"code","dae3fd41":"code","b416fdbf":"code","a00a947c":"code","ae6f9713":"code","36283a92":"code","c03c5edd":"code","b640221c":"code","78399d2e":"code","7d3739b0":"markdown","9387da4a":"markdown"},"source":{"75458a2f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a0893034":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","57ad40ea":"train.info()","e5f68f46":"combined  = pd.concat((train.loc[:,'Id':'SaleCondition'], test.loc[:, 'Id':'SaleCondition']), ignore_index = True)","eeb80237":"object_column = combined.dtypes[combined.dtypes == object]\nnum_column = combined.dtypes[combined.dtypes != object]","510fadaa":"# Transforming numerical variables\npd.set_option('display.max_columns', 500)\ncombined[num_column.index].head()","4fe1fd13":"# Not much effect from this\nfrom sklearn.preprocessing import StandardScaler\nstd = StandardScaler()\ncombined[num_column.index] = std.fit_transform(combined[num_column.index])","07a1c485":"#log transforming\ndf = np.abs(combined[num_column.index].skew()) > 0.75\ncombined[df[df.values == True].index] = np.log1p(combined[df[df.values == True].index])","95e7a462":"# Lets handle missing values first\npd.set_option('display.max_rows', 500)\ncombined.isnull().sum()","fc0563d5":"combined = pd.get_dummies(combined)","6bd39ba1":"combined.info()","edbc425c":"combined  = combined.fillna(combined.median())","93188590":"#Setting a CV strategy\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score, KFold\nimport xgboost as xgb","926ae16e":"#Setting a performance metric\ndef rmse(y_pred, y_act):\n    score = np.sqrt(np.mean((np.log(y_pred)-np.log(y_act))**2))\n    return score","5a1db926":"X_train = combined[:train.shape[0]]\nX_test = combined[train.shape[0]:]\ny = train.SalePrice","f6726550":"score = []\nk_fold = KFold(n_splits = 5)\nfor train_indices, test_indices in k_fold.split(X_train):\n    model = Ridge(alpha = 10)\n    model.fit(X_train.loc[train_indices],np.log(y.loc[train_indices]))\n    y_pred = model.predict(X_train.loc[test_indices])\n    y_pred = np.exp(y_pred)\n    score.append(rmse(y_pred, y.loc[test_indices]))","f76eb417":"sum(score)\/len(score)","dae3fd41":"score = []\nk_fold = KFold(n_splits = 5)\nfor train_indices, test_indices in k_fold.split(X_train):\n    model = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1)\n    model.fit(X_train.loc[train_indices],np.log(y.loc[train_indices]))\n    y_pred = model.predict(X_train.loc[test_indices])\n    y_pred = np.exp(y_pred)\n    score.append(rmse(y_pred, y.loc[test_indices]))","b416fdbf":"sum(score)\/len(score)","a00a947c":"sample_submission.SalePrice = np.exp(model.predict(X_test))\nsample_submission.to_csv('ridge1.csv',index=False)","ae6f9713":"#plotting residuals\nimport matplotlib.pyplot as plt\nmodel.fit(X_train, np.log(y))\ny_pred = model.predict(X_train)\nresiduals = y_pred - np.log(y)\nplt.scatter(x = y_pred, y = residuals)\nplt.show()","36283a92":"#Plotting feature importance\ncoef = pd.Series(model.feature_importances_, index = X_train.columns)\ncoef = coef.sort_values(ascending = False)","c03c5edd":"coef","b640221c":"score = np.sqrt(-cross_val_score(Ridge(alpha = 1), X_train, np.log(y), scoring=\"neg_mean_squared_error\", cv = 5))","78399d2e":"score.mean()","7d3739b0":"* train set dimension (1460, 81)\n* test set dimension (1459, 80)","9387da4a":"Now we have to run data preprocessing steps, this include \n* Handling missing data\n* Encoding or normalising variables\n\nBasically this is done so that the data fetched to the training algorithm is suitable for it.\nFor e.g. Learning models don't know what to do with NULL values and it might give different results based on how categorical variables are encoded and how numerical variables are represented.**"}}