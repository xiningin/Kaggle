{"cell_type":{"239d2b92":"code","69e53cda":"code","60c1d4e0":"code","f9863749":"code","0da819f9":"code","deeb399c":"code","636a765e":"code","3e4ae6a3":"code","ac727895":"code","b884a6dd":"code","4613f970":"code","255c9e4c":"code","952be878":"code","03db8261":"code","6b6568b4":"code","53b72114":"code","8e18d892":"code","d65b40f8":"code","f621c8ee":"code","79470cea":"code","46704e1e":"markdown","2dc6db86":"markdown","7a3680f3":"markdown","3558ada3":"markdown","994b3843":"markdown","9b5c241a":"markdown","8a01a4bf":"markdown"},"source":{"239d2b92":"!pip install segmentation-models-pytorch","69e53cda":"import os\nfrom copy import deepcopy\nfrom glob import glob\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Any, Optional\nfrom functools import partial\n\nimport numpy as np\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid, draw_segmentation_masks\nfrom sklearn.model_selection import train_test_split\n\n\n\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\n\nfrom tqdm.notebook import tqdm\n","60c1d4e0":"@dataclass\nclass Config:\n    train_image_path: str\n    train_mask_path: str\n    test_image_path: str\n    seed: int        \n\n    image_size: Tuple[int]\n    mask_clip_level: float\n        \n    model_name: str\n    encoder: str\n\n    epochs: int\n    batch_size: int\n    loss: str\n    mask_clip_lvl: float\n    lr: float\n    save_model_period: int\n    \n    \nconf = Config(\n    train_image_path='..\/input\/test-segm-comp\/train\/images',\n    train_mask_path='..\/input\/test-segm-comp\/train\/masks',\n    test_image_path='..\/input\/test-segm-comp\/test',\n    seed = 42,\n    \n    model_name = 'UNet',\n#     encoder = 'resnet18',\n    encoder = 'timm-mobilenetv3_large_075',\n#     encoder = 'timm-mobilenetv3_large_100',\n#     encoder = 'efficientnet-b2',\n\n    image_size = (512,512),\n    mask_clip_level=0.5,\n\n    epochs = 30,\n    batch_size = 8,\n    loss = 'IoULoss',\n    mask_clip_lvl = 0.5,\n    lr = 1e-3,\n    save_model_period = 10,\n    )\n\ndef set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.'''\n    # acknowledgment:\n    # https:\/\/www.kaggle.com\/rluethy\/efficientnet3d-with-one-mri-type\n    np.random.seed(seed)\n#     random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.cuda.is_available():\n#         torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\n        # When running on the CuDNN backend, two further options must be set\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed(conf.seed)\n\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nIMAGE_TO_SHOW = 8\nMODEL_ZOO_PATH = '..\/input\/flower-segmentation-data'","f9863749":"ensamble = [\n#     'UNet resnet18 PS ep_20 VL_-1.9 VM_0.00.model'\n#     'UNet timm-mobilenetv3_large_075 BL ep_29 VL_-0.892 VM_ 0.746.model',\n#     'UNet timm-mobilenetv3_large_100 BL ep_29 VL_-0.888 VM_ 0.729.model',\n#     'UNet timm-mobilenetv3_large_075 BL ep_70 VL_-0.898 VM_ 0.764.model',  \n#     'DeepLabV3 timm-mobilenetv3_large_075 BL ep_48 VL_-0.901 VM_ 0.773.model',\n#     'DeepLabV3 timm-mobilenetv3_large_075 BL ep_59 VL_-0.932 VM_0.833.model',\n    'DeepLabV3 timm-mobilenetv3_large_075 PS ep_90 VL_-0.932 VM_ 0.833.model',\n    ]\n","0da819f9":"test_df = pd.DataFrame()\ntest_df['image_path'] = glob(conf.test_image_path + '\/*.png')","deeb399c":"# We dont use pretrained segmentation models and pretrained encoders. \n# So we dont need use ImageNet mean and std. \n# Instead we compute means and std for our dataset .\ncolor_mean = np.array([0.274, 0.422, 0.430])\ncolor_std = np.array([0.057, 0.056, 0.065])","636a765e":"class FlowersDataset(Dataset):\n    def __init__(self, df_files:pd.DataFrame, is_train=False, transforms=None):\n        self.df_files = df_files\n        self.transforms = transforms\n        self.is_train = is_train\n    \n    def __len__(self):\n        return len(self.df_files.index)\n                   \n    def __getitem__(self, i):\n        item = self.df_files.iloc[i]\n        image = cv2.cvtColor(cv2.imread(item['image_path']), cv2.COLOR_BGR2RGB)\n        if self.is_train:\n            mask = cv2.cvtColor(cv2.imread(item['mask_path']), cv2.COLOR_BGR2GRAY)\n        image_name = item['image_path'].split('\/')[-1].strip('.png')\n        original_size = image.shape[:2] # remove chanels number from sizes\n        \n        if self.transforms:\n            if self.is_train:\n                transformed = self.transforms(image=image, mask=mask)\n                image = transformed['image']\n                mask = transformed['mask']\n                mask = (mask\/255).type(torch.int)\n            else:\n                image = self.transforms(image=image)['image']\n            \n        result = {'image': image, 'image_name': image_name, 'original_size': original_size}\n        if self.is_train:\n            result.update({'mask': mask})\n            \n        return result","3e4ae6a3":"resize_transforms = [albu.Resize(*conf.image_size),]\n\nconvert_transforms = [\n        albu.Normalize(mean=color_mean, std=color_std),\n        ToTensorV2(),\n    ]\n\n\n# train_transforms = albu.Compose(aug_transforms + resize_transforms + convert_transforms)\n# val_transforms = albu.Compose(resize_transforms + convert_transforms)\ntest_transforms = albu.Compose(resize_transforms + convert_transforms)                                \n\ndef inverse_transforms(image_tensor):\n    \n    inv_normalize = transforms.Normalize(\n            mean= [-m\/s for m, s in zip(color_mean, color_std)],\n            std= [1\/s for s in color_std]\n        )\n    \n    image_tensor = inv_normalize(image_tensor)\n    \n    image = np.transpose(image_tensor.numpy(), [1,2,0])\n    \n    return image\n    \n","ac727895":"# train_dataset = FlowersDataset(train_df, is_train=True, transforms=train_transforms)\n# val_dataset = FlowersDataset(val_df, is_train=True, transforms=val_transforms)\ntest_dataset = FlowersDataset(test_df, is_train=False, transforms=test_transforms)","b884a6dd":"# train_dataloader = DataLoader(train_dataset, batch_size=conf.batch_size, shuffle=True,\n#                               num_workers=os.cpu_count(), prefetch_factor=4, pin_memory=DEVICE.type=='cuda')\n# val_dataloader = DataLoader(val_dataset, batch_size=conf.batch_size, shuffle=True,\n#                               num_workers=os.cpu_count(), prefetch_factor=4, pin_memory=DEVICE.type=='cuda')\ntest_dataloader = DataLoader(test_dataset, batch_size=conf.batch_size, shuffle=False,\n                              num_workers=os.cpu_count(), prefetch_factor=4, pin_memory=DEVICE.type=='cuda')","4613f970":"def save_model(path, model_state, optimizer, scheduler, description:str, config:Optional[Config]=None):\n    torch.save({\n            'model_state_dict': model_state,\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'description': description,\n            'config': config,\n            }, path)\n    print('successfully saved')\n\ndef load_model(path, model, optimizer=None, scheduler=None):\n    checkpoint = torch.load(path, map_location=DEVICE)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    if scheduler is not None:\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    description = checkpoint['description']\n    try:\n        config = checkpoint['config']\n    except KeyError:\n        config = None\n        print('no config to load!')\n    print('successfully loaded')\n    return description, config\n\ndef load_model_config(path):\n    checkpoint = torch.load(path, map_location=DEVICE)\n    try:\n        config = checkpoint['config']\n    except KeyError:\n        config = None\n        print('no config to load!')\n    return config\n    ","255c9e4c":"# author: Ilya Ezepov;  https:\/\/www.kaggle.com\/iezepov\/fast-iou-scoring-metric-in-pytorch-and-numpy\/script\n\nSMOOTH = 1e-6\n\ndef tresholded_iou(outputs: torch.Tensor, labels: torch.Tensor):\n    # You can comment out this line if you are passing tensors of equal shape\n    # But if you are passing output from UNet or something it will most probably\n    # be with the BATCH x 1 x H x W shape\n    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n    \n    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n    \n    iou = (intersection + SMOOTH) \/ (union + SMOOTH)  # We smooth our devision to avoid 0\/0\n    \n    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() \/ 10  # This is equal to comparing with thresolds\n    \n    return thresholded  # or thresholded.mean() if you are interested in average across the batch\n","952be878":"def rle_encoding(x):\n# acknowledgment: https:\/\/github.com\/catalyst-team\/dl-course\/blob\/master\/homework-2\/segmentation\/segmentation_baseline.ipynb    \n    \"\"\"\n    x: numpy array of shape (height, width), 1 - mask, 0 - background\n    Returns run length as list\n    \"\"\"\n    dots = np.where(x.T.flatten() == 1)[\n        0\n    ]  # .T sets Fortran order down-then-right\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if b > prev + 1:\n            run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return \" \".join([str(i) for i in run_lengths])","03db8261":"def make_submition(model, dataloader):\n    torch.cuda.empty_cache()\n    model.eval()\n    \n    submission = {\"ImageId\": [], \"EncodedPixels\": []}\n    for batch in tqdm(dataloader):\n        \n        img_size = torch.stack(batch['original_size'], dim=1)\n        submission['ImageId'].extend(batch['image_name'])\n        \n        X_tens = batch['image'].to(DEVICE)        \n        prediction = model(X_tens)\n        prediction = torch.clip(prediction, min=0, max=1)\n        \n        # model return all mask same size\n        # code bellow resize each mask to original image size and treshold it after resizing\n        # m.unsqueeze(0) used for add batch dimension for F.interpolate() correct work\n        resized_masks = [(F.interpolate(m.unsqueeze(0), tuple(s)) > conf.mask_clip_lvl).type(torch.int) for m, s in zip(prediction, img_size)]\n        encoded_masks = [rle_encoding(m.cpu()) for m in resized_masks]\n        submission['EncodedPixels'].extend(encoded_masks)\n    return pd.DataFrame(submission)","6b6568b4":"import segmentation_models_pytorch as smp\n\nmodel_selector = {\n    'Unet': partial(smp.Unet, encoder_weights=None, in_channels=3, classes=1),\n    'Unet++': partial(smp.UnetPlusPlus, encoder_weights=None, in_channels=3, classes=1),\n    'MAnet': partial(smp.MAnet, encoder_weights=None, in_channels=3, classes=1),\n    'PSPNet': partial(smp.PSPNet, encoder_weights=None, in_channels=3, classes=1),\n    'PAN': partial(smp.PAN, encoder_weights=None, in_channels=3, classes=1),\n    'DeepLabV3': partial(smp.DeepLabV3, encoder_weights=None, in_channels=3, classes=1),\n}\n\n# model = model_selector[conf.model_name](encoder_name=conf.encoder).to(DEVICE)","53b72114":"def get_loaded_model(model_file_name:str):\n    model_path = MODEL_ZOO_PATH + '\/' + model_file_name\n    model_config = load_model_config(model_path)  # load config with which the model trained\n    assert model_config.model_name in model_selector, 'invalid model_name!'\n    \n    model = model_selector[model_config.model_name](encoder_name=model_config.encoder).to(DEVICE)  # use config to make correct model with correct encoder\n    load_model(model_path, model)  # load model state\n    return model","8e18d892":"def make_ensamble_submition(ensamble):\n    submissions = []\n    for model_file_name in ensamble:\n        print(model_file_name)\n        model = get_loaded_model(model_file_name)\n        submissions.append(make_submition(model, test_dataloader))\n    return submissions","d65b40f8":"submissions = make_ensamble_submition(ensamble)","f621c8ee":"df_submition = submissions[0]","79470cea":"df_submition.to_csv(\"submission.csv\", index=False)","46704e1e":"# Prediction","2dc6db86":"### Augmentations","7a3680f3":"### todo\n- [X] \u041f\u0440\u0435\u0434\u0438\u043a\u0448\u0435\u043d\n- [X] \u0421\u0430\u0431\u043c\u0438\u0448\u0435\u043d\n- [X] \u0420\u0435\u0441\u0430\u0439\u0437 \u043c\u0430\u0441\u043a\u0438 \u0434\u043e \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \n- [ ] \u0410\u043d\u0441\u0430\u043c\u0431\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435","3558ada3":"### Collect file paths into dataframe","994b3843":"### Configuration","9b5c241a":"## Dataset","8a01a4bf":"### Metric"}}