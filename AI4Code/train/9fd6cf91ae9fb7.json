{"cell_type":{"78822a10":"code","b03de25c":"code","e8a4c702":"code","bbf54c6a":"code","1ac64d91":"code","5fcf8b5a":"code","23ffff34":"code","a5a32a99":"code","94b2412e":"code","6585f009":"code","db87618e":"code","adc4682f":"code","f5b1b98d":"code","72f5ef5e":"code","8c6af7b8":"code","37490939":"code","fe96fc4b":"code","df2f8963":"code","5eae7024":"code","8942abbc":"code","4b25401e":"code","ca30b5e4":"code","9345dfc1":"code","c0d02a7a":"code","100dc521":"code","36d1c533":"code","16cdfda7":"code","0cd9e77a":"markdown","465f2069":"markdown","c18a9c50":"markdown","a713f55d":"markdown","b041d070":"markdown","510b3749":"markdown","c1ce2981":"markdown","fc1d0d30":"markdown","b3af32fc":"markdown","d0eb2132":"markdown","1706b428":"markdown","2d51d34b":"markdown","cfbc2d93":"markdown","5a18b92b":"markdown","4476f47b":"markdown","8573a265":"markdown","67383909":"markdown","3d51cfd9":"markdown","71dbdf56":"markdown","2a21e98e":"markdown","608ae49f":"markdown","b52d9462":"markdown","8f329e86":"markdown","9aa809cc":"markdown"},"source":{"78822a10":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, GRU\nfrom keras.layers import *\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam, SGD","b03de25c":"#Convert date\ndef to_datetime(df):\n    date = datetime.strptime(df, '%d.%m.%Y')\n    return date.strftime(\"%Y-%m-%d\")","e8a4c702":"df = pd.read_csv('..\/input\/tesla-stock-price-new\/Price Tesla.csv', sep=';')\ndf['Date'] = df['Date'].apply(lambda x: to_datetime(x))\ndf = df.sort_values('Date').reset_index(drop=True)","bbf54c6a":"df.head()","1ac64d91":"df.shape","5fcf8b5a":"df['Price'] = df['Price'].astype(float)\n\nplt.figure(figsize=(20,7))\nplt.plot(df['Date'].values, df['Price'].values, label = 'Tesla Stock Price', color = 'red')\nplt.xticks(np.arange(100,df.shape[0],200))\nplt.xlabel('Date')\nplt.ylabel('Price ($)')\nplt.legend()\nplt.show()","23ffff34":"num_shape = 1900\n\ntrain = df.iloc[:num_shape, 1:2].values\ntest = df.iloc[num_shape:, 1:2].values","a5a32a99":"sc = MinMaxScaler(feature_range = (0, 1))\ntrain_scaled = sc.fit_transform(train)","94b2412e":"X_train = []\n\n#Price on next day\ny_train = []\n\nwindow = 60\n\nfor i in range(window, num_shape):\n    X_train_ = np.reshape(train_scaled[i-window:i, 0], (window, 1))\n    X_train.append(X_train_)\n    y_train.append(train_scaled[i, 0])\nX_train = np.stack(X_train)\ny_train = np.stack(y_train)","6585f009":"# Initializing the Recurrent Neural Network\nmodel = Sequential()\n#Adding the first LSTM layer with a sigmoid activation function and some Dropout regularization\n#Units - dimensionality of the output space\n\nmodel.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50))\nmodel.add(Dropout(0.2))\n\n# Adding the output layer\nmodel.add(Dense(units = 1))\nmodel.summary()","db87618e":"model.compile(optimizer = 'adam', loss = 'mean_squared_error')\nmodel.fit(X_train, y_train, epochs = 1000, batch_size = 32);","adc4682f":"df_volume = np.vstack((train, test))\n\ninputs = df_volume[df_volume.shape[0] - test.shape[0] - window:]\ninputs = inputs.reshape(-1,1)\ninputs = sc.transform(inputs)\n\nnum_2 = df_volume.shape[0] - num_shape + window\n\nX_test = []\n\nfor i in range(window, num_2):\n    X_test_ = np.reshape(inputs[i-window:i, 0], (window, 1))\n    X_test.append(X_test_)\n    \nX_test = np.stack(X_test)","f5b1b98d":"predict = model.predict(X_test)\npredict = sc.inverse_transform(predict)","72f5ef5e":"diff = predict - test\n\nprint(\"MSE:\", np.mean(diff**2))\nprint(\"MAE:\", np.mean(abs(diff)))\nprint(\"RMSE:\", np.sqrt(np.mean(diff**2)))","8c6af7b8":"plt.figure(figsize=(20,7))\nplt.plot(df['Date'].values[1800:], df_volume[1800:], color = 'red', label = 'Real Tesla Stock Price')\nplt.plot(df['Date'][-predict.shape[0]:].values, predict, color = 'blue', label = 'Predicted Tesla Stock Price')\nplt.xticks(np.arange(100,df[1800:].shape[0],200))\nplt.title('Tesla Stock Price Prediction')\nplt.xlabel('Date')\nplt.ylabel('Price ($)')\nplt.legend()\nplt.show()","37490939":"pred_ = predict[-1].copy()\nprediction_full = []\nwindow = 60\ndf_copy = df.iloc[:, 1:2][1:].values\n\nfor j in range(20):\n    df_ = np.vstack((df_copy, pred_))\n    train_ = df_[:num_shape]\n    test_ = df_[num_shape:]\n    \n    df_volume_ = np.vstack((train_, test_))\n\n    inputs_ = df_volume_[df_volume_.shape[0] - test_.shape[0] - window:]\n    inputs_ = inputs_.reshape(-1,1)\n    inputs_ = sc.transform(inputs_)\n\n    X_test_2 = []\n\n    for k in range(window, num_2):\n        X_test_3 = np.reshape(inputs_[k-window:k, 0], (window, 1))\n        X_test_2.append(X_test_3)\n\n    X_test_ = np.stack(X_test_2)\n    predict_ = model.predict(X_test_)\n    pred_ = sc.inverse_transform(predict_)\n    prediction_full.append(pred_[-1][0])\n    df_copy = df_[j:]","fe96fc4b":"prediction_full_new = np.vstack((predict, np.array(prediction_full).reshape(-1,1)))","df2f8963":"df_date = df[['Date']]\n\nfor h in range(20):\n    df_date_add = pd.to_datetime(df_date['Date'].iloc[-1]) + pd.DateOffset(days=1)\n    df_date_add = pd.DataFrame([df_date_add.strftime(\"%Y-%m-%d\")], columns=['Date'])\n    df_date = df_date.append(df_date_add)\ndf_date = df_date.reset_index(drop=True)","5eae7024":"plt.figure(figsize=(20,7))\nplt.plot(df['Date'].values[1700:], df_volume[1700:], color = 'red', label = 'Real Tesla Stock Price')\nplt.plot(df_date['Date'][-prediction_full_new.shape[0]:].values, prediction_full_new, color = 'blue', label = 'Predicted Tesla Stock Price')\nplt.xticks(np.arange(100,df[1700:].shape[0],200))\nplt.title('Tesla Stock Price Prediction')\nplt.xlabel('Date')\nplt.ylabel('Price ($)')\nplt.legend()\nplt.show()","8942abbc":"# The GRU architecture\nmodelGRU = Sequential()\n\nmodelGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1)))\nmodelGRU.add(Dropout(0.2))\n\nmodelGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1)))\nmodelGRU.add(Dropout(0.2))\n\nmodelGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1)))\nmodelGRU.add(Dropout(0.2))\n\nmodelGRU.add(GRU(units=50))\nmodelGRU.add(Dropout(0.2))\n\nmodelGRU.add(Dense(units=1))\nmodelGRU.summary()","4b25401e":"modelGRU.compile(optimizer='sgd', loss='mean_squared_error')\nmodelGRU.fit(X_train, y_train, epochs=1000, batch_size=16)","ca30b5e4":"predict = modelGRU.predict(X_test)\npredict = sc.inverse_transform(predict)","9345dfc1":"diff = predict - test\n\nprint(\"MSE:\", np.mean(diff**2))\nprint(\"MAE:\", np.mean(abs(diff)))\nprint(\"RMSE:\", np.sqrt(np.mean(diff**2)))","c0d02a7a":"plt.figure(figsize=(20,7))\nplt.plot(df['Date'].values[1800:], df_volume[1800:], color = 'red', label = 'Real Tesla Stock Price')\nplt.plot(df['Date'][-predict.shape[0]:].values, predict, color = 'blue', label = 'Predicted Tesla Stock Price')\nplt.xticks(np.arange(100,df[1800:].shape[0],20))\nplt.title('Tesla Stock Price Prediction')\nplt.xlabel('Date')\nplt.ylabel('Price ($)')\nplt.legend()\nplt.show()","100dc521":"pred_ = predict[-1].copy()\nprediction_full = []\nwindow = 60\ndf_copy = df.iloc[:, 1:2][1:].values\n\nfor j in range(20):\n    df_ = np.vstack((df_copy, pred_))\n    train_ = df_[:num_shape]\n    test_ = df_[num_shape:]\n    \n    df_volume_ = np.vstack((train_, test_))\n\n    inputs_ = df_volume_[df_volume_.shape[0] - test_.shape[0] - window:]\n    inputs_ = inputs_.reshape(-1,1)\n    inputs_ = sc.transform(inputs_)\n\n    X_test_2 = []\n\n    for k in range(window, num_2):\n        X_test_3 = np.reshape(inputs_[k-window:k, 0], (window, 1))\n        X_test_2.append(X_test_3)\n\n    X_test_ = np.stack(X_test_2)\n    predict_ = modelGRU.predict(X_test_)\n    pred_ = sc.inverse_transform(predict_)\n    prediction_full.append(pred_[-1][0])\n    df_copy = df_[j:]","36d1c533":"prediction_full_new = np.vstack((predict, np.array(prediction_full).reshape(-1,1)))\n\ndf_date = df[['Date']]\n\nfor h in range(20):\n    kk = pd.to_datetime(df_date['Date'].iloc[-1]) + pd.DateOffset(days=1)\n    kk = pd.DataFrame([kk.strftime(\"%Y-%m-%d\")], columns=['Date'])\n    df_date = df_date.append(kk)\ndf_date = df_date.reset_index(drop=True)","16cdfda7":"plt.figure(figsize=(20,7))\nplt.plot(df['Date'].values[1700:], df_volume[1700:], color = 'red', label = 'Real Tesla Stock Price')\nplt.plot(df_date['Date'][-prediction_full_new.shape[0]:].values, prediction_full_new, color = 'blue', label = 'Predicted Tesla Stock Price')\nplt.xticks(np.arange(100,df_date[1700:].shape[0],20))\nplt.title('Tesla Stock Price Prediction')\nplt.xlabel('Date')\nplt.ylabel('Price ($)')\nplt.legend()\nplt.show()","0cd9e77a":"# GRU","465f2069":"### Training of the basic LSTM model","c18a9c50":"https:\/\/ru.investing.com\/equities\/tesla-motors-historical-data","a713f55d":"We'll use the LSTM for time series prediction","b041d070":"![](https:\/\/www.hdcarwallpapers.com\/walls\/tesla_roadster_4k-HD.jpg)","510b3749":"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/","c1ce2981":"# Regression","fc1d0d30":"Scaling our features using normalization. Normalizing data helps the algorithm in converging i.e. to find local\/ global minimum efficiently.","b3af32fc":"![](https:\/\/miro.medium.com\/max\/1400\/1*yBXV9o5q7L_CvY7quJt3WQ.png)","d0eb2132":"# Preparation","1706b428":"## LSTM Networks","2d51d34b":"# Introduction\n## Recurrent Neural Networks","cfbc2d93":"## Data Preprocessing","5a18b92b":"## Recurrent Neural Network","4476f47b":"## Load dataset","8573a265":"## Prediction","67383909":"### 20-day prediction","3d51cfd9":"Select the column \"Price\"","71dbdf56":"LSTM Networks\n\nLong Short Term Memory networks \u2013 usually just called \u201cLSTMs\u201d \u2013 are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used.\n\nLSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n\nAll recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.\nLSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.","2a21e98e":"### 20-day prediction","608ae49f":"## GRU","b52d9462":"The GRU is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM. GRU\u2019s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.\n\nThe update gate acts similar to the forget and input gate of an LSTM. It decides what information to throw away and what new information to add.\nThe reset gate is another gate is used to decide how much past information to forget.\n\nAnd that\u2019s a GRU. GRU\u2019s has fewer tensor operations; therefore, they are a little speedier to train then LSTM\u2019s. There isn\u2019t a clear winner which one is better. Researchers and engineers usually try both to determine which one works better for their use case.","8f329e86":"Humans don\u2019t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don\u2019t throw everything away and start thinking from scratch again. Your thoughts have persistence.\n\nTraditional neural networks can\u2019t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It\u2019s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.\n\nRecurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.","9aa809cc":"Now we take one row and cut it with a window of 60 elements"}}