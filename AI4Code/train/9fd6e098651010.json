{"cell_type":{"19762004":"code","8ced0ac5":"code","6ae5d764":"code","036b6f0e":"code","5dd85f70":"code","757ce554":"code","692b2ec3":"code","24bda3d8":"code","faaed85d":"code","3180a611":"code","ab967692":"code","84c09e42":"code","018beece":"code","63999de5":"code","b2e7c830":"code","b02be2d7":"code","251043ed":"code","271a17b8":"code","f9a45e56":"code","8e8d11ab":"code","8f2f419a":"code","14b8e768":"code","6e4e7201":"code","4fb24008":"code","5bb93b98":"code","96724fe0":"code","3b6cb4b0":"code","1a9dc08a":"code","522b5295":"code","42abdf48":"code","3b181a17":"code","f19e57e6":"code","259e51a9":"code","258e8f15":"code","67c10be0":"code","902724df":"code","4ff0b2cf":"code","ffcc62dd":"code","2cb721ed":"code","8817f821":"code","c22f8579":"code","894e41e8":"code","629f0a29":"code","c9a85d78":"code","6a35622e":"code","4fe4cd07":"code","6afec226":"code","0c0de0a3":"code","1a08ffe3":"code","47e2c0cb":"code","51cfbb5a":"code","e6f2129d":"code","1e1abeb4":"code","31ac2928":"code","4b1ff2c8":"code","80310c64":"code","1185db23":"code","b7705d78":"code","716d0f82":"code","aa36fec9":"code","9ac50697":"code","5d22fe04":"code","9d71d26e":"code","6ee34034":"code","7df310aa":"code","a7ee16e1":"code","fc5b9466":"code","ed9ea022":"code","09567815":"code","a4da90fe":"code","bcaa78fe":"code","8d59c680":"code","91770417":"code","dc6dd453":"code","7abc00b2":"code","96bf493e":"code","39350383":"code","21e8462a":"code","c01b0ddc":"code","ad62c29f":"code","455f621a":"code","b95a6181":"code","23ab8403":"code","2ef68913":"code","21957dad":"code","b3228614":"code","a7112d80":"code","276b062f":"code","1c0333da":"code","10cb5b46":"code","64186bda":"code","a1e72da8":"code","a93895e4":"code","869ace34":"code","4cd40f5a":"code","0262f0a8":"code","f0b54d21":"code","7b512189":"code","74dc325b":"code","418998fd":"code","83c5dbed":"code","226ea201":"code","5f693ae2":"markdown","28ff7ac2":"markdown","81bdf6fa":"markdown","ac1ddd32":"markdown","649fd48d":"markdown","8be5821e":"markdown","b9b7576f":"markdown","f061113d":"markdown","2537f058":"markdown","33da013c":"markdown","adae0c4c":"markdown","d11dd5a7":"markdown","5d0d837b":"markdown","cf6d1e61":"markdown","73a4e36b":"markdown","a8347577":"markdown","9f14acc5":"markdown","1e950db1":"markdown","14e03df7":"markdown","d6d52528":"markdown","54583c71":"markdown","c8d2af60":"markdown","f8601fcc":"markdown","cbe62697":"markdown","0dce54df":"markdown","39eb9038":"markdown","c7c52eea":"markdown","9b7cf714":"markdown","fae09f77":"markdown","49b8379b":"markdown","d7c8d0cd":"markdown","c84abdce":"markdown","959335e9":"markdown","0e11f27e":"markdown","7acd7ed9":"markdown","08bb4edb":"markdown","ad6c2f96":"markdown","abd15f98":"markdown"},"source":{"19762004":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#plt.style.use('fivethirtyeight')\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn import svm\nimport xgboost\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport shap\n\nimport warnings\nwarnings.filterwarnings('ignore')","8ced0ac5":"def perf_measure(y_actual, y_hat):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(y_hat)): \n        if y_actual[i]==y_hat[i]==1:\n           TP += 1\n        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n           FP += 1\n        if y_actual[i]==y_hat[i]==0:\n           TN += 1\n        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n           FN += 1\n\n    return(TP, FP, TN, FN)","6ae5d764":"!pip install xai\n!pip install sage-importance","036b6f0e":"#https:\/\/github.com\/EthicalML\/xai\nimport xai\n#https:\/\/github.com\/iancovert\/sage\nimport sage","5dd85f70":"df = pd.read_csv(\"..\/input\/heart-failure-prediction\/heart.csv\")","757ce554":"df.head()","692b2ec3":"df['ChestPainType'][df['ChestPainType'] == \"TA\"] = 0\ndf['ChestPainType'][df['ChestPainType'] == \"ATA\"] = 1\ndf['ChestPainType'][df['ChestPainType'] == \"NAP\"] = 2\ndf['ChestPainType'][df['ChestPainType'] == \"ASY\"] = 3\n\ndf['ExerciseAngina'][df['ExerciseAngina'] == \"N\"] = 0\ndf['ExerciseAngina'][df['ExerciseAngina'] == \"Y\"] = 1\n\ndf['RestingECG'][df['RestingECG'] == \"Normal\"] = 0\ndf['RestingECG'][df['RestingECG'] == \"LVH\"] = 1\ndf['RestingECG'][df['RestingECG'] == \"ST\"] = 2\n\ndf['ST_Slope'][df['ST_Slope'] == \"Down\"] = 0\ndf['ST_Slope'][df['ST_Slope'] == \"Flat\"] = 1\ndf['ST_Slope'][df['ST_Slope'] == \"Up\"] = 2\n\ndf['Sex'][df['Sex'] == \"F\"] = 0\ndf['Sex'][df['Sex'] == \"M\"] = 1","24bda3d8":"df = df.astype('float32')","faaed85d":"df.head()","3180a611":"np.unique(df[\"Sex\"]==1,return_counts=True)","ab967692":"figure, axes = plt.subplots(6, 2)\nfigure.set_figheight(70)\nfigure.set_figwidth(30)\n\na = sns.countplot(x=\"Sex\", data=df,ax=axes[0,0])\na.set_xticklabels([\"female\",\"male\"])\na.set_title(\"Gender\")\n\nb = sns.swarmplot(data=df, x=\"Sex\", y=\"Age\",ax=axes[0,1],color=\".2\")\nsns.violinplot(data=df, x=\"Sex\", y=\"Age\",ax=axes[0,1], inner=None)\nb.set_xticklabels([\"female\",\"male\"])\n\nc = sns.countplot(x=\"ChestPainType\", hue=\"Sex\", data=df,ax=axes[1,0])\nc.set_xticklabels([\"typical angina\", \"atypical angina\",\"non-anginal pain\",\"asymptomatic\"], rotation = 30)\nc.set_title(\"Experienced chest pain\")\n\nd= sns.swarmplot(data=df, x=\"Sex\", y=\"RestingBP\",ax=axes[1,1],color=\".2\")\nsns.violinplot(data=df, x=\"Sex\", y=\"RestingBP\",ax=axes[1,1], inner=None)\nd.set_xticklabels([\"female\",\"male\"])\nd.set_ylabel(\"Blood pressure at rest (mmHG)\")\n\ne=sns.swarmplot(data=df, x=\"Sex\", y=\"Cholesterol\",ax=axes[2,0],color=\".2\", edgecolor=\"gray\")\nsns.violinplot(data=df, x=\"Sex\", y=\"Cholesterol\",ax=axes[2,0], inner=None)\ne.set_xticklabels([\"female\",\"male\"])\ne.set_ylabel(\"cholesterol (mg\/dl)\")\n\nf = sns.countplot(x=\"FastingBS\", hue=\"Sex\", data=df,ax=axes[2,1])\nf.set_xticklabels([\"< 120\",\"> 120\"])\nf.set_title(\"fasting blood sugar (mg\/dl)\")\n\ng = sns.countplot(x=\"RestingECG\", hue=\"Sex\", data=df,ax=axes[3,0])\ng.set_title(\"ECG measurements (rest)\")\ng.set_xticklabels([\"normal\",\"ST-T wave abnormality\",\"ventricular hypertrophy\"], rotation = 10)\ng.set_xlabel(\" \")\n\nh = sns.swarmplot(data=df, x=\"Sex\", y=\"MaxHR\",ax=axes[3,1],color=\".2\", edgecolor=\"gray\")\nsns.violinplot(data=df, x=\"Sex\", y=\"MaxHR\",ax=axes[3,1], inner=None)\nh.set_xticklabels([\"female\",\"male\"])\nh.set_ylabel(\"maximum heart rate achieved (bpm)\")\n\ni = sns.countplot(x=\"ExerciseAngina\", hue=\"Sex\", data=df,ax=axes[4,0])\ni.set_title(\"Exercise induced angina\")\n#i.set_xticklabels([\"no\",\"yes\"])\n\nj = sns.swarmplot(data=df, x=\"Sex\", y=\"Oldpeak\",ax=axes[4,1],color=\".2\", edgecolor=\"gray\")\nj = sns.violinplot(data=df, x=\"Sex\", y=\"Oldpeak\",ax=axes[4,1], inner=None)\nj.set_ylabel(\"ST segment depression induced by exercise relative to rest\")\nj.set_xticklabels([\"female\",\"male\"])\n\nk = sns.countplot(x=\"ST_Slope\", hue=\"Sex\", data=df,ax=axes[5,0])\nk.set_title(\"slope of the peak exercise ST segment\")\n#k.set_xticklabels([\"upsloping\",\"flat\", \"downsloping\"])\n\nn = sns.countplot(x=\"HeartDisease\", hue=\"Sex\", data=df,ax=axes[5,1])\nn.set_title(\"Heart disease\")\nn.set_xticklabels([\"no\", \"yes\"])\n\nplt.show()","84c09e42":"_ = xai.correlations(df, include_categorical=True)","018beece":"_ = xai.correlations(df, include_categorical=True, plot_type=\"matrix\")","63999de5":"sns.pairplot(df)\nplt.show()","b2e7c830":"X = df.iloc[:,:11]\ny = df.iloc[:,11]","b02be2d7":"df.columns","251043ed":"x_dev, y_dev, x_test, y_test, train_idx, test_idx = \\\n    xai.balanced_train_test_split(\n            X, y, \"Sex\", \n            min_per_group=15,\n            max_per_group=15,\n            categorical_cols=['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol',\n       'FastingBS', 'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak',\n       'ST_Slope'])","271a17b8":"df_dev = x_dev.copy()","f9a45e56":"df_dev[\"HeartDisease\"]= y_dev","8e8d11ab":"df_exp = df_dev.copy() ","8f2f419a":"df_exp['Sex'][df_exp['Sex'] == 0] = 'female'\ndf_exp['Sex'][df_exp['Sex'] == 1] = 'male'\ndf_exp['HeartDisease'][df_exp['HeartDisease'] == 0] = 'no CVD'\ndf_exp['HeartDisease'][df_exp['HeartDisease'] == 1] = 'CVD'","14b8e768":"ims = xai.imbalance_plot(df_exp, \"Sex\")","6e4e7201":"ims = xai.imbalance_plot(df_exp, \"Sex\", \"HeartDisease\")","4fb24008":"bal_df = xai.balance(df_exp, \"Sex\", \"HeartDisease\", upsample=1.0)","5bb93b98":"figure, axes = plt.subplots(6, 2)\nfigure.set_figheight(70)\nfigure.set_figwidth(30)\n\na = sns.countplot(x=\"Sex\", data=bal_df,ax=axes[0,0])\na.set_xticklabels([\"male\",\"female\"])\na.set_title(\"Gender\")\n\nb = sns.swarmplot(data=bal_df, x=\"Sex\", y=\"Age\",ax=axes[0,1],color=\".2\")\nsns.violinplot(data=bal_df, x=\"Sex\", y=\"Age\",ax=axes[0,1], inner=None)\nb.set_xticklabels([\"male\",\"female\"])\n\nc = sns.countplot(x=\"ChestPainType\", hue=\"Sex\", data=bal_df,ax=axes[1,0])\nc.set_xticklabels([\"typical angina\", \"atypical angina\",\"non-anginal pain\",\"asymptomatic\"], rotation = 30)\nc.set_title(\"Experienced chest pain\")\n\nd= sns.swarmplot(data=bal_df, x=\"Sex\", y=\"RestingBP\",ax=axes[1,1],color=\".2\")\nsns.violinplot(data=bal_df, x=\"Sex\", y=\"RestingBP\",ax=axes[1,1], inner=None)\nd.set_xticklabels([\"male\",\"female\"])\nd.set_ylabel(\"Blood pressure at rest (mmHG)\")\n\ne=sns.swarmplot(data=bal_df, x=\"Sex\", y=\"Cholesterol\",ax=axes[2,0],color=\".2\", edgecolor=\"gray\")\nsns.violinplot(data=bal_df, x=\"Sex\", y=\"Cholesterol\",ax=axes[2,0], inner=None)\ne.set_xticklabels([\"male\",\"female\"])\ne.set_ylabel(\"cholesterol (mg\/dl)\")\n\nf = sns.countplot(x=\"FastingBS\", hue=\"Sex\", data=bal_df,ax=axes[2,1])\nf.set_xticklabels([\"< 120\",\"> 120\"])\nf.set_title(\"fasting blood sugar (mg\/dl)\")\n\ng = sns.countplot(x=\"RestingECG\", hue=\"Sex\", data=bal_df,ax=axes[3,0])\ng.set_title(\"ECG measurements (rest)\")\ng.set_xticklabels([\"normal\",\"ST-T wave abnormality\",\"ventricular hypertrophy\"], rotation = 10)\ng.set_xlabel(\" \")\n\nh = sns.swarmplot(data=bal_df, x=\"Sex\", y=\"MaxHR\",ax=axes[3,1],color=\".2\", edgecolor=\"gray\")\nsns.violinplot(data=bal_df, x=\"Sex\", y=\"MaxHR\",ax=axes[3,1], inner=None)\nh.set_xticklabels([\"male\",\"female\"])\nh.set_ylabel(\"maximum heart rate achieved (bpm)\")\n\ni = sns.countplot(x=\"ExerciseAngina\", hue=\"Sex\", data=bal_df,ax=axes[4,0])\ni.set_title(\"Exercise induced angina\")\n#i.set_xticklabels([\"no\",\"yes\"])\n\nj = sns.swarmplot(data=bal_df, x=\"Sex\", y=\"Oldpeak\",ax=axes[4,1],color=\".2\", edgecolor=\"gray\")\nj = sns.violinplot(data=bal_df, x=\"Sex\", y=\"Oldpeak\",ax=axes[4,1], inner=None)\nj.set_ylabel(\"ST segment depression induced by exercise relative to rest\")\nj.set_xticklabels([\"male\",\"female\"])\n\nk = sns.countplot(x=\"ST_Slope\", hue=\"Sex\", data=bal_df,ax=axes[5,0])\nk.set_title(\"slope of the peak exercise ST segment\")\n#k.set_xticklabels([\"upsloping\",\"flat\", \"downsloping\"])\n\nn = sns.countplot(x=\"HeartDisease\", hue=\"Sex\", data=bal_df,ax=axes[5,1])\nn.set_title(\"Heart disease\")\nn.set_xticklabels([\"no\", \"yes\"])\n\nplt.show()","96724fe0":"_ = xai.correlations(bal_df, include_categorical=True, plot_type=\"matrix\")","3b6cb4b0":"x_dev = x_dev.reset_index()\ny_dev = y_dev","1a9dc08a":"x_dev = x_dev.iloc[:,1:]","522b5295":"x_dev.head()","42abdf48":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(x_dev,y_dev):\n    model =GaussianNB()\n    X_train, X_val = x_dev.loc[train_index], x_dev.loc[val_index]\n    y_train, y_val = y_dev[train_index], y_dev[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") \n\n    ","3b181a17":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(x_dev,y_dev):\n    model =LogisticRegression(solver=\"liblinear\", random_state=0)\n    X_train, X_val = x_dev.loc[train_index], x_dev.loc[val_index]\n    y_train, y_val = y_dev[train_index], y_dev[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","f19e57e6":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(x_dev,y_dev):\n    model =svm.SVC()\n    X_train, X_val = x_dev.loc[train_index], x_dev.loc[val_index]\n    y_train, y_val = y_dev[train_index], y_dev[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","259e51a9":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(x_dev,y_dev):\n    model =xgboost.XGBClassifier(use_label_encoder=False, verbosity=0)\n    X_train, X_val = x_dev.loc[train_index], x_dev.loc[val_index]\n    y_train, y_val = y_dev[train_index], y_dev[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","258e8f15":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(x_dev,y_dev):\n    model = MLPClassifier(random_state=1, max_iter=500, learning_rate=\"adaptive\", solver=\"adam\", activation=\"relu\", tol=0.0001, momentum=0.99)\n    X_train, X_val = x_dev.loc[train_index], x_dev.loc[val_index]\n    y_train, y_val = y_dev[train_index], y_dev[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","67c10be0":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(x_dev,y_dev):\n    model = RandomForestClassifier()\n    X_train, X_val = x_dev.loc[train_index], x_dev.loc[val_index]\n    y_train, y_val = y_dev[train_index], y_dev[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","902724df":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(x_dev,y_dev):\n    model =xgboost.XGBClassifier(use_label_encoder=False, verbosity=0)\n    X_train, X_val = x_dev.loc[train_index], x_dev.loc[val_index]\n    y_train, y_val = y_dev[train_index], y_dev[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_val, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","4ff0b2cf":"model =xgboost.XGBClassifier(use_label_encoder=False, verbosity=0)\nmodel.fit(x_dev,y_dev)\ny_hat = model.predict(x_test)\nprint(f\"ROC = {roc_auc_score(y_test, y_hat)}\")\nprint(f\"Accuracy = {accuracy_score(y_test, y_hat)}\")\nprint(f\"F1-score(macro) = {f1_score(y_test, y_hat, average='macro')}\")\nprint(f\"F1-score(micro) = {f1_score(y_test, y_hat, average='micro')}\")\nprint(f\"Recall= {recall_score(y_test, y_hat)}\") \nprint(f\"Precision= {precision_score(y_test, y_hat)}\") ","ffcc62dd":"shap.initjs()","2cb721ed":"explainer = shap.KernelExplainer(model.predict_proba, x_dev, link=\"logit\")","8817f821":"shap_values = explainer.shap_values(x_test)","c22f8579":"shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], x_test.iloc[0,:], link=\"logit\")","894e41e8":"shap.force_plot(explainer.expected_value[0], shap_values[1][0,:], x_test.iloc[0,:], link=\"logit\")","629f0a29":"shap.force_plot(explainer.expected_value[0], shap_values[0], x_test, link=\"logit\")","c9a85d78":"shap.summary_plot(shap_values, x_test)","6a35622e":"explainer = shap.Explainer(model, x_dev)\nshap_values = explainer(x_test)\nshap.plots.beeswarm(shap_values,max_display=14)","4fe4cd07":"# Setup and calculate\nimputer = sage.MarginalImputer(model, np.asarray(x_dev))\nestimator = sage.PermutationEstimator(imputer, 'cross entropy')\nsage_values = estimator(np.asarray(x_test), np.asarray(y_test))","6afec226":"sage_values.plot(x_test.columns, title='Feature Importance (Marginal Sampling)')","0c0de0a3":"sensitivity = estimator(np.asarray(x_test))\nsensitivity.plot(x_test.columns, title='Model Sensitivity')","1a08ffe3":"y_hat_prob = model.predict_proba(x_test)[:,1]","47e2c0cb":"_= xai.metrics_plot(\n        y_test, \n        y_hat_prob)","51cfbb5a":"_ = xai.metrics_plot(\n    y_test, \n    y_hat_prob, \n    df=x_test, \n    cross_cols=[\"Sex\"],\n    categorical_cols=['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS',\n       'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope'])","e6f2129d":"xai.confusion_matrix_plot(y_test, y_hat)","1e1abeb4":"_ = xai.roc_plot(y_test, y_hat_prob)","31ac2928":"perf_measure(y_test[np.where(x_test[\"Sex\"]==0)[0]], y_hat[np.where(x_test[\"Sex\"]==0)[0]])","4b1ff2c8":"perf_measure(y_test[np.where(x_test[\"Sex\"]==1)[0]], y_hat[np.where(x_test[\"Sex\"]==1)[0]])","80310c64":"protected = [\"Sex\"]\n_ = [xai.roc_plot(\n    y_test, \n    y_hat_prob, \n    df=x_test, \n    cross_cols=[p],\n    categorical_cols=['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS',\n       'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope']) for p in protected]","1185db23":"d = xai.smile_imbalance(\n    y_test, \n    np.asarray(y_hat_prob),\n    display_breakdown=True)","b7705d78":"bal_df['HeartDisease'][bal_df['HeartDisease'] == 'no CVD'] = 0\nbal_df['HeartDisease'][bal_df['HeartDisease'] == 'CVD'] = 1\nbal_df['Sex'][bal_df['Sex'] == 'female'] = 0\nbal_df['Sex'][bal_df['Sex'] == 'male'] = 1","716d0f82":"bal_df.head()","aa36fec9":"X_bal = bal_df.iloc[:,:11]\ny_bal = bal_df.iloc[:,11]\n\nX_bal['Sex'] = X_bal['Sex'].astype(int)\ny_bal = y_bal.astype(int)","9ac50697":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X_bal,y_bal):\n    model =GaussianNB()\n    X_train, X_val = X_bal.loc[train_index], X_bal.loc[val_index]\n    y_train, y_val = y_bal[train_index], y_bal[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_val, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") \n\n    ","5d22fe04":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X_bal,y_bal):\n    model =LogisticRegression(solver=\"liblinear\", random_state=0)\n    X_train, X_val = X_bal.loc[train_index], X_bal.loc[val_index]\n    y_train, y_val = y_bal[train_index], y_bal[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","9d71d26e":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X_bal,y_bal):\n    model =svm.SVC()\n    X_train, X_val = X_bal.loc[train_index], X_bal.loc[val_index]\n    y_train, y_val = y_bal[train_index], y_bal[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","6ee34034":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X_bal,y_bal):\n    model = MLPClassifier(random_state=1, max_iter=500, learning_rate=\"adaptive\", solver=\"adam\", activation=\"relu\", tol=0.0001, momentum=0.99)\n    X_train, X_val = X_bal.loc[train_index], X_bal.loc[val_index]\n    y_train, y_val = y_bal[train_index], y_bal[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","7df310aa":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X_bal,y_bal):\n    model =xgboost.XGBClassifier(use_label_encoder=False,verbosity=0)\n    X_train, X_val = X_bal.loc[train_index], X_bal.loc[val_index]\n    y_train, y_val = y_bal[train_index], y_bal[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","a7ee16e1":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X_bal,y_bal):\n    model = RandomForestClassifier()\n    X_train, X_val = X_bal.loc[train_index], X_bal.loc[val_index]\n    y_train, y_val = y_bal[train_index], y_bal[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","fc5b9466":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X_bal,y_bal):\n    model =xgboost.XGBClassifier(use_label_encoder=False,verbosity=0 )\n    X_train, X_val = X_bal.loc[train_index], X_bal.loc[val_index]\n    y_train, y_val = y_bal[train_index], y_bal[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","ed9ea022":"model =xgboost.XGBClassifier(use_label_encoder=False, verbosity=0)\nmodel.fit(X_bal,y_bal)\ny_hat = model.predict(x_test)\nprint(f\"ROC = {roc_auc_score(y_test, y_hat)}\")\nprint(f\"Accuracy = {accuracy_score(y_test, y_hat)}\")\nprint(f\"F1-score(macro) = {f1_score(y_test, y_hat, average='macro')}\")\nprint(f\"F1-score(micro) = {f1_score(y_test, y_hat, average='micro')}\")\nprint(f\"Recall= {recall_score(y_test, y_hat)}\") \nprint(f\"Precision= {precision_score(y_test, y_hat)}\") ","09567815":"explainer = shap.KernelExplainer(model.predict_proba, X_bal, link=\"logit\")","a4da90fe":"shap_values = explainer.shap_values(x_test)","bcaa78fe":"shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], x_test.iloc[0,:], link=\"logit\")","8d59c680":"shap.force_plot(explainer.expected_value[0], shap_values[1][0,:], x_test.iloc[0,:], link=\"logit\")","91770417":"shap.force_plot(explainer.expected_value[0], shap_values[0], x_test, link=\"logit\")","dc6dd453":"shap.summary_plot(shap_values, x_test)","7abc00b2":"explainer = shap.Explainer(model, x_dev)\nshap_values = explainer(x_test)\nshap.plots.beeswarm(shap_values,max_display=14)","96bf493e":"# Setup and calculate\nimputer = sage.MarginalImputer(model, np.asarray(X_bal))\nestimator = sage.PermutationEstimator(imputer, 'cross entropy')\nsage_values = estimator(np.asarray(x_test), np.asarray(y_test))\n\nsage_values.plot(x_test.columns, title='Feature Importance (Marginal Sampling)')","39350383":"sensitivity = estimator(np.asarray(x_test))\nsensitivity.plot(x_test.columns, title='Model Sensitivity')","21e8462a":"y_hat_prob = model.predict_proba(x_test)[:,1]","c01b0ddc":"_= xai.metrics_plot(\n        y_test, \n        y_hat_prob)","ad62c29f":"_ = xai.metrics_plot(\n    y_test, \n    y_hat_prob, \n    df=x_test, \n    cross_cols=[\"Sex\"],\n    categorical_cols=['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS',\n       'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope'])","455f621a":"xai.confusion_matrix_plot(y_test, y_hat)","b95a6181":"_ = xai.roc_plot(y_test, y_hat_prob)","23ab8403":"perf_measure(y_test[np.where(x_test[\"Sex\"]==0)[0]], y_hat[np.where(x_test[\"Sex\"]==0)[0]])","2ef68913":"perf_measure(y_test[np.where(x_test[\"Sex\"]==1)[0]], y_hat[np.where(x_test[\"Sex\"]==1)[0]])","21957dad":"protected = [\"Sex\"]\n_ = [xai.roc_plot(\n    y_test, \n    y_hat_prob, \n    df=x_test, \n    cross_cols=[p],\n    categorical_cols=['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS',\n       'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope']) for p in protected]","b3228614":"df_m = df.loc[df['Sex'] == 1]\ndf_m = df_m.reset_index()","a7112d80":"X = df_m.iloc[:,1:-1]\ny = df_m.iloc[:,-1]","276b062f":"skf =  StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X,y):\n    model =xgboost.XGBClassifier(use_label_encoder=False, verbosity=0)\n    X_train, X_val = X.loc[train_index], X.loc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","1c0333da":"explainer = shap.KernelExplainer(model.predict_proba, X_train, link=\"logit\")","10cb5b46":"shap_values = explainer.shap_values(X_val)","64186bda":"shap.summary_plot(shap_values, X_val)","a1e72da8":"explainer = shap.Explainer(model, X_train)\nshap_values = explainer(X_val)\nshap.plots.beeswarm(shap_values, max_display=14)","a93895e4":"# Setup and calculate\nimputer = sage.MarginalImputer(model, np.asarray(X_train))\nestimator = sage.PermutationEstimator(imputer, 'cross entropy')\nsage_values = estimator(np.asarray(X_val), np.asarray(y_val))\n\nsage_values.plot(X_val.columns, title='Feature Importance (Marginal Sampling)')","869ace34":"sensitivity = estimator(np.asarray(X_val))\nsensitivity.plot(X_val.columns, title='Model Sensitivity')","4cd40f5a":"df_f = df.loc[df['sex'] == 0]\ndf_f = df_f.reset_index()","0262f0a8":"X = df_f.iloc[:,1:-1]\ny = df_f.iloc[:,-1]","f0b54d21":"skf =  StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X,y):\n    model =xgboost.XGBClassifier(use_label_encoder=False, verbosity=0)\n    X_train, X_val = X.loc[train_index], X.loc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","7b512189":"shap_values = explainer.shap_values(X_val)","74dc325b":"shap.summary_plot(shap_values, X_val)","418998fd":"explainer = shap.Explainer(model, X_train)\nshap_values = explainer(X_val)\nshap.plots.beeswarm(shap_values,max_display=14)","83c5dbed":"# Setup and calculate\nimputer = sage.MarginalImputer(model, np.asarray(X_train))\nestimator = sage.PermutationEstimator(imputer, 'cross entropy')\nsage_values = estimator(np.asarray(X_val), np.asarray(y_val))\n\nsage_values.plot(X_val.columns, title='Feature Importance (Marginal Sampling)')","226ea201":"sensitivity = estimator(np.asarray(X_val))\nsensitivity.plot(X_val.columns, title='Model Sensitivity')","5f693ae2":"## Global explanation using SAGE","28ff7ac2":"### XGBoost Classifier","81bdf6fa":"### Logistic Regression Classifier","ac1ddd32":"### Gaussian Naive Bayes Classifier","649fd48d":"### Random Forest Classifier","8be5821e":"## First we train and validate a couple of models based on the original dataset (almost twice as many males as females). Furthermore, we will explain the predictions using SHAP","b9b7576f":"### Now, lets take look back at the shapley values and feature importance for the model trained on the unbalanced dataset again","f061113d":"### Lets retrain the best model and look at the explanation using SHAP","2537f058":"# Conclusion\nWe can clearly see that in the balanced data set, the number of major vessels (ca) has a higher importance than in the imbalanced dataset dominated by males. Furthermore, we see that cholesterol (chol) and age (age) have a much higher impact on the total risk of developing heart disease in the balanced dataset than in the unbalanced dataset. This findings can be backed by previous research which shows that both cholesterol and age are a major risk factor in both male and females [1].\n\n[1] Lloyd-Jones DM, Wilson PWF, Larson MG, et al. Lifetime Risk of Coronary Heart Disease by Cholesterol Levels at Selected Ages. Arch Intern Med. 2003;163(16):1966\u20131972. doi:10.1001\/archinte.163.16.1966\n","33da013c":"### Logistic Regression Classifier","adae0c4c":"## Only men","d11dd5a7":"## Ethical AI","5d0d837b":"### XG Boost Classifier","cf6d1e61":"### Feed Forward Neural Network","73a4e36b":"### Lets retrain the best model and look at the explanation using SHAP","a8347577":"## Make a gender balanced dataset:","9f14acc5":"# <center> Gender differences in risk factors for heart disease<\/center>\n### In this notebook I use the well known Heart Disease UCI dataset to explore gender differences related to risk factors for developing heart disease\nA study done in Great Britain estimates that more than 8,200 women in England and Wales died over a ten-year period because they did not receive equal treatment to men. The risk of heart disease in women is often underestimated due to the misperception that females are \u2018protected\u2019 against cardiovascular disease. The under-recognition of heart disease and differences in clinical presentation in women lead to less aggressive treatment strategies and a lower representation of women in clinical trials. Furthermore, self-awareness in women and identification of their cardiovascular risk factors needs more attention, which should result in a better prevention of cardiovascular events.  AI-based algorithms are known to be prone to acquire bias and thus dedicated framwork should be used to reveal those biases in the data","1e950db1":"## Global explanation using SAGE","14e03df7":"## Check correlation","d6d52528":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https:\/\/c.tenor.com\/lJHChcg2eMgAAAAC\/beating-heart.gif\" alt=\"Heart beating\" style=\"height:500px;margin-top:3rem;\"> <\/div>","54583c71":"## Feature explanation:\n* age: The person's age in years\n* sex: The person's sex (1 = male, 0 = female)\n* cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n* trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n* chol: The person's cholesterol measurement in mg\/dl\n* fbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n* restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n* thalach: The person's maximum heart rate achieved\n* exang: Exercise induced angina (1 = yes; 0 = no)\n* oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n* slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n* target: Heart disease (0 = no, 1 = yes)","c8d2af60":"## Only women","f8601fcc":"## Global explanation using SAGE","cbe62697":"X = df.iloc[:,:13]\ny = df.iloc[:,13]\n\nskf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X,y):\n    model =xgboost.XGBClassifier(use_label_encoder=False, verbosity=0)\n    X_train, X_val = X.loc[train_index], X.loc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\n#print(f\"Mean ROC = {roc.mean()}\")\n#print(f\"Mean Accuracy = {acc.mean()}\")\n#print(f\"Mean F1-score(macro) = {f1ma.mean()}\")\n#print(f\"Mean F1-score(micro) = {f1mi.mean()}\") \n#print(f\"Mean Recall= {rec.mean()}\") \n#print(f\"Mean Precision= {pre.mean()}\") \n\nexplainer = shap.KernelExplainer(model.predict_proba, X_train, link=\"logit\")\nshap_values = explainer.shap_values(X_val)","0dce54df":"### Support Vector Machine Classifier","39eb9038":"### Random Forest Classifier","c7c52eea":"## Here we make a copy of the dataset to explore gender differences","9b7cf714":"## Global explanation using SAGE","fae09f77":"# Setup and calculate\nimputer = sage.MarginalImputer(model, np.asarray(X_train))\nestimator = sage.PermutationEstimator(imputer, 'cross entropy')\nsage_values = estimator(np.asarray(X_test), np.asarray(y_test))\n\nsage_values.plot(X_test.columns, title='Feature Importance (Marginal Sampling)')","49b8379b":"## Global explanation using SAGE","d7c8d0cd":"### Gaussian Naive Bayes Classifier","c84abdce":"sensitivity = estimator(np.asarray(X_test))\nsensitivity.plot(X_test.columns, title='Model Sensitivity')","959335e9":"## Ethical AI","0e11f27e":"shap.summary_plot(shap_values, X_test)","7acd7ed9":"### Feed Forward Neural Network","08bb4edb":"### Support Vector Machine Classifier","ad6c2f96":"### Lets now look at the same models using the balanced dataset as input","abd15f98":"## Make an equal dev\/test split"}}