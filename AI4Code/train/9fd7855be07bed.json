{"cell_type":{"dc9da08e":"code","e71bffe3":"code","eef5b625":"code","3077b330":"code","32b426c0":"code","ef9fa50a":"code","5e22af48":"code","2ee470c5":"code","bcee4f13":"code","97aa558b":"code","4e8e1de0":"code","cef24725":"code","493d6d43":"code","dc135dbc":"code","329a3cef":"code","c9d6ae10":"markdown","1a1330ba":"markdown","a803f212":"markdown","6d8b2f66":"markdown","86305cca":"markdown","2c866bca":"markdown"},"source":{"dc9da08e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e71bffe3":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain.shape, test.shape","eef5b625":"train.head()","3077b330":"train.isna().sum().sort_values()","32b426c0":"for col in ['Alley','FireplaceQu','Fence','MiscFeature','PoolQC']:\n    train[col].fillna('NA', inplace=True)\n    test[col].fillna('NA', inplace=True)\n    \ntrain['LotFrontage'].fillna(train[\"LotFrontage\"].value_counts().to_frame().index[0], inplace=True)\ntest['LotFrontage'].fillna(test[\"LotFrontage\"].value_counts().to_frame().index[0], inplace=True)\n\ntrain[['GarageQual','GarageFinish','GarageYrBlt','GarageType','GarageCond']].isna().head(7)\nfor col in ['GarageQual','GarageFinish','GarageYrBlt','GarageType','GarageCond']:\n    train[col].fillna('NA',inplace=True)\n    test[col].fillna('NA',inplace=True)\n\nfor col in ['BsmtQual','BsmtCond','BsmtFinType1','BsmtFinType2','BsmtExposure']:\n    train[col].fillna('NA',inplace=True)\n    test[col].fillna('NA',inplace=True)\n\ntrain['Electrical'].fillna('SBrkr',inplace=True)\n\nmissings = ['GarageCars','GarageArea','KitchenQual','Exterior1st','SaleType','TotalBsmtSF','BsmtUnfSF','Exterior2nd',\n            'BsmtFinSF1','BsmtFinSF2','BsmtFullBath','Functional','Utilities','BsmtHalfBath','MSZoning']\n\nnumerical=['GarageCars','GarageArea','TotalBsmtSF','BsmtUnfSF','BsmtFinSF1','BsmtFinSF2','BsmtFullBath','BsmtHalfBath']\ncategorical = ['KitchenQual','Exterior1st','SaleType','Exterior2nd','Functional','Utilities','MSZoning']\n\n# using Imputer class of sklearn libs.\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy='median',axis=0)\nimputer.fit(test[numerical] + train[numerical])\ntest[numerical] = imputer.transform(test[numerical])\ntrain[numerical] = imputer.transform(train[numerical])\n\nfor i in categorical:\n    train[i].fillna(train[i].value_counts().to_frame().index[0], inplace=True)\n    test[i].fillna(test[i].value_counts().to_frame().index[0], inplace=True)    \n\ntrain[train['MasVnrType'].isna()][['SalePrice','MasVnrType','MasVnrArea']]\n\ntrain[train['MasVnrType']=='None']['SalePrice'].median()\ntrain[train['MasVnrType']=='BrkFace']['SalePrice'].median()\ntrain[train['MasVnrType']=='Stone']['SalePrice'].median()\ntrain[train['MasVnrType']=='BrkCmn']['SalePrice'].median()\n\ntrain['MasVnrArea'].fillna(181000,inplace=True)\ntest['MasVnrArea'].fillna(181000,inplace=True)\n\ntrain['MasVnrType'].fillna('NA',inplace=True)\ntest['MasVnrType'].fillna('NA',inplace=True)\n\nprint(train.isna().sum().sort_values()[-2:-1])\nprint(test.isna().sum().sort_values()[-2:-1])","ef9fa50a":"int64 =[]\nobjects = []\nfor col in train.columns.tolist():\n    if np.dtype(train[col]) == 'int64' or np.dtype(train[col]) == 'float64':\n        int64.append(col)\n    else:\n        objects.append(col)                      #here datatype is 'object'\n\ncontinues_int64_cols = ['LotArea', 'LotFrontage', 'MasVnrArea','BsmtFinSF2','BsmtFinSF1','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF',\n                  'GrLivArea','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal']\ncategorical_int64_cols=[]\nfor i in int64:\n    if i not in continues_int64_cols:\n        categorical_int64_cols.append(i)\n\ndef barplot(X,Y):\n    plt.figure(figsize=(7,7))\n    sns.barplot(x=X, y=Y)\n    plt.show()\ndef scatter(X,Y):\n    plt.figure(figsize=(7,7))\n    sns.scatterplot(alpha=0.4,x=X, y=Y)\n    plt.show()\ndef hist(X):\n    plt.figure(figsize=(7,7))\n    sns.distplot(X, bins=40, kde=True)\n    plt.show()\ndef box(X):\n    plt.figure(figsize=(3,7))\n    sns.boxplot(y=X)\n    plt.show() \ndef line(X,Y):\n    plt.figure(figsize=(7,7))    \n    sns.lineplot(x=X, y=Y,color=\"coral\")\n    plt.show() \n\ntrain['MasVnrArea'] = train['MasVnrArea'].apply(lambda row: 1.0 if row>0.0 else 0.0)\ntrain['BsmtFinSF2'] = train['BsmtFinSF2'].apply(lambda row: 1.0 if row>0.0 else 0.0)\nbinary_cate_int64_cols = []\nbinary_cate_int64_cols.append('MasVnrArea')\nbinary_cate_int64_cols.append('BsmtFinSF2')\n\ntrain['LowQualFinSF'] = train['LowQualFinSF'].apply(lambda row: 1.0 if row>0.0 else 0.0)\nbinary_cate_int64_cols.append('LowQualFinSF')\n\nfor i in continues_int64_cols[14:]:\n    train[i] = train[i].apply(lambda row: 1.0 if row>0.0 else 0.0)\n    binary_cate_int64_cols.append(i)\n\nfor j in binary_cate_int64_cols:\n    if j in continues_int64_cols:\n        continues_int64_cols.remove(j)        #these special columns removing from the continues_int64_cols\n\n# we changed values of train only, here for test set\nfor i in binary_cate_int64_cols:\n    test[i] = test[i].apply(lambda row: 1.0 if row>0.0 else 0.0)\n# test[binary_cate_int64_cols].head(6)\n\nordinal_categorical_cols =[]\nordinal_categorical_cols.extend(['ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','HeatingQC','KitchenQual'])\nordinal_categorical_cols.extend(['FireplaceQu', 'GarageQual','GarageCond','PoolQC'])\n\nfor i in ordinal_categorical_cols:\n    if i in objects:\n        objects.remove(i)            # removing ordinal features from the objects\n\n# removinf 'Id' and 'SalePrice'\ncategorical_int64_cols.remove('Id')\ncategorical_int64_cols.remove('SalePrice')\n\ntrain_objs_num = len(train)\ndataset = pd.concat(objs=[train[categorical_int64_cols + objects], test[categorical_int64_cols+ objects]], axis=0)\ndataset_preprocessed = pd.get_dummies(dataset.astype(str), drop_first=True)\ntrain_nominal_onehot = dataset_preprocessed[:train_objs_num]\ntest_nominal_onehot= dataset_preprocessed[train_objs_num:]\n# train_nominal_onehot.shape, test_nominal_onehot.shape\n\ntrain['BsmtExposure'] = train['BsmtExposure'].map({'Gd':4, 'Av':3, 'Mn':2, 'No':1,'NA':0})\ntest['BsmtExposure'] = test['BsmtExposure'].map({'Gd':4, 'Av':3, 'Mn':2, 'No':1,'NA':0})\n\norder = {'Ex':5,\n        'Gd':4, \n        'TA':3, \n        'Fa':2, \n        'Po':1,\n        'NA':0 }\nfor i in ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']:\n    train[i] = train[i].map(order)\n    test[i] = test[i].map(order)\ntest[ordinal_categorical_cols].head()         \n\nX = pd.concat([train[ordinal_categorical_cols], train[continues_int64_cols], train[binary_cate_int64_cols], train_nominal_onehot], axis=1)\ny = train['SalePrice']\ntest_final = pd.concat([test[ordinal_categorical_cols], test[continues_int64_cols], test[binary_cate_int64_cols], test_nominal_onehot], axis=1)\n              \nX.shape, y.shape, test_final.shape","5e22af48":"X.head()","2ee470c5":"from sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor","bcee4f13":"def lgbm(X, y, test_final,param, kfolds=5):\n    \n    folds = KFold(n_splits=kfolds, shuffle=True, random_state=4590)\n    \n    feature_importance = np.zeros((X.shape[1],kfolds))\n    val_matrix = np.zeros(len(X))\n    pred_matrix = np.zeros(len(test_final))\n    \n    for fold_, (train_id, val_id) in enumerate(folds.split(X,X)):\n        print('--------------Fold-no---',fold_)\n        x0,y0 = X.iloc[train_id], y[train_id]\n        x1,y1 = X.iloc[val_id], y[val_id]\n        \n        train_data = lgb.Dataset(x0, label= y0) \n        val_data = lgb.Dataset(x1, label= y1)\n        \n        num_round = 10000\n        \n        clf = lgb.train(param, train_data, num_round, valid_sets = [train_data, val_data], \n                        verbose_eval=1500, early_stopping_rounds = 150)\n        \n        val_matrix[val_id] = clf.predict(x1, num_iteration=clf.best_iteration)\n        feature_importance[:, fold_] = clf.feature_importance()\n        pred_matrix += clf.predict(test_final, num_iteration=clf.best_iteration) \/ folds.n_splits\n\n    print(' Validation error: ',np.sqrt(mean_squared_error(val_matrix, y)))\n    return clf, pred_matrix, feature_importance","97aa558b":"param = {'num_leaves': 129,\n         'min_data_in_leaf': 148, \n         'objective':'regression',\n         'max_depth': 9,\n         'learning_rate': 0.005,\n         \"min_child_samples\": 24,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.7202,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.8125 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.3468,\n         \"verbosity\": -1}\n# Training LGB\nmodel, predictions,feature_importance = lgbm(X ,y , test_final,param = param, kfolds=10)\nprint(\"LightGBM Training Completed...\")","4e8e1de0":"ximp = pd.DataFrame()\nximp['feature'] = X.columns\nximp['importance'] = feature_importance.mean(axis = 1)\n\nplt.figure(figsize=(14,16))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=ximp.sort_values(by=\"importance\",ascending=False).iloc[:85,:])\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()","cef24725":"features = ximp.sort_values(by=\"importance\",ascending=False).iloc[:81,0].values\nlen(features),features","493d6d43":"params = {'boosting_type': 'gbdt',\n          'max_depth' : -1,\n          'objective': 'regression',\n          'num_leaves': 64,\n          'learning_rate': 0.05,\n          'max_bin': 512,\n          'subsample_for_bin': 200,\n          'subsample': 1,\n          'subsample_freq': 1,\n          'colsample_bytree': 0.8,\n          'reg_alpha': 5,\n          'reg_lambda': 10,\n          'min_split_gain': 0.5,\n          'min_child_weight': 1,\n          'min_child_samples': 5,\n          'scale_pos_weight': 1,\n          'num_class' : 1,\n          'metric' : 'rmse'}\n# Create parameters to search\ngridParams = {\n    'learning_rate': [0.005],\n    'n_estimators': [40],\n    'num_leaves': [6,8,12,16],\n    'colsample_bytree' : [0.65, 0.66],\n    'subsample' : [0.7,0.75],\n    'reg_alpha' : [1,1.2],\n    'reg_lambda' : [1,1.2,1.4],\n    }\nmdl = lgb.LGBMClassifier(boosting_type= 'gbdt',\n          objective = 'regression',\n          n_jobs = -1,\n          silent = True,\n          max_depth = params['max_depth'],\n          max_bin = params['max_bin'],\n          subsample_for_bin = params['subsample_for_bin'],\n          subsample = params['subsample'],\n          subsample_freq = params['subsample_freq'],\n          min_split_gain = params['min_split_gain'],\n          min_child_weight = params['min_child_weight'],\n          min_child_samples = params['min_child_samples'],\n          scale_pos_weight = params['scale_pos_weight'])\n\n# Create the grid\ngrid = GridSearchCV(mdl, gridParams,\n                    verbose=0,\n                    cv=5,\n                    n_jobs=-1)\n# Run the grid\ngrid.fit(X[features], y)\n\n# Print the best parameters found\nprint(grid.best_params_)\nprint(grid.best_score_)","dc135dbc":"best_params = {\n          'max_depth' : -1,\n          'num_leaves': 64,\n          'reg_alpha': 5,\n          'reg_lambda': 10,\n          'boosting_type': 'gbdt',\n          'objective': 'regression',\n          'learning_rate': 0.05,\n          'max_bin': 512,\n          'subsample_for_bin': 200,\n          'subsample': 1,\n          'subsample_freq': 1,\n          'colsample_bytree': 0.8,\n          'min_split_gain': 0.5,\n          'min_child_weight': 1,\n          'min_child_samples': 5,\n          'scale_pos_weight': 1,\n          'num_class' : 1,\n          'metric' : 'rmse'}\n# Training LGB\nmodel, predictions,feature_importance = lgbm(X[features] ,y , test_final[features],param = best_params, kfolds=10)\nprint(\"LightGBM Training Completed...\")","329a3cef":"\nsubmission = pd.DataFrame()\nsubmission['Id'] = test['Id']\nsubmission['SalePrice'] = predictions\nsubmission.to_csv('submission_lgbm_kfold.csv',index=False)\nsubmission.head()","c9d6ae10":"# 1) Data Cleaning","1a1330ba":"# 3) Modeling  ","a803f212":"## 3.1) LightGBM","6d8b2f66":"I made a separate kernels for understanding NaNs and choosing best filling NaNs strategy, you can see here : [house-price-fillna-strategy](http:\/\/www.kaggle.com\/ashishbarvaliya\/house-price-fillna-strategy), here i am not gonna repeat.","86305cca":"I am using LightGBM with 10 kfolds(why 10?,because i tried 5,7.., and 10 was best)","2c866bca":"# 2) Feature Extraction\nI made a separate kernels for feature extraction, visualization of the features, you can see here : [\/house-price-feature-extraction-strategy](https:\/\/www.kaggle.com\/ashishbarvaliya\/house-price-feature-extraction-strategy), here i am not gonna repeat."}}