{"cell_type":{"a7938d8f":"code","bd68d7a4":"code","6b55306b":"code","ab3ea88c":"code","cfd9c441":"code","91f79d53":"code","c13a5b0e":"code","7312149d":"code","ddc5a4d6":"code","387c1736":"code","4919bdfa":"code","96b2e27f":"markdown","85eedd83":"markdown","18f20858":"markdown","d9edf147":"markdown","c1a43758":"markdown","cc2a32ec":"markdown","5ed8ef69":"markdown","08e6f505":"markdown","2916abd9":"markdown","d5c89d80":"markdown","be51ba53":"markdown","c0c231a1":"markdown","9ffa116f":"markdown","d39c6e10":"markdown","41126bac":"markdown","f26927d6":"markdown"},"source":{"a7938d8f":"!git clone https:\/\/github.com\/rkuo2000\/DeepFashion_Try_On\n%cd DeepFashion_Try_On","bd68d7a4":"!mkdir Dataset","6b55306b":"# copy VITON dataset\n!cp -r \/kaggle\/input\/viton-dataset\/ACGPN_TestData\/test_color Dataset\n!cp -r \/kaggle\/input\/viton-dataset\/ACGPN_TestData\/test_edge Dataset\n!cp -r \/kaggle\/input\/viton-dataset\/ACGPN_TestData\/test_mask Dataset\n!cp -r \/kaggle\/input\/viton-dataset\/ACGPN_TestData\/test_colormask Dataset","ab3ea88c":"# copy TestData\n!cp -r \/kaggle\/input\/tryon-testdata\/Dataset\/test_img Dataset\n!cp -r \/kaggle\/input\/tryon-testdata\/Dataset\/test_pose Dataset\n!cp -r \/kaggle\/input\/tryon-testdata\/Dataset\/test_label Dataset","cfd9c441":"import os\nprint('test image    :', len(os.listdir('.\/Dataset\/test_img')))   # test image (person with clothes)\nprint('test pose     :', len(os.listdir('.\/Dataset\/test_pose')))  # pose keypoints per test image\nprint('test label    :', len(os.listdir('.\/Dataset\/test_label'))) # label (dark frame) of test image (for pose-map)\nprint('test color    :', len(os.listdir('.\/Dataset\/test_color')))     # color clothes  \nprint('test edge     :', len(os.listdir('.\/Dataset\/test_edge')))      # edge of clothes\nprint('test mask     :', len(os.listdir('.\/Dataset\/test_mask')))      # test mask        \nprint('test colormask:', len(os.listdir('.\/Dataset\/test_colormask'))) # test colormask","91f79d53":"# read pose\nimport numpy as np\nimport json\n#pose_name = '\/kaggle\/input\/tryon-testdata\/Dataset\/test_pose\/000000_0_keypoints.json'\npose_name = '.\/Dataset\/test_pose\/000000_0_keypoints.json'\nwith open(pose_name, 'r') as f:\n     pose_label = json.load(f)\n     pose_data = pose_label['people'][0]['pose_keypoints']\n     pose_data = np.array(pose_data)\n     pose_data = pose_data.reshape((-1,3))\nprint(pose_data)\nprint(len(pose_data))","c13a5b0e":"import matplotlib.pyplot as plt\nimg = plt.imread('.\/Dataset\/test_img\/000000_0.jpg')\nplt.imshow(img)\ni=0\nfor x,y,z in pose_data: \n    plt.plot(x, y, 'w.') # 'w.': color='white', marker='.'\n    plt.text(x, y, str(i), color='r', fontsize=10)\n    i+=1\nplt.show()\nprint(i)","7312149d":"%cd ACGPN_inference","ddc5a4d6":"# copy the pre-trained model (checkpoint)\n!cp -rf \/kaggle\/input\/acgpn-checkpoints\/label2city checkpoints","387c1736":"# clothes fitting, selecting color_name for the clothes\n!python test.py --dataroot ..\/Dataset --color_name 000129_1.jpg","4919bdfa":"from IPython.display import Image\nImage('.\/sample\/000000_0.jpg')","96b2e27f":"### read test_pose .json","85eedd83":"#### copy VITON dataset (color, edge, mask, colormask)","18f20858":"#### show number of files in the directory","d9edf147":"#### output : sample\/000000_0.jpg","c1a43758":"### show pose keypoints on test image","cc2a32ec":"## Paper: [Towards Photo-Realistic Virtual Try-On by Adaptively Generating, CVPR'20](https:\/\/arxiv.org\/abs\/2003.05863)","5ed8ef69":"### Display GAN result","08e6f505":"![TryOn.jpg](attachment:TryOn.jpg)\n\n#### real_image (original) -----> pose_map -----> cloth_mask -----> color (dress) -----> fake_image (generated)","2916abd9":"## Download pre-trained model (checkpoint)","d5c89d80":"## Test GAN model ","be51ba53":"#### copy TestData (image, pose, label)","c0c231a1":"## Repro [Github](https:\/\/github.com\/switchablenorms\/DeepFashion_Try_On)","9ffa116f":"## Copy Dataset","d39c6e10":"## Read Pose","41126bac":"#### License: The use of this software is RESTRICTED to non-commercial research and educational purposes.","f26927d6":"## Dataset: VITON dataset \n    This dataset contains 16,253 image pairs, further splitting into a training set of 14,221 paris and a testing set of 2,032 pairs."}}