{"cell_type":{"a014576f":"code","7da15358":"code","94e35464":"code","4ef78cc7":"code","3936c529":"code","8b7210b1":"code","bf4b334f":"code","e8319e33":"code","ae570630":"code","04dfeb2e":"code","c944dd8d":"code","fe63e433":"code","47e043fe":"code","e8876de1":"code","802a1cdd":"markdown","30b3ede8":"markdown","59fee39f":"markdown","71adf3ae":"markdown","40681969":"markdown","bea9cdf9":"markdown","034470d2":"markdown","4ff7d92e":"markdown","8886b110":"markdown","4e09ceb9":"markdown","8b57d1d4":"markdown","003f7c89":"markdown","b4452da9":"markdown","42c16236":"markdown"},"source":{"a014576f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7da15358":"df = pd.read_csv(\"..\/input\/hepatitis.data.txt\")\n#df.info()\ndf.replace(\"?\",-99999,inplace=True)\ndf = df.convert_objects(convert_numeric=True)","94e35464":"x = df.drop([\"class\"],axis=1)\ny = df[\"class\"].values\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)","4ef78cc7":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_test,y_test)\naccuracy_lor=lr.score(x_train,y_train)\nprint(\"linear regression accuracy:%{}\".format(accuracy_lor*100))","3936c529":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\ndtr.fit(x_test,y_test)\naccuracy_dtr = dtr.score(x_train,y_train)\nprint(\"decision tree regression accuracy:%{}\".format(accuracy_dtr*100))","8b7210b1":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(x_test,y_test)\naccuracy_rfr = rfr.score(x_train,y_train)\nprint(\"random forest regression accuracy:%{}\".format(accuracy_rfr*100))","bf4b334f":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\naccuracy_lr=lr.score(x_test,y_test)\nprint(\"logistic regression accuracy:%{}\".format(accuracy_lr*100))","e8319e33":"\n########################################\n#Choosing best k value\nfrom  sklearn.neighbors import KNeighborsClassifier\nfrom collections import Counter\nscores2={}\nscores=[]\nindex=[]\ndef sort(scores):        \n    votes = [i for i in sorted(scores)]\n    print(scores)\n    vote_result = max(scores)\n    return vote_result\n\nfor each in range(1,18):\n    knn2 = KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    score=(knn2.score(x_test,y_test))\n    scores2.update({each:score})\n    scores.append(knn2.score(x_test,y_test))\n    #index.append(each)\nvote_result=sort(scores2)  \nprint(vote_result)\n########################################\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=11,n_jobs=-1)#k default 3, n_jobs -1 for max performance\nknn.fit(x_train,y_train)\naccuracy_knn = knn.score(x_test,y_test)\nprint(\"knn accuracy:%{}\".format(accuracy_knn*100))\n########################################\nplt.plot(range(1,18),scores)\nplt.xlabel(\"k values\")\nplt.ylabel(\"scores\")\nplt.show()","ae570630":"from sklearn.svm import SVC\nsvm = SVC(random_state=1)\nsvm.fit(x_train,y_train)\naccuracy_svm = svm.score(x_test,y_test)\nprint(\"svm accuracy:%{}\".format(accuracy_svm*100))","04dfeb2e":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\naccuracy_nb = nb.score(x_test,y_test)\nprint(\"naive bayes accuracy:%{}\".format(accuracy_nb*100))","c944dd8d":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\naccuracy_dt = dt.score(x_test,y_test)\nprint(\"decision tree accuracy:%{}\".format(accuracy_dt*100))","fe63e433":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 1000)#n_estimators --> tree say\u0131s\u0131\nrf.fit(x_train,y_train)\naccuracy_rf = rf.score(x_test,y_test)\nprint(\"random forest accuracy:%{}\".format(accuracy_rf*100))","47e043fe":"print(\"linear regression accuracy:%{}\".format(accuracy_lor*100))\nprint(\"decision tree regression accuracy:%{}\".format(accuracy_dtr*100))\nprint(\"random forest regression accuracy:%{}\".format(accuracy_rfr*100))\nprint(\"logistic regression accuracy:%{}\".format(accuracy_lr*100))\nprint(\"knn accuracy:%{}\".format(accuracy_knn*100))\nprint(\"svm accuracy:%{}\".format(accuracy_svm*100))\nprint(\"naive bayes accuracy:%{}\".format(accuracy_nb*100))\nprint(\"decision tree accuracy:%{}\".format(accuracy_dt*100))\nprint(\"random forest accuracy:%{}\".format(accuracy_rf*100))","e8876de1":"from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\nmodels=[]\nmodels.append((\"LR\",LogisticRegression()))\nmodels.append((\"NB\",GaussianNB()))\nmodels.append((\"KNN\",KNeighborsClassifier(n_neighbors=5)))\nmodels.append((\"DT\",DecisionTreeClassifier()))\nmodels.append((\"SVM\",SVC()))\nfor name, model in models:\n    \n    clf=model\n\n    clf.fit(x_train, y_train)\n\n    y_pred=clf.predict(x_test)\n    print(10*\"=\",\"{} i\u00e7in Sonu\u00e7lar\".format(name).upper(),10*\"=\")\n    print(\"Ba\u015far\u0131 oran\u0131:{:0.2f}\".format(accuracy_score(y_test, y_pred)))\n    print(\"Kar\u0131\u015f\u0131kl\u0131k Matrisi:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n    print(\"S\u0131n\u0131fland\u0131rma Raporu:\\n{}\".format(classification_report(y_test,y_pred)))\n    print(30*\"=\")","802a1cdd":"# Naive Bayes","30b3ede8":"<h1>CLASSIFIER<h1>  \n","59fee39f":"\n# Support Vector Machines","71adf3ae":"# Logistic Regression","40681969":"# K Nearest Neighbors","bea9cdf9":"***Thanks for your attention. ***\n\nLeave a vote and comment please:)","034470d2":"# Decision Tree Regression","4ff7d92e":"# SUPERVISED LEARNING\n* Supervised learning: It uses data that has labels. Example, there are orthopedic patients data that have labels normal and abnormal.\n* There are features(predictor variable) and target variable. Features are like pelvic radius or sacral slope(If you have no idea what these are like me, you can look images in google like what I did :) )Target variables are labels normal and abnormal\n* Aim is that as given features(input) predict whether target variable(output) is normal or abnormal\n* Classification: target variable consists of categories like normal or abnormal\n* Regression: target variable is continious like stock market\n* If these explanations are not enough for you, just google them. However, be careful about terminology: features = predictor variable = independent variable = columns = inputs. target variable = responce variable = class = dependent variable = output = result","8886b110":"It seems Random Forest Classifier is the best Supervised Algo for us:)","4e09ceb9":"# Random Forest Classifier","8b57d1d4":"# REGRESSION","003f7c89":"\n# Random Forest Regression","b4452da9":"# Decision Tree Classifier","42c16236":"# Linear Regression"}}