{"cell_type":{"77627954":"code","682e9bdb":"code","f01df730":"code","b5a21533":"code","594e0eae":"code","db836470":"code","8aeee4a8":"code","103917cc":"code","d6db2fdf":"code","16bee5f9":"code","abd652fe":"code","c6b0982b":"code","6855ddd9":"code","babf3021":"code","54b4159a":"code","2c9470d0":"code","6f43f792":"code","87d91e78":"code","03cded77":"markdown","d703dc3a":"markdown"},"source":{"77627954":"%%writefile GLONASS_FCN_OSN_MAP.json\n{\n  \"test_2020-08-03-US-MTV-2_Mi8\": {\n    \"96\": -1,\n    \"98\": 13,\n    \"103\": 19,\n    \"104\": -1,\n    \"105\": 3\n  },\n  \"test_2020-08-13-US-MTV-1_Mi8\": {\n    \"105\": 7,\n    \"106\": 4,\n    \"103\": 19\n  },\n  \"test_2021-03-25-US-PAO-1_Mi8\": {\n    \"98\": 9,\n    \"102\": 24\n  },\n  \"train_2020-05-21-US-MTV-2_Pixel4\": {\n    \"96\": -1\n  },\n  \"train_2020-05-21-US-MTV-2_Pixel4XL\": {\n    \"96\": -1\n  },\n  \"train_2020-05-29-US-MTV-1_Pixel4\": {\n    \"103\": 23\n  },\n  \"train_2020-07-17-US-MTV-1_Mi8\": {\n    \"106\": -1,\n    \"99\": 16,\n    \"100\": 15,\n    \"103\": 23\n  },\n  \"train_2020-07-17-US-MTV-2_Mi8\": {\n    \"106\": -1,\n    \"99\": -1,\n    \"101\": 1,\n    \"103\": -1\n  },\n  \"train_2020-08-03-US-MTV-1_Mi8\": {\n    \"98\": 9,\n    \"99\": -1,\n    \"102\": -1,\n    \"103\": 19\n  },\n  \"train_2020-08-06-US-MTV-2_Mi8\": {\n    \"105\": 7,\n    \"102\": 20,\n    \"103\": -1\n  },\n  \"train_2020-09-04-US-SF-1_Mi8\": {\n    \"105\": 3,\n    \"106\": 8,\n    \"93\": 14,\n    \"103\": 23\n  },\n  \"train_2020-09-04-US-SF-2_Mi8\": {\n    \"106\": -1,\n    \"100\": 15,\n    \"101\": 1\n  },\n  \"train_2021-01-05-US-SVL-1_Mi8\": {\n    \"104\": 21,\n    \"97\": 22\n  },\n  \"train_2021-04-26-US-SVL-1_Mi8\": {\n    \"106\": 4\n  },\n  \"train_2021-04-28-US-SJC-1_Pixel4\": {\n    \"93\": -1\n  }\n}","682e9bdb":"%%writefile generate_phone_isrbM.py\nimport json\nfrom dataclasses import dataclass\nfrom collections import defaultdict\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nINPUT_PATH = '..\/input\/google-smartphone-decimeter-challenge'\n\n@dataclass\nclass Average:\n    total: float = 0\n    count: float = 0\n    def __add__(self, other):\n        total = self.total + other.total\n        count = self.count + other.count\n        return Average(total=total, count=count)\n    @property\n    def value(self):\n        return self.total \/ self.count\n\ndef main():\n    path_list = glob.glob(f'{INPUT_PATH}\/train\/*\/*') + glob.glob(f'{INPUT_PATH}\/test\/*\/*')\n    phone_to_drive = defaultdict(list)\n    for path in path_list:\n        prefix, drive, phone = path.split('\/')[-3:]\n        phone_to_drive[phone].append((prefix, drive))\n\n    PHONE_ISRBM = dict()\n    for phone in tqdm(phone_to_drive.keys()):\n        phone_isrbM = defaultdict(Average)\n        for prefix, drive in phone_to_drive[phone]:\n            df = pd.read_csv(f'{INPUT_PATH}\/{prefix}\/{drive}\/{phone}\/{phone}_derived.csv')\n            for sig_type, sig_df in df.groupby('signalType'):\n                isrbM = sig_df['isrbM'].values\n                q10   = np.quantile(isrbM, 0.10)\n                q90   = np.quantile(isrbM, 0.90)\n                cond  = (q10 <= isrbM) & (isrbM <= q90)\n                isrbM = isrbM[cond]\n                mu = np.mean(isrbM)\n                N  = min(5000, isrbM.shape[0])\n                phone_isrbM[sig_type] += Average(total=mu * N, count=N)\n        PHONE_ISRBM[phone] = {sig : ave.value for sig, ave in phone_isrbM.items()}\n    with open('phone_isrbM.json', 'w') as f:\n        json.dump(PHONE_ISRBM, f)\n    return","f01df730":"%%writefile constants.py\nimport json\nimport datetime\nfrom collections import defaultdict\nimport numpy as np\n\nwith open('GLONASS_FCN_OSN_MAP.json') as f:\n    GLONASS_FCN_OSN_MAP = json.load(f)\nwith open('phone_isrbM.json') as f:\n    PHONE_ISRBM = json.load(f)\ndel f\n\nGPS_ORIGIN_DAY       = datetime.date(1980, 1, 6)\nGPS_ORIGIN_DATETIME  = datetime.datetime(1980, 1, 6)\nGLONASS_LEAP_SECONDS = 18\nBEIDOU_LEAP_SECONDS  = 14\nTZ_MSK = datetime.timezone(datetime.timedelta(hours=+3), 'MSK')\n\nWGS84_SEMI_MAJOR_AXIS = 6378137.0\nWGS84_SEMI_MINOR_AXIS = 6356752.314245\nWGS84_SQUARED_FIRST_ECCENTRICITY  = 6.69437999013e-3\nWGS84_SQUARED_SECOND_ECCENTRICITY = 6.73949674226e-3\nWGS84_FIRST_ECCENTRICITY  = np.sqrt(WGS84_SQUARED_FIRST_ECCENTRICITY)\nWGS84_SECOND_ECCENTRICITY = np.sqrt(WGS84_SQUARED_SECOND_ECCENTRICITY)\n\nLIGHT_SPEED = 299792458.0\n\nOMEGA_EARTH = 7.2921151467e-5\nMU_EARTH    = 3.986005e+14\n\nFREQ_GPS_L1  = 1.575420e+09\nFREQ_GPS_L5  = 1.176450e+09\nFREQ_GAL_E1  = FREQ_GPS_L1\nFREQ_GAL_E5A = FREQ_GPS_L5\nFREQ_QZS_J1  = FREQ_GPS_L1\nFREQ_QZS_J5  = FREQ_GPS_L5\nFREQ_BDS_B1I = 1.561098e+09\nFREQ_GLO_G1_NOMINAL = 1602.00 * 1e+6\nFREQ_GLO_G1_DELTA   = 562.5 * 1e+3\n\nCONSTELLATION_TYPE_MAP = {\n    'GPS'     : 1,\n    'GLONASS' : 3,\n    'QZSS'    : 4,\n    'BEIDOU'  : 5,\n    'GALILEO' : 6,\n}\n\nRAW_STATE_BIT_MAP = {\n     0: \"Code Lock\",\n     1: \"Bit Sync\",\n     2: \"Subframe Sync\",\n     3: \"Time Of Week Decoded State\",\n     4: \"Millisecond Ambiguity\",\n     5: \"Symbol Sync\",\n     6: \"GLONASS String Sync\",\n     7: \"GLONASS Time Of Day Decoded\",\n     8: \"BEIDOU D2 Bit Sync\",\n     9: \"BEIDOU D2 Subframe Sync\",\n    10: \"Galileo E1BC Code Lock\",\n    11: \"Galileo E1C 2^nd^ Code Lock\",\n    12: \"Galileo E1B Page Sync\",\n    13: \"SBAS Sync\",\n    14: \"Time Of Week Known\",\n    15: \"GLONASS Time Of Day Known\",\n}\nRAW_STATE_BIT_INV_MAP = { value : key for key, value in RAW_STATE_BIT_MAP.items() }\n\nACCUMULATED_DELTA_RANGE_STATE_BIT_MAP = {\n    0 : 'VALID',\n    1 : 'RESET',\n    2 : 'CYCLE_SLIP',\n    3 : 'HALF_CYCLE_RESOLVED',\n    4 : 'HALF_CYCLE_REPORTED',\n}\nACCUMULATED_DELTA_RANGE_STATE_BIT_INT_MAP = {\n    value : key for key, value\n    in ACCUMULATED_DELTA_RANGE_STATE_BIT_MAP.items()\n}\n\nSYSTEM_NAME_MAP = {\n    'GPS'     : 'G',\n    'GLONASS' : 'R',\n    'GALILEO' : 'E',\n    'BEIDOU'  : 'C',\n    'QZSS'    : 'J',\n}\n\nGLONASS_FREQ_CHANNEL_MAP = {\n    1 : 1,\n    2 : -4,\n    3 : 5,\n    4 : 6,\n    5 : 1,\n    6 : -4,\n    7 : 5,\n    8 : 6,\n    9 : -2,\n    10 : -7,\n    11 : 0,\n    12 : -1,\n    13 : -2,\n    14 : -7,\n    15 : 0,\n    16 : -1,\n    17 : 4,\n    18 : -3,\n    19 : 3,\n    20 : 2,\n    21 : 4,\n    22 : -3,\n    23 : 3,\n    24 : 2,\n}\n\nQZSS_PRN_SVID_MAP = {\n    193 : 1,\n    194 : 2,\n    199 : 3,\n    195 : 4,\n}\n\nINIT_B = np.deg2rad(  37.5)\nINIT_L = np.deg2rad(-122.2)\nINIT_H = 0.0\n\nFREQ_TOL = 100.0\nCn0DbHz_THRESHOLD = 20.0\nReceivedSvTimeUncertaintyNanos_THRESHOLD = 500\nRAW_PSEUDO_RANGE_THRESHOLD = 50_000 * 1e+3\n\nCLOCK_TIME_MARGIN = datetime.timedelta(seconds=90)\nORBIT_TIME_MARGIN = datetime.timedelta(hours=3)\nIONO_TIME_MARGIN  = datetime.timedelta(hours=2)\n\nEPSILON_M = 0.01\nDEFAULT_TROPO_DELAY_M = 2.48\n\nHAVERSINE_RADIUS = 6_371_000","b5a21533":"%%writefile io_f.py\nimport io\nimport datetime\nfrom dataclasses import dataclass, asdict\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import InterpolatedUnivariateSpline, RectBivariateSpline\n\n@dataclass\nclass IONEX:\n    iono_height : float\n    base_radius : float\n    lat_1       : float\n    lat_2       : float\n    lat_delta   : float\n    lng_1       : float\n    lng_2       : float\n    lng_delta   : float\n    time_1      : np.datetime64\n    time_2      : np.datetime64\n    time_delta  : np.timedelta64\n    iono_map    : np.array\n    lat_range   : np.array\n    lng_range   : np.array\n\ndef concat_clk(clk_df_list):\n    clk_df = pd.concat(clk_df_list, axis=0)\n    sat_set_list = [frozenset(clk_df['SatName']) for clk_df in clk_df_list]\n    sat_prod = sat_set_list[0]\n    for sat_set in sat_set_list[1:]:\n        sat_prod = sat_prod & sat_set\n    clk_df = clk_df[clk_df['SatName'].isin(sat_prod)]\n    clk_df = clk_df.reset_index(drop=True)\n    return clk_df\n\ndef concat_sp3(sp3_df_list):\n    sp3_df  = pd.concat(sp3_df_list, axis=0)\n    sat_set_list = [frozenset(sp3_df['SatName']) for sp3_df in sp3_df_list]\n    sat_prod = sat_set_list[0]\n    for sat_set in sat_set_list[1:]:\n        sat_prod = sat_prod & sat_set\n    sp3_df = sp3_df[sp3_df['SatName'].isin(sat_prod)]\n    sp3_df = sp3_df.reset_index(drop=True)\n    return sp3_df\n\ndef concat_ionex(ionex_list):\n    assert(len(np.unique([ionex.iono_height for ionex in ionex_list])) == 1)\n    assert(len(np.unique([ionex.base_radius for ionex in ionex_list])) == 1)\n    assert(len(np.unique([ionex.lat_1       for ionex in ionex_list])) == 1)\n    assert(len(np.unique([ionex.lat_2       for ionex in ionex_list])) == 1)\n    assert(len(np.unique([ionex.lat_delta   for ionex in ionex_list])) == 1)\n    assert(len(np.unique([ionex.lng_1       for ionex in ionex_list])) == 1)\n    assert(len(np.unique([ionex.lng_2       for ionex in ionex_list])) == 1)\n    assert(len(np.unique([ionex.lng_delta   for ionex in ionex_list])) == 1)\n    assert(len(np.unique([ionex.time_delta  for ionex in ionex_list])) == 1)\n    N = len(ionex_list)\n    iono_map = []\n    for i in range(N-1):\n        assert(ionex_list[i].time_2 == ionex_list[i+1].time_1)\n        iono_map.append(ionex_list[i].iono_map[0:-1, :, :])\n    iono_map.append(ionex_list[-1].iono_map)\n    kw = asdict(ionex_list[0])\n    kw['time_2']   = ionex_list[-1].time_2\n    kw['iono_map'] = np.concatenate(iono_map, axis=0)\n    return IONEX(**kw)\n\ndef read_GnssLog_Raw(filename):\n    lines = []\n    with open(filename, 'r') as f:\n        for line in f:\n            if 'Raw' in line:\n                line = line.rstrip().lstrip('#')\n                lines.append(line)\n    sio = io.StringIO('\\n'.join(lines))\n    return pd.read_csv(sio)\n\ndef read_clock_file(filename):\n    with open(filename, 'r') as f:\n        lines = f.readlines()\n    for index, line in enumerate(lines):\n        if 'TIME SYSTEM ID' in line:\n            assert(line.strip().split()[0] == 'GPS')\n            continue\n        if 'END OF HEADER' in line:\n            start_index = index + 1\n            break\n    lines = lines[start_index:]\n    SAT, EPOCH, DELTA_TSV = [], [], []\n    for line in lines:\n        if not line.startswith('AS '):\n            continue\n        tokens = line.rstrip().split()\n        sat = tokens[1]\n        epoch = datetime.datetime(year   = int(tokens[2]),\n                                  month  = int(tokens[3]),\n                                  day    = int(tokens[4]),\n                                  hour   = int(tokens[5]),\n                                  minute = int(tokens[6]),\n                                  second = int(float(tokens[7])),\n                                  )\n        if 'D' in tokens[9]:\n            tokens[9] = tokens[9].replace('D', 'E')\n        delta_tsv = float(tokens[9])\n        SAT.append(sat)\n        EPOCH.append(epoch)\n        DELTA_TSV.append(delta_tsv)\n    df = pd.DataFrame({\n        'Epoch'    : EPOCH,\n        'SatName'  : SAT,\n        'DeltaTSV' : DELTA_TSV,\n    })\n    df = df[df['Epoch'] < (df['Epoch'].values[0] + pd.Timedelta(1, unit='day'))]\n    df = df.reset_index(drop=True)\n    return df\n\ndef read_sp3_file(filename):\n    with open(filename, 'r') as f:\n        lines = f.readlines()        \n    for index, line in enumerate(lines):\n        if line.startswith('%c '):\n            time_system = line.split()[3]\n            assert((time_system == 'GPS') or (time_system == 'ccc'))\n            continue\n        if line.startswith('* '):\n            start_index = index\n            break\n    lines = lines[start_index:]\n\n    data = []\n    for line in lines:\n        if line.startswith('* '):\n            tokens = line.rstrip().split()\n            epoch = datetime.datetime(\n                year   = int(tokens[1]),\n                month  = int(tokens[2]),\n                day    = int(tokens[3]),\n                hour   = int(tokens[4]),\n                minute = int(tokens[5]),\n                second = int(float(tokens[6])),\n            )\n        elif line.startswith('P'):\n            tokens = line.rstrip().split()\n            sat = tokens[0][1:]\n            x, y, z, delta_t = [float(s) for s in tokens[1:5]]\n            x = x * 1e+3\n            y = y * 1e+3\n            z = z * 1e+3\n            delta_t = delta_t * 1e-6\n            data.append([epoch, sat, x, y, z, delta_t])\n    columns = ['Epoch', 'SatName', 'X', 'Y', 'Z', 'DeltaTSV_SP3']\n    df = pd.DataFrame(data, columns=columns)\n    df = df[df['Epoch'] < (df['Epoch'].values[0] + pd.Timedelta(1, unit='day'))]\n    df = df[~((df['X'] == 0) & (df['Y'] == 0) & (df['Z'] == 0))]\n    df = df.reset_index(drop=True)\n    return df\n\ndef read_SINEX_TRO_file(filename):\n    with open(filename, 'r') as f:\n        lines = f.readlines()\n    for index, line in enumerate(lines):\n        if '+TROP\/SOLUTION' in line:\n            start_index = index + 2\n            break\n    lines = lines[start_index:]\n    data = []\n    for line in lines:\n        if '-TROP\/SOLUTION' in line:\n            break\n        tokens  = line.strip().split()\n        y, d, s = [int(x) for x in tokens[1].split(':')]\n        epoch = datetime.datetime(y+2000, 1, 1) + datetime.timedelta(days=d-1) + datetime.timedelta(seconds=s)\n        data.append([epoch] + [1e-3 * float(x) for x in tokens[2:]])\n    columns = ['Epoch',\n               'TROTOT', 'TROTOT_STD',\n               'TGNTOT', 'TGNTOT_STD',\n               'TGETOT', 'TGETOT_STD']\n    df = pd.DataFrame(data, columns=columns)\n    df = df[df['Epoch'] < (df['Epoch'].values[0] + pd.Timedelta(1, unit='day'))]\n    df = df.reset_index(drop=True)\n    return df\n\ndef read_IONEX_file(filename):\n    with open(filename, 'r') as f:\n        lines = f.readlines()\n    kw = dict()\n    #==============================\n    # read header\n    #==============================\n    for index, line in enumerate(lines):\n        tokens = line.strip().split()\n        if 'EPOCH OF FIRST MAP' in line:\n            kw['time_1'] = np.datetime64(datetime.datetime(\n                year   = int(tokens[0]),\n                month  = int(tokens[1]),\n                day    = int(tokens[2]),\n                hour   = int(tokens[3]),\n                minute = int(tokens[4]),\n                second = int(tokens[5]),\n            ))\n            continue\n        if 'EPOCH OF LAST MAP' in line:\n            kw['time_2'] = np.datetime64(datetime.datetime(\n                year   = int(tokens[0]),\n                month  = int(tokens[1]),\n                day    = int(tokens[2]),\n                hour   = int(tokens[3]),\n                minute = int(tokens[4]),\n                second = int(tokens[5]),                \n            ))\n            continue\n        if 'INTERVAL' in line:\n            kw['time_delta'] = np.timedelta64(datetime.timedelta(\n                seconds=int(tokens[0]),\n            ))\n            continue\n        if 'HGT1 \/ HGT2 \/ DHGT' in line:\n            h1, h2, dh = [float(x) for x in tokens[0:3]]\n            assert(h1 == h2)\n            assert(dh == 0.0)\n            kw['iono_height'] = h1 * 1000\n            continue\n        if 'LAT1 \/ LAT2 \/ DLAT' in line:\n            lat_1, lat_2, lat_delta = [float(x) for x in tokens[0:3]]\n            assert((lat_2 - lat_1) * lat_delta > 0)\n            if (lat_1 > lat_2):\n                flip_lat = True\n                lat_1, lat_2 = lat_2, lat_1\n                lat_delta = - lat_delta\n            else:\n                flip_lat = False\n            kw['lat_1']     = np.deg2rad(lat_1)\n            kw['lat_2']     = np.deg2rad(lat_2)\n            kw['lat_delta'] = np.deg2rad(lat_delta)\n            continue\n        if 'LON1 \/ LON2 \/ DLON' in line:\n            lng_1, lng_2, lng_delta = [float(x) for x in tokens[0:3]]\n            assert((lng_2 - lng_1) * lng_delta > 0)\n            if (lng_1 > lng_2):\n                flip_lng = True\n                lng_1, lng_2 = lng_2, lng_1\n                lng_delta = - lng_delta\n            else:\n                flip_lng = False\n            kw['lng_1']     = np.deg2rad(lng_1)\n            kw['lng_2']     = np.deg2rad(lng_2)\n            kw['lng_delta'] = np.deg2rad(lng_delta)\n            continue\n        if 'MAPPING FUNCTION' in line:\n            assert(tokens[0] == 'COSZ')\n            continue\n        if 'BASE RADIUS' in line:\n            kw['base_radius'] = 1000 * float(tokens[0])\n            continue\n        if 'EXPONENT' in line:\n            TEC_coeff = 10**float(tokens[0])\n            continue\n        if 'MAP DIMENSION' in line:\n            assert(int(tokens[0]) == 2)\n            continue\n        if 'END OF HEADER' in line:\n            line_count = index + 1\n            break\n    #==============================\n    # read data\n    #==============================\n    roundint = lambda x : int(round(x))\n    N_lat  = 1 + roundint((kw['lat_2'] - kw['lat_1']) \/ kw['lat_delta'])\n    N_lng  = 1 + roundint((kw['lng_2'] - kw['lng_1']) \/ kw['lng_delta'])\n    N_time = 1 + roundint((kw['time_2'] - kw['time_1']) \/ kw['time_delta'])\n    iono_map = np.zeros((N_time, N_lat, N_lng), dtype=np.float64)\n\n    data_per_line = 16\n    lines_per_data = (N_lng + data_per_line - 1) \/\/ data_per_line\n    \n    for time_count in range(N_time):\n        assert('START OF TEC MAP' in lines[line_count])\n        assert(int(lines[line_count].strip().split()[0]) == time_count + 1)\n        line_count += 1\n        assert('EPOCH OF CURRENT MAP' in lines[line_count])\n        line_count += 1\n        for lat_count in range(N_lat):\n            assert('LAT\/LON1\/LON2\/DLON\/H' in lines[line_count])\n            line_count += 1\n            values = []\n            for i in range(lines_per_data):\n                values.extend([int(x) for x in lines[line_count+i].strip().split()])\n            if 9999 in values:\n                print('Warning: There is non-available TEC values.')\n            iono_map[time_count, lat_count, :] = np.array(values).astype(float)\n            line_count += lines_per_data\n        assert('END OF TEC MAP' in lines[line_count])\n        assert(int(lines[line_count].strip().split()[0]) == time_count + 1)\n        line_count += 1\n\n    if flip_lat:\n        iono_map = np.flip(iono_map, axis=1)\n    if flip_lng:\n        iono_map = np.flip(iono_map, axis=2)\n    iono_map = iono_map * TEC_coeff\n    kw['iono_map']  = iono_map\n    kw['lat_range'] = np.linspace(kw['lat_1'], kw['lat_2'], N_lat)\n    kw['lng_range'] = np.linspace(kw['lng_1'], kw['lng_2'], N_lng)\n    return IONEX(**kw)","594e0eae":"%%writefile transform.py\nimport numpy as np\nfrom dataclasses import dataclass\n\nimport constants as C\n\n@dataclass\nclass ECEF:\n    x: np.array\n    y: np.array\n    z: np.array\n\n    def to_numpy(self):\n        return np.stack([self.x, self.y, self.z], axis=0)\n\n    @staticmethod\n    def from_numpy(pos):\n        x, y, z = [np.squeeze(w) for w in np.split(pos, 3, axis=-1)]\n        return ECEF(x=x, y=y, z=z)\n\n@dataclass\nclass BLH:\n    lat : np.array\n    lng : np.array\n    hgt : np.array\n\n@dataclass\nclass ENU:\n    east  : np.array\n    north : np.array\n    up    : np.array\n\n@dataclass\nclass AZEL:\n    elevation : np.array\n    azimuth   : np.array\n    zenith    : np.array\n\ndef BLH_to_ECEF(blh):\n    a  = C.WGS84_SEMI_MAJOR_AXIS\n    e2 = C.WGS84_SQUARED_FIRST_ECCENTRICITY\n    sin_B = np.sin(blh.lat)\n    cos_B = np.cos(blh.lat)\n    sin_L = np.sin(blh.lng)\n    cos_L = np.cos(blh.lng)\n    n = a \/ np.sqrt(1 - e2*sin_B**2)\n    x = (n + blh.hgt) * cos_B * cos_L\n    y = (n + blh.hgt) * cos_B * sin_L\n    z = ((1 - e2) * n + blh.hgt) * sin_B\n    return ECEF(x=x, y=y, z=z)\n\ndef ECEF_to_BLH_approximate(ecef):\n    a = C.WGS84_SEMI_MAJOR_AXIS\n    b = C.WGS84_SEMI_MINOR_AXIS\n    e2  = C.WGS84_SQUARED_FIRST_ECCENTRICITY\n    e2_ = C.WGS84_SQUARED_SECOND_ECCENTRICITY\n    x = ecef.x\n    y = ecef.y\n    z = ecef.z\n    r = np.sqrt(x**2 + y**2)\n    t = np.arctan2(z * (a\/b), r)\n    B = np.arctan2(z + (e2_*b)*np.sin(t)**3, r - (e2*a)*np.cos(t)**3)\n    L = np.arctan2(y, x)\n    n = a \/ np.sqrt(1 - e2*np.sin(B)**2)\n    H = (r \/ np.cos(B)) - n\n    return BLH(lat=B, lng=L, hgt=H)\n\nECEF_to_BLH = ECEF_to_BLH_approximate\n\ndef ECEF_to_ENU(pos, base):\n    dx = pos.x - base.x\n    dy = pos.y - base.y\n    dz = pos.z - base.z\n    base_blh = ECEF_to_BLH(base)\n    sin_B = np.sin(base_blh.lat)\n    cos_B = np.cos(base_blh.lat)\n    sin_L = np.sin(base_blh.lng)\n    cos_L = np.cos(base_blh.lng)\n    e = -sin_L*dx + cos_L*dy\n    n = -sin_B*cos_L*dx - sin_B*sin_L*dy + cos_B*dz\n    u =  cos_B*cos_L*dx + cos_B*sin_L*dy + sin_B*dz\n    return ENU(east=e, north=n, up=u)\n\ndef ENU_to_AZEL(enu):\n    e = enu.east\n    n = enu.north\n    u = enu.up\n    elevation = np.arctan2(u, np.sqrt(e**2 + n**2))\n    azimuth   = np.arctan2(e, n)\n    zenith    = (0.5 * np.pi) - elevation\n    return AZEL(elevation=elevation,\n                azimuth=azimuth,\n                zenith=zenith)\n\ndef ECEF_to_AZEL(pos, base):\n    return ENU_to_AZEL(ECEF_to_ENU(pos, base))\n\ndef haversine_distance(blh_1, blh_2):\n    dlat = blh_2.lat - blh_1.lat\n    dlng = blh_2.lng - blh_1.lng\n    a = np.sin(dlat\/2)**2 + np.cos(blh_1.lat) * np.cos(blh_2.lat) * np.sin(dlng\/2)**2\n    dist = 2 * C.HAVERSINE_RADIUS * np.arcsin(np.sqrt(a))\n    return dist\n\ndef hubenys_distance(blh_1, blh_2):\n    Rx = C.WGS84_SEMI_MAJOR_AXIS\n    Ry = C.WGS84_SEMI_MINOR_AXIS\n    E2 = C.WGS84_SQUARED_FIRST_ECCENTRICITY\n    num_M = Rx * (1 - E2)\n    Dy = blh_1.lat - blh_2.lat\n    Dx = blh_1.lng - blh_2.lng\n    P  = 0.5 * (blh_1.lat + blh_2.lat)\n    W  = np.sqrt(1 - E2 * np.sin(P)**2)\n    M  = num_M \/ W**3\n    N  = Rx \/ W\n    d2 = (Dy * M)**2 + (Dx * N * np.cos(P))**2\n    d  = np.sqrt(d2)\n    return d\n\ndef jacobian_BLH_to_ECEF(blh):\n    a  = C.WGS84_SEMI_MAJOR_AXIS\n    e2 = C.WGS84_SQUARED_FIRST_ECCENTRICITY\n    B = blh.lat\n    L = blh.lng\n    H = blh.hgt\n    cos_B = np.cos(B)\n    sin_B = np.sin(B)\n    cos_L = np.cos(L)\n    sin_L = np.sin(L)\n    N = a \/ np.sqrt(1 - e2*sin_B**2)\n    dNdB = a * e2 * sin_B * cos_B * (1 - e2*sin_B**2)**(-3\/2)\n    N_plus_H = N + H\n    cos_B_cos_L = cos_B * cos_L\n    cos_B_sin_L = cos_B * sin_L\n    sin_B_cos_L = sin_B * cos_L\n    sin_B_sin_L = sin_B * sin_L\n\n    dXdB = dNdB*cos_B_cos_L - N_plus_H*sin_B_cos_L\n    dYdB = dNdB*cos_B_sin_L - N_plus_H*sin_B_sin_L\n    dZdB = (1-e2)*dNdB*sin_B + (1-e2)*N_plus_H*cos_B\n\n    dXdL = - N_plus_H * cos_B_sin_L\n    dYdL =   N_plus_H * cos_B_cos_L\n    dZdL = np.zeros_like(dXdL)\n\n    dXdH = cos_B_cos_L\n    dYdH = cos_B_sin_L\n    dZdH = sin_B\n\n    J = np.stack([[dXdB, dXdL, dXdH],\n                  [dYdB, dYdL, dYdH],\n                  [dZdB, dZdL, dZdH]], axis=0)\n    axes = list(range(2, J.ndim)) + [0, 1]\n    J = np.transpose(J, axes)\n    return J\n\ndef jacobian_ECEF_to_ENU(blh):\n    B = blh.lat\n    L = blh.lng\n    cos_B = np.cos(B)\n    sin_B = np.sin(B)\n    cos_L = np.cos(L)\n    sin_L = np.sin(L)\n    \n    dEdX = -sin_L\n    dEdY =  cos_L\n    dEdZ = np.zeros_like(dEdX)\n    \n    dNdX = -sin_B*cos_L\n    dNdY = -sin_B*sin_L\n    dNdZ =  cos_B\n\n    dUdX = cos_B*cos_L\n    dUdY = cos_B*sin_L\n    dUdZ = sin_B\n\n    J = np.stack([[dEdX, dEdY, dEdZ],\n                  [dNdX, dNdY, dNdZ],\n                  [dUdX, dUdY, dUdZ]], axis=0)\n    axes = list(range(2, J.ndim)) + [0, 1]\n    J = np.transpose(J, axes)\n    return J\n\ndef pd_haversine_distance(df1, df2):\n    blh1 = BLH(\n        lat=np.deg2rad(df1['latDeg'].values),\n        lng=np.deg2rad(df1['lngDeg'].values),\n        hgt=0,\n    )\n    blh2 = BLH(\n        lat=np.deg2rad(df2['latDeg'].values),\n        lng=np.deg2rad(df2['lngDeg'].values),\n        hgt=0,\n    )\n    return haversine_distance(blh1, blh2)","db836470":"%%writefile urls.py\nimport os\nimport datetime\n\nfrom constants import GPS_ORIGIN_DAY\n\ndef clock_url(d):\n    gps_week_number = (d - GPS_ORIGIN_DAY).days \/\/ 7\n    days_of_year    = (d - datetime.date(d.year, 1, 1)).days + 1\n    dirname = f'\/gnss\/products\/mgex\/{gps_week_number:04d}'\n    if gps_week_number >= 2113:\n        clk_filename = f'COD0MGXFIN_{d.year:04d}{days_of_year:03d}0000_01D_30S_CLK.CLK.gz'\n    else:\n        clk_filename = f'GRG0MGXFIN_{d.year:04d}{days_of_year:03d}0000_01D_30S_CLK.CLK.gz'\n    return (dirname, clk_filename)\n\ndef orbit_url(d):\n    gps_week_number = (d - GPS_ORIGIN_DAY).days \/\/ 7\n    days_of_year    = (d - datetime.date(d.year, 1, 1)).days + 1\n    dirname = f'\/gnss\/products\/mgex\/{gps_week_number:04d}'\n    if gps_week_number >= 2113:\n        sp3_filename = f'COD0MGXFIN_{d.year:04d}{days_of_year:03d}0000_01D_05M_ORB.SP3.gz'\n    else:\n        sp3_filename = f'GRG0MGXFIN_{d.year:04d}{days_of_year:03d}0000_01D_15M_ORB.SP3.gz'\n    return (dirname, sp3_filename)\n\ndef ionosphere_url(d):\n    year  = d.year\n    year2 = year % 100\n    days_of_year = (d - datetime.date(year, 1, 1)).days + 1\n    dirname  = f'\/gnss\/products\/ionex\/{year:04d}\/{days_of_year:03d}'\n    filename = f'igsg{days_of_year:03d}0.{year2}i.Z'\n    return (dirname, filename)\n\ndef troposphere_url(d):\n    year  = d.year\n    year2 = year % 100\n    days_of_year = (d - datetime.date(year, 1, 1)).days + 1\n    dirname  = f'\/gnss\/products\/troposphere\/zpd\/{year:04d}\/{days_of_year:03d}'\n    filename = f'stfu{days_of_year:03d}0.{year2:02d}zpd.gz'\n    return (dirname, filename)\n\ndef clock_filename(root, d):\n    filename = os.path.splitext(clock_url(d)[1])[0]\n    return os.path.join(root, filename)\n\ndef orbit_filename(root, d):\n    filename = os.path.splitext(orbit_url(d)[1])[0]\n    return os.path.join(root, filename)\n\ndef ionosphere_filename(root, d):\n    filename = os.path.splitext(ionosphere_url(d)[1])[0]\n    return os.path.join(root, filename)\n\ndef troposphere_filename(root, d):\n    filename = os.path.splitext(troposphere_url(d)[1])[0]\n    return os.path.join(root, filename)","8aeee4a8":"%%writefile delay_model.py\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import RectBivariateSpline\n\nimport transform\n\ndef calc_iono_delay(ionex, epoch, carrier_freq, sat_azel, rec_blh):\n    assert(epoch >= ionex.time_1)\n    assert(epoch <= ionex.time_2)\n\n    th_el = sat_azel.elevation\n    th_az = sat_azel.azimuth\n    \n    R = ionex.base_radius\n    H = ionex.iono_height\n    z = np.arcsin((R \/ (R + H)) * np.cos(th_el))\n    F = 1 \/ np.cos(z)\n    \n    psi = (0.5 * np.pi) - th_el - z\n    sin_psi = np.sin(psi)\n    cos_psi = np.cos(psi)\n    sin_az  = np.sin(th_az)\n    cos_az  = np.cos(th_az)\n\n    sin_lat = np.sin(rec_blh.lat)\n    cos_lat = np.cos(rec_blh.lat)\n    \n    IPP_lat = np.arcsin( sin_lat*cos_psi + cos_lat*sin_psi*cos_az )\n    IPP_lng = rec_blh.lng + np.arcsin( sin_psi * sin_az \/ np.cos(IPP_lat) )\n\n    index_0 = (epoch - ionex.time_1) \/\/ ionex.time_delta\n    index_1 = index_0 + 1\n    T0 = ionex.time_1 + ionex.time_delta * index_0\n    T1 = T0 + ionex.time_delta\n    alpha = (epoch - T0) \/ ionex.time_delta\n    assert((0.0 <= alpha) and (alpha < 1.0))\n\n    omega = np.pi \/ (12 * 3600)\n    IPP_lng_0 = IPP_lng + omega * (epoch - T0).astype('timedelta64[s]').astype(int)\n    IPP_lng_1 = IPP_lng + omega * (epoch - T1).astype('timedelta64[s]').astype(int)\n\n    lat_range = ionex.lat_range\n    lng_range = ionex.lng_range\n    iono_map  = ionex.iono_map\n    E_0 = RectBivariateSpline(lat_range, lng_range, iono_map[index_0, :, :], kx=1, ky=1, s=0)(IPP_lat, IPP_lng_0, grid=False)\n    E_1 = RectBivariateSpline(lat_range, lng_range, iono_map[index_1, :, :], kx=1, ky=1, s=0)(IPP_lat, IPP_lng_1, grid=False)\n    E_t = ((1.0 - alpha) * E_0) + (alpha * E_1)\n    iono_delay = F * ((40.3 * 1e+16) \/ carrier_freq**2) * E_t\n    return iono_delay\n\ndef calc_tropo_delay(sinex, epoch, sat_azel):\n    th_el = sat_azel.elevation\n    th_az = sat_azel.azimuth\n\n    TIME = sinex['Epoch'].values\n    time_delta = TIME[1] - TIME[0]\n    index_0 = (epoch - TIME[0]) \/\/ time_delta\n    index_1 = index_0 + 1\n    alpha = (epoch - TIME[index_0]) \/ time_delta\n    assert((0.0 <= alpha) and (alpha < 1.0))\n\n    zpd_0 = sinex['TROTOT'].values[index_0]\n    zpd_1 = sinex['TROTOT'].values[index_1]\n    grad_n_0 = sinex['TGNTOT'].values[index_0]\n    grad_n_1 = sinex['TGNTOT'].values[index_1]\n    grad_e_0 = sinex['TGETOT'].values[index_0]\n    grad_e_1 = sinex['TGETOT'].values[index_1]\n\n    cos_az = np.cos(th_az)\n    sin_az = np.sin(th_az)\n    zpd_0  = zpd_0 + (grad_n_0 * cos_az) + (grad_e_0 * sin_az)\n    zpd_1  = zpd_1 + (grad_n_1 * cos_az) + (grad_e_1 * sin_az)\n\n    zpd_t = ((1 - alpha) * zpd_0) + (alpha * zpd_1)\n    M = 1.001 \/ np.sqrt(0.002001 + np.sin(th_el)**2)\n    return M * zpd_t","103917cc":"%%writefile carrier_smoothing.py\nimport numpy as np\nimport pandas as pd\nimport scipy.sparse\nimport scipy.sparse.linalg\n\nimport constants as C\n\ndef bit_check(X, name):\n    mask = 2**C.ACCUMULATED_DELTA_RANGE_STATE_BIT_INT_MAP[name]\n    return (X & mask) != 0\n\ndef do_carrier_smoothing(gnss_df, beta=0.1):\n    output_df_list = []\n    t_ref = gnss_df['millisSinceGpsEpoch'].min()\n    for (sat_name, sig_type), tmp_df in gnss_df.groupby(['SatName', 'SignalType']):\n        time = 1e-3 * (tmp_df['millisSinceGpsEpoch'] - t_ref)\n    \n        clock_bias_ref  = tmp_df['FullBiasNanos'].values[0]\n        clock_bias      = (tmp_df['FullBiasNanos'] - clock_bias_ref).values + tmp_df['BiasNanos'].values\n        clock_drift_m   = (C.LIGHT_SPEED * 1e-9) * np.diff(clock_bias)\n\n        raw_pseudo_range = tmp_df['rawPrM'].values\n\n        accumurated_delta_range = tmp_df['AccumulatedDeltaRangeMeters'].values\n        delta_range = np.diff(accumurated_delta_range) - clock_drift_m\n\n        can_compare_to_next = bit_check(tmp_df['AccumulatedDeltaRangeState'].values, 'VALID')\n        can_compare_to_prev = ( bit_check(tmp_df['AccumulatedDeltaRangeState'].values, 'VALID')\n                                & (~bit_check(tmp_df['AccumulatedDeltaRangeState'].values, 'RESET'))\n                                & (~bit_check(tmp_df['AccumulatedDeltaRangeState'].values, 'CYCLE_SLIP'))\n                               )\n        delta_range_valid = can_compare_to_next[0:-1] & can_compare_to_prev[1:]\n        delta_range_valid = delta_range_valid & (np.diff(time) < 2.0)\n\n        N = time.shape[0]\n        A = scipy.sparse.eye(N)\n        diag_B = delta_range_valid.astype(float) * np.full(N-1, beta**(-2))\n        B = scipy.sparse.spdiags(diag_B, [0], N-1, N-1)\n        D = scipy.sparse.spdiags(np.stack([-np.ones(N), np.ones(N)]), [0, 1], N-1, N)\n        Q = A + (D.T @ B @ D)\n        c = (A @ raw_pseudo_range) + (D.T @ (B @ delta_range))\n        tmp_df['smoothedPrM'] = scipy.sparse.linalg.spsolve(Q, c)\n        output_df_list.append(tmp_df)\n    output_df = pd.concat(output_df_list, axis=0)\n    output_df.sort_values(['millisSinceGpsEpoch', 'SatName', 'SignalType'], inplace=True, ignore_index=True)\n    return output_df","d6db2fdf":"%%writefile smooth_height.py\nimport numpy as np\nimport scipy.sparse\nimport scipy.sparse.linalg\n\ndef calc_C(time_x, time_y, dt):\n    N_x = time_x.shape[0]\n    N_y = time_y.shape[0]\n    x_index = np.floor(time_y \/ dt).astype(int)\n    alpha   = (time_y \/ dt) - x_index\n    C = np.empty(shape=(N_y, N_x), dtype=np.object)\n    for i_x in range(N_x):\n        C[0, i_x] = 0\n    for i_y in range(N_y):\n        i_x = x_index[i_y]\n        C[i_y, i_x] = alpha[i_y]\n        if i_x < N_x - 1:\n            C[i_y, i_x+1] = 1.0 - alpha[i_y]\n    C = scipy.sparse.bmat(C, format='csr')\n    return C\n\ndef smooth_height_once(time_y, height_y, T_max, beta, dt):\n    N_x   = int(np.ceil(T_max \/ dt) + 1)\n    N_y   = time_y.shape[0]\n    time_x  = dt * np.arange(N_x)\n    C = calc_C(time_x, time_y, dt)\n    B = scipy.sparse.spdiags(np.full(N_x-1, beta), [0], N_x-1, N_x-1)\n    D = scipy.sparse.spdiags(np.stack([-np.ones(N_x), np.ones(N_x)]), [0, 1], N_x-1, N_x)\n    Q = (C.T @ C) + (D.T @ B @ D)\n    c = C.T @ height_y\n    height_x = scipy.sparse.linalg.spsolve(Q, c)\n    height_y_star = C @ height_x\n    sol = dict()\n    sol['time_x']   = time_x\n    sol['height_x'] = height_x\n    sol['height_y_star'] = height_y_star\n    return sol\n\ndef smooth_height(time_pred, height_pred, valid, T_max, beta, dt):\n    time_y   = time_pred[valid].copy()\n    height_y = height_pred[valid].copy()\n    for loop in range(2):\n        sol = smooth_height_once(time_y, height_y, T_max=T_max, beta=beta, dt=dt)\n        valid = np.abs(height_y - sol['height_y_star']) < 30\n        time_y   = time_y[valid].copy()\n        height_y = height_y[valid].copy()\n    C = calc_C(sol['time_x'], time_pred, dt)\n    return C @ sol['height_x']","16bee5f9":"%%writefile preprocess.py\nimport sys\nimport os\nimport glob\nimport itertools\nimport traceback\nimport multiprocessing\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom scipy.interpolate import InterpolatedUnivariateSpline\n\nimport io_f\nimport urls\nimport constants as C\n\nINPUT_PATH    = '..\/input\/google-smartphone-decimeter-challenge'\nSAT_DATA_PATH = '..\/input\/gsdc-merged-clk-and-sp3'\nDEST_PATH     = '_features'\n\nCONSTE_ID_GPS = C.CONSTELLATION_TYPE_MAP['GPS']\nCONSTE_ID_GLO = C.CONSTELLATION_TYPE_MAP['GLONASS']\nCONSTE_ID_GAL = C.CONSTELLATION_TYPE_MAP['GALILEO']\nCONSTE_ID_QZS = C.CONSTELLATION_TYPE_MAP['QZSS']\nCONSTE_ID_BDS = C.CONSTELLATION_TYPE_MAP['BEIDOU']\n\ndef add_extra_features_1st(key, gnss_df, log):\n    nanosSinceGpsEpoch = gnss_df['TimeNanos'] - gnss_df['FullBiasNanos']\n    gnss_df['millisSinceGpsEpoch'] = nanosSinceGpsEpoch \/\/ 10**6\n    gnss_df['Epoch'] = pd.to_timedelta(nanosSinceGpsEpoch, unit='ns') + C.GPS_ORIGIN_DATETIME\n\n    N = gnss_df.shape[0]\n    SatName    = np.empty(shape=(N, ), dtype=np.object)\n    SignalType = np.empty(shape=(N, ), dtype=np.object)\n    FixedSvid  = np.empty(shape=(N, ), dtype=np.int32)\n    columns = ['ConstellationType', 'Svid', 'CarrierFrequencyHz']\n    for index, (conste, svid, freq) in enumerate(gnss_df[columns].itertuples(index=False, name=None)):\n        if (conste == CONSTE_ID_GPS) and abs(freq - C.FREQ_GPS_L1) < C.FREQ_TOL:\n            sat_name_prefix = C.SYSTEM_NAME_MAP['GPS']\n            signal_type = 'GPS_L1'\n            assert((1 <= svid) and (svid <= 32))\n            fixed_svid = svid\n        elif (conste == CONSTE_ID_GPS) and abs(freq - C.FREQ_GPS_L5) < C.FREQ_TOL:\n            sat_name_prefix = C.SYSTEM_NAME_MAP['GPS']\n            signal_type = 'GPS_L5'\n            assert((1 <= svid) and (svid <= 32))\n            fixed_svid = svid\n        elif (conste == CONSTE_ID_GAL) and abs(freq - C.FREQ_GAL_E1) < C.FREQ_TOL:\n            sat_name_prefix = C.SYSTEM_NAME_MAP['GALILEO']\n            signal_type = 'GAL_E1'\n            assert((1 <= svid) and (svid <= 36))\n            fixed_svid = svid\n        elif (conste == CONSTE_ID_GAL) and abs(freq - C.FREQ_GAL_E5A) < C.FREQ_TOL:\n            sat_name_prefix = C.SYSTEM_NAME_MAP['GALILEO']\n            signal_type = 'GAL_E5A'\n            assert((1 <= svid) and (svid <= 36))\n            fixed_svid = svid\n        elif (conste == CONSTE_ID_BDS) and abs(freq - C.FREQ_BDS_B1I) < C.FREQ_TOL:\n            sat_name_prefix = C.SYSTEM_NAME_MAP['BEIDOU']\n            signal_type = 'BDS_B1I'\n            assert((1 <= svid) and (svid <= 61))\n            fixed_svid = svid\n        elif (conste == CONSTE_ID_QZS) and abs(freq - C.FREQ_QZS_J1) < C.FREQ_TOL:\n            sat_name_prefix = C.SYSTEM_NAME_MAP['QZSS']\n            signal_type = 'QZS_J1'\n            fixed_svid = C.QZSS_PRN_SVID_MAP[svid]\n        elif (conste == CONSTE_ID_QZS) and abs(freq - C.FREQ_QZS_J5) < C.FREQ_TOL:\n            sat_name_prefix = C.SYSTEM_NAME_MAP['QZSS']\n            signal_type = 'QZS_J5'\n            fixed_svid = C.QZSS_PRN_SVID_MAP[svid]\n        elif (conste == CONSTE_ID_GLO):\n            sat_name_prefix = C.SYSTEM_NAME_MAP['GLONASS']\n            signal_type = 'GLO_G1'\n            if svid > 24:\n                freq_channel = svid - 100\n                fixed_svid   = C.GLONASS_FCN_OSN_MAP[key][str(svid)]\n            else:\n                freq_channel = C.GLONASS_FREQ_CHANNEL_MAP[svid]\n                fixed_svid   = svid\n            assert( (-7 <= freq_channel) and (freq_channel <= 6) )\n            freq_nominal = C.FREQ_GLO_G1_NOMINAL + freq_channel * C.FREQ_GLO_G1_DELTA\n            assert( abs(freq - freq_nominal) < C.FREQ_TOL )\n        else:\n            print((conste, svid, freq))\n            raise RuntimeError('unknown signal type')\n        SatName[index]    = f'{sat_name_prefix}{fixed_svid:02d}'\n        SignalType[index] = signal_type\n        FixedSvid[index]  = fixed_svid\n        del sat_name_prefix, signal_type, fixed_svid\n        pass\n    gnss_df['SatName']    = SatName\n    gnss_df['SignalType'] = SignalType\n    gnss_df['FixedSvid']  = FixedSvid\n    return gnss_df\n\ndef bit_check(X, name):\n    mask = 2**C.RAW_STATE_BIT_INV_MAP[name]\n    return (X & mask) != 0\n\ndef remove_invalid_measurements(df, log):\n    state    = df['State'].values\n    sig_type = df['SignalType'].values\n    conste   = df['ConstellationType'].values\n\n    is_gal_E1     = (sig_type == 'GAL_E1')\n    is_non_gal_E1 = np.logical_not(is_gal_E1)\n    code_lock_0   = is_non_gal_E1 & bit_check(state, 'Code Lock')\n    code_lock_1   = is_gal_E1     & bit_check(state, 'Galileo E1BC Code Lock')\n    code_lock_ok  = code_lock_0 | code_lock_1\n\n    is_glo     = (conste == C.CONSTELLATION_TYPE_MAP['GLONASS'])\n    is_nonglo  = np.logical_not(is_glo)\n    time_of_0  = is_nonglo & bit_check(state, 'Time Of Week Decoded State') & bit_check(state, 'Time Of Week Known')\n    time_of_1  = is_glo & bit_check(state, 'GLONASS Time Of Day Decoded') & bit_check(state, 'GLONASS Time Of Day Known')\n    time_of_ok = time_of_0 | time_of_1\n\n    msec_ambi_ok     = np.logical_not(bit_check(state, 'Millisecond Ambiguity'))\n    sigma_rectime_ok = (df['ReceivedSvTimeUncertaintyNanos'] <= C.ReceivedSvTimeUncertaintyNanos_THRESHOLD).values\n    cn0_ok           = (df['Cn0DbHz'] >= C.Cn0DbHz_THRESHOLD).values\n    svid_ok          = (df['FixedSvid'] >= 1).values\n\n    valid = (code_lock_ok\n             & time_of_ok\n             & msec_ambi_ok\n             & sigma_rectime_ok\n             & cn0_ok\n             & svid_ok)\n    df = df[valid]\n    df = df.reset_index(drop=True)\n    return df\n\ndef calc_rawPrM_GPS(df):\n    week_number  = (df['Epoch'].dt.date - C.GPS_ORIGIN_DAY).astype('timedelta64[D]').astype(np.int64) \/\/ 7\n    offset_nanos = week_number * 7 * 24 * 3600 * 10**9\n    dt_nanos = (df['TimeNanos'] - df['FullBiasNanos'] - offset_nanos - df['ReceivedSvTimeNanos'])\n    dt_nanos = dt_nanos + (dt_nanos < 0).astype(int) * 7 * 24 * 3600 * 10**9\n    dt_nanos = dt_nanos - df['BiasNanos']\n    return (C.LIGHT_SPEED * 1e-9) * dt_nanos.values\n\ndef calc_rawPrM_BDS(df):\n    week_number  = (df['Epoch'].dt.date - C.GPS_ORIGIN_DAY).astype('timedelta64[D]').astype(np.int64) \/\/ 7\n    offset_nanos = (week_number * 7 * 24 * 3600 + C.BEIDOU_LEAP_SECONDS) * 10**9\n    dt_nanos = (df['TimeNanos'] - df['FullBiasNanos'] - offset_nanos - df['ReceivedSvTimeNanos'])\n    dt_nanos = dt_nanos + (dt_nanos < 0).astype(int) * 7 * 24 * 3600 * 10**9\n    dt_nanos = dt_nanos - df['BiasNanos']\n    return (C.LIGHT_SPEED * 1e-9) * dt_nanos.values\n\ndef calc_rawPrM_GLO(df):\n    msk_day = pd.to_datetime(df['Epoch'], utc=True).dt.tz_convert(C.TZ_MSK).dt.date\n    msk_days_from_gps_origin = (msk_day - C.GPS_ORIGIN_DAY).astype('timedelta64[D]').astype(np.int64)\n    offset_nanos = ((msk_days_from_gps_origin * 24 - 3) * 3600 + C.GLONASS_LEAP_SECONDS) * 10**9\n    dt_nanos = (df['TimeNanos'] - df['FullBiasNanos'] - offset_nanos - df['ReceivedSvTimeNanos'])\n    dt_nanos = dt_nanos + (dt_nanos < 0).astype(int) * 24 * 3600 * 10**9\n    dt_nanos = dt_nanos - df['BiasNanos']\n    return (C.LIGHT_SPEED * 1e-9) * dt_nanos.values\n\ndef add_raw_pseudo_range(df, log):\n    is_GLO = (df['ConstellationType'] == CONSTE_ID_GLO)\n    is_BDS = (df['ConstellationType'] == CONSTE_ID_BDS)\n    df_GLO = df[is_GLO].copy()\n    df_BDS = df[is_BDS].copy()\n    df_GPS = df[~(is_GLO | is_BDS)].copy()\n    df_GPS['rawPrM'] = calc_rawPrM_GPS(df_GPS)\n    df_BDS['rawPrM'] = calc_rawPrM_BDS(df_BDS)\n    df_GLO['rawPrM'] = calc_rawPrM_GLO(df_GLO)\n    df = pd.concat([df_GPS, df_BDS, df_GLO], axis=0)\n    df = df[df['rawPrM'] < C.RAW_PSEUDO_RANGE_THRESHOLD]\n    df = df.sort_values(['millisSinceGpsEpoch', 'SatName', 'SignalType'])\n    df = df.reset_index(drop=True)\n    return df\n\ndef add_clock_bias_meter(raw_df, derived_df, log):\n    output_df_list = []\n    for (sat_name, sig_type), raw_sat_df in raw_df.groupby(['SatName', 'SignalType']):\n        svid = int(sat_name[1:])\n        derived_sat_df = derived_df[ (derived_df['signalType'] == sig_type) & (derived_df['svid'] == svid) ]\n        if derived_sat_df.shape[0] <= 1:\n            log.append(f'{sat_name} not in derived.csv.')\n            continue\n        t_ref = derived_sat_df['millisSinceGpsEpoch'].min()\n        X_in  = 1e-3 * (derived_sat_df['millisSinceGpsEpoch'] - t_ref).values\n        Y_in  = derived_sat_df['satClkBiasM'].values\n        X_out = 1e-3 * (raw_sat_df['millisSinceGpsEpoch'] - t_ref).values\n        Y_out = InterpolatedUnivariateSpline(X_in, Y_in, k=1, ext=3)(X_out)\n        raw_sat_df['satClkBiasM'] = Y_out\n        output_df_list.append(raw_sat_df)\n    output_df = pd.concat(output_df_list, axis=0)\n    output_df.sort_values(['millisSinceGpsEpoch', 'SatName', 'SignalType'], inplace=True, ignore_index=True)\n    return output_df\n\ndef add_satellite_orbit(gnss_df, log):\n    sp3_days = np.unique([(gnss_df['Epoch'].min() - C.ORBIT_TIME_MARGIN).date(),\n                          (gnss_df['Epoch'].max() + C.ORBIT_TIME_MARGIN).date()])\n    sp3_filelist = [os.path.join(SAT_DATA_PATH, d.strftime('SP3_%Y%m%d.csv')) for d in sp3_days]\n    sp3_df_list = [pd.read_csv(f) for f in sp3_filelist]\n    for df in sp3_df_list:\n        df['Epoch'] = df['Epoch'].astype(np.datetime64)\n    sp3_df = io_f.concat_sp3(sp3_df_list)\n\n    t0 = gnss_df['Epoch'].min()\n\n    gnss_sat_dfs = []\n    for sat, gnss_sat_df in gnss_df.groupby('SatName'):\n        sp3_sat_df = sp3_df[sp3_df['SatName'] == sat]\n        if sp3_sat_df.shape[0] == 0:\n            log.append(f'{sat} not available.')\n            continue\n        TIME_ref = (sp3_sat_df['Epoch'] - t0).astype(np.int64).values * 1e-9\n        xPos_fn  = InterpolatedUnivariateSpline(TIME_ref, sp3_sat_df['X'].values, k=5)\n        yPos_fn  = InterpolatedUnivariateSpline(TIME_ref, sp3_sat_df['Y'].values, k=5)\n        zPos_fn  = InterpolatedUnivariateSpline(TIME_ref, sp3_sat_df['Z'].values, k=5)\n        xVel_fn  = xPos_fn.derivative()\n        yVel_fn  = yPos_fn.derivative()\n        zVel_fn  = zPos_fn.derivative()\n\n        TIME = (gnss_sat_df['Epoch'] - t0).astype(np.int64).values * 1e-9\n        xPos = xPos_fn(TIME)\n        yPos = yPos_fn(TIME)\n        zPos = zPos_fn(TIME)\n        xVel = xVel_fn(TIME)\n        yVel = yVel_fn(TIME)\n        zVel = zVel_fn(TIME)\n        gnss_sat_df['xSatPosM']   = xPos\n        gnss_sat_df['ySatPosM']   = yPos\n        gnss_sat_df['zSatPosM']   = zPos\n        gnss_sat_df['xSatVelMps'] = xVel\n        gnss_sat_df['ySatVelMps'] = yVel\n        gnss_sat_df['zSatVelMps'] = zVel\n        gnss_sat_df['relClkBiasM'] = - (2 \/ C.LIGHT_SPEED) * (xPos*xVel + yPos*yVel + zPos*zVel)\n        gnss_sat_dfs.append(gnss_sat_df)\n        pass\n    gnss_df = pd.concat(gnss_sat_dfs, axis=0)\n    gnss_df = gnss_df.sort_values(['millisSinceGpsEpoch', 'SatName', 'SignalType'])\n    gnss_df = gnss_df.reset_index(drop=True)\n    return gnss_df\n\ndef remove_QZSS(df, log):\n    if df[df['SignalType'] == 'QZS_J1'].shape[0] < 50:\n        df = df[df['SignalType'] != 'QZS_J1']\n    if df[df['SignalType'] == 'QZS_J5'].shape[0] < 50:\n        df = df[df['SignalType'] != 'QZS_J5']\n    df = df.sort_values(['millisSinceGpsEpoch', 'SatName', 'SignalType'])\n    df = df.reset_index(drop=True)\n    return df\n\ndef fix_derived_timestamp(df_raw, df_derived):\n    raw_timestamps     = df_raw['millisSinceGpsEpoch'].unique()\n    derived_timestamps = df_derived['millisSinceGpsEpoch'].unique()\n    indexes = np.searchsorted(raw_timestamps, derived_timestamps)\n    from_t_to_fix_derived = dict(zip(derived_timestamps, raw_timestamps[indexes-1]))\n    df_derived['millisSinceGpsEpoch'] = np.array(list(map(lambda v: from_t_to_fix_derived[v], df_derived['millisSinceGpsEpoch'])))\n\n    df_derived = df_derived.drop_duplicates(['millisSinceGpsEpoch', 'constellationType', 'svid', 'signalType'])    \n    df_derived = df_derived.sort_values('millisSinceGpsEpoch', ignore_index=True)\n    \n    return df_derived\n\ndef process_gnss_df(prefix, drive, phone, key, df):\n    log = []\n    df  = add_extra_features_1st(key, df, log)\n    df['defaultIsrbM'] = df['SignalType'].map(C.PHONE_ISRBM[phone]).astype(float)\n    df  = remove_invalid_measurements(df, log)\n    df  = remove_QZSS(df, log)\n    df  = add_raw_pseudo_range(df, log)\n    \n    derived_df = pd.read_csv(f'{INPUT_PATH}\/{prefix}\/{drive}\/{phone}\/{phone}_derived.csv')\n    derived_df = fix_derived_timestamp(df, derived_df)\n    df  = add_clock_bias_meter(df, derived_df, log)\n    \n    df  = add_satellite_orbit(df, log)\n    log = [f'[{key:<50}] {msg}' for msg in log]\n    return df, log\n\ndef do_preprocess(path):\n    prefix, drive, phone = path.split('\/')[-3:]\n    key = f'{prefix}_{drive}_{phone}'\n    df1 = io_f.read_GnssLog_Raw(f'{INPUT_PATH}\/{prefix}\/{drive}\/{phone}\/{phone}_GnssLog.txt')\n    try:\n        df2, log = process_gnss_df(prefix, drive, phone, key, df1)\n        df2.to_csv(f'{DEST_PATH}\/{key}.csv', index=False)\n        N1  = df1.shape[0]\n        N2  = df2.shape[0]\n        percent = 100.0 * N2 \/ N1\n        log.append(f'[{key:<50}] {N2}\/{N1} ({percent:.1f})')\n        return log\n    except ValueError:\n        traceback.print_exc(file=sys.stderr)\n        return f'{key:<50}: Fail'\n    pass\n\ndef main():\n    os.makedirs(DEST_PATH, exist_ok=True)\n    \n    pathlist = sorted(glob.glob(f'{INPUT_PATH}\/train\/*\/*') + glob.glob(f'{INPUT_PATH}\/test\/*\/*'))\n\n    processes = multiprocessing.cpu_count()\n    with multiprocessing.Pool(processes=processes) as pool:\n        results = pool.imap_unordered(do_preprocess, pathlist)\n        results = tqdm(results, total=len(pathlist))\n        results = itertools.chain.from_iterable(results)\n        results = list(results)\n    with open('_preprocess.log', 'w') as f:\n        for msg in sorted(results):\n            print(msg, file=f)\n    return","abd652fe":"%%writefile positioning_stage1.py\nimport glob\nimport datetime\nimport sys\nimport os\nimport multiprocessing\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nimport io_f\nimport constants as C\nimport transform\nimport urls\nimport delay_model\nimport smooth_height\nimport carrier_smoothing\n\nINPUT_PATH      = '..\/input\/google-smartphone-decimeter-challenge'\nDELAY_DATA_PATH = '..\/input\/gsdc-external-data-for-delay-models'\nFEATURE_PATH    = '_features'\nDEST_PATH       = '_raw_gnss_stage1'\n\nINIT_REC_POS = transform.BLH_to_ECEF(transform.BLH(lat=C.INIT_B, lng=C.INIT_L, hgt=C.INIT_H)).to_numpy()\n\ndef get_ionex(days):\n    file_list  = [urls.ionosphere_filename(DELAY_DATA_PATH, d) for d in days]\n    ionex_list = [io_f.read_IONEX_file(f) for f in file_list]\n    ionex = io_f.concat_ionex(ionex_list)\n    return ionex\n\ndef get_default_sinex(days):\n    epoch = [datetime.datetime.combine(days[ 0], datetime.time( 0,  0)),\n             datetime.datetime.combine(days[-1], datetime.time(23, 55))]\n    df = pd.DataFrame({\n        'Epoch' : epoch,\n    })\n    df['TROTOT']     = C.DEFAULT_TROPO_DELAY_M\n    df['TROTOT_STD'] = 0.0\n    df['TGNTOT']     = 0.0\n    df['TGNTOT_STD'] = 0.0\n    df['TGETOT']     = 0.0\n    df['TGETOT_STD'] = 0.0\n    return df\n\ndef get_sinex(days):\n    file_list  = [urls.troposphere_filename(DELAY_DATA_PATH, d) for d in days]\n    if not np.all([os.path.exists(f) for f in file_list]):\n        return get_default_sinex(days)\n    sinex_list = [io_f.read_SINEX_TRO_file(f) for f in file_list]\n    sinex = pd.concat(sinex_list, axis=0)\n    return sinex\n\ndef L2_norm(x):\n    return np.sqrt(np.sum(x**2, axis=1))\n\ndef calc_azel(sat_pos, rec_pos):\n    sat = transform.ECEF.from_numpy(sat_pos)\n    rec = transform.ECEF.from_numpy(rec_pos)\n    return transform.ECEF_to_AZEL(pos=sat, base=rec)\n\ndef calc_blh(pos):\n    blh = transform.ECEF.from_numpy(pos)\n    return transform.ECEF_to_BLH(blh)\n\ndef mask_to_weight(mask, epsilon=1e-2):\n    return (1 - epsilon) * mask.astype(float) + epsilon\n\ndef solve_weight_least_square(WA, Wb, alpha, epsilon=1e-12):\n    \"\"\"\n    J = ||WA x - Wb||^2 + alpha * ||x[4:]||^2\n    \"\"\"\n    sys_A = WA.T @ WA\n    sys_b = WA.T @ Wb\n    N = WA.shape[1]\n    for i in range(4, N):\n        sys_A[i, i] += alpha\n    u, s, vh = np.linalg.svd(sys_A, hermitian=True)\n    cond = s[0] \/ s[-1]\n    if cond * epsilon > 1:\n        return\n    sys_x = vh.T @ ((u.T @ sys_b) \/ s)\n    return sys_x\n\ndef do_positioning_1epoch(name, ionex, sinex, epoch_df):\n    ms_epoch = epoch_df['millisSinceGpsEpoch'].values[0]\n    M = epoch_df.shape[0]\n    sig_type     = epoch_df['SignalType'].values\n    sig_type_set = set(sig_type) - set(['GPS_L1'])\n    N_isbr       = len(sig_type_set)\n    N_states     = N_isbr + 4\n    if M < 4:\n        print(f'[{name} {ms_epoch}] Too few satellites.')\n        return\n    A_t    = np.ones((M, 1))\n    A_isbr = np.zeros((M, N_isbr))\n    for i, s in enumerate(sorted(sig_type_set)):\n        A_isbr[:, i] = (sig_type == s).astype(float)\n    \n    sat_pos_rec    = epoch_df[['xSatPosM', 'ySatPosM', 'zSatPosM']].values\n    sat_vel_rec    = epoch_df[['xSatVelMps', 'ySatVelMps', 'zSatVelMps']].values\n    raw_pr_m       = epoch_df['smoothedPrM'].values\n    sat_clk_m      = epoch_df['satClkBiasM'].values\n    default_isrb_m = epoch_df['defaultIsrbM'].values\n    var_svtime_m   = (epoch_df['ReceivedSvTimeUncertaintyNanos'].values * (1e-9 * C.LIGHT_SPEED))**2\n    carrier_freq   = epoch_df['CarrierFrequencyHz'].values\n    epoch = epoch_df['Epoch'].values[0]\n\n    sat_pos  = sat_pos_rec\n    rec_pos  = INIT_REC_POS\n    rec_time = 0.0\n    outlier_mask = np.full(M, True)\n    for outer_loop in range(3):\n        success  = False\n        for inner_loop in range(8):\n            if inner_loop < 2:\n                tof      = L2_norm(rec_pos - sat_pos) \/ C.LIGHT_SPEED\n                sat_pos  = sat_pos_rec - (sat_vel_rec.T * (tof + rec_time)).T\n                sat_azel = calc_azel(sat_pos, rec_pos)\n                rec_blh  = calc_blh(rec_pos)\n                sagnac_m = (C.OMEGA_EARTH \/ C.LIGHT_SPEED) * (sat_pos[:, 0] * rec_pos[1] - sat_pos[:, 1] * rec_pos[0])\n                iono_delay_m  = delay_model.calc_iono_delay(ionex, epoch, carrier_freq, sat_azel, rec_blh)\n                tropo_delay_m = delay_model.calc_tropo_delay(sinex, epoch, sat_azel)\n                correct_pr_m  = raw_pr_m + sat_clk_m - iono_delay_m - tropo_delay_m - default_isrb_m - sagnac_m\n                var_el_err_m  = (0.8 \/ np.maximum(0.05, np.sin(sat_azel.elevation)))**2\n                el_mask       = (sat_azel.elevation > np.deg2rad(5.0))\n                W = mask_to_weight(outlier_mask & el_mask) * (var_svtime_m + var_el_err_m)**(-0.5)\n            pos_diff = rec_pos - sat_pos\n            rho      = L2_norm(pos_diff)\n            A_xyz    = ((1\/rho) * pos_diff.T).T\n            A  = np.concatenate([A_xyz, A_t, A_isbr], axis=1)\n            b  = correct_pr_m - rho\n            WA = (W * A.T).T\n            Wb = W * b\n            v = solve_weight_least_square(WA, Wb, alpha=(20.0)**(-2))\n            if v is None:\n                print(f'[{name} {ms_epoch}] Rank deficient.')\n                return\n            delta_pos = v[0:3]\n            rec_time  = v[3] \/ C.LIGHT_SPEED\n            rec_pos   = rec_pos + delta_pos\n            if np.sqrt(np.sum(delta_pos**2)) < C.EPSILON_M:\n                residual = b - (A @ v)\n                outlier_mask = (residual < 80.0)\n                success = True\n                break\n        if not success:\n            print(f'[{name} {ms_epoch}] Did not converge.')\n            return\n    rec_blh = calc_blh(rec_pos)\n    latDeg  = np.rad2deg(rec_blh.lat)\n    lngDeg  = np.rad2deg(rec_blh.lng)\n    height  = rec_blh.hgt\n    isrbM   = (A_isbr @ v[4:]) + default_isrb_m\n    log_df  = pd.DataFrame({\n        'SatName'      : epoch_df['SatName'].values,\n        'SignalType'   : sig_type,\n        'xSatPosM'     : sat_pos[:, 0],\n        'ySatPosM'     : sat_pos[:, 1],\n        'zSatPosM'     : sat_pos[:, 2],\n        'isrbM'        : isrbM,\n        'ionoDelayM'   : iono_delay_m,\n        'tropoDelayM'  : tropo_delay_m,\n        'residualM'    : residual,\n        'isOutlier'    : ~outlier_mask,\n        'elevationDeg' : np.rad2deg(sat_azel.elevation),\n        'azimuthDeg'   : np.rad2deg(sat_azel.azimuth),\n    })\n    log_df['millisSinceGpsEpoch'] = ms_epoch\n    log_df['xRecPosM']    = rec_pos[0]\n    log_df['yRecPosM']    = rec_pos[1]\n    log_df['zRecPosM']    = rec_pos[2]\n    log_df['recClkBiasM'] = v[3]\n    return latDeg, lngDeg, height, log_df\n\ndef do_positioning(path):\n    prefix, drive, phone, = path.split('\/')[-3:]\n    name = f'{prefix}_{drive}_{phone}'\n\n    gnss_df  = pd.read_csv(f'{FEATURE_PATH}\/{prefix}_{drive}_{phone}.csv')\n    gnss_df['Epoch'] = gnss_df['Epoch'].astype(np.datetime64)\n    gnss_df = carrier_smoothing.do_carrier_smoothing(gnss_df)\n\n    days = np.unique([(gnss_df['Epoch'].min() - C.IONO_TIME_MARGIN).date(),\n                      (gnss_df['Epoch'].max() + C.IONO_TIME_MARGIN).date()])\n    ionex = get_ionex(days)\n    sinex = get_sinex(days)\n\n    data    = []\n    log_dfs = []\n    NaN = float('nan')\n    for index, (epoch, epoch_df) in enumerate(gnss_df.groupby('millisSinceGpsEpoch')):\n        result = do_positioning_1epoch(name, ionex, sinex, epoch_df)\n        if result is not None:\n            latDeg, lngDeg, height, log_df = result\n            data.append((epoch, 1, latDeg, lngDeg, height))\n            log_dfs.append(log_df)\n        else:\n            data.append((epoch, 0, NaN, NaN, NaN))\n    columns = ['millisSinceGpsEpoch', 'valid', 'latDeg', 'lngDeg', 'heightAboveWgs84EllipsoidM']\n    pred_df = pd.DataFrame(data, columns=columns).sort_values('millisSinceGpsEpoch')\n\n    valid = (pred_df['valid'] == 1).values\n    t_ref = pred_df['millisSinceGpsEpoch'].min()\n    time_pred = 1e-3 * (pred_df['millisSinceGpsEpoch'] - t_ref).values\n    height_pred = pred_df['heightAboveWgs84EllipsoidM'].values\n    dt = 2.0\n    T_max = np.max(time_pred)\n    pred_df['height_smooth'] = smooth_height.smooth_height(\n        time_pred=time_pred,\n        height_pred=height_pred,\n        valid=valid,\n        T_max=T_max,\n        beta=5.0,\n        dt=dt,\n    )\n    \n    pred_df.to_csv(f'{DEST_PATH}\/{prefix}_{drive}_{phone}.csv', index=False)\n\n    log_df = pd.concat(log_dfs, axis=0)\n    log_df.to_csv(f'{DEST_PATH}\/log_{prefix}_{drive}_{phone}.csv', index=False)\n\n    return \n    \ndef main():\n    os.makedirs(DEST_PATH, exist_ok=True)\n\n    pathlist = sorted(glob.glob(f'{INPUT_PATH}\/train\/*\/*') + glob.glob(f'{INPUT_PATH}\/test\/*\/*'))\n\n    processes = multiprocessing.cpu_count()\n    with multiprocessing.Pool(processes=processes) as pool:\n        results = pool.imap_unordered(do_positioning, pathlist)\n        results = tqdm(results, total=len(pathlist))\n        results = list(results)\n    return","c6b0982b":"%%writefile positioning_stage2.py\nimport glob\nimport datetime\nimport sys\nimport os\nimport multiprocessing\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nimport io_f\nimport constants as C\nimport transform\nimport urls\nimport delay_model\nimport carrier_smoothing\n\nINPUT_PATH      = '..\/input\/google-smartphone-decimeter-challenge'\nDELAY_DATA_PATH = '..\/input\/gsdc-external-data-for-delay-models'\nFEATURE_PATH    = '_features'\nPREV_STAGE_PATH = '_raw_gnss_stage1'\nDEST_PATH       = '_raw_gnss_stage2'\n\nINIT_REC_POS = transform.BLH_to_ECEF(transform.BLH(lat=C.INIT_B, lng=C.INIT_L, hgt=C.INIT_H)).to_numpy()\n\ndef get_ionex(days):\n    file_list  = [urls.ionosphere_filename(DELAY_DATA_PATH, d) for d in days]\n    ionex_list = [io_f.read_IONEX_file(f) for f in file_list]\n    ionex = io_f.concat_ionex(ionex_list)\n    return ionex\n\ndef get_default_sinex(days):\n    epoch = [datetime.datetime.combine(days[ 0], datetime.time( 0,  0)),\n             datetime.datetime.combine(days[-1], datetime.time(23, 55))]\n    df = pd.DataFrame({\n        'Epoch' : epoch,\n    })\n    df['TROTOT']     = C.DEFAULT_TROPO_DELAY_M\n    df['TROTOT_STD'] = 0.0\n    df['TGNTOT']     = 0.0\n    df['TGNTOT_STD'] = 0.0\n    df['TGETOT']     = 0.0\n    df['TGETOT_STD'] = 0.0\n    return df\n\ndef get_sinex(days):\n    file_list  = [urls.troposphere_filename(DELAY_DATA_PATH, d) for d in days]\n    if not np.all([os.path.exists(f) for f in file_list]):\n        return get_default_sinex(days)\n    sinex_list = [io_f.read_SINEX_TRO_file(f) for f in file_list]\n    sinex = pd.concat(sinex_list, axis=0)\n    return sinex\n\ndef L2_norm(x):\n    return np.sqrt(np.sum(x**2, axis=1))\n\ndef calc_azel(sat_pos, rec_pos):\n    sat = transform.ECEF.from_numpy(sat_pos)\n    rec = transform.ECEF.from_numpy(rec_pos)\n    return transform.ECEF_to_AZEL(pos=sat, base=rec)\n\ndef calc_blh(pos):\n    blh = transform.ECEF.from_numpy(pos)\n    return transform.ECEF_to_BLH(blh)\n\ndef mask_to_weight(mask, epsilon=1e-2):\n    return (1 - epsilon) * mask.astype(float) + epsilon\n\ndef calc_height(rec_pos):\n    ecef = transform.ECEF.from_numpy(rec_pos)\n    blh  = transform.ECEF_to_BLH(ecef)    \n    return np.array([blh.hgt])\n\ndef calc_height_gain(rec_pos, N_states):\n    ecef = transform.ECEF.from_numpy(rec_pos)\n    blh  = transform.ECEF_to_BLH(ecef)    \n    J = transform.jacobian_BLH_to_ECEF(blh)\n    Jinv = np.linalg.inv(J)\n    A = np.zeros((1, N_states))\n    A[0, 0:3] = Jinv[2, :]\n    return A\n\ndef solve_weight_least_square(A1, b1, A2, b2, alpha, gamma, epsilon=1e-12):\n    \"\"\"\n    J = ||A1 x - b1||^2 + alpha * ||x[4:]||^2 + gamma * ||A2 x - b2||^2\n    \"\"\"\n    sys_A = (A1.T @ A1) + gamma * (A2.T @ A2)\n    sys_b = (A1.T @ b1) + gamma * (A2.T @ b2)\n    N = A1.shape[1]\n    for i in range(4, N):\n        sys_A[i, i] += alpha\n    u, s, vh = np.linalg.svd(sys_A, hermitian=True)\n    cond = s[0] \/ s[-1]\n    if cond * epsilon > 1:\n        return\n    sys_x = vh.T @ ((u.T @ sys_b) \/ s)\n    return sys_x\n\ndef do_positioning_1epoch(name, ionex, sinex, epoch_df, h0):\n    ms_epoch = epoch_df['millisSinceGpsEpoch'].values[0]\n    M = epoch_df.shape[0]\n    sig_type     = epoch_df['SignalType'].values\n    sig_type_set = set(sig_type) - set(['GPS_L1'])\n    N_isbr       = len(sig_type_set)\n    N_states     = N_isbr + 4\n    if M < 3:\n        print(f'[{name} {ms_epoch}] Too few satellites.')\n        return\n    A_t    = np.ones((M, 1))\n    A_isbr = np.zeros((M, N_isbr))\n    for i, s in enumerate(sorted(sig_type_set)):\n        A_isbr[:, i] = (sig_type == s).astype(float)\n    \n    sat_pos_rec    = epoch_df[['xSatPosM', 'ySatPosM', 'zSatPosM']].values\n    sat_vel_rec    = epoch_df[['xSatVelMps', 'ySatVelMps', 'zSatVelMps']].values\n    raw_pr_m       = epoch_df['smoothedPrM'].values\n    sat_clk_m      = epoch_df['satClkBiasM'].values\n    default_isrb_m = epoch_df['defaultIsrbM'].values\n    var_svtime_m   = (epoch_df['ReceivedSvTimeUncertaintyNanos'].values * (1e-9 * C.LIGHT_SPEED))**2\n    carrier_freq   = epoch_df['CarrierFrequencyHz'].values\n    epoch = epoch_df['Epoch'].values[0]\n\n    sat_pos  = sat_pos_rec\n    rec_pos  = INIT_REC_POS\n    rec_time = 0.0\n    outlier_mask = np.full(M, True)\n    for outer_loop in range(3):\n        success  = False\n        for inner_loop in range(8):\n            if inner_loop < 2:\n                tof      = L2_norm(rec_pos - sat_pos) \/ C.LIGHT_SPEED\n                sat_pos  = sat_pos_rec - (sat_vel_rec.T * (tof + rec_time)).T\n                sat_azel = calc_azel(sat_pos, rec_pos)\n                rec_blh  = calc_blh(rec_pos)\n                sagnac_m = (C.OMEGA_EARTH \/ C.LIGHT_SPEED) * (sat_pos[:, 0] * rec_pos[1] - sat_pos[:, 1] * rec_pos[0])\n                iono_delay_m  = delay_model.calc_iono_delay(ionex, epoch, carrier_freq, sat_azel, rec_blh)\n                tropo_delay_m = delay_model.calc_tropo_delay(sinex, epoch, sat_azel)\n                correct_pr_m  = raw_pr_m + sat_clk_m - iono_delay_m - tropo_delay_m - default_isrb_m - sagnac_m\n                var_el_err_m  = (0.8 \/ np.maximum(0.05, np.sin(sat_azel.elevation)))**2\n                el_mask       = (sat_azel.elevation > np.deg2rad(5.0))\n                W = mask_to_weight(outlier_mask & el_mask) * (var_svtime_m + var_el_err_m)**(-0.5)\n            pos_diff = rec_pos - sat_pos\n            rho      = L2_norm(pos_diff)\n            A_xyz    = ((1\/rho) * pos_diff.T).T\n            A  = np.concatenate([A_xyz, A_t, A_isbr], axis=1)\n            b  = correct_pr_m - rho\n            WA = (W * A.T).T\n            Wb = W * b\n            Ah = calc_height_gain(rec_pos, N_states)\n            bh = h0 - calc_height(rec_pos)\n            v = solve_weight_least_square(WA, Wb,\n                                          Ah, bh,\n                                          alpha=(5.0)**(-2),\n                                          gamma=2.2**(-2))\n            if v is None:\n                print(f'[{name} {ms_epoch}] Rank deficient.')\n                return\n            delta_pos = v[0:3]\n            rec_time  = v[3] \/ C.LIGHT_SPEED\n            rec_pos   = rec_pos + delta_pos\n            if np.sqrt(np.sum(delta_pos**2)) < C.EPSILON_M:\n                residual = b - (A @ v)\n                outlier_mask = (np.abs(residual) < 50.0)\n                success = True\n                break\n        if not success:\n            print(f'[{name} {ms_epoch}] Did not converge.')\n            return\n    rec_blh = calc_blh(rec_pos)\n    latDeg  = np.rad2deg(rec_blh.lat)\n    lngDeg  = np.rad2deg(rec_blh.lng)\n    height  = rec_blh.hgt\n    isrbM   = (A_isbr @ v[4:]) + default_isrb_m\n    log_df  = pd.DataFrame({\n        'SatName'      : epoch_df['SatName'].values,\n        'SignalType'   : sig_type,\n        'xSatPosM'     : sat_pos[:, 0],\n        'ySatPosM'     : sat_pos[:, 1],\n        'zSatPosM'     : sat_pos[:, 2],\n        'isrbM'        : isrbM,\n        'ionoDelayM'   : iono_delay_m,\n        'tropoDelayM'  : tropo_delay_m,\n        'residualM'    : residual,\n        'isOutlier'    : ~outlier_mask,\n        'elevationDeg' : np.rad2deg(sat_azel.elevation),\n        'azimuthDeg'   : np.rad2deg(sat_azel.azimuth),\n    })\n    log_df['millisSinceGpsEpoch'] = ms_epoch\n    log_df['xRecPosM']    = rec_pos[0]\n    log_df['yRecPosM']    = rec_pos[1]\n    log_df['zRecPosM']    = rec_pos[2]\n    log_df['recClkBiasM'] = v[3]\n    return latDeg, lngDeg, height, log_df\n\ndef do_positioning(path):\n    prefix, drive, phone, = path.split('\/')[-3:]\n    name = f'{prefix}_{drive}_{phone}'\n\n    gnss_df  = pd.read_csv(f'{FEATURE_PATH}\/{prefix}_{drive}_{phone}.csv')\n    gnss_df['Epoch'] = gnss_df['Epoch'].astype(np.datetime64)\n    gnss_df = carrier_smoothing.do_carrier_smoothing(gnss_df)\n\n    prev_df = pd.read_csv(f'{PREV_STAGE_PATH}\/{prefix}_{drive}_{phone}.csv')\n\n    log_df = pd.read_csv(f'{PREV_STAGE_PATH}\/log_{prefix}_{drive}_{phone}.csv')\n    isrbM_map = { 'GPS_L1' : 0.0 }\n    for sig_type, sig_df in log_df.groupby('SignalType'):\n        if sig_type != 'GPS_L1':\n            sig_df = sig_df.drop_duplicates(subset='millisSinceGpsEpoch')\n            q10 = sig_df['isrbM'].quantile(0.10)\n            q90 = sig_df['isrbM'].quantile(0.90)\n            sig_df = sig_df[(q10 <= sig_df['isrbM']) & (sig_df['isrbM'] <= q90)]\n            isrbM_map[sig_type] = sig_df['isrbM'].mean()\n    gnss_df['defaultIsrbM'] = gnss_df['SignalType'].map(isrbM_map)\n\n    days = np.unique([(gnss_df['Epoch'].min() - C.IONO_TIME_MARGIN).date(),\n                      (gnss_df['Epoch'].max() + C.IONO_TIME_MARGIN).date()])\n    ionex = get_ionex(days)\n    sinex = get_sinex(days)\n\n    data    = []\n    log_dfs = []\n    NaN = float('nan')\n    for index, (epoch, epoch_df) in enumerate(gnss_df.groupby('millisSinceGpsEpoch')):\n        h0 = prev_df[prev_df['millisSinceGpsEpoch'] == epoch]['height_smooth'].values[0]\n\n        result = do_positioning_1epoch(name, ionex, sinex, epoch_df, h0)\n        if result is not None:\n            latDeg, lngDeg, height, log_df = result\n            data.append((epoch, 1, latDeg, lngDeg, height))\n            log_dfs.append(log_df)\n        else:\n            data.append((epoch, 0, NaN, NaN, NaN))\n    columns = ['millisSinceGpsEpoch', 'valid', 'latDeg', 'lngDeg', 'heightAboveWgs84EllipsoidM']\n    df = pd.DataFrame(data, columns=columns).sort_values('millisSinceGpsEpoch')\n    df.to_csv(f'{DEST_PATH}\/{prefix}_{drive}_{phone}.csv', index=False)\n\n    log_df = pd.concat(log_dfs, axis=0)\n    log_df.to_csv(f'{DEST_PATH}\/log_{prefix}_{drive}_{phone}.csv', index=False)\n\n    return \n    \ndef main():\n    os.makedirs(DEST_PATH, exist_ok=True)\n\n    pathlist = sorted(glob.glob(f'{INPUT_PATH}\/train\/*\/*') + glob.glob(f'{INPUT_PATH}\/test\/*\/*'))\n\n    processes = multiprocessing.cpu_count()\n    with multiprocessing.Pool(processes=processes) as pool:\n        results = pool.imap_unordered(do_positioning, pathlist)\n        results = tqdm(results, total=len(pathlist))\n        results = list(results)\n    return","6855ddd9":"%%writefile make_new_baseline.py\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import InterpolatedUnivariateSpline\n\nimport transform\n\nINPUT_PATH  = '..\/input\/google-smartphone-decimeter-challenge'\nPREDICTION_PATH = '_raw_gnss_stage2'\n\nTRAIN_BASELINE_DF = pd.read_csv(f'{INPUT_PATH}\/baseline_locations_train.csv')\nTEST_BASELINE_DF  = pd.read_csv(f'{INPUT_PATH}\/baseline_locations_test.csv')\nBASELINE_DF = pd.concat([TRAIN_BASELINE_DF, TEST_BASELINE_DF], axis=0)\ndef get_baseline(drive, phone):\n    df = BASELINE_DF[BASELINE_DF['phone'] == f'{drive}_{phone}'].copy()\n    df.reset_index(drop=True, inplace=True)\n    return df\n\ndef make_output_df(prefix, drive, phone):\n    base_df = get_baseline(drive, phone)\n\n    pred_df = pd.read_csv(f'{PREDICTION_PATH}\/{prefix}_{drive}_{phone}.csv')\n    pred_df = pred_df[pred_df['valid'] == 1]\n    pred_df = pred_df.reset_index(drop=True)\n\n    t_ref    = base_df['millisSinceGpsEpoch'].min()\n    TIME_y   = 1e-3 * (base_df['millisSinceGpsEpoch'] - t_ref).values\n    TIME_x   = 1e-3 * (pred_df['millisSinceGpsEpoch'] - t_ref).values\n    latDeg_x = pred_df['latDeg'].values\n    lngDeg_x = pred_df['lngDeg'].values\n    latDeg_y = InterpolatedUnivariateSpline(TIME_x, latDeg_x, k=1)(TIME_y)\n    lngDeg_y = InterpolatedUnivariateSpline(TIME_x, lngDeg_x, k=1)(TIME_y)\n    interp_df = pd.DataFrame({\n        'collectionName'      : base_df['collectionName'],\n        'phoneName'           : base_df['phoneName'],\n        'millisSinceGpsEpoch' : base_df['millisSinceGpsEpoch'],\n        'latDeg' : latDeg_y,\n        'lngDeg' : lngDeg_y,\n        'phone'  : base_df['phone'],\n    })\n    # Replace outliers with host baseline\n    distance = transform.pd_haversine_distance(interp_df, base_df)\n    alpha = (distance < 50).astype(float)\n    output_df = pd.DataFrame({\n        'collectionName'      : base_df['collectionName'],\n        'phoneName'           : base_df['phoneName'],\n        'millisSinceGpsEpoch' : base_df['millisSinceGpsEpoch'],\n        'latDeg' : alpha * interp_df['latDeg'] + (1 - alpha) * base_df['latDeg'],\n        'lngDeg' : alpha * interp_df['lngDeg'] + (1 - alpha) * base_df['lngDeg'],\n        'phone'  : base_df['phone'],\n    })\n    return output_df\n\ndef print_score(output_df):\n    score_list = []\n    for gid, phone_df in output_df.groupby('phone'):\n        drive, phone = gid.split('_')\n        gt_df = pd.read_csv(f'{INPUT_PATH}\/train\/{drive}\/{phone}\/ground_truth.csv')\n        d = transform.pd_haversine_distance(phone_df, gt_df)\n        score = np.mean([np.quantile(d, 0.50), np.quantile(d, 0.95)])\n        score_list.append(score)\n    score = np.mean(score_list)\n    print(f'train score: {score:.3f}')\n    return\n\ndef main():\n    df_list = []\n    for phone in np.unique(TRAIN_BASELINE_DF['phone']):\n        collection_name, phone_name = phone.split('_')\n        df_list.append(make_output_df('train', collection_name, phone_name))\n    output_df = pd.concat(df_list)\n    output_df = output_df.sort_values(['phone', 'millisSinceGpsEpoch'])\n    output_df.to_csv('raw_gnss_train.csv', index=False)\n    print_score(output_df)\n\n    df_list = []\n    for phone in np.unique(TEST_BASELINE_DF['phone']):\n        collection_name, phone_name = phone.split('_')\n        df_list.append(make_output_df('test', collection_name, phone_name))\n    output_df = pd.concat(df_list)\n    output_df = output_df.sort_values(['phone', 'millisSinceGpsEpoch'])\n    output_df.to_csv('raw_gnss_test.csv', index=False)\n    \n    columns = ['phone', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']\n    sub_df = output_df[columns]\n    sub_df.to_csv('submission.csv', index=False)\n    return","babf3021":"import generate_phone_isrbM\ngenerate_phone_isrbM.main()","54b4159a":"import preprocess\npreprocess.main()","2c9470d0":"import positioning_stage1\npositioning_stage1.main()","6f43f792":"import positioning_stage2\npositioning_stage2.main()","87d91e78":"import make_new_baseline\nmake_new_baseline.main()","03cded77":"## main","d703dc3a":"## Libraries"}}