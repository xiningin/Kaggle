{"cell_type":{"6a226a54":"code","0b94c833":"code","24db3fe9":"code","ab744f0a":"code","44088c42":"code","f4670dfd":"code","8ba7a4e4":"code","9321630f":"code","5bb42c75":"code","13aee9a4":"code","73ff8985":"code","c0a71a30":"code","15fec2c4":"code","14a18c20":"code","d8fddee5":"code","9eb9769b":"code","d5a93492":"code","258b8553":"code","09e59a5a":"code","cd209444":"code","807578ed":"code","7b0d7b4b":"code","ce53b700":"code","4edf3550":"code","742f9f1a":"code","d7b2b94e":"code","b6d92732":"code","74cbc2e0":"code","a1a17349":"code","cf97473a":"code","4828e9ab":"code","a0b583c3":"code","d59d916f":"code","2ac9c287":"code","5ffdf014":"code","3935a0a0":"code","04269492":"code","4ce28e99":"code","9ac8ecf1":"code","dd84b64f":"code","bf75fec6":"code","ca1b3b7d":"code","e0490b8f":"code","035491b2":"code","d0ae427a":"code","62cc01b6":"code","3fc8fa4f":"code","e49d7298":"code","a25624b1":"code","3fd377e5":"code","b0d5d11d":"code","312ea7d9":"code","f7270f4b":"code","2b5ba0b7":"code","82a2afd1":"code","e0f6564b":"code","8db6a1bf":"code","9a94a12f":"code","bedbca79":"code","12075410":"code","37e5326f":"code","853c734b":"code","eb7ee393":"code","31a118cb":"code","ecbe0e4a":"code","ed9a6604":"code","322df0b7":"code","0c945c49":"code","d9dc7847":"code","bc26d280":"code","f6239556":"code","73de3ef5":"code","cba7ab17":"code","0ee112c0":"code","86520d8f":"code","57363fec":"code","fef699e4":"code","ed96a546":"code","59fd52bc":"code","a09c8529":"code","d70b62e4":"code","3dc81406":"code","6fe2935a":"code","8cc9301f":"code","f9e9f8e7":"code","4061312e":"code","89a235ff":"code","ddda7dfc":"code","908b0b54":"code","2c282a6b":"code","94751b4c":"code","fd460ae6":"code","9d17ad9f":"code","5a08344e":"code","3fc468c3":"code","ebf505a8":"code","c5b38707":"code","2284e376":"code","4725d755":"code","47a16966":"code","443d5f19":"code","8003cc48":"code","5fa40176":"code","e8ebf98c":"markdown","2c94b221":"markdown","cf3d58bd":"markdown","a6c4d686":"markdown","0fe9ce89":"markdown","5c32456c":"markdown","34e85fe2":"markdown","f842ac15":"markdown","bd94ab0d":"markdown","96983242":"markdown","47df90ce":"markdown","55a74306":"markdown","b6b6fec7":"markdown","413444ef":"markdown","406393de":"markdown","fc5dd146":"markdown","189732c2":"markdown","ab719444":"markdown","17875d9d":"markdown","fe95d8f4":"markdown","9d29dd24":"markdown","20861355":"markdown","e2e153ba":"markdown","146b59cf":"markdown","8d04f40a":"markdown","994de017":"markdown","3dc1bfe7":"markdown","67f491ed":"markdown"},"source":{"6a226a54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0b94c833":"import keras\nimport nltk\nimport re\nimport string","24db3fe9":"from keras.preprocessing.text import one_hot\nfrom keras.layers import LSTM,Embedding,Dense,Bidirectional,GlobalMaxPool2D,BatchNormalization,Dropout,TimeDistributed,GlobalMaxPool1D\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential,Model\nfrom keras.layers import SpatialDropout1D,GRU\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score","ab744f0a":"train= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","44088c42":"train.head()","f4670dfd":"ntrain= train.shape[0]\nntest= test.shape[0]","8ba7a4e4":"ntrain,ntest","9321630f":"label= train['target']","5bb42c75":"train.drop(['target'],axis=1,inplace=True)","13aee9a4":"data= pd.concat([train,test])","73ff8985":"data.shape,train.shape,test.shape","c0a71a30":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","15fec2c4":"# Applying the cleaning function to both test and training datasets\ndata['text'] = data['text'].apply(lambda x: clean_text(x))","14a18c20":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","d8fddee5":"data['text']=data['text'].apply(lambda x: remove_emoji(x))","9eb9769b":"contractions = { \n\"ain't\": \"am not \/ are not \/ is not \/ has not \/ have not\",\n\"aren't\": \"are not \/ am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had \/ he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall \/ he will\",\n\"he'll've\": \"he shall have \/ he will have\",\n\"he's\": \"he has \/ he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has \/ how is \/ how does\",\n\"I'd\": \"I had \/ I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall \/ I will\",\n\"I'll've\": \"I shall have \/ I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had \/ it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall \/ it will\",\n\"it'll've\": \"it shall have \/ it will have\",\n\"it's\": \"it has \/ it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had \/ she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall \/ she will\",\n\"she'll've\": \"she shall have \/ she will have\",\n\"she's\": \"she has \/ she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as \/ so is\",\n\"that'd\": \"that would \/ that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has \/ that is\",\n\"there'd\": \"there had \/ there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has \/ there is\",\n\"they'd\": \"they had \/ they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall \/ they will\",\n\"they'll've\": \"they shall have \/ they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had \/ we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall \/ what will\",\n\"what'll've\": \"what shall have \/ what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has \/ what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has \/ when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has \/ where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall \/ who will\",\n\"who'll've\": \"who shall have \/ who will have\",\n\"who's\": \"who has \/ who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has \/ why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had \/ you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall \/ you will\",\n\"you'll've\": \"you shall have \/ you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\ncontractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\ndef expand_contractions(s, contractions = contractions):\n    def replace(match):\n        return contractions[match.group(0)]\n    return contractions_re.sub(replace, s)\n\nexpand_contractions(\"can't stop won't stop\")","d5a93492":"data['text'] =data['text'].apply(expand_contractions)","258b8553":"data['text'].head()","09e59a5a":"test= data[ntrain:]","cd209444":"train= data[:ntrain]","807578ed":"train.shape,test.shape","7b0d7b4b":"tweets= train['text'].copy()","ce53b700":"tweets_test= test['text'].copy()","4edf3550":"\nfrom keras.preprocessing.text import Tokenizer\nt = Tokenizer()\nt.fit_on_texts(tweets)\nvocab_size_train = len(t.word_index) + 1\n# integer encode the documents\nencoded_docs = t.texts_to_sequences(tweets)\n# pad documents to a max length of 4 words\nmax_length = 50\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","742f9f1a":"tweets= padded_docs","d7b2b94e":"from keras.preprocessing.text import Tokenizer\nt = Tokenizer()\nt.fit_on_texts(tweets_test)\nvocab_size_tesr = len(t.word_index) + 1\n# integer encode the documents\nencoded_docs = t.texts_to_sequences(tweets_test)\n# pad documents to a max length of 4 words\nmax_length = 50\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","b6d92732":"tweets_test= padded_docs","74cbc2e0":"# load the whole embedding into memory\nembeddings_index = dict()\nf = open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt')\nfor line in f:\n\tvalues = line.split()\n\tword = values[0]\n\tcoefs = np.asarray(values[1:], dtype='float32')\n\tembeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","a1a17349":"# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab_size_train, \n                             200))\nfor word, i in t.word_index.items():\n\tembedding_vector = embeddings_index.get(word)\n\tif embedding_vector is not None:\n\t\tembedding_matrix[i] = embedding_vector","cf97473a":"e = Embedding(vocab_size_train, 200, weights=[embedding_matrix], input_length=100, trainable=False)","4828e9ab":"from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val= train_test_split(tweets,label)","a0b583c3":"opt = Adam(lr=0.001, decay=1e-6)","d59d916f":"model=Sequential()\nmodel.add(e)\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1, activation = \"sigmoid\"))\nmodel.compile(optimizer= opt, loss='binary_crossentropy', metrics=['accuracy'])","2ac9c287":"history= model.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=20,batch_size=64,verbose=1)","5ffdf014":"import matplotlib.pyplot as plt","3935a0a0":"plt.plot(history.history['accuracy'], label='train accuracy')\nplt.plot(history.history['val_accuracy'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","04269492":"y_pred_lstm= model.predict_classes(X_val)","4ce28e99":"from sklearn.metrics import accuracy_score","9ac8ecf1":"accuracy_lstm= accuracy_score(y_pred_lstm,y_val)","dd84b64f":"print(\"The accuracy for the Lstm model is {} %\".format(accuracy_lstm*100))","bf75fec6":"model=Sequential()\nmodel.add(e)\nmodel.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1, activation = \"sigmoid\"))\nmodel.compile(optimizer= 'adam', loss='binary_crossentropy', metrics=['accuracy'])","ca1b3b7d":"hist=model.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=20,batch_size=64,verbose=1)","e0490b8f":"y_pred_bilstm= model.predict_classes(X_val)","035491b2":"accuracy_bilstm= accuracy_score(y_pred_bilstm,y_val)","d0ae427a":"print(\"The accuracy for the Bidirectional Lstm model is {} %\".format(accuracy_bilstm*100))","62cc01b6":"plt.plot(hist.history['accuracy'], label='train accuracy')\nplt.plot(hist.history['val_accuracy'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","3fc8fa4f":"model=Sequential()\nmodel.add(e)\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(100))\nmodel.add(Dense(1, activation = \"sigmoid\"))\nmodel.compile(optimizer='adam' ,loss='binary_crossentropy', metrics=['accuracy'])","e49d7298":"hist=model.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=20,batch_size=64,verbose=1)","a25624b1":"y_pred_gru= model.predict_classes(X_val)","3fd377e5":"accuracy_gru= accuracy_score(y_pred_gru,y_val)","b0d5d11d":"print(\"The accuracy for the GRU model is {} %\".format(accuracy_gru*100))","312ea7d9":"plt.plot(hist.history['accuracy'], label='train accuracy')\nplt.plot(hist.history['val_accuracy'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","f7270f4b":"from keras.layers import Layer\nimport keras.backend as K","2b5ba0b7":"class attention(Layer):\n    def __init__(self,**kwargs):\n        super(attention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n        super(attention, self).build(input_shape)\n\n    def call(self,x):\n        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n        at=K.softmax(et)\n        at=K.expand_dims(at,axis=-1)\n        output=x*at\n        return K.sum(output,axis=1)\n\n    def compute_output_shape(self,input_shape):\n        return (input_shape[0],input_shape[-1])\n\n    def get_config(self):\n        return super(attention,self).get_config()","82a2afd1":"inputs=keras.Input(shape=(max_length,))\nx=(e)(inputs)\natt_in=LSTM(100,return_sequences=True,dropout=0.3,recurrent_dropout=0.2)(x)\natt_out=attention()(att_in)\noutputs=Dense(1,activation='sigmoid',trainable=True)(att_out)\nmodelA=Model(inputs,outputs)\nmodelA.summary()","e0f6564b":"modelA.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","8db6a1bf":"hist=modelA.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=20,batch_size=64,verbose=1)","9a94a12f":"y_pred_attention= modelA.predict(X_val)","bedbca79":"\nfor i in range(len(y_pred_attention)):\n    if (y_pred_attention[i]>=0.5):\n        y_pred_attention[i]=1\n    else:\n        y_pred_attention[i]=0","12075410":"from sklearn.metrics import accuracy_score","37e5326f":"accuracy_attention= accuracy_score(y_pred_attention,y_val)","853c734b":"print(\"The accuracy for the attention model is {} %\".format(accuracy_attention*100))","eb7ee393":"plt.plot(hist.history['accuracy'], label='train accuracy')\nplt.plot(hist.history['val_accuracy'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","31a118cb":"accuracy_score= {'accuracy_lstm':accuracy_lstm,'accuracy_bilstm':accuracy_bilstm,'accuracy_gru':accuracy_gru,'accuracy_attention':accuracy_attention}","ecbe0e4a":"accuracy_score","ed9a6604":"import torch\nimport torch.nn as nn","322df0b7":"import transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader","0c945c49":"RANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)","d9dc7847":"PRE_TRAINED_MODEL_NAME = '..\/input\/bert-base-uncased'","bc26d280":"sample_txt = 'These are tough times we must stand together'\n","f6239556":"tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","73de3ef5":"tokens = tokenizer.tokenize(sample_txt)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(f' Sentence: {sample_txt}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","cba7ab17":"encoding = tokenizer.encode_plus(\n  sample_txt,\n  max_length=32,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=False,\n  pad_to_max_length=True,\n  return_attention_mask=True,\n  return_tensors='pt',  # Return PyTorch tensors\n)\nencoding.keys()","0ee112c0":"token_lens = []\nfor txt in data.text:\n    \n    tokens = tokenizer.encode(txt, max_length=512)\n    token_lens.append(len(tokens))\n","86520d8f":"import matplotlib.pyplot as plt\nimport seaborn as sns","57363fec":"sns.distplot(token_lens)\nplt.xlim([0, 256]);\nplt.xlabel('Token count')","fef699e4":"MAX_LEN=50","ed96a546":"class DisasterTweet(Dataset):\n    \n    def __init__(self, tweets, label, tokenizer, max_len):\n        \n        \n        self.tweets = tweets\n        self.label = label\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self):\n        return len(self.tweets)\n    def __getitem__(self, item):\n        \n        tweets = str(self.tweets[item])\n        label = self.label[item]\n        encoding = self.tokenizer.encode_plus(\n        tweets,\n        add_special_tokens=True,\n        max_length=self.max_len,\n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        return_attention_mask=True,\n        return_tensors='pt')\n        return {\n        'tweet_text': tweets,\n         'input_ids': encoding['input_ids'].flatten(),\n         'attention_mask': encoding['attention_mask'].flatten(),\n         'labels': torch.tensor(label, dtype=torch.long)\n          }    ","59fd52bc":"train= list(zip(train['text'],label))","a09c8529":"df = pd.DataFrame(train, columns = ['tweets', 'label'])","d70b62e4":"df.head()","3dc81406":"from sklearn.model_selection import train_test_split","6fe2935a":"train, val = train_test_split(\n  df,\n  test_size=0.1,\n  random_state=RANDOM_SEED\n)","8cc9301f":"def create_data_loader(data, tokenizer, max_len, batch_size):\n    \n    ds = DisasterTweet(tweets=data.tweets.to_numpy(),\n    label=data.label.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len)\n    return DataLoader(\n    ds,\n    batch_size=batch_size,\n    num_workers=4)\nBATCH_SIZE = 32\ntrain_data_loader = create_data_loader(train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(val, tokenizer, MAX_LEN, BATCH_SIZE)","f9e9f8e7":"train.shape,val.shape","4061312e":"df = next(iter(train_data_loader))\ndf.keys()","89a235ff":"print(df['input_ids'].shape)\nprint(df['attention_mask'].shape)\nprint(df['labels'].shape)\n","ddda7dfc":"bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)","908b0b54":"class FakeNewsClassifier(nn.Module):\n    \n    def __init__(self, n_classes):\n        \n        super(FakeNewsClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n    def forward(self, input_ids, attention_mask):\n        \n        _, pooled_output = self.bert(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n    )\n        output = self.drop(pooled_output)\n        return self.out(output)","2c282a6b":"n_classes= 2","94751b4c":"#setting device to GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","fd460ae6":"model = FakeNewsClassifier(n_classes)\nmodel = model.to(device)","9d17ad9f":"input_ids = df['input_ids'].to(device)\nattention_mask = df['attention_mask'].to(device)","5a08344e":"import torch.nn.functional as F","3fc468c3":"F.softmax(model(input_ids, attention_mask),dim=1)","ebf505a8":"model.parameters","c5b38707":"EPOCHS = 20\noptimizer = AdamW(model.parameters(), lr=3e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)","2284e376":"def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler, n_examples):  \n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    for d in data_loader:\n        \n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"labels\"].to(device)\n        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    return correct_predictions.double() \/ n_examples, np.mean(losses)","4725d755":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    \n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"labels\"].to(device)\n            outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n        return correct_predictions.double() \/ n_examples, np.mean(losses)","47a16966":"from collections import defaultdict","443d5f19":"%%time\nhistory = defaultdict(list)\nbest_accuracy = 0\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}\/{EPOCHS}')\n    print('-' * 10)\n    train_acc, train_loss = train_epoch(model,train_data_loader,loss_fn,optimizer,device,scheduler,len(train))\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    val_acc, val_loss = eval_model(model,val_data_loader,loss_fn,device,len(val))\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    if val_acc > best_accuracy:\n        \n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc","8003cc48":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","5fa40176":"best_accuracy","e8ebf98c":"Creating an Embedding matrix","2c94b221":"function to remove emoji","cf3d58bd":"# Attention Model\nattention in deep can be learning broadly interpreted as vector of importance weights, in order to predict ofinfer one element  such as pixel in an image or  a word in a sentence , we estimate using the attention  vector how stronglyit is correlating  witn other elements and take the sum of their  values aweighted by attention vector as the key approximation of the target\n\nhttps:\/\/lilianweng.github.io\/lil-log\/2018\/06\/24\/attention-attention.html","a6c4d686":"Importing GLOVE's word \nTo learn more-\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/pretrained-word-embeddings-nlp\/","0fe9ce89":"# Transformers:-\nThe Transformer \u2013 a model that uses attention to boost the speed with which these models can be trained\nThe Transformer was proposed in the paper Attention is All You Need.\nA TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard\u2019s NLP group created a guide annotating the paper with PyTorch implementation. \n**The Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease**\n\n* https:\/\/www.analyticsvidhya.com\/blog\/2019\/06\/understanding-transformers-nlp-state-of-the-art-models\/\n* http:\/\/jalammar.github.io\/illustrated-transformer\/\n* https:\/\/medium.com\/inside-machine-learning\/what-is-a-transformer-d07dd1fbec04","5c32456c":"Creating an Embedding layer","34e85fe2":"Importing data","f842ac15":"# NLP from Zero to Bert:-\nThis is a all in one notebook if you're new to Nlp or this competition this is your stop, In this notebook i'll apply from a very basic Lstm model and then gradually move to Google's Bert.\nSo this is your one spot for learning, fasten your seatbelt and let's begin our journey\n\n***PLease hit an upvote, this notebook has taken days of work to put it all in one place please upvote it so that I keep creating such Notebooks***\n\nPS- Some outputs very lengthy so they are hidden please expand for better understanding","bd94ab0d":"# GRUs\n\nTo solve the vanishing gradient problem of a standard RNN, GRU uses, so-called, update gate and reset gate. Basically, these are two vectors which decide what information should be passed to the output. The special thing about them is that they can be trained to keep information from long ago, without washing it through time or remove information which is irrelevant to the prediction.\n\n* https:\/\/towardsdatascience.com\/understanding-gru-networks-2ef37df6c9be\n* https:\/\/www.youtube.com\/watch?v=8HyCNIVRbSU","96983242":"# The End\n**If you liked my notebook Please show your support by upvoting it, It keeps me motivated to create more notebooks like this**\n\nAnd please share any improvements and feedback","47df90ce":"# BERT Transformer\n*BERT stands for Bidirectional Encoder Representations from Transformers. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. *\n\nPlease head to my notebook\nhttps:\/\/www.kaggle.com\/alankritamishra\/covid-19-tweet-sentiment-analysis\nIn this notebook I've thouroughly explained the working and understanding of BERT","55a74306":"# tokenization","b6b6fec7":"Splitting Data into training and validation data set","413444ef":"Creating Dataset","406393de":"**Comparing Accuracies**","fc5dd146":"Plotting training and validation accuracy","189732c2":"Predicting on a random sentence","ab719444":"Here I am concating data, to apply cleaning on both train and test set","17875d9d":"We get 81% accuracy which is pretty good.","fe95d8f4":"# Cleaning data:\nI would advise you to follow these steps as it, if you have better ideas please let me know","9d29dd24":"If you are unable to understand any code implementation\ncheck out \n* https:\/\/www.kaggle.com\/alankritamishra\/covid-19-tweet-sentiment-analysis#What-is-BERT?\n* https:\/\/towardsml.com\/2019\/09\/17\/bert-explained-a-complete-guide-with-theory-and-tutorial\/\n* https:\/\/www.curiousily.com\/posts\/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python\/","20861355":"Importing necessary Libraries...","e2e153ba":"To expand abbreviations","146b59cf":"Defining optimizer","8d04f40a":"# Content of the Notebook:-\n1. Data Cleaning\n2. Lstm\n3. Bidirectional lstm\n4. GRU\n5. Attention Model\n6. Prepration of dataset for Bert transformer\n7. Bert Transformer","994de017":"# Bidirectional LSTM\nBidirectional LSTMs are an extension of traditional LSTMs that can improve model performance on sequence classification problems.\n\nIn problems where all timesteps of the input sequence are available, Bidirectional LSTMs train two instead of one LSTMs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. This can provide additional context to the network and result in faster and even fuller learning on the problem.\n\n*  https:\/\/www.coursera.org\/lecture\/nlp-sequence-models\/bidirectional-rnn-fyXnn\n* https:\/\/machinelearningmastery.com\/develop-bidirectional-lstm-sequence-classification-python-keras\/#:~:text=Bidirectional%20LSTMs%20are%20an%20extension,LSTMs%20on%20the%20input%20sequence.","3dc1bfe7":"# Word Embedding \nA word embedding is a learned representation for text where words that have the same meaning have a similar representation.\n\nWord embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning.\n\nKey to the approach is the idea of using a dense distributed representation for each word.\n\nThe distributed representation is learned based on the usage of words. This allows words that are used in similar ways to result in having similar representations, naturally capturing their meaning\nhttps:\/\/machinelearningmastery.com\/what-are-word-embeddings\/#:~:text=A%20word%20embedding%20is%20a,challenging%20natural%20language%20processing%20problems.","67f491ed":"Making a basic LSTM model,LSTM stands for Long Short Term Memory networks\nHumans don\u2019t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words.Traditional neural networks can\u2019t do this, and it seems like a major shortcoming.\nRecurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.\nLong Short Term Memory networks \u2013 usually just called \u201cLSTMs\u201d \u2013 are a special kind of RNN, capable of learning long-term dependencies\n\nRefer to this blog for better understanding of LSTMs\nhttps:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/"}}