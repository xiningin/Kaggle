{"cell_type":{"1dae350c":"code","10a9b6e6":"code","4d134868":"code","dbbce33c":"code","f82f78f5":"code","3776e525":"code","35198249":"code","ad6b8be8":"code","141070b1":"code","d62621cf":"code","ca9065a5":"code","30b16a92":"code","24215402":"code","e726d875":"code","a988af3b":"code","4471b356":"code","0ea64ca1":"code","ead2051b":"code","92a659fc":"code","0f706f09":"code","dde8c4f8":"code","8e9333ac":"code","d35b16e8":"code","300e1ea3":"markdown","407e0184":"markdown","e8f08821":"markdown","643dd224":"markdown","514d2f02":"markdown","4e37dd63":"markdown","0a44462e":"markdown","22c1e424":"markdown"},"source":{"1dae350c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n#Extend the range of pandas colmns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 80)","10a9b6e6":"base_file_path = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\"\ntrain_df = pd.read_csv(base_file_path + \"train.csv\", index_col=\"Id\")\ntest_df = pd.read_csv(base_file_path + \"test.csv\", index_col=\"Id\")\nprint(train_df.shape)\ntrain_df.head()","4d134868":"from feature_selector_kaggle import FeatureSelector\ndependent_col = \"SalePrice\"\nfs_start = FeatureSelector(data = train_df.drop(dependent_col, axis=1), labels = train_df[dependent_col])\nfs_start.identify_missing(missing_threshold = 0.6)","dbbce33c":"fs_start.missing_stats.head()","f82f78f5":"fs_start.identify_single_unique()","3776e525":"#Remove columns found\nsingle_missing_cols = fs_start.check_removal()\ntest_df.drop(single_missing_cols, axis=1, inplace=True)\ntrain_df.drop(single_missing_cols, axis=1, inplace=True)","35198249":"num_cols = list(train_df._get_numeric_data().columns)\ncat_cols = list(set(train_df.columns) - set(num_cols))\nlen(num_cols) + len(cat_cols) == len(train_df.columns)","ad6b8be8":"#Fill in missing for cat\/num columns\n#Simple fill for now; Imputing could be more useful in the future\ncat_cols_dict = dict((col, 'Missing') for col in cat_cols)\nnum_cols_dict = dict((col, 0) for col in num_cols)\ntrain_df.fillna(value=dict(**cat_cols_dict, **num_cols_dict), inplace=True)\ntest_df.fillna(value=dict(**cat_cols_dict, **num_cols_dict), inplace=True)","141070b1":"#Verify filled\nnans_df = pd.DataFrame(train_df.isnull().sum(axis=0), columns=[\"na_count\"])\nnans_df[\"pct_missing\"] = nans_df[\"na_count\"] \/ len(train_df)\nnans_df[nans_df[\"na_count\"] > 0].sort_values(by=\"na_count\", ascending=False)","d62621cf":"#Combine categoricals that are observed below the ratio as \"other\"\ncat_replace_dict = {}\ndef get_categorical_combine(col,ratio):\n    obsv_count = len(col)\n    obsv_ratio = col.value_counts() \/  obsv_count\n    obsv_ratio_fail = obsv_ratio[obsv_ratio < ratio]\n    repalce_str = \"|\".join(obsv_ratio_fail.index)\n    cat_replace_dict[col.name] = repalce_str\n    return \n\ntrain_df[cat_cols].apply(get_categorical_combine,ratio=.05,axis=0)\ntrain_df_trim = train_df.replace(cat_replace_dict,\"other\", regex=True)\ntest_df_trim = test_df.replace(cat_replace_dict,\"other\", regex=True)","ca9065a5":"#Encode ordinal and categorical variables\nimport category_encoders as ce\ndef categorical_features(train_df: pd.DataFrame, test_df:pd.DataFrame, dependent_col: str, to_skip: list, num_cats: list, bayes:list):\n    #Drop skip list\n    train_df = train_df.drop(to_skip, axis=1)\n    test_df = test_df.drop(to_skip, axis=1)\n    \n    ##Save Dep Col\n    dep_col_series = train_df[dependent_col].copy()\n    \n    #Drop Dep Col for Encoding\n    train_df = train_df.drop(dependent_col, axis=1)\n\n    #Get list of categorical columns, only based on type\n    categorical = list(train_df.select_dtypes(\"object\").columns)\n    \n    #Add in those that are numerical categoricals (i.e, nieghborhood identifier)\n    categorical = categorical + num_cats\n\n    #Remove those used for Baysien Encoding\n    categorical = list(set(categorical) - set(bayes))\n    \n    #Check for harighly cardinal and use binary encoding\n    to_binary = []\n    for var in categorical:\n        if train_df[var].nunique() > 10:\n            to_binary.append(var)\n            print(\"Use Binary Encoder on \", var)\n\n    ##Binary\n    # instantiate an encoder - here we use Binary()\n    ce_binary = ce.BinaryEncoder(cols=to_binary)\n    ce_binary.fit(train_df)\n\n\n    #Use binary encoder\n    train_df = ce_binary.transform(train_df)\n    test_df = ce_binary.transform(test_df)\n\n    #Baysien\n    for var in bayes:\n        print(\"Use Baysien Encoder on \", var)\n\n    #Use fit bayes encoder\n    ce_james = ce.JamesSteinEncoder(cols=bayes)\n    ce_james.fit(train_df, dep_col_series)\n\n    # Use bayes encoder\n    train_df = ce_james.transform(train_df)\n    test_df = ce_james.transform(test_df)\n\n    #One Hot\n    one_hot_cols = set(categorical) - set(to_binary)\n    train_df = pd.get_dummies(train_df, columns=one_hot_cols)\n    test_df = pd.get_dummies(test_df, columns=one_hot_cols)\n\n    #Add dependent column back\n    train_df[dependent_col] = dep_col_series\n    \n    return train_df, test_df\n\ntrain_df_encoded, test_df_encoded = categorical_features(train_df_trim, test_df_trim, \"SalePrice\", [], [], [\"GarageCond\", \"OverallQual\",\"OverallCond\"])\ntrain_df_encoded.head()","30b16a92":"#Remove any columns not found in both dataframes after encoding\nunused_features_train = list(set(train_df_encoded.columns) - set(test_df_encoded.columns))\nunused_features_train.remove(\"SalePrice\")\ntrain_df_encoded.drop(unused_features_train, axis=1,inplace=True)\n\nunused_features_test = list(set(test_df_encoded.columns) - set(train_df_encoded.columns))\ntest_df_encoded.drop(unused_features_test, axis=1,inplace=True)","24215402":"fs = FeatureSelector(data = train_df_encoded.drop(dependent_col, axis=1), labels = train_df_encoded[dependent_col])\nfs.identify_collinear(correlation_threshold = 0.98)","e726d875":"# Pass in the appropriate parameters\nfs.identify_zero_importance(task = 'regression', \n                            eval_metric = 'auc', \n                            n_iterations = 10, \n                             early_stopping = True)","a988af3b":"#Collect columns to drop\ncolinear_noimportance_cols = fs.check_removal()\ntrain_df_encoded.drop(colinear_noimportance_cols, axis=1, inplace=True)\ntest_df_encoded.drop(colinear_noimportance_cols, axis=1, inplace=True)","4471b356":"from sklearn.ensemble import ExtraTreesClassifier\ndependent_col = \"SalePrice\"\nX = train_df_encoded.drop(dependent_col, axis=1)\ny = train_df_encoded[dependent_col]\nclf = ExtraTreesClassifier(n_estimators=100, random_state=0)\nclf.fit(X,y)\ny_hat = clf.predict(test_df_encoded)","0ea64ca1":"submit_df = pd.concat([pd.Series(test_df_encoded.index), pd.Series(y_hat)], axis=1)\nsubmit_df.columns = [\"Id\", \"SalePrice\"]\nsubmit_df.head()","ead2051b":"%%capture\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import (StratifiedKFold, KFold,\n                                     ParameterGrid)\nimport xgboost\nfrom sklearn.metrics import mean_squared_error\n\nCLASS = False  # Whether classification or regression\nSCORE_MIN = True  # Optimizing score through minimum\nk = 5  # Number of folds\nbest_score = 10\nbest_params = None\nbest_iter = None\n\n# CV\ntrain_feats = train_df_encoded.drop(dependent_col, axis=1)\ntarget = train_df_encoded[dependent_col]\ntest_feats = test_df_encoded\n\ntrain = np.array(train_feats)\ntarget = np.log(np.array(target))  # Changes to Log\ntest = np.array(test_feats)\nprint(train.shape, test.shape)\n\nif CLASS:\n    kfold = StratifiedKFold(target, k)\nelse:\n    kfold_base = KFold()\n    kfold = list(kfold_base.split(train))\n\nearly_stopping = 50\n\nparam_grid = [\n              {'verbosity': [0],\n               'nthread': [2],\n               'eval_metric': ['rmse'],\n               'eta': [0.03],\n               'objective': ['reg:linear'],\n               'max_depth': [5, 7],\n               'num_round': [1000],\n               'subsample': [0.2, 0.4, 0.6],\n               'colsample_bytree': [0.3, 0.5, 0.7],\n               }\n              ]\n\n# Hyperparmeter grid optimization\nparam_num = 0\nfor params in ParameterGrid(param_grid):\n    print(params)\n    # Determine best n_rounds\n    xgboost_rounds = []\n    for train_index, test_index in kfold:\n        X_train, X_test = train[train_index], train[test_index]\n        y_train, y_test = target[train_index], target[test_index]\n\n        xg_train = xgboost.DMatrix(X_train, label=y_train)\n        xg_test = xgboost.DMatrix(X_test, label=y_test)\n\n        watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n\n        num_round = params['num_round']\n        xgclassifier = xgboost.train(params, xg_train, num_round,\n                                     watchlist,\n                                     early_stopping_rounds=early_stopping);\n        xgboost_rounds.append(xgclassifier.best_iteration)\n\n    if len(xgboost_rounds) == 0:  \n        num_round = 0\n    else:\n        num_round = int(np.mean(xgboost_rounds))\n    \n    # Solve CV\n    rmsle_score = []\n    for cv_train_index, cv_test_index in kfold:\n        X_train, X_test = train[cv_train_index, :], train[cv_test_index, :]\n        y_train, y_test = target[cv_train_index], target[cv_test_index]\n\n        # train machine learning\n        xg_train = xgboost.DMatrix(X_train, label=y_train)\n        xg_test = xgboost.DMatrix(X_test, label=y_test)\n\n        watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n\n        xgclassifier = xgboost.train(params, xg_train, num_round);\n\n        # predict\n        predicted_results = xgclassifier.predict(xg_test)\n        rmsle_score.append(np.sqrt(mean_squared_error(y_test, predicted_results)))\n\n    if SCORE_MIN:\n        if best_score > np.mean(rmsle_score):\n            print(np.mean(rmsle_score))\n            print('new best')\n            best_score = np.mean(rmsle_score)\n            best_params = params\n            best_iter = num_round\n    else:\n        if best_score < np.mean(rmsle_score):\n            print(np.mean(rmsle_score))\n            print('new best')\n            best_score = np.mean(rmsle_score)\n            best_params = params\n            best_iter = num_round\n    \n    #Iterate param index \n    param_num+=1\n\n# Solution using best parameters\nprint('best params: %s' % best_params)\nprint('best score: %f' % best_score)\nxg_train = xgboost.DMatrix(train, label=target)\nxg_test = xgboost.DMatrix(test)\nwatchlist = [(xg_train, 'train')]\nnum_round = best_iter  # already int\nxgclassifier = xgboost.train(best_params, xg_train, num_round, watchlist);","92a659fc":"#Prepare XGBoost Submission\nsubmission_name = '..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv'\nsubmission_col = 'SalePrice'\nsubmission_target = 'xgboost.csv'\nsubmission = pd.read_csv(submission_name)\nsubmission[submission_col] = np.exp(xgclassifier.predict(xg_test))\nsubmission.to_csv(submission_target, index=False)\nsubmission.head()","0f706f09":"from tpot import TPOTRegressor\nX_train, X_test, y_train, y_test = train_test_split(train_df_encoded.drop(dependent_col, axis=1),\n                                                    train_df_encoded[dependent_col],\n                                                    train_size=0.75, test_size=0.25, random_state=42)\n\ntpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_boston_pipeline.py')","dde8c4f8":"submission[submission_col] = tpot.predict(test_feats)\nsubmission.head()","8e9333ac":"submission[submission_col] = np.exp(xgclassifier.predict(xg_test)) *.5 + tpot.predict(test_feats)*.5\nsubmission.head()","d35b16e8":"submission.to_csv(\"submit_mixed.csv\", index=False)","300e1ea3":"## Test for colinearity and model usefulness","407e0184":"## Load train and test files","e8f08821":"# Basic Feature Selection and Model Building\n\nThis notebook is a rapid run through basic feature seleciton and very simple model development. This is a great starting ground in order to get a competition under your belt and be able to start collecting medals on Kaggle","643dd224":"# Feature Selection\n\n## Remove features with only one value or many missing observations","514d2f02":"# Make Model - Simple ExtraTrees\nRather than consinuous tries to classify into buckets of values","4e37dd63":"## Drop unused features","0a44462e":"## Divide up categorical and numerical","22c1e424":"# Getting Started\n\n## Get listing of files in current input"}}