{"cell_type":{"b12b9589":"code","c9f91808":"code","4f5fa25b":"code","f82dec0d":"code","38abcba9":"code","95c78fa7":"code","9f8bf43a":"code","2083c980":"code","ff5e13e9":"code","bbd1cd64":"code","051df5f7":"code","ab4c26e1":"code","449805ee":"markdown","0f3675e6":"markdown","9e47f722":"markdown","ded1b7bd":"markdown","35fbaa1b":"markdown","e89c8b16":"markdown"},"source":{"b12b9589":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\nimport os\nfrom tqdm import tqdm\n\n# Input data files are available in the \"..\/input\/\" directory.\n#####################################################################################\n#thanks for your work vasuji https:\/\/www.kaggle.com\/vasuji\/i-covid19-nlp-data-parsing\n#####################################################################################\ndatafiles = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        ifile = os.path.join(dirname, filename)\n        if ifile.split(\".\")[-1] == \"json\":\n            datafiles.append(ifile)","c9f91808":"id2title = []\nfor file in datafiles:\n    with open(file,'r')as f:\n        doc = json.load(f)\n    id = doc['paper_id'] \n    title = doc['metadata']['title']\n    id2title.append({id:title})\n\nprint ('finish load title')\n\nid2abstract = []\nfor file in datafiles:\n    with open(file,'r')as f:\n        doc = json.load(f)\n    id = doc['paper_id'] \n    abstract = ''\n    try:\n        for item in doc['abstract']:\n            abstract = abstract + item['text']\n            \n        id2abstract.append({id:abstract})\n    except KeyError:\n        None\n    \nprint ('finish load abstract')    \n     \nid2bodytext = []\nfor file in datafiles:\n    with open(file,'r')as f:\n        doc = json.load(f)\n    id = doc['paper_id'] \n    bodytext = ''\n    try:\n        for item in doc['body_text']:\n            bodytext = bodytext + item['text']\n            \n        id2bodytext.append({id:bodytext})\n    except KeyError:\n        None\nprint ('finish load bodytext')","4f5fa25b":"!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_bionlp13cg_md-0.2.4.tar.gz\n!pip install scispacy","f82dec0d":"import scispacy\nimport en_ner_bionlp13cg_md\nfrom spacy import displacy\nfrom scispacy.abbreviation import AbbreviationDetector\nfrom scispacy.umls_linking import UmlsEntityLinker\nfrom nltk import tokenize","38abcba9":"nlp = en_ner_bionlp13cg_md.load()","95c78fa7":"sentence_list=[]\nsentence_list_without_col = []\nID_list = []\nent_type_all_check = []\nent_type_all= []\nfor i in tqdm(range(len(id2abstract[:100]))):\n    ID = list(id2abstract[i].keys())[0]\n    text = list(id2abstract[i].values())[0]\n    a = tokenize.sent_tokenize(text) # Split Sentence\n    for sent in a:\n        print_flag = False\n        doc = nlp(sent)\n        count_label =0\n        ent_type = ''\n        check_dupl =[]\n        sent_withcol = sent\n        sent_withoutcol = sent\n        for x in doc.ents:\n            if x.text not in check_dupl:\n                sent_withcol = sent_withcol.replace(x.text,f\"\\033[1;40m{x.text}\\033[1;31;40m ({x.label_}) \\033[00m\")\n                sent_withoutcol = sent_withoutcol.replace(x.text,f\"{x.text} *{x.label_}*\")\n                check_dupl.append(x.text)\n                print_flag =True\n\n            if x.label_ not in ent_type:\n                ent_type+= f'{x.label_}, '\n                ent_type_all_check.append(x.label_)\n                \n        if print_flag== True:\n            sentence_list.append('* '+sent_withcol)\n            sentence_list_without_col.append('* '+sent)\n            ent_type_all.append(ent_type)\n            ID_list.append(ID)","9f8bf43a":"len(ID_list),len(sentence_list),len(sentence_list_without_col),len(ent_type_all)","2083c980":"pd_all = pd.DataFrame({'Paper_ID':ID_list,'Sentence_col':sentence_list,'Sentence_wo_col':sentence_list_without_col,'NER_Key':ent_type_all})","ff5e13e9":"type_list = pd.Series(ent_type_all_check).unique().tolist() # Entity type","bbd1cd64":"from __future__ import print_function\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets","051df5f7":"def f(Paper_ID='all',Show_Many='100', Entity='ORGANISM', Show_original=False):\n    pd_all2 = pd_all.copy()[:int(Show_Many)]\n    if Paper_ID != 'all':\n        pd_all2 = pd_all[pd_all.Paper_ID==Paper_ID].copy()[:int(Show_Many)]\n    pd_all2 = pd_all2[pd_all2['NER_Key'].str.contains(Entity)].reset_index(drop=True)\n    for i in range(len(pd_all2)):\n        if Show_original==True:\n            print (pd_all2['Sentence_wo_col'][i])  \n        else:\n            print (pd_all2['Sentence_col'][i])    ","ab4c26e1":"interact(f, Paper_ID='all',Show_Many='100',Entity=type_list, Original = False);","449805ee":"## Ipywidgets for Visaulize the text","0f3675e6":"## Load Model ","9e47f722":"## Extract Key Sentence on all paper abstract\n* I didn't extract on bodytext, it takes too much time (exceed the kernel time limit)\n* Only run on 100 data, it's an example to use NER","ded1b7bd":"## Install SciSpacy\n* Package for NER model : https:\/\/allenai.github.io\/scispacy\/","35fbaa1b":"## Load Data\n* Update key error for abstract and bodytext","e89c8b16":"# Using NER model to extract Bio relate sentence\nUsing NER to extract key sentece, we can fast to read whole bunch of paper.\n* Using Scispacy ner model to extract bio entity\n* Mark the extract entity and extract the bio relate sentence\n* Visualize the sentence by using Ipywidgets\n\n## P.S. Ipywidget only enable in edit mode\n![image.png](attachment:image.png)"}}