{"cell_type":{"6b5a6353":"code","189eb528":"code","2de4b5cd":"code","0bc6936a":"code","6d5eda70":"code","3859e36a":"code","3c7ec023":"code","8026df6e":"code","4c9f067f":"code","a0f163b4":"code","79a59636":"code","c376bb6f":"code","9f898856":"code","35afe30c":"code","5e2dc49c":"code","c1c7893b":"code","9316673b":"code","834427f9":"code","72072041":"code","94768ef2":"code","21328c7b":"code","f458bffb":"code","06b0799e":"code","d7bfe2b8":"code","f0ff7fba":"code","d7425ef9":"code","ba584e1e":"code","4eb2a252":"code","c13faaa3":"code","c38829b0":"code","0c01f753":"code","acd64f6c":"code","e3e4b3cf":"code","10f30011":"code","c0f1bc62":"code","be8b9254":"code","420ee683":"code","f68c54f7":"code","5cfe42ba":"code","6451d79d":"code","f294f6c2":"code","12711c56":"code","4a12bd90":"code","562bfdc5":"markdown","a15684de":"markdown","c5f5cb4f":"markdown","02c53970":"markdown","6db39dc6":"markdown","05bcde06":"markdown","3dde1666":"markdown","9986f542":"markdown","470b068b":"markdown","784a8d6f":"markdown","98872dea":"markdown"},"source":{"6b5a6353":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom time import time\n\nfrom collections import Counter\nfrom itertools import combinations\n\nfrom sklearn.model_selection import cross_val_score,cross_validate, train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score, make_scorer\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\n\nfrom mlxtend.classifier import StackingCVClassifier, StackingClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier, plot_importance\nfrom catboost import CatBoostClassifier\n\nfrom tqdm import tqdm\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","189eb528":"train = pd.read_csv(\"..\/input\/learn-together\/train.csv\", index_col='Id')\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\", index_col='Id')\n\ny = train['Cover_Type'] # this is the target\nX = train.drop('Cover_Type', axis = 1)\nX_test = test.copy()\n\nprint('Train set shape : ', X.shape)\nprint('Test set shape : ', X_test.shape)\n\nX.rename({'Horizontal_Distance_To_Roadways':'HDR',\n              'Horizontal_Distance_To_Hydrology':'HDH',\n              'Horizontal_Distance_To_Fire_Points':'HDF',\n              'Vertical_Distance_To_Hydrology':'VDH'}, axis=\"columns\", inplace=True)\nX_test.rename({'Horizontal_Distance_To_Roadways':'HDR',\n              'Horizontal_Distance_To_Hydrology':'HDH',\n              'Horizontal_Distance_To_Fire_Points':'HDF',\n              'Vertical_Distance_To_Hydrology':'VDH'}, axis=\"columns\", inplace=True)\n\n\ncolumns = X.columns\n\nX.head()\n\n","2de4b5cd":"# Save test predictions to file\ndef to_submission(preds, file_name):\n    output = pd.DataFrame({'Id': X_test.index,\n                           'Cover_Type': preds})\n    output.to_csv(file_name+'.csv', index=False)\n","0bc6936a":"count = { 1: 0.37062,\n 2: 0.49657,\n 3: 0.05947,\n 4: 0.00106,\n 5: 0.01287, \n 6: 0.02698, \n 7: 0.03238} \nweight = [count[x]\/(sum(count.values())) for x in range(1,7+1)]\nclass_weight_lgbm = {i: v for i, v in enumerate(weight)}\n","6d5eda70":"def imbalanced_accuracy_score(y_true, y_pred):\n    return accuracy_score(y_true, y_pred, sample_weight=[weight[x] for x in y_true-1])\n\nimbalanced_accuracy_scorer = make_scorer(imbalanced_accuracy_score, greater_is_better=True)\n\ndef imbalanced_cross_validate(clf, X, y, cfg_args={}, fit_params={}, cv=5):\n    return cross_validate(clf, X, y, scoring= imbalanced_accuracy_scorer, cv=cv, n_jobs=-1, fit_params=fit_params )","3859e36a":"X.head()","3c7ec023":"X_test.head()","8026df6e":"print('Missing Label? ', y.isnull().any())\nprint('Missing train data? ', X.isnull().any().any())\nprint('Missing test data? ', X_test.isnull().any().any())","4c9f067f":"print (X.dtypes.value_counts())\nprint (X_test.dtypes.value_counts())","a0f163b4":"#transform Soil_Type into categorial\ndef categorify(df, col_string_search, remove_original=False):\n    for key_str in col_string_search:\n        new_col_name = key_str+'_cat'\n        df[new_col_name]=0\n        for col in columns:\n            if ~str(col).find(key_str):\n                df[new_col_name]= df[new_col_name]+int(str(col).lstrip(key_str))*df[col]\n                if remove_original:\n                    df.drop(col, axis=1, inplace=True)\n#         df[new_col_name] = df[new_col_name].astype('category')\n    return df","79a59636":"cols_to_categorify = ['Soil_Type', 'Wilderness_Area']\nX = categorify(X, cols_to_categorify, remove_original=True)\nX_test = categorify(X_test, cols_to_categorify, remove_original=True)\nX_test.head()","c376bb6f":"X.describe()","9f898856":"for col in X.columns:\n    plt.figure(figsize=(15,5))\n    sns.distplot(X[col])\n    plt.show()","35afe30c":"print(X.Hillshade_3pm[(X.Hillshade_3pm<130).to_numpy() &  (X.Hillshade_3pm>120).to_numpy()].value_counts())\nprint((X.Hillshade_3pm==0).sum())\nprint((X_test.Hillshade_3pm==0).sum())\n","5e2dc49c":"corr = X[X.Hillshade_3pm!=0].corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(corr,annot=True)","c1c7893b":"#replacing the zeros for better guess, mainly to avoid zeros in the feature engineering and fake outliers. \nnum_train=len(X)\ncols_for_HS = ['Aspect','Slope', 'Hillshade_9am','Hillshade_Noon']\nall_data = X.append(X_test)\nHS_zero = all_data[all_data.Hillshade_3pm==0]\nHS_zero.shape\n\nHS_train = all_data[all_data.Hillshade_3pm!=0]\n# res = cross_val_score(RandomForestRegressor(n_estimators=100), HS_train.drop('Hillshade_3pm',axis=1), HS_train.Hillshade_3pm, n_jobs=-1, verbose=True)\n# print(res) #[0.9996774  0.99989463 0.9999186 ]\n##actually, the CV is so close to zero there is actually no new information here..keeping it for simplicity\nrf_hs = RandomForestRegressor(n_estimators=100).fit(HS_train[cols_for_HS], HS_train.Hillshade_3pm)\nout = rf_hs.predict(HS_zero[cols_for_HS]).astype(int)\nall_data.loc[HS_zero.index,'Hillshade_3pm'] = out\nX= all_data[:num_train]\nX_test= all_data[num_train:]","9316673b":"#X.nunique()","834427f9":"\ndef quick_fe(df, cols, operations, max_combination=2):\n    \n    if max_combination>=2:\n        for col1, col2 in combinations(cols, 2):\n            for ope in operations:\n                if ope=='add': df[col1 + \"_add_\" + col2] = df[col1]+df[col2]\n                elif ope=='minus': df[col1 + \"_minus_\" + col2] = df[col1]-df[col2]\n                elif ope=='time': df[col1 + \"_time_\" + col2] = df[col1]*df[col2]\n    if max_combination>=3:\n        for col1, col2, col3 in combinations(cols, 3):\n            for ope in operations:\n                if ope=='add': df[col1 + \"_add_\" + col2 + \"_add_\" + col3] = df[col1]+df[col2]+df[col3]\n                elif ope=='time': df[col1 + \"_time_\" + col2+ \"_time_\" + col3] = df[col1]*df[col2]*df[col3]\n    return df\n\n\n\nX.head()","72072041":"\n\ndef feature_eng(dataset):\n    # https:\/\/www.kaggle.com\/nadare\/eda-feature-engineering-and-modeling-4th-359#nadare's-kernel\n    #https:\/\/www.kaggle.com\/lukeimurfather\/adversarial-validation-train-vs-test-distribution\n    #https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition\n    \n    cols_to_combine = ['HDH', 'HDF', 'HDR']\n    dataset = quick_fe(dataset, cols_to_combine, ['add','time','minus'], max_combination=3)\n    \n    dataset['Ele_vert'] = dataset.Elevation-dataset.VDH\n\n    dataset['Distance_hyd'] = (dataset['HDH']**2+dataset['VDH']**2)**0.5\n    \n    dataset['Slope_hyd'] = np.arctan(dataset['VDH']\/(dataset['HDH']+0.001))\n    dataset.Slope_hyd=dataset.Slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n    \n    dataset['Sin_Slope_hyd'] = np.sin(np.radians(dataset['Slope_hyd']))\n    dataset['Cos_Slope_hyd'] = np.cos(np.radians(dataset['Slope_hyd']))\n\n    dataset['Mean_Distance']=(dataset.HDF + \n                               dataset.Distance_hyd + \n                               dataset.HDR) \/ 3 \n\n    # Hillshade\n    hillshade_col = [\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]\n    dataset = quick_fe(dataset,hillshade_col, ['add','minus'], max_combination=3)\n\n    dataset[\"Hillshade_std\"] = dataset[hillshade_col].std(axis=1)\n    dataset[\"Hillshade_max\"] = dataset[hillshade_col].max(axis=1)\n    dataset[\"Hillshade_min\"] = dataset[hillshade_col].min(axis=1)\n\n#     dataset['Is_Overwater'] = dataset['VDH'] > 0   #This shouldn't be very useful for Trees?\n            \n#     dataset['Elevation_log'] = np.log1p(dataset['Elevation']) #This shouldn't be very useful for Trees?\n    \n    dataset['Aspect'] = dataset['Aspect'].astype(int) % 360\n    \n    dataset['Sin_Aspect'] = np.sin(np.radians(dataset['Aspect'])) # not important feature at all\n    dataset['Cos_Aspect'] = np.cos(np.radians(dataset['Aspect']))\n    \n    dataset['Sin_Slope'] = np.sin(np.radians(dataset['Slope'])) # not important feature at all\n    dataset['Cos_Slope'] = np.cos(np.radians(dataset['Slope']))\n    \n      \n    dataset['Elevation_Adj_distanceH'] = dataset['Elevation'] - 0.25*dataset['Distance_hyd']\n    dataset['Elevation_Adj_distanceV'] = dataset['Elevation'] - 0.19*dataset['HDH']\n\n    from bisect import bisect\n    cardinals = [i for i in range(45, 361, 90)]\n    points = ['N', 'E', 'S', 'W']\n    dataset['Cardinal'] = dataset.Aspect.apply(lambda x: points[bisect(cardinals, x) % 4])\n    dataset.loc[:,'North']= dataset['Cardinal']=='N'\n    dataset.loc[:,'East']= dataset['Cardinal']=='E'\n    dataset.loc[:,'West']= dataset['Cardinal']=='W'\n    dataset.loc[:,'South']= dataset['Cardinal']=='S'\n#     dataset.drop('Cardinal', axis=1, inplace=True)\n    \n    # extremely stony = 4, very stony = 3, stony = 2, rubbly = 1, None = 0\n    Soil_to_stony = [4, 3, 1, 1, 1, 2, 0, 0, 3, 1,\n                1, 2, 1, 0, 0, 0, 0, 0, 0, 0,\n                0, 4, 4, 4, 4, 4, 3, 4, 4, 4, \n                4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n    dataset['Stonyness'] = [Soil_to_stony[x] for x in (dataset['Soil_Type_cat'].astype(int)-1)]\n    dataset.loc[:,'Extremely_Stony']= dataset['Stonyness']==4\n    dataset.loc[:,'Very_Stony']= dataset['Stonyness']==3\n    dataset.loc[:,'Stony']= dataset['Stonyness']==2\n    dataset.loc[:,'Rubbly']= dataset['Stonyness']==1\n    dataset.loc[:,'Stony_NA']= dataset['Stonyness']==0\n#     dataset.drop('stonyness',axis=1,inplace=True)\n\n    \n    return dataset\n\nX = feature_eng(X)\nX_test = feature_eng(X_test)\ncolumns = X.columns","94768ef2":"# Frequency encoding\n#https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/108575#latest-628340\ndef freq_encoding(df_train, df_test, cols_to_encode):\n    df = pd.concat([df_train[cols_to_encode], df_test[cols_to_encode]],axis=0)\n    for col in cols_to_encode:\n        new_name = col+'_counts'\n        temp = df[col].value_counts().to_dict()\n        df[new_name] = df[col].map(temp)\n        df[new_name] = df[new_name].astype('int32')\n        df_train[new_name] = df.loc[:len(df_train),new_name]\n        df_test[new_name] = df.loc[len(df_train):,new_name]\n    return df_train, df_test","21328c7b":"selected_cols = ['Soil_Type_cat', 'Wilderness_Area_cat', 'Stonyness', 'Cardinal']\nX, X_test = freq_encoding(X, X_test, selected_cols)\n\nX.drop('Stonyness',axis=1,inplace=True)\nX.drop('Cardinal', axis=1, inplace=True)\nX_test.drop('Stonyness',axis=1,inplace=True)\nX_test.drop('Cardinal', axis=1, inplace=True)\n","f458bffb":"droping_list = []# [col for col in X.columns if ~str(col).find('Soil_Type')]\n\nX.drop(droping_list, axis=1, inplace = True)\nX_test.drop(droping_list, axis=1, inplace = True)\n\ncolumns = X.columns\nX_test.describe()","06b0799e":"num_train = X.shape[0]\nall_data = pd.concat([X, X_test])","d7bfe2b8":"# Add PCA features\nfrom sklearn.decomposition import PCA\n\nt = time()\n\npca = PCA(n_components=0.95).fit(StandardScaler().fit_transform(all_data))\ntrans = pca.transform(all_data)\nprint('duration: '+ str(time()-t))\nprint(trans.shape)","f0ff7fba":"# # https:\/\/www.kaggle.com\/stevegreenau\/stacking-multiple-classifiers-clustering\nfrom sklearn.mixture import GaussianMixture\n\nt = time()\ngmix = GaussianMixture(n_components=10) \ngaussian = gmix.fit_predict(StandardScaler().fit_transform(all_data))\n\nprint('duration: '+ str(time()-t))","d7425ef9":"X['Test_Cluster'] = gaussian[:num_train]  #  Do we need to OHE this ? \nX_test['Test_Cluster'] = gaussian[num_train:]#  Do we need to OHE this ? \nfor i in range(trans.shape[1]):\n    col_name= 'pca'+str(i+1)\n    X[col_name] = trans[:num_train, i]\n    X_test[col_name] = trans[num_train:, i]\n","ba584e1e":"def mem_reduce(df):\n    for col in df.columns:\n        if df[col].dtype=='float64': \n            df[col] = df[col].astype('float32')\n        if df[col].dtype=='int64': \n            if df[col].max()<128: df[col] = df[col].astype('int8')\n            elif df[col].max()<32768: df[col] = df[col].astype('int16')\n            else: df[col].astype('int32')\n    return df\n\n# X= mem_reduce(X)\n# X_test=mem_reduce(X_test)","4eb2a252":"X.describe()","c13faaa3":"#prepare df to store pred proba\nId_train=train.index\nId_test=test.index\n\nx_train_L2=pd.DataFrame(Id_train)\nx_test_L2=pd.DataFrame(Id_test)\n","c38829b0":"def L1_Training(clf, clf_name, cv=5, early_stop=False):\n    scores = []\n    clf_cul=[str(clf_name)+str(i+1) for i in range(7)]\n    for i in clf_cul:\n        x_train_L2.loc[:, i]=0\n        x_test_L2.loc[:, i]=0\n\n    clf_proba = np.zeros((X_test.shape[0], 7))\n    for train, val in tqdm(StratifiedKFold(n_splits=cv, shuffle=True, random_state=9999).split(X, y)): \n        X_train_loc = X.iloc[train,:]\n        X_val_loc = X.iloc[val,:]\n        y_train_loc = y.iloc[train]\n        y_val_loc = y.iloc[val]\n        if early_stop:\n            # fit the model  ##Do we need to reset the model in between loops??\n            clf.fit(X_train_loc, y_train_loc, \n                verbose=False,\n                eval_set=[(X_train_loc, y_train_loc), (X_val_loc, y_val_loc)], \n                early_stopping_rounds=50)\n            # use this fitted model to predict Test set.\n            clf_pred_proba_test = clf.predict_proba(X_test)\n            x_test_L2.loc[:, clf_cul] +=  clf_pred_proba_test\/ cv  #average over the CV rounds\n        else :\n            # when no early stoping the prediction of the Test set will be done once for all after (better use the full training set)\n            clf.fit(X_train_loc, y_train_loc)\n            \n        #checking validation\n        clf_pred_proba_val = clf.predict_proba(X_val_loc)\n        x_train_L2.loc[val, clf_cul]= clf_pred_proba_val\n        y_pred = clf.predict(X_val_loc)\n        scores.append(imbalanced_accuracy_score(y_pred,y_val_loc))\n        \n    if ~early_stop:\n        #retrain on full data\n        clf.fit(X,y)\n        clf_pred_proba_test = clf.predict_proba(X_test)\n        x_test_L2.loc[:, clf_cul] = clf_pred_proba_test\n        \n    clf_pred_test = x_test_L2.loc[:,clf_cul].to_numpy().argmax(axis=1)+1\n    return scores, clf_pred_test\n","0c01f753":"xgb= XGBClassifier( n_estimator= 500, \n                    learning_rate= 0.1, \n                    max_depth= 50,  \n                    objective= 'binary:logistic', #this outputs probability,not one\/zero. should we use binary:hinge? is it better for the learning phase?\n                    random_state= 2019,\n                    sample_weight=count,\n                    n_jobs=-1)\nlgbc= LGBMClassifier(n_estimators=500,  \n                     learning_rate= 0.1,\n                     objective= 'multiclass', \n                     num_class=7,\n                     random_state= 2019,\n                     n_jobs=-1,\n                     class_weight=class_weight_lgbm)\nrf = RandomForestClassifier(n_estimators = 1000, \n                            max_features = 0.3, \n                            max_depth = 100, \n                            min_samples_split = 2, \n                            min_samples_leaf = 1,\n                            bootstrap = False,\n                            random_state=2019,\n                            class_weight=count)\nxtc= ExtraTreesClassifier(n_estimators = 750, \n                            max_features = 0.3, \n                            max_depth = None, \n                            min_samples_split = 2, \n                            min_samples_leaf = 1,\n                            bootstrap = False,\n                            random_state=2019, \n                            class_weight=count)\n# cat= CatBoostClassifier(n_estimators = 1000,\n#                         cat_features= ['Soil_Type_cat', 'Wilderness_Area_cat', 'Stonyness', 'Cardinal'],\n#                         learning_rate= 0.1, \n#                         random_state=2019,\n# #                         class_weights=weight,# ?? \n#                         verbose = 50) \nlr= LogisticRegression(max_iter=1000,\n                       n_jobs=-1,\n                       solver= 'lbfgs',\n                       multi_class = 'multinomial',\n                       class_weight=count)","acd64f6c":"# cat_score, cat_preds =  L1_Training(cat, 'cat', cv=8, early_stop=True) \n# to_submission(cat_preds, 'cat_Kfold_sub')\n# print(np.mean(cat_score))","e3e4b3cf":"\n# xgb_score, xgb_preds = L1_Training(xgb, 'xgb', cv=8, early_stop=True) \n# to_submission(xgb_preds, 'xgb_Kfold_sub')\n# print(np.mean(xgb_score))\n# #v11  LB:0.76478","10f30011":"# xgb.fit(X,y)\n# xgb_test_pred_full = xgb.predict(X_test)\n# to_submission(xgb_test_pred_full, 'xgb_full_sub')\n","c0f1bc62":"\n# lgbc_score, lgb_preds = L1_Training(lgbc, 'lgbc', cv=8, early_stop=True) \n# to_submission(lgb_preds, 'lgb_Kfold_sub')\n# print(np.mean(lgbc_score))\n# # v11 LB : 0.79057 (with GaussianMixture 10, without PCA)","be8b9254":"# lgbc.fit(X,y)\n# lgbc_test_pred_full = lgbc.predict(X_test)\n# to_submission(lgbc_test_pred_full, 'lgbc_full_sub')\n# # v11 LB : 0.79057 (with GaussianMixture 10, without PCA)","420ee683":"\n# rf_score, rf_preds = L1_Training(rf, 'rf', cv=8, early_stop=False) \n# to_submission(rf_preds, 'rf_sub')\n# print(np.mean(rf_score))\n## v12 LB:0.76981 (with GaussianMixture 10, without PCA)","f68c54f7":"\n# xtc_score, xtc_preds = L1_Training(xtc, 'xtc', cv=8, early_stop=False)\n# to_submission(xtc_preds, 'xtc_sub')\n# print(np.mean(xtc_score))\n# #v12 LB:0.78579 (with GaussianMixture 10, without PCA)","5cfe42ba":"\n# lr_score, lr_preds = L1_Training(lr, 'lr', cv=8, early_stop=False) \n# to_submission(lr_preds, 'lr_sub')\n# print(np.mean(lr_score))","6451d79d":"# preds_test = l2_lgbc.predict(x_test_L2).argmax(axis=1)+1\n# preds_sum = np.concatenate([xtc_preds, lgb_preds, xgb_preds, rf_preds])\n# to_submission(preds_sum, 'sum_preds_sub')","f294f6c2":"#redefine clf because the weight format doesnt match the StackingClassifier..\nxgb= XGBClassifier( n_estimator= 500, \n                    learning_rate= 0.1, \n                    max_depth= 50,  \n                    objective= 'binary:logistic', #this outputs probability,not one\/zero. should we use binary:hinge? is it better for the learning phase?\n                    random_state= 2019,\n                    n_jobs=-1)\nlgbc= LGBMClassifier(n_estimators=500,  \n                     learning_rate= 0.1,\n                     objective= 'multiclass', \n                     num_class=7,\n                     random_state= 2019,\n                     n_jobs=-1)\nlgbc2= LGBMClassifier(n_estimators=500,  \n                     learning_rate= 0.1,\n                     objective= 'multiclass', \n                     num_class=7,\n                     random_state= 888,\n                     n_jobs=-1)\nlgbc3= LGBMClassifier(n_estimators=750,  \n                     learning_rate= 0.1,\n                     objective= 'multiclass', \n                     num_class=7,\n                     random_state= 666,\n                     n_jobs=-1)\nrf = RandomForestClassifier(n_estimators = 1000, \n                            max_features = 0.3, \n                            max_depth = 100, \n                            min_samples_split = 2, \n                            min_samples_leaf = 1,\n                            bootstrap = False,\n                            random_state=2019)\nxtc= ExtraTreesClassifier(n_estimators = 750, \n                            max_features = 0.3, \n                            max_depth = None, \n                            min_samples_split = 2, \n                            min_samples_leaf = 1,\n                            bootstrap = False,\n                            random_state=2019)\nxtc2= ExtraTreesClassifier(n_estimators = 550, \n                            max_features = 0.3, \n                            max_depth = None, \n                            min_samples_split = 2, \n                            min_samples_leaf = 1,\n                            bootstrap = False,\n                            random_state=888)\nxtc3= ExtraTreesClassifier(n_estimators = 750, \n                            max_features = 0.3, \n                            max_depth = None, \n                            min_samples_split = 2, \n                            min_samples_leaf = 1,\n                            bootstrap = False,\n                            random_state=777)\n# lr= LogisticRegression(max_iter=1000,\n#                        n_jobs=-1)\n\n\n### ugly double stack.. just to check improvements of the hillshade fix.. to be removed...\nstack = StackingCVClassifier(classifiers=[lgbc,xtc,rf, xgb],#[lgbc, lgbc2, lgbc3, xtc, xtc2, xtc3, rf, xgb],\n                             meta_classifier=lgbc,\n                             use_probas=True,\n                             use_features_in_secondary=True,\n                             verbose=1,\n                             random_state=888,\n                             n_jobs=-1,\n                             cv =8)\n\n# stack_score, stack_preds = L1_Training(stack, 'stack', cv=8, early_stop=False) \n# # to_submission(lr_preds, 'lr_sub')\n# print(np.mean(stack_score))\n\nstack.fit(X,y)\n\n\nstack2 = StackingCVClassifier(classifiers=[lgbc, lgbc2, lgbc3, xtc, xtc2, xtc3, rf, xgb],\n                             meta_classifier=lgbc,\n                             use_probas=True,\n                             use_features_in_secondary=True,\n                             verbose=1,\n                             random_state=888,\n                             n_jobs=-1,\n                             cv =8)\n\n# stack_score, stack_preds = L1_Training(stack, 'stack', cv=8, early_stop=False) \n# # to_submission(lr_preds, 'lr_sub')\n# print(np.mean(stack_score))\n\nstack2.fit(X,y)\n","12711c56":"stack_preds = stack.predict(X_test.to_numpy())\nto_submission(stack_preds, 'stack_preds_sub')","4a12bd90":"stack2_preds = stack2.predict(X_test.to_numpy())\nto_submission(stack2_preds, 'stack2_preds_sub')","562bfdc5":"#Import datasets","a15684de":"# Model generation","c5f5cb4f":"Note : large difference between train and test size. Will need to check input distributions.","02c53970":"Hill_shade_3pm is missing ~30 values at 126. \nHill_shade_3pm has 88 values equal to 0 which probably should not. 1250 zeros in the test set.","6db39dc6":"No missing data, everything in numeric. \nSoil_type and Wilderness_area are categorial data already put as one hot encoded.","05bcde06":"This notebook follows https:\/\/www.kaggle.com\/arateris\/xgb-rf-with-gridsearch-for-forest-classifier\/ that looked for hyper-paramters and https:\/\/www.kaggle.com\/arateris\/probing-stats\/ for Test set stats","3dde1666":"# feature engineering,\nTODO : data cleaning","9986f542":"# Test label distribution\nhttps:\/\/www.kaggle.com\/arateris\/probing-stats\/\nThis is to know the distribution of the labels in the (public) test set. This allows to get a better accuracy check and validation during tuning\/training phase.\n","470b068b":"scaler = StandardScaler()  #--> Standard Scaler ?\nX.loc[:,:] = scaler.fit_transform(X)\nX_test.loc[:,:] = scaler.transform(X_test)\n","784a8d6f":"xgb= XGBClassifier( n_estimator= 500, \n                    learning_rate= 0.1, \n                    max_depth= 10,  \n                    objective= 'binary:logistic', #this outputs probability,not one\/zero. should we use binary:hinge? is it better for the learning phase?\n                    random_state= 2019,\n                    sample_weight=count,\n                    n_jobs=-1)\nrf = RandomForestClassifier(n_estimators = 719, \n                            max_features = 0.3, \n                            max_depth = 464, \n                            min_samples_split = 2, \n                            min_samples_leaf = 1,\n                            bootstrap = False,\n                            random_state=2019, \n                            class_weight=count)\nlgbc= LGBMClassifier(n_estimators=500,  \n                     max_depth= 25,\n                     learning_rate= 0.5,\n                     objective= 'multiclass', \n                     num_class=7,\n                     random_state= 2019,\n                     n_jobs=-1,\n                     class_weight=class_weight_lgbm)\nextc= ExtraTreesClassifier( n_estimators=1000,  #todo : search for good parameters\n                            random_state= 1,\n                            n_jobs=-1,\n                            class_weight=count)\nlr = LogisticRegression()","98872dea":"List of classifiers\n- XGBClassifier\n-- Params: {'n_estimators': 500, 'learning_rate': 0.1, 'max_depth': 10} ?\n-- n_estimators = 719, max_depth = 464 https:\/\/www.kaggle.com\/phsheth\/forestml-part-6-stacking-eval-selected-fets-2  https:\/\/www.kaggle.com\/joshofg\/pure-random-forest-hyperparameter-tuning\n- RFClassifier\n-- {'max_depth': 100, 'max_features': 0.3, 'n_estimators': 2000}  https:\/\/www.kaggle.com\/arateris\/xgb-rf-with-gridsearch-for-forest-classifier\/\n-- Params: {n_estimators = 719, max_features = 0.3, max_depth = 464, min_samples_split = 2, min_samples_leaf = 1, bootstrap = False} https:\/\/www.kaggle.com\/joshofg\/pure-random-forest-hyperparameter-tuning\n- ExtraTrees\n-- Params : n_estimators = 750, max_features = 0.3, max_depth = None,  https:\/\/www.kaggle.com\/arateris\/xgb-rf-with-gridsearch-for-forest-classifier\/\n- LGBM \n-- Params : n_estimators=400,  num_leaves=100  ?  https:\/\/www.kaggle.com\/stevegreenau\/stacking-multiple-classifiers-clustering\n-- {'learning_rate': 0.5, 'max_depth': 25, 'n_estimators': 500}  https:\/\/www.kaggle.com\/arateris\/xgb-rf-with-gridsearch-for-forest-classifier\/\n- ADABoost \n-- Params : {max_depth  = 464, min_samples_split = 2, min_samples_leaf = 1,}  https:\/\/www.kaggle.com\/phsheth\/forestml-part-6-stacking-eval-selected-fets-2\n\n"}}