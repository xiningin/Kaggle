{"cell_type":{"890b483c":"code","b7953763":"code","1664e418":"code","2c993d63":"code","efb9cf22":"code","65fc57e5":"code","24666212":"code","049c2205":"code","2204313b":"code","2176f264":"code","aca65d3b":"code","d7e44377":"code","8e619b40":"code","74371617":"code","8f1bf49f":"code","c7d4d26b":"code","a55cd1b0":"code","dad9464c":"code","7050c3ea":"code","4678b29a":"code","d00377ba":"code","fea2c905":"code","c7985b94":"code","627b9346":"code","67e055c6":"code","b2405e66":"code","5cd26c59":"code","13700e0c":"code","c973d70f":"code","42f1f3b1":"code","16ff754f":"markdown","311b054c":"markdown"},"source":{"890b483c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b7953763":"sample_submission = pd.read_csv('\/kaggle\/input\/california-house-prices\/sample_submission.csv')\ntrain_data = pd.read_csv('\/kaggle\/input\/california-house-prices\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/california-house-prices\/test.csv')","1664e418":"sample_submission.shape, train_data.shape, test_data.shape","2c993d63":"df=pd.read_csv('\/kaggle\/input\/california-house-prices\/train.csv')\ndf.info()","efb9cf22":"df_train=df.copy()\ndf_test=pd.read_csv('\/kaggle\/input\/california-house-prices\/test.csv')\nfor field in ['Listed On', 'Last Sold On']:\n    df_train[field]=pd.to_datetime(df[field])\n    df_test[field]=pd.to_datetime(df_test[field])","65fc57e5":"cate_cols = []\nnum_cols = []\ndate_cols = []\ndtypes = df_train.dtypes\nfor col, dtype in dtypes.items():\n    if dtype=='object':\n        cate_cols.append(col)\n    elif dtype.name.startswith('datetime'):\n        date_cols.append(col)\n    else:\n        num_cols.append(col)","24666212":"id_col = 'Id'\ntarget_col = 'Sold Price'\n\nfor col in [id_col, target_col]:\n    num_cols.remove(col)\nprint(cate_cols)\nprint(num_cols)\nprint(date_cols)","049c2205":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype\n\nclass Num_Features(BaseEstimator, TransformerMixin):\n    def __init__(self, cols = [], fillna = False, addna = False):\n        self.fillna = fillna\n        self.cols = cols\n        self.addna = addna\n        self.na_cols = []\n        self.imputers = {}\n    def fit(self, X, y=None):\n        for col in self.cols:\n            if self.fillna:\n                self.imputers[col] = X[col].median()\n            if self.addna and X[col].isnull().sum():\n                self.na_cols.append(col)\n        print(self.na_cols, self.imputers)\n        return self\n    def transform(self, X, y=None):\n        df = X.loc[:, self.cols]\n        for col in self.imputers:\n            df[col].fillna(self.imputers[col], inplace=True)\n        for col in self.na_cols:\n            df[col+'_na'] = pd.isnull(df[col])\n        return df","2204313b":"class Imputer(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy, fill_value):\n        self.strategy = strategy\n        self.fill_value = fill_value\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        for col, content in X.items():\n            X[col].fillna(self.fill_value, inplace=True)\n        return X","2176f264":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, StandardScaler\nfrom sklearn.impute import SimpleImputer\nnum_pipeline = Pipeline([\n    ('select_num', Num_Features(cols=num_cols, fillna='median', addna=True)),\n])\nX_num = num_pipeline.fit_transform(df_train)","aca65d3b":"class CatEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self,cols, max_n_cat=7, onehot_cols=[], orders={}):\n        self.cols = cols\n        self.onehot_cols=onehot_cols\n        self.cats = {}\n        self.max_n_cat = max_n_cat\n        self.orders = orders\n    def fit(self, X, y=None):\n        df_cat =  X.loc[:, self.cols]\n        for n,c in df_cat.items():\n            df_cat[n].fillna('NAN', inplace=True)\n            df_cat[n] = c.astype('category').cat.as_ordered()\n            if n in self.orders:\n                df_cat[n].cat.set_categories(self.orders[n], ordered=True, inplace=True)\n            cats_count = len(df_cat[n].cat.categories)\n            if cats_count<=2 or cats_count>self.max_n_cat:\n                self.cats[n] = df_cat[n].cat.categories\n                if n in self.onehot_cols:\n                    self.onehot_cols.remove(n)\n            elif n not in self.onehot_cols:\n                self.onehot_cols.append(n)\n\n        print(self.onehot_cols)\n        return self\n    def transform(self, df, y=None):\n        X = df.loc[:, self.cols]\n        for col in self.cats:\n            X[col].fillna('NAN', inplace=True)\n            X.loc[:,col] = pd.Categorical(X[col], categories=self.cats[col], ordered=True)\n            X.loc[:,col] = X[col].cat.codes\n\n#         for n,c in X.items():\n#             if n in self.cats:\n#                 X[n] = pd.Categorical(c, categories=self.cats[n], ordered=True)\n#                 X[n] = X[n].cat.codes + 1\n#             else:\n#                 X[n] = c.astype('category').cat.as_ordered()\n        if len(self.onehot_cols):\n            df_1h = pd.get_dummies(X[self.onehot_cols], dummy_na=True)\n            df_drop=X.drop(self.onehot_cols,axis=1)\n            return pd.concat([df_drop, df_1h], axis=1)\n\n        return X","d7e44377":"cat_pipeline = Pipeline([\n    ('cat_encoder', CatEncoder(cols=cate_cols))\n])\nX_cate = cat_pipeline.fit_transform(df_train)","8e619b40":"def add_datepart(df, field_name, prefix=None, drop=True, time=False):\n    field = df[field_name]\n    if prefix is None:\n        prefix = re.sub('[Dd]ate$', '', field_name)\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    # Pandas removed `dt.week` in v1.1.10\n    week = field.dt.isocalendar().week.astype(field.dt.day.dtype) if hasattr(field.dt, 'isocalendar') else field.dt.week\n    for n in attr: df[prefix + n] = getattr(field.dt, n.lower()) if n != 'Week' else week\n    mask = ~field.isna()\n    df[prefix + 'Elapsed'] = np.where(mask,field.values.astype(np.int64) \/\/ 10 ** 9,np.nan)\n    if drop: df.drop(field_name, axis=1, inplace=True)\n    return df","74371617":"import re\nclass Datepart(BaseEstimator, TransformerMixin):\n    def __init__(self, cols, time=False):\n        self.cols = cols\n        self.time = time\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        df_dates = X.loc[:, self.cols]\n        for col in self.cols:\n            add_datepart(df_dates, col, time=False)\n        return df_dates\n    \ndate_pipeline = Pipeline([\n    ('datepart', Datepart(cols=date_cols)),\n    ('imputer', Imputer(strategy=\"constant\", fill_value=-1)),\n])","8f1bf49f":"X_date = date_pipeline.fit_transform(df_train)","c7d4d26b":"y_train = np.log(df_train[target_col])\nX_train = pd.concat([X_num, X_cate,X_date], axis=1)\nX_train.shape, y_train.shape","a55cd1b0":"from sklearn.model_selection import RandomizedSearchCV\nimport lightgbm as lgb\nlgbmodel = lgb.LGBMRegressor()\n\nparam_grid = {\n    'boosting_type': ['gbdt'],\n    'objective': ['regression'],\n    'metric': ['auc'],\n    'nthread':[4],\n    \"n_estimators\" : np.arange(50, 300, 50),\n    \"learning_rate\" :[0.01,0.05,0.1,0.3],\n    \"max_depth\": np.arange(2,8,1),\n    'colsample_bytree':[0.6,0.8,1],\n    'lambda_l1': [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0],\n    'lambda_l2': [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0],\n    'feature_fraction': [0.6,0.7,0.8,0.9,1.0],\n    'bagging_fraction': [0.6,0.7,0.8,0.9,1.0],\n    'bagging_freq': np.arange(0,81,10),\n    'max_bin': np.arange(5,256,10),\n    'min_data_in_leaf':np.arange(1,101,10),\n    'num_leaves':np.arange(5, 100, 5),\n    'min_split_gain':[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n}\nlgbmodel = RandomizedSearchCV(estimator = lgbmodel ,\n                          param_distributions = param_grid,\n                          n_iter = 100,\n                          verbose=1,\n                          n_jobs = -1,\n                          cv = 5)\nlgbmodel.fit(X_train,y_train)\nlgbmodel.best_score_","dad9464c":"lgbmodel.best_estimator_.get_params()","7050c3ea":"modellgb = lgb.LGBMRegressor(boosting_type='gbdt',objective='regression',colsample_bytree=1,metrics='auc',learning_rate=lgbmodel.best_estimator_.get_params()['learning_rate'], n_estimators=lgbmodel.best_estimator_.get_params()['n_estimators'], max_depth=lgbmodel.best_estimator_.get_params()['max_depth'],num_leaves = lgbmodel.best_estimator_.get_params()['num_leaves'],max_bin = lgbmodel.best_estimator_.get_params()['max_bin'],min_child_samples=lgbmodel.best_estimator_.get_params()['min_child_samples'],min_child_weight=0.001,min_split_gain=lgbmodel.best_estimator_.get_params()['min_split_gain'],min_data_in_leaf = lgbmodel.best_estimator_.get_params()['min_data_in_leaf'], bagging_fraction = lgbmodel.best_estimator_.get_params()['bagging_fraction'],feature_fraction = lgbmodel.best_estimator_.get_params()['feature_fraction'],bagging_freq=lgbmodel.best_estimator_.get_params()['bagging_freq'],lambda_l1=lgbmodel.best_estimator_.get_params()['lambda_l1'],lambda_l2=lgbmodel.best_estimator_.get_params()['lambda_l2'])\n  \nmodellgb.fit(X_train,y_train)","4678b29a":"def rf_feat_importance(modellgb, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':modellgb.feature_importances_}).sort_values('imp', ascending=False)\n\nfi = rf_feat_importance(modellgb, X_train)\nfi[:50]","d00377ba":"import shap  #\u5c06\u535a\u5f08\u8bba\u4e0e\u7279\u5f81\u6743\u91cd\u589e\u76ca\u7ec4\u5408  SHAP\u503c\u662f\u5728\u8fd9\u4e9b\u7279\u5f81\u4e4b\u95f4\u7684\u516c\u5e73\u7684\u4fe1\u7528\u5206\u914d\uff0c\u5e76\u4e14\u5177\u6709\u535a\u5f08\u8bba\u4e00\u81f4\u6027\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u901a\u5e38\u6bd4\u6574\u4e2a\u6570\u636e\u96c6\u4e2d\u7684\u90a3\u4e9b\u5178\u578b\u7279\u5f81\u7684\u91cd\u8981\u6027\u66f4\u503c\u5f97\u4fe1\u8d56\u3002\nshap.initjs()   \n\nexplainer = shap.TreeExplainer(modellgb)\nshap_values = explainer(X_train)\nshap.plots.waterfall(shap_values[0])","fea2c905":"shap.plots.beeswarm(shap_values,max_display=18)","c7985b94":"shap.plots.bar(shap_values,max_display=19)","627b9346":"# \u6700\u597d\u63d0\u4ea4\u7684\u7279\u5f81\uff0c18\u4e2a\nto_keep_final=['Listed Price', \n               'Tax assessed value',\n               'Annual tax amount',\n               'Listed OnElapsed',\n               'Last Sold Price', \n               'Zip', \n               'Parking', \n               'Year built',\n               'Total interior livable area', \n               'Type',\n               'Elementary School Score',\n               'Listed OnYear',\n               'Elementary School Distance',\n               'Last Sold OnElapsed',\n               'Middle School Score',\n               'Lot',\n               'Appliances included',\n               'Listed OnDayofyear']\n# to_keep_final=['Listed Price', 'Tax assessed value', 'Last Sold Price', 'Zip', 'Total interior livable area', 'Listed OnElapsed', 'Elementary School Score', 'Last Sold OnElapsed', 'Year built', 'Listed OnYear', 'High School Distance', 'Lot', 'Parking', 'Middle School Score', 'Elementary School Distance', 'Region', 'Bedrooms', 'High School Score', 'Heating', 'Appliances included', 'Flooring', 'Middle School Distance']\nX_train_final = X_train[to_keep_final].copy()","67e055c6":"#2nd  model train\nfrom sklearn.model_selection import RandomizedSearchCV\nimport lightgbm as lgb\nlgbmodel = lgb.LGBMRegressor()\n\nparam_grid = {\n    'boosting_type': ['gbdt'],\n    'objective': ['regression'],\n    'metric': ['auc'],\n    'nthread':[4],\n    \"n_estimators\" : np.arange(50, 300, 50),\n    \"learning_rate\" :[0.01,0.05,0.1,0.3],\n    \"max_depth\": np.arange(2,8,1),\n    'colsample_bytree':[0.6,0.8,1],\n    'lambda_l1': [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0],\n    'lambda_l2': [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0],\n    'feature_fraction': [0.6,0.7,0.8,0.9,1.0],\n    'bagging_fraction': [0.6,0.7,0.8,0.9,1.0],\n    'bagging_freq': np.arange(0,81,10),\n    'max_bin': np.arange(5,256,10),\n    'min_data_in_leaf':np.arange(1,101,10),\n    'num_leaves':np.arange(5, 100, 5),\n    'min_split_gain':[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n}\nlgbmodel = RandomizedSearchCV(estimator = lgbmodel ,\n                          param_distributions = param_grid,\n                          n_iter = 100,\n                          verbose=1,\n                          n_jobs = -1,\n                          cv = 5)\nlgbmodel.fit(X_train_final,y_train)\nlgbmodel.best_score_","b2405e66":"lgbmodel.best_estimator_.get_params()","5cd26c59":"best_modellgb = lgb.LGBMRegressor(boosting_type='gbdt',objective='regression',colsample_bytree=1,metrics='auc',learning_rate=lgbmodel.best_estimator_.get_params()['learning_rate'], n_estimators=lgbmodel.best_estimator_.get_params()['n_estimators'], max_depth=lgbmodel.best_estimator_.get_params()['max_depth'],num_leaves = lgbmodel.best_estimator_.get_params()['num_leaves'],max_bin = lgbmodel.best_estimator_.get_params()['max_bin'],min_child_samples=lgbmodel.best_estimator_.get_params()['min_child_samples'],min_child_weight=lgbmodel.best_estimator_.get_params()['min_child_weight'],min_split_gain=lgbmodel.best_estimator_.get_params()['min_split_gain'],min_data_in_leaf = lgbmodel.best_estimator_.get_params()['min_data_in_leaf'], bagging_fraction = lgbmodel.best_estimator_.get_params()['bagging_fraction'],feature_fraction = lgbmodel.best_estimator_.get_params()['feature_fraction'],bagging_freq=lgbmodel.best_estimator_.get_params()['bagging_freq'],lambda_l1=lgbmodel.best_estimator_.get_params()['lambda_l1'],lambda_l2=lgbmodel.best_estimator_.get_params()['lambda_l2'])\n  \nbest_modellgb.fit(X_train_final,y_train)","13700e0c":"import graphviz #\u5b89\u88c5\u8fc7\u7a0b\u6bd4\u8f83\u56f0\u96be\uff0c\u8010\u5fc3\nlgb.create_tree_digraph(best_modellgb,tree_index=0,orientation='vertical')","c973d70f":"X_test_num = num_pipeline.transform(df_test)\nX_test_cate = cat_pipeline.transform(df_test)\nX_test_date = date_pipeline.transform(df_test)\ndf_t = pd.concat([X_test_num, X_test_cate, X_test_date], axis=1)\ndf_t = df_t[to_keep_final]","42f1f3b1":"pred=best_modellgb.predict(df_t)\ndf_pred=pd.DataFrame({'Id':df_test['Id'],'Sold Price': np.exp(pred)})\nprint(df_pred.head())\ndf_pred.to_csv('submission.csv', index=False)","16ff754f":"# Pre","311b054c":"\n# Feature Selection\n# # feature importance"}}