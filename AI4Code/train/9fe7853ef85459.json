{"cell_type":{"094c2044":"code","e34b1a48":"code","ed82d474":"code","3d4e79ef":"code","b32dcba2":"code","bca27f27":"code","a473794b":"code","8431f8a3":"code","9acd15b4":"code","771d35a0":"code","d11fb7f3":"code","d2cdfd4a":"code","15fbaed2":"code","9585a430":"code","d84e4454":"code","d05f8b02":"code","9c56d69c":"code","554945d6":"code","d58e3894":"code","8ebc3fcf":"code","65ceac96":"code","887bf743":"markdown","0b9f1c4b":"markdown","63fa4360":"markdown","b56238cf":"markdown","4c93ebf4":"markdown","99c21040":"markdown","41b052fc":"markdown","d6fd6043":"markdown","123688be":"markdown","4382e4e3":"markdown","85f67083":"markdown","a9ef0702":"markdown"},"source":{"094c2044":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e34b1a48":"import pandas as pd\n\ntrain_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n","ed82d474":"train_df.shape","3d4e79ef":"test_df.shape","b32dcba2":"X_train = train_df.drop([\"label\"],axis = 1)\nY_train = train_df[\"label\"]\nX_test=test_df","bca27f27":"X_train = X_train.values.reshape(len(X_train), 28, 28,1)\nX_test = X_test.values.reshape(len(X_test), 28, 28,1)","a473794b":"from keras.utils.np_utils import to_categorical\nY_train = to_categorical(Y_train, num_classes = 10)","8431f8a3":"# Initialize the random number generator\nimport random\nrandom.seed(0)\n\n# Ignore the warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9acd15b4":"from sklearn.model_selection import train_test_split\n# Set the random seed\nrandom_seed = 0\n# Split the train and the validation set for the fitting\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.2, random_state=random_seed)","771d35a0":"from tensorflow.keras.layers import Conv2D\n\nfrom keras.models import Sequential\nfrom keras.layers import  Flatten,Activation\nfrom keras.layers import Dense,Conv2D, MaxPooling2D,BatchNormalization,GlobalAveragePooling2D,Dropout\n\nmodel = Sequential()\nmodel.add(Conv2D(filters=32, kernel_size=3, activation=\"relu\", input_shape=(28, 28, 1))) #26\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=16,dilation_rate=(3, 3), kernel_size=3, activation=\"relu\")) # 24\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=64, kernel_size=3, activation=\"relu\"))#22\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv2D(filters=64, dilation_rate=(3, 3), kernel_size=3, activation=\"relu\"))# 8\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=32, kernel_size=2, activation=\"relu\"))# 7\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=10, kernel_size=1, activation=\"relu\"))# 7\nmodel.add(BatchNormalization())\n\n\n# model.add(Flatten())\nmodel.add(GlobalAveragePooling2D())\n\nmodel.add(Dense(16, activation=\"relu\"))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(10, activation=\"softmax\"))","d11fb7f3":"model.summary()","d2cdfd4a":"model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n","15fbaed2":"%%time\nimport tensorflow as tf\nbatch_size=32\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, min_delta=0.01)\nlr_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                 patience=10, \n                                 verbose=2, \n                                 factor=.75)\nhistory = model.fit(X_train, Y_train, epochs=35,verbose=1,validation_data = (X_val,Y_val),batch_size=batch_size,callbacks=[lr_reduction])","9585a430":"val_loss, val_accuracy= model.evaluate(X_train,Y_train)\nprint(\"Validation Accuracy:\",val_accuracy)","d84e4454":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nacc_train = history.history['accuracy']\n# print(len(acc_train))\nacc_val = history.history['val_accuracy']\n# print(len(acc_val))\n\nepochs = range(1,36)\nplt.plot(epochs, acc_train, 'g', label='Training accuracy')\nplt.plot(epochs, acc_val, 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy Model')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","d05f8b02":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nloss_train = history.history['loss']\nloss_val = history.history['val_loss']\n\nepochs = range(1,36)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss for Model')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","9c56d69c":"# predict results\nresults = model.predict(X_test)","554945d6":"test_results = np.argmax(results,axis=1)","d58e3894":"results = pd.Series(test_results,name=\"Label\")","8ebc3fcf":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"predictions_cnn_dilated_3.csv\",index=False)","65ceac96":"results","887bf743":"Create Model","0b9f1c4b":"Save the Results to CSV File for Submission","63fa4360":"Read The Data from Train & Test csv","b56238cf":"Reshape the Input Data for CNN in format(Img_height,Img_width, channel)","4c93ebf4":"Compile the model\n1. Set the Metrics\n2. Set the optimizer(need for backtracking for convergence of loss)\n3. Set the loss function","99c21040":"Print Validation Accuracy","41b052fc":"One-Hot Encoding for Output Label","d6fd6043":"Split the training & test data ","123688be":"Prepare Training & Test Data ","4382e4e3":"Plot Training & Validation Loss","85f67083":"Plot Training Accuracy vs Validation Accuracy","a9ef0702":"X_train = X_train\/255.0\nX_test = X_test\/255.0"}}