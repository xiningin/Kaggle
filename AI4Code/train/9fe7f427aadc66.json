{"cell_type":{"7952a478":"code","63506f84":"code","0add4ac2":"code","1a62cff7":"code","0faaa761":"code","86af446f":"code","998a3a9b":"code","cfef1ca8":"code","ce5bbbdd":"code","e599867b":"code","284c2d31":"code","71cb58f5":"code","7cdc665c":"code","0066ebe6":"code","5093e9ad":"code","538bcf3b":"code","e8960be1":"code","a70d40fe":"markdown","8a6855ee":"markdown","5318a732":"markdown","e2b1dbbf":"markdown","6ea92a71":"markdown","60f62c9f":"markdown","b6c45057":"markdown","d448e354":"markdown","d3305cb5":"markdown","124f8265":"markdown"},"source":{"7952a478":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","63506f84":"# Importing plotting libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Loading numpy file and coverting it to a DataFrame\nX = np.load('\/kaggle\/input\/sign-language-digits-dataset\/X.npy')\nX = pd.DataFrame(X.reshape(2062,-1))","0add4ac2":"# Centering the data around the Origin\n\nX_centered = X - X.mean(axis = 0) # It is madndatory to center the data incase of PCA calculation using SVD\n\nU, s, Vt = np.linalg.svd(X_centered) # SVD of zero centered array\nc1 = Vt.T[:,0] # first unit Vector for the PC 1\n\nc2 = Vt.T[:,1] # second unit vector for the PC 2","1a62cff7":"# Inorder to find the reduced datset identified by the PCs, we need to do the matrix multiplication of original dataset and \n# the matrix containg first d coulmns of the matrix V\n\nW2 = Vt.T[:,:2] # selecting the first 2 columns of the Matrix V\nX2D = X_centered.dot(W2) # reduced dataset obtained by Matrix Multiplication","0faaa761":"X2D.rename(columns = {0: 'PC1', 1:'PC2'}, inplace = True)\nprint(X2D.head())\nprint(X2D.shape)","86af446f":"from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX_2D = pca.fit_transform(X)","998a3a9b":"print(pca.explained_variance_ratio_)\nprint(\"Amount of variance explained by Both of these PCs is: {} %\".format(np.cumsum(pca.explained_variance_ratio_)[-1] * 100 ))","cfef1ca8":"from sklearn.metrics import mean_squared_error\n\nX2Rec = X2D.dot(W2.T) # Reconstruction of the dataset\n\nprint(X2Rec.shape)\n\nprint(\"Reconstruction error is: {}\".format(mean_squared_error(X, X2Rec)))\n","ce5bbbdd":"pca = PCA()\n# If n_componenets is not set explicitly, the value that gets passes by default is equal to the minimum of n_samples or n_features\nX_red_1 = pca.fit_transform(X)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\n\n\n\nprint(pca.explained_variance_ratio_)\nprint(\"Number of PCs: {}\".format(len(pca.explained_variance_ratio_))) # number of PCs in this case is 2062\nprint(\"The shape of reduced dataset is {} rows by {} columns\".format(X_red_1.shape[0],X_red_1.shape[0]))\n\nprint(cumsum)","e599867b":"d = np.argmax(cumsum >= 1) + 1\nprint(d)\n\n# It can be seen that 2056 PCs can explain 100 % of variance from the Original Dataset","284c2d31":"pca = PCA(n_components = 2056)\n\nX_red_2 = pca.fit_transform(X) # reduced Dataset\nX_rec_2 = pca.inverse_transform(X_red_2) # Recovered Dataset\n\nprint(\"The shape of recovered dataset is: {}\".format(X_rec_2.shape))","71cb58f5":"fig = plt.figure(figsize = (12,8))\n\nfor i, j in enumerate(range(1,2000,100)):\n    \n    plt.subplot(4,5, i+1)\n    plt.imshow(X.iloc[j,:].values.reshape(64,64))\n    \nfig.tight_layout(pad=1.0)","7cdc665c":"\nfig = plt.figure(figsize = (12,8))\n\nfor i, j in enumerate(range(1,2000,100)):\n    \n    plt.subplot(4,5, i+1)\n    plt.imshow(X_rec_2[j].reshape(64,64))\n    \nfig.tight_layout(pad=1.0)","0066ebe6":"from sklearn.metrics import mean_squared_error\n\nprint(\"Reconsturction error is: {}\".format(mean_squared_error(X, X_rec_2)))","5093e9ad":"\ndef analyse_PCA(df, variance):\n    \n    print(\"Generating & Analysing PCs for a preserved variance of {}\".format(variance))\n    \n    df = df.copy()\n    \n   \n        \n    pca = PCA(n_components = variance)\n        \n    X_red = pca.fit_transform(df)\n    \n    print(\"The shape of reduced dataset is {}\".format(X_red.shape))\n    X_rec = pca.inverse_transform(X_red)\n        \n    cumsum = np.cumsum(pca.explained_variance_ratio_)\n    d = np.argmax(cumsum) + 1 \n       \n    fig = plt.figure(figsize = (12,8))\n\n    for i, j in enumerate(range(1,2000,100)):\n    \n        plt.subplot(4,5, i+1)\n        plt.imshow(X_rec[j].reshape(64,64))\n    \n    fig.tight_layout(pad=1.0)\n    \n    \n    print(\"Reconsturction error is: {}\".format(mean_squared_error(X, X_rec)))\n    ","538bcf3b":"analyse_PCA(X, 0.8)","e8960be1":"# Implementation of randomized PCA\n\nimport time\n\nsolver = [\"auto\", \"full\", \"randomized\"]\n\n\ntime_taken = []\n\nfor i in solver:\n    start = time.time()\n    random_pca = PCA(n_components = 10, svd_solver = \"full\")\n\n\n    X3_red = random_pca.fit_transform(X)\n    end = time.time()\n    \n    print(\"Time taken with {} svd_solver is {}\".format(i,end-start))\n    \n    time_taken.append(end-start)\n","a70d40fe":"# Function to analyse the PCAs with desired Variance\n\n* PCA can be used for compressing a dataset and to reduce size\n* The compressed dataset can be decompressed using Sleans.inverse_tranform()\n* Due to compression and decompression, there is some amount of information loss and this can be referred as reconstruction error\n* Reconstruction error is calculated as mean_squared_error between the original data & the reconstructed data\n* below function implements the compression, decompression & the reconstruction error","8a6855ee":"# Other Forms of PCAs\n\n### Based on the size of the data, available computational resources, we can implement two more forms of PCAs\n\n","5318a732":"# Reconstruction using the Linear Algebra approach\n\n* The original dataset can be obtained by matrix multiplication of Reduced Matrix & the Transpose of the matrix containing unit vectors of the Principal Components\n* Below code implemets the decompression and the error arising out of it","e2b1dbbf":"## Plotting the images from Original Dataset","6ea92a71":"## Randomized PCA\n\nBy default, the \"svd_solver\" hyperparameter is set as \"auto\" in sklearn's PCA. This hyperparameter can be tweaked to obtain a rough approximation of the first d principal components. This can be obtained by setting svd_solver to  \"random\" and this is faster than using th efull SVD approach when d (number of PCs) is much smaller than n(number of features).\n\n\nSklearn uses randomized PCA when if m(number of instances) or n(features) are greater than 500 and d(no. of PCs) is less than 80% of m or n, or else it uses full SVD approach. We can force the PCA to use full SVD approach by setting the svd_solver to \"full\"","60f62c9f":"# Calculation of PCs using SVD\n* SVD - Singular Value Decomposition\n* It decomposes a matirx into matrix multiplication of three matrices (U, S, and V)\n* V contains the unit vectors that define the principal components","b6c45057":"# Selecting the right number of PCs \/ Dimension","d448e354":"## Randomly plot 20 images from the Recovered dataset","d3305cb5":"# Comparison between Original Dataset & Recovered Dataset ","124f8265":"# **Principal Component Analysis**\n\n* Idea is to identify a hyperplane that is closest to the data and project the data onto this Hyperplane\n* Identify & select the axes along which the maximum variance is preserved when compared to original data\n* Each of these principal components are orthogonal to each other\n"}}