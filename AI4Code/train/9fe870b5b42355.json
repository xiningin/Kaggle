{"cell_type":{"ca9b3e74":"code","199819c4":"code","6b3940d2":"code","1412740e":"code","04e9199c":"code","523b44c5":"code","44ae8ac3":"code","31296913":"code","d96d7239":"code","ef107d90":"code","f3e922ea":"code","f3cea962":"code","5d85a926":"code","dab804dc":"code","f7dba282":"code","77d8a5df":"code","87de7fcc":"code","4c49e893":"code","fa43942c":"code","962cd7c9":"code","ea319989":"markdown","63a358f3":"markdown","d1a73071":"markdown","ccf7e540":"markdown","bb61377c":"markdown"},"source":{"ca9b3e74":"pip install feature_engine","199819c4":"import os\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgbm\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom tqdm.notebook import tqdm\n\nfrom scipy.optimize import differential_evolution\nfrom sklearn.metrics import f1_score","6b3940d2":"from scipy.optimize import differential_evolution\n\nfrom feature_engine.encoding import MeanEncoder, RareLabelEncoder, CountFrequencyEncoder, OneHotEncoder\nfrom feature_engine.selection import DropFeatures\nfrom feature_engine.imputation import AddMissingIndicator\nfrom feature_engine.imputation import ArbitraryNumberImputer, MeanMedianImputer\nfrom sklearn.pipeline import Pipeline","1412740e":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","04e9199c":"KAGGLE_FLAG = True\nSEED = 945\nNUM_FOLDS = 5 #10\nNUM_REPEATS = 10 #5","523b44c5":"train_input_path = ('..\/input\/porto-seguro-data-challenge\/train.csv' if KAGGLE_FLAG else '\/home\/rapela\/Downloads\/kaggle\/porto_seguro_data_challenge\/data\/train.csv')\ntest_input_path = ('..\/input\/porto-seguro-data-challenge\/test.csv' if KAGGLE_FLAG else '\/home\/rapela\/Downloads\/kaggle\/porto_seguro_data_challenge\/data\/test.csv')\nsubmission_input_path = ('\/kaggle\/input\/porto-seguro-data-challenge\/submission_sample.csv' if KAGGLE_FLAG else '\/home\/rapela\/Downloads\/kaggle\/porto_seguro_data_challenge\/data\/submission_sample.csv')\nmetadata_input_path = ('\/kaggle\/input\/porto-seguro-data-challenge\/metadata.csv' if KAGGLE_FLAG else '\/home\/rapela\/Downloads\/kaggle\/porto_seguro_data_challenge\/data\/metadata.csv')","44ae8ac3":"train = pd.read_csv(train_input_path, na_values = -999).drop(['id'], axis=1)\ntest = pd.read_csv(test_input_path, na_values = -999).drop(['id'], axis=1)\nsubmission = pd.read_csv(submission_input_path)\nmetadata = pd.read_csv(metadata_input_path)","31296913":"cat_nom = [x for x in metadata.iloc[1:-1, :].loc[(metadata.iloc[:,1]==\"Qualitativo nominal\")].iloc[:,0]]\ncat_ord = [x for x in metadata.iloc[1:-1, :].loc[(metadata.iloc[:,1]==\"Qualitativo ordinal\")].iloc[:,0]]\nnum_dis = [x for x in metadata.iloc[1:-1, :].loc[(metadata.iloc[:,1]==\"Quantitativo discreto\")].iloc[:,0]]\nnum_con = [x for x in metadata.iloc[1:-1, :].loc[(metadata.iloc[:,1]==\"Quantitativo continua\")].iloc[:,0]]\n\ncat_cols = cat_nom+cat_ord\nnum_cols = num_dis+num_con","d96d7239":"meta = pd.read_csv('..\/input\/porto-seguro-data-challenge\/metadata.csv')\ncat_var = [ 'var1', 'var2', 'var7', 'var8', 'var9', 'var10', 'var14', 'var15',\n       'var16', 'var17', 'var18', 'var20', 'var22', 'var23', 'var28', 'var29',\n       'var30',  'var33', 'var34', 'var37', 'var39',\n       'var41', 'var5', 'var6', 'var11', 'var57']\ncat_ord = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==\"Qualitativo ordinal\")].iloc[:,0]]\ncat_var.extend(cat_ord)\n\nnum_dis = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==\"Quantitativo discreto\")].iloc[:,0]]\nnum_con = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==\"Quantitativo continua\")].iloc[:,0]]\n\nnum_cols = num_dis + num_con\nto_drop = ['var65', 'var66', 'var19', 'var31', 'var36', 'var68', 'var38']\n           \naddBinary_imputer = AddMissingIndicator()\nmedian_Imputer = MeanMedianImputer(imputation_method='median', variables = num_cols)\narbitrary_imputer = ArbitraryNumberImputer(arbitrary_number=-999)\narbitrary_imputer_zero = ArbitraryNumberImputer(arbitrary_number=0.22)\nrare_encoder = RareLabelEncoder(tol=0.02, n_categories=2, variables=cat_var,\n                           replace_with=-999, ignore_format = True)\nmean_encoder = MeanEncoder(variables=cat_var, ignore_format = True)\ndrop_Features = DropFeatures(features_to_drop = to_drop)\n\n\npipe1 = Pipeline([('indicator', addBinary_imputer),\n                  ('median_imputer', median_Imputer),\n                  ('ReplaceNa', arbitrary_imputer),\n                  ('RareLabelEncoder1', rare_encoder),\n                  ('MeanEncoder', mean_encoder),\n                  ('arbitrary_imputer_zero', arbitrary_imputer_zero),\n                 ('dropFeatures', drop_Features)])","ef107d90":"X =  train[cat_nom+cat_ord+num_dis+num_con]\nX_test = test[cat_nom+cat_ord+num_dis+num_con]\ny = train.y","f3e922ea":"X = pipe1.fit_transform(X, y)\nX_test = pipe1.transform(X_test)","f3cea962":"# fun\u00e7\u00f5es criadas para resolver o problema do neg\u00f3cio\n\ndef optimal_cutoff(y_target, y_predict_prob, only_fun = True, random_state = 1997):\n    \n    optimization = differential_evolution(lambda c: (-f1_score(y_target, (y_predict_prob > c[0]))),\n                                          [(0, 0.95)], tol = 0.00000001, seed = random_state, popsize=300)\n    if only_fun == False:\n        return -optimization['fun'], optimization['x']\n    else:\n        return 'f1score', -optimization['fun'], True\n        ","5d85a926":"rkf = RepeatedStratifiedKFold(n_splits=NUM_FOLDS, n_repeats=NUM_REPEATS, random_state=SEED)\nkf = StratifiedKFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)","dab804dc":"def cross_valid(model, train, target, test, num_folds=10, random_state=42):\n\n    train_oof = np.zeros((len(train)))\n    test_preds = 0\n    \n    train_pred = []\n    \n    scores=[]\n    cut = []\n    \n    for f, (train_ind, val_ind) in tqdm(enumerate(rkf.split(train, target))):\n\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        \n        train_target, val_target = target[train_ind], target[val_ind]\n\n        model = HistGradientBoostingClassifier(**hgbc_params)  \n        \n        model.fit(train_df, \n        train_target,\n        )\n\n        temp_oof = pd.Series(model.predict_proba(val_df)[:, -1], index=val_df.index)\n        \n        train_pred.append(temp_oof)\n        \n        business_metric_opt, cutoff = optimal_cutoff(val_target, temp_oof, only_fun= False)\n        cut.append(cutoff[0])        \n        \n        temp_oof  = (temp_oof > cutoff[0]).astype(int)\n        train_oof[val_ind] = temp_oof \n    \n        temp_test = model.predict_proba(test[columns])[:, -1]\n        temp_test = (temp_test > cutoff[0]).astype(int)\n        \n        test_preds += ((temp_test > cutoff[0])\/(NUM_FOLDS*NUM_REPEATS))\n        \n        scores.append(f1_score(val_target, temp_oof))\n        \n        print(f'Fold {f}: {f1_score(val_target, temp_oof)}')\n        print(f'Best f1 score: {business_metric_opt} ----  cutoff: {cutoff[0]}')\n\n        \n    #train_pred = (pd.concat(train_pred)).sort_index()\n    #     print(\"Mean F1 Score: \", np.mean(scores))\n    #     print(f'F1 Score OOF: {f1_score(y, train_oof)}')\n    #     print(f'Cutoff : {np.mean(cut)}')\n    print(f\"Mean F1 Score: {np.mean(scores)}, std: {np.std(scores)}\")\n    print(f'F1 Score OOF: {f1_score(y, train_oof)}')\n    \n    return train_oof, (test_preds >= 0.5).astype(int), np.mean(scores)","f7dba282":"hgbc_params = {\n        'loss': 'binary_crossentropy', \n        #'l2_regularization': 11.585979094165058, 'learning_rate': 0.03793198532566819, 'max_bins': 225, 'max_depth': 3, 'max_leaf_nodes': 17298, 'max_iter': 15356,\n        # 'l2_regularization': 30.02745171295389, 'learning_rate': 0.15879583186021623, 'max_bins': 204, 'max_depth': 3, 'max_leaf_nodes': 31792, 'max_iter': 12102,\n        'l2_regularization': 17.768015953169055, 'learning_rate': 0.17321509618530492, 'max_bins': 210, 'max_depth': 3, 'max_leaf_nodes': 20574, 'max_iter': 51031,\n        'random_state': SEED\n    }\n","77d8a5df":"columns = X_test.columns\n\ntrain_oof_1, test_preds_1, score_oof_1 = cross_valid(None, X, y, X_test, num_folds=NUM_FOLDS, random_state=SEED)","87de7fcc":"# F1 Score OOF: 0.6838037634408602\n# Cutoff : 0.2872982468667088","4c49e893":"print(f'F1 Score: {f1_score(y, train_oof_1)}')\n\nimport sklearn\n\nprint(f'ACC Score: {sklearn.metrics.accuracy_score(y, train_oof_1)}')","fa43942c":"np.save('train_oof_hgbc.npy', train_oof_1)\nnp.save('test_preds_hgbc.npy', test_preds_1)","962cd7c9":"submission['predicted'] = test_preds_1.astype(int)\nsubmission.to_csv('submission_output_hgbc.csv', index=False)\nprint(submission)","ea319989":"# Modeling","63a358f3":"# Submission","d1a73071":"# Imports","ccf7e540":"# Open Files","bb61377c":"# List Files"}}