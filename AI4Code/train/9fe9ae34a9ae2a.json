{"cell_type":{"3503fc8c":"code","8df30a49":"code","fe6fae90":"code","71d0aa8e":"code","99d0dfc5":"code","6bdbf714":"code","1ca31881":"code","0bfd1b3e":"code","f358ca52":"code","72eeacc8":"code","2e036f55":"code","c536ec28":"code","6ef4726c":"code","de462bd8":"code","df88f630":"code","0197c448":"code","36f056cf":"code","b03bf49a":"code","f2545b10":"code","5e9d418e":"code","8cbcaf99":"code","c0b88636":"code","e3cd1f1c":"code","0f656898":"code","9a395055":"code","33b07922":"code","613fef3e":"code","5007a04b":"code","562c87fc":"code","4ff9fa52":"code","248613bd":"code","21caafad":"code","a6ad3e3a":"code","bcdfc93b":"code","a7d03a99":"code","e9f9368d":"code","29873b51":"code","0ebc32e7":"code","488d0c3e":"code","b246fa3f":"code","f4721fea":"code","63616332":"code","bb3f67a1":"code","2863e78d":"code","aac5df03":"code","296231c4":"code","0dbbca55":"code","de4271a6":"code","b7aee852":"code","989fb3f7":"code","6f080a40":"code","42b87780":"code","af7e8125":"code","1a97f915":"code","4a8dc41b":"code","b46cc504":"code","e37c5366":"code","e20ebbf2":"code","ba5451a4":"code","11cacda7":"code","5dda7aeb":"code","ca0f595a":"code","cb8fd3a8":"code","cced29c3":"code","645d566a":"code","131aa6b0":"code","2f2302c4":"code","7c29c115":"code","df755aae":"code","6b396e21":"code","31a0ca5c":"code","dd9a3fc1":"code","de71fc82":"code","778a686c":"code","1d99957a":"code","8aa6e4f7":"code","22104485":"code","bb1abaf5":"code","2c84271c":"markdown","1618b36f":"markdown","0db6266a":"markdown","7439f08f":"markdown","8321bd66":"markdown","9cb663ba":"markdown","7d4c0248":"markdown","a2c54c29":"markdown","745b2c5d":"markdown","b43b22c6":"markdown","491d2215":"markdown","2d455aba":"markdown","b25e34c5":"markdown","83b43916":"markdown","51ac82e6":"markdown","f94ae0a5":"markdown","5429ecac":"markdown","a59b89b1":"markdown","e3813bf5":"markdown","40ac026f":"markdown","e0c621ad":"markdown","779ad50e":"markdown","8efc89f3":"markdown","d763db1b":"markdown","2f250cd6":"markdown","ab3f463c":"markdown","b66fceaa":"markdown","4e8d1525":"markdown","9e581d59":"markdown","1524d0f0":"markdown","d8f3a8ff":"markdown","ceffe6fa":"markdown","810d5d2d":"markdown","a6fff520":"markdown","ab4ba9de":"markdown","12eb41ee":"markdown","fe4632bd":"markdown","1b0ab579":"markdown","2c682b49":"markdown","4b9a7da9":"markdown","046c6574":"markdown","4bf04e84":"markdown","f24ad0ee":"markdown","aeb24ec2":"markdown","61401842":"markdown","35400fc6":"markdown","704f3797":"markdown","4d1da17b":"markdown","b6f0d609":"markdown","4b677a70":"markdown","a0f4f073":"markdown","192463c1":"markdown","8b8ff446":"markdown","b668f2a0":"markdown","54d625bb":"markdown","46f856bb":"markdown","b1113ae1":"markdown","2a2be873":"markdown","03b84b2c":"markdown","6655fc92":"markdown","a9d0b701":"markdown","c7b7b16a":"markdown","91c00c3f":"markdown","74ad75cc":"markdown","e9b35c7f":"markdown","9b2c2e8c":"markdown","ff7b303e":"markdown","74c027fe":"markdown","9fe72134":"markdown","f4129109":"markdown","22426b11":"markdown","2dc20cab":"markdown","b717f728":"markdown","9e79b8a8":"markdown","64b1c785":"markdown","c6e1f653":"markdown","486b11da":"markdown","cb34ab01":"markdown","765e2d9e":"markdown","9a9cc6d2":"markdown","579bf127":"markdown","891a8b1e":"markdown","7ca3277f":"markdown","c7c155bd":"markdown","511778a4":"markdown","d18b331a":"markdown","9e8f2a7b":"markdown","e49a7517":"markdown","61bcc764":"markdown","2aa5f8fb":"markdown","7d8bb950":"markdown","875f8502":"markdown","c75107f5":"markdown","25bfdae6":"markdown","df36f64c":"markdown","af6fae66":"markdown","5c682a80":"markdown"},"source":{"3503fc8c":"#load packages\uff0c \u6253\u5370\uff0c\u4fbf\u4e8e\u53ef\u590d\u73b0\nimport sys #access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\n#misc libraries\nimport random\nimport time\n\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\n","8df30a49":"#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","fe6fae90":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n\n'''\n\u9884\u6d4bSalePrice\u7684\u503c\n'''\n\n# train_df.columns \n# train_df.shape\ntrain_df.describe()\n# train_df.info()\n# print '%' * 40\n# test_df.info()\n# train_df.head(10)\n# train_df.info()\n# print train_df.describe(include = 'all')\n# print '%' * 40\n# test_df.info()","71d0aa8e":"# # Missing Data\u7684\u767e\u5206\u6bd4\n# total = train_df.isnull().sum().sort_values(ascending=False)\n# percent = (train_df.isnull().sum()\/train_df.isnull().count()).sort_values(ascending=False)\n# missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# # print missing_data.head(20)\n\n# total2 = test_df.isnull().sum().sort_values(ascending=False)\n# percent2 = (test_df.isnull().sum()\/test_df.isnull().count()).sort_values(ascending=False)\n# missing_data2 = pd.concat([total2, percent2], axis=1, keys=['Total', 'Percent'])\n# # print missing_data2.head(20)","99d0dfc5":"train_df.describe(include = 'all')\nprint ('%' * 40)\n\n# print train_df.loc[:,'MasVnrType']\n\n# train_df['MasVnrType'] = train_df['MasVnrType'].replace(['None'],None )\n# train_df.loc[:,'MasVnrType'] =  train_df['MasVnrType'].apply(lambda x: None if x == 'None' else x)\n# print type(train_df.loc[1,'MasVnrType'])\n\nlimit_missing_values = 0.25\ntrain_limit_missing_values = len(train_df) * limit_missing_values \nprint (\"Train columns with null values:\\n\", train_df.columns[train_df.isnull().sum().values > train_limit_missing_values])  # \u4f9d\u5217\u4e3a\u6807\u51c6\uff0ccolumn\nprint ('%'*40)\n\ntest_limit_missing_values = len(test_df) * limit_missing_values\nprint (\"Test columns with null values:\\n\", test_df.columns[test_df.isnull().sum().values > test_limit_missing_values])\n\n","6bdbf714":"missing_columns = list(train_df.columns[train_df.isnull().sum() != 0])\nprint (missing_columns)\n# train_df[missing_columns].describe(include = 'all')\nprint ('%' * 40)\ntest_missing_columns = list(test_df.columns[test_df.isnull().sum() != 0])\nprint (test_missing_columns)","1ca31881":"train_missing_numerical = list(train_df[missing_columns].dtypes[train_df[missing_columns].dtypes != 'object'].index)\ntrain_missing_category = [i for i in missing_columns if i not in train_missing_numerical]\n\ntest_missing_numerical = list(test_df[test_missing_columns].dtypes[test_df[test_missing_columns].dtypes != 'object'].index)\ntest_missing_category = [i for i in test_missing_columns if i not in test_missing_numerical]\n\nprint (\"Train missing numerical: \", train_missing_numerical, \"\\n\")\nprint (\"Train missing category: \", train_missing_category,  \"\\n\")\nprint (\"Test missing numerical: \", test_missing_numerical, \"\\n\")\nprint (\"Test missing category: \", test_missing_category, \"\\n\")","0bfd1b3e":"# \u53d6\u4f17\u6570\u7684\u7279\u5f81\ntrain_categories_Mode = ['Electrical']\ntest_categories_Mode = ['MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'KitchenQual', 'Functional', 'SaleType']\n\ntrain_categories_none = [i for i in train_missing_category if i not in train_categories_Mode]\ntest_categories_none = [i for i in test_missing_category if i not in test_categories_Mode]\n\n\n# \u901a\u8fc7\u5b57\u7b26\u4e32\u201cNone\u201d\u586b\u5145 \nfor category in train_categories_none:\n    train_df[category].fillna(\"None\", inplace=True)\n    \nfor category in test_categories_none:\n    test_df[category].fillna(\"None\", inplace=True)\n    \nfor category in train_categories_Mode:\n    train_df[category].fillna(train_df[category].mode()[0], inplace = True)\n\nfor category in test_categories_Mode:\n    test_df[category].fillna(test_df[category].mode()[0], inplace = True)","f358ca52":"# \u53d60\u7684\u6570\u503c\u7279\u5f81\n\nfor col in ('GarageArea', 'GarageCars', 'GarageYrBlt'):\n    train_df[col] = train_df[col].fillna(0)\n    test_df[col] = test_df[col].fillna(0)\n    \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    train_df[col] = train_df[col].fillna(0)\n    test_df[col] = test_df[col].fillna(0)\n    \n# \u901a\u8fc7\u4e2d\u4f4d\u6570\u586b\u5145\nfor column in train_missing_numerical:\n    train_df[column].fillna(train_df[column].median(), inplace=True)\n\nfor column in test_missing_numerical:\n    test_df[column].fillna(test_df[column].median(), inplace=True) ","72eeacc8":"print (train_df.isnull().sum().max())\nprint (test_df.isnull().sum().max())\n# print test_df.loc[test_df.isnull()","2e036f55":"train_df['SalePrice'].describe()","c536ec28":"sns.distplot(train_df['SalePrice'], color='green')","6ef4726c":"print (\"\u504f\u5ea6\u4e3a %f \" % train_df['SalePrice'].skew())\nprint (\"\u5cf0\u5ea6\u4e3a %f\"  % train_df['SalePrice'].kurt())","de462bd8":"var = 'BsmtFinSF1' # \u623f\u5c4b\u5c45\u4f4f\u9762\u79ef\n\n# concat - Series\u9ed8\u8ba4\u884c\u5408\u5e76\uff1b axis = 1\uff0c\u5217\u5408\u5e76\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1) \n# print dataFrame\ndata.plot.scatter(x=var, y='SalePrice', ylim = (0, 800000)) # y\u8f74\u9650\u5236","df88f630":"var = 'TotalBsmtSF'  # \u623f\u5c4b\u5730\u4e0b\u5ba4\u7684\u9762\u79ef\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x = var, y = 'SalePrice', ylim = (0, 800000))","0197c448":"var = 'OverallQual'  # \u623f\u5c4b\u6574\u4f53\u6750\u6599\u548c\u5149\u6d01\u5ea6\u7684\u8bc4\u4f30\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)  # \u6a2a\u5750\u6807\u7c7b\u522b\uff0c\u7eb5\u5750\u6807\u76ee\u6807\u53d8\u91cf\nfig.axis(ymin=0, ymax=800000);","36f056cf":"var = 'YearBuilt'  # \u623f\u5c4b\u5efa\u9020\u5e74\u4efd\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))  # \u6539\u53d8\u5927\u5c0f\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)  # \u6a2a\u5750\u6807\u7c7b\u522b\uff0c\u7eb5\u5750\u6807\u76ee\u6807\u53d8\u91cf\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);               # x\u5982\u679c\u90fd\u6c34\u5e73\u653e\u7f6e\uff0c\u4f1a\u7c98\u5230\u4e00\u8d77\u3002x\u503e\u659c90\u5ea6\uff0c\u51cf\u5c11\u5360\u7684\u4f4d\u7f6e","b03bf49a":"cols = ['MSSubClass', 'OverallCond', 'BedroomAbvGr', 'YrSold', 'MoSold']\n\nfor col in cols:\n    train_df[col] = train_df[col].apply(str)\n    test_df[col] = test_df[col].apply(str)","f2545b10":"train_df.columns","5e9d418e":"train_df = train_df[['LotFrontage', 'SalePrice', 'MasVnrArea', '1stFlrSF']]","8cbcaf99":"train_df","c0b88636":"corrmat = train_df.corr()\nf, ax = plt.subplots(figsize=(16, 8))\nsns.heatmap(corrmat, vmax= .8, square=True);   # vmax \u989c\u8272\u533a\u522b\uff0c\u6700\u6d45\u7684\u989c\u8272\u57280.8","e3cd1f1c":"# help(corrmat.nlargest)","0f656898":"# # cols = corrmat.nlargest(k, 'SalePrice').index\n# # print cols\n# len (train_df[cols].values)  # \u6570\u7ec4\uff0c1460 * 10 | \u8f6c\u7f6e 10 * 1460","9a395055":"old_features = ['1stFlrSF', '2ndFlrSF', 'TotalBsmtSF']\ntrain_df['TotalSF'] = 0\ntest_df['TotalSF'] = 0\nfor i in old_features:\n    print (train_df['SalePrice'].corr(train_df[i]))\n    train_df['TotalSF'] += train_df[i]\n    test_df['TotalSF'] += test_df[i]\ntrain_df['SalePrice'].corr(train_df['TotalSF'])","33b07922":"corrmat = train_df.corr().abs()","613fef3e":"#saleprice correlation matrix\nk = 36   #number of variables for heatmap\uff0c\u70ed\u529b\u56fe\u53d8\u91cf\u6570\u91cf \n\n# nlargest - \u6839\u636eSalePrice\u5217\u6392\u5e8f\uff0c\u8fd4\u56de\u524d10\u4e2a\u8ddfSalePrice\u76f8\u5173\u6027\u6700\u9ad8\u7684\u884c\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index \n\n# cm = corrmat.loc[cols,cols] \u540c\u4ee5\u4e0bcm\u8d4b\u503c\u76f8\u540c\n# \u8bad\u7ec3\u96c6\u4e2d\u53d6\u51fa\u76ee\u6807\u5217\u7684\u6837\u672c\uff0c\u8f6c\u7f6e\uff0c\u8ba1\u7b9710\u4e2a\u7279\u5f81\u4e4b\u95f4\u7684\u76f8\u5173\u6027\ncm = np.corrcoef(train_df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","5007a04b":"print (len(train_df.columns))\n# print len(drop_columns)\nprint (len(train_df.dtypes[train_df.dtypes == 'object'].index))","562c87fc":"# cols","4ff9fa52":"# #scatterplot\n# sns.set()\n# cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n# sns.pairplot(train_df[cols], size = 2.5)\n# plt.show();","248613bd":"# olss = []\n# numerical_features = ['GrLivArea']\n# for feature in numerical_features:\n    \n#     # \u8ba1\u7b9725%\u5206\u4f4d\u70b9\n#     Q1 = np.percentile(train_df[feature], 25)\n    \n#     # \u8ba1\u7b9775%\u5206\u4f4d\u70b9\n#     Q3 = np.percentile(train_df[feature], 75)\n#     print Q3\n    \n#     # \u5f02\u5e38\u9636\uff081.5\u500d\u56db\u5206\u4f4d\u8ddd\uff09IQR\n#     step = 1.5 * (Q3 - Q1)\n    \n#     print \"Feature\" + feature + \"Outlines\"\n#     print train_df[ (train_df[feature] <= Q1 - step) | (train_df[feature] >= Q3 + step)][numerical_features]\n#     ols = features_train[ (train_df[feature] <= Q1 - step) | (train_df[feature] >= Q3 + step)].index.tolist()\n#     olss.append(ols)\n    \n# olss_new = [ii for i in olss for ii in i]\n# # print olss_new\n\n# # \u5217\u8868\u65b9\u6cd5 .count(i) \u7edf\u8ba1\u5217\u8868\u4e2d\u67d0\u4e2a\u5143\u7d20\u51fa\u73b0\u7684\u6b21\u6570\n# more_than_one = list(set([i for i in olss_new if olss_new.count(i) > 1]))\n# more_than_two = list(set([i for i in olss_new if olss_new.count(i) > 2]))\n# print len(more_than_one), len(more_than_two)\n\n# ## \u79fb\u9664\u5f02\u5e38\u70b9\n# # features_train_new = features_train.drop(features_train.index[olss_new]).reset_index(drop = True)\n# # labels_train_new = labels_train.drop(features_train.index[olss_new]).reset_index(drop = True)","21caafad":"var = 'GrLivArea'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","a6ad3e3a":"# \u4ece\u6563\u70b9\u56fe\u4e2d\u786e\u5b9a\u53bb\u9664GrLivArea > 4000 \u4e14SalePrice < 200000\u7684\u70b9\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea'] > 4000) & (train_df['SalePrice'] < 200000)].index)","bcdfc93b":"train_df.dtypes[(train_df.dtypes == 'object')].index","a7d03a99":"var = 'BldgType'  # \u623f\u5c4b\u6574\u4f53\u6750\u6599\u548c\u5149\u6d01\u5ea6\u7684\u8bc4\u4f30\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)  # \u6a2a\u5750\u6807\u7c7b\u522b\uff0c\u7eb5\u5750\u6807\u76ee\u6807\u53d8\u91cf\nfig.axis(ymin=0, ymax=800000);","e9f9368d":"train_df['SalePrice'] = np.log1p(train_df['SalePrice'])\nsns.distplot(train_df['SalePrice'], color='blue')\n","29873b51":"print (train_df['SalePrice'].skew())\ntrain_df['SalePrice'].kurt()","0ebc32e7":"from sklearn.preprocessing import LabelEncoder\n# categories = [i for i in train_df.columns if i not in cols]\n# process columns, apply LabelEncoder to categorical features\n\ncategories = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold', 'BedroomAbvGr']\n\nfor c in categories:\n    lbl = LabelEncoder() \n    train_df[c] = lbl.fit_transform(list(train_df[c].values))\n    test_df[c] = lbl.fit_transform(list(test_df[c].values))\n\n\n# shape        \nprint ('Shape all_data: {}'.format(train_df.shape))\nprint ('Shape all_data: {}'.format(test_df.shape))\n","488d0c3e":"numeric_features = list(train_df.dtypes[train_df.dtypes != 'object'].index)\nnumeric_features.remove('SalePrice')\nlen(numeric_features)","b246fa3f":"# box-cox\u53d8\u6362\nfrom scipy.special import boxcox1p\n# skewed_features = list(skewness)\nlam = 0.15\nfor feature in numeric_features:\n    #all_data[feat] += 1\n    train_df[feature] = boxcox1p(train_df[feature], lam)\n    test_df[feature] = boxcox1p(test_df[feature], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","f4721fea":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = StandardScaler()\nscaler.fit(train_df[numeric_features])\ntrain_df[numeric_features] = scaler.transform(train_df[numeric_features])\ntest_df[numeric_features] = scaler.transform(test_df[numeric_features])\n\n# scaler = MinMaxScaler()\n# scaler.fit(train_df[numeric_features])\n# train_df[numeric_features] = scaler.transform(train_df[numeric_features])\n# test_df[numeric_features] = scaler.transform(test_df[numeric_features])","63616332":"category_columns = train_df.dtypes[train_df.dtypes == 'object'].index","bb3f67a1":"for i in category_columns:\n    if len(train_df[i].value_counts().index) <= 2:\n        print  (\"Train\\n\" +  i)\n        print  (train_df[i].value_counts())\n        \n    if len(test_df[i].value_counts().index) <= 2:\n        print (\"Test\\n\" + i)\n        print (test_df[i].value_counts())","2863e78d":"drop_columns = 'Utilities'\ntrain_df = train_df.drop(drop_columns, axis=1)\ntest_df = test_df.drop(drop_columns, axis=1)","aac5df03":"features_train = train_df.drop(['SalePrice'], axis=1)\nlabels_train = train_df['SalePrice']\nfeatures_test = test_df","296231c4":"features_train = pd.get_dummies(features_train)\nfeatures_test = pd.get_dummies(test_df)\n\nmissing_cols = set(features_train.columns) - set(features_test.columns)\nfor column in missing_cols:\n    features_test[column] = 0\n    \n# \u4fdd\u8bc1\u6d4b\u8bd5\u96c6columns\u7684\u987a\u5e8f\u540c\u8bad\u7ec3\u96c6columns\u76f8\u540c\uff0c\u7279\u522b\u91cd\u8981\uff01\uff01\uff01\uff01\uff01\uff01\nfeatures_test = features_test[features_train.columns]","0dbbca55":"print (features_train.columns)","de4271a6":"# from sklearn.model_selection import train_test_split\n\n# # \u5206\u5272features_train \u548c labels_train, \u6d4b\u8bd5\u96c6\u5927\u5c0f = 20%\uff0c\u72b6\u6001\uff1a\u968f\u673a\uff0c\u53ef\u590d\u73b0\n# # \u987a\u5e8f\uff1a\u6d4b\u8bd5\u7279\u5f81\uff0c\u8bad\u7ec3\u7279\u5f81\uff0c\u6d4b\u8bd5\u76ee\u6807\uff0c\u8bad\u7ec3\u76ee\u6807\n\n# X_train, X_test, y_train, y_test = train_test_split(features_train, labels_train, test_size = 0.2, random_state = 42)\n\n# \u8f93\u51fa\u6570\u91cf\u89c2\u5bdf\n\nX_train = features_train\ny_train = labels_train\nprint (len(X_train))","b7aee852":"# # \u5bfc\u5165\u7b97\u6cd5\u6a21\u578b\u548c\u8bc4\u5206\u6807\u51c6 \nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import fbeta_score, make_scorer, r2_score ,mean_squared_error\nfrom sklearn.linear_model import Lasso\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\n# def rmsle_cv(model, train, value):\n#     kf = KFold(5, shuffle=True, random_state=42).get_n_splits(train.values)\n#     rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n#     return rmse.mean()\n\n# # for key_num in range(10,25,3):\n\n# #     cols_model = cols[:key_num]\n# #     drop_columns = [i for i in corrmat.columns if i not in cols_model]\n# #     print drop_columns , \"\\n\"\n# #     X_train_model = X_train.drop(drop_columns, axis=1)\n\n# #         # \u521d\u59cb\u5316,\u786e\u5b9a\u968f\u673a\u72b6\u6001\uff0c\u53ef\u590d\u73b0\n# #     reg1 = DecisionTreeRegressor(random_state = 42)\n# #     reg2 = LinearRegression()\n# #     reg3 = RandomForestRegressor(random_state = 42)\n# #     reg4 = XGBRegressor()\n# #     reg5 = Lasso(alpha=0.001, random_state= 42)\n# #     reg6 = SVR(kernel = 'rbf')\n# #     reg7 = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)\n\n# #     # \u5efa\u7acb\u5b57\u5178\uff0c\u6536\u96c6\u5b66\u4e60\u5668\u7684\u6548\u679c\n# #     # \u5b66\u4e60\uff0c\u6536\u96c6\u9884\u6d4b\u5f97\u5206\n# #     results = {}\n# #     for reg in [reg1, reg2, reg3, reg4, reg5, reg6, reg7]:\n# #         # \u56de\u5f52\u5668\u7684\u540d\u79f0\n# #         reg_name = reg.__class__.__name__\n# # #         reg.fit(X_train_model, y_train)\n# # #         pred_test = reg.predict(X_test_model)\n# # #         results[reg_name] = rmse(y_test, pred_test)\n# #         results[reg_name] = rmsle_cv(reg, X_train_model, y_train)\n# #     print key_num, \"---\", results\n# #     print '\\n'","989fb3f7":"cols = cols[:22] \ndrop_columns = [i for i in corrmat.columns if i not in cols]\n\nprint (drop_columns , \"\\n\")\nX_train = X_train.drop(drop_columns, axis=1)\nprint (len(X_train.columns))","6f080a40":"features_test = features_test.drop(drop_columns, axis=1)","42b87780":"from sklearn.metrics import fbeta_score, make_scorer, r2_score ,mean_squared_error\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","af7e8125":"# \u6a21\u578b\uff1aRandomForest\n# \u5bfc\u5165Grid\nfrom sklearn.model_selection import GridSearchCV\n\n# \u521d\u59cb\u5316\u56de\u5f52\u6a21\u578b\nreg = RandomForestRegressor(random_state=42)\n\n# \u786e\u5b9a\u53c2\u6570\u5217\u8868\nparameters = {\n    'max_leaf_nodes': range(20, 100 ,10)\n}\n\n# \u786e\u5b9a\u8bc4\u5206\u6807\u51c6\nscorer = 'neg_mean_squared_error'\n\n# \u56de\u5f52\u6a21\u578b\u4f7f\u7528\u7f51\u683c\u641c\u7d22\ngrid_reg = GridSearchCV(reg, parameters, scoring = scorer)\n\n# \u8bad\u7ec3\ngrid_reg.fit(X_train, y_train)\n\n# grid_reg.cv_results_\n\n# \u83b7\u5f97\u6700\u4f73\u62df\u5408\u56de\u5f52\u5668\nbest_reg_rf = grid_reg.best_estimator_","1a97f915":"# print grid_reg.best_estimator_\nprint (grid_reg.best_estimator_)\npred_rf = best_reg_rf.predict(X_train)\nrmsle(pred_rf, y_train)","4a8dc41b":"# \u6a21\u578b\uff1a\u7ebf\u6027\u56de\u5f52\nfrom sklearn.model_selection import GridSearchCV\n\n# \u521d\u59cb\u5316\u56de\u5f52\u6a21\u578b\nreg = Lasso( alpha=0.001, random_state= 42)\n\n# \u786e\u5b9a\u53c2\u6570\u5217\u8868\nparameters = {\n    'normalize': [True, False],\n    'alpha': [0, 0.0001, 0.0005, 0.001]\n}\n\n# \u786e\u5b9a\u8bc4\u5206\u6807\u51c6\nscorer = 'neg_mean_squared_error'\n\n# \u56de\u5f52\u6a21\u578b\u4f7f\u7528\u7f51\u683c\u641c\u7d22\ngrid_reg = GridSearchCV(reg, parameters, scoring = scorer)\n\n# \u8bad\u7ec3\ngrid_reg.fit(X_train, y_train)\n\n# \u83b7\u5f97\u6700\u4f73\u62df\u5408\u56de\u5f52\u5668\nbest_reg_lasso = grid_reg.best_estimator_","b46cc504":"print (grid_reg.best_estimator_)\npred_lasso = best_reg_lasso.predict(X_train)\nrmsle(pred_lasso, y_train)","e37c5366":"reg = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state = 42)\n\nparameters = {\n    'alpha':[0, 0.0001, 0.0005, 0.001],\n    'l1_ratio':np.arange(0, 1, 0.1)\n}\n\nscorer = 'neg_mean_squared_error'\n\ngrid_reg = GridSearchCV(reg, parameters, scoring= scorer)\n\ngrid_reg.fit(X_train, y_train)\n\nbest_elasticNet_reg =  grid_reg.best_estimator_","e20ebbf2":"print (grid_reg.best_estimator_)\npred_elasticNet_reg = best_elasticNet_reg.predict(X_train)\nrmsle(pred_elasticNet_reg, y_train)","ba5451a4":"from sklearn.svm import SVR\n\nreg = SVR(kernel = 'rbf')\n\nparameters = {\n    'C': np.arange(1.1,2,0.1),\n    'gamma':np.arange(0,0.1,0.01)\n}\n\nscorer = 'neg_mean_squared_error'\n\ngrid_reg = GridSearchCV(reg, parameters, scoring = scorer)\n\ngrid_reg.fit(X_train, y_train)\n\nbest_reg_SVR = grid_reg.best_estimator_","11cacda7":"print (grid_reg.best_estimator_)\npred_lasso = best_reg_lasso.predict(X_train)\nrmsle(pred_lasso, y_train)","5dda7aeb":"# # \u786e\u5b9a\u6700\u4f73\u51b3\u7b56\u6811\u7684\u6570\u91cf, xgboost\u4e2d\u7684cv\u51fd\u6570\u6765\u786e\u5b9a\u6700\u4f73\u7684\u51b3\u7b56\u6811\u6570\u91cf, \u63d0\u9ad8Xgboost\u8c03\u53c2\u901f\u5ea6\n# # \u540e\u8fb9\u6709 early_stopping_rounds \u4e2armse\u6ca1\u6709\u4e0b\u964d\u7684\u5c31\u505c\u6b62\n# # \u539f\u8bdd\uff1a\n# # Activates early stopping. CV error needs to decrease at least\n# #        every <early_stopping_rounds> round(s) to continue.\n# #        Last entry in evaluation history is the one from best iteration.\n\n# import xgboost as xgb\n# from xgboost.sklearn import XGBRegressor\n\n# def modelfit(alg, X_train, y_train, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    \n#     if useTrainCV:\n#         xgb_param = alg.get_xgb_params()\n#         xgtrain = xgb.DMatrix(X_train.values, label= y_train.values)\n#         cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n#             metrics='rmse', early_stopping_rounds = early_stopping_rounds)\n#         alg.set_params(n_estimators=cvresult.shape[0])\n#         print cvresult\n\n#     #Fit the algorithm on the data\n#     alg.fit(X_train, y_train,eval_metric='rmse')\n\n#     #Predict training set:\n#     dtrain_predictions = alg.predict(X_train)\n\n#     # \u6a21\u578b\u8bad\u7ec3\u62a5\u544a\n#     print \"\\nModel Report\"\n#     print \"RMSE Score (Train): %f\" % rmse(y_train, dtrain_predictions)\n    \n#     # \u7279\u5f81\u91cd\u8981\u6027\n#     feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)[:10,]\n#     feat_imp.plot(kind='bar', title='Feature Importances')\n#     plt.ylabel('Feature Importance Score')","ca0f595a":"# \u6a21\u578b\uff1aXgboost\nfrom sklearn.model_selection import GridSearchCV\n\nbest_reg_xgb = XGBRegressor(learning_rate= 0.01, n_estimators = 5000, \n                    max_depth= 4, min_child_weight = 1.5, gamma = 0.1, \n                   subsample = 0.7, colsample_bytree = 0.6, \n                   seed = 27)\n\n# modelfit(reg, X_train, y_train)","cb8fd3a8":"best_reg_xgb.fit(X_train, y_train)\npred_y_XGB = best_reg_xgb.predict(X_train)\nprint (rmsle(pred_y_XGB, y_train))","cced29c3":"# rmsle_cv(best_reg_xgb, X_train, y_train)","645d566a":"# param_test1 = {\n#     'max_depth': range(3,10,2),\n#     'min_child_weight': range(1,6,2)\n# }\n\n# scorer = make_scorer(rmse)\n\n# # \u8d1f\u5747\u65b9\u8bef\u5dee\n# grid_reg1 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.01, n_estimators=1605, \n#                                                 max_depth=5, min_child_weight = 1,gamma = 0,\n#                                                 subsample = 0.8, colsample_bytree = 0.8, seed = 27), \n#                          param_grid = param_test1, scoring = 'neg_mean_squared_error')\n# grid_reg1.fit(X_train, y_train)\n# grid_reg1.grid_scores_, grid_reg1.best_params_, grid_reg1.best_score_\n","131aa6b0":"# param_test1 = {\n#     'max_depth': [4, 5, 6],\n#     'min_child_weight': np.arange(1.0, 4.0, 0.5)\n# }\n\n# scorer = make_scorer(rmse)\n\n# # \u8d1f\u5747\u65b9\u8bef\u5dee\n# grid_reg1 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1, n_estimators=115, \n#                                                 max_depth=5, min_child_weight = 1,gamma = 0,\n#                                                 subsample = 0.8, colsample_bytree = 0.8, seed = 27), \n#                          param_grid = param_test1, scoring = 'neg_mean_squared_error')\n# grid_reg1.fit(X_train, y_train)\n# grid_reg1.grid_scores_, grid_reg1.best_params_, grid_reg1.best_score_","2f2302c4":"# param_test3 = {\n#     'gamma' : [i\/10.0 for i in range(0,5)]\n# }\n\n\n# grid_reg1 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1, n_estimators=115, \n#                                                 max_depth=4, min_child_weight = 1.5,gamma = 0,\n#                                                 subsample = 0.8, colsample_bytree = 0.8, seed = 27), \n#                          param_grid = param_test3, scoring = 'neg_mean_squared_error')\n# grid_reg1.fit(X_train, y_train)\n# grid_reg1.grid_scores_, grid_reg1.best_params_, grid_reg1.best_score_","7c29c115":"# param_test4 = {\n#     'subsample':np.arange(0.5, 1.0 ,0.05),\n#     'colsample_bytree':np.arange(0.5, 1.0, 0.05)\n# }\n\n\n# grid_reg1 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1, n_estimators=115, \n#                                                 max_depth=4, min_child_weight = 1.5,gamma = 0,\n#                                                 subsample = 0.8, colsample_bytree = 0.8, seed = 27), \n#                          param_grid = param_test4, scoring = 'neg_mean_squared_error')\n# grid_reg1.fit(X_train, y_train)\n# grid_reg1.grid_scores_, grid_reg1.best_params_, grid_reg1.best_score_","df755aae":"# param_test5 = {\n#     \"reg_alpha\":np.arange(0.0, 1.1, 0.1),\n#     \"reg_lambda\":np.arange(0.0, 1.1, 0.1)\n# }\n\n# grid_reg1 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1, n_estimators=115, \n#                                                 max_depth=4, min_child_weight = 1.5,gamma = 0,\n#                                                 subsample = 0.7, colsample_bytree = 0.6, seed = 27), \n#                          param_grid = param_test5, scoring = 'neg_mean_squared_error')\n# grid_reg1.fit(X_train, y_train)\n# grid_reg1.grid_scores_, grid_reg1.best_params_, grid_reg1.best_score_","6b396e21":"# reg2 = XGBRegressor(learning_rate=0.1, n_estimators= 1000, \n#                          max_depth=4, min_child_weight = 1.5,gamma = 0,\n#                         subsample = 0.7, colsample_bytree = 0.6, seed = 27)\n\n# modelfit(reg2, X_train, y_train )","31a0ca5c":"# pred_y_test = reg2.predict(X_test)\n# rmse(pred_y_test, y_test)","dd9a3fc1":"from mlxtend.regressor import StackingRegressor\n\n# metal_reg = Lasso(alpha= 0.0, random_state=42)\nmetal_reg = SVR(kernel= 'rbf', C = 20)\n\nstregr = StackingRegressor(regressors = [ best_reg_lasso, best_elasticNet_reg], meta_regressor = metal_reg)\n\n# params = {'meta-lasso__alpha':[0.1, 1.0, 10.0] }\n\n# grid = GridSearchCV(estimator=stregr,\n#                     param_grid=params,\n#                     cv=5,\n#                     refit=True)\n\n# grid.fit(X_train, y_train)\n# for params, mean_score, scores in grid.grid_scores_:\n#     print(\"%0.3f +\/- %0.2f %r\"\n#         % (mean_score, scores.std() \/ 2.0, params))\n\n\n# rmsle_cv(stregr, X_train, y_train).mean()","de71fc82":"stregr.fit(X_train, y_train)\nstacked_y_pred  = stregr.predict(X_train)\nrmsle(y_train, stacked_y_pred)","778a686c":"# print(__doc__)\n\n# import numpy as np\n# import matplotlib.pyplot as plt\n# from sklearn.naive_bayes import GaussianNB\n# from sklearn.svm import SVC\n# from sklearn.datasets import load_digits\n# from sklearn.model_selection import learning_curve\n# from sklearn.model_selection import ShuffleSplit\n\n\n# def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n#                         n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n#     \"\"\"\n#     Generate a simple plot of the test and training learning curve.\n\n#     Parameters\n#     ----------\n#     estimator : object type that implements the \"fit\" and \"predict\" methods\n#         An object of that type which is cloned for each validation.\n\n#     title : string\n#         Title for the chart.\n\n#     X : array-like, shape (n_samples, n_features)\n#         Training vector, where n_samples is the number of samples and\n#         n_features is the number of features.\n\n#     y : array-like, shape (n_samples) or (n_samples, n_features), optional\n#         Target relative to X for classification or regression;\n#         None for unsupervised learning.\n\n#     ylim : tuple, shape (ymin, ymax), optional\n#         Defines minimum and maximum yvalues plotted.\n\n#     cv : int, cross-validation generator or an iterable, optional\n#         Determines the cross-validation splitting strategy.\n#         Possible inputs for cv are:\n#           - None, to use the default 3-fold cross-validation,\n#           - integer, to specify the number of folds.\n#           - An object to be used as a cross-validation generator.\n#           - An iterable yielding train\/test splits.\n\n#         For integer\/None inputs, if ``y`` is binary or multiclass,\n#         :class:`StratifiedKFold` used. If the estimator is not a classifier\n#         or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n#         Refer :ref:`User Guide <cross_validation>` for the various\n#         cross-validators that can be used here.\n\n#     n_jobs : integer, optional\n#         Number of jobs to run in parallel (default 1).\n#     \"\"\"\n#     plt.figure()\n#     plt.title(title)\n#     if ylim is not None:\n#         plt.ylim(*ylim)\n#     plt.xlabel(\"Training examples\")\n#     plt.ylabel(\"Score\")\n#     train_sizes, train_scores, test_scores = learning_curve(\n#         estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n#     train_scores_mean = np.mean(train_scores, axis=1)\n#     train_scores_std = np.std(train_scores, axis=1)\n#     test_scores_mean = np.mean(test_scores, axis=1)\n#     test_scores_std = np.std(test_scores, axis=1)\n#     plt.grid()\n\n#     plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n#                      train_scores_mean + train_scores_std, alpha=0.1,\n#                      color=\"r\")\n#     plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n#                      test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n#     plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n#              label=\"Training score\")\n#     plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n#              label=\"Cross-validation score\")\n\n#     plt.legend(loc=\"best\")\n#     return plt","1d99957a":"# title = \"Learning Curves (Xgboost)\"\n# # Cross validation with 100 iterations to get smoother mean test and train\n# # score curves, each time with 20% data randomly selected as a validation set.\n# cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\n# estimator = XGBRegressor()\n# plot_learning_curve(estimator, title, features_train, labels_train, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n\n# plt.show()","8aa6e4f7":"pred = np.expm1(stregr.predict(features_test) * 0.5 + best_reg_xgb.predict(features_test) * 0.5)\npred","22104485":"# \u589e\u52a0\u7d22\u5f15\uff0c\u5217\u540d\uff0c\u6784\u5efaDataFrame\uff0c\u7b26\u5408\u8f93\u51fa\u6570\u636e\u683c\u5f0f\n\ntest_df = pd.read_csv('..\/input\/test.csv')\npredict_df = pd.DataFrame({'Id': test_df['Id'], 'SalePrice': pred})\n\n# DataFrame\u8bbe\u5b9aindex\npredict_df = predict_df.set_index('Id')\n\n# \u91cd\u547d\u540dDataFrame\u7684\u5217\uff0c\u5217\u540d = \u5b57\u5178{\u539f\uff1a\u66ff\u6362\u540e}\n# predict_df.rename(columns = {predict_df.columns[0]: 'Id'}, inplace=True)\n\npredict_df","bb1abaf5":"predict_df.to_csv('Submission.csv')","2c84271c":"### Xgboost","1618b36f":"### \u5206\u5272\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e","0db6266a":"## \u6a21\u578b\u878d\u5408","7439f08f":"#### 4\u3001\u964d\u4f4e\u5b66\u4e60\u901f\u7387\uff0c\u786e\u5b9a\u7406\u60f3\u53c2\u6570","8321bd66":"#### \u72ec\u70ed\u7f16\u7801\u5176\u4ed6\u4e0e\u987a\u5e8f\u65e0\u5173\u7684\u7279\u5f81","9cb663ba":"\u5220\u9664\u76f8\u5173\u6027\u8f83\u5f31\u7684\u7279\u5f81","7d4c0248":"As soon as 'SalePrice' walked away, we went to Facebook. Yes, now this is getting serious. Notice that this is not stalking. It's just an intense research of an individual, if you know what I mean.\n\nAccording to her profile, we have some common friends. Besides Chuck Norris, we both know 'GrLivArea' and 'TotalBsmtSF'. Moreover, we also have common interests such as 'OverallQual' and 'YearBuilt'. This looks promising!\n\nTo take the most out of our research, we will start by looking carefully at the profiles of our common friends and later we will focus on our common interests.","a2c54c29":"## \u6570\u636e4C\u5206\u6790","745b2c5d":"\u867d\u7136\u659c\u7387\u5c0f\uff0c\u4f46\u662f\u65b0\u623f\u76f8\u5bf9\u4e8e\u8001\u623f\u6765\u8bf4\u623f\u5c4b\u4ef7\u683c\u76f8\u5bf9\u8f83\u9ad8","b43b22c6":"\u4f7f\u7528Sklearn\u4e2d\u5b66\u4e60\u66f2\u7ebf\u51fd\u6570\uff1ahttp:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html","491d2215":"\n\n1\u3001\u786e\u8ba4\u4e3b\u89c2\u7279\u5f81\u9009\u62e9\u53d8\u91cf'OverallQual'\uff0c'GrLivArea' \uff0c'TotalBsmtSF'\u540c\u623f\u5c4b\u4ef7\u683c\u5f3a\u76f8\u5173\n\n2\u3001\u56e0\u4e3a\u201cGarageCars\u201d \u540c \u201cGarageArea\u201d\u5f3a\u76f8\u5173\uff0c\u6545\u9009\u62e9\u540c\u623f\u5c4b\u4ef7\u683c\u76f8\u5173\u6027\u66f4\u9ad8\u7684\u201cGarageCars\u201d\n\n3\u3001\u540c\u4e0a\uff0c\u9009\u62e9\u201cTotalBasmtSF\u201d\n\n4\u3001\u201cTotRmsAbvGrd\u201d \u540c\u201cGrLivArea\u201d\u5f3a\u76f8\u5173\uff0c\u9009\u62e9\u201cGrLivArea\u201d\n\n5\u3001\"YearBuilt\"\u3010\u5f85\u3011\u65f6\u95f4\u5e8f\u5217\u5206\u6790\n\n6\u3001 \"FullBath\"\u6d74\u5ba4\u7684\u54c1\u8d28\uff0c\u4ece\u7bb1\u578b\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\u6d74\u5ba4\u7684\u54c1\u8d28\u540c\u623f\u5c4b\u7684\u4ef7\u683c\u5448\u6b63\u76f8\u5173\n\n    \u540c\u65f6\u5b58\u5728\u4e00\u4e9b\u65e0\u6d74\u5ba4\u7684\u623f\u5c4b","2d455aba":"\u4e22\u5f03\u8fc7\u591a\u7a7a\u6570\u636e\u7684\u5217\n\n\u90e8\u5206\u7c7b\u522b\u6570\u636e\u4e2d\u7684NA\u975e\u672a\u8ba1\u6570\/\u8ba1\u6570\u8bef\u5dee\uff0c\u800c\u662f\u65e0\uff08\u5730\u4e0b\u5ba4\uff0c\u6e38\u6cf3\u6c60\ud83c\udfca\u7b49\uff0c\u586b\u5145\u5b57\u7b26\u4e32\u201cNone\u201d\uff09","b25e34c5":"#### gamma\u53c2\u6570\u8c03\u4f18","83b43916":"#### \u603b\u7ed3","51ac82e6":"### \u968f\u673a\u68ee\u6797","f94ae0a5":"https:\/\/discuss.analyticsvidhya.com\/t\/what-should-be-the-allowed-percentage-of-missing-values\/2456\n\nI\uff1a\u5141\u8bb8\u7684missing values\u7684\u6570\u76ee\uff0c\u5148\u5b9a25%","5429ecac":"\u5b66\u4e60\u901f\u7387\u4e3a0.1\u7684\u60c5\u51b5\u4e0b\uff0c\u7406\u60f3\u51b3\u7b56\u6811115\uff0c 'max_depth'\u7406\u60f3\u503c 4, 'min_child_weight' \u7406\u60f3\u503c 1.5\uff0c gamma\u4e3a0","a59b89b1":"\u4f7f\u7528\u8f83\u4f4e\u7684\u5b66\u4e60\u901f\u7387\uff0c\u66f4\u591a\u7684\u51b3\u7b56\u6811","e3813bf5":"\u56de\u5f52\u6a21\u578b\u8bc4\u5206\u6307\u6807\u5305\u62ec\uff1a\n\n1\u3001SSE\u8bef\u5dee\u5e73\u65b9\u548c\uff0c RMSE\n\n2\u3001R-square\uff08\u51b3\u5b9a\u7cfb\u6570\uff09","40ac026f":"* <b>\u76f8\u5173\u6027\u77e9\u9635<\/b>\n* <b>\u623f\u5c4b\u4ef7\u683c\u76f8\u5173\u6027\u77e9\u9635<\/b>\n* <b>\u6700\u76f8\u5173\u53d8\u91cf\u6563\u70b9\u56fe<\/b>","e0c621ad":"*Everything started in our Kaggle party, when we were looking for a dance partner. After a while searching in the dance floor, we saw a girl, near the bar, using dance shoes. That's a sign that she's there to dance. We spend much time doing predictive modelling and participating in analytics competitions, so talking with girls is not one of our super powers. Even so, we gave it a try:*\n\n*'Hi, I'm Kaggly! And you? 'SalePrice'? What a beautiful name! You know 'SalePrice', could you give me some data about you? I just developed a model to calculate the probability of a successful relationship between two people. I'd like to apply it to us!'*","779ad50e":"## \u9996\u5148\u7528\u7b80\u5355\u7684\u6a21\u578b\u8fdb\u884c\u8bd5\u9a8c\uff0c\u89c2\u5bdf\u8bc4\u5206","8efc89f3":"# \u5f85\u4f18\u5316\u7b56\u7565","d763db1b":"## \u5bfc\u5165\u73af\u5883\u5e93","2f250cd6":"### \u5b66\u4e60\u66f2\u7ebf","ab3f463c":"1\u3001In order to have some discipline in our analysis, we can create an Excel spreadsheet with the following columns:\n\n* <b>Variable<\/b> - Variable name.\n* <b>Type<\/b> -  There are two possible values for this field: 'numerical' or 'categorical'. \n* <b>Segment<\/b> - Identification of the variables' segment. We can define three possible segments: building, space or location. When we say 'building', we mean a variable that relates to the physical characteristics of the building (e.g. 'OverallQual'). When we say 'space', we mean a variable that reports space properties of the house (e.g. 'TotalBsmtSF'). Finally, when we say a 'location', we mean a variable that gives information about the place where the house is located (e.g. 'Neighborhood').\n* <b>Expectation<\/b> - Our expectation about the variable influence in 'SalePrice'. We can use a categorical scale with 'High', 'Medium' and 'Low' as possible values.\n    \n    \u91cd\u8981\uff0c\u4e00\u4e00\u4e2a\u4e00\u4e2a\u9605\u8bfb\u7279\u5f81\u63cf\u8ff0\u3002\u662f\u5426\u5f71\u54cd\u6211\u4eec\u8d2d\u4e70\u623f\u5c4b -> \u5982\u679c\u662f\uff0c\u5f71\u54cd\u91cd\u8981\u6027\uff1f -> \u4fe1\u606f\u662f\u5426\u65e9\u5df2\u5728\u5176\u4ed6\u53d8\u91cf\u4e2d\u5305\u542b\n* <b>Conclusion<\/b> - Our conclusions about the importance of the variable, after we give a quick look at the data. We can keep with the same categorical scale as in 'Expectation'.\n* <b>Comments<\/b> - Any general comments that occured to us.\n","b66fceaa":"\n### SVR_rbf ","4e8d1525":"### \u3010\u6b63\u786e\u6027\u3011","9e581d59":"### \u76ee\u6807","1524d0f0":"1\u3001\u623f\u5c4b\u5c45\u4f4f\u9762\u79ef\u548c\u623f\u5c4b\u5730\u4e0b\u5ba4\u7684\u9762\u79ef\u540c\u623f\u5c4b\u4ef7\u683c\u5448\u6b63\u76f8\u5173\n\n2\u3001\u623f\u5c4b\u6574\u4f53\u6750\u6599\u8d28\u91cf\u548c\u5efa\u9020\u5e74\u4efd\u540c\u623f\u5c4b\u4ef7\u683c\u76f8\u5173\u3002\u5176\u4e2d\u623f\u5c4b\u6574\u4f53\u6750\u6599\u8d28\u91cf\u540c\u623f\u5c4b\u4ef7\u683c\u76f8\u5173\u6027\u66f4\u5f3a\n\n\u4ee5\u4e0a\uff1a\u7279\u5f81\u9009\u62e9\uff0c\u4e3b\u89c2\u6027\n\nThe trick here seems to be the choice of the right features (feature selection) and not the definition of complex relationships between them (feature engineering).\n\nThat said, let's separate the wheat from the chaff.\n\n\u4ee5\u4e0b\uff1a\u7279\u5f81\u5de5\u7a0b\uff0c\u5ba2\u89c2\u6027","d8f3a8ff":"### \u3010\u503e\u659c\u6570\u636e\u5904\u7406\u3011","ceffe6fa":"#### \u540c\u6570\u503c\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb (\u6563\u70b9\u56fe\uff09","810d5d2d":"Normality - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue.\n\nHowever, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.\n\n\u6b63\u6001\u5206\u5e03","a6fff520":"#### \u540c\u7c7b\u522b\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb \uff08\u7bb1\u578b\u56fe\uff09","ab4ba9de":"\u5b66\u4e60\u901f\u7387\u4e3a0.1\u7684\u60c5\u51b5\u4e0b\uff0c\u7406\u60f3\u51b3\u7b56\u6811115\uff0c 'max_depth'\u7406\u60f3\u503c 4, 'min_child_weight' \u7406\u60f3\u503c 1.5\uff0c gamma\u4e3a0, \n\ncolsample_bytree\u4e3a0.6\uff0c subsample\u4e3a0.7\uff0cMSE\u4e3a0.0156","12eb41ee":"\u95ee\u9898\u4e3a\u56de\u5f52\u95ee\u9898\uff0c\u53ef\u7528\u6a21\u578b\uff1a\u7ebf\u6027\u56de\u5f52\uff0c\u51b3\u7b56\u6811\uff08C&RT\u51b3\u7b56\u6811\uff09\uff0c\u968f\u673a\u68ee\u6797\uff0cGBDT","fe4632bd":"1\u3001TotalBsmtSF \u4e3a0 = \u623f\u5c4b\u4e2d\u6ca1\u6709\u5730\u4e0b\u5ba4\uff0c\u6570\u636e\u6b63\u5e38\n\n2\u3001\u6545\u4ec5\u9488\u5bf9GrLivArea \u505a\u5f02\u5e38\u503c\u53bb\u9664","1b0ab579":"\u66f4\u52a0\u5b8f\u89c2\n\n1\u3001\u201cTotalBsmtSF\u201d \u540c \u201c1stFlrSF\u201d\u5f3a\u76f8\u5173\uff0c\u8868\u8fbe\u540c\u6837\u7684\u4fe1\u606f\n\n2\u3001\u201cGarageCars\u201d  \u540c \u201cGarageArea\u201d\u5f3a\u76f8\u5173\uff0c\u8868\u8fbe\u540c\u6837\u7684\u4fe1\u606f\n\n3\u3001\u9a8c\u8bc1\u201cSalePrice\u201d \u540c\u4e3b\u89c2\u9009\u62e9\u7684\u7279\u5f81\u76f8\u5173\uff0c\u4ecd\u6709\u90e8\u5206\u7279\u5f81\u9700\u52a0\u5165\"SalePrice\"\u9884\u6d4b","2c682b49":"1\u3001\u4ec5\u9760\u53c2\u6570\u7684\u8c03\u6574\u548c\u6a21\u578b\u7684\u5c0f\u5e45\u4f18\u5316\uff0c\u60f3\u8981\u8ba9\u6a21\u578b\u7684\u8868\u73b0\u6709\u4e2a\u5927\u5e45\u5ea6\u63d0\u5347\u662f\u4e0d\u53ef\u80fd\u7684\n\n2\u3001\u8981\u60f3\u8ba9\u6a21\u578b\u7684\u8868\u73b0\u6709\u4e00\u4e2a\u8d28\u7684\u98de\u8dc3\uff0c\u9700\u8981\u4f9d\u9760\u5176\u4ed6\u7684\u624b\u6bb5\uff1a\n\n    1\u3001\u7279\u5f81\u5de5\u7a0b(feature egineering) \n    \n    2\u3001\u6a21\u578b\u7ec4\u5408(stacking)","4b9a7da9":"#### \u6700\u76f8\u5173\u53d8\u91cf\u6563\u70b9\u56fe","046c6574":"* <b> \u504f\u5ea6 <\/b>  \u6b63\u6001\u5206\u5e03\u7684\u504f\u5ea6\u4e3a0\u3002\u82e5\u6570\u636e\u5206\u5e03\u662f\u5bf9\u79f0\u7684\uff0c\u504f\u5ea6 = 0\u3002\n\n     \u82e5\u504f\u5ea6 > 0\uff0c\u5206\u5e03\u4e3a\u53f3\u504f\uff0c\u5373\u5206\u5e03\u6709\u4e00\u6761\u957f\u5c3e\u5728\u53f3\uff1b\n     \n     \u82e5\u504f\u5ea6 < 0\uff0c\u5206\u5e03\u4e3a\u5de6\u504f\uff0c\u5373\u5206\u5e03\u6709\u4e00\u6761\u957f\u5c3e\u5728\u5de6\u3002\u504f\u5ea6\u7684\u7edd\u5bf9\u503c\u8d8a\u5927\uff0c\u8bf4\u660e\u5206\u5e03\u7684\u504f\u79fb\u7a0b\u5ea6\u8d8a\u4e25\u91cd\u3002\n\n\n* <b> \u5cf0\u5ea6 <\/b>  \u6b63\u6001\u5206\u5e03\u7684\u5cf0\u5ea6\u4e3a0\u3002\n\n    \u5f53\u5cf0\u5ea6 > 0\uff0c\u5b83\u76f8\u6bd4\u4e8e\u6b63\u6001\u5206\u5e03\u8981\u66f4\u9661\u5ced\u6216\u5c3e\u90e8\u66f4\u539a\u3002\n    \n    \u5cf0\u5ea6\u7cfb\u6570 < 0, \u5b83\u76f8\u6bd4\u4e8e\u6b63\u6001\u5206\u5e03\u66f4\u5e73\u7f13\u6216\u5c3e\u90e8\u66f4\u8584\u3002","4bf04e84":"#### 3\u3001\u6b63\u5219\u5316\u53c2\u6570\u8c03\u4f18","f24ad0ee":"# \u6570\u636e\u9884\u5904\u7406","aeb24ec2":"\u623f\u5c4b\u4ef7\u683c\u540c\u5c45\u4f4f\u9762\u79ef\uff08\u5e73\u65b9\u82f1\u5c3a\uff09\u7ebf\u6027\u76f8\u5173","61401842":"Kaggle:https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data","35400fc6":"\u6570\u636e\u7684\uff1a\n\n\u6b63\u786e\u6027\uff1aReviewing the data, there does not appear to be any aberrant or non-acceptable data inputs.\u5f02\u5e38\u503c\n\n\u5b8c\u6574\u6027\uff1aNULL \/ NAN\u3002\u5220\u9664\/\u8865\u5168\n\n\u521b\u9020\u6027\uff1aFeature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome. For this dataset, we will create a title feature to determine if it played a role in survival.\n\n\u8f6c\u53d8\uff1a\u7c7b\u522b\u6570\u636e -> \u72ec\u70ed\u7f16\u7801","704f3797":"'max_depth'\u7406\u60f3\u503c 4, 'min_child_weight' \u7406\u60f3\u503c 1.5 \u4ea4\u53c9\u9a8c\u8bc1\u5f97\u5206\uff1a0.016","4d1da17b":"\u5b66\u4e60\u901f\u7387\u4e3a0.01\u7684\u60c5\u51b5\u4e0b\uff0c\u7406\u60f3\u51b3\u7b56\u68115000\uff0c \n\n'max_depth'\u7406\u60f3\u503c 4, 'min_child_weight' \u7406\u60f3\u503c 1.5\uff0c gamma\u4e3a0,\n\ncolsample_bytree\u4e3a0.6\uff0c subsample\u4e3a0.7\n\nalpha\u4e3a0.1\uff0clambda\u4e3a0\n\nMSE\u4e3a","b6f0d609":"# \u6a21\u578b\u5b9e\u73b0\u4e0e\u878d\u5408","4b677a70":"#### \u8c03\u6574subsample \u548c colsample_bytree \u53c2\u6570","a0f4f073":"### 1\u3001\u786e\u5b9a\u5b66\u4e60\u901f\u7387\u548ctree_based \u53c2\u6570\u8c03\u4f18\u7684\u4f30\u8ba1\u5668\u6570\u76ee","192463c1":"\u6240\u6709\u7c7b\u522b\u7279\u5f81","8b8ff446":"* Kaggle\u63cf\u8ff0\uff1a It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n\n\u9884\u6d4b\u623f\u4ef7 79\u4e2a\u7279\u5f81\u7684\u6570\u636e\u96c6","b668f2a0":"### ElasticNet","54d625bb":"### \u6570\u503c\u7279\u5f81\u7f3a\u5931\u503c\u586b\u5145","46f856bb":"\u7406\u60f3max_depth = 5\uff0c min_child_weight = 3\uff0c\u503c\u9644\u8fd1\u7ec6\u5316\u8c03\u6574","b1113ae1":"\u8282\u70b9\u5206\u88c2\u6240\u9700\u7684\u6700\u5c0f\u635f\u5931\u51fd\u6570\u4e0b\u964d\u503c","2a2be873":"max_depth \u6811\u7684\u6700\u5927\u6df1\u5ea6\n\nmin_child_weight \u6700\u5c0f\u53f6\u5b50\u8282\u70b9\u6837\u672c\u6743\u91cd\u548c\uff1a\n\n\n","03b84b2c":"# \u9884\u6d4b\u76ee\u6807\u6570\u636e","6655fc92":"### \u76ee\u6807\u6570\u636e\u7684Buddies and interests  \uff08\u4e24\u4e2a\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\uff09","a9d0b701":"### \u3010\u521b\u9020\u6027\u3011","c7b7b16a":"ROUND 1\uff1a\n\nI\uff1a\u57fa\u7840\u673a\u5668\u5b66\u4e60\u6846\u67b6\u642d\u5efa,\u91c7\u7528Random Forest 0.18129 \n\nII\uff1aXgboost + \u6240\u6709\u7279\u5f81: 0.14339 \u2b06\ufe0f\uff08\u53ef\u89e3\u91ca\u6027\u5dee\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\uff09\n\nIII\uff1a\u53ea\u53d6\u76f8\u5173\u6027\u5f3a\u6570\u503c\u7279\u5f81 \uff1a0.14870 \u2b07\ufe0f\n\nIV\uff1a\u76f8\u5173\u6027\u5f3a\u6570\u503c\u7279\u5f81 + Label Coder \uff1a0.14628 \u2b07\ufe0f\n\nV: \u90e8\u5206\u6570\u503c\u7279\u5f81\u8f6c\u6362\u4e3a\u7c7b\u522b\u7279\u5f81\uff1aMsubclass\u7b49\uff1a0.13902 \u2b06\ufe0f\n\nVI\uff1a\u90e8\u5206\u7c7b\u522b\u7279\u5f81Label coder\u6709\u5e8f\uff1b\u90e8\u5206\u72ec\u70ed\u7f16\u7801\uff1a0.13814 \u2b06\ufe0f\n\nVII\uff1a\u5904\u7406\u503e\u659c\u7279\u5f81\u548c\u76ee\u6807\u53d8\u91cf\uff1a0.13806 \u2b06\ufe0f\n \nROUND2\uff1a\n\nI\uff1aXGboost\u8c03\u53c2 0.1325 -> 0.131 -> 0.12907  \u2b06\ufe0f\n\nII\uff1a\u591a\u79cd\u7b97\u6cd5stacking\uff1a0.12818 \u2b06\ufe0f\n\nIII\uff1a\u524d10\u6709\u6548\u7279\u5f81\uff0c\u878d\u5408stacking\u548cxgb\uff1a0.12584 \u2b06\ufe0f\n\nIIII\uff1a\u8fc7\u62df\u5408\u8f83\u4e25\u91cd\uff0c\u7b80\u5316\u6a21\u578b\uff0cstacking\u53bb\u9664RF\uff1a0.12138 \u2b06\ufe0f\n\n\n\u5f85\u4f18\u5316\u70b9\u6709\uff1a\n\n1\u3001\u8bc4\u5206\u6807\u51c6\u66f4\u6539\u4e3aKaggle\u6807\u51c6 \u2705\n\n2\u3001\u7279\u5f81\u5206\u6790\uff0c\u521b\u9020\u65b0\u7279\u5f81 \u2705\n\n\u7279\u5f81\u8f83\u591a\uff1a74\uff0c\u5982\u4f55\u9009\u53d6\u4ee5\u53ca\u63a2\u7d22\u6070\u5f53\u7684\u7279\u5f81\uff1f\n\n3\u3001\u63a2\u7d22\u6027\u5206\u6790\uff0c\u786e\u5b9a\u5173\u952e\u7279\u5f81 \u2705\n\n\u3010\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\u3011\n\n4\u3001Xgboost\u5b66\u4e60\uff0c\u8c03\u53c2  \u2705\n\n5\u3001\u96c6\u6210\u65b9\u6cd5 stacking   \u2705\n\n6\u3001\u6a21\u578b\u6709\u4e9b\u8fc7\u62df\u5408\uff0c\u8bd5\u56fe\u51cf\u5c11\u7279\u5f81\u7684\u6570\u91cf \u2705\n\n\n\u5f85\u89e3\u51b3\u95ee\u9898\u6709\uff1a\n\n1\u3001\u600e\u4e48\u5224\u65ad\u6a21\u578b\u5b58\u5728\u8fc7\u62df\u5408\uff1f\u2705\n\n\u5b66\u4e60\u66f2\u7ebf\uff0cLearning curve\n\n2\u3001\u7ebf\u6027\u56de\u5f52\u4e3a\u4ec0\u4e48\u6709\u8d1f\u503c \u2705\n\n\u5728\u505a\u7c7b\u522b\u6570\u636e -> \u6570\u503c\u8f6c\u5316\u65f6\uff0c\u8bad\u7ec3\u96c6\u540c\u6d4b\u8bd5\u96c6\u5217\u540d\u5b58\u5728\u533a\u522b\u3002\u53ea\u65b0\u589e\u4e86\u6d4b\u8bd5\u96c6\u7f3a\u5931\u7684\u5217\uff0c\u672a\u8c03\u6574\u5217\u7684\u987a\u5e8f\u3002\n\u6ce8\u610f\uff1atest = test[train.columns]\n\n3\u3001XGboost\u8c03\u53c2\u6280\u5de7 \u2705","91c00c3f":"#### \u8f6c\u5316\u540e\u6240\u6709\u7279\u5f81\u6807\u51c6\u5316","74ad75cc":"## \u5bfc\u5165\u6570\u636e\u6a21\u578b\u4e0e\u53ef\u89c6\u5316\u5e93","e9b35c7f":"#### \u9884\u6d4b\u503c\u503e\u659c\uff0c\u5bf9\u6570\u5904\u7406","9b2c2e8c":"\u623f\u5c4b\u4ef7\u683c\u540c\u5730\u4e0b\u5ba4\u7684\u9762\u79ef\u6210\u5f3a\u7ebf\u6027\uff08\u6307\u6570\uff1f\uff09\u76f8\u5173\n\n\u5b58\u5728\u4e00\u4e9b\u65e0\u5730\u4e0b\u5ba4\u7684\u623f\u5c4b\uff0c\u623f\u5c4b\u4ef7\u683c\u5206\u5e03\u5728200000\u4ee5\u5185","ff7b303e":"\u611f\u8c22\uff1a https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python \u63d0\u51fa\u7684\u5bf9\u6570\u636e\u8fdb\u884c\u63a2\u7d22\u7684\u65b9\u6cd5","74c027fe":"# \u63a2\u7d22\u6027\u6570\u636e\u5206\u6790 EDA","9fe72134":"### \u3010\u5b8c\u6574\u6027\u3011 (\u7f3a\u5931\u503c)","f4129109":"#### \u76ee\u6807\u53d8\u91cf\u76f8\u5173\u6027\u77e9\u9635 (SalePrice\uff0c\u653e\u5927\u70ed\u529b\u56fe\uff09","22426b11":"# \u83b7\u53d6\u6570\u636e","2dc20cab":"### Lasso\u56de\u5f52","b717f728":"\u6837\u672c\u62bd\u6837\u548c\u5217\u62bd\u6837\u7684\u6bd4\u91cd","9e79b8a8":"## \u7f51\u683c\u641c\u7d22\u8c03\u53c2","64b1c785":"#### \u6574\u4f53\u76f8\u5173\u6027\u77e9\u9635","c6e1f653":"2\u3001we can filter the spreadsheet and look carefully to the variables with 'High' 'Expectation'. Then, we can rush into some scatter plots between those variables and 'SalePrice', filling in the 'Conclusion' column which is just the correction of our expectations.","486b11da":"# \u5b9a\u4e49\u95ee\u9898","cb34ab01":"### \u5ba2\u89c2\u5206\u6790","765e2d9e":"\u5f39\u6027\u7f51\u7edc\uff0c\u7ed3\u5408L1\u548cL2\u6b63\u5219\u5316","9a9cc6d2":"### 2\u3001\u8c03\u6574Xgboost\u76f8\u5173\u53c2\u6570","579bf127":"\u5b66\u4e60\u901f\u7387\u4e3a0.1\u7684\u60c5\u51b5\u4e0b\uff0c\u7406\u60f3\u51b3\u7b56\u6811115\uff0c \n\n'max_depth'\u7406\u60f3\u503c 4, 'min_child_weight' \u7406\u60f3\u503c 1.5\uff0c gamma\u4e3a0,\n\ncolsample_bytree\u4e3a0.6\uff0c subsample\u4e3a0.7\n\nalpha\u4e3a0.1\uff0clambda\u4e3a0\n\nMSE\u4e3a0.0154","891a8b1e":"### \u3010\u8f6c\u5316\u6027\u3011\nLabel Encoding \u4e00\u4e9b\u53ef\u4ee5\u5305\u542b\u4fe1\u606f\u7684\u7c7b\u522b\u7279\u5f81\n\n**\u8868\u8fbe\u7c7b\u522b\u7279\u5f81\u4e4b\u95f4\u7684\u9ad8\u4f4e\u987a\u5e8fExcellent > Good > bad** \n\n\u6bcf\u5355\u4f4d\u53d8\u5316\u8ddf\u76ee\u6807\u53d8\u91cf\u5e76\u6ca1\u6709\u5173\u7cfb\uff01","7ca3277f":"### \u76ee\u6807\u6570\u636e\u7684\u3010\u8eab\u6750\u3011 SalePrice \uff08\u5355\u53d8\u91cf\u5206\u5e03\uff09","c7c155bd":"####  \u628a\u4e00\u4e9b\u6570\u503c\u7279\u5f81\u8f6c\u5316\u4e3a\u7c7b\u522b\u7279\u5f81","511778a4":"\u623f\u5c4b\u4ef7\u683c\u540c\u623f\u5c4b\u6574\u4f53\u6750\u6599\u548c\u5149\u6d01\u5ea6\u7684\u8bc4\u4f30\u5f3a\u76f8\u5173\uff0c\u6574\u4f53\u8bc4\u4f30\u5206\u6570\u8d8a\u9ad8\uff0c\u623f\u5c4b\u4ef7\u683c\u8d8a\u9ad8","d18b331a":"#### max_depth \u548c min_weight \u53c2\u6570\u8c03\u4f18","9e8f2a7b":"\u4e00\u822c\u6765\u8bf4\uff0c\u5bf9\u4e8e\u9ad8\u7ef4\u7684\u7279\u5f81\u6570\u636e\uff0c\u5c24\u5176\u7ebf\u6027\u5173\u7cfb\u662f\u7a00\u758f\u7684\uff0c\u6211\u4eec\u4f1a\u91c7\u7528Lasso\u56de\u5f52\u3002\u6216\u8005\u662f\u8981\u5728\u4e00\u5806\u7279\u5f81\u91cc\u9762\u627e\u51fa\u4e3b\u8981\u7684\u7279\u5f81\uff0c\u90a3\u4e48Lasso\u56de\u5f52\u66f4\u662f\u9996\u9009\u4e86\u3002\u4f46\u662fLasso\u7c7b\u9700\u8981\u81ea\u5df1\u5bf9\u03b1\u8c03\u4f18\uff0c\u6240\u4ee5\u4e0d\u662fLasso\u56de\u5f52\u7684\u9996\u9009\uff0c\u4e00\u822c\u7528\u5230\u7684\u662f\u4e0b\u4e00\u8282\u8981\u8bb2\u7684LassoCV\u7c7b\u3002\n\nhttps:\/\/www.cnblogs.com\/pinard\/p\/6026343.html","e49a7517":"\u5b66\u4e60\u901f\u7387\u4e3a0.1\u65f6\uff0c\u7406\u60f3\u7684\u51b3\u7b56\u6811\u6570\u76ee\u4e3a115\u4e2a","61bcc764":"## \u5206\u6790\u95ee\u9898\uff0c\u786e\u5b9a\u6a21\u578b","2aa5f8fb":"# \u7279\u5f81\u5206\u6790","7d8bb950":"#### \u8f6c\u5316\u6027","875f8502":"### \u7c7b\u522b\u7279\u5f81\u7f3a\u5931\u503c\u586b\u5145","c75107f5":"## \u8f7d\u5165\u603b\u89c8\u6570\u636e","25bfdae6":"\u53bb\u9664\u5f02\u5e38\u503c","df36f64c":"# \u8f93\u51fa\u7ed3\u679c","af6fae66":"* <b> \u6b63\u504f\u6001\u5206\u5e03 <\/b> \u6570\u636e\u4e3b\u8981\u96c6\u4e2d\u572810w ~ 25w\u4e4b\u95f4","5c682a80":"### \u786e\u5b9a\u7f3a\u5931\u5217"}}