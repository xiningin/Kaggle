{"cell_type":{"96f1eea1":"code","8476f420":"code","ee2405cc":"code","b13d5753":"code","88a671e1":"code","e326f80d":"code","a5a7a2d6":"code","11a84b03":"code","0f6707c5":"code","8892fea6":"code","d3610bb9":"code","632c0e7f":"code","7fa8037c":"code","455d08f3":"code","a0703bbe":"code","5a093503":"code","0e997b1d":"code","5d61086e":"code","ec18f7c8":"code","5be159ed":"code","cb2e83e8":"code","7753d753":"code","ac5369ac":"code","1497ae84":"code","41ea4d92":"code","f255420c":"code","08146caf":"code","57f60a57":"code","8aad2b4d":"code","017b6dc7":"code","96d4a4c3":"code","5d3f4079":"code","fd6c5d0a":"code","70abb6d3":"code","8e2c1da6":"code","81e4c2b6":"code","d032c4ca":"code","f9d101de":"code","71d78ada":"code","da932f4b":"code","3b1d637a":"code","9c04225d":"code","ae9fd5ac":"code","88b58ef8":"code","a8bfa4e4":"code","cf6322eb":"markdown","5882bfbb":"markdown","2d3d612b":"markdown","46bf435d":"markdown","8722ebc1":"markdown","7a9a1ddc":"markdown","f4910ad2":"markdown","459e2ad9":"markdown","3d84373a":"markdown","4ca77c27":"markdown","3b33beb4":"markdown","990186d5":"markdown","1d8456f0":"markdown","0fdc185b":"markdown","8a1e823b":"markdown","72d242c3":"markdown","1ef78a95":"markdown","ae8215a4":"markdown","60c046ed":"markdown"},"source":{"96f1eea1":"#importing all required libraries\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression \n\n\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nSc = StandardScaler()\n\nfrom sklearn.preprocessing import LabelEncoder \nLE=LabelEncoder()\n\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","8476f420":"#Importing the dataset\ntrain=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","ee2405cc":"#let us have look at columns of dataset\ntrain.head()","b13d5753":"#let us find the total number ofrows and columns. \ntrain.shape","88a671e1":"#stats of passangers and on numeric cols\ntrain.describe()\n#lets us know that some age col are missing..\n#also gives us more info about the price(fare)..\n\n","e326f80d":"#lets find total count of survivors      \n#0-> died ,1-> survived\ntrain['Survived'].value_counts()","a5a7a2d6":"#showcase the count of survivours..\nsns.countplot(train['Survived'])","11a84b03":"#count of survivers for the counts for columns 'sex','pclass','sibSp','parch','embarked'\n\ncols = ['Sex','Pclass','SibSp','Parch','Embarked','Fare']\nn_rows=2\nn_cols=3\n#There will be total 5 valid plots but as the number of rows and columns are 2 and 3 respectively we will get an index out of bound warning.\n\n#subplot grid and figure size of each graph\nfig,axs = plt.subplots(n_rows,n_cols,figsize = (n_cols*3.2,n_rows*3.2))\nfor r in range(0,n_rows):\n    for c in range(0,n_cols):\n        i =r*n_cols + c#index to go through no of cols\n        ax = axs[r][c]#shows to where to position subplot\n        sns.countplot(train[cols[i]],hue = train['Survived'], ax = ax)\n        ax.set_title(cols[i])\n        ax.legend(title='survived',loc = \"best\")\n\nplt.tight_layout()","0f6707c5":"#looking at survival rate by sex\ntrain.groupby(\"Sex\")[['Survived']].mean()","8892fea6":"#look at survival rate by class and sex\ntrain.pivot_table('Survived',index = 'Sex',columns = \"Pclass\").plot()","d3610bb9":"#plot the survival rate of each class\nsns.barplot(x = 'Pclass',y = 'Survived',data = train)","632c0e7f":"#Look up for prices of each class\nplt.scatter(train['Fare'],train['Pclass'],color = 'purple',label = 'Passanger paid')\nplt.ylabel('Class')\nplt.xlabel('price\/Fare')\nplt.title('Price  of each class ')\nplt.legend()\nplt.show()","7fa8037c":"#count the empty values in each col\ntrain.isna().sum()","455d08f3":"train['Age'].fillna(train['Age'].median(), inplace = True)","a0703bbe":"#drop the cols \ntrain=train.drop(['Cabin','Ticket','Name','PassengerId','Embarked'],axis=1)\n","5a093503":"# count new number of rows and cols in dataset\ntrain.shape","0e997b1d":"#look at datatype\ntrain.dtypes","5d61086e":"train.shape","ec18f7c8":"#print the unique values in cols\nprint(train['Sex'].unique())\n","5be159ed":"#transforming object datatype into int\ntrain.iloc[:,2] = LE.fit_transform(train.iloc[:, 2].values)","cb2e83e8":"#print the unique values in cols after transformation\n#1 -> Male, 0->female\nprint(train['Sex'].unique())","7753d753":"#spliting the data into  independent X and dependent Y\nX =train.iloc[:,1:7].values\nY =train.iloc[:,0].values ","ac5369ac":"#spliting dataset\nX_tr,X_te,Y_tr,Y_te =train_test_split(X,Y,test_size = 0.2,random_state=0)","1497ae84":"#Function creation to check various Machine Learning Models\ndef models(X_tr,Y_tr):\n    \n   \n    \n    #using kNN(K nearest neighbours).\n    from sklearn.neighbors import KNeighborsClassifier\n    Knn = KNeighborsClassifier(n_neighbors = 5,metric = 'minkowski', p=2)\n    Knn.fit(X_tr,Y_tr)\n    \n    #using Decision tree clissifer\n    from sklearn.tree import DecisionTreeClassifier\n    tree = DecisionTreeClassifier(criterion = 'entropy',random_state = 0)\n    tree.fit(X_tr,Y_tr)\n    \n    #using Random Forest\n    from sklearn.ensemble import RandomForestClassifier\n    forest = RandomForestClassifier(n_estimators = 300,criterion = 'entropy',random_state=0,max_depth=1,oob_score=True)\n    forest.fit(X_tr,Y_tr)\n    \n    #printing the training accuracy for each module\n    \n    print('[0] K Nearest Neighbors',Knn.score(X_tr,Y_tr))\n    print('[1] Decision Tree',tree.score(X_tr,Y_tr))\n    print('[2] Random Forest',forest.score(X_tr,Y_tr))\n    \n    return Knn,tree,forest\n    ","41ea4d92":"#get and train all models\nmodel = models(X_tr,Y_tr)","f255420c":"#show the confusion matrix and accuracy of all models on test data\nfor i in range(len(model)):\n    cm = confusion_matrix(Y_te,model[i].predict(X_te))\n    \n    #extraction True Negative , False Positive , False Negative ,True Positive\n    TN,FP,FN,TP =cm.ravel()\n    \n    test_score = (TP+TN)\/(TP+TN+FN+FP)\n    \n    print(cm)\n    print('Model[{}] Testing Accuracy = \"{}\"'.format(i,test_score))\n    print()\n    ","08146caf":"#print the pridction of the Logistic Regression\npred = model[2].predict(X_te)\nprint(pred)\nprint(\"*\" * 100)\n\n#print the actual values\nprint(Y_te)","57f60a57":"# Checking what are the data types in test dataset\ntest.dtypes\n","8aad2b4d":"test.isna().sum()","017b6dc7":"#Adding the missing vaues  using the dataset\ntest['Age'].fillna(test['Age'].median(), inplace = True)\ntest['Fare'].fillna(test['Fare'].median(), inplace = True)","96d4a4c3":"test.describe()","5d3f4079":"#filtering the test data according to important features\ntest.isna().sum()\n\n#saving passanger id to diffrent variable for future reference\nids = test['PassengerId']\n\n#drop the columns according to Tranning dataset.\ntest=test.drop(['Cabin','Ticket','PassengerId','Name','Embarked'],axis=1)\n","fd6c5d0a":"test.dtypes","70abb6d3":"#converting object datatype into Int64\ntest.iloc[:,1] = LE.fit_transform(test.iloc[:,1].values)\n","8e2c1da6":"test.shape","81e4c2b6":"# Assigning values to prediction variable\nxte = test.iloc[:,0:6].values","d032c4ca":"#prediction of test dataset\n\npred = model[2].predict(xte)\n\nprint(pred)\nprint('*' * 100)\nfor i in pred:\n    if(i==0):\n        print('Dead')\n    else:\n        print('Survived')\n","f9d101de":"#ploting the prediction\nsns.countplot(pred)\n","71d78ada":"#importing gender_submission to check the accuracy of predictions.\ncheck = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ncheck=check.drop(['PassengerId'],axis=1)","da932f4b":"#Accuracy check\ncm = confusion_matrix(check,model[2].predict(xte))\nTN,FP,FN,TP =cm.ravel()\ntest_score = (TP+TN)\/(TP+TN+FN+FP)\nprint(cm)\nprint(\"Accuracy of our prediction usingRandom forest is\",test_score)","3b1d637a":"#Lets check the Accuracy of our predictions using other models too.\n#[0] logistic regressing\n#[1] K Nearest Neighbors\n#[2] Decision Tree\n#[3] Random Forest\n\nfor i in range(len(model)):\n    cm = confusion_matrix(check,model[i].predict(xte))\n    \n    #extraction True Negative , False Positive , False Negative ,True Positive\n    TN,FP,FN,TP =cm.ravel()\n    \n    test_score = (TP+TN)\/(TP+TN+FN+FP)\n    \n    print(cm)\n    print('Model[{}] Testing Accuracy = \"{}\"'.format(i,test_score))\n    print()\n    ","9c04225d":"#Let us Save our Predictions into CSV File.\n#using the passangerids which were kept aside previously.\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': y_pred })\noutput.to_csv('submissio.csv', index=False)\nsub = pd.read_csv(\"submissio.csv\")\nsub.head()","ae9fd5ac":"from sklearn.metrics import accuracy_score\n\nC_param_range = [0.001,0.01,0.1,1,10,100]\nj = 0\nfor i in C_param_range:\n    \n    # Apply logistic regression model to training data\n    lr = LogisticRegression(penalty = 'none', C = i,random_state = 0,max_iter=1000)\n    lr.fit(X_tr,Y_tr)\n    \n    # Predict using model\n    y_pred = lr.predict(xte)\n    \n    # Saving accuracy score in table\n    #st = accuracy_score(Y_te,y_pred)\n    j += 1\n    ","88b58ef8":"y_pred","a8bfa4e4":"tm = confusion_matrix(check,pred)\ntest_score = (TP+TN)\/(TP+TN+FN+FP)\nTN,FP,FN,TP =tm.ravel()\nprint(tm)\nprint(\"Accuracy of our prediction using Logstic Regression is\",test_score)","cf6322eb":"Give detailed Information about the dataset.","5882bfbb":"It is necessary to filter out the unwanted data such as their cabin,ticket number,passanger name and passanger id.\nAlso its very important to minize the error rate and increase accuracy of our prediction. ","2d3d612b":"Now here we interact with actual Test-Dataset.\nOur dataset contains some unwanted data so we now filter out that data and keep only the key features  that were used to train our Model earlier.","46bf435d":"we can observe there are lots of value missing for Age and cabin.\nSuch missing values affects the tranning of model and results in false prediction.\nSo it becomes necessary to choose carefully what key features would to take to train our model","8722ebc1":"Filling up the missing values and replacing their values from the avaliable data from dataset.","7a9a1ddc":"These are the key features that have been selected for our model.\nWe can observe that we have Sex colums with data type as object as it contains values such as Male And Female.So in next steps we convert them into unique numeric values.","f4910ad2":"* * **The Accuracy of the prediction is totally based on proper selection of Key features**","459e2ad9":"Here we do the prediction of our Test-Dataset on our selected model.\nThis states that in the data at particular key points the passanger would have survived or would have been dead by displaying Survived or Dead ","3d84373a":"Let us Observe the Training Dataset first.","4ca77c27":"Spliting out Train dataser into 80% train data and 20% test data.\nSpliting is been carried out just to check the accuracy of multiple models and find out which of them has the highest pergentage of accuracy which in futurte can be used to make our predictions.","3b33beb4":"Here we can take a look on our training accuracy of various models defined in the function.","990186d5":"Filling up the missing values with help of other values from the dataset.","1d8456f0":"Here we can have lok at the confusion matrix created after prediction done on test data and also the accuracy of the particulat Model.\nThis helps us in choosing  Correct Model for Prediction.\nHere we can see that Random Forest Classifier has the highest accuracy in predicting out test data thus using same Model for making Prediction on Test-Dataset. ","0fdc185b":"Graphical representation of the number of Survivers and Dead passangers after prediction\n","8a1e823b":"This graph gives a brief information stating that there were higher chances of survival of passangers with first class Tickets and then following the second class and at last 3 class with vcery few chances of survival.","72d242c3":"* **Here we can observe the by using Logistic Regression we can get the highest Accuracy of prediction**\n* This is the order of testing accuracy from Highest to Lowest \n> 1. **logistic regressing - 0.992822966507177**\n> 2. **K Nearest Neighbors - 0.937799043062201**\n> 3. **Random Forest - 0.9210526315789473** \n> 4. **Decision Tree - 0.8995215311004785**","1ef78a95":"Here we convert columns with object datatypes  into Integers to avoid any values bound error","ae8215a4":"Above Graph Give a brief information about classifaction of passangers accordning to their class and also displays the price\/Fare paind by Passangers for particulat class.\nAll Passangers who paid Fare 100 pounds and above would certainly  be placed in first class.\nSo as a result he women in the first class had a higher chance of surviving as compared to 2nd & 3rd class compartments.\nThis information will be much helpful whle predecting the survival","60c046ed":"Finding out any Null values in Test dataset."}}