{"cell_type":{"a0cb707c":"code","26e9f0b1":"code","54b2cb16":"code","71b45e24":"code","ff3061eb":"code","a7b153fe":"code","65a5f719":"code","68ad183e":"code","8b0002d4":"code","2cbd4f54":"code","ccee3939":"code","875abb63":"code","70b2fb2b":"code","fbc43d54":"code","7048fb27":"code","133c4e70":"code","94dbc31a":"code","b6c18f08":"code","b259333f":"code","698f1302":"code","fab1d3a0":"code","c0d8640a":"code","9d0115ad":"code","ea5db96a":"markdown","82fecd55":"markdown","f7e4b267":"markdown","7e4827c6":"markdown","72436a9e":"markdown","69321eed":"markdown","9c4ce358":"markdown","75ae1583":"markdown","88f8c49b":"markdown","2bc1e496":"markdown","e221341e":"markdown","68312072":"markdown","9660c631":"markdown","a680fa54":"markdown","210a4758":"markdown","28dd9bc8":"markdown","bd29e88c":"markdown","41f204b9":"markdown","1eeaf32a":"markdown","c4d37243":"markdown","86990cdd":"markdown","3d462ca4":"markdown","a23a4fc9":"markdown","581e916e":"markdown","b0cbddaf":"markdown","b402c64e":"markdown","3e5bc97b":"markdown","18d77746":"markdown","e7242278":"markdown","0afe93b9":"markdown","b0d65bd9":"markdown","866f6e7c":"markdown","1029d748":"markdown","acbe8286":"markdown","3b93b7a7":"markdown","88397027":"markdown"},"source":{"a0cb707c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# here we will import the libraries used for machine learning\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, data manipulation as in SQL\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression # Logistic regression model\nfrom sklearn.model_selection import train_test_split # to split the data into training and test set\nfrom sklearn.model_selection import KFold # use for cross validation\nfrom sklearn.model_selection import GridSearchCV# for tuning parameter of models\nfrom sklearn.ensemble import RandomForestClassifier # for random forest classifier model\nfrom sklearn.neighbors import KNeighborsClassifier # for K Neighbors model\nfrom sklearn.tree import DecisionTreeClassifier # for Decision Tree model\nfrom sklearn import svm # for Support Vector Machine model\nfrom sklearn import metrics # for the check the error and accuracy of the model\n\n### Read Data direclty on Kaggle but also available here : https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29 ###\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndata=pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv') # import from a csv file\ndata.drop(\"Unnamed: 32\",axis=1,inplace=True) # delete unnecessary columns\n\n#### transform the problem into binary classification : Malignant = 1 ans Benign = 0 ###\ndata['diagnosis']=data['diagnosis'].map({'M':1,'B':0}) \ndata.head(10) # show data","26e9f0b1":"# As I said above the data can be divided into three parts corresponding to 3 dimensional features '(mean, se, worst)' \n# computed with the  the 3 dimensonial values (X,Y,Z)\n\nfeatures_mean= list(data.columns[2:12]) # mean group\nfeatures_se= list(data.columns[12:22]) # standard error group\nfeatures_worst=list(data.columns[22:32]) # features_worst group\n\nprint(\"-----------------------------------\")\nprint('Mean set of all features')\nprint(features_mean)\nprint(len(features_mean), 'features')\nprint(\"-----------------------------------\")\nprint('Standard Error set of all features')\nprint(features_se)\nprint(len(features_se), 'features')\nprint(\"------------------------------------\")\nprint('Worst set of all features')\nprint(features_worst)\nprint(len(features_worst), 'features')\nprint(' ')\nprint('Description of data columns')\ndata.iloc[:,1:].describe() # description of all columns in the dataset (30 features + diagnosis)","54b2cb16":"sns.heatmap(data[features_mean].corr(),annot=True,cmap='Blues')","71b45e24":"sns.heatmap(data[features_se].corr(),annot=True,cmap='Blues')","ff3061eb":"sns.heatmap(data[features_worst].corr(),annot=True,cmap='Blues')","a7b153fe":"#####  delete features higly corrolated described above  ###########\n\ncol_to_drop_corrolated1=['radius_mean','radius_se','radius_worst','area_mean','area_se','area_worst']\ncol_to_drop_corrolated2=['concavity_mean','concave points_mean', 'concavity_se','concave points_se' ,'concavity_worst','concave points_worst']\ndata.drop(col_to_drop_corrolated1+col_to_drop_corrolated2,axis=1,inplace=True)\nprint('I keep only ', len(data.columns) ,' features which are not so corrolated based on the previous analysis')","65a5f719":"predict = data.diagnosis.unique() # unique values from diagnosis features : 1 and 0\npredict_n=['Malin', 'Benin'] # labels\n\nfig, axes = plt.subplots(nrows=6, ncols=3, figsize=(15,25)) # axis configuration : 18 features on 6 * 3 charts\ni=0\naxes = axes.ravel()\nfor idx,ax in enumerate(axes): # for each chart\n    col=data.columns[i+2]\n    i=i+1\n    ax.hist([data.loc[data.diagnosis == x, col] for x in predict], label=predict_n, bins=25,stacked=False, alpha=0.7,color=['r','b'], histtype='step') # plot the feature histogram \n    ax.legend(loc='upper right') # legend of each chart\n    ax.set_title(col) # title\n\nplt.show() # show the 18 charts","68ad183e":"col_to_drop_mean=['fractal_dimension_mean', 'symmetry_mean', 'smoothness_mean', 'texture_mean'] # mean variables not efficient for detection\ncol_to_drop_se=['fractal_dimension_se', 'symmetry_se', 'smoothness_se', 'texture_se'] # Standard error variables not efficient for detection\ncol_to_drop_worst=['fractal_dimension_worst'] # Worst variable not efficient for detection\ncol_to_drop_tot=col_to_drop_mean+col_to_drop_se+col_to_drop_worst\ndata.drop(col_to_drop_tot,axis=1,inplace=True)\nprint('Now the data set is only composed of ', len(data.columns), 'features')","8b0002d4":"color_function = {0: \"blue\", 1: \"red\"} # Here Red color will be 1 which means M and blue foo 0 means B\ncolors = data[\"diagnosis\"].map(lambda x: color_function.get(x))# mapping the color fuction with diagnosis column\npd.plotting.scatter_matrix(data.iloc[:,2:], c=colors, alpha = 0.5, figsize = (15, 15)); # plotting scatter plot matrix","2cbd4f54":"print('The most important final features for breast cancer detection are : ')\nlist(data.columns.values[2:])","ccee3939":"######## Classification model fonction that apply each above algortihm to the data and compute the scores on each fold of the cross validation   ###########\n\nfrom sklearn.preprocessing import StandardScaler # To norm the data\n\ntrain, test = train_test_split(data, test_size = 0.3)# main data is splitted into train and test set\n\n# As we are going to use many models lets make a function which we can use with different models\ndef classification_model(model,data,prediction_input,output):\n   \n    \"\"\" \n    paramters:\n        model : name of the model tested (Decision Tree, SVM, KNN, RandomForest), sklearn object.\n        data : Main dataframe of all data, pandas dataframe.\n        prediction_input : name of the prediction variables, in our case it is the 9 features described above, list(str).\n        output : name of target variable, in our case it is the diagnosis feature, str.\n        \n    return:\n        None \n        (print accuracy and cross validation score of each models)\n    \"\"\"\n    \n\n    #Fit the model:\n    train, test = train_test_split(data, test_size = 0.3) # in this our main data is splitted into train (70%) and test (30%) into the function (local variables)\n    train_X = train[prediction_input] # taking the training data input \n    train_y=train.diagnosis # This is output of our training data\n    \n    # same for data test\n    test_X= test[prediction_input] # taking test data inputs\n    test_y =test.diagnosis   #output value of test data\n    \n    # norm the data with mean of 0 and standard deviation of 1\n    sc = StandardScaler() # sklearn object\n    sc.fit_transform(data[prediction_input]) \n    train_X = sc.transform(train_X) #transform train set with the scaler method\n    test_X = sc.transform(test_X) # transform test set with scaler method\n    \n    model.fit(train_X,train_y) #Here we fit the model using training set\n  \n    #Make predictions on test set:\n    predictions = model.predict(test_X)\n  \n    # Check accuracy \n    accuracy = metrics.accuracy_score(predictions,test_y)\n    print(\"Accuracy on test set : %s\" % \"{0:.3%}\".format(accuracy))\n \n    # Cross validation on 5 random folds \n    kf = KFold(n_splits=5)\n    \n    error = []\n    print(kf)\n    print('  ')\n    \n    # For each folds, we fit the model and compute the accuracy, to prevent overfitting\n    for train, test in kf.split(data):\n        # as the data is divided into train and test using KFold\n        # so here also we are going to fit model\n        #in the cross validation the data in train and test will change for evry iteration\n        train_X = (data[prediction_input].iloc[train,:])# this iloc method is used for selecting trainig data\n        train_y = data[output].iloc[train]# here is only column so it repersenting only row in train\n        \n        train_X = sc.transform(train_X) # Scale the train data\n        \n        # Training the algorithm using the predictors and target.\n        model.fit(train_X, train_y)\n    \n        # now do this for test data also\n        test_X=data[prediction_input].iloc[test,:]\n        test_y=data[output].iloc[test]\n        test_X = sc.transform(test_X)\n        \n        # compute the score on test set\n        error.append(model.score(test_X,test_y))\n        \n        # printing the score \n        print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))\n\n\n","875abb63":"model = DecisionTreeClassifier()\nprediction_var=data.columns[2:]\noutcome_var= \"diagnosis\"\nclassification_model(model,data,prediction_var,outcome_var)","70b2fb2b":"model = svm.SVC()\nclassification_model(model,data,prediction_var,outcome_var)","fbc43d54":"model = KNeighborsClassifier()\nclassification_model(model,data,prediction_var,outcome_var)","7048fb27":"model = RandomForestClassifier(n_estimators=100)\nclassification_model(model,data,prediction_var,outcome_var)","133c4e70":"# lets Make a function for Grid Search CV\ndef Classification_model_gridsearchCV(model,param_grid,data_X,data_y):\n    \n    \"\"\"\n    parameter :\n        model : model we want to tune. In our case it is SVM classifier.\n        paramg_grid : dictionnary of all hyperparameters of the model we want to optimize\n        data_X : train features data\n        data_y : train prediciton data\n        \n    return :\n        None \n        (print the best parameters and estimator associated with the scores)\n    \n    \"\"\"\n    clf = GridSearchCV(model,param_grid,cv=10,scoring=\"accuracy\")\n    # this is how we use grid serch CV we are giving our model : we gave parameters those we want to tune\n    # Cv is for cross validation\n    \n    # Fit all models with all hyperparameter configuration\n    clf.fit(train_X,train_y)\n    print(\"The best parameter found on development set is :\")\n    \n    # this will gie us our best parameter to use\n    print(clf.best_params_)\n    print(\"the best estimator is \")\n    print(clf.best_estimator_)\n    print(\"The best score is \")\n    \n    # this is the best score that we can achieve using these parameters\n    print(clf.best_score_*100, '%')","94dbc31a":"#now split our data into train and test\ntrain, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test\n\ntrain_X = train[prediction_var]# taking the training data input \ntrain_y=train.diagnosis# This is output of our training data\n\n# same for test set\ntest_X= test[prediction_var] # taking test data inputs\ntest_y =test.diagnosis   #output value of test data\n\n# scale data\nsc = StandardScaler()\nsc.fit_transform(data[prediction_var])\ntrain_X = sc.transform(train_X)\ntest_X = sc.transform(test_X)\n\n# Here we have to take parameters that are used for SVM Classifier\n    # C: Regularization parameter. The strength of the regularization is inversely proportional to C. \n    # kernel : Specifies the kernel type to be used in the algorithm.\n\nparam_grid = {'C': [0.65,0.75,0.8,1], \n              'kernel': [ 'linear','poly','rbf','sigmoid'],\n                }\n# here our gridasearchCV will take all combinations of these parameter and apply it to model \n# and then it will find the best parameter for model\nmodel= svm.SVC()\nClassification_model_gridsearchCV(model,param_grid,train_X,train_y) # call the function","b6c18f08":"model=svm.SVC(C=0.65, kernel='rbf')# our best classifier\nmodel.fit(train_X,train_y)# now fit our model for traiing data\nprediction=model.predict(test_X)# predict for the test data\n\n# here we use accuracy measurement between our predicted value and our test output values\nprint('------------------------')\nprint('Accuracy score : ', metrics.accuracy_score(prediction,test_y)*100, ' %') # to check the accuracy\nprint('------------------------')","b259333f":"from sklearn.metrics import precision_score, recall_score, f1_score\n\n# Precision quantifies the number of positive class predictions that actually belong to the positive class.\nprecision = precision_score(test_y, prediction, average='binary')\n\n# Recall quantifies the number of positive class predictions made out of all positive examples in the dataset.\nrecall = recall_score(test_y, prediction, average='binary')\n\n# F-Measure provides a single score that balances both the concerns of precision and recall.\nscore = f1_score(test_y, prediction, average='binary')\n\nprint('Precision : ',precision *100 ,' %')\nprint('------------------------')\nprint('Recall : ' ,recall*100 ,' %') # Best score to considerate in our case beacause Recall is appropriate when minimizing False Negatives. \nprint('------------------------')\nprint('F1_score : ', score*100 ,' %')","698f1302":"#### Plotting confusion matrix   ######\n\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(model, test_X, test_y)\nplt.title('Confusion Matrix')\nmetrics.plot_roc_curve(model, test_X, test_y) \nplt.title('ROC Curve')\nplt.show()","fab1d3a0":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(16 , activation='relu', input_dim=9))\n# Adding dropout to prevent overfitting\nclassifier.add(Dropout(rate=0.1))\n\n# Adding the second hidden layer\nclassifier.add(Dense(16, activation='relu'))\n# Adding dropout to prevent overfitting\nclassifier.add(Dropout(rate=0.1))\n\n# Adding the output layer\nclassifier.add(Dense(1, activation='sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n#Optimizer is chosen as adam for gradient descent and Binary_crossentropy is the loss function used.\n\n# Fitting the ANN to the Training set\nclassifier.fit(train_X, train_y, batch_size=75, epochs=120)\n# Long scroll ahead but worth\n# The batch size and number of epochs have been set using trial and error. Still looking for more efficient ways. Open to suggestions.\n\ny_pred = classifier.predict(test_X)\ny_pred = (y_pred > 0.5)","c0d8640a":"# Precision quantifies the number of positive class predictions that actually belong to the positive class.\nprecision = precision_score(test_y, y_pred, average='binary')\n\n# Recall quantifies the number of positive class predictions made out of all positive examples in the dataset.\nrecall = recall_score(test_y, y_pred, average='binary')\n\n# F-Measure provides a single score that balances both the concerns of precision and recall.\nscore = f1_score(test_y, y_pred, average='binary')\n\nprint('Precision : ',precision *100 ,' %')\nprint('------------------------')\nprint('Recall : ' ,recall*100 ,' %') # Best score to considerate in our case beacause Recall is appropriate when minimizing False Negatives. \nprint('------------------------')\nprint('F1_score : ', score*100 ,' %')","9d0115ad":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(test_y, y_pred)\nsns.heatmap(cm,annot=True)\n","ea5db96a":"Most of the remaining features are now poorly correlated with each other, except perhaps for a few exceptions in the \"texture\" variables. \n\nThe general idea is to get good results while facilitating the analysis.","82fecd55":"* ###  RandomForest Classifier ","f7e4b267":"*  #### The current techniques in terms of AI enable to obtain satisfactory results at first sight (> 95% of good prediction). \n\n* #### However, it is almost impossible to obtain 100% good predictions, so the real issue is how to deal with cases where the algorithm predicts that a patient is healthy when in fact he has a cancerous tumor?\n\n* #### Fully automating the breast cancer detection process is still impossible with the techniques mentioned above, although we are getting closer and closer to 100% accuracy. (We will probably never reach it...)\n","7e4827c6":"![cancer4.gif](attachment:f896f534-8cdb-46da-a873-bedd8d10762f.gif)\n\n            Example of fine needle aspiration samples from a breast mass\n            \n            ","72436a9e":"* ### Univariate analysis on each features to discover those which allow to discretize the variable 'diagnosis'.","69321eed":"In concrete terms, the main characteristics of a cell that allow us to predict whether or not it is cancerous are : \n\n* the **perimeter**, \n* the **symmetry**, \n* the **compactness**, \n* the **smoothness**. ","9c4ce358":"### ---> However, even with results never reaching 100%, a solution exists : combining Human + AI. This is the key to success!","75ae1583":"The objective of this study is to automatically detect if a breast tumor is malignant or benign. \n\nA tumor is said to be malignant if the patient has a cluster of cancerous cells whereas a benign tumor is not a cancer and is therefore much easier to treat.\n\nThanks to a Fine Needle Aspiration (FNA) which is a surgical diagnostic method that involves taking cells and tissue from nodules of the breast tumor, it is possible to analyze tumor samples like these under the microscope :","88f8c49b":"* ### Delete features based on the above analysis","2bc1e496":"* ### Conclusion on features selection :","e221341e":"* ### Try on differents models :","68312072":"Many specialists such as medical doctors and computer engineers (*Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian*) have developed a whole process which, from digitized images of hundreds of tumors like those shown above, makes it possible to determine a whole set of characteristics describing each of the cells of the tumor.  \n\nThese methods are very well explained in [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\"] available [here](https:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.23.3307&rep=rep1&type=pdf).\n\nWith the work of all these researchers, it is possible to determine automatically ten real-valued features for each cell nucleus in the 3-dimensional space. \n\nWe thus obtain 3 groups : Mean, Standard Error and Worst values of 10 characteristics each associated with the doctors' diagnosis (Malignant or Benign).\n\nThese 10 features represent geometric characteristics of the cells, they include for example:\n\n* 1) radius (mean of distances from center to points on the perimeter)\n* 2) texture (standard deviation of gray-scale values)\n* 3) perimeter\n* 4) area\n* 5) smoothness (local variation in radius lengths)\n* 6) compactness (perimeter^2 \/ area - 1.0)\n* 7) concavity (severity of concave portions of the contour)\n* 8) concave points (number of concave portions of the contour)\n* 9) symmetry\n* 10) fractal dimension (\"coastline approximation\" - 1)","9660c631":"* ###  Decision Tree Classifier ","a680fa54":"# 5. Machine Learning Conclusion","210a4758":"### The best model seems to be the SVM classifier. So let's optimize it with a Grid Search CV to tune hyperparameter.","28dd9bc8":"# 4. Machine Learning Algorithm","bd29e88c":"* ### Study of the correlations between the 10 variables of the 3 groups. \n* ### The darker a box is, the more the 2 associated features are correlated.","41f204b9":"* ### The best Machine Learning model tested is : SVM (C = 0.65, kernel = 'rbf')\n* ### Let's compute some metrics with this model : (precision, recall, F1-score), confusion matrix and ROC curve","1eeaf32a":"* ## Main goal of this part : Apply Machine Learning Algorithms on the above data in order to detect cancerous cells.\n\n    * ### I start trying 4 algorithms : DecisionTree, SVM, K-Nearest Neighbors (KNN), RandomForestClassifier.\n\n    * ### Then, I compute the scores with Cross Validation and I select the one that maximizes accuracies","c4d37243":"* ### Summary Deep Learning\n    * #### The Recall Score seems to be better than with ML methods : 96.875 % \n    * #### But always 3 cancerous tumor are not detected...\n    ","86990cdd":"We can see that some variables allow more easily than others to discriminate between cancerous and healthy cells. \n\nFor example the variables: \"compactness_mean\", \"compactness_worst\" or \"perimeter mean\" have a big influence on the final prediction compared to other variables such as \"fractal_dimension_mean\", \"symmetry mean\", \"smoothness mean\" or \"texture_mean\".\n\nWe will therefore remove these features because they do not allow us to determine if a tumor is cancerous or not.\n\nThis choice is obviously questionable but the objective is to simplify the input data as much as possible to have a better understanding of the underlying algorithms.\n","3d462ca4":"# 7. Conclusion ","a23a4fc9":"* ### Summary Machine Learning\n    * #### Good accuracy with only 9 features : 98.24% (We make the right prediction in 98.24% of the cases)\n    \n    * #### Good Recall : 96.77 % but it is not 100% so we do not detect all cancerous tumor ! It is very problematic !\n    * #### So, the main problem is that a cancerous tumor is not detected in all cases. ( X patients are not detected as having a cancerous tumor : Value at the bottom left in the confusion matrix).\n    * #### Let's see with Deep Learing methods if we can decrease the number of cancerous cells non-detected","581e916e":"# Automated Brest Cancer Prediciton with Machine Learning & Deep Learning \n# --> Accuracy = 98,24 %\n##### Pierre-Louis Danieau","b0cbddaf":"* ###  SVM","b402c64e":"The dark blue boxes are those associated with two highly correlated features. \n\nIt is therefore possible to delete some of them which do not bring any added value to our data set.\n\n* The features 'radius', 'perimeter' and 'area' are strongly correlated, which is not surprising given the geometrical relationship by which they are linked. I decide to keep only the 'perimeter' variable.\n\n* Also, the variables 'compactness', 'concavity' and 'concavepoint' are strongly correlated so we will only use 'compactness_mean', 'compactness_se' and 'compactness_worst' in our case. ","3e5bc97b":"# 1. Introduction","18d77746":"* ### Univariate analysis conclusion :","e7242278":"### The best score to considerate is the **Recall** because in cancer detetcion we want to minimize False Negatives (e.g : prediction of a non-cancerous tumor when it is) ","0afe93b9":"# 3. Data Analysis","b0d65bd9":"* ### Study of the correlation on the remaining features (to verify the quality of previous work)","866f6e7c":"# 2. Loading packages + Data Reading","1029d748":"* ###  KNN","acbe8286":"* ###  Correlation conclusion : ","3b93b7a7":"# 6. Simple Deep Learning Algorithm (DNN)","88397027":"## *My work* :  From these 30 characteristics realize an algorithm that predicts if a tumor is cancerous or not."}}