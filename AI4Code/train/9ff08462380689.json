{"cell_type":{"2d74293b":"code","f0088f0d":"code","0d9477f7":"code","30a9f9d6":"code","67454d07":"code","a30f11e5":"code","607b80d1":"code","87d9f1c0":"code","51365337":"code","8b6514b5":"code","987b141a":"code","08ad9e21":"code","402902a0":"code","a4de04fb":"code","ea3416fa":"code","175ffff9":"code","452aa249":"code","686282ce":"code","0400a883":"code","941c8d39":"code","034b7f68":"code","16e44b6d":"code","23a288cd":"code","543389e3":"code","86df508a":"code","c3d0accc":"code","1f47616a":"code","f3df3a4d":"code","98e2fd6a":"code","105f5a08":"code","63cd8c22":"code","be82f9dd":"code","40208604":"markdown","cc5162d8":"markdown","0be0b3ac":"markdown","0ecd8faa":"markdown","da1f60bc":"markdown","7192c219":"markdown","cf5fb22b":"markdown","8e6d4f36":"markdown","842e89ab":"markdown","8303a4c3":"markdown","e70cbfa6":"markdown","49c5278c":"markdown","350851a1":"markdown","52109b67":"markdown","117f9773":"markdown","026909ed":"markdown","8800446f":"markdown","359a7e4d":"markdown","897755b1":"markdown","898ef8d4":"markdown","48d805e4":"markdown","5ca153c1":"markdown","e79403eb":"markdown","9ad24d2c":"markdown","66e33027":"markdown"},"source":{"2d74293b":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\nimport warnings\n\nsns.set()\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","f0088f0d":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\ndf_train = df_train.drop('Id', axis=1)\ndf_test = df_test.drop('Id', axis=1)","0d9477f7":"df_train.head(-10)","30a9f9d6":"df_train.columns","67454d07":"df_train['SalePrice'].describe()","a30f11e5":"sns.distplot(df_train['SalePrice'], fit=stats.norm)\nplt.show()","607b80d1":"stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","87d9f1c0":"print('SalePrice Skewness: ', df_train['SalePrice'].skew())\nprint('SalePrice Kurtosis: ', df_train['SalePrice'].kurt())","51365337":"corrmat = df_train.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(corrmat)\nplt.show()","8b6514b5":"cols = corrmat.nlargest(11, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nplt.figure(figsize=(15, 10))\nsns.heatmap(cm, annot=True, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","987b141a":"sns.pairplot(df_train[cols])\nplt.show()","08ad9e21":"def GetRelationWithSalePrice(X, show, x_figsize, y_figsize):\n    plt.figure(figsize=(x_figsize, y_figsize))\n    plt.ylim(0,800000)\n    plt.title(\"Relation Between \" + X + \" and SalePrice\")\n    plt.ylabel(\"SalePrice\"), plt.xlabel(X)\n    eval(show)(x=df_train[X], y=df_train['SalePrice'])\n    plt.show()","402902a0":"GetRelationWithSalePrice('GrLivArea', 'plt.scatter', 7, 5)","a4de04fb":"df_train = df_train.drop(df_train['GrLivArea'].argmax())\nGetRelationWithSalePrice('GrLivArea', 'plt.scatter', 7, 5)","ea3416fa":"GetRelationWithSalePrice('OverallQual', 'sns.boxplot', 10, 5)","175ffff9":"GetRelationWithSalePrice('YearBuilt', 'sns.boxplot', 20, 10)","452aa249":"train_total = df_train.isnull().sum().sort_values(ascending=False)\ntrain_percentage = (df_train.isnull().sum() \/ df_train.isnull().count()).sort_values(ascending=False)\ntrain_missing_data = pd.concat([train_total, train_percentage], axis=1, keys=['Total', 'Percentage'])\ntrain_missing_data.head(20)","686282ce":"print(df_train.shape)","0400a883":"df_train = df_train.drop((train_missing_data[train_missing_data['Total'] > 1]).index,1)\ndf_test = df_test.drop((train_missing_data[train_missing_data['Total'] > 1]).index,1)\n\ndf_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\n\nprint(\"Train Shape:\", df_train.shape)\nprint(\"Test Shape:\", df_test.shape)","941c8d39":"X_train = df_train.drop('SalePrice', axis=1)\ny = df_train['SalePrice']","034b7f68":"test_total = df_test.isnull().sum().sort_values(ascending=False)\ntest_percentage = (df_test.isnull().sum() \/ df_test.isnull().count()).sort_values(ascending=False)\ntest_missing_data = pd.concat([test_total, test_percentage], axis=1, keys=['Total', 'Percentage'])\ntest_missing_data.head(20)","16e44b6d":"Imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nX_test = pd.DataFrame(Imputer.fit_transform(df_test), columns=df_test.columns)","23a288cd":"train_len = len(X_train)\n\ndf = pd.concat([X_train, X_test], ignore_index=True)\n\nEncoder = LabelEncoder()\ncategorical = df.select_dtypes(include=['object'])\n\nfor column in categorical:\n    df[column] = Encoder.fit_transform(df[column].astype('str'))\n\nX_train = df[:train_len]\nX_test = df[train_len:]\n\nprint(\"DF Shape:\", df.shape)\nprint(\"Train Shape:\", X_train.shape)\nprint(\"Test Shape:\", X_test.shape)","543389e3":"df.head(-10)","86df508a":"X_train.head(-10)","c3d0accc":"X_test.head(-10)","1f47616a":"GBR = GradientBoostingRegressor(loss='huber', n_estimators=2000, learning_rate=.05)\n\ncv_score = cross_val_score(GBR, X_train, y, cv=10, n_jobs=-1)\n\nprint(f\"GBR Model Max Test Score: {cv_score.max().round(4)*100}%\")","f3df3a4d":"GBR.fit(X_train, y)\n\nprint(f\"GBR Model Train Score: {GBR.score(X_train, y).round(4)*100}%\")","98e2fd6a":"importance = GBR.feature_importances_\nindices = importance.argsort()[-10:]\nplt.figure(figsize=(15,7))\nplt.plot(X_train.columns[indices], importance[indices], 'o-', color=\"#2ecc71\")\nplt.show()","105f5a08":"y_pred = GBR.predict(X_test)\n\nprint(\"y_pred shape:\", y_pred.shape)\nprint(\"submission shape:\", submission.shape)\n\nsubmission.head(-10)","63cd8c22":"submission['SalePrice'] = y_pred\nsubmission.head(-10)","be82f9dd":"submission.to_csv('submission.csv', index=False)","40208604":"**4.a- Say Hi to GrLivArea!**","cc5162d8":"**Finally X_train and y are Ready**\n\nWe will update **\"y\"** after dropping rows and make the **\"Log\"** transformation.\n\n> Notice to drop **\"SalPrice\"** from **X_train**.\n","0be0b3ac":"These are our **columns** and we don't know much about them, but before that, but first We will meet our **output** or **\"y\"**.\n\n# 2- Say Hi to SalePrice!","0ecd8faa":"# 7- Let's Party\n\nAfter tring many **Algorithms: [LinearRegression, Laso, SVR ,DecisionTreeRegressor, RandomForestRegressor, KNeighborsRegressor]**, I found **GradientBoostingRegressor** is the best algorithm for this data.\n\nWe will check **test score** by **cross_val_score** then **fit** the model. ","da1f60bc":"Now We will deal with **Missing Data** in **Test Data**.","7192c219":"# 6- Let's LabelEncoder","cf5fb22b":"# 10- Output","8e6d4f36":"Ooh **SalePrice**, Nice hairstyle by the way.\nCan I scan your invitation, please?","842e89ab":"# # 5- Missing Data\nWe will know more about our data and its **null values**.","8303a4c3":"Hello again my friend, I wish your coffee is delicious\n\n# Let's Start, \n\nFirst we import our helpers and I want to know the order of them is the same order of using them.\n\n","e70cbfa6":"# 1- Data\nOur data has 2 files train and test so we will import them.\n\n1. **Train File:** We will deal with it in this notebook.\n2. **Test File:** This file we will predict its **\"y\"** after training our model.\n\n> Notice to drop **\"Id\"** column from **2 DataFrames** because I think this feature is **useless**. ","49c5278c":"**4.c- Say Hi to YearBuilt!**","350851a1":"Ooh, **SalePrice** has lots of friends. In my opinion, he wants to talk to his **close friends** only.\n\nI suggest entering a private room, Unfortunately, the room fits **11** people so **SalePrice** will enter the room with **10 friends only**.","52109b67":"The friends have a lot of talk with each other.","117f9773":"# 8- Feature Importane\nThese are the 10 most importan features.","026909ed":"# 9- Let's Predict","8800446f":"Hello Future Engineers, Nice to meet you!\n\nIn this notebook, we will learn together how to deal with data and apply Regression algorithms. So... make your coffee and come back to start coding, I promise I will wait for you.","359a7e4d":"**4.b- Say Hi to OverallQual!**","897755b1":"Let't apply **SimpleImputer** on data with **strategy='most_frequent'**.\n\n> You should give **X_test DataFrame columns names** because **SimpleImputer** class takes **pandas dataframes** and returns **numpy array** so we **Retrieve columns names**.**","898ef8d4":"# 4- Get Relation With SalePrice\n\nOur friend is very shy so We will facilitate communication with his friends.\n\nAfter checking relations between friends, We will focus on 3 of them and their relation with **SalePrice**\n\n**GetRelationWithSalePrice** is a very simple function, it takes 4 parameters:\n1. **X:** **Column** or **Feature** or \"The friend of our friend\"\n2. **show:** Type of relation\n3. **x_figsize:** Width of figure\n3. **y_figsize:** Height of figure\n\nFrom the frist great **\"SalePrice.describe()\"**, we knew the **max** of **\"SalePrice\"** is **\"755000\"** so we will make **\"ylim\"** = (0,**800000**)","48d805e4":"Everything is OK, Let's check the **categorical data**.","5ca153c1":"We will remove **null values columns** from **train** and **test** data except **\"Electrical\"** because it has 1 row only with **NaN**","e79403eb":"OK Sir, Have a nice time.","9ad24d2c":"# 3- SalePrice Friends","66e33027":"I can see an **anomaly point** If we keep it, it will take us to mislead so let's drop this row.\nwe can find, it's the **max** value of the **GrLivArea** column."}}