{"cell_type":{"3d594a78":"code","a2c0516a":"code","9296fa68":"code","60957ef8":"code","4c55c010":"code","69d72087":"code","78cfd72c":"code","4fbaecd0":"code","a2cfc3ef":"code","17c44cc7":"code","96f6c9e7":"code","9b857f93":"code","050080d7":"code","1befc687":"code","1bffb426":"code","3b7f79ef":"code","72de80b6":"code","0939b251":"code","6aa843a2":"code","7eaab163":"code","d3fbb624":"code","ca7c2b11":"code","7599b34c":"code","9feac643":"code","2b08d876":"code","9a3972ca":"code","0d0a92e3":"code","b0f4661f":"code","d75def8e":"code","7917857f":"code","8ece2233":"code","a6bf2140":"code","6f745b4e":"code","dc7c0f05":"code","e682d725":"code","9a73b932":"code","253f2731":"code","6e40fd6b":"code","1ef38169":"code","9f2f2bda":"code","f0eae86a":"code","c4510cda":"code","d5b6a1d0":"code","7568876a":"code","68066eea":"code","aa770a7c":"code","ab18873c":"code","e6536a31":"code","7f2c0f2c":"code","6197e2e1":"code","2ff329f6":"code","921db376":"code","7d636f52":"code","7cf63085":"code","5083286c":"code","f4bef4fa":"code","4332cca3":"code","93691ff3":"code","ae9e7b04":"code","9fc35475":"code","2f6127e0":"code","41263f33":"code","b2830669":"code","2d65bd75":"code","46efc389":"code","e5725b78":"code","bd4b88a0":"code","39b944c9":"code","1cf7495e":"code","df857286":"code","933cde15":"code","e9ebdf54":"code","5a0228ff":"code","d1e06939":"code","ee2dc434":"code","081ecb6a":"code","6bbe265b":"code","362f71be":"code","03563ed9":"code","bbc9e601":"code","ebbd144e":"code","ad4898b3":"code","4515930b":"code","b2b808ce":"code","cdeb8549":"code","b26a7bec":"code","bc50c54b":"code","0b623a1b":"code","963b3b94":"code","feba9565":"code","7f79cc8d":"code","0f8562ce":"code","0b9da892":"code","80fcc66e":"code","d813b9f7":"code","5b7bfa8b":"code","cd602d4d":"code","0e90ab03":"code","569a6582":"code","f7c8cc70":"code","0bad83b0":"code","2cea4c50":"code","533501ee":"code","bacdaca6":"code","c833bfd0":"code","f1cca3ee":"code","f9af4e28":"code","aa2b6cd4":"code","6a53e3a9":"code","a9e40658":"code","c3aa0a53":"code","ee286b4a":"code","ea2c6e67":"code","3ad6a1c1":"code","9c00df28":"code","a82cf8ba":"code","04fde9fa":"code","ec69645f":"code","52adbb75":"code","c56df3a4":"code","a7b9f398":"code","093ff39c":"code","22b72ac6":"code","3971d837":"code","47876bb3":"code","686cde33":"code","2fa1afc3":"code","f170c6b6":"code","25a0a1a6":"code","27e7e806":"code","9e575146":"code","9071f893":"code","0a480c90":"code","4aee5607":"code","03a193a5":"code","b7e9cfb8":"code","b926cae6":"code","7e746f91":"code","c0c73372":"markdown","a70e2939":"markdown","9077795b":"markdown","63722888":"markdown","9a631c26":"markdown","3d278e5f":"markdown","17565bda":"markdown","b72b0772":"markdown","819d0300":"markdown","0d2a664f":"markdown","405eae5d":"markdown","d2252d25":"markdown","946ce6b3":"markdown","d032dedf":"markdown","e387c994":"markdown","cfc16d97":"markdown","518b2159":"markdown"},"source":{"3d594a78":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2c0516a":"test=\"test\"\nsample=0.05\ntest=\"final\"","9296fa68":"import csv\nimport requests\nimport gc","60957ef8":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl","4c55c010":"jigsaw_df = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\n#jigsaw_df=jigsaw_df.sample(300)\nif test==\"test\":\n    jigsaw_df=jigsaw_df.sample(frac=sample)","69d72087":"jigsaw_df=jigsaw_df.rename(columns={\"comment_text\": \"text\"})","78cfd72c":"jigsaw_df.head(2)","4fbaecd0":"jigsaw_df.columns","a2cfc3ef":"col_list=[ 'toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']","17c44cc7":"for col in col_list:\n    jigsaw_df.astype({col: 'int32'})","96f6c9e7":"jigsaw_df.describe()","9b857f93":"jigsaw_df['toxicity_sum'] = jigsaw_df[col_list].sum(axis=1)","050080d7":"sns.set_theme(style=\"darkgrid\")\nax = sns.countplot(x=\"toxicity_sum\", data=jigsaw_df)","1befc687":"jigsaw_df['proposed_score']=5*jigsaw_df['toxic']+25*jigsaw_df['severe_toxic']+5*jigsaw_df['obscene']+30*jigsaw_df['threat']+10*jigsaw_df['insult']+20*jigsaw_df['identity_hate']","1bffb426":"jigsaw_df['proposed_score2']=jigsaw_df['toxic']+2*jigsaw_df['severe_toxic']+jigsaw_df['obscene']+jigsaw_df['threat']+jigsaw_df['insult']+jigsaw_df['identity_hate']","3b7f79ef":"jigsaw_df['proposed_score3']=0.32*jigsaw_df['toxic']+1.5*jigsaw_df['severe_toxic']+0.16*jigsaw_df['obscene']+1.5*jigsaw_df['threat']+0.64*jigsaw_df['insult']+1.5*jigsaw_df['identity_hate']","72de80b6":"sns.set_theme(style=\"darkgrid\")\nax = sns.countplot(x='proposed_score', data=jigsaw_df)","0939b251":"CSV_URL = 'https:\/\/raw.githubusercontent.com\/t-davidson\/hate-speech-and-offensive-language\/master\/data\/labeled_data.csv'\n\nwith requests.Session() as s:\n    download = s.get(CSV_URL)\n\n    decoded_content = download.content.decode('utf-8')\n\n    cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n    my_list = list(cr)","6aa843a2":"hateoffensive_df=pd.DataFrame(my_list)\n","7eaab163":"hateoffensive_df.columns","d3fbb624":"col_list=[ 'count', 'hate_speech', 'offensive_language', 'neither']","ca7c2b11":"hateoffensive_df.columns = hateoffensive_df.iloc[0]\nhateoffensive_df=hateoffensive_df.drop(0)\nhateoffensive_df.head()","7599b34c":"if test==\"test\":\n    hateoffensive_df=hateoffensive_df.sample(frac=sample)","9feac643":"for col in col_list:\n    hateoffensive_df.astype({col: 'int32'})","2b08d876":"#hateoffensive_df=hateoffensive_df.sample(300)","9a3972ca":"def scores (row):\n    if row=='0':\n        \n        return 2\n    if row=='2':\n        return 0\n    else:\n        return 1\ndef classes (row):\n    if row=='0': \n        return 'hate_speech'\n    if row=='2':\n        return 'neutral'\n    else:\n        return 'offensive'   \n","0d0a92e3":"hateoffensive_df['numeric_class']=hateoffensive_df['class'].apply(lambda x: scores(x))\nhateoffensive_df['classes']=hateoffensive_df['class'].apply(lambda x: classes(x))\nhateoffensive_df=hateoffensive_df.rename(columns={\"tweet\": \"text\"})","b0f4661f":"hateoffensive_df.head(3)","d75def8e":"ax=sns.countplot(x=\"classes\", data=hateoffensive_df)\n","7917857f":"fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n\nfig.suptitle('Automated Hate Speech Detection and the Problem of Offensive Language \\n Inter-Annotator Agreement')\nsns.countplot(ax=axes[0, 0],x=\"hate_speech\", data=hateoffensive_df)\nsns.countplot(ax=axes[0, 1],x=\"offensive_language\", data=hateoffensive_df)\nsns.countplot(ax=axes[1, 0],x=\"neither\", data=hateoffensive_df)\nsns.countplot(ax=axes[1, 1],x=\"class\", data=hateoffensive_df)\n             \n  ","8ece2233":"hateoffensive_df_temp = hateoffensive_df[hateoffensive_df.classes != 'neutral']","a6bf2140":"len(hateoffensive_df_temp)\n","6f745b4e":"f, ax = plt.subplots(figsize=(7, 5))\nsns.despine(f)\n\nsns.histplot(\n    hateoffensive_df_temp,\n    x=\"offensive_language\", hue=\"classes\",\n    multiple=\"stack\",\n    palette=\"light:m_r\",\n    edgecolor=\".3\",\n    linewidth=.5,\n\n)\nax.xaxis.set_major_formatter(mpl.ticker.ScalarFormatter())\nax.set_xticks([1, 3, 6, 8, 9])","dc7c0f05":"hateoffensive_df_temp = hateoffensive_df[hateoffensive_df.classes == 'hate_speech']","e682d725":"hateoffensive_df_temp.head(3)","9a73b932":"f, ax = plt.subplots(figsize=(7, 5))\nsns.despine(f)\n\nsns.histplot(\n    hateoffensive_df_temp,\n    x=\"offensive_language\", hue=\"classes\",\n    multiple=\"stack\",\n    palette=\"light:m_r\",\n    edgecolor=\".3\",\n    linewidth=.5,\n\n)\nax.xaxis.set_major_formatter(mpl.ticker.ScalarFormatter())\nax.set_xticks([1, 3, 6, 8, 9])","253f2731":"del hateoffensive_df_temp","6e40fd6b":"temp_df=jigsaw_df[jigsaw_df.toxic == 0]\n#ideal values is equal with majority class but here is a computational limitation\ntemp_df=temp_df.sample(4000)\nlen(temp_df)\n#hateoffensive_df[hateoffensive_df.classes == 'hate_speech']","1ef38169":"oversample=pd.DataFrame()\noversample[\"text\"]=temp_df.text","9f2f2bda":"len(oversample)","f0eae86a":"hateoffensive_df.columns","c4510cda":"data = {'count':3, 'hate_speech':0, 'offensive_language':0, 'neither':3, 'class':2, 'numeric_class':0, 'classes':'neutral'}\noversample = oversample.assign(**data)\noversample.tail()","d5b6a1d0":"del temp_df\ngc.collect() # Python thing","7568876a":"hateoffensive_df_neutral=hateoffensive_df","68066eea":"len(oversample)","aa770a7c":"hateoffensive_df=pd.concat([hateoffensive_df, oversample], axis=0)\nhateoffensive_df=hateoffensive_df.sample(frac=1).reset_index(drop=True)","ab18873c":"len(hateoffensive_df)","e6536a31":"ax=sns.countplot(x=\"classes\", data=hateoffensive_df)\n","7f2c0f2c":"!pip install ktrain","6197e2e1":"import ktrain","2ff329f6":"categories = ['neutral','offensive', 'hate speech']","921db376":"#for computing cost reasons in kaggle\nhateoffensive_df=hateoffensive_df.sample(frac=0.6)\nlen(hateoffensive_df)","7d636f52":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(hateoffensive_df, test_size=0.2)\ntest, valid = train_test_split(test, test_size=0.05)","7cf63085":"print(len(train),len(test))","5083286c":"x_train = train.text.astype(str).tolist()\nx_valid = valid.text.astype(str).tolist()\n#y_train = train.Offensive.to_numpy()\n#y_valid = valid.Offensive.to_numpy()\ny_train = train.numeric_class.astype(int).tolist()\ny_valid = valid.numeric_class.astype(int).tolist()","f4bef4fa":"type(y_train[0])","4332cca3":"x_test = test.text.astype(str).tolist()\n#y_test = test.Offensive.to_numpy()\ny_test = test.numeric_class.astype(int).tolist()","93691ff3":"bert_model_name='bert-base-uncased'","ae9e7b04":"#class_names=['agresiv_pozitiv', 'neclasificat-nonagresiv', 'agresiv_negativ','agresiv_contoversat']\nclass_names=['neutral','offensive', 'hate_speech']","9fc35475":"from ktrain import text\nt= text.Transformer (bert_model_name, maxlen=200, classes=class_names)","2f6127e0":"trn = t.preprocess_train(x_train, y_train)\nval = t.preprocess_test(x_test, y_test)","41263f33":"model = t.get_classifier()\n#config = AutoConfig.from_pretrained(bert_model_name)\n#model = t.AutoModelForTokenClassification.from_pretrained(bert_model_name)\n#model = t.TFAutoModelForSequenceClassification()\nlearner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=32)\n","b2830669":"learner.lr_find(show_plot=True, max_epochs=1)","2d65bd75":"learner.fit_onecycle(2e-5, 4)","46efc389":"#learner.autofit(9e-5, 2)","e5725b78":"learner.plot('loss')","bd4b88a0":"learner.validate(class_names=t.get_classes())","39b944c9":"predictor = ktrain.get_predictor(learner.model, preproc=t)","1cf7495e":"predictor.batch_size = 256","df857286":"predicted_scores = predictor.predict(jigsaw_df.text.values)","933cde15":"jigsaw_df['hateoffensive_class']=predicted_scores ","e9ebdf54":"ax=sns.countplot(x=\"hateoffensive_class\", data=jigsaw_df)","5a0228ff":"def severity  (row):\n    if row=='jigsaw_df': \n        return 3\n    if row=='neutral':\n        return 1\n    else:\n        return 2  ","d1e06939":"# scoring also neutral text see above\ndef severity_filter  (row):\n    if row=='jigsaw_df': \n        return 1\n    if row=='neutral':\n        return 0\n    else:\n        return 1  \n","ee2dc434":"jigsaw_df['severity_filter']=jigsaw_df['hateoffensive_class'].apply(lambda x: severity_filter(x))","081ecb6a":"jigsaw_df['jigsaw_df_severity']=jigsaw_df['hateoffensive_class'].apply(lambda x: severity(x))","6bbe265b":"ax=sns.countplot(x=\"jigsaw_df_severity\", data=jigsaw_df)","362f71be":"jigsaw_df.to_csv('\/kaggle\/working\/jigsaw_train_hateoffensive.csv')","03563ed9":"#hateoffensive_df.head(100)","bbc9e601":"hateoffensive_df=hateoffensive_df.dropna()","ebbd144e":"hateoffensive_df['hate_speech'] = pd.to_numeric(hateoffensive_df['hate_speech'], errors='coerce').fillna(0).astype(int)\nhateoffensive_df['count']=pd.to_numeric(hateoffensive_df['count'], errors='coerce').fillna(0).astype(int)\nhateoffensive_df['offensive_language']=pd.to_numeric(hateoffensive_df['offensive_language'], errors='coerce').fillna(0).astype(int)\nhateoffensive_df['neutral_language']=pd.to_numeric(hateoffensive_df['neither'], errors='coerce').fillna(0).astype(int)\n#df['StopTime'] =  pd.to_numeric(df['StopTime'","ad4898b3":"hateoffensive_df['hate_speech_agreement']=hateoffensive_df['hate_speech']\/hateoffensive_df['count']\nhateoffensive_df['offensive_language_agreement']=hateoffensive_df['offensive_language']\/hateoffensive_df['count']\nhateoffensive_df['neutral_language_agreement']=hateoffensive_df['neutral_language']\/hateoffensive_df['count']","4515930b":"hateoffensive_df.hate_speech_agreement.value_counts()","b2b808ce":"ax = sns.countplot(x=\"hate_speech_agreement\", data=hateoffensive_df)","cdeb8549":"hateoffensive_df.offensive_language_agreement.value_counts()","b26a7bec":"ax = sns.countplot(x=\"offensive_language_agreement\", data=hateoffensive_df)","bc50c54b":"ax = sns.countplot(x=\"neutral_language_agreement\", data=hateoffensive_df)","0b623a1b":"train, test = train_test_split(hateoffensive_df, test_size=0.25)\ntest, valid = train_test_split(test, test_size=0.05)","963b3b94":"#train[['offensive_language_agreement','hate_speech_agreement']]","feba9565":"description_train = train.text.values\n\ndescription_test = test.text.values\n\n\nlabels_train = train.offensive_language_agreement.values\nlabels_test = test.offensive_language_agreement.values\n\n\n#labels_train = train[['offensive_language_agreement','hate_speech_agreement']].values\n#labels_test = train[['offensive_language_agreement','hate_speech_agreement']].values","7f79cc8d":"x_train = description_train\ny_train = labels_train\nx_test = description_test\ny_test = labels_test","0f8562ce":"del trn\ndel val","0b9da892":"gc.collect() # Python thing","80fcc66e":"text.print_text_regression_models()","d813b9f7":"trn, val, preproc = text.texts_from_array(x_train=x_train, y_train=y_train,\n                                          x_test=x_test, y_test=y_test,\n                                          ngram_range=1, \n                                          maxlen=200,\n                                          max_features=50000)","5b7bfa8b":"model = text.text_regression_model('bigru', train_data=trn, preproc=preproc)\nlearner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=32)","cd602d4d":"learner.fit_onecycle(3e-4, 4)","0e90ab03":"learner.plot('loss')","569a6582":"predictor.batch_size = 128\npredictor = ktrain.get_predictor(learner.model, preproc)","f7c8cc70":"offensive_language_agreement = predictor.predict(jigsaw_df.text.values)","0bad83b0":"jigsaw_df['offensive_agreement_rating']=offensive_language_agreement","2cea4c50":"jigsaw_df.head()","533501ee":"#ax = sns.countplot(x=\"offensive_agreement_rating\", data=jigsaw_df)","bacdaca6":"sns.distplot(jigsaw_df[\"offensive_agreement_rating\"])","c833bfd0":"labels_train = train.hate_speech_agreement.values\nlabels_test = test.hate_speech_agreement.values\ny_train = labels_train\ny_test = labels_test","f1cca3ee":"del trn\ndel val\ngc.collect() # Python thing","f9af4e28":"trn, val, preproc = text.texts_from_array(x_train=x_train, y_train=y_train,\n                                          x_test=x_test, y_test=y_test,\n                                          ngram_range=1, \n                                          maxlen=200,\n                                          max_features=50000)","aa2b6cd4":"model = text.text_regression_model('bigru', train_data=trn, preproc=preproc)\nlearner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=32)","6a53e3a9":"learner.lr_find(show_plot=True, max_epochs=1)","a9e40658":"learner.fit_onecycle(2e-4, 5)","c3aa0a53":"learner.plot('loss')","ee286b4a":"predictor.batch_size = 128\npredictor = ktrain.get_predictor(learner.model, preproc)","ea2c6e67":"hate_speech_agreement = predictor.predict(jigsaw_df.text.values)","3ad6a1c1":"jigsaw_df['hate_speech_agreement']=hate_speech_agreement","9c00df28":"sns.distplot(jigsaw_df[\"hate_speech_agreement\"])","a82cf8ba":"len(hateoffensive_df_neutral)\nhateoffensive_df_neutral['count']=pd.to_numeric(hateoffensive_df_neutral['count'], errors='coerce').fillna(0).astype(int)\nhateoffensive_df_neutral['neutral_language']=pd.to_numeric(hateoffensive_df_neutral['neither'], errors='coerce').fillna(0).astype(int)\nhateoffensive_df_neutral['neutral_language_agreement']=hateoffensive_df_neutral['neutral_language']\/hateoffensive_df_neutral['count']","04fde9fa":"train, test = train_test_split(hateoffensive_df_neutral, test_size=0.25)\ntest, valid = train_test_split(test, test_size=0.05)","ec69645f":"len(train)","52adbb75":"len(test)","c56df3a4":"x_train = train.text.values\n\nx_test = test.text.values\n\nlabels_train = train.neutral_language_agreement.values\nlabels_test = test.neutral_language_agreement.values\ny_train = labels_train\ny_test = labels_test","a7b9f398":"len(labels_train)","093ff39c":"len(x_train)","22b72ac6":"del trn\ndel val\ngc.collect() # Python thing","3971d837":"trn, val, preproc = text.texts_from_array(x_train=x_train, y_train=y_train,\n                                          x_test=x_test, y_test=y_test,\n                                          ngram_range=1, \n                                          maxlen=200,\n                                          max_features=50000)","47876bb3":"model = text.text_regression_model('bigru', train_data=trn, preproc=preproc)\nlearner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=32)","686cde33":"learner.lr_find(show_plot=True, max_epochs=1)","2fa1afc3":"learner.fit_onecycle(2e-4,6)","f170c6b6":"learner.plot('loss')","25a0a1a6":"predictor.batch_size = 128\npredictor = ktrain.get_predictor(learner.model, preproc)","27e7e806":"neutral_speech_agreement = predictor.predict(jigsaw_df.text.values)","9e575146":"jigsaw_df['neutral_speech_agreement']=neutral_speech_agreement","9071f893":"sns.distplot(jigsaw_df[\"neutral_speech_agreement\"])","0a480c90":"jigsaw_df['agreement_score']=10*(1.0001-jigsaw_df['severity_filter'])**jigsaw_df['neutral_speech_agreement']+10*jigsaw_df['severity_filter']+10*(jigsaw_df['severity_filter']+0.1)*jigsaw_df[\"offensive_agreement_rating\"]+10*jigsaw_df['severity_filter']*jigsaw_df[\"hate_speech_agreement\"]","4aee5607":"#sns.distplot(jigsaw_df['agreement_score'])","03a193a5":"jigsaw_df.to_csv('\/kaggle\/working\/jigsaw_train_hate_annotationprob.csv')","b7e9cfb8":"#temp_df=jigsaw_df[jigsaw_df.toxic == 1]","b926cae6":"#temp_df.tail(200)","7e746f91":"#jigsaw_df.head(5)","c0c73372":"for neutral agreement there is a need for inverse function: \na.i total agreement means 0 = text is neutral\nno agreement raise the score","a70e2939":"# Read Jigsaw competition dataset\nDescription\nData from Toxic Comment Classification Challenge without modification\nFor using it in Jigsaw Rate Severity of Toxic Comments\nExample usage: \u2623\ufe0f Jigsaw - Super Simple Naive Bayes [LB=0.768]\nPlease, DO upvote if you use the dataset!\nby dataista0 (Juli\u00e1n Peller)\nData scientist at Toptal\nBuenos Aires, Buenos Aires, Argentina","9077795b":"Oversampled neutral class.","63722888":"An arbitrary score\nIt is a challange to get the right weights.","9a631c26":"better then nothing for neutral text","3d278e5f":"Since the Tom Davidson dataset has different addnotation we change the classes","17565bda":"## Hate speech augumentation model","b72b0772":"# Inter-Annotator Agreement","819d0300":"Search for Neutrality rating.\nfor exemple wiki commnents about valdalism are considered neutral. still... i believe there is not full agreement on this :)","0d2a664f":"# Hate speech augumentation\n\n@inproceedings{hateoffensive,\n  title = {Automated Hate Speech Detection and the Problem of Offensive Language},\n  author = {Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar}, \n  booktitle = {Proceedings of the 11th International AAAI Conference on Web and Social Media},\n  series = {ICWSM '17},\n  year = {2017},\n  location = {Montreal, Canada},\n  pages = {512-515}\n  }\n  \n  The data are stored as a CSV and as a pickled pandas dataframe (Python 2.7). Each data file contains 5 columns:\n\ncount = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n\nhate_speech = number of CF users who judged the tweet to be hate speech.\n\noffensive_language = number of CF users who judged the tweet to be offensive.\n\nneither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n\nclass = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\n\nMIT License\n\nCopyright (c) 2017 Tom Davidson\n","405eae5d":"Load orginal jigsaw dataset.","d2252d25":"# Calculate a sum of categories. \n\nApproaches proposed in the competition:\n### Create a score that measure how much toxic is a comment\ntoxicity_weights = {'obscene': 0.02, 'toxic': 0.05, 'threat': 0.27, 'insult': 0.11, 'severe_toxic': 0.28, 'identity_hate': 0.27}\n### Calculate a sum without weights:\n\u2623\ufe0f Jigsaw - Incredibly Simple Naive Bayes [0.768] by DATAISTA0 (JULI\u00c1N PELLER)\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0 ).astype(int)\n###  Create a score that measure how much toxic is a comment by adding a weight to severe_toxic\ndf['severe_toxic'] = df.severe_toxic * 2\n### Create a score that measure how much toxic is a comment: Toxic Linear Model + Pseudo Labelling [LB 0.864]\nFEATURE_WTS = {'severe_toxic': 1.5, 'identity_hate': 1.5, 'threat': 1.5, 'insult': 0.64, 'toxic': 0.32, 'obscene': 0.16}\n\n#from my experiments i have noted that the validation dataset seems biased for identity_hate\n","946ce6b3":"Question: Does the offensive_language and hate_speech sum up?","d032dedf":"count = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n\nhate_speech = number of CF users who judged the tweet to be hate speech.\n\noffensive_language = number of CF users who judged the tweet to be offensive.\n\nneither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n\nclass = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\n\n#### Feed back is highly appreciated for dataset creators:\n\nhttps:\/\/github.com\/t-davidson\/hate-speech-and-offensive-language","e387c994":"ktrain is a lightweight wrapper for the deep learning library TensorFlow Keras (and other libraries) to help build, train, and deploy neural networks and other machine learning models. Inspired by ML framework extensions like fastai and ludwig, ktrain is designed to make deep learning and AI more accessible and easier to apply for both newcomers and experienced practitioners. \nPlease cite the following paper when using ktrain:\n\n@article{maiya2020ktrain,\n    title={ktrain: A Low-Code Library for Augmented Machine Learning},\n    author={Arun S. Maiya},\n    year={2020},\n    eprint={2004.10703},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG},\n    journal={arXiv preprint arXiv:2004.10703},\n}","cfc16d97":"The dataset perform well but the challange are neutral tags.\nI propose a scoring algoritm for tagging the neutral text.","518b2159":"Not sure if it is relevant here :)"}}