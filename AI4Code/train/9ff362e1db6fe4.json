{"cell_type":{"c253fc86":"code","64fd0b41":"code","48d227a4":"code","c368e420":"code","1a27090e":"code","62d91aef":"code","68e29caf":"code","c9bba463":"code","975724df":"code","3b1dc86e":"code","00c63e09":"code","7cbcc425":"code","9bc25440":"code","c1df0375":"code","c5ad6285":"code","9495ad5c":"code","1f719c82":"code","7d7b8cad":"code","9208a1c0":"code","2a82b15e":"code","dddc45e3":"code","e7aa2e06":"code","584b5c46":"code","9effe106":"code","de1edf3a":"code","41dcf04e":"code","6b9899d4":"code","f5a38f50":"code","86459034":"code","f3498196":"code","beef5619":"code","658e96b1":"markdown","76ad6cf2":"markdown","4eb1d658":"markdown","e89f510e":"markdown","68f8c3a7":"markdown","e4cc8e61":"markdown","801c3a02":"markdown","213d703e":"markdown","dae653d2":"markdown","5f162102":"markdown","f7e8b8cb":"markdown","60b66718":"markdown","2afee5b8":"markdown","ac55bf5c":"markdown","f195e094":"markdown","d2dc4eda":"markdown","2a52c448":"markdown","647d2bb0":"markdown","ff54137a":"markdown","4aef5107":"markdown","87e92dd5":"markdown","6d3d2642":"markdown","0eb83919":"markdown","dc730feb":"markdown","ebd0109a":"markdown","3db427cc":"markdown","41de23c5":"markdown","b48cd81f":"markdown","26e1e564":"markdown","8c736f50":"markdown","286479cd":"markdown","21653c2a":"markdown","80445bac":"markdown","653087f0":"markdown"},"source":{"c253fc86":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nnp.random.seed(42)","64fd0b41":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col=0)\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv',index_col=0)\ntrain_data.head()","48d227a4":"X = train_data.drop(columns=['Survived'])\ny = train_data.Survived","c368e420":"print(X.info())\nprint(test_data.info())","1a27090e":"age_groups = train_data.Age \/\/ 5 + 1\nage_groups.fillna(0, inplace=True)\n\nnum_people = train_data.shape[0]\n\ndeceased_age_group = age_groups[train_data.Survived==0].value_counts()\nsurvived_age_group = age_groups[train_data.Survived==1].value_counts()\nwidth = 0.8\n\n\ninds = np.sort(deceased_age_group.index.values)\nplt.figure(figsize=(10, 8))\n\nage_groups_deceased_bar = plt.bar(inds, deceased_age_group[inds].values \/ num_people, width)\nage_groups_survived_bar = plt.bar(inds, survived_age_group[inds].values \/ num_people, width,\n                                  bottom=deceased_age_group[inds].values \/ num_people)\n\nplt.title('Percentage of Passengers by Age Group', fontdict={'fontsize':20})\n\nage_groups_ticks = ['None'] + ['{}-{}'.format(int(i * 5), int((i + 1)* 5)) for i in inds[0:]]\n\nplt.xticks(inds, age_groups_ticks)\nplt.legend((age_groups_deceased_bar[0], age_groups_survived_bar[0]), ('Deceased', 'Survived'))\n\nplt.show()","62d91aef":"train_data[train_data.Age <= 15].Survived.value_counts(normalize=True)","68e29caf":"deceased_gender = train_data.Sex[train_data.Survived==0].value_counts()\nsurvived_gender = train_data.Sex[train_data.Survived==1].value_counts()\n\n\ninds = ['male', 'female']\n\nplt.figure(figsize=(8, 6))\n\nmale_plt = plt.bar(inds, deceased_gender.values \/ num_people, width)\nfemale_plt = plt.bar(inds, survived_gender[inds].values \/ num_people,\n                     width, bottom=deceased_gender[inds].values \/ num_people)\n\nplt.title('Percentage of Passengers by Gender', fontdict={'fontsize':20})\n\nplt.xticks(inds, ('Men', 'Women'))\nplt.legend((male_plt[0], female_plt[0]), ('Deceased', 'Survived'))\n\nplt.show()","c9bba463":"print(train_data[train_data.Sex == 'male'].Survived.value_counts(normalize=True))\nprint('-'*40)\nprint(train_data[train_data.Sex == 'female'].Survived.value_counts(normalize=True))","975724df":"deceased_class = train_data.Pclass[train_data.Survived==0].value_counts()\nsurvived_class = train_data.Pclass[train_data.Survived==1].value_counts()\n\n\ninds = [1, 2, 3]\n\nplt.figure(figsize=(8, 6))\n\nmale_plt = plt.bar(inds, deceased_class[inds].values \/ num_people, width)\nfemale_plt = plt.bar(inds, survived_class[inds].values \/ num_people,\n                     width, bottom=deceased_class[inds].values \/ num_people)\n\nplt.title('Percentage of Passengers by Class', fontdict={'fontsize':20})\n\nplt.xticks(inds, ('First', 'Second', 'Third'))\nplt.legend((male_plt[0], female_plt[0]), ('Deceased', 'Survived'))\n\nplt.show()","3b1dc86e":"train_data.groupby('Pclass').Survived.value_counts(normalize=True, sort=False)","00c63e09":"ranks = X['Name'].str.extract(r'\\b(\\w+)\\.') #getting ranks using regex\nranks[0].value_counts()","7cbcc425":"    ranks[train_data.Age.isna()][0].value_counts()","9bc25440":"missing_age_ranks = ranks[train_data.Age.isna()][0].value_counts().index.values\nmissing_age_ranks","c1df0375":"age_na_fills = {}\nfor rank in missing_age_ranks:\n    age_na_fills[rank] = round(train_data[(ranks == rank).values].Age.mean())\nage_na_fills","c5ad6285":"def fill_age_na(data, age_na_fills, ranks):\n    data['Age'] = data.apply(lambda row: age_na_fills.get(ranks[row.name], 29) if np.isnan(row['Age']) else row['Age'], axis=1)","9495ad5c":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.impute import SimpleImputer\nclass AgeFillNa(BaseEstimator, TransformerMixin):\n    def __init__(self, age_na_fills=None):\n        self.age_na_fills = age_na_fills # dict {rank:mean_age}\n        \n    def fit(self, X, y=None):\n        return self\n  \n    def transform(self, X):\n        output = X.copy()\n        ranks = X['Name'].str.extract(r'\\b(\\w+)\\.')\n        \n        if self.age_na_fills is not None:\n            fill_age_na(output, self.age_na_fills, ranks[0])\n        else:\n            output.Age = output.Age.fillna(output.Age.mean())\n        \n        return output\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, columns_drop):\n        self.columns_drop = columns_drop #list of column to drop\n\n    def fit(self, X, y=None):\n        return self\n  \n    def transform(self, X):\n        return X.drop(columns=self.columns_drop)\n    \nclass MultiColumnLabelEncoder(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, columns=None):\n        self.columns = columns # list of column to encode\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        output = X.copy()\n        \n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col].astype(str))\n        else:\n            for colname, col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        \n        return output\n\ncolumns_to_drop = ['Ticket', 'Cabin', 'Name']\ncolumns_to_encode = ['Sex', 'Embarked']","1f719c82":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nprepare_pipeline = Pipeline([\n        ('age_fill_na', AgeFillNa(age_na_fills)), # processes missing age values\n        ('selector', DataFrameSelector(columns_to_drop)), # drops some columns\n        ('label_enc', MultiColumnLabelEncoder(columns=columns_to_encode)),# processes categorical columns\n        ('imputer', SimpleImputer(strategy=\"mean\")), #process missing values\n        ('std_scaler', StandardScaler()), # standardization\n    ])","7d7b8cad":"from sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nX = prepare_pipeline.fit_transform(train_data.drop(columns=['Survived']))","9208a1c0":"from sklearn.metrics import f1_score\nempt_models = [GradientBoostingClassifier(), LogisticRegression(solver='lbfgs'), SGDClassifier(), GaussianNB(), \n               KNeighborsClassifier(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), \n               SVC(gamma='auto')]\n\nfor model in empt_models:\n    results = cross_val_predict(model, X, y, cv=5)\n    print(\"model: {}, result: {:.3f}\".format(model.__class__, f1_score(y, results)))","2a82b15e":"from sklearn.model_selection import GridSearchCV","dddc45e3":"params = [{'kernel':['linear'], 'gamma':[i \/ 10 for i in range(1, 100, 5)], 'C':[i \/ 10 for i in range(5, 30, 5)]},\n          {'kernel':['rbf'], 'C':[i \/ 10 for i in range(5, 30, 5)]},\n          {'kernel':['poly'], 'degree':np.arange(2, 10), 'C':[i \/ 10 for i in range(5, 30, 5)]}]\n\ngrid_search_svc = GridSearchCV(SVC(gamma='auto'), params, cv=5, scoring='neg_mean_squared_error')\ngrid_search_svc.fit(X, y)\ngrid_search_svc.best_params_","e7aa2e06":"svc_model = SVC(gamma='auto', **grid_search_svc.best_params_)\nsvc_score = cross_val_predict(svc_model, X, y, cv=5)\nf1_score(y, svc_score)","584b5c46":"params = {'learning_rate':[i\/10 for i in range(3, 6)], 'n_estimators':np.arange(128, 132, 1),\n         'min_samples_split':np.arange(2, 3), 'min_samples_leaf':np.arange(6, 8),\n          'max_depth':np.arange(1, 3), 'max_features':np.arange(1, 8)}\n\ngrid_search_gbc = GridSearchCV(GradientBoostingClassifier(), params, cv=5, scoring='neg_mean_squared_error')\ngrid_search_gbc.fit(X, y)\ngrid_search_gbc.best_params_","9effe106":"gbc_model = GradientBoostingClassifier(**grid_search_gbc.best_params_)\ngbc_score = cross_val_predict(gbc_model, X, y, cv=5)\nf1_score(y, gbc_score)","de1edf3a":"params = {'n_neighbors':np.arange(10, 25), 'p':np.arange(1, 6)}\n\ngrid_search_knn = GridSearchCV(KNeighborsClassifier(), params, cv=5, scoring='neg_mean_squared_error')\ngrid_search_knn.fit(X, y)\ngrid_search_knn.best_params_","41dcf04e":"knn_model = KNeighborsClassifier(**grid_search_knn.best_params_)\nknn_score = cross_val_predict(knn_model, X, y, cv=5)\nf1_score(y, knn_score)","6b9899d4":"params = {'n_estimators':np.arange(50, 1000, 100), 'max_depth':np.arange(3, 8)}\n\ngrid_search_rfc = GridSearchCV(RandomForestClassifier(random_state=42), params, cv=5, scoring='neg_mean_squared_error')\ngrid_search_rfc.fit(X, y)\ngrid_search_rfc.best_params_","f5a38f50":"rfc_model = RandomForestClassifier(random_state=42,**grid_search_rfc.best_params_)\nrfc_score = cross_val_predict(rfc_model, X, y, cv=5)\nf1_score(y, rfc_score)","86459034":"X_test = prepare_pipeline.fit_transform(test_data)","f3498196":"svc_model.fit(X, y)\npredictions = svc_model.predict(X_test)","beef5619":"my_submission = pd.DataFrame({'PassengerId': test_data.index.values, 'Survived': predictions})\nmy_submission.to_csv('submission.csv', index=False)\n","658e96b1":"As we can see, more than half of children survived. We can't say the same about adults.","76ad6cf2":"# 1. Visualization","4eb1d658":"Now it's time to choose a model to work with. We will try some of scikit-learn classification models.","e89f510e":"### 3.4 RandomForestClassifier","68f8c3a7":"# 3. Model selection","e4cc8e61":"# Choosing the best model for titanic dataset.\nWe will make some visualization and data preparation. After we'll fit some models, fine tune them and choose the best one. \n### We will try the folowing  models:\n* GradientBoostingClassifier\n* LogisticRegression\n* SGDClassifier\n* GaussianNB\n* KNeighborsClassifier\n* DecisionTreeClassifier\n* RandomForestClassifier\n* SVC","801c3a02":"We will see how survival depends on some features.","213d703e":"Now we need to separate features and labeles as it is supervised learning task.","dae653d2":"Here can be a problem that for some ranks we have only one or two persons so it is impossible to calculate a mean value for them.","5f162102":"So that means that gender is quite a strong feature.","f7e8b8cb":"### 3.2 GradientBostingClassifier","60b66718":"Now we can take a quick look at the data.","2afee5b8":"And also we already created a model with the parameters we considered to be the best. So now we need only to fit model on the training set and predict target value on the test set.","ac55bf5c":"Here we can see that we have some columns with missing data.\n* in train set: Age, Cabin and Embarked\n* in test set: Age, Cabin and Ticket\n\nWe will process them later.","f195e094":"### 1.2 Gender","d2dc4eda":"### 3.1 SVC","2a52c448":"Although after tuning all the models GradientBoostringClassifier showed the best result SVC works better on test set(0.73205 vs 0.79425\n). So we will use SVC for predicting on the test set. \nWe created pipeline for data preparation and now we can just use it.","647d2bb0":"# 5. References\n* [Parameter tuning for SVC](https:\/\/medium.com\/all-things-ai\/in-depth-parameter-tuning-for-svc-758215394769)\n* [Parameter tuning for Gradient Boosting](https:\/\/medium.com\/all-things-ai\/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae)\n* [Parameter tuning for KNN](https:\/\/medium.com\/@mohtedibf\/in-depth-parameter-tuning-for-knn-4c0de485baf6)\n* [Parameter tuning for Random Forest](https:\/\/medium.com\/all-things-ai\/in-depth-parameter-tuning-for-random-forest-d67bb7e920d)\n* Hands-On Machine Learning with Scikit-Learn and TensorFlow","ff54137a":"Now we can create a pipeline to automate a whole preparation process.","4aef5107":"More than half of women survived but we have opposite situation with men.","87e92dd5":"But as we see for ranks with missing values we have other people to work with.","6d3d2642":"### 1.3 Class","0eb83919":"After we got predictions, we need to write it to .csv file.","dc730feb":"### 3.3 KNeighborsClassifier","ebd0109a":"# 4. Launching","3db427cc":"As we can see, the best models are SVC, GradientBoostingClassifier, KNeighborsClassifier. and RandomForestClassifier Now we need to fine tune them. For this task I'll use GridSearchCV.","41de23c5":"### 1.1 Age","b48cd81f":"First of all we see that we have a lot of missing values in the Age column. And as it is an important feature, we should deal with it.\nWe know the names of all people on titanic so we can get a \"rank\" of every person(Mr, Miss, Dr, Rev and etc.) and fill missing age values by means of other people ages with the same rank.","26e1e564":"First of all we need to upload data from .csv files.","8c736f50":"We got values to replace nan with. And we need a function to replace missing age values with defined dict, so we can use this function again. If we have no such a rank in our dict we return 29 as it is mean age value.","286479cd":"Firstly we will try all models without any tuning. Just to see how they work. Than we will choose some of them and fine tune selected models.","21653c2a":"Also we have two columns with categorical values - Sex and Embarked. And some columns that we can drop. And as I want to create a Pipline for data preparation, I need to realize transformers to process columns I want to drop and one for age column.","80445bac":"We see that the worse the class, the lower the percentage of surviving people.","653087f0":"# 2. Data preparation"}}