{"cell_type":{"2eabecca":"code","344a2a0d":"code","52d3c495":"code","2e5f49e8":"code","216beec4":"code","b8b0d4f9":"code","7a18cd55":"code","c63dff9a":"code","e79d6799":"code","d0de5a54":"code","494aa9aa":"code","02426b5e":"code","e6c8aaf0":"code","87b370f3":"code","a57977f0":"code","cce6eac8":"code","098c718d":"code","bda93bbe":"code","e98012de":"code","d803e4e7":"code","37fc6e42":"code","ba2511f6":"code","799f9d1d":"code","61467fe1":"code","3759788c":"code","870a0b1a":"code","abc0db63":"code","e26d0a45":"code","bd0273cb":"code","076f863b":"code","318be4a3":"code","23fe7658":"markdown","2227542f":"markdown","8663b718":"markdown","ddc9d47d":"markdown","01c3ec83":"markdown","74b6cd93":"markdown","3d567173":"markdown","a13d1235":"markdown","396dce15":"markdown","e401c3c5":"markdown","9cea2a30":"markdown","18ac71be":"markdown"},"source":{"2eabecca":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom IPython.display import display #Display DataFrame\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))","344a2a0d":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","52d3c495":"display(df_train.head())\ndisplay(df_test.head())","2e5f49e8":"df_train.info()","216beec4":"df_train.describe()","b8b0d4f9":"cabin_nan = df_train[\"Cabin\"].isna().sum()\nprint(\"Number of NaN in Cabin : \"+str(cabin_nan))\nprint(\"Percentage : \"+str(cabin_nan\/len(df_train[\"Cabin\"])*100)+\"%\")","7a18cd55":"# Training Data\ndf_train.drop(columns=\"Cabin\",inplace=True)\ndf_train.info()","c63dff9a":"# Testing Data\ndf_test.drop(columns=\"Cabin\",inplace=True)\ndf_test.info()","e79d6799":"df_train.Age.replace(np.nan, df_train.Age.median(), inplace=True)\ndf_train.info()","d0de5a54":"df_test.Age.replace(np.nan, df_test.Age.median(), inplace=True)\ndf_test.Fare.replace(np.nan, df_test.Fare.mean(), inplace=True)\ndf_test.info()","494aa9aa":"df_train.Embarked.replace(np.nan, df_train.Embarked.value_counts().idxmax(), inplace=True)\ndf_train.info()","02426b5e":"df_train.drop(columns=[\"Name\", \"Ticket\"], inplace=True)\ndf_train.head()","e6c8aaf0":"df_test.drop(columns=[\"Name\", \"Ticket\"], inplace=True)\ndf_test.head()","87b370f3":"#Pclass Training set\npclass_one_hot = pd.get_dummies(df_train['Pclass'])\ndf_train = df_train.join(pclass_one_hot)\ndf_train.head()","a57977f0":"#Pclass Test set\npclass_one_hot_t = pd.get_dummies(df_test['Pclass'])\ndf_test = df_test.join(pclass_one_hot)","cce6eac8":"#Embarked Training set\nembarked_one_hot = pd.get_dummies(df_train.Embarked)\ndf_train = df_train.join(embarked_one_hot)\ndf_train.head()","098c718d":"#Embarked Test set\nembarked_one_hot_t = pd.get_dummies(df_test.Embarked)\ndf_test = df_test.join(embarked_one_hot_t)","bda93bbe":"#Sex Training set\nsex_one_hot = pd.get_dummies(df_train.Sex)\ndf_train = df_train.join(sex_one_hot)\ndf_train.head()","e98012de":"#Sex Test set\nsex_one_hot_t = pd.get_dummies(df_test.Sex)\ndf_test = df_test.join(sex_one_hot_t)","d803e4e7":"# Removing \"Pclass\",\"Sex\",\"Embarked\" from Training set\ndf_train.drop(columns=[\"Pclass\",\"Sex\",\"Embarked\"], inplace=True)\ndf_train.head()","37fc6e42":"# Removing \"Pclass\",\"Sex\",\"Embarked\" from Testing set\ndf_test.drop(columns=[\"Pclass\",\"Sex\",\"Embarked\"], inplace=True)\ndf_test.head()","ba2511f6":"# Normalization in Training set\nx = df_train.values\nmin_max_scaler = MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\nnorm_df=pd.DataFrame(x_scaled)\ndf_train.Age = norm_df[2]\ndf_train.Fare = norm_df[5]\ndf_train.head()","799f9d1d":"# Normalization in Testing set\nx_t = df_test.values\nmin_max_scaler = MinMaxScaler()\nx_scaled_t = min_max_scaler.fit_transform(x_t)\nnorm_df_t=pd.DataFrame(x_scaled_t)\ndf_test.Age = norm_df_t[1]\ndf_test.Fare = norm_df_t[4]\ndf_test.head()","61467fe1":"y= df_train.Survived.values\nX= df_train.drop(columns=[\"Survived\",\"PassengerId\"]).values","3759788c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) ","870a0b1a":"clf = xgb.XGBClassifier(max_depth=8, learning_rate=0.2, n_estimators=100)\nclf.fit(X_train, y_train)","abc0db63":"print('Accuracy on test set: {:.2f}'.format(accuracy_score(y_test, clf.predict(X_test))))\nprint('Accuracy on Train set: {:.2f}'.format(accuracy_score(y_train, clf.predict(X_train))))","e26d0a45":"clf_rf = RandomForestClassifier(n_estimators=100, max_depth=9, random_state=0)\nclf_rf.fit(X_train, y_train)","bd0273cb":"print('Accuracy on test set: {:.2f}'.format(accuracy_score(y_test, clf_rf.predict(X_test))))\nprint('Accuracy on Train set: {:.2f}'.format(accuracy_score(y_train, clf_rf.predict(X_train))))","076f863b":"X_pre = df_test.drop(columns=\"PassengerId\").values\npredicted_lables = clf_rf.predict(X_pre)\ndf_temp = pd.DataFrame(predicted_lables)\ndf_temp.columns = [\"Survived\"]\ndf_submission = df_test.join(df_temp)\ndf_submission = df_submission[[\"PassengerId\", \"Survived\"]]\ndf_submission.head()","318be4a3":"df_submission.to_csv(\"submission.csv\", index=False)","23fe7658":"# Importing Libraries\n****\nLet's start by importing all the required libraries\n","2227542f":"### Author : Gauransh Kumar","8663b718":"As we know that Names and Ticket numbers are not very good features for clssification because these are very distinct and it is very hard to find patterns among them, this could result in an very complex and less accurate model.\n<br>Therfore here we are removing the columns **Name** and **Ticket** from the Dataset.","ddc9d47d":"# Prediction and Submission\n****\n   > As we can see that **Random forest Classifier** has the higher accuracy we are using it for final prediction.\n\nIn last we will predict the lables for the **unknown Test set** and Submit it.\nTo do so we have to follow the given steps:\n- Store the pridiction in a variable **predicted_lables**.\n- The pridiction will be in the form of a *Numpy array*, therefore we will convert it into a Dataframe with column name \"**Survived**\".\n- Then we will create a dataframe **df_submission** and slice it for two columns **\"PassengerId\"** and **\"Survived\"**\n- In last we will convert our Dataframe into a **CSV** file using **df_submission.to_csv()** method.\n\n> This **CSV** file generated has to be submitted manually.","01c3ec83":"<font color=\"#f4425f\"> <h1> Titanic: Machine Learning from Disaster Challenge<\/h1><\/font>\n****\n[Click Here For Homepage Of Challange](https:\/\/www.kaggle.com\/c\/titanic\/)\n****\n","74b6cd93":"# Prepaing and Fitting Data\n****\nIn this sectioin we will follow the steps given below to prepare the data for *Training and Testing*:<br>\n1. Separate the **Features and Lables** from the dataframe.\n2. Splitting the data into **training** and **testing** data using the **train_test_split()** method of *sklearn.model_selection*.\n3. Creating the classifiers and fitting the values. For this we are using two different classifiers : \n    - **XGB Classifier**\n    - **Random Forest Classifier**\n   <br>after that we select the classifier with higher accuracy for the final prediction.\n   \n4. After fitting the model we will test our model and check the accuracy.\n","3d567173":"# Creating and analysing Dataset\n****\nHere we will create two DataFrames \n- ***df_train***\n- ***df_test***\n\nAfter creating the DataFrames,<br>\nLets start by getting some information about it","a13d1235":"## One-Hot Encoding\n****\nIn this section we will convert the following columns into **ONE-HOT ENCODING**:\n- **Pclass**.\n- **Embarked**\n- **Sex**\n<br><br>\nAfter conversion we merge them into our Dataframe **df_train**.\n<br>In last we will drop the columns which are converted into One-Hot Encoding.","396dce15":"# Normalization\n****\nIn this section we will normalize the dataset.\n<br><br>As we can see in the dataset that **Age** and **Fare** have high values than other feature, this can *Overfit* our Model. So to come across this we need to normalize these columns.\n<br><br>\nFor normalization we will be using the **MinMaxScaler()** function from *sklearn.preprocessing*.","e401c3c5":"As we can see that **NaN** is *>=70%* of the total values in the column.\n<br>This shows that column **Cabin** is not a good feature for our model. So lets remove it. \n","9cea2a30":"# Cleaning Dataset\n****\nAs we can see in the **df_train.info()** that column **Cabin** has the lowest values and highest **NaN**.<br>\nSo first check the total **NaN** values in column **Cabin** of the dataframe.\nAlso we will find the Percentage of NaN values in the entier Column.\n\n> NOTE: We need to clean both of the dataset **Training and Testing**.","18ac71be":"As we can see in the info that column **Age** and **Embarked** in **Training Dataset** and **Fare**, **Age** and **Embarked** in **Testing Dataset** also contain some **NaN** values.<br>\nBut these **NaN** Values are very less in numbers so instead of removing it we replace it.\nFor different columns we are replacing it with different values:\n- Column **Age** with *Median*.\n- Column **Embarked** with *\"Item with maximum frequency\"*.\n- Column **Fare** with *Mean*."}}