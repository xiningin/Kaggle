{"cell_type":{"fb6663de":"code","e2630a13":"code","432e7411":"code","a60d7c9f":"code","8dcfa773":"code","b67639fe":"code","27caf5a2":"code","a1f92325":"code","db9111e8":"code","d66eca73":"code","e481b5d5":"code","27866c1d":"code","ee151ad9":"code","7d441875":"code","9350f032":"code","87cec856":"code","f9db6b93":"code","24a23342":"code","a41a577b":"code","4a528451":"code","b4ee961f":"code","469ae0b1":"code","372fcfb6":"code","a814cc5e":"code","13a8e1c1":"markdown","dbc6afcf":"markdown","801b8438":"markdown","e408d4c9":"markdown","8572ea3a":"markdown","a2c41b48":"markdown","55bc7c98":"markdown","7c807498":"markdown","b02fceac":"markdown","8c6affc6":"markdown","38243939":"markdown","131ba50d":"markdown","dd8818c5":"markdown","37080167":"markdown","8163f3bf":"markdown","9c95d21c":"markdown","e1624a29":"markdown","422eb07b":"markdown"},"source":{"fb6663de":"import datetime, re, spacy, nltk\nimport calendar\nfrom time import time\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk import ngrams\nimport string\nimport pandas as pd\nimport numpy as np\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.pipeline import Pipeline\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","e2630a13":"df_tweets_muni = pd.read_csv(\"..\/input\/tweets-municipalidad-asuncion\/tweets_municipalidad.csv\")\ndf_tweets_muni['created_at'] = pd.to_datetime(df_tweets_muni['created_at'])\ndf_tweets_muni['date'] = pd.to_datetime(df_tweets_muni['created_at'].dt.date)\ndf_tweets_muni['month'] = df_tweets_muni['date'].dt.month\ndf_tweets_muni['year'] = df_tweets_muni['date'].dt.year\ndf_tweets_muni['tweet'] = df_tweets_muni['tweet'].astype(str)\ndf_tweets_muni['year_month'] = df_tweets_muni['date'].dt.to_period('M')","432e7411":"df_tweets_muni.head(5)","a60d7c9f":"# Borrar tweets duplicados (nos quedamos con el primero)\ndf_tweets_muni = df_tweets_muni.drop_duplicates(subset=['tweet'], keep='first')","8dcfa773":"# Usuarios con mas tweets\ndf_tweets_username = df_tweets_muni.groupby('username').count().reset_index()\ndf_tweets_username.sort_values(by='tweet', ascending=False)[['username','tweet']].head(10)","b67639fe":"# Excluir cuentas de la municipalidad o de medios de comunicacion\nfiltro = ~(df_tweets_muni['username'].isin(['AsuncionMuni','pmtasuncion1','AsuDsu','ABCCardinal',\n                                           'LaUnionAM', '780AM', 'AsuDgrrd', 'Universo970py', \n                                           '1000_am', 'EssapSA', 'Ferreiromario1', 'AbastoAsu',\n                                           'ANDEOficial', 'mopcparaguay']))\n\n# Crear copia de dataframe con filtro aplicado\ndf_tweets_muni_filtro = df_tweets_muni[filtro].copy()","27caf5a2":"# Cargar stopwords en espa\u00f1ol\nstopwords_es = stopwords.words('spanish')\n\n\"\"\"\nExcluir menciones, emails, URLs y simbolos\n\"\"\"\ndef clean_tweet(tweet):\n    # Convertir a minusculas\n    tweet = tweet.lower()\n    \n    # Excluir menciones o emails\n    tweet = re.sub(r'\\w*@(\\w+\\.*\\w+\\.*\\w+)',' ', tweet)\n    \n    # Excluir simbolos\n    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n    \n    # Excluir URLs \n    tweet = re.sub(r'(?:www\\.|https?)[^\\s]+', ' ', tweet, flags=re.MULTILINE) \n    \n    # Borrar espacios\n    tweet = tweet.strip()\n    \n    # Considerar solo valores alfa numericos\n    tweet_alfa = re.compile(\"^(?![0-9]*$)[a-zA-Z0-9]+$\") \n    \n    # Eliminar stopwords y palabras con longitud <= 2\n    tokens = tweet.split()\n    text = [token for token in tokens if token not in stopwords_es and len(token)>2 and tweet_alfa.match(token)]\n    return ' '.join(text)\n\n# Aplicar filtro a tweets\ndf_tweets_muni_filtro['tweet_cleaned'] = df_tweets_muni_filtro['tweet'].apply(clean_tweet)","a1f92325":"# Tamano del ngram\nngram = 2\ntokenizer = TweetTokenizer()\n\n# Tokenizar y aplicar ngram\ndf_tweets_muni_filtro['tokenize'] = df_tweets_muni_filtro['tweet_cleaned'] \\\n                            .apply(tokenizer.tokenize)\ndf_tweets_muni_filtro['ngram'] = df_tweets_muni_filtro['tokenize'] \\\n                            .apply(lambda x: list(ngrams(x, ngram)))\n\n# Una fila por ngram\ndf_tweets_muni_exploded = df_tweets_muni_filtro \\\n                          .explode('ngram')[['date','year','ngram']]","db9111e8":"# Agrupar por cantidad de ocurrencias\ndf_tweets_muni_exploded_grouped = df_tweets_muni_exploded \\\n        .groupby(['year', 'ngram'])  \\\n        .agg({'date':'count'}) \\\n        .reset_index() \\\n        .sort_values(by=['year', 'date'], ascending=False) \\\n        .rename(columns={'date':'count'})\n\n# Top 10 por YYYY\ndf_tweets_muni_top_year = df_tweets_muni_exploded_grouped \\\n    .drop_duplicates(subset=['count','year']) \\\n    .groupby(['year']) \\\n    .head(10)","d66eca73":"import seaborn as sns\nax = sns.catplot(x=\"count\",y=\"ngram\",\n                   col=\"year\",\n                   data=df_tweets_muni_top_year, kind=\"bar\",\n                   height=5, aspect=.7);\nax.set(xlabel=\"Frequencia\", ylabel=\"Bigrama\")","e481b5d5":"# Generaci\u00f3n de un wordcloud paraguayo\ntexto_tweets = ' '.join(df_tweets_muni_filtro['tweet_cleaned'])\n\nmask = np.array(Image.open(\"..\/input\/paraguay-flag\/paraguay_flag_.png\"))\nwordcloud_py = WordCloud(background_color=\"white\", mode=\"RGBA\", max_words=1000, mask=mask).generate(texto_tweets)\n\n# Utilizaci\u00f3n de colores de la imagen \nimage_colors = ImageColorGenerator(mask)\nplt.figure(figsize=[12,8])\nplt.imshow(wordcloud_py.recolor(color_func=image_colors), interpolation=\"bilinear\")\nplt.axis(\"off\")\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ","27866c1d":"df_unique_users = df_tweets_muni_filtro.groupby(['date'])['username'].nunique()","ee151ad9":"fig = plt.figure(figsize=(15,5))\n\nsns.lineplot(x='date', y='username', data=df_unique_users.reset_index())\n\nplt.title('Usuarios Unicos por Dia')\nplt.ylabel('Usuarios')\nplt.xlabel('Fecha')","7d441875":"df_unique_users.reset_index().sort_values('username', ascending=False).head(5)","9350f032":"fig = plt.figure(figsize=(10,5))\n\ndf_top_unique_days = df_tweets_muni_exploded[df_tweets_muni_exploded['date'].isin(['2019-05-10','2019-05-11'])] \\\n        .groupby('ngram') \\\n        .count() \\\n        .reset_index() \\\n        .sort_values('date', ascending=False) \\\n        .head(10)\n        \nsns.barplot(x='date', y='ngram', data=df_top_unique_days, orient='h')\n\nplt.title('Top bigramas para 10-11\/05\/2019')\nplt.xlabel('Menciones')\nplt.ylabel('Bigrama')","87cec856":"# Filtrar tweets por categorias de acuerdo a palabras claves\ndef get_df_from_criteria(df, criteria):\n    df = df[df['tweet_cleaned'] \\\n            .str \\\n            .contains(criteria, flags=re.IGNORECASE, regex=True)] \\\n            .groupby(['date','year','month'], as_index=False) \\\n            .agg(['count'])['created_at'].reset_index().rename(columns={'count':'tweets'})\n    return df.copy()\n\nfiltro_baches = r'\\bbache|\\bvache|\\bcrater'\nbaches_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_baches)\n\nfiltro_basura = r'\\bbasura|\\brecicla|\\bdesecho|\\btoxico|\\bvertedero|\\bescombro|\\bsucio|\\basco'\nbasura_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_basura)\n \nfiltro_inundado = r'\\binunda|\\blluvia|\\bllueve|\\braudal|\\bdesagu*'    \ninundado_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_inundado)\n\nfiltro_dengue = r'\\bdengue|\\bmosquito|\\baedes|\\bcriadero|\\bminga|\\bfumiga'\ndengue_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_dengue)   ","f9db6b93":"fig = plt.figure(figsize=(15,5))\nax = fig.add_subplot(111)\n\nsns.lineplot(x='date', y='tweets', data=baches_x_dia_df[baches_x_dia_df['year'].isin([2017,2018,2019])], ax=ax)\nsns.lineplot(x='date', y='tweets', data=basura_x_dia_df[basura_x_dia_df['year'].isin([2017,2018,2019])], ax=ax)\nsns.lineplot(x='date', y='tweets', data=inundado_x_dia_df[inundado_x_dia_df['year'].isin([2017,2018,2019])], ax=ax)\nsns.lineplot(x='date', y='tweets', data=dengue_x_dia_df[dengue_x_dia_df['year'].isin([2017,2018,2019])], ax=ax)\n\nplt.title('Cantidad de Menciones por Dia')\nplt.ylabel('Menciones')\nplt.xlabel('Fecha')\n\nax.legend(['Baches', 'Basura', 'Raudales', 'Dengue'], loc='upper left', prop={'size': 12})","24a23342":"# Agrupar por mes\ndengue_x_mes_df = dengue_x_dia_df.groupby(['year','month']) \\\n                    .sum().reset_index()[['month','year','tweets']]\n\n# Pivotear\ndengue_x_mes_pivot = dengue_x_mes_df \\\n                    .pivot(index='month',columns='year', values='tweets')\n    \ndengue_x_mes_pivot.plot(kind='bar', figsize=(15, 5), color=['lightgray', 'gray', 'black'], rot=0)                                       \nplt.title(\"Comparaciones interanuales de menciones de Dengue\")\nplt.xlabel(\"Mes\", labelpad=16)\nplt.ylabel(\"Menciones (Dengue)\", labelpad=16)","a41a577b":"# Agrupar por mes\nbasura_x_mes_df = basura_x_dia_df.groupby(['year','month']) \\\n                    .sum().reset_index()[['month','year','tweets']]\n    \n# Pivotear\nbasura_x_mes_pivot = basura_x_mes_df \\\n                    .pivot(index='month',columns='year', values='tweets')\n\nbasura_x_mes_pivot.plot(kind='bar', figsize=(15, 5), color=['lightgray', 'gray', 'black'], rot=0)                                       \nplt.title(\"Comparaciones interanuales de menciones de Basura\")\nplt.xlabel(\"Mes\", labelpad=16)\nplt.ylabel(\"Menciones (Basura)\", labelpad=16)","4a528451":"# Agrupar por mes\ninundado_x_mes_df = inundado_x_dia_df.groupby(['year','month']) \\\n                    .sum().reset_index()[['month','year','tweets']]\n\n# Pivotear\ninundado_x_mes_pivot = inundado_x_mes_df \\\n                    .pivot(index='month',columns='year', values='tweets')\n    \ninundado_x_mes_pivot.plot(kind='bar', figsize=(15, 5), color=['lightgray', 'gray', 'black'], rot=0)                                       \nplt.title(\"Comparaciones interanuales de menciones de Raudales\")\nplt.xlabel(\"Mes\", labelpad=16)\nplt.ylabel(\"Menciones (Raudales)\", labelpad=16)","b4ee961f":"# Cantidad de topicos\nn_topics = 5\n\n# Pipeline con pasos a ejectuar\ntext_pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(ngram_range=(2,3), min_df=100, max_df=0.85)),\n    ('lda', LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method='online'))\n])\nt0 = time()\n# Entrenar y transformar modelo\nlda_model = text_pipeline.fit_transform(df_tweets_muni_filtro['tweet_cleaned'])\nprint(time() - t0)","469ae0b1":"# Modelos resultantes del pipeline\ntfidf = text_pipeline.steps[0][1]\nlda = text_pipeline.steps[1][1]\nvocabulario = tfidf.get_feature_names()","372fcfb6":"top_topics = 5\ntopic_dict = {}\ntopic_scores = []\n\n# Para cada topico, buscamos el top 5 de acuerdo a los scores calculados por el modelo\nfor topic_idx, topic in enumerate(lda.components_):\n    topic_dict[str(topic_idx)] = \",\".join([vocabulario[i] for i in topic.argsort()[:-top_topics - 1:-1]])\n    topic_scores.append([topic[i] for i in topic.argsort()[:-top_topics - 1:-1]])\ndf_topics_lda = pd.DataFrame(topic_dict, index=['bigrams'])\ndf_topics_lda = df_topics_lda.T.reset_index()\n\n# Cargamos las palabras claves por topico en un dataframe\ndf_topics_names = pd.DataFrame(df_topics_lda.bigrams.str.split(',').tolist(), index=df_topics_lda.index) \\\n            .stack() \\\n            .reset_index() \\\n            .drop(['level_1'], axis=1) \\\n            .rename(columns={0:'bigrams', 'level_0':'topic'})\n\n# Cargamos scores por topicos en un dataframe\ndf_topics_scores = pd.DataFrame(topic_scores) \\\n        .stack() \\\n        .reset_index(drop=True)\n\n# Concatenamos palabras claves con scores correspondientes\ndf_topics = pd.concat([df_topics_names, df_topics_scores], axis=1) \\\n        .rename(columns={0:'score'})\n    ","a814cc5e":"fig = plt.figure(figsize=(12,10))\n\nfor i in range(1,n_topics+1):    \n    plt.subplot(3,2,i,frameon=True)\n    sns.barplot('score', 'bigrams', data=df_topics[df_topics['topic']==i-1], orient='h') \n    plt.title(\"Topico {}\".format(i))\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.tight_layout()","13a8e1c1":"### Bigramas frecuentes por a\u00f1o\nLos bigramas son combinaciones de dos palabras que pueden dar una mejor idea de los temas de conversaci\u00f3n. En este caso, me interesa conocer los bigramas que m\u00e1s se repiten y para ellos aplico t\u00e9cnicas de tokenizaci\u00f3n que separan las palabras del texto y cada par se convierte en una fila. Tambi\u00e9n se puede modificar el tama\u00f1o del ngram para formar unigramas, trigramas, etc. El bigrama es una opci\u00f3n intermedia que permite tener algo m\u00e1s de contexto pero tiene suficientes ocurrencias para que sea significativa la muestra (mientras mayor sea el ngram, menor el n\u00famero de ocurrencias).","dbc6afcf":"### Deduplicaci\u00f3n y filtrado\n\nEn las siguientes celdas, elimino los tweets duplicados que puedieron aparecer al realizar la extracci\u00f3n o en caso que usuarios hayan publicado el mismo tweet varias veces.\n\nLuego, identifico los usuarios con m\u00e1s cantidad de tweets y excluyo aquellas cuentas relacionadas a la Municipalidad, medios de comunicaci\u00f3n u otros entes p\u00fablicos de tal manera a considerar \u00fanicamente las publicaciones de los ciudadanos.","801b8438":"## Introducci\u00f3n","e408d4c9":"En Mayo vemos picos de menciones a raudales que coinciden con los picos de usuarios unicos que vimos mas arriba. Tambi\u00e9n se observa un pico de conversaciones relacionadas al dengue en Marzo de 2018 que coincide con uno de los brotes mas importantes en los \u00faltimos a\u00f1os.\n\nPara facilitar comparaciones interanuales, en las pr\u00f3ximas gr\u00e1ficas podemos ver para cada tema como cambian las menciones en el tiempo. En el caso de dengue por ejemplo, se observa un aumento importante en Marzo de 2018. Por otro lado, en las menciones de raudales se ve un crecimiento notorio en Mayo de 2019 en comparaci\u00f3n con a\u00f1os anteriores.","8572ea3a":"### Carga de tweets a *pandas* dataframe\nLos tweets se encuentran contenidos en un archivo cuya estructura se detalla en el apartado de datos en Kaggle. Adem\u00e1s de cargar los tweets a un dataframe de *pandas*, derivo otros atributos que ser\u00e1n \u00fatiles m\u00e1s adelante. Extraer algunos tweets de ejemplo puede ser \u00fatil para entender los tipos de datos en cada columna.","a2c41b48":"La siguiente gr\u00e1fica muestra las ocurrencias de cada bigrama por a\u00f1o. Se pueden identificar por ejemplo bigramas que aparecen en multiples a\u00f1os","55bc7c98":"### Usuarios \u00fanicos\nLa cantidad de usuarios \u00fanicos que publican tweets nos puede ayudar a identificar situaciones que provocaron mayores picos de participaci\u00f3n en el tiempo. Si nos fijamos la gr\u00e1fica de abajo, se observan ciertos picos en el 2019, tanto en Mayo como en Julio. \n\u00bfQu\u00e9 se mencionaba con frecuencia en estas fechas?","7c807498":"Finalmente se observan los bigramas mas frecuentes para cada una de las categorias. No se identifican categorias especificas y esto puede deberse a que no existe una diferenciacion muy clara entre los temas de conversacion. Tambi\u00e9n se pueden obtener mejores resultados realizando validaciones adicionales y excluyendo otras combinaciones poco relevantes.","b02fceac":"### An\u00e1lisis por eventos\nCon lo analizado hasta ahora, tenemos una leve idea de los temas de conversaci\u00f3n. Adem\u00e1s, existen otros temas que pueden ser interesantes y no se encuentran a simple vista. Para entender un poco mejor como estos temas se mencionan en el tiempo, utilizo expresiones regulares que de acuerdo a ciertos patrones de b\u00fasqueda identifiquen tweets relacionados a los temas que nos interesan. En este caso, eleg\u00ed los siguientes temas que me parecieron los m\u00e1s reclamados por la ciudadan\u00eda: **basura, raudales, baches y dengue**. Es importante tener en cuenta que un mismo tweet puede pertenecer a mas de una categoria porque de hecho puede existir correlacion entre varios de estos temas.","8c6affc6":"### Wordcloud\nLas nubes de palabras o word clouds permiten visualizar las palabras m\u00e1s frecuentes de un texto utilizando el tama\u00f1o para representar la frecuencia o importancia. En este caso, las palabras se extraen de los tweets filtrados (cerca de 200.000).\n\nPara volverlo un poco m\u00e1s divertido y patriota, utilizo un fondo con nuestra bandera pero se puede adaptar a cualquier tipo de imagen.","38243939":"### Librerias\nEn primer lugar, importamos todas las librerias necesarias para el an\u00e1lisis. En este caso, decid\u00ed utilizar Pandas para la manipulaci\u00f3n de los datos con los conocidos dataframes, seaborn y matplotlib para visualizaciones, NLTK para tokenizaci\u00f3n, en conjunto con sklearn para implementar t\u00e9cnicas como TF-IDF que son \u00fatiles para extraer las combinaciones de palabras m\u00e1s relevantes y como entrada para otras t\u00e9cnicas como LDA, que detallo m\u00e1s adelante.","131ba50d":"# Reclamos ciudadanos en Twitter","dd8818c5":"## Modelado de Topicos con LDA\nPara casos como este, d\u00f3nde tenemos una buena idea de los temas de conversaci\u00f3n en los tweets, las expresiones regulares pueden ser suficientes. Sin embargo, existen otros casos d\u00f3nde se necesitan de t\u00e9cnicas m\u00e1s avanzadas para identificar temas que pueden estar escondidos, o latentes. \n\nExisten diferentes t\u00e9cnicas de identificaci\u00f3n de temas o t\u00f3picos pero una de las m\u00e1s utilizadas es Latent Dirichlet Allocation o LDA. Se trata de una t\u00e9cnica que genera un modelo probabil\u00edstico que asume que cada tema es una combinaci\u00f3n de palabras y que cada documento (o tweet en este caso) es una combinaci\u00f3n de temas con diferentes probabilidades.\n\nEn las celdas de abajo, creo un Pipeline que primero aplica una t\u00e9cnica conocida como TF-IDF que calcula la frecuencia de palabras en los tweets, y calcula un score para cada palabra dando menos importancia a aquellas que aparecen con demasiada frecuencia y son poco relevantes. En el siguiente paso del pipeline se entrena el modelo y transforma el dataframe original. En este caso, elijo clasificar los temas en 5 diferentes categorias pero el n\u00famero puede variar de acuerdo al caso. Existen formas de medir la calidad del modelo, calculando lo que se denomina *perplexity* pero no lo voy a utilizar en este caso.\n\nOtro punto importante es que el entrenamiento del modelo puede tomar mucho tiempo y por esto es clave definir ciertos parametros que limiten la busqueda y la cantidad de iteraciones. ","37080167":"\"gente puerca\", \"basura calle\", \"tira basura\".. estos bigramas dan para pensar que en estas fechas pudo haber llovido con mucha frequencia, lo que pudo haber ocasionado raudales que a su vez movieron las basuras de un lado para otro. ","8163f3bf":"## An\u00e1lisis Exploratorio","9c95d21c":"### Limpieza\nLos tweets publicados pueden tener informaci\u00f3n poco relevante para an\u00e1lisis textuales como por ejemplo URLs, emails, referencias a imagenes o simbolos. Adem\u00e1s, existen palabras conocidas como stopwords que se repiten frecuentemente y no aportan al entendimiento de la conversaci\u00f3n (el, la, y, con, para, mi, etc.). NLTK es una librer\u00eda de NLP que incluye stopwords en diferentes idiomas, incluyendo espa\u00f1ol.","e1624a29":"Desde hace ya unos a\u00f1os, las redes sociales se volvieron el medio de comunicaci\u00f3n preferido de los ciudadanos para realizar reclamos relacionados a la provisi\u00f3n de servicios p\u00fablicos e infraestructura (electricidad, agua potable, recolecci\u00f3n de basura, reportes de baches, etc.)\n\nSi bien esto produjo un avance importante en la comunicaci\u00f3n ciudadan\u00eda-autoridades, el exceso y la velocidad de generaci\u00f3n de la informaci\u00f3n impide tener un an\u00e1lisis certero de los reclamos como para reaccionar de manera eficaz, entender la causa ra\u00edz y prevenir futuros eventos.\n\nEntender esto nos ayudar\u00eda por ejemplo a responder las siguientes preguntas:\n\n- \u00bfCu\u00e1les son los reclamos m\u00e1s frecuentes realizados por los ciudadanos?\n- \u00bfC\u00f3mo var\u00eda la intensidad de estos reclamos en el tiempo?\n- \u00bfCu\u00e1l es el sentimiento de las publicaciones realizadas por los ciudadanos hacia las autoridades?\n\nCon el objetivo de lograr un entendimiento mas profundo que nos permita responder a estas y otras preguntas, utilizo los tweets o publicaciones realizadas en Twitter donde se mencionan a la Municipalidad de Asunci\u00f3n (@AsuncionMuni) y al Intendente (@FerreiroMario1).","422eb07b":"## Otros an\u00e1lisis que se podrian realizar con estos datos:\n- An\u00e1lisis de sentimiento\n- Correlaci\u00f3n entre temas y visualizaci\u00f3n con *scattertext*\n- Series temporales para predecir eventos (como brotes de dengue)"}}