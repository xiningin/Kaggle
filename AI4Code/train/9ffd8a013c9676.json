{"cell_type":{"525bb7cb":"code","2dc7b54a":"code","f62934fc":"code","0de9483d":"code","b27e8b1a":"code","c9c62f01":"code","fda8e2a4":"code","1bb68e53":"code","ad2e9c7a":"code","07b64273":"code","5d07e200":"code","467a44bb":"code","ae068845":"code","078cea9a":"code","bbb92959":"code","d6e3e5bc":"code","5c613365":"code","4b6969f0":"markdown","4a0d9949":"markdown","6504b055":"markdown","f4be8c63":"markdown"},"source":{"525bb7cb":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBClassifier\n\nimport matplotlib.pyplot as plt\n        \ninput_path = Path('\/kaggle\/input\/tabular-playground-series-mar-2021\/')","2dc7b54a":"train = pd.read_csv(input_path \/ 'train.csv', index_col='id')\ndisplay(train.head())","f62934fc":"# are there any missing values?\ntrain.isna().any().any()","0de9483d":"test = pd.read_csv(input_path \/ 'test.csv', index_col='id')\ndisplay(test.head())","b27e8b1a":"# are there any missing values?\ntest.isna().any().any()","c9c62f01":"submission = pd.read_csv(input_path \/ 'sample_submission.csv', index_col='id')\ndisplay(submission.head())","fda8e2a4":"train.iloc[:,20:31].corr()['target'].abs() > 0.1\n","1bb68e53":"X = train.drop(columns=['target'])\ny = train['target']\nT = test.copy()\n\n#X_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.60)","ad2e9c7a":"labels = []\ncategorical_cols = []\ndrop_list = []\ncontinuous_cols = []\nMAX_CAT = 15\nMIN_CORR = 0.1\nfor idx, c in enumerate(train.columns):\n    if train[c].dtype=='object': \n        all_labels = list(set(train[c].values).union(set(test[c].values)))\n        if len(all_labels) <= MAX_CAT:\n            labels.append(all_labels)\n            categorical_cols.append(c)\n        else:\n             drop_list.append(c)  \n    elif c not in ['target']:\n        if abs(train[[c, 'target']].corr()['target'][0]) > MIN_CORR:\n            continuous_cols.append(c)\n                \n# labels\n#print(categorical_cols)\n# drop_list\n#continuous_cols","07b64273":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('encoder', OrdinalEncoder(categories=labels))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols),\n        ('cont', 'passthrough', continuous_cols)  # list of continuous columns\n    ], remainder='drop')","5d07e200":"my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', XGBClassifier(n_estimators=500,\n                                                      booster='gbtree',\n                                                      use_label_encoder=False,\n                                                      learning_rate=0.02,\n                                                      eval_metric='auc',\n                                                      n_jobs=-1,\n                                                      random_state=42))\n                             ])\n","467a44bb":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(my_pipeline, X, y,\n                         cv=3,\n                         scoring='roc_auc')\n\nprint(\"ROC scores:\\n\", scores)","ae068845":"my_pipeline.fit(X, y) ;","078cea9a":"T.head()","bbb92959":"# My Prediction\nsubmission['target'] = my_pipeline.predict_proba(T)[:, 1]","d6e3e5bc":"submission.head(20)","5c613365":"submission.to_csv('random_forest.csv')","4b6969f0":"## We need to encode the categoricals.\n\nThere are different strategies to accomplish this, and different approaches will have different performance when using different algorithms.  You may decide to encode features with high cardinality (e.g., more distinct values) diffirently than features with low cardinality. For this starter notebook, we'll use simple encoding.\n\n### Update \n\n* I want to implement a pipeline with one hot encoding like I learned in the tutorials. But one hot encoding turned out to be worse.\n* The I wanted to use the LabelEncoder within a pipeline, but it turns out, this is not ment to be used for transforming [features](https:\/\/www.kaggle.com\/getting-started\/146568).\n* Next step I try the OrdinalEncoder. ","4a0d9949":"In this notebook, you will learn how to make your first submission to the [Tabular Playground Series - Mar 2021 competition.](https:\/\/www.kaggle.com\/c\/tabular-playground-series-mar-2021)\n\n# Make the most of this notebook!\n\nYou can use the \"Copy and Edit\" button in the upper right of the page to create your own copy of this notebook and experiment with different models. You can run it as is and then see if you can make improvements.","6504b055":"# Read in the data files","f4be8c63":"# Let's train it on all the data and make a submission!"}}