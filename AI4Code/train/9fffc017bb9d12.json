{"cell_type":{"14d84471":"code","7ccf9508":"code","cdc3c69c":"code","14fcab63":"code","10ceccbd":"code","f55a342d":"code","0be408d6":"code","b3eeaefc":"code","42b67c42":"code","40f0544b":"code","03468fa2":"code","71e606d0":"code","d466cdd8":"code","27f45247":"markdown","4ba8bd36":"markdown","cd722663":"markdown","b493de10":"markdown","3371cc6a":"markdown","d970d7b8":"markdown","f4abbd48":"markdown","67fbf768":"markdown","ecee3184":"markdown","4f2a2943":"markdown"},"source":{"14d84471":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport numpy as np\nimport pandas as pd\nimport datetime, random, math\nfrom catboost import CatBoostClassifier\nimport lightgbm as lgb\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\nimport gc, pickle\nimport ast\nfrom typing import Union\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, log_loss\nfrom sklearn.linear_model import Ridge,Lasso, BayesianRidge\nfrom sklearn.svm import LinearSVR\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.cluster import KMeans\n%matplotlib inline","7ccf9508":"class WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight \/ lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score \/ scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        group_ids = []\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            group_ids.append(group_id)\n            all_scores.append(lv_scores.sum())\n\n        return group_ids, all_scores","cdc3c69c":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef set_index(df, name):\n    d = {}\n    for col, value in df.iloc[0,:].items():\n        try:\n            if '_evaluation' in value:\n                d[col] = 'id'\n            if 'd_' in value:\n                d[col] = 'd'\n        except:\n            if type(value)!=str:\n                d[col]=name\n    return d\n\ndef dcol2int(col):\n    if col[:2]=='d_':\n        return int(col.replace('d_', ''))\n    else:\n        return col\n    \ndef str_category_2_int(data):\n    categories = [c for c in data.columns if data[c].dtype==object]\n    for c in categories:\n        if c=='id' or c=='d':\n            pass\n        else:\n            data[c] = pd.factorize(data[c])[0]\n            data[c] = data[c].replace(-1, np.nan)\n    return data\n\ndef select_near_event(x, event_name):\n    z = ''\n    for y in x:\n        if y in event_name:\n            z+=y+'_'\n    if len(z)==0:\n        return np.nan\n    else:\n        return z\n    \ndef sort_d_cols(d_cols):\n    d_cols = [int(d.replace('d_','')) for d in d_cols]\n    d_cols = sorted(d_cols)\n    d_cols = [f'd_{d}' for d in d_cols]\n    return d_cols","14fcab63":"def preprocessing(path, d_cols):\n    train_d_cols = d_cols[-(200+63):]\n    train_df = pd.read_csv(path+'sales_train_evaluation.csv')\n    calendar_df = pd.read_csv(path+'calendar.csv')\n    sell_prices_df = pd.read_csv(path+'sell_prices.csv')\n    sell_prices_df['price'] = sell_prices_df['sell_price']\n    del sell_prices_df['sell_price']\n    sample_submission_df = pd.read_csv(path+'sample_submission.csv')\n    \n    train_df.index = train_df.id\n    calendar_df['date']=pd.to_datetime(calendar_df.date)\n    calendar_df.index = calendar_df.d\n    \n    \n    cat_cols = [ col for col in train_df.columns if 'id' in str(col)]\n    new_columns = cat_cols+d_cols\n    train_df = train_df.reindex(columns=new_columns)\n    \n    data = train_df[train_d_cols].stack(dropna=False).reset_index()\n    data = data.rename(columns=set_index(data, 'TARGET'))\n    data.reset_index(drop=True, inplace=True)\n    \n    data['wm_yr_wk'] = data.d.map(calendar_df.set_index('d')['wm_yr_wk'])\n    for key, value in train_df[['item_id','dept_id', 'cat_id', 'state_id', 'store_id']].to_dict().items():\n        data[key] = data.id.map(value)\n    data = pd.merge(data, sell_prices_df[['store_id', 'item_id', 'wm_yr_wk', 'price']], on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n    data.drop('wm_yr_wk', axis=1, inplace=True)\n    data['dept_id_price'] = data['price']\/data.groupby(['d', 'dept_id', 'store_id'])['price'].transform('mean')\n    data['cat_id_price'] = data['price']\/data.groupby(['d', 'cat_id', 'store_id'])['price'].transform('mean')\n    data['is_sell_cnt'] = data.groupby(['dept_id', 'store_id', 'd'])['price'].transform(lambda x: x.notnull().sum())\n    \n    #snap_data\n    snap_data = calendar_df[['snap_CA', 'snap_WI', 'snap_TX', 'd']]\n    snap_data.set_index('d', inplace=True)\n    data[f'snap']=0\n    for key, value in snap_data.to_dict().items():\n        k = key.replace('snap_', '')\n        data.loc[data.state_id==k,'snap'] = data.loc[data.state_id==k, 'd'].map(value).fillna(0)\n    \n    \n    event_name = ['SuperBowl', 'ValentinesDay', 'PresidentsDay', 'LentStart', 'LentWeek2', 'StPatricksDay', 'Purim End', \n              'OrthodoxEaster', 'Pesach End', 'Cinco De Mayo', \"Mother's day\", 'MemorialDay', 'NBAFinalsStart', 'NBAFinalsEnd',\n              \"Father's day\", 'IndependenceDay', 'Ramadan starts', 'Eid al-Fitr', 'LaborDay', 'ColumbusDay', 'Halloween', \n              'EidAlAdha', 'VeteransDay', 'Thanksgiving', 'Christmas', 'Chanukah End', 'NewYear', 'OrthodoxChristmas', \n              'MartinLutherKingDay', 'Easter']\n    event_type = ['Sporting', 'Cultural', 'National', 'Religious']\n    event_names = {'event_name_1':event_name, 'event_type_1':event_type}\n    for event, event_name in event_names.items():\n        for w in [4]:\n            calendar_df[f'new_{event}_{w}']=''\n            for i in range(-1,-(w+1),-1):\n                calendar_df[f'new_{event}_{w}'] += calendar_df[event].shift(i).astype(str)+'|'\n            calendar_df[f'new_{event}_{w}'] = calendar_df[f'new_{event}_{w}'].apply(lambda x: x.split('|'))\n            calendar_df[f'new_{event}_{w}'] = calendar_df[f'new_{event}_{w}'].apply(lambda x: select_near_event(x, event_name))\n\n    #calendar_dict\n    cols = ['new_event_name_1_4', 'new_event_type_1_4', 'wday', 'month', 'year', 'event_name_1','event_type_1']\n    for key, value in calendar_df[cols].to_dict().items():\n        data[key] = data.d.map(value)\n    for shift in [-1,1]:\n        data[f'snap_{shift}'] = data.groupby(['id'])['snap'].shift(shift)\n        data[f'snap_{shift}'] = data[f'snap_{shift}'].fillna(0)\n    \n    data = reduce_mem_usage(data)\n    gc.collect()\n    return data","10ceccbd":"def make_roll_data(data, win):\n    data_2 = data.groupby(['id'])['TARGET'].apply(\n            lambda x:\n            x.shift(1).rolling(win, min_periods=1).agg({'mean'})\n        )\n    for col in data_2.columns:\n        data[f'roll_{win}_{col}'] = data_2[col]\n        \n    return data\n\ndef shift_diff_data(data):\n    data[f'shift_diff']=0\n    for i in range(4):\n        data[f'shift_diff'] += data.groupby(['id'])['TARGET'].apply( lambda x :x.diff(7*i).shift(7) )\/4\n    return data\n    \n\ndef make_lag_roll_data(data, lag):\n    data[f'lag{lag}_roll_14_mean'] = data.groupby(['id'])['TARGET'].apply(\n        lambda x:\n        x.shift(lag).rolling(28, min_periods=1).mean()\n    )\n    \n    return data\n\ndef make_shift_data(data):\n    for i in range(0,10,2):\n        data[f'shift_{7*(i+1)}'] = data.groupby(['id'])['TARGET'].shift(7*(i+1))\n        \n    return data\n\n\ndef fe(data):\n    data = make_roll_data(data, 7)\n    data = make_roll_data(data, 28)\n    data = shift_diff_data(data)\n    data = make_lag_roll_data(data, 56)\n    data = make_lag_roll_data(data, 84)\n    data = make_shift_data(data)\n    \n    return data","f55a342d":"def plot_importance(models, col):\n    importances = np.zeros(len(col))\n    for model in models:\n        importances+=model.feature_importance(importance_type='gain')\n    importance = pd.DataFrame()\n    importance['col'] = col\n    importance['importance'] = minmax_scale(importances)\n    #importance.to_csv(f'importance_{name}.csv',index=False)\n    return importance\n\ndef predict_cv(x_val, models):\n    preds = np.zeros(len(x_val))\n    for model in models:\n        pred = model.predict(x_val)\n        preds+=pred\/len(models)\n    return preds\n\ndef train_predict_RE(data, PARAMS):\n    days = sorted(data.d.unique())\n    days = sort_d_cols(days)\n    trn_days = days[:-28]\n    val_days = days[-28:]\n        \n    for i in range(28):\n        data = fe(data)\n        if i==0:\n            shift_cols = [col for col in data.columns if 'shift' in col]\n            roll_cols = [col for col in data.columns if 'roll' in col]\n            features=['dept_id', 'store_id', 'snap', 'snap_1', 'dept_id_price', 'cat_id_price', 'price',\n                      'new_event_type_1_4','event_name_1','event_type_1', 'wday', \n                      'is_sell_cnt'\n                     ]+shift_cols+roll_cols\n            models=[]\n            print(f' FEATIRE LEN {len(features)}')\n            X = data[data.d.isin(trn_days)][data.TARGET.notnull()]\n            X.reset_index(drop=True, inplace=True)\n            train_set = lgb.Dataset(X.loc[:,features], X.loc[:,'TARGET'])\n            for cy in range(3):\n                PARAMS['random_state']=2020+cy\n                model = lgb.train(train_set=train_set, params=PARAMS,verbose_eval=500)\n                models.append(model)\n        \n        val_day = val_days[i]\n        predict_data = data[data.d==val_day]\n        preds = predict_cv(predict_data[features], models)\n        data.loc[data.d==val_day, 'TARGET'] = preds\n        \n    sub = data[data.d.isin(val_days)][['id', 'd', 'TARGET', 'price']]\n    return sub","0be408d6":"def create_is_sell_data(sell_prices_df, calendar_df, train_df):\n    train_df.index = train_df['id']\n    sell_prices_df['id'] = sell_prices_df['item_id'].astype('str')+'_'+sell_prices_df['store_id']+'_evaluation'\n    sell_prices_data = sell_prices_df[sell_prices_df.wm_yr_wk.isin(calendar_df.wm_yr_wk.unique())]\n    sell_prices_data.reset_index(drop=True, inplace=True)\n    tmp = sell_prices_data.groupby(['id'])[['wm_yr_wk', 'sell_price']].apply(\n        lambda x: x.set_index('wm_yr_wk')['sell_price'].to_dict()\n    ).to_dict()\n    d = calendar_df.d\n    wm_yr_wk = calendar_df.wm_yr_wk\n    price_data = {}\n    for col in tqdm(train_df.id.unique()):\n        price_data[col] = wm_yr_wk.map(tmp[col])\n    price_data = pd.DataFrame(price_data)\n    price_data.index = d\n    is_sell = price_data.notnull().astype(float).T\n    price_data = price_data.fillna(0)\n    \n    is_sell.index=train_df.id\n    train_df.index=train_df.id\n    is_sell = pd.concat([\n        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], is_sell\n    ], axis=1)\n    price_data = pd.concat([\n        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], price_data.T  \n    ], axis=1)\n    \n    return price_data, is_sell\n    \ndef calendar_preprocessing(calendar):\n    calendar['qaurter'] = pd.to_datetime(calendar['date']).dt.day.apply(lambda x: x\/\/7)\n\n    event_name = ['SuperBowl', 'ValentinesDay', 'PresidentsDay', 'LentStart', 'LentWeek2', 'StPatricksDay', 'Purim End', \n                'OrthodoxEaster', 'Pesach End', 'Cinco De Mayo', \"Mother's day\", 'MemorialDay', 'NBAFinalsStart', 'NBAFinalsEnd',\n                \"Father's day\", 'IndependenceDay', 'Ramadan starts', 'Eid al-Fitr', 'LaborDay', 'ColumbusDay', 'Halloween', \n                'EidAlAdha', 'VeteransDay', 'Thanksgiving', 'Christmas', 'Chanukah End', 'NewYear', 'OrthodoxChristmas', \n                'MartinLutherKingDay', 'Easter']\n    event_type = ['Sporting', 'Cultural', 'National', 'Religious']\n    event_names = {'event_name_1':event_name, 'event_type_1':event_type}\n    for event, event_name in event_names.items():\n        calendar[f'new_{event}']=''\n        for i in range(-1,-5,-1):\n            calendar[f'new_{event}'] += calendar[event].shift(i).astype(str)+'|'\n        calendar[f'new_{event}'] = calendar[f'new_{event}'].apply(lambda x: x.split('|'))\n        calendar[f'new_{event}'] = calendar[f'new_{event}'].apply(lambda x: select_near_event(x, event_name))\n    return calendar\n\nclass M5_Data:\n    def __init__(self, path, d_cols):\n        train = pd.read_csv(path+'sales_train_evaluation.csv')\n        calendar = pd.read_csv(path+'calendar.csv')\n        price = pd.read_csv(path+'sell_prices.csv')\n        self.price_data, self.is_sell = create_is_sell_data(price, calendar, train)\n\n        self.d_cols = d_cols\n        train = train.reindex(\n            columns=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']+self.d_cols\n        )\n\n        train = train.set_index('id', drop=False)\n        self.train = pd.concat([\n            train[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']],\n            train[self.d_cols]*self.price_data[self.d_cols]\n        ], axis=1)\n        self.calendar = calendar_preprocessing(calendar)\n        \n        self.make_all_id()\n        \n    def make_all_id(self):\n        self.train['all_id'] = 'all_id'\n        self.price_data['all_id'] = 'all_id'\n        \n    \n    def make_id(self, id_1, id_2):\n        new_id = id_1+'X'+id_2\n        self.train[new_id] = self.train[id_1].astype(str)+'X'+self.train[id_2].astype(str)\n        self.price_data[new_id] = self.price_data[id_1].astype(str)+'X'+self.price_data[id_2].astype(str)","b3eeaefc":"def sale_cnt_by_group(ID, data, m5_data, trn_d_cols, group='state_id'):\n    for _id in m5_data.train[group].unique():\n        f = m5_data.train[m5_data.train[group]==_id].groupby(ID)[trn_d_cols].sum(min_count=1).T\n        f = f.stack(dropna=False).reset_index().rename(columns={0:f'TARGET_{_id}', 'level_0':'d'})\n        data = pd.merge(data, f, on=['d', ID])\n        \n        f = m5_data.price_data[m5_data.price_data[group]==_id].replace(0, np.nan).groupby(ID)[trn_d_cols].count().T\n        f = f.stack(dropna=False).reset_index().rename(columns={0:f'cnt_{_id}', 'level_0':'d'})\n        data = pd.merge(data, f, on=['d', ID])\n    return data\n\ndef fe_group(ID, data, log=False):\n    target_cols = [col for col in data.columns if 'TARGET' in col]\n    for target_col in target_cols:\n        if log:\n            data[target_col] = np.log1p(data[target_col])\n        for win in [7,28]:\n            agg = {'mean'}\n            data_2 = data.groupby(ID)[target_col].apply(\n                lambda x: x.rolling(win, min_periods=1).agg(agg)\n            )\n            for col in data_2.columns:\n                data[f'roll{win}_{col}_{target_col}'] = data_2[col]\n        data[f'roll28_mean_{target_col}_lag56'] = data.groupby(ID)[f'roll28_mean_{target_col}'].shift(56)\n        data[f'roll28_mean_{target_col}_lag84'] = data.groupby(ID)[f'roll28_mean_{target_col}'].shift(84)\n        \n        for i in range(0,10,2):\n            data[f'shift{7*(i+1)}_{target_col}'] = data.groupby(ID)[target_col].shift(7*i)\n                \n        data[f'shift_diff_{target_col}']=0\n        for i in range(4):\n            data[f'shift_diff_{target_col}'] += data.groupby(ID)[target_col].diff(7*i)\/4\n    \n    del_f = [col for col in target_cols if col!='TARGET']\n    data.drop(columns=del_f, inplace=True)\n    \n    return data\n\ndef map_calendar_data(ID, data, m5_data, map_1, map_2):\n    \"\"\"\n    map_1=['qaurter','wday','month','year','new_event_name_1', \n    'new_event_type_1','event_name_1','event_type_1',\n    'snap_CA','snap_WI','snap_TX']\n    map_2=['event_name_1','event_type_1','snap_CA','snap_WI','snap_TX']\n    \"\"\"\n    for key, value in m5_data.calendar.set_index('d')[map_1].items():\n        data[key] = data.d.map(value)\n        if data[key].dtypes==object:\n            data[key] = pd.factorize(data[key])[0]\n        if key in map_2:\n            for i in [-1,1]:\n                data[f'{key}_{i}'] = data.groupby(ID)[key].shift(i)\n    return data\n\ndef make_snap_data(ID, data, m5_data):\n    data['snap'] = 0\n    for key, value in m5_data.calendar.set_index('d')[['snap_CA','snap_WI','snap_TX']].items():\n        state = key.replace('snap_', '')\n        data.loc[data[ID].str.contains(state),'snap'] = data.loc[data[ID].str.contains(state),'d'].map(value)\n    for i in [ -1,1]:\n        data[f'snap_{i}'] = data.groupby(ID)['snap'].shift(i)\n    return data\n\ndef make_data_dept_cat(ID, trn_d_cols, m5_data):\n    data = m5_data.train.groupby(ID)[trn_d_cols].sum(min_count=1).T\n    data = data.stack(dropna=False).reset_index().rename(columns={0:'TARGET', 'level_0':'d'})\n    f = m5_data.price_data.replace(0,np.nan).groupby(ID)[\n        trn_d_cols\n    ].count().stack(dropna=False).reset_index().rename(columns={0:'cnt', 'level_1':'d', 'level_0':'d'})\n    data = pd.merge(data, f, on=['d', ID])\n    \n    data = sale_cnt_by_group(ID, data, m5_data, trn_d_cols)\n    data = fe_group(ID, data, log=True)\n\n    map_1 = ['qaurter','wday','month','year','new_event_name_1',\n             'new_event_type_1','event_name_1','event_type_1',\n             'snap_CA','snap_WI','snap_TX']\n    map_2 = ['event_name_1','event_type_1','snap_CA','snap_WI','snap_TX']\n    data = map_calendar_data(ID, data, m5_data, map_1, map_2)\n    \n    data[f'f_{ID}'] = pd.factorize(data[ID])[0]\n    data = data[data.d.isin(trn_d_cols[28:])]\n    return data    \n\ndef make_data_all_id(ID, trn_d_cols, m5_data):\n    data = m5_data.train.groupby(ID)[trn_d_cols].sum(min_count=1).T\n    data = data.stack(dropna=False).reset_index().rename(columns={0:'TARGET', 'level_0':'d'})\n    f = m5_data.price_data.replace(0,np.nan).groupby(ID)[\n        trn_d_cols\n    ].count().stack(dropna=False).reset_index().rename(columns={0:'cnt', 'level_1':'d', 'level_0':'d'})\n    data = pd.merge(data, f, on=['d', ID])\n    \n    data = sale_cnt_by_group(ID, data, m5_data, trn_d_cols)\n    data = fe_group(ID, data, log=False)\n    \n    map_1 = ['qaurter','wday','month','year','new_event_name_1',\n             'new_event_type_1','event_name_1','event_type_1',\n             'snap_CA','snap_WI','snap_TX']\n    map_2 = ['event_name_1','event_type_1','snap_CA','snap_WI','snap_TX']\n    data = map_calendar_data(ID, data, m5_data, map_1, map_2)\n\n    data[f'f_{ID}'] = pd.factorize(data[ID])[0]\n    data = data[data.d.isin(trn_d_cols[28:])]\n    return data\n\ndef make_data_state_store(ID, trn_d_cols, m5_data):\n    \n    data = m5_data.train.groupby(ID)[trn_d_cols].sum(min_count=1).T\n    data = data.stack(dropna=False).reset_index().rename(columns={0:'TARGET', 'level_0':'d'})\n    f = m5_data.price_data.replace(0,np.nan).groupby(ID)[\n        trn_d_cols\n    ].count().stack(dropna=False).reset_index().rename(columns={0:'cnt', 'level_1':'d', 'level_0':'d'})\n    data = pd.merge(data, f, on=['d', ID])\n    \n    group='cat_id'\n    for _id in m5_data.train[group].unique():\n        f = m5_data.train[m5_data.train[group]==_id].groupby(ID)[trn_d_cols].sum(min_count=1).T\n        f = f.stack(dropna=False).reset_index().rename(columns={0:f'TARGET_{_id}', 'level_0':'d'})\n        data = pd.merge(data, f, on=['d', ID])\n    \n    for _id in m5_data.price_data[group].unique():\n        f = m5_data.price_data[m5_data.price_data[group]==_id].replace(0, np.nan).groupby(ID)[trn_d_cols].count().T\n        f = f.stack(dropna=False).reset_index().rename(columns={0:f'cnt_{_id}', 'level_0':'d'})\n        data = pd.merge(data, f, on=['d', ID])\n        \n    data = fe_group(ID, data, log=False)\n    map_1 = ['qaurter','wday','month','year','new_event_name_1', 'new_event_type_1','event_name_1','event_type_1']\n    map_2 = ['event_name_1','event_type_1']\n    data = map_calendar_data(ID, data, m5_data, map_1, map_2)\n    data = make_snap_data(ID,data, m5_data)\n    \n    data[f'f_{ID}'] = pd.factorize(data[ID])[0]\n    data = data[data.d.isin(trn_d_cols[28:])]\n    return data\n\ndef make_data_2_id(ID, trn_d_cols, m5_data):\n    \n    data = m5_data.train.groupby(ID)[trn_d_cols].sum(min_count=1).T\n    data = data.stack(dropna=False).reset_index().rename(columns={0:'TARGET', 'level_0':'d'})\n    f = m5_data.price_data.replace(0,np.nan).groupby(ID)[\n        trn_d_cols\n    ].count().stack(dropna=False).reset_index().rename(columns={0:'cnt', 'level_1':'d', 'level_0':'d'})\n    data = pd.merge(data, f, on=['d', ID])\n    \n    data = fe_group(ID, data, log=True)\n    map_1 = ['qaurter','wday','month','year','new_event_name_1', 'new_event_type_1','event_name_1','event_type_1']\n    map_2 = ['event_name_1','event_type_1']\n    data = map_calendar_data(ID, data, m5_data, map_1, map_2)\n    data = make_snap_data(ID, data, m5_data)\n    \n    data[f'f_{ID}'] = pd.factorize(data[ID])[0]\n    data = data[data.d.isin(trn_d_cols[28:])]\n    return data\n\ndef make_data_item(ID, trn_d_cols, m5_data):\n    \n    data = m5_data.train.groupby(ID)[trn_d_cols].sum(min_count=1).T\n    data = data.stack(dropna=False).reset_index().rename(columns={0:'TARGET', 'level_0':'d'})\n    f = m5_data.price_data.replace(0,np.nan).groupby(ID)[\n        trn_d_cols\n    ].count().stack(dropna=False).reset_index().rename(columns={0:'cnt', 'level_1':'d', 'level_0':'d'})\n    data = pd.merge(data, f, on=['d', ID])\n    \n    data = fe_group(ID, data, log=False)\n    map_1 = ['qaurter','wday','new_event_name_1',\n             'new_event_type_1','event_name_1','event_type_1',\n             'snap_CA','snap_WI','snap_TX']\n    map_2 = ['event_name_1','event_type_1','snap_CA','snap_WI','snap_TX']\n    data = map_calendar_data(ID, data, m5_data, map_1, map_2)\n    \n    data[f'f_{ID}'] = pd.factorize(data[ID])[0]\n    data = data[data.d.isin(trn_d_cols[28:])]\n    return data","42b67c42":"def all_flow_dept_cat(m5_data, trn_d_cols, PARAMS):\n    Results={}\n    for ID in ['dept_id', 'cat_id']:\n        data = make_data_dept_cat(ID, trn_d_cols, m5_data)\n        sub =train_predict_group(data, ID, params=PARAMS[ID], n_split=5, log=True)\n        Results[ID] = sub\n    return Results\n\ndef all_flow_all_id(m5_data, trn_d_cols, PARAMS):\n    ID='all_id'\n    Results={}\n    data = make_data_all_id(ID, trn_d_cols, m5_data)\n    sub =train_predict_all(data, ID, params=PARAMS['all_id'], n_split=5, log=False)\n    Results['all_id'] = sub\n    return Results\n\ndef all_flow_2_id(m5_data, trn_d_cols, PARAMS):\n    Results = {}\n    for id_1 in ['dept_id', 'cat_id']:\n        for id_2 in ['store_id','state_id']:\n            m5_data.make_id(id_1, id_2)\n            ID = id_1+'X'+id_2\n            data = make_data_2_id(ID, trn_d_cols, m5_data)\n            sub =train_predict_group(data, ID, PARAMS[ID], n_split=5, log=True)\n            Results[ID] = sub\n    return Results\n\ndef all_flow_store_state(m5_data, trn_d_cols, PARAMS):\n    Results ={}\n    for ID in ['store_id', 'state_id']:\n        data = make_data_state_store(ID, trn_d_cols, m5_data)\n        sub =train_predict_group(data, ID, PARAMS[ID], n_split=5, log=False)\n        Results[ID] = sub\n    return Results\n\ndef all_flow_item(m5_data, trn_d_cols, PARAMS):\n    Results = {}\n    ID = 'item_id'\n    data = make_data_item(ID, trn_d_cols, m5_data)\n    sub =train_predict_item(data, ID, PARAMS[ID], n_split=5, log=False)\n    Results[ID] = sub\n    return Results","40f0544b":"def predict_cv_group(x_val, models, log):\n    preds = np.zeros(len(x_val))\n    for model in models:\n        pred = model.predict(x_val)\n        if log:\n            preds+=(np.e**(pred)-1)\/len(models)\n        else:\n            preds+=pred\/len(models)\n            \n    return preds\n\ndef train_predict_all(data,ID,params,n_split=5,log=False, cy=2):\n    days = data.d.unique().tolist()\n    days = sort_d_cols(days)\n    trn_days = days[:-28]\n    val_days = days[-28:]\n    data.reset_index(drop=True, inplace=True)\n\n    shift_cols = [col for col in data.columns if 'shift' in col]\n    roll_cols = [col for col in data.columns if 'roll' in col]\n    cat_cols = [col for col in data.columns if (not 'shift' in col) and (not 'roll' in col)]\n    cat_cols = [col for col in cat_cols if not col in [ID, 'd', 'TARGET']]\n    features=cat_cols+shift_cols+roll_cols\n    \n    for i in range(28):\n        if i%7==0:\n            data[shift_cols] = data.groupby(ID)[shift_cols].shift(7)\n        data[roll_cols] = data.groupby(ID)[roll_cols].shift(1)\n            \n        models=[]\n        X = data[data.d.isin(trn_days)][data.TARGET.notnull()]\n        \n        split = X[X.d.isin(trn_days[-500:])]['TARGET']\n        split = split.mean()-3.*split.std()\n        X = X[X.TARGET>split]\n        X.reset_index(drop=True, inplace=True)\n        groups =X['wday'].astype(str)\n        y = (500*minmax_scale(X['TARGET'])).astype(int)\n        k = StratifiedKFold(n_splits=n_split, shuffle=True, random_state=2020)\n        for _cy_ in range(cy):\n            params['random_state'] = 2020+_cy_\n            for trn, val in k.split(X,y=y, groups=groups):\n                train_set = lgb.Dataset(X.loc[:,features], X.loc[:,'TARGET'])\n                val_set = lgb.Dataset(X.loc[val,features], X.loc[val,'TARGET'])\n\n                model = lgb.train(train_set=train_set, valid_sets=[train_set, val_set], params=params, num_boost_round=3000, \n                                  early_stopping_rounds=100, verbose_eval=500)\n                models.append(model)\n                \n        val_day = val_days[i]\n        predict_data = data[data.d==val_day]\n        preds = predict_cv_group(predict_data[features], models, log)\n        \n        data.loc[data.d==val_day, 'TARGET'] = preds\n        \n    sub = data[data.d.isin(val_days)][[ID, 'd', 'TARGET']]\n    sub = sub.groupby(ID)[['d', 'TARGET']].apply(lambda x: x.set_index('d')['TARGET'].T)[val_days]\n    sub = sub.reset_index()\n    return sub\n\ndef train_predict_item(data,ID,params,n_split=5,log=False):\n    days = data.d.unique().tolist()\n    days = sort_d_cols(days)\n    trn_days = days[:-28]\n    val_days = days[-28:]\n    data.reset_index(drop=True, inplace=True)\n\n    shift_cols = [col for col in data.columns if 'shift' in col]\n    roll_cols = [col for col in data.columns if 'roll' in col]\n    cat_cols = [col for col in data.columns if (not 'shift' in col) and (not 'roll' in col)]\n    cat_cols = [col for col in cat_cols if not col in [ID, 'd', 'TARGET']]\n    features=cat_cols+shift_cols+roll_cols\n    \n    for i in range(28):\n        if i%7==0:\n            data[shift_cols] = data.groupby(ID)[shift_cols].shift(7)\n        data[roll_cols] = data.groupby(ID)[roll_cols].shift(1)\n            \n        models=[]\n        X = data[data.d.isin(trn_days)][data.TARGET.notnull()]\n        \n        split = X[X.d.isin(trn_days[-500:])]['TARGET']\n        split = split.mean()-3.*split.std()\n        X = X[X.TARGET>split]\n        X.reset_index(drop=True, inplace=True)\n        groups =X['wday'].astype(str)\n        y = (500*minmax_scale(X['TARGET'])).astype(int)\n        k = StratifiedKFold(n_splits=n_split, shuffle=True, random_state=2020)\n        c=0\n        params['random_state'] = 2020\n        for trn, val in k.split(X,y=y, groups=groups):\n            train_set = lgb.Dataset(X.loc[trn,features], X.loc[trn,'TARGET'])\n            val_set = lgb.Dataset(X.loc[val,features], X.loc[val,'TARGET'])\n            model = lgb.train(train_set=train_set, valid_sets=[train_set, val_set], params=params, num_boost_round=3000, \n                              early_stopping_rounds=100, verbose_eval=500)\n            models.append(model)\n            c+=1\n            if c==3:\n                break\n                \n        val_day = val_days[i]\n        predict_data = data[data.d==val_day]\n        preds = predict_cv_group(predict_data[features], models, log)\n        \n        data.loc[data.d==val_day, 'TARGET'] = preds\n        \n    sub = data[data.d.isin(val_days)][[ID, 'd', 'TARGET']]\n    sub = sub.groupby(ID)[['d', 'TARGET']].apply(lambda x: x.set_index('d')['TARGET'].T)[val_days]\n    sub = sub.reset_index()\n    return sub\n\ndef train_predict_group(data,ID,params,n_split=5,log=False, cy=3):\n    days = data.d.unique().tolist()\n    days = sort_d_cols(days)\n    trn_days = days[:-28]\n    val_days = days[-28:]\n    data.reset_index(drop=True, inplace=True)\n    \n    shift_cols = [col for col in data.columns if 'shift' in col]\n    roll_cols = [col for col in data.columns if 'roll' in col]\n    cat_cols = [col for col in data.columns if (not 'shift' in col) and (not 'roll' in col)]\n    cat_cols = [col for col in cat_cols if not col in [ID, 'd', 'TARGET']]\n    features=cat_cols+shift_cols+roll_cols\n    \n    for i in range(28):\n        if i%7==0:\n            data[shift_cols] = data.groupby(ID)[shift_cols].shift(7)\n        data[roll_cols] = data.groupby(ID)[roll_cols].shift(1)\n            \n        models=[]\n        X = data[data.d.isin(trn_days)][data.TARGET.notnull()]\n        \n        split = X[X.d.isin(trn_days[-500:])]['TARGET']\n        split = split.mean()-3.*split.std()\n        X = X[X.TARGET>split]\n        X.reset_index(drop=True, inplace=True)\n        groups =X['wday'].astype(str)\n        y = (500*minmax_scale(X['TARGET'])).astype(int)\n        k = StratifiedKFold(n_splits=n_split, shuffle=True, random_state=2020)\n        for _cy_ in range(cy):\n            params['random_state'] = 2020+_cy_\n            for trn, val in k.split(X,y=y, groups=groups):\n                train_set = lgb.Dataset(X.loc[trn,features], X.loc[trn,'TARGET'])\n                val_set = lgb.Dataset(X.loc[val,features], X.loc[val,'TARGET'])\n\n                model = lgb.train(train_set=train_set, valid_sets=[train_set, val_set], params=params, num_boost_round=3000, \n                                  early_stopping_rounds=100, verbose_eval=500)\n                models.append(model)\n                \n        val_day = val_days[i]\n        predict_data = data[data.d==val_day]\n        preds = predict_cv_group(predict_data[features], models, log)\n        \n        data.loc[data.d==val_day, 'TARGET'] = preds\n        \n    sub = data[data.d.isin(val_days)][[ID, 'd', 'TARGET']]\n    sub = sub.groupby(ID)[['d', 'TARGET']].apply(lambda x: x.set_index('d')['TARGET'].T)[val_days]\n    sub = sub.reset_index()\n    return sub","03468fa2":"PARAMS = {'boosting_type': 'gbdt', \n          'objective' : 'poisson',\n          #'objective' : 'tweedie', 'tweedie_variance_power': 1.141893486974509,\n          'random_state':2020,\n          \"metric\" :\"rmse\", \"force_row_wise\" : True, \"learning_rate\" : 0.075, \"sub_row\" : 0.75, \"bagging_freq\" : 1,\n          \"lambda_l2\" : 0.1, 'verbosity': 1, 'num_iterations' : 2500}\n\n# optimized using optuna\nPARAMS_GROUP={}\nPARAMS_GROUP['dept_id'] =  {'boosting_type': 'gbdt','objective': 'rmse','metric': 'rmse',\n                'max_bin': 100,'n_estimators': 1400,'boost_from_average': False,'verbose': -1,'random_state':2020,\n                'subsample': 0.8897026631967412, 'subsample_freq': 0.42708068942389565,\n                'learning_rate': 0.030223062885783494,'num_leaves': 66, 'feature_fraction': 0.4294129948533598,\n                'bagging_freq': 4, 'min_child_samples': 11, 'lambda_l2': 6.563593012634628}\nPARAMS_GROUP['cat_id'] = {'boosting_type': 'gbdt','objective': 'rmse','metric': 'rmse',\n              'max_bin': 100,'n_estimators': 1400,'boost_from_average': False,'verbose': -1,'random_state':2020,\n              'subsample': 0.8915155260035615, 'subsample_freq': 0.7106654494817621, 'learning_rate': 0.0439216532905,\n              'num_leaves': 24, 'feature_fraction': 0.48092257402284877, 'bagging_freq': 7, 'min_child_samples': 6,\n              'lambda_l2': 0.009774011952810685}\nPARAMS_GROUP['all_id'] =  {'boosting_type': 'gbdt','objective': 'rmse','metric': 'rmse','max_bin': 100,'n_estimators': 1400,\n                     'boost_from_average': False,'verbose': -1,'random_state':2020,\n                     'subsample': 0.7936430506570977, 'subsample_freq': 0.5206476443073623, \n                     'learning_rate': 0.09653801757509976,'num_leaves': 8, 'feature_fraction': 0.4986072155764939,\n                     'bagging_freq': 3, 'min_child_samples': 8,'lambda_l2': 1.319732403794593}\nPARAMS_GROUP['store_id'] = {'boosting_type': 'gbdt','objective': 'rmse','metric': 'rmse','max_bin': 100,'n_estimators': 1400,\n                      'boost_from_average': False,'verbose': -1,'random_state':2020,\n                      'subsample': 0.7936430506570977, 'subsample_freq': 0.5206476443073623, 'learning_rate': 0.09653801757509976,\n                      'num_leaves': 8, 'feature_fraction': 0.4986072155764939, 'bagging_freq': 3, 'min_child_samples': 8,\n                      'lambda_l2': 1.319732403794593}\nPARAMS_GROUP['state_id'] = {'boosting_type': 'gbdt','objective': 'rmse','metric': 'rmse','max_bin': 100,'n_estimators': 1400,\n                      'boost_from_average': False,'verbose': -1,'random_state':2020,\n                      'subsample': 0.7836421955591786, 'subsample_freq': 0.7516149374096728, 'learning_rate': 0.030576348046803408,\n                      'num_leaves': 11, 'feature_fraction': 0.4114638046420348, 'bagging_freq': 2, 'min_child_samples': 7, \n                      'lambda_l2': 0.13216719821842418}\nPARAMS_GROUP['cat_idXstore_id'] = {'boosting_type': 'gbdt','objective': 'rmse','metric': 'rmse','max_bin': 100,'n_estimators': 1400,\n                        'boost_from_average': False,'verbose': -1,'random_state':2020,\n                        'subsample': 0.8976728421649643, 'subsample_freq': 0.7133363789924351, 'learning_rate': 0.066033852,\n                        'num_leaves': 30, 'feature_fraction': 0.401368826100617, 'bagging_freq': 7, 'min_child_samples': 12,\n                        'lambda_l2': 0.0002899138139549539}\nPARAMS_GROUP['cat_idXstate_id'] = {'boosting_type': 'gbdt','objective': 'rmse','metric': 'rmse','max_bin': 100,'n_estimators': 1400,\n                        'boost_from_average': False,'verbose': -1,'random_state':2020,\n                        'subsample': 0.7304828007643646, 'subsample_freq': 0.7194392041924155, 'learning_rate': 0.065824,\n                        'num_leaves': 20, 'feature_fraction': 0.9953802109485864, 'bagging_freq': 1, 'min_child_samples': 6,\n                        'lambda_l2': 0.00026247413038444007}\nPARAMS_GROUP['dept_idXstore_id'] = {'boosting_type': 'gbdt','objective': 'rmse','metric': 'rmse','max_bin': 100,'n_estimators': 1400,\n                                  'boost_from_average': False,'verbose': -1,'random_state':2020,\n                                  'subsample': 0.767911185577838, 'subsample_freq': 0.7287605276865973, 'learning_rate': 0.041153778,\n                                  'num_leaves': 82,'feature_fraction': 0.5709048571103937, 'bagging_freq': 1, 'min_child_samples': 17,\n                                  'lambda_l2': 0.45656192806764545}\nPARAMS_GROUP['dept_idXstate_id'] = {'boosting_type': 'gbdt','objective': 'rmse','metric': 'rmse','max_bin': 100,'n_estimators': 1400,\n                                  'boost_from_average': False,'verbose': -1,'random_state':2020,\n                                  'subsample': 0.8901855563568182, 'subsample_freq': 0.5413800115150902, 'learning_rate': 0.0814993056342757,\n                                  'num_leaves': 32, 'feature_fraction': 0.5087751807175098, 'bagging_freq': 7, 'min_child_samples': 70,\n                                  'lambda_l2': 0.014539412980849776}\n\nPARAMS_GROUP['item_id'] = {'boosting_type': 'gbdt', 'objective': 'tweedie', 'metric': 'rmse', 'max_bin': 100, \n                     'n_estimators': 2000, 'boost_from_average': False, 'verbose': -1, 'random_state': 2020,\n                     'tweedie_variance_power': 1.141893486974509, 'subsample': 0.8710431222390667, \n                     'subsample_freq': 0.5692738176797527, 'learning_rate': 0.10957379305366494, 'num_leaves': 8,  \n                     'feature_fraction': 0.45380044045308154, 'bagging_freq': 4, 'min_child_samples': 5, \n                     'lambda_l1': 7.510525772813387e-06, 'lambda_l2': 4.1004528526443944e-07}","71e606d0":"def all_flow_all_preds(path, d_cols):\n    \n    data = preprocessing(path, d_cols)\n    data = str_category_2_int(data)\n    \n    use_days=data.d.unique().tolist()\n    use_days=sort_d_cols(use_days)[63:]\n    data = data[data.d.isin(use_days)]\n    gc.collect()\n    \n    mem = data.memory_usage().sum()\/1024**2\n    print(f\"\"\"\n    DATA SHAPE   {data.shape}\n    MEMORY USAGE   {mem:.2f}MB\n    DATA COLUMNS  {data.columns.tolist()}\n    \"\"\")\n    \n    gc.collect()\n    all_preds = train_predict_RE(data=data, PARAMS=PARAMS)\n    return all_preds\n\ndef all_flow_group(path, d_cols, PARAMS_GROUP):\n    m5_data = M5_Data(path, d_cols)\n    Sub={}\n    \n    trn_d_cols = d_cols[-200:]\n    rslt = all_flow_item(m5_data, trn_d_cols, PARAMS_GROUP)\n    Sub.update(rslt)\n    \n    trn_d_cols = d_cols[-730:]\n    rslt = all_flow_2_id(m5_data, trn_d_cols, PARAMS_GROUP)\n    Sub.update(rslt)\n    rslt = all_flow_dept_cat(m5_data, trn_d_cols, PARAMS_GROUP)\n    Sub.update(rslt)\n    rslt = all_flow_store_state(m5_data, trn_d_cols, PARAMS_GROUP)\n    Sub.update(rslt)\n    \n    \n    trn_d_cols = d_cols[-900:]\n    rslt = all_flow_all_id(m5_data, trn_d_cols, PARAMS_GROUP)\n    Sub.update(rslt)\n    \n    return Sub\n\ndef main():\n    \n    path = '..\/input\/m5-forecasting-accuracy\/'\n\n    d_cols=[f'd_{i+1}' for i in range(1969)]\n    \n    \n    group_preds = all_flow_group(path, d_cols, PARAMS_GROUP)\n    with open(f'group_preds.pickle', 'wb') as f:\n        pickle.dump(group_preds, f)\n    gc.collect()\n    \n    all_preds = all_flow_all_preds(path, d_cols)\n    gc.collect()\n    \n    cat=pd.read_csv(path+'sales_train_evaluation.csv').set_index('id')[['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']]\n    for key, value in cat.items():\n        all_preds[key] = all_preds['id'].map(value)\n    all_preds['all_id'] = 'all_id'\n    all_preds.to_csv('all_preds.csv', index=False)\n    \n    all_preds['TARGET_original'] = all_preds['TARGET']*all_preds['price']\n    all_preds['TARGET'] = all_preds['TARGET'].apply(lambda x: 0 if x<0.05 else x)\n    all_preds['TARGET']*=all_preds['price']\n    \n    for ID in ['dept_id', 'cat_id', 'store_id', 'state_id', 'all_id', 'item_id']:\n        data = group_preds[ID]\n        data = data.set_index(ID)\n        data = data.stack().reset_index().rename(columns={'level_1':'d', 0:f'{ID}_preds'})\n        all_preds = pd.merge(all_preds,data,on=['d', ID] )\n        if ID=='item_id':\n            all_preds[f'{ID}_preds'] = all_preds[f'{ID}_preds']*(all_preds['TARGET_original']\/all_preds.groupby(['d', ID])['TARGET_original'].transform('sum'))\n        else:\n            all_preds[f'{ID}_preds'] = all_preds[f'{ID}_preds']*(all_preds['TARGET']\/all_preds.groupby(['d', ID])['TARGET'].transform('sum'))\n\n    for id_1 in ['dept_id', 'cat_id']:\n        for id_2 in ['store_id', 'state_id']:\n            ID = f'{id_1}X{id_2}'\n            all_preds[ID] = all_preds[id_1].astype(str)+'X'+all_preds[id_2].astype(str)\n            data = group_preds[ID]\n            data = data.set_index(ID)\n            data = data.stack().reset_index().rename(columns={'level_1':'d', 0:f'{ID}_preds'})\n            all_preds = pd.merge(all_preds,data,on=['d', ID] )\n            all_preds[f'{ID}_preds'] = all_preds[f'{ID}_preds']*(all_preds['TARGET']\/all_preds.groupby(['d', ID])['TARGET'].transform('sum'))\n\n    p_1 = 0.35\/8\n    p_2 = 0.65\/2\n    all_preds['TARGET_2'] = p_2*all_preds['dept_idXstore_id_preds']+p_2*all_preds['cat_idXstore_id_preds']+\\\n                      p_1*all_preds['dept_id_preds']+p_1*all_preds['dept_idXstate_id_preds']+\\\n                      p_1*all_preds['store_id_preds']+p_1*all_preds['store_id_preds']+\\\n                      p_1*all_preds['cat_id_preds']+p_1*all_preds['all_id_preds']+p_1*all_preds['TARGET']+p_1*all_preds['item_id_preds']\n   \n    d_cols=sort_d_cols(all_preds.d.unique())\n    all_preds['TARGET_2'] = all_preds['TARGET_2']\/all_preds['price']\n    sub = all_preds.groupby(['id'])[['d', 'TARGET_2']].apply(lambda x: x.set_index('d')['TARGET_2'].T)\n    sub.columns=d_cols\n    \n    sample_sub=pd.read_csv(path+'sample_submission.csv')\n    sample_sub = sample_sub.set_index('id', drop=False)\n    sample_sub.loc[sub.index,[f'F{i}' for i in range(1,29)]] = sub[d_cols].values\n    sample_sub.reset_index(drop=True, inplace=True)\n    \n    sample_sub.to_csv('submission.csv', index=False)","d466cdd8":"main()","27f45247":"### Group train","4ba8bd36":"### FE","cd722663":"### PARAMS","b493de10":"### Library","3371cc6a":"### utils","d970d7b8":"### Preprocessing  Group","f4abbd48":"### FE Group","67fbf768":"### Preprocessing","ecee3184":"### Train utils","4f2a2943":"### flow for lavels"}}