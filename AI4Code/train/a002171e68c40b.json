{"cell_type":{"28cd8841":"code","8305baec":"code","f91ca861":"code","79b35f60":"code","34f48b3b":"code","59585ca2":"code","5636046f":"code","8064c35d":"code","229f6bdc":"code","96daeb77":"code","93273a47":"code","ab36f7c4":"code","9fbe1766":"code","9530b538":"code","4561fe01":"code","f1651217":"code","3624dd7a":"code","5e4afe95":"code","be5fadad":"code","59f8a68d":"code","4c21a0d6":"code","4e5526b6":"code","ea75d85d":"code","cfc52a82":"code","29bb8cbe":"code","e7a6818c":"code","344f161e":"code","664e61d9":"code","a7023d36":"markdown"},"source":{"28cd8841":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline","8305baec":"from fastai.tabular import *\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\nimport time\n\nimport lightgbm as lgb\nimport xgboost as xgb\n\nimport ast\n\nfrom sklearn.metrics import mean_squared_error\n\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.decomposition import TruncatedSVD","f91ca861":"PATH = \"..\/input\/tmdb-box-office-prediction\/\"","79b35f60":"train = pd.read_csv(f'{PATH}train.csv', parse_dates=['release_date'])\ntest = pd.read_csv(f'{PATH}test.csv', parse_dates=['release_date'])","34f48b3b":"train_votes = pd.read_csv('..\/input\/tmdb-prediction-votes\/trainRatingTotalVotes.csv')\ntest_votes = pd.read_csv('..\/input\/tmdb-prediction-votes\/testRatingTotalVotes.csv')\ntrain = pd.merge(train, train_votes, how='left', on=['imdb_id'])\ntest = pd.merge(test, test_votes, how='left', on=['imdb_id'])","59585ca2":"dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast']\n\ndef text_to_dict(df):\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n        \ntrain = text_to_dict(train)\ntest = text_to_dict(test)","5636046f":"def build_category_list(x, field, feature):\n    regex = re.compile('[^0-9a-zA-Z_]')\n    category_list = \"\"\n    \n    \n    for d in x:\n        new_category = regex.sub('', d[field].lower().replace(\" \",\"_\"))\n        \n        # Exception for cast: keep only 0 and 1 to limit nb of values\n        #        if feature == 'cast' and d['order'] > 1:\n        #            pass\n        #        else:\n        category_list += new_category + \",\"\n    return category_list.strip().strip(\",\").split(\",\")\n\n\ntarget_fields = {'belongs_to_collection': 'name', 'genres': 'name',\n                 'production_countries': 'iso_3166_1', 'production_companies': 'name',\n                 'spoken_languages': 'iso_639_1', 'Keywords': 'name', 'cast': 'name'\n                }\n\nfor k,v in target_fields.items():\n    train[k] = train[k].apply(lambda x: build_category_list(x, v, k))\n    test[k] = test[k].apply(lambda x: build_category_list(x, v, k))\n    \n    \ntarget_fields = {'cast':{'field':'name', 'role_field':'order', 'role_values':[0,1,2]}}","8064c35d":"train.loc[train['id'] == 16,'revenue'] = 192864          # Skinning\ntrain.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          \ntrain.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs\ntrain.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven\ntrain.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout \ntrain.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\ntrain.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood\ntrain.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\ntrain.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada\ntrain.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\ntrain.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\ntrain.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times\ntrain.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\ntrain.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \ntrain.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy \ntrain.loc[train['id'] == 1542,'budget'] = 1              # All at Once\ntrain.loc[train['id'] == 1542,'budget'] = 15800000       # Crocodile Dundee II\ntrain.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\ntrain.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit\ntrain.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon\ntrain.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\ntrain.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\ntrain.loc[train['id'] == 2491,'revenue'] = 6800000       # Never Talk to Strangers\ntrain.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\ntrain.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams\ntrain.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\ntrain.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture","229f6bdc":"test.loc[test['id'] == 3889,'budget'] = 15000000       # Colossal\ntest.loc[test['id'] == 6733,'budget'] = 5000000        # The Big Sick\ntest.loc[test['id'] == 3197,'budget'] = 8000000        # High-Rise\ntest.loc[test['id'] == 6683,'budget'] = 50000000       # The Pink Panther 2\ntest.loc[test['id'] == 5704,'budget'] = 4300000        # French Connection II\ntest.loc[test['id'] == 6109,'budget'] = 281756         # Dogtooth\ntest.loc[test['id'] == 7242,'budget'] = 10000000       # Addams Family Values\ntest.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\ntest.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage\ntest.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee","96daeb77":"power_six = train.id[train.budget > 1000][train.revenue < 100]\n\nfor k in power_six :\n    train.loc[train['id'] == k,'revenue'] =  train.loc[train['id'] == k,'revenue'] * 1000000","93273a47":"def multi_hot_encode(df, column_name, mlb=MultiLabelBinarizer()):\n    encoded = pd.DataFrame(mlb.fit_transform(df[column_name]))\n    encoded.columns = [f'{column_name}_{i}'.format(i) for i in mlb.classes_]\n    return mlb, encoded","ab36f7c4":"def conv_column_to_SVD(df, column_name, size=10, sparse_mlb=MultiLabelBinarizer(sparse_output=True)):\n    #sparse_mlb = MultiLabelBinarizer(sparse_output=True)\n    sparse_matrix = sparse_mlb.fit_transform(df[column_name])\n    sparse_SVD = TruncatedSVD(size)\n    sparse_TSVD = pd.DataFrame(sparse_SVD.fit_transform(sparse_matrix))\n    sparse_TSVD.columns = [f'{column_name}_{i}' for i in range(size)]\n    return sparse_mlb, sparse_TSVD","9fbe1766":"genre_mlb, genre_encoded = multi_hot_encode(train,'genres')\ncast_sparse_mlb, cast_TSVD = conv_column_to_SVD(train, 'cast', 10)\nkeywords_sparse_mlb, keywords_TSVD = conv_column_to_SVD(train, 'Keywords', 10)\nlanguages_mlb, languages_TSVD = conv_column_to_SVD(train, 'spoken_languages', 5)\nprodcomp_mlb, prodcomp_TSVD = conv_column_to_SVD(train, 'production_companies', 5)\nprodcountries_mlb, prodcountries_TSVD = conv_column_to_SVD(train, 'production_countries', 3)","9530b538":"_,test_genre= multi_hot_encode(test,'genres',genre_mlb)\n_,test_cast = conv_column_to_SVD(test, 'cast', 10)\n_,test_keywords = conv_column_to_SVD(test, 'Keywords', 10)\n_,test_languages = conv_column_to_SVD(test, 'spoken_languages', 5)\n_,test_prodcomp = conv_column_to_SVD(test, 'production_companies', 5)\n_,test_prodcountries = conv_column_to_SVD(test, 'production_countries', 3)","4561fe01":"train = train.join(genre_encoded)\ntrain = train.join(cast_TSVD)\ntrain = train.join(keywords_TSVD)\ntrain = train.join(languages_TSVD)\ntrain = train.join(prodcomp_TSVD)\ntrain = train.join(prodcountries_TSVD)","f1651217":"test = test.join(test_genre)\ntest = test.join(test_cast)\ntest = test.join(test_keywords)\ntest = test.join(test_languages)\ntest = test.join(test_prodcomp)\ntest = test.join(test_prodcountries)","3624dd7a":"add_datepart(train, 'release_date')\nadd_datepart(test, 'release_date')","5e4afe95":"cats_to_drop=['id','title','genres', 'cast', 'crew','Keywords','spoken_languages','production_companies','production_countries','homepage', 'belongs_to_collection','poster_path','imdb_id','original_language', 'original_title', 'overview','tagline','status']\ntrain = train.drop(cats_to_drop, axis=1)\ntrain = train.drop('genres_tv_movie', axis=1)\ntest = test.drop(cats_to_drop,axis=1)","be5fadad":"print(train.shape)\nprint(test.shape)","59f8a68d":"train[\"revenue\"]=np.log(train[\"revenue\"]).astype('float')\ntrain[\"budget\"]=np.log(train[\"budget\"]+0.1).astype('float')\ntest[\"budget\"]=np.log(test[\"budget\"]+0.1).astype('float')","4c21a0d6":"train.loc[train[\"runtime\"].isnull(),\"runtime\"]=train[\"runtime\"].mode()[0]\ntrain.loc[train[\"rating\"].isnull(),\"rating\"]=train[\"rating\"].mode()[0]\ntrain.loc[train[\"totalVotes\"].isnull(),\"totalVotes\"]=train[\"totalVotes\"].mode()[0]\n\ntest.loc[test[\"runtime\"].isnull(),\"runtime\"]=train[\"runtime\"].mode()[0]\ntest.loc[test[\"rating\"].isnull(),\"rating\"]=train[\"runtime\"].mode()[0]\ntest.loc[test[\"totalVotes\"].isnull(),\"totalVotes\"]=train[\"totalVotes\"].mode()[0]\n\ntest.loc[test[\"release_Year\"].isnull(),\"release_Year\"]=train[\"release_Year\"].mode()[0]\ntest.loc[test[\"release_Month\"].isnull(),\"release_Month\"]=train[\"release_Month\"].mode()[0]\ntest.loc[test[\"release_Week\"].isnull(),\"release_Week\"]=train[\"release_Week\"].mode()[0]\ntest.loc[test[\"release_Day\"].isnull(),\"release_Day\"]=train[\"release_Day\"].mode()[0]\ntest.loc[test[\"release_Dayofweek\"].isnull(),\"release_Dayofweek\"]=train[\"release_Dayofweek\"].mode()[0]\ntest.loc[test[\"release_Dayofyear\"].isnull(),\"release_Dayofyear\"]=train[\"release_Dayofyear\"].mode()[0]","4e5526b6":"X = train.drop(['revenue'], axis=1)\ny = train['revenue']\nX_test = test\nn_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\nrepeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)\n\n# scaler = StandardScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_test = scaler.transform(X_test)","ea75d85d":"def train_model(X, X_test, y, params, folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.loc[train_index], X.loc[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train)\n            valid_data = lgb.Dataset(X_valid, label=y_valid)\n            \n            model = lgb.train(params,\n                    train_data,\n                    num_boost_round=20000,\n                    valid_sets = [train_data, valid_data],\n                    verbose_eval=1000,\n                    early_stopping_rounds = 200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_train.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_train.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict_proba(X_valid).reshape(-1,)\n            score = mean_squared_error(y_valid, y_pred_valid)\n            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            # print('')\n            \n            y_pred = model.predict_proba(X_test)[:, 1]\n            \n        if model_type == 'glm':\n            model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n            model_results = model.fit()\n            model_results.predict(X_test)\n            y_pred_valid = model_results.predict(X_valid).reshape(-1,)\n            score = mean_squared_error(y_valid, y_pred_valid)\n            \n            y_pred = model_results.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000, learning_rate=0.1, loss_function='Logloss',  eval_metric='AUC', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test)\n            \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_squared_error(y_valid, y_pred_valid))\n\n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values  \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importance()\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction, scores\n    \n    else:\n        return oof, prediction, scores","cfc52a82":"params = {'num_leaves': 16,\n         'min_data_in_leaf': 2,\n         'objective': 'regression',\n         'max_depth': 20,\n         'learning_rate': 0.008,\n         'boosting': 'gbdt',\n         'bagging_freq': 5,\n         'feature_fraction': 0.82,\n         'bagging_seed': 11,\n         'reg_alpha': 1.7,\n         'reg_lambda': 6,\n         'random_state': 42,\n         'metric': 'mse',\n         'verbosity': -1,\n         'subsample': 0.81,\n         'min_gain_to_split': 0.01,\n         'min_child_weight': 10,\n         'num_threads': 8}\noof_lgb, prediction_lgb, scores = train_model(X, X_test, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)","29bb8cbe":"xgb_params = {'eta': 0.1, 'max_depth': 3, 'subsample': 0.9, 'colsample_bytree': 0.9, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True, 'nthread': 8}\noof_xgb, prediction_xgb, scores = train_model(X, X_test, y, params=xgb_params, folds=folds, model_type='xgb')","e7a6818c":"sns.distplot(prediction_lgb, hist=False) # blue\nsns.distplot(prediction_xgb, hist=False) # orange\nsns.distplot(train['revenue'], hist=False) #green","344f161e":"train['revenue'].mean()\/((prediction_lgb+prediction_xgb)\/2).mean()","664e61d9":"submission = pd.read_csv(f'{PATH}sample_submission.csv')\nsubmission['revenue'] = np.exp(0.50*(prediction_lgb+prediction_xgb))\nsubmission.to_csv(f'submission.csv', index=False)","a7023d36":"This notebook follows a similar approach, and steals with pride from the follow notebooks:\n* https:\/\/www.kaggle.com\/zero92\/stacking-xgb-lgbm-cat-authorized-variable. Thanks B H for the model set-up\n* https:\/\/www.kaggle.com\/shubhammank\/tmdb-eda. Thanks Shubham for the data cleaning functions\n\nThe real novel thing I'm doing is to  create multi-hot encodings for the genres, keywords, crew and cast using PCA (where number of unique categories is limited) or SVD with sparse matrices\n\nI hope this helps others improve their models. Please steal back with pride :)"}}