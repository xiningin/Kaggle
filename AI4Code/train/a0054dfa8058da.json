{"cell_type":{"c4fab3bc":"code","e05d9097":"code","9048177a":"code","db96f520":"code","711199d2":"code","aa389796":"code","4ab31e12":"code","555278ad":"code","4ff7f2ad":"code","2b9959c6":"code","d993bace":"code","63526368":"code","ad706d30":"code","e4b69f89":"code","e568372b":"code","81bfb54a":"code","26c86c2b":"code","90fff1c0":"code","b22859c5":"code","4b6138d3":"code","a9591666":"code","df7f028a":"code","126b164b":"markdown","980a487a":"markdown","8d36f93b":"markdown","8bbae6cc":"markdown","501515c3":"markdown","80f2e9ed":"markdown","bb346d3f":"markdown","247d5da9":"markdown","8872d9c1":"markdown","06e1e698":"markdown","19b52ce1":"markdown","80999aa2":"markdown","fd799274":"markdown","27477aa9":"markdown","744caa61":"markdown"},"source":{"c4fab3bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (20,3)\nimport statsmodels.api as sm\nimport seaborn as sns\nimport math\n%matplotlib inline\n\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.recurrent import LSTM\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tsa.ar_model import AR\nfrom sklearn.metrics import r2_score\n  \nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.layers import TimeDistributed\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom pandas.plotting import autocorrelation_plot\nfrom sklearn.preprocessing import MinMaxScaler\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.stattools import adfuller\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e05d9097":"data = pd.read_csv(\"..\/input\/hourly-energy-consumption\/PJMW_hourly.csv\")\ndata.head()","9048177a":"data.isna().sum()","db96f520":"data.dtypes","711199d2":"# Convert object date to datetime format\ndata['Datetime'] = pd.to_datetime(data['Datetime'])\ndata.dtypes","aa389796":"data.describe()","4ab31e12":"data.info()","555278ad":"# Viewing the spread of data using a histogram\n\ndata['PJMW_MW'].hist()","4ff7f2ad":"'1. Plotting the rolling statistics'\n\nplt.rcParams[\"figure.figsize\"] = (20,3)\ndata['PJMW_MW'].plot()","2b9959c6":"#Determing rolling statistics\nroll_mean=data.rolling(2, min_periods=1).sum()\nroll_std =data.rolling(2, min_periods=1).std()\n\n#Plot rolling statistics:\norig = plt.plot(data['PJMW_MW'], color='blue',label='Original')\nmean = plt.plot(roll_mean, color='red', label='Rolling Mean')\nstd = plt.plot(roll_std, color='black', label = 'Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')\nplt.show(block=False)","d993bace":"' 2. Dickey Fuller Test'\n\nprint ('Results of Dickey-Fuller Test:')\nadf_test = adfuller(data['PJMW_MW'],autolag='AIC')\ndfoutput = pd.Series(adf_test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n\nfor key,value in adf_test[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\nprint(dfoutput)","63526368":"pd.plotting.lag_plot(data['PJMW_MW'])","ad706d30":"' We use tra.diff()(differenced data), because this time series is unit root process.'\n\nfig,ax = plt.subplots(2,1,figsize=(20,10))\nfig = sm.graphics.tsa.plot_acf(data.iloc[:,1:2][0:int(0.8 * len(data))].diff().dropna(), lags=50, ax=ax[0])\nfig = sm.graphics.tsa.plot_pacf(data.iloc[:,1:2][0:int(0.8 * len(data))].diff().dropna(), lags=50, ax=ax[1])\nplt.show()","e4b69f89":"PJMW = data.iloc[:,1:2].values\n\ntrain_pct_index = int(0.8 * len(PJMW))\n\ntrain = PJMW[0:train_pct_index]\ntest =  PJMW[train_pct_index:]\n\nprint(train.shape)\nprint(test.shape)","e568372b":"#Scaling the values between 0 to 1\nfrom sklearn.preprocessing import MinMaxScaler\n\nss= MinMaxScaler(feature_range=(0,1))\ntrain= ss.fit_transform(train)\ntest= ss.fit_transform(test)","81bfb54a":"def create_dataset(dataset, look_back=10):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)\n\n\nlook_back=2\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)","26c86c2b":"# Create lagged dataset\nvalues = pd.DataFrame(data['PJMW_MW'].values)\ndataframe = pd.concat([values.shift(1), values], axis=1)\ndataframe.columns = ['t-1', 't+1']\nprint(dataframe.head(5))","90fff1c0":"# split into train and test sets\nX = dataframe.values\ntrain_size = int(len(X) * 0.80)\ntrain_persistence, test_persistence = X[1:train_size], X[train_size:]\ntrain_X, train_y = train_persistence[:,0], train_persistence[:,1]\ntest_X, test_y = test_persistence[:,0], test_persistence[:,1]","b22859c5":"# persistence model\ndef model_persistence(x):\n    return x\n \n# walk-forward validation\npredictions = list()\n\nfor x in testX:\n    yhat = model_persistence(x)\n    predictions.append(yhat[0])\n\ntest_score = mean_squared_error(testY, predictions)\nprint('Test MSE: %.3f' % test_score)\n\n# plot predictions vs expected\nplt.rcParams[\"figure.figsize\"] = (20,3)\nplt.plot(testY,color='blue')\nplt.plot(predictions, color='red')\nplt.show()","4b6138d3":"def autoregressor_model(train,test):\n    \n  # active_train & active_test\n\n  #train the autoregression model\n  model = AR(train)\n  model_fitted = model.fit()\n\n  print('The lag value chose is: %s' % model_fitted.k_ar)\n  \n  # line plot of residuals\n  residuals = pd.DataFrame(model_fitted.resid)\n  residuals.plot()\n  plt.show()\n  # density plot of residuals\n  residuals.plot(kind='kde')\n  plt.show()\n  # summary stats of residuals\n  print(residuals.describe())\n\n  \n  # make predictions \n  predictions = model_fitted.predict(start=len(train), end=len(train) + len(test)-1, dynamic=False)\n\n  # compute accuracy of model\n  \n  print( '     ')\n  print('MSE and RMSE values')\n  MSE = np.square(np.subtract(test,predictions)).mean() \n  print(MSE)\n  RMSE = math.sqrt(MSE)\n  print(RMSE)\n\nautoregressor_model(train,test)","a9591666":"def neural_network(trainX,trainY,testX,testY):\n  # reshape input to be [samples, time steps, features]\n  trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1],1))\n  testX = np.reshape(testX, (testX.shape[0], testX.shape[1],1))\n\n  # create and fit the LSTM network\n  model_hour = Sequential()\n  #Adding the first LSTM layer and some Dropout regularisation\n  model_hour.add(LSTM(units = 100, return_sequences = True, input_shape = (trainX.shape[1], 1)))\n  model_hour.add(Dropout(0.2))\n  # Adding a second LSTM layer and some Dropout regularisation\n  model_hour.add(LSTM(units = 100, return_sequences = True))\n  model_hour.add(Dropout(0.2))\n  # Adding a third LSTM layer and some Dropout regularisation\n  model_hour.add(LSTM(units = 100, return_sequences = True))\n  model_hour.add(Dropout(0.2))\n  # Adding a fourth LSTM layer and some Dropout regularisation\n  model_hour.add(LSTM(units = 50))\n  model_hour.add(Dropout(0.2))\n  # Adding the output layer\n  model_hour.add(Dense(units = 1))\n\n  # Compiling the RNN\n  model_hour.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])\n\n  # Fitting the RNN to the Training set\n  model_hour.fit(trainX, trainY, epochs = 100, batch_size = 1000)\n\n  # PREDICTION\n  trainPredict = model_hour.predict(trainX)\n  testPredict = model_hour.predict(testX)\n  # invert predictions\n  trainPredict = ss.inverse_transform(trainPredict)\n  trainY = ss.inverse_transform([trainY])\n  testPredict = ss.inverse_transform(testPredict)\n  testY = ss.inverse_transform([testY])\n  \n  return testY,testPredict\n\nActual,Predicted = neural_network(trainX,trainY,testX,testY)","df7f028a":"def plot_prediction(test,Predict):\n    # plotting actual vs predicted\n    plt.figure(figsize=(20,10))\n    plt.plot(test.transpose(), color = 'black', label = 'Actual Power Consumption')\n    plt.plot(Predict, color = 'green', label = 'Predicted Power Consumption')\n    plt.title('Power Consumed')\n    plt.xlabel('Time')\n    plt.ylabel('Power')\n    plt.legend()\n    plt.show()\n    \nplot_prediction(Actual,Predicted)","126b164b":"**PERSISTENCE MODEL**\n\nThe equivalent technique for use with time series dataset is the persistence algorithm. The persistence algorithm uses the value at the previous time step (t-1) to predict the expected outcome at the next time step (t+1).","980a487a":"**LAG PLOT**\n\nA lag plot is used to help evaluate whether the values in a dataset or time series are random. If the data are random, the lag plot will exhibit no identifiable pattern. If the data are not random, the lag plot will demonstrate a clearly identifiable pattern","8d36f93b":"**Our p-value is definitely less than 0.5 and so we can say with pretty good confidence that we can reject the null (unit root, non-stationary data) and can assume our data is stationary.**\n\n**Additionally, our ADF is much less than our 1% confidence value of -3.43, so we have another confirmation that we can reject the null.**","8bbae6cc":"**As we can see that the MSE and RMSE value are less than 1 and as we know the closer the value of RMSE is to zero , the better is the Regression Model.** \n\n**Hence the AR model also gave a good output.**","501515c3":"Here we can see that after applying the persistence model we get 2 outcomes\n\n**1.MSE = 0.005 ,which means our prediction was almost accurate**\n\n**2.The test and prediction plot overlap each other further prooving our prediction to be nearly same.**","80f2e9ed":"We will have a look at the PJMW_hourly data and predict the energy consumption for a day","bb346d3f":"We can very clearly see that our data has an identifiable pattern which is an increasing one. In addition to we are also able to identify some outliers very clearly.","247d5da9":"**STATIONARITY**\n\nThere are 2 ways to check stationarity of the data\n\n**1. Plotting the rolling statistics**\n\n**2. Dickey Fuller Test**\n\n**3. ADF test**","8872d9c1":"**CREATING MODEL**","06e1e698":"**We see that the Date coulumn of our data is a object datatype ,thus we need to convert it into Date time format. This can be done using *to_datetime()***","19b52ce1":"**LSTM (Long Short Term Memory)**","80999aa2":"**AR Model(Auto Regressor)**","fd799274":"Since we are dealing with a time series data. We need to check for following factors in the data\n\n**(a) Stationarity - To check whether our data has a constant mean,constant variance and autocovariance does not depend on time.**\n\n**(b) Autocorrelation - It refers to the degree of similarity between a given time series and lagged version of itself.** ","27477aa9":"Looking at the graph we cannot any increase in either mean or standard deviation of the data. However,to be very clear about the data we need to use the **Dickey Fuller test** ","744caa61":"**Exploratory Data Analysis**"}}