{"cell_type":{"5ae27721":"code","24269cdc":"code","ac03934d":"code","ab557ea0":"code","117c4dc2":"code","d2e09a46":"code","6b972584":"code","4cdabe3c":"code","19c306ba":"code","b285d176":"code","6e90a2f5":"code","ff699ace":"markdown","4fdc84a9":"markdown","684f987f":"markdown","2de08b0b":"markdown","4c05fd88":"markdown","4f5263f5":"markdown","cc99b572":"markdown","92ffb69b":"markdown","a095e9b4":"markdown","b0ec4293":"markdown","dda75e0f":"markdown","eb309b93":"markdown"},"source":{"5ae27721":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","24269cdc":"import numpy as np\nimport random\n\n# Each move is evaluated as to how good it was given the current grid\nclass Game:\n\n    def __init__(self,grid):\n        self.training_history = []\n        self.grid = grid\n\n    def check_win(self, grid):\n        # Finds the positions of the Xs or Os in the grid\n        resX = [np.where(grid == -1), np.where(np.transpose(grid) == -1)]\n        resO = [np.where(grid == 1), np.where(np.transpose(grid) == 1)]\n\n        # Gets the coordinates of the places occupied by X or O\n        zipX=list(zip(np.where(grid == -1)[0],np.where(grid == -1)[1]))\n        zipO=list(zip(np.where(grid == 1)[0],np.where(grid == 1)[1]))\n\n        # Check if the X positions result in a win\n        if self._check_row_col(resX):\n            return \"X WIN\"\n        # Check if the O positions result in a win\n        elif self._check_row_col(resO):\n            return \"O WIN\"\n        # Check the diagonals\n        elif self._check_diagonals(zipX):\n            return \"X WIN\"\n        elif self._check_diagonals(zipO):\n            return \"O WIN\"\n        # Check if the board is full\n        elif np.where(grid == 0)[0].size == 0:\n            # If there are no lines of 3 for XorO and the grid contains no more empty spaces\n            return \"DRAW\"\n\n\n    # Checks that the positions give a win irrespective of whether it is Os or Xs\n    def _check_row_col(self, res):\n        for g in res:\n            if any(sublist in np.array_str(np.array(g[0])) for sublist in ('0 0 0', '1 1 1','2 2 2')):\n                if '0 1 2' in np.array_str(np.array(g[1])):\n                    return True\n\n    def _check_diagonals(self, res):\n        # the diagonals\n        if (0,0) in res and (1,1) in res and (2,2) in res:\n            return True\n        if (0,2) in res and (1,1) in res and (2,0) in res:\n            return True\n\n    # This gets the current grid\n    def get_grid(self):\n        return self.grid\n\n    def make_move(self,pos,XorO):\n        if self.grid[pos] == 0:\n            self.grid[pos] = XorO\n            return True\n        else:\n            return False","ac03934d":"from keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nimport numpy as np\n\n\nclass TicTacToeModel:\n\n    def __init__(self, numberOfInputs, numberOfOutputs, epochs, batchSize):\n        self.epochs = epochs\n\n        self.batchSize = batchSize\n        self.numberOfInputs = numberOfInputs\n        self.numberOfOutputs = numberOfOutputs\n        self.model = Sequential()\n        self.model.add(Dense(64, activation='relu', input_shape=(numberOfInputs, )))\n        self.model.add(Dense(128, activation='relu'))\n        self.model.add(Dense(128, activation='relu'))\n        self.model.add(Dense(128, activation='relu'))\n        self.model.add(Dense(numberOfOutputs, activation='softmax'))\n        self.model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n\n    def train(self, dataset):\n        input = []\n        output = []\n        for data in dataset:\n            input.append(data[1])\n            output.append(data[0])\n\n        X = np.array(input).reshape((-1, self.numberOfInputs))\n        y =to_categorical(output, num_classes=3)\n        \n        # Train and test data split this gives 80%\n        boundary = int(0.8 * len(X))\n        X_train = X[:boundary]\n        X_test = X[boundary:]\n        y_train = y[:boundary]\n        y_test = y[boundary:]\n        self.model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=self.epochs, batch_size=self.batchSize)\n        return self.model\n\n\n    # This is to use a new model that has been loaded\n    def set_model(self, model):\n        self.model = model\n\n    def predict(self, data, index):\n        return self.model.predict(np.array(data).reshape(-1, self.numberOfInputs))[0][index]","ab557ea0":"import random\nimport copy\nimport numpy as np\n\n\nclass Agent:\n\n    def __init__(self):\n        pass\n\n    # This returns the location of the position on the grid to place the X or O\n    def set_location(self,grid,method, model):\n        # Find available positions\n        # select a location to place the xoro\n        if method == \"random\":\n            select = _get_random_position(grid)\n        if method == \"neural\":\n            select = _get_neural_position(grid, model)\n        return select\n\n# Static private methods\n# Returns the next position of the xoro randomly from the available positions\ndef _get_random_position(grid):\n    # Find positions the contain a 0 (blank)\n    available = _get_available_positions(grid)\n    return random.choice(available)\n\n# This is where the Neural network goes.\ndef _get_neural_position(grid, model):\n    availableMoves = _get_available_positions(grid)\n    maxValue = 0\n    bestMove = availableMoves[0]\n    for availableMove in availableMoves:\n        # get a copy of a board\n        boardCopy = copy.deepcopy(grid)\n        value = model.predict(boardCopy, 0)\n        if value > maxValue:\n            maxValue = value\n            bestMove = availableMove\n    selectedMove = bestMove\n    return selectedMove\n\ndef _get_available_positions(grid):\n    a = np.where(grid == 0)\n    return list(zip(a[0], a[1]))","117c4dc2":"import copy\nimport numpy as np\nimport tensorflow as tf\n\nhistory = []\n\n# Main game loop\ndef run_game(player1, player2, loaded_model, iterations, print_grid, show_result):\n    grid = np.full((3, 3), 0)\n\n    # create the game\n    g = Game(grid)\n    agent1 = Agent()\n    agent2 = Agent()\n\n    score = 0\n    moves = []\n    output = 0\n    # iterations = 1000\n    it = 0\n    X_wins = 0\n    O_wins = 0\n    Draws = 0\n    while it < iterations:\n        # The moves for each game\n        for i in [\"X\", \"O\"]:\n            while True:\n                if i == \"X\":\n                    loc = agent1.set_location(grid,player1,loaded_model)\n                    # Make sure the move is to a blank space before exiting the loop\n                    if g.make_move(loc,-1):\n                        break\n                if i == \"O\":\n                    loc = agent2.set_location(grid,player2,loaded_model)\n                    # Make sure the move is to a blank space before exiting the loop\n                    if g.make_move(loc,1):\n                        break\n            if print_grid:\n                print(grid)\n            res = g.check_win(grid)\n            last_state = grid.tolist()\n            moves.append(last_state)\n\n            # Goes here if there is a result\n            if res:\n                if show_result:\n                    print(res)\n                # X wins\n                if res[:1] == 'X':\n                    X_wins+=1\n                    grid = np.full((3, 3), 0)\n                    g = Game(grid)\n                    output=-1\n                # O wins\n                elif res[:1] == 'O':\n                    O_wins+=1\n                    grid = np.full((3, 3), 0)\n                    g = Game(grid)\n                    output=1\n                # Draw\n                else:\n                    Draws+=1\n                    grid = np.full((3, 3), 0)\n                    g = Game(grid)\n                    output=0\n                it += 1\n                # If the game is won by less than nine moves then append the last board state to make the array 9 long.\n                # Not sure how to make Keras deal with uneven data sizes\n                while len(moves) < 9:\n                    moves.append(last_state)\n                for m in moves:\n                    history.append((output,copy.deepcopy(m)))\n                moves = []\n                break\n    return history,X_wins, O_wins, Draws","d2e09a46":"# Create some random training data\nprint(\"--- Summary ---\")\nhistory,x,o,d=run_game(player1=\"random\", player2=\"random\", loaded_model=None, iterations=100, print_grid=False, show_result=False)\n# Get the results for these games to show later\nxw=str(x)\now=str(o)\ndd=str(d)\n# print out the results\nprint(\"Before training (Random vs Random)\")\nprint(\"X Wins = \"+xw)\nprint(\"O Wins = \"+ow)\nprint(\"Draws = \"+dd)","6b972584":"# Read in the raw data from the file\ndataset = pd.read_csv('\/kaggle\/input\/tic-tac-toe-endgame-dataset\/tic-tac-toe.data')\ndataset.head(5)\n","4cdabe3c":"# Extract this data into two datasets. Map Ordinal Values To Integers for ML\nX = dataset.iloc[:, 0:9]\nyd = dataset.iloc[:,9:10]\n\n# transform the 'x' into -1, 'o' into 1 and the 'b' into 0\nplayer_dict = {\n    'x':-1,\n    'o':1,\n    'b':0\n}\nX = X.replace(player_dict)\n\n# transform the results for an X win.\noutcome_dict= {\n    'positive':-1,\n    'negative':1\n}\nyd = yd.replace(outcome_dict)\n\nprint(X.head(5))\nprint(yd.head(5))","19c306ba":"# Convert the data into numpy arrays for reshaping.\n# Xr = an array to hold the 1D results [-1,-1,-1,-1,1,1,-1,1,-1] to a 3D array [[-1,-1,-1],[-1,1,-1],[-1,1,-1]]\nXr=[]\nfor a in X.values:\n    Xr.append(a.reshape(-1,3))\n    \nhistory=[]\n\n# append the game outcome to the array of endstate game positions, converting to python lists\nfor x in Xr:\n    for y in yd.values:\n        history.append((y[0],x.tolist()))\n\n# show the first 5 rows of the transformed data\nhistory[:5]","b285d176":"# Train the network using the results from the random games\n#### CAUTION: If you are using the tic-tac-toe predefined training data then make the number of epochs very small, e.g. 1-10 otherwise it'll take ages.\n# NOTE: epochs are the number fo times it passses through the data.\nticTacToeModel = TicTacToeModel(9, 3, 1, 32)\n# For much smaller amounts of randomly generated data use the following, with 100 epochs\n#ticTacToeModel = TicTacToeModel(9, 3, 100, 32)\nmodel = ticTacToeModel.train(history)\n\nprint(\"Before training (Random vs Random)\")\nprint(\"X Wins = \"+xw)\nprint(\"O Wins = \"+ow)\nprint(\"Draws = \"+dd)","6e90a2f5":"# Use the model - neural network vs random player\n\nticTacToeModel.set_model(model)\n_, x, o, d = run_game(player1=\"neural\", player2=\"random\", loaded_model=ticTacToeModel, iterations=100, print_grid=False, show_result=False)\n\nprint(\"After Learning (Neural = X vs Random = O):\")\nprint(\"X Wins = \"+str(x))\nprint(\"O Wins = \"+str(o))\nprint(\"Draws = \"+str(d))","ff699ace":"### Data Cleanup\nAny machine learning requires the data to be in a consistent format. This is so the analysis that is applied to the fields is done so correctly. Blank fields have to be processed, either by removiong the whole , as this blank field invalidates it, or filling in a value for that blank field.\nFor the data that is randomly generated it is easier to fill in the blank data as the data is generated. \n\nThe code from the run_game method, when it is returning all the games board states that make up the 9 possible borad states, adds the last move made if the game is won in less than 9 moves:\n\n    while len(moves) < 9:\n    moves.append(last_state)\n    for m in moves:\n        history.append((output,copy.deepcopy(m)))\n        \nSo if the game was:\n\n    (-1,[[0,0,0],[0,0,-1],[0,0,0]]),\n    (-1,[[1,0,0],[0,0,-1],[0,0,0]]),\n    (-1,[[1,0,-1],[0,0,-1],[0,0,0]]),\n    (1,[[1,0,-1],[0,0,-1],[1,0,0]]),\n    (-1,[[1,0,-1],[0,0,-1],[1,0,-1]])\n    \nThe game being won by X in 5\/9 moves.\n\nThe last 4 elements of this array would be the same as the final game state:\n\n    (-1,[[0,0,0],[0,0,-1],[0,0,0]]),\n    (-1,[[1,0,0],[0,0,-1],[0,0,0]]),\n    (-1,[[1,0,-1],[0,0,-1],[0,0,0]]),\n    (1,[[1,0,-1],[0,0,-1],[1,0,0]]),\n    (-1,[[1,0,-1],[0,0,-1],[1,0,-1]]),\n    (-1,[[1,0,-1],[0,0,-1],[1,0,-1]]),\n    (-1,[[1,0,-1],[0,0,-1],[1,0,-1]]),\n    (-1,[[1,0,-1],[0,0,-1],[1,0,-1]]),\n    (-1,[[1,0,-1],[0,0,-1],[1,0,-1]])\n    \nAnother option would be to remove these from the dataset. It has yet to be researched as to how this affects the outcome.\n\nAs for the predefined training data,this data is in the form \n\n    x,x,x,x,o,o,x,o,o,positive\nwhich is the first line of the data file, indicating a x win.\n\nThis data needs to form the input into the training model,which needs to be in the same format as the data above to use the same trainign model. Therefore the above example data needs to transformed into the format [(-1,[[-1,-1,-1],[-1,1,1],[-1,1,1]]),(res,[[1,2,3],[4,5,6],[7,8,9]]).....] where res is the player that won, either -1 or 1, X=-1, O=1 and a draw=0 and the numbers 1-9 are the positions on the board being occupied by -1, 1 or O (O corresponds to 'b' for blank in the raw dataset).\n\nThis can be achieved using the pandas functios - see code.\n","4fdc84a9":"## Model.py\nThis is the Keras model of the games, it creates 4 layers, this can be played around with to explore performance etc.\nThe train method takes a dataset from previously played games and uses Keras to cretae a network of the moves it's found.\nPredict uses the trained model to predict the best move for the game.\n\nnumberOfInputs = 9, positions on the grid\n\nnumberOfOutputs = 3, win, lose, draw\n\nThe number of epochs is the number of complete passes through the training dataset. \n\nbatchSize = number of samples processed before the model is updated.\n\n\nIn general: Larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster.\n\nIn general, the models improve with more epochs of training, to a point. They'll start to plateau in accuracy as they converge. ","684f987f":"### Randomly generated data\nFirst 100 games are played randomly, the dataset (history) produced from this is fed into the training function to train the network how to win. The neural network model returned is then used to play the game again to see if the X-wins are imporoved.","2de08b0b":"## Training Data\n### Either generate some random data or use predefined data\nIf you run this whole notebook you'll use the predefined data, you'll have to return and selectively run the random data to use that instead.\n\n\n\n","4c05fd88":"# Keras version of Tic Tac Toe\n## Intro\nAim of this project is to get machine learning , e.g. Tensorflow to play OXO (Noughts and Crosses)(Tic Tac Toe).\n\nThe idea of this is that training data is passed to a neural network, so the network can analyse it and work out what combinations of moves resulted in a win. These values are then used to apply a probability to the chance of a successful move being made give the current board state.\n\nThe training data that is used is the final board state and the result of 1000 random games. The output of the analysis is a neural network that can be fed into subsequent games.\n\nFor demonstration purposes the games used are the network against a random player.\n","4f5263f5":"### Predefined Training Data\nI got this data from here [https:\/\/www.kaggle.com\/aungpyaeap\/tictactoe-endgame-dataset-uci](https:\/\/www.kaggle.com\/aungpyaeap\/tictactoe-endgame-dataset-uci) There is now no dependancy on the data produced by the random games. This dataset contains all of the winning combinations of the board positions, that is 958 tic-tac-toe endgame boards.\n\n\n","cc99b572":"## Conclusion and Observations\nThe performance definately improves after the training. There are many tweeks that can be made that may improve this result. Firstly more training data can be used, for brevity this example only uses 100 random games with their results as the input into the training. The testing of the neural network only needs a handful (100) games to see improvements. \n\nThere is also a difference between the randomly created data and the predefined data. The randomly generated data contains all (up to 9) board states with the outcome of that game. The predefined data only contains the final board state with each outcome.\n\ne.g. Randomly generated:\n\n    (-1,[[0,0,0],[0,0,-1],[0,0,0]]),\n    (-1,[[1,0,0],[0,0,-1],[0,0,0]]),\n    (-1,[[1,0,-1],[0,0,-1],[0,0,0]]),\n    (1,[[1,0,-1],[0,0,-1],[1,0,0]]),\n    (-1,[[1,0,-1],[0,0,-1],[1,0,-1]])\n\nThis shows that the final outcome was a -1 (X) win, which is associated with each move of the game.\n\nPredefined:\n\n    [o,b,x,b,b,x,o,b,x,positive]\n    \nThis only records the outcome of the game against final board states.\n\n\n### To do\n* Add a new random game generator that only outputs the end board state with the outcome, not each move.\n* Change the parameters into the model generation to alter the layers.\n* Change the number of layers.\n* Plot graphs of performance using various parameters and volumes of training data.","92ffb69b":"## Use the model for game, random player vs neural network","a095e9b4":"## Train\nAnother experiment here could be to use the output from the game played with the neural network data as the input to the training model. See Model.py above for TicTacToeModel parameter details.","b0ec4293":"## Main Game\nThis runs the game for 2 players, X always goes 1st. It will run the game for 'iterations' either randomly to create training data or using the model that has been created by training. It returns the history of all the moves made during the game so that these can be fed into the training model and the results for X win, X loose, or a draw.\nIf a model is passed to it as an argument then it is used, otherwise the moves are made randomly.","dda75e0f":"## Game.py\nThe first part that I created was the game engine itself, this is fairly straightforward and takes a location of a move and places a X or an O there, then checks to see if anyone has won.","eb309b93":"## Agent\nThis is the the player that is going to make a move on the OXO board from the available moves either at random or from the model."}}