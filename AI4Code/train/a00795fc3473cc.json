{"cell_type":{"1e804a7a":"code","c5bf9410":"code","51344833":"code","dcf3bb6a":"code","d04ad93e":"code","0a122e65":"code","10289854":"code","649ab220":"code","7972744b":"code","c40bc343":"code","70856c4f":"code","e7385328":"code","9fdcb513":"code","abac169b":"code","19c6ff49":"code","4ecdfef3":"code","7711d186":"code","89036d59":"code","19583b84":"code","e4099b37":"code","677b64b5":"code","d7ceb46e":"code","7accd7ce":"code","a41cde34":"code","19f444ec":"code","9731b63c":"code","f3139b57":"code","78d61689":"code","793af17a":"code","ccb5a0d6":"code","54e72834":"code","680a5f7c":"markdown","a0a674f7":"markdown","a5674c0c":"markdown","1ef2c435":"markdown","dd9bb770":"markdown","97822313":"markdown","dad5f91c":"markdown","be8e5137":"markdown","2c6fa83d":"markdown","ac795aff":"markdown","90e59420":"markdown","9b81e8d2":"markdown","d91b559e":"markdown","a7739686":"markdown"},"source":{"1e804a7a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c5bf9410":"# let's start on mercedes car\ncclass = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/cclass.csv')\nfocus = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/focus.csv')\naudi = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/audi.csv')\ntoyota = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/toyota.csv')\nskoda = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/skoda.csv')\nford = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/ford.csv')\nvauxhall = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vauxhall.csv')\nbmw = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/bmw.csv')\nvw = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vw.csv')\nhyundai = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/hyundi.csv')\nmerc = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/merc.csv')\ndata = audi.copy()\ndata.head()","51344833":"data.info()","dcf3bb6a":"# check price distribution\nsns.histplot(data['price'], bins=30)","d04ad93e":"def plot_numerical(feature):\n    sns.lmplot(x=feature, y='price', data=data)\n    plt.show()\n    \ndef plot_categorical(feature, figsize=None):\n    df = data.groupby([feature])['price'].describe()[['mean', '50%', 'min', 'count']]\n\n    labels = df.index.values\n    x = np.arange(len(labels))\n    width = 0.9\n    fig, ax1 = plt.subplots(figsize=(12, 5))\n\n    # plot bars for min, median and mean house price\n    rects1 = ax1.bar(x-width\/2, df['50%'], width\/3, label='median')\n    rects2 = ax1.bar(x-width\/6, df['mean'], width\/3, label='mean')\n    rects3 = ax1.bar(x+width\/6, df['min'], width\/3, label='min')\n\n    ax1.set_ylabel('price', fontsize=15)\n    ax1.set_title(feature, fontsize=18)\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(labels, rotation=0)\n    ax1.legend()\n\n    # plot counts of data points\n    ax2 = ax1.twinx()\n    ax2.set_ylabel('Counts', fontsize=15)\n    ax2.plot(x-width\/2, df['count'], color='red', linestyle='dashed')\n\n    # annotate counts of data points\n    for i, rect in enumerate(rects2):\n        height = int(round(rect.get_height()))\n        ax1.annotate('{}'.format(int(df['count'].iloc[i])),\n                     xy=(rect.get_x() + rect.get_width()\/2, height),\n                     xytext=(0, 3), textcoords=\"offset points\",\n                     ha='center', va='bottom', color='red')\n    plt.show()","0a122e65":"for feature in ['model', 'transmission', 'fuelType']:\n    plot_categorical(feature)","10289854":"for feature in ['year', 'mileage', 'tax', 'mpg', 'engineSize']:\n    plot_numerical(feature)","649ab220":"data.columns","7972744b":"categorical_features = ['model', 'transmission', 'fuelType']\nnumerical_features = ['year', 'mileage', 'tax', 'mpg', 'engineSize']","c40bc343":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport joblib","70856c4f":"df = data.copy()\npath = '\/kaggle\/working'\nfor i, feature in enumerate(categorical_features):\n    le = LabelEncoder()\n\n    # create directory to save label encoding models\n    if not os.path.exists(os.path.join(path, \"TextEncoding\")):\n        os.makedirs(os.path.join(path, \"TextEncoding\"))\n\n    # perform label encoding\n    le.fit(df[feature])\n    #print(feature)\n    \n    # save the encoder\n    joblib.dump(le, open(os.path.join(path, \"TextEncoding\/le_{}.sav\".format(feature)), 'wb'))\n    \n    # transfrom training data\n    df[feature] = le.transform(df[feature])\n\n    # get classes & remove first column to elude from dummy variable trap\n    columns = list(map(lambda x: feature+' '+str(x), list(le.classes_)))[1:]\n    \n    # save classes\n    joblib.dump(columns, \n                open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'wb'))","e7385328":"plt.figure(figsize=(8, 5))\nsns.heatmap(round(data[numerical_features].corr(method='spearman'), 2), \n            annot=True, mask=None, cmap='GnBu')\nplt.show()","9fdcb513":"plt.figure(figsize=(10, 5))\nsns.heatmap(round(df[categorical_features+numerical_features+['price']].corr(method='spearman'), 2), annot=True,\n            mask=None, cmap='GnBu')\nplt.show()\n","abac169b":"from statsmodels.stats.outliers_influence import variance_inflation_factor","19c6ff49":"# Calculating VIF\nvif = pd.DataFrame()\nvif[\"variables\"] = [feature for feature in categorical_features+numerical_features if feature not in ['year']]\nvif[\"VIF\"] = [variance_inflation_factor(df[vif['variables']].values, i) for i in range(len(vif[\"variables\"]))]\nprint(vif)","4ecdfef3":"NumericData = data[['mileage']]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp = sns.stripplot(x='variable', y='value', data=NumericMelt, jitter=True, edgecolor='gray')\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","7711d186":"NumericData = data[['year']]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp = sns.stripplot(x='variable', y='value', data=NumericMelt, jitter=True, edgecolor='gray')\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","89036d59":"NumericData = data[['engineSize']]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp = sns.stripplot(x='variable', y='value', data=NumericMelt, jitter=True, edgecolor='gray')\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","19583b84":"NumericData = data[['tax', 'mpg']]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp = sns.stripplot(x='variable', y='value', data=NumericMelt, jitter=True, edgecolor='gray')\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","e4099b37":"# Percentage of outliers present in each variable\noutlier_percentage = {}\nfor feature in numerical_features:\n    tempData = data.sort_values(by=feature)[feature]\n    Q1, Q3 = tempData.quantile([0.25, 0.75])\n    IQR = Q3 - Q1\n    Lower_range = Q1 - (1.5 * IQR)\n    Upper_range = Q3 + (1.5 * IQR)\n    outlier_percentage[feature] = round((((tempData<(Q1 - 1.5 * IQR)) | (tempData>(Q3 + 1.5 * IQR))).sum()\/tempData.shape[0])*100,2)\noutlier_percentage","677b64b5":"df = data.copy()\npath = '\/kaggle\/working'\nfor i, feature in enumerate(categorical_features):\n    \n    le = LabelEncoder()\n    ohe = OneHotEncoder(sparse=False)\n\n    # create directory to save label encoding models\n    if not os.path.exists(os.path.join(path, \"TextEncoding\")):\n        os.makedirs(os.path.join(path, \"TextEncoding\"))\n\n    # perform label encoding\n    le.fit(df[feature])\n    # save the encoder\n    joblib.dump(le, open(os.path.join(path, \"TextEncoding\/le_{}.sav\".format(feature)), 'wb'))\n    \n    # transfrom training data\n    df[feature] = le.transform(df[feature])\n\n    # get classes & remove first column to elude from dummy variable trap\n    columns = list(map(lambda x: feature+' '+str(x), list(le.classes_)))[1:]\n    \n    # save classes\n    joblib.dump(columns, \n                open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'wb'))\n    # load classes\n    columns = joblib.load(\n        open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'rb'))\n\n    if len(le.classes_)>2:\n        # perform one hot encoding\n        ohe.fit(df[[feature]])\n        # save the encoder\n        joblib.dump(ohe, \n                    open(os.path.join(path, \"TextEncoding\/ohe_{}.sav\".format(feature)), 'wb'))\n\n        # transfrom training data\n        # removing first column of encoded data to elude from dummy variable trap\n        tempData = ohe.transform(df[[feature]])[:, 1:]\n\n        # create Dataframe with columns as classes\n        tempData = pd.DataFrame(tempData, columns=columns)\n    else:\n        tempData = df[[feature]]\n    \n    # create dataframe with all the label encoded categorical features along with hot encoding\n    if i==0:\n        encodedData = pd.DataFrame(data=tempData, columns=tempData.columns.values.tolist())\n    else:\n        encodedData = pd.concat([encodedData, tempData], axis=1)","d7ceb46e":"# merge numerical features and categorical encoded features\ndf = df[numerical_features+['price']]\ndf = pd.concat([df, encodedData], axis=1)\ndf.info()","7accd7ce":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics, preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score","a41cde34":"train_data = df.copy()\nfeature_cols = [feature for feature in train_data.columns if feature not in(['price'])]\nprint('features used: ', feature_cols)\n\n# RESCALING\n#scaler = MinMaxScaler()\n#scaler.fit(train_data[feature_cols])\n#train_data[feature_cols] = scaler.transform(train_data[feature_cols])","19f444ec":"X = train_data[feature_cols]\ny = train_data['price']\n\nvalidation_size = 0.2\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=validation_size, random_state=0)","9731b63c":"model = LinearRegression()\nmodel.fit(X_train, y_train)","f3139b57":"y_pred = model.predict(X_train)\n\nprint('Train metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_train, y_pred)))\nprint('r2_score: ', round(r2_score(y_train, y_pred)*100, 2))\n\ny_pred = model.predict(X_test)\n\nprint('Validation metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('r2_score: ', round(r2_score(y_test, y_pred)*100, 2))","78d61689":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=list(range(len(y_pred[-150:]))), y=y_pred[-150:],\n                         mode='lines',\n                         name='Prediction'))\nfig.add_trace(go.Scatter(x=list(range(len(y_test[-150:]))), y=y_test[-150:],\n                         mode='lines',\n                         name='True value'))\n\nfig.show()","793af17a":"model = XGBRegressor( \n    n_estimators = 1000,\n    learning_rate=0.09, \n    min_child_weight=5,\n    max_depth = 3,\n    subsample = 0.75,\n    seed=7)\n\n\nmodel = model.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    #early_stopping_rounds=10,\n    #eval_set=[(X_test, y_test)],\n    verbose=False)","ccb5a0d6":"y_pred = model.predict(X_train)\n\nprint('Train metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_train, y_pred)))\nprint('r2_score: ', round(r2_score(y_train, y_pred)*100, 2))\n\ny_pred = model.predict(X_test)\n\nprint('Validation metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('r2_score: ', round(r2_score(y_test, y_pred)*100, 2))","54e72834":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=list(range(len(y_pred[-150:]))), y=y_pred[-150:],\n                         mode='lines',\n                         name='Prediction'))\nfig.add_trace(go.Scatter(x=list(range(len(y_test[-150:]))), y=y_test[-150:],\n                         mode='lines',\n                         name='True value'))\n\nfig.show()","680a5f7c":"# Handling Categorical Features (Label Encoding & One Hot Encoding)","a0a674f7":"### Bivariate Analysis Correlation plot for numerical features","a5674c0c":"# Looking at Outliers","1ef2c435":"### Label encoding categorical features for correlation","dd9bb770":"**Observations-**\n* model - R8 models are the costliest ones while A1, A3, A4 & Q3 are the most popular ones\n* transmission - Manual has usually low cost\n* fuelType - Hybrid are the least popular and costliest ones\n* year - new cars are sold at higher prices\n* mileage - lower the mileage or car travelled, higher the price\n* mpg - lower the mpg, higher the car price (usually heavy or luxury cars have lower mpg)\n* engineSize - bigger the enginer, higher the price\n* tax - generally higher the tax, higher the car price","97822313":"# Analyzing features using VIF","dad5f91c":"# Training Model","be8e5137":"### Bivariate Analysis Correlation plot with the Categorical variables","2c6fa83d":"# CORRELATION","ac795aff":"# Model 2: XGB","90e59420":"# EDA","9b81e8d2":"**Observations-**\n* year - mileage -ve\n* mpg - tax -ve\n* year - mpg - mileage","d91b559e":"# Model 1: Linear Regresssion ","a7739686":"* mpg - miles per gallon\n* tax - road tax"}}