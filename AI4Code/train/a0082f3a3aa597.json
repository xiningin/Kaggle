{"cell_type":{"91021684":"code","cae7e3e4":"code","981737e2":"code","043a72df":"code","efb25d6b":"code","8c9ed00b":"code","1d439422":"code","b9698be9":"code","9572f833":"code","8db3b91e":"code","bb0d6634":"code","c8835ebe":"code","0c5007fb":"code","e0f8aa84":"code","fa578249":"code","2296f346":"code","7eff1738":"code","04a207c9":"code","e7d41927":"code","a447d329":"code","aac18a3f":"code","bcea3429":"code","663358b7":"code","bcdfa58e":"code","1b8fddec":"code","3abc625b":"code","cbf2d499":"code","a289dbd1":"markdown","65603de0":"markdown","88944077":"markdown","2ba910be":"markdown","0e486330":"markdown","a9ece6eb":"markdown","fe21a4e5":"markdown","586c84dc":"markdown","7609269b":"markdown","5d157c51":"markdown","ff68ea8f":"markdown","884968f7":"markdown","95913558":"markdown","17741e50":"markdown","d8c0730b":"markdown","f810928e":"markdown","b4425b2d":"markdown","fb3c7a91":"markdown","dcb9202d":"markdown","c769fba4":"markdown","07934763":"markdown","c50f61a2":"markdown","6d331109":"markdown","58417b60":"markdown","d217201c":"markdown","ba8a6163":"markdown","0f4e18ed":"markdown","4aa93e83":"markdown","c0e9b4b0":"markdown","7b601b32":"markdown","5c77777f":"markdown","0bacf1ed":"markdown"},"source":{"91021684":"#loading libraries\nimport pandas as pd\nimport string\nimport seaborn as sns\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport xgboost as xgb\nfrom sklearn.ensemble import forest \nfrom sklearn import tree\nfrom sklearn import linear_model\nfrom sklearn import svm\nfrom sklearn.model_selection import cross_val_score\nimport statistics as stats\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom skopt import BayesSearchCV\n\n\n\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')\n\n\n# !pip install scikit-optimize","cae7e3e4":"#reading the data            \ndf = pd.read_csv(\"..\/input\/traincsv\/train.csv\")\n\n\n#dummy variables\nid = df[\"id\"]\nlocation = df[\"location\"]\nkeyword = df[\"keyword\"]","981737e2":"df = df.rename(columns={\"text\":\"tweet\"})\ndf[\"original tweet\"] = df[\"tweet\"]\ndf.head()","043a72df":"#removing irrelevant features\ndf = df.drop(columns=[\"id\",\"location\",\"keyword\"],axis=1)\ndf.head()","efb25d6b":"#Dropping variables that has more than 60 percent missing values\n\n#Checking the percentage missing values by columns\nmissing_column = (df.isna().sum()\/len(df))*100\nprint(missing_column)\n\n","8c9ed00b":"duplicate = df.duplicated().sum()\nprint(duplicate)#we can see that there are duplicates values\n ","1d439422":"df = df.drop_duplicates()","b9698be9":"sns.countplot(df[\"target\"])\n#We can see the target column is balanced.","9572f833":"df[\"tweet\"] = df['tweet'].str.replace('http\\S+|www.\\S+', '', case=False)\n\ndf.head()","8db3b91e":"#Removing punctuation\ndf[\"tweet\"] = df['tweet'].str.replace('[{}]'.format(string.punctuation), '')\n\n\n#Changing the special characters to the usual alphabet letters\ndf['tweet'] = df[\"tweet\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\ndf.head()\n\n","bb0d6634":"df['tweet'] = df['tweet'].str.replace('\\d+', '')","c8835ebe":"df[\"tweet\"] = [word_tokenize(word) for word in df[\"tweet\"]]\ndf.head()\n","0c5007fb":"df[\"tweet\"] = [[word.lower() for word in words ] for words in df[\"tweet\"]]\ndf.head()","e0f8aa84":"stop_words = stopwords.words('english')\n\ndf[\"tweet\"] = [[words for words in word if words not in stop_words] for word in df[\"tweet\"]]\ndf.head()","fa578249":"lem = nltk.WordNetLemmatizer()\ndf[\"tweet\"] = [[lem.lemmatize(lema,\"v\") for lema in i]for i in df[\"tweet\"]]\n","2296f346":"x = df[\"tweet\"]\ny = df[\"target\"]","7eff1738":"x_train,x_test,y_train,y_test = train_test_split = train_test_split(x,y, test_size=0.2, random_state=0)\nprint(x_train.shape)\nprint(x_test.shape)","04a207c9":"# # Creating a dummy fuction so it can be passed to the  (tokenizer and preprocessor) parameter\n# def dummy(doc):\n#     return doc\n\n# cv = CountVectorizer(\n#         tokenizer=dummy,\n#         preprocessor=dummy,\n#         min_df = 0.000167\n        \n#     )  \n\n# x_train = cv.fit_transform(x_train)\n\n# x_test =  cv.transform(x_test)\n\n","e7d41927":"def dummy(doc):\n    return doc\n\ntfidf = TfidfVectorizer(\n         tokenizer=dummy,\n         preprocessor=dummy,\n         min_df = 0.000167\n\n)\n\nx_train = tfidf.fit_transform(x_train)\n\nx_test =  tfidf.transform(x_test)\n\n\n\n","a447d329":"xg = xgb.XGBClassifier()\nfo =  forest.RandomForestClassifier()\ntr = tree.DecisionTreeClassifier()\nlo = linear_model.LogisticRegression()\nsv = svm.SVC()\n\nxgb_score = cross_val_score(xg,x_train,y_train,cv=5)\nran_score = cross_val_score(fo,x_train,y_train,cv=5)\ndtree_score = cross_val_score(tr,x_train,y_train,cv=5)\nlog_score = cross_val_score(lo,x_train,y_train,cv=5)\nsvm_score = cross_val_score(sv,x_train,y_train,cv=5)\n\n# This Dataframe outputs the average score for each algorithms\ndf_score = pd.DataFrame({\"model\":[\"xgboost\",\"RandomForestClassifier\",\"DecisionTreeClassifier\",\"LogisticRegression\",\"Support vector machine\"],\"score\":[stats.mean(xgb_score),stats.mean(ran_score),stats.mean(dtree_score),stats.mean(log_score),stats.mean(svm_score)]})\ndf_score\n# We can see that Support vector machine classifier gave the best score\n","aac18a3f":"#Checking initial model score\ninitial_model = svm.SVC()\ninitial_model = initial_model.fit(x_train,y_train)\noriginal_score = initial_model.score(x_test,y_test)\nprint(f'Original Score = {original_score}')\n\n# Count vectorization score = Score = 0.7906976744186046\n# Tfidf score = Score = 0.7933554817275748\n\n#We see Tfidf gives the best score..so tfidf will be used for vectorization\n","bcea3429":"# Finding the best parameter\n# optimize_model  = svm.SVC()\n# param = {'C': [0.1,1, 10, 52,100], 'gamma': ('auto','scale'),'kernel': ['linear','rbf', 'poly', 'sigmoid']}\n# search = BayesSearchCV(optimize_model,param,scoring=\"accuracy\")\n# search = search.fit(x_train,y_train)\n# print(search.best_params_)\n\n#best_param = C=1.0,gamma='scale',kernel='rbf'\n","663358b7":"model = svm.SVC(C=1.0,gamma=1.0,kernel='rbf')\nmodel = model.fit(x_train,y_train)\nscore = model.score(x_test,y_test)\nprint(f'model Score = {score}')","bcdfa58e":"test = pd.read_csv(\"..\/input\/clean-test\/clean_test.csv\")\n#dummy variables\ntweet_id =  test[\"id\"]\ntweets = test[\"tweet\"]\nprint(tweets.shape)","1b8fddec":"tweets = tfidf.transform(tweets)","3abc625b":"pred = model.predict(tweets)\nnew_df = {\"id\":tweet_id,\"target\":pred}\nnew_df = pd.DataFrame(new_df)\nnew_df.head()","cbf2d499":"disaster_pred = new_df.to_csv(\"disaster_pred.csv\",index = False)\nprint(disaster_pred)","a289dbd1":"\nOptimized model","65603de0":"**dropping duplicates**","88944077":"# reading the data  ","2ba910be":"When applying TfidfVectorizer,CountVectorization etc on text they expect an array of string that has not been tokenized. So if you pass him an array of arrays of tokenz, it crashes.We will  be passing a tokenized text to the vectorizer, to deal with this We need to pass a dummy fuction to  tokenizer and preprocessor parameter.\n\n","0e486330":"***Data Cleaning***","a9ece6eb":"**Conveting text to Lower Case**\n\nThe model might treat a word which is in the beginning of a sentence with a capital letter different from the same word which appears later in the sentence but without any capital latter. This might lead to decline in the accuracy. Whereas lowering the words would be a better trade off.So that 'A' letter differ from 'a' letter in computer","fe21a4e5":"**Tokenizing data**\n\nWe use the method word_tokenize() to split a sentence into words. The output of word tokenization can be converted to DataFrame for better text understanding in machine learning applications. It can also be provided as input for further text cleaning steps such as  numeric character removal or stop words removal. Machine learning models need numeric data to be trained and make a prediction. Word tokenization becomes a crucial part of the text (string) to numeric data conversion.","586c84dc":"**Reading the data**","7609269b":"***Cleaning the tweet column***","5d157c51":"# Predicting which Tweets are about real disasters and which ones are not","ff68ea8f":"**Declaring Inpendent and Dependent Variable**\n","884968f7":"**Removing Punctuation And Changing The Special Characters To The Usual Alphabet Letters** \n\nRaw data contain  punctuation,Hyper Link,special character.These value can hamper the performance of model so before applying any text Vectorization first we need to convert raw data into meaningful data which is also called as text preprocessing .","95913558":"**Removing stop words from texts**\n\nRemoving stopwords can potentially help improve the performance as \nthere are fewer and only meaningful tokens left.\nThus, it could increase classification accuracy.","17741e50":"saving to csv","d8c0730b":"**Checking for missing values**","f810928e":"**Lematization**\n\nLemmatization usually aims to remove word endings. It helps in returning the base or dictionary form of a word, which is known as the lemma.\n\n<!-- congrats yey you found the easter egg (hahaha) \n\ntext me to claim your reward -->","b4425b2d":"# **Data Preprocessing**","fb3c7a91":"Using Cross Validation to find the algorithm that gives the best performance","dcb9202d":"**converting tweets to numeric using Tfidf vectorizer**","c769fba4":"**Removing Hyper Links**\n\nBy observing the data we can see that some text contains external links (\"http:\/\/\"..) which are irrelevant","07934763":"**Hyperparameter Tuning Using Using BayesSearchCV**\n\nFinding the hyperparameter values of a learning algorithm that produces the best result","c50f61a2":"**Removing numbers from dataframe**\n\nRemoving numbers from the text like \u201c1,2,3,4,5\u2026\u201d We will remove numbers because numbers doesn\u2019t give much importance to get the main words.","6d331109":"\n# Modelling","58417b60":"**Splitting the data**\n\nNow the data is clean we will be Spltting the dataframe into training and testing sample of 80% and 20% respectively.\n","d217201c":"Tfidfvectorizer","ba8a6163":"**renaming the text column**","0f4e18ed":"**Converting text to numeric**\n\nWe cannot work with text directly when using machine learning algorithms. Instead, we need to convert the text to numbers.\nComputers don\u2019t understand text and only understand and process numbers. ","4aa93e83":"**Predicting test data**","c0e9b4b0":"Optimizing parameters","7b601b32":"**Checking for duplicate values**","5c77777f":"Count vectorization","0bacf1ed":"**Removing irrelevant features**"}}