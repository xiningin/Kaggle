{"cell_type":{"90ae3a11":"code","48380b57":"code","c3f12d7b":"code","0441eed6":"code","7b6d7665":"code","2a7d2d08":"code","688e0eb0":"code","9c12759c":"code","a0716e79":"code","c9533adb":"code","fe91f413":"code","69013e32":"code","f9e8f1ad":"code","5f5d67e4":"code","d5d833ae":"code","d197f325":"code","10e21c1a":"code","66983967":"code","0dd20bfc":"markdown","fcd606f1":"markdown","1f843c82":"markdown","dc11abf8":"markdown","3097158b":"markdown","902644c1":"markdown","aacc47ea":"markdown","6145c3fa":"markdown","a543a68b":"markdown","8c443116":"markdown","77c8a32c":"markdown","367c98c5":"markdown","d9cfa131":"markdown","0fc38d65":"markdown","181b15f3":"markdown","69841f93":"markdown","f5f07adc":"markdown","bbf79921":"markdown","a1b6c39a":"markdown","6a55af78":"markdown","f754b169":"markdown","78c3dead":"markdown","fa23070d":"markdown","3c57e69f":"markdown","9cd3fbd9":"markdown","fa1bb2e6":"markdown","42faeeec":"markdown"},"source":{"90ae3a11":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.preprocessing import MinMaxScaler\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nfrom scipy.stats import beta\nfrom scipy.stats import f","48380b57":"data = pd.read_csv('\/kaggle\/input\/early-diabetes-classification\/diabetes_data.csv', sep=';')","c3f12d7b":"data.info()","0441eed6":"data.head()","7b6d7665":"# pandas profiling\nprofile = ProfileReport(data)\nprofile","2a7d2d08":"# Encoding\nlabel_encoder = LabelEncoder() \ndata['gender'] = label_encoder.fit_transform(data['gender'])\n\ndata.head(10)","688e0eb0":"# Choosing the features and the label\nX = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\nprint(X.shape)\nprint(X.iloc[:5, :])\nprint('\\n')\nprint(y.shape)\nprint(y[:5])","9c12759c":"# train and test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint('\\n')\nprint(X_test.shape)\nprint(y_test.shape)","a0716e79":"# Scaling\nencoder = MinMaxScaler()\nX_train = encoder.fit_transform(X_train)\nX_test = encoder.transform(X_test)\n\npd.DataFrame(X_train, columns = X.columns)","c9533adb":"Model_Score = []\n\n# Logistic Regression\nclassifier_lr = LogisticRegression(random_state=0)\nscores = cross_val_score(classifier_lr, X_train, y_train, cv=10, scoring='accuracy')\nlr_train_score_mean = scores.mean()\nlr_train_score_std = scores.std()\nclassifier_lr.fit(X_train, y_train)\nmodel_name = 'Logistic Regression'\nlr_test_score = classifier_lr.score(X_test, y_test)\n\nscore = list((model_name, lr_train_score_mean.round(4), lr_train_score_std.round(4), lr_test_score.round(4)))\nModel_Score.append(score)","fe91f413":"# SVC\ndef svm(degree, kernel, gamma, x_train, x_test, train_label, test_label):\n    if kernel == 'poly':\n        support_vector_machine = SVC(kernel='poly', degree=degree, random_state=0)\n        cv_score = cross_val_score(support_vector_machine, x_train, train_label, cv=10)\n        svm_train_score_mean = cv_score.mean()\n        svm_train_score_std = cv_score.std()\n        support_vector_machine.fit(x_train, train_label)\n        svm_test_score = support_vector_machine.score(x_test, test_label)\n        name = 'SVM with ' + str(degree) + '-degree polynomial kernel'\n    elif kernel == 'rbf':\n        support_vector_machine = SVC(kernel='rbf', gamma=gamma, random_state=0)\n        cv_score = cross_val_score(support_vector_machine, x_train, train_label, cv=10)\n        svm_train_score_mean = cv_score.mean()\n        svm_train_score_std = cv_score.std()\n        support_vector_machine.fit(x_train, train_label)\n        svm_test_score = support_vector_machine.score(x_test, test_label)\n        name = 'SVM with rbf kernel and ' + str(gamma) + ' coefficient'\n    else:\n        support_vector_machine = SVC(kernel='sigmoid', gamma=gamma, random_state=0)\n        cv_score = cross_val_score(support_vector_machine, x_train, train_label, cv=10)\n        svm_train_score_mean = cv_score.mean()\n        svm_train_score_std = cv_score.std()\n        support_vector_machine.fit(x_train, train_label)\n        svm_test_score = support_vector_machine.score(x_test, test_label)\n        name = 'SVM with sigmoid kernel and ' + str(gamma) + ' coefficient'\n    return support_vector_machine, list((name, svm_train_score_mean.round(4), svm_train_score_std.round(4),\n                                         svm_test_score.round(4)))\n\nsvc_models = [\n    [None, 'rbf', 'scale'], [None, 'rbf', 'auto'], [None, 'sigmoid', 'scale'], [None, 'sigmoid', 'auto'],\n    [1, 'poly', None], [2, 'poly', None], [3, 'poly', 'None'], [4, 'poly', 'None'], [5, 'poly', 'None']\n]\n\nfor i, x in enumerate(svc_models):\n    _, score = svm(svc_models[i][0], svc_models[i][1], svc_models[i][2], X_train, X_test, y_train, y_test)\n    Model_Score.append(score)","69013e32":"# Random Forest Classification\nacc_score = []\nstd_score = []\nmax_rf_ne = 50\nfor ne in range(1, max_rf_ne):\n    classifier_rf = RandomForestClassifier(n_estimators=ne, random_state=0)\n    scores = cross_val_score(classifier_rf, X_train, y_train, cv=10, scoring='accuracy')\n    rf_train_score_mean = scores.mean()\n    rf_train_score_std = scores.std()\n    acc_score.append(rf_train_score_mean)\n    std_score.append(rf_train_score_std)\n\nbest_rf_acc = max(acc_score)\nbest_rf_ne = acc_score.index(max(acc_score))\n\nclassifier_rf = RandomForestClassifier(n_estimators=best_rf_ne, random_state=0)\nclassifier_rf.fit(X_train, y_train)\nrf_test_score = classifier_rf.score(X_test, y_test)\n\nmodel_name = 'Random Forest with ' + str(best_rf_ne) + ' estimators'\n\nscore = list((model_name, rf_train_score_mean.round(4), rf_train_score_std.round(4), rf_test_score.round(4)))\nModel_Score.append(score)","f9e8f1ad":"f, ax = plt.subplots()\nax.plot(range(1, max_rf_ne), acc_score, marker='o')\nax.set_xlabel('# of trees')\nax.set_ylabel('accuracy')\nax.set_title('accuracy\/# of trees')","5f5d67e4":"# Max voting\ndef max_voting(estimators, x_train, x_test, train_label, test_label):\n    mv_classifier = VotingClassifier(estimators=estimators, voting='hard')\n    cv_score = cross_val_score(mv_classifier, x_train, train_label, cv=10)\n    mv_train_score_mean = cv_score.mean()\n    mv_train_score_std = cv_score.std()\n    mv_classifier.fit(x_train, train_label)\n    name = 'Max voting'  # Add the name of the base models (estimators)\n    mv_test_score = mv_classifier.score(x_test, test_label)\n    return mv_classifier, list((name, mv_train_score_mean.round(4), mv_train_score_std.round(4),\n                                mv_test_score.round(4)))\n\nmodel_1 = RandomForestClassifier(n_estimators=best_rf_ne)\nmodel_2 = LogisticRegression(random_state=0)\nmodel_3 = SVC(kernel='poly', degree=2)\nmodel_4 = SVC(kernel='poly', degree=3)\nmodel_5 = SVC(kernel='poly', degree=4)\nmodel_6 = SVC(kernel='poly', degree=5)\nmodel_7 = SVC(kernel='rbf', gamma='scale')\nmodel_8 = SVC(kernel='rbf', gamma='auto')\nmodel_9 = SVC(kernel='sigmoid', gamma='scale')\nmodel_10 = SVC(kernel='sigmoid', gamma='auto')\n\n# version 10\nclassifier_mv, score = max_voting([('rf', model_1), ('lr', model_2), ('SVM_2', model_3), ('SVM_3', model_4),\n                                   ('SVM_4', model_5), ('SVM_5', model_6), ('SVM_rbf_scale', model_7),\n                                   ('SVM_rbf_auto', model_8), ('SVM_sigmoid_scale', model_9),\n                                   ('SVM_sigmoid_auto', model_10)], X_train, X_test, y_train, y_test)\n\nModel_Score.append(score)","d5d833ae":"# Results\nModel_Score = pd.DataFrame(Model_Score, columns=['Model', 'Train Score Average', 'Train Score SD', 'Test Score'])\nModel_Score = Model_Score.sort_values(by=['Test Score'], ascending=False)","d197f325":"Model_Score","10e21c1a":"y_pred = classifier_rf.predict(X_test)\nlabels = target_names = ['Class 0', 'Class 1']\nprint(classification_report(y_test, y_pred, target_names=labels))","66983967":"# Confusion Matrix\ncm_lr = confusion_matrix(y_test, y_pred)\n\nprint(cm_lr)","0dd20bfc":"We can also benefit from using Pandas Profiling as well. It is an amazing tool that can generate thorough profile reports and interesting plots for EDA and descriptive analysis purposes. Here are some of the observations from this profile report:\n\n1. we have 17 variables or features and 520 observations.\n2. all of the features except age are categorical.\n3. There are some duplicates in the dataset. Meaning that some rows have the same values for different features. Since most of the features are categorical and binary, I expected to have some duplicates but, it's interesting to take a look at the most frequently occurred duplicates too.\n4. The skewness of the age variable is 0.329 (click on toggle details in Variables part and age). Meaning that age has a right-skewed distribution. \n5. Using the calculated IQR we can compute the Lower Whisker (LW) and Upper Whisker (WH), and detect the outliers afterward. Even before using such a method, I'm pretty sure that there are some outliers in the data by looking at the age histogram or age extreme values. These outliers might even be responsible for the right-skewed age distribution. However, only 1% of the observations are suspicious of being outliers. Therefore, I'm going to neglect them here, and do not remove them. For more information on outlier handling, you can take a look at [this notebook of mine](https:\/\/www.kaggle.com\/kamyarazar\/breast-cancer-classification-using-pca-in-python).\n6. We can see that other features are binary. Most of them are evenly distributed, meaning that we have close numbers of 0 and 1, except gender, genital_thrush, irritability, muscle_stiffness, and obesity. if you want to take a look at the pie chart of these variables, you can find it in Variables -> Toggle details -> Categories.\n7. Another important thing to always take a look at in healthcare-related data is the sparsity in the data. Again lets take a look at the pie chart of the class feature: 1 -> 320 -> 61.5% ||| 0 -> 200 -> 38.5%. As it's obvious sparsity is not a problem here, and we have enough 0 and 1 observations.\n8. The correlations between the features are mostly positive. There is a strong correlation between class, polyuria, and polydipsia.\n9. You can also take a look at the last table, Duplicate Rows, most frequently occurring. Since I don't have more information on the observations, like the name, observation time, ID, etc., I can't say if these are really duplicates, or either they were different people or the same person at different times. Hence, I'll have to leave them like that.","fcd606f1":"<a id=\"21\"><\/a> <br>\n## Pandas Profiling","1f843c82":"With that in mind, I chose the Random Forest with 13 estimators as our final model.","dc11abf8":"<a id=\"34\"><\/a> <br>\n## Max Voting","3097158b":"Now, data preprocessing is done, and we can go for the next part that is training prediction models. Here, we used Logistic Regression, SVM, and two ensemble learning methods known as Random Forest and Max Voting. To get a relevant measure of bias and variance and compare different models more reliably, we have applied K-fold Cross-Validation for each model as well. It is worthy to note that each model's hyperparameters need to be tuned, but here, we have done hyperparameter tuning just for Random Forest and SVM. To tune the Random Forest hyperparameter, we tried different values for the number of trees in the forest between [1, 50], and choose the value which has the highest 10-fold Cross-Validated accuracy score. Also, for tuning SVM's hyperparameter, we have tried different configurations of the hyperparameters to get the best result.\n\nThe results of the different models will be saved in a variable called *Model_Score* to compare at the end.","902644c1":"<a id=\"1\"><\/a> <br>\n# <center><span style=\"font-family:cursive;\">Import Libraries and the Data<\/span><\/center>","aacc47ea":"<a id=\"41\"><\/a> <br>\n## Model Scores","6145c3fa":"<a id=\"33\"><\/a> <br>\n## Random Forest Classifier","a543a68b":"# Contents\n\n* [Import Libraries and the Data](#1)\n* [Data Preprocessing](#2)\n    - [Pandas Profiling](#21)\n    - [Label Encoding](#23)\n    - [Choosing The Features and The Label, and Train-Test Split](#24)\n    - [Min-Max Normalization](#22)\n* [Data Prediction](#3)\n    - [Logistic Regression](#31)\n    - [Support Vector Machine (Support Vector Classifier)](#32)\n    - [Random Forest Classifier](#33)\n    - [Max Voting](#34)\n* [Results](#4)\n    - [Model Scores](#41)\n    - [Bias-Variance Tradeoff](#42)\n    - [Best Model Evaluation](#43)\n","8c443116":"<a id=\"23\"><\/a> <br>\n## Label Encoding","77c8a32c":"<a id=\"4\"><\/a> <br>\n# <center><span style=\"font-family:cursive;\">Results<\/span><\/center>","367c98c5":"<a id=\"32\"><\/a> <br>\n## Support Vector Machine (Support Vector Classifier)","d9cfa131":"So, after all, there are two things needed to be done in the data preprocessing phase.\n1. Scaling the Age feature\n2. Enoding the Gender\n\nFor scaling, I'm going to use MinMaxScaler but you can use StandardScaler as well. Also, for encoding the Age feature, I'll use LabelEncoder provided in the Scikt-learn library.","0fc38d65":"![](https:\/\/community.alteryx.com\/t5\/image\/serverpage\/image-id\/52874iE986B6E19F3248CF\/image-size\/large?v=v2&px=999)\n\n![](https:\/\/miro.medium.com\/max\/1400\/1*9hPX9pAO3jqLrzt0IE3JzA.png)","181b15f3":"<a id=\"1\"><\/a> <br>\n# <center><span style=\"font-family:cursive;\">If you liked my work then please upvote, Thank you.<\/span><\/center>","69841f93":"<a id=\"43\"><\/a> <br>\n## Best Model Evaluation","f5f07adc":"**Perfect!**","bbf79921":"In a classification problem, especially when it's used in the healthcare area, accuracy might not be the best metric to evaluate the performance of the model. For example, in healthcare, the false negatives (negative outcomes that the model predicted incorrectly) might be more important than the false positives (positive outcomes that the model predicted incorrectly). Why so? Consider these two scenarios:\n\n1. False negatives: if someone has diabetes, but our model predicted that he doesn't have (predicted incorrectly).\n2. False positives: if someone doesn't have diabetes, but our model predicted that he has (predicted incorrectly).\n\nObviously, the first scenario is far more dangerous than the second one. In the second case, by doing more experiments, we can finally understand that the patient doesn't have diabetes. The consequences might be only financial (hope this guy has insurance! :)). However, in the other case, we are sending this patient home because we think he doesn't have diabetes and he may come back again in the future when his illness is worse than before. Take a look at the below two pictures as well. I hope you got the idea! So let's take a look at the confusion matrix and other metrics like precision, recall, and f1-score. A detailed explanation on these metrics can be found [here](https:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/).\n\n![](https:\/\/miro.medium.com\/max\/559\/1*PuhUdkH2tKomTEYDRL1PwQ.png)\n![](https:\/\/miro.medium.com\/max\/700\/1*jYoVGvwPH0qt2M7Xq24jTQ.png)","a1b6c39a":"<a id=\"2\"><\/a> <br>\n# <center><span style=\"font-family:cursive;\">Data Preprocessing<\/span><\/center>","6a55af78":"<a id=\"42\"><\/a> <br>\n## Bias-Variance Tradeoff","f754b169":"<a id=\"31\"><\/a> <br>\n## Logistic Regression","78c3dead":"Here are the model accuracies for different numbers of trees in the forest.","fa23070d":"<a id=\"1\"><\/a> <br>\n# <center><span style=\"font-family:cursive;\">If you liked my work then please upvote, Thank you.<\/span><\/center>","3c57e69f":"<a id=\"22\"><\/a> <br>\n## Min-Max Normalization","9cd3fbd9":"<a id=\"24\"><\/a> <br>\n## Choosing The Features and The Label, and Train-Test Split","fa1bb2e6":"<a id=\"3\"><\/a> <br>\n# <center><span style=\"font-family:cursive;\">Data Prediction<\/span><\/center>","42faeeec":"Pay attention! As you can see we got 100% accuracy on the test set using SVM with rbf kernel and scale coefficient. However, we should always be more careful with 100% test accuracy. That's where Cross-Validation techniques like K-fold CV come in handy. As the result of the K-fold CV suggest, the mean of the cross-validated results is 0.9545 with an SD of 0.0445. The high variance of K-fold cross-validation means that the model is not robust enough, and if we run the model again, we may get different and even worse results. Also, if we use the model to make predictions on new observations that the model has not seen yet, we may not get good results. To better understand this concept, take a look at the below two pictures. We are looking for a model with no or low bias and low variability."}}