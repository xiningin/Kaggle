{"cell_type":{"664c52e6":"code","0046a000":"code","1707e053":"code","f39bdde3":"code","f7d56587":"code","94317199":"code","99c8fb6b":"code","61b5228f":"code","86fcffcd":"code","eb7d4cf9":"code","1201cfa8":"code","95e00c4c":"code","02014241":"code","ae68c4f7":"code","aa96d72b":"code","a3a875dc":"code","1f72dde1":"code","306a5733":"code","7cc09f6d":"code","0c2f9e96":"code","1383d60e":"code","ab7b0453":"code","1b449b13":"code","f8ee5717":"code","1e941d21":"markdown","7c2725eb":"markdown","abf017de":"markdown","46f553f9":"markdown","62b8d14e":"markdown","268d0a5c":"markdown","7f7ba98e":"markdown","b7d4c94d":"markdown","26fe8e9a":"markdown","7f6b0e48":"markdown","4b12a103":"markdown","aacd47de":"markdown","909ba81a":"markdown","addc4ad1":"markdown","4716a82e":"markdown","32910a3a":"markdown","eec88f38":"markdown","20727a22":"markdown","984c570e":"markdown","12b1ecc9":"markdown","349003f1":"markdown","581412ed":"markdown","4504ea83":"markdown","bbafe94b":"markdown","23640ebd":"markdown","a7b51d98":"markdown","025aeb7f":"markdown","97d3f04e":"markdown","79fd2adb":"markdown","ce4ebd83":"markdown"},"source":{"664c52e6":"# Installing:\n!pip install -Uqq fastai\n!pip install transformers","0046a000":"from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n\nimport torch\nfrom fastai.text.all import *\n\nimport pandas as pd","1707e053":"# Importing the model and the tokenizer:\n\npretrained_weights = 'gpt2'\ntokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\nmodel = GPT2LMHeadModel.from_pretrained(pretrained_weights)","f39bdde3":"# Encoding a sentence and checking it out:\n\nids = tokenizer.encode('This is an example of text, and')\nids","f7d56587":"# Decoding the same bad boy: \n\ntokenizer.decode(ids)","94317199":"# Reading the training CSV:\ndf_train = pd.read_csv(\"..\/input\/nazidataset\/nazitweets.csv\", header=None, lineterminator='\\n')\n\n# Removing the NaN rows.\ndf_train = df_train.dropna()\n\n# Taking a look at it:\ndf_train.head()\n","99c8fb6b":"# Converting all the texts into a numpy array:\nall_texts = np.array([df_train[1].values])","61b5228f":"# (Stolen Code Alert)\nclass TransformersTokenizer(Transform):\n    def __init__(self, tokenizer): self.tokenizer = tokenizer\n    def encodes(self, x): \n        toks = self.tokenizer.tokenize(x)\n        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))","86fcffcd":"# Defining the splits for the dataloader:\nsplits = [range_of(df_train), list(range(len(df_train), len(all_texts)))]\n\n# Defining the Transformed Lists:\ntls = TfmdLists(all_texts, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)","eb7d4cf9":"show_at(tls.train, 0)","1201cfa8":"# bs refers to the Batch Size while sl refers to the sequence length:\nbs,sl = 4,240 \n\n# Defining the Dataloader:\ndls = tls.dataloaders(bs=bs, seq_len=sl)","95e00c4c":"dls.show_batch(max_n=2)","02014241":"# Defining the function:\ndef tokenize(text):\n    toks = tokenizer.tokenize(text)\n    return tensor(tokenizer.convert_tokens_to_ids(toks))\n\n# Actually Tokenizing everything:\ntokenized = [tokenize(t) for t in progress_bar(all_texts)]","ae68c4f7":"class TransformersTokenizer(Transform):\n    def __init__(self, tokenizer): self.tokenizer = tokenizer\n    def encodes(self, x): \n        return x if isinstance(x, Tensor) else tokenize(x)\n        \n    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))","aa96d72b":"# Getting the dataloader:\n\ntls = TfmdLists(tokenized, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)\ndls = tls.dataloaders(bs=bs, seq_len=sl)","a3a875dc":"# Hey again, my old friend.\n\ndls.show_batch(max_n=2)","1f72dde1":"# The DropOutput Callback:\n\nclass DropOutput(Callback):\n    def after_pred(self): self.learn.pred = self.pred[0]","306a5733":"learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity()).to_fp16()","7cc09f6d":"learn.validate()","0c2f9e96":"learn.lr_find()","1383d60e":"learn.fit_one_cycle(5, 1e-5)","ab7b0453":"def predict(prompt, length, beams, temp):\n    prompt_ids = tokenizer.encode(prompt)\n    inp = tensor(prompt_ids)[None].cuda()\n    preds = learn.model.generate(inp, max_length=length, num_beams=beams, temperature=temp)\n    return tokenizer.decode(preds[0].cpu().numpy())","1b449b13":"predict(\"immigrants are bad for\", 10, 5, 1.5)","f8ee5717":"predict(\"i hate it that these brown dudes have\", 13, 5, 1)","1e941d21":"(Upvote this or the Chinese Government will take away my access to the internet)","7c2725eb":"(Stolen Code Alert) Now we change the previous `Tokenizer` like this:","abf017de":"Checking if the current configuration allow the model to actually work:","46f553f9":"## Learning Rate","62b8d14e":"# Playing Around with the Tokenizer \ud83c\udfc3\ud83c\udffd\u200d\u2640\ufe0f\n","268d0a5c":"Another way to gather the data is to preprocess the texts once and for all and only use the transform to decode the tensors to texts:","7f7ba98e":"# Importing Stuff","b7d4c94d":"# Fine-tuning the model \ud83e\udd39\ud83c\udffd\u200d\u2640\ufe0f","26fe8e9a":"Looks like it is kinda working, not that well BUT it is kinda working.","7f6b0e48":"Let's have a look at what those `csv` files look like:","4b12a103":"The `lr_find()` will find you the best suited learning rate.","aacd47de":"Defining a function to generate a sentence:","909ba81a":"And we can check it still works properly for showing purposes:","addc4ad1":"The HuggingFace model will return a tuple in outputs, with the actual predictions and some additional activations (should we want to use them in some regularization scheme). To work inside the fastai training loop, we will need to drop those using a `Callback`: we use those to alter the behavior of the training loop.\n\nHere we need to write the event `after_pred` and replace `self.learn.pred` (which contains the predictions that will be passed to the loss function) by just its first element. In callbacks, there is a shortcut that lets you access any of the underlying `Learner` attributes so we can write `self.pred[0]` instead of `self.learn.pred[0]`. That shortcut only works for read access, not write, so we have to write `self.learn.pred` on the right side (otherwise we would set a `pred` attribute in the `Callback`).","4716a82e":"Lets take a final look at the data:","32910a3a":"Yes it is **working**, time to find the Learning Rate.","eec88f38":"We specify `dl_type=LMDataLoader` for when we will convert this `TfmdLists` to `DataLoaders`: we will use an `LMDataLoader` since we have a language modeling problem, not the usual fastai `TfmdDL`.","20727a22":"Finally trying the model out \ud83e\udd14","984c570e":"# Generating some Stuff \ud83d\ude0f","12b1ecc9":"Now, we are ready to create our `Learner`, which is a fastai object grouping data, model and loss function and handles model training or inference. Since we are in a language model setting, we pass perplexity as a metric, and we need to use the callback we just defined. Lastly, we use mixed precision to save every bit of memory we can (and if you have a modern GPU, it will also make training faster):\n\nI genuinely have no clue how bad the perplexity is going to get since I literally passed the Sequence Length like 25% of the one they used while training the GPT2.","349003f1":"## Fitting the Model \ud83e\udd39\ud83c\udffd\u200d\u2640\ufe0f","581412ed":"So how do I plan to solve this problem?\n## Dumb Idea: Use Text Generator to Generate more data to feed into a classifier.","4504ea83":"# If you liked what I am doing here: UPVOTE!\n\nIf you don't the chinese government will take away my access to this laptop and I will be left alone in this basement, please save me.","bbafe94b":"**QUALITY DATA IS TOUGH TO BE FOUND.** \n\nYes it is tough to be found and no one wants to spend days trying to read research papers in this ordeal of trying to find that perfect dataset. The perfect publicly available dataset DOES NOT EXIST. \nSure you could be doing hours of feature engineering, but wait, we can generate our own dataset. YES. THAT IS THE PURPOSE.","23640ebd":"### So, what are the three main libraries required here?\nThe libraries required to using a hugging face transformer would be: the `Transformers`, `FastAI`, `Pandas` and `PyTorch` Libraries.","a7b51d98":"As seen above, pretty aweful loss and even awefully good perplexity.","025aeb7f":"# Preprocessing \ud83d\udc69\u200d\ud83d\udcbb","97d3f04e":"Before we move on to the fine-tuning part, let's have a look at this `tokenizer` and this `model`. The tokenizers in `HuggingFace` usually do the tokenization and the numericalization in one step (we ignore the padding warning for now):","79fd2adb":"The fastai library expects the data to be assembled in a `DataLoaders` object (something that has a training and validation dataloader). We can get one by using the `dataloaders` method. We just have to specify a batch size and a sequence length. Since the GPT2 model was trained with sequences of size 1024, **we will not use this sequence length (it's a stateless model, so it will mess the perplexity if we use less, I will do that nonetheless)**:","ce4ebd83":"> #### So what? Should I waste my money to use AWS or Azure's annotation services and get my already shady data more shady labels?\nAnswer: **NO.**"}}