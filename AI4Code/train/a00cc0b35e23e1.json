{"cell_type":{"dc5666a6":"code","0a549092":"code","819772ae":"code","6b2f2947":"code","7f085bac":"code","dede486c":"code","9edef7c7":"code","55bfcaff":"code","224529f9":"code","e2165f9f":"code","b3913071":"code","81e0749b":"code","e7f7b204":"code","6c20ee8b":"code","c7cf0180":"code","f250d170":"code","a0c1e040":"code","8381dbb5":"code","d9794f01":"code","4d562ae0":"code","ed3e4260":"code","cd104ab8":"code","553c62b0":"code","b83edebf":"code","48a30e58":"code","51496bd6":"code","fa405110":"code","9828bbe5":"code","554af45e":"code","b5419963":"code","131133cb":"code","2a6f4ce7":"code","4f763a40":"code","22038640":"code","807861b0":"code","089e9281":"code","488c8017":"markdown","dd452ebf":"markdown","d2748f24":"markdown","b76129a8":"markdown","0ae871b7":"markdown","63f68a7d":"markdown","25087fbc":"markdown","27a83a01":"markdown","3bd9b8b0":"markdown","d6b840f7":"markdown","04a886fb":"markdown","dffc3667":"markdown","e59f45e3":"markdown","fbf20c28":"markdown","c59399c9":"markdown","28d20b44":"markdown","69fab76a":"markdown","ac74a362":"markdown","2d0c87a3":"markdown","f6a91d4d":"markdown","cabc766a":"markdown","b8555c66":"markdown","d9671c71":"markdown"},"source":{"dc5666a6":"import math\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom datetime import date\nimport numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0a549092":"dataset = pd.read_csv('\/kaggle\/input\/customer-personality-analysis\/marketing_campaign.csv',sep='\\t')\ndataset.head()","819772ae":"dataset.shape","6b2f2947":"dataset.columns","7f085bac":"dataset.columns[dataset.isnull().sum() > 0] # Income is the only columns with null values","dede486c":"dataset.columns = dataset.columns.str.replace(' ', '') # Removing unwanted spaces from all columns\ndataset.Income.isnull().sum() # 24 null values","9edef7c7":"grouped = dataset.groupby(['Education']) \ngrouped = grouped.agg(np.mean)\ngrouped['Income']","55bfcaff":"for i in dataset.loc[dataset.Income.isnull()].index:\n    dataset.iloc[i,4] = math.floor(grouped.loc[dataset.iloc[i][2]][1])\n    # Income for nan at i pos = floor(grouped_data(:, Education at i)[1])\ndataset.Income.isnull().sum()","224529f9":"dataset.nunique()","e2165f9f":"dataset.drop(columns=['ID','Recency','Z_CostContact','Z_Revenue','Response'],inplace=True)","b3913071":"dataset.Marital_Status.value_counts()","81e0749b":"dataset['Marital_Status'] = dataset['Marital_Status'].replace(['Alone','YOLO','Absurd','Divorced','Widow'],'Single')\ndataset['Marital_Status'] = dataset['Marital_Status'].replace(['Married', 'Together'],'Relationship')\ndataset.Marital_Status.value_counts() ","e7f7b204":"num_coln = dataset.select_dtypes(include=np.number).columns.tolist()\nbins=10\nj=1\nfig = plt.figure(figsize = (20, 30))\nfor i in num_coln:\n    plt.subplot(7,4,j)\n    plt.boxplot(dataset[i])\n    j=j+1\n    plt.xlabel(i)\n    plt.legend()\nplt.show()","6c20ee8b":"dataset.drop(dataset[(dataset['Income']>200000)|(dataset['Year_Birth']<1920)].index,inplace=True)","c7cf0180":"dataset.rename(columns = {'Year_Birth':'Age'}, inplace = True)\ndataset['Age'] = dataset.Age.apply(lambda x: 2021-x)","f250d170":"dataset['MntTotal'] = np.sum(dataset.filter(regex='Mnt'), axis=1)\nfor i in dataset.filter(regex='Mnt').columns:\n    if(i!='MntTotal'):\n        dataset[i] = round((dataset[i]*100)\/dataset['MntTotal'],2)","a0c1e040":"dataset['NumTotal'] = np.sum(dataset.filter(regex='Purchases'), axis=1)\nfor i in dataset.filter(regex='Purchases').columns:\n    if(i!='NumTotal'):\n        dataset[i] = round((dataset[i]*100)\/dataset['NumTotal'],2)\n        dataset.fillna({i : 0},inplace=True)\ndataset.drop(columns=['NumTotal'],inplace=True)","8381dbb5":"dataset['ExpensePer'] = round((dataset['MntTotal']*100) \/ dataset['Income'],2)","d9794f01":"dataset['TotalAccepted'] = np.sum(dataset.filter(regex='Accepted'),axis=1)\ndataset.drop(columns=['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','AcceptedCmp2'],inplace=True)","4d562ae0":"dataset['AvgWeb'] = round(dataset['NumWebPurchases']\/dataset['NumWebVisitsMonth'],2)\ndataset.fillna({'AvgWeb' : 0},inplace=True) # Handling for cases where division by 0 may yield unwanted results\ndataset.replace(np.inf,0,inplace=True)","ed3e4260":"dataset['Children'] = dataset['Kidhome'] + dataset['Teenhome']","cd104ab8":"dataset['Dt_Customer'] = pd.to_datetime(dataset['Dt_Customer'] )","553c62b0":"dataset.rename(columns = {'Dt_Customer':'TotalEnrollDays'}, inplace = True)\ndataset['TotalEnrollDays'] = pd.to_datetime(date.today()) - dataset['TotalEnrollDays']\ndataset['TotalEnrollDays'] = [float(str(dataset['TotalEnrollDays'][x])[:4]) for x in dataset.index]\ndataset['TotalEnrollDays'] = round(dataset['TotalEnrollDays']\/365,2)\ndataset.rename(columns = {'TotalEnrollDays':'TotalEnrollYrs'}, inplace = True)","b83edebf":"dataset.head()","48a30e58":"dataset.columns","51496bd6":"df = dataset.copy()","fa405110":"from sklearn.preprocessing import LabelEncoder\nencode = LabelEncoder()\nfor i in ['Education', 'Marital_Status']:\n    df[i]=df[[i]].apply(encode.fit_transform)","9828bbe5":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler().fit_transform(df.values)\nscaled_df = pd.DataFrame(scale, index=df.index, columns=df.columns)","554af45e":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\ninertia = []\nfor i in range(1, 10):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 50)\n    kmeans.fit(scaled_df)\n    inertia.append(kmeans.inertia_)\nplt.plot(range(1, 10), inertia,marker = '*')\nplt.xlabel('Clusters')\nplt.ylabel('Inertia')\nplt.show()","b5419963":"from yellowbrick.cluster import KElbowVisualizer\nElbow_M = KElbowVisualizer(KMeans(init = 'k-means++'), k=10, metric='silhouette')\nElbow_M.fit(scaled_df)\nElbow_M.show()","131133cb":"Elbow_M = KElbowVisualizer(KMeans(init = 'k-means++'), k=10, metric='distortion')\nElbow_M.fit(scaled_df)\nElbow_M.show()","2a6f4ce7":"Elbow_M = KElbowVisualizer(KMeans(init = 'k-means++'), k=10, metric='calinski_harabasz')\nElbow_M.fit(scaled_df)\nElbow_M.show()","4f763a40":"kmeans = KMeans(n_clusters=3,init = 'k-means++').fit(scaled_df)\npred = kmeans.predict(scaled_df)\ndf['Cluster'] = pred + 1\ndataset['Cluster'] = pred + 1","22038640":"df[\"Cluster\"].value_counts()","807861b0":"sns.scatterplot(x='Income',y='MntTotal',hue='Cluster',data=dataset)","089e9281":"for i in dataset:\n    g = sns.FacetGrid(dataset, col = \"Cluster\", hue = \"Cluster\", palette = \"coolwarm\",sharey=False,sharex=False)\n    g.map(sns.histplot,i) \n    g.set_xticklabels(rotation=30)\n    g.set_yticklabels()\n    g.fig.set_figheight(5)\n    g.fig.set_figwidth(20)","488c8017":"This is in continuation to **Part 1** for the dataset. Part 1 includes EDA along with statistical analysis and visualisations. You can find it here -   \n**https:\/\/www.kaggle.com\/namanoberoi\/part-1-marketing-analytics-eda-visualizations**\n\n\nPlease do check out the following notebooks below which added value to this notebook ! \n\n1) https:\/\/www.kaggle.com\/karnikakapoor\/customer-segmentation-clustering  \n2) https:\/\/www.kaggle.com\/gaganmaahi224\/9-clustering-techniques-for-customer-segmentation  \n3) https:\/\/www.kaggle.com\/imakash3011\/customer-analysis-eda-report-clustering  \n4) https:\/\/www.kaggle.com\/miguelfzzz\/store-customers-clustering-analysis  ","dd452ebf":"Removing columns which do not provide important information to segment the customers or have only a single value. ","d2748f24":"**Scaling is essential** before performing clustering. Linking an excellent resource where you can learn and visualise this.  \n\nhttps:\/\/stats.stackexchange.com\/questions\/89809\/is-it-important-to-scale-data-before-clustering  \n\nAs mentioned it depends on the data, and it makes sense to do it here as we have different intervals of values for different features.  ","b76129a8":"For segmentation, it is better to have a more generalistic approach than diving into the finer details because if the number of clusters are less, then the algorithm will automatically group based on what the data is feeded. It won't be able to distinguish or categorize records into groups, like we have done that is Single or in a Relationship. I believe this is also why many of notebooks have followed this approach. As analysts, it becomes essential to use these insights instead of leaving everything to the algorithm.","0ae871b7":"So based on the different metrics, number of clusters selected are :  \n\n1) Intertia - 3 (Selected based on the plot)  \n2) Silhouette - 2  \n3) Distortion - 4,5,6,7 (varies alot with the random initialisation of clusters)     \n4) Calinski Harabasz -2  \n\nI will proceed with **3** as it would capture the 3 segments of the population based on income i.e. lower, middle and higher.   \nBonus: Average of all 4 methods is ~3. \n","63f68a7d":"# Section 3: Segmentation \/ Clustering","25087fbc":"But going over other notebooks, found this interesting library **yellowbrick**. There were some interesting metrics to help us find the number of clusters which most wouldn't know about. Thought to mention it here.   \n\n\" By default, the scoring parameter metric is set to **distortion**, which computes the sum of squared distances from each point to its assigned center. However, two other metrics can also be used with the KElbowVisualizer \u2013 **silhouette** and **calinski_harabasz**. The silhouette score calculates the mean Silhouette Coefficient of all samples, while the calinski_harabasz score computes the ratio of dispersion between and within clusters. \"   \n\nSource:  \nhttps:\/\/www.scikit-yb.org\/en\/latest\/api\/cluster\/elbow.html","27a83a01":"Using the classic elbow method for KMeans, the best cluster looks to be **3**. ","3bd9b8b0":"Based on the number of unique values, **Marital status** seems to have unusually large set of values. Will look into this. Other columns have unique values within expectation.","d6b840f7":"Importing data for use","04a886fb":"Based on the plots, there are a number of similarities across certain attributes but the **key ones** which help distinguish these clusters from one another have been summarised as follows -   \n\nCluster 1 includes those customers who have **high income**.  \n- Majority here have *no kids*  \n- This group has *higher expenses* and it is evident from the fact that they earn the most.  \n- As far as the products are concerned, there is nothing that stands out which tells whether the group as a whole prefer a certain product more or less.  \n- The cluster prefers to make *catalog purchases* and has the least deal purchases.  \n- Based on the previous 5 marketing campaigns, the *highest success rate* has come from this group.  \n\nCluster 2 includes those customers who have **moderate income**.   \n- Majority have *at least 1* child. (Child = Kid(s) + Teen(s))  \n- The group has a significant affinity to *wine products*.  \n- This cluster has the highest number of people.  \n\nCluster 3 includes those customers who have **lower income**.     \n- Customers with *Basic education* are found in this group only.  \n- Due to lower income, the expenses in absolute terms is also the least here.  \n- The group prefers to spend more on fruits, meat and fish and less on wine.  \n- The preferred channel to purchases products here is through *stores*.  \n- Majority of campaigns have failed to attract this group. The *success rate is the lowest*.   ","dffc3667":"We can see some clear outliers in **Income and Birth Year**. We will remove the rows where the Income is greater than $200,000 and birth year is less than 1920. For other columns, we cannot blindly remove these outliers as there could be cases where the requirement for these products is high by the user. Maybe the consumer is hosting a party or an event or is more comfortable getting his products from a particular channel. ","e59f45e3":"https:\/\/seaborn.pydata.org\/generated\/seaborn.FacetGrid.html","fbf20c28":"Encoding features which are not numeric","c59399c9":"## ***Feature Engineering***\n\nThis plays quite an important role while using clustering algorithms. I have noticed that many notebooks use the absolute number in terms of spend on various products, number of purchases through different channels, etc.  \n\nAnother way to look, besides to cluster based on absolute terms, would be to think **in terms of percentages**. And so I have used this approach to see if it brings any additional insights, or if it evens compares to the common approach.    \n\nRational behind this is to eliminate the effect of certain variables which may not depict the true picture or help with segmention. A simple **example** to understand this is to look at the relation between Income and Expense. As Income increases over a certain threshold (let's assume), the expenses would remain rather stagnant. The algorithm will not be able classify this properly if the effect of Income is not taken into account. Hence a feature can be created such as Expenses\/Income which would help capture this effect.  \n\n### For this I will be modifying the features as follows - \n\n   1. Replace Birth year with age as it is more meaningful. We can further divide age into intervals to segment the population.  \n   2. Find the total amount spent on various products so that we can find what percentage of expense is spent on which products.   \n   3. Find the total number of purchases made through different channels so that we can find the percentage these channels contribute for the given user    \n   4. Find the percentage of income that is used as expense as discussed earlier\n   5. Find the number of campaigns that a customer has accepted \n   6. Average of purchases per visit to the website  \n   7. Find the total number of children at home  \n   8. Find the Total Enrollment years","28d20b44":"Model Building","69fab76a":"The columns can easily be understood from their names.\n\nColumns beginning with **Mnt** denotes the amount that was spent by the consumer on certain types of products. This value will be in terms of dollars as the income is measured in dollars based on the first few entries. We can compare for the '$' sign with the total number of entries to see if there exists any discrepancy. However, this dataset has been preprocessed to a certain extent so we would not need to do it.  \n\nColumns beginning with **Num** denote a number. These values would not have any sign($,cm,kg,etc.) associated with them and are simply numbers. \n\nColumns beginning with **Accepted** tell us whether a marketing campaign has been successful or not. 5 different campaigns were held. A value of 1 denotes it was successful while a value of 0 denotes it wasn't.\n\nThere are some additional columns like Z_CostContact and Z_Revenue which we will need to look at.  \n\n***Tasks at hand:***\n  \n1. Check for missing values and impute them.  \n2. Check for data values which are extreme (Outliers).    \n3. Making meaningful columns such as age from DOB, total number of purchases from all channels, total amount spent on all products, etc.\n4. Remove unnecessary columns.","ac74a362":"There are 2240 entries out of which 24 have null values. Instead of removing these values, will fill them based on the **mean of values from Education**, assuming an idealistic scenario where the more educated you are, the more you will earn. In the Part 1 of the notebook, since country was an additional attribute in the dataset, had used that in conjuction with Education to find these values.  ","2d0c87a3":"# Section 2: Pre-processing","f6a91d4d":"Will check for any values in the dataset that may cause error in analysis. ","cabc766a":"#### If you have made it this far and learned something new, please consider **upvoting**. Do check out the Part 1 of this notebook. https:\/\/www.kaggle.com\/namanoberoi\/part-1-marketing-analytics-eda-visualizations  \n\n#### Open to feedback for improvement.\n\n### Thanks !\n\n## Special mention to **@imakash3011** for providing this dataset.","b8555c66":"### Feature engineering done","d9671c71":"# Section 1: Exploratory Data Analysis"}}