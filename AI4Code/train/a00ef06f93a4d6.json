{"cell_type":{"59b88e24":"code","02b33ef6":"code","c8a5709b":"code","8756debd":"code","aaf6f3a1":"code","a6435e7d":"code","a4cc121f":"code","305d3178":"code","05f7bae9":"code","26b000bf":"code","f513ab46":"code","d0b3f19b":"code","5afa3191":"code","1e043596":"code","0496033a":"code","ff2e6c1f":"code","9f8adf14":"code","7f38ccc8":"code","1eeb397b":"code","149d4184":"code","3315f386":"code","b2e23f85":"code","8d74980c":"code","65f9306b":"markdown","e103183b":"markdown","79aa474d":"markdown","c9e9943c":"markdown","624df9d7":"markdown","b7c32b6e":"markdown","40d36fa6":"markdown","fe89dd97":"markdown","0925d769":"markdown","ae62f0e6":"markdown","265d4468":"markdown","a5de716d":"markdown"},"source":{"59b88e24":"import numpy as np\nimport pandas as pd\nimport os\nprint(os.listdir(\"..\/input\"))","02b33ef6":"from sklearn.metrics import mean_squared_error\nimport gc\nimport warnings\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold, GridSearchCV, KFold\ngc.enable()\nfrom sklearn.model_selection import train_test_split\n#from bayes_opt import BayesianOptimization","c8a5709b":"train = pd.read_csv('..\/input\/create-extracted-json-fields-dataset\/extracted_fields_train.gz', \n                    dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\ntest = pd.read_csv('..\/input\/create-extracted-json-fields-dataset\/extracted_fields_test.gz', \n                   dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\ntrain.shape, test.shape","8756debd":"def get_folds(df=None, n_splits=5):\n    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n    # Get sorted unique visitors\n    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n\n    # Get folds\n    folds = GroupKFold(n_splits=n_splits)\n    fold_ids = []\n    ids = np.arange(df.shape[0])\n    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n        fold_ids.append(\n            [\n                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n            ]\n        )\n\n    return fold_ids","aaf6f3a1":"y_reg = train['totals.transactionRevenue'].fillna(0)\ndel train['totals.transactionRevenue']\n\nif 'totals.transactionRevenue' in test.columns:\n    del test['totals.transactionRevenue']\n\nfor df in [train, test]:\n    df['date'] = pd.to_datetime(df['visitStartTime'], unit='s')\n    df['sess_date_dow'] = df['date'].dt.dayofweek\n    df['sess_date_hours'] = df['date'].dt.hour\n    df['sess_date_dom'] = df['date'].dt.day","a6435e7d":"def browser_mapping(x):\n    browsers = ['chrome','safari','firefox','internet explorer','edge','opera','coc coc','maxthon','iron']\n    if x in browsers:\n        return x.lower()\n    elif  ('android' in x) or ('samsung' in x) or ('mini' in x) or ('iphone' in x) or ('in-app' in x) or ('playstation' in x):\n        return 'mobile browser'\n    elif  ('mozilla' in x) or ('chrome' in x) or ('blackberry' in x) or ('nokia' in x) or ('browser' in x) or ('amazon' in x):\n        return 'mobile browser'\n    elif  ('lunascape' in x) or ('netscape' in x) or ('blackberry' in x) or ('konqueror' in x) or ('puffin' in x) or ('amazon' in x):\n        return 'mobile browser'\n    elif '(not set)' in x:\n        return x\n    else:\n        return 'others'\n    \n    \ndef adcontents_mapping(x):\n    if  ('google' in x):\n        return 'google'\n    elif  ('placement' in x) | ('placememnt' in x):\n        return 'placement'\n    elif '(not set)' in x or 'nan' in x:\n        return x\n    elif 'ad' in x:\n        return 'ad'\n    else:\n        return 'others'\n    \ndef source_mapping(x):\n    if  ('google' in x):\n        return 'google'\n    elif  ('youtube' in x):\n        return 'youtube'\n    elif '(not set)' in x or 'nan' in x:\n        return x\n    elif 'yahoo' in x:\n        return 'yahoo'\n    elif 'facebook' in x:\n        return 'facebook'\n    elif 'reddit' in x:\n        return 'reddit'\n    elif 'bing' in x:\n        return 'bing'\n    elif 'quora' in x:\n        return 'quora'\n    elif 'outlook' in x:\n        return 'outlook'\n    elif 'linkedin' in x:\n        return 'linkedin'\n    elif 'pinterest' in x:\n        return 'pinterest'\n    elif 'ask' in x:\n        return 'ask'\n    elif 'siliconvalley' in x:\n        return 'siliconvalley'\n    elif 'lunametrics' in x:\n        return 'lunametrics'\n    elif 'amazon' in x:\n        return 'amazon'\n    elif 'mysearch' in x:\n        return 'mysearch'\n    elif 'qiita' in x:\n        return 'qiita'\n    elif 'messenger' in x:\n        return 'messenger'\n    elif 'twitter' in x:\n        return 'twitter'\n    elif 't.co' in x:\n        return 't.co'\n    elif 'vk.com' in x:\n        return 'vk.com'\n    elif 'search' in x:\n        return 'search'\n    elif 'edu' in x:\n        return 'edu'\n    elif 'mail' in x:\n        return 'mail'\n    elif 'ad' in x:\n        return 'ad'\n    elif 'golang' in x:\n        return 'golang'\n    elif 'direct' in x:\n        return 'direct'\n    elif 'dealspotr' in x:\n        return 'dealspotr'\n    elif 'sashihara' in x:\n        return 'sashihara'\n    elif 'phandroid' in x:\n        return 'phandroid'\n    elif 'baidu' in x:\n        return 'baidu'\n    elif 'mdn' in x:\n        return 'mdn'\n    elif 'duckduckgo' in x:\n        return 'duckduckgo'\n    elif 'seroundtable' in x:\n        return 'seroundtable'\n    elif 'metrics' in x:\n        return 'metrics'\n    elif 'sogou' in x:\n        return 'sogou'\n    elif 'businessinsider' in x:\n        return 'businessinsider'\n    elif 'github' in x:\n        return 'github'\n    elif 'gophergala' in x:\n        return 'gophergala'\n    elif 'yandex' in x:\n        return 'yandex'\n    elif 'msn' in x:\n        return 'msn'\n    elif 'dfa' in x:\n        return 'dfa'\n    elif '(not set)' in x:\n        return '(not set)'\n    elif 'feedly' in x:\n        return 'feedly'\n    elif 'arstechnica' in x:\n        return 'arstechnica'\n    elif 'squishable' in x:\n        return 'squishable'\n    elif 'flipboard' in x:\n        return 'flipboard'\n    elif 't-online.de' in x:\n        return 't-online.de'\n    elif 'sm.cn' in x:\n        return 'sm.cn'\n    elif 'wow' in x:\n        return 'wow'\n    elif 'baidu' in x:\n        return 'baidu'\n    elif 'partners' in x:\n        return 'partners'\n    else:\n        return 'others'\n\ntrain['device.browser'] = train['device.browser'].map(lambda x:browser_mapping(str(x).lower())).astype('str')\ntrain['trafficSource.adContent'] = train['trafficSource.adContent'].map(lambda x:adcontents_mapping(str(x).lower())).astype('str')\ntrain['trafficSource.source'] = train['trafficSource.source'].map(lambda x:source_mapping(str(x).lower())).astype('str')\n\ntest['device.browser'] = test['device.browser'].map(lambda x:browser_mapping(str(x).lower())).astype('str')\ntest['trafficSource.adContent'] = test['trafficSource.adContent'].map(lambda x:adcontents_mapping(str(x).lower())).astype('str')\ntest['trafficSource.source'] = test['trafficSource.source'].map(lambda x:source_mapping(str(x).lower())).astype('str')\n\ndef process_device(data_df):\n    print(\"process device ...\")\n    data_df['source.country'] = data_df['trafficSource.source'] + '_' + data_df['geoNetwork.country']\n    data_df['campaign.medium'] = data_df['trafficSource.campaign'] + '_' + data_df['trafficSource.medium']\n    data_df['browser.category'] = data_df['device.browser'] + '_' + data_df['device.deviceCategory']\n    data_df['browser.os'] = data_df['device.browser'] + '_' + data_df['device.operatingSystem']\n    return data_df\n\ntrain = process_device(train)\ntest = process_device(test)\n\ndef custom(data):\n    print('custom..')\n    data['device_deviceCategory_channelGrouping'] = data['device.deviceCategory'] + \"_\" + data['channelGrouping']\n    data['channelGrouping_browser'] = data['device.browser'] + \"_\" + data['channelGrouping']\n    data['channelGrouping_OS'] = data['device.operatingSystem'] + \"_\" + data['channelGrouping']\n    \n    for i in ['geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country','geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region','geoNetwork.subContinent']:\n        for j in ['device.browser','device.deviceCategory', 'device.operatingSystem', 'trafficSource.source']:\n            data[i + \"_\" + j] = data[i] + \"_\" + data[j]\n    \n    data['content.source'] = data['trafficSource.adContent'] + \"_\" + data['source.country']\n    data['medium.source'] = data['trafficSource.medium'] + \"_\" + data['source.country']\n    return data\n\ntrain = custom(train)\ntest = custom(test)","a4cc121f":"excluded_features = [\n    'date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', \n    'visitId', 'visitStartTime'\n]\n\ncategorical_features = [\n    _f for _f in train.columns\n    if (_f not in excluded_features) & (train[_f].dtype == 'object')\n]\n\nfor f in categorical_features:\n    train[f], indexer = pd.factorize(train[f])\n    test[f] = indexer.get_indexer(test[f])\ntrain.shape, test.shape","305d3178":"train_features = [_f for _f in train.columns if _f not in excluded_features]\n# X_train, X_test, y_train, y_test = train_test_split(train[train_features], y_reg, test_size=0.20, random_state=42)\n#X_train, y_train = train[train_features], y_reg for Bayesian Optimization","05f7bae9":"'''\ndef lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples,bagging_fraction,feature_fraction):\n    params = {\n    \"objective\" : \"regression\",\n    \"metric\" : \"rmse\", \n    \"num_leaves\" : int(num_leaves),\n    \"max_depth\" : int(max_depth),\n    \"lambda_l2\" : lambda_l2,\n    \"lambda_l1\" : lambda_l1,\n    \"num_threads\" : 2,\n    \"min_child_samples\" : int(min_child_samples),\n    \"learning_rate\" : 0.03,\n    \"bagging_fraction\" : bagging_fraction,\n    \"feature_fraction\" : feature_fraction,\n    \"subsample_freq\" : 5,\n    \"bagging_seed\" : 517,\n    \"verbosity\" : -1\n    }\n    lgtrain = lgb.Dataset(X_train, label=np.log1p(y_train.apply(lambda x : 0 if x < 0 else x)))\n    cv_result = lgb.cv(params,\n                       lgtrain,\n                       1500,\n                       categorical_feature=categorical_features,#category_features,\n                       early_stopping_rounds=100,\n                       stratified=False,\n                       nfold=5)\n    return -cv_result['rmse-mean'][-1]\n\ndef lgb_train(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples,bagging_fraction,feature_fraction):\n    params = {\n    \"objective\" : \"regression\",\n    \"metric\" : \"rmse\", \n    \"num_leaves\" : int(num_leaves),\n    \"max_depth\" : int(max_depth),\n    \"lambda_l2\" : lambda_l2,\n    \"lambda_l1\" : lambda_l1,\n    \"num_threads\" : 2,\n    \"min_child_samples\" : int(min_child_samples),\n    \"learning_rate\" : 0.03,\n    \"bagging_fraction\" : bagging_fraction,\n    \"feature_fraction\" : feature_fraction,\n    \"subsample_freq\" : 5,\n    \"bagging_seed\" : 517,\n    \"verbosity\" : -1\n    }\n    t_x,v_x,t_y,v_y = train_test_split(X_train,y_train,test_size=0.2)\n    lgtrain = lgb.Dataset(t_x, label=np.log1p(t_y.apply(lambda x : 0 if x < 0 else x)))\n    lgvalid = lgb.Dataset(v_x, label=np.log1p(v_y.apply(lambda x : 0 if x < 0 else x)))\n    model = lgb.train(params, lgtrain, 5000, valid_sets=[lgvalid], early_stopping_rounds=100, verbose_eval=100)\n    pred_test_y = model.predict(test_x, num_iteration=model.best_iteration)\n    return pred_test_y, model\n    \ndef param_tuning(init_points,num_iter,**args):\n    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 100),\n                                                'max_depth': (5, 20),\n                                                'lambda_l2': (0.0, 0.5),\n                                                'lambda_l1': (0.0, 0.5),\n                                                'bagging_fraction': (0.1, 0.99),\n                                                'feature_fraction': (0.1, 0.99),\n                                                'min_child_samples': (20, 50),\n                                                })\n\n    lgbBO.maximize(init_points=init_points, n_iter=num_iter,**args)\n    return lgbBO\n\nresult = param_tuning(8, 22)\nresult.res['max']['max_params']\n'''","26b000bf":"#extracted params\nparams={'learning_rate': 0.01,\n        'objective':'regression',\n        'metric':'rmse',\n        'num_leaves': 100,\n        'verbose': 1,\n        'bagging_fraction': 0.99,\n        'feature_fraction': 0.99,\n        \"random_state\":517,\n        'max_depth': 20,\n        \"bagging_seed\" : 517,\n        \"verbosity\" : -1,\n        \"bagging_frequency\" : 5,\n        'lambda_l2': 0,\n        'lambda_l1': 0.5,\n        'min_child_samples': 20\n       }","f513ab46":"folds = get_folds(df=train, n_splits=10)\n\ntrain_features = [_f for _f in train.columns if _f not in excluded_features]\n\noof_reg_preds = np.zeros(train.shape[0])\nsub_reg_preds = np.zeros(test.shape[0])\nfor fold_, (trn_, val_) in enumerate(folds):\n    print(\"Fold:\",fold_)\n    trn_x, trn_y = train[train_features].iloc[trn_], y_reg.iloc[trn_]\n    val_x, val_y = train[train_features].iloc[val_], y_reg.iloc[val_]\n    reg = lgb.LGBMRegressor(**params,\n         n_estimators=1500\n    )\n    reg.fit(\n        trn_x, np.log1p(trn_y),\n        eval_set=[(val_x, np.log1p(val_y))],\n        early_stopping_rounds=50,\n        verbose=100,\n        eval_metric='rmse'\n    )\n    \n    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_reg_preds[oof_reg_preds < 0] = 0\n    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_reg_preds += np.expm1(_preds) \/ len(folds)\n    \nmean_squared_error(np.log1p(y_reg), oof_reg_preds) ** .5","d0b3f19b":"train['predictions'] = np.expm1(oof_reg_preds)\ntest['predictions'] = sub_reg_preds\n\ntrn_data = train[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\n\ntrn_pred_list = train[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n    .apply(lambda df: list(df.predictions))\\\n    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})","5afa3191":"trn_all_predictions = pd.DataFrame(list(trn_pred_list.values), index=trn_data.index)\ntrn_feats = trn_all_predictions.columns\ntrn_all_predictions['t_mean'] = np.log1p(trn_all_predictions[trn_feats].mean(axis=1))\ntrn_all_predictions['t_median'] = np.log1p(trn_all_predictions[trn_feats].median(axis=1))\ntrn_all_predictions['t_sum_log'] = np.log1p(trn_all_predictions[trn_feats]).sum(axis=1)\ntrn_all_predictions['t_sum_act'] = np.log1p(trn_all_predictions[trn_feats].fillna(0).sum(axis=1))\ntrn_all_predictions['t_nb_sess'] = trn_all_predictions[trn_feats].isnull().sum(axis=1)\nfull_data = pd.concat([trn_data, trn_all_predictions], axis=1)\ndel trn_data, trn_all_predictions\ngc.collect()\nfull_data.shape","1e043596":"sub_pred_list = test[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n    .apply(lambda df: list(df.predictions))\\\n    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})","0496033a":"sub_data = test[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\nsub_all_predictions = pd.DataFrame(list(sub_pred_list.values), index=sub_data.index)\nfor f in trn_feats:\n    if f not in sub_all_predictions.columns:\n        sub_all_predictions[f] = np.nan\nsub_all_predictions['t_mean'] = np.log1p(sub_all_predictions[trn_feats].mean(axis=1))\nsub_all_predictions['t_median'] = np.log1p(sub_all_predictions[trn_feats].median(axis=1))\nsub_all_predictions['t_sum_log'] = np.log1p(sub_all_predictions[trn_feats]).sum(axis=1)\nsub_all_predictions['t_sum_act'] = np.log1p(sub_all_predictions[trn_feats].fillna(0).sum(axis=1))\nsub_all_predictions['t_nb_sess'] = sub_all_predictions[trn_feats].isnull().sum(axis=1)\nsub_full_data = pd.concat([sub_data, sub_all_predictions], axis=1)\ndel sub_data, sub_all_predictions\ngc.collect()\nsub_full_data.shape","ff2e6c1f":"train['target'] = y_reg\ntrn_user_target = train[['fullVisitorId', 'target']].groupby('fullVisitorId').sum()","9f8adf14":"del train, test, trn_pred_list, sub_pred_list\ngc.collect()","7f38ccc8":"full_columns = full_data.columns\ncategorical_features = list(set(categorical_features).intersection(set(full_columns)))\nnumerical_features = list(set(full_columns).difference(set(categorical_features)))\nfull_data[categorical_features] = full_data[categorical_features].fillna(-1)\nsub_full_data[categorical_features] = sub_full_data[categorical_features].fillna(-1)\nfull_data[numerical_features] = full_data[numerical_features].fillna(0)\nsub_full_data[numerical_features] = sub_full_data[numerical_features].fillna(0)","1eeb397b":"for num, category_name in enumerate(categorical_features):\n    unique, counts = np.unique(full_data[category_name], return_counts=True)\n    counts = (counts \/ counts.sum()).astype(np.float32)\n    for i in range(unique.shape[0]):\n        full_data.loc[full_data.loc[:, category_name] == unique[i], category_name] = counts[i]\n        sub_full_data.loc[sub_full_data.loc[:, category_name] == unique[i], category_name] = counts[i]\n    print('Column %i (of %i) processed!'%(num + 1, len(categorical_features)))","149d4184":"X_train = full_data.as_matrix().astype(np.float32)\nX_test = sub_full_data[full_data.columns].as_matrix().astype(np.float32)\ny_train = trn_user_target.as_matrix().astype(np.float32).ravel()","3315f386":"submission = pd.DataFrame({'PredictedLogRevenue': np.zeros(len(sub_full_data.index))},\n                          index=sub_full_data.index)\ndel full_data, sub_full_data, trn_user_target\ngc.collect()","b2e23f85":"n_folds = 20\nsub_preds = np.zeros(X_test.shape[0])\nreg = lgb.LGBMRegressor(**params,\n                        n_estimators=1500)\nkf = KFold(n_splits=n_folds, random_state=517, shuffle=True)\nfor train_idx, test_idx in kf.split(X_train):\n    reg.fit(X_train[train_idx], np.log1p(y_train[train_idx]),\n            eval_set=[(X_train[train_idx], np.log1p(y_train[train_idx])),\n                      (X_train[test_idx], np.log1p(y_train[test_idx]))],\n            eval_names=['TRAIN', 'VALID'],\n            early_stopping_rounds=50,\n            eval_metric='rmse',\n            verbose=101)\n    _preds = reg.predict(X_test, num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_preds += _preds \/ n_folds\n    print('Iteration completed!')","8d74980c":"submission['PredictedLogRevenue'] = sub_preds\nsubmission[['PredictedLogRevenue']].to_csv('submission.csv', index=True)","65f9306b":"## tuned hyperparameters","e103183b":"## saving predictions","79aa474d":"## training baseline model","c9e9943c":"## filling nulls","624df9d7":"## hyperparameters tuning","b7c32b6e":"## function for visitor-level cross validation","40d36fa6":"## training visitor-level model","fe89dd97":"## loading data","0925d769":"## forming visitor-level features","ae62f0e6":"# Introduction\n#### (preliminary notes: this kernel was created for educational purposes only)\nThis kernel is based on several kernels: [this](https:\/\/www.kaggle.com\/ashishpatel26\/bayesian-lgbm-xgb-cat-fe-groupkfold-cv) and [this](https:\/\/www.kaggle.com\/prashantkikani\/teach-lightgbm-to-sum-predictions-fe). Initial data was preprocessed using this [script](https:\/\/www.kaggle.com\/ogrellier\/create-extracted-json-fields-dataset). Optimization routine for hyperparameters estimation is based on this [kernel](https:\/\/www.kaggle.com\/qwe1398775315\/eda-lgbm-bayesianoptimization). My solution consists of these steps:\n* Extract preprocessed data\n* Add some features\n* Train baseline LightGBM model for session-level predictions\n* Encode categorical features using frequencies of categories\n* Train visitor-level LightGBM model to predict revenue\n","265d4468":"## target definition and feature extraction","a5de716d":"## encoding features"}}