{"cell_type":{"cf43a368":"code","2ae315bb":"code","3c43f964":"code","de40c708":"code","cf83dbe4":"code","3de3b09f":"code","1380b78f":"code","8b70808a":"code","c3ed5a6b":"code","3a87cc34":"code","46056990":"code","7be86b5b":"code","a138b02f":"code","05a5be9a":"code","8ff9ae99":"code","4f0751bb":"code","77f13d9e":"code","038d6443":"code","ac9409ac":"code","c74d8fef":"code","0aad19e5":"code","9470b6d1":"code","37d3ce81":"code","3ee7776e":"code","cf1e463f":"code","3df55ca7":"code","d24a6df6":"code","0bdc61e7":"code","36ca5e5c":"code","31f168a0":"code","8f955e78":"code","97267e7b":"code","8c8fef98":"code","ca1f43bc":"code","05957d52":"code","da70b634":"code","e36eec92":"code","93d62e15":"code","eff3049e":"code","430a246c":"code","bde7c0e1":"code","b11a90aa":"code","98c37f9b":"code","c89d8c31":"code","30c71552":"code","bcc1e711":"code","cd453756":"code","d69602f2":"code","a343cd85":"code","74d715af":"code","14cf1205":"code","0306b9da":"code","879490d4":"code","5f2f812b":"code","7ba77b96":"code","c97928b3":"code","aac73ea4":"code","529821a5":"code","f9816081":"code","286b5053":"code","592c7252":"code","4b8d4816":"code","66abf8ec":"code","d60446ac":"code","10344681":"code","7c338589":"code","86154c21":"markdown","d01f2a4f":"markdown","244b1bcf":"markdown","8b611e1b":"markdown","1002a01f":"markdown","71c792c3":"markdown","54ccb142":"markdown","cc02f043":"markdown","05989fb0":"markdown","ea9aff04":"markdown","79d587b5":"markdown","94857b0c":"markdown","923ae716":"markdown","89080f7f":"markdown","ba521b0c":"markdown","9257ca0b":"markdown","08a828b0":"markdown","81576896":"markdown","6f114e2f":"markdown","c52df6e1":"markdown","eb359529":"markdown","793c4362":"markdown","02afaadb":"markdown","4ee425aa":"markdown","48ddb432":"markdown","cd14a10d":"markdown","1076b9b5":"markdown","0179df5a":"markdown","c507e374":"markdown","25011c05":"markdown","05975674":"markdown","9ef18c59":"markdown","6291f2e6":"markdown","eac458ab":"markdown","eec33289":"markdown","11ea409f":"markdown","49533344":"markdown","3427a2ca":"markdown","adaa25ca":"markdown","e64f559e":"markdown","0ec1946b":"markdown","24dffb35":"markdown","4dae6cc1":"markdown","b9360fb2":"markdown","c383700f":"markdown","edfb64bd":"markdown","a4f4a039":"markdown","262dab6a":"markdown"},"source":{"cf43a368":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom IPython.display import display  \nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# for data preprocessing\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix,\\\ncohen_kappa_score, plot_confusion_matrix, roc_curve\n\n# import different classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm\nimport catboost\nimport xgboost\nfrom sklearn.neural_network import MLPClassifier","2ae315bb":"df = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")","3c43f964":"def data_explore(df):\n    display(df.head())\n    print(\"*\" * 30)\n    print(f\"shape of dataset {df.shape}\")\n    print(\"*\" * 30)\n    display(\"Info {}\".format(df.info()))\n    print(\"*\" * 30)\n    print(\"Dtypes: \\n{}\".format(df.dtypes.value_counts()))\n    print(\"*\" * 30)\n    print(df.columns)\n    print(\"*\" * 30)\n    print(\"Number of columns having null values: \", df.isnull().any().sum())\ndata_explore(df)","de40c708":"# describe for all numeric variables\ndf.describe().T","cf83dbe4":"# describe for all categorical variables\ndf.describe(include=['object']).T","3de3b09f":"# data type plots\nfig, ax = plt.subplots(1,2,figsize = (12,6))\n\ndf.dtypes.value_counts().plot.pie(explode = [0.05,0.05], autopct = \"%1.0f%%\",\n                                  shadow = True, ax = ax[1])\nax[1].set_title(\"datatype\")\n\ndf.dtypes.value_counts().plot(kind = 'bar', ax = ax[0])\nax[0].set_title(\"datatype\")","1380b78f":"# missing values in the target variable\ndf['RainTomorrow'].isnull().sum()","8b70808a":"df.dropna(subset=['RainTomorrow'], axis = 0, inplace = True)\n\ndf['RainTomorrow'].isnull().sum()","c3ed5a6b":"display(df['RainTomorrow'].value_counts())\ndisplay(df['RainTomorrow'].value_counts() * 100 \/ len(df))\n\ndf['RainTomorrow'].value_counts().plot(kind = 'bar', color = ['skyblue', 'navy'], rot = 0)","3a87cc34":"# conversion of target variable from categorical to numeric\ndf['RainTomorrow'] = df['RainTomorrow'].map({'No': 0, 'Yes': 1})","46056990":"cat_cols = df.select_dtypes('object').columns\ncat_cols","7be86b5b":"# number of unique values in each categorical column\nfor col in cat_cols:\n    print(col,\"\\t\", df[col].nunique())","a138b02f":"# number of missing values in each categorical column\nfor col in cat_cols:\n    print(col, '\\t', df[col].isnull().sum())","05a5be9a":"# frequency plot of each categorical variable\nplt.figure(figsize = (20,8))\nfor i, col in enumerate(cat_cols[1:]):\n    plt.subplot(2, 3, i+1)\n    sns.countplot(df[col])\n    plt.xticks(rotation = 90)\n    plt.title(f\"{col} has {df[col].nunique()} unique values\")","8ff9ae99":"df['Date'] = pd.to_datetime(df['Date'])\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['Day'] = df['Date'].dt.day\ndf[['Date', 'Year', 'Month', 'Day']].head()","4f0751bb":"# Now lets drop Date column\ndf.drop('Date', axis = 1, inplace = True)","77f13d9e":"# update cat_cols\ncat_cols = df.select_dtypes('object').columns\ncat_cols","038d6443":"# numerical columns\nnum_cols = df.select_dtypes(exclude=['object']).columns\nnum_cols = num_cols[:-4]\nnum_cols","ac9409ac":"# missing value check\ndf[num_cols].isnull().sum()","c74d8fef":"# Outlier check\ndf[num_cols].describe().T","0aad19e5":"# box plot of numerical variables\nplt.figure(figsize = (15,12))\nfor i, col in enumerate(num_cols):\n    plt.subplot(4, 4, i+1)\n    sns.boxplot(data = df, y = col, whis = 3)\n    plt.title(col)","9470b6d1":"outlier_cols = ['Rainfall', 'Evaporation', 'WindSpeed9am', 'WindSpeed3pm']\n# histogram plot to check distribution\nplt.figure(figsize = (12,10))\nfor i, col in enumerate(outlier_cols):\n    plt.subplot(2, 2, i+1)\n    sns.histplot(data = df,x = col, bins = 20)\n    plt.title(col)","37d3ce81":"def IQR(df, out_cols):\n    for col in out_cols:\n        iqr = df[col].quantile(0.75) - df[col].quantile(0.25)\n        lower =  df[col].quantile(0.25) - (iqr * 3)\n        upper = df[col].quantile(0.75) + (iqr * 3)\n        outlier_percent = round((df[df[col] > upper].shape[0] * 100)\/len(df), 2)\n        print( col , '\\t', lower.round(2), '\\t', upper.round(2), \n              '\\t', df[col].min(), '\\t', df[col].max(), '\\t', outlier_percent)\nprint('column \\t\\t lower \\t high \\t min \\t max \\t outlier_percent')\nIQR(df, outlier_cols)","3ee7776e":"# Heatmap\nsns.set_context('notebook', font_scale=1.0, rc = {'lines.linewidth': 2.5})\nplt.figure(figsize = (15,12))\n\n# mask the duplicate correlation values\nmask = np.zeros_like(df.corr())\nmask[np.triu_indices_from(mask, 1)] = True\n\na = sns.heatmap(df.corr(), mask = mask, annot=True, fmt = '.2f', cmap = 'viridis')\n\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation = 90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation = 30)\n","cf1e463f":"# Pair Plot for higly correlated variables\nsns.pairplot(data = df, vars = ['MinTemp', 'MaxTemp', 'Temp9am', 'Temp3pm', 'WindGustSpeed', 'WindSpeed3pm', 'Pressure9am', 'Pressure3pm'], \n             kind = 'scatter', \n             diag_kind= 'hist',\n             hue = 'RainTomorrow')","3df55ca7":"df['RainTomorrow'].value_counts().plot(kind='bar',color=['blue', 'cyan'])","d24a6df6":"no = df[df['RainTomorrow'] == 0]\nyes = df[df['RainTomorrow'] == 1]\n\nyes_os = resample(yes, replace = True, n_samples=len(no), random_state=21)\n\ndf_os = pd.concat([no, yes_os])\nprint(df_os.shape)","0bdc61e7":"fig = plt.figure(figsize = (8,5))\ndf_os['RainTomorrow'].value_counts(normalize = True).plot(kind = 'bar', \n                                                         color = ['skyblue', 'navy'], \n                                                         alpha = 0.9, \n                                                         rot = 0)\nplt.title('balanced dataset')","36ca5e5c":"X = df_os.drop(['RainTomorrow'], axis = 1)\ny = df_os['RainTomorrow']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\nX_train.shape, X_test.shape","31f168a0":"# categorical and numeric missing values in train and test datasets\n\ncat_miss = pd.concat([pd.DataFrame(X_train[cat_cols].isnull().sum()),\n                      pd.DataFrame(X_test[cat_cols].isnull().sum())],\n                     axis = 1)\nnum_miss = pd.concat([pd.DataFrame(X_train[num_cols].isnull().sum()),\n                      pd.DataFrame(X_test[num_cols].isnull().sum())],\n                     axis = 1)\ncat_miss.columns = ['train', 'test']\nnum_miss.columns = ['train', 'test']\ndisplay(cat_miss)\ndisplay(num_miss)\n","8f955e78":"# imputing missing values of numeric columns\nfor df1 in [X_train, X_test]:\n    for col in num_cols:\n        col_median = X_train[col].median()\n        df1[col].fillna(col_median, inplace = True)","97267e7b":"X_train[num_cols].isnull().any().sum(), X_test[num_cols].isnull().any().sum()","8c8fef98":"# imputing misssing values in categorical variables\nfor df1 in [X_train, X_test]:\n    for col in cat_cols[1:]:\n        col_mode = X_train[col].mode()[0]\n        df1[col].fillna(col_mode, inplace = True)","ca1f43bc":"X_train[cat_cols].isnull().any().sum(), X_test[cat_cols].isnull().any().sum()","05957d52":"X_train.isnull().any().sum(), X_test.isnull().any().sum()","da70b634":"# treating outliers\ndef max_value(df, col, top):\n    return np.where(df[col]> top, top, df[col])\nfor df in [X_train, X_test]:\n    df['Rainfall'] = max_value(df, 'Rainfall', 3.2)\n    df['Evaporation'] = max_value(df, 'Evaporation', 21.8)\n    df['WindSpeed9am'] = max_value(df, 'WindSpeed9am', 55.0 )\n    df['WindSpeed3pm'] = max_value(df, 'WindSpeed3pm', 57.0)","e36eec92":"X_train['Rainfall'].max(), X_test['Rainfall'].max()","93d62e15":"cat_cols","eff3049e":"for col in cat_cols:\n    print(col , '\\t', X_train[col].nunique())","430a246c":"num_cols = X_train.select_dtypes(exclude= 'object').columns\n\nX_train[num_cols].shape, X_train[cat_cols].shape, X_train.shape","bde7c0e1":"# categorical encoding for training dataset\nX_train.shape\nX_train = pd.concat([X_train[num_cols], \n                   pd.get_dummies(X_train['Location'], drop_first=True), \n                   pd.get_dummies(X_train['WindGustDir'], drop_first=True, prefix = 'WindGustDir'), \n                   pd.get_dummies(X_train['WindDir9am'], drop_first=True, prefix = 'WD9am'),\n                pd.get_dummies(X_train['WindDir3pm'], drop_first=True, prefix = 'WD3pm'), \n                    pd.get_dummies(X_train['RainToday'], drop_first=True, prefix ='RainToday')], \n                    axis = 1\n                   )\nX_train.shape","b11a90aa":"X_train.head(2)","98c37f9b":"# categorical encoding for test dataset\nX_test.shape\nX_test = pd.concat([X_test[num_cols], \n                   pd.get_dummies(X_test['Location'], drop_first=True), \n                   pd.get_dummies(X_test['WindGustDir'], drop_first=True, prefix = 'WindGustDir'), \n                   pd.get_dummies(X_test['WindDir9am'], drop_first=True, prefix = 'WD9am'),\n                   pd.get_dummies(X_test['WindDir3pm'], drop_first=True, prefix = 'WD3pm'), \n                   pd.get_dummies(X_test['RainToday'], drop_first=True, prefix ='RainToday')], \n                   axis = 1\n                   )\nX_test.shape","c89d8c31":"X_train.describe()","30c71552":"cols = X_train.columns","bcc1e711":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","cd453756":"# after scaling the dataframe is converted into np array \nX_train = pd.DataFrame(X_train, columns=[cols])\nX_test = pd.DataFrame(X_test, columns = [cols])","d69602f2":"X_train.describe()","a343cd85":"def plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color = 'red', label = 'ROC')\n    plt.plot([0,1], [0,1], color = 'navy', linestyle = '--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curver')\n    plt.legend()\n    plt.show()","74d715af":"# General method for model training\ndef model_run(clf, X_train, y_train, X_test, y_test, verbose = 1):\n    tic = time.time()\n    \n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    y_pred_train = clf.predict(X_train)\n    \n    accuracy_train = accuracy_score(y_train, y_pred_train)\n    accuracy_test = accuracy_score(y_test, y_pred)\n    roc_auc = roc_auc_score(y_test, y_pred)\n    coh_kap = cohen_kappa_score(y_test, y_pred)\n    \n    toc = time.time()\n    time_taken = toc-tic\n    \n    print(\"Training Accuracy = {}\".format(accuracy_train.round(2) * 100))\n    print(\"Test Accuracy = {}\".format(accuracy_test.round(2) * 100))\n    print(\"ROC Area under Curve = {}\".format(roc_auc.round(2)))\n    print(\"Cohen's Kappa = {}\".format(coh_kap.round(2)))\n    print(\"Time taken = {}\".format(time_taken))\n    print(classification_report(y_test,y_pred,digits=5))\n    \n    probs = clf.predict_proba(X_test)[:,1]\n    \n    fpr, tpr, threshold = roc_curve(y_test,probs)\n    plot_roc_curve(fpr, tpr)\n    \n    plot_confusion_matrix(clf, X_test, y_test, cmap=plt.cm.Blues, normalize='all')\n    \n    return clf, accuracy_train,accuracy_test, roc_auc, coh_kap, time_taken","14cf1205":"# Logistic regression\nclf_lr = LogisticRegression()\nclf_lr, acc_tr_lr, acc_lr, roc_lr, coh_kap_lr, time_lr = model_run(clf_lr, X_train, y_train, X_test, y_test)","0306b9da":"\nclf_knn = KNeighborsClassifier()\nclf_knn, acc_tr_knn, acc_knn, roc_knn, coh_kap_knn, time_knn = model_run(clf_knn, X_train, y_train, X_test, y_test)","879490d4":"# Decision Tree\nparam_dt = {'max_depth': 16, 'max_features': 'sqrt'}\n\nclf_dt = DecisionTreeClassifier(**param_dt)\nclf_dt, acc_tr_dt, acc_dt, roc_dt, coh_kap_dt, time_dt = model_run(clf_dt, X_train, y_train, X_test, y_test)","5f2f812b":"params_rf = {'max_depth': 16,\n             'min_samples_leaf': 1,\n             'min_samples_split': 2,\n             'n_estimators': 200,\n             'random_state': 21}\nclf_rf = RandomForestClassifier(**params_rf)\nclf_rf, acc_tr_rf, acc_rf, roc_rf, coh_kap_rf, time_rf = model_run(clf_rf, X_train, y_train, X_test, y_test)","7ba77b96":"params_lgbm = {'colsample_bytree': 0.95, \n         'max_depth': 16, \n         'min_split_gain': 0.1, \n         'n_estimators': 200, \n         'num_leaves': 50, \n         'reg_alpha': 1.2, \n         'reg_lambda': 1.2, \n         'subsample': 0.95, \n         'subsample_freq': 20}\nclf_lgbm = lightgbm.LGBMClassifier(**params_lgbm)\nclf_lgbm, acc_tr_lgbm, acc_lgbm, roc_lgbm, coh_kap_lgbm, time_lgbm = model_run(clf_lgbm, X_train.values, y_train.values, X_test, y_test)","c97928b3":"params_cboost ={'iterations': 50,\n            'max_depth': 16}\nclf_cbst = catboost.CatBoostClassifier(**params_cboost)\nclf_cbst, acc_tr_cbst, acc_cbst, roc_cbst, coh_kap_cbst, time_cbst = model_run(clf_cbst, X_train, y_train, X_test, y_test, verbose=0)","aac73ea4":"params_xgb ={'max_depth': 16}\nclf_xgb = xgboost.XGBClassifier(**params_xgb)\nclf_xgb, acc_tr_xgb,acc_xgb, roc_xgb, coh_kap_xgb, time_xgb = model_run(clf_xgb, X_train, y_train, X_test, y_test)","529821a5":"clf_nn = MLPClassifier(random_state=21, verbose=0)\nclf_nn, acc_tr_nn, acc_nn, roc_nn, coh_kap_nn, time_nn = model_run(clf_nn, X_train, y_train, X_test, y_test)","f9816081":"# comparison of accuracy, roc score, coh_kappa score and time\nacc_all = [acc_lr, acc_knn,acc_dt, acc_rf, acc_lgbm, acc_cbst, acc_xgb, acc_nn]\nacc_tr_all = [acc_tr_lr, acc_tr_knn,acc_tr_dt, acc_tr_rf, acc_tr_lgbm, acc_tr_cbst, acc_tr_xgb, acc_tr_nn]\nroc_all = [roc_lr, roc_knn, roc_dt, roc_rf, roc_lgbm, roc_cbst, roc_xgb, roc_nn]\ncoh_kap_all = [coh_kap_lr, coh_kap_knn, coh_kap_dt, coh_kap_rf, coh_kap_lgbm, coh_kap_cbst, coh_kap_xgb, coh_kap_nn]\ntime_taken = [time_lr, time_knn, time_dt, time_rf, time_lgbm, time_cbst, time_xgb, time_nn]\nmodels = ['Logistic Regression','KNN','Decision Tree','Random Forest','LightGBM','Catboost','XGBoost', 'Neural Network' ]\n\nmodel_comp_df = pd.DataFrame({'Model': models,\n                              'Train Accuracy' : acc_tr_all,\n                          'Test Accuracy': acc_all, \n                          'ROC_AUC': roc_all,\n                          'Cohen_kappa': coh_kap_all,\n                          'Time_taken': time_taken})\n\nmodel_comp_df.style.background_gradient(cmap='Blues')\nmodel_comp_df.to_csv('balanced_summery.csv')","286b5053":"fig = plt.figure(figsize = (15, 12))\n\n# plot of Test Accuracy of every model\nplt.subplot(221)\nsns.barplot(data = model_comp_df, x = 'Model', y = 'Test Accuracy', palette = 'winter')\nplt.title('Test Accuracy of classifier')\nplt.xticks(rotation = 90)\nplt.ylim(0.5, 1.0)\n\n# plot of Time of every model\nplt.subplot(222)\nsns.barplot(data = model_comp_df, x = 'Model', y = 'Time_taken', palette = 'summer' )\nplt.title('Time taken by classifier')\nplt.xticks(rotation = 90)\n\n# plot of ROC of every model\nplt.subplot(223)\nsns.barplot(data = model_comp_df, x = 'Model', y = 'ROC_AUC', palette = 'winter')\nplt.title('ROC-AUC Score of classifier')\nplt.xticks(rotation = 90)\nplt.ylim(0.5, 1.0)\n\n# plot of Cohen_kappa of every model\nplt.subplot(224)\nsns.barplot(data = model_comp_df, x = 'Model', y = 'Cohen_kappa', palette = 'summer')\nplt.title('Cohen_kappa Score of classifier')\nplt.xticks(rotation = 90)\nplt.ylim(0.5, 1.0)\n\nplt.savefig('balanced_plots.jpeg')","592c7252":"model_comp_df['Train Accuracy'] = model_comp_df['Train Accuracy'].apply(lambda x: round(x, 3))\nmodel_comp_df['Test Accuracy'] = model_comp_df['Test Accuracy'].apply(lambda x: round(x, 3))\nmodel_comp_df[['Model','Train Accuracy', 'Test Accuracy']] ","4b8d4816":"# calculating the most important features\nimportance = clf_xgb.feature_importances_\n\nfeat_imp_df = pd.DataFrame({'Features': cols, 'Importance': importance})\n\nf_imp = feat_imp_df[feat_imp_df.sort_values('Importance', ascending=False)['Importance'] >= 0.01].reset_index(drop= True)\nf_imp.shape","66abf8ec":"plt.figure(figsize = (20, 8))\nf_imp.plot.bar( x = 'Features', y = 'Importance')","d60446ac":"# model performance on important features\nimp_feat = f_imp['Features']\n\nclf_xgb_imp = xgboost.XGBClassifier(**params_xgb)\nclf_xgb_imp, acc_tr_xgb_imp,acc_xgb_imp, roc_xgb_imp, coh_kap_xgb_imp, time_xgb_imp = model_run(clf_xgb_imp, X_train[imp_feat], y_train, X_test[imp_feat], y_test)","10344681":"bal_clf_summ = pd.read_csv('..\/input\/balance-model-summary\/balanced_summery.csv', index_col = 0)\nbal_clf_summ","7c338589":"unbal_clf_summ = pd.read_csv('..\/input\/unbalance-model-summary-rainfall\/unbalanced_summery.csv', index_col=0)\nunbal_clf_summ","86154c21":"Columns `Rainfall`, `Evaporation`, `WindSpeed9am`, `WindSpeed3am` mainly contains outliers","d01f2a4f":"The normal range for `Rainfall` is -2.4 to 3.2 while its min and max values are 0 and 371 so we can limit the higher values only upto 3.2","244b1bcf":"## 13. Model Comparison  <a class=\"anchor\" id=\"13\"><\/a>","8b611e1b":"### 4. Random Forest  <a class=\"anchor\" id=\"12.4\"><\/a>","1002a01f":"## 11. Feature Scaling <a class=\"anchor\" id=\"11\"><\/a>","71c792c3":"## 1. Problem statement <a class=\"anchor\" id=\"1\"><\/a>    \nIn this notebook, the problem is to predict that whether or not it will rain tomorrow in Australia.    \nIn order to solve this problem, my approach is to build different classifiers (binary classifiers) and compare them to get the best classifier. Initially, I will explore and process the data and then implement classifiers.    \nSo, let's start this story.","54ccb142":"**Findings**    \n* Target variable has 3267 Nan values which were removed from the dataset.\n* 1103116 entries for `No` variable (77.58%)\n* 31877 entries for `Yes` variable  (22.41%)    \n    The dataset is highly imbalanced \n","cc02f043":"## 8. Handling Class Imbalance <a class=\"anchor\" id=\"8\"><\/a>","05989fb0":"### 5. LGBM  <a class=\"anchor\" id=\"12.5\"><\/a>","ea9aff04":"**Observations**    \n* Logistic regression has high bias and low variance.\n* XGBoost has low bias and low variance.\n\nHence XGBoost is considered as most optimized model for this problem. ","79d587b5":"### 7. XGBoost  <a class=\"anchor\" id=\"12.7\"><\/a>","94857b0c":"**Distribution check**   \n* To check whether the distibution is normal or skewed. ","923ae716":"In the dataset, 70% features are numeric (float 64) while rest 30% are categorical (object). ","89080f7f":"### 2. KNN  <a class=\"anchor\" id=\"12.2\"><\/a>","ba521b0c":"#### 2. Treating outliers <a class=\"anchor\" id=\"10.2\"><\/a>\n  * The max value of 4 columns having outlier is to change to the upper limit of IQR.","9257ca0b":"**Observation**\nIf we compare the model accuracy before and after balancing the minority class then it is observed that: \n* Models like Logisitic Regression and Decision tree perform better on unbalanced dataset.\n* Models including Catboost and XGBoost show high variance in case of unbalanced dataset as compared to balanced dataset.\n* Model Random Forest perform almost same in both the cases.\n\n<span style=\"color:blue\">**Hence Random Forest can be considered as good model in this case with low bias and low variance**\n","08a828b0":"Now before diving into Model training we should map all the feature variables onto the same scale using `feature scaling`. ","81576896":"## Table of Content\n\n1. [Problem statement](#1)\n1. [Import Libraries and dataset](#2)\n1. [Exploratory data analysis (EDA)](#3)\n1. [ Explore Target Variable `RainTomorrow`](#4)\n1. [Explore Categorical Variables](#5)\n1. [Explore Numerical Variables](#6)\n1. [Multivariate Analysis](#7)\n1. [Handling Class Imbalance](#8)\n1. [Splitting of data](#9)\n1. [Feature Engineering](#10)\n1. [Feature Scaling](#11)\n1. [Model training, making predictions and evaluation](#12)\n    - [Logistic Regression](#12.1)\n    - [KNN](#12.2)\n    - [Decision Tree](#12.3)\n    - [Random Forest](#12.4)\n    - [lightGBM](#12.5)\n    - [Catboost](#12.6)\n    - [XGBoost](#12.7)\n    - [Neural Network](#12.8)\n1. [Model Comparison](#13)\n1. [Bias and variance](#14)\n1. [Feature Importance](#15)\n1. [Model Performance on Imbalance dataset](#16)\n    \n    ","6f114e2f":"### 1. Logistic Regression <a class=\"anchor\" id=\"12.1\"><\/a>","c52df6e1":"There are 3267 entries in the dataset where target variable `RainTomorrow` is null.    \nWe can't imput these missing values so the only option left is to drop all the enties where target variable is `NaN`.","eb359529":"In order to observe the effect of balancing the dataset I have trained all the classifiers on imbalanced dataset and obtained the model comparison summary. ","793c4362":"**Observation**    \nNow max and min value of all the features is 1, 0 and count of all is same indicating the absence of null values. Now our training and test dataset is ready for model build. ","02afaadb":"We have 7 categorical columms ('Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm','RainToday')    \nLets explore these first\n","4ee425aa":"## 7. Multivariate Analysis <a class=\"anchor\" id=\"7\"><\/a>\n   * To discover patterns and relationships between variables in the dataset. \n   * Heatmap of correlation \n   * Pairplot to see the patterns","48ddb432":"## 6. Explore Numerical Variables <a class=\"anchor\" id=\"6\"><\/a>\n   * missing values check\n   * outlier check\n   * distribution check","cd14a10d":"**Observations**    \n* For the dataset containing most important features obtained from XGBoost, the test accuracy is decreased for 94 to 88. ","1076b9b5":"**Observations**     \nThere are few variables ('MinTemp', 'MaxTemp', 'Temp9am', 'Temp3pm', 'WindGustSpeed', 'WindSpeed3pm', 'Pressure9am', 'Pressure3pm') which have high correlation with other variables while none with 100% correlation so no need to remove any features.","0179df5a":"The target is highly imbalance so we can either increase the minor class samples or decrease the major class samples. Here I will use oversampling of the minority class.","c507e374":"## 5. Explore Categorical Variables <a class=\"anchor\" id=\"5\"><\/a>\n   * Unique values \n   * number of missing values\n   * frequency plot","25011c05":"## 2. Import Libraries and dataset <a class=\"anchor\" id=\"2\"><\/a>","05975674":"### 6. CatBoost  <a class=\"anchor\" id=\"12.6\"><\/a>","9ef18c59":"<span style=\"color:red\">I hope you Liked my kernel. An upvote is a gesture of appreciation and encouragement that fills me with energy to keep improving my efforts ,be kind to show one.","6291f2e6":"## 10. Feature Engineering <a class=\"anchor\" id=\"10\"><\/a>\n   1. Imputing missing values\n       - In numerical variables\n       - In categorical variables   \n   2. Treating outliers\n   3. Encoding of categorical variables","eac458ab":"#### 1. Imputing Missing values <a class=\"anchor\" id=\"10.1\"><\/a>  \n  * Numeric missing values are imputed with median of training dataset as median is more robust to outliers and there would be no data leakage because imputing all the missing values with median of **training dataset**\n  * Categorical missing values are imputed with mode of training dataset.","eec33289":"## 15. Feature Importance <a class=\"anchor\" id=\"15\"><\/a>","11ea409f":"All the missing values have been imputed, now we will treat the outliers","49533344":"## 12. Model training, making predictions and evaluation <a class=\"anchor\" id=\"12\"><\/a>\n    1. Logistic Regression\n    2. KNN\n    3. Decision Tree\n    4. Random Forest\n    5. lightGBM\n    6. Catboost\n    7. XGBoost\n    8. Neural network","3427a2ca":"## 4. Explore Target Variable `RainTomorrow`  <a class=\"anchor\" id=\"4\"><\/a>","adaa25ca":"## 8. Neural Network  <a class=\"anchor\" id=\"12.8\"><\/a>","e64f559e":"#### 3. Encoding Categorical Variables <a class=\"anchor\" id=\"10.3\"><\/a>\nThere are 5 categorical variables which have to be encoded, In this, I am using `get_dummies` with drop first. ","0ec1946b":"## 16. Model Performance on Imbalance dataset <a class=\"anchor\" id=\"16\"><\/a>","24dffb35":"## 3. Exploratory data analysis (EDA) <a class=\"anchor\" id=\"3\"><\/a>","4dae6cc1":"## 9. Splitting of data <a class=\"anchor\" id=\"9\"><\/a>","b9360fb2":"## 14. Bias and variance of different models <a class=\"anchor\" id=\"14\"><\/a>","c383700f":"**Observation**    \nThe distribution is skewed in case of all these columns so we will find IQR (Interquantile range) to detect ouliers.","edfb64bd":"### 3. Decision Tree  <a class=\"anchor\" id=\"12.3\"><\/a>","a4f4a039":"**Result Discussion**    \n1. Among all the classifiers, XGBoost, Catboost and Random Forest perform best with high accuracy score.\n2. On comparing the time_taken of all the classifiers, it was observed that KNN is taking highest time followed by neural network. \n3. Trends of ROC-AUC score and Cohen_kappa score is similar to that of accuracy.  \n\n<span style=\"color:blue\">**On considering time and accuracy, the best classifier for the current problem is XGBoost.** ","262dab6a":"**Date**   \n* Date column is categorical and it would not provide any significance if as such. So we could convert it into datetime format and then create separate feature of year, month, and day. "}}