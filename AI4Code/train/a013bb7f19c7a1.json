{"cell_type":{"c198dd11":"code","3ab963c4":"code","558e85f8":"code","1518e235":"code","0ce37089":"code","9f00eba5":"code","7221f948":"code","fc416b1b":"code","79ccea8e":"code","cff0148d":"code","0a9f3943":"code","8647860b":"code","87d46874":"code","72d5a7c1":"code","aed7f292":"code","28b3bb26":"code","3793f577":"code","1cebde00":"code","d4371f34":"code","89b6058c":"code","4ec28c55":"code","9995b82f":"code","a7202cbf":"code","4091739c":"code","3d31600b":"code","d611243c":"code","60a82f46":"code","83ec2c4f":"code","4aa94c7d":"code","924551a1":"code","75be325a":"code","575d4ec2":"code","bacb4d37":"code","ca5e2413":"code","68cef705":"code","8ea4eb0d":"code","feb7e2fd":"code","618e6dd5":"code","a0f3580e":"code","2155d1ab":"code","adea5564":"code","e36492c0":"code","1f00416b":"code","d1dfe042":"code","9e2403ca":"code","3bf2966d":"code","c845797d":"code","805e8532":"code","ff1176f4":"code","987824e6":"code","adae8bae":"code","e4db10e9":"code","2b59d474":"code","bf561847":"code","5e9e22c5":"code","54161394":"code","0adace45":"code","7f7b1018":"code","529c0b42":"code","32c5b7bd":"code","c1bb3fdc":"code","2fc0a248":"code","545e64af":"code","6042e0cf":"code","6e9d7729":"code","50ccd959":"code","64c44ffe":"code","71ada7e8":"code","74e2cb8b":"code","f8a62e21":"code","e77ef6bd":"code","07cf5310":"code","a46f6592":"code","47cc897a":"code","ebca0816":"code","a30db41e":"markdown","c6fdbc7b":"markdown","1a3676c4":"markdown","df38b822":"markdown","5133b7cb":"markdown","c2f46de3":"markdown","322a1d8e":"markdown","367a3b17":"markdown","c955f218":"markdown","af2046cd":"markdown","0fdc8dad":"markdown"},"source":{"c198dd11":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#This is an updated list aimed at responding to the round 2 challenge\n#Not used in the end\n\ndf_queries = pd.DataFrame({'question': [\\\n'Effectiveness of drugs being developed and tried to treat hypercoagulable states',\\\n'Clinical and bench trials to investigate treatment of hypercoagulable states',\\\n'What is the best method to combat the hypercoagulable state seen in COVID-19',\\\n]})","3ab963c4":"\nimport numpy as np # linear algebra\nimport pdb\nimport os\nimport nltk, string\nfrom nltk.corpus import stopwords \nfrom nltk.stem import WordNetLemmatizer\n\nimport covid19_tools as cvt\n\nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\nstop_words = set(stopwords.words('english'))\n\n'''remove punctuation, lowercase, stem'''\nremove_punctuation_map = dict((ord(char), ' ') for char in string.punctuation)    \ndef normalize(text):\n    return nltk.word_tokenize(text.lower().translate(remove_punctuation_map))\n\ndef clean_text(text):\n    text = text.lower().translate(remove_punctuation_map)\n    \n    return ' '.join(lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text))\n\n#From Andy White utility script. Thank you!\n\ndef abstract_title_filter(df, search_string):\n    return (df.Abstract.str.lower().str.replace('-', ' ')\n            .str.contains(search_string, na=False) |\n            df.Title.str.lower().str.replace('-', ' ')\n            .str.contains(search_string, na=False))\n\n\ncovid19_synonyms = ['covid',\n                    'coronavirus disease 19',\n                    'sars cov 2', # Note that search function replaces '-' with ' '\n                    '2019 ncov',\n                    '2019ncov',\n                    r'2019 n cov\\b',\n                    r'2019n cov\\b',\n                    'ncov 2019',\n                    r'\\bn cov 2019',\n                    'coronavirus 2019',\n                    'wuhan pneumonia',\n                    'wuhan virus',\n                    'wuhan coronavirus',\n                    r'coronavirus 2\\b']\n\ndef count_and_tag(df: pd.DataFrame,\n                  synonym_list: list,\n                  tag_suffix: str) -> (pd.DataFrame, pd.Series):\n    counts = {}\n    df[f'tag_{tag_suffix}'] = False\n    for s in synonym_list:\n        synonym_filter = abstract_title_filter(df, s)\n        counts[s] = sum(synonym_filter)\n        df.loc[synonym_filter, f'tag_{tag_suffix}'] = True\n    print(f'Added tag_{tag_suffix} to DataFrame')\n    return df, pd.Series(counts)\n#ends\n\ndf=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')\nprint ('Size of literature Set on 10th June 138,794,19')\nprint ('Size of literature Set', df.shape)\n#'Size of literature Set on 10th April 51078,18'\n","558e85f8":"import random\nrandom.seed(42)\n","1518e235":"\ndf = df.sort_values(by='publish_time',ascending=True)\n#to include only the most recent papers\ndf = df.loc[df['publish_time'] > '2020']\ndf.shape\n","0ce37089":"df = df.rename(columns={'source_x': 'Source', 'title': 'Title', 'abstract': 'Abstract', 'publish_time': 'Publish_Date', 'authors': 'Authors', 'journal': 'Journal', 'url': 'Ref URL'})\n#drop duplicate abstracts\ndf = df.drop_duplicates(subset='Abstract', keep=\"first\")\n\nprint ('Size of literature Set after removing duplicates on 10th June 23718,19')\nprint ('Size after removing duplicates', df.shape)\n#4\/3\/20 38667,18\n#Size of literature Set after removing duplicates on 10th April 41952,18","9f00eba5":"#Clean the text\ndf_queries['query_bow'] = df_queries.question.apply(clean_text)\ndf_queries['query_bow'] = df_queries['query_bow'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n\n#Only include papers that reference covid-19\ndf_a, covid19_counts = count_and_tag(df, covid19_synonyms, 'disease_covid19')\ndf_covid19 = df[df['tag_disease_covid19'] == True ]\ndf_covid19 = df_covid19.reset_index()\ndf_covid19 = df_covid19.drop(['index'], axis=1)\n\n#dont limit abstracts\n#limit Abstract to 3500 words\n#df_covid19[\"Abstract\"] = df_covid19[\"Abstract\"].str[:3500]\n\n#Split the abstract into sentences\ndf_covid19['org_abstract'] = df_covid19['Abstract']\ndf_covid19_by_sentence = df_covid19.set_index(df_covid19.columns.drop('org_abstract',1).tolist())\\\n.org_abstract.str.split('\\. ', expand=True).stack().reset_index()\\\n.rename(columns={0:'Sent Abstract'})\n","7221f948":"df_covid19.shape","fc416b1b":"print(covid19_counts)","79ccea8e":"df_by_sentence = df_covid19_by_sentence.copy()\n#df_covid19_bow_full ['bow_raw'] = df_covid19_bow_full ['title'] + \" \" + df_covid19_bow_full ['abstract']\ndf_by_sentence ['bow_raw'] = df_by_sentence ['Sent Abstract']","cff0148d":"df_by_sentence['bow'] = df_by_sentence.bow_raw.apply(clean_text)\ndf_by_sentence['bow'] = df_by_sentence['bow'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n#df_by_sentence.head(5)\n\n\n#Only consider sentences > 20 chars\n\ndf_covid19_bow_f = df_by_sentence[df_by_sentence['Abstract'].map(len) > 20]\ndf_covid19_bow = df_covid19_bow_f.reset_index()\n#Subset for testing\n# df_covid19_bow_fs = df_covid19_bow_f.loc[1218:1230].copy()\n# df_covid19_bow = df_covid19_bow_fs.reset_index(\n\n","0a9f3943":"#df_covid19_bow = df_covid19_bow[['Title','Sent Abstract','Abstract','bow', 'cord_uid', 'Journal', 'Authors','Publish_Date', 'Source', 'Ref URL']]\n\n#produce a bag of words for queries and sentence of papers\ntotal_bow = [\"\".join(x) for x in (df_queries['query_bow'])]\ntotal_bow += [\"\".join(x) for x in (df_covid19_bow['bow'])]\n","8647860b":"print (len(total_bow))","87d46874":"#Use the Spacy model\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_bc5cdr_md-0.2.4.tar.gz\n\nimport spacy\nimport en_ner_bc5cdr_md\nfrom spacy import displacy\n\nnlp = en_ner_bc5cdr_md.load()\n\n","72d5a7c1":"#'full_text_file', 'has_pdf_parse', 'has_pmc_xml_parse', 'WHO #Covidence', 'Microsoft Academic Paper ID'","aed7f292":"# abstract split into sentence\ndf_covid19_bow = df_covid19_bow[['Title','Sent Abstract','Abstract','bow', 'cord_uid', 'Journal', 'Authors','Publish_Date', 'Source', 'Ref URL']]\n\n#abstract complete\ndf_covid19['therapies'] = np.nan\ndf_covid19 = df_covid19[['therapies','cord_uid', 'sha', 'Source', 'Title', 'doi', 'pmcid', 'pubmed_id',\n       'license', 'Abstract', 'Publish_Date', 'Authors', 'Journal',\n         'Ref URL', 'tag_disease_covid19','org_abstract']]","28b3bb26":"#get a list of strings. each string is an abstract\nabstract_sent_list = [\"\".join(x) for x in (df_covid19_bow['bow'])]\n\n#convert the list of strings to one long string\nabstract_bow_full = ' '.join(abstract_sent_list)\n\n\n\n\n#split the string into a list of single word strings\nabstract_bow_full_list = abstract_bow_full.split()\n#removes repeats because of the Scapy memory limitation \nabstract_bow = ' '.join(set(abstract_bow_full.split()))\n#abstract_bow = abstract_bow_full","3793f577":"import re\n\n#Prep before running Spacy Model and search to find therapies\n\n\n#remove any number\nabstract_bow = re.sub(r'\\d+', '',abstract_bow )\n\n#remove single char words\nabstract_bow = re.sub(r'(?:^| ).(?:$| )', ' ', abstract_bow)\n#remove 2 char words\nabstract_bow = re.sub(r'(?:^| )..(?:$| )', ' ', abstract_bow)\n#remove 3 char words\nabstract_bow = re.sub(r'(?:^| )...(?:$| )', ' ', abstract_bow)\n\n#Then produce a list of unique words\n\nabstract_bow_list = abstract_bow.split() ","1cebde00":"#abstract_bow_list = abstract_bow.split() \nall_therapies_list = []\nall_chemicals_str = ''\nfor word in abstract_bow_list:\n    if len(word) > 5 and len(word) <30:\n        wordspace = word+' '\n        doc = nlp(wordspace)\n        entry = doc.ents\n        if entry:\n            #print ('word, entry[0].label_',word,entry[0].label_)\n            if entry[0].label_ == 'CHEMICAL':\n                #print ('word, nlp(word)',word, nlp(word).ents)            \n                all_chemicals_str += ' ' + word\n                all_therapies_list.append(word)\n                #pdb.set_trace()\n ","d4371f34":"print (len(all_therapies_list))","89b6058c":"#df_druglist=pd.read_csv('\/kaggle\/input\/fdadrugs\/druglist.csv')\ndf_druglist=pd.read_csv('\/kaggle\/input\/fdasubsubstance\/substancename.csv')\ndf_druglist = df_druglist.drop_duplicates(subset='SUBSTANCENAME', keep=\"first\")\ndf_druglist = df_druglist[df_druglist['SUBSTANCENAME'].notna()]\n\n#Split the names: one name per line\ndf_druglist['org_SUBSTANCENAME'] = df_druglist['SUBSTANCENAME']\ndf_druglist = df_druglist.set_index(df_druglist.columns.drop('SUBSTANCENAME',1).tolist())\\\n.SUBSTANCENAME.str.split('\\; ', expand=True).stack().reset_index()\\\n.rename(columns={0:'SUBSTANCENAME'})\ndf_druglist = df_druglist.drop_duplicates(subset='SUBSTANCENAME', keep=\"first\")\n\n#convert to list and go to lower case\ndrug_list = df_druglist['SUBSTANCENAME'].tolist()\ndrug_list = [element.lower() for element in drug_list] ; \n#drug_list = drug_list.tolower()\n#print (drug_list)","4ec28c55":"#Add FDA drug list\n\n\ndrugs_str = ''\n\nfor drug in drug_list:\n    #print (drug)\n    drugspace = ' '+drug+' '\n    if abstract_bow_full.find(drugspace) > -1:\n        #print (drug)\n        if drugs_str == '':\n            drugs_str = drug\n        else:\n            drugs_str = drugs_str + \", \" + drug\n        all_therapies_list.append(drug)\n        \n\n#Add therapies, interventions and issues mentioned in the literature        \nall_therapies_list.append(\"stem cell\")\nall_therapies_list.append(\"nitric oxide\")\nall_therapies_list.append(\"interferon\")\nall_therapies_list.append(\"cas13\")\nall_therapies_list.append(\"cepharanthine\")\nall_therapies_list.append(\"selamectin\")\nall_therapies_list.append(\"camostat\")\nall_therapies_list.append(\"nafamostat\")\nall_therapies_list.append(\"fusan\")\nall_therapies_list.append(\"hyperbaric oxygen therapy\")\nall_therapies_list.append(\"plasma exchange\")\nall_therapies_list.append(\"Mesenchymal Stromal Cells\")\nall_therapies_list.append(\"Enoxaparin\")\n#hypercoagulable relate\n\nall_therapies_list.append(\"anticoagulant\")\nall_therapies_list.append(\"antithrombosis\")\nall_therapies_list.append(\"tocovid\")\nall_therapies_list.append(\"thrombectomy\")\n\n\n","9995b82f":"#Add drugs from Wikipedia list\n#Add drugs mentioned in literature or task\n\ndrugs_str = ''\nfor word in abstract_bow_list:\n    ending = re.findall(r'(?:.*?(\\w{3})\\b)', word)\n    if ((ending == ['vir']) or (ending == ['mab']) or (ending == ['dol'])or (ending == ['axine'])or (ending == ['oxacin'])\\\n        or (ending == ['tinib']) or (ending == ['lisib'])\n        or word.find('interferon') > -1\\\n        or word.find('naproxen') > -1\\\n        or word.find('clarithromycin') > -1\\\n        or word.find('minocycline') > -1\\\n        or word.find('homoharringtonine') > -1):\n            #drugs_str = drugs_str + \" \" + word\n            all_therapies_list.append(word)\n\n# #drugs_unique = ' '.join(set(drugs_str.split()))\n","a7202cbf":"stopwords = ['air','lead','oxygen','hydrogen','urea',\\\n'diamond', 'creatinine','gold','nitrogen',\\\n'alanine','water',\\\n'creatine','alcohol',\\\n'silicon', 'lactic acid','sodium''phosphorus','egg',\\\n'zinc', 'influenza b virus','hydrogen peroxide',\\\n'hepatitis c virus', 'garlic','glycine',\\\n'thyroid', 'leucine','sodium hypochlorite',\\\n'potassium','squash', 'efavirenz',\\\n'sulbactam sodium','nifedipine,'\\\n'uric acid', 'legionella pneumophila'\\\n'hama','chine','acid','mechan','extract','iran','icacy','guan','isse','ester',\\\n'betacoronavirus', 'hydrogen','methyl', 'provincia','greg','virol', 'urea',\\\n'lombardy','phosphate','pakistan','sarbecovirus', 'hcovs', 'crrt', 'acei',\\\n'hunan', 'ecmo', 'statin','henan', 'daegu', 'biomed','jinyintan','aspartate','hama',\\\n'virus','hopkins','ncovs','tryptanthrin','oseltamivir', 'hpdi','macau','ethanol',\\\n'youtube','fcov','xiao','aedt','enac','philippine', 'york', 'repatriate',\\\n'phosphorus','alcohol','hebei', 'chloride', '\u2010ncov', 'thiol', 'yokohama', 'paul','stein', \\\n'saharan', 'zinc','indigodole','ncapp','glucose','methanol','phys',\\\n'oxygen','angiotensin','amino','ncip','nucleotide','lactate','tongji',\\\n'separ', 'tianjin','creatinine', 'nitrogen', 'cochrane','qpcr','dpcr','alanine','trypsin','purine','sichuan',\\\n'triphosphate','virus\u2019','ptsd', 'ddpcr','melatonin', 'sodium','emergencia',\\\n'creatine', 'tibetan', 'ciclesonide','sulfate','brote', 'cuidado', 'jama', 'tmax','proline', 'sirolimus,'\n'youan','quench','ccov','chicago', 'sudan','abstractan''espii','ncov\u201d', 'intensivos',\\\n'gompertz', 'iraq', 'berlin','nucleoside', 'virales','taiyuan','quencher','coronavirus\u2019',\\\n'cxcr','cpet','nucleal','brigham', 'niclosamide', 'nettree','\u51a0\u72b6\u75c5\u6bd2\uff08', 'hcov\u2013host',\\\n'covd','pbmcs','gyeongsangbuk','hydrochloride', 'iata', 'belgium', 'scovs', 'gyeongbuk',\\\n'abstractin','abstractan','espii','bscs','mercado','youan','fubar','leucine','bilirubin','mab','tcr',\\\n'hypochlorite','contexte', '\u03c4trans',\"we\u2019ve\",'dado','cardiovasculaire','particuli\u00e8re',\\\n'mographic','fema','primari','indonesia', 'hepg','infectado','denominada','ganglioside','fatty', 'fetp',\\\n'carbon', 'palo', 'mhla','zeit','qiaamp','tacrolimus','sanitarium', 'begg','contingencia','bolivia','calmette','vitamin','mape','ncov\u2019',\\\n'glycine', 'comorbidit\u00e9s', 'asif','ishr','existencia','dioxide','sanitizer', 'tracker','l\u00ednea','\u2122','unsaturated',\\\n'silver', 'chlorine', 'cholesterol', 'cobalt', 'formaldehyde', 'escherichia coli',\\\n'morphine','benzalkonium chloride','serine','papain',\\\n'catecholamine','confirmado','metformin','torovirus','minipcr', 'propone','\u2103','nab','oxide','\u2014','\\u200b','enc','\u221e',\\\n 'prednisolone','angiotensin ii','nitric', 'ibuprofen','abidol', 'hek',\\\n 'monoxide','formoterol','contiene','carbohydrate',\\\n 'bast','dlco','volatile','instrucciones','isopropyl',\\\n 'ferroprotein','avis', 'oral\u2013fecal',\\\n 'calcium','shankar', 'ccaa', 'lactobacillus','hydroxychloroquine sulfate', 'jingmen','pescado', 'speared',\\\n 'profesionales', 'wikipedia','bioline','tamil','anthony','bifidobacterium',\\\n '\u2212recovered','lebanon','legionella pneumophila','territorio', 'yeast',\\\n 'appetite','simplot','nf\u03bab', 'francisco', 'hower', 'dcps','toremifene', 'li\u00e9es', 'carol',\\\n 'ziff', 'l\u2019origine', 'urgencia', 'viele', 'adecuadas','spectacle', '\u00e1mbito', 'revu',\\\n 'grossesse','ethylene', 'mayor\u00eda', 'colorado','tehran','identificado','\u03c4end',\\\n 'aldehyde','engen','spine', 'scct',\\\n 'veroe','huoshenshan', 'uric acid','infect\u00e9es', 'evag', 'cipomo','soporte', 'leishenshan','ukraine',\\\n 'xiaobo', 'allyl', 'choloroquine','infecciones','hurst', 'equipos',\\\n 'tcga','normokalemia', 'number\u2014the','guerin', 'canadian',\\\n 'podr\u00eda','penta','\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u80ba\u708e\uff08covid', 'horsham',\\\n'\u2248','chloroquine phosphate','humboldt', 'eyedrop',\\\n'adenosine','adenosyl','thymosin','herramienta','coordenado','benzalkonium','estuvieron','disulfiram',\\\n'digluconate','collectrin','guanosine','provocan','methionine','guanine','garantice','thiazide',\\\n'sospechosos','chlorhexidine','rituximab','cefoperazone','sulbactam','mefloquine','cobicistat',\\\n'tin', 'irat','trat','treatmen', 'char','includi', 'pea','trib','contin', 'ques','inflamm', 'xpress', 'asymp', '1309'\\\n'ether','chloro', 'pregnan','quip','iron','amine','perte','clare','ather','uncer',\\\n'pathol', 'sage', 'fran', 'studie','virtu','mace','elm','maine', 'ipal','toch','merci','taco'\\\n'chas', 'breas', 'trus','huan','itus', 'viol', 'tibio', 'tenu','lora', 'coco',\\\n'sanita', 'xact','covide', 'decontaminat', 'mansfield','thea','proteo', 'gamm', 'nonin',\\\n'foca', 'laryngology','cooper', 'turkey', 'quot','spri', 'hemos','spice','lama', 'haled','quid',\\\n'scad','irvine', 'ilam','biotec', 'disinfectan', 'telle', 'ozone', 'transporte', 'rabbit',\\\n'incrementa', 'progrip', 'dice', 'apple','brescia', 'erythemato', 'louis', 'cas13','nicotine',\\\n'hfabp','eppi', '\u00f0\u00bd\u00f0\u00b5','lamb', '\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\uff08''capitulate', 'kinin','bean', 'accru',\\\n'testosterone','angii', 'frontlines', 'huangshi','ephrology','tambi', 'germicidal', 'sace',\\\n'chicken', 'carbon dioxide', 'silc', 'carmen', 'calu', 'btcov', 'essai', 'cannabis',\\\n'\u2013angiotensin', 'disembark', 'david', 'trevo', 'human milk', 'puesta', 'bata','conjugate',\\\n'paco', 'barr\u00e9','qiao','parietex\u2122', 'iodine', 'harte','zenker', 'conjugated', 'incheon', 'kegg','briggs',\\\n'ipom', 'imid', 'acetyl','contagios', 'ltcfs', 'crso', 'chla', 'titanium', 'dmts', 'utah','georgia',\\\n'nama', 'nitrous','kurdistan','hads','infodemiology', 'dornase alfa','covids','\u00f0\u00b8\u00f1\u2026', 'hyde', 'methylene',\\\n'sbrt','urethane','gypsum','glycol', 'utla', 'staphylococcus aureus', 'sanitarios', 'nord',\\\n'orange', 'benefice', 'bis\u00ae','devra', 'wisconsin', 'nont', 'zhao','ltch', 'tyrosine',\\\n'sras','cortisone','codogno','seiqrd', 'aaga', 'liquorice', 'scandinavia', 'gu\u00e9rin', '\u7126\u8651\u75c7\u72b6', 'gia\u2122','arabic', 'polyphenol', 'russian','quebec', 'habe', 'ldrt','psychotropes',\\\n'hrsace', 'fast\u2122', 'hepatitis b virus''chws', 'assistdem', 'pr\u00e9cautions', 'conoce','mcgrath', \\\n'cuadro', 'microsoft', 'mycoplasma pneumoniae', 'butyrate', 'sitr', 'cvir', 'methadone', 'arksey', 'sprayshield\u2122', \\\n'alteplase','hydrocortisone','langone','aspirin', 'hdrt', 'austrian', 'philadelphia','naproxen', 'doacs','nicht', 'istanbul',\\\n'sysf','chlorogenic', 'saho','assut','prendre', 'yemen', 'ffpe', 'vnrs', 'ceap', 'qfpdt','polyethylene', 'digestifs','ifnl', 'panama', 'escenarios', 'ammonium',\\\n'honey', 'gbind', 'puis', 'hpsc', 'suramin','rhine', 'ccbs', 'seg\u00fan', 'chad', 'sniffin','nagpaul', 'yolo','polysorb', 'mead','connaissances','zahl', 'alovudine',\\\n'm\u00e9decin', 'bridgepoint', 'dane', 'opium', 'connu', 'considerar', 'zinc sulfate','gelpoint', 'verlauf',\\\n'gps\u2122','chuv', 'mvaova', 'hslam', 'certaines', 'dinucleotides', 'saccharin', 'civid', 'geben', 'phenolic', \\\n'picovacc', 'stella', 'flavonoid'\\\n'folic acid', 'bengal','chup', '\u00f0\u00b0\u00f0\u00bc','multidisciplinario', '\u00f6l\u00fcm', 'slnb', 'cscs', 'twinsuk', 'enfin',\\\n'antihistamine', 'biologiques',  'oestrogen','methotrexate', 'sinophobic', '\u00e2\\x80\u00af\u00b1\u00e2\\x80\u00af','sulphate',\\\n'\u00f1\u20ac\u00f0\u00b5\u00f1\\x81\u00f0\u00bf\u00f0\u00b8\u00f1\u20ac\u00f0\u00b0\u00f1\u201a\u00f0\u00be\u00f1\u20ac\u00f0\u00bd\u00f0\u00be\u00f0\u00b3\u00f0\u00be', '\u00e2\\x80\u00afms', 'tropospheric','scfv', 'violencia','qualit\u00e9','amies',\\\n'tiotropium', 'xpert\u00ae','butyl', 'folic','courte','wssci', 'shcs', 'carbonate', 'cardiff',\\\n'caffeic','wifi','hoffmann','ciaad','hospitalarios', 'ionized', 'glucoside', 'hospitali\u00e8re'\\\n'auteur', 'copper', 'resnetv', 'folgen', 'higgins', 'calhr','jensenone', 'florian',\\\n'cov\u53ca', 'poliovirus','paraffin', 'novid', 'dynamesh\u00ae', 'dlnms', 'pyruvate', 'sdel', 'qifen',\\\n'cin','ether',\\\n'\u043d\u043e\u0432\u043e\u0439', 'hydroxybutyrate', 'erences', 'chelators', 'klebsiella pneumoniae','tdd\u2010ncp','trendstm',\\\n'indium', 'propio', 'macaflavanone', 'ppab', 'evicel', 'hypoth\u00e8se', 'tiantan', 'niran', 'progrip\u2122',\\\n'glucan', 'vodan', 'chiffon', 'kaplan\u2013meier', 'instituciones', 'alveolo', 'atlanta','antidepressant',\\\n'monophosphate', 'mycophenolic', 'hazelwood', 'amcs', 'flavone', 'eswabs', 'autacoid', 'ascorbic acid',\\\n'aichi', 'chemai', 'moroccan', 'grecco','permitan','cuimc', 'benzene', 'supone', 'ambarl', 'desarrolladas',\\\n'effectivene','steroid','nemen', 'taco','chas','immunol', 'nterleukin', 'hejia','\u043a', 'sterol',\\\n'luminal','\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\uff08', 'tcid', 'capitulate','resveratrol',\\\n'biliar','poblaci','tonavir','calcitonin','aldosterone',\\\n'cli', 'tri', 'gly','pandemonium','auteur',\\\n'lithuania', 'swissadme','constituye','preperiod','santiago', 'parsimony','magnesium',\\\n'cirug\u00edas', 'diesem', 'rtv\u2013ifn','servir', 'verteporfin', 'inflamatorio', 'hungarian', 'sildenafil', 'pr\u00e1cticas',\\\n'quir\u00fargicas', 'cell\u00b7\u03bcl', 'catharina', 'miglustat', 'gargle', 'manhattan','\u00e9quipes','human\u2010to\u2010human,'\\\n'condado', 'atovaquone', 'dihydrotestosterone','conformit\u00e9','stopcovid','lysosomotropic','matthew',\\\n'vir','\u00f0\u00b8\u00f0\u00bd\u00f1\u201e\u00f0\u00b5\u00f0\u00ba\u00f1\u2020\u00f0\u00b8\u00f0\u00b8','\u00f1\u2021\u00f0\u00b5\u00f0\u00bb\u00f0\u00be\u00f0\u00b2\u00f0\u00b5\u00f0\u00ba\u00f0\u00b0','\u00f0\u00bc\u00f0\u00b5\u00f1\u20ac\u00f0\u00be\u00f0\u00bf\u00f1\u20ac\u00f0\u00b8\u00f1\\x8f\u00f1\u201a\u00f0\u00b8\u00f0\u00b9', 'individuelle','dorries','face\u2010to\u2010face','sars\u2013cov','im\u00e1genes',\\\n'revisiones','palomo','nivolumab','persian','im\u221a\u00b0genes','remifentanil','velosorb\u201a\u00d1\u00a2','hyaluronan',\\\n'neurolog\u221a\u2260a','homocysteine','esfuerzos','clungene','sonicision\u201a\u00d1\u00a2','diasorin','biblioteca',\\\n'adecuado','pleuraseal','adrenaline','lockdown','pontryagin',\\\n'm\u00e9decine','charmm','selenium','wheat',\\\n'diphosphate','calmette\u2013gu\u00e9rin','bromhexine','alabama',\\\n'infectadas','flavonoid','telehospice','distr\u00e9s','citrate','high\u2010risk','minnesotan',\\\n'joseph','beneficios','metronomic','neurolog\u00eda','escherichia','elecsys\u00ae',\\\n'hepatitis b virus','human\u2010to\u2010human','pork','higiene',\\\n'alabama','ascorbic','egg yolk','allium','low\u2010risk','\u2018lockdown\u2019','infectadas','mild\u2010to\u2010moderate',\\\n'verpleegkundigen','verpleegkundige','sucrose','copenhagen','syndrome\u2010related','life\u2010saving','harvey','human papillomavirus',\\\n'implicaciones','demand\u2010side','covideo']\n","4091739c":"all_therapies_list  = [word for word in all_therapies_list if word not in stopwords]\n\n","3d31600b":"\n#Count occurrences of therapies in the full bag of words\npotentials =[[x,abstract_bow_full.count(\" \"+x+\" \") ]for x in set(all_therapies_list)]\n","d611243c":"#Count occurrences of therapies in the full bag of words\n#potentials =[[x,abstract_bow_full.count(x) ]for x in set(all_therapies_list)]\n\n#sort list\nfrom operator import itemgetter\nall_drugs = sorted (potentials, key=itemgetter(1), reverse  = True)\n#therapies = all_drugs\n#remove noise\ntherapies = []\nfor sublist in all_drugs:\n    if sublist[1] >3:\n        #print (sublist)\n        therapies.append(sublist)\n\n# #there is still noise and so exclude items mentioned twice or less\n# therapies_f = []\n# for sublist in all_drugs:\n#     if sublist[1] >2:\n#         print (sublist)\n#         therapies_f.append(sublist)\n\n# #Remove subwords found\n# #eg prednisolone and chloroquine\n# #as a by product ritonavir is also removed which is unfortuate but not a problem\n        \n# therapies = []\n# for therapy in therapies_f:\n#     subword = False\n#     for therapy_t in therapies_f:\n#         #print ('therapy, therapy_t',therapy, therapy_t)\n#         if (therapy_t[0].find(therapy[0])) >0:\n#             subword = True\n#     if subword == False:\n#         therapies.append(therapy)\n\n    ","60a82f46":"print (all_drugs)","83ec2c4f":"print (therapies)","4aa94c7d":"print(len(therapies))","924551a1":"import json\nwith open('therapyfile.txt','w') as f:\n    json.dump(therapies, f)","75be325a":"df_covid19.to_csv('df_covid19.csv')","575d4ec2":"df_covid19 = pd.read_csv('df_covid19.csv')","bacb4d37":"therapies = [['tizoxanide', 9], ['dupilumab', 9]]","ca5e2413":"import json\nwith open('therapyfile.txt') as f:\n    therapies = json.load(f)","68cef705":"from nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\nimport re\n\nimport nltk\nfrom textblob import TextBlob\n","8ea4eb0d":"def abstract_title_filter(df, search_string):\n    return (df.Abstract.str.lower().str.replace('-', ' ')\n            .str.contains(search_string, na=False) |\n            df.Title.str.lower().str.replace('-', ' ')\n            .str.contains(search_string, na=False))\n\n# def abstract_title_filter(df, search_string):\n#     return (df.Abstract.str.lower()\n#             .str.contains(search_string, na=False) |\n#             df.Title.str.lower()\n#             .str.contains(search_string, na=False))\n\ndef tag_therapy(df, synonym_list: list,\n                  tag_suffix: str) -> (pd.DataFrame, pd.Series):\n    counts = {}\n    df[f'tag_{tag_suffix}'] = False\n    for s in synonym_list:\n        synonym_filter = abstract_title_filter(df, s)\n        \n        df.loc[synonym_filter, f'tag_{tag_suffix}'] = True\n    print(f'Added tag_{tag_suffix} to DataFrame')\n    return df\n\n","feb7e2fd":"pd.options.mode.chained_assignment = None  # default='warn'\n\n#set a flag for occurrence of a therapy in abstract\nfor therapy in therapies:\n    s = therapy[0]\n    df_covid19[f'tag_{s}'] = False\n    therapy_filter = abstract_title_filter(df_covid19, s.replace('-', ' '))\n    df_covid19.loc[therapy_filter, f'tag_{s}'] = True\n\n    \n\n\n    \n","618e6dd5":"#set therapy based upon flags set\ndf_covid19['Therapy'] = 'none'\nfor therapy in therapies:\n    s = therapy[0]\n    df_covid19['Therapy'] = np.where(df_covid19[f'tag_{s}'],s,df_covid19['Therapy'])\n\n","a0f3580e":"#correct spello?\ndf_covid19['Abstract'] = df_covid19['Abstract'].replace({'ivermetin' : 'ivermectin'}, regex=True)\n","2155d1ab":"#set a flag for occurrence of \"hypercoagulable\" or similar\nhypercoagulable_synonyms = ['hypercoagul','coagula','platelet recovery','fibrin','plasma exchange',\n                    'thrombo','ccb','revascularization']\n\n#for test\n# hypercoagulable_synonyms = ['coagulant']\n\ndf_covid19, covid19_counts = count_and_tag(df_covid19, hypercoagulable_synonyms, 'hypercoagulable')\n\n    \n    \n","adea5564":"#Determine the Study Type\n    Randomized_Control_Trial = ['were randomized', 'randomized.*trial was']\n    Model   = ['modelling','model','molecular docking','modeling','immunoinformatics']\n    Systematic_Review = ['systematic review', 'meta-analysis', 'data sources.*searched', 'search.*published']\n    Literature_Review = ['literature','search in']\n    Retrospective_Observational = ['record review','retrospective observational', 'observational cohort','retrospective clinical','retrospective cohort','simulated.*study','scoping review']\n    Non_randomized_Trial = ['non-randomized trial','Clincal trial','single arm protocol']\n    d = {'Systematic Review' : Systematic_Review,\n     'Literature Review' : Literature_Review,\n     'Retrospective Observational' : Retrospective_Observational,\n    'Non-randomized_Trial' : Non_randomized_Trial,\n    'Randomized Control Trial' : Randomized_Control_Trial,'Model' : Model}\n\n    d1 = {k: oldk for oldk, oldv in d.items() for k in oldv}\n    df_covid19['Study Type'] = 'Other'\n    for k, v in d1.items():\n        df_covid19.loc[df_covid19['Abstract'].str.contains(k, case=False), 'Study Type'] = v\n","e36492c0":"# dftest = pd.DataFrame({'Yes': ['heart. Methods and liver.  functions (p<0.05). conclusion The degrees of lymphopenia and proinflammatory','blank. Method verse is here. Result So that in . Conclusion There is no word Primary endpoint Systematic review here. but  not here', 'AIM To evaluate the clinical value of abnormal laboratory results of multiple organs. RESULTS Elevated neutrophil-to-LYM ratio (NLR), D-dimer(D-D), interleukin (IL)-6, IL-10, IL-2, interferon-Y, and age were significantly associated with the severity of illness. D-D increased from 0.5 to 8, and the risk ratio increased from 2.75 to 55 heart and liver functions (p<0.05). CONCLUSIONS The degrees of lymphopenia and proinflammatory cytokine storm were higher in severe COVID-19 patients than in mild cases. The degree was associated with the disease severity. Advanced age, NLR, D-D, and cytokine levels may serve as useful prognostic factors for the early identification of severe COVID-19 cases. ','here we have endpoint. method molecular. conclusion docking', 'beginning here. result Here primary outcome. conclusion we have model'], 'No': [131, 10,6,2,4]})\n\n# endstr = '(?=(?i)\\..result)'\n\n# dftest['Method'] = dftest.Yes.str.extract('((?<=(?i)\\..method).*(?=(?i)\\..result))')\n# dftest.fillna('None', inplace=True)\n\n# dftest['Method1'] = dftest.Yes.str.extract('((?<=(?i)\\..method).*(?=(?i)\\..conclusion))')\n# dftest['Results'] = dftest.Yes.str.extract('((?<=(?i)\\..result).*(?=(?i)\\..conclusion))')\n# dftest['Method'] = np.where(dftest['Method']  == 'None', dftest['Method1'], dftest['Method'])\n\n# dftest\n\n\n#(?=(?i)\\..result)|","1f00416b":"df_covid19['Conclusion']= 'none'\ndf_covid19['Sample Severity of Symptoms'] = 'Not Available'\ndf_covid19['Primary Endpoint(s) of Study'] = 'Not Available'\n\n\n\n\n\n\n    #print (\"s\",s)\n    \n\n#Find the conclusions, assuming they start with the word conclusion and go on to the end of the abstract\nsearched_word_list   = ['\\. conclusion', 'in conclusion', '\\. discussion','in summary', 'preliminary findings','our findings','\\. recommendation','learning points','\\. interpretation','we conclude','we suggest']\nsearched_words = '|'.join(r\"{}\".format(x) for x in searched_word_list)\nsearched_words = '(?i)' + searched_words\nnew = df_covid19['Abstract'].str.split(searched_words,n=1, expand=True)\nnew['len'] = new[0].str.len()\n\n    \n#    new = df_covid19_t['Abstract'].str.split('(?i)\\. conclusion',n=1, expand=True)\n    #print ('new',new)\n    #print ('newshape',new.shape)\n    #print ('newtype',type(new))\n    #print ('new.columns',len(new.columns))\nif len(new.columns) == 3:\n    new[1].fillna('None', inplace=True)\n    df_covid19['Ab Len'] = new['len']\n    df_covid19['Conclusion'] = \"Conclusion\"+ new[1]\n    #if abstract is small give the full abstract\n    df_covid19['Conclusion'] = np.where(df_covid19['Ab Len'] < 350,df_covid19['Abstract'] , df_covid19['Conclusion'])\n    \nelse:\n    df_covid19['Conclusion'] = df_covid19['Abstract']\n\n                                              ","d1dfe042":"#fill results and methods column where found\n\ndf_covid19['Method'] = df_covid19.Abstract.str.extract('((?<=(?i)\\..method).*(?=(?i)\\..result))')\ndf_covid19.fillna('None', inplace=True)\n\ndf_covid19['Method1'] = df_covid19.Abstract.str.extract('((?<=(?i)\\..method).*(?=(?i)\\..conclusion))')\ndf_covid19['Method'] = np.where(df_covid19['Method']  == 'None', df_covid19['Method1'], df_covid19['Method'])\n\ndf_covid19['Results'] = df_covid19.Abstract.str.extract('((?<=(?i)\\..result).*(?=(?i)\\..conclusion))')\n","9e2403ca":"numberset = 'one|two|three|four|five|six|seven|eight|nine|ten'\n\n#Search for sample size\nsearched_word_list   = ['\\s\\d* eligible participants', '\\s\\d* patients','\\s' + numberset + 'patient']\nsearched_words = '|'.join(r\"\\b{}\\b\".format(x) for x in searched_word_list)\nsearched_words = '(?i)' + searched_words\ndf_covid19['Sample Size'] = df_covid19['Abstract'].apply(lambda texta: [sent for sent in sent_tokenize(texta)\n                                       if re.search(searched_words,sent)])\n ","3bf2966d":"#Search for treatment \n#removed 'all consecutive patients',\n    searched_word_list   = ['were.*treated','treated patients','patients treated','participants treated','participants receiving','we enrolled','patients in the study','patients enrolled','patients.*indentified','patients.*analyzed']\n    searched_words = '|'.join(r\"\\b{}\\b\".format(x) for x in searched_word_list)\n    searched_words = '(?i)' + searched_words\n    df_covid19['Treatment'] = df_covid19['Abstract'].apply(lambda texta: [sent for sent in sent_tokenize(texta)\n                                       if re.search(searched_words,sent)])\n    df_covid19['Treatment'].apply(str)\n","c845797d":"# pd.set_option('display.max_columns', 500)\n# df_covid19_bow","805e8532":"# w = 'molecular docking'\n# print (stemmer.stem(w.lower()))","ff1176f4":"\ndef therapy_prep_abstract_enhanced (df,s):\n\n    df_covid19_t = df[df[f'tag_{s}'] == True ]\n    \n    \n#    df_covid19_t = df_covid19_t[df_covid19_t['therapyinconc'== True ] ]\n#    df_covid19_t = df_covid19_t[df_covid19_t['conc'] == True ]\n    \n#     df_covid19_t = df_covid19_t[df_covid19_t['rev'] == False ]\n#     df_covid19_t = df_covid19_t[df_covid19_t['mmod'] == False ]\n#     df_covid19_t = df_covid19_t[df_covid19_t['hmod'] == False ]\n    \n    df_covid19_t = df_covid19_t.reset_index()\n    df_covid19_t = df_covid19_t.drop(['index'], axis=1)\n    \n#     df_covid19_t['Conclusion']= 'none'\n#     #print (\"s\",s)\n    \n\n\n        \n#For the therapy question, remove the paper if the therapy is not mentioned in the conclusion\n    if s != \"hypercoagulable\":      \n        df_covid19_t =  df_covid19_t[df_covid19_t.Conclusion.str.lower().str.replace('-', ' ').str.contains(s) ==True]\n        df_covid19_t['Therapy'] = s\n\n#Determine the sentiment of the Conclusion\n    df_covid19_t['Conclusion'].apply(str)\n    df_covid19_t['Conclusion Sentiment'] =df_covid19_t['Conclusion'].apply(lambda Conclusion:pd.Series(TextBlob(Conclusion).sentiment.polarity))\n\n   \n    \n    \n#Search for endpoints\n#     searched_word_list   = ['primary*endpoint', 'primary*end point', 'primary*outcome','secondary*endpoint', 'secondary*end point','secondary*outcome']\n#     searched_words = '|'.join(r\"\\b{}\\b\".format(x) for x in searched_word_list)\n#     searched_words = '(?i)' + searched_words\n#     df_covid19_t['Endpoint(s) of Study'] = df_covid19_t['Abstract'].apply(lambda texta: [sent for sent in sent_tokenize(texta)\n#                                        if re.search(searched_words,sent)])\n    \n#Search for symptoms\n#     searched_word_list  =['pneumonia']\n#     searched_words = '|'.join(r\"\\b{}\\b\".format(x) for x in searched_word_list)\n#     searched_words = '(?i)' + searched_words\n#     df_covid19_t['Sample Severity of Symptoms'] = df_covid19_t['Abstract'].apply(lambda texta: [sent for sent in sent_tokenize(texta)\n#                                        if re.search(searched_words,sent)])\n\n\n  \n    \n\n\n\n    \n    df_covid19_display = df_covid19_t[['Study Type','Therapy','Conclusion Sentiment','Conclusion','Method','Results','Treatment','Sample Size','Publish_Date','Title','Ref URL','Journal','Abstract',  'Authors', 'Source', 'doi','cord_uid','Sample Severity of Symptoms','Primary Endpoint(s) of Study']]\n#    df_covid19_display = df_covid19_t[['Study Type','Sample Severity of Symptoms','Conclusion Sentiment','Sentiment','Treatment Prep','Publish_Date','Title','Ref URL','Journal','Therapy','Conclusion','Endpoint(s) of Study','Authors', 'Source', 'doi','cord_uid']]\n    df_covid19_display = df_covid19_display.rename(columns={'Publish_Date': 'Date', 'Title':'Study', 'doi':'DOI', 'cord_uid':'CORD_UID','Ref URL':'Study Link',\\\n                                                           'Therapy':'Therapeutic method(s) utilized\/assessed'\\\n                                                           })\n\n    #    df_covid19_display = df_covid19_t[['Therapy','Endpoint','Publish_Date','Title','Abstract', 'Journal', 'Authors', 'Source', 'Ref URL','endp']]\n#sort by date, earliest first\n    df_covid19_display['Date'] = pd.to_datetime(df_covid19_display.Date)\n    df_covid19_display['Date'] = df_covid19_display['Date'].dt.date\n    df_covid19_display.sort_values(by=['Date'], inplace=True, ascending=False)\n    df_covid19_display = df_covid19_display.reset_index()\n    df_covid19_display = df_covid19_display.drop(['index'], axis=1)\n    \n    \n#    df_covid19_display = df_covid19_display.rename(columns={'Abstract': '___________________Abstract___________________'})\n    df_covid19_display = df_covid19_display.rename(columns={'Conclusion Sentiment':'Clinical Sentiment (range 1\/-1'})\n\n    \n    return (df_covid19_display)\n","987824e6":"#This method finds sentences with the word conclusion. Some conclusions are multisentence though.\n\n# searched_words=['conclus', 'conclude']\n# df_covid19 = df_covid19.replace(np.nan, '', regex=True)\n\n# #    df_covid19_t['abstract']=df_covid19_t['Abstract'].apply(str)\n\n# df_covid19['Abstract'].apply(str)\n# #    df_covid19_t = df_covid19_t.assign(Abstract=lambda df_covid19_t: df_covid19_t.Abstract +\" a conclusion and result Ends.\")\n\n# df_covid19['ConclusionList'] = df_covid19['Abstract'].apply(lambda texta: [sent for sent in sent_tokenize(texta)\n#                                        if any(True for w in word_tokenize(sent) \n#                                                if stemmer.stem(w.lower()) in searched_words)])\n# df_covid19['ConclusionList'].apply(str)\n# df_covid19['Conclusion']=df_covid19.ConclusionList.apply(lambda x: ' '.join(map(str, x)))","adae8bae":"df_covid19 = df_covid19.replace(np.nan, 'NA', regex=True)\n","e4db10e9":"# searched_word_list  =['pneumonia']\n# searched_words = '|'.join(r\"\\b{}\\b\".format(x) for x in searched_word_list)\n# searched_words = '(?i)' + searched_words\n\n# print (searched_words)\n\n# df_covid19['Sample Severity of Symptoms'] = df_covid19['Abstract'].apply(lambda texta: [sent for sent in sent_tokenize(texta)\n#                                        if re.search(searched_words,sent)])\n\n\n# df_covid19['Sample Severity of Symptoms'] = df_covid19['Abstract'].apply(lambda texta: [sent for sent in sent_tokenize(texta)\n#                                        if any(True for w in word_tokenize(sent) \n#                                                if stemmer.stem(w.lower()) in searched_words)])\n\n# .str.contains(k, case=False), 'Study Type'] ","2b59d474":"df_covid19.to_csv('df_covid19a.csv')","bf561847":"df_covid19 = pd.read_csv('df_covid19a.csv')","5e9e22c5":"#therapies by abstract\ni = 0\n\n\n\nfor therapy in therapies:\n    s = therapy[0]\n    df_covid19_display = therapy_prep_abstract_enhanced (df_covid19,s)\n#     if s == \"hypercoagulable\":\n#         df_covid19_display_hypercoagulable = df_covid19_display\n#     else:\n    if i ==0:\n        df_covid19_display_full = df_covid19_display\n    else:\n        df_covid19_display_full = pd.concat([df_covid19_display_full, df_covid19_display], ignore_index=True)\n    i = i + 1\n\n","54161394":"df_covid19_display_full.sort_values??","0adace45":"df_covid19_display_full = df_covid19_display_full.sort_values('Clinical Sentiment (range 1\/-1',ascending=False)\ndfStyler = df_covid19_display_full.style.set_properties(**{'text-align': 'left',\"font-size\": \"120%\"})\ndfStyler.set_table_styles([dict(selector='th', props=[(\"font-size\", \"150%\"),('text-align', 'center')])])\n   ","7f7b1018":"df_covid19_display_full.shape","529c0b42":"\ndf_covid19_display_full.to_csv('What is the efficacy of novel therapeutics being tested currently.csv')\n","32c5b7bd":"# dfStyler = df_covid19_display_hypercoagulable.style.set_properties(**{'text-align': 'left',\"font-size\": \"120%\"})\n# dfStyler.set_table_styles([dict(selector='th', props=[(\"font-size\", \"150%\"),('text-align', 'center')])])\n","c1bb3fdc":"# searched_words = '|'.join(r\"\\b{}\\b\".format(therapy[0]) for therapy in therapies)\n# searched_words = '(?i)' + searched_words\n ","2fc0a248":"s = \"hypercoagulable\"\ndf_covid19_display_hypercoagulable = therapy_prep_abstract_enhanced (df_covid19,s)\n","545e64af":"df_covid19_display_hypercoagulable = df_covid19_display_hypercoagulable.sort_values('Clinical Sentiment (range 1\/-1',ascending=False)\n\ndfStyler = df_covid19_display_hypercoagulable.style.set_properties(**{'text-align': 'left',\"font-size\": \"120%\"})\ndfStyler.set_table_styles([dict(selector='th', props=[(\"font-size\", \"150%\"),('text-align', 'center')])])\n","6042e0cf":"df_covid19_display_hypercoagulable.shape","6e9d7729":"df_covid19_display_hypercoagulable.to_csv('Hypercoagulable state seen in COVID-19 papers.csv')\n","50ccd959":"searched_words = '|'.join(r\"\\b{}\\b\".format(therapy[0]) for therapy in therapies)\nsearched_words = '(?i)' + searched_words\n   ","64c44ffe":"df_covid19_display_conc =  df_covid19_display_hypercoagulable[df_covid19_display_hypercoagulable.Conclusion.str.lower().str.replace('-', ' ').str.contains(searched_words) ==True]\n\n#add method and results\n#df_covid19_display_conc['method'] = df_covid19_display_conc.Abstract.str.extract(r'(?<=METHOD)*r'(?<=RESULT))')\n# df_covid19_display_conc['Method'] = df_covid19_display_conc.Abstract.str.extract('((?<=method).*(?=(?i)result))')\n# df_covid19_display_conc['Results'] = df_covid19_display_conc.Abstract.str.extract('((?<=result).*(?=(?i)conclusion))')\n\n","71ada7e8":"df_covid19_display_conc.shape","74e2cb8b":"df_covid19_display_treat =  df_covid19_display_hypercoagulable[df_covid19_display_hypercoagulable.Treatment.str.lower().str.replace('-', ' ').str.contains(searched_words) ==True]\n","f8a62e21":"df_covid19_display_treat.shape","e77ef6bd":"df_covid19_display = pd.concat([df_covid19_display_conc, df_covid19_display_treat], ignore_index=False)\ndf_covid19_display.shape","07cf5310":"df_covid19_display = df_covid19_display.drop_duplicates(subset='Abstract', keep=\"first\")\n","a46f6592":"df_covid19_display.shape","47cc897a":"df_covid19_display = df_covid19_display.sort_values('Clinical Sentiment (range 1\/-1',ascending=False)\n\ndfStyler = df_covid19_display.style.set_properties(**{'text-align': 'left',\"font-size\": \"120%\"})\ndfStyler.set_table_styles([dict(selector='th', props=[(\"font-size\", \"150%\"),('text-align', 'center')])])\n\n","ebca0816":"df_covid19_display.to_csv('What is the best method to combat the hypercoagulable state seen in COVID-19.csv')\n","a30db41e":"#### EndOfFile<a id='vote'><\/a>","c6fdbc7b":"# Papers that mention Hypercoagulable or a synonym in the conclusion or as a treatment","1a3676c4":"## Load and Prep Drug List","df38b822":"# Find Papers that mention Hypercoaguable or synonym","5133b7cb":"# Literature mentioning therapies \n\nmention of endpoint","c2f46de3":"## Discover chemicals in the papers ","322a1d8e":"## Finding Relevant Papers \n\nFor round 1 I found that the Universal Sentence encoder produced the most useful results.\n\nFor round 2 the requirement is much more focussed and so a keyword approach using regular expressions has been used to provide the required data.\n\nThe method presented here are produced completely automatically without an hand editting. To improve the chance of delivering useful papers, more data is provided than the minimum set required. Also selection has leaned towards \"false positives' so that borderline relevant papers are presented rather than being excluded.\n\nGiven the requirements and the limited data available from the early stage trials reported, it has not been possible to provide useful data on two requirements:\n\nendpoints\n\nillness severity\n\nThe csv files are ordered by the sentiment analysis of the conclusion column. The order is descending from  +1 to -1. \n\n\n## Therapy identification\n\nFour methods are used to identify potential therapies:\n\n* Using Spacy as identified in  medalCORD-19: Explore Drugs Being Developed by Maria and Gtteixeira (https:\/\/www.kaggle.com\/maria17\/cord-19-explore-drugs-being-developed). Thank you. \n* by identifying drugs by their ending as defined in Wikipedia (https:\/\/en.wikipedia.org\/wiki\/Drug_nomenclature).\n* by referring to the FDA's drug directory (https:\/\/www.fda.gov\/drugs\/drug-approvals-and-databases\/national-drug-code-directory)\n* A fourth method has been extended for round 2 and allows others to add potential therapies to the list of therapies examined.\n\n\nThe Spacy method produces many false matches and so an extensive stopword list has been produced and only those therapies mentioned more that 4 times are included here.\n\n\n### Results\n\n217 therapies have been identified and the 2020 papers have been examined to find matches.\n\nOf the 16k papers examined, a therapy was mentioned in the conclusion of 453 papers. These papers are saved in the 'What is the efficacy of novel therapeutics being tested currently' csv file.\n\nOf the 16k papers examined,hypercoagulable state or its synonym was mentioned \nin the 522 papers . These papers are saved in the \"hypercoagulable state seen in COVID-19 papers\" csv file.\n\nOf the 522 papers mentioned above, 39 papers include a mention of a therapy either in the conclusion or treatment cells. These papers are saved in the \"What is the best method to combat the hypercoagulable state seen in COVID-19\" csv file.\n\n\n## Pros and Cons on Therapy Discovery Method\n\nIdentifying potential therapies by the ending of their name is very quick but does not produce a complete list. The Spacy method can fill some of the gaps but I did identify a few problems. When I set it up to identify word bigrams, it sometimes produced long chains of unrelated words. Perhaps I was doing something wrong, anyway I decided to use the method on a single word basis. Bigrams are handled by exception which is not ideal. \n\nEach method contributes to the list of drugs found in the literature. Although the FDA's directory is comprehensive it does not always name drugs in the same way as done in the literature. Also some anti-virals are not currently listed.\n\n","367a3b17":"\n## Summary\nThis notebook is an entry in the COVID-19 Open Research Dataset Challenge task: Create summary tables that address therapeutics, interventions, and clinical studies\n\nSpecifically, the organisers want to know what the literature reports about:\n\n    What is the best method to combat the hypercoagulable state seen in COVID-19?\n    What is the efficacy of novel therapeutics being tested currently?\n\nTwo csv files are produced:\n\nWhat is the best method to combat the hypercoagulable state seen in COVID-19.csv\n\nWhat is the efficacy of novel therapeutics being tested currently.csv\n\nIn addition a 3rd csv file is produced which includes all papers that mention the hypercoagulable state or its synonyms:\n\nHypercoagulable state seen in COVID-19 papers.csv\n\n\nIf you found this notebook useful please give it a vote or better still please leave a comment at the [end of the notebook](#vote). Any comments would be much appreciated but ideas on improvement would be great.\n\n## Data Analysis\n\nOn the 10th June, the literature set provided consisted of 138,794 papers.\n\nAs the requirement is to find novel responses to recent problems, only those paper with a 2020 date have been analysed. This cut has been necessary from the point of view of memory constraints. \n\n2020 papers: 35,675\n\nThe following further cuts were made:\n\n23,718 after removing duplicates\n \n16,788 after removing papers that do not mention covid19 or its synomyms. This is acheived using the method supplied in covid19-tools provided by Andy White. Thank you!\n ","c955f218":"## Add drugs from FDA list","af2046cd":"# Find Therapies","0fdc8dad":"# Only present papers that mention Hypercoagulable in the conclusion or treatment "}}