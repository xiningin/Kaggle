{"cell_type":{"3c17477b":"code","d40459ed":"code","53f1a932":"code","dc1b212f":"code","3e4e1308":"code","c8e54bbf":"code","c1a252a6":"code","3eb0b6e3":"code","82b5b36a":"markdown"},"source":{"3c17477b":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/Restaurant_Reviews.tsv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Importing the dataset\ndataset = pd.read_csv('\/kaggle\/input\/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\ndataset.sample(5)","d40459ed":"# Cleaning the texts\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0, 1000):\n    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus.append(review)\n\n# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500)\nX = cv.fit_transform(corpus).toarray()\ny = dataset.iloc[:, 1].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\n# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","53f1a932":"#fitting KNN to the training set\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5,p=2)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","dc1b212f":"#Fitting logistic regression \nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","3e4e1308":"#fitting SVM \nfrom sklearn.svm import SVC\nclassifier = SVC(kernel='linear',random_state=0)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","c8e54bbf":"#fitting Kernel SVM\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel='rbf',random_state=0)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","c1a252a6":"#fitting Decision tree\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier=DecisionTreeClassifier(criterion='entropy',random_state=0)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","3eb0b6e3":"#fitting random forest \nfrom sklearn.ensemble import RandomForestClassifier\nclassifier=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)\nclassifier.fit(X_train,y_train)\n\ny_pred=classifier.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)","82b5b36a":"***Logistic regression:***\nAccuracy = 71%\nSensitivity = 78.3%\nSpecificity = 64%\n\n***Naive Bayes***\nAccuracy = 73%\nSensitivity = 56.7%\nSpecificity = 88.3%\n\n\n***KNN***\nAccuracy = 61%\nSensitivity = 76.2%\nSpecificity = 50.9%\n\n***SVM***\nAccuracy = 72%\nSensitivity = 76.2%\nSpecificity = 68%\n\n***Decision Tree***\nAccuracy = 71%\nSensitivity = 81.3%\nSpecificity = 66%\n\n***Random forest***\nAccuracy = 72%\nSensitivity = 89.6%\nSpecificity = 55.3"}}