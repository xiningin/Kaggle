{"cell_type":{"bcb2ec52":"code","900667a6":"code","8558faf7":"code","8aa7ae0e":"code","d322df2d":"code","b2cfb27c":"code","2ea78a58":"code","dc98b110":"code","13f0af69":"code","c627de55":"code","9e216fb1":"code","205eac10":"code","c085bd6f":"code","83bbeca0":"code","6d8884dc":"code","c2261b7c":"code","4a98eb65":"code","6abca470":"code","63bf9006":"code","a9f4a5c0":"code","beef805b":"code","15de70ad":"code","f91d6af3":"code","4e935c3a":"code","aacbb824":"code","ab3e782a":"code","7608ffd6":"code","f26a3560":"code","b9df6fa5":"code","70958d43":"code","704b4f76":"code","8010e36d":"code","0c6b83ea":"code","6be24c44":"code","e4af9180":"code","7ba792ab":"code","af1ec49c":"code","74063587":"code","b5094d72":"code","daff7257":"code","d5402c3b":"code","fd8b076d":"code","ef044e53":"code","7bd0e7c1":"code","d66d4de3":"code","fd859e5e":"code","24a4bd2f":"code","b4f2ccf8":"code","00f196bb":"code","17baa3b4":"markdown","f1d9a6b8":"markdown","949d602d":"markdown","9b4ccd89":"markdown","e1dade31":"markdown","13300575":"markdown","9da07149":"markdown","f62718c3":"markdown","bc3daa2b":"markdown","081e3fca":"markdown","4d1423d6":"markdown","32ae6b0c":"markdown","2a0e14fd":"markdown","904ea9d5":"markdown","81b7e5ce":"markdown","d10ba5f0":"markdown","adea76f4":"markdown","3550fb79":"markdown","ca2715e2":"markdown","43886e86":"markdown","7ae7cbb1":"markdown","e8befec4":"markdown","bfbe7973":"markdown","2c80761e":"markdown","b176d382":"markdown","183070bc":"markdown","ce3a605f":"markdown","c96f8bcc":"markdown","3f81eec2":"markdown","01aab149":"markdown","42f6e48f":"markdown","d9b0dc89":"markdown","7df9e284":"markdown","79ac220c":"markdown","6d72485f":"markdown","5f44cb64":"markdown","d34f7e7c":"markdown","dcc152af":"markdown","95c84447":"markdown"},"source":{"bcb2ec52":"from IPython.display import HTML\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n<\/script>\nThe raw code for this IPython notebook is by default hidden for easier reading.\nTo toggle on\/off the raw code, click <a href=\"javascript:code_toggle()\">here<\/a>.''')","900667a6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport nltk\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\n%matplotlib inline\n\n\n\ndf = pd.read_csv('..\/input\/17072018.csv', encoding='utf-8-sig')","8558faf7":"print(len(df))","8aa7ae0e":"df['social_media'].value_counts()","d322df2d":"df['country'].value_counts()[:15]","b2cfb27c":"unique_users = df.groupby('user')\nprint(len(unique_users))","2ea78a58":"df['published_on'] = df['published_on'].map(lambda x:pd.to_datetime(str(x).split()[0]))\ntweets_by_day = df.groupby('published_on').count().reset_index()\ntweets_by_day = tweets_by_day[tweets_by_day['published_on'] > '2018-05-19']\nlen(tweets_by_day)","dc98b110":"tweets_by_day = tweets_by_day.sort_values(by='published_on')\nx = tweets_by_day['published_on']\ny = tweets_by_day['text']\nplt.xlabel('Data')\nplt.ylabel('Quantidade de Ocorr\u00eancias')\nplt.xticks(rotation=45)\nplt.title('N\u00famero de Ocorr\u00eancias por dia')\nplt.plot(x,y, label='Tendencia Di\u00e1ria')\nplt.show()","13f0af69":"top_10_max_tweets_days = tweets_by_day.sort_values(by='user').tail(10)\nx = top_10_max_tweets_days['published_on']\ny = top_10_max_tweets_days['text']\nplt.xlabel('Data')\nplt.ylabel('N\u00famero de Ocorr\u00eancias')\nplt.title('Maior Ocorr\u00eancias em 1 dia')\nplt.xticks(range(10),x,rotation=45)\nplt.bar(range(10), y, label='Maior ocorr\u00eancias em 1 dia')\nplt.show\n","c627de55":"tweeteries = df.groupby(['user']).count().reset_index()\ntweeteries = tweeteries.sort_values(by='text').tail(10)\nx = tweeteries['user']\ny = tweeteries['text']\nplt.xlabel('Usu\u00e1rio')\nplt.ylabel(\"N\u00famero de Tweets\")\nplt.title(\"Maior n\u00famero de posts por usu\u00e1rio\")\nplt.xticks(range(10),x,rotation=45)\nplt.bar(range(10), y, label='Maior posts por usu\u00e1rio')\nplt.show","9e216fb1":"most_followed_users = df.drop_duplicates('user', keep='last')\nmost_followed_users_top_10 = most_followed_users.sort_values(by='reach').tail(10)\nx = most_followed_users_top_10['user']\ny = most_followed_users_top_10['reach']\nplt.xlabel('Usu\u00e1rio')\nplt.ylabel('Seguidores')\nplt.title('Usu\u00e1rios mais seguidos')\nplt.xticks(range(10), x, rotation=60)\nplt.bar(range(10), y, label='Usu\u00e1rios mais seguidos')\nplt.show()","205eac10":"tweets = df['text'].astype('str').dropna()\ntweets = ''.join(tweets)\ntweets = re.sub(r'[^\\x00-\\x7F]+',' ', tweets)\nhashtags_list = re.findall('#[a-z]+', tweets)\nfd = nltk.FreqDist(hashtags_list)\nfor x in fd.most_common(200): \n    print(x)\nfd.plot(10, cumulative=False)","c085bd6f":"df_plasticfreejuly = df[df['text'].str.contains('#plasticfreejuly',na=False)]\ntweets = df_plasticfreejuly[\"text\"].astype(\"str\").dropna()\ntweets =''.join(tweets)\ntweets = re.sub(r'[^\\x00-\\x7F]+',' ', tweets)\ntweets = re.sub('http[s]?:\\\/\\\/([\\w]+.?[\\w]+.?\\\/?)+','',tweets)\ntweets = re.sub(r'[\\W]+',' ',tweets)\ntweets_plasticfreejuly = tweets\ntokens = word_tokenize(tweets)\ntokens = [w.lower() for w in tokens if w.isalpha()]\nstopwords = nltk.corpus.stopwords.words('english')\ntokens = [w for w in tokens if w.lower() not in stopwords]\ntokens = [w for w in tokens if len(w) > 3]\nfd = nltk.FreqDist(tokens)\nprint('25 palavras mais comum para #plasticfreejuly:')\nprint(fd.most_common(25))","83bbeca0":"df_beatplasticpollution = df[df['text'].str.contains('#beatplasticpollution',na=False)]\ntweets = df_beatplasticpollution[\"text\"].astype(\"str\").dropna()\ntweets =''.join(tweets)\ntweets = re.sub(r'[^\\x00-\\x7F]+',' ', tweets)\ntweets = re.sub('http[s]?:\\\/\\\/([\\w]+.?[\\w]+.?\\\/?)+','',tweets)\ntweets = re.sub(r'[\\W]+',' ',tweets)\ntweets_beatplasticpollution = tweets\ntokens = word_tokenize(tweets)\ntokens = [w.lower() for w in tokens if w.isalpha()]\nstopwords = nltk.corpus.stopwords.words('english')\ntokens = [w for w in tokens if w.lower() not in stopwords]\ntokens = [w for w in tokens if len(w) > 3]\nfd = nltk.FreqDist(tokens)\nprint('25 palavras mais comum para #beatplasticpollution:')\nprint(fd.most_common(25))","6d8884dc":"df_plasticfreejuly = df_plasticfreejuly.groupby('published_on').count().reset_index()","c2261b7c":"df_plasticfreejuly = df_plasticfreejuly[df_plasticfreejuly['published_on'] > '2018-05-19']\ndf_plasticfreejuly = df_plasticfreejuly.sort_values(by='published_on')\nx = df_plasticfreejuly['published_on']\ny = df_plasticfreejuly['text']\nplt.xlabel('Data')\nplt.ylabel('Quantidade de Ocorr\u00eancias')\nplt.xticks(rotation=45)\nplt.title('N\u00famero de Ocorr\u00eancias por dia - #Plastic Free July')\nplt.plot(x,y, label='Tendencia Di\u00e1ria')\nplt.show()","4a98eb65":"df_beatplasticpollution = df_beatplasticpollution.groupby('published_on').count().reset_index()","6abca470":"df_beatplasticpollution = df_beatplasticpollution[df_beatplasticpollution['published_on'] > '2018-05-19']\ndf_beatplasticpollution = df_beatplasticpollution.sort_values(by='published_on')\nx = df_beatplasticpollution['published_on']\ny = df_beatplasticpollution['text']\nplt.xlabel('Data')\nplt.ylabel('Quantidade de Ocorr\u00eancias')\nplt.xticks(rotation=45)\nplt.title('N\u00famero de Ocorr\u00eancias por dia - #Beat Plastic Pollution')\nplt.plot(x,y, label='Tendencia Di\u00e1ria')\nplt.show()","63bf9006":"df_bagfree = df[df['text'].str.contains('#plasticbagfreeday',na=False)]\ndf_bagfree = df_bagfree.groupby('published_on').count().reset_index()","a9f4a5c0":"df_bagfree = df_bagfree[df_bagfree['published_on'] > '2018-05-19']\ndf_bagfree = df_bagfree.sort_values(by='published_on')\nx = df_bagfree['published_on']\ny = df_bagfree['text']\nplt.xlabel('Data')\nplt.ylabel('Quantidade de Ocorr\u00eancias')\nplt.xticks(rotation=45)\nplt.title('N\u00famero de Ocorr\u00eancias por dia - #Plastic Bag Free Day')\nplt.plot(x,y, label='Tendencia Di\u00e1ria')\nplt.show()","beef805b":"df_stopsucking = df[df['text'].str.contains('#stopsucking',na=False)]\ndf_stopsucking = df_stopsucking.groupby('published_on').count().reset_index()","15de70ad":"df_stopsucking = df_stopsucking[df_stopsucking['published_on'] > '2018-05-19']\ndf_stopsucking = df_stopsucking.sort_values(by='published_on')\nx = df_stopsucking['published_on']\ny = df_stopsucking['text']\nplt.xlabel('Data')\nplt.ylabel('Quantidade de Ocorr\u00eancias')\nplt.xticks(rotation=45)\nplt.title('N\u00famero de Ocorr\u00eancias por dia - #Stop Sucking')\nplt.plot(x,y, label='Tendencia Di\u00e1ria')\nplt.show()","f91d6af3":"rts = df['text'].astype('str').dropna()\nrts = ''.join(rts)\nrts = re.sub(r'[^\\x00-\\x7F]+',' ', rts)\nrts_list = re.findall('(@\\S+)', rts)\nfd = nltk.FreqDist(rts_list)\nprint(fd.most_common(20))\nfd.plot(20, cumulative=False)","4e935c3a":"\nrts = df['text'].value_counts()[:20].to_frame()\nrts.reset_index(level=0, inplace=True)\npd.set_option('display.max_colwidth', -1)\nrts.style.set_properties(**{'text-align': 'left'})","aacbb824":"top_posts = df.sort_values(by=\"reach\", ascending=False)\npd.set_option('display.max_colwidth', -1)\ntop_posts[['user','text','reach']][:20].style.set_properties(**{'text-align': 'left'})","ab3e782a":"pfreejuly = df[df['text'].str.contains('#plasticfreejuly',na=False)]\npfreejuly = pfreejuly.sort_values(by=\"reach\", ascending=False)\npd.set_option('display.max_colwidth', -1)\npfreejuly[['user','text','reach']][:20].style.set_properties(**{'text-align': 'left'})","7608ffd6":"ssucking = df[df['text'].str.contains('#stopsucking',na=False)]\nssucking = ssucking.sort_values(by=\"reach\", ascending=False)\npd.set_option('display.max_colwidth', -1)\nssucking[['user','text','reach']][:20].style.set_properties(**{'text-align': 'left'})","f26a3560":"empresas = [\"Starbucks\",\"McDonalds\",\"Adama\",\"AkzoNobel\",\"Ambev\",\"Arysta Lifescience\",\"Basf\",\"Bayer\",\"Cibraf\u00e9rtil\",\"DuPont\",\n\"Duratex\",\"FMC\",\"General Eletric\",\"Ihara\",\"LyondellBasell\",\"M&G Poli\u00e9ster\",\"Nestl\u00e9\",\"Nufarm\",\"O Boticario\",\"P&G\",\"Produqu\u00edmica\",\n\"Rhodia\/Solvay\",\"SIEMENS\",\"SUEZ\",\"SYNGENTA\",\"Syngenta\",\"Tetrapack\",\"Unigel Participa\u00e7\u00f5es\",\"Unilever\",\"Unipar\",\"WhiteMartins\",\n\"Yara Brasil Fertilizantes\",\"Dow Chemical\",\"Borealis\",\"Formosa Plastics\",\"Dell\",\"Renault\",\"Michelin \",\"Coca-Cola\",\"WalMart\",\n\"Evian\",\"Samsung\",\"Cisco\",'Sinopec','CNPC','ExxonMobil', 'Reliance','INEOS', 'NPC','Repsol','Petrobras', 'Petroken','Eco Petrol'\n'ENAP', 'Pampa Energ\u00eda', 'Koch Industries', 'PDVSA','Nova Chemicals', 'Chevron Phillips']\ncontent = []\nfor x in empresas:\n    temp_dataframe = df.loc[df['text'].str.contains(x)]\n    tweets = temp_dataframe[\"text\"].astype(\"str\").dropna()\n    tweets =''.join(tweets)\n    tweets = re.sub(r'[^\\x00-\\x7F]+',' ', tweets)\n    tweets = re.sub('http[s]?:\\\/\\\/([\\w]+.?[\\w]+.?\\\/?)+','',tweets)\n    tweets = re.sub(r'[\\W]+',' ',tweets)\n    tokens = word_tokenize(tweets)\n    tokens = [w.lower() for w in tokens if w.isalpha()]\n    stopwords = nltk.corpus.stopwords.words('english')\n    tokens = [w for w in tokens if w.lower() not in stopwords]\n    tokens = [w for w in tokens if len(w) > 3]\n    fd = nltk.FreqDist(tokens)\n    info = {\n        'Empresa': x,\n        'Ocorr\u00eancias': len(temp_dataframe),\n        'Palavras Frequentes': fd.most_common(25)\n    }\n    content.append(info)\nempresas_count = pd.DataFrame(content)\nempresas_count[empresas_count['Ocorr\u00eancias'] > 0]","b9df6fa5":"df_noticias = pd.read_excel('..\/input\/noticiasbeatplastic.xlsx', encoding='utf-8-sig')","70958d43":"len(df_noticias)","704b4f76":"import datetime\ndf_noticias['DATA DE PUBLICA\u00c7\u00c3O'] = df_noticias['DATA DE PUBLICA\u00c7\u00c3O'].map(lambda x:datetime.datetime.strptime(x, \"%d\/%m\/%Y\").strftime(\"%Y-%m-%d\"))\ndf_noticias['DATA DE PUBLICA\u00c7\u00c3O'] = pd.to_datetime(df_noticias['DATA DE PUBLICA\u00c7\u00c3O'])\ndf_nottempo = df_noticias\ndf_nottempo = df_nottempo.groupby('DATA DE PUBLICA\u00c7\u00c3O').count().reset_index()\ndf_nottempo = df_nottempo.sort_values(by='DATA DE PUBLICA\u00c7\u00c3O')\nx = df_nottempo['DATA DE PUBLICA\u00c7\u00c3O']\ny = df_nottempo['URL']\nplt.xlabel('Data')\nplt.ylabel('Quantidade de Ocorr\u00eancias')\nplt.xticks(rotation=45)\nplt.title('N\u00famero de Ocorr\u00eancias por dia - Not\u00edcias #Beat Plastic Pollution')\nplt.plot(x,y, label='Tendencia Di\u00e1ria')\nplt.show()\n#df_noticias['DATA DE PUBLICA\u00c7\u00c3O']","8010e36d":"palavras = df_noticias[\"T\u00cdTULO\"].astype(\"str\").dropna()\npalavras =''.join(palavras)\npalavras = re.sub(r'[^\\x00-\\x7F]+',' ', palavras)\npalavras = re.sub('http[s]?:\\\/\\\/([\\w]+.?[\\w]+.?\\\/?)+','',palavras)\npalavras = re.sub(r'[\\W]+',' ',palavras)\ntokens = word_tokenize(palavras)\ntokens = [w.lower() for w in tokens if w.isalpha()]\nstopwords = nltk.corpus.stopwords.words('english')\ntokens = [w for w in tokens if w.lower() not in stopwords]\ntokens = [w for w in tokens if len(w) > 3]\nfd = nltk.FreqDist(tokens)\nprint('25 palavras mais comum nos t\u00edtulos:')\nprint(fd.most_common(25))","0c6b83ea":"df_noticias['FONTE'].value_counts()[:15]","6be24c44":"df_noticias['LOCALIDADE'].value_counts()[:20]","e4af9180":"df_noticias.groupby('FONTE').first().reset_index()[['FONTE', 'AUDI\u00caNCIA']].sort_values(by='AUDI\u00caNCIA', ascending=False).head(15)","7ba792ab":"df_noticiasplastic = pd.read_excel('..\/input\/noticiasplasticfreejuly.xlsx', encoding='utf-8-sig')","af1ec49c":"len(df_noticiasplastic)","74063587":"import datetime\ndf_noticiasplastic['DATA DE PUBLICA\u00c7\u00c3O'] = df_noticiasplastic['DATA DE PUBLICA\u00c7\u00c3O'].map(lambda x:datetime.datetime.strptime(x, \"%d\/%m\/%Y\").strftime(\"%Y-%m-%d\"))\ndf_noticiasplastic['DATA DE PUBLICA\u00c7\u00c3O'] = pd.to_datetime(df_noticiasplastic['DATA DE PUBLICA\u00c7\u00c3O'])\ndf_nottempo2 = df_noticiasplastic\ndf_nottempo2 = df_nottempo2.groupby('DATA DE PUBLICA\u00c7\u00c3O').count().reset_index()\ndf_nottempo2 = df_nottempo2.sort_values(by='DATA DE PUBLICA\u00c7\u00c3O')\nx = df_nottempo2['DATA DE PUBLICA\u00c7\u00c3O']\ny = df_nottempo2['URL']\nplt.xlabel('Data')\nplt.ylabel('Quantidade de Ocorr\u00eancias')\nplt.xticks(rotation=45)\nplt.title('N\u00famero de Ocorr\u00eancias por dia - Not\u00edcias #Plastic Free July')\nplt.plot(x,y, label='Tendencia Di\u00e1ria')\nplt.show()\n#df_noticias['DATA DE PUBLICA\u00c7\u00c3O']","b5094d72":"palavras = df_noticiasplastic[\"T\u00cdTULO\"].astype(\"str\").dropna()\npalavras =''.join(palavras)\npalavras = re.sub(r'[^\\x00-\\x7F]+',' ', palavras)\npalavras = re.sub('http[s]?:\\\/\\\/([\\w]+.?[\\w]+.?\\\/?)+','',palavras)\npalavras = re.sub(r'[\\W]+',' ',palavras)\ntokens = word_tokenize(palavras)\ntokens = [w.lower() for w in tokens if w.isalpha()]\nstopwords = nltk.corpus.stopwords.words('english')\ntokens = [w for w in tokens if w.lower() not in stopwords]\ntokens = [w for w in tokens if len(w) > 3]\nfd = nltk.FreqDist(tokens)\nprint('25 palavras mais comum nos t\u00edtulos das not\u00edcias de Plastic Free July:')\nprint(fd.most_common(25))","daff7257":"df_noticiasplastic['FONTE'].value_counts()[:15]","d5402c3b":"df_noticiasplastic['LOCALIDADE'].value_counts()[:20]","fd8b076d":"df_noticiasplastic.groupby('FONTE').first().reset_index()[['FONTE', 'AUDI\u00caNCIA']].sort_values(by='AUDI\u00caNCIA', ascending=False).head(15)","ef044e53":"df_noticiasacabe = pd.read_excel('..\/input\/noticiasacabe.xlsx', encoding='utf-8-sig')","7bd0e7c1":"len(df_noticiasacabe)","d66d4de3":"import datetime\ndf_noticiasacabe['DATA DE PUBLICA\u00c7\u00c3O'] = df_noticiasacabe['DATA DE PUBLICA\u00c7\u00c3O'].map(lambda x:datetime.datetime.strptime(x, \"%d\/%m\/%Y\").strftime(\"%Y-%m-%d\"))\ndf_noticiasacabe['DATA DE PUBLICA\u00c7\u00c3O'] = pd.to_datetime(df_noticiasacabe['DATA DE PUBLICA\u00c7\u00c3O'])\ndf_nottempo3 = df_noticiasacabe\ndf_nottempo3 = df_nottempo3.groupby('DATA DE PUBLICA\u00c7\u00c3O').count().reset_index()\ndf_nottempo3 = df_nottempo3.sort_values(by='DATA DE PUBLICA\u00c7\u00c3O')\nx = df_nottempo3['DATA DE PUBLICA\u00c7\u00c3O']\ny = df_nottempo3['URL']\nplt.xlabel('Data')\nplt.ylabel('Quantidade de Ocorr\u00eancias')\nplt.xticks(rotation=45)\nplt.title('N\u00famero de Ocorr\u00eancias por dia - Not\u00edcias #Plastic Free July')\nplt.plot(x,y, label='Tendencia Di\u00e1ria')\nplt.show()\n#df_noticias['DATA DE PUBLICA\u00c7\u00c3O']","fd859e5e":"palavras = df_noticiasacabe[\"T\u00cdTULO\"].astype(\"str\").dropna()\npalavras =''.join(palavras)\npalavras = re.sub(r'[^\\x00-\\x7F]+',' ', palavras)\npalavras = re.sub('http[s]?:\\\/\\\/([\\w]+.?[\\w]+.?\\\/?)+','',palavras)\npalavras = re.sub(r'[\\W]+',' ',palavras)\ntokens = word_tokenize(palavras)\ntokens = [w.lower() for w in tokens if w.isalpha()]\nstopwords = nltk.corpus.stopwords.words('portuguese')\ntokens = [w for w in tokens if w.lower() not in stopwords]\ntokens = [w for w in tokens if len(w) > 3]\nfd = nltk.FreqDist(tokens)\nprint('25 palavras mais comum nos t\u00edtulos das not\u00edcias de Plastic Free July:')\nprint(fd.most_common(25))","24a4bd2f":"df_noticiasacabe['FONTE'].value_counts()[:15]","b4f2ccf8":"df_noticiasacabe['LOCALIDADE'].value_counts()[:15]","00f196bb":"df_noticiasacabe.groupby('FONTE').first().reset_index()[['FONTE', 'AUDI\u00caNCIA']].sort_values(by='AUDI\u00caNCIA', ascending=False).head(15)","17baa3b4":"# Empresas Mencionadas","f1d9a6b8":"# Not\u00edcias #Acabe Com A Polui\u00e7\u00e3o Pl\u00e1stica","949d602d":"Palavras Frequentes no T\u00edtulo","9b4ccd89":"Quantidade de Not\u00edcias","e1dade31":"Localidade das Not\u00edcias","13300575":"Fontes Mais Frequentes","9da07149":"# Tweets por dia","f62718c3":"# Usu\u00e1rios","bc3daa2b":"Fontes Mais Frequentes","081e3fca":"# Quantidade de Ocorr\u00eancias","4d1423d6":"# Maior n\u00famero de ocorr\u00eancias em um dia","32ae6b0c":"# Tweets mais Retuitados","2a0e14fd":"Localidades das Not\u00edcias","904ea9d5":"# Not\u00edcias - BeatPlastic Pollution","81b7e5ce":"# Not\u00edcias - Plastic Free July","d10ba5f0":"Timeline das Not\u00edcias","adea76f4":"# Usu\u00e1rios mais mencionados","3550fb79":"Localidade das Not\u00edcias","ca2715e2":"Palavras Frequentes no T\u00edtulo","43886e86":"Top Fontes por audi\u00eancia","7ae7cbb1":"Fontes mais frequentes","e8befec4":"# Usu\u00e1rios mais ativos","bfbe7973":"Timeline das Not\u00edcias","2c80761e":"# Posts com maior alcance (StopSucking)","b176d382":"# Timeline por Hashtag","183070bc":"# Hashtags mais utilizadas","ce3a605f":"Timeline das Not\u00edcias","c96f8bcc":"# Posts com maior alcance (PlasticFreeJuly)","3f81eec2":"# Por Localidade","01aab149":"# Posts com maior Alcance","42f6e48f":"Top Fontes por Audi\u00eancia","d9b0dc89":"Quantidade de Not\u00edcias","7df9e284":"# # Dias","79ac220c":"Palavras mais Frequentes nos T\u00edtulos\n","6d72485f":"# Por Rede Social","5f44cb64":"Quantidade de Not\u00edcias","d34f7e7c":"Top Fontes por Audi\u00eancia ","dcc152af":"# Palavras em comum com cada Hashtag","95c84447":"## Usu\u00e1rios mais seguidos"}}