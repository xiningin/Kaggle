{"cell_type":{"2465a6af":"code","b6f04f53":"code","3ae1bb9d":"code","ad1f5167":"code","144968ba":"code","d3d2261b":"code","6a7ae645":"code","a6ac9340":"code","49c089ca":"code","b761d19f":"code","8c119da7":"code","69f7e9ad":"code","40e7a540":"code","568ac733":"code","fa63d047":"code","1367cc4b":"code","3e3c2461":"code","777b3c11":"code","072cadd4":"code","b993c7b4":"code","6d4d2299":"code","d2006a9e":"code","25561047":"code","8d141ace":"code","4fddb0c4":"code","f39b5050":"code","3917dc42":"code","54d2a1b9":"code","90bc1f1b":"code","60e97171":"code","2f05233d":"code","51abe8fc":"code","d264f026":"code","26553335":"code","0749e77d":"code","528bd75d":"code","69ee7f8b":"code","b49e3388":"code","df2e7a15":"code","142e528d":"markdown","e86b83bd":"markdown","0a08e302":"markdown","0e83322d":"markdown","caee6722":"markdown","9da38af7":"markdown","caeed3ad":"markdown","733efcd5":"markdown","c6681c69":"markdown","641a7c62":"markdown","6e59d766":"markdown","58a145f5":"markdown","355a0cec":"markdown","eb8b4919":"markdown","16eb851a":"markdown","49ba40af":"markdown","a177f027":"markdown","b306d2a2":"markdown","5bf1ec37":"markdown","adc545f1":"markdown","a53775c7":"markdown","94074091":"markdown","78f194c4":"markdown","2c7139cc":"markdown","2aa8dac4":"markdown","5a3b8cd3":"markdown","74f9955c":"markdown","6a5c91e3":"markdown"},"source":{"2465a6af":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","b6f04f53":"df = pd.read_csv('..\/input\/creditcard.csv')","3ae1bb9d":"df.head()","ad1f5167":"df.describe()","144968ba":"df.isnull().sum()","d3d2261b":"print('Fraud \\n',df.Time[df.Class==1].describe(),'\\n',\n      '\\n Non-Fraud \\n',df.Time[df.Class==0].describe())","6a7ae645":"plt.figure(figsize=(12,30*4))\nimport matplotlib.gridspec as gridspec\nfeatures = df.iloc[:,0:30].columns\ngs = gridspec.GridSpec(30, 1)\nfor i, feature in enumerate(df[features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[feature][df.Class == 1], bins=50)\n    sns.distplot(df[feature][df.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('Feature: ' + str(feature))\nplt.show()","a6ac9340":"df2 = df.drop(['V15','V20','V22','V23','V25','V28', 'Time', 'Amount'], axis=1)","49c089ca":"from sklearn.metrics import confusion_matrix\ndef plot_cm(classifier, predictions):\n    cm = confusion_matrix(y_test, predictions)\n    \n    plt.clf()\n    plt.imshow(cm, interpolation='nearest', cmap='RdBu')\n    classNames = ['Normal','Fraud']\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames, rotation=45)\n    plt.yticks(tick_marks, classNames)\n    s = [['TN','FP'], ['FN', 'TP']]\n    \n    for i in range(2):\n        for j in range(2):\n            plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]), \n                     horizontalalignment='center', color='White')\n    \n    plt.show()\n        \n    tn, fp, fn, tp = cm.ravel()\n\n    recall = tp \/ (tp + fn)\n    precision = tp \/ (tp + fp)\n    F1 = 2*recall*precision\/(recall+precision)\n\n    print('Recall={0:0.3f}'.format(recall),'\\nPrecision={0:0.3f}'.format(precision))\n    print('F1={0:0.3f}'.format(F1))","b761d19f":"from sklearn.metrics import average_precision_score, precision_recall_curve\ndef plot_aucprc(classifier, scores):\n    precision, recall, _ = precision_recall_curve(y_test, scores, pos_label=0)\n    average_precision = average_precision_score(y_test, scores)\n\n    print('Average precision-recall score: {0:0.3f}'.format(\n          average_precision))\n\n    plt.plot(recall, precision, label='area = %0.3f' % average_precision, color=\"green\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision Recall Curve')\n    plt.legend(loc=\"best\")\n    plt.show()","8c119da7":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX = df2.iloc[:,:-1]\ny = df2.iloc[:,-1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)","69f7e9ad":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier","40e7a540":"pre = RandomForestClassifier(n_jobs=-1, random_state = 42,\n                             max_features= 'sqrt', \n                             criterion = 'entropy')\npre.fit(X_train, y_train)\n\n#Make predictions\ny_pred = pre.predict(X_test)\ntry:\n    scores = pre.decision_function(X_test)\nexcept:\n    scores = pre.predict_proba(X_test)[:,1]\n\n#Make plots\nplot_cm(pre, y_pred)\nplot_aucprc(pre, scores)","568ac733":"#from sklearn.model_selection import GridSearchCV\n#param_grid = { \n#    'n_estimators': [10, 500],\n#    'max_features': ['auto', 'sqrt', 'log2'],\n#    'min_samples_leaf' : [len(X)\/\/10000, len(X)\/\/28000, \n#                          len(X)\/\/50000, len(X)\/\/100000]\n#}\n\n#CV_rfc = GridSearchCV(estimator=pre, \n#                      param_grid=param_grid, \n#                      scoring = 'f1',\n#                      cv=10, \n#                      n_jobs=10,\n#                      verbose=2,\n#                      pre_dispatch='2*n_jobs',\n#                      refit=False)\n#CV_rfc.fit(X_train, y_train)\n\n#CV_rfc.best_params_","fa63d047":"#rfc = RandomForestClassifier(n_jobs=-1, random_state = 42,\n#                             n_estimators=CV_rfc.best_params_['n_estimators'], \n#                             min_samples_leaf=CV_rfc.best_params_['min_samples_leaf'], \n#                             max_features= CV_rfc.best_params_['max_features'])\n\n#RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n#            max_depth=None, max_features='auto', max_leaf_nodes=None,\n#            min_impurity_split=1e-07, min_samples_leaf=2,\n#            min_samples_split=2, min_weight_fraction_leaf=0.0,\n#            n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n#            verbose=0, warm_start=False)\n\n\nrfc = RandomForestClassifier(n_jobs=-1, random_state = 42,\n                             n_estimators=500, \n                             max_features='auto',\n                             min_samples_leaf=2,\n                             criterion = 'entropy')\n\nrfc.fit(X_train, y_train)","1367cc4b":"#Make predictions\ny_pred = rfc.predict(X_test)\ntry:\n    scores = rfc.decision_function(X_test)\nexcept:\n    scores = rfc.predict_proba(X_test)[:,1]\n\n#Make plots\nplot_cm(rfc, y_pred)\nplot_aucprc(rfc, scores)","3e3c2461":"# Fitting XGBoost to the Training set\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(random_state = 42, n_jobs = -1)\nxgb.fit(X_train, y_train)","777b3c11":"#Make predictions\ny_pred = xgb.predict(X_test)\ntry:\n    scores = xgb.decision_function(X_test)\nexcept:\n    scores = xgb.predict_proba(X_test)[:,1]\n#Make plots\ny_pred = xgb.predict(X_test)\nplot_cm(xgb, y_pred)\nplot_aucprc(xgb, scores)","072cadd4":"fraud_ratio=y_train.value_counts()[1]\/y_train.value_counts()[0]\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'max_depth': [1,3,5], \n             'min_child_weight': [1,3,5], \n             'n_estimators': [100,200,500,1000], \n             'scale_pos_weight': [1, 0.1, 0.01, fraud_ratio]}","b993c7b4":"#CV_GBM = GridSearchCV(estimator = xgb, \n#                      param_grid = param_grid,\n#                      scoring = 'f1', \n#                      cv = 10, \n#                      n_jobs = -1,\n#                      refit = True)\n\n#CV_GBM.fit(X_train, y_train)\n\n#CV_GBM.best_params_","6d4d2299":"#optimized_GBM = XGBClassifier(n_jobs=-1, random_state = 42,\n#                             n_estimators=CV_GBM.best_params_['n_estimators'], \n#                             max_depth=CV_GBM.best_params_['max_depth'],\n#                             min_child_weight=CV_GBM.best_params_['min_child_weight'],\n#                             criterion = 'entropy')\noptimized_GBM = XGBClassifier(n_jobs=-1, random_state = 42,\n                             n_estimators=100, \n                             max_depth=1,\n                             min_child_weight=1,\n                             criterion = 'entropy',\n                             scale_pos_weight=fraud_ratio)\noptimized_GBM.fit(X_train, y_train)","d2006a9e":"#Make predictions\ny_pred = optimized_GBM.predict(X_test)\ntry:\n    scores = optimized_GBM.decision_function(X_test)\nexcept:\n    scores = optimized_GBM.predict_proba(X_test)[:,1]\n    \n#Make plots\nplot_cm(optimized_GBM, y_pred)\nplot_aucprc(optimized_GBM, scores)","25561047":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Fitting SVM to the Training set\nfrom sklearn.svm import OneClassSVM\nclassifier = OneClassSVM(kernel=\"rbf\", random_state = 42)\nclassifier.fit(X_train, y_train)","8d141ace":"#Make predictions\ny_pred = classifier.predict(X_test)\ny_pred = np.array([y==-1 for y in y_pred])\n\ntry:\n    scores = classifier.decision_function(X_test)\nexcept:\n    scores = classifier.predict_proba(X_test)[:,1]\n\n#Make plots\nplot_cm(classifier, y_pred)\nplot_aucprc(classifier, scores)","4fddb0c4":"df3 = df2#.sample(frac = 0.1, random_state=42)\ntrain = df3[df3.Class==0].sample(frac=0.75, random_state = 42)\n\nX_train = train.iloc[:,:-1]\ny_train = train.iloc[:,-1]\n\nX_test = df3.loc[~df3.index.isin(X_train.index)].iloc[:,:-1]#.sample(frac=.50, random_state = 42)\ny_test = df3.loc[~df3.index.isin(y_train.index)].iloc[:,-1]#.sample(frac=.50, random_state = 42)\n\n#X_cval = df3.loc[~df3.index.isin(X_test.index)& ~df3.index.isin(X_train.index)].iloc[:,:-1]\n#y_cval = df3.loc[~df3.index.isin(y_test.index)& ~df3.index.isin(X_train.index)].iloc[:,-1]","f39b5050":"print('df3', df3.shape,'\\n',\n      'train',train.shape,'\\n',\n      'X_train',X_train.shape,'\\n',\n      'y_train',y_train.shape,'\\n',\n      'X_test',X_test.shape,'\\n',\n      'y_test',y_test.shape,'\\n', \n      #'X_val',X_cval.shape,'\\n',\n      #'y_val',y_cval.shape,'\\n'\n     )","3917dc42":"df3.shape[0] == train.shape[0] + X_test.shape[0]","54d2a1b9":"def covariance_matrix(X):\n    X = X.values\n    m, n = X.shape \n    tmp_mat = np.zeros((n, n))\n    mu = X.mean(axis=0)\n    for i in range(m):\n        tmp_mat += np.outer(X[i] - mu, X[i] - mu)\n    return tmp_mat \/ m","90bc1f1b":"cov_mat = covariance_matrix(X_train)","60e97171":"cov_mat_inv = np.linalg.pinv(cov_mat)\ncov_mat_det = np.linalg.det(cov_mat)\ndef multi_gauss(x):\n    n = len(cov_mat)\n    return (np.exp(-0.5 * np.dot(x, np.dot(cov_mat_inv, x.T))) \n            \/ (2. * np.pi)**(n\/2.) \n            \/ np.sqrt(cov_mat_det))","2f05233d":"eps = min([multi_gauss(x) for x in X_train.values])\npredictions = np.array([multi_gauss(x) <= eps for x in X_test.values])\ny_test = np.array(y_test, dtype=bool)","51abe8fc":"cm = confusion_matrix(y_test, predictions)\n\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap='RdBu')\nclassNames = ['Normal','Fraud']\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]), \n                 horizontalalignment='center', color='White')\n\nplt.show()\n\ntn, fp, fn, tp = cm.ravel()\n\nrecall = tp \/ (tp + fn)\nprecision = tp \/ (tp + fp)\nF1 = 2*recall*precision\/(recall+precision)\n\nprint(\"recall=\",recall,\"\\nprecision=\",precision)\nprint(\"F1=\",F1)","d264f026":"from scipy.stats import multivariate_normal\nfrom sklearn.metrics import f1_score\n\ndef feature_normalize(dataset):\n    mu = np.mean(dataset,axis=0)\n    sigma = np.std(dataset,axis=0)\n    return (dataset - mu)\/sigma\n\ndef estimateGaussian(dataset):\n    mu = np.mean(dataset, axis=0)\n    sigma = np.cov(dataset.T)\n    return mu, sigma\n    \ndef multivariateGaussian(dataset,mu,sigma):\n    p = multivariate_normal(mean=mu, cov=sigma, allow_singular=True)\n    return p.pdf(dataset)\n\ndef selectThresholdByCV(probs,gt):\n    best_epsilon = 0\n    best_f1 = 0\n    f = 0\n    stepsize = (max(probs) - min(probs)) \/ 1000;\n    epsilons = np.arange(min(probs),max(probs),stepsize)\n    for epsilon in np.nditer(epsilons):\n        predictions = (probs < epsilon)\n        f = f1_score(gt, predictions, average = \"binary\")\n        if f > best_f1:\n            best_f1 = f\n            best_epsilon = epsilon\n    return best_f1, best_epsilon","26553335":"#fit the model\nmu, sigma = estimateGaussian(X_train)\np = multivariateGaussian(X_train,mu,sigma)\n\np_cv = multivariateGaussian(X_test,mu,sigma)\nfscore, ep = selectThresholdByCV(p_cv,y_test)\noutliers = np.asarray(np.where(p < ep))","0749e77d":"predictions = np.array([p_cv <= ep]).transpose()\ny_test = np.array(y_test, dtype=bool)\n\ncm = confusion_matrix(y_test, predictions)\n\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap='RdBu')\nclassNames = ['Normal','Fraud']\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]), \n                 horizontalalignment='center', color='White')\n\nplt.show()\n\ntn, fp, fn, tp = cm.ravel()\n\nrecall = tp \/ (tp + fn)\nprecision = tp \/ (tp + fp)\nF1 = 2*recall*precision\/(recall+precision)\n\nprint(\"recall=\",recall,\"\\nprecision=\",precision)\nprint(\"F1=\",F1)","528bd75d":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX = df2.iloc[:,:-1]\ny = df2.iloc[:,-1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)","69ee7f8b":"from sklearn.ensemble import BaggingClassifier\nfrom imblearn.ensemble import BalancedBaggingClassifier\n\nfrom imblearn.metrics import classification_report_imbalanced\n\nbagging = BaggingClassifier(random_state=0)\nbalanced_bagging = BalancedBaggingClassifier(random_state=0)\n\nbagging.fit(X_train, y_train)\nbalanced_bagging.fit(X_train, y_train)\n\n#Make predictions\nprint('Classification of original dataset with Bagging (scikit-learn)')\ny_pred = bagging.predict(X_test)\ntry:\n    scores = bagging.decision_function(X_test)\nexcept:\n    scores = bagging.predict_proba(X_test)[:,1]\n\n#Make plots\nplot_cm(bagging, y_pred)\nplot_aucprc(bagging, scores)\n\n#Make predictions\nprint('Classification of original dataset with BalancedBagging (imbalanced-learn)')\ny_pred = balanced_bagging.predict(X_test)\ntry:\n    scores = balanced_bagging.decision_function(X_test)\nexcept:\n    scores = balanced_bagging.predict_proba(X_test)[:,1]\n\n#Make plots\nplot_cm(balanced_bagging, y_pred)\nplot_aucprc(balanced_bagging, scores)","b49e3388":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_res, y_res = smote.fit_sample(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.25, random_state = 42)\n\n#fit the best models so far\nxgb.fit(X_train, y_train)\nrfc.fit(X_train, y_train)\n\n#Make predictions\nprint('Classification of SMOTE-resampled dataset with XGboost')\ny_pred = xgb.predict(X_test)\ntry:\n    scores = xgb.decision_function(X_test)\nexcept:\n    scores = xgb.predict_proba(X_test)[:,1]\n#Make plots\ny_pred = xgb.predict(X_test)\nplot_cm(xgb, y_pred)\nplot_aucprc(xgb, scores)\n\n#Make predictions\nprint('Classification of SMOTE-resampled dataset with optimized RF')\ny_pred = rfc.predict(X_test)\ntry:\n    scores = rfc.decision_function(X_test)\nexcept:\n    scores = rfc.predict_proba(X_test)[:,1]\n\n#Make plots\nplot_cm(rfc, y_pred)\nplot_aucprc(rfc, scores)","df2e7a15":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n\n#Make predictions\ny_pred = rfc.predict(X_test)\ntry:\n    scores = rfc.decision_function(X_test)\nexcept:\n    scores = rfc.predict_proba(X_test)[:,1]\n\n#Make plots\nplot_cm(rfc, y_pred)\nplot_aucprc(rfc, scores)","142e528d":"# Fraud analysis: \n### Random Forest, XGBoost, OneClassSVM, Multivariate GMM and SMOTE, all in one cage against an imbalanced dataset.","e86b83bd":"Imbalanced dataset. Might be worth to work on upsampling\/downsampling of the data, but I will try without it for the moment and hope I get good results. Now let's check which variable is more correlated with the fraudulent activities. ","0a08e302":"Check for NaNs","0e83322d":"## 1.2. Test a Random Forest model","caee6722":"\"Pretty cool huh?\"","9da38af7":"So, the imbalanced-learn packages without resampling of the data allows us to have higher true positive numbers, but lowers the true negative, meaning that it's just mistakenly saying there are more frauds than in reality. Not good.  \nLet's try again, with SMOTE, which will produce synthetical samples of the under-represented class","caeed3ad":"Data preparation and general functions for plots","733efcd5":"The F-1 score is not that bad! Let's try to fine tune some parameters and see if we can improve that.\n\n*Note: since it takes too long for the Kaggle kernel, I executed it on my computer and here I am just showing the results of the GridSearchCV*","c6681c69":"I don't really like these results, honestly... ","641a7c62":"Yass! Nice increase! Now let's see if I can get any better with the most popular boosting algorithm...","6e59d766":"## 2.  Working on imbalanced dataset: upsampling of the underrepresented class","58a145f5":"I wonder if it should be treated as a data series rather than a table... ","355a0cec":"## 1.5. Test a (Multivariate) GMM module","eb8b4919":"## 1.4. OneClassSVM?","16eb851a":"This should be, according to Scikit-learn tutorials, the best algorithm to infer anomalies in an imbalanced dataset. Let's give it a try.","49ba40af":"WOW! So, if we now use this new RF classifier (which parameters were optimized on the resampled dataset) on the ORIGINAL dataset, we'll get our perfect fraud analysis prediction.","a177f027":"Adapting NG's code from MatLab","b306d2a2":"I will now test a series of different machine learning models (no Neural Networks!) to see which one performs better, with some optimization here and there. ","5bf1ec37":"WOW! Seriously, no NaNs? \n\nOk, let's check for the classes distributions","adc545f1":"Per Ng's lessons, we should divide the dataset into a training set with ONLY normal transactions, and validation and test set containing all the fraudulent transations. I will try at first without crossvalidation.","a53775c7":"Ok, now we're talking. Any chances of getting better with optimization?","94074091":"First, let's reset our original dataset, without the unwanted features.","78f194c4":"Remove the features that do not have significantly different distributions between the two classes (i.e. will not contribute to our model).","2c7139cc":"The package imbalanced-learn (not yet part of the official scikitlearn) contains an imbalanced classifier which should be able, using a bagging method, to increase our prediction capabilities without resampling the dataset.","2aa8dac4":"## 1.3. Test a XGBoost model","5a3b8cd3":"## 1. Supervised learning tests","74f9955c":"## 2.1 BalancedBaggingClassifier","6a5c91e3":"### 1.1 Data import, quick view and functions"}}