{"cell_type":{"3386f544":"code","8b053843":"code","289c9dab":"code","b67f8882":"code","46183074":"code","9aa2160c":"code","34f587ff":"code","a5c2fc5c":"code","a7bd7d4b":"code","4fb8d460":"code","b558547d":"code","c58fff89":"markdown","ca34c929":"markdown","8abaefb7":"markdown","860d9d3c":"markdown","2f968d29":"markdown","f3b66aa7":"markdown","abfd70e0":"markdown","ee267f2e":"markdown","b99f69ab":"markdown","c47f1565":"markdown"},"source":{"3386f544":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b053843":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n\nd_18 = pd.read_csv(\"\/kaggle\/input\/2018qb\/2018.csv\")\nd_17 = pd.read_csv(\"\/kaggle\/input\/2017qb\/2017.csv\")\nd_16 = pd.read_csv(\"\/kaggle\/input\/2016qb\/2016.csv\")\n\ndf = pd.concat([d_18, d_17, d_16], ignore_index=True)\ndf2 = df.drop(columns = ['Rk', 'Tm', 'Age', 'Pos', 'G', 'GS', 'QBrec', 'Lng', 'GWD', '4QC', 'Cmp', 'Att', 'Rate'])\nnames = df2[[\"Player\"]]\ndf3 = df2.drop(columns = ['Player'])\ndf3 = df3.astype(float)\ndf3 = df3.dropna().reindex()\ndf2","289c9dab":"df2.describe()","b67f8882":"%matplotlib inline\ng = sns.PairGrid(df2)\ng = g.map_upper(plt.scatter,marker='+')\ng = g.map_lower(sns.kdeplot, cmap=\"hot\",shade=True)\ng = g.map_diag(sns.kdeplot, shade=True)","46183074":"from sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\nX_train, X_test, y_train, y_test = train_test_split(df3[[\"Cmp%\", \"Yds\",\"TD\",\"TD%\",\"Int\",\"Int%\",\"1D\",\"Y\/A\",\"AY\/A\",\"Y\/C\",\"Y\/G\",\"Sk\",\"Yds.1\",\"NY\/A\",\"ANY\/A\",\"Sk%\"]], df2[[\"QBR\\n\"]], test_size=0.2, random_state=0)\n\ntrain = xgb.DMatrix(X_train, label=y_train)\ntest = xgb.DMatrix(X_test, label=y_test)","9aa2160c":"from sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot\n\niters = []\nerror = []\nfor i in np.linspace(0.1,1,15):\n    for j in range(1,16,1):\n        param = {\n        'eval_metric':'rmse',\n        'max_depth': j,\n        'eta': i,\n        'num_class': 1\n        } \n        epochs = 25 \n\n        model = xgb.train(param, train, epochs)\n        predictions = model.predict(test)\n        mse = mean_squared_error(y_test,predictions)\n        error.append(mse)\n        iters.append((i, j))\n\n        \nx_axis = np.linspace(0,86,225)\nfig, ax = pyplot.subplots()\nax.plot(x_axis, error)\npyplot.ylabel('Mean Squared Error')\npyplot.title('Error v.s Iteration')\npyplot.show()","34f587ff":"test2 = [x < 40 for x in error]\niter_index = [i for i in enumerate(error)]\niter_index\niters[120]\n","a5c2fc5c":"param_optimal = {\n        'eval_metric':'rmse',\n        'max_depth': 1,\n        'eta': 0.6142857142857143,\n        'num_class': 1\n        } \nepochs_optimal = 20\n\nmodel_optimal = xgb.train(param_optimal, train, epochs_optimal)\npredictions_optimal = model_optimal.predict(test)\nmse_optimal = mean_squared_error(y_test,predictions_optimal)\n\nprint(mse_optimal)\n#print(predictions_optimal)\n","a7bd7d4b":"merged = names.merge(y_test, left_index=True, right_index=True, how='inner')\nmerged.head()\n#names2 = names.merge(y_test,left_index= [y_test.index == names.index])","4fb8d460":"y_test.index\ndf_results = pd.merge(merged, X_test, how='outer', on = y_test.index)\ndf_results.insert(2, 'preds', predictions_optimal)\ndf_results","b558547d":"plt.scatter(df_results.index, df_results[\"preds\"].values)\nplt.scatter(df_results.index, df_results[\"QBR\\n\"].values)","c58fff89":"Merges two data frames of unequal length based on index","ca34c929":"Finding the paramters which minimize the mean squared error. ","8abaefb7":"In this notebook, I'm trying to predict QBR ratings based on NFL QBs from 2016-2018 seasons. I'm using an XG Boost model to train the data, and I'm finding the paramters that optimize the model. ","860d9d3c":"Looping over the 'eta' (learning rate) and max depth parameters. ","2f968d29":"Testing the predictions of the model given the optimal paramters. As you can see we get quite close to the testing set","f3b66aa7":"I chose to through away all the categorical data; along with \"rate\" which is a state similar to QBR that I thought might contradict the model. ","abfd70e0":"Splitting up my data into training and testing sets.","ee267f2e":"These are heatmaps between the features of the model.","b99f69ab":"These is the complete testing data along with predictions.","c47f1565":"Plotting the results"}}