{"cell_type":{"6178f501":"code","0fc09eb9":"code","8b760bfe":"code","cff61ac3":"code","92a6e546":"code","ef6c80e4":"code","9cc816c5":"code","6edb8a03":"code","c3e0d380":"code","4753ef7d":"code","a76efdf5":"code","6523fa58":"code","3367824d":"code","e44b9509":"code","548677a3":"code","b520f9e9":"code","521f8541":"code","7a707944":"code","acfbff71":"code","bbf251c2":"code","59e7bbd0":"code","c42aa857":"code","05852acd":"code","76287d0e":"code","2880be9e":"code","9f7bc2a8":"code","3a9abe98":"code","91df2910":"code","dba6abfe":"code","d72705de":"code","646c82e7":"code","3c296aa4":"code","283ba3fb":"code","349ad623":"code","028d4857":"code","1824a19e":"code","cafc6200":"code","a29aa96a":"code","7fa11c77":"code","4812486d":"code","a750900d":"code","7b0b1f04":"code","ec5c3991":"code","bdc15205":"code","bcc0cc88":"code","9d75facb":"code","5c47f120":"code","f77522e8":"code","cd484f23":"code","4db699dd":"code","c7eb71da":"code","0596d248":"code","d2d3673b":"code","d05ced98":"code","937c8a2c":"code","a7e816c3":"code","e9231400":"code","8404f72d":"code","3a4ad531":"code","5009fd79":"code","771d13d4":"code","981a112b":"code","062c027e":"code","0eb210c6":"code","4ba9e402":"code","1ab04415":"code","d908fd0e":"code","171373cf":"code","0b975c78":"code","e9f7660a":"code","ee96832d":"code","fd332be0":"code","2b4a1d60":"code","dc96574c":"code","99778a33":"code","fa347434":"code","056cd64c":"code","a4a682a3":"code","1eb25c8e":"code","412146a0":"code","604f856d":"code","ce57255e":"code","eedd6839":"code","9e23b0cf":"code","ceeeca5c":"code","e210d083":"code","b49d6d16":"code","341ede1c":"code","1d4fc1ea":"markdown","074a2029":"markdown","5ff84c09":"markdown","f20e12ec":"markdown","a3070ca3":"markdown","947806f0":"markdown","a8921ab1":"markdown","da465234":"markdown","db9ca9e1":"markdown","c754308d":"markdown","ee263fc1":"markdown","d2dad0bb":"markdown","c2ddf659":"markdown","29be86bd":"markdown","f697d84b":"markdown","cc2eac35":"markdown","0dd0762e":"markdown","25fb9972":"markdown","abc97aab":"markdown","354a7a5d":"markdown","989293aa":"markdown","d92ee7e6":"markdown","c9104ed1":"markdown","8b5a0c80":"markdown","08c0c6a9":"markdown","4791f4a9":"markdown","ad1cbf20":"markdown","7603be36":"markdown","1589540f":"markdown","ba30d7a4":"markdown","f229e86b":"markdown","cc4a69ec":"markdown","4a588c99":"markdown"},"source":{"6178f501":"import pandas as pd\nfrom pandas import DataFrame\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import rc\nimport plotly.graph_objs as go\nfrom sklearn import preprocessing\nimport matplotlib\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.tree import DecisionTreeRegressor\nimport math\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn import tree\nfrom sklearn.tree import export_graphviz\n# from sklearn.externals.six import StringIO\nfrom IPython.display import Image\n# import pydotplus\nfrom sklearn.ensemble import RandomForestRegressor","0fc09eb9":"vehicles = pd.read_csv(r\"..\/input\/craigslist-carstrucks-data\/vehicles.csv\") #reading the file","8b760bfe":"vehicles.head()","cff61ac3":"vehicles.shape","92a6e546":"vehicles.columns","ef6c80e4":"vehicles.drop(['id', 'url','region_url', 'vin', 'image_url', 'lat', 'long', 'description'], axis=1, inplace=True)","9cc816c5":"vehicles.info()","6edb8a03":"print(vehicles.isnull().sum())","c3e0d380":"null_val = vehicles.isna().sum()\ndef na_filter(na, threshold = .55): #only select variables that passees the threshold\n    col_pass = []\n    for i in na.keys():\n        if na[i]\/vehicles.shape[0]<threshold:\n            col_pass.append(i)\n    return col_pass\nvehicles_cleaned = vehicles[na_filter(null_val)]\nvehicles_cleaned.columns","4753ef7d":"vehicles_cleaned.manufacturer.unique()","a76efdf5":"vehicles_cleaned.model.unique()","6523fa58":"vehicles_cleaned.cylinders.unique()","3367824d":"vehicles_cleaned.fuel.unique()","e44b9509":"vehicles_cleaned.title_status.unique()","548677a3":"vehicles_cleaned.transmission.unique()","b520f9e9":"vehicles_cleaned.drive.unique()","521f8541":"vehicles_cleaned.type.unique()","7a707944":"vehicles_cleaned.paint_color.unique()","acfbff71":"vehicles_cleaned.describe()","bbf251c2":"vehicles_df = vehicles_cleaned.dropna()","59e7bbd0":"vehicles_df.shape","c42aa857":"print(vehicles_df.isnull().sum())","05852acd":"plt.figure(figsize=(3,6))\nsns.boxplot(y='price', data=vehicles_df,showfliers=False);","76287d0e":"vehicles_df.price.min()","2880be9e":"vehicles_df.price.max()","9f7bc2a8":"vehicles_df = vehicles_df[vehicles_df['price']>0]","3a9abe98":"vehicles_df.shape","91df2910":"y = vehicles_df['price']\nremoved_outliers = y.between(y.quantile(.05), y.quantile(.95))\nremoved_outliers","dba6abfe":"print(removed_outliers.value_counts())","d72705de":"index_names = vehicles_df[~removed_outliers].index # INVERT removed_outliers!!\nprint(index_names) # The resulting 11027 prices to drop.","646c82e7":"vehicles_df.drop(index_names, inplace=True)","3c296aa4":"vehicles_df.describe()","283ba3fb":"plt.figure(figsize=(3,6))\nsns.boxplot(y='odometer', data=vehicles_df,showfliers=False);","349ad623":"vehicles_df","028d4857":"vehicles_df = vehicles_df[['price','region','year','manufacturer','model','cylinders','fuel','odometer','title_status','transmission','drive','type','paint_color','state']]","1824a19e":"y = vehicles_df['price']\nx = vehicles_df['odometer']\nplt.scatter(x, y)\nplt.xlabel('odometer')\nplt.ylabel('price')\nplt.show()","cafc6200":"sns.catplot(y=\"manufacturer\", x=\"price\",kind=\"boxen\", data=vehicles_df)","a29aa96a":"sns.catplot(x=\"drive\", y=\"price\",kind=\"bar\", palette=\"ch:.25\", data=vehicles_df)","7fa11c77":"sns.violinplot(x=vehicles_df.fuel, y=vehicles_df.price)","4812486d":"sns.catplot(y=\"type\", x=\"price\",kind=\"violin\", data=vehicles_df)","a750900d":"sns.catplot(x=\"price\", y=\"paint_color\", kind=\"boxen\",\n            data=vehicles_df)","7b0b1f04":"sns.catplot(x=\"title_status\", y=\"price\",kind=\"violin\", palette=\"ch:.25\", data=vehicles_df)","ec5c3991":"y = vehicles_df['price']\nx = vehicles_df['year']\nplt.scatter(x, y)\nplt.xlabel('year')\nplt.ylabel('price')\nplt.show()","bdc15205":"le = preprocessing.LabelEncoder()","bcc0cc88":"vehicles_df[['region','manufacturer','model','cylinders','fuel','title_status','transmission','drive'\n             ,'type','paint_color','state']] = vehicles_df[['region','manufacturer','model','cylinders','fuel','title_status',\n                                                            'transmission','drive','type','paint_color','state']].apply(le.fit_transform)","9d75facb":"vehicles_df","5c47f120":"from sklearn.preprocessing import MinMaxScaler\nvehicles_df[\"odometer\"] = np.sqrt(preprocessing.minmax_scale(vehicles_df[\"odometer\"]))","f77522e8":"vehicles_df","cd484f23":"# vehicles_df.to_csv(r'C:\/Users\/Aneri\/Desktop\/Python datsets\/vehicles_df.csv')","4db699dd":"#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = vehicles_df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","c7eb71da":"print(vehicles_df[[\"drive\",\"odometer\"]].corr())\nprint(vehicles_df[[\"odometer\",\"cylinders\"]].corr())\nprint(vehicles_df[[\"cylinders\",\"fuel\"]].corr())\nprint(vehicles_df[[\"fuel\",\"year\"]].corr())\nprint(vehicles_df[[\"year\",\"drive\"]].corr())","0596d248":"features_p = vehicles_df[[\"drive\",\"odometer\",\"cylinders\",\"fuel\",\"year\"]]\ntarget_p = vehicles_df[[\"price\"]]","d2d3673b":"#splitting our dataset randomly with the test data containing 25% of the data,\nx_train, x_test, y_train, y_test = train_test_split(features_p,target_p, \n                                                    test_size=0.25, \n                                                    random_state=0)\n\n#view number of training and testing data\nprint('Our training prediction variable contains :',len(y_train) ,'rows')\nprint('Our training independent variable contains :',len(x_train) ,'rows')\nprint('Our testing prediction variable contains :',len(y_test) ,'rows')\nprint('Our testing independent variable contains :',len(x_test) ,'rows')","d05ced98":"#run the regression model with Pearson Correlation method\nreg_model_p = LinearRegression()","937c8a2c":"#fitting the training data to the model,\nreg_model_p.fit(x_train, y_train)\n#outputs the coefficients\nprint('Intercept :', reg_model_p.intercept_[0], '\\n')\nprint(pd.DataFrame({'features':x_train.columns,'coeficients':reg_model_p.coef_[0]}))\n","a7e816c3":"#prediction\nlr_pred_p = reg_model_p.predict(x_test)","e9231400":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, lr_pred_p))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, lr_pred_p))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, lr_pred_p)))","8404f72d":"score = r2_score(y_test, lr_pred_p)\nscore","3a4ad531":"features = vehicles_df.loc[:,vehicles_df.columns != 'price']\ntarget = vehicles_df.loc[:,vehicles_df.columns == 'price']","5009fd79":"#Adding constant column of ones, mandatory for sm.OLS model\nX_1 = sm.add_constant(features)\nX_1\n#Fitting sm.OLS model\nmodel = sm.OLS(target,X_1).fit()\nmodel.pvalues","771d13d4":"#Backward Elimination\ncols = list(features.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = features[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(target,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","981a112b":"features_be = vehicles_df.loc[:,vehicles_df.columns != 'price']\ntarget_be = vehicles_df.loc[:,vehicles_df.columns == 'price']","062c027e":"#splitting our dataset randomly with the test data containing 25% of the data,\nX_train, X_test, y_train, y_test = train_test_split(features_be,target_be, \n                                                    test_size=0.25, \n                                                    random_state=0)\n\n#view number of training and testing data\nprint('Our training prediction variable contains :',len(y_train) ,'rows')\nprint('Our training independent variable contains :',len(X_train) ,'rows')\nprint('Our testing prediction variable contains :',len(y_test) ,'rows')\nprint('Our testing independent variable contains :',len(X_test) ,'rows')","0eb210c6":"# run the regression model with backward elimination\nreg_model_be = LinearRegression()","4ba9e402":"#fitting the training data to the model,\nreg_model_be.fit(X_train, y_train)\n#outputs the coefficients\nprint('Intercept :', reg_model_be.intercept_[0], '\\n')\nprint(pd.DataFrame({'features':X_train.columns,'coeficients':reg_model_be.coef_[0]}))","1ab04415":"lr_pred_be = reg_model_be.predict(X_test)","d908fd0e":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, lr_pred_be))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, lr_pred_be))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, lr_pred_be)))","171373cf":"from sklearn.metrics import r2_score\nscore = r2_score(y_test, lr_pred_be)\nscore","0b975c78":"features = vehicles_df.loc[:,vehicles_df.columns != 'price']\ntarget = vehicles_df.loc[:,vehicles_df.columns == 'price']","e9f7660a":"#no of features\nnof_list=np.arange(1,13)            \nhigh_score=0\n#Variable to store the optimum features\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(features,target, test_size = 0.25, random_state = 0)\n    model = LinearRegression()\n    rfe = RFE(model,nof_list[n])\n    X_train_rfe = rfe.fit_transform(X_train,y_train.values.ravel())\n    X_test_rfe = rfe.transform(X_test)\n    model.fit(X_train_rfe,y_train.values.ravel())\n    score = model.score(X_test_rfe,y_test)\n    score_list.append(score)\n    if(score>high_score):\n        high_score = score\n        nof = nof_list[n]\nprint(\"Optimum number of features: %d\" %nof)\nprint(\"Score with %d features: %f\" % (nof, high_score))","ee96832d":"cols = list(features.columns)\nmodel = LinearRegression()\n#Initializing RFE model\nrfe = RFE(model, 12)             \n#Transforming data using RFE\nX_rfe = rfe.fit_transform(features,target.values.ravel())  \n#Fitting the data to model\nmodel.fit(X_rfe,target)              \ntemp = pd.Series(rfe.support_,index = cols)\nselected_features_rfe = temp[temp==True].index\nprint(selected_features_rfe)","fd332be0":"features_rfe = vehicles_df[['region', 'year', 'manufacturer', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'paint_color','state']]\ntarget_rfe = vehicles_df.loc[:,vehicles_df.columns == 'price']","2b4a1d60":"#splitting our dataset randomly with the test data containing 25% of the data,\nX_train, X_test, y_train, y_test = train_test_split(features_rfe,target_rfe, \n                                                    test_size=0.25, \n                                                    random_state=0)\n\n#view number of training and testing data\nprint('Our training prediction variable contains :',len(y_train) ,'rows')\nprint('Our training independent variable contains :',len(X_train) ,'rows')\nprint('Our testing prediction variable contains :',len(y_test) ,'rows')\nprint('Our testing independent variable contains :',len(X_test) ,'rows')","dc96574c":"# run the regression model for recursive feature elimination\nreg_model_rfe = LinearRegression()","99778a33":"#fitting the training data to the model,\nreg_model_rfe.fit(X_train, y_train)\n#outputs the coefficients\nprint('Intercept :', reg_model_rfe.intercept_[0], '\\n')\nprint(pd.DataFrame({'features':X_train.columns,'coeficients':reg_model_rfe.coef_[0]}))","fa347434":"lr_pred_rfe = reg_model_rfe.predict(X_test)","056cd64c":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, lr_pred_rfe))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, lr_pred_rfe))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, lr_pred_rfe)))","a4a682a3":"score = r2_score(y_test, lr_pred_rfe)\nscore","1eb25c8e":"x_final = vehicles_df[['region','year','manufacturer','model','cylinders','fuel','odometer','title_status','transmission','drive'\n             ,'type','paint_color','state']]\ny_final = vehicles_df[['price']]","412146a0":"x_train, x_test, y_train, y_test = train_test_split(x_final,y_final, test_size = 0.25, random_state=0)\n#view number of training and testing data\nprint('Our training prediction variable contains :',len(y_train) ,'rows')\nprint('Our training independent variable contains :',len(x_train) ,'rows')\nprint('Our testing prediction variable contains :',len(y_test) ,'rows')\nprint('Our testing independent variable contains :',len(x_test) ,'rows')","604f856d":"dtree = DecisionTreeRegressor()\nmodel = dtree.fit(x_train, y_train)  #train parameters: features and target\npred = dtree.predict(x_test)","ce57255e":"text_representation = tree.export_text(dtree)\nprint(text_representation)","eedd6839":"from sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, pred)))","9e23b0cf":"max_depth = []\nacc_mse = []\nacc_mae= []\nacc_friedman_mse = []\nfor i in range(1,30):\n    dtree = DecisionTreeRegressor(criterion='mse', max_depth=i)\n    dtree.fit(x_train, y_train)\n    pred = dtree.predict(x_test)\n    acc_mse.append(np.sqrt(metrics.mean_squared_error(y_test, pred)))\n    dtree = DecisionTreeRegressor(criterion='mae', max_depth=i)\n    dtree.fit(x_train, y_train)\n    pred = dtree.predict(x_test)\n    acc_mae.append(np.sqrt(metrics.mean_squared_error(y_test, pred)))\n    ####\n    dtree = DecisionTreeRegressor(criterion='friedman_mse', max_depth=i)\n    dtree.fit(x_train, y_train)\n    pred = dtree.predict(x_test)\n    acc_friedman_mse.append(np.sqrt(metrics.mean_squared_error(y_test, pred)))\n    ####\n    max_depth.append(i)\n    d = pd.DataFrame({'acc_mse':pd.Series(acc_mse), \n    'acc_mae':pd.Series(acc_mae),\n    'acc_friedman_mse':pd.Series(acc_friedman_mse),\n    'max_depth':pd.Series(max_depth)})\n                                                \n# visualizing changes in parameters\nplt.plot('max_depth','acc_mse', data=d, label='mse')\nplt.plot('max_depth','acc_mae', data=d, label='mae')\nplt.plot('max_depth','acc_friedman_mse', data=d, label='friedman_mse')\nplt.xlabel('max_depth')\nplt.ylabel('RMSE')\nplt.legend()","ceeeca5c":"dtree_m = DecisionTreeRegressor(criterion='mae',max_depth = 14)\nmodel = dtree_m.fit(x_train, y_train)  #train parameters: features and target\npred = dtree_m.predict(x_test)","e210d083":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, pred)))","b49d6d16":"rf = RandomForestRegressor(random_state=1).fit(x_train, y_train.values.ravel())\nrf_pred = rf.predict(x_test)","341ede1c":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, rf_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, rf_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, rf_pred)))","1d4fc1ea":"### Multiple Linear Regression.","074a2029":"# EDA and price prediction of used vehicles","5ff84c09":"**I am going to use three method for variable selection in MLR and select the one with highest accuracy.**\n\n**1) Filter Method: As the name suggest, I will filter and take subset of relevant features. I have done filtering using correlation matrix with Pearson Correlation.**","f20e12ec":"**Read in data**","a3070ca3":"**I want to remove some irrelavent columns that are not important for our prediction. These columns may important\/necessary for some other type of prediction\/analysis.**","947806f0":"**We want the value of RMSE as short as possible. The depth 14 is giving lowest Root Mean Squared Error with criterian mse or friedman_mse. Thus, I am going to apply mae for deptth 14.**","a8921ab1":"### Decision Tree(CART)\n\n**As we know that variable selection and reduction is automatic in CART, let's apply the algorithm.**","da465234":"**We will remove outliers of price using IQR.**","db9ca9e1":"**price, year, and odometer are numerical varaibles.**","c754308d":"**In order to do the predict the price, first we need to understand, clean, and scale the data.**","ee263fc1":"**We are left with 118898 rows and 14 coulmns. Also, our dataset doesn't contain any missing values now.**\n\n**Handling outliers.**","d2dad0bb":"**2)Backward Elimination(Wrapper Method): This is an iterative and computationally expensive process but it is more accurate. As the name suggest, we feed all the possible features to the model at first. We check the performance of the model and then iteratively remove the worst performing features one by one till the overall performance of the model comes in acceptable range.**\n\n**The performance metric used here to evaluate feature performance is pvalue. If the pvalue is above 0.05 then we remove the feature, else we keep it.**","c2ddf659":"**Our final set of variables are shown above. It seems it has all the variables in dataset. Let's apply them into the model.**","29be86bd":"**Predictive Modeling.**","f697d84b":"**As we can see that odometer is a feature with larger magnitude. We need to reduce the scale of it to prevent from dominating the prediction model.**\n\n**In order to have fair glass to see all variables from the same lands, I have applied MinMaxScaler so prediction model will perform better.**","cc2eac35":"**Understand the data.**","0dd0762e":"**To handle rest of the missing values, we will drop all rows with missing values.**","25fb9972":"### Craigslist is the world's largest collection of used vehicles for sale. The dataset contains all information of used vehicles such as model, year, condition, cylinders, drive, size, paint color, price etc. ","abc97aab":"**12 features give optimum score. Now let's figure out these 12 features.**","354a7a5d":"**As we can see there are many columns with missing values. We need to handle those missing values and clean the data for accuarate prediction.**\n\n**Clean the data.**\n**Since here missing values are very large in numbers, we are only going to keep columns which have less than 55% of missing values.**","989293aa":"**As we can see from the heatmap that drive, odometer, cylinders, fuel, and year have relatively high postive and negative relationship with price. So we will drop the features apart from this.**\n\n**One of the assumptions of linear regression is that the independent variables need to be uncorrelated with each other. We also need to check if these variables are related with eachother.**","d92ee7e6":"**3) Recursive Feature Elimination(Wrapper Method): This method is recursively removing attributes and building a model on those attributes that remain. It uses accuracy metric to rank the feature according to their importance.**","c9104ed1":"**These are the few algorithms\/models we applied on given dataset.**","8b5a0c80":"**In order to prepare data for predictive modeling we will use Label Encoder since we have many categories for categorical variables. Label encoding is simply converting each value in a column to a number.**","08c0c6a9":"**As we know that full tree is always overfitted and this tree is also pretty long. Let\u2019s change a couple of parameters to see if there is any effect on the accuracy and also to make the tree shorter.**\n\n**Criterion: defines what function will be used to measure the quality of a split. The options are \"mse\",\"mae\", and \"friedman mse\".**\n\n**Max_depth: defines the maximum depth of the tree. If it\u2019s \u201cnone\u201d, the tree will be as long as possible, when all the leaves are pure (risk of overfitting the model).**","4791f4a9":"**Both backward elimination and recursive feature elimination are giving highest R value.**","ad1cbf20":"**Let's change the order of the column and place target column first for simplicity.**","7603be36":"**It doesn't seem that these variables have high relation with each other.**","1589540f":"**Let's check distribution of all predictors with respect to target(price) for general understanding.**","ba30d7a4":"**p values of all features.**","f229e86b":"**Price of the vehicle can never be zero. So, we will remove rows with price as 0.**","cc4a69ec":"**Following is just to show different catagories of categorical variables.**","4a588c99":"### Random Forest Regressor"}}