{"cell_type":{"5f3b4e63":"code","acbbe717":"code","dd71f2de":"code","0907ef7f":"code","cf768487":"code","8993ab82":"code","b20034ce":"code","4fb3265f":"code","2ce7efee":"code","675f4a71":"code","8e48445c":"code","c2e07c88":"code","4a5fe18c":"code","c09c4664":"code","28c38625":"code","b6e32ebe":"code","3d8d1130":"code","acdfdc94":"code","c64a1d3e":"code","88ae9e2e":"code","3e1853a6":"code","a44f3ecc":"code","e6b7c79c":"code","88d24178":"code","49fc0e99":"code","10020fc4":"code","0a8fee13":"code","550fabfa":"code","d94da829":"code","bf15c6e3":"code","b80d16be":"code","7bcdfa34":"code","ee47f83d":"code","858249de":"code","28ba3518":"code","488d733d":"code","e2906d9a":"code","a26bb617":"code","a05331aa":"code","49f4f700":"markdown","46d3c23d":"markdown","727602c3":"markdown","4629f76c":"markdown","7830403a":"markdown","f013e2af":"markdown","8191b5db":"markdown","29c3ae32":"markdown","a9825024":"markdown","d5466546":"markdown","740b75ec":"markdown","630c68f4":"markdown","2c5ab61b":"markdown","4273d5c9":"markdown","7b383ff1":"markdown","9d38bb50":"markdown","864a6b23":"markdown","37e304a2":"markdown","05951bbc":"markdown","8cc72619":"markdown","df003879":"markdown","7888a8ce":"markdown","9984c3c9":"markdown","fc042014":"markdown","9fc5895a":"markdown","efa6ba07":"markdown"},"source":{"5f3b4e63":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pylab as plt","acbbe717":"import gc\nimport time\nfrom datetime import datetime\nimport warnings\nwarnings.simplefilter(action = 'ignore')","dd71f2de":"from sklearn.metrics import roc_auc_score, log_loss, accuracy_score, confusion_matrix\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression","0907ef7f":"from scipy.stats import mannwhitneyu","cf768487":"train = pd.read_csv('..\/input\/train.csv', index_col = 'id')\ntrain.shape","8993ab82":"target = train['target']\ntrain.drop('target', axis = 1, inplace = True)\ntarget.value_counts()","b20034ce":"test = pd.read_csv('..\/input\/test.csv', index_col = 'id')\ntest.shape","4fb3265f":"index_train = train.index\nindex_test = test.index\nprint(len(index_train), len(index_test))","2ce7efee":"df_full = pd.concat([train, test], axis = 0)\n\ndel train, test\ngc.collect()","675f4a71":"df_stats = df_full.T.describe().T.drop('count', axis = 1)\ndf_stats.columns = ['source_' + c for c in df_stats.columns]\ndf_stats.head()","8e48445c":"df_stats.shape","c2e07c88":"df_stats.loc[index_train].corrwith(target)","4a5fe18c":"PARAMS = {}\nPARAMS['random_state'] = 0\nPARAMS['n_jobs'] = -1\nPARAMS['C'] = .2\nPARAMS['penalty'] = 'l1'\nPARAMS['class_weight'] = 'balanced'\nPARAMS['solver'] = 'saga'","c09c4664":"logreg_scores = pd.DataFrame(columns = ['auc', 'acc', 'loss', 'tn', 'fn', 'fp', 'tp'])\n\ndef get_logreg_score(train_, target_):\n    folds = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 20, random_state = 0)\n    predict = pd.DataFrame(index = train_.index)\n    \n    # Cross-validation cycle\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(target_, target_)):\n        train_x, train_y = train_.iloc[train_idx], target_.iloc[train_idx]\n        valid_x, valid_y = train_.iloc[valid_idx], target_.iloc[valid_idx]\n        \n        clf = LogisticRegression(**PARAMS)\n        clf.fit(train_x, train_y)\n        predict[n_fold] = pd.Series(clf.predict_proba(valid_x)[:, 1], index = valid_x.index)\n\n        del train_x, train_y, valid_x, valid_y\n        gc.collect()\n        \n    predict = predict.mean(axis = 1)\n    tn, fp, fn, tp = confusion_matrix(target_, (predict >= .5) * 1).ravel()\n    return [\n                 roc_auc_score(target_, predict), \n                 accuracy_score(target_, (predict >= .5) * 1), \n                 log_loss(target_, predict),\n                 tn, fn, fp, tp\n            ]","28c38625":"def get_submit(train_, test_, target_):\n    predict = pd.DataFrame(index = test_.index)\n    \n    clf = LogisticRegression(**PARAMS)\n    clf.fit(train_, target_)\n    \n    predict = pd.Series(clf.predict_proba(test_)[:, 1], index = test_.index).reset_index()\n    predict.columns = ['id', 'target']\n    \n    return predict","b6e32ebe":"step = 'source dataset'\nlogreg_scores = logreg_scores.T\nlogreg_scores[step] = get_logreg_score(df_full.loc[index_train], target)\nlogreg_scores = logreg_scores.T\nlogreg_scores","3d8d1130":"submit = get_submit(df_full.loc[index_train], df_full.loc[index_test], target)\n\nscore_auc = logreg_scores.loc[step, 'auc']\nscore_acc = logreg_scores.loc[step, 'acc']\nscore_loss = logreg_scores.loc[step, 'loss']\nfilename = 'subm_{}_{:.4f}_{:.4f}_{:.4f}_{}.csv'.format('source', score_auc, score_acc, score_loss,\n                                                        datetime.now().strftime('%Y-%m-%d'))\nprint(filename)\nsubmit.to_csv(filename, index = False)","acdfdc94":"dist_to_origin_sqr = (df_full**2).sum(axis = 1)\ndist_to_origin_sqr.describe()","c64a1d3e":"rad_sphere_sqr = 300\nrad_sphere = np.sqrt(rad_sphere_sqr)\nrad_sphere","88ae9e2e":"df_stats['dist_to_sphere'] = np.sqrt(dist_to_origin_sqr) - rad_sphere\ndf_stats['dist_to_sphere'].describe()","3e1853a6":"np.corrcoef(df_stats['dist_to_sphere'].loc[index_train], target)[0, 1]","a44f3ecc":"np.corrcoef(abs(df_stats['dist_to_sphere'].loc[index_train]), target)[0, 1]","e6b7c79c":"mannwhitneyu(df_stats['dist_to_sphere'].loc[index_train], df_stats['dist_to_sphere'].loc[index_test])","88d24178":"df_full_sphere = (df_full * rad_sphere).divide(np.sqrt(dist_to_origin_sqr), axis = 'rows')\n(df_full_sphere**2).sum(axis = 1).describe()","49fc0e99":"tmp = df_full_sphere.T.describe().T.drop('count', axis = 1)\ntmp.columns = ['sphere_' + c for c in tmp.columns]\ntmp.loc[index_train].corrwith(target)","10020fc4":"df_stats = pd.concat([df_stats, tmp], axis = 1)\n\ndel tmp\ngc.collect()\n\ndf_stats.head()","0a8fee13":"step = 'projection onto sphere'\nlogreg_scores = logreg_scores.T\nlogreg_scores[step] = get_logreg_score(df_full_sphere.loc[index_train], target)\nlogreg_scores = logreg_scores.T\nlogreg_scores","550fabfa":"submit = get_submit(df_full_sphere.loc[index_train], df_full_sphere.loc[index_test], target)\n\nscore_auc = logreg_scores.loc[step, 'auc']\nscore_acc = logreg_scores.loc[step, 'acc']\nscore_loss = logreg_scores.loc[step, 'loss']\nfilename = 'subm_{}_{:.4f}_{:.4f}_{:.4f}_{}.csv'.format('full_sphere', score_auc, score_acc, score_loss,\n                                                        datetime.now().strftime('%Y-%m-%d'))\nprint(filename)\nsubmit.to_csv(filename, index = False)","d94da829":"df_signes = np.sign(df_full_sphere).astype(int)\ndf_signes.head()","bf15c6e3":"df_signes.replace(-1, 2).astype(str).apply(lambda x: ''.join(x), axis = 1).nunique()","b80d16be":"2**300","7bcdfa34":"df_stats['positive_cnt'] = (df_signes > 0).sum(axis = 1)\ndf_stats['positive_cnt'].describe()","ee47f83d":"np.corrcoef(df_stats['positive_cnt'].loc[index_train], target)[0, 1]","858249de":"mannwhitneyu(df_stats['positive_cnt'].loc[index_train], df_stats['positive_cnt'].loc[index_test])","28ba3518":"step = 'quadrants'\nlogreg_scores = logreg_scores.T\nlogreg_scores[step] = get_logreg_score(df_signes.loc[index_train], target)\nlogreg_scores = logreg_scores.T\nlogreg_scores","488d733d":"submit = get_submit(df_signes.loc[index_train], df_signes.loc[index_test], target)\n\nscore_auc = logreg_scores.loc[step, 'auc']\nscore_acc = logreg_scores.loc[step, 'acc']\nscore_loss = logreg_scores.loc[step, 'loss']\nfilename = 'subm_{}_{:.4f}_{:.4f}_{:.4f}_{}.csv'.format('quad', score_auc, score_acc, score_loss,\n                                                        datetime.now().strftime('%Y-%m-%d'))\nprint(filename)\nsubmit.to_csv(filename, index = False)","e2906d9a":"df_stats['angle_w_bis'] = np.arccos(abs(df_full_sphere).sum(axis = 1) \/ rad_sphere_sqr)\ndf_stats['angle_w_bis'].describe()","a26bb617":"np.corrcoef(df_stats['angle_w_bis'].loc[index_train], target)[0, 1]","a05331aa":"mannwhitneyu(df_stats['angle_w_bis'].loc[index_train], df_stats['angle_w_bis'].loc[index_test])","49f4f700":"Hmm... It looks like another spheres with the same radius which centered at the intersection of \"bisectors\" with the source sphere. Each ponts is located near the intersection of such sphere with the source one.","46d3c23d":"If we are looking for second-order logic, let's first check the distance from the points to the origin.","727602c3":"It looks like the distanse to the sphere has no useful information.\n\nAnd it has no difference between train and test sets.","4629f76c":"## Let's start...","7830403a":"And we have only 20000 points. Can we accidentally get such a distribution of points? Yes. Is it an accident here? I hope no. This is a synthetic dataset.\n\nLet's explore the distribution of the quadrants with points.","f013e2af":"## Let's prepare everything we need","8191b5db":"...and check the score change.","29c3ae32":"Data set for research with some basic source statistics:","a9825024":"Now let's project the points onto the sphere...","d5466546":"First of all, let's calculate the score for the source data.","740b75ec":"__LB = 0.845__","630c68f4":"The magnitude of the angle is not important. \n\nThe distribution of angles on the train and test sets differs only slightly higher than for previous statistics. But it can still be considered the same.","2c5ab61b":"Wow! It looks like a sphere centered at the origin with a radius of sqrt(300)! All ponts are located near it.\n\nHmm... The square of the radius is equal to the dimension of space... What does this mean? For example, for such a sphere, the coordinates of the \"bisectors\" of quadrants are 1 or -1. Or may be initial coordinates were 1 and -1, and then some kind of transformation was applied. For synthetic set this is well likely assumption.\n\nNow let's try to project the points onto the sphere and analyze the distance to it. Does it have any useful information?","4273d5c9":"__LB = 0.845__","7b383ff1":"## Distribution of points in 300D space","9d38bb50":"Functions and table for comparing scores of logistic regression and make a submittion:","864a6b23":"Surprize! There are 20000 unique combinations of coordinate signs. In each quadrant is no more than one point! \n\nBut...\n\nThere are 2**300 quadrants in the 300D space. It's a very big number:","37e304a2":"It will be more convenient to use the combined data set:","05951bbc":"The distribution of quadrants contains meaningful information for prediction, but not all of its. The distribution of points within quandrants is important too.\n\nLet's calculate, for example, angles between vector of point and vector of \"bisector\".","8cc72619":"# Don't Overfit! II","df003879":"On average, for each point half the coordinates are positive. Not less 109 and not more 185. It means, there are no points with almost all positive or all negative coordinates.\n\nIs it useful? Let's check.","7888a8ce":"__LB = 0.748__","9984c3c9":"## To be continued...\n\nI hope :)","fc042014":"I was interested in ellipses on this kernel https:\/\/www.kaggle.com\/cyones77\/t-sne-projection. \u0410nd I asked myself - is there some kind of second-order logic in the data, if I present a data set as the coordinates of points in 300D space?\n\n__Spoiler - there is!__\n\nLet's start the research...","9fc5895a":"It doesn't change significantly. We removed the some kind of noise. All points are really located on this sphere.\n\nLet's explore this set.\n\nFirst, let's take a closer look at the location of the points relative to the \"bisectors\" of the quadrants. For this let's define the average density of points in each quadrant.","efa6ba07":"This is rather small correlation for using count of positive coordinates for prediction. But not the smallest of the values found :)\n\nWhat about the quadrants themselves? Let's replace coordinates of points to coordinates of \"bisectors\" of quadrants and look at the prediction."}}