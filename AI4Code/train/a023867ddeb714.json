{"cell_type":{"1966d657":"code","6aaaa135":"code","f9544307":"code","1bf9a84e":"code","c5e88947":"code","374c81a1":"code","75d834ce":"code","673df570":"code","f859e2c6":"code","8e3db8ca":"code","df5b4554":"code","b4d23c41":"code","716c9eea":"code","856440d2":"code","cfc99e70":"markdown"},"source":{"1966d657":"import pandas as pd\nimport numpy as np\nimport io\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.svm import LinearSVC","6aaaa135":"train_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\npure_df = pd.read_csv('..\/input\/november21\/train.csv')\ntest_df['chunk'] = test_df.id \/\/ 60000","f9544307":"def postprocess_separate(submission_df, test_df=None, pure_df=None):\n    \"\"\"Update submission_df so that the predictions for the two sides of the hyperplane don't overlap.\n    \n    Parameters\n    ----------\n    submission_df : pandas DataFrame with columns 'id' and 'target'\n    test_df : the competition's test data\n    pure_df : the competition's original training data\n    \n    From https:\/\/www.kaggle.com\/ambrosm\/tpsnov21-007-postprocessing\n    \"\"\"\n    if pure_df is None: pure_df = pd.read_csv('..\/input\/november21\/train.csv')\n    if pure_df.shape != (600000, 102): raise ValueError(\"pure_df has the wrong shape\")\n    if test_df is None: test_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\n    if test_df.shape[0] != submission_df.shape[0] or test_df.shape[1] != 101: raise ValueError(\"test_df has the wrong shape\")\n\n    # Find the separating hyperplane for pure_df, step 1\n    # Use an SVM with almost no regularization\n    model1 = make_pipeline(StandardScaler(), LinearSVC(C=1e5, tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=1))\n    model1.fit(pure_df.drop(columns=['id', 'target']), pure_df.target)\n    pure_pred = model1.predict(pure_df.drop(columns=['id', 'target']))\n    #print((pure_pred != pure_df.target).sum(), (pure_pred == pure_df.target).sum()) # 1 599999\n    # model1 is not perfect: it predicts the wrong class for 1 of 600000 samples\n\n    # Find the separating hyperplane for pure_df, step 2\n    # Fit a second SVM to a subset of the points which contains the support vectors\n    pure_pred = model1.decision_function(pure_df.drop(columns=['id', 'target']))\n    subset_df = pure_df[(pure_pred > -5) & (pure_pred < 0.9)]\n    model2 = make_pipeline(StandardScaler(), LinearSVC(C=1e5, tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=1))\n    model2.fit(subset_df.drop(columns=['id', 'target']), subset_df.target)\n    pure_pred = model2.predict(pure_df.drop(columns=['id', 'target']))\n    #print((pure_pred != pure_df.target).sum(), (pure_pred == pure_df.target).sum()) # 0 600000\n    # model2 is perfect: it predicts the correct class for all 600000 training samples\n    \n    pure_test_pred = model2.predict(test_df.drop(columns=['id', 'target'], errors='ignore'))\n    lmax, rmin = submission_df[pure_test_pred == 0].target.max(), submission_df[pure_test_pred == 1].target.min()\n    if lmax < rmin:\n        print(\"There is no overlap. No postprocessing needed.\")\n        return\n    # There is overlap. Remove this overlap\n    submission_df.loc[pure_test_pred == 0, 'target'] -= lmax + 1\n    submission_df.loc[pure_test_pred == 1, 'target'] -= rmin - 1\n    print(submission_df[pure_test_pred == 0].target.min(), submission_df[pure_test_pred == 0].target.max(),\n          submission_df[pure_test_pred == 1].target.min(), submission_df[pure_test_pred == 1].target.max())\n","1bf9a84e":"# name = name of chunk as in https:\/\/www.kaggle.com\/ambrosm\/tpsnov21-012-leaderboard-probing\n# len = number of samples in this chunk (len.sum() == 540000)\n# auc = public leaderboard score of this chunk\n# diff = difference of this chunk's auc score minus the baseline of 0.74723 = area of the added triangle\n# ratio = unused\n\nprobes_l = '''name\tlen\tauc\tdiff\tratio\n10H0\t30343\t74653\t-70\t -0.00231 \n17H0\t36335\t74671\t-52\t -0.00143 \n16H0\t41892\t74720\t-3\t -0.00007 \n13H0\t36383\t74724\t1\t 0.00003 \n18H0\t23746\t74729\t6\t 0.00025 \n11H0\t20501\t74732\t9\t 0.00044 \n14H0\t25270\t74747\t24\t 0.00095 \n12H0\t40308\t74763\t40\t 0.00099 \n15H0\t25965\t74762\t39\t 0.00150 \n'''\n\nprobes_r = '''\nname\tlen\tauc\tdiff\tratio\n10H1\t29657\t74695\t-28\t -0.00094 \n11H1\t39499\t74760\t37\t 0.00094 \n12H1\t19692\t74750\t27\t 0.00137 \n13H1\t23617\t74690\t-33\t -0.00140 \n14H1\t34730\t74735\t12\t 0.00035 \n15H1\t34035\t74815\t92\t 0.00270 \n16H1\t18108\t74682\t-41\t -0.00226 \n17H1\t23665\t74694\t-29\t -0.00123 \n18H1\t36254\t74683\t-40\t -0.00110 \n'''\n\nprobes_l_df = pd.read_csv(io.StringIO(probes_l), sep='\\t')\nprobes_r_df = pd.read_csv(io.StringIO(probes_r), sep='\\t')\n","c5e88947":"# Left side\nl_dict = {}\nplt.figure(figsize=(10,10))\nfor row in probes_l_df.itertuples():\n    #print(row)\n    y0 = row.diff \/ 100000 * 8\n    plt.plot([0, 0.25], [y0, y0+0.75], color='r') # parallel for all points with this auc difference\n    plt.plot([0, row.len\/270000], [row.len\/270000, 0], color='g') # all points for this row.len\n    x = (row.len\/270000 - y0) \/ 4\n    y = 3 * x + y0\n    plt.scatter([x], [y], color='k')\n    #print(f\"{row.name} {y\/x:.5f} {x\/(x+y):.5f}\")\n    l_dict[int(row.name[:2])] = x\/(x+y)\n    print(f\"{row.name[:2]}: {x\/(x+y):.5f},\")\nplt.plot([0, 0.25, 1], [0, 0.75, 1], color=\"y\", lw=1) # baseline roc curve (two segments)\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\") # diagonal\nplt.gca().set_aspect('equal')\nif False:\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\nelse:\n    plt.xlim([0.0, 0.2])\n    plt.ylim([0.0, 0.2])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver operating characteristic\")\nplt.legend(loc=\"lower right\")\nplt.show()\nl_dict","374c81a1":"# Right side\nr_dict = {}\nplt.figure(figsize=(10,10))\nfor row in probes_r_df.itertuples():\n    y0 = row.diff \/ 100000 * 8\n    plt.plot([0.25-y0, 1-y0], [0.75, 1], color='r') # parallel for all points with this auc difference\n    plt.plot([1-row.len\/270000, 1], [1, 1-row.len\/270000], color='g') # all points for this row.len\n    #x = (row.len\/270000 - y0) \/ 4\n    #y = 3 * x + y0\n    x = (4 - 3*row.len\/270000 - y0) \/ 4\n    y = 2 - row.len\/270000 - x\n    plt.scatter([x], [y], color='k')\n    #print(f\"{row.name} {y\/x:.5f} {x\/(x+y):.5f}\")\n    r_dict[int(row.name[:2])] = (1-x)\/(row.len\/270000)\n    print(f\"{row.name[:2]}: {r_dict[int(row.name[:2])]:.5f},\")\nplt.plot([0, 0.25, 1], [0, 0.75, 1], color=\"y\", lw=1) # baseline roc curve (two segments)\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\") # diagonal\nplt.gca().set_aspect('equal')\nif False:\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\nelse:\n    plt.xlim([0.8, 1])\n    plt.ylim([0.8, 1])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver operating characteristic\")\nplt.legend(loc=\"lower right\")\nplt.show()\nr_dict","75d834ce":"baseline = pd.DataFrame({'id': test_df.id, 'target': 0})\npostprocess_separate(baseline, test_df=test_df.drop(columns='chunk'), pure_df=pure_df)\n","673df570":"# 18 probabilities for the 18 half-chunks -> lb 0.75110\nsub = baseline.copy()\nfor chunk in range(10, 19):\n    sub.loc[(test_df.chunk == chunk) & (baseline.target < 0), 'target'] = l_dict[chunk]\n    sub.loc[(test_df.chunk == chunk) & (baseline.target >= 0), 'target'] = r_dict[chunk]\nsub.to_csv(f'submission_probed.csv', index=False)\nsub.head(20)","f859e2c6":"# 8 % of @jmcslk's submission (which has lb 0.74996) -> lb 0.75209\njmcslk_submission = pd.read_csv('..\/input\/tps-nov-2021-simple-single-nn-3\/submission.csv')\npostprocess_separate(jmcslk_submission, test_df=test_df.drop(columns='chunk'), pure_df=pure_df)\nsub_8b = sub.copy()\nsub_8b['target'] += jmcslk_submission.target \/ 11.5\nsub_8b.to_csv(f'submission_probed_blended_8b.csv', index=False)\nsub_8b.head(20)","8e3db8ca":"# 8 % of @sfktrkl's submission (which has lb 0.75002) -> lb 0.75204\nsfktrkl_submission = pd.read_csv('..\/input\/tps-nov-2021-power-averaging\/submission.csv')\npostprocess_separate(sfktrkl_submission, test_df=test_df.drop(columns='chunk'), pure_df=pure_df)\nsub_8c = sub.copy()\nsub_8c['target'] += sfktrkl_submission.target \/ 11.5\nsub_8c.to_csv(f'submission_probed_blended_8c.csv', index=False)\nsub_8c.head(20)\n","df5b4554":"# 6 % of @jmcslk's submission (which has lb 0.74996) and\n# 2 % of @sfktrkl's submission (which has lb 0.75002) -> lb 0.75208\nsub_8d = sub.copy()\nsub_8d['target'] *= 92 \/ 100\nsub_8d['target'] += jmcslk_submission.target * 6 \/ 100 + sfktrkl_submission.target * 2 \/ 100\nsub_8d.to_csv(f'submission_probed_blended_8d.csv', index=False)\nsub_8d.head(20)\n","b4d23c41":"# 7 % of @jmcslk's submission (which has lb 0.74996) and\n# 1 % of @sfktrkl's submission (which has lb 0.75002) -> lb 0.75209\nsub_8e = sub.copy()\nsub_8e['target'] *= 92 \/ 100\nsub_8e['target'] += jmcslk_submission.target * 7 \/ 100 + sfktrkl_submission.target * 1 \/ 100\nsub_8e.to_csv(f'submission_probed_blended_8e.csv', index=False)\nsub_8e.head(20)\n","716c9eea":"# 8 % of @jmcslk's submission (which has lb 0.74996) and\n# 1 % of @sfktrkl's submission (which has lb 0.75002) -> lb 0.75206\nsub_8_1 = sub.copy()\nsub_8_1['target'] *= 91 \/ 100\nsub_8_1['target'] += jmcslk_submission.target * 8 \/ 100 + sfktrkl_submission.target * 1 \/ 100\nsub_8_1.to_csv(f'submission_probed_blended_8_1.csv', index=False)\nsub_8_1.head(20)\n","856440d2":"# 7 % of @jmcslk's submission (which has lb 0.74996)\nsub_7 = sub.copy()\nsub_7['target'] *= 93 \/ 100\nsub_7['target'] += jmcslk_submission.target * 7 \/ 100\nsub_7.to_csv(f'submission_probed_blended_7.csv', index=False)\nsub_7.head(20)\n","cfc99e70":"# Leaderboard Probing\n\nThis notebook is directly or indirectly based on work by @jmcslk, @criskiev, @grayjay, @javiervallejos, @adityasharma01, @sfktrkl, @motloch, @chaudharypriyanshu and others.\n\nWe first model the target distribution as a partition of 18 chunks, where each chunk has a fixed probability determined by [leaderboard probing](https:\/\/www.kaggle.com\/ambrosm\/tpsnov21-012-leaderboard-probing) and then blend the resulting model with the output of another [postprocessed](https:\/\/www.kaggle.com\/ambrosm\/tpsnov21-007-postprocessing) high-scoring public notebook using the weights 93 : 7.\n"}}