{"cell_type":{"084f86ec":"code","e543da95":"code","3c989023":"code","577f7f40":"code","40774ba7":"code","407388f8":"code","61f0a405":"code","127c6cb4":"code","8f5b8e1c":"code","7cd493b4":"code","911fce3d":"code","a9a87a2f":"code","936faaa5":"code","b3f8b282":"code","065a8261":"code","68e737c8":"code","e194b285":"code","29e91ef8":"code","38ef9e35":"code","44e067fe":"code","b25aa0fa":"code","ccf86206":"code","20d32d3c":"code","835f8a4a":"code","ac8aef7c":"code","3b3f8ab0":"code","1e998ae6":"code","c08e546d":"code","7aaa3ef5":"code","e9f8671c":"markdown","dc734c5d":"markdown","6c06439b":"markdown","2113043f":"markdown","a18f8504":"markdown","ed4eb0d6":"markdown","76c3cdf9":"markdown","6cb2029a":"markdown"},"source":{"084f86ec":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","e543da95":"colors = ['#001c57','#40948f','#a6a6a6','#99d1df']\nsns.palplot(sns.color_palette(colors))","3c989023":"train = pd.read_csv('\/kaggle\/input\/mercedesbenz-greener-manufacturing\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/mercedesbenz-greener-manufacturing\/test.csv')","577f7f40":"plt.figure(figsize=(16,6))\nplt.subplot(121)\nsns.distplot(train.y.values, bins=50, color=colors[1])\nplt.title('Target Value Distribution - y\\n',fontsize=15)\nplt.xlabel('Value in Seconds'); plt.ylabel('Frequecy');\n\nplt.subplot(122)\nsns.boxplot(train.y.values, color=colors[3])\nplt.title('Target Value Distribution - y\\n',fontsize=15)\nplt.xlabel('Value in Seconds')","40774ba7":"train['y'].describe()","407388f8":"train.dtypes.value_counts()","61f0a405":"train.dtypes[train.dtypes=='float']","127c6cb4":"dtype_df = train.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","8f5b8e1c":"train.dtypes[train.dtypes=='object']","7cd493b4":"obj_dtype = train.dtypes[train.dtypes=='object'].index\nfor i in obj_dtype:\n    print(i, train[i].unique())","911fce3d":"train.isna().sum()[train.isna().sum()>0]","a9a87a2f":"fig,ax = plt.subplots(len(obj_dtype), figsize=(18,80))\n\nfor i, col in enumerate(obj_dtype):\n    sns.boxplot(x=col, y='y', data=train, ax=ax[i])","936faaa5":"num = train.dtypes[train.dtypes=='int'].index[1:]","b3f8b282":"nan_num = []\nfor i in num:\n    if (train[i].var()==0):\n        print(i, train[i].var())\n        nan_num.append(i)","065a8261":"train = train.drop(columns=nan_num, axis=1)","68e737c8":"train.shape","e194b285":"for i in obj_dtype:\n    le = LabelEncoder()\n    le.fit(list(train[i].values) + list(train[i].values))\n    train[i] = le.transform(list(train[i].values))","29e91ef8":"train[obj_dtype].head()","38ef9e35":"corr = train[train.columns[1:10]].corr()\n\nfig,ax = plt.subplots(figsize=(12,10))\nsns.heatmap(corr, vmax=.7, square=True,annot=True);","44e067fe":"threshold = 1\n\ncorr_all = train.drop(columns=obj_dtype, axis=1).corr()\ncorr_all.loc[:,:] =  np.tril(corr_all, k=-1) ","b25aa0fa":"train.shape","ccf86206":"already_in = set()\nresult = []\nfor col in corr_all:\n    perfect_corr = corr_all[col][corr_all[col] == threshold ].index.tolist()\n    if perfect_corr and col not in already_in:\n        already_in.update(set(perfect_corr))\n        perfect_corr.append(col)\n        result.append(perfect_corr)","20d32d3c":"result","835f8a4a":"train.T.drop_duplicates().T","ac8aef7c":"# Let me run an ensable model Random Forest\n\nfrom sklearn.model_selection import train_test_split\n\nx = train.drop('y',axis=1)\nx = train.drop('ID',axis=1)\ny = train['y']\nx_train,x_test, y_train, y_test = train_test_split(x, y, test_size=.2,random_state=10) \n\n\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators=200, max_depth=200, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=10)\nmodel.fit(x_train, y_train)\n\nprint(\"Traiing Score:- \",model.score(x_train,y_train)*100)\nprint(\"Testing Score:- \",model.score(x_test,y_test)*100)","3b3f8ab0":"# Let me run an ensable model Gradient Boosting Regressor \n\nfrom sklearn.model_selection import train_test_split\n\nx = train.drop('y',axis=1)\nx = train.drop('ID',axis=1)\ny = train['y']\nx_train,x_test, y_train, y_test = train_test_split(x, y, test_size=.2,random_state=10) \n\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nmodel = GradientBoostingRegressor()\n#model = ensemble.RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\nmodel.fit(x_train, y_train)\n\nprint(\"Traiing Score:- \",model.score(x_train,y_train)*100)\nprint(\"Testing Score:- \",model.score(x_test,y_test)*100)","1e998ae6":"predicted = model.predict(x_test)","c08e546d":"predicted","7aaa3ef5":"plt.figure(figsize=(15,5))\nplt.subplot(121)\nsns.distplot(predicted, bins=50, color=colors[1])\nplt.title('Target Value Distribution - y\\n',fontsize=15)\nplt.xlabel('Value in Seconds'); plt.ylabel('Frequecy');\n\nplt.subplot(122)\nsns.boxplot(predicted, color=colors[3])\nplt.title('Target Value Distribution - y\\n',fontsize=15)\nplt.xlabel('Value in Seconds');","e9f8671c":"When analyzing numerical variables, we found that some of them have a direct correlation with others, therefore, in order to avoid multicollinearity, we can remove the variables with correlation 1 (leave one of the group), or use regularization so that the algorithm does it in automatic mode.\nHow else can we remove such variables without correlation? It's simple, we delete duplicates in the column section.\n\n","dc734c5d":"We have a set of numeric variables, where the value is set to 1 or 0, so there is no need to carry out volumetric analysis. In this case, we should be interested in whether the value of indicators changes within the variables, for this we examine the variance of these variables, use the var () function, and select only those where the variance is zero (that is, always 0, or 1 on the entire dataset in variable cut)","6c06439b":"The objective of the competition is to predict the time it will take to complete the testing phase. The dataset represents various permutations of the characteristics of Mercedes-Benz vehicles. Reducing the algorithm run time can also help reduce carbon dioxide emissions without compromising Daimler's standards.\n\nThe dataset contains an anonymized set of variables (user-defined functions) in a Mercedes vehicle. For example, a variable could be 4WD, it could be an added air suspension, or a head display.\n\ny is the variable to be predicted, this is the time (in seconds) it took for the car to be tested for each variable\n\nVariables containing letters are categorical. Variables with 0\/1 are of binary type.\n\n","2113043f":"We received several such variables, we can remove them from the analysis, since they will not affect the target in any way, thereby increasing the performance of the algorithm.","a18f8504":"Covert the object data using label encoder","ed4eb0d6":"Inference from the graphs:\n\n1) Since there is a need to reduce the testing time, the best values in the variables at which this time is minimal are az and bc (X0), y (X1), n (X2), x and h (X5) (hypothesis: on y?)\n\n2) Variables X3, X5, X6, X8 have similar distributions of values, where there are no special differences within the feature between values in the context of means and quartiles\n\n3) X0 and X2 have the greatest variety within variables, which can potentially indicate a greater usefulness of these features\n","76c3cdf9":"Among the categorical variables, we did not find a direct relationship with the target y","6cb2029a":"The target variable has a standard distribution of about 72 to 140 seconds. The first and third quartiles lie in the range from about 91 to 109 seconds, the median is 100 seconds, we also note that there are outliers starting from 140 seconds that we can remove from the training sample, since these values \u200b\u200bwill add noise to our algorithm.\n"}}