{"cell_type":{"d578cf73":"code","a3ea3f2d":"code","b118e4e5":"code","83b08e8d":"code","8d210bfa":"code","77518985":"code","d429f155":"code","c847509c":"markdown","ece5dc84":"markdown","31defb3f":"markdown","81f85ff0":"markdown","e2fed453":"markdown","f8183d03":"markdown","b8f14ac9":"markdown","42250999":"markdown"},"source":{"d578cf73":"import numpy as np \nimport pandas as pd \n\nimport xgboost as xgb  # import the XGBoost package\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score","a3ea3f2d":"d_set = datasets.load_iris()\nX = d_set.data\ny = d_set.target","b118e4e5":"X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)","83b08e8d":"D_train = xgb.DMatrix(X_train, label=Y_train)\nD_test = xgb.DMatrix(X_test, label=Y_test)","8d210bfa":"param = {\n    'eta': 0.3,  # Step size shrinkage\n    'max_depth': 3,  # Maximum depth of a tree\n    'objective': 'multi:softprob',  # specifying the learning objective\n    # multi:softprob : softmax for multiclass classification with a output \n    # vector of 'ndata * nclass'\n    'num_class': 3,  # number of classed\n    'eval_metric': 'merror',  # evaluation metric\n}\n\nsteps = 20  # The number of training iterations","77518985":"model = xgb.train(param, D_train, steps)\n\npreds = model.predict(D_test)\nbest_preds = np.asarray([np.argmax(line) for line in preds])\n\nprint(\"Precision = {}\".format(precision_score(Y_test, best_preds, average='macro')))\nprint(\"Recall = {}\".format(recall_score(Y_test, best_preds, average='macro')))\nprint(\"Accuracy = {}\".format(accuracy_score(Y_test, best_preds)))","d429f155":"kfolds = KFold(n_splits=5)\n\nevals_result = dict()\nbest_iterations = []\nerrors = []\n\nfor train_id, test_id in kfolds.split(X):\n    D_train = xgb.DMatrix(X[train_id], label=y[train_id])\n    D_test = xgb.DMatrix(X[test_id], label=y[test_id])\n    \n    model = xgb.train(param, D_train, steps, evals=[(D_train, 'train'), (D_test, 'valid')],\n                            evals_result=evals_result, verbose_eval=False)\n    \n    best_iteration = np.argmin(evals_result['valid']['merror'])\n    best_iterations.append(best_iteration)\n    er = evals_result['valid']['merror'][best_iteration]\n    errors.append(er)\n    \n    preds = model.predict(D_test)\n    best_preds = np.asarray([np.argmax(line) for line in preds])\n\nprint('mean oof merror = {}'.format(np.mean(errors)))","c847509c":"The parameters of the gradient boosting algorithm needed to be defined. All the parameters are available at the official [website](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html). ","ece5dc84":"Now its the training time. We train the model with the defined parameters and perform a prediction task as well.","31defb3f":"# Getting the data","81f85ff0":"# Training and Testing","e2fed453":"# What is XGBoost?\nThe eXtreme Gradient Boosting (XGBoost) algorithm is a scalable and optimized variant of the gradient boosting algorithm that is optimized for efficiency, computational speed, and model output. It is an open-source library that belongs to the Distributed Machine Learning Community. Boosting is a sequential ensemble learning method used in machine learning to turn a weak hypothesis or weak learners into strong learners in order to improve model performance.\n\nI am trying to learn and taking help from the following posts:\n* [XGBoost: A Deep Dive into Boosting](https:\/\/medium.com\/sfu-cspmp\/xgboost-a-deep-dive-into-boosting-f06c9c41349)","f8183d03":"In order for XGBoost to be able to use our data, we need to transform it into [DMatrix](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.core) format. DMatrix is an internal data structure that is used by XGBoost, which is optimized for both memory efficiency and training speed.","b8f14ac9":"# k-fold Cross validation\nData is shuffled and split into k equal-sized subsamples in k-fold cross-validation. One of the k subsamples is used as a test\/validation set, while the remaining (k -1) subsamples are combined to form training data. Then, using training data, we fit a model and evaluate it using the test set. This procedure is repeated k times to ensure that each data point only appears in the validation set once. To obtain the final estimate, the k results from each model should be summed. The benefit of this approach is that it greatly reduces bias and variance while also increasing the model's robustness.","42250999":"# How boosting works?\nBefore getting into the mechanism of boosting, we need to know about the Ensemble model or ensemble learning. It is a process where multiple ML models are combined to reduce errors and improve performance. Finally, the maximum voting technique is used to deduce the final prediction on the aggregated decisions. The idea can be visualized in the following figure taken from [here](https:\/\/miro.medium.com\/max\/2400\/1*q-ZQz1EZeFCPr5ijx9nNdQ.png).\n\n![Ensemble learning](https:\/\/miro.medium.com\/max\/2400\/1*q-ZQz1EZeFCPr5ijx9nNdQ.png)\n\nThe boosting algorithm generates new models and sequentially integrates their predictions to increase the model's overall performance. Larger weights are assigned to misclassified samples for an incorrect prediction. On the other hand, lower weights are assigned to the samples for a correct prediction. Weak learner models with higher performance have more weight in the final ensemble process. Boosting never changes the previous predictor and only corrects the next predictor by learning from mistakes.\n\n# Gradient Boosting\nGradient boosting is a variant of the boosting algorithm in which errors are minimized using a gradient descent algorithm and produce a model in the form of weak prediction models e.g. decision trees. Gradient boosting adjusts weights by the use of gradient using Gradient Descent algorithm, which iteratively optimizes the loss of the model by updating weights.\n\nGradient boosting employs Additive Modeling, in which a new decision tree is applied to a model that minimizes the loss using gradient descent one at a time. Existing trees in the model are left alone, slowing the pace of overfitting. \n\n**Let's have a look at the features of XGBoost from the following figure.**\n\n![Features of XGBoost](https:\/\/miro.medium.com\/max\/5248\/1*1kjLMDQMufaQoS-nNJfg1Q.png)\n\nNow let's get into action!!!"}}