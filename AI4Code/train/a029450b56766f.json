{"cell_type":{"ba84c8d8":"code","528a352d":"code","662d827e":"code","f957f3cd":"code","554772e2":"code","b897ed36":"code","d84ae591":"code","be6f4053":"code","c09b1e3d":"code","de0a0efe":"code","d68fba5f":"code","a9793466":"markdown","5857c988":"markdown","9a937109":"markdown","1a758eb8":"markdown","73764981":"markdown","a8b24199":"markdown","4d9afbf1":"markdown","b0a90d8e":"markdown","96cad1f3":"markdown","f7ac7bb6":"markdown"},"source":{"ba84c8d8":"import pandas as pd\nimport numpy as np\nimport random\nimport time\nimport os\nfrom os import path\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\n\nfrom tqdm import tqdm\n\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom lightgbm import LGBMClassifier, LGBMRegressor\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, QuantileTransformer\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.ensemble import StackingRegressor, StackingClassifier\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nfrom optuna.visualization import plot_contour\nfrom optuna.visualization import plot_edf\nfrom optuna.visualization import plot_intermediate_values\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_parallel_coordinate\nfrom optuna.visualization import plot_param_importances\nfrom optuna.visualization import plot_slice\n\nimport gc\n\ndef seed_everything(seed=2021):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything()","528a352d":"STUDY_PATH = r\"..\/input\/tps-10-21-catboost-optuna-baseline\/study.db\"\n\nif path.exists(STUDY_PATH):\n    print(\"Found existing study\")\n    !cp \"..\/input\/tps-10-21-catboost-optuna-baseline\/study.db\" \"study.db\"\n    !chmod +rwx \"study.db\"\n    \nstudy = optuna.create_study(direction='maximize', sampler=TPESampler(), study_name='CatClassifier', storage=r\"sqlite:\/\/\/study.db\", load_if_exists=True)","662d827e":"res = study.trials_dataframe(attrs=('number', 'value', 'params'))\ndisplay(res.info())\nres.tail(10)","f957f3cd":"DEPTH = \"depth\"\nL2_LEAF_REG = \"l2_leaf_reg\"\nLEARNING_RATE = \"learning_rate\"\nMIN_DATA_IN_LEAF = \"min_data_in_leaf\"\nVALUE = \"value\"\nLEAF_EST = \"leaf_estimation_iterations\"\nRAND = \"random_strength\"\nSUB = \"subsample\"","554772e2":"plot_optimization_history(study)","b897ed36":"plot_param_importances(study)","d84ae591":"fig = plot_contour(study, params=[DEPTH, RAND, SUB, L2_LEAF_REG, LEAF_EST])\nfig.update_layout({\"width\":1200, \"height\":1200})\ndef update_colorscale(trace):\n    try:\n        trace[\"colorscale\"]= \"Tealrose\"\n        trace[\"reversescale\"]= False\n    except:\n        pass\nfig.for_each_trace(update_colorscale)","be6f4053":"N_trials = 60\n\nN_last_ls = res.iloc[-N_trials:,:].index.to_list()\nlast_trials = [study.trials[idx] for idx in N_last_ls]\nstudy_N_last = optuna.create_study(direction='maximize', sampler=TPESampler(), study_name='N_last', storage=r\"sqlite:\/\/\/study2.db\", load_if_exists=True)\nstudy_N_last.add_trials(last_trials)","c09b1e3d":"plot_optimization_history(study_N_last)","de0a0efe":"plot_param_importances(study_N_last)","d68fba5f":"fig = plot_contour(study_N_last, params=[DEPTH, LEARNING_RATE, SUB, L2_LEAF_REG, RAND])\nfig.update_layout({\"width\":1200, \"height\":1200})\ndef update_colorscale(trace):\n    try:\n        trace[\"colorscale\"]= \"Tealrose\"\n    except:\n        pass\nfig.for_each_trace(update_colorscale)","a9793466":"   \n<h1 id=\"contour\" style=\"font-family:verdana;\"> \nConclusion\n<\/h1>\n\n<div style=\"font-size:15px; font-family:verdana;\">\n    <p> This last graph gives me some intuition on how to continue optmizing : <\/p>\n        <ul>\n            <li> the \"right\"  tree-depth for this problem might be 3 !! so I will freeze this parameter <\/li>\n            <li> best results are obtained with small learning rate, I should allow this parameter to go further down <\/li>\n            <li> try to focus the subsample parameter on smaller value<\/li>\n    <\/ul>\n    <code> param_grid = {\n        'iterations': trial.suggest_categorical('iterations',[N_ESTIMATORS]),\n        'learning_rate' : trial.suggest_uniform('learning_rate', 5e-4, 5e-3),\n        'depth': 3,\n        'l2_leaf_reg': trial.suggest_uniform('l2_leaf_reg', 1e-5,100),\n        'subsample': trial.suggest_uniform('subsample',0,0.5),\n        'random_strength' : trial.suggest_uniform('random_strength', 1, 50),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15)\n        }<\/code> \n     \n<\/div>","5857c988":"<h1 id=\"contour\" style=\"font-family:verdana;\"> \nContour Plot\n<\/h1>\n\n<div style=\"font-size:15px; font-family:verdana;\">\n    <p>\n        <ul>\n            <li> Plotting the 5 most important parameters <\/li>\n    <\/p>\n<\/div>","9a937109":"\n<h1 id=\"importance_1\" style=\"font-family:verdana;\"> \nHyperparameters importance\n<\/h1>","1a758eb8":"<h1 id=\"select\" style=\"font-family:verdana;\"> \nSelecting only the N last trials\n<\/h1>\n\n<div style=\"font-size:15px; font-family:verdana;\">\n    <p>\n        <ul>\n            <li> First trials had quite low objective value, plots are diffult to analyze <\/li>\n            <li> Starting again using only the n-last trials <\/li>\n    <\/p>\n<\/div>","73764981":"<h1 id=\"basics\" style=\"font-family:verdana;\"> \nList all the trials\n<\/h1>\n\n<div style=\"font-size:15px; font-family:verdana;\">\n    <p>\n        <ul>\n            <li> List of all trials (set of parameter) that have been tested and the corresponding objective value <\/li>\n    <\/p>\n<\/div>","a8b24199":"<div style=\"font-size:15px; font-family:verdana;\">","4d9afbf1":"<h1 id=\"history_1\" style=\"font-family:verdana;\"> \nHistory of optimization\n<\/h1>","b0a90d8e":"<h1 id=\"basics\" style=\"font-family:verdana;\"> \nImporting the study (study.db)\n<\/h1>\n\n<div style=\"font-size:15px; font-family:verdana;\">\n    <p>\n        <ul>\n            <li> Moving the .db file from the input directory (which is read-only) to the output to make it works <\/li>\n            <li> The same trick can be used to continue optimizing a study using the same notebook and get over the 9-hour notebook execution time limit <\/li>\n    <\/p>\n<\/div>","96cad1f3":"<h1 id=\"basics\" style=\"font-family:verdana;\"> \nVisualizing optuna results\n<\/h1>\n\n<div style=\"font-size:15px; font-family:verdana;\">\n\nI'm starting my journey in kaggle competition and I'm beginning to realize the importance of hyperparameters tuning.\n    \n    \nSo far, I find it kind of hard to get a feeling of the different parameters in boosting models (e.g. catboost, xgboost...) and so I have being relying on the optuna library that helps you maximize\/minimize an objective function under a user defined search space.\n    \n    \nI've spent quite some (computer-)time tuning parameters with Optuna and today I'm investigating the plot commodities that are available (relying on plotly)\n\nHere is the code of the objective function i'm maximizing:\n \n    \n <code>def objective(trial):\n\n    param_grid = {\n        'iterations': trial.suggest_categorical('iterations',[N_ESTIMATORS]),\n        'learning_rate' : trial.suggest_uniform('learning_rate', 0.005,0.1),\n        'depth': trial.suggest_int('depth', 3, 12),\n        'l2_leaf_reg': trial.suggest_uniform('l2_leaf_reg', 1e-5,100),\n        'subsample': trial.suggest_uniform('subsample',0,1),\n        'random_strength' : trial.suggest_uniform('random_strength', 1, 50),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15)\n     } \n\n    display(param_grid)\n    \n    model = CatBoostClassifier(\n        grow_policy='Depthwise',\n        leaf_estimation_method='Newton', \n        bootstrap_type='Bernoulli',\n        loss_function= LOSS,\n        eval_metric= EVAL_METRIC,\n        task_type='GPU',\n        silent=True,\n        random_seed = SEED,\n        **param_grid\n    )\n     \n    scores,_,_ = run_kfold(model, N_SPLITS)\n\n    return scores.mean()\n<\/code>\n    \n<\/div>","f7ac7bb6":"<div style=\"font-size:15px; font-family:verdana;\">\n    <p> Happy tuning ! Please upvote if you find this notebook usefull <\/p>\n<\/div> "}}