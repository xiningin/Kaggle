{"cell_type":{"da7691f5":"code","f8c489cb":"code","4882e1b7":"code","75758109":"code","209a5517":"code","44fa78f2":"code","6e50cb0c":"code","9517f98c":"code","b954e22a":"code","ba9c4dea":"code","04297a98":"code","409686b3":"code","d11ab36e":"code","7b7681ff":"code","c0d3f515":"code","d02e4b99":"code","390df074":"code","8e697cba":"code","96543d7d":"code","ceedfcdd":"code","eb5c538e":"code","89c203b8":"markdown","8e80e491":"markdown","ca30b259":"markdown","ef79eb2e":"markdown","62a7674c":"markdown","653adf25":"markdown","10c6daeb":"markdown"},"source":{"da7691f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f8c489cb":"!pip install --upgrade git+https:\/\/github.com\/tusharsarkar3\/XBNet.git","4882e1b7":"import torch\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom XBNet.training_utils import training,predict\nfrom XBNet.models import XBNETClassifier\nfrom XBNet.run import run_XBNET ","75758109":"df=pd.read_csv('\/kaggle\/input\/cusersmarildownloadsgermancsv\/german.csv',encoding ='ISO-8859-1',sep=\";\")\ndf.head()","209a5517":"y=df[['Creditability']].to_numpy()\ny","44fa78f2":"x=df.loc[:,'Duration_of_Credit_monthly':].to_numpy()\nx","6e50cb0c":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.20, random_state= True,stratify=y) \nx_train.shape,x_test.shape,y_train.shape,y_test.shape","9517f98c":"y_train=y_train.reshape((-1))\ny_train.shape","b954e22a":"y_test=y_test.reshape((-1))\ny_test.shape","ba9c4dea":"#In and out layer dimensions 10\/Set bias False\/Last layer Softmax\/Softmax Dimension 1\n\nmodel = XBNETClassifier(x_train,y_train,num_layers=2)","04297a98":"criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)","409686b3":"m,acc, lo, val_ac, val_lo = run_XBNET(x_train,x_test,y_train,y_test,model,criterion,optimizer,epochs=25,batch_size=32)","d11ab36e":"import matplotlib.pyplot as plt\nplt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nplt.plot(acc,label='training accuracy')\nplt.plot(val_ac,label = 'validation accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.grid()\nplt.subplot(1,2,2)\nplt.plot(lo,label='training loss')\nplt.plot(val_lo,label = 'validation loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend() \nplt.grid()","7b7681ff":"#In and out layer dimensions 10\/Set bias True\/None \n\nmodel = XBNETClassifier(x_train,y_train,num_layers=2)","c0d3f515":"criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)","d02e4b99":"m,acc, lo, val_ac, val_lo = run_XBNET(x_train,x_test,y_train,y_test,model,criterion,optimizer,epochs=25,batch_size=32)","390df074":"import matplotlib.pyplot as plt\nplt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nplt.plot(acc,label='training accuracy')\nplt.plot(val_ac,label = 'validation accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.grid()\nplt.subplot(1,2,2)\nplt.plot(lo,label='training loss')\nplt.plot(val_lo,label = 'validation loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend() \nplt.grid()","8e697cba":"#In and out layer dimensions 100\/Set bias True\/Sigmoid \n\nmodel = XBNETClassifier(x_train,y_train,num_layers=2)","96543d7d":"criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)","ceedfcdd":"m,acc, lo, val_ac, val_lo = run_XBNET(x_train,x_test,y_train,y_test,model,criterion,optimizer,epochs=25,batch_size=32)","eb5c538e":"import matplotlib.pyplot as plt\nplt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nplt.plot(acc,label='training accuracy')\nplt.plot(val_ac,label = 'validation accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.grid()\nplt.subplot(1,2,2)\nplt.plot(lo,label='training loss')\nplt.plot(val_lo,label = 'validation loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend() \nplt.grid()","89c203b8":"#XBNet developed by Tushar Sarkar https:\/\/github.com\/tusharsarkar3\/XBNet\n\n#Code by Gaurav Dutta https:\/\/www.kaggle.com\/gauravduttakiit\/xbnet-on-digit-completion-data","8e80e491":"In the snippet below\n\nAnswers to \"Enter your last layer\" Sigmoid and None: 23:37:42 WARNING: ..\/src\/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\nIf you choose Softmax (Enter your last layer), the programm will ask what is the Softmax Dimension.","ca30b259":"#With Sigmoid and Softmax is Dead. When it's None (digit 3), the charts show some movement.","ef79eb2e":"#My Accuracy is Dead!","62a7674c":"#XBNet developed by Tushar Sarkar https:\/\/github.com\/tusharsarkar3\/XBNet\n\n\"XBNET that is built on PyTorch combines tree-based models with neural networks to create a robust architecture that is trained by using a novel optimization technique, Boosted Gradient Descent for Tabular Data which increases its interpretability and performance. Boosted Gradient Descent is initialized with the feature importance of a gradient boosted tree, and it updates the weights of each layer in the neural network in two steps:\"\n\n\"Update weights by gradient descent.\"\n\n\"Update weights by using feature importance of a gradient boosted tree in every intermediate layer.\n\nFeatures\n\n\"Better performance, training stability and interpretability for tabular data.\"\n\n\"Easy to implement with rapid prototyping capabilities\"\n\n\"Minimum Code requirements for creating any neural network with or without boosting\"\n\nhttps:\/\/github.com\/tusharsarkar3\/XBNet","653adf25":"#I think it's better. At least is still alive.","10c6daeb":"#Let's try some \"Resuscitation\" procedure changing Dimensions of layer to True"}}