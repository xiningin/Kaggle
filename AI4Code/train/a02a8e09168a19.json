{"cell_type":{"6bd192ca":"code","bb90c3b4":"code","192040a8":"code","dc263781":"code","7880621c":"code","931ae94f":"code","25c20cb3":"code","94e0da42":"code","90a998fb":"code","9416f008":"code","c9c220ef":"code","63843b1b":"code","369a3644":"code","84a7a44e":"code","37e20747":"code","b7300aff":"code","c648a437":"code","431d99d3":"code","c8af8567":"code","582def17":"code","e9cedb99":"code","b59a0cc9":"code","567530d8":"code","36e6e394":"code","217c719c":"code","86253fd2":"code","c224d5f4":"code","cccc429a":"code","3d52d577":"code","e21c1cf3":"code","b760e180":"code","a60573df":"code","5daaa07c":"code","b266ccd5":"code","7372cdd0":"code","51aef651":"code","20787bbb":"code","3efe3d67":"code","89f26a65":"code","8f5f71b1":"code","c8765534":"code","b384c777":"code","9d0d28f2":"code","eaef8143":"code","5cad977b":"code","ee5c4540":"code","dc0e75ad":"code","1f68d8de":"code","fd69fc5b":"code","1f0b894b":"code","72a3caf8":"code","98664990":"code","4f817c98":"code","d23befb8":"code","2973a26b":"code","45adfe96":"code","64f5c592":"code","6ae0b895":"code","fd85cf98":"code","f11c2f6e":"code","c3337150":"code","461de6d3":"code","68f8f2d2":"code","8236b5fd":"code","2988b93f":"code","a5579738":"code","c2de3ebe":"code","cec46659":"code","06bf7705":"code","0abb0021":"code","e6989f3f":"code","c3fbc2bb":"code","8fc5424b":"code","955f7dad":"code","b5bc3cc9":"code","f3cf9c73":"code","83421a5d":"code","357721a4":"code","01825ccf":"code","34b60bd9":"code","fce7d7e3":"code","a54c6c9a":"code","c0ed9f17":"code","52c752a8":"code","c0ca620f":"code","3cd95415":"code","26e39144":"markdown","e1528537":"markdown","b2ff7ddc":"markdown","b0f7ca03":"markdown","c9386ff0":"markdown","2c62687f":"markdown","254de8cd":"markdown","e74a1a02":"markdown","17d32d40":"markdown","3d519966":"markdown","944c926a":"markdown","da66290b":"markdown","9e22f72b":"markdown","807fc6ad":"markdown","28368001":"markdown","010e4350":"markdown","b5db57f6":"markdown","33723a9f":"markdown","b5a48987":"markdown","e46cdc78":"markdown","f5a5585a":"markdown","a77c8979":"markdown","79ef0163":"markdown","d2b69e80":"markdown","752eea14":"markdown","6f2f0a48":"markdown","f2ad4909":"markdown","fee81a66":"markdown","e0bbb882":"markdown","ad48438b":"markdown","2a439bde":"markdown","0ad3a941":"markdown","98e0ad77":"markdown","180b9321":"markdown","c0a4a24a":"markdown","c663ceda":"markdown","0606c664":"markdown","df3cc97a":"markdown"},"source":{"6bd192ca":"import pandas as pd\nimport numpy as np\n","bb90c3b4":"df = pd.read_csv('..\/input\/melbourne-housing-snapshot\/melb_data.csv')","192040a8":"df.head()","dc263781":"df.info()","7880621c":"df.shape","931ae94f":"import seaborn as sns\nsns.set_style('darkgrid')\nsns.set_color_codes(palette = 'deep')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","25c20cb3":"\n# Check the distribution\nsns.displot(df['Price'], \n            color = 'b', \n            height = 6.5 , \n            aspect = 1.5,\n            bins = 60, \n            kde = True);\n","94e0da42":"## Skew and kurt \n\nprint(\"Skewness: %f\" % df['Price'].skew())\nprint(\"Kurtosis: %f\" % df['Price'].kurt())","90a998fb":"numerical_df = df.select_dtypes(exclude = 'object')\ncategorical_df = df.select_dtypes(include = 'object')","9416f008":"# \n\nnumerical_df.shape[1] , categorical_df.shape[1]","c9c220ef":"numerical_df[\"Id\"] = df.index + 1","63843b1b":"# Let's check their distribution\n\na = 5 # number of rows\nb = 3 # number of columns\nc = 1 # initialize plot counter\n\nfig = plt.figure(figsize = (20,30))\n\nfor i in numerical_df:\n    if c == 14:\n        break\n    else:\n        plt.subplot(a, b, c)\n        plt.title(i.format(i, a, b, c))\n        plt.xlabel(i)\n        sns.scatterplot(x = i, y = \"Id\", data = numerical_df)\n        c = c + 1\n        \nplt.show()","369a3644":"# How the numeric values is realated with \"Price\"\n\na = 5 # number of rows\nb = 3 # number of columns\nc = 1 # initialize plot counter\n\nfig = plt.figure(figsize = (20,30))\n\nfor i in numerical_df:\n    if c == 14:\n        break\n    else:\n        plt.subplot(a, b, c)\n        plt.title(i.format(i, a, b, c))\n        plt.xlabel(i)\n        sns.scatterplot(x = i, y = df['Price'], data = df)\n        c = c + 1\n        \nplt.show()","84a7a44e":"corr = df.corr()\nplt.subplots(figsize = (15,12))\nsns.heatmap(corr, vmax = 0.9, annot = True,square = True);","37e20747":"categorical_df.shape","b7300aff":"# Let's see the Countplot of each Categorical attribute\n\na = 3 # number of rows\nb = 3 # number of columns\nc = 1 # initialize plot counter\n\nfig = plt.figure(figsize = (20,20))\n\nfor i in categorical_df:\n    if c == 9:\n        break\n    else:\n        plt.subplot(a, b, c)\n        plt.title(i.format(i, a, b, c))\n        plt.xlabel(i)\n        sns.countplot(x = i, data = df)\n        c = c + 1\n        \nplt.show()","c648a437":"## council area\n\nplt.figure(figsize  = (12,8))\nsns.boxplot(y = df['CouncilArea'], x = 'Price', data = df);","431d99d3":"## council area\n\nplt.figure(figsize  = (12,8))\nsns.boxplot(y = df['Regionname'], x = 'Price', data = df);","c8af8567":"import numpy as np\nfrom scipy.stats import norm","582def17":"xdf = df.copy()","e9cedb99":"# Starting with Target Attribute\n\nsns.displot(xdf['Price'], color = 'b', aspect = 2, height = 6, kde = True, bins = 60);","b59a0cc9":"# log(1+x) transform\n\nxdf['Price'] = np.log1p(xdf['Price'])","567530d8":"# Let's plt the SalePrice again\n\nplt.figure(figsize = (10,8))\nsns.distplot(xdf['Price'], fit = norm, color = 'b');\n\n(mu, sigma) = norm.fit(xdf['Price'])\n\nprint('\\n mu = {:.2f} and sigma = {:.2f} \\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=${:.2f})'.format(mu, sigma)],loc = 'best');","36e6e394":"tf = xdf.copy()","217c719c":"outliers_df = ['Distance','Bedroom2','Bathroom','Car','Landsize','BuildingArea','YearBuilt','Propertycount']","86253fd2":"# Let's take a close look again\n\na = 3 # number of rows\nb = 3 # number of columns\nc = 1 # initialize plot counter\nfig = plt.figure(figsize = (20,20))\n\nfor i in outliers_df:\n    if c == 9:\n        break\n    else:\n        plt.subplot(a, b, c)\n        plt.xlabel(i)\n        sns.scatterplot(x = i, y = xdf['Price'], data = tf)\n        c = c + 1\n        \nplt.show()","c224d5f4":"out_df = tf.copy()","cccc429a":"out_df.shape","3d52d577":"# We will remove all the outliers manually\n\nout_df.drop(out_df[(out_df['Bathroom'] > 6.5)].index, inplace = True)\nout_df.drop(out_df[(out_df['Bedroom2'] > 12.5)].index, inplace = True)\nout_df.drop(out_df[(out_df['Car'] > 8.2)].index, inplace = True)\nout_df.drop(out_df[(out_df['Landsize'] > 30000)].index, inplace = True)\nout_df.drop(out_df[(out_df['BuildingArea'] > 1200)].index, inplace = True)\nout_df.drop(out_df[(out_df['YearBuilt'] < 1600)].index, inplace = True)","e21c1cf3":"out_df.shape","b760e180":"total = out_df.isnull().sum().sort_values(ascending = False)\npercent = ((out_df.isnull().sum() \/ out_df.shape[0]) * 100).sort_values(ascending = False)\npercent = np.round(percent, 3)\ntypes = out_df[percent.index].dtypes\n\nmissing_data = pd.concat([total, percent, types], axis = 1, keys = ['Total','Percent','Type'])\nmissing_data.head(6)","a60573df":"out_df.drop([\"BuildingArea\",\"YearBuilt\"], axis = 1, inplace = True)","5daaa07c":"# CouncilArea\n\nout_df[\"CouncilArea\"].fillna(out_df[\"CouncilArea\"].mode()[0], inplace = True)","b266ccd5":"# Car\nout_df['Car'].fillna(out_df['Car'].median(), inplace = True)","7372cdd0":"out_df.isnull().sum()","51aef651":"# Fetch all numeric features\n\nnumeric_dtypes = ['int16','int32','int64','float16','float32','float64']\n\nnumeric = []\n\nfor i in out_df.columns:\n    if out_df[i].dtype in numeric_dtypes:\n        numeric.append(i)","20787bbb":"## Create box plot for all numerica features\n\nplt.figure(figsize = (10,12))\nsns.boxplot(data = out_df[numeric])\nplt.xlabel(\"Numeric values\")\nplt.ylabel(\"Feature names\")\nplt.title(\"Numeric Distribution of Features\")","3efe3d67":"from scipy.stats import skew","89f26a65":"# Find skewed numerical features\n\nskew_features = out_df[numeric].apply(lambda x: skew(x)).sort_values(ascending = False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5:\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew': high_skew})\nskew_features.head(10)","8f5f71b1":"from scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax","c8765534":"# Normalize skewed features\n\nfor i in skew_index:\n    out_df[i] = boxcox1p(out_df[i], boxcox_normmax(out_df[i] + 1))","b384c777":"# Plotting again\n\nplt.figure(figsize = (10,12))\nsns.boxplot(data = out_df[numeric])\nplt.xlabel(\"Numeric values\")\nplt.ylabel(\"Feature names\")\nplt.title(\"Numeric Distribution of Features\")","9d0d28f2":"plt.figure(figsize = (14,12))\ncorr = xdf.corr()\nsns.heatmap(corr, mask = corr < 0.6, linewidth = 0.5, cmap = \"Blues\")","eaef8143":"# Let's list all the corrleated values\n\nnum = xdf.select_dtypes(exclude = 'O')\ncorr = num.corr()\ncorr[['Price']].sort_values(['Price'], ascending = False)","5cad977b":"for i in categorical_df:\n    print(\"Unique values: \" + i + \" :\", len(out_df[i].unique()))\n    print(out_df[i].value_counts())\n    print(\"-\"*60)","ee5c4540":"out_df.drop(['Address','SellerG','Suburb'], axis =1 , inplace = True)","dc0e75ad":"# Let's update the value in categorical_df\n\ncategorical_df = out_df.select_dtypes(include = 'O')","1f68d8de":"categorical_df.shape","fd69fc5b":"# Let's again check their relationship with \"Price\"\n\na = 2 # number of rows\nb = 3 # number of columns\nc = 1 # initialize plot counter\n\nfig = plt.figure(figsize = (20,20))\n\nfor i in categorical_df:\n    if c == 6:\n        break\n    else:\n        plt.subplot(a, b, c)\n        plt.title(i.format(i, a, b, c))\n        plt.xlabel(i)\n        sns.boxplot(x = i, y = xdf['Price'], data = out_df)\n        c = c + 1\n        \nplt.show()","1f0b894b":"# Let's retreive Day ,Month and Year from Date\n\nout_df['Date'] = pd.to_datetime(out_df['Date'])","72a3caf8":"out_df['Day'] = pd.DatetimeIndex(out_df['Date']).day\nout_df['Month'] = pd.DatetimeIndex(out_df['Date']).month\nout_df['Year'] = pd.DatetimeIndex(out_df['Date']).year","98664990":"# Let's see their relationship with Price","4f817c98":"date = ['Day','Month','Year']","d23befb8":"\na = 2 # number of rows\nb = 3 # number of columns\nc = 1 # initialize plot counter\n\nfig = plt.figure(figsize = (20,20))\n\nfor i in date:\n    if c == 6:\n        break\n    else:\n        plt.subplot(a, b, c)\n        plt.title(i.format(i, a, b, c))\n        plt.xlabel(i)\n        sns.scatterplot(x = i, y = xdf['Price'], data = out_df)\n        c = c + 1\n        \nplt.show()","2973a26b":"out_df.drop(['Year','Date'], axis = 1, inplace = True)","45adfe96":"categorical_df = out_df.select_dtypes(include = 'O')","64f5c592":"categorical_df","6ae0b895":"out_df['Type'].value_counts()","fd85cf98":"out_df['Method'].value_counts()","f11c2f6e":"out_df['CouncilArea'].value_counts()","c3337150":"all_features = pd.get_dummies(out_df)","461de6d3":"plt.figure(figsize = (14,12))\ncorr = all_features.corr()\nsns.heatmap(corr, mask = corr < 0.6, linewidth = 0.5, cmap = \"Blues\")","68f8f2d2":"## Split Feautures and labels\n\ntrain_labels = all_features['Price'].reset_index(drop = True)\ntrain_features = all_features.drop(['Price'], axis = 1)\n\ntrain_features.shape","8236b5fd":"X = train_features.iloc[:len(train_labels), :]","2988b93f":"X.shape, train_labels.shape","a5579738":"from sklearn.model_selection import KFold","c2de3ebe":"# Setup cross validation folds\n\nkf = KFold(n_splits = 12, random_state= 42, shuffle = True)","cec46659":"# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","06bf7705":"# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n#\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler","0abb0021":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","e6989f3f":"scores = {}\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgb'] = (score.mean(), score.std())","c3fbc2bb":"score = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgb'] = (score.mean(), score.std())","8fc5424b":"score = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['svr'] = (score.mean(), score.std())","955f7dad":"score = cv_rmse(ridge)\nprint(\"ridge: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['ridge'] = (score.mean(), score.std())","b5bc3cc9":"score = cv_rmse(rf)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())","f3cf9c73":"score = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gbr'] = (score.mean(), score.std())","83421a5d":"print('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(train_labels))","357721a4":"print('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, train_labels)","01825ccf":"print('xgboost')\nxgb_model_full_data = xgboost.fit(X, train_labels)","34b60bd9":"print('Svr')\nsvr_model_full_data = svr.fit(X, train_labels)","fce7d7e3":"print('Ridge')\nridge_model_full_data = ridge.fit(X, train_labels)","a54c6c9a":"print('RandomForest')\nrf_model_full_data = rf.fit(X, train_labels)","c0ed9f17":"print('GradientBoosting')\ngbr_model_full_data = gbr.fit(X, train_labels)","52c752a8":"# Blend models in order to make the final predictions more robust to overfitting\ndef blended_predictions(X):\n    return ((0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.2 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.1 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.05 * rf_model_full_data.predict(X)) + \\\n            (0.35 * stack_gen_model.predict(np.array(X))))","c0ca620f":"# Get final precitions from the blended model\nblended_score = rmsle(train_labels, blended_predictions(X))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","3cd95415":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","26e39144":"# Observe Dataset","e1528537":"**Reference**:\n\nhttps:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition","b2ff7ddc":"**From the above observation I see, these attribute have maximum outliers**\n- Distance\n- Bedroom2\n- Bathroom\n- Car\n- Landsize\n- BuildingArea\n- YearBuilt\n- Propertycount\n\nLet's visulaize these attributes closely.","b0f7ca03":"### Multicollinearity","c9386ff0":"### Setup cross validation and define error metrics","2c62687f":"We use the scipy function **boxcox1p** which computes the Box-Cox transformation. The goal is to find a simple transformation that lets us normalize data.","254de8cd":"## Remove Outliers","e74a1a02":"![1_cfWYbVWYm5oCQqYWEPX5Bw.png](attachment:1_cfWYbVWYm5oCQqYWEPX5Bw.png)\n\nSo there are two things to notice \u2014 The peak of the curve and the tails of the curve, Kurtosis measure is responsible for capturing this phenomenon. The formula for kurtosis calculation is complex (4th moment in the moment-based calculation) so we will stick to the concept and its visual clarity. A normal distribution has a kurtosis of 3 and is called mesokurtic. Distributions greater than 3 are called leptokurtic and less than 3 are called platykurtic. So the greater the value more the peakedness. Kurtosis ranges from 1 to infinity. As the kurtosis measure for a normal distribution is 3, we can calculate excess kurtosis by keeping reference zero for normal distribution. Now excess kurtosis will vary from -2 to infinity.","17d32d40":"### Let's check Multicollinearity again","3d519966":"# Feature Engineering","944c926a":"# Exploratory Data Analysis","da66290b":"### Blend models and get predictions","9e22f72b":"To make it easiers, let's seperate <b> Numerical <\/b> attribute and <b> Categorical <\/b> attributes.","807fc6ad":"The SalePrice is now normally distributed.","28368001":"#### How to handle skewness?\n\n![inbox_3360047_cf6ef72ac8178d7f58600cba76433c0a_pearson-mode-skewness.jpg](attachment:inbox_3360047_cf6ef72ac8178d7f58600cba76433c0a_pearson-mode-skewness.jpg)\n\n<b>Reducing Right Skewness or Postive Skewness<\/b>\n\n- Square root \n\nThe square root, x to x^(1\/2) = sqrt(x), is a transformation with a\nmoderate effect on distribution shape: it is weaker than the logarithm\nand the cube root. It is also used for reducing right skewness, and also\nhas the advantage that it can be applied to zero values. Note that the\nsquare root of an area has the units of a length. It is commonly applied\nto counted data, especially if the values are mostly rather small.\n\n\n- Cube Root\n\nThe cube root, x to x^(1\/3). This is a fairly strong transformation with\na substantial effect on distribution shape: it is weaker than the\nlogarithm. It is also used for reducing right skewness, and has the\nadvantage that it can be applied to zero and negative values. Note that\nthe cube root of a volume has the units of a length. It is commonly\napplied to rainfall data\n\n- Logarithms\n\nThe logarithm, x to log base 10 of x, or x to log base e of x (ln x), or x to log base 2 of x, is a strong transformation with a major effect on distribution shape. It is commonly used for reducing right skewness and is often appropriate for measured variables. It can not be applied to zero or negative values.\n\n- BoxCox\n\n<b> Reducing Left Skewness <\/b>\n\n- squares\n\nThe square, x to x\u00b2, has a moderate effect on distribution shape and it could be used to reduce left skewness. Squaring usually makes sense only if the variable concerned is zero or positive, given that (-x)\u00b2 and x\u00b2 are identical.\n\n- Cubes\n\nThe cube, x to x\u00b3, has a better effect on distribution shape than squaring and it could be used to reduce left skewness.\n\n- Higher powers\n\nWhen simple transformation like square and cubes doesn\u2019t reduce the skewness in the data distribution, we can use Higher powers to transform to data. It is only useful in left skewness.","010e4350":"![1_r_DMuupDzRa0ZxOoe7TYDg.png](attachment:1_r_DMuupDzRa0ZxOoe7TYDg.png)\n\n<b> So, when is the skewness too much? <\/b>\n\nThe rule of thumb seems to be:\n- If the skewness is between -0.5 and 0.5, the data are fairly symmetrical.\n- If the skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed), the data are moderately skewed.\n- If the skewness is less than -1(negatively skewed) or greater than 1(positively skewed), the data are highly skewed.\n\n### Kurtosis\n\nThink of puching or pulling the normal distribution curve from the top, what impact will it have on the shape of the distribution? Let's visualize","b5db57f6":"## Multivariate Analysis (Numeric)","33723a9f":"<b> What is Skewness and Kurtosis? <\/b>\n\n- Skewness is a measure of symmetry or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\n\n- Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or lack of outliers. A uniform distirubtion would be the extreme case.\n\nhttps:\/\/towardsdatascience.com\/skewness-kurtosis-simplified-1338e094fc85\n\n### Skewness\n\nWhat if we encounter an asymmetrical distribution, how do we detect the extent of asymmetry? Let's see visually what happens to the measures of central tendency when we encounter such graphs.\n\n![1_Gqd6Ioie0sa_Hryb3grsFQ.png](attachment:1_Gqd6Ioie0sa_Hryb3grsFQ.png)\n\nNotice how these central tendency measures tend to spread when the normal distributions is distorted. For the nomenclature just follow the direction of the tail - For the left graph since the tail is to the left, it is left-skewed (negatively skewed) and the right graph has the tail to the right, so it is right-skewed (positively skewed).","b5a48987":"### Setup Models","e46cdc78":"So we can see, what datas are missing. Since, **YearBuilt** and **BuildingArea** has higher number of missing values and it's also negatively correlated with **Price** we will drop them. \n\n**CouncilArea** is a categorical variable, so we will impute the missing values with **mode**\nWhereas, **Car** will be imputed by, **median**.","f5a5585a":"'Year' doesn't have much data, we will drop the original \"Date\" and \"Year\".","a77c8979":"`xdf` we will the check the accuracy on `xdf` too.","79ef0163":"Excess Kurtosis for Normal Distribution = 3\u20133 = 0\nThe lowest value of Excess Kurtosis is when Kurtosis is 1 = 1\u20133 = -2\n\n![1_lU3NEdbwWeGoJyuYfmuQVw.png](attachment:1_lU3NEdbwWeGoJyuYfmuQVw.png)","d2b69e80":"Let's re","752eea14":"The SalePrice is skewed to the right. This is a problem because most ML models don't do well with non-normally distributed data. We can apply a log(1+x) transform to fix the skew.","6f2f0a48":"# Preicting Melbourne Housing Dataset\n\n**Notes on Specific Variables**\n\n- Rooms: Number of rooms\n\n- Price: Price in dollars\n\n- Method: S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withdrawn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N\/A - price or highest bid not available.\n\n- Type: br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential.\n\n- SellerG: Real Estate Agent\n\n- Date: Date sold\n\n- Distance: Distance from CBD\n\n- Regionname: General Region (West, North West, North, North east \u2026etc)\n\n- Propertycount: Number of properties that exist in the suburb.\n\n- Bedroom2 : Scraped # of Bedrooms (from different source)\n\n- Bathroom: Number of Bathrooms\n\n- Car: Number of carspots\n\n- Landsize: Land Size\n\n- BuildingArea: Building Size\n\n- CouncilArea: Governing council for the area\n\n\n## Content\n\n1. Observe Dataset\n2. Exploratory Data Analysis\n    - Univariate Analysis \n    - Multivariate Analysis\n3. Feature Engineering\n    - Remove Outliers\n    - Fill Missing Values\n    - Fix Skewed Features\n    - Multicollinearity\n    - Encoding\n4. Models and Evaluation Metrics\n    - LightGBM\n    - XGBoost\n    - Ridge\n    - SVR\n    - GBR\n    - RandomForestRegressor\n    - StackingCVRegressor\n    - Blending\n    ","f2ad4909":"# Models and Evaluation Metrics","fee81a66":"## Fill Missing Values","e0bbb882":"## Fix skewed features","ad48438b":"### Recreating training and test sets","2a439bde":"### Encode Categorical Features","0ad3a941":"and let's plot how the features are correlated to each other, and to SalePrice","98e0ad77":"## Multivariate Analaysis (Categorical)","180b9321":"### Train Models","c0a4a24a":"Let's drop large number of categorical attributes.","c663ceda":"We have already fixed skewness on **Target Attribute**. We will fix skewness on other attribute to. To summarize, generally if the distribution of data is skewed to the left, the mean is less than the median, which is often less than the mode. If the distribution of data is skewed to the right, the mode is often less than the median, which is less than the mean.\n\nSo in skewed data, the tail region may act as an outlier for the statistical model and we know that outliers adversely affect the model\u2019s performance especially regression-based models. There are statistical model that are robust to outlier like a Tree-based models but it will limit the possibility to try other models. So there is a necessity to transform the skewed data to close enough to a Gaussian distribution or Normal distribution. This will allow us to try more number of statistical model.\n\nhttps:\/\/towardsdatascience.com\/skewed-data-a-problem-to-your-statistical-model-9a6b5bb74e37","0606c664":"## Fit the models","df3cc97a":"### Univariate Analysis (Target Attribute)"}}