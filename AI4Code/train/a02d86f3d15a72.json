{"cell_type":{"13ed7e0c":"code","3d90aa83":"code","064dbda9":"code","c23a78a3":"code","324a7f72":"code","21454fdf":"markdown","558dcb9d":"markdown","d52718a8":"markdown","e9977e93":"markdown","fcd8ad01":"markdown","0f86336c":"markdown","3aab3ee0":"markdown","be59f2da":"markdown"},"source":{"13ed7e0c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport seaborn as sb\nimport matplotlib\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n","3d90aa83":"# load the Data\ntraining = pd.read_csv(\"..\/input\/train.csv\")\n\ntarget = training[\"label\"]\ntraining = training.drop(\"label\", axis=1)\n\n# set up figure\nplt.figure(figsize=(15, 13))\nfor digit in range(0, 50):\n    plt.subplot(5, 10, digit+1)\n    data_grid = training.iloc[digit].values.reshape(28, 28)\n    plt.imshow(data_grid)\nplt.tight_layout(pad=1)\n    \n    ","064dbda9":"# Set up a variable N = number of rows to apply PCA to\nN = 10000\nX = training[:N].values\n\n# Y = Target Values\nY = target[:N]\n\n# Standardize the values\nX_std = StandardScaler().fit_transform(X)\n\n# Call PCA method from sklearn toolkit\npca = PCA(n_components=4)\nprinciple_components = pca.fit_transform(X_std)","c23a78a3":"trace = go.Scatter(\n    x = principle_components[:, 0],\n    y = principle_components[:, 1],\n    mode=\"markers\",\n    text=Y,\n    marker=dict(\n        size=8, color=Y, \n        colorscale=\"Electric\", \n        opacity=0.7\n    )\n)\n\ndata = [trace]\n\nlayout = go.Layout(\n    title=\"Principle Component Analysis\",\n    xaxis=dict(\n        title=\"First Principle Component\",\n        gridwidth=2\n    ),\n    yaxis=dict(\n        title=\"Second Princple Component\",\n        gridwidth=2\n    )\n)\n\nfig = dict(data=data, layout=layout)\npy.iplot(fig)","324a7f72":"# Use Kmeans method from sklearn.cluster library\nkmn = KMeans(init='k-means++', n_clusters=9, random_state=0)\nX_kmean = kmn.fit_predict(principle_components)\n\ntrace_k = go.Scatter(\n    x=principle_components[:,0],\n    y=principle_components[:,1],\n    mode=\"markers\",\n    marker=dict(\n        size=8,\n        color=X_kmean,\n        colorscale=\"Picnic\",\n    )\n)\ndata_k = [trace_k]\n\nlayout_k = go.Layout(\n    title=\"K-Means Clustering result\",\n    xaxis=dict(title='First Principle Component', gridwidth=2),\n    yaxis=dict(title='Second Principle Component', gridwidth=2)\n)\n\nfig_k = dict(data=data_k, layout=layout_k)\npy.iplot(fig_k)\n\n","21454fdf":"### Intro: Why do we reduce dimensionality?\n\nThe curse of dimensionality is ever present:\n- Too many dimensions\/features = complexity\n- Processing power needed increases, as data needed to fill in all those dimensions increases!\n- Reducing the number of dimensions is a good idea, but we need to do it in a way to make sure that not too   much important data is thrown away \n\nLets start by importing the necessary modules\/tools\n","558dcb9d":"Now, lets plot the results by way of a scatter plot","d52718a8":"### Sklearn PCA\n\n- We will use Python's useful sklearn toolkit to apply PCA to the training data\n","e9977e93":"### Discussion of plot\n- We see that the clusters are much more distinguishable from each other. Success","fcd8ad01":"## Dimensionality Reduction Techniques for MINST dataset classification (Beginner alert!)\n\n","0f86336c":"### What PCA (Principle Component Analysis) is all about!\n\nGreat video:\nhttps:\/\/www.youtube.com\/watch?v=_UVHneBUBW0&list=PLLIH2ZW8RSTg3e8BEoMglp5XSJAWhM1VD&index=2&t=151s\n\n- Helps extract new \"better\" set of variables from an existing large set of variables\n- This is done by forming a linear combination of those original variables\n- These new variables are essentially the \"Principle Components\"\n\n- First Component tries to explain the maximum variance in the dataset\n- Each component thereafter, continually tries to explain the remaining variance in the dataset (i.e variance that is not explained by all the previous principle components\n\n\nFirst Lets plot some of the images representing digits","3aab3ee0":"### KMeans Clustering Technique\n\n- We can make up for this by applying the popular Kmeans Clustering Technique to the new data that has just passed through the PCA run\n- Basically, it attempts to split a given anonymous dataset into fixed \"k\" number of clusters\n    - Centroids are initialized, and data generally moves towards the closest centroid (rough explanation)\n","be59f2da":"### Reviewing the Graph\n\n- The groupings of points are becoming apparent, but there is no obvious seperation between groups.\n- PCA is actually an unsupervised model, meaning it does not use class labels to train the data\n- This is a reason why the above graph shows not much \"clear\" distinction between clusters of points\n- The different groups \n"}}