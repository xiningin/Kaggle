{"cell_type":{"c5f4f5a0":"code","d2deedb6":"code","ecceaca7":"code","69218039":"code","57b5b98f":"code","c102485e":"code","02ebd825":"code","544e26e5":"code","ec7b4d93":"code","1ce6a046":"code","e34ce97a":"code","64ac7226":"code","1cd5d48f":"code","28a4e428":"code","ece42751":"code","c4bd2452":"code","e9fbd52b":"code","b782ff5f":"markdown","04b5e806":"markdown","61eaab78":"markdown","c5c1261b":"markdown","dfdb495d":"markdown","b2648a8c":"markdown","d8195b25":"markdown","adc366e7":"markdown","9675d8f8":"markdown"},"source":{"c5f4f5a0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv', \n                    parse_dates=[\"date_time\"])\ntrain = train.set_index('date_time')\ntarget = train[['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']]\ntrain = train.drop(['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'], axis=1)\ntest = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv',\n                  parse_dates=[\"date_time\"])\ntest = test.set_index('date_time')","d2deedb6":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(train, target, test_size=0.2)","ecceaca7":"from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='box-cox')\nX_train_sc = pd.DataFrame(data = pt.fit_transform(X_train), columns=X_train.columns, \n                          index=X_train.index)\nX_val_sc = pd.DataFrame(data = pt.transform(X_val), columns=X_val.columns, \n                          index=X_val.index)\n\nfig = X_train_sc.hist(figsize=(100, 100), bins=30)\n[x.title.set_size(80) for x in fig.ravel()]\nplt.show()","69218039":"from sklearn.metrics import mean_squared_log_error\n\ndef score_model(model, tr, y_train, val, y_val, fitted=False):\n    preds = [[],[],[]]\n    for i in range(3):\n        if not fitted:\n            model.fit(tr, y_train.iloc[:,i])\n        preds[i] = model.predict(val)\n    return mean_squared_log_error(y_val, np.array(preds).T)","57b5b98f":"from sklearn.linear_model import GammaRegressor\n\nmodel = GammaRegressor()\nprint('Baseline error (scaled):',\n      \"{0:.4f}\".format(score_model(model, X_train_sc, y_train, X_val_sc, y_val)),\n      '\\nWithout scaling:',\n      \"{0:.4f}\".format(score_model(model, X_train, y_train, X_val, y_val)))","c102485e":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\nprint('Random Forest (scaled):',\n      \"{0:.4f}\".format(score_model(model, X_train_sc, y_train, X_val_sc, y_val)),\n      '\\nWithout scaling:',\n      \"{0:.4f}\".format(score_model(model, X_train, y_train, X_val, y_val)))","02ebd825":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=0, objective='reg:gamma')\nprint('XGBoost (scaled):',\n      \"{0:.4f}\".format(score_model(model, X_train_sc, y_train, X_val_sc, y_val)),\n      '\\nWithout scaling:',\n      \"{0:.4f}\".format(score_model(model, X_train, y_train, X_val, y_val)))","544e26e5":"from lightgbm import LGBMRegressor\n\nmodel = LGBMRegressor(n_estimators=500, learning_rate=0.01, random_state=0, objective='gamma')\nprint('LightGBM (scaled):',\n      \"{0:.4f}\".format(score_model(model, X_train_sc, y_train, X_val_sc, y_val)),\n      '\\nWithout scaling:',\n      \"{0:.4f}\".format(score_model(model, X_train, y_train, X_val, y_val)))","ec7b4d93":"from catboost import CatBoostRegressor\n\nmodel = CatBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=0, verbose=0,\n                          objective='Tweedie:variance_power=1.5') # Gamma regression\nprint('CatBoost (scaled):',\n      \"{0:.4f}\".format(score_model(model, X_train_sc, y_train, X_val_sc, y_val)),\n      '\\nWithout scaling:',\n      \"{0:.4f}\".format(score_model(model, X_train, y_train, X_val, y_val)))","1ce6a046":"from random import sample\n\nsampling = np.array(sample(list(X_train.index),len(X_train))).reshape((4,-1))\n  \nX_train0 = X_train[X_train.index.isin(sampling[0])]\ny_train0 = y_train[y_train.index.isin(sampling[0])]\n\nX_train1 = X_train[X_train.index.isin(sampling[1])]\ny_train1 = y_train[y_train.index.isin(sampling[1])]\n\nX_train2 = X_train[X_train.index.isin(sampling[2])]\ny_train2 = y_train[y_train.index.isin(sampling[2])]\n\nX_train3 = X_train[X_train.index.isin(sampling[3])]\ny_train3 = y_train[y_train.index.isin(sampling[3])]\n\npreds = [[],[],[]]\nfor i in range(3):\n    model0 = CatBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=0, verbose=0,\n                              objective='Tweedie:variance_power=1.5') \n    model0.fit(X_train0, y_train0.iloc[:,i])\n\n    model1 = RandomForestRegressor(n_estimators=100, random_state=0)\n    model1.fit(X_train1, y_train1.iloc[:,i])\n\n    model2 = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=0, objective='reg:gamma')\n    model2.fit(X_train2, y_train2.iloc[:,i])\n\n    model3 = LGBMRegressor(n_estimators=500, learning_rate=0.01, random_state=0, objective='gamma')\n    model3.fit(X_train3, y_train3.iloc[:,i])\n\n    preds[i] = pd.DataFrame()\n    preds[i]['Catboost'] = model0.predict(X_val)\n    preds[i]['RandomForest'] = model1.predict(X_val)\n    preds[i]['XGBoost'] = model2.predict(X_val)\n    preds[i]['LGBM'] = model3.predict(X_val)\n\npreds = pd.DataFrame(np.array(preds).mean(axis=2).T, columns=y_val.columns, index=y_val.index)\nprint('Ensemble error:',\n     mean_squared_log_error(y_val, preds))","e34ce97a":"from random import sample\n\nsampling = np.array(sample(list(X_train.index),len(X_train))).reshape((4,-1))\n  \nX_train_sc0 = X_train_sc[X_train_sc.index.isin(sampling[0])]\ny_train0 = y_train[y_train.index.isin(sampling[0])]\n\nX_train_sc1 = X_train_sc[X_train_sc.index.isin(sampling[1])]\ny_train1 = y_train[y_train.index.isin(sampling[1])]\n\nX_train_sc2 = X_train_sc[X_train_sc.index.isin(sampling[2])]\ny_train2 = y_train[y_train.index.isin(sampling[2])]\n\nX_train_sc3 = X_train_sc[X_train_sc.index.isin(sampling[3])]\ny_train3 = y_train[y_train.index.isin(sampling[3])]\n\npreds = [[],[],[]]\nfor i in range(3):\n    model0 = CatBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=0, verbose=0,\n                              objective='Tweedie:variance_power=1.5') \n    model0.fit(X_train_sc0, y_train0.iloc[:,i])\n\n    model1 = RandomForestRegressor(n_estimators=100, random_state=0)\n    model1.fit(X_train_sc1, y_train1.iloc[:,i])\n\n    model2 = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=0, objective='reg:gamma')\n    model2.fit(X_train_sc2, y_train2.iloc[:,i])\n\n    model3 = LGBMRegressor(n_estimators=500, learning_rate=0.01, random_state=0, objective='gamma')\n    model3.fit(X_train_sc3, y_train3.iloc[:,i])\n\n    preds[i] = pd.DataFrame()\n    preds[i]['Catboost'] = model0.predict(X_val_sc)\n    preds[i]['RandomForest'] = model1.predict(X_val_sc)\n    preds[i]['XGBoost'] = model2.predict(X_val_sc)\n    preds[i]['LGBM'] = model3.predict(X_val_sc)\n\npreds = pd.DataFrame(np.array(preds).mean(axis=2).T, columns=y_val.columns, index=y_val.index)\nprint('Ensemble error (scaled):',\n     mean_squared_log_error(y_val, preds))","64ac7226":"from sklearn.ensemble import StackingRegressor\n\nreg = StackingRegressor(\n        estimators=[\n            ('RandomForest', RandomForestRegressor(n_estimators=100, random_state=0)),\n            ('XGBoost', XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=0, objective='reg:gamma')),\n            ('LGBM', LGBMRegressor(n_estimators=500, learning_rate=0.01, random_state=0, objective='gamma')),\n            ('CatBoost', CatBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=0, verbose=0,\n                              objective='Tweedie:variance_power=1.5'))\n        ],\n        final_estimator=GammaRegressor()\n     )\n\nprint('Stacking (scaled):',\n      \"{0:.4f}\".format(score_model(reg, X_train_sc, y_train, X_val_sc, y_val)),\n      '\\nWithout scaling:',\n      \"{0:.4f}\".format(score_model(reg, X_train, y_train, X_val, y_val)))","1cd5d48f":"from sklearn.ensemble import VotingRegressor\n\nreg = VotingRegressor(\n        estimators=[\n            ('RandomForest', RandomForestRegressor(n_estimators=100, random_state=0)),\n            ('XGBoost', XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=0, objective='reg:gamma')),\n            ('LGBM', LGBMRegressor(n_estimators=500, learning_rate=0.01, random_state=0, objective='gamma')),\n            ('CatBoost', CatBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=0, verbose=0,\n                              objective='Tweedie:variance_power=1.5'))\n        ]\n     )\n\nprint('Voting (scaled):',\n      \"{0:.4f}\".format(score_model(reg, X_train_sc, y_train, X_val_sc, y_val)),\n      '\\nWithout scaling:',\n      \"{0:.4f}\".format(score_model(reg, X_train, y_train, X_val, y_val)))","28a4e428":"final_model = VotingRegressor(\n        estimators=[\n            ('RandomForest', RandomForestRegressor(n_estimators=100, random_state=0)),\n            ('XGBoost', XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=0, objective='reg:gamma')),\n            ('LGBM', LGBMRegressor(n_estimators=500, learning_rate=0.01, random_state=0, objective='gamma')),\n            ('CatBoost', CatBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=0, verbose=0,\n                              objective='Tweedie:variance_power=1.5'))\n        ]\n     )\npreds = [[],[],[]]\nfor i in range(3):\n    final_model.fit(train, target.iloc[:,i])\n    preds[i] = final_model.predict(test)","ece42751":"preds = pd.DataFrame(data=np.array(preds).T, columns=target.columns, index=test.index)","c4bd2452":"fig, ax = plt.subplots(3,1, figsize=(50,20))\ntarget.target_benzene.plot(ax=ax[0])\npreds.target_benzene.plot(ax=ax[0])\n\ntarget.target_carbon_monoxide.plot(ax=ax[1])\npreds.target_carbon_monoxide.plot(ax=ax[1])\n\ntarget.target_nitrogen_oxides.plot(ax=ax[2])\npreds.target_nitrogen_oxides.plot(ax=ax[2])\nplt.show()","e9fbd52b":"preds.reset_index().to_csv('submission.csv', index=False)","b782ff5f":"## Ensemble","04b5e806":"## XGBoost","61eaab78":"## Model selection\n\nAs always we will try linear models as baseline, then random forest, xgboost, ligthboost, catboost and finally and ensemble.","c5c1261b":"# Tabular Playground\n## Data loading and preprocessing\n\nFollowing the same steps as the other notebook I will standardize the data.","dfdb495d":"### Linear model","b2648a8c":"## Random forest","d8195b25":"## LightGBM","adc366e7":"## CatBoost","9675d8f8":"## Prediction\n\nIt seems the voting method with the four methods is the best of all."}}