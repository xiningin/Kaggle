{"cell_type":{"57885e41":"code","90c34f9a":"code","22b086d3":"code","36e7f4de":"code","2a3629fe":"code","fa60385c":"code","2d925bbd":"code","281d4fd6":"code","329254eb":"code","fdfe565a":"code","482cf14c":"code","09d966d4":"code","0c9587b5":"code","874b6cf7":"code","8630eaf3":"code","b8af1d83":"code","6581e7de":"code","4591e103":"code","67ca87f5":"code","2be37038":"code","5326adf9":"code","1d8d83ab":"code","0ccc91b1":"code","3be1eb3e":"code","9a095104":"code","a1948e52":"code","3804a094":"code","20482a68":"code","ffe16429":"code","a2fd50e7":"code","b95d33f8":"code","1b9a2e4e":"code","a8b02966":"code","2c13fb1c":"code","da32b766":"code","340565ed":"code","e3763de6":"code","fedb7162":"code","1bc9a7e4":"code","c20ce0d0":"code","8b15248f":"code","7d1dd0d1":"code","6fac03f0":"code","ce29ef45":"code","e093803a":"code","71822636":"code","2c080655":"code","dce72b65":"code","9d5b02e7":"code","5ee22168":"code","aebda6ff":"code","92a124c6":"code","862bb239":"code","bb96df83":"code","389022e9":"code","360aec8a":"code","e0ee15ae":"code","2de1b340":"code","8ac9278d":"markdown","9f1c7ee2":"markdown","09f872d2":"markdown","fb287c58":"markdown","bdda239a":"markdown","00812113":"markdown","ce426af8":"markdown","258cbfff":"markdown","0c983321":"markdown","78262957":"markdown","4df3577d":"markdown"},"source":{"57885e41":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","90c34f9a":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nfrom sklearn.model_selection import train_test_split","22b086d3":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam","36e7f4de":"import re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom tqdm import tqdm","2a3629fe":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\nfrom sklearn.preprocessing import LabelEncoder\n\n# fix random seed for reproducibility\nseed = 42\nnp.random.seed(seed)","fa60385c":"import os","2d925bbd":"df= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ndf.head(10)","281d4fd6":"df.tail(10)","329254eb":"test.head(2)","fdfe565a":"print('There are {} rows and {} columns in train'.format(df.shape[0],df.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))","482cf14c":"x=df.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","09d966d4":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=df[df['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='blue')\nax1.set_title('disaster')\nword=df[df['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","0c9587b5":"def concat_df(df, test):\n    # Returns a concatenated df of training and test set on axis 0\n    return pd.concat([df, test], sort=True).reset_index(drop=True)\ndf_all = concat_df(df, test)\nprint(df.shape)\nprint(test.shape)\nprint(df_all.shape)\ndf_all.head()","874b6cf7":"features = ['keyword','location']\nfor feat in features : \n    print(\"The number of missing values in \"+ str(feat)+\" is \"+str(df_all[feat].isnull().sum())+ \" for the combined dataset\")\n    print(\"The number of missing values in \"+ str(feat)+\" is \"+str(df[feat].isnull().sum())+ \" for the train dataset\")\n    print(\"The number of missing values in \"+ str(feat)+\" is \"+str(test[feat].isnull().sum())+ \" for the test dataset\")","8630eaf3":"t_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n# A helper function to clean the data, you can lines based on the dataset. Note that this dataset\n# is lot cleaner than usual NLP problems. In usual NLP, you need to keenly look at the data and then clean.\n# Remember, the uncleaned text is very crucial for feature engineering(Metafeatures). So keep them in one column\ndef clean_text(text):\n    import re\n    text = text.lower()\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"you'll\", \"you will\", text)\n    text = re.sub(r\"i'll\", \"i will\", text)\n    text = re.sub(r\"she'll\", \"she will\", text)\n    text = re.sub(r\"he'll\", \"he will\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"there's\", \"there is\", text)\n    text = re.sub(r\"here's\", \"here is\", text)\n    text = re.sub(r\"who's\", \"who is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"don't\", \"do not\", text)\n    text = re.sub(r\"shouldn't\", \"should not\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"[^a-z]\", \" \", text) # This removes anything other than lower case letters(very imp)\n    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n    return text","b8af1d83":"df['clean_text'] = df['text'].apply(clean_text)\ntest['clean_text'] = test['text'].apply(clean_text)","6581e7de":"# Removal of punctuations (we had already done this in the clean text, but this way also faster to compute)\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndf[\"clean_text\"] = df[\"clean_text\"].apply(lambda text: remove_punctuation(text))\ntest[\"clean_text\"] = test[\"clean_text\"].apply(lambda text: remove_punctuation(text))","4591e103":"# Removal of stopwords\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ndf[\"clean_text\"] = df[\"clean_text\"].apply(lambda text: remove_stopwords(text))\ntest[\"clean_text\"] = test[\"clean_text\"].apply(lambda text: remove_stopwords(text))","67ca87f5":"# Lemmatizing the words using WordNet\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ndf[\"clean_text\"] = df[\"clean_text\"].apply(lambda text: lemmatize_words(text))\ntest[\"clean_text\"] = test[\"clean_text\"].apply(lambda text: lemmatize_words(text))","2be37038":"df.head(5)","5326adf9":"test.head(5)","1d8d83ab":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","0ccc91b1":"def create_corpus(target):\n    corpus=[]\n    \n    for x in df[df['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","3be1eb3e":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')","9a095104":"stop=set(stopwords.words('english'))","a1948e52":"counter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","3804a094":"sns.barplot(x=y,y=x)","20482a68":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(df['text'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","ffe16429":"example=\"My Profile: https:\/\/auth.geeksforgeeks.org\\\n\/ user \/ Chinmoy % 20Lenka \/ articles in\\\nthe portal of http:\/\/www.geeksforgeeks.org\/\"","a2fd50e7":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","b95d33f8":"df['text']=df['text'].apply(lambda x : remove_URL(x))","1b9a2e4e":"example = \"\"\"<div>\n<h1>Reel or Real<\/h1>\n<p>Hindustan <\/p>\n<a href=\"https:\/\/www.hindustan.com\/c\/nlp-open-the source\">get the source<\/a>\n<\/div>\"\"\"","a8b02966":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","2c13fb1c":"df['text']=df['text'].apply(lambda x : remove_html(x))","da32b766":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","340565ed":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","e3763de6":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #GREAT Man.,\"\nprint(remove_punct(example))","fedb7162":"df['text']=df['text'].apply(lambda x: remove_punct(x))","1bc9a7e4":"df.head(5)","c20ce0d0":"pip install -U symspellpy","8b15248f":"!pip install pyspellchecker","7d1dd0d1":"from spellchecker import SpellChecker\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)","6fac03f0":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam","ce29ef45":"def cv(data):\n    count_vectorizer = CountVectorizer()\n\n    emb = count_vectorizer.fit_transform(data)\n\n    return emb, count_vectorizer\n\nlist_corpus = df[\"text\"].tolist()\nlist_labels = df[\"target\"].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n                                                                                random_state=40)\n\nX_train_counts, count_vectorizer = cv(X_train)\nX_test_counts = count_vectorizer.transform(X_test)","e093803a":"from sklearn.decomposition import TruncatedSVD","71822636":"import matplotlib.pyplot as plt","2c080655":"def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue']\n        if plot:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='orange', label='Not')\n            blue_patch = mpatches.Patch(color='blue', label='Real')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\n\nfig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_counts, y_train)\nplt.show()","dce72b65":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n\n    train = tfidf_vectorizer.fit_transform(data)\n\n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","9d5b02e7":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_tfidf, y_train)\nplt.show()","5ee22168":"def create_corpus_new(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet)]\n        corpus.append(words)\n    return corpus ","aebda6ff":"corpus=create_corpus_new(df)","92a124c6":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","862bb239":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","bb96df83":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","389022e9":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec    ","360aec8a":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=3e-4)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","e0ee15ae":"model.summary()","2de1b340":"train=tweet_pad[:df.shape[0]]\ntest=tweet_pad[df.shape[0]:]","8ac9278d":"# BAG OF WORDS","9f1c7ee2":"# REMOVE HTML ","09f872d2":"# REMOVE PUNCTUATIONS","fb287c58":"Distribution for Target","bdda239a":"Observation: There are more tweets with class 0 (No disaster) than class 1 (disaster)","00812113":"**COMMON WORDS**","ce426af8":"# **N GRAM **","258cbfff":"# REMOVE EMOJI'S","0c983321":"# **Remove URL**","78262957":"**PROBLEM STATEMENT:** Build a machine learning model that predicts which Tweets are about real disasters and Not","4df3577d":"# CHECK THE SPELLING "}}