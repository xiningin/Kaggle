{"cell_type":{"37babaa1":"code","ec1fdaa8":"code","da26bb12":"code","a3b0ffe9":"code","a1558a63":"code","0b4f774b":"code","20bc5419":"code","693f6d10":"code","6dd4bedd":"code","beb964c4":"code","cc5db1b4":"code","9fe8a631":"code","41f624fb":"code","5ff6c8a7":"code","e448ec87":"code","25b7bcad":"code","0a5a3733":"code","900f7bee":"code","adf8fa8f":"code","cc4d33df":"code","c3ccf945":"code","ebe1abdf":"code","5bb12d3b":"code","64d43b01":"code","dd7e60f1":"code","f0aecf8b":"code","2192b673":"code","6d0eb1d4":"markdown","53287560":"markdown","9ca5ce14":"markdown","498e7128":"markdown","03385072":"markdown","606ce3ac":"markdown","d653957e":"markdown","253413ee":"markdown","6f2ab480":"markdown","9c863ebf":"markdown","bde1408d":"markdown","f393aec6":"markdown","8b8b9ab7":"markdown","9b1ab9bb":"markdown","c20a2057":"markdown","4ea7bf96":"markdown","c2994bb8":"markdown","55e0dcd2":"markdown","8c5d41ce":"markdown","74527dff":"markdown","90c08d5e":"markdown","b472c349":"markdown","2d347fd4":"markdown","a9d9ee5d":"markdown","2f04ee9a":"markdown","3fdeea3a":"markdown","458ffae0":"markdown"},"source":{"37babaa1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndata.head()","ec1fdaa8":"data.info()","da26bb12":"data.TotalCharges = pd.to_numeric(data.TotalCharges, errors='coerce')\ndata.info()","a3b0ffe9":"#delete rows including null values\ndata.dropna(inplace = True)","a1558a63":"data.drop([\"customerID\"],axis=1,inplace = True)","0b4f774b":"data.gender = [1 if each == \"Male\" else 0 for each in data.gender]\n\ncolumns_to_convert = ['Partner', \n                      'Dependents', \n                      'PhoneService', \n                      'MultipleLines',\n                      'OnlineSecurity',\n                      'OnlineBackup',\n                      'DeviceProtection',\n                      'TechSupport',\n                      'StreamingTV',\n                      'StreamingMovies',\n                      'PaperlessBilling', \n                      'Churn']\n\nfor item in columns_to_convert:\n    data[item] = [1 if each == \"Yes\" else 0 if each == \"No\" else -1 for each in data[item]]\n    \ndata.head()","20bc5419":"sns.countplot(x=\"Churn\",data=data);","693f6d10":"sns.pairplot(data,vars = ['tenure','MonthlyCharges','TotalCharges'], hue=\"Churn\")","6dd4bedd":"sns.set(style=\"whitegrid\")\ng1=sns.catplot(x=\"Contract\", y=\"Churn\", data=data,kind=\"bar\")\ng1.set_ylabels(\"Churn Probability\")\n\ng2=sns.catplot(x=\"InternetService\", y=\"Churn\", data=data,kind=\"bar\")\ng2.set_ylabels(\"Churn Probability\")","beb964c4":"data = pd.get_dummies(data=data)\ndata.head()","cc5db1b4":"data.corr()['Churn'].sort_values()","9fe8a631":"#assign Class_att column as y attribute\ny = data.Churn.values\n\n#drop Class_att column, remain only numerical columns\nnew_data = data.drop([\"Churn\"],axis=1)\n\n#Normalize values to fit between 0 and 1. \nx = (new_data-np.min(new_data))\/(np.max(new_data)-np.min(new_data)).values","41f624fb":"#Split data into Train and Test \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,random_state =1)","5ff6c8a7":"# %%KNN Classification\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3) #set K neighbor as 3\nknn.fit(x_train,y_train)\npredicted_y = knn.predict(x_test)\nprint(\"KNN accuracy according to K=3 is :\",knn.score(x_test,y_test))","e448ec87":"score_array = []\nfor each in range(1,25):\n    knn_loop = KNeighborsClassifier(n_neighbors = each) #set K neighbor as 3\n    knn_loop.fit(x_train,y_train)\n    score_array.append(knn_loop.score(x_test,y_test))\n    \nplt.plot(range(1,25),score_array)\nplt.xlabel(\"Range\")\nplt.ylabel(\"Score\")\nplt.show()","25b7bcad":"knn_model = KNeighborsClassifier(n_neighbors = 11) #set K neighbor as 11\nknn_model.fit(x_train,y_train)\npredicted_y = knn_model.predict(x_test)\naccuracy_knn = knn_model.score(x_test,y_test)\nprint(\"KNN accuracy according to K=11 is :\",accuracy_knn)","0a5a3733":"# %%Logistic regression classification\nfrom sklearn.linear_model import LogisticRegression\nlr_model = LogisticRegression()\nlr_model.fit(x_train,y_train)\naccuracy_lr = lr_model.score(x_test,y_test)\nprint(\"Logistic Regression accuracy is :\",accuracy_lr)","900f7bee":"# %%SVM Classification\nfrom sklearn.svm import SVC\nsvc_model = SVC(random_state = 1)\nsvc_model.fit(x_train,y_train)\naccuracy_svc = svc_model.score(x_test,y_test)\nprint(\"SVM accuracy is :\",accuracy_svc)","adf8fa8f":"# %%Naive Bayes Classification\nfrom sklearn.naive_bayes import GaussianNB\nnb_model = GaussianNB()\nnb_model.fit(x_train,y_train)\naccuracy_nb = nb_model.score(x_test,y_test)\nprint(\"Naive Bayes accuracy is :\",accuracy_nb)","cc4d33df":"# %%Decision Tree Classification\nfrom sklearn.tree import DecisionTreeClassifier\ndt_model = DecisionTreeClassifier()\ndt_model.fit(x_train,y_train)\naccuracy_dt = dt_model.score(x_test,y_test)\nprint(\"Decision Tree accuracy is :\",accuracy_dt)","c3ccf945":"# %%Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrf_model_initial = RandomForestClassifier(n_estimators = 5, random_state = 1)\nrf_model_initial.fit(x_train,y_train)\nprint(\"Random Forest accuracy for 5 trees is :\",rf_model_initial.score(x_test,y_test))","ebe1abdf":"score_array = []\nfor each in range(1,50):\n    rf_loop = RandomForestClassifier(n_estimators = each, random_state = 1) #set K neighbor as 3\n    rf_loop.fit(x_train,y_train)\n    score_array.append(rf_loop.score(x_test,y_test))\n    \nplt.plot(range(1,50),score_array)\nplt.xlabel(\"Range\")\nplt.ylabel(\"Score\")\nplt.show()","5bb12d3b":"rf_model = RandomForestClassifier(n_estimators = 33, random_state = 1) #set tree number as 33\nrf_model.fit(x_train,y_train)\naccuracy_rf = rf_model.score(x_test,y_test)\nprint(\"Random Forest accuracy for 33 trees is :\",accuracy_rf)","64d43b01":"# %%Confusion Matrix libraries\nfrom sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report\n\n#for Logistic Regression\ncm_lr = confusion_matrix(y_test,lr_model.predict(x_test))\n\n# %% confusion matrix visualization\nimport seaborn as sns\nf, ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm_lr, annot = True, linewidths = 0.5, color = \"red\", fmt = \".0f\", ax=ax)\nplt.xlabel(\"y_predicted\")\nplt.ylabel(\"y_true\")\nplt.title(\"Confusion Matrix of Logistic Regression\")\nplt.show()","dd7e60f1":"# the function that prints all scores\ndef print_scores(headline, y_true, y_pred):\n    print(headline)\n    acc_score = accuracy_score(y_true, y_pred)\n    print(\"accuracy: \",acc_score)\n    pre_score = precision_score(y_true, y_pred)\n    print(\"precision: \",pre_score)\n    rec_score = recall_score(y_true, y_pred)                            \n    print(\"recall: \",rec_score)\n    f_score = f1_score(y_true, y_pred, average='weighted')\n    print(\"f1_score: \",f_score)","f0aecf8b":"report = classification_report(y_test, lr_model.predict(x_test))\nprint(report)","2192b673":"print_scores(\"Logistic Regression;\",y_test, lr_model.predict(x_test))\nprint_scores(\"SVC;\",y_test, svc_model.predict(x_test))\nprint_scores(\"KNN;\",y_test, knn_model.predict(x_test))\nprint_scores(\"Naive Bayes;\",y_test, nb_model.predict(x_test))\nprint_scores(\"Decision Tree;\",y_test, dt_model.predict(x_test))\nprint_scores(\"Random Forest;\",y_test, rf_model.predict(x_test))","6d0eb1d4":"We don't need customerID column for analyzing, so we can drop this column. ","53287560":"For logistic regression confusion matrix; \n\n    TN = 927\n    FP = 114\n    FN = 169\n    TP = 197\n \n This means; there are total 927+114 = 1041 actual non-churn values and the algorithm predict 927 of them as non-churn and 114 of them churn. Also there are total 169 + 197 = 366 actual churn values and the algorithm predict 169 of them as non-churn and 197 of them as churn. \n \n Acuuracy should not be used as solely metric for imbalance datasets. There are some other metrics named as recall and precision.\n\n<figure>\n    <img src='https:\/\/drive.google.com\/uc?export=view&id=1_4ZejjBAy3WUo-7OkS0hJpzBEaRr3otQ' \n         style=\"width: 400px; max-width: 100%; height: auto\"\n         alt='missing'\/>\n<\/figure>\n\nSometimes we get high recall and low precision or vice versa. There is another metric that combines both precision and recall like below. We will use F1 score to identify the best algorithm score.\n\n<figure>\n    <img src='https:\/\/drive.google.com\/uc?export=view&id=1sbGivo8TRDQr-iEBrM4HdHTuhVJD8kms' \n         style=\"width: 300px; max-width: 100%; height: auto\"\n         alt='missing'\/>\n<\/figure>","9ca5ce14":"**CONCLUSION**\n* Since data set is imbalanced, we prefered to use F1 score rather than accuracy.\n* Logistic Regression gives the highest F1 Score, so it is the best model. \n* Naive Bayes is the worst model because it gives the lowest F1 score.\n* Sex has no impact on churn.\n* People having month-to-month contract tend to churn more than people having long term conracts.\n* As the tenure increases, the probability of churn decreases.\n* As tmonthly charges increases, the probability of churn increases.\n\nI hope you liked my Kernel! If you have any comments or  suggestions, please write below.\n\nThank you!\n","498e7128":"We assume K = 3 for first iteration, but actually we don't know what is the optimal K value that gives maximum accuracy. So we can write a for loop that iterates for example 25 times and gives the accuracy at each iteartion. So that we can find the optimal K value.","03385072":"As you can see above, if we use K = 11, then we get maximum score of %78.7","606ce3ac":"**Splitting Data**\n\nSplit the data set as train and test with %20-%80 ratio.","d653957e":"As you can see, the highest accuracy is at n_estimators = 33.","253413ee":"Logistic regression and SVC classificagtion algorithms have the highest accuracy. But as I mentioned before, our data is imbalanced. So it is important to look at the confusion matrix according to these two algorithms. With imbalanced datasets, the highest accuracy does not give the best model. Assume we have 1000 total rows, 10 rows are churn and 990 rows are non-churn. If we find all these 10 churn rows as non-churn, then the accuracy will be still %99. Althogh it is a wrong model, if we do not look at the confusion matrix, then we can not see the mistake.\n\nConfusion matrix gives us FN(false negative), FP(false positive), TN(true negative) and TP(true positive) values.\n<figure>\n    <img src='https:\/\/drive.google.com\/uc?export=view&id=1Ks14UX9YOnXvSVZx4ucfU1bIgNpiYMdz' \n         style=\"width: 400px; max-width: 100%; height: auto\"\n         alt='missing'\/>\n<\/figure>\n\n","6f2ab480":"Let's write a function that calculates and print both accuracy, recall, precision and weighted F1 score.","9c863ebf":"We can also use classification_report function from skleran library to show all these metrics.","bde1408d":"Let's see the correlation between churn and the remaining columns. Customers having month-to-month contract, having fiber optic internet service and using electronic payment are tend to churn more whereas people having two-year contract and having internet service are tend to not churn.","f393aec6":"**Prepare x and y**\n\nFirst, seperate x and y values. y would be our class which is Churn column in this dataset. x would be the remaing columns.\nAlso, apply normalization to x in order to scale all values between 0 and 1.","8b8b9ab7":"**SVM(Support Vector Machine) Classification**","9b1ab9bb":"Convert remaining text based columns to dummy columns using pandas get_dummies function. This function creates new columns named as values of the related columns.\n\nNow our data set only have integer and numerical columns so that we can apply statistical models.","c20a2057":"TotalCharges column is seen as object type, but includes numeric type values. Convert this column to numeric. ","4ea7bf96":"## **Churn Prediction of Telco Customers using Classification Algorithms**\n\nIn this kernel, I want to apply some classification algorithm on Telco customers churn data set.","c2994bb8":"**Random Forest Classification**","55e0dcd2":"Replace text columns to integers. The columns below includes similar text values so I changed them once.","8c5d41ce":"**Decision Tree Classification**","74527dff":"**Logistic Regression Classification**","90c08d5e":"Print all results of each algorithm.","b472c349":"## **Apply Machine Learning Algorithms**\n\nLet's start to apply some machine learning algorithms and find the accuracy of each.\n\n\n\n**KNN Classification**","2d347fd4":"Let's look at the distribution of Churn values. As you can see below, the data set is imbalanced. But for now, I will ignore this.","a9d9ee5d":"People having lower tenure and higher monthly charges are tend to churn more.\nAlso as you can see below; having month-to-month contract and fiber obtic internet have a really huge effect on churn probability.","2f04ee9a":"There are 11 missing values in TotalCharges column. We can fill the missing values with median data, set it to 0 or delete these rows, it is up to you. I prefer deleting these columns because it is a small part compared to all data.","3fdeea3a":"I set tree number as 5 initially. But I want to find the appropriate tree number. Let's try to find the best number with trying 1 to 50.","458ffae0":"**Naive Bayes Classification**"}}