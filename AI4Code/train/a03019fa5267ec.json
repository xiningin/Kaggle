{"cell_type":{"1552bc04":"code","c8abb249":"code","86ecd541":"code","1ea4a6d3":"code","1d73b5a1":"code","01f9ad47":"code","b9d5e823":"code","dbc39b72":"code","34861c93":"code","5e6c51bd":"code","47619aa7":"code","7db92522":"code","5798dfa4":"code","66c5dbe7":"code","bf3cb19d":"code","02ffd037":"code","9e5d5f8f":"code","89e304ab":"code","c5f26ac4":"code","7dbd3ebc":"code","54bf331f":"code","bcfc3ebd":"code","43657ffd":"code","fb4f83db":"code","b24d1131":"code","158d2766":"code","8f6fe000":"code","d531453b":"code","33f93f4a":"code","f3a64ac4":"code","92970405":"markdown","25e17d30":"markdown"},"source":{"1552bc04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","c8abb249":"import torch\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision import transforms as T # for simplifying the transforms\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, sampler, random_split\nfrom torchvision import models","86ecd541":"!pip install timm # kaggle doesnt have it installed by default\nimport timm\nfrom timm.loss import LabelSmoothingCrossEntropy","1ea4a6d3":"import warnings\nwarnings.filterwarnings(\"ignore\")","1d73b5a1":"import matplotlib.pyplot as plt\n%matplotlib inline","01f9ad47":"import sys\nfrom tqdm import tqdm\nimport time\nimport copy\n","b9d5e823":"torch.backends.cudnn.benchmark = True","dbc39b72":"def get_classes(data_dir):\n    all_data = datasets.ImageFolder(data_dir)\n    return all_data.classes","34861c93":"def get_data_loaders(data_dir, batch_size, train = False):\n    if train:\n        #train\n        transform = T.Compose([\n            T.RandomHorizontalFlip(),\n            T.RandomVerticalFlip(),\n            T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.25),\n            T.Resize(256),\n            T.CenterCrop(224),\n            T.ToTensor(),\n            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # imagenet means\n            T.RandomErasing(p=0.2, value='random')\n        ])\n        train_data = datasets.ImageFolder(os.path.join(data_dir, \"train\/\"), transform = transform)\n        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n        return train_loader, len(train_data)\n    else:\n        # val\/test\n        transform = T.Compose([ # We dont need augmentation for test transforms\n            T.Resize(256),\n            T.CenterCrop(224),\n            T.ToTensor(),\n            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # imagenet means\n        ])\n        val_data = datasets.ImageFolder(os.path.join(data_dir, \"valid\/\"), transform=transform)\n        test_data = datasets.ImageFolder(os.path.join(data_dir, \"test\/\"), transform=transform)\n        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=4)\n        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n        return val_loader, test_loader, len(val_data), len(test_data)","5e6c51bd":"dataset_path = \"\/kaggle\/input\/butterfly-images40-species\/\"","47619aa7":"(train_loader, train_data_len) = get_data_loaders(dataset_path, 128, train=True)\n(val_loader, test_loader, valid_data_len, test_data_len) = get_data_loaders(dataset_path, 32, train=False)","7db92522":"classes = get_classes(\"\/kaggle\/input\/butterfly-images40-species\/train\/\")\nprint(classes, len(classes))","5798dfa4":"dataloaders = {\n    \"train\": train_loader,\n    \"val\": val_loader\n}\ndataset_sizes = {\n    \"train\": train_data_len,\n    \"val\": valid_data_len\n}","66c5dbe7":"print(len(train_loader), len(val_loader), len(test_loader))\nprint(train_data_len, valid_data_len, test_data_len)","bf3cb19d":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","02ffd037":"def set_model(model):\n    for param in model.parameters(): #freeze model\n        param.requires_grad = False\n\n    n_inputs = model.classifier.in_features\n    model.classifier = nn.Sequential(\n        nn.Linear(n_inputs, 512),\n        nn.ReLU(),\n        nn.Dropout(0.3),\n        nn.Linear(512, len(classes))\n    )\n    model = model.to(device)\n    return model","9e5d5f8f":"def print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")\/1e6)\n    os.remove('temp.p')","89e304ab":"teacher = timm.create_model(\"efficientnet_b3\", pretrained=True)\nstudent = timm.create_model(\"efficientnet_b0\", pretrained=True)","c5f26ac4":"teacher = set_model(teacher)\nstudent = set_model(student)","7dbd3ebc":"loaded = torch.jit.load(\"..\/input\/efficientnet-pytorch-butterfly-dataset\/butterfly_efficientnet_b3.pt\")\nteacher.load_state_dict(loaded.state_dict())","54bf331f":"print_size_of_model(teacher)\nprint_size_of_model(student)","bcfc3ebd":"criterion = LabelSmoothingCrossEntropy()\ndivergence_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\ncriterion = criterion.to(device)\ndivergence_loss_fn = divergence_loss_fn.to(device)","43657ffd":"optimizer = optim.AdamW(student.classifier.parameters(), lr=0.001)\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)","fb4f83db":"teacher.eval()\nstudent.train()","b24d1131":"test_loss = 0.0\nclass_correct = list(0 for i in range(len(classes)))\nclass_total = list(0 for i in range(len(classes)))\nteacher.eval()\n\nfor data, target in tqdm(test_loader):\n    data, target = data.to(device), target.to(device)\n    with torch.no_grad(): # turn off autograd for faster testing\n        output = teacher(data)\n        loss = criterion(output, target)\n    test_loss = loss.item() * data.size(0)\n    _, pred = torch.max(output, 1)\n    correct_tensor = pred.eq(target.data.view_as(pred))\n    correct = np.squeeze(correct_tensor.cpu().numpy())\n    if len(target) == 32:\n        for i in range(32):\n            label = target.data[i]\n            class_correct[label] += correct[i].item()\n            class_total[label] += 1\n\ntest_loss = test_loss \/ test_data_len\nprint('Test Loss: {:.4f}'.format(test_loss))\nfor i in range(len(classes)):\n    if class_total[i] > 0:\n        print(\"Test Accuracy of %5s: %2d%% (%2d\/%2d)\" % (\n            classes[i], 100*class_correct[i]\/class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])\n        ))\n    else:\n        print(\"Test accuracy of %5s: NA\" % (classes[i]))\nprint(\"Test Accuracy of %2d%% (%2d\/%2d)\" % (\n            100*np.sum(class_correct)\/np.sum(class_total), np.sum(class_correct), np.sum(class_total)\n        ))","158d2766":"import torch.nn.functional as F","8f6fe000":"def distill_model(teacher, student, criterion, divergence_loss, optimizer, scheduler, num_epochs=10):\n    since = time.time()\n    best_model_wts = copy.deepcopy(student.state_dict())\n    best_acc = 0.0\n    temp=7\n    alpha=0.3\n    teacher.eval()\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}\/{num_epochs - 1}')\n        print(\"-\"*10)\n        \n        for phase in ['train', 'val']: # We do training and validation phase per epoch\n            if phase == 'train':\n                student.train() # model to training mode\n            else:\n                student.eval() # model to evaluate\n            \n            running_loss = 0.0\n            running_corrects = 0.0\n            \n            for inputs, labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                ## Supposedly faster than optimizer.zero_grad() ##\n                for param in student.parameters():\n                    param.grad = None\n                \n                with torch.set_grad_enabled(phase == 'train'): # no autograd makes validation go faster\n                    teacher_preds = teacher(inputs)\n                    student_preds = student(inputs)\n                    \n                    _, preds = torch.max(student_preds, 1) # used for accuracy\n                    \n                    student_loss = criterion(student_preds, labels)\n                    ditillation_loss = divergence_loss(\n                        F.softmax(student_preds \/ temp, dim=1),\n                        F.softmax(teacher_preds \/ temp, dim=1)\n                    )\n                    \n                    loss = alpha * student_loss + (1 - alpha) * ditillation_loss\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n            if phase == 'train':\n                scheduler.step() # step at end of epoch\n            \n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc =  running_corrects.double() \/ dataset_sizes[phase]\n            \n            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n            \n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(student.state_dict()) # keep the best validation accuracy model\n        print()\n    time_elapsed = time.time() - since # slight error\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n    \n    student.load_state_dict(best_model_wts)\n    return student","d531453b":"student_ft = distill_model(teacher, student, criterion, divergence_loss_fn, optimizer, exp_lr_scheduler, num_epochs=5)","33f93f4a":"test_loss = 0.0\nclass_correct = list(0 for i in range(len(classes)))\nclass_total = list(0 for i in range(len(classes)))\nstudent_ft.eval()\n\nfor data, target in tqdm(test_loader):\n    data, target = data.to(device), target.to(device)\n    with torch.no_grad(): # turn off autograd for faster testing\n        output = student_ft(data)\n        loss = criterion(output, target)\n    test_loss = loss.item() * data.size(0)\n    _, pred = torch.max(output, 1)\n    correct_tensor = pred.eq(target.data.view_as(pred))\n    correct = np.squeeze(correct_tensor.cpu().numpy())\n    if len(target) == 32:\n        for i in range(32):\n            label = target.data[i]\n            class_correct[label] += correct[i].item()\n            class_total[label] += 1\n\ntest_loss = test_loss \/ test_data_len\nprint('Test Loss: {:.4f}'.format(test_loss))\nfor i in range(len(classes)):\n    if class_total[i] > 0:\n        print(\"Test Accuracy of %5s: %2d%% (%2d\/%2d)\" % (\n            classes[i], 100*class_correct[i]\/class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])\n        ))\n    else:\n        print(\"Test accuracy of %5s: NA\" % (classes[i]))\nprint(\"Test Accuracy of %2d%% (%2d\/%2d)\" % (\n            100*np.sum(class_correct)\/np.sum(class_total), np.sum(class_correct), np.sum(class_total)\n        ))","f3a64ac4":"example = torch.rand(1, 3, 224, 224)\ntraced_script_module = torch.jit.trace(student_ft.cpu(), example)\ntraced_script_module.save(\"butterfly_efficientnet_b0.pt\")","92970405":"## Evaluate Student","25e17d30":"## Evaluate Teacher, make sure it is good"}}