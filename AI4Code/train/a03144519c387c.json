{"cell_type":{"880cca1f":"code","1fec0baa":"code","1e140b24":"code","8fea207a":"code","9e97273c":"code","c585c1f8":"code","cdfd9beb":"code","33846af4":"code","339be9a3":"markdown","0043f082":"markdown","4a850db7":"markdown","8488e29e":"markdown","0e79f515":"markdown","f9c7d0d4":"markdown","e893da82":"markdown","394c1a34":"markdown","6096ad46":"markdown","1be23e0c":"markdown","a5d5e4c4":"markdown"},"source":{"880cca1f":"from keras.models import Sequential\nfrom keras.layers import Convolution2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\n\nimport os\nimport calendar\nimport time","1fec0baa":"classifier = Sequential()\n\n# Step 1 - Convolution\n# 3 first parameters\n#       nb_filters = number of filters (feature filter to create feature map) for convolution - tipically starts with 32, and then add another convolution layers with more filters (64, 128, 256...)\n#       nb_rows = rows of filter\n#       nb_columns = columns of filter\n#       input_shape = image size + number of channels (WARNING: for Theano backend use (128, 128, 3)\n#       activation = activation function for the output of current layer\nclassifier.add(Convolution2D(32, 6, 6, input_shape=(64, 64, 3), activation = 'relu'))\n\n# Step 2 - Max Pooling\n#       pool_size = shape of the pooling vector, recommended is 2x2\nclassifier.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Step 1 and 2 one more time - THIS IS DEEP LEARNING!!\nclassifier.add(Convolution2D(64, 6, 6, activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Step 3 - Flattening\nclassifier.add(Flatten())\n\n# Step 4 - Full Connection\n#       units = experimental value, something not so high to not cost much of computation, but not so low to not power the model. Usually is a power of 2. Or, of course is the number of categories we need for output\n#       activation = activation function for the output of current layer\nclassifier.add(Dense(activation = 'relu', units = 128))\nclassifier.add(Dense(activation = 'sigmoid', units = 1))\n\n# Compiling model\n#       optimizer = algorithm used to calculate teh weights, usually is adam\n#       loss = function used to validate the errors and improve the search for the minor errors, binary_crossentropy is for binary outputs, if we have more use categorical_crossentropy\n#       metrics = metrics used to evaluate model during the training, usually accuracy\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","1e140b24":"from keras.preprocessing.image import ImageDataGenerator","8fea207a":"# Consider random small changes in trainning dataset to improve the capacity of generalization of the model, this object is just for prepare the train data\ntrain_datagen = ImageDataGenerator(\n        rescale=1.\/255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True\n)\n\n# Image Augmentation\n# for test is not necessary changes\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\n# Image Augmentation and prepare training data\n# Training set\n#       target_size should be the size of input of first convolution layer\n#       batch_size is the size of packages presented to the model at once within an epoch, the number of batches presented to the model will be total samples divided by batch_size\n#       class_mode define basically if we are working with binary classification or categorical\ntraining_set = train_datagen.flow_from_directory('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/train', target_size=(64, 64), batch_size=32, class_mode='binary')\n\n# Image Augmentation and prepare training data\n# Test set - the parameters are the same as above\ntest_set = test_datagen.flow_from_directory('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/test', target_size=(64, 64), batch_size=32, class_mode='binary')","9e97273c":"# !!!Train the classifier!!!\n#       samples_per_epoch is the total number of samples in the training dataset\n#       nb_val_samples is the total number of samples in the test dataset\n#       nb_epoch is the number of times all samples will be shown to the model for weights adjustments\nclassifier.fit_generator(training_set, samples_per_epoch = 5216, nb_epoch=25, validation_data=test_set, nb_val_samples = 624)","c585c1f8":"# Save\nname_tosave = 'cnn' + str(calendar.timegm(time.gmtime()))\n\n# Save the class indices\nfile1 = open(os.path.join(name_tosave + '.txt'),\"w\")\nfile1.writelines(str(training_set.class_indices)) \nfile1.close() #to change file access modes\n\n# Save the model\nclassifier.save(os.path.join(name_tosave + '.h5'))","cdfd9beb":"import numpy as np\nfrom keras.preprocessing import image\nfrom keras.models import load_model\nimport os","33846af4":"# Initialize variables\nsuccess_normal = 0\nsuccess_pneumonia = 0\nfalse_positive = 0\nfalse_negative = 0\ntotal_samples_normal = 0\ntotal_samples_pneumonia = 0\n\n# get model\nmodel = load_model(name_tosave + '.h5')\n\nprint('')\nprint('******************************PREDICTIONS******************************')\nprint('')\n\n# Predictions for NORMAL x rays\nfor filename in os.listdir('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/val\/NORMAL'):\n    total_samples_normal = total_samples_normal + 1\n\n    # pre processing image\n    test_image = image.load_img('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/val\/NORMAL\/' + filename, target_size=(64, 64))\n    test_image = image.img_to_array(test_image)\n    test_image = np.expand_dims(test_image, axis = 0) # this is necessary because precit method expect 4 dimensions, 3 for image, and one for batch number\n    result = model.predict(test_image)\n\n    # result\n    if result[0][0] == 1:\n        prediction = 'PNEUMONIA'\n        false_positive = false_positive + 1\n    else:\n        prediction = 'NORMAL'\n        success_normal = success_normal + 1\n\n    print('NORMAL: ' + prediction)\n\n# prediction for PNEUMONIA x rays\nfor filename in os.listdir('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/val\/PNEUMONIA'):\n    total_samples_pneumonia = total_samples_pneumonia + 1\n\n    # pre processing image\n    test_image = image.load_img('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/val\/PNEUMONIA\/' + filename, target_size=(64, 64))\n    test_image = image.img_to_array(test_image)\n    test_image = np.expand_dims(test_image, axis = 0) # this is necessary because precit method expect 4 dimensions, 3 for image, and one for batch number\n    result = model.predict(test_image)\n\n    # result\n    if result[0][0] == 1:\n        prediction = 'PNEUMONIA'\n        success_pneumonia = success_pneumonia + 1\n    else:\n        prediction = 'NORMAL'\n        false_negative = false_negative + 1\n\n    print('PNEUMONIA: ' + prediction)\n\ntotal_samples = success_normal + success_pneumonia + false_negative + false_positive\naccuracy = ((success_normal + success_pneumonia) \/ total_samples) * 100\n\n# Summary\nprint('')\nprint('******************************SUMMARY******************************')\nprint('')\nprint('Total samples: ' + str(total_samples))\nprint('Accuracy: ' + str(accuracy) + '%')\nprint('')\nprint('------NORMAL------')\nprint('Total samples NORMAL: ' + str(total_samples_normal))\nprint('Prediction correct NORMAL: ' + str(success_normal))\nprint('False positives: ' + str(false_positive))\nprint('')\nprint('------PNEUMONIA------')\nprint('Total samples PNEUMONIA: ' + str(total_samples_pneumonia))\nprint('Prediction correct PNEUMONIA: ', str(success_pneumonia))\nprint('False negatives: ' + str(false_negative))","339be9a3":"# Part 3 - Predictions","0043f082":"## Image Augmentation","4a850db7":"### Importing the libraries","8488e29e":"## Initializing CNN","0e79f515":"## Train the classifier","f9c7d0d4":"## Imports","e893da82":"## Imports","394c1a34":"## Save the model","6096ad46":"## Predictions","1be23e0c":"# Part 1 - Building CNN","a5d5e4c4":"# Part 2 - Fitting CNN to the images"}}