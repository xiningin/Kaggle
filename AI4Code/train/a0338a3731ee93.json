{"cell_type":{"44af0f8c":"code","f153e240":"code","bbac81a4":"code","cdf6f452":"code","c3100984":"code","3e62bc95":"code","da985e65":"code","53453d74":"code","3f77f28b":"code","6449a8f5":"code","1ff47b79":"code","00c573fc":"code","555c178e":"code","cd21a493":"code","0d036480":"code","6fe859f4":"code","0b4631a4":"code","070ae71d":"code","569bb6f3":"code","68bee3cd":"code","c1e05c59":"code","325fc793":"code","1a0e38b3":"code","4a6a1653":"code","a10c09dc":"code","dc078ea0":"code","cf366687":"code","f7e92877":"code","ab24a284":"code","8df8911f":"code","ff861221":"code","9fd0af8b":"code","e21e79ef":"code","dde08331":"code","b74fda4b":"code","c5b9642e":"code","82929284":"code","68f5b904":"code","39f460ba":"code","41679253":"code","075c072c":"code","e9920b40":"code","608d8dda":"code","8b0d0f03":"code","eb4d4dee":"code","d1042bd7":"code","bde89dd6":"code","97f76aad":"code","88ee9ae1":"code","f8403ef3":"code","b671785f":"code","468cf64b":"code","ed20a17c":"code","ded140b7":"code","154aa962":"code","1fac7dd5":"code","ecc953a1":"code","58cc5230":"code","4bab679d":"code","fe081263":"code","fe4d627d":"code","ba3584c5":"code","3bd7fab3":"code","ea3af728":"code","7c8698ad":"code","ee5ef4e2":"code","bef5c08c":"code","97d3498c":"code","687f6891":"code","7eb8d5cc":"code","a3fe201b":"code","47609357":"code","4bd49d05":"code","73b6a8bf":"code","eed47f69":"code","a5b6d5e5":"code","0160ead6":"code","d3191290":"code","7706fb09":"code","253362c3":"code","0f7e1aae":"code","7d05b352":"code","e86ec011":"code","ee2c5e7c":"code","6e0568b0":"code","0d19f111":"code","ea1021da":"code","cfeccd27":"code","d3fe8f54":"code","a15c11d8":"code","30c37889":"code","04159630":"markdown","a3c79114":"markdown","19f01259":"markdown","51d2e98d":"markdown","6c1703bf":"markdown","c6bdbd07":"markdown","b569381a":"markdown","af0a9de3":"markdown","48243ab1":"markdown","513e86a8":"markdown","89f9102b":"markdown","bd0a5bb3":"markdown","c995ef25":"markdown","86221573":"markdown","e2cc8d22":"markdown","86a88257":"markdown","1802675d":"markdown","eef8ad81":"markdown","efb6af9c":"markdown","3077b442":"markdown","4daf8028":"markdown","1645f8be":"markdown","d34ea1b0":"markdown","010eb398":"markdown","ed789516":"markdown","1f639394":"markdown","59b5980a":"markdown","abed9fba":"markdown","bc78118e":"markdown","600d903e":"markdown","0db2bc35":"markdown","cb439c92":"markdown","18bf4b49":"markdown","f2e84a36":"markdown","288a6f37":"markdown","8af4f886":"markdown","9a0a2da6":"markdown","f41cc3f8":"markdown","7b85f41b":"markdown"},"source":{"44af0f8c":"import warnings\nwarnings.filterwarnings('ignore')","f153e240":"# Importing libraries to read data and visualisation\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Importing libraries for model building and evaluation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport statsmodels.api as sm\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn import metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","bbac81a4":"# Reading the data\nlead= pd.read_csv('..\/input\/leadscore\/Leads.csv')\n","cdf6f452":"lead.info()","c3100984":"lead.head()","3e62bc95":"lead.shape","da985e65":"lead.describe()","53453d74":"# Dropping the variables which are not important or required for model building\n\nlead= lead.drop(['Country','Specialization','Prospect ID','Lead Origin','Lead Source','Do Not Email','Do Not Call','Last Activity','How did you hear about X Education','What matters most to you in choosing a course','Search','Magazine', 'Newspaper Article','X Education Forums','Newspaper','Digital Advertisement','Through Recommendations','Receive More Updates About Our Courses','Tags','Update me on Supply Chain Content','Get updates on DM Content','City','Asymmetrique Activity Index','Asymmetrique Profile Index','I agree to pay the amount through cheque','A free copy of Mastering The Interview'],1)\nlead.shape","3f77f28b":"# Creating dummy variables for categorical variables\n\ndummy= pd.get_dummies(lead[['Lead Profile','What is your current occupation','Lead Quality','Last Notable Activity']], drop_first=True)\n\n# Adding dummy variable to data set\n\nlead= pd.concat([lead, dummy], axis=1)","6449a8f5":"# We have created dummies for the below variables, so we can drop them\n\nlead = lead.drop(['Lead Profile','What is your current occupation','Lead Quality','Last Notable Activity'], 1)","1ff47b79":"# Checking for outliers in the continuous variables\n\nnum_lead = lead[['TotalVisits','Total Time Spent on Website','Page Views Per Visit','Asymmetrique Activity Score','Asymmetrique Profile Score']]\n\n# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%\n\nnum_lead.describe(percentiles=[.25, .5, .75, .90, .95, .99])","00c573fc":"# Removing the outliers from 'Total Visits' and 'Page Vies Per Visit'\n\nlead = lead[~(lead['TotalVisits'] > 17)]\nlead = lead[~(lead['Page Views Per Visit'] > 9)]","555c178e":"#Checking percentage of null values present in data\n\nround((lead.isnull().sum()\/len(lead))*100,2)","cd21a493":"# Removing rows with small NAN data\n\nlead = lead[~np.isnan(lead['TotalVisits'])]\nlead = lead[~np.isnan(lead['Page Views Per Visit'])]","0d036480":"# Removing columns with more than 40% NaN data\n\nlead= lead.drop(['Asymmetrique Activity Score','Asymmetrique Profile Score'],1)","6fe859f4":"#Checking percentage of null values present in data\n\nround((lead.isnull().sum()\/len(lead))*100,2)","0b4631a4":"# Putting feature variable to X\nX = lead.drop(['Converted','Lead Number'], axis=1)\nX.head()","070ae71d":"# Putting response variable to y\ny = lead['Converted']\ny.head()","569bb6f3":"# Splitting the data into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","68bee3cd":"# Using the Standard scaler for scaling the numeric variables\nscaler = StandardScaler()\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\nX_train.head()","c1e05c59":"# Checking the Not_paying customers Rate\n\nNot_paying = (((len(lead['Converted'])- sum(lead['Converted'])))\/len(lead['Converted']))*100\nNot_paying","325fc793":"# Let's see the correlation matrix \nplt.figure(figsize = (30,15))        # Size of the figure\nsns.heatmap(lead.corr(),annot = True, cmap='Greens')\nplt.show()","1a0e38b3":"# Dropping the dummy variables which are highly correlated\n\nX_test = X_test.drop(['Last Notable Activity_Email Opened','Last Notable Activity_Modified','Lead Profile_Select','What is your current occupation_Working Professional'], 1)\nX_train = X_train.drop(['Last Notable Activity_Email Opened','Last Notable Activity_Modified','Lead Profile_Select','What is your current occupation_Working Professional'], 1)","4a6a1653":"# Creating new correlation matrix after dropping the highly correlated variables\n\nplt.figure(figsize = (20,10))\nsns.heatmap(X_train.corr(),annot = True, cmap='Greens')\nplt.show()","a10c09dc":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","dc078ea0":"logreg = LogisticRegression(solver='liblinear')\n\nrfe = RFE(logreg, 15)           # running RFE with 15 variables as output\nrfe = rfe.fit(X_train, y_train)","cf366687":"rfe.support_","f7e92877":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","ab24a284":"col = X_train.columns[rfe.support_]\ncol","8df8911f":"X_train.columns[~rfe.support_]","ff861221":"# Adding constant\nX_train_sm = sm.add_constant(X_train[col])\n\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","9fd0af8b":"# Dropping the column with high P value\ncol = col.drop('Last Notable Activity_Email Received')","e21e79ef":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","dde08331":"# Dropping 'Last Notable Activity_Resubscribed to emails' varible as it high P value\ncol = col.drop('Last Notable Activity_Resubscribed to emails')","b74fda4b":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","c5b9642e":"# Dropping the column due to high P value\ncol = col.drop('Lead Profile_Lateral Student')","82929284":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","68f5b904":"# Dropping the varible as it has high P value of 0.999\ncol = col.drop('What is your current occupation_Housewife')\n\n# Assessing the model\nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","39f460ba":"# Dropping the varible as it has high P value of 0.999\ncol = col.drop('Last Notable Activity_Had a Phone Conversation')\n\n# Assessing the model\nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","41679253":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","075c072c":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","e9920b40":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_Prob':y_train_pred})\ny_train_pred_final['Lead Number'] = y_train.index\ny_train_pred_final.head()","608d8dda":"y_train_pred_final['predicted'] = y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Checking final predicted values\ny_train_pred_final.head()","8b0d0f03":"# Confusion matrix \n\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nprint(confusion)","eb4d4dee":"# Checking the overall accuracy\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","d1042bd7":"# Checking for the VIF values of the feature variables. \n# Also creating a dataframe that will contain the names of all the feature variables and their respective VIFs\n\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","bde89dd6":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","97f76aad":"# Sensitivity \nTP \/ float(TP+FN)","88ee9ae1":"# Specificity\nTN \/ float(TN+FP)","f8403ef3":"# False Postive Rate (FPR)\nprint(FP\/ float(TN+FP))","b671785f":"# Precision (Positive Predictive Value)\nprint (TP \/ float(TP+FP))","468cf64b":"# Negative Predictive Value\nprint (TN \/ float(TN+ FN))","ed20a17c":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","ded140b7":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Converted_Prob, drop_intermediate = False )","154aa962":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)","1fac7dd5":"# Creating columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\n\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","ecc953a1":"# Calculating the accuracy, sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n\n# Importing library for confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","58cc5230":"# Plotting accuracy, sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","4bab679d":"# Taking 0.4 as cut-off probability for final prediction\n\ny_train_pred_final['final_predicted'] = y_train_pred_final.Converted_Prob.map( lambda x: 1 if x > 0.4 else 0)\ny_train_pred_final.head()","fe081263":"# Checking the overall accuracy\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","fe4d627d":"# Looking for confusion matrix\nconfusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion2","ba3584c5":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","3bd7fab3":"# Sensitivity\nTP \/ float(TP+FN)","ea3af728":"# Specificity\nTN \/ float(TN+FP)","7c8698ad":"# False Postive Rate(FPR)\nprint(FP\/ float(TN+FP))","ee5ef4e2":"# Precision (Positive Predictive Value) \nprint (TP \/ float(TP+FP))","bef5c08c":"# Negative Predictive Value\nprint (TN \/ float(TN+ FN))","97d3498c":"X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])","687f6891":"X_test = X_test[col]\nX_test.head()","7eb8d5cc":"# Adding constant\nX_test_sm = sm.add_constant(X_test)","a3fe201b":"y_test_pred = res.predict(X_test_sm)","47609357":"y_test_pred[:10]","4bd49d05":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","73b6a8bf":"# Let's see the head\ny_pred_1.head()","eed47f69":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","a5b6d5e5":"# Putting 'Lead Number' to index\ny_test_df['Lead Number'] = y_test_df.index","0160ead6":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","d3191290":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","7706fb09":"y_pred_final.head()","253362c3":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_Prob'})","0f7e1aae":"# Rearranging the columns\ny_pred_final = y_pred_final.reindex(columns=['Lead Number','Converted','Converted_Prob'])","7d05b352":"# Let's see the head of y_pred_final\ny_pred_final.head()","e86ec011":"y_pred_final['final_predicted'] = y_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.4 else 0)","ee2c5e7c":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted)","6e0568b0":"confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\nconfusion2","0d19f111":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","ea1021da":"# Sensitivity \nTP \/ float(TP+FN)","cfeccd27":"# Specificity\nTN \/ float(TN+FP)","d3fe8f54":"# False Postive Rate(FPR)\nprint(FP\/ float(TN+FP))","a15c11d8":"# Precision (Positive Predictive Value) \nprint (TP \/ float(TP+FP))","30c37889":"# Negative Predictive Value\nprint (TN \/ float(TN+ FN))","04159630":"From above table we can say that there are some outliers present in the 'Total Visits' and 'Page Views Per Visit'. There is big difference between the values at 99% & max value. Other than these two variables 'Total Time Spent on website' has very few outliers.","a3c79114":"## 6. Checking the Correlations","19f01259":"### Checking VIFs","51d2e98d":"The Sensitivity is 78.7% that means number of actual positive values are correctly predicted and we got Specificity of 82.3%. The values should be high for both Sensitivity & Specificity for a good model.  ","6c1703bf":"## 8. Feature Selection Using RFE","c6bdbd07":"## 10. Finding Optimal Cutoff Point","b569381a":"Taking 0.5 as cut off value as it is ideal value.","af0a9de3":"### Confusion Matrix","48243ab1":"An education company named X Education sells online courses to industry professionals. The people who are interested in the courses land on their website and browse for courses. The company markets it's courses on several websites. Once the people check the website either they browse or fill the forms or watch video to know more about the courses.\n\nThe problem here is, there are many people\/leads who visit the website but only few opts for the courses or pay for it. Here, we need to find out promising leads as paying customers.We need to build a model where we can assign a lead score to each of the leads such that the customers with higher lead score have a higher chance conversion and the customers with lower lead score have a lower chances conversion.\n\nTo find out high lead scoring customer we need to go through the activities of leads. How much time do they spend on website? What is their current position? What is their motto for joining the course? etc.\n\n##### Objective:\n\nOur aim is to build the model to find out most promising leads with high chace of conversion. This will help company to focus on the potential leads. ","513e86a8":"## 4. Test-Train Split","89f9102b":"With cutoff of 0.5 i.e ideal cutoff we get the sensitivity of 65.4% which not so good for model as Sensitivity should be high for good model. But Specificity is high for the same.","bd0a5bb3":"## Calculating Sensitivity, Specificity, FPR & Precision","c995ef25":"## 9. Plotting the ROC Curve","86221573":"### Checking for Outliers","e2cc8d22":"#### From the curve above, 0.4 is the optimum point to take it as a cutoff probability.","86a88257":"## Problem Statement\n\n","1802675d":"We have almost 62% Not Paying customers","eef8ad81":"### Dropping the repeated variables","efb6af9c":"## 7. Model Building","3077b442":"## 5. Feature Scaling","4daf8028":"ROC curve which shows the performance of the classification model. For our model ROC curve(AUC) is 0.87 which is closer to 1 which means our model is working satisfactorily.","1645f8be":"Precision for test set is same as of train set i.e 73%","d34ea1b0":"# Lead Scoring Case Study","010eb398":"Above table shows all the variables have low VIFs","ed789516":"As Precision is a probability that a predicted positive(yes) is actually positive(yes). We have precision of 73% so, we can say that there are 73% of chance for lead conversion.","1f639394":"#### Creating new column 'predicted' with 1 if Converted_Prob > 0.5 else 0","59b5980a":"### For categorical variables with multiple levels, creating the dummy variables","abed9fba":"![](https:\/\/saaslist.com\/uploads\/lead-scoring-model-1.jpg)","bc78118e":"- We will be using Sensitivity - Specificity metrics for finding the optimal cutoff for model.\n- Optimal cutoff probability is that prob where we get balanced sensitivity and specificity.\n","600d903e":"Predicted     not_Converted    Converted\nActual\nnot_Converted        3449      433\nConverted            825       1560  ","0db2bc35":"## 3. Data Preparation","cb439c92":"## 11. Making predictions on the test set","18bf4b49":"## 2. Inspecting the Dataframe","f2e84a36":"These are the final variables which helps to find who contributes towards lead conversion. As we can see dummy variables like 'Lead Profile_Potential Lead' and 'Last Notable Activity_SMS Sent' and variable 'TotalTime Spent on Website' which helps us to focus on which lead should be targeted. ","288a6f37":"With a Roc curve of 0.87, We got 78% of Sensitivity & 82.5% of Specificity for the Test set which is almost the same as to train set. So we can say that model built is a good model and works satisfactorily.","8af4f886":"## 1. Importing the Libraries and Reading the Data","9a0a2da6":"### Assessing the model with StatsModels","f41cc3f8":"#### Making predictions on the test set","7b85f41b":"### Creating a dataframe with the actual converted flag and the predicted probabilities"}}