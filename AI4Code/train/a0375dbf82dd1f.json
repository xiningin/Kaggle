{"cell_type":{"7c134929":"code","ff257335":"code","7523c576":"code","714b9bd9":"code","537a690d":"code","97f23310":"code","e65a3ded":"code","a096b967":"code","6d7c41d3":"code","adffa311":"code","c09965ef":"code","08ff56fc":"code","992b24b9":"code","d9790f53":"code","3ec5b139":"code","61d0364c":"code","2743c4b5":"code","2b6b33d6":"code","1a3ef85d":"code","84f26ab3":"code","39b1c7a5":"code","149a79e9":"code","d4f36826":"code","4b21552c":"code","9b4d2cef":"code","a407a399":"code","3cd73cfc":"code","ee50a6b2":"code","53470f1e":"code","5afee357":"code","9bb2a276":"code","1b7ee754":"code","c11590f3":"code","9fef802e":"code","7c579a95":"code","6dbdbed3":"code","c3e66505":"code","eec60bde":"code","ad05f9c4":"markdown","b2c4e79e":"markdown","f807e47c":"markdown","2070f527":"markdown","c6690f05":"markdown","6f172801":"markdown","0ba0dff9":"markdown","a212a232":"markdown","051186e8":"markdown","bac67ecf":"markdown","143fae58":"markdown","f5089ae1":"markdown","2c7fe203":"markdown","fbde6942":"markdown","b435c229":"markdown","0312b66a":"markdown","8bbe57c6":"markdown","f91924db":"markdown","5a825066":"markdown","ecbf9082":"markdown","e11688a1":"markdown","78e929e3":"markdown","d14b663f":"markdown","e40ba853":"markdown","c3e78572":"markdown","34a3e8b5":"markdown","a8352db7":"markdown","dc3dd777":"markdown","145de91e":"markdown","8a6f0cb4":"markdown","f0989a14":"markdown"},"source":{"7c134929":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import *\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nimport time\nimport pickle\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff257335":"train = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ncats = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nsample_submission = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')","7523c576":"# Sales table view\ntrain.head()","714b9bd9":"# Revision of missing values\ntrain.isnull().sum()","537a690d":"# Sales graph per month\nsale_by_month = train.groupby('date_block_num')['item_cnt_day'].sum()\nsale_by_month.plot()","97f23310":"#Chart of quantity of items sold\nplt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nflierprops = dict(marker='o', markerfacecolor='red', markersize=6,\n                  linestyle='none', markeredgecolor='black')\nsns.boxplot(x=train.item_cnt_day, flierprops=flierprops)\n\n#Chart of item prices\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price, flierprops=flierprops)","e65a3ded":"#Removal of prices over 100.000\ntrain = train[train.item_price<100000]\n#Removal of items over 1.001\ntrain = train[train.item_cnt_day<1001]","a096b967":"median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0, 'item_price'] = median","6d7c41d3":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","adffa311":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\ncats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n\n# if subtype is nan then type\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]\n\nitems.drop(['item_name'], axis=1, inplace=True)","c09965ef":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time() - ts","08ff56fc":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","992b24b9":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float32))\ntime.time() - ts","d9790f53":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","3ec5b139":"ts = time.time()\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) \ntime.time() - ts","61d0364c":"ts = time.time()\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\ntime.time() - ts","2743c4b5":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","2b6b33d6":"ts = time.time()\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\ntime.time() - ts","1a3ef85d":"def add_group_stats(matrix_, groupby_feats, target, enc_feat, last_periods):\n    if not 'date_block_num' in groupby_feats:\n        print ('date_block_num must in groupby_feats')\n        return matrix_\n    \n    group = matrix_.groupby(groupby_feats)[target].sum().reset_index()\n    max_lags = np.max(last_periods)\n    for i in range(1,max_lags+1):\n        shifted = group[groupby_feats+[target]].copy(deep=True)\n        shifted['date_block_num'] += i\n        shifted.rename({target:target+'_lag_'+str(i)},axis=1,inplace=True)\n        group = group.merge(shifted, on=groupby_feats, how='left')\n    group.fillna(0,inplace=True)\n    for period in last_periods:\n        lag_feats = [target+'_lag_'+str(lag) for lag in np.arange(1,period+1)]\n        # we do not use mean and svd directly because we want to include months with sales = 0\n        mean = group[lag_feats].sum(axis=1)\/float(period)\n        mean2 = (group[lag_feats]**2).sum(axis=1)\/float(period)\n        group[enc_feat+'_avg_sale_last_'+str(period)] = mean\n        group[enc_feat+'_std_sale_last_'+str(period)] = (mean2 - mean**2).apply(np.sqrt)\n        group[enc_feat+'_std_sale_last_'+str(period)].replace(np.inf,0,inplace=True)\n        # divide by mean, this scales the features for NN\n        group[enc_feat+'_avg_sale_last_'+str(period)] \/= group[enc_feat+'_avg_sale_last_'+str(period)].mean()\n        group[enc_feat+'_std_sale_last_'+str(period)] \/= group[enc_feat+'_std_sale_last_'+str(period)].mean()\n    cols = groupby_feats + [f_ for f_ in group.columns.values if f_.find('_sale_last_')>=0]\n    matrix = matrix_.merge(group[cols], on=groupby_feats, how='left')\n    return matrix","84f26ab3":"ts = time.time()\nmatrix = add_group_stats(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'item', [6,12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'shop', [6,12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'category', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'city', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'type_code'], 'item_cnt_month', 'type', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'subtype_code'], 'item_cnt_month', 'subtype', [12])\ntime.time() - ts","39b1c7a5":"# First use the target coding of each group, then change the month to create lags functions\ndef target_encoding(matrix_, groupby_feats, target, enc_feat, lags):\n    print ('target encoding for',groupby_feats)\n    group = matrix_.groupby(groupby_feats).agg({target:'mean'})\n    group.columns = [enc_feat]\n    group.reset_index(inplace=True)\n    matrix = matrix_.merge(group, on=groupby_feats, how='left')\n    matrix[enc_feat] = matrix[enc_feat].astype(np.float16)\n    matrix = lag_feature(matrix, lags, enc_feat)\n    matrix.drop(enc_feat, axis=1, inplace=True)\n    return matrix","149a79e9":"ts = time.time()\nmatrix = target_encoding(matrix, ['date_block_num'], 'item_cnt_month', 'date_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'date_item_avg_item_cnt', [1,2,3,6,12])\nmatrix = target_encoding(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'date_shop_avg_item_cnt', [1,2,3,6,12])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'date_cat_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'shop_id', 'item_category_id'], 'item_cnt_month', 'date_shop_cat_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'date_city_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_id', 'city_code'], 'item_cnt_month', 'date_item_city_avg_item_cnt', [1])\ntime.time() - ts","d4f36826":"ts = time.time()\ngroup = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) \/ matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\n\ntime.time() - ts","4b21552c":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\ntime.time() - ts","9b4d2cef":"matrix['month'] = matrix['date_block_num'] % 12\nmatrix['year'] = (matrix['date_block_num'] \/ 12).astype(np.int8)","a407a399":"#Month since the last sale of each pair of shops\/article.\nts = time.time()\nlast_sale = pd.DataFrame()\nfor month in range(1,35):    \n    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby(['item_id','shop_id'])['date_block_num'].max()\n    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n                       'item_id': last_month.index.get_level_values(0).values,\n                       'shop_id': last_month.index.get_level_values(1).values,\n                       'item_shop_last_sale': last_month.values})\n    last_sale = last_sale.append(df)\nlast_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n\nmatrix = matrix.merge(last_sale, on=['date_block_num','item_id','shop_id'], how='left')\ntime.time() - ts","3cd73cfc":"#Month since the last sale of each item.\nts = time.time()\nlast_sale = pd.DataFrame()\nfor month in range(1,35):    \n    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby('item_id')['date_block_num'].max()\n    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n                       'item_id': last_month.index.values,\n                       'item_last_sale': last_month.values})\n    last_sale = last_sale.append(df)\nlast_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n\nmatrix = matrix.merge(last_sale, on=['date_block_num','item_id'], how='left')\ntime.time() - ts","ee50a6b2":"# Months from the first sale for each pair of shops\/item and for item only.\nts = time.time()\nmatrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts","53470f1e":"matrix = matrix[matrix.date_block_num > 11]\nmatrix.columns","5afee357":"data = matrix[[\n    'date_block_num',\n    'shop_id',\n    #'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code','subtype_code',\n    'item_cnt_month_lag_1','item_cnt_month_lag_2','item_cnt_month_lag_3','item_cnt_month_lag_6','item_cnt_month_lag_12',\n    'item_avg_sale_last_6', 'item_std_sale_last_6',\n    'item_avg_sale_last_12', 'item_std_sale_last_12',\n    'shop_avg_sale_last_6', 'shop_std_sale_last_6',\n    'shop_avg_sale_last_12', 'shop_std_sale_last_12',\n    'category_avg_sale_last_12', 'category_std_sale_last_12',\n    'city_avg_sale_last_12', 'city_std_sale_last_12',\n    'type_avg_sale_last_12', 'type_std_sale_last_12',\n    'subtype_avg_sale_last_12', 'subtype_std_sale_last_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1','date_item_avg_item_cnt_lag_2','date_item_avg_item_cnt_lag_3','date_item_avg_item_cnt_lag_6','date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1','date_shop_avg_item_cnt_lag_2','date_shop_avg_item_cnt_lag_3','date_shop_avg_item_cnt_lag_6','date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month','year',\n    'item_shop_last_sale','item_last_sale',\n    'item_shop_first_sale','item_first_sale',\n]]\n\ncat_feats = ['shop_id','city_code','item_category_id','type_code','subtype_code']","9bb2a276":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","1b7ee754":"ts = time.time()\n\nmodel_lgbm = LGBMRegressor(\n    max_depth = 8,\n    n_estimators = 500,\n    colsample_bytree=0.7,\n    min_child_weight = 300,\n    reg_alpha = 0.1,\n    reg_lambda = 1,\n    random_state = 42,\n)\n\nmodel_lgbm.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=10, \n    early_stopping_rounds = 40,\n    categorical_feature = cat_feats) # use LGBM's build-in categroical features.\n\ntime.time() - ts","c11590f3":"Y_pred_lgbm = model_lgbm.predict(X_valid).clip(0, 20)\nY_test_lgbm = model_lgbm.predict(X_test).clip(0, 20)\n\nX_train_lgbm = pd.DataFrame({\n    \"ID\": np.arange(Y_pred_lgbm.shape[0]), \n    \"item_cnt_month\": Y_pred_lgbm\n})\nX_train_lgbm.to_csv('lgb_valid.csv', index=False)\n\nsubmission_lgbm = pd.DataFrame({\n    \"ID\": np.arange(Y_test_lgbm.shape[0]), \n    \"item_cnt_month\": Y_test_lgbm\n})\nsubmission_lgbm.to_csv('submission_lgb.csv', index=False)","9fef802e":"ts = time.time()\n\nmodel_xgb = XGBRegressor(\n    max_depth=7,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    gamma = 0.005,\n    eta=0.1,    \n    seed=42)\n\nmodel_xgb.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=10, \n    early_stopping_rounds = 40,\n    )\n\ntime.time() - ts","7c579a95":"Y_pred_xgb = model_xgb.predict(X_valid).clip(0, 20)\nY_test_xgb = model_xgb.predict(X_test).clip(0, 20)\n\nX_train_xgb = pd.DataFrame({\n    \"ID\": np.arange(Y_pred_xgb.shape[0]), \n    \"item_cnt_month\": Y_pred_xgb\n})\nX_train_xgb.to_csv('xgb_valid.csv', index=False)\n\nsubmission_xgb = pd.DataFrame({\n    \"ID\": np.arange(Y_test_xgb.shape[0]), \n    \"item_cnt_month\": Y_test_xgb\n})\nsubmission_xgb.to_csv('submission_xgb.csv', index=False)","6dbdbed3":"pkl_filename = \"pickle_model_LGBM.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(model_lgbm, file)","c3e66505":"pkl_filename = \"pickle_model_XGB.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(model_xgb, file)","eec60bde":"pkl_filename = \"data.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(data, file)","ad05f9c4":"**Trend features**\n\nPrice trend for the last six months.","b2c4e79e":"**Traget lags**","f807e47c":"* ### XGBoost","2070f527":"### Remove outliers and duplicated data","c6690f05":"**Monthly sales**\n\nThe train data set is extended with the pairs of items and stores with zero monthly sales. To make it more similar to the test data set\n","6f172801":"**Shops\/Items\/Cats features**","0ba0dff9":"## Load Data","a212a232":"**Group sale stats in recent**","051186e8":"The most relevant outliers are eliminated. In this case the articles that were sold more than 1001 in one day and the articles with prices over 100.000.","bac67ecf":"It adds to the data set train the pairs shops\/item and then shortens them in a range of (0 to 20). So that the train target be similar to the test prediction.","143fae58":"\nThe sales dataset has no missing value.","f5089ae1":"**Lag features**","2c7fe203":"Training and test data are adjusted. for duplicate stores.","fbde6942":"**Add month and year**","b435c229":"## Models\n\nIn this section we proceed to create, train and predict the models.","0312b66a":"The item with a price lower than zero will be replaced by a value calculated by the median.","8bbe57c6":"## Exploratory Data Analysis (EDA)","f91924db":"## Feature engineering and data cleaning","5a825066":"## Import libraries","ecbf9082":"Last month shop revenue trend","e11688a1":"**Add month since the last and first sale**\n","78e929e3":"* ### LightGBM","d14b663f":"In the graph of total sales for each month, we see that sales are decreasing over time and seasonally we see peaks for November.","e40ba853":"\nFinally, the columns with calculated values that cannot be calculated for the test set are eliminated and the first 12 months are also eliminated, since they were used as lags values","c3e78572":"## **References:**\n\n* https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost\n* https:\/\/www.kaggle.com\/uladzimirkapeika\/feature-engineering-lightgbm-top-1\n* https:\/\/www.kaggle.com\/gordotron85\/future-sales-xgboost-top-3","34a3e8b5":"## Partitioning the data set for training and testing.\n\nThe partition is:\n* 34 months for the test set.\n* 33 months for the validation set.\n* and from 13 to 32 months for the training team.","a8352db7":"# Predict futures sales","dc3dd777":"## **Serialization of the models and data**","145de91e":"## pre-processing in the shops, cats and item data set\n\n* The data sets of the warehouses are reconstructed. Separating the city from the store name, standardizing and coding the records.\n* The same procedure is performed with the category data set. Separating the type and subtype from the category name.","8a6f0cb4":"The Future Sales competition is the final assesment in the 'How to win a Data Science' course in the Advanced Machine Learning specialisation from HSE University, Moscow. The aim is to predict the monthly sales of items in specific shops, given historical data. The sale counts are clipped between 0 and 20.","f0989a14":"**Test set**"}}