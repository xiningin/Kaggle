{"cell_type":{"d423608a":"code","37a38fab":"code","85f0252d":"code","62de8a03":"code","5c946953":"code","b14b8927":"code","4c2b8417":"code","4ea4c1ea":"code","8c19171b":"code","e216fd12":"markdown","0607d4a9":"markdown","ae26d79b":"markdown","385a5a4d":"markdown","7324b926":"markdown","f4a160d6":"markdown","8afd87f4":"markdown","9a93d7e6":"markdown","18a84025":"markdown","06f26937":"markdown","fb419a6a":"markdown","e3c8e99c":"markdown","13be1dec":"markdown","db5da322":"markdown"},"source":{"d423608a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","37a38fab":"%matplotlib notebook\nimport pandas as pd\nimport seaborn as sn\nimport numpy as np\nfrom sklearn.neural_network import MLPClassifier   #Multi-Layer Perceptron Classifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nimport matplotlib.pyplot as plt     \nfrom prettytable import from_csv    #To draw tables\n\ndataset=pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\n\nprint(dataset.head(5))     #prints first 5 values for all columns\n\narray=dataset.values\ndata=array[:,0:13]\nlabels=array[:,13]\n#model1=MLPClassifier(activation='relu',solver='lbfgs',alpha=1e-5,random_state=1,) #doesnot work with neural networks\nmodel2=SVC(kernel='linear')    \nmodel3=LogisticRegression()","85f0252d":"from sklearn.feature_selection import RFE\n#rfe1=RFE(model1,6)  \nrfe2=RFE(model2,6)    #here we wish to keep 6 most important features\nrfe3=RFE(model3,6)","62de8a03":"x_train,x_test,y_train,y_test=train_test_split(data,labels,test_size=0.2,random_state=4)","5c946953":"#k1=rfe1.fit(data,labels)\nk2=rfe2.fit(x_train,y_train)\nprint(\"Num Features: \",k2.n_features_)\nprint(\"Selected Features: \",k2.support_)\nprint(\"Feature Ranking: \",k2.ranking_)\n","b14b8927":"k3=rfe3.fit(x_train,y_train)\nprint(\"Num Features: \",k3.n_features_)\nprint(\"Selected Features: \",k3.support_)\nprint(\"Feature Ranking: \",k3.ranking_)","4c2b8417":"p2=k2.predict(x_test)\nprint(\"Accuracy of the SVC model on unseen data is \"+str(accuracy_score(p2,y_test)*100)+\" %\")","4ea4c1ea":"p3=k3.predict(x_test)\nprint(\"Accuracy of the SVC model on unseen data is \"+str(accuracy_score(p3,y_test)*100)+\" %\")","8c19171b":"cm1=confusion_matrix(y_test,p2)\ncm2=confusion_matrix(y_test,p3)\n\nplt.figure()\nplt.title(\"Confusion Matrix for SVC model after RFE\")\ndf_cm = pd.DataFrame(cm1, range(2),range(2))\nsn.set(font_scale=1.4)     #for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}) # font size\n\nplt.figure()\nplt.title(\"Confusion Matrix for Logistic Regression model after RFE\")\ndf_cm = pd.DataFrame(cm2, range(2),range(2))\nsn.set(font_scale=1.4)     #for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}) # font size","e216fd12":"As we can see the accuracy of the SVC model has imporved. \n\nThe manually selected features with the same linear kernel gave SVC a test data accuracy of 81.9672131147541 %.","0607d4a9":"Here since we have reduced the feature from the original dataset, we have to fit the rfe values not to the main model.\n\nFirst, we will try for the SVC classifier.","ae26d79b":"Here as the manually selected features code we import all the required dependencies and also load the dataset. \nBut unlike previously i'm not manually selecting the features instead i allow the Recursive Feature Elimination(RFE) algorithm to eliminate the least important features and keep the rest to train the model.\n\nThe RFE algorithm only works with SVC with linear kernel and works all the time for support vector regression(SVR).\n","385a5a4d":"Here, for the Logistic regression model, the selected features are in the index position 1,2,8,9,10,11\n\nThe corresponding data being \"sex\",\"cp\",\"exang\",\"oldpeak\",\"slope\",\"ca\".\n","7324b926":"Do compare it with the confusion matrices of SVC and Logistic Regression without RFE.","f4a160d6":"## Accuracy for test data","8afd87f4":"The Recursive Feature Elimination(RFE) algorithm is implemented using the sklearn in-built method for RFE :- RFE(model,number of features required)\n\n\nThe RFE method does not seem to work with the neural network model, so i eliminnate the MLPClassifer model.","9a93d7e6":"## Feature reduction using RFE","18a84025":"## Dependencies and Data","06f26937":"Here we can see the features with 'True' value in 'Selected Features' or have '1' in corresponding index place are the 6 most preffered features for SVC model.\n\nHere the positions(considering first index value to be 0) 1,2,8,9,11,12 are said to be the most important features.\n\nThe corresponding data are \"sex\",\"cp\",\"exang\",\"oldpeak\",\"ca\",\"thal\".\n\nThese data are said to be give the best results for our data classification.","fb419a6a":"## Confusion matrix for SVC and Logisitic Regression after RFE","e3c8e99c":"Here we can see the accuracy of the Logistic regression model has reduced after using the feature elimination technique.\n\nThe accuracy with manually selected features was 90.1639344262295 %.\n\nThis means that feature selection RFE algorithm works best on SVC of the three selected algorithms( MLP,SVC and Logistic Regression) having said that RFE technique doesnot work on MLP. ","13be1dec":"## Fitting the data and finding the ranked features","db5da322":"## Splitting the data into train data and test data"}}