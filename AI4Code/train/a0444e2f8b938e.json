{"cell_type":{"f4f2949f":"code","1568ebaa":"code","81c743c1":"code","bb08a32a":"code","2cfee7b5":"code","0c790549":"code","7ea8e73f":"code","b39b832d":"code","b3be7ca9":"code","5d19be79":"code","3e9f3e7c":"code","e7ba69da":"code","1abb8ebf":"code","ee3f5ef5":"markdown","04553e5e":"markdown","702d0d59":"markdown","780f8f8a":"markdown","510071bf":"markdown","960473d5":"markdown","d8d42bb3":"markdown","9715abad":"markdown"},"source":{"f4f2949f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score","1568ebaa":"df = pd.read_csv(\"..\/input\/creditcard.csv\")\ndf.head()","81c743c1":"df['Class'].value_counts()","bb08a32a":"y = df['Class']\ny = np.array(y).astype(np.float)\nX = df.drop(['Class'], axis=1)\nX = np.array(X).astype(np.float)","2cfee7b5":"natural_acc = (1 - y.sum()\/len(y)) * 100\nprint('Anything with an accuracy below %.4f would be useless' % natural_acc)","0c790549":"def plot_data(X, y):\n    plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n    plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='r')\n    plt.legend()\n    return plt.show()","7ea8e73f":"plot_data(X, y)","b39b832d":"method = SMOTE(kind='regular')\nX_resampled, y_resampled = method.fit_sample(X, y)","b3be7ca9":"plot_data(X_resampled, y_resampled)","5d19be79":"new_ratio = (1 - y_resampled.sum()\/len(y_resampled)) * 100\nprint(new_ratio)","3e9f3e7c":"X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=0)\n\nmodel = RandomForestClassifier(random_state=5, \n                               class_weight='balanced_subsample', criterion= 'entropy') \n# weights are calculated with each iteration of growing a tree in the forest\n\nmodel.fit(X_train, y_train) # the resampled data are used for training only, not for testing\npredicted = model.predict(X) ","e7ba69da":"print(accuracy_score(y, predicted) * 100)\nprint(confusion_matrix(y, predicted))","1abb8ebf":"probabilities = model.predict_proba(X)\n\nprint(roc_auc_score(y, probabilities[:,1]) * 100)\nprint(classification_report(y, predicted))","ee3f5ef5":"## Model ","04553e5e":"Dataset from https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n\nThe goal is to train a model capable of spotting fraud cases. ","702d0d59":"This is a **work in progress**, the model will be tuned using GridSearchCV. ","780f8f8a":"* The dataset is particularly interesting because of the unbalance of variability of the target variable Class. ","510071bf":"The imbalance between the two classes will be a problem when training the model, to adjust the balance it could be possible to oversample the minority class, but then the model would be trained on a lot of duplicates, which is second best or undersample the majority class, which would mean to throw away data. SMOTE (Synthetic Minority Oversampling Technique) uses characteristics of nearest neighbor to create synthetic fraud cases \n","960473d5":"### Variables \n\n- TimeNumber of seconds elapsed between this transaction and the first transaction in the dataset\n- V1 to V27 may be result of a PCA Dimensionality reduction to protect user identities and sensitive features\n- AmountTransaction amount\n- Class1 for fraudulent transactions, 0 otherwise","d8d42bb3":"### Natural accuracy\nThe natural accuracy is the baseline, it describes how often one would be correct if it was always assumed that there was no fraud.","9715abad":"Assuming that false alarms (false positives) are more acceptable for a credit card company than unspotted cases of fraud, it is important to maximize recall over precision. "}}