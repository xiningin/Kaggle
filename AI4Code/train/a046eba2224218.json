{"cell_type":{"c06b0ae6":"code","82df4d87":"code","b4237cc0":"code","a5704837":"code","028541cc":"code","50a2065f":"code","e96aaf71":"code","f69baa0b":"code","3174156b":"code","f7a6707e":"code","452e4d1a":"code","13dc6411":"code","fb9fb9b1":"code","3632e58f":"code","31a85739":"code","e2eb00fd":"code","d1dd3f69":"code","80ff0ee8":"code","d495e2ea":"code","d3c04fe1":"code","722678c1":"code","c4473b4b":"code","f0b59ff0":"code","30fa9208":"code","60594896":"code","55546189":"code","5da12f60":"code","7c5a04c3":"code","d27d5610":"code","3c164aa3":"code","78efdac5":"code","a2e2d3be":"code","126a85e6":"code","7a8f71fa":"code","34f6bd0f":"code","2311a032":"code","df97c08a":"code","41bb16df":"code","28575670":"code","0cdf9b9a":"code","fc3e3072":"code","3fe7ed1d":"code","21b650b9":"code","8ed89712":"code","24e81b33":"code","aa9c15a1":"code","07d8d95a":"code","e109a742":"code","7b1a800a":"code","b71c3bd9":"code","13f0250e":"code","ac715820":"code","fc86d872":"code","33786352":"code","cde0be8a":"code","ac8eb074":"code","f2c2e94f":"code","5b477062":"code","3950b1f9":"code","664c638d":"code","da84d6f3":"code","4755285a":"code","de52e3ce":"code","6679fa48":"code","7c276a57":"code","5b32a393":"code","28f9f02f":"code","5056758e":"code","5e113b0b":"code","ae405e08":"code","773ad256":"markdown","eebc7096":"markdown","34f8cdea":"markdown","4bb88a89":"markdown","20ecff7f":"markdown","c87eb2c6":"markdown","e9e3333a":"markdown","77c875b5":"markdown","b28468e2":"markdown","2086c11b":"markdown","a39ff915":"markdown","03d821c7":"markdown","b553414c":"markdown","77dd094a":"markdown","95099d1b":"markdown","d8e40901":"markdown","8c1437b1":"markdown","eba2e532":"markdown","51636ac6":"markdown","0450c5d8":"markdown","65f18644":"markdown","0f6cb673":"markdown","aeb711c8":"markdown","90495dce":"markdown","f30feeed":"markdown","d8a2020c":"markdown","208088ba":"markdown","78362559":"markdown","f6902336":"markdown","cd3de54e":"markdown","fd38638e":"markdown","95ecd296":"markdown","178ebb40":"markdown","d05eb7ce":"markdown","2ab1a885":"markdown","5b253ec5":"markdown","6c007cb6":"markdown","b464041c":"markdown","33af325d":"markdown","79d86526":"markdown","081565be":"markdown","d21a8b00":"markdown"},"source":{"c06b0ae6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport random\nimport numpy as np     # linear algebra\nimport pandas as pd    # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82df4d87":"gender_data = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\n\nprint('Setup Complete')","b4237cc0":"pd.set_option('display.max_rows', 5)\npd.set_option('display.max_columns', None)\n\ngender_data","a5704837":"train_data","028541cc":"test_data","50a2065f":"pd.set_option('display.max_rows', None)","e96aaf71":"print(train_data.info(verbose=True),'\\n','-'*50, '\\n','-'*50)\ntrain_desc = pd.DataFrame()\ntrain_desc['isna'] = train_data.isna().sum()\ntrain_desc['nunique'] = train_data.nunique()\n\nprint(train_desc)","f69baa0b":"train_data.describe()","3174156b":"print(test_data.info(verbose=True),'\\n','-'*50, '\\n','-'*50)\ntest_desc = pd.DataFrame()\ntest_desc['isna'] = test_data.isna().sum()\ntest_desc['nunique'] = test_data.nunique()\n\nprint(test_desc)","f7a6707e":"test_data.describe()","452e4d1a":"pd.set_option('display.max_rows', 10)","13dc6411":"def labelvalue(graph, data, rotationticklabels=0):\n    for p in graph.patches:\n        graph.annotate(format(p.get_height()), (p.get_x() + p.get_width() \/ 2, p.get_height()), ha = 'center', \n                       va = 'center', xytext = (0, 5), textcoords = 'offset points')\n        graph.annotate(('{:.2f}%'.format(p.get_height()\/(data.value_counts().sum())*100)), \n                       (p.get_x() + p.get_width() \/ 2, p.get_height()), ha = 'center',\n                       va = 'center', xytext = (0, 20), textcoords = 'offset points')\n        graph.set_xticklabels(graph.get_xticklabels(), rotation=rotationticklabels)","fb9fb9b1":"train_data = train_data.drop(columns=['Cabin', 'Ticket'])\ntest_data = test_data.drop(columns=['Cabin', 'Ticket'])\n\n# Only drop NA value in train data, to make predict more accurate\ntrain_data = train_data.dropna() ","3632e58f":"fig, ax = plt.subplots(figsize=(18, 5))\n\ngraph = sns.countplot(x=train_data['Survived'])\nlabelvalue(graph, train_data['Survived'])","31a85739":"train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e2eb00fd":"PclassFig = sns.FacetGrid(train_data, col='Survived', row='Pclass', height=3, aspect=1.5)\nPclassFig.map(plt.hist, 'Age', alpha=0.5, bins=80)\nPclassFig.add_legend()","d1dd3f69":"fig, ax = plt.subplots(figsize=(18, 5))\n\ngraph = sns.countplot(x=train_data['Sex'])\ngraph.set_xticklabels(graph.get_xticklabels())\nlabelvalue(graph, train_data['Sex'])","80ff0ee8":"ssurvived = train_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean()\nssurvived['NotSurvived'] = 1 - ssurvived['Survived']\nssurvived","d495e2ea":"combine = [train_data, test_data]\n\nfor dataset in combine:\n    for interval in range(0, 1):\n        dataset['Sex'] = dataset['Sex'].replace({'male':0, 'female':1}).astype(int)\n\ntrain_data.head()","d3c04fe1":"AgeFig = sns.FacetGrid(train_data, col='Survived', height=3, aspect=2)\nAgeFig.map(plt.hist, 'Age', bins=80) # because the max value of the Age is 80, so it's divided into 80 bars","722678c1":"guess_age = np.zeros((2,3))\n\ncombine = [train_data, test_data]\n\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_data = dataset[(dataset['Sex'] == i) & \\\n                                 (dataset['Pclass'] == j+1)]['Age'].dropna()\n            \n            age_guess = guess_data.median()\n            \n            guess_age[i, j] = int(age_guess\/0.5 + 0.5) * 0.5\n    \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[(dataset.Age.isnull()) & (dataset.Sex == i) & \\\n                        (dataset.Pclass == j+1), 'Age'] = guess_age[i, j]\n    \n    dataset['Age'] = dataset['Age'].astype(int)","c4473b4b":"for dataset in combine:\n    for interval in range(0, 16):\n        dataset.loc[(dataset['Age'] > (interval * 5)) & (dataset['Age'] <= ((interval + 1) * 5)),'Age'] = interval\n        dataset['Age'] = dataset['Age'].astype(int)\n\nAgeBand = train_data[['Age', 'Survived']].groupby(['Age']).mean().sort_values(by = 'Age', ascending = True)\nAgeBand","f0b59ff0":"combine = [train_data, test_data]\n\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_data['Title'], test_data['Sex'])","30fa9208":"train_data['Title'].value_counts()","60594896":"combine = [train_data, test_data]\n\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady','Countess','Capt',\\\n                                                                'Don','Dr','Major','Rev','Col',\\\n                                                                'Sir','Jonkheer','Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace(['Mlle','Ms'], 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme','Mrs')\n\ntrain_data[['Title','Survived']].groupby(['Title'], as_index=False).mean()","55546189":"combine = [train_data, test_data]\n\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace({'Mr':'1', 'Miss':'2', 'Mrs':'3', 'Master':'4', 'Rare':'5'}).astype(int).fillna(0)\n    \ntrain_data.head()","5da12f60":"train_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","7c5a04c3":"combine = [train_data, test_data]\n\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].replace({'S':0, 'C':1, 'Q':2})\n    \ntrain_data.head()","d27d5610":"combine = [train_data, test_data]\n\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_data[['FamilySize', 'Survived']].groupby(['FamilySize']).mean().sort_values(by = 'FamilySize')","3c164aa3":"train_data['IsAlone'] = 0\ntest_data['IsAlone'] = 0\ntrain_data.loc[train_data['FamilySize'] == 1, 'IsAlone'] = 1\ntest_data.loc[test_data['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index = False).mean()","78efdac5":"train_data","a2e2d3be":"pd.set_option('display.max_rows', None)\ntest_data.isna().sum()","126a85e6":"train_data = train_data.drop(['Name', 'PassengerId', 'Fare', 'SibSp', 'Parch', 'FamilySize'], axis=1).reset_index(drop=True)\ntest_data_raw = test_data.drop(['Name', 'Fare', 'SibSp', 'Parch', 'FamilySize'], axis=1).reset_index(drop=True)\ntest_data = test_data.drop(['Name', 'PassengerId', 'Fare', 'SibSp', 'Parch', 'FamilySize'], axis=1).reset_index(drop=True)\ntrain_data = train_data.astype(int)\ntest_data = test_data.astype(int)\n\ntrain_data.shape, test_data.shape","7a8f71fa":"plt.figure(figsize=(10,10))\nsns.heatmap(train_data.corr(), annot=True, fmt='.2f', cmap='coolwarm')\nplt.show()","34f6bd0f":"low_cardinality_cols = ['Pclass', 'Embarked', 'Title']\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nNew_Columns = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_S', 'Embarked_C', 'Embarked_Q',\n               'Title_Mr', 'Title_Miss', 'Title_Mrs', 'Title_Master', 'Title_Rare']\n\nnew_data_train = pd.DataFrame(OH_encoder.fit_transform(train_data[low_cardinality_cols]))\nnew_data_train.columns = New_Columns\ntrain_data = pd.concat([train_data, new_data_train], axis=1)\ntrain_data = train_data.drop(['Pclass', 'Embarked', 'Title'], axis=1)\n\nnew_data_test = pd.DataFrame(OH_encoder.fit_transform(test_data[low_cardinality_cols]))\nnew_data_test.columns = New_Columns\ntest_data = pd.concat([test_data, new_data_test], axis=1)\ntest_data = test_data.drop(['Pclass', 'Embarked', 'Title'], axis=1)\n\ntrain_data.head()","2311a032":"train_data.shape","df97c08a":"X_train = train_data.drop('Survived', axis = 1)\nY_train = train_data['Survived']\nX_test = test_data\n\nprint(X_train.shape, Y_train.shape, X_test.shape)","41bb16df":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\nprint(X_train,\"\\n\\n=======================================\\n\\n\", X_test)","28575670":"def set_seed(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n\nSEED = 25\nset_seed(SEED)","0cdf9b9a":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred_logreg_ori = logreg.predict(X_test)\n\nacc_logreg_ori = round(logreg.score(X_train, Y_train), 4)\nprint(\"acc logreg =\", acc_logreg_ori)","fc3e3072":"svm = SVC()\nsvm.fit(X_train, Y_train)\nY_pred_svm_ori = svm.predict(X_test)\n\nacc_svm_ori = round(svm.score(X_train, Y_train), 4)\nprint(\"acc svm =\", acc_svm_ori)","3fe7ed1d":"knn = KNeighborsClassifier()\nknn.fit(X_train, Y_train)\nY_pred_knn_ori = knn.predict(X_test)\n\nacc_knn_ori = round(knn.score(X_train, Y_train), 4)\nprint(\"acc knn =\", acc_knn_ori)","21b650b9":"parameters = {'n_neighbors':[15, 17, 19], 'weights':['uniform', 'distance'],\n              'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'], 'leaf_size':[250, 275, 300]}\nknn = KNeighborsClassifier()\nknn = GridSearchCV(knn, parameters, cv=5, scoring='accuracy')\nknn.fit(X_train, Y_train)\nY_pred_knn_best = knn.predict(X_test)\n\nacc_knn = round(knn.score(X_train, Y_train), 4)\nprint(\"acc knn =\", acc_knn)\n\nprint('-----')\nprint(f'Best parameters {knn.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{knn.best_score_:.3f}'\n)\nbest_score_knn = knn.best_score_\nprint('-----')","8ed89712":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred_gaus_ori = gaussian.predict(X_test)\n\nacc_gaussian_ori = round(gaussian.score(X_train, Y_train), 4)\nprint(\"acc gaussian =\", acc_gaussian_ori)","24e81b33":"perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred_per_ori = perceptron.predict(X_test)\n\nacc_perceptron_ori = round(perceptron.score(X_train, Y_train), 4)\nprint(\"acc perceptron =\", acc_perceptron_ori)","aa9c15a1":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred_svc_ori = linear_svc.predict(X_test)\n\nacc_linear_svc_ori = round(linear_svc.score(X_train, Y_train), 4)\nprint(\"acc linear_svc =\", acc_linear_svc_ori)","07d8d95a":"sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred_sgd_ori = sgd.predict(X_test)\n\nacc_sgd_ori = round(sgd.score(X_train, Y_train), 4)\nprint(\"acc sgd =\", acc_sgd_ori)","e109a742":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred_dectree_ori = decision_tree.predict(X_test)\n\nacc_decision_tree_ori = round(decision_tree.score(X_train, Y_train), 4)\nprint(\"acc decision_tree =\", acc_decision_tree_ori)","7b1a800a":"parameters = {'max_depth':[15, 17, 19, 21], 'max_features':['auto', 'sqrt'],\n              'criterion':['gini', 'entropy']}\ndecision_tree = DecisionTreeClassifier(random_state=SEED)\ndecision_tree = GridSearchCV(decision_tree, parameters, cv=5, scoring='accuracy')\ndecision_tree.fit(X_train, Y_train)\nY_pred_dectree_best = decision_tree.predict(X_test)\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train), 4)\nprint(\"acc decision_tree =\", acc_decision_tree)\n\nprint('-----')\nprint(f'Best parameters {decision_tree.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{decision_tree.best_score_:.3f}'\n)\nbest_score_decision_tree = decision_tree.best_score_\nprint('-----')","b71c3bd9":"random_forest = RandomForestClassifier()\nrandom_forest.fit(X_train, Y_train)\nY_pred_rf_ori = random_forest.predict(X_test)\n\nacc_random_forest_ori = round(random_forest.score(X_train, Y_train), 4)\nprint(\"acc random_forest =\", acc_random_forest_ori)","13f0250e":"parameters = {'max_depth':[11, 13, 15], 'n_estimators':[300, 500],\n              'max_features':['auto', 'sqrt'], 'criterion':['gini', 'entropy']}\nrandom_forest = RandomForestClassifier(random_state=SEED)\nrandom_forest = GridSearchCV(random_forest, parameters, cv=5, scoring='accuracy')\nrandom_forest.fit(X_train, Y_train)\nY_pred_rf_best = random_forest.predict(X_test)\n\nacc_random_forest = round(random_forest.score(X_train, Y_train), 4)\nprint(\"acc random_forest =\", acc_random_forest)\n\nprint('-----')\nprint(f'Best parameters {random_forest.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{random_forest.best_score_:.3f}'\n)\nbest_score_random_forest = random_forest.best_score_\nprint('-----')","ac715820":"import xgboost\n\nxgboost = xgboost.XGBClassifier()\nxgboost.fit(X_train, Y_train)\nY_pred_xgb_ori = xgboost.predict(X_test)\n\nacc_xgboost_ori = round(xgboost.score(X_train, Y_train), 4)\nprint(\"acc xgboost =\", acc_xgboost_ori)","fc86d872":"import xgboost\n\nparameters = {'max_depth':[15, 17, 19], 'n_estimators':[250, 300, 500, 750, 1000],\n              'learning_rate':[0.01]}\nxgboost = xgboost.XGBClassifier(random_state=SEED)\nxgboost = GridSearchCV(xgboost, parameters, cv=5, scoring='accuracy')\nxgboost.fit(X_train, Y_train)\nY_pred_xgb_best = xgboost.predict(X_test)\n\nacc_xgboost = round(xgboost.score(X_train, Y_train), 4)\nprint(\"acc xgboost =\", acc_xgboost)\n\nprint('-----')\nprint(f'Best parameters {xgboost.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{xgboost.best_score_:.3f}'\n)\nbest_score_xgboost = xgboost.best_score_\nprint('-----')","33786352":"adaboost = AdaBoostClassifier()\nadaboost.fit(X_train, Y_train)\nY_pred_ada_ori = adaboost.predict(X_test)\n\nacc_adaboost_ori = round(adaboost.score(X_train, Y_train), 4)\nprint(\"acc adaboost =\", acc_adaboost_ori)","cde0be8a":"parameters = {'n_estimators':[300, 400, 500, 750, 1000],\n              'learning_rate':[0.01]}\nadaboost = AdaBoostClassifier(random_state=SEED)\nadaboost = GridSearchCV(adaboost, parameters, cv=5, scoring='accuracy')\nadaboost.fit(X_train, Y_train)\nY_pred_ada_best = adaboost.predict(X_test)\n\nacc_adaboost = round(adaboost.score(X_train, Y_train), 4)\nprint(\"acc adaboost =\", acc_adaboost)\n\nprint('-----')\nprint(f'Best parameters {adaboost.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{adaboost.best_score_:.3f}'\n)\nbest_score_adaboost = adaboost.best_score_\nprint('-----')","ac8eb074":"import catboost\n\ncatboost = catboost.CatBoostClassifier(verbose=False)\ncatboost.fit(X_train, Y_train)\nY_pred_cat_ori = catboost.predict(X_test)\n\nacc_catboost_ori = round(catboost.score(X_train, Y_train), 4)\nprint(\"acc catboost =\", acc_catboost_ori)","f2c2e94f":"import catboost\n\nparameters = {'iterations':[1000, 1100],\n              'learning_rate':[0.01],\n              'depth':[9, 11, 13]}\ncatboost = catboost.CatBoostClassifier(verbose=False)\ncatboost = GridSearchCV(catboost, parameters, cv=5, scoring='accuracy')\ncatboost.fit(X_train, Y_train)\nY_pred_cat_best = catboost.predict(X_test)\n\nacc_catboost = round(catboost.score(X_train, Y_train), 4)\nprint(\"acc catboost =\", acc_catboost)\n\nprint('-----')\nprint(f'Best parameters {catboost.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{catboost.best_score_:.3f}'\n)\nbest_score_catboost = catboost.best_score_\nprint('-----')","5b477062":"import lightgbm\n\nlightgbm = lightgbm.LGBMClassifier()\nlightgbm.fit(X_train, Y_train)\nY_pred_lgbm_ori = lightgbm.predict(X_test)\n\nacc_lightgbm_ori = round(lightgbm.score(X_train, Y_train), 4)\nprint(\"acc lightgbm =\", acc_lightgbm_ori)","3950b1f9":"import lightgbm\n\nparameters = {'n_estimators':[500, 625, 750, 875, 1000],\n              'learning_rate':[0.01],\n              'num_leaves':[25, 30, 35]}\nlightgbm = lightgbm.LGBMClassifier()\nlightgbm = GridSearchCV(lightgbm, parameters, cv=5, scoring='accuracy')\nlightgbm.fit(X_train, Y_train)\nY_pred_lgbm_best = lightgbm.predict(X_test)\n\nacc_lightgbm = round(lightgbm.score(X_train, Y_train), 4)\nprint(\"acc lightgbm =\", acc_lightgbm)\n\nprint('-----')\nprint(f'Best parameters {lightgbm.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{lightgbm.best_score_:.3f}'\n)\nbest_score_lightgbm = lightgbm.best_score_\nprint('-----')","664c638d":"model_eval = pd.DataFrame({'Model' : ['Logistic Regression', 'SVM', 'KNN', 'Gaussian Naive Bayes', 'Perceptron', \n                                      'Linear SVC', 'Stochastic Gradient Descent', 'Decision Tree', 'Random Forest',\n                                      'XGBoost', 'AdaBoost', 'CatBoost', 'LightGBM'],\n                           'Accuracy Ori' : [acc_logreg_ori, acc_svm_ori, acc_knn_ori, acc_gaussian_ori, acc_perceptron_ori, \n                                             acc_linear_svc_ori, acc_sgd_ori, acc_decision_tree_ori, acc_random_forest_ori,\n                                             acc_xgboost_ori, acc_adaboost_ori, acc_catboost_ori, acc_lightgbm_ori],\n                           'Accuracy Best' : [0, 0, acc_knn, 0, 0, \n                                              0, 0, acc_decision_tree, acc_random_forest,\n                                              acc_xgboost, acc_adaboost, acc_catboost, acc_lightgbm],\n                           'Grid Search CV Value' : [0, 0, best_score_knn, 0, 0,\n                                                     0, 0, best_score_decision_tree, best_score_random_forest,\n                                                     best_score_xgboost, best_score_adaboost, best_score_catboost, best_score_lightgbm]})\n\nmodel_eval.sort_values(by = ['Grid Search CV Value'], ascending = False)","da84d6f3":"submission_logreg_ori = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_logreg_ori})\nsubmission_logreg_ori.to_csv('submission_logreg_ori.csv', index=False)\n\nsubmission_svm_ori = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_svm_ori})\nsubmission_svm_ori.to_csv('submission_svm_ori.csv', index=False)\n\nsubmission_knn_ori = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_knn_ori})\nsubmission_knn_ori.to_csv('submission_knn_ori.csv', index=False)\n\nsubmission_gaus_ori = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_gaus_ori})\nsubmission_gaus_ori.to_csv('submission_gaus_ori.csv', index=False)\n\nsubmission_per_ori = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_per_ori})\nsubmission_per_ori.to_csv('submission_per_ori.csv', index=False)\n\nsubmission_svc_ori = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_svc_ori})\nsubmission_svc_ori.to_csv('submission_svc_ori.csv', index=False)\n\nsubmission_sgd_ori = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_sgd_ori})\nsubmission_sgd_ori.to_csv('submission_sgd_ori.csv', index=False)\n\nsubmission_dectree_ori = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_dectree_ori})\nsubmission_dectree_ori.to_csv('submission_dectree_ori.csv', index=False)\n\nsubmission_rf_ori = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_rf_ori})\nsubmission_rf_ori.to_csv('submission_rf_ori.csv', index=False)\n\nsubmission_dec_tree = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_xgb_ori})\nsubmission_dec_tree.to_csv('submission_dec_tree.csv', index=False)\n\nsubmission_xgb_ori = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_ada_ori})\nsubmission_xgb_ori.to_csv('submission_xgb_ori.csv', index=False)\n\nsubmission_cat_ori = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_cat_ori})\nsubmission_cat_ori.to_csv('submission_cat_ori.csv', index=False)\n\nsubmission_lgbm_ori = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_lgbm_ori})\nsubmission_lgbm_ori.to_csv('submission_lgbm_ori.csv', index=False)","4755285a":"knn = KNeighborsClassifier(algorithm='auto', leaf_size=300, \n                           n_neighbors=19, weights='uniform')\nknn.fit(X_train, Y_train)\nY_pred_knn = knn.predict(X_test)\n\nacc_knn = round(knn.score(X_train, Y_train), 4)\nprint(\"acc knn =\", acc_knn)\n\n#acc knn = 0.8104\n#Best parameters {'algorithm': 'auto', 'leaf_size': 300, 'n_neighbors': 19, 'weights': 'uniform'}\n#Mean cross-validated accuracy score of the best_estimator: 0.785\n\nsubmission_knn = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_knn})\nsubmission_knn.to_csv('submission_knn.csv', index=False)\n\nimport pickle\nknn = 'knn_model.sav'\npickle.dump(knn, open(knn, 'wb'))","de52e3ce":"decision_tree = DecisionTreeClassifier(max_depth=15, max_features='auto',\n                                       criterion='gini')\ndecision_tree.fit(X_train, Y_train)\nY_pred_dec_tree = decision_tree.predict(X_test)\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train), 4)\nprint(\"acc dec tree =\", acc_decision_tree)\n\n#acc decision_tree = 0.868\n#Best parameters {'criterion': 'gini', 'max_depth': 15, 'max_features': 'auto'}\n#Mean cross-validated accuracy score of the best_estimator: 0.760\n\nsubmission_dec_tree = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_dec_tree})\nsubmission_dec_tree.to_csv('submission_dec_tree.csv', index=False)\n\nimport pickle\ndec_tree = 'dec_tree_model.sav'\npickle.dump(decision_tree, open(dec_tree, 'wb'))\n  \n# load the model from disk\n#loaded_model = pickle.load(open(filename, 'rb'))\n#result = loaded_model.score(X_test, Y_test)\n#print(result)","6679fa48":"random_forest = RandomForestClassifier(criterion='gini', max_depth=11, max_features= 'auto', n_estimators= 300)\nrandom_forest.fit(X_train, Y_train)\nY_pred_rf = random_forest.predict(X_test)\n\nacc_random_forest = round(random_forest.score(X_train, Y_train), 4)\nprint(\"acc random_forest =\", acc_random_forest)\n\n#acc random_forest = 0.868\n#Best parameters {'criterion': 'gini', 'max_depth': 11, 'max_features': 'auto', 'n_estimators': 300}\n#Mean cross-validated accuracy score of the best_estimator: 0.773\n\nsubmission_rf = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_rf})\nsubmission_rf.to_csv('submission_rf.csv', index=False)\n\nrf = 'rf_model.sav'\npickle.dump(random_forest, open(rf, 'wb'))","7c276a57":"import xgboost\nxgboost = xgboost.XGBClassifier(learning_rate= 0.01, max_depth=15, n_estimators=750)\nxgboost.fit(X_train, Y_train)\nY_pred_xgboost = xgboost.predict(X_test)\n\nacc_xgboost = round(xgboost.score(X_train, Y_train), 4)\nprint(\"acc xgboost =\", acc_xgboost)\n\n#acc xgboost = 0.8567\n#Best parameters {'learning_rate': 0.01, 'max_depth': 15, 'n_estimators': 750}\n#Mean cross-validated accuracy score of the best_estimator: 0.778\n\nsubmission_xgboost = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_xgboost})\nsubmission_xgboost.to_csv('submission_xgboost.csv', index=False)\n\npickle.dump(xgboost, open(\"xgboost.dat\", \"wb\"))","5b32a393":"adaboost = AdaBoostClassifier(learning_rate=0.01, n_estimators=750)\nadaboost.fit(X_train, Y_train)\nY_pred_adaboost = adaboost.predict(X_test)\n\nacc_adaboost = round(adaboost.score(X_train, Y_train), 4)\nprint(\"acc adaboost =\", acc_adaboost)\n\n#acc adaboost = 0.7949\n#Best parameters {'learning_rate': 0.01, 'n_estimators': 750}\n#Mean cross-validated accuracy score of the best_estimator: 0.791\n\nsubmission_adaboost = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_adaboost})\nsubmission_adaboost.to_csv('submission_adaboost.csv', index=False)\n\npickle.dump(adaboost, open(\"adaboost.dat\", \"wb\"))","28f9f02f":"import catboost\n\ncatboost = catboost.CatBoostClassifier(verbose=False, depth=9, iterations=1100, learning_rate=0.01)\ncatboost.fit(X_train, Y_train)\nY_pred_catboost = catboost.predict(X_test)\n\nacc_catboost = round(catboost.score(X_train, Y_train), 4)\nprint(\"acc catboost =\", acc_catboost)\n\n#acc catboost = 0.8666\n#Best parameters {'depth': 9, 'iterations': 1100, 'learning_rate': 0.01}\n#Mean cross-validated accuracy score of the best_estimator: 0.780\n\nsubmission_catboost = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_catboost})\nsubmission_catboost.to_csv('submission_catboost.csv', index=False)\n\npickle.dump(catboost, open(\"catboost.dat\", \"wb\"))","5056758e":"import lightgbm\n\nlightgbm = lightgbm.LGBMClassifier(learning_rate=0.01, n_estimators=625, num_leaves=25)\nlightgbm.fit(X_train, Y_train)\nY_pred_lgbm = lightgbm.predict(X_test)\n\nacc_lightgbm = round(lightgbm.score(X_train, Y_train), 4)\nprint(\"acc lightgbm =\", acc_lightgbm)\n\n#acc lightgbm = 0.8469\n#Best parameters {'learning_rate': 0.01, 'n_estimators': 625, 'num_leaves': 25}\n#Mean cross-validated accuracy score of the best_estimator: 0.785\n\nsubmission_lgbm = pd.DataFrame({'PassengerId': test_data_raw['PassengerId'], 'Survived':Y_pred_lgbm})\nsubmission_lgbm.to_csv('submission_lgbm.csv', index=False)\n\npickle.dump(lightgbm, open(\"lightgbm.dat\", \"wb\"))","5e113b0b":"pd.set_option('display.max_rows', 5)\n\nsubs = pd.read_csv('.\/submission_dec_tree.csv')\nsubs","ae405e08":"from sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X_train, Y_train, random_state=1)\nmy_model = DecisionTreeClassifier(max_depth=5, max_features='auto',\n                                  criterion='gini').fit(X_train, Y_train)\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())","773ad256":"## FamilySize (New Feature)","eebc7096":"# Last Drop values\nDrop some features that are from new features","34f8cdea":"### Logistic Regression","4bb88a89":"### Linear SVC","20ecff7f":"# Question or Insight\n* What sorts of people were more likely to survive?\n* Did people from p1 class were more highly to survive?\n* What age of people were more highly to survive?\n\n**Hypothesis**\n* All female were more take precedence\n* Family or related more higher to be survive\n* Same cabin have more higher to survive, since 'maybe' they know each other [can't be used, because the data are very small, less than 50%]\n* Age, maybe around optimal age are having more higher to survive","c87eb2c6":"## Title\ntitle of social strata makes the people have more chance to survived, for the higher person strata","e9e3333a":"# Data Wrangle\n\n**Analyze each features**\n\nSince hypothesis said that people who are have higher survive is because they are having differences of Pclass, Sex, SibSp, Parch, and Age","77c875b5":"**Note**\n\nStill hasn't done yet, need more study to make better accuracy, so I submit everything and let submission choose which one that have better accuracy :)","b28468e2":"## Dataset Identify","2086c11b":"# Model Evaluation","a39ff915":"### CatBoost","03d821c7":"Create new feature combined of Parch and SibSp feature, this new feature is because the reason from closed family have a highly survived to look each other","b553414c":"## Age","77dd094a":"### k-Nearest Neighbors (KNN) Classifier","95099d1b":"## Drop Column and DropNA 1","d8e40901":"## Went Alone (New Feature)\nAnd create feature for someone who went to ship alone","8c1437b1":"### XGBoost","eba2e532":"### LightGBM","51636ac6":"### Support Vector Machines (SVM)","0450c5d8":"# Model, Predict, and Solve\n","65f18644":"### AdaBoost","0f6cb673":"# Additional Section to look Correlation","aeb711c8":"## Permutation Importance","90495dce":"## Seed","f30feeed":"# Import and Load Data","d8a2020c":"### Random Forest","208088ba":"### Gaussian Naive Bayes","78362559":"## Data Description","f6902336":"## Sex","cd3de54e":"### Perceptron","fd38638e":"* Survival | Survival | 0 = No, 1 = Yes\n* Pclass | Ticket class | 1 = Upper, 2 = Middle, 3 = Lower | *A proxy for socio-economic status (SES)*\n* Sex | Sex\n* Age |Age in years | *Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5*\n* Sibsp | siblings \/ spouses aboard the Titanic | *The dataset defines family relations in this way...*\n> * Sibling = brother, sister, stepbrother, stepsister\n> * Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n* Parch | parents \/ children aboard the Titanic | *The dataset defines family relations in this way...*\n> * Parent = mother, father\n> * Child = daughter, son, stepdaughter, stepson\n> * Some children travelled only with a nanny, therefore parch=0 for them.\n* Ticket | Ticket number\n* Fare | Passenger fare\n* Cabin | Cabin number\n* Embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton","95ecd296":"### Decision Tree","178ebb40":"# EDA (Exploratory Data Analysis)","d05eb7ce":"## Survived","2ab1a885":"## Read Data","5b253ec5":"## Pclass","6c007cb6":"# Convert feature with LabelEncoding","b464041c":"## Train the Data\nList of train model\n* Logistic Regression\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forrest\n* Perceptron\n* Artificial neural network\n* RVM or Relevance Vector Machine","33af325d":"## Embarked","79d86526":"### Stochastic Gradient Descent","081565be":"## Normalization","d21a8b00":"# Submission"}}