{"cell_type":{"27a0c828":"code","c739d901":"code","520d2693":"code","3314c59b":"code","292461f7":"code","2b728a8c":"code","ca18085e":"code","a2689771":"code","262082ff":"code","48304aae":"code","4ea0fd34":"code","a27b0330":"code","86c4b175":"code","84edc0db":"code","99790f85":"code","65d403aa":"code","1479db78":"code","87e268db":"code","ce64d8ab":"code","d4c2afc8":"code","e507cc15":"code","c42e0d8e":"code","7a0235cb":"code","bed07a9f":"code","31f52972":"code","6edd37f3":"markdown"},"source":{"27a0c828":"import math","c739d901":"#import galearn\nimport os\nimport glob\nfrom joblib import Parallel, delayed\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.svm import SVR\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)\n#from galearn import *\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import roc_auc_score\nimport pandas as pd\nimport bqplot\nimport matplotlib.pyplot as plt\nimport time\nimport numpy as np\nfrom tqdm.notebook import tqdm, trange\nimport seaborn as sns\nimport optuna\nimport ipywidgets as widgets\nsns.set({'figure.figsize': (12, 8)})\nsns.set_color_codes(\"pastel\")\nrng = np.random.default_rng()","520d2693":"#generates a dictionary from the pool of genes\ndef generate_parent(gene_pool):\n    parent = dict()\n    for gene in gene_pool.keys():\n        parent[gene] = rng.choice(gene_pool[gene])\n    fitness = get_fitness(estimator(**parent), fitness_function)\n    #print(f\"new individuals fitness is {fitness}\")\n    if math.isnan(fitness):\n        print(\"nan : \", parent)\n    return Individual(parent, fitness)","3314c59b":"def mutate(parent, gene_pool):\n    gene = rng.choice(list(params))\n    child = parent.copy()\n    new_gene, alternate = rng.choice(gene_pool[gene], 2)\n    #help make sure the gene get's mutated\n    child[gene] = alternate if new_gene == child[gene] else new_gene\n    return child","292461f7":"class Individual:\n    def __init__(self, genes, fitness):\n        self._genes = genes\n        self._fitness = fitness\n        self._fp = 0 #fitness_proportion to be used when selection == fp\n        \n    def __eq__(self, other):\n        return self.genes == other.genes\n\n    def __lt__(self, other):\n        return self.fitness < other.fitness\n    \n    def __gt__(self, other):\n        return self.fitness > other.fitness\n    \n    def __str__(self):\n        return f\"Individual with genes: {self._genes} and fitness:{self._fitness}\"\n        \n    @property    \n    def genes(self):\n        return self._genes\n    \n    @property\n    def fitness(self):\n        return self._fitness\n    \n    #may add cv = cv\/skf as an option\n    def set_fitness(self):\n        self._fitness = get_fitness(estimator(**self.genes), fitness_function)\n    \n    def set_gene(self, gene, value):\n        self._genes[gene] = value\n        \n    def get_gene_from_window(self, gene):\n        min_c = gene_pool[gene].min()\n        max_c = gene_pool[gene].max()\n        dist_1 = self._genes[gene] - min_c\n        dist_2 = max_c - self._genes[gene]\n        dist = min(dist_1, dist_2)*gnp_window\n        lb = self._genes[gene] - dist\n        ub = self._genes[gene] + dist\n        new_gene, alternate = rng.choice(gene_pool[gene][(gene_pool[gene] >= lb) & (gene_pool[gene] <= ub)], 2)\n        return new_gene, alternate\n        \n    \n    def mutate(self):\n        gene = rng.choice(list(self._genes))\n        if restrict_gnp and isinstance(gene_pool[gene], float):\n            #give chance of diversity = 1-p_mutate until 10% chance\n            if rng.random() < p_outlier:\n                print(f\"got an outlier\")\n                new_gene, alternate = rng.choice(gene_pool[gene], 2)\n            else:\n                new_gene, alternate = self.get_gene_from_window(gene)\n        else:\n            new_gene, alternate = rng.choice(gene_pool[gene], 2)\n    #help make sure the gene get's mutated\n        self._genes[gene] = alternate if new_gene == self._genes[gene] else new_gene\n        return\n    \n    def get_estimator(self):\n        return estimator(**self._genes)","2b728a8c":"#creates a population of size size with parameters from gene_pool\ndef create_population(gene_pool, size = 10):\n    population = []\n    for i in range(size):\n        population.append(generate_parent(gene_pool))\n    population.sort(reverse = True)\n    return population","ca18085e":"class Population:\n    #create initial population\n    def __init__(self, gene_pool, size = 10):\n        self._population = create_population(gene_pool, size)\n        self._size = size\n      \n    #note that if several individuals have == best fitness anyone of them is returned in the sorted list\n    @property\n    def best_individual(self):\n        return self._population[0]\n    \n    @property\n    def best_fitness(self):\n        return self._population[0].fitness\n    \n    @property\n    def population(self):\n        return self._population\n    \n    @property\n    def size(self):\n        return self._size\n    \n    def replace_generation(self, new_gen):\n        new_gen.sort(reverse = True)\n        self._population = new_gen","a2689771":"def get_fitness(individual, fitness_function, cv = 3):\n    score = cross_val_score(individual, X_train, y_train, cv=cv, scoring = fitness_function)\n    return score.mean()","262082ff":"def select_breeding(population, selection = 'truncation', frac = 0.75):\n    if selection == 'truncation':\n        cut = int(len(population.population)*frac)\n        breeding = population.population[:cut]\n        return breeding\n    elif selection == 'fitness_proportionate' or selection == 'fp':\n        size = int(population.size * frac)\n        return fp_selection(population, size)\n    elif selection == 'tournament':\n        size = int(population.size * frac)\n        return tournament_selection(population, size)\n    elif selection == 'sus':\n        size = int(population.size * frac)\n        return sus_selection(population, size)","48304aae":"def fp_selection(pop, size):\n    p = np.array([ind.fitness for ind in pop.population])\n    total_fitness = p.sum()\n    p = p \/ total_fitness\n    #p = np.cumsum(p) nice alternative solution\n    return rng.choice(pop.population, size = size, p = p).tolist()","4ea0fd34":"#stochastic universal sampling\ndef sus_selection(pop, size):\n    p = np.array([ind.fitness for ind in pop.population]).cumsum()\n    total_fitness = np.array([ind.fitness for ind in pop.population]).sum()\n    step = total_fitness \/ size\n    start = rng.uniform(0, step)\n    steps = [(start + i*step) for i in range(size)]\n    i = 0\n    breeding = []\n    for s in steps:\n        while p[i] < s:\n            i = i + 1\n            breeding.append(pop.population[i])\n    return breeding","a27b0330":"#add requirement size and elitism have to be even!\n#also elitism is almost unnecessary if tournament, almost!\ndef tournament_selection(pop, size):\n    participants = [ind for ind in pop.population]\n    breeding = []\n    #could implement different rounds here\n    #but I think that's almost the same as calling tournament different times with smaller sizes\n    for i in range(size):\n        a, b = rng.choice(participants, 2)\n        if a > b:\n            breeding.append(a)\n            participants.remove(a)\n        else:\n            breeding.append(b)\n            participants.remove(b)\n    return breeding\n        \n    \n#reverse tournament, eliminates need for elitism\n#could use with parallelism\ndef rev_tournament_selection(pop, size):\n    breeding = [ind for ind in pop.population]\n    num_eliminated = len(breeding) - size\n    for i in range(num_eliminated):\n        a, b = rng.choice(participants, 2)\n        if a > b:\n            breeding.remove(b)\n        else:\n            breeding.remove(a)\n    return breeding\n        ","86c4b175":"def breed(parent_1, parent_2, p_cross, p_mutate):\n    # check for recombination\n    # if crossover happens at probability p then not crossover would happen at probability 1-p\n    #rand() will draw a number larger than p_cross 1-p times\n    #and a number < p_cross p times\n    # children are copies of parents by default\n    child_1, child_2 = Individual(parent_1.genes, parent_1.fitness), Individual(parent_2.genes, parent_2.fitness)\n    if np.random.rand() < p_cross: \n        \n        genes = list(child_1.genes)\n        child_1, child_2 = crossover(parent_1, parent_2, child_1, child_2)\n        #mutate if p\n    if np.random.rand() < p_mutate:\n        child_1.mutate()\n    if np.random.rand() < p_mutate:\n        child_2.mutate()\n        \n        child_1.set_fitness()\n        child_2.set_fitness()\n    return child_1, child_2","84edc0db":"# crossover two parents to create two children\n# should not be called by itself because it doesn't set fitness \ndef crossover(parent_1, parent_2, child_1, child_2):\n    # children are copies of parents by default\n    genes = list(child_1.genes) #make global to make more efficient!\n    # select crossover point that is not on the end of the string\n    start = rng.choice(range(len(genes) - 1))\n    #no crossover happening\n    if start == len(genes) -1:\n        return [child_1, child_2]\n    cut = rng.choice(range(start, len(genes)))\n    #no crossover happening\n    if cut == start:\n        return [child_1, child_2]\n    # perform crossover\n    for gene in genes[start:cut]:\n        if isinstance(gene_pool[gene], float): #introduce more diversity by modified crossover for continous values\n            #could also solve this with algebra, but I like using the predefined gene_pool\n            lower = parent_1[gene]\n            higher = parnt_2[gene]\n            if parent_1[gene] > parent_2[gene]:\n                lower = parent_2[gene]\n                higher = parent_1[gene]\n                \n            new_gene_1, new_gene_2, = rng.choice(gene_pool[gene][(gene_pool[gene] >= lower) & (gene_pool[gene] <= higher)], 2)\n            child_1.set_gene(gene, new_gene_1)\n            child_2.set_gene(gene, new_gene_2)\n        else:\n            child_1.set_gene(gene, parent_2.genes[gene])\n            child_2.set_gene(gene, parent_1.genes[gene])\n        \n    return child_1, child_2","99790f85":"def simulate(params,\n             scorer,\n             iterations,\n             model,\n             train_set,\n             train_labels,\n             selection = 'fp',\n             p_cross = 1,\n             cv = 3,\n             p_mutate = 1,\n             sim_ann = True, \n             restrict_gene_pool = True, #narrow genes i.e. finetune\n             gene_pool_window = 1.0, #initial size of window\n             decay = None,\n             elitism = 2,\n             population_size = 10):\n    #add some fixed genes\n    global X_train, y_train, estimator, fitness_function, gene_pool, restrict_gnp, gnp_window, rng\n    global p_outlier\n    generations = np.arange(0, iterations)\n    fitness_prog = []\n    p_outlier = 1 - p_mutate\n    rng = np.random.default_rng()\n    X_train, y_train = train_set, train_labels\n    fitness_function = scorer\n    estimator = model\n    gene_pool = params\n    restrict_gnp = restrict_gene_pool\n    gnp_window = gene_pool_window\n    population = Population(gene_pool, size = population_size)\n    best_fitness = population.best_fitness\n    if decay == None:\n        decay = 1 \/ iterations\n    print(f\"best initial fitness: {population.best_fitness}\")\n    for i in trange(iterations):\n        fitness_prog.append(best_fitness)\n        new_gen = []\n        breeding = select_breeding(population, selection)\n        for elite in range(elitism):\n            new_gen.append(population.population[elite])\n       \n        #elitism to be implemented here\n        while(len(new_gen) < population.size): #let populatin size oscillate +1 -1?\n            parent_1, parent_2 = rng.choice(breeding, 2) #possibility of selecting the same individual\n            child_1, child_2 = breed(parent_1, parent_2, p_cross, p_mutate)\n            new_gen.append(child_1)\n            new_gen.append(child_2)\n        #replace the previous generation\n        population.replace_generation(new_gen)\n        #are you better than the last?\n        if (best_fitness < population.best_fitness):\n            diff = population.best_fitness - best_fitness\n            best_fitness = population.best_fitness\n            display(f\"child {population.best_individual} with fitness {population.best_fitness}, which is {diff} better than before\")\n        if sim_ann:\n            gnp_window = gnp_window - gnp_window*decay\n            p_cross = p_cross - p_cross*decay\n            p_mutate = p_mutate - p_mutate*decay\n            if p_outlier < 0.1:\n                p_outlier = 1 - p_mutate\n    #note if several individuals have same fitness anyone of them is returned\n    return population.best_individual, (generations, fitness_prog)","65d403aa":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x \/ y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std]\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return -np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","1479db78":"params = dict()\nparams['kernel'] = ['rbf', 'poly', 'sigmoid']\nparams['degree'] = [2, 3, 4]\nparams['gamma'] = ['scale', 'auto']\nparams['C'] = np.linspace(0.001, 10, 1000)\nparams['epsilon'] = np.linspace(0.01, 10, 500)","87e268db":"from sklearn.metrics import make_scorer","ce64d8ab":"train, test = read_train_test()\nx = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nx_test = test.drop(['row_id', 'time_id'], axis = 1)\n# Transform stock id to a numeric value\nx['stock_id'] = x['stock_id'].astype(int)\nx_test['stock_id'] = x_test['stock_id'].astype(int)","d4c2afc8":"#reg = lgb.LGBMRegressor(**nanpams)","e507cc15":"scorer = make_scorer(rmspe)","c42e0d8e":"#np.mean(cross_val_score(reg, x, y, scoring=scorer))","7a0235cb":"best, history= simulate(params, scorer, 500,\n                        SVR,\n                        x, y,\n                        selection = 'tournament',\n                        p_cross = 1,\n                        p_mutate = 1,\n                        sim_ann = True)","bed07a9f":"sns.regplot(x = history[0], y = history[1])\nplt.xlabel('generation')\nplt.ylabel('fitness')\nplt.savefig('svr.png')\nplt.show()","31f52972":"#test_p = reg.predict(x_test)","6edd37f3":"This is just another lgbm baseline for this competition, a lot of the work, ideas and function are copied from https:\/\/www.kaggle.com\/tommy1028\/lightgbm-starter-with-feature-engineering-idea\n\nThe features that improve the model are the aggregartions stats using time_id and stock_id using the realized volatility for different windows from the past\n\nCheers and have fun"}}