{"cell_type":{"cd44472e":"code","f0c6e92e":"code","adc20c5c":"code","9942aa8d":"code","5fb270b4":"code","f9aafd42":"code","77777705":"code","df75d8c1":"code","dca05140":"code","2a92c598":"code","8c49cb63":"code","bad95800":"code","d145ac8c":"code","0e0be0a4":"code","8cceb3c9":"code","30cee829":"code","30fd5696":"code","093f9372":"code","85c6e5da":"code","b2debc05":"code","3451d2d1":"code","8bf706a2":"code","d3442cf7":"code","8bd8f5f2":"code","ba5d50fa":"code","0b5d8b2e":"code","a0c48865":"code","83e28114":"code","268fbd33":"code","459bbdf6":"code","a4234974":"code","afad1eb7":"code","b2193ae6":"code","85e38c87":"code","5034fb4f":"code","fb90bb5f":"code","1bdc181a":"code","2d1b12b4":"code","16f28515":"code","1faaf51d":"code","8f4484f0":"code","5bc7b935":"code","4627f847":"code","60a6a4d4":"code","da560277":"code","c2f268bd":"code","b0c0e7e7":"code","6503caf1":"code","ac595240":"code","8711933f":"code","ac7c5740":"code","ed54d591":"code","28a57dbc":"code","5624971f":"code","9c2a1a0d":"code","66ac200c":"code","8ceeb446":"code","0ded762e":"code","c0a7766f":"code","b9f290e1":"code","72bf72c4":"markdown","c1612a6d":"markdown","40b599c6":"markdown","f034cccd":"markdown","22a0df71":"markdown","5b1c2202":"markdown","570446e4":"markdown","ff85a3d3":"markdown","3de12d6b":"markdown","34e84c24":"markdown","21ab5978":"markdown","9f86af76":"markdown","271c100e":"markdown","a4dd13e5":"markdown","957a9b37":"markdown","83ddd8f1":"markdown","989d5013":"markdown","ac17e4a1":"markdown","4d64aa77":"markdown","f59844a1":"markdown","5c0196f4":"markdown","7ae2bb3f":"markdown","819d7a9e":"markdown","8d679f10":"markdown","b969358b":"markdown","6df28ca2":"markdown","ec0bf394":"markdown","6467e78b":"markdown","1032765b":"markdown","5f7f00ca":"markdown","30e3f1fc":"markdown","97f62d73":"markdown","e12091dd":"markdown","0cab9c57":"markdown","8421a38e":"markdown","6424fc13":"markdown","40245af3":"markdown","d4ac9c52":"markdown","26b892ca":"markdown","3ad12b88":"markdown","edd170bf":"markdown"},"source":{"cd44472e":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd   \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nimport itertools\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier","f0c6e92e":"data = pd.read_csv(\"..\/input\/application_record.csv\", encoding = 'utf-8') \nrecord = pd.read_csv(\"..\/input\/credit_record.csv\", encoding = 'utf-8')  ","adc20c5c":"plt.rcParams['figure.facecolor'] = 'white'","9942aa8d":"# find all users' account open month.\nbegin_month=pd.DataFrame(record.groupby([\"ID\"])[\"MONTHS_BALANCE\"].agg(min))\nbegin_month=begin_month.rename(columns={'MONTHS_BALANCE':'begin_month'}) \nnew_data=pd.merge(data,begin_month,how=\"left\",on=\"ID\") #merge to record data","5fb270b4":"record['dep_value'] = None\nrecord['dep_value'][record['STATUS'] =='2']='Yes' \nrecord['dep_value'][record['STATUS'] =='3']='Yes' \nrecord['dep_value'][record['STATUS'] =='4']='Yes' \nrecord['dep_value'][record['STATUS'] =='5']='Yes' ","f9aafd42":"cpunt=record.groupby('ID').count()\ncpunt['dep_value'][cpunt['dep_value'] > 0]='Yes' \ncpunt['dep_value'][cpunt['dep_value'] == 0]='No' \ncpunt = cpunt[['dep_value']]\nnew_data=pd.merge(new_data,cpunt,how='inner',on='ID')\nnew_data['target']=new_data['dep_value']\nnew_data.loc[new_data['target']=='Yes','target']=1\nnew_data.loc[new_data['target']=='No','target']=0","77777705":"print(cpunt['dep_value'].value_counts())\ncpunt['dep_value'].value_counts(normalize=True)","df75d8c1":"new_data.rename(columns={'CODE_GENDER':'Gender','FLAG_OWN_CAR':'Car','FLAG_OWN_REALTY':'Reality',\n                         'CNT_CHILDREN':'ChldNo','AMT_INCOME_TOTAL':'inc',\n                         'NAME_EDUCATION_TYPE':'edutp','NAME_FAMILY_STATUS':'famtp',\n                        'NAME_HOUSING_TYPE':'houtp','FLAG_EMAIL':'email',\n                         'NAME_INCOME_TYPE':'inctp','FLAG_WORK_PHONE':'wkphone',\n                         'FLAG_PHONE':'phone','CNT_FAM_MEMBERS':'famsize',\n                        'OCCUPATION_TYPE':'occyp'\n                        },inplace=True)","dca05140":"new_data.dropna()\nnew_data = new_data.mask(new_data == 'NULL').dropna()","2a92c598":"ivtable=pd.DataFrame(new_data.columns,columns=['variable'])\nivtable['IV']=None\nnamelist = ['FLAG_MOBIL','begin_month','dep_value','target','ID']\n\nfor i in namelist:\n    ivtable.drop(ivtable[ivtable['variable'] == i].index, inplace=True)","8c49cb63":"# Calculate information value\ndef calc_iv(df, feature, target, pr=False):\n    lst = []\n    df[feature] = df[feature].fillna(\"NULL\")\n\n    for i in range(df[feature].nunique()):\n        val = list(df[feature].unique())[i]\n        lst.append([feature,                                                        # Variable\n                    val,                                                            # Value\n                    df[df[feature] == val].count()[feature],                        # All\n                    df[(df[feature] == val) & (df[target] == 0)].count()[feature],  # Good (think: Fraud == 0)\n                    df[(df[feature] == val) & (df[target] == 1)].count()[feature]]) # Bad (think: Fraud == 1)\n\n    data = pd.DataFrame(lst, columns=['Variable', 'Value', 'All', 'Good', 'Bad'])\n    data['Share'] = data['All'] \/ data['All'].sum()\n    data['Bad Rate'] = data['Bad'] \/ data['All']\n    data['Distribution Good'] = (data['All'] - data['Bad']) \/ (data['All'].sum() - data['Bad'].sum())\n    data['Distribution Bad'] = data['Bad'] \/ data['Bad'].sum()\n    data['WoE'] = np.log(data['Distribution Good'] \/ data['Distribution Bad'])\n    \n    data = data.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n\n    data['IV'] = data['WoE'] * (data['Distribution Good'] - data['Distribution Bad'])\n\n    data = data.sort_values(by=['Variable', 'Value'], ascending=[True, True])\n    data.index = range(len(data.index))\n\n    if pr:\n        print(data)\n        print('IV = ', data['IV'].sum())\n\n    iv = data['IV'].sum()\n    print('This variable\\'s IV is:',iv)\n    print(df[feature].value_counts())\n    return iv, data","bad95800":"def convert_dummy(df, feature,rank=0):\n    pos = pd.get_dummies(df[feature], prefix=feature)\n    mode = df[feature].value_counts().index[rank]\n    biggest = feature + '_' + str(mode)\n    pos.drop([biggest],axis=1,inplace=True)\n    df.drop([feature],axis=1,inplace=True)\n    df=df.join(pos)\n    return df","d145ac8c":"def get_category(df, col, binsnum, labels, qcut = False):\n    if qcut:\n        localdf = pd.qcut(df[col], q = binsnum, labels = labels) # quantile cut\n    else:\n        localdf = pd.cut(df[col], bins = binsnum, labels = labels) # equal-length cut\n        \n    localdf = pd.DataFrame(localdf)\n    name = 'gp' + '_' + col\n    localdf[name] = localdf[col]\n    df = df.join(localdf[name])\n    df[name] = df[name].astype(object)\n    return df","0e0be0a4":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        \n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","8cceb3c9":"new_data['Gender'] = new_data['Gender'].replace(['F','M'],[0,1])\nprint(new_data['Gender'].value_counts())\niv, data = calc_iv(new_data,'Gender','target')\nivtable.loc[ivtable['variable']=='Gender','IV']=iv\ndata.head()","30cee829":"new_data['Car'] = new_data['Car'].replace(['N','Y'],[0,1])\nprint(new_data['Car'].value_counts())\niv, data=calc_iv(new_data,'Car','target')\nivtable.loc[ivtable['variable']=='Car','IV']=iv\ndata.head()","30fd5696":"new_data['Reality'] = new_data['Reality'].replace(['N','Y'],[0,1])\nprint(new_data['Reality'].value_counts())\niv, data=calc_iv(new_data,'Reality','target')\nivtable.loc[ivtable['variable']=='Reality','IV']=iv\ndata.head()","093f9372":"new_data['phone']=new_data['phone'].astype(str)\nprint(new_data['phone'].value_counts(normalize=True,sort=False))\nnew_data.drop(new_data[new_data['phone'] == 'nan' ].index, inplace=True)\niv, data=calc_iv(new_data,'phone','target')\nivtable.loc[ivtable['variable']=='phone','IV']=iv\ndata.head()","85c6e5da":"print(new_data['email'].value_counts(normalize=True,sort=False))\nnew_data['email']=new_data['email'].astype(str)\niv, data=calc_iv(new_data,'email','target')\nivtable.loc[ivtable['variable']=='email','IV']=iv\ndata.head()","b2debc05":"new_data['wkphone']=new_data['wkphone'].astype(str)\niv, data = calc_iv(new_data,'wkphone','target')\nnew_data.drop(new_data[new_data['wkphone'] == 'nan' ].index, inplace=True)\nivtable.loc[ivtable['variable']=='wkphone','IV']=iv\ndata.head()","3451d2d1":"new_data.loc[new_data['ChldNo'] >= 2,'ChldNo']='2More'\nprint(new_data['ChldNo'].value_counts(sort=False))","8bf706a2":"iv, data=calc_iv(new_data,'ChldNo','target')\nivtable.loc[ivtable['variable']=='ChldNo','IV']=iv\ndata.head()","d3442cf7":"new_data = convert_dummy(new_data,'ChldNo')","8bd8f5f2":"new_data['inc']=new_data['inc'].astype(object)\nnew_data['inc'] = new_data['inc']\/10000 \nprint(new_data['inc'].value_counts(bins=10,sort=False))\nnew_data['inc'].plot(kind='hist',bins=50,density=True)","ba5d50fa":"new_data = get_category(new_data,'inc', 3, [\"low\",\"medium\", \"high\"], qcut = True)\niv, data = calc_iv(new_data,'gp_inc','target')\nivtable.loc[ivtable['variable']=='inc','IV']=iv\ndata.head()","0b5d8b2e":"new_data = convert_dummy(new_data,'gp_inc')","a0c48865":"new_data['Age']=-(new_data['DAYS_BIRTH'])\/\/365\t\nprint(new_data['Age'].value_counts(bins=10,normalize=True,sort=False))\nnew_data['Age'].plot(kind='hist',bins=20,density=True)","83e28114":"new_data = get_category(new_data,'Age',5, [\"lowest\",\"low\",\"medium\",\"high\",\"highest\"])\niv, data = calc_iv(new_data,'gp_Age','target')\nivtable.loc[ivtable['variable']=='DAYS_BIRTH','IV'] = iv\ndata.head()","268fbd33":"new_data = convert_dummy(new_data,'gp_Age')","459bbdf6":"new_data['worktm']=-(new_data['DAYS_EMPLOYED'])\/\/365\t\nnew_data[new_data['worktm']<0] = np.nan # replace by na\nnew_data['DAYS_EMPLOYED']\nnew_data['worktm'].fillna(new_data['worktm'].mean(),inplace=True) #replace na by mean\nnew_data['worktm'].plot(kind='hist',bins=20,density=True)","a4234974":"new_data = get_category(new_data,'worktm',5, [\"lowest\",\"low\",\"medium\",\"high\",\"highest\"])\niv, data=calc_iv(new_data,'gp_worktm','target')\nivtable.loc[ivtable['variable']=='DAYS_EMPLOYED','IV']=iv\ndata.head()","afad1eb7":"new_data = convert_dummy(new_data,'gp_worktm')","b2193ae6":"new_data['famsize'].value_counts(sort=False)","85e38c87":"new_data['famsize']=new_data['famsize'].astype(int)\nnew_data['famsizegp']=new_data['famsize']\nnew_data['famsizegp']=new_data['famsizegp'].astype(object)\nnew_data.loc[new_data['famsizegp']>=3,'famsizegp']='3more'\niv, data=calc_iv(new_data,'famsizegp','target')\nivtable.loc[ivtable['variable']=='famsize','IV']=iv\ndata.head()","5034fb4f":"new_data = convert_dummy(new_data,'famsizegp')","fb90bb5f":"print(new_data['inctp'].value_counts(sort=False))\nprint(new_data['inctp'].value_counts(normalize=True,sort=False))\nnew_data.loc[new_data['inctp']=='Pensioner','inctp']='State servant'\nnew_data.loc[new_data['inctp']=='Student','inctp']='State servant'\niv, data=calc_iv(new_data,'inctp','target')\nivtable.loc[ivtable['variable']=='inctp','IV']=iv\ndata.head()","1bdc181a":"new_data = convert_dummy(new_data,'inctp')","2d1b12b4":"new_data.loc[(new_data['occyp']=='Cleaning staff') | (new_data['occyp']=='Cooking staff') | (new_data['occyp']=='Drivers') | (new_data['occyp']=='Laborers') | (new_data['occyp']=='Low-skill Laborers') | (new_data['occyp']=='Security staff') | (new_data['occyp']=='Waiters\/barmen staff'),'occyp']='Laborwk'\nnew_data.loc[(new_data['occyp']=='Accountants') | (new_data['occyp']=='Core staff') | (new_data['occyp']=='HR staff') | (new_data['occyp']=='Medicine staff') | (new_data['occyp']=='Private service staff') | (new_data['occyp']=='Realty agents') | (new_data['occyp']=='Sales staff') | (new_data['occyp']=='Secretaries'),'occyp']='officewk'\nnew_data.loc[(new_data['occyp']=='Managers') | (new_data['occyp']=='High skill tech staff') | (new_data['occyp']=='IT staff'),'occyp']='hightecwk'\nprint(new_data['occyp'].value_counts())\niv, data=calc_iv(new_data,'occyp','target')\nivtable.loc[ivtable['variable']=='occyp','IV']=iv\ndata.head()         ","16f28515":"new_data = convert_dummy(new_data,'occyp')","1faaf51d":"iv, data=calc_iv(new_data,'houtp','target')\nivtable.loc[ivtable['variable']=='houtp','IV']=iv\ndata.head()","8f4484f0":"new_data = convert_dummy(new_data,'houtp')","5bc7b935":"new_data.loc[new_data['edutp']=='Academic degree','edutp']='Higher education'\niv, data=calc_iv(new_data,'edutp','target')\nivtable.loc[ivtable['variable']=='edutp','IV']=iv\ndata.head()","4627f847":"new_data = convert_dummy(new_data,'edutp')","60a6a4d4":"new_data['famtp'].value_counts(normalize=True,sort=False)","da560277":"iv, data=calc_iv(new_data,'famtp','target')\nivtable.loc[ivtable['variable']=='famtp','IV']=iv\ndata.head()","c2f268bd":"new_data = convert_dummy(new_data,'famtp')","b0c0e7e7":"ivtable=ivtable.sort_values(by='IV',ascending=False)\nivtable.loc[ivtable['variable']=='DAYS_BIRTH','variable']='agegp'\nivtable.loc[ivtable['variable']=='DAYS_EMPLOYED','variable']='worktmgp'\nivtable.loc[ivtable['variable']=='inc','variable']='incgp'\nivtable","6503caf1":"new_data.to_pickle('data.pkl')\nnew_data.columns","ac595240":"Y = new_data['target']\nX = new_data[['Gender','Reality','ChldNo_1', 'ChldNo_2More','wkphone',\n              'gp_Age_high', 'gp_Age_highest', 'gp_Age_low',\n       'gp_Age_lowest','gp_worktm_high', 'gp_worktm_highest',\n       'gp_worktm_low', 'gp_worktm_medium','occyp_hightecwk', \n              'occyp_officewk','famsizegp_1', 'famsizegp_3more',\n       'houtp_Co-op apartment', 'houtp_Municipal apartment',\n       'houtp_Office apartment', 'houtp_Rented apartment',\n       'houtp_With parents','edutp_Higher education',\n       'edutp_Incomplete higher', 'edutp_Lower secondary','famtp_Civil marriage',\n       'famtp_Separated','famtp_Single \/ not married','famtp_Widow']]","8711933f":"Y = Y.astype('int')\nX_balance,Y_balance = SMOTE().fit_sample(X,Y)\nX_balance = pd.DataFrame(X_balance, columns = X.columns)","ac7c5740":"X_train, X_test, y_train, y_test = train_test_split(X_balance,Y_balance, \n                                                    stratify=Y_balance, test_size=0.3,\n                                                    random_state = 10086)","ed54d591":"model = LogisticRegression(C=0.8,\n                           random_state=0,\n                           solver='lbfgs')\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\n\nprint('Accuracy Score is {:.5}'.format(accuracy_score(y_test, y_predict)))\nprint(pd.DataFrame(confusion_matrix(y_test,y_predict)))\n\nsns.set_style('white') \nclass_names = ['0','1']\nplot_confusion_matrix(confusion_matrix(y_test,y_predict),\n                      classes= class_names, normalize = True, \n                      title='Normalized Confusion Matrix: Logistic Regression')\n","28a57dbc":"model = DecisionTreeClassifier(max_depth=12,\n                               min_samples_split=8,\n                               random_state=1024)\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\n\nprint('Accuracy Score is {:.5}'.format(accuracy_score(y_test, y_predict)))\nprint(pd.DataFrame(confusion_matrix(y_test,y_predict)))\n\nplot_confusion_matrix(confusion_matrix(y_test,y_predict),\n                      classes=class_names, normalize = True, \n                      title='Normalized Confusion Matrix: CART')","5624971f":"model = RandomForestClassifier(n_estimators=250,\n                              max_depth=12,\n                              min_samples_leaf=16\n                              )\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\n\nprint('Accuracy Score is {:.5}'.format(accuracy_score(y_test, y_predict)))\nprint(pd.DataFrame(confusion_matrix(y_test,y_predict)))\n\nplot_confusion_matrix(confusion_matrix(y_test,y_predict),\n                      classes=class_names, normalize = True, \n                      title='Normalized Confusion Matrix: Ramdom Forests')","9c2a1a0d":"model = svm.SVC(C = 0.8,\n                kernel='linear')\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\n\nprint('Accuracy Score is {:.5}'.format(accuracy_score(y_test, y_predict)))\nprint(pd.DataFrame(confusion_matrix(y_test,y_predict)))\n\nplot_confusion_matrix(confusion_matrix(y_test,y_predict),\n                      classes=class_names, normalize = True, \n                      title='Normalized Confusion Matrix: SVM')","66ac200c":"model = LGBMClassifier(num_leaves=31,\n                       learning_rate=0.02,\n                       n_estimators=250,\n                       subsample = 0.8,\n                       colsample_bytree =0.8\n                      )\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\nprint('Accuracy Score is {:.5}'.format(accuracy_score(y_test, y_predict)))\nprint(pd.DataFrame(confusion_matrix(y_test,y_predict)))","8ceeb446":"def plot_importance(classifer, x_train, point_size = 25):\n    '''plot feature importance'''\n    values = sorted(zip(x_train.columns, classifer.feature_importances_), key = lambda x: x[1] * -1)\n    imp = pd.DataFrame(values,columns = [\"Name\", \"Score\"])\n    imp.sort_values(by = 'Score',inplace = True)\n    sns.scatterplot(x = 'Score',y='Name', linewidth = 0,\n                data = imp,s = point_size, color='red').set(\n    xlabel='importance', \n    ylabel='features')\n    \nplot_importance(model, X_train,20)   ","0ded762e":"model.booster_.feature_importance(importance_type='gain')","c0a7766f":"model = XGBClassifier(max_depth=12,\n                      n_estimators=250,\n                      min_child_weight=8, \n                      subsample=0.8, \n                      learning_rate =0.02,    \n                      seed=42)\n\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\nprint('Accuracy Score is {:.5}'.format(accuracy_score(y_test, y_predict)))\nprint(pd.DataFrame(confusion_matrix(y_test,y_predict)))","b9f290e1":"plot_importance(model, X_train, 20)   ","72bf72c4":"## IV\u3001WOE\uff1aConcept and Application","c1612a6d":"# Feature Engineering","40b599c6":"## Logistic Regression   \n\n$$\\log ({p \\over {1 - p}}) = {\\beta _0} + {\\beta _1}{x_1} +  \\cdot  \\cdot  \\cdot  + {\\beta _q}{x_q}$$","f034cccd":"#### Working Years\n+ Equal-length Bucketing","22a0df71":"#### Famliy Size","5b1c2202":"## Features","570446e4":"#### Having a phone or not","ff85a3d3":"## Decision Tree","3de12d6b":"+ Define `calc_iv` function to [calculate](https:\/\/www.kaggle.com\/puremath86\/iv-woe-starter-for-python) Information Value and WOE Value","34e84c24":"<font size=5 >Credit Card Approval Prediction Using Sklearn<\/font>","21ab5978":"####  Marriage Condition","9f86af76":"#### Having a Work Phone or not","271c100e":"#### Annual Income\nbins the data based on sample quantiles","a4dd13e5":"Relationship between IV value and predictive power\n\n| IV| Ability to predict | \n|:------|:------:| \n| <0.02 | Almost no predictive power | \n|0.02~0.1 |weak predictive power|\n|0.1~0.3|Moderate predictive power|\n|0.3~0.5|Strong predictive power|\n|>0.5|Predictive power is too strong, need to check variables| ","957a9b37":"<font size=3 > Please upvote it if you like it! <\/font>","83ddd8f1":"Showing important features:","989d5013":"+ After over sampling, the number between 1 and 0 is balanced. It can be seen from the confusion matrix.","ac17e4a1":"#### Education","4d64aa77":"## LightGBM","f59844a1":"<h1>Table of Content<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Feature Engineering<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Response-Variable\" data-toc-modified-id=\"Response-Variable-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Response Variable<\/a><\/span><\/li><li><span><a href=\"#Features\" data-toc-modified-id=\"Features-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Features<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Binary-Features\" data-toc-modified-id=\"Binary-Features-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;<\/span>Binary Features<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Gender\" data-toc-modified-id=\"Gender-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;<\/span>Gender<\/a><\/span><\/li><li><span><a href=\"#Having-a-car-or-not\" data-toc-modified-id=\"Having-a-car-or-not-1.2.1.2\"><span class=\"toc-item-num\">1.2.1.2&nbsp;&nbsp;<\/span>Having a car or not<\/a><\/span><\/li><li><span><a href=\"#Having-house-reality-or-not\" data-toc-modified-id=\"Having-house-reality-or-not-1.2.1.3\"><span class=\"toc-item-num\">1.2.1.3&nbsp;&nbsp;<\/span>Having house reality or not<\/a><\/span><\/li><li><span><a href=\"#Having-a-phone-or-not\" data-toc-modified-id=\"Having-a-phone-or-not-1.2.1.4\"><span class=\"toc-item-num\">1.2.1.4&nbsp;&nbsp;<\/span>Having a phone or not<\/a><\/span><\/li><li><span><a href=\"#Having-an-email-or-not\" data-toc-modified-id=\"Having-an-email-or-not-1.2.1.5\"><span class=\"toc-item-num\">1.2.1.5&nbsp;&nbsp;<\/span>Having an email or not<\/a><\/span><\/li><li><span><a href=\"#Having-a-Work-Phone-or-not\" data-toc-modified-id=\"Having-a-Work-Phone-or-not-1.2.1.6\"><span class=\"toc-item-num\">1.2.1.6&nbsp;&nbsp;<\/span>Having a Work Phone or not<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Continuous-Variables\" data-toc-modified-id=\"Continuous-Variables-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;<\/span>Continuous Variables<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Children-Numbers\" data-toc-modified-id=\"Children-Numbers-1.2.2.1\"><span class=\"toc-item-num\">1.2.2.1&nbsp;&nbsp;<\/span>Children Numbers<\/a><\/span><\/li><li><span><a href=\"#Annual-Income\" data-toc-modified-id=\"Annual-Income-1.2.2.2\"><span class=\"toc-item-num\">1.2.2.2&nbsp;&nbsp;<\/span>Annual Income<\/a><\/span><\/li><li><span><a href=\"#Age\" data-toc-modified-id=\"Age-1.2.2.3\"><span class=\"toc-item-num\">1.2.2.3&nbsp;&nbsp;<\/span>Age<\/a><\/span><\/li><li><span><a href=\"#Working-Years\" data-toc-modified-id=\"Working-Years-1.2.2.4\"><span class=\"toc-item-num\">1.2.2.4&nbsp;&nbsp;<\/span>Working Years<\/a><\/span><\/li><li><span><a href=\"#Famliy-Size\" data-toc-modified-id=\"Famliy-Size-1.2.2.5\"><span class=\"toc-item-num\">1.2.2.5&nbsp;&nbsp;<\/span>Famliy Size<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Categorical-Features\" data-toc-modified-id=\"Categorical-Features-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;<\/span>Categorical Features<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Income-Type\" data-toc-modified-id=\"Income-Type-1.2.3.1\"><span class=\"toc-item-num\">1.2.3.1&nbsp;&nbsp;<\/span>Income Type<\/a><\/span><\/li><li><span><a href=\"#Occupation-Type\" data-toc-modified-id=\"Occupation-Type-1.2.3.2\"><span class=\"toc-item-num\">1.2.3.2&nbsp;&nbsp;<\/span>Occupation Type<\/a><\/span><\/li><li><span><a href=\"#House-Type\" data-toc-modified-id=\"House-Type-1.2.3.3\"><span class=\"toc-item-num\">1.2.3.3&nbsp;&nbsp;<\/span>House Type<\/a><\/span><\/li><li><span><a href=\"#Education\" data-toc-modified-id=\"Education-1.2.3.4\"><span class=\"toc-item-num\">1.2.3.4&nbsp;&nbsp;<\/span>Education<\/a><\/span><\/li><li><span><a href=\"#Marriage-Condition\" data-toc-modified-id=\"Marriage-Condition-1.2.3.5\"><span class=\"toc-item-num\">1.2.3.5&nbsp;&nbsp;<\/span>Marriage Condition<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#IV\u3001WOE\uff1aConcept-and-Application\" data-toc-modified-id=\"IV\u3001WOE\uff1aConcept-and-Application-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;<\/span>IV\u3001WOE\uff1aConcept and Application<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Algorithms\" data-toc-modified-id=\"Algorithms-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Algorithms<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Logistic Regression<\/a><\/span><\/li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Decision Tree<\/a><\/span><\/li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Random Forest<\/a><\/span><\/li><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;<\/span>SVM<\/a><\/span><\/li><li><span><a href=\"#LightGBM\" data-toc-modified-id=\"LightGBM-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;<\/span>LightGBM<\/a><\/span><\/li><li><span><a href=\"#Xgboost\" data-toc-modified-id=\"Xgboost-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;<\/span>Xgboost<\/a><\/span><\/li><li><span><a href=\"#Keras-Neural-Networks\" data-toc-modified-id=\"Keras-Neural-Networks-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;<\/span>Keras Neural Networks<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","5c0196f4":"## Random Forest   \n\n\n\n<center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https:\/\/d1rwhvwstyk9gu.cloudfront.net\/2019\/03\/Random-Forest-Algorithm.jpg\">\n    <br>\n    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n    display: inline-block;\n    color: #999;\n    padding: 2px;\">Random Forest<\/div>\n<\/center>","7ae2bb3f":"#### Age\nBucketing Continuous Variables","819d7a9e":"## SVM\n\n\n<center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https:\/\/i.loli.net\/2019\/11\/13\/fryWG5al7OPHDiA.gif\">\n    <br>\n    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n    display: inline-block;\n    color: #999;\n    padding: 2px;\">Support Vector Machine<\/div>\n<\/center>","8d679f10":"### Continuous Variables\n\n#### Children Numbers","b969358b":"#### Having an email or not","6df28ca2":"#### Occupation Type","ec0bf394":"#### Having house reality or not","6467e78b":"#### Having a car or not","1032765b":"Weight of Evidence(WoE):  \n\n$$wo{e_i} = \\ln {{{P_{yi}}} \\over {{P_{ni}}}} = \\ln {{{y_i}\/{y_s}} \\over {{n_i}\/{n_s}}}$$\n$wo{e_i}$ is the I category's WOE value. ${{P_{yi}}}$ is the proportion of the positive samples in this category to all positive samples.   ${{P_{ni}}}$ is the ratio of negative samples (${{n_i}}$) in this class to all negative samples (${{n_s}}$).\n\nInformation Value (IV):  \n$$I{V_i} = ({P_{yi}} - {P_{ni}}) \\times wo{e_i}$$  \nThe IV values of the various types are the difference between the conditional positive rate and the conditional negative rate multiplied by the WOE value of the variable. The total IV value of the variable can be understood as the weighted sum of the conditional positive rate and the conditional negative rate difference:\n$$IV = \\sum\\limits_i^n {I{V_i}} $$  \n\nThe IV value measures the variable's ability to predict.\n","5f7f00ca":"## Xgboost","30e3f1fc":"### Categorical Features","97f62d73":"#### Gender","e12091dd":"# Algorithms","0cab9c57":"#### Income Type","8421a38e":"+ Using Synthetic Minority Over-Sampling Technique(`SMOTE`) to overcome sample imbalance problem.","6424fc13":"## Response Variable","40245af3":"#### House Type","d4ac9c52":"+ rename ","26b892ca":"### Binary Features","3ad12b88":"+ Split Dataset","edd170bf":"Generally, users in risk should be in 3%, thus I choose users who overdue for more than 60 days as target risk users. Those samples are marked as '1', else are '0'."}}