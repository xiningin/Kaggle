{"cell_type":{"cc5533f7":"code","d293e771":"code","bb6276d8":"code","22525b71":"code","0381940b":"code","29ed07e9":"code","fb242822":"code","b4370710":"markdown","2ea0be21":"markdown","ec0881bf":"markdown","a3f210c3":"markdown"},"source":{"cc5533f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\ndf = pd.read_csv('\/kaggle\/input\/customer-support-on-twitter\/twcs\/twcs.csv',  \n                 usecols=[\n                     'author_id', \n                     'inbound', \n                     'text', \n                     'in_response_to_tweet_id'],\n                 dtype={\n                     'author_id': str,\n                     'inbound': np.bool_,\n                     'text': str,\n                     'in_response_to_tweet_id': pd.Int64Dtype()\n                 })\ndf.head(10)","d293e771":"start_discussion_indices = df.index[df['in_response_to_tweet_id'].isna()].tolist()\n\nconversations = []\nlast_end_of_conversation = 0\n\n# Split the dataframe at every indices in the start_discussion_indices\nfor index in start_discussion_indices:\n    conversations.append(df[last_end_of_conversation : index + 1].drop('in_response_to_tweet_id', axis=1)) # Don't need the in response to tweet id column anymore\n    last_end_of_conversation = index + 1\n","bb6276d8":"for conversation in conversations:\n    lastInbound = None\n    rows_to_remove = []\n    messages_to_append = {}\n    for index, row in conversation.iterrows():\n        inbound = row['inbound']\n        message = row['text']\n        \n        # Two concecutive messages from the same person\n        if lastInbound is not None and (lastInbound == inbound):\n            # Remove this row \n            rows_to_remove.append(index)\n            if not index - 1 in messages_to_append.keys():\n                messages_to_append[index - 1] = []\n                \n            messages_to_append[index - 1].append(message)\n            pass\n\n        lastInbound = inbound\n        \n\n    for index, message_to_add in messages_to_append.items():\n        conversation.at[index, 'text'] = conversation.loc[index]['text'] + '. ' + message_to_add[0]\n        \n    conversation.drop(rows_to_remove, axis=0, inplace=True)","22525b71":"import re\nimport string\nfrom langdetect import detect\n\ntagging_regex = re.compile(r\"@\\S*\")\nurl_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\nsignature_pattern = re.compile(r\"-\\S*\")\nweird_thing_pattern = re.compile(r\"\\^\\S*\")\nnew_line_pattern = re.compile(r\"\\n+\\S*\")\n\nchat_words = {\n    \"AFAIK\": \"As Far As I Know\",\n    \"AFK\": \"Away From Keyboard\",\n    \"ASAP\": \"As Soon As Possible\",\n    \"ATK\": \"At The Keyboard\",\n    \"ATM\": \"At The Moment\",\n    \"A3\": \"Anytime, Anywhere, Anyplace\",\n    \"BAK\": \"Back At Keyboard\",\n    \"BBL\": \"Be Back Later\",\n    \"BBS\": \"Be Back Soon\",\n    \"BFN\": \"Bye For Now\",\n    \"B4N\": \"Bye For Now\",\n    \"BRB\": \"Be Right Back\",\n    \"BRT\": \"Be Right There\",\n    \"BTW\": \"By The Way\",\n    \"B4\": \"Before\",\n    \"B4N\": \"Bye For Now\",\n    \"CU\": \"See You\",\n    \"CUL8R\": \"See You Later\",\n    \"CYA\": \"See You\",\n    \"FAQ\": \"Frequently Asked Questions\",\n    \"FC\": \"Fingers Crossed\",\n    \"FWIW\": \"For What It's Worth\",\n    \"FYI\": \"For Your Information\",\n    \"GAL\": \"Get A Life\",\n    \"GG\": \"Good Game\",\n    \"GN\": \"Good Night\",\n    \"GMTA\": \"Great Minds Think Alike\",\n    \"GR8\": \"Great!\",\n    \"G9\": \"Genius\",\n    \"IC\": \"I See\",\n    \"ICQ\": \"I Seek you (also a chat program)\",\n    \"ILU\": \"ILU: I Love You\",\n    \"IMHO\": \"In My Honest\/Humble Opinion\",\n    \"IMO\": \"In My Opinion\",\n    \"IOW\": \"In Other Words\",\n    \"IRL\": \"In Real Life\",\n    \"KISS\": \"Keep It Simple, Stupid\",\n    \"LDR\": \"Long Distance Relationship\",\n    \"LMAO\": \"Laugh My A.. Off\",\n    \"LOL\": \"Laughing Out Loud\",\n    \"LTNS\": \"Long Time No See\",\n    \"L8R\": \"Later\",\n    \"MTE\": \"My Thoughts Exactly\",\n    \"M8\": \"Mate\",\n    \"NRN\": \"No Reply Necessary\",\n    \"OIC\": \"Oh I See\",\n    \"PITA\": \"Pain In The A..\",\n    \"PRT\": \"Party\",\n    \"PRW\": \"Parents Are Watching\",\n    \"ROFL\": \"Rolling On The Floor Laughing\",\n    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n    \"ROTFLMAO\": \"Rolling On The Floor Laughing My Ass Off\",\n    \"SK8\": \"Skate\",\n    \"STATS\": \"Your sex and age\",\n    \"ASL\": \"Age, Sex, Location\",\n    \"THX\": \"Thank You\",\n    \"TTFN\": \"Ta-Ta For Now!\",\n    \"TTYL\": \"Talk To You Later\",\n    \"U\": \"You\",\n    \"U2\": \"You Too\",\n    \"U4E\": \"Yours For Ever\",\n    \"WB\": \"Welcome Back\",\n    \"WTF\": \"What The F...\",\n    \"WTG\": \"Way To Go!\",\n    \"WUF\": \"Where Are You From?\",\n    \"W8\": \"Wait\",\n    \"IMMA\": \"I am going to\",\n    \"2NITE\": \"tonight\",\n    \"DMED\": \"mesaged\",\n    'DM': \"message\",\n    \"SMH\": \"I am dissapointed\"\n}\n\n# Thanks to https:\/\/stackoverflow.com\/a\/43023503\/3971619\ncontractions = {\n    \"ain't\": \"are not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he shall have \/ he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"i'd\": \"I would\",\n    \"i'd've\": \"I would have\",\n    \"i'll\": \"I will\",\n    \"i'll've\": \"I will have\",\n    \"i'm\": \"I am\",\n    \"i've\": \"I have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so is\",\n    \"that'd\": \"that had\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'll\": \"you will\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\",\n}\n\n# Reference : https:\/\/stackoverflow.com\/a\/49986645\/3971619\ndef remove_emoji(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\n\n# Thanks to user sudalairajkumar\ndef remove_url(string):\n    return url_pattern.sub(r'', string)\n\ndef remove_chat_words_and_contractions(string):\n    new_text = []\n    for word in string.split(' '):\n        if word.upper() in chat_words.keys():\n            new_text += chat_words[word.upper()].lower().split(' ')\n        if word.lower() in contractions.keys():\n            new_text += contractions[word.lower()].split(' ')\n        else:\n            new_text.append(word)\n            \n    return ' '.join(new_text)\n\ndef remove_signature(text):\n    return signature_pattern.sub(r'', text)\n    \n\n# Thanks to user sudalairajkumar\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndef clean_message(message):\n    # Remove user taggings\n    message = re.sub(tagging_regex, '', message) # Replace by you. Good idea?\n    \n    # Remove the emojis\n    message = remove_emoji(message)\n    \n    # Remove urls\n    message = remove_url(message)\n    \n    # Remove signatures\n    message = remove_signature(message)\n    \n    # Remove the chat words and contractions\n    message = remove_chat_words_and_contractions(message)\n    \n    # Remove weird things\n    message = weird_thing_pattern.sub(r'', message)\n\n    # Change new line to dot\n    message = new_line_pattern.sub(r'.', message)\n    \n    # Remove punctuation\n    message = remove_punctuation(message)\n    \n    # Remove start and end whitespace\n    message = message.strip()\n    \n    # Make multiple spaces become a single space\n    message = ' '.join(message.split())\n    \n    # Lower case the message\n    message = message.lower()\n    \n    # If not in english, return empty string\n#     if message and len(message) > 15:\n#         if detect(message) != 'en':\n#             return \"\"\n    \n    return message\n\nfor conversation in conversations:\n    conversation['cleaned_text'] = conversation.apply(lambda row: clean_message(row['text']), axis=1)\n    ","0381940b":"# Keep only the conversations with more than 2 nonempty messages\n# Also keep only the conversations with even number of messages\nconversations = [x for x in conversations if len(x[x['cleaned_text'] == '']) == 0 and len(x) % 2 == 0]\n\nfor index, conversation in enumerate(conversations):\n    conversation.drop(['text'], axis=1, inplace=True)\n    # Add an ID to each conversation because I will put them all back together\n    conversation[\"ID\"] = index\n    \n    # Remove unececery columns\n    conversation.drop(['author_id', 'inbound'], inplace=True, axis=1)\n    \n    # Reverse the conversation\n    conversation.sort_index(axis=0 ,ascending=False, inplace=True)\n    \nfull_conversations = pd.concat(conversations)\nfull_conversations = full_conversations[[\"ID\", \"cleaned_text\"]]\n\n","29ed07e9":"questions = full_conversations.iloc[::2]['cleaned_text'].tolist()\nids = full_conversations.iloc[::2]['ID'].tolist()\nresponses = full_conversations.iloc[1::2]['cleaned_text'].tolist()\n\n# Make all the fields the same size\nmin_length = min(len(questions), len(responses), len(ids))\nquestions = questions[:min_length]\nids = ids[:min_length]\nresponses = responses[:min_length]\n\nnew_full_df = pd.DataFrame({\n    'ID': ids,\n    'question': questions,\n    'response': responses\n})","fb242822":"# todo: Save\nnew_full_df.to_csv(\"\/kaggle\/working\/cleaned_data.csv\", index=False)","b4370710":"# First I need to process the data","2ea0be21":"I will concatenate concecutive tweets from the same user into a single tweet:","ec0881bf":"Now for cleaning the data:","a3f210c3":"I need to split the dataset into discussion"}}