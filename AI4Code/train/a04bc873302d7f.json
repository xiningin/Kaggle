{"cell_type":{"f3cb0232":"code","cc4defc7":"code","7415e1fd":"code","7c546755":"code","90770a7e":"code","59729ded":"code","8c38b668":"code","13149567":"code","92e03acf":"code","e725b463":"code","341d4701":"code","349ae00d":"code","07980d9e":"code","cfd6b1dd":"code","c29abc48":"code","02c77d63":"code","c9e3d66f":"code","792f1f6f":"code","19d37114":"code","e64c0e70":"code","e5a50ea4":"code","1256e7ef":"code","336509ed":"code","89eaf1af":"code","578df9ef":"markdown"},"source":{"f3cb0232":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cc4defc7":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","7415e1fd":"print(df_train.shape)\nprint(df_test.shape)","7c546755":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\n\n# Figures inline and set visualization style\n%matplotlib inline\nsns.set()","90770a7e":"df_train.info()","59729ded":"df_train.head()","8c38b668":"df_train.describe()","13149567":"sns.countplot(x='Survived', data=df_train);\n","92e03acf":"sns.factorplot(x='Survived', col='Sex', kind='count', data=df_train);\n","e725b463":"df_train.groupby(['Sex']).Survived.sum()\n","341d4701":"sns.factorplot(x='Survived', col='Pclass', kind='count', data=df_train);\n","349ae00d":"sns.factorplot(x='Survived', col='Embarked', kind='count', data=df_train);\n","07980d9e":"sns.distplot(df_train.Fare, kde=False);\n","cfd6b1dd":"df_train.groupby('Survived').Fare.hist(alpha=0.6);\n","c29abc48":"sns.distplot(df_train.dropna().Age, kde=False);\n","02c77d63":"sns.swarmplot(x='Survived', y='Fare', data=df_train);\n","c9e3d66f":"df_train.info()","792f1f6f":"df_train['Title']=df_train.Name.str.extract(r'(\\w+)\\.')\n","19d37114":"\ndef get_features(df):\n    \n    #Age Normalize\n    df['Age'].fillna(df['Age'].median(),inplace=True)\n    df['AgeBin'] = pd.cut(df['Age'].astype(int), 5)\n\n    #FamilySize SibSp+Parch\n    df['FamilySize']=df['SibSp']+df['Parch']+1\n    \n    #Extract Title from Name\n    df['Title']=df.Name.str.extract(r'(\\w+)\\.')\n    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n    df['Title'] = df['Title'].replace('Ms', 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n    \n    #Embarked\n    df['Embarked'] = df['Embarked'].fillna('mode')\n    \n    #FareBin\n    df['Fare'].fillna(df['Fare'].median(),inplace=True)\n    df['FareBin'] = pd.qcut(df['Fare'], 4)\n    \n    #keep Pclass\n    return df.loc[:,['Pclass','Sex','FamilySize','Title','Embarked','AgeBin','FareBin']]\n    \n                   \n                   ","e64c0e70":"X_train=get_features(df_train)\nX_test=get_features(df_test)","e5a50ea4":"X_train['Sex'].values.reshape(-1,1).shape","1256e7ef":"\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n\n#one_hot encoding\nfor col in ['Sex','Title','Embarked']:\n    encoder = OneHotEncoder()\n    X_train= pd.concat([X_train,  pd.DataFrame(encoder.fit_transform(X_train[col].values.reshape(-1,1)).toarray(),columns=encoder.get_feature_names())],axis=1)\n    X_train.drop(col,axis=1,inplace=True)\n    X_test= pd.concat([X_test, pd.DataFrame(encoder.transform(X_test[col].values.reshape(-1,1)).toarray(),columns=encoder.get_feature_names())],axis=1)\n    X_test.drop(col,axis=1,inplace=True)\n\nfor col in ['AgeBin','FareBin']: \n    label = LabelEncoder()\n    label.fit(X_train[col].append(X_test[col]))\n    X_train[col]=label.transform(X_train[col])\n    X_test[col]=label.transform(X_test[col])\n\n\n#scaler\n# scaler = MinMaxScaler()\n# X_train[['Age']]=scaler.fit_transform(X_train[['Age']])\n# X_test[['Age']]=scaler.transform(X_test[['Age']])\n    \nprint(X_train.columns)\n\n\ny_train=df_train['Survived']","336509ed":"\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import KFold, GridSearchCV,train_test_split\nfrom xgboost import XGBClassifier\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\ngrid_n_estimator = [10, 50, 100, 300]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\nmodels=[\n    {\n        \"name\":ensemble.AdaBoostClassifier(),\n        \"param_grid\":\n        {\n            #AdaBoostClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html\n            'n_estimators': grid_n_estimator, #default=50\n            'learning_rate': grid_learn, #default=1\n            'random_state': grid_seed\n        }\n    },\n    {\n        \"name\":XGBClassifier(),\n        \"param_grid\":\n            {\n            #XGBClassifier - http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n            'learning_rate': grid_learn, #default: .3\n            'max_depth': [1,2,4,6,8,10], #default 2\n            'n_estimators': grid_n_estimator, \n            'seed': grid_seed  \n             }\n    },\n    {\n        \"name\":svm.SVC(),\n        \"param_grid\":{\n            #SVC - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC\n            #http:\/\/blog.hackerearth.com\/simple-tutorial-svm-parameter-tuning-python-r\n            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n            'C': [1,2,3,4,5], #default=1.0\n            'gamma': grid_ratio, #edfault: auto\n            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n            'probability': [True],\n            'random_state': grid_seed\n             }\n    },\n    {\n        \"name\":ensemble.RandomForestClassifier(),\n        \"param_grid\":{\n            #RandomForestClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=\u201dgini\u201d\n            'max_depth': grid_max_depth, #default=None\n            'oob_score': [True], #default=False -- 12\/31\/17 set to reduce runtime \n             ##-- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n            'random_state': grid_seed\n             }\n    },\n    {\n        \"name\":ensemble.BaggingClassifier(),\n        \"param_grid\":{\n            #BaggingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'max_samples': grid_ratio, #default=1.0\n            'random_state': grid_seed\n            }\n    },\n    {\n        \"name\": neighbors.KNeighborsClassifier(),\n        \"param_grid\":{\n            #KNeighborsClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n            'weights': ['uniform', 'distance'], #default = \u2018uniform\u2019\n            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n            }\n    },\n    {\n        \"name\": naive_bayes.GaussianNB(),\n        \"param_grid\":{\n            \n        }\n    },\n    {\n        \"name\": naive_bayes.BernoulliNB(),\n        \"param_grid\":{\n            #BernoulliNB - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n            'alpha': grid_ratio, #default: 1.0\n             }\n    },\n    {\n        \"name\": linear_model.LogisticRegressionCV(),\n        \"param_grid\":{\n            #LogisticRegressionCV - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n            'fit_intercept': grid_bool, #default: True\n            #'penalty': ['l1','l2'],\n            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n            'random_state': grid_seed\n           }\n    }\n]\n\ntrain_x, val_x, train_y, val_y = train_test_split(X_train, y_train, random_state = 0)\n\ntrained_model={}\n\nfor item in models:\n    grid_search = GridSearchCV(\n                                estimator = item['name'], param_grid = item['param_grid'], \n                                cv = KFold(n_splits=5, shuffle=True, random_state=555),\n                                scoring = 'roc_auc'\n                            )\n    \n    grid_search.fit(X_train,y_train)\n    \n    best_param = grid_search.best_params_\n    clf = item['name'].set_params(**best_param) \n\n    model=clf.fit(train_x,train_y)\n    \n    trained_model[clf.__class__.__name__]=model\n    \n    pred_y=model.predict(val_x)\n    #metrics\n    print(\"{}:{}\".format(clf.__class__.__name__,accuracy_score(train_y,model.predict(train_x))))\n    print(\"{}:{}\".format(clf.__class__.__name__,accuracy_score(val_y,pred_y)))\n    \n    \n","89eaf1af":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission['Survived'] = trained_model['XGBClassifier'].predict(X_test)\nsubmission.head()\nsubmission.to_csv('submission.csv', index=False)","578df9ef":"I took two days to go throught some higt-voted notebooks and then make my first public notebook step by step.\nBest score is 0.79904 (2019-10-25), keep learning and keep improving!\nAcknowledge for all of your great work. "}}