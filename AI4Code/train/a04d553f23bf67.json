{"cell_type":{"b0f64120":"code","565a4fb7":"code","089f0dd2":"code","e715d652":"code","edbc381b":"code","3ebb4bc3":"code","c655d8ac":"code","5036e3f7":"code","a886f15b":"code","7a0bf43f":"code","78cc2f8d":"code","68b559d9":"code","215d8dd9":"code","6a4f1390":"code","bda4c0a5":"code","6287e504":"code","2e83e99b":"markdown","a092987c":"markdown","14227d00":"markdown","8358e760":"markdown","dece63d0":"markdown","1e6ffa79":"markdown","21ae6981":"markdown","bf9547f3":"markdown","4b3b5182":"markdown","7d25dec9":"markdown","6e54e9bd":"markdown","c48eaf64":"markdown","e57377a4":"markdown","4025c0f5":"markdown"},"source":{"b0f64120":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef sigmoid(x, w, w0):\n    den = 1 + np.exp(-(x*w + w0))\n    return 1.0\/den\n\ndef plot_sigmoid(ax, w, w0):\n    x = [0.1*i for i in range(-75, 76)]\n    y = [sigmoid(x_i, w = w, w0 = w0) for x_i in x]\n    out = ax.scatter(x = x, y = y)\n    out = ax.axhline(y = 0.5, color = 'black', linestyle = '--')\n    out = ax.axhline(y = 0, color = 'black', linestyle = '--')\n    out = ax.axhline(y = 1, color = 'black', linestyle = '--')\n    out = ax.axvline(x = 0, color = 'black', linestyle = '--')\n    out = ax.set_title(\n        'One-dimensional sigmoid \\n p = sigmoid(wx + w0), w = ' + str(w) + ', w0 = ' + str(w0),\n        fontsize = 16)\n    out = ax.set_xlabel('x', fontsize = 14)\n    out = ax.set_ylabel('p(x)', fontsize = 14)\n    out = ax.grid()\n    \nfig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, nrows = 1, figsize = (18,6))\nplot_sigmoid(ax1, w = 1, w0 = 0)\nplot_sigmoid(ax2, w = 10, w0 = 40)\nplot_sigmoid(ax3, w = 0.001, w0 = 0)","565a4fb7":"import pandas as pd\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')","089f0dd2":"train_df.head()","e715d652":"train_df.describe()","edbc381b":"train_df['is_female'] = train_df['Sex'].apply(lambda x: 1 if x == 'female' else 0)","3ebb4bc3":"train_df.drop(columns = ['PassengerId','Survived', 'Age']).describe()","c655d8ac":"def plot_one_col(df0, df1, col, ax, bins):\n    ax.hist(df0[col], label = \"didn't survive\", density = True, bins = bins)\n    ax.hist(df1[col], label = \"survived\", density = True, bins = bins, alpha = 0.5)\n    ax.set_title(col, fontsize = 16)\n    ax.set_xlabel(col + ' value', fontsize = 14)\n    ax.set_ylabel('N entries per bin', fontsize = 14)\n    ax.legend(fontsize = 14)","5036e3f7":"df0 = train_df.query('Survived == 0')\ndf1 = train_df.query('Survived == 1')\nfig, ax = plt.subplots(ncols = 3, nrows = 2, figsize = (18, 12))\nplot_one_col(df0, df1, col = 'Pclass', ax = ax[0,0], bins = [0.5, 1.5, 2.5, 3.5])\nplot_one_col(df0, df1, col = 'SibSp', ax = ax[0,1], bins = [-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5])\nplot_one_col(df0, df1, col = 'Parch', ax = ax[0,2], bins = [-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5])\nplot_one_col(df0, df1, col = 'Fare', ax = ax[1,0], bins = [0, 15, 50, 75, 100, 150, 200, 300, 500])\nplot_one_col(df0, df1, col = 'is_female', ax = ax[1,1], bins = [-0.5, 0.5, 1.5])","a886f15b":"import numpy as np\nfrom sklearn.model_selection import KFold\nN_folds = 5\nkf = KFold(n_splits = N_folds, random_state = 13, shuffle = True)\nindexes = []\nfor train_index, valid_index in kf.split(train_df):\n    print(\"TRAIN:\", train_index[0:5], \"VALID:\", valid_index[0:5])\n    indexes.append({'train':train_index, 'valid':valid_index})","7a0bf43f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","78cc2f8d":"clf = LogisticRegression(random_state = 13,  solver='lbfgs')\nthreshold = 0.5","68b559d9":"def fit_on_feature_set(features):\n    valid_acc = [0] * N_folds\n    train_acc = [0] * N_folds\n    acc = [0] * N_folds\n    for fold in range(N_folds):\n        inds_t = indexes[fold]['train']\n        fold_train_df = train_df.loc[inds_t]\n        inds_v = indexes[fold]['valid']\n        fold_valid_df = train_df.loc[inds_v]\n       \n        clf.fit(fold_train_df[features], fold_train_df['Survived'])    \n        predictions_train = clf.predict_proba(fold_train_df[features])[:,1] > threshold\n        fold_train_df['predictions'] = predictions_train\n        train_acc[fold] = accuracy_score(fold_train_df['Survived'], fold_train_df['predictions'])\n    \n        clf.predict_proba(fold_valid_df[features])\n        predictions = clf.predict_proba(fold_valid_df[features])[:,1] > threshold\n        fold_valid_df['predictions'] = predictions\n        valid_acc[fold] = accuracy_score(fold_valid_df['Survived'], fold_valid_df['predictions'])\n    \n        acc[fold] = min(valid_acc[fold], train_acc[fold])\n    return acc","215d8dd9":"num_features = ['Pclass', 'SibSp', 'Parch', 'Fare', 'is_female']\nd_acc = {}\nfor feat in num_features:\n    d_acc[feat] = fit_on_feature_set([feat])\ndf_acc = pd.DataFrame(d_acc)\ndf_acc['fold'] = list(x for x in range(N_folds))\ndf_acc","6a4f1390":"colors = ['blue', 'orange', 'green', 'red', 'purple', 'black']\nfig, ax = plt.subplots(ncols = 1, nrows = 1, figsize = (12,6))\nfor i in range(len(num_features)):\n    col = num_features[i]\n    ax.scatter(\n        y = df_acc['fold'], x = df_acc[col], \n        label = col, s = 180, color = colors[i]\n    )\n    m = df_acc[col].mean()\n    s = df_acc[col].std()\/(N_folds**0.5)\n    ax.axvline(\n        x = m, \n        color = colors[i], linestyle = '--', alpha = 0.5\n    )\n    ax.axvline(\n        x = m + s, \n        color = colors[i], alpha = 0.5\n    )\n    ax.axvline(\n        x = m - s, \n        color = colors[i], alpha = 0.5\n    )\n    ax.axvspan(m-s, m+s, facecolor=colors[i], alpha=0.1)\n    ax.set_xlim(0.5, 1.0)\n    ax.set_ylabel('fold', fontsize = 20)\n    ax.set_xlabel('accuracy', fontsize = 20)\n    t1 = 'Compare log-reg models on one feature'\n    t2 = 'Accuracy score vs fold'\n    ax.set_title(t1 + '\\n' + t2, fontsize = 20)\n    ax.grid()\n    ax.legend(fontsize = 16)","bda4c0a5":"print('Mean accuracy score for one-feature based models: \\n')\nfor col in num_features:\n    print(\n        col, \n        round(df_acc[col].mean(),3),'+-',\n        round(df_acc[col].std()\/(N_folds**0.5),3)\n    )","6287e504":"clf.fit(train_df[['is_female']], train_df['Survived'])\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_df['is_female'] = test_df['Sex'].apply(lambda x: 1 if x == 'female' else 0)\npredictions = clf.predict_proba(test_df[['is_female']])[:,1] > threshold\ntest_df['Survived'] = predictions\ntest_df['Survived'] = test_df['Survived'].astype(int)\ntest_df[['PassengerId','Survived']].to_csv('submission.csv', index = False)","2e83e99b":"As one can see on the plots above, sigmoid maps any real number into a range from 0 to 1. On the left plot the weights of sigmoid are $w = 1$ and $w_0 = 0$. $w$ determines the slope of the function, and may vary the sigmoid from being a step function (center plot) to a constant within a certain range (right plot). The $w0\/w$ fraction is responsible for the shift of the sigmoid, as one can notice looking carefully at the left and the center plots. ","a092987c":"__'PassengerId'__ is irrelevant\n\n__'Survived'__ is our target\n\n__'Age'__ has missing values, let us drop it for now, so that we don't worry about the imputation\n\nIn addition, let us construct a binary feature from a categorical feature __'Sex'__:","14227d00":"Let us look at numerical features","8358e760":"- - -\n### Predcit Titanic Survival\n\nTitanic survival prediction is [a getting started competition on kaggle](https:\/\/www.kaggle.com\/c\/titanic) very much loved by beginners as well as by more experienced data scientists willing to test new tools and approaches on this well known dataset.\n\nThe objective of the competition is to predict whether a given passenger in the test dataset survived or didn't survive (1 or 0) with accuracy as a performance metric. We will \n- start with a brief EDA, \n- train several one-feature models, and \n- identify possible steps for improvement and further learning.","dece63d0":"#### Cost Function\n\nTo find the values of $w$ and $w_0$ that best describe the observed data, the machine learning algorithm minimizes \n\nlog loss $ = - \\sum_{i = 1}^{N} (y_i ln(p_i) + (1 - y_i) ln(1 - p_i))$, where\n\n$N$ is a number of objects in the train data,\n\n$y_i$ are real labels of a target variable (0 or 1),\n\nand $p_i = sigmoid(x_i, w, w_0)$ for given $x_i$\n\n- - - \nOne of the most popular implementations of logistic regression in python is [LogisticClassifier by sklearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html), and we are going to use it to predict who of Titanic passengers survived the tragedy, and who didn't.","1e6ffa79":"- - -\n## One-Dimensional Logistic Regression\n\n### Background\n\nFor a binary classification tasks, the target variable is $y = 0$ or $y = 1$ \n\nOne of the possible ways to predict the probability of an object to belong to class 1 is to fit $y(x)$ by sigmoid which is in a one-dimensional space takes a form of\n\n$p(x) = sigmoid(wx + w_0) = \\frac{1}{1 + exp(-(wx + w_0))}$,\n\nwhere \n- $x$ is a value of a known variable (our single feature), \n- $p$ is a probability that an object with a given value $x$ of feature belong to class $1$, and\n- $w$, $w_0$ are fit parameters \n\n__Let us plot a one-dimensional sigmoid__","21ae6981":"### Setting up the Cross Validation","bf9547f3":"- - -\n## Sources for More In-Depth Study\n\n#### Theory\n- video lectures at [statquests](https:\/\/www.youtube.com\/channel\/UCtYLUTtgS3k1Fg4y5tAhLbw), playlist Logistic Regression \n- for Russian speakers only: \u043a\u0443\u0440\u0441  \u043d\u0430 Coursera [\"\u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0432 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\"](https:\/\/ru.coursera.org\/learn\/vvedenie-mashinnoe-obuchenie) \u043e\u0442 \u0428\u0410\u0414 \u0438 \u0412\u0428\u042d, \u043d\u0430 \u044d\u0442\u043e\u043c \u043a\u0443\u0440\u0441\u0435 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0433\u043b\u0443\u0431\u043e\u043a\u043e \u0434\u0430\u0451\u0442\u0441\u044f \u0442\u0435\u043e\u0440\u0438\u044f. \u041f\u043e \u043c\u043e\u0438\u043c \u043e\u0449\u0443\u0449\u0435\u043d\u0438\u044f\u043c, \u0434\u043b\u044f \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u0430 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0432 \u043e\u0431\u044a\u0451\u043c\u0435 2-3 \u043b\u0435\u0442 \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u0430 (\u043c\u0430\u0442. \u0430\u043d\u0430\u043b\u0438\u0437, \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0430\u043b\u0433\u0435\u0431\u0440\u0430, \u043c\u0430\u0442. \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430)\n\n#### Practice\n- course [Python for data science and machine learning bootcamp](https:\/\/www.udemy.com\/course\/python-for-data-science-and-machine-learning-bootcamp\/). If you are not confident with python for data science libraries, I highly recommend this course to get a quick hands-on experience\n\n#### Python libraries\n- sklearn.linear_model.LogisticRegression\n- statsmodels.discrete.discrete_model.Logit","4b3b5182":"# Logistic Regression for Binary Classification \n\nWelcome to the wondeful world of logistic regression!\n\nLogistic regression is a basic machine learning algorithm mostly used for classification tasks. It's a very simple tool, great for interpretability, fast in computation, and sometimes capable to outperform much more advanced and computation-costly algorithms.\n\nWhile being an amazing classifier by itself, the logistic regression is also a core in many neural network architectures, and, thus, a good understanding of this basic algorithm is very useful for any data scientist.\n\nThis kernel gives a light introduction in the world of logistic regression with one-feature based models for the famous Titanic survival classification problem.","7d25dec9":"The leaderboard score of __0.76555__ is compatible with the cross validation score","6e54e9bd":"---\n## Summary\n\nIn this kernel, we discussed the one-dimensional logistic regression, and how it is application for the Titanic survival classification model. We compared 5 one-feature-based models, and found out that a feature 'is_female' gives the best accuracy score of 0.76 both in the leaderboard and the cross validation. \n\nThis kernel's brief introductory is certainly just a beggining in the logistic regression mastering journey. Important concepts we haven't discuss in this kernel (yet?) include but are not limited by:\n\n#### for one-dimensional logistic regression\n- feature transformations\n- imputation of missing values\n- threshold selection for choosing the best accuracy score\n\n#### for multi-dimensional logistic regression\n- sigmoid and cost function in a multi-dimensional space\n- feature scaling\n- mutual correlations of features and regularization\n- feature interaction & feature engineering\n","c48eaf64":"---\n## Prepare Submission File\n\nIn this karnel we have considered 5 one-feature models. The best score was achieved when using an engineered binary feature 'is_female'\n\nLet us train the model on the whole train data, make predictions on the kaggel test dataset, and submit the result","e57377a4":"we are left __5 numerical features__, and our plan is to build a one-dimensional logistic regression based on each of them individually \n\nlet us start with looking at feature distributions","4025c0f5":"What features are available in the dataframe?"}}