{"cell_type":{"43628192":"code","82699e54":"code","707cde37":"code","c6456361":"code","62e1b072":"code","af3a9dad":"code","a5fa01b4":"code","fae29df3":"code","feee2365":"code","236c2fa0":"code","75881c7e":"code","3ce9ca5f":"code","c412055e":"code","cbd40def":"code","519e6470":"code","62fea878":"code","612fc059":"code","7b9cec6a":"code","d9dc0362":"code","aad502d2":"code","9744de47":"code","7a77056b":"code","c78d4da9":"code","a4a6c354":"code","8290363d":"code","21ec33b3":"code","4b472220":"code","fb01b593":"code","925a3868":"code","390130db":"code","8cc43167":"code","6026a156":"code","86543388":"code","a4f2cf14":"code","6b8dd8b0":"code","ed86712d":"code","38f581e1":"code","bcbdaa91":"code","5334a5f9":"code","a8290f37":"markdown","83dde71b":"markdown","5de89b28":"markdown","e200bd43":"markdown","8b67c380":"markdown","1d76660c":"markdown","44269810":"markdown","fc574e54":"markdown","bd89d1e2":"markdown","d6292f02":"markdown","06a086eb":"markdown","42a61ccf":"markdown","022e3878":"markdown","dd533068":"markdown","dfb55c64":"markdown","31a81664":"markdown","dea56aa4":"markdown","7cda5106":"markdown","2ebbceeb":"markdown","238a28b6":"markdown","5888a4a1":"markdown","4fbe9258":"markdown","56a4f5e5":"markdown","cb69bc12":"markdown"},"source":{"43628192":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","82699e54":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n# nltk.download('stopwords')\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n# nltk.download('punkt')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport re\nimport string\nimport os\nfrom collections import defaultdict\nfrom collections import Counter\n\nplt.style.use('ggplot')\nstop = set(stopwords.words('english'))\n\nimport gensim\nfrom tqdm.notebook import tqdm","707cde37":"sample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","c6456361":"train.head()","62e1b072":"train.loc[1]","af3a9dad":"def create_corpus(target):\n    corpus = []\n    \n    for x in train.loc[train['target'] == target, 'text'].str.split():\n        for i in x:\n            corpus.append(i)\n            \n    return corpus","a5fa01b4":"corpus = create_corpus(0)\n\ndic = defaultdict(int)\n\nfor word in corpus:\n    if word in stop:\n        dic[word] += 1\n        \ntop = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n\nx, y = zip(*top)\nplt.title('With no disaster')\nplt.bar(x, y)","fae29df3":"corpus = create_corpus(1)\n\ndic = defaultdict(int)\n\nfor word in corpus:\n    if word in stop:\n        dic[word] += 1\n        \ntop = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n\nx, y = zip(*top)\n\nplt.title('With disaster')\nplt.bar(x, y, color='green')","feee2365":"plt.figure(figsize=(10, 5))\ncorpus = create_corpus(0)\n\ndic = defaultdict(int)\n\nspecial = string.punctuation\nfor i in corpus:\n    if i in special:\n        dic[i] += 1\n        \nx, y = zip(*dic.items())\nplt.bar(x, y)","236c2fa0":"plt.figure(figsize=(10, 5))\ncorpus = create_corpus(0)\n\ndic = defaultdict(int)\n\nspecial = string.punctuation\nfor i in corpus:\n    if i in special:\n        dic[i] += 1\n        \nx, y = zip(*dic.items())\nplt.bar(x, y, color='green')","75881c7e":"counter = Counter(corpus)\nmost_common = counter.most_common()\n\nx = list()\ny = list()\n\nfor word, count in most_common[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y, y=x, orient='h')","3ce9ca5f":"def get_top_tweet_bigrams(corpus, n=10):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    \n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    \n    return words_freq[:n]","c412055e":"plt.figure(figsize=(10, 5))\ntop_tweet_bigrams = get_top_tweet_bigrams(train['text'])[:10]\n\nx, y = map(list, zip(*top_tweet_bigrams))\n\nsns.barplot(x=y, y=x)","cbd40def":"df = pd.concat([train, test])\ndf.shape","519e6470":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    \n    return url.sub('', text)\n\nexample = 'New competition launched: https:\/\/www.kaggle.com\/c\/nlp-getting-started'\n\nremove_URL(example)","62fea878":"df['text'] = df['text'].apply(lambda x: remove_URL(x))","612fc059":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","7b9cec6a":"def remove_html(text):\n    html = re.compile(r'<.*?>')\n    \n    return html.sub('', text)\n\nprint(remove_html(example))","d9dc0362":"df['text'] = df['text'].apply(lambda x: remove_html(x))","aad502d2":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    \n    return emoji_pattern.sub(r'', text)\n\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","9744de47":"df['text'] = df['text'].apply(lambda x: remove_emoji(x))","7a77056b":"def remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    \n    return text.translate(table)\n\nexample = \"I am #king\"\nprint(remove_punct(example))","c78d4da9":"df['text'] = df['text'].apply(lambda x: remove_punct(x))","a4a6c354":"keywords = train.keyword.unique()[1:]\nkeywords = list(map(lambda x: x.replace('%20', ' '), keywords))\n\nwnl = WordNetLemmatizer()\n\ndef lemmatize_sentence(sentence):\n    sentence_words = sentence.split(' ')\n    new_sentence_words = list()\n    \n    for sentence_word in sentence_words:\n        sentence_word = sentence_word.replace('#', '')\n        new_sentence_word = wnl.lemmatize(sentence_word.lower(), wordnet.VERB)\n        new_sentence_words.append(new_sentence_word)\n        \n    new_sentence = ' '.join(new_sentence_words)\n    new_sentence = new_sentence.strip()\n    \n    return new_sentence","8290363d":"df['text'] = df['text'].apply(lambda x: lemmatize_sentence(x))","21ec33b3":"import torch\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\n\nfrom torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import Vectors, GloVe","4b472220":"def prepare_csv(df_train, df_test, seed=27, val_ratio=0.3):\n    idx = np.arange(df_train.shape[0])\n    \n    np.random.seed(seed)\n    np.random.shuffle(idx)\n    \n    val_size = int(len(idx) * val_ratio)\n    \n    if not os.path.exists('cache'):\n        os.makedirs('cache')\n    \n    df_train.iloc[idx[val_size:], :][['id', 'target', 'text']].to_csv(\n        'cache\/dataset_train.csv', index=False\n    )\n    \n    df_train.iloc[idx[:val_size], :][['id', 'target', 'text']].to_csv(\n        'cache\/dataset_val.csv', index=False\n    )\n    \n    df_test[['id', 'text']].to_csv('cache\/dataset_test.csv',\n                   index=False)","fb01b593":"def get_iterator(dataset, batch_size, train=True,\n                 shuffle=True, repeat=False):\n    \n    device = torch.device('cuda:0' if torch.cuda.is_available()\n                          else 'cpu')\n    \n    dataset_iter = data.Iterator(\n        dataset, batch_size=batch_size, device=device,\n        train=train, shuffle=shuffle, repeat=repeat,\n        sort=False\n    )\n    \n    return dataset_iter","925a3868":"import logging\nfrom copy import deepcopy\n\nLOGGER = logging.getLogger('tweets_dataset')\n\ndef get_dataset(fix_length=100, lower=False, vectors=None):\n    \n    if vectors is not None:\n        lower=True\n        \n    LOGGER.debug('Preparing CSV files...')\n    prepare_csv(train, test)\n    \n    TEXT = data.Field(sequential=True, \n#                       tokenize='spacy', \n                      lower=True, \n                      include_lengths=True, \n                      batch_first=True, \n                      fix_length=25)\n    LABEL = data.Field(use_vocab=True,\n                       sequential=False,\n                       dtype=torch.float16)\n    ID = data.Field(use_vocab=False,\n                    sequential=False,\n                    dtype=torch.float16)\n    \n    \n    LOGGER.debug('Reading train csv files...')\n    \n    train_temp, val_temp = data.TabularDataset.splits(\n        path='cache\/', format='csv', skip_header=True,\n        train='dataset_train.csv', validation='dataset_val.csv',\n        fields=[\n            ('id', ID),\n            ('target', LABEL),\n            ('text', TEXT)\n        ]\n    )\n    \n    LOGGER.debug('Reading test csv file...')\n    \n    test_temp = data.TabularDataset(\n        path='cache\/dataset_test.csv', format='csv',\n        skip_header=True,\n        fields=[\n            ('id', ID),\n            ('text', TEXT)\n        ]\n    )\n    \n    LOGGER.debug('Building vocabulary...')\n    \n    TEXT.build_vocab(\n        train_temp, val_temp, test_temp,\n        max_size=20000,\n        min_freq=10,\n        vectors=GloVe(name='6B', dim=300)  # We use it for getting vocabulary of words\n    )\n    LABEL.build_vocab(\n        train_temp\n    )\n    ID.build_vocab(\n        train_temp, val_temp, test_temp\n    )\n    \n    word_embeddings = TEXT.vocab.vectors\n    vocab_size = len(TEXT.vocab)\n    \n    train_iter = get_iterator(train_temp, batch_size=32, \n                              train=True, shuffle=True,\n                              repeat=False)\n    val_iter = get_iterator(val_temp, batch_size=32, \n                            train=True, shuffle=True,\n                            repeat=False)\n    test_iter = get_iterator(test_temp, batch_size=32, \n                             train=False, shuffle=False,\n                             repeat=False)\n    \n    \n    LOGGER.debug('Done preparing the datasets')\n    \n    return TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter","390130db":"TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter = get_dataset()","8cc43167":"class LSTMClassifier(torch.nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, weights):\n        super(LSTMClassifier, self).__init__()\n        \n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.word_embeddings = torch.nn.Embedding(vocab_size,\n                                                  embedding_dim)\n        self.word_embeddings.weight = torch.nn.Parameter(weights,\n                                                         requires_grad=False)\n        \n        self.dropout_1 = torch.nn.Dropout(0.3)\n        self.lstm = torch.nn.LSTM(embedding_dim,\n                                  hidden_dim,\n                                  n_layers,\n                                  dropout=0.3,\n                                  batch_first=True)\n        \n        self.dropout_2 = torch.nn.Dropout(0.3)\n        self.label_layer = torch.nn.Linear(hidden_dim, output_size)\n        \n        self.act = torch.nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        \n        x = self.word_embeddings(x)\n        \n        x = self.dropout_1(x)\n        \n        lstm_out, hidden = self.lstm(x, hidden)\n                \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        out = self.dropout_2(lstm_out)\n        out = self.label_layer(out)    \n        \n        out = out.view(batch_size, -1, self.output_size)\n        out = out[:, -1, :]\n\n        out = self.act(out)\n        \n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        \n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n        \n        return hidden","6026a156":"def train_model(model, train_iter, val_iter, optim, loss, num_epochs, batch_size=32):\n    h = model.init_hidden(batch_size)\n    \n    clip = 5\n    val_loss_min = np.Inf\n    \n    total_train_epoch_loss = list()\n    total_train_epoch_acc = list()\n        \n    total_val_epoch_loss = list()\n    total_val_epoch_acc = list()\n        \n    \n    device = torch.device('cuda:0' if torch.cuda.is_available()\n                           else 'cpu')\n    \n    for epoch in range(num_epochs):\n\n        model.train()\n        \n        train_epoch_loss = list()\n        train_epoch_acc = list()\n        \n        val_epoch_loss = list()\n        val_epoch_acc = list()\n        \n        for idx, batch in enumerate(tqdm(train_iter)):\n            h = tuple([e.data for e in h])\n\n            text = batch.text[0]\n            target = batch.target\n            target = target - 1\n            target = target.type(torch.LongTensor)\n\n            text = text.to(device)\n            target = target.to(device)\n\n            optim.zero_grad()\n            \n            if text.size()[0] is not batch_size:\n                continue\n            \n            prediction, h = model(text, h)\n                \n            loss_train = loss(prediction.squeeze(), target)\n            loss_train.backward()\n\n            num_corrects = (torch.max(prediction, 1)[1].\n                                view(target.size()).data == target.data).float().sum()\n\n            acc = 100.0 * num_corrects \/ len(batch)\n\n            train_epoch_loss.append(loss_train.item())\n            train_epoch_acc.append(acc.item())\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n            \n            optim.step()\n    \n        print(f'Train Epoch: {epoch}, Training Loss: {np.mean(train_epoch_loss):.4f}, Training Accuracy: {np.mean(train_epoch_acc): .2f}%')\n\n        model.eval()\n\n        with torch.no_grad():\n            for idx, batch in enumerate(tqdm(val_iter)):\n                val_h = tuple([e.data for e in h])\n\n                text = batch.text[0]\n                target = batch.target\n                target = target - 1\n                target = target.type(torch.LongTensor)\n                \n                text = text.to(device)\n                target = target.to(device)\n                \n                if text.size()[0] is not batch_size:\n                    continue\n\n                prediction, h = model(text, h)\n                loss_val = loss(prediction.squeeze(), target)\n\n                num_corrects = (torch.max(prediction, 1)[1].\n                                view(target.size()).data == target.data).float().sum()\n\n                acc = 100.0 * num_corrects \/ len(batch)\n\n                val_epoch_loss.append(loss_val.item())\n                val_epoch_acc.append(acc.item())\n                \n            print(f'Vadlidation Epoch: {epoch}, Training Loss: {np.mean(val_epoch_loss):.4f}, Training Accuracy: {np.mean(val_epoch_acc): .2f}%')\n                \n            if np.mean(val_epoch_loss) <= val_loss_min:\n#                 torch.save(model.state_dict(), 'state_dict.pth')\n                print('Validation loss decreased ({:.6f} --> {:.6f})'.\n                      format(val_loss_min, np.mean(val_epoch_loss)))\n                \n                val_loss_min = np.mean(val_epoch_loss)\n                \n        total_train_epoch_loss.append(np.mean(train_epoch_loss))\n        total_train_epoch_acc.append(np.mean(train_epoch_acc))\n    \n        total_val_epoch_loss.append(np.mean(val_epoch_loss))\n        total_val_epoch_acc.append(np.mean(val_epoch_acc))\n    \n    return (total_train_epoch_loss, total_train_epoch_acc,\n            total_val_epoch_loss, total_val_epoch_acc)","86543388":"lr = 1e-4\nbatch_size = 32\noutput_size = 2\nhidden_size = 128\nembedding_length = 300\n\nmodel = LSTMClassifier(vocab_size=vocab_size, \n                       output_size=output_size, \n                       embedding_dim=embedding_length,\n                       hidden_dim=hidden_size,\n                       n_layers=2,\n                       weights=word_embeddings\n)\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available()\n                      else 'cpu')\n    \nmodel.to(device)\noptim = torch.optim.Adam(model.parameters(), lr=lr)\nloss = torch.nn.CrossEntropyLoss()\n    \ntrain_loss, train_acc, val_loss, val_acc = train_model(model=model,\n                                                       train_iter=train_iter,\n                                                       val_iter=val_iter,\n                                                       optim=optim,\n                                                       loss=loss,\n                                                       num_epochs=20,\n                                                       batch_size=batch_size)\n    ","a4f2cf14":"plt.figure(figsize=(10, 6))\nplt.title('Loss')\nsns.lineplot(range(len(train_loss)), train_loss, label='train')\nsns.lineplot(range(len(val_loss)), val_loss, label='test')","6b8dd8b0":"plt.figure(figsize=(10, 6))\nplt.title('Accuracy')\nsns.lineplot(range(len(train_acc)), train_acc, label='train')\nsns.lineplot(range(len(val_acc)), val_acc, label='test')","ed86712d":"results_target = list()\n\nwith torch.no_grad():\n    for batch in tqdm(test_iter):\n        for text, idx in zip(batch.text[0], batch.id):\n            text = text.unsqueeze(0)\n            res, _ = model(text, hidden=None)\n\n            target = np.round(res.cpu().numpy())\n            \n            results_target.append(target[0][1])","38f581e1":"sample_submission['target'] = list(map(int, results_target))","bcbdaa91":"sample_submission.head()","5334a5f9":"sample_submission.to_csv('submission.csv', index=False)","a8290f37":"Now <i>in<\/i> is almost on the first plays. During disaster we try to explain where is it.","83dde71b":"1 - real disaster\n<br>\n0 - no disaster","5de89b28":"## Data Cleaning","e200bd43":"If we want to use torchtext we should save <i>train<\/i>, <i>test<\/i> and <\/i>validation<\/i> datasets into separated files.","8b67c380":"### Most common words","1d76660c":"## PyTorch dataset","44269810":"We can remove URLs, HTML tags and emojis. It is hard to get information from them.","fc574e54":"Punctuation is almost the same. Maybe we can delete it.","bd89d1e2":"## EDA","d6292f02":"## PyTorch","06a086eb":"For embeddings we are using [GloVe](https:\/\/github.com\/maciejkula\/glove-python):\n\n> Glove produces dense vector embeddings of words, where words that occur together are close in the resulting vector space.\n> <br><br> While this produces embeddings which are similar to word2vec (which has a great python implementation in gensim), the method is different: GloVe produces embeddings by factorizing the logarithm of the corpus word co-occurrence matrix.","42a61ccf":"### Punctuation","022e3878":"Good thing to do is [lemmatizing](https:\/\/en.wikipedia.org\/wiki\/Lemmatisation). We can do it using [nltk](http:\/\/www.nltk.org\/book\/) library.\n\n> Lemmatization in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.","dd533068":"Method for wrapping TabularDataset into iterator. So, we can iterate threw dataset.","dfb55c64":"Let's concatenate <i>train<\/i> and <i>test<\/i> datasets.","31a81664":"### Bigrams","dea56aa4":"Method for network training. ","7cda5106":"## Predictions","2ebbceeb":"## Real or Not? NLP with Disaster Tweets\n\nIn this kernel you can find Basic EDA of Disaster Tweet dataset. Simple lemmatization using WordNetLemmatizer from nltk. Creation of dataset using torchtext. And LSTM implementation using PyTorch. Let's have some fun!\n\nWhat's in this kernel?\n* [Basic EDA](#EDA)\n* [Data Cleaning](#Data-Cleaning)\n* [PyTorch dataset](#PyTorch-dataset)\n* [PyTorch Model](#PyTorch-Model)\n* [PyTorch train](#PyTorch-train)\n* [Predictions](#Predictions)\n\n### Acknowledgments\n\n1. [Basic EDA,Cleaning and GloVe](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove#Exploratory-Data-Analysis-of-tweets) - For such a great EDA and cleaning\n2. [Text-Classification-Pytorch](https:\/\/github.com\/prakashpandey9\/Text-Classification-Pytorch) - For LSTM model","238a28b6":"For this task we will try to use LSTM network.","5888a4a1":"The main words are articles. It makes sense.","4fbe9258":"## Imports","56a4f5e5":"## PyTorch train","cb69bc12":"## PyTorch Model"}}