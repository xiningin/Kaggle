{"cell_type":{"f68ce9ff":"code","f210059a":"code","e592379e":"code","428ca047":"code","f25cb85d":"code","5322b300":"code","7bbe1a68":"code","316d08d8":"code","31bbc4ad":"code","e306e98e":"code","4a52e13a":"code","956d224e":"code","24d1899d":"code","98a7afe1":"code","9bbbb45d":"code","0560e739":"code","dcc9f37a":"code","9ada4f2d":"code","4354fab3":"code","c3298f3d":"code","e64a9368":"code","82519c50":"code","d1877f0b":"code","bd664501":"code","73652560":"code","55e3ade7":"code","f498d675":"markdown","e2da4033":"markdown","6a9e09be":"markdown","8d09e41b":"markdown","ab743664":"markdown","21b9a7fa":"markdown","3eeb0c71":"markdown","31ea7785":"markdown","5606e187":"markdown","ee83b7ad":"markdown","acd04c95":"markdown","174baf1f":"markdown","722d4d0d":"markdown","8bc3e150":"markdown","cb2c8be3":"markdown","d452b5bd":"markdown","630759fd":"markdown","a24ff2e4":"markdown","3abc76b4":"markdown"},"source":{"f68ce9ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datatable as dt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f210059a":"%%time\nexample_sample_submission = dt.fread(\"\/kaggle\/input\/jane-street-market-prediction\/example_sample_submission.csv\")\nfeatures = dt.fread(\"\/kaggle\/input\/jane-street-market-prediction\/features.csv\")\nexample_test = dt.fread(\"\/kaggle\/input\/jane-street-market-prediction\/example_test.csv\")\ntrain = dt.fread(\"\/kaggle\/input\/jane-street-market-prediction\/train.csv\")","e592379e":"example_sample_submission = example_sample_submission.to_pandas()\nfeatures = features.to_pandas()\nexample_test = example_test.to_pandas()\ntrain = train.to_pandas()","428ca047":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage. Credits to Guillaume Martin  \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","f25cb85d":"%%time\n\nfor name, ds in zip([\"submission\",\"features\",\"train\",\"test\"],[example_sample_submission, features, train, example_test]):\n    print(\"{} \\n\".format(name))\n    ds = reduce_mem_usage(ds)\n    print(\"\\n\")","5322b300":"for name, ds in zip([\"submission\",\"features\",\"train\",\"test\"],[example_sample_submission, features, train, example_test]):\n    print(\"dataset {} \\n\".format(name))\n    print(\"{}\".format(ds.info()))\n    print(\"\\n\")","7bbe1a68":"#missing values\nfor name, ds in zip([\"submission\",\"features\",\"train\",\"test\"],[example_sample_submission, features, train, example_test]):\n    print(\"dataset {} \\n\".format(name))\n    print(\"{} \\n\".format(ds.isnull().sum().sort_values(ascending=False)))\n    print(\"\\n\")","316d08d8":"pd.concat([train.head(), train.tail()],axis=0)","31bbc4ad":"def calc_p(date, weight,resp,action):\n    p = np.sum(np.bincount(date,weight * resp * action))\n    return p","e306e98e":"example = train[train[\"date\"]==0]\nexample.head()","4a52e13a":"calc_p(example[\"date\"],example[\"weight\"],example[\"resp\"], np.ones(example.shape[0]))","956d224e":"calc_p(example[\"date\"],example[\"weight\"],example[\"resp\"], np.zeros(example.shape[0]))","24d1899d":"a = (example[\"resp\"]>0).astype(\"int\")\n\ncalc_p(example[\"date\"],example[\"weight\"],example[\"resp\"],a)","98a7afe1":"def utility_score(date, weight, resp, action, print_Pi=False):\n    Pi = np.bincount(date, weight * resp * action)\n    if print_Pi:\n        print(Pi)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ len(Pi))\n    u = min(max(t, 0), 6) * np.sum(Pi)\n    return u","9bbbb45d":"a = (example[\"resp\"]>0).astype(\"int\")\n\nutility_score(example[\"date\"],example[\"weight\"],example[\"resp\"],a,print_Pi=True)","0560e739":"a = (train[\"resp\"]>0).astype(\"int\")\n\nutility_score(train[\"date\"],train[\"weight\"],train[\"resp\"],a)","dcc9f37a":"fig, sub = plt.subplots(int((130\/5)),5,figsize=(20,40))\n\nfor feat,sp in zip(train.columns[train.columns.str.contains(\"feature\")],sub.flatten()):\n    \n    sns.distplot(train[feat].sample(5*10**5), hist_kws={\"edgecolor\":\"black\"}, ax=sp)\n    sp.grid();\nfig.tight_layout()","9ada4f2d":"fig, sub = plt.subplots(1,1,figsize=(35,30))\n\ncorr_matrix = train[train.columns[train.columns.str.contains(\"feature\")]].corr()\nmask = np.triu(np.ones_like(corr_matrix,dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(corr_matrix, mask=mask,lw=0.09, cmap=cmap, linecolor=\"grey\", ax= sub)","4354fab3":"corr_flatten = corr_matrix.abs().unstack().sort_values(ascending=False)\ncorr_flatten = corr_flatten.drop_duplicates().reset_index().rename(columns={\"level_0\":\"first\",\"level_1\":\"second\",0:\"corr\"})\ncorr_flatten = corr_flatten.drop(corr_flatten.index[[0]]).reset_index(drop=True).sort_values(by=\"corr\",ascending=False)","c3298f3d":"corr_flatten[corr_flatten[\"corr\"]>0.95]","e64a9368":"from sklearn.decomposition import PCA\n\npca = PCA()\npca_components = pca.fit_transform(train[train.columns[train.columns.str.contains(\"feature\")]].dropna())","82519c50":"fig,sub = plt.subplots(1,2,figsize=(22,6))\nsns.lineplot(x=range(0,50),y=np.cumsum(pca.explained_variance_ratio_[:50]),ax=sub[0])\nsns.lineplot(x=range(0,50),y=pca.explained_variance_[:50],ax=sub[1])\nsub[0].set_xticks(range(0,50))\nsub[1].set_xticks(range(0,50))\n\nsub[0].set_ylabel(\"cum. variance explained by principal comp.\")\nsub[1].set_ylabel(\"variance explained by principal comp.\")\n\nsub[0].set_xlabel(\"number of principal comp.\")\nsub[1].set_xlabel(\"principal comp.\")\n\nsub[0].grid()\nsub[1].grid()\n\nfig.tight_layout()","d1877f0b":"train_col_ls = train.columns[train.columns.str.contains(\"feature\")].to_list()\ntrain_col_ls.extend([\"date\"])","bd664501":"feature_df = train[train_col_ls].groupby(\"date\").mean()","73652560":"colors = ['#8ECAE6','#219EBC','#023047','#023047','#023047','#0E402D','#023047','#023047','#F77F00','#D62828']\n\nplt.figure(figsize=(20,20))\nfor i in range(0,10):\n    plt.subplot(10,1,i+1)\n    ax = sns.lineplot(x=feature_df.index,y=feature_df[f'feature_{i}'],color=colors[i],lw=1)\n    ax.grid()\nplt.show()","55e3ade7":"fig, sub = plt.subplots(1,1,figsize=(16,8))\ncolors = ['#219EBC','#023047','#023047','#023047','#0E402D']\ncounter = 0\n\nfor feat in train.columns[2:7]:\n    df = train[feat]\n    sub.plot(np.cumsum(df))\n    counter+=1\nsub.grid()","f498d675":"and the utility  score:","e2da4033":"if we say that all of the opportunities seem useful to us ( $ \\forall $ action = 1) we got the following:","6a9e09be":"for performance reasons let's have a look at the first 10 features to grasp an understanding of the data:","8d09e41b":"* the suspicion that there are some features being highly correlated seems to be justified \n* maybe it's possible to consolidate the information included in the original features in a reduced number of features (Principal Component Analysis)","ab743664":"![grafik.png](attachment:grafik.png)","21b9a7fa":"The maximum utility score would (over all days) hence be:","3eeb0c71":"* There are some features (e.g. feat 60 - 63) seeming to have the same distribution\n* some of them have a positive kurtosis\n* some of them are skewed\n* having a look at the pearson corr matrix could be beneficial to backup this hypothesis","31ea7785":"# How to decide whether to take or not to take an opportunity?","5606e187":"if we say that none of the opportunities seem useful to us ( $ \\forall $ action = 0) we got the following:","ee83b7ad":"First Look at the Features:","acd04c95":"# General information about the data","174baf1f":"Having clarified that, it seems obvious that the feature 'action' can be derived from the feature 'resp' (the return).\nSo, we can choose if we want to build a regression model on the feature 'resp' or to build a model for classification on the derived feature 'action'","722d4d0d":"# load and compress the data","8bc3e150":"* in general the goal of the competition is to build a model to maximize the utility score based on historical, anonymized stock exchange data\n* the data at hand contains for each day a number of trading opportunities and the model should choose the relevant trades to finally maximize the utility function \n* in really simplified terms, the following graphic resumes the problem:\n    * we ask the model a given day which trades to do\n    * the model evaluates if the given trade should be done (if it's adding up utility for the trader) or not\n    \n![grafik.png](attachment:grafik.png)","cb2c8be3":"Let's have a deeper look into the anonymized attributes containing the string \"feature\" provided by the competition:","d452b5bd":"* through simple linear combination it is possible to express the information included in the 130 features in 20-50 features\n* thus a reduction of features of more than ~61 % is possible to keep the model simpler","630759fd":"but why is a trade useful for us? \nwithout considering more complex decision factors, logically we would say that it is avantageous if a trades creates positive return (green) and the trade shouldn't be executed if its return is negative (red):\n\n![grafik.png](attachment:grafik.png)\n\nThe maximum p for the day 0 is therefore:","a24ff2e4":"To be continued...","3abc76b4":"In the evaluation metric utilization score, there is a attribute \"action\" which indicates whether the opportunity should be taken (action = 1) or not (action = 0)\n\n![grafik.png](attachment:grafik.png)\n\nIn the data sets, there isn't a dichotomous attribute called 'action'. \nSo we have to derive the attribute and train the classification model on the generated attribute 'action' or we train the model on the attribute(s) used to derive action\n\nbut it's useful to understand first of all the utility score function:\n\nin essence the most important part to find out which feature is best to derive the feature 'action' is the calculation of p:"}}