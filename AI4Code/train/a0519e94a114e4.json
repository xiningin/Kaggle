{"cell_type":{"6541618e":"code","7b87aff2":"code","b8e03476":"code","6a1b5117":"code","6d56e151":"code","b05d694c":"code","4e11823f":"code","fd864b69":"code","a21b2671":"code","09e5bfe8":"code","d84fa26c":"code","c3ec36c5":"code","22c29655":"code","3c86fce6":"code","2e7115b6":"code","cc7d64cf":"code","44ba36e3":"code","5bab951e":"code","155da8de":"code","738dcc0b":"code","4eb48f12":"code","227ebf7a":"code","b397b115":"code","dc49dc5b":"code","b1df1cae":"code","23189254":"code","48d99cf9":"code","ec668a92":"code","77e34af1":"code","b4313714":"code","c76dbfdc":"code","44f71d78":"markdown","7d0981b7":"markdown","8d6f267f":"markdown","52c51a3c":"markdown","c283b81c":"markdown","46f254be":"markdown","94baacdd":"markdown","6de1e0bf":"markdown","6c529066":"markdown","55bbd921":"markdown","06a4c28f":"markdown","ca7b2142":"markdown","880dc40b":"markdown","d50dd8f9":"markdown","b8824621":"markdown","afa53250":"markdown","aa6efbf8":"markdown","4f35ccc6":"markdown","56eee632":"markdown","f198ccf9":"markdown"},"source":{"6541618e":"import pandas as pd\nnoshow = pd.read_csv('..\/input\/medicalappointmentnoshown\/KaggleV2-May-2016.csv',sep = ',')\nnoshow.head(3)","7b87aff2":"noshow.isna().sum()","b8e03476":"print('--> No-show vs Alcoholism')\nprint(noshow.groupby(['No-show','Alcoholism'])['PatientId'].count())\nprint('-------------------------------------------------------------------')\n\nprint('--> No-show vs Diabetes')\nprint(noshow.groupby(['No-show','Diabetes'])['PatientId'].count())\nprint('-------------------------------------------------------------------')\n\nprint('--> No-show vs SMS_received')\nprint(noshow.groupby(['No-show','SMS_received'])['PatientId'].count())\nprint('-------------------------------------------------------------------')\n\n\nprint('--> No-show vs Hipertension')\nprint(noshow.groupby(['No-show','Hipertension'])['PatientId'].count())\nprint('-------------------------------------------------------------------')\n\nprint('--> No-show vs Scholarship')\nprint(noshow.groupby(['No-show','Scholarship'])['PatientId'].count())\nprint('-------------------------------------------------------------------')\n\nprint('--> No-show vs Handcap')\nprint(noshow.groupby(['No-show','Handcap'])['PatientId'].count())\nprint('-------------------------------------------------------------------')\n\nprint('--> No-show vs Gender')\nprint(noshow.groupby(['No-show','Gender'])['PatientId'].count())\nprint('-------------------------------------------------------------------')\n","6a1b5117":"import seaborn as sns\nfrom matplotlib import pyplot as plt\nbox1 = plt.subplots()\nbox1 = sns.boxplot(x='No-show', y='Age', data=noshow)\nbox1.set_title('Boxplot do ano pela presen\u00e7a ou n\u00e3o de no show')\nbox1.set_xlabel('Paciente teve no show?')\nbox1.set_ylabel('Idade')\nplt.show()","6d56e151":"grafico = sns.FacetGrid(noshow, col='No-show')\ngrafico.map(sns.distplot, 'Age', rug=True)\nplt.show()","b05d694c":"#Filtrando pessoas com idade negativa\nnoshow[noshow['Age']<0]","4e11823f":"#capturando as mediana das pessoas com desfecho dnegativo\nmedian_noshow_no = noshow[noshow['No-show']=='No']['Age'].median()\nmedian_noshow_no","fd864b69":"import numpy as np\nnoshow['Age'] = np.where(noshow['Age']<1,median_noshow_no,noshow['Age'])\nprint('No-show = N\u00e3o com ajuste')\nprint('----------------------------')\nprint(noshow.loc[noshow['No-show'] == 'No','Age'].describe()) #nova descri\u00e7\u00e3o de idade","a21b2671":"conditions  = [ noshow['Age'] < 10\n               , (noshow['Age'] < 20) & (noshow['Age']>= 10)\n               , (noshow['Age'] < 30) & (noshow['Age']>= 20)\n               , (noshow['Age'] < 40) & (noshow['Age']>= 30)\n               , (noshow['Age'] < 50) & (noshow['Age']>= 40)\n               , (noshow['Age'] < 60) & (noshow['Age']>= 50)\n               , (noshow['Age'] < 70) & (noshow['Age']>= 60)\n               ,  noshow['Age'] >= 70 ]\n\nchoices     = ['0-10','10-20','20-30','30-40','40-50','50-60','60-70','>70']\n\nnoshow['fx_etaria'] = np.select(conditions, choices, default=np.nan)","09e5bfe8":"noshow['ScheduledDay'] = pd.to_datetime(noshow['ScheduledDay'])\nnoshow['AppointmentDay'] = pd.to_datetime(noshow['AppointmentDay'])\n\n#variaveis relacionada a scheduled\nnoshow['Scheduled_Month'] = noshow['ScheduledDay'].apply(lambda x: x.month)\nnoshow['Scheduled_Year'] = noshow['ScheduledDay'].apply(lambda x: x.year)\nnoshow['Scheduled_WeekDay'] = noshow['ScheduledDay'].apply(lambda x: x.strftime(\"%A\"))\n\n#variaveis reacionadas ao appointment\nnoshow['Appointment_Month'] = noshow['AppointmentDay'].apply(lambda x: x.month)\nnoshow['Appointment_Year'] = noshow['AppointmentDay'].apply(lambda x: x.year)\nnoshow['Appointment_WeekDay'] = noshow['AppointmentDay'].apply(lambda x: x.strftime(\"%A\"))\n\n#Diferenca entre datas\nnoshow['DeltaScheduleAppointment_Days'] = noshow['ScheduledDay']-noshow['AppointmentDay']\nnoshow['DeltaScheduleAppointment_Days'] = noshow['DeltaScheduleAppointment_Days']\/np.timedelta64(1,'D')\n\n#tratando dados diferenca negativa\nnoshow['DeltaScheduleAppointment_Days'] = np.where(noshow['DeltaScheduleAppointment_Days'] < 0 \n                                                   ,0,noshow['DeltaScheduleAppointment_Days'] )\n","d84fa26c":"from category_encoders.one_hot import OneHotEncoder\nnoshow_bin = noshow\n\nbinarizar = OneHotEncoder(cols= ['Gender','Handcap','Appointment_WeekDay','Scheduled_WeekDay','fx_etaria'],use_cat_names=True)\nbinarizar.fit(noshow_bin)\nnoshow_bin = binarizar.transform(noshow_bin)\n\nnoshow_bin.head()","c3ec36c5":"noshow_bin.reset_index()\n\nfrom sklearn import preprocessing\npadronizar = preprocessing.StandardScaler().fit(noshow_bin[['Age','Scheduled_Month','Scheduled_Year'\n                                                            ,'Appointment_Month','Appointment_Year'\n                                                            ,'DeltaScheduleAppointment_Days']])\n\nnoshow_bin[['Age','Scheduled_Month','Scheduled_Year','Appointment_Month','Appointment_Year'\n,'DeltaScheduleAppointment_Days']] = padronizar.transform(noshow_bin[['Age'\n                                                                     ,'Scheduled_Month'\n                                                                     ,'Scheduled_Year'\n                                                                     ,'Appointment_Month'\n                                                                     ,'Appointment_Year'\n                                                                     ,'DeltaScheduleAppointment_Days']])","22c29655":"noshow_bin.columns","3c86fce6":"x = noshow_bin.loc[:,['Age','Appointment_Month', 'Appointment_Year','Scheduled_Year','DeltaScheduleAppointment_Days'\n                      ,'Scheduled_Month'#numericos\n                      ,'Gender_F', 'Gender_M','Scholarship','Hipertension','Diabetes', 'Alcoholism'#categoricos\n                      , 'Handcap_0.0', 'Handcap_1.0', 'Handcap_2.0','Handcap_3.0', 'Handcap_4.0'\n                      , 'SMS_received','Scheduled_WeekDay_Friday'\n                      ,'Scheduled_WeekDay_Wednesday', 'Scheduled_WeekDay_Tuesday'\n                      ,'Scheduled_WeekDay_Thursday', 'Scheduled_WeekDay_Monday'\n                      ,'Scheduled_WeekDay_Saturday','Appointment_WeekDay_Friday'\n                      , 'Appointment_WeekDay_Tuesday','Appointment_WeekDay_Monday'\n                      , 'Appointment_WeekDay_Wednesday','Appointment_WeekDay_Thursday'\n                      , 'Appointment_WeekDay_Saturday','fx_etaria_60-70', 'fx_etaria_50-60'\n                      , 'fx_etaria_0-10', 'fx_etaria_>70','fx_etaria_20-30', 'fx_etaria_30-40'\n                      , 'fx_etaria_10-20','fx_etaria_40-50']]\ny = noshow_bin.loc[:,'No-show']","2e7115b6":"from sklearn.preprocessing import LabelEncoder\n\nx[['Gender_F', 'Gender_M','Scholarship','Hipertension','Diabetes', 'Alcoholism'\n          , 'Handcap_0.0', 'Handcap_1.0', 'Handcap_2.0','Handcap_3.0', 'Handcap_4.0'\n          , 'SMS_received','Scheduled_WeekDay_Friday'\n          ,'Scheduled_WeekDay_Wednesday', 'Scheduled_WeekDay_Tuesday'\n          ,'Scheduled_WeekDay_Thursday', 'Scheduled_WeekDay_Monday'\n          ,'Scheduled_WeekDay_Saturday','Appointment_WeekDay_Friday'\n          , 'Appointment_WeekDay_Tuesday','Appointment_WeekDay_Monday'\n          , 'Appointment_WeekDay_Wednesday','Appointment_WeekDay_Thursday'\n          , 'Appointment_WeekDay_Saturday','fx_etaria_60-70', 'fx_etaria_50-60'\n          , 'fx_etaria_0-10', 'fx_etaria_>70','fx_etaria_20-30', 'fx_etaria_30-40'\n          , 'fx_etaria_10-20','fx_etaria_40-50']].apply(LabelEncoder().fit_transform)\n\ny = LabelEncoder().fit_transform(y)","cc7d64cf":"from sklearn.model_selection import train_test_split\nx_treino, x_teste, y_treino, y_teste = train_test_split(x,y,test_size=0.3,random_state=1)","44ba36e3":"import xgboost as xgb\nimport matplotlib.pyplot as plt \nfrom sklearn.metrics import recall_score,accuracy_score,classification_report,confusion_matrix\n\nxgboost_ = xgb.XGBClassifier(objective ='reg:logistic', colsample_bytree = 0.8, learning_rate = 0.2,\n                max_depth = 7, n_estimators = 100,random_state=0)\nxgboost_.fit(x_treino,y_treino)\n\n#realiza\u00e7\u00e3o do predict\nprevisoes = xgboost_.predict(x_teste)\n\nprint('recall:' , recall_score(previsoes,y_teste))\nprint('accuracy:' , accuracy_score(previsoes,y_teste))\nprint('---------------------------------------------')\nprint(confusion_matrix(previsoes,y_teste))\nprint('---------------------------------------------')\nprint(classification_report(previsoes,y_teste))\n\nxgb.plot_importance(xgboost_)\nplt.show() \n","5bab951e":"#Visulaizando feature_importance\nfeature = []\nfor feature in zip(x_treino, xgboost_.feature_importances_):\n    print(feature)","155da8de":"from sklearn.model_selection import GridSearchCV\n\nparametros = [{'learning_rate':[0.01,0.1,0.2],\n                'max_depth':[5,7],\n                'colsample_bytree':[0.7,0.8,0.9]}]\n\nxgboost = xgb.XGBClassifier(objective ='reg:logistic', n_estimators = 100,random_state=0)\n\ngrid_search =  GridSearchCV(xgboost,parametros,scoring='recall',cv=4,verbose=1)\n\ngrid_search.fit(x_treino,y_treino)","738dcc0b":"#Melhores hiperparametros\ngrid_search.best_params_","4eb48f12":"from skopt import dummy_minimize\nimport lightgbm as lgb   \ngradient = lgb.LGBMClassifier(learning_rate=0.09955911573844406 #resultado do randomsearch\n                             ,colsample_bytree=0.7472177953903952  #resultado do randomsearch\n                             ,max_depth=6  #resultado do randomsearch\n                             ,n_estimators=176  #resultado do randomsearch\n                              ,random_state=0\n                                )\ngradient.fit(x_treino,y_treino)\nprevisoes = gradient.predict(x_teste)\nprint('recall:' , recall_score(previsoes,y_teste))\nprint('accuracy:' , accuracy_score(previsoes,y_teste))\nprint('---------------------------------------------')\nprint(confusion_matrix(previsoes,y_teste))\nprint('---------------------------------------------')\nprint(classification_report(previsoes,y_teste))","227ebf7a":"from skopt import dummy_minimize\nfrom lightgbm import LGBMClassifier\ndef treinar_modelo(params):\n    learning_rate = params[0]\n    colsample_bytree = params[1]\n    max_depth = params[2]\n    n_estimators= params[3]\n    \n    print(params, '\\n')\n    \n    modelo = LGBMClassifier(learning_rate=learning_rate\n                         ,colsample_bytree=colsample_bytree\n                         ,max_depth=max_depth\n                         ,n_estimators=n_estimators,random_state = 0)\n    modelo.fit(x_treino, y_treino)\n    \n    previsoes = modelo.predict(x_treino)\n    \n    return -recall_score(y_treino, previsoes,average=\"binary\")\n\nspace = [(1e-3, 1e-1, 'log-uniform'), #learning rate\n         (0.7,0.9),#colsample_bytree\n         (5,9), #max_depth\n         (100, 200)] #n_estimators\n\nresultado = dummy_minimize(treinar_modelo, space, random_state=1, verbose=1, n_calls=30)","b397b115":"resultado.x","dc49dc5b":"#Visulaizando feature_importance\nfeature_lgbm = []\nfor feature_lgbm in zip(x_treino, gradient.feature_importances_):\n    print(feature_lgbm)","b1df1cae":"from sklearn.feature_selection import SelectFromModel\n\n#Treinando o modelo usando as features mais importantes\nthresholds = sorted(gradient.feature_importances_,reverse=True) #ordenando as features com mais poder \n\nfor thresh in thresholds:\n    selection = SelectFromModel(gradient, threshold=thresh, prefit=True)\n    select_x_treino = selection.transform(x_treino)\n\n    #treinando o modelo\n    selection_model = lgb.LGBMClassifier(learning_rate=0.09955911573844406 #resultado do randomsearch\n                             ,colsample_bytree=0.7472177953903952  #resultado do randomsearch\n                             ,max_depth=6  #resultado do randomsearch\n                             ,n_estimators=176  #resultado do randomsearch\n                              ,random_state=0\n                                )\n    selection_model.fit(select_x_treino, y_treino)\n\n    #avaliando os modelos\n    select_x_teste = selection.transform(x_teste)\n    y_pred = selection_model.predict(select_x_teste)\n    previsoes= [round(value) for value in y_pred]\n    accuracy = accuracy_score(previsoes,y_teste)\n    recall = recall_score(previsoes,y_teste)\n    print(\"Thresh=%.3f, n=%d, Recall:%.2f%%, Accuracy: %.2f%%\" % (thresh, select_x_treino.shape[1]\n                                                                 ,recall*100.0, accuracy*100.0))","23189254":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import recall_score,accuracy_score,classification_report,confusion_matrix\n\nmodelo = BaggingClassifier(bootstrap=True,n_jobs = -1,n_estimators=100)\nmodelo.fit(x_treino,y_treino)\nprevisoes = modelo.predict(x_teste)\nprint('recall:' , recall_score(previsoes,y_teste))\nprint('accuracy:' , accuracy_score(previsoes,y_teste))\nprint('---------------------------------------------')\nprint(confusion_matrix(previsoes,y_teste))\nprint('---------------------------------------------')\nprint(classification_report(previsoes,y_teste))","48d99cf9":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import recall_score,accuracy_score,classification_report,confusion_matrix\n\nmodelo = RandomForestClassifier(max_depth=5, random_state=0,n_estimators=100)\nmodelo.fit(x_treino,y_treino)\nprevisoes = modelo.predict(x_teste)\nprint('recall:' , recall_score(previsoes,y_teste))\nprint('accuracy:' , accuracy_score(previsoes,y_teste))\nprint('---------------------------------------------')\nprint(confusion_matrix(previsoes,y_teste))\nprint('---------------------------------------------')\nprint(classification_report(previsoes,y_teste))","ec668a92":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import recall_score,accuracy_score,classification_report,confusion_matrix\n\nmodelo = LogisticRegression(random_state=0,solver='liblinear',penalty='l1')\nmodelo.fit(x_treino,y_treino)\nprevisoes = modelo.predict(x_teste)\nprint('recall:' , recall_score(previsoes,y_teste))\nprint('accuracy:' , accuracy_score(previsoes,y_teste))\nprint('---------------------------------------------')\nprint(confusion_matrix(previsoes,y_teste))\nprint('---------------------------------------------')\nprint(classification_report(previsoes,y_teste))","77e34af1":"#transformando o feature inportance do modelo LGBM em um dataframe\nresults=pd.DataFrame()\nresults['columns']=x_treino.columns\nresults['importances'] = gradient.feature_importances_\nresults.sort_values(by='importances',ascending=False,inplace=True)\nresults.reset_index().head(3)","b4313714":"x_treino, x_teste, y_treino, y_teste = train_test_split(x,y,test_size=0.3,random_state=1)\n#Cortando as features de acordo com os modelos rodados na etapa feautre selection com lgbm\nfeatures_final = results.iloc[:31,0] #Resultado com feature selection \n\n#redefinindo x_treino e x_teste\nx_treino = x_treino.loc[:,features_final]\nx_teste = x_teste.loc[:,features_final]","c76dbfdc":"#Reaplicado o algoritimo\ngradient = lgb.LGBMClassifier(learning_rate=0.09955911573844406 #resultado do randomsearch\n                             ,colsample_bytree=0.7472177953903952  #resultado do randomsearch\n                             ,max_depth=6  #resultado do randomsearch\n                             ,n_estimators=176  #resultado do randomsearch\n                              ,random_state=0\n                                )\ngradient.fit(x_treino,y_treino)\nprevisoes = gradient.predict(x_teste)\nprint('Resultado final:')\nprint('----------------------------------------')\nprint('Dos exemplos que s\u00e3o noshow positivo qual a porcetangem de acerto do modelo?')\nprint('recall final:' , recall_score(previsoes,y_teste)*100)\nprint('----------------------------------------')\nprint('Porcentagem de acerto da minhas observa\u00e7\u00f5es?')\nprint('accuracy final:', accuracy_score(previsoes,y_teste)*100)\n","44f71d78":"# Xgboost\nmodelo baseando em gradient boosting onde o previsor sucessor \u00e9 criado baseado no residuo do previsor antecessor","7d0981b7":"# Criando faixa et\u00e1ria","8d6f267f":"# Separando vari\u00e1vel target das features","52c51a3c":"# Rela\u00e7\u00e3o entre a Vari\u00e1vel Target (noshow) com todas as vari\u00e1veis categ\u00f3ricas do dataframe","c283b81c":"# Feature Selection - LGBM","46f254be":"# Substitui\u00e7\u00e3o das idades negativas\nNesse caso foi utilizado a mediana de quem possui desfecho negativo devido a presen\u00e7a de valores abaixo de ser ter ocorrido em pessoas com desfecho negativo","94baacdd":"# LGBM","6de1e0bf":"# Distribui\u00e7\u00e3o das idades por no show","6c529066":"# OneHotEncoder nas variav\u00e9is categoricas Gender, Handcap, Appointment_WeekDay,Scheduled_WeekDay e faixa et\u00e1ria\nAplicando OneHotEncoder as variav\u00e9is se transformam em novas colunas onde 1 representa o valor afirmativo e 0 o valor negativo\n","55bbd921":"# Algoritmo final","06a4c28f":"# Dividindo o dataframe em 70% treino e 30% teste","ca7b2142":"# Importando e lendo o Dataframe com a  biblioteca Pandas","880dc40b":"# Random Forest\nGeralmente \u00e9 um algoritmo de bagging ou pasting com max_sample ajustada para o n\u00famero de instancias do meu dataframe,ou seja ir\u00e1 utilizar todas as linhas do dataframe.Al\u00e9m disso, possui uma aleatoriedade nas caracteristicas selecionadas,ou seja para cada \u00e1rvore de decis\u00e3o \u00e9 selecionada um subconjunto de caracteristicas\/feautures.Logo nem todas minhas \u00e1rvores v\u00e3o ser iguais","d50dd8f9":"# Tuning Hiperpar\u00e2metros - Grid Search - Xgboost\nTentar todos as combina\u00e7\u00f5es poss\u00edveis utilizando os hiperparametros passados, foi utilizado os hiperparametros:\nlearning_rate = taxa de aprendizado, max_depth = m\u00e1xima rofundidade das arvores, colsample_bytree = porcentagem de colunas utilizadas em cada \u00e1rvore","b8824621":"# Bagging \nAlgoritmo baseado em \u00e1rvores de decis\u00e3o na qual ele faz a amostragem com repeti\u00e7\u00e3o (bootstrap) nas inst\u00e2ncias onde cada conjunto de amostras resultar\u00e1 em um modelo diferente, sendo que a previs\u00e3o final ser\u00e1 estimada baseada em hard voting, nesse desafio como estamos tratando de um problema de classifica\u00e7\u00e3o o valor estimado ser\u00e1 baseada na frequencia em rela\u00e7\u00e3o a previs\u00e3o de todas arvores de decis\u00e3o criadas","afa53250":"# Padronizando das vari\u00e1veis cont\u00ednuas \npadroniza\u00e7\u00e3o = x - m\u00e9dia\/desvio padrao \nO processo faz com que todas minhas variaveis num\u00e9ricas permanecam na mesma escala","aa6efbf8":"# Tuning Hiperpar\u00e2metros - Random Search - LGBM\nAo inv\u00e9s de tentar todas as combina\u00e7\u00f5es poss\u00edveis como o Gridsearch ela seleciona o valor aleat\u00f3rio para o hiperparametro e vai testando aleatoriamente as combina\u00e7\u00f5es o random search vai executar o n\u00famero de itera\u00e7\u00f5es definido pelo usu\u00e1rio e no final ter\u00e1 como saida os melhores hiperparametros dessa busca aleatoria em n itera\u00e7\u00f5es","4f35ccc6":"# Transforma\u00e7\u00e3o das vari\u00e1veis datetime\n","56eee632":"# Verificando a presen\u00e7a de valores nulos dentro do dataframe","f198ccf9":"# Regress\u00e3o logistica"}}