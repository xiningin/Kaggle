{"cell_type":{"54d597a4":"code","e483dca2":"code","eb10de22":"code","5b6ad29b":"code","6b13dfba":"code","8463120c":"code","1efd5cd8":"code","3eae5d80":"code","6f84325e":"code","4f9f31f6":"code","a151d2d2":"code","c357b42e":"code","f7652b41":"code","696183fe":"code","d59fd96a":"code","b209a078":"code","c4156543":"code","f3bc4faa":"code","a7b0fdb5":"code","fb34d7eb":"code","f9bb5de9":"code","91593728":"code","3b63c024":"code","0d3ce0c8":"code","4dd9173c":"code","ed1f314a":"code","5be71207":"code","b8c5e62a":"code","4f17c537":"code","bbbc1c4a":"code","fb640015":"code","fecaccf9":"code","b7bfd73e":"code","4755b321":"code","5c608d60":"code","064a25f9":"code","6a1c0b54":"code","78302e51":"code","b9887715":"code","d48f1941":"code","1aa55623":"code","85da958a":"code","4b7cc53f":"code","c8350eb3":"code","de9ee5f4":"code","12a8273e":"code","dfc6e732":"markdown","1271007f":"markdown","772853bf":"markdown","fb01edc6":"markdown","3ecc1693":"markdown","aa1c004a":"markdown","413abe64":"markdown","83159596":"markdown"},"source":{"54d597a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e483dca2":"import seaborn as sns \nimport matplotlib.pyplot as plt \nfrom scipy.stats import norm \n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom matplotlib import pyplot\nfrom sklearn.preprocessing import LabelEncoder","eb10de22":"#input data\nr_dir = '\/kaggle\/input\/widsdatathon2021\/'\n\nsample_submission = pd.read_csv(os.path.join(r_dir,'SampleSubmissionWiDS2021.csv'))\nsol_temp = pd.read_csv(os.path.join(r_dir,'SolutionTemplateWiDS2021.csv'))\ndata_dict = pd.read_csv(os.path.join(r_dir,'DataDictionaryWiDS2021.csv'))\ntest = pd.read_csv(os.path.join(r_dir,'UnlabeledWiDS2021.csv'))\ntrain = pd.read_csv(os.path.join(r_dir,'TrainingWiDS2021.csv'))","5b6ad29b":"print(data_dict.shape)\nprint(test.shape)\nprint(train.shape)\nprint(sol_temp.shape)","6b13dfba":"train.head(10)","8463120c":"train.drop(['Unnamed: 0'],axis=1,inplace = True)","1efd5cd8":"train.isnull().sum()","3eae5d80":"train.shape","6f84325e":"train.columns","4f9f31f6":"train.info","a151d2d2":"train.describe()","c357b42e":"train.dtypes.value_counts()","f7652b41":"test.head()","696183fe":"test.drop(['Unnamed: 0'],axis=1,inplace = True)","d59fd96a":"test.isnull().sum()","b209a078":"test.shape","c4156543":"test.columns","f3bc4faa":"test.info","a7b0fdb5":"test.describe()","fb34d7eb":"test.dtypes.value_counts()","f9bb5de9":"train.corr(method ='pearson') ","91593728":"corrmat = train.corr() \n  \nf, ax = plt.subplots(figsize =(9, 8)) \nsns.heatmap(corrmat, ax = ax, cmap =\"coolwarm\", linewidths = 0.5) ","3b63c024":"pct_null = train.isnull().sum() \/ len(train) \nmissing_features = pct_null[pct_null > 0.50].index ","0d3ce0c8":"missing_features","4dd9173c":"import missingno as msno\n%matplotlib inline","ed1f314a":"msno.matrix(train.sample(1000),figsize=(35, 60), width_ratios=(10, 1), color=(.0, 0.5, 0.5),           fontsize=16)","5be71207":"cat_list = train.select_dtypes('object').columns\ndisplay(cat_list)","b8c5e62a":"# Creating Label Encoder object\nle = LabelEncoder()\nfor ob in cat_list:\n    train[ob] = le.fit_transform(train[ob].astype(str))\n    test[ob] = le.fit_transform(test[ob].astype(str))\nprint(train.info())    \nprint(test.info()) ","4f17c537":"train.fillna(-9999,inplace = True)\ntrain.isnull().sum()","bbbc1c4a":"Target = 'diabetes_mellitus'\ntrain_labels = train[Target]\ntrain_df_NT = train.drop(columns = [Target])\nfeatures = list(train_df_NT.columns)\nprint('Training data shape:', train_df_NT.shape)\nprint('Test data shape:', test.shape)","fb640015":"X, y = train_df_NT, train_labels","fecaccf9":"import xgboost as xgb\nfrom xgboost import XGBClassifier","b7bfd73e":"data_dmatrix = xgb.DMatrix(data=X,label=y)","4755b321":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","5c608d60":"model = XGBClassifier()","064a25f9":"model.fit(X_train, y_train)","6a1c0b54":"xbg_pred = model.predict(X_test)\nmodel.score(X_train,y_train)","78302e51":"model.score(X_test,y_test)","b9887715":"print(model.feature_importances_)","d48f1941":"features = X.columns\nimportances = model.feature_importances_","1aa55623":"print(importances)","85da958a":"plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\nplt.show()","4b7cc53f":"import xgboost\nimport matplotlib.pylab as plt\nxgboost.plot_importance(model)\nplt.title(\"model.plot_importance(model)\")\nplt.show()","c8350eb3":"df_feature_importance = pd.DataFrame(model.feature_importances_, index=features, columns=['feature importance']).sort_values('feature importance', ascending=False)\ndf_feature_importance","de9ee5f4":"# Make sure to select the second column only\nxgb_test = model.predict_proba(test)[:,1]","12a8273e":"submit = test[['encounter_id']]\nsubmit['diabetes_mellitus'] = xgb_test\nsubmit.to_csv('wids_datathon.csv',index=False)\nsubmit.head()","dfc6e732":"## Dealing With Training Data","1271007f":"## Let's check with missing values","772853bf":"The testing data has 10234 rows x 179 cols","fb01edc6":"There are 3 unique datatypes in the dataset","3ecc1693":"There are 3 unique datatypes in the dataset","aa1c004a":"## Splitting of data into train and test","413abe64":"## Explore The Test Data","83159596":"The training data has 130157 rows x 180 cols"}}