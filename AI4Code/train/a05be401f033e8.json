{"cell_type":{"3ad27ec2":"code","df799b1a":"code","72392d47":"code","3a562e5f":"code","e3474215":"code","1f8528c3":"code","093fe863":"code","0bf81269":"code","824c8ad6":"code","ce1bccbe":"code","28a3d877":"code","422c0f1a":"code","d10e9200":"code","2eeb512c":"code","ba5ef3f6":"code","26f58dd4":"code","9895f88b":"code","be5371c3":"code","911035b8":"code","c8208d0b":"code","74d5bc07":"code","7029004e":"code","056d36c2":"code","3907de2a":"code","48d69d61":"code","9fc91b4a":"code","ab3df404":"code","4725fe0d":"markdown","5fcea7d7":"markdown","23208ebc":"markdown","54f3efdb":"markdown","e71b9b3d":"markdown","c57e528a":"markdown","23a94182":"markdown","51fa840f":"markdown","7e92a101":"markdown","b378c0da":"markdown","ba09630a":"markdown","8144c161":"markdown","795da135":"markdown","d0928223":"markdown","5267dac0":"markdown"},"source":{"3ad27ec2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df799b1a":"#importing necessery libraries for future analysis of the dataset\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport folium\nfrom folium.plugins import HeatMapWithTime, TimestampedGeoJson","72392d47":"# Pandas to read Covid Tweets dataset\ntweets = pd.read_csv('..\/input\/covid19-tweets\/covid19_tweets.csv')","3a562e5f":"# Examining the dataset from begining\ntweets.head()","e3474215":"# Checking the shape of dataset\ntweets.shape","1f8528c3":"\ntweets.info()","093fe863":"# World City Dataset\n\ncities = pd.read_csv('..\/input\/world-cities-datasets\/worldcities.csv')","0bf81269":"# Exploring city dataset\ncities.head()","824c8ad6":"## Empty Columns of Latitude and Longitudes are added in Tweets Dataset\n\ntweets[\"lat\"] = np.NaN\ntweets[\"lng\"] = np.NaN\ntweets[\"location\"] = tweets[\"user_location\"]\n","ce1bccbe":"user_location = tweets['location'].fillna(value='').str.split(',')","28a3d877":"# AVG Location Dataset\n\navg_countries_loc = pd.read_csv('https:\/\/gist.githubusercontent.com\/tadast\/8827699\/raw\/3cd639fa34eec5067080a61c69e3ae25e3076abb\/countries_codes_and_coordinates.csv')","422c0f1a":"avg_countries_loc.head()","d10e9200":"# Make a list of all countries in Avg Location Dataset\ncodes = avg_countries_loc['Alpha-2 code'].str.replace('\"','').str.strip().to_list()\nworld_city_iso2 = []\nfor c in cities['iso2'].str.lower().str.strip().values.tolist():\n    if c not in world_city_iso2:\n        world_city_iso2.append(c)\n        \n# Try to identify if both external share same countries codes for tracking between them\nl_codes = [c.lower() for c in codes]\nfor a in world_city_iso2:\n    if a not in l_codes:\n        print(a)","2eeb512c":"# Adding the missing country codes manually\n\ncodes = avg_countries_loc['Alpha-2 code'].str.replace('\"','').str.strip().to_list() + ['XW','SX', 'CW','XK']\ncode_lat = avg_countries_loc['Latitude (average)'].str.replace('\"','').to_list() + ['31.953112', '18.0255', '12.2004', '42.609778']\ncode_lng = avg_countries_loc['Longitude (average)'].str.replace('\"','').to_list() + ['35.301170', '-63.0450', '-69.0200', '20.918062']","ba5ef3f6":"lat = cities['lat'].fillna(value = '').values.tolist()\nlng = cities['lng'].fillna(value = '').values.tolist()\n\n\n# Getting all alpha 3 codes into  a list\nworld_city_iso3 = []\nfor c in cities['iso3'].str.lower().str.strip().values.tolist():\n    if c not in world_city_iso3:\n        world_city_iso3.append(c)\n        \n# Getting all alpha 2 codes into  a list    \nworld_city_iso2 = []\nfor c in cities['iso2'].str.lower().str.strip().values.tolist():\n    if c not in world_city_iso2:\n        world_city_iso2.append(c)\n        \n# Getting all countries into  a list        \nworld_city_country = []\nfor c in cities['country'].str.lower().str.strip().values.tolist():\n    if c not in world_city_country:\n        world_city_country.append(c)\n\n# Getting all amdin names into  a list\nworld_states = []\nfor c in cities['admin_name'].str.lower().str.strip().tolist():\n    world_states.append(c)\n\n\n# Getting all cities into  a list\nworld_city = cities['city'].fillna(value = '').str.lower().str.strip().values.tolist()\n\n","26f58dd4":"\nfor each_loc in range(len(user_location)):\n    ind = each_loc\n    order = [False,False,False,False,False]\n    each_loc = user_location[each_loc]\n    for each in each_loc:\n        each = each.lower().strip()\n        if each in world_city:\n            order[0] = world_city.index(each)\n        if each in world_states:\n            order[1] = world_states.index(each)\n        if each in world_city_country:\n            order[2] = world_city_country.index(each)\n        if each in world_city_iso2:\n            order[3] = world_city_iso2.index(each)\n        if each in world_city_iso3:\n            order[4] = world_city_iso3.index(each)\n\n    if order[0]:\n        tweets['lat'][ind] = lat[order[0]]\n        tweets['lng'][ind] = lng[order[0]]\n        continue\n    if order[1]:\n        tweets['lat'][ind] = lat[order[1]]\n        tweets['lng'][ind] = lng[order[1]]\n        continue\n    if order[2]:\n        try:\n            tweets['lat'][ind] = code_lat[codes.index(world_city_iso2[order[2]].upper())]\n            tweets['lng'][ind] = code_lng[codes.index(world_city_iso2[order[2]].upper())]\n        except:\n            pass\n        continue\n    if order[3]:\n        tweets['lat'][ind] = code_lat[codes.index(world_city_iso2[order[3]].upper())]\n        tweets['lng'][ind] = code_lng[codes.index(world_city_iso2[order[3]].upper())]\n        continue\n    if order[4]:\n        tweets['lat'][ind] = code_lat[codes.index(world_city_iso2[order[4]].upper())]\n        tweets['lng'][ind] = code_lng[codes.index(world_city_iso2[order[4]].upper())]\n        continue\n","9895f88b":"# Null values of location in tweets\nall_tweets = len(tweets)\nbad_tweets_without_location = tweets['user_location'].isnull().sum()\ntweets_unrecovered_location = tweets['lat'].isnull().sum()\n\nprint(all_tweets, bad_tweets_without_location, tweets_unrecovered_location)\nprint('\\nPercentage of recovering Tweet Locations using extrenal datasets...')\nprint((all_tweets-(tweets_unrecovered_location))\/(all_tweets-bad_tweets_without_location))\n","be5371c3":"map_df = tweets[['lat','lng','user_location','date']].dropna()","911035b8":"map_df.head()","c8208d0b":"dates = map_df['date'].str.split(' ').str.get(0).unique().tolist()\nprint('Number of Days in dataset:', len(dates))","74d5bc07":"\ndaily_tweets = folium.Map(tiles='cartodbpositron', min_zoom=2) \n\n# Ensure you're handing it floats\nmap_df['lat'] = map_df['lat'].astype(float)\nmap_df['lng'] = map_df['lng'].astype(float)\nmap_df['date'] = map_df['date'].str.split(' ').str.get(0)\n\n\n# List comprehension to make out list of lists\nheat_data = [[[row['lat'],row['lng']] for index, row in map_df[map_df['date'] == i].iterrows()] for i in dates]\n\n# Plot it on the map\nhm = HeatMapWithTime(data=heat_data, name=None, radius=7, min_opacity=0, max_opacity=0.8, \n                     scale_radius=False, gradient=None, use_local_extrema=False, auto_play=False, \n                     display_index=True, index_steps=1, min_speed=0.1, max_speed=10, speed_step=0.1, \n                     position='bottomleft', overlay=True, control=True, show=True)\nhm.add_to(daily_tweets)\n# Display the map\ndaily_tweets.save('daily_tweets.html')\ndaily_tweets","7029004e":"def geojson_features(map_df):\n    features = []\n    for _, row in map_df.iterrows():\n        feature = {\n            'type': 'Feature',\n            'geometry': {\n                'type':'Point', \n                'coordinates':[row['lng'],row['lat']]\n            },\n            'properties': {\n                'time': row['date'],\n                'style': {'color' : 'red'},\n                'icon': 'circle',\n                'iconstyle':{\n                    'fillColor': 'red',\n                    'fillOpacity': 0.5,\n                    'stroke': 'true',\n                    'radius': 3\n                }\n            }\n        }\n        features.append(feature)\n    return features\n","056d36c2":"\nmap_df = tweets[['lat','lng','user_location','date']].dropna()\ntimely_tweets = folium.Map(tiles='cartodbpositron', min_zoom=2) \n\n# Ensure you're handing it floats\nmap_df['lat'] = map_df['lat'].astype(float)\nmap_df['lng'] = map_df['lng'].astype(float)\nmap_df['date'] = map_df['date']\n\n\n# List comprehension to make out list of lists\nheat_data = [[[row['lat'],row['lng']] for index, row in map_df[map_df['date'] == i].iterrows()] for i in dates]\n\n# Plot it on the map\nhm = TimestampedGeoJson(geojson_features(map_df), transition_time=200, loop=True, auto_play=False, add_last_point=True, \n                   period='P1D', min_speed=0.1, max_speed=10, loop_button=False, date_options='YYYY-MM-DD HH:mm:ss', \n                   time_slider_drag_update=False, duration=None)\nhm.add_to(timely_tweets)\n# Display the map\ntimely_tweets\n","3907de2a":"!apt install git\n!git clone https:\/\/github.com\/AkhilRam7\/Covid19Tweets.git\n%cd Covid19Tweets","48d69d61":"!pip install flask-ngrok","9fc91b4a":"\nfrom flask_ngrok import run_with_ngrok\nfrom flask import Flask, render_template\napp = Flask(__name__)\nrun_with_ngrok(app)   #starts ngrok when the app is run\n@app.route('\/')\ndef index():\n    return render_template('daily_tweets.html')\napp.run()","ab3df404":"from IPython.display import IFrame\n\n#Add NGROK SERVING address below in src\n\nIFrame(src='http:\/\/705d513a190c.ngrok.io\/', width=700, height=600)","4725fe0d":"# Data Mapping among Datasets\n\n1. Each User Location may possibly contain combination of city name, country name with country codes. (Ex: Pewee Valley, KY)\n2. We need to split the user location from user location\n3. Now each location will be possibily containing list of (country, city, code)\n4. If we find city name or admin name match in world city database we will assign the location from world city database to tweets dataset for a respective row.\n5. If we find alpha 2, alpha 3 or country match in world city database then we will get average location from Avg countries dataset and add it to tweet dataset.","5fcea7d7":"We have recovered atmost 71% of tweet geo cordinate locations which is highly satisfactory.\n\nNow we will create a partial dataframe from datasets for visualizing animations of geographical distribution","23208ebc":"# Conclusion\n\nWe have generated Animation for tweets and their geographical distribution\n\nWe will take a big leap by making an realtime web application displaying geographical distribution of tweets along with realtime top tweets displaying.\n\nHere i am trying to tweak the folium generated html file. \n\nI found a js function in folium generated html which is triggred to update the maps which we can use to call another function which will display the top tweets at that timestamp.\n\nAnd now I will host this html in a simple flask server\n\n\nHere I am cloning already tweaked html file and will try to render it in thi kernal","54f3efdb":"## Missing Data Handling in tweets dataset\n\nHere i dont want to remove the NaN values now. So i'm trying to replace it with a empty string.","e71b9b3d":"# Daily Tweets and their Geographical Distribution","c57e528a":"# Timely Tweets and Their Geographic Distribution\n\nThis is very bulky cell as we are plotting for all existing timestamps (!Cell output map animation will be slow)","23a94182":"# <center> COVID'19 Tweets Geographical Distribution \ud83c\udf0e<\/center>","51fa840f":"<center><h2>Real Time Tweets and Geographical Distribution Application<\/h2><\/center><br>\n<center><h5>Recovered more than 70% of Geo Cordinates from user tweeted address<\/h5><\/center>","7e92a101":"# Feature Engineering","b378c0da":"# Data Insights\nWe have around 166,656 Tweets in the dataset and each tweet will possibly contain 13 parameters. Lets try to explore how many of them are necessary for visualizing the geographical distribution. To visualize geographically we need to have either country codes or Location co-ordinates. But te dataset has a parameter '*user_location*' in which the locations are very vague to plot them.\n\n\nMain idea is to extract geographical cordinates of user. Here I'm using two other datasets to improve the location feature in dataset.\n* world-cities-datasets (https:\/\/www.kaggle.com\/viswanathanc\/world-cities-datasets)\n* Average Locations cordinates based on country cides (https:\/\/gist.githubusercontent.com\/tadast\/8827699\/raw\/3cd639fa34eec5067080a61c69e3ae25e3076abb\/countries_codes_and_coordinates.csv')\n\nworld-cities-datasets is used to identify Alpha 2,Alpha 3,Country Name, City Name from user location field in Covid Tweet Dataset. T\n\nAverage Locations cordinates dataset is used to consider average geo cordinates of Alpha2, Alpha3, Country Name for world-cities-datasets.","ba09630a":"# Dataset Exploration","8144c161":"<center><img src='https:\/\/raw.githubusercontent.com\/AkhilRam7\/Covid19Tweets\/master\/Webp.net-gifmaker.gif')><\/center>","795da135":"## Dataset Description\n\nThese tweets are collected using Twitter API and a Python script. A query for this high-frequency hashtag (#covid19) is run on a daily basis for a certain time period, to collect a larger number of tweets samples.\n\n**Content**\nThe tweets have #covid19 hashtag. Collection started on 25\/7\/2020, with an initial 17k batch and will continue on a daily basis.\n\n\nThe collection script can be found here: https:\/\/github.com\/gabrielpreda\/covid-19-tweets","d0928223":"## Importing Necesssary Libraries","5267dac0":"**Here I Tried to check if both datasets world city dataset and abg location dataset are having same county codes to track and nothing is missing betwwen them. Found that four country codes are missing in Avg Location Dataset. So added these four country codes and their geo cordinate manually.**"}}