{"cell_type":{"4ef75c07":"code","16fee3fc":"code","8ac2a293":"code","bfdd323a":"code","4756c441":"code","ea9ed2e0":"code","4ba64663":"code","951fa754":"code","08c0c0ba":"code","5317127a":"code","a734702a":"code","e545deef":"code","b6430281":"code","5d2abb55":"code","c31fce5c":"code","f7b85301":"code","1e25de61":"code","8a9430a1":"code","15618d3f":"code","6080b1ec":"markdown","e79b182c":"markdown","046b66af":"markdown","0e784c7e":"markdown","01d9b969":"markdown","71da261a":"markdown","0f28b4c0":"markdown","ac3c47e6":"markdown","24a14809":"markdown","7d793ec7":"markdown","85c8d1b4":"markdown","fb47977c":"markdown","d085fe05":"markdown","2cd75752":"markdown","4e8d08a7":"markdown","bc3932f5":"markdown","dd04fbc5":"markdown","c53f6145":"markdown","e60bcf65":"markdown","a2ba58ad":"markdown"},"source":{"4ef75c07":"!pip install fuggle","16fee3fc":"from fuggle import setup, PlotBar\nsetup(\"spark\")","8ac2a293":"import seaborn as sns\nimport zipfile\nimport os\nimport pandas as pd\n\ndef hist(df:pd.DataFrame, x:str, by=None) -> None:\n    if by is not None:\n        sns.histplot(df, x=x, hue=by)\n    else:\n        sns.histplot(df, x=x)","bfdd323a":"file_list = [\n    '\/kaggle\/input\/instacart-market-basket-analysis\/aisles.csv.zip',\n    '\/kaggle\/input\/instacart-market-basket-analysis\/orders.csv.zip',\n    '\/kaggle\/input\/instacart-market-basket-analysis\/sample_submission.csv.zip',\n    '\/kaggle\/input\/instacart-market-basket-analysis\/order_products__train.csv.zip',\n    '\/kaggle\/input\/instacart-market-basket-analysis\/products.csv.zip',  \n    '\/kaggle\/input\/instacart-market-basket-analysis\/order_products__prior.csv.zip',    \n    '\/kaggle\/input\/instacart-market-basket-analysis\/departments.csv.zip']\n\nfor file_name in file_list:\n    with zipfile.ZipFile(file=file_name) as target_zip:\n        target_zip.extractall()\n    name = file_name.split(\"\/\")[-1][:-4]\n    pd.read_csv(name).to_parquet(name[:-4]+\".parquet\")\n    os.remove(name)","4756c441":"%%fsql\n-- load the raw orders data from file into a dataframe\nLOAD \"\/kaggle\/working\/orders.parquet\"\n\n-- consume the loaded dataframe, convert eval_set -> eval, and remove eval_set\nSELECT *, \n    CASE WHEN eval_set='prior' THEN 0\n         WHEN eval_set='train' THEN 1\n         ELSE 2 END AS eval\nDROP COLUMNS eval_set\n\n-- for each user_id, compute the total number of orders, as a new column of the previous dataframe\n-- question for you, why do we use a window function here not MAX, GROUP BY?\nSELECT *, MAX(order_number) OVER (PARTITION BY user_id) AS num_orders\n\n-- invert the order_number, for inv_order_number, it starts from 0, and smaller means newer\nSELECT *, num_orders - order_number AS inv_order_number\n\n-- save the processed dataframe into a new file, load it back as res, for visualization\nres = SAVE AND USE OVERWRITE \"\/kaggle\/working\/orders_all.parquet\"\n-- print res\nPRINT\n\n-- for each user_id, the latest order must be either train or test, can't be eval\nTAKE 1 ROW FROM res PREPARTITION BY user_id PRESORT order_number DESC\nSELECT COUNT(*) AS ct WHERE eval=0\n-- make sure the count is 0\nPRINT\n\n-- print the size of prior, train, test set\nSELECT eval, COUNT(DISTINCT order_id) AS ct FROM res GROUP BY eval\nPRINT\n\n-- draw the distribution of number of orders per users\nSELECT eval, user_id, MAX(order_number) AS num_orders FROM res GROUP BY eval, user_id\nOUTPUT USING hist(x=\"num_orders\")","ea9ed2e0":"%%fsql\n-- union prior and train data so that it's easy for the following steps\nSELECT *, 0 AS eval FROM (LOAD \"\/kaggle\/working\/order_products__prior.parquet\")\nUNION ALL\nSELECT *, 1 AS eval FROM (LOAD \"\/kaggle\/working\/order_products__train.parquet\")\n\n-- normalize the position in cart -> cart_pos\n-- notice: we use window again instead of group by\norder_products = \n    SELECT *, (add_to_cart_order-1)\/MAX(add_to_cart_order) OVER (PARTITION BY order_id, product_id) AS cart_pos\n    \n-- load products metadata and join with the main dataframe\nproducts = LOAD \"\/kaggle\/working\/products.parquet\"\n\nSELECT order_products.*, department_id, aisle_id\n    FROM order_products INNER JOIN products ON order_products.product_id = products.product_id\n    \n-- save the processed order-product dataframe, load back for visualization\nres = SAVE AND USE OVERWRITE \"\/kaggle\/working\/order_products_all.parquet\"\nPRINT\n\n-- print the size of prior and train data, and compare with the previous output to make sure they match\nSELECT eval, COUNT(DISTINCT order_id) AS ct FROM res GROUP BY eval\nPRINT\n\n-- draw the distribution of number of products per order\nSELECT order_id, COUNT(product_id) AS num_products FROM res GROUP BY order_id\nSELECT num_products, COUNT(*) AS ct GROUP BY num_products\nOUTPUT USING PlotBar(x=\"num_products\", title=\"products per order\")","4ba64663":"%%fsql\n-- load back the dataframes generated by the last two steps\norders = LOAD \"\/kaggle\/working\/orders_all.parquet\"\nproducts = LOAD \"\/kaggle\/working\/order_products_all.parquet\"\n\n-- inner join\ndata =\n    SELECT orders.*, product_id,reordered,department_id,aisle_id,cart_pos\n    FROM orders INNER JOIN products ON orders.order_id=products.order_id\n    \n-- save and print a few rows to make sure it's fine\nSAVE AND USE OVERWRITE \"\/kaggle\/working\/all.parquet\"\nPRINT","951fa754":"%%fsql\n-- load the entire dataset\nall = LOAD \"\/kaggle\/working\/all.parquet\"\n\n-- take 1% users as the sample\nusers = SAMPLE 1 PERCENT SEED 0 FROM (SELECT DISTINCT user_id)\n\n-- use the sample users to filter the original dataset, so we make sure each user's data is complete\nSELECT * FROM all LEFT SEMI JOIN users ON all.user_id = users.user_id PERSIST\n\n-- since the sample_set will be very small, we can use YIELD DATAFRAME here so later steps can consume sample_set directly\nYIELD DATAFRAME AS sample_set","08c0c0ba":"from fugue import WorkflowDataFrames, WorkflowDataFrame, FugueWorkflow\n\ndef join_all(dag:FugueWorkflow, dfs:WorkflowDataFrames, how:str=\"inner\") -> WorkflowDataFrame:\n    return dag.join(dfs, how=how)\n\nrecent_n=5","5317127a":"%%fsql native\na = CREATE [[0,1,10],[0,2,20]] SCHEMA user:int,prod:int,v1:int\nb = CREATE [[0,1000]] SCHEMA user:int,v2:int\nc = CREATE [[0,1,100],[0,2,200]] SCHEMA user:int,prod:int,v3:int\nSUB a,b,c USING join_all(how=\"inner\")\nPRINT","a734702a":"%%fsql\nlabels = SELECT user_id, product_id, 1 AS label FROM sample_set WHERE eval=1 AND reordered=1\nprior_data = SELECT * FROM sample_set WHERE eval=0\n\n-- average cart position for each product\np_cart_pos = SELECT product_id, AVG(cart_pos) AS p_cart_pos FROM prior_data GROUP BY product_id\n\n-- user-product level features\nup_features = \n    SELECT user_id, product_id, \n           AVG(cart_pos) AS up_cart_pos,\n           MAX(reordered) AS reordered, -- as long as there is one reordered, it's reordered\n           COUNT(*)\/(MAX(inv_order_number)+1) AS up_order_freq\n    FROM prior_data GROUP BY user_id, product_id\n\n-- for each product, how many users ever reordered\np_reorder_ratio = SELECT product_id, SUM(reordered)\/COUNT(*) AS p_reorder_ratio FROM up_features GROUP BY product_id\n\n-- for each user, how many product even reordered\nu_reorder_ratio = SELECT user_id, SUM(reordered)\/COUNT(*) AS u_reorder_ratio FROM up_features GROUP BY user_id\n\n-- truncate the data to keep only recent_n orders, this entire cell can be a jinja template, and variables can be set in previous cells\nrecent = SELECT * FROM prior_data WHERE inv_order_number<{{recent_n}}\n\n-- user-product level recent features\nrecent_features =\n    SELECT user_id, product_id,\n           AVG(cart_pos) AS up_recent_cart_pos,\n           MAX(reordered) AS up_recent_reordered,\n           COUNT(*)\/{{recent_n}} AS up_recent_order_freq\n    FROM recent GROUP BY user_id, product_id\n\n-- join all non-recent features using inner join\nup = SUB up_features,p_cart_pos,p_reorder_ratio,u_reorder_ratio USING join_all\n\n-- construct the final feature set by left joining recent features, fill nulls are done by COALESCE\nfeatures=\n    SELECT up.*,\n        COALESCE(up_recent_cart_pos,1.0) AS up_recent_cart_pos,\n        COALESCE(up_recent_reordered,0) AS up_recent_reordered,\n        COALESCE(up_recent_order_freq,0.0) AS up_recent_order_freq\n    FROM up LEFT OUTER JOIN recent_features \n        ON up.user_id=recent_features.user_id AND up.product_id=recent_features.product_id\n\n-- we only keep the features whose users are in positive labels\ntraining_features = SELECT * FROM features LEFT SEMI JOIN labels ON features.user_id=labels.user_id\n\n-- final join with label to get negative labels, now we get the training data\ntraining =\n    SELECT training_features.*, COALESCE(label,0) AS label\n    FROM training_features \n        LEFT OUTER JOIN labels ON training_features.user_id=labels.user_id AND training_features.product_id=labels.product_id\n    PERSIST\n    YIELD DATAFRAME\n\nPRINT","e545deef":"!pip install 'fugue_incubator==0.0.8'","b6430281":"tdf = training.as_pandas().drop([\"user_id\",\"product_id\"],axis=1)\ntdf","5d2abb55":"%%time\nfrom fugue_tune import Grid, Rand, RandInt, Choice\nfrom fugue_tune.sklearn import sk_space as ss, suggest_sk_model\nfrom fugue_tune.hyperopt import HyperoptRunner\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nspace = sum(\n    ss(LogisticRegression, max_iter=1000),\n    ss(LGBMClassifier,num_leaves=RandInt(20,100), max_depth=RandInt(5,50), learning_rate=Rand(0.8,2.0), n_estimators=RandInt(100,300), random_state=0)\n)\n\n\nsuggest_sk_model(\n    space,\n    tdf,\n    scoring=\"f1\", # score for cross validation\n    serialize_path = \"\/tmp\", # we need specify a temp path for storing some intermediate data\n    objective_runner = HyperoptRunner(max_iter=30, seed=0)\n)\n","c31fce5c":"%%fsql\nsource = LOAD \"\/kaggle\/working\/all.parquet\"\nlabels = SELECT user_id, product_id, 1 AS label FROM source WHERE eval=1 AND reordered=1\nprior_data = SELECT * FROM source WHERE eval=0\n\nup_features = \n    SELECT user_id, product_id, \n           AVG(cart_pos) AS up_cart_pos,\n           MAX(reordered) AS reordered,\n           COUNT(*)\/(MAX(inv_order_number)+1) AS up_order_freq\n    FROM prior_data GROUP BY user_id, product_id\n\np_cart_pos = SELECT product_id, AVG(cart_pos) AS p_cart_pos FROM prior_data GROUP BY product_id\n\np_reorder_ratio = SELECT product_id, SUM(reordered)\/COUNT(*) AS p_reorder_ratio FROM up_features GROUP BY product_id\nu_reorder_ratio = SELECT user_id, SUM(reordered)\/COUNT(*) AS u_reorder_ratio FROM up_features GROUP BY user_id\n\nrecent = SELECT * FROM prior_data WHERE inv_order_number<{{recent_n}}\n\nrecent_features =\n    SELECT user_id, product_id,\n           AVG(cart_pos) AS up_recent_cart_pos,\n           MAX(reordered) AS up_recent_reordered,\n           COUNT(*)\/{{recent_n}} AS up_recent_order_freq\n    FROM recent GROUP BY user_id, product_id\n\nup = SUB up_features,p_cart_pos,p_reorder_ratio,u_reorder_ratio USING join_all\n\nfeatures=\n    SELECT up.*,\n        COALESCE(up_recent_cart_pos,1.0) AS up_recent_cart_pos,\n        COALESCE(up_recent_reordered,0) AS up_recent_reordered,\n        COALESCE(up_recent_order_freq,0.0) AS up_recent_order_freq\n    FROM up LEFT OUTER JOIN recent_features \n        ON up.user_id=recent_features.user_id AND up.product_id=recent_features.product_id\n\n\ntraining_features = SELECT * FROM features LEFT SEMI JOIN labels ON features.user_id=labels.user_id\n\n-- we do a random split by using a rand number\ndata =\n    SELECT training_features.*, COALESCE(label,0) AS label, rand(0)<0.2 AS is_test\n    FROM training_features \n        LEFT OUTER JOIN labels ON training_features.user_id=labels.user_id AND training_features.product_id=labels.product_id\n    PERSIST\n\n-- select test data, clean, and save to a single file so pandas can consume directly\nSELECT * FROM data WHERE is_test\nDROP COLUMNS user_id, product_id, is_test\nSAVE OVERWRITE SINGLE \"\/kaggle\/working\/test.parquet\"\n\n-- select train data, clean, and save to a single file so pandas can consume directly\nSELECT * FROM data WHERE NOT is_test\nDROP COLUMNS user_id, product_id, is_test\nSAVE OVERWRITE SINGLE  \"\/kaggle\/working\/train.parquet\"","f7b85301":"train = pd.read_parquet(\"\/kaggle\/working\/train.parquet\")\ntrain_x = train.drop(\"label\",axis=1)\ntrain_y = train[\"label\"]\ntest = pd.read_parquet(\"\/kaggle\/working\/test.parquet\")\ntest_x = test.drop(\"label\",axis=1)\ntest_y = test[\"label\"]","1e25de61":"%%time\nfrom joblib import dump, load\nfrom sklearn.metrics import f1_score\n\nmodel = LGBMClassifier(random_state=0, num_leaves=23, max_depth=15, learning_rate=1.2983994130341636, n_estimators=132)\nmodel.fit(train_x,train_y)\ndump(model, \"\/kaggle\/working\/model\")\n\npred = load(\"\/kaggle\/working\/model\").predict(test_x)\nf1_score(test_y, pred)","8a9430a1":"# schema: *,pred:int\ndef infer(df:pd.DataFrame, model_path:str) -> pd.DataFrame:\n    model = load(model_path)\n    return df.assign(pred=model.predict(df))\n\ninfer(test_x, \"\/kaggle\/working\/model\")","15618d3f":"%%fsql\nLOAD \"\/kaggle\/working\/test.parquet\"\nDROP COLUMNS label\nTRANSFORM PREPARTITION 16 USING infer(model_path=\"\/kaggle\/working\/model\")\nPRINT","6080b1ec":"## Convert all files to parquet\n\nNow we extract all zip files, load and save as parquet. Parquet is the preferred data format for scalable solutions:\n\n1. It has explicit schema, so you don't need any framework to infer schema, it can improve consistency and reduce overhead.\n2. It is columnar storage and it is normally smaller than CSV.\n3. All major computing frameworks support parquet and optimize the read performance.","e79b182c":"## Featurization\n\nThis is a highly iterative step. But since we use the sample_set, it should be very fast.","046b66af":"## Hyperparameter Tuning\n\nWith the training data generated from the previous step, we are ready to try different models with different parameters.\n\nFirst of all, we will install `fugue_tune`, a library for paramter tuning.","0e784c7e":"Different features will be generated by different featurization logic, each featurizer should output certain keys with the feature values. For this case, we will have user level features, product level features, user-product level features etc. So a big final join must happend (this is also common in any kind of featurization task). In order to make it more convenient, we write a simple fugue extension `join_all` that can be invoked in Fugue SQL code.","01d9b969":"## Explore and transform order data\n\nNow you will see Fugue SQL, it looks like standard SQL but you can see\n\n1. There are additional syntax such as `LOAD`, `DROP`, `SAVE`, they are the necessary features to make SQL a real programming language\n2. You can assign each step's output to a variable just like python.\n3. But most of the time, you don't specify an explicit variable, indicating the steps are consuming the output of the last step. This is called anonymity, a very important feature of Fugue SQL, making your expression simpler. But it's optional, so if you don't like it, you can assign variable for each step.","71da261a":"We import a few dependencies and create a simple python function to draw histograms, it will be used in the later steps","0f28b4c0":"And based on the suggested hyperparameters, we create `LGBMClassifier` and train on 80% of the entire training data. We also save the model to disk. This is because it's the best way to be used by distributed inference.","ac3c47e6":"## Join all data together to be a single dataframe\n\nNotice, this may not be a good idea for two reasons:\n\n1. The joined data has a lot of redundancy and may take a lot of disk space (or memory space, depending on how you want to keep it)\n2. The machine may not be big enough to keep the data, and loading back for compute may also slow down\n\nBut the advantages are also obvious:\n\n1. You no longer need to deal with multiple data sources, the logic can be simplified\n2. It's no longer necessary to join data sources to extract information, you could gain speed when sacrificing space\n\nYou really need to make the decision based on specific cases. For this case, the joined dataframe is small in the parquet format, so it's a great idea to do so. For your practical case (out of Kaggle), you may base on the main cloud providers with unlimited storage space to use, and it's also a good idea to do so, as long as you clean up the temporary file after the job.\n\nAlso, in general, if permanent space is not concern, it's a great idea to save intermediate data into permanent space, so even your compute environment is restarted, you can still resume from the last step. Reading large amount of data from disk isn't as slow as you think.","24a14809":"## Summary\n\nIn this notebook, we have demonstrated how to use Fugue on different steps of ML pipeline. And we also showed that Fugue SQL is a great language for machine learning. It helps you write scale agnostic code. And with Fugue framework, it's much easier to test each step of your pipeline.\n\nIt is worth to note that, Fugue also has functional API. If you are not a fan of SQL, then you may consider the functional API.\n\nYou may have noticed that, Fugue related code is minimal in the entire notebook. We solved this problem mostly by standard SQL and native python.","7d793ec7":"And before using `join_all` in the pipeline, we can test if it is working. Here we use `%%fsql native` meaning that we test the logic using NativeExecutionEngine, which is using pandas rather than spark. Since Fugue guarantees the same expression generates the same result regardless of execution engine, this is a practical approach to test your logic without heavy dependency, and this can also become a unittest to stay in your codebase.","85c8d1b4":"## Explore and transform Order-Product data\n\n","fb47977c":"Now, let's convert the training dataframe to pandas dataframe dropping irrelevant columns","d085fe05":"Now, we just need the simplest pandas opeations","2cd75752":"# Building Scalable ML Pipeline with Fugue\n\nIn this demo, we will show how to use Fugue to do preprocessing, hyperparameter tuning and distributed inference.\n\nYou will find minimal presense of Fugue specific code in this notebook, because the philosophy of Fugue is that you should try to use native python and SQL to solve problems, and use Fugue as a glue only when necessary.\n\nIn this particular case, we want to demonstrate that SQL is a great language for ML preprocessing including data cleaning and featurization. Fugue SQL is mostly standard SQL but with enhanced syntax to let you do more with less code.\n\nWe are not competing with other solutions, we only provide general ideas of what should be done and can be done for a scalable ML pipeline.\n\n\n## Links \n\nFugue is a pure abstraction layer that makes code portable across differing computing frameworks such as Pandas, Spark and Dask. It allows users to write code compatible across all 3 frameworks. It guarantees consistency regardless of scale and a unified framework for compute.\n\n[Fugue Repo](https:\/\/github.com\/fugue-project\/fugue)\n\n[Fugue Slack](https:\/\/join.slack.com\/t\/fugue-project\/shared_invite\/zt-jl0pcahu-KdlSOgi~fP50TZWmNxdWYQ)","4e8d08a7":"## Setup environment\n\nNow we use the fuggle package (Fugue customized environment for Kaggle) and set the default execution engine to Spark. Yes, even on Kaggle machines, Spark can still be faster than pandas (if you use appropriately).\n\nThe `setup` will create shortcut to use Spark, Dask and Pandas. It will also enable the cell highlight for ``%%fsql`` - FugueSQl cells in notebook","bc3932f5":"## Distributed inference\n\nDistributed inference is a much simpler problem compared with distributed training and tuning. Using Fugue, it is even simpler. You can treat this as a transformation problem, so you only need to write a transformer to do the inference. And to create a transformer, you only write the simplest python code with no dependency on Fugue.","dd04fbc5":"`fugue_tune` supports hybrid search, you can define a search space with model sweeping + grid search + bayesian optimization. Notice the `space` expression in the following code, that is how you define a search space for sklearn compatible models. In this case, we are going to try the simplest LogisticRegression and LGBM classifier.\n\nIn `LGBMClassifier`, we defined a few hyperparamters, and we want to do bayesian optimization so we use `Rand` or `RandInt`. If we want to do grid search on the parameter, we can do for example `num_leaves=Grid(20,30,40)`.\n\nFugue treats grid search differently, it will fully parallelize the grid search, and then do sequential bayesian optimization in each independent task, so to maximize the parallelism. On Kaggle machine, it has only 4 cores, so in the following case, we didn't use grid search, because it will not work well. But with a real cluster, grid search can be a lot faster than bayesian optimization.\n\nIn general, grid search requires more computing resource, takes less time, while bayesian optmization is the opposite. So when you define the space using `fugue_tune`, you try to find a way to balance time and cost for your problem.","c53f6145":"Now we can use this extension in Fugue SQL. `PREPARTITION` is added to ensure the data is well distributed so to achieve better load balance.","e60bcf65":"With `suggest_sk_model` we get a suggestion for the most promising hyperparameter combinations. And without surprise, `LGBMClassifier` outperforms `LogisticRegression`. We must notice that currently, the model is trained and cross validated on only 1% of the entire training data. We use the small data to achieve faster iteration.\n\nWhen we get the suggested model, we should retrain the model with the full training data. So should we also do hyperparameter tuning on the entier training data? It depends on how fast you want the iteration to be, and how much compute resource you have.\n\n## Apply the featurization on the entire dataset\n\nTo apply the process to the entire dataset is straightforward, you only need to change the input data. Fugue is scale agnostic, so the main part of the featurization logic is not changed.\n\nIn real situations, the following step may be done with a powerful cluster, you also need to pay attention to specify how much compute resource you want to use. For example if you use spark, for the previous steps, you may use small cluster, but at this step, you should increase the number of instances, cores and memory accordingly.","a2ba58ad":"## Preparation for building featurization logic\n\nFeaturization is one of the most important steps in machine learning. You can't expect to extract the best features at one time, it must be an iterative process. You extract, test, think and adjust.\n\nSo for building a practical machine learning pipeline, making the featurization step isolated and extendable is even more important than what feature you extracted. The higher level design must be clean and flexible so later on you have more time to focus on what features you want.\n\nYou need to pay attention to a few general rules:\n\n1. For faster iteration and faster prototyping, you should sample the original data to make it small enough to work on\n2. For robustness, you need to make your logic testable. Smaller dataset is helpful, you can also try to separate your featurization logic from the infra and from each other, try to implement some core feature logic using simple python so you can unit test them\n3. Before using, you should test the entire featurization logic with full scale to make sure you cover all edge cases and there is no performance issue.\n\nIt has to be an iterative process, be patient but don't be slow."}}