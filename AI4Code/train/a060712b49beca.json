{"cell_type":{"8dae9fa4":"code","3a01fb11":"code","b9503b3b":"code","e63a75d1":"code","3ae74323":"code","3a67dfa4":"code","d6fd64f1":"code","e6ef0a47":"code","1d50cdb4":"code","d9b99add":"code","0d5d09db":"code","810a0d32":"code","d87e3523":"code","abc15552":"code","805e1829":"code","ea62cafb":"code","a7e2cc8f":"code","25966955":"code","fd55b2f0":"code","e2d46fa2":"code","158c227b":"code","7dccc128":"code","cff0d6c3":"code","eab95107":"code","8cfca74d":"code","c0234c75":"code","1f824839":"code","aed424ca":"code","8824af50":"markdown","05607a12":"markdown","d029f78b":"markdown","5caf11c8":"markdown","68a23a4b":"markdown","879e30c1":"markdown","263dfe4b":"markdown","8e617d0a":"markdown","b8eb785d":"markdown","3bc152d0":"markdown","124e6d0e":"markdown","6c7d0b6d":"markdown","e446c397":"markdown","fd15cfcf":"markdown","fb1fa3a3":"markdown","34dd4801":"markdown","61ccb90d":"markdown","74d2f3c3":"markdown","2271249f":"markdown","bf4d4b33":"markdown","7667197b":"markdown"},"source":{"8dae9fa4":"#Import libraries for text import\nimport glob\nimport os\n\n#Import libraries for generating word clouds\nfrom wordcloud import WordCloud,STOPWORDS\nimport matplotlib.pyplot as plt\n\n#Import libraries for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Import libraries for developing database \nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport re\nimport geopandas as gpd\nimport geoplot\n\n#Import libraries for sentiment analysis\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.util import bigrams\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist\nfrom nltk.collocations import *\nfrom nltk.draw.dispersion import dispersion_plot","3a01fb11":"#remove warning messages that reduce readability\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b9503b3b":"#Save speech data separated by rally\nlist_of_speech = []\nlist_of_rally = []\n\n#Generate corpus using all speeches\ncorpus = \"\"\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        file_path = os.path.join(dirname, filename)\n        print(file_path)\n        text = open(file_path,'r').read()\n        list_of_speech.append(text)\n        rallyname = (file_path.rsplit('\/', 1)[-1])[:-4]\n        list_of_rally.append(rallyname)\n        corpus = corpus + text\n\n## Reference: Abdelkader Rhouati[https:\/\/www.kaggle.com\/arhouati\/analysis-of-trump-speech-what-he-talk-about]\n## Reference: Nitesh Halai[https:\/\/www.kaggle.com\/niteshhalai\/donald-trump-s-rallies-nlp]","e63a75d1":"#Change all to lowercase and remove symbols other than periods\nshortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\nfor speech in list_of_speech:\n    speech = speech.lower()\n    speech = re.sub(r'[^\\w\\s]', '', speech) \n    speech = shortword.sub('', speech)\n    \ncorpus = corpus.lower()\ncorpus = re.sub(r'[^\\w\\s]', '', corpus)\ncorpus = shortword.sub('', corpus)","3ae74323":"#Split filename into rally location and date\nelems = []\nfor r in list_of_rally:\n    r_new = re.sub(r'\\b\\w{1,2}\\b', '', r)\n    elem = re.sub( r\"([A-Z])\", r\" \\1\", r_new).split()\n    elems.append(elem)","3a67dfa4":"#Create list of rally dates\ndates = []\n\n#Retrieve date info from text and add to dates list\nfor elem in elems:\n    date = elem[-1]\n    date = date.replace('_', '\/') \n    date = date[0:3] + '\/' + date[3:]\n    dates.append(date)\n    elem = elem.remove(elem[-1])\n    \n#Change date format from string to datetime\ndate_update = []\nfor date in dates:\n    date_new = datetime.strptime(date, '%b\/%d\/%Y')\n    date_update.append(date_new)","d6fd64f1":"#Create list of rally locations\nlocations = []\n\n#Retrieve location info from text and add to locations list\nfor elem in elems:\n    loc = ' '.join(elem)\n    locations.append(loc)","e6ef0a47":"#Define set of stop words for remaining analysis\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use', 'going', 'said', 'you','and'])","1d50cdb4":"#Frequency of words in each speech and remove stop words from count\nspeech_fdist = []\nfdist = FreqDist()\nfor speech in list_of_speech:\n    for sentence in sent_tokenize(speech):\n        sentence = re.sub(r'\\b\\w{1,2}\\b', '', sentence)\n        sentence = re.sub(r'[^\\w]', ' ', sentence)\n        for word in word_tokenize(sentence):\n            if word not in stop_words: \n                fdist[word] += 1\n    speech_fdist.append(fdist)\n    fdist = FreqDist()\n","d9b99add":"#Initialize sentiment analyzer\nsid = SentimentIntensityAnalyzer()","0d5d09db":"#Assign sentiment for each speech\npositive_sentiment = []\nnegative_sentiment = []\nneutral_sentiment = []\n\nfor speech in list_of_speech:\n            sentiment = sid.polarity_scores(speech)\n            positive_sentiment.append(sentiment['pos'])\n            negative_sentiment.append(sentiment['neg'])\n            neutral_sentiment.append(sentiment['neu'])","810a0d32":"#Generate final data frame\ndict = {'Date':date_update, 'Location':locations, 'Speech': list_of_speech, \n        'Speech Word Frequency': speech_fdist, 'Positive_Sentiment':positive_sentiment, \n        'Negative_Sentiment':negative_sentiment, 'Neutral Sentiment': neutral_sentiment}  \ndf = pd.DataFrame(dict)","d87e3523":"#Preview dataset\ndf.head(10)","abc15552":"#See datatypes of entries\ndf.info()","805e1829":"#Plot charts to visualize speech sentiment\nlabels = 'Positive', 'Negative', 'Neutral'\n\nfor i in range(len(df)) :\n    pos = df.iloc[i,4] * 100\n    neg = df.iloc[i,5] * 100\n    neu = df.iloc[i,6] * 100\n    sizes = [pos, neg, neu]\n    fig1, ax1 = plt.subplots()\n    ax1.pie(sizes, labels=labels, autopct='%1.1f%%')\n    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\nplt.show()","ea62cafb":"#tokenize speech text corpus\ncorpus_tokenize = word_tokenize(corpus)\n#remove smaller insignificant texts and stop words\ncorpus_tokenize_list = []\nfor corp in corpus_tokenize:\n    new_corp = re.sub(r'\\b\\w{1,2}\\b', '', corp)\n    new_corp = re.sub(r'\\W+', '', new_corp)\n    if new_corp not in stop_words:\n        corpus_tokenize_list.append(new_corp)\n    \n#rejoin tokenized corpus as one text string\nnew_corpus_tokenize = \" \".join(corpus_tokenize_list)","a7e2cc8f":"#Generate word cloud from corpus (which is text from all speeches)\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, colormap='Pastel1', \n                      collocations=False, stopwords = stop_words).generate(new_corpus_tokenize)","25966955":"# Plot\nplt.figure(figsize=(30,30))\nplt.axis('off')\nplt.imshow(wordcloud)","fd55b2f0":"# #develop bigrams for speeches\n# bigram_speech = []\n# speeches = df.Speech\n# for speech in speeches:\n#     speech = re.sub(r'\\b\\w{1,2}\\b', '', speech)\n#     speech = re.sub(r'[^\\w]', ' ', speech)\n#     speech_bigrams = (bigrams(word_tokenize(speech)))\n    \n# #convert bigram tuples to strings\n# for bigram in speech_bigrams:\n#     bigram = ' '.join(bigram) \n#     bigram_speech.append(bigram)","e2d46fa2":"#develop bigrams for speeches\n#speech_bigrams_2 = (bigrams(corpus_tokenize))","158c227b":"# remove_from_set = set()     #remove unimportant words\/phrases\n# unique_set = set()          # identify nouns identified based on article use\n# the_set = set()\n# for i in mostCommonBigrams:\n#     if (i[0][0] in ('and','we','you','they','the','that')):\n#         remove_from_set.add(i)\n#     if (i[0][0] in ('the')):\n#         the_set.add(i)\n#     if (int(i[1]) <= 1000 and int(i[1]) > 100):  #select based on range of frequency\n#         unique_set.add(i)\n#unique_set = unique_set - remove_from_set","7dccc128":"# #get frequecy of word-pairs\n# bi_grams_freq = FreqDist(speech_bigrams_2)\n# mostCommonBigrams= bi_grams_freq.most_common(100)\n\n# bigram = [bigram[0] for bigram in mostCommonBigrams]\n# count = [bigram[1] for bigram in mostCommonBigrams]","cff0d6c3":"#develop bigrams for speeches\nbigram_speech = []\nspeeches = df.Speech\nfor speech in speeches:\n    speech = re.sub(r'\\b\\w{1,2}\\b', '', speech)\n    speech = re.sub(r'[^\\w]', ' ', speech)\n    speech_bigrams = (bigrams(word_tokenize(speech)))\n    \n#convert bigram tuples to strings\nfor bigram in speech_bigrams:\n    bigram = ' '.join(bigram) \n    bigram_speech.append(bigram)","eab95107":"#get frequecy of word-pairs\nbi_grams_freq = FreqDist(bigram_speech)\nmostCommonBigrams= bi_grams_freq.most_common(100)\n\nbigram = [bigram[0] for bigram in mostCommonBigrams]\ncount = [bigram[1] for bigram in mostCommonBigrams]","8cfca74d":"#plot bigram frequency\nplt.figure(figsize=(50,30))\nplt.bar(bigram,count)\nplt.xticks(rotation='vertical', size=20)\nplt.grid(True, lw = 0.65)\nplt.show()","c0234c75":"#Topics or words we want to look for\ntopics = ['america','china','biden','democrat','great','democracy','government','people','economy']","1f824839":"#Once plot it for the overall corpus\nplt.figure(figsize=(16,5))\ndispersion_plot(corpus_tokenize_list,topics)\nplt.show()","aed424ca":"#Plot dispersion plot for each speech\nfor speech in list_of_speech:\n    speech = word_tokenize(speech)\n    dispersion_plot(speech,topics)\n    plt.show()","8824af50":"## Generate Word Cloud","05607a12":"## Introduction","d029f78b":"The purpose of this project is to carry out NLP analysis of Donald Trump's political rally speeches. <br> Our goal is to identify any characteristic features of these speeches, and to determine if there is any relationships between the nature of the speech and the time and place it was given.","5caf11c8":"### Bigrams","68a23a4b":"## Import Libraries","879e30c1":"## Generate Dataset","263dfe4b":"### Extract Rally Dates","8e617d0a":"## Lexical Dispersion Plot","b8eb785d":"We want to gain more information about his choice of words, the overall sentiment of the speeches, the key topics of speeches, if speeches differ over time and place. <br>\nTo do this we will generate a dataset including data regarding:\n1. Date when each speech was made for when we want to evaluate speeches over time\n2. Locations of the rallies to see if the city and state influence the speech\n3. Frequency of words from each speech to determine the general topic\n4. Overall speech sentiments to see if the tone of the speeches change over time","3bc152d0":"## Tokenization","124e6d0e":"## Import Text Data","6c7d0b6d":"### Speech Sentiment","e446c397":"## Dataset Analysis","fd15cfcf":"### Clean up Rally Data","fb1fa3a3":"### Generate Final Dataset","34dd4801":"From this we can see that the majority of speeches have a neutral sentiment majority. This means that the choice of words in those speeches were not outright positive or negative.","61ccb90d":"### Word Count per Speech","74d2f3c3":"The purpose of a lexical dispersion plot is to identify key topics or words used in the text. Each word occurrence is a strip denoting it's position relative to the entire text.\n\nIn this case we are using it to identify what is the main topic of discussion, or what are common words used, in the speeches.","2271249f":"We can start off by making a word cloud using text from all of the speeches to see if the choice and frequency of words used demonstrates anything interesting about the overall nature of his speeches.","bf4d4b33":"Now we will develop some bigrams to look into common phrases from the speeches.","7667197b":"### Extract Rally Locations"}}