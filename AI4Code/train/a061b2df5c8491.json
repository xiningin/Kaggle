{"cell_type":{"519b07a6":"code","77fa5e57":"code","c609b1a2":"code","c8d0a82d":"code","ef121610":"code","d09a06f1":"code","cc74d094":"code","acc7fe17":"code","a93f7e17":"code","b79a8d89":"code","0217935b":"code","792c496a":"code","3a707a03":"code","3baed9c6":"code","5b1e5a11":"code","930f7c84":"code","f769ac3f":"code","a3e93d01":"code","285ec856":"code","2bccbd77":"code","d03001e8":"code","1907c53d":"code","83ef4901":"code","fd069286":"code","2ad0cbdb":"code","5d135eed":"code","f7731dba":"code","6cbe071a":"code","2cdc1d5e":"code","77f57088":"code","229a1b74":"code","bf9bf197":"code","b7ed8824":"code","8aae6349":"code","04a59e37":"code","f05f8845":"code","09564101":"code","35df68ea":"code","577411a7":"code","9fc24e15":"code","0c43f28c":"code","8ab237f0":"code","a837dd26":"code","8d3a102e":"code","5e4a6423":"code","cca65fe4":"code","40e0fe35":"code","6a0d1b75":"code","7e801054":"code","359916df":"code","720d9ade":"code","a67cd97e":"code","bae79961":"code","62dbdccf":"code","753113b5":"code","b4d1c5ab":"code","0e19fc3e":"code","89fcd7f5":"code","944ba182":"code","8971e8a8":"code","91171328":"code","4c639ad3":"code","35e078ce":"code","d5fc129a":"code","09fdf00d":"code","e20978de":"code","c79c83f0":"code","83b6345f":"code","d8edfbe4":"code","aa52e1e6":"code","c7272f3d":"code","729db819":"code","94cdd6f4":"code","5bb2d9a7":"code","1a028b75":"code","377ffce0":"code","94f7a458":"code","c2be9705":"markdown","6d4e5804":"markdown","04212386":"markdown","cb947c40":"markdown","e17d10b1":"markdown","9620f06a":"markdown","13300a05":"markdown","b6bfc8c2":"markdown","c6967c86":"markdown","0a6fcc02":"markdown","08769da1":"markdown","eef72f5f":"markdown","c575e965":"markdown","bef287f5":"markdown","10a72aad":"markdown","b04885a5":"markdown","bc256a19":"markdown","64eb58b3":"markdown","75214c82":"markdown","868adf6c":"markdown"},"source":{"519b07a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","77fa5e57":"# Import Pandas and NumPy\nimport pandas as pd, numpy as np","c609b1a2":"# Importing all datasets\nimport pandas as pd\ndf = pd.read_csv(\"..\/input\/telecom_churn_data.csv\")","c8d0a82d":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","ef121610":"\n\ndf.head()","d09a06f1":"# The dimensions of the dataframe\ndf.shape","cc74d094":"df.info()","acc7fe17":"# Some columns are not useful for us. We can remove them directly\ncolumns_to_drop = ['last_date_of_month_6', 'last_date_of_month_7', 'last_date_of_month_8','last_date_of_month_9',\\\n                   'mobile_number','circle_id']\ndf=df.drop(columns_to_drop,axis=1)","a93f7e17":"# date_of_last_rech is not useful as recharge amount directly gives insight of this recharge on that month\n# date_of_last_rech_data also is directly inferred from total_rech_data\ncolumns_to_drop = ['date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8','date_of_last_rech_9',\\\n                   'date_of_last_rech_data_6','date_of_last_rech_data_7','date_of_last_rech_data_8','date_of_last_rech_data_9']\ndf=df.drop(columns_to_drop,axis=1)","b79a8d89":"df.shape","0217935b":"# Check column-wise null percentages\nround(100*(df.isnull().sum()\/len(df.index)),2)","792c496a":"# All columns which has more than 30% values null\ncolumns_to_drop = df.columns[df.apply(lambda df : round(100*(df.isnull().sum()\/len(df.index)),2)>30)]\n# All the columns getting dropped are consistent across June to September months, So our model wouldnt be affected.\ncolumns_to_drop","3a707a03":"# Removing all columns which has more than 30% values null\ndf=df.drop(columns_to_drop,axis=1)","3baed9c6":"# Columns which have only one unique value other than blank\ncolumns_to_drop = df.columns[df.apply(lambda df : df.nunique()==1)]\n# All the columns getting dropped are consistent across June to September months, So our model wouldnt be affected.\ncolumns_to_drop","5b1e5a11":"# Removing all columns with only one unique value\ndf=df.drop(columns_to_drop,axis=1)","930f7c84":"# Check column-wise null percentages\ndf.columns[df.apply(lambda df : round(100*(df.isnull().sum()\/len(df.index)),2)>0)]\n# There are still columns with NaN.","f769ac3f":"# Fill all NaN with median values\ndf = df.fillna(df.median())","a3e93d01":"# High value customers are determined by average recharge amount in the first two months\ndf['average_recharge_amount_per_month_good'] = (df['total_rech_amt_6'] + df['total_rech_amt_7'])\/2","285ec856":"df['average_recharge_amount_per_month_good'].describe(percentiles = [0.70])\n# 70th percentile cutoff is 368.50","2bccbd77":"# Retrieve high value customers\nhigh_value_customers = df[(df['average_recharge_amount_per_month_good']>368.50)]\nhigh_value_customers.shape\n# We have 29979 rows and 166 columns","d03001e8":"# Sum all the columns showing customer is active\nhigh_value_customers['usage'] = high_value_customers[['total_ic_mou_9','total_og_mou_9', 'vol_2g_mb_9', 'vol_3g_mb_9']]\\\n                                .sum(axis = 1) ","1907c53d":"# calculate the churn column and drop the newly created 'usage' column, which is no longer required\nhigh_value_customers['churn'] = np.where(high_value_customers['usage']> 0, 0,1)\nhigh_value_customers = high_value_customers.drop('usage',axis=1)\nhigh_value_customers['churn'].head()","83ef4901":"# Drop all columns with '_9' in it\nhigh_value_customers = high_value_customers.loc[:, ~high_value_customers.columns.str.contains('_9')]\nhigh_value_customers.shape","fd069286":"# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%\nhigh_value_customers.describe(percentiles=[.25, .5, .75, .90, .95, .99])\n# Almost all the columns seem to have outliers.","2ad0cbdb":"high_value_customers = np.minimum(high_value_customers, pd.DataFrame(high_value_customers.quantile(0.99)).T)\n# Checking outliers at 1%, 25%, 50%, 75%, 90%, 95% and 99% after setting 99th percentile to Max.\nhigh_value_customers.describe(percentiles=[.01, .25, .5, .75, .90, .95, .99])","5d135eed":"from sklearn.model_selection import train_test_split","f7731dba":"# Putting feature variable to X\nX = high_value_customers.drop(['churn'], axis=1)\n\nX.head()","6cbe071a":"# Putting response variable to y\ny = high_value_customers['churn']\n\ny.head()","2cdc1d5e":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","77f57088":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","229a1b74":"X_train[X_train.columns] = scaler.fit_transform(X_train)\nX_train.head()","bf9bf197":"### Checking the Churn Rate\nchurn_rate = (sum(high_value_customers['churn'])\/len(high_value_customers['churn'].index))*100\nchurn_rate\n# churn_rate is very less (8.64%). We need to handle class imbalance while building model","b7ed8824":"# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","8aae6349":"# Let's see the correlation matrix \nplt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(high_value_customers.corr(),annot = True)\nplt.show()\n# It is difficult to visualize wit 127 columns","04a59e37":"corr_matrix = high_value_customers.corr().abs()\n\nsol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n                 .stack()\n                 .sort_values(ascending=False))\nsol.head(20)\n\n# There are many features very high (+ve\/-ve) corelation\n# Instead of dropping these features, we can perform a PCA","f05f8845":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(random_state=100)","09564101":"# Perform PCA on normalized data frame\npca.fit(X_train)","35df68ea":"#Create PCA data frame from performed PCA\ncolnames = list(X_train.columns)\npca_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1],'PC3':pca.components_[2],\n                       'PC4':pca.components_[3],'PC5':pca.components_[4],\n                       'PC6':pca.components_[5],'PC7':pca.components_[6],'PC8':pca.components_[7],\n                       'PC9':pca.components_[8],'Feature':colnames})\npca_df","577411a7":"print(\"Variance of each component is \" , pca.explained_variance_ratio_, \"\\n\")\nprint(\"Cumulative variance of component is \" , np.cumsum(pca.explained_variance_ratio_))","9fc24e15":"# Screeplot to check components explaining variance\n%matplotlib inline\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative variance')\nplt.show()\n# As can be seen in number as well as screeplot, 48 components explain 90.08 variance","0c43f28c":"prdt = np.dot(pca.components_[0],pca.components_[1])\nprdt.round(10)\n# First two components are orthogonal to each other","8ab237f0":"# Use PCA with 48 components, as they cover 90% of variance\npca_final = PCA(n_components=48)","a837dd26":"df_pca = pca_final.fit_transform(X_train)\ndf_pca.shape","8d3a102e":"#creating correlation matrix for the principal components\ncorrmat = np.corrcoef(df_pca.transpose())","5e4a6423":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (20,20))   # Fig size\nsns.heatmap(corrmat)\n# Plot shows the almost no correlation among the Principal components.","cca65fe4":"corrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\nprint(\"max corr:\",corrmat_nodiag.max(), \", min corr: \", corrmat_nodiag.min(),)\n# correlations are very close to 0","40e0fe35":"# Scale X_test before transformation\nX_test[X_test.columns] = scaler.transform(X_test)","6a0d1b75":"# Applying selected components to the test data - 48 components\n\ndf_test_pca = pca_final.transform(X_test)\ndf_test_pca.shape","7e801054":"# Modeling using SVC\n# Data Modeling using Linear model\nfrom sklearn.svm import SVC\n\n# class_weight = balanced to handle class imbalance\nmodel_linear = SVC(kernel='linear', class_weight='balanced')\nmodel_linear.fit(df_pca, y_train)\n\n# predict\ny_pred = model_linear.predict(df_test_pca)","359916df":"from sklearn import metrics\n# accuracy of the linear model\nprint(\"accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")","720d9ade":"print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))\n\n# recall\/sensitivity\nprint(\"recall\", metrics.recall_score(y_test, y_pred))","a67cd97e":"# Data Modeling using non-Linear model with rbf\n# Use rbf kernel\n\n# model\n# class_weight = balanced to handle class imbalance\nnon_linear_model = SVC(kernel='rbf', class_weight='balanced')\n\n# fit\nnon_linear_model.fit(df_pca, y_train)\n\n# Note scaling was already performed on X_test during linear model prediction. No need to scale it again.\n\n# predict\ny_pred = non_linear_model.predict(df_test_pca)","bae79961":"# accuracy of the non linear model\nprint(\"accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")","62dbdccf":"print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))\n\n# recall\/sensitivity\nprint(\"recall\", metrics.recall_score(y_test, y_pred))","753113b5":"#Training the model on the train data\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlearner_pca = LogisticRegression(class_weight='balanced')\nmodel_pca = learner_pca.fit(df_pca,y_train)","b4d1c5ab":"# Making prediction on the test data - Accuracy\npred_probs_test = model_pca.predict_proba(df_test_pca)[:,1]\n\"{:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test))","0e19fc3e":"# Predict the values in y_pred\ny_pred = model_pca.predict(df_test_pca)","89fcd7f5":"print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))","944ba182":"# recall\/sensitivity\nprint(\"Recall \/ Sensitivity\", metrics.recall_score(y_test, y_pred))","8971e8a8":"# precision\nprint(\"Precision\", metrics.precision_score(y_test, y_pred))","91171328":"# Importing random forest classifier from sklearn library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create decision tree classifer object\nrfc = RandomForestClassifier(class_weight=\"balanced\")","4c639ad3":"# fit\nrfc.fit(X_train,y_train)","35e078ce":"# Making predictions\npredictions = rfc.predict(X_test)","d5fc129a":"print(metrics.confusion_matrix(y_true=y_test, y_pred=predictions))","09fdf00d":"# recall\/sensitivity\nprint(\"Recall \/ Sensitivity\", metrics.recall_score(y_test, predictions))","e20978de":"\nfrom sklearn.grid_search import GridSearchCV\nparam_grid = {\n    'max_depth': [2,8,10],\n    'min_samples_leaf': range(100, 400, 200),\n    'min_samples_split': range(200, 500, 200),\n    'n_estimators': [100,200, 300], \n    'max_features': [5, 10]\n}\n# Create a based model\nrf = RandomForestClassifier(class_weight=\"balanced\")\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, scoring=\"recall\",\n                          cv = 3, n_jobs = -1,verbose = 1)","c79c83f0":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)","83b6345f":"# printing the optimal score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","d8edfbe4":"# model with the best hyperparameters\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=10,\n                             min_samples_leaf=100, \n                             min_samples_split=400,\n                             max_features=10,\n                             class_weight=\"balanced\",\n                             n_estimators=100)","aa52e1e6":"# fit\nrfc.fit(X_train,y_train)","c7272f3d":"# predict on Final RandomForestClassifier model with optimized hyper paramaters\npredictions = rfc.predict(X_test)","729db819":"print(metrics.confusion_matrix(y_true=y_test, y_pred=predictions))\n# Confusion matrix looks good with low False Negative and high True Negative","94cdd6f4":"# Recall \/ sensitivity\nprint(\"Recall \/ Sensitivity\", metrics.recall_score(y_test, predictions))","5bb2d9a7":"print(\"accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=predictions), \"\\n\")","1a028b75":"importances = list(rfc.feature_importances_)\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(X_train.columns, importances)]\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","377ffce0":"%matplotlib inline\nplt.figure(figsize = (20,20))   # Fig size\n# list of x locations for plotting\nx_values = list(range(len(importances))) \n# Make a bar chart\nplt.bar(x_values, importances, orientation = 'vertical')\n# Tick labels for x axis\nplt.xticks(x_values, X_train.columns, rotation='vertical')\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Columns'); plt.title('Important predictors');","94f7a458":"critical_columns = ['total_ic_mou_8', 'arpu_8', 'last_day_rch_amt_8', \\\n                    'loc_og_t2m_mou_8', 'loc_og_mou_8', 'total_og_mou_8', 'roam_ic_mou_8', \\\n                    'roam_og_mou_8', 'loc_ic_t2t_mou_8', 'loc_ic_t2m_mou_8', 'loc_ic_mou_8', 'churn']\nhigh_value_customers[critical_columns].describe()","c2be9705":"**Filter high value customers**","6d4e5804":"Sensitivity\/recall has reduced when we moved from linear to rbf model\nNote: Sensitivity is more accurate parameter than accuracy, as we don't want to miss any high value customer to be churned\nNo need to perform Grid Search for Gamma and C as the linear model performs better than the default non linear model","04212386":"**Model 2 - SVC - rbf - to check if non linear model performs better**","cb947c40":"****Scaling****","e17d10b1":"**Derived columns**","9620f06a":"**Test-Train Split**","13300a05":"A comparison between Logistic Regression and SVC - Linear shows that Logistic regression have performed better with a Recall of 82.45% and Accuracy of 89%\n\nRecall of 82.45% is very good as we are able to capture 82.45% of churned customers\n\nPCA doesnt give predictor columns. We can use Random Forest to predict the crtical columns","b6bfc8c2":"**Model 4 - Default RandomForestClassifier model with no hyper parameters**","c6967c86":"**Model 3 - Logistic Regression**","0a6fcc02":"**Correlation among attributes**","08769da1":"Top 11 important predictor columns are total_ic_mou_8, arpu_8, last_day_rch_amt_8, loc_og_t2m_mou_8, loc_og_mou_8,\ntotal_og_mou_8, roam_ic_mou_8, roam_og_mou_8, loc_ic_t2t_mou_8, loc_ic_t2m_mou_8 and loc_ic_mou_8\n\n\n\nSummary Statistics of predictor columns\u00b6","eef72f5f":"Model 1 - SVC Linear","c575e965":"**Model 6 - Final RandomForestClassifier model with optimized hyper paramaters**","bef287f5":"**Remove all the attributes corresponding to the churn phase**","10a72aad":"**Define target variable - Churn**","b04885a5":"***Model 5 - Grid search - RandomForest***","bc256a19":"**2. Data cleaning**","64eb58b3":"1. Data","75214c82":"Important features","868adf6c":"**Model building**"}}