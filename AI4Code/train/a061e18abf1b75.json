{"cell_type":{"51d8970e":"code","13b6042a":"code","f7b52e6d":"code","09ac8e57":"code","1c44a042":"code","c86ad4d9":"code","66101279":"code","99d990f2":"code","99a47531":"code","73dfec35":"code","7c7c63e4":"code","b3c1b79b":"code","56f86d89":"code","9d4a6638":"code","7e6836d4":"code","52059a99":"code","c697b75a":"code","b6a21a6a":"code","43d0ebb1":"code","08894d44":"code","bd8ec13b":"code","44ba1af9":"code","5644d1fc":"code","6f933a66":"code","6be5cb35":"code","2235bf93":"code","d594ab0b":"code","0994f7e8":"code","3ad50027":"code","664e9ea6":"code","40a3dd7d":"code","055a8601":"code","bbf5311c":"code","2e51056f":"code","6d83b1a5":"code","2158550c":"code","3aaa4022":"code","257bc71d":"code","19a17252":"code","c815427e":"code","e662fce0":"code","ee627afa":"code","a6af768b":"code","5d852c5b":"code","a2bad89f":"code","9450edda":"code","c61a8b1e":"code","c8ab2168":"code","fa638283":"code","5fea3bbf":"code","ecefd276":"code","b0a1ef7a":"code","a9371079":"code","7ee7179c":"code","291f501d":"markdown","09e8d0c4":"markdown","7e272f01":"markdown","c6e798c0":"markdown","d5162d48":"markdown","a9d4b68e":"markdown","e898619a":"markdown","2278438d":"markdown","8f8ee763":"markdown","f0838320":"markdown","6fcc1b7c":"markdown","509f0fa9":"markdown","2581ae9b":"markdown","a1415daf":"markdown","c92f0654":"markdown","2d687672":"markdown","802d6c75":"markdown","e11088bf":"markdown","def70290":"markdown","b46f8801":"markdown","e563859d":"markdown","4d33392c":"markdown","0220c03e":"markdown","45396ac4":"markdown","383db08e":"markdown","1d1aaeb7":"markdown","7c21e039":"markdown","53d08ac4":"markdown","799a56df":"markdown"},"source":{"51d8970e":"!pip install volumentations\nfrom nilearn import plotting\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML,display\n\nimport nilearn as ni\nimport nibabel as nib\n\nimport dipy\nfrom dipy.io.image import load_nifti, save_nifti\nfrom dipy.align.imaffine import (transform_centers_of_mass,\n                                 AffineMap,\n                                 MutualInformationMetric,\n                                 AffineRegistration)\nfrom dipy.align.transforms import (TranslationTransform3D,\n                                   RigidTransform3D,\n                                   AffineTransform3D)\nfrom volumentations import *\nfrom skimage.measure import block_reduce\nfrom scipy.ndimage import zoom\nfrom torch.utils.data import Dataset,random_split,DataLoader\n\nimport sys\nsys.path.insert(0, '..\/input\/brain-ptm-2021-data-and-conv4d-impl')\n\nfrom conv4d import Conv4d\nfrom convtranspose4d import ConvTranspose4d\n\nimport torch\nimport torch.nn as nn\n\n\nimport numpy as np\nimport cv2\nimport os\nimport gc\nimport pandas as pd\nimport time\nfrom typing import Union,Tuple\nfrom collections.abc import Iterable\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","13b6042a":"class config():\n    #ENV\n    KAGGLE = True\n    DS_PATH = \"..\/input\/brain-ptm-2021-data-and-conv4d-impl\/\" if KAGGLE else \".\/\"\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(\"Using device : \",DEVICE)\n    \n    #DATASET\n    TRAIN_DATA_PATH = os.path.join(DS_PATH , \"sheba75_data_train\")\n    TRAIN_STREAMLINES_PATH = os.path.join(DS_PATH, \"sheba75_streamlines_train\")\n    TRAIN_LABELS_PATH = os.path.join(DS_PATH,\"sheba75_tracts_train\")\n    TEST_DATA_PATH = os.path.join(DS_PATH, \"sheba75_data_test\")\n    SAMPLE_SUB_PATH = os.path.join(DS_PATH, \"sheba75_tracts_test_dummy\")\n    \n    TRACTS_NAMES = [\"OR_left\",\"OR_right\",\"CST_left\",\"CST_right\"]\n\n    VAL_SPLIT=0.1\n    RESIZE_SCALE=(0.5,0.5,0.5)\n    \n    #DATALOADER\n    BATCH_SIZE = 1\n    N_WORKERS = 0\n    PIN_MEMORY = False if DEVICE == \"cpu\" else True\n    \n    #MODEL\n    IN_CHANNELS = 1\n    \n    #TRAINING\n    LR=0.002\n    ALPHA = 10.0\n    GAMMA = 2.0\n    EPOCHS=5\n    \nconfig()","f7b52e6d":"sample_case_id= 7\nsample_case_path=f'case_{sample_case_id}'","09ac8e57":"sample_t1_path = os.path.join(config.TRAIN_DATA_PATH , sample_case_path, \"T1.nii\")\nsample_t1 = ni.image.load_img(sample_t1_path)\nplotting.view_img(sample_t1,dim=np.inf)","1c44a042":"!dipy_horizon {sample_t1_path}","c86ad4d9":"def plot_diff(diff,z_cut=58,interval=100):\n    if type(diff) == \"str\":\n        diff = ni.image.load_img(diff)\n        \n\n    fig, ax = plt.subplots()\n    \n    if type(diff)==np.ndarray:\n        volumes=[[ax.imshow(img[:,:,z_cut], animated=True)] for img in np.transpose(diff,(3,0,1,2))]\n    else:\n        volumes=[[ax.imshow(img.get_fdata()[:,:,z_cut], animated=True)] for img in ni.image.iter_img(diff)]\n    ani = animation.ArtistAnimation(fig, volumes, interval=interval, blit=True,\n                                    repeat_delay=1000)\n    display(HTML(ani.to_jshtml()))\n    \n    \nsample_diff_path = os.path.join(config.TRAIN_DATA_PATH, sample_case_path, \"Diffusion.nii\")\nsample_diff=ni.image.load_img(sample_diff_path)\nplot_diff(sample_diff_path,z_cut=50)","66101279":"sample_brain_mask=ni.image.load_img(os.path.join(config.TRAIN_DATA_PATH,sample_case_path,\"brain_mask.nii\")).get_fdata()\nsample_brain_mask=np.ma.masked_where(sample_brain_mask == 1,sample_brain_mask)\nsample_brain_mask=np.ma.getmask(sample_brain_mask)\nnp.unique(sample_brain_mask)","99d990f2":"sample_brain_mask.shape","99a47531":"fig,axs=plt.subplots(1,1)\naxs.imshow(sample_diff.get_fdata()[:,:,50,0])\naxs.imshow(sample_brain_mask[:,:,50],alpha=0.5)","73dfec35":"def apply_brain_mask(img: Union[nib.Nifti1Image,np.ndarray], brain_mask,plot=True,z_cut=50):\n    if len(img.shape)==4:\n        masked_diff = []\n        for v in ni.image.iter_img(img):\n            v=v.get_fdata()\n            v[brain_mask == False] = 0\n            masked_diff.append(v)\n        masked_diff=np.asarray(masked_diff).transpose(1,2,3,0)\n        if plot:\n            plot_diff(masked_diff,z_cut=z_cut)\n        return masked_diff\n    \n    elif len(img.shape)==3:\n        \n        if type(img) is nib.Nifti1Image:\n            img=img.get_fdata()\n        \n        if type(img) is np.memmap:\n            img=np.array(img)\n                \n        img[brain_mask == False] = 0\n        if plot:\n            plt.imshow(img[:,:,z_cut])\n        return img\n    else:\n        raise Error(\"idk\")\nprint(\"Masked Diffusion\")\n_ = apply_brain_mask(sample_diff,sample_brain_mask)\n","7c7c63e4":"sample_tract_path=os.path.join(config.TRAIN_LABELS_PATH,sample_case_path,'CST_left.nii')\nplotting.view_img(sample_tract_path,bg_img=sample_t1, dim=-1)","b3c1b79b":"sample_tract=ni.image.load_img(sample_tract_path)\nprint(np.unique(sample_tract.get_fdata()))\nsample_tract.shape","56f86d89":"def merge_tracts_masks(case_tracts_path,tracts_names = config.TRACTS_NAMES , target_shape = (128,144,128),clip=True):\n    sample_tracts_paths=[os.path.join(case_tracts_path, tract_name+'.nii') for tract_name in tracts_names]\n    sample_tracts_mask = np.zeros(target_shape)\n    for tract_path in sample_tracts_paths:\n        sample_tracts_mask += ni.image.load_img(tract_path).get_fdata().astype(np.uint8)\n    if clip:\n        return sample_tracts_mask.clip(0,1)\n    return sample_tracts_mask","9d4a6638":"sample_tracts_path=os.path.join(config.TRAIN_LABELS_PATH,sample_case_path)\ntracts_mask=merge_tracts_masks(sample_tracts_path,clip=False)\nni_tracts_mask=ni.image.new_img_like(sample_tract,tracts_mask)\nplotting.view_img(ni_tracts_mask,bg_img=sample_t1,dim=-1)","7e6836d4":"ni_tracts_mask.to_filename(os.path.join(sample_tracts_path,\"tracts_mask.nii\"))\nsample_tracts_mask_path=os.path.join(config.TRAIN_LABELS_PATH,sample_case_path,'tracts_mask.nii')\n!dipy_horizon {sample_tracts_mask_path}","52059a99":"sample_streamline_path=os.path.join(config.TRAIN_STREAMLINES_PATH, sample_case_path,'OR_left.trk')\n!dipy_horizon {sample_streamline_path}","c697b75a":"sample_streamline_cst_path=os.path.join(config.TRAIN_STREAMLINES_PATH , sample_case_path, \"CST_left.trk\")\n!dipy_horizon {sample_streamline_cst_path}","b6a21a6a":"for c in range(1,61):\n    diff_path=os.path.join(config.TRAIN_DATA_PATH,f\"case_{c}\",\"Diffusion.nii\")\n    tract_left_path=os.path.join(config.TRAIN_LABELS_PATH,f\"case_{c}\",\"OR_left.nii\")\n    tract_right_path=os.path.join(config.TRAIN_LABELS_PATH,f\"case_{c}\",\"OR_right.nii\")\n    \n    diff = ni.image.load_img(diff_path)\n    tract_left=ni.image.load_img(tract_left_path)\n    tract_right=ni.image.load_img(tract_right_path)\n\n    print(diff.get_fdata().shape,tract_left.get_fdata().shape,tract_right.get_fdata().shape)","43d0ebb1":"for c in range(61,76):\n    diff_path=os.path.join(config.TEST_DATA_PATH,f\"case_{c}\",\"Diffusion.nii\")\n    diff = ni.image.load_img(diff_path)\n    print(diff.get_fdata().shape)","08894d44":"diff_frames=26\ntarget_frames=65\nrepeat_idxs = [(target_frames \/\/ diff_frames) + 1 if i < (target_frames%diff_frames) else target_frames \/\/ diff_frames for i in range(diff_frames)]\nprint(\"Input Frames :\",diff_frames)\nprint(\"Output Frames :\",sum(repeat_idxs))\nrepeat_idxs","bd8ec13b":"def diff_frame_interp(diff,target_frames = 65):\n    diff_frames = diff.shape[-1]\n    \n    repeat_idxs = [(target_frames \/\/ diff_frames) + 1 if i < (target_frames%diff_frames) -1 else target_frames \/\/ diff_frames for i in range(diff_frames)]\n    result=np.zeros((diff.shape[0],diff.shape[1],diff.shape[2],65))\n    curr_frame=0\n    for idx,v in enumerate(ni.image.iter_img(diff)):\n        result[:,:,:,curr_frame:curr_frame+repeat_idxs[idx]] = np.expand_dims(v.get_fdata(),-1)\n        curr_frame+=repeat_idxs[idx]\n    result[:,:,:,-1]=v.get_fdata()\n    return ni.image.new_img_like(diff,result)\n        \n","44ba1af9":"case_59=ni.image.load_img(os.path.join(config.TRAIN_DATA_PATH, \"case_59\",\"Diffusion.nii\"))\ncase_69=ni.image.load_img(os.path.join(config.TEST_DATA_PATH,\"case_69\",\"Diffusion.nii\"))\n\n#case_69\nprint(\"case_69 frames:\",case_69.get_fdata().shape[-1])\nplot_diff(case_69,z_cut=50)\ncase_69_interp=diff_frame_interp(case_69)\nprint(\"case_69_interp frames:\",case_69_interp.get_fdata().shape[-1])\nplot_diff(case_69_interp,z_cut=50)\n\n#case_59\nprint(\"case_59 frames:\",case_59.get_fdata().shape[-1])\nplot_diff(case_59,z_cut=50)\ncase_59_interp=diff_frame_interp(case_59)\nprint(\"case_59_interp frames:\",case_59_interp.get_fdata().shape[-1])\nplot_diff(case_59_interp,z_cut=50)\n","5644d1fc":"case_69_interp.to_filename(os.path.join(config.TEST_DATA_PATH, \"case_69\",\"Diffusion.nii\"))\ncase_59_interp.to_filename(os.path.join(config.TRAIN_DATA_PATH, \"case_59\",\"Diffusion.nii\"))","6f933a66":"def nifti_apply(img: nib.Nifti1Image,fn,fn_args):\n    fn=list([fn])\n    if len(img.shape) == 4:\n        result=[]\n        for v in ni.image.iter_img(img):\n            if isinstance(fn,Iterable):\n                for func,args in zip(fn,fn_args):\n                    v=func(v,**fn_args)\n            result.append(v)\n        return np.asarray(result).transpose(1,2,3,0)\n    elif len(img.shape) == 3:\n        for func,args in zip(fn,fn_args):\n            img=func(img,**fn_args)\n        return img","6be5cb35":"class BrainPTMDataset(Dataset):\n    def __init__(self,data_dirs,labels_dirs,resize_scale=None,transforms=None,mask=True):\n        self.data_dirs=data_dirs\n        self.labels_dirs=labels_dirs\n        self.transforms = transforms\n        self.resize_scale = resize_scale\n        self.mask = mask\n            \n    def __len__(self):\n        return len(self.data_dirs)\n        #60\n    def __getitem__(self,idx):\n        \n        X=ni.image.load_img(os.path.join(self.data_dirs[idx],\"Diffusion.nii\"))\n        if self.mask:\n            brain_mask=ni.image.load_img(os.path.join(self.data_dirs[idx],\"brain_mask.nii\")).get_fdata()\n            brain_mask=np.ma.getmask(np.ma.masked_where(brain_mask == 1,brain_mask))\n        y=merge_tracts_masks(self.labels_dirs[idx])\n        \n        if self.mask or self.resize_scale or self.transforms:\n            X_preprocessed=[]\n            y_preprocessed=y\n            for v in ni.image.iter_img(X):\n                v=v.get_fdata()\n                if self.mask:\n                    v=apply_brain_mask(v,brain_mask,plot=False)\n\n                if self.resize_scale:\n                    v=zoom(v,self.resize_scale)\n\n                if self.transforms:\n                    v=self.transforms(v=v)['v']                \n\n                X_preprocessed.append(v)\n                \n            if self.mask:\n                y_preprocessed=apply_brain_mask(y_preprocessed,brain_mask,plot=False)\n\n            if self.resize_scale:\n                y_preprocessed=zoom(y_preprocessed,self.resize_scale)\n\n            if self.transforms:\n                y_preprocessed=self.transforms(y_preprocessed=y_preprocessed)['y_preprocessed']                \n            X_preprocessed = torch.tensor(np.asarray(X_preprocessed))\n            y_preprocessed = torch.tensor(y_preprocessed)\n            return X_preprocessed.unsqueeze(0),y_preprocessed.unsqueeze(0)\n        \n        return torch.tensor(X.get_fdata()).unsqueeze(0),torch.tensor(y).unsqueeze(0)","2235bf93":"data_dirs=[os.path.join(config.TRAIN_DATA_PATH, case_dir) for case_dir in os.listdir(config.TRAIN_DATA_PATH)]\nlabels_dirs=[os.path.join(config.TRAIN_LABELS_PATH,case_dir) for case_dir in os.listdir(config.TRAIN_LABELS_PATH)]\n             \n#transforms = Compose([\n#   Resize(config.RESIZE_SHAPE, interpolation=1, always_apply=True, p=1.0)\n#],p=1.0)\n             \nds = BrainPTMDataset(data_dirs,labels_dirs,resize_scale=config.RESIZE_SCALE,mask=True)\n\nval_len = (len(ds) * config.VAL_SPLIT) \/\/ 1\ntrain_ds, val_ds = random_split(ds,[int((len(ds) - val_len)), int(val_len)])\nprint(\"Train_cases : \",len(train_ds))\nprint(\"Val_cases : \",len(val_ds))","d594ab0b":"train_loader = DataLoader(train_ds,\n                          batch_size=config.BATCH_SIZE,shuffle=False,\n                          num_workers=config.N_WORKERS,pin_memory=config.PIN_MEMORY)\nval_loader = DataLoader(val_ds,\n                          batch_size=config.BATCH_SIZE,shuffle=False,\n                          num_workers=config.N_WORKERS,pin_memory=config.PIN_MEMORY)","0994f7e8":"sample_batch=next(iter(train_loader))\nsample_batch[0].shape","3ad50027":"plot_diff(sample_batch[0][0][0].numpy().transpose(1,2,3,0),z_cut=25)","664e9ea6":"bn=nn.BatchNorm1d(1)\na=torch.randn((1,1,64,72,64,65))\nprint(a[:,:,2,50,20:30,4])\na=a.view(1,1,-1)\na=bn(a)\na=a.reshape((1,1,64,72,64,65))\nprint(a[:,:,2,50,20:30,4])","40a3dd7d":"def view_as_blocks(tensor_in, block_shape):\n    \n    if not isinstance(block_shape, tuple):\n        raise TypeError('block needs to be a tuple')\n\n    block_shape = torch.tensor(block_shape)\n    if (block_shape <= 0).any():\n        raise ValueError(\"'block_shape' elements must be strictly positive\")\n\n    if len(block_shape) != tensor_in.ndim:\n        raise ValueError(\"'block_shape' must have the same length \"\n                         \"as 'arr_in.shape'\")\n\n    tensor_shape = torch.tensor(tensor_in.shape)\n    if (tensor_shape % block_shape).sum() != 0:\n        raise ValueError(\"'block_shape' is not compatible with 'arr_in'\")\n\n    # -- restride the array to build the block view\n    new_size = tuple(tensor_shape \/\/ block_shape) + tuple(block_shape)\n    print(new_size)\n    new_stride = tuple(tensor_in.stride() * block_shape) + tensor_in.stride()##Problem woth this line,else it would work find i guess :\/\n\n    tensor_out = torch.as_strided(tensor_in, size=new_size, stride=new_stride)\n\n    return tensor_out\n\ndef block_reduce_pytorch(image:torch.Tensor, block_size=2, func=torch.amax, cval=0, func_kwargs=None):\n    if np.isscalar(block_size):\n        block_size = (block_size,) * image.ndim\n    elif len(block_size) != image.ndim:\n        raise ValueError(\"`block_size` must be a scalar or have \"\n                         \"the same length as `image.shape`\")\n\n    if func_kwargs is None:\n        func_kwargs = {}\n\n    pad = []\n    for i in range(len(block_size)):\n        if block_size[i] < 1:\n            raise ValueError(\"Down-sampling factors must be >= 1. Use \"\n                             \"`skimage.transform.resize` to up-sample an \"\n                             \"image.\")\n        if image.shape[i] % block_size[i] != 0:\n            dim_pad = block_size[i] - (image.shape[i] % block_size[i])\n        else:\n            dim_pad = 0\n        pad.append(dim_pad)\n\n    image = torch.nn.functional.pad(image, pad=pad, mode='constant',\n                   value=cval)\n\n    blocked = view_as_blocks(image, block_size)\n\n    return func(blocked, dims=tuple(range(image.ndim, blocked.ndim)),\n                **func_kwargs) ","055a8601":"a=torch.randn(3,2,2,2)\nprint(a.shape)\na_flat=a.flatten(1,-1)\nprint(a_flat)\na_flat.transpose(1,0)[0]","bbf5311c":"class MaxPooling4d(nn.Module):\n    def __init__(self,\n                pool_size: Union[Tuple[int, int, int, int],int],\n                ##stride : Union[Tuple[int, int, int, int],int],\n                ##padding: Union[Tuple[int, int, int, int],int],\n                ):\n        super(MaxPooling4d, self).__init__()\n        \n        if np.isscalar(pool_size):\n            pool_size = (block_size,) * 4\n        else:\n            self.pool_size = pool_size\n        self.pool_3d = nn.MaxPool3d(pool_size[:-1])\n        self.pool_1d = nn.MaxPool1d(pool_size[-1])\n        \n    def forward(self, x):\n            \n        pads=[]\n        for i in range(len(self.pool_size)):\n            \n            if x.shape[i+2] % self.pool_size[i] != 0:\n                pad = (self.pool_size[i] - (x.shape[i+2] % self.pool_size[i]))\n            else:\n                pad = 0\n            pads.append(pad)\n            \n        padded_shape=torch.add(torch.tensor(x.shape[2:]),torch.tensor(pads))\n        print(padded_shape)\n        pooling_shape=tuple(torch.div(padded_shape,torch.tensor(self.pool_size),rounding_mode=\"floor\"))\n        print(pooling_shape)\n        \n        res = torch.tensor([])\n        for batch_n,batch in enumerate(x):\n            print(batch.shape)\n            channel = torch.tensor([])\n            for c_idx, c in enumerate(batch):\n                print(c.shape)\n                \n                volumes = torch.tensor([])\n                for v_idx,v in enumerate(c):\n                    \n                    v_pool = self.pool_3d(v.unsqueeze(0).unsqueeze(0))\n                    \n                    volumes=torch.stack([*volumes,v_pool.squeeze(0).squeeze(0)])\n                print(\"v_pool shape:\\t \",tuple(v_pool.shape))\n                print(\"volumes shape:\\t\",tuple(volumes.shape))\n                windows=torch.tensor([])\n                t_pixels=volumes.flatten(start_dim=1,end_dim=-1).transpose(1,0)\n                print(\"t_pixel shape:\\t\",t_pixels.shape)\n                for window in t_pixels:\n                    t_pool=self.pool_1d(window.unsqueeze(0).unsqueeze(0))\n                    windows=torch.stack([*windows,t_pool.squeeze(0).squeeze(0)])\n                print(\"window shape:\\t \",window.shape)\n                    \n                pooled_volumes=torch.tensor([])\n                channel=torch.stack([*channel,windows.reshape(pooling_shape)])\n                print(\"channel shape:\\t \",channel.shape)\n                channel=torch.stack([*channel,volumes])\n                \n                \n            res=torch.stack([*res,channel])\n        return res","2e51056f":"def my_pool_nd(inp: torch.Tensor,\n            block_size: Union[tuple,int],\n           cval=0,\n           verbose=False):\n    if verbose:\n        print(\"\\n\")\n        print(\"=\"*20)\n        print(\"Input shape:\\t \",tuple(inp.shape))\n        print(\"block_size:\\t \",block_size)\n        \n    if np.isscalar(block_size):\n        block_size = (block_size,) * inp.ndim\n    elif len(block_size) != inp.ndim:\n        raise ValueError(\"`block_size` must be a scalar or have \"\n                         \"the same length as `image.shape`\")\n    #PADDING\n    #==============================\n    pads=[]\n    for i in range(len(block_size)):\n        \n        if block_size[i] < 1:\n            raise ValueError(\"Down-sampling factors must be >= 1. Use \"\n                             \"`skimage.transform.resize` to up-sample an \"\n                             \"image.\")\n            \n        if inp.shape[i] % block_size[i] != 0:\n            pad = (block_size[i] - (inp.shape[i] % block_size[i]))\n        else:\n            pad = 0\n        pads=(pad,0,*pads)\n        #==============================\n\n\n    if verbose:\n        print(\"pads(inverted):\\t \",tuple(pads)[::-1])\n\n    out_padded = torch.nn.functional.pad(inp, pad=pads, mode=\"constant\",\n                   value=cval)\n    if verbose:\n        print(\"padded_shape:\\t \",tuple(out_padded.shape))\n        \n    out_shape=list(np.asarray(out_padded.shape)\/\/np.asarray(block_size))\n    \n    n_blocks=np.prod(out_shape)\n    if verbose:\n        print(\"n_blocks:\\t \",n_blocks)\n        \n    #out_blocked=out_padded.view(inp.shape[0],inp.shape[1],n_blocks,*block_size[2:])\n    blocked_shape=[None]*(len(block_size)*2)\n    blocked_shape[::2]=out_shape\n    blocked_shape[1::2]=block_size\n    batch_output=[]\n    out_blocked=out_padded.view(blocked_shape)\n    if verbose:\n        print(\"blocked shape:\\t \",tuple(out_blocked.shape))\n        \n    \n    dims_to_reduce=tuple(range(1,len(blocked_shape),2))\n    if verbose:\n        print(\"dims_to_reduce:\\t \",dims_to_reduce)\n        print(\"=\"*20,\"\\n\")\n    out_reduced=torch.amax(out_blocked,dim=dims_to_reduce,keepdim=True)\n    return out_reduced.reshape(out_shape)","6d83b1a5":"b=torch.randint(low=10,high=592,size=(43,41,2,5))\nr=my_pool_nd(b,block_size=(5,5,5,5),verbose=True)\nprint(r.shape)","2158550c":"a=torch.randint(low=10,high=592,size=(13,13))\nprint(a)\nr=my_pool_nd(a,block_size=(4,4),verbose=True)\nprint(\"Output:\\t \",r)","3aaa4022":"baby_yoda_path=os.path.join(config.DS_PATH, \"baby_yoda.jpeg\")\nbaby_yoda_img=cv2.imread(baby_yoda_path, cv2.IMREAD_GRAYSCALE)\nprint(baby_yoda_img.shape)\nplt.imshow(baby_yoda_img,cmap=\"gray\")","257bc71d":"pool=nn.MaxPool2d(2)\nstart=time.time()\n\npytorch_pool_out=pool.forward(torch.tensor(baby_yoda_img,dtype=torch.float32).unsqueeze(0).unsqueeze(0)).squeeze(0).squeeze(0)\nprint(f\"Runtime : {time.time() - start:.2f}\")\nprint(\"Pytorch's MaxPool2d output:\")\nprint(tuple(pytorch_pool_out.shape))\nplt.imshow(pytorch_pool_out,cmap=\"gray\")","19a17252":"start=time.time()\npooled_baby_yoda_img=my_pool_nd(torch.tensor(baby_yoda_img),block_size=(2,2),verbose=True)\nprint(f\"Runtime : {time.time() - start:.2f}\")\nprint(\"My PoolNd output:\")\nprint(tuple(pooled_baby_yoda_img.squeeze(0).squeeze(0).shape))\nplt.imshow(pooled_baby_yoda_img.squeeze(0).squeeze(0),cmap=\"gray\")","c815427e":"#Same output\nnp.unique((pytorch_pool_out==pooled_baby_yoda_img).numpy())","e662fce0":"class MaxPoolNd(nn.Module):\n    def __init__(self,\n                 dim,\n                pool_size: Union[tuple,int],\n                 verbose=False,\n                 cval=0\n                ##stride : Union[Tuple[int, int, int, int],int],\n                ##padding: Union[Tuple[int, int, int, int],int],\n                ):\n        super(MaxPoolNd, self).__init__()\n        \n        if np.isscalar(pool_size):\n            self.pool_size = [pool_size for i in range(dim)]\n        else:\n            self.pool_size = pool_size\n        self.cval = cval\n        self.verbose = verbose\n    def forward(self, x):\n        block_size=self.pool_size\n        batch_output = []\n        for b in x:\n            channels_output = []\n            for c in b:\n                channels_output.append(my_pool_nd(c, block_size, cval = self.cval, verbose=self.verbose))\n            batch_output.append(torch.stack(channels_output))\n        return torch.stack(batch_output)","ee627afa":"start=time.time()\nmax_pool_4d=MaxPoolNd(4,2)\nsample_X=sample_batch[0]\nprint(\"Input shape:\",sample_X.numpy().shape)\nsample_X_pooled=max_pool_4d(sample_X )\nprint(\"Output shape: \",tuple(sample_X_pooled.shape))\nprint(f\"Time of execution : {time.time() - start :.2f}s\")","a6af768b":"plot_diff(sample_X_pooled.squeeze(0).squeeze(0).numpy().transpose(1,2,3,0),z_cut=20)","5d852c5b":"class DoubleConv4d(nn.Module):\n    def __init__(self,\n                 in_channels: int,\n                 out_channels: int,\n                 mid_channels: int = None,\n                 kernel_size: Union[Tuple[int,int,int,int],int] = 3,\n                 padding: int = 1):\n        super(DoubleConv4d, self).__init__()\n        \n        if np.isscalar(kernel_size):\n            self.kernel_size = [kernel_size for i in range(4)]\n        else:\n            self.kernel_size = kernel_size\n            \n        if not mid_channels:\n            mid_channels = out_channels\n        \n        self.conv4d_1 = Conv4d(in_channels, mid_channels, kernel_size=self.kernel_size, stride=1, padding = padding, bias = False)\n        self.bn_1 = nn.BatchNorm1d(mid_channels)\n        self.relu_1 = nn.ReLU(inplace = False)\n        \n        self.conv4d_2 = Conv4d(mid_channels, out_channels,kernel_size=self.kernel_size, stride=1, padding = padding, bias = False)\n        self.bn_2 = nn.BatchNorm1d(out_channels)\n        self.relu_2 = nn.ReLU(inplace = False)\n        \n    def forward(self, x):\n        x = self.conv4d_1(x)\n        x = self.bn_1(x.view(x.shape[0],x.shape[1],-1)).reshape(x.shape)\n        x = self.relu_1(x)\n        \n        x = self.conv4d_2(x)\n        x = self.bn_2(x.view(x.shape[0],x.shape[1],-1)).reshape(x.shape)\n        x = self.relu_2(x)\n        return x","a2bad89f":"class DownBlock(nn.Module):\n    def __init__(self,\n                 in_channels: int,\n                 out_channels: int,\n                 mid_channels: int = None,\n                 pool_size : Union[Tuple[int,int,int,int],int] = 2,\n                 kernel_size: Union[Tuple[int,int,int,int],int] = 3,\n                 padding: int = 1):\n        super(DownBlock, self).__init__()\n        \n        self.maxpool4d = MaxPoolNd(4,pool_size)\n        self.doubleconv4d = DoubleConv4d(in_channels,out_channels, mid_channels, kernel_size, padding)\n        \n    def forward(self, x):\n        \n        x = self.maxpool4d(x)\n        x = self.doubleconv4d(x)\n        return x","9450edda":"class Upsample4d(nn.Module):\n    def __init__(self,\n                size=None, scale_factor=None,\n                mode='nearest', align_corners=None):\n        \n        super(Upsample4d, self).__init__()\n        \n        if np.isscalar(size):\n            self.size = [size for i in range(4)]\n        else:\n            self.size = size\n            \n        if np.isscalar(scale_factor):\n            self.scale_factor = [scale_factor for i in range(4)]\n        else:\n            self.scale_factor = scale_factor\n        self.upsample3d = nn.Upsample(size=size, scale_factor=scale_factor, mode=mode, align_corners=align_corners)\n        \n    def forward(self, x):\n        \n        batch_output=[]\n        for b in x:\n            \n            channels_output=[]\n            for c in b:\n                channels_output.append(self.upsample3d(c))\n                 \n            batch_output.append(torch.stack(channels_output))\n        \n        return torch.stack(batch_output)","c61a8b1e":"class UpBlock(nn.Module):\n    def __init__(self,\n                 in_channels: int,\n                 out_channels: int,\n                 mid_channnels: int = None,\n                 bilinear: bool = True,\n                 kernel_size: Union[Tuple[int,int,int,int],int] = 3,\n                 \n                size=None, scale_factor=(2,2,2),\n                 align_corners=True):\n        super(UpBlock, self).__init__()\n        \n        assert bilinear == True, \\\n            \"ConvTranspose4d stride not yet implemented\"\n            \n        if bilinear:\n            self.up = Upsample4d(size=size, scale_factor=scale_factor, mode=\"bilinear\", align_corners=align_corners)\n            self.conv = DoubleConv4d(in_channels, out_channels, mid_channels = in_channels \/\/ 2, kernel_size = kernel_size,)\n        else:\n            self.up = ConvTranspose4d(in_channels, in_channels \/\/ 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv4d(in_channels, out_channels, mid_channels)\n\n    def forward(self, x1, x2):\n        print(\"x1 shape before up:\",x1.shape)\n        x1 = self.up(x1)\n        print(\"x1 shape\",x1.shape)\n        print(\"x2 shape\",x2.shape)\n        # SKIP CONNECTIONS\n        diffT = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        diffY = x2.size()[4] - x1.size()[4]\n        diffZ = x2.size()[5] - x1.size()[5]\n        print(dict(zip([\"diff\"+i for i in [\"T\",\"X\",\"Y\",\"Z\"]],[diffT,diffX,diffY,diffZ])))\n        x1 = nn.functional.pad(x1, [diffT \/\/ 2, diffT - diffT \/\/ 2,\n                        diffX \/\/ 2, diffX - diffX \/\/ 2,\n                        diffY \/\/ 2, diffY - diffY \/\/ 2,\n                        diffZ \/\/ 2, diffZ - diffZ \/\/ 2])\n        \n        x = torch.cat([x2, x1], dim=1)\n        x = self.conv(x)\n        return x","c8ab2168":"class UNet4D(nn.Module):\n    def __init__(self, bilinear:bool = True):\n        super(UNet4D, self).__init__()\n        \n        self.conv = DoubleConv4d(config.IN_CHANNELS,32)\n        \n        self.down1 = DownBlock(32,64)\n        self.down2 = DownBlock(64,128)\n        self.down3 = DownBlock(128,256)\n        \n        self.up1 = UpBlock(256,128)\n        self.up2 = UpBlock(128,64)\n        self.up3 = UpBlock(64,32)\n        \n        self.out = Conv4d(32,1,(1,1,1,1))\n    def forward(self,x):\n        x1 = self.conv(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        \n        x = self.up1(x4,x3)\n        x = self.up2(x,x2)\n        x = self.up2(x,x1)\n        x = self.out(x)\n        return x\n        ","fa638283":"model = UNet4D().float().to(config.DEVICE)\nmodel","5fea3bbf":"#Registering hook for debug\n\ndef forward_hook(m,inp,out):\n    print(m)\n    \n    print(\"=\"*30)\n    \n    for i in inp:\n        try:\n            print(\"Input shape : \", i.shape)\n        except:\n            continue\n    for o in out:\n        try:\n            print(\"Output shape : \",o.shape)\n        except:\n            continue\n        \n    print(\"=\"*30)\n    \nfor m in vars(model)['_modules']:\n    vars(model)['_modules'][m].register_forward_hook(forward_hook)","ecefd276":"start=time.time()\nsample_X=sample_X.float().to(config.DEVICE)\nmodel(sample_X)\nprint(f\"iteration time : {time.time() - start :.2f}s\")","b0a1ef7a":"class IoULoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(IoULoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #intersection is equivalent to True Positive count\n        #union is the mutually inclusive area of all labels & predictions \n        intersection = (inputs * targets).sum()\n        total = (inputs + targets).sum()\n        union = total - intersection \n        \n        IoU = (intersection + smooth)\/(union + smooth)\n                \n        return 1 - IoU","a9371079":"class DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, pred, target, smooth=1):\n        \n        #pred = F.sigmoid(pred)       \n        \n        #flatten label and prediction tensors\n        pred = pred.view(-1)\n        target = target.view(-1)\n        \n        intersection = (pred * target).sum()                            \n        dice = (2.*intersection + smooth)\/(pred.sum() + target.sum() + smooth)  \n        \n        return 1 - dice\n    \nclass FocalLoss(nn.Module):\n    \n    def __init__(self, gamma):\n        super().__init__()\n        self.gamma = gamma\n\n    def forward(self, pred, target):\n        if not (target.size() == pred.size()):\n            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n                             .format(target.size(), input.size()))\n        max_val = (-pred).clamp(min=0)\n        loss = pred - pred * target + max_val + \\\n            ((-max_val).exp() + (-pred - max_val).exp()).log()\n        invprobs = F.logsigmoid(-pred * (target * 2.0 - 1.0))\n        loss = (invprobs * self.gamma).exp() * loss\n        return loss.mean()\n\n\nclass DiceFocalLoss(nn.Module):\n    def __init__(self, alpha, gamma):\n        super().__init__()\n        self.alpha = alpha\n        self.focal = FocalLoss(gamma)\n\n    def forward(self, pred, target):\n        loss = self.alpha*self.focal(pred, target) - torch.log(dice_loss(pred, target))\n        return loss.mean()\n    \noptimizer = torch.optim.Adam(model.parameters(),lr=config.LR)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3,verbose=True)\nloss_fn = DiceFocalLoss(config.ALPHA,config.GAMMA)","7ee7179c":"best_val_loss=float(1e6)\ntrain_losses=[]\nval_losses=[]\nstart = time.time()\nfor n_epoch in range(config.EPOCHS):\n    print(\"EPOCH : \"+str(n_epoch)+\"\/\"+str(config.EPOCHS))\n    \n    running_train_loss=0.0\n    running_val_loss=0.0\n    \n    ## TRAINING\n    model.train()\n    for train_batch_idx,train_batch in enumerate(train_loader):\n        optimizer.zero_grad()\n\n        inputs,targets = train_batch\n        inputs,targets = inputs.float().to(config.DEVICE),targets.float().to(config.DEVICE)\n\n        preds=model(inputs)\n        loss = loss_fn(preds,targets)\n        loss.backward()\n\n        optimizer.step()\n        \n\n        gc.collect()\n        running_train_loss+=loss.item()\n        train_losses.append(loss.item())\n        del train_batch,inputs,targets\n        \n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        \n    ###VALIDATION\n    model.eval()\n    with torch.no_grad():\n        for val_batch_idx,val_batch in enumerate (val_loader):\n            inputs,targets = val_batch\n            inputs,targets=inputs.float().to(DEVICE),targets.float().to(DEVICE)\n            preds=model(inputs)\n            loss=loss_fn(val_preds,targets)\n\n            gc.collect()\n            del val_batch,inputs,targets\n            \n            running_val_loss+=loss.item()\n            val_losses.append(loss.item())\n    running_train_loss \/= train_batch_idx+1\n    running_val_loss \/= val_batch_idx+1\n    \n    scheduler.step(running_val_loss)\n    print(f\"EPOCH : {n_epoch} Train Loss : {running_train_loss:.5f}, Val Loss : {running_val_loss:.5f}\")\n    if(running_val_loss < best_val_loss):\n        torch.save(model.state_dict(), \".\/best_model.pth\")\n        print(\"Model Saved\")\n        best_val_loss=running_val_loss\ntime_elapsed = time.time() - start\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\nprint('Best Val Loss: {:4f}'.format(best_val_loss))","291f501d":"#### Second try\nyeah....uh...i guess i won't even bother explaining this failed abortion xD\n\nbasically at one point i started to doubt my understanding on the \"how\" to compute a 4d pooling so i **kinda** unpythonized everything and started coding more in a c++ fashion (with indexed for loops etc).\n\nSo what i did was first using the MaxPool3d layer from pytorch to iteratively compute a pooling on each volume of the time series,then flattened all volumes along the time axis and transposed it so that the output tensor would be `[volume_pixels,time]` allowing me to compute a MaxPool1d for each pixel along the remaining time axis.\nThe padding is calculated automatically by these layers so i just reshaped the pooled tensor as `padded_shape \/\/ block_size` and proceeded to feeling ashamed for this code \u00af\\\\\\_(\u30c4)_\/\u00af\n\n\n![](https:\/\/i.pinimg.com\/564x\/c7\/b9\/d1\/c7b9d1ceae447c7a05697552cbae79b2.jpg)","09e8d0c4":"#### White Matter masks","7e272f01":"Apparently, as you can see from the color bar, no there seems to be little to no overlapping between masks, so we're going to just merge them in the preprocessing phase and clip them to the range of (0,1) to fix those few overlaps :D","c6e798c0":"#### Finally","d5162d48":"# W.I.P","a9d4b68e":"# Data preprocessing","e898619a":"now....to implement this thingy i went trough 3 tries,one worst than the other :D, but eventually ended up implementing a N-Dimensional Pooling layer with actually the same runtimes of the official torch ones (at least on small datasets)","2278438d":"# INFERENCE","8f8ee763":"There's also no MaxPooling4D of course \u253b\u2501\u253b \ufe35\u30fd(\\`\u0414\u00b4)\uff89\ufe35 \u253b\u2501\u253b\n\nso i introduced it as a pytorch implementation of the block_reduce function, because for the autograd to work, the autograd engine needs to be able to know how to compute the gradient for each operation that is done.\nUnfortunately, if you don\u2019t use pytorch ops (and instead call `detach()`,disloging the tensor from the computational graph), it cannot know how to compute the gradients and so you won\u2019t be able to get gradients.\n\nYou will either have to re-implement it using pytorch operations or to write a custom function where you tell the autograd how to compute the backward pass.\n\nSo we're going for the first one :)","f0838320":"\n## Background\n\nThe accurate mapping of white matter (WM) tracts is critical to the success of neuro-surgical planning\nand navigation. For the last 15 years, this task was exclusively based on white matter tractography\nalgorithms. Deterministic algorithms, that tracks the principal direction of diffusion (PDD), as well as\nprobabilistic approaches, that generate tracts in a random walk process, have both been applied for this\npurpose . In both cases, the manual delineation of accurate seeding regions is performed, requiring specific\nneuroanatomical knowledge and significant amount of time. Moreover, the tract generation itself may\nrequire significant computing time or power, as probabilistic algorithms are generally needed to obtain a\nsufficiently complete reconstruction of the motor and optic radiation tracts using clinical diffusion weighted\nimaging scans.\n\nWhile the fiber tracts representation is useful for brain research, as it enables quantitative measurements\nsampled along the tracts, neuro-surgeons rather need a volumetric segmentation of the tracts for mapping\npurpose. Automating and accelerating this process would significantly reduce the amount of time spent on\nneuro-surgical planning phase and it might improve the accuracy of tracts mapping which is crucial for \nbrain surgery.  Recent progresses in multi-modal deep neural networks, suggest these may benefit to white\nmatter tracts mapping, either by automating seeding regions generation, or by  direct segmentation of tract\nvoxels . \n\nIn this context, the CILAB organizes this challenge to encourage the development of  machine learning approaches  \nto white matter tracts mapping in clinical  brain MRI scans. Significant progress in this area\nwill improve the  neuro-surgical planning procedure in terms of time, accuracy, and robustness.\n\n## Challenge\n\nIn this challenge we ask the participants to perform direct white matter tracts mapping in clinical brain\nMRI scans we provide. The data that is provided consists of 75 cases (patients referred for brain tumor\nremoval) that were acquired at Sheba Medical Center at Tel HaShomer, Israel. Patient pathologies\ninclude oligodendrogliomas , astrocytomas, glioblastomas and cavernomas, on first occurrence or in a\npost-surgical recurrence. According to the neuro-radiologist's estimation, the tumor volumes ranged\nfrom 4 (cavernoma) to 60 \\[cm^3\\] (glioblastoma multiforme). Also, different levels of edema are present\naround the dataset tumors, from inexistent to very significant.\n\nAlong with each case both T1 Structural and Diffusion Weighted modalities are provided. \n* For 60 cases (training) semi-manual white matters tracts mapping is provided in the form of binary segmentation maps.\n* For the rest 15 cases (test) no tracts annotations are provided as these will be used for participants algorithms evaluation.","6fcc1b7c":"#### Anatomical Image","509f0fa9":"# Output :\n![](https:\/\/s10.gifyu.com\/images\/ezgif-6-c3dfb14fc1.gif)","2581ae9b":"### Output :\n![](https:\/\/s10.gifyu.com\/images\/ezgif-5-c71b57acfc.gif)","a1415daf":"# EDA","c92f0654":"#### Firrst try\nwas to convert the source code of the `skimage.ndimage.block_reduce` function (from numpy to torch), but eventually found out deep differences on how torch and numpy handle strides (the necessary jump in bytes to go from one dimension of the array to another) so i kinda went for another approach :3","2d687672":"# Model","802d6c75":"# Training","e11088bf":"this model will consists of blocks of Conv4d(implemented separately cause pytorch currently does not support it :\/) ,BatchNorm and ReLU; The batchnorm4d of course also is not implemented in pytorch \n\n\u0295\u30ce\u2022\u1d25\u2022\u0294\u30ce \ufe35 \u253b\u2501\u253b\n\nso we're going to normalize the data as follows:","def70290":"### Output :\n\n![](https:\/\/s10.gifyu.com\/images\/ezgif-7-d4f244f868.gif)","b46f8801":"# Brain Pre-surgical white matter Tractography Mapping challenge (BrainPTM) 2021","e563859d":"# Data Cleaning\nAll images already have the same shape,so there's no need to resize.Though, literally two of them have different numbers of frames (last dimension)... for some reason...\n\n\u0295\u30ce\u2022\u1d25\u2022\u0294\u30ce \ufe35 \u253b\u2501\u253b .\n\nspecifically 26 and 64 , instead of 65.so we're going to interpolate them C:","4d33392c":"#### Diffusion Image (4D data)","0220c03e":"## Creating an interpolation function\nof course i couldn't find a 4d interpolation library so i had to come up with this kind of distributed duplication of frames method","45396ac4":"#### Streamlines","383db08e":"### _Already ran Frame Interpolation locally and included the original_samples in a separated folder in the dataset_","1d1aaeb7":"#### Brain mask","7c21e039":"# Loss and Optimization","53d08ac4":"### Output :\n![](https:\/\/s10.gifyu.com\/images\/ezgif-6-5dafc70da8.gif)","799a56df":"## Data visualization"}}