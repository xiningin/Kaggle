{"cell_type":{"dff8657d":"code","f6873f6f":"code","9672f099":"code","c6aaefa1":"code","bae1cfb8":"code","fdb9419a":"code","630d85ee":"code","6d7704bb":"code","b71afd21":"code","ead1c297":"code","8c47d65e":"code","8d0ba236":"code","00c22a73":"code","b2e1cf90":"code","e1ec403b":"code","33dd1445":"code","679a1bad":"code","c0ec7f19":"code","efc7d308":"code","9412647e":"code","e7f87356":"code","759a52e8":"code","248df3c5":"code","a96113b8":"code","ac6a739b":"code","03e79ca0":"code","a6934e19":"code","f5fe9768":"code","1925b39e":"code","516f3053":"code","6a23bb38":"code","90e1e54a":"code","539e5b9f":"code","e1b1d562":"code","47777fd0":"markdown","b91d665b":"markdown","b4531543":"markdown","579d2c8a":"markdown","4e4d5e11":"markdown","bcc2d6df":"markdown"},"source":{"dff8657d":"## Data Analysis Phase\n## Main aim is to understand more about the data\n\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n## Display all columns of the dataframe\n\npd.pandas.set_option('display.max_columns', None)","f6873f6f":"dataset = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\n\n## print shape of the dataset\nprint(dataset.shape)","9672f099":"dataset.head()","c6aaefa1":"dataset.drop(['id', 'Unnamed: 32'], axis=1, inplace = True)","bae1cfb8":"## Here we will check the percentage of missing values in each feature\n## Step - 1: make the list of features which have missing values\n\nfeatures_with_na = [features for features in dataset.columns if dataset[features].isnull().sum()>1]\n\n## Step - 2: Print the feature name and the percentage of missing values\nfor feature in features_with_na:\n    print(feature, np.round(dataset[feature].isnull().mean(), 4), '%missing values')","fdb9419a":"## NO missing values available in the dataset","630d85ee":"## LabelEncoding (Convert the value of M and N into 1 and 0)\nfrom sklearn.preprocessing import LabelEncoder\nlabelEncoder_y = LabelEncoder()\ndataset.iloc[:, 0] = labelEncoder_y.fit_transform(dataset.iloc[:, 0].values)","6d7704bb":"dataset.head()","b71afd21":"continous_features = dataset.drop(['diagnosis'], axis=1)\ncontinous_features.head()","ead1c297":"## lets analyze the continous values by creating histogram to understand the distribution\nfor feature in continous_features:\n    data=dataset.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"count\")\n    plt.title(feature)\n    plt.show()","8c47d65e":" ## The data is not distributed Normally","8d0ba236":"## Check and removing Outliers\nfor feature in continous_features:\n    data=dataset.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","00c22a73":"### There is so many outliers","b2e1cf90":"corr = dataset.corr()\nplt.figure(figsize=(20, 20))\nsns.heatmap(corr, cbar=True, square=True, fmt='.1f', annot= True, annot_kws={'size':15}, cmap='GnBu')\nplt.show()","e1ec403b":"dataprocessed = dataset.drop(['diagnosis'], axis=1)","33dd1445":"dataprocessed.head()","679a1bad":"corr = dataprocessed.corr()\nplt.figure(figsize=(15, 15))\nsns.heatmap(corr, cbar=True, square=True, fmt='.1f', annot= True, annot_kws={'size':10}, cmap='GnBu')\nplt.show()","c0ec7f19":"droplist = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'compactness_mean', 'concavity_mean', \n           'concave points_mean', 'radius_se', 'perimeter_se', 'area_se', 'radius_worst', 'texture_worst', 'perimeter_worst', \n           'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst']\ndataprocessed = dataprocessed.drop(droplist, axis=1)","efc7d308":"dataprocessed.head()","9412647e":"for feature in dataprocessed.columns:\n    sns.displot(dataprocessed[feature])","e7f87356":"def outlierLimit(column):\n    q1, q3 = np.nanpercentile(column, [25, 75])\n    iqr = q3 - q1\n    \n    uplimit = q3 + 1.5*iqr\n    lowlimit = q1 - 1.5*iqr\n    return uplimit, lowlimit","759a52e8":"for column in dataprocessed.columns:\n    if dataprocessed[column].dtype != 'object':\n        uplimit, lowlimit = outlierLimit(dataprocessed[column])\n        dataprocessed[column] = np.where((dataprocessed[column]>uplimit) | (dataprocessed[column]<lowlimit), np.nan, dataprocessed[column])","248df3c5":"dataprocessed.isnull().sum()","a96113b8":"## Now you can see we change outliers into Nan values","ac6a739b":"from sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=4)\ndataprocessed.iloc[:, :] = imputer.fit_transform(dataprocessed)\n","03e79ca0":"dataprocessed.isnull().sum()","a6934e19":"dataprocessed.head()","f5fe9768":"y = dataset['diagnosis']\nX = dataprocessed","1925b39e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","516f3053":"from sklearn.linear_model import LogisticRegression","6a23bb38":"def models(X_train, y_train):\n    ## LogisticRegression \n    lr = LogisticRegression(random_state=42)\n    lr.fit(X_train, y_train)\n    \n    ## DecisionTreeClassifier\n    from sklearn.tree import DecisionTreeClassifier\n    tree = DecisionTreeClassifier(random_state=42, criterion='entropy')\n    tree.fit(X_train, y_train)\n    \n    ##  Random Forest\n    from sklearn.ensemble import RandomForestClassifier\n    forest = RandomForestClassifier(random_state=42, criterion='entropy', n_estimators = 10)\n    forest.fit(X_train, y_train)\n    \n    print('[0]LogisticRegression Accuracy: ', lr.score(X_train, y_train))\n    print('[0]DecisionTreeClassifier Accuracy: ', tree.score(X_train, y_train))\n    print('[0]Random Forest Accuracy: ', forest.score(X_train, y_train))\n    return lr, tree, forest\n","90e1e54a":"model = models(X_train, y_train)","539e5b9f":"from sklearn.metrics import classification_report, accuracy_score, recall_score\n\nfor i in range(len(model)):\n    print(\"Model\",i)\n    print(classification_report(y_test, model[i].predict(X_test)))\n    print(accuracy_score(y_test, model[i].predict(X_test)))\n    print(recall_score(y_test, model[i].predict(X_test)))","e1b1d562":"# print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n# print('Recall: {}'.format(recall_score(y_test, y_pred)))","47777fd0":"### Data Preprocessing","b91d665b":"## Exploratory Data Analysis","b4531543":"# Feature Selection","579d2c8a":"# Model Training and Testing","4e4d5e11":"## Outliers","bcc2d6df":"## Correlation"}}