{"cell_type":{"8ecfa798":"code","ceb15307":"code","ca347cce":"code","36165b3f":"code","bd4054db":"code","51e415a9":"code","eb277f3c":"code","0ba1ad5a":"code","95574786":"code","373dce91":"code","4c9b5a2d":"code","26c82f5c":"code","d02cced3":"code","3dc2989f":"code","307b4677":"code","de6c30af":"code","0934f5ef":"code","4797282b":"code","c693ab3d":"code","9b1d30f3":"code","68d6dacc":"code","a077b035":"code","bd51165a":"code","00cddd77":"code","9771f40d":"code","1450c192":"code","0894be14":"code","a807968f":"code","b1219a3d":"markdown","b8325e48":"markdown","b4fadc79":"markdown","02a69950":"markdown","18b3ad56":"markdown","3f683387":"markdown","659ae14a":"markdown","db34685e":"markdown","a814377c":"markdown"},"source":{"8ecfa798":"import numpy as np\nimport pandas as pd","ceb15307":"# Data loading\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ngender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","ca347cce":"# Number of rows and columns\nprint('Train', train.shape)\nprint('Test', test.shape)","36165b3f":"train.head()","bd4054db":"train.describe()","51e415a9":"test.head()","eb277f3c":"test.describe()","0ba1ad5a":"# Quantity of null values\ntrain_missing1 = train.isnull().sum()\ntest_missing1 = test.isnull().sum()\n\n# Propotion of null values\ntrain_missing2 = round(train.isnull().sum() \/ len(train) * 100)\ntest_missing2 = round(test.isnull().sum() \/ len(test) * 100)\n\n# Show as a table\nmissing_values = pd.DataFrame({'Num of null (train)' : train_missing1, '% of null (train)' : train_missing2, 'Num of null (test)' : test_missing1, '% of null (test)' : test_missing2})\nmissing_values","95574786":"import matplotlib.pyplot as plt\n\n# Distribution of survivers in age\ntrain['Age'].fillna(train.median(), inplace=True)\nx1 = train['Age'][train['Survived'] == 1]\nx2 = train['Age'][train['Survived'] == 0]\n\nplt.figure(figsize=(10,6))\nplt.hist(x1, bins=10, color='red', alpha=0.4)\nplt.hist(x2, bins=10, color='blue',alpha=0.4)\nplt.title('Histogram of age: survived vs. not survived group')\nplt.xlabel('Age')\nplt.legend(['Survived', 'Not survived'])\nplt.show()","373dce91":"# Comparison of gender between survived and not survived groups\nfrom statsmodels.graphics.mosaicplot import mosaic\nplt.rcParams[\"figure.figsize\"]=(10, 6)\ngender_colors = lambda key: {'color': 'chocolate' if 'female' in key else 'lightblue'}\nmosaic(train, ['Sex', 'Survived'], title='Proportion of gender: survived vs. not survived\\n1: Survived, 0: Not survived', \n       properties = gender_colors, gap = 0.02)\nplt.show()","4c9b5a2d":"# Comparison of ticket class between survived and not survived groups\ntrain['Pclass2'] = train['Pclass'].replace(1, '1st class').replace(2, '2nd class').replace(3, '3rd class')\n\nplt.rcParams[\"figure.figsize\"]=(10, 6)\nclass_colors = lambda key: {'color': 'lightblue' if '1st class' in key else ('chocolate' if '2nd class' in key else 'lightgreen')}\nmosaic(train.sort_values('Pclass2', ascending=True).sort_values('Survived'), ['Pclass2', 'Survived'], title='Proportion of passenger class: survived vs. not survived\\n1: Survived, 0: Not survived', \n       properties = class_colors, gap = 0.02)\nplt.show()","26c82f5c":"# Age: Fill null values with median\ntrain['Age'].fillna(train['Age'].median(), inplace=True)\ntest['Age'].fillna(test['Age'].median(), inplace=True)\n\n# Cast data type from float to int\ntrain['Age'] = train['Age'].astype(int)\ntest['Age'] = test['Age'].astype(int)","d02cced3":"# Embarked: Fill null values with mode\ntrain['Embarked'].fillna(train['Embarked'].mode()[0], inplace=True)","3dc2989f":"# Fare: Fill null values with median\ntest['Fare'].fillna(test['Fare'].median(), inplace=True)","307b4677":"# Transform categorical variables in Sex and Embarked columns to numerical variables\ntrain['Sex'] = train['Sex'].replace('male', 0).replace('female', 1)\ntrain['Embarked'] = train['Embarked'].replace('S', 1).replace('C', 2).replace('Q', 3)\n\ntest['Sex'] = test['Sex'].replace('male', 0).replace('female', 1)\ntest['Embarked'] = test['Embarked'].replace('S', 1).replace('C', 2).replace('Q', 3)\n\ntrain.head()","de6c30af":"# Create Accompanying feature (1 if passengers was accompanied by siblings or child)\ntrain['Accompanying'] = 1\ntrain.loc[(train['SibSp'] == 0) & (train['Parch'] == 0), 'Accompanying'] = 0\n\ntest['Accompanying'] = 1\ntest.loc[(test['SibSp'] == 0) & (test['Parch'] == 0), 'Accompanying'] = 0","0934f5ef":"# Create Family feature (sum of sibling and child) and Family_group feature (binning)\n\n# Binning family size\ndef f_group(x):\n  if x == 0:\n    return 0\n  elif x >= 1 and x < 4:\n    return 1\n  elif x >= 4 and x < 7:\n    return 2\n  elif x >= 7 and x < 9:\n    return 3\n  else:\n    return 4\n\ntrain['Family'] = train['SibSp'] + train['Parch']\ntrain['Family_group'] = train['Family'].map(f_group)\n\ntest['Family'] = test['SibSp'] + test['Parch']\ntest['Family_group'] = test['Family'].map(f_group)","4797282b":"# Create Age_group feature (Binning)\n\n# Binning age group\ndef a_group(x):\n  if x < 10:\n    return 0\n  elif x >= 10 and x < 30:\n    return 1\n  elif x >= 20 and x < 30:\n    return 2\n  elif x >= 30 and x < 40:\n    return 3\n  elif x >= 40 and x < 50:\n    return 4\n  elif x >= 50 and x < 60:\n    return 5\n  elif x >= 60 and x < 70:\n    return 6\n  elif x >= 70 and x < 80:\n    return 7\n  elif x >= 80 and x < 90:\n    return 8\n  else:\n    return 9\n\ntrain['Age_group'] = train['Age'].map(a_group)\ntest['Age_group'] = test['Age'].map(a_group)","c693ab3d":"train.head()","9b1d30f3":"# Use only numerical attributes\ntrain2 = train[['Survived', 'Pclass', 'Sex', 'Age', 'Age_group', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Accompanying', 'Family', 'Family_group']].copy()\ntest2 = test[['Pclass', 'Sex', 'Age', 'Age_group', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Accompanying', 'Family', 'Family_group']].copy()","68d6dacc":"import seaborn as sns\n\n# Check the correlation coefficients of the attributes\nplt.subplots(figsize=(8, 8))\nmask = np.triu(train2.corr())\nsns.heatmap(train2.corr(), annot=True, mask=mask, vmin=-1, vmax=1, cmap='coolwarm', square=True)\nplt.show()","a077b035":"# Check potentially impactful attributes using Random Forest Classifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndata_std = train2.drop('Survived', axis=1)\ntarget = np.array(train['Survived'])\n\n# Split dataset 70 : 30\nX_train, X_test, y_train, y_test = train_test_split(data_std, target, random_state=0, train_size=0.7)\n\n# Set Random Forest Classifier\nrfc = RandomForestClassifier(random_state=0)\nrfc.fit(X_train, y_train)\n\n# Visualize the importance ranking\nplt.figure(figsize=(8,5))\nplt.barh(\n    X_train.columns[np.argsort(rfc.feature_importances_)],\n    rfc.feature_importances_[np.argsort(rfc.feature_importances_)],\n    label='RandomForestClassifier'\n)\nplt.title('Random Forest Classifier feature importance')\nplt.show()","bd51165a":"# Combinations of attribute to train\n# Combination 1 \nc1 = train2[['Sex', 'Fare', 'Age']] \n\n# Combination 2\nc2 = train2[['Sex', 'Fare', 'Age', 'Pclass']] \n\ncombinations = [c1, c2]\ntarget = np.array(train['Survived'])\n","00cddd77":"from sklearn.preprocessing import StandardScaler\n\n# Standardize seleceted features\nstandard = StandardScaler()\nc1_std = pd.DataFrame(standard.fit_transform(c1), columns=c1.columns)\nc2_std = pd.DataFrame(standard.fit_transform(c2), columns=c2.columns)\ncombinations_std = [c1_std, c2_std]\n","9771f40d":"# Model selection\nfrom sklearn.model_selection import cross_val_score\n\n# Models\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \n\n# Performance indicators\nfrom sklearn.metrics import classification_report","1450c192":"# KNN: Function to determine optimal k \n\ndef k_search(x_train, y_train):\n  neighbors = list(range(1, 50, 2)) # Number of attempts of different k\n  list_score = []\n\n  for k in neighbors:\n    # 10-fold cross validation with f1 score\n    knc = KNeighborsClassifier(n_neighbors = k)\n    score = cross_val_score(knc, X_train, y_train, cv=10, scoring='f1')\n    list_score.append(score.mean())\n\n  # Changing to misclassification error\n  mse = [1 - x for x in list_score]\n\n  # Determining best k\n  optimal_k = neighbors[mse.index(min(mse))]\n  return optimal_k","0894be14":"# Model comparison with f1 score and accuracy for not standardized attributions\nfor c in range(len(combinations)):\n    # set a combination\n    attributes = combinations[c]\n    print(\"\u25a0\u25a0\u25a0 Combination \" + str(c+1) + \": \" + str(attributes.columns))\n\n    # split training dataset\n    X_train, X_test, y_train, y_test = train_test_split(attributes, target, random_state=42, train_size=0.7)\n\n    # kNN\n    print(\"k-Nearest Neighbor\")\n    k_value = k_search(X_train, y_train)\n    knn = KNeighborsClassifier(n_neighbors = k_value)\n    \n    f1_knn = cross_val_score(knn, X_train, y_train, scoring='f1', cv=10)\n    f1Mean_knn = f1_knn.mean()\n    f1SD_knn = f1_knn.std()\n\n    accu_knn = cross_val_score(knn, X_train, y_train, scoring='accuracy', cv=10)\n    accuMean_knn = accu_knn.mean()\n    accuSD_knn = accu_knn.std()\n\n    print(\"- Mean F1 score for Survived=1: \" + str(round(f1Mean_knn, 2)) + \" *SD=\" + str(round(f1SD_knn, 2)))\n    print(\"- Mean accuracy score: \" + str(round(accuMean_knn, 2)) + \" *SD=\" + str(round(accuSD_knn, 2)) + '\\n')\n\n    # Linear Discriminant Analysis\n    print(\"Linear Discriminant Analysis\")\n    lda = LinearDiscriminantAnalysis()\n    f1_lda = cross_val_score(lda, X_train, y_train, scoring='f1', cv=10)\n    f1Mean_lda = f1_lda.mean()\n    f1SD_lda = f1_lda.std()\n    \n    accu_lda = cross_val_score(lda, X_train, y_train, scoring='accuracy', cv=10)\n    accuMean_lda = accu_lda.mean()\n    accuSD_lda = accu_lda.std()\n    print(\"- Mean F1 score for Survived=1: \" + str(round(f1Mean_lda, 2)) + \" *SD=\" + str(round(f1SD_lda, 2)))\n    print(\"- Mean accuracy score: \" + str(round(accuMean_lda, 2)) + \" *SD=\" + str(round(accuSD_lda, 2)) + '\\n')\n\n    # Logistic Regression\n    print(\"Logistc Regression\")\n    logreg = LogisticRegression()\n    f1_lr = cross_val_score(logreg, X_train, y_train, scoring='f1', cv=10)\n    f1Mean_lr = f1_lr.mean() \n    f1SD_lr = f1_lr.std()\n    \n    accu_lr = cross_val_score(logreg, X_train, y_train, scoring='accuracy', cv=10)\n    accuMean_lr = accu_lr.mean()\n    accuSD_lr = accu_lr.std()\n    print(\"- Mean F1 score for Survived=1: \" + str(round(f1Mean_lr, 2)) + \" *SD=\" + str(round(f1SD_lr, 2)))\n    print(\"- Mean accuracy score: \" + str(round(accuMean_lr, 2)) + \" *SD=\" + str(round(accuSD_lr, 2)) + '\\n')\n    \n    # Naive Bayes\n    print(\"Naive Bayes\")\n    nb = GaussianNB()\n    f1_nb = cross_val_score(nb, X_train, y_train, scoring='f1', cv=10)\n    f1Mean_nb = f1_nb.mean() \n    f1SD_nb = f1_nb.std()\n\n    accu_nb = cross_val_score(nb, X_train, y_train, scoring='accuracy', cv=10)\n    accuMean_nb = accu_nb.mean()\n    accuSD_nb = accu_nb.std()\n    print(\"- Mean F1 score for Survived=1: \" + str(round(f1Mean_nb, 2)) + \" *SD=\" + str(round(f1SD_nb, 2)))\n    print(\"- Mean accuracy score: \" + str(round(accuMean_nb, 2)) + \" *SD=\" + str(round(accuSD_nb, 2)) + '\\n')\n","a807968f":"# Model comparison with f1 score and accuracy for standardized attributions\nfor c in range(len(combinations_std)):\n    # set a combination\n    attributes = combinations_std[c]\n    print(\"\u25a0\u25a0\u25a0 Standardized Combination \" + str(c+1) + \": \" + str(attributes.columns))\n\n    # split training dataset\n    X_train, X_test, y_train, y_test = train_test_split(attributes, target, random_state=42, train_size=0.7)\n\n    # kNN\n    print(\"k-Nearest Neighbor\")\n    k_value = k_search(X_train, y_train)\n    knn = KNeighborsClassifier(n_neighbors = k_value)\n    \n    f1_knn = cross_val_score(knn, X_train, y_train, scoring='f1', cv=10)\n    f1Mean_knn = f1_knn.mean()\n    f1SD_knn = f1_knn.std()\n\n    accu_knn = cross_val_score(knn, X_train, y_train, scoring='accuracy', cv=10)\n    accuMean_knn = accu_knn.mean()\n    accuSD_knn = accu_knn.std()\n\n    print(\"- Mean F1 score for Survived=1: \" + str(round(f1Mean_knn, 2)) + \" *SD=\" + str(round(f1SD_knn, 2)))\n    print(\"- Mean accuracy score: \" + str(round(accuMean_knn, 2)) + \" *SD=\" + str(round(accuSD_knn, 2)) + '\\n')\n\n    # Linear Discriminant Analysis\n    print(\"Linear Discriminant Analysis\")\n    lda = LinearDiscriminantAnalysis()\n    f1_lda = cross_val_score(lda, X_train, y_train, scoring='f1', cv=10)\n    f1Mean_lda = f1_lda.mean()\n    f1SD_lda = f1_lda.std()\n    \n    accu_lda = cross_val_score(lda, X_train, y_train, scoring='accuracy', cv=10)\n    accuMean_lda = accu_lda.mean()\n    accuSD_lda = accu_lda.std()\n    print(\"- Mean F1 score for Survived=1: \" + str(round(f1Mean_lda, 2)) + \" *SD=\" + str(round(f1SD_lda, 2)))\n    print(\"- Mean accuracy score: \" + str(round(accuMean_lda, 2)) + \" *SD=\" + str(round(accuSD_lda, 2)) + '\\n')\n\n    # Logistic Regression\n    print(\"Logistc Regression\")\n    logreg = LogisticRegression()\n    f1_lr = cross_val_score(logreg, X_train, y_train, scoring='f1', cv=10)\n    f1Mean_lr = f1_lr.mean() \n    f1SD_lr = f1_lr.std()\n    \n    accu_lr = cross_val_score(logreg, X_train, y_train, scoring='accuracy', cv=10)\n    accuMean_lr = accu_lr.mean()\n    accuSD_lr = accu_lr.std()\n    print(\"- Mean F1 score for Survived=1: \" + str(round(f1Mean_lr, 2)) + \" *SD=\" + str(round(f1SD_lr, 2)))\n    print(\"- Mean accuracy score: \" + str(round(accuMean_lr, 2)) + \" *SD=\" + str(round(accuSD_lr, 2)) + '\\n')\n    \n    # Naive Bayes\n    print(\"Naive Bayes\")\n    nb = GaussianNB()\n    f1_nb = cross_val_score(nb, X_train, y_train, scoring='f1', cv=10)\n    f1Mean_nb = f1_nb.mean() \n    f1SD_nb = f1_nb.std()\n\n    accu_nb = cross_val_score(nb, X_train, y_train, scoring='accuracy', cv=10)\n    accuMean_nb = accu_nb.mean()\n    accuSD_nb = accu_nb.std()\n    print(\"- Mean F1 score for Survived=1: \" + str(round(f1Mean_nb, 2)) + \" *SD=\" + str(round(f1SD_nb, 2)))\n    print(\"- Mean accuracy score: \" + str(round(accuMean_nb, 2)) + \" *SD=\" + str(round(accuSD_nb, 2)) + '\\n')\n","b1219a3d":"## Standardization","b8325e48":"## Feature engineering","b4fadc79":"# Feature Selection","02a69950":"# Preprocessing","18b3ad56":"# Supervised Learning (kNN, LDA, Logistic Regression, Naive Bayes)","3f683387":"## Dummy coding","659ae14a":"# Data checking","db34685e":"## Deal with null values","a814377c":"## Visualization"}}