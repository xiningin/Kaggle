{"cell_type":{"a717af89":"code","7666c255":"code","47daf9a1":"code","2e04592e":"code","22a855ac":"code","49e5d00f":"code","b58163e2":"code","86383506":"code","9c1238bf":"code","a5b6ac23":"code","394be387":"code","b533aa2c":"code","77641376":"code","d6c18c60":"code","fe4b2381":"code","d8d84bb4":"code","1a591c0f":"code","f7d8194f":"code","39665e07":"code","34efe366":"code","3dbd401c":"code","e44ea8a4":"code","a4aa41f0":"code","060fa9a9":"code","24468748":"code","35b937cd":"code","91e5394f":"code","9af7b58c":"code","f9985e6a":"code","8bb3a16e":"code","dbdc0f17":"code","dd4892c0":"code","9805ad62":"code","1e06e88c":"markdown","50af2992":"markdown","657fdca7":"markdown","2c5d0f10":"markdown","85a50698":"markdown","caf75710":"markdown","cc95f26c":"markdown","4561e1b9":"markdown","a2558281":"markdown","d206809c":"markdown","fd96c896":"markdown","49304f10":"markdown","5b146d03":"markdown","7be934ec":"markdown","0432a01a":"markdown","5b9f8240":"markdown","973b2f31":"markdown","b4555865":"markdown","cfcd4830":"markdown","9fb301eb":"markdown"},"source":{"a717af89":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nROOT_DIR = '..\/input\/cassava-leaf-disease-classification\/'\nos.listdir(ROOT_DIR)\n\nimport json # to read in the 'label_num_to_disease_map.json' file","7666c255":"import matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nimport cv2\nimport random","47daf9a1":"from sklearn.model_selection import train_test_split","2e04592e":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.applications import EfficientNetB3, EfficientNetB5\nfrom tensorflow.keras.utils import plot_model","22a855ac":"# set the training and test directory paths\nTRAIN_DIR = '..\/input\/cassava-leaf-disease-classification\/train_images\/'\nTEST_DIR = '..\/input\/cassava-leaf-disease-classification\/test_images\/'","49e5d00f":"# set seed\nseed = 42\n\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(seed)","b58163e2":"train_df = pd.read_csv(ROOT_DIR + 'train.csv')\nsample_df = pd.read_csv(ROOT_DIR + 'sample_submission.csv')","86383506":"print(train_df.shape, sample_df.shape)\ndisplay(train_df.head())","9c1238bf":"f = open(ROOT_DIR + 'label_num_to_disease_map.json')\ndata = json.load(f)\nprint(json.dumps(data, indent = 2))","a5b6ac23":"z = train_df.sample(20)\ndisplay(z)\nimages, labels = z['image_id'].tolist(), z['label'].tolist()","394be387":"plt.figure(figsize = (20,20))\nfor i in range(20):\n    plt.subplot(4,5,i+1)\n    img = cv2.imread(TRAIN_DIR + images[i])\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.title(data[str(labels[i])])","b533aa2c":"sns.set_style('whitegrid')\nsns.countplot(x = 'label', data = train_df, palette = 'Pastel1');","77641376":"pie_df = train_df['label'].value_counts().reset_index()\npie_df.columns = ['label', 'count']\nfig = px.pie(pie_df, values = 'count', names = 'label', color_discrete_sequence = px.colors.qualitative.Pastel)\nfig.show()","d6c18c60":"train_df = train_df.astype({\"label\": str})","fe4b2381":"train, test = train_test_split(train_df, test_size = 0.15, random_state = seed)\nprint(train.shape, test.shape)","d8d84bb4":"IMG_SIZE = 300\nsize = (IMG_SIZE,IMG_SIZE)\nbatch_size = 32","1a591c0f":"datagen = ImageDataGenerator(\n                    rotation_range = 30,\n                    width_shift_range = 0.2,\n                    height_shift_range = 0.2,\n                    shear_range = 0.2,\n                    zoom_range = 0.2,\n                    brightness_range = [0.5,1.5],\n                    horizontal_flip = True,\n                    vertical_flip = True,\n                    fill_mode = 'nearest'\n)","f7d8194f":"validgen = ImageDataGenerator()","39665e07":"train_generator = datagen.flow_from_dataframe(\n                    train,\n                    directory = TRAIN_DIR,\n                    x_col = \"image_id\",\n                    y_col = \"label\",\n                    target_size = size,\n                    class_mode = \"sparse\",\n                    batch_size = batch_size,\n                    shuffle = True,\n                    seed = seed,\n                    interpolation = \"nearest\"\n)","34efe366":"valid_generator = validgen.flow_from_dataframe(\n                    test,\n                    directory = TRAIN_DIR,\n                    x_col = \"image_id\",\n                    y_col = \"label\",\n                    target_size = size,\n                    class_mode = \"sparse\",\n                    batch_size = batch_size,\n                    shuffle = False,\n                    seed = seed,\n                    interpolation = \"nearest\"\n)","3dbd401c":"img = cv2.imread(os.path.join(TRAIN_DIR,'1000201771.jpg'))\nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.imshow(img)","e44ea8a4":"plt.figure(figsize = (10,10))\n# set the title\nplt.title('Augmented Images')\n# load the image\nimg = load_img(os.path.join(TRAIN_DIR,'1000201771.jpg'))\n# convert to numpy array\ndata = img_to_array(img)\n# expand dimension to one sample\nsamples = np.expand_dims(data, 0)\n# iterator\nitr = datagen.flow(samples, batch_size = 1)\n# generate samples and plot\nfor i in range(12):\n    # define subplot\n    plt.subplot(4,3,i+1)\n    # generate batch of images\n    batch = itr.next()\n    # convert to unsigned integers for viewing\n    image = batch[0].astype('uint8')\n    # plot raw pixel data\n    plt.imshow(image)\n# show the figure\nplt.show()","a4aa41f0":"NUM_CLASSES = 5","060fa9a9":"def create_model():\n    \n    model = models.Sequential()\n    # initialize EfficientNetB3 model with input shape as (300,300,3)\n    model.add(EfficientNetB3(input_shape = (IMG_SIZE, IMG_SIZE, 3), include_top = False, weights = 'imagenet'))\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dense(256, activation = 'relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(NUM_CLASSES, activation = 'softmax'))\n    \n    return model","24468748":"model = create_model()","35b937cd":"model.compile(loss = 'sparse_categorical_crossentropy',\n             optimizer = Adam(learning_rate = 0.001),\n             metrics = ['accuracy'])","91e5394f":"# Stop training when the validation loss metric has stopped decreasing for 5 epochs.\nearly_stopping = EarlyStopping(monitor = 'val_loss',\n                               patience = 5,\n                               mode = 'min',\n                               restore_best_weights = True)\n\n# Save the model with the maximum validation accuracy \ncheckpoint = ModelCheckpoint('best_model.hdf5', \n                             monitor = 'val_accuracy',\n                             verbose = 1,\n                             mode = 'max', \n                             save_best_only = True)\n# reduce learning rate\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n                              factor = 0.2,\n                              patience = 2,\n                              mode = 'min',\n                              verbose = 1)","9af7b58c":"EPOCHS = 15\nSTEP_SIZE_TRAIN = train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n\/\/valid_generator.batch_size","f9985e6a":"history = model.fit(train_generator,\n                    validation_data = valid_generator,\n                    epochs = EPOCHS,\n                    steps_per_epoch = STEP_SIZE_TRAIN,\n                    validation_steps = STEP_SIZE_VALID,\n                    callbacks = [early_stopping, checkpoint, reduce_lr]\n                   )","8bb3a16e":"model.summary()","dbdc0f17":"plot_model(model, show_shapes = True)","dd4892c0":"model.evaluate_generator(generator = valid_generator, steps = STEP_SIZE_VALID)","9805ad62":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'c-', label='Training accuracy')\nplt.plot(epochs, val_acc, 'y-', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'c-', label='Training Loss')\nplt.plot(epochs, val_loss, 'y-', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","1e06e88c":"### Short version history:\n* Baseline model - 0.601\n* EfficientNetB0 - image_size = (224,224), batch_size =\t64\t- 0.841\n* EfficientNetB3 - image_size =\t(300,300), batch_size =\t32\t- 0.856\n* EfficientNetB3 with more augmentations - 0.865\n","50af2992":"We will take a quick look at the data files and train our model in this notebook. The model can be saved and used for inference in a later notebook.","657fdca7":"<center><h2> Visualizing some of the augmented images <\/h2><\/center>","2c5d0f10":"<center><h1> Model creation and training <\/h1><\/center> ","85a50698":"<center><h1> Data exploration <\/h1><\/center> ","caf75710":"<center><h1> Introduction <\/h1><\/center>\n","cc95f26c":"<center><h1> Plotting a count plot and pie chart for class distribution. <\/h1><\/center> ","4561e1b9":"* **Competition** - Given a set of images of Cassava leaves we need to classify them as one of the four diseases or healthy.\n* **Data** - A collection of 21397 labelled images\n* **Evaluation** - Classification accuracy","a2558281":"<center><h1> Plotting 20 randomly sampled images with class <\/h1><\/center> ","d206809c":"<center><h1> Creating ImageDataGenerator to generate data in batches and perform image augmentation. <\/h1><\/center> ","fd96c896":"There is a class imbalance problem here. The Cassava Mosaic Disease (CMD) samples are more compared to other classes. We can solve this by either upsampling other class samples or downsampling the CMD samples.","49304f10":"<center><h1> Importing necessary libraries <\/h1><\/center> ","5b146d03":"## Do consider upvoting if you found it useful :)\n### Thank you for reading the notebook.","7be934ec":"<center><h1> Split dataset for training and validation <\/h1><\/center> \n<center> Reserving 15% of data for validation <\/center>","0432a01a":"| Base model\t  |resolution |\n|-----------------|-----------|\n| EfficientNetB0  |\t224 |\n| EfficientNetB1  |\t240 |\n| EfficientNetB2  |\t260 |\n| EfficientNetB3  |\t300 |\n| EfficientNetB4  |\t380 |\n| EfficientNetB5  |\t456 |\n| EfficientNetB6  |\t528 |\n| EfficientNetB7  |\t600 |","5b9f8240":"### Some useful links:\n* The prediction and submission notebook can be found here : [Inference Notebook](https:\/\/www.kaggle.com\/lavanyask\/cassava-leaf-disease-inference)\n* More about keras EfficientNets: [here](https:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/)","973b2f31":"### Images after Augmentation","b4555865":"### Further experiments:\n* Trying out different network architecture, changing number of layers in the network\n* Hyperparameter tuning - changing epochs, batch size, number of neurons in hiddden layers, activation function ...\n* Cross Validation\n* More augmentation techniques\n* PyTorch","cfcd4830":"<center><h1> Model evaluation <\/h1><\/center> ","9fb301eb":"### Image before Augmentation"}}