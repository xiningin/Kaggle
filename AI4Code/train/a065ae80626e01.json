{"cell_type":{"2e4e1356":"code","9f66decd":"code","24fc768d":"code","7d777aef":"code","83dc65cb":"code","480e367e":"code","d6d34563":"code","680bacdf":"code","9bc37d85":"code","88fa2f5a":"code","14c4be3c":"code","dd14a975":"code","9f043a6d":"code","1c420c1e":"code","c0229f56":"code","0e386f8c":"code","dcd70e41":"code","6f29c5b4":"code","41be1be8":"code","6e4de065":"code","3edec1bc":"code","a5495dab":"code","3f2d84f4":"code","1ccf09a2":"markdown","229fb7a5":"markdown","630a0f3a":"markdown","ee658df7":"markdown","3c96d569":"markdown","c61251cd":"markdown","530ea4ea":"markdown","f9838221":"markdown","f613f0f9":"markdown","6486232e":"markdown","195c9f02":"markdown","75512d55":"markdown","9d1d7ea5":"markdown","7bc77a30":"markdown","d1410d3c":"markdown"},"source":{"2e4e1356":"# Importing libraries:\n\n# Importing numpy, pandas, matplotlib and seaborn:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Imports for plotly:\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n\n# To keep graph within the nobebook:\n%matplotlib inline\n\n# To hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')","9f66decd":"# Read data from dataset's csv file:\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","24fc768d":"# Display the shape of dataset (#of rows, #of columns):\nprint('The shape of the dataset is: ', df.shape)","7d777aef":"# Show first 5 rows of dataset:\ndf.head()","83dc65cb":"# Explore dataframe's statistics (numerical values only):\ndf.describe()","480e367e":"# Function to describe variables\ndef desc(df):\n    d = pd.DataFrame(df.dtypes,columns=['Data_Types'])\n    d = d.reset_index()\n    d['Columns'] = d['index']\n    d = d[['Columns','Data_Types']]\n    d['Missing'] = df.isnull().sum().values    \n    d['Uniques'] = df.nunique().values\n    return d\n\n# Use desc() function to describe df:\ndesc(df)","d6d34563":"# Class distribution dataframe:\n\ncls_df = pd.DataFrame(df.groupby(['Class'])['Class'].count())\ncls_df['Category'] = ['Not-Fraud', 'Fraud']\n\n# Create bar chart for class distribution:\n\ndata=go.Bar( x = cls_df.Category\n           , y = cls_df.Class\n           ,  marker=dict( color=['#4c5cff', '#ff4682'])\n           , text=cls_df.Class\n           , textposition='auto' \n           )\n\n\n\nlayout = go.Layout( title = 'Class distribution'\n                  , xaxis = dict(title = 'Class')\n                  , yaxis = dict(title = 'Volume')\n                  )\n\nfig = go.Figure(data,layout)\n\nfig.show()","680bacdf":"# Count the occurrences of Fraud and Not-Fraud:\nvol = df.Class.value_counts()\n\n# Print the % of Not-Fraud and Fraud cases:\nprint(vol * 100 \/ len(df.index))","9bc37d85":"# Scatter graph for Distribution of Transactions split by Category (Not-Fraud\/Fraud):\n\ndf['Category'] = df['Class'].map({0:'Not-Fraud', 1:'Fraud'})\n\nfig = px.scatter(df\n                 , x='V15'\n                 , y='Amount'\n                 , color = 'Category'\n                 , size = 'Amount'\n                 #, facet_col='Category'\n                 , color_continuous_scale= ['#4c5cff','#ff4682']\n                 , render_mode=\"webgl\"\n                )\n\nfig.update_layout(title='Distribution of Transactions - by Category'\n                  , xaxis_title='V15'\n                  , yaxis_title='Transaction Value ($)'\n                 )\n\nfig.show()","88fa2f5a":"# Correlation matrix for Credit Card - Fraud Detection dataset features:\n\ncorr = df.corr()\nl = list(corr.columns)\n\nfig = go.Figure(data=go.Heatmap(z=corr\n                                , x=l\n                                , y=l\n                                , hoverongaps = False\n                                \n                               )\n               )\n\nfig.update_layout(title='Correlation for Features')\n\n\nfig.show()","14c4be3c":"# Create dataset for Fraud & Non-Fraud features (mask_1&mask_0 respectively):\nmask_1 = df.loc[df.Class == 1]\nmask_0 = df.loc[df.Class == 0]\n\n# Create a list of columns (excluding Class and Category):\ncolumns = list(df.columns)[0:-2]","dd14a975":"# Crate box plot and histograms for Fraud and Not-Fraud transactions:\nfor i in columns:\n    fig = plt.figure(figsize = (12,4))\n\n    plt.subplot(1,3,1)\n    sns.boxplot(x='Class',\n                y=i,\n                hue='Category', \n                palette=  ['#ff4682','#4c5cff'],\n                data=df\n                )\n\n    # Label axes\n    plt.title('Boxplot '+'('+i+')')\n    plt.xlabel('')\n    plt.ylabel('value')\n    \n    plt.tight_layout() \n\n    plt.subplot(1,3,2)\n    plt.hist(x=i\n         , edgecolor = 'black'\n         , linewidth = 1.5\n         , bins = 20\n         , color = '#ff4682'\n         , label = 'Fraud'\n        # , alpha = 0.5\n         , data=mask_1)\n\n    # Label axes\n    plt.title('Fraud Histogram '+'('+i+')')\n    plt.xlabel('')\n    plt.ylabel('count')\n    plt.tight_layout() \n\n    plt.subplot(1,3,3)\n    plt.hist(x=i\n         , edgecolor = 'black'\n         , linewidth = 1.5\n         , bins = 20\n         , color = '#4c5cff'\n         , label = 'Not-Fraud'\n         , alpha = 0.5\n         , data=mask_0)\n\n    # Label axes\n    plt.title('Non-Fraud Histogram '+'('+i+')')\n    plt.xlabel('')\n    plt.ylabel('count')\n    \n    plt.tight_layout()    \n    \n\n# Display label\nplt.legend(loc='best')\n\n\nplt.show()\n","9f043a6d":"X=df.drop(['Time', 'Class', 'Category'], axis = 1)\ny=df.Class","1c420c1e":"# Import train_test_split form sklearn:\nfrom sklearn.model_selection import train_test_split\n\n# Keep 30% of data for testing:\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=42)","c0229f56":"# Count the occurrences of Fraud and Not-Fraud:\nv = y_train.value_counts()\n\n# Print the % of Not-Fraud and Fraud cases:\nprint(v * 100 \/ len(y_train.index))","0e386f8c":"from imblearn.over_sampling import SMOTE\nsmt = SMOTE()","dcd70e41":"# Transform the dataset\n\nX, y = smt.fit_resample(X_train, y_train)","6f29c5b4":"# Count the occurrences of Fraud and Not-Fraud:\nv = y.value_counts()\n\n# Print the % of Not-Fraud and Fraud cases:\nprint(v * 100 \/ len(y.index))","41be1be8":"from sklearn.linear_model import LogisticRegression\n\n# Importing classification_method and confusion_matrix:\nfrom sklearn.metrics import classification_report, confusion_matrix","6e4de065":"# Fit a logistic regression model to our data\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Obtain model predictions\ny_pred = model.predict(X_test)\n\n# Print the classifcation report and confusion matrix\nprint('Classification report:\\n', classification_report(y_test, y_pred))\nconf_mat = confusion_matrix(y_true=y_test, y_pred = y_pred)\nprint('Confusion matrix:\\n', conf_mat)","3edec1bc":"z = confusion_matrix(y_test, y_pred)\n\nx = ['Genuine', 'Fraud']\ny = ['Genuine', 'Fraud']\n\n# change each element of z to type string for annotations\nz_text = [[str(y) for y in x] for x in z]\n\n# set up figure \nfig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='RdBu')\n\n# add title\nfig.update_layout(title_text='<i><b>Confusion matrix<\/b><\/i>',\n                  #xaxis = dict(title='x'),\n                  #yaxis = dict(title='x')\n                 )\n\n# add custom xaxis title\nfig.add_annotation(dict(font=dict(color=\"black\",size=14),\n                        x=0.5,\n                        y=-0.15,\n                        showarrow=False,\n                        text=\"Predicted value\",\n                        xref=\"paper\",\n                        yref=\"paper\"))\n\n\n# add custom yaxis title\nfig.add_annotation(dict(font=dict(color=\"black\",size=14),\n                        x=-0.35,\n                        y=0.5,\n                        showarrow=False,\n                        text=\"Real value\",\n                        textangle=-90,\n                        xref=\"paper\",\n                        yref=\"paper\"))\n\n# adjust margins to make room for yaxis title\nfig.update_layout(margin=dict(t=50, l=200))\n\n# add colorbar\nfig['data'][0]['showscale'] = True\nfig.show()","a5495dab":"# Import the random forest model from sklearn\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Define the model as the random forest\nmodel = RandomForestClassifier(random_state=5)\n\n# Fit the model to our training set\nmodel.fit(X_train, y_train)\n\n# Obtain predictions from the test data \npredicted = model.predict(X_test)\n","3f2d84f4":"# Import the packages to get the different performance metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n\n# Predict probabilities\nprobs = model.predict_proba(X_test)\n\n# Print the ROC curve, classification report and confusion matrix\nprint(roc_auc_score(y_test, probs[:,1]))\nprint(classification_report(y_test, predicted))\nprint(confusion_matrix(y_test, predicted))","1ccf09a2":"### Importing Libraries","229fb7a5":"From the visualistion above, we can see that only minority of transactions is fraud related. It is also interesting to find out that fraudulent transactions tend to be of lower amount (below $2,000).","630a0f3a":"It is important to apply train_test_split to our dataset to create train and test dataset before applying any of the resampling methods.","ee658df7":"There is no correlation between features V1 to V28. Class is not correlated with quite a few features, for example V19 to V28, Time and Amount.","3c96d569":"We will try to incease a success rate of fraud detection by using dataresampling. There are a few techniques that can be used for example:\n\n**UNDERSAMPLING**\n\nRandom Undersampling (RUS) - we throw away some data to make dataset more balanced. This technique is computationally efficient but requires huge amont of data.\n<img src='https:\/\/miro.medium.com\/max\/335\/1*YH_vPYQEDIW0JoUYMeLz_A.png' width=300 align='center'\/>\n**OVERSAMPLING**\n\nRandom Oversampling (ROS) - duplicating minority class (Fraud) to balance dataset. The disadvantage is that we are training our model on duplicates.\n<img src='https:\/\/miro.medium.com\/max\/375\/1*aKJJOozIlVVH1gT-4rYy4w.png' width=300 align='center'\/>\n**SMOTE**\n\nSynthetic Minority Oversampling Technique (SMOTE) - this method creates data artificially to balance dataset. It creates more sophisticated and realistic dataset, but we are basically training on 'fake' data.\n\n<img src='https:\/\/miro.medium.com\/max\/2760\/1*bSOwLuDleEEGiuw7PtooOQ.png' width=300 align='center'\/>","c61251cd":"The following visualisation is to give us more overview of distribution of features split by Class (Fraud or Not-Fraud). It's always a good practice to explore your features and gain more insight. ","530ea4ea":"#### Applying SMOTE","f9838221":"Credit card fraud is any kind of theft or fraud that involves a credit card. The aim of credit card fraud is to purchase goods without paying, or to steal money from someone else\u2019s credit account.\n\nWe can break down the types of credit card fraud into four main areas:\n\n 1. Lost or stolen cards that are used without their owner\u2019s permission\n 2. Skimmed cards - this is when the card is cloned or copied with a special swipe machine to make a duplicate of the card\n 3. Card-not-present - card details such as card number, holder\/account name, date of birth and address are stolen, often from online databases or through phishing, then sold and used on the internet, or over the phone\n 4. Committing fraudulent applications in someone else\u2019s name for a new credit card, without that person knowing ","f613f0f9":"# Credit Card - Fraud Detection","6486232e":"### Exploratory Data Analysis (EDA)","195c9f02":"### Resampling Methods","75512d55":"We can now see how imbalanced the dataset is. About 99.8% transactions are genuine and those identified as fraudulent represent approximately 0.2%. We can get over 99% model accuracy easily just by assuming that every transaction Class is equal to 0. This sounds great!\n\nActually it's not great news. Our model won't have enough data for fraudulent transactions to learn how to recognise them. This will cause that we won't be able to capture fraud.\n\nIn reality Fraud Analytics teams use the combination of different techniques to identify fraud, for example:\n - Rules based systems, with thresholds set manually and based on previous experience;\n - Matching data to external lists of fraudulet accounts and names;\n - Use machine learning algorithms to detect fraud or suspicious behaviour","9d1d7ea5":"### Training the Data","7bc77a30":"<img src='https:\/\/menafn.com\/updates\/pr\/2019-12\/03\/N_38956036-3image_story.jpg' width=600 align='center'\/>\n\nCredit card fraud is in general a rare event in comparison to the amount of genuine transactions. We can expect the dataset to be imbalanced and with only small ratio of data being fraudulent.\n\nThis brings some challenges! We can get a high accuracy score by predicting majority class (not fraudulent) but might fail to predict minority class. This is actually contradicting with our aim - to highlight and prevent from fraudulent transactions. \n\n\n\n","d1410d3c":"### Loading the Data and Initial Exploration"}}