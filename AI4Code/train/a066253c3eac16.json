{"cell_type":{"261732fd":"code","616440f3":"code","48e28d68":"code","24897a9d":"code","fb2cfa45":"code","7efef0ca":"code","e411c02f":"code","2cba100f":"code","39263256":"code","08d4b369":"code","556fc89c":"code","7d4299c2":"code","13281e1b":"code","7783f841":"code","b3bb4e6f":"code","59ec78e3":"code","98a4ad59":"code","b52cd81c":"code","0f239638":"code","d724e1d3":"code","36f0e520":"code","4bc3093e":"code","71c62dd3":"code","ec1e6dc2":"code","998f0f0d":"code","ff32a63d":"code","3d2458b1":"code","cb77bc23":"code","9888e6fc":"code","1350d6a9":"code","89aa27eb":"code","e06bcbff":"code","10441770":"code","711cd23e":"code","8b7498db":"code","f4d62e59":"code","2d2015d7":"markdown","49db6483":"markdown","216a1a3c":"markdown","1975592b":"markdown","69cbe63c":"markdown","b21cc3a5":"markdown","07d0a8b9":"markdown","b72497a2":"markdown","bb22d65d":"markdown","86ec8efe":"markdown","98c4d5a7":"markdown","5addd218":"markdown","9de1a479":"markdown","4d489f89":"markdown","b56821ed":"markdown","577b06d3":"markdown","f09fb580":"markdown","bd0998ac":"markdown","eb442820":"markdown","1ce4b80c":"markdown","ac3ee358":"markdown","bd65b6a9":"markdown","826886e3":"markdown","2a50e151":"markdown"},"source":{"261732fd":"from tensorflow import keras\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras.layers import MaxPool2D,Convolution2D,Flatten,Dense,MaxPooling2D,Dropout\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nimport os\nfrom PIL import Image","616440f3":"im1=Image.open('..\/input\/intel-image-classification\/seg_train\/seg_train\/buildings\/10029.jpg')\nim1","48e28d68":"# Let's have a look at another image\nim2 = Image.open('..\/input\/intel-image-classification\/seg_train\/seg_train\/sea\/10087.jpg')\nim2","24897a9d":"# Get the training and testing data\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255, \n                                   shear_range = 0.2, \n                                   zoom_range = 0.2, \n                                   horizontal_flip = True)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntraining_set = train_datagen.flow_from_directory('..\/input\/intel-image-classification\/seg_train\/seg_train', \n                                                    target_size = (64, 64), \n                                                    batch_size = 32)\ntest_set = test_datagen.flow_from_directory('..\/input\/intel-image-classification\/seg_test\/seg_test',\n                                                target_size = (64, 64),\n                                                 batch_size = 32)\n","fb2cfa45":"# creating the model_glorot_init\nmodel_glorot_init=Sequential()\nmodel_glorot_init.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal'))\nmodel_glorot_init.add(MaxPooling2D(pool_size=(2,2)))\nmodel_glorot_init.add(Flatten())\nmodel_glorot_init.add(Dense(3000,activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal'))\nmodel_glorot_init.add(Dense(1024,activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal'))\nmodel_glorot_init.add(Dense(units=6,activation='softmax',  kernel_initializer='glorot_normal', bias_initializer='glorot_normal'))","7efef0ca":"model_glorot_init.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])\n","e411c02f":"glorot_model = model_glorot_init.fit_generator(training_set,\n                        steps_per_epoch = 100,\n                        epochs = 10,\n                        validation_data = test_set,\n                        validation_steps = 10)","2cba100f":"# Plot for loss of training and validation\nplt.plot([i for i in range(10)],glorot_model.history['loss'],\"-b\",label='Training Loss')\nplt.plot([i for i in range(10)],glorot_model.history['val_loss'],color=\"orange\",label=\"Validation Loss\")\nplt.legend(loc=\"upper right\")\nplt.show()","39263256":"# Plot for loss of training and validation\nplt.plot([i for i in range(10)],glorot_model.history['accuracy'],\"-b\",label='Training Accuracy')\nplt.plot([i for i in range(10)],glorot_model.history['val_accuracy'],color=\"orange\",label=\"Validation Accuracy\")\nplt.legend(loc=\"lower right\")\nplt.show()","08d4b369":"# From the last file we saw that the best model was with Xe\/Xavier initialization ....\n# But what about regularization\n# creating the model_l1_reg\nmodel_l1_reg=Sequential()\nmodel_l1_reg.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal'))\nmodel_l1_reg.add(MaxPooling2D(pool_size=(2,2)))\nmodel_l1_reg.add(Flatten())\nmodel_l1_reg.add(Dense(3000,activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal',                \n                               kernel_regularizer=tf.keras.regularizers.l1(0.01)))\nmodel_l1_reg.add(Dense(1024,activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal',                \n                               kernel_regularizer=tf.keras.regularizers.l1(0.01)))\nmodel_l1_reg.add(Dense(units=6,activation='softmax',  kernel_initializer='glorot_normal', bias_initializer='glorot_normal'))","556fc89c":"model_l1_reg.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])","7d4299c2":"l1_model = model_l1_reg.fit(training_set,\n                        steps_per_epoch = 100,\n                        epochs = 10,\n                        validation_data = test_set,\n                        validation_steps = 10)","13281e1b":"# Plot for loss of training and validation\nplt.plot([i for i in range(10)],l1_model.history['loss'],\"-b\",label='Training Loss')\nplt.plot([i for i in range(10)],l1_model.history['val_loss'],color=\"orange\",label=\"Validation Loss\")\nplt.legend(loc=\"upper right\")\nplt.show()","7783f841":"# Plot for loss of training and validation\nplt.plot([i for i in range(10)],l1_model.history['accuracy'],\"-b\",label='Training Accuracy')\nplt.plot([i for i in range(10)],l1_model.history['val_accuracy'],color=\"orange\",label=\"Validation Accuracy\")\nplt.legend(loc=\"upper right\")\nplt.show()","b3bb4e6f":"# From the last file we saw that the best model was with Xe\/Xavier initialization ....\n# But what about regularization\n# creating the model_l1_reg\nmodel_l2_reg=Sequential()\nmodel_l2_reg.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal'))\nmodel_l2_reg.add(MaxPooling2D(pool_size=(2,2)))\nmodel_l2_reg.add(Flatten())\nmodel_l2_reg.add(Dense(3000,activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal',                \n                               kernel_regularizer=tf.keras.regularizers.l2(0.01)))\nmodel_l2_reg.add(Dense(1024,activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal',                \n                               kernel_regularizer=tf.keras.regularizers.l2(0.01)))\nmodel_l2_reg.add(Dense(units=6,activation='softmax',  kernel_initializer='glorot_normal', bias_initializer='glorot_normal'))","59ec78e3":"model_l2_reg.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])","98a4ad59":"l2_model = model_l2_reg.fit(training_set,\n                        steps_per_epoch = 100,\n                        epochs = 10,\n                        validation_data = test_set,\n                        validation_steps = 10)","b52cd81c":"# Plot for loss of training and validation\nplt.plot([i for i in range(10)],l2_model.history['loss'],\"-b\",label='Training Loss')\nplt.plot([i for i in range(10)],l2_model.history['val_loss'],color=\"orange\",label=\"Validation Loss\")\nplt.legend(loc=\"upper right\")\nplt.show()","0f239638":"# Plot for loss of training and validation\nplt.plot([i for i in range(10)],l2_model.history['accuracy'],\"-b\",label='Training Accuracy')\nplt.plot([i for i in range(10)],l2_model.history['val_accuracy'],color=\"orange\",label=\"Validation Accuracy\")\nplt.legend(loc=\"lower right\")\nplt.show()","d724e1d3":"model_l1l2_reg=Sequential()\nmodel_l1l2_reg.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal'))\nmodel_l1l2_reg.add(MaxPooling2D(pool_size=(2,2)))\nmodel_l1l2_reg.add(Flatten())\nmodel_l1l2_reg.add(Dense(3000,activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal',                \n                               kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.001,l2=0.01)))\nmodel_l1l2_reg.add(Dense(1024,activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal',                \n                               kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.001,l2=0.01)))\nmodel_l1l2_reg.add(Dense(units=6,activation='softmax',  kernel_initializer='glorot_normal', bias_initializer='glorot_normal'))","36f0e520":"model_l1l2_reg.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])","4bc3093e":"l1l2_model = model_l1l2_reg.fit(training_set,\n                        steps_per_epoch = 100,\n                        epochs = 10,\n                        validation_data = test_set,\n                        validation_steps = 10)","71c62dd3":"# Plot for loss of training and validation\nplt.plot([i for i in range(10)],l1l2_model.history['loss'],\"-b\",label='Training Loss')\nplt.plot([i for i in range(10)],l1l2_model.history['val_loss'],color=\"orange\",label=\"Validation Loss\")\nplt.legend(loc=\"upper right\")\nplt.show()","ec1e6dc2":"# Plot for loss of training and validation\nplt.plot([i for i in range(10)],l1l2_model.history['accuracy'],\"-b\",label='Training Accuracy')\nplt.plot([i for i in range(10)],l1l2_model.history['val_accuracy'],color=\"orange\",label=\"Validation Accuracy\")\nplt.legend(loc=\"lower right\")\nplt.show()","998f0f0d":"model_dropout=Sequential()\nmodel_dropout.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal'))\nmodel_dropout.add(MaxPooling2D(pool_size=(2,2)))\nmodel_dropout.add(Flatten())\nmodel_dropout.add(Dense(3000,activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal'))\nmodel_dropout.add(Dropout(0.4))\nmodel_dropout.add(Dense(1024,activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal'))\nmodel_dropout.add(Dense(units=6,activation='softmax',  kernel_initializer='glorot_normal', bias_initializer='glorot_normal'))","ff32a63d":"model_dropout.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])","3d2458b1":"dropout_model = model_dropout.fit(training_set,\n                        steps_per_epoch = 100,\n                        epochs = 10,\n                        validation_data = test_set,\n                        validation_steps = 10)","cb77bc23":"# Plot for loss of training and validation\nplt.plot([i for i in range(10)],dropout_model.history['loss'],\"-b\",label='Training Loss')\nplt.plot([i for i in range(10)],dropout_model.history['val_loss'],color=\"orange\",label=\"Validation Loss\")\nplt.legend(loc=\"upper right\")\nplt.show()","9888e6fc":"# Plot for loss of training and validation\nplt.plot([i for i in range(10)],dropout_model.history['accuracy'],\"-b\",label='Training Accuracy')\nplt.plot([i for i in range(10)],dropout_model.history['val_accuracy'],color=\"orange\",label=\"Validation Accuracy\")\nplt.legend(loc=\"lower right\")\nplt.show()","1350d6a9":"model_dropout.summary()","89aa27eb":"dropout_model = model_dropout.fit(training_set,\n                        steps_per_epoch = 100,\n                        epochs = 20,\n                        validation_data = test_set,\n                        validation_steps = 10)","e06bcbff":"def image_prediction(path,model):\n    imm=Image.open(path)\n    imm=imm.resize((64,64))\n    x=np.array(imm)\n    x=np.expand_dims(x,axis=0)\n    classs=model.model.predict_classes(x)\n    l=os.listdir('..\/input\/intel-image-classification\/seg_train\/seg_train')\n    l.sort()\n    return l[classs[0]]","10441770":"# Printing the image to be predicted\nim=Image.open('..\/input\/intel-image-classification\/seg_pred\/seg_pred\/10012.jpg')\nim","711cd23e":"image_prediction('..\/input\/intel-image-classification\/seg_pred\/seg_pred\/10012.jpg',dropout_model)","8b7498db":"# Printing the image to be predicted\nim=Image.open('..\/input\/intel-image-classification\/seg_pred\/seg_pred\/10127.jpg')\nim","f4d62e59":"image_prediction('..\/input\/intel-image-classification\/seg_pred\/seg_pred\/10127.jpg',dropout_model)","2d2015d7":"# Having a look at the data","49db6483":"![](data:image\/jpeg;base64,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)","216a1a3c":"# Difference between L1 and L2 regularization and which was better here and why??","1975592b":"# Trying both L1 and L2 Regularization","69cbe63c":"# Create the testing and training data","b21cc3a5":"# Dropout Regularization\n![](https:\/\/miro.medium.com\/max\/518\/0*EY8R7nS10y5kQzOx)\n\nJust liked the name suggests this regularization technique drops units (i.e. neurons) during the training phase of certain set of neurons which is chosen at random. By \u201cdropping\u201d, I mean these units are not considered during a particular forward or backward pass. More technically, At each training stage, individual nodes are either dropped out of the net with probability 1-p or kept with probability p, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed.","07d0a8b9":"# What is Overfitting and how to solve it ??","b72497a2":"# Different types of regularization and their effect on the result:\n","bb22d65d":"LASSO regression, L1 regularization, includes a hyper-parameter \u03b1 times the sum of the absolute value of the coefficients as penalty term in its cost function, shown below (marked in red):<br>\n![](https:\/\/qphs.fs.quoracdn.net\/main-qimg-6449847b08c91639e5c59242f2f4263f)<br>\nOn the one hand, if we do not apply any penalty (set \u03b1 =0), the above formula turns into a regular OLS regression, which may overfit.\n\nOn the other hand, the model will probably underfit if we apply a very large penalty (or, a large \u03b1 value), because we have falsely penalized all coefficients (the most important ones included).","86ec8efe":"Overfitting is a phenomenon that occurs when a machine learning or statistics model is tailored to a particular dataset and is unable to generalise to other datasets. This usually happens in complex models, like deep neural networks.\nRegularization is a process of introducing additional information in order to prevent overfitting.\n\nHere we will be discussing some of the standard regularization techniques:\n\n* L1 regularization\n* L2 regularization\n* L1-L2 Regularization\n* Dropout Regularization\n\nRegularization is also used to stabilize the estimates especially when there's collinearity in the data.","98c4d5a7":"# Trying out some predictions","5addd218":"### Reference : \n### 1)https:\/\/www.quora.com\/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when <br>\n### 2) https:\/\/medium.com\/@amarbudhiraja\/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5","9de1a479":"# Importing the packages","4d489f89":"Before this file there is a starter file : https:\/\/www.kaggle.com\/accountstatus\/using-keras-cnn-on-intel-data\/comments\n<br>And the second part \"Effect of initializers on a model\" : https:\/\/www.kaggle.com\/accountstatus\/effect-of-weight-initialization-on-a-model","b56821ed":"## Why does dropout regularization work ain't it just randomly dropping neurons??\n\nA fully connected layer occupies most of the parameters, and hence, neurons develop co-dependency amongst each other during training which curbs the individual power of each neuron leading to over-fitting of training data. But dropout makes neurons get dropped randomly making them independent and letting us get better result without overfitting the data .","577b06d3":"# L2 Regularization","f09fb580":"![](https:\/\/i.pinimg.com\/originals\/8c\/40\/05\/8c4005377742272315e792545a9c93df.gif)","bd0998ac":"# Model without regularization","eb442820":"# L1 Regularization","1ce4b80c":"## why to use both L1 and L2 regularization together ??\n\nThe practical advantage, at least in theory, is that you get the best of both worlds. L2 generally beats L1 in terms of accuracy and it is easier to adjust. On the other hand, L1 can deal with sparse feature spaces and helps doing feature selection. So, combining L2 and L1 gets you both accuracy and sparsity.","ac3ee358":"## The best results were given by dropout and we were also able to save our model from overfitting :)","bd65b6a9":"# Training data with Dropout Regularization and Xavier Initialization ","826886e3":"L2 is called regularization for simplicity. Instead of shrinking to zero, L2 regularization slows down as the rate goes towards 0. In each iteration, L2 removes a small percentage of weights and so never converges to 0.<br>\n\n![](https:\/\/qphs.fs.quoracdn.net\/main-qimg-0986b796cfc0a0c8864375a27884c464)","2a50e151":"From a practical standpoint, L1 tends to shrink coefficients to zero whereas L2 tends to shrink coefficients evenly. L1 is therefore useful for feature selection, as we can drop any variables associated with coefficients that go to zero. L2, on the other hand, is useful when you have collinear\/codependent features.\n\nIn our data we weren't trying to remove the features of the image on the other hand we were tryng to figure out which feature of the image was important for classification .\n\nThat is why we can clearly see L2 performed a lot better than the l1 regularization."}}