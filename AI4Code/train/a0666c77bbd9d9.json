{"cell_type":{"a921e2b2":"code","46d678c9":"code","28be0e44":"code","6087e691":"code","c58facee":"code","6ecff5e7":"code","cc094580":"code","81e8b19d":"code","d4223c68":"code","f69763b7":"code","d64eee08":"code","f44e71e3":"code","caf2cfc6":"code","4dc0a226":"code","157b496b":"code","8efdc815":"code","d32a9fdc":"code","bde4dd60":"code","6f2d7357":"code","472de067":"code","196506b7":"code","03f29a99":"code","f8ac7902":"code","b0cdfe91":"code","a0ac5250":"code","b7b48462":"code","e35dcad6":"code","4cf5fc7c":"code","c92a3c14":"code","ed05091d":"markdown","0938c712":"markdown","b2ff73e4":"markdown","0a3551b5":"markdown","2a0b4e83":"markdown","4ac14ca5":"markdown","39ece9f5":"markdown","30c896c3":"markdown","75c09c34":"markdown","fed104c6":"markdown","2221551b":"markdown","6240c8d3":"markdown","54774fb1":"markdown","30516d37":"markdown","88c90c26":"markdown","4f01876d":"markdown","64d5f303":"markdown","7d950493":"markdown","2b3b939f":"markdown","d12c0aaf":"markdown"},"source":{"a921e2b2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport string\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.feature_extraction.text import CountVectorizer\nnltk.download('vader_lexicon')\nfrom sklearn.cluster import KMeans\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom wordcloud import WordCloud,STOPWORDS\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AR,AutoReg\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom nltk.util import ngrams\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport random\nimport pymc3 as pm\nimport theano.tensor as T\nfrom wordcloud import STOPWORDS\nimport warnings\nimport os\nwarnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n                        FutureWarning)\nwarnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',\n                        FutureWarning)\n\nplt.rc('figure',figsize=(17,13))\nplt.rc('figure',autolayout=True)\nplt.style.use('ggplot')\n#sns.set_style('whitegrid')\nsns.set_context('paper',font_scale=2)\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()","46d678c9":"t_data = pd.read_csv('\/kaggle\/input\/all-trumps-twitter-insults-20152021\/trump_insult_tweets_2014_to_2021.csv',index_col=0)\nt_data.date = pd.to_datetime(t_data.date)\nt_data.head(3)","28be0e44":"text_dict = dict()\nfor (tweet,date) in zip(t_data.tweet,t_data.date):\n    text_dict[(tweet,date)] = text_dict.get((tweet,date),0)+1\n    \ntext_dict = list(text_dict.keys())\nf_data = pd.DataFrame(text_dict,columns=['text','date'])\n\nf_data.text =f_data.text.str.lower()\n\n#Remove twitter handlers\nf_data.text = f_data.text.apply(lambda x:re.sub('@[^\\s]+','',x))\n\n#remove hashtags\nf_data.text = f_data.text.apply(lambda x:re.sub(r'\\B#\\S+','',x))\n\n\n# Remove URLS\nf_data.text = f_data.text.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n\n\n#remove all single characters\nf_data.text = f_data.text.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n\n# Substituting multiple spaces with single space\nf_data.text = f_data.text.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n\n\n# Remove all the special characters\nf_data.text = f_data.text.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n\n\n\n","6087e691":"#Vader Setiment Analysis\nsid = SIA()\nf_data['sentiments']           = f_data['text'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\nf_data['Positive Sentiment']   = f_data['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \nf_data['Neutral Sentiment']    = f_data['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\nf_data['Negative Sentiment']   = f_data['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\nf_data.drop(columns=['sentiments'],inplace=True)\n","c58facee":"\n\n#Number of Words\nf_data['Number_Of_Words'] = f_data.text.apply(lambda x:len(x.split(' ')))\n#Average Word Length\nf_data['Mean_Word_Length'] = f_data.text.apply(lambda x:np.round(np.mean([len(w) for w in x.split(' ')]),2) )","6ecff5e7":"plt.subplot(2,1,1)\nplt.title('Distriubtion Of Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\nsns.kdeplot(f_data['Negative Sentiment'],label='Negative Sentiment',lw=2.5)\nsns.kdeplot(f_data['Positive Sentiment'],label='Positive Sentiment',lw=2.5)\nsns.kdeplot(f_data['Neutral Sentiment'], label='Neutral Sentiment',lw=2.5 )\nplt.legend()\nplt.subplot(2,1,2)\nplt.title('CDF Of Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\nsns.kdeplot(f_data['Negative Sentiment'],cumulative=True ,label='Negative Sentiment',lw=2.5)\nsns.kdeplot(f_data['Positive Sentiment'],cumulative=True ,label='Positive Sentiment',lw=2.5)\nsns.kdeplot(f_data['Neutral Sentiment'],cumulative=True  ,label='Neutral Sentiment' ,lw=2.5)\nplt.xlabel('Sentiment Value',fontsize=19)\nplt.legend()\nplt.show()","cc094580":"#Sorting And Feature Engineering\nf_data = f_data.sort_values(by='date')\nft_data=f_data.copy()\nft_data['date'] = pd.to_datetime(f_data['date']).dt.date\n\nft_data['year']         = pd.DatetimeIndex(ft_data['date']).year\nft_data['month']        = pd.DatetimeIndex(ft_data['date']).month\nft_data['day']          = pd.DatetimeIndex(ft_data['date']).day\nft_data['day_of_year']  = pd.DatetimeIndex(ft_data['date']).dayofyear\nft_data['quarter']      = pd.DatetimeIndex(ft_data['date']).quarter\nft_data['season']       = ft_data.month%12 \/\/ 3 + 1","81e8b19d":"f_data=f_data.reset_index().drop(columns=['index'])\npartitions = []\npartitions.append(f_data.loc[44:np.round(len(f_data)\/3,0)-1,:])\npartitions.append(f_data.loc[np.round(len(f_data)\/3,0):2*int(len(f_data)\/3)-1,:])\npartitions.append(f_data.loc[2*np.round(len(f_data)\/3,0):3*int(len(f_data)\/3)-1,:])\n\n\n\nneg_part_means =[]\nneg_part_std   =[]\npos_part_means =[]\npos_part_std   =[]\nfor part in partitions:\n    neg_part_means.append(part['Negative Sentiment'].mean())\n    neg_part_std.append(part['Negative Sentiment'].std())\n    pos_part_means.append(part['Positive Sentiment'].mean())\n    pos_part_std.append(part['Positive Sentiment'].std())\n    \nres_df = pd.DataFrame({'Positive Sentiment Mean':pos_part_means,'Negative Sentiment Mean':neg_part_means,'Positive Sentiment SD':pos_part_std,'Negative Sentiment SD':neg_part_std},\n                     index = [f'Partition_{i}' for i in range(1,4)])\n\n\ndef highlight_greater(x):\n    temp = x.copy()\n    temp = temp.round(0).astype(int)\n    m1 = (temp['Partition_1_Mean'] == temp['Partition_2_Mean'])\n    m2 = (temp['Partition_1_SD'] == temp['Partition_2_SD'])\n    m3 = (temp['Partition_1_Mean'] < temp['Partition_2_Mean']+3) & (temp['Partition_1_Mean'] > temp['Partition_2_Mean']-3)\n    m4 = (temp['Partition_1_SD'] < temp['Partition_2_SD']+3) & (temp['Partition_1_SD'] > temp['Partition_2_SD']-3)\n\n    df1 = pd.DataFrame('background-color: ', index=x.index, columns=x.columns)\n    #rewrite values by boolean masks\n    df1['Partition_1_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_2_Mean'])\n    df1['Partition_1_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_2_Mean'])\n    df1['Partition_1_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_Mean'])\n\n    df1['Partition_1_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_2_SD'])\n    df1['Partition_1_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_2_SD'])\n    df1['Partition_1_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_SD'])\n\n    return df1\n\n\n\n#res_df.style.apply(highlight_greater,axis=None)\nres_df = res_df.T\nres_df = pd.DataFrame(res_df.values,columns=res_df.columns,index=['Positive Sentiment','Negative Sentiment','Positive Sentiment','Negative Sentiment'])\nres_df = pd.concat([res_df.iloc[:2,:],res_df.iloc[2:,:]],axis=1)\nres_df.columns = ['Partition_1_Mean','Partition_2_Mean','Partition_3_Mean','Partition_1_SD','Partition_2_SD','Partition_3_SD']\nres_df.style.apply(highlight_greater,axis=None)","d4223c68":"fig = make_subplots(rows=3, cols=2)\n\nfor idx,prt in enumerate(partitions):\n    fig.add_trace(\n    go.Scatter(x=prt['date'], y=prt['Positive Sentiment'],name=f'Positive Part {idx+1}'),\n    row=idx+1, col=1)\n    fig.add_trace(\n    go.Scatter(x=prt['date'], y=prt['Negative Sentiment'],name=f'Negative Part {idx+1}'),\n    row=idx+1, col=2)\n\nfig.update_layout(height=600, width=900, title_text=\"Distibution Of Daily Sentiments Over Our Time Line For Each Partition\")\nfig.show()","f69763b7":"fig = make_subplots(rows=4, cols=2, subplot_titles=('Observed Pos', 'Observed Neg', 'Trend Pos','Trend Neg','Seasonal Pos','Seasonal Neg','Residual Pos','Residual Neg'))\nscope = 'month'\nb_date_mean = ft_data.groupby(by=scope).mean().reset_index()\n\n\nlbl = ['Positive','Negative']\n\nfor idx,column in enumerate(['Positive Sentiment','Negative Sentiment']):\n    res = seasonal_decompose(b_date_mean[column], period=3, model='additive', extrapolate_trend='freq')\n    \n    fig.add_trace(\n    go.Scatter(x=b_date_mean[scope], y=res.observed,name='{} Observed'.format(lbl[idx])),\n    row=1, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=b_date_mean[scope], y=res.trend,name='{} Trend'.format(lbl[idx])),\n    row=2, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=b_date_mean[scope], y=res.seasonal,name='{} Seasonal'.format(lbl[idx])),\n    row=3, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=b_date_mean[scope], y=res.resid,name='{} Residual'.format(lbl[idx])),\n    row=4, col=idx+1)\n            \nfig.update_layout(height=600, width=900, title_text=\"Decomposition Of Our Monthly Average Sentiments into Trend,Level,Seasonality and Residuals\")\nfig.show()","d64eee08":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\n\nax[0].set_title('Positive Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(b_date_mean['Positive Sentiment'],ax=ax[0],lw=3)\nax[1].set_title('Negative Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(b_date_mean['Negative Sentiment'],ax=ax[1],color='tab:red',lw=3)\nplt.show()","f44e71e3":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\nax[0].set_ylim(-1.1,1.1)\nax[1].set_ylim(-1.1,1.1)\n\nplot_pacf(b_date_mean['Negative Sentiment'],lags=5, ax=ax[0],title='Partial Autocorrelation Negative')\nplot_pacf(b_date_mean['Positive Sentiment'],lags=5, ax=ax[1],color='tab:blue',title='Partial Autocorrelation Positive')\nplt.show()","caf2cfc6":"arma_5 = SARIMAX(endog=b_date_mean['Negative Sentiment'],order=(0,0,5)).fit()\n","4dc0a226":"fig = plt.figure(figsize=(16,9))\nfig = arma_5.plot_diagnostics(fig=fig, lags=5)","157b496b":"predicted_AR_1 = arma_5.predict(1)\n\noutput = pd.DataFrame({'Prediction':predicted_AR_1,'Actual':b_date_mean['Positive Sentiment']})\n\nfig = make_subplots(\n    rows=3, cols=2,subplot_titles=('','Actual','Predictions','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"table\",\"rowspan\": 3}     ,{\"type\": \"scatter\"}] ,\n           [None                               ,{\"type\": \"scatter\"}]            ,           \n           [None                               ,{\"type\": \"scatter\"}]                           \n          ]\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=b_date_mean[scope],\n        y=output[\"Actual\"],\n        mode=\"lines+markers\",\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=b_date_mean[scope],\n        y=output[\"Prediction\"],\n        mode=\"lines+markers\",\n    ),\n    row=2, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"]-output[\"Actual\"],\n        mode=\"lines+markers\",\n    ),\n    row=3, col=2\n)\n\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['Prediction','Actual'],\n            font=dict(size=10),\n            align=\"left\"\n        ),\n        cells=dict(\n            values=[output[k].tolist() for k in output.columns],\n            align = \"left\")\n    ),\n    row=1, col=1\n)\n\n\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=(output[\"Prediction\"]-output[\"Actual\"]).mean(), x1=len(output[\"Prediction\"]), y1=(output[\"Prediction\"]-output[\"Actual\"]).mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x3', \n        yref='y3'\n)\n\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"Prediction Evaluation\",\n)\n\nfig.show()","8efdc815":"b_date_mean = ft_data.groupby(by=scope).mean().reset_index()\nb_date_std = ft_data.groupby(by=scope).std().reset_index()\n\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=(f'{scope}ly Average Positive Sentiment',  f'{scope}ly Average Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean[scope], y=b_date_mean['Positive Sentiment'],name='Positive Sentiment Mean'),\n    row=1, col=1\n)\n\n    \n#positive mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean[scope].values[0], y0=b_date_mean['Positive Sentiment'].mean(), x1=b_date_mean[scope].values[-1], y1=b_date_mean['Positive Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean'\n)\n\nfig.add_annotation(x=b_date_mean[scope].values[3], y=b_date_mean['Positive Sentiment'].mean(),\n            text=r\"$\\mu : {:.2f}$\".format(b_date_mean['Positive Sentiment'].mean()),\n            showarrow=True,\n            arrowhead=3,\n            yshift=10)\n\n\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean[scope], y=b_date_mean['Negative Sentiment'],name='Negative Sentiment Mean'),\n    row=2, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean[scope].values[0], y0=b_date_mean['Negative Sentiment'].mean(), x1=b_date_mean[scope].values[-1], y1=b_date_mean['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\n# fig.add_annotation(x=b_date_mean[scope].values[3], y=b_date_mean['Negative Sentiment'].mean(),\n#             text=r\"$\\mu : {:.2f}$\".format(b_date_mean['Negative Sentiment'].mean()),\n#             showarrow=True,\n#             arrowhead=3,\n#             yshift=10,\n#             xref='x2', \n#             yref='y2')\n\n\n\n# fig.add_annotation(x=b_date_mean[scope].values[5], y=b_date_mean['Negative Sentiment'].mean()+0.01,\n#             text=r\"Start Of Decline\",\n#             showarrow=True,\n#             arrowhead=6,\n#             yshift=10,\n#             xref='x2', \n#             yref='y2')\n\n# fig.add_annotation(x=b_date_mean[scope].values[15], y=.024,\n#             text=r\"Start Of Incline\",\n#             showarrow=True,\n#             arrowhead=6,\n#             yshift=10,\n#             xref='x2', \n#             yref='y2')\n\nfig['layout']['xaxis2']['title'] = scope\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Average Change With Time\")\nfig.show()","d32a9fdc":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=(f'{scope}ly Deviation in Positive Sentiment',  f'{scope}ly Deviation in Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=b_date_std[scope], y=b_date_std['Positive Sentiment'],name='Positive Sentiment SD'),\n    row=1, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_std[scope].values[0], y0=b_date_std['Negative Sentiment'].mean(), x1=b_date_std[scope].values[-1], y1=b_date_std['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_std[scope].values[0], y0=b_date_std['Positive Sentiment'].mean(), x1=b_date_std[scope].values[-1], y1=b_date_std['Positive Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x1', \n        yref='y1'\n)\n\n\nfig.add_trace(\n    go.Scatter(x=b_date_std[scope], y=b_date_std['Negative Sentiment'],name='Negative Sentiment SD'),\n    row=2, col=1\n)\n\nfig['layout']['xaxis2']['title'] = scope\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Deviation Change With Time\")\nfig.show()","bde4dd60":"b_date_count = ft_data.groupby(by=scope).count().reset_index()\nb_date_count = b_date_count.rename(columns={ 'date':f'Tweets Per {scope}'})\nfig = ex.line(b_date_count,x=scope,y=f'Tweets Per {scope}')\n\n# fig.add_annotation(x=b_date_mean['date'].values[15], y=.024,\n#             text=r\"Start Of Incline\",\n#             showarrow=True,\n#             arrowhead=6,\n#             yshift=10)\n\n\nfig.add_shape(type=\"line\",\n    x0=b_date_count[scope].values[0], y0=b_date_count['Negative Sentiment'].mean(), x1=b_date_count[scope].values[-1], y1=b_date_count['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n)\n\nfig.update_traces(mode=\"markers+lines\")\nfig.update_layout(hovermode=\"x unified\")\n\n\nfig.update_layout(title=f'<b>{scope}ly Tweet Count<b>')\nfig.show()","6f2d7357":"june = ft_data.query('month==7')\noctober = ft_data.query('month==10')\n\noctober_text = ' '.join(october.text)\njune_text    = ' '.join(june.text)\noctober_text = ' '.join([i for i in october_text.split(' ') if len(i) > 2])\njune_text    = ' '.join([i for i in june_text.split(' ') if len(i) > 2])\n\npwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(october_text)\nnwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(june_text)\n\nplt.subplot(1,2,1)\nplt.title('Common Words Among October Tweets',fontsize=16,fontweight='bold')\nplt.imshow(pwc)\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.title('Common Words Among June Tweets',fontsize=16,fontweight='bold')\nplt.imshow(nwc)\nplt.axis('off')\n\nplt.show()","472de067":"ag_june = nltk.FreqDist(june_text.split(' '))\nag_october = nltk.FreqDist(october_text.split(' '))\n\n\ntoken=nltk.word_tokenize(' '.join(i for i in june_text.split(' ') if i not in STOPWORDS and i.isalpha()))\njune_bigram=ngrams(token,2)\njune_trigram=ngrams(token,3)\n\ntoken=nltk.word_tokenize(' '.join(i for i in october_text.split(' ') if i not in STOPWORDS and i.isalpha()))\noctober_bigram=ngrams(token,2)\noctober_trigram=ngrams(token,3)\n\n\njune_bdict = dict()\nfor i in list(june_bigram):\n    june_bdict[i] = june_bdict.get(i,0)+1\n    \njune_bdict = {k: v for k, v in sorted(june_bdict.items(), key=lambda item: item[1],reverse=True)}\n    \noctober_bdict = dict()\nfor i in list(october_bigram):\n    october_bdict[i] = october_bdict.get(i,0)+1\noctober_bdict = {k: v for k, v in sorted(october_bdict.items(), key=lambda item: item[1],reverse=True)}\n\n\njune_tridict = dict()\nfor i in list(june_trigram):\n    june_tridict[i] = june_tridict.get(i,0)+1\n    \njune_tridict = {k: v for k, v in sorted(june_tridict.items(), key=lambda item: item[1],reverse=True)}\n    \noctober_tridict = dict()\nfor i in list(october_trigram):\n    october_tridict[i] = october_tridict.get(i,0)+1\noctober_tridict = {k: v for k, v in sorted(october_tridict.items(), key=lambda item: item[1],reverse=True)}\n\n\ndef calculate_tri_prob(tri,firt_order,second_order,third_order):\n    fw,sw,tw = tri\n    p_fw=firt_order[fw]\n    p_fwsw = second_order[(fw,sw)]\/p_fw\n    p_fwswtw = third_order[(fw,sw,tw)]\/p_fw\n    return p_fwswtw\n\ndef calculate_sentence_prob(sentence,firt_order,second_order):\n    p_first = firt_order[sentence[0]]\/len(firt_order.keys())\n    p_second_first = second_order[(sentence[0],sentence[1])]\/len(second_order)\n    p_thid_second = second_order[(sentence[1],sentence[2])]\/len(second_order)\n    return p_first*p_second_first*p_thid_second\n\ntop_10_tri_june = []\n\nfor key in list(june_tridict.keys())[:15]:\n    top_10_tri_june.append((key,calculate_tri_prob(key,ag_june,june_bdict,june_tridict)))\n    \n    \njune_tri_df = pd.DataFrame(top_10_tri_june,columns=['Sentence','Probability Given First Words'])\n\njune_tri_df['First Word'] = june_tri_df.Sentence.apply(lambda x: x[0])\njune_tri_df['Sentence Probability-Markov Assumption'] = june_tri_df.Sentence.apply(lambda x: calculate_sentence_prob(x,ag_june,june_bdict))\njune_tri_df = june_tri_df[['First Word','Sentence','Probability Given First Words','Sentence Probability-Markov Assumption']]\njune_tri_df.Sentence=june_tri_df.Sentence.apply(lambda x: ' '.join(x))\njune_tri_df.style.background_gradient(cmap='coolwarm')\n","196506b7":"top_10_tri_october = []\n\nfor key in list(october_tridict.keys())[:15]:\n    top_10_tri_october.append((key,calculate_tri_prob(key,ag_october,october_bdict,october_tridict)))\n    \n    \noctober_tri_df = pd.DataFrame(top_10_tri_october,columns=['Sentence','Probability Given First Words'])\n\noctober_tri_df['First Word'] = october_tri_df.Sentence.apply(lambda x: x[0])\noctober_tri_df               = october_tri_df[['First Word','Sentence','Probability Given First Words']]\noctober_tri_df.Sentence=october_tri_df.Sentence.apply(lambda x: ' '.join(x))\noctober_tri_df.style.background_gradient(cmap='coolwarm')\n","03f29a99":"plt.subplot(2,1,1)\nax = sns.barplot(x=[' '.join(i) for i in list(june_bdict.keys())[:10]],y=list(june_bdict.values())[:10])\nax.set_title('Most Common Two Word Sentences In June')\nax.set_xlabel('Sentence')\nax.set_ylabel('Frequency')\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.subplot(2,1,2)\nax = sns.barplot(x=[' '.join(i) for i in list(october_bdict.keys())[:10]],y=list(october_bdict.values())[:10])\nax.set_title('Most Common Two Word Sentences In October')\nax.set_xlabel('Sentence')\nax.set_ylabel('Frequency')\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","f8ac7902":"plt.title('Distribution Of Positive Sentimnet',fontweight='bold',fontsize=20)\nax= sns.kdeplot(f_data['Positive Sentiment'],label='Positive Sentiment',lw=3)\nax= sns.distplot(f_data['Positive Sentiment'],label='Positive Sentiment')\n\nplt.legend(prop=dict(size=23))\nplt.show()","b0cdfe91":"\nwith pm.Model():\n    #Probability Of Belonging To Cluster 1\n    p1 = pm.Uniform('p',0,1)\n    #Probability Of Belonfing To Cluster 2\n    p2 = 1-p1\n    p = T.stack([p1, p2])\n    assignment = pm.Categorical(\"assignment\", p, \n                                shape=f_data.shape[0],\n                                testval=np.random.randint(0, 2, f_data.shape[0]))\n    \n    sds = pm.Uniform('sds',0,1,shape=2)\n    centers = pm.Normal(\"centers\", \n                        mu=np.array([0, 0.1]), \n                        sd=np.array([0.3, 0.3]), \n                        shape=2)\n    center_i = pm.Deterministic('center_i', centers[assignment])\n    sd_i = pm.Deterministic('sd_i', sds[assignment])\n    observations = pm.Normal(\"obs\", mu=center_i, sd=sd_i, observed=f_data['Positive Sentiment'])\n    \n    step1 = pm.Metropolis(vars=[p, sds, centers])\n    step2 = pm.ElemwiseCategorical(vars=[assignment])\n    trace = pm.sample(5000, step=[step1, step2])","a0ac5250":"std_trace = trace[\"sds\"][8000:]\ncenter_trace = trace[\"centers\"][8000:]\n\ncolors = [\"#348ABD\", \"#A60628\"] if center_trace[-1, 0] > center_trace[-1, 1] \\\n    else [\"#A60628\", \"#348ABD\"]\n_i = [1, 2, 3, 4]\nfor i in range(2):\n    plt.subplot(2, 2, _i[2 * i])\n    plt.title(\"Posterior of center of cluster %d\" % i)\n    plt.hist(center_trace[:, i], color=colors[i],\n             histtype=\"stepfilled\")\n\n    plt.subplot(2, 2, _i[2 * i + 1])\n    plt.title(\"Posterior of standard deviation of cluster %d\" % i)\n    plt.hist(std_trace[:, i], color=colors[i],\n             histtype=\"stepfilled\")","b7b48462":"import matplotlib as mpl\ncmap = mpl.colors.LinearSegmentedColormap.from_list(\"BMH\", colors)\nassign_trace = trace[\"assignment\"]\nplt.subplot(2,1,1)\n\nplt.scatter(f_data['Positive Sentiment'], 1 - assign_trace.mean(axis=0), cmap=cmap,\n        c=assign_trace.mean(axis=0), s=50)\nplt.ylim(-0.05, 1.05)\n#plt.xlim(35, 300)\nplt.title(\"Probability of data point belonging to cluster 0\")\nplt.ylabel(\"probability\")\nplt.xlabel(\"value of data point\");\n\nplt.subplot(2,1,2)\nplt.scatter(f_data['Positive Sentiment'], 1 - assign_trace.mean(axis=0), cmap=cmap,\n        c=assign_trace.mean(axis=0), s=50)\nplt.ylim(-0.001, 0.003)\n#plt.xlim(35, 300)\nplt.title(\"Probability of data point belonging to cluster 1\")\nplt.ylabel(\"probability\")\nplt.xlabel(\"value of data point\");","e35dcad6":"import scipy.stats as stats\nnorm = stats.norm\nx = np.linspace(0, 1, 50)\nposterior_center_means = center_trace.mean(axis=0)\nposterior_std_means = std_trace.mean(axis=0)\nposterior_p_mean = trace[\"p\"].mean()\ndata = f_data['Positive Sentiment']\n\nplt.hist(data, bins=20, histtype=\"step\", density=True, color=\"k\",\n     lw=2, label=\"histogram of data\")\ny = posterior_p_mean * norm.pdf(x, loc=posterior_center_means[0],\n                                scale=posterior_std_means[0])\nplt.plot(x, y, label=\"Cluster 0 (using posterior-mean parameters)\", lw=3)\nplt.fill_between(x, y, color=colors[1], alpha=0.3)\n\ny = (1 - posterior_p_mean) * norm.pdf(x, loc=posterior_center_means[1],\n                                      scale=posterior_std_means[1]*10000)\nplt.plot(x, (y-y.mean())\/y.std(), label=\"Cluster 1 (using posterior-mean parameters)\", lw=3)\nplt.fill_between(x, (y-y.mean())\/y.std(), color=colors[0], alpha=0.3)\n\nplt.legend(loc=\"upper left\")\nplt.title(\"Visualizing Clusters using posterior-mean parameters\");","4cf5fc7c":"NUMBER_OF_COMPONENTS = 450\n\nCVZ = CountVectorizer()\nSVD = TruncatedSVD(NUMBER_OF_COMPONENTS)\n\ntext_data = f_data.text.copy()\ntext_data = text_data.apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS and len(word) > 1]).strip())\n\nstemmer= PorterStemmer()\nlemmatizer=WordNetLemmatizer()\n\ntext_data = text_data.apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\ntext_data = text_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n\nC_vector = CVZ.fit_transform(text_data)\n\n\npc_matrix = SVD.fit_transform(C_vector)\n\nevr = SVD.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\", \n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(     title='{:.2f}% of the Tweet Text Variance Can Be Explained Using {} Words'.format(np.sum(evr)*100,NUMBER_OF_COMPONENTS))\nfig.show()","c92a3c14":"best_fearures = [[CVZ.get_feature_names()[i],SVD.components_[0][i]] for i in SVD.components_[0].argsort()[::-1]]\nworddf = pd.DataFrame(np.array(best_fearures[:450])[:,0]).rename(columns={0:'Word'})\nworddf['Explained Variance'] =  np.round(evr*100,2)\nworddf['Explained Variance'] =worddf['Explained Variance'].apply(lambda x:str(x)+'%')\napp = []\nfor word in worddf.Word:\n    total_count = 0\n    for tweet in text_data:\n        if tweet.find(word)!= -1:\n            total_count+=1\n    app.append(total_count)\nworddf['Appeared_On_X_Tweets'] = app\nworddf\n\nfig = go.Figure()\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['<b>Word<b>',\"<b>Explaines X% of Variance<b>\",'<b>Appeared On X Tweets<b>'],\n            font=dict(size=19,family=\"Lato\"),\n            align=\"center\"\n        ),\n        cells=dict(\n            values=[worddf[k].tolist() for k in ['Word',\"Explained Variance\",'Appeared_On_X_Tweets']],\n            align = \"center\")\n    ),\n    \n)\n\nfig.show()","ed05091d":"<a id=\"2\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Text Preprocessing<\/h1>\n","0938c712":"**Observation**:  It seems that the peaks may be due to Trump's political campaigns; we can see that the October word-cloud contains \"Biden\" as one of the most common words; similarly, the word-cloud of June contains \"Hillary.\"","b2ff73e4":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Vader Sentiment Analysis<\/h1>\n","0a3551b5":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Above is a trigram of 15 sentences that start with one of the top 10 words  in June  and the probability that the sentence will appear in Trump's Tweets given he starts the setntce with the one the top 10 first words.<\/span><\/p>\n<p><br><\/p>","2a0b4e83":"**Observation**: It seems that the mean and standard deviation are constant in our partitions which may suggest the sentiments are stationary over time but still, it is not enough evidence; we will keep testing the stationarity.","4ac14ca5":"<a id=\"4\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h1>\n","39ece9f5":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Text Decomposition Analysis<\/h1>\n\n","30c896c3":"**Observation**:  It appears that there are significantly more tweets with insults in June and October; we will investigate those months to try and answer the question, what happened in those months which caused the increased amount of tweets.\n","75c09c34":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Probabilistic Inference<\/h1>\n\n","fed104c6":"**Observation**:  We see that naively using an MA(5) model to try and predict the seasonal behavior we hypothesized earlier came out to be far from perfect; if it would be beneficial to come to such insight, it might be better to try and generate a more sophisticated ARIMA or GARCH model.\n","2221551b":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Naive Feature Engineering<\/h1>\n","6240c8d3":"**Observation**:  As we saw earlier, our sentiments are bimodal, and it is of interest to understand what generates the bimodality and what is unique about each underlying group.\n\n<p>In order to get a better understanding, we will make the assumption that our data consists of two clusters; we will assume that each point (Positive sentiment of a tweet) belongs to either cluster 1 or cluster 2 with probability $P_1$ and $1-P_1$ respectfully.\nOur goal will be to create a posterior space where the centers or our distribution modes may reside together with the standard deviations.\nWe will define $P_1$ to be a random uniform variable and try and use MCMC to converge on potential $P_1$ value while exploring the possible centers ($\\mu_i$) and SD's ($\\sigma_i$). <p>\n","54774fb1":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Above is a trigram of 15 sentences that start with one of the top 10 words  in October  and the probability that the sentence will appear in Trump's Tweets given he starts the setntce with the one the top 10 first words.<\/span><\/p>\n<p><br><\/p>","30516d37":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Modeling Future Negative Sentiment<\/h1>\n\n","88c90c26":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Libraires And Utilities<\/h1>\n","4f01876d":"**Observation**:  We see that after sampling from our posterior space, thousand of samples which are close to are real unknown parameters that the center of the first cluster is 0.1 with high confidence and the center of the second cluster is 0 also with high confidence, as for the standard deviation we have a lower confidence interval, but we can see that most likely the standard deviation of the first cluster is between 0.082 and 0.086 , as for the second cluster the standard deviation is between w.25 * 1e-10 and 2.45*1e-10 ","64d5f303":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Time Based Analysis<\/h1>\n","7d950493":"**Observation**: Notice that all our sentiments follow a bimodal distribution, suggesting that there might be two underlying groups in our dataset; we will indeed explore and try to understand the two modes better later in our analysis.","2b3b939f":"**Observation**: when looking at the overall average monthly sentiment in Trump's tweets, we see that the negative sentiment tends to drops towards December and the positive sentiment tends to increase around December on average.","d12c0aaf":"**Observation**: Looking at our ACF and PACF plots, we see no significant direct correlations between the data and lagged version of itself except the indirect correlation observed in the PACF with the fifth lag.\nBeing able to predict a month's sentiment based on the previous months may indicate a seasonal behavior among Trump's tweets.\n"}}