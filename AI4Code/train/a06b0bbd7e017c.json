{"cell_type":{"8c34313f":"code","587d0d97":"code","aa7fcd67":"code","2f968c6c":"code","27bf0180":"code","9d5fb20c":"code","a8b11e00":"code","5d4f7111":"code","8288630b":"code","66e3d088":"code","8ad5be14":"code","56efdf5f":"code","6b9dd5b2":"code","4941c86f":"code","aa1e24f4":"code","6197fbd7":"code","935fb80b":"code","67ecb76c":"code","54602983":"code","1b33e2d6":"code","8a02fc95":"code","cb9b6dac":"code","45b4ba1d":"code","8312af56":"code","8d8c27c3":"code","db2f782b":"code","acbed079":"code","dfa25175":"markdown","239f877d":"markdown","c971f140":"markdown","b0cd97c4":"markdown","fd52b212":"markdown","13281a3d":"markdown","14a2028e":"markdown","dd6338c4":"markdown","d792fbc0":"markdown","9bf798f8":"markdown","21c6cdd3":"markdown","c5936405":"markdown","2ec05cca":"markdown","b974d6ff":"markdown","d09df90e":"markdown","34c05337":"markdown","ac665c99":"markdown"},"source":{"8c34313f":"import numpy as np \nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC, SVR\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n                              AdaBoostClassifier)\nfrom sklearn.tree import DecisionTreeClassifier\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","587d0d97":"dataset=pd.read_csv(\"..\/input\/faults.csv\")\ndataset[0:6]","aa7fcd67":"headers = list(dataset.columns.values)\nheaders_input = headers[:-7]\nheaders_output = headers[-7:]\nprint('features for input X:', headers_input)\nprint('classes for output Y:', headers_output)","2f968c6c":"input_x = dataset[headers_input]\noutput_y = dataset[headers_output]","27bf0180":"min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\nnp_scaled = min_max_scaler.fit_transform(input_x)\nheaders_27 = list(input_x.columns.values)\ninput_x_27 = pd.DataFrame(np_scaled)\ninput_x_27.columns = headers_27\ninput_x_27[0:3]","9d5fb20c":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ntargets=(output_y.iloc[:,:]==1).idxmax(1)\nprint(targets.value_counts())\nY=le.fit_transform(targets)\nprint(len(Y))","a8b11e00":"X_train_27, X_test_27, y_train_27, y_test_27 = train_test_split(input_x_27, Y, test_size=0.3)","5d4f7111":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train_27,y_train_27)\nheaders_norm = list(X_train_27.columns.values)\nfeature_imp = pd.Series(clf.feature_importances_,index=headers_norm).sort_values(ascending=False)\n# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","8288630b":"# Create correlation matrix\ncorr_matrix = input_x.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\nprint(to_drop)","66e3d088":"plt.matshow(input_x.corr())\nplt.xticks(range(len(input_x.columns)), input_x.columns)\nplt.yticks(range(len(input_x.columns)), input_x.columns)\nplt.colorbar()\nplt.show()","8ad5be14":"input_x = input_x.drop(input_x[to_drop], axis=1)\ninput_x[0:6]","56efdf5f":"input_x.var()","6b9dd5b2":"numeric = input_x\nvar = numeric.var()\nnumeric = numeric.columns\nvariable = [ ]\nfor i in range(0,len(var)):\n    if var[i]>=10:   #setting the threshold as 10%\n        variable.append(numeric[i+1])","4941c86f":"variable","aa1e24f4":"# important features according to Random Forest\nprint(feature_imp)","6197fbd7":"print(len(input_x.iloc[0,:]))","935fb80b":"min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\nnp_scaled = min_max_scaler.fit_transform(input_x)\ninput_x_norm = pd.DataFrame(np_scaled)\nheaders_22= list(input_x.columns.values)\ninput_x_norm.columns = headers_22\ninput_x_norm[0:6]","67ecb76c":"# Import train_test_split function\nX_train, X_test, y_train, y_test = train_test_split(input_x_norm, Y, test_size=0.3)","54602983":"# Parameters\nn_classes = 7\nn_estimators = 100\nRANDOM_SEED = 13  # fix the seed on each iteration\n\nnames = ['DecisionTreeClassifier', 'RandomForestClassifier', 'ExtraTreesClassifier', 'AdaBoostClassifier']\nmodels = [DecisionTreeClassifier(max_depth=None),\n          RandomForestClassifier(n_estimators=n_estimators),\n          ExtraTreesClassifier(n_estimators=n_estimators),\n          AdaBoostClassifier(DecisionTreeClassifier(max_depth=None),\n                             n_estimators=n_estimators)]\nfor counter, model in enumerate(models):\n    # Train\n    model.fit(X_train, y_train)\n    y_pred=model.predict(X_test)\n    # Model Accuracy, how often is the classifier correct?\n    print(\"Accuracy \" + names[counter] + \":\",metrics.accuracy_score(y_test, y_pred))","1b33e2d6":"input_x_NN = dataset[headers_input]\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\nnp_scaled = min_max_scaler.fit_transform(input_x_NN)\ndf_normalized = pd.DataFrame(np_scaled)\ndf_normalized.columns = headers_input\ndf_normalized[0:6]","8a02fc95":"#One Hot Encode our Y:\nfrom sklearn.preprocessing import LabelBinarizer\nencoder = LabelBinarizer()\nY_one_hot = encoder.fit_transform(Y)\nprint(Y_one_hot)","cb9b6dac":"from imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nimport seaborn as sns\nros = RandomOverSampler(random_state=0)\nX = input_x_norm #df_normalized\nros.fit(X, Y)\nX_resampled, y_resampled = ros.fit_sample(X, Y)\nprint('Amount of elements before:', len(X))\nprint('Amount of elements after:', len(X_resampled))","45b4ba1d":"unique, counts = np.unique(y_resampled, return_counts=True)\ndict(zip(unique, counts))","8312af56":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,\n                                                 Y_one_hot,\n                                                 test_size = 0.3,#%70 train, 30% test\n                                                 random_state = 3)","8d8c27c3":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\n\nmodel.add(Dense(64, activation='relu', input_dim=22))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(7, activation='softmax'))\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=sgd,\n              metrics=['accuracy'])\n\nmodel.fit(X_train, y_train,\n          epochs=2000,\n          batch_size=128)","db2f782b":"score = model.evaluate(X_test, y_test, batch_size=128)\nprint(score)\nprint(model.metrics_names)","acbed079":"Y_predicted = model.predict(X_test, batch_size=32, verbose=0)\npercent = 0\nfor i in range(0, len(Y_predicted)):\n    class_id_predicted = np.argmax(Y_predicted[i])\n    class_id_real = np.argmax(y_test[i])\n    if class_id_predicted == class_id_real: \n        percent += 1\nprint('val accuracy: ', percent\/len(Y_predicted))","dfa25175":"We will drop values which are not important based on dimensionality reduction: high Correlation filter","239f877d":"The accuracy on test set:","c971f140":"Value of Y is number between [0;6], which represent each class:","b0cd97c4":"Drop features that have correlation with other features as they are not important according to Random Forest feature selection.","fd52b212":"Decision Trees, Random Forest Classifier, ExtraTrees, AdaBoost:","13281a3d":"We will compare distribution of each classes.  ","14a2028e":"Multilayer perceptron: ","dd6338c4":"Extra Trees Classifier is the most accurate algorithm","d792fbc0":"Low variance filter shows features which are important according to Random Forest feature selection, so we will not drop  them. ","9bf798f8":"Split data to train and test:","21c6cdd3":"The most important features accoding to Random Forest are: Length_of_Conveyer, LogOfAreas, Pixels_Areas, Steel_Plate_Thickness, Sum_of_Luminosity, Log_X_Index, X_Maximum. ","c5936405":"We will create more samples for the class, which has less amount of samples:","2ec05cca":"We will drop values further based on dimensionality reduction: low variance filter.\nWe will check variations of each columns, and drop some features wich have less variation.","b974d6ff":"We increased the amount of classes, so we have 4711 elements instead of 1941, each class has 673 elements ","d09df90e":"We find all the columns, which values varies less then 10%:","34c05337":"We create one-hot encoding for the output y:","ac665c99":"Which features are not important"}}