{"cell_type":{"3a02b860":"code","0a53f339":"code","0e31082a":"code","d0136873":"code","636c3d80":"code","9e511bfa":"code","037885df":"code","a6abb049":"code","a531a06b":"code","6fe60fe7":"code","e23b6047":"code","7ca7e86e":"code","7de600da":"code","c38ffc29":"code","42536988":"code","c5d5a0d1":"code","c75bb0c5":"code","53a8b34f":"markdown"},"source":{"3a02b860":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.losses import categorical_crossentropy\nfrom sklearn.metrics import accuracy_score\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport cv2 \nfrom skimage import io\nfrom keras.preprocessing import image\nfrom os import walk\nimport os\n","0a53f339":"filename = '\/kaggle\/input\/facial-expression\/fer2013.csv'\ndata=pd.read_csv(filename)\n\ndata.head(20)","0e31082a":"labels = {0: 'Angry', 1: 'Digust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\nemotion_counts = data['emotion'].value_counts().reset_index()\nemotion_counts.columns = ['emotion', 'number']\nemotion_counts['emotion'] = emotion_counts['emotion'].map(labels)\nemotion_counts\n","d0136873":"def row2image(row):\n    pixels, emotion = row['pixels'], labels[row['emotion']]\n    img = np.array(pixels.split())\n    img = img.reshape(48,48)\n    image = np.zeros((48,48,3))\n    image[:,:,0] = img\n    image[:,:,1] = img\n    image[:,:,2] = img\n    return np.array([image.astype(np.uint8), emotion])\n\nplt.figure(0, figsize=(16,10))\nfor i in range(1,8):\n    face = data[data['emotion'] == i-1].iloc[0]\n    img = row2image(face)\n    plt.subplot(2,4,i)\n    plt.imshow(img[0])\n    plt.title(img[1])\n\nplt.show()  \n","636c3d80":"data_train = data[data['Usage']=='Training'].copy()\ndata_val   = data[data['Usage']=='PublicTest'].copy()\ndata_test  = data[data['Usage']=='PrivateTest'].copy()","9e511bfa":"def pre_Processing(data):\n    temp = data.pixels.apply(lambda row: [float(p) for p in row.split()])\n    X = np.array(temp.tolist())\n    X = X.reshape(-1,48,48,1)\n    X = X\/255.0\n    Y = to_categorical(data['emotion'], len(labels)) \n    return X,Y\n\nX_train,y_train = pre_Processing(data_train)\nX_test,y_test = pre_Processing(data_test)\nX_val,y_val = pre_Processing(data_val)\n\n\n","037885df":"print(np.shape(X_train))","a6abb049":"def get_model():\n    model = Sequential()\n    \n    model.add(Conv2D(64,kernel_size=(3,3),input_shape=(48,48,1)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(64,kernel_size=(3,3),padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    \n    model.add(Conv2D(128,kernel_size=(3,3),padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(128,kernel_size=(3,3),padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    \n    model.add(Conv2D(256,kernel_size=(3,3),padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    \n    model.add(Flatten())\n\n    model.add(Dense(256))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add(Dense(128))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add(Dense(64))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add(Dense(7, activation='softmax'))\n\n    return model","a531a06b":"cp_callBack = tf.keras.callbacks.ModelCheckpoint('model_filter.h5',verbose=0,save_freq=25,save_best_only=False)\nmodel = get_model()\nmodel.summary()\noptimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])","6fe60fe7":"data_generator = ImageDataGenerator(\n                        featurewise_center=False,\n                        featurewise_std_normalization=False,\n                        rotation_range=10,\n                        width_shift_range=0.1,\n                        height_shift_range=0.1,\n                        zoom_range=.1,\n                        horizontal_flip=True)\n\n","e23b6047":"#history = model.fit_generator(data_generator.flow(X_train,y_train,64),steps_per_epoch=len(X_train) \/64,epochs=50,verbose=1,callbacks = [cp_callBack],validation_data=(X_val, y_val),shuffle=True)\nmodel.load_weights('\/kaggle\/input\/facialexpressionweights\/model_filter.h5')","7ca7e86e":"model.evaluate(X_test,y_test)","7de600da":"def getFace(img,faces):\n    for (x, y, w, h) in faces:\n        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        roi_color = img[y:y + h, x:x + w]\n        return cv2.resize(roi_color,(48,48),interpolation=cv2.INTER_AREA)\n   \n","c38ffc29":"def getPercentage(custom):\n    sum =0\n    for i in custom[0]:\n        sum += i\n    for i in range(len(custom[0])):\n        custom[0][i] = int((custom[0][i] \/ sum) * 100)\n    return custom[0]","42536988":"def emotion_analysis(emotions):\n    objects = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n    y_pos = np.arange(len(objects))\n    plt.bar(y_pos, emotions, align='center', alpha=0.9)\n    plt.tick_params(axis='x', which='both', pad=10,width=4,length=10)\n    plt.xticks(y_pos, objects)\n    plt.ylabel('percentage')\n    plt.title('emotion')\n    \n    plt.show()","c5d5a0d1":"def printOutput(image_path):\n    show_img=image.load_img(image_path, grayscale=False, target_size=(400, 500))\n\n    face_cascade = cv2.CascadeClassifier('\/kaggle\/input\/facedetectionopencv\/face_detection.xml') \n    img = cv2.imread(image_path,0)\n\n    faces = face_cascade.detectMultiScale(\n        img,\n        scaleFactor=1.3,\n        minNeighbors=3\n    )\n\n\n    croped_img = getFace(img,faces)\n    if(np.shape(croped_img) == ()):\n        croped_img = cv2.resize(img,(48,48),interpolation=cv2.INTER_AREA)\n\n    x = image.img_to_array(croped_img)\n    x = np.expand_dims(x, axis = 0)\n    x \/= 255\n    plt.gray()\n    plt.imshow(show_img)\n    plt.show()\n    custom = model.predict(x)\n    \n    emotion_analysis(custom[0])\n\n    x = np.array(x, 'float32')\n    x = x.reshape([48, 48]);\n\n    \n\n    \n   \n\n","c75bb0c5":"image_path = \"\/kaggle\/input\/facialexpresionimages\/\"\n\n\nfor (dirpath, dirnames, filenames) in walk(image_path):\n    for i in range(len(filenames)):\n        printOutput(image_path+''+filenames[i])\n    ","53a8b34f":"<h4>Visualizing Data<\/h4>"}}