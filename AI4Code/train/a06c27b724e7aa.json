{"cell_type":{"8512e817":"code","965ea0e3":"code","71d3f832":"code","51bcec2e":"code","f4aa866f":"code","a5b7e385":"code","86420ac5":"code","c4d212d6":"code","2392c034":"code","a8e5e5c7":"code","53e90550":"code","c6d919b0":"markdown","8ba3dd6d":"markdown","5edcf8b0":"markdown","897847b7":"markdown","fcb0c7ad":"markdown"},"source":{"8512e817":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras.utils import to_categorical\n\n\n# Input data files are available in the \"..\/input\/\" directory.\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.\n\n# This is a bit of magic to make matplotlib figures appear inline in the notebook\n# rather than in a new window.\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'","965ea0e3":"from tensorflow.python.keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n# Import Data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest= pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train size:{}\\nTest size:{}\".format(train.shape, test.shape))\n\n# Transform Train and Test into images\\labels.\nx_train = train.drop(['label'], axis=1).values.astype('float32') # all pixel values\ny_train = train['label'].values.astype('int32') # only labels i.e targets digits\nx_test = test.values.astype('float32')\n#Reshape\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1) \/ 255.0\ny_train = y_train.reshape(y_train.shape[0], 1)\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1) \/ 255.0\n\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state=42)\ny_train = to_categorical(y_train)\ny_val = to_categorical(y_val)\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_val.shape)\nprint(y_val.shape)\nprint(x_test.shape)","71d3f832":"INPUT_NODE = 784\nOUTPUT_NODE = 10\n\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nNUM_LABELS = 10\n\nCONV1_DEEP = 32\nCONV1_SIZE = 5\n\nCONV2_DEEP = 64\nCONV2_SIZE = 5\n\nFC_SIZE = 512\n\nMINI_BATCH_COUNT = 100\nLEARNING_RATE = 0.01\nLEARNING_RATE_DECAY = 0.99\nREGULARIZATION_RATE = 0.0001\nTRAINING_EPOCHS = 100\nMOVING_AVERAGE_DECAY = 0.99","51bcec2e":"def inference(input_tensor, train, regularizer):\n    with tf.variable_scope(\"layer\", reuse=tf.AUTO_REUSE):\n        conv1_weights = tf.get_variable(\n            \"weight1\", [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n            initializer=tf.truncated_normal_initializer(stddev=0.1))\n        conv2_weights = tf.get_variable(\n            \"weight2\", [CONV2_SIZE,CONV2_SIZE,CONV1_DEEP,CONV2_DEEP],\n            initializer = tf.truncated_normal_initializer(stddev=0.1))\n        fc1_weights = tf.get_variable(\n            \"weight3\", [3136, FC_SIZE],\n            initializer=tf.truncated_normal_initializer(stddev=0.1))\n        if regularizer != None: tf.add_to_collection('losses', regularizer(fc1_weights))\n        fc2_weights = tf.get_variable(\n            \"weight4\", [FC_SIZE, NUM_LABELS],\n            initializer=tf.truncated_normal_initializer(stddev=0.1))\n        if regularizer != None: tf.add_to_collection('losses', regularizer(fc2_weights))\n    \n        conv1_biases = tf.get_variable(\n            \"bias1\", [CONV1_DEEP], initializer=tf.constant_initializer(0.0))\n        conv2_biases = tf.get_variable(\n            \"bias2\", [CONV2_DEEP], initializer = tf.constant_initializer(0.0))\n        fc1_biases = tf.get_variable(\n            \"bias3\", [FC_SIZE], initializer=tf.constant_initializer(0.1))\n        fc2_biases = tf.get_variable(\n            \"bias4\", [NUM_LABELS], initializer=tf.constant_initializer(0.1))\n    \n    \n    \n    conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1,1,1,1], padding='SAME')\n    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n    pool1 = tf.nn.max_pool(relu1, ksize = [1,2,2,1], strides = [1,2,2,1], padding=\"SAME\")\n    conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1,1,1,1], padding='SAME')\n    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n    pool2 = tf.contrib.layers.flatten(pool2)\n    fc1 = tf.nn.relu(tf.matmul(pool2, fc1_weights) + fc1_biases)\n    if train: fc1 = tf.nn.dropout(fc1, 0.5)\n    logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n\n    return logit\n        ","f4aa866f":"sess = tf.Session()","a5b7e385":"with tf.variable_scope(\"layer\", reuse=tf.AUTO_REUSE):\n    x = tf.placeholder(tf.float32, [None,IMAGE_SIZE,IMAGE_SIZE,NUM_CHANNELS],name='x-input')\n    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n    x_val_ph = tf.placeholder(tf.float32, [None,IMAGE_SIZE,IMAGE_SIZE,NUM_CHANNELS],name='x-val')\n    y_val_ph = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-val')\n    x_test_ph = tf.placeholder(tf.float32, [None,IMAGE_SIZE,IMAGE_SIZE,NUM_CHANNELS])\n\n\n    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n    y = inference(x,True,regularizer)\n    global_step = tf.Variable(0, trainable=False)\n    \n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n    \n    train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n        \n    y_val_result = inference(x_val_ph,None,None)\n    correct_prediction = tf.equal(tf.argmax(y_val_result, 1), tf.argmax(y_val_ph, 1))\n    evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    y_test_result = inference(x_test_ph,None,None)","86420ac5":"init_op = tf.global_variables_initializer()\nsess.run(init_op)\nbatch_size = x_train.shape[0]\/MINI_BATCH_COUNT\nprint(f\"batch size is {batch_size}\")\nfor epoch in range(TRAINING_EPOCHS):\n    for i in range(MINI_BATCH_COUNT):\n        x_train_batch = x_train[int(i*batch_size):int((i+1)*batch_size)]\n        y_train_batch = y_train[int(i*batch_size):int((i+1)*batch_size)]\n        _, loss_value = sess.run([train_step, loss], feed_dict={x: x_train_batch, y_: y_train_batch})\n\n    validation_accuracy = sess.run(evaluation_step, feed_dict={x_val_ph: x_val, y_val_ph: y_val})\n    print(f\"After {epoch} training epoch(s), loss on training batch is {loss_value}, accuracy on test batch is {validation_accuracy}\")","c4d212d6":"y_result = sess.run([y_test_result], feed_dict={x_test_ph: x_test})\ntest_result = sess.run(tf.nn.softmax(y_result[0]))","2392c034":"sess.close()","a8e5e5c7":"results = np.argmax(test_result, axis=1)\nprint(f\"result len is {len(results)}\")\nprint(f\"First 5 result is:\\n{results[0:100]}\")","53e90550":"results = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"MNIST-CNN-ENSEMBLE.csv\",index=False)","c6d919b0":"### Model architecture","8ba3dd6d":"### Model training","5edcf8b0":"### Data processing","897847b7":"### Model parameter definition","fcb0c7ad":"## CNN Sample for MNIST dataset use Tensorflow\nhi, this is a CNN model for classifying MINIST datasets, which may be helpful. The data processing section references Shay Guterman's Kernel. Thanks to Shay Guterman, your example is very streamlined."}}