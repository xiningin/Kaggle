{"cell_type":{"5fdac4a8":"code","66659ee2":"code","5ead8ebd":"code","92abf12f":"code","d30f8e55":"code","7d1d8fa6":"code","f6ee9ee5":"code","09881f3e":"code","396f68b5":"code","eec4a742":"code","c7110734":"code","6a50d68d":"code","293aa31f":"code","b758d08f":"code","ffa363f9":"code","6fc0abaf":"code","9324fdfa":"code","60bd160d":"code","5df276c5":"code","24202bb3":"code","00f6e0a2":"markdown","07e1ffc8":"markdown","9b17671d":"markdown","0df82b78":"markdown","feec74e3":"markdown","154a8521":"markdown","f40dc748":"markdown","c7a4e7ee":"markdown","7c0f57f9":"markdown","c257d0cc":"markdown","3abf29ca":"markdown","48fe4e46":"markdown","90257392":"markdown","4117d1ca":"markdown","2509f1e2":"markdown","f286b1b8":"markdown","72518a8f":"markdown","243d093d":"markdown","4211056e":"markdown","9e336be0":"markdown"},"source":{"5fdac4a8":"import os\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","66659ee2":"test_data_USA = pd.read_csv(\"\/kaggle\/input\/youtube-new\/USvideos.csv\")\n\ntest_data_USA.head(3)","5ead8ebd":"test_data_UK = pd.read_csv(\"\/kaggle\/input\/youtube-new\/GBvideos.csv\")\n\ntest_data_UK.head(3)","92abf12f":"test_data_USA = pd.read_csv(\"\/kaggle\/input\/youtube-new\/USvideos.csv\")\n\n# lets look at columns in this data and try to understand which are helpful for our task\nprint(\"The dataset has \" + str(test_data_USA.shape[0]) + \" rows.\")\nprint(\"And \" + str(test_data_USA.shape[1]) + \" columns atleast.\")\nprint(test_data_USA.columns)\n\nprint(\" \")\n\ntest_data_UK = pd.read_csv(\"\/kaggle\/input\/youtube-new\/GBvideos.csv\")\n\n# lets look at columns in this data and try to understand which are helpful for our task\nprint(\"The dataset has \" + str(test_data_UK.shape[0]) + \" rows.\")\nprint(\"And \" + str(test_data_UK.shape[1]) + \" columns atleast.\")\nprint(test_data_UK.columns)","d30f8e55":"# the list which includes names of useful columns \ncolumns_list = [\"views\", \"likes\", \"dislikes\", \"comment_count\"]\n\nus_data = pd.read_csv(\"\/kaggle\/input\/youtube-new\/USvideos.csv\", usecols = columns_list) # USA\nru_data = pd.read_csv(\"\/kaggle\/input\/youtube-new\/RUvideos.csv\", usecols = columns_list, encoding='ISO-8859-1') # Russia (use different encoding here)\nmx_data = pd.read_csv(\"\/kaggle\/input\/youtube-new\/MXvideos.csv\", usecols = columns_list, encoding='ISO-8859-1') # Mexico (use different encoding here)\nkr_data = pd.read_csv(\"\/kaggle\/input\/youtube-new\/KRvideos.csv\", usecols = columns_list, encoding='ISO-8859-1') # South Korea (use different encoding here)\njp_data = pd.read_csv(\"\/kaggle\/input\/youtube-new\/JPvideos.csv\", usecols = columns_list, encoding='ISO-8859-1') # Japan (use different encoding here)\nin_data = pd.read_csv(\"\/kaggle\/input\/youtube-new\/INvideos.csv\", usecols = columns_list) # India\ngb_data = pd.read_csv(\"\/kaggle\/input\/youtube-new\/GBvideos.csv\", usecols = columns_list) # Great Britain (UK)\nfr_data = pd.read_csv(\"\/kaggle\/input\/youtube-new\/FRvideos.csv\", usecols = columns_list) # France\nde_data = pd.read_csv(\"\/kaggle\/input\/youtube-new\/DEvideos.csv\", usecols = columns_list) # Germany\nca_data = pd.read_csv(\"\/kaggle\/input\/youtube-new\/CAvideos.csv\", usecols = columns_list) # Canada","7d1d8fa6":"# create lists of datasets` names and countries` names for our while cycle\ndatasets_list = [us_data, ru_data, mx_data, kr_data, jp_data,\n           in_data, gb_data, fr_data, de_data, ca_data]\n\ndatasets_name_list = [\"United States\", \"Russia\", \"Mexico\", \"South Korea\", \"Japanese\",\n                \"India\", \"UK\", \"France\", \"Deutchland\", \"Canada\"]\n\n# create a dataframe with useful columns \nall_in_data = pd.DataFrame(columns=[\"views\", \"likes\", \"dislikes\", \"comment_count\", \"country\"])","f6ee9ee5":"#now to create a master dataframe with all the datapoints in it\ni = 0\n\nwhile i != 10: # not equal 10, because we have only 10 countries in our lists\n    df_X = datasets_list[i]\n    names = datasets_name_list[i]\n    df_X[\"country\"] = names\n    all_in_data = pd.merge(all_in_data, df_X, how='outer')\n    i += 1\n\n# let`s see some information about this dataframe\nprint(all_in_data.shape)\nall_in_data.head(3)","09881f3e":"# correlation between variables in different datasets\n# size\nplt.figure(figsize=(12, 4))\n\n# list of useful columns in datasets\nlist_of_columns = [\"views\", \"likes\", \"dislikes\", \"comment_count\"]","396f68b5":"# correlation between variables in US data\nusa_corr = us_data[list_of_columns].corr()\n\nsns.heatmap(usa_corr, annot = True, cmap = \"Greens\")","eec4a742":"# correlation between variables in Russia data\nru_corr = ru_data[list_of_columns].corr()\n\nsns.heatmap(ru_corr, annot = True, cmap = \"Greens\")","c7110734":"# correlation between variables in Mexico data\nmx_corr = mx_data[list_of_columns].corr()\n\nsns.heatmap(mx_corr, annot = True, cmap = \"Greens\")","6a50d68d":"# correlation between variables in South Korean data\nkr_corr = kr_data[list_of_columns].corr()\n\nsns.heatmap(kr_corr, annot = True, cmap = \"Greens\")","293aa31f":"# correlation between variables in Japan data\njp_corr = jp_data[list_of_columns].corr()\n\nsns.heatmap(jp_corr, annot = True, cmap = \"Greens\")","b758d08f":"# correlation between variables in India data\nin_corr = in_data[list_of_columns].corr()\n\nsns.heatmap(in_corr, annot = True, cmap = \"Greens\")","ffa363f9":"# correlation between variables in UK data\ngb_corr = gb_data[list_of_columns].corr()\n\nsns.heatmap(gb_corr, annot = True, cmap = \"Greens\")","6fc0abaf":"# correlation between variables in France data\nfr_corr = fr_data[list_of_columns].corr()\n\nsns.heatmap(fr_corr, annot = True, cmap = \"Greens\")","9324fdfa":"# correlation between variables in Deutchland data\nde_corr = de_data[list_of_columns].corr()\n\nsns.heatmap(de_corr, annot = True, cmap = \"Greens\")","60bd160d":"# correlation between variables in Canada data\nca_corr = ca_data[list_of_columns].corr()\n\nsns.heatmap(ca_corr, annot = True, cmap = \"Greens\")","5df276c5":"# correlation between variables in our biggest dataset in different color maps\nbigdf_corr = all_in_data[list_of_columns].corr()\n\n# green\nsns.heatmap(bigdf_corr, annot = True, cmap = \"Greens\")","24202bb3":"# red heatmap\nsns.heatmap(bigdf_corr, annot = True, cmap = \"Reds\")","00f6e0a2":"Covariance between X and Y is expected value of the product of the deviation of X and Y from their respective means. Covariance formula:\n\n![image.png](attachment:image.png)","07e1ffc8":"Also, in my opinion we can **merge all datasets** in one big data now. So, we can see many interesting things with this big dataset, for example correlation between some variables not only at one country, but at all. To do this we run this code (If there are something strange or hard moments in this cycle, you can ask me in comments about it):","9b17671d":"It is defined as the covariance between two variables divided by the product of the standard deviations of the two variables.\nAt this picture someone can see many hard things. I will explain this:","0df82b78":"In USA data we can see that:\n\n\u2022 There is strong positive correlation between views, likes and comment_count. I think it is clear, because if viewers love video they will like and leave a comment.\n\n\u2022 Also, column comment_count have strong correlation with likes and dislikes. In my opinion is clear too, because when video is not interesting or vice versa people will give a like, dislike and leave comment about reasons about their opinion.\n\nThis was an example how to understand correlation matrixes. So, in my opinion you can try make conclusions to other matrixes which I will present here, but outputs with  these matrix are hidden, so you must to expand them.","feec74e3":"Main cycle","154a8521":"To start we can make correlation matrixes for every country to know correlation with variables in different countries. It will look like that for USA: ","f40dc748":"This is Covariance: \n\n![image.png](attachment:image.png)","c7a4e7ee":"# 1) Import libraries and load data. ","7c0f57f9":"Firstly, I think we must to know what Correlation is? \nA correlation coefficient is a number that denotes the strength of the relationship between two variables. It is defined in the range from -1 to 1. If correlation coefficient = 1 it means relationship between variables is very strong and positive. If correlation coefficient = 0 it means there isn\u2019t relationship at least. And if correlation coefficient = -1 it means there is too negative relationship. The formula of correlation looks like this:\n\n![image.png](attachment:image.png)","c257d0cc":"This thing means Pearson`s correlation between two variables X and Y:\n\n![image.png](attachment:image.png)","3abf29ca":"Here we can see that columns in these datasets are similar with each other and we can take only helpful columns from other datasets. How we can see there are many useless columns like thumbnail_link, tags, channel_title, title, description and others, because values are not numeric here and we can`t use them in our task. Well, we can just use argument usecols to load a special columns from dataframes. I had some problems with encoding, thats why I found how to solve this here: https:\/\/ru.stackoverflow.com\/questions\/933077\/%D0%9E%D1%88%D0%B8%D0%B1%D0%BA%D0%B0-%D0%BF%D1%80%D0%B8-%D1%87%D1%82%D0%B5%D0%BD%D0%B8%D0%B8-%D1%84%D0%B0%D0%B9%D0%BB%D0%B0-%D0%B2-%D0%B2%D0%B8%D1%80%D1%82%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%BC-%D0%BE%D0%BA%D1%80%D1%83%D0%B6%D0%B5%D0%BD%D0%B8%D0%B8-utf-8-codec-cant-decode-byte So, code will look like that:","48fe4e46":"In my opinion every data scientist and maybe begginer knows what Is it.\nAfter this theory time we can go to practice! ","90257392":"We will use .csv files, because it easier and in my opinion it more comfortable than use .json files. Well, let`s load only two datasets for example about USA and UK. After that we can watch on it and see about columns.","4117d1ca":"About this data. In this dataset we can see a lot of information about YouTube trend videos and stats about them. Also, this information collected from YouTube in different counties, that\u2019s why this task about correlation became a really interesting and hard. For more intrigue, I make hypothesis that there is will be strong positive correlation between likes, dislikes and views, because in my opinion these values must depend on each other. So, let`s start.","2509f1e2":"This is standard deviations of the two variables (X and Y):\n\n![image.png](attachment:image.png)","f286b1b8":"**Hello everyone!** In this kernel we will work with YouTube dataset which includes a lot of information and calculate correlation of some variables.","72518a8f":"# 4)\tConclusion. \nWell, in the conclusion, after exploring datasets, I can say that there is strong correlation between likes, views, dislikes and comments. It means that my hypothesis was correct and we proved this!\nWell, if you like this notebook, please upvote it and leave a comment. Also, you can check this notebook too, because I think it is really interesting (link:https:\/\/www.kaggle.com\/artemborzenko\/simple-exploring-the-dataset)\nGood Luck!","243d093d":"This was an example how to understand correlation matrixes. So, in my opinion you can try make conclusions to other matrixes which I will present here. \nWell, after all of these small data frames about each country we go to out big dataset. Let`s see correlation matrix. Code and output:","4211056e":"# 2)\tCorrelation.\nIt is the main stage in this notebook, because calculate correlation is our task. ","9e336be0":"Here we can see that:\n\n\u2022 There is strong positive correlation between views, likes and comment_count like in USA data. Because, if you watched correlation matrixes of other countries you saw almost in every matrix this correlation.\n\n\u2022 Also, column comment_count have strong correlation with likes and dislikes. Here the same situation like with correlation between views and other variables."}}