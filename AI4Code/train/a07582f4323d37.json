{"cell_type":{"77a596cf":"code","918b1f09":"code","8666905f":"code","375f7101":"code","24d2c84b":"code","48079d92":"code","18592a2e":"code","a6c6f710":"code","7ce5e0ac":"code","6d0f49ef":"code","b5db46bf":"code","9de64eb0":"code","ec187d1e":"code","1da028e8":"code","3cc0a2fa":"code","37c1b4b2":"code","99cbbf28":"code","bc296c38":"markdown","4c5faad7":"markdown","a483f83f":"markdown","f419a5e9":"markdown","c006da49":"markdown","d8619034":"markdown","24247305":"markdown","71179df6":"markdown","28fb9201":"markdown","d8e6897c":"markdown","22aab86c":"markdown","791c3999":"markdown","c4826913":"markdown","3da05904":"markdown","ff9c606d":"markdown"},"source":{"77a596cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","918b1f09":"train=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\n","8666905f":"from sklearn.model_selection import train_test_split, KFold, GridSearchCV,StratifiedKFold,LeavePOut\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler","375f7101":"train.head()","24d2c84b":"train.isnull().sum()","48079d92":"from sklearn.impute import SimpleImputer\nsi=SimpleImputer(strategy='mean')\nss=StandardScaler()\ndef impute_N_scale(df):\n    for i in df.columns.values:\n        df[i]=si.fit_transform(df[i].to_numpy().reshape(-1,1))\n    for i in df.columns.values:\n        df[i]=ss.fit_transform(df[i].to_numpy().reshape(-1,1))\n    return df","18592a2e":"X=train.iloc[:,1:119]\ny=train.iloc[:,-1]","a6c6f710":"X=impute_N_scale(X)","7ce5e0ac":"X_train,X_val,y_train,y_val=train_test_split(X,y, random_state=42)","6d0f49ef":"log_model=LogisticRegression(multi_class='ovr',#binary classification problem\n                       solver='saga',#supports both penalth l1 and l2\n                      random_state=42)\nlog_model.fit(X_train,y_train)\npreds=log_model.predict(X_val)\nprint(f'Error for regular train\/test splits: {np.sqrt(mean_squared_error(preds,y_val))}')","b5db46bf":"X_train,X_val,y_train,y_val=train_test_split(X,y, random_state=42, stratify=y)","9de64eb0":"log_model=LogisticRegression(multi_class='ovr',\n                       solver='saga',\n                      random_state=42)\nlog_model.fit(X_train,y_train)\npreds=log_model.predict(X_val)\nprint(f'Error for LogisticRegression Model: {np.sqrt(mean_squared_error(preds,y_val))}')","ec187d1e":"# k=5\nkf=KFold(n_splits=5,shuffle =True,random_state=42)\nerr=[]\nfor fold, (train_index, val_index) in enumerate(kf.split(X)):\n    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n    X_val, y_val = X.iloc[val_index], y.iloc[val_index]\n    model=LogisticRegression()\n    model.fit(X_train,y_train)\n    preds=model.predict(X_val)\n    print(f'Error for KFold {fold+1}: {np.sqrt(mean_squared_error(preds,y_val))}')\n    err.append(np.sqrt(mean_squared_error(preds,y_val)))\nprint('#'*20)\nprint(f'Average error of 5 folds:  {np.mean(err)}')","1da028e8":"# k=5\nskf=StratifiedKFold(n_splits=5,shuffle =True,random_state=42)\nerr=[]\nfor fold, (train_index, val_index) in enumerate(skf.split(X,y)):\n    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n    X_val, y_val = X.iloc[val_index], y.iloc[val_index]\n    model=LogisticRegression()\n    model.fit(X_train,y_train)\n    preds=model.predict(X_val)\n    print(f'Error for KFold {fold+1}: {np.sqrt(mean_squared_error(preds,y_val))}')\n    err.append(np.sqrt(mean_squared_error(preds,y_val)))\nprint('#'*20)\nprint(f'Average error of 5 folds:  {np.mean(err)}')","3cc0a2fa":"# k=n\n# for the convenience first 20 samples are used.Normally all of them should be used.\n# In other words: replace all X[:20] and y[:20] with X and y.\nkf=KFold(n_splits=len(X[:20]),shuffle =True,random_state=42)\nerr=[]\nfor fold, (train_index, val_index) in enumerate(kf.split(X[:20])):\n    X_train, y_train = X[:20].iloc[train_index], y[:20].iloc[train_index]\n    X_val, y_val = X[:20].iloc[val_index], y[:20].iloc[val_index]\n    model=LogisticRegression()\n    model.fit(X_train,y_train)\n    preds=model.predict(X_val)\n    print(f'Error for KFold {fold+1}: {np.sqrt(mean_squared_error(preds,y_val))}')\n    err.append(np.sqrt(mean_squared_error(preds,y_val)))\nprint('#'*20)\nprint(f'Average error of all folds:  {np.mean(err)}')","37c1b4b2":"#p=2\nlpo=LeavePOut(2)\nerr=[]\nfor fold, (train_index, val_index) in enumerate(lpo.split(X[:20])):\n    X_train, y_train = X[:20].iloc[train_index], y[:20].iloc[train_index]\n    X_val, y_val = X[:20].iloc[val_index], y[:20].iloc[val_index]\n    model=LogisticRegression()\n    model.fit(X_train,y_train)\n    preds=model.predict(X_val)\n    print(f'Error for KFold {fold+1}: {np.sqrt(mean_squared_error(preds,y_val))}')\n    err.append(np.sqrt(mean_squared_error(preds,y_val)))\nprint('#'*20)\nprint(f'Average error of all folds:  {np.mean(err)}')","99cbbf28":"\"\"\"\nfrom sklearn.model_selection import KFold\nclf=LogisticRegression(multi_class='ovr',#binary classification problem\n                       solver='saga',#supports both penalth l1 and l2\n                      random_state=42)\np_grid = {\"penalty\": ['l1','l2'],\n          \"C\": [1,100,10]}\n\ninner_cv=KFold(n_splits=2,shuffle=True, random_state=42)\n\n\n\ngcv=GridSearchCV(estimator=clf,\n                param_grid=p_grid,scoring='accuracy',\n                 n_jobs=-1,\n                 cv=inner_cv)\nouter_cv=KFold(n_splits=5, shuffle=True,random_state=42)\n\nfor train_idx,valid_idx in outer_cv.split(X_train,y_train):\n    gcv.fit(X_train.iloc[train_idx],y_train.iloc[train_idx])\n    print(f'Best Parameters: {gcv.best_params_}')\n    print(f'Best Score: {gcv.best_score_*100}%')\n    print(f'Best Estimator: {gcv.best_estimator_}')\n    print(f'Accuracy on outer fold: {gcv.best_estimator_.score(X_train.iloc[valid_idx],y_train.iloc[valid_idx])*100}%')\n    \"\"\"\n# The code does not work well on kaggle platform. So I will share my results below.","bc296c38":"<img src= \"https:\/\/miro.medium.com\/max\/3000\/1*_Ygt7XSGmDvBIdXX-z6TWw.png\" alt =\"KFold\" style='width: 500px;'>","4c5faad7":"\n<img src= \"https:\/\/i1.wp.com\/dataaspirant.com\/wp-content\/uploads\/2020\/12\/7-LOOCV-Leave-One-Out-Cross-Validation.png?ssl=1\" alt =\"Leave One Out\" style='width: 500px;'>\n","a483f83f":"# Stratified Cross Validation\nIt is similar to KFold. However, in this method, KFold returns stratified results.","f419a5e9":"# Stratifying \n\nBy stratifying we will be make sure that each split have the same proprotions of outcome values.","c006da49":"Best Parameters: {'C': 10, 'penalty': 'l1'}\nBest Score: 52.78239934232418%\nBest Estimator: LogisticRegression(C=10, multi_class='ovr', penalty='l1', random_state=42,\n                   solver='saga')\nAccuracy on outer fold: 52.86752616332176%\n\nBest Parameters: {'C': 1, 'penalty': 'l1'}\nBest Score: 52.78051893037035%\nBest Estimator: LogisticRegression(C=1, multi_class='ovr', penalty='l1', random_state=42,\n                   solver='saga')\nAccuracy on outer fold: 52.77326495592659%\n\nBest Parameters: {'C': 1, 'penalty': 'l2'}\nBest Score: 52.79683031655961%\nBest Estimator: LogisticRegression(C=1, multi_class='ovr', random_state=42, solver='saga')\nAccuracy on outer fold: 53.040771986141834%\n\nBest Parameters: {'C': 1, 'penalty': 'l2'}\nBest Score: 52.834672757464595%\nBest Estimator: LogisticRegression(C=1, multi_class='ovr', random_state=42, solver='saga')\nAccuracy on outer fold: 52.890707066752796%\n\nBest Parameters: {'C': 1, 'penalty': 'l1'}\nBest Score: 52.80857451221069%\nBest Estimator: LogisticRegression(C=1, multi_class='ovr', penalty='l1', random_state=42,\n                   solver='saga')\nAccuracy on outer fold: 52.71062916348595%","d8619034":"I have influenced the code below from 'mlanhenke'. You can find his code from [here](https:\/\/www.kaggle.com\/mlanhenke\/tps-09-single-catboostclassifier)","24247305":"# KFold Cross Validation\n\nKFold Cross-Validation is used for evaluating the model more intelligently.  We divide our training data into k splits. Each time one split will be used for validation and the rest is used for training(k-1).    ","71179df6":"<img src= \"https:\/\/www.researchgate.net\/profile\/Danilo-Bzdok\/publication\/324829283\/figure\/fig4\/AS:631579718197312@1527591747794\/A-diagram-of-the-nested-k-fold-cross-validation-with-model-selection.png\" alt =\"Nested\" style='width: 700px;'>\n","28fb9201":"# Leave One Out Cross Validation\nThis is just a specific case of the KFold Cross-Validation. This time number of folds is equal to the number of samples. (k=n)\nIt is good for evaluation however so much computation takes place which requires lot of time.","d8e6897c":"The goal of this notebook is to illustrate and explain different cross-validation variations.\n\nWe will use LogisticRegression Model because it evaluates faster than many of the machine learning algorithms.\n\nHope you'll enjoy it!","22aab86c":"# Leave P Out Cross Validation\nIn this type of Cross-Validation, we split p number of samples from our dataset and train with the rest of the data. Then we iterate over the all samples.","791c3999":"# Conclusion\nIn this part of this notebook we,\n\n1. First split the train data to X and y.(Features and outcomes)\n2. Filled the empty areas with the mean of the corresponding feature columns.\n3. We split X and y by using train_test_split function. Then we evaluated with LogisticRegression\n4. We split X and y by using train_test_split function but this time we stratified.Then we evaluated with LogisticRegression\n5. We used KFold CV, StratifiedKFold CV, Leave One Out CV, Leave P Out CV and Nested CV.\n\nThanks for your kind attention!","c4826913":"# Nested Cross Validation\nIn nested cross validation we divide dataset into two loops. One loop is used for finding the best parameters of the machine learning model (inner loop), and the othe one is used for regular KFold Cross Validation Process.","3da05904":"# Train\/Test Splits\n\n'train_test_split' function basically splits the dataset into train and test datasets. Train dataset is used for training the model and test dataset is used for evaluating the model performance.\n\nFor the problems whose results of the test split are unknown (prediction) or maybe due to some other reasons, we split the training dataset into train and validation datasets. By doing that we can train and evaluate the model we construct without using a test dataset.  ","ff9c606d":"<img src= \"https:\/\/algotrading101.com\/learn\/wp-content\/uploads\/2020\/06\/training-validation-test-data-set.png\" alt =\"train_test_split\" style='width: 500px;'>"}}