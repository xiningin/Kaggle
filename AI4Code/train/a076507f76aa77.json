{"cell_type":{"1422c85b":"code","cf4f3a51":"code","a1785c89":"code","3fb4dad7":"code","feadf670":"code","0c71286b":"code","de85bfb1":"code","02f2f867":"code","dd0e476b":"code","30314732":"code","3d18b80a":"code","f7e75976":"code","7ec303fa":"code","78e3fe46":"code","c833a4a8":"code","b488d11c":"code","9849359b":"code","3d908c46":"code","c600ae81":"code","cf25938f":"code","9ed8f14f":"code","13159312":"code","472fce39":"code","14cc6ac0":"code","901d5480":"code","aeb9d59a":"code","74ce41ee":"code","7a23ee9b":"code","cf7723ad":"code","62c16778":"markdown","6ecd2eef":"markdown","7ca0dcfb":"markdown","0b9649f5":"markdown"},"source":{"1422c85b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cf4f3a51":"#loading the data\nfake=pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\ntrue=pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")","a1785c89":"#Creating a category for whether fake or not\n#where 1 stand for fake news and 0 stands for true news\n\nfake[\"category\"]=1\ntrue[\"category\"]=0","3fb4dad7":"#joining the data the two data frame and reseting index\ndf=pd.concat([fake,true]).reset_index(drop=True)","feadf670":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","0c71286b":"#creating a count plot for category column\nfig = plt.figure(figsize=(10,5))\n\n\n\ngraph = sns.countplot(x=\"category\", data=df)\nplt.title(\"Count of Fake and True News\")\n\n#removing boundary\ngraph.spines[\"right\"].set_visible(False)\ngraph.spines[\"top\"].set_visible(False)\ngraph.spines[\"left\"].set_visible(False)\n\n#annoting bars with the counts  \nfor p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 0.2,height ,ha=\"center\",fontsize=12)","de85bfb1":"#creating a count plot for subject column\nfig = plt.figure(figsize=(10,5))\n\n\n\ngraph = sns.countplot(x=\"subject\", data=df)\nplt.title(\"Count of Subjecs\")\n\n#removing boundary\ngraph.spines[\"right\"].set_visible(False)\ngraph.spines[\"top\"].set_visible(False)\ngraph.spines[\"left\"].set_visible(False)\n\n#annoting bars with the counts  \nfor p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 0.2,height ,ha=\"center\",fontsize=12)","02f2f867":"#checking the missing values in each columns\ndf.isna().sum()*100\/len(df)","dd0e476b":"#checking if there is empty string in TEXT column\nblanks=[]\n\n#index,label and review of the doc\nfor index,text in df[\"text\"].iteritems(): # it will iter through index,label and review\n    if text.isspace(): # if there is a space\n        blanks.append(index) #it will be noted down in empty list\n\nlen(blanks)","30314732":"#instead of dropping these values we are going to merge title with text\n\ndf[\"text\"] =df[\"title\"]+df[\"text\"]\n\n#we only need two columns rest can be ignored\n\ndf=df[[\"text\",\"category\"]]","3d18b80a":"#importing libraries for cleaning puprose\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nimport spacy\nimport re\n","f7e75976":"#loading spacy library\nnlp=spacy.load(\"en_core_web_sm\")\n\n#creating instance\nlemma=WordNetLemmatizer()","7ec303fa":"#creating list of stopwords containing stopwords from spacy and nltk\n\n#stopwords of spacy\nlist1=nlp.Defaults.stop_words\nprint(len(list1))\n\n#stopwords of NLTK\nlist2=stopwords.words('english')\nprint(len(list2))\n\n#combining the stopword list\nStopwords=set((set(list1)|set(list2)))\nprint(len(Stopwords))","78e3fe46":"#text cleaning function\ndef clean_text(text):\n    \n    \"\"\"\n    It takes text as an input and clean it by applying several methods\n    \n    \"\"\"\n    \n    string = \"\"\n    \n    #lower casing\n    text=text.lower()\n    \n    #simplifying text\n    text=re.sub(r\"i'm\",\"i am\",text)\n    text=re.sub(r\"he's\",\"he is\",text)\n    text=re.sub(r\"she's\",\"she is\",text)\n    text=re.sub(r\"that's\",\"that is\",text)\n    text=re.sub(r\"what's\",\"what is\",text)\n    text=re.sub(r\"where's\",\"where is\",text)\n    text=re.sub(r\"\\'ll\",\" will\",text)\n    text=re.sub(r\"\\'ve\",\" have\",text)\n    text=re.sub(r\"\\'re\",\" are\",text)\n    text=re.sub(r\"\\'d\",\" would\",text)\n    text=re.sub(r\"won't\",\"will not\",text)\n    text=re.sub(r\"can't\",\"cannot\",text)\n    \n    #removing any special character\n    text=re.sub(r\"[-()\\\"#!@$%^&*{}?.,:]\",\" \",text)\n    text=re.sub(r\"\\s+\",\" \",text)\n    text=re.sub('[^A-Za-z0-9]+',' ', text)\n    \n    for word in text.split():\n        if word not in Stopwords:\n            string+=lemma.lemmatize(word)+\" \"\n    \n    return string\n","c833a4a8":"#cleaning the whole data\ndf[\"text\"]=df[\"text\"].apply(clean_text)","b488d11c":"from wordcloud import WordCloud","9849359b":"#True News\nplt.figure(figsize = (20,20))\nWc = WordCloud(max_words = 500 , width = 1600 , height = 800).generate(\" \".join(df[df.category == 0].text))\nplt.axis(\"off\")\nplt.imshow(Wc , interpolation = 'bilinear')","3d908c46":"#creating more intiuive wordcloud \n\n#pil is pillow and used for image manupulation\nfrom PIL import Image\n","c600ae81":"#creating a mask of thumb\nthumb=\"..\/input\/images-coud\/thumbs-up.png\"\nicon=Image.open(thumb)\nmask=Image.new(mode=\"RGB\",size=icon.size, color=(255,255,255))\nmask.paste(icon, box=icon)\n\nrgb_array=np.array(mask)","cf25938f":"#True News\nplt.figure(figsize = (10,10))\nWc = WordCloud(mask=rgb_array,max_words = 2000 , width = 1600 ,\n               height = 800)\n\nWc.generate(\" \".join(df[df.category == 0].text))\nplt.axis(\"off\")\nplt.imshow(Wc , interpolation = 'bilinear')","9ed8f14f":"#creating word cloud using skull image for fake news which depict that \n#fake news are dangerous \n\nskull=\"..\/input\/images-coud\/skull-icon.png\"\nicon=Image.open(skull)\nmask=Image.new(mode=\"RGB\",size=icon.size, color=(255,255,255))\nmask.paste(icon, box=icon)\n\nrgb_array=np.array(mask)","13159312":"#Fake News\nplt.figure(figsize = (15,15))\nWc = WordCloud(mask=rgb_array,max_words = 2000 , width = 1600 ,\n               height = 800)\n\nWc.generate(\" \".join(df[df.category == 1].text))\nplt.axis(\"off\")\nplt.imshow(Wc , interpolation = 'bilinear')","472fce39":"#splitting the \nfrom sklearn.model_selection import train_test_split\n\n\nX=df[\"text\"] #feature \ny=df[\"category\"] # traget\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","14cc6ac0":"#importing libraries to build a pipline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\n","901d5480":"#this pipe line will take the text and vectorise it , and then TF-IDF, then fitting the model\n\ntext_clf=Pipeline([(\"tfidf\",TfidfVectorizer()),(\"clf\",LinearSVC())])\ntext_clf.fit(X_train,y_train)","aeb9d59a":"#making prediction using the model\npredictions=text_clf.predict(X_test)","74ce41ee":"from sklearn import metrics\nprint(metrics.classification_report(y_test,predictions))","7a23ee9b":"#overall acuracy\nprint(metrics.accuracy_score(y_test,predictions))","cf7723ad":"#confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))","62c16778":"### Stopwords \nA stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.We would not want these words to take up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to stop words. NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages. You can find them in the nltk_data directory. home\/pratima\/nltk_data\/corpora\/stopwords is the directory address.(Do not forget to change your home directory name)\n\n### Lemmatisation\nlemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a morphological analysis to words. The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'. Further, the lemma of 'meeting' might be 'meet' or 'meeting' depending on its use in a sentence.","6ecd2eef":"# Data Cleaning","7ca0dcfb":"# Word Cloud\n\nA word cloud is a collection, or cluster, of words depicted in different sizes. The bigger and bolder the word appears, the more often it\u2019s mentioned within a given text and the more important it is.","0b9649f5":"#  Feature-Extraction & Model building \n"}}