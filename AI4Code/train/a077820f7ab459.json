{"cell_type":{"ac969fcf":"code","1ebea328":"code","f7ac690f":"code","58a4c870":"code","04d8e564":"code","5003d5e6":"code","2979daf7":"code","ce7667c4":"code","5850bf9e":"code","63529cd1":"code","7a7a3bac":"code","f135640b":"code","dbb77f66":"code","4f815a74":"code","72f6e87c":"code","e314906b":"code","f6b0d409":"code","f88896bd":"code","9cd88996":"code","9960b797":"code","6d147265":"code","8758a5ba":"code","02101ced":"code","a54e8090":"code","64fe09ad":"code","18022f1c":"code","04f77e02":"code","aa9c802a":"code","01a80e7e":"code","948519be":"code","86374bcd":"code","4c924edb":"code","704743d6":"code","e1d26200":"code","9e34aecd":"code","1bad00b6":"code","2e1297f0":"markdown","250a57f0":"markdown","ab0b7dc0":"markdown","00aefc98":"markdown","a8f05f0c":"markdown","af6b88f6":"markdown","15ec6772":"markdown","c0898236":"markdown","2475ace3":"markdown","4441070c":"markdown","11f88cff":"markdown","ade9dece":"markdown","58eab402":"markdown","5a2a3da3":"markdown","f390f9b5":"markdown","0ef9243f":"markdown","5ecb50b5":"markdown","7d815f6b":"markdown","3ce9577b":"markdown","e07c6e21":"markdown","2bbf16c4":"markdown","00faa1e0":"markdown","7e345405":"markdown"},"source":{"ac969fcf":"import requests\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms as pth_transforms\nimport numpy as np\nfrom PIL import Image\nimport os\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm import tqdm","1ebea328":"patch_size = 8  #8\nmodel = torch.hub.load('facebookresearch\/dino:main', 'dino_vits16')\n#device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","f7ac690f":"for p in model.parameters():\n    p.requires_grad = False\n\nif torch.cuda.is_available():\n    model.cuda()\n    \nmodel.eval()\nmodel.to(device)","58a4c870":"transform = pth_transforms.Compose([\n    pth_transforms.ToTensor(),\n    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])","04d8e564":"Name0=os.listdir('..\/input\/rock-paper-scissors-dataset\/train')\nName=sorted(Name0)\nN=[0,1,2]\nnormal_mapping=dict(zip(Name,N)) \nreverse_mapping=dict(zip(N,Name)) ","5003d5e6":"for name in Name:\n    !mkdir {name}","2979daf7":"data_dir='..\/input\/rock-paper-scissors-dataset\/train'","ce7667c4":"for name in Name:\n    path0=os.path.join(data_dir,name)\n    \n    for im in tqdm(os.listdir(path0)):\n        \n        path1=os.path.join(path0,im)\n        img_npy = cv2.imread(path1)\n        img_npy = cv2.resize(img_npy,dsize=(600,600),interpolation=cv2.INTER_CUBIC)    ####\n        img = transform(img_npy)\n        w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - img.shape[2] % patch_size\n        img = img[:, :w, :h].unsqueeze(0)\n        w_featmap = img.shape[-2] \/\/ patch_size\n        h_featmap = img.shape[-1] \/\/ patch_size\n        attentions = model.get_last_selfattention(img.cuda())    ###img.cuda()\n        \n        nh = attentions.shape[1]\n        attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n        val, idx = torch.sort(attentions)\n        val \/= torch.sum(val, dim=1, keepdim=True)\n        cumval = torch.cumsum(val, dim=1)\n\n        threshold = 0.6\n        th_attn = cumval > (1 - threshold)\n        idx2 = torch.argsort(idx)\n\n        for head in range(nh):\n            th_attn[head] = th_attn[head][idx2[head]]\n\n        th_attn = th_attn.reshape(nh, w_featmap\/\/2, h_featmap\/\/2).float()\n        th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\")[0].cpu().numpy()\n        attentions = attentions.reshape(nh, w_featmap\/\/2, h_featmap\/\/2)\n        attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\")[0].cpu().numpy()\n        attentions_mean = np.mean(attentions, axis=0)\n\n        path2=os.path.join(name,im[0:-4]+'.png')\n        #print(path2)\n        #plt.imshow(attentions_mean)\n        cv2.imwrite(path2,attentions_mean*5000)     ### \n        #plt.show()\n        \n        #img2=cv2.imread(path2,cv2.IMREAD_GRAYSCALE)\n        #print(img2.shape)\n        #plt.imshow(img2)\n        #plt.show()","5850bf9e":"import tensorflow as tf \nimport numpy as np \nimport matplotlib.pyplot as plt \ntf.__version__","63529cd1":"img_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n                            #rotation_range=90,\n                            brightness_range=(0.5,1), \n                            #shear_range=0.2, \n                            #zoom_range=0.2,\n                            channel_shift_range=0.2,\n                            horizontal_flip=True,\n                            vertical_flip=True,\n                            rescale=1.\/255,\n                            validation_split=0.3)","7a7a3bac":"root_dir = '\/kaggle\/working'   #Transfer learning of DINO attention map image\n\nimg_generator_flow_train = img_generator.flow_from_directory(\n    directory=root_dir,\n    target_size=(600,600),\n    batch_size=16,\n    shuffle=True,\n    subset=\"training\")\n\nimg_generator_flow_valid = img_generator.flow_from_directory(\n    directory=root_dir,\n    target_size=(600,600),\n    batch_size=16,\n    shuffle=True,\n    subset=\"validation\")","f135640b":"imgs, labels = next(iter(img_generator_flow_train))\nprint(labels)","dbb77f66":"for img, label in zip(imgs, labels):\n    print(img.shape)\n    plt.imshow(img)\n    value=np.argmax(label)\n    plt.title(reverse_mapping[value])\n    plt.show()","4f815a74":"base_model = tf.keras.applications.InceptionV3(input_shape=(600,600,3),\n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )","72f6e87c":"base_model.trainable = False","e314906b":"model = tf.keras.Sequential([\n    base_model,\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(3, activation=\"softmax\")\n])","f6b0d409":"model.summary()","f88896bd":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001),\n              loss = tf.keras.losses.CategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.CategoricalAccuracy()])","9cd88996":"model.fit(img_generator_flow_train, \n          validation_data=img_generator_flow_valid, \n          steps_per_epoch=8, epochs=40) #8,32","9960b797":"# Visualise train \/ Valid Accuracy\nplt.plot(model.history.history[\"categorical_accuracy\"], c=\"r\", label=\"train_accuracy\")\nplt.plot(model.history.history[\"val_categorical_accuracy\"], c=\"b\", label=\"test_accuracy\")\nplt.legend(loc=\"upper left\")\nplt.show()","6d147265":"# Visualise train \/ Valid Loss\nplt.plot(model.history.history[\"loss\"], c=\"r\", label=\"train_loss\")\nplt.plot(model.history.history[\"val_loss\"], c=\"b\", label=\"test_loss\")\nplt.legend(loc=\"upper left\")\nplt.show()","8758a5ba":"imgs, labels = next(iter(img_generator_flow_valid))","02101ced":"print(labels)","a54e8090":"for layer in model.layers:\n    print(layer.name)","64fe09ad":"base_model = model.layers[0]","18022f1c":"tf.keras.utils.plot_model(base_model, show_shapes=True, show_layer_names=True)","04f77e02":"for layer in base_model.layers:\n    print(layer.name)","aa9c802a":"last_conv_layer_name = \"mixed10\"\nclassifier_layer_names = [layer.name for layer in model.layers][1:]","01a80e7e":"# We start by setting up the dependencies we will use\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Display\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm","948519be":"# The Grad-CAM algorithm\ndef get_img_array(img_path, size):\n    # `img` is a PIL image of size 299x299\n    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n    # `array` is a float32 Numpy array of shape (299, 299, 3)\n    array = keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 299, 299, 3)\n    array = np.expand_dims(array, axis=0)\n    return array\n\n\ndef make_gradcam_heatmap(\n    img_array, base_model, model, last_conv_layer_name, classifier_layer_names):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer\n    last_conv_layer = base_model.get_layer(last_conv_layer_name)\n    last_conv_layer_model = keras.Model(base_model.inputs, last_conv_layer.output)\n\n    # Second, we create a model that maps the activations of the last conv\n    # layer to the final class predictions\n    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n    x = classifier_input\n    for layer_name in classifier_layer_names:\n        x = model.get_layer(layer_name)(x)\n    classifier_model = keras.Model(classifier_input, x)\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        # Compute activations of the last conv layer and make the tape watch it\n        last_conv_layer_output = last_conv_layer_model(img_array)\n        tape.watch(last_conv_layer_output)\n        # Compute class predictions\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n\n    # This is the gradient of the top predicted class with regard to\n    # the output feature map of the last conv layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = np.maximum(heatmap, 0) \/ np.max(heatmap)\n    return heatmap","86374bcd":"# Print what the top predicted class is\npreds = model.predict(imgs)\npred_labels = tf.argmax(preds, axis = -1)\n\nprint(\"Prediction output:\", preds)\nprint(\"Predicted label:\", pred_labels)","4c924edb":"# Generate class activation heatmap\nheatmaps = []\n\nfor img in imgs:\n    heatmap = make_gradcam_heatmap(\n    tf.expand_dims(img,axis=0),\n        base_model, model, \n        last_conv_layer_name, \n        classifier_layer_names\n  )\n    heatmaps.append(heatmap)\n\n# Display heatmap\nplt.matshow(heatmaps[0])\nplt.show()\n","704743d6":"from pathlib import Path\n\nfor img, pred_label, true_label, heatmap in zip(imgs, pred_labels, labels, heatmaps): \n    # We rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # We use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # We use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # We create an image with RGB colorized heatmap\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * 0.003 + img\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    save_path = \"saved_img.jpg\"\n    superimposed_img.save(save_path)\n\n    # Display Grad CAM\n    pred_file_path = np.argmax(img_generator_flow_valid.labels == pred_label)\n    pred_label_name = Path(img_generator_flow_valid.filepaths[pred_file_path]).parent.name\n\n    true_file_path = np.argmax(img_generator_flow_valid.labels == tf.argmax(true_label))\n    true_label_name = Path(img_generator_flow_valid.filepaths[true_file_path]).parent.name\n\n    print(\"Predicted label:\",pred_label_name)\n    print(\"True label:\", true_label_name)\n\n    display(Image(save_path,width=300,height=300))","e1d26200":"LABEL=[]\nfor item in labels:\n    LABEL+=[np.argmax(item)]\nPRED=pred_labels.numpy().tolist()","9e34aecd":"print(LABEL)\nprint(PRED)","1bad00b6":"from sklearn.metrics import classification_report\nprint(classification_report(LABEL,PRED))","2e1297f0":"# RPS DINO Attention Image Transfer Learning \n","250a57f0":"### Prepare img_generator_flow","ab0b7dc0":"## Transfer Learning ","00aefc98":"## Preprocessing with ImageDataGenerator","a8f05f0c":"### Visualize accuracy and loss","af6b88f6":"## compare the result with Transfer Learning for RPS Classification \nhttps:\/\/www.kaggle.com\/stpeteishii\/transfer-learning-for-rps-classification","15ec6772":"### Visualize a batch of images","c0898236":"## Interpretation with Grad Cam","2475ace3":"### Work Flow\n- Create DINO attention map images and saved\n- Transfer learning using DINO attention map image","4441070c":"### Set the weights of the imported model","11f88cff":"### Train the model","ade9dece":"### Prepare ImageDataGenerator\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator","58eab402":"## Classification Report","5a2a3da3":"# Create DINO attention map images and saved\nhttps:\/\/www.kaggle.com\/stpeteishii\/dandelion-image-dino-vision-transformers","f390f9b5":"### Import a pretrained model\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications\/InceptionV3","0ef9243f":"### make_gradcam_heatmap","5ecb50b5":"### Predicted label and heatmap","7d815f6b":"### Create heatmap","3ce9577b":"### Create imgs and labels","e07c6e21":"### Predict","2bbf16c4":"# Transfer learning of DINO attention map image","00faa1e0":"### Compile model","7e345405":"### Create model"}}