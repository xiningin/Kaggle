{"cell_type":{"13cf4bc3":"code","91ce8c79":"code","c4bd93fd":"code","8bfcb838":"code","78680adc":"code","ca916e55":"code","c7c7435a":"code","713e3670":"code","55cfd63b":"code","b6eb6de4":"code","ab1eb0dc":"code","8bb1df88":"code","5c753e94":"code","bad9adcd":"code","18b207ac":"code","c38b71b0":"code","e1630391":"code","878c2c4d":"code","3e13f27b":"code","e29ae549":"code","21d6bdfe":"code","917df1a7":"code","724b0f51":"code","82dcb355":"code","eb796e5d":"code","9a3996f3":"code","475944c7":"code","2a1fae5b":"code","57b109a9":"code","c5ce3b57":"code","5a812bea":"code","a8605e7b":"code","06f7b16d":"code","2f824546":"code","3568ea33":"code","ad551e88":"code","ccf32ca5":"code","47f9b93e":"code","40d05ab4":"code","f2a3c5bf":"code","7e9fd9ca":"code","f70cc7ce":"code","7a5a6bbb":"code","cf7024db":"code","7a01192c":"code","ab5976d5":"code","1ac003cd":"code","2baade1b":"code","6906e399":"code","941f9ebf":"code","7aaf1609":"code","ddf9b13a":"code","933f819f":"code","560d21de":"code","bd6d5345":"code","3bb86aff":"code","cdaf1c43":"code","4262843b":"code","e7cd9059":"code","dc02f33d":"code","02180369":"code","22b18db0":"code","f3bebb66":"code","b0ba17a5":"code","81129745":"code","356a166f":"code","d62c41c3":"code","3d2b46d7":"code","1a983499":"code","39f42510":"markdown","2a42a415":"markdown","47f83314":"markdown","0aec872d":"markdown","bde2cab8":"markdown","684d9d9a":"markdown","ca9155ca":"markdown","da3f2748":"markdown","2ddd1ed5":"markdown","08810400":"markdown","3b3dc73d":"markdown","582cd5fb":"markdown","254e72ec":"markdown","01753bff":"markdown","4be9f985":"markdown","7d0b7ed3":"markdown","b5063e00":"markdown","894ddfa9":"markdown","9657a304":"markdown","e223e3fe":"markdown","c8966c4c":"markdown","098d8e77":"markdown"},"source":{"13cf4bc3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","91ce8c79":"# --------------------------\n# Importing Dependencies\n# --------------------------\n\nimport pandas as pd #we save pandas in name of pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","c4bd93fd":"# --------------------------\n# Load the dataset\n# --------------------------\n\ndata = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","8bfcb838":"# --------------------------\n# Take a look at data\n# --------------------------\n\ndata.head() #call 5 first rows of data","78680adc":"data.describe().T ","ca916e55":"data.shape ","c7c7435a":"data.info() ","713e3670":"# --------------------------\n# Distribution of Normal CC and Fraud CC\n# --------------------------\n\nfraud = data[(data['Class'] ==1)]  #return the Class variable that has value of 1\nnormal = data[(data['Class'] != 1)] # return the Class variable that value except 1\nlabels = ['fraud','normal']","55cfd63b":"fig, ax = plt.subplots()\nax.pie([len(fraud),len(normal)], labels = labels, colors = ['red','blue'], autopct='%1.1f%%') #pie chart\nax.set_title('Fraud CC Distribution') #to set title\nplt.show() #crucial script when u show the plot","b6eb6de4":"fraud_pct = np.round(len(fraud)\/len(data)*100, 4)\nfraud_pct","ab1eb0dc":"# --------------------------\n# SCALE Amount Variable\n# --------------------------\n\nfrom sklearn.preprocessing import StandardScaler\n\ndata['amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))","8bb1df88":"# --------------------------\n# Drop unnessesary variable (Time and Amount)\n# --------------------------\n\ndf = data.drop(['Time','Amount'], axis=1)\ndf.head()","5c753e94":"# --------------------------\n# Correlation Between Variable\n# --------------------------\n\ncorr= df.corr()\nf, ax = plt.subplots(figsize=(25, 25))\nsns.heatmap(corr, cmap=\"Blues\", annot=True)","bad9adcd":"y = df['Class']\nX = df.drop('Class', axis=1) ","18b207ac":"# --------------------------\n# Data is highly imbalanced. Apply Resampling method to address this problem. \n# --------------------------","c38b71b0":"from imblearn.combine import SMOTETomek","e1630391":"#from imblearn.over_sampling import SMOTE\n","878c2c4d":"# --------------------------\n# Resampling with SMOTETomek\n# --------------------------\n\nsmote = SMOTETomek(sampling_strategy ='auto', random_state= 42) \nX_sm, y_sm = smote.fit_resample(X, y) ","3e13f27b":"X_sm.shape","e29ae549":"y_sm.shape","21d6bdfe":"y_sm.head()","917df1a7":"# --------------------------\n# Look at the data distribution after resampling applied\n# --------------------------\n\nfig, ax = plt.subplots()\nax.pie([len(y_sm==1),len(y_sm!=1)], labels = ['fraud', 'normal'], colors = ['blue','red'], autopct='%1.1f%%')\nax.set_title('Fraud CC Distribution')\nplt.show()","724b0f51":"from sklearn.model_selection import train_test_split","82dcb355":"# --------------------------\n# Split Data into Train and Test Set\n# --------------------------\n\nseed=42\nX_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size = 0.3, random_state = seed, stratify = y_sm)","eb796e5d":"#!pip install catboost","9a3996f3":"import catboost\nfrom catboost import CatBoostClassifier","475944c7":"#from sklearn.model_selection import RandomizedSearchCV","2a1fae5b":"#cbc = CatBoostClassifier(loss_function= 'Logloss',eval_metric = 'AUC:hints=skip_train~false', random_seed = seed)","57b109a9":"#params = {\n    #'depth': [6,8,10],\n    #'iterations': [50, 100, 200],\n    #'learning_rate': [0.01, 0.1, 0.3],\n    #'l2_leaf_reg' : [0.01, 0.1, 1],\n    #}","c5ce3b57":"#%%time\n#grid = RandomizedSearchCV(estimator=cbc, param_grid = params, cv = 3, n_jobs=-1)\n#grid.fit(X_train, y_train)","5a812bea":"#grid.best_estimator_","a8605e7b":"#grid.best_score_","06f7b16d":"#grid.best_params_","2f824546":"%%time\ncbc = CatBoostClassifier(loss_function= 'Logloss',eval_metric = 'AUC:hints=skip_train~false', random_seed = seed)\ncat_untuned = cbc.fit(X_train, y_train)\n","3568ea33":"from catboost import Pool\nfrom catboost.utils import get_roc_curve","ad551e88":"(fpr, tpr, thresholds) = get_roc_curve(cbc, Pool (X_test, y_test), plot=True)","ccf32ca5":"from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, f1_score, confusion_matrix","47f9b93e":"val_data = (X_test, y_test)","40d05ab4":"train_data = (X_train, y_train)","f2a3c5bf":"# --------------------------\n# Create Function To Perform all the metric evaluation at once\n# --------------------------\n\ndef validate(model, val_data):\n    y_pred = model.predict(val_data[0])\n    print('Accuracy =', accuracy_score(y_pred, val_data[1]))\n    print('ROC AUC =', roc_auc_score(y_pred, val_data[1]))\n    print('F1 =', f1_score(y_pred, val_data[1]))\n    print('Confusion Matrix =', confusion_matrix(y_pred, val_data[1]))","7e9fd9ca":"# --------------------------\n# Performance on training set\n# --------------------------\n\nvalidate(cbc, train_data)","f70cc7ce":"# --------------------------\n# Validation of Untuned CatBoost\n# --------------------------\n\nvalidate(cbc, val_data)","7a5a6bbb":"%%time\ncbc_tuned = CatBoostClassifier(depth = 5,\n                               iterations = 200,\n                               learning_rate = 0.1,\n                               l2_leaf_reg = 0.1,\n                               loss_function= 'Logloss',\n                               eval_metric = 'AUC:hints=skip_train~false', \n                               random_seed = seed)\ncbc_tuned.fit(X_train, y_train)","cf7024db":"(fpr, tpr, thresholds) = get_roc_curve(cbc_tuned, Pool (X_test, y_test), plot=True)","7a01192c":"# --------------------------\n# Performance on training set\n# --------------------------\n\nvalidate(cbc_tuned, train_data)","ab5976d5":"#Validation of CatBoost with Hyperparameter Tuned\nvalidate(cbc_tuned, val_data)","1ac003cd":"import xgboost\nfrom xgboost import XGBClassifier","2baade1b":"%%time\nxgb_untuned = xgboost.XGBClassifier(objective= 'binary:logistic', eval_metric = 'auc', seed= seed, use_label_encoder = False)\nxgb_untuned.fit(X_train, y_train)","6906e399":"# --------------------------\n# Performance on training set\n# --------------------------\n\nvalidate(xgb_untuned, train_data)","941f9ebf":"validate(xgb_untuned, val_data)","7aaf1609":"from sklearn.metrics import roc_curve, auc","ddf9b13a":"# --------------------------\n# Generate ROC-AUC Curve\n# --------------------------\n\npred = xgb_untuned.predict(val_data[0])\nfpr, tpr, threshold = roc_curve(pred, val_data[1])\nroc_auc = auc(fpr, tpr)","933f819f":"plt.title ('Receiver Operating Characteristic (ROC)')\nplt.plot (fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc= 'lower right')\nplt.plot ( [0,1], [0,1], 'r--')\nplt.xlim ( [0,1])\nplt.ylim ( [0,1])\nplt.xlabel ( 'True Positive Rate')\nplt.ylabel ( 'False Positive Rate')\nplt.show()","560d21de":"%%time\nxgb_tuned = xgboost.XGBClassifier(max_depth = 5, \n                                  n_estimators= 200, \n                                  learning_rate = 0.1, \n                                  reg_lambda = 0.1,\n                                  objective= 'binary:logistic', \n                                  eval_metric = 'auc', \n                                  seed= seed, \n                                  use_label_encoder = False)\nxgb_tuned.fit(X_train, y_train)","bd6d5345":"# --------------------------\n# Performance on training set\n# --------------------------\n\nvalidate(xgb_tuned, train_data)","3bb86aff":"validate(xgb_tuned, val_data)","cdaf1c43":"#Create ROC-AUC Curve\npred_ = xgb_tuned.predict(val_data[0])\nfpr, tpr, threshold = roc_curve(pred_, val_data[1])\nroc_auc = auc(fpr, tpr)","4262843b":"plt.title ('Receiver Operating Characteristic (ROC)')\nplt.plot (fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc= 'lower right')\nplt.plot ( [0,1], [0,1], 'r--')\nplt.xlim ( [0,1])\nplt.ylim ( [0,1])\nplt.xlabel ( 'True Positive Rate')\nplt.ylabel ( 'False Positive Rate')\nplt.show()","e7cd9059":"import lightgbm\nfrom lightgbm import LGBMClassifier","dc02f33d":"%%time\nlgbm_untuned = lightgbm.LGBMClassifier(objective= 'binary', metric = 'auc', seed= seed)\nlgbm_untuned.fit(X_train, y_train)","02180369":"# --------------------------\n# Performance on training set\n# --------------------------\n\nvalidate(lgbm_untuned, train_data)","22b18db0":"validate(lgbm_untuned, val_data)","f3bebb66":"# --------------------------\n# Generate ROC-AUC Curve\n# --------------------------\n\npred_lgbm = lgbm_untuned.predict(val_data[0])\nfpr, tpr, threshold = roc_curve(pred_lgbm, val_data[1])\nroc_auc = auc(fpr, tpr)","b0ba17a5":"plt.title ('Receiver Operating Characteristic (ROC)')\nplt.plot (fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc= 'lower right')\nplt.plot ( [0,1], [0,1], 'r--')\nplt.xlim ( [0,1])\nplt.ylim ( [0,1])\nplt.xlabel ( 'True Positive Rate')\nplt.ylabel ( 'False Positive Rate')\nplt.show()","81129745":"%%time\nlgbm_tuned = lightgbm.LGBMClassifier(max_depth = 5, \n                                     num_iterations = 200, \n                                     learning_rate = 0.1, \n                                     reg_lambda = 0.1,\n                                     objective= 'binary', \n                                     metric = 'auc', \n                                     seed= seed, \n                                    )\nlgbm_tuned.fit(X_train, y_train)","356a166f":"# --------------------------\n# Performance on training set\n# --------------------------\n\nvalidate(lgbm_tuned, train_data)","d62c41c3":"validate(lgbm_tuned, val_data)","3d2b46d7":"# --------------------------\n# Create ROC-AUC Curve\n# --------------------------\n\npred_lgbm_ = lgbm_tuned.predict(val_data[0])\nfpr, tpr, threshold = roc_curve(pred_lgbm_, val_data[1])\nroc_auc = auc(fpr, tpr)","1a983499":"plt.title ('Receiver Operating Characteristic (ROC)')\nplt.plot (fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc= 'lower right')\nplt.plot ( [0,1], [0,1], 'r--')\nplt.xlim ( [0,1])\nplt.ylim ( [0,1])\nplt.xlabel ( 'True Positive Rate')\nplt.ylabel ( 'False Positive Rate')\nplt.show()","39f42510":"Data have no missing value.","2a42a415":"Scale variable 'Amount',as unscaled input variables can result in a slow or unstable learning process.","47f83314":"# CATBOOST CLASSIFIER","0aec872d":"We gonna set 'depth': 5, 'iterations': 200, 'learning_rate': 0.1, l2 regularization of 0.1;  cv =3, n_jobs= -1, for hyperparamater on randomized search setting.\n","bde2cab8":"Generate a pie chart to show Fraud distribution.","684d9d9a":"Before dive in more into dataset, it's always good to take a look at the data first.","ca9155ca":"We can see that the data is highly imbalanced. Lets take a closer look to the distribution of fraud cc","da3f2748":"**LIGHTGBM WITH HYPERPARAMETER TUNING**","2ddd1ed5":"Lets see the summary of dataset","08810400":"**XGBOOST WITH HYPERPARAMETER TUNING**","3b3dc73d":"**UNTUNED LIGHT GBM**","582cd5fb":"Import model solving classification task","254e72ec":"# CatBoost with Hyperparameter Tuning","01753bff":"**TRAIN TEST SPLIT**","4be9f985":"for further details of catboost.\nhttps:\/\/catboost.ai\/en\/docs\/concepts\/python-reference_catboostclassifier","7d0b7ed3":"Percentage of fraud cc in this dataset is 0.1727 percent.","b5063e00":"# CatBoost Untuned","894ddfa9":"# XGBOOST","9657a304":"# DATA PREPROCESSING","e223e3fe":"**DEFINE X AND Y**","c8966c4c":"# LIGHT GBM","098d8e77":"**UNTUNED XGBOOST**"}}