{"cell_type":{"49a4cf91":"code","a30224d0":"code","7e498457":"code","b8ca3ada":"code","bfd9b3ef":"code","63fc1767":"code","466cc584":"code","092528fd":"code","ff3534ff":"code","9c7d12bd":"code","2cbd3f32":"code","139bd6fa":"code","912a62fd":"code","5e7645bf":"code","0533405d":"markdown","ddae5ed1":"markdown","53903a8d":"markdown","9b97830a":"markdown","813bac60":"markdown","7f6abd5b":"markdown","202e2d89":"markdown","e3d8f7ab":"markdown"},"source":{"49a4cf91":"model_handle_map = {\n  \"efficientnetv2-s\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_s\/feature_vector\/1\",\n  \"efficientnetv2-m\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_m\/feature_vector\/1\",\n  \"efficientnetv2-l\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_l\/feature_vector\/1\",\n  \"efficientnetv2-s-21k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_s\/feature_vector\/1\",\n  \"efficientnetv2-m-21k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_m\/feature_vector\/1\",\n  \"efficientnetv2-l-21k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_l\/feature_vector\/1\",\n  \"efficientnetv2-s-21k-ft1k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_s\/feature_vector\/1\",\n  \"efficientnetv2-m-21k-ft1k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_m\/feature_vector\/1\",\n  \"efficientnetv2-l-21k-ft1k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_l\/feature_vector\/1\",\n  \"efficientnetv2-b0\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_b0\/feature_vector\/2\",\n  \"efficientnetv2-b1\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_b1\/feature_vector\/1\",\n  \"efficientnetv2-b2\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_b2\/feature_vector\/1\",\n  \"efficientnetv2-b3\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_b3\/feature_vector\/1\",\n  \"efficientnet_b0\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b0\/feature-vector\/1\",\n  \"efficientnet_b1\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b1\/feature-vector\/1\",\n  \"efficientnet_b2\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b2\/feature-vector\/1\",\n  \"efficientnet_b3\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b3\/feature-vector\/1\",\n  \"efficientnet_b4\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b4\/feature-vector\/1\",\n  \"efficientnet_b5\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b5\/feature-vector\/1\",\n  \"efficientnet_b6\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b6\/feature-vector\/1\",\n  \"efficientnet_b7\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b7\/feature-vector\/1\",\n  \"bit_s-r50x1\": \"https:\/\/tfhub.dev\/google\/bit\/s-r50x1\/1\",\n  \"inception_v3\": \"https:\/\/tfhub.dev\/google\/imagenet\/inception_v3\/feature_vector\/4\",\n  \"inception_resnet_v2\": \"https:\/\/tfhub.dev\/google\/imagenet\/inception_resnet_v2\/feature-vector\/4\",\n  \"resnet_v1_50\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v1_50\/feature-vector\/4\",\n  \"resnet_v1_101\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v1_101\/feature-vector\/4\",\n  \"resnet_v1_152\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v1_152\/feature-vector\/4\",\n  \"resnet_v2_50\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v2_50\/feature_vector\/5\",\n  \"resnet_v2_101\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v2_101\/feature_vector\/5\",\n  \"resnet_v2_152\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v2_152\/feature-vector\/4\",\n  \"nasnet_large\": \"https:\/\/tfhub.dev\/google\/imagenet\/nasnet_large\/feature_vector\/4\",\n  \"nasnet_mobile\": \"https:\/\/tfhub.dev\/google\/imagenet\/nasnet_mobile\/feature_vector\/4\",\n  \"pnasnet_large\": \"https:\/\/tfhub.dev\/google\/imagenet\/pnasnet_large\/feature_vector\/4\",\n  \"mobilenet_v2_100_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v2_100_224\/feature_vector\/4\",\n  \"mobilenet_v2_130_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v2_130_224\/feature_vector\/4\",\n  \"mobilenet_v2_140_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v2_140_224\/feature_vector\/4\",\n  \"mobilenet_v3_small_100_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v3_small_100_224\/feature_vector\/5\",\n  \"mobilenet_v3_small_075_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v3_small_075_224\/feature_vector\/5\",\n  \"mobilenet_v3_large_100_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v3_large_100_224\/feature_vector\/5\",\n  \"mobilenet_v3_large_075_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v3_large_075_224\/feature_vector\/5\",\n}\n\nmodel_image_size_map = {\n  \"efficientnetv2-s\": 384,\n  \"efficientnetv2-m\": 480,\n  \"efficientnetv2-l\": 480,\n  \"efficientnetv2-s-21k\": 384,\n  \"efficientnetv2-m-21k\": 480,\n  \"efficientnetv2-l-21k\": 480,\n  \"efficientnetv2-s-21k-ft1k\": 384,\n  \"efficientnetv2-m-21k-ft1k\": 480,\n  \"efficientnetv2-l-21k-ft1k\": 480,\n  \"efficientnetv2-b0\": 224,\n  \"efficientnetv2-b1\": 240,\n  \"efficientnetv2-b2\": 260,\n  \"efficientnetv2-b3\": 300,\n  \"efficientnet_b0\": 224,\n  \"efficientnet_b1\": 240,\n  \"efficientnet_b2\": 260,\n  \"efficientnet_b3\": 300,\n  \"efficientnet_b4\": 380,\n  \"efficientnet_b5\": 456,\n  \"efficientnet_b6\": 528,\n  \"efficientnet_b7\": 600,\n  \"inception_v3\": 299,\n  \"inception_resnet_v2\": 299,\n  \"nasnet_large\": 331,\n  \"pnasnet_large\": 331,\n}","a30224d0":"import os\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nimport tensorflow_addons as tfa\n\nfrom sklearn.metrics import *\nimport scikitplot as skplt\n\nfrom functools import partial\nimport albumentations as A\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ndata = pd.read_csv('..\/input\/breakhis\/Folds.csv')\nimg_dir = '..\/input\/breakhis\/BreaKHis_v1\/'\nclass_names = ['benign', 'malignant']","7e498457":"data = data.rename(columns={'filename':'path'})\ndata['label'] = data.path.apply(lambda x: x.split('\/')[3])\ndata['label_int'] = data.label.apply(lambda x: class_names.index(x))\ndata['filename'] = data.path.apply(lambda x: x.split('\/')[-1])\ndata.head(3)","b8ca3ada":"ax = sns.displot(data=data, x='label')\nprint('Count of Benign    : ', data[data.label == 'benign'].label.count())\nprint('Count of Malignant : ', data[data.label == 'malignant'].label.count())","bfd9b3ef":"# remove 600 from dataset for testing\ntest_df = data.groupby('label').sample(n=300)\ntrain_df = data.drop(test_df.index).reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\n\n# split training and validation set\nvalid_df = train_df.sample(frac=0.2)\ntrain_df = train_df.drop(valid_df.index).reset_index(drop=True)\nvalid_df = valid_df.reset_index(drop=True)\n\ntest_df['set'] = 'test'\ntrain_df['set'] = 'train'\nvalid_df['set'] = 'valid'\ndata_new = pd.concat([train_df,valid_df, test_df])\n\nax = sns.displot(data=data_new, x='label', col='set')\n\nprint('Training set')\nprint(train_df.label.value_counts())\n\nprint('\\nValidation set')\nprint(valid_df.label.value_counts())\n\nprint('\\nTest set')\nprint(test_df.label.value_counts())","63fc1767":"# upsampling training dataset\nmax_count = np.max(train_df.label.value_counts())\nmin_count = np.min(train_df.label.value_counts())\ntrain_df = train_df.groupby('label').sample(n=max_count, replace=True)\ntrain_df = train_df.reset_index(drop=True)\ntrain_df.label.value_counts()\n\nax = sns.displot(data=train_df, x='label')","466cc584":"def parse_image(path, label):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_png(img, channels=3)\n    return img, label\n\ndef resize_rescale(image, label):\n    img = tf.cast(image, tf.float32)\n    img = tf.image.resize(img, [IMAGE_SIZE, IMAGE_SIZE])\/255\n    return img, label\n\ndef aug_fn(image): \n    transforms = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(p=0.5, limit=15),\n        A.RandomBrightnessContrast(p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.1, 0.1), brightness_by_max=True),\n        A.RandomResizedCrop(p=0.8, height=IMAGE_SIZE, width=IMAGE_SIZE, scale=(0.9, 1.1), ratio=(0.05, 1.1), interpolation=0),\n        A.Blur(p=0.3, blur_limit=(1, 1)),\n    ])\n    data = {\"image\":image}\n    aug_data = transforms(**data)\n    aug_img = aug_data[\"image\"]\n    aug_img = tf.cast(aug_img, tf.float32)\n    aug_img = tf.image.resize(aug_img, [IMAGE_SIZE, IMAGE_SIZE])\/255\n    return aug_img\n\ndef augmentor(image, label):\n    aug_img = tf.numpy_function(func=aug_fn, inp=[image], Tout=tf.float32)\n    return aug_img, label\n\ndef view_image(ds, col=8, row=2, size=(25,7)):\n    plt.figure(figsize=size)\n    plt.subplots_adjust(wspace=0.05, hspace=0.15)\n    for images, labels in ds.take(1):\n        for i in range(col*row):\n            ax = plt.subplot(row, col, i + 1)\n            shape = str(images[i].numpy().shape)\n            plt.imshow(images[i].numpy())\n            plt.title(class_names[labels[i].numpy()])\n            plt.axis(\"off\") \n    plt.tight_layout\n    return None\n\ndef training_history(history):\n    accuracy = history['accuracy']\n    val_accuracy = history['val_accuracy']\n\n    loss = history['loss']\n    val_loss = history['val_loss']\n\n    epochs_range = range(len(history['loss']))\n\n    plt.figure(figsize=(16, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, accuracy, label='Training accuracy')\n    plt.plot(epochs_range, val_accuracy, label='Validation accuracy')\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Loss')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n\n    plt.show()\n    return None\n\ndef decode_test(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_png(img, channels=3)\n    img = tf.cast(img, tf.float32)\n    img = tf.image.resize(img, [224, 224])\/255\n    return img\n\ndef build_network(image_size):\n    print('building model...')\n    model = tf.keras.Sequential([\n        layers.InputLayer(input_shape=(image_size, image_size, 3)),\n        hub.KerasLayer(model_handle, trainable=True, name='base_model'),\n        layers.Dense(512, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.5),\n        layers.Dense(128, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dense(1, activation='sigmoid', name='classifier') \n    ],name=model_name)\n    model.build((None, image_size, image_size, 3))\n    model.summary()\n    print('model loaded!!!')\n    return model\n\ndef view_prediction():\n    plt.figure(figsize=(25,8))\n    plt.rcParams.update({'font.size': 8})\n    plt.subplots_adjust(wspace=0.05, hspace=0.15)\n    for i in range(30):\n        ax = plt.subplot(3, 10, i + 1)\n        shape = str(test_img[i].numpy().shape)\n        plt.imshow(test_img[i].numpy())\n        plt.title(pred_label[i][0])\n        plt.axis(\"off\") \n        plt.tight_layout\n    return None \n\ndef view_wrong_prediction(df):\n    plt.figure(figsize=(len(df)*4,8))\n    plt.rcParams.update({'font.size': 8})\n    plt.subplots_adjust(wspace=0.05, hspace=0.15)\n    for i in range(len(df)):\n        img = decode_test(img_dir+df.path.iloc[i])\n        ax = plt.subplot(1, len(df), i + 1)\n        plt.imshow(img)\n        plt.title('wrongly predicted as '+df.prediction.iloc[i])\n        plt.axis(\"off\") \n        plt.tight_layout\n    return None ","092528fd":"model_name = 'efficientnetv2-b0'\nmodel_handle = model_handle_map.get(model_name)\nIMAGE_SIZE = model_image_size_map.get(model_name, 224)\nBATCH_SIZE = 64\nEPOCHS = 12\nSAMPLE_SIZE = len(train_df)\n\nprint(f\"Selected model: {model_name} : {model_handle}\")\nprint(f\"Input size {IMAGE_SIZE}\")","ff3534ff":"train_loader = tf.data.Dataset.from_tensor_slices((img_dir+train_df.path, train_df.label_int))\nvalid_loader = tf.data.Dataset.from_tensor_slices((img_dir+valid_df.path, valid_df.label_int))\n\ntrain_ds = (\n    train_loader.shuffle(len(train_df))\n    .map(parse_image, num_parallel_calls=AUTOTUNE)\n    .map(partial(augmentor),num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTOTUNE) \n)\nvalid_ds = (\n    valid_loader.shuffle(len(valid_df))\n    .map(parse_image, num_parallel_calls=AUTOTUNE)\n    .map(resize_rescale, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTOTUNE)\n)","9c7d12bd":"view_image(train_ds)","2cbd3f32":"tf.keras.backend.clear_session()\nmodel = build_network(IMAGE_SIZE)\n\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True)\nclr_scheduler = tfa.optimizers.CyclicalLearningRate( \n    initial_learning_rate=2e-1,  maximal_learning_rate=7e-3, \n    step_size=3*(SAMPLE_SIZE\/\/BATCH_SIZE),  \n    scale_fn=lambda x: 1 \/ (2.0 ** (x - 1)), \n    scale_mode='cycle'\n)\nMETRICS = [\n    'accuracy',\n    tf.keras.metrics.Precision(name='precision'),\n    tf.keras.metrics.Recall(name='recall'),\n]\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.SGD(learning_rate=clr_scheduler) , \n    loss=tf.keras.losses.BinaryCrossentropy(), \n    metrics=METRICS\n)\n\nhistory = model.fit(\n    train_ds, \n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    verbose=1,\n    callbacks = [checkpoint_cb],\n    validation_data=valid_ds,\n)\ntraining_history(history.history)","139bd6fa":"test_df = test_df.sample(frac=1).reset_index(drop=True)\ntest_ds = tf.data.Dataset.from_tensor_slices(img_dir+test_df.path) \ntest_ds = test_ds.map(decode_test,num_parallel_calls=AUTOTUNE).batch(len(test_df))\ntest_img = next(iter(test_ds))\ntest_index = test_df.label_int.values\ntest_label = test_df.label.values\n\ntest_pred = model.predict(test_ds)\npred_index = np.round(test_pred).astype('uint8')\npred_label = np.array(class_names)[pred_index]\n\nprint(classification_report(test_index, pred_index, target_names=class_names,zero_division=0))\nprint('f1_score        :', f1_score(test_index, pred_index, average='micro'))\nprint('accuracy_score  :', accuracy_score(test_index, pred_index))\n\ncm = skplt.metrics.plot_confusion_matrix(test_label, pred_label, figsize=(8, 8), normalize=False)","912a62fd":"prediction_df = pd.DataFrame({'filename':test_df.filename.values,'actual':test_df.label.values, 'prediction': np.squeeze(pred_label),'path':test_df.path.values,})\nwrong_df = prediction_df[prediction_df.actual != prediction_df.prediction].reset_index(drop=True)\n\n#view first 30 prediction\nview_prediction()","5e7645bf":"#view wrong prediction\nview_wrong_prediction(wrong_df)","0533405d":"## View sample images","ddae5ed1":"## Evaluate neural network performance","53903a8d":"## Construct neural network & start training!!!","9b97830a":"## Define Helpers","813bac60":"### Import Libraries","7f6abd5b":"## Load Dataset","202e2d89":"### Process Dataset","e3d8f7ab":"# Breast Cancer Histopathology Images Classification\n![https:\/\/pathology.jhu.edu\/build\/assets\/breast\/_gallery\/invasive-lobular-carcinoma.jpg](https:\/\/pathology.jhu.edu\/build\/assets\/breast\/_gallery\/invasive-lobular-carcinoma.jpg)\n## Introduction\nBreast cancer is a common cancer in women, and one of the major causes of death among women around the world with 627,000 deaths among 2.1 million diagnosed cases in 2018. Invasive ductal carcinoma (IDC) is the most widespread type of breast cancer with about 80% of all diagnosed cases. Early accurate diagnosis plays an important role in choosing the right treatment plan and improving survival rate among the patients. Microscopic evaluation of histopathalogic stained tissue & its subsequent digitalisation is now a more feasible due to the advances in slide scanning technology, as well a reduction in digital storage cost in recent years. There are certain advantages that come with such digitalised pathology; including remote diagnosis, instant archival access & simplified procedure of consultations with expert pathologists. Digitalised Analysis based on Deep Learning has shown potential benefits as a potential diagnosis tool & strategy.\n\n## Challenges\nThe sensitivity of SLN assessment by pathologists, however, is not optimal. A retrospective study showed that pathology review by experts changed the nodal status in 24% of patients.\nSLN assessment is tedious and time-consuming. It has been shown that deep learning algorithms could identify metastases in SLN slides with 100% sensitivity, whereas 40% of the slides without metastases could be identified as such. This could result in a significant reduction in the workload of pathologists.\n\n## Dataset\nThe Breast Cancer Histopathological Image Classification (BreakHis) is composed of 9,109 microscopic images of breast tumor tissue collected from 82 patients using different magnifying factors (40X, 100X, 200X, and 400X). To date, it contains 2,480 benign and 5,429 malignant samples (700X460 pixels, 3-channel RGB, 8-bit depth in each channel, PNG format). This database has been built in collaboration with the P&D Laboratory \u2013 Pathological Anatomy and Cytopathology, Parana, Brazil (http:\/\/www.prevencaoediagnose.com.br). We believe that researchers will find this database a useful tool since it makes future benchmarking and evaluation possible."}}