{"cell_type":{"03d75214":"code","6ba9ee4f":"code","9ac278f8":"code","f34038ab":"code","f920c740":"code","f1744f5a":"code","380ae238":"code","ca9443f4":"code","aa54c29b":"code","eea6cfed":"code","3f5da664":"code","685ba9af":"code","12d3a051":"code","40fe035f":"code","6b88ee6e":"code","67a2c165":"code","fedc9534":"code","6f9a31a1":"code","3d579169":"code","83dade14":"code","2f5b96e6":"code","93d8bf42":"code","f854939b":"code","a5c74ba6":"code","48cfcdcb":"code","deef125c":"code","fa1711c8":"code","d8478510":"code","af911b90":"code","1e366f36":"code","95a598f6":"code","661f3cd4":"code","9fe6d433":"code","02f6f5fd":"code","b5fecb15":"code","4120ef3b":"code","38123633":"code","0682f6a1":"code","557c49e3":"code","1d48366e":"markdown","3318c732":"markdown","a68524c6":"markdown","ff09f31c":"markdown","fbc647bf":"markdown","330c2d79":"markdown","955fb4f6":"markdown","37063edf":"markdown","c2d70b48":"markdown","4cbef799":"markdown","5a293f24":"markdown","5e40d749":"markdown","351a0d4a":"markdown","ab2829aa":"markdown","3ff77a11":"markdown","1b27bf5d":"markdown","0b71e18b":"markdown","3d37a5d8":"markdown","663dc201":"markdown","5a5764fc":"markdown","9886f487":"markdown","186ae68a":"markdown","1473bc30":"markdown","969c6ddc":"markdown","6927c49f":"markdown","83a116b8":"markdown","9c4bcc09":"markdown","b6255fb1":"markdown","28de7ad8":"markdown","d93df5e6":"markdown"},"source":{"03d75214":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt     \nimport seaborn as sns\n\nimport string\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM\nfrom keras.layers import Convolution1D, GlobalMaxPooling1D,GlobalAveragePooling1D\nfrom keras.layers import Bidirectional, SpatialDropout1D, GRU\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom keras.utils import to_categorical\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.optimizers import RMSprop, Adam\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\n\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\nimport pickle\nimport time\nimport gc\n## warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6ba9ee4f":"train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')","9ac278f8":"# Missing values\ntrain.isnull().sum()","f34038ab":"print(train.shape, '\\n')\ntrain.info()","f920c740":"train.head()","f1744f5a":"plt.figure(figsize=(10,6))\ngraph_1 = sns.distplot(train[train['target'] > 0]['target'], color = 'red')\nplt.title('Toxicity (Target) Distribution')\nplt.xlabel(\"Toxicity Rate\")\nplt.ylabel(\"Distribution\") \nplt.show()","380ae238":"comment_adjective = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']\n\nplt.figure(figsize=(15,6))\n\nfor col in comment_adjective:\n    graph_2 = sns.distplot(train[train[col] > 0][col], label=col, hist=False)\n    plt.xlabel(\"Rate\", fontsize=16)\n    plt.ylabel(\"Distribution\", fontsize=16)\n    plt.legend(loc=1, prop={'size': 14})\n\nplt.show()","ca9443f4":"GLOVE_EMBEDDING_PATH = \"..\/input\/glove-6b-100d\/glove.6B.100d.txt\"","aa54c29b":"# This function is for if you use pickled files of pre-trained word vectors\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\n# glove_embeddings = load_embeddings(GLOVE_EMBEDDING_PATH)\n# print('Found and loaded {} word vectors'.format(len(glove_embeddings)))","eea6cfed":"# Use the method below if you are using text files of pre-trained word vectors\n\"\"\"\nprint('Loading embeddings...')\nembeddings_index = {}\nwith open(EMBEDDINGS_PATH) as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nembedding_matrix = np.zeros((len(tokenizer.word_index) + 1,\n                                 EMBEDDINGS_DIMENSION))\nnum_words_in_embedding = 0\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        num_words_in_embedding += 1\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\nprint(\"\\nEmbeddings loaded!\")\n\"\"\"","3f5da664":"# \"check_coverage\" goes through a given vocabulary and tries to find word vectors in embedding matrix. \"build_vocab\" builds a ordered dictionary of words and their frequency in the text corpus.\n\nimport operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\n#vocab = build_vocab(list(train['comment_text'].apply(lambda x:x.split())))\n#oov = check_coverage(vocab,glove_embeddings)\n#oov[:10]\n\n### Symbols in GloVe\n\n\"\"\"\nletter_digit_list = string.ascii_letters + string.digits + ' '\nletter_digit_list += \"'\"\n\n# Symbols that have embedding vectors in GloVe:\n\nglove_chars = ''.join([c for c in tqdm(glove_embeddings) if len(c) == 1])\nglove_symbols = ''.join([c for c in glove_chars if not c in letter_digit_list])\nglove_symbols\n\n# Symbols that have no embedding vectors in GloVe:\n\njigsaw_chars = build_vocab(list(train[\"comment_text\"]))\n\njigsaw_symbols = ''.join([c for c in jigsaw_chars if not c in letter_digit_list])\njigsaw_symbols\n\nsymbols_to_delete = ''.join([c for c in jigsaw_symbols if not c in glove_symbols])\nsymbols_to_delete\n\nsymbols_to_isolate = ''.join([c for c in jigsaw_symbols if c in glove_symbols])\nsymbols_to_isolate\n\ndel glove_embeddings\ndel vocab\ndel glove_chars\ndel glove_symbols\n\ngc.collect()\n\"\"\"","685ba9af":"#tokenizer = TreebankWordTokenizer()\n\n\n#isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n#remove_dict = {ord(c):f'' for c in symbols_to_delete}\n\ndef cleaning_text(x):\n    punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~`\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    x = clean_special_chars(x, punct)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = cleaning_text(x)\n    x = fix_quote(x)\n    return x","12d3a051":"# Make sure all comment_text values are strings\ntrain['comment_text'] = train['comment_text'].astype(str) \n\n# List all identities\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\n# Convert target and identity columns to booleans\ndef convert_to_bool(df, col_name):\n    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n    \ndef convert_dataframe_to_bool(df):\n    bool_df = df.copy()\n    for col in ['target'] + identity_columns:\n        convert_to_bool(bool_df, col)\n    return bool_df\n\ntrain = convert_dataframe_to_bool(train)","40fe035f":"train['comment_text'] = train['comment_text'].progress_apply(lambda x:preprocess(x))","6b88ee6e":"train_df, validate_df = model_selection.train_test_split(train, test_size=0.2)\nprint('%d train comments, %d validate comments' % (len(train_df), len(validate_df)))","67a2c165":"MAX_NUM_WORDS = 10000\nTARGET_COLUMN = 'target'\nTEXT_COLUMN = 'comment_text'","fedc9534":"# Create a text tokenizer.\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(train[TEXT_COLUMN])\n\n# All comments must be truncated or padded to be the same length.\nMAX_SEQUENCE_LENGTH = 250\ndef pad_text(texts, tokenizer):\n    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)","6f9a31a1":"\"\"\"\n!kaggle datasets download -d fizzbuzz\/cleaned-toxic-comments\n\n!unzip \".\/cleaned-toxic-comments.zip\"\n\npre_data = \".\/train_preprocessed.csv\"\n\n\n## Get the Corpus of all the comments and related Toxicity fields\n\ndata = pd.read_csv(pre_data)\ndata.head()\n\n## Dividing the dataset into features and labels:\nFeatures = \"comment\"            \nLabels = \"toxicity\"\n\nFeatures = data['comment_text']\nLabels = np.array([0 if y == 0 else 1 for y in data['toxicity']])\n\n### Tokenizing and preprocessing the data\n\nNUM_WORDS = 40000 # Maximum number of unique words which need to be tokenized\nMAXLEN = 50 # Maximum length of a sentence\/ comment\nPADDING = 'post' # The type of padding done for sentences shorter than the Max len\n\ntokenizer = Tokenizer(num_words=NUM_WORDS)\n\n# Fit the tokenizer on the comments \ntokenizer.fit_on_texts(Features)\n\n# Get the word index of the top 20000 words from the dataset\nword_idx = tokenizer.word_index\n\n# Convert the string sentence to a sequence of their numerical values\nFeature_sequences = tokenizer.texts_to_sequences(Features)\n\n# Pad the sequences to make them of uniform length\npadded_sequences = pad_sequences(Feature_sequences, maxlen = MAXLEN, padding = PADDING)\n\nprint(\"The Transformation of sentence::\")\nprint(\"\\n\\nThe normal Sentencen:\\n\")\nprint(Features[2])\nprint(\"\\n\\nThe tokenized sequence:\\n\")\nprint(Feature_sequences[2])\nprint(\"\\n\\nThe padded sequence:\\n\")\nprint(padded_sequences[2])\n\n# Convert to array for passing through the model\nX = np.array(padded_sequences)\n\"\"\"","3d579169":"EMBEDDINGS_PATH = '..\/input\/glove-6b-100d\/glove.6B.100d.txt'\nEMBEDDINGS_DIMENSION = 100\nLEARNING_RATE = 0.0005\nNUM_EPOCHS = 5\nBATCH_SIZE = 128","83dade14":"# Prepare data\ntrain_text = pad_text(train_df[TEXT_COLUMN], tokenizer)\ntrain_label = to_categorical(train_df[TARGET_COLUMN])","2f5b96e6":"del train\ngc.collect()","93d8bf42":"X_train, X_test, y_train, y_test=train_test_split(train_text, train_label, test_size=0.20, random_state=42)","f854939b":"gc.collect()","a5c74ba6":"# Load embeddings\nprint('loading embeddings')\nembeddings_index = {}\nwith open(EMBEDDINGS_PATH) as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nembedding_matrix = np.zeros((len(tokenizer.word_index) + 1,\n                                 EMBEDDINGS_DIMENSION))\nnum_words_in_embedding = 0\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        num_words_in_embedding += 1\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\nprint(\"Embeddings loaded!\")","48cfcdcb":"# Create model layers.\nmodel=Sequential()\nmodel.add(Embedding(len(tokenizer.word_index) + 1,100,input_length = 100,weights = [embedding_matrix],trainable = False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Bidirectional(LSTM(128,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(128,return_sequences=True)))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(512,activation = 'relu'))\nmodel.add(Dense(512,activation = 'relu'))\nmodel.add(Dense(2,activation='softmax'))\n\n# Compile model.\nprint('Compiling model...')\nmodel.compile(loss='categorical_crossentropy',\n                  optimizer=Adam(lr=LEARNING_RATE),\n                  metrics=['acc'])\nprint(\"Compiled model!\")","deef125c":"model.summary()","fa1711c8":"\"\"\"\nhistory = model.fit(\n            X, \n            Labels,\n            batch_size = 128,\n            epochs = 10,\n            validation_split = 0.2, # 20 percent data reserved for validation to avoid or monitor overfitting\/ underfitting\n            verbose = verbose,\n        )\n\"\"\"","d8478510":"model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=\"best_lstm_toxic.h5\",\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)","af911b90":"# Train model.\nprint('Training model...')\nstart = time.time()\nhistory = model.fit(X_train,\n              y_train,\n              batch_size=BATCH_SIZE,\n              epochs= NUM_EPOCHS,\n              validation_data=(X_test, y_test),\n              verbose=1, callbacks = [model_checkpoint_callback])\n\nend = time.time()\nprint(\"\\nTraining duration: {} minutes\".format(str((end-start)\/60)))","1e366f36":"MODEL_NAME = 'lstm_model'\nvalidate_df[MODEL_NAME] = model.predict(pad_text(validate_df[TEXT_COLUMN], tokenizer))[:, 1]","95a598f6":"validate_df.head()","661f3cd4":"SUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]]\n    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[df[subgroup] & df[label]]\n    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n\nbias_metrics_df = compute_bias_metrics_for_model(validate_df, identity_columns, MODEL_NAME, TARGET_COLUMN)\nbias_metrics_df","9fe6d433":"def calculate_overall_auc(df, model_name):\n    true_labels = df[TARGET_COLUMN]\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total \/ len(series), 1 \/ p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n    \nget_final_metric(bias_metrics_df, calculate_overall_auc(validate_df, MODEL_NAME))","02f6f5fd":"del X_train, y_train, X_test, y_test\ngc.collect()","b5fecb15":"#model.save('..\/toxicity_classifier.h5')","4120ef3b":"# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","38123633":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","0682f6a1":"test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\nsubmission = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv', index_col='id')","557c49e3":"submission['prediction'] = model.predict(pad_text(test[TEXT_COLUMN], tokenizer))[:, 1]\nsubmission.to_csv('submission.csv')","1d48366e":"## Train Data","3318c732":"## Exploratory Data Analysis","a68524c6":"### Visualizing Non-null Target distribuition","ff09f31c":"## Data Pre-processing","fbc647bf":"# Training","330c2d79":"### Keeping 20% data to validate on bias metric","955fb4f6":"### Functions for cleaning the text strings","37063edf":"### Hyper-parameters","c2d70b48":"### Embedding Matrix","4cbef799":"# ======== Phase 3 ========","5a293f24":"### Embeddings\n\nGloVe pre-trained word vectors: https:\/\/nlp.stanford.edu\/projects\/glove\/\n\n6B tokens, 100d vectors","5e40d749":"### Validating model's performance from bias perspective","351a0d4a":"References:\n* https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-for-glove-part1-eda\n* https:\/\/www.kaggle.com\/dborkan\/benchmark-kernel","ab2829aa":"## Pre-training Dataset","3ff77a11":"### Checkpoint to obtain best model","1b27bf5d":"### Final submission","0b71e18b":"### Creating Vocabulary","3d37a5d8":"# Phase 1","663dc201":"### Train-Test(=Validation) Split","5a5764fc":"## Pre-training on external data (Skipping due to hardware limitation)","9886f487":"# Phase 2","186ae68a":"### Tokenization and Padding","1473bc30":"### Non-null Sub-classes Distribution","969c6ddc":"### Defining Bias Metrices","6927c49f":"## Final Training on competition data","83a116b8":"# ======== Phase 4 ========","9c4bcc09":"### Recurrent Neural Network","b6255fb1":"### Plotting","28de7ad8":"## Libraries and Packages","d93df5e6":"### Converting target (float) values to booleans using identity columns"}}