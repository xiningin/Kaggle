{"cell_type":{"5c57d580":"code","f1aa6b82":"code","fdaf363a":"code","d33eecad":"code","a2dacb01":"code","bc0ef58c":"code","9b392943":"code","622622ca":"code","b826aa6a":"code","213f119a":"code","5b67b62e":"code","0709cd1c":"code","542ee184":"code","b9c06c30":"code","ff451ede":"code","c57b7cee":"code","c425ff59":"code","40a5ba33":"code","506d220c":"code","333a1d0b":"code","ed74aab5":"code","06b45eae":"code","c09e90aa":"code","a27aa5ba":"code","8c1843ce":"code","211c06df":"code","00cb4904":"code","b7b5dbf0":"code","95745c10":"code","c4f2fcfa":"code","365a5c9b":"code","cb6f1d60":"markdown","53ed293f":"markdown","48269f49":"markdown","e86e2456":"markdown","4adaa9c4":"markdown","ccc7e04f":"markdown","f1fb3c21":"markdown","11dc8fe6":"markdown","73c26cfe":"markdown","679e78e5":"markdown","7b5ca7e2":"markdown","ee03373e":"markdown","9ea2310f":"markdown","763c467c":"markdown","d090bbc8":"markdown","eedd038a":"markdown","cb33d02a":"markdown","df4e3340":"markdown","a593053d":"markdown"},"source":{"5c57d580":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize'] = (16, 8)\nplt.style.use('fivethirtyeight')","f1aa6b82":"df = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head()","fdaf363a":"df.columns = df.columns.str.lower().str.replace(' ', '_')\n\ncategorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n\nfor c in categorical_columns:\n    df[c] = df[c].str.lower().str.replace(' ', '_')\n    \ndf.head()","d33eecad":"df.info()","a2dacb01":"df['totalcharges'].value_counts()","bc0ef58c":"\"\"\"\nWe saw above that total charges column is object type\nHence we convert that to numerical column\nwe also see that when we replaced \" \" by \"_\" missing values here got replaced by \"_\"\n\"\"\"\n# We convert totalcharges to numerical and to ignore the errors (in our case  \"_\") we use errors='coerce'\n# Store the results back to the original column\ndf.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n\n#  Finally take care of the the missing values in totalcharges columns by filling up by 0.\ndf.totalcharges = df.totalcharges.fillna(0)","9b392943":"\"\"\"Next we convert YES\/NO values in churn variable to int 1\/0 and store it back to original col\"\"\"\ndf.churn = (df.churn == 'yes').astype(int)","622622ca":"\"\"\"We are using sklearn train_test_split function to split the data into full_train and test set\"\"\"\nfrom sklearn.model_selection import train_test_split\n\n# Making full_train and test set\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n\n# Next making train and validation set from full_train set\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n\n# checking the number of values in datasets\nprint(\"Train_set %s:\" %len(df_train), \"Validation_set %s:\" %len(df_val), \"Test_set %s:\" %len(df_test))\n\n# Dropping indexes of these datasets\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\n# Creating Dependent Variables\ny_train = df_train.churn.values\ny_val = df_val.churn.values\ny_test = df_test.churn.values\n\n# Deleting target column i.e. churn from training datasets\ndel df_train['churn']\ndel df_val['churn']\ndel df_test['churn']","b826aa6a":"\"\"\"Removing the index column\"\"\"\ndf_full_train = df_full_train.reset_index(drop=True)\n\n\"\"\"Checking for missing values\"\"\"\ndf_full_train.isnull().sum()","213f119a":"\"\"\"Checking the target variable- churn, for the ratio of 0s and 1s\nalso known as Global Churn Rate\"\"\"\nprint(df_full_train['churn'].value_counts(normalize = True))\n\n\"\"\"Also means Global Churn Rate of 1s as when adding in mean only 1 will be adding.\"\"\"\nprint(df_full_train['churn'].mean())\n\n\n\"\"\"Creating Categorical and Numerical Columns\"\"\"\nuseful_features = [col for col in df_full_train.columns if col not in ['customerid', 'churn']]\ncategorical = [col for col in useful_features if df_full_train[col].dtype == 'object'] + ['seniorcitizen']\nnumerical = [col for col in useful_features if col not in categorical]","5b67b62e":"# Library to use display multiple output, especially when using loops\nfrom IPython.display import display\n\n\"\"\"Calculating Global Churn Rate\"\"\"\nglobal_churn = df_full_train.churn.mean()\n\n\"\"\"Creating a for loop to display Differences and Risk Ratio for different groups\"\"\"\nfor c in categorical:\n    print(c)\n    df_group = df_full_train.groupby(c)['churn'].agg(['mean', 'count'])\n    df_group['diff'] = df_group['mean'] - global_churn\n    df_group['risk'] = df_group['mean'] \/ global_churn\n    display(df_group.style.background_gradient(\"bone_r\"))\n    print()\n    print()","0709cd1c":"\"\"\"Library for mutual Information\"\"\"\nfrom sklearn.metrics import mutual_info_score\n\n\n\"\"\"Defining a function to calculate mutaul info score and it takes only one variable\"\"\"\ndef mutual_info_churn_score(series):\n    return mutual_info_score(series, df_full_train.churn)\n\n\"\"\"Applying the above function to categorical variables\"\"\"\nmi = df_full_train[categorical].apply(mutual_info_churn_score)\nmi.sort_values(ascending = False).to_frame().reset_index().rename({'index': 'Variables', 0: 'Improtance'}, axis = 1).style.background_gradient('crest')","542ee184":"\"\"\"Checking the correaltion of numerical Columns with Target variable churn\"\"\"\n\ndf_full_train[numerical].corrwith(df_full_train.churn)","b9c06c30":"\"\"\"Correlation matrix\"\"\"\ndf[numerical].corr()","ff451ede":"\"\"\"Using heatmap\"\"\"\nsns.heatmap(df[numerical].corr(), square = True, annot = True, lw = 0.2)","c57b7cee":"\"\"\"Checking the correlation by groups in Numeric columns\"\"\"\nprint(\"Mean Churn Rate when Tenure <= 2: \",df_full_train[df_full_train.tenure <= 2].churn.mean())\nprint(\"Mean Churn Rate when Tenure between 2 and 12: \",df_full_train[(df_full_train.tenure > 2) & (df_full_train.tenure <= 12)].churn.mean())\nprint(\"Mean Churn Rate when Tenure more than 12: \",df_full_train[df_full_train.tenure > 12].churn.mean())\nprint()\nprint(\"Mean Churn Rate when monthlycharges <= 20: \",df_full_train[df_full_train.monthlycharges <= 20].churn.mean())\nprint(\"Mean Churn Rate when monthlycharges between 20 and 50: \",df_full_train[(df_full_train.monthlycharges > 20) & (df_full_train.monthlycharges <= 50)].churn.mean())\nprint(\"Mean Churn Rate when monthlycharges more than 50: \",df_full_train[df_full_train.monthlycharges > 50].churn.mean())","c425ff59":"from sklearn.feature_extraction import DictVectorizer\n\ndv = DictVectorizer(sparse=False)                              \n\ntrain_dict = df_train[useful_features].to_dict(orient='records') # For making dict row-wise we use orient = 'records\nX_train = dv.fit_transform(train_dict)\n\nval_dict = df_val[useful_features].to_dict(orient='records') # For making dict row-wise we use orient = 'records'\nX_val = dv.transform(val_dict)","40a5ba33":"def sigmoid(z):\n    return 1 \/ (1 + np.exp(-z))","506d220c":"def linear_regression(xi):\n    result = w0\n    \n    for j in range(len(w)):\n        result = result + xi[j] * w[j]\n        \n    return result\n\ndef logistic_regression(xi):\n    score = w0\n    \n    for j in range(len(w)):\n        score = score + xi[j] * w[j]\n        \n    result = sigmoid(score)\n    return result","333a1d0b":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver='lbfgs')\n# solver='lbfgs' is the default solver in newer version of sklearn\n# for older versions, you need to specify it explicitly\nmodel.fit(X_train, y_train)","ed74aab5":"\"\"\"model intercept: Bias Term (w0)\"\"\"\nprint(\"w0 = \", model.intercept_[0])\n\n\"\"\"Coeffficients: Weights (wi)\"\"\"\nprint(\"wi = \",model.coef_[0].round(3))","06b45eae":"\"\"\"Applying the model to Validation set to predict the probability\"\"\"\ny_pred = model.predict_proba(X_val)[:, 1]\n\n\"\"\"Making churn decision using probability threshold of 0.5\"\"\"\nchurn_decision = (y_pred >= 0.5)\n\n\"\"\"Accuracy Rate: mean of correct predictions\"\"\"\n(y_val == churn_decision).mean()","c09e90aa":"\"\"\"Observe the above result in a dataset\"\"\"\ndf_pred = pd.DataFrame()\ndf_pred['probability'] = y_pred\ndf_pred['prediction'] = churn_decision.astype(int)\ndf_pred['actual'] = y_val\n\ndf_pred['correct'] = df_pred.prediction == df_pred.actual\ndisplay(df_pred)\ndisplay(\"Accuracy Rate\", df_pred['correct'].mean())","a27aa5ba":"\"\"\"Understanding the use of Zip function\"\"\"\na = [1, 2, 3, 4]\nb = 'abcd'\n\n\"\"\"zip() comnines the both values by position\"\"\"\ndict(zip(a, b))","8c1843ce":"\"\"\"Now using zip to combine features and their respective coefficients\"\"\"\ndict(zip(dv.get_feature_names(), model.coef_[0].round(3)))","211c06df":"\"\"\"Now to understand Interpretation: Let's take small subset of the variables\"\"\"\nsmall = ['contract', 'tenure', 'monthlycharges']\n\n\"\"\"Create a model on these subset of columns\"\"\"\ndf_train[small].iloc[:10].to_dict(orient='records')\n\n# Creating training and Validation set\ndicts_train_small = df_train[small].to_dict(orient='records')\ndicts_val_small = df_val[small].to_dict(orient='records')\n\n# Using dictVectorizer for OHE on categorical colums\ndv_small = DictVectorizer(sparse=False)\ndv_small.fit(dicts_train_small)\n\n# Checking the feature names to see how many new created by DictVectorizer\ndv_small.get_feature_names()","00cb4904":"\"\"\"Creating Training set\"\"\"\nX_train_small = dv_small.transform(dicts_train_small)\n\n\"\"\"making model on Training set\"\"\"\nmodel_small = LogisticRegression(solver='lbfgs')\nmodel_small.fit(X_train_small, y_train)\n\n\"\"\"Assigning variables w0 and wi's to bias term and weights\"\"\"\nw0 = model_small.intercept_[0]\nwi = model_small.coef_[0].round(3)\n\n\"\"\"Finally zipping w0 and wi understand interpret the model prediction\"\"\"\ndict(zip(dv_small.get_feature_names(), wi.round(3)))","b7b5dbf0":"\"\"\"\nWe are given following info about the customer:\n- Contract : Two years\n- monthly_charges : 30\n- Tenure : 24\n\nWe also know the coeff\/w of:\n- Contract : two years = -0.949\n- monthlycharges = 0.027\n- tenure' = -0.036\n\nAlso bias term(w0) = -2.47\n\nBasic Interpretation: When we dont know anything about the customer, customer is less likely to churn\n\nUsing our Linear Formula: \nw0 + w1.x1 + w2.x2 + . . .\nwe get:\n\"\"\"\n\nprint(-2.47 + (-0.949) + 30 * 0.027 + 24 * (-0.036))\n\n# Finally using sigmoid function on this value will give us the Probability of customer will churn or not\nsigmoid(-3.473)","95745c10":"\"\"\"Now we train the model on the full_train set and predict on test set\"\"\"\n\ndicts_full_train = df_full_train[categorical + numerical].to_dict(orient='records')\n\ndv = DictVectorizer(sparse=False)\nX_full_train = dv.fit_transform(dicts_full_train)\n\ny_full_train = df_full_train.churn.values\n\nmodel = LogisticRegression(solver='lbfgs')\nmodel.fit(X_full_train, y_full_train)\n\ndicts_test = df_test[categorical + numerical].to_dict(orient='records')\n\nX_test = dv.transform(dicts_test)\n\ny_pred = model.predict_proba(X_test)[:, 1]\n\nchurn_decision = (y_pred >= 0.5)\n\nprint(\"Accuracy on Test Set: \", (churn_decision == y_test).mean())","c4f2fcfa":"\"\"\"Predicting on an individual customer to check how the model behaves\"\"\"\ncustomer = dicts_test[-1]\ncustomer","365a5c9b":"X_small = dv.transform([customer])\nprint(\"Predicted Probability: \",model.predict_proba(X_small)[0, 1])\nprint(\"Original Value: \",y_test[-1])","cb6f1d60":"[back to top](#table-of-contents)\n<a id=\"14\"><\/a>\n\n<div style=\"background:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">14.  Summary\n<ul style= \"font-size:15px;font-family:cursive;color:white\">\n<li >Feature importance - risk, mutual information, correlation<\/li>\n<li >One-hot encoding can be implemented with DictVectorizer<\/li>\n<li >Logistic regression - linear model like linear regression<\/li>\n<li >Output of log reg - probability<\/li>\n<li >Interpretation of weights is similar to linear regression<\/li><\/ul>\n<\/div>","53ed293f":"# <div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:15px;color:white\">1. Difference = Group Churn Rate - Global Churn Rate <\/div>\n- **If this value is less than 0 : ``More Likely  to  churn``**\n- **If this value is more than 0 : ``Less Likely  to  churn``**\n\n# <div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:15px;color:white\">2. Risk Ratio = Group Churn Rate \/ Global Churn Rate <\/div>\n- **If this value is more than 1 : ``More Likely  to  churn``**\n- **If this value is less than 1 : ``Less Likely  to  churn``**\n","48269f49":"[back to top](#table-of-contents)\n<a id=\"10\"><\/a>\n\n<div style=\"background:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">10. Logistic Regression<ul style= \"font-size:15px;font-family:cursive;color:white\">\n<li >Similar to Linear Regression as both of them are linear models.<\/li>\n<li >Logistic Regression uses sigmoid function over the score of dot product and hence converts the results into the range of 0,1.<\/li>\n<li >Sigmoid Function = $\\large g(x_i) = \\Large \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-x} }  $<\/li>\n<li >Linear regression is straightforward to understand and explain, and can be regularized to avoid overfitting. In addition, linear models can be updated easily with new data using stochastic gradient descent.<\/li><\/ul>\n<\/div> ","e86e2456":"[back to top](#table-of-contents)\n<a id=\"12\"><\/a>\n\n<div style=\"background:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">12.  Model interpretation\n    <ul style= \"font-size:15px;font-family:cursive;color:white\">\n<li > Look at the coefficients<\/li>\n<li >Train a smaller model with fewer features<\/li><\/ul>\n<\/div> ","4adaa9c4":"[back to top](#table-of-contents)\n<a id=\"5\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">5. Data Preparation\n    \n<\/div>","ccc7e04f":"[back to top](#table-of-contents)\n<a id=\"11\"><\/a>\n\n<div style=\"background:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">11.  Training logistic regression with Scikit-Learn<ul style= \"font-size:15px;font-family:cursive;color:white\">\n<li >Train a model with Scikit-Learn<\/li>\n<li >Apply it to the validation dataset<\/li>\n<li >Calculate the accuracy<\/li><\/ul>\n<\/div> ","f1fb3c21":"[back to top](#table-of-contents)\n<a id=\"3\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">3. Loading and Reading Data\n    \n<\/div>","11dc8fe6":"[back to top](#table-of-contents)\n<a id=\"8.2\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">8.2 Feature Importance: Mutual Information \n<ul>\n<li style= \"font-size:15px;font-family:cursive;color:white\";>Concept from information theory, it tells us how much we can learn about one variable if we know the value of another <\/li><\/ul>\n<\/div>","73c26cfe":"[back to top](#table-of-contents)\n<a id=\"13\"><\/a>\n\n<div style=\"background:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">13.  Using the model\n<\/div> ","679e78e5":"[back to top](#table-of-contents)\n<a id=\"7\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">7. Exploratory Data Analysis\n    \n<\/div>","7b5ca7e2":"[back to top](#table-of-contents)\n<a id=\"8.3\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">8.3 Feature Importance: Correlation\n<ul style= \"font-size:15px;font-family:cursive;color:white\">\n<li >We use Correlation to udnerstand the relation between two numeric variables.<\/li>\n<li>We use Correlation coefficient to quatify the realtion between two numeric variables.<\/li><\/ul>\n<\/div>","ee03373e":"[back to top](#table-of-contents)\n<a id=\"4\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">4. Data Cleaning: Cleaning Strings in Column and values\n    \n<\/div>","9ea2310f":"[back to top](#table-of-contents)\n<a id=\"1\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:yellow\">1. Introduction\n<p style=\"font-family:cursive;font-size:17px;color:white\"><u>Context<\/u>: Predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\" [IBM Sample Data Sets]<\/p>\n<p style=\"font-family:cursive;font-size:17px;color:white\"><u>Content<\/u>: Each row represents a customer, each column contains customer\u2019s attributes described on the column Metadata.<\/p>   \n<ul style=\"font-family:cursive;font-size:17px;color:white\">The data set includes information about:\n<li> Customers who left within the last month \u2013 the column is called Churn<\/li>\n<li > Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies<\/li>\n    <li >Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges<\/li>\n    <li>Demographic info about customers \u2013 gender, age range, and if they have partners and dependents<\/li><\/ul>\n    \n    \n<p style=\"font-family:cursive;font-size:17px;color:white\"><p style=\"font-family:cursive;font-size:17px; color:white\" >\n<a style=\"font-family:cursive;font-size:17px; color: yellow\" href=\"https:\/\/www.kaggle.com\/blastchar\/telco-customer-churn\" target=\"_blank\"> <u>Dataset in Kaggle<\/u><\/a><\/p>\n<p style=\"font-family:cursive;font-size:17px; color:white\" >\n<a style=\"font-family:cursive;font-size:17px; color: yellow\" href=\"https:\/\/raw.githubusercontent.com\/alexeygrigorev\/mlbookcamp-code\/master\/chapter-03-churn-prediction\/WA_Fn-UseC_-Telco-Customer-Churn.csv\" target=\"_blank\"> <u>GitHub Repository<\/u><\/a><\/p>\n<\/div>\n","763c467c":"[back to top](#table-of-contents)\n<a id=\"2\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">2. Importing Libraries\n    \n<\/div>","d090bbc8":"[back to top](#table-of-contents)\n<a id=\"6\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">6. Setting up the validation framework\n    \n<\/div>","eedd038a":"<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\"><u>Content in this Notebook<\/u><\/div>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\"><u>Predict churn: Logistic Regression<\/u>    \n<p style=\"font-family:cursive;font-size:17px; color:white\" >Notebook is a part of FREE ML course by Glexey Grigorev.\n<a style=\"font-family:cursive;font-size:17px; color: yellow\" href=\"https:\/\/datatalks.club\/courses\/2021-winter-ml-zoomcamp.html\" target=\"_blank\"> <u>Link for the Course<\/u><\/a><\/p>\n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#1\"> 1. Introduction <\/a><\/li>      \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#2\" target=\"_blank\"> 2. Impoting Libraries <\/a><\/li> \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15;\" href = \"#3\"> 3. Loading and Reading Data <\/a><\/li> \n<li  style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#4\"> 4. Cleaning Strings in Column and Values<\/a><\/li> \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#5\"> 5. Data Preparation <\/a><\/li>  \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#6\"> 6. Setting up Validation Framework <\/a><\/li>  \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#7\"> 7. Exploratory Data Analysis  <\/a><\/li>\n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#8.1\"> 8. Feature Importance <\/a><\/li>\n    <ul> \n    <li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#8.1\"> 8.1 Feature Importance: Churn Rate and Risk Ratio<\/a><\/li> \n    <li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#8.2\"> 8.2 Feature Importance: Mutual Information <\/a><\/li> \n    <li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#8.3\">  8.3 Feature Importance: Correlation<\/a><\/li>\n <\/ul>\n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#9\"> 9. One Hot Encoding using DictVectorizer <\/a><\/li>\n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#10\"> 10. Logistic Regression<\/a><\/li>     \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#11\"> 11. Training logistic regression with Scikit-Learn<\/a><\/li> \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#12\"> 12. Model interpretation<\/a><\/li>  \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#13\"> 13. Using the model <\/a><\/li>  \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#14\"> 14. Summary<\/a><\/li>    \n           \n  \n<\/div>\n","cb33d02a":"[back to top](#table-of-contents)\n<a id=\"8.1\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">8.1 Feature Importance: Churn Rate and Risk Ratio\n    \n<\/div>","df4e3340":"<a id=\"table-of-contents\"><\/a>","a593053d":"[back to top](#table-of-contents)\n<a id=\"9\"><\/a>\n\n<div style=\"background:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">9. One Hot Encoding using DictVectorizer\n    <ul style= \"font-size:15px;font-family:cursive;color:white\">\n<li ;>Takes in Dictionary and converts itno vector form. Categorical columns are converted using One Hot Encoding. Numerical Values are untouched and returned as Vectors as is.<\/li>\n<li >Sparsity of the matrix can be controlled by parameter sparse = False<\/li><\/ul>\n<\/div>"}}