{"cell_type":{"9e4ecf13":"code","dc9e3dff":"code","b18dd1fb":"code","7f79d1b4":"code","759218a9":"code","4b66c79c":"code","9dedf9d3":"code","ae8a3491":"code","f8b50edb":"code","f1677b64":"code","00360886":"code","046b3d8e":"code","0950ae25":"code","a2f510a4":"code","3ef15d13":"code","027b6be2":"code","0f2be44c":"code","6383f3a4":"code","f370a508":"code","3c46c3f7":"code","ffd2eb52":"code","d74c9610":"code","ce63f478":"code","ace70199":"code","78544c26":"code","3050f854":"code","93e601fc":"code","36762d9a":"code","99da6a18":"code","b096738c":"code","9b1613dd":"code","f4c8fe3f":"code","3daf7732":"code","ff8afecb":"code","e153798e":"code","22fe6653":"code","b70f4f55":"code","4cd9a8c7":"code","143ca3f3":"code","2cbca5e7":"code","29ee9f8d":"code","917313fe":"code","6bc520fe":"code","79cc870a":"code","b8c183af":"code","40fb236d":"code","39b57862":"code","4c2fee77":"code","4f410164":"markdown","fef9a3e4":"markdown","14cafaeb":"markdown","a61e761c":"markdown","36b0f521":"markdown","b64ced3a":"markdown","f428c3c8":"markdown","080edf1c":"markdown","3a1ec469":"markdown","189e1fb4":"markdown","15c3eff6":"markdown"},"source":{"9e4ecf13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dc9e3dff":"import nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\n\nimport pandas as pd\nimport random, time\nfrom babel.dates import format_date, format_datetime, format_time\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, accuracy_score\n\n\nimport torch\nfrom torch import Tensor\nfrom torch import nn, optim\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn.functional as F\n\nimport transformers, os\nfrom transformers import BertModel, AutoModel, AdamW, get_linear_schedule_with_warmup, BertTokenizer, BertForSequenceClassification","b18dd1fb":"# Check device \n# Get the GPU device name if available.\nif torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available. {}'.format(torch.cuda.device_count()))\n    print('We will use the GPU: {}'.format(torch.cuda.get_device_name(0)))\n\n# If we dont have GPU but a CPU, training will take place on CPU instead\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n    \ntorch.cuda.empty_cache()\n    \n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","7f79d1b4":"path_fake =\"..\/input\/fake-and-real-news-dataset\/Fake.csv\"\npath_true =\"..\/input\/fake-and-real-news-dataset\/True.csv\"\n\n# Read both files\ndf_fake = pd.read_csv(path_fake)\ndf_true = pd.read_csv(path_true)\n\n# Set value  0 to fake news and value 1 to true news\ndf_fake['label'] = 0\ndf_true['label'] = 1","759218a9":"df = pd.concat([df_fake, df_true])\ndf","4b66c79c":"df.describe()","9dedf9d3":"df.info()","ae8a3491":"# get length of all the titles in the dataframe\nseq_len_premise = [len(i.split()) for i in df['title']]\n\npd.Series(seq_len_premise).hist(bins = 25)","f8b50edb":"# get length of all the text in the dataframe\nseq_len_premise = [len(i.split()) for i in df['text']]\n\npd.Series(seq_len_premise).hist(bins = 25)","f1677b64":"# Plot the count of fake and true news \nsns.countplot(df['label'])","00360886":"# Wordcloud of text\n\n# Get stopwords\n# Define nltk stopwords in english\nstop_words = stopwords.words('english')\nstop_words.extend(['u', 'wa', 'ha', 'would', 'com'])\n\n# Get a string of all the texts available\ndata_text = \",\".join(txt.lower() for txt in df.text)\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=100, \n                      stopwords=stop_words,\n                      scale=5,\n                      background_color=\"white\").generate(data_text)\n\n# Display the generated image:\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Most repeated words in all texts',fontsize=15)\nplt.show()","046b3d8e":"# Get a string of the true texts only\ndata_text_true = \",\".join(txt.lower() for txt in df.text[df.label == 1])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=100, \n                      stopwords=stop_words,\n                      scale=5,\n                      background_color=\"white\").generate(data_text_true)\n\n# Display the generated image:\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Most repeated words in all true texts',fontsize=15)\nplt.show()","0950ae25":"# Get a string of the fake news text only\ndata_text_fake = \",\".join(txt.lower() for txt in df.text[df.label==0])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=100,\n                      stopwords=stop_words,\n                      scale=5,\n                      background_color=\"white\").generate(data_text_fake)\n\n# Display the generated image:\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Most repeated words in all true texts',fontsize=15)\nplt.show()","a2f510a4":"# Wordcloud of titles\n\n# Get a string of all the titles available\ndata_title = \",\".join(t_title.lower() for t_title in df.title)\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=100, \n                      stopwords=stop_words,\n                      scale=5,\n                      background_color=\"white\").generate(data_title)\n\n# Display the generated image:\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Most repeated words in all titles',fontsize=15)\nplt.show()","3ef15d13":"## Check the word frequency in texts\n#\n## lemmatize text column by using a lemmatize function\n#def lemmatize_text(text):\n#    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text.lower())]\n#\n#\n## Initialize the Lemmatizer and Whitespace Tokenizer\n#w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n#lemmatizer = nltk.stem.WordNetLemmatizer()\n#\n## Lemmatize words\n#df['text_lemmatized'] = df.text.apply(lemmatize_text)\n#df['text_lemmatized'] = df['text_lemmatized'].apply(lambda x: [word for word in x if word not in stop_words])\n#\n## use explode to expand the lists into separate rows\n#wf_text = df.text_lemmatized.explode().to_frame().reset_index(drop=True)\n#\n## plot\n#sns.countplot(x='text_lemmatized', data=wf_text, order=wf_text.text_lemmatized.value_counts().iloc[:10].index)\n#plt.xlabel('Most common used words in all texts')\n#plt.ylabel('Frequency [%]')\n#plt.xticks(rotation=70)","027b6be2":"## Check the word frequency in titles\n#\n## Lemmatize words\n#df['title_lemmatized'] = df.title.apply(lemmatize_text)\n#df['title_lemmatized'] = df['title_lemmatized'].apply(lambda x: [word for word in x if word not in stop_words])\n#\n## use explode to expand the lists into separate rows\n#wf_title = df.title_lemmatized.explode().to_frame().reset_index(drop=True)\n#\n## plot dfe\n#sns.countplot(x='title_lemmatized', data=wf_title, order=wf_title.title_lemmatized.value_counts().iloc[:10].index)\n#plt.xlabel('Most common used words in all titles')\n#plt.ylabel('Frequency [%]')\n#plt.xticks(rotation=70)","0f2be44c":"#  Preprocess train dataset\n# remove special characters from text column\ndf.text = df.text.str.replace('[#,@,&]', '')\n# Remove digits\ndf.text = df.text.str.replace('\\d*','')\n#Remove www\ndf.text = df.text.str.replace('w{3}','')\n# remove urls\ndf.text = df.text.str.replace(\"http\\S+\", \"\")\n# remove multiple spaces with single space\ndf.text = df.text.str.replace('\\s+', ' ')\n#remove all single characters\ndf.text = df.text.str.replace(r'\\s+[a-zA-Z]\\s+', '')\n\n# Remove english stopwords\ndf['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","6383f3a4":"# Split test and train data using 25% of the dataset for validation purposes\nx_train, x_test, y_train, y_test = train_test_split(df['text'], \n                                                      df['label'], test_size=0.25, shuffle=True, random_state=42)","f370a508":"# Obtain a 10% test set from train set\nX_train_Transformer, X_val_Transformer, y_train_Transformer, y_val_Transformer = train_test_split(\n                                                    x_train, y_train, test_size=0.20, random_state=42)","3c46c3f7":"model_name = 'bert-base-uncased'\nSEQ_LEN = 200\nbatch_size = 16\nepochs = 5\nlearning_rate = 1e-5 # Controls how large a step is taken when updating model weights during training.\nsteps_per_epoch = 50\nnum_workers = 3","ffd2eb52":"def get_split(text1):\n    '''Get split of the text with 200 char lenght'''\n    l_total = []\n    l_parcial = []\n    if len(text1.split())\/\/150 >0:\n        n = len(text1.split())\/\/150\n    else: \n        n = 1\n    for w in range(n):\n        if w == 0:\n            l_parcial = text1.split()[:200]\n            l_total.append(\" \".join(l_parcial))\n        else:\n            l_parcial = text1.split()[w*150:w*150 + 200]\n            l_total.append(\" \".join(l_parcial))\n    return str(l_total)\n\n# Splits train and validation sets to be feed to the transformer which only accepts 512 tokens maximum\nsplit_train_text = [get_split(t) for t in X_train_Transformer]\nsplit_valid_text = [get_split(t) for t in X_val_Transformer]\nsplit_test_text = [get_split(t) for t in x_test]","d74c9610":"#split_valid_text","ce63f478":"# Load the RoBERTa tokenizer and tokenize the data\nprint('Loading BERT tokenizer...')\ntokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)","ace70199":"trencoding = tokenizer.batch_encode_plus(\n  list(split_train_text),\n  max_length=SEQ_LEN,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=True,\n  truncation=True,\n  padding='longest',\n  return_attention_mask=True,\n)\n\nvalencoding = tokenizer.batch_encode_plus(\n  list(split_valid_text),\n  max_length=SEQ_LEN,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=True,\n  truncation=True,\n  padding='longest',\n  return_attention_mask=True,\n)\n\n\ntestencoding = tokenizer.batch_encode_plus(\n  list(split_test_text),\n  max_length=SEQ_LEN,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=True,\n  truncation=True,\n  padding='longest',\n  return_attention_mask=True,\n)\n","78544c26":"tokenizer.special_tokens_map","3050f854":"trencoding.keys()","93e601fc":"#compute the class weights\nclass_wts = compute_class_weight('balanced', np.unique(df['label'].values.tolist()), \n                                 df['label'])\n\n#print(class_wts)\n\n# convert class weights to tensor\nweights= torch.tensor(class_wts,dtype=torch.float)\nweights = weights.to(device)\n\n# loss function\n#cross_entropy  = nn.NLLLoss(weight=weights) \ncross_entropy  = nn.CrossEntropyLoss(weight=weights)","36762d9a":"def loadData(prep_df, batch_size, num_workers, sampler):\n    \n    return  DataLoader(\n            prep_df,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            sampler=sampler,\n            pin_memory=True\n        )\n\n## convert lists to tensors\ntrain_seq = torch.tensor(trencoding['input_ids'])\ntrain_mask = torch.tensor(trencoding['attention_mask'])\ntrain_token_ids = torch.tensor(trencoding['token_type_ids'])\ntrain_y = torch.tensor(y_train_Transformer.tolist())\n\nval_seq = torch.tensor(valencoding['input_ids'])\nval_mask = torch.tensor(valencoding['attention_mask'])\nval_token_ids = torch.tensor(valencoding['token_type_ids'])\nval_y = torch.tensor(y_val_Transformer.tolist())\n\ntest_seq = torch.tensor(testencoding['input_ids'])\ntest_mask = torch.tensor(testencoding['attention_mask'])\ntest_token_ids = torch.tensor(testencoding['token_type_ids'])\ntest_y = torch.tensor(y_test.tolist())\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_token_ids, train_y)\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n# Train Data Loader\ntraindata = loadData(train_data, batch_size, num_workers, train_sampler)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_token_ids, val_y)\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n# Val Data Loader\nvaldata = loadData(val_data, batch_size, num_workers, val_sampler)\n\n# wrap tensors\ntest_data = TensorDataset(test_seq, test_mask, test_token_ids, test_y)\n# sampler for sampling the data during training\ntest_sampler = SequentialSampler(test_data)\n# Val Data Loader\ntestdata = loadData(test_data, batch_size, num_workers, test_sampler)\n\n\nprint('Number of data in the train set', len(traindata))\nprint('Number of data in the validation set', len(valdata))\nprint('Number of data in the test set', len(testdata))","99da6a18":"batch_size","b096738c":"class BERT_Arch(nn.Module):\n    \n    def __init__(self, n_classes, freeze_bert=False):\n        \n        super(BERT_Arch,self).__init__()\n        # Instantiating BERT model object\n        self.bert = BertModel.from_pretrained(model_name, return_dict=False)\n        \n        # Freeze bert layers\n        if freeze_bert:\n            for p in self.bert.parameters():\n                p.requires_grad = False\n                \n        self.bert_drop_1 = nn.Dropout(0.3)\n        self.fc = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size) # (768, 64)\n        self.bn = nn.BatchNorm1d(768) # (768)\n        self.bert_drop_2 = nn.Dropout(0.25)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes) # (768,2)\n\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        _, output = self.bert(\n            input_ids = input_ids,\n            attention_mask = attention_mask,\n            token_type_ids = token_type_ids\n        )\n        output = self.bert_drop_1(output)\n        output = self.fc(output)\n        output = self.bn(output)\n        output = self.bert_drop_2(output)\n        output = self.out(output)        \n        return output","9b1613dd":"class_names = np.unique(df['label'])\nprint('Downloading the BERT custom model...')\nmodel = BERT_Arch(len(class_names))\nmodel.to(device) # Model to GPU.\n\n#optimizer parameters\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [{'params': [p for n, p in param_optimizer \n                                    if not any(nd in n for nd in no_decay)],'weight_decay':0.001},\n                        {'params': [p for n, p in param_optimizer \n                                    if any(nd in n for nd in no_decay)],'weight_decay':0.0}]\n\nprint('Preparing the optimizer...')\n#optimizer \noptimizer = AdamW(optimizer_parameters, lr=learning_rate)\nsteps = steps_per_epoch\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = steps\n)","f4c8fe3f":"# function to train the bert model\ndef trainBERT():\n  \n    print('Training...')\n    model.train()\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save model predictions\n    total_preds=[]\n\n    # iterate over batches\n    for step, batch in enumerate(traindata):\n    \n        # progress update after every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(traindata)))\n\n        if torch.cuda.is_available():\n            # push the batch to gpu\n            batch = [r.to(device) for r in batch]\n\n        sent_id, mask, token_type_ids, labels = batch\n        # clear previously calculated gradients \n        model.zero_grad()        \n        # get model predictions for the current batch\n        preds = model(sent_id, mask, token_type_ids)\n        # compute the loss between actual and predicted values\n        loss = cross_entropy(preds, labels)\n        # add on to the total loss\n        total_loss = total_loss + loss.item()\n        # backward pass to calculate the gradients\n        loss.backward()\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        # update parameters\n        optimizer.step()\n        # model predictions are stored on GPU. So, push it to CPU\n        preds=preds.detach().cpu().numpy()\n        # append the model predictions\n        total_preds.append(preds)\n        \n        torch.cuda.empty_cache()\n\n    # compute the training loss of the epoch\n    avg_loss = total_loss \/ len(traindata)\n\n    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    #returns the loss and predictions\n    return avg_loss, total_preds","3daf7732":"# function for evaluating the model\ndef evaluate():\n  \n    print(\"\\nEvaluating...\")\n    t0 = time.time()\n    \n    model.eval() # deactivate dropout layers\n    total_loss, total_accuracy = 0, 0\n    \n    # empty list to save the model predictions\n    total_preds = []\n\n    # iterate over batches\n    for step, batch in enumerate(valdata):\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(valdata)))\n\n        if torch.cuda.is_available():\n            # push the batch to gpu\n            batch = [t.to(device) for t in batch]\n\n        sent_id, mask, token_type_ids, labels = batch\n\n        # deactivate autograd\n        with torch.no_grad(): # Dont store any previous computations, thus freeing GPU space\n\n            # model predictions\n            preds = model(sent_id, mask, token_type_ids)\n            # compute the validation loss between actual and predicted values\n            loss = cross_entropy(preds, labels)\n            total_loss = total_loss + loss.item()\n            preds = preds.detach().cpu().numpy()\n            total_preds.append(preds)\n\n        torch.cuda.empty_cache()\n    # compute the validation loss of the epoch\n    avg_loss = total_loss \/ len(valdata) \n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    return avg_loss, total_preds","ff8afecb":"# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# Empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n# for each epoch perform training and evaluation\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} \/ {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = trainBERT()\n    \n    #evaluate model\n    valid_loss, _ = evaluate()\n    \n    print('Evaluation done for epoch {}'.format(epoch + 1))\n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        print('Saving model...')\n        torch.save(model.state_dict(), 'bert_weights.pt') # Save model weight's (you can also save it in .bin format)\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","e153798e":"print('\\nTest Set...')\n\ntest_preds = []\n\nprint('Total batches:', len(testdata))\n\nfor fold_index in range(0, 3):\n    \n    print('\\nFold Model', fold_index)\n    \n    # Load the fold model\n    path_model = 'bert_weights.pt'\n    model.load_state_dict(torch.load(path_model))\n\n    # Send the model to the GPU\n    model.to(device)\n\n    stacked_val_labels = []\n    \n    # Put the model in evaluation mode.\n    model.eval()\n\n    # Turn off the gradient calculations.\n    # This tells the model not to compute or store gradients.\n    # This step saves memory and speeds up validation.\n    torch.set_grad_enabled(False)\n\n\n    # Reset the total loss for this epoch.\n    total_val_loss = 0\n\n    for j, test_batch in enumerate(testdata):\n\n        inference_status = 'Batch ' + str(j + 1)\n\n        print(inference_status, end='\\r')\n\n        b_input_ids = test_batch[0].to(device)\n        b_input_mask = test_batch[1].to(device)\n        b_token_type_ids = test_batch[2].to(device)\n        b_test_y = test_batch[3].to(device)\n\n\n        outputs = model(b_input_ids, \n                        attention_mask=b_input_mask,\n                        token_type_ids=b_token_type_ids)\n\n        # Get the preds\n        preds = outputs[0]\n\n        # Move preds to the CPU\n        val_preds = preds.detach().cpu().numpy()\n        \n        #true_labels.append(b_test_y.to('cpu').numpy().flatten())\n        \n        # Stack the predictions.\n        if j == 0:  # first batch\n            stacked_val_preds = val_preds\n            \n        else:\n            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n            \n    test_preds.append(stacked_val_preds)\n    \n            \nprint('\\nPrediction complete.')","22fe6653":"print(len(test_preds))\nprint(test_preds[:5])","b70f4f55":"# Sum the predictions of all fold models\nfor i, item in enumerate(test_preds):\n    if i == 0:\n        preds = item\n    else:\n        # Sum the matrices\n        preds = item + preds\n\n# Average the predictions\navg_preds = preds\/(len(test_preds))\n\n#print(preds)\n#print()\n#print(avg_preds)\n\n# Take the argmax. \n# This returns the column index of the max value in each row.\ntest_predictions = np.argmax(avg_preds, axis=1)\n\n# Take a look of the output\nprint(type(test_predictions))\nprint(len(test_predictions))\nprint()\nprint(test_predictions)","4cd9a8c7":"true_y = []\nfor j, test_batch in enumerate(testdata):\n    true_y.append(int(test_batch[3][0].numpy().flatten()))\nprint(true_y)","143ca3f3":"# Accuracy and classification report \ntarget_names = ['true_y', 'predicted_y']\n\ndata = {'true_y': true_y,\n       'predicted_y': test_predictions}\n\ndf_pred_BERT = pd.DataFrame(data, columns=['true_y','predicted_y'])\n\nconfusion_matrix = pd.crosstab(df_pred_BERT['true_y'], df_pred_BERT['predicted_y'], rownames=['True'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrix, annot=True)\nplt.show()","2cbca5e7":"print('Accuracy of BERT model', accuracy_score(true_y, test_predictions))","29ee9f8d":"print(classification_report(true_y, test_predictions, target_names=target_names))","917313fe":"# Create a Pipeline with the TfidfVectorizer and LogisticRegression model\nLR_pipeline = Pipeline(steps = [('tf', TfidfVectorizer()), \n                                ('lgrg', LogisticRegression())]) # initialize TfidfVectorizer and LogisticRegression\n\n\n# Create Parameter Grid\npgrid_lgrg = {\n 'tf__max_features' : [1000, 2000, 3000],\n 'tf__ngram_range' : [(1,1),(1,2)],\n 'tf__use_idf' : [True, False],\n 'lgrg__penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n 'lgrg__class_weight' : ['balanced', None]\n}\n\n# Apply GridSearch to Pipeline to find the best parameters\ngs_lgrg = GridSearchCV(LR_pipeline, pgrid_lgrg, cv=2, n_jobs=-1, verbose=2)","6bc520fe":"gs_lgrg.fit(x_train, y_train) # Train LR model","79cc870a":"gs_lgrg.best_params_","b8c183af":"print('Score of train set', gs_lgrg.score(x_train, y_train))\nprint('Score of test set',gs_lgrg.score(x_test, y_test))","40fb236d":"LR_pred = gs_lgrg.predict(x_test) # Predict on validation data\n\ndata = {'true_y': y_test,\n       'predicted_y': LR_pred}\ndf_pred = pd.DataFrame(data, columns=['true_y','predicted_y'])\nconfusion_matrix = pd.crosstab(df_pred['true_y'], df_pred['predicted_y'], rownames=['True'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrix, annot=True)\nplt.show()","39b57862":"print('Accuracy of LR model', accuracy_score(y_test, LR_pred))","4c2fee77":"print(classification_report(y_test, LR_pred, target_names=target_names))","4f410164":"# Load BERT model","fef9a3e4":"# Try Logistic regression","14cafaeb":"## Train the BERT model","a61e761c":"## Wordclouds","36b0f521":"## Term frequencies","b64ced3a":"# Try transformer model","f428c3c8":"# Get to know the data","080edf1c":"# Cleaning the data","3a1ec469":"## Find Class Weights","189e1fb4":"In this section I am going to train a Logistic Regression model by using a Pipeline containing the TfidfVectorizer and LogisticRegression. Also, I am going to apply a GridSearchCV to the Pipeline to find the best parameters for the model. This is going to find the optimal parameters, however, it's a bit time consuming.","15c3eff6":"As we can see, we are obtaining a 0.99 acc by just training a Logistic Regression model. Sometimes the simplest solution is the best choice to solve a certain task if it can save us computation time. In any case, the results are very similar."}}