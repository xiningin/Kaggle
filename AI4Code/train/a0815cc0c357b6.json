{"cell_type":{"1e797cb7":"code","c9f7d3b3":"code","347a0bab":"code","4270285b":"code","58c60a39":"code","1bf18c95":"code","d4e382ac":"code","5a7c465a":"code","02bbf9d9":"code","a625fd37":"code","08f60b38":"code","3059b9d6":"code","c9cb576e":"code","f756a134":"markdown","ea91180e":"markdown","41d4112a":"markdown","cdea1fef":"markdown","63cbf769":"markdown","0f8f9b81":"markdown","cd3fff4d":"markdown","27519423":"markdown","0f5cee14":"markdown"},"source":{"1e797cb7":"pip install chess\n","c9f7d3b3":"%load_ext autoreload\n%autoreload 2\n!pip install --upgrade git+https:\/\/github.com\/arjangroen\/RLC.git@maintain-tree  # RLC is the Reinforcement Learning package\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)import os\nimport inspect\nimport matplotlib.pyplot as plt","347a0bab":"from keras.models import load_model","4270285b":"import os\nos.listdir('..\/input')","58c60a39":"from RLC.real_chess import agent, environment, learn, tree\nimport chess\nfrom chess.pgn import Game\n\n\nopponent = agent.GreedyAgent()\nenv = environment.Board(opponent, FEN=None)\nplayer = agent.Agent(lr=0.0005,network='big')\nlearner = learn.TD_search(env, player,gamma=0.9,search_time=0.9)\nnode = tree.Node(learner.env.board, gamma=learner.gamma)\nplayer.model.summary()","1bf18c95":"n_iters = 10000  # maximum number of iterations\ntimelimit = 3600 # maximum time for learning\nnetwork_replacement_interval = 10  # For the stability of the nearal network updates, the network is not continuously replaced","d4e382ac":"learner.learn(iters=n_iters,timelimit_seconds=timelimit,c=network_replacement_interval) ","5a7c465a":"reward_smooth = pd.DataFrame(learner.reward_trace)\nreward_smooth.rolling(window=1000,min_periods=1000).mean().plot(figsize=(16,9),title='average reward over the last 1000 steps')\nplt.show()","02bbf9d9":"reward_smooth = pd.DataFrame(learner.piece_balance_trace)\nreward_smooth.rolling(window=50,min_periods=50).mean().plot(figsize=(16,9),title='average piece balance over the last 50 episodes')\nplt.show()","a625fd37":"learner.env.reset()\nlearner.search_time = 60\nlearner.temperature = 1\/3","08f60b38":"learner.play_game(n_iters,maxiter=128)","3059b9d6":"pgn = Game.from_board(learner.env.board)\nwith open(\"rlc_pgn\",\"w\") as log:\n    log.write(str(pgn))","c9cb576e":"learner.agent.model.save('RLC_model.h5')","f756a134":"**Import**","ea91180e":"# References\n**Reinforcement Learning: An Introduction**  \nRichard S. Sutton and Andrew G. Barto  \n1st Edition  \nMIT Press, march 1998  \n  \n**RL Course by David Silver: Lecture playlist**  \nhttps:\/\/www.youtube.com\/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ","41d4112a":"### Launching the network","cdea1fef":"# Reinforcement Learning Chess\nReinforcement Learning Chess is a series of notebooks where I implement Reinforcement Learning algorithms to develop a chess AI. I start of with simpler versions (environments) that can be tackled with simple methods and gradually expand on those concepts untill I have a full-flegded chess AI.","63cbf769":"![chess_gif](https:\/\/images.chesscomfiles.com\/uploads\/game-gifs\/90px\/green\/neo\/0\/cc\/0\/0\/b3cwS2tzM05pcTlxYnEhVGRrVENrQ1hIQzQ_MzQ1N0Z3RjNWNTY4MHFrVlVtdVVnaGcyTTY_TUZnbzFUYXFXR2ZIWUlxR0tDSHlaUj84.gif)","0f8f9b81":"### Notebook 5: Monte Carlo tree search (MCTS)\nThe aim of this notebook is to build a chess AI that can plan moves and defeat a simple opponent in a regular game of chess. My approach can be summarized as follows:\n- Instead of Q-learning (learning action values) I use \"V-learning\" (learning state-values).\n    - An advantage is that the Neural Network can learn with fewer parameters since it doesn't need to learn a seperate value for each action. In my Q-learning and Policy Gradient notebook, the output vector has a size > 4000. Now the size is only 1.\n- The V-network is updated using Temporal Difference (TD) Learning, like explained in Notebook 1. \n    - This option is the simplest to code.  Other options are TD-lambda and Monte Carlo Learning.\n- The Architecture of the V-network is quite arbitrary and can probably be optimized. I used a combination of small and large convolutions, combined with convolutions that range a full file or rank (1-by-8 or 8-by-1).\n- Moves are planned using Monte Carlo Tree Search. This involves simulating playouts. \n    - Monte Carlo Tree Search greatly improves performance on games like chess and go because it helps the agent to plan ahead.\n- These playouts are truncated after N steps and bootstrapped. \n    - This reduces the variance of the simulation outcomes and gives performance gains, since the simulation doesn't require a full playout. \n- For this version, the opponent of the RL-agent is a myopic player, that always chooses the move that results in the most material on the board or a checkmate.","cd3fff4d":"### Final performance with large searchtime and more greedy behavior","27519423":"### Learning performance ","0f5cee14":"[Notebook 1: Policy Iteration](https:\/\/www.kaggle.com\/arjanso\/reinforcement-learning-chess-1-policy-iteration)  \n[Notebook 2: Model-free learning](https:\/\/www.kaggle.com\/arjanso\/reinforcement-learning-chess-2-model-free-methods)  \n[Notebook 3: Q-Learning](https:\/\/www.kaggle.com\/arjanso\/reinforcement-learning-chess-3-q-networks)  \n[Notebook 4: Policy Gradients](https:\/\/www.kaggle.com\/arjanso\/reinforcement-learning-chess-4-policy-gradients)  "}}