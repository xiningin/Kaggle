{"cell_type":{"26263039":"code","19a58ff7":"code","e36a9dbb":"code","ebd89763":"code","b11bbec8":"code","c12c8697":"code","46282bf6":"code","0671436e":"code","c5b467da":"code","b6513f7b":"code","79951144":"code","94ab7737":"code","cf944ea9":"code","02109d83":"code","821304d1":"code","dad591a2":"code","83dfbea8":"code","8d59a7e0":"code","5234fa93":"code","10a16dd7":"code","5e6bd21a":"code","c1747f93":"code","6398fbb5":"code","5a053671":"code","a87f59ac":"code","1537bd07":"code","786df9f9":"code","a06a0ada":"code","6d02e6a8":"code","e7c244d3":"code","7b49acd0":"code","bc280ae7":"code","52e10a2b":"code","0406d8ac":"markdown","94290ab5":"markdown","3dfc266a":"markdown","c2e8f166":"markdown","0f10873c":"markdown","fa761f9a":"markdown","f291454c":"markdown","1046d9cd":"markdown","37f86d48":"markdown","12ac5ca3":"markdown","f5b594f8":"markdown","8c3610e0":"markdown","c8af16f8":"markdown","47876019":"markdown","f02a16d4":"markdown","c3f9c06b":"markdown","30e6807f":"markdown","0adb737a":"markdown","b4a52a3a":"markdown","d146a087":"markdown","9c918cd7":"markdown","7716c0cf":"markdown","0a344f29":"markdown","caec17ce":"markdown","7bc6fc0a":"markdown","1e7a6f72":"markdown","ca948833":"markdown","6185b71c":"markdown","dc85b3a3":"markdown","2e4c82f5":"markdown","687ec9ed":"markdown","85e42bd4":"markdown","bb6264aa":"markdown","80d3c5a4":"markdown","ff210939":"markdown","68bac7e9":"markdown","53bf6ed5":"markdown","ede41fb4":"markdown","f06ab475":"markdown","aa34c21e":"markdown","2ea02204":"markdown","09df17cb":"markdown","e95b4cf1":"markdown","c4bd9bc2":"markdown","c5dd390e":"markdown","cc41b9ad":"markdown","587e55d4":"markdown","17f1d1fb":"markdown","377144d5":"markdown","ebbeae53":"markdown","08922d01":"markdown","184eea0c":"markdown","ef113900":"markdown","1f8f9761":"markdown","6f8d6c83":"markdown","e76646cf":"markdown","f05b7113":"markdown","9b0b8cfd":"markdown","4db9c2cc":"markdown","78c53a28":"markdown","d2107b88":"markdown","164af0cc":"markdown","e972da3a":"markdown","74931805":"markdown","8030042c":"markdown","0edb045d":"markdown"},"source":{"26263039":"from fastai import __version__\nprint(f\"fastai version {__version__}\")\n\n!pip install -Uqq jmd_imagescraper\n\nimport graphviz\nimport matplotlib as mpl\nimport shutil\n\nfrom fastai.vision.all import *\nfrom jmd_imagescraper.core import *  \nfrom jmd_imagescraper.imagecleaner import *\nfrom pandas.api.types import CategoricalDtype\nfrom scipy.cluster import hierarchy as hc\n\nmpl.rcParams['savefig.dpi']= 200\nmpl.rcParams['font.size']=12\n\nset_seed(88)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\npd.set_option('display.max_columns',999)\nnp.set_printoptions(linewidth=200)\ntorch.set_printoptions(linewidth=200)\n\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"' + s + '; }')\n\n# Prepare diagram 'Top Level'\ntop_level = '''ordering=in\n            problem[shape=cds width=1 height=1 label=\"1\\nFrame business problem \\ninto a ML problem\"]\n            data[shape=cylinder width=1 height=1 label=\"2\\nCollect, and prepare \\ndata, incl. labeling\"]\n            modeling[shape=box3d width=1 height=1 label=\"3\\nBuild model \\nand train it\"]\n            evaluate[shape=component width=1 height=1 label=\"4\\nEvaluate and \\nvalidate model\"]\n            improve[shape=rarrow width=1 height=1 label=\"5\\nImprove model to \\nreach expected standard\"]\n            problem->data->modeling->evaluate->improve'''\n\nprint('import and configurations finished')","19a58ff7":"gv(top_level)","e36a9dbb":"gv(top_level)","ebd89763":"search_phrase_mask = \"person with a disposable mask\"\nsearch_phrase_no_mask = \"real person faces\"\n\n# Replace the phrase in red above by your phrase. Keep the appostrophes \" \"\n\nclass_name_for_mask = \"with_mask\"\nclass_name_for_no_mask = \"without_mask\"","b11bbec8":"number_images_to_download = 100         # this can go up to 450 at the time of writing","c12c8697":"path = Path.cwd()\/ 'images'\nprint(path)\n\n# Search\nduckduckgo_search(path, class_name_for_mask, search_phrase_mask, max_results=number_images_to_download)\nduckduckgo_search(path, class_name_for_no_mask, search_phrase_no_mask, max_results=number_images_to_download)\n\n# Check that all images can be used and delete any defective image\ndef check_img(img):\n    try: _ = Image.open(img)\n    except Exception as e:\n        img = str(img).replace(\" \",\"\\ \")\n        os.system(f\"rm -f {img}\");print(f\"removing error img:{img}\")\n\nfor cls in path.iterdir():\n    for img in cls.iterdir():\n        check_img(img)","46282bf6":"dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), \n                   get_items=get_image_files, \n                   get_y=parent_label,\n                   item_tfms=Resize(128),\n                   splitter=RandomSplitter(valid_pct=0.2, seed=88),\n                   )","0671436e":"dls = dblock.dataloaders(path, bs=16)","c5b467da":"dls.show_batch(max_n=20)","b6513f7b":"display_image_cleaner(path)","79951144":"dls = dblock.dataloaders(path, bs=16)","94ab7737":"dls.show_batch(max_n=25)","cf944ea9":"gv(top_level)","02109d83":"learn = cnn_learner(dls, \n                    resnet18, \n                    metrics=[accuracy, Precision(), Recall()]\n                   )","821304d1":"nr_epochs = 5\n\nlearn.fine_tune(nr_epochs, 1e-4)","dad591a2":"gv(top_level)","83dfbea8":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()","8d59a7e0":"interp.plot_top_losses(k=4, figsize=(12,6))","5234fa93":"test_path = Path('\/kaggle\/input\/facemask44k\/Facemask-44k\/dataset')","10a16dd7":"nbr_test_images = 100\n\ntest_image_fnames = get_image_files(test_path)\nidxs = np.random.choice(len(test_image_fnames)-1, nbr_test_images)\ntest_image_fnames = test_image_fnames[idxs]","5e6bd21a":"test_dl = dls.test_dl(test_image_fnames, with_labels=True)","c1747f93":"test_dl.show_batch(max_n=16)","6398fbb5":"interp_test = ClassificationInterpretation.from_learner(learn, dl=test_dl)","5a053671":"interp_test.plot_confusion_matrix()","a87f59ac":"test_results = learn.validate(dl=test_dl)\n# print(f\"Loss Function: {test_results[0]*100:.3f}%\")\nfor i, m in enumerate(learn.metrics):\n    print(f\"{m.name:16s}:  {test_results[i+1]*100:.1f}%\")","1537bd07":"interp_test.plot_top_losses(k=9, figsize=(15, 15))","786df9f9":"gv(top_level)","a06a0ada":"dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), \n                   get_items=get_image_files, \n                   get_y=parent_label,\n                   item_tfms=Resize(128),\n                   splitter=RandomSplitter(valid_pct=0.3, seed=42),\n                   )","6d02e6a8":"path_44k = Path('\/kaggle\/input\/facemask44k\/Facemask-44k\/dataset')\n\ndls = dblock.dataloaders(path_44k)","e7c244d3":"dls.show_batch(max_n=16)","7b49acd0":"learn = cnn_learner(dls, \n                    resnet18, \n                    metrics=[accuracy, Precision(), Recall()])\nlearn.fine_tune(5)","bc280ae7":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()","52e10a2b":"interp.plot_top_losses(k=9, figsize=(18, 18))","0406d8ac":"#### Train - Finetune the model\n\nNow we are ready to train our model (`learner`).  As we use a pretrained model, we will **finetune** it.\n\nThe way the model learns is by \"looking\" at each image several times and each time it calculate a prediction, checks whether the prediction is correct compared to the label provided in the training set, and calculate how far it is from the correct prediction (the loss). Then it corrects its parameters to come closer to the correct prediction. This is an iterative process.\n\nTo learn, the model needs to see all images in the training set several times. At each iteration, the model \"sees\" each of the images once. In deep learning jargon, this iteration over which the modell sees all images in the training set is called an **`epoch`**. \n\nWhen models are trained from scratch, it is sometimes necessary use hundred of epochs. But in the case of a pre-trained model, a few epochs are often enough in first instance.\n\nWe run 5 epochs. But fee free to try less of more\n\nAt the end of each epoch, the system returns information of the metrics:\n\n|epoch|train_loss|valid_loss|accuracy|precision_score|recall_score|time\n|-----|----------|----------|--------|---------------|------------|----\n0|1.081642|0.391084|0.833333|0.818182|0.818182|00:01\n\nAccuracy, Precision and Recall are the three metrics we will follow. The closet to one the better.","94290ab5":"The `dataloaders` `dls` gives us access to all images. One thing we can do is to see a radom sample of the images and their labels. We can play with the number of images to display (`max_n`). ","3dfc266a":"### Evaluate based on the training and validation set","c2e8f166":"#### Launch the scraper application\nOnce you have a good phrase for each of our two classes, save them in the cell below for further use.","0f10873c":"## 2\ufe0f\u20e3 Collect and Prepare Data, including Labeling","fa761f9a":"OK, this looks better now.","f291454c":"3. Create a model\n    - select a specific architecture (for image, CNN Resnet with 18 layers)\n    - in this case we use a pretrained model for general images\n    - use the data loaders created aboce `dls`\n    - specify metrics to see how the training evolve\n```\nlearn = cnn_learner(dls, resnet18, metrics=[accuracy, Recall(), Precision()])\n","1046d9cd":"## 3\ufe0f\u20e3 Build a model and train it","37f86d48":"- Our company has a good temperature control system for access control to public areas. \n- We want to add a feature to detect people who are not wearing a mask and sound an alarm or block their access.\n\n<div align=\"center\"><img src=\"https:\/\/raw.githubusercontent.com\/vtecftwy\/fastbook\/master\/resources\/img\/temp_scan_01.jpg\" width=25%><img src=\"https:\/\/raw.githubusercontent.com\/vtecftwy\/fastbook\/master\/resources\/img\/temp_scan_03.png\" width=25%><\/div>","12ac5ca3":"### Create the dataloaders\n\nThis is exactly the same recipe as before, except that we keep a vaidation set of 30% as we have more images in general.","f5b594f8":"4. Train the model\n    - train it for a given number of iterations (epochs)\n    - we have a pretrained model, so we only need to fine tune it\n```\nlearn.fine_tune(5)\n```","8c3610e0":"## 4\ufe0f\u20e3 Evaluate the trained model","c8af16f8":"## How to create your own notebook and execute code?\nIf you are not familiar with Kaggle of hosted Jupyter notebooks, read this section. ","47876019":"Now we apply this \"recipe\" to all images in the folder that includes all images of Facemask-44k. It will take a little while as there are many images. ","f02a16d4":"### Prepare data to feed the model.\n\nComputer only do what they are told to do, therefore we need to tell where the data is, what type of data it is and what to do with it. We do that with a `DataBlock`.\n\nIn this example, the code below tells the computer:\n- we have **images** and for each image, we have one label a.k.a. class a.k.a **`category`** a.k.a. `y`\n- the computer will get the images by using a function **`get_image_file`** which returns a list of all the images in a given folder\n- the computer will know which is the class of each image by looking in which **folder** (a.k.a. `parent_label`) it is located\n- before using each image, the computer should **resize** it into a standard square of size 128 pixels\n- finaly, we want to keep a **validation set** of 20% of the images. These images will **not** be used for training, but they are only used to test whether the trained model generalizes well on new images.","c3f9c06b":"We are ready to build the model. \n\nWe will not build a neural network from scratch. Instead, we will use a **pre-trained models** and perform something called **transfer learning**. We use a model that has been train on a very large number of images to solve a specific classification problem, and then we will finetune it for our own classification problem.\n\nSounds like a big thing, but we will do this in two lines of code with `fastai`.\n\n#### Architecture Selection\nWe select: RESNET 18. \n> Resnet is a state-of-the-art convolutional neural network (CNN), pre-trained on a large dataset called Imagenet, including 1,281,167 training images organized in 1,000 classes.\n>\n><img src=\"https:\/\/image-net.org\/static_files\/index_files\/logo.jpg\" height=20px>\n> \n>CNNs are the currently go-to type of architectures for computer vision problem and Resnet is an excellent choice. Resnet comes in several versions, including \n> - resnet-18 with 18 layers\n> - resnet-34 with 34 layers\n> - resnet-50 with 50 layers\n><div align=\"center\"><img src=\"https:\/\/raw.githubusercontent.com\/vtecftwy\/fastbook\/master\/resources\/img\/resnet-18-01.png\" width=66%><\/div>","30e6807f":"Now we have downloaded our images into a folder called `images` and \n- all images of faces with mask are grouped into `images\/with_mask`\n- all images of faces without mask are grouped into `images\/without_mask `\n\nWe are ready to start with the model","0adb737a":"### What is the problem we want to solve?\n\n\ud83d\udc77 Think of this question and write your answer in the text cell below.","b4a52a3a":"### Evaluate with new data\n\nWe only had a small set of unseen images (validation set). \n\nWe should use a larger test set. We could download more images and create a bigger test set. But it hapens that there is a mask dataset on Kaggle: **Face Mask Detection-44k Augmented**. It includes more than 44,000 labelled images. Let's use a subset of these to use as test set.","d146a087":"### How do we state this as a machine learning problem?\n- What we will pass to the system ?\n- What we want to get from the system ?\n- Type of ML problem?\n- Type of data we need?\n- Type of performance we want to achieve and minimum performance to succeed\n\n\ud83d\udc77 Think of this write your ideas below","9c918cd7":"#### Experiment with keywords\n\nExperiment a little on Duckduckgo directly: Click on the Duckduckgo icon above and it will lead you to the search engine.\n\nThe objective is to define one **search phrase** that generates a good set of images or faces with masks and one **search phrase** that generates a good set of images of faces with not mask. You will notice, for instance, that the search phrase `faces with no mask` will return lots of images with masks \ud83d\ude41.\n\nFor this simple exercise, select **one** search phrase for the class `with_mask` and **one** search phrase for the class `without_mask`.\n\nDo not worry if you have a few unrelated images, we will clean this up later.","7716c0cf":"Now the computer has a **recipe** to handle images in a specific folder. We have put our images in `path`. Now we tell the computer to apply the recipe (`dblock`) to a specific folder (`path`):\n- go to `path`\n- identify all images there\n- build a dataset for training and a dataset for validation\n- get ready to resize the data\n- use the images in groups of 16 at the time\n\nWat we get after that is called a `dataloaders` because it is the tool the computer will use to load data into the model and there are two sets: training and validation.","0a344f29":"#### \ud83d\udd0e End of details","caec17ce":"<h1>Deep Dive in AI<\/h1>","7bc6fc0a":"Do not forget to save your notebook before you close this window.\n\nNext session goes to a similar process for structured data \/ tabular data. Click on the link below to reach the reference notebook which you can copy and edit:\n\n<a href=\"https:\/\/www.kaggle.com\/vtecftwy\/ai-deep-dive-02-td\" target=\"_black\"><div align=\"center\"><button >Click here to open the \"Tabular Data: notebook<\/button><\/div><\/a>","1e7a6f72":"You may have heard that (supervised) machine learning uses **data** to train a **model**, which we then can use to make prediction on new data:\n\n><div align=\"left\" width=100%><img src=\"https:\/\/raw.githubusercontent.com\/vtecftwy\/fastbook\/master\/resources\/img\/anatomy_dl_01.svg\" width=33%><\/div>\n\n**Deep learning** is one type of supervised machine learning, using an architecture remotely inspired from biological neurons. We use **data** (inputs and solutions a.k.a. labels or ground truth) and a specific **architecture** (artificial neural network architecture) to train a **model**. The training process will give us learned **parameters**. \n\n><div align=\"left\" width=100%><img src=\"https:\/\/raw.githubusercontent.com\/vtecftwy\/fastbook\/master\/resources\/img\/anatomy_dl_02.svg\" width=33%><\/div>\n\nOnce we have our trained model (i.e. an **architecture** with **parameters**), we can use it to make prediction on **new data**.\n><div align=\"left\" width=100%><img src=\"https:\/\/raw.githubusercontent.com\/vtecftwy\/fastbook\/master\/resources\/img\/anatomy_dl_03.svg\" width=33%><\/div>\n\n\n","ca948833":"## 1\ufe0f\u20e3 Frame the problem","6185b71c":"#### Create a model CNN model for computer vision\nWe create the model by telling the computer:\n- the data to use to train the model: `dls`\n- the architecture to use: `resnet18`. if we do not tell otherwise, `fastai` will assume we use the pretrained resnet\n- the metrics we want to monitor to evaluate the performance. We will use the accuracy, precision and recall. We will explain later what these are\n\nThe first time we run the cell, fastai will download the pretrained model parameters to use them. After that it will use the copy it has saved on disk.","dc85b3a3":"\ud83d\udc4d This is a much better performance. We still have a few mistakenly classified images but a much smaller percentage on a total of more than 13,000 images in the validation set (30% of 44,000). \n\nThe metrics on the validation set are also much better:\n\nModel | Accuracy | Precision | Recall\n:-----|:--------:|:---------:|:-----:\nDGG   | 0.875000 | 0.833333  | 0.909091\n44k   | 0.980200 | 0.981170 | 0.978976\n\n\nWe can expect much less complaints by people with a mask stopped at the gate \ud83d\ude00 !\n\nIn addition by looking at the top mistakes, it is clear that some of them are wrongly classified or just unclassifiable even by humans.","2e4c82f5":"# \ud83d\udcd8 Anatomy of a deep learning model","687ec9ed":"#### Clean up data\nLooking above, it clear that some of the images are not correct or not optimum for training. We need to delete some of them. \n- images with something not related to the class (e.g. mask but no face, ...)\n- images that are drawings and not real photos\n- multiple faces, ...\n\nIt is a judgment call, there is no formal rule for that. Use common sense.\n\nTo make this easier, we will use a tool called `display_image_cleaner`","85e42bd4":"We have seen that we can get a decent model with only between 50 adn 100 images of each class. But we also saw that the result need improvement. The best way to get more out of the model is to feed it more data. \n\nWe could download 400 images and clean these, and then retrain the model. \n\nBut let's go crazy !\n\nWe will use the **Facemask-44k** dataset already available on Kaggle. With 44,000 images, we are going all the way toward large dataset. It is an overkill but it will certainly give us a better result.","bb6264aa":"The cell below will automate the search and download process for you. It will:\n- create a path to the folder where we want the images to be saved\n- run the Duckduckgo search for \"with_mask\" and \"without_mask\" classes\n- check that all images are correctly downloaded and delete any defective ones","80d3c5a4":"# \ud83d\udcf7 Computer Vision","ff210939":"## Create a new model and train it\n\nBe prepared, training will take longer then before. We have close to 300 times more images. You can count on 2 to 4 minutes per epoch.\n\nIt is longer, but as a comparison, a full training from scratch of resnet would take something like **14 days** on a normal GPU like we have here.\n\nWe create a model like before and then finetune it for 5 epochs","68bac7e9":"\n1. Collecting the data: `DataBlock()`\n    - defines what type of data, e.g. `ImageBlock` and `CategoryBlock`\n    - define how the data is collected, e.g. get the image from a function that will look into a folder the return any image file\n    - define how to get the label names, e.g. class name\/label name is the name of the folder where the images are\n    - some preprocessing to do for each image, e.g. resizing then all to a square image or fixed value\n\n```\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock), \n                   get_items=get_image_files, \n                   splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                   get_y=parent_label,\n                   item_tfms=Resize(128)\n                   )\n```\n","53bf6ed5":"## 5\ufe0f\u20e3 Improve the model by using more data","ede41fb4":"The code below randomly selects a small number of images from the huge dataset to use as a test set.","f06ab475":"[Double click here to edit this cell and and write your ideas.]","aa34c21e":"#### Update our `dataloaders`\nOur data are now cleaned up a little. We recreate our dataloaders, because we have deleted some images and it is important to refresh it all.","2ea02204":"**Conclusion**:\nThe results are not as good as it first seemed, we only have an accuracy of less then 53%.\n- The good thing is that we do not let pass too many people without a mask \ud83d\udc4d\n\n- But the bad thing is that we would block quite a few people with a mask \ud83d\udc4e !\n\nThis will certainly raise many complaints \ud83d\udca3 !","09df17cb":"Now we do the interpretation again, but using the test dataloader `test_dl`, instead of the validation set.","e95b4cf1":"### Collect data from internet\n\nQuickest and simplest way to create an initial image dataset is to collect them online.\n- Search images with key words or search phrases.\n- Download images and organize them in classes\/categories based on the searches.\n- \ud83d\udc4d : easy and fast. labeling is straigthforward if search well defined\n- \ud83d\udc4e : quality of the images depends of the quality of the search engine and the search phrases. \n\nGood for a baseline, and to explore how things work out. Then we will need to improve.","c4bd9bc2":"### Evaluate the results","c5dd390e":"We will use [duckduckgo](https:\/\/duckduckgo.com\/) which provides good results and is easily reachable.\n><a href=\"https:\/\/duckduckgo.com\" target=\"_blank\"><img src=\"https:\/\/duckduckgo.com\/assets\/common\/dax-logo.svg\" width=5%><\/a>\n\nWe will do the following:\n1. Experiment directly on Duckduckgo to pick a good search phrase for each class\n2. Use these two search phrases to scrape images and download them onto the server for furtehr use","cc41b9ad":"## Program for the deep dive:\n- \ud83d\udcd8 Anatomy of a deep learning application\n- \ud83d\udcf7 **Computer Vision**\n    - \ud83d\udcd8 Introduction to the project\n    - \ud83d\udc68\u200d\ud83d\udcbb Data processing, train model, evaluate model\n    - \ud83d\udc68\u200d\ud83d\udcbb Improve model\n- \ud83d\udcc8 **Structured Data \/ Tabular Data**\n    - \ud83d\udcd8 Introduction to the project\n    - \ud83d\udc68\u200d\ud83d\udcbb Data processing, train model, evaluate model\n- \ud83d\udd24 **Natural Language Processing**\n    - \ud83d\udc68\u200d\ud83d\udcbb Experiment with several tools\n\nThe current notebook covers an introduction and the computer vision section. At the end, a link will bring you to the next notebook.","587e55d4":"It is clear that the model misses quite a few cases when the image is not that good. But a good model should be able to see that, we, humans, can.","17f1d1fb":"We have reserved a about 20% of the images for `validation`, with is checking how the model performs on images it has not seen during training. If the model **generalizes**, the result of these images should be good as well.\n\nOne good tool to evalute the model is to plot the confusion matrix:","377144d5":"# Final comments and next step","ebbeae53":"#### \ud83d\udd0e Details on how each step is structure in code (optional)","08922d01":"For these deep dive sessions, we will use a python library called **fast.ai**. It provides state or the art implementation of deep learning, while also making it possible to access this power from a high level API (simple high level code).\n><a href=\"https:\/\/docs.fast.ai\/\"><img src=\"https:\/\/docs.fast.ai\/images\/company_logo.png\"><\/a>\n\n**The steps to train a model include:**\n1. **Collecting the data and preparing them so that the model can learn from them**\n2. **Selecting an architecture and creating a specific model using that architecture**\n3. **Training the model by \"feeding\" the data to the model.**","184eea0c":"### Create a test set\nNow we create a test dataset. To do so, we use the same dataloaders`dls` as before, but we tell the computer to look at the images from the list of image file names `test_image_fnames`, and to perform the same type of actions (recipe) as for the training dataset.","ef113900":"How good is our model? The metrics have improved during training, but is the final result a good one?","1f8f9761":"[Double click here to edit this cell and and write your ideas.]","6f8d6c83":"Let's have a look at the images again","e76646cf":"# \ud83e\uddf0 Imports and configurations\nJust run the cells in this section to get ready. Do not worry about the code here.","f05b7113":"Let's have a look at the biggest mistakes","9b0b8cfd":"From here, there are many techniques to further improve and stress test the model. But this is beyond what we wanted you to experience during this session.\n\nThe next steps, beside further improvement, will include:\n- saving the model so that it can be used by others\n- creating an application using the saved model and easiy classify new images.\n\nIn our case, the application will be integrated into our existing temperature system and interface with the access control system. This is pure IT design, and will use the outcome of this discovery phase to build a strong and resilient model,","4db9c2cc":"We can define between 50 and 450 images to download for each class. We will start with 100","78c53a28":"# Introduction\nThis notebook is the first of three, guiding anyone interested in understanding what deep learning do through the steps of what deep learning practitioners do. For each step, you will see an explanation in words, and the corresponding simple code allowing you to execute\/run each step.\n\nFollow all the steps one by one and you will build your own deep learning model.","d2107b88":"To solve our ML problem, we need a dataset with the following information:\n- a set of images with faces of people wearing a mask\n- a set of images with faces of people not wearing a mask\n\nWe should have between at meast between 50 and 100 images for each set. Since we will need to clean up and delete some images from our set, it is safer to start with 100 or 150.","164af0cc":"This page is a hosted Jupyter notebook. This is a special tooks that allows to write a page with text and illustration but also to run specific code and see the result of your own code. This facility is made available by Kaggle, and provides this convenient space to learn deep learning or other machine learning techniques.\n\n\n**Step by step instructions**:\n1. You got to this page. You should see a button \"Copy and Edit\" at the top right of the page. Click on it and after a little while you will see a new notebook opened, with a new name. This is your version of the notebook, You can run it and modify it.\n\n>Note: if you right click on the \"Copy and Edit\" button to open the new notebook in a new tab, you will keep the original read-only version of the notebook for comparison as well.\n\n>Once your new notebook is active, you have access to your own computing and storage capacity on a computer in the cloud. It is available to you for a few hours and will stay active unless you do not do anything for 15 min or so.\n\n2. You can run\/execute code in the notebook now. But you will need to sign in or register in order to save your own notebook, access internet resources to download external resources onto the cloud computer, of access the GPU computing capabilities (usefull to accelerate the computer vision section). It is therefore recommended that you sign in if you have a kaggle account, or create one. Also, be aware that Kaggle requires a two-step authentication using SMS, for cloud internet access and GPU access, which means you will have to give a mobile phone number.\n\n>What happened if you prefer to skip the \"Get phone verified\" step?\n> - you will not be able to perform the first step of the computer vision notebook because it requires to download images from internet and to add a new software package also from internet.\n> - you can run the second part of the notebook (Improve the model) but because you will not use a GPU, training may take a long time.\n\n3. Once you have signed in, check that the settings are correct. You access the setting by looking at the right side:\n    - Language: Python\n    - Environment: Preferences\n    - Accelerator: GPU\n    - Internet: The selector should show that it is enabled (Check mark instead of a minus sign)\n    \n4. Now you are ready to start the deep dive into deep learning. How to you proceed?\n    - The notebook will have two types of text blocks: (1) descriptions using text and images and (2) code cells (in a light grey box - )\n    - Once you reach a code box, click inside and run it by clicking on the small arrow that appears next to the top left side of the box or at the top on the page.  \n    - While the cell is running, the arrow is replaced by a square, with a rotating circle. Once the code is completed, the arrow will appear again and you may also see additional output.","e972da3a":"(note number may be different when you run your model)\n\nThe table above says that from all the images in the validation set (remember, we only use 20% or a small set of less then 200):\n- 4 images true \"with_mask\" were classified correctly \n- 16 images true \"without_mask\" were classified correctly\n- 3 images true \"with_mask\" were classified wrongly\n- 0 image true \"without_mask\" was classified wrongly\n\n>Accuracy = 0.869: total number of correctly classified images \/ total images = (4+16)\/23\n\n>Precision = 0.842: nbr true \"without_mask\" \/ (nbr true \"without_mask\"+nbr false \"without_mask\") = 16\/(16+3)\n\n>Recall = 1.000: nbr true \"without_mask\" \/ (number true \"without_mask\" + number of false \"with_mask\") = 16\/(16+0)","74931805":"2. Indicate where the data are, e.g. a path to a specific folder: `dataloaders()`\n```\ndls = dblock.dataloaders(path)<br\/>\n```\n","8030042c":"### What we have:\n- An access control system\n- Camera system for person detection and identification\n- Temperature system to measure the temperature of each individual and control access if temperature is too high\n- Control system linking both systems and the physical access control\n\n","0edb045d":"We also can visualize the images where we have errors"}}