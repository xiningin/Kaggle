{"cell_type":{"9ea06c45":"code","456945b7":"code","e88aeb9e":"code","2d47543f":"code","04e2c6fd":"code","7be25665":"code","11cd3dc9":"code","9ceb312f":"code","f758ecf1":"code","dfe74101":"code","62c970ce":"code","47a508ed":"code","19273680":"code","015dccd3":"code","a5c7ef85":"code","f29236fa":"code","dad8387f":"code","c826518f":"code","aa0dd425":"code","6b43a91f":"code","86c82197":"code","0a727bab":"code","6cc81567":"code","ef81478b":"code","c5d69410":"code","4920b73e":"code","8a3d380e":"code","4b5b21a9":"code","504c008c":"code","d7da6912":"code","3dafee38":"code","a69d22ac":"code","b80c7d5b":"code","a8ebcb51":"code","127d66d4":"code","55f8313a":"code","a795df2f":"code","e3a873e3":"code","277a9464":"code","cdc5d538":"code","bcd6b1c9":"code","79da15b3":"code","bb336c1e":"code","9592b6e0":"code","06a06a13":"code","f5437d8a":"code","5fa45e25":"code","d529e733":"code","546ba881":"code","d92a03bf":"code","8c567978":"code","c78d221c":"code","7b4c6e49":"code","087120cf":"code","fb749f8e":"markdown","420f8d47":"markdown","b8fda5c8":"markdown","f754b47a":"markdown","6d484a6a":"markdown","fd2b4a62":"markdown","3449d721":"markdown","9cdd65f7":"markdown","8514b98f":"markdown","7a88cb03":"markdown","74f5c1dc":"markdown","d9041936":"markdown","1c95351d":"markdown","d998940f":"markdown","09c445bd":"markdown","7231aff3":"markdown"},"source":{"9ea06c45":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_palette(\"coolwarm\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","456945b7":"train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv', index_col ='id')\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv', index_col ='id')","e88aeb9e":"pd.set_option(\"display.max.columns\", None)\npd.set_option(\"display.precision\", 2)","2d47543f":"train.head()","04e2c6fd":"test.head()","7be25665":"print('Rows and Columns in train dataset:', train.shape)\nprint('Rows and Columns in test dataset:', test.shape)","11cd3dc9":"# A very big thank you to @dwin183287. The notebook really eased my job here! https:\/\/www.kaggle.com\/dwin183287\/tps-mar-2021-eda-models\n\ncat_features = [feature for feature in train.columns if 'cat' in feature]\ncont_features = [feature for feature in train.columns if 'cont' in feature]","9ceb312f":"plt.figure(figsize=(10,4), facecolor=\"#f7e4e4\")\nsns.heatmap(train.isnull(), cbar=False, yticklabels=False, cmap=\"coolwarm\")\nax = plt.axes()\nax.set_facecolor(\"#f7e4e4\")\nplt.title(\"Figure 1: HeatMap to check Null values in Train Dataset\", fontsize=16)\nplt.show()","f758ecf1":"plt.figure(figsize=(10,4), facecolor=\"#f7e4e4\")\nsns.heatmap(test.isnull(), cbar=False, yticklabels=False, cmap=\"coolwarm\")\nax = plt.axes()\nax.set_facecolor(\"#f7e4e4\")\nplt.title(\"Figure 2: HeatMap to check Null values in Test Dataset\", fontsize=16)\nplt.show()","dfe74101":"print('Missing values in train dataset:', sum(train.isnull().sum()))\nprint('Missing values in test dataset:', sum(test.isnull().sum()))","62c970ce":"plt.figure(figsize=(10,4), facecolor=\"#f7e4e4\")\nsns.kdeplot(x=\"cont0\", data=train, shade=True)\nax = plt.axes()\nax.set_facecolor(\"#f7e4e4\")\nplt.title(\"Figure 3: Continuous feature 'cont0'\", fontsize=16)\nsns.despine(left=True)\nplt.ylabel(\"\")\nplt.show()","47a508ed":"plt.figure(figsize=(10,4), facecolor=\"#f7e4e4\")\nsns.kdeplot(x=\"cont1\", data=train, shade=True)\nax = plt.axes()\nax.set_facecolor(\"#f7e4e4\")\nplt.title(\"Figure 3: Continuous feature 'cont1'\", fontsize=16)\nsns.despine(left=True)\nplt.ylabel(\"\")\nplt.show()","19273680":"fig, axes = plt.subplots(4,3, figsize=(15, 10), facecolor='#f7e4e4', sharey = 'all', sharex = True)\nfig.suptitle(\"Figure 5. Continuous Features Distribution on Train Dataset\", fontsize=16, y=0.92)\nsns.kdeplot(ax=axes[0,0], data=train, x=\"cont0\", shade=True)\nsns.kdeplot(ax=axes[0,1], data=train, x=\"cont1\", shade=True)\nsns.kdeplot(ax=axes[0,2], data=train, x=\"cont2\", shade=True)\nsns.kdeplot(ax=axes[1,0], data=train, x=\"cont3\", shade=True)\nsns.kdeplot(ax=axes[1,1], data=train, x=\"cont4\", shade=True)\nsns.kdeplot(ax=axes[1,2], data=train, x=\"cont5\", shade=True)\nsns.kdeplot(ax=axes[2,0], data=train, x=\"cont6\", shade=True)\nsns.kdeplot(ax=axes[2,1], data=train, x=\"cont7\", shade=True)\nsns.kdeplot(ax=axes[2,2], data=train, x=\"cont8\", shade=True)\nsns.kdeplot(ax=axes[3,0], data=train, x=\"cont9\", shade=True)\nsns.kdeplot(ax=axes[3,1], data=train, x=\"cont10\", shade=True)\nplt.delaxes(ax=axes[3,2])\nplt.yticks([], [])\nsns.despine(left=True)\nplt.show()","015dccd3":"train[cont_features].describe()","a5c7ef85":"fig, axes = plt.subplots(4,3, figsize=(15, 10), facecolor='#f7e4e4', sharey = 'all', sharex = True)\nfig.suptitle(\"Figure 6. Continuous Features Distribution on Test Dataset\", fontsize=16, y=0.92)\nsns.kdeplot(ax=axes[0,0], data=test, x=\"cont0\", shade=True)\nsns.kdeplot(ax=axes[0,1], data=test, x=\"cont1\", shade=True)\nsns.kdeplot(ax=axes[0,2], data=test, x=\"cont2\", shade=True)\nsns.kdeplot(ax=axes[1,0], data=test, x=\"cont3\", shade=True)\nsns.kdeplot(ax=axes[1,1], data=test, x=\"cont4\", shade=True)\nsns.kdeplot(ax=axes[1,2], data=test, x=\"cont5\", shade=True)\nsns.kdeplot(ax=axes[2,0], data=test, x=\"cont6\", shade=True)\nsns.kdeplot(ax=axes[2,1], data=test, x=\"cont7\", shade=True)\nsns.kdeplot(ax=axes[2,2], data=test, x=\"cont8\", shade=True)\nsns.kdeplot(ax=axes[3,0], data=test, x=\"cont9\", shade=True)\nsns.kdeplot(ax=axes[3,1], data=test, x=\"cont10\", shade=True)\nplt.delaxes(ax=axes[3,2])\nplt.yticks([], [])\nsns.despine(left=True)\nplt.show()","f29236fa":"test[cont_features].describe()","dad8387f":"# Thank you to @andreshg notebook. https:\/\/www.kaggle.com\/andreshg\/tps-march-a-complete-study\nplt.figure(figsize=(15, 10), facecolor='#f7e4e4')\nmask = np.triu(np.ones_like(train[cont_features].corr().abs(), dtype=np.bool))\nsns.heatmap(train[cont_features].corr().abs(), mask= mask, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={\"shrink\": .8}, vmin=0, vmax=1)\nax = plt.axes()\nax.set_facecolor(\"#f7e4e4\")\nplt.title(\"Figure 7. Features Correlation on the Train Dataset\", fontsize=16)\nplt.show()","c826518f":"plt.figure(figsize=(15, 10), facecolor='#f7e4e4')\nmask = np.triu(np.ones_like(test[cont_features].corr().abs(), dtype=np.bool))\nsns.heatmap(test[cont_features].corr().abs(), mask= mask, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={\"shrink\": .8}, vmin=0, vmax=1)\nax = plt.axes()\nax.set_facecolor(\"#f7e4e4\")\nplt.title(\"Figure 8. Features Correlation on the Test Dataset\", fontsize=16)\nplt.show()","aa0dd425":"fig, axes = plt.subplots(7,3, figsize=(25, 20), facecolor='#f7e4e4', sharey = True)\nfig.suptitle(\"Figure 9. Categorical Features Distribution on Train Dataset\", fontsize=16, y=0.90)\nsns.countplot(ax=axes[0,0], data=train, x=\"cat0\", palette=\"coolwarm\")\nsns.countplot(ax=axes[0,1], data=train, x=\"cat1\", palette=\"coolwarm\")\nsns.countplot(ax=axes[0,2], data=train, x=\"cat2\", palette=\"coolwarm\")\nsns.countplot(ax=axes[1,0], data=train, x=\"cat3\", palette=\"coolwarm\")\nsns.countplot(ax=axes[1,1], data=train, x=\"cat4\", palette=\"coolwarm\")\nsns.countplot(ax=axes[1,2], data=train, x=\"cat5\", palette=\"coolwarm\")\nsns.countplot(ax=axes[2,0], data=train, x=\"cat6\", palette=\"coolwarm\")\nsns.countplot(ax=axes[2,1], data=train, x=\"cat7\", palette=\"coolwarm\")\nsns.countplot(ax=axes[2,2], data=train, x=\"cat8\", palette=\"coolwarm\")\nsns.countplot(ax=axes[3,0], data=train, x=\"cat9\", palette=\"coolwarm\")\nsns.countplot(ax=axes[3,1], data=train, x=\"cat10\", palette=\"coolwarm\")\nsns.countplot(ax=axes[3,2], data=train, x=\"cat11\", palette=\"coolwarm\")\nsns.countplot(ax=axes[4,0], data=train, x=\"cat12\", palette=\"coolwarm\")\nsns.countplot(ax=axes[4,1], data=train, x=\"cat13\", palette=\"coolwarm\")\nsns.countplot(ax=axes[4,2], data=train, x=\"cat14\", palette=\"coolwarm\")\nsns.countplot(ax=axes[5,0], data=train, x=\"cat15\", palette=\"coolwarm\")\nsns.countplot(ax=axes[5,1], data=train, x=\"cat16\", palette=\"coolwarm\")\nsns.countplot(ax=axes[5,2], data=train, x=\"cat17\", palette=\"coolwarm\")\nsns.countplot(ax=axes[6,0], data=train, x=\"cat18\", palette=\"coolwarm\")\nsns.despine(left=True)\nplt.delaxes(ax=axes[6,1])\nplt.delaxes(ax=axes[6,2])\nplt.show()","6b43a91f":"fig, axes = plt.subplots(7,3, figsize=(25, 20), facecolor='#f7e4e4', sharey = True)\nfig.suptitle(\"Figure 10. Categorical Features Distribution on Test Dataset\", fontsize=16, y=0.90)\nsns.countplot(ax=axes[0,0], data=test, x=\"cat0\", palette=\"coolwarm\")\nsns.countplot(ax=axes[0,1], data=test, x=\"cat1\", palette=\"coolwarm\")\nsns.countplot(ax=axes[0,2], data=test, x=\"cat2\", palette=\"coolwarm\")\nsns.countplot(ax=axes[1,0], data=test, x=\"cat3\", palette=\"coolwarm\")\nsns.countplot(ax=axes[1,1], data=test, x=\"cat4\", palette=\"coolwarm\")\nsns.countplot(ax=axes[1,2], data=test, x=\"cat5\", palette=\"coolwarm\")\nsns.countplot(ax=axes[2,0], data=test, x=\"cat6\", palette=\"coolwarm\")\nsns.countplot(ax=axes[2,1], data=test, x=\"cat7\", palette=\"coolwarm\")\nsns.countplot(ax=axes[2,2], data=test, x=\"cat8\", palette=\"coolwarm\")\nsns.countplot(ax=axes[3,0], data=test, x=\"cat9\", palette=\"coolwarm\")\nsns.countplot(ax=axes[3,1], data=test, x=\"cat10\", palette=\"coolwarm\")\nsns.countplot(ax=axes[3,2], data=test, x=\"cat11\", palette=\"coolwarm\")\nsns.countplot(ax=axes[4,0], data=test, x=\"cat12\", palette=\"coolwarm\")\nsns.countplot(ax=axes[4,1], data=test, x=\"cat13\", palette=\"coolwarm\")\nsns.countplot(ax=axes[4,2], data=test, x=\"cat14\", palette=\"coolwarm\")\nsns.countplot(ax=axes[5,0], data=test, x=\"cat15\", palette=\"coolwarm\")\nsns.countplot(ax=axes[5,1], data=test, x=\"cat16\", palette=\"coolwarm\")\nsns.countplot(ax=axes[5,2], data=test, x=\"cat17\", palette=\"coolwarm\")\nsns.countplot(ax=axes[6,0], data=test, x=\"cat18\", palette=\"coolwarm\")\nsns.despine(left=True)\nplt.delaxes(ax=axes[6,1])\nplt.delaxes(ax=axes[6,2])\nplt.show()","86c82197":"fig, axes = plt.subplots(4,3, figsize=(15, 10), facecolor='#f7e4e4', sharey=True, sharex = True)\nfig.suptitle(\"Figure 11. Continuous Features Distribution X Target\", fontsize=16, y=0.93)\nsns.kdeplot(ax=axes[0,0], data=train, x=\"cont0\", shade=True, hue=\"target\", multiple=\"stack\", palette=\"coolwarm\")\nsns.kdeplot(ax=axes[0,1], data=train, x=\"cont1\", shade=True, hue=\"target\", multiple=\"stack\", palette=\"coolwarm\")\nsns.kdeplot(ax=axes[0,2], data=train, x=\"cont2\", shade=True, hue=\"target\", multiple=\"stack\", palette=\"coolwarm\")\nsns.kdeplot(ax=axes[1,0], data=train, x=\"cont3\", shade=True, hue=\"target\", multiple=\"stack\", palette=\"coolwarm\")\nsns.kdeplot(ax=axes[1,1], data=train, x=\"cont4\", shade=True, hue=\"target\", multiple=\"stack\", palette=\"coolwarm\")\nsns.kdeplot(ax=axes[1,2], data=train, x=\"cont5\", shade=True, hue=\"target\", multiple=\"stack\", palette=\"coolwarm\")\nsns.kdeplot(ax=axes[2,0], data=train, x=\"cont6\", shade=True, hue=\"target\", multiple=\"stack\", palette=\"coolwarm\")\nsns.kdeplot(ax=axes[2,1], data=train, x=\"cont7\", shade=True, hue=\"target\", multiple=\"stack\", palette=\"coolwarm\")\nsns.kdeplot(ax=axes[2,2], data=train, x=\"cont8\", shade=True, hue=\"target\", multiple=\"stack\", palette=\"coolwarm\")\nsns.kdeplot(ax=axes[3,0], data=train, x=\"cont9\", shade=True, hue=\"target\", multiple=\"stack\", palette=\"coolwarm\")\nsns.kdeplot(ax=axes[3,1], data=train, x=\"cont10\", shade=True, hue=\"target\", multiple=\"stack\", palette=\"coolwarm\")\nplt.delaxes(ax=axes[3,2])\nsns.despine(left=True)\nplt.show()","0a727bab":"train_target_0 = train[train[\"target\"]==0]\ntrain_target_1 = train[train[\"target\"]==1]","6cc81567":"fig, axes = plt.subplots(7,3, figsize=(25, 20), facecolor='#f7e4e4', sharey=True)\nfig.suptitle(\"Figure 12. Categorical Features Distribution X Target 0\", fontsize=16, y=0.90)\nsns.countplot(ax=axes[0,0], x=\"cat0\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[0,1], x=\"cat1\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[0,2], x=\"cat2\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[1,0], x=\"cat3\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[1,1], x=\"cat4\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[1,2], x=\"cat5\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[2,0], x=\"cat6\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[2,1], x=\"cat7\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[2,2], x=\"cat8\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[3,0], x=\"cat9\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[3,1], x=\"cat10\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[3,2], x=\"cat11\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[4,0], x=\"cat12\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[4,1], x=\"cat13\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[4,2], x=\"cat14\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[5,0], x=\"cat15\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[5,1], x=\"cat16\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[5,2], x=\"cat17\", data=train_target_0, palette=\"coolwarm\")\nsns.countplot(ax=axes[6,0], x=\"cat18\", data=train_target_0, palette=\"coolwarm\")\nplt.delaxes(ax=axes[6,1])\nplt.delaxes(ax=axes[6,2])\nsns.despine(left=True)\nplt.show()","ef81478b":"fig, axs = plt.subplots(7,3, figsize=(25, 20), facecolor='#f7e4e4', sharey=True)\nfig.suptitle(\"Figure 13. Categorical Features Distribution X Target 1\", fontsize=16, y=0.90)\nsns.countplot(ax=axs[0,0], x=\"cat0\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[0,1], x=\"cat1\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[0,2], x=\"cat2\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[1,0], x=\"cat3\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[1,1], x=\"cat4\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[1,2], x=\"cat5\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[2,0], x=\"cat6\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[2,1], x=\"cat7\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[2,2], x=\"cat8\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[3,0], x=\"cat9\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[3,1], x=\"cat10\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[3,2], x=\"cat11\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[4,0], x=\"cat12\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[4,1], x=\"cat13\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[4,2], x=\"cat14\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[5,0], x=\"cat15\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[5,1], x=\"cat16\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[5,2], x=\"cat17\", data=train_target_1, palette=\"coolwarm\")\nsns.countplot(ax=axs[6,0], x=\"cat18\", data=train_target_1, palette=\"coolwarm\")\nplt.delaxes(ax=axs[6,1])\nplt.delaxes(ax=axs[6,2])\nsns.despine(left=True)\nplt.show()","c5d69410":"target = train.pop('target')","4920b73e":"from sklearn.model_selection import train_test_split\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(train, target, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","8a3d380e":"# Get list of categorical variables\ns = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","4b5b21a9":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","504c008c":"X_train.select_dtypes(exclude=['object']).head()","d7da6912":"drop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n\nprint(\"MAE from Approach 1 (Drop categorical variables):\")\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))","3dafee38":"from sklearn.preprocessing import LabelEncoder","a69d22ac":"# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    train[col] = label_encoder.fit_transform(train[col])","b80c7d5b":"X_train, X_valid, y_train, y_valid = train_test_split(train, target, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","a8ebcb51":"print(\"MAE from Approach 2 (Label Encoding):\") \nprint(score_dataset(X_train, X_valid, y_train, y_valid))","127d66d4":"from sklearn.preprocessing import OneHotEncoder","55f8313a":"# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))","a795df2f":"# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index","e3a873e3":"# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)","277a9464":"# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)","cdc5d538":"print(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))","bcd6b1c9":"train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv', index_col ='id')\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv', index_col ='id')","79da15b3":"combine_df = pd.concat([train, test], axis=0)","bb336c1e":"# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_train = pd.DataFrame(OH_encoder.fit_transform(train[object_cols]))\n\n# One-hot encoding removed index; put it back\nOH_train.index = train.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_train = train.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_train_final = pd.concat([num_train, OH_train], axis=1)","9592b6e0":"target = OH_train_final.pop('target')","06a06a13":"X_train, X_valid, y_train, y_valid = train_test_split(OH_train_final, target, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","f5437d8a":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)","5fa45e25":"y_pred = lr.predict(X_valid)","d529e733":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\ndef cross_val(model):\n    pred = cross_val_score(model, OH_train_final, target, cv=10)\n    return pred.mean()\n\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)\n    \ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return mae, mse, rmse, r2_square","546ba881":"train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv', index_col ='id')\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv', index_col ='id')","d92a03bf":"combine_df = pd.concat([train, test], axis=0)","8c567978":"combine_df","c78d221c":"# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in cat_features:\n    combine_df[col] = label_encoder.fit_transform(combine_df[col])","7b4c6e49":"combine_df","087120cf":"X_train, X_valid, y_train, y_valid = train_test_split(, target, train_size=0.8, test_size=0.2,\n                                                                random_state=42)","fb749f8e":"### Categorical Features Data Exploration","420f8d47":"### Logistic Regression","b8fda5c8":"### Work on progress...","f754b47a":"### Continuous Features Data Exploration","6d484a6a":"### Preparation","fd2b4a62":"### Target Feature Data Exploration","3449d721":"### Correlation ","9cdd65f7":"## Machine Learning","8514b98f":"## Exploratory Data Analysis","7a88cb03":"## Categorical Variables","74f5c1dc":"### Null values","d9041936":"**Important Points:**\n\n* EDA on Continuous Variables                       \u2705\n* EDA on Categorical Variables                      \u2705\n* One Hot Encoding                                  \u2705\n* Logistic Regression\n* KNN\n* Random Forest Regression      (To be learned yet)\n* Naive Bayes                   (To be learned yet)\n* AdaBoost Regression           (To be learned yet)\n* Gradient Boost Regression     (To be learned yet)\n* LGBM Regression               (To be learned yet)\n* XGB Regression                (To be learned yet)\n* Feature Engineering           (To be learned yet)\n* MLP Regressor                 (To be learned yet)\n* Extra Trees Regressor         (To be learned yet)","1c95351d":"------------------------","d998940f":"### Drop Categorical Variables","09c445bd":"### Label Encoding","7231aff3":"### One Hot Encoding"}}