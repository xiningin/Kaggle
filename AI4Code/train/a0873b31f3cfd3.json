{"cell_type":{"9cd9f9a9":"code","5d278a76":"code","7f502a1c":"code","01a3df09":"code","e08af477":"code","1bedf9a0":"code","17b14ac6":"code","76407e6e":"code","4704edbb":"code","6f40da6d":"code","28336103":"code","a83fab51":"code","20d636cb":"code","cf729239":"code","94874f90":"code","35f769d5":"code","359a0a02":"code","425d4d32":"code","a87ad1d2":"code","0430962c":"code","94103523":"code","5242a8b5":"code","bd1e88e9":"code","a4b5bff4":"code","88d0bb2a":"code","f040a4d9":"code","b98f8cd3":"code","d604e6fd":"code","421a0ad0":"code","7c0dc39a":"code","29569e5a":"code","89734e28":"code","7e9886cc":"markdown","b95ccd05":"markdown","949f8105":"markdown","bf104fbe":"markdown","348b633a":"markdown","5d984aea":"markdown","604fbae9":"markdown","d37096f5":"markdown","da3228bd":"markdown","fcd1df8f":"markdown","5ff44a10":"markdown","960197eb":"markdown","a791d819":"markdown","c1aca309":"markdown","330daac3":"markdown","bfe4acd0":"markdown"},"source":{"9cd9f9a9":"!pip install ipython-autotime\n%load_ext autotime","5d278a76":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re \nimport os\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder","7f502a1c":"#listdir returns a list containing the names of the entries in the directory given as a parameter\nlabels = os.listdir('..\/input\/sanad-dataset') \n\n# tf.keras.preprocessing.text_dataset_from_directory Generates a \n# tf.data.Dataset from text files in a directory.\nraw_data = tf.keras.preprocessing.text_dataset_from_directory(\n    '..\/input\/sanad-dataset',\n    # \"inferred\" : the labels are generated from the directory structure\n    labels = \"inferred\",\n    # \"int\": the labels are encoded as integers\n    label_mode = \"int\",\n    # Maximum size of a text string. Texts longer than this will be shortened \n    # to max_length unless it's None ra7at explanation f kil zit.\n    max_length = None,\n    # Whether to shuffle the data. If False, sorts the data in alphanumeric order.\n    shuffle=True,\n    # Finally haja fahmetha mn bkri\n    seed=11,\n    # Optional float between 0 and 1, fraction of data to reserve for validation\n    validation_split=None,\n    # Only used if validation_split is set, mahich set alors sotit\n    subset=None,\n)\n","01a3df09":"print(\"Classes names:\\n\",raw_data.class_names)","e08af477":"type(raw_data)","1bedf9a0":"x=[]\ny=[]\nfor text_batch, label_batch in raw_data:\n    for i in range(len(text_batch)):\n        s=text_batch.numpy()[i].decode(\"utf-8\") \n        x.append(s)\n        y.append(raw_data.class_names[label_batch.numpy()[i]])\nprint(len(x))\nprint(len(y))","17b14ac6":"type(x)","76407e6e":"x[:1]","4704edbb":"y[:1]","6f40da6d":"unique, counts = np.unique(y, return_counts=True)\nplt.figure(\"classe Pie\", figsize=(10, 10))\nplt.title(\"Pie plot of the class frequencies\")\nplt.pie(counts, labels=labels)\nplt.legend(unique)\nplt.show();","28336103":"plt.bar( labels,counts)\nplt.show();","a83fab51":"data =pd.DataFrame({\"text\":x,\"label\":y}) ","20d636cb":"data.head()","cf729239":"data.isnull().sum()","94874f90":"data.info()","35f769d5":"data.shape","359a0a02":"data.duplicated().sum()","425d4d32":"data.drop_duplicates(inplace=True)\ndata.duplicated().sum()","a87ad1d2":"stop_words = list(set(stopwords.words('arabic')))\nprint(stop_words)","0430962c":"import re\nimport string\nimport sys\nimport argparse","94103523":"arabic_punctuations = '''`\u00f7\u00d7\u061b<>_()*&^%][\u0640\u060c\/:\"\u061f.,'{}~\u00a6+|!\u201d\u2026\u201c\u2013\u0640'''\nenglish_punctuations = string.punctuation\npunctuations_list = arabic_punctuations + english_punctuations","5242a8b5":"arabic_diacritics = re.compile(\"\"\"\n                             \u0651    | # Tashdid\n                             \u064e    | # Fatha\n                             \u064b    | # Tanwin Fath\n                             \u064f    | # Damma\n                             \u064c    | # Tanwin Damm\n                             \u0650    | # Kasra\n                             \u064d    | # Tanwin Kasr\n                             \u0652    | # Sukun\n                             \u0640     # Tatwil\/Kashida\n                         \"\"\", re.VERBOSE)","bd1e88e9":"def remove_diacritics(text):\n    text = re.sub(arabic_diacritics, '', text)\n    return text","a4b5bff4":"type(data['text'])","88d0bb2a":"#for index, row in data.iterrows():\n    #data['text'] = remove_diacritics(data['text'].to_string())","f040a4d9":"stop_words = set(stopwords.words('arabic'))\n\ndef remove_diacritics(text):\n    arabic_diacritics = re.compile(\"\"\" \u0651    | # Tashdid\n                             \u064e    | # Fatha\n                             \u064b    | # Tanwin Fath\n                             \u064f    | # Damma\n                             \u064c    | # Tanwin Damm\n                             \u0650    | # Kasra\n                             \u064d    | # Tanwin Kasr\n                             \u0652    | # Sukun\n                             \u0640     # Tatwil\/Kashida\n                         \"\"\", re.VERBOSE)\n    text = re.sub(arabic_diacritics, '', str(text))\n    return text\n\ndef remove_emoji(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)\n\ndef clean_text(text):\n    text = \"\".join([word for word in text if word not in string.punctuation])\n    text = remove_emoji(text)\n    text = remove_diacritics(text)\n    tokens = word_tokenize(text)\n    text = ' '.join([word for word in tokens if word not in stop_words])\n    return text","b98f8cd3":"data['cleaned_text'] = data['text'].apply(clean_text)\ndata.head()","d604e6fd":"List = []\nfor row in data.itertuples():\n   List.append(re.sub(r'[\\W\\s]', ' ', row.text))","421a0ad0":"data['attempt_million_cleaned'] = List","7c0dc39a":"data.head()","29569e5a":"label_encoder = LabelEncoder()\ndata['encodedLabel'] = label_encoder.fit_transform(data['label'])\ndata.head()","89734e28":"data.to_csv(r'.\/textClassSecond.csv', index = False)","7e9886cc":"**Just to get time of execution of each cell**","b95ccd05":"# Exploratory Data analysis","949f8105":"**Each class got the (same)\/(almost the same) frequency as each of the other classes**","bf104fbe":"**Ma7btch tprintili head roht chft type ta3ha \ud83e\udd37**","348b633a":"**Remove **","5d984aea":"**L head  \ud83d\udc83 \ud83d\udc4f\ud83c\udffd \ud83d\udc4f\ud83c\udffd \ud83d\udc4f\ud83c\udffd \ud83d\udc4f\ud83c\udffd**","604fbae9":"# Encode label","d37096f5":"**Check if any duplicate rows in dataset**","da3228bd":"**mb3d ki ylhg l next cell bda ygoli ``Your notebook tried to allocate more memory than is available. It has restarted`` \ud83d\ude22 hani commentit'ha w khalito \ud83d\ude42 w roht nhws 3la code okher  \ud83d\ude43**","fcd1df8f":"**After printing the result seems that for that code (copied from some kaggle notebook) to remove punctuation properly there should be a space between the word and the punct so a second fix**","5ff44a10":"**Displaying the same information in another way hihi \ud83d\ude0e**","960197eb":"**drop the duplicate values**","a791d819":"**From BatchDataset to list (on the way to print l head   \ud83c\udfc3\u200d\u2640\ufe0f)**","c1aca309":"``to_string`` **to transform from type series to string because the functions I found for cleanin only accepts a string as input**","330daac3":"# Data cleaning","bfe4acd0":"**Lists to dataframe psq habit nchouf head \ud83d\ude42**"}}