{"cell_type":{"fa24f08b":"code","2d38c138":"code","33e9a596":"code","62d4b01c":"code","94bb38bc":"code","858c045c":"code","ebc6110a":"code","19a78a8b":"code","14cf7f42":"code","d3cf0a0d":"code","d22467e5":"code","b873a99a":"code","c2f411ee":"code","da76b631":"code","dc161fcb":"code","42d9a43f":"code","261f84ea":"code","a3f04a0a":"code","01236966":"code","49701f55":"code","c7ff5765":"code","4cfbdbcc":"code","853d11a3":"code","90d3cdc7":"code","746cc44d":"code","62b885f3":"code","4bbb0c78":"code","638f7055":"code","39d604ab":"markdown","29279db0":"markdown","74bbfddc":"markdown","41db0a54":"markdown","a1b254ec":"markdown","79e5ccf0":"markdown","cdb5080d":"markdown","50d02378":"markdown","4c8c61e6":"markdown","67545fc6":"markdown","0bdd026b":"markdown","a2851f0e":"markdown","c4d9c0c6":"markdown","1250b166":"markdown","f0e96c96":"markdown","4e081b40":"markdown","6fb33a7b":"markdown","85f87024":"markdown","0d220782":"markdown","1332a226":"markdown","f5ebe91d":"markdown","d9419eab":"markdown","11359121":"markdown","00ff6d35":"markdown","c3a2993d":"markdown","0b1f0845":"markdown","e9d68152":"markdown","dba0b402":"markdown"},"source":{"fa24f08b":"%matplotlib inline\n\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom itertools import chain\n\nfrom scipy.stats import chi2_contingency, zscore\nfrom sklearn.decomposition import PCA \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import (train_test_split, \n                                     GridSearchCV)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import (RandomForestClassifier, \n                              GradientBoostingClassifier)\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')\nsns.set_style(\"whitegrid\")\npd.set_option('precision', 3) ","2d38c138":"df = pd.read_csv('..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nsample_n, var_n = df.shape\nprint(f'This dataset contains {sample_n} samples and {var_n} varialbes')","33e9a596":"var_with_missing_values = df.columns[df.isnull().sum() > 0]\nprint(f'{len(var_with_missing_values)} variable has missing values')","62d4b01c":"cols_with_one_value = [col for col in df.columns if len(df[col].unique()) == 1]\nto_drop = cols_with_one_value + ['EmployeeNumber']\ndf.drop(to_drop, axis=1, inplace=True)","94bb38bc":"measurement_vars = [col for col, dtype in df.dtypes.items() if dtype == 'int' and len(df[col].unique()) > 5]\nordinal_vars = [col for col, dtype in df.dtypes.items() if dtype == 'int' and len(df[col].unique()) <= 5]\nnominal_vars = [col for col, dtype in df.dtypes.items() if dtype != 'int']\nnominal_vars = nominal_vars[1:]\n\ndef print_vars(data_type, var_names):\n    print(f'{data_type}:\\n', ', '.join(var_names), end=\"\\n\\n\")\n\nprint_vars('Measurement Data', measurement_vars)\nprint_vars('Ordinal Data', ordinal_vars)\nprint_vars('Nominal Data', nominal_vars)","858c045c":"df['BusinessTravel'].replace({'Travel_Rarely': 'Rare',\n                             'Travel_Frequently': 'Freq',\n                             'Non-Travel': 'None'}, \n                             inplace=True)\ndf['Department'].replace({'Research & Development': 'R&D',\n                          'Human Resources': 'HR'},\n                            inplace=True)\ndf['EducationField'].replace({'Life Sciences': 'LifeSci',\n                             'Medical': 'Med',\n                             'Marketing': ' Mktg',\n                             'Technical Degree': 'Tech',\n                             'Human Resources': 'HR'},\n                            inplace=True)\ndf['JobRole'].replace({'Sales Executive': 'SalesExec',\n                      'Research Scientist': 'ResSci',\n                      'Laboratory Technician': 'LabTech',\n                      'Manufacturing Director': 'ManuDir',\n                      'Healthcare Representative': 'HCRep',\n                      'Sales Representative': 'SalesRep',\n                      'Research Director': 'ResDir',\n                      'Human Resources': 'HR'},\n                     inplace=True)","ebc6110a":"X = df.drop('Attrition', axis=1)\ny = df['Attrition']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\ndf_train = pd.concat([X_train, y_train], axis=1)","19a78a8b":"df_prop = pd.concat([y_train.value_counts()\/ len(y_train),\n          y_test.value_counts()\/ len(y_test)],\n          axis=1)\ndf_prop.columns = ['training', 'testing']\nprint(df_prop)","14cf7f42":"def plot_EDA(df, cols, subplot_func, title=None, \n             sbp_info =(3,5), sbp_adj={},**kwargs):\n    \"\"\"\n    plot feature distributions\n    \"\"\"\n    n_col, subplot_size = sbp_info\n    n_row = int(np.ceil(len(cols) \/ n_col))\n    \n    fig, ax = plt.subplots(n_row, n_col, \n                           figsize=(n_col * subplot_size, n_row * subplot_size))\n    ax = ax.ravel()\n    \n    for i, col in enumerate(cols):\n        subplot_func(df, col, ax[i], **kwargs)\n    \n    for ax in ax[len(cols):]:\n        ax.axis('off')\n        \n    fig.suptitle(title, fontsize=20)\n    fig.subplots_adjust(**sbp_adj)\n    \n\ndef feature_dist(df, col, ax):\n    \"\"\"\n    subplot function\n    \"\"\"\n    if isinstance(df[col][0], str):\n        data = pd.DataFrame(df[col].value_counts()).reset_index()\n        data.columns = [col, 'count']\n    \n        rotation = 0\n        if sum(map(lambda x: len(x),list(data[col]))) > 20:\n            rotation = 50\n\n        sns.barplot(x=col, y='count', data = data , \n                alpha=0.4, ax=ax).set_title(col, fontsize=14)\n    \n        ax.set_xticklabels(ax.get_xticklabels(), \n                       fontsize=12,\n                       rotation=rotation)\n    else:\n        sns.distplot(df[col], kde=False, ax=ax).set_title(col, fontsize=14)\n        \n    ax.set_xlabel('')   \n","d3cf0a0d":"plot_EDA(X_train, \n         measurement_vars + ordinal_vars + nominal_vars,\n         feature_dist, \n         title=\"Feature Distribution\", \n         sbp_info=(4, 4),\n         sbp_adj={'top': .94, 'hspace': .6})","d22467e5":"def get_ES(df, cols, group=None):\n    \"\"\"\n    get effect size\n    \"\"\"\n    df = df[[group] + cols]\n    means = df.groupby(group).mean().T\n    std = df.groupby(group).std().T\n\n    n1, n2 = df[group].value_counts().values\n    cohen_d = ((means.values * [1, -1]).sum(axis=1) \/ \n          ((std.values ** 2 * [n1 -1, n2 -1]).sum(axis=1) \/ (n1 + n2 -2)) ** .5 )\n    return dict(zip(cols, abs(cohen_d))) \n\ndef get_chi2(df, cols, group=None):\n    \"\"\"\n    get chi square (independence of variables)\n    \"\"\"\n    cols_chi2 = []\n    for col in cols:\n        ct = pd.crosstab(df['Attrition'], df[col])\n        chi2, _, _, _ = chi2_contingency(ct)\n        cols_chi2.append(chi2)\n    return dict(zip(cols, cols_chi2))\n\ndef sort_by_results(df, cols, func=None, group=None, with_value=False):\n    \"\"\"\n    sort by the results of a given test\n    \"\"\"\n    measure = func(df, cols, group=group)\n    if with_value:\n        return sorted(measure.items(), key=lambda x: x[1], reverse=True)\n    else:\n        return sorted(measure, key=measure.get, reverse=True)\n    \ndef measurement_dist_against_attrition(df, col, ax):\n    \"\"\"\n    distribution comparisons for measurement data \n    \"\"\"\n    sns.distplot(df.loc[df['Attrition']=='Yes', col], ax=ax).set_title(col, fontsize=14)\n    sns.distplot(df.loc[df['Attrition']=='No', col], ax=ax)\n    ax.set_xlabel('')\n    #ax.set_xticklabels(ax.get_xticklabels(), fontsize=12)\n\n\ndef categorical_dist_against_attrition(df, col, ax):\n    \"\"\"\n    distribution comparisons for categorical data \n    \"\"\"\n    ct = pd.crosstab(df['Attrition'], df[col]).apply(lambda r: r\/r.sum(), axis=1)\n    stacked = ct.stack().reset_index().rename(columns={0:'value'})\n    \n    rotation = 0\n    if sum(map(lambda x: len(str(x)),list(stacked[col].unique()))) > 30:\n        rotation = 40\n\n    sns.barplot(x=stacked[col], y=stacked.value, \n                hue=stacked.Attrition, \n                alpha=.4, ax=ax).set_title(col, fontsize=14)\n    \n    ax.set_xticklabels(ax.get_xticklabels(), rotation=rotation, fontsize=12)\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n\n    \ndef dist_against_attrition(df, col, ax):\n    if (not isinstance(df[col][0], str)) and len(df[col].unique()) > 6:\n        measurement_dist_against_attrition(df, col, ax)\n    else:\n        categorical_dist_against_attrition(df, col, ax)\n\n","b873a99a":"# sort features by the effect size of comparison on Attrition\nsorted_measurement_vars = sort_by_results(df_train, measurement_vars, \n                                          func=get_ES, group='Attrition')\nsorted_categorical_vars = sort_by_results(df_train, ordinal_vars + nominal_vars, \n                                      func=get_chi2, group='Attrition')\n\n\nplot_EDA(df_train, sorted_measurement_vars + sorted_categorical_vars, \n         dist_against_attrition,\n         title=\"Distribution Against Employee Attrition\",\n         sbp_info=(2, 8),\n         sbp_adj={'top': 0.97, 'hspace': 0.6, 'wspace': 0.4 })","c2f411ee":"y_train = y_train.replace({'No': 0, 'Yes': 1})\ny_test = y_test.replace({'No': 0, 'Yes': 1})","da76b631":"def recode_features(df):\n    new_df = pd.DataFrame()\n    new_df['FarFromHome'] = df['DistanceFromHome'] > 10\n    \n    new_df['YearsPerCompany'] = np.log(df['TotalWorkingYears'] \/ \n                                   (df['NumCompaniesWorked'] + 1) + 1)\n    \n    new_df['Satisfaction'] = zscore(df[['JobSatisfaction', \n                                'EnvironmentSatisfaction', \n                                'RelationshipSatisfaction',\n                                'JobInvolvement', \n                                'WorkLifeBalance']].mean(axis=1))\n    \n    new_df['TravelOften'] = df['BusinessTravel'].map({'Rare': False, 'None': False, 'Freq': True})\n\n    new_df['JuniorLevel'] = df['JobLevel'] == 1\n    new_df['NoStock'] = df['StockOptionLevel'] == 0\n    return new_df\n    \nrecode_cols = ['DistanceFromHome', 'TotalWorkingYears', \n               'NumCompaniesWorked', 'JobSatisfaction', \n               'EnvironmentSatisfaction', 'JobInvolvement', \n               'WorkLifeBalance', 'BusinessTravel','RelationshipSatisfaction',\n               'JobLevel', 'StockOptionLevel']\nremove_cols = ['EducationField', 'Department']","dc161fcb":"def making_dummy(df):\n    return pd.get_dummies(df, drop_first=True)","42d9a43f":"def normalize_vars(training, testing, power=False):\n    if power:\n        tr = PowerTransformer()\n    else:\n        tr = StandardScaler()\n    \n    train = tr.fit_transform(training)\n    test = tr.transform(testing)\n    train_df = pd.DataFrame(train,\n                            index = training.index,\n                            columns = training.columns)\n    test_df = pd.DataFrame(test,\n                           index = testing.index,\n                           columns = testing.columns)\n    return train_df, test_df","261f84ea":"numeric_cols = list(set(measurement_vars) - set(recode_cols) - set(remove_cols))\ncategorical_cols = list(set(nominal_vars + ordinal_vars) - set(recode_cols) - set(remove_cols))\n\n# recode variable\nX_train_recode = recode_features(X_train)\nX_test_recode = recode_features(X_test)\n\n# use dummy coding\nX_train_dummies = making_dummy(\n    pd.concat([X_train[categorical_cols], X_train_recode], axis=1)\n)\nX_test_dummies = making_dummy(\n    pd.concat([X_test[categorical_cols], X_test_recode], axis=1)\n)\n# standard scaling \nX_train_normalized, X_test_normalized = normalize_vars(\n    X_train[numeric_cols], X_test[numeric_cols]\n    )\n\n# combine all features\nX_train_preprocessed = pd.concat([X_train_normalized, \n                                    X_train_dummies], axis=1)\nX_test_preprocessed = pd.concat([X_test_normalized, \n                                    X_test_dummies], axis=1)\n","a3f04a0a":"corr = X_train_preprocessed.corr()\nfig, ax = plt.subplots(1, 1, figsize=(9, 8))\nax = sns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","01236966":"def get_correlated_with_cutoff(corr_matrix, cutoff):\n    \"\"\"\n    get the pairs of variables with correlations higher than the cutoff\n    \"\"\"\n    corr_df =  abs(corr_matrix.unstack().sort_values(ascending=False).drop_duplicates())\n    filtered_df = corr_df[(corr_df < 1) & (corr_df > cutoff)]\n    \n    links = (corr > cutoff).sum() -1\n    \n    return filtered_df, set(chain(*filtered_df.index)), links[links > 0]\n\ncorr_df, corr_items, corr_links = get_correlated_with_cutoff(corr, .7)\n\nprint('Highly Correlated Features:', end='\\n\\n')\nprint(corr_df)","49701f55":"to_drop_cols = [\n    'NoStock',\n    'PerformanceRating', \n    'YearsInCurrentRole', \n    'YearsWithCurrManager', \n]\n\nX_train_preprocessed.drop(to_drop_cols, axis=1, inplace=True)\nX_test_preprocessed.drop(to_drop_cols, axis=1, inplace=True)","c7ff5765":"def print_results(model, X_test, y_test):\n    \"\"\"\n    print summrized results\n    \"\"\"\n    y_pred = model.predict(X_test)\n    y_pred_prob = model.predict_proba(X_test)[:, 1]\n    \n    auc = metrics.roc_auc_score(y_test, y_pred_prob)\n    acc = metrics.accuracy_score(y_test, y_pred)\n    f1 = metrics.f1_score(y_test, y_pred)\n    conf_mat = metrics.confusion_matrix(y_test, y_pred)\n    conf_mat_df = pd.DataFrame(conf_mat, \n                               index= ['true_no', 'true_yes'],\n                              columns = ['predict_no', 'predict_yes'])\n    \n    model_name = str(model).split('(')[0]\n    print(f'{\"*\" *10} {model_name} {\"*\" *10}')\n    print(f'Accuracy: {acc:.3f}')\n    print(f'AUC: {auc:.3f}')\n    print(f'F1: {f1:.3f}\\n', end='\\n')\n    print('Confusion Matrix:', end='\\n')\n    print(conf_mat_df, end='\\n\\n')\n    print('Classification Report:', end='\\n')\n    print(metrics.classification_report(y_test, y_pred))\n\n\ndef params_tuning(X_train, y_train, X_test, y_test,\n                  model=None, param_grid=None,\n                 scoring=None, balance_weight=False):\n    \"\"\"\n    Use the best params resulted from GridSearchCV to fit models\n    \"\"\"\n    if balance_weight:\n        sample_weight = y_train.map(dict(0.5 \/ y_train.value_counts())).values\n    else:\n        sample_weight = None\n    \n    # Grid Search for the best parameters\n    clf = GridSearchCV(model, param_grid, cv=5, scoring=scoring)\n    clf.fit(X_train, y_train, sample_weight=sample_weight)\n    # train model with the selected best parameters\n    model.set_params(**clf.best_params_)\n    model.fit(X_train, y_train)\n    # pring results\n    print_results(model, X_test, y_test)\n    return model","4cfbdbcc":"lr_params = {\n    'max_iter': 1000,\n    'class_weight': 'balanced'\n}\n# lr_grid_params = {\n#     'C': np.linspace(.1, 1, 10),\n#     'penalty': ['l1', 'l2'],\n#     'class_weight': ['balanced']\n# }\n\n# lr = params_tuning(\n#     X_train_preprocessed, y_train,\n#     X_test_preprocessed, y_test,\n#     model=LogisticRegression(**lr_params),\n#     param_grid=lr_grid_params,\n#     scoring='f1',\n# )\n\nlr_tuned_params = {'penalty': 'l2', 'C': .6}\nlr = LogisticRegression(**lr_params, **lr_tuned_params)\nlr.fit(X_train_preprocessed, y_train)\nprint_results(lr, X_test_preprocessed, y_test)","853d11a3":"oversampler=SMOTE(random_state=0)\nX_train_smote, y_train_smote = oversampler.fit_sample(\n    X_train_preprocessed, y_train)\n\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 1000,\n    'min_samples_leaf': 2,\n    'verbose': 0,\n    'oob_score': True,\n}\n\n#rf_grid_params = {\n#     'max_depth': [2, 5, 8, 12],\n#     'max_features': [.3, .5, .8]\n#                  }\n\n# rf = params_tuning(\n#     X_train_smote, y_train_smote,\n#     X_test_preprocessed, y_test,\n#     model=RandomForestClassifier(**rf_params),\n#     param_grid=rf_grid_params,\n#     scoring='f1',\n# )\n\nrf_tuned_params = {'max_depth': 12, 'max_features': .3}\nrf = RandomForestClassifier(**rf_params, **rf_tuned_params)\nrf.fit(X_train_smote, y_train_smote)\nprint_results(rf, X_test_preprocessed, y_test)","90d3cdc7":"gb_params = {\n    'n_estimators': 1000,\n    'min_samples_leaf': 2,\n}\n# gb_grid_params = {'learning_rate': np.linspace(.1, 1, 10),\n#                  'max_depth': [2, 3, 5, 8],\n#                  'max_features': [.3, .5, .8]}\n\n# gb = params_tuning(\n#     X_train_smote, y_train_smote,\n#     X_test_preprocessed, y_test,\n#     model=GradientBoostingClassifier(**gb_params),\n#     param_grid=gb_grid_params,\n#     scoring='f1',\n# )\n\ngb_tuned_params = {'max_depth': 8, 'max_features': .3, 'learning_rate': 0.4}\ngb = GradientBoostingClassifier(**gb_params, **gb_tuned_params)\ngb.fit(X_train_smote, y_train_smote)\nprint_results(gb, X_test_preprocessed, y_test)","746cc44d":"from sklearn.svm import SVC\n\nsvc_params = {\n    'cache_size': 200, \n    'kernel': 'linear',\n    'gamma': 'auto',\n    'class_weight': 'balanced',\n    'probability': True\n}\n# svc_grid_params = {'C': [.05, .1, .2, .5, 1, 2, 5, 10, 20]}\n\n# svc = params_tuning(\n#     X_train_preprocessed, y_train,\n#     X_test_preprocessed, y_test,\n#     model=SVC(**svc_params),\n#     param_grid=svc_grid_params,\n#     scoring='f1',\n# )\n\nsvc_tuned_params = {'C': 10}\nsvc = SVC(**svc_params, **svc_tuned_params)\nsvc.fit(X_train_preprocessed, y_train)\nprint_results(svc, X_test_preprocessed, y_test)","62b885f3":"class VotingClassifier:\n    def __init__(self, models):\n        self.models = models\n        \n    def __repr__(self):\n        model_info = ', '.join(list(models.keys()))\n        return f'EnsembleVoting({model_info})'\n    \n    def predict(self, X_test):\n        predictions = [model.predict(X_test) for model in self.models.values()]\n        return np.array(predictions).mean(0) > .5\n        \n    def predict_proba(self, X_test):\n        prob= [model.predict_proba(X_test) for model in models.values()]\n        return np.stack(prob).mean(0)\n        \n        \nmodels = {\n    'Logistic Regression': lr, \n    'Random Forest': rf,\n    'SVC': svc,\n    }\nvt = VotingClassifier(models)\nprint_results(vt, X_test_preprocessed, y_test)","4bbb0c78":"def plot_curves(y_test, y_pred_prob, ax1, ax2):\n    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_prob)\n    pr, rc, _ = metrics.precision_recall_curve(y_test, y_pred_prob)\n    ax1.plot(fpr, tpr)\n    ax2.plot(rc, pr)\n    \n\ndef get_model_results(y_test, y_pred, y_pred_prob):\n    return pd.Series({\n        'Accuracy': metrics.accuracy_score(y_test, y_pred),\n        'ROC AUC': metrics.roc_auc_score(y_test, y_pred_prob),\n        'F1': metrics.f1_score(y_test, y_pred),\n    })\n    \n\ndef plot_comparison(models, X_test, y_test):\n    \"\"\"\n    Plot ROC and Precision\/Recall Curves\n    \"\"\"\n    fig,(ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n    df_results = pd.DataFrame()\n    for name, model in models.items():\n        y_pred_prob = model.predict_proba(X_test)[:, 1]\n        y_pred = model.predict(X_test)\n        plot_curves(y_test, y_pred_prob, ax1, ax2)\n        df_results[name] = get_model_results(y_test, y_pred, y_pred_prob)\n\n    ax1.set_xlabel('Specificity', fontsize=14)\n    ax1.set_ylabel('Sensitivity', fontsize=14)\n    ax1.set_title('ROC', fontsize=18)\n    ax1.legend(handles=ax1.lines, labels=list(models.keys()), fontsize=12)\n    ax1.plot([0, 1], [0, 1], 'k--')\n\n    ax2.set_xlabel('Recall', fontsize=14)\n    ax2.set_ylabel('Precision', fontsize=14)\n    ax2.set_title('Precision\/Recall', fontsize=18)\n    \n    print(\"Benchmark Results:\")\n    print(df_results.T, end='\\n\\n')\n    #ax[1].legend(handles=ax[1].lines, labels=list(models.keys()), fontsize=12)\n    \n    # bechmark results\n#     df_results = df_results.unstack().reset_index()\n#     df_results.columns = ['models', 'metrics', 'values']\n#     sns.factorplot(x='metrics', y='values', hue='models',\n#                     data=df_results, kind='bar', ax=ax3,\n#                   legend_out=True)\n#     ax3.set_ylim(0,1)\n#     ax3.set_title('Benchmak Model Performance', fontsize=18)\n#     ax3.set_ylabel('')\n#     ax3.set_xlabel('')\n#     ax3.set_xticklabels(ax3.get_xticklabels(), fontsize=14)\n#     ax3.legend(bbox_to_anchor=(.7, 1), loc=2, borderaxespad=0., fontsize=12)\n#     plt.close(2)\n#     plt.subplots_adjust(hspace=.4)\n\n    \ncompare_models = models.copy()\ncompare_models['Ensemble'] = vt\nplot_comparison(compare_models, X_test_preprocessed, y_test)\n","638f7055":"def plot_feature_importance(features, importance, \n                            topN=20, ax=None):\n\n    df = pd.DataFrame({'features': features, \n                  'importance': importance})\n    df['abs_importance'] = abs(df['importance'])\n    df.sort_values('abs_importance', inplace=True, ascending=False)\n    \n    sns.barplot(y=\"features\", x=\"importance\", data=df[:20], \n                color=\"b\", alpha=.4, ax=ax)\n    ax.set_yticklabels(ax.get_yticklabels(), fontsize=12)\n    ax.set_ylabel(\"\")\n    ax.set_xlabel(\"\")\n    \n\nmodel_feature_info ={\n    'Logistic Regression': ('Coef', lr.coef_[0] ),\n    'SVC': ('Coef', svc.coef_[0] ),\n    'Random Forest': ('Feature Importance', rf.feature_importances_)\n}\n\ndef compare_feature_importance(model_feature_info):\n    col_n, ph, pw = 2, 6, 6\n    model_n = len(model_feature_info)\n    row_n = int(np.ceil( model_n \/ col_n))\n    \n    f, ax = plt.subplots(row_n, col_n, \n                         figsize=(col_n * ph, row_n * pw))\n    ax = ax.ravel()\n    \n    features = list(X_test_preprocessed.columns)\n    for i, (model_name, model_info) in enumerate(model_feature_info.items()):\n        plot_feature_importance(\n            features, model_info[1], ax=ax[i])\n        ax[i].set_title(model_name, fontsize=14)\n        ax[i].set_xlabel(model_info[0], fontsize=12)\n        \n    plt.subplots_adjust(wspace=0.8)\n    \n    for ax in ax[model_n:]:\n        ax.axis('off')\n\nmodel_feature_info ={\n    'Logistic Regression': ('Feature Coefficient', lr.coef_[0] ),\n    'SVC': ('Feature Weight', svc.coef_[0] ),\n    'Random Forest': ('Feature Importance', rf.feature_importances_)\n}\ncompare_feature_importance(model_feature_info)","39d604ab":"## <a class=\"anchor\" id=\"section_one\">1. Load libraries and dataset  <\/a>\n#### 1.1 Load libraries","29279db0":"##  <a class=\"anchor\" id=\"section-two\">2. Exploratory Data Analysis<\/a>\n#### 2.1 Feature distribution","74bbfddc":"#### 4.2.1 Logistic Regression Classifier\n\nWe first trained a Logistic Regression model with L2 regularization to predict employee attritions. The performance was not bad for this simple model.","41db0a54":"**Takeaways**:\n\nWe could draw some insights on how experience, job roles, and working environment affect attrition from the above EDA\n\n1. Employees who have left the job tent to be younger, and have a lower number of working years. \n\n2. Sales representatives and Lab technicians had higher attrition rates than other roles. \n\n3. Working overtime, traveling frequently were also related to employee attrition.\n","a1b254ec":"#### 4.2 Train and test models","79e5ccf0":"#### 1.4 Separate different data types\n\n* Separate features into three groups according to their data type (Measurement, Ordinal or Nominal).","cdb5080d":"#### 4.2.3 Gradient Boosting Classifier(with oversampling)\n\nThough Gradient Boosting Classifier showed high accuracy, it didn't perform well on AUC and F1 scoring, which are more important metrics for imbalanced datasets.","50d02378":"#### 2.2 Distribution Against Employee Attrition\n\n* Compared the distribution of each feature between the employees who have left their jobs and who stayed in the company.\n* Features were sorted by the effect sizes of the comparison against Attrition, separately for different types of data.","4c8c61e6":"**Takeaways**:\n\nMany features showed skewed distributions. In section 3, we could recode those features into more symmetrical distributions.","67545fc6":"#### 1.3 Drop meaningless variables \n* Drop variables with only one value (EmployeeCount, StandardHours, Over18)\n* Drop EmployeeNumber (ID)\n","0bdd026b":"#### 3.4 Normalize features","a2851f0e":"## <a class=\"anchor\" id=\"section-four\">4. Build Models<\/a>\n#### 4.1 Choose Metrics\n\nWe aimed to build machine learning models that predict whether employees would leave their positions or not. One challenge of this dataset is the inherent imbalance of the attrition data: only a small proportion (16%) of employees have left their jobs. We choose the F1 score as our metric since it combines both recall and precision. Also, because the goal is to identify high-risk employees, recall rate is more important than the precision rate in this project.","c4d9c0c6":"## <a class=\"anchor\" id=\"section-three\">3. Data Engineering<\/a>\n#### 3.1 Recode target variable","1250b166":"#### 3.3 Making dummy variables ","f0e96c96":"#### 4.2.2 Random Forest Classifier (with oversampling)\nClass imbalance affects Ensemble classifiers a lot. Therefore, we employed the Synthetic Minority Oversampling Technique (SMOTE) before model training. On the testing set, random forest classifier showed the highest accuracy among all the tested models. However, this classifier had a low recall rate.","4e081b40":"#### 1.2 Load Data","6fb33a7b":"The benchmark analysis revealed better performance (F1, AUC score) in SVC than in Logistic Regression and Random Forest model. Though Random Forest classifier showed highest classification accuracy, it tent to classify employees as without attritions, causing low recall rate. However, since the goal of this project is to identify the employees with high attrition risk (minority class), it is more important to consider recall than precision. \n\nNext, we combined the three models with majority voting. This classifier further enhanced the performance on F1 score. ","85f87024":"#### 4.4 Feature Importance","0d220782":"#### 1.5 Rename feature levels\n\n* Rename long descriptions with shorter abbreviations  ","1332a226":"#### 3.5 Remove multicollinearity","f5ebe91d":" In some scenarios, a complicated black box model with the best performance is all we need. This project, however, was not the case. The **interpretability** of the model is equally critical since HR would rely on these insights to devise intervention strategies. \n\nThough the exact feature importance levels differ among the three examined models, their patterns share a lot in common.  \n\n1. Several working environment factors were associated with higher attrition risks: such as working overtime, living far away from the company, lack of satisfaction, and traveling a lot. \n2. Some job roles, such as sales and HR, had higher attrition risks; in contrast, research scientists and directors showed lower risks. Also, employees in junior level positions were more likely to leave their jobs. \n3. Some personal attributes also contributed to the attrition risks, such as single, young age, and a history of short stays at each worked company.\n\nThough formulating policies to increase retention is beyond the scope of this project, we could draw some actionable insights. Such as:\n\n1. Give recognition or compensations to the employees who have been working overtime, or spending a lot of time in commute or traveling\n2. Allocate more resources to the departments with low retention rate for team and culture building\n3. Provide more guidance (orientation, mentorship) to junior employees","d9419eab":"#### 1.6 Split dataset \n* Split the dataset into a training set and a testing set\n* Keep the proportion of values around the same level between the training and testing set","11359121":"#### 4.2.4 Support Vector Classifier\n\nWe then trained and tested a Support Vector Classifier with a linear kernel and balanced class weights; it showed the highest AUC score and F1 score among the examined machine learning models. Though, in comparison with the Logistic regression model, the feature weights of SVC were harder to interpret.","00ff6d35":"#### 3.2 Create new features","c3a2993d":"#### 4.2.5 Ensemble Classifier\n\nWe combined the predictions of the models with decent performance (Logistic Regression, Random Forest, and SVC) and took the majority vote. This Ensemble Classifier showed even higher F1 score than the best classifier (SVC) in the ensemble.","0b1f0845":"# IBM Attrition Prediction with a Ensemble Classifier","e9d68152":"## Summary\n\nTo increase retention rates, HR might first identify the employees who might leave the company. This notebook built an ensemble model with the [IBM attrition data][1]* to predict high-risk employees and generated actionable insights. \n\nWhen I was working on this project, I learned a lot from [Anisotropic][2], [Alja\u017e][3], and [Vincent Lugat][4]'s work. \n\nHere are the steps I've taken:\n\n1. [**Load libraries and data**](#section-one)\n2. [**Exploratory data analysis**](#section-two)\n    * Examine Feature distribution and the distribution against employee attrition\n3. [**Data engineering**](#section-three)\n    * Create new features and remove collinear ones\n4. [**Build Modesl**](#section-four)\n    * Use F1 score as the metric to benchmark models\n    * Interpret Feature Importance\n    \nFurture steps:\n* Examine feature interactions and their impact on predictions\n\n*Please note that this is a fictional dataset\n\n[1]:https:\/\/www.kaggle.com\/pavansubhasht\/ibm-hr-analytics-attrition-dataset\n[2]:https:\/\/www.kaggle.com\/arthurtok\/employee-attrition-via-ensemble-tree-based-methods\n[3]:https:\/\/www.kaggle.com\/aljaz91\/ibm-s-attrition-tackling-class-imbalance-with-gbm\n[4]:https:\/\/www.kaggle.com\/vincentlugat\/ibm-attrition-analysis-and-prediction","dba0b402":"#### 4.3 Bechmark Classifiers"}}