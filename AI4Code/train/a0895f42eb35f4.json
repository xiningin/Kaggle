{"cell_type":{"ea8f73c9":"code","d28c0703":"code","2678ad15":"code","f71cd413":"code","d11f1689":"code","4d7c9e9b":"code","152a8639":"code","1780ca2f":"code","eac3f734":"code","a7b3e708":"code","8a628e16":"code","0080f748":"code","f3b10884":"code","d3531da9":"code","9e9e3464":"code","20525b95":"code","8fa1097a":"code","03474295":"code","f0bee958":"code","f580d448":"code","4cb5e1fd":"code","c59dfb8b":"code","8930d56d":"code","78d9d64f":"code","1748f367":"code","a9d1549c":"code","93f7069b":"code","53105bef":"code","2efbcd76":"code","cc72c803":"code","87d108f1":"code","90242219":"code","c9276f41":"code","08812edc":"code","f87d7e45":"code","e5147fb7":"code","af145b75":"code","15592412":"code","ec55ac47":"code","1501068e":"markdown","5a6519c3":"markdown","d60669c7":"markdown","6ab28277":"markdown","62a8879b":"markdown","3eb0bfb9":"markdown","40a7f5de":"markdown","839e02ac":"markdown","de55027b":"markdown","0bd52b57":"markdown","ce2b5440":"markdown","c2408c61":"markdown","9a4316b2":"markdown","b43ee9de":"markdown","4b96ef8c":"markdown","ff0abb55":"markdown","fc130fc3":"markdown","f8706526":"markdown","994fd343":"markdown","72f482bc":"markdown","45f3f2c7":"markdown"},"source":{"ea8f73c9":"from sklearn.cluster import KMeans \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize","d28c0703":"from collections import Counter\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom nltk.corpus import stopwords\nimport matplotlib\nfrom nltk.tokenize import word_tokenize\nfrom nltk import edit_distance\nimport nltk\nimport matplotlib.pyplot as plt\n%matplotlib inline","2678ad15":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","f71cd413":"import pandas as pd \ndf = pd.read_csv('\/kaggle\/input\/polymerasesubset\/POLYMERASE_ABSTRACT_TFIDF_10clustersResultsWithOriginalABS.csv')\ndf.head()","d11f1689":"df.shape","4d7c9e9b":"df.isnull().sum()","152a8639":"outString = ' '.join(df[\"ABS\"])\nprint (outString)","1780ca2f":"def stems(words, method) :\n    prtr = nltk.stem.PorterStemmer()\n    snob = nltk.stem.SnowballStemmer('english')\n    lema = nltk.wordnet.WordNetLemmatizer()\n    \n    word_to_stem = stopwords_removal(words)\n\n    stem = [w for w in word_to_stem]\n    stem = []\n    \n    if method == 'porter' :\n        for w in word_to_stem:\n            stem.append(prtr.stem(w))\n \n    elif method == 'snowball': \n        for w in word_to_stem:\n            stem.append(snob.stem(w))\n\n    return (stem)","eac3f734":"def stopwords_removal(words) :\n    stop_word = set(stopwords.words('english'))\n    word_token = word_tokenize(words)\n    output_sentence = [words for word in word_token if not word in stop_word]\n    output_sentence = []\n    for w in word_token:\n        if w not in stop_word:\n            output_sentence.append(w)\n    return(output_sentence)\n","a7b3e708":"stopwords_output = stopwords_removal(outString)\nfor w in stopwords_output:\n    print(w+\"|\",end=' ')","8a628e16":"snowball_stems = stems(outString, \"snowball\")\nprint(\"After stemming, there are\",len(snowball_stems),\"words. And they are as following:\")\nprint()\nfor s in snowball_stems:\n    print(s+\"|\",end=' ')\n    \n #After stemming, there are 290097 words. And they are as following:","0080f748":"import string\nx=snowball_stems\nx = [''.join(c for c in s if c not in string.punctuation) for s in x]\nx","f3b10884":"x = [s for s in x if s]\nx","d3531da9":"from collections import Counter\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport matplotlib\n\nmatplotlib.rcParams['font.sans-serif'] = \"Arial\"\nmatplotlib.rcParams['figure.dpi'] = 300\nbarWidth = 0.25\nplt.figure(figsize=(20,15))\n\ncounts = Counter(x)\ncommon = counts.most_common(50)\nlabels = [item[0] for item in common]\nnumber = [item[1] for item in common]\nnbars = len(common)\n\nplt.bar(np.arange(nbars), number,width=barWidth, tick_label=labels)\nplt.xticks(rotation = 90, fontweight='bold',fontsize=12,)\nplt.show()","9e9e3464":"barWidth = 0.25\nplt.figure(figsize=(20,40))\n\ncounts = Counter(x)\ncommon = counts.most_common(200)\nlabels = [item[0] for item in common]\nnumber = [item[1] for item in common]\nnbars = len(common)\n\nplt.barh(np.arange(nbars), number,tick_label=labels) #width=barWidth, \nplt.xticks( fontweight='bold',fontsize=12,) #rotation = 90,\nplt.title('Top 200 words in titles of ' +str(df.ABS.shape[0])+  ' research papers mentioned Polymerase',fontsize=15)#,fontweight='bold'   #df2.title.shape\nplt.show()","20525b95":"customize_stop_words2 = [\n    'used', 'using', 'SARS CoV','MERS CoV','Abstract','found','result','method','conclusion','results','case','cases',\n    'compared','many','well','including','identified','Although','present','Middle East','infection','patient'\n    'infectious','treatment','China','East','Role','COVID','human','model','Chapter','viruses','methods','disease'\n]\n#capital letter must match\n\nSTOPWORDS2 = list(STOPWORDS)  + customize_stop_words2","8fa1097a":"text = outString\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(stopwords = STOPWORDS2,  background_color=\"white\").generate(text)\n\n#matplotlib.rcParams['font.sans-serif'] = \"Arial\"\nmatplotlib.rcParams['figure.dpi'] = 300\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Top words cloud in abstracts of ' +str(df.ABS.shape[0])+  ' research papers mentioned Polymerase',fontsize=10)#,fontweight='bold'\nplt.show()","03474295":"tf_idf_vectorizor = TfidfVectorizer(stop_words = 'english',max_features = 2**12)\ntf_idf = tf_idf_vectorizor.fit_transform(df.ABS)\ntf_idf_norm = normalize(tf_idf)\ntf_idf_array = tf_idf_norm.toarray()\npd.DataFrame(tf_idf_array, columns=tf_idf_vectorizor.get_feature_names()).head()","f0bee958":"sklearn_pca = PCA(n_components = 30)\nX = sklearn_pca.fit_transform(tf_idf_array)\nkmeans = KMeans(n_clusters=5, max_iter=600, algorithm = 'auto')\nfitted = kmeans.fit(X)\nprediction = kmeans.predict(X)\n\nplt.scatter(X[:, 0], X[:, 1], c=prediction, s=50, cmap='viridis')\ncenters = fitted.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.6);","f580d448":"sklearn_pca = PCA(n_components = 30)\nX = sklearn_pca.fit_transform(tf_idf_array)\n\ncls = KMeans(n_clusters=6, init='k-means++',random_state=1) # \ncls.fit(X)\nnewfeature = cls.labels_ # the labels from kmeans clustering\n\nX2 = np.column_stack((X,pd.get_dummies(newfeature)))\n\nplt.figure()\n#plt.subplot(1,2,1)\nX2=X2\nplt.scatter(X2[:, 0], X2[:, 1]+np.random.random(X2[:, 1].shape)\/2, c=newfeature, cmap=plt.cm.rainbow, s=20, linewidths=0,alpha=0.5)\nplt.xlabel(''), plt.ylabel('')\nplt.grid()","4cb5e1fd":"#3D plot\n\nfrom matplotlib import pyplot\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nfig = pyplot.figure()\nax = Axes3D(fig)\n\n\nax.scatter(X2[:, 0], X2[:, 1]+np.random.random(X2[:, 1].shape)\/2,X2[:, 2]+np.random.random(X2[:, 1].shape)\/2, c=newfeature, cmap=plt.cm.rainbow,alpha=0.25)\npyplot.show()","c59dfb8b":"sse = []\nlist_k = list(range(1, 20))\n\nfor k in list_k:\n    km = KMeans(n_clusters=k)\n    km.fit(X)\n    sse.append(km.inertia_)\n\n# Plot sse against k\nplt.figure(figsize=(6, 6))\nplt.plot(list_k, sse, '-o')\nplt.xlabel(r'Number of clusters *k*')\nplt.ylabel('Sum of squared distance');","8930d56d":"\ndata=X\n\n# Silhouette vs Cluster Size\n# do it for the k-means\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\n\nseuclid = []\nscosine = []\nk = range(2,11)\nfor i in k:\n    kmeans_model = KMeans(n_clusters=i, init=\"k-means++\").fit(X)\n    labels = kmeans_model.labels_\n    seuclid.append(metrics.silhouette_score(data, labels, metric='euclidean'))\n    scosine.append(metrics.silhouette_score(data, labels, metric='cosine'))\n    \nplt.figure(figsize=(10,5))\nplt.plot(k,seuclid,label='euclidean')\nplt.plot(k,scosine,label='cosine')\nplt.ylabel(\"Silhouette\")\nplt.xlabel(\"Cluster\")\nplt.title(\"Silhouette vs Cluster Size\")\nplt.legend()\nplt.show()","78d9d64f":"\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nprint(__doc__)\n\n# y_lower = 10?\nX=X2\ny=newfeature\nrange_n_clusters = [ 3, 4, 5, 6,7,8,9,10,11] # [3, 4, 5, 6,7,8,9,10,11,12,13,14,15,16,17,18,19,20] \n\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) \/ n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\nplt.show()\n","1748f367":"data1 = np.array(df.ABS.drop_duplicates(keep='last'))\ndata1","a9d1549c":"data1.shape\n","93f7069b":"data1list=data1.tolist()","53105bef":"%%capture\n# Install the latest Tensorflow version.\n#!pip3 install --upgrade tensorflow-gpu\n# Install TF-Hub.\n!pip3 install tensorflow-hub\n#!pip3 install seaborn","2efbcd76":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nembed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/3\")","cc72c803":"embeddings = embed(data1list)[\"outputs\"]\n\n#print(embeddings)","87d108f1":"embeddings.shape","90242219":"NParray1941papers512vector=np.array(embeddings)\nNParray1941papers512vector","c9276f41":"testAbstract1=[\"background enterovirus 71 ev71 is one of the major causative agents of hand foot and mouth disease hfmd which is sometimes associated with severe central nervous system disease in children there is currently no specific medication for ev71 infection quercetin one of the most widely distributed flavonoids in plants has been demonstrated to inhibit various viral infections however investigation of the antiev71 mechanism has not been reported to date methods the antiev71 activity of quercetin was evaluated by phenotype screening determining the cytopathic effect cpe and ev71induced cells apoptosis the effects on ev71 replication were evaluated further by determining virus yield viral rna synthesis and protein expression respectively the mechanism of action against ev71 was determined from the effective stage and timeofaddition assays the possible inhibitory functions of quercetin via viral 2apro 3cpro or 3dpol were tested the interaction between ev71 3cpro and quercetin was predicted and calculated by molecular docking results quercetin inhibited ev71mediated cytopathogenic effects reduced ev71 progeny yields and prevented ev71induced apoptosis with low cytotoxicity investigation of the underlying mechanism of action revealed that quercetin exhibited a preventive effect against ev71 infection and inhibited viral adsorption moreover quercetin mediated its powerful therapeutic effects primarily by blocking the early postattachment stage of viral infection further experiments demonstrated that quercetin potently inhibited the activity of the ev71 protease 3cpro blocking viral replication but not the activity of the protease 2apro or the rna polymerase 3dpol modeling of the molecular binding of the 3cproquercetin complex revealed that quercetin was predicted to insert into the substratebinding pocket of ev71 3cpro blocking substrate recognition and thereby inhibiting ev71 3cpro activity conclusions quercetin can effectively prevent ev71induced cell injury with low toxicity to host cells quercetin may act in more than one way to deter viral infection exhibiting some preventive and a powerful therapeutic effect against ev71 further quercetin potently inhibits ev71 3cpro activity thereby blocking ev71 replication\"]","08812edc":"testAbstract2=[\"background to investigate the effects and immunological mechanisms of the traditional chinese medicine xinjiaxiangruyin on controlling influenza virus fm1 strain infection in mice housed in a hygrothermal environment methods mice were housed in normal and hygrothermal environments and intranasally infected with influenza virus fm1 a highperformance liquid chromatography fingerprint of xinjiaxiangruyin was used to provide an analytical method for quality control realtime quantitative polymerase chain reaction rtqpcr was used to measure messenger rna expression of tolllike receptor 7 tlr7 myeloid differentiation primary response 88 myd88 and nuclear factorkappa b nfb p65 in the tlr7 signaling pathway and virus replication in the lungs western blotting was used to measure the expression levels of tlr7 myd88 and nfb p65 proteins flow cytometry was used to detect the proportion of th17tregulatory cells results xinjiaxiangruyin effectively alleviated lung inflammation in c57bl6 mice in hot and humid environments guizhimahuanggebantang significantly reduced lung inflammation in c57bl6 mice the expression of tlr7 myd88 and nfb p65 mrna in lung tissue of wt mice in the normal environment gzmhgbt group was significantly lower than that in the model group p  005 in wt mice exposed to the hot and humid environment the expression levels of tlr7 myd88 and nfb p65 mrna in the xjxry group were significantly different from those in the virus group the expression levels of tlr7 myd88 and nfb p65 protein in lung tissue of wt mice exposed to the normal environment gzmhgbt group was significantly lower than those in the model group in wt mice exposed to hot and humid environments the expression levels of tlr7 myd88 and nfb p65 protein in xjxry group were significantly different from those in the virus group conclusion guizhimahuanggebantang demonstrated a satisfactory therapeutic effect on mice infected with the influenza a virus fm1 strain in a normal environment and xinjiaxiangruyin demonstrated a clear therapeutic effect in damp and hot environments and may play a protective role against influenza through downregulation of the tlr7 signal pathway\"]","f87d7e45":"Question1=['What are clinical effective therapeutics or drugs for COVID-19?']","e5147fb7":"embeddingsT1 = embed(testAbstract1)[\"outputs\"]\nembeddingsT2 = embed(testAbstract2)[\"outputs\"]\n\nembeddingsQ1 = embed(Question1)[\"outputs\"]\n\ntest1=np.array(embeddingsT1)\ntest2=np.array(embeddingsT2)\n\nquestion1=np.array(embeddingsQ1)\n\nimport textwrap","af145b75":"result1 = np.sum(NParray1941papers512vector*test1,axis=1)\/(np.sqrt(np.sum(NParray1941papers512vector*NParray1941papers512vector,axis=1))*np.sqrt(np.sum(test1*test1)))\nmaxRows1=result1.argsort()[-10:][::-1]  #https:\/\/stackoverflow.com\/questions\/6910641\/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\nprint(\"The indexes for most similar papers are:\") \nprint(maxRows1)\nprint(\"\\n\")\nprint(\"The cosine similarity for top 10 papers are:\") \nprint(result1[result1.argsort()[-10:][::-1]])\nprint(\"\\n\")\nprint(\"For Paper Abstract:\\n\")\nprint(textwrap.fill(testAbstract1[0],100))\nprint(\"\\nWe found the top 10 most similar papers as listed below:\\n\")\nprint(df.ABS.iloc[maxRows1])","15592412":"result2 = np.sum(NParray1941papers512vector*test2,axis=1)\/(np.sqrt(np.sum(NParray1941papers512vector*NParray1941papers512vector,axis=1))*np.sqrt(np.sum(test2*test2)))\nmaxRows2=result2.argsort()[-10:][::-1]  #https:\/\/stackoverflow.com\/questions\/6910641\/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\nprint(\"The indexes for most similar papers are:\") \nprint(maxRows2)\nprint(\"\\n\")\nprint(\"The cosine similarity for top 10 papers are:\") \nprint(result2[result2.argsort()[-10:][::-1]])\nprint(\"\\n\")\nprint(\"For Paper Abstract:\\n\")\nprint(textwrap.fill(testAbstract2[0],100))\nprint(\"\\nWe found the top 10 most similar papers as listed below:\\n\")\nprint(df.ABS.iloc[maxRows2])","ec55ac47":"resultq1 = np.sum(NParray1941papers512vector*question1,axis=1)\/(np.sqrt(np.sum(NParray1941papers512vector*NParray1941papers512vector,axis=1))*np.sqrt(np.sum(question1*question1)))\nmaxRowsq1=resultq1.argsort()[-20:][::-1]  #https:\/\/stackoverflow.com\/questions\/6910641\/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\nprint(\"The indexes for most related papers are:\") \nprint(maxRowsq1)\nprint(\"\\n\")\nprint(\"The cosine similarity for top 20 papers are:\") \nprint(resultq1[resultq1.argsort()[-20:][::-1]])\nprint(\"\\n\")\nprint(\"For Question:\\n\")\nprint(textwrap.fill(Question1[0],100))\nprint(\"\\nWe found the top 20 most related papers as listed below:\\n\")\nprint(df.ABS.iloc[maxRowsq1])","1501068e":"## Word Cloud","5a6519c3":"## Remove stopwords and apply stemmer on words","d60669c7":"### Notice the paper find iteself as the most similar papers with cosine similarity score of 1.","6ab28277":"#### Try 5 clusters","62a8879b":"## Identify similar papers based cosine similarity\n\nTo test if we can automatically identify similar papers (based on abstracts) from existing ones, I start with universal-sentence-encoder which convert paper abstracts into vectors of size 512. Then I search for the top 5 similar papers with cosine similarity. \nSee the example output below. Input dataset generated as method in notebook *Keyword window \u201cpolymerase + therapeut\" R* and *Part 2 TFIDF Clustering at Abstract level* (a subset of CORD19 data, 1941 \u201cpolymerase\u201d related paper abstracts) \n\n\nhttps:\/\/www.kaggle.com\/leijiang1\/part-3-keyword-window-polymerase-therapeut-r\n\nhttps:\/\/www.kaggle.com\/leijiang1\/part-2-tfidf-clustering-at-abstract-level\n\n\nFor test 2, after manually reading the input, I think the original abstract focus on influenza medicine. My top results are focus on influenza. The reason I chose abstract level analysis is because I think use full body text for topic modeling could have the risk of picking up too much noise. Another point is I chose sentence encoder becasue we'd better not to tokenize medical words (it\u2019s a domain specific property).\n\n#### This method can also be used for a Question Answering system although the performance of which is still subject to evaluation.\n\nI will manually read the output 20 papers for the question \"What are clinical effective therapeutics or drugs for COVID-19?\" and update.\n\n\n\n## Pros and Cons of my method\n### So the pros of my method is\nMy method used Transfer learning of universal-sentence-encoder which means I stand on the shoulder of giant. \ud83d\ude42 \n\n### cons of my method is \nneed to manually read the outputs to validate if the results are solid.\n\n\n### Universal Sentence Encoder family\n\nThere are a number of different Universal Sentence Encoders. I think universal-sentence-encoder-qa should be the best fit for the task of Question Answering.\nHowever, I could not get it to run (ResourceExhaustedError).\n\nembed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/3\") #this step takes long on local, fast on cloud. worked well\n\n\nembed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\") # took some time even on cloud about 1-2 min, got ResourceExhaustedError at next step\nThe universal-sentence-encoder-large model is trained with a Transformer encoder. try to run on cloud but got got ResourceExhaustedError# \n\n\nembed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/3?tf-hub-format=compressed\") #try compressed large, got TypeError: 'AutoTrackable' object is not callable\n\nembed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-qa\/3\") # ResourceExhaustedError  \n\n\n\n\n## Major Findings\nBy Keyword Window search with keywords identified by domain knowledge, I found the following \n### Potential drug candidates:\ntherapeutic peptides , antiviral nucleosides target rna polyermase; protease inhibitor; monoclonal antibody can be a target; macrocyclic peptides; gs5734; quercetin; pb2 subunit; xinjiaxiangruyin; iav usurps zbtb25; m2 inhibitors; nitazoxanide ntz; active small molecular inhibitors; NHC;\u2026 (Based on past researches on sars\/mers,flu, hcv,herp, ebov \u2013 other diseases caused by rna viruses)\n\n\n## Conclusion\nCORD19 project is about Text Mining, Knowledge Discovery & Question Answer.\nOur goal: efficiently extract information from 45,000 + papers to find cures for COVID-19.\n\n\nMy method of identify similar papers based cosine similarity is tested to be effective.\n\nQuestion Answering system based on the same method also seem to work, but need further validation. How to design a informative enough question is actually the tricky part.\n\nMore research can be done use the top words identified in text mining in combination with domain knowledge. I plan to come back after my final projects to dig deeper.\n\nAlso got pretty good document dlustering results. Worth trying to label the clusters. May find useful information.\n\n### Use methods in combination\n\nFurthermore, after finding the most similar papers\/abstracts, we can use the Keyword window method described in previous posts to retrieve more detailed information. So these methods can be used in combination to achieve better results.\n","3eb0bfb9":"#### 5 clusters looks good.","40a7f5de":"## Customize stop words","839e02ac":"## EDA - top words","de55027b":"#### The most informative words are usually not the biggest. By reading the papers, we know \u201crdrp\u201d is a critical target. \u201crna-dependent rna\u201d could mean \u201cRNA-dependent RNA polymerase or RNA replicase\u201d. In addition, the plot shows \"polymerase chain\" is a high frequency word. By domain knowledge we know it is becasue PCR. ","0bd52b57":"## Vectorize the abstracts with TF-IDF","ce2b5440":"3D Plot of 6 clusters","c2408c61":"#### Before modeling, I'd like to check what words talked about most in this subset of \"Polymerase\" papers. We can also identify more keywords for further analysis form the top word list and word cloud.","9a4316b2":"#### Plot of 5 clusters","b43ee9de":"## KMeans Clustering","4b96ef8c":"#### Elbow method below picked n_clusters=5 or 6. However, Silhouette score picked n_clusters=6. So we also plot the results of n_clusters=6.","ff0abb55":"## Import Data","fc130fc3":"## Import Libraries","f8706526":"## Vectorize the abstracts using Universal Sentence Encoder","994fd343":"## Examples of recommend similar papers","72f482bc":"### Plot of 6 clusters","45f3f2c7":"## Example of Question Answering system"}}