{"cell_type":{"38a56f91":"code","9b4e6fa3":"code","561b1c3a":"code","b94b77a4":"code","fdfaf79b":"code","c589b84a":"code","e9561c83":"code","7c7943af":"code","c0e8e117":"code","7d96df03":"code","ef7ff6e0":"code","565530a1":"code","8671d447":"code","660a1af8":"code","cea794f9":"code","a2533370":"code","d78256a7":"code","558b7fc9":"markdown","9de558bc":"markdown","6aa28486":"markdown","786463cb":"markdown","05da8ae0":"markdown","11c337e9":"markdown","8367c35e":"markdown","92b901c9":"markdown","35a28360":"markdown","2cc307d8":"markdown","a8f8e1c5":"markdown","f3467f4b":"markdown","d2e7978a":"markdown"},"source":{"38a56f91":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9b4e6fa3":"df = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict.csv')\ndf.head()","561b1c3a":"df=df.drop('Serial No.',axis=1)\ndf.head()","b94b77a4":"df.info()","fdfaf79b":"df.describe()","c589b84a":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfig, axs = plt.subplots(nrows=2, ncols=4, figsize=(20,10))\nindex = 0\naxs = axs.flatten()\nfor k,v in df.items():\n    sns.boxplot(y=k, data=df, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","e9561c83":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","7c7943af":"fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(20,10))\nindex = 0\naxs = axs.flatten()\nfor k in df.columns:\n    sns.distplot(df[k], ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","c0e8e117":"fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor i, k in enumerate(df.columns):\n    sns.scatterplot(y=df[\"Chance of Admit \"], x=df[k], ax=axs[i])\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","7d96df03":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nplt.figure(figsize=(20, 10))\nsns.heatmap(df.corr().abs(),  annot=True)","ef7ff6e0":"x = df.drop(\"Chance of Admit \", axis = 1)\ny = df['Chance of Admit ']","565530a1":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nscaler = MinMaxScaler()\nx_new = scaler.fit_transform(x)\nx_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size = 0.2, random_state = 42)","8671d447":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nmodel = LinearRegression()\nmodel.fit(x_train, y_train)\npredictions = model.predict(x_test)\nrms=np.sqrt(mean_squared_error(y_test, predictions))\nrms","660a1af8":"model.score(x_train,y_train)","cea794f9":"model.score(x_test,y_test)","a2533370":"coef = pd.DataFrame({'column':x.columns,'coefficient':model.coef_*100}).sort_values('coefficient').reset_index()\ncoef.drop('index',axis=1)","d78256a7":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfig = plt.figure(figsize = (15,5))\nplt.xticks(rotation=45)\ncolors = ['#adebad','#adebad','#adebad','#adebad','#99e699','#85e085','#70db70']\nsns.barplot(coef['column'],coef['coefficient'],palette = colors)\nplt.ylabel(\"Additional % Admission\")\nplt.xlabel('Factor')\nplt.show()","558b7fc9":"**We got an RMSE score around 0.06 and an accuracy above 82% on the test set.**","9de558bc":"From the above plot we can conclude there are very few or no outliers present in the data.","6aa28486":"# Data Visualization and Data Cleaning","786463cb":"If you like this kernel, please upvote. Thanks!","05da8ae0":"We can conclude, most importantly, the following:\n* An additional point on the GRE increases chances of graduate admission by about 9.31%.\n* An additional point on the TOEFL increases chances of graduate admission by about 7.62%\n* An additional point in college CGPA increases chances of graduate admission by about 35.77%.\n* Performing research increases chances of graduate admission by about 2.22%.","11c337e9":"From the above correlation matrix we can see that CGPA and GRE SCORE are highly correlated with Chance of Admit.","8367c35e":"Data Visualization","92b901c9":"From the above graph we can conclude that GRE Score, TOEFL Score and CGPA have a linear relationship with Chance of Admit, hence higher the scores higher will be the chances for getting admissions.","35a28360":"Dropping the column 'Serial' from the dataset","2cc307d8":"# Linear Regression Model implementation","a8f8e1c5":"There are no null values present.","f3467f4b":"# Conclusion","d2e7978a":"With the above plot we can conclude that the data is not skewed. GRE Score, TOEFL Score and CGPA have a normal distribution. University Ratings, SOP and LOR have ordinal values."}}