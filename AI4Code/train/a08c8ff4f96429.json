{"cell_type":{"1c621a7b":"code","3d0b2fa6":"code","e1c91e7e":"code","0d403e99":"code","fcec505e":"code","3524f70d":"code","5824a9ad":"code","63241e05":"code","9ae4a670":"code","1d2bb761":"code","0da2450f":"code","972fb18b":"code","b3e64a7e":"code","e0b8a36f":"code","69d0e86d":"code","b61b7b09":"code","e3734d19":"code","2026b6ac":"code","5ee69772":"code","5b341c68":"code","221c7147":"code","5b73fc4c":"markdown","7177635a":"markdown","b72d3add":"markdown","004ca9e7":"markdown","06900d7b":"markdown","dbd15110":"markdown","8e612a18":"markdown","cd7318e4":"markdown","43614dda":"markdown","92197225":"markdown","5187d767":"markdown","bfb06653":"markdown","e1370a16":"markdown","33d284bf":"markdown"},"source":{"1c621a7b":"%matplotlib inline\nimport os\nimport glob\nimport shutil\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport torchvision.utils\nimport numpy as np\nimport random\nimport torch\nimport PIL.ImageOps    \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader,Dataset\nfrom sklearn.model_selection import train_test_split","3d0b2fa6":"def imshow(img,text=None,should_save=False):\n    npimg = img.numpy()\n    plt.axis(\"off\")\n    if text:\n        plt.text(75, 8, text, style='italic',fontweight='bold',\n            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()    \n\ndef show_plot(iteration,loss):\n    plt.plot(iteration,loss)\n    plt.show()","e1c91e7e":"#here we define our classes\nclasses = ['Hourse Mackerel',\n 'Black Sea Sprat',\n 'Sea Bass',\n 'Red Mullet',\n 'Trout',\n 'Striped Red Mullet',\n 'Shrimp',\n 'Gilt-Head Bream',\n 'Red Sea Bream']","0d403e99":"#creating train\/test folder structure\ntry:\n    os.mkdir('train')\n    os.mkdir('test')\n\n    for i in classes:\n        os.mkdir('train'+ os.sep + i)\n        os.mkdir('test'+ os.sep + i)\nexcept:\n    pass","fcec505e":"%%bash\n#checking if our folder structure is ok\ncd test\nls","3524f70d":"#Let's use only the true image.\nimages_pth = [i for i in glob.glob('..\/input\/*\/*\/Fish_Dataset\/*\/*\/*.png') if 'GT' not in i]\nX_train, X_test = train_test_split(images_pth,random_state=42,test_size=0.2)","5824a9ad":"if len(glob.glob('train\/*\/*')) == 0:\n    for item in tqdm(X_train):\n        for class_d in classes:\n            if class_d in item:\n                shutil.copy(item,'train'+ os.sep + class_d + os.sep)","63241e05":"if len(glob.glob('test\/*\/*')) == 0:\n    for item in tqdm(X_test):\n        for class_d in classes:\n            if class_d in item:\n                shutil.copy(item,'test'+ os.sep + class_d + os.sep)","9ae4a670":"print('Train images',len(glob.glob('train\/*\/*')))\nprint('Test images',len(glob.glob('test\/*\/*')))","1d2bb761":"class Config():\n    training_dir = \".\/train\/\"\n    testing_dir = \".\/test\/\"\n    train_batch_size = 64\n    train_number_epochs = 30","0da2450f":"class SiameseNetworkDataset(Dataset):\n    \n    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n        self.imageFolderDataset = imageFolderDataset    \n        self.transform = transform\n        self.should_invert = should_invert\n        \n    def __getitem__(self,index):\n        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n        #we need to make sure approx 50% of images are in the same class\n        should_get_same_class = random.randint(0,1) \n        if should_get_same_class:\n            while True:\n                #keep looping till the same class image is found\n                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n                if img0_tuple[1]==img1_tuple[1]:\n                    break\n        else:\n            while True:\n                #keep looping till a different class image is found\n                \n                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n                if img0_tuple[1] !=img1_tuple[1]:\n                    break\n\n        img0 = Image.open(img0_tuple[0])\n        img1 = Image.open(img1_tuple[0])\n        img0 = img0.convert(\"L\")\n        img1 = img1.convert(\"L\")\n        \n        if self.should_invert:\n            img0 = PIL.ImageOps.invert(img0)\n            img1 = PIL.ImageOps.invert(img1)\n\n        if self.transform is not None:\n            img0 = self.transform(img0)\n            img1 = self.transform(img1)\n        \n        return img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n    \n    def __len__(self):\n        return len(self.imageFolderDataset.imgs)","972fb18b":"folder_dataset = dset.ImageFolder(root=Config.training_dir)","b3e64a7e":"siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,\n                                        transform=transforms.Compose([transforms.Resize((256,256)),\n                                                                      transforms.ToTensor()\n                                                                      ])\n                                       ,should_invert=False)","e0b8a36f":"vis_dataloader = DataLoader(siamese_dataset,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=8)\ndataiter = iter(vis_dataloader)\n\n\nexample_batch = next(dataiter)\nconcatenated = torch.cat((example_batch[0],example_batch[1]),0)\nimshow(torchvision.utils.make_grid(concatenated))\nprint(example_batch[2].numpy())","69d0e86d":"class SiameseNetwork(nn.Module):\n    def __init__(self):\n        super(SiameseNetwork, self).__init__()\n        self.cnn1 = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(1, 4, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(4),\n            \n            nn.ReflectionPad2d(1),\n            nn.Conv2d(4, 8, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(8),\n\n\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(8, 8, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(8),\n\n\n        )\n\n        self.fc1 = nn.Sequential(\n            nn.Linear(8*256*256, 500),\n            nn.ReLU(inplace=True),\n\n            nn.Linear(500, 500),\n            nn.ReLU(inplace=True),\n\n            nn.Linear(500, 5))\n\n    def forward_once(self, x):\n        output = self.cnn1(x)\n        output = output.view(output.size()[0], -1)\n        output = self.fc1(output)\n        return output\n\n    def forward(self, input1, input2):\n        output1 = self.forward_once(input1)\n        output2 = self.forward_once(input2)\n        return output1, output2","b61b7b09":"class ContrastiveLoss(torch.nn.Module):\n    \"\"\"\n    Contrastive loss function.\n    Based on: http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/hadsell-chopra-lecun-06.pdf\n    \"\"\"\n\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, label):\n        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n\n\n        return loss_contrastive","e3734d19":"train_dataloader = DataLoader(siamese_dataset,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=Config.train_batch_size)","2026b6ac":"net = SiameseNetwork().cuda()\ncriterion = ContrastiveLoss()\noptimizer = optim.Adam(net.parameters(),lr = 0.0005 )","5ee69772":"counter = []\nloss_history = [] \niteration_number= 0","5b341c68":"for epoch in range(0,Config.train_number_epochs):\n    for i, data in enumerate(train_dataloader,0):\n        img0, img1 , label = data\n        img0, img1 , label = img0.cuda(), img1.cuda() , label.cuda()\n        optimizer.zero_grad()\n        output1,output2 = net(img0,img1)\n        loss_contrastive = criterion(output1,output2,label)\n        loss_contrastive.backward()\n        optimizer.step()\n        if i %10 == 0 :\n            #print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.item()))\n            iteration_number +=10\n            counter.append(iteration_number)\n            loss_history.append(loss_contrastive.item())\nshow_plot(counter,loss_history)","221c7147":"folder_dataset_test = dset.ImageFolder(root=Config.testing_dir)\nsiamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,\n                                        transform=transforms.Compose([transforms.Resize((256,256)),\n                                                                      transforms.ToTensor()\n                                                                      ])\n                                       ,should_invert=False)\n\ntest_dataloader = DataLoader(siamese_dataset,num_workers=8,batch_size=1,shuffle=True)\ndataiter = iter(test_dataloader)\nx0,_,_ = next(dataiter)\n\nfor i in range(10):\n    _,x1,label2 = next(dataiter)\n    concatenated = torch.cat((x0,x1),0)\n    \n    output1,output2 = net(Variable(x0).cuda(),Variable(x1).cuda())\n    euclidean_distance = F.pairwise_distance(output1, output2)\n    imshow(torchvision.utils.make_grid(concatenated),'Dissimilarity: {:.2f}'.format(euclidean_distance.item()))\n\n","5b73fc4c":"## Neural Net Definition\nWe will use a standard convolutional neural network","7177635a":"# One Shot Learning with Siamese Networks\n\nThis is the jupyter notebook that accompanies\nBased on [One Shot Learning with Siamese Networks in PyTorch](https:\/\/hackernoon.com\/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e)","b72d3add":"## Some simple testing\nThe last 3 subjects were held out from the training, and will be used to test. The Distance between each image pair denotes the degree of similarity the model found between the two images. Less means it found more similar, while higher values indicate it found them to be dissimilar.","004ca9e7":"## Training Time!","06900d7b":"## Helper functions\nSet of helper functions","dbd15110":"## Contrastive Loss","8e612a18":"## Imports\nAll the imports are defined here","cd7318e4":"## Custom Dataset Class\nThis dataset generates a pair of images. 0 for geniune pair and 1 for imposter pair","43614dda":"## Moving files to the folder structure to be able to use ImageFolder","92197225":"## Visualising some of the data\nThe top row and the bottom row of any column is one pair. The 0s and 1s correspond to the column of the image.\n1 indiciates dissimilar, and 0 indicates similar.","5187d767":"## Creating our train\/valid partition\n","bfb06653":"# If you like please upvote!","e1370a16":"## Using Image Folder Dataset","33d284bf":"## Configuration Class\nA simple class to manage configuration"}}