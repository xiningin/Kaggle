{"cell_type":{"39b7e17b":"code","76d43607":"code","5dcebddb":"code","95f49a53":"code","318512b4":"code","6f19dc93":"code","addbfa99":"code","2aa39735":"code","22e26504":"code","9b864b7e":"code","6022ed65":"code","59e7c4f7":"code","44ad2f5c":"code","9fd3cefc":"code","73733170":"code","5dc82da6":"code","e19d628d":"code","2da7b5d5":"code","efeeb718":"code","0786ac7a":"code","f69c89c4":"code","8ef50d4a":"code","0e901ba2":"code","878862a8":"code","0efd1884":"code","d70e0896":"code","264c7b5f":"code","4779c331":"code","83175d5d":"code","8a17a062":"code","b5171952":"code","401042f1":"code","1f1e428e":"code","0d14eb95":"code","a087e2ff":"code","43266a4e":"code","58ea7109":"code","c57ccedb":"code","0df90c3e":"code","9396c775":"code","47a1d9c6":"code","58092d62":"code","03480cfb":"code","9da9ceb2":"code","7d692172":"code","ebfa9975":"code","d54db23c":"code","1de5e82b":"code","77a21858":"code","9fdfe76a":"code","f9789622":"code","623e410e":"code","a13a2a5a":"code","b8ec4646":"code","bb64cc6a":"code","d6589e72":"markdown","6ccc4c83":"markdown","9516d0f0":"markdown","40d68b8c":"markdown","4b8b24de":"markdown","39a89ff1":"markdown","95441bae":"markdown","19e32d52":"markdown","e4982160":"markdown","1bde3516":"markdown","f81d50ea":"markdown","4f20451d":"markdown","31b2eb2f":"markdown","467bf543":"markdown","b9e6ea92":"markdown","45c8b22a":"markdown","e5a94c56":"markdown","dbabd963":"markdown","01d4ed30":"markdown","e2926253":"markdown","db9c3ffb":"markdown","2aecd6eb":"markdown","96fcebf7":"markdown","f359843c":"markdown","8021789c":"markdown","c4c8a4a3":"markdown","fd4fcfca":"markdown","24a85cb4":"markdown","22df96c5":"markdown","b2379681":"markdown","686c3062":"markdown","d2c9b316":"markdown","3b09bf69":"markdown","343e98c4":"markdown","62153eb7":"markdown","0672a3f7":"markdown","d463e25b":"markdown","b6c1eabe":"markdown","208cb4fc":"markdown","c1159249":"markdown","a04def11":"markdown","ac8ce9dd":"markdown","55e2d91e":"markdown","17473397":"markdown","a5225b06":"markdown","411ac854":"markdown","6dce09cb":"markdown","9980ec90":"markdown","a1b9c829":"markdown","c33dc5c2":"markdown","14137289":"markdown","11afedb2":"markdown","2376c1e1":"markdown","3111513f":"markdown","c4dd044b":"markdown","c7bdf8d8":"markdown","10b2bc6e":"markdown","2cea40ff":"markdown","1f377d53":"markdown","d7b0cce3":"markdown","b974eb1e":"markdown","15d21620":"markdown","1555cbfa":"markdown","299b01bf":"markdown","3849afb0":"markdown","0c25890e":"markdown","798bb453":"markdown","31d1205e":"markdown","9ceb7ce2":"markdown","a1d98003":"markdown","24445340":"markdown","781369b7":"markdown","948cb36c":"markdown","21fb15ee":"markdown"},"source":{"39b7e17b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Some styling\nsns.set_style(\"darkgrid\")\nplt.style.use(\"fivethirtyeight\")\npd.pandas.set_option('display.max_columns', None)\n\n%matplotlib inline","76d43607":"# set dataset path\n\ntrain_data = os.path.join(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\", \"train.csv\")\ntest_data = os.path.join(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\", \"test.csv\")","5dcebddb":"# read datasets\n\ntrain = pd.read_csv(train_data)\ntest = pd.read_csv(test_data)","95f49a53":"# size of dataset\n# as 81 features so we can try different feature selection and feature engineering methods\n\ntrain.shape","318512b4":"# display info about the dataset\n\ntrain.columns","6f19dc93":"# sneak peek\n\ntrain.head()","addbfa99":"# let's take look at stats for SalePrice\n\ntrain[\"SalePrice\"].describe()","2aa39735":"_ = sns.distplot(train.SalePrice)","22e26504":"#skewness and kurtosis\n\nprint(f\"Skewness: {train['SalePrice'].skew()}\")\nprint(f\"Kurtosis: {train['SalePrice'].kurt()}\")","9b864b7e":"#scatter plot\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.scatter(train['GrLivArea'], train['SalePrice'], c='red', alpha=0.25)\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')\nplt.ylim(0,800000)\n\nplt.subplot(1, 2, 2)\nplt.scatter(train['TotalBsmtSF'], train['SalePrice'], c='k', alpha=0.25)\nplt.xlabel('TotalBsmtSF')\nplt.ylabel('SalePrice')\nplt.ylim(0,800000)\nplt.show()","6022ed65":"plt.figure(figsize=(8, 6))\nvar = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\n_ = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nplt.axis(ymin=0, ymax=800000)\nplt.show()","59e7c4f7":"plt.figure(figsize=(20, 8))\nvar = 'YearBuilt'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)\n_ = plt.xticks(rotation=90)","44ad2f5c":"plt.figure(figsize=(20, 6))\nvar = 'TotRmsAbvGrd'\nplt.subplot(1, 2, 1)\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\n_ = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nplt.axis(ymin=0, ymax=800000)\nplt.show()","9fd3cefc":"plt.figure(figsize=(18, 10))\ncorrmat = train.drop('Id', 1).corr()\n_ = sns.heatmap(corrmat, vmax=1.0, square=True, fmt='.2f', \n            cmap='coolwarm', annot_kws={'size': 8})\nplt.show()","73733170":"plt.figure(figsize=(6, 6))\n\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', cmap='coolwarm',\n                 annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","5dc82da6":"numerical_features = train.select_dtypes('number').columns.to_list()\nnumerical_features.pop(0)\nprint(f\"Number of numerical features: {len(numerical_features)}\")\n\n# although numerical there are features which reprsent years\n# there are some dicrete numerical features too.\n# we need to remove them from the list.\n\n\nyear_features = [feature for feature in numerical_features \n                 if 'Yr' in feature or 'Year' in feature]\nprint(f\"Number of Temporal features: {len(year_features)}\")\n\n\ndiscrete_features = [feature for feature in numerical_features \n                     if train[feature].nunique()<= 15 and feature not in year_features]\nprint(f\"Number of discrete numerical features: {len(discrete_features)}\")\n\ncontinuous_num_features = [feature for feature in numerical_features \n                     if feature not in discrete_features + year_features] \n                                               \nprint(f\"Number of continuous numerical features: {len(continuous_num_features)}\")","e19d628d":"plt.figure(figsize=(15, 10))\ncorrmat = train[continuous_num_features].corr()\nsns.heatmap(corrmat, vmax=1.0, square=True, fmt='.2f', \n            annot=True, cmap='coolwarm', annot_kws={'size': 8});","2da7b5d5":"# SalePrice scatter plot with highly correlated features","efeeb718":"plt.figure(figsize=(10, 8))\n\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n_ = sns.pairplot(train[cols], size = 2.5, diag_kind='kde')\nplt.show()","0786ac7a":"features_with_na = [features for features in train.columns if train[features].isnull().sum() >= 1]\n\na = pd.DataFrame({\n    'features': features_with_na,\n    'Total': [train[i].isnull().sum() for i in features_with_na],\n    'Missing_PCT': [np.round(train[i].isnull().sum()\/ train.shape[0], 4) for i in features_with_na]\n}).sort_values(by='Missing_PCT', ascending=False).reset_index(drop=True)\na.style.background_gradient(cmap='Reds') \n","f69c89c4":"train['FireplaceQu'].value_counts()","8ef50d4a":"train[train['FireplaceQu'].isnull()][['Fireplaces','FireplaceQu']]","0e901ba2":"#Unique elements\ntrain['MasVnrType'].unique()","878862a8":"train[train['MasVnrType'].isnull()][['MasVnrType','MasVnrArea']]","0efd1884":"# Let's look at the repeated value in MasVnrType column\n\ntrain['MasVnrType'].mode()","d70e0896":"train[train['BsmtQual'].isnull()][['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','BsmtFinSF1',\n                        'BsmtFinSF2','BsmtUnfSF','TotalBsmtSF']].head(15)","264c7b5f":"train[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n 'BsmtFinType2']].mode()","4779c331":"train[train['GarageType'].isnull()][['GarageType', 'GarageYrBlt', 'GarageFinish',\n       'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond']]","83175d5d":"train[['GarageType','GarageFinish',\n 'GarageQual','GarageCond']].mode()","8a17a062":"plt.figure(figsize=(20, 20))\nfor i, feature in enumerate(features_with_na, 1):\n    plt.subplot(5, 5, i)\n    data = train.copy()\n    \n    # let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    data[feature] = np.where(data[feature].isnull(), 1, 0)\n    \n    # calculate the median SalePrice where the information is missing or present\n    temp = data.groupby(feature)['SalePrice'].median()\n    _ = sns.barplot(x=temp.index, y=temp.values)\n    plt.title(feature)\n    plt.xlabel(\"\")\n    plt.xticks([0, 1], [\"Present\", \"Missing\"])\n    plt.ylabel(\"Sales Price\", rotation=90)\nplt.tight_layout(h_pad=2, w_pad=2)\nplt.show()","b5171952":"#standardizing data\nsaleprice_scaled = StandardScaler().fit_transform(train['SalePrice'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\n\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","401042f1":"data = train.copy()\n# sale_price = np.log(train['SalePrice'])\n\nfor i, feature in enumerate(continuous_num_features, 1):\n    data = train[feature].copy()\n    if 0 in data.unique(): # as log 0 is undefinedz\n        continue\n    else:\n        data = np.log(data)   \n        data.name = feature\n        _ = plt.figure(figsize=(6, 6))\n        _ = sns.boxplot(y=data)\n    \nplt.show()","1f1e428e":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","0d14eb95":"train.select_dtypes('number').columns","a087e2ff":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend([f'Normal dist. ($\\mu=$ {mu:.2f} and $\\sigma=$ {sigma:.2f} )'],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","43266a4e":"# GrLivArea\n# histogram and normal probability plot\n\nsns.distplot(train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['GrLivArea'], plot=plt)","58ea7109":"# TotalBsmtSF\n# histogram and normal probability plot\n\nsns.distplot(train['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['TotalBsmtSF'], plot=plt)","c57ccedb":"print(\"\\nSkew in numerical features: \\n\")\npd.DataFrame({\n    'Feature': numerical_features,\n    'Skewness': skew(train[numerical_features])}).sort_values(by='Skewness',\n                                                              ascending=False).set_index('Feature').head(10)","0df90c3e":"train[year_features].head()","9396c775":"plt.figure(figsize=(15, 9))\nfor i, feature in enumerate(year_features, 1):\n    plt.subplot(2, 2, i)\n    temp = train.groupby(feature)['SalePrice'].median().plot()\n    plt.xlabel(feature)\n    # plt.xlim([2006, 2010])\n    # plt.xticks(range(2006, 2011))\n    plt.title(f\"Median price vs {feature}\")\n    plt.ylabel(\"Sale price\", rotation=90)\nplt.tight_layout(w_pad=1.2, h_pad=1.2)\nplt.show()","47a1d9c6":"plt.figure(figsize=(20, 6))\n\nfor i, feature in enumerate(year_features, 1):\n    \n    if feature != 'YrSold':\n        data = train.copy()\n        \n        data[feature] = data['YrSold'] - data[feature]\n        plt.subplot(1, 3, i)\n#         plt.scatter()\n        plt.title(feature)\n        \n        plt.ylabel('SalePrice')\n        sns.regplot(data[feature], data['SalePrice'], \n                   scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"red\"})\n        plt.xlabel(f\"No. of years: {feature}\")\nplt.tight_layout()\nplt.show()        ","58092d62":"sns.heatmap(pd.DataFrame({\n    'SalePrice': train['SalePrice'],\n    'YearBuiltAge': train['YrSold'] - train['YearBuilt'],\n    'YearRemodAddAge': train['YrSold'] - train['YearRemodAdd'],\n    'GarageYrBltAge': train['YrSold'] - train['GarageYrBlt'],\n}).corr(), annot=True, cmap='coolwarm')\nplt.title('Features as age vs SalePrice')\nplt.show()","03480cfb":"discrete_features","9da9ceb2":"train[discrete_features].head()","7d692172":"plt.figure(figsize=(20, 20))\nfor i, feature in enumerate(discrete_features, 1):\n    plt.subplot(6, 3, i)\n    data = train.copy()\n    \n    # let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    \n    # calculate the median SalePrice where the information is missing or present\n    temp = data.groupby(feature)['SalePrice'].median()\n    _ = sns.barplot(x=temp.index, y=temp.values)\n    plt.title(feature)\n    plt.xlabel(\"\")\n    plt.ylabel(\"Sales Price\", rotation=90)\n    plt.xticks(rotation=45)\nplt.tight_layout(h_pad=2, w_pad=2)\nplt.show()","ebfa9975":"print(f\"Number of Continuous numerical feature: {len(continuous_num_features)}\")","d54db23c":"continuous_num_features","1de5e82b":"train[continuous_num_features].head()","77a21858":"plt.figure(figsize=(20, 20))\nfor i, feature in enumerate(continuous_num_features, 1):\n    plt.subplot(5, 4, i)\n#     _ = sns.distplot(train[feature], kde_kws={'bw': 1.05})    \n    _ = sns.distplot(train[feature], kde=False, rug=True)\n    \nplt.tight_layout(h_pad=2, w_pad=2)\nplt.show()","9fdfe76a":"data = train.copy()\n\nsale_price = np.log(train['SalePrice'])\n\nfor i, feature in enumerate(continuous_num_features[:-1], 1):\n    data = train[feature].copy()\n    \n    if 0 in data.unique(): # as log 0 is undefinedz\n        continue\n    else:\n        data = np.log(data)    \n        data.name = feature\n        _ = plt.figure(figsize=(15, 8))\n        plt.subplot(1, 2, 1)\n        sns.regplot(data, sale_price, fit_reg=True,\n                   scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"red\"}).set_title(f\"Correlation: {data.corr(sale_price)}\")\n        plt.subplot(1, 2, 2)\n        sns.distplot(data).set_title(f\"Log transformation of: {feature}\")\nplt.show()","f9789622":"categorical_features = [feature for feature in train.columns if train[feature].dtypes=='O']\nprint(f\"Number of categorical feature: {len(categorical_features)}\")","623e410e":"categorical_features","a13a2a5a":"train[categorical_features].head()","b8ec4646":"pd.DataFrame({\n    \"features\": categorical_features,\n    \"Nunique\": [train[feature].nunique() for feature in categorical_features]             \n             })","bb64cc6a":"plt.figure(figsize=(40, 30))\n\nfor i, feature in enumerate(categorical_features, 1):\n    data = train.copy()\n    temp = data.groupby(feature)['SalePrice'].median()\n    plt.subplot(9, 5, i)\n    sns.barplot(temp.index, temp.values)\n    plt.xticks(rotation=45)\n\nplt.tight_layout(h_pad=1.2)\nplt.show()","d6589e72":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers. Therefore, we can safely delete them.","6ccc4c83":"### Performing Logarithmic transformation to the data","9516d0f0":"## Skewed Features","40d68b8c":"**Summary:**\n* `GrLivArea` and `TotalBsmtSF` seem to be linearly related with 'SalePrice'. Both relationships are positive.\n* `OverallQual`, `TotRmsAbvGrd` and `YearBuilt` also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of `OverallQual`, where the box plot shows how sales prices increase with the overall quality. The same conclusion can be reached with `TotRmsAbvGrd`, though it's not as stronger as `OverallQual`.\n","4b8b24de":"**Observations:**\n* We can see the skewness present.\n* We can also see that many of the values are zero.\n* We cannot simply apply the log transformation on this.\n* What we can do is can do is create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.","39a89ff1":"relationship between discrete features and SalePrice","95441bae":"If we look at the types of masonry venner and their corresponding area,\nfor all the missing values.\n\nArea is zero.\n\nSo we can fill these missing values with \"None\"","19e32d52":"After going through the proces, we can clearly that the features as selected by [Pedro Marcelino](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) are completely valid with the addition of one more i.e. `TotRmsAbvGrd` could also be an important factor for prediction of SalePrice.\nAs pointed out in the kernel, `Neigborhood` feature didn't appeal as much information than the selected ones.\nFeatures to inspect:\n* OverallQual\n* YearBuilt.\n* TotalBsmtSF.\n* GrLivArea\n* TotRmsAbvGrd","e4982160":"**Observations:**\n* Though the data has been transformed we can see some irregulaties for eg. `LotArea` looking at the scatter we can see that for some _same_ values the SalePrice increases. We can see that there's not much of correlation between the two\n* log(`GrLivArea`) is heavily correlated with `SalePrice` and so is `1stFlrSF`\n* We were not able to take log of other continuous features as `log(0)` is undefined","1bde3516":"## Normality\n\nThe point here is to test 'SalePrice' in a very lean way. We'll do this paying attention to:\n\n* **Histogram** - Kurtosis and skewness.\n* **Normal probability plot** - Data distribution should closely follow the diagonal that represents the normal distribution.","f81d50ea":"3. `Bsmt` variable\n\n* `BsmtQual`: 37 missing values\n* `BsmtCond`: 37 missing values \n* `BsmtExposure`: 38 missing values\n* `BsmtFinType1`: 37 missing values\n* `BsmtFinType1`: 38 missing values\n\n","4f20451d":"**Observation:**\n* What we did was take a look at the `SalePrice` values through the years for different features.\n* We can see that although there were different intervals, the regression line passing is almost the same.\n* We can see that new houses built or remodelled or had a newly built garage were sold for higher prices.","31b2eb2f":"## Correlation matrix Heatmap\n\n","467bf543":"We have gone through quite a number of differnt numerical, categorical, discrete, continuous and temporal features and have a plan on how to go around the feature engineering process.\n\nIn the next kernel we'll see how we can from the concepts learned from this notebook apply it to the feature engineering process and later the end goal of this project: model building \n\nPart 2: Kernel => [Project 2 P2: Model building](https:\/\/www.kaggle.com\/veb101\/project-2-p2-model-building)","b9e6ea92":"##Examining Missing Features\n\n1. `FirePlaceQu`: 690 missing values","45c8b22a":"How 'SalePrice' looks with her new clothes:\n\n* Low range values are similar and not too far from 0.\n* High range values are far from 0 and the 7.something values are really out of range.\n\nFor now, we'll not consider any of these values as an outlier but we should be careful with those two 7.something values.","e5a94c56":"## Temporal variables\n\n* There are total of 4 datetime variables\/columns in the dataset.\n* These columns are useful for prediction as we can assume that there will be changes in price values depending upon time.","dbabd963":"**Observations**\n* We can see an unusual trend between Sales price and Year Sold, typically the prices of house increases with time but from the graph we can see the opposite.\n* One reason could due to the financial drought of '08 when in the US collapsed.\n* We can see that the houses and garage which were built and remoddeled during 90s have less Sale Price than the newer ones.\n* With every year new houses that are built, the house price increases.","01d4ed30":"It's natural the more room the house has the higher the price.","e2926253":"# Target Assumptions\n\n**Question:** Who is `SalePrice`?\n\n\nThe answer to this question lies in testing for the assumptions underlying the statistical bases for multivariate analysis. We already did some data cleaning and discovered a lot about `SalePrice`. Now it's time to go deep and understand how 'SalePrice' complies with the statistical assumptions that enables us to apply multivariate techniques.\n","db9c3ffb":"# Missing data","2aecd6eb":"# Types of numerical variables\n1. Continuous\n2. Discrete","96fcebf7":"**[NOTE]**: We'll look at feature transformation during feature engineering process.","f359843c":"## Discrete numerical values","8021789c":"We can see that there is a linear relationship between `SalePrice` and `GrLivArea` whereas there's a slight exponential relationship with `TotalBsmtSF`","c4c8a4a3":"Eg.\n```python\n#if area>0 it gets 1, for area==0 it gets 0\ntrain['HasBsmt'] = 0 \ntrain.loc[train['TotalBsmtSF']>0,'HasBsmt'] = 1\n\n# transform data\ntrain.loc[train['HasBsmt']==1,'TotalBsmtSF'] = np.log(train['TotalBsmtSF'])\n```","fd4fcfca":"## homoscedasticity\n\nThe best approach to test homoscedasticity for two metric variables is graphically. Departures from an equal dispersion are shown by such shapes as cones (small dispersion at one side of the graph, large dispersion at the opposite side) or diamonds (a large number of points at the center of the distribution).","24a85cb4":"Some of the continuous numerical features seem to be correlated with each other though not as strongly. Though some new correlation are noticeable we still reach the same conclusions as above","22df96c5":"Without even plotting the histogram or KDE we can clearly see that the feature is skewed as the `mean` and meadian i.e.`25%` values are different, also we can see that `75%` of the prices values are between **35000 - 215000** but the max value is way of the scale than it should be. Let's plot this to see if our conclusion is True.","b2379681":"# EDA","686c3062":"According to [Hair et al. (2013)](https:\/\/amzn.to\/2uC3j9p), four assumptions should be tested:\n\n* **Normality** - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.\n\n* **Homoscedasticity** - I just hope I wrote it right. Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' (Hair et al., 2013). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.\n\n* **Linearity**- The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.\n\n* **Absence of correlated errors** - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.","d2c9b316":"* The GarageX variables have the same number of missing data.\n* We can simply fill the variables with _No garage_.\n* But mostly these values are correlateed and `GarageCars` variable can only be used.","3b09bf69":"**Observation**\n* We can see that for some features when the value is *NA*, the median value for that set of columns is higher than when values are available.\n* So it is safe to assume that the missing values need to be replaced with some meaningful values.","343e98c4":"* Looks like `FireplaceQu` is missing at places where `FirePlaces` feature is missing.\n* We can't do anything but fill these values with \"Not avaliable\".\n\n2. `MasVnrType`: 8 missing values\n","62153eb7":"* The missing values would most probably  be because there is no basement.\n* We can fill the missing values with _No Basement_\n","0672a3f7":"We only have half of Fireplace quality data. \n\nLet's have a look at the Fireplaces feature.","d463e25b":"**Observations:**\n* For starters we can see that the most of the continuous features don't follow a normal distribution.\n* Some of the features are heavily skewed so we would have to perform some transformation to them.","b6c1eabe":"* `OverallQual`, `GrLivArea` and `TotalBsmtSF` are strongly correlated with `SalePrice`.\n* `TotRmsAbvGrd` and `GrLivArea` are strongly correlated each other. We can drop `TotRmsAbvGrd` as it's not as correlated with `SalePrice` than `GrLivArea`.\n* `GarageCars` and `GarageArea` are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. `GarageCars` and `GarageArea` are like twin brothers. You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n* `TotalBsmtSF` and `1stFloor` also seem to be twin brothers. We can keep `TotalBsmtSF` just to say that our first guess was right.\n* `FullBath` seems odd but expected\n* `TotRmsAbvGrd` and `GrLivArea` are also highly correlated with each other. \n* `YearBuilt`: It seems that `YearBuilt` is slightly correlated with `SalePrice`. Honestly, it scares me to think about `YearBuilt` because I start feeling that we should do a little bit of time-series analysis to get this right. I'll leave this as a homework for you.","208cb4fc":"# Personal selection and understanding of features","c1159249":"## Bivariate analysis","a04def11":"Let's find the continuous numerical variables in out dataset and plot a correlation matrix Heatmap","ac8ce9dd":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>EDA<\/a><\/span><\/li><li><span><a href=\"#Personal-selection-and-understanding-of-features\" data-toc-modified-id=\"Personal-selection-and-understanding-of-features-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Personal selection and understanding of features<\/a><\/span><\/li><li><span><a href=\"#Taking-a-look-at-SalePrice\" data-toc-modified-id=\"Taking-a-look-at-SalePrice-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Taking a look at SalePrice<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Relationship-between-selected-features\" data-toc-modified-id=\"Relationship-between-selected-features-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Relationship between selected features<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Relation-with-numerical-variables\" data-toc-modified-id=\"Relation-with-numerical-variables-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;<\/span>Relation with numerical variables<\/a><\/span><\/li><li><span><a href=\"#Relation-with-categorical-variables\" data-toc-modified-id=\"Relation-with-categorical-variables-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;<\/span>Relation with categorical variables<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Analysis-of-other-variables\" data-toc-modified-id=\"Analysis-of-other-variables-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Analysis of other variables<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Correlation-matrix-Heatmap\" data-toc-modified-id=\"Correlation-matrix-Heatmap-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Correlation matrix Heatmap<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Missing-data\" data-toc-modified-id=\"Missing-data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Missing data<\/a><\/span><\/li><li><span><a href=\"#Considering-effect-of-Outliers\" data-toc-modified-id=\"Considering-effect-of-Outliers-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Considering effect of Outliers<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Univariate-analysis\" data-toc-modified-id=\"Univariate-analysis-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Univariate analysis<\/a><\/span><\/li><li><span><a href=\"#Bivariate-analysis\" data-toc-modified-id=\"Bivariate-analysis-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>Bivariate analysis<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Target-Assumptions\" data-toc-modified-id=\"Target-Assumptions-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Target Assumptions<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Normality\" data-toc-modified-id=\"Normality-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Normality<\/a><\/span><\/li><li><span><a href=\"#homoscedasticity\" data-toc-modified-id=\"homoscedasticity-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>homoscedasticity<\/a><\/span><\/li><li><span><a href=\"#Skewed-Features\" data-toc-modified-id=\"Skewed-Features-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;<\/span>Skewed Features<\/a><\/span><\/li><li><span><a href=\"#Temporal-variables\" data-toc-modified-id=\"Temporal-variables-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;<\/span>Temporal variables<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Types-of-numerical-variables\" data-toc-modified-id=\"Types-of-numerical-variables-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Types of numerical variables<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Discrete-numerical-values\" data-toc-modified-id=\"Discrete-numerical-values-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span>Discrete numerical values<\/a><\/span><\/li><li><span><a href=\"#Continuous-numerical-values\" data-toc-modified-id=\"Continuous-numerical-values-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span>Continuous numerical values<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Performing-Logarithmic-transformation-to-the-data\" data-toc-modified-id=\"Performing-Logarithmic-transformation-to-the-data-8.2.1\"><span class=\"toc-item-num\">8.2.1&nbsp;&nbsp;<\/span>Performing Logarithmic transformation to the data<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Categorical-variables\" data-toc-modified-id=\"Categorical-variables-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Categorical variables<\/a><\/span><\/li><\/ul><\/div>","55e2d91e":"**From the kernel:**\n\n\nAt first sight, there are two red colored squares that get my attention. The first one refers to the `TotalBsmtSF` and `1stFlrSF` variables, and the second one refers to the `GarageX` variables. Both cases show how significant the correlation is between these variables. Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, we can conclude that they give almost the same information so multicollinearity really occurs. Heatmaps are great to detect this kind of situations and in problems dominated by feature selection, like ours, they are an essential tool.\n\nAnother thing that got my attention was the 'SalePrice' correlations. We can see our well-known `GrLivArea`, `TotalBsmtSF`, and `OverallQual` saying a big 'Hi!', but we can also see many other variables that should be taken into account. That's what we will do next.","17473397":"The plot just confirms the fact the higher quality, higher the price.","a5225b06":"5. Garage\n* `GarageType`: 81 missing values\n* `GarageFinish`: 81 missing values\n* `GarageQual`: 81 missing values\n* `GarageCond`: 81 missing values\n","411ac854":"**Observations**\n* We can clearly see that time interval of `YrBuilt` and `GarageYrBuild` are highly correlated with each other, so is `YrRemodAdd` with `GarageYrBuild`.\n* Interval of `YrBuilt` is not correlated with  `SalePrice`.\n* During feature engineering we can convert these temporal features to represent the interval.\n* We can also see that time intervals of `YrBuilt` and `YrRemodAdd` are highly negatively correlated with `SalePrice`. \n* If we need to keep one of them, from a subjective point of view I'll choose to keep the `YrRemodAdd` as from my perspective, the year the house was built though important we will most likely ask for the date the house was remodelled as a way to get info has an effect on the price of the house. We can use different models that indicate feature importance to check this out.","6dce09cb":"Let's examine the outliers present in continuous features after applying a log transformation ","9980ec90":"# Considering effect of Outliers","a1b9c829":"# Categorical variables","c33dc5c2":"## Relationship between selected features","14137289":"Though we can never quantify this for sure, but more recently built houses will have a high price.","11afedb2":"* We can see a lot of numerical features are heavily skewed.\n* We would need to apply a **log** or **box cox** transformations to these features.","2376c1e1":"**Observations**:\n* It's clear that the values of discrete numerical features has an effect on the SalePrice.\n* At some places we can see that as the feature value increases the price also increases\n* At some places the bars has pretty high for only some values.\n* We can see some features that have almost the same SalePrice for it's different values\n* Looking from the top, it's not clear how most of the features will have an impact on the SalePrice.\n* Feature selection process might be able to clear this up.","3111513f":"### Relation with numerical variables","c4dd044b":"And...we were right. The data is right skewed(tail is on the right hand side).\nLet's look at how skewed the data is.","c7bdf8d8":"Check if the columns with missing values has some valuable relationship with the outputs.","10b2bc6e":"all numerical features","2cea40ff":"* We can either delete the features with more than 50$ missing values or can fill them with appropriate values. None of these variables seem to be very important, since most of them are not aspects in which we think about when buying a house\n* The `GarageX` variables have the same number of missing data.Since the most important information regarding garages is expressed by `GarageCars` and considering that we are just talking about 5% of missing data, the same logic can be applied to the `BSMTX` variables.\n* For `MasVnrArea` and `MasVnrType`, these variables can be considered as not essential. Furthermore, they have a strong correlation with `YearBuilt` and `OverallQual` which are already considered. \n* `Electrical` has just one missing value we can either delete that row or fill it with mode value.","1f377d53":"## Continuous numerical values","d7b0cce3":"This notebook is focused on performing **Exploratory Data Analysis** on the [House Prices: Advanced Regression techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques).\n\nBefore starting I want to thank other kaggle users for their work on this problem. It helped me alot in understanding this problem.\n\nThis and others notebooks onn this project series relies heavily on other great kernels made on this dataset.\nNaming a few:\n1. [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)\n2. [A study on Regression applied to the Ames dataset](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset)\n3. [Eda and prediction of House Price](https:\/\/www.kaggle.com\/siddheshpujari\/eda-and-prediction-of-house-price)\n3. [Stacked Regressions to predict House Prices](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)\n4. [Regularized Linear Models](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models)\n5. [How I made top 0.3% on a Kaggle competition](https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition)","b974eb1e":"For labelling a point as an *outlier* we need to define a threshold value that defines the datapoint as an outlier. We can do this bu standardizing the data.","15d21620":"Find out the relationship between categorical features and SalePrice","1555cbfa":"## Univariate analysis","299b01bf":"As suggested in the notebook before starting it would be a good idea to to explore the data description file given as it would help to better understand the available features. It will also be helpful because we can see that there are **81** features present and some of them won't provide useful information in terms of the dependant feature i.e. `SalePrice`.","3849afb0":"`SalePrice` w.r.t. `YrSold` ","0c25890e":"`SalePrice` is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line.\n\nA simple data transformation can solve the problem. Similarly with `GrLivArea`","798bb453":"4. Electrical: 1 missing value\n\nWe can either just delete that row or fill the missing row with the mode of the feature","31d1205e":"# Taking a look at SalePrice","9ceb7ce2":"**Observations**:\n* There are plenty of outliers present even after log transformation in every featufres.\n* These outliers will most likely interfere during the model building process.\n* We will need to handle them during feature engineering\n* There are a few techniques for outlier handling:\n1. Outlier removal\n2. Treating outliers as missing values\n3. Top \/ bottom \/ zero coding\n4. Discretisation\n","a1d98003":"**Insights on all categorical features**\n* After reading features decription the categorical features, my conclusions are:\n1. `MSZoning`: Tells about the zone the house is located in, but we already have a neigborhood variable that'll be more effective.\n2. `Street`, `Alley` and `LotConfig` don't seem that important.\n3. `LandContour`,  `LotShape`, `LandSlope`: are basically giving the same information, they might be correlated with each other too.\n4. `Utilities`: is an important feature as it generally affects the price of a house.\n5. `Neigborhood`: Location is important while buying a house though we ended up discarding it in the initial selection as most of the SalePrice values are have overlapping with each other.\n6. `Condition1` and `Condition2`: `Condition1` seems a good choice.\n7. `BldgType` and `HouseStyle`: Both are important factors for pricing.\n8. `RoofStyle`, `RoofMatl`: Does it matter as long is it sturdy?. \n9. `Exterior1st`, `Exterior2nd`: Both seem to have effect on the Sales Price.\n10. `MasVnrType`: From above it does seem to be important (feature selection will help in determining this).\n11. `ExterQual` and `ExterCond`: `ExterCond` seems a likely choice as the current condition of the externals determines the price more than what it was.\n12. `Foundation`: one of the main ingredients in determing house prices.\n13. `BsmtQual`, `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2`: Earlier during missing value analysis there was consistency in  the rows that matched. These features might be correlated with each other so we can either take just one of two features that represents the `BsmtX` variables and have high correlation with the `SalePrice`. `BsmtQual` and `BsmtExposure` seem optimal choices.\n14. `Heating`, `HeatingQC`: `HeatingQC` seems a good choice of the two.\n15. `CentralAir`, `Electrical`: Though not many of us would ask the type of electrical system in the house as an initial question but information about the conditioning might play a role in determining the price.\n16. `KitchenQual`: an important factor we would all agree on.\n17. `FireplaceQu`: It has a lot of missing values as if fireplace is not present, but does affect the pricing of the house if present.\n18. `GarageType`, `GarageFinish`, `GarageQual`,  `GarageCond`: as dicussed earlier to represent all these features `GarageCars` might be sufficienct.\n19. `PoolQC`, `Fence`: too many missing values.\n20. `MiscFeature`: `MiscVal` feature might be all that's necessary.\n21. `SaleType` and `SaleCondition`: Personally don't think would affect the Sale price value of the house that much.\n22. `PavedDrive`: Surroundings do affect the price of a house.\n","24445340":"Woah, it's more badly skewed than I hoped it would be, to negate this effect we would have to apply some kind of transformation to the feature viz. `log` or `BoxCox` during the feature engineering process.","781369b7":"# Analysis of other variables\n\nSo far we only took the variables that we subjectively chose and made performed analysis about them, but it's the best approach, we need to look at the available data subjectively.","948cb36c":"Calculate the difference between all `year_features` with `SalePrice`. Doing this will allow us to get the `age` of the houses and with the help of scatter plot can see the relationship between `SalePrice` with the `age` of the house","21fb15ee":"### Relation with categorical variables"}}