{"cell_type":{"95e5f896":"code","4065429b":"code","0da5aef8":"code","c071c652":"code","fb1d1f84":"code","799353b5":"code","8a68ebf7":"code","dc1adbac":"code","43256822":"code","f3a37e11":"code","45bd5a78":"code","c698fa49":"code","168e972d":"code","6de0ef6c":"markdown"},"source":{"95e5f896":"# IMPORT LIBRARIES\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import ArtistAnimation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import StratifiedKFold","4065429b":"test = pd.read_csv('..\/input\/test.csv')\ntrain = pd.read_csv('..\/input\/train.csv')","0da5aef8":"train2 = train.loc[train['wheezy-copper-turtle-magic'] == 0, :].reset_index(drop=True)\ntrain2.drop('id', axis=1, inplace=True)\ntarget2 = train2.target\ntrain2.drop('target', axis=1, inplace=True)\ntrain2.drop('wheezy-copper-turtle-magic', axis=1, inplace=True)\ncols = np.array([c for c in train2.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']])\nsel = VarianceThreshold(threshold=1.5).fit(train2[cols])\nuseful_cols = cols[np.where(sel.variances_ > 1.5)[0]]\ntrain2 = train2[cols]","c071c652":"results = []\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=2019)\n\nfor col in tqdm(train2.columns):\n    lre_oof_preds = np.zeros(len(train2))\n    for fold_, (trn_, val_) in enumerate(folds.split(train2, target2)):\n        trn_x, trn_y = train2.loc[trn_, col], target2[trn_]\n        val_x, val_y = train2.loc[val_, col], target2[val_]\n        clf = LogisticRegression(solver='liblinear',penalty='l2',C=0.1,class_weight='balanced')\n        clf.fit(train2[col].values.reshape(-1, 1), target2.values)\n        lre_oof_preds[val_] = clf.predict(val_x.values.reshape(-1,1))\n    results.append([col, roc_auc_score(target2, lre_oof_preds), col in useful_cols, np.std(train2[col].values)])\n\nresults_pd = pd.DataFrame(results, columns=['variable', 'roc auc', 'in useful cols?', 'std']).sort_values(by='roc auc', ascending=False).reset_index(drop=True)","fb1d1f84":"results_pd.head(30)","799353b5":"results_pd.to_csv('resulting_dude.csv', index=None)","8a68ebf7":"# IMPORT LIBRARIES\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import ArtistAnimation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import roc_auc_score\n\nonly_new_cols = results_pd.loc[(results_pd['roc auc'] > 0.5) & (results_pd['in useful cols?'] == False), 'variable'].values\ntrain2 = train.loc[train['wheezy-copper-turtle-magic'] == 0, :].reset_index(drop=True)\ntarget2 = train2.target\ntrain2.drop('target', axis=1, inplace=True)\ntrain2 = train2[only_new_cols]\ncols = [c for c in train2.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\ntrain2 = train2[cols].values\n# FIND NORMAL TO HYPERPLANE\nclf = LogisticRegression(solver='liblinear',penalty='l2',C=0.1,class_weight='balanced')\nclf.fit(train2, target2)\nu1 = clf.coef_[0]\nu1 = u1\/np.sqrt(u1.dot(u1))\n# CREATE RANDOM DIRECTION PERPENDICULAR TO U1\nu2 = np.random.normal(0,1,len(u1))\nu2 = u2 - u1.dot(u2)*u1\nu2 = u2\/np.sqrt(u2.dot(u2))\n\n# CREATE RANDOM DIRECTION PERPENDICULAR TO U1 AND U2\nu3 = np.random.normal(0,1,len(u1))\nu3 = u3 - u1.dot(u3)*u1 - u2.dot(u3)*u2\nu3 = u3\/np.sqrt(u3.dot(u3))\nidx0 = np.where(target2==0)\nidx1 = np.where(target2==1)\n# CREATE AN ANIMATION\nimages = []\nsteps = 60\nfig = plt.figure(figsize=(8,8))\nfor k in range(steps):\n    # CALCULATE NEW ANGLE OF ROTATION\n    angR = k*(2*np.pi\/steps)\n    angD = round(k*(360\/steps),0)\n    u4 = np.cos(angR)*u1 + np.sin(angR)*u2\n    u = np.concatenate([u4,u3]).reshape((2,len(u1)))  \n    # PROJECT TRAIN AND TEST ONTO U3,U4 PLANE\n    p = u.dot(train2.transpose())\n    # PLOT TRAIN DATA (KEEP CORRECT COLOR IN FRONT)\n    if angD<180:\n        plt.title('wheezy-copper-turtle-magic=1; all features')\n        img2 = plt.scatter(p[0,idx1],p[1,idx1],c='yellow')\n        img3 = plt.scatter(p[0,idx0],p[1,idx0],c='blue')\n    else:\n        plt.title('wheezy-copper-turtle-magic=1; all features')\n        img2 = plt.scatter(p[0,idx0],p[1,idx0],c='blue')\n        img3 = plt.scatter(p[0,idx1],p[1,idx1],c='yellow')\n    images.append([img2, img3])\n# SAVE MOVIE TO FILE\nani = ArtistAnimation(fig, images)\nani.save('all_features.gif', writer='imagemagick', fps=15)\nplt.close()\n# PROJECT TRAIN ONTO U2, U3 PLANE\nu = np.concatenate([u1,u2]).reshape((2,len(u1)))\np = u.dot(train2.transpose())\nplt.figure(figsize=(6,6))\nplt.title('wheezy-copper-turtle-magic=1; bunch of useless columns')\nplt.scatter(p[0,idx1],p[1,idx1],c='yellow', alpha=0.9)\nplt.scatter(p[0,idx0],p[1,idx0],c='blue', alpha=0.9)\nprint(f'ROC: {roc_auc_score(target2, clf.predict(train2))}')","dc1adbac":"# check if it applies to the test set\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=2019)\nfor wctm in tqdm(range(train['wheezy-copper-turtle-magic'].nunique())):\n    train2 = train.loc[train['wheezy-copper-turtle-magic'] == wctm, :]\n    train2 = train2.reset_index(drop=True)\n    test2 = test.loc[test['wheezy-copper-turtle-magic'] == wctm, :]\n    test2_ids = test2.index\n    test2 = test2.reset_index(drop=True)\n    train2.drop('id', axis=1, inplace=True)\n    target2 = train2.target\n    train2.drop('target', axis=1, inplace=True)\n    train2.drop('wheezy-copper-turtle-magic', axis=1, inplace=True)\n    cols = np.array([c for c in train2.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']])\n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    useful_cols = cols[np.where(sel.variances_ > 1.5)[0]]\n    train2 = train2[cols]\n\n    results = []\n    for col in train2.columns:\n        lre_oof_preds = np.zeros(len(train2))\n        for fold_, (trn_, val_) in enumerate(folds.split(train2, target2)):\n            trn_x, trn_y = train2.loc[trn_, col], target2[trn_]\n            val_x, val_y = train2.loc[val_, col], target2[val_]\n            clf = LogisticRegression(solver='liblinear',penalty='l2',C=0.1,class_weight='balanced')\n            clf.fit(train2[col].values.reshape(-1, 1), target2.values)\n            lre_oof_preds[val_] = clf.predict(val_x.values.reshape(-1,1))\n        results.append([col, roc_auc_score(target2, lre_oof_preds), col in useful_cols, np.std(train2[col].values)])\n\n    results_pd = pd.DataFrame(results, columns=['variable', 'roc auc', 'in useful cols?', 'std'])\n    only_new_cols = results_pd.loc[(results_pd['roc auc'] > 0.5) & (results_pd['in useful cols?'] == False), 'variable'].values\n    clf.fit(train2[only_new_cols], target2)\n    \n    test.loc[test2_ids, 'target'] = clf.predict_proba(test2[only_new_cols])[:, 1]","43256822":"# submit test predictions\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = test.target\n\nsub.to_csv('submission_based_on_useless_cols.csv', index=None)","f3a37e11":"train2 = train.loc[train['wheezy-copper-turtle-magic'] == 111, :].reset_index(drop=True)\ntrain2.drop('id', axis=1, inplace=True)\ntarget2 = train2.target\ntrain2.drop('target', axis=1, inplace=True)\ntrain2.drop('wheezy-copper-turtle-magic', axis=1, inplace=True)\ncols = np.array([c for c in train2.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']])\nsel = VarianceThreshold(threshold=1.5).fit(train2[cols])\nuseful_cols = cols[np.where(sel.variances_ > 1.5)[0]]\ntrain2 = train2[cols]","45bd5a78":"results = []\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=2019)\n\nfor col in tqdm(train2.columns):\n    lre_oof_preds = np.zeros(len(train2))\n    for fold_, (trn_, val_) in enumerate(folds.split(train2, target2)):\n        trn_x, trn_y = train2.loc[trn_, col], target2[trn_]\n        val_x, val_y = train2.loc[val_, col], target2[val_]\n        clf = LogisticRegression(solver='liblinear',penalty='l2',C=0.1,class_weight='balanced')\n        clf.fit(train2[col].values.reshape(-1, 1), target2.values)\n        lre_oof_preds[val_] = clf.predict(val_x.values.reshape(-1,1))\n    results.append([col, roc_auc_score(target2, lre_oof_preds), col in useful_cols, np.std(train2[col].values)])\n\nresults_pd = pd.DataFrame(results, columns=['variable', 'roc auc', 'in useful cols?', 'std']).sort_values(by='roc auc', ascending=False).reset_index(drop=True)","c698fa49":"results_pd.head(30)","168e972d":"results_pd.to_csv('resulting_dude_of_111.csv', index=None)","6de0ef6c":"This kernel is an effort to show that the so-called \"useless\" columns that has smaller variances have actually predictive power. I tried to show it once in [this kernel](https:\/\/www.kaggle.com\/mhviraf\/there-is-predictive-power-in-the-useless-columns). However, Chris pointed out that the results I showed there are actually due to the curse of dimensionality which he described in detail [here](https:\/\/www.kaggle.com\/c\/instant-gratification\/discussion\/93379#latest-537928). So I digged deeper to assess whether or not there is predictive power in \"useless\" columns. Here is what I did:\n1. filtered dataset by `wheezy-copper-turtle-magic=0`\n2. trained a logistic regression classifier on every column (512 10fold models) and calculated the ROC AUC\n3. results show that 14 of the top 30 features with the highest predictive power are among the \"useless\" columns.\n4. to make sure it's not an accident, I repeated 1 to 3 for `wheezy-copper-turtle-magic=111` and this time 16 out of top 30 most predictive columns were among \"useless\" columns.\n5. update: I also added the plot that shows hyperplane of logistic regression that is trained on only the \"useless\" columns which resulted in ROC AUC > 0.5 in the table below (condition: ROC AUC > 0.5 & in useful cols == False). The plot shows they can give us separable classes. Hence, they have predictive power.\n6. update2: I am making predictions on the test set based on \"useless\" columns only, if the ROC AUC is greater than 0.5 then we know for sure that they have predictive power. "}}