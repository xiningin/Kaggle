{"cell_type":{"d5c3ff26":"code","2be18f43":"code","d32a2134":"code","cc953a6a":"code","3879553f":"code","f7c7f2c5":"code","adfb5e38":"code","70711c47":"code","55e38044":"code","f87be996":"code","d521cead":"code","500d63a0":"code","0a1f8974":"code","9f9ae1eb":"code","3534be50":"code","71b82213":"code","201c7c07":"code","c929a56f":"code","6c35fc29":"code","7dcf6b9e":"markdown","af0ab11f":"markdown","593a1e98":"markdown","3f18c601":"markdown","fdbbc288":"markdown","1b002523":"markdown","446e4f4c":"markdown","e4ed4af2":"markdown","8c7d000c":"markdown","fc9e805e":"markdown","020388fa":"markdown","0195ab12":"markdown","726aaf6b":"markdown","d6e0408d":"markdown","a47dd316":"markdown","592887e8":"markdown","285c5195":"markdown","a5d04aa2":"markdown"},"source":{"d5c3ff26":"import numpy as np, pandas as pd\nimport matplotlib.pyplot as plt, seaborn as sns\nimport statsmodels.api as sm","2be18f43":"raw_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nraw_data.head(3)","d32a2134":"raw_data.describe(include = \"all\")","cc953a6a":"data = raw_data.copy()\n\ndata = data.drop([\"Name\", \"Cabin\", \"Ticket\"], axis = 1)\ndata.head(3)","3879553f":"data.info()","f7c7f2c5":"data[\"Age\"] = data[\"Age\"].fillna(data[\"Age\"].mean())\ndata = data.dropna()\n\ndata.info()","adfb5e38":"data = pd.get_dummies(data, drop_first = True)\ndata = data.set_index(\"PassengerId\") # PassengerID is not a feature as it is unique for all\n\ndata.head()","70711c47":"data.columns","55e38044":"features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Embarked_Q', 'Embarked_S']\nx1 = data[features]\ny = data[\"Survived\"]","f87be996":"x = sm.add_constant(x1)\nreg_logit = sm.Logit(y, x)\nresults_logit = reg_logit.fit()\nresults_logit.summary()","d521cead":"features = ['Pclass', 'Age', 'SibSp', 'Sex_male']\nfeatures","500d63a0":"x1 = data[features]\ny = data[\"Survived\"]\n\nx = sm.add_constant(x1)\nreg_logit = sm.Logit(y, x)\nresults_logit = reg_logit.fit()\nresults_logit.summary()","0a1f8974":"def confusion_matrix(data, actual_values, model):\n\n        pred_values = model.predict(data)\n        \n        bins=np.array([0,0.5,1])\n        cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n        \n        accuracy = (cm[0,0]+cm[1,1])\/cm.sum()\n        return cm, accuracy\n\nconfusion_matrix(x, y, results_logit)","9f9ae1eb":"test_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest_data = test_data.drop([\"Name\", \"Fare\", \"Embarked\", \"Parch\", \"Ticket\", \"Cabin\"], axis = 1)\ntest_data[\"Age\"] = test_data[\"Age\"].fillna(test_data[\"Age\"].mean())\ntest_data = test_data.set_index(\"PassengerId\")\n\ntest_data.head()","3534be50":"test_data.info()","71b82213":"test_data = pd.get_dummies(test_data, drop_first = True)\ntest_data.head()","201c7c07":"test_data = sm.add_constant(test_data)\n\npred_values = results_logit.predict(test_data)\ncsv = pd.DataFrame(pred_values, columns = [\"Survival Probability\"])\npred_values = csv.reset_index()\npred_values","c929a56f":"def convert(x):\n    if x < 0.5: return 0\n    else: return 1\n    \npred_values[\"Survived\"] = pred_values[\"Survival Probability\"].apply(convert)\npred_values = pred_values.drop([\"Survival Probability\"], axis = 1)  # dropping 'Survival Probability'\npred_values.head()","6c35fc29":"pred_values.to_csv(\"MyFirstSubmission.csv\", index = False)","7dcf6b9e":"Survival Probability in this dataframe is the probability of survival for each passenger.\n\n**Converting Survival Probability into Survival**","af0ab11f":"## Titanic Survival Prediction using Logistic Regression\nThe objective of this notebook is to attempt a submission for the Titanic survival predictor on kaggle.\n\nWe will be using simple logistic regression for this problem. The goal, now, is not to get high score but is to see how well simple logistic regression model can help us in getting the job done.\n\n**Importing libraries:**","593a1e98":"Our model is 78.74% accurate for the data it got trained upon. We will now apply the model on unseen i.e. Test Data. But, before model application, we need to preprocess data a little bit.\n\n**Loading test data and making it compatible to model**","3f18c601":"**Data info**","fdbbc288":"**Applying model on test data features**","1b002523":"There are no null values.\n\n**Getting dummies for categorical data**","446e4f4c":"**Re-applying reggression with updated features**","e4ed4af2":"**Loading training data**","8c7d000c":"**Applying logistic regression**","fc9e805e":"P-Values are fine now.\n\n**Creating a confusion matrix to see how well our model has incorporated the data**","020388fa":"**Declaring dependent and independent variables**","0195ab12":"**Data Description**","726aaf6b":"Looking at the data description, we can see there are some columns that can be dropped from the data. Each name is unique, better to drop it. Too many unique values for Ticket and Cabin, let's drop them too.\n\n**Dropping columns**","d6e0408d":"**Writing resulted dataframe in a csv file**","a47dd316":"P-Value is too high for Parch, Embarked_Q and Embarked_S. We will drop them all as they seem not to affect variability of our target much.\n\n**Updating feature list**","592887e8":"**Test data info**","285c5195":"There are no null values left.\n\n**Getting dummy variables for categorical data**","a5d04aa2":"Info tells that there are some null values for Age and Embarked. We will fill mean age for Age and will drop null values of Embarked as they are only two.\n\n**Taking care of missing data**"}}