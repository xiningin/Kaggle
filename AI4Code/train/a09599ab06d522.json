{"cell_type":{"f5836bc3":"code","d02f3e76":"code","ff55d9bd":"code","0ff19cf8":"code","2fa9817c":"code","f171eead":"code","8fb2bf2b":"code","c2f0d738":"code","54c0aba7":"code","70af2c50":"code","d9df19be":"code","30e9fb97":"code","8933dbfc":"code","4925f940":"code","8e4ef1f4":"code","80eb1ec6":"code","7c6a9a96":"code","1b3e5fd1":"code","b6a77f80":"code","2342c12a":"code","5b2e25bd":"code","01b63a12":"code","0ee99085":"code","70bc31fb":"code","873ce01c":"code","f6288b07":"code","6857afe7":"code","081c2a17":"code","ac781f53":"code","fdd064ae":"code","5c65e0ff":"code","648ff90f":"code","cd8253aa":"markdown","dd116993":"markdown","c62ac2b9":"markdown","59720072":"markdown","01c4771d":"markdown","26d8d8c0":"markdown","6b70891e":"markdown","b92101d9":"markdown","5d127a3c":"markdown","5aad97a6":"markdown","8a4bf8db":"markdown","d7a9abb4":"markdown","29d86bb5":"markdown","c77d8fb6":"markdown","2107e624":"markdown","7e39f34e":"markdown","1685a2c1":"markdown","08724840":"markdown","9273d44c":"markdown","896faf87":"markdown","17436f53":"markdown","a3a4afd8":"markdown"},"source":{"f5836bc3":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","d02f3e76":"#load the csv file and make the data frame\nbank_data=pd.read_csv('..\/input\/personal-loan-modeling\/Bank_Personal_Loan_Modelling.csv')","ff55d9bd":"#display the first 10 rows of data frame\nbank_data.head(10)","0ff19cf8":"# Shape of the Data\n\nbank_data.shape","2fa9817c":"# Information of the DataSet\n\nbank_data.info()","f171eead":"# Check null values are present or not\n\nbank_data.isnull().values.any()","8fb2bf2b":"# Plotting histogram \nbank_data.hist()","c2f0d738":"bank_data['Personal Loan'].value_counts()","54c0aba7":"print(\"Percentage of customer accept personal loan is {}%\".format((480\/5000)*100))\nprint(\"Percentage of customer not accept personal loan is {}%\".format((4520\/5000)*100))","70af2c50":"sns.distplot(bank_data['Personal Loan'],kde=False)\nplt.show()","d9df19be":"# Five Point Summary Of The DataSet\n\nbank_data.describe()","30e9fb97":"neg_val=bank_data[bank_data['Experience']<0]['Experience'].count()\nprint(\"Total number of customers whose experience is -ve are: \",neg_val)","8933dbfc":"bank_data['Experience'] = bank_data['Experience'].apply(lambda x : abs(x) if(x<0) else x)\nneg_value=bank_data[bank_data['Experience']<0].shape[0]\nprint(\"Now after manipulation the total number of customers whose experience is in negative are : \",neg_value)","4925f940":"#after manipulation display Five point summarry of the data set\n\nbank_data.describe()","8e4ef1f4":"bank_data.drop(['ID','ZIP Code'],axis=1,inplace=True)\nbank_data.head(5)","80eb1ec6":"sns.pairplot(data=bank_data,hue='Personal Loan')\nplt.show()","7c6a9a96":"x=bank_data.drop(['Personal Loan'],axis=1)\ny=bank_data['Personal Loan']","1b3e5fd1":"x.info()","b6a77f80":"from sklearn.model_selection import train_test_split","2342c12a":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30)","5b2e25bd":"print(\"The training feature are {} % of dataset and training labels are {} % of dataset\".format(((x_train.shape[0]\/5000)*100),((y_train.shape[0]\/5000)*100)))\nprint(\"The test feature are {} % of dataset and test labels are {} % of dataset\".format(((x_test.shape[0]\/5000)*100),((y_test.shape[0]\/5000)*100)))","01b63a12":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,confusion_matrix ","0ee99085":"bank_lr = LogisticRegression(solver='liblinear') #Instantiate the LogisticRegression object\nbank_lr.fit(x_train,y_train) #call the fit method of logistic regression to train the model or to learn the parameters of model\ny_predict_lr = bank_lr.predict(x_test) #predicting the result of test dataset and storing in a variable called y_predict_lr\nprint(\"Accuracy Score is : \",accuracy_score(y_test,y_predict_lr)*100)#printing overall accuracy score\nprint(\"Confusion matrix\")\ncm=confusion_matrix(y_test,y_predict_lr) ##creating confusion matrix\nprint(cm)#printing confusion matrix","70bc31fb":"df_cm=pd.DataFrame(cm,index=[i for i in [\"1\",\"0\"]],columns=[i for i in [\"Predict 1\",\"Predict 0\"]])\n# Heat map for confusion matrix\nsns.heatmap(df_cm,annot=True)","873ce01c":"from sklearn.neighbors import KNeighborsClassifier","f6288b07":"bank_knn = KNeighborsClassifier(n_neighbors=5) #Initialize the object\nbank_knn.fit(x_train,y_train)  #call the fit method of knn classifier to train the model\ny_predict_knn = bank_knn.predict(x_test) #predicting the result of test dataset and storing in a variable called knn_y_predict\nprint(\"Accuracy Score is : \",accuracy_score(y_test,y_predict_knn)*100) #printing overall accuracy score\nprint(\"Confusion matrix\")\ncm_knn=confusion_matrix(y_test,y_predict_knn) #creating confusion matrix\nprint(cm_knn) #printing confusion matrix","6857afe7":"df_cm_knn=pd.DataFrame(cm_knn,index=[i for i in [\"1\",\"0\"]],columns=[i for i in [\"Predict 1\",\"Predict 0\"]])","081c2a17":"# Heat map for confusion matrix\nsns.heatmap(df_cm_knn,annot=True)","ac781f53":"from sklearn.naive_bayes import GaussianNB","fdd064ae":"bank_nb = GaussianNB() #Initialize the object\nbank_nb.fit(x_train,y_train)  #call the fit method of gaussian naive bayes to train the model or to learn the parameters of model\ny_predict_nb = bank_nb.predict(x_test)  #predicting the result of test dataset and storing in a variable called nb_y_predict\nprint(\"Accuracy Score is : \",accuracy_score(y_test,y_predict_nb)*100)  #printing overall accuracy score\nprint(\"Confusion matrix\")\ncm_nb=confusion_matrix(y_test,y_predict_nb)  #creating confusion matrix\nprint(cm_nb)#printing creating matrix","5c65e0ff":"df_cm_nb=pd.DataFrame(cm_nb,index=[i for i in [\"1\",\"0\"]],columns=[i for i in [\"Predict 1\",\"Predict 0\"]])","648ff90f":"# Heat map for confusion matrix\nsns.heatmap(df_cm_nb,annot=True)","cd8253aa":"# 2.KNN","dd116993":"#There are 52 record are found that has -ve values.\n\n#We have to convert the negative values into Positive values.\n\n","c62ac2b9":"# Conclusion","59720072":"In above code we have split data in 'x' by dropping 'Personal Loan' to train our model and 'y' with 'Personal Loan' set that we will be predicting.","01c4771d":"**Data Description:**\nThe file Bank.xls contains data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign.\n\n**Domain: Banking**\nContext: This case is about a bank (Thera Bank) whose management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio with minimal budget.\n\n**Objective:**\nThe classification goal is to predict the likelihood of a liability customer buying personal loans.","26d8d8c0":"# 3.Naive Bayes'\n","6b70891e":"In shows that 'Experience' column has -ve values. So,we have to fix it.","b92101d9":"It shows that our 'x' is in correct format.","5d127a3c":"Pair plot shows that ,the customers with high income are more likely to accept Personal Loan.","5aad97a6":"It shows there are 5000 Rows and 14 Columns in the Data Set.","8a4bf8db":"Data Cleaning","d7a9abb4":"As we have seen in Five Point summary of the data set that 'Experience' column has -ve values.\n\nSo,we have to calculate the -ve values of that column by using count()","29d86bb5":"Pair Plot","c77d8fb6":"Here test size is 30% means out of 5000 rows of the data set 3500 rows are used to train the data set and 1500 rows are used to test the data set.","2107e624":"# Split Data into train and test set\n","7e39f34e":"There are no null values in the dataset.","1685a2c1":"It shows that only 480\/5000 customers accept personal loan and 4520\/5000 customers not accept personal loan.","08724840":"Now we will divide the data set into two parts.\nSmall part of data used for training the data.\nRemaining part is used to test the model and to check how accurate the training is done.","9273d44c":"#Now there are no negative values in the data set.\n\n#In this data set 'ID'and 'ZIP Code' has no relevance.So,we can drop it.","896faf87":"It shows that there are no null values in the dataframe and all columns are numeric type.","17436f53":"The aim of the universal bank is to convert there liability customers into loan customers. They want to set up a new marketing campaign; hence, they need information about the connection between the variables given in the data.Three algorithms were used in this study. It seems like Logistic Regression algorithm have the highest accuracy score and we can choose that as our final model.","a3a4afd8":"# 1.Logistic Regression"}}