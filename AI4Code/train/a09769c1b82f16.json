{"cell_type":{"8d27a367":"code","d7bfa9f7":"code","c27c15b0":"code","bde9342b":"code","d19f01be":"code","736dcb8b":"code","e9e0ffa0":"code","9d57c52b":"code","4610e6e2":"code","6ab751a3":"code","3ac87dd7":"code","48ea247c":"code","20aef8db":"code","d43f01a4":"code","89be34e9":"code","a48aef64":"code","3d8de1dd":"code","0682383a":"code","31a788d0":"code","4341ce62":"code","5d70a4f2":"code","cb7405e2":"code","530134c4":"code","c3d1a7e6":"code","9e3b21b1":"code","399bc3d4":"code","9d46947c":"code","5e4acf10":"code","53e70cf4":"markdown","8ad22d0d":"markdown","fb78b9a7":"markdown","f5dfbbdb":"markdown","01b2d065":"markdown","0f00bddc":"markdown","52e55ba7":"markdown","1ab9cd58":"markdown","b9e0dcbe":"markdown","8f2430f0":"markdown","e1ada86c":"markdown","ac5f58c1":"markdown","3f90c399":"markdown","67b67f68":"markdown","827b82bc":"markdown","b1b86490":"markdown","ee88c29a":"markdown","47fa7ad8":"markdown","bfa1bd6e":"markdown","28eaa3fe":"markdown","1c7d0cc0":"markdown","c3759675":"markdown","2349f8d5":"markdown","fa728688":"markdown","b95e39db":"markdown","02771031":"markdown","5bdbe7c2":"markdown","70808736":"markdown","9e908ae5":"markdown","d8a88a76":"markdown","07073e91":"markdown","c0ded510":"markdown"},"source":{"8d27a367":"# import packages\nimport os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\n# setting up options\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n# import datasets\ntrain_df = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\n\n# converting column without decimal to integer\nfor col in train_df.columns:\n    if np.sum((train_df[col] - train_df[col].astype('int'))) == 0:\n        train_df[col] = train_df[col].astype('int')\n        \nfor col in test_df.columns:\n    if np.sum((test_df[col] - test_df[col].astype('int'))) == 0:\n        test_df[col] = test_df[col].astype('int')","d7bfa9f7":"train_df.head()","c27c15b0":"print(f'Number of rows: {train_df.shape[0]};  Number of columns: {train_df.shape[1]}; No of missing values: {sum(train_df.isna().sum())}')","bde9342b":"train_df.dtypes","d19f01be":"train_df.describe()","736dcb8b":"test_df.head()","e9e0ffa0":"print(f'Number of rows: {test_df.shape[0]};  Number of columns: {test_df.shape[1]}; No of missing values: {sum(test_df.isna().sum())}')","9d57c52b":"test_df.dtypes","4610e6e2":"submission.head()","6ab751a3":"integer_features = ['f1', 'f16', 'f27', 'f55', 'f60', 'f86']\nunique_values_train = pd.DataFrame(train_df[integer_features].nunique())\nunique_values_train = unique_values_train.reset_index(drop=False)\nunique_values_train.columns = ['Features', 'Count']\n\nunique_values_percent_train = pd.DataFrame(train_df[integer_features].nunique()\/train_df.shape[0])\nunique_values_percent_train = unique_values_percent_train.reset_index(drop=False)\nunique_values_percent_train.columns = ['Features', 'Count']\n\nunique_values_test = pd.DataFrame(test_df[integer_features].nunique())\nunique_values_test = unique_values_test.reset_index(drop=False)\nunique_values_test.columns = ['Features', 'Count']\n\nunique_values_percent_test = pd.DataFrame(test_df[integer_features].nunique()\/test_df.shape[0])\nunique_values_percent_test = unique_values_percent_test.reset_index(drop=False)\nunique_values_percent_test.columns = ['Features', 'Count']","3ac87dd7":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(6, 4), facecolor='#f6f5f5')\ngs = fig.add_gridspec(2, 2)\ngs.update(wspace=0.4, hspace=0.5)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ffd514']*6)\n\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_color)\nax0_sns = sns.barplot(ax=ax0, y=unique_values_train['Features'], x=unique_values_train['Count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"Unique Values\",fontsize=4, weight='bold')\nax0_sns.set_ylabel(\"Features\",fontsize=4, weight='bold')\nax0_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0.text(0, -1.5, 'Unique Values - Train Dataset', fontsize=6, ha='left', va='top', weight='bold')\nax0.text(0, -1, 'f1 and f86 can be considered as classification features', fontsize=4, ha='left', va='top')\nax0.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():,.0f}'\n    x = p.get_x() + p.get_width() + 6000\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='left', va='center', fontsize=4, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n    \nax1 = fig.add_subplot(gs[0, 1])\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1.set_facecolor(background_color)\nax1_sns = sns.barplot(ax=ax1, y=unique_values_percent_train['Features'], x=unique_values_percent_train['Count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax1_sns.set_xlabel(\"Percentage Unique Values\",fontsize=4, weight='bold')\nax1_sns.set_ylabel(\"Features\",fontsize=4, weight='bold')\nax1_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax1_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1.text(0, -1.5, 'Percentage Unique Values - Train Dataset', fontsize=6, ha='left', va='top', weight='bold')\nax1.text(0, -1, 'f1 and f86 can be considered as classification features', fontsize=4, ha='left', va='top')\n# data label\nfor p in ax1.patches:\n    value = f'{p.get_width():.2f}'\n    x = p.get_x() + p.get_width() + 0.03\n    y = p.get_y() + p.get_height() \/ 2 \n    ax1.text(x, y, value, ha='left', va='center', fontsize=4, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ff355d']*6)\n    \nax3 = fig.add_subplot(gs[1, 0])\nfor s in [\"right\", \"top\"]:\n    ax3.spines[s].set_visible(False)\nax3.set_facecolor(background_color)\nax3_sns = sns.barplot(ax=ax3, y=unique_values_test['Features'], x=unique_values_test['Count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax3_sns.set_xlabel(\"Unique Values\",fontsize=4, weight='bold')\nax3_sns.set_ylabel(\"Features\",fontsize=4, weight='bold')\nax3_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax3_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax3_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax3.text(0, -1.5, 'Unique Values - Test Dataset', fontsize=6, ha='left', va='top', weight='bold')\nax3.text(0, -1, 'Test dataset is quite similar with train dataset', fontsize=4, ha='left', va='top')\nax3.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n# data label\nfor p in ax3.patches:\n    value = f'{p.get_width():,.0f}'\n    x = p.get_x() + p.get_width() + 6000\n    y = p.get_y() + p.get_height() \/ 2 \n    ax3.text(x, y, value, ha='left', va='center', fontsize=4, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n    \nax4 = fig.add_subplot(gs[1, 1])\nfor s in [\"right\", \"top\"]:\n    ax4.spines[s].set_visible(False)\nax4.set_facecolor(background_color)\nax4_sns = sns.barplot(ax=ax4, y=unique_values_percent_test['Features'], x=unique_values_percent_test['Count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax4_sns.set_xlabel(\"Percentage Unique Values\",fontsize=4, weight='bold')\nax4_sns.set_ylabel(\"Features\",fontsize=4, weight='bold')\nax4_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax4_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax4_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax4.text(0, -1.5, 'Percentage Unique Values - Test Dataset', fontsize=6, ha='left', va='top', weight='bold')\nax4.text(0, -1, 'Test dataset is quite similar with train dataset', fontsize=4, ha='left', va='top')\n# data label\nfor p in ax4.patches:\n    value = f'{p.get_width():.2f}'\n    x = p.get_x() + p.get_width() + 0.03\n    y = p.get_y() + p.get_height() \/ 2 \n    ax4.text(x, y, value, ha='left', va='center', fontsize=4, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))","48ea247c":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.2, 1.4, 'Features Distribution', fontsize=10, fontweight='bold')\nax0.text(-0.2, 1.3, 'f0-f24', fontsize=6, fontweight='light')        \n\nfeatures = list(train_df.columns[1:26])\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#ffd514')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#ff355d')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","20aef8db":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-4, 0.165, 'Features Distribution ', fontsize=10, fontweight='bold')\nax0.text(-4, 0.15, 'f25-f49', fontsize=6, fontweight='light')\n\nfeatures = list(train_df.columns[26:51])\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#ffd514')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#ff355d')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","d43f01a4":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-1, 1.8, 'Features Distribution ', fontsize=10, fontweight='bold')\nax0.text(-1, 1.65, 'f50-f74', fontsize=6, fontweight='light')\n\nfeatures = list(train_df.columns[51:76])\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#ffd514')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#ff355d')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","89be34e9":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-15000, 0.00033, 'Features Distribution ', fontsize=10, fontweight='bold')\nax0.text(-15000, 0.00030, 'f75-f99', fontsize=6, fontweight='light')\n\nfeatures = list(train_df.columns[76:101])\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#ffd514')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#ff355d')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","a48aef64":"unique_values_target = pd.DataFrame(train_df['loss'].value_counts())\nunique_values_target = unique_values_target.reset_index(drop=False)\nunique_values_target.columns = ['Value', 'Count']\n\nunique_values_percentage_target = pd.DataFrame(train_df['loss'].value_counts()\/train_df.shape[0] * 100)\nunique_values_percentage_target = unique_values_percentage_target.reset_index(drop=False)\nunique_values_percentage_target.columns = ['Value', 'Count']\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(6, 4), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.4, hspace=0.5)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ffd514']*43)\n\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_color)\nax0_sns = sns.barplot(ax=ax0, y=unique_values_target['Value'], x=unique_values_target['Count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"No of Occurances\",fontsize=4, weight='bold')\nax0_sns.set_ylabel(\"Unique Value\",fontsize=4, weight='bold')\nax0_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0.text(0, -2.8, 'Target Variable', fontsize=6, ha='left', va='top', weight='bold')\nax0.text(0, -1.5, 'Zero is dominating the target variable with 60,144 occurances', fontsize=4, ha='left', va='top')\nax0.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():,.0f}'\n    x = p.get_x() + p.get_width() + 800\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='left', va='center', fontsize=2, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n    \nsns.set_palette(['#ff355d']*43)\n\nax0 = fig.add_subplot(gs[0, 1])\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_color)\nax0_sns = sns.barplot(ax=ax0, y=unique_values_percentage_target['Value'], x=unique_values_percentage_target['Count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"Percentage\",fontsize=4, weight='bold')\nax0_sns.set_ylabel(\"Unique Value\",fontsize=4, weight='bold')\nax0_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0.text(0, -2.8, 'Percentage Target Variable', fontsize=6, ha='left', va='top', weight='bold')\nax0.text(0, -1.5, 'Zero is dominating the target variable with 24.1% occurances', fontsize=4, ha='left', va='top')\nax0.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():,.1f}'\n    x = p.get_x() + p.get_width() + 0.5\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='left', va='center', fontsize=2, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n","3d8de1dd":"features = list(train_df.columns[1:26])\n\nplt.rcParams['figure.dpi'] = 400\nfig = plt.figure(figsize=(10,160), facecolor='#f6f5f5')\ngs = fig.add_gridspec(100, 1)\ngs.update(wspace=0, hspace=0.5)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#fcd12a']*43)\n\nrun_no = 0\nfor row in range(0, 25):\n    for col in range(0, 1):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.violinplot(ax=locals()[\"ax\"+str(run_no)], y=train_df[col], x=train_df['loss'], \n                     saturation=1, linewidth=0.3, zorder=1, inner='quartile')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.7)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.7)\n    locals()[\"ax\"+str(run_no)].set_axisbelow(True) \n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize = 8, fontdict=dict(weight='bold'))\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=5, width=0.5, length=1.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n    \nax0.text(0, 1.8, 'Features Distribution', fontsize=10, fontweight='bold')\nax0.text(0, 1.5, 'Showing features f0 - f24 distribution on train dataset relative to target variable values', fontsize=8)\n\nplt.show()","0682383a":"features = list(train_df.columns[26:51])\n\nplt.rcParams['figure.dpi'] = 400\nfig = plt.figure(figsize=(10,160), facecolor='#f6f5f5')\ngs = fig.add_gridspec(100, 1)\ngs.update(wspace=0, hspace=0.5)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#fcd12a']*43)\n\nrun_no = 0\nfor row in range(0, 25):\n    for col in range(0, 1):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.violinplot(ax=locals()[\"ax\"+str(run_no)], y=train_df[col], x=train_df['loss'], \n                     saturation=1, linewidth=0.3, zorder=1, inner='quartile')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.7)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.7)\n    locals()[\"ax\"+str(run_no)].set_axisbelow(True) \n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize = 8, fontdict=dict(weight='bold'))\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=5, width=0.5, length=1.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n    \nax0.text(-0.5, 32, 'Features Distribution', fontsize=10, fontweight='bold')\nax0.text(-0.5, 27, 'Showing features f25 - f49 distribution on train dataset relative to target variable values', fontsize=8)\n\nplt.show()","31a788d0":"features = list(train_df.columns[51:76])\n\nplt.rcParams['figure.dpi'] = 400\nfig = plt.figure(figsize=(10,160), facecolor='#f6f5f5')\ngs = fig.add_gridspec(100, 1)\ngs.update(wspace=0, hspace=0.5)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#fcd12a']*43)\n\nrun_no = 0\nfor row in range(0, 25):\n    for col in range(0, 1):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.violinplot(ax=locals()[\"ax\"+str(run_no)], y=train_df[col], x=train_df['loss'], \n                     saturation=1, linewidth=0.3, zorder=1, inner='quartile')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.7)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.7)\n    locals()[\"ax\"+str(run_no)].set_axisbelow(True) \n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize = 8, fontdict=dict(weight='bold'))\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=5, width=0.5, length=1.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n    \nax0.text(-0.6, 9, 'Features Distribution', fontsize=10, fontweight='bold')\nax0.text(-0.6, 7.8, 'Showing features f50 - f74 distribution on train dataset relative to target variable values', fontsize=8)\n\nplt.show()","4341ce62":"features = list(train_df.columns[76:101])\n\nplt.rcParams['figure.dpi'] = 400\nfig = plt.figure(figsize=(10,160), facecolor='#f6f5f5')\ngs = fig.add_gridspec(100, 1)\ngs.update(wspace=0, hspace=0.5)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#fcd12a']*43)\n\nrun_no = 0\nfor row in range(0, 25):\n    for col in range(0, 1):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.violinplot(ax=locals()[\"ax\"+str(run_no)], y=train_df[col], x=train_df['loss'], \n                     saturation=1, linewidth=0.3, zorder=1, inner='quartile')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.7)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.7)\n    locals()[\"ax\"+str(run_no)].set_axisbelow(True) \n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize = 8, fontdict=dict(weight='bold'))\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=5, width=0.5, length=1.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n    \nax0.text(-0.6, 67000, 'Features Distribution', fontsize=10, fontweight='bold')\nax0.text(-0.6, 57000, 'Showing features f75 - f99 distribution on train dataset relative to target variable values', fontsize=8)\n\nplt.show()","5d70a4f2":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor, XGBClassifier \nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom lightgbm import LGBMRegressor, LGBMClassifier\n\nfolds = 5\nfeatures = list(train_df.columns[1:101])","cb7405e2":"train_oof = np.zeros((250000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['loss'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['loss']\n    y_valid = X_valid['loss']\n    X_train = X_train.drop('loss', axis=1)\n    X_valid = X_valid.drop('loss', axis=1)\n    \n    model = LinearRegression()\n\n    model =  model.fit(X_train, y_train)\n    temp_oof = model.predict(X_valid)\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} RMSE: ', mean_squared_error(y_valid, temp_oof, squared=False))\n    \nprint(f'OOF Accuracy: ', mean_squared_error(train_df['loss'], train_oof, squared=False))","530134c4":"train_oof = np.zeros((250000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['loss'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['loss']\n    y_valid = X_valid['loss']\n    X_train = X_train.drop('loss', axis=1)\n    X_valid = X_valid.drop('loss', axis=1)\n    \n    model = XGBRegressor(random_state=42, tree_method='gpu_hist')\n\n    model =  model.fit(X_train, y_train, verbose=False)\n    temp_oof = model.predict(X_valid)\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} RMSE: ', mean_squared_error(y_valid, temp_oof, squared=False))\n    \nprint(f'OOF Accuracy: ', mean_squared_error(train_df['loss'], train_oof, squared=False))","c3d1a7e6":"train_oof = np.zeros((250000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['loss'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['loss']\n    y_valid = X_valid['loss']\n    X_train = X_train.drop('loss', axis=1)\n    X_valid = X_valid.drop('loss', axis=1)\n    \n    model = LGBMRegressor(random_state=42)\n\n    model =  model.fit(X_train, y_train, verbose=False)\n    temp_oof = model.predict(X_valid)\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} RMSE: ', mean_squared_error(y_valid, temp_oof, squared=False))\n    \nprint(f'OOF Accuracy: ', mean_squared_error(train_df['loss'], train_oof, squared=False))","9e3b21b1":"train_oof = np.zeros((250000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['loss'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['loss']\n    y_valid = X_valid['loss']\n    X_train = X_train.drop('loss', axis=1)\n    X_valid = X_valid.drop('loss', axis=1)\n    \n    model = CatBoostRegressor(random_state=42)\n\n    model =  model.fit(X_train, y_train, verbose=False)\n    temp_oof = model.predict(X_valid)\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} RMSE: ', mean_squared_error(y_valid, temp_oof, squared=False))\n    \nprint(f'OOF Accuracy: ', mean_squared_error(train_df['loss'], train_oof, squared=False))","399bc3d4":"train_oof = np.zeros((250000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['loss'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['loss']\n    y_valid = X_valid['loss']\n    X_train = X_train.drop('loss', axis=1)\n    X_valid = X_valid.drop('loss', axis=1)\n\n    model = XGBClassifier(random_state=42, tree_method='gpu_hist', \n                          verbosity=0, use_label_encoder=False)\n\n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = pd.DataFrame(model.predict(X_valid))[0]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} RMSE: ', mean_squared_error(y_valid, temp_oof, squared=False))\n    \nprint(f'OOF Accuracy: ', mean_squared_error(train_df['loss'], train_oof, squared=False))","9d46947c":"train_oof = np.zeros((250000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['loss'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['loss']\n    y_valid = X_valid['loss']\n    X_train = X_train.drop('loss', axis=1)\n    X_valid = X_valid.drop('loss', axis=1)\n\n    model = LGBMClassifier(\n        random_state=42\n    )\n\n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict(X_valid)\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} RMSE: ', mean_squared_error(y_valid, temp_oof, squared=False))\n    \nprint(f'OOF Accuracy: ', mean_squared_error(train_df['loss'], train_oof, squared=False))","5e4acf10":"train_oof = np.zeros((250000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['loss'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['loss']\n    y_valid = X_valid['loss']\n    X_train = X_train.drop('loss', axis=1)\n    X_valid = X_valid.drop('loss', axis=1)\n\n    model = CatBoostClassifier(\n        random_state=42,\n        task_type=\"GPU\"\n    )\n\n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = pd.DataFrame(model.predict(X_valid))[0]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} RMSE: ', mean_squared_error(y_valid, temp_oof, squared=False))\n    \nprint(f'OOF Accuracy: ', mean_squared_error(train_df['loss'], train_oof, squared=False))","53e70cf4":"### 4.1.2 Individual features\n\nCount how many unique values in each features and perform differences calculation on train and test datasets to see how both datasets differ from one to another.\n\n**Observations:**\n- It seems `f1` and `f86` can be treated as classification features as the unique numbers is small compared with the total observation which can be seen on the percentage to the total observations.","8ad22d0d":"[back to top](#table-of-contents)\n<a id=\"3.3\"><\/a>\n## 3.3 Submission\nThe submission file is expected to have an `id` and `loss` columns.\n\nBelow is the first 5 rows of submission file:","fb78b9a7":"[back to top](#table-of-contents)\n<a id=\"6.4\"><\/a>\n### 6.1.4 Catboost Regressor","f5dfbbdb":"### 5.2.2 Features f25 - f49","01b2d065":"[back to top](#table-of-contents)\n<a id=\"6.2.1\"><\/a>\n### 6.2.1 XGBoost Classifier","0f00bddc":"[back to top](#table-of-contents)\n<a id=\"3.2\"><\/a>\n## 3.2 Test dataset\nTest dataset is used to make a prediction based on the model that has previously trained. Exploration in this dataset is also needed to see how the data is structured and especially on it\u2019s similiarity with the train dataset.\n\n**Observations:**\n\nFeatures column in `test` dataset are similar with `train` with details as follow:\n- There are `100` features which start from `f0` to `f99`.\n- `test` dataset contain of `150,000` observations without any missing values with total of `101` columns.\n- Only features `id`, `f1`, `f16`, `f27`, `f55`, `f60` and `f86` are in `int64` type, other features are in `float64`.\n\n### 3.2.1 Quick view\nBelow is the first 5 rows of test dataset:","52e55ba7":"[back to top](#table-of-contents)\n<a id=\"6.2.2\"><\/a>\n### 6.2.2 LGBM Classifier","1ab9cd58":"[back to top](#table-of-contents)\n<a id=\"1\"><\/a>\n# 1 Introduction\n\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, Kaggle have launched many Playground competitions that are more approachable than Featured competition, and thus more beginner-friendly.\n\nThe goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with calculating the loss associated with a loan defaults. Although the features are anonymized, they have properties relating to real-world features.\n\nSubmissions are evaluated using Submissions are scored on the root mean squared error. RMSE is defined as:\n\n$$ \\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}} $$\n\nwhere $\\hat{y_{i}}$ is the predicted value, $y$ is the ground truth value, and $n$ is the number of rows in the test data.","b9e0dcbe":"### 5.2.1 Features f0 - f24","8f2430f0":"[back to top](#table-of-contents)\n<a id=\"6.1.3\"><\/a>\n### 6.1.3 LGBM Regressor","e1ada86c":"[back to top](#table-of-contents)\n<a id=\"3\"><\/a>\n# 3 Dataset Overview\nThe intend of the overview is to get a feel of the data and its structure in train, test and submission file. An overview on train and test datasets will include a quick analysis on missing values and basic statistics, while sample submission will be loaded to see the expected submission.\n\n<a id=\"3.1\"><\/a>\n## 3.1 Train dataset\nAs stated before, train dataset is mainly used to train predictive model as there is an available target variable in this set. This dataset is also used to explore more on the data itself including find a relation between each predictors and the target variable.\n\n**Observations:**\n- **target**\n    - `loss` column is the target variable which is only available in the `train` dataset.\n    - The interesting part with the `loss` column is in `int64` type. **Is this a classification task with a regression evaluation metrics?**\n- **features**\n    - There are `100` features which start from `f0` to `f99`.\n    - `train` dataset contain of `250,000` observations without any missing values with total of `102` columns.\n    - Only features `id`, `f1`, `f16`, `f27`, `f55`, `f60`, and `f86` are in `int64` type, other features are in `float64`.\n    - `f31`, `f36`, `f46`, `f78` mean are quite close with target variable mean.\n\n### 3.1.1 Quick view\nBelow is the first 5 rows of train dataset:","ac5f58c1":"The dimension and number of missing values in the train dataset is as below:","3f90c399":"[back to top](#table-of-contents)\n<a id=\"6.1.2\"><\/a>\n### 6.1.2 XGBoost Regressor","67b67f68":"<a id=\"5.2\"><\/a>\n## 5.2 Features Distribution by Target\nThis section try to see if there is any distinct feature distribution if it is classified by target variable. The distribution will be shown using violinplot.\n\n**Observations:**\n- All features that fall into value `0` to `10` (which cover 75.5% of target variables) have a relative similar distribution.\n- Most features changes their distribution at between value `39` to `42` of target variables.\n- Value `0` in features `f59`, `f77`, `81`, `86` and `93` distribution tails are quite different from other values.","827b82bc":"[back to top](#table-of-contents)\n<a id=\"6\"><\/a>\n# 6 Base Model\nEvaluate the performance of base model which will be divided into 2 parts: regression and classification models. Models will be evaluated using five cross validation without any hyperparameters tuning. *(to see the packages used, please expand)*\n\n**Observations:**\n- `Classification` models perform worse compare to `Regression` models.\n- `Catboost` in `regression` and `classification` beat other models performance.\n- The best model in `classification` can not beat the best model in the `regression`:\n    - As the evaluation metric is using RMSE, `classification` model may need to have a more correct prediction than `regression` model. For example: `classification` model can not have a decimal value assuming it predicts `11` while the actual is `12` there will be a gap of `1`, while the `regression` model may have a prediction of `11.5` and have a lower gap due to decimal places.\n    - The best model in regression model is `Catboost Regressor` with OOF RMSE of `7.85`.\n    - Once again the best model in classification is `Catboost Classifier` with OOF RMSE of `10.24`.\n","b1b86490":"[back to top](#table-of-contents)\n<a id=\"4\"><\/a>\n# 4 Features\nNumber of features available to be used to create a prediction model are `100`. The analysis is started by looking on number of uniques value on integer features which are `f1`, `f16`, `f27`, `f55`, `f60` and `f86`.\n\n<a id=\"4.1\"><\/a>\n## 4.1 Unique values\nCounting number of unique value and it's relative with their respective observations between train & test dataset.\n\n## 4.1.1 Preparation\nPrepare train and test dataset for data analysis and visualization. *(to see the details, please expand)*","ee88c29a":"[back to top](#table-of-contents)\n<a id=\"6.1.1\"><\/a>\n## 6.1.1 Linear Regression","47fa7ad8":"### 4.2.4 Features f75 - f99","bfa1bd6e":"### 4.2.2 Features f25 - f49","28eaa3fe":"[back to top](#table-of-contents)\n<a id=\"6.2\"><\/a>\n## 6.2 Classification\nThis will be counterintuitive as it will assume the target is a classification. Models that will be used are `XGBoost Classifier`, `LGBM Classifier` and `Catboost lassifier`.\n\n**Observations:**\n- `Catboost Classifier` performs the best compared to another model with OOF RMSE of `10.24`.\n- `XGBoost Classifier` is in the second place with OOF RMSE of `10.25`.\n- `LGBM Classifier` is in the last place with OOF RMSE of `12.62`.","1c7d0cc0":"[back to top](#table-of-contents)\n<a id=\"6.1\"><\/a>\n## 6.1 Regression\nModels that will be evaluated are `Linear Regression`, `XGBoost Regressor`, `LGBM Regressor` and `Catboost Regressor`.\n\n**Observations:**\n- `Catboost Regressor` performs the best compared to another model with OOF RMSE of `7.85`.\n- `LGBM Regressor` is in the second place with OOF RMSE of `7.86`.\n- `Linear Regression` is beating the performance of XGBoost with OOF RMSE of `7.89`.\n- `XGBoost Regressor` is in the last place with OOF RMSE of `7.93`.","c3759675":"### 3.1.3 Basic statistics\nBelow is the basic statistics for each variables which contain information on `count`, `mean`, `standard deviation`, `minimum`, `1st quartile`, `median`, `3rd quartile` and `maximum`.","2349f8d5":"### 5.2.4 Features f75 - f99","fa728688":"[back to top](#table-of-contents)\n<a id=\"5\"><\/a>\n# 5 Target\nThough the metrics that is used to evaluate the model is `RMSE`, it's tempting to see the target variables in a classification point of view. It may be possible to use multi-classification model to make the prediction, though the metrics used is `RMSE`.\n\n<a id=\"5.1\"><\/a>\n## 5.1 Distribution\nTarget variable has a range of `0` to `42` with total of `43` values. Let's see the individual numbers occurances from the `train` dataset.\n\n**Observations:**\n- `0` has the highest occurances in the train dataset which reach `24%`.\n- `0` to `10` combined has an occurances of `75.5%`, meaning most of the `loss` is `<=10`. \n- Above `12` the percentage occurances are below `2%`.","b95e39db":"### 5.2.3 Features f50 - f74","02771031":"[back to top](#table-of-contents)\n<a id=\"2\"><\/a>\n# 2 Preparations\nPreparing packages and data that will be used in the analysis process. Packages that will be loaded are mainly for data manipulation, data visualization and modeling. There are 2 datasets that are used in the analysis, they are train and test dataset. The main use of train dataset is to train models and use it to predict test dataset. While sample submission file is used to informed participants on the expected submission for the competition. *(to see the details, please expand)*","5bdbe7c2":"### 3.1.2 Data types\nExcept for column `id`, `f1`, `f16`, `f27`, `f55`, `f60`, `f86` and `loss` column which are in `int64` type, other columns are in `float64`. *(to see the details, please expand)*","70808736":"# Table of Contents\n<a id=\"table-of-contents\"><\/a>\n- [1 Introduction](#1)\n- [2 Preparations](#2)\n- [3 Datasets Overview](#3)\n    - [3.1 Train dataset](#3.1)\n    - [3.2 Test dataset](#3.2)\n    - [3.3 Submission](#3.3)\n- [4 Features](#4)\n    - [4.1 Unique values](#4.1)\n    - [4.2 Distribution](#4.2)\n- [5 Target](#5)\n    - [5.1 Distribution](#5.1)\n    - [5.2 Features Distribution by Target](#5.2)\n- [6 Base Model](#6)\n    - [6.1 Regression](#6.1)\n        - [6.1.1 Linear Regression](#6.1.1)\n        - [6.1.2 XGBoost Regressor](#6.1.2)\n        - [6.1.3 LGBM Regressor](#6.1.3)\n        - [6.1.4 Catboost Regressor](#6.1.4)\n    - [6.2 Classification](#6.2)\n        - [6.2.1 XGBoost Classifier](#6.2.1)\n        - [6.2.2 LGBM Classifier](#6.2.2)\n        - [6.2.3 Catboost Classifier](#6.2.3)","9e908ae5":"[back to top](#table-of-contents)\n<a id=\"6.2.3\"><\/a>\n### 6.2.3 Catboost Classifier","d8a88a76":"[back to top](#table-of-contents)\n<a id=\"4.2\"><\/a>\n## 4.2 Distribution\nShowing distribution on each feature that are available in train and test dataset. As there are 100 features, it will be broken down into 25 features for each sections. `Yellow` represents train dataset while `pink` will represent test dataset\n\n**Observations:**\n- All features distribution on train and test dataset are almost similar.\n\n### 4.2.1 Features f0 - f24","07073e91":"### 3.2.2 Data types\nExcept for column `id`, `f1`, `f16`, `f27`, `f55`, `f60`, `f86` and `loss` column which are in `int64` type, other columns are in `float64` which is consistent with the train dataset. *(to see the details, please expand)*","c0ded510":"### 4.2.3 Features f50 - f74"}}