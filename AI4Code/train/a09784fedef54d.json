{"cell_type":{"d5221c14":"code","f2670a0a":"code","3cb6da41":"code","0ecd97de":"code","497283cc":"code","0b46b063":"code","90a04e8f":"code","e1d7dcc0":"code","3c8c713e":"code","1807bfa3":"code","01634da9":"code","c40117f9":"code","6f09d08d":"code","432c33d9":"code","936c624a":"code","bcbcdf66":"code","22e0df34":"code","0a440996":"code","1a18907d":"code","f28e98f0":"code","146f8e17":"code","1802957b":"code","998f94a8":"code","2818ff8e":"code","82b64f62":"code","e38b0c71":"code","fcf56d04":"code","a9150ade":"code","5f70586b":"code","de2462e4":"code","d62a9d76":"code","13ff2021":"code","100edbb9":"code","ddac98ee":"markdown","0bed4323":"markdown","17329b1f":"markdown","800d4a94":"markdown","4dbe5eaa":"markdown","0c4a4075":"markdown","b1a6b7e6":"markdown","1e952043":"markdown","305dfc1f":"markdown","38aa03a2":"markdown","16be4a42":"markdown","cf20dc2a":"markdown","8673adcf":"markdown","e5552a4c":"markdown","4fea138e":"markdown","b0ad49f6":"markdown","d6e3a584":"markdown","38932ffc":"markdown","965097f1":"markdown","319e430c":"markdown","bb7a087e":"markdown","29a5a0b6":"markdown","4d9bef12":"markdown","d9169530":"markdown","4890f750":"markdown","dc46fa49":"markdown","19654371":"markdown","6b79319d":"markdown","ac6f189b":"markdown","e4b86566":"markdown","82534c5e":"markdown","0efd9063":"markdown"},"source":{"d5221c14":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.style.use('seaborn-darkgrid')\nsns.set_theme(style=\"darkgrid\")","f2670a0a":"# loading the dataset\ndata = pd.read_csv('..\/input\/titanic\/train.csv')\ndata.head(3)","3cb6da41":"data.info()","0ecd97de":"plt.subplots(3,1,figsize=(15,6))\nplt.subplot(131)\ndata.Survived.value_counts().plot(kind='pie',\n                                  title='Percentage of survival',\n                                  autopct='%1.1f%%', shadow=False, startangle=0)\n\nplt.subplot(132)\ndata.Sex.value_counts().plot(kind='pie',\n                                  title='Percentage of genders',\n                                  autopct='%1.1f%%', shadow=False, startangle=0)\n\nplt.subplot(133)\nsns.countplot(x='Sex', hue='Pclass', data=data)","497283cc":"grouped_sex = data.groupby('Sex')['Survived'].value_counts(normalize=True).mul(100).rename('percent').reset_index()#\nsns.catplot(x='Sex',y='percent',hue='Survived',kind='bar',data=grouped_sex)\nplt.title('survived by sex - normalized')","0b46b063":"grouped_class = data.groupby('Pclass')['Survived'].value_counts(normalize=True).mul(100).rename('percent').reset_index()#\nsns.catplot(x='Pclass',y='percent',hue='Survived',kind='bar',data=grouped_class)\nplt.title('survived by class - normalized')\n#grouped_class","90a04e8f":"sns.factorplot(x = 'Pclass', y='Survived',hue='Sex',data=data)","e1d7dcc0":"grouped_embarked = data.groupby('Embarked')['Survived'].value_counts(normalize=True).mul(100).rename('percent').reset_index()#\nsns.catplot(x='Embarked',y='percent',hue='Survived',kind='bar',data=grouped_embarked)\nplt.title('Embarked by class - normalized')\n#grouped_embarked","3c8c713e":"grouped_embarked_sex = data.groupby('Embarked')['Sex'].value_counts(normalize=True).mul(100).rename('percent').reset_index()#\nsns.catplot(x='Embarked',y='percent',hue='Sex',kind='bar',data=grouped_embarked_sex)\nplt.title('Embarked by sex - normalized')\n#grouped_embarked_sex","1807bfa3":"grouped_embarked_Pclass = data.groupby('Embarked')['Pclass'].value_counts(normalize=True).mul(100).rename('percent').reset_index()#\nsns.catplot(x='Embarked',y='percent',hue='Pclass',kind='bar',data=grouped_embarked_Pclass)\nplt.title('Embarked by Pclass - normalized')\n#grouped_embarked_Pclass","01634da9":"# Binning using pd.cut()\n#data.Age.max(), data.Age.mean(), data.Age.min()\ndata['age_range'] = 0\ndata.head(3)\n# 0 == baby, 1 == child, 2== young adult, 3== adul, 4 == eldery\ndata['age_range'] = pd.cut(data.Age,bins=[0,2,16,35,55,100],labels=[0, 1, 2, 3,4])\n#\nplt.subplots(figsize=(24,10))\nplt.subplot(241)\ndata[data['Survived']==0].Age.plot.hist(bins=40)\ndata[data['Survived']==1].Age.plot.hist(bins=40, alpha=0.6)\nplt.subplot(242)\ndata[data['Survived']==0].Age.plot.hist(bins=5)\ndata[data['Survived']==1].Age.plot.hist(bins=5, alpha=0.6)\nplt.subplot(243)\ndata[data['Survived']==0].Age.plot.hist(bins=[0,2,16,35,55,100])\ndata[data['Survived']==1].Age.plot.hist(bins=[0,2,16,35,55,100], alpha=0.6)\nplt.subplot(244)\ndata.age_range.value_counts().plot(kind='pie')","c40117f9":"data.Fare.max(), data.Fare.mean(), data.Fare.min()\n# now this separates in 4 different equally separated groups\nng_fare = 4\ndata['fare_range']=pd.qcut(data['Fare'],ng_fare, labels=list(range(0, ng_fare)))\nsns.catplot(x= 'fare_range', hue='Survived', col='Sex', kind='count', data=data);","6f09d08d":"plt.subplots(2,1,figsize=(20,8))\n\nplt.subplot(231)\ndata.SibSp.value_counts().plot(kind='pie')\n\nplt.subplot(232)\nsns.countplot(x='SibSp', hue='Survived', data=data)\n\nplt.subplot(233)\nsns.countplot(x='SibSp', hue='Pclass', data=data)\n\nplt.subplot(234)\ndata.Parch.value_counts().plot(kind='pie')\n\nplt.subplot(235)\nsns.countplot(x='Parch', hue='Survived', data=data)\n\nplt.subplot(236)\nsns.countplot(x='Parch', hue='Pclass', data=data)","432c33d9":"#sns.catplot(x='SibSp',  col='Pclass', kind='count', data=data)\n#sns.catplot(x='Parch',  col='Pclass', kind='count', data=data)","936c624a":"plt.subplots(2,1,figsize=(12,5))\nplt.subplot(121)\nsns.barplot(x = 'SibSp',y ='Survived',data=data) # admits ,hue='Sex' !!!\nplt.subplot(122)\nsns.barplot(x = 'Parch',y ='Survived',data=data)","bcbcdf66":"data[\"Family\"] = 0\ndata['Family'] = data['Parch'] + data['SibSp']\n\n# and plot this\nplt.subplots(2,1,figsize=(22,6))\nplt.subplot(131)\nsns.barplot(x = 'Family',y ='Survived', data=data)\nplt.subplot(132)\nsns.barplot(x = 'Family',y ='Survived', hue='Sex', data=data)\nplt.subplot(133)\ndata.Family.value_counts().plot(kind='pie')","22e0df34":"data = pd.read_csv('..\/input\/titanic\/train.csv')\ndata['age_range'] = 0\n# 0 == baby, 1 == child, 2== young adult, 3== adul, 4 == eldery\ndata['age_range'] = pd.cut(data.Age,bins=[0,2,16,35,55,100],labels=[0, 1, 2, 3,4])\nng_fare = 4\ndata['fare_range']=pd.qcut(data['Fare'],ng_fare, labels=list(range(0, ng_fare)))\ndata[\"Family\"] = 0\ndata['Family'] = data['Parch'] + data['SibSp']\ndata.drop(['PassengerId','Name','Age','Ticket','Fare','Cabin', 'SibSp', 'Parch'],axis=1,inplace=True)\ndata.dropna(inplace=True) # and drop nan\ndata['Sex'].replace(['male','female'],[0,1],inplace=True) # male=0, female=1\n# change the strings that represent each port to numbers\ndata['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True) \ndata.head(2), len(data)","0a440996":"# lets use X as the matrix of features\nX = data.copy()\ny = X['Survived']\nX.drop('Survived',axis=1,inplace=True)","1a18907d":"from sklearn.model_selection import train_test_split\n\nsplit= 0.25\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=5)\n\nfrom sklearn.preprocessing import StandardScaler\n# Data Standardization give data zero mean and unit variance\n#  should be done after train test split\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled  = scaler.transform(X_test)","f28e98f0":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score #, confusion_matrix, classification_report\n\nparams = {\n    \"criterion\":(\"gini\", \"entropy\"), \n    \"splitter\":(\"best\", \"random\"), \n    \"max_depth\":(list(range(1, 20)))\n    }\n\ntree_clf = DecisionTreeClassifier()\ntree_cv = RandomizedSearchCV(tree_clf, params, scoring=\"accuracy\") # n_jobs=-1, verbose=0, cv=3\ntree_cv.fit(X_train, y_train)\nbest_params = tree_cv.best_params_\nprint(f\"Best paramters: {best_params})\")\n\ntree_clf = DecisionTreeClassifier(**best_params)\ntree_clf.fit(X_train, y_train)\npred_train = tree_clf.predict(X_train)\npred_test = tree_clf.predict(X_test)\n\n# print accuracies\nprint(f\"Accuracy Score during trainning was: {accuracy_score(y_train, pred_train) * 100:.2f}%\")\nprint(f\"Accuracy Score on test set was: {accuracy_score(y_test, pred_test) * 100:.2f}%\")","146f8e17":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\nkmax = 24 # number of K to test\nmean_acc = np.zeros((kmax-1)) \nstd_acc = np.zeros((kmax-1))\n\nfor n in range(1,kmax):\n    # loop over different values\n    clf_neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat = clf_neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n    std_acc[n-1] = np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\n    #print('k={} and acc={:.6f}'.format(n, mean_acc[n-1]))\n\n# Plot the obtained valuess in the loop above, incuding 2std\nplt.figure(figsize=(10, 4))\nplt.plot(range(1,kmax),mean_acc,'g')\nplt.fill_between(range(1,kmax),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.fill_between(range(1,kmax),mean_acc - 2 * std_acc,mean_acc + 2 * std_acc, alpha=0.10,color=\"green\")\nplt.legend(('Accuracy ', '+\/- 1xstd','+\/- 2xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\n\nprint( \"The best accuracy with accuracy score was {:.6f} with k= {}\".format( mean_acc.max(), mean_acc.argmax()+1)) \nbest_k  = mean_acc.argmax()+1\n\n# fit again with the best model\nclf_neigh = KNeighborsClassifier(n_neighbors = best_k).fit(X_train,y_train)\n# predict & check accuracy\npred_train = clf_neigh.predict(X_train)\npred_test = clf_neigh.predict(X_test)\nprint(f\"Accuracy Score during trainning was: {accuracy_score(y_train, pred_train) * 100:.2f}%\")\nprint(f\"Accuracy Score on test set was: {accuracy_score(y_test, pred_test) * 100:.2f}%\")","1802957b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\n\nparams = {\"solver\":('liblinear', 'lbfgs', 'saga', 'newton-cg', 'sag'), \n          \"C\":[0.1, 0.01, 0.001, 0.0001]\n         }\n\nlogreg_clf = LogisticRegression()\nlogreg_cv = RandomizedSearchCV(logreg_clf, params) \nlogreg_cv.fit(X_train, y_train)\nbest_params = logreg_cv.best_params_\nprint(f\"Best paramters: {best_params})\")\n\nlogreg_model = LogisticRegression(**best_params)\nlogreg_model.fit(X_train, y_train)\npred_train = logreg_model.predict(X_train)\npred_test = logreg_model.predict(X_test)\n\nyhat_prob = logreg_model.predict_proba(X_test)\na = log_loss(pred_test, yhat_prob)\nprint('logloss', a)\nprint(f\"Accuracy Score during trainning was: {accuracy_score(y_train, pred_train) * 100:.2f}%\")\nprint(f\"Accuracy Score on test set was: {accuracy_score(y_test, pred_test) * 100:.2f}%\")","998f94a8":"from sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {\"kernel\":('linear', 'poly', 'rbf', 'sigmoid'),\n              'C':[1, 10, 100]\n             }\n\nsvc = svm.SVC()\nsvm_model = GridSearchCV(svc, parameters,  n_jobs=-1, verbose=1)\nsvm_model.fit(X_train, y_train)\nbest_params = svm_model.best_params_\nprint(f\"Best paramters: {best_params})\")\nprint(\"================\")\n\nsvm_model = svm.SVC(**best_params)\nsvm_model.fit(X_train, y_train)\n\npred_train = svm_model.predict(X_train)\npred_test = svm_model.predict(X_test)\nprint(f\"Accuracy Score during trainning was: {accuracy_score(y_train, pred_train) * 100:.2f}%\")\nprint(f\"Accuracy Score on test set was: {accuracy_score(y_test, pred_test) * 100:.2f}%\")","2818ff8e":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\n\nkfold = KFold(n_splits=10) # k=10, split the data into 10 equal parts\nmodels = [tree_clf, clf_neigh,svm_model, logreg_model]\nmodels_name = ['Decision Tree', 'KNN', 'SVM', 'LogRegr']\nmeans = []\nstds = []\naccs = []\nfor n, model in enumerate(models):    \n    crossval_result = cross_val_score(model, X, y, cv = kfold,scoring = \"accuracy\")\n    means.append(crossval_result.mean()) \n    stds.append(crossval_result.std())\n    accs.append(crossval_result)","82b64f62":"metric_results = pd.DataFrame(np.column_stack([means, stds]), columns =['mean_model', 'std_model'], index=models_name)\nmetric_results","e38b0c71":"plt.figure(figsize=(18,6))\nbplot = pd.DataFrame(accs,index=[models_name])\nbplot.T.boxplot()","fcf56d04":"'''\n# find the best model as the one with higher mean and lower std\nres = metric_results\nans = res.groupby(['mean_model']).max()\nbest_model_name = res[res.mean_model == ans.std_model.idxmin()].index[0]\nbest_model_name\n'''","a9150ade":"# take indistinticly of the std\nbest_model_name = metric_results.mean_model.idxmax()","5f70586b":"best_model = models[models_name.index(best_model_name)]\nbest_model","de2462e4":"data_test = pd.read_csv('..\/input\/titanic\/test.csv')\nX_data_test = data_test.copy()\nX_data_test['age_range'] = 0\n# 0 == baby, 1 == child, 2== young adult, 3== adul, 4 == eldery\nX_data_test['age_range'] = pd.cut(X_data_test.Age,bins=[0,2,16,35,55,100],labels=[0, 1, 2, 3,4])\nng_fare = 4\nX_data_test['fare_range']=pd.qcut(X_data_test['Fare'],ng_fare, labels=list(range(0, ng_fare)))\nX_data_test[\"Family\"] = 0\nX_data_test['Family'] = X_data_test['Parch'] + X_data_test['SibSp']\nX_data_test.drop(['PassengerId','Name','Age','Ticket','Fare','Cabin', 'SibSp', 'Parch'],axis=1,inplace=True)\n#data_test.dropna(inplace=True) # and drop nan\nX_data_test['Sex'].replace(['male','female'],[0,1],inplace=True) # male=0, female=1\nX_data_test['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True) # change the strings to numbers\n\nX_data_test.head(1), len(X_data_test)","d62a9d76":"print('Using:', best_model)\nyhat = best_model.predict(X_data_test)\nyhat[:10], len(yhat)","13ff2021":"submission = pd.DataFrame({\"PassengerId\": data_test[\"PassengerId\"],\"Survived\": yhat})\nsubmission\nsubmission.to_csv('submission.csv', index=False)","100edbb9":"# best score acchieved was : 0.78708\n# ussing SVM with {'C': 1, 'kernel': 'rbf'}\n# and age settings as now","ddac98ee":"## 3.2. KNN","0bed4323":"## 5. Returning the best -- submission","17329b1f":"In the model I will be using only the family feture, and dropping the Parch and SibSp.","800d4a94":"We can clearlly see that the higher the fare the bigger chances to survive (especially if you're a woman).","4dbe5eaa":"Let us take a look to Parch and SibSp.\n- SibSp. # of siblings \/ spouses aboard the Titanic\n- Parch. # of parents \/ children aboard the Titanic","0c4a4075":"## 3.4. SVM","b1a6b7e6":"Now wee see that the higher survival rates were for those indviduals with 1-2-0-3-4 sibblings (in that order) and from 0 to 3 children in the boat. \nThen, for using this information into the model I create a new variable that is family, counting the number of family aboard.","1e952043":"In this notebook I present some models from sci-kit library.\n   1.  Decision Tree Classifier\n   2.  KNN\n   3. Logistic Regression\n   4. SVM\n","305dfc1f":"In the histograms we see the superimposition of orange (survived) to blue (died). The groups of age were selected considering a baby from 0 to 2 years old, a child from 2 to 16. 16-25 young adult, adults from 35-55, and from that on eldery. This selection has been done thinking that in 1900, the median age of marriage for women was 21.9 years and the average age for childbirth was 22 years. Kids were supposed to work from earlies ages than today, the life expentacie in 1900 was 46 y.o for men and 48 y.o for woman.","38aa03a2":"With the visualizations above, one could say that both sex and class seem interesting variables for using in a classification model. \n\nLet us check now what we can learn distinguishing where the passenrgs embarked:","16be4a42":"Doing the same, but this time for the class, we see how the percentage of not surviving increases as we from 1st to 3rd class, and the opposite for surviving.","cf20dc2a":"In this dataset I tried some EDA to understand the data and wich features can be useful for predicting survival. Then I applied some of the basic ML algorithms using sci-kit.\n\n![](https:\/\/freesvg.org\/img\/titanicsinks.png)","8673adcf":"# 1.- Exploratory Data Analysis and visualizations.","e5552a4c":"## 3.3. Logistic Regression","4fea138e":"There is no clear realation of more male\/female embarking from an specific location, but there is a difference on the class they belong. \nFor example, in Queenstown embarked considerably more passangers belonging to 3rd class.The port with higher survival rate is also the port where more people from 1st class embarked. So even if Embarked may be a 'secondary' feature, this may help to increase the chances to get a better classification model.","b0ad49f6":"## 3.1. Decision Tree","d6e3a584":"First, familiarize with the data.\nIn the following, we see that a 61.6\\% of the pasengers died after the crash with the iceberg. \nAlso, in the boat there were more males than females, and especially males belonging to 3rd class.","38932ffc":"The biggest differences appear between the people that went alone with the ones that had at least one child or a sibblin, but this is also the biggest group.\nThen let us create a bar plot to chech the survival date depending on the number of sibblings.","965097f1":"# 2.- Preprocess data","319e430c":"## 4. Evaluating models ","bb7a087e":"Then, we can do the same but comparing the class.","29a5a0b6":"Finally, the factor plot below comprises information about these three features.\n\n- If you were a female belonging to the 1st class, then you would survive. \n- In the second class the percentage decreases but it is still high\n- If you are a female in the 3rd class, then your probabilities of surviving had decreased considerably, but still higher compared with males.\n\nComparing the distribution of male and female, this reflect that they priorized woman in the lifeboats.","4d9bef12":"Then fare is also a continouous variable","d9169530":"Then, what we can do is to normalize the distribution to see the percentage of survival by sex.\nIf you were a male, there was a bigger probabilty for you to die:","4890f750":"# 3.- Sci-kit models","dc46fa49":"Data:\n\n    - PassengerId. \n    - Survived. 0 = No, 1 = Yes. (this is be what we want to predict later, y )\n    - Pclass. Ticket class: 1 = 1st, 2 = 2nd, 3 = 3rd\n    - *Name. Will use name to find the social status of each passenger (next version of the notebook)\n    - Sex. Sex\n    - Age. Age in years\t\n    - SibSp. # of siblings \/ spouses aboard the Titanic\n    - Parch. # of parents \/ children aboard the Titanic\n    - *Ticket. Ticket number. (will try to use in the next version of the notebook\n    - Fare. Passenger fare\t\n    - *Cabin. Cabin number. Too many missing values, not using this now.\n    - Embarked. Port of Embarkation: C = Cherbourg, Q = Queenstown, S = Southampto~\n\nThe features marked with an * are not going to be considered in this version of the model.","19654371":"So we see the data types and that there are missin values in Age, Cabin and Embarked!\nThe Cabin has less than the half of the dataset so makes no sense to maintain this variable for our model (i will check this later, to see which cabin information related to which groups we have). We may be able to fill some Age values based on other atributes, in this version we directly drop those rows with NaN values. Additionally, I am not trying to use the ticket information here.","6b79319d":"Titanic. Learning from disaster (1st Version)\n------\n------","ac6f189b":"### Binning (continuous variables to categorical variable)","e4b86566":"In the plot above, we presented the normalized distribution of survival depending on where they embarked. \nIt seems that those that embarked in C survived in a higher percentage. Could this be related to the cabin location? Sex distribution by embarked? age?\n\nFirst, we have below the normalized histogram of Embarked depending on the gender of the passenger.","82534c5e":"## 1. Exploratory Data Analysis","0efd9063":"Both Age and Fare are continuous variables, so it is interesting to perform a binning to include them in our classification model. "}}