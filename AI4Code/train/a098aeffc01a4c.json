{"cell_type":{"37a1331a":"code","45ac9dff":"code","9ae40452":"code","d09d3975":"code","87a6cb90":"code","5c0ea714":"code","4170c28d":"code","d17263ba":"code","3cacdcfd":"code","acbeecac":"code","4c1b0b17":"code","520f142f":"code","2fc00cb4":"code","0f313513":"code","6044cb31":"code","41bc1ce8":"code","fb65707b":"code","59644ce0":"code","c0f96450":"code","343d60a8":"code","2093a89d":"code","5f673bed":"code","86afb6a1":"code","6feb8856":"code","7c2adb01":"code","1e549031":"code","dbc9c7f8":"code","73045195":"code","3d238f89":"code","9ceba433":"code","032f71a4":"code","7eb463d6":"code","06eb1871":"code","d7076531":"code","72e7c447":"code","709fe038":"code","fe9bf218":"code","e4cca79e":"code","9f0f6cb7":"code","afc9c3b9":"code","a64a1fc4":"code","a7ec6137":"code","35f208b4":"code","f7b824a3":"code","bf8d5fc9":"code","41701f1a":"code","451bc647":"code","3a757bea":"code","1b1cc640":"code","248f2531":"code","46cb67a6":"code","964e776d":"code","36af83e3":"code","eb749505":"code","d0159fb0":"code","30868b84":"code","04360f84":"code","aee66bbb":"code","1775f734":"code","4b1bf495":"code","967bb931":"code","4fa32af8":"code","4b22c6d9":"code","19885792":"code","f1043e7a":"code","8d18e70e":"markdown","d550b034":"markdown","8a0eb7f0":"markdown","8e5dee7e":"markdown","be1a5f04":"markdown","27bc1e98":"markdown","4955ca87":"markdown","d01b4064":"markdown","a77a6468":"markdown","3270e1f6":"markdown"},"source":{"37a1331a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport scipy\nfrom sklearn import linear_model\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 15, 8\nimport pandas as pd\n%precision 2\nu'%.3f'\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","45ac9dff":"x=.3\ny=.7\na=scipy.special.kl_div(x, y, out=None)\n","9ae40452":"data=pd.read_csv('..\/input\/data-train-test\/train and test - Sheet1.csv')","d09d3975":"data.head()","87a6cb90":"data = data.sample(frac = 1)","5c0ea714":"heatmap = sns.heatmap(data.corr(), vmin=-1, vmax=1, annot=True)\n# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","4170c28d":"#data =data.drop(columns=['B (mm)'])\n#data =data.drop(columns=['H (mm)'])\n\n","d17263ba":"data.head()\n","3cacdcfd":"X=data.drop((['def']),axis=1)\nX.columns","acbeecac":"y=data['def']\ny.head()","4c1b0b17":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","520f142f":" def mean_absolute_percentage_error(actual, pred): \n    actual, pred = np.array(actual), np.array(pred)\n    return np.mean(np.abs((actual - pred) \/ actual)) * 100","2fc00cb4":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\ndef cross_val(model):\n    pred = cross_val_score(model, X, y, cv=10)\n    return pred.mean()\n\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    mape=mean_absolute_percentage_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    kl_di=scipy.special.kl_div(true, predicted)\n    kl_di=kl_di.replace([np.inf, -np.inf], np.nan)\n    \n     \n    print('MAE:', mae)\n    print('MAPE:', mape)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R\u00b2', r2_square)\n    print('KL Divergence',np.sum(kl_di))\n    \ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mape=mean_absolute_percentage_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    kl_di=scipy.special.kl_div(true, predicted)\n    kl =np.sum(kl_di.replace([np.inf, -np.inf], np.nan))\n    return mae,mape, mse, rmse, r2_square , kl","0f313513":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\ndef cross_val1(model):\n    pred = cross_val_score(model, X, y, cv=10)\n    return pred.mean()\n\ndef print_evaluate1(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mape=mean_absolute_percentage_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    kl_d=scipy.special.kl_div(true, predicted)\n    \n     \n    print('MAE:', mae)\n    print('MAPE:', mape)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R\u00b2', r2_square)\n    print('KL Divergence',np.sum(kl_d))\n    \ndef evaluate1(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mape=mean_absolute_percentage_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    kl_d=scipy.special.kl_div(true, predicted)\n    kl =np.sum(kl_d)\n    return mae,mape, mse, rmse, r2_square , kl","6044cb31":"def plots(true, predicted):\n    sns.color_palette(\"Paired\") \n    sns.distplot((predicted), bins=60,label='Predicted Deflection',color='grey').set_title(\" True Deflection and Predicted Deflection Plot\",fontsize=20)\n    sns.distplot((true), bins=60,label='True Deflection',color='black').set_title(\"True Deflection and Predicted Deflection Plot\",fontsize=20)\n    plt.xlabel('True Deflection and Predicted Deflection',fontsize=12)\n    plt.ylabel('Density',fontsize=12)\n    plt.legend(fontsize=15)\n     \ndef plotd(true, predicted):\n    sns.color_palette(\"Paired\")\n    sns.distplot((true - predicted), bins=50,color='grey').set_title(\"Absolute Error Plot\", fontsize=20)\n    plt.xlabel('Absolute Error',fontsize=12)\n    plt.ylabel('Density',fontsize=12)","41bc1ce8":"#std scaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('std_scalar', StandardScaler())\n])\n\nX_train = pipeline.fit_transform(X_train)\nX_test = pipeline.transform(X_test)","fb65707b":"\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train,y_train)\nprint(lin_reg.intercept_)","59644ce0":"coeff_df = pd.DataFrame(lin_reg.coef_, X.columns, columns=['Coefficient'])\ncoeff_df","c0f96450":"test_pred = lin_reg.predict(X_test)\ntrain_pred = lin_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","343d60a8":"predicted_def=y_test.to_frame()","2093a89d":"plt.subplot(2, 2, 1)\nplots(y_test, test_pred)\n\nplt.figure(1)\nplt.subplot(2, 2, 2)\nplotd(y_test, test_pred)\nplt.show()\n","5f673bed":"predicted_def['Linear Regression Prediction']=test_pred\npredicted_def","86afb6a1":"results_df = pd.DataFrame(data=[[\"Linear Regression\", *evaluate(y_test, test_pred) , cross_val(LinearRegression())]], \n                          columns=['Model','MAE', 'MAPE','MSE', 'RMSE', 'R\u00b2',\"KL Divergence\", \"Cross Validation\"])\nresults_df","6feb8856":"from sklearn.linear_model import Ridge\n\nRidge_model = Ridge(alpha=100, solver='cholesky', tol=0.0001, random_state=42)\nRidge_model.fit(X_train, y_train)\npred = Ridge_model.predict(X_test)\n\ntest_pred = Ridge_model.predict(X_test)\ntrain_pred = Ridge_model.predict(X_train)\n\npred = Ridge_model.predict(X_test)\n\ncoeff_df = pd.DataFrame(Ridge_model.coef_, X.columns, columns=['Coefficient'])\ncoeff_df","7c2adb01":"print('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","1e549031":"plt.subplot(2, 2, 1)\nplots(y_test, test_pred)\n\nplt.figure(1)\nplt.subplot(2, 2, 2)\nplotd(y_test, test_pred)\nplt.show()\n","dbc9c7f8":"predicted_def['Ridge Regression Prediction']=test_pred\npredicted_def","73045195":"results_df_2 = pd.DataFrame(data=[[\"Ridge Regression\", *evaluate(y_test, test_pred) , cross_val(Ridge())]], \n                            columns=['Model','MAE', 'MAPE','MSE', 'RMSE', 'R\u00b2', \"KL Divergence\", \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","3d238f89":"from sklearn.linear_model import Lasso\n\nLasso_model = Lasso(alpha=0.1, \n              precompute=True, \n#               warm_start=True, \n              positive=True, \n              selection='random',\n              random_state=42)\nLasso_model.fit(X_train, y_train)\n\ntest_pred = Lasso_model.predict(X_test)\ntrain_pred = Lasso_model.predict(X_train)\n\npred = Lasso_model.predict(X_test)\ncoeff_df = pd.DataFrame(Lasso_model.coef_, X.columns, columns=['Coefficient'])\ncoeff_df","9ceba433":"print('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","032f71a4":"plt.subplot(2, 2, 1)\nplots(y_test, test_pred)\n\nplt.figure(1)\nplt.subplot(2, 2, 2)\nplotd(y_test, test_pred)\nplt.show()\n","7eb463d6":"predicted_def['Lasso Regression Prediction']=test_pred\npredicted_def","06eb1871":"results_df_2 = pd.DataFrame(data=[[\"Lasso Regression\", *evaluate(y_test, test_pred) , cross_val(Ridge())]], \n                            columns=['Model','MAE', 'MAPE', 'MSE', 'RMSE', 'R\u00b2', \"KL Divergence\", \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","d7076531":"from xgboost import XGBRegressor\nXGBModel = XGBRegressor()\nXGBModel.fit(X_train,y_train , verbose=False)\n\ntest_pred = XGBModel.predict(X_test)\ntrain_pred = XGBModel.predict(X_train)\n\npred = XGBModel.predict(X_test)","72e7c447":"print('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","709fe038":"plt.subplot(2, 2, 1)\nplots(y_test, test_pred)\n\nplt.figure(1)\nplt.subplot(2, 2, 2)\nplotd(y_test, test_pred)\nplt.show()\n","fe9bf218":"predicted_def['XGBoost Prediction']=test_pred\npredicted_def","e4cca79e":"results_df_2 = pd.DataFrame(data=[[\"XGB\", *evaluate(y_test, test_pred) , cross_val(Ridge())]], \n                            columns=['Model','MAE', 'MAPE', 'MSE', 'RMSE', 'R\u00b2', \"KL Divergence\", \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","9f0f6cb7":"from sklearn.preprocessing import PolynomialFeatures\n\npoly_reg = PolynomialFeatures(degree=2)\n\nX_train_2_d = poly_reg.fit_transform(X_train)\nX_test_2_d = poly_reg.transform(X_test)\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train_2_d,y_train)\n\ntest_pred = lin_reg.predict(X_test_2_d)\ntrain_pred = lin_reg.predict(X_train_2_d)\n","afc9c3b9":"print('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n","a64a1fc4":"plt.subplot(2, 2, 1)\nplots(y_test, test_pred)\n\nplt.figure(1)\nplt.subplot(2, 2, 2)\nplotd(y_test, test_pred)\nplt.show()\n","a7ec6137":"predicted_def['Polynomial Regression Prediction']=test_pred\npredicted_def","35f208b4":"results_df_2 = pd.DataFrame(data=[[\"Polynomial Regression\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model','MAE', 'MAPE', 'MSE', 'RMSE', 'R\u00b2', \"KL Divergence\", \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","f7b824a3":"from sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor()\nsgd_reg.fit(X_train, y_train)\n\ntest_pred = sgd_reg.predict(X_test)\ntrain_pred = sgd_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","bf8d5fc9":"plt.subplot(2, 2, 1)\nplots(y_test, test_pred)\n\nplt.figure(1)\nplt.subplot(2, 2, 2)\nplotd(y_test, test_pred)\nplt.show()\n","41701f1a":"predicted_def['SGD Prediction']=test_pred\npredicted_def","451bc647":"results_df_2 = pd.DataFrame(data=[[\"Stochastic Gradient Descent\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model','MAE', 'MAPE', 'MSE', 'RMSE', 'R\u00b2', \"KL Divergence\", \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","3a757bea":"from sklearn.ensemble import RandomForestRegressor\n\nrf_reg = RandomForestRegressor()\nrf_reg.fit(X_train, y_train)\n\ntest_pred = rf_reg.predict(X_test)\ntrain_pred = rf_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","1b1cc640":"plt.subplot(2, 2, 1)\nplots(y_test, test_pred)\n\nplt.figure(1)\nplt.subplot(2, 2, 2)\nplotd(y_test, test_pred)\nplt.show()\n","248f2531":"predicted_def['RF Regressor Prediction']=test_pred\npredicted_def","46cb67a6":"results_df_2 = pd.DataFrame(data=[[\"Random Forest Regressor\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model','MAE', 'MAPE', 'MSE', 'RMSE', 'R\u00b2', \"KL Divergence\", \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","964e776d":"from sklearn.svm import SVR\n\nsvm_reg = SVR()\nsvm_reg.fit(X_train, y_train)\n\ntest_pred = svm_reg.predict(X_test)\ntrain_pred = svm_reg.predict(X_train)\n\n","36af83e3":"print('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","eb749505":"plt.subplot(2, 2, 1)\nplots(y_test, test_pred)\n\nplt.figure(1)\nplt.subplot(2, 2, 2)\nplotd(y_test, test_pred)\nplt.show()\n","d0159fb0":"predicted_def['SVM Prediction']=test_pred\npredicted_def","30868b84":"results_df_2 = pd.DataFrame(data=[[\"SVM\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model','MAE', 'MAPE', 'MSE', 'RMSE', 'R\u00b2', \"KL Divergence\", \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","04360f84":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nX_train = np.array(X_train)\nX_test = np.array(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\nmodel = Sequential()\n\nmodel.add(Dense(X_train.shape[1], activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(128, activation='relu'))\n# model.add(Dropout(0.2))\n\n#model.add(Dense(512, activation='relu'))\n#model.add(Dropout(0.2))\n\nmodel.add(Dense(512, activation='relu'))\n#model.add(Dropout(0.1))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer=Adam(), loss='mse')\n\nr = model.fit(X_train, y_train,\n              validation_data=(X_test,y_test),\n              batch_size=1,\n              epochs=100)","aee66bbb":"plt.figure(figsize=(10, 6))\n\nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()","1775f734":"test_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\n","4b1bf495":"test_pred1=test_pred.flatten()\ny_test1=y_test.flatten()\ntrain_pred1=train_pred.flatten()\ny_train1=y_train.flatten()","967bb931":"print('Test set evaluation:\\n_____________________________________')\nprint_evaluate1(y_test1, test_pred1)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate1(y_train1, train_pred1)","4fa32af8":"plt.subplot(2, 2, 1)\nplots(y_test, test_pred)\n\nplt.figure(1)\nplt.subplot(2, 2, 2)\nplotd(y_test, test_pred)\nplt.show()\n","4b22c6d9":"predicted_def['ANN Prediction']=test_pred\npredicted_def","19885792":"results_df_2 = pd.DataFrame(data=[[\"ANN\", *evaluate1(y_test1, test_pred1), 0]], \n                            columns=['Model','MAE', 'MAPE', 'MSE', 'RMSE', 'R\u00b2', \"KL Divergence\", \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","f1043e7a":"results_df.to_excel(r'.\/\\Fi.xlsx', index = False)","8d18e70e":"# SVM_REGRESSION","d550b034":"# Stochastic_Gradient_Descent_Regressor","8a0eb7f0":"# XGBoost_REGRESSION","8e5dee7e":"# LINEAR_REGRESSION","be1a5f04":"# RANDOM_FOREST_REGRESSION","27bc1e98":"# RIDGE_REGRESSION","4955ca87":"# ANN","d01b4064":"# LASSO_REGRESSION","a77a6468":"# Summary ","3270e1f6":"# POLYNOMIAL_REGRESSION"}}