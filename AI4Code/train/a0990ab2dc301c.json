{"cell_type":{"a681a6f8":"code","7dd7e8a7":"code","9d6173aa":"code","f3c521fb":"code","35183fd9":"code","fcff7ec6":"code","abcc8f6e":"code","52575ce4":"code","f2db13b8":"code","89f99184":"code","a2f45b1e":"code","770ab8b8":"code","bd1a02cf":"code","39ef340f":"code","ab386dee":"code","19153ced":"code","c871fadf":"code","f6cca8d2":"code","81f40b0e":"code","586deec2":"code","26dd2a90":"code","435bf3e7":"code","ce6fd8d4":"code","10a21e99":"code","65fa9e44":"code","aa7c97a9":"code","4128c391":"code","2d6cc06b":"code","db4e294e":"code","73ca32e2":"code","600bfef4":"code","c449e0a2":"code","bbcad5f4":"code","1c19c05c":"code","6238aece":"code","89d71ad1":"code","c032b9d9":"code","48bfa75b":"code","9a119658":"code","ffa829e4":"code","6c6e774f":"code","ad35af19":"code","9a1f3d26":"code","9fba5683":"markdown","e96bc83a":"markdown","890caf13":"markdown","0646a418":"markdown","8df36e74":"markdown","4023d808":"markdown","d175fb44":"markdown","a8af60b0":"markdown","d8b9db26":"markdown","2334b8cd":"markdown","5328fefc":"markdown","08ae9f38":"markdown","eb9da2a8":"markdown","315b37ef":"markdown","773456f3":"markdown","55b9d4cb":"markdown","7fe76016":"markdown","f4afbda9":"markdown","ff9002c1":"markdown","ccee5c18":"markdown","05bab9f0":"markdown","d689299b":"markdown","e6a0dfcc":"markdown","e8a4c1ca":"markdown","88e2ad48":"markdown","211933f3":"markdown","a9a50bcb":"markdown","b580ea18":"markdown","d176a000":"markdown","5583a299":"markdown","bd87ba0f":"markdown","4c5da0c5":"markdown","98e17a6e":"markdown","bafd173f":"markdown","cefd8236":"markdown","1d102d66":"markdown","a83614af":"markdown","7c1e7744":"markdown","f01e2f05":"markdown","af43822e":"markdown","fc90e335":"markdown","a6adc1f3":"markdown","7e36d790":"markdown","8e56036e":"markdown","d66fc441":"markdown","f291bb58":"markdown","8a4c196e":"markdown","bebe7b21":"markdown","de496af9":"markdown","7d1b771d":"markdown","2d826d12":"markdown","f4d6f063":"markdown","bca36418":"markdown","84d7fffc":"markdown","04ffa2dc":"markdown","b9f4b67b":"markdown","b63c5c2e":"markdown","210e7952":"markdown","827db61c":"markdown","fd173d2b":"markdown","d3441e0f":"markdown"},"source":{"a681a6f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport os\nimport numpy as np # linear algebra\nfrom numpy import mean\nfrom numpy import std\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.stats import norm,randint\nfrom math import ceil # round numbers up\nimport time\nimport pprint\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport lightgbm as lgbm\n\nfrom catboost import CatBoostRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import make_regression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import (AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier,GradientBoostingRegressor,RandomForestClassifier,VotingClassifier)\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, LogisticRegression, PassiveAggressiveClassifier,RidgeClassifierCV,Ridge\nfrom sklearn.metrics import accuracy_score,auc,classification_report,confusion_matrix,mean_squared_error, precision_score, recall_score,roc_curve\nfrom sklearn.model_selection import cross_val_score,cross_val_predict,cross_validate,train_test_split,GridSearchCV,KFold,learning_curve,RandomizedSearchCV,StratifiedKFold\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV,train_test_split,cross_val_score,RepeatedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.svm import SVC, LinearSVC,SVR\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import ensemble, linear_model,neighbors, svm, tree, model_selection,preprocessing\nfrom sklearn import utils\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.metrics import mean_squared_error as MSE\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7dd7e8a7":"# Get data\ntrain=pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest=pd.read_csv('..\/input\/30-days-of-ml\/test.csv',index_col='id')\nfull_df=train.copy()\nfulltest_df=test.copy()\nprint('Data Import Complete')","9d6173aa":"# Take a first look at training data\nprint(train.shape)\nprint(train.head())","f3c521fb":"# Take a first look at test data\nprint(test.shape)\nprint(test.head())","35183fd9":"print(train.describe())","fcff7ec6":"print(test.describe())","abcc8f6e":"# Get number of unique entries in each column with categorical data\nobject_cols=train.columns[0:10]\nobject_nunique = list(map(lambda col: train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","52575ce4":"# Get number of unique entries in each column with categorical data\nobject_cols=test.columns[0:10]\nobject_nunique = list(map(lambda col: test[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","f2db13b8":"# Number of missing values in each column of training data\nmissing_val_count_by_column = (train.isnull().sum())\nif missing_val_count_by_column.sum()==0:print(\"No missing training data\")\nelse: print(missing_val_count_by_column[missing_val_count_by_column > 0]) \n\n# Number of missing values in each column of test data\nmissing_val_count_by_column = (test.isnull().sum())\nif missing_val_count_by_column.sum()==0:print(\"No missing test data\")\nelse: print(missing_val_count_by_column[missing_val_count_by_column > 0]) ","89f99184":"# Get data\ntrain=pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest=pd.read_csv('..\/input\/30-days-of-ml\/test.csv',index_col='id')\n\ny = train.target\ntrain.drop(['target'], axis=1, inplace=True)\nprint('Target data separated')\n\n# Convert categorical data to numeric data\nobject_cols = [col for col in train.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nordinal_encoder = OrdinalEncoder()\ntrain[object_cols] = ordinal_encoder.fit_transform(train[object_cols])\ntest[object_cols] = ordinal_encoder.transform(test[object_cols])\nprint('All category columns converted to ordinal')\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(train, y, train_size=0.025, test_size=0.025,random_state=0)\ntest=test\nprint('Data split')\n\n# Preview the ordinal-encoded features\n#print(train.head())","a2f45b1e":"print(train.head())","770ab8b8":"# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(train, y, train_size=0.025, test_size=0.025,random_state=0)\nprint('Data split')","bd1a02cf":"# get mean and std (mu, sigma)\n(mu, sigma) = norm.fit(full_df['target'])\n# plot distribution\nfig = plt.figure(figsize=(18, 7))\nsns.distplot(full_df['target'], kde=True, hist=False, fit=norm)\nplt.title('Target Distribution', fontsize=12)\nplt.legend([f\"mu:{mu}, sigma:{sigma}\"],loc=\"best\")\nplt.show()","39ef340f":"print(f\"Skewness: {full_df['target'].skew()}\")\nprint(f\"Kurtosis: {full_df['target'].kurt()}\")","ab386dee":"## plot key distributions of numeric training data\n# separate categorical and numerical data\nuseful_features = [c for c in train.columns if c not in (\"id\",\"target\",\"kfold\")]\n#Nfeatures = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6','cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13'] #manual list of numeric columns\nNfeatures = [col for col in useful_features if 'cont' in col] #dynamic list of numeric columns\n# create layout\ncols = 3\nrows = len(Nfeatures) \/\/ cols+1\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(15,15), sharex=False)\nfig.subplots_adjust(top=2)\nfig.tight_layout()\nplt.subplots_adjust(hspace = 0.4)\ni=0\nfor row in np.arange(0, rows, 1):\n    for column in np.arange(0, cols, 1):\n        if i >= len(Nfeatures):\n            axs[row, column].set_visible(False)\n        else:\n            axs[row,column].hist(train[Nfeatures[i]].values,color=\"blue\",edgecolor=\"black\",alpha=0.7,label=\"Train Dataset\",bins=40)\n            axs[row, column].set_title(Nfeatures[i], fontsize=17, pad=4)\n            axs[row, column].tick_params(axis=\"y\", labelsize=11)\n            axs[row, column].tick_params(axis=\"x\", labelsize=11)\n            axs[row,column].spines['right'].set_visible(False)\n            axs[row,column].spines['top'].set_visible(False)\n        i+=1\nplt.show();\n\n## The layout for this came from https:\/\/www.kaggle.com\/yogidsba\/eda-kfold-30-day-ml","19153ced":"## plot key distributions of numeric test data\n# separate categorical and numerical data\nuseful_features = [c for c in train.columns if c not in (\"id\",\"target\",\"kfold\")]\nNfeatures = [col for col in useful_features if 'cont' in col] #dynamic list of numeric columns\n# create layout\ncols = 3\nrows = len(Nfeatures) \/\/ cols+1\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(15,15), sharex=False)\n#fig.suptitle(\"Distributions of categorical training data\", verticalalignment='bottom',fontsize=24)\nfig.subplots_adjust(top=2)\nfig.tight_layout()\nplt.subplots_adjust(hspace = 0.4)\ni=0\nfor row in np.arange(0, rows, 1):\n    for column in np.arange(0, cols, 1):\n        if i >= len(Nfeatures):\n            axs[row, column].set_visible(False)\n        else:\n            axs[row,column].hist(test[Nfeatures[i]].values,color=\"red\",edgecolor=\"black\",alpha=0.7,label=\"Test Dataset\",bins=40)\n            axs[row, column].set_title(Nfeatures[i], fontsize=17, pad=4)\n            axs[row, column].tick_params(axis=\"y\", labelsize=11)\n            axs[row, column].tick_params(axis=\"x\", labelsize=11)\n            axs[row,column].spines['right'].set_visible(False)\n            axs[row,column].spines['top'].set_visible(False)\n        i+=1\nplt.show();\n\n## The layout for this came from https:\/\/www.kaggle.com\/yogidsba\/eda-kfold-30-day-ml","c871fadf":"# set up layout\n# Cfeatures = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8','cat9'] #manual list of categories\nCfeatures = [col for col in useful_features if 'cat' in col] # dynamic list columns with categorical data\ncols = 3\nrows = len(Cfeatures) \/\/ cols \nfig, axes = plt.subplots(rows,cols,figsize=(15,15))\n# populate plot\nfor feature, ax in zip(Cfeatures, axes.flatten()):\n    full_df1= full_df.sort_values([feature]).reset_index(drop=True)\n    sns.histplot(data = full_df1, x = feature,ax=ax,color='cornflowerblue',alpha=0.7)\n    \nplt.show()","f6cca8d2":"# set up layout\n# Cfeatures = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8','cat9'] #manual list of categories\nCfeatures = [col for col in useful_features if 'cat' in col] # dynamic list columns with categorical data\ncols = 3\nrows = len(Cfeatures) \/\/ cols \nfig, axes = plt.subplots(rows,cols,figsize=(15,15))\n# populate plot\nfor feature, ax in zip(Cfeatures, axes.flatten()):\n    fulltest_df1= fulltest_df.sort_values([feature]).reset_index(drop=True)\n    sns.histplot(data = fulltest_df1, x = feature,ax=ax,color='coral',alpha=0.7)    \nplt.show()","81f40b0e":"# Cfeatures = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8','cat9'] #manual list of categories\nuseful_features = [c for c in train.columns if c not in (\"id\",\"kfold\")]\nCfeatures = [col for col in full_df if 'cat' in col] # dynamic list columns with categorical data\ncols = 3\nrows = len(Cfeatures) \/\/ cols\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(20,20), sharex=False, sharey=False)\ngs = fig.add_gridspec(rows, cols)\ni=0    \nfor row in np.arange(0, rows, 1):\n    for col in np.arange(0, cols, 1):\n            feature=Cfeatures[i]\n            ax = fig.add_subplot(gs[row,col])\n            ax.legend(loc=\"upper right\")\n            full_df1 = full_df.sort_values([feature]).reset_index(drop=True)\n            sns.violinplot(x=Cfeatures[i],y='target', data=full_df1,features_type='categorical',hspace=0.5)\n            i+=1    \n# show only the outside spines\nfor ax in fig.get_axes():\n    ax.set_xticks([0,1], minor=False)\n    ax.set_yticks([], minor=False)\nplt.show()","586deec2":"useful_features = [c for c in train.columns if c not in (\"id\",\"target\",\"kfold\")]\nNfeatures = [col for col in useful_features if 'cont' in col] #dynamic list of numeric columns\n\n# correlate features to target\ncorr = train[Nfeatures].corr()\n\n# create heatmap\nmask = np.triu(np.ones_like(corr, dtype = bool))\nplt.figure(figsize = (15, 15))\nplt.title('Correlation matrix for Train data')\nsns.heatmap(corr, mask = mask,annot=True, cmap = 'RdBu', linewidths = .5,square=True,cbar_kws={\"shrink\": .60})\nplt.show()","26dd2a90":"features = full_df.drop(['target'], axis=1, inplace=False)\ntarget = full_df['target'].copy()\n\nfor cols in features.select_dtypes(\"object\"):\n    features[cols], _ = features[cols].factorize()\n    \ndiscrete_features = features.dtypes == int\n\nmi_value = mutual_info_regression(features, target, discrete_features=discrete_features)\nmi_value = pd.Series(mi_value, name=\"MI\", index=features.columns)\nmi_value = mi_value.sort_values(ascending=True)\nwidth = np.arange(len(mi_value))\nticks = list(mi_value.index)\n\nplt.figure(dpi=100, figsize=(8,5))\nplt.barh(width, mi_value)\nplt.yticks(width, ticks)\nplt.title(\"Mutual Information\")","435bf3e7":"#convert continuous values to float\nlab_enc = preprocessing.LabelEncoder()\ny_train = lab_enc.fit_transform(y_train)\n\n# instanciate and fit model\nRandomForest_checker = RandomForestRegressor(random_state=1)\nRandomForest_checker.fit(X_train, y_train)\n\n# put feature impoartance into table\nimportances_df = pd.DataFrame(RandomForest_checker.feature_importances_, columns=['Feature_Importance'],\n                              index=X_train.columns)\nimportances_df.sort_values(by=['Feature_Importance'], ascending=False, inplace=True)\n#print(importances_df)\n\n# plot importance as bar chart\nimportances=importances_df.index\nimportances_df = importances_df.sort_values(['Feature_Importance'])\ny_pos = np.arange(len(importances_df))\nplt.figure(figsize=(8,5))\nplt.barh(y_pos,importances_df['Feature_Importance'])\nplt.yticks(y_pos, importances,fontsize=10)\nplt.ylabel('importance')\nplt.title('Feature importance for RandomForestRegressor')\nplt.show()","ce6fd8d4":"# importance and muluality\n\n#sort by index\nmi_value = mi_value.sort_index(ascending=True)\nimportances_df = importances_df.sort_index(ascending=True)\ny_pos = np.arange(len(importances_df))\nimportances=importances_df.index\n\n# plot\nplt.figure(figsize=(25,20))\nplt.barh(y_pos,importances_df['Feature_Importance'],label='feature importance')\nplt.barh(y_pos,mi_value,label='mutuality',color=\"skyblue\")\nplt.yticks(y_pos, importances,fontsize=20)\nplt.ylabel('importance')\nplt.title('Multuality and Feature importance', fontsize=20)\nplt.legend()\nplt.show()\n","10a21e99":"## plot clustermap\nsns.clustermap(train.corr())","65fa9e44":"# Create an empty dataframe for performance results\nBasicModelPerformanced_df = pd.DataFrame(columns=['mean_squared_error'])\nprint('Dataframe created')","aa7c97a9":"# Get data\ntrain=pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest=pd.read_csv('..\/input\/30-days-of-ml\/test.csv',index_col='id')\nprint('Data Import Complete')\n\n# List of numeric columns\nnumerical_cols = [cname for cname in train.columns if train[cname].dtype in ['int64', 'float64']]\n# List of categorical columns\nobject_cols = [col for col in train.columns if 'cat' in col]\n\n# high and low cardinality\nlow_card_columns=[cname for cname in train.columns if train[cname].nunique() < 10 and train[cname].dtype == \"object\"]\nhigh_card_columns=[cname for cname in train.columns if train[cname].nunique() >= 10 and train[cname].dtype == \"object\"]\n\n# ordinal_encode columns with high cardinality\nXtrain = train.copy()\nXtest = test.copy()\nordinal_encoder = OrdinalEncoder()\ntrain[high_card_columns] = ordinal_encoder.fit_transform(train[high_card_columns])\ntest[high_card_columns] = ordinal_encoder.transform(test[high_card_columns])\n\n# one-hot_encode columns with low cardinality\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train[low_card_columns]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(test[low_card_columns]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train.index\nOH_cols_test.index = test.index\n\n# Relabel columns\nall_cols=OH_cols_train.columns\nnew_cols = [i for i in all_cols if isinstance(i, (int, float))]\nOH_cols_train=OH_cols_train[new_cols].add_prefix('cat_encode_')\nOH_cols_test=OH_cols_test[new_cols].add_prefix('cat_encode_')\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = train.drop(low_card_columns, axis=1)\nnum_X_test = test.drop(low_card_columns, axis=1)\n\n# Add one-hot encoded columns to numerical features\ntrain= pd.concat([num_X_train, OH_cols_train], axis=1)\ntest = pd.concat([num_X_test, OH_cols_test], axis=1)\ntrain_full=train.copy()\n# separate target data\ny = train.target\ntrain.drop(['target'], axis=1, inplace=True)\nprint('Target data separated')\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(train, y, train_size=0.025, test_size=0.025,random_state=0)\ntest=test\nprint('Data split')","4128c391":"# Define the model \nmodel = RandomForestRegressor(random_state=1)\n\n# Train the model (will take about 10 minutes to run)\nmodel.fit(X_train, y_train)\npreds_valid = model.predict(X_valid)\n\n# score model\nmodelname='RandomForestRegressor'\nmse_score=mean_squared_error(y_valid, preds_valid, squared=False)\ntry:\n    BasicModelPerformanced_df.at[modelname,'mean_squared_error']=mse_score\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'mean_squared_error': mse_score})\nprint(mse_score)\n\n# Use the model to generate predictions\npredictions = model.predict(test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.index,\n                       'target': predictions})\noutput.to_csv('basic_random_forest_submission.csv', index=False)\nprint('basic random forest submission completed')","2d6cc06b":"# Define the model \nmodel = XGBRegressor(n_estimators=1000, learning_rate=0.03, random_state=1, n_jobs=2)\n\n# Train the model \nmodel.fit(X_train, y_train, early_stopping_rounds = 20, eval_set=[(X_valid, y_valid)], verbose=False)\npreds_valid = model.predict(X_valid)\n\n# score model\nmodelname='xgboost'\nmse_score=mean_squared_error(y_valid, preds_valid, squared=False)\ntry:\n    BasicModelPerformanced_df.at[modelname,'mean_squared_error']=mse_score\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'mean_squared_error': mse_score})\nprint(mse_score)\n\n# generate predictions\npredictions = model.predict(test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.index,'target': predictions})\noutput.to_csv('basic_xgboost_submission.csv', index=False)\nprint('basic xgboost submission complete')","db4e294e":"# gradient boosting for regression in scikit-learn\n\n# instaciate model\nmodel = GradientBoostingRegressor()\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n\n# fit the model on the whole dataset\nmodel = GradientBoostingRegressor()\nmodel.fit(X_train, y_train)\n\npreds_valid = model.predict(X_valid)\n\n# score model\nmodelname='LGBMRegressor'\nmse_score=mean_squared_error(y_valid, preds_valid, squared=False)\ntry:\n    BasicModelPerformanced_df.at[modelname,'mean_squared_error']=mse_score\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'mean_squared_error': mse_score})\nprint(mse_score)\n\n# make a single prediction\npredictions = model.predict(test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.index,'target': predictions})\noutput.to_csv('LGBM_submission.csv', index=False)\nprint('LGBM submission complete')","73ca32e2":"# Initialize ridge\nmodel = Ridge(alpha=1.0)\n# Fit model\nmodel.fit(X_train,y_train)\n\n# Get predictions\npreds_valid = model.predict(X_valid)\n\n# score model\nmodelname='Ridge'\nmse_score=mean_squared_error(y_valid, preds_valid, squared=False)\ntry:\n    BasicModelPerformanced_df.at[modelname,'mean_squared_error']=mse_score\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'mean_squared_error': mse_score})\nprint(mse_score)\n\n# make a single prediction\npredictions = model.predict(test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.index,'target': predictions})\noutput.to_csv('ridge_submission.csv', index=False)\nprint('Ridge submission complete')","600bfef4":"# Initialize CatBoostRegressor\nmodel = CatBoostRegressor(iterations=10,\n                          learning_rate=1,\n                          depth=3)\n# Fit model\nmodel.fit(X_train,y_train)\n\n# Get predictions\npreds_valid = model.predict(X_valid)\n\n# score model\nmodelname='Catboost'\nmse_score=mean_squared_error(y_valid, preds_valid, squared=False)\ntry:\n    BasicModelPerformanced_df.at[modelname,'mean_squared_error']=mse_score\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'mean_squared_error': mse_score})\nprint(mse_score)\n\n# make a single prediction\npredictions = model.predict(test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.index,'target': predictions})\noutput.to_csv('catboost_submission.csv', index=False)\nprint('Catboost submission complete')","c449e0a2":"# Initialize CatBoostRegressor\nmodel = SVR(C=1.0, epsilon=0.2)\nmodel.fit(X_train,y_train)\npreds_valid = model.predict(X_valid)\n\n# score model\nmodelname='SVR'\nmse_score=mean_squared_error(y_valid, preds_valid, squared=False)\ntry:\n    BasicModelPerformanced_df.at[modelname,'mean_squared_error']=mse_score\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'mean_squared_error': mse_score})\nprint(mse_score)\n\n# generate predictions\npredictions = model.predict(test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.index,'target': predictions})\noutput.to_csv('SVR_submission.csv', index=False)\nprint('SVR submission complete')","bbcad5f4":"# sort the dataframe\nBasicModelPerformanced_df = BasicModelPerformanced_df.sort_values(\"mean_squared_error\",ascending=True)\nprint(BasicModelPerformanced_df)\n\n# save the dataframe to csv\nfilename=('mse_'+time.strftime('%Y_%m_%d_%H_%M') + '.csv')\noutput = pd.DataFrame({'Model': BasicModelPerformanced_df.index,'mean_squared_error': BasicModelPerformanced_df.mean_squared_error})\noutput.to_csv(filename, index=False)\nprint('\\nreview saved as',filename)","1c19c05c":"# Get data\ntrain=pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest=pd.read_csv('..\/input\/30-days-of-ml\/test.csv',index_col='id')\nfull_df=train.copy()\nfulltest_df=test.copy()\nprint('Data Import Complete')\n\n# remove target\ny = train.target\n\n# get features\nuseful_features = [c for c in train.columns if c not in (\"id\",\"target\",\"kfold\")]\n\nprint(useful_features)\n\n# endode features\ntrain = pd.get_dummies(train[useful_features])\ntest=pd.get_dummies(test[useful_features])\n\nprint('Target data separated')\n\n# split test(validation) and training data\nX_train, X_valid, y_train, y_valid = train_test_split(train, y, train_size=0.025, test_size=0.025,random_state=48)","6238aece":"#import the data and shape\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nprint(train.shape,test.shape)\n\n#add kfold one columns and populate\ntrain['kfold']=-1\nkfold = model_selection.KFold(n_splits=10, shuffle= True, random_state = 1)\nfor fold, (train_indicies, valid_indicies) in enumerate(kfold.split(train)):\n    print(fold,train_indicies,valid_indicies)\n    train.loc[valid_indicies,'kfold'] = fold\n\nprint(train.kfold.value_counts()) #total data 300000 = kfold split :5 * 60000\n\n#output of train folds data\ntrain.to_csv(\"trainfold_10.csv\",index=False)\n\n#create variables\nfinal_predictions = []\nscore= []\n\n#features(categorical and numerical datas separate)\nuseful_features = [c for c in train.columns if c not in (\"id\",\"target\",\"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ntest = test[useful_features]\n\nfor fold in range(10):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #ordinal encode categorical colums and standardscaler is applied (mean0,sd=1)\n    ordinal_encoder = OrdinalEncoder()\n    \n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n    \n    #Model hyperparameter of XGboostRegressor\n    xgb_params = {'n_estimators': 4000,\n              'learning_rate': 0.11,\n              'subsample': 0.98,\n              'colsample_bytree': 0.13,\n              'max_depth': 2,\n              'booster': 'gbtree', \n              'reg_lambda': 68.1,\n              'reg_alpha': 15.7,\n              'random_state': 38,\n              }\n    \n    #model= XGBRegressor(**xgb_params,tree_method='gpu_hist',gpu_id=0,predictor='gpu_predictor') #use for gpu acceleration\n    model= XGBRegressor(**xgb_params) #use without gpu\n    model.fit(xtrain,ytrain)\n    preds_valid = model.predict(xvalid)\n    \n    #Training model apply the test data and predict the output\n    test_pre = model.predict(xtest)\n    final_predictions.append(test_pre)\n    \n    #Rootmeansquared output\n    rms = mean_squared_error(yvalid,preds_valid,squared=False)\n    \n    score.append(rms)\n    #way of output is display\n    print(f\"fold:{fold},rmse:{rms}\")\n\n#mean of repeation of fold data and identify the  mean and standard deviation \nprint(np.mean(score),np.std(score))\n\n#prediction of data\npredictions = np.mean(np.column_stack(final_predictions),axis=1)\n#print(predictions)\n\n# Save the predictions to a CSV file\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsample_submission.target = predictions\nsample_submission.to_csv(\"xgboost_with_kfold_submission.csv\",index=False)\nprint('xgboost with kfold submission complete')","89d71ad1":"# Get data\ntrain=pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest=pd.read_csv('..\/input\/30-days-of-ml\/test.csv',index_col='id')\nfull_df=train.copy()\nfulltest_df=test.copy()\nprint('Data Import Complete')\n\n# remove target\ny = train.target\n\n# get features\nuseful_features = [c for c in train.columns if c not in (\"id\",\"target\",\"kfold\")]\n\nprint(useful_features)\n# endode features\ntrain = pd.get_dummies(train[useful_features])\ntest=pd.get_dummies(test[useful_features])\n\nprint('Target data separated')\n\n# split test(validation) and training data\nX_train, X_valid, y_train, y_valid = train_test_split(train, y, train_size=0.025, test_size=0.025,random_state=48)\n\nrandom_grid = {'n_estimators': [5,20,50,100], # number of trees in the random forest\n               'max_features': ['auto', 'sqrt'], # number of features in consideration at every split\n               'max_depth': [int(x) for x in np.linspace(10, 120, num = 12)], # maximum number of levels allowed in each decision tree\n               'min_samples_split': [2, 6, 10], # minimum sample number to split a node\n               'min_samples_leaf': [1, 3, 4], # minimum sample number that can be stored in a leaf node\n               'bootstrap': [True, False]} # method used to sample data points\n# instanciate model\nmodel = RandomForestRegressor()\n\n# instanciate random search\nmodel_random = RandomizedSearchCV(estimator = model,param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=35, n_jobs = -1)\n\n# fit model\nmodel_random.fit(X_train, y_train)\n\n# print results\nprint('Best results: ', np.sqrt(np.abs(model_random.best_score_)))\nprint ('Random grid: ', random_grid, '\\n')\n# print the best parameters\nprint ('Best Parameters: ', model_random.best_params_, ' \\n')      \n","c032b9d9":"# predict data using the best parameters\n#instanciate final model\nmodel = RandomForestRegressor(n_estimators = 100, min_samples_split = 6, min_samples_leaf= 4, max_features = 'sqrt', max_depth= 120, bootstrap=False) \n# train fiinal model\nmodel.fit( X_train, y_train)\npredict= model.predict(test)\n\noutput = pd.DataFrame({'id':test.index, 'target':predict})\noutput.to_csv('Hyper_randomforest_submission.csv', index=False)\nprint('Hyper randomforest submission complete')\n","48bfa75b":"# create parameter grid\ngbm_param_grid = {\n    'booster':['gbtree','dart'],\n    'colsample_bytree': np.arange(0, 1, 0.1),\n    'n_estimators': np.arange(1000, 2000, 4000),\n    'max_depth': [3, 5, 6, 7, 8],\n    'learning_rate': [0.05, 0.1, 0.2, 0.3],\n    'gamma': np.arange(0.5, 0.9, 0.1),\n    'reg_alpha': np.arange(10, 80, 10),\n    'reg_lambda': np.arange(0, 10, 1),\n    'subsample': np.arange(0, 1, 0.1),\n    'colsample_bytree': np.arange(0, 1, 0.1)\n}\n\n# instanciate model\ngbm = XGBRegressor()\n# instanciate random search\nrandomized_mse = RandomizedSearchCV(param_distributions=gbm_param_grid, \n                                    estimator=gbm, \n                                    scoring='neg_mean_squared_error', \n                                    n_iter=150, \n                                    cv=4, \n                                    verbose=1,\n                                    random_state=0)\n# fit model\nrandomized_mse.fit(X_train,y_train)\n# print results\nprint(randomized_mse.best_params_, np.sqrt(np.abs(randomized_mse.best_score_)))\nprint(randomized_mse.best_estimator_)","9a119658":"# Define the model \nmodel = XGBRegressor(n_estimators=4000,learning_rate= 0.03,subsample= 0.80, colsample_bytree=0.70,max_depth= 2, reg_lambda= 9,reg_alpha=20,random_state=38)\n# Train the model \nmodel.fit(X_train, y_train, early_stopping_rounds = 20, eval_set=[(X_valid, y_valid)], verbose=False)\npreds_valid = model.predict(X_valid)\n\n# score model\nmodelname='xgboost'\nmse_score=mean_squared_error(y_valid, preds_valid, squared=False)\ntry:\n    BasicModelPerformanced_df.at[modelname,'mean_squared_error']=mse_score\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'mean_squared_error': mse_score})\nprint(mse_score)\n\n# generate predictions\npredictions = model.predict(test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.index,'target': predictions})\noutput.to_csv('advanced_xgboost_submission.csv', index=False)\nprint('Advanced xgboost submission complete')","ffa829e4":"# Get data\ntrain=pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest=pd.read_csv('..\/input\/30-days-of-ml\/test.csv',index_col='id')\n\ny = train.target\ntrain.drop(['target'], axis=1, inplace=True)\nprint('Target data separated')\n\n#remove none numeric features\nuseful_features = [c for c in train.columns if c not in (\"id\",\"target\",\"kfold\")]\nNfeatures= [col for col in useful_features if 'cont' in col]\n\ntrain = train[Nfeatures]\ntest = test[Nfeatures]\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(train, y, train_size=0.025, test_size=0.025,random_state=0)\ntest=test\nprint('Data split')","6c6e774f":"# Get data\ntrain=pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest=pd.read_csv('..\/input\/30-days-of-ml\/test.csv',index_col='id')\n\ny = train.target\ntrain.drop(['target'], axis=1, inplace=True)\nprint('Target data separated')\n\n# Convert categorical data to numeric data\nobject_cols = [col for col in train.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nordinal_encoder = OrdinalEncoder()\ntrain[object_cols] = ordinal_encoder.fit_transform(train[object_cols])\ntest[object_cols] = ordinal_encoder.transform(test[object_cols])\nprint('All category columns converted to ordinal')\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(train, y, train_size=0.025, test_size=0.025,random_state=0)\ntest=test\nprint('Data split')\n\n# Preview the ordinal-encoded features\n#print(train.head())","ad35af19":"# Get data\ntrain=pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest=pd.read_csv('..\/input\/30-days-of-ml\/test.csv',index_col='id')\nprint('Data Import Complete')\n\n# List of numeric columns\nnumerical_cols = [cname for cname in train.columns if train[cname].dtype in ['int64', 'float64']]\n# List of categorical columns\nobject_cols = [col for col in train.columns if 'cat' in col]\n\n# high and low cardinality\nlow_card_columns=[cname for cname in train.columns if train[cname].nunique() < 10 and train[cname].dtype == \"object\"]\nhigh_card_columns=[cname for cname in train.columns if train[cname].nunique() >= 10 and train[cname].dtype == \"object\"]\n\n# ordinal_encode columns with high cardinality\nXtrain = train.copy()\nXtest = test.copy()\nordinal_encoder = OrdinalEncoder()\ntrain[high_card_columns] = ordinal_encoder.fit_transform(train[high_card_columns])\ntest[high_card_columns] = ordinal_encoder.transform(test[high_card_columns])\n\n# one-hot_encode columns with low cardinality\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train[low_card_columns]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(test[low_card_columns]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train.index\nOH_cols_test.index = test.index\n\n# Relabel columns\nall_cols=OH_cols_train.columns\nnew_cols = [i for i in all_cols if isinstance(i, (int, float))]\nOH_cols_train=OH_cols_train[new_cols].add_prefix('cat_encode_')\nOH_cols_test=OH_cols_test[new_cols].add_prefix('cat_encode_')\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = train.drop(low_card_columns, axis=1)\nnum_X_test = test.drop(low_card_columns, axis=1)\n\n# Add one-hot encoded columns to numerical features\ntrain= pd.concat([num_X_train, OH_cols_train], axis=1)\ntest = pd.concat([num_X_test, OH_cols_test], axis=1)\ntrain_full=train.copy()\n# separate target data\ny = train.target\ntrain.drop(['target'], axis=1, inplace=True)\nprint('Target data separated')\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(train, y, train_size=0.025, test_size=0.025,random_state=0)\ntest=test\nprint('Data split')","9a1f3d26":"# Get data\ntrain=pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest=pd.read_csv('..\/input\/30-days-of-ml\/test.csv',index_col='id')\nprint('Data Import Complete')\n\n# List of numeric columns\nnumerical_cols = [cname for cname in train.columns if train[cname].dtype in ['int64', 'float64']]\n# List of categorical columns\nobject_cols = [col for col in train.columns if 'cat' in col]\n\n# high and low cardinality\nlow_card_columns=[cname for cname in train.columns if train[cname].nunique() < 20 and train[cname].dtype == \"object\"]\nhigh_card_columns=[cname for cname in train.columns if train[cname].nunique() >= 20 and train[cname].dtype == \"object\"]\n\n# ordinal_encode columns with high cardinality\nXtrain = train.copy()\nXtest = test.copy()\nordinal_encoder = OrdinalEncoder()\ntrain[high_card_columns] = ordinal_encoder.fit_transform(train[high_card_columns])\ntest[high_card_columns] = ordinal_encoder.transform(test[high_card_columns])\n\n# one-hot_encode columns with low cardinality\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train[low_card_columns]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(test[low_card_columns]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train.index\nOH_cols_test.index = test.index\n\n# Relabel columns\nall_cols=OH_cols_train.columns\nnew_cols = [i for i in all_cols if isinstance(i, (int, float))]\nOH_cols_train=OH_cols_train[new_cols].add_prefix('cat_encode_')\nOH_cols_test=OH_cols_test[new_cols].add_prefix('cat_encode_')\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = train.drop(low_card_columns, axis=1)\nnum_X_test = test.drop(low_card_columns, axis=1)\n\n# Add one-hot encoded columns to numerical features\ntrain= pd.concat([num_X_train, OH_cols_train], axis=1)\ntest = pd.concat([num_X_test, OH_cols_test], axis=1)\ntrain_full=train.copy()\n# separate target data\ny = train.target\ntrain.drop(['target'], axis=1, inplace=True)\nprint('Target data separated')\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(train, y, train_size=0.025, test_size=0.025,random_state=0)\ntest=test\nprint('Data split')","9fba5683":"### Ordinal encoding of Categorical data","e96bc83a":"## Basic Ridge Model","890caf13":"## Basic xgboost model","0646a418":"## Dealing with Categorical Data\nTo start with we are simply going to use ordinal encoding to convert categorical data to numeric data, lated on we will use feature engineering to improve on this. (if you skip to the bottom of the notebook you will find code to use one-hot encoding etc. You can run that now to see how alternate approaches affect the basic models).","8df36e74":"### Observation on numerical training data\n\n    \u2022 There are 14 columns of numerical data.\n    \u2022 Each column contains 300000 rows of data.\n    \u2022 All numbers are between -1 and 1.1 \n    \u2022 All standard deviations are between 0.2 and 0.3\n    \n### Observation on target training data\n\n    \u2022 Between 0.140 and 10.41\n    \u2022 mean 8.24","4023d808":"### Ignore Categorical data","d175fb44":"## Build xgboost model (with kfold validation)\ngpu accelerator required","a8af60b0":"## Ensemble","d8b9db26":"## Check for missing data","2334b8cd":"### Observations on numeric data visualisations\n\n    \u2022 Each variable appears to made up of several normal distributions \n    \u2022 Test and training datasets have similar distributions     ","5328fefc":"### Observations on random forest model with category data\n\nWithout any work other than deleting the categorical data this submission scores 0.74036. The highest score on leaderboard at this time is 0.71778 which is a reasonable base line.\nWith all categories converted to columns using one-hot enoding we get a score of 0.74003 where as with high cardinality columns converted with ordinal encoding and low with one-hot encoding we get 0.73918. So a very small improvement but an improvement!!!","08ae9f38":"# Advanced Models","eb9da2a8":"## Visualising target data","315b37ef":"# Set up\nIn this part of the notebook we will import the required libraries, import the data and take a first look at it.","773456f3":"## Build Randomforest model (with randomized cv search hypertuning)","55b9d4cb":"## Visualize categorical test data","7fe76016":"## Visualize numeric test data","f4afbda9":"## Basic RandomForest model\n\nLet's start by building a basic model and getting a score that we can compare any future results to see whether feature engineering or new models are better or worse than doing nothing!\n\nFor simplicity we will only model numeric features and we will use a Random Forest Generator with default parameters as the model. ","ff9002c1":"### Observations on basic model performance\nAt this stage we have used the mean sqaured error is a basic measure of model performance we have not yet introduced cross validation and the model performance may change because of under or over fitting, we have also only used a small amount of the available data to speed up the initial model evaluation. \n\nTHe models achieved scores of between 0.7412 and 0.7306 (Based on numeric data and categorical data being converted to ordinal data) \n\nXGboosting and LGBMRegressor performed best out of the models we evaluated.","ccee5c18":"### One-hot encoding | Ordinal Encoding","05bab9f0":"## Categorical overview of training data","d689299b":"## Statistical overview of training data","e6a0dfcc":"## Build xgboost model (with randomized cv search hyper tuning)","e8a4c1ca":"## Visualize categorical training data","88e2ad48":"### Observation on categorical data\n\n    \u2022 There are 10 columns of categorical data.\n    \u2022 9 of the columns have low cardinality, 1 has high cardinality\n    \u2022 The data in the categorical columns are single digit alphabetical ranges a-b,a-d,a-g,a-o \n    \n    - The distributions of test and training data are very similar","211933f3":"## Split training data","a9a50bcb":"## Look at correlation between features and target ","b580ea18":"### Prepare data","d176a000":"### Observations on xgboost with kfold model\n\nThe hypertuned model only scored 0.74081 which is lower than the basic model suggesting that more tuning is required.","5583a299":"### Observations on xgboost with kfold model\n\ny = train.targetThe xgboost with kfold model scored 0.71908 an improvement of 0.01058","bd87ba0f":"### Skewness and Kurtosis\n\nSkewness is a measure of symmetry, or more precisely, the lack of symmetry.\n\nKurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution.","4c5da0c5":"# Data Cleaning & Feature Engineering\nIn this section of the notebook we will take a closer look at the data. We will look for and deal with any missing data and identify any outliers. We will move on to look at the categorical data and work out the best way to handle it. \n\nFinally we will attempt to use feature engineering to optimize our data\/models. \n\nWhere different approaches can be used we will aim to output objects with standard names so that we can test different techniques with different models by running different code blocks.","98e17a6e":"## First look at training data","bafd173f":"### Observations on correlation between features and target \n    \u2022 There correlations between individual features and target seem very weak.  \n    \u2022 There are weak correlations between both continuous and categorical features.\n    \u2022 There are both posative and negative correlations.\n    \n    \u2022 cat5 and cont 2 have the strongest correlations.\n    \u2022 most of the cat columns have positive correlations\n    \u2022 most of the cont columns have negative correlations   \n    \u2022 cat5 and cont 2 have the opposite correlations to their type.","cefd8236":"# Visualize data","1d102d66":"## Categorical overview of test data","a83614af":"### Observation on numerical test data\n\n    \u2022 There are 14 columns of numerical data.\n    \u2022 Each column contains 200000 rows of data.\n    \u2022 All numbers are between -1 and 1.1 \n    \u2022 All standard deviations are between 0.2 and 0.3\n    \n    \u2022 The distribution of values in test and training data are very similar   ","7c1e7744":"## Mutuality\n\nWhile correlation is limited to linear relationships, mutual information can be used for any kind of relationships. Mutual Information describes how presence of a given features reduces the uncertainty of the target variable.\n\nMI's lower bound is 0 and there is no upper bound. The variables are independent if MI is 0.","f01e2f05":"## Visualize numeric training data","af43822e":"### One-hot all","fc90e335":"## Feature importance and Mutuality","a6adc1f3":"## Clustermapping features","7e36d790":"## Statistical overview of test data","8e56036e":"## First look at test data","d66fc441":"### Observations on xgboost model\n\nThe basic xgboost model scored 0.72966 an improvement of 0.00952 ","f291bb58":"# Credit where credit's due\n\n#### The following video was really useful! \n\nScikit-optimize for LightGBM Tutorial https:\/\/www.youtube.com\/watch?v=AFtjWuwqpSQ&list=PLqFaTIg4myu9uAPsqXBBZRr8kcj9IvAIf&index=3 Luca Massaron\n\n#### The following notebook's were super inspirational in creating this notebook, and gave me insights I may not have otherwise come up with. Thank you all for sharing your code it's awesome! \n\nLayout for visualisations\nhttps:\/\/www.kaggle.com\/yogidsba\/eda-kfold-30-day-ml by yogidsba\n\nFeature Importance chart\nhttps:\/\/www.kaggle.com\/rishirajacharya\/30-days-xgboost-with-5-folds-eda-gpu by rishirajacharya (Rishiraj Acharya)\n\nFeature Engineering https:\/\/www.kaggle.com\/pranav2109\/featureengineering-ensemble-xgbpipeline by pranav2109 (Pranav Agarwal)\n\nXGBoost Kfold\nhttps:\/\/www.kaggle.com\/venkatkumar001\/30days-xgb-eda by Venkatkumar R\n\nHyperparameter Tuning LGBM https:\/\/www.kaggle.com\/prk007\/hyperparameter-tuning-lgbm-with-optuna by prk007\n\nHyperparameter Tuning Random forest https:\/\/www.kaggle.com\/arjunprasadsarkhel\/simple-random-forest-with-hyperparameter-tuning by \nArjun Prasad Sarkhel","8a4c196e":"# Modelling\n\nWe are trying to predict an unknown value from a number of known numeric and categorical features using supervised learning which is essentially a regression project. We'll start by looking at a number of regressors with the basic settings to see which look like th best candidates then we will pick the best ones and hpertune their parameters.","bebe7b21":"## Basic SVR Model","de496af9":"## Distribution of categorical data within target","7d1b771d":"## Alternate ways of dealing with with categorical data.\n\nIn our basic model we ignored the categorical data, in an attempt to try and improved the result we could convert those columns to ordinal values. We are still using the default parameters for the random forrest and for speed only using 20% of the training data to fit the model. ","2d826d12":"## Catboost","f4d6f063":"### Observations on LGBM model\nThe mean sqaured error for the Basic LGNM model is relatively low at 0.73510. It looks like it could be worth investigating hyper tune the parameters.","bca36418":"## Basic LGBMRegressor","84d7fffc":"### Observations on sparse categorical data\n\nConverting low cardinality categorical data to sparse data improved the score of the basic random forest model from 0.74036 to 0.7401 which is an improvement but not a significant one.","04ffa2dc":"# About 30 days of machine learning\n\nThe dataset used for this competition is synthetic, but based on a real dataset. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.\n\nThis is a regression problem.","b9f4b67b":"## Feature Importance (randomforestregressor)","b63c5c2e":"### Observations on SVM model\nThe mean sqaured error on a svm is huge compared with other models confirmng that even wit hyper tuning this is not a likely candidate.","210e7952":"## Get the data","827db61c":"Observations on Catboost model\nThe mean sqaured error for the basic catboost model is 0.74408 compared with other models this score is relatively high so its unlikely to proof the best model.","fd173d2b":"# Review the Basic Models","d3441e0f":"# About David Coxon and this notebook\n\nI am David Coxon, I'm an IT manager and having been using python to analyse data within by organisation for several years. I learnt Data Science through online training course from plases like DataCamp and Kaggle. I was a moderator for datacamps slackgroup for a couple of years, and signed up to the 30 days of ML programme as a bit of a refresher.\n\nPlease this notebook is very much a work in progress that i am still adding to and amending. The Notebooks primary purpose is to gather together information about the dataset and to experiment with different approaches for personal study. Its not intended as a training resource for others and it borrows many elements from shared code, i have tried to credit all those whose code was useful at the end of this note book. Please check those out and upvote if you find them helpful. Please feel to upvote this note book if you find it useful. \n\nNote: The code in this notebook is quite raw, in that its not as effeficient as it could be. I could propably write some functions to make it a lot less verbose and run more quickly. I could also possibly be more consistent in the way i've named variables, functions, list and dataframes, but like i said earlier its a work in progress. "}}