{"cell_type":{"2f36d928":"code","4f9cb1ef":"code","79ea2c06":"code","ad698229":"code","f95240b6":"code","1c51d68e":"code","2690e69a":"code","304d1c48":"code","96eee57b":"code","a45188ba":"code","cb360aba":"code","47f51b44":"code","f83e89e1":"code","8533a868":"code","f3515ae0":"code","d2bb69bd":"code","b5ae64b7":"code","fadca92d":"code","e05eae54":"code","b477ba67":"code","364056c2":"code","67602d7e":"code","68c6e9f9":"code","dc0b030d":"code","2023b293":"code","d232f756":"code","432abe9b":"code","188850c4":"code","382d5018":"code","56ced169":"code","053aac23":"code","3779c6f9":"code","ff0c665f":"code","abb7acd2":"code","b5ae9991":"code","c1e2e4ff":"code","4b382d40":"code","814bf4a7":"markdown","b209cebf":"markdown","bd342416":"markdown","b58d9310":"markdown","2f9cc23c":"markdown","74d22f86":"markdown"},"source":{"2f36d928":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport datatable as dt # reads data faster than pandas\n\nimport gc #to manage ram \nimport subprocess\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\n#from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\nimport tensorflow as tf","4f9cb1ef":"%%time\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv', nrows = 750000)\n\nprint(df_train.shape)","79ea2c06":"# this method reads data faster and doesn't  memory errors as frequent as pandas.\n\n# %%time\n# train_dt = dt.fread(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\").to_pandas()\n\n# train_dt.head()throw","ad698229":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","f95240b6":"train = reduce_memory_usage(df_train, verbose=True)","1c51d68e":"# train.head()","2690e69a":"del df_train\ngc.collect()","304d1c48":"# train.shape","96eee57b":"print(train['target'].value_counts())\nsns.countplot(x = train['target'],data = train);","a45188ba":"train['std'] = train.std(axis=1)\ntrain['min'] = train.min(axis=1)\ntrain['max'] = train.max(axis=1)\n\ngc.collect()","cb360aba":"continous_cols= ['f'+str(i) for i in range(242)] + ['std']\ncontinous_cols.remove('f22')\ncontinous_cols.remove('f43')\ncategorical_cols = ['f'+str(i) for i in range(242,285)]+['f22','f43','min', 'max']\ncols = continous_cols + categorical_cols\n\ngc.collect()","47f51b44":"#creating a random temperory dataframe to get an idea of how the data is distributed \n\nnp.random.seed(2110)\ntmp_train = train.sample(10000)","f83e89e1":"# plotting only first 60 features to give an idea\ni = 1\nplt.figure()\nfig, ax = plt.subplots(15, 4,figsize=(20, 22))\nfor feature in continous_cols[:60]:\n    plt.subplot(15, 4,i)\n    sns.histplot(tmp_train[feature], kde=True,bins=100, label='train_'+feature)\n    #sns.histplot(tmp_test[feature],color=\"orange\", kde=True,bins=100, label='test_'+feature)\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","8533a868":"del tmp_train\ngc.collect()","f3515ae0":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\ntrain[continous_cols] = scaler.fit_transform(train[continous_cols])","d2bb69bd":"gc.collect()","b5ae64b7":"Y = train['target']\nX = train.drop(['target', 'id'], axis=1)\n\nx_train, x_val, y_train, y_val = train_test_split(X, Y, train_size=0.7, test_size=0.3, random_state=0)\n\ndel train\ngc.collect()","fadca92d":"X_train_expanded = tf.expand_dims(x_train, axis=-1)\nX_val_expanded = tf.expand_dims(x_val, axis=-1) \n\ndel x_train\ndel x_val\ngc.collect()","e05eae54":"model = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Input(shape=(X_train_expanded.shape[1], 1,)))\n\nmodel.add(tf.keras.layers.Conv1D(128, kernel_size=1, activation='relu', padding='same'))\nmodel.add(tf.keras.layers.Conv1D(128, kernel_size=1, activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=1, strides=1))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Conv1D(256, kernel_size=1, activation='relu', padding='same'))\nmodel.add(tf.keras.layers.Conv1D(128, kernel_size=1, activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=1, strides=1))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Conv1D(512, kernel_size=1, activation='relu', padding='same'))\nmodel.add(tf.keras.layers.Conv1D(128, kernel_size=1, activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=1, strides=1))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.add(tf.keras.layers.Dense(512, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.00001), bias_regularizer=tf.keras.regularizers.l2(0.0001)))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(512, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.00001), bias_regularizer=tf.keras.regularizers.l2(0.0001)))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(512, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.00001), bias_regularizer=tf.keras.regularizers.l2(0.0001)))\nmodel.add(tf.keras.layers.Dropout(0.5))\n\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()","b477ba67":"auc = tf.keras.metrics.AUC(name='aucroc')\n\n#optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n#optimizer = tf.keras.optimizers.Nadam( learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', auc])","364056c2":"earlystopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5, verbose=1, restore_best_weights=True)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.3, patience=3, verbose=1, min_delta=1e-4)\n\ncallbacks = [earlystopping, reduce_lr]\n\nhistory = model.fit(x=X_train_expanded, y=y_train, batch_size=1024, shuffle=True, epochs=20, validation_data=(X_val_expanded, y_val), callbacks=callbacks)","67602d7e":"model.save('TPS_Oct_Model.h5')","68c6e9f9":"del X_train_expanded\ndel X_val_expanded\ngc.collect()","dc0b030d":"history_dict = history.history\nhistory_dict.keys()","2023b293":"loss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nplt.plot(loss_values, 'b', label='Training loss')\nplt.plot(val_loss_values, color = 'orange', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","d232f756":"acc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\n\nplt.plot(acc_values, 'b', label='accuracy')\nplt.plot(val_acc_values, color = 'orange', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","432abe9b":"auc = history_dict['aucroc']\nval_auc = history_dict['val_aucroc']\n\nplt.plot(auc, 'b', label='aucroc')\nplt.plot(val_auc, color = 'orange', label='val_aucroc')\nplt.title('Training and validation aucroc')\nplt.xlabel('Epochs')\nplt.ylabel('aucroc')\nplt.legend()\nplt.show()","188850c4":"gc.collect()","382d5018":"test_dt = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")\nsample = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")","56ced169":"test = test_dt.drop(['id'], axis = 1)\n\ngc.collect()","053aac23":"test['std'] = test.std(axis=1)\ntest['min'] = test.min(axis=1)\ntest['max'] = test.max(axis=1)","3779c6f9":"del test_dt\ngc.collect()","ff0c665f":"test[continous_cols] = scaler.transform(test[continous_cols])","abb7acd2":"test_extended = tf.expand_dims(test, axis=-1);\n\ndel test\ngc.collect()","b5ae9991":"sub = pd.DataFrame()\nsub['id'] = sample['id']\nsub['target'] = model.predict(test_extended)\nsub = sub.set_index('id')","c1e2e4ff":"sub.head()","4b382d40":"sub.to_csv('submission.csv')","814bf4a7":"We can reduce the datset using the below function without altering the data. Thanks to https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro  ","b209cebf":"The distribution of the targetvalues is balanced.","bd342416":"The dataset doesn't contain any missing values and the train and test data feature distributions are similar.","b58d9310":"We will delete any useless dataframes to conserve RAM. gc - garbage collector","2f9cc23c":"As the dataset is very large, we cannot load all the dataset into RAM (it is giving memory error). You can use chunks of rows to train a model and save the model for subsequent training chunks. But here I'm going to take random data points(rows) from the data and use it to train and validate the model. This maynot be the best way to do (most probably(-_-).","74d22f86":"Try using other scalers."}}