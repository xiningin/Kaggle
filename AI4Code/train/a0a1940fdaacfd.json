{"cell_type":{"29165ec2":"code","ef282bf5":"code","d763f973":"code","d91c457d":"code","ca71bdbb":"code","a17a4457":"code","e5d466dd":"code","da80316c":"code","159a8806":"code","cbab168b":"code","c2cd5bfd":"code","b62e415d":"code","61db837e":"code","024af9e4":"code","a5569ba6":"code","badb4393":"code","1bc5db99":"code","cd8e2a7d":"code","597f68fa":"code","431ed30e":"code","90f0cac9":"code","22efe164":"code","4303141b":"code","ad2d9eb9":"code","1045a1f9":"markdown","d2e95103":"markdown","5da6ae40":"markdown","59fba950":"markdown","80a10932":"markdown","bf14280e":"markdown","6ec19699":"markdown","76aef4cb":"markdown","e1d81a28":"markdown","1632556c":"markdown","528b0c74":"markdown","3d08e816":"markdown","689a6ccc":"markdown","e93ae5dd":"markdown","3d1452a2":"markdown","c6d4c52c":"markdown","edfb385c":"markdown","5be3afe7":"markdown","608711bb":"markdown"},"source":{"29165ec2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef282bf5":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndata = pd.concat([train,test]).reset_index(drop=True)\ndata.info()","d763f973":"data['Fare'] = data['Fare'].fillna(data.query('Pclass==3 & Embarked==\"S\"')['Fare'].median())\ndata['Embarked'] = data['Embarked'].fillna('S')","d91c457d":"data['Family_size'] = data['SibSp']+data['Parch']\ndata['Family_size_bin'] = 0\ndata.loc[(data['Family_size']>=1) & (data['Family_size']<=3),'Family_size_bin'] = 1\ndata.loc[(data['Family_size']>=4) & (data['Family_size']<=6),'Family_size_bin'] = 2\ndata.loc[(data['Family_size']>=7) ,'Family_size_bin'] = 3","ca71bdbb":"data['Title'] = data['Name'].map(lambda x: x.split(', ')[1].split('. ')[0])\ndata['Title'].replace(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer', inplace=True)\ndata['Title'].replace(['Don', 'Sir',  'the Countess', 'Lady', 'Dona'], 'Royalty', inplace=True)\ndata['Title'].replace(['Mme', 'Ms'], 'Mrs', inplace=True)\ndata['Title'].replace(['Mlle'], 'Miss', inplace=True)\ndata['Title'].replace(['Jonkheer'], 'Master', inplace=True)\ndata['Title'] = data['Title'].map({'Officer':0, 'Royalty':1, 'Mr':2, 'Miss':3, 'Mrs':4, 'Master':5})\ndata['name_len'] = data['Name'].map(lambda x: len(x))","a17a4457":"title_list = data['Title'].unique().tolist()\nfor t in title_list:\n    index = data[data['Title']==t].index.values.tolist()\n    age = data.iloc[index]['Age'].mean()\n    age = np.round(age,1)\n    data.iloc[index,5] = data.iloc[index,5].fillna(age)\n\ndata['Age_bin'] = 0\ndata.loc[(data['Age']>18) & (data['Age']<=60),'Age_bin'] = 1\ndata.loc[(data['Age']>60),'Age_bin'] = 2","e5d466dd":"data['Last_name'] = data['Name'].apply(lambda x: x.split(\",\")[0])\n\ndata['Family_survival'] = 0.5 #\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u5024\n\nfor grp, grp_df in data.groupby(['Last_name', 'Fare']):\n    if (len(grp_df) != 1):\n        for index, row in grp_df.iterrows():\n            smax = grp_df.drop(index)['Survived'].max()\n            smin = grp_df.drop(index)['Survived'].min()\n            passID = row['PassengerId']\n\n            if (smax == 1.0):\n                data.loc[data['PassengerId'] == passID, 'Family_survival'] = 1\n            elif (smin == 0.0):\n                data.loc[data['PassengerId'] == passID, 'Family_survival'] = 0\n            #\u30b0\u30eb\u30fc\u30d7\u5185\u306e\u81ea\u8eab\u4ee5\u5916\u306e\u30e1\u30f3\u30d0\u30fc\u306b\u3064\u3044\u3066\n            #1\u4eba\u3067\u3082\u751f\u5b58\u3057\u3066\u3044\u308b \u2192 1\n            #\u751f\u5b58\u8005\u304c\u3044\u306a\u3044(NaN\u3082\u542b\u3080) \u2192 0\n            #\u5168\u54e1NaN \u2192 0.5\n\n#\u30c1\u30b1\u30c3\u30c8\u756a\u53f7\u3067\u30b0\u30eb\u30fc\u30d4\u30f3\u30b0\nfor grp, grp_df in data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        #\u30c1\u30b1\u30c3\u30c8\u756a\u53f7\u304c\u540c\u3058\u4eba\u304c2\u4eba\u4ee5\u4e0a\u3044\u308b\u5834\u5408\n        #\u30b0\u30eb\u30fc\u30d7\u5185\u30671\u4eba\u3067\u3082\u751f\u5b58\u8005\u304c\u3044\u308c\u3070'Family_survival'\u30921\u306b\u3059\u308b\n        for ind, row in grp_df.iterrows():\n            if (row['Family_survival'] == 0) | (row['Family_survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    data.loc[data['PassengerId'] == passID, 'Family_survival'] = 1\n                elif (smin == 0.0):\n                    data.loc[data['PassengerId'] == passID, 'Family_survival'] = 0","da80316c":"data['Sex'] = data['Sex'].map({'male':0, 'female':1})\ndata['Embarked'] = data['Embarked'].map({'S':0, 'Q':1, 'C':2})","159a8806":"data = data.drop(['PassengerId','Name','Age','SibSp','Parch','Ticket','Cabin','Family_size','Last_name'], axis=1)\ndata.info()","cbab168b":"train = data[:891]\ntest = data[891:]\ntest.head()","c2cd5bfd":"import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport random","b62e415d":"def seed_all(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_all(3141)","61db837e":"feature = [\"Pclass\",\"Sex\",\"Fare\",\"Embarked\",\"Family_size_bin\",\"Title\",\"name_len\",\"Age_bin\",\"Family_survival\"]\ntarget = \"Survived\"","024af9e4":"class Net(nn.Module):\n    def __init__(self,n1,n2,n3):\n        super(Net,self).__init__() \n        self.fc1 = nn.Linear(len(feature),n1)\n        self.fc2 = nn.Linear(n1,n2)\n        self.fc3 = nn.Linear(n2,n3)\n        self.fc4 = nn.Linear(n3,2)\n    \n    def forward(self,x): \n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = self.fc4(x) \n        return x \n    \nmodel=Net(pow(2,10),pow(2,9),pow(2,7))\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\nmodel","a5569ba6":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\ntrain_x,val_x,train_y,val_y = train_test_split(train[feature],train[\"Survived\"],test_size=0.2,random_state=123)\n\n# to array\ntrain_X = np.array(train_x)\ntrain_Y = np.array(train_y)\nval_X = np.array(val_x)\nval_Y = np.array(val_y)\n\n# \u6a19\u6e96\u5316\nscaler = StandardScaler()\nscaler.fit(train_X)\ntrain_X = scaler.transform(train_X)\nval_X = scaler.transform(val_X)\n\n# array to tensor\ntrain_X = torch.from_numpy(train_X).float()\ntrain_Y = torch.from_numpy(train_Y).long()\n\nval_X = torch.from_numpy(val_X).float()\nval_Y = torch.from_numpy(val_Y).long()\n\n# dataset, dataload \ntrain_dataset = TensorDataset(train_X,train_Y)\nval_dataset = TensorDataset(val_X,val_Y)\n\ntrain_dataloader = DataLoader(train_dataset,batch_size=128,shuffle = True)\nval_dataloader = DataLoader(val_dataset,batch_size=32,shuffle = False)\n\nfor a,b in train_dataloader:\n    print(a)\n    print(b)\n    break","badb4393":"total_loss = 0 # Initializing total loss\nmodel.train()\nfor train_x,train_y in train_dataloader:\n        print(train_x)\n        print(train_y)\n        train_x, train_y = Variable(train_x),Variable(train_y) # Wrap the tensor and record the calculation operation\n        optimizer.zero_grad() # Set the gradient of optimizer to 0\n        output = model(train_x) # prediction\n        loss = criterion(output,train_y) # calculationg loss between predictions and answers\n        loss.backward() # backward\n        optimizer.step() # optimizing weight\n        total_loss += loss.item() # integration of loss\n        break","1bc5db99":"output","cd8e2a7d":"torch.max(output.data,1)","597f68fa":"total_loss","431ed30e":"def training(train_dataloader,model):\n    \n    total_loss = 0\n    model.train()\n    \n    for train_x,train_y in train_dataloader:\n        train_x, train_y = Variable(train_x),Variable(train_y)\n        optimizer.zero_grad()\n        output = model(train_x)\n        \n        loss = criterion(output,train_y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        \n    return model,total_loss\n\n\ndef val_eval(val_dataloader,model):\n    \n    total_val_loss = 0\n    model.eval()\n    \n    with torch.no_grad():\n        for val_x,val_y in val_dataloader:\n            output = model(val_x)\n            valloss = criterion(output,val_y)\n            total_val_loss += valloss.item()\n            \n    return total_val_loss","90f0cac9":"model=Net(pow(2,10),pow(2,9),pow(2,7))\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\nall_trainloss = []\nall_valloss = []\n\nall_trainscore = []\nall_valscore = []\n\nbestscore = -1\n\ndef calc_accuracy(x,y,model):\n    preds = model(x)\n    preds2 = torch.max(preds.data,1)[1]\n    return accuracy_score(y,preds2) \n\nfor epoch in range(200):\n    model,trainloss = training(train_dataloader,model)\n    all_trainloss.append(trainloss)\n    valloss = val_eval(val_dataloader,model)\n    all_valloss.append(valloss)\n    trainscore = calc_accuracy(train_X,train_Y,model)\n    all_trainscore.append(trainscore)\n    valscore = calc_accuracy(val_X,val_Y,model)\n    all_valscore.append(valscore)\n    if bestscore  < valscore:\n        bestscore = valscore\n        state = {\n                    'state_dict': model.state_dict(),\n                    'optimizer_dict': optimizer.state_dict(),\n                    \"bestscore\":bestscore\n                }\n\n        torch.save(state, \"model1.pth\") ","22efe164":"import matplotlib.pyplot as plt\nx = np.arange(200)\nplt.plot(x,all_trainscore)\nplt.plot(x,all_valscore)","4303141b":"state = torch.load(\".\/model1.pth\")\nmodel.load_state_dict(state[\"state_dict\"])","ad2d9eb9":"submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntest_X = np.array(test[feature])\ntest_X = scaler.transform(test_X)\ntest_X = torch.from_numpy(test_X).float()\npreds = model(test_X)\nsubmission[\"Survived\"] = torch.max(preds.data,1)[1] \nsubmission.to_csv(\"sub3.csv\",index = False)\n\nsubmission","1045a1f9":"* \u5bb6\u65cf\u3001\u30c1\u30b1\u30c3\u30c8\u306b\u3064\u3044\u3066\n  * \u3084\u308b\u3068\u3044\u3044\u3089\u3057\u3044\u3002\n  * \u3042\u3063\u305f\u30b3\u30fc\u30c9\u3092\u305d\u306e\u307e\u307e\u8cbc\u3063\u305f\u306e\u3067\u3088\u304f\u5206\u304b\u3063\u3066\u306a\u3044","d2e95103":"## \u51fa\u529b\u306e\u78ba\u8a8d\u7528\u306b\u4e00\u56de\u30c6\u30b9\u30c8","5da6ae40":"## \u7279\u5fb4\u91cf\u3065\u304f\u308a\n\u3044\u308d\u3044\u308d\u898b\u3066\u826f\u3055\u305d\u3046\u306a\u3082\u306e\u3092\u30d1\u30af\u308a\u307e\u3057\u305f","59fba950":"* \u30a8\u30dd\u30c3\u30af\u6570200\u3067\u8d70\u3089\u305b\u308b\n* \u4e00\u756a\u3044\u3044\u611f\u3058\u3060\u3063\u305f\u72b6\u614b\u3092\u4fdd\u5b58\u3057\u3066\u304a\u304f\n* \u30a8\u30dd\u30c3\u30af\u6570\u306e\u63a8\u5b9a\u306e\u305f\u3081\u3060\u3051\u306b(\u56f3\u793a\u3059\u308b\u305f\u3081\u3060\u3051\u306b)\u6700\u521d\u306b\u30ea\u30b9\u30c8\u3092\u7528\u610f\u3057\u3066\u308b","80a10932":"## \u30c7\u30fc\u30bf\u53d6\u5f97\n* \u691c\u8a3c\u7528\u306b\u30c7\u30fc\u30bf\u3092\u5206\u5272\u3059\u308b\n* np.array\u306b\u3064\u3063\u3053\u3093\u3060\u3042\u3068\u3001\u6a19\u6e96\u5316\n* \u4ee5\u4e0bpytorch\u7279\u6709\n  * from_numpy : numpy.ndarray\u3092tensor\u306b\u5909\u3048\u308b\n  * TensorDataset : \u7279\u5fb4\u91cf\u3068\u6b63\u89e3\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u30bb\u30c3\u30c8 TensorDataset(\u7279\u5fb4\u91cf,\u6b63\u89e3)\n  * DataLoader: dataset\u3092\u30d0\u30c3\u30c1\u51e6\u7406\u3059\u308b\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306b\u5909\u3048\u308b Dataloader(data set\u3001batch size)","bf14280e":"* \u3088\u304b\u3063\u305f\u3084\u3064\u3092\u3082\u3063\u3066\u304d\u3066submission\u3092\u3064\u304f\u308b","6ec19699":"* \u6570\u5b57\u3058\u3083\u306a\u3044\u3084\u3064\u3092\u6570\u5b57\u306b\u5909\u3048\u308b\n* label coding\u3088\u308aone hot\u306e\u307b\u3046\u304cNN\u306b\u306f\u826f\u3044\u3089\u3057\u3044\u3002\u3002\u3002*1 (\u30e2\u30c7\u30eb\u3064\u304f\u308b\u3068\u304d\u306b\u30a2\u3060\u3063\u305f\u306e\u3067label\u306b\u3057\u305f)\n* *1 https:\/\/yolo-kiyoshi.com\/2018\/12\/23\/post-1016\/","76aef4cb":"* \u540d\u524d\u306e\u52a0\u5de5\u305d\u306e1\n  * Dr\u3068\u304bMiss\u3068\u304bCap\uff08\u30ad\u30e3\u30d7\u30c6\u30f3\uff09\u3068\u304b\u3042\u308b\n  * \u5358\u7d14\u306b\u7537\u5973\u3067\u5206\u3051\u308b\u3088\u308a\u3082\u591a\u304f\u306e\u60c5\u5831\u3092\u4e0e\u3048\u3066\u304f\u308c\u305d\u3046\n  * \u5206\u3051\u305f\u5f8c\u3001one-hot encoding\u3059\u308b\u3068\u304d\u306b\u6b21\u5143\u304c\u5897\u3048\u308b\u306e\u304c\u5acc\u3060\u3063\u305f\u306e\u3067\u3001\u5c0f\u6570\u306e\u656c\u79f0\u3092\u307e\u3068\u3081\u308b\n  * \u540d\u524d\u306e\u8868\u8a18\u306e\u9577\u3055\u3068\u751f\u5b58\u7387\u306b\u3082\u5f37\u3044\u76f8\u95a2\u304c\u3042\u308b\u3001\u5049\u3044\u4eba\u307b\u3069\u540d\u524d\u304c\u9577\u3044","e1d81a28":"## Pytorch\n* document\u3092\u898b\u3088\u3046\uff01\uff01\nhttps:\/\/pytorch.org\/docs\/stable\/index.html","1632556c":"* \u5bb6\u65cf\u306e\u4eba\u6570\n  * sibsp,parch\u305d\u308c\u305e\u308c\u5358\u4f53\u3060\u3068survived\u3068\u306e\u76f8\u95a2\u304c\u5206\u304b\u3089\u306a\u3044\n  * \u5358\u7d14\u306b\u8db3\u3057\u305f\u3082\u306e\uff08\u8239\u306b\u4e57\u3063\u3066\u308b\u5bb6\u65cf\u306e\u4eba\u6570\u3068\u8003\u3048\u3089\u308c\u308b\uff09\u306f\u76f8\u95a2\u304c\u3042\u308b\n  * \u307c\u3063\u3061\u3068\u5927\u5bb6\u65cf\u306f\u6b7b\u306b\u304c\u3061\n  * 2~4\u4eba\u5bb6\u65cf\u306e\u751f\u5b58\u7387\u304c\u9ad8\u3044(sibsp+parch<=3\u3067\u5d16)","528b0c74":"* \u7d50\u69cb\u3042\u308b\u5e74\u9f62\u306enull\u57cb\u3081\n  * \u5e74\u9f62\u3068\u751f\u5b58\u7387\u306b\u3082\u76f8\u95a2\u304c\u3042\u308b\n  * \u5b50\u4f9b\u306840\u6b73\u3050\u3089\u3044\u306e\u751f\u5b58\u7387\u304c\u9ad8\u3044\n  * \u5e74\u9f62\u3068Title\u306f\u95a2\u4fc2\u304c\u3042\u308a\u305d\u3046\u3060\u3063\u305f\u306e\u3067\u3001\u305d\u308c\u305e\u308c\u306e\u30b0\u30eb\u30fc\u30d7\u306e\u5e73\u5747\u3067\u57cb\u3081(Pclass,Title\u3067\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3059\u308b\u3088\u308a\u3053\u3063\u3061\u306e\u307b\u3046\u304c\u826f\u304b\u3063\u305f\uff09\n  * \u53c2\u8003\u306b\u3057\u3066\u305f\u306e\u304cbin\u306b\u3057\u3066\u305f\u306e\u3067\u30d1\u30af\u3063\u305f\u3002\u5e74\u9f62\u306e\u307e\u307e\u3067\u3082\u3044\u3044\u53ef\u80fd\u6027\u3042\u308b","3d08e816":"# \u3042\u3068\u306f\u3084\u308b\u3060\u3051\n## training\u3068validate\u306e\u95a2\u6570\n * \u4e2d\u8eab\u307f\u305f\u3089\u96f0\u56f2\u6c17\u304c\u5206\u304b\u308b","689a6ccc":"## \u30e2\u30c7\u30eb\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u69cb\u7bc9\n  * \u3053\u3093\u306a\u611f\u3058\u3067\u304b\u306a\u308a\u76f4\u611f\u7684\u306b\u66f8\u3051\u308b\n  * \u4eca\u56de\u306f4\u5c64\u306b\u3057\u305f(\u3042\u307e\u308a\u8003\u3048\u3066\u306a\u3044)\n  * \u30ce\u30fc\u30c9\u306e\u6570\u3082\u9069\u5f53\u3002\u304b\u306a\u308a\u8a66\u3057\u305f\uff08\u5168\u54e1\u751f\u5b58\u3068\u304b\u5168\u54e1\u6b7b\u4ea1\u3092\u4f55\u56de\u3082\u3057\u305f\uff09\n  * forward(\u9806\u4f1d\u64ad)\u306e\u95a2\u6570\u306f\u5f53\u7136relu\u4ee5\u5916\u306b\u3082\u305f\u304f\u3055\u3093\u3042\u308b\n  * criterion:\u640d\u5931\u95a2\u6570\u3092\u6c7a\u3081\u308b\u3002MSE\u3082\u3042\u308b\u3088\uff08\u3042\u307e\u308a\u8003\u3048\u305a\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u306b\u3057\u305f\uff09\n  * optimizer:\u6700\u9069\u5316\u624b\u6cd5\u3092\u6c7a\u3081\u308b\u3002(lr\u306f\u5b66\u7fd2\u7387)","e93ae5dd":"* \u518d\u73fe\u6027\u306e\u305f\u3081\u306b\u4e71\u6570\u3092\u56fa\u5b9a\u3059\u308b\n* \u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3060\u3051\u3067\u5168\u90e8\u306e\u4e71\u6570\u3092\u56fa\u5b9a\u3067\u304d\u308b","3d1452a2":"\u52a0\u5de5\u3057\u3084\u3059\u3044\u3088\u3046\u306b\u304f\u3063\u3064\u3051\u3066\u305ftrain\u3068test\u3092\u5206\u3051\u308b","c6d4c52c":"* null\u57cb\u3081\n  * Fare\u304c1\u3064\u3068Embarked\u304c2\u3064\u3002\u3069\u3061\u3089\u3082\u751f\u5b58\u7387\u3068\u5f37\u3044\u76f8\u95a2\u304c\u3042\u308b\u3002\n  * Fare\u304c\u6b20\u640d\u3057\u3066\u3044\u305f\u4eba\u306fPclass=3\u3001Embarked=S\u3060\u3063\u305f\u306e\u3067\u305d\u308c\u306e\u4e2d\u592e\u5024\u3092\uff08Fare\u3068Pclass\u3001Embarked\u306b\u5f37\u3044\u76f8\u95a2\u304c\u3042\u308b)\n  *Embarked\u306fS\u304c\u5727\u5012\u7684\u306b\u591a\u3044\u306e\u3067S\u3092","edfb385c":"\u307e\u305a\u306fmodule\u306e\u30a4\u30f3\u30dd\u30fc\u30c8","5be3afe7":"* \u3044\u3089\u306a\u3044column\u3092\u6d88\u3059\n* \u6b20\u640d\u5024\u304c\u591a\u3059\u304e\u308bcabin\u306e\u6271\u3044\u306f\u672a\u3060\u306b\u5206\u304b\u3089\u306a\u3044\u306e\u3067\u6d88\u3059\n* PassengerID\u3082\u3042\u308b\u3068\u5f8c\u3067\u30d0\u30b0\u3063\u305f\u306e\u3067\u6d88\u3059","608711bb":"## \u7279\u5fb4\u91cf\u3065\u304f\u308a\u7d42\u308f\u308a\uff01\uff01\uff01"}}