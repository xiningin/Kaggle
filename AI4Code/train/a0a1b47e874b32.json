{"cell_type":{"c3b2480c":"code","308b2bd6":"code","e8a1e150":"code","ebc75574":"code","b7600aa1":"code","7367c251":"code","2ffc0fd1":"code","4ed152b0":"code","319a2610":"code","5546f1fa":"code","c30257dd":"code","f23a47eb":"code","41d83723":"code","fc79a7a5":"code","373d4af8":"code","7e85d261":"code","2617363b":"code","afa4b395":"code","298df685":"code","b2a77933":"code","e193c71d":"code","93ae7da7":"code","08b22b0f":"code","b9a6f78c":"code","f987bf1f":"code","219ebb6b":"code","cf13d178":"code","ff5028a9":"code","ec3c886a":"code","88952945":"code","8bf83a8f":"code","1e86ae68":"code","7087dd9d":"code","fb56ff22":"code","a48f72b3":"code","8337a97c":"code","209841dc":"code","b9629842":"code","1ba4c0b8":"code","fe01dff0":"code","48e77edd":"code","a99acbff":"code","3f02be83":"code","ae4aa6f0":"code","44e216ef":"code","d772072b":"code","049281dd":"code","69c6d805":"code","765af198":"markdown","cf7a6756":"markdown","bcf50a03":"markdown","3d7e8c9f":"markdown","15f3dc57":"markdown","66fa556d":"markdown","7c9f6336":"markdown","a2a91baa":"markdown","5e0f31cf":"markdown","c81bf5fc":"markdown","11c00b9c":"markdown","7e0c31d2":"markdown","9087e0d2":"markdown","624b7d91":"markdown","211b4ae6":"markdown","a55a8666":"markdown","5feda5ec":"markdown","70598ccd":"markdown","03c4bd9b":"markdown","0bf49a37":"markdown","dc707da4":"markdown","e6d4f4b0":"markdown","9fa517d9":"markdown","b8e297ed":"markdown","cb975783":"markdown","0add6e8b":"markdown","49400159":"markdown","3c7e9e92":"markdown","11c92e93":"markdown","4dd30b03":"markdown","a7815419":"markdown","2a024d3e":"markdown","60f88fe3":"markdown","cd0da4b4":"markdown","097de47b":"markdown","970e6d0b":"markdown","8aa52432":"markdown","3f154544":"markdown","4ec0b984":"markdown","7e05a3af":"markdown","9ee5f229":"markdown","65568bdc":"markdown","e7f84416":"markdown","3774f2a2":"markdown","3a670677":"markdown","12982f01":"markdown","ad55266a":"markdown","fc4292b2":"markdown","8db23aa8":"markdown","0ed8b07d":"markdown","2af98bf4":"markdown","b7331d54":"markdown","b9702d35":"markdown","fd11dfe1":"markdown","ddf4f214":"markdown","b14aff98":"markdown","56f24a65":"markdown","fd694401":"markdown","c7ddef4e":"markdown","18edfa38":"markdown","137664f4":"markdown","d3d66a82":"markdown","99a8a654":"markdown","d1795e1c":"markdown"},"source":{"c3b2480c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport seaborn as sns\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\n%matplotlib inline\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\ntrain = pd.read_csv('\/kaggle\/input\/mldub-comp1\/train_data.csv')\ntest = pd.read_csv('\/kaggle\/input\/mldub-comp1\/test_data.csv')\nsample_sub = pd.read_csv('\/kaggle\/input\/mldub-comp1\/sample_sub.csv')","308b2bd6":"train.head()","e8a1e150":"train.describe()","ebc75574":"sns.pairplot(train, diag_kind=\"kde\")","b7600aa1":"train.info()","7367c251":"train.isnull().sum()","2ffc0fd1":"fig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=train['comments'], y=train['target_variable'], color=('yellowgreen'), alpha=0.5)\n#plt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Comments scatter plot', fontsize=15, weight='bold' )\n\nfig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=train['videos'], y=train['target_variable'], color=('yellowgreen'), alpha=0.5)\n#plt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Videos scatter plot', fontsize=15, weight='bold' )\n\nfig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=train['photos'], y=train['target_variable'], color=('yellowgreen'), alpha=0.5)\n#plt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Photos scatter plot', fontsize=15, weight='bold' )","4ed152b0":"feature = 'comments'\ndf = train.copy()\nupper_limit = df[feature].quantile(0.95)\nlower_limit = df[feature].quantile(0.05)\ndf.loc[(df[feature] > upper_limit),feature] = upper_limit\ndf.loc[(df[feature] < lower_limit),feature] = lower_limit","319a2610":"fig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=df['comments'], y=df['target_variable'], color=('yellowgreen'), alpha=0.5)\n#plt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Comments scatter plot (no outliers)', fontsize=15, weight='bold' )","5546f1fa":"df['comments'].hist()","c30257dd":"def numeric(df, feature_list):\n    df_tmp = pd.DataFrame()\n    for feature in feature_list:\n        df_tmp[feature] = df[feature]\n        upper_limit = df_tmp[feature].quantile(0.95)\n        lower_limit = df_tmp[feature].quantile(0.05)\n        df_tmp.loc[(df_tmp[feature] > upper_limit),feature] = upper_limit\n        df_tmp.loc[(df_tmp[feature] < lower_limit),feature] = lower_limit\n        df_tmp[feature] = np.log1p(np.abs(df_tmp[feature]))\n    return df_tmp","f23a47eb":"df_tmp = pd.DataFrame()\ndf_tmp['added_year'] =  pd.to_datetime(train['date_added']).dt.year\ndf_tmp['current_year'] = pd.to_datetime('2020-01-01')\ndf_tmp['current_year'] = df_tmp['current_year'].dt.year\ndf_tmp['age'] = df_tmp['current_year'] - df_tmp['added_year']\ndf_tmp['age'].hist()","41d83723":"df_tmp['target_variable'] = train['target_variable']\nfig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=df_tmp['age'], y=df_tmp['target_variable'], color=('yellowgreen'), alpha=0.5)\n#plt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Age scatter plot (no outliers)', fontsize=15, weight='bold' )","fc79a7a5":"def age(df):\n    df_tmp = pd.DataFrame()\n    df_tmp['added_year'] =  pd.to_datetime(df['date_added']).dt.year\n    df_tmp['current_year'] = pd.to_datetime('2020-01-01')\n    df_tmp['current_year'] = df_tmp['current_year'].dt.year\n    df_tmp['age'] = df_tmp['current_year'] - df_tmp['added_year']\n    df_tmp['age_log'] = np.log1p(np.abs(df_tmp['age']))\n    return pd.DataFrame(df_tmp['age_log'])","373d4af8":"age(train)","7e85d261":"df[feature].fillna('').str.len()","2617363b":"def text_chars_len(df, feature_list):\n    df_tmp = pd.DataFrame()\n    for feature in feature_list:\n        df_tmp[feature+'_chars_len'] = np.log1p(np.abs(df[feature].fillna('').str.len()))\n    return df_tmp","afa4b395":"text_length = ['about', 'name', 'origin', 'origin_place', 'other_text', 'tags']\ntext_chars_len(train, text_length)","298df685":"text_chars_len(train, text_length).hist()","b2a77933":"print(train['status'].unique())\nprint(test['status'].unique())","e193c71d":"train['status'].value_counts()","93ae7da7":"test['status'].value_counts()","08b22b0f":"for df in [train, test]:\n    df.loc[~df['status'].isin(['Deadpool', 'Submission', 'Confirmed']), 'status'] = 'Unknown'","b9a6f78c":"def get_status_frequency():\n    df_tmp = pd.DataFrame()\n    df_tmp['status'] = train['status']\n    df_tmp.loc[~df_tmp['status'].isin(['Deadpool', 'Submission', 'Confirmed']), 'status'] = 'Unknown'\n    freq_dict = df_tmp['status'].value_counts().to_dict()\n    return freq_dict\n\ndef status_frequency(df):\n    df_tmp = pd.DataFrame()\n    freq_dict = get_status_frequency()\n    df_tmp['status'] = df['status']\n    df_tmp.loc[~df_tmp['status'].isin(['Deadpool', 'Submission', 'Confirmed']), 'status'] = 'Unknown'\n    df_tmp['status'] = np.log1p(np.abs(df_tmp['status'].apply(lambda x: freq_dict[x])))\n    return df_tmp","f987bf1f":"status_frequency(train)","219ebb6b":"status_frequency(train).hist()","cf13d178":"train['type'].unique()","ff5028a9":"train['type'].value_counts()","ec3c886a":"def dum_type(df):\n    df_tmp = pd.DataFrame()\n    df_concat = pd.concat([train['type'].str.lower(), test['type'].str.lower()])\n    df_concat.fillna('unknown', inplace = True) # It can be either Other or Unknown \n    freq_dict = df_concat.value_counts().head(100).to_dict()\n    item_list = [key for key in freq_dict]   \n    df_tmp['type_dum'] = df['type'].str.lower().fillna('unknown')\n    df_tmp.loc[~df_tmp['type_dum'].isin(item_list), 'type_dum'] = 'other'\n    df_tmp = pd.get_dummies(df_tmp)\n    return df_tmp","88952945":"dum_type(train)","8bf83a8f":"def dum_status(df):\n    df_tmp = pd.DataFrame()\n    df_tmp['status'] = df['status']\n    df_tmp.loc[~df_tmp['status'].isin(['Deadpool', 'Submission', 'Confirmed']), 'status'] = 'Unknown'\n    df_tmp = pd.get_dummies(df_tmp)\n    return df_tmp","1e86ae68":"dum_status(train)","7087dd9d":"def cat_status(df):\n    df_tmp = pd.DataFrame()\n    df_tmp['status_cat'] = df['status']\n    df_tmp.loc[~df_tmp['status_cat'].isin(['Deadpool', 'Submission', 'Confirmed']), 'status_cat'] = 'Unknown'\n    mapping = {'Deadpool': 0,\n               'Submission': 1,\n               'Confirmed': 2,\n               'Unknown': -1}\n    df_tmp['status_cat'] = df_tmp['status_cat'].apply(lambda x: mapping[x]).astype(int)\n    return df_tmp","fb56ff22":"cat_status(train)","a48f72b3":"def numeric_origin_year(df):\n    df_tmp = pd.DataFrame()\n    df_tmp['origin_year_numeric'] = df['origin_year'].apply(lambda x: 1900 if x == 'Unknown' else 2020 - int(x)).astype(int)\n    df_tmp['origin_year_numeric'] = np.log1p(np.abs(df_tmp['origin_year_numeric']))\n    return df_tmp","8337a97c":"numeric_origin_year(train)","209841dc":"numeric_origin_year(train).hist()","b9629842":"train.isnull().sum()","1ba4c0b8":"def text_mining(df, feature):\n    df_tmp = pd.DataFrame()\n    lemmatizer = WordNetLemmatizer()\n    train_text = train[feature].fillna('')\n    test_text = test[feature].fillna('')\n    text = pd.concat([train_text, test_text]).apply(lambda x: \" \".join(lemmatizer.lemmatize(word) for word in nltk.word_tokenize(x)))\n    df[feature+'_norm'] = df[feature].fillna('').apply(lambda x: \" \".join(lemmatizer.lemmatize(word) for word in nltk.word_tokenize(x)))\n    word_vectorizer = TfidfVectorizer(min_df=0.05, stop_words = 'english',analyzer='word', token_pattern=r\"(?u)\\b[^\\d\\W]+\\w\\b\")\n    word_vectorizer.fit(text)\n    df_tmp = word_vectorizer.transform(df[feature+'_norm'].fillna(''))\n    df_tmp = pd.DataFrame(df_tmp.toarray(), columns=word_vectorizer.get_feature_names())\n    df_tmp = df_tmp.add_prefix(feature+'_')\n    print('Text mining {} - done'.format(feature))\n    return df_tmp","fe01dff0":"text_mining(train, 'tags')","48e77edd":"# Numeric features from dataset\ndef numeric(df, feature_list):\n    df_tmp = pd.DataFrame()\n    for feature in feature_list:\n        df_tmp[feature] = df[feature]\n        upper_limit = df_tmp[feature].quantile(0.95)\n        lower_limit = df_tmp[feature].quantile(0.05)\n        df_tmp.loc[(df_tmp[feature] > upper_limit),feature] = upper_limit\n        df_tmp.loc[(df_tmp[feature] < lower_limit),feature] = lower_limit\n        df_tmp[feature] = np.log1p(np.abs(df_tmp[feature]))\n    return df_tmp\n\n# Add numeric feature \"age\"\ndef age(df):\n    df_tmp = pd.DataFrame()\n    df_tmp['added_year'] =  pd.to_datetime(df['date_added']).dt.year\n    df_tmp['current_year'] = pd.to_datetime('2020-01-01')\n    df_tmp['current_year'] = df_tmp['current_year'].dt.year\n    df_tmp['age'] = df_tmp['current_year'] - df_tmp['added_year']\n    df_tmp['age_log'] = np.log1p(np.abs(df_tmp['age']))\n    return pd.DataFrame(df_tmp['age'])\n\n# Add numeric features: count chars in the text and apply log function\ndef text_chars_len(df, feature_list):\n    df_tmp = pd.DataFrame()\n    for feature in feature_list:\n        df_tmp[feature+'_chars_len'] = np.log1p(np.abs(df[feature].fillna('').str.len()))\n    return df_tmp\n\n# Get frequency of values in categorical field \"status\"\ndef get_status_frequency():\n    df_tmp = pd.DataFrame()\n    df_tmp['status'] = train['status']\n    df_tmp.loc[~df_tmp['status'].isin(['Deadpool', 'Submission', 'Confirmed']), 'status'] = 'Unknown'\n    freq_dict = df_tmp['status'].value_counts().to_dict()\n    return freq_dict\n\n# Add numeric feature \"status\" frequency\ndef status_frequency(df):\n    df_tmp = pd.DataFrame()\n    freq_dict = get_status_frequency()\n    df_tmp['status'] = df['status']\n    df_tmp.loc[~df_tmp['status'].isin(['Deadpool', 'Submission', 'Confirmed']), 'status'] = 'Unknown'\n    df_tmp['status'] = np.log1p(np.abs(df_tmp['status'].apply(lambda x: freq_dict[x])))\n    return df_tmp\n\n# Add dummy features of categorical field \"type\"\ndef dum_type(df):\n    df_tmp = pd.DataFrame()\n    df_concat = pd.concat([train['type'].str.lower(), test['type'].str.lower()])\n    df_concat.fillna('unknown', inplace = True) # It can be either Other or Unknown \n    freq_dict = df_concat.value_counts().head(100).to_dict()\n    item_list = [key for key in freq_dict]   \n    df_tmp['type_dum'] = df['type'].str.lower().fillna('unknown')\n    df_tmp.loc[~df_tmp['type_dum'].isin(item_list), 'type_dum'] = 'other'\n    df_tmp = pd.get_dummies(df_tmp)\n    return df_tmp\n\n# Add dummy features of categorical field \"status\"\ndef dum_status(df):\n    df_tmp = pd.DataFrame()\n    df_tmp['status'] = df['status']\n    df_tmp.loc[~df_tmp['status'].isin(['Deadpool', 'Submission', 'Confirmed']), 'status'] = 'Unknown'\n    df_tmp = pd.get_dummies(df_tmp)\n    return df_tmp\n\n# Add categorical feature on field \"status\"\ndef cat_status(df):\n    df_tmp = pd.DataFrame()\n    df_tmp['status_cat'] = df['status']\n    df_tmp.loc[~df_tmp['status_cat'].isin(['Deadpool', 'Submission', 'Confirmed']), 'status_cat'] = 'Unknown'\n    mapping = {'Deadpool': 0,\n               'Submission': 1,\n               'Confirmed': 2,\n               'Unknown': -1}\n    df_tmp['status_cat'] = df_tmp['status_cat'].apply(lambda x: mapping[x]).astype(int)\n    return df_tmp\n\n# Add numeric feature on field \"origin_year\"\ndef numeric_origin_year(df):\n    df_tmp = pd.DataFrame()\n    df_tmp['origin_year_numeric'] = df['origin_year'].apply(lambda x: 1900 if x == 'Unknown' else 2020 - int(x)).astype(int)\n    df_tmp['origin_year_numeric'] = np.log1p(np.abs(df_tmp['origin_year_numeric']))\n    return df_tmp\n\n# Text mining\ndef text_mining(df, feature):\n    df_tmp = pd.DataFrame()\n    lemmatizer = WordNetLemmatizer()\n    train_text = train[feature].fillna('')\n    test_text = test[feature].fillna('')\n    text = pd.concat([train_text, test_text]).apply(lambda x: \" \".join(lemmatizer.lemmatize(word) for word in nltk.word_tokenize(x)))\n    df[feature+'_norm'] = df[feature].fillna('').apply(lambda x: \" \".join(lemmatizer.lemmatize(word) for word in nltk.word_tokenize(x)))\n    word_vectorizer = TfidfVectorizer(min_df=0.05, stop_words = 'english',analyzer='word', token_pattern=r\"(?u)\\b[^\\d\\W]+\\w\\b\")\n    word_vectorizer.fit(text)\n    df_tmp = word_vectorizer.transform(df[feature+'_norm'].fillna(''))\n    df_tmp = pd.DataFrame(df_tmp.toarray(), columns=word_vectorizer.get_feature_names())\n    df_tmp = df_tmp.add_prefix(feature+'_')\n    print('Text mining {} - done'.format(feature))\n    return df_tmp\n\n# Assemble all features in dataset\ndef dataset(df): \n    df_numerics = numeric(df, numerics)\n    df_age = age(df)\n    df_text_chars_len = text_chars_len(df, text_length)\n    df_status_frequency = status_frequency(df)\n    df_dum_type = dum_type(df)\n    df_dum_status = dum_status(df)\n    df_cat_status = cat_status(df)\n    df_numeric_origin_year = numeric_origin_year(df)\n    df_text_mining = text_mining(df, 'tags')\n\n    df_tmp = df_numerics.copy()\n    df_tmp = df_tmp.merge(df_age, left_index=True, right_index=True) \n    df_tmp = df_tmp.merge(df_text_chars_len, left_index=True, right_index=True)   \n    df_tmp = df_tmp.merge(df_status_frequency, left_index=True, right_index=True)    \n    df_tmp = df_tmp.merge(df_dum_type, left_index=True, right_index=True)    \n    df_tmp = df_tmp.merge(df_dum_status, left_index=True, right_index=True) \n    df_tmp = df_tmp.merge(df_numeric_origin_year, left_index=True, right_index=True)\n    df_tmp = df_tmp.merge(df_text_mining, left_index=True, right_index=True)    \n    return df_tmp\n\n# Main body\nnumerics = ['comments', 'photos', 'videos']\ntext_length = ['about', 'name', 'origin', 'origin_place', 'other_text', 'tags']\nX_train = dataset(train)\nX_test = dataset(test)\ny_train = train['target_variable']","a99acbff":"def normalize(df):\n    return ((df-df.mean())\/df.std())\nX_train = normalize(X_train)\nX_test = normalize(X_test)","3f02be83":"X_train.head()","ae4aa6f0":"df = X_train.copy()\ndf['target_variable'] = train['target_variable']\nnumcorr = df.corr()\nNum=numcorr['target_variable'].sort_values(ascending=False).head(100).to_frame()\ncm = sns.light_palette(\"cyan\", as_cmap=True)\nplt = Num.style.background_gradient(cmap=cm)\nplt","44e216ef":"from sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)\nrf_random.best_params_","d772072b":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [False],\n    'max_depth': [70,80,90],\n    'max_features': ['sqrt'],\n    'min_samples_leaf': [1,2,3],\n    'min_samples_split': [3,5,7],\n    'n_estimators': [60, 70, 80]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_","049281dd":"cv = KFold(3, shuffle=True, random_state=42)\n# Update parametrs from grid search\nmodel = RandomForestRegressor(bootstrap=False,\n                              max_depth=90,\n                              max_features='sqrt',\n                              min_samples_leaf=1,\n                              min_samples_split=7,\n                              n_estimators=60,\n                              random_state=42,\n                              n_jobs=-1,\n                              verbose=2,\n                            )\ncv_results = cross_val_score(model,\n                             X_train,\n                             y_train,\n                             cv=cv,\n                             scoring='neg_mean_squared_error')\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\nprint('RMSE: {}'.format(np.sqrt([-x for x in cv_results])))\nprint('RMSE MEAN: {}'.format(np.sqrt([-x for x in cv_results]).mean()))","69c6d805":"preds = model.predict(normalize(X_test))\nsubmission = pd.DataFrame()\nsubmission['id'] = test['id']\nsubmission['target_variable'] = preds\nsubmission.to_csv('submission.csv', index=False)","765af198":"Review normolized dataset","cf7a6756":"# 5 - DATA NORMALIZATION","bcf50a03":"# 7 - Machine Learning","3d7e8c9f":"And apply log for better data distribution","15f3dc57":"![](https:\/\/imgs.xkcd.com\/comics\/machine_learning_2x.png)","66fa556d":"Verify output","7c9f6336":"* We will be using Lemmatizing technic (Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item)\n* We will be using TfidfVectorizer with:\n  * token_pattern: to exclude digits and other symbols but alpha\n  * stop_words: to exclude the less important words\n  * min_df: to limit scope of potential features and find balance between model speed and score","a2a91baa":"# 4 - Lets assemble all functions and run code to get all feature set for Machine Learning","5e0f31cf":"In machine learning there are different ways for handling outliers, we will use percentile approach to cap outliers instead of deleting data","c81bf5fc":"## *Random Search*","11c00b9c":"# Loading datasets from csv files","7e0c31d2":"**Note:** As per contest example, let's try to create new numeric features using simpliest option - just count the number of letters in the text features","9087e0d2":"# 9 - Conclusion","624b7d91":"# 1 - DATA ANALYSIS","211b4ae6":"## 3.9. Add feature using text mining technic based on field '*tags*'","a55a8666":"## *Train Model*","5feda5ec":"---\n# ML@DUB Competition #1\n---","70598ccd":"# 2 - DATA CLEANING","03c4bd9b":"We will use dummy encoding of categorical feature.\nLet's explore data.\n\nTo read more: [Handling categorical data](https:\/\/www.datacamp.com\/community\/tutorials\/categorical-data)","0bf49a37":"Since, number of unique types can be different in 2 datasets (train and test) we will be using merged list and use Top 100 the most frequent types.\n\n**Note:** we can play arround and explore which number (top 100 or other) is better","dc707da4":"## 3.5. Add dummy features based on field '*type*'.","e6d4f4b0":"* **Note:** Text only columns have empty records. That is very good, that means all numeric fields contain data. We will decide how to handle data imputation for text\/categorical features on further steps.\n    ","9fa517d9":"Let's explore field \"*status*\"","b8e297ed":"There are some data which are different in train and test datasets. As per output bellow these categories \"Trach\", \"Literally me\" and etc appeared only once, so we need to replace it with 'Unknown' rather than delete records","cb975783":"Function below is function which returns modified numeric features","0add6e8b":"## 3.4. Add numeric feature where we will count frequency of items in categorical feature \"status\"","49400159":"Specify grid having output of RandomSearch","3c7e9e92":"Let's explore numeric features","11c92e93":"Here is a glimpse of what we will be working with:","4dd30b03":"Let's take a look at data structure: what fields are numeric and what are text\/categorical.","a7815419":"## 3.8 Add numeric feature based on field '*origin_year*'","2a024d3e":"Let's calculate '*status*' frequency and apply log for better distribution","60f88fe3":"**Note:** Here is we can notice some outliers exist, lets keep this in mind to deal with them in further steps\n\nLet's plot numeric features for better data understanding and outliers highlighting","cd0da4b4":"Verify output","097de47b":"Verify output","970e6d0b":"## 3.3. Add numeric feature \"Text length\"","8aa52432":"Let's replace all data which are not in the list ['Deadpool', 'Submission', 'Confirmed']","3f154544":"# 6 - Review feature correlation\/importance","4ec0b984":"Machine Learning in general emphasises well-known and quite clear steps e.g. data analysis, data cleaning, feature engineering and etc. But practically it is combination of experience, art and stubborness, where complex methods doesn't necessarily mean it is the best method for all datasets.\n\nParticipating in current competion we tried dozens of back and force steps looking for the best feature combination:\n* numeric features, created new features based on numeric features: X1 x X2, X1 x X1 and etc\n* different encoding methods for categorical features like: one-hot, label encoding, dummy encoding\n* text mining: like TfidfVectorizer, CountVectorizer, played with their parameters\n\nFeature selection:\n* SelectKBest\n* f_regression\n\nTried different models:\n* RandomForestRegressor()\n* TensorFlow()\n* Lasso()\n* Ridge()\n* ElasticNetCV\n* H2OGradientBoostingEstimator()\n* XGBoost()\n\n**P.S. We would like to say thank you for this wonderful ML Contest!**\n","7e05a3af":"## 3.7. Add label encoding for categorical feature '*status*'","9ee5f229":"Verify output","65568bdc":"### 3.1. Native numeric features","e7f84416":"## 3.2. Add numeric feature \"Age\"","3774f2a2":"Verify output","3a670677":"Let's take a look how much data is missing.","12982f01":"Verify output","ad55266a":"We normalize data using Z-score. \n\nTo read more: [Normalization](https:\/\/www.codecademy.com\/articles\/normalization#:~:text=Z%2DScore%20Normalization,-Z%2Dscore%20normalization&text=If%20a%20value%20is%20exactly,will%20be%20a%20positive%20number.&text=This%20is%20the%20same%20data,re%20using%20z%2Dscore%20normalization.)","fc4292b2":"**Note**: You can play with different text fields. We played around and found balance between speed and feature importance using field \"tags\".\nAlso on step above, where we did data analysis we found that other text fields has some different amount of missing data, so field 'tags' sounds like the most appropriate","8db23aa8":"![thank you](https:\/\/www.meme-arsenal.com\/memes\/546f9b0cbdc106a5fabdeb6abcdda35e.jpg)","0ed8b07d":"Train model with KFold validation and using hyperparameters from GridSearch function","2af98bf4":"## 3.6. Add features dummy encoding for categorical feature 'status'","b7331d54":"Convert text in field 'origin_year' to numeric, apply capping for outliers and apply log function then.","b9702d35":"Let's plot numeric features and detect outliers","fd11dfe1":"And apply log for better data distribution","ddf4f214":"Verify output","b14aff98":"We will be using RandomForestRegressor(), but we need to tune hyperparameters. For this purpose we will be doing 2 prelimenary steps:\n* Random Search\n* Grid Search","56f24a65":"Let's generate new feature \"Age\" using field \"date_added\", where we will find difference between current year and \"date_added\".","fd694401":"# 8 - Submission","c7ddef4e":"Verify that outliers have been caped and distribution looks normal","18edfa38":"# 3 - FEATURE ENGINEERING","137664f4":"Verify output","d3d66a82":"Essentially throughout the competition we will go through following steps:\n\n1. DATA ANALYSIS\n2. DATA CLEANING\n3. FEATURE ENGINEERING\n4. ENCODING CATEGORICAL FEATURES\n5. DETECTING OUTLIERS\n6. MACHINE LEARNING\n\n---\n\n**Note:**\nEvery feature will be derived calling their corresponding function which will include: \n* Data cleaning\n* Log transform step to get data skewness value to be around 0.\n\n**Note:** In order to get code ready to train model, I will assemble all function and main all together before Machine Learning clause.\n\n---","99a8a654":"**Note:** below we will describe how we will be handling outliers having \"*comments*\" field as an example. But we will apply the same for all numeric features.","d1795e1c":"## *GridSearchCV*"}}