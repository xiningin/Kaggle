{"cell_type":{"a7eeb949":"code","8c68f374":"code","fffc0d99":"code","125a6d63":"code","54a672fe":"code","fcb51bf2":"code","654dedc3":"code","fdf65016":"code","4b0fd22d":"code","ec7619c2":"code","e5d0abdb":"code","145cd5bf":"code","b5c4e36e":"code","a8a1eca3":"code","add5b58b":"code","8ad38cf5":"code","47ce7dc5":"code","8facf317":"code","26273b67":"code","f16bf8fc":"code","653723f0":"code","9fd8af2b":"code","92fc05ee":"markdown","9d1468b4":"markdown","86a28534":"markdown","1706dd44":"markdown","b2a235fd":"markdown","06aadfec":"markdown"},"source":{"a7eeb949":"!wget -q https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/modeling.py \n!wget -q https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/optimization.py \n!wget -q https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/run_classifier.py \n!wget -q https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/tokenization.py ","8c68f374":"import os\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport sys\nimport zipfile\nimport modeling\nimport optimization\nimport run_classifier\nimport tokenization\n\nfrom tokenization import FullTokenizer\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow_hub as hub\nfrom tqdm import tqdm_notebook\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model","fffc0d99":"sess = tf.Session()\n\n# Params for bert model and tokenization\nbert_path = \"https:\/\/tfhub.dev\/google\/bert_uncased_L-12_H-768_A-12\/1\"\nmax_seq_length = 128","125a6d63":"train_df = pd.read_csv('..\/input\/train.csv', index_col='id')\nval_df = pd.read_csv('..\/input\/valid.csv', index_col='id')\ntest_df = pd.read_csv('..\/input\/test.csv', index_col='id')","54a672fe":"label_encoder = LabelEncoder().fit(pd.concat([train_df['label'], val_df['label']]))","fcb51bf2":"X_train_val, X_test = pd.concat([train_df['text'], val_df['text']]).values, test_df['text'].values","654dedc3":"y_train_val = label_encoder.fit_transform(pd.concat([train_df['label'], val_df['label']]))","fdf65016":"X_train, X_val, y_train, y_val = train_test_split(\n        X_train_val,y_train_val, test_size=0.1, random_state=0, stratify = y_train_val\n        )","4b0fd22d":"train_text = X_train\ntrain_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\ntrain_text = np.array(train_text, dtype=object)[:, np.newaxis]\ntrain_label = y_train\n\nval_text = X_val\nval_text = [' '.join(t.split()[0:max_seq_length]) for t in val_text]\nval_text = np.array(val_text, dtype=object)[:, np.newaxis]\nval_label = y_val\n\ntest_text = X_test\ntest_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\ntest_text = np.array(test_text, dtype=object)[:, np.newaxis]","ec7619c2":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport os\nimport re\nimport numpy as np\nfrom tqdm import tqdm_notebook\n#from tensorflow.keras import backend as K\nfrom keras import backend as K\nfrom keras.layers import Layer\n\n\nclass BertLayer(Layer):\n    \n    '''BertLayer which support next output_representation param:\n    \n    pooled_output: the first CLS token after adding projection layer () with shape [batch_size, 768]. \n    sequence_output: all tokens output with shape [batch_size, max_length, 768].\n    mean_pooling: mean pooling of all tokens output [batch_size, max_length, 768].\n    \n    \n    You can simple fine-tune last n layers in BERT with n_fine_tune_layers parameter. For view trainable parameters call model.trainable_weights after creating model.\n    \n    '''\n    \n    def __init__(self, n_fine_tune_layers=10, tf_hub = None, output_representation = 'pooled_output', trainable = False, **kwargs):\n        \n        self.n_fine_tune_layers = n_fine_tune_layers\n        self.is_trainble = trainable\n        self.output_size = 768\n        self.tf_hub = tf_hub\n        self.output_representation = output_representation\n        self.supports_masking = True\n        \n        super(BertLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n\n        self.bert = hub.Module(\n            self.tf_hub,\n            trainable=self.is_trainble,\n            name=\"{}_module\".format(self.name)\n        )\n        \n        \n        variables = list(self.bert.variable_map.values())\n        if self.is_trainble:\n            # 1 first remove unused layers\n            trainable_vars = [var for var in variables if not \"\/cls\/\" in var.name]\n            \n            \n            if self.output_representation == \"sequence_output\" or self.output_representation == \"mean_pooling\":\n                # 1 first remove unused pooled layers\n                trainable_vars = [var for var in trainable_vars if not \"\/pooler\/\" in var.name]\n                \n            # Select how many layers to fine tune\n            trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n            \n            # Add to trainable weights\n            for var in trainable_vars:\n                self._trainable_weights.append(var)\n\n            # Add non-trainable weights\n            for var in self.bert.variables:\n                if var not in self._trainable_weights:\n                    self._non_trainable_weights.append(var)\n                \n        else:\n             for var in variables:\n                self._non_trainable_weights.append(var)\n                \n\n        super(BertLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n        input_ids, input_mask, segment_ids = inputs\n        bert_inputs = dict(\n            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n        )\n        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n        \n        if self.output_representation == \"pooled_output\":\n            pooled = result[\"pooled_output\"]\n            \n        elif self.output_representation == \"mean_pooling\":\n            result_tmp = result[\"sequence_output\"]\n        \n            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) \/ (\n                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n            input_mask = tf.cast(input_mask, tf.float32)\n            pooled = masked_reduce_mean(result_tmp, input_mask)\n            \n        elif self.output_representation == \"sequence_output\":\n            \n            pooled = result[\"sequence_output\"]\n       \n        return pooled\n    \n    def compute_mask(self, inputs, mask=None):\n        \n        if self.output_representation == 'sequence_output':\n            inputs = [K.cast(x, dtype=\"bool\") for x in inputs]\n            mask = inputs[1]\n            \n            return mask\n        else:\n            return None\n        \n        \n    def compute_output_shape(self, input_shape):\n        if self.output_representation == \"sequence_output\":\n            return (input_shape[0][0], input_shape[0][1], self.output_size)\n        else:\n            return (input_shape[0][0], self.output_size)","e5d0abdb":"import keras","145cd5bf":"def build_model(max_seq_length, tf_hub, n_classes, n_fine_tune): \n    in_id = keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n    in_mask = keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n    in_segment = keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n    bert_inputs = [in_id, in_mask, in_segment]\n    \n    bert_output = BertLayer(n_fine_tune_layers=n_fine_tune, tf_hub = tf_hub, output_representation = 'mean_pooling', trainable = True)(bert_inputs)\n    drop = keras.layers.Dropout(0.3)(bert_output)\n    dense = keras.layers.Dense(256, activation='sigmoid')(drop)\n    drop = keras.layers.Dropout(0.3)(dense)\n    dense = keras.layers.Dense(64, activation='sigmoid')(drop)\n    pred = keras.layers.Dense(n_classes, activation='softmax')(dense)\n    \n    model = keras.models.Model(inputs=bert_inputs, outputs=pred)\n    Adam = keras.optimizers.Adam(lr = 0.0005)\n    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam, metrics=['sparse_categorical_accuracy'])\n    model.summary()\n\n    return model\n\ndef initialize_vars(sess):\n    sess.run(tf.local_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.tables_initializer())\n    K.set_session(sess)","b5c4e36e":"\nn_classes = len(label_encoder.classes_)\nn_fine_tune_layers = 48\nmodel = build_model(max_seq_length, bert_path, n_classes, n_fine_tune_layers)\n\n# Instantiate variables\ninitialize_vars(sess)","a8a1eca3":"model.trainable_weights","add5b58b":"class PaddingInputExample(object):\n    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n  When running eval\/predict on the TPU, we need to pad the number of examples\n  to be a multiple of the batch size, because the TPU requires a fixed batch\n  size. The alternative is to drop the last batch, which is bad because it means\n  the entire output data won't be generated.\n  We use this class instead of `None` because treating `None` as padding\n  battches could cause silent errors.\n  \"\"\"\n\nclass InputExample(object):\n    \"\"\"A single training\/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\"Constructs a InputExample.\n    Args:\n      guid: Unique id for the example.\n      text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n      text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n      label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\ndef create_tokenizer_from_hub_module(tf_hub):\n    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n    bert_module =  hub.Module(tf_hub)\n    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n    vocab_file, do_lower_case = sess.run(\n        [\n            tokenization_info[\"vocab_file\"],\n            tokenization_info[\"do_lower_case\"],\n        ]\n    )\n    \n    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n\ndef convert_single_example(tokenizer, example, max_seq_length=256):\n    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n\n    if isinstance(example, PaddingInputExample):\n        input_ids = [0] * max_seq_length\n        input_mask = [0] * max_seq_length\n        segment_ids = [0] * max_seq_length\n        label = 0\n        return input_ids, input_mask, segment_ids, label\n\n    tokens_a = tokenizer.tokenize(example.text_a)\n    if len(tokens_a) > max_seq_length - 2:\n        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n\n    tokens = []\n    segment_ids = []\n    tokens.append(\"[CLS]\")\n    segment_ids.append(0)\n    for token in tokens_a:\n        tokens.append(token)\n        segment_ids.append(0)\n    tokens.append(\"[SEP]\")\n    segment_ids.append(0)\n    \n    #print(tokens)\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n\n    return input_ids, input_mask, segment_ids, example.label\n\ndef convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n\n    input_ids, input_masks, segment_ids, labels = [], [], [], []\n    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n        input_id, input_mask, segment_id, label = convert_single_example(\n            tokenizer, example, max_seq_length\n        )\n        input_ids.append(input_id)\n        input_masks.append(input_mask)\n        segment_ids.append(segment_id)\n        labels.append(label)\n    return (\n        np.array(input_ids),\n        np.array(input_masks),\n        np.array(segment_ids),\n        np.array(labels).reshape(-1, 1),\n    )\n\ndef convert_text_to_examples(texts, labels):\n    \"\"\"Create InputExamples\"\"\"\n    InputExamples = []\n    for text, label in zip(texts, labels):\n        InputExamples.append(\n            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n        )\n    return InputExamples\n","8ad38cf5":"# Instantiate tokenizer\ntokenizer = create_tokenizer_from_hub_module(bert_path)\n\n# Convert data to InputExample format\ntrain_examples = convert_text_to_examples(train_text, train_label)\nval_examples = convert_text_to_examples(val_text, val_label)\n\n# Convert to features\n(train_input_ids, train_input_masks, train_segment_ids, train_labels \n) = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n(val_input_ids, val_input_masks, val_segment_ids, val_labels\n) = convert_examples_to_features(tokenizer, val_examples, max_seq_length=max_seq_length)\n\n","47ce7dc5":"from keras.callbacks import EarlyStopping\n\nBATCH_SIZE = 256\nMONITOR = 'val_sparse_categorical_accuracy'\nprint('BATCH_SIZE is {}'.format(BATCH_SIZE))\ne_stopping = EarlyStopping(monitor=MONITOR, patience=3, verbose=1, mode='max', restore_best_weights=True)\ncallbacks =  [e_stopping]\n\nhistory = model.fit(\n   [train_input_ids, train_input_masks, train_segment_ids], \n    train_labels,\n    validation_data = ([val_input_ids, val_input_masks, val_segment_ids], val_labels),\n    epochs = 10,\n    verbose = 1,\n    batch_size = BATCH_SIZE,\n    callbacks= callbacks\n)","8facf317":"test_examples = convert_text_to_examples(test_text, np.zeros(len(test_text)))","26273b67":"(test_input_ids, test_input_masks, test_segment_ids, test_labels\n) = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)","f16bf8fc":"prediction = model.predict([test_input_ids, test_input_masks, test_segment_ids], verbose = 1)","653723f0":"preds = label_encoder.classes_[np.argmax(prediction, axis =1)]","9fd8af2b":"pd.DataFrame(preds, columns=['label']).to_csv('bert_keras_submission.csv',\n                                                  index_label='id')","92fc05ee":"# Test","9d1468b4":"# Bert tf-hub\n","86a28534":"# Train model","1706dd44":"# Load Data","b2a235fd":"Keras Bert\n\nSimple wrapper of tf-hub Bert models for use in Keras.","06aadfec":"# Tokenization "}}