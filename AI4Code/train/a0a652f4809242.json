{"cell_type":{"fd8b9a18":"code","3e9d7354":"code","5d0ea183":"code","dc339f7a":"code","c0641762":"code","95265b7c":"code","68400d27":"code","38c2fd67":"code","d9131dc2":"code","2e882dc8":"code","514faade":"code","f989d117":"code","6ed82bd8":"code","948a1b5c":"code","c473b220":"code","ac4dbf1a":"code","f00a8f8e":"code","d9bf2483":"code","42c21a89":"code","ff993cd3":"code","eba2d18f":"code","5ef8dbe9":"code","9aeb8df5":"code","4af0a8a1":"code","8e484b98":"code","0a8c4bab":"code","562fd9a3":"code","74bd7019":"code","2d3e6b46":"code","c094280c":"code","069a2f11":"code","1b58af05":"code","560bbbf7":"code","6cc1cb8f":"code","ae209248":"code","fd057d4e":"code","9b1ef157":"code","d10be529":"code","91e26b54":"code","ff037bac":"code","45552ca3":"code","88356f2e":"markdown","85864844":"markdown","5bf18d7c":"markdown","8321a57d":"markdown","60cab1f4":"markdown","1f6d8111":"markdown","d92f27e6":"markdown","477ce36c":"markdown","2b3b0fea":"markdown","cfd7e6b0":"markdown","a45d0cb9":"markdown","abe08e73":"markdown","75f4702b":"markdown","b2c8e971":"markdown","438e9400":"markdown","b0d23a27":"markdown","8fb5cabe":"markdown","b5836563":"markdown","cc20ae21":"markdown"},"source":{"fd8b9a18":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e9d7354":"df=pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\n","5d0ea183":"print(\"shape \",df.shape)","dc339f7a":"df.head()","c0641762":"df.drop('Unnamed: 32',axis=1,inplace=True)","95265b7c":"df.describe(include=\"all\")","68400d27":"df.info()","38c2fd67":"df.isnull().sum()","d9131dc2":"df.rename({'diagnosis':'target'}, axis=1,inplace=True)","2e882dc8":"df.head()","514faade":"df['target'].value_counts()","f989d117":"df['target']=[1 if i == \"M\" else 0 for i in df['target']]\n##replacing malignant with 1 and benign with 0","6ed82bd8":"df.plot(subplots=True, sharex=True ,figsize=(20,50))","948a1b5c":"ax = sns.countplot(x=\"target\", data=df)\nplt.title(\"Diagnosis M=1 , B=0\")\nplt.show()","c473b220":"df.drop('id',axis=1,inplace=True)\ncorr=df.corr()","ac4dbf1a":"import matplotlib.style as style\nstyle.use(\"ggplot\")\nsns.set_style('whitegrid')\nplt.subplots(figsize = (16,9))\n\nsns.heatmap(corr,annot=True)","f00a8f8e":"df.corr()['target'].sort_values(ascending=False)","d9bf2483":"plt.subplots(figsize = (9,13))\nsns.heatmap(df.corr()[['target']].sort_values(by='target', ascending=False), annot=True, cmap='BrBG')","42c21a89":"def Box_plots(df):\n    plt.figure(figsize=(10, 4))\n    plt.title(\"Box Plot\")\n    sns.boxplot(df)\n    plt.show()\nfor i in df.columns:\n    Box_plots(df[i])","ff993cd3":"X = df.drop([\"target\"], axis = 1)\ny = df.target","eba2d18f":"col = X.columns.tolist()","5ef8dbe9":"df.shape","9aeb8df5":"from sklearn.neighbors import LocalOutlierFactor","4af0a8a1":"clf = LocalOutlierFactor()\ny_pred = clf.fit_predict(X)","8e484b98":"X_score = clf.negative_outlier_factor_\noutlier_score = pd.DataFrame()\noutlier_score[\"score\"] = X_score\nthreshold = -2.0\nfiltre = outlier_score[\"score\"] < threshold\noutlier_index = outlier_score[filtre].index.tolist()","0a8c4bab":"X = X.drop(outlier_index)\ny = y.drop(outlier_index).values","562fd9a3":"X.shape","74bd7019":"X_train, X_test, Y_train, Y_test = train_test_split(X, y,test_size = 0.3,random_state = 42)","2d3e6b46":"allAlgo = [('lr', LogisticRegression()),('knn', KNeighborsClassifier()),('dclf', DecisionTreeClassifier()),\n          ('svm', SVC()),('nb', GaussianNB()),('rf',RandomForestClassifier()),]","c094280c":"res = []\nalgoName = []\nfor name, model in allAlgo:\n    kfold = KFold(n_splits=10)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=\"accuracy\")\n    res.append(cv_results)\n    algoName.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n","069a2f11":"fig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(res)\nax.set_xticklabels(algoName)\nplt.show()","1b58af05":"pipelines = []\n\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('lr',LogisticRegression())])))\n\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('knn',KNeighborsClassifier())])))\n\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('dclf',DecisionTreeClassifier())])))\n\npipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('nb',GaussianNB())])))\n\npipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('svm', SVC())])))\n\npipelines.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('rf', RandomForestClassifier())])))\n\nres = []\nalgoName = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=10)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=\"accuracy\")\n    res.append(cv_results)\n    algoName.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","560bbbf7":"fig = plt.figure()\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(res)\nax.set_xticklabels(algoName)\nplt.show()","6cc1cb8f":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nc_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel = SVC()\nkfold = KFold(n_splits=11)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=\"accuracy\", cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","ae209248":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = SVC(kernel='linear',C=0.3)\nmodel.fit(rescaledX, Y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = model.predict(rescaledValidationX)\nprint(accuracy_score(Y_test, predictions))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","fd057d4e":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nnb_classifier = GaussianNB()\nnb_classifier.fit(rescaledX, Y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = nb_classifier.predict(rescaledValidationX)\nprint(accuracy_score(Y_test, predictions))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","9b1ef157":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nrf=RandomForestClassifier(random_state=101)\nrf.fit(rescaledX, Y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = rf.predict(rescaledValidationX)\nprint(accuracy_score(Y_test, predictions))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","d10be529":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nlr=LogisticRegression(solver='saga',penalty='l2',l1_ratio=0.6,random_state=41)\nlr.fit(rescaledX, Y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = lr.predict(rescaledValidationX)\nprint(accuracy_score(Y_test, predictions))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","91e26b54":"from xgboost import XGBClassifier\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nxgb_classifier = XGBClassifier()\nxgb_classifier.fit(rescaledX, Y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = xgb_classifier.predict(rescaledValidationX)\nprint(accuracy_score(Y_test, predictions))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","ff037bac":"from sklearn.model_selection import cross_val_score\ncross_validation = cross_val_score(estimator = rf, X = X_train, y =Y_train, cv =10)\nprint(\"Cross validation of SVC model = \",cross_validation)\nprint(\"Cross validation of SVC model (in mean) = \",cross_validation.mean())","45552ca3":"cross_validation = cross_val_score(estimator = lr, X = X_train, y =Y_train, cv =10)\nprint(\"Cross validation of SVC model = \",cross_validation)\nprint(\"Cross validation of SVC model (in mean) = \",cross_validation.mean())","88356f2e":"## SVM","85864844":"#### RandomforestClassifier","5bf18d7c":"## Observation \nData is looking quite balanced and we can move on to visualization","8321a57d":"concave points_worst, perimeter_worst, concave points_mean, radius_worst, perimeter_mean\nThey resemble high correlation with target","60cab1f4":"We can see logistic regression and random forest classifier shows good accuracy without scaling data\n","1f6d8111":"#### Logistic Regression","d92f27e6":"We can see there are some outliers in our data we will have to remove this for better result","477ce36c":"## Modeling","2b3b0fea":"## XGBoost","cfd7e6b0":"## GaussianNB","a45d0cb9":"## Tuning SVM","abe08e73":"## Logistic Regression","75f4702b":"We can see the difference after scaling data!! LR, SVM, Rf show quite good accuracy","b2c8e971":"#### 357 benign, 212 malignant","438e9400":"## RandomForestClassifier","b0d23a27":"### Cross-Validation","8fb5cabe":"## Data Visulaization","b5836563":"No null values here!!\n","cc20ae21":"## Outliers "}}