{"cell_type":{"b1abf244":"code","14f15363":"code","0c42b5b3":"code","134addef":"code","ba667a58":"code","ef7f1638":"code","28e45868":"code","dba58029":"markdown","6f9dd4af":"markdown","970d3010":"markdown","893f9519":"markdown","4d3dc0b3":"markdown","c5919955":"markdown","aaa0f105":"markdown","745054d6":"markdown","85d52070":"markdown","3a6ffe5e":"markdown","860eb2fa":"markdown","104e4a58":"markdown"},"source":{"b1abf244":"#Load data\nimport pandas as pd\n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n#Drop features we are not going to use\ntrain = train.drop(['Name','SibSp','Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],axis=1)\ntest = test.drop(['Name','SibSp','Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],axis=1)\n\n#Look at the first 3 rows of our training data\ntrain.head()","14f15363":"#Convert ['male','female'] to [1,0] so that our decision tree can be built\nfor df in [train,test]:\n    df['Sex_binary']=df['Sex'].map({'male':1,'female':0})\n    \n#Fill in missing age values with 0 (presuming they are a baby if they do not have a listed age)\ntrain['Age'] = train['Age'].fillna(0)\ntest['Age'] = test['Age'].fillna(0)\n\n#Select feature column names and target variable we are going to use for training\nfeatures = ['Pclass','Age','Sex_binary']\ntarget = 'Survived'\n\n#Look at the first 3 rows (we have over 800 total rows) of our training data.; \n#This is input which our classifier will use as an input.\ntrain[features].head(3)","0c42b5b3":"#Display first 3 target variables\ntrain[target].head(3).values","134addef":"from sklearn.tree import DecisionTreeClassifier\n\n#Create classifier object with default hyperparameters\nclf = DecisionTreeClassifier()  \n\n#Fit our classifier using the training features and the training target values\nclf.fit(train[features],train[target]) ","ba667a58":"#Make predictions using the features from the test data set\npredictions = clf.predict(test[features])\n\n#Display our predictions - they are either 0 or 1 for each training instance \n#depending on whether our algorithm believes the person survived or not.\npredictions","ef7f1638":"#Create a  DataFrame with the passengers ids and our prediction regarding whether they survived or not\nsubmission = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':predictions})\n\n#Visualize the first 5 rows\nsubmission.head()","28e45868":"#Convert DataFrame to a csv file that can be uploaded\n#This is saved in the same directory as your notebook\nfilename = 'Titanic Predictions 1.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","dba58029":"# 2. Create and fit the decision tree","6f9dd4af":"This tutorial walks you through submitting a \".csv\" file of predictions to Kaggle for the first time.","970d3010":"If you simply run the code below, your score will be fairly poor. I have intentionally left lots of room for improvement regarding the model used (currently a simple decision tree classifier).\n\nThe idea of this tutorial is to get you started and have you make the decisions of how to improve your score. At the bottom of the tutorial are challenges which, if you follow them, will significantly improve your score.","893f9519":"5. Submit file to Kaggle\u00b6\nGo to the submission section of the Titanic competition. Drag your file from the directory which contains your code and make your submission.\n\nCongratulations - you're on the leaderboard!**","4d3dc0b3":"# 3. Make Predictions","c5919955":"**# Load data**","aaa0f105":"This tree is definitely going to overfit our data. When you get to the challenge stage, you can return here and tune hyperparameters in this cell. For example, you could reduce the maximum depth of the tree to 3 by setting max_depth=3 with the following command:\n\nclf = DecisionTreeClassifier(max_depth=3)\n\nTo change multiple hyperparameters, seperate out the parameters with a comma. For example, to change the learning rate and minimum samples per leaf and the maximum depth fill in the parentheses with the following:\n\nclf = DecisionTreeClassifier(max_depth=3,min_samples_leaf=2)\n\nThe other parameters are listed below. You can also access the list of parameters by reading the documentation for decision tree classifiers. Another way to access the parameters is to place your cursor in between the parentheses and then press shift-tab.","745054d6":"# 4. Create csv to upload to Kaggle","85d52070":"Let's look at the first 3 corresponding target variables. This is the measure of whether the passenger survived or not (i.e. the first passenger (22 year-old male) did not survive, but the second passenger (38 year-old female did survive).\n\nOur classifier will use this to know what the output should be for each of the training instances.","3a6ffe5e":"# 1. Process the data","860eb2fa":"Our data has the following columns:\n\n* PassengerId - Each passenger's id\n* Survived - Whether the passenger survived or not (1 - yes, 0 - no)\n* Pclass - The passenger class: (1st class - 1, 2nd class - 2, third class - 3)\n* Sex - Each passenger's sex\n* Age - Each passenger's age","104e4a58":"# Prepare the data to be read by our algorithm"}}