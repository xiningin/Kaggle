{"cell_type":{"96f8bcb8":"code","1c9d2572":"code","16245898":"code","97427ed1":"code","06cfd51e":"code","88871644":"code","f4a8468e":"code","203c857f":"code","768cf3f9":"code","b242ca1e":"code","42ab0ae9":"code","0438697e":"code","c320fbc1":"code","a0b291f5":"code","3bdcfd8f":"code","1f4c743e":"code","8ef47ee7":"code","c4551e13":"code","5ca48792":"code","671f5355":"code","defbe9ab":"code","815fb0f4":"markdown","b5b06073":"markdown","3cdc5b90":"markdown","e7c84c3b":"markdown","ba7f38e8":"markdown","78117ae4":"markdown","7b01d02f":"markdown","622cf438":"markdown","6c6d9705":"markdown","d4f067c5":"markdown","7aa35dc5":"markdown","6dfb1e43":"markdown","14b158bf":"markdown","c386ed15":"markdown","45434b1a":"markdown","4b7c69e2":"markdown","fbf11e2e":"markdown","355f25a8":"markdown","2243c0fd":"markdown","ba4d391a":"markdown"},"source":{"96f8bcb8":"import numpy as np \nimport pandas as pd","1c9d2572":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","16245898":"train.head()","97427ed1":"train.describe()","06cfd51e":"# Find number of nulls in each column\nnull_columns=train.columns[train.isnull().any()]\ntrain[null_columns].isnull().sum()","88871644":"# Prediction target\ny = train.Survived\n# Features (what's used to predict)\nfeatures = ['Fare', 'Pclass']\nX = train[features]","f4a8468e":"from sklearn.linear_model import LogisticRegression\n\n# random_state controls the random number generator being used \ntitanic_mdl = LogisticRegression(random_state=0)\n\ntitanic_mdl.fit(X, y)","203c857f":"X_preds = titanic_mdl.predict(X)\nprint(X_preds)","768cf3f9":"train.Survived","b242ca1e":"from sklearn.metrics import mean_absolute_error\n\nstr(mean_absolute_error(y, X_preds) * 100) + '% average error' ","42ab0ae9":"train.head()","0438697e":"new_features = ['Pclass', 'Sex', 'Fare']\nX = train[new_features]\n\nnull_columns=X.columns[X.isnull().any()]\nX[null_columns].isnull().sum()","c320fbc1":"new_features = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare']\nX = train[new_features]\nX.head(2)","a0b291f5":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.pipeline import make_pipeline\n\n# Add Sex as feature\nnew_features = ['Pclass', 'Sex', 'Fare']\n\n# Group numeric and non-numeric into lists\nnumeric_select = make_column_selector(dtype_include = \"number\")\nno_numeric_select = make_column_selector(dtype_exclude = 'number')\n\n# Create preprocessor for OH encoder and Imputer for missing numeric values\npreprocessor = make_column_transformer(\n    (make_pipeline(SimpleImputer(strategy = \"mean\"), StandardScaler()), numeric_select), # select all numeric columns, impute mean and scale the data\n    (make_pipeline(SimpleImputer(strategy = \"constant\"), OneHotEncoder(handle_unknown='ignore')), no_numeric_select) # select all non numeric columns, impute most frequent value and OneHotEncode\n) \n# Make pipeline for with model\npipe = make_pipeline(preprocessor, LogisticRegression(random_state=0))\n\npipe.fit(X, y)\nX_preds = pipe.predict(X)\n\n# get_feature_mae(new_features, test, y)","3bdcfd8f":"def get_feature_mae(X, y):\n    \"\"\"Pass features as list along with DataFrame and predicted Series\"\"\"    \n    # Group numeric and non-numeric into lists\n    numeric_select = make_column_selector(dtype_include = \"number\")\n    no_numeric_select = make_column_selector(dtype_exclude = 'number')\n\n    # Create preprocessor for OH encoder and Imputer for missing numeric values\n    preprocessor = make_column_transformer(\n        (make_pipeline(SimpleImputer(strategy = \"mean\"), StandardScaler()), numeric_select), # select all numeric columns, impute mean and scale the data\n        (make_pipeline(SimpleImputer(strategy = \"constant\"), OneHotEncoder(handle_unknown='ignore')), no_numeric_select) # select all non numeric columns, impute most frequent value and OneHotEncode\n    ) \n    # Make pipeline for with model\n    pipe = make_pipeline(preprocessor, LogisticRegression(random_state=0))\n\n    pipe.fit(X, y)\n    X_preds = pipe.predict(X)\n    \n    return str(mean_absolute_error(y, X_preds) * 100) + '% average error' \n\ny = train.Survived\n\n# Sibling\/Spouse feature\nnew_features = ['Pclass', 'SibSp', 'Fare']\nX = train[new_features]\nsib_spouse_mae = get_feature_mae(X, y)\n\n# Parent\/Child feature\nnew_features = ['Pclass', 'Parch', 'Fare']\nX = train[new_features]\nparent_child_mae = get_feature_mae(X, y)\n\n# Sex feature\nnew_features = ['Pclass', 'Sex', 'Fare']\n# parent_child_mae = get_feature_mae(new_features, train, y)\n\nprint('SibSp: '+sib_spouse_mae, '\\nParch: '+parent_child_mae)","1f4c743e":"y = train.Survived\ntest.head()","8ef47ee7":"from sklearn.model_selection import cross_val_score\n\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\nX = pd.get_dummies(train[features])\nX_test = pd.get_dummies(test[features])\ny = train.Survived\n\n# Group numeric and non-numeric into lists\nnumeric = make_column_selector(dtype_include = \"number\")\nnon_numeric = make_column_selector(dtype_exclude = 'number')\n\n# Create preprocessor for OH encoder and Imputer for missing numeric values\npreprocessor = make_column_transformer(\n    (make_pipeline(SimpleImputer(strategy = \"mean\"), StandardScaler()), numeric), # select all numeric columns, impute mean and scale the data\n    (make_pipeline(SimpleImputer(strategy = \"constant\"), OneHotEncoder(handle_unknown='ignore')), non_numeric) # select all non numeric columns, impute most frequent value and OneHotEncode\n) \n# Make pipeline for with model\npipe = make_pipeline(preprocessor, LogisticRegression(random_state=0))\n\npipe.fit(X, y)\ny_pred = pipe.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': y_pred})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")\n\n# get_feature_mae(features, train, train.Survived)\n# print(\"CV score is {}\".format(cross_val_score(pipe, X, y).mean()))","c4551e13":"from xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score, roc_auc_score, mean_squared_error as MSE\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\n# from sklearn.selection import \n\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')","5ca48792":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","671f5355":"# They all pretty much control the model fitting\nparams = {\n    'subsample': [0.8], \n    'min_child_weight': [5], \n    'max_depth': [3], \n    'gamma': [1], \n    'colsample_bytree': [0.8]\n}","defbe9ab":"features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\nX = pd.get_dummies(train[features])\nX_test = pd.get_dummies(test[features])\ny = train.Survived\n\n# Group numeric and non-numeric into lists\nnumeric = make_column_selector(dtype_include = \"number\")\nnon_numeric = make_column_selector(dtype_exclude = 'number')\n\n# Create preprocessor for OH encoder and Imputer for missing numeric values\npreprocessor = make_column_transformer(\n    (make_pipeline(SimpleImputer(strategy = \"mean\"), StandardScaler()), numeric), # select all numeric columns, impute mean and scale the data\n    (make_pipeline(SimpleImputer(strategy = \"constant\"), OneHotEncoder(handle_unknown='ignore')), non_numeric) # select all non numeric columns, impute most frequent value and OneHotEncode\n) \n\nmodel_xgb = XGBClassifier(\n    random_state=42,\n)\n\nmodel_xgb = RandomizedSearchCV(\n    model_xgb, \n    params, \n    cv=5,\n    scoring='accuracy',\n)\n\n# Make pipeline for with model\npipe = make_pipeline(preprocessor, model_xgb)\n\npipe.fit(X, y)\n\ny_pred = pipe.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': y_pred})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\n# get_feature_mae(features, train, train.Survived)\nprint(\"CV score is {}\".format(cross_val_score(pipe, X, y).mean()))","815fb0f4":"**StratifiedKFold** provides a class to split the cross-validation and test data into different groups to prevent overfitting. ```n_splits``` defines how many groups.\n\n\n**RandomizedSearchCV** looks for best parameters","b5b06073":"Each prediction is on average 32% off which is pretty terrible.","3cdc5b90":"### Pipelines\nThe Sex column contains categorical data like male and female. The model cannot make sense of string values so converting them to numeric values is the solution. Luckily, sklearn has a class that does this for us OneHotEncoder.\n\nFor missing numeric values, replacing them is called imputation.\n\nBoth solutions are combined in a pipeline to ease preprocessing.","e7c84c3b":"## Combining All Columns\n21% is a big improvement from where I started but it's still pretty inaccurate.","ba7f38e8":"## Make Model\nLogistic Regression is used because it better predicts binary (yes\/no) fields.\n\n\n### Regression Analysis\n- Logistic regression is a type of regression analysis. Regression analysis is a predictive modeling technique whicg finds the relationship between a dependent and independent variable(s). \n- It is useful for predicting specific metrics like the number of units a company need to produce to meet demand. Another might be measuring the impact an independent variable has on the dependent.\n\nRegresson analysis has two camps: **Linear regression** and **Logistic regression**\n\n### Linear Regression\nIs used for predictive analysis to find the extent of the relationship between the independent and dependent variables. It would be used to describe the effect ads would have on a company's revenue. It would output a trend line with ads on the x and revenue on the y.\n\n\n### Logistic Regression\n**This a classification algorithm used to predict a binary outcome based on a set of independent variables**\n\nIn other words, only use this if the prediction is a yes\/no, pass\/fail, etc.\n\nFor the independent variables, the same does not apply. They can be:\n1. Continuous - temperature, mass, price.\n2. Discrete, ordinal - rated customer satisfaction (scaled from 1-5)\n3. Discrete, nominal - fits into groups but there is no hierarchy (colours)","78117ae4":"## Add New Features\nAdding features will give the model more data to work with. Hopefully, this will increase the accuracy but first I need to find suitable features.","7b01d02f":"177 entries in the set are null for the Age column, to save deleting\/imputing so many rows of data I will exclude this from the features for now and include it later on.","622cf438":"I'm trying to find features here that influence the survivablity of a passenger:\n- Pclass: determines where a person sleeps on the ship (useful)\n- Name: probably no effect\n- Sex: women were given preference for lifeboats at the time\n- Age: younger passengers (to an extent) are more mobile and healthy\n- SibSp: may have caused the passenger to stay aboard\n- Parch: similar effect to SibSp\n- Fare: the price of the fare indicates order to the lifeboats\n- Ticket: probably no effect\n- Cabin: probably no effect\n- Embarked: probably no effect\n\nThe features to start with are the ones most likely to affect:\n1. Pclass\n2. Sex\n3. Age\n4. SibSp\n5. Parch\n6. Fare\n\nBefore adding them, I'm going to check if values are null in these columns.","6c6d9705":"## Bias & Variance\nWe're graphing two variables and need to find relationship the relationship between them. Using **Linear Regression** first, a line is drawn between the data points but the relationship is non-linear so the line does not fit. \nThis is called **Bias-a models inabilty to capture the true relationship**\n\n![bias-variance-comparison](https:\/\/miro.medium.com\/max\/1400\/1*9hPX9pAO3jqLrzt0IE3JzA.png)\n\nThe high variance scenario models the points too accurately causing overfitting with good performance in training but falling behind in the real world. \n\nIf we were to measure the MSE (Mean Squared Error) for the training the error the high bias model it score very high but with the high variance it would be nill.\n\nNow testing on the training set, the high bias scores a moderate MSE where as the hig variance which did well last time now does far worse as it was too accustomed to the training data.\n\nFinding the sweet spot between overfitting and underfitting is called regularisation and boosting.","d4f067c5":"## Cross Validation\nWhen trying to predict whether a patient has heart disease or not, choosing which machine learning method is very important-this is where cross validation is used.\n\n**Cross Validation** allows us to compare different ML methods to get a feel for how they work in practise.\n\nWe have a column recording who has heart disease, we need to do two things with it\n\n1. Estimate the parameters for the ML methods-training\n2. Evaluate how well the methods works-testing\n\nUsing all the data at once would lead to overfitting to the training data. A better approach would be to split it 75:25 in favour of testing. The ML methods could then be compared to see how well the test data is categorised.\n\n***Why not use the first 25% for training and the last 75% for testing? Or use the middle 25%?***\n\nCross validation solves this by using them all to find the best way to split the data. It loops over each quarter and keeps track of which split is the best way to train the algorithm. Dividing the data into four blocks-this called **Four-Fold cross validation**.\n\nUsing each row in the data as a block is **Leave One Out Cross Validation** \n\nIn practise, **Ten-Fold Cross Validation** is used more often where it's split into ten parts.\n\nThe different ML methods can now be compared.","7aa35dc5":"## Decision Trees\nDecison Tress can give boolean predictions like Logistic Regression but also do the same with numeric data. Each branch down the tree narrows the search.\n\nLingo:\n- Top box is the Root Node. Have arrows pointing from them\n- Middle boxes are Nodes or Internal Nodes. Have arrows pointing from and to them.\n- Bottom boxes are the Leaf Nodes or Leaves. Have arrows pointing to them.\n\n![decison-tree](https:\/\/www.displayr.com\/wp-content\/uploads\/2018\/07\/what-is-a-decision-tree.png)\n\n### Example\nGoing from columns and rows to a decision tree. \n\nWe're trying to predict whether a patient has heart disease based on a number of parameters. \n\nParameters:\n1. Chest Pain\n2. Good Circulation\n3. Blocked Arteries\n\nThe decision tree does this by figuring out which parameter should be the root node of the decision tree.\n\nSteps:\n- The tree goes through each row for the first parameter (chest pain)\n- Checks if whether chest pain and heart disease are true\n- Stores the result in a node\n- Repeats for each row\n- Moves onto remaining parameters and repeats earlier steps\n\nAfter completing the steps above, there are no leaf nodes which 100% indicate heart disease so they are all considered **impure**. To find the root node now, we need to find a way of measuring impurity.\n\n### Gini Method\nThis is a way of measuring the impurity of leaf nodes.\n\n$$ Gini(t) = 1 - \\sum_{i=1}^{j} P(i | t)^2 $$\n\n- j is the length of the set\n- i is yes\n- t is no\n\nFor this example, one of the leaves has a **105 with heart disease and chest pain** while having **39 with only chest pain**.\n\nThe probability of yes:\n$$ = \\frac{105}{105 + 39} ^2 = 0.5329 $$\n\nThe probability of no:\n$$ = \\frac{39}{105 + 39} ^2 = 0.0729 $$\n\nGini Impurity is these taken away:\n$$ 1 - 0.0729 - 0.5329 = 0.395 $$\n\nThe same is repeated for another leaf node which has a Gini Impurity of **0.336**. The issue here is that the first leaf node has 144 patients and the second one has 159. The total Gini impurity for using Chest Pain is the **weighted mean of the leaf node impurities**.\n\n$$ \\frac{144}{144 + 159} 0.395 + \\frac{159}{144 + 159} 0.336 = 0.364 $$\n\nWhen it comes to Good Blood Circulation, 0.36 was the Gini impurity. The lower the impurity the better suited is for the root node. \n\nGood Blood Circulation will now be used for the root node. Now the same method is applied but with Good Blood Circulation at the top of the tree.\n\n1. Calculate all of the Gini impurities\n2. The lowest impurity becomes a leaf node\n3. If separating the data results in an improvement, pick the separation with the lowest value\n\n","6dfb1e43":"&nbsp;\n## Gradient Descent with XGBoost\nGradient descent is an algoritm used to improve the accuracy of a model. When plotting the error of the model against the parameters, peaks and troughs appear; peaks corresponding to higher error; troughs to lower error.\n\n![gradient-descent-graph](https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fs3-ap-south-1.amazonaws.com%2Fav-blog-media%2Fwp-content%2Fuploads%2F2017%2F03%2F06100746%2Fgrad.png&f=1&nofb=1)\n\nThe goal of gradient descent is to \"slide\" down these waves to the lowest part, or in the lingo \"a global miniumum\".\n\nBelow is the formula used: \n\n$$Repeat \n$$\\{ \n$$ \\theta_{j} := \\theta_{j} - \\alpha \\frac {\\partial} {\\partial\\theta_{j}} J_{\\theta}\n$$\\}\n\nacross the entire set until the error does not decrease.\n\nThe alpha parameter above defines the learning rate of Gradient Descent; the bigger the alpha, the faster it learns. Setting aplha to a huge number seems like the easiest solution but a value of alpha too high causes the descent to overshoot and miss the minimum, sometimes leading to an increase in error. The ideal solution is to start small, check the error; if the error doesn't reduce much; increase, if the error goes up, reducing alpha might be the best course of action.","14b158bf":"### Validate Model\nThis puts a number on the accuracy of the model. Validation finds the error between the predicted and actual values. One metric is Mean Absolute Error (MAE) which finds the absolute value of the error (converts to positive) then finds the average of in the set.","c386ed15":"### Predict Model\nThe prediction below is made using the model I created in the previous cell. The next cell down are the actual values.","45434b1a":"That's an 11% decrease in MAE; seems the sex of the passenger has quite a big impact on how likely they are to survive.","4b7c69e2":"### XGBoost: Parameter Grid\nThis grid of possible values for model parameters can be used to tweak the model and iterate over different parameters to find the best fit for the model. [Docs](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#parameters-for-tree-booster).  This [guy](https:\/\/www.kaggle.com\/tilii7\/hyperparameter-grid-search-with-xgboost) helped.","fbf11e2e":"**Parameters:**\n- ```learning_rate```: Alpha learning rate\n- ```n_estimators```: how many times to run gradient boost\n- ```objective```: classification type\n- ```silent```: display output\n- ```n_threads```: number of threads to run","355f25a8":"### Define Predictors & Features\n\nAfter watchin the movie, most of the ones who didn't make it off the boat were in the lower decks (third\/second class). Suggesting the more money spent (Fare) gets a higher ticket class (Pclass).","2243c0fd":"## Ridge Regression (L2)\n#### Example\nWhen finding the relationship between the weight and size of mice, Linear Regression is used. Linear Regression uses the Least Squares method, finding the minimum absolute value from the line to the point.\n\n![linear-regression](https:\/\/i.ytimg.com\/vi\/Q81RR3yKn30\/maxresdefault.jpg)\n\nLinear Regression can accurately measure the relationship if there are many data points, but if there are only two, like above, the training and test models are mismatched. \n\n- The green dots above are the test data, the red are the training data.\n- The training data has a sum of squares equal to zero\n- The testing data has a large sum of squares = high **variance**-overfit \n\nThe idea behind ridge regression is to find a new line to less fit the training data by introducing a small amount of **Bias**, in turn greatly reducing **Variance**. \n\n**Ridge Regression starts with a worse fit but in the long term yields more accurate predictions**\n\n\n#### How it works\nWhen ***Least Squares*** determines the values for the equation below:\n\n$$ y = mx + c $$\n\nit minimises:\n\n$$ sum of the square residuals $$\n\nWhen ***Ridge Regression*** determines the values for the equation below:\n\n$$ y = mx + c $$\n\nit minimises:\n\n$$ sum of the square residuals + (\\lambda m^2)$$\n\n- m (slope) adds a penalty to the Least Squares method \n- Lambda determines the severity of that penalty.\n\nThe bias used to reduce variance in Ridge Regression is the lambda symbol above. To find the optimal value for the Ridge Regression penalty (lambda), Ten-Fold Cross Validation is used.\n\nMore complex equations with many hyperparameters, follow the same Ridge Regression formula. Say we have the slopes of three different lines, m1, m2 and m3, the Ridge Regression Penalty for that is:\n$$\\lambda(m_1^2 + m_2^2 + m_3^2)$$\n\nThe Least Squares equations requires a data point for each parameter to make a prediction, e.g. a dataset with 2 paramaters requires 2 data points and one with 10,000 parameters requires 10,000 points. It costs a huge amount to sequence DNA, thus, many parameters and a small sample size-bad for Least Squares.\n\nRidge Regression can help here by using Cross Validation and the Ridge Regression Penalty to square and sum slopes of each parameter.","ba4d391a":"Rather than throw all features in at one time and compare them to the original predictions, adding features individually might yield more insight into each their influence."}}