{"cell_type":{"ed6bc5f7":"code","0ef6d828":"code","2b42aa6b":"code","2065378f":"code","c324764d":"code","1211bffd":"code","1fe73756":"code","f92bf437":"code","755a2652":"code","fcd36425":"code","784aaeab":"code","dc5f480c":"code","710e0e0c":"code","6708e5a7":"code","d926ce1b":"code","6e39843c":"code","f0c5471d":"code","d5b37b3c":"code","4aa9151c":"code","fd7eab92":"code","c60866f1":"code","7b52f9ce":"code","3273d3cb":"code","15f43258":"code","cb752904":"code","df0bf5dd":"code","e17eb69f":"code","e768d6cd":"code","c3ea7d26":"code","ed7878c1":"code","2f3e26c3":"code","48de5910":"code","84e0536b":"code","bb91a8ef":"code","debaf3aa":"code","ac371ce2":"code","cb64d387":"code","0aa98936":"code","3fe18444":"code","6a640f13":"code","e5a98c90":"code","407e76c0":"code","a4126467":"code","e7c4076d":"code","a8e467b4":"code","8b3db46e":"code","987daaaa":"code","9126f42c":"code","4fa80e9f":"code","4df59db0":"code","beae01bb":"code","952e37be":"code","fa8dfe7f":"code","bd2fea33":"code","6d46816f":"code","26fc751d":"code","f3fa688f":"code","8cac607b":"code","ce5e8dbf":"code","5a04387f":"code","1588ed10":"code","d2b2669b":"code","3e618597":"code","89ad67e8":"code","20d8c2b5":"code","7ed72a9c":"code","2df0020d":"code","17925b05":"code","ae7dd222":"code","1def8d13":"code","8bf8801b":"code","de73e698":"code","7bd297fe":"code","2255eafd":"code","45bfcb43":"markdown","ac3d8238":"markdown","26446cd7":"markdown","020dd020":"markdown","46593f76":"markdown","58fbff74":"markdown","49ad421d":"markdown","832cd8db":"markdown","3be14de8":"markdown"},"source":{"ed6bc5f7":"!wget 'http:\/\/nlp.stanford.edu\/data\/glove.42B.300d.zip'\n# !unzip '\/content\/glove.42B.300d.zip'\n!unzip '.\/glove.42B.300d.zip'","0ef6d828":"pip install contractions","2b42aa6b":"# from google.colab import drive\n# drive.mount('\/content\/drive')","2065378f":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport contractions\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras import backend as K \nfrom tensorflow.python.keras.layers import Layer\n","c324764d":"# !cp \/content\/drive\/MyDrive\/news_summary_more.csv \/content","1211bffd":"##Colab\n# data_path = '\/content\/news_summary_more.csv'\n# data = pd.read_csv(data_path)\n# data.head()","1fe73756":"#Kaggle\ndata_path = '..\/input\/news-summary\/news_summary_more.csv'\ndata = pd.read_csv(data_path)\ndata.head()","f92bf437":"data.drop_duplicates(subset=['headlines'],inplace=True)\ndata.reset_index(inplace=True, drop=True)","755a2652":"data.info()","fcd36425":"stop_words = stopwords.words('english')\n\ndef preprocess(text):\n    text = text.lower()\n    text = ' '.join([contractions.fix(word) for word in text.split(\" \")])    \n    \n    tokens = [w for w in text.split() if not w in stop_words]\n    text = \" \".join(tokens)\n    text = text.replace(\"'s\",'')\n    text = text.replace(\".\",'')\n    text = re.sub(r'\\(.*\\)','',text)\n    text = re.sub(r'[^a-zA-Z0-9. ]',' ',text)\n    text = re.sub(r'\\.','. ',text)\n    text = re.sub(r'\\s+', ' ', text)\n    return text\n","784aaeab":"data['headlines'] = data['headlines'].apply(preprocess)\ndata['text'] = data['text'].apply(preprocess)\ndata['headlines'] = data['headlines'].apply(lambda x : '_START_ '+ x + ' _END_')\n\nfor i in range(2):\n    print('Summary:', data['headlines'][i],'Text:', data['text'][i], sep='\\n')\n    print()","dc5f480c":"headlines_length = [len(x.split()) for x in data.headlines]\ntext_length = [len(x.split()) for x in data.text]","710e0e0c":"fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,5))\nax1.hist(headlines_length, bins = 20)\nax2.hist(text_length, bins = 20)\n\nax1.title.set_text(\"Words in Headlines\")\nax2.title.set_text(\"Words in Text\")\nplt.show()","6708e5a7":"glove_size = 300\n#colab\n# f = open('\/content\/glove.42B.300d.txt')\n#Kaggle\nf = open('..\/input\/glove42b300dtxt\/glove.42B.300d.txt')","d926ce1b":"embeddings_index = dict()\nfor line in f:\n\tvalues = line.split()\n\tembeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\nf.close()","6e39843c":"words_source_train = []\nfor i in data['text'] :\n  words_source_train.extend(i.split(' '))\n\nprint(\"all the words in the corpus\", len(words_source_train))\nwords_source_train = set(words_source_train)\nprint(\"the unique words in the corpus\", len(words_source_train))\ninter_words = set(embeddings_index.keys()).intersection(words_source_train)\nprint(\"The number of words that are present in both glove vectors and our corpus are {} which \\\nis nearly {}% \".format(len(inter_words), np.round((float(len(inter_words))\/len(words_source_train))\n*100)))\n\nwords_corpus_source_train = {}\nwords_glove = set(embeddings_index.keys())\nfor i in words_source_train:\n  if i in words_glove:\n    words_corpus_source_train[i] = embeddings_index[i]\nprint(\"word 2 vec length\", len(words_corpus_source_train))","f0c5471d":"\ndef num(text):\n  words = [w for w in text.split() if not w in inter_words]\n  return len(words)\n\ndata['unique_words'] = data['text'].apply(num)\n","d5b37b3c":"t=0\nr=[t+1 for w in data.unique_words if w>0]\nprint(\"Number of examples with words outside the GloVe vocabulary\",sum(r))\n\n","4aa9151c":"#Removing examples which have more than 4 unknown words out of the corpus\ndata = data[data['unique_words'] < 4]\ndata.reset_index(inplace=True, drop=True)","fd7eab92":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(data['text'], data['headlines'], test_size = 0.2, random_state = 20)\nX_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size = 0.5, random_state = 20)","c60866f1":"max_length_x = max(text_length)\nmax_length_y = max(headlines_length)","7b52f9ce":"x_t = Tokenizer()\nx_t.fit_on_texts(data['text'] + data['headlines'])\nx_vocab_size = len(x_t.word_index) + 1\n\nencoded_xtrain = x_t.texts_to_sequences(X_train)\nencoded_xval = x_t.texts_to_sequences(X_val)\nencoded_xtest = x_t.texts_to_sequences(X_test)\n\npadded_xtrain = pad_sequences(encoded_xtrain, maxlen=max_length_x, padding='post')\npadded_xval = pad_sequences(encoded_xval, maxlen=max_length_x, padding='post')\npadded_xtest = pad_sequences(encoded_xtest, maxlen=max_length_x, padding='post')","3273d3cb":"y_t = Tokenizer()\ny_t.fit_on_texts(data['headlines'])\ny_vocab_size = len(y_t.word_index) + 1\n\nencoded_ytrain = y_t.texts_to_sequences(y_train)\nencoded_yval = y_t.texts_to_sequences(y_val)\nencoded_ytest = y_t.texts_to_sequences(y_test)\n\npadded_ytrain = pad_sequences(encoded_ytrain, maxlen=max_length_y, padding='post')\npadded_yval = pad_sequences(encoded_yval, maxlen=max_length_y, padding='post')\npadded_ytest = pad_sequences(encoded_ytest, maxlen=max_length_y, padding='post')","15f43258":"print('Loaded %s word vectors.' % len(embeddings_index))\n\nembedding_matrix = np.zeros((x_vocab_size, glove_size))\nfor word, i in x_t.word_index.items():\n\tembedding_vector = embeddings_index.get(word)\n\tif embedding_vector is not None:\n\t\tembedding_matrix[i] = embedding_vector","cb752904":"class AttentionLayer(Layer):\n\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n\n        self.W_a = self.add_weight(name='W_a',\n                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.U_a = self.add_weight(name='U_a',\n                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.V_a = self.add_weight(name='V_a',\n                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer='uniform',\n                                   trainable=True)\n\n        super(AttentionLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        encoder_out_seq, decoder_out_seq = inputs\n\n        def energy_step(inputs, states):\n          \n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  \n            \n            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n            e_i = K.softmax(e_i)\n\n            return e_i, [e_i]\n\n        def context_step(inputs, states):\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            return c_i, [c_i]\n\n        def create_inital_state(inputs, hidden_size):\n            \n            fake_state = K.zeros_like(inputs)  \n            fake_state = K.sum(fake_state, axis=[1, 2])  \n            fake_state = K.expand_dims(fake_state)  \n            fake_state = K.tile(fake_state, [1, hidden_size])  \n            return fake_state\n\n        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  \n\n        last_out, e_outputs, _ = K.rnn(\n            energy_step, decoder_out_seq, [fake_state_e],\n        )\n\n        last_out, c_outputs, _ = K.rnn(\n            context_step, e_outputs, [fake_state_c],\n        )\n        return c_outputs, e_outputs\n\n    def compute_output_shape(self, input_shape):\n        return [\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n        ]\n","df0bf5dd":"latent_dim=500\n\nK.clear_session() \n\nencoder_inputs = Input(shape=(max_length_x,)) \nenc_emb = Embedding(x_vocab_size, glove_size, weights=[embedding_matrix],input_length=max_length_x, trainable=False)(encoder_inputs) \n\n#LSTM \nencoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \nencoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \nencoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True) \nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n\n# Decoder. \ndecoder_inputs = Input(shape=(None,)) \ndec_emb_layer = Embedding(x_vocab_size, glove_size, weights=[embedding_matrix],input_length=max_length_x, trainable=False) \ndec_emb = dec_emb_layer(decoder_inputs) \n\n#LSTM using encoder_states as initial state\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n\n#Attention Layer\nattn_layer = AttentionLayer(name='attention_layer') \nattn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n\ndecoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\ndecoder_dense = TimeDistributed(Dense(y_vocab_size, activation='softmax')) \ndecoder_outputs = decoder_dense(decoder_concat_input) \n\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs) \nprint(model.summary())","e17eb69f":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n# checkpoint_filepath = '\/content\/model.{epoch:02d}-{val_loss:.2f}.h5'\ncheckpoint_filepath = '.\/model.{epoch:02d}-{val_loss:.2f}.h5'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,save_weights_only=True,monitor='loss',mode='min',save_best_only=True, save_freq = \"epoch\")\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nhistory=model.fit([padded_xtrain,padded_ytrain[:,:-1]], padded_ytrain.reshape(padded_ytrain.shape[0],padded_ytrain.shape[1], 1)[:,1:] ,epochs=25,batch_size=512, validation_data=([padded_xval,padded_yval[:,:-1]], padded_yval.reshape(padded_yval.shape[0],padded_yval.shape[1], 1)[:,1:]), callbacks=[es, model_checkpoint_callback])","e768d6cd":"model1 = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel1.load_weights(\".\/model.11-3.22.h5\")\nmodel1.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\nhistory=model1.fit([padded_xtrain,padded_ytrain[:,:-1]], padded_ytrain.reshape(padded_ytrain.shape[0],padded_ytrain.shape[1], 1)[:,1:] ,epochs=3,batch_size=512, validation_data=([padded_xval,padded_yval[:,:-1]], padded_yval.reshape(padded_yval.shape[0],padded_yval.shape[1], 1)[:,1:]), callbacks=es)","c3ea7d26":"from matplotlib import pyplot \npyplot.plot(history.history['loss'], label='train') \npyplot.plot(history.history['val_loss'], label='test') \npyplot.legend() \npyplot.show()","ed7878c1":"reverse_target_word_index = y_t.index_word \nreverse_source_word_index = x_t.index_word \ntarget_word_index = y_t.word_index","2f3e26c3":"encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(max_length_x,latent_dim))\n\ndec_emb2= dec_emb_layer(decoder_inputs)\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\nattn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\ndecoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n\ndecoder_outputs2 = decoder_dense(decoder_inf_concat)\n\ndecoder_model = Model(\n[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n[decoder_outputs2] + [state_h2, state_c2])","48de5910":"def decode_sequence(input_seq):\n    input_seq= input_seq.reshape(1,max_length_x)\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n    target_seq = np.zeros((1,1))\n    target_seq[0, 0] = target_word_index['start']\n\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n  \n        if(sampled_token!='end'):\n            decoded_sentence += ' '+sampled_token\n \n        if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_length_y-1)):\n                stop_condition = True\n\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n        e_h, e_c = h, c\n\n    return decoded_sentence","84e0536b":"def seq2summary(input_seq):\n    newString=''\n    for i in input_seq:\n      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n        newString=newString+reverse_target_word_index[i]+' '\n    return newString\n\ndef seq2text(input_seq):\n    newString=''\n    for i in input_seq:\n      if(i!=0):\n        newString=newString+reverse_source_word_index[i]+' '\n    return newString","bb91a8ef":"for i in range(10):\n  print(\"Review:\",seq2text(padded_xtrain[i]))\n  print(\"Original summary:\",seq2summary(padded_ytrain[i]))\n  print(\"Predicted summary:\",decode_sequence(padded_xtrain[i]))\n  print(\"\\n\")","debaf3aa":"import tensorflow_hub as hub\nfrom scipy import spatial\nmodule_url = \"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\" \nsentence_encoder = hub.load(module_url)\nprint (\"module %s loaded\" % module_url)","ac371ce2":"def cosine_similarity(padded_xval, padded_yval):\n  scores = []\n  for i in range(len(padded_xval)):\n    \n    str1 = seq2summary(padded_yval[i])\n    str2 = decode_sequence(padded_xval[i])\n    embeddings = sentence_encoder([str1, str2]).numpy()\n    result = 1 - spatial.distance.cosine(embeddings[0], embeddings[1])\n    scores.append(result)\n  return scores","cb64d387":"%%time\nscores = cosine_similarity(padded_xtest[:500],padded_ytest[:500] )","0aa98936":"np.mean(np.sort(scores)[-50:])","3fe18444":"np.mean(scores)","6a640f13":"import math\nimport collections\ndef bleu(pred_seq, label_seq, k): \n    \"\"\"Compute the BLEU.\"\"\"\n    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n    len_pred, len_label = len(pred_tokens), len(label_tokens)\n    score = math.exp(min(0, 1 - len_label \/ len_pred))\n    for n in range(1, k + 1):\n        num_matches, label_subs = 0, collections.defaultdict(int)\n        for i in range(len_label - n + 1):\n            label_subs[''.join(label_tokens[i:i + n])] += 1\n        for i in range(len_pred - n + 1):\n            if label_subs[''.join(pred_tokens[i:i + n])] > 0:\n                num_matches += 1\n                label_subs[''.join(pred_tokens[i:i + n])] -= 1\n        score *= math.pow(num_matches \/ (len_pred - n + 1), math.pow(0.5, n))\n    return score","e5a98c90":"for i in range(0,5):\n  scoresbleu = []\n  s = bleu(seq2summary(padded_yval[i]),decode_sequence(padded_xval[i].reshape(1,max_length_x)),2)\n  scoresbleu.append(s)\n  print(s)","407e76c0":"scoresbleu","a4126467":"p","e7c4076d":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\ntfidf.fit([\"hi\", \"hello\"])","a8e467b4":"X = tfidf.transform([\"hi\", \"hello\"])","8b3db46e":"X","987daaaa":"print([X[1, tfidf.vocabulary_['hi']]])\n","9126f42c":"sentence_embeddings = model([\"keep asking google assistant marry you google india users\",\"keep asking google assistant in modi google ai google india\"])\nembed=sentence_embeddings.numpy()","4fa80e9f":"embed","4df59db0":"from scipy import spatial\n\ndataSetI = embed[0]\ndataSetII = embed[1]\nresult = 1 - spatial.distance.cosine(dataSetI, dataSetII)\nresult","beae01bb":"import sklearn\ndef cos(X,Y):\n  c = 0\n  for i in range(len(X)):\n      c += X[i]*Y[i]\n  cosine = c\/ float((sum(X)*sum(Y))**0.5)\n  print(\"similarity: \", c)","952e37be":"cos(embed[0],embed[1])","fa8dfe7f":"import torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","bd2fea33":"device","6d46816f":"SOS_token = 0\nEOS_token = 1\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n        self.n_words = 2\n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1","26fc751d":"def readLangs(text, summary):\n    \n    pairs = [[text[i],summary[i]] for i in range(len(text))]\n\n    input_lang = Lang(text)\n    output_lang = Lang(summary)\n\n    return input_lang, output_lang, pairs","f3fa688f":"def prepareData(lang1, lang2):\n    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n\n    print(f'Read {pairs} sentence pairs', end='\\n\\n')\n\n    for pair in pairs:\n        input_lang.addSentence(pair[0])\n        output_lang.addSentence(pair[1])\n\n    print(input_lang.name, input_lang.n_words, end='\\n\\n')\n    print(output_lang.name, output_lang.n_words, end='\\n\\n')\n    return input_lang, output_lang, pairs","8cac607b":"input_lang, output_lang, pairs = prepareData(x, y)\n","ce5e8dbf":"pairs[0]","5a04387f":"MAX_LENGTH = 70","1588ed10":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n\n    def forward(self, input, hidden):\n        embedded = self.embedding(input).view(1, 1, -1)\n        output = embedded\n        output, hidden = self.gru(output, hidden)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)","d2b2669b":"class DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        output = self.embedding(input).view(1, 1, -1)\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n        output = self.softmax(self.out(output[0]))\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)","3e618597":"class AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n        super(AttnDecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n\n    def forward(self, input, hidden, encoder_outputs):\n        embedded = self.embedding(input).view(1, 1, -1)\n        embedded = self.dropout(embedded)\n\n        attn_weights = F.softmax( self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n\n        output = torch.cat((embedded[0], attn_applied[0]), 1)\n        output = self.attn_combine(output).unsqueeze(0)\n\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n\n        output = F.log_softmax(self.out(output[0]), dim=1)\n        return output, hidden, attn_weights\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)","89ad67e8":"def indexesFromSentence(lang, sentence):\n    return [lang.word2index[word] for word in sentence.split(' ')]\n\n\ndef tensorFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(input_lang, pair[0])\n    target_tensor = tensorFromSentence(output_lang, pair[1])\n    return (input_tensor, target_tensor)","20d8c2b5":"teacher_forcing_ratio = 0.5\ndef train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n    encoder_hidden = encoder.initHidden()\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n\n    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n    loss = 0\n\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(\n            input_tensor[ei], encoder_hidden)\n        encoder_outputs[ei] = encoder_output[0, 0]\n\n    decoder_input = torch.tensor([[SOS_token]], device=device)\n\n    decoder_hidden = encoder_hidden\n\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    if use_teacher_forcing:\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di]\n\n    else:\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()\n\n            loss += criterion(decoder_output, target_tensor[di])\n            if decoder_input.item() == EOS_token:\n                break\n\n    loss.backward()\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item() \/ target_length","7ed72a9c":"import time\nimport math\n\ndef asMinutes(s):\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return f'{m}m {s}s'\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return f'Time: {asMinutes(s)} (ETA: {asMinutes(rs)})'","2df0020d":"import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport numpy as np\n\n\ndef showPlot(points):\n    plt.figure()\n    fig, ax = plt.subplots()\n    loc = ticker.MultipleLocator(base=0.2)\n    ax.yaxis.set_major_locator(loc)\n    plt.plot(points)","17925b05":"def trainIters(encoder, decoder, n_iters, print_every=100, plot_every=100, learning_rate=0.01):\n    print(\"Training....\")\n    start = time.time()\n    plot_losses = []\n    print_loss_total = 0\n    plot_loss_total = 0\n\n    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n    # encoder_optimizer = optim.AdamW(encoder.parameters())\n    # decoder_optimizer = optim.AdamW(decoder.parameters())\n    training_pairs = [tensorsFromPair(random.choice(pairs))\n                      for i in range(n_iters)]\n    \n    criterion = nn.NLLLoss()\n\n    for iter in range(1, n_iters + 1):\n        if iter% 100 == 0:\n            print(iter,\"\/\",n_iters + 1)\n        training_pair = training_pairs[iter - 1]\n        input_tensor = training_pair[0]\n        target_tensor = training_pair[1]\n\n        loss = train(input_tensor, target_tensor, encoder,\n                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n        print_loss_total += loss\n        plot_loss_total += loss\n\n        if iter % print_every == 0:\n            print_loss_avg = print_loss_total \/ print_every\n            print_loss_total = 0\n            #print(f'{(timeSince(start, iter \/ n_iters)} (iter: {iter} percent: {iter \/ n_iters * 100}%%) %.4f', % print_loss_avg))\n\n        if iter % plot_every == 0:\n            plot_loss_avg = plot_loss_total \/ plot_every\n            plot_losses.append(plot_loss_avg)\n            plot_loss_total = 0\n\n    showPlot(plot_losses)","ae7dd222":"def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n    with torch.no_grad():\n        input_tensor = tensorFromSentence(input_lang, sentence)\n        input_length = input_tensor.size()[0]\n        encoder_hidden = encoder.initHidden()\n\n        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n        for ei in range(input_length):\n            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n                                                     encoder_hidden)\n            encoder_outputs[ei] += encoder_output[0, 0]\n\n        decoder_input = torch.tensor([[SOS_token]], device=device)\n\n        decoder_hidden = encoder_hidden\n\n        decoded_words = []\n        decoder_attentions = torch.zeros(max_length, max_length)\n\n        for di in range(max_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            decoder_attentions[di] = decoder_attention.data\n            topv, topi = decoder_output.data.topk(1)\n            if topi.item() == EOS_token:\n                decoded_words.append('<EOS>')\n                break\n            else:\n                decoded_words.append(output_lang.index2word[topi.item()])\n\n            decoder_input = topi.squeeze().detach()\n\n        return decoded_words, decoder_attentions[:di + 1]","1def8d13":"def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print('Text:       ', pair[0])\n        print('Summary:    ', pair[1])\n        output_words, attentions = evaluate(encoder, decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n        print('Prediction: ', output_sentence)\n        print('')","8bf8801b":"type(input_lang.n_words)","de73e698":"import random\nhidden_size = 200\nencoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\nattn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n\ntrainIters(encoder1, attn_decoder1, 500, print_every=10)","7bd297fe":"torch.save(encoder1.state_dict(), '.\/enc.w')\ntorch.save(attn_decoder1.state_dict(), '.\/att.w')","2255eafd":"evaluateRandomly(encoder1, attn_decoder1)","45bfcb43":"### Building Model","ac3d8238":"**Torch Req**","26446cd7":"# Text Summarization - Encoder Decoder with Attention Mechanism","020dd020":"## Model","46593f76":"### Importing Basic libraries","58fbff74":"### Embedding Matrix from Glove\n","49ad421d":"### Inference","832cd8db":"### Importing Data","3be14de8":"### Evaluation"}}