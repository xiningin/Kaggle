{"cell_type":{"bee95fdf":"code","f30000e5":"code","83f49371":"code","8a67b55c":"code","aff10365":"code","8b3e2a03":"code","a5b2ea1c":"code","874b5151":"code","23c6dc66":"code","fdbd8eb1":"code","a5a43792":"code","4e3bc345":"code","af65edbb":"code","4f9f5446":"markdown","adad7061":"markdown","085d8f27":"markdown","ac35170c":"markdown","fc8a980c":"markdown"},"source":{"bee95fdf":"import pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\n\nfrom learntools.core import *\n\niowa_file_path = '..\/input\/train.csv'\niowa_test_file_path = '..\/input\/test.csv'\ntrain_data = pd.read_csv(iowa_file_path)\ntest_data = pd.read_csv(iowa_test_file_path)\n\ny = train_data.SalePrice\ntrain_features = train_data.drop(['Id', 'SalePrice'], axis = 1)\ntest_data_no_id = test_data.drop(['Id'], axis = 1)","f30000e5":"# fill in missing numeric values\nfrom sklearn.impute import SimpleImputer\n\n# impute\ntrain_data_num = train_features.select_dtypes(exclude=['object'])\ntest_data_num = test_data_no_id.select_dtypes(exclude=['object'])\nimputer = SimpleImputer()\ntrain_num_cleaned = imputer.fit_transform(train_data_num)\ntest_num_cleaned = imputer.transform(test_data_num)\n\n# columns rename after imputing\ntrain_num_cleaned = pd.DataFrame(train_num_cleaned)\ntest_num_cleaned = pd.DataFrame(test_num_cleaned)\ntrain_num_cleaned.columns = train_data_num.columns\ntest_num_cleaned.columns = test_data_num.columns","83f49371":"# string columns: transform to dummies\ntrain_data_str = train_data.select_dtypes(include=['object'])\ntest_data_str = test_data_no_id.select_dtypes(include=['object'])\ntrain_str_dummy = pd.get_dummies(train_data_str)\ntest_str_dummy = pd.get_dummies(test_data_str)\ntrain_dummy, test_dummy = train_str_dummy.align(test_str_dummy, \n                                                join = 'left', \n                                                axis = 1)","8a67b55c":"# convert numpy dummy tables to pandas DataFrame\ntrain_num_cleaned = pd.DataFrame(train_num_cleaned)\ntest_num_cleaned = pd.DataFrame(test_num_cleaned)","aff10365":"# joining numeric (after imputing) and string (converted to dummy) data\ntrain_all_clean = pd.concat([train_num_cleaned, train_dummy], axis = 1)\ntest_all_clean = pd.concat([test_num_cleaned, test_dummy], axis = 1)","8b3e2a03":"# detect NaN in already cleaned test data \n# (there could be completely empty columns in test data)\ncols_with_missing = [col for col in test_all_clean.columns\n                                if test_all_clean[col].isnull().any()]\nfor col in cols_with_missing:\n    print(col, test_all_clean[col].isnull().any())","a5b2ea1c":"# since there are empty columns in test we need to drop them in train and test data\ntrain_all_clean_no_nan = train_all_clean.drop(cols_with_missing, axis = 1)\ntest_all_clean_no_nan = test_all_clean.drop(cols_with_missing, axis = 1)","874b5151":"train_X, val_X, train_y, val_y = train_test_split(train_all_clean_no_nan, y, random_state=1)\n\n# default XGBoost\nxgb_model = XGBRegressor(random_state = 1)\nxgb_model.fit(train_X, train_y, verbose = False)\nxgb_predictions = xgb_model.predict(val_X)\nxgb_mae = mean_absolute_error(val_y, xgb_predictions)\nprint(\"XGBoost MAE default: {:,.0f}\".format(xgb_mae))\n\n# fine tuned XGBoost\nxgb_model = XGBRegressor(n_estimators = 1000, learning_rate=0.05, random_state = 1)\nxgb_model.fit(train_X, train_y, early_stopping_rounds = 25, eval_set = [(val_X, val_y)], verbose = False)\n# with verbose = True we have the best iteration on step 757 => n_estimators = 757","23c6dc66":"xgb_predictions = xgb_model.predict(val_X)\nxgb_mae = mean_absolute_error(val_y, xgb_predictions)\nprint(\"XGBoost MAE default: {:,.0f}\".format(xgb_mae))","fdbd8eb1":"# To improve accuracy, create a new Random Forest model which you will train on all training data\nxgb_model_on_full_data = xgb_model = XGBRegressor(n_estimators = 757, learning_rate=0.05)\nxgb_model_on_full_data.fit(train_all_clean_no_nan, y)","a5a43792":"from xgboost import plot_importance\n\nplot_importance(xgb_model_on_full_data, max_num_features = 20)","4e3bc345":"test_X = test_all_clean_no_nan\ntest_preds = xgb_model_on_full_data.predict(test_X)","af65edbb":"output = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)","4f9f5446":"# XGBoost Model (on all training data)","adad7061":"# Missing and categorical values","085d8f27":"# Make Predictions","ac35170c":"# XGBoost training and validation","fc8a980c":"# Partial dependencies plots"}}