{"cell_type":{"1e01c321":"code","99bf0e25":"code","fbdf5727":"code","9251ef7b":"code","6bb3978a":"code","dd0b2874":"code","14dd81c2":"code","3c87948a":"code","bc3e1c41":"code","48b6a9dd":"code","241af23d":"code","7b5bf630":"code","052a9a5e":"code","d986d9a0":"code","db5b640d":"code","1975b537":"code","a9a3b728":"code","7fc5c162":"code","3456c0db":"code","4e62a32a":"code","d67313a8":"code","7f6c9c72":"code","cf9adeae":"code","75b56eb9":"code","a6052951":"code","887d301e":"code","d07170a1":"code","d5fbdfa4":"code","06b7885f":"code","551c475e":"code","b65d25ff":"code","7ad99935":"code","d22a2fd1":"code","4ba6c1f7":"code","309d07b6":"code","d357ce46":"markdown","15a8b0a6":"markdown","fc4e3940":"markdown","2a403e29":"markdown","17e8586b":"markdown","60203408":"markdown","ca952b80":"markdown","b0948b60":"markdown","5985bf60":"markdown","89e61a87":"markdown","5a43d8b1":"markdown","414e53d5":"markdown","ac4cf422":"markdown","8b18e8c4":"markdown","5e6f65a6":"markdown","9ab12718":"markdown","c9daa183":"markdown","ba540eca":"markdown","f651c825":"markdown","8e6baabf":"markdown","0b764f99":"markdown","73cf58dc":"markdown","3e57384a":"markdown","e7259870":"markdown","7a6c7578":"markdown","c5d0b6f6":"markdown","8aad5246":"markdown","9ba25c36":"markdown","a335f818":"markdown","dfcf8608":"markdown","5a95af33":"markdown"},"source":{"1e01c321":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","99bf0e25":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","fbdf5727":"df=pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\").copy()","9251ef7b":"df.head()","6bb3978a":"df.info()","dd0b2874":"print(df[\"DEATH_EVENT\"][df[\"DEATH_EVENT\"]==0].count())\nprint(df[\"DEATH_EVENT\"][df[\"DEATH_EVENT\"]==1].count())","14dd81c2":"sns.countplot(\"DEATH_EVENT\", data=df, palette=\"Set3\")","3c87948a":"cat_columns=[\"anaemia\",\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\"]","bc3e1c41":"import scipy.stats\nfor i in cat_columns:  \n    print(\"Chi-Square Test Between\", \"\\033[4m\", \"DEATH_EVENT\",\"\\033[0m\",\"and\",\"\\033[4m\",i,'\\033[0m',\"\\n\")\n    cross=pd.crosstab(index=df[\"DEATH_EVENT\"], columns=df[i])\n    chi2,p,dof,expected= scipy.stats.chi2_contingency(cross)\n    print(\"Chi=\", chi2, \"\\033[1m\", \"p_value=\", p,\"\\033[0m\", \"\\n\")\n ","48b6a9dd":"cross=pd.crosstab(index=df[\"DEATH_EVENT\"], columns=df[\"diabetes\"])\nprint(cross)","241af23d":"sns.countplot(\"diabetes\", hue=\"DEATH_EVENT\",data=df,palette=\"Set3\")\nplt.show()","7b5bf630":"num_data=df.drop(cat_columns,axis=1)\ntarget=num_data[\"DEATH_EVENT\"]\nnum_corr=num_data.drop(\"DEATH_EVENT\", axis=1)","052a9a5e":"corr=num_corr.corr()\nfig,ax = plt.subplots(figsize=(7, 7))\nsns.heatmap(corr, ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"GnBu\")\nplt.show()","d986d9a0":"def two_sample_t_test(cl1,cl2, data):\n        import scipy.stats as stats\n        from pandas import Series\n        A=Series(data[cl1].iloc[:][data[cl2]==1])\n        B=Series(data[cl1].iloc[:][data[cl2]==0])\n        print(\"\\033[1m\",\"'{}' - '{}'\". format(cl1,cl2),\"\\033[0m\")\n                       \n        stat, p=stats.levene(A,B)\n       \n        if p<0.05:\n            var=False\n        else:\n            var=True\n        print(\"RESULT:\")\n        stat, p=stats.ttest_ind(A,B, equal_var=var)\n        print(\"\\033[1m\",\"p-val=\",p,\"\\033[0m\")\n        if p<0.05:\n            print(\"\\033[1m\",\"H0--> Reject. They have different mean\\n\",\"\\033[0m\")\n        else:\n            print(\"\\033[1m\",\"H0--> Fail to Reject.\\n\",\"\\033[0m\")","db5b640d":"for col in num_data.columns:\n    two_sample_t_test(col,\"DEATH_EVENT\",num_data)","1975b537":"plt.figure(figsize=(5,5))\nsns.boxplot(x=\"DEATH_EVENT\",y=\"platelets\", data=df, palette=[\"lightblue\", \"pink\"])","a9a3b728":"sns.boxplot(x=\"DEATH_EVENT\",y=\"age\", data=df, palette=[\"lightblue\", \"pink\"])","7fc5c162":"num_selected=num_data[[\"age\",\"ejection_fraction\",\"serum_creatinine\",\"serum_sodium\",\"time\"]].copy() \n# new numeric data without \"creatinine_phosphokinase\" and \"platelets\".","3456c0db":"for i in num_selected.columns:\n    sns.boxplot(num_selected[i],palette=[\"lightblue\"])   \n    plt.show()","4e62a32a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(num_selected,target,test_size=0.30, random_state=42)","d67313a8":"X_train.head()","7f6c9c72":"X_train_scaled=X_train.copy()\nX_test_scaled=X_test.copy()","cf9adeae":"from sklearn.preprocessing import StandardScaler\nfor i in X_train_scaled.columns:\n    scaler=StandardScaler().fit(X_train[[i]])\n    X_train_scaled[i]=scaler.transform(X_train[[i]])\n    X_test_scaled[i]=scaler.transform(X_test[[i]])","75b56eb9":"X_train_scaled.head()","a6052951":"def result(model,X_train,X_test,y_train,y_test):\n    print( \"\\033[1m\",\"         ****** RESULT ****** \", \"\\033[0;0m\")\n    y_pred = model.predict(X_test)\n    print(\"\\033[1m\",\"TEST Accuracy=\",\"\\033[0m\",accuracy_score(y_test, y_pred))\n    print(\"\\033[1m\",\"TEST Report=\\n\",\"\\033[0m\",classification_report(y_test,model.predict(X_test)),\"\\n\")\n    y_pred = model.predict(X_train)\n    print(\"\\033[1m\",\"TRAIN Accuracy=\",\"\\033[0m\",accuracy_score(y_train, y_pred))\n    print(\"\\033[1m\",\"TRAIN Report=\\n\",\"\\033[0m\",classification_report(y_train,model.predict(X_train)),\"\\n\")\n    \n    print(\"\\033[1m\",\"Cross Validation TEST:\\n\",\"\\033[0m\",cross_val_score(model, X_test, y_test, cv = 5).mean())    \n    print(\"\\033[1m\",\"Cross Validation TRAIN:\\n\",\"\\033[0m\",cross_val_score(model, X_train, y_train, cv = 5).mean(),\"\\n\")\n    print(\"\\033[1m\",\"ROC CURVES\",\"\\033[0m\")\n    nb_roc_auc=roc_auc_score(y_test,model.predict(X_test))\n    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n    plt.figure()\n    plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % nb_roc_auc)\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive ')\n    plt.ylabel('True Positive ')\n    plt.title('ROC-TEST')\n    plt.show()\n    nb_roc_auc=roc_auc_score(y_train,model.predict(X_train))\n    fpr, tpr, thresholds = roc_curve(y_train, model.predict_proba(X_train)[:,1])\n    plt.figure()\n    plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % nb_roc_auc)\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive ')\n    plt.ylabel('True Positive ')\n    plt.title('ROC-TRAIN')\n    plt.show() \n   ","887d301e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score,roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score","d07170a1":"logreg=LogisticRegression()\nmodel=logreg.fit(X_train_scaled,y_train)","d5fbdfa4":"result(logreg,X_train_scaled,X_test_scaled,y_train,y_test)","06b7885f":"from sklearn.svm import LinearSVC\nfrom sklearn.calibration import CalibratedClassifierCV\nsvm = LinearSVC()\nclf = CalibratedClassifierCV(svm) \nclf.fit(X_train_scaled, y_train)","551c475e":"result(clf,X_train_scaled,X_test_scaled,y_train,y_test)","b65d25ff":"from sklearn.naive_bayes import GaussianNB","7ad99935":"nb=GaussianNB().fit(X_train_scaled,y_train)","d22a2fd1":"result(nb,X_train_scaled,X_test_scaled,y_train,y_test)","4ba6c1f7":"from sklearn.svm import SVC\nsvm_model = SVC(kernel = \"linear\",probability=True).fit(X_train_scaled, y_train)\nsvm_model","309d07b6":"result(svm_model,X_train_scaled,X_test_scaled,y_train,y_test)","d357ce46":"### Target Variable","15a8b0a6":"We can see in the dataframe,the columns \"anaemia\", \"diabetes\", \"high_blood_pressure\", \"sex\", \"smoking\" are categorical variables. Now, we will apply chi-square test for figuring out the association between these variables and target variable. In the result, if p value>0.05 (for alpha = 5%) it means we don't have evidence to believe that there is association between target and that categorical variable. Namely, they are independent. We can ignore that variable because if they dont have any effect on target, keeping them is not useful.","fc4e3940":"## MODELS","2a403e29":"If we look the information of the columns, we see that there is no null variables.(There is 299 sample and all of the columns have 299 non-null).","17e8586b":"First of all, I will write a method that allows us to access all result information of the models. It will give us train-test scores, cross-validation results and ROC Curves.","60203408":"In the result of t-test, the variables \"creatinine_phosphokinase\", \"platelets\" have p-values greater than 0.05. As you can see in the box plots, for both of the variables, the mean are the same wheter it is dead or not. These variables independent from target. So, we will not use them for model.","ca952b80":"#### LOGISTIC REGRESSION","b0948b60":"The target variable \"DEATH_EVENT\" has 0 observations more than 1.","5985bf60":"Look at the data, the unit of values different. So, we will scale them.","89e61a87":"## DATA ANALYSIS","5a43d8b1":"Because of the all categorical data has no effect on the target, we will continue with the numeric data.","414e53d5":"### Categorical Variables","ac4cf422":"#### SUPPORT VECTOR MACHINES","8b18e8c4":"For logistic regression, linear SVC and SVC, the grid search was applied but nothing change. So, this process was not shown. When we look at the train-test accuracy score results, we can see the difference like 0.08, 0.10.. etc. These difference show the the models not enough for the generalization. It can be happened beacuse of our data set size. Logistic regression, linear SVC, SVC perform well when the data set is large; and Naive Bayes performs well when high-dimensional data set. (After feature elimination we had 5 feature and sample size 299 is not large data set.)","5e6f65a6":"### Numeric Varibales","9ab12718":"Now we will look at the outliers. \"serum_creatinine\" and \"serum_sodium\" seems like they have a lot of outliers but if we search about these variables, outliers of them are not abnormal values. For example, serum creatinine value is normal value up to 10 mg\/dL . This also applies to the serum_sodium variable . So, we will not delete them.","c9daa183":" First, we will find the correlation in between numeric variables. If there is a any high correlaton (negative or positive) in between two variable, we will use one of them. \n As you can see in the correlation table there is no high correlation in  between variables. We can use all of them.","ba540eca":"LinearSVC has no function of predict_proba, so we will use CalibratedClassifierCV for getting probabilities.","f651c825":"#### Scaling ","8e6baabf":"After scaling, their units become more regular.","0b764f99":"#### t-test","73cf58dc":"Now, we will do t-test between numeric variables and target. Result of this will show us the dependence between of them.\nIn the two sample t-test, null hypotesis, H0, says that \"the means of two samples are equal\". So, if p-value<0.05, H0 is rejected. It means they have different mean, so there is a meaningful difference between of them. They are dependent. We will eliminate the variables that have p-value>0.05, because they are not dependent on target.","3e57384a":"#### NAIVE BAYES","e7259870":"#### Correlation Matrix","7a6c7578":"#### Linear SVC","c5d0b6f6":"#### Train- Test Split","8aad5246":"All categoric variables has p-value that is greater than 0.05, so we will not use any of them.\nWe can explain it like this; for example, we take the \"diabetes\" variable, we find the cross table of it below. \nLook at the numbers, if a sample is not diabete(0), the probability of deadth is 56\/174=0.3218.\nOtherhand, if the sample is diabete(1), the probability of death is 40\/125= 0.32.\nSo, whether the sample is diabete or not, the probability of dead is equal. So diabete is not effect on the death.","9ba25c36":"In this work we will predict heart failure. Here is the dataframe that we will work on. If the patient deceased during the follow-up period, it is 1. Otherwise, it is 0.","a335f818":"#### TUNING","dfcf8608":"For example, if we look at the age-dead_event association, we can see that, \"DEATH_EVENT\" values that are 1, has greater age mean. ","5a95af33":"#### Outliers"}}