{"cell_type":{"8e6076de":"code","3bc81ef5":"code","08cbe738":"code","a074f166":"code","4c9c40fa":"code","e003038d":"code","e55650ae":"code","d2a6e17a":"code","2e628e68":"code","8154028d":"code","8e2732eb":"code","9ad372e6":"code","d592223f":"code","597d6188":"code","df6a4fac":"code","c3ad5c5c":"code","9f305f5f":"code","a462ba63":"code","7fa7988e":"code","ee1cb828":"code","da8d8eb0":"code","28ff624d":"code","9d2cda0b":"code","85d1e4fa":"code","c5017698":"code","f179d2ba":"code","5badce13":"code","e1d70d7b":"code","97fa8b91":"code","5568c94f":"code","f43ef7d1":"code","fa952014":"code","5bcd845d":"code","c3168b47":"code","acedfef1":"code","e5444370":"code","4225a857":"code","a398ee93":"code","010facd5":"code","2572d33b":"code","7b32e536":"markdown","9ca0232e":"markdown","dc7d0fc4":"markdown","d9a77305":"markdown","5a46ea5e":"markdown","0e427bfa":"markdown","f76bbff3":"markdown","8627a0b4":"markdown","51bc3ead":"markdown","eee726a6":"markdown","11517d1d":"markdown","6911fa41":"markdown","a6e78344":"markdown","c46cce70":"markdown","2eaf3041":"markdown","cd2d5ba6":"markdown","da3cd0c4":"markdown","94c4151a":"markdown","62c7605a":"markdown","9ec5a67a":"markdown","a86e9c1f":"markdown","6a80cb86":"markdown","ce395455":"markdown","39065841":"markdown","27b516a4":"markdown"},"source":{"8e6076de":"# Importing the packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","3bc81ef5":"import seaborn as sns","08cbe738":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","a074f166":"# Loading the dataset (Here .tsv means tabs separated value)\n\ndf_reviews=pd.read_csv(\"\/kaggle\/input\/amazon-alexa-reviews\/amazon_alexa.tsv\",sep='\\t')\ndf_reviews.head()","4c9c40fa":"df_reviews.shape             # Checking the number of rows and columns.","e003038d":"df_reviews.info()","e55650ae":"df_reviews.describe()                  # There are 2 numeric columns 'rating' and 'feedback'","d2a6e17a":"df_reviews['date']=pd.to_datetime(df_reviews['date'])","2e628e68":"df_reviews['date'].dt.year.value_counts()","8154028d":"df_reviews['date'].min()","8e2732eb":"df_reviews['date'].max()","9ad372e6":"# Lets have a look at the time series plot.\nplt.figure(figsize=(15,6))\nsns.countplot(x='date',data=df_reviews)\nplt.xticks(rotation=90)\nplt.show();","d592223f":"# Lets look at the purchases on each of the three months\n\nsns.countplot(df_reviews['date'].dt.month)","597d6188":"# This is the count of products sold or the reviews recieved in each month.\n\ndf_reviews['date'].dt.month.value_counts()","df6a4fac":"sns.countplot(x='rating',data=df_reviews)","c3ad5c5c":"df_reviews.rating.value_counts()","9f305f5f":"sns.countplot(x='feedback',data=df_reviews)","a462ba63":"# Now lets have a look at the word count of the reviews.\n\ndf_reviews['length']=df_reviews['verified_reviews'].apply(lambda x:len(x.split(\" \")))\ndf_reviews.head()","7fa7988e":"# Lets have a look at the histogram of the length of reviews.\nplt.hist(x='length',data=df_reviews,bins=30);","ee1cb828":"df_reviews.length.describe()","da8d8eb0":"# Is there a relation between review word count and the rating\nsns.lmplot(x='length',y='rating',data=df_reviews)","28ff624d":"neg=df_reviews[df_reviews['feedback']== 0]","9d2cda0b":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ntext = neg['verified_reviews'].values\nwordcloud = WordCloud(\nwidth=3000,\nheight=2000,\nbackground_color = 'black',\nstopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\nfigsize=(40,30),\nfacecolor = 'k',\nedgecolor = 'k')\nplt.imshow(wordcloud,interpolation='bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","85d1e4fa":"sns.countplot(x='rating',data=neg)","c5017698":"N = df_reviews.shape[0]\nN","f179d2ba":"corpus = []\n\nimport re\nimport nltk                                                 # For text preprocessing we use nltk\nfrom nltk.corpus import stopwords                           # Removing all the stopwords\nfrom nltk.stem.porter import PorterStemmer                  # Reducing words to base form","5badce13":"nltk.set_proxy('SYSTEM PROXY')\n\n##nltk.download()\nnltk.download('stopwords')","e1d70d7b":"ps=PorterStemmer()\n\nfor i in range (0,N):\n    review = re.sub('[^a-zA-Z ]',' ',df_reviews['verified_reviews'][i]) # Removing special symbols like...,! and kkeeping only \n    review = review.lower()                                             # Lower case\n    review = review.split()                                             # String split into words\n    review = [ps.stem(word) for word in review                         # Reducing words to base form\n    if not word in set(stopwords.words('english'))]\n    review = \" \".join(review)\n    corpus.append(review)","97fa8b91":"corpus","5568c94f":"#TF-IDF Vectorizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(use_idf=True, strip_accents='ascii')","f43ef7d1":"y = df_reviews['feedback']","fa952014":"X = vectorizer.fit_transform(corpus)","5bcd845d":"X.shape","c3168b47":"y.shape","acedfef1":"# Split the test and train\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42, test_size=0.2)","e5444370":"# Importing the decission tree classifier\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt_clf = DecisionTreeClassifier(class_weight='balanced')","4225a857":"dt_clf.fit(X_train, y_train)               # Model fitting on X_train,y_train\ny_pred = dt_clf.predict(X_test)            # Model prediction on X_test","a398ee93":"# Split the test and train\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n# Confusion Matrix\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)","010facd5":"# Classification Report\ncr = classification_report(y_test ,y_pred)\nprint(cr)","2572d33b":"# Accuracy Score\nacc=accuracy_score(y_test,y_pred)\nprint(acc)","7b32e536":"\n# Observation:\nThe median number of words are 14 and the mean of the words are 25.","9ca0232e":"\n**Stowords: Words which are filtered before applying NLP as they have no meaning.\nE.g . a,again,am,are,but,etc....\nPorter Stemmer is used to remove any morphological affixes from words,leaving ony the word stem.\nE.g. running--> run\nhaving --> have","dc7d0fc4":"# Lets have a look at the negative feedbacks.","d9a77305":"# Observation:\nAll the data is from the year 2018","5a46ea5e":"# Amazon Alexa Reviews","0e427bfa":"# Observation:\nThere are a lot of good reviews .Lets have a look at their numbers.","f76bbff3":"You can find other words which would be a reason ,why the feedback was bad?","8627a0b4":"\n# Observation:\nThere are a lot of people who have given short reviews.Very few reviews are long.","51bc3ead":"# Observation:\nThe data is provided for days between 16th May and 31st July.","eee726a6":"\n# Observation:\n There are no missing values in dataframe.\n'Date' column should be in datetime type.","11517d1d":"# Train Test Split","6911fa41":"# Converting the date column","a6e78344":"# Observation:\nAlot of people have given positive feedback.","c46cce70":"# Building the classifier","2eaf3041":"# separating Labels and Features","cd2d5ba6":"# Sentiment Analysis\n","da3cd0c4":"\nAll the negative feedback are 1 or 2 ratings.","94c4151a":"# Techniques  will explore here as:\nLoading Data, Data cleaning and Data visualization using Python\nEnd to End EDA using Statistics\nPredictive Modelling of Customer Sentiment Reviews using Decision trees","62c7605a":"**Apart from \"Amazon ,device,Alexa,work\",we find negative words such as problem,disappointed,speaker,sound.These are not words which make the feedback negative.**","9ec5a67a":"\n# Observation :\nThere are rating values from 1 to 5 and feedback from 0 to 1","a86e9c1f":"\n# Applying TF-IDF\n\nTf-idf is an NLP technique to weight words how important they are. How do we calculate the importance of words?\nWords that are used frequently in many documents will have a lower weighting while infrequent ones will have a higher weighting.\nThe Tf-idf value increases proportionally to the number of times a word appears in the document,but is offset by the frequency of helps to adjust for the fact that some words appear more frequently in general.\nWe calculate the item frequency and inverse document frequency by the following formula.\nTerm Frequency: TF(t) = (Number of times term t appears in a document)\/(Total number of terms in the document)\nInverse Document Frequency: IDF(t) =log_e(Total number of documents\/Number of documents with term t in it).\ntf-idf score=TF(t)*IDF(t)","6a80cb86":"# Visualizations","ce395455":"# Decission Tree","39065841":"\nPeople who have left poor reviews have written less number of words,but the people who have written good reviews have written a lot of words.","27b516a4":"\nThere are a lot of reviews obtained for the month of July. This can also mean that there was a lot of devices purchases on the month of July."}}