{"cell_type":{"00b17319":"code","56ea461c":"code","fa9da84c":"code","15c9eaa0":"code","bfbd4f47":"code","d06a5f64":"code","b286f45b":"code","e5e05f7c":"code","312f5f6a":"code","a23ce900":"code","c52458d0":"code","c2d81c6b":"code","e8c18043":"code","4cfc0f2d":"code","d04b7b0d":"code","fdfe2b90":"code","4093f5ab":"code","0410affe":"code","e911cbc9":"code","503185ea":"code","56da5718":"code","c389b864":"code","2b626859":"code","9730abd5":"code","3027fddf":"code","af551d3a":"code","5fb115cd":"code","e34c9a2b":"code","c2c3f449":"code","231b1bb0":"code","828cd5cb":"code","a1c73a16":"code","47fa4a7e":"code","5935f006":"code","8289d9f7":"code","2928de0d":"code","c61fc05f":"code","5c7dc55f":"code","c5693401":"code","f62d2345":"code","afa1ec99":"code","79f4a2cb":"code","24ad37f5":"code","fce58878":"code","a5d9e70b":"code","e2ad4398":"code","b1e40684":"code","18ce41f6":"code","b0abb55b":"code","829ff5d9":"code","246e5a6f":"code","18449bb8":"code","3c75729b":"code","5f4fbf89":"code","6d3e61df":"code","d4751eb5":"code","8a4f733c":"code","00bcf00d":"code","4abf0706":"code","070de9d1":"code","b39fe979":"code","ec820b4e":"code","a28b672b":"code","839a7ca4":"code","378fceac":"code","f21ebbaa":"code","9bf0e654":"code","4a27bffb":"code","357cec89":"code","efc455f5":"code","72f5f725":"code","24164191":"code","fa14f047":"code","ec12eb1c":"code","3495139d":"code","e91e9d94":"code","acdb9711":"code","a85a493e":"code","2a008bf1":"code","03c5f316":"code","3a4fc994":"code","6f40adce":"code","67a7f917":"code","caf33e5c":"code","785b69b7":"code","26e17660":"code","6104d3c3":"markdown","93174a6d":"markdown","3c195cca":"markdown","de7a836e":"markdown","36203412":"markdown","d46f78c5":"markdown","38d498e0":"markdown","cbb178c7":"markdown","8d64f5a7":"markdown","64a1980a":"markdown","44233775":"markdown","db598811":"markdown","bb59ef4f":"markdown","42303c33":"markdown","ef6ec6a3":"markdown","3394f82d":"markdown","fb5735af":"markdown","6033832f":"markdown","8bed01d1":"markdown","8e95c310":"markdown","a1d18c86":"markdown","332fb656":"markdown","3dd901eb":"markdown","3ce6a956":"markdown","357e5467":"markdown","a998529c":"markdown","5eb74438":"markdown","feb3e280":"markdown","af9a6753":"markdown","5eece0d0":"markdown","ce36d242":"markdown","d35f35ea":"markdown"},"source":{"00b17319":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output","56ea461c":"# HTML\nfrom IPython.display import HTML\nfrom IPython.display import display\n\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly\nimport missingno as msno\n\n#plotly.offline.init_notebook_mode (connected = True)\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import ShuffleSplit, RepeatedKFold, train_test_split, GridSearchCV,  cross_val_score\n\n# warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")","fa9da84c":"# Load train and Test set\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n","15c9eaa0":"import plotly.graph_objects as go\n\nheaderColor = 'royalblue'\nrowEvenColor = 'paleturquoise'\nrowOddColor = 'paleturquoise'\n\nfig = go.Figure(data=[go.Table(\n  header=dict(\n    values=['<b>Variable<\/b>','<b>Definition<\/b>','<b>Key<\/b>','<b>Independent\/Dependent Variable<\/b>','<b>Data Type <\/b>'],\n    line_color='darkslategray',\n    fill_color=headerColor,\n    align=['center','center'],\n    font=dict(color='white', size=12)\n  ),\n  cells=dict(\n    values=[\n      [\"PassengerId\",\"Name\",\"Pclass\",\"Sex\",\"Age\",\"Sibsp\",\"Parch\",\"Ticket\",\"Fare\",\"Cabin\",\"Embarked\",\"Survival\"],\n      [\"PassengerId\",\"PassengerName\",\"Ticket class\",\"Sex\",\"Age in years\",\"# of siblings \/ spouses aboard the Titanic\",\"# of parents \/ children aboard the Titanic\",\"Ticket number\",\"Passenger fare\",\"Cabin number\",\"Port of Embarkation\",\"Survival\"],\n      [\"\", \"1 = 1st = Upper, 2 = 2nd = Middle, 3 = 3rd = Lower\",\"\",\"\",\" (Sibling =(brother, sister, stepbrother, stepsister))\",\"(pouse = husband, wife (mistresses and fianc\u00e9s were ignored))\",\"(Parent = mother, father),(Child = daughter, son, stepdaughter, stepson),(Some children travelled only with a nanny, therefore parch=0 for them.)\",\"\",\"\",\"C = Cherbourg, Q = Queenstown, S = Southampton\",\"\",\"0 = No, 1 = Yes\"],\n      [\"Independent\",\"Independent\",\"Independent\",\"Independent\",\"Independent\",\"Independent\",\"Independent\",\"Independent\",\"Input\/Independent\",\"Independent\",\"Independent\",\"Dependent\"],\n      [\"Numerical\",\"Text\",\"Ordinal\",\"Categorical\",\"Numerical(Continous)\",\"Numerical(Discrete)\",\"Numerical(Discrete)\",\"Mixed(numeric and alphanumeric)\",\"Numerical(Continous)\",\"Mixed(numeric and alphanumeric)\",\"Categorical\",\"Categorical\"]],\n    line_color='darkslategray',\n    # 2-D list of colors for alternating rows\n    fill_color = [['white','white','white', 'white','white']*5],\n    align = ['center', 'center'],\n    font = dict(color = 'darkslategray', size = 15)\n    ))\n])\n\nfig.show()\n","bfbd4f47":"# color settings:\nsns.set(rc={'axes.facecolor':\"#E1E8ED\",  #00ffff\n            \"figure.facecolor\":\"#E1E8ED\", # background color\n            \"grid.color\":\"#ff6347\",\n            \"axes.edgecolor\":\"#ff6347\",\n            \"axes.labelcolor\":\"#ff6347\",\n            \"text.color\":\"#424949\" # color for headlines and sub headlines\n           }) \n\n# font size settings\nsns.set_context(rc={\"axes.labelsize\" : 15})\n\n# change font tipe to Times New Roman: (newspaper look)\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']","d06a5f64":"# Outlier detection \nfrom collections import Counter\n#Once initialized, counters are accessed just like dictionaries.\n#Also, it does not raise the KeyValue error (if key is not present) instead the value\u2019s count is shown as 0.\ndef detect_outliers(df,n,features):\n    outlier_indices = []\n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col],25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        # outlier step\n        outlier_step = 1.5 * IQR\n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index       \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    return multiple_outliers   \n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ntrain.loc[Outliers_to_drop] # Show the outliers rows","b286f45b":"# Drop outliers\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\n\n# after removing outlier, let's concat the data sets\ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)\n","e5e05f7c":"# Fill Empty and NaNs values with NaN\ndataset = dataset.fillna(np.nan)\n\n# Create table for missing data analysis\ndef find_missing_data(data):\n    Total_Dataset = dataset.isnull().sum().sort_values(ascending = False)\n    Percentage_Dataset = (dataset.isnull().sum()\/dataset.isnull().count()).sort_values(ascending = False)\n    Total_Train = train.isnull().sum().sort_values(ascending = False)\n    Percentage_Train = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending = False)\n    Total_Test = test.isnull().sum().sort_values(ascending = False)\n    Percentage_Test = (test.isnull().sum()\/test.isnull().count()).sort_values(ascending = False)\n    return pd.concat([Total_Dataset,Percentage_Dataset,Total_Train,Percentage_Train,Total_Test,Percentage_Test] , axis = 1 , keys = ['Total_Dataset' , 'Percentage_Dataset','Total_Train','Percentage_Train'\n                                                                                                         ,'Total_Test','Percentage_Test'])\n# call find_missing_data\nfind_missing_data(dataset)","312f5f6a":"# Infos train\ntrain.info()\nprint(f\"Overall probability to survive:\",train.isnull().sum())","a23ce900":"# Summarie and statistics\ntrain.describe().T","c52458d0":"# creating mask\nmask = np.triu(np.ones_like(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr()))\n\n# build figure\nf, ax = plt.subplots(figsize=(20, 10))\n# change x- and y-label size\nax.tick_params(axis='both', which='major', labelsize=15)\n\n# plotting a triangle correlation heatmap\ndataplot = sns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(), cmap=\"YlGnBu\", annot=True, mask=mask,center=0,square=True,\n                      linewidths=1, linecolor=\"#424949\",cbar_kws={\"shrink\": 0.6}).set_title('Pairwise correlation', fontsize=\"30\")","c2d81c6b":"train.dtypes","e8c18043":"screening_df = train.copy()\n\nfg = sns.displot(\n    screening_df, kde=True,  x=\"Age\", col=\"Pclass\", row=\"Sex\",multiple=\"stack\",fill=True,\n    binwidth=3, height=4, facet_kws=dict(margin_titles=True), aspect=1.63, linewidth = 1)\nfg.set_xticklabels(fontsize=15)\n# change range for x axis\nplt.xlim(0, 85)\n# add headline\nfg.fig.subplots_adjust(top=0.85)\nfg.fig.suptitle('Age distribution per Pclass and Sex', fontsize=\"28\");","4cfc0f2d":"screening_df = train.copy()\n# build figure\nfig = plt.figure(figsize=(25,5))\n# add grid to figure\ngs = fig.add_gridspec(2,3)\n\n# fill grid with subplots\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\nax3 = fig.add_subplot(gs[1,0])\nax4 = fig.add_subplot(gs[1,1])\nax5 = fig.add_subplot(gs[1,2])\n\n\n# adjust subheadline fontsize\nax0.set_title('Pclass = 1', fontsize=20)\nax1.set_title('Pclass = 2', fontsize=20)\nax2.set_title('Pclass = 3', fontsize=20)\nax3.set_title('Pclass = 1', fontsize=20)\nax4.set_title('Pclass = 2', fontsize=20)\nax5.set_title('Pclass = 3', fontsize=20)\n\n# adjust lable fontsize\nax0.tick_params(labelsize=15)\nax1.tick_params(labelsize=15)\nax2.tick_params(labelsize=15)\nax3.tick_params(labelsize=15)\nax4.tick_params(labelsize=15)\nax5.tick_params(labelsize=15)\n\n\n# plot data into subplots \nsns.kdeplot(data=screening_df[screening_df['Pclass']==1], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax0, linewidth = 3, ec=\"#424949\").set(xlabel=\"Fare\", ylabel=\"\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==2], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax1, linewidth = 3, ec=\"#424949\").set(xlabel=\"Fare\", ylabel=\"\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==3], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax2, linewidth = 3, ec=\"#424949\").set(xlabel=\"Fare\", ylabel=\"\")\n\n#\nsns.boxplot(x=\"Fare\",data=screening_df[screening_df['Pclass']==1], orient=\"h\", color=\"#97A7B2\", ax=ax3, linewidth = 3).set(xlabel=\"Fare\")\nsns.boxplot(x=\"Fare\",data=screening_df[screening_df['Pclass']==2], orient=\"h\", color=\"#97A7B2\", ax=ax4, linewidth = 3).set(xlabel=\"Fare\")\nsns.boxplot(x=\"Fare\",data=screening_df[screening_df['Pclass']==3], orient=\"h\", color=\"#97A7B2\", ax=ax5, linewidth = 3).set(xlabel=\"Fare\")\n\n\n# add headline\nfig.subplots_adjust(top=1.5,hspace=0.7,bottom=0.1, left=0.125,right=0.9, wspace=0.4)\nfig.suptitle('Fare-Distribution for each class', fontsize=\"28\",y=1.70);","d04b7b0d":"fig = plt.figure(figsize=(25, 4))\nax = sns.boxplot(x=\"Age\", y=\"Pclass\",hue=\"Survived\",data=screening_df, orient=\"h\", palette={0: \"#E6B0AA\", 1:\"#A9DFBF\"}, linewidth = 3)\nax.tick_params(labelsize=15)\n# add headline\nfig.subplots_adjust(top=0.8)\nfig.suptitle(\"Age\/Pclass and Survived\", fontsize=\"28\");","fdfe2b90":"# calculate the survival probability per class and sex\nsex_class_prob_dict = {}\nfor s in ['male', 'female']:\n    for p in [1,2,3]:\n        df = screening_df[(screening_df['Pclass'] == p) & (screening_df['Sex'] == s)].copy()\n        sex_class_prob_dict[f\"{s} {str(p)}. class\"] = round(len(df[df['Survived'] == 1]) \/ len(df['Survived']),2)\n\n# let's write a function for the probability visualization. we will need it later on.\n\ndef probability_visualization(prob_dict, title):\n    df = pd.DataFrame.from_dict(prob_dict, orient='index').rename(columns={0: \"survival_probability\"})\n    df['label'] = df.index\n    fg = sns.catplot(data=df, kind=\"bar\", y=\"label\", x=\"survival_probability\", height=5, color=\"#97A7B2\",  aspect=3.9, linewidth = 3, ec=\"#424949\")\n    fg.set_xticklabels(fontsize=15)\n    fg.set_yticklabels(fontsize=15)\n    fg.fig.subplots_adjust(top=0.8)\n    fg.fig.suptitle(title, fontsize=\"28\");\n    \n\n","4093f5ab":"probability_visualization(prob_dict=sex_class_prob_dict, title='Probability of survival for Sex and Pclass')","0410affe":"#train.isna().sum()\n\nplt.figure(figsize=(25,15))\n\nsns.displot(\n    data=train.isna().melt(value_name=\"missing\"),\n    y=\"variable\",\n    hue=\"missing\",\n    multiple=\"fill\",\n    aspect=1.25,\n)\nplt.savefig(\"visualizing_missing_data_with_barplot_Seaborn_distplot.png\", dpi=100)","e911cbc9":"cols = train.columns \ncolours = ['g', 'r']\nf, ax = plt.subplots(figsize = (12,8))\nsns.set_style(\"whitegrid\")\nplt.title('Missing Values Heatmap', )\nsns.heatmap(train[cols].isnull(), cmap=sns.color_palette(colours));","503185ea":"fig = plt.figure(figsize=(15,7))\n\nax1 = fig.add_subplot(1,2,1)\nmsno.bar(train, color=\"tomato\", fontsize=12, sort=\"ascending\", ax=ax1);\n\nax2 = fig.add_subplot(1,2,2)\nmsno.bar(train, log=True, color=\"tab:green\", fontsize=12,sort=\"ascending\", ax=ax2);\n\n\nplt.tight_layout()","56da5718":"def summary(df):\n    \n    types = df.dtypes\n    counts = df.apply(lambda x: x.count())\n    uniques = {e:df[e].unique() for e in df.columns}\n    nas = df.apply(lambda x: x.isnull().sum())\n    distincts = df.apply(lambda x: x.unique().shape[0])\n    missing = (df.isnull().sum() \/ df.shape[0]) * 100\n    sk = df.skew()\n    krt = df.kurt()\n    \n    print('Data shape:', df.shape)\n\n    a = {\"Type\" : types , \"Total count\" :counts ,\"Missing Ratio\" :missing ,\"Null Values\":nas,\n    \"Distinct Values\":distincts ,\"Skewness\":sk,\"Kurtosis\":krt ,\"Unique Values\": uniques}\n    s =pd.DataFrame(a).sort_values(by=[\"Type\"])\n    \n    return s","c389b864":"# find categorical values\nfor col in train.columns:\n     if (train[col].nunique() < 10):\n        print(f\"{col}: {train[col].unique()}\")","2b626859":"details = summary(train)\ndetails","9730abd5":"train.isnull().sum()","3027fddf":"numerical = [ \"Sex\",\"Pclass\"]\nfig, axes = plt.subplots(1, 2, figsize=(25, 5), sharey=True )\nfig.suptitle('Analyze by pivoting features',fontsize = \"25\")\n\ndef analyze_numerical_data(data):\n    k=0\n    for j in numerical:\n         print(\"Analyze\"+ \" \" +j,\"\\n\",train[[j, \"Survived\"]].groupby(train[j], as_index=False).mean().sort_values(by='Survived', ascending=False),\"\\n\\t\")\n         sns.barplot(ax=axes[k], x=j, y =\"Survived\", data=train, palette=\"Set3\",\n                    linewidth=1,errcolor=\".4\", edgecolor=\".2\")\n         k +=1\n         \n    return\nanalyze_numerical_data(train)","af551d3a":"numerical = [ \"Embarked\",\"SibSp\",\"Parch\"]\nfig, axes = plt.subplots(1, 3, figsize=(25, 5), sharey=True )\nfig.suptitle('Analyze by pivoting features',fontsize = \"25\")\n\ndef analyze_numerical_data(data):\n    k=0\n    for j in numerical:\n         print(\"Analyze\"+ \" \" +j,\"\\n\",train[[j, \"Survived\"]].groupby(train[j], as_index=False).mean().sort_values(by='Survived', ascending=False),\"\\n\\t\")\n         sns.barplot(ax=axes[k], x=j, y =\"Survived\", data=train, palette=\"Set3\",\n                    linewidth=1,errcolor=\".4\", edgecolor=\".2\")\n         k +=1\n         \n    return\nanalyze_numerical_data(train)","5fb115cd":"train['Ticket'].describe()\n","e34c9a2b":"train['Ticket'].head()\n","c2c3f449":"tick_df = train['Ticket'].str.split(' ', 1, expand=True).copy()\n","231b1bb0":"tick_df","828cd5cb":"tick_df[1][tick_df[1].isnull()] = tick_df[0]\ntick_df\n","a1c73a16":"tick_df[0][tick_df[1] == tick_df[0]] = 'no_prefix' \ntrain[['Ticket_prefix', 'Ticket_number']]= tick_df\ntrain['Ticket_prefix'].value_counts().nlargest(10)","47fa4a7e":"tick_df","5935f006":"# split Ticket into two parts and move all numbers to Ticket_number if there is no space (' ') included. (workaround)\n# we continue using the screening_df since we want to have a look at the chance of survival in each group.\n# IN BAR BA screening_df\ntmp_df = screening_df['Ticket'].str.split(' ', 1, expand=True).copy()\ntmp_df","8289d9f7":"\ntmp_df[1][tmp_df[1].isnull()] = tmp_df[0]\ntmp_df[0][tmp_df[1] == tmp_df[0]] = 'no_prefix' \ntmp_df","2928de0d":"screening_df[['Ticket_prefix', 'Ticket_number']]= tmp_df","c61fc05f":"screening_df['Leading_ticket_numbers'] = screening_df['Ticket_number'].map(lambda x : x[0:3])\nscreening_df","5c7dc55f":"screening_df['First_ticket_numbers'] = screening_df['Ticket_number'].map(lambda x : x[0:1])\nscreening_df","c5693401":"l = screening_df['First_ticket_numbers'].unique()\nl","f62d2345":"screening_df['First_ticket_numbers'].value_counts()\n","afa1ec99":"for l in screening_df['First_ticket_numbers'].unique():\n    df = screening_df[screening_df['First_ticket_numbers'] == l].copy()\n    print(f\"First_ticket_number {l}: # 1. class ticket {len(df[df['Pclass']==1])} \/ # 2. class tickets {len(df[df['Pclass']==2])} \/ # 3. class tickets {len(df[df['Pclass']==3])}\")","79f4a2cb":"screening_df['Leading_ticket_numbers'].value_counts().nlargest(5) ","24ad37f5":"print(\"How many passengers are in groups of a certain sizes?\")\nprint(55*\"_\")\nfor g in [0, 1, 2, 3, 4, 5, 10, 15, 20]:\n    list_of_groups = [x[1] for x in screening_df['Leading_ticket_numbers'].value_counts().items() if x[1] > g]\n    print(list_of_groups )\n    print(f\"min group size {g}: includes {sum(list_of_groups)} \/ {len(screening_df)} (# groups: {len(list_of_groups)})\")","fce58878":"# Calculate survival probabilities for each group with more than 15 members:\nnumber_groups = [x[0] for x in screening_df['Leading_ticket_numbers'].value_counts().items() if x[1] > 15]\ngroup_prob_dict = {}\nfor n in number_groups:\n    df = screening_df[screening_df['Leading_ticket_numbers'] == n].copy()\n    group_prob_dict[n] = round(len(df[df['Survived'] == 1]) \/ len(df['Survived']),2)\n\n# plot probabilities\nprobability_visualization(prob_dict=group_prob_dict, title='Probability of survival for each ticket number group')","a5d9e70b":"def probability_visualization(prob_dict, title):\n    df = pd.DataFrame.from_dict(prob_dict, orient='index').rename(columns={0: \"survival_probability\"})\n    df['label'] = df.index\n    fg = sns.catplot(data=df, kind=\"bar\", y=\"label\", x=\"survival_probability\", height=5, color=\"#97A7B2\",  aspect=3.9, linewidth = 3, ec=\"#424949\")\n    fg.set_xticklabels(fontsize=15)\n    fg.set_yticklabels(fontsize=15)\n    fg.fig.subplots_adjust(top=0.8)\n    fg.fig.suptitle(title, fontsize=\"28\");","e2ad4398":"# calculat survival probability for each group of Ticket_prefix\nprefix_prob_dict = {}\nfor n in ['no_prefix', 'PC', 'C.A.', 'STON\/O', 'A\/5', 'W.\/C.', 'CA.', 'SOTON\/O.Q.', 'A\/5.', 'SOTON\/OQ']:\n    df = train[train['Ticket_prefix'] == n].copy()\n    prefix_prob_dict[n] = round(len(df[df['Survived'] == 1]) \/ len(df['Survived']),2)\n    print(prefix_prob_dict)\n #plot probabilities\nprobability_visualization(prob_dict=prefix_prob_dict, title='Probability of survival for each ticket prefix')","b1e40684":"f, ax = plt.subplots(figsize=(15, 6))\n\n# Plot the total crashes\ndf = pd.DataFrame.from_dict(prefix_prob_dict, orient='index').rename(columns={0: \"survival_probability\"})\ndf['label'] = df.index\nsns.set_color_codes(\"pastel\")\nsns.barplot(x= \"survival_probability\", y=\"label\", data=df)\n                  \n\n# Add a legend and informative axis label\nax.legend(ncol=2, loc=\"lower right\", frameon=True)\nax.set(xlim=(0, 1), ylabel=\"\",\n       xlabel=\"Probability of survival for each ticket prefix\")\nsns.despine(left=True, bottom=True)","18ce41f6":"# add fare per person and group size: (excluding child discounts)\ndataset['Fare_per_person'] = 0.\ndataset['Group_size'] = 0.\nfor index, row in dataset.iterrows():\n    # using the combined_df for group size\n    group_size = dataset['Ticket'].value_counts()[row['Ticket']]\n    dataset.at[index, 'Fare_per_person'] = row['Fare'] \/ group_size\n    dataset.at[index, 'Group_size'] = group_size","b0abb55b":"dataset.head(10)","829ff5d9":"screening_df['Leading_ticket_numbers'] .unique()","246e5a6f":"# add fare per person and group size: (excluding child discounts)\nscreening_df['Fare_per_person'] = 0.\nscreening_df['Group_size'] = 0.\nfor index, row in screening_df.iterrows():\n    # using the combined_df for group size\n    group_size = dataset['Ticket'].value_counts()[row['Ticket']]\n    screening_df.at[index, 'Fare_per_person'] = row['Fare'] \/ group_size\n    screening_df.at[index, 'Group_size'] = group_size","18449bb8":"print(45* \"-\")\nprint(f\"# rows with Leading_ticket_numbers '2. : {len(screening_df[screening_df['Leading_ticket_numbers'] =='2. '])}\")\nprint(f\"# rows with Leading_ticket_numbers 'LIN: {len(screening_df[screening_df['Leading_ticket_numbers'] =='LIN'])}\")\nprint(f\"# rows with Leading_ticket_numbers Bas': {len(screening_df[screening_df['Leading_ticket_numbers'] =='Bas'])}\")","3c75729b":"# what about the class in tease groups?\nprint(60* \"-\")\nprint(f\" Unique class values for Leading_ticket_numbers {'2. '}: {screening_df['Pclass'][screening_df['Leading_ticket_numbers'] =='2. '].unique()}\")\nprint(f\" Unique class values for Leading_ticket_numbers {'LIN'}: {screening_df['Pclass'][screening_df['Leading_ticket_numbers'] =='LIN'].unique()}\")\nprint(f\" Unique class values for Leading_ticket_numbers {'Bas'}: {screening_df['Pclass'][screening_df['Leading_ticket_numbers'] =='Bas'].unique()}\")\nprint(60* \"-\")","5f4fbf89":"print(60* \"-\")\nprint(f\" Unique class values for Leading_ticket_numbers & rows with Leading_ticket_numbers {'2. '}: {screening_df['Pclass'][screening_df['Leading_ticket_numbers'] =='2. '].value_counts()}\")\nprint(f\" Unique class values for Leading_ticket_numbers & rows with Leading_ticket_numbers {'LIN'}: {screening_df['Pclass'][screening_df['Leading_ticket_numbers'] =='LIN'].value_counts()}\")\nprint(f\" Unique class values for Leading_ticket_numbers & rows with Leading_ticket_numbers {'Bas'}: {screening_df['Pclass'][screening_df['Leading_ticket_numbers'] =='Bas'].value_counts()}\")\nprint(60* \"-\")","6d3e61df":"# lets just use 303, 304 and 202 as Leading_ticket_numbers for these values then. (We remember that the first number usually corresponded to the Pclass)\n# Are 301, 302 amd 201 already taken?\nprint(45* \"-\")\nprint(f\"# rows with Leading_ticket_numbers 303: {len(screening_df[screening_df['Leading_ticket_numbers'] =='303'])}\")\nprint(f\"# rows with Leading_ticket_numbers 304: {len(screening_df[screening_df['Leading_ticket_numbers'] =='304'])}\")\nprint(f\"# rows with Leading_ticket_numbers 202: {len(screening_df[screening_df['Leading_ticket_numbers'] =='202'])}\")\nprint(45* \"-\")","d4751eb5":"# Looks good. Let's adjust the values amd cast Leading_ticket_number.\nscreening_df.loc[screening_df['Leading_ticket_numbers'] =='2. ', 'Leading_ticket_numbers']  = 303\nscreening_df.loc[screening_df['Leading_ticket_numbers'] =='LIN', 'Leading_ticket_numbers']  = 304\nscreening_df.loc[screening_df['Leading_ticket_numbers'] =='Bas', 'Leading_ticket_numbers']  = 202\nscreening_df['Leading_ticket_numbers'] = screening_df['Leading_ticket_numbers'].astype(int)","8a4f733c":"screening_df['Leading_ticket_numbers'] .unique()","00bcf00d":"screening_df.head()","4abf0706":"# build figure\nfig = plt.figure(figsize=(25,5))\n\n# add grid to figure\ngs = fig.add_gridspec(1,3)\n\n# fill grid with subplots\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\n\n# cahnge fontsize for subheadlines\nax0.set_title('Pclass = 1', fontsize=20)\nax1.set_title('Pclass = 2', fontsize=20)\nax2.set_title('Pclass = 3', fontsize=20)\n\n# change lable size for x and y \nax0.tick_params(labelsize=15)\nax1.tick_params(labelsize=15)\nax2.tick_params(labelsize=15)\n\n# set x range for each subplot\nax0.set_xlim(0, 100)\nax1.set_xlim(0, 50)\nax2.set_xlim(0, 30)\n\n# plot data onto each axis\nsns.kdeplot(data=screening_df[screening_df['Pclass']==1], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax0, label=\"Fare\").set(xlabel=\"Fare\", ylabel=\"\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==1], x=\"Fare_per_person\", color=\"#935116\", fill = True, ax=ax0, label=\"Fare_per_person\", linewidth = 1)\nsns.kdeplot(data=screening_df[screening_df['Pclass']==2], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax1, label=\"Fare\").set(xlabel=\"Fare\", ylabel=\"\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==2], x=\"Fare_per_person\", color=\"#935116\", fill = True, ax=ax1, label=\"Fare_per_person\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==3], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax2, label=\"Fare\").set(xlabel=\"Fare\", ylabel=\"\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==3], x=\"Fare_per_person\", color=\"#935116\", fill = True, ax=ax2, label=\"Fare_per_person\")\n\n# add legend (You must first set the \"label\" parameter for each plot)\nax0.legend(facecolor=\"#E0D3AF\", edgecolor=\"#424949\")\nax1.legend(facecolor=\"#E0D3AF\", edgecolor=\"#424949\")\nax2.legend(facecolor=\"#E0D3AF\", edgecolor=\"#424949\")\n\n# add headline\nfig.subplots_adjust(top=0.8)\nfig.suptitle('Fare-Distribution compared to Fare_per_person-Distribution for each class', fontsize=\"28\", color=\"#424949\");","070de9d1":"fg = sns.displot(\n    screening_df, x=\"Group_size\", col=\"Pclass\", hue = 'Survived',\n    binwidth=1, height=4, aspect=1.67)\n\n# set label size for x and y labels\nfg.set_xticklabels(fontsize=15)\nfg.set_yticklabels(fontsize=15)\n\n# add headline\nfg.fig.subplots_adjust(top=0.8)\nfg.fig.suptitle('Group_size vs Survived in each Pclass', fontsize=\"28\", color=\"#424949\")","b39fe979":"# building feature\nscreening_df.loc[screening_df['Age'] <= 9, 'Child'] = 1\nscreening_df.loc[screening_df['Age'] > 9, 'Child'] = 0","ec820b4e":"# claculate probabilites for survival for childs in groups\nchild_prob_dict = {}\nfor n in [0, 1]:\n    for p in [1, 2, 3]:\n        df = screening_df[(screening_df['Child'] == n) & (screening_df['Pclass'] == p)].copy()\n        if n == 1:\n            dict_key= f\"child_{p}_class\"\n            \n        else:\n            dict_key= f\"adult_{p}_class\"\n        child_prob_dict[dict_key] = round(len(df[df['Survived'] == 1]) \/ len(df['Survived']),2)\n        \n# plot probabilities\nprobability_visualization(prob_dict=child_prob_dict, title='Probability of survival for children compared to adults in each class.')","a28b672b":"# calculate correlations\ncorr_2 = screening_df.corr()\n\n# plot correlations\nmask = np.triu(np.ones_like(corr_2, dtype=bool))\nf, ax = plt.subplots(figsize=(25, 15))\nax.tick_params(axis='both', which='major', labelsize=15)\ng = sns.heatmap(corr_2, mask=mask, cmap=\"coolwarm\", center=0, square=True, linewidths=1, linecolor=\"#424949\", annot=True,\n            cbar_kws={\"shrink\": 0.6}).set_title('Pairwise correlation', fontsize=\"28\")","839a7ca4":"import string\ndef substrings_in_string(big_string, substrings):\n    for substring in substrings:\n        if str.find(big_string, substring) != -1:\n            return substring\n    print (big_string)\n    return np.nan\ntitle_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n                    'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',\n                    'Don', 'Jonkheer']\ntrain['Title']=train['Name'].map(lambda x: substrings_in_string(x, title_list))\n \n#replacing all titles with mr, mrs, miss, master\ndef replace_titles(x):\n    title=x['Title']\n    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n        return 'Mr'\n    elif title in ['Countess', 'Mme']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='Male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title\ntrain['Title']=train.apply(replace_titles, axis=1)","378fceac":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train, test]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)\n# we can now drop the cabin feature\ntrain = train.drop(['Cabin'], axis=1)\ntest = test.drop(['Cabin'], axis=1)","f21ebbaa":"train.head()","9bf0e654":"train[train[\"Ticket\"] == '347082']","4a27bffb":"train[\"Embarked\"]=train[\"Embarked\"].fillna(value=\"Q\")","357cec89":"train.isna().sum()","efc455f5":"# find unique data \n\ndef find_unique_data(data):\n    for i in data:\n            if i  in [\"Embarked\", \"Sex\",\"Pclass\",\"SibSp\",\"Parch\"]:\n                        Total = np.sort(data[i].unique())\n                        print(Total)\n    return Total\n# call_find_unique_data\nfind_unique_data(train)","72f5f725":"# drop \ntrain_new = train.drop(columns=[\"Name\",\"Ticket\",\"PassengerId\"])\ntrain_new","24164191":"le = LabelEncoder()\ntrain_new[\"Sex\"] = le.fit_transform(train_new['Sex'])\ntrain_new[\"Embarked\"] = le.fit_transform(train_new['Embarked'])\ntrain_new","fa14f047":"train_new['Sex'] = train_new['Sex'].astype('category')\ntrain_new['Embarked'] = train_new['Embarked'].astype('category')\ntrain_new['Pclass'] = train_new['Pclass'].astype('category')\ntrain_new['SibSp'] = train_new['SibSp'].astype('category')\ntrain_new['Parch'] = train_new['Parch'].astype('category')\n\ntrain_new.dtypes","ec12eb1c":"train_new.isna().sum()","3495139d":"#\ndata_age_nan_index = train_new[train_new[\"Age\"].isnull()].index\nfor i in data_age_nan_index:\n    mean_age = train_new[\"Age\"][(train_new[\"Pclass\"]==train_new.iloc[i][\"Pclass\"])].median()\n    train_new[\"Age\"].iloc[i] = mean_age","e91e9d94":"train_new.isna().sum()","acdb9711":"from scipy import stats\n\nCATEGORICAL_VARIABLES = [\"Embarked\", \"Sex\",\"Pclass\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]                     \n\nfor c in CATEGORICAL_VARIABLES:\n    if c not in [\"Embarked\", \"Sex\",\"Pclass\",\"SibSp\",\"Parch\"]:\n        correlation = stats.pearsonr(train_new[c], train_new[\"Survived\"])\n    else:\n        correlation = stats.pointbiserialr(train_new[c], train_new[\"Survived\"])\n    print(\"Correlation of %s to Survived is %s\" %(c, correlation))","a85a493e":"\n# the independent variables set\nX = train_new[[\"Embarked\", \"Sex\",\"Pclass\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]]\n  \n# VIF dataframe\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                          for i in range(len(X.columns))]\n  \nprint(vif_data)","2a008bf1":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(\n    rows=2, cols=2,\n    specs=[[{\"type\": \"Box\"}, {\"type\": \"Box\"}],\n           [{\"type\": \"Box\"}, {\"type\": \"Box\"}]],\n)\n\nfig.add_trace(go.Box(\n    y=train[\"Age\"],\n    name='Age(Only Mean)',\n    marker_color='darkblue',\n    boxmean=True # represent mean\n) , row=1, col=1 )\nfig.add_trace(go.Box(\n    y=train[\"Age\"], \n    name='Age(Mean & SD)',\n    marker_color='royalblue',\n    boxmean='sd' # represent mean and standard deviation\n) , row=1, col=1)\n\n\nfig.add_trace(go.Box(y =train[\"SibSp\"],name = \"SibSp\"),\n               row=1, col=2)\n\nfig.add_trace(go.Box(y = [\"Parch\"],boxpoints='all' ,name = \"Parch\"),\n              row=2, col=1)\n\nfig.add_trace(go.Box(y=train[\"Fare\"],boxpoints='all', name = \"Fare\"),\n              row=2, col=2)\n\n\nfig.update_layout(height=700, showlegend=True)\n\nfig.show()","03c5f316":"train.head()","3a4fc994":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(20, 4))\nwomen = train[train['Sex']=='female']\nmen = train[train['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","6f40adce":"women.head(15)","67a7f917":"def copy_df(df):\n    return df.copy()\n\ndef fill_age(df):\n    df.loc[:, \"Age\"] = df.groupby(['Pclass', 'Sex']).transform(lambda x: x.fillna(x.mean())) \n    return df\n\ndef drop_missing_embarked_rows(df): \n    return df[df['Embarked'].notnull()]\n\ndef fill_fare(df):\n    df.loc[:, \"Fare\"] = df.groupby(['Pclass', 'Sex']).transform(lambda x: x.fillna(x.mean())) \n    return df\n\ndef drop_columns(df, columns): \n    df.drop(columns=columns, inplace=True)\n    return df\n\ndef add_leading_ticket_number_feature(df):\n    tmp_df = df['Ticket'].str.split(' ', 1, expand=True).copy()\n    tmp_df[1][tmp_df[1].isnull()] = tmp_df[0]\n    tmp_df['Leading_ticket_numbers'] = tmp_df[1].map(lambda x : x[0:3])\n    df['Leading_ticket_numbers'] = tmp_df['Leading_ticket_numbers'].copy()\n    return df\n\ndef cast_leading_ticket_number_to_int(df):\n    df.loc[df['Leading_ticket_numbers'] =='2. ', 'Leading_ticket_numbers']  = 301\n    df.loc[df['Leading_ticket_numbers'] =='LIN', 'Leading_ticket_numbers']  = 301\n    df.loc[df['Leading_ticket_numbers'] =='Bas', 'Leading_ticket_numbers']  = 201\n    df['Leading_ticket_numbers'] = df['Leading_ticket_numbers'].astype(int)\n    return df\n    \ndef add_group_size_feature(df):\n    df['Group_size'] = 0.\n    for index, row in df.iterrows():\n        df.at[index, 'Group_size'] =  dataset['Ticket'].value_counts()[row['Ticket']]\n    return df\n        \ndef add_fare_per_person_feature(df):\n    df['Fare_per_person'] = 0.\n    for index, row in df.iterrows():\n        df.at[index, 'Fare_per_person'] = row['Fare'] \/ dataset['Ticket'].value_counts()[row['Ticket']]\n    return df\n\ndef add_child_feature(df):\n    df.loc[df['Age'] <= 9, 'Child'] = 1\n    df.loc[df['Age'] > 9, 'Child'] = 0\n    df['Child'] = df['Child'].astype(int)\n    return df\n\ndef one_hot_encoding(df, column):\n    # Get one hot encoding of columns B\n    one_hot_df = pd.get_dummies(df[column])\n    # Drop input column as it is now encoded\n    df = df.drop(column, axis=1)\n    # Join the encoded df\n    df = df.join(one_hot_df)\n    return df\n\ndef norm_col(df, column):\n    df[column] = (df[column]-df[column].mean())\/df[column].std() \n    return df\n\ndef pipeline(df):\n    df = (df\n          .pipe(copy_df)\n          .pipe(fill_age)\n          .pipe(fill_fare)\n          .pipe(drop_missing_embarked_rows)\n          .pipe(add_group_size_feature)\n          .pipe(add_fare_per_person_feature)\n          .pipe(add_child_feature)\n          .pipe(add_leading_ticket_number_feature)\n          .pipe(cast_leading_ticket_number_to_int)\n          .pipe(one_hot_encoding, \"Pclass\")\n          .pipe(one_hot_encoding, \"Sex\")\n          .pipe(one_hot_encoding, \"Embarked\")\n          .pipe(norm_col, \"Age\")\n          .pipe(norm_col, \"Fare_per_person\")\n          .pipe(norm_col, \"Leading_ticket_numbers\")\n          .pipe(norm_col, \"Group_size\")\n          .pipe(drop_columns, ['Name', 'PassengerId', 'Cabin', 'Ticket', 'SibSp', 'Fare']))\n    return df","caf33e5c":"pipeline(train).head()","785b69b7":"train.head()","26e17660":"#moving soon","6104d3c3":"### 2.5 Create table for null and missing values","93174a6d":"### 2.3 Outlier Detection\n","3c195cca":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b><button style='font-size:20px'>Thougts <i class='fas fa-bullhorn'><\/i><\/button>\n<\/b> There seems to be some correlations between Ticket_prefix and survivability but since most of the tickets do not include a prefix we will leave this feature aside for now.\nWhat about the ticket numbers?\nCan we group some of them and gain some information about the deck or cabin of these tickets?","de7a836e":"<div style=\"color:#424949;\n           display:fill;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:rgba(180, 180, 180,0.3);\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n\n<h1> 1. Importing the Libraries <\/h1>\n<hr>   \n\n<h1> 2. Load and check data <\/h1>\n<h5> 2.1) load data <\/h5>\n<h5> 2.2) Features identification (Data Dictionary with plotly)<\/h5>\n<h5> 2.3) Outlier detection <\/h5>\n<h5> 2.4) joining train and test set <\/h5>\n<h5> 2.5) Checking Missing Data  <h5>\n\n<hr>   \n<h1> 3. Load and check data <\/h1>\n\n    \n \n<\/div>\n\n\n\n\n\n1 Introduction\n###2 Load and check data\n2.1 load data\n2.2 Outlier detection\n2.3 joining train and test set\n2.4 check for null and missing values\n3 Feature analysis\n3.1 Numerical values\n3.2 Categorical values\n4 Filling missing Values\n4.1 Age\n5 Feature engineering\n5.1 Name\/Title\n5.2 Family Size\n5.3 Cabin\n5.4 Ticket\n6 Modeling\n6.1 Simple modeling\n6.1.1 Cross validate models\n6.1.2 Hyperparamater tunning for best models\n6.1.3 Plot learning curves\n6.1.4 Feature importance of the tree based classifiers\n6.2 Ensemble modeling\n6.2.1 Combining models\n6.3 Prediction\n6.3.1 Predict and Submit results","36203412":"As you can see top 3 categorical variables which have the highest correlation coefficients are:\n\n* 1. Sex (0.54)\n* 2. Pclass (0.33)\n* 3. Fare (0.26)\n","d46f78c5":"<html>\n<body>\n\n   \n   \n      \n<p>\n\n<p style=\"color:rgb(60, 60, 60);text-align: justify; text-justify: inter-word; font-family:Garamond ; font-size:120% \">\n Outliers are extreme values that deviate from other observations on data , they may indicate a variability in a measurement, experimental errors or a novelty. In other words, an outlier is an observation that diverges from an overall pattern on a sample.\nTypes of outliers\nOutliers can be of two kinds: univariate and multivariate. Univariate outliers can be found when looking at a distribution of values in a single feature space. Multivariate outliers can be found in a n-dimensional space (of n-features). Looking at distributions in n-dimensional spaces can be very difficult for the human brain, that is why we need to train a model to do it for us.\nOutliers can also come in different flavours, depending on the environment: point outliers, contextual outliers, or collective outliers. Point outliers are single data points that lay far from the rest of the distribution. Contextual outliers can be noise in data, such as punctuation symbols when realizing text analysis or background noise signal when doing speech recognition. Collective outliers can be subsets of novelties in data such as a signal that may indicate the discovery of new phenomena.\n<ul>Some of the most popular methods for outlier detection are:\n  <li>Z-Score or Extreme Value Analysis (parametric)  <\/li>\n  <li>Probabilistic and Statistical Modeling (parametric) <\/li>\n  <li>Linear Regression Models (PCA, LMS)<\/li>\n  <li>Proximity Based Models (non-parametric)<\/li>\n    <li><span style= \"color:red\";>IQR method -- I will use this method<\/span><\/li>\n   <hr>\n   \nOne way to identify \"Outlier Data\" is to draw a \"Boxplot\" diagram, sometimes called a \"Box and Whiskers\" diagram.\nThis chart is based on the values of<span style= \"color:red\"> Minimum, First Quartile, Median, Third Quartile and Maximum<\/span> The image below shows the location of each on the chart.According to the image, the length of the box is the distance between the first and third quarters, which is called the interquartile range. The end of the lines also indicates the maximum and minimum values. This chart was invented by statistician John Tukey.\n    \n    \n  <!--<h1> <img src= \"https:\/\/miro.medium.com\/max\/700\/1*NRlqiZGQdsIyAu0KzP7LaQ.png\" alt=\"Smiley face\"                 align=\"center\" width=\"550\" height=\"600\"\/> -->\n    \n<h1> <img src= \"https:\/\/www.simplypsychology.org\/boxplot.jpg?ezimgfmt=rs:555x285\/rscb26\/ng:webp\/ngcb26\" alt=\"Smiley face\"align=\"CENTER\" width=\"550\" height=\"600\"\/>\n                       \n<br>\n<span style= \"color:rgb(60, 179, 113);font-size:60%\"> 1)Minimum(Q1 -1.5*IQR) score<\/span>  <br> \n<span style= \"color:blue;font-size:60%\">2)First (lower) quartile(Q1\/25th Percentile)<\/span>  <br>\n<span style= \"color:red;font-size:60%\">3)Median(Q2\/50th Percentile)<\/span> <br>\n<span style= \"color:rgb(255, 165, 0);font-size:60%\">4)Third (upper) quartile(Q3\/75th Percentile)<\/span> <br>\n<span style= \"color:rgb(60, 179, 113);font-size:60%\">5)Maximum(Q3 + 1.5*IQR) score<\/span> <br>  \n<span style= \"color:rgb(106, 90, 205);font-size:60%\">Note1:(interquartile range (IQR)= Q3-Q1)<\/span> <br>\n\n     \n<\/body>\n<\/html>","38d498e0":"<html>\n<body>\n\n <h1 style=\"background-color:rgb(180, 180, 180); text-align:center; border-radius:20px\">Titanic<\/h1>\n   \n      \n<p>\n<img src=\"https:\/\/www.lva.virginia.gov\/exhibits\/titanic\/img\/titanic_cutout.png\" alt=\"Smiley face\" align=\"left\" width=\"600\" height=\"1000\" \/>\n<p style=\"color:rgb(60, 60, 60);text-align: justify; text-justify: inter-word; font-family:Garamond ; font-size:120% \">  \n<b>On the 10th of April 1912, the liner Titanic of the White Star line, set sail on its maiden voyage from Southampton, England for New York.Carrying an estimated 2,224 souls, it was touted as the \u201clargest vessel that ever sailed the seas.\"\nLate in the evening of April 14, 1912, the Titanic collided with an iceberg and slowly sank into the icy waters of the North Atlantic, taking 1,503 lives with it. The next day, newspapers around the world began to tell the tragic story in text, in photographs, even through editorial cartoons. The sinking of the Titanic came at a time when newspapers flourished 22,837 newspapers were operating in the United States that year alone and many covered the tragedy\u2019s aftermath in the striking black and white of newsprint.\nGiven that newspapers from Sidney to New York reported on the Titanic, its story was one of the first truly international news events. Providing readers with gripping daily updates, almost obsessive newspaper coverage of the sad tragedy at sea served as a teasing preview of, what decades later, Marshall Mcluhan would call the information global village and J. G. Ballard would describe as the media landscape.\nTitanic in Black and White represents the Virginia Newspaper Project\u2019s effort to highlight the potential of newspapers as a source for historical and social research and encourages readers to visit Virginia Chronicle and Chronicling America to delve deeper into the dark mysteries surrounding the story of the Titanic. <\/p>\nThis is <a href=\"https:\/\/www.lva.virginia.gov\/exhibits\/titanic\/index.php\" title=\"Wikipedia\" >a reference<\/a>\n\n\n<\/body>\n<\/html>","cbb178c7":"As we can see, there is no very high values of VIF, indicating that these two variables are highly correlated. This is expected as the height of a person does influence their weight. Hence, considering these two features together leads to a model with high multicollinearity.","8d64f5a7":"#### 4.1.2 Seaborn Heatmap","64a1980a":" <h1 style=\"background-color:rgb(180, 180, 180); text-align:center; border-radius:20px\">Decks of the Titanic: Comprehensive Details - 1912<\/h1>\n   \n      \n\n<html>\n<body>  \n      \n<p>\n<!--  <img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/5\/5d\/Titanic_side_plan_annotated_English.png\/1920px-Titanic_side_plan_annotated_English.png\" alt=\"Smiley face\" align=\"left\" width=\"1200\" height=\"1200\" \/> -->\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/0\/0d\/Olympic_%26_Titanic_cutaway_diagram.png\/800px-Olympic_%26_Titanic_cutaway_diagram.png\" alt=\"Smiley face\" align=\"left\" width=\"600\" height=\"800\" \/>\n\n<p style=\"color:rgb(60, 60, 60);text-align: justify; text-justify: inter-word; font-family:Garamond ; font-size:120% \">  \nThe Boat Deck, on which the lifeboats were housed. It was from here during the early hours of 15 April 1912 that Titanic's lifeboats were lowered into the North Atlantic. The bridge and wheelhouse were at the forward end, in front of the captain's and officers' quarters. The bridge stood 8 feet (2.4 m) above the deck, extending out to either side so that the ship could be controlled while docking. The wheelhouse stood within the bridge. The entrance to the First Class Grand Staircase and gymnasium were located midships along with the raised roof of the First Class lounge, while at the rear of the deck were the roof of the First Class smoke room and the relatively modest Second Class entrance. The wood-covered deck was divided into four segregated promenades: for officers, First Class passengers, engineers, and Second Class passengers respectively. Lifeboats lined the side of the deck except in the First Class area, where there was a gap so that the view would not be spoiled.\nA Deck, also called the Promenade Deck, extended along the entire 546 feet (166 m) length of the superstructure. It was reserved exclusively for First Class passengers and contained First Class cabins, the First Class lounge, smoke room, reading and writing rooms and Palm Court.\nB Deck, the Bridge Deck, was the top weight-bearing deck and the uppermost level of the hull. More First Class passenger accommodations were located here with six palatial staterooms (cabins) featuring their own private promenades. On Titanic, the \u00c0 La Carte Restaurant and the Caf\u00e9 Parisien provided luxury dining facilities to First Class passengers. Both were run by subcontracted chefs and their staff; all were lost in the disaster. The Second Class smoking room and entrance hall were both located on this deck. The raised forecastle of the ship was forward of the Bridge Deck, accommodating Number 1 hatch (the main hatch through to the cargo holds), numerous pieces of machinery and the anchor housings.[c] Aft of the Bridge Deck was the raised Poop Deck, 106 feet (32 m) long, used as a promenade by Third Class passengers. It was where many of Titanic's passengers and crew made their last stand as the ship sank. The forecastle and Poop Deck were separated from the Bridge Deck by well decks.\nC Deck, the Shelter Deck, was the highest deck to run uninterrupted from stem to stern. It included both well decks; the aft one served as part of the Third Class promenade. Crew cabins were housed below the forecastle and Third Class public rooms were housed below the Poop Deck. In between were the majority of First Class cabins and the Second Class library.\nD Deck, the Saloon Deck, was dominated by three large public rooms\u2014the First Class Reception Room, the First Class Dining Saloon and the Second Class Dining Saloon. An open space was provided for Third Class passengers. First, Second and Third Class passengers had cabins on this deck, with berths for firemen located in the bow. It was the highest level reached by the ship's watertight bulkheads (though only by eight of the fifteen bulkheads).\nE Deck, the Upper Deck, was predominantly used for passenger accommodation for all three classes plus berths for cooks, seamen, stewards and trimmers. Along its length ran a long passageway nicknamed Scotland Road, in reference to a famous street in Liverpool. Scotland Road was used by Third Class passengers and crew members.\nF Deck, the Middle Deck, was the last complete deck and mainly accommodated Second and Third Class passengers and several departments of the crew. The Third Class dining saloon was located here, as were the swimming pool, Turkish bath and kennels.\nG Deck, the Lower Deck, was the lowest complete deck that carried passengers, and had the lowest portholes, just above the waterline. The squash court was located here along with the travelling post office where letters and parcels were sorted ready for delivery when the ship docked. Food was also stored here. The deck was interrupted at several points by orlop (partial) decks over the boiler, engine and turbine rooms.\nThe Orlop Decks and the Tank Top below that were on the lowest level of the ship, below the waterline. The orlop decks were used as cargo spaces, while the Tank Top\u2014the inner bottom of the ship's hull\u2014provided the platform on which the ship's boilers, engines, turbines and electrical generators were housed. This area of the ship was occupied by the engine and boiler rooms, areas which passengers would have been prohibited from seeing. They were connected with higher levels of the ship by flights of stairs; twin spiral stairways near the bow provided access up to D Deck. <\/p>\n\n\n<\/body>\n<\/html>\n","44233775":"<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h1> 3. Data Preparation using pipe <\/h1>\n<h2> 3.1 Summary of observations <\/h2>\n    <b> So what have we learned that must be considered in the pipeline?  <\/b> \n<ol>\n  <li>We have some missing Age values and since there is some correlation between Age, Pclass and Sex we should use this group to fill those missing values.<\/li>\n  <li>There are two missing Embarked values in the training data but not in the test data. So we can just drop these two lines.<\/li>\n  <li>There is one missing Fare value in the Test data. We can simply group by Pclass, Sax to fill this value.<\/li>\n  <li>Categorical features like Sex and Embarked are important and using integer encoding alone might not be a good idear since there is no order in theas features.<\/li>\n  <li>Other important features like Pclass, Leading_ticket_numbers and Group_size are ordert. So we probably don't need to use one hot encoding on those features.<\/li>\n  <li>Group_size combinds informations of SibSp and Parch and could replace them.<\/li>\n  <li>We also know that Fare is the price of one ticket and this can include up to 9 passengers. Even if there is no strong evidence that Fare_per_person holds more information we will include it in our model.<\/li>\n  <li>Furthermore we will drop PassengerId, Name, SibSp, Cabin, Ticket and Fare. There is no strong indication that PassengerId and Name includes valuable information. SibSp is included in Group_size and Cabin has too many missing values. Ticket on the other hand includes a lot of information but we will only use the first three numbers without any prefix as categorical feature. Fare will be replaced bei Fare_per_person<\/li>\n  <li>We will also add the Child feature and use one hot incoding for Sex and Embarked.<\/li>\n  <li>Last but not least, we should normalize Age and Fare_per_person<\/li>\n<\/ol> \n    <b>So lets dive right in!<\/b> \n<h2> 3.2 Pipeline functions <\/h2>","db598811":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b><button style='font-size:20px'>Thougts <i class='fas fa-bullhorn'><\/i><\/button>\n<\/b>  Before we create different visualizations we should set a uniform style. Maybe something that looks old. Like an old newspaper reporting the Titanic disaster.\n","bb59ef4f":"\n## 3. Feature analysis\n* Numerical Analysis\n* Categorical Analysi","42303c33":"# Detecting Multicollinearity using VIF\n","ef6ec6a3":"# j","3394f82d":"<h2 style=\"background-color:rgb(180, 180, 180); text-align:left; border-radius:20px\">2. Load and check data<\/h2>\n\n### 2.1 Load data\n","fb5735af":"# Ticket frequently","6033832f":"### 4.1 mothod 1 \n#### 4.1.1 seaborn displot","8bed01d1":"Been playing with the Ticket Variable. The Typical ticket is either a number or a number with a text prefix. First task was to split this into two, Ticket_Pre and Ticket_Num.","8e95c310":"### 2.2 Features identification (Data Dictionary with plotly)","a1d18c86":"NEW FEATURE","332fb656":"\n<h1 style=\"background-color:rgb(180, 180, 180); text-align:center; border-radius:20px\">Titanic Problem Roadmap (EDA & CLASSIFICATION)<\/h1>\n<h1 style=\"border:2px ;text-align:left;color:#0000FF;font-size:16px\">2021-8-22<\/h1> ","3dd901eb":"# Titanic\u2019s Decks\n","3ce6a956":"### 2.4 Drop Outliers & Joining Train and Test Data","357e5467":"<h2 style=\"background-color:rgb(180, 180, 180); text-align:left; border-radius:25px\">1. Importing the Libraries<\/h2>\n","a998529c":"<div style=\"color:#424549;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E000AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2>New Feature Child from Age (0-9) <\/h2>\nWe have previously seen that young children had a greatly increased probability of survival and that age generally did not have a large impact on survival. So let's create a feature that maps whether a passenger is a child or not as a subset of Age. This could make life a little easier for the algorithm.","5eb74438":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> This is interesting. Being alone in the first class seems to have a negative impact on your chance of survival. Being in a large group (more than 4 people) in the third class seems to lower your survival probability as well. This could be a useful feature to predict whether someone has survived.","feb3e280":"<div style=\"color:#424949;\n           display:fill;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:rgba(180, 180, 180, 0.3);\n           font-size:16px;\n           font-family: Garamond;\n           letter-spacing:0.3px;\n           padding: 0.7em;\n           text-align:left\">\n\n<h1> <b> VIF (Variable Inflation Factors) <\/b> <\/h1>\n<b>Multicollinearity can be detected via various methods. In this article, we will focus on the most common one \u2013 VIF (Variable Inflation Factors).\n    \n\u201d VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable. \u201c OR \"VIF score of an independent variable represents how well the variable is explained by other independent variables.\"\n\nR^2 value is determined to find out how well an independent variable is described by the other independent variables. A high value of R^2 means that the variable is highly correlated with the other variables. This is captured by the VIF which is denoted below:\n\n    \n\\begin{split}\n\tVIF & = \\frac{1}{1-R^2}\\\\\n\t & \n\t\\end{split}\n  \n\n\nSo, the closer the R^2 value to 1, the higher the value of VIF and the higher the multicollinearity with the particular independent variable.","af9a6753":"#### 4.1.3 Missinggo","5eece0d0":"# 4.Checking Missing Data ","ce36d242":"new feature\n","d35f35ea":"### 3.1 Numerical Analysis\n#### 3.1.1 Correlation between independent variables\n"}}