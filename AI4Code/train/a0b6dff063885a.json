{"cell_type":{"d4d1545c":"code","50134b8b":"code","149d57f1":"code","1e22cd65":"code","3cb78117":"code","a0c924a1":"code","29b82fc1":"code","150cf761":"code","9c2e438b":"code","c6e9ddb6":"code","fc02095a":"code","3af05f05":"code","6f23c05e":"code","877db075":"code","7b10f2f2":"code","5bc52aea":"code","146ce88b":"code","ba1700f3":"code","3ef9be47":"markdown","4509cb3c":"markdown","e5d0b1f5":"markdown","dcb432f3":"markdown","0c0a54a2":"markdown","00906364":"markdown","f122e46c":"markdown","320c7dad":"markdown","45842eb2":"markdown","0aebe064":"markdown","de2e5deb":"markdown","57304c35":"markdown","d24726ff":"markdown","0d2dd266":"markdown","180b5f6b":"markdown"},"source":{"d4d1545c":"import re\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set()","50134b8b":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain.sample(5)","149d57f1":"def preprocess(text):\n    text = text.lower()\n    # links\n    text = re.sub(r'http\\S+', 'link', text)\n    text = re.sub(r'www\\S+', 'link', text)\n    # mails\n    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n    # abbreviations\n    text = text.replace(r'&amp;?', r'and')\n    text = text.replace(r'&lt;', r'<')\n    text = text.replace(r'&gt;', r'>')\n    # non ascii symbols\n    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n    # safe only latters, numbers and punktuations\n    text = re.sub(r'[^a-zA-Z0-9!?. ]', ' ', text)\n    # duplicated punkt\n    text = re.sub(r'[!]+', '!', text)\n    text = re.sub(r'[?]+', '?', text)\n    text = re.sub(r'[.]+', '.', text)\n    # several spaces\n    text = re.sub(r'\\s+', ' ', text)\n    return text\n\nprint(preprocess('&gt;&gt; $15 Aftershock : Protect Yourself and Profit in the    Next Global Financial... ##book http:\/\/t.co\/f6ntUc734Z\\n@esquireattire'))","1e22cd65":"train['text'] = train['text'].apply(preprocess)","3cb78117":"disast_top_keywords = list(train[train['target'] == 1]['keyword'].value_counts()[:50].index)\nnon_disast_top_keywords = list(train[train['target'] == 0]['keyword'].value_counts()[:50].index)\ntop_keywords = [disast_top_keywords + non_disast_top_keywords]","a0c924a1":"import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom transformers import DistilBertTokenizer, DistilBertModel\nfrom transformers import BertTokenizer, BertModel","29b82fc1":"class DataLoader:\n    \"\"\"Texts and non-text features are putted to the input,\n    at the output we get data that can be loaded into the our neural network\"\"\"\n    def __init__(self, dataframe, tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased'), \n                 cat_features=[], batch_size=1, categories='auto', shuffle=True):\n        \n        self.shuffle = shuffle\n        self.dataframe = dataframe\n        self.cat_features = cat_features\n        self.size = self.dataframe.shape[0]\n        self.batch_size = batch_size\n        \n        categories = [np.array(column_cats) for column_cats in categories]\n        self.cat_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, categories=categories)\n        self.cat_encoder.fit(self.dataframe[cat_features])\n        \n        self.tokenizer = tokenizer\n        \n    def get_cat_features_dim(self):\n        return self.cat_encoder.get_feature_names().shape[0]\n    \n    def __iter__(self):\n        self.idx = 0\n        if self.shuffle:\n            self.dataframe = self.dataframe.sample(self.size).reset_index(drop=True)\n        return self\n        \n    def __next__(self):\n        if self.idx < self.size:\n            bounds = (self.idx, self.idx + self.batch_size)\n            dataframe_batch = self.dataframe.iloc[bounds[0]: bounds[1]]\n            \n            # Transforming categorical features into an one hot vector\n            encoded_features = self.cat_encoder.transform(dataframe_batch[self.cat_features])\n            \n            text = dataframe_batch['text'].tolist()\n            # Tokenizing text features\n            encoded_text = self.tokenizer(text, return_tensors='pt', padding=True)\n            # For python >= 3.9:\n            # inp_batch = {'nontext_features': torch.FloatTensor(encoded_features)} | encoded_text\n            inp_batch = {'nontext_features': torch.FloatTensor(encoded_features), **encoded_text}\n            target_batch = torch.FloatTensor(dataframe_batch['target'].tolist())\n            \n            self.idx = bounds[1]\n            \n            return inp_batch, target_batch\n        else:\n            raise StopIteration\n\n# Simple example of DataLoder work:\n# loader = DataLoader(train, cat_features=['keyword'], batch_size=2, categories=top_keywords)\n# print(next(iter(loader)))","150cf761":"class ConcatModel(nn.Module):\n    def __init__(self, nontext_features_dim,\n                 bert_model=DistilBertModel.from_pretrained(\"distilbert-base-uncased\")):\n        super().__init__()\n        self.bert = bert_model\n        \n        bert_embedding_dim = self.bert.embeddings.word_embeddings.embedding_dim\n        dense_input_dim = nontext_features_dim + bert_embedding_dim\n        \n        # In my case I'll use this layers for classification.\n        # But you can modify it for your task as you want.\n        self.dropout = nn.Dropout(p=0.1)\n        self.batch_norm = torch.nn.LayerNorm(dense_input_dim)\n        self.dense = nn.Linear(dense_input_dim, 1)\n        self.activation = nn.Sigmoid()\n        \n    def forward(self, nontext_features, **kwargs):\n        # Getting the output of [CLS] token of the BERT.\n        cls_output = self.bert.forward(**kwargs)[0][:, 0]\n        \n        # nontext_feautes shape: [batch_size, nontext_features_dim]\n        # Concatenation of text and non-text features\n        x = torch.cat((nontext_features, cls_output), 1)\n        \n        x = self.dropout(x)\n        x = self.batch_norm(x)\n        x = self.dense(x)\n        x = self.activation(x)\n        x = x.flatten()\n        \n        return x\n\n# Simple example of ConcatLoader output\n# model = ConcatModel(loader.get_cat_features_dim())\n# print(model(**next(iter(loader))[0]))","9c2e438b":"from datetime import datetime\n\nfrom IPython.display import clear_output\nfrom sklearn.metrics import accuracy_score, f1_score","c6e9ddb6":"BATCH_SIZE = 320\n\ntrainset, testset = train_test_split(train, test_size=0.2)\n\ntrain_loader = DataLoader(trainset, cat_features=['keyword'], batch_size=BATCH_SIZE, \n                                     categories=top_keywords)\ntest_loader = DataLoader(testset, cat_features=['keyword'], batch_size=BATCH_SIZE, \n                                    categories=top_keywords, shuffle=False)","fc02095a":"cat_features_dim = train_loader.get_cat_features_dim()\n\nmodel = ConcatModel(cat_features_dim)\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr= 1e-5)","3af05f05":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","6f23c05e":"# auxiliary function for transferring the contents of the dictionary to the device.\ndef dict_to_device(dict_, device):\n    device_dict = {}\n    for key in dict_:\n        if isinstance(dict_[key], torch.Tensor):\n            device_dict[key] = dict_[key].to(device)\n        else:\n            device_dict[key] = dict_[key]\n    return device_dict","877db075":"history = []\nrolling_history = []\n\ntrain_f1 = []\nvalid_f1 = []\n\nepochs = 6\n\nmodel.to(device)\n\ntime_start = datetime.now()\nfor i in range(epochs):\n    # train epoch\n    model.train()\n    \n    train_preds = []\n    train_targets = []\n    for j, (inp, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n\n        inp = dict_to_device(inp, device)\n        target = target.to(device)\n        output = model(**inp)\n\n        loss = criterion(output, target)\n        \n        # Calculate l2 norm of last dense layer.\n        l2_norm = 0.1 * (torch.norm(model.dense.weight, p=2) +\n                         torch.norm(model.dense.bias, p=2)).to(device)\n        # Add this to the loss. So we do the regularization of the last layer of the network.\n        loss += l2_norm\n        \n        loss.backward()\n        optimizer.step()\n        \n        train_preds.extend(output.detach().cpu().tolist())\n        train_targets.extend(target.detach().cpu().tolist())\n        \n        history.append(loss.item())\n        # Online plotting rolling average train loss, train and test F1 score.\n        if j%10 == 0 and j > 0:\n\n            rolling_history.append(\n                sum(history[-10: ]) \/ 10\n            )\n            clear_output(wait=True)\n            \n            fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n            \n            ax[0].plot(rolling_history)\n            ax[0].set_title('Training loss')\n\n            ax[1].plot(train_f1)\n            ax[1].set_title('Train f1')\n            \n            ax[2].plot(valid_f1)\n            ax[2].set_title('Validation f1')\n            \n            plt.show()\n        \n    # validation:\n    model.eval()\n    with torch.no_grad():\n        y_pred = []\n        y_true = []\n        # Calculate F1 score on test\n        for inp, target in test_loader:\n            inp = dict_to_device(inp, device)\n            output = model(**inp)\n            pred = [int(ans > 0.5) for ans in output.cpu().tolist()]\n            y_pred.extend(pred)\n            y_true.extend(target.tolist())\n            \n        valid_f1.append(f1_score(y_pred, y_true))\n        \n        # Calculate F1 score on train\n        train_preds = [int(ans > 0.5) for ans in train_preds]\n        train_f1.append(f1_score(train_preds, train_targets))\n        \ntime_end = datetime.now()\ndelta_time = time_end - time_start\nprint(delta_time.seconds, 'seconds')","7b10f2f2":"test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest['target'] = 0\ntest['text'] = test['text'].apply(preprocess)","5bc52aea":"BATCH_SIZE = 320\n\ntrain_loader = DataLoader(train, cat_features=['keyword'], batch_size=BATCH_SIZE, \n                                     categories=top_keywords)\ntest_loader = DataLoader(test, cat_features=['keyword'], batch_size=BATCH_SIZE, \n                                    categories=top_keywords, shuffle=False)","146ce88b":"cat_features_dim = train_loader.get_cat_features_dim()\n\nmodel = ConcatModel(cat_features_dim)\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr= 1e-5)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.train()\n\nepochs = 5\nfor i in range(epochs):\n    train_preds = []\n    train_targets = []\n    for j, (inp, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n\n        inp = dict_to_device(inp, device)\n        target = target.to(device)\n        output = model(**inp)\n\n        loss = criterion(output, target)\n        l2_norm = 0.1 * (torch.norm(model.dense.weight, p=2) +\n                         torch.norm(model.dense.bias, p=2)).to(device)\n        loss += l2_norm\n        \n        loss.backward()\n        optimizer.step()","ba1700f3":"y_pred = []\nmodel.eval()\nwith torch.no_grad():\n    for inp, _ in test_loader:\n        inp = dict_to_device(inp, device)\n        output = model(**inp)\n        pred = [int(ans > 0.5) for ans in output.cpu().tolist()]\n        y_pred.extend(pred)\n        \ntest['target'] = y_pred\ntest[['id', 'target']].to_csv('submission.csv', index=False)","3ef9be47":"## Training and validation","4509cb3c":"# Loading data\nYou cat find data in this competition https:\/\/www.kaggle.com\/c\/nlp-getting-started","e5d0b1f5":"Select device","dcb432f3":"# Training and validation","0c0a54a2":"`Scheme of ConcatModel`","00906364":"# Preprocessing\n## Let's create a simple preprocessor for our text data.","f122e46c":"![image.png](attachment:3497b37c-81bd-4dfc-aee5-2620c18411fd.png)","320c7dad":"Initialize model, loss function(criterion) and optimizer","45842eb2":"\n### This notebook describes how you can use non-text features when training BERT for a scpecific task. Also in the notebook there is a description of how to use regularization on a specific network layer. I will show this using an open \"nlp-getting-started\" competition as an example.\n\nContent:\n* Loading data\n* Preprocess data\n* Building custom DataLoader\n* Building Network\n* Training and validation\n* Predicting test data","0aebe064":"# Building network\n## Main part of notebook. Now let's build the model.","de2e5deb":"Initialize train and test loaders","57304c35":"## Selecting non-text features. In our case it will be categorical features named keyword\nWe will use the top 50 popular keywords in each class as signs.","d24726ff":"`PyTorch implementation`\n\nBut l2 regularization will be added later when we calculate the loss","0d2dd266":"# Predict test data\n## Training on whole dataset and predict test samples","180b5f6b":"# Building custom DataLoader\n## Let's create data loader which we will put into the our future model.\nAs a BERT architecture we will use DistilBERT."}}