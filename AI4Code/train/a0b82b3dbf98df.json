{"cell_type":{"afef46e8":"code","a51dd664":"code","90ae9619":"code","f4c1ecf6":"code","0a81bce0":"code","70989812":"code","f0165bd9":"code","450fdb87":"code","7402c783":"code","b8fb83d9":"code","1e444663":"code","8765b6ee":"code","e53309ad":"code","a7b1fecd":"code","614a869e":"code","b8537f0e":"code","10b0d344":"code","e88c0c1f":"code","b9aa7cea":"code","95abd4a5":"code","fa8154d5":"markdown","52660534":"markdown","5719fa5a":"markdown","ee6a5615":"markdown","f0d84b97":"markdown","1179028c":"markdown","1f08cfd7":"markdown","9824a273":"markdown","6979f20f":"markdown","4822d7cf":"markdown","bbcc9453":"markdown"},"source":{"afef46e8":"import os\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.patches as mpl_patches\nimport matplotlib.pylab as pylab\nparams = {'legend.fontsize': 'x-large',\n          'figure.figsize': (15, 8),\n         'axes.labelsize': 'x-large',\n         'axes.titlesize':'x-large',\n         'xtick.labelsize':'x-large',\n         'ytick.labelsize':'x-large',\n         'text.color' : \"green\",\n          \n         }\npylab.rcParams.update(params)\n\nimport torch\nimport torchaudio\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data.dataloader import DataLoader\nfrom torchvision.models import resnet34\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport librosa\nfrom tqdm.notebook import tqdm\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\nimport IPython.display as ipd\nfrom IPython.display import display, HTML\n\n","a51dd664":"train_path = Path(\"..\/input\/rfcx-species-audio-detection\/train\/\")\ntrain_recs = list(train_path.glob(\"*.flac\"))","90ae9619":"tp_train = pd.read_csv(\"..\/input\/rfcx-species-audio-detection\/train_tp.csv\")\nfp_train = pd.read_csv(\"..\/input\/rfcx-species-audio-detection\/train_fp.csv\")\nsub = pd.read_csv(\"..\/input\/rfcx-species-audio-detection\/sample_submission.csv\")","f4c1ecf6":"print(\"{0}Number of rows in train data: {1}{2}\\n{0}Number of columns in train data for true positive: {1}{3}\".format(y_,r_,tp_train.shape[0],tp_train.shape[1]))\nprint(\"{0}Number of rows in train data: {1}{2}\\n{0}Number of columns in train data for false positive: {1}{3}\".format(m_,r_,fp_train.shape[0],fp_train.shape[1]))\n\n","0a81bce0":"fp_train.head()","70989812":"tp_train.head()","f0165bd9":"sub.head()","450fdb87":"print(f\"{m_}number of unique species: {r_}{tp_train.species_id.nunique()}\")\nprint(f\"{c_}number of unique records: {r_}{tp_train.recording_id.nunique()}\")\nprint(f\"{y_}number of unique song ids: {r_}{tp_train.songtype_id.nunique()}\")","7402c783":"g = sns.countplot(data=tp_train, x = \"species_id\")\nplt.title(\"record count per species\")\nfor p in g.patches:\n    g.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()","b8fb83d9":"g = sns.countplot(data=tp_train, x=\"songtype_id\")\nplt.title(\"song ids\")\nfor p in g.patches:\n    g.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()","1e444663":"rec_dict = {}\n\nfor spid in tp_train[\"species_id\"].unique().tolist():\n    rec_id = tp_train.loc[np.where(tp_train[\"species_id\"] == spid)][\"recording_id\"].tolist()[0]\n#     print(f\"{spid} : {rec_id}\")\n    rec_dict[spid] = rec_id\n    fig, ax = plt.subplots()\n    rec_path = Path(train_path) \/ f\"{rec_id}.flac\"\n    wf, sr = torchaudio.load(rec_path)\n    labels = []\n    handles = [mpl_patches.Rectangle((0, 0), 1, 1, fc=\"white\", ec=\"white\", lw=0, alpha=0)] * 4 \n    labels.append(f\"label = {spid}\")\n    labels.append(f\"min of waveform: {np.around(wf.min(), 5)}\")\n    labels.append(f\"max of waveform: {np.around(wf.max(), 5)}\")\n    labels.append(f\"mean of waveform: {np.around(wf.mean(), 5)}\")\n    ax.legend(handles, labels, loc='best', fontsize='large', \n          fancybox=True, framealpha=0.7, \n          handlelength=0, handletextpad=0)\n    \n    plt.plot(wf.t().numpy())\n    plt.title(\"species wave form plot\")\n    plt.show()","8765b6ee":"rec_dict = {}\ndisplay(HTML(f'<span style=\"color:green\"> <h3>Sound of each species <\/h3><\/span>'))\ndef get_species_audio():\n    \"\"\"\n    get audio image for each of the species\n    \"\"\"\n    for spid in tp_train[\"species_id\"].unique().tolist():\n        rec_id = tp_train.loc[np.where(tp_train[\"species_id\"] == spid)][\"recording_id\"].tolist()[0]\n    #     print(f\"{spid} : {rec_id}\")\n        rec_dict[spid] = rec_id\n        rec_path = Path(train_path) \/ f\"{rec_id}.flac\"\n        wf, sr = torchaudio.load(rec_path)\n        display(HTML(f'<span style=\"color:green\"> species id: {spid} <\/span>'))\n        ipd.display(ipd.Audio(data=wf, rate=sr))\nget_species_audio()","e53309ad":"def get_melspectrogram_db(file_path, sr=None, n_fft=2048, hop_length=512, n_mels=128, fmin=20, fmax=8300, top_db=80):\n    wav,sr = librosa.load(file_path,sr=sr)\n    if wav.shape[0]<5*sr:\n        wav=np.pad(wav,int(np.ceil((5*sr-wav.shape[0])\/2)),mode='reflect')\n    else:\n        wav=wav[:5*sr]\n    spec=librosa.feature.melspectrogram(wav, sr=sr, n_fft=n_fft,\n              hop_length=hop_length,n_mels=n_mels,fmin=fmin,fmax=fmax)\n    spec_db=librosa.power_to_db(spec,top_db=top_db)\n    return spec_db","a7b1fecd":"rec_dict = {}\ndisplay(HTML(f'<span style=\"color:green\"> <h3>spectrogram of each species <\/h3><\/span>'))\ndef get_species_spectrogram():\n    \"\"\"\n    get audio image for each of the species\n    \"\"\"\n    for spid in tp_train[\"species_id\"].unique().tolist():\n        rec_id = tp_train.loc[np.where(tp_train[\"species_id\"] == spid)][\"recording_id\"].tolist()[0]\n        rec_dict[spid] = rec_id\n        rec_path = Path(train_path) \/ f\"{rec_id}.flac\"\n        wf, sr = torchaudio.load(rec_path)\n        specgram = torchaudio.transforms.Spectrogram()(wf)\n\n        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 4))\n        ax.tick_params(axis='x', labelsize=10)\n        ax.tick_params(axis='y', labelsize=10)\n\n        cax = ax.matshow(\n            specgram.log2()[0,:,:].numpy(),\n            interpolation=\"nearest\",\n            aspect=\"auto\",\n            cmap=plt.cm.afmhot,\n            origin=\"lower\",\n        )\n        fig.colorbar(cax)\n\n        plt.title(f\"spectrogram for species id: {spid}\")\n        plt.show()\nget_species_spectrogram()","614a869e":"rec_dict = {}\ndisplay(HTML(f'<span style=\"color:green\"> <h3>mel spectrogram of each species <\/h3><\/span>'))\ndef get_species_melspectrogram():\n    \"\"\"\n    get audio image for each of the species\n    \"\"\"\n    for spid in tp_train[\"species_id\"].unique().tolist():\n        rec_id = tp_train.loc[np.where(tp_train[\"species_id\"] == spid)][\"recording_id\"].tolist()[0]\n        rec_dict[spid] = rec_id\n        rec_path = Path(train_path) \/ f\"{rec_id}.flac\"\n        wf, sr = torchaudio.load(rec_path)\n        specgram = torchaudio.transforms.MelSpectrogram()(wf)\n\n        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 4))\n        ax.tick_params(axis='x', labelsize=10)\n        ax.tick_params(axis='y', labelsize=10)\n\n        cax = ax.matshow(\n            specgram.log2()[0,:,:].numpy(),\n            interpolation=\"nearest\",\n            aspect=\"auto\",\n            cmap=plt.cm.afmhot,\n            origin=\"lower\",\n        )\n        fig.colorbar(cax)\n\n        plt.title(f\"mel spectrogram for species id: {spid}\")\n        plt.show()\nget_species_melspectrogram()","b8537f0e":"def spec_to_image(spec, eps=1e-6):\n    mean = spec.mean()\n    std = spec.std()\n    spec_norm = (spec - mean) \/ (std + eps)\n    spec_min, spec_max = spec_norm.min(), spec_norm.max()\n    spec_scaled = 255 * (spec_norm - spec_min) \/ (spec_max - spec_min)\n    spec_scaled = spec_scaled.astype(np.uint8)\n    return spec_scaled","10b0d344":"class RCFDataset(Dataset):\n    \"\"\" prepare data loader for training model\"\"\"\n    def __init__(self, base, df, in_col, out_col):\n        self.df = df\n        self.data = []\n        self.labels = []\n        self.c2i={}\n        self.i2c={}\n        self.categories = sorted(df[out_col].unique())\n        for i, category in enumerate(self.categories):\n            self.c2i[category]=i\n            self.i2c[i]=category\n        for ind in tqdm(range(len(df))):\n            row = df.iloc[ind]\n            file_path = os.path.join(base,row[in_col]+ \".flac\")\n            self.data.append(spec_to_image(get_melspectrogram_db(file_path))[np.newaxis,...])\n            self.labels.append(self.c2i[row[out_col]])\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n# train_data = RCFDataset(train_path.as_posix(), tp_train, 'recording_id', 'species_id')\n# valid_data = RCFDataset(train_path.as_posix(), tp_train, 'recording_id', 'species_id')\n# train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n# valid_loader = DataLoader(valid_data, batch_size=16, shuffle=True)","e88c0c1f":"if torch.cuda.is_available():\n    device=torch.device('cuda')\nelse:\n    device=torch.device('cpu')","b9aa7cea":"\nresnet_model = resnet34(pretrained=True)\n# resnet_model.fc = nn.Linear(512,50)\n# resnet_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)","95abd4a5":"learning_rate = 2e-4\noptimizer = optim.Adam(resnet_model.parameters(), lr=learning_rate)\nepochs = 50\nloss_fn = nn.CrossEntropyLoss()\nresnet_train_losses=[]\nresnet_valid_losses=[]\ndef lr_decay(optimizer, epoch):\n    if epoch%10==0:\n        new_lr = learning_rate \/ (10**(epoch\/\/10))\n        optimizer = setlr(optimizer, new_lr)\n        print(f'Changed learning rate to {new_lr}')\n    return optimizer\ndef train(model, loss_fn, train_loader, valid_loader, epochs, optimizer, train_losses, valid_losses, change_lr=None):\n    for epoch in tqdm(range(1,epochs+1)):\n        model.train()\n        batch_losses=[]\n        if change_lr:\n            optimizer = change_lr(optimizer, epoch)\n        for i, data in enumerate(train_loader):\n            x, y = data\n            optimizer.zero_grad()\n            x = x.to(device, dtype=torch.float32)\n            y = y.to(device, dtype=torch.long)\n            y_hat = model(x)\n            loss = loss_fn(y_hat, y)\n            loss.backward()\n            batch_losses.append(loss.item())\n            optimizer.step()\n    train_losses.append(batch_losses)\n    print(f'Epoch - {epoch} Train-Loss : {np.mean(train_losses[-1])}')\n    model.eval()\n    batch_losses=[]\n    trace_y = []\n    trace_yhat = []\n    for i, data in enumerate(valid_loader):\n        x, y = data\n        x = x.to(device, dtype=torch.float32)\n        y = y.to(device, dtype=torch.long)\n        y_hat = model(x)\n        loss = loss_fn(y_hat, y)\n        trace_y.append(y.cpu().detach().numpy())\n        trace_yhat.append(y_hat.cpu().detach().numpy())      \n        batch_losses.append(loss.item())\n    valid_losses.append(batch_losses)\n    trace_y = np.concatenate(trace_y)\n    trace_yhat = np.concatenate(trace_yhat)\n    accuracy = np.mean(trace_yhat.argmax(axis=1)==trace_y)\n    print(f'Epoch - {epoch} Valid-Loss : {np.mean(valid_losses[-1])} Valid-Accuracy : {accuracy}')\n# train(resnet_model, loss_fn, train_loader, valid_loader, epochs, optimizer, resnet_train_losses, resnet_valid_losses, lr_decay)","fa8154d5":"<center style=\"background-color:yellow\"> descriptive stats <\/center>","52660534":"<h1 align=\"center\" style=\"background-color:powderblue;\" style=\"font-size:300%;\"> Rainforest Connection Species Audio Detection <\/h1>","5719fa5a":"### data loader","ee6a5615":"# Short-Time Fourier Transform (STFT)","f0d84b97":"<center style=\"background-color:yellow\"> Load Libraries <\/center>","1179028c":"<center style=\"background-color:yellow\"> load data <\/center>","1f08cfd7":"<center style=\"background-color:yellow\"> column description <\/center>\n\n- *recording_id* - unique identifier for recording\n- *species_id* - unique identifier for species\n- *songtype_id* - unique identifier for songtype\n- *t_min* - start second of annotated signal\n- *f_min* - lower frequency of annotated signal\n- *t_max* - end second of annotated signal\n- *f_max* - upper frequency of annotated signal\n- *is_tp* - [tfrecords only] an indicator of whether the label is from the train_tp (1) or train_fp (0) file.","9824a273":"### Modeling","6979f20f":"<center style=\"background-color:yellow\"> audio transformation <\/center>","4822d7cf":"<h1 align=\"center\"  style=\"background-color:yellow;\" style=\"font-family:verdana;\"> \u2b06\ufe0f\u2b06\ufe0f\u2b06\ufe0f If you find this note book helpful. <b>please upvote!<\/b> \u2b06\ufe0f\u2b06\ufe0f\u2b06\ufe0f <\/h1>","bbcc9453":"<img src=\"https:\/\/i.pinimg.com\/originals\/2e\/c1\/60\/2ec1606caa1b9451e8d4dcfe62c5849e.jpg\" align=\"center\"> <\/img>"}}