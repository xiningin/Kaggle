{"cell_type":{"34f98ee9":"code","0c0dc57c":"code","0042fffe":"code","d5089bf2":"code","9460fdea":"code","10797441":"code","858d60c0":"code","15c8a8c4":"code","b07cc440":"code","f5c4fcdc":"code","fa95b34b":"code","e93796e3":"code","70bece2c":"code","398cc128":"code","dabbb89d":"code","8268e481":"code","21676c59":"code","a786cb3b":"code","9e9aa0c1":"code","eb811772":"code","d13e7021":"code","c056c4e9":"code","72d77f2e":"code","b535f490":"code","07114210":"code","9fe2f085":"code","32914b20":"code","99675b89":"code","28c43d6b":"code","90975c57":"code","0f916fe0":"code","6d2abb13":"code","cf91c019":"code","24feaf74":"code","6eb19da9":"code","55f90aae":"code","42579b57":"code","f83aea6d":"code","d5880f3e":"code","d1663ba2":"code","112a823f":"code","3433fc1b":"code","b4f5843d":"code","573b352e":"code","04cae232":"markdown","fa0bb828":"markdown","ba928a5c":"markdown","5436802a":"markdown","6ae03dd4":"markdown","2a2853f5":"markdown","f8d99268":"markdown","d5a1eb0d":"markdown"},"source":{"34f98ee9":"#importing packages\n%matplotlib inline\nimport scipy.stats as stats\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')","0c0dc57c":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","0042fffe":"#shape\nprint('This data frame has {} rows and {} columns.'.format(df.shape[0], df.shape[1]))","d5089bf2":"#peek at data\ndf.sample(5)","9460fdea":"#info\ndf.info()","10797441":"#numerical summary -> only non-anonymized columns of interest\npd.set_option('precision', 3)\ndf.loc[:, ['Time', 'Amount']].describe()","858d60c0":"#visualizations of time and amount\nplt.figure(figsize=(10,8))\nplt.title('Distribution of Time Feature')\nsns.distplot(df.Time)","15c8a8c4":"plt.figure(figsize=(10,8))\nplt.title('Distribution of Monetary Value Feature')\nsns.distplot(df.Amount)","b07cc440":"#fraud vs. normal transactions \ncounts = df.Class.value_counts()\nnormal = counts[0]\nfraudulent = counts[1]\nperc_normal = (normal\/(normal+fraudulent))*100\nperc_fraudulent = (fraudulent\/(normal+fraudulent))*100\nprint('There were {} non-fraudulent transactions ({:.3f}%) and {} fraudulent transactions ({:.3f}%).'.format(normal, perc_normal, fraudulent, perc_fraudulent))","f5c4fcdc":"plt.figure(figsize=(8,6))\nsns.barplot(x=counts.index, y=counts)\nplt.title('Count of Fraudulent vs. Non-Fraudulent Transactions')\nplt.ylabel('Count')\nplt.xlabel('Class (0:Non-Fraudulent, 1:Fraudulent)')","fa95b34b":"corr = df.corr()\ncorr","e93796e3":"#heatmap\ncorr = df.corr()\nplt.figure(figsize=(12,10))\nheat = sns.heatmap(data=corr)\nplt.title('Heatmap of Correlation')","70bece2c":"#skewness\nskew_ = df.skew()\nskew_","398cc128":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler2 = StandardScaler()\n#scaling time\nscaled_time = scaler.fit_transform(df[['Time']])\nflat_list1 = [item for sublist in scaled_time.tolist() for item in sublist]\nscaled_time = pd.Series(flat_list1)","dabbb89d":"#scaling the amount column\nscaled_amount = scaler2.fit_transform(df[['Amount']])\nflat_list2 = [item for sublist in scaled_amount.tolist() for item in sublist]\nscaled_amount = pd.Series(flat_list2)","8268e481":"#concatenating newly created columns w original df\ndf = pd.concat([df, scaled_amount.rename('scaled_amount'), scaled_time.rename('scaled_time')], axis=1)\ndf.sample(5)","21676c59":"#dropping old amount and time columns\ndf.drop(['Amount', 'Time'], axis=1, inplace=True)","a786cb3b":"#manual train test split using numpy's random.rand\nmask = np.random.rand(len(df)) < 0.9\ntrain = df[mask]\ntest = df[~mask]\nprint('Train Shape: {}\\nTest Shape: {}'.format(train.shape, test.shape))","9e9aa0c1":"train.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)","eb811772":"#how many random samples from normal transactions do we need?\nno_of_frauds = train.Class.value_counts()[1]\nprint('There are {} fraudulent transactions in the train data.'.format(no_of_frauds))","d13e7021":"#randomly selecting 442 random non-fraudulent transactions\nnon_fraud = train[train['Class'] == 0]\nfraud = train[train['Class'] == 1]","c056c4e9":"selected = non_fraud.sample(no_of_frauds)\nselected.head()","72d77f2e":"#concatenating both into a subsample data set with equal class distribution\nselected.reset_index(drop=True, inplace=True)\nfraud.reset_index(drop=True, inplace=True)","b535f490":"subsample = pd.concat([selected, fraud])\nlen(subsample)","07114210":"#shuffling our data set\nsubsample = subsample.sample(frac=1).reset_index(drop=True)\nsubsample.head(10)","9fe2f085":"new_counts = subsample.Class.value_counts()\nplt.figure(figsize=(8,6))\nsns.barplot(x=new_counts.index, y=new_counts)\nplt.title('Count of Fraudulent vs. Non-Fraudulent Transactions In Subsample')\nplt.ylabel('Count')\nplt.xlabel('Class (0:Non-Fraudulent, 1:Fraudulent)')","32914b20":"#taking a look at correlations once more\ncorr = subsample.corr()\ncorr = corr[['Class']]\ncorr","99675b89":"#negative correlations smaller than -0.5\ncorr[corr.Class < -0.5]","28c43d6b":"#positive correlations greater than 0.5\ncorr[corr.Class > 0.5]","90975c57":"#visualizing the features w high negative correlation\nf, axes = plt.subplots(nrows=2, ncols=4, figsize=(26,16))\n\nf.suptitle('Features With High Negative Correlation', size=35)\nsns.boxplot(x=\"Class\", y=\"V3\", data=subsample, ax=axes[0,0])\nsns.boxplot(x=\"Class\", y=\"V9\", data=subsample, ax=axes[0,1])\nsns.boxplot(x=\"Class\", y=\"V10\", data=subsample, ax=axes[0,2])\nsns.boxplot(x=\"Class\", y=\"V12\", data=subsample, ax=axes[0,3])\nsns.boxplot(x=\"Class\", y=\"V14\", data=subsample, ax=axes[1,0])\nsns.boxplot(x=\"Class\", y=\"V16\", data=subsample, ax=axes[1,1])\nsns.boxplot(x=\"Class\", y=\"V17\", data=subsample, ax=axes[1,2])\nf.delaxes(axes[1,3])","0f916fe0":"#visualizing the features w high positive correlation\nf, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,9))\n\nf.suptitle('Features With High Positive Correlation', size=20)\nsns.boxplot(x=\"Class\", y=\"V4\", data=subsample, ax=axes[0])\nsns.boxplot(x=\"Class\", y=\"V11\", data=subsample, ax=axes[1])","6d2abb13":"#Only removing extreme outliers\nQ1 = subsample.quantile(0.25)\nQ3 = subsample.quantile(0.75)\nIQR = Q3 - Q1\n\ndf2 = subsample[~((subsample < (Q1 - 2.5 * IQR)) |(subsample > (Q3 + 2.5 * IQR))).any(axis=1)]","cf91c019":"len_after = len(df2)\nlen_before = len(subsample)\nlen_difference = len(subsample) - len(df2)\nprint('We reduced our data size from {} transactions by {} transactions to {} transactions.'.format(len_before, len_difference, len_after))","24feaf74":"from sklearn.manifold import TSNE\n\nX = df2.drop('Class', axis=1)\ny = df2['Class']","6eb19da9":"#t-SNE\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)","55f90aae":"# t-SNE scatter plot\nimport matplotlib.patches as mpatches\n\nf, ax = plt.subplots(figsize=(24,16))\n\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\nax.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax.set_title('t-SNE', fontsize=14)\n\nax.grid(True)\n\nax.legend(handles=[blue_patch, red_patch])","42579b57":"def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn","f83aea6d":"# train test split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=77)","d5880f3e":"X_train = X_train.values\nX_validation = X_test.values\ny_train = y_train.values\ny_validation = y_test.values","d1663ba2":"print('X_shapes:\\n', 'X_train:', 'X_validation:\\n', X_train.shape, X_validation.shape, '\\n')\nprint('Y_shapes:\\n', 'Y_train:', 'Y_validation:\\n', y_train.shape, y_validation.shape)","112a823f":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier","3433fc1b":"\n\n##Spot-Checking Algorithms\n\nmodels = []\n\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('SVM', SVC()))\nmodels.append(('XGB', XGBClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\n\n#testing models\n\nresults = []\nnames = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=42)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='roc_auc')\n    results.append(cv_results)\n    names.append(name)\n    msg = '%s: %f (%f)' % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n","b4f5843d":"\n\n#Compare Algorithms\n\nfig = plt.figure(figsize=(12,10))\nplt.title('Comparison of Classification Algorithms')\nplt.xlabel('Algorithm')\nplt.ylabel('ROC-AUC Score')\nplt.boxplot(results)\nax = fig.add_subplot(111)\nax.set_xticklabels(names)\nplt.show()\n\n","573b352e":"#visualizing RF\nmodel = RandomForestClassifier(n_estimators=10)\n\n# Train\nmodel.fit(X_train, y_train)\n# Extract single tree\nestimator = model.estimators_[5]\n\nfrom sklearn.tree import export_graphviz\n# Export as dot file\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = X.columns.tolist(),\n                class_names = ['0',' 1'],\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\n# Convert to png using system command (requires Graphviz)\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'tree.png')","04cae232":"Around 88 dollars is the mean of all credit card transactions in this data set. The biggest transaction had a monetary value of around 25,691 dollars","fa0bb828":"# Scaling Amount and Time","ba928a5c":"# Splitting Data into Train and Test","5436802a":"# Dimensionality Reduction","6ae03dd4":"# Extreme Outlier Removal\n","2a2853f5":"# Conclusion & Future Work\n\nFraud detection is a complex issue that requires a substantial amount of planning before throwing machine learning algorithms at it. Nonetheless, it is also an application of data science and machine learning for the good, which makes sure that the customer\u2019s money is safe and not easily tampered with.\n\nFuture work will include a comprehensive tuning of the Random Forest algorithm I talked about earlier. Having a data set with non-anonymized features would make this particularly interesting as outputting the feature importance would enable one to see what specific factors are most important for detecting fraudulent transactions.","f8d99268":"# Creating a subsample data set with balanced class distributions","d5a1eb0d":"# Classification Algorithms"}}