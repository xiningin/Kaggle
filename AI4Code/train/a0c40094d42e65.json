{"cell_type":{"9e55076f":"code","060eedbd":"code","e2cb8077":"code","f30420fd":"code","3b133552":"code","27990b56":"code","21542b1e":"code","3d7bbada":"code","c1d5e3f6":"code","44c50390":"code","49835653":"code","29e45d65":"code","a4b9c8bf":"code","ee012dee":"code","2da33cd9":"code","b5c2acbd":"code","533e5afe":"code","c0a661b4":"code","2da7d264":"code","cdfc6a5e":"code","cb23119e":"code","634143be":"code","d88af1cd":"code","abadaab5":"code","8e3b95ef":"code","adbdefda":"code","50d26df7":"code","4767311e":"code","a4fedf45":"code","0439d5f3":"code","25cd8374":"code","7ce17c34":"code","0bba4349":"code","6a7f138f":"code","220a5543":"code","e03f70e5":"markdown"},"source":{"9e55076f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","060eedbd":"import numpy as np\n\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100,1)","e2cb8077":"X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\ntheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)","f30420fd":"theta_best","3b133552":"X_new = np.array([[0], [2]])\nX_new_b = np.c_[np.ones((2,1)), X_new]\ny_predict = X_new_b.dot(theta_best)","27990b56":"import matplotlib.pyplot as plt\n\n\nplt.plot(X_new, y_predict, \"r-\")\nplt.plot(X, y, \"b.\")\nplt.axis([0, 2, 0, 15])\nplt.show()\n","21542b1e":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg.intercept_, lin_reg.coef_","3d7bbada":"lin_reg.predict(X_new)","c1d5e3f6":"theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\ntheta_best_svd","44c50390":"np.linalg.pinv(X_b).dot(y)","49835653":"eta = 0.1  # learning rate\nn_iterations = 1000\nm = 100\n\ntheta = np.random.randn(2,1)  # random initialization\n\nfor iteration in range(n_iterations):\n    gradients = 2\/m * X_b.T.dot(X_b.dot(theta) - y)\n    theta = theta - eta * gradients","29e45d65":"theta","a4b9c8bf":"n_epochs = 50\nt0, t1 = 5, 50  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 \/ (t + t1)\n\ntheta = np.random.randn(2,1)  # random initialization\n\nfor epoch in range(n_epochs):\n    for i in range(m):\n        random_index = np.random.randint(m)\n        xi = X_b[random_index:random_index+1]\n        yi = y[random_index:random_index+1]\n        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n        eta = learning_schedule(epoch * m + i)\n        theta = theta - eta * gradients","ee012dee":"theta","2da33cd9":"from sklearn.linear_model import SGDRegressor\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\nsgd_reg.fit(X, y.ravel())","b5c2acbd":"sgd_reg.intercept_, sgd_reg.coef_","533e5afe":"m = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n","c0a661b4":"from sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)","2da7d264":"X[0]","cdfc6a5e":"X_poly[0]","cb23119e":"lin_reg = LinearRegression()\nlin_reg.fit(X_poly, y)\nlin_reg.intercept_, lin_reg.coef_","634143be":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\ndef plot_learning_curves(model, X, y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n    train_errors, val_errors = [], []\n    for m in range(1, len(X_train)):\n        model.fit(X_train[:m], y_train[:m])\n        y_train_predict = model.predict(X_train[:m])\n        y_val_predict = model.predict(X_val)\n        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n        val_errors.append(mean_squared_error(y_val, y_val_predict))\n    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")","d88af1cd":"lin_reg = LinearRegression()\nplot_learning_curves(lin_reg, X, y)","abadaab5":"from sklearn.pipeline import Pipeline\n\npolynomial_regression = Pipeline([\n        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n        (\"lin_reg\", LinearRegression()),\n    ])\n\nplot_learning_curves(polynomial_regression, X, y)","8e3b95ef":"from sklearn.linear_model import Ridge\nridge_reg = Ridge(alpha=1, solver=\"cholesky\")\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])","adbdefda":"sgd_reg = SGDRegressor(penalty=\"l2\")\nsgd_reg.fit(X, y.ravel())\nsgd_reg.predict([[1.5]])","50d26df7":"from sklearn.linear_model import Lasso\nlasso_reg = Lasso(alpha=0.1)\nlasso_reg.fit(X,y)\nlasso_reg.predict([[1.5]])","4767311e":"from sklearn.linear_model import ElasticNet\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\nelastic_net.fit(X, y)\nelastic_net.predict([[1.5]])","a4fedf45":"from sklearn import datasets\niris = datasets.load_iris()\nlist(iris.keys())","0439d5f3":"X = iris[\"data\"][:, 3:]\ny = (iris[\"target\"] == 2).astype(np.int)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)","25cd8374":"X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\ny_proba = log_reg.predict_proba(X_new)\nplt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris virginica\")\nplt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris virginica\")\n# + more Matplotlib code to make the image look pretty","7ce17c34":"log_reg.predict([[1.7], [1.5]])","0bba4349":"X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\ny = iris[\"target\"]\n\nsoftmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)\nsoftmax_reg.fit(X, y)","6a7f138f":"softmax_reg.predict([[5, 2]])","220a5543":"softmax_reg.predict_proba([[5, 2]])","e03f70e5":"THIS IS FOLLOWING CHAPTER 4 OF Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition\n\n"}}