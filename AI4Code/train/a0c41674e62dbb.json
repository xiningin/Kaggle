{"cell_type":{"9b46b872":"code","76dd471b":"code","21953863":"code","1cd4bddd":"code","2caa7725":"code","890f3c51":"code","f759c15c":"code","7d72874c":"code","99358c05":"code","b1ae7987":"code","946a5372":"code","3a60134b":"code","1df05306":"code","00b75bd9":"code","bc765167":"code","2080f953":"code","7163943d":"code","ecd54c2e":"code","da0eb87d":"code","cb050406":"code","4756d4db":"code","49c95c53":"code","0fb20a08":"code","a1ab7bf8":"code","ce2706ec":"markdown","4edc66d9":"markdown","bfbf6e02":"markdown","7896ad87":"markdown","3549de65":"markdown","9e030295":"markdown","b487fa20":"markdown","48d08d11":"markdown","a2d41fd1":"markdown","0742dcbc":"markdown","54cb70c8":"markdown","21f66e08":"markdown","1ba2a8c7":"markdown","3e6a4820":"markdown","3d9c9707":"markdown","0407ba49":"markdown"},"source":{"9b46b872":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","76dd471b":"df = pd.read_csv('..\/input\/breast-cancer-prediction-dataset\/Breast_cancer_data.csv')","21953863":"df.head()","1cd4bddd":"features = df.drop(['diagnosis'], axis = 1)\ntarget = df['diagnosis']","2caa7725":"m = len(target)\nn = features.shape[1]\nX = np.ones([m, n+1])\nX[:, 1:] = features\ny = np.array(target).reshape(-1, 1)\nprint(X.shape, y.shape)","890f3c51":"plt.figure(figsize = (16, 8))\nplt.scatter(X[:, 1], y)\nplt.xlabel('Mean Radius')\nplt.ylabel('Diagnosis')","f759c15c":"theta = np.array([-10, 0.1, 0.1, 0.01, 0.001, 10]).reshape(-1, 1)\nz = np.dot(X, theta)\npred = 1 \/ (1 + np.exp(-z))\npred.shape","7d72874c":"print(pred[:5], y[:5])","99358c05":"def cost_function(X, y, theta):\n    m = len(y)\n    z = np.dot(X, theta)\n    pred = 1 \/ (1 + np.exp(-z))\n    J = (-1) * np.sum(np.dot(np.transpose(y), np.log(pred)) + np.dot(np.transpose(1 - y), np.log(1 - pred))) \/ m\n    return J","b1ae7987":"cost_function(X, y, theta)","946a5372":"def gradient_descent(X, y, theta, alpha, iters):\n    m = len(y)\n    for i in range(iters):\n        z = np.dot(X, theta)\n        pred = 1 \/ (1 + np.exp(-z))\n        theta = theta - np.dot(np.transpose(X), (pred - y)) * (alpha \/ m)\n        J = cost_function(X, y, theta)\n    return J, theta","3a60134b":"J_min, theta_min = gradient_descent(X, y, theta, 0.1, 700)","1df05306":"theta_min","00b75bd9":"z = np.dot(X, theta_min)\nbest_preds = 1 \/ (1 + np.exp(-z))\nfor i in range(len(y)):\n    if best_preds[i] <= 0.5:\n        best_preds[i] = 0\n    else:\n        best_preds[i] = 1\naccuracy_score(y, best_preds)","bc765167":"logreg = LogisticRegression().fit(features, target)\nsk_preds = logreg.predict(features)\naccuracy_score(target, sk_preds)","2080f953":"logreg.coef_","7163943d":"logreg.intercept_","ecd54c2e":"sk_theta = np.ones([6, 1])\nsk_theta[0, 0] = logreg.intercept_[0]\nsk_theta[1:, 0] = logreg.coef_[0]","da0eb87d":"df0 = df.loc[df['diagnosis'] == 0]\ndf1 = df.loc[df['diagnosis'] == 1]\nprint(\"zero: \" + str(df0.shape[0]) + \", one: \" + str(df1.shape[0]))\nplt.figure(figsize = (16, 10))\nplt.scatter(df0['mean_radius'], df0['mean_area'], label = '0')\nplt.scatter(df1['mean_radius'], df1['mean_area'], label = '1')\nplt.xlabel(\"Mean Radius\")\nplt.ylabel(\"Mean Area\")\nplt.legend()","cb050406":"plt.figure(figsize = (12, 6))\nplt.scatter(X[:, 1], y, label = 'original')\nplt.scatter(X[:, 1], best_preds, label = 'Logistic Regression using Gradient Descent')\nplt.xlabel('Mean Radius')\nplt.ylabel('Diagnosis')\nplt.legend()\nplt.figure(figsize = (12, 6))\nplt.scatter(X[:, 1], y, label = 'original')\nplt.scatter(X[:, 1], sk_preds, label = 'Logistic Regression using sklearn', color = 'g')\nplt.xlabel('Mean Radius')\nplt.ylabel('Diagnosis')\nplt.legend()","4756d4db":"mat = np.dot(np.transpose(X), X)\ninv = np.linalg.inv(mat)\nk = np.dot(np.transpose(X), y)\ntheta_m = np.dot(inv, k)","49c95c53":"preds = np.dot(X, theta_m)\nfor i in range(len(y)):\n    if preds[i] <= 0.5:\n        preds[i] = 0\n    else:\n        preds[i] = 1\naccuracy_score(y, preds)","0fb20a08":"from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression().fit(X, y)\nlin_preds = linreg.predict(X)\nfor i in range(len(y)):\n    if lin_preds[i] <= 0.5:\n        lin_preds[i] = 0\n    else:\n        lin_preds[i] = 1\naccuracy_score(y, preds)","a1ab7bf8":"plt.figure(figsize = (12, 6))\nplt.scatter(X[:, 1], y, label = 'original')\nplt.scatter(X[:, 1], preds, label = 'Linear Regression using Normal Equation')\nplt.xlabel('Mean Radius')\nplt.ylabel('Diagnosis')\nplt.legend()\nplt.figure(figsize = (12, 6))\nplt.scatter(X[:, 1], y, label = 'original')\nplt.scatter(X[:, 1], sk_preds, label = 'Logistic Regression using sklearn', color = 'g')\nplt.xlabel('Mean Radius')\nplt.ylabel('Diagnosis')\nplt.legend()","ce2706ec":"### Comapring Linear Regression & sklearn Logistic Regression","4edc66d9":"## Visualizing distribution of classes","bfbf6e02":"### Tuning parameters alpha & number of iterations for getting best possible prediction accuracy","7896ad87":"It turns out that linear regression is performing better than sklearn Logistic Regression.\n\n***Hence the best accuracy was achieved by Linear Regression(comparitively between Linear & Logistic Regression) where both were implemented by writing custom code from scratch.***","3549de65":"## Comparing Gradient Descent & sklearn Logistic Regression","9e030295":"# Logistic Regression with Gradient Descent","b487fa20":"# sklearn LogisticRegression","48d08d11":"### Calculating prediction accuracy","a2d41fd1":"## Cost Function","0742dcbc":"**According to the above visuals, it can be observed that the lag for gradient descent is occuring in predicting zeros, everything else is just fine, which may be because of the regularisation that is applied by default in sklearn LogisticRegression or may be gradient descent is not giving us global minimum, instead its giving a local minimum based on the initial values of weights(i.e. theta).**\n\nBasically the aim of this notebook was to implement ***Logistic Regression from Scratch*** which is now achieved.","54cb70c8":"# Linear Regression using Normal Equation\n***theta = (X<sup>T<\/sup>X)<sup>-1<\/sup>X<sup>T<\/sup>y***","21f66e08":"## Visualizing distribution of the data","1ba2a8c7":"# sklearn Linear Regression","3e6a4820":"*Linear Regression using Normal Equation just gives the exact same result as sklearn Linear Regression.*\n\nBasically the above two same results tells us that our implementation of Linear Regression using Normal Equation is just fine.","3d9c9707":"## Prediction for Logistic Regression","0407ba49":"# Gradient Descent"}}