{"cell_type":{"6b8a7c1f":"code","06152ab8":"code","0268e9a3":"code","0c39806b":"code","3d97c78e":"code","fd32f0d4":"code","09aa5a70":"code","f4ff2914":"code","0e0fcbb5":"code","75880e57":"code","adc78c30":"code","9d4c62f5":"code","8bce7c5c":"code","e64c2d19":"code","76e79b48":"code","9b4a27fc":"code","562b7c7b":"code","1a08e983":"code","03bc43a1":"code","c895f73c":"code","ca3b8ed0":"code","5ead3d64":"code","311494bb":"code","81d0aae3":"code","c6548faa":"code","a3c2912b":"code","2231a5e6":"code","952a962c":"code","5d0f15e8":"markdown"},"source":{"6b8a7c1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn')\nsns.set(font_scale=2.5)\n\nimport missingno as msno\nfrom sklearn.preprocessing import LabelEncoder\n\n#ignore warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inlinethe current session","06152ab8":"df = pd.read_csv('..\/input\/advertising\/advertising.csv')\ndf.head()","0268e9a3":"for col in df.columns : \n    msg = 'columnn : {:>10}\\t count of NaN value : {:.0f}'.format(col, 100 * (df[col].isnull().sum() ))\n    print(msg)","0c39806b":"df.info()","3d97c78e":"int_column = df.dtypes[df.dtypes =='int64'].index |  df.dtypes[df.dtypes =='float64'].index","fd32f0d4":"df.describe()","09aa5a70":"for col in int_column : \n    plt.figure(figsize=(12,4))\n    \n    plt.subplot(1,2,1)\n    sns.distplot(df[col])\n    plt.xlabel(col)\n    plt.ylabel('Density')\n    \n    plt.subplot(1,2,2)\n    sns.boxplot(x='Clicked on Ad', y = col, data =df, showmeans = True)\n    plt.xlabel('Target')\n    plt.ylabel(col)\n    \n    plt.show()","f4ff2914":"obj_column = df.dtypes[df.dtypes == 'object'].index\nfor i in range(0, len(obj_column)) :\n    print(obj_column[i])\n    print(len(df[obj_column[i]].unique()))\n    print()","0e0fcbb5":"df.head()","75880e57":"plt.style.use(\"fivethirtyeight\")\nsns.jointplot(df[\"Daily Time Spent on Site\"], df.Age, kind='kde')","adc78c30":"sns.pairplot(df, hue='Clicked on Ad')","9d4c62f5":"plt.figure(figsize=(12, 8))\nsns.heatmap(df.corr(), annot=True)","8bce7c5c":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","e64c2d19":"df","76e79b48":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import train_test_split","9b4a27fc":"X = df.drop(['Timestamp', 'Clicked on Ad', 'Ad Topic Line', 'Country', 'City'], axis=1)\ny = df['Clicked on Ad']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\nnum_columns = ['Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage', 'Male']\n\n\nct = make_column_transformer(\n    (MinMaxScaler(), num_columns),\n#     (StandardScaler(), num_columns),\n    remainder='passthrough'\n)\n\nX_train = ct.fit_transform(X_train)\nX_test = ct.transform(X_test)\nX_train = pd.DataFrame(X_train, columns=num_columns)\nX_test = pd.DataFrame(X_test, columns=num_columns)","562b7c7b":"from sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression(solver='liblinear')\nlr_clf.fit(X_train, y_train)\n\nprint_score(lr_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(lr_clf, X_train, y_train, X_test, y_test, train=False)","1a08e983":"from sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n\n\nmodels = [LogisticRegression(),\n          RandomForestClassifier(),\n          XGBClassifier()\n         ]\n\nnames = [ 'LogisticRegression',\n          'RandomForestClassifier',\n          'XGBClassifier'\n        ]\n\nfor model,name in zip(models,names):\n  m = model.fit(X_train,y_train)\n  print(name, 'report:')\n  print('Train score',model.score(X_train,y_train))\n  print('Test score',model.score(X_test,y_test))\n  print()\n  print(\"Train confusion matrix:\\n\",confusion_matrix(y_train, model.predict(X_train)),'\\n')\n  print(\"Test confusion matrix:\\n\",confusion_matrix(y_test, model.predict(X_test)))\n  print('*'*50)\n\n","03bc43a1":"from sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import accuracy_score","c895f73c":"m = RandomForestClassifier().fit(X_train,y_train)\npred_y = m.predict(X_test)\nprint('*'*50)\nprint('Report')\nprint('model : RandomForestClassifier')\nprint('Train score',model.score(X_train,y_train))\nprint('Test score',model.score(X_test,y_test))\nprint()\nprint(\"accuracy: %.2f\" %accuracy_score(y_test, pred_y))\nprint(\"Precision : %.3f\" % precision_score(y_test, pred_y))\nprint(\"Recall : %.3f\" % recall_score(y_test, pred_y))\nprint(\"F1 : %.3f\" % f1_score(y_test, pred_y))\nprint()\nprint(\"Train confusion matrix:\\n\",confusion_matrix(y_train, model.predict(X_train)),'\\n')\nprint(\"Test confusion matrix:\\n\",confusion_matrix(y_test, model.predict(X_test)))\nprint('*'*50)","ca3b8ed0":"lr_clf = LogisticRegression(solver='liblinear')\nlr_clf.fit(X_train, y_train)\n\nprint('Train score',model.score(X_train,y_train))\nprint('Test score',model.score(X_test,y_test))\n\nlrCoef = LogisticRegression().fit(X_train,y_train).coef_\nprint(lrCoef)","5ead3d64":"coefdf = pd.DataFrame(data=X_train.columns, index=range(0, len(lrCoef[0])), columns=['Feature'])\ncoefdf['Coef'] = lrCoef[0]\ncoefdf['Absuolute num of Coef'] = abs(lrCoef[0])\ncoefdf = coefdf.sort_values(by='Absuolute num of Coef', ascending=False).reset_index(drop=True)\ncoefdf","311494bb":"fig, ax = plt.subplots(figsize=(10,5))\nsns.barplot(data=coefdf, y=coefdf['Feature'], x=coefdf['Coef'])\n\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.title('Coefficient of Logistic Regression\\n(score : 95%)', fontsize=20)\nplt.xlabel('Coefficient')\n\nplt.savefig('Coefficient of Logistic Regression.png')\nplt.show()","81d0aae3":"import shap","c6548faa":"XGBmodel = XGBClassifier().fit(X_train, y_train)\n\nexplainer = shap.Explainer(XGBmodel)\nshap_values = explainer(X_train)\n\n# visualize the first prediction's explanation\nshap.plots.waterfall(shap_values[0], show=False)\nplt.title('SHAP of XGBoost Classifier\\n(score : 95%)')\nplt.figsize=(10,5)\nplt.show()","a3c2912b":"RFmodel = RandomForestClassifier().fit(X_train, y_train)\n\nexplainer = shap.TreeExplainer(RFmodel)\nshap_values = explainer.shap_values(X_test)\n\n# fig, ax = plt.subplots(figsize=(10,5))\nshap.summary_plot(shap_values, X_test, plot_size=(10,5), show=False)\nplt.title('SHAP of Random Forest Classifier\\n(score : 96%)')\nplt.show()","2231a5e6":"from sklearn.inspection import permutation_importance\n\nresult = permutation_importance(RFmodel, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\nsorted_idx = result.importances_mean.argsort()\n\nplt.figure(figsize=(10,5))\nplt.title('Permutation Importance of Random Forest Classifier')\n\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.boxplot(result.importances[sorted_idx].T,\n            vert=False, labels=X.columns[sorted_idx]);","952a962c":"from pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=RFmodel, dataset=X_train,\n                            model_features=X_train.columns, feature='Daily Internet Usage')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Daily Internet Usage')\nplt.show()","5d0f15e8":"**Daily Internet Usage** is the most important feature. \n\nThe lower Daily Internet Usage and Daily Time Spent on Site, the more likely to click the ad.\n\nSex and age are the least relevant feature.\n\nArea Income affect a little.\n\nThus, targeting Ad to the people who use internet little and rarely spend time on website is efficient to make more likely to click the ad."}}