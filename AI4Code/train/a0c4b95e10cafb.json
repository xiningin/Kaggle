{"cell_type":{"c3fc27d5":"code","8cf4189b":"code","b0f0fc05":"code","339fd9b6":"code","1c219c5d":"code","bf2e6a47":"code","107bd19c":"code","ca211196":"code","ee30d731":"code","586df621":"code","691c378b":"code","3daf655a":"code","210f9d96":"code","f3c0fd47":"code","3e0e6a64":"code","0b5a7626":"code","1d9be670":"code","2e9d81f2":"code","2a9c3b7f":"code","f8299924":"code","7076ce57":"code","09074890":"code","a99b79f1":"code","132c0a67":"code","cb2cf983":"code","8a56daa5":"code","8929888e":"code","37f610d6":"code","0fb051f1":"code","f9e42361":"code","b9d3d4bb":"code","4d72a69e":"code","77e2c69c":"code","d3d1c14f":"code","768cc3a7":"code","3fe33bea":"code","7812f0e1":"code","70392d89":"code","2590c8ae":"code","67893510":"code","a68dd49a":"markdown","5272da7e":"markdown","46e97bad":"markdown","da270e4e":"markdown","8033d4c2":"markdown","a87b7295":"markdown"},"source":{"c3fc27d5":"import numpy as np # linear algebra\nimport pandas as pd\nimport cv2\nimport os\nimport math\nimport imageio\nimport seaborn as sns\nfrom glob import glob\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\n\nfrom sklearn.model_selection import train_test_split","8cf4189b":"import seaborn as sns\nimport plotly.express as px\nfrom IPython.display import SVG\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n%matplotlib inline","b0f0fc05":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.applications import VGG16,inception_v3\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\nfrom tensorflow.keras.layers import Input, Lambda,Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                                    Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\n                                    LSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape,\\\n                                    Conv2DTranspose, LeakyReLU, Conv1D, AveragePooling1D, MaxPooling1D\n\n\n","339fd9b6":"Main_Video_Path = Path(\"..\/input\/real-life-violence-situations-dataset\/Real Life Violence Dataset\")\nVideo_Path = list(Main_Video_Path.glob(r\"*\/*.mp4\"))\nVideo_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"MP4\").astype(str)\nVideo_Labels_Series = pd.Series(Video_Labels,name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","1c219c5d":"Violence_Data = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"Violence\"]\nNonViolence_Data = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"NonViolence\"]\n\nViolence_Data = Violence_Data.reset_index()\nNonViolence_Data = NonViolence_Data.reset_index()","bf2e6a47":"Violence_Data","107bd19c":"Main_Video_Path = Path(\"..\/input\/violencedetectionsystem\")\nVideo_Path = list(Main_Video_Path.glob(r\"*\/*.mp4\"))\nVideo_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"MP4\").astype(str)\nVideo_Labels_Series = pd.Series(Video_Labels,name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","ca211196":"Main_MP4_Data[\"CATEGORY\"].replace({'fight':'Violence','noFight':'NonViolence'}, inplace=True)\nVD = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"Violence\"]\nNVD = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"NonViolence\"]\n\n","ee30d731":"Violence_Data = Violence_Data.append(VD,ignore_index=True, sort=False)\nNonViolence_Data = NonViolence_Data.append(NVD,ignore_index=True, sort=False)","586df621":"Violence_Data","691c378b":"NonViolence_Data","3daf655a":"Main_Video_Path3 = Path(\"..\/input\/ucf-crime-full\/Normal_Videos_for_Event_Recognition\")\nVideo_Path = list(Main_Video_Path3.glob(r\"*.mp4\"))\n# Video_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"mp4\").astype(str)\nVideo_Labels_Series = pd.Series('NonViolence',name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","210f9d96":"Main_MP4_Data['CATEGORY'] = Main_MP4_Data['CATEGORY'].fillna(\"NonViolence\")","f3c0fd47":"Main_MP4_Data.rename(columns = {'mp4':'MP4'}, inplace = True)","3e0e6a64":"Main_MP4_Data","0b5a7626":"NonViolence_Data = NonViolence_Data.append(Main_MP4_Data,ignore_index=True, sort=False)","1d9be670":"NonViolence_Data","2e9d81f2":"Main_Video_Path3 = Path(\"..\/input\/ucf-crime-full\/Fighting\")\nVideo_Path = list(Main_Video_Path3.glob(r\"*.mp4\"))\n# Video_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"mp4\").astype(str)\nVideo_Labels_Series = pd.Series('Violence',name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","2a9c3b7f":"Main_MP4_Data['CATEGORY'] = Main_MP4_Data['CATEGORY'].fillna(\"Violence\")","f8299924":"Main_MP4_Data.rename(columns = {'mp4':'MP4'}, inplace = True)","7076ce57":"Main_MP4_Data","09074890":"Violence_Data = Violence_Data.append(Main_MP4_Data,ignore_index=True, sort=False)","a99b79f1":"Violence_Data","132c0a67":"NonViolence_Data","cb2cf983":"Cdata = NonViolence_Data","8a56daa5":"Cdata = Cdata.append(Violence_Data,ignore_index=True, sort=False)","8929888e":"Cdata","37f610d6":"Cdata[\"CATEGORY\"].replace({'Violence':1,'NonViolence':0}, inplace=True)","0fb051f1":"# Cdata.CATEGORY[Cdata.CATEGORY == 'Violence']","f9e42361":"! mkdir .\/Frames\n!cd .\/Frames; mkdir .\/violence; mkdir .\/nonviolence","b9d3d4bb":"violence_frame_list = []\nv = 0\nfor file_video in Violence_Data.MP4:\n    Video_File_Path = file_video\n    \n    Video_Caption = cv2.VideoCapture(Video_File_Path)\n    Frame_Rate = 15\n    count = 0\n    temp = []\n    while Video_Caption.isOpened():\n        \n        Current_Frame_ID = Video_Caption.get(1)\n        \n        ret,frame = Video_Caption.read()\n        \n        if ret != True:\n            break\n            \n        if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n            image = cv2.resize(frame,(256,256))\n            cv2.imwrite(\".\/Frames\/violence\/frame{}{}.jpg\".format(v,count), image)\n            violence_frame_list.append([\"frame{}{}.jpg\".format(v,count),1])\n            count += 1\n    v += 1\n        \n    Video_Caption.release()\n    \nlen(violence_frame_list)","4d72a69e":"non_violence_frame_list = []\nv = 0\nfor file_video in NonViolence_Data.MP4:\n    Video_File_Path = file_video\n    \n    Video_Caption = cv2.VideoCapture(Video_File_Path)\n    Frame_Rate = 10\n    count = 0\n    while Video_Caption.isOpened():\n        \n        Current_Frame_ID = Video_Caption.get(1)\n        \n        ret,frame = Video_Caption.read()\n        \n        if ret != True:\n            break\n            \n        if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n            image = cv2.resize(frame,(256,256))\n            cv2.imwrite(\".\/Frames\/nonviolence\/frame{}{}.jpg\".format(v,count), image)\n            non_violence_frame_list.append([\"frame{}{}.jpg\".format(v,count),0])\n            count += 1\n    v += 1\n    Video_Caption.release()\n    \nlen(non_violence_frame_list)","77e2c69c":"datagen = ImageDataGenerator(validation_split=0.2, \n                             rescale=1.\/255,\n                             preprocessing_function=preprocess_input)","d3d1c14f":"# train_generator = datagen.flow_from_directory(\".\/Frames\",\n#                                               target_size=(224, 224), color_mode='rgb',\n#                                               class_mode='binary', batch_size=32,)","768cc3a7":"video = Input(shape=(224,224,3))\ncnn_base = VGG16(input_shape=(224,224,3),\n                 weights=\"imagenet\",\n                 include_top=False)\n# cnn_out = GlobalAveragePooling2D()(cnn_base.output)\ncnn_out = cnn_base.output\ncnn = Model(cnn_base.input,cnn_out)\ncnn.trainable = False\n\nmodel = keras.Sequential()\nmodel.add(keras.Input(shape=(224,224,3)))\nmodel.add(cnn)\nmodel.add(TimeDistributed(Flatten()))\nmodel.add(Bidirectional(LSTM(256,return_sequences=True,\n                                  dropout=0.5,\n                                  recurrent_dropout=0.5)))\nmodel.add(LSTM(256,return_sequences=True,))\nmodel.add(Dense(128,\"relu\"))\nmodel.add(Dense(32,\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.summary()","3fe33bea":"train = datagen.flow_from_directory(\".\/Frames\",\n                                              target_size=(224, 224), color_mode='rgb',\n                                              class_mode='binary',shuffle=True, batch_size=32,subset=\"training\" )\nvalid = datagen.flow_from_directory(\".\/Frames\",\n                                              target_size=(224, 224), color_mode='rgb',\n                                              class_mode='binary',shuffle=True, batch_size=8,subset=\"validation\")","7812f0e1":"train[100][1]","70392d89":"Callback_Stop_Early = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3,mode='auto')\n\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n\nhistory = model.fit(train,\n                    validation_data=valid,\n                      callbacks=[Callback_Stop_Early],\n                      epochs=25)","2590c8ae":"model.save(\".\/vgg16_lstm1.hdf5\")","67893510":"print(history.history.keys())\nsns.set()\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","a68dd49a":"## **Video Preprocessing**","5272da7e":"## **Data Importing from different datasets**","46e97bad":"### **Merging all different Data**","da270e4e":"# Part-2 VGG16 + LSTM","8033d4c2":"##  **Import Libraries**","a87b7295":"<a href=\".\/vgg16_lstm.hdf5\"> Download File <\/a>"}}