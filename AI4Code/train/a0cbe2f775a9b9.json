{"cell_type":{"1590fa0f":"code","4182332c":"code","9f50e95c":"code","bf31d989":"code","97aa6af9":"code","932611a8":"code","399a7526":"code","97dd80fb":"code","3659186c":"code","87e3e411":"code","058d35d5":"code","e92fa558":"code","0259ebb9":"code","1ab7fbb5":"code","6133abad":"code","f48e00a2":"code","f5f1c6c3":"code","04abd9d2":"code","da95e554":"code","e8b48936":"code","a810f594":"code","39ad5d2c":"code","fbf6e69b":"code","bc69914d":"code","4394879b":"code","f446b9b2":"code","fb0eee50":"code","b513575e":"code","a474bcc3":"code","c6e3d502":"code","9ab4ebce":"code","12ea7a68":"code","d931d92a":"code","18f5e5d7":"code","859cfaf4":"code","a0e7f531":"code","19e7072c":"code","8043f0cf":"code","3158b943":"code","65e54939":"code","1106d099":"code","981c0b3e":"code","8bace3f2":"code","fa72fa85":"code","45e02a40":"code","8710da4b":"code","562ba4f9":"code","c37dd3fb":"code","8fb04ffd":"code","922ee0cc":"code","860f0cc1":"code","ce033196":"code","c9ee8c5f":"code","43a6c7a1":"code","56064d41":"code","920d036a":"code","7053c28c":"code","17db6549":"code","7c30f3ab":"code","e390c618":"code","c9f4d418":"code","ddaee58e":"code","7757280d":"code","28cf91ef":"code","a4c09918":"code","b45b2008":"code","bf3b43ab":"code","278b437b":"code","17c0c416":"code","3e2ab016":"code","8766c93d":"code","6aa3b8f1":"code","b6ca9333":"code","f9a02156":"code","99c2af41":"code","e79d02c5":"code","9cf65136":"code","c7135c38":"code","c45c46ce":"markdown","c2559711":"markdown","372d48f2":"markdown","bb87038b":"markdown","a39c516e":"markdown","297cce26":"markdown","6cf79030":"markdown","53ea84d7":"markdown","85261f12":"markdown","bb802998":"markdown","f4a2cb2e":"markdown","5e8bac7c":"markdown","bece7340":"markdown","1fcc24ec":"markdown","1d793fcb":"markdown","1a8b6a13":"markdown","2e9e3e86":"markdown","a77dfa81":"markdown","8b3b73e0":"markdown","1ffb92a3":"markdown","0f852288":"markdown","231da2dc":"markdown","2730ea46":"markdown","dd9ae95b":"markdown","22a9a4a3":"markdown","019f74d3":"markdown","68ba40c2":"markdown","d8a6175e":"markdown"},"source":{"1590fa0f":"# Importing needed lib's\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","4182332c":"# settings\npd.set_option('max_colwidth', 1000)\npd.set_option('max_rows',40)\npd.set_option('max_columns',1000)\n\nsns.set_style('darkgrid')","9f50e95c":"# reading CSV\ndf = pd.read_csv('..\/input\/lending-club\/lending_club_loan_two.csv')","bf31d989":"df.shape","97aa6af9":"df.info()","932611a8":"df.head(5)","399a7526":"sns.countplot(x=df['loan_status'])","97dd80fb":"# In this cell we'll map the Fully paid to a numeric feature to 1 and 0 intoa Paid Column\n# The column has no missing values \nmapping = {'Fully Paid': 1, 'Charged Off': 0}\ndf['Paid'] = df['loan_status'].map(mapping).astype(int)","3659186c":"# Seprating the numeric and categorical \n\nnumeric_ = df.select_dtypes(exclude=['object']).copy()\nprint(numeric_.shape)\nnumeric_.sample(5)","87e3e411":"cat_ = df.select_dtypes(include=['object']).copy()\ncat_.sample(5)","058d35d5":"fig = plt.figure(figsize=(12,16))\nfor index,col in enumerate(numeric_):\n    plt.subplot(7,2,index+1)\n    sns.distplot(numeric_.loc[:,col].dropna(), kde=False,bins=30)\nfig.tight_layout(pad=1.0)","e92fa558":"fig = plt.figure(figsize=(12,16))\nfor index,col in enumerate(numeric_):\n    plt.subplot(7,2,index+1)\n    sns.boxplot(data = numeric_.dropna(), y = col)\nfig.tight_layout(pad=1.0)","0259ebb9":"cat_.shape","1ab7fbb5":"cat_.columns","6133abad":"sns.countplot(x=cat_.term,data=cat_.dropna())\nplt.xticks(rotation=90)","f48e00a2":"Order=sorted(df.grade.unique())\nsns.countplot(x=cat_.grade,data=cat_,order=Order,palette='Spectral',hue='loan_status')","f5f1c6c3":"Order=sorted(df.sub_grade.unique())\nplt.figure(figsize=(10,8))\nsns.countplot(x=cat_.sub_grade,data=cat_,order=Order,palette='Spectral',hue='loan_status')","04abd9d2":"cat_.emp_title.value_counts().head(20)","da95e554":"# there is a lot of Emp Titles, lets leave them to dummies or remove them ","e8b48936":"plt.figure(figsize=(10,8))\ncat_.emp_length.value_counts()\nsns.countplot(x=cat_.emp_length,data=cat_,palette='Spectral',order=[ '< 1 year',\n                      '1 year',\n                     '2 years',\n                     '3 years',\n                     '4 years',\n                     '5 years',\n                     '6 years',\n                     '7 years',\n                     '8 years',\n                     '9 years',\n                     '10+ years'],hue='loan_status')\n#print(cat_.emp_length.isnull().sum())","a810f594":"sns.countplot(x=cat_.home_ownership,data=cat_,palette='Spectral',hue='loan_status')\ncat_.home_ownership.value_counts()\n# We can map these home_ownwership too","39ad5d2c":"cat_.verification_status.value_counts()","fbf6e69b":"cat_.issue_d.value_counts()","bc69914d":"cat_.purpose.value_counts()\n# We can Map these too","4394879b":"cat_.title.value_counts()\n# Titile and purpose are almost the same, so we can drop one of these","f446b9b2":"cat_.info()","fb0eee50":"cat_.earliest_cr_line.value_counts()","b513575e":"sns.countplot(x=cat_.application_type,data=cat_,palette='Spectral',hue='loan_status')\ncat_.application_type.value_counts()","a474bcc3":"# Lets check the correlation with the Loan_status\ncorrelation = numeric_.corr()\ncorrelation[['Paid']].sort_values(['Paid'], ascending=False)","c6e3d502":"fig = plt.figure(figsize=(10,10))\nsns.heatmap(correlation,annot=True,cmap='plasma')\nfig.tight_layout()","9ab4ebce":"# Checking Null values  \nplt.figure(figsize=(10,8))\nsns.heatmap(df.isnull(),cmap='plasma')","12ea7a68":"def percent_missing():\n    percentage = pd.DataFrame(100*(df.isnull().sum()\/len(df)),columns=['Missing_%']).sort_values('Missing_%',ascending=False)\n    return (percentage.head(10))","d931d92a":"percent_missing()","18f5e5d7":"# 1.Removing the higly correlated \ndf = df.drop(['installment'],axis=1)","859cfaf4":"# removing the loan status coz we have Paid column \ndf = df.drop(['loan_status'], axis=1)","a0e7f531":"# As emp_title as many values, putting dummies will drastically increase the features column, which is again a trouble. And 6% missing values \ndf = df.drop(['emp_title'], axis=1)","19e7072c":"# As purpose is substring of title we shall drop it too\ndf = df.drop(['title'],axis=1)","8043f0cf":"# As Sub_grade comes under grade we'll shall drop grade too\ndf = df.drop(['grade'],axis=1)","3158b943":"# Issue date is a data leakage coz, this tells us whether or not the loan is approved before we even calculate. \ndf = df.drop(['issue_d'],axis=1)","65e54939":"#df.describe()\n#Removing the Outliers revol_util ,dti ,revol_bal ,open_acc ,pub_rec\ndf = df[df['revol_util'] < 600]\ndf = df[df['dti'] < 8000]\ndf = df[df['revol_bal'] < 1.5*1e6]\ndf = df[df['open_acc']  < 65]\ndf = df[df['pub_rec'] < 70]","1106d099":"mapp = {'< 1 year':0.5,\n                      '1 year':1,\n                     '2 years':2,\n                     '3 years':3,\n                     '4 years':4,\n                     '5 years':5,\n                     '6 years':6,\n                     '7 years':7,\n                     '8 years':8,\n                     '9 years':9,\n                     '10+ years':10}\n\ndf['emp_length'] = df['emp_length'].map(mapp)","981c0b3e":"df['emp_length'] = df['emp_length'].fillna(value=0)","8bace3f2":"# Let's drop out the values with Na which in above we replaced by 0\ndf = df[df['emp_length'] > 0] ","fa72fa85":"from sklearn import preprocessing\n\nlabel_encoder = preprocessing.LabelEncoder()\ndf['home_ownership']=df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')\n\ndf['home_ownership'] = label_encoder.fit_transform(df['home_ownership'])","45e02a40":"df['term'] = label_encoder.fit_transform(df['term'])","8710da4b":"df['verification_status'] = label_encoder.fit_transform(df['verification_status'])","562ba4f9":"df['purpose'] = label_encoder.fit_transform(df['purpose'])","c37dd3fb":"df['initial_list_status'] = label_encoder.fit_transform(df['initial_list_status'])","8fb04ffd":"df['purpose'] = label_encoder.fit_transform(df['purpose'])","922ee0cc":"mapp_={'INDIVIDUAL':1,'JOINT':0,'DIRECT_PAY':0}\ndf['application_type'] = df['application_type'].map(mapp_)","860f0cc1":"subgrade_dummies = pd.get_dummies(df['sub_grade'],drop_first=True).astype(int)\ndf = pd.concat([df.drop('sub_grade',axis=1),subgrade_dummies],axis=1)","ce033196":"# Now lets deal with address\n# Collecting the last values coz its zipcode \ndf['Address_code'] = df['address'].apply(lambda address:address[-5:])","c9ee8c5f":"df = df.drop('address',axis=1)\ndf['Address_code'] = label_encoder.fit_transform(df['Address_code'])","43a6c7a1":"# Now lets deal with the last object [earliest_cr_line]\ndf['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda date:int(date[-4:]))\n# Exctrating the year alone","56064d41":"# df.describe()['earliest_cr_year']\n# df['earliest_cr_year'].count_values()\n# As we can see the years has a lot of values ranging from 1 to ten thousand's+\n# So we break them into ranges then assign a value to them\n# We'll keep the range as decades (10 years)\ndf['earliest_cr_year'] = pd.cut(df.earliest_cr_year, bins=[1943.931,1953.857,1963.714,1973.571,1983.429,1993.286,2003.143,2013.0],\n                               labels=['43','53','63','73','83','93','03'])","920d036a":"# As 1943.931-1953.857-1963.714 years have a very few we'll join them with the next range 1963.714 - 1973.571\ndf['earliest_cr_year']=df['earliest_cr_year'].replace(['43', '53'], '63')\n\ndf['earliest_cr_year']=label_encoder.fit_transform(df['earliest_cr_year'])","7053c28c":"# Lets drop of the line\ndf = df.drop('earliest_cr_line',axis=1)","17db6549":"# Filling with the frequent value\ndf['pub_rec_bankruptcies'] = df['pub_rec_bankruptcies'].fillna(df['pub_rec_bankruptcies'].dropna().mode()[0])","7c30f3ab":"# 8,7,6,5,4,3,2 have a very few values so we'll join them with 1 , i.e.. leaving us with only two sub-category \ndf['pub_rec_bankruptcies']=df['pub_rec_bankruptcies'].replace([8,7,6,5,4,3,2], 1)","e390c618":"percent_missing()","c9f4d418":"# Ok lets deal with the last missing value\n# It has almost 10% missing data, so we'll fill him up w.r.t corr()with other variable\n# mort_acc","ddaee58e":"df.corr()['mort_acc'].sort_values(ascending=False)\n# We can see total_acc correlates with it.  ","7757280d":"acc_avg = df.groupby('total_acc').mean()['mort_acc']","28cf91ef":"def fill_mort_acc(total_acc,mort_acc):\n    if np.isnan(mort_acc):\n        return acc_avg[total_acc]\n    else:\n        return mort_acc","a4c09918":"df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis=1)","b45b2008":"percent_missing()","bf3b43ab":"plt.figure(figsize=(20,8))\nsns.heatmap(df.isnull(),cmap='plasma')\n# No null values ","278b437b":"df.info()","17c0c416":"fig = plt.figure(figsize=(20,25))\nsns.heatmap(df.corr(),cmap='plasma')\nfig.tight_layout(pad=1)\n# Checking for highly correlated term ","3e2ab016":"from sklearn.model_selection import train_test_split #for split the data\nfrom sklearn.metrics import accuracy_score  #for accuracy_score\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nX = df.drop([\"Paid\"],axis=1)\ny = df[\"Paid\"]\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","8766c93d":"from sklearn.preprocessing import MaxAbsScaler\nscaler = MaxAbsScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","6aa3b8f1":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\npred_lr = model.predict(X_test)\nprint('-'*30)\nprint('Accuracy of Logistic Regression: {:.2f}'.format(accuracy_score(pred_lr,y_test)*100))\nprint('-'*40)\n#cross_val_lr = cross_val_score(model,X,y,cv=5,scoring='accuracy')\n#print('Cross Validation Score of Logistic Regression: {:.2f}'.format(cross_val_lr.mean()*100))\n#print('-'*50)\n#y_pred_lr = cross_val_predict(model,X,y,cv=5)\n#sns.heatmap(confusion_matrix(y,y_pred_lr),annot=True,cmap='coolwarm')\n#plt.title('Confusion Matrix',y=1.05, size=15)\n\n\n# ------------------------------\n# Accuracy of Logistic Regression: 83.80\n# ----------------------------------------\n# Cross Validation Score of Logistic Regression: 80.62\n# --------------------------------------------------","b6ca9333":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train,y_train)\npred_lr = model.predict(X_test)\nprint('-'*30)\nprint('Accuracy of RandomForestClassifier: {:.2f}'.format(accuracy_score(pred_lr,y_test)*100))\nprint('-'*40)\n#cross_val_lr = cross_val_score(model,X,y,cv=5,scoring='accuracy')\n#print('Cross Validation Score of RandomForestClassifier: {:.2f}'.format(cross_val_lr.mean()*100))\n#print('-'*50)\n#y_pred_lr = cross_val_predict(model,X,y,cv=5)\n#sns.heatmap(confusion_matrix(y,y_pred_lr),annot=True,cmap='coolwarm')\n#plt.title('Confusion Matrix',y=1.05, size=15)\n\n# ------------------------------\n# Accuracy of Logistic Regression: 89.02\n# ----------------------------------------\n# Cross Validation Score of Logistic Regression: 89.05\n# --------------------------------------------------","f9a02156":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier()\nmodel.fit(X_train,y_train)\npred_lr = model.predict(X_test)\nprint('-'*30)\nprint('Accuracy of KNeighborsClassifier: {:.2f}'.format(accuracy_score(pred_lr,y_test)*100))\nprint('-'*40)\n#cross_val_lr = cross_val_score(model,X,y,cv=5,scoring='accuracy')\n#print('Cross Validation Score of KNeighborsClassifier: {:.2f}'.format(cross_val_lr.mean()*100))\n#print('-'*50)\n#y_pred_lr = cross_val_predict(model,X,y,cv=5)\n#sns.heatmap(confusion_matrix(y,y_pred_lr),annot=True,cmap='coolwarm')\n#plt.title('Confusion Matrix',y=1.05, size=15)\n\n# ------------------------------\n# Accuracy of KNeighborsClassifier: 81.19\n# ----------------------------------------\n# Cross Validation Score of KNeighborsClassifier: 77.58\n# --------------------------------------------------","99c2af41":"from sklearn.ensemble import GradientBoostingClassifier\n\nmodel = GradientBoostingClassifier(n_estimators=200)\nmodel.fit(X_train,y_train)\npred_lr = model.predict(X_test)\nprint('-'*30)\nprint('Accuracy of GradientBoostingClassifier: {:.2f}'.format(accuracy_score(pred_lr,y_test)*100))\nprint('-'*40)\n#cross_val_lr = cross_val_score(model,X,y,cv=5,scoring='accuracy')\n#print('Cross Validation Score of GradientBoostingClassifier: {:.2f}'.format(cross_val_lr.mean()*100))\n#print('-'*40)\n#y_pred_lr = cross_val_predict(model,X,y,cv=5)\n#sns.heatmap(confusion_matrix(y,y_pred_lr),annot=True,cmap='coolwarm')\n#plt.title('Confusion Matrix',y=1.05, size=15)\n\n# ------------------------------\n# Accuracy of GradientBoostingClassifier: 89.18\n# ----------------------------------------\n# Cross Validation Score of GradientBoostingClassifier: 89.14\n# ----------------------------------------","e79d02c5":"from sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\nmodel.fit(X_train,y_train)\npred_lr = model.predict(X_test)\nprint('-'*30)\nprint('Accuracy of GaussianNB: {:.2f}'.format(accuracy_score(pred_lr,y_test)*100))\nprint('-'*40)\n#cross_val_lr = cross_val_score(model,X,y,cv=5,scoring='accuracy')\n#print('Cross Validation Score of GaussianNB: {:.2f}'.format(cross_val_lr.mean()*100))\n#print('-'*40)\n#y_pred_lr = cross_val_predict(model,X,y,cv=5)\n#sns.heatmap(confusion_matrix(y,y_pred_lr),annot=True,cmap='coolwarm')\n#plt.title('Confusion Matrix',y=1.05, size=15)\n\n# ------------------------------\n# Accuracy of GaussianNB: 76.14\n# ----------------------------------------\n# Cross Validation Score of GaussianNB: 81.92\n# ----------------------------------------","9cf65136":"from sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train,y_train)\npred_lr = model.predict(X_test)\nprint('-'*30)\nprint('Accuracy of DecisionTreeClassifier: {:.2f}'.format(accuracy_score(pred_lr,y_test)*100))\nprint('-'*40)\n#cross_val_lr = cross_val_score(model,X,y,cv=5,scoring='accuracy')\n#print('Cross Validation Score of DecisionTreeClassifier: {:.2f}'.format(cross_val_lr.mean()*100))\n#print('-'*40)\n#y_pred_lr = cross_val_predict(model,X,y,cv=5)\n#sns.heatmap(confusion_matrix(y,y_pred_lr),annot=True,cmap='coolwarm')\n#plt.title('Confusion Matrix',y=1.05, size=15)\n\n# ------------------------------\n# Accuracy of DecisionTreeClassifier: 83.28\n# ----------------------------------------\n# Cross Validation Score of DecisionTreeClassifier: 83.37\n# ----------------------------------------","c7135c38":"from sklearn.ensemble import AdaBoostClassifier\n\nmodel = AdaBoostClassifier()\nmodel.fit(X_train,y_train)\npred_lr = model.predict(X_test)\nprint('-'*30)\nprint('Accuracy of AdaBoostClassifier: {:.2f}'.format(accuracy_score(pred_lr,y_test)*100))\nprint('-'*40)\n#cross_val_lr = cross_val_score(model,X,y,cv=5,scoring='accuracy')\n#print('Cross Validation Score of AdaBoostClassifier: {:.2f}'.format(cross_val_lr.mean()*100))\n#print('-'*40)\n#y_pred_lr = cross_val_predict(model,X,y,cv=5)\n#sns.heatmap(confusion_matrix(y,y_pred_lr),annot=True,cmap='coolwarm')\n#plt.title('Confusion Matrix',y=1.05, size=15)\n\n# ------------------------------\n# Accuracy of AdaBoostClassifier: 89.09\n# ----------------------------------------\n# Cross Validation Score of AdaBoostClassifier: 89.09\n# ----------------------------------------","c45c46ce":"### Numeric and Category seprations  ","c2559711":"# Ada Boost Classifier ","372d48f2":"# K-Neighbours Classifier","bb87038b":"## *Imports* ","a39c516e":"# Modelling  ","297cce26":"**Lets Look at the data**","6cf79030":"**This is the column we'll be predicting  In this cell we'll map them to 1 and 0 into a new column called Paid**","53ea84d7":"# Data Processing ","85261f12":"**As I said above I'll remove the highly correlated column installment**","bb802998":"## I'm just starting out in DS,ML\n\n## If there is some improvement to be made please do add it in comments, it will be usefull for me!! \n\n## Thank you ","f4a2cb2e":"# GaussianNB","5e8bac7c":"# Logistic Regression","bece7340":"**We can Map them into there respective years**  ","1fcc24ec":"**We got six columns with missing values**","1d793fcb":"**We have some Outliers in dti, open_acc, revol_utl, revol_bal,pub_rec**","1a8b6a13":"### I've Written down the cross validation score in comments  ","2e9e3e86":"# *Univariate analysis of Categorical features*","a77dfa81":"# Bi-Variate Analysis","8b3b73e0":"# Gradient Boosting Classifier","1ffb92a3":"# Removing Useless Features","0f852288":"# Risk Classification using lending_club_dataset \n### [by oldwine__](https:\/\/www.kaggle.com\/oldwine357)","231da2dc":"# ***Univariate analysis of Numeric features***","2730ea46":"# Random Forest Classifier","dd9ae95b":"# Mapping Values for Categories ","22a9a4a3":"**We can see some high correlation between installment and loan amount**\n<br>\n**In future lest drop them, they may cause inefficency**  ","019f74d3":"# Introduction \n\nWe'll build a model that can predict wether or nor a borrower will pay back their loan from the given historical data on loans given out with information on whether or not the borrower defaulted (charge-off)\n\n## Dataset\n* We will be using a subset of the LendingClub DataSet obtained from Kaggle: https:\/\/www.kaggle.com\/wordsforthewise\/lending-club\n\n## Description \n0\tloan_amnt >> The listed amount of the loan applied for by the   borrower. If at some point in time, the credit department          reduces the loan amount, then it will be reflected in this    value.\n\n1\tterm >>\tThe number of payments on the loan. Values are in months and can be either 36 or 60.\n\n2\tint_rate >>\tInterest Rate on the loan\n\n3\tinstallment\t>> The monthly payment owed by the borrower if the loan originates.\n\n4\tgrade >> LC assigned loan grade\n\n5\tsub_grade >> LC assigned loan subgrade\n\n6\temp_title >>The job title supplied by the Borrower when applying for the loan.*\n\n7\temp_length >> Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n\n8\thome_ownership >> The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER\n\n9\tannual_inc >> The self-reported annual income provided by the borrower during registration.\n\n10\tverification_status >> Indicates if income was verified by LC, not verified, or if the income source was verified\n\n11\tissue_d\t>> The month which the loan was funded\n\n12\tloan_status\t>> Current status of the loan\n\n13\tpurpose\t>> A category provided by the borrower for the loan request.\n\n14\ttitle >> The loan title provided by the borrower\n\n15\tzip_code >> The first 3 numbers of the zip code provided by the borrower in the loan application.\n\n16\taddr_state >> The state provided by the borrower in the loan application\n\n17\tdti\t>> A ratio calculated using the borrower\u2019s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower\u2019s self-reported monthly income.\n\n18\tearliest_cr_line >> The month the borrower's earliest reported credit line was opened\n\n19\topen_acc >> The number of open credit lines in the borrower's credit file.\n\n20\tpub_rec >> Number of derogatory public records\n\n21\trevol_bal >> Total credit revolving balance\n\n22\trevol_util >> Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n\n23\ttotal_acc >> The total number of credit lines currently in the borrower's credit file\n\n24\tinitial_list_status >> The initial listing status of the loan. Possible values are \u2013 W, F\n\n25\tapplication_type >> Indicates whether the loan is an individual application or a joint application with two co-borrowers\n\n26\tmort_acc >> Number of mortgage accounts.\n\n27\tpub_rec_bankruptcies >> Number of public record bankruptcies","68ba40c2":"### Loading The Data ","d8a6175e":"**We can also see int_rate corr() negatively with Paid**"}}