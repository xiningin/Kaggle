{"cell_type":{"01d900eb":"code","ae4578fc":"code","5dfd4b76":"code","55ed606a":"code","13aa9447":"code","4581bb6a":"code","9d1a13e3":"code","0a3840e8":"code","3fe0a396":"code","356386c8":"code","65a9d0d6":"code","bb534ffc":"code","c8a89751":"code","d391503d":"code","9b662be9":"code","a6a28b0b":"code","8e2b4b5e":"markdown","7f506fac":"markdown"},"source":{"01d900eb":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.callbacks import TensorBoard, EarlyStopping\nfrom imblearn.over_sampling import RandomOverSampler\nimport tensorflow_addons as tfa\n","ae4578fc":"train = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\n\n#Utility function to get the file path by the image id\ndef get_train_file_path(image_id):\n    return \"..\/input\/seti-breakthrough-listen\/train\/{}\/{}.npy\".format(image_id[0], image_id)\n\ntrain['file_path'] = train['id'].apply(get_train_file_path)\ntrain_y = train['target']\ntrain, validation = train_test_split(train, test_size=0.1, stratify=train['target'])\n","5dfd4b76":"class DataGenerator(Sequence):\n    def __init__(self, list_IDs, batch_size=500, dim=256, steps=1,\n             shuffle=True, normalize=False, prediction=False):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.shuffle = shuffle\n        self.steps = steps\n        self.normalize = normalize\n        self.prediction  = prediction\n        self.on_epoch_end()\n\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(self.list_IDs.shape[0])\n        \n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n            \n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, self.steps, self.dim))\n        if not self.prediction:\n            y = np.empty((self.batch_size), dtype=int)\n        else:\n            y = None\n\n        # Generate data\n        i=0\n        for _, row in list_IDs_temp.iterrows():\n            # Store sample\n            X[i] = self.__extract_sequence(row[2])\n            print(X[i].shape)\n            if not self.prediction:            \n                y[i] = row[1]\n            if self.normalize:\n                X[i] = ((X[i] - np.mean(X[i], axis=0)) \/ np.std(X[i], axis=0))\n            i= i+1\n        return X, y            \n                                                \n    \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(self.list_IDs.shape[0] \/ self.batch_size))\n    \n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # Find list of IDs\n        list_IDs_temp = self.list_IDs.iloc[indexes]\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n        if self.prediction:\n            return X\n        else:\n            return X, y\n\n    \n    def __get_train_filename_by_id(self, _id: str) -> str:\n        return f\"..\/input\/seti-breakthrough-listen\/train\/{_id[0]}\/{_id}.npy\"\n\n    def __extract_sequence(self, filename: str):\n        arr = np.load(filename)\n        arr = np.vstack(arr)\n        return arr\n\n","55ed606a":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, GaussianNoise, Activation\n\nloss=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n\n\nmodel = Sequential(name='Model')\n#model.add(BatchNormalization())\nmodel.add(LSTM(80, input_shape=(1638, 256), return_sequences=True, name='LSTM1',\n            kernel_regularizer=tf.keras.regularizers.l2(0.00005),\n            recurrent_regularizer=tf.keras.regularizers.l2(0.00005),\n            bias_regularizer=tf.keras.regularizers.l2(0.00001)\n        ))\nmodel.add(Dropout(0.4, name='lstm_dropout_1'))\nmodel.add(GaussianNoise(0.5))\nmodel.add(Activation('relu'))\n# model.add(LSTM(150, input_shape=(1638, 256), return_sequences=True, name='LSTM2',\n# #             kernel_regularizer=tf.keras.regularizers.l2(0.00005),\n# #             recurrent_regularizer=tf.keras.regularizers.l2(0.00005)\n# #             bias_regularizer=tf.keras.regularizers.l1_l2(l1=1e-3, l2=1e-3)\n#         ))               \n# model.add(GaussianNoise(0.5))\n# model.add(Activation('relu'))\n#model.add(Dropout(0.8, name='lstm_dropout_2'))\nmodel.add(LSTM(80, input_shape=(1638, 256), return_sequences=False, name='LSTM3',\n            kernel_regularizer=tf.keras.regularizers.l2(0.00005),\n            recurrent_regularizer=tf.keras.regularizers.l2(0.00005),\n            bias_regularizer=tf.keras.regularizers.l2(0.00005)\n        ))\nmodel.add(Dropout(0.4, name='lstm_dropout_3'))\nmodel.add(GaussianNoise(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(128, name='dense1', activation='relu'))\nmodel.add(Dropout(0.4, name='dense_dropout1'))\nmodel.add(Dense(1, name='Output', activation='sigmoid'))\nmodel.compile(loss=loss, optimizer='adam',metrics=['accuracy',tf.keras.metrics.AUC(), tfa.metrics.F1Score(num_classes=1, threshold=0.5)])\n#model.summary()","13aa9447":"# Parameters\ntrain_params = {'dim': 256,\n          'batch_size': 100,\n          'steps': 1638,\n          'shuffle': True,\n          'normalize':False}\n\nval_params = {'dim': 256,\n          'batch_size': 100,\n          'steps': 1638,\n          'shuffle': False,\n          'normalize':False}\n\n# Generators\ntraining_generator = DataGenerator(train, **train_params)\nvalidation_generator = DataGenerator(validation, **val_params)\n","4581bb6a":"#From tensorflow documentation:\n#https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data\n\npos = train.loc[train['target'] == 1].count()['id']\nneg = train.loc[train['target'] == 0].count()['id']\ntotal = train.count()['id']\nweight_for_0 = (1 \/ neg) * (total \/ 2.0)\nweight_for_1 = (1 \/ pos) * (total \/ 2.0)\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","9d1a13e3":"tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\".\/logs\")\nes = EarlyStopping(monitor='f1_score', mode='max', verbose=1, patience=20)\nh = model.fit(training_generator, epochs=200, verbose=1, validation_data=validation_generator, \n              callbacks=[es, tensorboard_callback], class_weight=class_weight)","0a3840e8":"train_params_test = {'dim': 256,\n          'batch_size': 4,\n          'steps': 1638,\n          'shuffle': False,\n          'normalize':False}\ntraining_generator_test = DataGenerator(train, **train_params_test)","3fe0a396":"#From tensorflow documentation:\n#https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data\n\ndef plot_cm(labels, predictions, p=0.5):\n    cm = confusion_matrix(labels, predictions > p)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.title('Confusion matrix @{:.2f}'.format(p))\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n\n    print('True Negatives: ', cm[0][0])\n    print('False Positives: ', cm[0][1])\n    print('False Negatives: ', cm[1][0])\n    print('True Positives: ', cm[1][1])\n    print('Total UFO sights: ', np.sum(cm[1]))","356386c8":"train_predictions_baseline = model.predict(training_generator_test, verbose=1)\n","65a9d0d6":"from sklearn.metrics import confusion_matrix\n\nplot_cm(train['target'], train_predictions_baseline)","bb534ffc":"# summarize history for accuracy\nplt.plot(h.history['accuracy'])\nplt.plot(h.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(h.history['loss'])\nplt.plot(h.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(h.history['f1_score'])\nplt.plot(h.history['val_f1_score'])\nplt.title('model f1_score')\nplt.ylabel('f1_score')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","c8a89751":"prediction_params = {'dim': 256,\n          'batch_size': 1,\n          'steps': 1638,\n          'shuffle': False,\n          'normalize':False,\n          'prediction':True}\nprediction_generator = DataGenerator(validation, **prediction_params)","d391503d":"predictions = model.predict(prediction_generator)","9b662be9":"conf_matrix = tf.math.confusion_matrix(labels=validation['target'],\n                                       predictions=predictions)","a6a28b0b":"conf_matrix","8e2b4b5e":"The training set is very unbalanced, so I decided to use the class_weight parameter of the train function.","7f506fac":"**Hello community.**\n\nI'm having a lot of problems regulazing my network so I decided to publish my notebook hoping that someone could help me.\n\nI am thinking of the competition data as time steps of frequency readings.\nSo, keeping in mind one of the images we are analyzing, I stacked the images so that they can be read top-down.\nEach line of the image corresponds to a step of the sequence.\n\nThen I implemented an LSTM network discovering that overfitting is easily achieved, without using particularly complex networks, but the validation set result is a disaster.\nActually, the loss and the accuracy are not bad but the problem is the F1-score (don't look at the results in this particular example: I only published this to show you the code).\nIts value remain low and almost constant throughout the training phase. The confusion matrix is pretty crap too.\n\nI have tried many combinations of regularization mechanisms, including noising data between the layers of the network, but none of these have improved the values of the score.\n\nAt this point, I'm afraid, the error is in the way I created the data sequence.\n\nThanks to anyone who wants to take a look at this.\n"}}