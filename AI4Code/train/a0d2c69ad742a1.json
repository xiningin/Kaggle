{"cell_type":{"6fe49492":"code","1bd95444":"code","1a851a79":"code","f2a2aad0":"code","52e543ae":"code","dee8f265":"code","67287eda":"code","4e10949b":"code","a76c7194":"code","8ca5a583":"code","4986ce27":"code","4338e9ba":"code","8d3444f4":"code","b78cce1c":"code","a54c5fac":"code","abd44705":"code","fbecf0dc":"code","f4fa78d9":"code","d582aec5":"code","51096fe5":"code","3512b862":"code","112005d2":"code","604f18f5":"code","43039adc":"code","36984cf5":"code","6e693468":"code","af4ec087":"code","a3ec58a9":"code","0f07a763":"code","3678cf24":"code","1a6f5f60":"code","848ac8e7":"code","770f6005":"code","cd5205b7":"code","7d663741":"code","795b34a5":"code","db02275a":"code","ca9db0ba":"code","c462cfb0":"code","2e262eee":"code","32553b53":"code","4a133a0b":"code","76151e53":"code","0ad61670":"code","471a44ad":"code","4b3a302c":"code","eaab2a07":"code","ff137b75":"code","7086496a":"code","aeb219c4":"code","41edd189":"code","b6eb3fcc":"code","6df90f40":"markdown","447307b0":"markdown","b9cabdc6":"markdown","f78d7f20":"markdown","4e9dd6e6":"markdown","36e8c9f4":"markdown","5a1bdae1":"markdown","bde73a47":"markdown","d169329a":"markdown","06bee1a6":"markdown","bc1a4be8":"markdown","9a0be23a":"markdown","be72f6b6":"markdown","af8b7158":"markdown","4caaa343":"markdown","cf4867d1":"markdown","1c5aea90":"markdown","d771a475":"markdown","3e9a63df":"markdown","4bb9d68c":"markdown","df7f8932":"markdown","e9c0f187":"markdown","2f24207b":"markdown","19c262d9":"markdown","ecda75c3":"markdown","ad1a716c":"markdown","2ecda453":"markdown"},"source":{"6fe49492":"text_path = \"..\/input\/an-inquiry-into-the-nature-and-causes-of-wealth\/An Inquiry.txt\"\nraw_text = open(text_path, \"r\").read()","1bd95444":"type(raw_text), len(raw_text)","1a851a79":"print(raw_text[0:1000])","f2a2aad0":"text = raw_text[719:-18868]","52e543ae":"from nltk.tokenize import word_tokenize","dee8f265":"all_tokens = word_tokenize(text)","67287eda":"all_tokens[1:10]","4e10949b":"from collections import Counter","a76c7194":"token_counts = Counter(all_tokens)\ntoken_counts.most_common(10)","8ca5a583":"# An example\n\"work\".isalpha(), \",\".isalpha()","4986ce27":"alpha_tokens = [word for word in all_tokens if word.isalpha()]\nlen(all_tokens), len(alpha_tokens)","4338e9ba":"token_counts = Counter(alpha_tokens)\ntoken_counts.most_common(10)","8d3444f4":"# An example of stopwords in English\nfrom nltk.corpus import stopwords\nstopwords.words(\"english\")[10:20]","b78cce1c":"# May take a little time to run\nalpha_tokens = [word.lower() for word in alpha_tokens]\ncleaned_tokens = [word for word in alpha_tokens if not word in stopwords.words(\"english\")]\n\nlen(cleaned_tokens), len(alpha_tokens)","a54c5fac":"token_counts = Counter(cleaned_tokens)\ntoken_counts.most_common(10)","abd44705":"%matplotlib inline \nimport matplotlib.pyplot as plt\n\n# Configure the way the plots will look\nplt.style.use([{\n    \"figure.dpi\": 300,\n    \"figure.figsize\":(12,9),\n    \"xtick.labelsize\": \"large\",\n    \"ytick.labelsize\": \"large\",\n    \"legend.fontsize\": \"x-large\",\n    \"axes.labelsize\": \"x-large\",\n    \"axes.titlesize\": \"xx-large\",\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n},'seaborn-poster', \"fivethirtyeight\"])","fbecf0dc":"from wordcloud import WordCloud\nstr_cleaned_tokens = \" \".join(cleaned_tokens) # the word cloud needs raw text as argument not list\nwc = WordCloud(background_color=\"white\", width= 800, height= 400).generate(str_cleaned_tokens)\nplt.imshow(wc)\nplt.axis(\"off\");","f4fa78d9":"from nltk.tokenize import sent_tokenize, RegexpTokenizer","d582aec5":"sentences = sent_tokenize(text)","51096fe5":"print(sentences[100])","3512b862":"tokenizer = RegexpTokenizer(r'\\w+')\n\ndef remove_stopwords(text, stopw = stopwords.words(\"english\")):\n    list_of_sentences = []\n    \n    for sentences in text:\n        list_of_words = []\n        for word in sentences:\n            if not word in stopw:\n                list_of_words.append(word)\n        list_of_sentences.append(list_of_words)\n    return list_of_sentences\n\ndef clean_sent(sentences):\n    \"\"\"Sentence must be a list containing string\"\"\"\n    stopw = stopwords.words(\"english\")\n    # Lower each word in each sentence        \n    sentences = [tokenizer.tokenize(sent.lower()) for sent in sentences]\n    sentences = remove_stopwords(sentences)\n    return sentences","112005d2":"cleaned_sentences = clean_sent(sentences)","604f18f5":"print(cleaned_sentences[100])","43039adc":"from gensim.models import Word2Vec","36984cf5":"model = Word2Vec(\n    min_count= 10,# minimum word occurence \n    size = 300, # number of dimensions\n    alpha = 0.01, #The initial learning rate\n)","6e693468":"model.build_vocab(cleaned_sentences)\nmodel.train(cleaned_sentences, total_examples = model.corpus_count, epochs = 60)","af4ec087":"model.wv.most_similar(\"wealth\")","a3ec58a9":"model.wv.most_similar(\"france\")","0f07a763":"model.wv.most_similar(\"africa\")","3678cf24":"import pandas as pd\nsimilar = pd.DataFrame(model.wv.most_similar(\"africa\", topn= 10), columns = [\"name\", \"height\"])","1a6f5f60":"similar.plot.barh(x = \"name\", y = \"height\");","848ac8e7":"model.wv.most_similar(\"king\")","770f6005":"all_words = model.wv.vectors","cd5205b7":"def wv_to_df(model):\n    all_wv = model.wv.vectors\n    \n    df = pd.DataFrame(\n        all_wv,\n        index = model.wv.vocab.keys(),\n        columns = [\"dim\" + str(i+1) for i in range(all_wv.shape[1])]\n    )\n    return df","7d663741":"df = wv_to_df(model)","795b34a5":"df.head()","db02275a":"df[\"idx\"] = df.index\ndf.head()","ca9db0ba":"ax = df.plot.scatter(\"dim1\", \"dim2\")\nfor i, point in df.iterrows():\n    ax.text(point.dim1 + 0.005, point.dim2 + 0.008, point.idx)","c462cfb0":"ax = df.plot.scatter(\"dim10\", \"dim20\")\nfor i, point in df.iterrows():\n    ax.text(point.dim10 + 0.005, point.dim20 + 0.008, point.idx)","2e262eee":"def plot_region(df, x, y,label, x_bounds, y_bounds, s=35, ftsize = None):\n    slices = df[\n        (x_bounds[0] <= df[x]) &\n        (df[x] <= x_bounds[1]) & \n        (y_bounds[0] <= df[y]) &\n        (df[y] <= y_bounds[1])\n    ]\n    print(slices.shape)\n    ax = slices.plot.scatter(x, y, s=s)\n    for i, point in slices.iterrows():\n        ax.text(point[x] + 0.005, point[y] + 0.005, point[label], fontsize = ftsize)","32553b53":"plot_region(df, \"dim1\", \"dim2\", \"idx\", (-0.5, 0), (-1, -0.3))","4a133a0b":"from sklearn.decomposition import PCA","76151e53":"pca_dimension_reduction = PCA(n_components= 2)\nres =  pca_dimension_reduction.fit_transform(df.drop(columns = \"idx\"))","0ad61670":"pca_coords = pd.DataFrame(res, columns = [\"x\", \"y\"])\npca_coords[\"words\"] = df.index","471a44ad":"pca_coords.head()","4b3a302c":"plot_region(pca_coords, \"x\", \"y\", \"words\", (0, 1), (1, 2))","eaab2a07":"from sklearn.manifold import TSNE","ff137b75":"tsne_dimension_reduction = TSNE(n_components=2)\nres = tsne_dimension_reduction.fit_transform(df.drop(columns = \"idx\"))","7086496a":"res.shape","aeb219c4":"tsne_coords = pd.DataFrame(res, columns = [\"x\", \"y\"])\ntsne_coords[\"words\"] = df.index","41edd189":"tsne_coords.plot.scatter(x = \"x\", y = \"y\")\nplt.title(\"Book corpus in 2D\");","b6eb3fcc":"plot_region(tsne_coords, \"x\", \"y\", \"words\", (5, 20), (20, 40))","6df90f40":"# What is this kernel for ?\nYou may wonder why I am using Smith's book here. \nFor this project my goal is to apply **Natural Language Processing** techniques to analyse and extract insights in the book. \nThis book is very referred in the Economic Litterature but few of the people who cited it have actually read it. In this project I would like to study the core thoughts of Adam Smith","447307b0":"Now that we have successfully removed the punctuations, we need to remove the stop words. Stop words are words that do not have a particular meaning. They are used to connect ideas, and making the language smooth. \nThe Natural Language Toolkit (nltk) package comes with a list of stopwords in English.   \n\nBefore doing so it's important to lower all the tokens. It's a strategy to normalize the text so there's no difference between \"The\", \"the\", or \"THE\".","b9cabdc6":"Here's how a sentence looks like.","f78d7f20":"Since the text is read as a big string object we cannot really exploit it. To make it exploitable we need to tokenize the text.  \nThere are two methods to tokenize the text : \n- we can tokenize sentences or   \n- tokenize tokens (words, punctuation ...). \n\n# EXPLORATORY DATA ANALYSIS\n\nIn the first part of this project I want to explore the text in an statiscal fashion. So I will tokenize the text such that I can count the occurence of every token (word) used in the corpus.","4e9dd6e6":"# Introduction\n![Wealth of Nations](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/1a\/Wealth_of_Nations.jpg)\n\nAn Inquiry into the Nature and Causes of the Wealth of Nations, generally referred to by its shortened title The Wealth of Nations, is the magnum opus of the Scottish economist and moral philosopher Adam Smith. First published in 1776, the book offers one of the world's first collected descriptions of what builds nations' wealth, and is today a fundamental work in classical economics. By reflecting upon the economics at the beginning of the Industrial Revolution, the book touches upon such broad topics as the division of labour, productivity, and free markets.","36e8c9f4":"Now let's try to zoom in (-0.5, 0) on the x-axis and (-1, -0.5) on the y-axis.","5a1bdae1":"Now every token is represented as an element of the list. The advantage is that we can count the occurence of every token.  \nBut it is likely that punctuations and stop words are the most frequent words in the corpus. ","bde73a47":"# DIMENSIONALITY REDUCTION\n\nThe word2vec model we build yields a 300 hundred dimensions dataset. This is quite huge. In order to represent the variability of the data in a 2D or 3D graph, it is important to reduce the dimensionality of the dataset. We can do this using t-SNE and PCA.","d169329a":"Now it would be interesting to visualize all the corpus. \nBut first, we can store all the word vectors in a Pandas data frame.","06bee1a6":"## Word2vec\n\nThe word2vec algorithm is implemented in the `gensim` library. It takes as input a sequence of sequences like this \n```python\n[[learning, data, science, good], [data, science, hot, job]]\n```\n\nTo make our corpus look like a sequence of sequence we'll write some functions to automate the process.  \nThe functions will :\n- lower the text\n- remove punctuations\n- remove stop words\n","bc1a4be8":"# MODELLING\n\nSo far what we've done is to split the text into tokens that we have studied individually. Even if this technique have given us some insights about the text, it is not enough because we took each token individually as if the book only contains 1262 occurences of price, 1232 occurences of country ignoring all the surroundings of these occurences.  \n\nI want to come up with a technique that captures the surroundings of each token so that I can have a better understanding of the corpus.  \nI am going to use a family of word embedding algorithms called word2vec.\n\nThese models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. **Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space**. (Wikipedia)\n\nI will then use the properties of this techniques to visualize the data more accurately.","9a0be23a":"We see that `,` is the most frequent word in the corpus. In overall the 10 most common words do not give us any clue on what the text is about. We'll need to clean it before visualizing the text.","be72f6b6":"The `word_tokenize()` function returns a list containing all the tokens that are in the corpus. Let's have a glance.","af8b7158":"# Text reading and cleaning\nFrom now I am going to read the text in memory and analyse it. \nThe text is freely available on the [Project Gutenberg website](https:\/\/www.gutenberg.org\/ebooks\/3300). I downloaded it in text format beforehand and uploaded on Kaggle.  \nProject Gutenberg was the first provider of free electronic books, or eBooks. Michael Hart, founder of Project Gutenberg, invented eBooks in 1971 and his memory continues to inspire the creation of eBooks and related technologies today.\n\n## Reading it with `open()`\nThe built-in `open()` function helps us read the text. We are going to open it in read mode.","4caaa343":"We assign the content of the text to the raw_text variable. From here we can have a glimpse of the text. \nNotice that Python reads the text as a large single string. \nFrom now what we can do with the text is to slice some part of it to display. For example we can print the thousand first characters for sanity check.","cf4867d1":"To remove the stopwords in the text we can write a list comprehension. ","1c5aea90":"Remember we say, we can tokenize our text into tokens or into sentences. For the exploratory part we were interested in counting occurences of each token, so we tokenized the text int individual tokens. Here we will tokenize it into sentences.","d771a475":"# VISUALIZATION","3e9a63df":"## Fitting the word2vec model. \n\nNow that the data is prepared we can feed it into the word2vec algorithm.","4bb9d68c":"Here's how the sentence will look after we apply the function on it.","df7f8932":"## Dimensionality reduction with PCA\n\nPrincipal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. (Wikipedia)\n\nThe reason why I decide to use PCA here is not because the dimensions are correlated among them but because the PCA procedure will transform the data in a way that most of the variability of the dataset is found in the first n components.  \nThe procedure is implemented in the scikit-learn library.","e9c0f187":"## Remove punctuations & stopwords\n\nAn easy way to remove punctuations in the list of the tokens is to use the `.isalpha()` method of the string class. Knowing that we can filter the list to keep only alphanumeric tokens.","2f24207b":"As we can read, each sentence contains punctuations, stop words","19c262d9":"It would be interesting if we could choose to zoom in a certain region of the scatterplot instead of having that big black hole. We can define a function that does that for us.","ecda75c3":"# t-SNE\nIt is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.","ad1a716c":"Now we are ready to go since we've cleaned the tokens.","2ecda453":"As we see in this slice of the string, there are meta information provided by the Project Gutenberg in the beginning of the book. These information are not part of the original book, so we will remove them. To do so we are reading the text from the 719th index.  \nThe last 18868 characters are also legal disclaimers added by the Project Gutenberg, so I will also remove them to have the text only."}}