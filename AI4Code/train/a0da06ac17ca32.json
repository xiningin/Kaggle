{"cell_type":{"ad752caf":"code","6aeefe85":"code","7910db97":"code","d8a8d38f":"code","cbcc81bc":"code","3a0f4a9b":"code","6641558f":"code","3a254b1f":"code","2530fe2a":"code","1e2cc197":"code","d6d64c63":"code","fc122fba":"code","509cb292":"code","9ead9932":"code","1a4b68a1":"code","2277263f":"code","26ffe422":"code","8011331f":"code","b653b1ab":"code","6999b804":"code","550c6e87":"code","306dbc9b":"code","ae75861c":"code","12fe58f7":"code","db9f2b21":"code","c2507a73":"code","9466684a":"code","5daf2127":"code","dbacae0d":"code","9547fc25":"code","c9990fe1":"code","139f8f19":"code","7cb500e5":"code","38066f0b":"code","b909542c":"code","0e34001e":"code","9c883a9e":"code","f5f846cd":"code","f9758a7a":"code","1413af5f":"code","6bb08adc":"code","4f461c75":"code","209fded1":"code","97310309":"code","84e395c2":"code","4ee2e681":"code","cbaacdf0":"code","1ff33eb7":"code","7171aa03":"code","387a41f7":"code","5e123837":"code","ca11ad02":"code","3faa959e":"code","84838b0a":"code","9e183907":"code","0e443cb1":"code","e468f094":"code","9f51ecfb":"code","5990a19c":"markdown","8955424a":"markdown","6d32cf1c":"markdown","d68d27ef":"markdown","c313f2da":"markdown","f0344b77":"markdown","cf80adee":"markdown","ceffe505":"markdown","f9e76277":"markdown","a6de6937":"markdown","55e78a9f":"markdown","8021d2b2":"markdown","921c2c26":"markdown","e20f0fdd":"markdown","310619d4":"markdown","fe4f756f":"markdown","d2862ecf":"markdown","113c4d35":"markdown","48df05ce":"markdown","1dc837d9":"markdown","87de6a01":"markdown","86b3bcba":"markdown"},"source":{"ad752caf":"timesteps = 28\nstartDay = 0","6aeefe85":"import pandas as pd\nimport numpy as np\nimport sklearn as skl\nimport matplotlib.pyplot as plt\nimport scipy as sc\nimport gc #importing garbage collector\nimport time\nimport sys\nfrom scipy import signal\nfrom itertools import chain\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline  \n\nSEED = 42\n#Pandas - Displaying more rorws and columns\npd.set_option(\"display.max_rows\", 500)\npd.set_option(\"display.max_columns\", 500)","7910db97":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","d8a8d38f":"#!kaggle competitions download -c m5-forecasting-accuracy","cbcc81bc":"df_train = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\n#df_prices = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sell_prices.csv')\ndf_days = pd.read_csv('..\/input\/m5-forecasting-accuracy\/calendar.csv')\n\ndf_train = reduce_mem_usage(df_train)\n#df_prices = reduce_mem_usage(df_prices)\ndf_days = reduce_mem_usage(df_days)","3a0f4a9b":"df_train=df_train.T","6641558f":"df_train.iloc[:10,:10]","3a254b1f":"df_train.shape","2530fe2a":"df_days.tail(10)","1e2cc197":"df_days.shape","d6d64c63":"#my_df_summary(df_train)","fc122fba":"#my_df_summary(df_prices)","509cb292":"df_days['is_workday'] = 0\ndf_days['is_workday'].loc[df_days['wday']>2] =1\ndf_days['is_workday'] = df_days['is_workday'].astype(np.int8)","9ead9932":"df_days['is_event_day'] = [1 if x ==False else 0 for x in df_days['event_name_1'].isnull()] \ndf_days['is_event_day'] = df_days['is_event_day'].astype(np.int8)","1a4b68a1":"df_days[\"date\"] = pd.to_datetime(df_days['date'])","2277263f":"df_days['week'] = df_days[\"date\"].dt.week\ndf_days['week'] = df_days['week'].astype(np.int8)","26ffe422":"df_days['num_events_week'] = df_days.groupby(by=['year','week'])['is_event_day'].transform('sum')\ndf_days['num_events_week'] = df_days['num_events_week'].astype(np.int8)","8011331f":"df_days['is_event_week'] = [1 if x >0 else 0 for x in df_days['num_events_week']]\ndf_days['is_event_week'] = df_days['is_event_week'].astype(np.int8)","b653b1ab":"df_days.info()","6999b804":"df_days.set_index('date', inplace=True)","550c6e87":"day_after_event = df_days[df_days['is_event_day']==1].index.shift(1,freq='D')\ndf_days['is_event_day_after'] = 0\ndf_days['is_event_day_after'][df_days.index.isin(day_after_event)] = 1\ndf_days['is_event_day_after'] = df_days['is_event_day_after'].astype(np.int8)\n\ndel day_after_event","306dbc9b":"day_before_event = df_days[df_days['is_event_day']==1].index.shift(-1,freq='D')\ndf_days['is_event_day_before'] = 0\ndf_days['is_event_day_before'][df_days.index.isin(day_before_event)] = 1\ndf_days['is_event_day_before'] = df_days['is_event_day_before'].astype(np.int8)\n\ndel day_before_event","ae75861c":"df_days.loc[:, \"is_sport_event\"] = ((df_days[\"event_type_1\"] == \"Sporting\") | (df_days[\"event_type_2\"] == \"Sporting\")).astype(\"int8\")\ndf_days.loc[:, \"is_cultural_event\"] = ((df_days[\"event_type_1\"] == \"Cultural\") | (df_days[\"event_type_2\"] == \"Cultural\")).astype(\"int8\")\ndf_days.loc[:, \"is_national_event\"] = ((df_days[\"event_type_1\"] == \"National\") | (df_days[\"event_type_2\"] == \"National\")).astype(\"int8\")\ndf_days.loc[:, \"is_religious_event\"] = ((df_days[\"event_type_1\"] == \"Religious\") | (df_days[\"event_type_2\"] == \"Religious\")).astype(\"int8\")","12fe58f7":"# adding 'id' column as well as 'cat_id', 'dept_id' and 'state_id', then changing the type to 'categorical'\n#df_prices.loc[:, \"id\"] = df_prices.loc[:, \"item_id\"] + \"_\" + df_prices.loc[:, \"store_id\"] + \"_validation\"\n#df_prices['state_id'] = df_prices['store_id'].str.split('_',expand=True)[0]\n#df_prices = pd.concat([df_prices, df_prices[\"item_id\"].str.split(\"_\", expand=True)], axis=1)\n#df_prices = df_prices.rename(columns={0:\"cat_id\", 1:\"dept_id\"})\n#df_prices[[\"store_id\", \"item_id\", \"cat_id\", \"dept_id\", 'state_id']] = df_prices[[\"store_id\",\"item_id\", \"cat_id\", \"dept_id\", 'state_id']].astype(\"category\")\n#df_prices = df_prices.drop(columns=2)\n#\n## It seems that leadings zero values in each train_df item row are not real 0 sales but mean absence for the item in the store\n## we can safe some memory by removing such zeros\n#release_df = df_prices.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n#release_df.columns = ['store_id','item_id','release']\n#\n#df_prices = pd.merge(df_prices, release_df, on=['store_id','item_id'])\n#del release_df","db9f2b21":"# We can do some basic aggregations\n#df_prices['price_max'] = df_prices.groupby(['store_id','item_id'])['sell_price'].transform('max')\n#df_prices['price_min'] = df_prices.groupby(['store_id','item_id'])['sell_price'].transform('min')\n#df_prices['price_std'] = df_prices.groupby(['store_id','item_id'])['sell_price'].transform('std')\n#df_prices['price_mean'] = df_prices.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n#df_prices['price_norm'] = df_prices['sell_price']\/df_prices['price_max']\n#\n#df_prices['price_nunique'] = df_prices.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n#df_prices['item_nunique'] = df_prices.groupby(['store_id','sell_price'])['item_id'].transform('nunique').astype(np.int8)","c2507a73":"df_days[['wm_yr_wk','month','year']].head()","9466684a":"# I would like some \"rolling\" aggregations\n# but would like months and years as \"window\"\n#calendar_prices = df_days[['wm_yr_wk','month','year']]\n#calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n#df_prices = df_prices.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n#del calendar_prices","5daf2127":"#df_prices['price_momentum'] = df_prices['sell_price']\/df_prices.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n#df_prices['price_momentum_m'] = df_prices['sell_price']\/df_prices.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n#df_prices['price_momentum_y'] = df_prices['sell_price']\/df_prices.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')","dbacae0d":"#df_prices","9547fc25":"#df_prices['release'] = df_prices['release'] - df_prices['release'].min()\n#df_prices['release'] = df_prices['release'].astype(np.int16)","c9990fe1":"gc.collect()","139f8f19":"df_train_full = df_train.copy()","7cb500e5":"startDay = 0\ntimesteps = 14","38066f0b":"daysBeforeEventTest = df_days['is_event_day_before'][1941:1969]\ndaysBeforeEvent = df_days['is_event_day_before'][startDay:1941]\ndaysBeforeEvent.index = df_train.iloc[6:,:].index[startDay:1941]","b909542c":"df_final = pd.concat([df_train, daysBeforeEvent], axis = 1)\ndf_final.columns","0e34001e":"df_final = df_final[startDay:]","9c883a9e":"#Feature Scaling\n#Scale the features using min-max scaler in range 0-1\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0, 1))\ndt_scaled = scaler.fit_transform(df_final.iloc[6:,:])","f5f846cd":"dt_scaled.shape","f9758a7a":"X_train = []\ny_train = []\nfor i in range(timesteps, 1941 - startDay):\n    X_train.append(dt_scaled[i-timesteps:i])\n    y_train.append(dt_scaled[i][0:dt_scaled.shape[1]-1]) ","1413af5f":"X_train = np.array(X_train)\ny_train = np.array(y_train)\nprint('Shape of X_train :'+str(X_train.shape))\nprint('Shape of X_train :'+str(y_train.shape))","6bb08adc":"gc.collect()","4f461c75":"inputs = df_final[-timesteps:]\ninputs = scaler.transform(inputs)","209fded1":"df_final[-timesteps:].shape","97310309":"gc.collect()","84e395c2":"from keras import backend as K","4ee2e681":"# defining rmse as loss function\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true)))","cbaacdf0":"# defining wrmsse as loss function\n#%run .\/WRMSSE.ipynb","1ff33eb7":"%who","7171aa03":"del  df_final, df_train_full, df_days, df_train, dt_scaled, time, sys, signal, reduce_mem_usage","387a41f7":"gc.collect()","5e123837":"# Importing the Keras libraries and packages\nimport tensorflow_probability as tfp\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\n\n# Initialising the RNN\nmodel = Sequential()\n\n# Adding the first LSTM layer and some Dropout regularisation\nlayer_1_units=40\nmodel.add(LSTM(units = layer_1_units, return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\nmodel.add(Dropout(0.2))\n\n# Adding a second LSTM layer and some Dropout regularisation\nlayer_2_units=400\nmodel.add(LSTM(units = layer_2_units, return_sequences = True))\nmodel.add(Dropout(0.2))\n\n# Adding a third LSTM layer and some Dropout regularisation\nlayer_3_units=400\nmodel.add(LSTM(units = layer_3_units))\nmodel.add(Dropout(0.2))\n\n# Adding the output layer\nmodel.add(Dense(units = y_train.shape[1]))\n\n# Compiling the RNN\nmodel.compile(optimizer = 'adam', loss = root_mean_squared_error)\n# alternative loss 'mse' or wrmsse","ca11ad02":"# Fitting the RNN to the Training set\nepoch_no=100 # going through the dataset 32 times\nbatch_size_RNN=44 # with each training step the model sees 44 examples\nfit = model.fit(X_train, y_train, epochs = epoch_no, batch_size = batch_size_RNN)","3faa959e":"plt.plot(fit.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","84838b0a":"model.layers","9e183907":"gc.collect()","0e443cb1":"X_test = []\nX_test.append(inputs[0:timesteps])\nX_test = np.array(X_test)\npredictions = []\n\nfor j in range(timesteps,timesteps + 28):\n    predicted_volume = model.predict(X_test[0,j - timesteps:j].reshape(1, timesteps, 30491))\n    testInput = np.column_stack((np.array(predicted_volume), daysBeforeEventTest[0 + j - timesteps]))\n    X_test = np.append(X_test, testInput).reshape(1,j + 1,30491)\n    predicted_volume = scaler.inverse_transform(testInput)[:,0:30490]\n    predictions.append(predicted_volume)","e468f094":"submission = pd.DataFrame(data=np.array(predictions).reshape(28,30490))\n\nsubmission = submission.T\n    \nsubmission = pd.concat((submission, submission), ignore_index=True)\n\nsample_submission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\n    \nidColumn = sample_submission[[\"id\"]]\n    \nsubmission[[\"id\"]] = idColumn  \n\ncols = list(submission.columns)\ncols = cols[-1:] + cols[:-1]\nsubmission = submission[cols]\n\ncolsdeneme = [\"id\"] + [f\"F{i}\" for i in range (1,29)]\n\nsubmission.columns = colsdeneme\n\nsubmission.to_csv(\"submission_evaluation_newloss_nodrop.csv\", index=False)","9f51ecfb":"predictions","5990a19c":"Next, we start our modelling. We will use LSTM Neural Networks with different layers. \n\nIn general, neural networks are easily described by the following picture:\n1. The neural network model starts with random weights and tries to find the best weights for the different layers, predicting outcomes and comparing them with the true target outcomes. For this it uses the loss function. \n2. The loss function measures the quality of  the network\u2019s output\n3. Then, the loss score is used as a feedback signal to adjust the weights.","8955424a":"![grafik.png](attachment:grafik.png)","6d32cf1c":"Data exists in three files:\n1. File 1: \u201ccalendar.csv\u201d: Contains information about the dates the products are sold.\n    * date: The date in a \u201cy-m-d\u201d format.\n    * wm_yr_wk: The id of the week the date belongs to.\n    * weekday: The type of the day (Saturday, Sunday, \u2026, Friday).\n    * wday: The id of the weekday, starting from Saturday.\n    * month: The month of the date.\n    * year: The year of the date.\n    * event_name_1: If the date includes an event, the name of this event.\n    * event_type_1: If the date includes an event, the type of this event.\n    * event_name_2: If the date includes a second event, the name of this event.\n    * event_type_2: If the date includes a second event, the type of this event.\n    * snap_CA, snap_TX, and snap_WI: A binary variable (0 or 1) indicating whether the stores of CA, TX or WI allow SNAP  purchases on the examined date. 1 indicates that SNAP purchases are allowed.\n\n\n2. File 2: \u201csell_prices.csv\u201d: Contains information about the price of the products sold per store and date.\n    * store_id: The id of the store where the product is sold. \n    * item_id: The id of the product.\n    * wm_yr_wk: The id of the week.\n    * sell_price: The price of the product for the given week\/store. The price is provided per week (average across seven days). If not available, this means that the product was not sold during the examined week. Note that although prices are constant at weekly basis, they may change through time (both training and test set).  \n\n\n3. File 3: \u201csales_train.csv\u201d: Contains the historical daily unit sales data per product and store.\n    * item_id: The id of the product.\n    * dept_id: The id of the department the product belongs to.\n    * cat_id: The id of the category the product belongs to.\n    * store_id: The id of the store where the product is sold.\n    * state_id: The State where the store is located.\n    * d_1, d_2, \u2026, d_i, \u2026 d_1941: The number of units sold at day i, starting from 2011-01-29. \n","d68d27ef":"In the next part we have to decide how many features we want to take for test and training datasets. In the first part we are taking only one extra feature (i.e. limited features).","c313f2da":"In the next step, let's create X_train and y_train by creating different dataframes with 14 days of projection. For y_train we only use sales values for predictions. As we only predict sales, only 0:30490 columns are choosen.","f0344b77":"# Modeling","cf80adee":"When creating X_test, we are using the last 14 days in order to predict day 1915 sales.\nTherefore, in order to predict 1916th day, 13 days from our input data and 1 day from our prediction are used. After that we slide the window one by one, i.e.:\n* 12 days from input data + 2 days from our prediction to predict 1917th day\n* 11 days from input data + 3 days from our prediction to predict 1918th day\n* .....\n* 14 days our prediction to predict last 1941th day sales.\n\n","ceffe505":"# M5 Competition\n\nThe M5 comptition ran from 2 March to 30 June 2020. Basis of the competition is to predict sales forecasts for walm,art stores. For that, we use hierarchical sales data, generously made available by Walmart, starting at the item level and aggregating to that of departments, product categories, stores in three geographical areas of the US: California, Texas, and Wisconsin.\n\nEach row contains an id that is a concatenation of an item_id and a store_id, which is either validation (corresponding to the Public leaderboard), or evaluation (corresponding to the Private leaderboard). \n\nWe are predicting 28 forecast days (F1-F28) of items sold for each row. For the **validation rows**, this corresponds to d_1914 - d_1941, and for the **evaluation rows**, this corresponds to d_1942 - d_1969. (Note: a month before the competition close, the ground truth for the validation rows will be provided.)\n\nDetailed Information can be found [here](https:\/\/mofc.unic.ac.cy\/m5-competition\/) and [here](https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/data).\n\nAn overview of the data given, can be seen here:","f9e76277":"The feature wday has the numerical weekdays in it. We can create a new feature to separate between weekdays (Monday to Friday) and Weekend (Saturday or Sunday). Weekdays equal numbers wday > 2.","a6de6937":"# Submission file","55e78a9f":"# Feature Engineering","8021d2b2":"# Data Overview","921c2c26":"Aknowledgements\nAs a starting point I mainly used two notebooks:  \nhttps:\/\/www.kaggle.com\/bountyhunters\/baseline-lstm-with-keras-0-8#Future-Improvements  \nhttps:\/\/www.kaggle.com\/kuberiitb\/let-s-start-from-here-beginners-data-analysis#Data-Cleaning  \nhttps:\/\/www.kaggle.com\/kyakovlev\/m5-custom-features","e20f0fdd":"![grafik.png](attachment:grafik.png)","310619d4":"# Generating Train and Test Data Option a: Limited features","fe4f756f":"![grafik.png](attachment:grafik.png)","d2862ecf":"![grafik.png](attachment:grafik.png)","113c4d35":"Please feel free to comment!\nThanks for upvoting!","48df05ce":"# Importing Modules and functions","1dc837d9":"## Part a: Modeling with Limited Features","87de6a01":"As we want to have each day as row and 30490 items' sales as columns (features) we take the transpose of the dataframe","86b3bcba":"For each X_train item, 14 past days' sales and 14 daysBeforeEvent feature are included. So one element of X_train's shape is (14, 30491). For y_train we are predicting one day sales of 30490 items therefore one element of y_train's shape is (1, 30490)"}}