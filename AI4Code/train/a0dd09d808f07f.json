{"cell_type":{"23ac0165":"code","bceefae0":"code","1a974888":"code","699cd924":"code","e8c5ddf2":"code","11fb3fb2":"code","ae4a26f5":"code","52fcbe73":"code","a0cdbb62":"code","e60362f5":"code","bebc1756":"code","c654b6fc":"code","b78e60ac":"code","daa34c58":"code","9edc46cf":"code","85698224":"code","c372b67f":"code","ce871864":"code","4b917d13":"code","b516b0a5":"code","73e174c0":"code","2d8abc88":"code","89c965a7":"code","d1cf1e0f":"code","d55fa8d1":"code","c9b0726f":"code","51456826":"code","e497b5b9":"code","828f2e8d":"code","3bca939e":"code","b98adf6d":"code","d1161dfd":"code","6379acfd":"code","cc179fff":"code","f7cde474":"code","646490e7":"code","384d0430":"code","25e0d681":"code","95c15612":"code","75f5c2a3":"code","b84488bd":"code","e1d37b1f":"code","c94eebea":"code","d86482be":"code","cd5ba234":"code","0d2bbbd9":"code","a06e7a3e":"code","5ee46332":"code","94373ed2":"code","ab53f400":"code","40c42dd8":"code","db0d122c":"markdown","64021a2f":"markdown","f67632ad":"markdown","639bdc0e":"markdown","b3e08c9f":"markdown","c72ae698":"markdown","464755b9":"markdown","cacbedfb":"markdown","176558a8":"markdown","0ceba527":"markdown","81687d46":"markdown","14d04dff":"markdown","9fb24fd0":"markdown","0f578bc2":"markdown","35fe955c":"markdown","bf673934":"markdown","a2c17280":"markdown","3c021e55":"markdown","494f1cc4":"markdown","40d0ae26":"markdown","816b838a":"markdown","b0c18b73":"markdown","7d8f61b5":"markdown","8cf42d2b":"markdown","e3921ed1":"markdown","c2b3dbbf":"markdown","b4e71ac1":"markdown","7f18c96e":"markdown","0e66ee3b":"markdown","595db721":"markdown","7a951b1d":"markdown","41ff09a2":"markdown","9dfbac5a":"markdown","c79e2957":"markdown"},"source":{"23ac0165":"import os\nimport math\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport itertools\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstopw = stopwords.words('english')\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotly.offline import iplot\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable = True , offline = False)\n\n\nfrom transformers import AdamW\nimport transformers\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nfrom transformers import BertTokenizer, BertConfig, BertForSequenceClassification\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n\nfrom tqdm import tqdm","bceefae0":"df = pd.read_csv('..\/input\/bbc-fulltext-and-category\/bbc-text.csv')\n\nfrom sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\ndf['label'] = enc.fit_transform(df['category'])\n\nclass_names = df.groupby(['category', 'label']).count().reset_index().loc[:,['category', 'label']]\nclass_name_tokenizers = {}\nfor class_name in class_names.category:\n    class_name_tokenizers[class_name] = [tokenizer.encode(class_name)[1],class_names[class_names['category'] == class_name]['label'].iloc[0]]","1a974888":"#Class names are encoded one hot encoding method and encoded with Bert tokenizer.\nclass_name_tokenizers","699cd924":"category_counts = df['category'].value_counts()\ncategories = category_counts.index\n\nfig = plt.figure(figsize = (12,5))\nax = fig.add_subplot(111)\nsns.barplot(x = category_counts.index , y = category_counts)\nfor a, p in enumerate(ax.patches):\n    ax.annotate(f'{categories[a]}\\n' + format(p.get_height(), '.0f'), xy = (p.get_x() + p.get_width() \/ 2.0, p.get_height()), xytext = (0,-25), size = 13, color = 'white' , ha = 'center', va = 'center', textcoords = 'offset points', bbox = dict(boxstyle = 'round', facecolor='none',edgecolor='white', alpha = 0.5) )\nplt.xlabel('Categories', size = 15)\nplt.ylabel('The Number of News', size= 15)\nplt.xticks(size = 12)\n\nplt.title(\"The number of News by Categories\" , size = 18)\nplt.show()","e8c5ddf2":"def clean(text, punctuation = False, stopword = False):\n    # filter to allow only alphabets\n#    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n    # Deleting Stopwords\n    if stopword == True:\n        text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n    \n        text = text.split()\n        text = [word for word in text if word not in stopw]\n        text = \" \".join(text)\n    # Seperating the punctuations to tokenize them\n    if punctuation == True:\n        punc = '@#!?+&*[]-%.:\/();$\u00a3=><|{}^'\n        for p in punc:\n            text = text.replace(p, f' {p} ')    \n\n    # Urls\n    text = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", text)\n    text = re.sub(r\"  \", \" \", text)\n    text = re.sub(r\"   \", \" \", text)\n    return text\n      ","11fb3fb2":"df['cleaned_from_stopw'] = df['text'].apply(lambda x : clean(x, stopword = True))\ndf['cleaned'] = df[\"text\"].apply(lambda x : clean(x,punctuation = True))\ndf[\"count\"] = df[\"cleaned\"].apply(lambda x: len(x.split()))","ae4a26f5":"plt.figure(figsize = (8,8))\nsns.distplot(df['count'] )\nplt.xlim(0,1000)\nplt.xlabel('The number of words', fontsize = 16)\nplt.title(\"The Number of Words Distribution\", fontsize = 18)\nplt.show()","52fcbe73":"df =df.iloc[:-64,:].reset_index(drop= True)\ndf_test = df.iloc[-64:,:].reset_index(drop= True)","a0cdbb62":"def create_n_gram(df, category= '',text_column = 'cleaned_from_stopw' ,n_gram = 1):\n    n_gram_dict = {}\n    if category != '':                                              # This condition is created in case of filtering the label.\n        df = df[df['category'] == category]\n\n    for k in tqdm(df['cleaned_from_stopw']):\n\n        for i in range(len(k)):\n            words = k.split()[i:i+n_gram]\n            words = \" \".join(words)\n            \n            if (len(words.split()) % n_gram) > 0 or words == '' :       # This condition is created to drop last words of text.  \n                continue\n            elif words in n_gram_dict.keys():                           \n                n_gram_dict[words] +=1\n            else:                                                      # We add new word into dictionary. If the word is already in the dictionary, it adds 1 in values.\n                n_gram_dict[words] =1\n\n    results = pd.DataFrame.from_dict([n_gram_dict]).T.reset_index()\n    results.columns = [category+'_n_grams',category+ '_counts']\n\n    return results","e60362f5":"all_words = create_n_gram(df)\nprint(\"There are {} unique words in the dataset.\".format(len(all_words)))","bebc1756":"n_gram_dict = {}\nfor class_name in class_name_tokenizers.keys():\n    for i in range(1,4):\n        temp_result =create_n_gram(df,category = class_name, n_gram = i)\n        temp_result = temp_result.sort_values(by = class_name + \"_counts\", ascending = False).head(30).reset_index(drop = True)\n        temp_result.columns = [class_name + '_n_grams'+ str(i),class_name + '_counts_'+str(i)]\n        n_gram_dict[class_name + str(i)] = temp_result\nnp_result = np.ones((30,1))\nn_gram_result = pd.DataFrame(np_result)\nn_gram_result.drop(columns = [0], inplace= True)\nfor key in n_gram_dict.keys():\n    n_gram_result = n_gram_result.join(n_gram_dict[key])","c654b6fc":"n_gram_result.iloc[:,:6]","b78e60ac":"n_gram_result.iloc[:,6:12]","daa34c58":"n_gram_result.iloc[:,12:18]","9edc46cf":"n_gram_result.iloc[:,18:24]","85698224":"n_gram_result.iloc[:,24:30]","c372b67f":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical","ce871864":"glove_embeddings = np.load('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', allow_pickle=True)\nprint(\"There are {} words and {} dimensions in Glove Dictionary. And, the number of dimenson is {}. I used the word 'sister' as an example\".format(len(glove_embeddings.keys()),len(glove_embeddings['sister']),len(glove_embeddings['sister'])))\n","4b917d13":"count = 0\nuncovered_words = {}\ncovered_words = {}\nembedding_matrix = {}\nfor text in df.cleaned:\n    text = text.split()\n    for word in text:\n        \n        if word not in glove_embeddings.keys():\n            count += 1\n            if word not in uncovered_words:\n                uncovered_words[word] = 1\n            else:\n                uncovered_words[word] += 1\n        \n        else:\n            if word not in covered_words:\n                covered_words[word] = 1\n            else:\n                covered_words[word] += 1\nprint(\"---There are {} words in the whole dataset, and {:.2f}% of the words aren't covered by Glove---\".format((len(uncovered_words) + len(covered_words)),len(uncovered_words) \/ (len(uncovered_words)+len(covered_words))*100))\nprint('---Top 20 most commong uncovered words---')\nprint(pd.DataFrame([uncovered_words]).T.reset_index().sort_values(by = 0, ascending = False).head(20))","b516b0a5":"df['cleaned'][0]","73e174c0":"tokenizer_keras = Tokenizer(num_words = 29479, oov_token = \"<OOV>\")\ntokenizer_keras.fit_on_texts(df['cleaned'])\nword_index = tokenizer_keras.word_index                # After tokenization, we get all the words and characters in the dataset. \nvocab_size_keras = len(word_index)\nembedding_dim = len(glove_embeddings['the'] )          # All of words in glove have same dimmensions(300), so we choose one example \"the\".\nlist(word_index.items())[0:10]","2d8abc88":"tokenizer_keras = Tokenizer(num_words = 29479, oov_token = \"<OOV>\")\ntokenizer_keras.fit_on_texts(df['text'])\nword_index = tokenizer_keras.word_index                # After tokenization, we get all the words and characters in the dataset. \nvocab_size_keras = len(word_index)\n\n\n# Creating embedding matrix for all words in dataset\nembedding_matrix = np.zeros((vocab_size_keras+1,embedding_dim))       # We added 1 to vocab size because tokenizer starts with 1, so 0th is not gonna used\nfor word, i in word_index.items():\n    if word in glove_embeddings.keys():\n        embedding_vector = glove_embeddings[word]\n        embedding_matrix[i] = embedding_vector","89c965a7":"tokenized = pd.DataFrame([word_index]).T.reset_index()\ntokenized.columns = ['words','index']\ntemp_emd_matrix = pd.DataFrame(embedding_matrix).reset_index()\ntemp_emd_matrix = temp_emd_matrix.drop(0, axis = 0)\ndf_embedding_matrix = pd.merge(tokenized, temp_emd_matrix, on = 'index')\ndf_embedding_matrix.rename(columns = {\"index\": \"tokens\"})\ndf_embedding_matrix","d1cf1e0f":"def prepare_data(df,tokenizer, max_len= 64):\n    sequences = tokenizer.texts_to_sequences(df['cleaned'])\n    padded = pad_sequences(sequences, maxlen = max_len, padding = 'post', truncating = 'post')\n    labels = tf.keras.utils.to_categorical(df['label'])\n    return padded, labels","d55fa8d1":"padded, labels = prepare_data(df_test,tokenizer_keras, max_len = 512)","c9b0726f":"training_portion =0.75\ntraining_size = int(len(df) * training_portion)\npadded_training = padded[:training_size]\nlabels_training = labels[:training_size]\npadded_val = padded[training_size:]\nlabels_val = labels[training_size:]","51456826":"\nmodel_glove = tf.keras.Sequential()\nmodel_glove.add(tf.keras.layers.Embedding( vocab_size_keras+1,embedding_dim, input_length = max_len, weights = [embedding_matrix], trainable = False))\nmodel_glove.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True)))\nmodel_glove.add(tf.keras.layers.Dropout(0.5))\nmodel_glove.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\nmodel_glove.add(tf.keras.layers.Dropout(0.5))\nmodel_glove.add(tf.keras.layers.Dense(64, activation = 'relu'))\nmodel_glove.add( tf.keras.layers.Dense(5 , activation = 'softmax'))\n\nmodel_glove.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])\nmodel_glove.summary()\n\n","e497b5b9":"model_glove.fit(padded_training,labels_training,epochs= 10, verbose =1 , validation_data = (padded_val, labels_val))","828f2e8d":"test_data, test_label = prepare_data(df_test,tokenizer_keras, max_len = 512)\ntest_prediction = model_glove.predict(test_data)\n\noutput_flat = np.argmax(test_label, axis=1).flatten()\nprediction_flat = np.argmax(test_prediction, axis=1).flatten()\ntest_accuracy = np.sum(prediction_flat == output_flat) \/ len(output_flat)\nprint(\"The test set includes {} texts and the accuracy is {}\".format(len(output_flat), test_accuracy))","3bca939e":"# This is for tensorflow\nfrom numba import cuda \ndevice = cuda.get_current_device()\ndevice.reset()\n# This is for pytorch\ntorch.cuda.empty_cache()","b98adf6d":"print(tokenizer.encode(df.cleaned[7], max_length = 128 ) + [0] * 128)    # This is example of input_ids token and max length is 256","d1161dfd":"class process_dataset:\n    '''\n    This class is created to to prepare the dataset for data loader.\n    Bert needs 3 token vecors, so processing_data function or processing_data_encode_plus function will prepare these tokens. Both returns same vectors. Both can be used.\n    This getitem method returns data dictionary that includes tokens and labels.\n    Labels are encoded to 5 dimensions vectors.\n    \n    '''\n    def __init__(self, df, token_ids_with_label,  max_len, tokenizer, text_column , label_column):\n        self.df = df\n        self.text = self.df[text_column]\n        self.label = self.df[label_column]\n       # self.encoded_label = self.df['label']\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.token_ids_with_label = token_ids_with_label\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self,index):\n        \n        row = self.text[index]\n        row_label = self.label[index]\n        convert_token_ids = self.token_ids_with_label\n        input_ids, attention_masks, token_type_ids, label = processing_data(row, row_label, self.max_len, convert_token_ids)\n\n        y =torch.LongTensor(label)\n\n        emb = nn.Embedding(5, len(y))\n        emb.weight.data = torch.eye(5)\n        label_one_hot = emb(torch.LongTensor(y))\n        \n        data = {\n            'input_ids' : torch.tensor(input_ids),\n            'attention_masks' : torch.tensor(attention_masks),\n            'token_type_ids' : torch.tensor(token_type_ids),\n            'labels' : label_one_hot\n        }\n        \n        return data\n    \ndef processing_data( row, row_label, max_len, convert_token_ids = False):\n    if convert_token_ids == False:\n\n        label = [class_name_tokenizers[row_label][1]]\n    \n        temp_input_ids = tokenizer.encode(row, max_length = max_len)\n\n        pad_len = max_len - len(temp_input_ids)  \n\n        input_ids =temp_input_ids + [0] * pad_len\n\n        attention_masks= [1] * len(temp_input_ids)+ [0] * pad_len\n\n        token_type_ids =  [0] * max_len\n        \n        return np.array(input_ids), np.array(attention_masks), np.array(token_type_ids), np.array(label)\n\n    else:     \n        \n        \n        label_token = tokenizer.encode(row_label)\n        \n        label = [class_name_tokenizers[row_label][1]]\n\n        max_len = max_len - (len(label_token)-1)\n\n        temp_input_ids = tokenizer.encode(row, max_length = max_len)\n\n        pad_len = max_len - len(temp_input_ids) \n\n        input_ids = label_token  + temp_input_ids[1:] + [0] * pad_len                                    \n\n        attention_masks= [1] *len(label_token) + [1] * len(temp_input_ids[1:]) + [0] * pad_len\n\n        token_type_ids = [0] *len(label_token) + [1] * len(temp_input_ids[1:])+  [0] * pad_len\n\n        return np.array(input_ids), np.array(attention_masks), np.array(token_type_ids), np.array([label])\n\n# Second way to create input_ids, attention_masks, and token_type_ids\n\ndef processing_data_encode_plus(df_text, df_label, max_len , convert_token_ids = False):\n\n    input_ids= []\n    attention_masks = []\n    token_type_ids = []\n    if convert_token_ids == False:\n\n        encoded = tokenizer.encode_plus(df_text,                                   \n                                   add_special_tokens = True,\n                                   max_length = max_len,\n                                   pad_to_max_length = True,\n                                   return_token_type_ids = True,\n                                   return_attention_mask = True\n                                   )\n        input_ids = encoded['input_ids']\n        attention_masks = encoded['attention_mask']\n        token_type_ids = encoded['token_type_ids']\n        \n        label = tokenizer.encode(df_label)[1]\n\n        return input_ids,attention_masks, token_type_ids, label\n    \n    else:\n\n        encoded = tokenizer.encode_plus(df_label,\n                                    df_text,                                   \n                                   add_special_tokens = True,\n                                   max_length = max_len,\n                                   pad_to_max_length = True,\n                                   return_token_type_ids = True,\n                                   return_attention_mask = True\n                                   )\n        input_ids = encoded['input_ids']\n        attention_masks = encoded['attention_mask']\n        token_type_ids = encoded['token_type_ids']\n        \n        label = tokenizer.encode(df_label)[1]\n        \n        return input_ids,attention_masks, token_type_ids, label","6379acfd":"#df[\"input_ids\"],df[\"attention_masks\"], df['token_type_ids'],  df['label']= map(list, zip(*df[['text', 'category']].apply(lambda x: processing_data(x.text, x.category,convert_token_ids = False, max_len = 512), axis = 1)))\n\n#df[\"input_ids\"],df[\"attention_masks\"], df['token_type_ids'],  df['label']= map(list, zip(*df[['text', 'category']].apply(lambda x: processing_data_encode_plus(x.text, x.category,convert_token_ids = True, max_len = 512), axis = 1)))","cc179fff":"def get_data_loader(df,train_index, val_index,tokenizer, batch_size = 16, max_len = 128, num_workers = 0,text_column = 'text', label_column = 'category'):\n    df_train = df.iloc[train_index].reset_index(drop= True)\n    df_val = df.iloc[val_index].reset_index(drop= True)\n    train_loader = torch.utils.data.DataLoader(process_dataset(df_train, tokenizer = tokenizer, token_ids_with_label = False, max_len = max_len, text_column = text_column, label_column = label_column),\n                                        batch_size = batch_size,\n                                        shuffle = False,\n                                        drop_last=True,\n                                          pin_memory=False,\n                                        num_workers = num_workers)\n                                              \n    \n    test_loader = torch.utils.data.DataLoader(process_dataset(df_val, tokenizer = tokenizer, token_ids_with_label = False,max_len = max_len, text_column = text_column, label_column = label_column),\n                                             batch_size= batch_size,\n                                             shuffle = False,\n                                            drop_last=True,\n                                              pin_memory=False,\n                                             num_workers = num_workers)\n    return {'train' : train_loader, \"val\": test_loader}\n\ndef get_test_loader(df,tokenizer, batch_size = 16, max_len = 128, num_workers = 0,text_column = 'text', label_column = 'category'):\n    \n    test_loader = torch.utils.data.DataLoader(process_dataset(df, tokenizer = tokenizer, token_ids_with_label = False, max_len = max_len, text_column = text_column, label_column = label_column),\n                                             batch_size = batch_size,\n                                             shuffle = False,\n                                             num_workers = num_workers)\n    return test_loader","f7cde474":"class NewsModel(nn.Module):\n    def __init__(self):\n        super(NewsModel, self).__init__()\n        conf = transformers.BertConfig()\n        conf.output_hidden_states = True\n        self.model = transformers.BertModel.from_pretrained('bert-base-uncased', config = conf)\n        self.dropout = nn.Dropout(0.5)\n        self.classifier = nn.Linear(conf.hidden_size, 5)\n        nn.init.xavier_normal_(self.classifier.weight)\n    def forward(self, input_ids, attention_masks , token_type_ids):\n        out = self.model(input_ids,attention_masks, token_type_ids)\n        last_hidden_state_cls = out[0][:,0,:]                                 # This is the output of bert layer like pictures\n\n        output = self.dropout(last_hidden_state_cls)\n            \n        logits = self.classifier(output)\n                \n        return logits\n            ","646490e7":"def loss_fn(output, predicted ):\n    ce_loss = nn.BCEWithLogitsLoss()\n    loss = ce_loss(output, predicted)\n    return loss","384d0430":"def flat_accuracy(output, prediction):\n    prediction_flat = np.argmax(prediction, axis=1).flatten()\n    output_flat = np.argmax(output, axis=1).flatten()\n    return np.sum(prediction_flat == output_flat) \/ len(output_flat)","25e0d681":"def train_model(model, data_loaders, criterion, optimizer, num_epochs, device, flat_accuracy):\n\n    for epoch in range(num_epochs):\n\n        for phase in ['train', 'val']:\n\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n            losses = []        \n            epoch_loss = 0.0\n            accuracy = 0.0\n            counter = 0.0\n            best_accuracy = 0\n            dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n\n            for data in data_loaders[phase]:\n\n                input_ids = data['input_ids'].to(device)\n                attention_masks = data['attention_masks'].to(device)\n                token_type_ids = data['token_type_ids'].to(device)\n                output = data['labels'].to(device, dtype = torch.long)\n                \n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    \n\n\n\n                    prediction = model( input_ids, attention_masks,  token_type_ids)\n                    output = torch.max(output.float(), 1)[0]\n\n                    loss = criterion(output,prediction)\n                    if phase == 'train':\n\n                        loss.backward()\n                        optimizer.step()\n                    losses.append(loss.item())\n\n                    prediction = prediction.detach().cpu().numpy()\n                    output = output.detach().cpu().numpy()\n\n                    accuracy += flat_accuracy(output, prediction)\n                    counter +=1\n                    epoch_loss += loss.item() * len(input_ids)\n            epoch_loss = epoch_loss \/len(data_loaders[phase].dataset)\n            epoch_accuracy =accuracy \/ counter\n            if (phase == 'val' and epoch_accuracy > best_accuracy):\n                best_model_weights = model.state_dict()\n                best_accuracy = epoch_accuracy\n            print(\"Epoch {} - {}- Loss {:.3f} - Accuracy: {:.3f} , number of examples :{}\".format(epoch,phase,epoch_loss,epoch_accuracy,len(data_loaders[phase].dataset)))\n\n    return best_model_weights, losses , best_accuracy","95c15612":"data_loaders = get_data_loader(df, range(100),range(100),text_column = 'text', label_column = 'category',tokenizer = tokenizer)","75f5c2a3":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\nstop_cnt = 0\nmodel = NewsModel()\n\nfor i in data_loaders['train']:\n    \n    output = i['labels']\n    print(i[\"input_ids\"])\n    print(i[\"attention_masks\"])\n    print(i[\"token_type_ids\"])\n    prediction = model(i[\"input_ids\"],i[\"attention_masks\"],i[\"token_type_ids\"]) \n    stop_cnt+=1\n    if stop_cnt == 1:\n        break\n        \noutput = torch.max(output.float(),1)[0]\nprediction = prediction.detach().cpu().numpy()\noutput = output.detach().cpu().numpy()\n","b84488bd":"print(output)\nprint(prediction)","e1d37b1f":"skf = StratifiedKFold(n_splits = 5, shuffle = True)\nbest_accuracy = 0\nfor fold, (train_index, val_index) in enumerate(skf.split(df, df.category),start = 1):\n\n    print(f\"fold :{fold}\")\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n    model = NewsModel()\n    model.to(device)\n    optimizer = AdamW(model.parameters(), lr = 3e-5)\n    data_loaders = get_data_loader(df,train_index, val_index ,tokenizer = tokenizer, batch_size = 64, max_len = 128,text_column = 'cleaned', label_column = 'category')\n    criterion = loss_fn\n    \n\n    model_weights, losses, accuracy = train_model(model, data_loaders, criterion, optimizer,flat_accuracy=flat_accuracy, num_epochs = 4, device = device)\n    \n    if accuracy > best_accuracy:\n        final_model_weights = model_weights        # This keeps the best model's weights.\n        best_accuracy = accuracy","c94eebea":"print('The best accuracy of the validation dataset is {:.2f}'.format(best_accuracy*100))","d86482be":"def prediction_class(final_model_weights, text):\n    batch_test_losses = []\n    batch_test_accuracies = []\n    best_test_accuracy = 0\n    use_cuda = torch.cuda.is_available()\n    device = torch.device('cuda:0' if use_cuda else 'cpu')\n    model = NewsModel()\n\n    model.load_state_dict(final_model_weights)\n    class_names = pd.DataFrame(class_name_tokenizers).T\n    test_data_loaders = get_test_loader(text, tokenizer, batch_size = 1, max_len = 128, num_workers = 0,text_column = 'cleaned', label_column = 'category')\n    for i, data in enumerate(test_data_loaders):\n\n        prediction = model(data['input_ids'], data['attention_masks'], data['token_type_ids'])\n        output = data[\"labels\"]\n        output =torch.max(output.float(),1)[0]\n\n        prediction = prediction.detach().cpu().numpy()\n        output = output.detach().cpu().numpy()\n#        print(output)\n#        print(prediction)\n#        print(text.loc[i,'text'])\n#        print(\"Real output is {}, Prediction is :{}\\n\".format(text.loc[i,'category'], class_names[class_names[1] == np.argmax(output)].index[0]))\n        batch_test_accuracy = flat_accuracy(output, prediction)\n        batch_test_accuracies.append(batch_test_accuracy)\n        test_accuracy = sum(batch_test_accuracies) \/ len(batch_test_accuracies)\n    return print(\"Test dataset includes {} texts and the test set accuracy is {:.2f}.\".format(len(batch_test_accuracies),test_accuracy))","cd5ba234":"prediction_class(final_model_weights, df_test)","0d2bbbd9":"torch.cuda.empty_cache()","a06e7a3e":"tf.keras.backend.clear_session()\nimport tensorflow_hub as hub","5ee46332":"def build_model(max_len = 128):\n    ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32)\n    masks = tf.keras.layers.Input(shape =(max_len,), dtype = tf.int32)\n    token_ids = tf.keras.layers.Input(shape =(max_len,), dtype = tf.int32)\n\n    bert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",  trainable=True)\n    pooled_output, sequence_output = bert_layer([ids, masks ,token_ids])\n\n    output = sequence_output[:,0,:]\n\n    out = tf.keras.layers.Dropout(0.5)(output)\n\n    out = tf.keras.layers.Dense(5, activation = 'softmax')(out)\n\n    model = tf.keras.models.Model(inputs = [ids,masks,token_ids], outputs= out)\n\n    optimizer = tf.optimizers.Adam(learning_rate = 3e-5)\n\n    model.compile(loss ='categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n    \n    return model","94373ed2":"def encode(df, max_len = 128):\n\n    ids = np.zeros((len(df), max_len), dtype = 'float32')\n    masks = np.zeros((len(df), max_len), dtype = 'float32')\n    token_ids = np.zeros((len(df), max_len), dtype = 'float32')\n    labels = np.zeros((len(df), max_len), dtype = 'float32')\n\n    ids, masks, token_ids, labels=map(list, zip(*df[['text', 'category']].apply(lambda x: processing_data(x.text, x.category,convert_token_ids = False, max_len = max_len), axis = 1)))\n\n    ids = np.array(ids, dtype ='float32')\n    masks = np.array(masks, dtype ='float32')\n    token_ids = np.array(token_ids, dtype ='float32')\n    labels = tf.keras.utils.to_categorical(np.array(labels))\n    return ids,masks, token_ids, labels","ab53f400":"skf = StratifiedKFold(n_splits = 4, shuffle = True)\n\nids, masks, token_ids, labels = encode(df, max_len = 128)\n\nfor k , (train_index,val_index) in enumerate(skf.split(ids, labels.argmax(1)), start =1):\n    ids_train = ids[train_index,:],\n    masks_train = masks[train_index,:]\n    token_train = token_ids[train_index,:]\n    labels_train = labels[train_index,:]\n    ids_val = ids[train_index,:],\n    masks_val = masks[train_index,:]\n    token_val = token_ids[train_index,:]\n    labels_val = labels[train_index,:]\n    print(\"fold :{}\".format(k))\n    model = build_model()\n    history = model.fit((ids_train, masks_train, token_train), labels_train, epochs = 3, verbose = 1, batch_size = 32, validation_data = ((ids_val, masks_val, token_val), labels_val))","40c42dd8":"ids_test, masks_test, token_ids_test, labels_test = encode(df_test, max_len = 128)\nprediction = model.predict((ids_test, masks_test, token_ids_test))\noutput = tf.keras.utils.to_categorical(df_test['label'])\ntest_result = flat_accuracy(output, prediction)\nprint(\"The test set includes {} texts and the accuracy is {}\".format(len(output), test_result))","db0d122c":"# # The BBC News Glove(TF) vs Bert(TF-Pytorch) \n\n* The dataset includes 2225 News and all are labeled.\n* There are 5 different label categories for these news.\n* This dataset is very clean in terms of nlp because it is written by bbc editors. \n* Three different Neural Networks will be used to predict labels.\n* I have created BERT with two different libraries to see different. I also more familiar with Tensorflow, so it helps to understand how Pytorch works.\n\n*1) Glove Embeddings*\n* Glove provides vector representation for words. I will use Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors).\n* It basically provides 300 dimensinal vector. Every dimension represents different feature of word. For example, one is positivity value one is negativity and so on.\n* This helps to create emmbedding matrix for Neural Network input.\n* The bidirectional layer will be used in neural network. Bidirectional layer is impoartant to train nlp dataset because it trains the sentence or text as is and reversed of sentence.\n* For example, \"I like a bar that plays jazz music\" - \"I like a bar of white chocolate not small piece\". As you can see first 3 words are same. If we don't train with bidirectional layers, we cannot understand real meaning of the bar.\n\n*2) BERT*\n* BERT stands for Bidirectional Encoder Representation from Transformers\n* It is pretrained model and as you can see from its name, it pre-trained deep bidirectional with own embedding matrix.\n* Bert has hidden state that has 768 dimensions that is similar to embedding matrix.\n* BERT-Base-Uncased: 12-layers, 768-hidden_size, 12-heads, 110M parameters.\n* The max number of tokens is 512 for BERT.\n\n\n*References*\n* https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\/output#0.-Introduction-and-References\n* https:\/\/www.kaggle.com\/shoheiazuma\/tweet-sentiment-roberta-pytorch\n* https:\/\/www.kaggle.com\/abhishek\/bert-base-uncased-using-pytorch\n* http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/    (Pictures)\n\n* I have inspired from these notebooks. Please upvote them, as well.","64021a2f":"* Labels is converted into binary vector. All of the models use these labels.","f67632ad":"# N-Grams\n* This function returns the unique words(bigram) or sequence of words and its occurrence frequency.","639bdc0e":"* Glove model test data pediction","b3e08c9f":"* This function is for prediction from unseen dataset.","c72ae698":"* Cuda library's empty_cache() method clears the gpu for pytorch, so in next neural network gpu memory is not gonna be overload.","464755b9":"# Impporting Dataset\n* Importing the dataset and found all class names.\n* The labels are encoded with one hot encoding. ","cacbedfb":"# Business N-grams","176558a8":"![bert-output-tensor-selection.png](attachment:bert-output-tensor-selection.png)![bert-output-cls-senteence-embeddings.png](attachment:bert-output-cls-senteence-embeddings.png)","0ceba527":"# Import Libraries","81687d46":"* We will find the words that Glove doesn't include and proportion of uncovered words.\n* As you can see from the below table, most common uncovered words are the celebrities, politician and athletes.","14d04dff":"* Cuda library's reset method clears the gpu, so in next neural network gpu memory is not gonna be overload.","9fb24fd0":"* The model improved from 96% to 98%. ","0f578bc2":"* Creating embedding matrix dataframe to show.","35fe955c":"* The picutes below shows that the output of Bert Layer (last_hidden_states).\n* In the pictures, there are 2000 sentences and max length is 66. (We have 2225 news(64 batch size) and max length is 128).\n* Actually, every sentence's shape is 66x768, but we are selecting only fist value of each feature(there are 768). It represents whole sentence.\n* Hence, the input of classifier is 768 features and output is 5 category labels.","bf673934":"# Entertainment N-grams","a2c17280":"* We tokenize all off the words from cleaned dataset.\n* We use oov_token argument for unkown words like above.\n* As you can see from below, we used cleaned data, but this doesn't matter for keras tokenizer because the tokenizer delete all special character. Cleaned data helps to improve the model in Bert. ","3c021e55":"* We saved the weights of best model and use in the prediction class method.\n* You can see the accuracy of the test set. It predicted correctly all test set. It is amazing result.","494f1cc4":" # Bert - Tensorflow\n* Compared to pytorch, tensorflow process is easier. \n* There is another library to load bert_layer for tensorflow. First bert library needs special data type that is converted by pytorch data_loader, so I loaded by tensorflow hub library.\n","40d0ae26":"* These are the first batch of labels and prediction before training. As you can see the predictions are really bad before training.","816b838a":"* These are the model inputs. It shows first batch from data_loader.","b0c18b73":"# Summary\n\n* This dataset was very clean and there weren't many special characters because these news is written by BBC editors. So, our final accuracy is very high.\n* BERT performs almost perfectly in tensorflow model, the validation accuracy is around 99.5%.\n* **BERT(Pytorch - Tensorflow) models predicted all 64 unseen News correctly** which is great. However, **in the model with Glove embeddings, test set accuracy was 0.875**.\n\n\n* PS: I haven't builded many neural networks with PyTorch, so probably, this is the reason of that the accuracy is lower in my Pytorch model than Tensorflow model. Please give me recommendations if you see anything that can improve the model.\n* If you like it pelase upvote, thanks.","7d8f61b5":"# Tech N-grams","8cf42d2b":"* Splitting the dataset into training and validation.","e3921ed1":"# Bigram, 2-Gram, and 3-Gram\n* The new dataframe is created to find top 30 sequence of words in terms of categories.","c2b3dbbf":"* Tensorflow performs better than pytorch. I was expecting similar result, but this result is 2.5% better than pytorch.","b4e71ac1":"# Politics N-grams","7f18c96e":"# Cleaning the texts\n* This dataset doesn't require a lot of cleaning because it has written by editor of BBC. There are not links special characters or emojis.\n* If we work with twitter text, it needs lot of cleaning.\n* Just two different condition added for cleaning. First is punctuation and second is stopwords.\n* Punctuation is helping to split the special characters from the words. For example \" $15bn \" is becoming \" 15 bn \". It is not necessary for Glove because it deletes all of them. Howevet, Bert needs them. Seperating punctuations is not compulsory for Bert because it is intelligent to seperate them, but not always, so we will do that to improve model. ","0e66ee3b":"# Bert - Pytorch\n* If you usually use tensorflow like me, pytorch is a little bit complicated but it has more comprehensive. We will basically build network from strach.\n* Glove is vector representation of words, but Bert is pre-trained model and it need special tokenization. There are three special token sequences for each text input.\n* Input vectors are input_ids, attention_masks and token_type_ids.\n* Input_ids ---> [[CLS] + sentence tokens + [SEP] + 0 0 0] ---> [101 + sentence tokens + 102 + 0 0 0 0]. You can see below. We add 0 end of the vector based on max length.\n* For example, if the sentence is 'I like ice cream' and max length is 10. ---> input_ids: [101, token, token, token, token, 102, 0, 0, 0, 0 ]\n* attention_masks vector contains information about masked tokens. 1 is for not masked and 0 for masked. \n* token_type_ids, we actually don't need token type ids because our model is for classification, so we will assign 0 for this vector. If you build bert model answer the questions you will need it. So, I have created argument to use the token_type_ids or not for future projects.\n\n* I have also created two function to create these tokens both function give same results. I just want to show how to create the token from strach and with tokenizer method. Second is easier. \n\n","595db721":"# Pytorch steps\n* Process data ---> process_dataset ---> data_data_loader ---> News_model(Bert_layer) ---> train_model ---> Results","7a951b1d":"* The last 64 texts are splitted for test unseen data.","41ff09a2":"# Glove Embedding - Tensorflow","9dfbac5a":"* I cleared the cache to prevent cuda overload, but I still cannot use the higher max length or higher batch size. If anyone has solution, please give advice :)","c79e2957":"# Sport N-grams"}}