{"cell_type":{"82eba4c9":"code","af12943e":"code","69a399bc":"code","980bd141":"code","30979307":"code","cbf67b1d":"code","72988852":"code","882b320d":"code","8149fd1c":"code","f172b38c":"code","9a596ad1":"code","22013d42":"code","d17e7911":"code","36816688":"code","8aba5e8c":"code","59f9c9a2":"code","61e03b6c":"code","1a34a55c":"code","1bcb49c2":"code","fbf663b6":"code","97cc2fd0":"code","f341b0d2":"code","7458e219":"code","a403379b":"code","c9e8a04e":"code","614f0466":"code","5037fb1c":"code","0f9bb5e2":"code","65418fff":"code","aacd3467":"code","2733b7d0":"code","2eb4eeb8":"code","d7cc1d9c":"code","6bf136ef":"code","9fc40ea2":"code","dd41dfb4":"code","163b2c6d":"markdown","23a34379":"markdown","a95025f5":"markdown","d114035c":"markdown","d454da8c":"markdown","ec411e68":"markdown","730adc9d":"markdown","b7967719":"markdown","cd35e3e4":"markdown","13184fae":"markdown","86adda7d":"markdown","150968a6":"markdown","7bf2b57d":"markdown","a42cd2ca":"markdown","026c0d91":"markdown","b3ab512c":"markdown","b830fe95":"markdown","ffb7d451":"markdown","7cb81ea0":"markdown","9f1c00df":"markdown","1e77a422":"markdown","54a58805":"markdown","c5612443":"markdown","3aead324":"markdown","0411dbc1":"markdown","c0b19183":"markdown","13aaa23c":"markdown","27583948":"markdown","0db5f09a":"markdown","4caa19b6":"markdown","05312ae0":"markdown","a3a0f234":"markdown","d54d1565":"markdown","7a8afb2c":"markdown","c9b94358":"markdown","ecc0c2af":"markdown","7dc144c2":"markdown","09fa4be7":"markdown","019ca83f":"markdown","817ef9f3":"markdown","67bde349":"markdown","c74d6cd1":"markdown","e67ec187":"markdown"},"source":{"82eba4c9":"# Basic library\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","af12943e":"# Statistics library\nfrom scipy.stats import norm\nfrom scipy import stats\nimport scipy\nimport statsmodels.formula.api as smf\nfrom statsmodels.formula.api import ols\nimport statsmodels.api as sm\nimport statsmodels.stats.anova as anova\n\n# random value\nfrom numpy.random import *\n\n# Visualization\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\nimport seaborn as sns","69a399bc":"df = pd.read_csv(\"\/kaggle\/input\/avocado-prices\/avocado.csv\", header=0)","980bd141":"df.head()","30979307":"# Null value\ndf.isnull().sum()","cbf67b1d":"sns.pairplot(df.sample(200))","72988852":"# data\ndata = df[\"AveragePrice\"]\n\n# calculation of skew and kurtosis\nskew = scipy.stats.skew(data)\nkurt = scipy.stats.kurtosis(data)\n\n# basic check with \nfig, ax = plt.subplots(1,2,figsize=(20,6))\nsns.distplot(data, fit=norm, ax=ax[0])\nax[0].set_ylabel(\"frequency\")\nax[0].set_title(\"Distribution plot\\n<skewness:%.2f>\\n<kurtosis:%.2f>\" % (skew,kurt))\nstats.probplot(data, plot=ax[1])\nax[1].set_title(\"Probability plot\")","882b320d":"# data\ndata = np.log(df[\"AveragePrice\"])\n\n# calculation of skew and kurtosis\nskew = scipy.stats.skew(data)\nkurt = scipy.stats.kurtosis(data)\n\n# basic check with \nfig, ax = plt.subplots(1,2,figsize=(20,6))\nsns.distplot(data, fit=norm, ax=ax[0])\nax[0].set_ylabel(\"frequency\")\nax[0].set_title(\"Distribution plot\\n<skewness:%.2f>\\n<kurtosis:%.2f>\" % (skew,kurt))\nstats.probplot(data, plot=ax[1])\nax[1].set_title(\"Probability plot\")","8149fd1c":"# data\ndata = np.log(df[\"AveragePrice\"])\n\n# with stats model\nWS, p = stats.shapiro(data.sample(4999))","f172b38c":"print(\"p value:{}\".format(p))","9a596ad1":"# data\ndata = np.log(df[\"AveragePrice\"])\n\n# with stats model\nKS, p = stats.kstest(data, \"norm\")","22013d42":"print(\"p value:{}\".format(p))","d17e7911":"# data\ndata = np.log(df[\"AveragePrice\"])\n\nstatistic, critical_values, significance_level = scipy.stats.anderson(data, \"norm\")","36816688":"print(\"critical values:{}\".format(critical_values))\nprint(\"significant_level:{}\".format(significance_level))","8aba5e8c":"# create norm data, about Shapiro Wilk test, N<5000\nnorm_data = randn(4999)\n\n# Visualization\nsns.distplot(norm_data)","59f9c9a2":"# with stats model\nWS, p = stats.shapiro(norm_data)\nprint(\"Shapiro Wilk test p value:{}\".format(p))\n\n# with stats model\nKS, p = stats.kstest(norm_data, \"norm\")\nprint(\"Kolmogorov\u2013Smirnov test p value:{}\".format(p))\n\n# with stats model\nstatistic, critical_values, significance_level = scipy.stats.anderson(norm_data, \"norm\")\nprint(\"critical values:{}\".format(critical_values))\nprint(\"significant_level:{}\".format(significance_level))","61e03b6c":"plt.figure(figsize=(10,6))\nsns.boxplot(x=\"year\", y=\"AveragePrice\", data=df)\nprint(\"2015 average price:{}\".format(df.query(\"year==2015\")[\"AveragePrice\"].mean()))\nprint(\"2016 average price:{}\".format(df.query(\"year==2016\")[\"AveragePrice\"].mean()))","1a34a55c":"plt.figure(figsize=(10,6))\nsns.distplot(df.query(\"year==2015\")[\"AveragePrice\"])\nsns.distplot(df.query(\"year==2016\")[\"AveragePrice\"])\nprint(\"2015 average price:{}\".format(df.query(\"year==2015\")[\"AveragePrice\"].mean()))\nprint(\"2016 average price:{}\".format(df.query(\"year==2016\")[\"AveragePrice\"].mean()))","1bcb49c2":"# data\nprice_2015 = np.log(df.query(\"year==2015\")[\"AveragePrice\"].values)\nprice_2016 = np.log(df.query(\"year==2016\")[\"AveragePrice\"].values)\n\n# with stats model, \nstats.ttest_ind(price_2015, price_2016)","fbf663b6":"# with stats model, equal_var=False\nstats.ttest_ind(price_2015, price_2016, equal_var=False)","97cc2fd0":"# with stats model\nstats.mannwhitneyu(price_2015, price_2016)","f341b0d2":"# pivot table\npivot = pd.pivot_table(df, index=\"type\", columns=\"year\", values=\"AveragePrice\", aggfunc=\"mean\")\npivot.head()","7458e219":"# visualization\nplt.figure(figsize=(10,6))\nplt.plot(pivot.T.index, pivot.T[\"conventional\"])\nplt.plot(pivot.T.index, pivot.T[\"organic\"])\nplt.xlabel(\"year\")\nplt.xticks([2015,2016,2017,2018])\nplt.ylabel(\"Average price\")\nplt.yticks([0.5,1,1.5,2])","a403379b":"# stats model\nx2, p, dof, expected = scipy.stats.chi2_contingency(pivot)","c9e8a04e":"# result\nprint(\"x2:{}\".format(x2))\nprint(\"p:{}\".format(p))\nprint(\"dof:{}\".format(dof))\nprint(\"expectd:\\n{}\".format(expected))","614f0466":"# data\nprice_2015 = df.query(\"year==2015\")[\"AveragePrice\"].values\nprice_2016 = df.query(\"year==2016\")[\"AveragePrice\"].values\n\n# stats model\nscipy.stats.bartlett(price_2015, price_2016)","5037fb1c":"# Visualization check\nplt.figure(figsize=(10,6))\nsns.distplot(price_2015)\nsns.distplot(price_2016)\nplt.xlabel(\"variance\")\nplt.title(\"Distribution \\n variance at 2015 %.2f \\n variance at 2016 %.2f\" % (price_2015.var(), price_2016.var()))","0f9bb5e2":"plt.figure(figsize=(10,6))\nsns.boxplot(x=\"year\", y=\"AveragePrice\", data=df)\nprint(\"2015 average price:{}\".format(df.query(\"year==2015\")[\"AveragePrice\"].mean()))\nprint(\"2016 average price:{}\".format(df.query(\"year==2016\")[\"AveragePrice\"].mean()))\nprint(\"2017 average price:{}\".format(df.query(\"year==2017\")[\"AveragePrice\"].mean()))\nprint(\"2018 average price:{}\".format(df.query(\"year==2018\")[\"AveragePrice\"].mean()))","65418fff":"# data\nprice_2015 = df.query(\"year==2015\")[\"AveragePrice\"].values\nprice_2016 = df.query(\"year==2016\")[\"AveragePrice\"].values\nprice_2017 = df.query(\"year==2017\")[\"AveragePrice\"].values\nprice_2018 = df.query(\"year==2018\")[\"AveragePrice\"].values\n\nprint(\"Shapiro Wilk test\")\nprint(\"price_2015 p-value:{}:\".format(stats.shapiro(price_2015[:4999])[1]))\nprint(\"price_2016 p-value:{}:\".format(stats.shapiro(price_2016[:4999])[1]))\nprint(\"price_2017 p-value:{}:\".format(stats.shapiro(price_2017[:4999])[1]))\nprint(\"price_2018 p-value:{}:\".format(stats.shapiro(price_2018[:4999])[1]))","aacd3467":"# Levene test\nstats.levene(price_2015, price_2016, price_2017, price_2018)","2733b7d0":"# Reference) Bartlett test\nstats.bartlett(price_2015, price_2016, price_2017, price_2018)","2eb4eeb8":"# Reference) Parametric version, if can be eaual variances.\nf, p = stats.f_oneway(price_2015, price_2016, price_2017, price_2018)\n\nprint(\"p-value:{}\".format(p))","d7cc1d9c":"# Check data frame summary\ndf.groupby([\"year\", \"type\"])[\"AveragePrice\"].mean()","6bf136ef":"# Create dataframe\nsample_data = df[[\"AveragePrice\", \"type\", \"year\"]]\nsample_data.head()","9fc40ea2":"# Statsmodel\nformula = 'AveragePrice ~ C(type)+C(year) + C(type):C(year)'\n\nmodel = ols(formula, sample_data).fit()\n\n# Result\nmodel.summary()","dd41dfb4":"aov_table = sm.stats.anova_lm(model, typ=2)\nprint(aov_table)","163b2c6d":"From the Anderson Darling test results, it can be said that there is normality because almost all the numbers are below the significance level.","23a34379":"p-value<0.05, we can reject the null hypothesis. As a result, the average prices for 2015 and 2016 are difference.","a95025f5":"### Visualization check","d114035c":"p-value<0.05, we can reject the null hypothesis. As a result, the average prices for 2015 and 2016 are difference.","d454da8c":"# ANOVA, two-way analysis of variance\n","ec411e68":"Objective: To test whether the average price for each type of year is independent. <br>\n\nThe null hypothesis: Average prices are independent. <br>\n\nConflict hypothesis: Average prices are dependent. <br>\n(We cannot say that average prices are independent)\n\nSuperiority level: 5% <br>","730adc9d":"Objective: To test the difference between 2015 and 2016 price averages. <br>\n\nThe null hypothesis: The average prices for 2015 and 2016 are same. <br>\n\nConflict hypothesis: The average prices for 2015 and 2016 are difference.<br>\n(we cannot say that the average price for 2015 and 2016 are same.) <br>\n\nSuperiority level: 5% <br>","b7967719":"### ANOVA with no correspondence\n\nThe null hypothesis : Average of each year is equal.\n### I could not reject the null hypothesis, but this time I will perform analysis of variance as it is because the method is recorded.","cd35e3e4":"# t-test (test for difference in mean)\n\n*Subsequent prices are normal price values. not log.","13184fae":"### For reference, let's artificially create a normal distribution and execute a normality test.","86adda7d":"### Unpaired, student t test","150968a6":"Result) None of the data is normally distributed.","7bf2b57d":"Create a summary of Statistical test using Avocado price data as the subject.","a42cd2ca":"This data does not return the value normally.","026c0d91":"The p-value is small for all variables and interactions, and the null hypothesis cannot be rejected.<br>\nTherefore, each average is different.","b3ab512c":"# Normality test\nPerform normality test of average price","b830fe95":"## Kolmogorov\u2013Smirnov test","ffb7d451":"Previous notebooks<br>\n\nNotebooks<br>\nClassification method<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/notebook-classification-method<br>\nRegression method<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/notebook-regression-method<br>\nDimension reduction method<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/notebook-dimension-reduction<br>\nImage preprocessing OpenCV library<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/notebook-image-preprocessing-opencv-library<br>","7cb81ea0":"p-value <0.5, the null hypothesis can be rejected.\n\nSo, need to non parametric method.","9f1c00df":"Objective: To test the difference between 2015 and 2016 price variance. <br>\n\nThe null hypothesis: The average variance for 2015 and 2016 are same. <br>\n\nConflict hypothesis: The average variance for 2015 and 2016 are difference.<br>\n(we cannot say that the average variance for 2015 and 2016 are same.) <br>\n\nSuperiority level: 5% <br>","1e77a422":"# Notebook, Statistical test with Avocado price","54a58805":"The distribution is closer to the left because sker is greater than 0. In this case, it may be possible to approximate the normal distribution by taking the logarithm. As a result, the regularity was improved.","c5612443":"p-value<0.05, we can reject the null hypothesis. As a result, the averages are difference.","3aead324":"p value is small, so we can reject the null hypothesis. That is, it is not a normal distribution.","0411dbc1":"Objective: Test the average price of 2015, 2016, 2017, 2018. <br>\n\nThe null hypothesis: The average variance for each year are same. <br>\n\nConflict hypothesis: The average variance for each year are difference.<br>\n(we cannot say that the average variance for each year are same.) <br>\n\nSuperiority level: 5% <br>","c0b19183":"# Anderson-Darling test","13aaa23c":"### Unpaired,Mannwhitney u test","27583948":"If the data are normally distributed, you can reject the null hypothesis. = The data population is normally distributed.","0db5f09a":"p-value>0.05, we cannot reject the null hypothesis. As a result, the average price are independent.","4caa19b6":"Objective: Test the average price of (2015, 2016, 2017, 2018) vs (conventional, organic). <br>\n\nThe null hypothesis: The average variance for each are same. <br>\n\nConflict hypothesis: The average variance for each are difference.<br>\n(we cannot say that the average variance for each are same.) <br>\n\nSuperiority level: 5% <br>","05312ae0":"# Chi-square test (Test of independence)","a3a0f234":"## Shapiro Wilk test\nThe null hypothesis is that the population is normally distributed.","d54d1565":"### Unpaired, weltch t test","7a8afb2c":"Change raw data to log.","c9b94358":"p-value<0.05, we can reject the null hypothesis. As a result, the average prices for 2015 and 2016 are difference.","ecc0c2af":"p-value<0.05, we can reject the null hypothesis. As a result, the variance are difference.","7dc144c2":"# F test (Test of variance)","09fa4be7":"distplot and qqplot with rawdata","019ca83f":"### 2nd) Test for homoscedasticity\n\nif normaly data \u21d2 Bartlett test, else \u21d2Levene test(It can be used to some extent even if it is not normally distributed.)\n\nThis time, i select Levene test.\n\nThe null hypothesis : equal variance for the four samples.","817ef9f3":"# ANOVA, one-way analysis of variance","67bde349":"## Data loading and checks","c74d6cd1":"### Statistical method\n- Normality test\n- t test\n- chi-square test\n- F test\n- ANOVA (one-way)\n- ANOVA (two-way)","e67ec187":"### 1st) Perform normality test on four data."}}