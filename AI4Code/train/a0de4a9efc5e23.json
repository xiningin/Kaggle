{"cell_type":{"c0d749a0":"code","61227bf5":"code","82ef7056":"code","2154abc5":"code","58fbd466":"code","f312eb84":"code","88265ba2":"code","8c384e56":"code","58729468":"code","0a25a35a":"code","d5a14970":"code","52406791":"code","9cb59e0d":"code","5aa6c718":"code","899ead65":"code","b9573d84":"code","695f526f":"code","026a9f9e":"code","07ec7312":"code","45642deb":"code","74c28029":"code","028317ec":"code","3f1ac05d":"code","58c4c377":"code","b71a05fb":"code","6ed17fec":"code","dff8659b":"code","c755c19c":"code","d31f6b80":"code","e8f9722e":"code","38ed7ce4":"code","b7ff021c":"code","98404f5e":"code","819bb26d":"code","11eb73fb":"code","d55bd254":"code","6ea5d5ab":"code","f1a7181e":"code","a38668ad":"code","67c201c5":"code","77ba2c65":"code","6cb0cf86":"code","276b48f6":"code","55c4b369":"code","753720a9":"code","9c94f0f3":"code","1f2536a3":"code","a95d13ca":"code","fc78d275":"code","6cb2c0c1":"code","891f0213":"code","d84fe8a3":"code","57cd9d96":"code","6b0efc3b":"markdown","88e15450":"markdown","9465f22d":"markdown","ad8a5087":"markdown","00efb4db":"markdown","ff7e28ca":"markdown","e8ac2712":"markdown","f2cf72ca":"markdown","32a5c401":"markdown","caa637a1":"markdown","56ec2f55":"markdown"},"source":{"c0d749a0":"import sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\n\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score","61227bf5":"housing_dataset = pd.read_csv('..\/input\/california-housing-prices\/housing.csv')","82ef7056":"# Showing the first 5 columns \nhousing_dataset.head()","2154abc5":"# Showing random 5 samples\nhousing_dataset.sample(5)","58fbd466":"# the shape of the data\nhousing_dataset.shape","f312eb84":"housing_dataset.isna().sum()","88265ba2":"# drop the missing data\nhousing_dataset = housing_dataset.dropna()\n\n# the shape after dropping the missing data\nhousing_dataset.shape","8c384e56":"housing_dataset.isna().sum()","58729468":"# Exporing the categorical data\nhousing_dataset['ocean_proximity'].unique()","0a25a35a":"# Converting categorical values to numeric values using one-hot encoding\nhousing_dataset = pd.get_dummies(housing_dataset, columns= ['ocean_proximity'])\n\n# Another techinque:\n'''\nocean_proximity = ['NEAR BAY', '<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'ISLAND']\nlabel_encoding = preprocessing.LabelEncoder()\nlabel_encoding = label_encoding.fit(ocean_proximity)\nhousing_dataset['ocean_proximity'] = label_encoding.transform(housing_dataset['ocean_proximity'])\nlabel_encoding.classes_\n'''\n\n# Showing the data after Converting categorical values to numeric values\nhousing_dataset.head()","d5a14970":"# Original data frame had 10 columns, we now have 14 columns\nhousing_dataset.shape","52406791":"# Showing the correlation between data\nhousing_dataset_correlation = housing_dataset.corr()\nhousing_dataset_correlation","9cb59e0d":"# housing dataset correlation in heat map\nplt.figure(figsize=(15,12))\nsns.heatmap(housing_dataset_correlation, annot = True)","5aa6c718":"# Extracting the data\nX = housing_dataset.drop('median_house_value', axis = 1)  # Features\nY = housing_dataset['median_house_value']                 # Target","899ead65":"# Splitting the data into traing and testing data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=45)","b9573d84":"linear_regressor = LinearRegression(normalize = True, fit_intercept = False, copy_X = True).fit(x_train, y_train)\n# Normailzation scales all numeric features to be between 0 and 1. \n# Having features in the same scale can vastly improve tne performance of your ML model","695f526f":"print(\"Training score : \", linear_regressor.score(x_train, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","026a9f9e":"y_pred = linear_regressor.predict(x_test)\n\nprint(\"testing score : \", r2_score(y_test, y_pred))","07ec7312":"linear_regressor.intercept_ , linear_regressor.coef_","45642deb":"# Hyper-Parameters Tuning\n'''\nlinear_regressor_parameter = {'normalize': [True, False], 'fit_intercept': [True, False]}\nlinear_regressor_grid_search = GridSearchCV(LinearRegression(), linear_regressor_parameter, cv = 2)\nlinear_regressor_grid_search.fit(X, Y)\nprint('The best score',linear_regressor_grid_search.best_score_)\nprint('The best parameters',linear_regressor_grid_search.best_params_)\n'''\n\n# The result:\n\n# The best parameters {'fit_intercept': False, 'normalize': True}","74c28029":"lasso_regressor = Lasso(alpha = 1, fit_intercept= True, normalize= False, max_iter = 20000).fit(x_train, y_train)","028317ec":"print(\"Training score : \", lasso_regressor.score(x_train, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","3f1ac05d":"y_pred = lasso_regressor.predict(x_test)\nprint(\"testing score : \", r2_score(y_test, y_pred))","58c4c377":"lasso_regressor.intercept_ , lasso_regressor.coef_ , lasso_regressor. n_iter_","b71a05fb":"# hyper-Parameters Tuning\n'''\nlasso_regressor_parameter = {'alpha': [0.2,0.4,0.6,0.8,1], 'normalize': [True, False], 'fit_intercept': [True, False]}\nlasso_regressor_grid_search = GridSearchCV(Lasso(max_iter = 400000), lasso_regressor_parameter, cv = 2)\nlasso_regressor_grid_search.fit(X, Y)\nprint('The best score',lasso_regressor_grid_search.best_score_)\nprint('The best parameters',lasso_regressor_grid_search.best_params_)\n'''\n\n# The result:\n\n# The best parameters {'alpha': 1, 'fit_intercept': True, 'normalize': False}","6ed17fec":"ridge_regressor = Ridge(alpha = 0.4, fit_intercept= True, normalize= False, max_iter = 20000).fit(x_train, y_train)","dff8659b":"print(\"Training score : \", ridge_regressor.score(x_train, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","c755c19c":"y_pred = ridge_regressor.predict(x_test)\nprint(\"testing score : \", r2_score(y_test, y_pred))","d31f6b80":"ridge_regressor.intercept_ , ridge_regressor.coef_ , ridge_regressor. n_iter_","e8f9722e":"# hyper-Parameters Tuning\n'''\nridge_regressor_parameter = {'alpha': [0.2,0.4,0.6,0.8,1], 'normalize': [True, False], 'fit_intercept': [True, False]}\nridge_regressor_grid_search = GridSearchCV(Ridge(max_iter = 400000), ridge_regressor_parameter, cv = 2)\nridge_regressor_grid_search.fit(X, Y)\nprint('The best score',ridge_regressor_grid_search.best_score_)\nprint('The best parameters',ridge_regressor_grid_search.best_params_)\n'''\n\n# The result:\n\n# The best parameters {'alpha': 0.4, 'fit_intercept': True, 'normalize': False}","38ed7ce4":"elastic_regressor = ElasticNet(alpha = 1, l1_ratio = 1, normalize = False, fit_intercept= True, max_iter = 20000).fit(x_train, y_train)","b7ff021c":"print(\"Training score : \", elastic_regressor.score(x_train, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","98404f5e":"y_pred = elastic_regressor.predict(x_test)\nprint(\"testing score : \", r2_score(y_test, y_pred))","819bb26d":"elastic_regressor.intercept_ , elastic_regressor.coef_ , elastic_regressor. n_iter_","11eb73fb":"# hyper-Parameters Tuning\n'''\nelastic_regressor_parameter = {'alpha': [0.2,0.4,0.6,0.8,1], 'l1_ratio': [0,0.2,0.5,0.8,1] , 'normalize': [True, False], 'fit_intercept': [True, False]}\nelastic_regressor_grid_search = GridSearchCV(ElasticNet(max_iter = 400000), elastic_regressor_parameter, cv = 2)\nelastic_regressor_grid_search.fit(X, Y)\nprint('The best score',elastic_regressor_grid_search.best_score_)\nprint('The best parameters',elastic_regressor_grid_search.best_params_)\n'''\n\n# The result:\n\n# The best parameters {'alpha': 1, 'fit_intercept': True, 'l1_ratio': 1, 'normalize': False}","d55bd254":"# SVR tries to fit as many points as possiple into a margine surrounding the best fit line\nsvr_regressor = SVR(kernel='linear', epsilon = 0.2, C = 1).fit(x_train, y_train)","6ea5d5ab":"print(\"Training score : \", svr_regressor.score(x_train, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","f1a7181e":"y_pred = svr_regressor.predict(x_test)\nprint(\"testing score : \", r2_score(y_test, y_pred))","a38668ad":"# hyper-Parameters Tuning\n'''\nsvr_regressor_parameter = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'epsilon': [0.05,0.1,0.2,0.3], 'C': [0.2,0.5,0.8,1]}\nsvr_regressor_grid_search = GridSearchCV(SVR(), svr_regressor_parameter, cv = 2)\nsvr_regressor_grid_search.fit(X, Y)\nprint('The best score',svr_regressor_grid_search.best_score_)\nprint('The best parameters',svr_regressor_grid_search.best_params_)\n'''\n","67c201c5":"# Nearest Nieghbors regression uses training data to find what is most similar to the current sample\n# Average y-values of K nearest nieghbors\n\nknn_regressor = KNeighborsRegressor(n_neighbors = 10).fit(x_train, y_train)","77ba2c65":"print(\"Training score : \", knn_regressor.score(x_train, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","6cb0cf86":"y_pred = knn_regressor.predict(x_test)\nprint(\"testing score : \", r2_score(y_test, y_pred))","276b48f6":"# hyper-Parameters Tuning\n'''\nknn_regressor_parameter = {'n_neighbors': [3,5,8,10,15,20,25]}\nknn_regressor_grid_search = GridSearchCV(KNeighborsRegressor(), knn_regressor_parameter, cv = 2)\nknn_regressor_grid_search.fit(X, Y)\nprint('The best score',knn_regressor_grid_search.best_score_)\nprint('The best parameters',knn_regressor_grid_search.best_params_)\n'''","55c4b369":"# Decision trees set up a tree structure on training data which helps make decisions based on rules\n\ntree_regressor = DecisionTreeRegressor(max_depth = 7).fit(x_train, y_train)","753720a9":"print(\"Training score : \", tree_regressor.score(x_train, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","9c94f0f3":"y_pred = tree_regressor.predict(x_test)\nprint(\"testing score : \", r2_score(y_test, y_pred))","1f2536a3":"# hyper-Parameters Tuning\n'''\ntreen_regressor_parameter = {'max_depth': [2,3,4,5,6,7,8,9,10]}\ntree_regressor_grid_search = GridSearchCV(DecisionTreeRegressor(), tree_regressor_parameter, cv = 2)\ntree_regressor_grid_search.fit(X, Y)\nprint('The best score',tree_regressor_grid_search.best_score_)\nprint('The best parameters',tree_regressor_grid_search.best_params_)\n'''","a95d13ca":"# This is an iterative model where you use multiple iteration to find \n# the best linear model that fit your underlyning data.\n\n# It works well with standarized features\n# I will standarize the all features except the categorical features\nfeatures_not_categorical_columns = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income']\nfeatures_not_categorical_train = x_train[features_not_categorical_columns]\nx_train_scaled = x_train.copy()\nscaler = StandardScaler()\nfeatures_not_categorical_train_scaled = scaler.fit_transform(features_not_categorical_train)\nx_train_scaled[features_not_categorical_columns] = features_not_categorical_train_scaled\n\nsgd_regressor = SGDRegressor(max_iter = 100000, tol = 1e-3).fit(x_train_scaled, y_train)","fc78d275":"print(\"Training score : \", sgd_regressor.score(x_train_scaled, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","6cb2c0c1":"features_not_categorical_test = x_test[features_not_categorical_columns]\nx_test_scaled = x_test.copy()\nscaler = StandardScaler()\nfeatures_not_categorical_test_scaled = scaler.fit_transform(features_not_categorical_test)\nx_test_scaled[features_not_categorical_columns] = features_not_categorical_test_scaled\n\ny_pred = sgd_regressor.predict(x_test_scaled)\nprint(\"testing score : \", r2_score(y_test, y_pred))","891f0213":"# It do well with standarized features\n\nnn_regressor = MLPRegressor(activation = 'relu', hidden_layer_sizes = (32,64,128,64,8), solver= 'lbfgs', max_iter= 20000).fit(x_train_scaled, y_train)\n\n# 1- hidden_layer_sizes = (No. of units or neurons in 1st hidden layer, No. of units or neurons in 2nd hidden layer, .... )\n# 2- Activation function for the hidden layer: {\u2018identity\u2019, \u2018logistic\u2019, \u2018tanh\u2019, \u2018relu\u2019}, default=\u2019relu\u2019  \n# 3- solver for weight optimization : {\u2018lbfgs\u2019, \u2018sgd\u2019, \u2018adam\u2019}, default=\u2019adam\u2019\n# 4- learning_rate_initdouble, default=0.001, The initial learning rate used. \n# It controls the step-size in updating the weights. Only used when solver=\u2019sgd\u2019 or \u2018adam\u2019.","d84fe8a3":"print(\"Training score : \", nn_regressor.score(x_train_scaled, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","57cd9d96":"y_pred = nn_regressor.predict(x_test_scaled)\nprint(\"testing score : \", r2_score(y_test, y_pred))","6b0efc3b":"# 7. Using Decision trees","88e15450":"# Solving the California Housing Prices Prediction problem using:\n## 1. Linear Regression\n## 2. Lasso Regression\n## 3. Ridge Regression\n## 4. ElasticNet Regression\n## 5. Support Vector Regressor (SVR)\n## 6. Nearest Nieghbors regression\n## 7. Decision trees\n## 8. SGD Regressor\n## 9. Neural Network\n","9465f22d":"# 6. Using Nearest Nieghbors regression","ad8a5087":"# 4. Using ElasticNet Regression","00efb4db":"# 1. Using Linear Regression","ff7e28ca":"# 9. Using Neural Network","e8ac2712":"## Reading the dataset (California Housing Prices)","f2cf72ca":"# 2. Using Lasso Regression","32a5c401":"# 3. Using Ridge Regression","caa637a1":"# 5. Using Support Vector Regressor (SVR)","56ec2f55":"# 8. Using SGD Regressor"}}