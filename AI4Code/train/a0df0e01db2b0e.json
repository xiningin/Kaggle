{"cell_type":{"aff4ad23":"code","00ef4c04":"code","cad3c88f":"code","ea707e22":"code","0bf8ee55":"code","b8551740":"code","c2a45d1e":"code","e5e23eb4":"code","76733f55":"code","6ce6e30a":"code","1192d6e2":"code","ae2eb885":"code","11628fc8":"code","a46c1132":"code","b54d672c":"code","885690e3":"code","10869744":"code","36fbcacd":"code","a790da63":"code","732cea93":"code","4e80091e":"code","6a3afca4":"code","480f5bd5":"code","afcf0091":"code","efbd389b":"code","0cf37645":"markdown","e8739dd6":"markdown","9f85c9d1":"markdown","ed7d1d8e":"markdown","55c2d2ff":"markdown","3594212e":"markdown","8f374c58":"markdown","605cea8a":"markdown","93e79c52":"markdown"},"source":{"aff4ad23":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","00ef4c04":"train_feat_df = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_features.csv\", index_col='sig_id')\ntrain_feat_df.loc[:,'cp_dose_cat'] = train_feat_df.cp_dose.astype('category').cat.codes\ntrain_feat_df.loc[:,'cp_type_cat'] = train_feat_df.cp_type.astype('category').cat.codes\ntrain_feat_df.loc[:,'cp_time_cat'] = train_feat_df.cp_time.astype('category').cat.codes\ntrain_feat_df.drop(columns=['cp_dose', 'cp_type' ], inplace=True)\ntrain_feat_df.drop(columns=['cp_time' ], inplace=True)\ntrain_tg_scored_df = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_targets_scored.csv\", index_col='sig_id')\ntrain_tg_nonscored_df = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv\", index_col='sig_id')\n","cad3c88f":"train_feat_df = train_feat_df.loc[train_feat_df.cp_type_cat==1, train_feat_df.columns != \"cp_type_cat\"]\ntrain_feat_df.head(2)","ea707e22":"test_feat_df = pd.read_csv(\"\/kaggle\/input\/lish-moa\/test_features.csv\", index_col='sig_id')\ntest_feat_df.loc[:,'cp_dose_cat'] = test_feat_df.cp_dose.astype('category').cat.codes\ntest_feat_df.loc[:,'cp_type_cat'] = test_feat_df.cp_type.astype('category').cat.codes\n# ??\ntest_feat_df.loc[:,'cp_time_cat'] = test_feat_df.cp_time.astype('category').cat.codes\ntest_feat_df.drop(columns=['cp_dose', 'cp_type' ], inplace=True)\ntest_feat_df.drop(columns=['cp_time' ], inplace=True)\ncontrol_indexes_test = [test_feat_df.index.get_loc(x) for x in test_feat_df[test_feat_df.cp_type_cat==0].index.values]\n","0bf8ee55":"#test_feat_df = test_feat_df.loc[test_feat_df.cp_type_cat==1, test_feat_df.columns != \"cp_type_cat\"]\ntest_feat_df = test_feat_df.loc[:, test_feat_df.columns != \"cp_type_cat\"]\ntest_feat_df.head(2)","b8551740":"from skmultilearn.model_selection import IterativeStratification\n# sample_distribution_per_fold = [test_size, 1.0-test_size]\nstratifier = IterativeStratification(n_splits=2, order=2, sample_distribution_per_fold=[0.3, 0.7])\ntrain_indexes, test_indexes = next(stratifier.split(train_feat_df, train_tg_scored_df.loc[train_feat_df.index,:]))\nX_train, Y_train = train_feat_df.iloc[train_indexes], train_tg_scored_df.iloc[train_indexes]\nX_val, Y_val = train_feat_df.iloc[test_indexes], train_tg_scored_df.iloc[test_indexes]","c2a45d1e":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, InputLayer, BatchNormalization\nfrom tensorflow.keras.layers import Dropout","e5e23eb4":"model_nonscored = Sequential()\nmodel_nonscored.add(BatchNormalization(input_shape=(X_train.shape[1],)))\nmodel_nonscored.add(Dense(2048, activation='relu', input_shape=(875,)))\nmodel_nonscored.add(BatchNormalization())\nmodel_nonscored.add(Dense(1024, activation='relu'))\nmodel_nonscored.add(BatchNormalization())\nmodel_nonscored.add(Dense(512, activation='relu'))\nmodel_nonscored.add(Dropout(0.2))\nmodel_nonscored.add(BatchNormalization(name=\"Last_batch_norm_l\"))\nmodel_nonscored.add(Dense(402, activation='sigmoid'))","76733f55":"p_min = 0.001\np_max = 0.999\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -tf.keras.backend.mean(y_true*tf.keras.backend.log(y_pred) + (1-y_true)*tf.keras.backend.log(1-y_pred))\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.1, verbose=0,mode='min',\n                              patience=3, min_lr=1E-7)\nearly_st = tf.keras.callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=7, verbose=0, mode='min',\n    baseline=None, restore_best_weights=True)","6ce6e30a":"callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n\n# default learning rate = 0.001\noptimizer = tf.keras.optimizers.Adam(\n    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False\n)\n\nmodel_nonscored.compile(optimizer='adam',loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), metrics=logloss )\nmodel_nonscored.summary()","1192d6e2":"tf.keras.utils.plot_model(model_nonscored, to_file='model.png')","ae2eb885":"_ = model_nonscored.fit(train_feat_df, train_tg_nonscored_df.loc[train_feat_df.index,:], epochs=10, callbacks=[reduce_lr, early_st])","11628fc8":"from tensorflow.keras.models import Model","a46c1132":"model1_out = model_nonscored.get_layer('Last_batch_norm_l').output\nout = Dense(206, activation='sigmoid', name='output_layer_tlmodel')(model1_out)\ntlmodel = Model(inputs = model_nonscored.inputs, outputs = out)","b54d672c":"#last_frozen = \"dropout_1\"\nlast_frozen = \"Last_batch_norm_l\"\nfor layer in tlmodel.layers:\n    layer.trainable = False    \n    if layer.name == last_frozen:\n        break\nfor layer in tlmodel.layers:\n    print(f\"{layer.name}: {layer.trainable}.\")","885690e3":"tlmodel.summary()","10869744":"tlmodel.compile(optimizer='adam',loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), metrics=logloss )","36fbcacd":"_ = tlmodel.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=1000, callbacks=[reduce_lr, early_st])","a790da63":"# clipping predictions\np_min = 0.001\np_max = 0.999\n","732cea93":"predictions = np.clip(tlmodel.predict(test_feat_df),p_min,p_max)\npredictions[control_indexes_test, :] = 0\n\npredictions_df = pd.DataFrame(predictions)\npredictions_df.shape","4e80091e":"submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv', index_col='sig_id')\nprint(submission.shape)\nsubmission.head(2)","6a3afca4":"predictions_df.index = submission.index","480f5bd5":"predictions_df.head(3)","afcf0091":"predictions_df.columns = submission.columns\npredictions_df.head(3)","efbd389b":"predictions_df.to_csv(\"submission.csv\")","0cf37645":"### Test data","e8739dd6":"### Training data","9f85c9d1":"## Read data","ed7d1d8e":"# Transfer learning and Mechanisms of Action\nFor the last few weeks, I have been thinking the following: This data set cries of transfer learning. Why?\nFor the following reasons:\n* We have an additional dataset with labels\n* Most people here tend to think this is a multioutput scenario. If this is true, inner layers should not depend too much on outputs\n* Some of the outputs have only a few examples. In two cases, there is a single positive outcome (therefore no way to predict it?). Since we have apparently very incomplete information, I would think more the more information the better.\n\nAll this smells like a textbook example of transfer learning, doesn't it? Yet after weeks of trying, I have never achieved better outcome with transfer model than with the same architecture which was directly fitted (not much worse, sure... but on competition score, I never scored better with transfer model). \n\nAny idea why?\n\n* Is my thinking wrong all along for some apparent reason?\n* Is my model construction erroneous?\n* What is going on here?\n\nThis only got public score of 0.23. This is not a particularly optimized version of the notebook... but still what's wrong with transfer learning here?","55c2d2ff":"I know this is not the most elegant usage of functional API. But it seems to work...","3594212e":"This might be an issue. Here I did not split the dataset and used the whole training set to fit the transfer model. Yet I also tested the proper split and it did not result in anything better...","8f374c58":"## Build keras transfer learning model\n### First create the featurizer which uses nonscored data","605cea8a":"## Stratified train test split for multioutput case","93e79c52":"## Display input data"}}