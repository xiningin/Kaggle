{"cell_type":{"5e0ba060":"code","5b387d36":"code","00b804d9":"code","d86bf3ca":"code","dee38b68":"code","24d7fe3d":"code","4eb56119":"code","be8f2380":"code","276ee46c":"code","520d74cf":"code","d1c97324":"code","615f1a68":"code","5f948a15":"code","8977149b":"code","6481c44b":"code","9b229a01":"code","4a6bb378":"code","3b180860":"code","3df271ee":"code","ab6dba85":"code","de19c828":"markdown","a33784ae":"markdown","705931dd":"markdown"},"source":{"5e0ba060":"import numpy as np\nimport pandas as pd\n#a = pd.read_csv('..\/input\/amazon_cells_labelled.txt',names=['sentence','label'],sep='\\t')[:10]","5b387d36":"files={'yelp':'yelp_labelled.txt',\n       'amazon':'amazon_cells_labelled.txt',\n       'imdb':'imdb_labelled.txt'}\ndf_list=[]\nfor k,v in files.items():\n    path='..\/input\/'+v\n    df = pd.read_csv(path,names=['sentence','label'],sep='\\t')\n    df['source']=k\n    df_list.append(df)\ndf = pd.concat(df_list)\nprint(df.iloc[0])\nprint(type(df))","00b804d9":"from sklearn.model_selection import train_test_split\n#Split the data\ndf_yelp = df[df['source']=='yelp']\nsentence = df_yelp['sentence']\nlabel = df_yelp['label']\nsen_train,sen_test,y_train,y_test = train_test_split(sentence,label,test_size=0.25,random_state=1000)\nlen(sen_train),len(sen_test)","d86bf3ca":"#Best way to understand feature vector\n'''\nsentences = ['John John likes icecream','utsav','John is utsav']\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer(min_df=0,lowercase=False)\nvec.fit(sentences)\nvec.vocabulary_\nvec.transform(sentences).toarray()\n'''\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nvectorizer.fit(sen_train)\nX_train = vectorizer.transform(sen_train)\nX_test = vectorizer.transform(sen_test)\nX_train,X_test","dee38b68":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='lbfgs')\nlr.fit(X_train,y_train)\npredict_lr = lr.predict(X_test)\nscore = lr.score(X_test,y_test)\nresult_lr = pd.DataFrame({'Predict':predict_lr,'actual':y_test})\nprint(f'Accuracy: {score}')\nresult_lr[0:10]","24d7fe3d":"#Acuuracy\nfrom sklearn.metrics import classification_report,confusion_matrix\nclr = classification_report(y_test,predict_lr)\ncom = confusion_matrix(y_test,predict_lr)\nprint(clr)\nprint(\"Confusion_matrix\")\nprint(com)","4eb56119":"#Test for unseen data\nfor source in df['source'].unique():\n    df_yelp = df[df['source']==source]\n    sentence = df_yelp['sentence']\n    label = df_yelp['label']\n    sen_train,sen_test,y_train,y_test = train_test_split(sentence,label,test_size=0.25,random_state=1000)\n    \n    vectorizer = CountVectorizer()\n    vectorizer.fit(sen_train)\n    X_train = vectorizer.transform(sen_train)\n    X_test = vectorizer.transform(sen_test)\n\n    lr = LogisticRegression(solver='lbfgs')\n    lr.fit(X_train,y_train)\n    score = lr.score(X_test,y_test)\n    print('Source: {} Accuracy: {:.3f}'.format(source,score))\n    ","be8f2380":"from sklearn.model_selection import train_test_split\n#Split the data\ndf_yelp = df[df['source']=='yelp']\nsentence = df_yelp['sentence']\nlabel = df_yelp['label']\nsen_train,sen_test,y_train,y_test = train_test_split(sentence,label,test_size=0.25,random_state=1000)\nlen(sen_train),len(sen_test)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nvectorizer.fit(sen_train)\nX_train = vectorizer.transform(sen_train)\nX_test = vectorizer.transform(sen_test)\nX_train,X_test","276ee46c":"from keras.models import Sequential\nfrom keras import layers\ninput_dim = X_train.shape[1]\nmodel = Sequential()\nmodel.add(layers.Dense(10,input_dim=input_dim,activation='relu'))\nmodel.add(layers.Dense(1,activation='sigmoid'))","520d74cf":"model.compile(loss='binary_crossentropy',\n             optimizer='adam',\n             metrics=['accuracy'])\nmodel.summary()","d1c97324":"history = model.fit(X_train,y_train,\n                   epochs=100,verbose=True,\n                   validation_data=(X_test,y_test),\n                   batch_size=10)","615f1a68":"loss,accuracy = model.evaluate(X_train,y_train,verbose=False)\nprint('Training accuracy {}'.format(accuracy))\nloss,accuracy = model.evaluate(X_test,y_test,verbose=False)\nprint('Testing accuracy {}'.format(accuracy))","5f948a15":"def hist(history):\n    from matplotlib import pyplot as plt\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    x = range(1,len(loss)+1)\n    plt.subplot(1,2,1)\n    plt.plot(x,loss,'r',label='training loss')\n    plt.plot(x,val_loss,'b',label='validation loss')\n    plt.legend()\n    plt.subplot(1,2,2)\n    plt.plot(x,acc,'r',label='training accuracy')\n    plt.plot(x,val_acc,'b',label='validation accuracy')\n    plt.legend()\nhist(history)","8977149b":"#Two possible ways to represent a word as a vector are one-hot encoding and word embeddings\n#1)label encoding\nfrom sklearn.preprocessing import LabelEncoder\ncities = ['London', 'Berlin', 'Berlin', 'New York', 'London']\nle =LabelEncoder()\ncity_label = le.fit_transform(cities)\nprint(\"Label encoding -> \" ,city_label)\n\n#2)one-hot encoding\nfrom sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False,categories='auto')\ncity_label = city_label.reshape((5,1))\nohe.fit_transform(city_label)","6481c44b":"#Word embeddings\n#This method represents words as dense word vectors (also called word embeddings) \n#which are trained unlike the one-hot encoding which are hardcoded.\n#This means that the word embeddings collect more information into fewer dimensions.\n#Now you need to tokenize the data into a format that can be used by the word embeddings\nfrom keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(sen_train)\n\nvocab_size = len(tokenizer.word_index) + 1\n\nX_train = tokenizer.texts_to_sequences(sen_train)\nX_test = tokenizer.texts_to_sequences(sen_test)\n\nfrom keras.preprocessing.sequence import pad_sequences\nmaxlen=100\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)","9b229a01":"#input_dim: the size of the vocabulary\n#output_dim: the size of the dense vector\n#input_length: the length of the sequence\nfrom keras.models import Sequential\nfrom keras import layers\n\nout_dim=50\nmodel=Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size,\n                          output_dim=out_dim,\n                          input_length=maxlen))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","4a6bb378":"history = model.fit(X_train,y_train,\n                   epochs=20,\n                   verbose=True,\n                   validation_data=(X_test,y_test),\n                   batch_size=10)","3b180860":"loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nhist(history)","3df271ee":"from keras.models import Sequential\nfrom keras import layers\n\nout_dim=50\nmodel=Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size,\n                          output_dim=out_dim,\n                          input_length=maxlen))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","ab6dba85":"history = model.fit(X_train, y_train,\n                    epochs=20,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nhist(history)","de19c828":"**Overfitted model**","a33784ae":"**sparse matrix**:\n                This is a data type that is optimized for matrices with only a few non-zero elements, which only keeps track of the non-zero elements reducing the memory load","705931dd":"**1. LogisticRegression classifier **"}}