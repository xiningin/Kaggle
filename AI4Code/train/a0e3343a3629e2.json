{"cell_type":{"00815933":"code","b413732d":"code","e1e04008":"code","4134977d":"code","3c78779e":"code","15a7a7ce":"code","fe3eb0f3":"code","2b17b3d0":"code","a2c54ebe":"code","c5fc1af5":"code","33509551":"code","90d4e5a4":"code","e04a5a91":"markdown","3f337838":"markdown","5f589ea1":"markdown","fb5cc2b3":"markdown","ac6635e0":"markdown","c1d81d1d":"markdown"},"source":{"00815933":"#C\u00e0i \u0111\u1eb7t th\u00eam library feature_engine v\u00e0o kaggle (Settings \u0111\u1ec3 Internet: On)\n!pip install feature_engine","b413732d":"from itertools import combinations\n\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom itertools import combinations\n\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport re\nimport warnings\n\nimport lightgbm as lgb\nfrom unidecode import unidecode\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom feature_engine import categorical_encoders as ce\n\nfrom itertools import combinations\nfrom datetime import datetime\nfrom contextlib import contextmanager","e1e04008":"lgbm_param = {'boosting_type': 'gbdt', \n              'colsample_bytree': 0.6602479798930369, \n              'is_unbalance': False, \n              'learning_rate': 0.01,\n              'max_depth': 15, \n              'metric': 'auc', \n              'min_child_samples': 25, \n              'num_leaves': 80,\n              'objective': 'binary', \n              'reg_alpha': 0.4693391197064131, \n              'reg_lambda': 0.16175478669541327, \n              'subsample_for_bin': 60000}\n\nNUM_BOOST_ROUND= 10000\n\nDROP = ['currentLocationCity', 'currentLocationName', 'homeTownCity', 'homeTownName', 'partner0_B', 'partner0_K', 'partner0_L', 'partner1_B', 'partner1_D', 'partner1_E', 'partner1_F', 'partner1_K', 'partner1_L','partner2_B', 'partner2_G', 'partner2_K', 'partner2_L', 'partner3_B', 'partner3_C', 'partner3_F', 'partner3_G', 'partner3_H', 'partner3_K', 'partner3_L', 'partner4_A', 'partner4_B', 'partner4_C', 'partner4_D', 'partner4_E', 'partner4_F', 'partner4_G', 'partner4_H', 'partner4_K','partner5_B', 'partner5_C', 'partner5_D', 'partner5_E', 'partner5_F', 'partner5_H', 'partner5_K', 'partner5_L'] + [f\"Field_{c}\" for c in [11, 14, 15, 16, 17, 18, 24, 26, 30, 31, 37, 38, 42, 52, 56, 60, 61, 77, 2, 62, 66, 40, 6, 47, 7, 5, 65, 19, 27, 59, 10, 53, 57, 3, 21, 13]]\nDATE = [\"Field_{}\".format(i) for i in [5, 6, 7, 8, 9, 11, 15, 25, 32, 33, 34, 35, 40]]\nDATETIME = [\"Field_{}\".format(i) for i in [1, 2, 43, 44]]","4134977d":"@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\ndef Gini(y_true, y_pred):\n    # check and get number of samples\n    assert y_true.shape == y_pred.shape\n    n_samples = y_true.shape[0]\n\n    # sort rows on prediction column\n    # (from largest to smallest)\n    arr = np.array([y_true, y_pred]).transpose()\n    true_order = arr[arr[:, 0].argsort()][::-1, 0]\n    pred_order = arr[arr[:, 1].argsort()][::-1, 0]\n\n    # get Lorenz curves\n    L_true = np.cumsum(true_order) * 1. \/ np.sum(true_order)\n    L_pred = np.cumsum(pred_order) * 1. \/ np.sum(pred_order)\n    L_ones = np.linspace(1 \/ n_samples, 1, n_samples)\n\n    # get Gini coefficients (area between curves)\n    G_true = np.sum(L_ones - L_true)\n    G_pred = np.sum(L_ones - L_pred)\n\n    # normalize to true Gini coefficient\n    return G_pred * 1. \/ G_true\n\ndef lgb_gini(y_pred, dataset_true):\n    y_true = dataset_true.get_label()\n    return 'gini', Gini(y_true, y_pred), True","3c78779e":"def subtract_date(date1,date2, df):\n    df[date1] = pd.to_datetime(df[date1], infer_datetime_format=True)\n    df[date2] = pd.to_datetime(df[date2], infer_datetime_format=True)\n    df[date1+date2] = (df[date2] - df[date1]).dt.days\n    \ndef process_ngaySinh(s):\n    if s != s:\n        return np.nan\n    try:\n        s = int(s)\n    except ValueError:\n        s = s.split(\" \")[0]\n        \n    return datetime.strptime(str(s)[:6], \"%Y%m\")\n\ndef datetime_normalize(s):\n    if s != s:\n        return np.nan\n    \n    s = s.split(\".\")[0]\n    if s[-1] == \"Z\":\n        s = s[:-1]\n        \n    date, time = s.split(\"T\")\n    datetime_obj = datetime.strptime(s, \"%Y-%m-%dT%H:%M:%S\")\n    return datetime_obj\n\ndef date_normalize(s):\n    if s != s:\n        return np.nan\n    \n    try:\n        datetime_obj = datetime.strptime(s, \"%m\/%d\/%Y\")\n    except:\n        datetime_obj = datetime.strptime(s, \"%Y-%m-%d\")\n        \n    return datetime_obj\n  \ndef process_datetime_cols(df):\n    cat_cols = []\n    for col in DATETIME:\n        df[col] = df[col].apply(datetime_normalize)\n        \n    for col in DATE:\n        if col == \"Field_34\":\n            continue\n        df[col] = df[col].apply(date_normalize)\n\n    df[\"Field_34\"] = df[\"Field_34\"].apply(process_ngaySinh)\n    df[\"ngaySinh\"] = df[\"ngaySinh\"].apply(process_ngaySinh)\n    \n    cat_cols += DATE + DATETIME\n    for col in DATE + DATETIME:\n        df[col] = df[col].dt.strftime('%d-%m-%Y')\n    \n    subtract_date('Field_5','Field_6',df)\n    subtrac_List = ['Field_1', 'Field_2', 'Field_43', 'Field_44', 'Field_7','Field_8', 'Field_9']\n    subtract_2C = list(combinations(subtrac_List, 2))\n    for l in subtract_2C:\n        subtract_date(l[0],l[1],df)\n      \n    for cat in ['F', 'E', 'C', 'G', 'A']:\n        subtract_date(f'{cat}_startDate', f'{cat}_endDate', df)\n    print(df.shape) \n    return df\n  \ndef str_normalize(s):\n    s = str(s).strip().lower()\n    s = re.sub(' +', \" \", s)\n    return s\n\ndef process_location(df):\n    for col in [\"currentLocationLocationId\", \"homeTownLocationId\", \"currentLocationLatitude\", \"currentLocationLongitude\", \n                   \"homeTownLatitude\", \"homeTownLongitude\"]:\n        df[col].replace(0, np.nan, inplace=True)\n\n    return df\n\ndef job_category(x):\n    if type(x) == str:\n        if \"c\u00f4ng nh\u00e2n\" in x or \"cnv\" in x or \"cn\" in x or \"may c\u00f4ng nghi\u1ec7p\" in x or \"lao \u0111\u1ed9ng\" in x\\\n        or \"th\u1ee3\" in x or \"co\u00f5ng nha\u00f5n tr\u1eed\u00f9c tie\u1ecfp ma\u1ef1y may co\u00f5ng nghie\u1ecdp\" in x or \"c.n\" in x or \"l\u0111\" in x:\n            return \"CN\"\n        elif \"gi\u00e1o vi\u00ean\" in x or \"gv\" in x or \"g\u00edao vi\u00ean\" in x:\n            return \"GV\"\n        elif \"nh\u00e2n vi\u00ean\" in x or \"k\u1ebf to\u00e1n\" in x or \"c\u00e1n b\u1ed9\" in x or \"nv\" in x or \"cb\" in x or \"nh\u00f5n vi\u1eddn\" in x:\n            return \"NV\"\n        elif \"t\u00e0i x\u1ebf\" in x or \"l\u00e1i\" in x or \"t\u00e0i x\u00ea\" in x:\n            return \"TX\"\n        elif \"qu\u1ea3n l\u00fd\" in x or \"ph\u00f3 ph\u00f2ng\" in x or \"hi\u1ec7u ph\u00f3\" in x:\n            return \"QL\"\n        elif \"undefined\" in x:\n            return \"missing\"\n        elif \"gi\u00e1m \u0111\u1ed1c\" in x or \"hi\u1ec7u tr\u01b0\u1edfng\" in x:\n            return \"G\u0110\"\n        elif \"ph\u1ee5c v\u1ee5\" in x:\n            return \"PV\"\n        elif \"chuy\u00ean vi\u00ean\" in x:\n            return  \"CV\"\n        elif \"b\u00e1c s\u0129\" in x or \"d\u01b0\u1ee3c s\u0129\" in x or \"y s\u0129\" in x or \"y s\u1ef9\" in x:\n            return \"BS\"\n        elif \"y t\u00e1\" in x:\n            return \"YT\"\n        elif \"h\u1ed9 sinh\" in x:\n            return \"HS\"\n        elif \"ch\u1ee7 t\u1ecbch\" in x:\n            return \"CT\"\n        elif \"b\u1ebfp\" in x:\n            return \"\u0110B\"\n        elif \"s\u01b0\" in x:\n            return \"KS\"\n        elif \"d\u01b0\u1ee1ng\" in x:\n            return \"\u0110D\"\n        elif \"k\u1ef9 thu\u1eadt\" in x or \"k\u0129 thu\u1eadt\" in x:\n            return \"KTV\"\n        elif \"di\u1ec5n vi\u00ean\" in x:\n            return \"DV\"\n        else:\n            return \"missing\"\n    else:\n        return x    \n    \ndef process_maCv(df):\n    df[\"maCv\"] = df[\"maCv\"].apply(str_normalize).apply(job_category).astype(\"category\")\n    return df\n    \ndef combine_gender(s):\n    x, y = s\n    return x if x != None else y if y != None else None\n\ndef process_gender(df):\n    df[\"gender\"] = df[[\"gioiTinh\", \"info_social_sex\"]].apply(combine_gender, axis=1).astype(\"category\")\n    return df\n\ndef process_ordinal(df):        \n    df[\"subscriberCount\"].replace(0, np.nan, inplace=True)\n    df[\"friendCount\"].replace(0, np.nan, inplace=True)\n    \n    df[\"Field_13\"] = df[\"Field_13\"].apply(lambda x: 1 if x == x else 0)\n    df[\"Field_38\"] = df[\"Field_38\"].map({0: 0.0, 1: 1.0, \"DN\": np.nan, \"TN\": np.nan, \"GD\": np.nan})\n    df[\"Field_62\"] = df[\"Field_62\"].map({\"I\": 1, \"II\": 2, \"III\": 3, \"IV\": 4, \"V\": 5, \"Ngo\u00e0i qu\u1ed1c doanh Qu\u1eadn 7\": np.nan})\n    df[\"Field_47\"] = df[\"Field_47\"].map({\"Zezo\": 0, \"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4})\n    \n    df[\"Field_27\"] = df[\"Field_27\"].replace({0.0: np.nan})\n    df[\"Field_28\"] = df[\"Field_28\"].replace({0.0: np.nan})\n        \n    for col in df.columns:\n        if df[col].dtype.name == \"object\":\n            df[col] = df[col].apply(str_normalize).astype(\"category\")\n            \n    return df\n\ndef transform(df):\n    df = process_datetime_cols(df)\n    df = process_gender(df)\n    df = process_maCv(df)\n    df = process_location(df)\n    df = process_ordinal(df)\n    df[\"null_sum\"] = df.isnull().sum(axis=1)\n    return df.drop(DROP, 1)","15a7a7ce":"def kfold(train_fe,y_label,test_fe):\n    seeds = np.random.randint(0, 10000, 1)\n    preds = 0    \n    feature_important = None\n    avg_train_gini = 0\n    avg_val_gini = 0\n\n    for s in seeds:\n        skf = StratifiedKFold(n_splits=5, random_state = 6484, shuffle=True)        \n        lgbm_param['random_state'] = 6484    \n        seed_train_gini = 0\n        seed_val_gini = 0\n        for i, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(y_label)), y_label)):                \n            X_train, X_val = train_fe.iloc[train_idx].drop([\"id\"], 1), train_fe.iloc[val_idx].drop([\"id\"], 1)                \n            y_train, y_val = y_label[train_idx], y_label[val_idx]\n\n            lgb_train = lgb.Dataset(X_train, y_train)\n            lgb_eval  = lgb.Dataset(X_val, y_val)\n\n            evals_result = {}\n            model = lgb.train(lgbm_param,\n                        lgb_train,\n                        num_boost_round=NUM_BOOST_ROUND,  \n                        early_stopping_rounds=400,\n                        feval=lgb_gini,\n                        verbose_eval= 200,\n                        evals_result=evals_result,\n                        valid_sets=[lgb_train, lgb_eval])\n\n            seed_train_gini += model.best_score[\"training\"][\"gini\"] \/ skf.n_splits\n            seed_val_gini += model.best_score[\"valid_1\"][\"gini\"] \/ skf.n_splits\n\n            avg_train_gini += model.best_score[\"training\"][\"gini\"] \/ (len(seeds) * skf.n_splits)\n            avg_val_gini += model.best_score[\"valid_1\"][\"gini\"] \/ (len(seeds) * skf.n_splits)\n\n            if feature_important is None:\n                feature_important = model.feature_importance() \/ (len(seeds) * skf.n_splits)\n            else:\n                feature_important += model.feature_importance() \/ (len(seeds) * skf.n_splits)        \n\n            pred = model.predict(test_fe.drop([\"id\"], 1))\n            preds += pred \/ (skf.n_splits * len(seeds))\n\n            print(\"Fold {}: {}\/{}\".format(i, model.best_score[\"training\"][\"gini\"], model.best_score[\"valid_1\"][\"gini\"]))\n        print(\"Seed {}: {}\/{}\".format(s, seed_train_gini, seed_val_gini))\n\n    print(\"-\" * 50)\n    print(\"Avg train gini: {}\".format(avg_train_gini))\n    print(\"Avg valid gini: {}\".format(avg_val_gini))\n    print(\"=\" * 50)\n    return preds","fe3eb0f3":"def main():\n    df_train = pd.read_csv('..\/input\/klps-creditscring-challenge-for-students\/train.csv')\n    df_test = pd.read_csv('..\/input\/klps-creditscring-challenge-for-students\/test.csv')\n    df_all = df_train.drop(['label'], 1).append(df_test)\n    \n    \n    with timer(\"Preprocess\"):\n        df_all_fe = transform(df_all.copy())\n        print(\"Bureau df shape:\", df_all_fe.shape)\n        df_all_fe['Age'] = df_all_fe.ngaySinh.apply(lambda x: 2020 - x.year)\n        df_all_fe = df_all_fe.drop('ngaySinh', axis = 1)\n        cols_select = [x for x in df_all_fe.columns if x not in DATE + DATETIME  + [f'{cat}_endDate' for cat in ['F', 'E', 'C', 'G', 'A']] + [f'{cat}_startDate' for cat in ['F', 'E', 'C', 'G', 'A']]]\n        df_fe = df_all_fe[cols_select]\n        df_fe.replace([np.inf, -np.inf], -99999, inplace=True)\n\n        y_label = df_train[\"label\"]\n        train_fe = df_fe[df_fe[\"id\"] < df_train.shape[0]]\n        test_fe = df_fe[df_fe[\"id\"] >= df_train.shape[0]]\n\n        # Label-Encoding\n        lbl = LabelEncoder()\n        for col in df_fe.columns:\n          if df_fe[col].dtype.name == \"category\":\n            train_fe[col] = train_fe[col].astype(str)\n            test_fe[col] = test_fe[col].astype(str)\n            encoder = ce.CountFrequencyCategoricalEncoder(encoding_method='frequency')\n            train_fe[col] = train_fe[col].fillna('None')\n            test_fe[col] = test_fe[col].fillna('None')\n\n            # Only take the common values in Train\/Test set\n            common_vals = list(set(train_fe[col]).intersection(set(test_fe[col])))\n\n            # Replace not-common values with \"Missing\" or np.nan\n            train_fe.loc[~train_fe[col].isin(common_vals), col] = 'Missing'\n            test_fe.loc[~test_fe[col].isin(common_vals), col] = 'Missing'\n\n            # Implement LE\n            lbl.fit(train_fe[col].tolist() + test_fe[col].tolist())\n            train_fe[col] = lbl.transform(train_fe[col])\n            test_fe[col] = lbl.transform(test_fe[col])\n\n        print(train_fe.shape)\n        print(test_fe.shape)\n    with timer(\"Kfold\"):\n        preds = kfold(train_fe,y_label,test_fe)\n        df_test[\"label\"] = preds\n        df_test[['id', 'label']].to_csv('submission1.csv', index=False)","2b17b3d0":"if __name__ == \"__main__\":\n    submission_file_name = \"submission1.csv\"\n    \n    with timer(\"Full model run\"):\n        main()","a2c54ebe":"lgbm_param = {'boosting_type': 'gbdt', \n              'colsample_bytree': 0.6602479798930369, \n              'is_unbalance': False, \n              'learning_rate': 0.01,\n              'max_depth': 15, \n              'metric': 'auc', \n              'min_child_samples': 25, \n              'num_leaves': 80,\n              'objective': 'binary', \n              'reg_alpha': 0.4693391197064131, \n              'reg_lambda': 0.16175478669541327, \n              'subsample_for_bin': 60000}\n\nNUM_BOOST_ROUND= 10000\n\nDROP = [\"gioiTinh\",\"info_social_sex\", 'currentLocationCity', 'currentLocationName', 'homeTownCity', 'homeTownName', 'partner0_B', 'partner0_K', 'partner0_L', 'partner1_B', 'partner1_D', 'partner1_E', 'partner1_F', 'partner1_K', 'partner1_L','partner2_B', 'partner2_G', 'partner2_K', 'partner2_L', 'partner3_B', 'partner3_C', 'partner3_F', 'partner3_G', 'partner3_H', 'partner3_K', 'partner3_L', 'partner4_A', 'partner4_B', 'partner4_C', 'partner4_D', 'partner4_E', 'partner4_F', 'partner4_G', 'partner4_H', 'partner4_K','partner5_B', 'partner5_C', 'partner5_D', 'partner5_E', 'partner5_F', 'partner5_H', 'partner5_K', 'partner5_L'] + [f\"Field_{c}\" for c in [11, 14, 15, 16, 17, 18, 24, 26, 30, 31, 37, 38, 42, 52, 56, 60, 61]]\nDATE = [\"Field_{}\".format(i) for i in [5, 6, 7, 8, 9, 11, 15, 25, 32, 33, 34, 35, 40]]\nDATETIME = [\"Field_{}\".format(i) for i in [1, 2, 43, 44]]\nunicode_cols = ['Field_18', 'maCv', 'diaChi', 'Field_46', 'Field_48', 'Field_49', 'Field_56', 'Field_61', 'homeTownCity', \n                'homeTownName', 'currentLocationCity', 'currentLocationName', 'currentLocationState', 'homeTownState']\nobject_cols = (unicode_cols + \n               [f'Field_{str(i)}' for i in '4 12 36 38 47 62 45 54 55 65 66 68'.split()] +\n               ['data.basic_info.locale', 'currentLocationCountry', 'homeTownCountry', 'brief'])\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\ndef Gini(y_true, y_pred):\n    # check and get number of samples\n    assert y_true.shape == y_pred.shape\n    n_samples = y_true.shape[0]\n\n    # sort rows on prediction column\n    # (from largest to smallest)\n    arr = np.array([y_true, y_pred]).transpose()\n    true_order = arr[arr[:, 0].argsort()][::-1, 0]\n    pred_order = arr[arr[:, 1].argsort()][::-1, 0]\n\n    # get Lorenz curves\n    L_true = np.cumsum(true_order) * 1. \/ np.sum(true_order)\n    L_pred = np.cumsum(pred_order) * 1. \/ np.sum(pred_order)\n    L_ones = np.linspace(1 \/ n_samples, 1, n_samples)\n\n    # get Gini coefficients (area between curves)\n    G_true = np.sum(L_ones - L_true)\n    G_pred = np.sum(L_ones - L_pred)\n\n    # normalize to true Gini coefficient\n    return G_pred * 1. \/ G_true\n\ndef lgb_gini(y_pred, dataset_true):\n    y_true = dataset_true.get_label()\n    return 'gini', Gini(y_true, y_pred), True\n\n# New feature from columns 63, 64\ndef process_63_64(z):\n    x, y = z\n    if x != x and y != y:\n        return np.nan\n    if (x, y) in [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 8.0), (7.0, 5.0), (5.0, 6.0), (9.0, 43.0), (8.0, 9.0)]: return True\n    else: return False\n\ndef subtract_date(date1,date2, df):\n    df[date1] = pd.to_datetime(df[date1], infer_datetime_format=True)\n    df[date2] = pd.to_datetime(df[date2], infer_datetime_format=True)\n    df[date1+date2] = (df[date2] - df[date1]).dt.days\n    \ndef process_ngaySinh(s):\n    if s != s:\n        return np.nan\n    try:\n        s = int(s)\n    except ValueError:\n        s = s.split(\" \")[0]\n        \n    return datetime.strptime(str(s)[:6], \"%Y%m\")\n\ndef datetime_normalize(s):\n    if s != s:\n        return np.nan\n    \n    s = s.split(\".\")[0]\n    if s[-1] == \"Z\":\n        s = s[:-1]\n        \n    date, time = s.split(\"T\")\n    datetime_obj = datetime.strptime(s, \"%Y-%m-%dT%H:%M:%S\")\n    return datetime_obj\n\ndef date_normalize(s):\n    if s != s:\n        return np.nan\n    \n    try:\n        datetime_obj = datetime.strptime(s, \"%m\/%d\/%Y\")\n    except:\n        datetime_obj = datetime.strptime(s, \"%Y-%m-%d\")\n        \n    return datetime_obj\n  \ndef process_datetime_cols(df):\n    cat_cols = []\n    for col in DATETIME:\n        df[col] = df[col].apply(datetime_normalize)\n        \n    for col in DATE:\n        if col == \"Field_34\":\n            continue\n        df[col] = df[col].apply(date_normalize)\n\n    df[\"Field_34\"] = df[\"Field_34\"].apply(process_ngaySinh)\n    df[\"ngaySinh\"] = df[\"ngaySinh\"].apply(process_ngaySinh)\n    \n    cat_cols += DATE + DATETIME\n    for col in DATE + DATETIME:\n        df[col] = df[col].dt.strftime('%d-%m-%Y')\n    \n    subtract_date('Field_5','Field_6',df)\n    subtrac_List = ['Field_1', 'Field_2', 'Field_43', 'Field_44', 'Field_7','Field_8', 'Field_9']\n    subtract_2C = list(combinations(subtrac_List, 2))\n    for l in subtract_2C:\n        subtract_date(l[0],l[1],df)\n      \n    for cat in ['F', 'E', 'C', 'G', 'A']:\n        subtract_date(f'{cat}_startDate', f'{cat}_endDate', df)\n    print(df.shape) \n    return df\n  \ndef str_normalize(s):\n    s = str(s).strip().lower()\n    s = re.sub(' +', \" \", s)\n    return s\n\ndef process_location(df):\n    for col in [\"currentLocationLocationId\", \"homeTownLocationId\", \"currentLocationLatitude\", \"currentLocationLongitude\", \n                   \"homeTownLatitude\", \"homeTownLongitude\"]:\n        df[col].replace(0, np.nan, inplace=True)\n\n#Manh Nguyen\n    df['diaChi'] = df['diaChi'].str.split(',').str[-1]\n    df[unicode_cols] = df[unicode_cols].applymap(str_normalize).applymap(lambda x: unidecode(x) if x==x else x)\n\n    df['Field_45_Q'] = df['Field_45'].str[:-3].astype('category')\n    df['Field_45_TP_55'] = df['Field_45'].str[:2] == df['Field_55']\n    df['is_homeTown_diaChi'] = df['homeTownCity'] == df['diaChi']\n    df['is_homeTown_current_city'] = df['homeTownCity'] == df['currentLocationCity']\n    df['is_homeTown_current_state'] = df['homeTownState'] == df['currentLocationState']\n    df['F48_49'] = df['Field_48'] == df['Field_49']\n    \n    #df[\"gender\"] = df[[\"gioiTinh\", \"info_social_sex\"]].apply(combine_gender, axis=1).astype(\"category\")\n    \n    df[[\"currentLocationLocationId\", \"homeTownLocationId\", \"currentLocationLatitude\", \"currentLocationLongitude\", \n        \"homeTownLatitude\", \"homeTownLongitude\"]].replace(0, np.nan, inplace=True) # value == 0: noisy\n\n    df[[\"currentLocationLocationId\", \"homeTownLocationId\"]] = (df[[\"currentLocationLocationId\", \"homeTownLocationId\"]]\n                                                             .applymap(str_normalize).astype(\"category\"))\n    df[object_cols] = df[object_cols].astype('category')\n    df['F_63_64'] = df[['Field_63', 'Field_64']].apply(process_63_64, axis=1).astype('category')\n#     df[\"currentLocationLocationId\"] = df[\"currentLocationLocationId\"].apply(str_normalize).astype(\"category\")\n#     df[\"homeTownLocationId\"] = df[\"homeTownLocationId\"].apply(str_normalize).astype(\"category\")\n    return df\n\ndef job_category(x):\n    if type(x) == str:\n        if \"c\u00f4ng nh\u00e2n\" in x or \"cnv\" in x or \"cn\" in x or \"may c\u00f4ng nghi\u1ec7p\" in x or \"lao \u0111\u1ed9ng\" in x\\\n        or \"th\u1ee3\" in x or \"co\u00f5ng nha\u00f5n tr\u1eed\u00f9c tie\u1ecfp ma\u1ef1y may co\u00f5ng nghie\u1ecdp\" in x or \"c.n\" in x or \"l\u0111\" in x:\n            return \"CN\"\n        elif \"gi\u00e1o vi\u00ean\" in x or \"gv\" in x or \"g\u00edao vi\u00ean\" in x:\n            return \"GV\"\n        elif \"nh\u00e2n vi\u00ean\" in x or \"k\u1ebf to\u00e1n\" in x or \"c\u00e1n b\u1ed9\" in x or \"nv\" in x or \"cb\" in x or \"nh\u00f5n vi\u1eddn\" in x:\n            return \"NV\"\n        elif \"t\u00e0i x\u1ebf\" in x or \"l\u00e1i\" in x or \"t\u00e0i x\u00ea\" in x:\n            return \"TX\"\n        elif \"qu\u1ea3n l\u00fd\" in x or \"ph\u00f3 ph\u00f2ng\" in x or \"hi\u1ec7u ph\u00f3\" in x:\n            return \"QL\"\n        elif \"undefined\" in x:\n            return \"missing\"\n        elif \"gi\u00e1m \u0111\u1ed1c\" in x or \"hi\u1ec7u tr\u01b0\u1edfng\" in x:\n            return \"G\u0110\"\n        elif \"ph\u1ee5c v\u1ee5\" in x:\n            return \"PV\"\n        elif \"chuy\u00ean vi\u00ean\" in x:\n            return  \"CV\"\n        elif \"b\u00e1c s\u0129\" in x or \"d\u01b0\u1ee3c s\u0129\" in x or \"y s\u0129\" in x or \"y s\u1ef9\" in x:\n            return \"BS\"\n        elif \"y t\u00e1\" in x:\n            return \"YT\"\n        elif \"h\u1ed9 sinh\" in x:\n            return \"HS\"\n        elif \"ch\u1ee7 t\u1ecbch\" in x:\n            return \"CT\"\n        elif \"b\u1ebfp\" in x:\n            return \"\u0110B\"\n        elif \"s\u01b0\" in x:\n            return \"KS\"\n        elif \"d\u01b0\u1ee1ng\" in x:\n            return \"\u0110D\"\n        elif \"k\u1ef9 thu\u1eadt\" in x or \"k\u0129 thu\u1eadt\" in x:\n            return \"KTV\"\n        elif \"di\u1ec5n vi\u00ean\" in x:\n            return \"DV\"\n        else:\n            return \"missing\"\n    else:\n        return x    \n    \ndef process_maCv(df):\n    df[\"maCv\"] = df[\"maCv\"].apply(str_normalize).apply(job_category).astype(\"category\")\n    return df\n    \ndef combine_gender(s):\n    x, y = s\n    return x if x != None else y if y != None else None\n\ndef process_gender(df):\n    df[\"gender\"] = df[[\"gioiTinh\", \"info_social_sex\"]].apply(combine_gender, axis=1).astype(\"category\")\n    return df\n\ndef process_ordinal(df):        \n    df[\"subscriberCount\"].replace(0, np.nan, inplace=True)\n    df[\"friendCount\"].replace(0, np.nan, inplace=True)\n    \n    df[\"Field_13\"] = df[\"Field_13\"].apply(lambda x: 1 if x == x else 0)\n    df[\"Field_38\"] = df[\"Field_38\"].map({0: 0.0, 1: 1.0, \"DN\": np.nan, \"TN\": np.nan, \"GD\": np.nan})\n    df[\"Field_62\"] = df[\"Field_62\"].map({\"I\": 1, \"II\": 2, \"III\": 3, \"IV\": 4, \"V\": 5, \"Ngo\u00e0i qu\u1ed1c doanh Qu\u1eadn 7\": np.nan})\n    df[\"Field_47\"] = df[\"Field_47\"].map({\"Zezo\": 0, \"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4})\n    \n    df[\"Field_27\"] = df[\"Field_27\"].replace({0.0: np.nan})\n    df[\"Field_28\"] = df[\"Field_28\"].replace({0.0: np.nan})\n        \n    for col in df.columns:\n        if df[col].dtype.name == \"object\":\n            df[col] = df[col].apply(str_normalize).astype(\"category\")\n            \n    return df\n\ndef transform(df):\n    df = process_datetime_cols(df)\n    df = process_gender(df)\n    df = process_maCv(df)\n    df = process_location(df)\n    df = process_ordinal(df)\n    df[\"null_sum\"] = df.isnull().sum(axis=1)\n    return df.drop(DROP, 1)\n\ndef kfold(train_fe,y_label,test_fe):\n    seeds = np.random.randint(0, 10000, 1)\n    preds = 0    \n    feature_important = None\n    avg_train_gini = 0\n    avg_val_gini = 0\n\n    for s in seeds:\n        skf = StratifiedKFold(n_splits=5, random_state = 6484, shuffle=True)        \n        lgbm_param['random_state'] = 6484    \n        seed_train_gini = 0\n        seed_val_gini = 0\n        for i, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(y_label)), y_label)):                \n            X_train, X_val = train_fe.iloc[train_idx].drop([\"id\"], 1), train_fe.iloc[val_idx].drop([\"id\"], 1)                \n            y_train, y_val = y_label[train_idx], y_label[val_idx]\n\n            lgb_train = lgb.Dataset(X_train, y_train)\n            lgb_eval  = lgb.Dataset(X_val, y_val)\n\n            evals_result = {}\n            model = lgb.train(lgbm_param,\n                        lgb_train,\n                        num_boost_round=NUM_BOOST_ROUND,  \n                        early_stopping_rounds=400,\n                        feval=lgb_gini,\n                        verbose_eval= 200,\n                        evals_result=evals_result,\n                        valid_sets=[lgb_train, lgb_eval])\n\n            seed_train_gini += model.best_score[\"training\"][\"gini\"] \/ skf.n_splits\n            seed_val_gini += model.best_score[\"valid_1\"][\"gini\"] \/ skf.n_splits\n\n            avg_train_gini += model.best_score[\"training\"][\"gini\"] \/ (len(seeds) * skf.n_splits)\n            avg_val_gini += model.best_score[\"valid_1\"][\"gini\"] \/ (len(seeds) * skf.n_splits)\n\n            if feature_important is None:\n                feature_important = model.feature_importance() \/ (len(seeds) * skf.n_splits)\n            else:\n                feature_important += model.feature_importance() \/ (len(seeds) * skf.n_splits)        \n\n            pred = model.predict(test_fe.drop([\"id\"], 1))\n            preds += pred \/ (skf.n_splits * len(seeds))\n\n            print(\"Fold {}: {}\/{}\".format(i, model.best_score[\"training\"][\"gini\"], model.best_score[\"valid_1\"][\"gini\"]))\n        print(\"Seed {}: {}\/{}\".format(s, seed_train_gini, seed_val_gini))\n\n    print(\"-\" * 50)\n    print(\"Avg train gini: {}\".format(avg_train_gini))\n    print(\"Avg valid gini: {}\".format(avg_val_gini))\n    print(\"=\" * 50)\n    return preds\n\ndef main():\n    df_train = pd.read_csv('..\/input\/klps-creditscring-challenge-for-students\/train.csv')\n    df_test = pd.read_csv('..\/input\/klps-creditscring-challenge-for-students\/test.csv')\n    df_all = df_train.drop(['label'], 1).append(df_test)\n    \n    \n    with timer(\"Preprocess\"):\n        df_all_fe = transform(df_all.copy())\n        print(\"Bureau df shape:\", df_all_fe.shape)\n        df_all_fe['Age'] = df_all_fe.ngaySinh.apply(lambda x: 2020 - x.year)\n        df_all_fe = df_all_fe.drop('ngaySinh', axis = 1)\n        cols_select = [x for x in df_all_fe.columns if x not in DATE + DATETIME  + [f'{cat}_endDate' for cat in ['F', 'E', 'C', 'G', 'A']] + [f'{cat}_startDate' for cat in ['F', 'E', 'C', 'G', 'A']]]\n        df_fe = df_all_fe[cols_select]\n        df_fe.replace([np.inf, -np.inf], -99999, inplace=True)\n\n        y_label = df_train[\"label\"]\n        train_fe = df_fe[df_fe[\"id\"] < df_train.shape[0]]\n        test_fe = df_fe[df_fe[\"id\"] >= df_train.shape[0]]\n\n        # Label-Encoding\n        lbl = LabelEncoder()\n        for col in df_fe.columns:\n          if df_fe[col].dtype.name == \"category\":\n            train_fe[col] = train_fe[col].astype(str)\n            test_fe[col] = test_fe[col].astype(str)\n            encoder = ce.CountFrequencyCategoricalEncoder(encoding_method='frequency')\n            train_fe[col] = train_fe[col].fillna('None')\n            test_fe[col] = test_fe[col].fillna('None')\n\n            # Only take the common values in Train\/Test set\n            common_vals = list(set(train_fe[col]).intersection(set(test_fe[col])))\n\n            # Take if vals appeared both 5 times\n            #common_vals = set(train_fe[col].value_counts()[train_fe[col].value_counts() > 4].index).intersection(test_fe[col].value_counts()[test_fe[col].value_counts()>4].index)\n\n            # Replace not-common values with \"Missing\" or np.nan\n            train_fe.loc[~train_fe[col].isin(common_vals), col] = 'Missing'\n            test_fe.loc[~test_fe[col].isin(common_vals), col] = 'Missing'\n\n            # Implement LE\n            lbl.fit(train_fe[col].tolist() + test_fe[col].tolist())\n            train_fe[col] = lbl.transform(train_fe[col])\n            test_fe[col] = lbl.transform(test_fe[col])\n\n        print(train_fe.shape)\n        print(test_fe.shape)\n    with timer(\"Kfold\"):\n        preds = kfold(train_fe,y_label,test_fe)\n        df_test[\"label\"] = preds\n        df_test[['id', 'label']].to_csv('submission2.csv', index=False)\n\nif __name__ == \"__main__\":\n    submission_file_name = \"submission2.csv\"\n    \n    with timer(\"Full model run\"):\n        main()","c5fc1af5":"from lightgbm import LGBMClassifier\nimport category_encoders as ce\n# %matplotlib inline\n\ntrain_df = pd.read_csv('..\/input\/klps-creditscring-challenge-for-students\/train.csv')\ntest_df = pd.read_csv('..\/input\/klps-creditscring-challenge-for-students\/test.csv')\n\n\"\"\"## 2. Feature Engineering\"\"\"\n\ndef feature_engineering(train, test):\n    labels = train['label']\n    data = train.drop(columns=['label']).append(test, ignore_index=True)\n    remove_features = ['Field_1', 'Field_2', 'Field_4', 'Field_5', 'Field_6', 'Field_7', 'Field_8', 'Field_9',\n                       'Field_11', 'Field_12', 'Field_15', 'Field_18', 'Field_25', 'Field_32', 'Field_33',\n                       'Field_34', 'Field_35', 'gioiTinh', 'diaChi', 'Field_36', 'Field_38', 'Field_40',\n                       'Field_43', 'Field_44', 'Field_45', 'Field_46', 'Field_47', 'Field_48', 'Field_49',\n                       'Field_54', 'Field_55', 'Field_56', 'Field_61', 'Field_62', 'Field_65', 'Field_66',\n                       'Field_68', 'maCv', 'info_social_sex', 'data.basic_info.locale', 'currentLocationCity',\n                       'currentLocationCountry', 'currentLocationName', 'currentLocationState', 'homeTownCity',\n                       'homeTownCountry', 'homeTownName', 'homeTownState', 'F_startDate', 'F_endDate',\n                       'E_startDate', 'E_endDate', 'C_startDate', 'C_endDate', 'G_startDate', 'G_endDate',\n                       'A_startDate', 'A_endDate', 'brief']\n\n    cat_features_count_encode = ['Field_4', 'Field_12', 'Field_18', 'Field_34', 'gioiTinh', 'diaChi', 'Field_36',\n                                 'Field_38', 'Field_45', 'Field_46', 'Field_47', 'Field_48', 'Field_49',\n                       'Field_54', 'Field_55', 'Field_56', 'Field_61', 'Field_62', 'Field_65', 'Field_66',\n                       'Field_68', 'maCv', 'info_social_sex', 'data.basic_info.locale', 'currentLocationCity',\n                       'currentLocationCountry', 'currentLocationName', 'currentLocationState', 'homeTownCity',\n                       'homeTownCountry', 'homeTownName', 'homeTownState', 'brief']\n    \n    cat_date_array = ['Field_1', 'Field_2', 'Field_5', 'Field_6', 'Field_7', 'Field_8', 'Field_9', 'Field_11',\n                      'Field_15', 'Field_25', 'Field_32', 'Field_33', 'Field_35', 'Field_40', 'Field_43',\n                      'Field_44', 'F_startDate', 'F_endDate', 'E_startDate', 'E_endDate', 'C_startDate',\n                      'C_endDate', 'G_startDate', 'G_endDate', 'A_startDate', 'A_endDate']\n    for col in cat_date_array:\n        data[col+'Year'] = pd.DatetimeIndex(data[col]).year\n        data[col+'Month'] = pd.DatetimeIndex(data[col]).month\n        data[col+'Day'] = pd.DatetimeIndex(data[col]).day\n    \n    data[remove_features].fillna(\"Missing\", inplace=True)\n    count_en = ce.CountEncoder()\n    cat_ce = count_en.fit_transform(data[cat_features_count_encode])\n    data = data.join(cat_ce.add_suffix(\"_ce\"))\n    \n    data.replace(\"None\", -1, inplace=True)\n    data.replace(\"Missing\", -999, inplace=True)\n    data.fillna(-999, inplace=True)\n\n    _train = data[data['id'] < 53030]\n    _test = data[data['id'] >= 53030]\n    \n    _train[\"label\"] = labels\n\n    _train.drop(columns=remove_features, inplace=True)\n    _test.drop(columns=remove_features, inplace=True)\n    \n    return _train, _test\n\ntrain_data, test_data = feature_engineering(train_df, test_df)\n\n\"\"\"## 3. Feature Selection\"\"\"\n\ndef calculate_woe_iv(dataset, feature, target):\n    lst = []\n    for i in range(dataset[feature].nunique()):\n        val = list(dataset[feature].unique())[i]\n        lst.append({\n            'Value': val,\n            'All': dataset[dataset[feature] == val].count()[feature],\n            'Good': dataset[(dataset[feature] == val) & (dataset[target] == 0)].count()[feature],\n            'Bad': dataset[(dataset[feature] == val) & (dataset[target] == 1)].count()[feature]\n        })\n    dset = pd.DataFrame(lst)\n    dset['Distr_Good'] = dset['Good'] \/ dset['Good'].sum()\n    dset['Distr_Bad'] = dset['Bad'] \/ dset['Bad'].sum()\n    dset['WoE'] = np.log(dset['Distr_Good'] \/ dset['Distr_Bad'])\n    dset = dset.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n    dset['IV'] = (dset['Distr_Good'] - dset['Distr_Bad']) * dset['WoE']\n    iv = dset['IV'].sum()\n    dset = dset.sort_values(by='WoE')\n    return dset, iv\n\nUSELESS_PREDICTOR = []\nWEAK_PREDICTOR = []\nMEDIUM_PREDICTOR = []\nSTRONG_PREDICTOR = []\nGOOD_PREDICTOR = []\nIGNORE_FEATURE = USELESS_PREDICTOR + WEAK_PREDICTOR\nfor col in train_data.columns:\n    if col == 'label' or col == 'id': continue\n    elif col in IGNORE_FEATURE: continue\n    else:\n        print('WoE and IV for column: {}'.format(col))\n        final, iv = calculate_woe_iv(train_data, col, 'label')\n        iv = round(iv,2)\n        print('IV score: ' + str(iv))\n        print('\\n')\n        if (iv < 0.02) and col not in USELESS_PREDICTOR:\n            USELESS_PREDICTOR.append(col)\n        elif iv >= 0.02 and iv < 0.1 and col not in WEAK_PREDICTOR:\n            WEAK_PREDICTOR.append(col)\n        elif iv >= 0.1 and iv < 0.3 and col not in MEDIUM_PREDICTOR:\n            MEDIUM_PREDICTOR.append(col)\n        elif iv >= 0.3 and iv < 0.5 and col not in STRONG_PREDICTOR:\n            STRONG_PREDICTOR.append(col)\n        elif iv >= 0.5 and col not in GOOD_PREDICTOR:\n            GOOD_PREDICTOR.append(col)\n\nprint('USELESS_PREDICTOR')\nprint(len(USELESS_PREDICTOR))\nprint('WEAK_PREDICTOR')\nprint(len(WEAK_PREDICTOR))\nprint('MEDIUM_PREDICTOR')\nprint(len(MEDIUM_PREDICTOR))\nprint('STRONG_PREDICTOR')\nprint(len(STRONG_PREDICTOR))\nprint('GOOD_PREDICTOR')\nprint(len(GOOD_PREDICTOR))\n\nIGNORE_FEATURE = USELESS_PREDICTOR\nfinal_train_data = train_data.drop(columns=IGNORE_FEATURE)\nfinal_test_data = test_data.drop(columns=[col for col in IGNORE_FEATURE if col not in ['label']])\n\n\"\"\"## 4. Build Model\"\"\"\n\nimport gc\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc, f1_score, confusion_matrix, recall_score, classification_report\nimport seaborn as sns\n\n\n# Display\/plot feature importance\ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances01.png')\n    \ndef display_roc_curve(y_, oof_preds_,sub_preds_,folds_idx_):\n    # Plot ROC curves\n    plt.figure(figsize=(6,6))\n    scores = [] \n    for n_fold, (_, val_idx) in enumerate(folds_idx_):  \n        # Plot the roc curve\n        fpr, tpr, thresholds = roc_curve(y_.iloc[val_idx], oof_preds_[val_idx])\n        score = 2 * auc(fpr, tpr) -1\n        scores.append(score)\n        plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (Gini = %0.4f)' % (n_fold + 1, score))\n    \n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n    fpr, tpr, thresholds = roc_curve(y_, oof_preds_)\n    score = 2 * auc(fpr, tpr) -1\n    plt.plot(fpr, tpr, color='b',\n             label='Avg ROC (Gini = %0.4f $\\pm$ %0.4f)' % (score, np.std(scores)),\n             lw=2, alpha=.8)\n    \n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('LightGBM ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.tight_layout()\n    plt.savefig('roc_curve.png')\n\n\n# LightGBM GBDT with Stratified KFold\ndef kfold_lightgbm(train_df, test_df, num_folds, stratified = False, debug= False):\n    # Divide in training\/validation and test data\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n    gc.collect()\n    # Cross validation model\n    folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=500)\n\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in ['label','id']]\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['label'])):        \n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['label'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['label'].iloc[valid_idx]\n\n        clf = LGBMClassifier(\n            nthread=4,\n            n_estimators=10000,\n            learning_rate=0.02,\n            num_leaves=128,\n            colsample_bytree=0.9497036,\n            subsample=0.8715623,\n            max_depth=8,\n            reg_alpha=0.041545473,\n            reg_lambda=0.0735294,\n            min_split_gain=0.0222415,\n            min_child_weight=39.3259775,\n            silent=-1,\n            verbose=-1\n        )\n\n        clf.fit(train_x, train_y.ravel(), eval_set=[(train_x, train_y), (valid_x, valid_y)], \n            eval_metric='auc', verbose= 1000, early_stopping_rounds= 200)\n\n        oof_pred = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n        \n        pred = clf.predict(valid_x, num_iteration=clf.best_iteration_)\n        print('F1 Score: ' + str( f1_score(valid_y, pred) ))\n        print('Recall Score: ' + str( recall_score(valid_y, pred) ))\n        \n        sub_pred = clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] \/ folds.n_splits\n        oof_preds[valid_idx] = oof_pred\n        sub_preds += sub_pred\n                \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n        del clf, train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    print('Full AUC score %.6f' % roc_auc_score(train_df['label'], oof_preds))\n    \n    folds_idx = [(trn_idx, val_idx) for trn_idx, val_idx in folds.split(train_df[feats], train_df['label'])]\n    display_roc_curve(y_=train_df['label'],oof_preds_=oof_preds,sub_preds_ = sub_preds, folds_idx_=folds_idx)\n    \n    # Write submission file and plot feature importance\n    test_df['label'] = sub_preds\n    test_df[['id', 'label']].to_csv('submission3.csv', index= False)\n    #display_importances(feature_importance_df)\n\nkfold_lightgbm(final_train_data, final_test_data, 5)","33509551":"plt.style.use('fivethirtyeight')\n\nfrom unidecode import unidecode\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nimport category_encoders as ce\n\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('..\/input\/klps-creditscring-challenge-for-students\/train.csv')\ntest = pd.read_csv('..\/input\/klps-creditscring-challenge-for-students\/test.csv')\n\n\"\"\"# 2. Feature Engineering\"\"\"\n\n# Drop some columns are duplicated, with correlation = NaN\nignore_columns = ([\"gioiTinh\", \"info_social_sex\", \"ngaySinh\", \"namSinh\"] + \n        [f\"Field_{c}\" for c in [14, 16, 17, 24, 26, 30, 31, 37, 52, 57]] + \n        ['partner0_K', 'partner0_L', \n         'partner1_B', 'partner1_D', 'partner1_E', 'partner1_F', 'partner1_K', 'partner1_L',\n         'partner2_B', 'partner2_G', 'partner2_K', 'partner2_L',\n         'partner3_B', 'partner3_C', 'partner3_F', 'partner3_G', 'partner3_H', 'partner3_K', 'partner3_L',\n         *['partner4_' + i for i in 'ABCDEFGHK'],\n         'partner5_B', 'partner5_C', 'partner5_H', 'partner5_K', 'partner5_L'])\n\n# Some auto columns could make new better columns\nall_auto_columns = list(set([c for c in train.columns if train[c].dtype in [np.int64, np.float64]])\n                    .difference(ignore_columns + ['currentLocationLocationId', 'homeTownLocationId', 'label', 'id']))\n\nauto_columns_1 = [c for c in all_auto_columns if 'Field_' in c]\nauto_columns_2 = [c for c in all_auto_columns if 'partner' in c]\nauto_columns_3 = [c for c in all_auto_columns if 'num' in c]\nauto_columns_4 = [c for c in all_auto_columns if c not in auto_columns_1 + auto_columns_2 + auto_columns_3]\nprint(len(auto_columns_1), len(auto_columns_2), len(auto_columns_3), len(auto_columns_4), len(all_auto_columns))\n\n\"\"\"### 2.1. Datetime columns\"\"\"\n\ndate_cols = [\"Field_{}\".format(i) for i in [5, 6, 7, 8, 9, 11, 15, 25, 32, 33, 35, 40]]\ndatetime_cols = [\"Field_{}\".format(i) for i in [1, 2, 43, 44]]\ncorrect_dt_cols = ['Field_34', 'ngaySinh']\ncat_cols = date_cols + datetime_cols + correct_dt_cols\n\n# Normalize Field_34, ngaySinh\ndef ngaysinh_34_normalize(s):\n    if s != s: return np.nan\n    try: s = int(s)\n    except ValueError: s = s.split(\" \")[0]\n    return datetime.strptime(str(s)[:6], \"%Y%m\")\n\n# Normalize datetime data\ndef datetime_normalize(s):\n    if s != s: return np.nan\n    s = s.split(\".\")[0]\n    if s[-1] == \"Z\": s = s[:-1]\n    return datetime.strptime(s, \"%Y-%m-%dT%H:%M:%S\")\n\n# Normalize date data\ndef date_normalize(s):\n    if s != s: return np.nan\n    try: t = datetime.strptime(s, \"%m\/%d\/%Y\")\n    except: t = datetime.strptime(s, \"%Y-%m-%d\")\n    return t\n\ndef process_datetime_cols(df):\n    df[datetime_cols] = df[datetime_cols].applymap(datetime_normalize)  \n    df[date_cols] = df[date_cols].applymap(date_normalize)\n    df[correct_dt_cols] = df[correct_dt_cols].applymap(ngaysinh_34_normalize)\n\n    # Some delta columns\n    for i, j in zip('43 1 2'.split(), '1 2 44'.split()): df[f'DT_{j}_{i}'] = (df[f'Field_{j}'] - df[f'Field_{i}']).dt.seconds\n    for i, j in zip('5 6 7 33 8 11 9 15 25 6 7 8 9 15 25 2'.split(), '6 34 33 40 11 35 15 25 32 7 8 9 15 25 32 8'.split()): \n        df[f'DT_{j}_{i}'] = (df[f'Field_{j}'] - df[f'Field_{i}']).dt.days\n    \n    # Age, month\n    df['age'] = 2020 - pd.DatetimeIndex(df['ngaySinh']).year\n    df['birth_month'] = pd.DatetimeIndex(df['ngaySinh']).month\n    \n    # Days from current time & isWeekday\n    for col in cat_cols:\n        name = col.split('_')[-1]\n        df[f'is_WD_{name}'] = df[col].dt.dayofweek.isin(range(5))\n        df[f'days_from_now_{name}'] = (datetime.now() - pd.DatetimeIndex(df[col])).days\n        df[col] = df[col].dt.strftime('%m-%Y')\n    \n    # Delta for x_startDate and x_endDate\n    for cat in ['F', 'E', 'C', 'G', 'A']:\n        df[f'{cat}_startDate'] = pd.to_datetime(df[f\"{cat}_startDate\"], infer_datetime_format=True)\n        df[f'{cat}_endDate'] = pd.to_datetime(df[f\"{cat}_endDate\"], infer_datetime_format=True)\n        \n        df[f'{cat}_start_end'] = (df[f'{cat}_endDate'] - df[f'{cat}_startDate']).dt.days\n        \n    for i, j in zip('F E C G'.split(), 'E C G A'.split()):\n        df[f'{j}_{i}_startDate'] = (df[f'{j}_startDate'] - df[f'{i}_startDate']).dt.days\n        df[f'{j}_{i}_endDate'] = (df[f'{j}_endDate'] - df[f'{i}_endDate']).dt.days\n    \n    temp_date = [f'{i}_startDate' for i in 'ACEFG'] + [f'{i}_endDate' for i in 'ACEFG']\n    \n    for col in temp_date:\n        df[col] = df[col].dt.strftime('%m-%Y')\n        \n    for col in cat_cols + temp_date:\n        df[col] = df[col].astype(\"category\")\n        \n    return df\n\n\"\"\"### 2.2. Categorical columns\"\"\"\n\nunicode_cols = ['Field_18', 'maCv', 'diaChi', 'Field_46', 'Field_48', 'Field_49', 'Field_56', 'Field_61', 'homeTownCity', \n                'homeTownName', 'currentLocationCity', 'currentLocationName', 'currentLocationState', 'homeTownState']\nobject_cols = (unicode_cols + \n               [f'Field_{str(i)}' for i in '4 12 36 38 47 62 45 54 55 65 66 68'.split()] +\n               ['data.basic_info.locale', 'currentLocationCountry', 'homeTownCountry', 'brief'])\n\ndef str_normalize(s):\n    s = str(s).strip().lower()\n    s = re.sub(' +', \" \", s)\n    return s\n\ndef combine_gender(s):\n    x, y = s \n    if x != x and y != y: return \"nan\"\n    if x != x: return y.lower()\n    return x.lower()\n\ndef process_categorical_cols(df):\n    df['diaChi'] = df['diaChi'].str.split(',').str[-1]\n    df[unicode_cols] = df[unicode_cols].applymap(str_normalize).applymap(lambda x: unidecode(x) if x==x else x)\n    \n    # Normalize some columns\n    df[\"Field_38\"] = df[\"Field_38\"].map({0: 0.0, 1: 1.0, \"DN\": np.nan, \"TN\": np.nan, \"GD\": np.nan})\n    df[\"Field_62\"] = df[\"Field_62\"].map({\"I\": 1, \"II\": 2, \"III\": 3, \"IV\": 4, \"V\": 5, \"Ngo\u00e0i qu\u1ed1c doanh Qu\u1eadn 7\": np.nan})\n    df[\"Field_47\"] = df[\"Field_47\"].map({\"Zezo\": 0, \"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4})\n    \n    # Make some new features\n    df['Field_45_Q'] = df['Field_45'].str[:-3].astype('category')\n    df['Field_45_TP_55'] = df['Field_45'].str[:2] == df['Field_55']\n    df['is_homeTown_diaChi'] = df['homeTownCity'] == df['diaChi']\n    df['is_homeTown_current_city'] = df['homeTownCity'] == df['currentLocationCity']\n    df['is_homeTown_current_state'] = df['homeTownState'] == df['currentLocationState']\n    df['F48_49'] = df['Field_48'] == df['Field_49']\n    \n    df[\"gender\"] = df[[\"gioiTinh\", \"info_social_sex\"]].apply(combine_gender, axis=1).astype(\"category\")\n    \n    df[[\"currentLocationLocationId\", \"homeTownLocationId\", \"currentLocationLatitude\", \"currentLocationLongitude\", \n        \"homeTownLatitude\", \"homeTownLongitude\"]].replace(0, np.nan, inplace=True) # value == 0: noisy\n\n    df[[\"currentLocationLocationId\", \"homeTownLocationId\"]] = (df[[\"currentLocationLocationId\", \"homeTownLocationId\"]]\n                                                             .applymap(str_normalize).astype(\"category\"))\n    df[object_cols] = df[object_cols].astype('category')\n    \n    return df\n\n\"\"\"### 2.3. Others\"\"\"\n\n# New feature from columns 63, 64\ndef process_63_64(z):\n    x, y = z\n    if x != x and y != y:\n        return np.nan\n    if (x, y) in [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 8.0), (7.0, 5.0), (5.0, 6.0), (9.0, 43.0), (8.0, 9.0)]: return True\n    else: return False\n    \ndef process_others(df):        \n    df[[\"Field_27\", \"Field_28\"]].replace(0.0, np.nan, inplace=True)\n    df['F18_isnumeric'] = df['Field_18'].str.isnumeric()\n    df['F18_isalpha'] = df['Field_18'].str.isalpha()\n    \n    # Delta from some pairs of columns\n    for i, j in [(20, 27), (28, 27), (39, 41), (41, 42), (50, 51), (51, 53)]:\n        df[f'F{str(i)}_{str(j)}_delta'] = df[f'Field_{str(j)}'] - df[f'Field_{str(i)}']\n    df['F_59_60'] = df['Field_59'] - df['Field_60'] - 2\n    df['F_63_64'] = df[['Field_63', 'Field_64']].apply(process_63_64, axis=1).astype('category')\n    \n    # Mean, std from partnerX columns\n    for i in '1 2 3 4 5'.split():\n        col = [c for c in df.columns if f'partner{i}' in c]\n        df[f'partner{i}_mean'] = df[col].mean(axis=1)\n        df[f'partner{i}_std'] = df[col].std(axis=1)\n\n    # Reference columns\n    columns = set(df.columns).difference(ignore_columns)\n    df['cnt_NaN'] = df[columns].isna().sum(axis=1)\n    df['cnt_True'] = df[columns].applymap(lambda x: isinstance(x, bool) and x).sum(axis=1)\n    df['cnt_False'] = df[columns].applymap(lambda x: isinstance(x, bool) and not x).sum(axis=1)\n\n    # Combinations of auto columns\n    lst_combination = (list(combinations(auto_columns_2, 2)) + list(combinations(auto_columns_3, 2)) + list(combinations(auto_columns_4, 2)))\n    for l, r in lst_combination:\n        for func in 'add subtract divide multiply'.split():\n            df[f'auto_{func}_{l}_{r}'] = getattr(np, func)(df[l], df[r])\n            \n    return df\n\n\"\"\"### 2.4. Combine all parts\"\"\"\n\ndef transform(df):\n    df = process_datetime_cols(df)\n    df = process_categorical_cols(df)\n    df = process_others(df)\n    return df.drop(ignore_columns, axis=1)\n\ntrain = transform(train)\ntest = transform(test)\n\n\"\"\"### 2.5. Try Count Encoding\"\"\"\n\n# Support catboost modelling\ncat_features = [c for c in train.columns if (train[c].dtype not in [np.float64, np.int64])]\ntrain[cat_features] = train[cat_features].astype(str)\ntest[cat_features] = test[cat_features].astype(str)\n\n# Create the encoder\nt = pd.concat([train, test]).reset_index(drop=True)\ncount_enc = ce.CountEncoder().fit_transform(t[cat_features])\ntt = t.join(count_enc.add_suffix(\"_count\"))\n\nf2_train = tt.loc[tt.index < train.shape[0]]\nf2_test = tt.loc[tt.index >= train.shape[0]]\n\ncolumns = sorted(set(f2_train.columns).intersection(f2_test.columns))\nprint(len(columns))\n\n\"\"\"# 3. Modelling\n\n### 3.1. Model\n\"\"\"\n\ngini, feature_importance_df = {}, pd.DataFrame()\n\nTRAIN, TEST = f2_train[columns].drop(['id', 'label'], axis=1), f2_test[columns].drop(['id', 'label'], axis=1)\nLABEL = f2_train['label']\npreds, oof_preds = np.zeros(TRAIN.shape[0]), {}\n\ncv = StratifiedKFold(n_splits=5, shuffle=True)\nfor i, (train_idx, val_idx) in enumerate(cv.split(TRAIN, LABEL)):\n    X_train, y_train = TRAIN.iloc[train_idx], LABEL.iloc[train_idx]\n    X_val, y_val = TRAIN.iloc[val_idx], LABEL.iloc[val_idx]\n\n    gbm = CatBoostClassifier(eval_metric='AUC', \n                             use_best_model=True,\n                             iterations=1000, \n                             learning_rate=0.1, \n                             random_seed=42).fit(X_train, y_train, \n                                                 cat_features=set(cat_features),\n                                                 eval_set=(X_val, y_val), verbose=500)\n\n    y_pred = gbm.predict(X_val)\n    y_pred_proba = gbm.predict_proba(X_val)[:, 1]\n        \n    preds[val_idx] = y_pred_proba\n    oof_preds[f'F{i+1}'] = gbm.predict_proba(TEST)[:, 1]\n    \n    gini[f'F{i+1}'] = 2 * roc_auc_score(y_val, y_pred_proba) - 1\n    \n    # For create feature importances\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = TRAIN.columns\n    fold_importance_df[\"importance\"] = gbm.feature_importances_\n    fold_importance_df[\"fold\"] = i + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    print('Fold %2d GINI : %.5f' % (i + 1, 2*roc_auc_score(y_val, y_pred_proba) - 1))\n    \n# Resulting\nroc_auc = roc_auc_score(LABEL, preds)\nprint('Avg GINI score:', 2*roc_auc - 1)\n\nresult = np.array(list(gini.values()))\nprint('GINI: {:.5f} +- {:.5f}'.format(result.mean(), result.std()))\n\n# One tips to better performance is re-training the model with dropped low important columns\nto_drop = feature_importance_df[feature_importance_df.importance == 0]['feature'].unique()\nto_drop.shape\n\n\"\"\"# 4. Submisison\"\"\"\n\ntest['label'] = pd.DataFrame(oof_preds).mean(axis=1).values\ntest[['id', 'label']].to_csv('submission4.csv', index=False)","90d4e5a4":"sub1 = pd.read_csv('.\/submission1.csv')\nsub2 = pd.read_csv('.\/submission2.csv')\nsub3 = pd.read_csv('.\/submission3.csv')\nsub4 = pd.read_csv('.\/submission4.csv')\nsubmission_final = pd.DataFrame()\nsubmission_final['id'] = sub1['id']\nsubmission_final['label'] = sub1['label']*0.4 + sub2['label']*0.1 + sub3['label']*0.2 + sub1['label']*0.3\nsubmission_final.to_csv('submission_final.csv', index=False)\nsubmission_final.head()","e04a5a91":"# Model 3\nModel n\u00e0y m\u00ecnh d\u1ef1a tr\u00ean solution share b\u1edfi anh \u0110\u1eb7ng Duy Thanh (https:\/\/www.kaggle.com\/duythanhng\/kalapa-s-creditscoring-challenge-simple), c\u0169ng s\u1eed d\u1ee5ng Lightgbm.","3f337838":"Xin ch\u00e0o, m\u00ecnh l\u00e0 Nguy\u1ec5n Nh\u01b0 Ng\u00e2n - team UEBers. M\u00ecnh xin ph\u00e9p \u0111\u01b0\u1ee3c chia s\u1ebb solution \u0111\u1ea1t #8 Gi\u1ea3i \u0111\u1ed3ng \u0111\u1ed9i cu\u1ed9c thi Kalapa Credit Scoring Challenge. \nT\u1ea5t c\u1ea3 4 th\u00e0nh vi\u00ean trong team m\u00ecnh \u0111\u1ec1u \u0111\u1ebfn t\u1eeb Tr\u01b0\u1eddng \u0110\u1ea1i h\u1ecdc Kinh t\u1ebf - \u0110\u1ea1i h\u1ecdc Qu\u1ed1c gia H\u00e0 N\u1ed9i. \u0110\u1eb7c bi\u1ec7t l\u00e0 khi m\u1edbi tham gia cu\u1ed9c thi n\u00e0y m\u00ecnh th\u1eadm ch\u00ed ch\u01b0a c\u00f3 ki\u1ebfn th\u1ee9c g\u00ec v\u00e0 ph\u1ea3i t\u00ecm hi\u1ec3u t\u1eeb con s\u1ed1 0 v\u1ec1 Data Science =)))\n\nNh\u1eefng ng\u00e0y \u0111\u1ea7u team m\u00ecnh t\u00ecm hi\u1ec3u k\u1ef9 data c\u1ee7a cu\u1ed9c thi v\u00e0 t\u00ecm c\u00e1ch x\u1eed l\u00fd t\u1ed1t nh\u1ea5t c\u00f3 th\u1ec3. Team \u0111\u00e3 m\u00f4 t\u1ea3 t\u1eebng Field m\u1ed9t v\u00e0 t\u1ed5ng h\u1ee3p l\u1ea1i th\u00e0nh file Google Sheet \u0111\u1ec3 c\u1ea3 team ti\u1ec7n theo d\u00f5i: https:\/\/docs.google.com\/spreadsheets\/d\/1OpcOCQ-0PZXALZWSlBj3lRqyTfv3kTKmJJ8x3ttCEdk\/\n\nC\u00f3 th\u1ec3 th\u1ea5y nh\u1eefng Field \u0111\u01b0\u1ee3c t\u00f4 m\u00e0u \u0111en l\u00e0 nh\u1eefng Field kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb khai th\u00e1c, c\u1ea7n lo\u1ea1i b\u1ecf kh\u1ecfi m\u00f4 h\u00ecnh. V\u00ec h\u1ea7u h\u1ebft c\u00e1c Field \u0111\u1ec1u kh\u00f4ng c\u00f3 t\u00ean n\u00ean team ph\u1ea3i d\u1ef1 \u0111o\u00e1n kh\u00e1 nhi\u1ec1u. V\u00ed d\u1ee5 nh\u01b0 Field_36 th\u00ec team m\u00ecnh \u0111o\u00e1n \u0111\u01b0\u1ee3c l\u00e0 \u0111\u1ea7u m\u00e3 c\u1ee7a th\u1ebb B\u1ea3o hi\u1ec3m x\u00e3 h\u1ed9i :))\n\nModel ban \u0111\u1ea7u team m\u00ecnh d\u1ef1 \u0111\u1ecbnh s\u1ebd d\u00f9ng Logistic Regression nh\u01b0 \u0111\u00e3 d\u00f9ng trong cu\u1ed9c thi Home Credit Default Risk tr\u01b0\u1edbc \u0111\u00f3. Sau khi x\u1eed l\u00fd v\u00e0 l\u00e0m s\u1ea1ch h\u1ebft data, team c\u00f3 ch\u1ea1y Logistic v\u00e0 H2O nh\u01b0ng public score \u0111\u1ec1u r\u1ea5t th\u1ea5p. Sau \u0111\u00f3 team chuy\u1ec3n sang l\u00e0m d\u1ef1a tr\u00ean Baseline model c\u1ee7a BTC,c\u1ed9ng v\u1edbi solution m\u00e0 c\u00e1c \u0111\u1ed9i thi chia s\u1ebb. D\u1ef1a tr\u00ean c\u00e1c model c\u00f3 s\u1eb5n, m\u00ecnh ti\u1ebfp t\u1ee5c x\u1eed l\u00fd th\u00eam data v\u1edbi hy v\u1ecdng c\u00f3 th\u1ec3 t\u0103ng gini.\n\nKhi g\u1ea7n k\u1ebft th\u00fac ch\u01b0\u01a1ng tr\u00ecnh, m\u00ecnh c\u00f3 \u00fd t\u01b0\u1edfng l\u00e0 k\u1ebft h\u1ee3p c\u00e1c k\u1ebft qu\u1ea3 c\u1ee7a nhi\u1ec1u model kh\u00e1c nhau, hy v\u1ecdng s\u1ebd tr\u00e1nh \u0111\u01b0\u1ee3c t\u00ecnh tr\u1ea1ng overfitting, m\u00ecnh \u0111\u00e3 cho ch\u1ea1y l\u1ea7n l\u01b0\u1ee3t 4 model v\u00e0 k\u1ebft h\u1ee3p 4 k\u1ebft qu\u1ea3 n\u00e0y l\u1ea1i \u0111\u1ec3 submit :v \nL\u00fac \u0111\u00f3 team \u0111\u1ecbnh l\u00e0m theo ki\u1ec3u Ensemble learning s\u1eed d\u1ee5ng Voting Classifier, ho\u1eb7c \u0111\u01b0a b\u00e0i to\u00e1n v\u1ec1 d\u1ea1ng linear \u0111\u1ec3 t\u00ecm h\u1ec7 s\u1ed1 t\u1ed1i \u01b0u nh\u1ea5t. Nh\u01b0ng do kh\u00f4ng c\u00f3 nhi\u1ec1u th\u1eddi gian, bu\u1ed5i t\u1ed1i h\u00f4m tr\u01b0\u1edbc khi k\u1ebft th\u00fac ch\u01b0\u01a1ng tr\u00ecnh m\u00ecnh \u0111\u00e3 t\u1ef1 ch\u1ecdn ra t\u1ef7 l\u1ec7 gi\u1eefa c\u00e1c submission sao cho public score cao ch\u00fat l\u00e0 \u0111\u01b0\u1ee3c =)))\n\nD\u01b0\u1edbi \u0111\u00e2y l\u00e0 4 model m\u00e0 m\u00ecnh \u0111\u00e3 s\u1eed d\u1ee5ng, t\u1ed5ng th\u1eddi gian ch\u1ea1y kh\u00e1 l\u00e2u, kho\u1ea3ng 2 ti\u1ebfng \u0111\u1ed3ng h\u1ed3 :v M\u1eb7c d\u00f9 c\u00f2n t\u1ef1 \"m\u00f2 m\u1eabm\" nhi\u1ec1u th\u1ee9, nh\u01b0ng qua cu\u1ed9c thi m\u00ecnh \u0111\u00e3 h\u1ecdc \u0111\u01b0\u1ee3c r\u1ea5t nhi\u1ec1u \u0111i\u1ec1u v\u1ec1 c\u00e1ch x\u1eed l\u00fd d\u1eef li\u1ec7u v\u00e0 x\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh. \n\nNh\u00f3m xin g\u1eedi l\u1eddi c\u00e1m \u01a1n \u0111\u1ebfn BTC ch\u01b0\u01a1ng tr\u00ecnh \u0111\u00e3 t\u1ea1o ra m\u1ed9t cu\u1ed9c thi v\u00f4 c\u00f9ng \u00fd ngh\u0129a n\u00e0y, t\u1ea1o c\u01a1 h\u1ed9i cho nh\u1eefng newbies nh\u01b0 m\u00ecnh c\u00f3 th\u1ec3 h\u1ecdc h\u1ecfi nhi\u1ec1u \u0111i\u1ec1u t\u1eeb m\u1ecdi ng\u01b0\u1eddi <3","5f589ea1":"# Model 4\nModel cu\u1ed1i m\u00ecnh d\u1ef1a tr\u00ean solution chia s\u1ebb c\u1ee7a anh Nguy\u1ec5n H\u1eefu M\u1ea1nh, s\u1eed d\u1ee5ng Catboost. \u0110\u00e2y l\u00e0 solution c\u0169 c\u1ee7a anh n\u00ean kh\u00f4ng th\u1ec3 truy c\u1eadp l\u1ea1i \u0111\u01b0\u1ee3c n\u1eefa (https:\/\/www.kaggle.com\/manhitv\/kalapa-credit-scoring). Anh c\u00f3 vi\u1ebft l\u1ea1i solution m\u1edbi gi\u00fap anh \u0111\u1ea1t #1 Gi\u1ea3i c\u00e1 nh\u00e2n: https:\/\/www.kaggle.com\/manhitv\/1-kalapa-credit-scoring","fb5cc2b3":"# Model 1\n\nModel \u0111\u1ea7u ti\u00ean m\u00ecnh d\u1ef1a tr\u00ean model Lightgbm c\u1ee7a b\u1ea1n Ph\u1ea1m Ng\u1ecdc B\u1ea3o Anh (https:\/\/colab.research.google.com\/drive\/1kzcFJomC86e7NTNdcPtjbwyXEPLZZcxd)\n\nM\u00ecnh drop nh\u1eefng Field \u0111\u01b0\u1ee3c b\u00f4i \u0111en trong ggsheet v\u00e0 drop c\u00e1c feature kh\u00f4ng quan tr\u1ecdng d\u1ef1a tr\u00ean b\u00e0i chia s\u1ebb c\u1ee7a anh \u0110\u00e0o Trung Hi\u1ebfu (https:\/\/www.kaggle.com\/hieutrungdao\/kalapa-eda). Ngo\u00e0i ra m\u00ecnh c\u00f3 x\u1eed l\u00fd th\u00eam m\u1ed9t s\u1ed1 Field category.\nModel 1 sau khi ch\u1ea1y tr\u00ean Google Colab th\u00ec Public score \u0111\u1ea1t kho\u1ea3ng 0.50103","ac6635e0":"Sau khi c\u00f3 4 file submission c\u1ee7a 4 model, m\u00ecnh t\u1ed5ng h\u1ee3p 4 k\u1ebft qu\u1ea3 n\u00e0y b\u1eb1ng 4 t\u1ef7 l\u1ec7 l\u1ea7n l\u01b0\u1ee3t l\u00e0 0.4, 0.1, 0.2 v\u00e0 0.3. K\u1ebft qu\u1ea3 public score \u0111\u1ea1t 0.50539 v\u00e0 Private \u0111\u1ea1t 0.44113. Tuy nhi\u00ean k\u1ebft qu\u1ea3 n\u00e0y l\u00e0 m\u00ecnh ch\u1ea1y tr\u00ean Google Colab, n\u00ean khi ch\u1ea1y tr\u00ean Kaggle c\u00f3 th\u1ec3 thay \u0111\u1ed5i \u0111\u00f4i ch\u00fat.","c1d81d1d":"# Model 2\nModel 2 c\u0169ng gi\u1ed1ng nh\u01b0 model 1, nh\u01b0ng m\u00ecnh c\u00f3 drop th\u00eam 2 Field \"gioiTinh\",\"info_social_sex\" v\u00e0 ch\u1ec9 drop c\u00e1c Field nh\u01b0 trong ggsheet m\u00e0 team m\u00ecnh mu\u1ed1n lo\u1ea1i b\u1ecf.\nB\u00ean c\u1ea1nh \u0111\u00f3 m\u00ecnh c\u00f3 x\u1eed l\u00fd th\u00eam m\u1ed9t s\u1ed1 Field v\u1ec1 location, tuy nhi\u00ean khi submit k\u1ebft qu\u1ea3 Model 2 l\u1ea1i th\u1ea5p h\u01a1n Model 1 (Public score \u0111\u1ea1t 0.49920)."}}