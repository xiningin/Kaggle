{"cell_type":{"c4ee71c8":"code","35aa3fc3":"code","ef3add9e":"code","85d2e150":"code","fe1edd53":"code","605eeaa3":"code","9d2e5f85":"code","b09211be":"code","ee3479dc":"code","be9a8f82":"code","887819fb":"code","99ee169d":"code","eb1edbc6":"code","063cb31b":"code","b5efbc90":"code","57050ebe":"code","f7a3d16d":"code","4b646f2c":"code","38275940":"code","1ffa6a52":"code","7969b63d":"code","b9ebe221":"code","d43edc3d":"code","6f151254":"code","9584cd92":"code","d98cd9a0":"code","a54a8284":"code","dcf038c6":"code","b7938145":"code","be0c8887":"markdown","851c25c2":"markdown","0ff9293d":"markdown","7a82d05f":"markdown","83717b16":"markdown","5f82cb9f":"markdown","bed0a0d8":"markdown","7e18af26":"markdown","f9d3eeb7":"markdown","a50e7d33":"markdown","a4942ed2":"markdown","bc41258f":"markdown","b8f727de":"markdown","bdf08f27":"markdown","01ea6057":"markdown","0aacdff6":"markdown","f9be68af":"markdown","664fb4e2":"markdown","47fdbac2":"markdown"},"source":{"c4ee71c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","35aa3fc3":"train = pd.read_csv(\"\/kaggle\/input\/task-04\/training_data.csv\")\ntrain.drop([\"id\", \"num_atend\"], axis = 1, inplace = True)","ef3add9e":"test = pd.read_csv(\"\/kaggle\/input\/task-04\/test_data_without_label.csv\")\nId = test.id\ntest.drop([\"id\", \"num_atend\"], axis = 1, inplace = True)","85d2e150":"nulos = pd.concat([train.isna().sum(), round((train.isna().sum()\/train.shape[0])*100, 2)], axis = 1).reset_index()\nnulos.columns = [\"coluna\", \"absoluto\", \"porcentagem\"]\nnulos ","fe1edd53":"def arrumaTemperatura(df):\n    \n    df.temperatura = df.temperatura.apply(abs) #Muda valores negativos para positivo\n    df.temperatura = df.temperatura.replace(0, np.nan) #temperatura 0 vai pra np.nan\n    \n    df.temperatura = df.temperatura.apply(lambda x: x+30 if x < 10 else x) #temperatura 6 vai pra 36\n    \n    df.temperatura = df.temperatura.apply(lambda x: x\/1000 if x \/\/ 1000 != 0 else x) #Muda 3602 para 36.02\n    df.temperatura = df.temperatura.apply(lambda x: x\/100 if x \/\/ 100 != 0 else x) #Muda 350 para 35.0\n    \n    #Se a temperatura estiver maior q 45 ou menor do q 10 \u00e9 melhor vc ligar pro cemit\u00e9rio e n pro m\u00e9dico.\n    df.temperatura = df.temperatura.apply(lambda x: np.nan if x >= 45 else x) \n    df.temperatura = df.temperatura.apply(lambda x: np.nan if x <= 10 else x) \n    \n    return df","605eeaa3":"def arrumaPulso(df):\n    \n    df.pulso = df.pulso.replace(0, np.nan) #pulso 0 vai pra np.nan\n    \n    df.pulso = df.pulso.apply(lambda x: x*10 if 5 <= x <= 15 else x) #Pulso entre 5 e 15 vai pra entre 50 e 150\n    df.pulso = df.pulso.apply(lambda x: x\/10 if 500 <= x <= 1000 else x) #Pulso entre 500 e 1000 vai pra entre 50 e 100\n\n    df.pulso = df.pulso.apply(lambda x: np.nan if 0 < x <= 50 else x) #Se o pulso continuar estiver muito baixo, coloca como NA\n    \n    return df","9d2e5f85":"def arrumaRespiracao(df):\n    \n    df.respiracao = df.respiracao.replace(0, np.nan) #respiracao 0 vai pra np.nan\n    \n    return df","b09211be":"def arrumaPressao(df, flagTeste):\n    \n    #Como os valores da press\u00e3o m\u00ednima estavam menores que os valores da press\u00e3o m\u00e1xima,\n    # n\u00f3s trocamos os nomes das vari\u00e1veis para fazer mais sentido.\n    if flagTeste:\n        pass\n    else:\n        df.columns = [\"temperatura\", \"pulso\", \"respiracao\", \"pa_max\", \"pa_min\", \"sepse\"]\n    \n    df.pa_min = df.pa_min.replace(0, np.nan) #0 vai pra np.nan\n    df.pa_max = df.pa_max.replace(0, np.nan) #0 vai pra np.nan\n    \n    df.pa_min = df.pa_min.apply(lambda x: x\/1000 if x \/\/ 1000 != 0 else x) #Muda 3602 para 36.02\n    df.pa_max = df.pa_max.apply(lambda x: x\/1000 if x \/\/ 1000 != 0 else x) #Muda 3602 para 36.02\n    \n    df.pa_min = df.pa_min.apply(lambda x: x\/10 if 500 <= x <= 1000 else x) #entre 500 e 1000 vai pra entre 50 e 100\n    df.pa_max = df.pa_max.apply(lambda x: x\/10 if 500 <= x <= 1000 else x) #entre 500 e 1000 vai pra entre 50 e 100\n    \n    df.pa_min = df.pa_min.apply(lambda x: np.nan if 0 < x <= 30 else x) #Se a press\u00e3o estiver muito pequena, coloca como NA \n    df.pa_max = df.pa_max.apply(lambda x: np.nan if 0 < x <= 30 else x) #Se a press\u00e3o estiver muito pequena, coloca como NA \n    \n    return df","ee3479dc":"train = arrumaTemperatura(train)\ntrain = arrumaPulso(train)\ntrain = arrumaRespiracao(train)\ntrain = arrumaPressao(train, False)\n\ntest = arrumaTemperatura(test)\ntest = arrumaPulso(test)\ntest = arrumaRespiracao(test)\ntest = arrumaPressao(test, True)","be9a8f82":"nulos = pd.concat([train.isna().sum(), round((train.isna().sum()\/train.shape[0])*100, 2)], axis = 1).reset_index()\nnulos.columns = [\"coluna\", \"absoluto\", \"porcentagem\"]\nnulos ","887819fb":"corr = train.corr(method = \"pearson\")\nsns.heatmap(corr, cmap=\"Blues\", annot=True)","99ee169d":"corr = train.corr(method = \"spearman\")\nsns.heatmap(corr, cmap=\"Blues\", annot=True)","eb1edbc6":"sns.set_theme(style=\"ticks\")\nsns.pairplot(train, hue=\"sepse\")","063cb31b":"def arrumaNA(df, target, flagTest):\n    \n    if flagTest == False: #Train\n        for col in [x for x in df.columns if x != target]:\n            mediana = df.groupby('sepse')[col].transform('median')\n            df[col] = df[col].fillna(mediana)\n    else: #Test\n        for col in [x for x in df.columns if x != target]:\n            mediana = df[col].median()\n            df[col] = df[col].fillna(mediana)\n        \n    return df\n        \ntrain = arrumaNA(train, \"sepse\", False)\ntest = arrumaNA(test, \"sepse\", True)","b5efbc90":"train.describe()","57050ebe":"from sklearn.model_selection import train_test_split\n\ndf = train.copy()\n\ncolumns = df.columns.values\ntarget = \"sepse\"\ny_columns = [target]\nx_columns = [x for x in columns if x != target]\n\nX = df[x_columns]\ny = df[y_columns]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2, stratify = y)","f7a3d16d":"from sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import StackingClassifier\n\nfrom sklearn.pipeline import make_pipeline","4b646f2c":"def get_models():\n    models = dict()\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    models['svm'] = SVC()\n    models['bayes'] = GaussianNB()\n    return models\n\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='balanced_accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores","38275940":"#Aplica os modelos.\nmodels = get_models()\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n    \nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()\n\n#Aplica os modelos com StandardScaler()\nmodels = get_models()\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(make_pipeline(StandardScaler(), model), X, y)\n    results.append(scores)\n    names.append(\"std_\"+name)\n    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n    \nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","1ffa6a52":"def get_stacking():\n    \n    level0 = list()\n    level0.append(('lr', LogisticRegression()))\n    level0.append(('knn', KNeighborsClassifier()))\n    level0.append(('cart', DecisionTreeClassifier()))\n    level0.append(('svm', SVC()))\n    level0.append(('bayes', GaussianNB()))\n    \n    level1 = LogisticRegression()\n    \n    model = StackingClassifier(estimators=level0, final_estimator = level1, stack_method = \"predict\",\n                               cv=5)\n    return model\n\ndef get_models():\n    models = dict()\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    models['svm'] = SVC()\n    models['bayes'] = GaussianNB()\n    models['stacking'] = get_stacking()\n    return models\n\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='balanced_accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores","7969b63d":"#Aplica os modelos.\nmodels = get_models()\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X_train, y_train)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n    \nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()\n\n\n#Aplica os modelos com StandardScaler()\nmodels = get_models()\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(make_pipeline(StandardScaler(), model), X_train, y_train)\n    results.append(scores)\n    names.append(\"std_\"+name)\n    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n    \nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","b9ebe221":"level0 = list()\nlevel0.append(('lr',make_pipeline(StandardScaler(),  LogisticRegression())))\nlevel0.append(('knn', make_pipeline(StandardScaler(), KNeighborsClassifier())))\nlevel0.append(('cart', make_pipeline(StandardScaler(), DecisionTreeClassifier())))\nlevel0.append(('svm', make_pipeline(StandardScaler(), SVC())))\nlevel0.append(('bayes', make_pipeline(StandardScaler(), GaussianNB())))\n  \nlevel1 = make_pipeline(StandardScaler(), LogisticRegression())\n  \nmodel = StackingClassifier(estimators=level0, final_estimator = level1, stack_method = \"predict\",\n                               cv=10)\n\nmodel.fit(X_train, y_train)","d43edc3d":"y_pred = model.predict(X_test)\n\nprint(\"Balanced Acc. Score: {}\".format(balanced_accuracy_score(y_test, y_pred)))","6f151254":"model.fit(X, y)\ny_pred = model.predict(test)\n\n#Propor\u00e7\u00e3o de valores preditos com sepse e sem sepse.\npd.concat([test,pd.DataFrame({\"sepse\":y_pred})], axis = 1).groupby(\"sepse\").size()","9584cd92":"predicoes = pd.DataFrame(data = {\"id\":Id.values, \"sepse\":y_pred})\npredicoes.to_csv(\"respostaStacking.csv\",index = False)","d98cd9a0":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score","a54a8284":"params = {\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.01],\n    \"max_depth\":[3,5,8],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"criterion\": [\"mse\"],\n    \"n_estimators\":[25, 100]\n    }\n\ngs = GridSearchCV(GradientBoostingClassifier(), params, n_jobs = -1, verbose = 5)\n\ngs.fit(X_train, y_train)\nprint(gs.best_estimator_)","dcf038c6":"gb_model = GradientBoostingClassifier(n_estimators = 100, learning_rate = 0.01, loss = 'deviance', \n                                 max_depth = 8, max_features = 'log2', criterion='mse', random_state=0)\ngb_model.fit(X_train, y_train)\n\ny_pred = gb_model.predict(X_test)\nscore_a = accuracy_score(y_pred, y_test)\nscore_ba = balanced_accuracy_score(y_pred, y_test)\nprint('Acur\u00e1cia:', round(score_a, 4))\nprint('Acur\u00e1cia balanceada:', round(score_ba, 4))","b7938145":"# Predi\u00e7\u00e3o do conjunto teste\npred = gb_model.predict(test)\npredicoes = pd.DataFrame(data = {\"id\": Id.values, \"sepse\": pred})\npredicoes.to_csv(\"gb.csv\",index = False, sep = \",\", decimal = \",\", float_format = str)","be0c8887":"Aplica as fun\u00e7\u00f5es criadas anteriormente no treino e no teste.","851c25c2":"# Introdu\u00e7\u00e3o a SEPSE","0ff9293d":"Tendo em vista os valores dos coeficientes de Pearson e Spearman e as visualiza\u00e7\u00f5es feitas acima, decidimos n\u00e3o usar modelos preditivos para tratar os valores NA.\n\nPortanto, a nova ideia \u00e9 usar a mediana dos valores segundo o grupo (sepse, n\u00e3o sepse) para o conjunto de treino e apenas a mediana para o teste.\n\nAcreditamos que a ideia da mediana deveria ter sido escolhida antes da ideia do tratamento atrav\u00e9s de modelos preditivos, j\u00e1 que ela \u00e9 mais simples e, como veremos mais adiante, gerou um resultado muito satisfat\u00f3rio.","7a82d05f":"Verifica como est\u00e1 a distribui\u00e7\u00e3o dos valores","83717b16":"Data frame com quantidade de NA por colunas (valor absoluto e porcentual)","5f82cb9f":"## Stacking Classifier","bed0a0d8":"## Gradient Boosting","7e18af26":"# Pr\u00e9 processamento","f9d3eeb7":"Note que o KNN e o SVM tem seu resultado melhorado quando se aplica a padroniza\u00e7\u00e3o dos dados.","a50e7d33":"A seguir, aplica os modelos que far\u00e3o parte do stacking para ver como se saem separados.\nAl\u00e9m disso, os modelos s\u00e3o aplicados sem e com transforma\u00e7\u00e3o (padroniza\u00e7\u00e3o) dos dados.","a4942ed2":"## Obs: \nInicialmente, a ideia era arrumar os valores NAs usando modelos como regress\u00e3o linear. Assim, observa-se a correla\u00e7\u00e3o linear entre as vari\u00e1veis atrav\u00e9s do coeficiente de Pearson.\n\nComo as vari\u00e1veis n\u00e3o tiveram uma correla\u00e7\u00e3o linear muito forte, usa-se o coeficiente de Spearman para verificar se h\u00e1 alguma correla\u00e7\u00e3o que n\u00e3o seja linear\n","bc41258f":"A seguir, h\u00e1 fun\u00e7\u00f5es para tratar dos valores (outliers e nulos) de cada vari\u00e1vel do conjunto de dados.","b8f727de":"# Modelos","bdf08f27":"![image.png](attachment:87bf50f3-6af4-4cf6-98d7-c18c45d08bb2.png)\n![image.png](attachment:e2be9073-569e-41f4-9406-6a6a90dab57c.png)","01ea6057":"**O QUE \u00c9 SEPSE?**\n\nDe modo simples, sepse \u00e9 a doen\u00e7a que surge quando germes, principalmente bact\u00e9rias, invadem a corrente sangu\u00ednea e provocam uma intensa resposta inflamat\u00f3ria por todo o organismo. Geralmente, as infec\u00e7\u00f5es come\u00e7am em locais espec\u00edficos do organismo, como pele, pulm\u00f5es, vias urin\u00e1rias, ouvidos, etc.\n\n**Sintomas:**\n\n*   Temperatura corporal maior que 38\u00baC ou menor que 35\u00baC.\n*   Frequ\u00eancia card\u00edaca maior que 90 batimentos por minuto.\n*   Frequ\u00eancia respirat\u00f3ria maior que 20 incurs\u00f5es por minutos.\n\n**Considera-se sepse grave aquelas que apresentam:**\n\n*   Hipotens\u00e3o ou choque circulat\u00f3rio.\n*   Piora da fun\u00e7\u00e3o dos rins.\n*   Queda do n\u00famero de plaquetas.\n*   Altera\u00e7\u00e3o do estado de consci\u00eancia.\n*   Dificuldade respirat\u00f3ria.\n*   Altera\u00e7\u00f5es da coagula\u00e7\u00e3o.\n*   Diminui\u00e7\u00e3o da fun\u00e7\u00e3o do cora\u00e7\u00e3o.\n\nAcesso: https:\/\/www.mdsaude.com\/doencas-infecciosas\/sepse\/\n","0aacdff6":"Em seguida, aplica os modelos separadamente no conjunto de dados novamente e aplica tamb\u00e9m o Stacking destes modelos.\n\nNovamente, usa os modelos sem e com transforma\u00e7\u00e3o (padroniza\u00e7\u00e3o) dos dados.","f9be68af":"# Tratamento de valores NA","664fb4e2":"Para os modelos, n\u00f3s juntamos as ideias de dois integrantes do grupo.\n\n* 1\u00ba - Stacking de Naive Bayes, KNN, Dec. Tree, Logistic Reg. e SVM\n* 2\u00ba - Gradient Boosting","47fdbac2":"Novamente um data frame com quantidade de NA por colunas (valor absoluto e porcentual).\nIsso \u00e9 feito para checar se houve um aumento muito grande no n\u00famero de valores NA ap\u00f3s a aplica\u00e7\u00e3o das fun\u00e7\u00f5es apresentadas acima."}}