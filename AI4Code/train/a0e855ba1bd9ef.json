{"cell_type":{"8706a37c":"code","b866011b":"code","14adab6e":"code","ad027e1c":"code","5a109a8b":"code","55035bdf":"code","b6c6a75c":"code","6c4518ae":"code","995c3263":"code","5fd5cd5c":"code","d4533bc0":"code","5d64d00f":"code","94bb8a4b":"code","8256948c":"code","eb641c26":"code","1ed8f030":"code","c3784174":"code","87de7e96":"code","cd4b6228":"code","7594869e":"code","3f0598f4":"code","709fb8c4":"code","692b9b14":"code","50ad5d70":"code","56446131":"code","37454807":"code","1d1ca545":"code","33a5d898":"code","f3a89e66":"code","fd2f9c8e":"code","cb33e678":"code","ef918f73":"code","905349b5":"code","70abbda8":"code","bd5a60ba":"code","e561a3e2":"code","263fafc2":"markdown","e2421b7b":"markdown","605dae92":"markdown","0f5c9a71":"markdown","705e9be1":"markdown","c5495761":"markdown","2a1da278":"markdown","0a690917":"markdown","9fdca197":"markdown","ebbe76a3":"markdown","e1812368":"markdown","b81e8dcb":"markdown","6f51b447":"markdown","ce1319dc":"markdown","e9dcc90f":"markdown","ec9d41cc":"markdown","a02e6a21":"markdown","045fe84b":"markdown","2f98a55b":"markdown","a1753d3d":"markdown","3501dfd0":"markdown","b1f7d831":"markdown","76f5794e":"markdown","52f5d6b9":"markdown","a5c4d4a6":"markdown"},"source":{"8706a37c":"import pandas as pd\n\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n\n\ndf = pd.read_csv('..\/input\/ionosphere-data\/ionosphere_data.csv')","b866011b":"df.head()","14adab6e":"df.info()","ad027e1c":"df.isnull().sum()","5a109a8b":"from sklearn.utils import shuffle\n\ndf = shuffle(df)","55035bdf":"df.reset_index(inplace=True, drop=True)","b6c6a75c":"df.head()","6c4518ae":"df.shape","995c3263":"train_data = df.iloc[0:210, 0:-1]\ntrain_targets = df.iloc[0:210, -1]\n\ntest_data = df.iloc[210:, 0:-1]\ntest_targets = df.iloc[210:, -1] ","5fd5cd5c":"print(train_data.shape)\nprint(train_targets.shape)\nprint(test_data.shape)\nprint(test_targets.shape)","d4533bc0":"train_targets.head()","5d64d00f":"df['feature1'].value_counts()","94bb8a4b":"df['feature2'].value_counts()","8256948c":"train_data.drop('feature2', inplace=True, axis=1)","eb641c26":"test_data.drop('feature2', inplace=True, axis=1)","1ed8f030":"# if we normalize targets too then we will predict normalized values which will need inverse normalization\n\nmean = train_data.mean()\nstd = train_data.std()\n\ntrain_data-=mean\ntrain_data\/=std\n\ntest_data-=mean\ntest_data\/=std\n","c3784174":"train_data.head()","87de7e96":"test_data.head()","cd4b6228":"pd.options.display.float_format = '{:.4f}'.format","7594869e":"train_data.mean()[0:5]","3f0598f4":"train_data.std()[0:5]","709fb8c4":"train_targets = train_targets.map({'g':1, 'b':0})","692b9b14":"train_targets.head()","50ad5d70":"test_targets = test_targets.map({'g':1, 'b':0})","56446131":"test_targets.head()","37454807":"from keras import models, layers, regularizers\n\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(16, input_shape=(train_data.shape[1],), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(layers.Dense(10, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(layers.Dense(6, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","1d1ca545":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","33a5d898":"# import numpy as np\n\n# train_targets = np.asarray(train_targets).astype('float32').reshape((-1,1))\n# test_targets = np.asarray(test_targets).astype('float32').reshape((-1,1))","f3a89e66":"history = model.fit(train_data, train_targets, validation_split=0.2, epochs=500)","fd2f9c8e":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nloss = history.history['accuracy']\nval_loss = history.history['val_accuracy']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, val_loss, 'r', label='Validation acc', linewidth=10)\nplt.plot(epochs, loss, 'b--', label='Training acc', linewidth=4)\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('acc')\nplt.legend()\nplt.show()","cb33e678":"from keras import models, layers, regularizers\n\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(16, input_shape=(train_data.shape[1],), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(layers.Dense(10, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(layers.Dense(6, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(train_data, train_targets, validation_split=0.2, epochs=103)","ef918f73":"model.evaluate(test_data, test_targets)","905349b5":"predictions = model.predict(test_data)","70abbda8":"len(predictions)","bd5a60ba":"import numpy as np","e561a3e2":"pd.DataFrame({'Actual':test_targets, 'Predicted':np.round(predictions.reshape(141,))})","263fafc2":"# Splitting Data","e2421b7b":"# Encode labels","605dae92":"# Prediction Step","0f5c9a71":"# Checking Missing Values","705e9be1":"### A little bit of feature engineering: Feature 2 has only 1 value (0), which is not very helpful in predictions. So it can be dropped.","c5495761":"# Train the Model with Epochs (100)","2a1da278":"### WORKFLOW :\n- Load Data\n- Check Missing Values ( If Exist ; Fill each record with mean of its feature ) or any usless column.\n- Shuffle the data if needed.\n- Standardized the Input Variables. **Hint**: Centeralized the data\n- Split into 60 and 40 ratio.\n- Encode labels.\n- Model : 1 hidden layers including 16 unit.\n- Compilation Step (Note : Its a Binary problem , select loss , metrics according to it)\n- Train the Model with Epochs (100).\n- If the model gets overfit tune your model by changing the units , No. of layers , epochs , add dropout layer or add Regularizer according to the need .\n- Prediction should be > **92%**\n- Evaluation Step\n- Prediction\n","0a690917":"# Prediction should be > 92%","9fdca197":"### If the model gets overfit tune your model by changing the units , No. of layers , epochs , add dropout layer or add Regularizer according to the need ","ebbe76a3":"# Final Model","e1812368":"we have to split the dataset in 60-40.","b81e8dcb":"There is no missing data, so there is no need for handling missing data.","6f51b447":"# Evaluation Step","ce1319dc":"## Note: We have to do the same transformations (dropping, replacing etc) on test data that we do in training data","e9dcc90f":"# Assignment: Ionosphere Data Problem\n\n### Dataset Description: \n\nThis radar data was collected by a system in Goose Bay, Labrador. This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. See the paper for more details. The targets were free electrons in the ionosphere. \"Good\" radar returns are those showing evidence of some type of structure in the ionosphere. \"Bad\" returns are those that do not; their signals pass through the ionosphere.\n\nReceived signals were processed using an autocorrelation function whose arguments are the time of a pulse and the pulse number. There were 17 pulse numbers for the Goose Bay system. Instances in this databse are described by 2 attributes per pulse number, corresponding to the complex values returned by the function resulting from the complex electromagnetic signal.\n\n### Attribute Information:\n\n- All 34 are continuous\n- The 35th attribute is either \"good\" or \"bad\" according to the definition summarized above. This is a binary classification task.\n\n <br><br>\n\n<table border=\"1\"  cellpadding=\"6\">\n\t<tbody>\n        <tr>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Data Set Characteristics:&nbsp;&nbsp;<\/b><\/p><\/td>\n\t\t<td><p class=\"normal\">Multivariate<\/p><\/td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Number of Instances:<\/b><\/p><\/td>\n\t\t<td><p class=\"normal\">351<\/p><\/td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Area:<\/b><\/p><\/td>\n\t\t<td><p class=\"normal\">Physical<\/p><\/td>\n        <\/tr>\n     <\/tbody>\n    <\/table>\n<table border=\"1\" cellpadding=\"6\">\n    <tbody>\n        <tr>\n            <td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Attribute Characteristics:<\/b><\/p><\/td>\n            <td><p class=\"normal\">Integer,Real<\/p><\/td>\n            <td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Number of Attributes:<\/b><\/p><\/td>\n            <td><p class=\"normal\">34<\/p><\/td>\n            <td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Date Donated<\/b><\/p><\/td>\n            <td><p class=\"normal\">N\/A<\/p><\/td>\n        <\/tr>\n     <\/tbody>\n    <\/table>\n<table border=\"1\" cellpadding=\"6\">\t\n    <tbody>\n    <tr>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Associated Tasks:<\/b><\/p><\/td>\n\t\t<td><p class=\"normal\">Classification<\/p><\/td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Missing Values?<\/b><\/p><\/td>\n\t\t<td><p class=\"normal\">N\/A<\/p><\/td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Number of Web Hits:<\/b><\/p><\/td>\n\t\t<td><p class=\"normal\">N\/A<\/p><\/td>\n\t<\/tr>\n    <\/tbody>\n    <\/table>","ec9d41cc":"# Standardize the Input Variables\nHint: Centeralized the data","a02e6a21":"model hits 92.8% accuracy on 103 epochs","045fe84b":"# Shuffling Data","2f98a55b":"# Load Data:\n[Click Here to Download DataSet](https:\/\/github.com\/ramsha275\/ML_Datasets\/blob\/main\/ionosphere_data.csv)","a1753d3d":"# Compilation Step \n(Note : Its a Binary problem , select loss , metrics according to it)","3501dfd0":"![image.png](attachment:image.png)","b1f7d831":"Shuffling of data is important to break any bias during training, for example, without shuffling it is possible that 70% training data is 'g' and 30% is 'b' etc","76f5794e":"we have 34 features, all are numerical.","52f5d6b9":"By looking at the data, feature 1 and feature 2 seem to be binary features. Standardization\/ Normalization has no effect on binary features.\nAlso, it may result in NaN values. For example, feature 2 results in NaN values afer normalization","a5c4d4a6":"# Model : 1 hidden layers including 16 units"}}