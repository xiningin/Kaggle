{"cell_type":{"04d0e95d":"code","8c2c2bc4":"code","2365efda":"code","42bca8d8":"code","328c224b":"code","c8e8510c":"code","eeff64ce":"code","758ea27b":"code","8018e9a8":"code","d6c96578":"code","06141540":"code","5649f969":"code","4a35e4d6":"code","289816fe":"code","0bddcedb":"code","65e17c42":"code","3378fce9":"code","c675d948":"code","a1f7bc93":"code","c0bdb68e":"code","19b0bf67":"code","68a7dcb4":"code","0a15f3b0":"code","eb362a76":"code","63f56fd1":"code","44732b93":"code","15f6e745":"code","b71d4d08":"code","1901a1d5":"code","fa7b197b":"code","142e055f":"code","0a401330":"code","ba12e3a6":"code","69100376":"code","bc24d958":"code","3a641edc":"code","3f6d825a":"code","03c1551e":"code","89006c31":"code","1670037d":"markdown","ec741ce0":"markdown","3639d963":"markdown","8e508675":"markdown","4e04cc70":"markdown","1b2fe944":"markdown","8a508ca7":"markdown","9f05379a":"markdown","7e5d38e2":"markdown","b4087b53":"markdown","ba3a317a":"markdown","cd342d89":"markdown","55e08a49":"markdown","349fd73f":"markdown","4c94299d":"markdown","2e373cf3":"markdown"},"source":{"04d0e95d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport string\nimport tqdm\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\nfrom nltk.corpus import stopwords\nimport xgboost\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,HashingVectorizer\nfrom sklearn import decomposition, ensemble\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score,f1_score\n\nimport dask\nimport multiprocessing\nfrom joblib import delayed, Parallel, parallel_backend\n","8c2c2bc4":"cores = multiprocessing.cpu_count()","2365efda":"train_data = pd.read_csv(\"..\/input\/tweets-with-sarcasm-and-irony\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/tweets-with-sarcasm-and-irony\/test.csv\")","42bca8d8":"train_tweets=train_data['tweets'].tolist()\ntest_tweets=test_data['tweets'].tolist()","328c224b":"def keep_uniques(array, df):\n    dels=[]\n    for i in array:\n        if array.count(i)>1:\n            dels.append(i)\n    dels=list(set(dels))\n    for i in dels:\n        df.drop( df[ df['tweets'] == i ].index, inplace=True)\n    return df","c8e8510c":"train_data=keep_uniques(train_tweets, train_data)\ntest_data=keep_uniques(test_tweets, test_data)","eeff64ce":"len(train_data['tweets'].unique())","758ea27b":"len(test_data['tweets'].unique())","8018e9a8":"train_data.describe().T","d6c96578":"train_data = train_data.sample(frac = 1)\ntest_data = test_data.sample(frac = 1)","06141540":"train_data['class'].value_counts()","5649f969":"temp=train_data.loc[train_data['class'] == 'regular']\nlis=temp['tweets'].tolist()\n\nimport random\nreg_del=[]\nvisited=set()\nfor _ in range(3600):\n    n=random.randint(0,18556)\n    if n not in visited:\n        reg_del.append(lis[n])\n        \n        \nfor i in reg_del:\n    train_data.drop( train_data[ train_data['tweets'] == i ].index, inplace=True)","4a35e4d6":"train_data['class'].value_counts()","289816fe":"test_data['class'].value_counts()","0bddcedb":"test_data = test_data.dropna()","65e17c42":"def clean(tweet): \n    \n\n    # Special characters\n    tweet = re.sub(r\"\\x89\u00db_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d3\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cf\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00f7\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00aa\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c8\", \"\", tweet)\n    tweet = re.sub(r\"Jap\u00cc_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"\u00cc\u00a9\", \"e\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a8\", \"\", tweet)\n    tweet = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c7\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a33million\", \"3 million\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c0\", \"\", tweet)\n    \n    #emojis\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    tweet =  emoji_pattern.sub(r'', tweet)\n    \n    # usernames mentions like \"@abc123\"\n    ment = re.compile(r\"(@[A-Za-z0-9]+)\")\n    tweet =  ment.sub(r'', tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"don\u00e5\u00abt\", \"do not\", tweet)   \n            \n    # Character entity references\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # html tags\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    tweet = re.sub(html, '', tweet)\n    \n    # Urls\n    tweet = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", tweet)\n    tweet = re.sub(r'https?:\/\/\\S+|www\\.\\S+','', tweet)\n        \n    #Punctuations and special characters\n    \n    tweet = re.sub('[%s]' % re.escape(string.punctuation),'',tweet)\n    \n    tweet = tweet.lower()\n    \n    splits = tweet.split()\n    splits = [word for word in splits if word not in set(nltk.corpus.stopwords.words('english'))]\n    tweet = ' '.join(splits)\n    \n    \n    return tweet","3378fce9":"train_data['cleaned_text']= train_data['tweets'].apply((lambda x: clean(x))) \ntest_data['cleaned_text'] = test_data['tweets'].apply((lambda x: clean(x)))\nprint(\"Cleaned\")","c675d948":"train_data.head()","a1f7bc93":"sns.set(rc={'figure.figsize':(10,10)})\nsns.countplot(train_data['class'])","c0bdb68e":"from wordcloud import WordCloud\nstopwords = nltk.corpus.stopwords.words('english')\n\nplt.figure(figsize=(12,6))\ntext = ' '.join(train_data.cleaned_text[train_data['class']=='regular'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)\nplt.axis('off')\nplt.title('Regular Tweets',fontsize=25)\n\nplt.figure(figsize=(12,6))\ntext = ' '.join(train_data.cleaned_text[train_data['class']=='irony'])\nwc1 = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc1)\nplt.axis('off')\nplt.title('Irony Tweets',fontsize=25)\n\nplt.figure(figsize=(12,6))\ntext = ' '.join(train_data.cleaned_text[train_data['class']=='sarcasm'])\nwc2 = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc2)\nplt.axis('off')\nplt.title('Sarcasm Tweets',fontsize=25)\n\nplt.figure(figsize=(12,6))\ntext = ' '.join(train_data.cleaned_text[train_data['class']=='figurative'])\nwc3 = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc3)\nplt.axis('off')\nplt.title('Figurative Tweets',fontsize=25)","19b0bf67":"def encode_target(t_class):\n    t_class=str(t_class)\n    class_dict = {\n        'irony':0,\n        'sarcasm':1,\n        'regular':2,\n        'figurative':3\n    }\n    return class_dict[t_class]","68a7dcb4":"train_data[\"target\"] = train_data['class'].apply(lambda x: encode_target(x))\ntest_data[\"target\"] = test_data['class'].apply(lambda x: encode_target(x))","0a15f3b0":"train = train_data[['cleaned_text','target']]\ntrain.columns = ['text','labels']\n\ntest = test_data[['cleaned_text','target']]\ntest.columns = ['text','labels']","eb362a76":"train.head()","63f56fd1":"test.head()","44732b93":"traintexts=train['text'].tolist()\ntesttexts=test['text'].tolist()\n\nall_texts = traintexts + testtexts","15f6e745":"count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\ncount_vect.fit(all_texts)\n\n# transform the training and test data using count vectorizer object\nxtrain_count =  count_vect.transform(train['text'])\nxtest_count =  count_vect.transform(test['text'])","b71d4d08":"tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=55000)\ntfidf_vect.fit(all_texts)\nxtrain_tfidf =  tfidf_vect.transform(train['text'])\nxtest_tfidf =  tfidf_vect.transform(test['text'])","1901a1d5":"hash_vectorizer = HashingVectorizer(n_features=55000)\nhash_vectorizer.fit(all_texts)\nxtrain_hash_vectorizer =  hash_vectorizer.transform(train['text']) \nxtest_hash_vectorizer =  hash_vectorizer.transform(test['text'])","fa7b197b":"def train_model(classifier, feature_vector_train, label, feature_vector_test, test_y):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_test)\n    \n    return metrics.accuracy_score(predictions, test_y)","142e055f":"print(\"NAIVE BAYES CLASSIFIER\")\nprint(\"========================================================\")\n# Naive Bayes on Count Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train['labels'], xtest_count, test['labels'])\nprint(\"-> Count Vectors Accuracy: {}% \".format(round(accuracy,4)*100))\n\n# Naive Bayes on Word Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train['labels'], xtest_tfidf, test['labels'])\nprint(\"-> WordLevel TF-IDF Accuracy: {}%\".format(round(accuracy,4)*100))\n\n# Naive Bayes on Hash Vectors\n# accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_hash_vectorizer, train['labels'], xtest_hash_vectorizer, test['labels'])\n# print(\"-> Hash Vectors Accuracy: \", round(accuracy,2))","0a401330":"print(\"LINEAR CLASSIFIER\")\nprint(\"========================================================\")\n# Linear Classifier on Count Vectors\naccuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=13000, n_jobs=2), xtrain_count, train['labels'], xtest_count, test['labels'])\nprint(\"-> Count Vectors Accuracy: {}% \".format(round(accuracy,4)*100))\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=13000, n_jobs=2), xtrain_tfidf, train['labels'], xtest_tfidf, test['labels'])\nprint(\"-> WordLevel TF-IDF Accuracy: {}%\".format(round(accuracy,4)*100))\n\n# Linear Classifier on Hash Vectors\naccuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=13000, n_jobs=2), xtrain_hash_vectorizer, train['labels'], xtest_hash_vectorizer, test['labels'])\nprint(\"-> Hash Vectors Accuracy: {}%\".format(round(accuracy,4)*100))","ba12e3a6":"\nprint(\"XGBOOST CLASSIFIER\")\nprint(\"========================================================\")\n# Linear Classifier on Count Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_count, train['labels'], xtest_count, test['labels'])\nprint(\"-> Count Vectors Accuracy: {}% \".format(round(accuracy,4)*100))\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf, train['labels'], xtest_tfidf, test['labels'])\nprint(\"-> WordLevel TF-IDF Accuracy: {}%\".format(round(accuracy,4)*100))\n\n# Linear Classifier on Hash Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_hash_vectorizer, train['labels'], xtest_hash_vectorizer, test['labels'])\nprint(\"-> Hash Vectors Accuracy: {}%\".format(round(accuracy,4)*100))","69100376":"from sklearn.neighbors import KNeighborsClassifier","bc24d958":"print(\"KNN CLASSIFIER\")\nprint(\"========================================================\")\n# Linear Classifier on Count Vectors\naccuracy = train_model(KNeighborsClassifier(n_neighbors = 80, n_jobs=4) , xtrain_count, train['labels'], xtest_count, test['labels'])\nprint(\"-> Count Vectors Accuracy: {}% \".format(round(accuracy,4)*100))\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(KNeighborsClassifier(n_neighbors = 80, n_jobs=4) , xtrain_tfidf, train['labels'], xtest_tfidf, test['labels'])\nprint(\"-> WordLevel TF-IDF Accuracy: {}%\".format(round(accuracy,4)*100))\n\n# Linear Classifier on Hash Vectors\naccuracy = train_model(KNeighborsClassifier(n_neighbors = 80, n_jobs=4) , xtrain_hash_vectorizer, train['labels'], xtest_hash_vectorizer, test['labels'])\nprint(\"-> Hash Vectors Accuracy: {}%\".format(round(accuracy,4)*100))","3a641edc":"from sklearn.linear_model import SGDClassifier","3f6d825a":"print(\"SGD CLASSIFIER\")\nprint(\"========================================================\")\n# Linear Classifier on Count Vectors\naccuracy = train_model(SGDClassifier(max_iter=500, n_jobs=4) , xtrain_count, train['labels'], xtest_count, test['labels'])\nprint(\"-> Count Vectors Accuracy: {}% \".format(round(accuracy,4)*100))\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(SGDClassifier(max_iter=500, n_jobs=4) , xtrain_tfidf, train['labels'], xtest_tfidf, test['labels'])\nprint(\"-> WordLevel TF-IDF Accuracy: {}%\".format(round(accuracy,4)*100))\n\n# Linear Classifier on Hash Vectors\naccuracy = train_model(SGDClassifier(max_iter=500, n_jobs=4) , xtrain_hash_vectorizer, train['labels'], xtest_hash_vectorizer, test['labels'])\nprint(\"-> Hash Vectors Accuracy: {}%\".format(round(accuracy,4)*100))","03c1551e":"from sklearn.svm import LinearSVC","89006c31":"print(\"LINEAR SVC CLASSIFIER\")\nprint(\"========================================================\")\n# Linear Classifier on Count Vectors\naccuracy = train_model(LinearSVC(max_iter=3500) , xtrain_count, train['labels'], xtest_count, test['labels'])\nprint(\"-> Count Vectors Accuracy: {}% \".format(round(accuracy,4)*100))\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(LinearSVC(max_iter=3500) , xtrain_tfidf, train['labels'], xtest_tfidf, test['labels'])\nprint(\"-> WordLevel TF-IDF Accuracy: {}%\".format(round(accuracy,4)*100))\n\n# Linear Classifier on Hash Vectors\naccuracy = train_model(LinearSVC(max_iter=3500) , xtrain_hash_vectorizer, train['labels'], xtest_hash_vectorizer, test['labels'])\nprint(\"-> Hash Vectors Accuracy: {}%\".format(round(accuracy,4)*100))","1670037d":"### Naive Bayes Classifier","ec741ce0":"### Exploring Dataset","3639d963":"## Building the Model","8e508675":"### Preparing our train and test sets","4e04cc70":"Here, we see that the `regular` class has 18k tweets, which causes our dataset to be imbalanced. So we shall delete some tweets from this class","1b2fe944":"## Data Cleaning & Preprocessing","8a508ca7":"## Feature Extraction","9f05379a":"### 2. Word level tf-idf","7e5d38e2":"### 3. Hashing Vectorizer","b4087b53":"### XGBoost Classifier","ba3a317a":"### Linear Classifier","cd342d89":"### 1. Count Vectors","55e08a49":"### Loading data","349fd73f":"### Encode our text classes","4c94299d":"### Remove recurring tweets to prevent ambiguity","2e373cf3":"## EDA and Visualization"}}